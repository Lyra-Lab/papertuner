{
  "id": "http://arxiv.org/abs/2305.17473v3",
  "title": "A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU",
  "authors": [
    "Farhad Mortezapour Shiri",
    "Thinagaran Perumal",
    "Norwati Mustapha",
    "Raihani Mohamed"
  ],
  "abstract": "Deep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
  "text": "  \n \n \nReview Article \nA Comprehensive Overview and Comparative Analysis on \nDeep Learning Models \nFarhad Mortezapour Shiria\n, Thinagaran Perumala\n, Norwati Mustaphaa, and \nRaihani Mohameda\n \naFaculty of Computer Science and Information Technology, University Putra Malaysia (UPM), Serdang, \n43400, Malaysia \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n1 Introduction \nArtificial intelligence (AI) aims to emulate human-level intelligence in machines. In computer \nscience, AI refers to the study of \"intelligent agents,\" which are objects capable of perceiving their \nenvironment and taking actions to maximize their chances of achieving specific goals [1]. Machine \nlearning (ML) is a field that focuses on the development and application of methods capable of \nlearning from datasets [2]. ML finds extensive use in various domains, such as speech recognition, \nABSTRACT \n \nDeep learning (DL) has emerged as a powerful subset of machine \nlearning (ML) and artificial intelligence (AI), outperforming \ntraditional ML methods, especially in handling unstructured and \nlarge datasets. Its impact spans across various domains, including \nspeech recognition, healthcare, autonomous vehicles, cybersecurity, \npredictive analytics, and more. However, the complexity and \ndynamic nature of real-world problems present challenges in \ndesigning effective deep learning models. Consequently, several \ndeep learning models have been developed to address different \nproblems and applications. In this article, we conduct a \ncomprehensive survey of various deep learning models, including \nConvolutional Neural Network (CNN), Recurrent Neural Network \n(RNN), Temporal Convolutional Networks (TCN), Transformer, \nKolmogorov-Arnold networks (KAN), Generative Models, Deep \nReinforcement Learning (DRL), and Deep Transfer Learning. We \nexamine the structure, applications, benefits, and limitations of each \nmodel. Furthermore, we perform an analysis using three publicly \navailable datasets: IMDB, ARAS, and Fruit-360. We compared the \nperformance of six renowned deep learning models: CNN, RNN, \nLong Short-Term Memory (LSTM), Bidirectional LSTM, Gated \nRecurrent Unit (GRU), and Bidirectional GRU alongside two newer \nmodels, TCN and Transformer, using the IMDB and ARAS datasets. \nAdditionally, we evaluated the performance of eight CNN-based \nmodels, including VGG (Visual Geometry Group), Inception, \nResNet (Residual Network), InceptionResNet, Xception (Extreme \nInception), MobileNet, DenseNet (Dense Convolutional Network), \nand NASNet (Neural Architecture Search Network), for image \nclassification tasks using the Fruit-360 dataset. \nKEYWORDS \n \nDeep Learning \nConvolutional Neural Network (CNN) \nLong Short-Term Memory (LSTM) \nGated Recurrent Unit (GRU) \nTemporal Convolutional Network (TCN) \nTransformer \nKolmogorov-Arnold networks (KAN) \nDeep Reinforcement Learning (DRL) \nDeep Transfer Learning (DTL) \nAutoencoder \nGenerative Adversarial Network (GAN) \nDeep Belief Network (DBN) \n \n \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n2 \n \ncomputer vision, text analysis, video games, medical sciences, and cybersecurity. \nIn recent years, deep learning (DL) techniques, a subset of machine learning (ML), have \noutperformed traditional ML approaches across numerous tasks, driven by several critical \nadvancements [3]. The proliferation of large datasets has been pivotal in enabling models to learn \nintricate patterns and relationships, thereby significantly enhancing their performance [4]. \nConcurrently, advancements in hardware acceleration technologies, notably Graphics Processing \nUnits (GPUs) and Field-Programmable Gate Arrays (FPGAs) [5] have markedly reduced model \ntraining times by facilitating rapid computations and parallel processing capabilities. These \nadvancements have substantially accelerated the training process.2 Moreover, enhancements in \nalgorithmic techniques for optimization and training have further augmented the speed and \nefficiency of deep learning models, leading to quicker convergence and superior generalization \ncapabilities [4]. Deep learning techniques have demonstrated remarkable success across a wide \nrange of applications, including computer vision (CV), natural language processing (NLP), and \nspeech recognition. These applications underscore the transformative impact of DL in various \ndomains, where it continues to set new performance benchmarks [6, 7]. \nDeep learning models draw inspiration from the structure and functionality of the human \nnervous system and brain. These models employ input, hidden, and output layers to organize \nprocessing units. Within each layer, the nodes or units are interconnected with those in the layer \nbelow, and each connection is assigned to a weight value. The units sum the inputs after multiplying \nthem by their corresponding weights [8]. Fig. 1 illustrates the relationship between AI, ML, and \nDL, highlighting that machine learning and deep learning are subfields of artificial intelligence.  \nThe objective of this research is to provide a comprehensive overview of various deep learning \nmodels and compare their performance across different applications. In Section 2, we introduce a \nfundamental definition of deep learning. Section 3 covers supervised deep learning models, \nincluding Multi-Layer Perceptron (MLP), Convolutional Neural Networks (CNN), Recurrent \nNeural Networks (RNN), Temporal Convolutional Networks (TCN), and Kolmogorov-Arnold \nNetworks (KAN). Section 4 reviews generative models such as Autoencoders, Generative \nAdversarial Networks (GANs), and Deep Belief Networks (DBNs). Section 5 presents a \ncomprehensive survey of Transformer architecture. Deep Reinforcement Learning (DRL) is \ndiscussed in Section 6, while Section 7 addresses Deep Transfer Learning (DTL). The principles \nof hybrid deep learning are explored in Section 8, followed by a discussion of deep learning \napplications in Section 9. Section 10 surveys the challenges in deep learning and potential \nalternative solutions. In Section 11, we conduct experiments and analyze the performance of \ndifferent deep learning models using three datasets. Research directions and future aspects are \ncovered in Section 12. Finally, Section 13 concludes the paper. \n \n \n \n \n \n \n \n \n \n \nFigure 1. Relationship between artificial intelligence (AI), machine learning (ML), and deep \nlearning (DL). \n \nDeep \nLearning \nMachine \nLearning \nArtificial \nIntelligent \nF. M. Shiri et al.    \n3 \n \n2 Deep Learning  \nDeep learning (DL) involves the process of learning hierarchical representations of data by \nutilizing architectures with multiple hidden layers. With the advancement of high-performance \ncomputing facilities, deep learning techniques using deep neural networks have gained increasing \npopularity [9]. In a deep learning algorithm, data is passed through multiple layers, with each layer \nprogressively extracting features and transmitting information to the subsequent layer. The initial \nlayers extract low-level characteristics, which are then combined by later layers to form a \ncomprehensive representation [6]. \nIn traditional machine learning techniques, the classification task typically involves a \nsequential process that includes pre-processing, feature extraction, meticulous feature selection, \nlearning, and classification. The effectiveness of machine learning methods heavily relies on \naccurate feature selection, as biased feature selection can lead to incorrect class classification. In \ncontrast, deep learning models enable simultaneous learning and classification, eliminating the \nneed for separate steps. This capability makes deep learning particularly advantageous for \nautomating feature learning across diverse tasks [10]. Fig. 2 visually illustrates the distinction \nbetween deep learning and traditional machine learning in terms of feature extraction and learning. \nIn the era of deep learning, a wide array of methods and architectures have been developed. \nThese models can be broadly categorized into two main groups: discriminative (supervised) and \ngenerative (unsupervised) approaches. Among the discriminative models, two prominent groups \nare Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). Additionally, \ngenerative approaches encompass various models such as Generative Adversarial Networks (GANs) \nand Auto-Encoders (AEs) [11]. In the following sections, we provide a comprehensive survey of \ndifferent types of deep learning models. \n \n \nFigure 2. Visual illustration of the distinction between deep learning and traditional machine \nlearning in terms of feature extraction and learning [10]. \n3 Supervised Deep Learning Models \nIn supervised learning and classification tasks, this family of deep learning algorithms is used \nto perform discriminative functions. These supervised deep architectures typically model the \nposterior distributions of classes based on observable data, enabling effective pattern classification. \nCommon supervised models include Multi-Layer Perceptron (MLP), Convolutional Neural \nNetworks (CNN), Recurrent Neural Networks (RNN), Temporal Convolutional Networks (TCN), \nKolmogorov-Arnold Networks (KAN), and their variations. A brief overview of these methods \nfollows. \n3.1 Multi Layers Perceptron (MLP)  \nThe Multi-Layer Perceptron (MLP) model is a type of feedforward artificial neural network \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n4 \n \n(ANN) that serves as a foundation architecture for deep learning or deep neural networks (DNNs) \n[11]. It operates as a supervised learning approach. The MLP consists of three layers: the input \nlayer, the output layer, and one or more hidden layers [12]. It is a fully connected network, meaning \neach neuron in one layer is connected to all neurons in the subsequent layer. \nIn an MLP, the input layer receives the input data and performs feature normalization. The \nhidden layers, which can vary in number, process the input signals. The output layer makes \ndecisions or predictions based on the processed information [13]. Fig. 3 (a) depicts a single-neuron \nperceptron model, where the activation function φ (Eq. (1)) is a non-linear function used to map \nthe summation function (𝑥𝑤+ 𝑏) to the output value 𝑦. \n𝑦 =  𝜑(𝑥𝑤+ 𝑏)                                                                                                                                            (1) \nIn Eq. (1), the terms 𝑥, 𝑤, 𝑏, and 𝑦 represent the input vector, weighting vector, bias, and \noutput value, respectively [14]. Fig. 3 (b) illustrates the structure of the multilayer perceptron (MLP) \nmodel. \n \n \n \n \n \n \n \n \n \nFigure 3. (a) Single-neuron perceptron model. (b) Structure of the MLP [14]. \n3.2 Convolutional Neural Networks (CNN) \n \nConvolutional Neural Networks (CNNs) are a powerful class of deep learning models widely \napplied in various tasks, including object detection, speech recognition, computer vision, image \nclassification, and bioinformatics [15]. They have also demonstrated success in time series \nprediction tasks [16]. CNNs are feedforward neural networks that leverage convolutional structures \nto extract features from data [17]. CNN has a two-stage architecture that combines a classifier and \na feature extractor to provide automatic feature extraction and end-to-end training with the least \namount of pre-processing necessary [18]. Unlike traditional methods, CNNs automatically learn \nand recognize features from the data without the need for manual feature extraction by humans \n[19]. The design of CNNs is inspired by visual perception [17]. The major components of CNNs \ninclude the convolutional layer, pooling layer, fully connected layer, and activation function [20, \n21]. Fig. 4 presents the pipeline of the convolutional neural network, highlighting how each layer \ncontributes to the efficient processing and successful progression of input data through the network. \n \n \n \n \n \n \n \nFigure 4. The pipeline of a Convolutional Neural Network. \nInput \nData \nClass 1 \nClass N \nFully connected layer \nconvolution layer \nPooling layer \n \n(a) \n \n(b) \nF. M. Shiri et al.    \n5 \n \n \nFigure 5. Schematic diagram of the convolution process [22]. \n \nConvolutional Layer: The convolutional layer is a pivotal component of CNN. Through \nmultiple convolutional layers, the convolution operation extracts distinct features from the input. \nIn image classification, lower layers tend to capture basic features such as texture, lines, and edges, \nwhile higher layers extract more abstract features. The convolutional layer comprises learnable \nconvolution kernels, which are weight matrices typically of equal length, width, and an odd number \n(e.g., 3x3, 5x5, or 7x7). These kernels are convolved with the input feature maps, sliding over the \nregions of the feature map and executing convolution operations [22]. Fig. 5 illustrates the \nschematic diagram of the convolution process. \nPooling Layer: Typically following the convolutional layer, the pooling layer reduces the \nnumber of connections in the network by performing down-sampling and dimensionality reduction \non the input data [23]. Its primary purpose is to alleviate the computational burden and address \noverfitting issues [24]. Moreover, the pooling layer enables CNN to recognize objects even when \ntheir shapes are distorted or viewed from different angles, by incorporating various dimensions of \nan image through pooling [25]. The pooling operation produces output feature maps that are more \nrobust against distortion and errors in individual neurons [26]. There are various pooling methods, \nincluding Max Pooling, Average Pooling, Spatial Pyramid Pooling, Mixed Pooling, Multi-Scale \nOrder-Less, and Stochastic Pooling [27-30]. Fig. 6 depicts an example of Max Pooling, where a \nwindow slides across the input, and the contents of the window are processed by a pooling function \n[31]. \n \nFigure 6. Computing the output values of a 3 × 3 max pooling operation on a 5 × 5 input. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n6 \n \n \n \n \n \n \n \n \n \n \n \nFigure 7. The general structure of activation functions. \n \nFully Connected (FC) Layer: The FC layer is typically located at the end of a CNN \narchitecture. In this layer, every neuron is connected to all neurons in the preceding layer, adhering \nto the principles of a conventional multi-layer perceptron neural network. The FC layer receives \ninput from the last pooling or convolutional layer, which is a vector created by flattening the feature \nmaps. The FC layer serves as the classifier in the CNN, enabling the network to make predictions \n[10]. \nActivation Functions: Activation functions are fundamental components in convolutional \nneural networks (CNNs), indispensable for introducing non-linearity into the network. This non-\nlinearity is crucial for CNN’s ability to model complex patterns and relationships within the data, \nallowing it to perform tasks beyond simple linear classification or regression. Without non-linear \nactivation functions, a CNN would be limited to linear operations, significantly constraining its \ncapacity to accurately represent the intricate, non-linear behaviors typical of many real-world \nphenomena [32].  \nFig. 7 typically illustrates how these activation functions modulate input signals to produce \noutput, highlighting the non-linear transformations applied to the input data across different regions \nof the function curve. In this figure, 𝑥𝑖 represents the input feature, while 𝑤𝑖𝑗 denotes the weight \nassociated with the connection between the input feature 𝑥𝑖 and neuron 𝑗. The figure shows that \nneuron 𝑗 receives 𝑛 features simultaneously. The output from neuron 𝑗 is labeled by 𝑦𝑗, and its \ninternal state, or bias, is indicated by 𝑏𝑗. The activation function, depicted as 𝑓(. ), could be any \none of several types such as the Rectified Linear Unit (ReLU), hyperbolic tangent (Tanh), Sigmoid \nfunction, or others [33, 34].  \nThese various activation functions are shown in Fig. 8, with emphasis on their distinct \ncharacteristics and profiles. These activation functions are essential for convolutional neural \nnetworks (CNNs) to be more effective in a variety of applications by allowing them to recognize \nintricate patterns and provide accurate predictions. Sigmoid and Tanh functions are frequently \nreferred to as saturating nonlinearities due to the way they act when inputs are very large or small. \nAs per the reference, the Sigmoid function approaches values of 0 or 1, whereas the Tanh function \nleans towards -1 or 1[17]. Different alternative nonlinearities have been suggested for reducing \nproblems associated with these saturating effects, including Rectified Linear Unit (ReLU) [35], \nLeaky ReLU [36], Parametric Rectified Linear Units (PReLU) [37], Randomized Leaky ReLU \n(RReLU) [38], S-shaped ReLU (SReLU) [39], and Exponential Linear Units (ELUs) [40], Gaussian \nError Linear Units (GELUs) [41].  \n \n \n \nX1 \nX2 \nXN \n. \n. \n. \n𝒛= σ 𝒘𝒊+ 𝒙𝒊\n𝒊\n+ 𝒃   f (z) \nw1 \nw2 \nWN \nOutput \nF. M. Shiri et al.    \n7 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 8. Diagram of different activation functions. \n \nReLU (Rectified Linear Unit) is one of the most often used activation functions in modern \nCNNs because of how well it solves the vanishing gradient issue during training. The definition of \nReLU in mathematics is as Eq. (2), where the input to the neuron is represented by 𝑥 [34]. \n𝑓(𝑥) = max(0, 𝑥) = {𝑥𝑖,   𝑖𝑓 𝑥𝑖≥0 \n0,   𝑖𝑓 𝑥𝑖< 0                                                                                                       (2)  \nThis feature helps CNN learn complicated features more efficiently by effectively \"turning \noff\" any negative input values while maintaining positive values. It also keeps neurons from being \nsaturated during training. \nAs an alternative, the definition of the Sigmoid function is represented by Eq. (3), where 𝑥 \nstands for the input of the neuron. \n𝑓(𝑥) =\n1\n𝑒−𝑥                                                                                                                                                                  (3)   \nAlthough the sigmoid distinctive S-shape and capacity to condense real numbers into a range \nbetween 0 and 1 make it useful for binary classification, its propensity to saturate can hinder \ntraining by causing the vanishing gradient problem in deep neural networks. \nConvolutional Neural Networks (CNNs) are extensively used in various fields, including \nnatural language processing, image segmentation, image analysis, video analysis, and more. \nSeveral CNN variations have been developed, such as AlexNet [42], VGG (Visual Geometry Group) \n[43], Inception [44, 45], ResNet (Residual Networks) [46, 47], WideResNet [48], FractalNet [49], \nSqueezeNet [50], InceptionResNet [51], Xception (Extreme Inception) [52], MobileNet [53, 54], \nDenseNet (Dense Convolutional Network) [55], SENet (Squeeze-and-Excitation Network) [56], \nEfficientnet [57, 58] among others. These variants are applied in different application areas based \non their learning capabilities and performance. \n \nSigmoid \n \n   Hyperbolic Tangent \n \nReLU \n \nLeaky ReLU \n \nELU \nGELU \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n8 \n \n3.3 Recurrent Neural Networks (RNN) \n \nRecurrent Neural Networks (RNNs) are a class of deep learning models that possess internal \nmemory, enabling them to capture sequential dependencies. Unlike traditional neural networks that \ntreat inputs as independent entities, RNNs consider the temporal order of inputs, making them \nsuitable for tasks involving sequential information [59]. By employing a loop, RNNs apply the \nsame operation to each element in a series, with the current computation depending on both the \ncurrent input and the previous computations [60]. \nThe ability of RNNs to utilize contextual information is particularly valuable in tasks such as \nnatural language processing, video classification, and speech recognition. For example, in language \nmodeling, understanding the preceding words in a sentence is crucial for predicting the next word. \nRNNs excel at capturing such dependencies due to their recurrent nature[61-63]. \nHowever, a limitation of simple RNN is their short-term memory, which restricts their ability \nto retain information over long sequences [64]. To overcome this, more advanced RNN variants \nhave been developed, including Long Short-Term Memory (LSTM) [65], bidirectional LSTM [66], \nGated Recurrent Unit (GRU) [67], bidirectional GRU [68], Bayesian RNN [69], and others. \n \n \nFigure 9. Simple RNN internal operation [70]. \n \nFig. 9 depicts a simple recurrent neural network, where the internal memory (ℎ𝑡) is computed \nusing Eq. (4) [70]: \nℎ𝑡=  𝑔(𝑊𝑥𝑡+  𝑈ℎ𝑡+  𝑏)                                                                                                                                            (4) \nIn this equation, 𝑔() represents the activation function (typically Tanh), 𝑈 and 𝑊 are \nadjustable weight matrices for the hidden state (ℎ), 𝑏 is the bias, and 𝑥 denotes the input vector. \nRNNs have proven to be powerful models for processing sequential data, leveraging their \nability to capture dependencies over time. Various types of RNN models, such as LSTM, \nbidirectional LSTM, GRU, and bidirectional GRU, have been developed to address specific \nchallenges in different applications. \n3.3.1 Long Short-Term Memory (LSTM) \nLong Short-Term Memory (LSTM) is an advanced variant of Recurrent Neural Networks \n(RNN) that addresses the issue of capturing long-term dependencies. LSTM was initially \nintroduced by [65] in 1997 and further improved by [71] in 2013, gaining significant popularity in \nthe deep learning community. Compared to standard RNN, LSTM models have proven to be more \neffective at retaining and utilizing information over longer sequences.  \nIn an LSTM network, the current input at a specific time step and the output from the previous \ntime step are fed into the LSTM unit, which then generates an output that is passed to the next time \nstep. The final hidden layer of the last time step, sometimes along with all hidden layers, is \ncommonly employed for classification purposes [72]. The overall architecture of an LSTM network \nis depicted in Fig. 10 (a). LSTM consists of three gates: input gate, forget gate, and output gate. \nEach gate performs a specific function in controlling the flow of information. The input gate decides \nhow to update the internal state based on the current input and the previous internal state. The forget \ngate determines how much of the previous internal state should be forgotten. Finally, the output \ngate regulates the influence of the internal state on the system [60, 73]. \nF. M. Shiri et al.    \n9 \n \n \n \n \n \n \n \n \n \n \nFigure 10. (a) The high-level architecture of LSTM. (b) The inner structure of LSTM unit [60]. \n \nFig. 10 (b) illustrates the update mechanism within the inner structure of an LSTM. The update \nfor the LSTM unit is expressed by Eq. (5): \n{\n  \n \n  \n ℎ(𝑡) = 𝑔𝑜\n(𝑡)𝑓ℎ(𝑠(𝑡))                                                                                                                                           \n𝑠(𝑡−1) = 𝑔𝑓\n(𝑡)𝑠(𝑡−1) + 𝑔𝑖\n(𝑡)𝑓𝑠(𝑤ℎ(𝑡−1)) + 𝑢𝑋(𝑡) + 𝑏                                                                                \n𝑔𝑖\n(𝑡) =  𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑤𝑖ℎ(𝑡−1) + 𝑢𝑖𝑋(𝑡) + 𝑏𝑖)                                                                                          (5)\n𝑔𝑓\n(𝑡) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑤𝑓ℎ(𝑡−1) + 𝑢𝑓𝑋(𝑡) + 𝑏𝑓)                                                                                                \n𝑔𝑜\n(𝑡) =  𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (𝑤𝑜ℎ(𝑡−1) + 𝑢𝑜𝑋(𝑡) + 𝑏𝑜)                                                                                               \n \nwhere 𝑓ℎ and 𝑓𝑠 represent the activation functions of the system state and internal state, \ntypically utilizing the hyperbolic tangent function. The gating operation, denoted as g, is a \nfeedforward neural network with a sigmoid activation function, ensuring output values within the \nrange of [0, 1], which are interpreted as a set of weights. The subscripts 𝑖, 𝑜, and 𝑓 correspond to \nthe input gate, output gate, and forget gate, respectively. \nWhile standard LSTM has demonstrated promising performance in various tasks, it may \nstruggle to comprehend input structures that are more complex than a sequential format. To address \nthis limitation, a tree-structured LSTM network, known as S-LSTM, was proposed by [74]. S-\nLSTM consists of memory blocks comprising an input gate, two forget gates, a cell gate, and an \noutput gate. While S-LSTM exhibits superior performance in challenging sequential modeling \nproblems, it comes with higher computational complexity compared to standard LSTM [75]. \n3.3.2 Bidirectional LSTM \nBidirectional Long Short-Term Memory (Bi-LSTM) is an extension of the LSTM architecture \nthat addresses the limitation of standard LSTM models by considering both past and future context \nin sequence modeling tasks. While traditional LSTM models process input data only in the forward \ndirection, Bi-LSTM overcomes this limitation by training the model in two directions: forward and \nbackward [76]. \nA Bi-LSTM consists of two parallel LSTM layers: one processes the input sequence in the \nforward direction, while the other processes it in the backward direction. The forward LSTM layer \nreads the input data from left to right, as indicated by the green arrow in Fig. 11. Simultaneously, \nthe backward LSTM layer reads the input data from right to left, as represented by the red arrow \n[77]. This bidirectional processing enables the model to capture information from both past and \nfuture contexts, allowing for a more comprehensive understanding of temporal dependencies within \nthe sequence. \n \n(b) \n \n(a) \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n10 \n \n \nFigure 11. The architecture of a Bidirectional LSTM model [76]. \n \nDuring the training phase, the forward and backward LSTM layers independently extract \nfeatures and update their internal states based on the input sequence. The output of each LSTM \nlayer at each time step is a prediction score. These prediction scores are then combined using a \nweighted sum to generate the final output result [77]. By incorporating information from both \ndirections, Bi-LSTM models can capture a broader context and improve the model's ability to \nmodel temporal dependencies in sequential data. \nBi-LSTM has been widely applied in various sequence modeling tasks such as natural \nlanguage processing, speech recognition, and sentiment analysis. It has shown promising results in \ncapturing complex patterns and dependencies in sequential data, making it a popular choice for \ntasks that require an understanding of both past and future context. \n3.3.3 Gated Recurrent Unit (GRU) \nThe Gated Recurrent Unit (GRU) is another variant of the RNN architecture that addresses the \nshort-term memory issue and offers a simpler structure compared to LSTM [59]. GRU combines \nthe input gate and forget gate of LSTM into a single update gate, resulting in a more streamlined \ndesign. Unlike LSTM, GRU does not include a separate cell state. A GRU unit consists of three \nmain components: an update gate, a reset gate, and the current memory content. These gates enable \nthe GRU to selectively update and utilize information from previous time steps, allowing it to \ncapture long-term dependencies in sequences [78]. Fig. 12 illustrates the structure of a GRU unit \n[79]. \nThe update gate (Eq. (6)) determines how much of the past information should be retained and \ncombined with the current input at a specific time step. It is computed based on the concatenation \nof the previous hidden state ℎ𝑡−1 and the current input 𝑥𝑡, followed by a linear transformation \nand a sigmoid activation function. \n𝑧𝑡= 𝜎(𝑊𝑧[ℎ𝑡−1, 𝑥𝑡] + 𝑏𝑧)                                                                                                                         (6)  \nThe reset gate (Eq. (7)) decides how much of the past information should be forgotten. It is \ncomputed in a similar manner to the update gate using the concatenation of the previous hidden \nstate and the current input. \n𝑟𝑡= 𝜎(𝑊𝑟[ℎ𝑡−1, 𝑥𝑡] + 𝑏𝑟)                                                                                                                          (7)  \nThe current memory content (Eq. (8)) is calculated based on the reset gate and the \nconcatenation of the transformed previous hidden state and the current input. The result is passed \nthrough a hyperbolic tangent activation function to produce the candidate activation. \nℎ̃𝑡= 𝑡𝑎𝑛ℎ(𝑊ℎ[𝑟𝑡ℎ𝑡−1, 𝑥𝑡])                                                                                                                        (8)  \n \nF. M. Shiri et al.    \n11 \n \n \nFigure 12. The structure of a GRU unit [79]. \n \nFinally, the final memory state ℎ𝑡 is determined by a combination of the previous hidden \nstate and the candidate activation (Eq. (9)). The update gate determines the balance between the \nprevious hidden state and the candidate activation. Additionally, an output gate 𝑜𝑡 can be \nintroduced to control the information flow from the current memory content to the output (Eq. (10)). \nThe output gate is computed using the current memory state ℎ𝑡 and is typically followed by an \nactivation function, such as the sigmoid function. \nℎ𝑡= (1 −𝑧𝑡)ℎ𝑡−1 + 𝑧𝑡ℎ̃𝑡                                                                                                                            (9) \n𝑜𝑡= 𝜎𝑜(𝑊𝑜ℎ𝑡+ 𝑏𝑜)                                                                                                                                   (10) \nwhere the weight matrix of the output layer is 𝑊𝑜 and the bias vector of the output layer is \n𝑏𝑜. \nGRU offers a simpler alternative to LSTM with fewer tensor operations, allowing for faster \ntraining. However, the choice between GRU and LSTM depends on the specific use case and \nproblem at hand. Both architectures have their advantages and disadvantages, and their \nperformance may vary depending on the nature of the task [59]. \n3.3.4 Bidirectional GRU \nThe Bidirectional Gated Recurrent Unit (Bi-GRU) [80] improves upon the conventional GRU \narchitecture through the integration of contexts from the past and future in sequential modeling \ntasks. In contrast to the conventional GRU, which exclusively processes input sequences forward, \nthe Bi-GRU manages sequences in both forward and backward directions. In order to do this, two \nparallel GRU layers are used, one of which processes the input data forward and the other in reverse. \nFig. 13 shows the Bi-GRU's structural layout. \n \n \nFigure 13. The structure of a Bi-GRU model [81]. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n12 \n \n3.4 Temporal Convolutional Networks (TCN) \nTemporal Convolutional Networks (TCN) represent a significant advancement in neural \nnetwork architectures, specifically tailored for handling sequential data, particularly time series. \nOriginating as an extension of the one-dimensional Convolutional Neural Network (CNN), TCN \nwas first introduced by [82] in 2017 for the task of action segmentation in video data, and its \napplication was further generalized to other types of sequential data by [83] in 2018. TCN retains \nthe powerful feature extraction capabilities inherent to CNN while being highly efficient in \nprocessing and analyzing time series data. \nThe purpose of training TCN is to forecast the next 𝑙 values of the input time series. Assume \nthat we have a sequence of inputs 𝑥0, 𝑥1, … . , 𝑥𝑙. We would like to predict, at each time step, some \ncorresponding output 𝑦0, 𝑦1, … . , 𝑦𝑙, whose values are equal to the input shifted forward 𝑙 time \nsteps. The primary limitation is that it can only use the inputs that have already been observed: \n𝑥0, 𝑥1, … . , 𝑥𝑡, when forecasting the output 𝑦𝑡 for a given time step 𝑡 [84]. TCN is characterized \nby two fundamental properties: (1) The convolutions within the network are causal, ensuring that \nthe output at any given time step depends solely on the current and past inputs, without any \ninfluence from future inputs. (2) Similar to Recurrent Neural Networks (RNNs), TCN can process \nsequences of arbitrary length and produce output sequences of identical length. The three primary \ncomponents of a typical TCN are residual connections, dilated convolution, and causal convolution \n[83, 85, 86]. Fig. 14 illustrates the schematic architecture of a TCN model.  \n \n \nFigure 14. Schematic diagram of the TCN model architecture [87]. \nCausal Convolution:  \nTCN architecture is built upon two foundational principles. To adhere to the first principle, \nthe initial layer of a TCN is a one-dimensional fully convolutional network, wherein each hidden \nlayer maintains the same length as the input layer, achieved through zero-padding. This padding \nensures that each successive layer remains the same length as the preceding one. To satisfy the \nsecond principle, TCN employs causal convolutions. A causal convolution is a specialized one-\ndimensional convolutional network where only elements from time 𝑡 and earlier are convolved to \nproduce the output at time 𝑡. Fig. 15 demonstrates the structure of a causal convolutional network. \nDilated Convolution:  \nTCN aims to effectively capture long-range dependencies in sequential data. A simple causal \nconvolution can only consider a history that scales linearly with the depth of the network. This \nlimitation would necessitate the use of large filters or an exceptionally deep network structure, \nwhich could hinder performance, particularly for tasks requiring a longer history.  \nF. M. Shiri et al.    \n13 \n \n \nFigure 15. The structure of the causal convolutional network [85]. \n \nThe depth of the network could lead to issues such as vanishing gradients, ultimately degrading \nnetwork performance or causing it to plateau. To address these challenges, TCN employs dilated \nconvolutions [88], which exponentially expand the receptive field, allowing the network to process \nlarge time series efficiently without a proportional increase in computational complexity. The \narchitecture of a dilated convolutional network is depicted in Fig. 16. \nBy inserting gaps between the weights of the convolutional kernel, dilated convolutions \neffectively increase the network's receptive field while maintaining computational efficiency. The \nmathematical formulation of a dilated convolution is given by Eq. (11). \n𝐹(𝑠) = (𝑥∗𝑑𝑓)(𝑠) = ∑𝑓(𝑖) ∙𝑥𝑠−𝑑∙𝑖                                                                                                  (11)\n𝑘−1\n𝑖=0\n \nwhere 𝑑 is the dilation rate, 𝑘 is the size of the filter, and 𝑠−𝑑∙𝑖 accounts for the \ndirection of the past. Dilation is the same as adding a fixed step in between each pair of neighboring \nfilter taps. When 𝑑 =  1, dilated convolution becomes a regular convolution. As 𝑑 increases, the \noutput at the higher layers reflects a broader range of inputs, improving performance on long-range \ndependencies in time series. \nResidual Connections:  \nTo construct a more expressive TCN model, it is essential to use small filter sizes and stack \nmultiple layers. However, stacking dilated and causal convolutional layers increases the depth of \nthe network, potentially leading to problems such as gradient decay or vanishing gradients during \ntraining. To mitigate these issues, TCN incorporates residual connections into the output layer. \nResidual connections facilitate the flow of data across layers by adding a shortcut path, allowing \nthe network to learn residual functions, which are modifications to the identity mapping, rather than \nlearning a full transformation. This approach has been shown to be highly effective in very deep \nnetworks. \n \nFigure 16. Dilated convolutional structure [85]. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n14 \n \nA residual block [46] has a branch that lead to a set of transformations F, whose outputs are \nappended to block's input x, as shown in Eq. (12). \n𝑜= 𝐴𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 (𝑥+ 𝐹(𝑥))                                                                                                                     (12) \nThis method enables the network to focus on learning residual functions rather than the entire \nmapping. The TCN residual block typically consists of two layers of dilated causal convolutions \nfollowed by a non-linear activation function, such as Rectified Linear Unit (ReLU). The \nconvolutional filters within the TCN are normalized using weight normalization [89], and dropout \n[90] is applied to each dilated convolution layer for regularization, where an entire channel is zeroed \nout at each training step. In contrast to a conventional ResNet, where the input is directly added to \nthe output of the residual function, TCN adjusts for differing input-output widths by performing an \nadditional 1 × 1 convolution to ensure that the element-wise addition ⊕ operates on tensors of \nmatching dimensions. \n3.6 Kolmogorov-Arnold Network (KAN) \nKolmogorov-Arnold Networks (KANs) represent a promising alternative to traditional Multi-\nLayer Perceptrons (MLPs) by leveraging the Kolmogorov-Arnold theorem, a sophisticated \nmathematical framework that enhances the capacity of neural networks to process complex data \nstructures. KANs were first introduced in 2024 by [91], with the goal of incorporating advanced \nmathematical theories into deep learning architectures to improve their performance on intricate \ntasks. While MLPs are inspired by the universal approximation theorem, KANs are motivated by \nthe Kolmogorov-Arnold representation theorem [92], which states that any multivariate continuous \nfunction 𝑓 over a bounded domain can be expressed as a finite composition of simpler one-\ndimensional continuous functions: \n𝑓(𝑥1, … . , 𝑥𝑛) = ∑Φ𝑞\n2𝑛+1\n𝑞=1\n(∑𝜙𝑞,𝑝(𝑥𝑝)\n𝑛\n𝑝=1\n)                                                                                            (13) \nwhere 𝜙𝑞,𝑝 is a mapping [0,1] → ℝ and Φ𝑞 is a mapping ℝ → ℝ.  \nKAN maintains a fully connected structure like MLP, but with a key distinction: while MLP \nassign fixed activation functions to nodes (neurons), KAN assigns learnable activation functions to \nedges (weights). Consequently, KAN does not employ traditional linear weight matrices; instead, \neach weight parameter is replaced by a learnable one-dimensional function parameterized as a \nspline. Unlike MLP, which apply non-linear activation functions at each node, KAN nodes only \nsum the incoming data, relying on the rich, learnable spline functions to introduce non-linearity. \nAlthough this approach might initially seem computationally expensive, KAN often result in \nsignificantly smaller computation graphs compared to MLP. Fig. 17 illustrates the structure of a \nKAN.  \n \n \nFigure 17. The structure of Kolmogorov-Arnold Network (KAN) [91]. \n \nF. M. Shiri et al.    \n15 \n \nThe Kolmogorov-Arnold Network (KAN) can be expressed specifically as follows:  \n𝐾𝐴𝑁(𝑥) = (Φ𝐿−1 ◦Φ𝐿−2 ◦ · · · ◦ Φ1 ◦ Φ1)(𝑥)                                (14)  \nThe transformation of each layer, Φ𝑙 , operates on the input 𝑥𝑙 to generate 𝑥𝑙+1, the input \nfor the following layer, as follows:   \n𝑥𝑙+1 = Φ𝑙(𝑥𝑙) =   \n(\n \n \n𝜙𝑙,1,1(∙)\n𝜙𝑙,1,2(∙)\n𝜙𝑙,2,1(∙)\n𝜙𝑙,2,2(∙)\n…\n𝜙𝑙,1,𝑛𝑙(∙)\n…\n𝜙𝑙,2,𝑛𝑙(∙)\n⋮\n⋮\n𝜙𝑙,𝑛𝑙+1,1(∙)\n𝜙𝑙,𝑛𝑙+1,2(∙)\n⋱\n⋮\n…\n𝜙𝑙,𝑛𝑙+1,𝑛𝑙(∙))\n \n  𝑥𝑙                                            (15) \nWhere each activation function ∅𝑙,𝑗,𝑖 is a spline, offering a rich, flexible response surface to \ninputs from the model: \n𝑠𝑝𝑙𝑖𝑛𝑒(𝑥) = ∑𝑐𝑖𝐵𝑖(𝑥),     𝑐𝑖 𝑎𝑟𝑒 𝑡𝑟𝑎𝑖𝑛𝑎𝑏𝑙𝑒 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡𝑠                                                           (16)\n𝑖\n \nSeveral variants of KANs have emerged to tackle specific challenges in various applications: \n➢ Convolutional KAN (CKAN) [93]: CKAN is a pioneering alternative to standard CNN, which \nhave significantly advanced the field of computer vision. Convolutional KAN integrate the \nnon-linear activation functions of KAN into the convolutional layers, leading to a substantial \nreduction in the number of parameters and offering a novel approach to optimizing neural \nnetwork architectures. \n➢ Temporal KAN (TKAN) [94]: Temporal Kolmogorov-Arnold Networks combines the \nprinciples of KAN and Long Short-Term Memory (LSTM) networks to create an advanced \narchitecture for time series analysis. Comprising layers of Recurrent Kolmogorov-Arnold \nNetworks (RKANs) with embedded memory management, TKAN excels in multi-step time \nseries forecasting. The TKAN architecture offers tremendous promise for improvement in \ndomains needing one-step-ahead forecasting by solving the shortcomings of existing models \nin handling complicated sequential patterns [95, 96]. \n➢ Multivariate Time Series KAN (MT-KAN) [97]: MT-KAN is specifically designed to handle \nmultivariate time series data. The primary objective of MT-KAN is to enhance forecasting \naccuracy by modeling the intricate interactions between multiple variables. MT-KAN utilizes \nspline-parametrized univariate functions to capture temporal relationships while incorporating \nmethods to model cross-variable interactions. \n➢ Fractional KAN (fKAN) [98]: fKAN is an enhancement of the KAN architecture that \nintegrates the unique properties of fractional-orthogonal Jacobi functions into the network's \nbasis functions. This method guarantees effective learning and improved accuracy by utilizing \nthe special mathematical characteristics of fractional Jacobi functions, such as straightforward \nderivative equations, non-polynomial behavior, and activity for positive and negative input \nvalues. \n➢ Wavelet KAN (Wav-KAN) [99]: The purpose of this innovative neural network design is to \nimprove interpretability and performance by incorporating wavelet functions into the \nKolmogorov-Arnold Networks (KAN) framework. Wav-KAN is an excellent way to capture \ncomplicated data patterns by utilizing wavelets' multiresolution analysis capabilities. It offers \na reliable solution to the drawbacks of both recently suggested KANs and classic multilayer \nperceptrons (MLPs). \n➢ Graph KAN [100]: This innovative model applies KAN principles to graph-structured data, \nreplacing the MLP and activation functions typically used in Graph Neural Networks (GNNs) \nwith KAN. This substitution enables more effective feature extraction from graph-like data \nstructures. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n16 \n \n4 Generative (Unsupervised) Deep Learning Models \nSupervised machine learning is widely used in artificial intelligence (AI), while unsupervised \nlearning remains an active area of research with numerous unresolved questions. However, recent \nadvancements in deep learning and generative modeling have injected new possibilities into \nunsupervised learning. A rapidly evolving domain within computer vision research is generative \nmodels (GMs). These models leverage training data originating from an unknown data-generating \ndistribution to produce novel samples that adhere to the same distribution. The ultimate goal of \ngenerative models is to generate data samples that closely resemble real data distribution [101]. \nVarious generative models have been developed and applied in different contexts, such as \nAuto-Encoder [102], Generative Adversarial Network (GAN) [103], Restricted Boltzmann \nMachine (RBM) [104], and Deep Belief Network (DBN) [105]. \n4.1 Autoencoder \nThe concept of an autoencoder originated as a neural network designed to reconstruct its input \ndata. Its fundamental objective is to learn a meaningful representation of the data in an unsupervised \nmanner, which can have various applications, including clustering [102]. \nAn autoencoder is a neural network that aims to replicate its input at its output. It consists of \nan internal hidden layer that defines a code representing the input data. The autoencoder network \nis comprised of two main components: an encoder function, denoted as 𝑧 =  𝑓(𝑥), and a decoder \nfunction that generates a reconstruction, denoted as 𝑟 =  𝑔(𝑧)  [106]. The function 𝑓(𝑥) \ntransforms a data point 𝑥 from the data space to the feature space, while the function 𝑔(𝑧) \ntransforms 𝑧 from the feature space back to the data space to reconstruct the original data point \n𝑥. In modern autoencoders, these functions 𝑧 =  𝑓(𝑥)  and 𝑟 =  𝑔(𝑧)  are considered as \nstochastic functions, represented as 𝑝𝑒𝑛𝑐𝑜𝑑𝑒𝑟 (𝑧|𝑥) and 𝑝𝑑𝑒𝑛𝑐𝑜𝑑𝑒𝑟 (𝑟|𝑧), respectively, where 𝑟 \ndenotes the reconstruction of 𝑥 [107]. Fig. 18 illustrates an autoencoder model.  \nAutoencoder models find utility in various unsupervised learning tasks, such as generative \nmodeling [108], dimensionality reduction [109], feature extraction [110], anomaly or outlier \ndetection [111], and denoising [112]. \n \n \n \n \n \n \n \nFigure 18. The structure of autoencoders.   \n \nIn general, autoencoder models can be categorized into two major groups: Regularized \nAutoencoders, which are valuable for learning representations for subsequent classification tasks, \nand Variational Autoencoders [113], which can function as generative models. Examples of \nregularized autoencoder models include Sparse Autoencoder (SAE) [114], Contractive \nAutoencoder (CAE) [115], and Denoising Autoencoder (DAE) [116]. \nVariational Autoencoder (VAE) is a generative model that employs probabilistic distributions, \nsuch as the mean and variance of a Gaussian distribution, for data generation [102]. VAE provide \na principled framework for learning deep latent-variable models and their associated inference \nmodels. The VAE consists of two coupled but independently parameterized models: the encoder \nor recognition model and the decoder or generative model. During \"expectation maximization\" \nOriginal data \n \nReconstructed \ndata \nCompressed  \nRepresentation \nF. M. Shiri et al.    \n17 \n \nlearning iterations, the generative model receives an approximate posterior estimation of its latent \nrandom variables from the recognition model, which it uses to update its parameters. Conversely, \nthe generative model acts as a scaffold for the recognition model, enabling it to learn meaningful \nrepresentations of the data, such as potential class labels. In terms of Bayes' rule, the recognition \nmodel is roughly the inverse of the generative model [117]. \n4.2 Generative Adversarial Network (GAN) \nA notable neural network architecture for generative modeling, capable of producing realistic \nand novel samples on demand, is the Generative Adversarial Network (GAN), initially proposed \nby Ian Goodfellow in 2014 [103]. A GAN consists of two key components: a generative model and \na discriminative model. The generative model aims to generate data that resemble real ones, while \nthe discriminative model aims to differentiate between real and synthetic data. Both models are \ntypically implemented using multilayer perceptrons [118]. Fig. 19 depicts the framework of a GAN, \nwhere a two-player adversarial game is played between a generator (G) and a discriminator (D). \nThe generator's updating gradients are determined by the discriminator through an adaptive \nobjective [119]. \n \n \nFigure 19. The framework of a GAN.   \n \nAs previously mentioned, GANs operate based on principles derived from neural networks, \nutilizing a training set as input to generate new data that resembles the training set. In the case of \nGANs trained on image data, they can generate new images exhibiting human-like characteristics. \nThe following outlines the step-by-step operation of a GAN [120]: \n1. \nThe generator, created by a discriminative network, generates content based on the real data \ndistribution. \n2. \nThe system undergoes training to increase the discriminator's ability to distinguish between \nsynthesized and real candidates, allowing the generator to better fool the discriminator. \n3. \nThe discriminator initially trains using a dataset as the training data. \n4. \nTraining sample datasets are repeatedly presented until the desired accuracy is achieved. \n5. \nThe generator is trained to process random input and generate candidates that deceive the \ndiscriminator. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n18 \n \n6. \nBackpropagation is employed to update both the discriminator and the generator, with the \nformer improving its ability to identify real images and the latter becoming more adept at \nproducing realistic synthetic images. \n7. \nConvolutional Neural Networks (CNNs) are commonly used as discriminators, while \ndeconvolutional neural networks are utilized as generative networks. \nGenerative Adversarial Networks (GANs) have introduced numerous applications across \nvarious domains, including image blending [121], 3D object generation [122], face aging [123], \nmedicine [124, 125], steganography [126], image manipulation [127], text transfer [128], language \nand speech synthesis [129], traffic control [130], and video generation [131]. \nFurthermore, several models have been developed based on the Generative Adversarial \nNetwork (GAN) framework to address specific tasks. These models include Laplacian GAN (Lap-\nGAN) [132], Coupled GAN (Co-GAN) [118], Markovian GAN [133], Unrolled GAN [134], \nWasserstein GAN (WGAN) [135], and Boundary Equilibrium GAN (BEGAN) [136], CycleGAN \n[137], DiscoGAN [138], Relativistic GAN [139], StyleGAN [140], Evolutionary GAN (E-GAN) \n[119], Bayesian Conditional GAN [141], Graph Embedding GAN (GE-GAN) [130].  \n4.3 Deep Belief Network (DBN)  \nThe Deep Belief Network (DBN) is a type of deep generative model utilized primarily in \nunsupervised learning to uncover patterns within large datasets. Consisting of multiple layers of \nhidden units, DBNs are adept at identifying intricate patterns and extracting features from data. \nUnlike discriminative models, DBNs exhibit a higher resistance to overfitting, making them well-\nsuited for feature extraction from unlabeled data [142].  \nThe stack of Restricted Boltzmann Machines (RBMs), which operate in an unsupervised \nlearning framework, is a fundamental part of DBN. Every RBM in a DBN is made up of a hidden \nlayer that contains latent representations and a visible layer that represents observable data features \n[143]. RBMs are trained layer by layer: first, each RBM is trained independently, and then all of \nthe RBMs are fine-tuned together as a whole within the DBN. \nDuring the forward pass, the activations represent the probability of an output given a weighted \ninput. In the backward pass, the activations estimate the probability of inputs given the weighted \noutputs. Through iterative training of RBMs within a DBN, these processes converge to form joint \nprobability distributions of activations and inputs, allowing the network to effectively capture the \nunderlying data structure [144, 145]. Fig. 20 illustrates the schematic structure of a Deep Belief \nNetwork (DBN). \n \nFigure 20. structure of a DBN model [143]. \nF. M. Shiri et al.    \n19 \n \n5 Transformer Architecture \nThe Transformer architecture was originally introduced by Vaswani et al. [146] in 2017 for \nmachine translation and has since become a foundational model in deep learning, especially for \nnatural language processing (NLP). The transformer functions as a self-attention encoder-decoder \nstructure. The encoder consists of a stack of identical layers, and each layer consists of two \nsublayers. A multi-head self-attention mechanism is the first layer, while the other layer is a \nposition-wise fully connected feed-forward network. Also, A normalizing layer [147] and residual \nconnections [46] connect the multi-headed self-attention module's inputs and output. After that, a \ndecoder uses the representation that the encoder produced to create an output sequence. A stack of \nidentical layers makes up the decoder as well. The decoder adds a third sub-layer to each encoder \nlayer in addition to the primary two, and this sub-layer handles multi-head attention over the \nencoder stack's output. Like the encoder, residual connections and a normalizing layer are used \nsurrounding each of the sub-layers. The encoder and decoder's overall Transformer design is \ndepicted in Fig. 21, left and right halves respectively [148, 149].  \nTraditional RNN-based Seq2Seq models could be replaced with attention layers. Using \nvarious projection matrices, the query, key, and value vectors in the self-attention layer are all \nproduced from the same sequence [150]. RNN training takes a very long period because it is \nsequential and iterative. Transformer training, on the other hand, is parallel and enables all features \nto be learned concurrently, significantly improving computational efficiency and cutting down on \nthe amount of time needed for model training [151]. \nMulti-Head Attention: In the Transformer model, a multi-headed self-attention mechanism \nis employed to enhance the model's ability to capture dependencies between elements in a sequence. \nThe core principle of the attention mechanism is that every token in the sequence can aggregate \ninformation from other tokens, allowing the model to understand contextual relationships more \neffectively. This is achieved by mapping a query, a set of key-value pairs, and an output (each \nrepresented as vectors) to form an attention function. The output is computed as a weighted sum of \nthe values, where the weights are determined by the compatibility function between the query and \nits corresponding key [146].  \n   \nFigure 21. The architecture of the Transformer model [146]. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n20 \n \nMulti-head attention is equivalent to the blended of 𝑛 distinct scaled dot-product attention \n(self-attention). It can effectively process the three vectors Q, K, and V, in parallel to obtain the \nfinal result by combining and calculating. The formula is visible in Eq. (17). \n{𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑 (𝑄, 𝐾, 𝑉) = 𝐶𝑜𝑛𝑐𝑎𝑡(ℎ𝑒𝑎𝑑1, … , ℎ𝑒𝑎𝑑2)𝑊𝑂\n𝑤ℎ𝑒𝑟𝑒 ℎ𝑒𝑎𝑑𝑖= 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄𝑊𝑖\n𝑄, 𝐾𝑊𝑖\n𝐾, 𝑉𝑊𝑖\n𝑉)\n                                                                 (17) \nWhere the projections are parameter matrices 𝑊𝑖\n𝑄 ∈ ℝ𝑑𝑚𝑜𝑑𝑒𝑙×𝑑𝑘, 𝑊𝑖\n𝐾 ∈ ℝ𝑑𝑚𝑜𝑑𝑒𝑙×𝑑𝑘, 𝑊𝑖\n𝑉 ∈\n ℝ𝑑𝑚𝑜𝑑𝑒𝑙×𝑑𝑉, 𝑎𝑛𝑑 𝑊𝑂 ∈ ℝℎ𝑑𝑣×𝑑𝑚𝑜𝑑𝑒𝑙. \nThe main component of the transformer, scaled dot-product attention (self-attention), uses the \nweight of each sensor event in the input vector, which is represented by \n𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄, 𝐾, 𝑉) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑄𝐾𝑇\n√𝑑𝑘\n) 𝑉                                                                                         (18) \nThe initial step in scaled dot-product attention is to convert the input data into an embedding \nvector and the three vectors of query vector (Q), key vector (K), and value vector (V) are then \nextracted from the embedding vectors. Next, a score is determined for every vector: score is equal \nto 𝑄·  𝐾. Score normalization (dividing by √𝑑𝑘) is used for gradient stability. Next, the score is \nprocessed using the softmax activation function. The weighted score 𝑣 for every input vector is \nobtained by taking the softmax dot product value 𝑣. The final result is produced after summing. \nScaled dot-product attention and multi-head attention are displayed in Fig. 22 [152].  \nPosition-wise Feed-Forward Networks: Each encoder and decoder layer have a fully connected \nfeed-forward network in addition to attention sub-layers. This feed-forward network is applied to \neach position independently and in the same way. This is made up of two linear transformations \nconnected by a ReLU activation.  \n𝐹𝐹𝑁(𝑥) =  𝑚𝑎𝑥(0, 𝑥𝑊1  + 𝑏1)𝑊2 + 𝑏2                                                                                             (19) \nPositional Encoding: Since the Transformer model does not rely on recurrence or convolution, it \nrequires a way to capture the relative or absolute positions of tokens within a sequence to effectively \nutilize the sequence's order. To address this, positional encoding is introduced at the input level of \nboth the encoder and decoder stacks. These positional encodings are added to the input embeddings, \nas they share the same dimensionality, 𝑑𝑚𝑜𝑑𝑒𝑙. This combination enables the model to incorporate \npositional information, allowing it to better understand the sequential nature of the data [146]. \n \n \n(a) \n \n  \n \n \n \n \n \n \n  (b) \n \nFigure 22. (a) Scaled Dot-Product Attention, (b) Multi-Head Attention. \nF. M. Shiri et al.    \n21 \n \nPositional encodings in transformer architecture were achieved by using sine and cosine \nfunctions of various frequencies: \n{\n𝑃𝐸(𝑝𝑜𝑠,2𝑖)  =  𝑠𝑖𝑛(𝑝𝑜𝑠/100002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙)               \n𝑃𝐸(𝑝𝑜𝑠,2𝑖+1)  =  𝑐𝑜𝑠(𝑝𝑜𝑠/100002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙)                                                                                       (20) \nwhere 𝑝𝑜𝑠 is the position and 𝑖 is the dimension. Every dimension of the positional encoding \nhas a sinusoidal relationship. The wavelengths range from 2𝜋 𝑡𝑜 10000 · 2𝜋 in a geometric \ndevelopment. This function was selected because it would make it simple for the model to learn \nhow to attend to relative positions, since for any fixed offset 𝑘, 𝑃𝐸𝑝𝑜𝑠+𝑘 can be expressed as a \nlinear function of 𝑃𝐸𝑝𝑜𝑠. \n5.1 Transformer Variants \nThe Transformer architecture has proven to be highly versatile, with numerous variants \ndeveloped to address specific challenges across different domains. Typically, Transformers are pre-\ntrained on large datasets using unsupervised methods to learn general representations, which are \nthen fine-tuned on specific tasks using supervised learning. This hybrid approach leverages the \nstrengths of both learning paradigms. Some notable Transformer variants include: \n➢ Bidirectional Encoder Representations from Transformers (BERT) [153]: A multi-layer \nbidirectional Transformer encoder for unsupervised pre-training in natural language \nunderstanding (NLU) tasks. \n➢ Generative pre-training Transformer (GPT) [154, 155]: A type of Transformer model \ndeveloped by OpenAI that excels in natural language processing (NLP) tasks through \nunsupervised pre-training followed by supervised fine-tuning. \n➢ Transformer-XL [156]: It is proposed for language modeling to permit learning reliance \nbeyond a set length without compromising temporal coherence. Transformer-XL \n(Transformer-Extra Long) comprises a unique relative positional encoding method and a \nsegment-level recurrence mechanism. This approach not only makes it possible to record \nlonger-term dependencies, but also fixes the issue of context fragmentation.  \n➢ XLNet [157]: It is a generalized autoregressive (AR) pretraining technique that combines the \nbenefits of autoencoding (AE) and autoregressive (AR) techniques with a permutation \nlanguage modeling aim. XLNet's neural architecture, which integrates Transformer-XL and the \ntwo-stream attention mechanism, is built to function effortlessly with the autoregressive (AR) \nobjective.       \n➢ Fast Transformer [158]: It introduces multi-query attention as an alternative to multi-head \nattention. This approach reduces memory bandwidth requirements, leading to increased \nprocessing speed. \n➢ Multimodal Transformer (MulT) [159]: It is designed for analyzing human multimodal \nlanguage. At the heart of MulT is the crossmodal attention mechanism, which provides a latent \ncrossmodal adaptation that fuses multimodal information by directly attending to low-level \nfeatures in other modalities. \n➢ Vision Transformer (ViT) [160]: An innovative approach based on Transformer structure for \nvisual tasks like image classification. \n➢ Pyramid Vision Transformer (PVT) [161]: An Transformer framework for complex \nprediction tasks like semantic segmentation and object recognition. \n➢ Swin Transformer [162]: A hierarchical transformer that uses shifted windows to construct \nits representation. A wide variety of vision tasks, including semantic segmentation, object \ndetection, and image classification, may be performed with Swin Transformer. \n➢ Tokens-to-Token Vision Transformer (T2T-ViT) [163]: A vision transformer that can be \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n22 \n \ntrained from scratch on ImageNet. T2T-ViT overcomes ViT's drawbacks by accurately \nmodeling the structural information of images and enhancing feature richness. \n➢ Transformer in Transformer (TNT) [164]: A vision transformer for visual recognition. Both \nlocal and global representations are extracted by the TNT architecture through the use of an \ninner transformer and an outer transformer. \n➢ PyramidTNT [165]: A improved TNT model which used pyramid architecture, and \nconvolutional stem in order to greatly enhance the original TNT model. \n➢ Switch Transformers [166]: It is suggested as a straightforward and computationally effective \nmethod of increasing a Transformer model's parameter count. \n➢ ConvNeXt [167]: A redesigned transformer architecture that makes use of the transformer \nattention mechanism and incorporates convolutional layers into the encoder and decoder \nmodules to extract spatially localized data. \n➢ Evolutionary Algorithm Transformer (EATFormer) [168]: An improved vision \ntransformer influenced by an evolutionary algorithm. \n6 Deep Reinforcement Learning \nReinforcement learning (RL) is a machine learning approach that deals with sequential \ndecision-making, aiming to map situations to actions in a way that maximizes the associated reward. \nUnlike supervised learning, where explicit instructions are given after each system action, in the \nRL framework, the learner, known as an agent, is not provided with explicit guidance on which \nactions to take at each timestep 𝑡. The RL agent must explore through trial and error to determine \nwhich actions yield the highest rewards [169]. Furthermore, unlike supervised learning, where the \ncorrect output is obtained and the model is updated based on the loss or error, RL uses gradients \nwithout a differentiable loss function to teach a model to explore randomly and learn to make \noptimal decisions [170]. Fig. 23 depicts the agent-environment interaction in reinforcement \nlearning (RL). The standard theoretical framework for RL is based on a Markov Decision Process \n(MDP), which extends the concept of a Markov process and is used to model decision-making \nbased on states, actions, and rewards [171]. \nDeep reinforcement learning combines the decision-making capabilities of reinforcement \nlearning with the perception function of deep learning. It is considered a form of \"real AI\" as it \naligns more closely with human thinking. Fig. 24 illustrates the basic structure of deep \nreinforcement learning, where deep learning processes sensory inputs from the environment and \nprovides the current state data. The reinforcement learning process then links the current state to \nthe appropriate action and evaluates values based on anticipated rewards [172, 173]. \n \nF. M. Shiri et al.    \n23 \n \nFigure 23.  Agent-Environment interaction in RL. \n \nFigure 24.  Basic structure of Deep Reinforcement Learning (DRL) [172]. \n \nOne of the most renowned deep reinforcement learning models is the Deep Q-learning \nNetwork (DQN) [174], which directly learns policies from high-dimensional inputs using \nConvolutional Neural Network (CNN). Other common models in deep reinforcement learning \ninclude Double DQN [175], Dueling DQN [176], and Monte Carlo Tree Search (MCTS) [177]. \nDeep reinforcement learning (DRL) models find applications in various domains, such as \nvideo game playing [178, 179], robotic manipulation [180, 181], image segmentation [182, 183], \nvideo analysis [184, 185], energy management [186, 187], and more. \n7 Deep Transfer Learning \nDeep neural networks have significantly improved performance across various machine \nlearning tasks and applications. However, achieving these remarkable performance gains often \nrequires large amounts of labeled data for supervised learning, as it relies on capturing the latent \npatterns within the data [188]. Unfortunately, in certain specialized domains, the availability of \nsufficient training data is a major challenge. Constructing a large-scale, high-quality annotated \ndataset is costly and time-consuming [189]. \nTo address the issue of limited training data, transfer learning (TL) has emerged as a crucial \ntool in machine learning. The concept of transfer learning finds its roots in educational psychology, \nwhere the theory of generalization suggests that transferring knowledge from one context to another \nis facilitated by generalizing experiences. To achieve successful transfer, there needs to be a \nconnection between the two learning tasks. For example, someone who has learned to play the \nviolin is likely to learn the piano more quickly due to the shared characteristics between musical \ninstruments [190]. Fig. 25 depicts the learning process of transfer learning. Deep transfer learning \n(DTL) makes use of the learning experience to reduce the time and effort needed to train large \nnetworks as well as the time and effort needed to create the weights for an entire network from \nscratch [191]. \nWith the growing popularity of deep neural networks in various fields, numerous deep transfer \nlearning techniques have been proposed. Deep transfer learning can be categorized into four main \ntypes based on the techniques employed [189]: instances-based deep transfer learning, mapping-\nbased (feature-based) deep transfer learning, network-based (model-based) deep transfer learning, \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n24 \n \nand adversarial-based deep transfer learning. \n \nFigure 25.  Learning process of transfer learning. \n \nInstances-based deep transfer learning involves selecting a subset of instances from the source \ndomain and assigning appropriate weight values to these selected instances to supplement the \ntraining set in the target domain. Algorithms such as TaskTrAdaBoost [192] and TrAdaBoost.R2 \n[193] are well-known approaches based on this strategy.  \nMapping-based deep transfer learning focuses on mapping instances from both the source and \ntarget domains into a new data space, where instances from the two domains exhibit similarity and \nare suitable for training a unified deep neural network. Successful methods based on this approach \ninclude Extend MMD (Maximum Mean Discrepancy) [194], and MK-MMD (Multiple Kernel \nvariant of MMD) [195]. \nNetwork-based (model-based) deep transfer learning involves reusing a segment of a pre-\ntrained network from the source domain, including its architecture and connection parameters, and \napplying it to a deep neural network in the target domain. These model-based approaches are highly \neffective for domain adaptation between source and target data by adjusting the network (model), \nmaking them the most widely adopted strategies in deep transfer learning (DTL). Remarkably, \nthese methods can even adapt target data that is significantly different from the source data [196].  \nNetwork-based (model-based) approaches in deep transfer learning typically involve pre-\ntraining, freezing, fine-tuning, and adding new layers. Pre-trained models consist of layers from a \ndeep learning network (DL model) that have been trained using source data. Two key methods for \ntraining a model with target data are freezing and fine-tuning. These methods involve using some \nor all layers of a pre-defined model. When layers are frozen, they retain fixed parameters/weights \nfrom the pre-trained model. In contrast, fine-tuning involves initializing parameters and weights \nwith pre-trained values instead of starting with random values, either for the entire network or \nspecific layers [196]. \nA recent advancement in model-based deep transfer learning is Progressive Neural Networks \n(PNNs). This strategy involves the freezing of a pre-trained model and integrating new layers \nspecifically for training on target data [197]. The concept behind progressive learning is grounded \nin the idea that acquiring a new skill necessitates leveraging existing knowledge. This mirrors the \nway humans learn new abilities. For instance, a child learns to run by employing all the skills \nacquired during crawling and walking. PNN constructs a new model for each task it encounters. \nF. M. Shiri et al.    \n25 \n \nEach freshly generated model is interconnected with all others, aiming to learn a new task by \napplying the knowledge accumulated from preceding models. \nAdversarial-based methods focus on gathering transferable features from both the source and \ntarget data by leveraging logical relationships or rules acquired in the source domain. Alternatively, \nthey may utilize techniques inspired by generative adversarial networks (GANs) [198].   \nThese deep transfer learning techniques have proven to be effective in overcoming the \nchallenge of limited training data, enabling knowledge transfer across domains, and facilitating \nimproved performance in various applications such as image classification [199, 200], speech \nrecognition [201, 202], video analysis [203, 204], signal processing [205, 206], and other.  \nIn transfer learning, several popular pre-trained deep learning models are frequently used, \nincluding Xception [52], MobileNet [53], DenseNet [55], EfficientNet [57], NasNet [207], and \namong others. These models are initially trained on large-scale datasets like ImageNet, and their \nlearned weights are then transferred to a target domain. The architectures of these networks reflect \na broader trend in deep learning design, transitioning from manually crafted by human experts to \nautomatically optimized patterns. This evolution focuses on striking a balance between model \naccuracy and computational complexity [208].  \n8 Hybrid Deep Learning Models \nHybrid deep learning architectures, which integrate elements from various deep learning \nmodels, demonstrate significant potential in enhancing performance. By combining different \nfundamental generative or discriminative models, the following three categories of hybrid deep \nlearning models can be particularly effective for addressing real-world problems: \n• \nCombination of various supervised models to extract more relevant and robust features, such \nas CNN+LSTM or CNN+GRU. By leveraging the strengths of different architectures, these \nhybrid models effectively capture both spatial and temporal dependencies within the data. \n• \nIntegrating various types of generative models, such as combining Autoencoders (AE) with \nGenerative Adversarial Networks (GANs), to harness their strengths and enhance performance \nacross a range of tasks. \n• \nIntegrating the capabilities of generative models with supervised models to leverage the \nstrengths of both approaches can significantly enhance performance on various tasks. This \nhybrid strategy improves feature learning, data augmentation, and model robustness. Examples \nof such combinations include DBN+MLP, GAN+CNN, AE+CNN, and so on. \n9 Application of Deep Learning \nIn recent years, deep learning has demonstrated remarkable effectiveness across a wide range \nof applications, tackling various challenges in fields including healthcare, computer vision, speech \nrecognition, natural language processing (NLP), e-learning, smart environments, and more. Fig. 26 \nhighlights several potential real-world application areas of deep learning. \nFive useful categories have been established for these applications: classification, detection, \nlocalization, segmentation, and regression [10]. A concept called classification divides a collection \nof facts into classes. Detection typically involves recognizing objects and their boundaries within \nimages, videos, or other data types. Localization refers to the process of identifying and determining \nthe position of specific objects or features within an image or other types of data. Segmentation \ninvolves dividing an image or dataset into distinct regions or segments, with each segment \nrepresenting a particular object or feature of interest. Regression is used to model and analyze the \nrelationships between a dependent variable and one or more independent variables. It predicts \ncontinuous outcomes based on input features.  \n \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n26 \n \n \n \nFigure 26. Numerous possible domains for deep learning applications in the real world. \n \nHowever, each real-world application area has its own specific goals and requires particular \ntasks and deep learning techniques. Table 1 provides a summary of various deep learning tasks and \nmethods applied across multiple real-world application domains. \n \nTable 1: A summary of the practical applications of deep learning models in real-world domains. \nApplication Setting \nTasks \nModels \nReference \nSmart Homes & \nSmart Cities \n \nHuman Activity Recognition  \nCNN+LSTM \n[209] \nSmart Energy Management \nReinforcement learning  \n[210] \nTraffic Management \nGRU based \n[211] \nWaste Management \nCNN based  \n[212] \nSmart Parking System \nStacked GRU+LSTM \n[213] \nEducation \n  \nStudent Engagement Detection  DenseNet self-attention \n[214] \nStudent Affective States \nRecognition \nConvNeXt + GRU \n[82] \nAutomatic Attendance System \nCNN+LSTM \n[215] \nAutomated Exam Control \nCNN based (VGG) \n[216] \nHealthcare \nMedical Image Analysis \nVision transformer  \n[217] \nEarly Disease Detection \nInceptionV3 \n[218] \nF. M. Shiri et al.    \n27 \n \nRemote Patient Monitoring \nCNN based \n[219] \nAnalyze Genomic Data \nTransfer learning based \n[220] \nNatural Language \nProcessing (NLP) \nQuestion Answering Systems \nBERT based \n[221] \nSentiment Analysis \nTransformer based \n[222] \nText Summarization \nAttentional LSTM \n[223] \nSpeech Recognition Speech Emotion Recognition \nLSTM+CNN \n[224] \nAutomatic Speech Translation \nDeep transfer learning \n[201] \nAgriculture \nPlant Disease Detection \nViT +CNN \n[225] \nPrecision Agriculture \nGRU+CNN \n[226] \nSmart Irrigation System \nAutoencoders, GAN \n[227] \nSoil Quality Prediction \nCNN \n[228] \nNatural Disaster \nManagement \nEarthquake Prediction \nCNN+RNN \n[229] \nFlood Forecasting \nAttention GRU \n[230] \nTsunami Prediction \nLSTM based \n[231] \nRemote Sensing \nLand Cover Classification \nExtended ViT \n[232] \nInvestigation Wildfire Area \nCNN based \n[233] \nDeforestation Detection \nTransformer based \n[234] \nCybersecurity \nIntrusion Detection  \nCNN+ Bi-LSTM \n[235] \nMalware Detection \nLSTM based \n[236] \nPhishing Detection \nLSTM+CNN \n[237] \nCredit Card Fraud Detection \nDeep Autoencoder \n[238] \nBiometric Authentication \nCNN+LSTM \n[239] \nRecommender \nSystems \nContext-Aware \nRecommendation \nRNN based  \n[240] \nSequential Recommendation \nLSTM based \n[241] \nMultimodal Recommendation \nCNN based \n[242] \nBusiness \nPurchase Behavior Prediction \nRNN based \n[243] \nLoan Default Prediction \nCNN based \n[244] \nStock Trend Prediction \nBi-LSTM \n[245] \nAutonomous \nVehicles \nObject Detection \nSwin transformer +CNN \n[246] \nPedestrian Detection \nDeep CNN \n[247] \nLocalization And Mapping \nCNN-GRU \n[248] \nLane Detection & Path Planning CNN based  \n[249] \nManufacturing \nDefect Detection  \nTransformer based \n[250] \nPredictive Maintenance \nLSTM, GRU, CNN \n[251] \nProcess Optimization \nReinforcement learning  \n[252] \nSupply Chain Optimization \nLSTM \n[253] \nRobotics \nRobotic Grasping \nReinforcement learning  \n[254] \nTracking And Motion Planning \nReinforcement learning  \n[255] \nHuman-Robot Interaction \nRNN based \n[256] \n \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n28 \n \n10 Deep Learning Challenges  \nWhile deep learning models have achieved remarkable success across various domains, they \nalso come with significant challenges. Below are some of the most critical challenges, followed by \npotential solutions to address them.  \n10.1 Insufficient Data  \nDeep learning models require large amounts of data to perform well. The performance of these \nmodels typically improves as the volume of data increases. However, in many cases, sufficient data \nmay not be available, making it difficult to train deep learning models effectively [10].  \nThree possible approaches may be used to appropriately handle the insufficient data problem. \nThe first method is Transfer Learning (TL), which is used to DL models by reusing pre-trained \nmodel pieces in new models. We thoroughly reviewed the transfer learning strategy in section 7.  \nData augmentation is the second method of gathering additional data. The goal of data \naugmentation is to improve the trained models' capacity for generalization. Generalization is \nnecessary for networks to overcome small datasets or datasets with unequal class distributions, and \nit is especially crucial for real-world data [257]. There are several strategies for augmenting data, \nand each one is contingent upon the characteristics of the datasets [258]. A few of these techniques \nare geometric transformations [259], Mixup augmentation [260], Random oversampling [261], \nFeature space augmentation [262], generative data augmentation [263], and many more. \n The third approach considers using simulated data to increase the training set's volume. If \nyou have a good understanding of the physical process, you can sometimes build simulators from \nit. Consequently, the outcome will include simulating as much data as necessary [10, 264].    \n \n10.2 Imbalanced Data \nIn real-world situations, particularly in those that deep learning models address, the issue of \nclass imbalance is common. If the majority of instances in the data set belong to one class and the \nremaining instances belong to the other class, then there is a class imbalance in a binary \nclassification scenario. In multi-class, multi-label, multi-instance learning as well as in regression \ndifficulties and other situations, class imbalances are present and are actually reinforced [265].  \nIt has been determined that there are three main approaches to addressing imbalanced data: \ndata-level techniques, algorithm-level techniques, and hybrid techniques. The focus of data-level \ntechniques is to add or remove samples from training sets in order to balance the data distributions. \nThese techniques balance the data distributions by adding new samples to the minority class \n(oversampling) or removing samples from the majority class (undersampling) [266, 267]. A variety \nof oversampling techniques, including Synthetic Minority Over-sampling Technique (SMOTE) \n[268], Borderline-SMOTE [269], Adaptive Synthetic (ADASYN) [270], SVM (Support Vector \nMachine)-SMOTE [271], Majority Weighted Minority Oversampling Technique (MWMOTE) \n[272], Sampling With the Majority (SWIM) [273], Reverse-SMOTE (R-SMOTE) [274], \nConstrained Oversampling (CO) [275], SMOTE Based on Furthest Neighbor Algorithm \n(SOMTEFUNA) [276], and many more can be used to solve imbalanced data problems. Also, there \nare several techniques for undersampling, including EasyEnsemble [277], BalanceCascade [277], \nInverse Random Undersampling [278], MLP-based Undersampling Technique (MLPUS) [279], \nand others. \nAlgorithm-level approaches modify existing learning algorithms to mitigate the bias towards \nthe majority class. These techniques require specialized knowledge of both the application domain \nand the learning algorithm to diagnose why a classifier fails under imbalanced class distributions \n[266]. Two of the most commonly used methods in this context are Cost-Sensitive Learning [280, \n281] and One-Class Learning [282]. \nF. M. Shiri et al.    \n29 \n \nThe third approach consists of hybrid methods, which combine algorithm-level techniques \nwith data-level methods in the appropriate way. Hybridization is required to address issues with \nalgorithm and data-level approaches and improve classification accuracy [283].     \n \n10.3 Overfitting \nOverfitting occurs when a deep learning model learns the systematic and noise components of \nthe training data to the point that it adversely affects the model's performance on new data. In fact, \noverfitting occurs as a result of noise, the small size of the training set, and the complexity of the \nclassifiers. Overfitted models tend to memorize all the data, including the inevitable noise in the \ntraining set, rather than understanding the underlying patterns in the data [24]. Overfitting is \naddressed with methods including dropout [90], weight decay [284], batch normalization [285, \n286], regularization [287], data augmentation, and others, although determining the ideal balance \nis still difficult.  \n \n10.4 Vanishing and Exploding Gradient  \nIn deep neural networks, the computation of gradients is propagated layer by layer, leading to \na phenomenon known as the vanishing or exploding gradient problem. As gradients are \nbackpropagated through the network, they can exponentially diminish or grow, respectively, \ncausing significant issues in training. When gradients vanish, the weights of the network are \nadjusted so minimally that the model's learning process becomes exceedingly slow, potentially \nstalling altogether. Conversely, exploding gradients can cause weights to be updated excessively, \nleading to instability and divergence during training. This problem is particularly pronounced with \nnon-linear activation functions such as sigmoid and tanh, which compress the output into a narrow \nrange, further exacerbating the issue by limiting the gradient's magnitude. Consequently, the model \nstruggles to learn effectively, especially in deep networks where gradients must pass through many \nlayers [8]. \nTo mitigate the vanishing and exploding gradient problem, several strategies have been \ndeveloped. One effective approach is to use the Rectified Linear Unit (ReLU) activation function, \nwhich does not saturate and therefore helps to maintain the gradient flow throughout the network \n[288]. Proper weight initialization techniques, such as Xavier initialization [289] can also reduce \nthe likelihood of gradient issues by ensuring that initial weights are set in a way that prevents \ngradients from becoming too small or too large [290]. Another solution is batch normalization, \nwhich normalizes the inputs of each layer to maintain a stable distribution of activations throughout \ntraining. By doing so, batch normalization helps to alleviate the vanishing gradient problem and \ncan accelerate convergence by reducing internal covariate shifts. Overall, addressing the vanishing \nand exploding gradient problem is crucial for training deep neural networks effectively, enabling \nthem to learn complex patterns without succumbing to instability or inefficiency [286]. \n \n10.5 Catastrophic Forgetting \nCatastrophic forgetting is a critical challenge in the pursuit of artificial general intelligence \nwithin neural networks. It occurs when a model, after being trained on a new task, loses its ability \nto perform previously learned tasks. This phenomenon is particularly problematic in scenarios \nwhere a model is expected to learn sequentially across multiple tasks without forgetting earlier ones, \nsuch as in lifelong learning or continual learning applications. The root cause of catastrophic \nforgetting lies like neural networks, which update their weights based on new training data. When \ntrained on a new task, the model adjusts its parameters to optimize performance on that task, often \nat the expense of previously acquired knowledge. As a result, the model may exhibit excellent \nperformance on the most recent task but perform poorly on earlier ones, effectively \"forgetting\" \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n30 \n \nthem [291]. \nSeveral strategies have been proposed to address catastrophic forgetting. One such approach \nis Elastic Weight Consolidation (EWC) [292], which penalizes changes to the weights that are \nimportant for previous tasks, thereby preserving learned knowledge while allowing the model to \nadapt to new tasks. Incremental Moment Matching (IMM) ) [293] is another technique that merges \nmodels trained on different tasks into a single model, balancing the performance across all tasks. \nThe iCaRL (incremental Classifier and Representation Learning) [294] method combines \nclassification with representation learning, enabling the model to learn new classes without \nforgetting previously learned ones. Additionally, the Hard Attention to the Task (HAT) [291] \napproach employs task-specific masks that prevent interference between tasks, reducing the \nlikelihood of forgetting. \n \n10.6 Underspecifcation \nUnderspecification is an emerging challenge in the deployment of machine learning (ML) \nmodels, particularly deep learning (DL) models, in real-world applications. It refers to the \nphenomenon where an ML pipeline can produce a multitude of models that all perform well on the \nvalidation set but exhibit unpredictable behavior in deployment. This issue arises because the \npipeline's design does not fully specify which model characteristics are critical for generalization \nin real-world scenarios. The underspecification problem is often linked to the high degrees of \nfreedom inherent in ML pipelines. Factors such as random seed initialization, hyperparameter \nselection, and the stochastic nature of training can lead to the creation of models with similar \nvalidation performance but divergent behaviors in production. These differences can manifest as \ninconsistent predictions when the model is exposed to new data or deployed in environments \ndifferent from the training conditions [295]. \nAddressing underspecification requires rigorous testing and validation beyond standard \nmetrics. Stress tests, as proposed by D’Amour et al. [295], are designed to evaluate a model's \nrobustness under various real-world conditions, identifying potential failure points that may not be \napparent during standard validation. These tests simulate different deployment scenarios, such as \nvarying input distributions or environmental changes, to assess how the model's predictions might \nvary. Moreover, some researches have been conducted to analyze and mitigate underspecification \nacross different ML tasks [296, 297].  \n11 Analysis of Deep Learning Models \nThis section details the methodology used in this study, which focuses on applying and \nevaluating various deep learning models for classification tasks across three distinct datasets. For \nour experimental analysis, we utilized three publicly available datasets: IMDB [298], ARAS [299], \nand Fruit-360 [300]. The objective is to conduct a comparative analysis of the performance of these \ndeep learning models. \nThe IMDB dataset, which stands for Internet Movie Database, provides a collection of movie \nreviews categorized as positive or negative sentiments. ARAS is a dataset comprising annotated \nsensor events for human activity recognition tasks. Fruit-360 is a dataset consisting of images of \nvarious fruit types for classification purposes. \nWe began by evaluating eight different models: CNN, RNN, LSTM, Bidirectional LSTM, \nGRU, Bidirectional GRU, TCN, and Transformer on the IMDB and ARAS datasets. Our analysis \naimed to compare the performance of these deep learning models across diverse datasets. The CNN \nmodel (Convolutional Neural Network) is particularly effective in capturing spatial dependencies, \nmaking it suitable for tasks involving structured data. RNN (Recurrent Neural Network) is well-\nsuited for sequential data analysis, while LSTM (Long Short-Term Memory) and GRU (Gated \nRecurrent Unit) models are designed to capture long-term dependencies in sequential data. The \nF. M. Shiri et al.    \n31 \n \nBidirectional LSTM and Bidirectional GRU models provide an additional advantage by processing \ninformation in both forward and backward directions. \nAdditionally, we evaluated eight different CNN-based models: VGG, Inception, ResNet, \nInceptionResNet, Xception, MobileNet, DenseNet, and NASNet for the classification of fruit \nimages using the Fruit-360 dataset. Given that image data is not sequential or time-dependent, \nrecurrent models were not suitable for this task. CNN-based models are particularly effective for \nimage analysis because of their ability to capture spatial dependencies. Moreover, the faster training \ntime of CNN models is due to their parallel processing capabilities, which allow for efficient \ncomputation on GPU (Graphics Processing Unit), thereby accelerating the training process. \nTo evaluate the performance of these models, we employed assessment metrics such as \naccuracy, precision, recall, and F1-measure. Accuracy measures the overall correctness of the \nmodel's predictions, while precision evaluates the proportion of correctly predicted positive \ninstances. Recall assesses the model's ability to correctly identify positive instances, and F1-\nmeasure provides a balanced measure of precision and recall [301]. \n𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦=\n𝑇𝑝+ 𝑇𝑛\n𝑇𝑝+ 𝑇𝑛+ 𝐹𝑝+ 𝐹𝑛                                                                                                          (21) \n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=\n𝑇𝑝\n𝑇𝑝+ 𝐹𝑝                                                                                                                               (22) \n𝑅𝑒𝑐𝑎𝑙𝑙=\n𝑇𝑝\n𝑇𝑝+ 𝐹𝑛                                                                                                                                     (23) \n𝐹1 −𝑆𝑐𝑜𝑟𝑒= 2 × 𝑅𝑒𝑐𝑎𝑙𝑙× 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛\n𝑅𝑒𝑐𝑎𝑙𝑙+ 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛                                                                                               (24) \nWhere 𝑇𝑝 = True Positive, 𝑇𝑛 = True Negative, 𝐹𝑝 = False Positive, and 𝐹𝑛 = False \nNegative.  \nBy conducting a comprehensive analysis using these metrics, we can gain insights into the \nstrengths and weaknesses of each deep learning model. This comparative evaluation enables us to \nidentify the most effective model for specific datasets and applications, ultimately advancing the \nfield of deep learning and its practical applications. \nAll experiments were conducted on a GeForce RTX 3050 GPU (Graphics Processing Unit) \nwith 4 Gigabyte of RAM (Random Access Memory). \n11.1 Methodology and Experiments on IMDB Dataset \nThe IMDB dataset is a widely used dataset for sentiment analysis tasks. It consists of movie \nreviews along with their corresponding binary sentiment polarity labels. The dataset contains a total \nof 50,000 reviews, evenly split into 25,000 training samples and 25,000 testing samples. There is \nan equal distribution of positive and negative labels, with 25,000 instances of each sentiment. To \nreduce the correlation between reviews for a given movie, only 30 reviews are included in the \ndataset [298]. Positive reviews often contain words like \"great,\" \"well,\" and \"love,\" while negative \nreviews frequently use words like \"bad\" and \"can't.\" However, certain words such as \"one,\" \n\"character,\" and \"well\" appear frequently in both positive and negative reviews, although their \nusage may differ in terms of frequency between the two sentiment classes [72].  \nIn our analysis, we employed eight different deep learning models including CNN, RNN, \nLSTM, Bidirectional LSTM, GRU, Bidirectional GRU, TCN, and Transformer for sentiment \nclassification using the IMDB dataset. Fig. 27 presents a structural overview of the deep learning \nmodel intended for analyzing the performance of eight different models on the IMDB dataset. \n \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n32 \n \n \nFigure 27.  The structural for analysis of different deep learning models on IMDB dataset \n \nIn this architecture, text data is first passed through an embedding layer, which transforms the \nhigh-dimensional, sparse input into dense, lower-dimensional vectors of real numbers. This allows \nthe model to capture semantic relationships within the data. In the second layer, one of eight models: \nCNN, RNN, LSTM, Bi-LSTM, GRU, Bi-GRU, TCN, or Transformer is employed for feature \nextraction and data training. This layer is crucial for capturing patterns and dependencies in the \ndata. Following this, a dropout layer is included to address the issue of overfitting by randomly \ndeactivating a portion of the neurons during training, which helps improve the model's \ngeneralization. Subsequently, the multi-dimensional vector turns into a one-dimensional vector \nusing a flatten layer, enabling it to work with fully connected layers. Finally, the output is passed \nthrough a fully connected (Dense) layer, which uses a Softmax function for classification, \nconverting the model's predictions into probabilities for each class. \nBuilding a neural network with high accuracy necessitates careful attention to hyperparameter \nselection, as these adjustments significantly influence the network's performance. For example, \nsetting the number of training iterations too high can lead to overfitting, where the model performs \nwell on the training data but poorly on unseen data. Another critical hyperparameter is the learning \nrate, which affects the rate of convergence during training. If the learning rate is too high, the \nnetwork may converge too quickly, potentially overshooting the global minimum of the loss \nfunction. Conversely, if the learning rate is too low, the convergence process may become \nexcessively slow, prolonging training. Therefore, finding the optimal balance of hyperparameters \nis essential for maximizing the network's performance and ensuring effective learning. \nIn the experiment phase, consistent parameters were applied across all models to ensure a \nstandardized comparison. The parameters were set as follows: epochs = 30, batch size = 64, dropout \n= 0.2, with the loss function set to \"Binary Crossentropy,\" and the optimizer function set to \nStochastic Gradient Descent (SGD) with a learning rate of 0.2. For the CNN model, 100 filters \nwere used with a kernel size of 3, along with the Rectified Linear Unit (ReLU) activation function. \nThe RNN, LSTM, Bi-LSTM, GRU, and Bi-GRU models each employed 64 units. The TCN model \nwas configured with 16 filters, a kernel size of 5, and dilation rates of [1, 2, 4, 8]. The Transformer \nF. M. Shiri et al.    \n33 \n \nmodel was set up with 2 attention heads, a hidden layer size of 64 in the feed-forward network, and \nthe ReLU activation function. These parameter settings and architectural choices were designed to \nallow for a standardized comparison of the deep learning models on the IMDB dataset. This \nstandardization facilitates an accurate analysis of each model's performance, enabling a comparison \nof their accuracy and loss values. \nTable 2 shows the result of different deep learning models on IMDB review dataset based on \nvarious metrics including Accuracy, Precision, Recall, F1-Score, and Time of training.  \n \nTable 2: Result of different deep learning models on the IMDB dataset \nmodel \nAccuracy % \nPrecision % \nRecall % \nF1-Score % \nTime (h:m:s) \nCNN \n85.90 \n85.89 \n85.88 \n85.89 \n0:02:57 \nRNN \n59.03 \n59.03 \n59.02 \n59.03 \n0:12:23 \nLSTM \n87.53 \n87.53 \n87.54 \n87.54 \n0:09:09 \nBi-LSTM \n87.45 \n87.46 \n87.47 \n87.46 \n0:10:43 \nGRU \n87.55 \n87.56 \n87.57 \n87.56 \n0:05:10 \nBI-GRU \n87.97 \n87.92 \n87.99 \n87.95 \n0:09:54 \nTCN \n84.42 \n84.40 \n84.42 \n84.41 \n0:07:38 \nTransformer \n88.03 \n88.04 \n88.01 \n88.03 \n0:03:44 \n \nTo compare the performance of these models, we utilized accuracy, validation-accuracy, loss, \nand validation-loss diagrams. These diagrams provide insights into how well the models are \nlearning from the data and help in evaluating their effectiveness for sentiment classification tasks. \n Fig. 28 shows the accuracy and validation-accuracy diagrams where the accuracy provides a \nvisual representation of how the different deep learning models perform in terms of accuracy during \nthe training process and validation-accuracy shows the trend of accuracy values on the testing set \nacross multiple epochs for each model.  \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 28.  Accuracy and validation-accuracy of deep learning models on IMDB dataset. \n \n(a) Accuracy Diagram \n \n    (b) Validation-Accuracy Diagram \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n34 \n \n \n \n \n \n \n \n \n \n \n \nFigure 29.  Loss and validation- loss diagrams of deep learning models on IMDB dataset. \n \nFig. 29 illustrates the loss and validation-loss diagram where the loss diagram is a visual \nrepresentation of loss values during the training process for six different models, and the validation-\nloss diagram depicts the variation in loss values on the testing set during the evaluation process for \nthe different models. The loss function measures the discrepancy between the predicted sentiment \nlabels and the actual labels.  \nFurthermore, the confusion matrices for the various deep learning models are displayed in Fig. \n30. These matrices provide a detailed breakdown of each model's performance, highlighting how \nwell the models classify different classes. By closely examining these confusion matrices, we can \ngain insights into the precision of the models and identify patterns of misclassification for each \nclass. This analysis helps in understanding the strengths and weaknesses of the models' predictions. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 30. Confusion matrix for different deep learning models on IMDB dataset. \n \n \n(a) Loss Diagram \n \n(b) Validation-Loss Diagram \n \nRNN \n \nLSTM \n \nBi-LSTM \n \nGRU \n \nBi-GRU \n \nTCN \n \nTransformer \nCNN \nF. M. Shiri et al.    \n35 \n \n \nFigure 31. ROC-AUC diagrams for different deep learning models. \n \nAdditionally, Fig. 31 displays the ROC-AUC (Receiver Operating Characteristic-Area Under \nCurve) diagrams for eight different deep learning models. These diagrams offer valuable insights \ninto the classification performance of the models, aiding in the assessment of their effectiveness. \nBy analyzing the ROC-AUC curves, we can make informed decisions regarding model selection \nand threshold adjustments, ensuring a more accurate and effective classification approach. \nBased on the results provided, it can be concluded that the Transformer and Bi-GRU models \nachieved the best performance on the IMDB review dataset for sentiment analysis. Both models \ndemonstrated high accuracy in classifying the sentiment of movie reviews. However, it is worth \nnoting that the training time of the Transformer model was significantly less than that of the Bi-\nGRU model. This suggests that the Transformer model was faster to train compared to the Bi-GRU \nmodel while still achieving excellent performance. Additionally, the GRU model also exhibited \ngood accuracy in sentiment classification and required less training time than the Bi-GRU model. \nOverall, the results suggest that the Transformer, and GRU models are effective deep learning \nmodels for sentiment analysis on the IMDB review dataset, with varying trade-offs between \nperformance and training time.  \n11.2 Methodology and Experiments on ARAS Dataset \nBased on the provided information, the ARAS dataset [299] is a valuable resource for \nrecognizing human activities in smart environments. It consists of data streams collected from two \nhouses over a period of 60 days, with 20 binary sensors installed to monitor resident activity. The \ndataset includes information on 27 different activities performed by two residents, and the sensor \nevents are recorded on a per-second basis.  \nEight distinct deep learning models were used in our investigation to recognize human \nactivities: CNN, RNN, LSTM, Bidirectional LSTM, GRU, Bidirectional GRU, TCN, and \nTransformer. A structural overview of the deep learning model designed to analyze the \nperformance of eight different models on the ARAS dataset is shown in Fig. 32. \nThe first phase involves preprocessing the sensor data to ensure it is in a suitable and \nstandardized format for deep learning models. The initial task in this phase is data cleaning, where \nany recorded instances where all sensor events are zero, and the resident is inside the house, are \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n36 \n \nremoved from the dataset. Next, a time-based static sliding window technique is applied for \nsegmenting sensor events. This method groups sequences of sensor events into intervals of equal \nduration. Optimizing the time interval is crucial for effective segmentation; after evaluating \nintervals ranging from 30 to 360 seconds, a 90-second interval was determined to be optimal for \nthe ARAS dataset. The segmentation task aids in decreasing training time and increasing accuracy \nfor the deep learning models.  \n \n \nFigure 32.  The structural for analysis of different deep learning models on the ARAS dataset \n \nAfter preprocessing, the data is passed through an input layer. In the second layer, one of eight \nmodels: CNN, RNN, LSTM, Bi-LSTM, GRU, Bi-GRU, TCN, or Transformer is employed for \nfeature extraction and training. This layer plays a vital role in capturing patterns and dependencies \nwithin the data. To mitigate overfitting, a dropout layer follows, which randomly deactivates a \nportion of the neurons during training, thereby improving the model's generalization. Subsequently, \na flatten layer is used to convert the multi-dimensional vector into a one-dimensional vector, \nmaking it compatible with fully connected layers. Finally, the output passes through a fully \nconnected (Dense) layer, which uses a Softmax function for classification, transforming the \nmodel’s predictions into probability distributions across the classes.   \nIn the experimental phase, we split the data from the first resident of house B, allocating 70% \nfor training and 30% for testing, using a random split. Additionally, 20% of the training data was \nset aside for validation. The models were trained with a fixed set of parameters: 30 epochs, a batch \nsize of 64, a dropout rate of 0.2, the \"Categorical Crossentropy\" loss function, and the Adam \noptimizer. For the CNN model, we used 100 filters with a kernel size of 3 and the rectified linear \nunit (ReLU) activation function. The RNN, LSTM, Bi-LSTM, GRU, and Bi-GRU models were \nconfigured with 64 units each. The TCN model was set with 16 filters, a kernel size of 5, and \ndilation rates of [1, 2, 4, 8]. The Transformer model utilized 2 attention heads, a hidden layer size \nof 64 in the feedforward network, and the ReLU activation function.  \nTable 3 illustrates the results of experiments on ARAS dataset with various metrices including \nAccuracy, Precision, Recall, F1-Score, and Time of training.  \n \nF. M. Shiri et al.    \n37 \n \nTable 3: Result of different deep learning models on the ARAS dataset \nmodel \nAccuracy % Precision % Recall % F1-Score % Time (h:m:s) \nCNN \n93.14 \n95.59 \n92.43 \n93.98 \n0:01:18 \nRNN \n93.17 \n96.19 \n91.67 \n93.88 \n0:04:09 \nLSTM \n93.29 \n95.56 \n92.82 \n93.81 \n0:03:23 \nBi-LSTM \n93.33 \n96.66 \n92.12 \n94.15 \n0:04:01 \nGRU \n93.65 \n96.08 \n91.78 \n94.31 \n0:03:15 \nBI-GRU \n93.90 \n95.87 \n92.61 \n94.49 \n0:03:56 \nTCN \n94.04 \n95.37 \n93.48 \n94.42 \n0:04:06 \nTransformer \n94.56 \n95.61 \n94.06 \n94.83 \n0:03:14 \n \n Also, Fig. 33 presents the accuracy diagram and validation-accuracy diagram for the deep \nlearning models, while Fig. 34 shows the loss diagram and validation-loss diagram for deep \nlearning models. \n \n \n \n \n \n \n \n \n \n \nFigure 33. Accuracy and validation- accuracy diagrams of deep learning models on ARAS dataset. \n \n \n \n \n \n \n \n \n \n \nFigure 34.  Loss and validation- loss diagrams of deep learning models on ARAS dataset \n \n  (a) Accuracy Diagram \n       (b) Validation-Accuracy Diagram \n  (a) Loss Diagram \n     (b) Validation-Loss Diagram \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n38 \n \nSince we performed preprocessing tasks like data cleaning and segmentation, the data is nearly \nnormalized and balanced, leading to consistent and closely grouped results across all models. \nHowever, the results indicate that the Transformer and TCN models outperformed the others on \nthe ARAS dataset. This outcome aligns with the dataset's nature, which comprises spatial and \ntemporal sequences of sensor events. Among the models, the Transformer exhibited the highest \nperformance in terms of accuracy, recall, and F1-score, while the Bi-LSTM model excelled in the \nprecision metric. Moreover, the Transformer model demonstrated a notable advantage in training \ntime, second only to the CNN model, underscoring its efficiency in processing and learning from \ntime-series data. Additionally, when examining the accuracy and loss curves, it is evident that the \nTransformer, TCN, and CNN models stabilized earlier than the others. Overall, the Transformer \nmodel proved to be the most effective for working with the ARAS dataset, striking a balance \nbetween accuracy, training time, and consistency throughout the training phases, making it the \noptimal choice for recognizing human activities based on sensor data.   \n11.3 Methodology and Experiments on the Fruit-360 Dataset \nSince images are not sequential or time-dependent, recurrent models were less effective for \nthese tasks. CNN-based models, on the other hand, are highly valuable for image analysis due to \ntheir ability to capture spatial relationships. Consequently, the analysis of deep learning models on \nthe Fruit-360 dataset for image classification focused on eight CNN variants: VGG, Inception, \nResNet, InceptionResNet, Xception, MobileNet, DenseNet, and NASNet. These models use deep \ntransfer learning technique for training image data and improving classification accuracy. Fig. 35 \nprovides a structural overview of the deep learning models used to evaluate the performance of \nthese eight variants on the Fruit-360 dataset. \n \n \nFigure 35.  The structural for analysis of different CNN-based models on Fruit-360 dataset. \nF. M. Shiri et al.    \n39 \n \nFirst, the fruit images are passed through an input layer. In the second layer, one of eight \nmodels (VGG, Inception, ResNet, InceptionResNet, Xception, MobileNet, DenseNet, or NASNet) \nis employed for feature extraction and training. Next, a Global Average Pooling 2D (GAP) layer is \napplied, which significantly reduces the spatial dimensions of the data by collapsing each feature \nmap into a single value. To combat overfitting, a dropout layer is then introduced, randomly \ndeactivating a portion of the neurons during training, which enhances the model's ability to \ngeneralize. Finally, the output is passed through a fully connected (Dense) layer, where a Softmax \nfunction is used to classify the fruit images. \nThe dataset comprises 55,244 images of 81 different fruit classes, each with a resolution of \n100 × 100 pixels. For the experiments, a subset of 60 fruit classes was selected, containing 28,484 \nimages for training and 9,558 images for testing. Non-fruit items such as chestnuts and ginger root \nwere removed from the dataset.  \nAll models were trained with a consistent set of parameters: 20 epochs, a batch size of 512, a \ndropout rate of 0.2, the \"Categorical Crossentropy\" loss function, and the Adam optimizer. \nAdditionally, all models utilized the “ImageNet” dataset for pre-training. \nTable 4 presents the experimental results for various models on the Fruit-360 dataset, \nincluding VGG16, InceptionV3, ResNet50, InceptionResNetV2, Xception, MobileNet, \nDenseNet121, and NASNetLarge. The table includes metrics such as Accuracy, Precision, Recall, \nF1-Score, and Time of training.  \n \nTable 4: Result of different deep learning models on the Fruit360 dataset \nmodel \nAccuracy % Precision % Recall % F1-Score % Time (h:m:s) \nVGG \n94.39 \n99.79 \n80.65 \n89.20 \n2:17:32 \nInception \n95.86 \n96.65 \n95.14 \n95.89 \n0:23:34 \nResNet \n94.59 \n95.30 \n93.64 \n94.46 \n1:12:56 \nInceptionResNet 96.05 \n97.01 \n95.36 \n96.18 \n0:54:18 \nXception \n97.38 \n98.28 \n96.61 \n97.44 \n1:01:11 \nMobileNet \n98.54 \n98.88 \n98.28 \n98.58 \n0:17:22 \nDenseNet \n98.94 \n99.12 \n98.75 \n98.94 \n1:10:30 \nNASNet \n96.99 \n97.69 \n96.56 \n97.12 \n3:50:05 \n \nFurthermore, the accuracy, validation-accuracy, loss, and validation-loss diagrams were used \nto compare the performance of various models. When assessing the models' performance for tasks \ninvolving the categorization of fruit photos, these graphs offer valuable insights into how \neffectively the models are learning from the data.  Fig. 36 shows the accuracy and validation-\naccuracy diagram of the deep learning models, while Fig. 37 illustrates the loss diagram and \nvalidation-loss diagram of the deep learning models.  \nBased on the results, it can be concluded that the DenseNet and MobileNet models achieved \nthe best performance for fruit image classification on the Fruit-360 dataset. Both models \ndemonstrated high accuracy in classifying fruit images. Notably, MobileNet had a significantly \nshorter training time compared to DenseNet, indicating that it was faster to train while still \ndelivering performance close to that of DenseNet. Additionally, the Xception model also showed \ngood accuracy and required less training time than DenseNet. Overall, the MobileNet model stands \nout as a favorable choice due to its balance between accuracy and training efficiency. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n40 \n \n \n \n \n \n \n \n \n \n \n \nFigure 36.  Accuracy and validation- accuracy diagrams of different CNN-based deep learning \nmodels on Friut-360 dataset. \n \n \n \n \n \n \n \n \n \n \nFigure 37.  Loss and validation- loss diagrams of different CNN-based deep learning models on \nFriut-360 dataset. \n12 Research Directions and Future Aspects  \nIn the preceding sections, we explored a range of deep learning topics, highlighting both the \nadvantages and limitations of various deep learning models. Additionally, we examined the \napplication of several models across different domains. Despite the benefits demonstrated, our \nresearch has identified certain gaps, indicating that further advancements are necessary. This \nsection outlines potential future research directions based on our analysis. \n- \nGenerative (Unsupervised) Models: Generative models, a key category of deep learning \nmodels discussed in Section 4, hold significant promise for future research. These models enable \nthe creation of new data representations through exploratory analysis and can identify high-order \ncorrelations or features in data. Unlike supervised learning, unsupervised models can derive \ninsights from data without the need for labeled examples, making them valuable for various \napplications. Several generative models, including Autoencoders, Generative Adversarial \nNetworks (GANs), Deep Belief Networks (DBNs), and Self-Organizing Maps (SOMs), have been \ndeveloped and employed across diverse contexts. A promising research avenue involves analyzing \nthese models in various settings and developing new methods or variations that enhance data \nmodeling or representation for specific real-world applications. The rising interest in GANs is \n \n(a) Accuracy Diagram \n   (b) Validation-Accuracy Diagram \n \n   (a) Loss Diagram \n \n   (b) Validation-Loss Diagram \nF. M. Shiri et al.    \n41 \n \nparticularly noteworthy, as they excel in leveraging unlabeled image data for deep representation \nlearning and training highly non-linear mappings between latent and data spaces. The GAN \nframework offers flexibility to formulate new theories and methods tailored to emerging deep \nlearning applications, positioning it as a pivotal area for future exploration. \n- \nHybrid/Ensemble Modeling: Hybrid deep learning architectures have shown great potential in \nenhancing model performance by combining components from multiple models. For instance, the \nintegration of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) \ncan capture both temporal and spatial dependencies in data, leveraging the strengths of each model. \nHybrid models also benefit from combining generative and supervised learning, offering superior \nperformance and improved uncertainty handling in high-risk scenarios. Developing effective \nhybrid models, whether supervised or unsupervised, presents a significant research opportunity to \naddress a wide range of real-world problems, including semi-supervised learning tasks and model \nuncertainty. This approach moves beyond conventional, isolated models, emphasizing the need for \nsophisticated methods that can handle the complexity of various data types and applications. \n- \nHyperparameter Optimization for Efficient Deep Learning: As deep learning models have \nevolved, the number of parameters, computational latency, and resource requirements have \nincreased substantially [150]. Selecting the appropriate hyperparameters is critical to building a \nneural network with high accuracy. Key hyperparameters include learning rate, loss function, batch \nsize, number of training iterations, and dropout rate, among others. The challenge lies in finding an \noptimal balance of these parameters, as they significantly influence network performance. However, \niterating through all possible combinations of hyperparameters is computationally expensive. To \naddress this, metaheuristic optimization techniques, such as Genetic Algorithm (GA) [302], Particle \nSwarm Optimization (PSO) [303], and others, can be employed to explore the search space more \nefficiently than exhaustive methods. Meta-heuristic algorithms are a type of heuristic optimization \nalgorithm that include mechanisms to avoid getting trapped in local optimization [304]. Future \nresearch should focus on optimizing hyperparameters tailored to specific data types and contexts. \nFor example, the learning rate plays a crucial role in training, where a rate too high may cause the \nmodel to converge prematurely, while a rate too low can lead to slow convergence and prolonged \ntraining times. Adaptive learning rate techniques, such as including Adaptive Moment Estimation \n(Adam) [305], Stochastic Gradient Descent (SGD) [306], adaptive gradient algorithm (ADAGRAD) \n[307], and Nesterov-accelerated Adaptive Moment Estimation (Nadam) [308], and more recent \ninnovations like Evolved Sign Momentum (Lion) [309], offer promising avenues for improving \nnetwork performance and minimizing loss functions. Future research could further explore these \noptimizers, focusing on their comparative effectiveness in enhancing model performance through \niterative weight and bias adjustments. \n- \nFederated Learning: Federated learning is an emerging deep learning paradigm that enables \ncollaborative model training across multiple organizations or teams without the need to share raw \ndata. This approach is particularly relevant in contexts where data privacy is paramount. However, \nfederated learning introduces new challenges, especially with the advent of data fusion technologies \nthat combine data from multiple sources with varying formats. As data diversity and volume \ncontinue to grow, optimizing data and model utilization in federated learning becomes increasingly \nimportant. Addressing challenges such as safeguarding user privacy, developing universal models, \nand ensuring the stability of data fusion outcomes will be crucial for the future application of \nfederated learning across multiple domains [310]. \n- \nQuantum Deep Learning: Quantum computing and deep learning have both seen significant \nadvancements over the past few decades. Quantum computing, which leverages the principles of \nquantum mechanics to store and process information, has the potential to outperform classical \nsupercomputers on certain tasks, making it a powerful tool for complex problem-solving. The \nintersection of quantum computing and deep learning has led to the emergence of quantum deep \nlearning and quantum-inspired deep learning algorithms. Future research directions in this area \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n42 \n \ninclude investigating and developing quantum deep learning models, such as Quantum \nConvolutional Neural Network (Quantum CNN) [311], Quantum Recurrent Neural Network \n(Quantum RNN) [312], Quantum Generative Adversarial Network (Quantum GAN) [313], and \nothers. Additionally, exploring the application of these models across various domains and creating \nnovel quantum deep learning architectures represents a cutting-edge frontier in the field [314, 315].   \nIn conclusion, the research directions outlined above underscore the dynamic and evolving \nnature of deep learning. By addressing these challenges and exploring new avenues, the field can \ncontinue to advance, driving innovation and enabling the development of more powerful and \nefficient models for a wide range of applications. \n13 Conclusion \nThis article provides an extensive overview of deep learning technology and its applications \nin machine learning and artificial intelligence. The article covers various aspects of deep learning, \nincluding neural networks, MLP models, and different types of deep learning models such as CNN, \nRNN, TCN, Transformer, generative models, DRL, and transfer learning. The classification of deep \nlearning models allows for a better understanding of their specific applications and characteristics. \nThe RNN models, including LSTM, Bi-LSTM, GRU, and Bi-GRU, are particularly suited for time \nseries data due to their ability to capture temporal dependencies. On the other hand, CNN-based \nmodels excel in image data analysis by effectively capturing spatial features.  \nThe experiments conducted on three public datasets, namely IMDB, ARAS, and Fruit-360, \nfurther reinforce the suitability of specific deep learning models for different data types. The results \ndemonstrate that the CNN-based models such as DenseNet and MobileNet perform exceptionally \nwell in image classification tasks. RNN models, such as LSTM and GRU, show strong performance \nin time series analysis. However, the Transformer model outperforms classical RNN-based models, \nparticularly in text analysis, due to its use of the attention mechanism.  \nOverall, this article highlights the diverse applications and effectiveness of deep learning \nmodels in various domains. It emphasizes the importance of selecting the appropriate deep learning \nmodel based on the nature of the data and the task at hand. The insights gained from the experiments \ncontribute to a better understanding of the strengths and weaknesses of different deep learning \nmodels, facilitating informed decision-making in practical applications. \nReferences \n[1] P. P. Shinde and S. Shah, \"A review of machine learning and deep learning applications,\" in 4th Int. \nConf. Comput. Commun. Ctrl. Autom. (ICCUBEA), Pune, India, 16-18 Aug 2018: IEEE, pp. 1-6, doi: \n10.1109/ICCUBEA.2018.8697857.  \n[2] C. Janiesch, P. Zschech, and K. Heinrich, \"Machine learning and deep learning,\" Electron. Mark., vol. \n31, no. 3, pp. 685-695, 2021, doi: 10.1007/s12525-021-00475-2. \n[3] W. Han et al., \"A survey of machine learning and deep learning in remote sensing of geological \nenvironment: Challenges, advances, and opportunities,\" ISPRS J. Photogramm. Remote. Sens., vol. 202, \npp. 87-113, 2023, doi: 10.1016/j.cogr.2023.04.001. \n[4] S. Zhang et al., \"Deep Learning in Human Activity Recognition with Wearable Sensors: A Review on \nAdvances,\" Sens., vol. 22, no. 4, Feb 14 2022, doi: 10.3390/s22041476. \n[5] S. Li, Y. Tao, E. Tang, T. Xie, and R. Chen, \"A survey of field programmable gate array (FPGA)-based \ngraph convolutional neural network accelerators: challenges and opportunities,\" PeerJ Computer \nScience, vol. 8, pp. e1166, 2022. \nF. M. Shiri et al.    \n43 \n \n[6] A. Mathew, P. Amudha, and S. Sivakumari, \"Deep learning techniques: an overview,\" in Adv. Mach. \nLearn. Technol. App.: AMLTA 2020, 2021, pp. 599-608. \n[7] J. Liu and Y. Jin, \"A comprehensive survey of robust deep learning in computer vision,\" J. Autom. \nIntell. , 2023, doi: 10.1016/j.jai.2023.10.002. \n[8] A. Shrestha and A. Mahmood, \"Review of deep learning algorithms and architectures,\" IEEE access., \nvol. 7, pp. 53040-53065, 2019, doi: 10.1109/ACCESS.2019.2912200. \n[9] M. A. Wani, F. A. Bhat, S. Afzal, and A. I. Khan, Advances in deep learning. Springer, 2020. \n[10] L. Alzubaidi et al., \"Review of deep learning: Concepts, CNN architectures, challenges, applications, \nfuture directions,\" J. Big. Data., vol. 8, pp. 1-74, 2021, doi: 10.1186/s40537-021-00444-8. \n[11] I. H. Sarker, \"Deep learning: a comprehensive overview on techniques, taxonomy, applications and \nresearch directions,\" SN Comput. Sci., vol. 2, no. 6, pp. 420, 2021, doi: 10.1007/s42979-021-00815-1. \n[12] M. N. Hasan, T. Ahmed, M. Ashik, M. J. Hasan, T. Azmin, and J. Uddin, \"An Analysis of Covid-19 \nPandemic Outbreak on Economy using Neural Network and Random Forest,\" J. Inf. Syst. Telecommun. \n(JIST), vol. 2, no. 42, pp. 163, 2023, doi: 10.52547/jist.34246.11.42.163. \n[13] N. B. Gaikwad, V. Tiwari, A. Keskar, and N. Shivaprakash, \"Efficient FPGA implementation of \nmultilayer perceptron for real-time human activity classification,\" IEEE Access., vol. 7, pp. 26696-\n26706, 2019, doi: 10.1109/ACCESS.2019.2900084. \n[14] K.-C. Ke and M.-S. Huang, \"Quality prediction for injection molding by using a multilayer perceptron \nneural network,\" Polym., vol. 12, no. 8, pp. 1812, 2020, doi: 10.3390/polym12081812. \n[15] A. Tasdelen and B. Sen, \"A hybrid CNN-LSTM model for pre-miRNA classification,\" Sci. Rep., vol. \n11, no. 1, pp. 1-9, 2021, doi: 10.1038/s41598-021-93656-0. \n[16] L. Qin, N. Yu, and D. Zhao, \"Applying the convolutional neural network deep learning technology to \nbehavioural recognition in intelligent video,\" Tehnički vjesnik, vol. 25, no. 2, pp. 528-535, 2018, doi: \n10.17559/TV-20171229024444. \n[17] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, \"A Survey of Convolutional Neural Networks: Analysis, \nApplications, and Prospects,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 12, pp. 6999-7019, \nDec 2022, doi: 10.1109/TNNLS.2021.3084827. \n[18] B. P. Babu and S. J. Narayanan, \"One-vs-All Convolutional Neural Networks for Synthetic Aperture \nRadar Target Recognition,\" Cybern. Inf. Technol, vol. 22, pp. 179-197, 2022, doi: 10.2478/cait-2022-\n0035. \n[19] S. Mekruksavanich and A. Jitpattanakul, \"Deep convolutional neural network with rnns for complex \nactivity recognition using wrist-worn wearable sensor data,\" Electro., vol. 10, no. 14, pp. 1685, 2021, \ndoi: 10.3390/electronics10141685. \n[20] W. Lu, J. Li, J. Wang, and L. Qin, \"A CNN-BiLSTM-AM method for stock price prediction,\" Neural \nComput. Appl., vol. 33, pp. 4741-4753, 2021, doi: 10.1007/s00521-020-05532-z. \n[21] W. Rawat and Z. Wang, \"Deep convolutional neural networks for image classification: A \ncomprehensive review,\" Neural Comput., vol. 29, no. 9, pp. 2352-2449, 2017, doi: \n10.1162/NECO_a_00990. \n[22] L. Chen, S. Li, Q. Bai, J. Yang, S. Jiang, and Y. Miao, \"Review of image classification algorithms based \non convolutional neural networks,\" Remote Sens., vol. 13, no. 22, pp. 4712, 2021, doi: \n10.3390/rs13224712. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n44 \n \n[23] J. Gu et al., \"Recent advances in convolutional neural networks,\" Pattern. Recognit., vol. 77, pp. 354-\n377, 2018, doi: 10.1016/j.patcog.2017.10.013. \n[24] S. Salman and X. Liu, \"Overfitting mechanism and avoidance in deep neural networks,\" arXiv preprint \narXiv:1901.06566, 2019. \n[25] A. Ajit, K. Acharya, and A. Samanta, \"A review of convolutional neural networks,\" in 2020 Int. Conf. \nEmerg. Tren. Inf. Technol. Engr. (ic-ETITE). 2020: IEEE, pp. 1-5.  \n[26] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, \"A survey of deep neural network \narchitectures and their applications,\" Neurocomputing., vol. 234, pp. 11-26, 2017, doi: \n10.1016/j.neucom.2016.12.038. \n[27] K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling in deep convolutional networks for visual \nrecognition,\" IEEE Trans. Pattern. Anal. Mach. Intell., vol. 37, no. 9, pp. 1904-1916, 2015, doi: \n10.1109/TPAMI.2015.2389824. \n[28] D. Yu, H. Wang, P. Chen, and Z. Wei, \"Mixed pooling for convolutional neural networks,\" in Rough. \nSets. Knwl. Technol.: 9th Int. Conf., RSKT Shanghai, China, October 24-26 2014: Springer, pp. 364-\n375, doi: 10.1007/978-3-319-11740-9_34.  \n[29] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, \"Multi-scale orderless pooling of deep convolutional \nactivation features,\" in Comput. Vis. (ECCV): 13th Europ. Conf., Zurich, Switzerland, September 6-12 \n2014: Springer, pp. 392-407.  \n[30] M. D. Zeiler and R. Fergus, \"Stochastic pooling for regularization of deep convolutional neural \nnetworks,\" arXiv preprint arXiv:1301.3557, 2013. \n[31] V. Dumoulin and F. Visin, \"A guide to convolution arithmetic for deep learning,\" arXiv preprint \narXiv:1603.07285, 2016. \n[32] M. Krichen, \"Convolutional neural networks: A survey,\" Comput. , vol. 12, no. 8, pp. 151, 2023, doi: \n10.3390/computers12080151. \n[33] S. Kılıçarslan, K. Adem, and M. Çelik, \"An overview of the activation functions used in deep learning \nalgorithms,\" J. New Results Sci., vol. 10, no. 3, pp. 75-88, 2021, doi: 10.54187/jnrs.1011739. \n[34] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall, \"Activation functions: Comparison of trends \nin practice and research for deep learning,\" arXiv preprint arXiv:1811.03378, 2018. \n[35] K. Hara, D. Saito, and H. Shouno, \"Analysis of function of rectified linear unit used in deep learning,\" \nin Int. Jt. Conf. Neural. Netw. (IJCNN), Killarney, Ireland, 2015, pp. 1-8.  \n[36] A. L. Maas, A. Y. Hannun, and A. Y. Ng, \"Rectifier nonlinearities improve neural network acoustic \nmodels,\" in Proc. Int. Conf. Mach. Learn. (ICML), 2013, vol. 30, no. 1: Atlanta, GA, USA, pp. 3.  \n[37] K. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers: Surpassing human-level performance \non imagenet classification,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1026-1034.  \n[38] B. Xu, N. Wang, T. Chen, and M. Li, \"Empirical evaluation of rectified activations in convolutional \nnetwork,\" arXiv preprint arXiv:1505.00853, 2015. \n[39] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, and S. Yan, \"Deep learning with s-shaped rectified linear \nactivation units,\" in Proc. AAAI Conf. Artif. Intell., 2016, vol. 30, no. 1, doi: 10.1609/aaai.v30i1.10287.  \n[40] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, \"Fast and accurate deep network learning by \nexponential linear units (elus),\" arXiv preprint arXiv:1511.07289, 2015. \nF. M. Shiri et al.    \n45 \n \n[41] D. Hendrycks and K. Gimpel, \"Gaussian error linear units (gelus),\" arXiv preprint arXiv:1606.08415, \n2016. \n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural \nnetworks,\" Adv. Neural Inf. Process. Syst., vol. 25, 2012. \n[43] K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" \narXiv preprint arXiv:1409.1556, 2014. \n[44] C. Szegedy et al., \"Going deeper with convolutions,\" in Proc. IEEE Conf. Comput. Vis. Pattern. \nRecognit., 2015, pp. 1-9.  \n[45] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for \ncomputer vision,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2016, pp. 2818-2826.  \n[46] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE/CVF \nConf. Comput. Vis. Pattern. Recognit., 2016, pp. 770-778.  \n[47] K. He, X. Zhang, S. Ren, and J. Sun, \"Identity mappings in deep residual networks,\" in Comput. Vis. \n(ECCV): 14th Europ. Conf., Amsterdam, The Netherlands, October 11–14 2016: Springer, pp. 630-645.  \n[48] S. Zagoruyko and N. Komodakis, \"Wide residual networks,\" arXiv preprint arXiv:1605.07146, 2016. \n[49] G. Larsson, M. Maire, and G. Shakhnarovich, \"Fractalnet: Ultra-deep neural networks without \nresiduals,\" arXiv preprint arXiv:1605.07648, 2016. \n[50] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, \"SqueezeNet: \nAlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size,\" arXiv preprint \narXiv:1602.07360, 2016. \n[51] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, \"Inception-v4, inception-resnet and the impact of \nresidual connections on learning,\" in Proc. AAAI  Conf. Artif. Intell., 2017, vol. 31, no. 1, doi: \n10.1609/aaai.v31i1.11231.  \n[52] F. Chollet, \"Xception: Deep learning with depthwise separable convolutions,\" in Proc. IEEE Conf. \nComput. Vis. Pattern. Recognit., 2017, pp. 1251-1258.  \n[53] A. G. Howard et al., \"Mobilenets: Efficient convolutional neural networks for mobile vision \napplications,\" arXiv preprint arXiv:1704.04861, 2017. \n[54] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \"Mobilenetv2: Inverted residuals and \nlinear bottlenecks,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2018, pp. 4510-4520.  \n[55] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \"Densely connected convolutional \nnetworks,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2017, pp. 4700-4708.  \n[56] J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proc. IEEE Conf. Comput. Vis. \nPattern. Recognit., 2018, pp. 7132-7141.  \n[57] M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for convolutional neural networks,\" in Int. \nConf. Mach. Learn., 2019: PMLR, pp. 6105-6114.  \n[58] M. Tan and Q. Le, \"Efficientnetv2: Smaller models and faster training,\" in Int. Conf. Mach. Learn., \n2021: PMLR, pp. 10096-10106.  \n[59] S. Abbaspour, F. Fotouhi, A. Sedaghatbaf, H. Fotouhi, M. Vahabi, and M. Linden, \"A Comparative \nAnalysis of Hybrid Deep Learning Models for Human Activity Recognition,\" Sens., vol. 20, no. 19, \n2020, doi: 10.3390/s20195707. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n46 \n \n[60] W. Fang, Y. Chen, and Q. Xue, \"Survey on research of RNN-based spatio-temporal sequence prediction \nalgorithms,\" J. Big. Data., vol. 3, no. 3, pp. 97, 2021, doi: 10.32604/jbd.2021.016993. \n[61] J. Xiao and Z. Zhou, \"Research progress of RNN language model,\" in 2020 IEEE Int. Conf. Artif. Intell. \nComput. App. (ICAICA), Dalian, China, 27-29 June 2020: IEEE, pp. 1285-1288, doi: \n10.1109/ICAICA50127.2020.9182390.  \n[62] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici, \"Beyond \nshort snippets: Deep networks for video classification,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. \nRecognit., 2015, pp. 4694-4702.  \n[63] A. Shewalkar, D. Nyavanandi, and S. A. Ludwig, \"Performance evaluation of deep neural networks \napplied to speech recognition: RNN, LSTM and GRU,\" J. Artif. Intell. Soft Comput. Res., vol. 9, no. 4, \npp. 235-245, 2019, doi: 10.2478/jaiscr-2019-0006. \n[64] H. Apaydin, H. Feizi, M. T. Sattari, M. S. Colak, S. Shamshirband, and K.-W. Chau, \"Comparative \nanalysis of recurrent neural network architectures for reservoir inflow forecasting,\" Water., vol. 12, no. \n5, pp. 1500, 2020, doi: 10.3390/w12051500. \n[65] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural. Comput., vol. 9, no. 8, pp. 1735-\n1780, 1997. MIT-Press. \n[66] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber, \"A novel \nconnectionist system for unconstrained handwriting recognition,\" IEEE Trans. Pattern. Anal. Mach. \nIntell., vol. 31, no. 5, pp. 855-868, 2008, doi: 10.1109/TPAMI.2008.137. \n[67] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \"Empirical evaluation of gated recurrent neural networks \non sequence modeling,\" arXiv preprint arXiv:1412.3555, 2014. \n[68] J. Chen, D. Jiang, and Y. Zhang, \"A hierarchical bidirectional GRU model with attention for EEG-based \nemotion \nclassification,\" \nIEEE \nAccess., \nvol. \n7, \npp. \n118530-118540, \n2019, \ndoi: \n10.1109/ACCESS.2019.2936817. \n[69] M. Fortunato, C. Blundell, and O. Vinyals, \"Bayesian recurrent neural networks,\" arXiv preprint \narXiv:1704.02798, 2017. \n[70] F. Kratzert, D. Klotz, C. Brenner, K. Schulz, and M. Herrnegger, \"Rainfall–runoff modelling using long \nshort-term memory (LSTM) networks,\" Hydrol. Earth Syst. Sci., vol. 22, no. 11, pp. 6005-6022, 2018, \ndoi: 10.5194/hess-22-6005-2018. \n[71] A. Graves, \"Generating sequences with recurrent neural networks,\" arXiv preprint arXiv:1308.0850, \n2013. \n[72] S. Minaee, E. Azimi, and A. Abdolrashidi, \"Deep-sentiment: Sentiment analysis using ensemble of cnn \nand bi-lstm models,\" arXiv preprint arXiv:1904.04206, 2019. \n[73] D. Gaur and S. Kumar Dubey, \"Development of Activity Recognition Model using LSTM-RNN Deep \nLearning Algorithm,\" J. inf. organ. sci., vol. 46, no. 2, pp. 277-291, 2022, doi: 10.31341/jios.46.2.1. \n[74] X. Zhu, P. Sobihani, and H. Guo, \"Long short-term memory over recursive structures,\" in Int. Conf.  \nMach. Learn., 2015: PMLR, pp. 1604-1612.  \n[75] F. Gu, M.-H. Chung, M. Chignell, S. Valaee, B. Zhou, and X. Liu, \"A survey on deep learning for \nhuman activity recognition,\" ACM Comput. Surv., vol. 54, no. 8, pp. 1-34, 2021, doi: 10.1145/3472290. \nF. M. Shiri et al.    \n47 \n \n[76] T. H. Aldhyani and H. Alkahtani, \"A bidirectional long short-term memory model algorithm for \npredicting COVID-19 in gulf countries,\" Life., vol. 11, no. 11, pp. 1118, 2021, doi: \n10.3390/life11111118. \n[77] D. Liciotti, M. Bernardini, L. Romeo, and E. Frontoni, \"A sequential deep learning application for \nrecognising human activities in smart homes,\" Neurocomputing., vol. 396, pp. 501-513, 2020, doi: \n10.1016/j.neucom.2018.10.104. \n[78] A. Dutta, S. Kumar, and M. Basu, \"A gated recurrent unit approach to bitcoin price prediction,\" J. Risk \nFinancial Manag., vol. 13, no. 2, pp. 23, 2020, doi: 10.3390/jrfm13020023. \n[79] A. Gumaei, M. M. Hassan, A. Alelaiwi, and H. Alsalman, \"A Hybrid Deep Learning Model for Human \nActivity Recognition Using Multimodal Body Sensing Data,\" IEEE Access., vol. 7, pp. 99152-99160, \n2019, doi: 10.1109/access.2019.2927134. \n[80] D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and \ntranslate,\" arXiv preprint arXiv:1409.0473, 2014. \n[81] C. Chai et al., \"A Multifeature Fusion Short‐Term Traffic Flow Prediction Model Based on Deep \nLearnings,\" J. Adv. Transp., vol. 2022, no. 1, pp. 1702766, 2022, doi: 10.1155/2022/1702766. \n[82] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, \"Temporal convolutional networks for action \nsegmentation and detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern. Recognit., 2017, pp. 156-165.  \n[83] S. Bai, J. Z. Kolter, and V. Koltun, \"An empirical evaluation of generic convolutional and recurrent \nnetworks for sequence modeling,\" arXiv preprint arXiv:1803.01271, 2018. \n[84] Y. He and J. Zhao, \"Temporal convolutional networks for anomaly detection in time series,\" J. Phys.: \nConf. Ser., vol. 1213, no. 4, pp. 042050, 2019, doi: 10.1088/1742-6596/1213/4/042050. \n[85] J. Zhu, L. Su, and Y. Li, \"Wind power forecasting based on new hybrid model with TCN residual \nmodification,\" Energy AI., vol. 10, pp. 100199, 2022, doi: 10.1016/j.egyai.2022.100199  \n[86] D. Li, F. Jiang, M. Chen, and T. Qian, \"Multi-step-ahead wind speed forecasting based on a hybrid \ndecomposition method and temporal convolutional networks,\" Energy., vol. 238, pp. 121981, 2022, doi: \n10.3390/en16093792. \n[87] X. Zhang, F. Dong, G. Chen, and Z. Dai, \"Advance prediction of coastal groundwater levels with \ntemporal convolutional and long short-term memory networks,\" Hydrol. Earth Syst. Sci., vol. 27, no. 1, \npp. 83-96, 2023, doi: 10.5194/hess-27-83-2023. \n[88] F. Yu and V. Koltun, \"Multi-scale context aggregation by dilated convolutions,\" arXiv preprint \narXiv:1511.07122, 2015. \n[89] T. Salimans and D. P. Kingma, \"Weight normalization: A simple reparameterization to accelerate \ntraining of deep neural networks,\" Adv. Neural Inf. Process. Syst., vol. 29, no. 29, 2016. \n[90] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: a simple way \nto prevent neural networks from overfitting,\" The journal of machine learning research, vol. 15, no. 1, \npp. 1929-1958, 2014. \n[91] Z. Liu et al., \"Kan: Kolmogorov-arnold networks,\" arXiv preprint arXiv:2404.19756, 2024. \n[92] J. Braun and M. Griebel, \"On a constructive proof of Kolmogorov’s superposition theorem,\" Constr. \nApprox., vol. 30, pp. 653-675, 2009, doi: 10.1007/s00365-009-9054-2. \n[93] A. D. Bodner, A. S. Tepsich, J. N. Spolski, and S. Pourteau, \"Convolutional Kolmogorov-Arnold \nNetworks,\" arXiv preprint arXiv:2406.13155, 2024. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n48 \n \n[94] R. Genet and H. Inzirillo, \"Tkan: Temporal kolmogorov-arnold networks,\" arXiv preprint \narXiv:2405.07344, 2024. \n[95] K. Pan, X. Zhang, and L. Chen, \"Research on the Training and Application Methods of a Lightweight \nAgricultural Domain-Specific Large Language Model Supporting Mandarin Chinese and Uyghur,\" Appl. \nSci., vol. 14, no. 13, pp. 5764, 2024, doi: 10.3390/app14135764. \n[96] R. Genet and H. Inzirillo, \"A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting,\" \narXiv preprint arXiv:2406.02486, 2024. \n[97] K. Xu, L. Chen, and S. Wang, \"Kolmogorov-Arnold Networks for Time Series: Bridging Predictive \nPower and Interpretability,\" arXiv preprint arXiv:2406.02496, 2024. \n[98] A. A. Aghaei, \"fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions,\" \narXiv preprint arXiv:2406.07456, 2024. \n[99] Z. Bozorgasl and H. Chen, \"Wav-kan: Wavelet kolmogorov-arnold networks,\" arXiv preprint \narXiv:2405.12832, 2024. \n[100] \nF. Zhang and X. Zhang, \"GraphKAN: Enhancing Feature Extraction with Graph Kolmogorov \nArnold Networks,\" arXiv preprint arXiv:2406.13597, 2024. \n[101] \nA. Jabbar, X. Li, and B. Omar, \"A survey on generative adversarial networks: Variants, \napplications, and training,\" ACM Comput. Surv., vol. 54, no. 8, pp. 1-49, 2021, doi: 10.1145/3463475. \n[102] \nD. Bank, N. Koenigstein, and R. Giryes, \"Autoencoders,\" in Machine learning for data science \nhandbook:data mining and knowledge discovery handbook: Springer, 2023, pp. 353-374. \n[103] \nI. Goodfellow et al., \"Generative adversarial nets,\" Adv. Neural. Inf. Process. Syst., vol. 27, pp. \n2672–2680, 2014. \n[104] \nN. Zhang, S. Ding, J. Zhang, and Y. Xue, \"An overview on restricted Boltzmann machines,\" \nNeurocomputing., vol. 275, pp. 1186-1199, 2018, doi: 10.1016/j.neucom.2017.09.065. \n[105] \nG. E. Hinton, \"Deep belief networks,\" Scholarpedia, vol. 4, no. 5, pp. 5947, 2009, doi: \n10.4249/scholarpedia.5947. \n[106] \nI. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016. \n[107] \nJ. Zhai, S. Zhang, J. Chen, and Q. He, \"Autoencoder and its various variants,\" in 2018 IEEE Int. \nConf. Syst. Man. Cybern. (SMC), Miyazaki, Japan, 7-10 Oct 2018: IEEE, pp. 415-419, doi: \n10.1109/SMC.2018.00080.  \n[108] \nA. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, \"Adversarial autoencoders,\" arXiv \npreprint arXiv:1511.05644, 2015. \n[109] \nY. Wang, H. Yao, and S. Zhao, \"Auto-encoder based dimensionality reduction,\" \nNEUROCOMPUTING, vol. 184, pp. 232-242, 2016, doi: 10.1016/j.neucom.2015.08.104. \n[110] \nY. N. Kunang, S. Nurmaini, D. Stiawan, and A. Zarkasi, \"Automatic features extraction using \nautoencoder in intrusion detection system,\" in 2018 Int. Conf. Electr. engr. Compu. Sci. (ICECOS), \nPangkal, Indonesia, 2-4 Oct 2018: IEEE, pp. 219-224, doi: 10.1109/ICECOS.2018.8605181.  \n[111] \nC. Zhou and R. C. Paffenroth, \"Anomaly detection with robust deep autoencoders,\" in Proc. 23rd \nACM \nSIGKDD \nInt. \nConf. \nKnwl. \nDiscov. \nData \nMining., \n2017, \npp. \n665-674, \ndoi: \n10.1145/3097983.3098052.  \n[112] \nA. Creswell and A. A. Bharath, \"Denoising adversarial autoencoders,\" IEEE Trans. Neural Netw. \nLearn. Syst., vol. 30, no. 4, pp. 968-984, 2018, doi: 10.1109/TNNLS.2018.2852738. \nF. M. Shiri et al.    \n49 \n \n[113] \nD. P. Kingma and M. Welling, \"Auto-encoding variational bayes,\" arXiv preprint \narXiv:1312.6114, 2013. \n[114] \nA. Ng, \"Sparse autoencoder,\" CS294A Lecture notes, vol. 72, no. 2011, pp. 1-19, 2011. \n[115] \nS. Rifai et al., \"Higher order contractive auto-encoder,\" in Mach. Learn. Knwl. Discov. DB.: Europ. \nConf. ECML PKDD, Athens, Greece, September 5-9 2011: Springer, pp. 645-660.  \n[116] \nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, \"Extracting and composing robust \nfeatures with denoising autoencoders,\" in Proc. 25th Int. Conf.  Mach. Learn., 2008, pp. 1096-1103, \ndoi: 10.1145/1390156.1390294.  \n[117] \nD. P. Kingma and M. Welling, \"An introduction to variational autoencoders,\" Foundations and \nTrends® in Machine Learning, vol. 12, no. 4, pp. 307-392, 2019. \n[118] \nM.-Y. Liu and O. Tuzel, \"Coupled generative adversarial networks,\" Adv. Neural Inf. Process. \nSyst., vol. 29, 2016. \n[119] \nC. Wang, C. Xu, X. Yao, and D. Tao, \"Evolutionary generative adversarial networks,\" IEEE Trans. \nEvol. Comput., vol. 23, no. 6, pp. 921-934, 2019, doi: 10.1109/TEVC.2019.2895748. \n[120] \nA. Aggarwal, M. Mittal, and G. Battineni, \"Generative adversarial network: An overview of theory \nand applications,\" Int. J. Inf. Manag. Data Insights., vol. 1, no. 1, pp. 100004, 2021, \ndoi:10.1016/j.jjimei.2020.100004. \n[121] \nB.-C. Chen and A. Kae, \"Toward realistic image compositing with adversarial learning,\" in Proc. \nIEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2019, pp. 8415-8424.  \n[122] \nD. P. Jaiswal, S. Kumar, and Y. Badr, \"Towards an artificial intelligence aided design approach: \napplication to anime faces with generative adversarial networks,\" Procedia Comput. Sci., vol. 168, pp. \n57-64, 2020, doi: 10.1016/j.procs.2020.02.257. \n[123] \nY. Liu, Q. Li, and Z. Sun, \"Attribute-aware face aging with wavelet-based generative adversarial \nnetworks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2019, pp. 11877-11886.  \n[124] \nJ. Islam and Y. Zhang, \"GAN-based synthetic brain PET image generation,\" Brain Inform., vol. 7, \npp. 1-12, 2020, doi: 10.1186/s40708-020-00104-2. \n[125] \nH. Lan, A. D. N. Initiative, A. W. Toga, and F. Sepehrband, \"SC-GAN: 3D self-attention \nconditional GAN with spectral normalization for multi-modal neuroimaging synthesis,\" BioRxiv, pp. \n2020.06. 09.143297, 2020, doi: 10.1101/2020.06.09.143297. \n[126] \nK. A. Zhang, A. Cuesta-Infante, L. Xu, and K. Veeramachaneni, \"SteganoGAN: High capacity \nimage steganography with GANs,\" arXiv preprint arXiv:1901.03892, 2019. \n[127] \nS. Nam, Y. Kim, and S. J. Kim, \"Text-adaptive generative adversarial networks: manipulating \nimages with natural language,\" Adv. Neural Inf. Process. Syst., vol. 31, 2018. \n[128] \nL. Sixt, B. Wild, and T. Landgraf, \"Rendergan: Generating realistic labeled data,\" Front. Robot. \nAI. , vol. 5, pp. 66, 2018, doi: 10.3389/frobt.2018.00066. \n[129] \nK. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun, \"Adversarial ranking for language generation,\" \nAdv. Neural Inf. Process. Syst., vol. 30, 2017. \n[130] \nD. Xu, C. Wei, P. Peng, Q. Xuan, and H. Guo, \"GE-GAN: A novel deep learning framework for \nroad traffic state estimation,\" Transp. Res. Part C Emerg., vol. 117, pp. 102635, 2020, doi: \n10.1016/j.trc.2020.102635. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n50 \n \n[131] \nA. Clark, J. Donahue, and K. Simonyan, \"Adversarial video generation on complex datasets,\" \narXiv preprint arXiv:1907.06571, 2019. \n[132] \nE. L. Denton, S. Chintala, and R. Fergus, \"Deep generative image models using a  laplacian \npyramid of adversarial networks,\" Adv. Neural Inf. Process. Syst., vol. 28, 2015. \n[133] \nC. Li and M. Wand, \"Precomputed real-time texture synthesis with markovian generative \nadversarial networks,\" in Comput. Vis. (ECCV): 14th Europ. Conf., Amsterdam, Netherlands, October \n11-14 2016: Springer, pp. 702-716, doi: 10.1007/978-3-319-46487-9_43.  \n[134] \nL. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein, \"Unrolled generative adversarial networks,\" \narXiv preprint arXiv:1611.02163, 2016. \n[135] \nM. Arjovsky, S. Chintala, and L. Bottou, \"Wasserstein generative adversarial networks,\" in Int. \nConf. Mach. Learn., 2017: PMLR, pp. 214-223.  \n[136] \nD. Berthelot, T. Schumm, and L. Metz, \"Began: Boundary equilibrium generative adversarial \nnetworks,\" arXiv preprint arXiv:1703.10717, 2017. \n[137] \nJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \"Unpaired image-to-image translation using cycle-\nconsistent adversarial networks,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2223-2232.  \n[138] \nT. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim, \"Learning to discover cross-domain relations with \ngenerative adversarial networks,\" in Int. Conf. Mach. Learn., 2017: PMLR, pp. 1857-1865.  \n[139] \nA. Jolicoeur-Martineau, \"The relativistic discriminator: a key element missing from standard \nGAN,\" arXiv preprint arXiv:1807.00734, 2018. \n[140] \nT. Karras, S. Laine, and T. Aila, \"A style-based generator architecture for generative adversarial \nnetworks,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2019, pp. 4401-4410.  \n[141] \nG. Zhao, M. E. Meyerand, and R. M. Birn, \"Bayesian conditional GAN for MRI brain image \nsynthesis,\" arXiv preprint arXiv:2005.11875, 2020. \n[142] \nK. Chen, D. Zhang, L. Yao, B. Guo, Z. Yu, and Y. Liu, \"Deep learning for sensor-based human \nactivity recognition: Overview, challenges, and opportunities,\" ACM Comput. Surv., vol. 54, no. 4, pp. \n1-40, 2021, doi: 10.1145/3447744. \n[143] \nN. Alqahtani et al., \"Deep belief networks (DBN) with IoT-based alzheimer’s disease detection \nand classification,\" Appl. Sci., vol. 13, no. 13, pp. 7833, 2023, doi: 10.3390/app13137833. \n[144] \nA. P. Kale, R. M. Wahul, A. D. Patange, R. Soman, and W. Ostachowicz, \"Development of Deep \nbelief network for tool faults recognition,\" Sens., vol. 23, no. 4, pp. 1872, 2023, doi: 10.3390/s23041872. \n[145] \nE. Sansano, R. Montoliu, and O. Belmonte Fernandez, \"A study of deep neural networks for human \nactivity recognition,\" Comput. Intell., vol. 36, no. 3, pp. 1113-1139, 2020, doi: 10.1111/coin.12318. \n[146] \nA. Vaswani et al., \"Attention is all you need,\" Advances in neural information processing systems, \nvol. 30, 2017. \n[147] \nJ. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv preprint arXiv:1607.06450, \n2016. \n[148] \nK. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek, \"Actor-transformers for group activity \nrecognition,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2020, pp. 839-848.  \n[149] \nY. Tay, M. Dehghani, D. Bahri, and D. Metzler, \"Efficient transformers: A survey,\" ACM Comput. \nSurv., vol. 55, no. 6, pp. 1-28, 2022, doi: 10.24963/ijcai.2023/764. \nF. M. Shiri et al.    \n51 \n \n[150] \nG. Menghani, \"Efficient deep learning: A survey on making deep learning models smaller, faster, \nand better,\" ACM Comput. Surv., vol. 55, no. 12, pp. 1-37, 2023, doi: 10.1145/3578938. \n[151] \nY. Liu and L. Wu, \"Intrusion Detection Model Based on Improved Transformer,\" Applied Sciences, \nvol. 13, no. 10, pp. 6251, 2023. \n[152] \nD. Chen, S. Yongchareon, E. M. K. Lai, J. Yu, Q. Z. Sheng, and Y. Li, \"Transformer With \nBidirectional GRU for Nonintrusive, Sensor-Based Activity Recognition in a Multiresident \nEnvironment,\" IEEE Internet Things J., vol. 9, no. 23, pp. 23716-23727, 2022, doi: \n10.1109/jiot.2022.3190307. \n[153] \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional \ntransformers for language understanding,\" arXiv preprint arXiv:1810.04805, 2018. \n[154] \nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, \"Improving language understanding \nby generative pre-training,\" 2018. \n[155] \nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language models are \nunsupervised multitask learners,\" OpenAI blog, vol. 1, no. 8, pp. 9, 2019. \n[156] \nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \"Transformer-xl: \nAttentive language models beyond a fixed-length context,\" arXiv preprint arXiv:1901.02860, 2019. \n[157] \nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, \"Xlnet: Generalized \nautoregressive pretraining for language understanding,\" Adv. Neural Inf. Process. Syst., vol. 32, 2019. \n[158] \nN. Shazeer, \"Fast transformer decoding: One write-head is all you need,\" arXiv preprint \narXiv:1911.02150, 2019. \n[159] \nY.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov, \"Multimodal \ntransformer for unaligned multimodal language sequences,\" in Proc. Conf. Assoc. Comput. Linguist. \nMtg., 2019, vol. 2019: NIH Public Access, p. 6558.  \n[160] \nA. Dosovitskiy et al., \"An image is worth 16x16 words: Transformers for image recognition at \nscale,\" arXiv preprint arXiv:2010.11929, 2020. \n[161] \nW. Wang et al., \"Pyramid vision transformer: A versatile backbone for dense prediction without \nconvolutions,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2021, pp. 568-578.  \n[162] \nZ. Liu et al., \"Swin transformer: Hierarchical vision transformer using shifted windows,\" in Proc. \nIEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 10012-10022.  \n[163] \nL. Yuan et al., \"Tokens-to-token vit: Training vision transformers from scratch on imagenet,\" in \nProc. IEEE/CVF Int. Conf. Comput. Vis. , 2021, pp. 558-567.  \n[164] \nK. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, \"Transformer in transformer,\" Adv. Neural. \nInf. Process. Syst., vol. 34, pp. 15908-15919, 2021. \n[165] \nK. Han, J. Guo, Y. Tang, and Y. Wang, \"Pyramidtnt: Improved transformer-in-transformer \nbaselines with pyramid architecture,\" arXiv preprint arXiv:2201.00978, 2022. \n[166] \nW. Fedus, B. Zoph, and N. Shazeer, \"Switch transformers: Scaling to trillion parameter models \nwith simple and efficient sparsity,\" J. Mach. Learn. Res. , vol. 23, no. 120, pp. 1-39, 2022. \n[167] \nZ. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \"A convnet for the 2020s,\" in \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11976-\n11986.  \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n52 \n \n[168] \nJ. Zhang et al., \"Eatformer: Improving vision transformer inspired by evolutionary algorithm,\" Int. \nJ. Comput. Vis., pp. 1-28, 2024, doi: 10.1007/s11263-024-02034-6. \n[169] \nN. Vithayathil Varghese and Q. H. Mahmoud, \"A survey of multi-task deep reinforcement \nlearning,\" Electron., vol. 9, no. 9, pp. 1363, 2020, doi: 10.3390/electronics9091363. \n[170] \nN. Le, V. S. Rathour, K. Yamazaki, K. Luu, and M. Savvides, \"Deep reinforcement learning in \ncomputer vision: a comprehensive survey,\" Artif. Intell. Rev., pp. 1-87, 2022, doi: 10.1007/s10462-021-\n10061-9. \n[171] \nM. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John \nWiley & Sons, 2014. \n[172] \nZ. Zhang, D. Zhang, and R. C. Qiu, \"Deep reinforcement learning for power system applications: \nAn overview,\" CSEE J. Power Energy Syst., vol. 6, no. 1, pp. 213-225, 2019, doi: \n10.17775/CSEEJPES.2019.00920. \n[173] \nS. E. Li, \"Deep reinforcement learning,\" in Reinforcement learning for sequential decision and \noptimal control: Springer, 2023, pp. 365-402. \n[174] \nV. Mnih et al., \"Human-level control through deep reinforcement learning,\" NATURE, vol. 518, \nno. 7540, pp. 529-533, 2015, doi: 10.1038/nature14236. \n[175] \nH. Van Hasselt, A. Guez, and D. Silver, \"Deep reinforcement learning with double q-learning,\" in \nProc. AAAI Conf. Artif. Intell., 2016, vol. 30, no. 1, doi: 10.1609/aaai.v30i1.10295.  \n[176] \nZ. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas, \"Dueling network \narchitectures for deep reinforcement learning,\" in Int. Conf.  Mach. Learn., 2016: PMLR, pp. 1995-\n2003.  \n[177] \nR. Coulom, \"Efficient selectivity and backup operators in Monte-Carlo tree search,\" in Comput. \nGam.: 5th Int. Conf., Turin, Italy, 2007: Springer, pp. 72-83.  \n[178] \nN. Justesen, P. Bontrager, J. Togelius, and S. Risi, \"Deep learning for video game playing,\" IEEE \nTrans. Games., vol. 12, no. 1, pp. 1-20, 2019, doi: 10.1109/TG.2019.2896986. \n[179] \nK. Souchleris, G. K. Sidiropoulos, and G. A. Papakostas, \"Reinforcement learning in game \nindustry—Review, prospects and challenges,\" Appl. Sci., vol. 13, no. 4, pp. 2443, 2023, doi: \n10.3390/app13042443. \n[180] \nS. Gu, E. Holly, T. Lillicrap, and S. Levine, \"Deep reinforcement learning for robotic manipulation \nwith asynchronous off-policy updates,\" in 2017 IEEE Int. Conf. robot. autom. (ICRA), 2017: IEEE, pp. \n3389-3396.  \n[181] \nD. Han, B. Mulyana, V. Stankovic, and S. Cheng, \"A survey on deep reinforcement learning \nalgorithms for robotic manipulation,\" Sens., vol. 23, no. 7, pp. 3762, 2023, doi: 10.3390/s23073762. \n[182] \nK. M. Lee, H. Myeong, and G. Song, \"SeedNet: Automatic Seed Generation with Deep \nReinforcement Learning for Robust Interactive Segmentation,\" in IEEE/CVF Conf. Comput. Vis. \nPattern. Recognit. (CVPR), Salt Lake City, UT, USA, 18-23 June 2018: IEEE Computer Society, pp. \n1760-1768, doi: 10.1109/cvpr.2018.00189.  \n[183] \nH. Allioui et al., \"A multi-agent deep reinforcement learning approach for enhancement of \nCOVID-19 CT image segmentation,\" J. Pers. Med., vol. 12, no. 2, pp. 309, 2022, doi: \n10.3390/jpm12020309. \nF. M. Shiri et al.    \n53 \n \n[184] \nF. Sahba, \"Deep reinforcement learning for object segmentation in video sequences,\" in 2016 Int. \nConf. Comput. Sci. Comput. Intell. (CSCI), Las Vegas, NV, USA, 15-17 Dec 2016: IEEE, pp. 857-860, \ndoi: 10.1109/CSCI.2016.0166.  \n[185] \nH. Liu et al., \"Learning to identify critical states for reinforcement learning from videos,\" in Proc. \nIEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2023, pp. 1955-1965.  \n[186] \nA. Shojaeighadikolaei, A. Ghasemi, A. G. Bardas, R. Ahmadi, and M. Hashemi, \"Weather-Aware \nData-Driven Microgrid Energy Management Using Deep Reinforcement Learning,\" in 2021 North. \nAmerican. Power. Symp. (NAPS), College Station, TX, USA, 14-16 Nov 2021: IEEE, pp. 1-6, doi: \n10.1109/NAPS52732.2021.9654550.  \n[187] \nB. Zhang, W. Hu, A. M. Ghias, X. Xu, and Z. Chen, \"Multi-agent deep reinforcement learning \nbased distributed control architecture for interconnected multi-energy microgrid energy management \nand \noptimization,\" \nEnergy \nConv. \nManag., \nvol. \n277, \npp. \n116647, \n2023, \ndoi: \n10.1016/j.enconman.2022.116647. \n[188] \nM. Long, H. Zhu, J. Wang, and M. I. Jordan, \"Deep transfer learning with joint adaptation \nnetworks,\" in Int. Conf. Mach. Learn., 2017: PMLR, pp. 2208-2217.  \n[189] \nC. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, \"A survey on deep transfer learning,\" in \nArtif. Neural NET. Mach. Learn. ICANN 2018: 27th Int. Conf. Artif. Neural NET., Rhodes, Greece, \nOctober 4-7 2018: Springer, pp. 270-279, doi: 10.1007/978-3-030-01424-7_27.  \n[190] \nF. Zhuang et al., \"A comprehensive survey on transfer learning,\" P IEEE, vol. 109, no. 1, pp. 43-\n76, 2020. \n[191] \nM. K. Rusia and D. K. Singh, \"A Color-Texture-Based Deep Neural Network Technique to Detect \nFace Spoofing Attacks,\" Cybern. Inf. Technol., vol. 22, no. 3, pp. 127-145, 2022, doi: 10.2478/cait-\n2022-0032. \n[192] \nY. Yao and G. Doretto, \"Boosting for transfer learning with multiple sources,\" in 2010 IEEE \nComput. Conf. Comput. socy. Vis. Pattern. Recognit., San Francisco, CA, USA, 13-18 June 2010: IEEE, \npp. 1855-1862, doi: 10.1109/CVPR.2010.5539857.  \n[193] \nD. Pardoe and P. Stone, \"Boosting for regression transfer,\" in Proc. 27th Int. Conf.  Mach. Learn., \n2010, pp. 863-870.  \n[194] \nE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, \"Deep domain confusion: Maximizing \nfor domain invariance,\" arXiv preprint arXiv:1412.3474, 2014. \n[195] \nM. Long, Y. Cao, J. Wang, and M. Jordan, \"Learning transferable features with deep adaptation \nnetworks,\" in Int. Conf. Mach. Learn., 2015: PMLR, pp. 97-105.  \n[196] \nM. Iman, H. R. Arabnia, and K. Rasheed, \"A review of deep transfer learning and recent \nadvancements,\" Technol., vol. 11, no. 2, pp. 40, 2023, doi: 10.3390/technologies11020040. \n[197] \nA. A. Rusu et al., \"Progressive neural networks,\" arXiv preprint arXiv:1606.04671, 2016. \n[198] \nY. Guo, J. Zhang, B. Sun, and Y. Wang, \"Adversarial Deep Transfer Learning in Fault Diagnosis: \nProgress, Challenges, and Future Prospects,\" Sens., vol. 23, no. 16, pp. 7263, 2023, doi: \n10.3390/s23167263. \n[199] \nY. Gulzar, \"Fruit image classification model based on MobileNetV2 with deep transfer learning \ntechnique,\" Sustain., vol. 15, no. 3, pp. 1906, 2023, doi: 10.3390/su15031906. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n54 \n \n[200] \nN. Kumar, M. Gupta, D. Gupta, and S. Tiwari, \"Novel deep transfer learning model for COVID-\n19 patient detection using X-ray chest images,\" J. Ambient Intell. Humaniz. Comput., vol. 14, no. 1, pp. \n469-478, 2023, doi: 10.1007/s12652-021-03306-6. \n[201] \nH. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, and F. Bensaali, \"Deep transfer learning for \nautomatic speech recognition: Towards better generalization,\" Knowl.-Based Syst., vol. 277, pp. 110851, \n2023, doi: 10.1016/j.knosys.2023.110851. \n[202] \nL. Yuan, T. Wang, G. Ferraro, H. Suominen, and M.-A. Rizoiu, \"Transfer learning for hate speech \ndetection in social media,\" Journal of Computational Social Science, vol. 6, no. 2, pp. 1081-1101, 2023. \n[203] \nA. Ray, M. H. Kolekar, R. Balasubramanian, and A. Hafiane, \"Transfer learning enhanced vision-\nbased human activity recognition: A decade-long analysis,\" Int. J. Inf. Manag. Data Insights. , vol. 3, \nno. 1, pp. 100142, 2023, doi: 10.1016/j.jjimei.2022.100142. \n[204] \nT. Kujani and V. D. Kumar, \"Head movements for behavior recognition from real time video based \non deep learning ConvNet transfer learning,\" J. Ambient Intell. Humaniz. Comput., vol. 14, no. 6, pp. \n7047-7061, 2023, doi: 10.1007/s12652-021-03558-2. \n[205] \nA. Maity, A. Pathak, and G. Saha, \"Transfer learning based heart valve disease classification from \nPhonocardiogram signal,\" Biomed. Signal Process. Control. , vol. 85, pp. 104805, 2023, doi: \n10.1016/j.bspc.2023.104805. \n[206] \nK. Rezaee, S. Savarkar, X. Yu, and J. Zhang, \"A hybrid deep transfer learning-based approach for \nParkinson's disease classification in surface electromyography signals,\" Biomed. Signal Process. \nControl., vol. 71, pp. 103161, 2022, doi: 10.1016/j.bspc.2021.103161. \n[207] \nB. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, \"Learning transferable architectures for scalable \nimage recognition,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern. Recognit., 2018, pp. 8697-8710.  \n[208] \nY. Zhang et al., \"Deep learning in food category recognition,\" Inf. Fusion., vol. 98, pp. 101859, \n2023, doi: 10.1016/j.inffus.2023.101859. \n[209] \nE. Ramanujam and T. Perumal, \"MLMO-HSM: Multi-label Multi-output Hybrid Sequential \nModel for multi-resident smart home activity recognition,\" J. Ambient Intell. Humaniz. Comput., vol. \n14, no. 3, pp. 2313-2325, 2023, doi: 10.1007/s12652-022-04487-4. \n[210] \nM. Ren, X. Liu, Z. Yang, J. Zhang, Y. Guo, and Y. Jia, \"A novel forecasting based scheduling \nmethod for household energy management system based on deep reinforcement learning,\" Sustain. \nCities Soc., vol. 76, pp. 103207, 2022, doi: 10.1016/j.scs.2021.103207. \n[211] \nS. M. Abdullah et al., \"Optimizing traffic flow in smart cities: Soft GRU-based recurrent neural \nnetworks for enhanced congestion prediction using deep learning,\" Sustain., vol. 15, no. 7, pp. 5949, \n2023, doi: 10.3390/su15075949. \n[212] \nM. I. B. Ahmed et al., \"Deep learning approach to recyclable products classification: Towards \nsustainable waste management,\" Sustain., vol. 15, no. 14, pp. 11138, 2023, doi: 10.3390/su151411138. \n[213] \nC. Zeng, C. Ma, K. Wang, and Z. Cui, \"Parking occupancy prediction method based on multi \nfactors and stacked GRU-LSTM,\" IEEE Access., vol. 10, pp. 47361-47370, 2022, doi: \n10.1109/ACCESS.2022.3171330. \n[214] \nN. K. Mehta, S. S. Prasad, S. Saurav, R. Saini, and S. Singh, \"Three-dimensional DenseNet self-\nattention neural network for automatic detection of student’s engagement,\" Appl. Intell., vol. 52, no. 12, \npp. 13803-13823, 2022, doi: 10.1007/s10489-022-03200-4. \nF. M. Shiri et al.    \n55 \n \n[215] \nA. K. Shukla, A. Shukla, and R. Singh, \"Automatic attendance system based on CNN–LSTM and \nface recognition,\" Int. J. Inf. Technol., vol. 16, no. 3, pp. 1293-1301, 2024, doi: 10.1007/s41870-023-\n01495-1. \n[216] \nB. Rajalakshmi, V. K. Dandu, S. L. Tallapalli, and H. Karanwal, \"ACE: Automated Exam Control \nand E-Proctoring System Using Deep Face Recognition,\" in 2023 Int. Conf. Circuit. Power. Comput. \nTechnol. \n(ICCPCT), \nKollam, \nIndia, \n10-11 \nAug \n2023: \nIEEE, \npp. \n301-306, \ndoi: \n10.1109/ICCPCT58313.2023.10245126.  \n[217] \nI. Pacal, \"MaxCerVixT: A novel lightweight vision transformer-based Approach for precise \ncervical \ncancer \ndetection,\" \nKnowl.-Based \nSyst. \n, \nvol. \n289, \npp. \n111482, \n2024, \ndoi: \n10.1016/j.knosys.2024.111482. \n[218] \nM. M. Rana et al., \"A robust and clinically applicable deep learning model for early detection of \nAlzheimer's,\" IET Image Process., vol. 17, no. 14, pp. 3959-3975, 2023, doi: 10.1049/ipr2.12910. \n[219] \nS. Vimal, Y. H. Robinson, S. Kadry, H. V. Long, and Y. Nam, \"IoT based smart health monitoring \nwith CNN using edge computing,\" J. Internet Technol., vol. 22, no. 1, pp. 173-185, 2021, doi: \n10.3966/160792642021012201017. \n[220] \nT. S. Johnson et al., \"Diagnostic Evidence GAuge of Single cells (DEGAS): a flexible deep \ntransfer learning framework for prioritizing cells in relation to disease,\" Genome Med., vol. 14, no. 1, \npp. 11, 2022, doi: 10.1186/s13073-022-01012-2. \n[221] \nW. Zheng, S. Lu, Z. Cai, R. Wang, L. Wang, and L. Yin, \"PAL-BERT: an improved question \nanswering model,\" Comput. Model. engr. Sci., pp. 1-10, 2023, doi: 10.32604/cmes.2023.046692. \n[222] \nF. Wang et al., \"TEDT: transformer-based encoding–decoding translation network for multimodal \nsentiment analysis,\" Cogn. Comput., vol. 15, no. 1, pp. 289-303, 2023, doi: 10.1007/s12559-022-10073-\n9. \n[223] \nM. Nafees Muneera and P. Sriramya, \"An enhanced optimized abstractive text summarization \ntraditional approach employing multi-layered attentional stacked LSTM with the attention RNN,\" in \nComput. Vis. Mach. Intell. Paradigm., 2023: Springer, pp. 303-318, doi: 10.1007/978-981-19-7169-\n3_28.  \n[224] \nM. A. Uddin, M. S. Uddin Chowdury, M. U. Khandaker, N. Tamam, and A. Sulieman, \"The \nEfficacy of Deep Learning-Based Mixed Model for Speech Emotion Recognition,\" Comput. Mater. \nContin., vol. 74, no. 1, 2023, doi: 10.32604/cmc.2023.031177. \n[225] \nM. De Silva and D. Brown, \"Multispectral Plant Disease Detection with Vision Transformer–\nConvolutional Neural Network Hybrid Approaches,\" Sens., vol. 23, no. 20, pp. 8531, 2023, doi: \n10.3390/s23208531. \n[226] \nT. Akilan and K. Baalamurugan, \"Automated weather forecasting and field monitoring using \nGRU-CNN model along with IoT to support precision agriculture,\" Expert Syst. Appl, vol. 249, pp. \n123468, 2024, doi: 10.1016/j.eswa.2024.123468. \n[227] \nR. Benameur, A. Dahane, B. Kechar, and A. E. H. Benyamina, \"An Innovative Smart and \nSustainable Low-Cost Irrigation System for Anomaly Detection Using Deep Learning,\" Sens., vol. 24, \nno. 4, pp. 1162, 2024, doi: 10.3390/s24041162. \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n56 \n \n[228] \nM. Hosseinpour-Zarnaq, M. Omid, F. Sarmadian, and H. Ghasemi-Mobtaker, \"A CNN model for \npredicting soil properties using VIS–NIR spectral data,\" Environ. Earth. Sci., vol. 82, no. 16, pp. 382, \n2023, doi: 10.1007/s12665-023-11073-0. \n[229] \nM. Shakeel, K. Itoyama, K. Nishida, and K. Nakadai, \"Detecting earthquakes: a novel deep \nlearning-based approach for effective disaster response,\" Appl. Intell., vol. 51, no. 11, pp. 8305-8315, \n2021, doi: 10.1007/s10489-021-02285-7. \n[230] \nY. Zhang, Z. Zhou, J. Van Griensven Thé, S. X. Yang, and B. Gharabaghi, \"Flood Forecasting \nUsing Hybrid LSTM and GRU Models with Lag Time Preprocessing,\" Water, vol. 15, no. 22, pp. 3982, \n2023, doi: 10.3390/w15223982. \n[231] \nH. Xu and H. Wu, \"Accurate tsunami wave prediction using long short-term memory based neural \nnetworks,\" Ocean Model., vol. 186, pp. 102259, 2023, doi: 10.1016/j.ocemod.2023.102259. \n[232] \nJ. Yao, B. Zhang, C. Li, D. Hong, and J. Chanussot, \"Extended vision transformer (ExViT) for \nland use and land cover classification: A multimodal deep learning framework,\" IEEE Trans. Geosci. \nRemote Sens., vol. 61, pp. 1-15, 2023, doi: 10.1109/TGRS.2023.3284671. \n[233] \nA. Y. Cho, S.-e. Park, D.-j. Kim, J. Kim, C. Li, and J. Song, \"Burned area mapping using \nUnitemporal Planetscope imagery with a deep learning based approach,\" IEEE J. Sel. Top. Appl. Earth \nObs. Remote Sens., vol. 16, pp. 242-253, 2022, doi: 10.1109/JSTARS.2022.3225070. \n[234] \nM. Alshehri, A. Ouadou, and G. J. Scott, \"Deep Transformer-based Network Deforestation \nDetection in the Brazilian Amazon Using Sentinel-2 Imagery,\" IEEE Geosci. Remote Sens. Lett., 2024, \ndoi: 10.1109/LGRS.2024.3355104. \n[235] \nV. Hnamte and J. Hussain, \"DCNNBiLSTM: An efficient hybrid deep learning-based intrusion \ndetection system,\" Telemat. Inform. Rep., vol. 10, pp. 100053, 2023, doi: 10.1016/j.teler.2023.100053. \n[236] \nE. S. Alomari et al., \"Malware detection using deep learning and correlation-based feature \nselection,\" Symmetry., vol. 15, no. 1, pp. 123, 2023, doi: 10.3390/sym15010123. \n[237] \nZ. Alshingiti, R. Alaqel, J. Al-Muhtadi, Q. E. U. Haq, K. Saleem, and M. H. Faheem, \"A deep \nlearning-based phishing detection system using CNN, LSTM, and LSTM-CNN,\" Electron., vol. 12, no. \n1, pp. 232, 2023, doi: 10.3390/electronics12010232. \n[238] \nH. Fanai and H. Abbasimehr, \"A novel combined approach based on deep Autoencoder and deep \nclassifiers for credit card fraud detection,\" Expert Syst. Appl., vol. 217, pp. 119562, 2023, doi: \n10.1016/j.eswa.2023.119562. \n[239] \nR. A. Joshi and N. Sambre, \"Personalized CNN Architecture for Advanced Multi-Modal Biometric \nAuthentication,\" in 2024 Int. Conf. Invent. Comput. Technol. (ICICT), Lalitpur, Nepal, 24-26 April 2024: \nIEEE, pp. 890-894, doi: 10.1109/ICICT60155.2024.10544987.  \n[240] \nJ. Sohafi-Bonab, M. H. Aghdam, and K. Majidzadeh, \"DCARS: Deep context-aware \nrecommendation system based on session latent context,\" Appl. Soft Comput., vol. 143, pp. 110416, \n2023, doi: 10.1016/j.asoc.2023.110416. \n[241] \nJ. Duan, P.-F. Zhang, R. Qiu, and Z. Huang, \"Long short-term enhanced memory for sequential \nrecommendation,\" World. Wide. Web., vol. 26, no. 2, pp. 561-583, 2023, doi: 10.1007/s11280-022-\n01056-9. \nF. M. Shiri et al.    \n57 \n \n[242] \nP. Mondal, D. Chakder, S. Raj, S. Saha, and N. Onoe, \"Graph convolutional neural network for \nmultimodal movie recommendation,\" in Proc. 38th ACM/SIGAPP Symp. Appl. Comput., 2023, pp. \n1633-1640, doi: 10.1145/3555776.3577853.  \n[243] \nZ. Liu, \"Prediction Model of E-commerce Users' Purchase Behavior Based on Deep Learning,\" \nFront. Bus. Econ. Manag., vol. 15, no. 2, pp. 147-149, 2024, doi: 10.54097/p22ags78. \n[244] \nS. Deng, R. Li, Y. Jin, and H. He, \"CNN-based feature cross and classifier for loan default \nprediction,\" in 2020 Int. Conf. Image. video. Process.  Artif. Intell., 2020, vol. 11584: SPIE, pp. 368-\n373.  \n[245] \nC. Han and X. Fu, \"Challenge and opportunity: deep learning-based stock price prediction by using \nBi-directional LSTM model,\" Front. Bus. Econ. Manag., vol. 8, no. 2, pp. 51-54, 2023, doi: \n10.54097/fbem.v8i2.6616. \n[246] \nY. Cao, C. Li, Y. Peng, and H. Ru, \"MCS-YOLO: A multiscale object detection method for \nautonomous driving road environment recognition,\" IEEE Access., vol. 11, pp. 22342-22354, 2023, doi: \n10.1109/ACCESS.2023.3252021. \n[247] \nD. K. Jain, X. Zhao, G. González-Almagro, C. Gan, and K. Kotecha, \"Multimodal pedestrian \ndetection using metaheuristics with deep convolutional neural network in crowded scenes,\" Inf. Fusion., \nvol. 95, pp. 401-414, 2023, doi: 10.1016/j.inffus.2023.02.014. \n[248] \nS. Sindhu and M. Saravanan, \"An optimised extreme learning machine (OELM) for simultaneous \nlocalisation and mapping in autonomous vehicles,\" Int. J. Syst. Syst. Eng., vol. 13, no. 2, pp. 140-159, \n2023, doi: 10.1504/IJSSE.2023.131231. \n[249] \nG. Singal, H. Singhal, R. Kushwaha, V. Veeramsetty, T. Badal, and S. Lamba, \"RoadWay: lane \ndetection for autonomous driving vehicles via deep learning,\" Multimed. Tools Appl., vol. 82, no. 4, pp. \n4965-4978, 2023, doi: 10.1007/s11042-022-12171-0. \n[250] \nH. Shang, C. Sun, J. Liu, X. Chen, and R. Yan, \"Defect-aware transformer network for intelligent \nvisual surface defect detection,\" Adv. Eng. Inform., vol. 55, pp. 101882, 2023, doi: \n10.1016/j.aei.2023.101882. \n[251] \nT. Zonta, C. A. Da Costa, F. A. Zeiser, G. de Oliveira Ramos, R. Kunst, and R. da Rosa Righi, \"A \npredictive maintenance model for optimizing production schedule using deep neural networks,\" J. \nManuf. Syst., vol. 62, pp. 450-462, 2022, doi: 10.1016/j.jmsy.2021.12.013. \n[252] \nZ. He, K.-P. Tran, S. Thomassey, X. Zeng, J. Xu, and C. Yi, \"A deep reinforcement learning based \nmulti-criteria decision support system for optimizing textile chemical process,\" Comput. Ind., vol. 125, \npp. 103373, 2021, doi: 10.1016/j.compind.2020.103373. \n[253] \nM. Pacella and G. Papadia, \"Evaluation of deep learning with long short-term memory networks \nfor time series forecasting in supply chain management,\" PROC CIRP, vol. 99, pp. 604-609, 2021, doi: \n10.1016/j.procir.2021.03.081. \n[254] \nP. Shukla, H. Kumar, and G. C. Nandi, \"Robotic grasp manipulation using evolutionary computing \nand deep reinforcement learning,\" Intell. Serv. Robot., vol. 14, no. 1, pp. 61-77, 2021, doi: \n10.1007/s11370-020-00342-7. \n[255] \nK. Kamali, I. A. Bonev, and C. Desrosiers, \"Real-time motion planning for robotic teleoperation \nusing dynamic-goal deep reinforcement learning,\" in 2020 17th Conf. Comput. Robot. Vis. (CRV), 13-\n15 May 2020: IEEE, pp. 182-189, doi: 10.1109/CRV50864.2020.00032.  \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n58 \n \n[256] \nJ. Zhang, H. Liu, Q. Chang, L. Wang, and R. X. Gao, \"Recurrent neural network for motion \ntrajectory prediction in human-robot collaborative assembly,\" CIRP annals, vol. 69, no. 1, pp. 9-12, \n2020, doi: 10.1016/j.cirp.2020.04.077. \n[257] \nB. K. Iwana and S. Uchida, \"An empirical survey of data augmentation for time series \nclassification with neural networks,\" PLOS ONE, vol. 16, no. 7, pp. e0254841, 2021, doi: \n10.1371/journal.pone.0254841. \n[258] \nC. Khosla and B. S. Saini, \"Enhancing performance of deep learning models with different data \naugmentation techniques: A survey,\" in 2020 Int. Conf. Intell. engr. Mgmt. (ICIEM), London, UK, 17-\n19 June 2020: IEEE, pp. 79-85, doi: 10.1109/ICIEM48762.2020.9160048.  \n[259] \nM. Paschali, W. Simson, A. G. Roy, R. Göbl, C. Wachinger, and N. Navab, \"Manifold exploring \ndata augmentation with geometric transformations for increased performance and robustness,\" in Inf. \nProcess. Medical. Image.: 26th Int. Conf., IPMI 2019, Hong Kong, China, June 2–7 2019: Springer, pp. \n517-529.  \n[260] \nH. Guo, Y. Mao, and R. Zhang, \"Augmenting data with mixup for sentence classification: An \nempirical study,\" arXiv preprint arXiv:1905.08941, 2019. \n[261] \nO. O. Abayomi-Alli, R. Damaševičius, A. Qazi, M. Adedoyin-Olowe, and S. Misra, \"Data \naugmentation and deep learning methods in sound classification: A systematic review,\" Electro., vol. \n11, no. 22, pp. 3795, 2022, doi: 10.3390/electronics11223795. \n[262] \nT.-H. Cheung and D.-Y. Yeung, \"Modals: Modality-agnostic automated data augmentation in the \nlatent space,\" in Int. Conf. Learn. Represen., 2020.  \n[263] \nC. Shorten, T. M. Khoshgoftaar, and B. Furht, \"Text data augmentation for deep learning,\" J. Big \nData, vol. 8, no. 1, pp. 101, 2021, doi: 10.1186/s40537-021-00492-0. \n[264] \nF. Wang, H. Wang, H. Wang, G. Li, and G. Situ, \"Learning from simulation: An end-to-end deep-\nlearning approach for computational ghost imaging,\" Opt. Express, vol. 27, no. 18, pp. 25560-25572, \n2019, doi: 10.1364/OE.27.025560. \n[265] \nK. Ghosh, C. Bellinger, R. Corizzo, P. Branco, B. Krawczyk, and N. Japkowicz, \"The class \nimbalance problem in deep learning,\" Mach. Learn., vol. 113, no. 7, pp. 4845-4901, 2024, doi: \n10.1007/s10994-022-06268-8. \n[266] \nD. Singh, E. Merdivan, J. Kropf, and A. Holzinger, \"Class imbalance in multi-resident activity \nrecognition: an evaluative study on explainability of deep learning approaches,\" Univers. Access. Inf. \nSoc., pp. 1-19, 2024, doi: 10.1007/s10209-024-01123-0. \n[267] \nA. S. Tarawneh, A. B. Hassanat, G. A. Altarawneh, and A. Almuhaimeed, \"Stop oversampling for \nclass imbalance learning: A review,\" IEEE ACCESS, vol. 10, pp. 47643-47660, 2022, doi: \n10.1109/ACCESS.2022.3169512. \n[268] \nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, \"SMOTE: synthetic minority \nover-sampling technique,\" J. Artif. Intell. Res., vol. 16, pp. 321-357, 2002, doi: 10.1613/jair.953. \n[269] \nH. Han, W.-Y. Wang, and B.-H. Mao, \"Borderline-SMOTE: a new over-sampling method in \nimbalanced data sets learning,\" in Int. Conf. Intell. Comput., 2005: Springer, pp. 878-887.  \n[270] \nH. He, Y. Bai, E. A. Garcia, and S. Li, \"ADASYN: Adaptive synthetic sampling approach for \nimbalanced learning,\" in 2008 Int. Jt. Conf. Neural. Netw., 2008: IEEE, pp. 1322-1328.  \nF. M. Shiri et al.    \n59 \n \n[271] \nY. Tang, Y.-Q. Zhang, N. V. Chawla, and S. Krasser, \"SVMs modeling for highly imbalanced \nclassification,\" IEEE Trans. Syst. Man. Cybern., Part B (Cybernetics), vol. 39, no. 1, pp. 281-288, 2008, \ndoi: 10.1109/TSMCB.2008.2002909. \n[272] \nS. Barua, M. M. Islam, X. Yao, and K. Murase, \"MWMOTE--majority weighted minority \noversampling technique for imbalanced data set learning,\" IEEE Trans. Knowl. Data Eng., vol. 26, no. \n2, pp. 405-425, 2012, doi: 10.2478/cait-2022-0035. \n[273] \nC. Bellinger, S. Sharma, N. Japkowicz, and O. R. Zaïane, \"Framework for extreme imbalance \nclassification: SWIM—sampling with the majority class,\" Knowl. Inf. Syst., vol. 62, pp. 841-866, 2020, \ndoi: 10.1007/s10115-019-01380-z. \n[274] \nR. Das, S. K. Biswas, D. Devi, and B. Sarma, \"An oversampling technique by integrating reverse \nnearest neighbor in SMOTE: Reverse-SMOTE,\" in 2020 Int. Conf. Smart. Electron. Commun. \n(ICOSEC), 2020: IEEE, pp. 1239-1244.  \n[275] \nC. Liu et al., \"Constrained oversampling: An oversampling approach to reduce noise generation \nin imbalanced datasets with class overlapping,\" IEEE ACCESS, vol. 10, pp. 91452-91465, 2020, doi: \n10.1109/ACCESS.2020.3018911. \n[276] \nA. S. Tarawneh, A. B. Hassanat, K. Almohammadi, D. Chetverikov, and C. Bellinger, \"Smotefuna: \nSynthetic minority over-sampling technique based on furthest neighbour algorithm,\" IEEE ACCESS, \nvol. 8, pp. 59069-59082, 2020, doi: 10.1109/ACCESS.2020.2983003. \n[277] \nX.-Y. Liu, J. Wu, and Z.-H. Zhou, \"Exploratory undersampling for class-imbalance learning,\" \nIEEE Trans. Syst. Man. Cybern., Part B (Cybernetics), vol. 39, no. 2, pp. 539-550, 2008, doi: \n10.1109/TSMCB.2008.2007853. \n[278] \nM. A. Tahir, J. Kittler, and F. Yan, \"Inverse random under sampling for class imbalance problem \nand its application to multi-label classification,\" Pattern. Recognit., vol. 45, no. 10, pp. 3738-3750, 2012, \ndoi: 10.1016/j.patcog.2012.03.014. \n[279] \nV. Babar and R. Ade, \"A novel approach for handling imbalanced data in medical diagnosis using \nundersampling technique,\" Commun. Appl. Electron., vol. 5, no. 7, pp. 36-42, 2016. \n[280] \nZ. H. Zhou and X. Y. Liu, \"On multi‐class cost‐sensitive learning,\" Comput. Intell., vol. 26, no. \n3, pp. 232-257, 2010, doi: 10.1111/j.1467-8640.2010.00358.x. \n[281] \nC. X. Ling and V. S. Sheng, \"Cost-sensitive learning and the class imbalance problem,\" ency. \nMach. Learn., vol. 2011, pp. 231-235, 2008. \n[282] \nN. Seliya, A. Abdollah Zadeh, and T. M. Khoshgoftaar, \"A literature review on one-class \nclassification and its potential applications in big data,\" J. Big Data, vol. 8, pp. 1-31, 2021, doi: \n10.1186/s40537-021-00514-x. \n[283] \nV. S. Spelmen and R. Porkodi, \"A review on handling imbalanced data,\" in Int. Conf. Curr. Trend. \nToward. Converg. Technol. (ICCTCT), Coimbatore, India, 1-3 March 2018: IEEE, pp. 1-11, doi: \n10.1109/ICCTCT.2018.8551020.  \n[284] \nG. Zhang, C. Wang, B. Xu, and R. Grosse, \"Three mechanisms of weight decay regularization,\" \narXiv preprint arXiv:1810.12281, 2018. \n[285] \nC. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio, \"Batch normalized recurrent neural \nnetworks,\" in 2016 IEEE Int. Conf. Acoust. Speech. Signal. Process. (ICASSP), Shanghai, China, 20-25 \nMarch 2016: IEEE, pp. 2657-2661, doi: 10.1109/ICASSP.2016.7472159.  \nA Comprehensive Overview and Comparative Analysis on Deep Learning Models \n60 \n \n[286] \nS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing \ninternal covariate shift,\" in Int. Conf.  Mach. Learn., 2015: pmlr, pp. 448-456.  \n[287] \nG. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton, \"Regularizing neural networks by \npenalizing confident output distributions,\" arXiv preprint arXiv:1701.06548, 2017. \n[288] \nG. E. Dahl, T. N. Sainath, and G. E. Hinton, \"Improving deep neural networks for LVCSR using \nrectified linear units and dropout,\" in IEEE Int. Conf. Acoust. Speech. Signal. Process., 2013: IEEE, pp. \n8609-8613.  \n[289] \nX. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural \nnetworks,\" in Proc. 13  Int. Conf. Artif. Intell. Stats., 2010: JMLR Workshop and Conference \nProceedings, pp. 249-256.  \n[290] \nG. Srivastava, S. Vashisth, I. Dhall, and S. Saraswat, \"Behavior analysis of a deep feedforward \nneural network by varying the weight initialization methods,\" in Smart Innov. Commun. Comput. Sci.: \nProc. ICSICCS 2020, 2021: Springer, pp. 167-175, doi: 10.1007/978-981-15-5345-5_15.  \n[291] \nJ. Serra, D. Suris, M. Miron, and A. Karatzoglou, \"Overcoming catastrophic forgetting with hard \nattention to the task,\" in Int. Conf. Mach. Learn., 2018: PMLR, pp. 4548-4557.  \n[292] \nJ. Kirkpatrick et al., \"Overcoming catastrophic forgetting in neural networks,\" Proc. Natl. Acad. \nSci., vol. 114, no. 13, pp. 3521-3526, 2017, doi: 10.1073/pnas.1611835114. \n[293] \nS.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, \"Overcoming catastrophic forgetting by \nincremental moment matching,\" Adv. Neural Inf. Process. Syst., vol. 30, 2017. \n[294] \nS.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \"icarl: Incremental classifier and \nrepresentation learning,\" in Proc. IEEEConf. Comput. Vis. Pattern. Recognit., 2017, pp. 2001-2010.  \n[295] \nA. D'Amour et al., \"Underspecification presents challenges for credibility in modern machine \nlearning,\" J. Mach. Learn. Res., vol. 23, no. 226, pp. 1-61, 2022. \n[296] \nD. Teney, M. Peyrard, and E. Abbasnejad, \"Predicting is not understanding: Recognizing and \naddressing underspecification in machine learning,\" in Europ. Conf. Comput. Vis., 2022: Springer, pp. \n458-476.  \n[297] \nN. Chotisarn, W. Pimanmassuriya, and S. Gulyanon, \"Deep learning visualization for \nunderspecification analysis in product design matching model development,\" IEEE ACCESS, vol. 9, pp. \n108049-108061, 2021, doi: 10.1109/ACCESS.2021.3102174. \n[298] \nA. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, \"Learning word vectors for \nsentiment analysis,\" in Proc. 49th Annual. Meeting. Assoc. Comput. Linguist.: Hum. langu. Tech., \nPortland Oregon, June 19 - 2 2011, pp. 142-150.  \n[299] \nH. Alemdar, H. Ertan, O. D. Incel, and C. Ersoy, \"ARAS human activity datasets in multiple homes \nwith multiple residents,\" in 2013 7th Int. Conf. Perv. Comput. Technol. Healthcare. Workshop., 2013: \nIEEE, pp. 232-235.  \n[300] \nH. Mureşan and M. Oltean, \"Fruit recognition from images using deep learning,\" arXiv preprint \narXiv:1712.00580, 2017. \n[301] \nF. M. P. Shiri, T. Perumal, N. Mustapha, R. Mohamed, M. A. Ahmadon, and S. Yamaguchi, \"A \nSurvey on Multi-Resident Activity Recognition in Smart Environments,\" Evolution of Information, \nCommunication and Computing System, pp. 12-27, 2023. \nF. M. Shiri et al.    \n61 \n \n[302] \nX. Xiao, M. Yan, S. Basodi, C. Ji, and Y. Pan, \"Efficient hyperparameter optimization in deep \nlearning using a variable length genetic algorithm,\" arXiv preprint arXiv:2006.12703, 2020. \n[303] \nH. J. Escalante, M. Montes, and L. E. Sucar, \"Particle swarm model selection,\" J. Mach. Learn. \nRes., vol. 10, no. 2, pp. 405–440, 2009. \n[304] \nM. Parhizgar and F. M. Shiri, \"Solving quadratic assignment problem using water cycle \noptimization algorithm,\" International Journal of Intelligent Information Systems, vol. 3, no. 6-1, pp. \n75-79, 2014. \n[305] \nD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint \narXiv:1412.6980, 2014. \n[306] \nL. Bottou, \"Stochastic gradient descent tricks,\" in Neural Networks: Tricks of the Trade: Second \nEdition: Springer, 2012, pp. 421-436. \n[307] \nJ. Duchi, E. Hazan, and Y. Singer, \"Adaptive subgradient methods for online learning and \nstochastic optimization,\" J. Mach. Learn. Res., vol. 12, no. 7, pp. 2121–2159, 2011. \n[308] \nT. Dozat, \"Incorporating nesterov momentum into adam,\" in Proc. 4th Int. Conf. Learn. Represent. \n(ICLR) Workshop Track., San Juan, Puerto Rico, 2016, pp. 1-4.  \n[309] \nX. Chen et al., \"Symbolic discovery of optimization algorithms,\" Adv. Neural Inf. Process. Syst., \nvol. 36, 2024. \n[310] \nL. Alzubaidi et al., \"A survey on deep learning tools dealing with data scarcity: definitions, \nchallenges, solutions, tips, and applications,\" J. Big. Data., vol. 10, no. 1, pp. 46, 2023, doi: \n10.1186/s40537-023-00727-2. \n[311] \nI. Cong, S. Choi, and M. D. Lukin, \"Quantum convolutional neural networks,\" Nat. Phys, vol. 15, \nno. 12, pp. 1273-1278, 2019, doi: 10.1038/s41567-019-0648-8. \n[312] \nY. Takaki, K. Mitarai, M. Negoro, K. Fujii, and M. Kitagawa, \"Learning temporal data with a \nvariational quantum recurrent neural network,\" Phys. Rev. A, vol. 103, no. 5, pp. 052414, 2021, doi: \n10.1103/PhysRevA.103.052414. \n[313] \nS. Lloyd and C. Weedbrook, \"Quantum generative adversarial learning,\" Phys. Rev. Lett., vol. 121, \nno. 4, pp. 040502, 2018, doi: 10.1103/PhysRevLett.121.040502. \n[314] \nS. Garg and G. Ramakrishnan, \"Advances in quantum deep learning: An overview,\" arXiv preprint \narXiv:2005.04316, 2020. \n[315] \nF. Valdez and P. Melin, \"A review on quantum computing and deep learning algorithms and their \napplications,\" Soft Comput., vol. 27, no. 18, pp. 13217-13236, 2023, doi: 10.1007/s00500-022-07037-\n4. \n \n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-05-27",
  "updated": "2024-10-24"
}