{
  "id": "http://arxiv.org/abs/2305.18357v1",
  "title": "DeepSI: Interactive Deep Learning for Semantic Interaction",
  "authors": [
    "Yali Bian",
    "Chris North"
  ],
  "abstract": "In this paper, we design novel interactive deep learning methods to improve\nsemantic interactions in visual analytics applications. The ability of semantic\ninteraction to infer analysts' precise intents during sensemaking is dependent\non the quality of the underlying data representation. We propose the\n$\\text{DeepSI}_{\\text{finetune}}$ framework that integrates deep learning into\nthe human-in-the-loop interactive sensemaking pipeline, with two important\nproperties. First, deep learning extracts meaningful representations from raw\ndata, which improves semantic interaction inference. Second, semantic\ninteractions are exploited to fine-tune the deep learning representations,\nwhich then further improves semantic interaction inference. This feedback loop\nbetween human interaction and deep learning enables efficient learning of user-\nand task-specific representations. To evaluate the advantage of embedding the\ndeep learning within the semantic interaction loop, we compare\n$\\text{DeepSI}_{\\text{finetune}}$ against a state-of-the-art but more basic use\nof deep learning as only a feature extractor pre-processed outside of the\ninteractive loop. Results of two complementary studies, a human-centered\nqualitative case study and an algorithm-centered simulation-based quantitative\nexperiment, show that $\\text{DeepSI}_{\\text{finetune}}$ more accurately\ncaptures users' complex mental models with fewer interactions.",
  "text": "DeepSI: Interactive Deep Learning for Semantic Interaction\nYALI BIAN, Virginia Tech, United States\nCHRIS NORTH, Virginia Tech, United States\n 1 \n 2 \n 3 \nCancer\nKidney Disease\nNeurological Disorders\nSmoking Status\nFig. 1. Screenshots during the analysis of COVID-19 research articles about four risk factors (depicted in different colors) using our\nproposed model DeepSIfinetune: (1) the initial layout of all articles projected from pretrained BERT representations of the raw text\ndata; (2) the analyst performs semantic interactions to provide visual feedback regarding articles about different risk factors; these\ninteractions are then exploited to tune the underlying DL model BERT; (3) the resulting projection updated by the tuned BERT.\nIn this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The\nability of semantic interaction to infer analysts’ precise intents during sensemaking is dependent on the quality of the underlying\ndata representation. We propose the DeepSIfinetune framework that integrates deep learning into the human-in-the-loop interactive\nsensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which\nimproves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations,\nwhich then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables\nefficient learning of user- and task-specific representations. To evaluate the advantage of embedding the deep learning within the\nsemantic interaction loop, we compare DeepSIfinetune against a state-of-the-art but more basic use of deep learning as only a feature\nextractor pre-processed outside of the interactive loop. Results of two complementary studies, a human-centered qualitative case\nstudy and an algorithm-centered simulation-based quantitative experiment, show that DeepSIfinetune more accurately captures users’\ncomplex mental models with fewer interactions.\nCCS Concepts: • Human-centered computing →Interaction techniques; Visual analytics; • Computing methodologies →\nNatural language processing; Learning from demonstrations.\nAdditional Key Words and Phrases: Semantic Interaction, BERT, Visual Analytics, Interactive Deep Learning\nACM Reference Format:\nYali Bian and Chris North. 2021. DeepSI: Interactive Deep Learning for Semantic Interaction. In 26th International Conference on\nIntelligent User Interfaces (IUI ’21), April 14–17, 2021, College Station, TX, USA. ACM, New York, NY, USA, 17 pages. https://doi.org/10.\n1145/3397481.3450670\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\nManuscript submitted to ACM\narXiv:2305.18357v1  [cs.LG]  26 May 2023\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\n1\nINTRODUCTION\nSemantic interaction (SI) [20, 21] is an interaction methodology that is commonly utilized to enhance visual analytics (VA)\nsystems. SI-enabled systems let the analyst directly manipulate interactive projections of data [58]. The semantic meaning\nbehind these projection interactions is the similarity relationships the analyst wishes to find within the data during\nthe sensemaking process [49]. As shown in Fig. 1-2, the analyst drags 12 COVID-19 article points into four clusters to\nprovide the visual feedback of grouping articles based on their perceived relevant risk factors. With these intuitive and\nnatural interactions, the analyst can remain within the cognitive zone [25], thereby enhancing the analyst’s efficiency\nin performing analytic tasks [65]. In the system, an interactive dimensionality reduction (DR) component [54, 65] plays\na key role in capturing the analyst’s intent behind these interactions by learning a new projection layout (Fig. 1-3). To\ndetermine the analyst’s precise intent, increasingly powerful interactive DR models [65] have been proposed, from\nlinear [29, 30, 37] to non-linear models [35, 40], and from single-model to multi-model approaches [10, 18, 19, 66].\nHowever, the ability of semantic interaction to infer analysts’ precise intents during sensemaking is dependent on\nthe quality of the underlying data representation. Deep learning (DL) [36] is a state-of-the-art representation learning\nmethod [5], which can automatically extract abstract and useful hierarchical representations from raw data [6]. This\noffers the new opportunity to power SI in capturing the analyst’s intent. We denote the DL-enhanced SI system as\nDeepSI. Previous researches have shown that even the usage of the pretrained DL representations as fixed data features\nhave better performance than hand-crafted features in SI-enabled VA systems [7, 8]. We denote this straightforward\nDeepSI design with the basic use the pretrained DL as only a feature extractor in SI pipeline as DeepSIvanilla.\nIn this paper, we aim to further improve semantic interaction inference by fine-tuning the model to obtain user- and\ntask-specific representations from the pretrained DL model. Central to this design goal are two research questions:\n• How to exploit semantic interactions to accurately adapt the pretrained representations to current analytic tasks?\n• How to make efficient adaptations, so that a small number of semantic interactions are enough for analysts to express\ntheir intents?\nTo address these two questions, we propose a novel DeepSI framework, DeepSIfinetune, with the following two design\ngoals. First, we insert the interactive DL training into the bidirectional structure of the semantic interaction pipeline, so\nthat interactions trigger the DL adaptation. Thereby, new user- and task-specific representations are generated based\non semantic interactions provided by analysts during their sensemaking process. Second, we employ the fine-tuning\nbased DL adaption approach and the MDS-based interactive DR model to minimize the number of parameters that\nrequire training in the underlying model. Therefore, DeepSIfinetune can tune the DL model efficiently from the analyst’s\ninteractions without information loss. Specifically, we use the pretrained BERT [16], a state-of-the-art DL model for\nNLP tasks, as the DL model representative inside DeepSIfinetune for visual text analysis tasks.\nTo assess how well DeepSIfinetune addresses these questions by integrating DL into the semantic interaction loop, we\ncompare it with the well-evaluated baseline model DeepSIvanilla [7, 8], which uses DL outside of the interactive loop, in\ntwo complementary experiments: a human-centered qualitative case study about COVID-19 academic articles; and an\nalgorithm-centered simulation-based quantitative analysis of three commonly used text corpora: Standford Sentiment\nTreebank (SST), Vispubdata, and 20 Newsgroups. The results of both experiments show that DeepSIfinetune not only\ncaptures the analyst’s precise intent more accurately, but also requires fewer interactions from the analyst.\nSpecifically, we claim the following contributions:\n(1) The DeepSIfinetune framework that integrates DL into the human-in-the-loop iterative sensemaking pipeline to\nimprove semantic interaction inference.\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\n(2) Two complementary studies, a user-centered qualitative case study and an algorithm-centered simulation-based\nquantitative experiment, that measure the performance of our method and reveal improvements.\n2\nRELATED WORKS\nFour related components support our design: interactive DR models used in semantic interaction; basic knowledge of\nthe DL model BERT; pretrained DL model adaptation approaches; and other work about user-centered interactive DL.\nParameters\nUpdatable Component\nmodel prediction\nmodel updating\nAnalyst\n(Perceive)\nAnalyst\n(Interact)\nDR\nDR-1\nModel\nSpatialization\nHuman\nSpatialization\nInsights\nData\nData\nInteractive DR\nVisualization\nAnalyst\nFig. 2. SI pipeline showing the communication between the analyst and VA system, adapted from [21, 56]. The interactive DR\ncomponent is responsible for capturing the analyst’s intent from the human modified projection (denoted as human spatialization)\nand, consequently, updating the projection in response (denoted as model spatialization).\n2.1\nInteractive Dimensionality Reduction\nWhile using the human-in-the-loop sensemaking SI pipeline (Fig. 2), analysts gain insights from the model projec-\ntion (spatialization) and express their preferences by repositioning data points in the projection. It is the interactive DR\nmodel’s responsibility to learn new model parameters that capture the analyst’s intent behind the modified projection\nand, in response, use the learned parameters to update the projection. Therefore, increasingly powerful DR models\nhave been adapted in a semi-supervised manner to improve SI inference. VA frameworks V2PI [37] and BaVA [29]\nadapted linear DR models, including principal component analysis (PCA) [67] and weighted multidimensional scaling\n(WMDS) [55], to the bidirectional SI pipeline. To support more complex tasks and interactions, multiple models were\nchained together as a single interactive DR model, which is called multi-model SI [10, 19, 66]. Recently, to adapt more\npowerful but complex non-invertible DR algorithms, such as t-SNE [39] and UMAP [41], Zexplorer [40] used the\ninvertible neural encoder [22] to emulate these models as the interactive DR model in SI applications.\nSimilarly, DeepSIfinetune also aims to improve SI inference. However, DeepSIfinetune highlights the importance of\nfinding user- and task-specific data representations instead of more powerful DR. Therefore, we use the simple but\ncommonly used WMDS as the default DR [11, 17, 56, 57] and focus on extracting meaningful DL representations.\n2.2\nBERT\nBERT (Bidirectional Encoder Representations from Transformers) [16] is a DL language representation model. BERT is\nfirst pretrained on raw text data to learn general language representations. The pretrained BERT then can be easily\nadapted to downstream NLP tasks such as sentiment analysis and semantic textual similarity [13]. The adapted BERT\nmodel is able to provide task-specific representations and shows state-of-the-art performance in these downstream\ntasks. Technically, BERT is a Transformer encoder [62], containing a stack of transformer layers. The transformer\nlayer learns token-level representations. For input token sequences, the transformer layer learns a new vector for each\ntoken based on all other tokens, using the self-attention mechanism [3]. Through the stack of transfer layers, BERT can\nconvert a sequence of tokens into deep representations. In this paper, we use the pretrained BERT model as the default\nDL model in the DeepSI pipeline to provide text representations for visual text analytic tasks.\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\nTable 1. A list of variables used throughout this paper and their descriptions.\nVariable\nDescription\n𝑑\nA set of documents for analysis\n𝑁, 𝑀\nNumber of samples in 𝑑, number of dimensions of 𝑑\n𝑛\nNumber of samples moved by the analyst, 𝑛≪𝑁\n𝑥\nHigh-dimensional feature of 𝑑. DL representations (768 dimension-size BERT embeddings)\n𝑦\nCoordinates in the 2D visual spatialization of 𝑑.\nSet either by analysts’ interactions on the visualization, or by the underlying SI model, which maps 𝑥to 𝑦\n𝑤dimension\nParameters of dimension weights of 𝑥. (a 768 dimension-size vector)\n𝑤BERT\nInternal parameters of the pretrained BERT model. BERT𝑏𝑎𝑠𝑒is used in this paper, which contains 110 million parameters\n𝑑𝑖𝑠𝑡\nEuclidean distance between data samples in 𝑑. Weighted Euclidean distance is used if 𝑤dimension applied.\n𝑑𝑖𝑠𝑡𝐻defines the high-dimensional similarity. 𝑑𝑖𝑠𝑡𝐿defines the low-dimensional similarity\n2.3\nPretrained Representation Adaptation\nThere are two main paradigms to adapt the pretrained DL representation model to downstream tasks: feature extraction\nand fine-tuning [48]. The feature extraction approach uses a task-specific architecture to adapt the pretrained represen-\ntations to downstream tasks (e.g. ELMo [47]). In this approach, parameters inside the task-specific architecture are\ntrained on the downstream tasks. In contrast, instead of using a new architecture, the fine-tuning method appends one\nadditional output layer to the pretrained DL and tunes the whole pretrained model with the downstream tasks. The\nfine-tuning approach requires relatively less training data because it introduces minimal task-specific parameters and\ndoes not need to learn randomly initialized task-specific parameters from scratch. Therefore, we use the fine-tuning\napproach in the DeepSIfinetune framework to adapt the pre-trained DL model to visual analytic tasks.\n2.4\nHuman-centered Deep Learning\nThere are other human-centered DL techniques proposed to assist users in complex data analytic tasks. Hsueh-Chien et.\nal. [14] used CNN techniques to assist users in volume visualization designing through facilitating user interaction with\nhigh-dimensional DL features. In RetainVis [34], an interactive and interpretable RNN model was designed for electronic\nmedical records analysis and patient risk predictions, which can be steered interactively by domain experts. Gehrmann\net al. [24] proposed a framework of collaborative semantic inference that enables the visual collaboration between\nhumans and DL algorithms. Sharkzor [50] is an interactive deep learning system for image sorting and summary,\nbased on users’ semantic interactions. Of these, Sharkzor is the most similar to our DeepSIfinetune. Both works provide\nusers with semantic interactions to tune the DL model interactively. However, our work emphasizes a general solution\nto integrate DL models into SI systems to improve inference. While Sharkzor is only designed for image analysis,\nDeepSIfinetune can be applied to other data analytic tasks and relevant DL models.\n3\nBACKGROUND\nIn order to frame our discussion of our model DeepSIfinetune, this section briefly describes DeepVA, the state-of-the-art\nSI model with pretrained DL [8]. For the purpose of comparison, we implement a specific version of DeepVA that uses\nBERT, which we denote as DeepSIvanilla. For reference, Table 1 describes frequently used variables throughout this\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\npaper. We use the pretrained BERT model as a representative DL model in DeepSI system designs. Note that WMDS\nis used as the default interactive DR in DeepSI frameworks for three reasons. First, the WMDS is a simple linear DR\nalgorithm, so that we can focus on assessing the effects of data representations on the model performance. Second,\nWMDS is agnostic to the choice of the weighted distance function. Third, WMDS enables analysts to express their\nsynthesis process by manipulating data point proximities to reflect their perceived similarity [58].\nBERT\nAnalyst \n(Perceive)\nAnalyst \n(Interact)\n9\nWMDS\nWMDS-1\nDimension\nWeights\n \nUpdatable Component\nmodel prediction\nmodel updating\n \nFrozen Component\nModel\nSpatialization\nHuman\nSpatialization\nInsights\nData \n(Raw Text) \nInteractive Training Loop\nRepresentations\nFig. 3. DeepSIvanilla pipeline, adapted from [7]: using the pretrained BERT as only a feature extractor pre-processed outside of the\ninteractive loop in SI pipeline. All parameters inside the pretrained BERT model are frozen and the output data representations are\nfixed. WMDS is the interactive DR, which is responsible to tune the dimension weights 𝒘dimension to capture the analyst’s intent.\nDeepSIvanilla uses the DL model as only a feature extractor in the SI pipeline. As shown in Fig. 3, the pretrained\nparameters inside the BERT model are frozen. Thereby, for an input, BERT provides a fixed general-purpose repre-\nsentation, which is then used as the data features in the interactive training loop. The BERT model is outside of the\ninteractive loop. Therefore, the interactive DR model, WMDS, is responsible for updating dimension weights 𝒘dimension to\ncapture the analyst’s intent as a weighting of the BERT features. The complete process of the pipeline is as follows.\nBefore entering the interactive training loop, the data representations are initialized by the BERT model with the\npretrained parameters 𝒘BERT:\n𝒙= BERT(𝑑,𝑤BERT)\n(1)\nIn the forward model-prediction direction, WMDS is performed to project high-dimensional data points (𝒙) into the\ntwo-dimensional spatialization (𝒚), with current dimension weights 𝑤dimension (initially, all weights are equal). This\nprovides a new projection for the analyst to perceive and interact.\n𝒚= arg min\n𝑦\n∑︁\n𝑖<𝑗≤𝑁\n\u0010\n𝑑𝑖𝑠𝑡𝐿(𝑦𝑖,𝑦𝑗) −𝑑𝑖𝑠𝑡𝐻(𝑥𝑖,𝑥𝑗,𝑤dimension)\n\u00112\n(2)\nIn the backward model-updating direction, the analyst provides visual feedback by repositioning 𝑛data points within the\nprojection. WMDS−1 uses the low-dimensional pairwise distances between the moved 𝑛data points as input, to learn\nnew dimension weights𝒘dimension to make sure these moved data points have similar relationships in the high-dimensional\nspace, based on the following optimization criterion:\n𝒘dimension = arg min\n𝑤\n∑︁\n𝑖<𝑗≤𝑛\n\u0010\n𝑑𝑖𝑠𝑡𝐿(𝑦𝑖,𝑦𝑗) −𝑑𝑖𝑠𝑡𝐻(𝑥𝑖,𝑥𝑗,𝑤dimension)\n\u00112\n(3)\nTherefore, through this loop, the dimension weights 𝒘dimension are trained interactively and incrementally based on\nanalysts’ interactions to capture their intents.\nDeepSIvanilla has been well-evaluated previously. DeepVA [8] used ResNet [27] as an image data feature extractor in\nthe SI system that assists users performing visual concepts analysis using DL representations. In [7], Bian et al. compared\nSI systems that use embedding vectors as features and those that use bag-of-words as features in visual text analysis tasks.\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\nTransformer\nLayer 2\n...\n...\nTransformer\nLayer 1\n...\nTransformer\nLayer n\nRepresentations\n...\nUpdatable Component\nmodel prediction\nmodel updating\nDL Model (BERT)\nAnalyst\n(Perceive)\nAnalyst\n(Interact)\nMDS\nMDS-1\nModel\nSpatialization\nHuman\nSpatialization\nInsights\nData\n(Raw Data)\nInteractive Training Loop\nFig. 4. DeepSIfinetune pipeline: embedding BERT within the SI loop. Semantic interactions are exploited to fine-tune BERT interactively\nthrough backpropagation. The tuned BERT is responsible for generating new representations, so as to capture the analyst’s intent.\nThereby, no external parameters are needed.\nExperiments in both works show that even the general-purpose representations of pretrained DL models can enable\nSI to better capture the analyst’s intent than hand-crafted features. However, using the general-purpose pretrained\nrepresentations still restricts SI inference. In the next section, we propose DeepSIfinetune, which exploits fine-tuned\nrepresentations to further improve SI inference. As the best-performing model from previous studies, DeepSIvanilla is\nthe baseline model for comparison.\n4\nMODEL DESCRIPTION\nThis section outlines the main design, model pipeline, and implementation details of DeepSIfinetune.\n4.1\nModel Design\nWe propose two main design goals to address the two research questions discussed in Sec. 1.\nDesign goal 1 - Integrating DL into the human-in-the-loop interactive sensemaking pipeline. To get user-\nand task-specific representations, it is necessary to iteratively train the DL model with semantic interactions during\nthe human-in-the-loop process. Inspired by multi-model SI systems [10, 19, 66], we inserted the DL model update\nand prediction process into the bidirectional semantic interaction loop, as shown in Fig. 4. The DL model update and\nprediction process occurs before the interactive DR model. The interactive DR model passes the analyst’s visual feedback\nfrom the human spatialization to the DL model. The visual feedback is then used to update the parameters inside the DL\nmodel (𝒘BERT) through the DL backpropagation [53] (red arrows inside the DL component). With updated parameters\n𝒘BERT, the BERT model calculates new representations for input data by the forward propagation [70] through the\ninternal transformer layers (black arrows inside the DL component). Through the interactive sensemaking process, the\nDL model is trained by semantic interactions in an interactive machine learning setting [2, 23]. Thereby, improved\nrepresentations are generated to accurately capture the analyst’s intent.\nDesign goal 2 - Introducing minimal parameters into the interactive DL training pipeline. To solve analytic\ntasks efficiently, the analyst prefers to perform fewer interactions in each sensemaking loop. However, DL model\ntraining typically needs a relatively large amount of training data. To reduce the number of interactions needed for\ntraining, we should introduce minimal parameters into the pipeline while integrating the DL model training. For this\ndesign goal, we made specific modifications to both the DL and the interactive DR components. First, we used the\nfine-tuning approach to adapt the pretrained BERT model with semantic interactions. Unlike the feature-based method,\nfine-tuning approach introduced minimal task-specific paramters [26]. This drastically reduced the required training\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\ndata. Further, we used MDS/MDS−1 as the interactive DR component. During the interactive BERT model training,\nrepresentations are updated to capture analysts’ intents. It is unnecessary to tune extra parameters for the same purpose\nin the interactive DR model. Therefore, we used MDS/MDS−1 without dimension weights 𝒘dimension as the interactive DR\ncomponent. There are no parameters to tune in this component. Therefore, users’ interactions can be passed directly to\nDL training without information loss.\n4.2\nModel Pipeline\nWe illustrate the DeepSIfinetune pipeline (Fig. 4) in detail through the human-in-the-loop sensemaking process. In\nthe forward model-prediction direction, new representations are generated for the dataset 𝑑through the forward\npropagation calculation of the BERT model, with current BERT parameters (𝒘BERT):\n𝒙= BERT(𝑑,𝑤BERT)\n(4)\nThe high-dimensional DL representations 𝑥are then projected to the 2D spatialization (model spatialization) by MDS\nthrough the following equation:\n𝒚= arg min\n𝑦\n∑︁\n𝑖<𝑗≤𝑁\n\u0010\n𝑑𝑖𝑠𝑡𝐿(𝑦𝑖,𝑦𝑗) −𝑑𝑖𝑠𝑡𝐻(𝑥𝑖,𝑥𝑗)\n\u00112\n(5)\nIn contrast to Eq. 2, the high-dimensional distance function is not explicitly weighted. Instead, the updates to 𝒚in each\nloop are captured by the fine-tuned representation 𝑥itself. The analyst perceives the updated spatialization and gains\ninsight.\nIn the backward model-updating direction, the analyst modifies the visual layout (human spatialization) by reposition-\ning some samples to express the preferred similarities between them. Then, MDS−1 uses the human-defined similarities\nbetween 𝑛moved data points, 𝑑𝑖𝑠𝑡𝐿(𝑦𝑖,𝑦𝑗)), to steer the BERT model parameters to generate better high-dimensional\nrepresentations 𝑥, such that the similarity of the representations reflects the proximity of the points in the modified\nprojection, as follows:\n𝑤BERT = arg min\n𝑤BERT\n∑︁\n𝑖<𝑗≤𝑛\n\u0010\n𝑑𝑖𝑠𝑡𝐿(𝑦𝑖,𝑦𝑗) −𝑑𝑖𝑠𝑡𝐻(BERT(𝑑𝑖,𝑤BERT), BERT(𝑑𝑗,𝑤BERT))\n\u00112\n(6)\nThe optimization objective is to fine-tune BERT weights 𝒘BERT to minimize the difference between low-dimensional\nand high-dimensional distances of 𝑛moved data points through backpropagation. All internal parameters of the BERT\nmodel (𝒘BERT) are updated in order, from last transformer layers to previous layers, by a gradient descent optimization\nalgorithm [52]. After the backpropagation, the updated 𝒘BERT is used in the forward propagation to calculate new\nrepresentations 𝒙in Eq. 4.\nThrough this human-in-the-loop interactive DL process, the BERT model is tuned properly to generate user- and\ntask-specific representations so as to capture analysts’ precise intents: samples that should be closer to each other in\nthe visualization obtain similar features, while more distant samples gain differing features.\n4.3\nPrototyping Detail\nHere, we describe the implementation details of DeepSI prototypes used in our experiments, including model settings\nand visualization design. These implementations are applicable for both DeepSIfinetune and DeepSIvanilla.\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\n4.3.1\nModel Settings. We use Pytorch [44], a well-known Python DL framework, to implement the DeepSI system. For\nthe forward DR component, MDS is adapted from Scikit-Learn [45]. The MDS−1 is implemented in Pytorch as a neural\nnetwork layer. The pretrained BERT model is adapted from the publicly available Python library, Transformers [68].\nTransformers provides two sizes of pretrained BERT models: BERTBASE, and BERTLARGE. We used the small BERT\nmodel (𝐵𝐸𝑅𝑇𝐵𝐴𝑆𝐸) (bert-base-uncased, 12-layers, 768-hidden, 12-heads, 110M parameters), because it is more stable\non small datasets. For a document containing a list of tokens, BERTBASE can convert each of the tokens into a 768-\ndimensional vector. To generate fixed-length encoding vectors from documents of different lengths, we appended a\nMEAN pooling layer to the last transformer layer of the BERT model, such that the output representation for a document\nwas a 768-dimensional vector. Therefore, the 𝑤dimension used in DeepSIvanilla is also a 768 dimension vector. We also tested\nother pooling strategies, such as MAX pooling and CLS pooling [51]. However, there was no obvious performance\ndifference, and the MEAN pooling showed slightly better performance. In addition, we used the Adam optimizer [33] to\noptimize the DeepSI model parameters in the model-updating direction. We also explored other optimizers provided\nby PyTorch. Across all our experiments, we found that Adam optimizer performed the best. Further, we found that\nthe suggested learning rate (3𝑒−5) for finetuning BERT models in [16] led to optimal DeepSIfinetune performance in\nexperiments.\n4.3.2\nVisualization Design. We drew inspiration for visualization design from SI-enabled VA applications, including\nAndromeda [58], ForSPIRE [21], and Dis-Function [11]. As shown in Fig. 1, the visual interface mainly uses a scatterplot\nas the projection layout. This scatterplot not only displays the relationships between data updated by the underlying\nprojection model, but also allows the analyst to intervene and modify the layout. Specifically, in the forward model-\nprediction direction, the positions between data points on the scatterplot reflect the points’ relative similarity learned\nby underlying models, either by the projection method or by the fine-tuned BERT model, shown in Fig. 1-1 (model\nspatialization). In the backward model-updating direction, the user can drag several data points to new positions to\nmodify similarities between points based on their preference, shown in Fig. 1-2 (human spatialization). Having both the\nunderlying models and analysts work on the same visualization provides direct and effective communication between\nhumans and computation. In addition to the scatterplot view, the prototype also provides a sidebar view to help analysts\nreview the content of a selected document when exploring in the scatterplot view. In this paper, we intentionally focus\non the scatterplot view in the screenshots, to focus on the analysis of the model performance.\n5\nEXPERIMENTS\nTo evaluate DeepSIfinetune, we conducted the following experiments. To examine how well DeepSIfinetune addresses the\ngoals, we measured its performance in two respects:\n• Accuracy: How accurately can DeepSIfinetune capture the analyst’s intent?\n• Efficiency: How many interactions does DeepSIfinetune need to capture the analyst’s intent properly?\nWe use DeepSIvanilla, described in Sec. 3, as the baseline model to evaluate the advantage of DeepSIfinetune’s task-specific,\ninstead of general-purpose, representations. Boukhelifa et al. [9] proposed the complementary evaluation of interactive\nmachine learning systems by using both algorithm-centered and human-centered evaluation methods. We perform\nboth evaluation methods in our experiments: the case study in Sec. 5.1 is the human-centered qualitative analysis, and\nthe simulation-based evaluation method in Sec. 5.2 is the algorithm-centered quantitative analysis.\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\n5.1\nCase Study: COVID-19\nRecently, COVID-19 [61] has become a global pandemic. It is essential that medical researchers quickly find relevant\ndocuments about a specific research question, given the extensive coronavirus literature. We used an analysis task on\nacademic articles related to COVID-19 in this case study to examine our proposed DeepSIfinetune, compared with the\nbaseline model DeepSIvanilla. In this study, we performed the same task with the help of both DeepSI prototypes and\nthen measure the model performance in the following two perspectives:\n• Accuracy: the quality of the projection updated by the underlying model given the task’s ground truth.\n• Efficiency: how many interactions are needed for the underlying model to provide a useful projection.\n5.1.1\nDataset and Task. The COVID-19 Open Research Dataset (CORD-19) 1 contains a collection of more than 200, 000\nacademic articles about COVID-19. CORD-19 also proposes a series of tasks in the form of important research questions\nabout the coronavirus. One of the research tasks focuses on identifying COVID-19 risk factors 2. In this case study,\nwe selected a task that requires identifying articles related to specific risk factors for COVID-19. We asked an expert\nto choose as many research papers as possible about risk factors from CORD-19. We found four main risk factors:\ncancer (15 articles), chronic kidney disease (13 articles), neurological disorders (23 articles), and smoking status (11\narticles). We used these four risk factors as the ground truth for the test task, and loaded all these 62 articles into\nour DeepSI prototypes. Therefore, the test task was to organize these 62 articles into four clusters with our DeepSI\nprototype such that each cluster represented articles of a specific risk factor.\nThis particular ground truth is just one possible way an analyst might want to organize this group of documents.\nSo our goal is to see if this particular set of expert knowledge can be easily injected using SI to help re-organize the\ndocuments in this particular way. To help judge the quality of the visual layout in organizing this particular ground\ntruth, we color the dots according to the ground-truth risk factors in the visualization: cancer (black dot •), chronic\nkidney disease (red dot •), neurological disorders (blue dot •), Smoking status (green dot •). It should be noted\nthat the underlying model was not provided with the ground truth or color information. The ground truth is only\ninjected via semantic interaction from the human in the form of partial groupings of only a few of the documents.\n5.1.2\nStudy Procedure. To compare the projection layouts updated by two models based on the same input interactions,\nsemantic interactions based on the ground truth are performed in the shared visual projection and then applied to\nthe two models separately. Fig. 1 and Fig. 5 show the process of interactions applied separately to DeepSIfinetune and\nDeepSIvanilla prototypes. In both figures, frame 1 and frame 2 are from the shared visual projection. Frame 1 in both\nfigures (Fig. 1-1 and Fig. 5-1) shows the same initial layout updated by the default pretrained BERT model. In the initial\nprojection layout, all the articles are combined. This means that the pretrained BERT model cannot distinguish these\narticles by their related risk factors. Interactions were performed within the projection based on the ground truth to\nreflect the perceived connections between articles: grouping three articles about cancer to the top-left region of the\nprojection, indicated by the black arrows; three articles about chronic kidney disease to the top-right region indicated\nby red arrows; three articles about smoking status to the bottom-left part indicated by green arrows; and three articles\nabout about neurological disorders to the bottom-right part indicated by blue arrows.\nFrame 2 (Fig. 1-2 and Fig. 5-2) is the same human spatialization and shows four clusters created by the ground truth.\nAfter we clicked the ‘model update’ button on the menu bar to start the model training process. Then, the same human\n1https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n2https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=558\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\nspatialization was used to train both models. After the two models had been updated, the updated projections of these\ntwo models showed the performance difference in frame 3 (Fig. 1-3 and Fig. 5-3). Subsequently, the performance of the\nmodel could be assessed based on how reasonable the layout was in comparison with the ground truth.\n 1 \n 2 \n 3 \nCancer\nKidney Disease\nNeurological Disorders\nSmoking Status\nFig. 5. Screenshots during the case study using DeepSIvanilla: Frame 1 and 2 show the similar initial steps performed by the analyst\nin Fig. 1. Frame 3 shows the resulting projection updated by DeepSIvanilla.\nDeepSIfinetune spatialization: The projection updated by DeepSIfinetune is shown in Fig. 1-3. There are four clear\nclusters, and all articles are clearly grouped into the correct clusters. The top left cluster contains all the articles about\ncancer (•), the top right cluster contains articles about kidney disease (•), the bottom left contains articles about smoking\nstatus (•), and the bottom right contains articles about neurological disorders (•). This means the new representations\ngenerated by the fine-tuned BERT model are able to accurately capture the semantic meanings behind users’ interactions.\nDeepSIvanilla spatialization: With the same interactions as input, the updated DeepSIvanilla shows a different\nlayout. As shown in Fig. 5-3, there are no clear clusters in the updated layout compared with Fig. 1-3. Articles about\ndifferent risk factors still overlap. Even after continued interactions based on the ground truth, DeepSIvanilla is unable\nto properly capture the user’s semantic intent and differentiate these articles.\nFurther study for DeepSIvanilla: However, DeepSIvanilla did work well at separating articles into two clusters\nin two opposite positions in the projection. For example, in Fig 5-3, smoking status articles (•) are separated from\nkidney disease articles (•). Exploring further, after resetting the model as shown in Fig. 6-2, three neurological disorders\narticles (•) are dragged to the bottom-left and three chronic kidney disease articles (•) are dragged to the top-right on\nthe scatterplot view. After the layout updates, articles from these two dragged clusters are well placed in two opposite\nsides of the visualization in Fig. 6-3, ignoring articles in the other two clusters (about cancer • and smoking status •).\n 1 \n 2 \n 3 \nKidney Disease\nNeurological Disorders\nFig. 6. Further case study using DeepSIvanilla in grouping two clusters: Frame 1 is the initial projection layout, Frame 2 shows\ninteractions performed within the projection, and Frame 3 shows the resulting projection updated by DeepSIvanilla.\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nTransformer\nLayer 2\n...\n...\nTransformer\nLayer 1\n...\nTransformer\nLayer n\nEvaluation\nSimulator\nInteraction\nSimulator\nMDS\nMDS-1\nRepresentations\n...\nDL Model (BERT)\nData Labels\nModel\nSpatialization\nHuman\nSpatialization\nSampling\nSubset\nPerformance \nover Iterations\nSimulated Analyst\nData\n(Raw Data)\nFig. 7. Simulation-based evaluation pipeline. The analyst is replaced by the ‘simulated analyst’ component where: analyst perception\nis simulated by the kNN classifier and analyst interaction by sampling a subset of ground truth. In each SI loop, the kNN classification\nis employed to calculate the accuracy of the model spatialization, which is updated by the underlying model DeepSIfinetune and\nreflects the model performance.\n5.1.3\nQualitative Results. In terms of accuracy, DeepSIfinetune grouped articles correctly based on the user-defined risk\nfactors. In contrast, DeepSIvanilla did not provide a useful projection. The further study also confirmed that DeepSIvanilla\ncan handle more straightforward tasks with only two clusters. The semantics of the ground truth knowledge provided\nby the contest organizers are more recognizable in the DeepSIfinetune projection. Articles in each group are clearly\nclustered. However, DeepSIvanilla only partially separated in two separate directions, instead of into distinct clusters,\nwhich requires more cognitive effort to identify the boundary between the groups. In terms of efficiency, DeepSIfinetune\nis more efficient than DeepSIvanilla. DeepSIfinetune needed a small number of interactions (moving three articles in each\ncluster, 12 dots movement in total) in one interactive SI loop to fine-tune the BERT model properly for this task. In\ncontrast, in DeepSIvanilla, the same amount of interactions only supported the simpler task with two clusters, and\nadditional rounds of interaction still did not uncover all four clusters.\n5.2\nSimulation-based Evaluation\nFrom the machine learning algorithm perspective, DeepSI systems are transductive models [71] that interactively learn\nprojections provided by the analyst. Therefore, the performance of DeepSI systems can be measured by the predicted\nprojections. To conduct the quantitative comparisons between the predicted projections from DeepSI systems, we\nreplaced the analyst with a simulation component (simulated analyst). As shown in Fig. 7, the simulated analyst uses the\ninteraction simulator to generate a training projection (human spatialization) based on data labels, and the evaluation\nsimulator to evaluate the accuracy of the predicted projection. After training iteration, the simulated analyst outputs a\ncurrent projection accuracy. The projection accuracy over iterations reflects the learning curve [46] of the DeepSI model.\nTherefore, performances of both DeepSI models could be compared through their learning curves in both accuracy and\nefficiency perspectives.\n5.2.1\nSimulated Analyst. As shown in the simulation pipeline (Fig. 7), data labels are the ground truth to support both\ninteraction simulator and evaluation simulator. First, the interaction simulator uses these labels to calculate the pairwise\ndistances between a subset of data samples, simulating the human-defined similarities between these samples. Further,\nthe evaluation simulator uses these class labels to measure how well the predicted projection grouped data samples\ninto correct classes based on their labels.\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\nInteraction simulator: In each interaction, three samples from each class are selected using random sampling [60].\nThen the interaction simulator calculates the pairwise distance 𝑑𝑖𝑠𝑡𝐿(𝑦𝑖,𝑦𝑗) of these selected samples based on:\n𝑑𝑖𝑠𝑡𝐿(𝑑𝑖,𝑑𝑗) =\n\n\n0\nif 𝑑𝑖and 𝑑𝑗have the same label\n√\n2\notherwise\nAs shown in the above Equation, if two selected samples have different labels, the distance between them is𝑑𝑖𝑠𝑡𝐿(𝑑𝑖,𝑑𝑗) =\n√\n2, because the analyst should move them away from each other to obtain the farthest distance on the 2D spatialization.\nIf the two samples have the same label, the analyst should move them as close as possible (𝑑𝑖𝑠𝑡𝐿(𝑑𝑖,𝑑𝑗) = 0) in the\nprojection, because they belong to the same cluster. Therefore, the interaction simulator provides the calculated pairwise\ndistances between the selected samples as the training projection for DeepSI models.\nEvaluation simulator: After the interaction simulator trains the DeepSI model, the trained model predicts a new\nprojection. The predicted projection reflects the similarity relationships between samples in the low-dimensional\nspatialization. We used a kNN (K-nearest-neighbour) classifier [15] as the evaluation simulator to measure the predicted\nprojection [7, 11]. The kNN classifier uses the neighbor information on the projection to train and predict the data\nclasses. The performance of the learned kNN classifier can directly reflect the quality of the projection [64]. Concretely,\nwe used the leave-one-out cross-validation [42] and set k = 5 closest training examples to predict the unlabelled sample.\nWe also explored other values for k, such as 3, 7, 9, 11, but these did not produce significant changes in the results. We\ncould thus obtain the trained kNN classifier accuracy by comparing the predicted output with the ground truth.\nPerformance over iteration: A new accuracy from the kNN classifier was returned from the simulation pipeline in\neach iteration loop. These are accumulated into a plot of kNN classifier performance over the iterations of the simulated\ninteraction loop. This learning curve shows how rapidly the DeepSI model learned during the interactive process.\n5.2.2\nDataset and Task. We explored three commonly used text corpora in natural language processing and visual\ntext analysis tasks. These corpora contain different numbers of labels and are from different domains, providing a\ncomprehensive evaluation of performance comparisons.\nSST with two clusters: The SST dataset [59] is a collection of movie reviews with both fine-grained labels (out of\nfive stars) and binary labels (positive and negative reviews). We used the binary version of the dataset, which contains\n1821 reviews in total: 909 positive and 912 negative. The task, denoted as 𝑇sst, used the SST dataset to train the DeepSI\nmethods to obtain two clusters (positive and negative).\nVispubdata with three clusters: The Vispubdata dataset [31] contains academic papers published in the IEEE\nVIS conference series. These papers belong to one of the three conferences: InfoVis (Information Visualization),\nSciVis (Scientific Visualization), and VAST (Visual Analytics Science and Technology). We used the papers published\nbetween 2008 and 2018 (including 397 papers from InfoVis, 534 papers from SciVis, and 521 papers from VAST) in this\ntask, denoted as 𝑇vis. In 𝑇vis, the simulated analyst need to iteratively drag papers into these three conferences clusters\nto evaluate the DeepSI.\n20 Newsgroups with four clusters: The 20 Newsgroup dataset 3 is a collection of newsgroup posts on 20 topics.\nBased on this dataset, we create the task (𝑇news) to classify four topics into different clusters in the spatialization.\nWe picked four topics from the same sub-category ‘rec’ including: 594 reports from ‘rec.autos’, 598 reports from\n‘rec.motorcycles’, 597 reports from ‘rec.sport.baseball’, and 600 reports from ‘res.sport.hockey’.\n3http://qwone.com/ jason/20Newsgroups/\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIteration\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nDeepSI-Finetuning\nDeepSI-Vanilla\n(a) 𝑇sst\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIteration\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nDeepSI-Finetuning\nDeepSI-Vanilla\n(b) 𝑇vis\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIteration\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nDeepSI-Finetuning\nDeepSI-Vanilla\n(c) 𝑇news\nFig. 8. The accuracies of both DeepSIfinetune and DeepSIvanilla updated projections over 200 iterations across the three tasks (𝑇sst, 𝑇vis,\nand 𝑇news) during the simulation-based experiment.\n5.2.3\nQuantitative Results. Fig. 8 shows the learning curves of both DeepSI methods in all three tasks. There is no\ncrossing of the curves between these two models, and the performance curve of DeepSIfinetune is above the curve\nof DeepSIvanilla through all iterations. This means DeepSIfinetune showed better performance on all three tasks than\nDeepSIvanilla. For model accuracy, DeepSIfinetune converged to more than or nearly 90% accuracy in all three tasks. On\nthe contrary, the best performance of DeepSIvanilla only showed slightly higher accuracy (less than 80%) than the initial\nperformance in the first task (𝑇sst) with two clusters. For tasks with more than two clusters (𝑇vis in Fig. 8(b) and 𝑇news\nin Fig. 8(c)), DeepSIvanilla did not show noticeable accuracy increases. This is consistent with our findings in the case\nstudy (Sec. 5.1). For model efficiency, the performance over iterations of DeepSIfinetune in all three tasks showed steeper\nincreases and quickly approximated peak accuracy. Furthermore, the performance of DeepSIfinetune increased fairly\nconsistently compared to DeepSIvanilla. This provides analysts with more consistent feedback over iterations.\n6\nDISCUSSION\n6.1\nGenerality and Applicability\nIn this paper, we use pretrained BERT as the specific DL model in DeepSIfinetune to advance SI-enabled applications.\nConsidering DeepSIfinetune as a framework, it is general enough to apply other pretrained DL models into the semantic\ninteraction pipeline for other VA tasks. First, other transformer-based models, such as RoBERTa [38], XLNet [69] and\nGPT-3 [12], can be applied directly in the DeepSI pipeline without any special configuration. In addition, fine-tunable\nDL models with other structures, such as CNN and RNN models [1], can also be integrated into a DeepSI pipeline by\nappending a special pooling layer to transform hidden states into proper representations. Further, other feature-based\nDL models could also be applied to the inteactive fine-tune process with specific designs. For example, ELMo [47] could\nbe fine-tuned by using max-pool over the model’s internal states and adding a softmax layer [48].\n6.2\nScalability\nBeyond measuring accuracy and efficiency, our two experiments also illuminated scalability. All our experiments were\nconducted on a desktop computer with an Intel i9-9900k processor, 32G Ram, and one NVIDIA GeForce RTX 2080Ti\nGPU, running Windows 10. In the case study, DeepSIfinetune captured the analyst’s intent and provided an accurate\nprojection with 62 data points in real time. In the simulation-based experiment, DeepSIfinetune also provided accurate\nprojections that contains thousands of data points. During the simulation, MDS projection calculation (𝑂(𝑛2) algorithm)\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\nconsumed the majority of the time. The amount of time required for the DL model update and prediction was negligible\nin comparison. This highlights the potential for improving the DR method in DeepSI.\n6.3\nInteractive Deep Metric Learning\nDis-Function [11] describes the WMDS-based SI model as an interactive distance function learning model from the\ninteractive machine learning perspective. Likewise, DeepSIfinetune can be regarded as an interactive deep metric learning\nmodel [32]. As shown in Fig. 4, in the model-updating direction, the underlying pretrained BERT model is trained\ninteractively to output better presentations that can capture the analyst-desired distance relationships. Deep metric\nlearning methods usually use metric loss functions [32] for labelled data, such as contrastive loss [26], triplet loss [28]\nand angular loss [63]. In contrast, DeepSIfinetune uses a metric loss function specially designed for semantic interactions\nbased on MDS−1.\n6.4\nLimitations and Future Work\nOur DeepSIfinetune proved effective in capturing analysts’ precise intents and displaying intuitive projections. However,\ncurrent DeepSIfinetune prototypes could be extended in two directions. First, it is important to make the internal status\nof the underlying model interpretable to analysts in order to facilitate them making hypotheses and decisions during the\nsensemaking process, which is known as interpretable machine learning [43]. Other than the projection scatterplot, the\ncurrent DeepSIfinetune prototype does not provide any other visual hints about the status of the DL model. Therefore,\nwe plan to add more specific visual designs in future work to better expose the effects of tuning the DL model.\nIn addition, DeepSIfinetune uses the traditional metric learning method [4] as the interactive DR component to\ncommunicate between the DL model and the analyst. As discussed above (Sec. 6.3), a MDS-based loss function LossSI is\nused to interactively tune the DL model. Inversely, these deep metric learning loss functions, such as contrastive loss\nand triplet loss, could also be used as interactive DR components. We plan in future work to use deep metric learning\nloss functions as the interactive DR component in DeepSIfinetune.\n7\nCONCLUSION\nIn this work, we focused on DeepSI and the research question of how to integrate the DL model into the SI pipeline to\nleverage its capability to better capture the semantics behind user interactions. We identified two design requirements of\neffective DeepSI systems: the DL model is trained interactively in the SI pipeline and the DL model can be tuned properly\nwith a small number of interactions. We presented DeepSIfinetune, which incorporates DL fine-tuning and MDS-based\ninteractive DR methods into the DeepSI pipeline to meet these requirements. We performed two complementary\nexperiments to measure the effectiveness of DeepSIfinetune, including a case study of a real-world task relating to\nCOVID-19 and a simulation-based quantitative evaluation method on three commonly used text corpora. The results of\nthese two experiments demonstrated that DeepSIfinetune improves performance over the state-of-the-art alternative that\nuses DL only as a pre-processed feature extractor, indicating the importance of integrating the DL into the interactive\nloop.With a small number of semantic interactions as input, DeepSIfinetune better captures the semantic intent of the\nanalyst behind these interactions.\nACKNOWLEDGMENTS\nThis work was supported in part by NSF I/UCRC CNS-1822080 via the NSF Center for Space, High-performance, and\nResilient Computing (SHREC).\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nREFERENCES\n[1] Oludare Isaac Abiodun, Aman Jantan, Abiodun Esther Omolara, Kemi Victoria Dada, Nachaat AbdElatif Mohamed, and Humaira Arshad. 2018.\nState-of-the-art in artificial neural network applications: A survey. Heliyon 4, 11 (2018), e00938. https://doi.org/10.1016/j.heliyon.2018.e00938\n[2] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014. Power to the people: The role of humans in interactive machine\nlearning. AI Magazine 35, 4 (2014), 105–120.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint\narXiv:1409.0473 (2014).\n[4] Aurélien Bellet, Amaury Habrard, and Marc Sebban. 2015. Metric Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning 9, 1\n(2015), 1–151. https://doi.org/10.2200/S00626ED1V01Y201501AIM030 arXiv:https://doi.org/10.2200/S00626ED1V01Y201501AIM030\n[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 35, 8 (2013), 1798–1828. https://doi.org/10.1109/tpami.2013.50\n[6] Y. Bengio, A. Courville, and P. Vincent. 2013. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 35, 8 (Aug 2013), 1798–1828. https://doi.org/10.1109/TPAMI.2013.50\n[7] Yali Bian, Michelle Dowling, and Chris North. 2019. Evaluating Semantic Interaction on Word Embeddings via Simulation. EValuation of Interactive\nVisuAl Machine Learning systems, an IEEE VIS 2019 Workshop. (2019).\n[8] Yali Bian, John Wenskovitch, and Chris North. 2019. DeepVA: Bridging Cognition and Computation through Semantic Interaction and Deep Learning.\nProceedings of the IEEE VIS Workshop MLUI 2019: Machine Learning from User Interactions for Visualization and Analytics. (2019).\n[9] Nadia Boukhelifa, Anastasia Bezerianos, and Evelyne Lutton. 2018. Evaluation of interactive machine learning systems. In Human and Machine\nLearning. Springer, 341–360.\n[10] Lauren Bradel, Chris North, Leanna House, and Scotland Leman. [n.d.]. Multi-model semantic interaction for text analytics. In 2014 IEEE Conference\non Visual Analytics Science and Technology (VAST). IEEE, 163–172.\n[11] Eli T Brown, Jingjing Liu, Carla E Brodley, and Remco Chang. 2012. Dis-function: Learning distance functions interactively. 2012 IEEE Conference on\nVisual Analytics Science and Technology (VAST) (2012), 83–92. https://doi.org/10.1109/VAST.2012.6400486\n[12] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]\n[13] Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris\nTar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder. arXiv:1803.11175 [cs.CL]\n[14] H. Cheng, A. Cardone, S. Jain, E. Krokos, K. Narayan, S. Subramaniam, and A. Varshney. 2019. Deep-Learning-Assisted Volume Visualization. IEEE\nTransactions on Visualization and Computer Graphics 25, 2 (Feb 2019), 1378–1391. https://doi.org/10.1109/TVCG.2018.2796085\n[15] T. Cover and P. Hart. 2006. Nearest Neighbor Pattern Classification. IEEE Trans. Inf. Theor. 13, 1 (Sept. 2006), 21–27. https://doi.org/10.1109/TIT.\n1967.1053964\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n[17] M. Dowling, J. Wenskovitch, J. T. Fry, S. Leman, L. House, and C. North. 2019. SIRIUS: Dual, Symmetric, Interactive Dimension Reductions. IEEE\nTransactions on Visualization and Computer Graphics 25, 1 (2019), 172–182.\n[18] Michelle Dowling, John Wenskovitch, Peter Hauck, Adam Binford, Nicholas Polys, and Chris North. 2018. A bidirectional pipeline for semantic\ninteraction. In Proc. Workshop on Machine Learning from User Interaction for Visualization and Analytics (at IEEE VIS 2018), Vol. 11.\n[19] Michelle Dowling, Nathan Wycoff, Brian Mayer, John Wenskovitch, Scotland Leman, Leanna House, Nicholas Polys, Chris North, and Peter Hauck.\n2019. Interactive Visual Analytics for Sensemaking with Big Text. Big Data Research 16 (2019), 49–58. https://doi.org/10.1016/j.bdr.2019.04.003\n[20] A. Endert, R. Chang, C. North, and M. Zhou. 2015. Semantic Interaction: Coupling Cognition and Computation through Usable Interactive Analytics.\nIEEE Computer Graphics and Applications 35, 4 (July 2015), 94–99. https://doi.org/10.1109/MCG.2015.91\n[21] Alex Endert, Patrick Fiaux, and Chris North. 2012. Semantic interaction for visual text analytics. In Proceedings of the SIGCHI conference on Human\nfactors in computing systems. ACM, 473–482.\n[22] Mateus Espadoto, Nina Sumiko Tomita Hirata, and Alexandru C Telea. 2020. Deep learning multidimensional projections. Information Visualization\n(2020), 1473871620909485.\n[23] Jerry Alan Fails and Dan R Olsen Jr. 2003. Interactive machine learning. In Proceedings of the 8th international conference on Intelligent user interfaces.\n39–45.\n[24] Sebastian Gehrmann, Hendrik Strobelt, Robert Krüger, Hanspeter Pfister, and Alexander M. Rush. 2020. Visual Interaction with Deep Learning\nModels through Collaborative Semantic Inference. IEEE Transactions on Visualization and Computer Graphics 26, 1 (2020), 884–894.\nhttps:\n//doi.org/10.1109/tvcg.2019.2934595\n[25] Tera Marie Green, William Ribarsky, and Brian Fisher. 2009. Building and applying a human cognition model for visual analytics. Information\nvisualization 8, 1 (2009), 1–13.\nIUI ’21, April 14–17, 2021, College Station, TX, USA\nBian and North\n[26] R. Hadsell, S. Chopra, and Y. LeCun. 2006. Dimensionality Reduction by Learning an Invariant Mapping. In 2006 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’06), Vol. 2. 1735–1742.\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition. 770–778.\n[28] Elad Hoffer and Nir Ailon. 2015. Deep Metric Learning Using Triplet Network. In Similarity-Based Pattern Recognition, Aasa Feragen, Marcello\nPelillo, and Marco Loog (Eds.). Springer International Publishing, Cham, 84–92.\n[29] Leanna House, Scotland Leman, and Chao Han. 2015. Bayesian visual analytics: BaVA. Statistical Analysis and Data Mining: The ASA Data Science\nJournal 8, 1 (Jan. 2015), 1–13.\n[30] X. Hu, L. Bradel, D. Maiti, L. House, C. North, and S. Leman. 2013. Semantics of Directly Manipulating Spatializations. IEEE Transactions on\nVisualization and Computer Graphics 19, 12 (2013), 2052–2059.\n[31] Petra Isenberg, Florian Heimerl, Steffen Koch, Tobias Isenberg, Panpan Xu, Chad Stolper, Michael Sedlmair, Jian Chen, Torsten Möller, and John\nStasko. 2017. vispubdata.org: A Metadata Collection about IEEE Visualization (VIS) Publications. IEEE Transactions on Visualization and Computer\nGraphics 23, 9 (Sept. 2017), 2199–2206. https://doi.org/10.1109/TVCG.2016.2615308\n[32] Mahmut Kaya and Hasan Şakir Bilge. 2019. Deep Metric Learning: A Survey. Symmetry 11, 9 (2019), 1066. https://doi.org/10.3390/sym11091066\n[33] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n[34] Bum Chul Kwon, Min-Je Choi, Joanne Taery Kim, Edward Choi, Young Bin Kim, Soonwook Kwon, Jimeng Sun, and Jaegul Choo. 2018. RetainVis:\nVisual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records. IEEE Transactions on Visualization\nand Computer Graphics 25, 1 (2018), 299–309. https://doi.org/10.1109/tvcg.2018.2865027 arXiv:1805.10724\n[35] B. C. Kwon, H. Kim, E. Wall, J. Choo, H. Park, and A. Endert. 2017. AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User\nDrawings. IEEE Transactions on Visualization and Computer Graphics 23, 1 (Jan 2017), 221–230. https://doi.org/10.1109/TVCG.2016.2598446\n[36] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436.\n[37] Scotland C Leman, Leanna House, Dipayan Maiti, Alex Endert, and Chris North. 2013. Visual to Parametric Interaction (V2PI). PLOS ONE 8, 3\n(March 2013), e50474.\n[38] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n[39] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, Nov (2008), 2579–2605.\n[40] Alberto González Martínez, Billy Troy Wooton, Nurit Kirshenbaum, Dylan Kobayashi, and Jason Leigh. 2020. Exploring Collections of research\npublications with Human Steerable AI. (2020), 339–348. https://doi.org/10.1145/3311790.3396646\n[41] Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv\npreprint arXiv:1802.03426 (2018).\n[42] Matthew Mullin and Rahul Sukthankar. 2000. Complete Cross-Validation for Nearest Neighbor Classifiers. In Proceedings of the Seventeenth\nInternational Conference on Machine Learning (ICML ’00). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 639–646.\n[43] W James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. 2019. Interpretable machine learning: definitions, methods, and\napplications. arXiv preprint arXiv:1901.04592 (2019).\n[44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural\nInformation Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc.,\n8024–8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\n[45] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12\n(2011), 2825–2830.\n[46] Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree Induction vs. Logistic Regression: A Learning-Curve Analysis. J. Mach. Learn.\nRes. 4, null (Dec. 2003), 211–255. https://doi.org/10.1162/153244304322972694\n[47] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized\nword representations. arXiv preprint arXiv:1802.05365 (2018).\n[48] Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks.\nCoRR abs/1903.05987 (2019). arXiv:1903.05987 http://arxiv.org/abs/1903.05987\n[49] Peter Pirolli and Stuart Card. 2005. The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis.\n(2005), 2–4. https://analysis.mitre.org/proceedings/Final_Papers_Files/206_Camera_Ready_Paper.pdf\n[50] Meg Pirrung, Nathan Hilliard, Artëm Yankov, Nancy O’Brien, Paul Weidert, Courtney D. Corley, and Nathan O. Hodas. 2018. Sharkzor: Interactive\nDeep Learning for Image Triage, Sort and Summary. CoRR abs/1802.05316 (2018). arXiv:1802.05316 http://arxiv.org/abs/1802.05316\n[51] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing. Association for Computational Linguistics. http://arxiv.org/abs/1908.10084\n[52] Sebastian Ruder. 2016. An overview of gradient descent optimization algorithms. arXiv:1609.04747 [cs.LG]\nDeepSI: Interactive Deep Learning for Semantic Interaction\nIUI ’21, April 14–17, 2021, College Station, TX, USA\n[53] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating errors. nature 323, 6088 (1986),\n533–536.\n[54] Dominik Sacha, Leishi Zhang, Michael Sedlmair, John A Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen C North, and Daniel A Keim. 2016. Visual\ninteraction with dimensionality reduction: A structured literature analysis. IEEE transactions on visualization and computer graphics 23, 1 (2016),\n241–250.\n[55] Susan S Schiffman, M Lance Reynolds, and Forrest W Young. 1981. Introduction to multidimensional scaling: Theory, methods, and applications.\nEmerald Group Publishing.\n[56] Jessica Zeitz Self, Michelle Dowling, John Wenskovitch, Ian Crandell, Ming Wang, Leanna House, Scotland Leman, and Chris North. 2018. Observation-\nLevel and Parametric Interaction for High-Dimensional Data Analysis. ACM Trans. Interact. Intell. Syst. 8, 2, Article 15 (June 2018), 36 pages.\nhttps://doi.org/10.1145/3158230\n[57] Jessica Zeitz Self, Radha Krishnan Vinayagam, JT Fry, and Chris North. 2016. Bridging the gap between user intention and model parameters for\nhuman-in-the-loop data analytics. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics. ACM, 3.\n[58] Jessica Zeitz Self, Radha Krishnan Vinayagam, J. T. Fry, and Chris North. 2016. Bridging the Gap Between User Intention and Model Parameters for\nHuman-in-the-loop Data Analytics. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics (San Francisco, California) (HILDA ’16).\nACM, New York, NY, USA, Article 3, 6 pages. https://doi.org/10.1145/2939502.2939505\n[59] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing.\n1631–1642.\n[60] Yves Tillé. 2006. Sampling algorithms. Springer.\n[61] Julio Torales, Marcelo O’Higgins, João Mauricio Castaldelli-Maia, and Antonio Ventriglio. 2020. The outbreak of COVID-19 coronavirus and\nits impact on global mental health. International Journal of Social Psychiatry 66, 4 (2020), 317–320. https://doi.org/10.1177/0020764020915212\narXiv:https://doi.org/10.1177/0020764020915212 PMID: 32233719.\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is\nAll You Need. arXiv:1706.03762 [cs.CL]\n[63] Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. 2017. Deep Metric Learning With Angular Loss. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV).\n[64] Kilian Q Weinberger and Lawrence K Saul. 2009. Distance metric learning for large margin nearest neighbor classification. Journal of Machine\nLearning Research 10, 2 (2009).\n[65] John Wenskovitch, Michelle Dowling, and Chris North. 2020. With Respect to What? Simultaneous Interaction with Dimension Reduction and\nClustering Projections. In Proceedings of the 25th International Conference on Intelligent User Interfaces (Cagliari, Italy) (IUI ’20). Association for\nComputing Machinery, New York, NY, USA, 177–188. https://doi.org/10.1145/3377325.3377516\n[66] John Wenskovitch and Chris North. 2017. Observation-level interaction with clustering and dimension reduction algorithms. In Proceedings of the\n2nd Workshop on Human-In-the-Loop Data Analytics. 1–6.\n[67] Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. Chemometrics and Intelligent Laboratory Systems 2, 1 (1987), 37 –\n52. https://doi.org/10.1016/0169-7439(87)80084-9 Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists.\n[68] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. ArXiv\nabs/1910.03771 (2019).\n[69] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding. arXiv:1906.08237 [cs.CL]\n[70] Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. 2020. Dive into Deep Learning. https://d2l.ai.\n[71] X. Zhu and A. Goldberg. 2009. Introduction to Semi-Supervised Learning. Morgan & Claypool. https://ieeexplore.ieee.org/document/6813505\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.HC"
  ],
  "published": "2023-05-26",
  "updated": "2023-05-26"
}