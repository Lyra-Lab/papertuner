{
  "id": "http://arxiv.org/abs/2305.19804v1",
  "title": "Distance Rank Score: Unsupervised filter method for feature selection on imbalanced dataset",
  "authors": [
    "Katarina Firdova",
    "Céline Labart",
    "Arthur Martel"
  ],
  "abstract": "This paper presents a new filter method for unsupervised feature selection.\nThis method is particularly effective on imbalanced multi-class dataset, as in\ncase of clusters of different anomaly types. Existing methods usually involve\nthe variance of the features, which is not suitable when the different types of\nobservations are not represented equally. Our method, based on Spearman's Rank\nCorrelation between distances on the observations and on feature values, avoids\nthis drawback. The performance of the method is measured on several clustering\nproblems and is compared with existing filter methods suitable for unsupervised\ndata.",
  "text": "DISTANCE RANK SCORE: UNSUPERVISED FILTER METHOD FOR\nFEATURE SELECTION ON IMBALANCED DATASET\nKATARÍNA FIRDOVÁ, CÉLINE LABART, AND ARTHUR MARTEL\nAbstract. This paper presents a new filter method for unsupervised feature selection.\nThis method is particularly effective on imbalanced multi-class dataset, as in case of\nclusters of different anomaly types. Existing methods usually involve the variance of the\nfeatures, which is not suitable when the different types of observations are not represented\nequally. Our method, based on Spearman’s Rank Correlation between distances on the\nobservations and on feature values, avoids this drawback. The performance of the method\nis measured on several clustering problems and is compared with existing filter methods\nsuitable for unsupervised data.\nKeywords :\nunsupervised feature selection, dimension reduction, imbalanced classes\n1. Introduction\n1.1. Motivation. Data preprocessing usually requires a step of dimension reduction for\nvarious reasons. Dataset can initially contain many dimensions if it comes from a domain\nwhere a large number of parameters are measured (e.g.\ngenomics) or if the original\ndimension of the dataset has been expanded by transformation of the initial parameters\nin order to obtain new ones, more meaningful for the analysis purpose. It is the case, for\nexample, when performing automatic feature engineering in chronological data, since it\ncan lead to hundreds of features for each time series.\nBoth cases lead to a situation in which we have to deal with a large multidimensional\ndataset containing many irrelevant variables.\nIf we want to use the data for further\nmodeling, it may be preferable to reduce the dimension, because models with less input\nparameters are:\n(1) faster: computations are less complicated as there are less parameters and then\nless interactions to consider.\n(2) easier to explain: the role of an individual input to the output is easier to describe\nas there is a limited number of inputs.\n(3) usually better: model robustness tends to be higher.\n1.2. Challenges. In this paper we focus on multi-class unsupervised and imbalanced\ndatasets, while the results are also demonstrated on the balanced dataset. We aim to\nselect the most relevant original features keeping information about all present types of\nobservations. More precisely, our feature selection method has the following properties:\nWork on unsupervised data. Labeled observations are rare in practice, it is therefore im-\nportant to propose an unsupervised method, i.e. which does not need to be guided by the\ntype labels.\nDate: June 1, 2023.\n1\narXiv:2305.19804v1  [stat.ML]  31 May 2023\n2\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\nPreserve the variability between different types of observations. One of the challenges of\nthe dimension reduction is to lose as little information about original data as possible while\nkeeping as few variables as possible. In multi-class data, the objective is to reduce the\ndimension while preserving information about all present classes, allowing the clustering\nor other algorithms to correctly separate or analyse the various types of observations.\nBesides, the selection should preserve information on the various types of observations\neven if the size of some groups is very small compared to the size of the whole dataset,\ne.g. cluster containing rare anomalies.\nKeep explainable results. Common unsupervised dimension reduction techniques like PCA\ntransforms the data into a new coordinate system where most of the variation in the data\ncan be described with fewer dimensions that the initial data. This approach is not conve-\nnient if we aim to clarify the contribution of individual parameters to the output, because\nwe cannot retrospectively explain the contribution of each initial parameter. It is prefer-\nable to keep original features.\nThis paper is divided into five parts. Section 2 lays out the existing work in the field.\nSection 3 presents new method for feature selection.\nSection 4 compares the results\nobtained with the new method to those obtained with selected existing methods. The\nexperimental setup is discussed and performance is demonstrated on both balanced and\nimbalanced multi-class datasets. Lastly, section 5 concludes the results and provides the\nfinal remarks.\n2. Related work\nAs described in [KJ97], feature selection methods can be divided in two groups: filter\nmethods and wrapper methods.\nFilter methods evaluate the features importance in the preprocessing step, i.e. indepen-\ndently of the model algorithm. They use various statistical tests and techniques to return\na score for each feature. One of the most typical filter method is Maximum Variance.\nThe variance of each feature is computed and features whose variance is below a chosen\nthreshold are removed as they are considered to be of little importance for the model.\nWrapper methods use a subset of features to train the model. They evaluate the fea-\ntures subset impact to the model performance and add or remove as many features as\nneeded to improve the results. Forward selection is a greedy wrapper method that starts\nwith an empty model and add one variable with the best single improvement at each step.\nMore recent literature mentions embedded methods which combine filter and wrapper\nmethods. Feature selection method is integrated as a part of the learning algorithm.\nRecently, several feature selection algorithms have been designed for imbalanced datasets\nbut many of them are supervised (we refer for example to [Tiw14], [MGIP22], [PS16],\n[KZP+10], [GCM11], [YGX+13]).\nIn [Tiw14], the author extends the RELIEFF algorithm (see [KR92]) to the case of im-\nbalanced datasets. In [MGIP22], the authors use autoencoders ensemble trained only on\nthe majority class to reconstruct all classes. From the analysis of the reconstruction error,\nfeatures with different values distribution on the minority class are selected. [SZDX22]\nDISTANCE RANK SCORE\n3\nproposes a feature reduction method for imbalanced data classification using similarity-\nbased feature clustering with adaptive weighted K-nearest neighbors. [CLFL19] proposes\na feature selection for imbalanced datasets based on neighborhood rough sets. [LYL+18]\nproposes a method based on Weighted Mutual Information.\nPapers proposing an unsupervised method for imbalanced datasets are quite rare (see\n[HC10] and [AHH11]). [HC10] is an unsupervised method for two-class datasets which\nrequires to know the proportion of observations in the minor class and to fix the number\nof relevant features we want to keep. [AHH11] proposes an unsupervised method which\nconsists on removing redundant features. To do so, the authors approximate the proba-\nbility density function (PDF) of each feature and then remove the features having a PDF\nwith a high covering area.\nCorrelation-based approaches for feature selection have been studied in [Hal99], [Lee21],\n[YJS16], [HG07], [PA18] and [CGH22], but without any special interest in imbalanced\nclasses.\nConsidering practical needs and objectives mentioned in the previous section, we focus\non unsupervised filter methods which select the most relevant features. We present in\ndetail below two unsupervised filter methods close to our method.\n2.1. Laplacian score. [HCN05] proposes an unsupervised approach for feature selection,\ncalled Laplacian score.\nThe method assigns a score to each feature according to its\nimportance to keep the local structure. The main idea relies on the fact that a relevant\nfeature should have very similar values on similar observations, i.e. those close to each\nother in the undirected graph.\nFollowing this reasoning, a relevant feature r should\nminimize the Laplacian score\nLaplacianr =\nPn\ni,j=1(Mi,r −Mj,r)2Si,j\nV (M:,r)\n,\n(1)\nwhere M is a data matrix of size n×p, n is the number of observations, p is the number\nof features, Mi,r represents the value of the r-th feature of the i-th observation. S is a\nsimilarity matrix and V (M:,r) is the variance of r−th feature. The role of V (M:,r) is to\nprioritize features with large variance as they have a more representative power.\nThe computed score of a feature aims to reflect its locality preserving power because\nit is assumed that the local structure of the data space is more important for problems\nsuch as classification than its global structure. Similarity matrix in [HCN05] considers\nonly k nearest neighbors, i.e. Si,j = 0 if observation i is not among k nearest neighbors\nof observation j, in order to focus on the local structure. Usually k is chosen to be 5, i.e.\nthe five nearest neighbors are taken into account.\n2.2. Compactness score. Similarly, [ZHWN23] proposes an unsupervised filter method\nfor feature selection based on the idea of keeping the internal structure of the data space.\nAccording to the authors, relevant features should have compact internal structures, i.e.\nminimize distances within its nearest neighbors. Variance is used to indicate the level of\ndivergence of the samples and select those with relatively large variance as they are more\nrepresentative. Based on this argumentation, the Compactness score for the feature r can\n4\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\nbe written as\nCompactness scorer =\nPn\ni=1\nP\nj∈S |Mi,r −Mj,r|\nV (M:,r)\n,\n(2)\nwhere S is the set of k-nearest neighbors of Mi,r, the other notations are kept unchanged.\nThe most relevant features should have the lowest Compactness score.\n3. Methodology\nLaplacian and Compactness scores described in subsections 2.1 and 2.2 meet certain\nrequirements on the suitable method of feature selection - they are unsupervised filter\nmethods based on a comprehensive theory. However, they may lead to information loss\nas the relations between distant observations are not considered.\nAnother disadvantage is the use of the variance as an indicator of a representative fea-\nture. Let us consider the case of a dataset with two imbalanced classes and a feature\nwhich has similar values on the big group, similar values on the small one, but differen-\ntiated values from one group to the other. This feature is important to distinguish the\nsmall group, but it has a small total variance. In case of detection of groups of anomalous\nevents (which are usually rare) this can be an important drawback.\nIdeally, we should consider a feature as relevant if besides its similar behaviour for\nthe close observations it behaves significantly different when the observations are distant.\nThis is the motivation to propose a method which adhere to the mentioned assumption.\n3.1. Distance Rank Score. In order to include the behavior of all observations, we\nsuggest to measure the relation between pairwise total distances and pairwise distances\non a specific feature. This relation is measured by computing the correlation between\nthe ranks of the above mentioned distances. Considering the rank of the distances rather\nthan the distances themselves leads to a robust indicator to extreme values. This type of\nvalues often arise in case of anomaly detection.\nLet us keep the same notations as in subsection 2.1. In the following, N represents\nthe number of pairs of observations, i.e. N =\nn(n−1)\n2\n, (V D\nk )k∈[1,N] is a vector of total\ndistances and (V D\nr )k∈[1,N] is a vector of the distances on the r-th feature.\nFormally,\nwe have V D\nk\n= Pp\nr=1(Mi,r −Mj,r)2 and V r\nk = (Mi,r −Mj,r)2, where (i, j) is such that\nk = n × i + j. Then, we compute Spearman’s Rank Correlation Coefficient between RV D\nand RV r, which are respectively the rank vectors of V D and V r. This can be written as\nDistance Rank Scorer = 1 −\n6\nN(N 2 −1)\nN\nX\nk=1\nd2\nk\n(3)\nwhere d2\nk is the distance between the k-th elements of RV D and RV r.\nThe Spearman’s Rank Correlation can be used for non-linear relationships. Like other\ncorrelation coefficients, values vary between −1 and 1. A strong positive correlation, i.e.\na Distance Rank Score close to 1, suggests a high importance of the considered feature.\n4. Experimental results\nIn this section we demonstrate the feature selection process on several datasets in the\ncase of clustering problems. Although a feature selection as a preprocessing step is not\nDISTANCE RANK SCORE\n5\nlimited only to clustering problems, it is a common approach to illustrate the efficiency\nof unsupervised feature selection methods as we easily evaluate the quality of clustering\nresults before and after the feature selection step.\nWe focus on the problem of imbalanced datasets, i.e. datasets where classes contain\nsignificantly different numbers of observations. These types of datasets arise in the con-\ntext of multi-class anomaly detection. In this case, the objective is twofold: detect the\nanomalies, i.e. separate the clusters of anomalies from the normal one, and distinguish\nthe different types of anomalies, i.e. separate the different clusters of anomalies.\nTo show the efficiency of our feature selection method on imbalanced dataset, we con-\nsider two multi-class types of problems: time series anomaly detection and image anomaly\ndetection. To test the performance of our method on balanced dataset, we consider a clus-\ntering problem in the case of groups of the same size. As we want to compare unsupervised\nmethods we use the label information only for the purpose of the multi-class evaluation.\nTable 1. Data used for demonstration in the following subsections, num-\nber of observations in the dataset (n), number of dimensions (p) after pre-\nprocessing (see subsection 4.2) and number of classes (k).\nName\nn\np\nk\nDataset 1\n500\n1043 10\nDataset 2\n520\n1002\n5\nMNIST5\n1785\n563\n5\norlraws10P\n100\n2740 10\n4.1. Datasets. Dataset 1 contains the characteristics of ten different kinds of two-dimen-\nsional time series. A large sample of periodical series is completed by few series with\nunusual behavior such as constant values or white noise in one or both dimensions. This\ntype of anomalies arise for example in the case of continuous industrial processes. More\ndetails about original series can be found in [CFLM22]. Dataset 1 gathers hundreds of\ncharacteristics for each series, extracted by using the tsfresh [CKLF17] Python package.\nDataset 2 contains the characteristics of five different kinds of two-dimensional time\nseries. The most numerous group contains time series generated by an autoregressive\nprocess with lag 4. They are completed by a few series representing different processes in\nboth dimensions. As Dataset 1, such type of data arise in the case of continuous indus-\ntrial processes. More details can be found in [CFLM22]. Dataset 2 gathers hundreds of\ncharacteristics for each series, extracted using the tsfresh Python package [CKLF17].\nRemark 1. Dataset 1 and Dataset 2 illustrate the utility of the feature selection pro-\ncess. Original datasets are composed of 2-dimensional chronological series. One of the\napproaches to analyse time series is to model their characteristics instead of all original\nvalues. However, when we do not know the behavior of the series, it is challenging to find\na procedure which automatically extracts only the most relevant characteristics. Tools like\ntsfresh [CKLF17] automatically compute hundreds of characteristics from each parameter\n6\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\nof a multidimensional time series and the role of the feature selection algorithm is to de-\ntermine the most important features. Consequently, we can proceed with modeling with a\nreasonable number of characteristics.\nMNIST is a database of greyscale images of handwritten digits which is widely used to\nevaluate machine learning algorithms. Each digit image is normalized to fit into a 28×28\npixel format and is represented by values from 0 to 255 for each pixel. MNIST5 is a subset\nof the original dataset, containing only observations among five digits (from 0 to 4). To\nobtain an imbalanced dataset, we modify MNIST5 to have one major and many minor\nclasses of digits (0 is the major class). Minor classes contain only 10 samples. The major\nclass is unchanged. This modified dataset is named MNIST5m.\nOrlraws10P is a high dimensional dataset from scikit-feature repository in Python. It\ncontains 10 groups with the same number of observations.\n4.2. Data preparation. Features with identical values on all observations are useless in\nclustering algorithm as they do not carry any useful information which can contribute to\nseparate the clusters correctly. Besides, their zero variance may cause problems in denomi-\nnator of Equations (1) and (2). Constant features are therefore removed from the datasets.\nTwo or more highly correlated features do not improve the ability of clustering. In\norder to avoid redundant features in the final selection we remove features with standard\nPearson correlation coefficient bigger than 0.95 in absolute value.\nAs the demonstrated methods use Euclidean metric to compute distances between\nobservations, values should be on the same scale.\nWe use Python MinMaxScaler to\ntransform each feature to a range of [0, 1] to avoid issues with varying magnitudes. The\nchoice of MinMaxScaler is motivated by its high sensitivity to the anomalies compared to\nstandardisation techniques.\n4.3. Evaluation.\n4.3.1. Metrics. The performance of the clustering methods is measured by using three\nwell-known metrics (Accuracy, Normalized Mutual Information and F1 measure) which\ndeal with multi-class clustering. Accuracy and Normalized Mutual Information are suit-\nable metrics for balanced datasets whereas F1 measure is a suitable metric for imbalanced\ndatasets.\nLet CT denote the vector of the true classes and CP denote the vector of predicted\nones, both of length n. Note that predicted classes are adjusted to correspond the true\nclasses by using a permutation mapping function.\n• Accuracy is defined by\nACC =\nPn\ni=1 δ(CTi, CPi)\nn\n,\n(4)\nwhere δ(x, y) = 1 if x = y and 0 otherwise.\n• Mutual Information is defined by\nMI =\nX\nCTi∈CT\nX\nCPi∈CP\np(CTi, CPi)log p(CTi, CPi)\np(CTi)p(CPi),\n(5)\nDISTANCE RANK SCORE\n7\nwhere p(CTi) (resp. p(CPi)) denotes the probability that a randomly chosen ob-\nservation belongs to the cluster CTi (resp. to the cluster CPi) and p(CTi, CPi)\ndenotes the joint probability that a randomly chosen observation belongs to both\nclusters. We use Normalized Mutual Information (NMI) which scales MI to values\nbetween 0 and 1.\n• F1 measure is defined by\nF1 = Precision × Recall\nPrecision + Recall\n(6)\nwhere\nPrecision =\nTrue Positive\nTrue Positive + False Positive\n(7)\nand\nRecall =\nTrue Positive\nTrue Positive + False Negative\n(8)\nIn a multiclass case, we compute a F1 score for each class and then these\nscores are combined to give an overall score.\nMore precisely, the overall score\nis a weighted-F1 score which is a weighted average of all per-class scores. The\nscore of each class is weighted by the number of samples in this class.\nLet (C1, · · · , CM) denote the M clusters, Ni the number of samples of cluster\nCi, N the total number of observations and F1(Ci) the F1 score of cluster Ci. The\nweighted F1 score is given by\nF W\n1\n=\nM\nX\ni=1\nNi\nN F1(Ci)\n(9)\n4.3.2. Clustering evaluation. As mentioned in section 4.1, we consider some datasets con-\ntaining imbalanced classes, more precisely one major and several minor classes. Misclas-\nsification within minor classes can be masked if the observations from the major group\nare clustered correctly. A choice of a proper evaluation metric avoids this obstacle but its\nscore does not give us more details. In the context of multi-class anomaly detection, it\nmay be interesting to know if a misclassification occurs in the major group or within the\nminor groups.\nWe then propose two types of clustering evaluation in case of imbalanced datasets:\n• A global evaluation on the whole dataset using a weighted F1-score (see (9))\n• A two step evaluation which is particularly relevant in case of anomaly detection.\nThis two step evaluation consists in:\n(1) merging all minor clusters and evaluating the clustering performance the ma-\njor cluster vs all other clusters\n(2) considering only minor clusters and evaluating the clustering performance\namong them\nRemark 2. The two step evaluation approach allows us to evaluate the capacity to sepa-\nrate minor groups from the major group as well as the capacity to select relevant features\nwhich enables the separation of minor groups although they are not representative in terms\nof quantity in the original dataset. In case of multi-class anomaly detection, the first step\nconsists in evaluating the performance of the method to separate normal observations to\nanormal ones. The second step consists in evaluating the performance of the method to\nseparate anormal observations between them.\n8\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\nRemark 3. Note that only the evaluation part is divided in two steps. The choice of the\nfeatures and clustering are done on the whole dataset, as illustrated in Figure 1.\nFigure 1. Scheme illustrating the process of 2 steps of the clustering eval-\nuation\n4.3.3. Clustering. The clustering task is done with the K-means algorithm because of its\nefficiency and popularity. The action is repeated 5 times and the results are averaged\nin order to minimise the different initialization effects. The standard deviation is also\nprovided. Note that the clustering is done on all samples without train/test split since\nthe objective is not to evaluate the performance of the clustering algorithm.\n4.4. Number of features. Filter methods for feature selection assign a relevance score\nfor each feature but do not provide a criteria to determine how many features we should\nretain. In general, it is advised to consider only few features to improve computational\ntime and explainability. We propose to examine the evolution of the feature score to\nestimate the suitable number of features to select.\nFigures 2, 3, 4 and 5 display the score evolution of the ranked features for the four\ndatasets. X-axis represents the feature rank in ascending order and y-axis represents the\nscore for a given feature according to the tested method. Note that values are rescaled\nto the range from 0 to 1 and Distance Rank Score values are subtracted from 1 in order\nto be consistent with other methods where a low score indicates a high relevance. One\nway to choose the number of relevant features is to fix a threshold and to select features\nwhose score is below this threshold. The concave shape of Distance Rank and Laplacian\nscores suggests that these methods would lead to select a few number of features, which\nis not the case for Compactness score.\nAnother option than setting a threshold consists in applying an heuristic elbow rule to\nestimate the suitable number of features to select. For example in the case of Dataset 1,\nDataset 2 and orlraws10P and according to the Distance Rank and Laplacian method,\nthe ideal number of features to select is around 100 (see Figures 2, 3 and 5).\nDataset 1 and Dataset 2 are inspired by an explainability problem of anomaly detection\n[CFLM22]. In the following sections we select 20 and 10 features respectively, which seems\nto be a more reasonable quantity to be able to describe and characterize the various\nclusters in a comprehensive way. We also study the performance of the different methods\nwhen the number of selected features is higher, as suggested by the elbow rule.\n4.5. Results - imbalanced case. This subsection is devoted to imbalanced datasets.\nMore precisely, we consider one large homogenous group which is completed by groups\nwith few observations of different kind. The feature selection algorithm needs to find the\nparameters characterizing the clusters of all types of observations, even those which are\nDISTANCE RANK SCORE\n9\nFigure 2. Evolution of score in Dataset 1\nFigure 3. Evolution of score in Dataset 2\nFigure 4. Evolution of score in dataset MNIST5m. Same trend was ob-\nserved for data MNIST5\nFigure 5. Evolution of score in dataset orlraws10P\n10\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\nnot highly represented. Table 2 compares the clustering performance of different methods\non three imbalanced datasets by using the F W\n1 -score (see equation (9)), showing that\nDistance Rank outperforms or is competitive with the other methods.\nAs mentioned in subsection 4.3.2, imbalanced datasets also occur in the context of\nanomaly detection, i.e. when different types of misclassifications have not the same impact\non the studied process. Since the total score does not provide detailed information on\nthe origin of the errors, we display the results of two evaluation steps, as mentioned in\nsubsection 4.3.2.\nTable 3 compares the clustering performance of the methods according to the first step\nof evaluation in case of imbalanced datasets, which can be interpreted as the capacity\nto isolate minor groups from the major group. Distance Rank method is the best or\nis comparable with the other methods according to F W\n1\nmetric for the three considered\ndatasets. It is especially performant on datasets with clearly distinguishable clusters, like\nit is a case on simulated Dataset 1 and Dataset 2.\nTable 4 compares the clustering performance of the methods according to the second\nstep of evaluation in case of imbalanced datasets, which can be interpreted as the capacity\nto select the features characterizing each type of observation, even if some of these obser-\nvations are not highly represented in the considered dataset. The Distance Rank method\ngives better results than the other methods when studying Dataset 1 and MNIST5m.\nSince minor groups are of comparable size, we consider Accuracy and NMI metrics to\nevaluate the performance of the methods.\nTable 2. Clustering performance on whole dataset when using different\nmethods for feature selection.\nPerformance - total\nData name\nDataset 1\nDataset 2\nMNIST5m\nEval. metric\nF W\n1\nF W\n1\nF W\n1\nAll features\n0.33 (0.01)\n0.61 (0.01)\n0.40 (0.01)\nMaxVariance\n0.19 (0.02)\n0.34 (0.04)\n0.41 (0.005)\nLaplacian score\n0.55 (0.004) 0.67 (0.001)\n0.35 (0.02)\nCompactness score\n0.22 (0.003)\n0.37 (0.05)\n0.37 (0.03)\nDistance Rank score 0.98 (0.003)\n0.96 (0.0)\n0.40 (0.02)\nThe computations in Tables 2, 3 and 4 on Dataset 1, Dataset 2 and MNIST5m have\nbeen done with 20, 10 and 30 features respectively. Mean and standard deviation of 5\nexecutions are displayed. Table 5 compares, in the context of anomaly detection, how\nthe performances of the methods change when the number of features increases. Distance\nRank method is the most performant or competitive with other methods which suggest\nthat this method is efficient to determine smaller subsets of relevant features. It is an\nadvantage to have a restricted number of features, especially in a situation when we want\nto describe the various clusters in a comprehensive way and to explain why a group of\nobservations differs from the others.\nDISTANCE RANK SCORE\n11\nTable 3. Major group vs others - Clustering performance when using dif-\nferent methods for feature selection.\nPerformance I\nData name\nDataset 1\nDataset 2\nMNIST5m\nEval. metric\nF W\n1\nF W\n1\nF W\n1\nAll features\n0.31 (0.02)\n0.57 (0.01)\n0.42 (0.02)\nMaxVariance\n0.20 (0.02)\n0.36 (0.04)\n0.43 (0.005)\nLaplacian score\n0.50 (0.004)\n0.62 (0.001)\n0.37 (0.02)\nCompactness score\n0.23 (0.01)\n0.39 (0.05)\n0.39 (0.03)\nDistance Rank score 0.997 (0.003) 0.98 (0.002)\n0.40 (0.02)\nTable 4. Clustering performance within minor groups when using different\nmethods for feature selection.\nPerformance II\nData name\nDataset 1\nDataset 2\nMNIST5m\nEval. metric\nNMI\nACC\nNMI\nACC\nNMI\nACC\nAll features\n0.78 (0.03)\n0.56 (0.05)\n0.86 (0.05)\n0.84 (0.1)\n0.01 (0.02)\n0.26 (0.01)\nMaxVariance\n0.43 (0.04)\n0.33 (0.03)\n0.21 (0.002)\n0.43 (0.02)\n0.01 (0.02)\n0.28 (0.01)\nLaplacian score\n0.84 (0.001) 0.74 (0.001) 0.88 (0.001) 0.89 (0.001) 0.11 (0.03)\n0.30 (0.01)\nCompactness score\n0.49 (0.02)\n0.34 (0.05)\n0.21 (0.01)\n0.45 (0.01)\n0.05 (0.01)\n0.28 (0.01)\nDistance Rank score 0.90 (0.01)\n0.79 (0.02)\n0.90 (0.03)\n0.76 (0)\n0.23 (0.07) 0.43 (0.04)\nTable 5. Clustering performance of the methods according to F W\n1\nmetric\nfor increasing number of features. The two evaluation steps are denoted by\nI and II (see 4.3.2).\nData name\nDataset 1\nDataset 2\nMNIST5m\nN. of features\n5\n10\n20\n30\n50\n75\n100\n5\n10\n20\n30\n50\n75\n100\n5\n10\n20\n30\n50\n75\n100\nMaxVar I\n0.2\n0.19\n0.17\n0.2\n0.17\n0.2\n0.19\n0.42\n0.32\n0.33\n0.32\n0.33\n0.32\n0.34 0.44 0.43\n0.4\n0.43 0.45\n0.38\n0.37\nLaplacian I\n0.3\n0.47\n0.5\n0.6\n0.92 0.82\n0.66\n0.78 0.62\n0.66\n0.53\n0.53\n0.53\n0.53\n0.39\n0.34\n0.38\n0.35\n0.33\n0.43\n0.4\nDistance Rank I\n0.98\n1.\n1.\n1.\n1.\n0.76\n0.64\n0.71 0.98\n1.\n0.99 0.97\n1.\n1.\n0.35\n0.42\n0.38\n0.4\n0.43 0.48 0.46\nCompactness I\n0.18\n0.18\n0.23\n0.22 0.24\n0.25\n0.27\n0.5\n0.36\n0.34\n0.33\n0.33\n0.32\n0.34\n0.43\n0.37\n0.44\n0.39\n0.4\n0.38\n0.37\nMaxVar II\n0.36\n0.36\n0.3\n0.36 0.33\n0.26\n0.26\n0.36\n0.37\n0.4\n0.42\n0.43\n0.42\n0.33\n0.24\n0.3\n0.25\n0.16\n0.17\n0.17\n0.2\nLaplacian II\n0.21\n0.43\n0.67\n0.76\n1.\n1.\n1.\n0.74 0.86 0.98 0.92\n0.73\n0.77\n0.77\n0.15\n0.19\n0.19\n0.19\n0.15 0.19\n0.19\nDistance Rank II 0.65\n0.8\n0.79 0.79 0.75\n0.75\n0.75\n1.\n0.78 0.98\n0.9\n0.99 0.99 0.99 0.25 0.29 0.32 0.36 0.29\n0.14\n0.15\nCompactness II\n0.44\n0.36\n0.24\n0.27 0.23\n0.23\n0.18\n0.35\n0.42\n0.43\n0.47\n0.46\n0.39\n0.39\n0.23\n0.2\n0.2\n0.16\n0.24\n0.15\n0.16\n4.6. Results - balanced case. This subsection deals with balanced datasets, i.e. we\nconsider several types of observations while each type is represented equally. The ob-\njective is to reduce the number of original variables to continue the analyses with an\n12\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\nacceptable number of variables.\nRemark 4. We emphasize that studying balanced datasets is not equivalent to the second\nstep of evaluation presented above. In case of imbalanced datasets the relevant features\nare selected once for all when considering all observations (major and minor groups).\nThe case of balanced datasets would be equivalent to the second step of evaluation if new\nrelevant features were selected when considering only abnormal observations.\nTable 6 compares the clustering performance of the methods in the case of two balanced\ndatasets. We notice that in this case the Distance Rank method is not the most powerful\none, regarding either MNIST5 or orlraws10P. Regarding MNIST5, we obtain the best\nresults using all available features.\nIt may suggest that the choice of the number of\nfeatures is not suitable and has led to a loss of information.\nTable 6. Efficiency of clustering using different methods for feature selec-\ntion. Mean and standard deviation of 5 executions are displayed. 100 and\n30 features were used for orlraws10P and MNIST5, respectively.\nClustering performance - balanced clusters\nData name\norlraws10P\nMNIST5\nEval. metric\nNMI\nACC\nNMI\nACC\nAll features\n0.79 (0.03)\n0.71 (0.05) 0.70 (0.002) 0.87 (0.001)\nMaxVariance\n0.81 (0.02) 0.72 (0.04)\n0.64 (0.001) 0.48 (0.0003)\nLaplacian score\n0.79 (0.01) 0.73 (0.03) 0.49 (0.001)\n0.65 (0.001)\nCompactness score\n0.80 (0.03) 0.73 (0.04) 0.44 (0.002)\n0.62 (0.001)\nDistance Rank score 0.68 (0.03)\n0.59 (0.05)\n0.3 (0.001)\n0.49 (0.004)\n4.7. Optimisation. Laplacian and Compactness methods consider the local neighbors\nof each observation to compute the feature scores. The search of the nearest neighbors is\noptimised and thus these methods gain an advantage in terms of computational time in\ncomparison with methods such as Distance Rank which consider all observations for the\ncomputations. The complexity of the Distance Rank method grows linearly with number\nof dimensions but grows in a quadratic way in the number of observations.\nIn order to reduce the computational time of our method, we examine the efficiency of\nthe feature selection when considering data subsets. Ideally, if the observations chosen\nto subsample are selected randomly and if the subsample is not too small, each type of\nobservation should be represented, hence the algorithms should have enough information\nto determine the relevant features.\nTable 7 shows the results of clustering when the features are selected using Distance\nRank and Laplacian score when using 50 % and 30 % of the observations of the original\ndatasets. To present concise results, only Laplacian score, which is the most competitive\none of the tested methods, has been kept. For simplicity, we display the results of the first\nDISTANCE RANK SCORE\n13\nevaluation step in case of imbalanced classes measured with F W\n1\nmetric. Other results have\nshown the same trend. We notice that the efficiency of our feature selection method on\nsubset depends on the dataset, but does not decrease significantly with the subsampling.\nFurther research on optimisation of the computation time without efficiency loss is needed\nto make the new method more competitive.\nTable 7. Comparison of the computational time and clustering perfor-\nmance when the process of feature selection is done on a subset. The F W\n1\nscore on subsamples is averaged on 5 executions, each execution using a dif-\nferent subsample for feature selection while the performance is measured on\nwhole dataset. Time is measured in seconds as an average of 5 executions\nof feature selection algorithm on a subset of corresponding size. As in the\nprevious section, 20, 10 and 30 features have been selected for Dataset 1,\nDataset 2 and MNIST5m, respectively.\nData name\nDataset 1\nDataset 2\nMNIST5m\nMethod\nDistance Rank\nLaplacian\nDistance Rank\nLaplacian\nDistance Rank\nLaplacian\nEval. metric\nF W\n1\nTime\nF W\n1\nTime\nF W\n1\nTime\nF W\n1\nTime\nF W\n1\nTime\nF W\n1\nTime\n100 %\n0.99\n17.6\n0.50 0.046 0.98\n21.70\n0.61 0.045 0.41\n3.2\n0.37 0.022\n50 %\n0.99\n4.25\n0.46 0.015 0.99\n4.69\n0.60 0.016 0.43\n0.84\n0.40 0.008\n30 %\n0.93\n1.61\n0.55 0.008 0.94\n1.73\n0.45 0.009 0.42\n0.350\n0.44 0.006\n5. Conclusion\nThis paper has presented a filter method for unsupervised feature selection based on\nSpearman’s Rank Correlation between distances on the observations and on feature val-\nues. Our method is particularly efficient in case of imbalanced datasets. Moreover, the\nexplainability of the method is reinforced compared to other methods since only a few rel-\nevant features are needed to achieve a good performance. More research on the automatic\ndetermination of the suitable number of features and optimisation of the computational\ntime is needed. Improvements of the performance in case of balanced datasets is also an\nobjective of further research.\nReferences\n[AHH11]\nMina Alibeigi, Sattar Hashemi, and Ali et al. Hamzeh. Unsupervised feature selection based\non the distribution of features attributed to imbalanced data sets. International Journal of\nArtificial Intelligence and Expert Systems, 2:2011–14, 2011.\n[CFLM22] Mathieu Cura, Katarina Firdova, Céline Labart, and Arthur Martel. Explainable multi-class\nanomaly detection on functional data. https://arxiv.org/abs/2205.02935, 2022.\n[CGH22]\nCheng-Han Chua, Meihui Guo, and Shih-Feng Huang. Using the kriging correlation for un-\nsupervised feature selection problems. Scientific Reports, 12(11522), 2022.\n[CKLF17]\nM. Christ, A. W. Kempa-Liehr, and M. Feindta. Distributed and parallel time series feature\nextraction for industrial big data applications. https://arxiv.org/abs/1610.07717, 2017.\n[CLFL19]\nHongmei Chen, Tianrui Li, Xin Fan, and Chuan Luo. Feature selection for imbalanced data\nbased on neighborhood rough sets. Information Sciences, 483:1–20, 2019.\n[GCM11]\nAngélica Muñoz-Meléndez German Cuaya and Eduardo F. Morales. A minority class feature\nselection method. In Iberoamerican Congress on Pattern Recognition, Springer, 2011.\n[Hal99]\nMark A. Hall. Correlation-based feature selection for machine learning. PhD thesis, 1999.\n14\nK. FIRDOVÁ, C. LABART, AND A. MARTEL\n[HC10]\nJingrui He and Jaime Carbonell. Co-selection of features and instances for unsupervised rare\ncategory analysis. Statistical Analysis and Data Mining, 3:417–430, 12 2010.\n[HCN05]\nXiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. In NIPS’05:\nProceedings of the 18th International Conference on Neural Information Processing Systems,\n2005.\n[HG07]\nMichael Edward Houle and Nizar Grira. A correlation-based model for unsupervised feature\nselection. In Proceedings of the Sixteenth ACM Conference on Conference on Information\nand Knowledge Management, CIKM ’07, page 897–900, 2007.\n[KJ97]\nRon Kohavi and George H. John. Wrappers for feature subset selection. Artificial Intelligence,\n97(1):273–324, 1997.\n[KR92]\nKenji Kira and Larry A. Rendell. The feature selection problem: Traditional methods and\na new algorithm. In Proceedings of the Tenth National Conference on Artificial Intelligence,\nAAAI’92, page 129–134. AAAI Press, 1992.\n[KZP+10]\nAbu Kamal, Xingquan Zhu, Abhijit Pandya, Sam Hsu, and Ramaswamy Narayanan. Feature\nselection for datasets with imbalanced class distributions. International Journal of Software\nEngineering and Knowledge Engineering, 20:113–137, 2010.\n[Lee21]\nJoshua Ka-Wing Lee. Maximal correlation feature selection and suppression with applications.\nPhD thesis, 2021.\n[LYL+18]\nKewen Li, Mingxiao Yu, Lu Liu, Timing Li, and Jiannan Zhai. Feature selection method based\non weighted mutual information for imbalanced data. In International Journal of Software\nEngineering and Knowledge Engineering, 2018.\n[MGIP22]\nMichela Carlotta Massi, Francesca Gasperoni, Francesca Ieva, and Anna Maria Paganoni.\nFeature selection for imbalanced data with deep sparse autoencoders ensemble. Stat. Anal.\nData Min., 15(3):376–395, 2022.\n[PA18]\nTanuja Pattanshetti and Vahida Z. Attar. Unsupervised feature selection using correlation\nscore. Advances in Intelligent Systems and Computing, 2018.\n[PS16]\nHemlata Pant and Reena Srivastava. Mindex ib: A feature selection method for imbalanced\ndataset. In International Journal of Advanced Research in Computer and Communication\nEngineering, 2016.\n[SZDX22]\nLin Sun, Jiuxiao Zhang, Weiping Ding, and Jiucheng Xu. Feature reduction for imbalanced\ndata classification using similarity-based feature clustering with adaptive weighted k-nearest\nneighbors. Information Sciences, 593:591–613, 2022.\n[Tiw14]\nDeepika Tiwari. Handling class imbalance problem using feature selection. In International\nJournal of Advanced Research in Computer Science and Technology, 2014.\n[YGX+13] Liuzhi Yin, Yong Ge, Keli Xiao, Xuehua Wang, and Xiaojun Quan. Feature selection for\nhigh-dimensional imbalanced data. Neurocomputing, 105:3–11, 04 2013.\n[YJS16]\nXiucai Ye, Kaiyang Ji, and Tetsuya Sakurai. Unsupervised feature selection with correlation\nand individuality analysis. In International Journal of Machine Learning and Computing,\n2016.\n[ZHWN23] Peican Zhu, Xin Hou, Zhen Wang, and Feiping Nie. Compactness score: A fast filter method\nfor unsupervised feature selection. Annals of Operations Research, 2023.\nUniv.\nGrenoble Alpes, Univ.\nSavoie Mont Blanc, CNRS, LAMA, 73000 Chambéry,\nFrance\nOptimistik, 536 rue Costa de Beauregard, 73000 Chambéry, France\nEmail address: katarina.firdova@optimistik.fr\nUniv.\nGrenoble Alpes, Univ.\nSavoie Mont Blanc, CNRS, LAMA, 73000 Chambéry,\nFrance\nEmail address: celine.labart@univ-smb.fr\nOptimistik, 536 rue Costa de Beauregard, 73000 Chambéry, France\nEmail address: arthur.martel@optimistik.fr\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2023-05-31",
  "updated": "2023-05-31"
}