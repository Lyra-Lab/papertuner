{
  "id": "http://arxiv.org/abs/2306.00687v1",
  "title": "Adversarial Robustness in Unsupervised Machine Learning: A Systematic Review",
  "authors": [
    "Mathias Lundteigen Mohus",
    "Jinyue Li"
  ],
  "abstract": "As the adoption of machine learning models increases, ensuring robust models\nagainst adversarial attacks is increasingly important. With unsupervised\nmachine learning gaining more attention, ensuring it is robust against attacks\nis vital. This paper conducts a systematic literature review on the robustness\nof unsupervised learning, collecting 86 papers. Our results show that most\nresearch focuses on privacy attacks, which have effective defenses; however,\nmany attacks lack effective and general defensive measures. Based on the\nresults, we formulate a model on the properties of an attack on unsupervised\nlearning, contributing to future research by providing a model to use.",
  "text": "Adversarial Robustness in Unsupervised Machine Learning: A Systematic\nReview\nMATHIAS LUNDTEIGEN MOHUS and JINGYUE LI, NTNU Department of Computer Science, Norway\nAs the adoption of machine learning models increases, ensuring robust models against adversarial attacks is increasingly important.\nWith unsupervised machine learning gaining more attention, ensuring it is robust against attacks is vital. This paper conducts a\nsystematic literature review on the robustness of unsupervised learning, collecting 86 papers. Our results show that most research\nfocuses on privacy attacks, which have effective defenses; however, many attacks lack effective and general defensive measures. Based\non the results, we formulate a model on the properties of an attack on unsupervised learning, contributing to future research by\nproviding a model to use.\nCCS Concepts: • Computing methodologies →Unsupervised learning; • Security and privacy →Software security engi-\nneering.\nAdditional Key Words and Phrases: unsupervised learning, machine learning, adversarial robustness, adversarial attack, systematic\nliterature review\nACM Reference Format:\nMathias Lundteigen Mohus and Jingyue Li. 2023. Adversarial Robustness in Unsupervised Machine Learning: A Systematic Review. 1,\n1 (June 2023), 38 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nAs technologies mature, hardware capabilities increase, and legislation is introduced, adopting machine learning\ntechnologies in various applications seems inevitable, with the technology generally divided into three types: Supervised\nLearning (SL), Reinforcement Learning (RL), and Unsupervised Learning. SL focuses on approaches where the training\ndata is labeled, i.e., a data sample maps to a correct value. A variety of cases use this, e.g., object detection [7], image\nclassification [113], and voice recognition [6]. RL is an approach where the model trains in the working environment,\nrewarding or punishing the model based on its performance. Use cases for this are bug detection in software [119],\ncomputing resource optimization [131], and control of cyber-physical systems [96]. UL performs training of its models\nwithout guidance; pure data are used as the basis for training. Use cases for this is data generation [45], e.g., GANs;\nencoding/decoding [133], e.g., autoencoders , and data clustering [29].\nBy its lack of need for guided information, UL can enable more data to be usable. In a world that generates a large\namount of data (around 2.5 quintillion bytes each day [46]), methods for using unsupervised data would be invaluable.\nHowever, where technology is used, adversaries inevitably attempt to exploit vulnerabilities in these technologies. With\nthe increased use of ML in various fields, it is crucial to explore the adversarial robustness of ML models, particularly\nthe use of UL, as this field has received less focus than supervised and reinforcement learning.\nAuthors’ address: Mathias Lundteigen Mohus, mathias.l.mohus@ntnu.no; Jingyue Li, jingyue.li@ntnu.no, NTNU Department of Computer Science, Sem\nSælandsvei 9, Trondheim, Trøndelag, Norway, NO-7491.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2023 Association for Computing Machinery.\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2306.00687v1  [cs.LG]  1 Jun 2023\n2\nMohus et. al.\nLiu et al. [93] summarized attacks on machine learning models broadly, while Guan et al. [63] cover machine learning\nwithin cybersecurity. Zhang et al. [155] outline privacy vulnerabilities in machine learning, which could reveal sensitive\ninformation used to train synthetic data generators in the context of UL. At the same time, Martins et al. [99]cover\nadversarial examples relating to detection techniques, where the input to an ML model is perturbed in a specific way in\norder to disrupt the functionality of the model.\nIn contrast with Liu et al. [93], Guan et al. [63] and Zhang et al. [155], where parts of UL adversarial robustness are\ncovered, we aim to conduct a Systematic Literature Review (SLR) on the topic of adversarial robustness within the\nwhole field of unsupervised machine learning. The papers focus on answering the three research questions below:\n• RQ1: What types of attacks exist against UL-based systems, and what are the common attributes of the attacks?\n• RQ2: What defensive countermeasures exist for attacks against UL-based systems?\n• RQ3: What are the challenges to defending against possible attacks targeting UL-based systems?\nThe SLR follows the guidelines outlined by Keele et al. [79] and identified 86 papers relevant to adversarial robustness\nin unsupervised learning. Thematic analysis was then used on these papers to summarize the papers according to the\nRQs systematically.\nThe results of SLR reveal that:\n• GANs are mostly targeted by privacy attacks.\n• AutoEncoders are mostly targeted by adversarial examples.\n• Poisoning attacks are used against most types of UL.\n• Most attacks utilize distance metrics in limiting how much the attack alters samples for the attack.\n• Several defensive methods that work in a supervised or reinforcement context are not applicable in an unsuper-\nvised context. For example, fine pruning and output activation clustering are not suitable defenses for generative\nmodels.\n• The adversary’s knowledge of the target system greatly affects the attack’s performance.\n• There is a noticeable lack of defensive methods used in UL, except for differential privacy.\nThe contributions of the study are:\n• We provide a complete and novel summary of the state-of-the-art adversarial robustness of unsupervised learning\nrelating to attributes of the attack, target, and defense.\n• We have summarized and outlined the main challenges facing the adversarial robustness of unsupervised learning.\n• We create a model of the attributes associated with attacks against unsupervised machine learning to analyze\nand compare adversarial attacks against UL.\nThe rest of the paper is organized as follows. Section 2 covers the basics of unsupervised learning. Section 3 covers\nsurveys related to the security and robustness of machine learning. Section 4 explains our research method. Sections 5\nto 7 present the results. Section 8 discusses the results. Section 9 concludes and proposes future work.\n2\nBACKGROUND\nWithin UL, two types of technologies generally exist: discriminatory and generative. Discriminatory covers models\nwhere data are input into the model, producing a specific output based on its training. Examples of these are clustering\n[61, 65], AutoEncoders [13, 117] and super-resolution networks [9].\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n3\n• Clustering refers to a broad family of methods that defines clusters of data points based on certain heuristics.\nUnsupervised clustering finds these clusters based on distance metrics between data points to find which data\npoints are mathematically close enough to belong to the same cluster. Clustering is often used for data analysis\nin data-heavy fields, such as gene information [135].\n• AutoEncoders is a method for training a neural network to create an inner representation of the training data,\nsince the model trains to output the same data as the input. In most cases, AutoEncoders force a dimensionality\nreduction on the inner representation by limiting the number of hidden nodes in the network, where the first\npart of the network training is to create the encoding, and the last part of the network training is to decode\nthe representation into the original data. One application of AutoEncoders is denoising data [13] where data is\npassed through the AutoEncoder to remove noise.\n• Super-resolution networks are neural networks that work to increase the dimensionality of the data, by training\nthe model to output a higher-dimensional piece of data, based on a lower-dimensional piece of data, e.g. a\nlow-resolution image. Super-resolution is often used in image technologies to upscale the resolution of an image\n[9].\nGenerative models cover models which do not directly use input data but are used to create new data with properties\ntied to the properties of the training set. Examples of this are Variational AutoEncoders [81] and Generative Adversarial\nNetworks [41, 59, 144].\n• Variational AutoEncoder is a method to use AutoEncoder to generate data by forcing constraints on the inner\nrepresentation of the AutoEncoder, in turn making it possible to input random noise into the decoder part of the\nAutoEncoder and producing an output that is statistically similar to the training set used to train the AutoEncoder.\nThis model is one of the main components of generating deepfake images by supplanting some of the encoded\nfeatures of an image with another, generating an image with features from both images Rocco [122].\n• Generative adversarial networks consist of two models, the generator and the discriminator. The generator is a\nneural network with random noise as input, while the discriminator is a classifier trained to differentiate between\nthe output from the generator and the training data. The generator and the discriminator are, in turn, trained\nagainst each other based on the performance of the discriminator. GANs are used for various synthetic data\ngeneration, e.g., medical data generation [66].\n3\nRELATED WORK\nAs the field of machine learning security has matured, several papers have been published that survey the state of the\nart within both broad and narrow fields within ML security.\nLiu et al. [93] use the taxonomy from Fredrikson et al. [54] to analyze 42 papers on attacks against machine learning\nmodels. The survey group the papers based on 1) The targeted algorithm, with the types being: Naïve Bayes/Logistic\nRegression/Decision Tree, Support Vector Machines (SVM), Principal Component Analysis (PCA)/Least Absolute\nShrinkage and Selection Operator (LASSO), Clustering, and Deep Neural Network (DNN), and 2) The type of attack,\nwith the types being: Poisoning Attack, Evasion Attack, Impersonation Attack, and Inversion Attack. Liu et al. [93]\nalso focus explicitly on comparing types of Adversarial Examples attacks, comparing the techniques, advantages,\ndisadvantages, and formations of the adversarial examples. Additionally, Liu et al. [93] summarize defensive techniques\nfor ML, outlining how its surveyed papers assess the security of their ML models and which defensive mechanisms\nare put in place: proactive or reactive. Defenses in training are also discussed, such as Data Sanitation, Bootstrap\nManuscript submitted to ACM\n4\nMohus et. al.\nAggregating, Random Subspace Method, and designing more robust algorithms. For defenses in the testing/inferring\nphase, Liu et al. [93] state that “countermeasures in the testing/inferring phase mainly focus on the improvement of\nlearning algorithms’ robustness,” describing techniques like Game Theory, Adversarial Training, Defensive Distillation,\nDimension Reduction, as well as ensemble methods. Lastly, Liu et al. [93] also cover the privacy of data used in ML,\nincluding Differential Privacy and Homomorphic Encryption.\nMartins et al. [99] perform a survey on the use of adversarial examples targeted towards detection techniques,\nparticularly malware (11 papers) and intrusion detection (nine papers), surveyed from mainly IEEE Xplore, Springer,\narXiv, ScienceDirect, Research Gate, and Google, using the keywords “Intrusion Detection,” “Malware Detection” and\n“Adversarial Machine Learning.” The paper compares several features recurring in its literature: Attack Techniques,\nClassifiers, Datasets, Metrics, and Results and Conclusions. Martins et al. [99] also cover defensive techniques against\nadversarial examples: Adversarial Training, Gradient Masking, Defensive Distillation, Feature Squeezing, Universal\nPerturbation Defense Method, and MagNet.\nGuan et al. [63] cover two related topics: ML’s impact on cybersecurity and AI systems’ robustness, presenting 14\npapers. The paper labels attributes covering the Research Direction, whether AI for cybersecurity or security of AI,\npoints of innovation in the paper, and a summary of the idea for each paper. The paper also synthesizes common\nthreats and solutions in AI security, such as Data Sanitation and Learning Algorithm Improvement being the solution to\nPoisoning attacks, Adversarial Training and Output Smoothing being solutions to Evasion and Impersonation Attacks,\nand Differential Privacy and Homomorphic Encryption being the solution to Inversion Attacks.\nZhang et al. [155] present a survey on privacy threats and mitigations within ML. The paper outlines a taxonomy\nfor attacks, covering the Stage (where the attack happens), Goal, Knowledge, Capability, and Strategy of the attack.\nThe authors summarize privacy threats against models as Parameter/Function, Hyperparameter, and Architecture,\nwith defensive measures being Confidence Perturbation and Malicious Query Behaviour Detection. Privacy threats\nagainst data are summarized as Data Memorization attacks, Model Inversion Attacks, and Membership Inference\nattacks, with defensive measures being Hierarchical Model Stacking Techniques, Attribute Correlation with Adversarial\nPerturbations, Meta-Classifier, Collaborative Learning, Homomorphic Encryption, and Differential Privacy.\nTable 1 outlines the contribution of the existing surveys. Only one survey covers GAN [155], two surveys cover\nclustering [93, 99], and none covers attacks on AutoEndoers. The systematic description of attack goals is also only\npresent in two papers [63, 93]. There is also a need for systematic metrics descriptions to determine attack success,\nwith only one paper [99] summarizing the metrics. Two papers cover the attacker’s knowledge, while the other two\nsomewhat systematically cover the knowledge. Additionally, three papers cover attacks in training and production\nsystems, while Martins et al. [99] do not cover attacks in training scenarios. Finally, all the papers cover defensive\nmeasures against attacks.\nIn summary, there is an evident lack of a systematic and comprehensive survey focusing on attacks targeting all kinds\nof ULs and corresponding defenses. Considering the potential importance of unsupervised learning in environments\nwith a large amount of unlabeled data, a survey to cover the adversarial robustness of unsupervised machine learning\nwould provide the industry with insight into the shortcomings of UL technologies and academia with knowledge on\nthe state-of-the-art attacks and defenses that are covered in the field of adversarial robustness within UL.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n5\nTable 1. Comparison between the four surveys and this paper regarding categories covered and whether the survey is performed\nsystematically. ✓is when the paper covers the category, ✗is when the paper does not cover the category, and * is when the category\nis partly covered by the paper\nRef.\nShort Description\nSystematic\nAttack Goal\nAttack Knowledge\nAttack Focus\nMetrics\nTarget Types\nDefenses\nProduction\nTraining\nGAN\nAutoEncoder\nClustering\nClassification\nSuper-resolution\n[93]\nAdversarial Examples attacks against Ma-\nchine Learning\n✗\n✓\n*\n✓\n✓\n✗\n✗\n✗\n✓\n✗\n✗\n✓\n[99]\nAdversarial Examples against detection\ntechniques\n*\n✗\n✓\n✓\n✗\n✓\n✗\n✗\n✓\n✗\n✗\n✓\n[63]\nImpact of ML on security, and robustness\nof AI systems\n✗\n✓\n*\n✓\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n[155]\nPrivacy threats and mitigations within\nML\n✗\n✗\n✓\n✓\n✓\n✗\n✓\n✗\n✗\n✗\n✗\n✓\nThis\nwork\nA systematic review on adversarial ro-\nbustness of unsupervised machine learn-\ning\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n4\nRESEARCH DESIGN AND IMPLEMENTATION\n4.1\nSystematic Literature Review outline\nIn computer science, Keele et al. [79] show one framework for conducting an SLR within software engineering and\nrelated fields. This SLR uses the method outlined in Keele et al. [79] as a base, which includes 1) Creating the search\nquery based on inclusion criteria, 2) Filtering based on exclusion criteria, and 3) Validating the quality of papers.\n4.2\nCreating the search query\nBased on the research questions and research focus, we generate the search query using a modified version of PICOC\n(Population, Intervention, Comparison, Outcome, Context) from Petticrew and Roberts [114], without the Comparison\nsection, as it was considered not applicable to the context of this SLR. While the PIOC framework groupings guide the\nquery’s creation, the specific interpretation in Keele et al. [79] do not map perfectly to the chosen tokens, as described\nbelow.\n• Population: Relating to the security of unsupervised machine learning, the population will cover general areas of\ntechnologies that are related to or specifically UL.\n• Intervention: The intervention relates here to general descriptions of how an attacker might exploit vulnerabilities\nin UL systems.\n• Outcome: Outcome relates to the desired data or information that a piece of research produces through their\npapers, specifically finding the vulnerabilities and defensive methods of UL methods.\nManuscript submitted to ACM\n6\nMohus et. al.\nTable 2. Evaluation of different queries\nSearch query evaluation round\nPeer-reviewed\nPre-published\nTotal\nMatch with ad-hoc list\n1\n529\n39\n568\n8%\n2\n120 241\n763\n121 004\n85%\n3\n4623\n26\n4649\n15%\n4\n2647\n57\n2704\n46%\n• Context: The context relates to the context in which the research is explored, specifically which part the UL is\nconsidered, e.g., the technology versus human error.\nAs Keele et al. [79] do not outline a framework for evaluating the quality of the search query, other than following\nthe PICOC framework, while Denyer et al. [44] only state “it is often wise to spend significant time on constructing the\nsearch strings, for this investment can improve the efficiency of the search”, we formulate here a method for evaluating\nthe query. First, a list of 13 papers relating to security in UL was created, based on an ad-hoc search, in addition\nto previous knowledge of specific papers. We then search the IEEE Xplore, ACM Digital Library, arXiv, and Scopus\ndatabases using the search query and note how many of the papers in the ad-hoc list are present in the resulting list.\nWhile the ad-hoc nature of the list is not ideal, we consider this method better than the alternative, i.e., not having a\nbaseline of comparison, as we know the papers in the ad-hoc list should ideally be present in the resulting list. The\nsecond consideration metric was the number of papers produced by the search query. For this, we would only accept\nqueries that produced between 1000 and 10 000 papers to ensure enough relevant papers were included and ensure the\nfollowing filtration process would not take too long. The results of the query testing are summarized in Table 2. Rounds\n1 and 2 were good examples of casting a too-narrow and too-wide net, highlighted by the matching percentage to the\nad-hoc list and the total number of papers being very low and very high, respectively. Round 3 produced the correct\nnumber of papers but was considered not precise enough, with a matching score of 15%. Round 4 also produced the\ncorrect number of papers, increased the matching rate to the ad-hoc list, and was therefore chosen as the query for the\nSLR.\nThe resulting query based on the PIOC categories produces the following search query, modified to fit each database\nsearch engine:\n(“Clustering” OR “cluster” OR “AutoEncoder” OR “GAN” OR “Generative Adversarial Network” OR “Unsupervised\nLearning” OR “UL”) AND (“vulnerability” OR “exploitation” OR “Poisoning” OR “extraction” OR “obfuscation” OR\n“Adversarial attack” OR “cyber-attack” OR “cyber attack”) AND (“Increase robustness” OR “defend against” OR “increase\ndefence” OR “Mitigate attack” OR “Attack success” OR “attack against” OR “attack vector”) AND (“model” OR “models”\nOR “system” OR “systems”)\n4.3\nPaper filtering\nThe initial list of papers was generated using the query in IEEE Xplore, ACM Digital Library, arXiv, and Scopus.\nIncluding unpublished papers in this review follows from the recommendation from Denyer et al. [44] “extensive\nsearches are conducted to include both published and unpublished studies.” The initial list was imported into EndNote,\nautomatically removing any duplicates. We used the inclusion criterion for filtering the papers: the paper must cover\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n7\nattacks against UL technologies. We show the exclusion criteria in Table 3. We specify criteria E3 to 1) avoid including\noutdated research and 2) limit the number of papers that would have to be filtered.\nTable 3. The exclusion criteria for the systematic review\nIndex\nCriteria\nE1\nThe paper covers attack against general technologies which is not cur-\nrently used in an unsupervised manner\nE2\nNot written in English\nE3\nWritten before 2011\nE4\nPaper is not available\nE5\nNot relevant to UL\nWe performed several rounds of filtering. The first round focused on removing irrelevant papers from the list. In this\nround, the papers were considered only based on the title and evaluated on whether 1) It does not relate to security, 2)\nIt does not relate to UL, or 3) The title singularly refers to supervised or reinforcement learning. The second round\nfocused on further limiting the number of irrelevant papers from the list. The evaluation criteria remain the same\nas for filtration round 1. Still, we give the evaluation for each paper more time, and we document each decision to\nfilter in or out. The third round limits the number of irrelevant papers even further, where we use the same criteria\nas filtration rounds 1 and 2, use more time for each evaluation and consider the paper’s abstract. Here, the evaluator\ncan also consider a paper in its entirety to confidently ascertain whether it is relevant or not, according to the criteria\nreferred to here as round 3.5. The workflow for the filtration steps and quality assessment is outlined in Figure 1.\nFig. 1. Workflow for the filtration rounds and quality assessment\n4.4\nQuality Assessment\nTo ensure the papers contributing to the SLR have a certain quality, Keele et al. [79] also outline the importance of\nquality assessment (QA). Our QA consists of 11 questions based on the approach presented by Keele et al. [79], scoring\n0, 0.5, or 1 point per question. For any paper scoring lower than 5.5 points, we exclude the paper. Additionally, to ensure\ninternal validity, a random sample of papers will be analyzed using the Cohen Kappa statistic [145], measured between\nthe primary researcher doing the QA, and a separate researcher, to ensure the QA scores are accurately applied. The\nquestions used for the QA are in Appendix A.\nManuscript submitted to ACM\n8\nMohus et. al.\n4.5\nSnowballing\nAfter the initial round of search, filtration, and QA, we also perform two backward and forward snowballing rounds to\nensure we include as many relevant papers as possible in this SLR. For the first round of snowballing, the final list of\npapers after the QA is used as a base, and for the second round, we use the list of papers after QA from the first round\nof snowballing. Backward snowballing (BS) is performed by compiling a list of all the references used in each paper. In\ncontrast, forward snowballing (FS) is performed using Semantic Scholar to find papers referencing each paper. After one\nround of initial search and two rounds of forward and backward snowballing, we combine the papers into a single list\nof papers to be analyzed.\n4.6\nIdentified papers\nFor each round of the systematic literature review, the number of papers for consideration is summarized in Table 4.\nAfter the snowballing steps, we further reduced the list of 114 papers to 86 by finding duplicates, updated versions, or\npublished duplicates of unpublished papers, with the last forward snowballing round performed in August 2022.\nTable 4. Summary of the number of papers after each round of filtration and quality assurance. We base each snowballing round on\nthe list of papers from the quality assurance in the previous round.\nInitial\nRound\nForward\nSnowballing 1\nBackward\nSnowballing 1\nForward\nSnowballing 2\nBackward\nSnowballing 2\nSum\nInitial List\n5825\n996\n365\n683\n1260\n9129\nFiltration 1\n1002\n651\n160\n95\n171\n2079\nFiltration 2\n478\n330\n75\n53\n36\n972\nFiltration 3\n28\n54\n15\n37\n11\n145\nAfter Quality Assurance\n24\n46\n11\n27\n6\n114\n4.7\nData analysis and synthesis\nThematic analysis (TA) is a widely used method for analyzing qualitative data [23], as it “offers an accessible and\ntheoretically flexible approach to analyzing qualitative data.” Performing a TA consists of coding sections of the\nqualitative data based on its content and categorizing the codes into different themes, which in turn is used for\ndescribing and comparing data. Two main ways of performing the TA are either by grounding it in the content or using\npredetermined codes. In grounding the TA in the content, the codes and themes emerge from the data itself instead\nof being based on a theory of which codes should exist [143]. The grounded codes can, in turn, be used for creating\nhypotheses or theories based on the data. Using predetermined codes revolves around codes and themes emerging from\npredetermined assumptions, hypotheses, and knowledge, on the subject, and the codes and themes used within this for\nresearch based on these preconceptions. Grounded and predetermined codes can be combined into a hybrid process,\nwith specific predetermined codes and themes complemented by grounded codes to generate the finished set of codes\nand themes.\nThe workflow for our applied thematic analysis is as described in Figure 2. The predetermined list of codes relating\nto the RQs, derived from our initial understanding of UL and its security risks and defenses, are as follows:\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n9\n• Clustering\n• GAN\n• AutoEncoder\n• Training\n• Production\n• Model\n• Confidentiality\n• Privacy attacks\n• Integrity\n• Poisoning\n• Adversarial\nExamples\n• Disruption\n• Privacy\n• Self-driving cars\n• Intrusion detection\nFig. 2. Workflow for the thematic analysis of the produced papers\nFor the analysis, we also split the coding into two sections, grounded and predetermined coding. The grounded coding\ncovers the initial set of papers and is used to establish the primary set of grounded codes. Here, we code each paper\nthoroughly and in-depth. The predetermined coding is performed on the two rounds of snowballing papers and is used\nto cover the general content of the papers using the established codes. For each paper, the codes relating to the RQs are\nguaranteed to be coded in this process. While this produces less detail because of the scope of the paper focusing on the\nbig picture of robustness in UL, we consider this a worthy tradeoff to speed up the process of coding all the papers.\nAfter the thematic analysis, we extract the data to answer each research question using the relevant codes. Additionally,\nwe also summarize the co-occurrence of all the code types, i.e., attack types, target models, attack goals, attack knowledge\nmetrics, target types, and defenses. Based on a collection of existing taxonomies and models found in [63, 93, 99, 155]\ndescribing adversarial attacks against machine learning, as well as this review, we construct a general model of the\nattributes that are associated with the attack in Figure 3. The model follows the seven coded groups from the thematic\nanalysis.\nThe model could outline the aspects of adversarial attacks that should be considered in any research into adversarial\nattacks on ML. The replication package of the study is publicly available at https://github.com/mathialm/Adversarial-\nRobustness-in-Unsupervied-Machine-Learning-A-Systematic-Review.\nOf the final 86 papers for consideration, 39 are published as conference or journal papers, and 47 are unpublished. It\ncan be noted that even though several of the unpublished papers also have published versions, only the papers found\nthrough the SLR directly are included. The years of publication for all the papers are found in Figure 4, and the number\nof papers published in different journals and conferences are found in Figure 5. Most papers on this topic were published\naround 2020, with relatively few papers before 2018. As expected, the journals and conferences on this topic are mainly\nfrom the databases selected to be directly used, i.e., ACM, and IEEE.\nManuscript submitted to ACM\n10\nMohus et. al.\nFig. 3. A 3-part model describing the relationship between the different aspects of an adversarial attack\nFig. 4. Number of papers published in a specific year, from 2013\nto 2022\nFig. 5. Number of papers in Journals and Conferences, with the\nlong name available in Appendix A\n5\nRESULTS OF RQ1: ATTACKS TARGETING UL-BASED SYSTEMS\nBelow is a summary of the different kinds of attack methods, attack knowledge, attack goals, metrics, and attack focus, for\neach of the UL types. A summary of the relations between kinds of attacks and targets is found in Figure 6, showing\nthat most target types have a specific attack mainly associated with it.\nThe type of unsupervised machine learning technologies targeted by attacks is summarized in Figure 7, showing\nthat most of the target types covered consist of GAN, autoencoder, or clustering.\nIn the following sections, the total percentage would, in some cases, be above 100%, meaning some papers would be\ncovered in multiple groups and counted more than once.\nDetails on target types can be found in Appendix B.1; attack types in Appendix B.2; and metrics in Appendix B.3.\n5.1\nAttacks targeting AutoEncoder-based systems\nWe found 25 papers (29%) that cover AutoEncoder models in the SLR.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n11\nFig. 6. Sankey diagram on the number of papers covering dif-\nferent attack methods against different target types, showing\nthe degree to which papers in this field cover different attacks\nagainst different technologies\nFig. 7. Distribution of the different types of UL technologies\nfound in the review\n5.1.1\nAttack Method: Attacks against autoencoders include adversarial examples, inference and extraction, and poisoning,\nwhich are covered by 60%, 16%, and 16% of the papers on autoencoders, respectively, and one paper [139] cover the\nimplementation of a differentially private autoencoder-GAN hybrid generator, aimed to protect against privacy attacks\ngenerally.\nAdversarial Examples can work at two points, i.e., perturbations of the actual input [16, 24, 27, 39, 58, 80, 83, 87, 88, 136–\n138, 146], or on the latent code passed to the decoder [4, 43, 83, 87, 125, 137]. The attacks appear to rely on where an\nattacker could have reasonable access to the input data. In contexts where the entire AutoEncoder is used as a single\nstep, e.g., removing noise, attacks do not focus directly on the latent code but rather construct an attack that tricks the\nencoder into creating different latent codes, changing the output. Similarly, in contexts where the adversary has direct\naccess to the latent code, e.g., physical transmission or generation, the focus is to create perturbations that trick the\ndecoder.\nInference and extraction attacks are limited to membership inference [18, 30, 32, 69], where the adversary aims to\nextract information on whether the training used a specific sample.\nPoisoning attacks can be placed into three categories, depending on how and what the adversary does. Injection\nattacks [105, 137] aims to disrupt a model by introducing new samples into the training set, manipulation attacks\n[40, 47, 137] changes existing samples, and logic corruption [127, 137] aims to introduce a backdoor into the model,\nwhich can later be abused.\n5.1.2\nAttack Knowledge: We separate an adversary’s knowledge into white box, grey box, and black box, which are\ncovered by 60%, 40%, and 28% of the papers on autoencoders, respectively.\nWhite box enable the attacker to craft their attack specifically to the model to enable their objective. For certain\nattacks like membership inference [32, 137], adversarial examples [27, 39, 58, 83, 87, 88, 138], poisoning [40, 105, 127], and\nPGD [27], white box knowledge enables a more optimized attack. In a physical system, knowledge of the AutoEncoder\nenables the adversary to perform a general attack [4, 125]. Additionally, white box attacks appear easier and more\nsuccessful than scenarios of less knowledge [4, 32, 80, 125].\nManuscript submitted to ACM\n12\nMohus et. al.\nGrey box attacks mean the adversary has incomplete knowledge of the AutoEncoder and has to make a sub-optimal\nattack. One such knowledge is the mapping between input and output values, whether the input is the latent code\n[24, 32, 137] or the actual data [16, 24, 47, 136, 137, 146]. Other attacks, like adversarial examples [43], create substitute\nnetworks from the available information to craft universal perturbations, enabling attacks against any AutoEncoder.\nFor attacks like membership inference [18, 30], the adversary utilizes available training and test data to infer whether\nthe training used specific samples. Grey box attacks also appear more successful than a black box and less than white\nbox attacks [32].\nBlack box attacks rely on the known vulnerabilities of AutoEncoders and make assumptions on the model and data to\ncreate the attack. For generative AutoEncoders like VAE, the adversary would only have direct access to the generated\nsamples [32, 87, 137]. In other systems like physical communication [4], adversarial examples [80, 125], and membership\ninference [69], the adversary uses an existing model as a substitute for the actual model, which enables the adversary to\nturn a black box attack into a kind of white box attack against the target AutoEncoder.\nOne paper [139] focuses on implementing differential privacy, which is commonly used for membership inference\nattacks and would be most helpful in the context of limited knowledge.\n5.1.3\nAttack Goal: We separate an adversary’s goal into integrity, privacy, and availability, which are covered by 68%,\n20%, and 12% of the papers on autoencoders, respectively.\nIntegrity attacks aim to introduce new functionality, change existing functionality, or avoid the existing functionality\nof an AutoEncoder. Attacks often focus on maximization of distortion of the model’s output, on all, or subsets, of the\ninput data [58, 137]. For attacks like adversarial examples [16, 24, 27, 39, 40, 80, 83, 87, 88, 136, 138, 146], the adversary\naim to force a change in the latent representation of the autoencoder, in turn producing different output from what the\ninput data intended to produce. Similarly, poisoning attacks focus on introducing unintended functionality into the\nmodel, e.g., backdoors on specific input [47, 127], or prevent intended functionality of the model [105].\nPrivacy attacks have as a goal to extract some private information from the autoencoder [139]. Membership inference\n[18, 30, 32, 69] is the primary type of privacy attack against AutoEncoders, aiming to determine if a specific sample was\nused in training by analyzing the output from the model.\nAvailability attacks aim to prevent the intended functionality of the model by performing the attack. In physical\ncommunication [4, 43, 125], using adversarial examples causes a jamming attack on the decoder, preventing the model\nfrom decoding the encoded message.\n5.1.4\nMetrics to measure attack success: We separate the metrics used by an adversary to determine attack success into\ndistance metrics, and true/false positive/negative based, which are covered by 60%, and 56% of the papers on autoencoders,\nrespectively.\nDistance metrics refer to using a mathematical formula comparing two or two sets of samples to determine the amount\nof difference between them. Some metrics, like Structured Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio\n(PSNR), Mean Square Deviation (MSD), and Root Mean Square Deviation (RMSD) are used for comparing whether there is\na noticeable difference between malicious and clean samples [32, 47, 58, 69, 83, 87, 88, 127, 138, 146], with Evidence Lower\nBOund (ELBO) [39] denoting the performance of the model in regards to the quality. Area Under Distortion-Distortion\nCurve (AUDDC) is used to measure how well an attack can influence the output of the model [136]. Other metrics, like\n𝐿2, are used as a threshold of perturbations in the input [16, 40, 83, 137].\nTrue/False Positive/Negative based metrics are used to determine the rate of success and failure by the attackers and\ndetermine how many measured instances are successes (true positive and true negative) and how many are failures\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n13\n(false positive and false negative). One way of using these metrics is to measure the performance of the attacked\nsystem with and without the influence of the attacker, e.g., detection systems [105], classifiers using output from\nUL systems [4, 58, 80, 88], and physical communication [4, 43, 125]. In adversarial examples, success rate is used for\ndetermining if the attack works, both in a targeted and untargeted manner [83]. For other attacks like membership\ninference [18, 27, 30, 32, 69], the success is measured by how well their attack can differentiate if a sample was used for\ntraining by the autoencoder.\nAdditionally, two papers do not contribute with metrics to determine attack success, as they cover implementation\nof differential privacy [139], or an analysis on adversarial robustness to adversarial examples [24].\n5.1.5\nAttack focus: We separate where the adversary focuses into production and training, which are covered by 92%,\nand 12% of the papers on autoencoders, respectively.\nProduction models exist within their respective working environments, exposing them somewhat to the outside\nworld. Privacy attacks like membership inference [18, 30, 32, 69] mainly only focus on production models, as the data\npursued are no longer directly used by the model. Defenses like differential privacy [139] also would only be applicable\nfor protecting a production model. Adversarial examples [4, 27, 39, 40, 43, 58, 80, 83, 87, 88, 125, 136–138, 146] is another\ntype of attack mainly used on production models, where the adversary somehow has direct access to the input to the\nmodel, and can introduce perturbations to the input samples. Some papers cover generalized increases of autoencoder\nrobustness to perturbations [16, 24], while others cover uses of differential privacy to enable privacy guarantees on the\ntraining data [18, 139].\nTraining models are models in the context of training for a specific purpose. Poisoning attacks [40, 47, 105, 127] is the\nonly attack targeting autoencoder in training by modifying the training data itself to modify the training of the model\nin a specific direction.\nSummary of attacks on AutoEncoders:\nFor autoencoders, the main source of attacks stems from adversarial examples, being usable against both\ndiscriminatory and generative models, often in a limited knowledge scenario in order to disrupt the output\nfrom the model. There are also attacks focusing on extracting information about the training data, which\noften are performed in minimum knowledge scenarios. Poisoning attacks are also used to disrupt or introduce\nnew functionality to the model. An adversary targeting autoencoders also often relies on distance metrics to\ndetermine the difference between clean and malicious data in order to avoid simple detection techniques, while\ntrue/false positive/negative metrics are used for determining how successful the attack is.\n5.2\nAttacks targeting UL supported classification\nWe found 14 (16%) papers that cover UL-supported classification. However, not every paper covered focuses on the\nadversarial robustness of the classifier itself, and instead focuses on the use of classifiers to increase robustness [94], or\nthe use of classifiers as a baseline for determining whether attacks were successful [12, 18, 33, 64, 83, 85, 106, 118, 154].\nIn other words, only four (5%) papers cover the adversarial robustness of classifiers in conjunction with unsupervised\nlearning.\n5.2.1\nAttack Method: Attacks against UL supported classification include poisoning and adversarial examples, which are\ncovered by 75%, and 25% of the papers on UL supported classification, respectively.\nManuscript submitted to ACM\n14\nMohus et. al.\nPoisoning attacks [53, 95] in the classifier’s training are particularly effective in disrupting its performance. Similarly,\npoisoning attacks are also effective at disrupting the performance of a classifier based on domain adaption [147] by\naffecting the training of the unsupervised model.\nAdversarial examples against UL-supported classifiers means that the adversary crafts perturbations, which affect the\nUL system to the degree that the classification is also affected. An AutoEncoder is sometimes used before a classifier to\ndenoise the input before the classifier. Attacks on this often consist of adversarial examples, constructed to disrupt the\nclassifier performance [80].\n5.2.2\nAttack Knowledge: We separate an adversary’s knowledge into white box and black box, which are covered by\n75% and 75% of the papers on UL supported classification, respectively.\nWhite box attacks, in the use of AutoEncoder, corresponds to improved accuracy under adversarial examples [80].\nIn semi-supervised [95] and domain adaption [147] scenarios, poisoning with white-box knowledge can significantly\nreduce the classifier performance.\nBlack box attacks against UL supported classification with an AutoEncoder has a decreased performance with adversarial\nexamples [80]. Additionally, semi-supervised [53, 95] and black box poisoning attacks appear to be marginally worse\nthan the white box, but are still able to significantly reduce classifier performance.\n5.2.3\nAttack Goal: An adversary’s goal against UL supported classification is integrity in 100% of the papers.\nIntegrity attacks like poisoning against domain adaption [147] and semi-supervised models [53, 95] can construct\npoisoned samples that decrease the performance of the resulting classifier, while adversarial examples [80] can manipulate\nthe classification results by perturbing the input.\n5.2.4\nMetrics to measure attack success: We separate the metrics used by an adversary to determine attack success into\ndistance metrics, and true/false positive/negative based, which is covered by 50% and 75% of the papers on UL supported\nclassification, respectively.\nDistance metrics have several different ways of measurement. One measurement of success is the degree to which the\nattacker can influence the results of the classifier through an unsupervised model, like with semi-supervised [95], where\nthe RMSE of the classification is used to determine success. When joined with an autoencoder, classification accuracy is\nused to determine the success of an attack [80]. Another measurement is the degree to which the attacker perturbs the\ninput, as unrestricted perturbation would be visible to statistical analysis or human evaluation. For this, the 𝐿2 norm\n[95] can be used to determine perturbations in a poisoning attack.\nTrue/False Positive/Negative-based metrics, like how well the classifier can function under attack, is one way of\nmeasuring attack success, e.g., adversarial examples [80] and poisoning [53, 147] both utilizing the accuracy of the\nclassifier.\n5.2.5\nAttack focus: Where the adversary focuses their attack is separated into training and production, which are\ncovered by 75% and 25% of the papers on UL supported classification, respectively.\nTraining systems have adversaries that focus on disrupting classifier functionality by performing a poisoning attack\non semi-supervision [53, 95] and domain adaption [147] models.\nProduction systems have adversaries where the focus is on disrupting the functionality of a classifier in a real-world\nenvironment, specifically using an autoencoder before the classifier itself [80], as it is targeted by adversarial examples.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n15\nSummary of attacks on UL supported classification:\nAttacks on UL supported classification are mainly adversarial examples, where the adversary perturbs data to\ninfluence the UL part, in turn influencing the classification; and poisoning, where the adversary introduces\npoisoned samples into the training of domain adaption which influence the performance of the resulting\nclassifier. Attacks appear to be most successful in a high-knowledge scenario. The measurements of success\nmainly focus on the classifier’s performance, e.g., accuracy, with distance metrics used to measure how much\nthe samples are perturbed to avoid simple detection.\n5.3\nAttacks targeting clustering\nWe found ten papers (12%) covering unsupervised clustering.\n5.3.1\nAttack Method: Attacks against clustering include poisoning, evasion, and adversarial examples, which are covered\nby 60%, 50%, and 10% of the papers on clustering, respectively.\nPoisoning attacks work by inserting crafted samples that compromise the cluster’s functionality. One such attack\nis the bridging attack [19–21, 124], where the adversary crafts samples that \"bridges\" gaps between clusters, in turn\naltering the resulting clusters. Other approaches involve manipulating nodes and edges of graph-based clustering [34] to\ndisrupt the system or to alter existing samples from the training set according to a specified maximum total perturbation\n[38].\nEvasion attacks consist of crafting samples with features overlapping with the actual sample set to conceal the sample\nand avoid detection without significantly altering the clustering functionality [8, 12, 19–21].\nAdversarial examples work by optimizing the perturbation relating to the maximization of the clustering results\n[151].\nOne paper [123] covers the construction of a privacy constraint enforced in training to prevent private sample\ninformation from being inferred to protect against privacy attacks.\n5.3.2\nAttack Knowledge: We separate an adversary’s knowledge into white box, grey box, and black box, which are\ncovered by 50%, 50%, and 10% of the papers on clustering, respectively.\nWhite box attacks can optimize poisoning [19–21, 34, 124] attacks, manipulate the resulting cluster, and optimize\nobfuscation [20] attacks by crafting adversarial samples to hide samples in existing clusters.\nGrey box means adversaries would have access to a limited data mapping with its resulting cluster. Bridging attacks\n[20] can be used to gradually discover samples that disrupt the function of the resulting cluster, or for evasion [8]\nand adversarial examples [151], use the mapping between samples and resulting clusters to optimize perturbation\nneeded to avoid detection by the clustering. Some methods of poisoning attacks [38] rely on constructing a general\nperturbation based on the mapping between samples and resulting clusters by using a genetic algorithm to discover\noptimizations iteratively. Other limited knowledge attacks rely on using approximate models based on open-sourced\ndata and performing white box attacks against these approximate models to construct their attack [34].\nBlack box attacks include adversaries who can employ an evasion attack [12] by slowly enforcing drift in the clustering\ntowards a specific point, which does not require any information of the model.\nOne paper [123] enforces privacy constraints in training and would likely be most applicable in limited knowledge\nscenarios.\nManuscript submitted to ACM\n16\nMohus et. al.\n5.3.3\nAttack Goal: We separate an adversary’s goal into integrity, privacy, and availability, which are covered by 68%,\n20%, and 12% of the papers on clustering, respectively.\nAvailability attacks work by preventing or disrupting the general functionality of the clustering. Poisoning attacks\n[19–21, 34, 38, 124] work by introducing specific samples to the training to influence the shape of the resulting clusters,\nto the degree that prevents the clustering from working as intended.\nIntegrity attacks focus on influencing the clustering results regarding specific samples. Evasion attacks aim to hide\nspecific samples in existing clusters by overlapping their features with those of actual samples [12, 20] or by slowly\nintroducing modified samples to drift the clustering in a specific direction [8]. Adversarial examples [151] aim to modify\nsamples in a way that results in the misclustering of samples.\nPrivacy attacks cover theoretical constraints on clusters to ensure the privacy of the samples used to train the model\n[123].\n5.3.4\nMetrics to measure attack success: We separate the metrics used by an adversary to determine attack success into\ntrue/false positive/negative, distance metrics, number of clusters, and objective function, which are covered by 50%, 40%,\n40%, and 30% of the papers on clustering, respectively.\nTrue/false positive/negative metrics, like accuracy used in detection [8, 12] to determine its performance against\nevasion attacks. In poisoning attacks, F-measure [21] and success rate [34] are used for determining the quality of the\nclusters produced under attack. In adversarial examples [151], accuracy measures how well the clusters fit the labels.\nDistance metrics are used in poisoning attacks [20, 38] to craft samples to avoid simple detection. Metrics like 𝐿0, 𝐿2,\nand 𝐿∞norms are used to limit the number of perturbations added to the samples. In contrast, others use a feature\nmodification threshold [8] to limit the amount any single feature can be modified. For adversarial examples, Normalized\nMutual Information (NMI) is used to compare actual and predicted cluster labels, and distortion measures the perturbation\nof the sample [151].\nNumber of clusters is used to compare the performance of attacks and their capability of reducing or increasing the\nnumber of clusters the model produces. Poisoning attacks [19–21, 124] use this metric to determine how well the attack\nworks.\nObjective function based metrics are used in [19–21] and are crafted for the specific attack and model.\nOne paper [123] does not describe concrete attacks and, as such, does not outline any metrics for the success of an\nattack.\n5.3.5\nAttack focus: We separate the adversary focus into production and training, which are covered by 50%, and 50%\nof the papers on clustering, respectively.\nProduction systems are targeted by attacks like evasion [8, 12, 20] and adversarial examples [151], where the aim is to\nreduce the functionality of specific samples while keeping the general clustering functionality intact, while other attacks\nlike targeted noise injection and small community attacks [34] aim to disrupt the functionality of existing graph-based\nclusters. Additionally, papers, like [123], aim to ensure clusters’ privacy guarantees to prevent privacy attacks, such as\nmembership inference.\nTraining systems are targeted by poisoning attacks [19–21, 38, 124], modifying or injecting malicious samples to the\ntraining set, which can significantly reduce the functionality of the clustering systems.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n17\nSummary of attacks on clustering:\nPoisoning attacks on clustering involve introducing samples to disrupt the cluster functionality, merge, or\nincrease the number of clusters found. Evasion attacks work by hiding specific samples within clusters to avoid\ndetection without altering the cluster’s functionality. Most attacks on clustering rely on increased knowledge,\nas the adversary has a hard time accomplishing their goals without knowing the data samples used for training.\nTrue/False Positive/Negative metrics are used by the adversary to determine how good the resulting cluster is,\nmeasuring how well the cluster fits the labels. In contrast, distance metrics limit the amount of perturbation a\nsample can have. The number of clusters is also used to determine the success of the attacks, particularly when\nthe attack’s goal revolves around changing the cluster.\n5.4\nAttacks targeting GAN\nWe found 42 papers (48%) covering GAN models.\n5.4.1\nAttack Method: Attacks against GAN include inference and extraction, poisoning, adversarial examples, and evasion,\nwhich are covered by 33%, 14%, 10%, and 2% of the papers on GAN, respectively. Additionally, 45% of the papers cover\nimplementations of privacy measures to prevent privacy attacks without specifying concrete attacks. These include\nimplementations of differential privacy for GANs in order to facilitate sharing models and datasets without risk of privacy\nviolations [10, 17, 31, 33, 52, 64, 77, 84, 97, 98, 118, 139, 141, 148, 149, 156, 158], context-dependent implementations of\nprivacy guarantees to enable private sharing [106], and a survey of privacy challenges in GANs [140].\nInference and extraction attacks, like membership inference attacks, are methods of attacks to determine whether\nspecific samples were used during the training of the GAN model [55, 90]. Methods for this include using distance\nmetrics between the sample and generated samples [32, 67, 69, 102, 137], using discriminator confidence [67, 68, 137],\nor training a classifier [85, 89, 137, 159], in order to determine whether the training used a sample. Other methods\ndetermine membership by controlling the latent code of the generator and determining membership based on the\nreconstruction error of the generator when trying to generate specific samples [134]. Attribute inference is another way\nof extracting private attributes from GANs by looking at publicly known features, collecting generated samples, and\nusing regression to predict the private features [85, 137]. Model extraction is a method for cloning an existing model\nbased on the generated samples of a target GAN, like accuracy extraction [72, 137], where the adversary measures the\ndifference in the distribution of the cloned and actual model; and fidelity extraction [72, 137] compares the distribution\nof the cloned model to the actual training data set. The cloned model can subsequently be used as the substitute model\nin a white-box membership inference attack [68, 71].\nPoisoning attacks is, among other, used against federated GAN training, where the adversary can contribute to the\nglobal model as one or multiple participant(s) and subsequently modify the information that it contributes, and affect\nthe performance of the global model [76, 154]. Other methods modify the training data directly, either by injecting\nnew samples [137], modifying existing samples to disrupt the model [137], or modifying samples to introduce new\nby-products in the generated samples [47, 76, 120, 127, 137].\nAdversarial examples attack relies on access to the latent code used by the generator and perturbing some input into it\nto generate a completely different sample from what would have been generated without the perturbation. Adversarial\nexamples can be used during training for adversarial training [94, 126], increasing the robustness of the model. Other\npapers focus on implementing changes to how the GAN works in order to be theoretically robust against adversarial\nManuscript submitted to ACM\n18\nMohus et. al.\nexamples [37]. Additionally, metrics commonly used to determine sample quality, e.g., Fréchet Inception Distance (FID)\nand Inseption Score (IS), are also explored and are determined to be insufficient in discovering adversarial examples [5].\nEvasion attacks focus on finding minimal perturbations on the latent codes, which produce unsatisfactory samples,\ncompared to what is produced by the unperturbed latent code [137], in order to generate unsatisfactory samples, but\nstill following the statistical distribution of the training samples.\n5.4.2\nAttack Knowledge: We separate an adversary’s knowledge into white box, grey box, and black box, which are\ncovered by 26%, 26%, 26% of the papers on GAN, respectively. In contrast, 45% of the papers cover differential privacy\nimplementations [10, 17, 31, 33, 52, 64, 77, 84, 97, 98, 118, 139, 141, 148, 149, 156, 158], other privacy guarantees [106],\nand general privacy challenges [140], and would likely be applicable in scenarios of limited knowledge, where the\nadversary does not have access to training data, i.e., black box or grey box.\nWhite box attacks, like membership inference [32, 68, 69, 137], fidelity extraction [72], and poisoning [47, 127, 137],\nenable full knowledge of the parameters of the models which allow the attacker to fine-tune the attack towards the\nspecific model. Additionally, for membership inference attacks, full knowledge allows for querying the discriminator to\nextract information on the training data [55, 67, 68, 72, 102, 134, 137]. Against GANs, the access to internal information\non the model allows for more successful attacks compared to more restricted knowledge scenarios [32, 68, 69, 72].\nGrey box attacks, e.g., membership inference [32], mean that adversaries know the input-output mapping of the\nmodel, i.e., the latent code and the generated samples, but not the internal parameters of the discriminator or generator\nmodels. Other membership inference attacks [68, 137, 159], as well as fidelity extraction attacks [72, 137], require limited\nknowledge of the training and test data, representing potential public databases that an adversary would have access\nto. In comparison, poisoning [76, 154] attacks on federated GAN allow for knowing the training model’s structure but\nnot the training data of the other participants. Other poisoning [120, 137] attacks, evasion [137] attacks, adversarial\nexamples [5, 37, 94, 126], and property inference attacks [159] instead rely entirely on the ability of the adversary to\ncontrol the input to the model, i.e., training data or latent code. In general, the more knowledge an adversary has, the\nbetter performance of the attack [32, 72, 159].\nBlack box attacks, like in the case of membership inference, mean that the adversary only has access to the generated\nsamples from the model, where attacks rely on inferring if samples are part of the training based on distance calculations\nbetween the sample and the generated set [32, 137], or by training classifiers on the generated set [71, 89, 159]. Other\napproaches for membership inference [68, 69, 85, 90, 159] and model extraction [72, 137] attacks rely on recreating a\nsubstitute model and perform white-box attacks on this model, later used for an attack on the original model.\n5.4.3\nAttack Goal: We separate an adversary’s goal into privacy and integrity, which are covered by 79%, and 24% of\nthe papers on GAN, respectively.\nPrivacy attacks, like membership inference [32, 55, 67–69, 71, 85, 89, 90, 102, 134, 137, 159], have a goal to determine\nwhether the GAN model used a specific sample for training, often measured by the success rate [89], accuracy [55, 68, 69,\n102], or Area Under the Curve of the Receiver Operator Characteristic curve (AUCROC) [32]. Other attacks like attribute\ninference [137], and model inference [72, 137] attacks extract data distribution, for the former, and model information, for\nthe latter, using metrics like accuracy [72] to determine the success of the attack. Many papers also cover the application\nof differential privacy [10, 17, 31, 33, 52, 64, 77, 84, 97, 98, 118, 139, 141, 148, 149, 156, 158], or other privacy guarantees\n[106] in training to ensure the privacy of the training samples, as a way to prevent private data from being inferred\nfrom released datasets or models. One paper also covers the privacy challenges in GANs [140].\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n19\nIntegrity attacks like poisoning [47, 76, 120, 127, 137, 154] focus on decreasing the quality of generated samples or\naltering the mapping between latent code and generated samples, while evasion attacks [137] and adversarial examples\n[37, 94, 126] force the model to output unsatisfactory output with small perturbations in the input.\n5.4.4\nMetrics to measure attack success: We separate the metrics used by an adversary to determine attack success\ninto true/false positive/negative, and distance metrics based, which are covered by 38%, and 19% of the papers on GAN,\nrespectively, with 45% of the papers covering protection against general privacy attacks, like, differential privacy\n[10, 17, 31, 33, 52, 64, 77, 84, 97, 98, 118, 139, 141, 148, 149, 156, 158], context-dependent implementations of privacy\nguarantees [106], and general explorations of privacy challenges in GANs [140], and do not outline any metrics for\nmeasuring attack success.\nTrue/false positive/negative metrics in membership inference attacks measure how well the adversary can determine if\nthe query sample belongs to the training set, measured by AUCROC [32, 85, 134, 159], Area Under Precision-Recall Curve\n(AUPRC) [71, 85], F1-score [85], accuracy [55, 67–69, 72, 85, 102], precision [71], recall [71], true positive [55, 90, 134],\nfalse positive [55, 90, 134], and success rate [89, 137]. For poisoning attacks, accuracy [154] is used to determine both if\nthe poisoned task and the main task of the model perform well. For adversarial examples, accuracy of the discriminator\ndetermines how well the attack can influence the generated samples [94].\nDistance metrics in membership inference attacks, consists of reconstruction distance [137] and 𝜖-ball distance [137],\nwhich determine the distance between the query sample, and the set of generated samples, to determine if the training\nused the sample. In poisoning attacks, SSIM [47], PSNR [47], IS [76, 120], FID [76, 120, 127], and Kernel Inception\nDistance (KID) [76] are used for evaluating the performance of the model in non-adversarial and adversarial scenarios.\nWith adversarial examples, SSIM and FID are used for evaluating the performance of the generated samples under\nadversarial noise [37]. Additionally, distance metrics measure the perturbations applied to data in evasion attacks [137]\nand adversarial examples [126]. Alfarra et al. [5] evaluate the FID and IS metrics in terms of the metrics’ ability to\ndiscover adversarial examples, which the metrics are not able to.\n5.4.5\nAttack focus: We separate the adversaries’ focus into production and training, which are covered by 88%, and 12%\nof the papers on GAN, respectively.\nProduction systems under membership inference [32, 55, 67–69, 71, 85, 89, 90, 102, 134, 137, 159], attribute inference,\n[85, 137] and model extraction [72, 137] attacks rely on using the generated samples from the model to infer information.\nTraining systems under poisoning attacks are mainly attacked by altering the training data of the model [47, 120, 127].\nIn contrast, in federated GAN works by poisoning the global model as a participant in the federated training [76, 154].\nManuscript submitted to ACM\n20\nMohus et. al.\nSummary of attacks against GANs:\nThe primary attacks against GANs revolve around extracting information from the model based on the generated\nsamples they produce, including: membership inference, where the adversary tries to find if the training used a\nspecific sample, attribute inference to extract the features of the training data, and model extraction, where one\nclone the model, in turn giving the adversary access to optimize attacks on the cloned model to attack the\noriginal model subsequently. Poisoning attacks are also used, where the adversary crafts samples to be used in\ntraining, either to decrease the utility of the generated models or to introduce backdoors into the generated\nsamples. Knowledge of the model is also essential, as more knowledge enables optimized attacks. However,\nmembership inference attacks are made with only knowledge of the generated samples. Distance metrics are\nused for determining the model’s performance by analyzing the clean and attacked generated samples. In\ncontrast, true/false positive/negative determines the degree to which the attack was successful.\n5.5\nSuper Resolution\nWe found three papers (3%) covering super-resolution models.\n5.5.1\nAttack Method: Attacks against super resolution only include adversarial examples.\nAdversarial example attacks perturb the input to the model to alter the result significantly [26, 36, 153]. One paper also\nproduces general perturbations for super-resolution [36], i.e., the perturbation is independent of the model’s training.\n5.5.2\nAttack Knowledge: The adversary’s knowledge is only covered by white box attacks.\nWhite-box attacks revolve around knowing a mapping between input and output, and, with limited perturbations on\nthe input, being able to produce large perturbations on the output [26, 36, 153]. For this attack, an adversary would\nrequire access to the model to know the perturbed output. The perturbation is iteratively fine-tuned, which also applies\nto the general perturbation [36].\n5.5.3\nAttack Goal: Integrity attacks are the only goal covered for super-resolution.\nIntegrity attacks, like adversarial examples, have as a goal to insert imperceptible perturbation on the input to the\nmodel, in turn causing an extensive deterioration of the output from the model [26, 36, 153].\n5.5.4\nMetrics to measure attack success: distance metrics are the only metrics an adversary uses to determine attack\nsuccess for super-resolution.\nDistance metrics include the difference between perturbed and unperturbed input and output measurements. The\nmetrics used for this include PSNR [26, 36, 153] measuring noise, SSIM [26, 153] measuring perceptual differences, and\nLearned Perceptual Image Patch Similarity (LPIPS) [26] measuring the difference in activation in the model on two\ninputs.\n5.5.5\nTarget Focus: The adversary only focuses their attack on production systems.\nProduction systems under adversarial examples mean an adversary makes perturbations to the input to the model\n[26, 36, 153], in order to cause large perturbations in the output.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n21\nSummary of attacks against super-resolution:\nAgainst super resolution, adversarial examples is the only attack, focusing on disrupting the model’s output by\nperturbing the input, requiring complete knowledge of the model to perform the attack. The attack focuses\nmainly on using distance metrics to measure the difference between clean and adversarial input and output.\n6\nRESULTS OF RQ2: DEFENSES METHODS AGAINST ATTACKS\nA summary of the percentage of papers with defenses used by different target types is covered in Figure 8, which shows\nthe proportion of defense types for each target type, e.g., differential privacy contributes 33%, 50%, and 72%, to the total\nnumber of defenses of AutoEncoder, classification, and GAN, respectively. Details on the different defenses can be found\nin Appendix B.4.\nFig. 8. Distribution of the different types of defenses found in\nthe review\nFig. 9. Sankey diagram showing the relative appearance between\nattack types and defenses for autoencoders\n6.1\nDefenses against attacks targeting AutoEncoder\nA summary of the relationship between the attack type and defensive measure for autoencoders is shown in Figure 9.\nAdversarial training consists of using adversaries’ strategy to generate samples to be learned by the model, making\nthe model more robust against the specific attack used to generate the samples. This defense has certain drawbacks,\nlike reducing performance on clean samples [80, 125] or lacking generalizability on other perturbations [125].\nSmoothing, also referred to as gradient masking, works by preventing adversaries from inferring information on the\ninternal gradients of the model [16, 27, 80, 136, 137, 146], providing noticeable robustness increase against adversarial\nexamples in white-box settings [80], and marginal robustness increase in black-box settings [80].\nRobustness constraints work by enforcing a guarantee on the lower bounds of when perturbations in the input would\naffect the output. In particular, this is targeted as a general defense against adversarial examples [24, 39].\nMarkov Chain Monte Carlo (MCMC) is employed on the latent code from the encoder, where the variables are updated\nbefore being used by the decoder [88]. This method provides robustness against adversarial examples and higher quality\ndata than autoencoder under attack.\nManuscript submitted to ACM\n22\nMohus et. al.\nDifferential privacy ensures that, under certain constraints, an adversary cannot determine if a specific sample\nwas used in training, providing slight [30, 32] to large [18] amount of privacy protection, depending on the privacy\nbudget [18]. Attacks like membership inference [18, 32, 137], attribute inference [137], and model extraction [137] can be\nmitigated by using this method. Differential privacy is also used as a way of ensuring the privacy of synthetic data\n[139]. However, this method requires more data resources and causes a decrease in the quality of the data generated\n[18, 30, 32, 139].\nPrivacy Constraints is another method for ensuring the privacy of training data when releasing synthetic data [30].\nHere though, differential privacy is implemented with a causal model, where the utility of the generated samples is\nhigher than non-causal differential privacy implementations for the same privacy budget.\nDropout works on neural networks by dropping random neurons during training to prevent the model from overfitting\nthe data. While the method is effective against membership inference [69, 137], it also results in a quality decrease of\ngenerated samples and requires more computer resources [69, 137].\nWeight normalization works by periodically re-parameterizing the neural network weights. In turn, the re-parametrizing\ncan increase the model’s generalizability and provide some protection against membership inference attacks [137].\nFine pruning works in neural networks by removing specific neurons that are not improving the performance on\nclean samples. The method is not, however, proven effective for autoencoders, as \"fine-pruning does not remove the\nby-product task\" introduced by poisoning attacks [47, 137]. Additionally, fine pruning also decreases the quality of\ngenerated samples and requires more computing resources [47, 137].\nActivation Output Clustering works by analyzing a hidden layer of a neural network to discover different distributions\nbetween clean and adversarial input. The defense does not work for generative models. However, the strategy struggles\nto differentiate between real and adversarial input [47, 137].\nLastly, several papers do not describe defensive measures against their respective adversarial attack, like adversarial\nexamples [4, 43, 58, 83, 83, 87, 138], and poisoning [40, 105].\n6.2\nDefenses against attacks targeting classification\nAdversarial training for defending UL-supported classification is done by adversarially training an autoencoder and\nthe classifier [80], with adversarial robustness increasing or decreasing depending on the training set. Kim et al. [80]\nconclude that adversarial robustness using an autoencoder and adversarial training provide the same effect, differing\nfrom the robustness provided by gradient masking.\nRelabeling entails calculating the influence a specific sample would have on the training in case of poisoning [53],\nand re-labeling the sample, in turn decreasing the performance of the poisoning attack.\n6.3\nDefenses against attacks targeting clustering\nA summary of the relationship between the attack type and defensive measure for clustering is shown in Figure 10.\nGame theory is proposed as a general defensive strategy for clustering. The general strategy involves the target\nemploying defensive measures depending on specified conditions [12] or modeling the attacker-target system as an\nadversarial game to determine optimal defenses [20] for a variety of attacks.\nAdversarial training is employed with adversarial strategies like adversarial examples [151], noise injection [34],\nand small community [34], which ensures the clustering trains to be more robust against these specific attacks. The\ndrawback of this method, however, is the potential reduction of the overall accuracy of the models [34], though some\nresults indicate that adversarial training does improve the performance [151].\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n23\nRobustness enhancement constructs feature sets for the clustering consisting of binaries for important features and\nnumerical values for secondary features [124], preventing adversarial attacks from merging clusters.\nPrivacy enhancement applies privacy constraints to the clustering process for the anonymity of samples belonging to\na specific cluster to be ensured [123].\nLastly, several papers do not describe defensive measures against adversarial attacks, like poisoning [19, 21, 38] and\nevasion [8, 20].\nFig. 10. Sankey diagram showing the relative appearance be-\ntween attack types and defenses for clustering\nFig. 11. Sankey diagram showing the relative appearance be-\ntween attack types and defenses for GANs\n6.4\nDefenses against attacks targeting GAN\nA summary of the relationship between the attack type and defensive measure for GANs is shown in Figure 11.\nDifferential privacy is used as a defensive method in 27 papers. The research in [10, 17, 31, 33, 52, 64, 77, 84, 97, 98,\n118, 139–141, 148, 149, 156, 158] all cover theoretical applications of differential privacy. The defense is also applied\nspecifically to counter membership inference in [32, 55, 68, 71, 85, 89, 90, 102, 137], providing slight [32, 71, 85] to large\n[55, 71, 102] privacy protection, depending on the privacy budget [68, 71, 102] and training set size [55]. Differential\nprivacy can also cause quality decreases in generated samples [68, 102] and cause groups of data to become over-\nrepresented by the generator [102].\nPrivacy Enchancement are methods for ensuring the privacy of training samples without implementing differential\nprivacy. In [67], the model trains to generate samples independent of the set of samples used for training to protect\nthe model from membership inference attacks. In [106], the model trains the discriminator to determine whether two\nsamples belong to the same identity, which, through the adversarial min-max game between the generator and the\ndiscriminator, forces the generator to produce private samples.\nWeight normalization recalculates the parameters of the weight in the model in order to improve generalizability\n[137]. While the method can reduce the accuracy of memberhsip inference attacks [68, 137], it also can result in instability\nbetween the discriminator and the generator [68, 137].\nDropout works by removing random neurons in a neural network to prevent overfitting by the model and might be\nviable for GANs, as it decreases the accuracy of membership inference attacks [68, 69], but at the same time reduces the\nquality of the generated samples [68, 69, 137].\nManuscript submitted to ACM\n24\nMohus et. al.\nData perturbation is a method to perturb the output data produced by the model in order to protect against model\nextraction attacks [72, 137], by adding different kinds of noise, e.g., Gaussian Filtering and JPEG compression.\nAdversarial training is used by Liu and Hsieh [94], with an additional classifier to increase the generator’s robustness\nand generalizability. In contrast, Sajeeda and Hossain [126] find other research covering adversarial training for the\ndiscriminator and how adversarially trained GAN can be robust against perturbations.\nRobustness enhancement is a method for forcing the generator to generate samples within the target space, in\nparticular samples under adversarial and non-adversarial noise [37], in order to avoid lousy quality data from being\ngenerated under noise.\nFine pruning works by removing neurons in a neural network that does not provide good quality generated samples\nbut did not remove the introduced by-products from poisoning attacks and decreased the performance of the model\n[137].\nActivation Output Clustering analyzes one of the layers of a neural network and clusters the values from these, with\nthe hope of differentiating between actual and malicious data from a poisoning attack [47].\nInspection relies on analyzing the model parameters, i.e., static model inspection [120]; analyzing the model behavior\nin action, i.e., dynamic model inspection [120]; or analyzing the output of the model to compare with known adversarial\ndata [120]. The defensive measure appears unreliable in analyzing the output, and model inspection methods require\nknowledge of the adversary to be effective.\nLastly, some papers cover adversarial attacks that are not covered by any defensive measures, like membership\ninference [134, 159], poisoning [76, 127, 154], and adversarial examples [5].\n6.5\nDefenses against attacks targeting super-resolution:\nAs shown in Section 5.5, all attacks on super-resolution consists of adversarial examples.\nData pre-processing is one such defense, covered by Choi et al. [36], where the data size is reduced by one pixel and\nsubsequently resized to the original size, resulting in an increase of the measured PSNR on the adversarial input.\nEnsamble methods covered by Choi et al. [36] use a geometric self-ensemble method, where the original images are\ngeometrically transformed and passed through the model, subsequently inversely transformed and combined to create\nthe final upscaled image. This method increases the adversarial inputs measured by PSNR.\nAdversarial training is performed by including adversarial examples in the re-training of the super-resolution model,\nwhich increases model generalizability, with the model performing better against adversarial examples [26, 153].\n7\nRESULTS OF RQ3: CHALLENGES TO SECURE UL-BASED SYSTEMS\n7.1\nChallenges to secure systems using AutoEncoders\nAs autoencoders are applicable for several different purposes, there are also different challenges depending on the use.\nFor generative autoencoders, privacy [18, 30, 32, 69] attack is a problem, particularly when an adversary has access to\nthe model parameters [32]. Though there are defenses like differential privacy [18, 32, 139], fine pruning [47], activation\noutput clustering [47], the impact on generated samples is notable. Autoencoder systems for encoding/decoding, or\nnoise reduction, are also vulnerable, particularly against adversarial examples in the input [102, 138], or latent space\n[4, 40]. Defensive measures for these kinds of attacks on autoencoders are also not very well researched [125].\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n25\n7.2\nChallenges to secure UL supported classifications\nWe can divide the adversarial robustness of classification into two groups. Firstly is the use of UL systems to provide\nadversarial robustness of the classifier, e.g., using an autoencoder in front of the classifier [80]. Secondly is the exploration\nof adversarial robustness in systems employing UL in tandem with classification, e.g., using clustering to separate\nmalicious and legitimate data for classification [12], training classifiers on generated samples [18, 33, 64, 85, 106, 118, 154],\nor employing a classifier in addition to discriminator in a GAN [94]. In this case, while there exist several attacks, like\npoisoning [53, 95, 147, 154], defensive measures in the literature is mainly theoretical, like game theory [12].\n7.3\nChallenges to secure clustering\nWhile clustering as a concept relies on finding optimal groups of samples based on some distance metric, how the\ndistance is calculated impacts the vulnerabilities and adversarial robustness of clustering. It is speculated that the\nspecific poisoning attacks by Biggio et al. [20] might only apply to single-linkage hierarchical clustering, while other\nclustering methods would not. Additionally, known vulnerabilities of clustering are still not explored enough in the\nliterature, as most attacks focus on specific clustering methods [19–21, 123]. Defending clustering methods is also not\nvery well explored [20], with methods like adversarial simulation [20] and general privacy constraints [123] left as\nfuture work in the surveyed papers.\n7.4\nChallenges to secure GAN-based systems\nAdversarial robustness for GAN models is covered by various papers, with many limitations. One of the major limitations\nis the compromise between GAN utility and the implementation of defensive measures. Differential privacy provides\nprivacy for training samples [32, 33, 52, 55, 64, 68, 84, 85, 89, 97, 98, 118, 137, 139, 141, 149, 156]; and dropout [68, 69, 137]\nand weight normalization [68, 137] reduce the effectiveness of membership inference attacks. Simultaneously, the\ndefenses increase computational cost [32, 55, 64, 68, 137], lower the quality of the generated data [32, 52, 55, 64, 67–\n69, 72, 84, 85, 89, 97, 106, 137, 139, 148, 149], lower the stability of the model [5, 33, 52, 137, 148, 149, 158], and is not\neffective for other kinds of inference attacks [71, 89]. When it comes to poisoning attacks, defensive measures that work\nwith discriminatory models, e.g., fine pruning and activation output clustering, do not work to prevent or detect poisoning\nattacks against generative models like GAN [47]. Other solutions like output inspection [120] rely on knowledge of\nthe poisoning attack and would be impractical to implement outside particular situations. Additionally, the notion\nof generalization [32, 55, 67–69, 71, 84, 90, 102, 134, 137] and diversity [68, 71, 102, 134, 156, 158] in generated data is\ndiscussed as underlying properties of GAN that influence the effectiveness of attacks, e.g., when the GAN produces\ndiverse but non-generalized samples, the model is vulnerable to membership inference [32, 55, 67–69, 71, 90, 102, 134, 137].\nIn the literature, attacks like data injection [137] and defenses like data augmentation [47, 137] are not explored but\nmentioned as potential future work. No single defensive method can defend against several attacks simultaneously\n[126].\n7.5\nChallenges to secure super-resolution-based systems\nIn the papers covering adversarial robustness of super-resolution models [26, 36, 153], the papers do not cover the\nlimitations of the work, other than \"more advanced defense methods can be investigated in the future work\" [36] about\ndefenses against adversarial examples attacks.\nManuscript submitted to ACM\n26\nMohus et. al.\n8\nDISCUSSION\n8.1\nComparison with related work\nLiu et al. [93], Guan et al. [63], and Zhang et al. [155] do not outline their method of discovering the list of papers used\nfor their analysis, and Martins et al. [99] only outline their search engines and keywords. This study follows a more\nSLR systematic approach, and the papers covered by this study are, therefore, more complete and representative of the\nstate of the field.\nTable 5 compares the papers regarding the number of papers and which defenses were covered. Several of the\ndefenses are covered both by the other surveys and this one, like adversarial training, smoothing/gradient masking,\nand differential privacy. However, this paper does cover a multitude of papers related to each defense, while most of\nthe other papers cover only one or a few papers on the same defensive measure. The exception is Zhang et al. [155],\nwhich covers differential privacy with several papers. The most notable difference between this paper, and the other\npapers, is that this paper has not found any defenses using homomorphic encryption, which can be explainable by 1) this\nSLR did not craft its search well enough to include homomorphic encryption, 2) unsupervised learning does not work\nwell with homomorphic encryption, or 3) homomorphic encryption has not yet been widely used as a defensive measure\nfor unsupervised learning. Doing a surface search on Google Scholar on homomorphic encryption with unsupervised\nlearning does not reveal a large number of papers on this subject, which leads us to believe that number three is the\nmost likely answer as to why it does not show up in this SLR. With this, research into defensive measures for UL using\nhomomorphic encryption might be a viable future endeavor. Other defensive measures that are present in the other\npapers but not in ours, like reject on negative impact, defensive distillation, universal perturbation defense method, do\nnot appear to be applicable in an unsupervised scenario, as they are only applicable for classification. Lastly, defenses\nlike transferability block, MagNet, reverse poisoning, query distribution detection, hierarchical model stacking, alternating\ntraining, defensive perturbation, and layer separation appear on the surface to be applicable in an unsupervised scenario,\nand in future research should be evaluated to find out whether or not they work for UL.\n8.2\nImplications to academia\nOur study gives a structured and comprehensive summary of the state-of-the-art adversarial robustness of unsupervised\nlearning and how this can be used for further research. Attack knowledge is one of the core parts of the attack, and\nwhile specific attacks like adversarial examples and membership inference can work with minimal knowledge, all attacks\nperform noticeably better with an increase in knowledge of the system. In this regard, our paper aims at structuring and\nsummarizing the performance of attacks and defenses depending on the amount of knowledge, which could be used in\nfuture research to make comparisons easier. Specific attacks also employ substitute models trained to mimic the actual\nmodel and perform full-knowledge attacks on these, which can transfer to the actual model. Future research should\ntherefore consider a broader range of knowledge scenarios in their analysis, as a black box attack could be turned into a\nwhite box attack using such methods.\nAnother critical attack aspect is the attack goal. On the one hand, our paper shows that most attacks focus on\ninfringing on the privacy of the target, which in turn motivates future research into finding methods for ensuring\nprivacy since this is the most popular attack vector. Additionally, we believe our findings that differential privacy impact\nmodel performance should motivate future research into providing privacy guarantees with less impact on performance.\nOn the other hand, our paper also shows that integrity attacks are effective. However, their defenses are not given as\nmuch focus in the research, which is something we wish to motivate further research in doing.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n27\nTable 5. Comparison between surveys on adversarial robustness in machine learning. Cross (✗) shows that the column label is not\napplicable, Tick (✓) shows that the column label does apply, and Asterisk (∗) shows that the column label partially applies.\nRef.\nShort description\nNum.\npapers\nDefenses Covered\n[93]\nAdversarial\nEx-\namples\nattacks\nagainst Machine\nLearning\n42\nReject on Negative Impact [104], Adversarial Training [60], Defense Distilla-\ntion [25, 112], Ensemble method [2, 130, 142], Differential Privacy [1, 49, 86],\nHomomorphic Encryption [42, 152]\n[99]\nAdversarial\nEx-\namples\nagainst\ndetection\ntech-\nniques\n20\nAdversarial Training [60, 129], Gradient Masking [48, 110, 111], Defensive\nDistillation [112], Feature Squeezing [150], Transferability Block [70], Uni-\nversal Perturbation Defense Method [3], MagNet [101]\n[63]\nImpact of ML on\nsecurity, and ro-\nbustness of AI sys-\ntems\n14\nReject on Negative Impact [104], Learning Algorithm Improvement [15, 91],\nAdversarial Retraining, Output Smoothness [62], Defensive Distillation [25,\n112], Differential Privacy [22], Homomorphic Encryption [42]\n[155]\nPrivacy\nthreats\nand\nmitigations\nwithin ML\n45\nReverse Poisoning [107], Query Distribution Detection [78], Hierarchical\nModel Stacking [128], Alternating Training [103], Defensive Perturbation\n[74, 75], Collaborative Training [100, 132], Homomorphic Encryption [14,\n28, 56, 57, 92, 121, 157] , Differential Privacy [1, 11, 50, 109, 115, 116, 132] ,\nLayer Separation [35, 108]\nThis\nwork\nA systematic re-\nview on adversar-\nial robustness of\nunsupervised ma-\nchine learning\n86\nAdversarial training [26, 34, 80, 94, 125, 126, 151, 153], Differential Privacy [10,\n17, 18, 30–33, 52, 55, 64, 68, 71, 77, 84, 85, 89, 90, 97, 98, 102, 118, 137, 139–141,\n148, 149, 156, 158], Dropout [68, 69, 137], Fine Pruning [137], Smoothing [16,\n27, 80, 136, 137, 146], Weight Normalization [68, 137], Robustness Constraints\n[39], Activation Output Clustering [47, 137], MCMC [88], Relabeling [53],\nGame Theory [12], Robustness Enhancement [124], Privacy Enhancement\n[123], Inspection [120], Data pre-processing [36] , Ensamble [36]\nFuture research would also benefit from this paper concerning the kinds of attacks leveraged against specific\ntechnologies. We can, for example, see that poisoning attacks target most technologies; or that there are no privacy\nattacks on clustering, which future research could use as motivation for studying these topics.\nWe also believe the summaries on metrics can enable future research with both motivations for selecting specific\nmetrics and categorizing which metrics are used by which papers, making comparisons between papers easier.\nRegarding defenses, we believe our paper provides future research with several significant findings. We categorize\nhow each defensive measure is used, for which technology, and against which attack they are leveraged. We hope to\nmotivate future research to utilize this survey in finding defenses and make for easier comparisons when researching\ndefensive measures. We have also presented how each defense performs in terms of effectiveness against specific attacks\nand, where applicable, how they impact the model’s performance. In turn, we hope to motivate future research into less\nperformance-reducing defenses. Lastly, while many papers cover complete knowledge scenarios to defend against a\nworst-case attack, only focusing on one level of knowledge might also be a problem, as defensive measures in different\nattacker knowledge scenarios are not necessarily transferable. Therefore, as mentioned in regards to attack knowledge,\nwe believe future research should consider how the defense is impacted when employed against attacks in different\nknowledge scenarios.\nManuscript submitted to ACM\n28\nMohus et. al.\nOne of the main challenges facing adversarially robust UL is the general need for more focus on constructing\ndefensive measures against attacks. Much research focuses primarily on the vulnerability of a specific model without\nformulating defenses. Additionally, when research does focus on defensive measures, it is mostly against specific attacks.\nWhile exploring specific defenses is good, there is a noticeable lack of more general defensive measures, with the big\nexception of differential privacy, which has seen the most focus out of any defense in UL. We, therefore, believe that\nfuture research would benefit from focusing on more generalizable defenses.\nWe believe our model in Figure 3 of analyzing adversarial robustness, i.e., looking at target type, attack method,\nattack knowledge, attack goal, attack focus, metrics, and defenses can be used in future research as a guide for what is\nessential to cover in this kind of research, in order to structure knowledge and make comparisons of results between\npapers easier. Future research also needs to be performed in a manner that enables easy comparisons between results\nby explicitly defining these different categories.\n8.3\nImplication to industry\nThose in the industry who utilize any of the technologies presented, e.g., clustering for anomaly detection or synthetic\ndata generation, can use the results in this paper to better evaluate their use of this technology, e.g., in risk assessment.\nAdditionally, since a significant part of this paper explicitly outlines the state-of-the-art concerning defensive measures\nfor these technologies, this paper would be a good resource for finding and implementing defensive measures to protect\ntheir UL models. Lastly, our focus on finding vulnerabilities in these technologies could motivate the industry to consider\nthe security of their models and not just use unsupervised models for their utility.\n8.4\nThreats to validity\nWe strictly followed the SLR guideline to ensure the method was unambiguous and considered biases that could\ninfluence the results’ quality. However, we could still identify some possible validity threats related to this study.\nThe researchers’ knowledge of adversarial robustness in unsupervised learning would inevitably impact the search\nquery. It would impact the specific words and the decision for when the query is good enough. The researchers have\ntried to mitigate this impact by 1) Documenting the specific papers that served as a basis for what the search should\nproduce and 2) The initial search query would only be accepted if it produced between 1000 and 10 000 papers.\nAnother possible threat to validity is the research databases used for finding papers. This study covered as many\nrelevant databases as possible, including IEEE Xplore, ACM Digital Library, arXiv, and Scopus.\nWhen filtering the papers, for the first round of filtration, where papers were excluded based on the title alone,\nthe researchers had to decide to exclude instead of include a paper explicitly. The reasoning is that deciding to\nexclude requires more thought than an inclusive decision, making the researcher less likely to exclude relevant papers.\nFrom the second round onward, the researchers documented their reasoning for exclusion or inclusion and produced\ndocumentation that could be scrutinized.\nRegarding the quality assurance of the papers, there are also several possible biases to consider. First, there is the\nbias in choosing which categories contribute to the overall quality score. For this, we considered four different papers\n[51, 73, 79, 82] for constructing a list of questions. Of these, [79] and [82] appeared most relevant. We compared these\ntwo and concluded that neither paper was better than the other, ultimately producing the 11 questions for the QA by\nexcluding one irrelevant question from [82]. Secondly, there is a bias in how the papers were graded based on these\nquestions. To combat this, after the QA of the remaining 28 papers in the initial round, we conducted a kappa analysis\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n29\non the scoring between the two authors to indicate whether the scoring was externally valid. The two authors scored a\nrandom sample of eight papers, and a weighted kappa score was calculated, producing a weighted kappa score of 0.68.\n9\nCONCLUSION AND FUTURE WORK\nThe existing literature on adversarial robustness in machine learning needs more coverage on unsupervised machine\nlearning technologies. We have therefore performed a systematic literature review to summarize adversarial attacks,\ndefenses, and research gaps related to unsupervised machine learning. From an initial list of 9129 papers, we have\nidentified a list of 86 primary studies. We summarized the attack types, properties of targeted systems, metrics used for\nattack success, and the impact of the attacks. For each target type, we summarized and analyzed the papers in relation\nto 1) attack type, attack goal, attack focus, attack knowledge, and metrics, 2) summarized and analyzed the papers relating\nto defense, and 3) summarized and analyzed the current challenges in research into adversarial robustness of UL.\nOne of the notable findings is that, except for differential privacy, the field of unsupervised learning has a noticeable\nlack of focus on effective defensive methods, as several papers also conclude that defensive methods used in other fields\nof machine learning do not work in the context of unsupervised learning. Additionally, despite the fact that unsupervised\nlearning systems are fairly diverse in function and form, the metrics used to determine whether attacks are successful\nare primarily shared, and many adversaries use distance metrics to determine the strength of the attack to avoid\ndetection. Knowing this, crafting more sophisticated detection methods that would not be affected by distance-metric\nlimitations might be worthwhile.\nAs mentioned in the discussion, further research into poisoning attacks is a worthwhile direction due to both the\napplicability of poisoning attacks against, seemingly, any unsupervised machine learning (and perhaps all types of\nmachine learning) and due to the amount of data that is used in UL, which could contain poisoned samples since the\ndata could be sourced from public databases or data that anyone can contribute to.\nREFERENCES\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential\nprivacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 308–318.\n[2] Mahdieh Abbasi and Christian Gagné. 2017. Robustness to adversarial examples through an ensemble of specialists. arXiv preprint arXiv:1702.06856\n(2017).\n[3] Naveed Akhtar, Jian Liu, and Ajmal Mian. 2018. Defense against universal adversarial perturbations. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 3389–3398.\n[4] Abdullatif Albaseer, Bekir Sait Ciftler, and Mohamed M Abdallah. 2020. Performance evaluation of physical attacks against e2e autoencoder over\nrayleigh fading channel. In 2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT). IEEE, 177–182.\n[5] Motasem Alfarra, Juan C Pérez, Anna Frühstück, Philip HS Torr, Peter Wonka, and Bernard Ghanem. 2022. On the Robustness of Quality Measures\nfor GANs. arXiv preprint arXiv:2201.13019 (2022).\n[6] A Tahseen Ali, Hasanen S Abdullah, and Mohammad N Fadhil. 2021. Voice recognition system using machine learning techniques. Materials\nToday: Proceedings (2021), 1–7.\n[7] Saad Ali and Mubarak Shah. 2005. A supervised learning framework for generic object detection in images. In Tenth IEEE International Conference\non Computer Vision (ICCV’05) Volume 1, Vol. 2. IEEE, 1347–1354.\n[8] Imrul Chowdhury Anindya and Murat Kantarcioglu. 2018. Adversarial anomaly detection using centroid-based clustering. In 2018 IEEE International\nConference on Information Reuse and Integration (IRI). IEEE, 1–8.\n[9] Saeed Anwar, Salman Khan, and Nick Barnes. 2020. A deep journey into super-resolution: A survey. ACM Computing Surveys (CSUR) 53, 3 (2020),\n1–34.\n[10] Godwin Badu-Marfo, Bilal Farooq, and Zachary Patterson. 2020. A Differentially Private Multi-Output Deep Generative Networks Approach For\nActivity Diary Synthesis. arXiv preprint arXiv:2012.14574 (2020).\n[11] Ho Bae, Jaehee Jang, Dahuin Jung, Hyemi Jang, Heonseok Ha, Hyungyu Lee, and Sungroh Yoon. 2018. Security and privacy issues in deep learning.\narXiv preprint arXiv:1807.11655 (2018).\nManuscript submitted to ACM\n30\nMohus et. al.\n[12] Nikhil Banerjee, Thanassis Giannetsos, Emmanouil Panaousis, and Clive Cheong Took. 2018. Unsupervised learning for trustworthy IoT. In 2018\nIEEE international conference on fuzzy systems (FUZZ-IEEE). IEEE, 1–8.\n[13] Dor Bank, Noam Koenigstein, and Raja Giryes. 2020. Autoencoders. arXiv preprint arXiv:2003.05991 (2020).\n[14] Ankur Bansal, Tingting Chen, and Sheng Zhong. 2011. Privacy preserving back-propagation neural network learning over arbitrarily partitioned\ndata. Neural Computing and Applications 20 (2011), 143–150.\n[15] Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and Jaehoon Amir Safavi. 2017. Mitigating poisoning attacks on machine learning models: A data\nprovenance based approach. In Proceedings of the 10th ACM workshop on artificial intelligence and security. 103–110.\n[16] Ben Barrett, Alexander Camuto, Matthew Willetts, and Tom Rainforth. 2022. Certifiably robust variational autoencoders. In International Conference\non Artificial Intelligence and Statistics. PMLR, 3663–3683.\n[17] Brett K Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams, Ran Lee, Sanjeev P Bhavnani, James Brian Byrd, and Casey S Greene. 2019. Privacy-\npreserving generative deep neural networks support clinical data sharing. Circulation: Cardiovascular Quality and Outcomes 12, 7 (2019), e005122.\n[18] Daniel Bernau, Jonas Robl, and Florian Kerschbaum. 2022. Assessing Differentially Private Variational Autoencoders under Membership Inference.\narXiv preprint arXiv:2204.07877 (2022).\n[19] Battista Biggio, Samuel Rota Bulò, Ignazio Pillai, Michele Mura, Eyasu Zemene Mequanint, Marcello Pelillo, and Fabio Roli. 2014. Poisoning\ncomplete-linkage hierarchical clustering. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural\nand Syntactic Pattern Recognition (SSPR). Springer, 42–52.\n[20] Battista Biggio, Ignazio Pillai, Samuel Rota Bulò, Davide Ariu, Marcello Pelillo, and Fabio Roli. 2013. Is data clustering in adversarial settings\nsecure?. In Proceedings of the 2013 ACM workshop on Artificial intelligence and security. 87–98.\n[21] Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, and Fabio Roli. 2014. Poisoning behavioral\nmalware clustering. In Proceedings of the 2014 workshop on artificial intelligent and security workshop. 27–36.\n[22] Daniel M Bittner, Anand D Sarwate, and Rebecca N Wright. 2017. Differentially Private Noisy Search with Applications to Anomaly Detection. In\nProceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 53–53.\n[23] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77–101.\n[24] Alexander Camuto, Matthew Willetts, Stephen Roberts, Chris Holmes, and Tom Rainforth. 2021. Towards a theoretical understanding of the\nrobustness of variational autoencoders. In International Conference on Artificial Intelligence and Statistics. PMLR, 3565–3573.\n[25] Nicholas Carlini and David Wagner. 2016. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311 (2016).\n[26] Angela Castillo, María Escobar, Juan C Pérez, Andrés Romero, Radu Timofte, Luc Van Gool, and Pablo Arbelaez. 2021. Generalized Real-World\nSuper-Resolution through Adversarial Robustness. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1855–1865.\n[27] Taylan Cemgil, Sumedh Ghaisas, Krishnamurthy Dj Dvijotham, and Pushmeet Kohli. 2019. Adversarially robust representations with smooth\nencoders. In International Conference on Learning Representations.\n[28] Hervé Chabanne, Amaury De Wargny, Jonathan Milgram, Constance Morel, and Emmanuel Prouff. 2017. Privacy-preserving classification on deep\nneural network. Cryptology ePrint Archive (2017).\n[29] Satish Chander and P Vijaya. 2021. Unsupervised learning methods for data clustering. In Artificial Intelligence in Data Mining. Elsevier, 41–64.\n[30] Varun Chandrasekaran, Darren Edge, Somesh Jha, Amit Sharma, Cheng Zhang, and Shruti Tople. 2021. Causally Constrained Data Synthesis for\nPrivate Data Release. arXiv preprint arXiv:2105.13144 (2021).\n[31] Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. 2020. Gs-wgan: A gradient-sanitized approach for learning differentially private generators.\nAdvances in Neural Information Processing Systems 33 (2020), 12673–12684.\n[32] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. 2020. Gan-leaks: A taxonomy of membership inference attacks against generative models. In\nProceedings of the 2020 ACM SIGSAC conference on computer and communications security. 343–362.\n[33] Jia-Wei Chen, Chia-Mu Yu, Ching-Chia Kao, Tzai-Wei Pang, and Chun-Shien Lu. 2022. DPGEN: Differentially Private Generative Energy-Guided\nNetwork for Natural Image Synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8387–8396.\n[34] Yizheng Chen, Yacin Nadji, Athanasios Kountouras, Fabian Monrose, Roberto Perdisci, Manos Antonakakis, and Nikolaos Vasiloglou. 2017. Practical\nattacks against graph-based clustering. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security. 1125–1142.\n[35] Jianfeng Chi, Emmanuel Owusu, Xuwang Yin, Tong Yu, William Chan, Patrick Tague, and Yuan Tian. 2018. Privacy partitioning: Protecting user\ndata during the deep learning inference phase. arXiv preprint arXiv:1812.02863 (2018).\n[36] Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, and Jong-Seok Lee. 2019. Evaluating robustness of deep image super-resolution against\nadversarial attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 303–311.\n[37] Grigorios G Chrysos, Jean Kossaifi, and Stefanos Zafeiriou. 2020. Rocgan: Robust conditional gan. International Journal of Computer Vision 128, 10\n(2020), 2665–2683.\n[38] Antonio Emanuele Cinà, Alessandro Torcinovich, and Marcello Pelillo. 2022. A black-box adversarial attack for poisoning clustering. Pattern\nRecognition 122 (2022), 108306.\n[39] Filipe Condessa and Zico Kolter. 2020. Provably robust deep generative models. arXiv preprint arXiv:2004.10608 (2020).\n[40] Antonia Creswell, Anil A Bharath, and Biswa Sengupta. 2017. Latentpoison-adversarial attacks on the latent space. arXiv preprint arXiv:1711.02879\n(2017).\n[41] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. 2018. Generative adversarial networks:\nAn overview. IEEE signal processing magazine 35, 1 (2018), 53–65.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n31\n[42] Ivan Damgård, Valerio Pastro, Nigel Smart, and Sarah Zakarias. 2012. Multiparty computation from somewhat homomorphic encryption. In\nAdvances in Cryptology–CRYPTO 2012: 32nd Annual Cryptology Conference, Santa Barbara, CA, USA, August 19-23, 2012. Proceedings. Springer,\n643–662.\n[43] Zhixiang Deng and Qian Sang. 2020. Harnessing the Adversarial Perturbation to Enhance Security in the Autoencoder-Based Communication\nSystem. Electronics 9, 2 (2020), 294.\n[44] David Denyer, David Tranfield, D Buchanan, and A Bryman. 2009. The Sage handbook of organizational research methods. Reference and Research\nBook News 24, 3 (2009), 776.\n[45] Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. 2020. Meta-sim2: Unsupervised learning of scene structure for synthetic data generation. In\nComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16. Springer, 715–733.\n[46] Dihuni. 2020. Every Day Big Data Statistics – 2.5 Quintillion Bytes of Data Created Daily. https://www.dihuni.com/2020/04/10/every-day-big-data-\nstatistics-2-5-quintillion-bytes-of-data-created-daily/\n[47] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong. 2019. Trojan attack on deep generative models in autonomous driving. In\nInternational Conference on Security and Privacy in Communication Systems. Springer, 299–318.\n[48] Vasisht Duddu. 2018. A survey of adversarial machine learning in cyber warfare. Defence Science Journal 68, 4 (2018), 356.\n[49] Cynthia Dwork. 2006. Differential privacy. In Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July\n10-14, 2006, Proceedings, Part II 33. Springer, 1–12.\n[50] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private data analysis. In Theory of\nCryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3. Springer, 265–284.\n[51] Tore Dybå and Torgeir Dingsøyr. 2008. Strength of evidence in systematic reviews in software engineering. In Proceedings of the Second ACM-IEEE\ninternational symposium on Empirical software engineering and measurement. 178–187.\n[52] Liyue Fan. 2020. A survey of differentially private generative adversarial networks. In The AAAI Workshop on Privacy-Preserving Artificial Intelligence.\n8.\n[53] Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves Le Traon. 2022. Influence-Driven Data Poisoning in Graph-Based\nSemi-Supervised Classifiers. In 2022 IEEE/ACM 1st International Conference on AI Engineering–Software Engineering for AI (CAIN). IEEE, 77–87.\n[54] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence information and basic countermeasures.\nIn Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 1322–1333.\n[55] Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, and Patrick Duverger. 2019. Differentially private generative adversarial networks\nfor time series, continuous, and discrete open data. In IFIP International Conference on ICT Systems Security and Privacy Protection. Springer, 151–164.\n[56] Craig Gentry. 2009. Fully homomorphic encryption using ideal lattices. In Proceedings of the forty-first annual ACM symposium on Theory of\ncomputing. 169–178.\n[57] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. 2016. Cryptonets: Applying neural networks\nto encrypted data with high throughput and accuracy. In International conference on machine learning. PMLR, 201–210.\n[58] George Gondim-Ribeiro, Pedro Tabacof, and Eduardo Valle. 2018. Adversarial attacks on variational autoencoders. arXiv preprint arXiv:1806.04646\n(2018).\n[59] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative\nadversarial networks. Commun. ACM 63, 11 (2020), 139–144.\n[60] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572\n(2014).\n[61] Nizar Grira, Michel Crucianu, and Nozha Boujemaa. 2004. Unsupervised and semi-supervised clustering: a brief survey. A review of machine\nlearning techniques for processing multimedia content 1, 2004 (2004), 9–16.\n[62] Shixiang Gu and Luca Rigazio. 2014. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068\n(2014).\n[63] Zhenyu Guan, Liangxu Bian, Tao Shang, and Jianwei Liu. 2018. When machine learning meets security issues: A survey. In 2018 IEEE international\nconference on intelligence and safety for robotics (ISR). IEEE, 158–165.\n[64] Saurabh Gupta, Arun Balaji Buduru, and Ponnurangam Kumaraguru. 2020. imdpgan: Generating private and specific data with generative\nadversarial networks. In 2020 Second IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA).\nIEEE, 64–72.\n[65] Florian Hahne, Wolfgang Huber, Robert Gentleman, Seth Falcon, R Gentleman, and VJ Carey. 2008. Unsupervised machine learning. Bioconductor\ncase studies (2008), 137–157.\n[66] Changhee Han, Hideaki Hayashi, Leonardo Rundo, Ryosuke Araki, Wataru Shimoda, Shinichi Muramatsu, Yujiro Furukawa, Giancarlo Mauri, and\nHideki Nakayama. 2018. GAN-based synthetic brain MR image generation. In 2018 IEEE 15th international symposium on biomedical imaging (ISBI\n2018). IEEE, 734–738.\n[67] Parisa Hassanzadeh and Robert E Tillman. 2022. Generative Models with Information-Theoretic Protection Against Membership Inference Attacks.\narXiv preprint arXiv:2206.00071 (2022).\n[68] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2017. Logan: Membership inference attacks against generative models.\narXiv preprint arXiv:1705.07663 (2017).\nManuscript submitted to ACM\n32\nMohus et. al.\n[69] Benjamin Hilprecht, Martin Härterich, and Daniel Bernau. 2019. Monte Carlo and Reconstruction Membership Inference Attacks against Generative\nModels. Proc. Priv. Enhancing Technol. 2019, 4 (2019), 232–249.\n[70] Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. Blocking transferability of adversarial examples in\nblack-box learning systems. arXiv preprint arXiv:1703.04318 (2017).\n[71] Aoting Hu, Renjie Xie, Zhigang Lu, Aiqun Hu, and Minhui Xue. 2021. TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized\nTabular Data Releasing. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security. 2096–2112.\n[72] Hailong Hu and Jun Pang. 2021. Model Extraction and Defenses on Generative Adversarial Networks. arXiv preprint arXiv:2101.02069 (2021).\n[73] Martin Ivarsson and Tony Gorschek. 2011. A method for evaluating rigor and industrial relevance of technology evaluations. Empirical Software\nEngineering 16, 3 (2011), 365–395.\n[74] Jinyuan Jia and Neil Zhenqiang Gong. 2018. Attriguard: A practical defense against attribute inference attacks via adversarial machine learning. In\n27th {USENIX} security symposium ({USENIX} security 18). 513–529.\n[75] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. 2019. Memguard: Defending against black-box membership\ninference attacks via adversarial examples. In Proceedings of the 2019 ACM SIGSAC conference on computer and communications security. 259–274.\n[76] Ruinan Jin and Xiaoxiao Li. 2022. Backdoor Attack is a Devil in Federated GAN-Based Medical Image Synthesis. In International Workshop on\nSimulation and Synthesis in Medical Imaging. Springer, 154–165.\n[77] James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. 2018. PATE-GAN: Generating synthetic data with differential privacy guarantees. In\nInternational conference on learning representations.\n[78] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. 2019. PRADA: protecting against DNN model stealing attacks. In 2019 IEEE European\nSymposium on Security and Privacy (EuroS&P). IEEE, 512–527.\n[79] Staffs Keele et al. 2007. Guidelines for performing systematic literature reviews in software engineering. Technical Report. Technical report, ver. 2.3\nebse technical report. ebse.\n[80] Byeong Cheon Kim, Jung Uk Kim, Hakmin Lee, and Yong Man Ro. 2020. Revisiting role of autoencoders in adversarial settings. In 2020 IEEE\nInternational Conference on Image Processing (ICIP). IEEE, 1856–1860.\n[81] Diederik P Kingma, Max Welling, et al. 2019. An introduction to variational autoencoders. Foundations and Trends® in Machine Learning 12, 4\n(2019), 307–392.\n[82] Barbara Ann Kitchenham, David Budgen, and Pearl Brereton. 2015. Evidence-based software engineering and systematic reviews. Vol. 4. CRC press.\n[83] Jernej Kos, Ian Fischer, and Dawn Song. 2018. Adversarial examples for generative models. In 2018 ieee security and privacy workshops (spw). IEEE,\n36–42.\n[84] Tabea Kossen, Manuel A Hirzel, Vince I Madai, Franziska Boenisch, Anja Hennemuth, Kristian Hildebrand, Sebastian Pokutta, Kartikey Sharma,\nAdam Hilbert, Jan Sobesky, et al. 2022. Toward Sharing Brain Images: Differentially Private TOF-MRA Images With Segmentation Labels Using\nGenerative Adversarial Networks. Frontiers in artificial intelligence 5 (2022).\n[85] Aditya Kunar, Robert Birke, Zilong Zhao, and Lydia Chen. 2021. DTGAN: Differential Private Training for Tabular GANs. arXiv preprint\narXiv:2107.02521 (2021).\n[86] Matt Kusner, Jacob Gardner, Roman Garnett, and Kilian Weinberger. 2015. Differentially private Bayesian optimization. In International conference\non machine learning. PMLR, 918–927.\n[87] Anna Kuzina, Max Welling, and Jakub M Tomczak. 2021. Diagnosing vulnerability of variational auto-encoders to adversarial attacks. arXiv\npreprint arXiv:2103.06701 (2021).\n[88] Anna Kuzina, Max Welling, and Jakub M Tomczak. 2022. Defending Variational Autoencoders from Adversarial Attacks with MCMC. arXiv\npreprint arXiv:2203.09940 (2022).\n[89] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. Using GANs for sharing networked time series data: Challenges, initial\npromise, and open questions. In Proceedings of the ACM Internet Measurement Conference. 464–483.\n[90] Zinan Lin, Vyas Sekar, and Giulia Fanti. 2021. On the privacy properties of gan-generated samples. In International Conference on Artificial\nIntelligence and Statistics. PMLR, 1522–1530.\n[91] Chang Liu, Bo Li, Yevgeniy Vorobeychik, and Alina Oprea. 2017. Robust linear regression against training data poisoning. In Proceedings of the 10th\nACM workshop on artificial intelligence and security. 91–102.\n[92] Jian Liu, Mika Juuti, Yao Lu, and Nadarajah Asokan. 2017. Oblivious neural network predictions via minionn transformations. In Proceedings of the\n2017 ACM SIGSAC conference on computer and communications security. 619–631.\n[93] Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor CM Leung. 2018. A survey on security threats and defensive techniques of machine\nlearning: A data driven view. IEEE access 6 (2018), 12103–12117.\n[94] Xuanqing Liu and Cho-Jui Hsieh. 2019. Rob-gan: Generator, discriminator, and adversarial attacker. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 11234–11243.\n[95] Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. 2019. A unified framework for data poisoning attack to graph-based semi-supervised\nlearning. arXiv preprint arXiv:1910.14147 (2019).\n[96] Xing Liu, Hansong Xu, Weixian Liao, and Wei Yu. 2019. Reinforcement learning for cyber-physical systems. In 2019 IEEE International Conference\non Industrial Internet (ICII). IEEE, 318–327.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n33\n[97] Yi Liu, Jialiang Peng, JQ James, and Yi Wu. 2019. PPGAN: Privacy-preserving generative adversarial network. In 2019 IEEE 25Th international\nconference on parallel and distributed systems (ICPADS). IEEE, 985–989.\n[98] Yunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, and Bo Li. 2021. G-PATE: Scalable Differentially Private\nData Generator via Private Aggregation of Teacher Discriminators. Advances in Neural Information Processing Systems 34 (2021), 2965–2977.\n[99] Nuno Martins, José Magalhães Cruz, Tiago Cruz, and Pedro Henriques Abreu. 2020. Adversarial machine learning applied to intrusion and malware\nscenarios: a systematic review. IEEE Access 8 (2020), 35403–35419.\n[100] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep\nnetworks from decentralized data. In Artificial intelligence and statistics. PMLR, 1273–1282.\n[101] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC conference\non computer and communications security. 135–147.\n[102] Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Nabajyoti Patowary, and Juan Lavista Ferres. 2021. privGAN: Protecting GANs from membership\ninference attacks at low cost to utility. Proc. Priv. Enhancing Technol. 2021, 3 (2021), 142–163.\n[103] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine learning with membership privacy using adversarial regularization. In Proceedings\nof the 2018 ACM SIGSAC conference on computer and communications security. 634–646.\n[104] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini, Charles Sutton, JD Tygar, and Kai Xia.\n2009. Misleading learners: Co-opting your spam filter. Machine learning in cyber trust: Security, privacy, and reliability (2009), 17–51.\n[105] D Nkashama, Arian Soltani, Jean-Charles Verdier, Marc Frappier, Pierre-Marting Tardif, and Froduald Kabanza. 2022. Robustness Evaluation of\nDeep Unsupervised Learning Algorithms for Intrusion Detection Systems. arXiv preprint arXiv:2207.03576 (2022).\n[106] Witold Oleszkiewicz, Peter Kairouz, Karol Piczak, Ram Rajagopal, and Tomasz Trzciński. 2018. Siamese generative adversarial privatizer for\nbiometric data. In Asian Conference on Computer Vision. Springer, 482–497.\n[107] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. 2020. Prediction poisoning: Utility-constrained defenses against model stealing attacks. In\nInternational Conference on Representation Learning (ICLR) 2020.\n[108] Seyed Ali Osia, Ali Shahin Shamsabadi, Sina Sajadmanesh, Ali Taheri, Kleomenis Katevas, Hamid R Rabiee, Nicholas D Lane, and Hamed Haddadi.\n2020. A hybrid deep learning architecture for privacy-preserving mobile analytics. IEEE Internet of Things Journal 7, 5 (2020), 4505–4518.\n[109] Nicolas Papernot, Martín Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. 2016. Semi-supervised knowledge transfer for deep learning\nfrom private training data. arXiv preprint arXiv:1610.05755 (2016).\n[110] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against\nmachine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security. 506–519.\n[111] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2016. Towards the science of security and privacy in machine learning.\narXiv preprint arXiv:1611.03814 (2016).\n[112] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against\ndeep neural networks. In 2016 IEEE symposium on security and privacy (SP). IEEE, 582–597.\n[113] José M Peña, Pedro A Gutiérrez, César Hervás-Martínez, Johan Six, Richard E Plant, and Francisca López-Granados. 2014. Object-based image\nclassification of summer crops with machine learning methods. Remote sensing 6, 6 (2014), 5019–5041.\n[114] Mark Petticrew and Helen Roberts. 2008. Systematic reviews in the social sciences: A practical guide. John Wiley & Sons.\n[115] NhatHai Phan, Yue Wang, Xintao Wu, and Dejing Dou. 2016. Differential privacy preservation for deep auto-encoders: an application of human\nbehavior prediction. In Thirtieth AAAI Conference on Artificial Intelligence.\n[116] NhatHai Phan, Xintao Wu, Han Hu, and Dejing Dou. 2017. Adaptive laplace mechanism: Differential privacy preservation in deep learning. In 2017\nIEEE international conference on data mining (ICDM). IEEE, 385–394.\n[117] Walter Hugo Lopez Pinaya, Sandra Vieira, Rafael Garcia-Dias, and Andrea Mechelli. 2020. Autoencoders. In Machine learning. Elsevier, 193–208.\n[118] Jean-Francois Rajotte and Raymond T Ng. 2020. Private data sharing between decentralized users through the privGAN architecture. In 2020 IEEE\n24th International Enterprise Distributed Object Computing Workshop (EDOCW). IEEE, 37–42.\n[119] Geeta Rani, Upasana Pandey, Aniket Anil Wagde, and Vijaypal Singh Dhaka. 2023. A deep reinforcement learning technique for bug detection in\nvideo games. International Journal of Information Technology 15, 1 (2023), 355–367.\n[120] Ambrish Rawat, Killian Levacher, and Mathieu Sinn. 2021. The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks.\narXiv preprint arXiv:2108.01644 (2021).\n[121] Ronald L Rivest, Len Adleman, Michael L Dertouzos, et al. 1978. On data banks and privacy homomorphisms. Foundations of secure computation 4,\n11 (1978), 169–180.\n[122] Fabrizio Rocco. 2021. Deepfake generation: evaluating state of the art generative networks. (2021).\n[123] Clemens Rösner and Melanie Schmidt. 2018. Privacy preserving clustering with constraints. arXiv preprint arXiv:1802.02497 (2018).\n[124] Carl Sabottke, Daniel Chen, Lucas Layman, and Tudor Dumitraş. 2019. How to trick the Borg: threat models against manual and automated\ntechniques for detecting network attacks. Computers & Security 81 (2019), 25–40.\n[125] Meysam Sadeghi and Erik G Larsson. 2019. Physical adversarial attacks against end-to-end autoencoder communication systems. IEEE Communi-\ncations Letters 23, 5 (2019), 847–850.\n[126] Afia Sajeeda and BM Mainul Hossain. 2022. Exploring Generative Adversarial Networks and Adversarial Training. International Journal of Cognitive\nComputing in Engineering (2022).\nManuscript submitted to ACM\n34\nMohus et. al.\n[127] Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. 2020. Baaan: Backdoor attacks against autoencoder and\ngan-based machine learning models. arXiv preprint arXiv:2010.03007 (2020).\n[128] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. 2018. Ml-leaks: Model and data independent\nmembership inference attacks and defenses on machine learning models. arXiv preprint arXiv:1806.01246 (2018).\n[129] Swami Sankaranarayanan, Arpit Jain, Rama Chellappa, and Ser Nam Lim. 2018. Regularizing deep networks using efficient layerwise adversarial\ntraining. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.\n[130] Sailik Sengupta, Tathagata Chakraborti, and Subbarao Kambhampati. 2018. Mtdeep: boosting the security of deep neural nets against adversarial\nattacks with moving target defense. In Workshops at the thirty-second AAAI conference on artificial intelligence.\n[131] Yi Shi, Yalin E Sagduyu, and Tugba Erpek. 2020. Reinforcement learning for dynamic resource optimization in 5G radio access network slicing. In\n2020 IEEE 25th international workshop on computer aided modeling and design of communication links and networks (CAMAD). IEEE, 1–6.\n[132] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and\ncommunications security. 1310–1321.\n[133] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. 2015. Unsupervised learning of video representations using lstms. In International\nconference on machine learning. PMLR, 843–852.\n[134] Kin Sum Liu, Chaowei Xiao, Bo Li, and Jie Gao. 2018. Performing Co-Membership Attacks Against Deep Generative Models. arXiv e-prints (2018),\narXiv–1805.\n[135] M Leone Sumedha and Martin Weigt. 2008. Unsupervised and semi-supervised clustering by message passing: soft-constraint affinity propagation.\nThe European Physical Journal B 66 (2008), 125–135.\n[136] Chengjin Sun, Sizhe Chen, and Xiaolin Huang. 2020. Double backpropagation for training autoencoders against adversarial attack. arXiv preprint\narXiv:2003.01895 (2020).\n[137] Hui Sun, Tianqing Zhu, Zhiqiu Zhang, Dawei Jin Xiong, Wanlei Zhou, et al. 2021. Adversarial Attacks Against Deep Generative Models on Data: A\nSurvey. arXiv preprint arXiv:2112.00247 (2021).\n[138] Pedro Tabacof, Julia Tavares, and Eduardo Valle. 2016. Adversarial images for variational autoencoders. arXiv preprint arXiv:1612.00155 (2016).\n[139] Uthaipon Tao Tantipongpipat, Chris Waites, Digvijay Boob, Amaresh Ankit Siva, and Rachel Cummings. 2021. Differentially private synthetic\nmixed-type data generation for unsupervised learning. In 2021 12th International Conference on Information, Intelligence, Systems & Applications\n(IISA). IEEE, 1–9.\n[140] Patrick Tinsley, Adam Czajka, and Patrick Flynn. 2021. This face does not exist... but it might be yours! identity leakage in generative models. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 1320–1328.\n[141] Amirsina Torfi, Edward A Fox, and Chandan K Reddy. 2022. Differentially private synthetic medical data generation using convolutional gans.\nInformation Sciences 586 (2022), 485–500.\n[142] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. Ensemble adversarial training: Attacks\nand defenses. arXiv preprint arXiv:1705.07204 (2017).\n[143] Diane Walker and Florence Myrick. 2006. Grounded theory: An exploration of process and procedure. Qualitative health research 16, 4 (2006),\n547–559.\n[144] Kunfeng Wang, Chao Gou, Yanjie Duan, Yilun Lin, Xinhu Zheng, and Fei-Yue Wang. 2017. Generative adversarial networks: introduction and\noutlook. IEEE/CAA Journal of Automatica Sinica 4, 4 (2017), 588–598.\n[145] Matthijs J Warrens. 2015. Five ways to look at Cohen’s kappa. Journal of Psychology & Psychotherapy 5, 4 (2015), 1.\n[146] Matthew Willetts, Alexander Camuto, Tom Rainforth, Stephen Roberts, and Chris Holmes. 2019. Improving VAEs’ Robustness to Adversarial\nAttack. arXiv preprint arXiv:1906.00230 (2019).\n[147] Jun Wu and Jingrui He. 2021. Indirect Invisible Poisoning Attacks on Domain Adaptation. In Proceedings of the 27th ACM SIGKDD Conference on\nKnowledge Discovery & Data Mining. 1852–1862.\n[148] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. 2018. Differentially private generative adversarial network. arXiv preprint\narXiv:1802.06739 (2018).\n[149] Chugui Xu, Ju Ren, Deyu Zhang, Yaoxue Zhang, Zhan Qin, and Kui Ren. 2019. GANobfuscator: Mitigating information leakage under GAN via\ndifferential privacy. IEEE Transactions on Information Forensics and Security 14, 9 (2019), 2358–2371.\n[150] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint\narXiv:1704.01155 (2017).\n[151] Xu Yang, Cheng Deng, Kun Wei, Junchi Yan, and Wei Liu. 2020. Adversarial learning for robust deep clustering. Advances in Neural Information\nProcessing Systems 33 (2020), 9098–9108.\n[152] YC Yao, L Song, and E Chi. 2017. Investigation on distributed K-means clustering algorithm of homomorphic encryption. Comput. Technol. Develop.\n2 (2017), 81–85.\n[153] Jiutao Yue, Haofeng Li, Pengxu Wei, Guanbin Li, and Liang Lin. 2021. Robust real-world image super-resolution against adversarial attacks. In\nProceedings of the 29th ACM International Conference on Multimedia. 5148–5157.\n[154] Jiale Zhang, Junjun Chen, Di Wu, Bing Chen, and Shui Yu. 2019. Poisoning attack in federated learning using generative adversarial nets. In 2019\n18th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/13th IEEE International Conference On Big\nData Science And Engineering (TrustCom/BigDataSE). IEEE, 374–380.\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n35\n[155] Jiliang Zhang, Chen Li, Jing Ye, and Gang Qu. 2020. Privacy threats and protection in machine learning. In Proceedings of the 2020 on Great Lakes\nSymposium on VLSI. 531–536.\n[156] Longling Zhang, Bochen Shen, Ahmed Barnawi, Shan Xi, Neeraj Kumar, and Yi Wu. 2021. FedDPGAN: federated differentially private generative\nadversarial networks framework for the detection of COVID-19 pneumonia. Information Systems Frontiers 23, 6 (2021), 1403–1415.\n[157] Qingchen Zhang, Laurence T Yang, and Zhikui Chen. 2015. Privacy preserving deep computation model on cloud for big data feature learning.\nIEEE Trans. Comput. 65, 5 (2015), 1351–1362.\n[158] Xinyang Zhang, Shouling Ji, and Ting Wang. 2018. Differentially private releasing via deep generative model (technical report). arXiv preprint\narXiv:1801.01594 (2018).\n[159] Junhao Zhou, Yufei Chen, Chao Shen, and Yang Zhang. 2021. Property Inference Attacks Against GANs. arXiv preprint arXiv:2111.07608 (2021).\nManuscript submitted to ACM\n36\nMohus et. al.\nA\nSLR DETAILS\nThe questions for the QA:\n• Are the objectives, research questions, and hypotheses (if applicable), clear and relevant?\n• Are the case and its units of analysis well defined?\n• Is the suitability of the case to address the research questions clearly motivated?\n• Are the data collection procedures sufficient for the purpose of the case study (data sources, collection, validation)?\n• Is sufficient raw data presented to provide understanding of the case and the analysis?\n• Are the analysis procedures sufficient for the purpose of the case study (repeatable, transparent)?\n• Is a clear chain of evidence established from observations to conclusions?\n• Are threats to validity analyses conducted in a systematic way, and are countermeasures taken to reduce threats?\n• Is triangulation applied (multiple collection and analysis methods, multiple authors, multiple theories)?\n• Are ethical issues properly addressed (personal intentions, integrity, confidentiality, consent, review board\napproval)?\n• Are conclusions, implications for practice, and future research, suitably reported for its audience?\nThe full name for the abbreviated journals and conferences.\n• ACM workshop on Artificial intelligence and security (AISec)\n• Circulation\n• IEEE Communications Letters (IEEE COMML)\n• IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n• International Conference on Artificial Intelligence and Statistics (AISTATS)\n• IEEE International Conference on Image Processing (IEEE ICIP)\n• IEEE International Conference On Trust, Security And Privacy In Computing and Communications (TrustCom)\n• AAAI Workshop on Privacy-Preserving Artificial Intelligence (PPAI)\n• ACM Transactions on Internet Technology (TOIT)\n• Computers & Security\n• Frontiers in Artificial Intelligence, (Front. Artif. Intell.)\n• EAI SecureComm\n• IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY (TIFS)\n• Privacy Enhancing Technologies (PoPETs)\n• IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)\n• IEEE International Conference on Information Reuse and Integration for Data Science (IEEE IRI)\n• IEEE Conference on Network Function Virtualization and Software Defined Networks (IEEE NFV-SDN)\n• IEEE international conference on fuzzy systems (FUZZ-IEEE)\n• ACM Internet Measurement Conference (ACM IMC)\n• ACM SIGKDD Conference on Knowledge Discovery and Data Mining (ACM SIGKDD)\n• International Conference on learning representations (ICLR)\n• International Journal of Computer Vision (IJCV)\n• Pattern Recognition (Pattern Recognit)\n• IEEE Symposium on Security and Privacy Workshops (IEEE SPW)\n• Structural, Syntactic, and Statistical Pattern Recognition (S+SSPR)\nManuscript submitted to ACM\nAdversarial Robustness in Unsupervised Machine Learning: A Systematic Review\n37\n• ACM SIGSAC conference on computer and communications security (CCS)\n• Conference on Neural Information Processing Systems (NeurIPS)\n• ACM international conference on Multimedia (ACM MM)\n• Electronics\n• Information Systems Frontiers (Inf Syst Front)\n• International Journal of Cognitive Computing in Engineering (JCCE)\nB\nCODE DESCRIPTIONS\nB.1\nTarget types\n• GAN: ML consisting of two models, a generator, and\na discriminator, training by competing against each\nother\n• AutoEncoder: Neural network which is trained on\nthe input and output being the same value\n• Clustering: Methods using distance functions to de-\ntermine the size of clusters of the same groups\n• Classification: Here, classification methods are mod-\nels which have the output from a UL system as its\ninput\n• Super Resolution: ML method for increasing the di-\nmensionality of a sample\nB.2\nAttack types\n• Adversarial Examples: Specific perturbation in input\nto cause disruption\n• Poisoning: Modification or addition of specifically\ncrafted training samples\n• Membership Inference: Extraction of knowledge\nabout training by using the output of a model\n• Evasion: Attacks crafted to avoid detection tech-\nniques\n• Obfuscation: Modification of input to be hidden in\nexisting groups or clusters\n• Bridging: Addition of crafted samples to bridge be-\ntween groups or clusters, causing disruption\n• Saturation: Flooding the system with an excess of\ndata\n• Model Extraction: Extraction of the model functions\nand parameters\n• Backdoor: Inclusion of a specific trigger in a model\ncausing disruption\n• Attribute Inference: Extraction of attributes of train-\ning data\n• Reconstruction: Extraction of statistical features of\nthe training set for a model\n• Accuracy Extraction: Extraction of a model through\ncomparison to generated samples\n• Fidelity Extraction: Extraction of a model through\ncomparison to training samples\nB.3\nMetrics\n• Accuracy/ F1-Score: Measurement of how many sam-\nples were correctly identified\n• L2: Measurement of perturbation, by taking the root\nof all squares of perturbations in the data\nManuscript submitted to ACM\n38\nMohus et. al.\n• AUCROC: Measurement of how well a model dis-\ntinguishes samples, by measuring true and false\npositive rates for all thresholds\n• L-Infinity: Measurement of perturbation, by taking\nthe largest values among all perturbations\n• False Positive: The number of values measured as\npositive, while being negative\n• BLER: The rate of erroneous blocks out of all blocks\n• True Positive: The number of values measured as\npositive, while being positive\n• Misclassification Score: Measurement of how many\nclassifications are not correct\n• L1: Measurement of perturbation, by taking the sum\nof all perturbations\n• Precision: A measurement of the rate of true posi-\ntives, out of all measured positives\n• SSIM: A quantification of the perceived quality of\nan image\n• AUCPRC: Measurement of how well a model per-\nforms, by measuring precision and recall over all\nthresholds\n• Recall: A measurement of the rate of true positives,\nout of all measured true values\n• True Negative: The number of values measured as\nnegative, while being negative\n• False Negative: The number of values measured as\nnegative, while being positive\n• Average Precision: Average value of the precision\nscore for all thresholds\n• RMSE: The Root Mean Square Error is a measure-\nment of average error in measurement\n• PSNR: Peak Signal to Noise Ratio measures the\namount of noise relative to real data is in a sam-\nple\n• MSSSIM: Multi Scale Structural Similarity Index\nMeasure uses SSIM on different scales\n• LPIPS: Learned Perceptual Image Patch Similarity\nmeasures similarities in features between generated\nand original samples\n• Jensen-Shannon Divergence: A measurement for de-\ntermining the similarities between probability dis-\ntributions\n• SRMSE: Standardised RMSE\n• Reconstruction Distance: A measurement for the dis-\ntance between the original and generated sample\n• Total Variation Distance: Measurement of the dis-\ntance between variations of different distributions\n• Attack Loss: Measurement of the distance between\nthe latent space distribution of samples with small\ndistortions\nB.4\nDefenses\n• Differential Privacy: Using mathematical proofs in training to force the constraint that a single sample cannot be\nknown to be used for training\n• Adversarial Training: The method of using an adversarial method for constructing samples the model otherwise\nwould be vulnerable to, and subsequently training using these samples\n• Gradient Masking: A method of obfuscating the gradients of the output of a model, to prevent useful information\nfrom being obtained\n• Game Theory: The employment of defensive methods according to a strategy using a multitude of parameters\n• Differentially Private Stochastic Gradient Descent (DPSGD): The application of DP to the training of GANs\n• Weight Normalization: Reparametrization of the weight vectors in an already existing NN\n• Bootstrapping: The use of existing robust models as the base of training a model\n• Fine Pruning: The removal of neurons in a NN which does not improve the performance\n• Dropout: Random removal of neurons during training\nManuscript submitted to ACM\n",
  "categories": [
    "cs.LG",
    "cs.CR",
    "I.2.0"
  ],
  "published": "2023-06-01",
  "updated": "2023-06-01"
}