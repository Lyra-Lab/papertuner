{
  "id": "http://arxiv.org/abs/2306.02144v2",
  "title": "A two-way translation system of Chinese sign language based on computer vision",
  "authors": [
    "Shengzhuo Wei",
    "Yan Lan"
  ],
  "abstract": "As the main means of communication for deaf people, sign language has a\nspecial grammatical order, so it is meaningful and valuable to develop a\nreal-time translation system for sign language. In the research process, we\nadded a TSM module to the lightweight neural network model for the large\nChinese continuous sign language dataset . It effectively improves the network\nperformance with high accuracy and fast recognition speed. At the same time, we\nimprove the Bert-Base-Chinese model to divide Chinese sentences into words and\nmapping the natural word order to the statute sign language order, and finally\nuse the corresponding word videos in the isolated sign language dataset to\ngenerate the sentence video, so as to achieve the function of text-to-sign\nlanguage translation. In the last of our research we built a system with sign\nlanguage recognition and translation functions, and conducted performance tests\non the complete dataset. The sign language video recognition accuracy reached\nabout 99.3% with a time of about 0.05 seconds, and the sign language generation\nvideo time was about 1.3 seconds. The sign language system has good performance\nperformance and is feasible.",
  "text": "A two-way translation system of Chinese sign\nlanguage based on computer vision\n1st Shengzhuo Wei\nHarbin Institute of Technology\nHarbin 150001, China\nCherrling2021@outlook.com\n2nd Yan Lan\nCommunication University of China\nBeiJing 100024, China\nlanyan761091450@gmail.com\nAbstract\nAs the main means of communication for deaf people, sign language has a special grammatical order, so it is meaningful\nand valuable to develop a real-time translation system for sign language. In the research process, we added a TSM module to the\nlightweight neural network model for the large Chinese continuous sign language dataset . It effectively improves the network\nperformance with high accuracy and fast recognition speed. At the same time, we improve the Bert-Base-Chinese model to\ndivide Chinese sentences into words and mapping the natural word order to the statute sign language order, and finally use the\ncorresponding word videos in the isolated sign language dataset to generate the sentence video, so as to achieve the function\nof text-to-sign language translation. In the last of our research we built a system with sign language recognition and translation\nfunctions, and conducted performance tests on the complete dataset. The sign language video recognition accuracy reached about\n99.3% with a time of about 0.05 seconds, and the sign language generation video time was about 1.3 seconds. The sign language\nsystem has good performance performance and is feasible.\nIndex Terms\nChinese sign language,sign language recognition, sign language generation, language model, Transformer\nI. INTRODUCTION\nA. Background\nWhile able-bodied people can use verbal language to communicate easily, people with hearing impairment (deaf or aphasic\npeople, etc.) need to communicate their thoughts through sign language. There are about 20.57 million deaf people in China,\naccounting for 1.67% of the total Chinese population, including about 800,000 children under the age of 7. They cannot\ncommunicate through language as normal people do, but communicate through sign language.\nSince most of the able-bodied people have not learned sign language, there are obstacles to promote sign language to\nmake it applicable for communication in normal society. Sign language recognition and interpretation technology facilitates\ncommunication between hearing-impaired and able-bodied people. Sign language research should not only enable hearing\npeople to read sign language, but also enable hearing people to understand what able-bodied people are saying.\nSign language recognition and interpretation are the former, and sign language generation research is the latter. This interaction\nprocess is particularly important for people with hearing impairment.\nTherefore, the study of sign language recognition and interpretation and sign language generation has important theoretical\nand applied values as well as social significance. Sign language recognition technology and sign language generation technology\ncan help daily communication, sign language interpretation and sign language education activities between deaf and able-bodied\npeople, as well as improve the social skills and quality of life of deaf people, promote mutual understanding and communication\nbetween deaf and able-bodied people, and have practicality and applicability in the deaf community.\nB. Related work\n1) Current status of research on sign language recognition: The sign language recognition method based on wearable\ndevices, i.e., using data gloves to directly obtain the hand shape, angle and relative position of fingers and other precise data of\nthe granting person, so as to obtain the main characteristics of sign language and use recognition algorithms for recognition. This\nmethod does not require pre-processing of various information, and the acquired data is accurate and free from environmental\ninterference, with the disadvantage of high cost and complexity of use.\nComputer vision-based sign language recognition method, i.e., the sign language gesture image or dynamic change in-\nformation is obtained by camera or radar and input to the algorithm for sign language recognition. Compared with sign\nlanguage recognition based on wearable devices, sign language personnel do not need wearable devices and the promotion is\nmore advantageous. The disadvantage is that the exclusion of fuzzy frames, the pre-processing of data to exclude interfering\ninformation and the accuracy of information is not high.\nTable I shows the current status of research on sign language recognition in China and abroad.\narXiv:2306.02144v2  [cs.CV]  17 Jun 2023\nTABLE I\nCURRENT STATUS OF SIGN LANGUAGE RECOGNITION RESEARCH\nResearch Team\nData collection method\nIdentify the content\nRecognition accuracy\nKadous\nPowerGlove data glove\n95 Australian Sign Language Words\n80%\nKakoty\nSensor Stacking Gloves\nSimple one-handed sign language\n96.7%\nJiang Li\nCAS-Glove Data Glove\nCommon Sign Language Words\n97.2%\nWang Jincheng\nData gloves and smartphones\nSimple Sign Language\n95%\nForeign Researcher Team\nKinect depth information\nCommon sign language words or simple sentences\n95%\nZhang Liangguo\nNew TMHMM Model\n439 common words in sign language\n92.5%\nLiu Yaling\nVGG-Net convolutional neural network\n22 letters of the alphabet\n97%\n2) Current status of research on sign language generation: With the continuous update of deep neural network theory, the\nupdate of intelligent devices and the development of computer technology, inspired by various generative models, researchers\naround the world have provided several methods or systems for solving the problem of sign language generation.\nTable II shows the current status of research on sign language generation in China and abroad.\nTABLE II\nCURRENT STATUS OF RESEARCH ON SIGN LANGUAGE GENERATION\nResearch Team\nGenerating method\nApplicable direction\nGreek scholar Karpouzis\nStandard virtual character animation technology\nVirtual animation of its signature\nGlauert\nCompositing to generate body animations\nSpeech-driven for e-government\nSaunders\nConfrontational multi-channel SLP approach\nGenerate facial features and mouth patterns\nBen\nImproved Transformer Generator\nPose video generation using conditional GAN networks\nStephanie\nNeural machine translation\nGenerating gesture videos from sign language word sequences\nJan Zelinka\nPose Sequence Generation Framework\nTrain a sign language generation network using this skeleton data\nIn summary, domestic and foreign research scholars have proposed many methods for sign language generation, however\nmost of the generation methods are useful for generating sequences containing temporal information, on the other hand Chinese\nsign language is a sign language expression different from other languages, and the generation methods proposed by research\nscholars for Japanese, English or Greek cannot be directly used for the generation of Chinese sign language, and the models\nare not universal. To address the above problems this paper proposes a deep learning-based sign language recognition and sign\nlanguage generation method and provides a system that can be used for two-way communication between deaf and normal\npeople, which can be extended for other scenarios.\nC. Research Content\nThis paper describes the importance of sign language in the communication between deaf and able-bodied people, and\npresents the research of Chinese sign language recognition algorithm and sign language generation algorithm based on deep\nlearning and the implementation of the system.\nDeep learning is an artificial neural network technique that can learn and classify a large amount of data. In sign language\nrecognition technology, deep learning technology can learn and classify a large amount of sign language images and video data\nto achieve recognition and translation of sign language. The research of Chinese sign language recognition technology and\nsign language generation technology and system based on deep learning can provide more accurate and efficient sign language\nrecognition and translation services for communication between deaf and able-bodied people.\nIn this paper, we combine different behavior recognition depth models, study the advantages and shortcomings of currently\nexisting sign language recognition algorithms, delve into the latest research results of deep learning at home and abroad,\nconsider the structural characteristics and structural advantages of existing convolutional neural network models, and explore\nthe applicability, stability, and reliability of different structural convolutional neural networks in the field of sign language\nrecognition.\nIn the part of sign language recognition algorithm research, this paper firstly conducts video preprocessing, video feature\nextraction on the Chinese sign language continuous utterance SLR dataset constructed by the University of Science and\nTechnology of China, and then adds the TSM module with better processing capability of temporally strongly correlated video\nto ResNet-50 and MobileNet models for training sign language video and analyzing experimental data; meanwhile, conducts\nAction-net, another type of behavior recognition model, is experimented to recognize sign language videos and analyze the\nexperimental data; through experimental comparison, the performance and effect of the models are verified, and a sign language\nrecognition algorithm suitable for CSL continuous utterance dataset is selected to realize the recognition and translation of\nsign language continuous utterances.\nIn the part of sign language generation, the research aims to transform the spoken Chinese text into the sequence of sign\nlanguage language through jieba splitting and then processed by Bert model, corresponding to the corresponding sign language\nvocabulary video, to complete the translation from text to video, and to verify the performance and effect of the model.\nII. METHOD\nA. System Structure Design\nThis project is dedicated to designing a two-way sign language translation system, which is mainly divided into sign language\nrecognition part and sign language generation part, and the main flow chart of the two parts is as the following Figure1.\nFig. 1. Flow chart of sign language two-way translation system\nIn this system, since sign language recognition translation is a temporal strong correlation process, TSM in behavior\nrecognition model with Action-net is used for correlation model training.\nIn this system, the sign language generation part relies heavily on the natural corpus segmentation and processing, so the\nBert pre-training model, which is more efficient and accurate, is chosen as the basis for development.\nB. Sign language dataset\nThe correspondence rules between sign language grammar and natural language grammar are important indicators for the\nselection of sign language datasets. According to the study of sign language linguistics, usually the sign languages of some\ncountries can be divided into natural sign language and statute sign language (or gestural sign language). Taking Chinese sign\nlanguage as an example, Chinese sign language can be divided into natural sign language and gestural Chinese. Natural sign\nlanguage is mainly used by the hearing impaired and has a set of systematic grammatical rules, while gestural Chinese is an\nartificial language that operates directly on the basis of spoken grammar with gestures and has a one-to-one correspondence\nwith Chinese characters, and is therefore also called written sign language.\nHow to map natural sign language and statute sign language is one of the challenges of sign language translation research.\nMost of the existing research on sign language translation is based on continuous sign language recognition, combined with\nlanguage models to obtain natural language translations that conform to spoken descriptions. In the future, we can consider\nconstructing large text pair datasets, i.e., the natural sign language annotation set and the corresponding statute sign language\nannotation set, and pre-training the language model on the text pair dataset first, and then migrating it to the language model\nof sign language translation.\nIn the continuous sign language utterance dataset, a portion of the dataset has annotations against the text in normal spoken\nlanguage order, which can be used for sign language generation translation functions, mainly including Boston-104, RWTH-\nPHOENIX-WEATHER-2014-T, KETI, GSL, MEDIAPI-SKEL corpus, and CSL-Daily datasets. The CSL-Daily dataset can be\nused for continuous sign language recognition and translation tasks, and it provides spoken language translation and lexical\nlevel annotation. Compared with USTC-CCSL, CSL-Daily focuses more on daily life scenarios, including multiple topics such\nas family life, healthcare and school life. the training, validation and test sets of CSL-Daily contain 18,401, 1,077 and 1,176\nvideo samples, respectively. Figure2 shows the main struct of the dataset and the Table III shows the difference between these\ndatasets.\nFig. 2.\nRelevant composition of the sign language dataset\nTABLE III\nOVERVIEW OF CHINESE SIGN LANGUAGE DATASET\nData set name\nYear\nChina\nType\nCategory\nSample Examples\nRecorders Numbers\nData Characteristics\nDEVIDIGN-G\n2014\nChina\nIsolated words\n36\n432\n8\nRGB\nDEVIDIGN-D\n500\n6000\nDEVIDIGN-L\n2000\n24000\nUSTC-CCSL\n2015\nChina\nIsolated words\n500\n125000\n50\nRGB, depth, skeleton\nContinuous\n100\n25000\nRGB\nStatements\n100\n25000\nRGB\nC. TSM Model\nTime Shift Module (TSM), which provides a new approach for effective temporal modeling in video understanding, is an\nenhanced derivative model for temporal information learning based on the TSN model. In the TSN model, temporal information\nis fused by taking N images from the video relatively equally and randomly, and then averaging their classification results to\nachieve a certain degree of temporal information modeling. The TSM model absorbs the advantages of the TSN model, and at\nthe same time, after the selection of each image, uses the shift of the time dimension to enable a single image to contain the\ninformation features of multiple neighboring images, thus greatly improving the efficiency of temporal information recognition,\nand at the same time, because only part of the image information is shifted for information aggregation, feature fusion between\ndifferent frames can theoretically be achieved on the basis of zero additional computational overhead Joint modeling, which\ncan be computed freely on the basis of two-dimensional convolution, but has a strong time-domain modeling capability. Figure\n3 explains the main principles of the TSM model for frame shifting.\nFig. 3. TSM model core principle - time series displacement\nTSM performs efficient temporal modeling by shifting feature mapping along the temporal dimension.TSM efficiently\nsupports offline and online video recognition. Two-way TSM mixes past and future frames with current frames for high-\nthroughput offline video recognition. One-way TSM blends only past frames with current frames and is suitable for low-latency\nonline video recognition.\nAs shown in the figure, one-way TSM online video recognition requires features from future frames to replace the features\nin the current frame. One-way TSM online recognition can be achieved by simply transferring features from the previous frame\nto the current frame. The one-way TSM inference diagram for online video recognition is shown in Figure 4.\nFig. 4. Time-shift operation performed by TSM on the actual video\nDuring the inference process, for each frame, we save the first 1/8 feature mapping of each residual block and cache it in\nmemory. For the next frame, we replace the first 1/8 of the current feature mapping with the cached feature mapping. We use\na combination of 7/8 of the current feature mapping and 1/8 of the old feature mapping to generate the next layer, and repeat.\nUsing one-way TSM for online video recognition has several unique advantages: low latency inference. TSM is an enhanced\nderivative model for temporal information learning based on the TSN model. In the TSN model, the temporal information is\nfused by taking N images from the video relatively equally and randomly, and then averaging their classification results to\nachieve a certain degree of temporal information modeling.\nThe TSM model absorbs the advantages of the TSN model, and at the same time, after the selection for each image, uses the\nshift of the temporal dimension to enable a single image to contain the information features of multiple neighboring images,\nthus greatly improving the efficiency of temporal information recognition, and at the same time, because only part of the image\ninformation is shifted for information aggregation, feature fusion between different frames can be achieved in theory on the\nbasis of zero additional computational overhead Joint modeling.\nThe TSM model part of this project was trained on a cloud computing platform using 5 vCPU Intel(R) Xeon(R) Silver\n4210R CPU @ 2.40GHz with RTX 3090 (24GB) * 1. The training was conducted in about 30 hours for SLR-100 sentence\ndataset and 46 hours for SLR-500 word dataset.\nD.\nAction-net model\nTraditional 2D CNNs are computationally inexpensive but cannot capture temporal relationships. In contrast, 3D CNN\ncan capture the temporal relationships but is computationally expensive. In the design of Action-Net, it consists of three\nattention mechanism modules, including the Spatial-Temporal Excitation module, the Channel Excitation module and the\nMotion Excitation module, so that it can be more effective for the temporally strongly correlated The video framing is better\nthan the video framing. The format of the image is [N,T,C,H,W], where N denotes batch size, T denotes number of segments,\nC denotes number of channels, H denotes height height, W denotes width, and r is the channel loss rate channel reduce radio.\nFigure 5 illustrates the main structure of the Acrion-net module.\nFig. 5. The main structure of the Acrion-net module\nACTION module: The ACTION module is made up of the three attention modules mentioned above in parallel. This module\nis plug-and-play like the previous work TSM. The base model uses the same ResNet-50 as in the previous work. The sign\nlanguage recognition and translation process designed in this project belongs to the temporal strong related project, meanwhile,\nsince TSM and Action-net perform well in behavior recognition and there are more related materials, this project uses TSM\nand Action-net models for sign language translation related training Traditional 2D CNNs are computationally inexpensive\nbut cannot capture temporal relationships. While 3D CNN can capture the temporal relationship, but it is computationally\nmore expensive. In the design of Actionnet, the Spatial-Temporal Excitation module, the Channel Excitation module, and the\nMotion Excitation module are proposed. Finally, they are added together to implement the Action module. Thus, it has a good\neffect on the video frame splitting with strong timing correlation. The Action model natively supports jester, sth-sth v1, v2\nand many other mainstream datasets. Considering the file structure and other issues, we finally chose to process the existing\nSLR dataset into jester standard format. The processing starts with frame-slicing the video from the existing dataset using tools\nsuch as ffmpeg. After the video is frame-separated, a python-based script is used to count the number of frames, RGB and\noptical flow information of the frame-separated file. Since the Action native code uses pkl files to save the tag information,\nthe Pickle library is called to save the corresponding path information and tag information to the corresponding tag pkl files\nafter distinguishing the training set and the test set.\nIn this project, the TSM model was partially trained on a cloud computing platform using 15 vCPU AMD EPYC 7543\n32-Core Processor with RTX A5000 (24GB) * 1, where the SLR-100 sentence dataset was trained for about 20 hours.\nE. Bert model (bert-base-Chinese)\nBert is an unsupervised pre-trained language model for natural language processing tasks. The goal of the Bert model is\nto use a large-scale unlabeled corpus to train and obtain a textual semantic information-rich The goal of the Bert model is\nto train a large-scale unlabeled corpus, obtain a semantic representation of the text containing rich semantic information, and\nthen fine-tune the semantic representation of the text for a specific NLP task and finally apply it to that NLP task.\nWork on sign language generation relies heavily on the analysis and segmentation of natural language, and Bert has a good\nefficiency and performance in processing natural corpus. Therefore, in this project, a pre-trained bert-base-Chinese model is\nused as the basis, and further training and tuning is performed on top of it to realize the process of segmenting and adapting\nnatural language into sign language sequences.\nIn the sign language video-to-annotation-to-text based sign language translation paradigm, the sign language translation\nprocess is divided into two phases: the first phase treats sign language recognition as an intermediate tokenization component\nthat extracts sign language annotations from the video; the second phase is a language translation task that maps the sign\nlanguage annotations to spoken text.\nIn the processing of the CSL-Daily dataset, its video-map is first extracted to select the corresponding information of the\nuseful natural and sign language sequences. Then it is converted into a format suitable for training the bert model. In training,\nthe model first reads the original sentences of each data item, and then compares them with the validated sentences after the\nword separation process.\nThe experiments in this chapter are built on the datasets corresponding to large sign language texts, i.e., the natural sign\nlanguage annotation set and the corresponding statute sign language annotation set, and the language model is first pre-trained\non the text pair dataset and then migrated to the language model for sign language translation. The input spoken text is taken\nthrough the Bert-Base-Chinese model, and the corresponding sign language video is automatically output by the system, as\nshown in Figure 6:\nFig. 6. Basic principles of sign language generation\nThis experimental model is partially trained on a cloud computing platform using a 5 vCPU Intel(R) Xeon(R) Silver 4210R\nCPU @ 2.40GHz with RTX 3090 (24GB) * 1 for training.\nThe Bert model can already be used for natural language word separation, and is trained on top of the existing Bert-base-\nChinese pre-training model by importing CSL-Daily related data sets. The purpose of this training is to output the words\ngenerated by the model in sign language sequence for subsequent generation of sign language videos, and to complete the\nspoken text-syntax-sign language annotation. Figure 7 illustrates the main steps of sign language segmentation using the Bert\nmodel.\nFig. 7.\nFlow chart of sign language generation\nIII. RESULTS\nA. TSM experimental results\nIn the training for 100 consecutive sentences, the following results were obtained after 117 iterations based on resnet50.\nFigure 8 shows the training results of TSM based on resnet50 for 100 sentences.\nTesting Results: Prec@1 99.300 Prec@5 99.960 Loss 0.02676\nFig. 8. Training results of TSM based on resnet50 for 100 sentences\nIn the training of 500 isolated sign language words, the following results were obtained after 30 iterations based on resnet50.\nFigure 9 shows the training results of TSM based on resnet50 for 500 isolated words\nTesting Results: Prec@1 96.840 Prec@5 96.840 Loss 0.11426\nFig. 9. Training results of TSM based on resnet50 for 500 isolated words\nIn the training for 100 consecutive sentences, the following results were obtained after 117 iterations based on mobilenet.\nFigure 9 shows the training results of mobilenet-based TSM for 500 isolated words.\nTesting Results: Prec@1 95.240 Prec@5 95.240 Loss 0.17194\nFig. 10. Training results of mobilenet-based TSM for 500 isolated words\nB. Action-net experiment results\nIn the training for 100 sentences, the following results were obtained after 48 iterations.\nTesting Results: Prec@1 34.580 Prec@5 62.050 Loss 2.6734\nC. Comparison of TSM and Action-net experiments\nAfter 48 iterations of training in TSM and Actionnet respectively, the results are as the follow Table IV.\nTABLE IV\nCOMPARISON OF TSM AND ACTION-NET TRAINING RESULTS\nPrec@1\nPrec@5\nLoss\nTSM-resnet50\n97.720\n99.740\n0.07499\nActionnet\n34.580\n62.050\n2.6734\nConsidering the model accuracy and model convergence speed, TSM is used as the implementation model in this project.\nAlso in the TSM training based on different underlying models, after 100 iterations each, the results are as the follow Table\nV:\nTABLE V\nCOMPARISON OF TSM TRAINING RESULTS BASED ON RESNET50 AND MOBILENET\nPrec@1\nPrec@5\nLoss\nTSM-resnet50\n97.720\n99.740\n0.07499\nTSM-mobilenet\n95.240\n95.240\n0.17194\nConsidering the model accuracy and model convergence speed, and due to the poor performance of mobilenet in the post-test,\nthe TSM model with resnet50 as the underlying layer was finally adopted for the target implementation in this project.\nD. Sign Language Interpretation Test Results\nThe test for the sign language translation module of this project is mainly divided into two parts: continuous sentence\nrecognition and isolated word recognition, and the main test contents are as follows.\nThe video of continuous sign language sentences was obtained from the friendly assistance of students from the sign language\nclub of HIT, and the video of isolated word sign language was obtained from the public sign language information website\nand related data sets.\nTested on the trained model, both recognition results are better, and the isolated word sign language recognition model will\nbe mainly used in the follow-up.\nE. Bert experimental results\nAfter iterating a total of about 23k steps over a total of 20654 statements in the CSL-Daily dataset, the following results\nare obtained. Figure 11 shows the Bert model training results graph.\nFig. 11.\nBert model training results graph\nIV. DISSCUSSION\nFinally, we realize a two-way interactive system for sign language recognition and sign language generation. In sign language\nrecognition, based on the CSL Chinese sign language dataset, the relatively stable and high recognition rate TSM-Resnet50\nmodel is selected, and the continuous sign language sentences are cut and recognized in the sign language video in the test,\nand the natural order Chinese is generated by adjusting the order.\nIn the sign language generation, based on the Chinese corpus, word cutting and division of natural utterances are realized by\nBert model and adjusted to sign language order, and corresponding videos in the Chinese sign language dataset and generating\nsign language videos, which realize the two-way recognition and generation of sign language and natural language.\nAfter the test of dividing and recognizing 20 videos of consecutive sign language utterances with less than 10 words, and\nthe test of generating 20 videos of Chinese utterances with less than 20 words in sign language, the accuracy and stability of\nthe two-way process of the system reach a high level.\nAfter future development and improvement, thanks to the convenience and light weight of computer vision, the system will be\nused in hardware terminals or mobile devices to realize two-way communication between normal groups and hearing-impaired\npeople, and for sign language education to improve the quality of life and happiness of hearing-impaired people, help them\nbetter integrate into the general society, and also promote the better development of special education and welfare.\nWe hope to further improve the accuracy and fault tolerance of recognition by improving the frame cut recognition model\nand expanding the Chinese sign language corpus in the future, so that it can recognize continuous complex utterances in\ncomplex light environments and be put into the welfare business of integrating hearing-impaired people into society as soon\nas possible.\nREFERENCES\n[1] A. Vaswani et al., ”Attention Is All You Need,” arXiv, 2017.\n[2] J. Lin et al., ”TSM: Temporal Shift Module for Efficient Video Understanding,” 2018.\n[3] Z. Wang et al., ”ACTION-Net: Multipath Excitation for Action Recognition,” 2021.\n[4] C. Wei et al., ”Semantic Boundary Detection With Reinforcement Learning for Continuous Sign Language Recognition,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. PP, no. 99, pp. 1-1, 2020.\n[5] H. Zhou et al., ”Improving Sign Language Translation with Monolingual Data by Sign Back-Translation,” 2021.\n[6] S. Egea et al., ”Syntax-aware Transformers for Neural Machine Translation: the Case of Text to Sign Gloss Translation,” 2021.\n[7] D. Guo et al., ”A review of sign language recognition, translation and generation,” Computer Science, vol. 048, no. 003, pp. 60-70, 2021.\n[8] D. Guo et al., ”Connectionist Temporal Modeling of Video and Language: a Joint Model for Translation and Sign Labeling,” in Twenty-Eighth International\nJoint Conference on Artificial Intelligence IJCAI-19, 2019.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-06-03",
  "updated": "2023-06-17"
}