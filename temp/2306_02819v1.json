{
  "id": "http://arxiv.org/abs/2306.02819v1",
  "title": "Enhancing Language Representation with Constructional Information for Natural Language Understanding",
  "authors": [
    "Lvxiaowei Xu",
    "Jianwang Wu",
    "Jiawei Peng",
    "Zhilin Gong",
    "Ming Cai",
    "Tianxiang Wang"
  ],
  "abstract": "Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.",
  "text": "Enhancing Language Representation with Constructional Information for\nNatural Language Understanding\nLvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming Cai∗, Tianxiang Wang\nDepartment of Computer Science and Technology, Zhejiang University\n{xlxw, wujw, pengjw, zhilingong, cm, wang_tx}@zju.edu.cn\nAbstract\nNatural language understanding (NLU) is an\nessential branch of natural language process-\ning, which relies on representations generated\nby pre-trained language models (PLMs). How-\never, PLMs primarily focus on acquiring lexico-\nsemantic information, while they may be un-\nable to adequately handle the meaning of con-\nstructions. To address this issue, we introduce\nconstruction grammar (CxG), which highlights\nthe pairings of form and meaning, to enrich lan-\nguage representation. We adopt usage-based\nconstruction grammar as the basis of our work,\nwhich is highly compatible with statistical mod-\nels such as PLMs. Then a HyCxG framework\nis proposed to enhance language representa-\ntion through a three-stage solution. First, all\nconstructions are extracted from sentences via\na slot-constraints approach. As constructions\ncan overlap with each other, bringing redun-\ndancy and imbalance, we formulate the condi-\ntional max coverage problem for selecting the\ndiscriminative constructions. Finally, we pro-\npose a relational hypergraph attention network\nto acquire representation from constructional\ninformation by capturing high-order word in-\nteractions among constructions. Extensive ex-\nperiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.\n1\nIntroduction\nRecent progress in natural language processing re-\nlies on pre-trained language models (PLMs) (De-\nvlin et al., 2019; Liu et al., 2019; Raffel et al., 2020;\nBrown et al., 2020; Raffel et al., 2020; He et al.,\n2021), which generate word representations by cap-\nturing syntactic and semantic features from their\ncontext. Typically, PLMs (e.g., BERT) employ the\nMasked Language Model (MLM) as a pre-training\nobjective, randomly masking some tokens from the\ninput before predicting the original tokens. How-\never, PLMs primarily focus on acquiring lexico-\nsemantic information (Tayyar Madabushi et al.,\n∗Corresponding author.\nSentence\nConstruction\nThe\n[staff]N should be a bit\nmore friendly.\nNOUN–AUX–be\n(staff–should–be)\nThe restaurants try too hard to\nmake fancy [food]N .\nADV–hard–to\n(too–hard–to)\nI can understand the prices if it\nserved better [food]N .\nif–PRON–VERB\n(if–it–served)\nTable 1: Visualization of attention scores from misclas-\nsified examples in aspect-based sentiment analysis. [·]N\nrefers to the aspects with negative sentiment polarity.\n2020) while the meaning is not attached to the\nwords instantiating the construction, but rather to\nthe abstract pattern itself (Weissweiler et al., 2023).\nLearning the representation of more complex and\nabstract linguistic units (called constructions) can\nbe more challenging, such as collocations, argu-\nment structures and sentence patterns. For exam-\nple, the ditransitive construction (Goldberg, 1995),\nwhich involves the form “Subject–Verb–Object1–\nObject2”, denotes the meaning of transfer or giving\nof an entity (Object2) from an agent (Subject) to\na recipient (Object1), such as “John gave Mary a\nbook”. Linguistic experiments have proved that\nconstructions can substantially contribute to sen-\ntence comprehension in multiple languages, such\nas English (Bencini and Goldberg, 2000) and Ger-\nman (Gries and Wulff, 2005).\nAs the meaning of a construction is assigned to\na language pattern rather than specific words (Hoff-\nmann and Trousdale, 2013), the constructional in-\nformation is rarely acquired by MLM and requires\nlarge bulk training data, which may lead to fail-\nure in natural language understanding tasks with\nconstrained low resource data. As shown in Ta-\nble 1, we illustrate three examples from aspect-\nbased sentiment analysis (ABSA) tasks. The at-\ntention scores are derived from BERT-SPC (De-\nvlin et al., 2019), a simple baseline by fine-tuning\narXiv:2306.02819v1  [cs.CL]  5 Jun 2023\naspect-specific representation of BERT. In the first\nexample, the model focuses mainly on the opin-\nion word “friendly” while ignoring the modality\nconstruction of “should be”, resulting in the mis-\nclassification. In the second case, “too hard to”\nexpresses a negation for the “fancy food”. How-\never, the model ignores the collocation of “too...to”\nstructure, and the “food” is wrongly considered to\nbe positive. The third example illustrates that lack\nof understanding of conditional sentence (if clause)\ncauses the model to make an incorrect prediction.\nObservations from both linguistics and NLU\nstandpoints motivate us to exploit construction\ngrammar (CxG; Goldberg, 1995, 2006) as the in-\nductive bias to complement language representa-\ntion with constructional information. Construction\ngrammar refers to a family of linguistic approaches\nwhich regard constructions as the basic unit of lan-\nguage. Composed of pairings of form and meaning,\nconstructions pervade all levels of language along a\ncontinuum (e.g., morphology, lexicon and syntax).\nHowever, when bridging the gap between CxG\nand NLU, we face three critical questions:\n(Q1) Which CxG approach is applicable for NLU?\n(Q2) How can typical constructions be identified?\n(Q3) How can constructions be encoded?\nTo answer (Q1), we investigate different vari-\nants of CxG. Instead of formal methods (e.g., fluid\nconstruction grammar and embodied construction\ngrammar), we adopt usage-based approaches which\nassume that grammar acquisition involves statisti-\ncal inference. Therefore constructions can be in-\nduced as frequent instances of linguistic units. This\nassumption makes usage-based approaches highly\ncompatible with statistical models such as PLMs\n(Kapatsinski, 2014). Specifically, we follow the\nefforts of Dunn (2019) with a computationally slot-\nconstraints approach that formulates constructions\nas the combinations of immutable components and\nfillable slots. As shown in Table 1, the modality\nconstruction “NOUN–AUX–be” in the first exam-\nple contains an immutable component “be” and two\nfillable slots (NOUN and AUX) with a meaning of\nexpressing advice or suggestion.\nAs for (Q2), from the usage-based perspective,\nconstructions are often stored redundantly at dif-\nferent levels of language. Therefore constructions\ncan overlap with each other, which results in re-\ndundancy and imbalance. Consequently, we for-\nmulate the problem of selecting discriminative con-\nstructions as multi-objective optimization for con-\nditional maximum coverage. To alleviate the com-\nputational complexity, we adopt a heuristic search\nalgorithm, i.e., simulated annealing (SA) to deter-\nmine the composition of constructions.\nIn order to address (Q3), we propose a relational\nhypergraph attention network (R-HGAT) to cap-\nture high-order word interactions inside the con-\nstructions, so as to acquire representation from\nconstructional information. R-HGAT generalizes\nhypergraph attention network using its flexible la-\nbeled hyperedges. We refer to the entire framework\nwhich involves construction extraction, selection\nand encoding as hypergraph network of construc-\ntion grammar (HyCxG)1.\nExtensive experiments have been conducted to\nillustrate the superiority of HyCxG on NLU tasks,\nwhile multilingual experiments further indicate the\nconstructional information is beneficial across lan-\nguages. Additionally, based on the constructional\nrepresentations acquired by our model, we conduct\nan empirical study of building a network of con-\nstructions, which provides meaningful implication\nand in turn contributes to usage-based CxG.\n2\nConstruction Extraction and Selection\nIn this section, we elaborate on the details of ex-\ntraction (Q1) and selection of constructions (Q2).\n2.1\nComputational Construction Grammar\nConstruction grammar is a branch of cognitive lin-\nguistics which assumes that syntax and patterns\nhave specific meanings. A construction is a form-\nmeaning pair whose structure varies in different\nlevel of abstractness, including partially or fully\nfilled components and general linguistic patterns.\nAccording to CxG, speakers can recognize pat-\nterns after coming across them a certain number of\ntimes, comparable to merging n-grams (i.e., con-\nstructions) at different schematic or abstract levels\n(Goldberg, 2003, 2006; Tayyar Madabushi et al.,\n2020). However, the methodology for acquiring\nthe constructions is always labor-intensive and re-\nquires careful definition, while the computational\ngeneration of CxG is a relatively new research field.\nAs our answer to (Q1), the usage-based approach\nis employed to obtain constructions following the\nwork of Dunn (2019). They propose a grammar\ninduction algorithm, which can learn the patterns of\nconstructions with a slot-constraints approach from\n1Our code is publicly avaliable at https://github.com/\nxlxwalex/HyCxG.\nthe corpus. Therefore, we adapt Dunn’s (2019)\nsystem in our work for construction extraction.\nConstructions are represented as a combination\nof slots and are separated by dashes (Table 1). The\nslot fillers are drawn from lexical units (words),\nsemantic categories (the discrete semantic domains\nare clustered with fastText embeddings (Grave\net al., 2018) via K-Means) and syntax (Universal\nPart-of-Speech tags; Petrov et al., 2012), which are\nprogressively higher in abstraction level.\n2.2\nConditional Max Coverage Problem\nSince constructions are often stored redundantly\nat different levels of abstractness, overlapping con-\nstructions can be captured by the grammar induc-\ntion algorithm (Dunn, 2017, 2019). We summarize\nthe phenomenon of overlap into two scenarios: In-\nclusion and Intersection. Inclusion refers to the\ncase where one construction is a subpart or instan-\ntiation of another construction, while Intersection\nindicates that the constructions have some common\nslots. Redundancy and imbalance encoding prob-\nlem can be brought by overlap phenomenon. The\nredundancy of the constructions can cause high\ncomputational complexity. And imbalanced con-\nstruction distribution may introduce interference.\nThis is due to the fact that the words in high dense\nparts are updated frequently, while this seldom hap-\npens in low dense parts. There is a virtual example\nto illustrate these phenomena in Figure 1.\nTherefore, the key issue is to select the dis-\ncriminative constructions from all candidates (Q2).\nWe formulate this as multi-objective optimiza-\ntion for conditional maximum coverage (Cond-\nMC) problem.\nSpecifically, we seek an opti-\nmal subset CO = {c1, c2, · · · , cm} from the set\nC = {c1, c2, · · · , cn} containing all n construc-\ntions. The constructions in CO are supposed to\nreach the following three objectives:\n1. The constructions cover as many words as\npossible in sentences.\n2. Intersection among constructions is minimal.\n3. Constructions preferably contain more slots\nfrom concrete levels.\nObjectives 1 and 2 spread the constructions\nthroughout the entire sentence to ensure balanced\ndistribution, while the more discriminative con-\nstructions can be selected by objective 3. To unify\nthe objective functions for optimization, we for-\nmalize each objective as a specific score func-\ntion and weight them together to generate the to-\ntal score. Therefore, Cond-MC can be converted\nAlgorithm 1: SA algorithm for Cond-MC\nInput: The set of all constructions C.\nOutput: The optimal set CO.\n1 Initialize the feasible solutions CO ←Cf\n2 Initialize temperature T ←T0, step k ←0\n3 while k <kmax do\n4\nT ←COOL(T, k, kmax)\n5\nCk ←RANDFLIP(CO, T)\n6\ndE ←SCORE(Ck) −SCORE(CO)\n7\nif dE > 0 or RAND() ≤P(dE, T) then\n8\nCO ←Ck\n9\nend\n10\nk ←k + 1\n11 end\n12 return CO\nto the problem of maximizing the score for CO.\nWe define each construction ci as a set of r slots\nci = {w1, w2, · · · , wr}, while D(·) is utilized to\ncompute the number of elements in a set. Then the\nscore function of objectives 1 and 2 can be formu-\nlated as D(c1 ∪· · · ∪cm) and D(c1 ∩· · · ∩cm),\nwith Sob1 and Sob2 referring to them respectively.\nAnd the score function of objective 3 is written as:\nSob3 =\nm\nX\ni=1\nr\nX\nj=1\nsr\nr , sr ∈{ssyn, ssem, slex}\n(1)\nwhere three scores ssyn, ssem and slex are assigned\nfor slots in syntax, semantic and lexical level. The\nscore of each construction is calculated by averag-\ning the slot scores, while Sob3 computes the total\nscore of all constructions in CO. Then Cond-MC\ncan be formulated as:\n(\nmax\nw1Sob1 −w2Sob2 + w3Sob3\ns.t.\nCO ⊂C\n(2)\nwhere w1, w2 and w3 are the weighting factors for\nbalancing the scores of three objectives.\n2.3\nSolution for Cond-MC Problem\nAs we formulate a multi-objective optimization\nproblem for selecting typical constructions, it is\ncomplicated to solve Cond-MC since it is an NP\nproblem. When the solution space is large, i.e.,\nthere is a significant amount of constructions in\nthe sentence, which leads to an unacceptable com-\nputational complexity. To alleviate this issue, we\nemploy the Simulated Annealing (SA; Kirkpatrick\net al., 1983) algorithm in heuristic search of CO,\nwhere the procedure is stated in Algorithm 1.\nWe define CO as a binary set to indicate whether\na construction is selected. RANDFLIP(·) is applied\nto reverse t elements stochastically, while step k is\ninversely related to temperature T. SCORE(·) is\nthe overall score function in Equation 2. Besides,\nwe utilize RAND() to compare with the transition\nprobability P to estimate whether to accept the new\nsolution under metropolis criterion (i.e., accepting\na new solution with a certain probability to increase\nthe perturbation). A cooling schedule is used to\ncontrol the evolution of the new solution with a\nreduced temperature function COOL(·) to regulate\nthe temperature in an exponential form.\n3\nModel\nAfter extracting and determining the optimal set for\nconstructions, the next challenge is to encode con-\nstructions in an effective way (Q3). In this section,\nwe first introduce hypergraph for modelling com-\nplex data correlation within constructions. Then a\nrelational hypergraph attention network (R-HGAT)\nis proposed to encode the high-order word interac-\ntions inside each construction. Finally, enhanced\nby the constructional information, the language\nrepresentation is ready for NLU tasks.\n3.1\nConstruction Hypergraph Generation\nSince the associations within constructions are not\nonly dyadic, but could be triadic, tetradic or even\nhigher-order, we adopt hypergraph for modelling\nsuch data correlation. Hypergraph is a generaliza-\ntion of conventional graph that the hyperedge can\nconnect arbitrary number of nodes. A hypergraph\ncan be defined as G = {V, E}, which contains a\nnode set V = {v1, v2, · · · , vm} and a hyperedge\nset E = {e1, e2, · · · , en}. As each hyperedge can\nconnect two or more nodes, the hypergraph G can\nbe denoted by an incidence matrix H ∈R|V|×|E|,\nwith entries being defined as:\nHij =\n\u001a 1,\nif vi ∈ej\n0,\nif vi /∈ej\n(3)\nGiven a sentence with m words (i.e., nodes in G),\nwe can model high-order word interactions through\nn typical constructions. In specific, each construc-\ntion is regarded as a hyperedge, while the words\ncontained in the construction are considered as the\nmember nodes of certain hyperedges. Thus, we\ncan generate the hypergraph for each sentence as\nillustrated in Figure 1.\n3.2\nHypergraph Attention Network\nDifferent from the simple graph that nodes are\npairwise linked, the information of nodes cannot\nbe directly aggregated from neighboring nodes in\nhypergraph. Thus, hypergraph attention network\n(HGATT) is employed to learn the node representa-\ntions, which separates the computation procedure\ninto two steps, i.e., node aggregation and hyperedge\naggregation. It first aggregates the information of\nnodes for generating the hyperedge representation.\nThen the information is fused back to the nodes\nfrom hyperedges via hyperedge aggregation. In\ngeneral, the procedure is formulated as follows:\ngl\nj ←AGGRNode\n\u0010n\nhl−1\ns\n| ∀vs ∈ej\no\u0011\nhl\ni ←AGGREdge\n\u0010\nhl−1\ni\n,\nn\ngl\nj | ∀ej ∈Ei\no\u0011 (4)\nwhere AGGRNode and AGGREdge denote the two-\nstep aggregation functions for nodes and hyper-\nedges. hl\ni and gl\nj denote the representations of\nnode vi and hyperedge ej in l-th layer, while Ei is\nthe set of hyperedges connected to node vi.\nThe HGATT is mainly implemented based\non graph attention mechanism (Veliˇckovi´c et al.,\n2017), such as HyperGAT (Ding et al., 2020a).\nHowever, this attention mechanism applies the\nsame weight matrices for different types of informa-\ntion within hyperedges, which inhibits the ability\nfor the models to capture high-order relations in\nhypergraphs (Fan et al., 2021).\n3.3\nR-HGAT Network\nTo tackle the limitation of HGATT and exploit the\ninformation of constructions, we propose a rela-\ntional hypergraph attention network (R-HGAT).\nThe mutual attention mechanism, i.e., node-level\nattention and edge-level attention, is adopted to ag-\ngregate the information from nodes and hyperedges.\nThe entire architecture is described as follows:\nNode-level attention.\nNode-level attention is uti-\nlized to encode representation of hyperedges that\naggregates information from the nodes. Given the\nnode vi and the set of hyperedges Ei that connect\nto vi, we first embed each hyperedge (i.e., con-\nstruction) into vector space zj by looking up an\nembedding matrix Ec ∈R|V|×d, while |V| is the\nsize of construction set within a certain language\nand d refers to the dimension of the vectors. After\nthat, the attention mechanism is applied to compute\nthe importance score for spotlighting the nodes that\nConstruction Extraction\nHypergraph Generation\nRelational Hypergraph Attention Network\nDownstream Task\nConstruction Selection\nInput Sentence\n0\n10\n20\n1\n2\n3\n4\n5\n6\n7\n8\n9\n11 12 13 14 15 16 17 18 19\n21 22 23\n(0 - 2)\n(5 - 8)\n(10 - 13)\n(14 - 16)\n(21 - 23)\n(10 - 13)\n(21 - 23)\n(2 - 4)\n(5 - 9)\n(14 - 16)\n(5 - 9)\n(14 - 16)\n(19 - 21)\n(0 - 4)\n(8 - 10)\n(11 - 13)\n(17 - 20)\n(0 - 4)\n(17 - 20)\n19\nHyperedge\nEmbedding\nHGATT\nAdd \n&\nNorm\nFeed\nForward\nPooling\nTask-specific\nRepresentation\nSoftmax\nSentiment Analysis\n(e.g., ABSA, SST-2)\nLinguistic Acceptability\n(e.g., CoLA)\nNLI Tasks\n(e.g., QNLI, MNLI, RTE)\nParaphrase Tasks\n(e.g., QQP, MRPC)\nSimilarity Tasks\n(e.g., STS-B)\n× 𝑳\nEc\nhz\nhl−1\n0\nhl−1\n4\nhl−1\n3\nhl−1\n2\nhl−1\n1\nhl−1\n23\nhl−1\n22\nhl−1\n21\nPre-trained Language Model \n(e.g., BERT, RoBERTa)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n20\n21\n22\n23\nNode-level \nAttention\nNode-level \nAttention\n⓪\n①\n19\n②\n③\n④\n⑤\ng\n0l\n5\ng\n0l\n0\nEdge-level \nAttention\nEdge-level \nAttention\nhl\n23\nhl\n0\nHl\nH\n0l\nFigure 1: Overview of the proposed HyCxG framework. For illustration, we show the entire system with a virtual\nexample. In construction extraction module, CxG (5-8) is a subpart of CxG (5-9), which demonstrates the inclusion\nrelation of the overlap phenomenon, while CxG (5-9) and CxG (8-10) are intersection relations.\nare crucial to the hyperedge according to zj. Then\nthe aggregation procedure can be formulated as:\ngl\nj =\nX\nvs∈ej\nαjsWnhl−1\ns\n(5)\nwhere Wn is the weight matrix and αjs is the atten-\ntion score between the representation of node hl−1\ns\nand hyperedge zj that can be computed by:\nrl−1\njs\n= σ((Wczj)TWshl−1\ns\n)\nαjs =\nexp(rl−1\njs )\nP\nvs∈ej exp(rl−1\njs )\n(6)\nwhere Wc and Ws are trainable matrices, while σ(·)\nis the non-linear activation function (e.g., ReLU).\nIn particular, the information of constructions is\ninjected to hyperedges with trainable matrix Wg\nas:\ng\n′l\nj = gl\nj + Wgzj\n(7)\nEdge-level attention.\nAs an inverse procedure,\nwe fuse the information of the hyperedges back to\neach node via the edge-level attention. Formally:\nhl\ni =\nX\nej∈Ei\nβijWeg\n′l\nj\ntl−1\nij\n= σ((Wohl−1\ni\n)TWrg\n′l\nj )\nβij =\nexp(tl−1\nij )\nP\nej∈Ei exp(tl−1\nij )\n(8)\nwhere We, Wo and Wr are trainable matrices. βij\nis the attention score of hyperedge ej on node vi.\nAfter the enhanced representation Hl is acquired\nwith mutual attention mechanism in R-HGAT net-\nwork, we apply the feed-forward network (FFN)\nconsisting of two fully-connected layers coupled\nwith residual connections to generate final node\nrepresentation H\n′l, it can be formulated as:\nH\n′l = LN(Hl−1 +σ(W2(W1Hl +b1)+b2)) (9)\nwhere W1 and W2 refer to trainable weight matri-\nces, while b1 and b2 are the biases. Besides, LN(·)\ndenotes the layer normalization operation.\n3.4\nModel Training\nTo apply representations to downstream tasks, we\nutilize average-pooling to obtain the task-specific\nrepresentation hz, which retains most of the in-\nformation on node representations. For classifica-\ntion tasks (e.g., sentiment analysis and paraphrase\ntasks), hz is passed to a fully connected softmax\nlayer, where the objective function to be minimized\nis the cross-entropy loss. As for regression task\n(i.e., similarity task), mean squared error loss is\nadopted as the objective function for optimization.\n4\nExperiments\n4.1\nExperiments Setup\nDatasets.\nExperiments are conducted on a vari-\nety of NLU tasks for evaluation. We adopt ABSA\ndatasets from SemEval and MAMS (Pontiki et al.,\n2014, 2015, 2016; Jiang et al., 2019) as well as\nGLUE (Wang et al., 2018) benchmark to demon-\nstrate the effectiveness of our HyCxG. GLUE con-\ntains a broad range of datasets, including natural\nlanguage inference (MNLI, QNLI, RTE), sentence-\nlevel sentiment analysis (SST-2), paraphrase and\nsimilarity (MRPC, QQP, STS-B), and linguistic ac-\nModel\nRest14\nLap14\nRest15\nRest16\nMAMS\nAcc\nF1\nAcc\nF1\nAcc\nF1\nAcc\nF1\nAcc\nF1\nBERT-SPC\n85.09\n78.43\n79.47\n75.90\n83.21\n61.43\n90.74\n74.54\n82.34\n81.94\nLCFS-ASC\n86.71\n80.31\n80.52\n77.13\n82.47\n66.39\n89.77\n75.23\n82.78\n82.25\nR-GAT\n86.60\n81.35\n78.21\n74.07\n84.32\n68.47\n91.56\n75.85\n84.13\n83.78\nKumaGCN\n86.43\n80.30\n81.98\n78.81\n86.35\n70.76\n92.53\n79.24\n84.37\n83.83\nDGEDT\n86.30\n80.00\n79.80\n75.60\n84.00\n71.00\n91.90\n79.00\n84.21\n83.65\ndotGCN\n86.16\n80.49\n81.03\n78.10\n85.24\n72.74\n93.18\n82.32\n84.95\n84.44\nDualGCN\n87.13\n81.16\n81.80\n78.10\n84.50\n71.65\n91.72\n79.46\n84.51\n84.18\nHyCxG\n87.32\n82.24\n82.29\n79.11\n86.16\n74.63\n93.83\n82.27\n85.03\n84.40\nTable 2: Experimental results on ABSA datasets with BERT encoder. The best result on each dataset is in bold.\nceptability (CoLA). The detailed statistics of these\ndatasets are provided in Appendix B.\nImplementation.\nTo solve Cond-MC problem,\nwe set the weight scores to 1.0, 1.2 and 1.5 for ssyn,\nssem and slex (Equation 1), while w1, w2 and w3 are\nset to 1.0, 0.4 and 0.3 (Equation 2). Meanwhile, the\nCOOL(·) function is formulated as T0 · ak, while\na is the factor calculated as −ln(T0/Tf).\nHyCxG is optimized with AdamW (Loshchilov\nand Hutter, 2018) optimizer. The optimal hyper-\nparameters are selected when the model achieves\nthe highest performances on the development set\nvia grid search. More detailed setups are described\nin Appendix E, and the pre-trained model weights\nare obtained from Hugging Face (Wolf et al., 2020).\nBaselines.\nWe compare HyCxG with BERT-\nbased state-of-the-art baselines on ABSA datasets:\n(1) BERT-SPC denotes the fine-tuning BERT with\naspect-specific representation.\n(2) LCFS-ASC\n(Phan and Ogunbona, 2020) employs the syntactic\ndistance to alleviate the interference from unrelated\nwords. (3) R-GAT (Wang et al., 2020) utilizes a\nrelational graph attention network to encode the\npruned dependency tree. (4) KumaGCN (Chen\net al., 2020) synthesizes the dependency tree and la-\ntent graphs to enhance representation. (5) DGEDT\n(Tang et al., 2020) provides a dependency enhanced\ndual-transformer for classification. (6) DualGCN\n(Li et al., 2021) links the syntactic structure and se-\nmantic relevance to generate features. (7) dotGCN\n(Chen et al., 2022) builds the induced tree with\nsyntactic distances to encode opinion information.\nFor other NLU tasks from the GLUE benchmark,\nwe compare HyCxG with the base and large ver-\nsions of the RoBERTa (Liu et al., 2019) model to\ndemonstrate the performance improvement.\n4.2\nExperimental Results\nResults on ABSA tasks.\nExperiments on ABSA\ntasks are conducted firstly, which are intuitive to\nanalyze the function of constructions with aspects.\nMeanwhile, the results can be extended to multilin-\ngual settings as well as accessing to case studies.\nThe model performances are shown in Table 2,\nfrom which several observations can be obtained.\nFirst, compared to BERT-SPC model, most\nof syntax-based baselines achieve higher perfor-\nmances on five datasets, since they can alleviate\ninterference introduced and build the relationship\nbetween aspects and their corresponding opinion\nwords via synthesizing syntactic knowledge (e.g.,\ndependency tree). Second, the baseline models\n(i.e., DualGCN and dotGCN) with more informa-\ntion, such as semantic and opinion representations,\ngain better performances than others.\nThird, our HyCxG significantly outperforms all\nbaselines with constructional information incorpo-\nrated, which demonstrates the effectiveness of Hy-\nCxG. Furthermore, the perspective that construc-\ntion grammar can contribute to sentence compre-\nhension for NLU tasks can also be substantiated.\nResults on GLUE tasks.\nAs shown in Table 3,\nwe conduct performance comparison among Hy-\nCxG with the base (B) and large (L) version of\nRoBERTa on GLUE development sets. And the re-\nsults on test set are reported via online leaderboard.\nThe average results over tasks illustrate that our\nHyCxGB outperforms RoBERTaB, and HyCxGL is\nalso better than RoBERTaL on both development\nand test sets. It indicates the validity of HyCxG\nand the benefits of constructional information.\nAs a branch of grammar, HyCxG has huge im-\nprovement on CoLA task, which shows its poten-\nModel\nCoLA\nSST\nMNLI\nQNLI\nRTE\nQQP\nMRPC\nSTS\nAvg.\nMcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nPear\n• Results on GLUE development set\nRoBERTaB\n63.6\n94.8\n87.6\n87.5\n92.8\n78.7\n91.9\n90.2\n91.2\n86.5\nRoBERTaL\n68.0\n96.4\n90.2\n90.2\n94.7\n86.6\n92.2\n90.9\n92.4\n89.1\nHyCxGB\n64.9\n95.4\n87.8\n87.7\n93.1\n80.9\n91.9\n90.9\n91.7\n87.1\nHyCxGL\n69.6\n97.1\n90.8\n90.4\n95.0\n89.5\n92.3\n91.9\n92.6\n89.9\n• Results on GLUE test set (from online leaderboard as of Dec., 2022)\nRoBERTaB\n60.3\n95.7\n87.5\n87.2\n93.0\n79.7\n89.5\n88.1\n89.5\n85.6\nRoBERTaL\n63.0\n96.3\n89.9\n89.4\n94.5\n85.2\n89.6\n88.5\n91.4\n87.5\nHyCxGB\n61.6\n96.0\n87.7\n87.4\n93.2\n81.2\n89.5\n88.6\n90.7\n86.2\nHyCxGL\n65.9\n96.7\n90.5\n89.9\n94.7\n86.4\n89.7\n90.0\n91.9\n88.4\nTable 3: Results for NLU tasks on GLUE development and test set. The best result on each task is in bold. Mcc\nrefers to Matthews correlation coefficient, and Pear refers to Pearson correlation. MNLI task consists of the\nmatched and mismatched datasets, while SST and STS denote the SST-2 and STS-B datasets, respectively.\nModel\nRest14\nLap14\nRest15\nRest16\nOML\n85.80\n80.25\n84.32\n91.56\nOML+Pretrain\n86.25\n81.35\n85.06\n92.05\nOLL+HyCxG\n86.34\n80.88\n84.69\n92.21\nHyCxG\n87.32\n82.29\n86.16\n93.83\nTable 4: Accuracy comparisons between HyCxG, OML\nand OLL+HyCxG on ABSA tasks with BERT encoder.\ntial capability for evaluating linguistic acceptabil-\nity. Meanwhile, HyCxG also significantly outper-\nforms RoBERTa on RTE and MRPC tasks. Espe-\ncially on the MRPC task, HyCxGB even surpasses\nRoBERTaL, which further validates the language\nrepresentation can be enhanced by constructional\ninformation. Besides, HyCxG also boosts the per-\nformances on all other NLU tasks as well.\n4.3\nComparative Analysis\nAblation study on model complexity.\nThough\nthe efficiency of our HyCxG model is illustrated\nfrom the results on NLU tasks, we conduct ab-\nlation study to further investigate whether the\nperformance enhancement is due to an increase\nin model complexity. We implement OML and\nOLL+HyCxG to address this concern. As increas-\ning the depth of the PLMs can improve perfor-\nmance, OML adds an additional transformer layer\non BERT, while OLL+HyCxG removes the last\nlayer of BERT in HyCxG. Besides, we continue\npre-training OML model (OML+Pretrain) on the\nmassive corpus to achieve its optimal performance.\nAs shown in Table 4, OML achieves higher re-\nStrategy\nRest14\nLap14\nRest15\nRest16\nCond-MC\n87.32\n82.29\n86.16\n93.83\nw/o Cond\n85.98\n80.41\n85.24\n92.86\nw/o Selection\n86.43\n81.03\n84.32\n92.05\nTable 5: Results for different strategies of selecting\nconstructions on four SemEval datasets.\nsults than BERT-SPC, indicating that more com-\nplicated models tend to yield better performance.\nOML+Pretrain performs better compared to OML,\nwhich indicates the effect of pre-training. How-\never, OLL+HyCxG surpasses OML with lack of\ntwo transformer layers, while it also overtakes\nOML+Pretrain on Rest 14 and 16 datasets. Mean-\nwhile, HyCxG significantly outperforms OML and\nOML+Pretrain.\nAs the details of computation\ncomplexity comparison between HyCxG and other\nbaselines shown in Appendix D, the results il-\nlustrate that HyCxG achieves the highest perfor-\nmance at a relatively low computational complexity.\nThese evidences all prove the efficacy of HyCxG\nand constructional information. The ablation study\non GLUE benchmark is conducted in Appendix G.\nComparison of construction selection strategies.\nIn section 2, we formulate the Cond-MC problem\nfor selecting typical constructions. Table 5 illus-\ntrates the performances for different strategies.\nThe strategy of w/o Cond denotes that construc-\ntions cannot overlap with each other, while w/o Se-\nlection employs all constructions. As redundancy\nand imbalance can occur in w/o Selection and w/o\nModel\nFrench\nSpanish Turkish Dutch\nBERT-SPC\n83.15\n90.42\n91.72\n87.56\nR-GAT\n83.84\n90.78\n92.41\n88.32\nKumaGCN\n83.70\n91.79\n91.72\n88.07\nDGEDT\n83.43\n91.34\n93.10\n88.32\nDualGCN\n83.98\n91.24\n92.41\n88.58\nHyCxG\n84.54\n92.18\n94.48\n89.34\nTable 6: Accuracy comparisons on SemEval multilin-\ngual datasets with BERT encoder.\nCond, w/o Cond performs better on Rest 15 and\n16 datasets, while fails on Rest 14 and Lap 14\ndatasets compared to w/o Selection. In contrast,\nCond-MC strategy consistently outperforms w/o\nSelection and w/o Cond on all datasets. The results\nillustrate the necessity of selecting typical construc-\ntions and the validity of Cond-MC strategy.\nMultilingual results.\nSince construction gram-\nmar is applicable to multilingual analysis and can\nbe learned via a unified framework for different\nlanguages (Dunn, 2022), we can further discuss the\nperformance improvements of our HyCxG model\nin multilingual settings. The ABSA datasets in\nSemEval 2016 (Pontiki et al., 2016) with multi-\nple languages are conducted for our experiments\n(statistics are shown in Table 8). Meanwhile, we\nalso adapt baselines to multilingual environment\nwith their official implementations. For models that\ninvolve dependency tree parsing (e.g., R-GAT, Du-\nalGCN), we maintain the same parsing tools that\nthey employ (spaCy or Stanford Parser (Manning\net al., 2014)). The results are shown in Table 6.\nWe can observe that the inclusion of syntactic in-\nformation can also improve model performances\non multilingual settings compared to BERT-SPC.\nDualGCN has relatively high performance on dif-\nferent languages, since it incorporates the semantic\nfeatures via SemGCN to the syntactic foundation\nwith regularizers. Moreover, HyCxG outperforms\nthese baselines across all four languages, indicating\nthat constructional information can also enhance\nsemantic understanding for other languages.\nMore empirical studies.\nWe conduct experi-\nments to investigate the potential capacity of Hy-\nCxG. First, experiments in Appendix F demon-\nstrate the superior ability of HyCxG in perceiving\nstatements and patterns. Second, we conduct ab-\nlation study in Appendix I to examine the benefits\nfor each component in HyCxG. We also investigate\nthe results on colloquial datasets in Appendix H.\nNOUN—AUX—be\nIF—PRON—AUX\nPRON—JUST—VERB\nHAVE—DET—ADJ\nHOW—ADJ—NOUN\n(a) BERT\n(b) HyCxG\nFigure 2: 2D t-SNE plot of construction representations.\nAs the constructions are derived from a formal lan-\nguage corpus, which has a different register (Dunn,\n2023) than the colloquial dataset, some of the edge\nconstructions are not extracted to enhance the rep-\nresentation, resulting in suboptimal performances.\nIt illustrates the necessity of obtaining construc-\ntions on a diverse corpus. Besides, we present an\napproach in Appendix J to build the construction\nnetwork with the representations of constructions\nacquired in HyCxG. It depicts the inheritance rela-\ntions between constructions, which provides mean-\ningful implications to usage-based CxG.\n4.4\nCase Study\nThe case study is utilized to illustrate the mech-\nanism inherent in HyCxG for NLU tasks.\nWe\nfirst visualize the learned representations via t-\nSNE (Van der Maaten and Hinton, 2008) in Fig-\nure 2, while the representations of these five ran-\ndomly selected constructions are obtained via ap-\nplying average-pooling in BERT-SPC and HyCxG.\nThe results demonstrate that the representations\nin HyCxG form clusters with distinct boundaries,\nwhile the representations in BERT-SPC are diffuse\namong different clusters. It proves that HyCxG\ncan model higher-order interactions and synthesize\nconstructional information.\nMeanwhile, we visualize the attention scores to\nillustrate the benefits of constructional information\nin Figure 3. As discussed in Section 1, BERT-\nSPC mainly focuses on the opinion word “friendly”,\nleading to misclassification. In contrast, HyCxG\ncan produce correct prediction, since it captures the\nmodality pattern (i.e., NOUN–AUX–be).\n5\nRelated Work\nApplications of CxG in NLP. CxG theories have\nbeen explored to NLP applications. Doubleday\net al. (2017) exploit embodied CxG to parse the\ntext of requests from users, while Nevens et al.\n(2019) employ fluid CxG for semantic parsing in\nBERT-SPC\nHyCxG\nLow\nHigh\n# Positive\n# Negative\n(Ours)\nCxG: [NOUN    AUX       be]\n\"\n!\nFigure 3: Visualization of attention scores from BERT-\nSPC and our HyCxG. Sentiment polarities after the\nmarker # refer to the prediction label of each model.\nvisual question answering. Besides, a knowledge\nnetwork is generated (Kiselev., 2020) with CxG on\nwinograd schema challenge. These approaches all\nrely on hand-made definition with prior knowledge\nand are not compatible with PLMs, thus we employ\nthe usage-based CxG in our work. However, we\nobserve the problems of redundancy and imbalance\nin usage-based approach, which motivates us to\nformulate the conditional max coverage problem\nto acquire discriminative constructions.\nProbing CxG in PLMs. Recent work has con-\nducted empirical studies to probe whether PLMs\nacquire constructions. CxGBERT (Tayyar Mad-\nabushi et al., 2020) finds that constructional in-\nformation is accessed by BERT but cannot be ex-\nploited for lack of proper methods. CxLM (Tseng\net al., 2022) shows that while PLMs are aware of\nthe construction, they are confused at the variable\nslots. Weissweiler et al. (2022) discover that PLMs\ncan recognize the comparative correlative construc-\ntions but fail to utilize their meaning. Furthermore,\nWeissweiler et al. (2023) argue that fine-tuning on\ndownstream tasks necessitates explicit access to\nconstructional information. Our work is the first\nattempt to exploit the constructional information\nfor representation enhancement on NLU tasks.\nHypergraph neural networks. Recent research\nhas extended conventional graph to hypergraph\n(Feng et al., 2019; Ding et al., 2020a; Cai et al.,\n2022), which can model high-order data correla-\ntion. However, when the networks ignore the repre-\nsentation of hyperedges, the capability of capturing\nhigher-order relationships between nodes can be in-\nhibited (Fan et al., 2021). It motivates us to propose\nR-HGAT to encode constructional information.\nAspect-based sentiment analysis. There has\nbeen much work on syntax-based methods (Wang\net al., 2020; Tang et al., 2020; Phan and Ogunbona,\n2020; Li et al., 2021; Tang et al., 2022; Xu et al.,\n2023), which establish syntactic connections be-\ntween each aspect and their corresponding opinion\nwords via dependency and constituency parsing.\n6\nConclusion\nIn this work, we introduce usage-based construc-\ntion grammar (CxG) to enrich language represen-\ntation with constructional information. Then a hy-\npergraph framework of CxG named HyCxG is pro-\nposed to integrate constructional information for\nlanguage representation enhancement. In HyCxG,\nwe extract constructions through a slot-constraint\nmethod, and formulate the conditional max cover-\nage problem for selecting the discriminative con-\nstructions. Then we propose a relational hyper-\ngraph attention network to encode high-order word\ninteractions within constructions. Extensive ex-\nperiments illustrate the validity of our HyCxG on\nnatural language understanding tasks.\nLimitations\nIn this study, the limitations can be summarized\ninto two major aspects:\n(1) The usage-based approach (Dunn, 2017,\n2019) being employed in our work has the ability to\nextract most of constructions, while a small portion\nof non-contiguous constructions (e.g., comparative\ncorrelative constructions) are neglected. These non-\ncontiguous constructions are probably fragmented\ninto multiple independent constructions. We will\ninvestigate the approaches to capture these non-\ncontiguous constructions via incorporating more\nsyntactic knowledge in future work for language\nrepresentation enhancement.\n(2) As discussed in Appendix H, the perfor-\nmances are not significant improved on the tasks\nthat contain a large amount of colloquial expres-\nsions. Since our constructions are mainly learned\nfrom the formal corpus, which has less colloquial\nexpressions. It causes fewer constructions to be ac-\ncessed in these tasks, which encourages us to learn\nconstructions in more diverse corpus to enhance\nthe language representation for natural language\nunderstanding tasks in the future.\nAcknowledgements\nWe thank all anonymous reviewers for their insight-\nful comments and suggestions. And we also ap-\npreciate High-Flyer for providing us with computa-\ntional cards for pre-training. This research is sup-\nported by the Science and Technology Project of\nZhejiang Province (2022C01044) and the National\nNatural Science Foundation of China (51775496).\nReferences\nGiulia ML Bencini and Adele E Goldberg. 2000. The\ncontribution of argument structure constructions to\nsentence meaning.\nJournal of Memory and Lan-\nguage, 43(4):640–651.\nRonny Boogaart, Timothy Colleman, and Gijsbert Rut-\nten. 2014. Extending the scope of construction gram-\nmar. De Gruyter Mouton.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDerun Cai, Moxian Song, Chenxi Sun, Baofeng Zhang,\nShenda Hong, and Hongyan Li. 2022. Hypergraph\nstructure learning for hypergraph neural networks. In\nIJCAI, volume 7, pages 1923–1929.\nChenhua Chen, Zhiyang Teng, Zhongqing Wang, and\nYue Zhang. 2022. Discrete opinion tree induction\nfor aspect-based sentiment analysis. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2051–2064. Association for Computational\nLinguistics.\nChenhua Chen, Zhiyang Teng, and Yue Zhang. 2020.\nInducing target-specific latent structures for aspect\nsentiment classification. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5596–5607. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics, pages 4171–4186. Asso-\nciation for Computational Linguistics.\nKaize Ding, Jianling Wang, Jundong Li, Dingcheng Li,\nand Huan Liu. 2020a. Be more with less: Hyper-\ngraph attention networks for inductive text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4927–4936. Association for Com-\nputational Linguistics.\nXiao Ding, Dingkui Hao, Yuewei Zhang, Kuo Liao,\nZhongyang Li, Bing Qin, and Ting Liu. 2020b. HIT-\nSCIR at SemEval-2020 task 5: Training pre-trained\nlanguage model with pseudo-labeling data for coun-\nterfactuals detection. In Proceedings of the Four-\nteenth Workshop on Semantic Evaluation, pages 354–\n360. International Committee for Computational Lin-\nguistics.\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming\nZhou, and Ke Xu. 2014. Adaptive recursive neural\nnetwork for target-dependent Twitter sentiment clas-\nsification. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 49–54. Association\nfor Computational Linguistics.\nSteve Doubleday, Sean Trott, and Jerome Feldman.\n2017. Processing natural language about ongoing\nactions in ecg. In 2017 aaai spring symposium se-\nries.\nJonathan Dunn. 2017.\nComputational learning of\nconstruction grammars. Language and cognition,\n9(2):254–292.\nJonathan Dunn. 2019.\nFrequency vs. association\nfor constraint selection in usage-based construction\ngrammar. In Proceedings of the Workshop on Cogni-\ntive Modeling and Computational Linguistics, pages\n117–128, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJonathan Dunn. 2022.\nExposure and emergence in\nusage-based grammar: computational experiments in\n35 languages. Cognitive Linguistics.\nJonathan Dunn. 2023.\nExploring the constructicon:\nLinguistic analysis of a computational CxG.\nIn\nProceedings of the First International Workshop\non Construction Grammars and NLP (CxGs+NLP,\nGURT/SyntaxFest 2023), pages 1–11, Washington,\nD.C. Association for Computational Linguistics.\nJonathan Dunn and Sidney Wong. 2022. Stability of\nsyntactic dialect classification over space and time.\nIn Proceedings of the 29th International Conference\non Computational Linguistics, pages 26–36. Interna-\ntional Committee on Computational Linguistics.\nVyvyan Evans. 2007. Glossary of cognitive linguistics.\nEdinburgh University Press.\nHaoyi Fan, Fengbin Zhang, Yuxuan Wei, Zuoyong Li,\nChangqing Zou, Yue Gao, and Qionghai Dai. 2021.\nHeterogeneous hypergraph variational autoencoder\nfor link prediction. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence.\nYifan Feng, Haoxuan You, Zizhao Zhang, Rongrong\nJi, and Yue Gao. 2019. Hypergraph neural networks.\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume 33, pages 3558–3565.\nAdele Goldberg. 1995. Constructions: a construction\ngrammar approach to argument structure. Chicago:\nThe University of Chicago.\nAdele Goldberg. 2006. Constructions at work: Con-\nstructionist approaches in context. Oxford University\nPress on Demand.\nAdele E Goldberg. 2003. Constructions: A new the-\noretical approach to language. Trends in cognitive\nsciences, 7(5):219–224.\nAdele E Goldberg, Mirjam Fried, and Jan-Ola Östman.\n2005. Construction grammar (s): Cognitive and\ncross-language dimension. John Benjamins Amster-\ndam.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In LREC 2018. Eu-\nropean Language Resources Association.\nStefan Th Gries and Stefanie Wulff. 2005. Do foreign\nlanguage learners also have constructions? Annual\nReview of Cognitive Linguistics, 3(1):182–200.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nMartin Hilpert. 2014. Construction grammar and its\napplication to English. Edinburgh University Press.\nThomas Hoffmann and Graeme Trousdale. 2013. The\nOxford handbook of construction grammar. Oxford\nUniversity Press.\nQingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and\nMin Yang. 2019. A challenge dataset and effective\nmodels for aspect-based sentiment analysis. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 6280–6285.\nVsevolod Kapatsinski. 2014. What is grammar like? a\nusage-based constructionist perspective. In Linguis-\ntic Issues in Language Technology, Volume 11, 2014\n- Theoretical and Computational Morphology: New\nTrends and Synergies. CSLI Publications.\nScott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vec-\nchi. 1983. Optimization by simulated annealing. sci-\nence, 220(4598):671–680.\nDenis Kiselev. 2020. An ai using construction grammar:\nAutomatic acquisition of knowledge about words. In\nProceedings of the 12th International Conference\non Agents and Artificial Intelligence - Volume 2:\nICAART,, pages 289–296.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71. Association for Com-\nputational Linguistics.\nRonald W Langacker. 1987. Foundations of cognitive\ngrammar: Theoretical prerequisites, volume 1. Stan-\nford university press.\nRonald W Langacker. 2005. Constructing a language:\nA usage-based theory of language acquisition. Lan-\nguage, 81(3):748–750.\nDong-Hyun Lee et al. 2013. Pseudo-label: The simple\nand efficient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in\nrepresentation learning, ICML, volume 3, page 896.\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xi-\naojie Wang, and Eduard Hovy. 2021. Dual graph\nconvolutional networks for aspect-based sentiment\nanalysis. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6319–6329. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations (ICLR).\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 55–60. Association\nfor Computational Linguistics.\nJens Nevens, Paul Van Eecke, and Katrien Beuls. 2019.\nComputational construction grammar for visual ques-\ntion answering. Linguistics Vanguard, 5(1).\nSlav Petrov, Dipanjan Das, and Ryan McDonald. 2012.\nA universal part-of-speech tagset. In LREC’12, pages\n2089–2096. European Language Resources Associa-\ntion.\nMinh Hieu Phan and Philip O. Ogunbona. 2020. Mod-\nelling context and syntactical features for aspect-\nbased sentiment analysis. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3211–3220. Association for Com-\nputational Linguistics.\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou,\nIon Androutsopoulos, Suresh Manandhar, Moham-\nmad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan\nZhao, Bing Qin, Orphée De Clercq, Véronique\nHoste, Marianna Apidianaki, Xavier Tannier, Na-\ntalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel,\nSalud María Jiménez-Zafra, and Gül¸sen Eryi˘git.\n2016. SemEval-2016 task 5: Aspect based sentiment\nanalysis. In SemEval-2016, pages 19–30.\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemEval-2015 task 12: Aspect based sentiment anal-\nysis. In SemEval 2015, pages 486–495.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. SemEval-2014 task 4: Aspect\nbased sentiment analysis. In SemEval 2014, pages\n27–35.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nGiulia Rambelli, Emmanuele Chersoni, Philippe Blache,\nChu-Ren Huang, and Alessandro Lenci. 2019. Dis-\ntributional semantics meets construction grammar.\ntowards a unified usage-based model of grammar and\nmeaning. In Proceedings of the First International\nWorkshop on Designing Meaning Representations,\npages 110–120. Association for Computational Lin-\nguistics.\nLuc Steels and Joachim de Beule. 2006. A (very) brief\nintroduction to fluid construction grammar. In Pro-\nceedings of the Third Workshop on Scalable Natural\nLanguage Understanding, pages 73–80, New York\nCity, New York. Association for Computational Lin-\nguistics.\nHao Tang, Donghong Ji, Chenliang Li, and Qiji Zhou.\n2020. Dependency graph enhanced dual-transformer\nstructure for aspect-based sentiment classification. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6578–\n6588. Association for Computational Linguistics.\nSiyu Tang, Heyan Chai, Ziyi Yao, Ye Ding, Cuiyun\nGao, Binxing Fang, and Qing Liao. 2022. Affective\nknowledge enhanced multiple-graph fusion networks\nfor aspect-based sentiment analysis. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 5352–5362. As-\nsociation for Computational Linguistics.\nHarish Tayyar Madabushi, Laurence Romain, Dagmar\nDivjak, and Petar Milin. 2020. CxGBERT: BERT\nmeets construction grammar. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 4020–4032. International Committee\non Computational Linguistics.\nYu-Hsiang Tseng, Cing-Fang Shih, Pin-Er Chen, Hsin-\nYu Chou, Mao-Chang Ku, and Shu-Kai Hsieh. 2022.\nCxLM: A construction and context-aware language\nmodel. In LREC’22, pages 6361–6369, Marseille,\nFrance. European Language Resources Association.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017.\nGraph attention networks.\narXiv preprint\narXiv:1710.10903.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP.\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,\nand Rui Wang. 2020. Relational graph attention net-\nwork for aspect-based sentiment analysis. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 3229–3238.\nAssociation for Computational Linguistics.\nLeonie Weissweiler, Taiqi He, Naoki Otani, David\nR. Mortensen, Lori Levin, and Hinrich Schütze.\n2023. Construction grammar provides unique in-\nsight into neural language models. In Proceedings\nof the First International Workshop on Construction\nGrammars and NLP (CxGs+NLP, GURT/SyntaxFest\n2023), pages 85–95, Washington, D.C. Association\nfor Computational Linguistics.\nLeonie Weissweiler, Valentin Hofmann, Abdullatif Kök-\nsal, and Hinrich Schütze. 2022. The better your syn-\ntax, the better your semantics? probing pretrained\nlanguage models for the english comparative correl-\native. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing.\nMichael Wojatzki, Eugen Ruppert, Sarah Holschneider,\nTorsten Zesch, and Chris Biemann. 2017. Germeval\n2017: Shared task on aspect-based sentiment in social\nmedia customer feedback. GermEval, pages 1–12.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45. Association for Com-\nputational Linguistics.\nLvxiaowei Xu, Xiaoxuan Pang, Jianwang Wu, Ming Cai,\nand Jiawei Peng. 2023. Learn from structural scope:\nImproving aspect-level sentiment analysis with hy-\nbrid graph convolutional networks. Neurocomputing,\n518:373–383.\nXiaoyu Yang, Stephen Obadinma, Huasha Zhao, Qiong\nZhang, Stan Matwin, and Xiaodan Zhu. 2020.\nSemEval-2020 task 5: Counterfactual recognition. In\nProceedings of the Fourteenth Workshop on Semantic\nEvaluation, pages 322–335. International Committee\nfor Computational Linguistics.\nA\nBackground\nA.1\nConstruction Grammar\nConstruction Grammar (CxG) theory is a branch of\ncognitive linguistics. It assumes that grammar is a\nmeaningful continuum of lexicon, morphology and\nsyntax rather than solely relying on a system com-\nposed of stable but arbitrary rules for generating\nwell-formed sequences. Therefore, constructions\n(i.e., symbolic elements that connect a certain mor-\nphosyntactic form to a meaning) are thought to\nbe the primary objects of grammatical description\n(Langacker, 1987; Goldberg, 1995, 2006). Con-\nstructions can be defined as linguistic patterns that\nstore different form and meaning pairs. They can-\nnot be strictly predictable by their components.\n(Goldberg, 1995; Langacker, 2005). Besides, pat-\nterns that occur with sufficient frequency can also\nbe considered as constructions (Goldberg, 2006).\nIn terms of the form for CxG, their syntactic\nstructure varies in level of abstractness, including\npartially or fully filled components (e.g., idioms),\nand general linguistic patterns (Goldberg, 2003,\n2006; Dunn and Wong, 2022). In another perspec-\ntive, each construction has immutable components\nand unfilled slots that the element in the slots can\nbe altered with certain words to boost productivity.\nMeanwhile, constructions can also be regarded\nas linguistic knowledge. This knowledge is based\non cognitive results and the product of empirical\ngeneralization and abstraction. In the evolution\nof human language experience, the knowledge is\nformed, acquired and applied by our human (Gold-\nberg, 2003; Hilpert, 2014). Analogy to Knowledge\nGraph (KG), the language systems are considered\nas a network of constructions (Goldberg, 1995).\nThis network consists of two fundamental compo-\nnents: (1) nodes in the graph (i.e., specific construc-\ntions). (2) the edges between nodes. These edges\nrefer to diverse relationships between constructions\n(e.g., polysemy link, metaphorical extension and in-\nstance link). It is worth mentioning that there may\nexist multiple connections between constructions\nin the network.\nA.2\nUsage-based Construction Grammar\nThe CxG paradigm has already developed a variety\nof implementations, including formal approaches,\ni.e., fluid construction grammar (FCG), embod-\nied construction grammar (ECG) and sign-based\nconstruction grammar (SBCG) as well as usage-\nbased approaches. However, the FCG, ECG and\nDataset\n|L|\n#Train\n#Test\n#Task\n• Single-sentence task\nCoLA\n2\n8,551\n1,063\nAcceptability\nSST-2\n2\n67,349\n1,821\nSentiment\n• Sentence-pair task\nQQP\n2\n363,846\n390,965\nParaphrase\nMRPC\n2\n3,668\n1,725\nParaphrase\nMNLI\n3\n392,702\n19,643\nNLI\nQNLI\n2\n104,743\n5,463\nNLI\nRTE\n2\n2,490\n3,000\nNLI\nSTS-B\n*\n5,749\n1,379\nSimilarity\nTable 7: Statistics for the GLUE benchmark. |L| in-\ndicates the number of classes for classification tasks,\nwhile * refers to regression task. The labels of STS-B\nare continuous values from 0 to 5.\nSBCG rely heavily on hand-made definition with\nprior knowledge (Steels and de Beule, 2006; Gold-\nberg et al., 2005; Rambelli et al., 2019). Though\nthese methods can provide high-quality represen-\ntations, they cannot automatically mine the con-\nstructions from the data with the emergence of slot-\nconstraints (Dunn, 2017, 2019).\nWe follow the efforts of Dunn (2019) that pro-\npose a computationally slot-constraints approach\nfor CxG extraction via the data-driven pipeline.\nFirst, three types of slots (i.e., lexical, syntactic\nand joint semantic-syntactic) are defined accord-\ning to the different levels of abstraction. Note that\nthe syntactic slots refer to the POS tags, while the\nsemantic-syntactic slots are derived from word em-\nbeddings that are clustered in discrete semantic\ndomains based on K-Means algorithm (Dunn and\nWong, 2022). Secondly, they generate the poten-\ntial construction templates from the corpus. Then\nthe statistics of frequency and association strength\nare calculated for pruning templates. Finally, the\ntabu search is applied to determine the optimal\nset of constructions with Minimum Description\nLength (MDL) metric (Dunn, 2019). Based on this\npipeline, they develop the c2xg2 toolkit.\nB\nData Statistics\nWe conduct experiments on a variety of natural\nlanguage understanding tasks in main experiments,\nincluding eight tasks from the GLUE benchmark\n(Wang et al., 2018) and five aspect-level senti-\nment analysis tasks (Pontiki et al., 2014, 2015,\n2016; Jiang et al., 2019). Table 7 presents the\n2https://pypi.org/project/c2xg/\nDataset\nDivision\n#Pos.\n#Neu.\n#Neg.\n• Main Experiments\nMAMS\nTrain\n3,380\n5,042\n2,764\nDevelop\n403\n604\n325\nTest\n400\n607\n329\nRest 14\nTrain\n2,164\n637\n807\nTest\n728\n196\n196\nLap 14\nTrain\n994\n464\n870\nTest\n341\n169\n128\nRest 15\nTrain\n912\n36\n256\nTest\n326\n34\n182\nRest 16\nTrain\n1,240\n69\n439\nTest\n469\n30\n117\n• Multilingual Experiments\nFrench\nTrain\n901\n116\n753\nTest\n364\n69\n285\nSpanish\nTrain\n1,368\n89\n479\nTest\n521\n34\n176\nTurkish\nTrain\n770\n111\n504\nTest\n93\n6\n46\nDutch\nTrain\n758\n118\n407\nTest\n245\n24\n125\n• Colloquial Experiments\nTwitter\nTrain\n1,561\n3,127\n1,560\nTest\n173\n346\n173\nGermEval\nTrain\n1,592\n812\n4,554\nDevelop\n207\n83\n777\nTest\n127\n175\n1,112\nTable 8: Statistics for ABSA datasets that be employed\nin the main, multilingual and colloquial experiments.\ndata statistics for GLUE benchmark tasks. We\ndivide the GLUE benchmark tasks into two cate-\ngories: single-sentence task and sentence-pair task.\nIn single-sentence task, the language acceptability\ntask (CoLA) and the sentence-level sentiment task\n(SST-2) are included. As for sentence-pair task,\nit contains similarity task (STS-B), paraphrasing\ntasks (MRPC, QQP) and several natural language\ninference (NLI) tasks (MNLI, QNLI and RTE). All\ntasks except STS-B are classification tasks, while\nSTS-B is a regression task.\nThe statistics for aspect-based sentiment anal-\nysis datasets are shown in Table 8. They are all\nannotated with aspects and corresponding polari-\nties (i.e., positive, neutral or negative). In multi-\nlingual settings, we adopt four different language\ndatasets from SemEval 2016 (Pontiki et al., 2016).\nMeanwhile, Twitter (Dong et al., 2014) and Ger-\nmEval (Wojatzki et al., 2017) datasets, which col-\nlect tweets and messages from social media, are\nemployed in the colloquial expression experiments.\nC\nLearning and Alignment of CxG\nIn order to utilize constructional information, the\ninventory of construction grammar needs to be\nlearned at first. As we present a brief description\nof construction learning procedure in Section A.2,\nprior work has shown that construction grammars\ncan converge on stable representations with suffi-\ncient training data (Dunn, 2022). Therefore, the\nconstruction grammar in English is learned from\nsampling of multi-source corpus (WIKIPEDIA,\nBOOKCORPUS and CC-NEWS) (Devlin et al.,\n2019; Liu et al., 2019) which contains about 1,200\nmillion words. As for other languages utilized in\nmultilingual experiments, constructions are learned\nfrom sampling the multilingual portion of the C4\ncorpus (Raffel et al., 2020). Since the construc-\ntions on a variety of languages are learned in c2xg\ntoolkit, we validate that our HyCxG can achieve\ncomparable performances on NLU tasks with the\nconstruction grammar list in c2xg.\nAfter obtaining the construction grammar list,\nwe are able to derive all possible constructions from\na given sentence via c2xg toolkit. However, the\ntoolkit can only detect which construction patterns\nare present in the sentences and cannot provide\nspecific position indexes of them. Besides, there\nare discrepancies between c2xg and pre-trained\nlanguage models in tokenization procedure. The\nc2xg toolkit utilizes the basic white-space tok-\nenizer, while pre-trained language models adopt\nmore complex algorithms (e.g., WordPiece (Devlin\net al., 2019), SentencePiece (Kudo and Richardson,\n2018)). Therefore, we have to align the position in-\ndexes of the constructions under different tokeniza-\ntion algorithms with specific mapping function.\nTo tackle these imperatives, we develop a wrap-\nper for c2xg. It can be adapted to different tok-\nenization methods with the output of the start and\nend position index for construction spans. We hope\nthat it can facilitate future research on construction\ngrammar in natural language processing tasks.\nD\nAnalysis of Computational Complexity\nIn Table 4, we demonstrate that the effectiveness\nof our model does not result from the increased\ncomplexity of the model. As shown in Table 9,\nwe further analyze the computational complexity\nof the models via DEEPSPEED to illustrate more\nintuitive results. The number of model parameters\nand the multiply–accumulate operations (MACs)\nare utilized to compare complexity on aspect-based\nModel\nParams\nw/o PLM\nMACs\nBERT-SPC\n109.49M\n0.01M\n13.16G\nR-GAT\n110.45M\n0.97M\n13.20G\nLCFS-ASC\n228.42M\n9.46M\n26.79G\nKumaGCN\n112.49M\n3.01M\n13.87G\nDGEDT\n112.50M\n3.02M\n14.08G\nDualGCN\n111.85M\n2.37M\n13.74G\nOML\n116.57M\n7.09M\n14.25G\nHyCxG\n112.24M\n2.76M\n13.48G\nTable 9: Computational complexity analysis of models.\nParams, w/o PLM and MACs represent the number of\nparameters, model parameters without PLM and multi-\nply–accumulate operations, respectively.\nsentiment analysis models. To ensure a fair com-\nparison, we unify the public hyper-parameters of\nall baselines while preserving other unique hyper-\nparameters consistent with their official implemen-\ntations. Besides, the additional embedding param-\neters for constructions in HyCxG are not counted.\nThen three observations can be derived.\nFirst, LCFS-ASC is significantly more complex\nthan other models due to its dual BERT architec-\nture, while other models all use a single PLM.\nSecond, adding an additional transformer layer\nto BERT (OML) indeed improves performances\nbut is far more complex than our HyCxG. How-\never, our model outperforms OML, which proves\nthe improvement of our HyCxG is not just due\nto the increased complexity of the model struc-\nture. Third, our model has lower computational\ncost and memory consumption than most of base-\nlines. Meanwhile, our HyCxG achieves even better\nperformances without dependency information in-\njection, which empirically validates the effective-\nness of constructional information.\nE\nHyper-parameters and Settings\nFor fine-tuning our HyCxG on GLUE and aspect-\nbased sentiment analysis datasets, we search for\nthe optimal task-specific hyper-parameters with the\nrange of values in Table 10 (We implement our\nmodel in PYTORCH and use GeForce RTX 3090\ndevices for experiments):\nIn order to pre-train OML models in Table 4\nand 12, we follow the same pre-training procedure\nof BERT and RoBERTa. OML models are imple-\nmented in PYTORCH and pre-trained on 8 × Tesla\nA100 devices. We employ the officially released\nPLM checkpoints from Hugging Face3 to initial-\n3https://huggingface.co/models\nConfiguration\nValue\nEpoch\n4 / 10 / 50\nBatch size\n16 / 24 / 32\nSequence length\n150 / 250 / 300\nLearning rate\n1e-5 / 2e-5 / 3e-5\nDropout (PLM)\n[0.0, 0.5]\nDropout (R-HGAT)\n[0.0, 0.5]\nLayers\n1\nWeight decay\n1e-1 / 1e-2\nWarmup ratio\n0.03 / 0.06\nTable 10: Detailed hyper-parameter configurations.\nModels\nEnsemble Precision Recall\nF1\nHIT\n88.27\n90.76\n89.49\nHIT+PL\n91.90\n90.00\n90.90\nHyCxG\n94.09\n93.83\n93.96\nHyCxG+PL\n95.21\n94.24\n94.72\nTable 11: Results on counterfactual detection dataset.\nPL is the abbreviation for pseudo-labeling.\nize basic parameters (12 layers) in OML, while\nthe parameters in additional transformer layer are\ninitialized to the average of the preceding 12 lay-\ners. Besides, we set the batch size to 64 per GPU\ndevice, and the training steps to 400,000 during pre-\ntraining procedure with AdamW optimizer (learn-\ning rate is 5e-5 and warmup ratio is 0.1) and the\ncorpus of WIKIPEDIA and BOOKCORPUS.\nF\nPattern Recognition Capability of CxG\nIn this work, we demonstrate that the construction\ngrammar is capable of improving natural language\nunderstanding via infusing constructional informa-\ntion. However, can CxG only serve to enhance\nlanguage representation? We explore more appli-\ncation scenarios for construction grammar based\non this concern and a viable scene is pattern recog-\nnition. The identification of this scenario can be\nviewed as an investigation into whether the text con-\ntains particular specific structures (e.g., negation\nand counterfactual expression). We take counter-\nfactual detection task as an example to verify the\nfeasibility of the scenario.\nThe counterfactual dataset in SemEval 2020\n(Yang et al., 2020) are adopted in this experiment\nwith train (13,000 instances) and test (7,000 in-\nstances) sets. We employ Precision, Recall and F1\nscore as the metrics to evaluate the performance.\nTo better evaluate our method, we compare the\nperformances with the best system in SemEval.\nModel\nCoLA\nSST-2\nRTE\nMRPC\nOML\n64.24\n95.07\n79.06\n90.44\nOML+Pretrain\n64.84\n95.18\n80.51\n91.18\nOLL+HyCxG\n64.42\n95.30\n79.78\n90.69\nHyCxG\n64.90\n95.41\n80.87\n90.93\nTable 12: Ablation study on four GLUE development\ndatasets with RoBERTa-base encoder.\nHIT (Ding et al., 2020b) is the state-of-the-art\nmethod that employs the ensemble model (com-\nbining the large version of BERT, RoBERTa and\nXLNet via weighted average for their probability\npredictions) with pseudo-labeling (Lee et al., 2013)\nstrategy and 10-fold cross-validation. Table 11\nshows the results and the above system is marked\nas HIT+PL. HIT is the single model of RoBERTa-\nlarge without pseudo-labeling. Our HyCxG model\nis also trained with 10-fold cross-validation based\non RoBERTa-large, while HyCxG+PL incorporates\npseudo-labeling strategy to HyCxG.\nSurprisingly, we observe that our HyCxG signif-\nicantly outperforms HIT system by 4.47 F1 points.\nEven for HIT+PL (applying ensemble and pseudo-\nlabeling), there is still a massive gap with HyCxG.\nFurthermore, our HyCxG+PL also has a 3.82 F1\npoints boost compared to HIT+PL. It endorses the\nability of our model in pattern recognition.\nAs discussed by Yang et al. (2020), the main\nchallenge in counterfactual detection task is that\nexisting models focus excessively on token level\nfeatures while neglecting to understand statements.\nMost counterfactual cases are expressed with sub-\njunctive mood to convey wishes, suggestions and\ndemands. There is a practical example “if I were\nasked to, I would be happy to talk to anyone.”,\nwhich is misclassified by HIT model, while Hy-\nCxG can predict correctly. It is a counterfactual\nexpression that regards the part guided by the con-\njunction “if” as the antecedent. Based on such hy-\npothesis, the possible consequent is stated. In this\nsentence, our system can extract two critical con-\nstruction “if–PRON–were–VERB” →“if I were\nasked” and “AUX–be–ADJ” →“would be happy”\nfor capturing the counterfactual statements. Thus,\nHyCxG is capable of recognizing specific patterns\nvia encoding constructional information.\nSimilarly, negation detection is also a recogni-\ntion task relying on a series of special patterns.\nThere are more application scenarios of CxG,\nwhich we leave those discussions to future work.\n10\n0\n30\n40\n50\nRatio (%)\nRest14\nLap14\nTwitter\nGermEval\nACR\nAoC\n20\nFormal Group\nColloquial Group\n48.1\n43.3\n50.0 49.5\n24.3\n31.8\n33.8\n22.7\nFigure 4: Comparison of construction sparsity based on\nAoC and ACR metrics.\nG\nAblation Study on GLUE Datasets\nSimilar to the ablation study on ABSA tasks (Ta-\nble 4), we conduct experiments on GLUE bench-\nmark with four datasets (CoLA, SST-2, RTE and\nMRPC). As discussed previously, the PLM (here is\nRoBERTa-base) with an additional layer on top is\ncalled OML. OML+Pretrain performs an extra pre-\ntraining procedure on OML (details of pre-training\nare shown in Section E). OLL+HyCxG is the model\nthat removes the last transformer layer from PLM\nin our HyCxG model. For the results in Table 12,\nOML+Pretrain always outperforms OML on four\ntasks, which illustrates the utility of pre-training\nprocedure. Besides, our OLL+HyCxG achieve bet-\nter performances compared to OML, even though\nit has two fewer transformer layers. It is worth\nmentioning that the result of OLL+HyCxG is even\nhigher than OML+Pretrain on SST-2 dataset.\nMoreover, HyCxG performs better than OML\nand OML+Pretrain, while it has a smaller number\nof parameters with lower complexity. These results\nindicate the benefits of constructional information\nin natural language understanding tasks and high\ncost performance of our HyCxG.\nH\nColloquial Expression Results\nAs we discuss in Limitations, the performance im-\nprovement of HyCxG is not significant for datasets\nwith informalized expressions (i.e., colloquial ex-\npressions). In this work, constructions are learned\nfrom the standard corpus (e.g., WIKIPEDIA and\nBOOKCORPUS) with formal language. This causes\nsome of edge constructions not being extracted for\nlanguage representation enhancement. We conduct\nexperiments on Twitter (Dong et al., 2014) and Ger-\nmEval (Wojatzki et al., 2017) datasets to investigate\nthe issue for English and German. These datasets\nare collected from social media which consist of\nModel\nTwitter\nGermEval\nAcc\nF1\nAcc\nF1\nBERT-SPC\n75.14\n73.59\n84.65\n66.76\nR-GAT\n76.15\n74.88\n85.50\n68.81\nKumaGCN\n77.89\n77.03\n85.08\n69.26\nDGEDT\n77.90\n75.40\n86.71\n72.81\nDualGCN\n77.40\n76.02\n86.96\n73.44\nHyCxG\n77.17\n75.70\n86.85\n71.71\nTable 13: Experimental performances of datasets with\ncolloquial expressions.\nnumerous colloquial sentences (statistics are shown\nin Table 8). As shown in Table 13, the results on\nTwitter are retrieved from the papers of baselines.\nThen we adapt all the models with multilingual set-\ntings (Section 4.3) to the GermEval dataset. From\nthe results, we observe the performance of our Hy-\nCxG on Twitter is only in the middle compared to\nbaselines. As for GermEval, HyCxG exceeds most\nof models but also falls below DualGCN.\nTo further analyze the evidence for supporting\nour conclusion, two metrics are proposed to eval-\nuate the sparsity of constructions: average ratio\nof construction (AoC) and average coverage ratio\n(ACR). We define AoC as the average proportion\nof extracted constructions in each sentence, while\nACR means the average ratio for the total length\nof the constructions to sequence length in each\nsentence under maximum coverage (Section 2.2).\nMetrics can also be formulated as follows:\nAoC = 1\nN\nN\nX\ni=1\nCxG (si)\nL (si)\nACR = 1\nN\nN\nX\ni=1\nPM\nj=1 L (cj)\nL (si)\n, cj ∈MC (si)\n(10)\nwhere N is the number of sentences in each dataset\nwhile si is the i-th sentence. L(·) is the function,\nthat computes the length of the input and CxG(·)\nis employed to calculate the total number of con-\nstructions in a sentence. Besides, MC(·) refers to\nthe set of constructions in the sentence that satisfy\nthe maximum coverage (w/o condition), while M\nis the size of the set and cj is the j-th construction.\nWe compare these metrics between formal group\n(Rest14, Lap14) and colloquial group (Twitter, Ger-\nmEval) and the results are shown in Figure 4. There\nis a huge gap, where colloquial group is signifi-\ncantly lower than formal group on AoC and ACR.\nIt demonstrates that constructions are much sparser\nModel\nRest14 Lap14\nRest15 Rest16\nHyCxG\n87.32\n82.29\n86.16\n93.83\nw/o HGATT\n86.34\n81.19\n84.69\n92.37\nw/o FFN\n86.70\n81.50\n85.06\n93.02\nTable 14: Experimental results of ablation study on\ncomponents of HyCxG.\nin Twitter and GermEval. In such circumstances,\nconstructional information learned by our model\nhas been inadequate so far to enhance the represen-\ntation. For this limitation in our work, we believe\nthat it can be improved via adding more colloquial\ncorpus to the construction learning (Section C),\nwhich requires further investigations in the future.\nI\nAblation Study on Model Components\nTo further investigate the influence of different com-\nponents in our HyCxG, we conduct extensive ab-\nlation studies. As shown in Table 14, w/o HGATT\nand FFN indicate that we temporarily remove each\nof these components from our model. The ex-\nperimental results illustrate the importance of the\nHGATT network, since its removal can cause sig-\nnificant performance degradation. Meanwhile, it\nalso demonstrates the validity of injecting construc-\ntional information for language representation en-\nhancement. Overall, our HyCxG with all compo-\nnents can achieve the highest performance.\nJ\nNetwork of Constructions\nConstruction grammar is not an unordered set in\na language, but rather a network linked by inher-\nitance relations. The whole network of construc-\ntions in a particular language is called the Con-\nstructicon (Evans, 2007). As briefly introduced\nin Section A.1, Constructicon consists of nodes\n(constructions) and edges (inheritance relations)\n(Goldberg, 2003). There are four types of relations\nbetween constructions: polysemy link (IP ), subpart\nlink (IS), instance link (II) and metaphorical exten-\nsion (IM). Previous efforts are taken by linguists\nusing case studies to manually build the networks\n(Boogaart et al., 2014). To the best of our knowl-\nedge, there is no attempt to construct Constructicon\nfrom the perspective of distributional representa-\ntion. In this work, we can acquire representation\nof each construction via the embedding matrix in\nHyCxG. Therefore, our HyCxG can contribute to\nlinguistics in turn to provide interpretability for the\nnetwork of constructions.\nAlgorithm 2: Compute MD between con-\nstructions\nInput: The set of constructions G.\nOutput: MD matrix for G.\n1 G = [g1, g2, · · · , gn]\n2 Initialize MD ∈Rn×n matrix with 0\n3 for i = 0 to n −1 do\n4\nfor j = 0 to n −1 do\n5\nif i = j then\n6\nMD[i,j]←0\n7\nelse\n8\nMD[i,j]←Compute_md(gi, gj)\n9\nend\n10\nend\n11 end\n12 return MD\n13\n14 Function Compute_md(A, B):\n15\nA = [c1, c2, · · · , cp], B = [e1, e2, · · · , eq]\n16\nInitialize dp ∈R(p+1)×(q+1) matrix with 0\n17\nfor i = 1 to p + 1 do\n18\nfor j = 1 to q + 1 do\n19\nif Is_Match(A[i-1], B[j-1]) then\n20\nd←0\n21\nelse\n22\nd←1\n23\nend\n24\ndp←min(dp[i-1][j]+1,dp[i][j-\n1]+1, dp[i-1][j-1]+d)\n25\nend\n26\nend\n27\ndist←dp[m][n]\n28\nreturn dist\n29 end function\nWe discuss three types of inheritance relation-\nships (IP , IS and II) in this work, while metaphor-\nical extension is more abstract and complex. IP\ncharacterizes the semantic relationship between a\nspecific meaning of a construction and its extended\nmeaning. IS is referred to the connection that a\nconstruction is an independent subpart of another\nconstruction. Besides, when a concrete construc-\ntion is a special instance of another construction,\nthe relationship is defined as II. For the purpose\nof modeling inheritance relationships, we measure\nthe distances between different constructions from\nsemantic and morphological perspectives. Then we\nweight the sum of the two distances and the top-k\nclosest constructions are selected for each construc-\ntion to determine the relationships. Eventually, the\nnetwork of constructions is generated.\nSince the representation of each construction\nis obtained, semantic distance (SD) can be com-\nRelation\nExample\nPolysemy\nthink–PRON–AUX\nbelieve–PRON–AUX\nknow–PRON–AUX\nSubpart\nSCONJ–PRON–AUX–VERB–to\nSCONJ–PRON–AUX–VERB\nSCONJ–PRON–AUX\nInstance\nSCONJ–PRON–AUX\nif–PRON–AUX\nif–PRON–can\nTable 15: Examples for three types of inheritance rela-\ntionships in constructicon.\nputed via cosine similarity. Meanwhile, we pro-\npose multi-level edit distance (MED) algorithm to\nevaluate the morphological distance (MD) between\ntwo constructions with different abstract level. In\nthis setting, semantic distance can provide evidence\nfor polysemy link, while morphological distance\nconstrains variation in structures for subpart link\nand instance link. Thus, these distances are comple-\nmentary on modeling inheritance relations. Under\ndefinition, semantic distance can be formulated as:\nSD (gi, gj) = 1 −\n−→\nvgi · −→\nvgj\n∥−→\nvgj∥∥−→\nvgj∥\n(11)\nwhere −→\nvgi and −→\nvgi are the embedding vector of con-\nstruction gi and gj, which can be looked up from\nconstruction embedding matrix Ec, respectively.\nMED is proposed to compute the morphological\ndistances. As slots in constructions are defined for\ndifferent levels (Lexical, Syntactic and Semantic)\nof abstraction, we need to match slots across the\nlevels. We define a function Is_Match(·, ·) to indi-\ncate whether two slots match or not. The matching\ncriteria can be summarized as follows:\n• (Lexical↔Lexical) Match when the slots are\nidentical with the same POS tags or belong to\ncommon semantic cluster.\n• (Syntactic↔Syntactic, Semantic↔Semantic)\nMatch if and only if the slots are identical.\n• (Lexical↔Syntactic) Match when the slot of\nlexical has the same POS tags as the other.\n• (Lexical↔Semantic) Match when the seman-\ntic cluster of the lexical slot is identical to the\nslot in semantic level.\nBased on such matching strategy, MED algo-\nrithm is illustrated in Algorithm 2. After obtaining\nsemantic and morphological distances, we synthe-\nsize them to compute the construction distance.\nfound--PRON--VERB\nfound--PRON--VERB\ni--VERB--PRON\ni--VERB--PRON\nADP--the--best--NOUN\nADP--the--best--NOUN\nwho--VERB--his\nwho--VERB--his\nwe--VERB--it\nwe--VERB--it\nPRON--VERB--her\nPRON--VERB--her\nthey--VERB--it\nthey--VERB--it\nyou--VERB--PRON\nyou--VERB--PRON\nwhy--PRON--VERB--PRON\nwhy--PRON--VERB--PRON\ndid--PRON--VERB--PRON\ndid--PRON--VERB--PRON\nPRON--VERB--your\nPRON--VERB--your\nwhen--PRON--VERB--PRON\nwhen--PRON--VERB--PRON\nPRON--VERB--you\nPRON--VERB--you\nADP--the--public--NOUN\nADP--the--public--NOUN\nthey--VERB--their\nthey--VERB--their\nSCONJ--PRON--VERB\nSCONJ--PRON--VERB\nVERB--DET--same\nVERB--DET--same\nRON--VERB--PRON\nRON--VERB--PRON\nVERB--DET--latest\nVERB--DET--latest\nshe--VERB--me\nshe--VERB--me\nVERB--you--VERB\nVERB--you--VERB\nON--VERB--PRON--VERB\nON--VERB--PRON--VERB\nhe--VERB--PRON\nhe--VERB--PRON\nADP--the--last--NOUN\nADP--the--last--NOUN\nVERB--PRON--VERB\nVERB--PRON--VERB\nmade--PRON--VERB\nmade--PRON--VERB\nwho--VERB--PRON--VERB\nwho--VERB--PRON--VERB\nPRON--VERB--their\nPRON--VERB--their\nshe--VERB--PRON\nshe--VERB--PRON\nwho--VERB--PRON\nwho--VERB--PRON\nADP--the--ADJ--NOUN\nADP--the--ADJ--NOUN\nPRON--VERB--PRON\nPRON--VERB--PRON\nyou--VERB--you--VERB\nyou--VERB--you--VERB\nPRON--VERB--you--VERB\nPRON--VERB--you--VERB\nPRON--VERB--PRON--<704>\nPRON--VERB--PRON--<704>\nVERB--DET--whole\nVERB--DET--whole\nADP--the--ADJ--NOUN--is\nADP--the--ADJ--NOUN--is\nthe--best--NOUN\nthe--best--NOUN\nVERB--DET--ADJ\nVERB--DET--ADJ\nADP--DET--ADJ--NOUN\nADP--DET--ADJ--NOUN\nVERB--PRON--VERB--me\nVERB--PRON--VERB--me\nAUX--VERB--PRON--VERB--PRON\nAUX--VERB--PRON--VERB--PRON\nyou--VERB--PRON--VERB--PRON\nyou--VERB--PRON--VERB--PRON\nADP--DET--ADJ--NOUN--and\nADP--DET--ADJ--NOUN--and\nyou--VERB--what\nyou--VERB--what\nwho--VERB--DET--ADJ\nwho--VERB--DET--ADJ\nthey--VERB--PRON\nthey--VERB--PRON\nsaid--PRON--VERB\nsaid--PRON--VERB\nget--DET--ADJ\nget--DET--ADJ\nwe--VERB--PRON\nwe--VERB--PRON\nthought--PRON--VERB\nthought--PRON--VERB\nthey--VERB--DET--ADJ\nthey--VERB--DET--ADJ\nVERB--PRON--VERB--PRON\nVERB--PRON--VERB--PRON\ni--VERB--PRON--VERB\ni--VERB--PRON--VERB\nSCONJ--PRON--VERB--PRON\nSCONJ--PRON--VERB--PRON\nwhere--PRON--VERB--PRON\nwhere--PRON--VERB--PRON\nthey--VERB--PRON--VERB\nthey--VERB--PRON--VERB\nPRON--VERB--DET--ADJ\nPRON--VERB--DET--ADJ\nADP--DET--ADJ--NOUN--AUX\nADP--DET--ADJ--NOUN--AUX\nInstance-link\nSubpart-link\nPolysemy-link\nFigure 5: An example of a sub-network with three in-\nheritance relationships with our visualization tool.\nThen the top-k closest constructions for each con-\nstruction can be selected as the potential inheritance\nset. It can be derived as:\n{Gg ⊂G|arg min\n|Gg|=k\n(γ SDg + MDg)}\n(12)\nwhere Gg is the potential inheritance set for con-\nstruction g, while G is the set of all constructions.\nSDg and MDg refer to semantic and morphological\ndistances between g and the other constructions in\nG, respectively. γ is the coupling co-efficiency that\nregulates the two distances. Then we can determine\nthe inheritance relationship within the set based on\nthe definition of IP , IS and II.\nIn order to analyze Constructicon, the embed-\nding matrix Ec of constructions is extracted after\ntraining on CoLA task as an example. We set k\nto 15 and γ to 10 in Equation 12. In such settings,\nwe are able to illustrate the effectiveness of our ap-\nproach with practical examples for each inheritance\nrelationship. As shown in Table 15, the relationship\nbetween “think–PRON–AUX”, “believe–PRON-\nAUX” and “know–PRON–AUX” belongs to polyse-\nmous inheritance. They can all be considered as in-\nstantiations of construction “VERB–PRON–AUX”.\nThus, the subjective belief that someone has done\nor can do something is the primary meaning of this\ntype of constructions. After that, different verbs\nare filled in the slots to inject expanded meanings.\nAs for subpart inheritance, “SCONJ–PRON–AUX”\nis the smallest independent construction in the ex-\nample. It is embodied in “SCONJ–PRON–AUX–\nVERB”, while “SCONJ–PRON–AUX–VERB” is\nalso a subpart of “SCONJ–PRON–AUX–VERB–\nto”. The instance inheritance establishes the con-\nModel\nRest14\nLap14\nRest15\nRest16\nBERT-SPC\n84.91\n77.59\n81.55\n90.26\nR-GAT\n85.54\n78.53\n84.13\n90.90\nKumaGCN\n85.09\n79.00\n83.76\n91.07\nDGEDT\n85.27\n78.67\n82.84\n90.42\nDualGCN\n85.44\n78.84\n83.95\n91.23\nHyCxG\n85.98\n79.31\n84.32\n91.88\nTable 16: Experimental results on aspect-based senti-\nment analysis datasets with validation set.\nnection between the construction and its completely\nor partially filled constructions . “if–PRON–can” is\na specific construction that fills “if” and “can” into\n“SCONJ–PRON–AUX” as a substitute for “SCONJ”\nand “AUX”. Besides, it is worth mentioning that\nIP is undirected relationship, while IS and II are\ndirected relationships in our Constructicon.\nFurthermore, we develop a visualization toolkit\nto demonstrate the network of constructions. As\nshown in Figure 5, a sub-network that depicts the\nthree inheritance relationships is constructed as an\nexample under our settings.\nK\nDetailed Results on ABSA Tasks\nSince all four ABSA datasets (Rest14, Lap14,\nRest15 and Rest16) of SemEval do not have official\nvalidation sets, randomness may be introduced for\nperformance comparison. Therefore, we randomly\ndivide the training set into a new training set and a\nvalidation set with the ratio of 9:1 for these ABSA\ndatasets. Then we conduct the experiments with\nbaseline models and our HyCxG. Meanwhile, the\nhyper-parameter searching is conducted for each\nmodel for fair comparison.\nAs shown in Table 16, we can observe that the\nperformance comparison of the model is almost\nconsistent with that in Table 2. First, the base-\nline models that inject syntactic information always\nachieve better performances than BERT-SPC. Sec-\nond, R-GAT outperforms the other baseline mod-\nels on the Rest14 and Rest15 datasets, while Ku-\nmaGCN and DualGCN have better performance on\nLap14 and Rest16, respectively. Third, HyCxG out-\nperforms all baseline models with constructional\ninformation incorporated on these datasets, which\nillustrates the validity of HyCxG. Meanwhile, it\ncan also substantiate that construction grammar\ncan be regarded as the inductive bias to enhance\nthe language representation in NLU tasks.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-06-05",
  "updated": "2023-06-05"
}