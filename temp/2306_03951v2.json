{
  "id": "http://arxiv.org/abs/2306.03951v2",
  "title": "Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor",
  "authors": [
    "Arshad Javeed",
    "Valent√≠n L√≥pez Jim√©nez"
  ],
  "abstract": "The objective of the project is to explore synergies between classical\ncontrol algorithms such as PID and contemporary reinforcement learning\nalgorithms to come up with a pragmatic control mechanism to control the\nCrazyFlie 2.X quadrotor. The primary objective would be performing PID tuning\nusing reinforcement learning strategies. The secondary objective is to leverage\nthe learnings from the first task to implement control for navigation by\nintegrating with the lighthouse positioning system. Two approaches are\nconsidered for navigation, a discrete navigation problem using Deep Q-Learning\nwith finite predefined motion primitives, and deep reinforcement learning for a\ncontinuous navigation approach. Simulations for RL training will be performed\non gym-pybullet-drones, an open-source gym-based environment for reinforcement\nlearning, and the RL implementations are provided by stable-baselines3",
  "text": "Reinforcement Learning-Based Control of\nCrazyFlie 2.X Quadrotor\nArshad Javeed1\nValent¬¥ƒ±n L¬¥opez Jim¬¥enez2\nProject Advisor: Johan Gr¬®onqvist3\n1ar1886ja-s@student.lu.se\n2va7764lo-s@student.lu.se\n3johan.gronqvist@control.lth.se\nAbstract:\nThe objective of the project is to explore synergies between classical control algorithms\nsuch as PID and contemporary reinforcement learning algorithms to come up with a pragmatic\ncontrol mechanism to control the CrazyFlie 2.X quadrotor. The primary objective would be\nperforming PID tuning using reinforcement learning strategies. The secondary objective is to\nleverage the learnings from the first task to implement control for navigation by integrating\nwith the lighthouse positioning system. Two approaches are considered for navigation, a discrete\nnavigation problem using Deep Q-Learning with finite predefined motion primitives, and deep\nreinforcement learning for a continuous navigation approach. Simulations for RL training will\nbe performed on gym-pybullet-drones, an open-source gym-based environment for reinforcement\nlearning, and the RL implementations are provided by stable-baselines3.\n1.\nIntroduction\nModeling a quadrotor such as CrazyFlie (figure 1) is not a\nstraightforward task due to the non-linearities involved. Of-\nten, the system is linearized around a specific stationary point,\nbut this is task-specific and could be daunting. Instead, we\nfocus on a gray box approach, where we simulate our sys-\ntem using a physics engine [6] to circumvent the physical\nmodeling of the system. In this paper, we synergize between\nclassical PID control and reinforcement learning is explored\nto perform a navigation task in the CrazyFlie 2.X quadrotor.\nPure classical or reinforcement learning approaches are not\nfeasible in terms of convergence and are less interpretable.\nA pure RL approach demands a higher network architecture\nand long training hours to reach convergence. On the other\nhand, an end-to-end classical approach [3][4] controller‚Äôs im-\nplementations can be complex as the design specification are\nnot trivial.\nIn the research, first, we focus on PID tuning, where the pa-\nrameters for the attitude and position Mellinger controller [5]\nare approximated using the Twin-Deep Deterministic Policy\nGradient[1] algorithm and compared against the quadcopter‚Äôs\noriginal values. Next, using the obtained PID parameters, a\nclosed-loop controller is implemented in the simulation envi-\nronment, where the quadrotor‚Äôs task is to navigate to a deter-\nmined point in space in a continuous environment. The RL\nagent is responsible for the high-level tasks and the PID loop\nexecutes the actions. Finally, robustness performance differ-\nences are explored in training with and without noise distur-\nbances for the previous hovering navigation task.\nDuring the implementation of the RL tasks, the algorithm\nselected is TD3, from stable-baselines3 [7], considering\nits simplicity and robustness. We expect other more advanced\nactor-critic models to perform similarly.\nFigure 1.\nCrazyFlie 2.1\n2.\nPID Tuning\nThe objective of the task is to determine the PID parameters\nfor the reinforcement learning agent, based on Mellinger‚Äôs\nPID architecture which consists of 18 parameters divided into\nposition and attitude controllers (table 5. This architecture is\nalready implemented on CrazyFlie‚Äôs firmware, where the user\nwrites the parameters internally to its memory.\nThe following subsections will describe the process of\nobtaining and validating the parameters from agent-train to\nhardware test.\n2.1\nAgent training\nAgent‚Äôs PID parameters are trained over a determined number\nof episodes, starting from random parameters to an optimal\nsolution. The task‚Äôs main objective is to complete successfully\na trajectory, for training a circle and testing a helix.\nUsing TD3 as a reinforcement learning algorithm, con-\nvergence has been achieved in 1000 time steps with a small\nnetwork architecture for the Actor [50,50] and Critic [50,50].\nThe observation space is defined by position ùëãùëåùëçand ori-\nentation ùëÖùëÉùëå. The reward function is computed as (ùë†‚Ä≤ ‚àíùë°)2\n1\narXiv:2306.03951v2  [cs.RO]  14 Jun 2023\nwhere ùë°is the next position in the target trajectory and ùë†‚Ä≤ is the\ncurrent state. See table 1 for detailed information.\nTable 1.\nHyperparameters for PID Tuning\nHyperparameter\nValue\nActor Net Arch\nCritic Net Arch\n[50,50]\n[50,50]\nNumber of Timesteps\nLearning Rate\n1000\n0.001\nReward Function\n‚àí(ùë†‚Ä≤ ‚àíùë°)2\nHelix Height\nHelix Radius\n0.5 m\n0.3 m\n2.2\nHardware implementation\nThe same helix trajectory implemented in simulation is used\nin hardware applying a constant velocity. Crazyflie‚Äôs flying\nreference system is achieved using one of two methods, a rela-\ntive (Flow Deck) and an Absolute (Lighthouse). The absolute\nposition system brings accuracy and stability but depends on\nexternal modules in a fixed environment, meanwhile the rel-\native offers mobility without any extra tracking devices, but\nadds drift and instability to the actions.\n3.\nNavigation\nThe aim of the navigation task is to train a reinforcement-\nlearning policy that learns to navigate the CrazyFlie 2.X\nquadrotor given the destination coordinates in the specified\nenvironment. In contrast to the previous task, where we fed in\na sequence/trajectory, here the RL policy is expected to learn\nto generate the trajectory (actions) on its own. The idea is to\nformulate a navigation environment and train the model in the\nsimulation bed and then export the model for evaluation on\nreal hardware.\nTo start things off, we focus on a relatively simple task of\nhovering the CF2.X quadrotor to get a sense of convergence.\nAlthough hovering is something that can be accomplished by\nclassical PID (as done in the previous task), the idea here is to\ngradually introduce complexities in the environment where a\nsimple PID control would fall short, for instance, introducing\na wall and having to maneuver around it or have a dynamic\nor stochastic environment where the classical path planning\nalgorithms like Dijkstra or A* algorithm can have a hard time\naccommodating the dynamics. It is also worth noting that the\nphase space for a system like CF2.X has 12 states, which can\nfurther exacerbate the task when employing such algorithms.\n3.1\nEnvironment\nThe objective is simple, the goal is for for the reinforcement-\nlearning agent (CF2.X) is to move to the specified destination\nspecified by a set of coordinates [ùë•ùë°, ùë¶ùë°, ùëßùë°] starting from an\ninitial state ùëÜ0. The state space/observation space consists of\nthe coordinates, orientation, and linear and angular velocities\nof the quadrotor. So the state vector comprises of 12 state\nvariables, ùëÜ= [ùë•, ùë¶, ùëß, ùëü, ùëù, ùë¶, ùë£ùë•, ùë£ùë¶, ùë£ùëß, ùë§ùë•, ùë§ùë¶, ùë§ùëß]. Given\na state ùëÜ, the possible actions constitute moving within a 3D\ncube |Œîùë•| ‚â§1, |Œîùë¶| ‚â§1, |Œîùëß| ‚â§1 (a continuous action space).\nThere are several ways of executing the action: i. A pure RL\napproach, where the policy function outputs the low-level con-\ntrol signals - the 4 motor RPMs. ii. Using an open loop con-\ntrol, where a controller is used to compute the control signals\n(motor RPMs) to move to execute the action, and the control\nsignals are applied for a fixed number of iterations. iii. Execut-\ning a closed loop control, here the RL policy is responsible for\npredicting the optimal high-level actions (Œîùë•, Œîùë¶, Œîùëß) and the\nactions are successfully executed by the trusted PID controller,\nrelieving the RL policy from having to learn granular controls.\nApproaches i and ii are supported by gym-pybullet out of\nthe box, while approach iii proposed as part of the project is a\ncustom implementation, and was found to outperform in terms\nof convergence and expected reward.\nThe RL agent was trained using the Twin-Deep De-\nterministic Policy Gradient (TD3) implementation from\nstable_baselines3. The actor is responsible of predic-\ntion an optimal action (ùëé) given a state (ùëÜ), ùúã: ùëÜ‚Üíùëéand\nthe critic is responsible for predicting the reward (ùëü) given\na state and an action, ùëÑ: (ùëÜ, ùëé) ‚Üíùëü. Table 2 summarizes\nthe variables involved. The reward function is defined as the\nnegative squared error of the current state and the target state\n(equation 1). Thus, the objective is to maximize the negative\nreward (ideally close to 0). Both the actor and critic are deep\nneural networks, the actor net has a tanh output action and\nthe critic has a linear output activation.\nTable 2.\nNavigation Task Variables\nVariable\nNotation\nState Space\nùëÜ= [ùë•, ùë¶, ùëß, ùëü, ùëù, ùë¶, ùë£ùë•, ùë£ùë¶, ùë£ùëß, ùë§ùë•, ùë§ùë¶, ùë§ùëß]\nAction Space\nùëé= [Œîùë•, Œîùë¶, Œîùëß]\nMin Action\nùëéùëöùëñùëõ= [‚àí1, ‚àí1, ‚àí1]\nMax Action\nùëéùëöùëéùë•= [+1, +1, +1]\nùëü(ùëÜ, ùëé) = ‚àí[(ùë•+Œîùë•‚àíùë•ùë°)2+(ùë¶+Œîùë¶‚àíùë¶ùë°)2+(ùëß+Œîùëß‚àíùëßùë°)2] (1)\nGiven a state (ùëÜ\n=\n[ùë•, ùë¶, ùëß]) and an action (ùëé\n=\n[Œîùë•, Œîùë¶, Œîùëß]), executing the action involves successfully mov-\ning to the relative coordinates, i.e. the next state is ùëÜ‚Ä≤ =\n(ùë•+ Œîùë•, ùë¶+ Œîùë¶, ùëß+ Œîùëß). To avoid the agent taking long stride\nwhile executing the actions, we scale the output of the actor\naccording to equation 2, The scaling factor of 0.05 implies\nthat the agent is restricted to a distance of 0.05 m (relatively)\nalong individual axes. The TD3 algorithm also defines an ac-\ntion noise for exploration and better convergence. We define\nthe action noise as a multivariate Gaussian (equation 3), so\nthe effective action is then ùëé= 0.05(ùëé+ ùëéùëõ).\nùëé= 0.05 ‚àó[Œîùë•, Œîùë¶, Œîùëß]\n(2)\nùëéùëõ‚àºexp(‚àí1\n2 (ùëé‚àíùúá)ùëáŒ£(ùëé‚àíùúá))\n‚àöÔ∏Å\n(2ùúã)3|Œ£|\n(3)\nTable 3 lists the hyperparameters for training. The con-\ntrol frequency implies the number of control actions during\na period of 2 simulation secs to execute the corresponding\naction.\n2\nTable 3.\nHyperparameters\nHyperparameter\nValue\nActor Net Arch\n[50, 100, 500, 100, 50, 3]\nCritic Net Arch\n[50, 100, 500, 100, 50, 1]\nNumber of Timesteps\n100 000\nLearning Rate\n0.001\nAction Noise Mean (ùúá)\n[0, 0, 0]\nAction Noise Variance (Œ£)\nÔ£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\n0.5\n0\n0\n0\n0.5\n0\n0\n0\n0.5\nÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\nControl Frequency\n50 Hz\nInitial State (of the drone)\nùëÜ0 = [0, 0, 0]\n4.\nRobustness\nAssessing the robustness of black box models has always been\na challenge. While the control algorithms employing the clas-\nsical control principles have an empirical to quantify robust-\nness and evaluating performance, quantifying the robustness\nin the case of black box models relies on deliberate distur-\nbance and adversarial techniques. We resort to the approach\nof injecting disturbance to evaluate the RL policy and also\nexplore the possibility of improving robustness by subjecting\nthe agent to external disturbance during the training phase.\nAt first glance, it might be reasonable to expect that training\nwith disturbance would make the RL policy more resilient to\nexternal disturbances. However, the experimental results re-\nfute the assumption. We find the RL agent to have inherent\nrobustness and training with external disturbance did not have\na significant impact. Our findings corroborate similar results\nreported for other continuous control systems [2].\nWe focus on step/pulse disturbance, as it is more real-\nistic and emulates the wind disturbance experienced by the\nquadcopter. However, the disturbance is applied in multiple\ndirections (along the XYZ axes). To make it challenging, dur-\ning the training phase, the direction of the disturbances is\nswitched every few iterations randomly. Figure 2 shows the\ntraining disturbances applied, the disturbance is applied along\nX, then Z, and then along all three axes XYZ. The evaluation\nis based on the fixed step disturbances and the performance is\nmeasured individually for varying magnitudes of disturbance\nalong individual axes. This also ensures that the replay buffer\nalways contains a good sample.\nFigure 2.\nExternal Disturbances Applied - Training\n4.1\nHardware Implementation\nOnce the reinforcement learning was done, the actor-network\n(neural network) was extracted. The real-time inference was\nachieved by running the model on the computer and sending\nthe actions to CrazyFlie 2.1 over the radio by continuously\nreading the sensor measurements. The compatibility of states\nwas ensured by reading all of the required measurements from\nthe onboard sensors and converting the quantities to respec-\ntive units. The position ([ùë•, ùë¶, ùëß]), orientation ([ùëü, ùëù, ùë¶]) and\nlinear velocities ([ùë£ùë•, ùë£ùë¶, ùë£ùëß]) were read from the Kalman fil-\nter estimates. And the angular velocities ([ùúîùë•, ùúîùë¶, ùúîùëß]) were\nobtained from the gyro.\n5.\nResults\n5.1\nPID\nThe objective of the agent is to complete a full 360 degrees he-\nlix. In the simulation environment, a circle is used for training,\nand convergence is measured against the Crazyflie trajectory\nto trace a helix. In figures 3a, 3b, and 3c the output plot com-\npares the tuned (output parameters from the algorithm) vs the\ndefault for test trajectory along X, Y and Z axis.\nHardware testing includes the two methods mentioned\nabove. First, the relative positioning system uses the Flow\ndeck, which shows a rougher point-to-point displacement,\nmeanwhile, the absolute positioning using the Lighthouse\nshows a smoother performance. See figures 4a and 4b for\nmore details.\nFinally, figure 3d shows the step responses, where tuned\nand default parameters have similar results. Therefore, we can\nconclude that the estimated model is approximated enough to\nthe real, for more details table 4 presents more step response\ninformation.\nFigure 3.\nHelix Test Results in Gym-Pybullet-Drones\n(a) X\n(b) Y\n(c) Z\n(d) Step Response\n5.2\nNavigation\nThe objective for the RL agent is to hover at ùëÜùë°= [0, 0, 1]\nstarting from the origin ùëÜ0 = [0, 0, 0]. Throughout the train-\ning iterations, the starting point remains fixed. Once trained,\nwe evaluate the RL policy in a similar setting to the training\ntask, i.e. starting at the origin, but also test the ability to gen-\neralize by changing the starting point to an arbitrary location\nwhich was never explored by the agent during training, we also\nsubject agent to external disturbances (wind) while evaluating\nthe model in real-world.\n3\nFigure 4.\nHelix - Hardware Results\n(a) Flow Deck (Relative Positioning)\n(b) Lighthouse Deck (Absolute Positioning)\nTable 4.\nStep Response Characteristics - Randomly initialized vs Tuned vs Default\nCharacteristics\nRandomly Initialized\nParams\nTuned Params\nDefault Params\nRise Time (s)\n1.557\n0.649\n0.7728\nSettling Time (s)\n2.445\n1.199\n1.4028\nOvershoot\n0.548\n0.0373\n0.1901\nPeak\n1.0546\n1.000\n1.0067\nTable 5.\nPID Tuning - Default vs Tuned Coefficients\nPosition Controller\nkp xy\nkp z\nki xy\nki z\nkd xy\nkd z\nDefault\n0.4\n1.25\n0.05\n0.05\n0.2\n0.5\nTuned\n0.364\n1.169\n0.052\n0.052\n0.234\n0.586\nAttitude Controller\nkR xy\nkR z\nki m xy\nki m z\nkw xy\nkw z\nDefault\n70000\n60000\n0.0\n500\n20000\n12000\nTuned\n64786.842\n55531.579\n0.0\n599.666\n17217.406\n10330.444\nThe results are presented and compared for the three ap-\nproaches described before i. Pure RL. ii. RL with open loop\ncontrol. iii. RL with closed loop control. Table 6 summarizes\nthe results. The pure RL approach (directly controlling the\nmotor RPMs) was not found to converge even after training\nfor 10 million steps. The RL with the open loop control ap-\nproach managed to converge after 10 hrs of training, while\nthe custom implementation of RL and a PID loop managed\nto converge in significantly less training time and achieves a\nbetter-expected reward (reward per step in the episode). Figure\n5 compare the results of approaches (ii) and (iii). We see that\nthe agent in approach (iii) learns significantly faster and man-\nages to attain a higher reward early in the training phase and\nalso attains a higher expected reward at the end of training.\nThis is due to the fact that the agent has robust low-level con-\ntrol to execute the necessary motion primitives (using a closed\nloop PID controller) and does not have to learn them from\nscratch, unlike approach (ii). It is also worth noting that the\nmagnitude of expected reward in (iii) is quite high compared\nto (ii), a negative reward of -6 vs -3 (per step), this is again due\nto the fact that we leverage robust low-level control, because\nof which the agent can execute actions concretely and explore\nmore of the environment. The actor reward after convergence\nis close to about 1, which is expected, as the shortest distance\nbetween ùëÜ0 = [0, 0, 0], ùëÜùë°= [0, 0, 1] is 1 (Euclidean distance).\nApproach (iii) was simulated after training converged. Fig-\nure 6a is the same as the training task, with the starting point\nbeing the origin. In figure 6b, the starting point was chosen ar-\nbitrarily, ùëÜ0 = [3, 3, 3], which was never explored by the agent\nduring the training phase, yet the policy generalizes well, but\nthe subtle thing to note is that the trajectory is not entirely\noptimal (the shortest path between two points is a straight\nline), instead the agent first navigates to a location close to the\ndestination and then pursues a familiar trajectory encountered\nduring training.\nThe model was also tested to control the real hardware and\nthe results were similar. While testing on the real hardware,\nexternal disturbances were applied in attempts to drift the\nCF2.X from the hovering point, and the model could stabilize\nfairly stabilize and return to the hovering point.\nTable 6.\nTraining Summary\nMetrics\nApproach\nPure RL\nRL + Open loop\nRL + Closed loop\nDescription\nNo controller needed, the RL agent\nlearns to execute the\nlow-level actions on its own.\nThe open-loop makes the RL\ntask easier, but the execution\nof actions is not consistent.\nAbstracts all of the low-level control\nfrom RL and ensures consistency\nin action execution\nNumber of Timesteps\n>10 Million\n‚àº1 Million\n‚àº100 K\nTraining Time ‚Ä†\n>13 hrs\n‚àº10 hrs\n‚àº1.5 hrs\nExpected Reward\n-\n-0.4\n-0.1\nConvergence\nNo\nYes\nYes\n‚Ä† all training durations are reported on the same hardware\n4\nFigure 6.\nRL Policy Test\nThe plots trace the movements of the agent in the\nenvironment. Red indicates the starting point and green is the\ndestination.\n(a) Case 1: Starting at origin (same\nas training task)\n(b) Case 2: Starting at an arbi-\ntrary location (trajectory not ex-\nplored during training)\n5.3\nRoubstness\nFigures 7a - 7c compare the performance of the model trained\nwith and without the disturbance. During the evaluation run,\nthe disturbance vector is held constant throughout the episode\n(step disturbance). The magnitude of the disturbance is var-\nied along with the directions and the expected rewards are\nrecorded individually.\nWhen a disturbance is applied along X or XYZ (figures 7a\n& 7c), the expected reward appears to increase while applying\nthe disturbance along Z (figure 7b) seems to have a negative\nimpact. A disturbance along XY direction appears to tilt the\nquadrotor and it appears to leverage the combined effect of\nthe actions taken and the XY component of the wind force,\nmaking it approach the hovering point much faster, thus a\nbetter reward. This explains why a disturbance along Z has\na drastic negative impact, the wind is constantly pushing the\nquadrotor upwards, and when the magnitude gets large, the\nquadrotor has to work against the wind. We also observe that\nthe model trained without any disturbance attains a better\nreward when evaluating disturbances along XY direction. In\nthe case of disturbance along Z, both models seem to perform\nsimilarly, and as expected the expected reward decreases with\nthe magnitude of noise, because the agent cannot use the wind\nforce along Z to its advantage, instead it was to work against\nit.\nThe robustness evaluation reveals the model trained with-\nout any disturbance during the training phase is inherently\nrobust to external disturbances when subjected to wind forces\nduring the testing phase. Not only the model exhibits some\nform of robustness, but in fact outperforms the model that\nwas subjected to random step disturbances during training.\nIt was also observed that during training injecting a higher\nmagnitude of disturbance resulted in the model not converg-\ning, going unstable as expected. A plausible explanation is\nthat a single RL agent model is unable to learn/differentiate\nbetween the behavior of the combined system, i.e. the drone\nplus the environment dynamics. Our observations corroborate\nsimilar findings reported by a similar study for the cart pole\nexperiment[2]. Thus we conclude that training a model sub-\nject to external disturbance does not have a significant impact\non the model‚Äôs robustness, but it does help in exploring the\nenvironment faster.\nFigures 8a - 8b are the simulated 3D trajectories, that show\nthe model stable, reaching the target point [0, 0, 1]. Behavior\nthat is confirmed in hardware with figures 8c and 9, where\nthe CrazyFlie is subject to disturbances in different directions\n(wind force exerted manually).\n5\nFigure 5.\nRL Training Performance: Comparison of RL + Open Loop Control and RL + Closed Loop Control\nFigures plotting the RL results for the task of hovering using open loop control to execute the actions. The X-axis represents the\nrelative timesteps. The Y-axis represents the magnitude. Total number of timesteps for RL + open loop system is 1 000 000 (1\nM), and 100 000 (100 K) for the RL + closed loop system. The performance metric used here is the expected reward per step\n(higher the better)\nFigure 7.\nRobustness Evaluation - External Disturbance Injection\nComparing expected rewards of RL policy trained without\nand with external steps disturbance\n(a) Distrubance Along X\n(b) Distrubance Along Z\n(c) Distrubance Along XYZ\nFigure 8.\n3D trajectories against disturbances for final model\nRobustness against disturbances in simulation and hardware\n(a) Distrubance Along X in simula-\ntion\n(b) Distrubance Along XYZ in sim-\nulation\n(c) Distrubances along XYZ on\nHardware\n6\nFigure 9.\nFinal model hardware test\n6.\nConclusion\nThe project focused on reinforcement learning-based control\nof a CrazyFlie, exploring aspects of combining classical con-\ntrol and reinforcement learning approaches. Our first objective\nwas to determine how suitable was this virtual environment,\nchoosing as a primary task, the PID tuning of the controller\ncoefficients. In order to do so, the agent needed to complete\na predefined trajectory in simulation and reality. This formed\nthe basis for the navigation task, where the RL agent was\nresponsible for predicting high-level actions for maneuvers,\nand the PID controller was leveraged to execute the actions\nsuccessfully.\nThe navigation task also explores 3 distinct approaches,\ni. A pure RL approach, where RL agent directly outputs the\nlow-level control signal (without the PID controller). ii. RL\nand open-loop approach, where the agent predicts the actions\nand the low-level PID control computes the control signal and\nexecutes them in an open-loop fashion. iii. Finally, the RL\nand closed-loop approach was improved to run a closed-loop\nPID to ensure the high-level actions were perfectly executed\nbefore moving on to the next RL step. We observe that ap-\nproach (iii) performed remarkably, achieving the best results.\nApproach (iii) aims to borrow the best from both fields, robust\nand explainable principles (PID) from classical control, and\nthe ability to adapt and perform complex tasks from reinforce-\nment learning.\nFinally, as part of robustness evaluation, we turn towards\nassessing the inherent robustness of RL models and explore if\nsubjecting the agent to external disturbances would improve\nthe performance. The main objective was to establish a com-\nparison of how an agent trained against disturbances could im-\nprove the model performance. In our case, the model subject to\ndisturbance showed poor performance. We conclude that for\nour experiment the RL agents have inherent robustness and\ntraining with disturbances does not improve the performance,\nbut it does help the agent explore the environment.\n7.\nAcknowledgements\nWe acknowledge our project advisors Johan Gr¬®onqvist and\nEmma Tegling for all the guidance and support. We would also\nlike to acknowledge the Department of Automatic Control,\nLund University for all the resources.\nReferences\n[1]\nS. Fujimoto, H. van Hoof, and D. Meger. ‚ÄúAddressing\nfunction approximation error in actor-critic methods‚Äù.\nCoRR abs/1802.09477 (2018). arXiv: 1802.09477. url:\nhttp://arxiv.org/abs/1802.09477.\n[2]\nC. Glossop, J. Panerati, A. Krishnan, Z. Yuan, and\nA. P. Schoellig. ‚ÄúCharacterising the robustness of rein-\nforcement learning for continuous control using distur-\nbance injection‚Äù. In: Progress and Challenges in Build-\ning Trustworthy Embodied AI. 2022. url: https://\nopenreview.net/forum?id=IgXOXUVObLB.\n[3]\nGreiff, Marcus. Modelling and Control of the Crazyflie\nQuadrotor for Aggressive and Autonomous Flight by Op-\ntical Flow Driven State Estimation. eng. Student Paper.\n2017.\n[4]\nGreiff, Marcus. Nonlinear Control of Unmanned Aerial\nVehicles : Systems With an Attitude. eng. PhD thesis.\nLund University, 2021. isbn: 978-91-8039-047-7. url:\n%7Bhttps : / / lup . lub . lu . se / search / files /\n109517053/MG thesis final.pdf%7D.\n[5]\nD. Mellinger and V. Kumar. ‚ÄúMinimum snap trajectory\ngeneration and control for quadrotors‚Äù. In: 2011 IEEE\nInternational Conference on Robotics and Automation.\n2011, pp. 2520‚Äì2525. doi: 10 . 1109 / ICRA . 2011 .\n5980409.\n[6]\nJ. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P.\nSchoellig. ‚ÄúLearning to fly‚Äîa gym environment with py-\nbullet physics for reinforcement learning of multi-agent\nquadcopter control‚Äù. In: 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS).\n2021.\n[7]\nA. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernes-\ntus, and N. Dormann. ‚ÄúStable-baselines3: reliable rein-\nforcement learning implementations‚Äù. Journal of Ma-\nchine Learning Research 22:268 (2021), pp. 1‚Äì8. url:\nhttp://jmlr.org/papers/v22/20-1364.html.\n7\nAppendix A: Software/Tools Used\nLibrary\nDescription\nVersion\nLicense\ngym\nA universal API for reinforcement learning environments\n0.21.0\nMIT License\ngym-pybullet-drones\nPyBullet Gym environments for single and multi-agent\nreinforcement learning of quadcopter control\n0.0.3\nMIT License\nPyTorch\nTensors and Dynamic neural networks in Python\nwith strong GPU acceleration\n1.11.0\nBSD-3\nstable-baselines3\nPytorch version of Stable Baselines, implementations of\nreinforcement learning algorithms\n1.8.0\nMIT License\ncflib\nCrazyflie python driver\n0.1.22\nGPLv3\nAppendix B: Hardware\nHardware\nDescription\nVersion\nCrazyFlie\nMini quadrotor\n2.1\nCaryradio\nUSB dongle for comm over radio\n2.0\nFlow deck\nRelative positioning system\n2.0\nLighthouse Deck\nAbsolute positioning system\n2 Lighthouse Base stations\nEnables on-board positioning together\nwith the lighthouse deck\n2.0\n8\n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2023-06-06",
  "updated": "2023-06-14"
}