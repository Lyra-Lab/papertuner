{
  "id": "http://arxiv.org/abs/2306.04469v1",
  "title": "Model-Based Deep Learning",
  "authors": [
    "Nir Shlezinger",
    "Yonina C. Eldar"
  ],
  "abstract": "Signal processing traditionally relies on classical statistical modeling\ntechniques. Such model-based methods utilize mathematical formulations that\nrepresent the underlying physics, prior information and additional domain\nknowledge. Simple classical models are useful but sensitive to inaccuracies and\nmay lead to poor performance when real systems display complex or dynamic\nbehavior. More recently, deep learning approaches that use deep neural networks\nare becoming increasingly popular. Deep learning systems do not rely on\nmathematical modeling, and learn their mapping from data, which allows them to\noperate in complex environments. However, they lack the interpretability and\nreliability of model-based methods, typically require large training sets to\nobtain good performance, and tend to be computationally complex. Model-based\nsignal processing methods and data-centric deep learning each have their pros\nand cons. These paradigms can be characterized as edges of a continuous\nspectrum varying in specificity and parameterization. The methodologies that\nlie in the middle ground of this spectrum, thus integrating model-based signal\nprocessing with deep learning, are referred to as model-based deep learning,\nand are the focus here. This monograph provides a tutorial style presentation\nof model-based deep learning methodologies. These are families of algorithms\nthat combine principled mathematical models with data-driven systems to benefit\nfrom the advantages of both approaches. Such model-based deep learning methods\nexploit both partial domain knowledge, via mathematical structures designed for\nspecific problems, as well as learning from limited data. We accompany our\npresentation with running examples, in super-resolution, dynamic systems, and\narray processing. We show how they are expressed using the provided\ncharacterization and specialized in each of the detailed methodologies.",
  "text": "Foundations and Trends® in Signal Processing\nModel-Based Deep Learning\nSuggested Citation: Nir Shlezinger and Yonina C. Eldar (2023), “Model-Based Deep\nLearning”, Foundations and Trends® in Signal Processing: Vol. xx, No. xx, pp 1–18. DOI:\n10.1561/XXXXXXXXX.\nNir Shlezinger\nSchool of Electrical and Computer Engineering,\nBen-Gurion University of the Negev, Be’er Sheva, Israel\nnirshl@bgu.ac.il\nYonina C. Eldar\nFaculty of Mathematics and Computer Science,\nWeizmann Institute of Science, Rehovot, Israel\nyonina.eldar@weizmann.ac.il\nThis article may be used only for the purpose of research, teaching,\nand/or private study. Commercial use or systematic downloading (by\nrobots or other automatic processes) is prohibited without explicit\nPublisher approval.\nBoston — Delft\narXiv:2306.04469v1  [eess.SP]  5 Jun 2023\nContents\n1\nIntroduction\n3\n2\nInference Rule Design\n9\n2.1\nDecision Mapping . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2\nInference Rule Types\n. . . . . . . . . . . . . . . . . . . .\n11\n2.3\nTuning an Inference Rule\n. . . . . . . . . . . . . . . . . .\n13\n3\nModel-Based Methods\n15\n3.1\nObjective Function\n. . . . . . . . . . . . . . . . . . . . .\n15\n3.2\nExplicit Solvers\n. . . . . . . . . . . . . . . . . . . . . . .\n22\n3.3\nIterative Optimizers . . . . . . . . . . . . . . . . . . . . .\n25\n3.4\nApproximation and Heuristic Algorithms . . . . . . . . . .\n35\n3.5\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n4\nDeep Learning\n42\n4.1\nEmpirical Risk . . . . . . . . . . . . . . . . . . . . . . . .\n42\n4.2\nNeural Networks . . . . . . . . . . . . . . . . . . . . . . .\n44\n4.3\nTraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n4.4\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n5\nModel-Based Deep Learning\n59\n5.1\nLearning to Cope with Model Deficiency . . . . . . . . . .\n62\n5.2\nLearned Optimization . . . . . . . . . . . . . . . . . . . .\n68\n5.3\nDeep Unfolding\n. . . . . . . . . . . . . . . . . . . . . . .\n76\n5.4\nDNN-Aided Priors . . . . . . . . . . . . . . . . . . . . . .\n84\n5.5\nDNN-Aided Inference . . . . . . . . . . . . . . . . . . . .\n92\n5.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6\nConclusions\n107\n6.1\nAdvantages of Model-Based Deep Learning\n. . . . . . . . 107\n6.2\nChoosing a Model-Based Deep Learning Strategy . . . . . 109\nAcknowledgements\n113\nAppendices\n114\nA Notations and Abbreviations\n115\nReferences\n117\nModel-Based Deep Learning\nNir Shlezinger1 and Yonina C. Eldar2\n1School of Electrical and Computer Engineering, Ben-Gurion\nUniversity of the Negev; nirshl@bgu.ac.il\n2Faculty of Mathematics and Computer Science, Weizmann Institute of\nScience; yonina.eldar@weizmann.ac.il\nABSTRACT\nSignal processing traditionally relies on classical statisti-\ncal modeling techniques. Such model-based methods utilize\nmathematical formulations that represent the underlying\nphysics, prior information and additional domain knowledge.\nSimple classical models are useful but sensitive to inaccura-\ncies and may lead to poor performance when real systems\ndisplay complex or dynamic behavior. More recently, deep\nlearning approaches that use highly parametric deep neural\nnetworks (DNNs) are becoming increasingly popular. Deep\nlearning systems do not rely on mathematical modeling,\nand learn their mapping from data, which allows them to\noperate in complex environments. However, they lack the\ninterpretability and reliability of model-based methods, typ-\nically require large training sets to obtain good performance,\nand tend to be computationally complex.\nModel-based signal processing methods and data-centric\ndeep learning each have their pros and cons. These paradigms\ncan be characterized as edges of a continuous spectrum vary-\ning in specificity and parameterization. The methodologies\nthat lie in the middle ground of this spectrum, thus integrat-\ning model-based signal processing with deep learning, are\nNir Shlezinger and Yonina C. Eldar (2023), “Model-Based Deep Learning”, Foun-\ndations and Trends® in Signal Processing: Vol. xx, No. xx, pp 1–18. DOI:\n10.1561/XXXXXXXXX.\n©2023 N. Shlezinger and Y. Eldar\n2\nreferred to as model-based deep learning, and are the focus\nhere.\nThis monograph provides a tutorial style presentation of\nmodel-based deep learning methodologies. These are fam-\nilies of algorithms that combine principled mathematical\nmodels with data-driven systems to benefit from the advan-\ntages of both approaches. Such model-based deep learning\nmethods exploit both partial domain knowledge, via mathe-\nmatical structures designed for specific problems, as well as\nlearning from limited data. We accompany our presentation\nwith running signal processing examples, in super-resolution,\ntracking of dynamic systems, and array processing. We show\nhow they are expressed using the provided characterization\nand specialized in each of the detailed methodologies. Our\naim is to facilitate the design and study of future systems\nat the intersection of signal processing and machine learn-\ning that incorporate the advantages of both domains. The\nsource code of our numerical examples are available and\nreproducible as Python notebooks.\n1\nIntroduction\nThe philosophical idea of artificial intelligence (AI), dating back to\nthe works of McCarthy from the 1950’s [1], is nowadays evolving into\nreality. The growth of AI is attributed to the emergence of machine\nlearning (ML) systems, which learn their operation from data, and\nparticularly to deep learning, which is a family of ML algorithms that\nutilizes neural networks as a form of brain-inspired computing [2]. Deep\nlearning is demonstrating unprecedented success in a broad range of\napplications: deep neural networks (DNNs) surpass human ability in\nclassifying images [3]; reinforcement learning allows computer programs\nto defeat human experts in challenging games such as Go [4] and\nStarcraft [5]; generative models translate text into images [6] and create\nimages of fake people which appear indistinguishable from true ones\n[7]; and large language models generate sophisticated documents and\ntextual interactions [8].\nWhile deep learning systems rely on data to learn their operation,\ntraditional signal processing is dominated by algorithms that are based\non simple mathematical models which are hand-designed from domain\nknowledge. Such knowledge can come from statistical models based\non measurements and understanding of the underlying physics, or\n3\n4\nIntroduction\nfrom fixed deterministic representations of the particular problem at\nhand. These knowledge-based processing algorithms, which we refer to\nhenceforth as model-based methods, carry out inference based on domain\nknowledge of the underlying model relating the observations at hand\nand the desired information. Model-based methods, which form the\nbasis for many classical and fundamental signal processing techniques,\ndo not rely on data to learn their mapping, though data is often used\nto estimate a small number of parameters. Classical statistical models\nrely on simplifying assumptions (e.g., linear systems, Gaussian and\nindependent noise, etc.) that make inference tractable, understandable,\nand computationally efficient. Simple models frequently fail to represent\nnuances of high-dimensional complex data, and dynamic variations,\nsettling with the famous observation made by statistician George E. P.\nBox that “Essentially, all models are wrong, but some are useful\". The\nusage of mismatched modeling tends to notably affect the performance\nand reliability of classical methods.\nThe success of deep learning in areas such as computer vision and\nnatural language processing made it increasingly popular to adopt\nmethodologies geared towards data for tasks traditionally tackled with\nmodel-based techniques. It is becoming common practice to replace\nprincipled task-specific decision mappings with abstract purely data-\ndriven pipelines, trained with massive data sets. In particular, DNNs can\nbe trained in a supervised way end-to-end to map inputs to predictions.\nThe benefits of data-driven methods over model-based approaches are\nthreefold:\n1. Purely data-driven techniques do not rely on analytical approxi-\nmations and thus can operate in scenarios where analytical models\nare not known. This property is key to the success of deep learn-\ning systems in computer vision and natural language processing,\nwhere accurate statistical models are typically scarce.\n2. For complex systems, data-driven algorithms are able to recover\nfeatures from observed data which are needed to carry out infer-\nence [9]. This is sometimes difficult to achieve analytically, even\nwhen complex models are perfectly known, e.g., when the env-\niornement is characterized by a fully-known complex simulator or\n5\na partial differential equation.\n3. The main complexity in utilizing ML methods is in the train-\ning stage. In most signal processing domains, this procedure is\ncarried out offline, i.e., prior to deployment of the device which\nutilizes the system. Once trained, they often implement infer-\nence at a lower delay compared with their analytical model-based\ncounterparts [10].\nDespite the aforementioned advantages of deep learning methods,\nthey are subject to several drawbacks. These drawbacks may be limit-\ning factors particularly for various signal processing, communications,\nand control applications, which are traditionally tackled via principled\nmethods based on statistical modeling. For one, the fact that massive\ndata sets, i.e., large number of training samples, and high computa-\ntional resources are typically required to train such DNNs to learn\na desirable mapping, may constitute major drawbacks. Furthermore,\neven using pre-trained DNNs often gives rise to notable computational\nburden due to their immense parameterization. This is highly rele-\nvant for hardware-limited devices, such as mobile phones, unmanned\naerial vehicles, and Interent of Things systems, which are often limited\nin their ability to utilize highly-parametrized DNNs [11], and require\nadapting to dynamic conditions. Furthermore, the abstractness and\nextreme parameterization of DNNs results in them often being treated\nas black-boxes; understanding how their predictions are obtained and\ncharacterizing confidence intervals tends to be quite challenging. As\na result, deep learning does not offer the interpretability, flexibility,\nversatility, reliability, and generalization capabilities of model-based\nmethods [12].\nThe limitations associated with model-based methods and conven-\ntional deep learning systems gave rise to a multitude of techniques\nfor combining model-based processing and ML, aiming to benefit from\nthe best of both approaches. These methods are typically application-\ndriven, and are thus designed and studied in light of a specific task. For\nexample, the combination of DNNs and model-based sparse recovery\nalgorithms was shown to facilitate sparse recovery [10], [13] as well as\nenable compressed sensing beyond the domain of sparse signals [14], [15];\n6\nIntroduction\nDeep learning was used to empower regularized optimization methods\n[16]–[18], while model-based optimization contributed to the design\nand training of DNNs for such tasks [19]–[21]; Digital communication\nreceivers used DNNs to learn to carry out and enhance symbol detection\nand decoding algorithms in a data-driven manner [22]–[24], while symbol\nrecovery methods enabled the design of model-aware deep receivers [25]–\n[27]. The proliferation of hybrid model-based/data-driven systems, each\ndesigned for a unique task, motivates establishing a concrete systematic\nframework for combining domain knowledge in the form of model-based\nmethods and deep learning, which is the focus here.\nIn this monograph we present strategies for designing algorithms\nthat combine model-based methods with data-driven deep learning\ntechniques. While classic model-based inference and deep learning are\ntypically considered to be distinct disciplines, we view them as edges\nof a continuum varying in specificity and parameterization. We build\nupon this characterization to provide a tutorial-style presentation of the\nmain methodologies which lie in the middle ground of this spectrum,\nand combine model-based optimization with ML. This hybrid paradigm,\nwhich we coin model-based deep learning, is relevant to a multitude of\nresearch domains where one has access to some level of reliable mathe-\nmatical modelling. While the presentation here is application-invariant,\nit is geared towards families of problems typically studied in the signal\nprocessing literature. This is reflected in our running examples, which\ncorrespond to three common signal processing tasks of compressed signal\nrecovery, tracking of dynamic systems, and direction-of-arrival (DoA)\nestimation in array processing. These running examples are repeatedly\nspecialized throughout the monograph for each surveyed methodology,\nfacilitating the comparison between the considered approaches.\nWe begin by providing a unified characterization for inference and\ndecision making algorithms in Chapter 2. There, we discuss different\ntypes of inference rules, present the running examples, and discuss the\nmain pillars of designing inference rules, which we identify as selecting\ntheir type, setting the objective, and their evaluation procedure. Then,\nwe show how classical model-based optimization as well as data-centric\ndeep learning are obtained as special instances of this unified character-\nization in Chapters 3 and 4, respectively. We there also review relevant\n7\nbasics that are core to the design of many model-based deep learning sys-\ntems, including fundamentals in convex optimization (for model-based\nmethods) and in neural networks (for deep learning). We identify the\ncomponents dictating the distinction between model-based and data-\ndriven methodologies in the formulated objectives, the corresponding\ndecision rule types, and their associated parameters.\nThe main bulk of this monograph, which builds upon the fun-\ndamental aspects presented in Chapters 2-4, is the review of hybrid\nmodel-based deep learning methodologies in Chapter 5. A core principle\nof model-based deep learning is to leverage data by converting classical\nalgorithms into trainable models with varying levels of abstractness\nand specificity, as opposed to the more classical model-based approach\nwhere data is used to characterize the underlying model. These two\nrationales are highly related to the ML paradigms of generative and\ndiscriminative learning [28], [29]. Consequently, we commence this part\nby presenting a spectrum of decision making approaches which vary\nin specificity and parameterization, with model-based methods and\ndeep learning constituting its edges, followed by a review of generative\nand discriminative learning. Based on these concepts, we provide a\nsystematic categorization of model-based deep learning techniques as\nconcrete strategies positioned along the continuous spectrum.\nWe categorize model-based deep learning methods into three main\nstrategies:\n1. Learned optimization: This approach is highly geared towards\nclassical optimization and aims at leveraging data to fit model-\nbased solvers. In particular, learned optimizers use automated deep\nlearning techniques to tune parameters conventionally configured\nby hand.\n2. Deep unfolding: This family of techniques converts iterative op-\ntimizers into trainable parametrized architectures. Its instances\nnotably vary in their parameterization and abstractness based\non the interplay imposed in the system design between the train-\nable architecture and the model-based algorithm from which it\noriginates.\n8\nIntroduction\n3. DNN-aided inference: These schemes augment model-based al-\ngorithms with trainable neural networks, encompassing a broad\nfamily of different techniques which vary in the module being\nreplaced with a DNN.\nWe exemplify the considered methodologies for the aforementioned\nrunning examples via both analytical derivations as well as simulations.\nBy doing so, we provide a systematic qualitative and quantitative\ncomparison between representative instances of the detailed approaches\nfor signal processing oriented scenarios. The source code used for the\nresults presented in this monograph is available as Python Notebook\nscripts1, detailed in a pedagogic fashion such that they can be presented\nalongside lectures, either as a dedicated graduate level course, or as\npart of a course on topics related to ML for signal processing.\n1The source code and Python notebooks can be found online at https://github.\ncom/ShlezingerLab/MBDL_Book.\n2\nInference Rule Design\nThe first part of this monograph is dedicated to reviewing both classical\ninference that is based on knowledge and modeling, as well as deep\nlearning based inference. We particularly adopt a unified perspective\nwhich allows these distinct approaches to be viewed as edges of a\ncontinuous spectrum, as elaborated further in Chapter 5. To that aim,\nwe begin with the basic concept of inference or decision making.\n2.1\nDecision Mapping\nWe consider a generic setup where the goal is to design a decision\nmapping. A decision rule f maps the input, denoted x ∈X, which is\nthe available observations, into a decision denoted ˆs ∈S, namely,\nf : X 7→S.\n(2.1)\nThis generic formulation encompasses a multitude of settings involving\nestimation, classification, prediction, control, and many more. Conse-\nquently, it corresponds to a broad range of different applications. The\nspecific task dictates the input space X and the possible decisions S.\nTo keep the presentation focused, we repeatedly use henceforth three\nconcrete running examples:\n9\n10\nInference Rule Design\nFigure 2.1: Super-resolution recovery illustration.\nFigure 2.2: Tracking of dynamic system illustration.\nExample 2.1 (Super-Resolution). Here, ˆs is some high-resolution image,\nwhile x is a distorted low-resolution version of ˆs. Thus, X and S are the\nspaces of low-resolution and high-resolution images, respectively. The\ngoal of the decision rule is to reconstruct s from its noisy compressed\nversion x, as illustrated in Fig. 2.1.\nExample 2.2 (Tracking of Dynamic Systems). In our second running\nexample, we consider a dynamic system. At each time instance t, the\ngoal is to map the noisy state observations xt, with X being the space\nof possible sensory measurements, into an estimate if the underlying\nstate ˆst within some possible space S. In particular, the latent state\nvector st evolves in a random fashion that is related to the previous\nstate st−1, while being partially observable via the noisy xt. The setup\nis illustrated in Fig. 2.2.\nExample 2.3 (Direction-of-Arrival Estimation). Our third running exam-\nple considers the array signal processing problem of DoA estimation.\nHere, the observations are a sequence of T multivariate array measure-\nments, i.e., x1, . . . , xT , which correspond to a set of d impinging signals\n2.2. INFERENCE RULE TYPES\n11\nFigure 2.3: DoA estimation setup illustration.\narriving from different angles, as illustrated in Fig. 2.3. The goal is\nto recover the angles, and thus s is the DoAs. Accordingly, X is the\nset of vectors whose cardinality is determined by the number of array\nelements, while S is the set of valid angle values, e.g., S =\n\u0002−π\n2 , π\n2\n\u0003d.\nThe design of an inference rule is typically a two-step procedure.\nThe first step involves selecting the type of inference rule, while the\nsecond step tunes its parameters based on some design objective. We\nnext elaborate on each of these steps.\n2.2\nInference Rule Types\nThe above generic formulation allows the inference rule f to be any\nmapping from X into S. In practice, inference rules are often carried\nout using a structured form, i.e., there exists some set of mappings F\nfrom which f is selected. Some common types of inference rules are:\n• A linear model given by ˆs = W x for some matrix W .\n• A decision tree chooses ˆs from some possible decisions {sk}k by\nexamining a set of nested conditions {condk}k, e.g., if cond1(x)\nthen ˆs = s1; else inspect cond2(x), and so on.\n• An iterative algorithm refines its decision using a mapping g :\nS × X 7→S, repeating\nˆs(k+1) = g\n\u0000ˆs(k); x\n\u0001,\nk = 0, 1, 2, . . .\n(2.2)\n12\nInference Rule Design\nFigure 2.4: Examples of different inference rule types: a) linear model; b) decision\ntree; c) iterative algorithm; d) neural network.\nfrom some initial guess ˆs(0) until convergence. These types of\ninference rules are discussed in detail in Chapter 3.\n• A neural network is typically given by a concatenation of K affine\nlayers and non-linear activations, such that\nˆs = hK\n\u0000hK−1\n\u0000 · · · h1(x)\n\u0001\u0001,\n(2.3)\nwhere each hk(·) is given by hk(z) = σ\n\u0000W kz +bk\n\u0001 with σ(·) being\nan activation and (W k, bk) are parameters of the affine transfor-\nmation. These types of inference rules are discussed extensively\nin Chapter 4.\nThe above inference rules are illustrated in Fig. 2.4.\nThe boundaries between inference rule types are not always strict, so\nthere may be overlap between the categories. For instance, an iterative\nalgorithm with K iterations can often be viewed as a neural network,\nas we further elaborate on in Chapter 5.\n2.3. TUNING AN INFERENCE RULE\n13\n2.3\nTuning an Inference Rule\nInference rules are almost always parameterized, and determine how\nthe context is processed and mapped into ˆs based on some predefined\nstructure. In some cases the number of parameters is small, such as\ndecision trees with a small number of conditions involving comparison\nto some parametric threshold. In other cases, such as when f is a DNN,\ndecision mappings involve a massive number of parameters. These\nparameters capture the different mappings one can represent as decision\nrules.\nIn general, the more parameters there are, the broader the family\nof mappings captured by F, which in turn results in the inference rule\ncapable of accommodating more diverse and generic functions. Decision\nrules with fewer parameters are typically more specific, capturing a\nlimited family of mappings. Let Θ denote the parameter space for a\ndecision rule family F, such that for each θ ∈Θ, f(·; θ) is a mapping in\nF. We refer to the setting of the inference rule parameters as tuning. The\npreference of one parameterization over another involves the formulation\nof an objective function.\nA decision rule is evaluated using a loss function. The objective can\nbe given by a cost or a loss function which one aims at minimizing, or\nit can be specified by an application utility or reward, which we wish\nto maximize. We henceforth formulate the objective as a loss function,\nwhich evaluates the inference rule for a given input compared with some\ndesired decision; such a loss function is formulated as a mapping [30]\nl : F × X × S 7→R+.\n(2.4)\nBroadly speaking, (2.4) dictates the success criteria of a decision map-\nping for a given context-decision pair. For instance, in classification\ntasks, candidate losses include the error rate (zero-one) loss\nlErr(f, x, s) = 1f(x)̸=s,\n(2.5)\nwhile the ℓ2 loss given by\nlEst(f, x, s) = ∥s −f(x)∥2\n2,\n(2.6)\nis suitable for estimation tasks. In addition, prior knowledge on the\ndomain of x is often incorporated into loss measures in the form of\n14\nInference Rule Design\nFigure 2.5: Inference rule selection procedure illustration.\nregularization. For instance, ℓ1 regularization is often used to encourage\nsparsity, while regularizing by the total variation norm is often used in\nimage denoising to reduce noise while preserving details such as edges.\nIt is emphasized that the objective function that is used for tun-\nning the inference rule is often surrogate to the true system objective.\nIn particular, loss functions as in (2.4) often include simplifications,\napproximations, and regularizations, introduced for tractability and\nto facilitate tuning. Furthermore, the true goal of the system is often\nchallenging to capture in mathematical form. For instance, in medical\nimaging evaluation often involves inspecting the outcome by human\nexperts, and can thus not be expressed as a closed-form mathematical\nexpression.\nThe overall design and evaluation procedure is illustrated in Fig. 2.5.\nIn the next chapters we discuss strategies to carry out the design\nprocedure. We begin in Chapter 3 with traditional approaches, referred\nto as model-based or classic methods, that are based on modeling and\nknowledge. Then, in Chapter 4 we discuss the data-centric approach\nwhich uses ML, particularly focusing on deep learning being a leading\nfamily of ML techniques.\n3\nModel-Based Methods\nIn this section we review the model-based approach for designing decision\nmappings based on domain knowledge. Such methods can be generally\nstudied based on the objective used to set the decision rule, and the\nsolver designed based on the objective function.\n3.1\nObjective Function\nIn the classic model-based approach, knowledge of an underlying model\nrelating the input x with the desired decision s is used along with the\nloss measure l(·) in (2.4) to formulate an analytical objective, denoted\nL : F 7→R. The objective can then be applied to select the decision\nrule from F, which can either be a pre-defined inference rule type or\neven from the entire space of mappings from X to S.\nA common model-based approach is to model the inputs as being\nrelated to the targets via some statistical distribution measure P defined\nover X × S. The stochastic nature implies that the targets are not\nfully determined by the input we are observing. Formally, P is a joint\ndistribution over the domain of inputs and targets. One can view such\na distribution as being composed of two parts: a distribution over the\ninput Px (referred to as the marginal distribution), and the conditional\n15\n16\nModel-Based Methods\nprobability of the targets given the inputs Ps|x (referred to as the inverse\ndistribution). Similarly, P can be decomposed into the distribution over\nthe target Ps (coined the prior distribution), and the probability of the\ninputs conditioned on the targets Px|s (referred to as the likelihood).\nModel-based methods are typically divided into two main frame-\nworks: the first is the Bayesian setting, where it is assumed there exists\na joint distribution P; the second paradigm is the non-Bayesian one,\nwhere the target is viewed as a deterministic unknown variable. In the\nnon-Bayesian case there is no prior distribution Ps, and the statistical\nmodel underlying the problem is that of Px|s.\nIn a Bayesian setting, given knowledge of a statistical distribution\nmeasure P, one can formulate the risk\nLP(f) = E(x,s)∼P{l(f, x, s)},\n(3.1)\nand aim at setting the mapping f to the one which minimizes the risk\nLP(f). For some commonly-used loss measures, one can analytically\ncharacterize the risk-minimizing inference rule, as discussed below.\nError-Rate Loss\nWhen using the error rate loss (2.5), the inference\nrule which minimizes the risk among all mappings from X to S is the\nmaximum a-posteriori probability (MAP) rule, given by:\nfMAP(x) = arg min\nf(·)\nE(x,s)∼P{lErr(f, x, s)}\n= arg max\ns∈S\nPr(s|x)\n= arg min\ns∈S\n−log Pr(s|x)\n= arg min\ns∈S\n−log Pr(x|s) −log Pr(s).\n(3.2)\nCross-Entropy Loss\nAn alternative widely-used loss function for classi-\nfication problems is the cross-entropy loss. Here, the inference rule does\nnot produce a label (hard decision), but rather a probability mass func-\ntion over the label space (soft decision). Namely, f(x) is a K × 1 vector\nf1(x), . . . , fK(x) with non-negative entries such that P\nk fk(x) = 1. For\n3.1. OBJECTIVE FUNCTION\n17\nthis setting, the cross entropy loss is given by:\nlCE(f, x, s) = −\nK\nX\nk=1\n1s=sk log fk(x),\n(3.3)\nwhere 1(·) is the indicator function.\nWhile the main motivation for using the cross-entropy loss stems\nfrom its ability to provide a measure of confidence in the decision, as\nwell as computational reasons, it turns out that the optimal inference\nrule in the sense of minimal cross-entropy risk is the true conditional\ndistribution. To see this, we write p(sk|x) = fk(x), and note that for\nany distribution measure p(s|x) over S it holds that\nLP(f) = EP {lCE(f, x, s)}\n= EP {−log p(s|x)}\n= EP\n\u001a\n−log p(s|x)\nPr(s|x)\n\u001b\n+ EP {−log Pr(s|x)}\n= DKL (Pr(s|x)||p(s|x)) + H(s|x),\n(3.4)\nwhere H(s|x) is the true cross-entropy of s conditioned on x, and\nDKL(·||·) is the Kullback-Leibler divergence [31]. As DKL(Pr(s|x)||p(s|x))\nis non-negative and equals zero only when p(s|x) ≡Pr(s|x), it holds\nthat (3.4) is minimized when the inference output is the true conditional\ndistribution, i.e.,\nfCE(x) = [Pr(s1|x), . . . , Pr(sK|x)].\n(3.5)\nWe note that (3.5) implies that the MAP rule can be obtained by taking\nthe arg max over the entries of fCE(·).\nℓ2 Loss\nSimilarly, under the ℓ2 loss (2.6), the risk objective becomes\nthe mean-squared error (MSE), which is minimized by the conditional\nexpectation\nfMMSE(x) = arg min\nf(·)\nE(x,s)∼P{lEst(f, x, s)}\n= Es∼Ps|x{s|x}.\n(3.6)\n18\nModel-Based Methods\nIn the non-Bayesian setting, the risk function as given in (3.1) cannot\nbe used, since there is no prior distribution Ps which is essential for\nformulating the stochastic expectation. Several alternative objectives\nthat are often employed in such setups are discussed below.\nNegative Likelihood Loss\nA common inference rule used in the non-\nBayesian setting is the one which minimizes the negative likelihood, i.e.,\nthe maximum likelihood rule, given by\nfML(x) = arg min\ns∈S\n−Pr(x|s)\n= arg max\ns∈S\nlog Pr(x|s).\n(3.7)\nNote that the maximum likelihood rule in (3.7) coincides with the MAP\nrule in (3.2) when the target obeys a uniform distribution over a discrete\nset S.\nRegularized Least Squares\nAnother popular inference rule employed\nin non-Bayesian setups aims at minimizing the ℓ2 loss while introducing\nregularization to account for some prior knowledge on the deterministic\ntarget variable s. This inference rule is given by\nfRegLS(x) = arg min\ns∈S\n∥s −f(x)∥2\n2 + ρϕ(s),\n(3.8)\nwhere ϕ(s) is a predefined regularization function describing a property\nthat the target is expected to exhibit, and ρ > 0 is a regularization\ncoefficient.\n3.1.1\nRunning Examples\nThe formulation of the objective function is dictated by the model\nimposed on the underlying relationship between x and the desired\ndecision s. This objective typically contains parameters of the model,\nwhich we denote by θo, and henceforth write Lθo(f). We next show the\nobjective parameters which arise in the context of the running examples\nintroduced in Chapter 2.\n3.1. OBJECTIVE FUNCTION\n19\nExample 3.1. A common approach to treat the super-resolution problem\nin Example 2.1 is to assume the compression obeys a linear Gaussian\nmodel, i.e.,\nx = Hs + w,\nw ∼N(0, σ2I).\n(3.9)\nThe matrix H in (3.9) represents, e.g., the point-spread function of the\nsystem. In this case, the log likelihood in (3.2) is given by\n−log Pr(x|s) = −log\n\u0010\n(2π)−k/2det(σ2I)−1/2e−\n1\n2σ2 ∥x−Hs∥2\u0011\n=\n1\n2σ2 ∥x −s∥2 + const.\n(3.10)\nConsequently, the MAP rule in (3.2) becomes\nfMAP(x) = arg min\ns\n1\n2∥x −Hs∥2 + σ2ϕ(s),\n(3.11)\nwhere ϕ(s) := −log p(s). The resulting objective requires imposing\na prior on S encapsulated in ϕ(s). The parameters of the objective\nfunction in (3.11) are thus H and any parameters used for representing\nthe prior ϕ(·).\nThe formulation of the MAP mapping in (3.11) coincides with\nthe regularized least-squares rule in (3.8). However, there is a subtle\ndistinction between the two approaches: In Example 3.1, the rule (3.11)\nis derived from the underlying model P and the regularization is dictated\nby the known prior distribution Ps. In the non-Bayesian setting, there\nis no prior distribution, and ϕ(·) is selected to match some expected\nproperty of the target, as exemplified next.\nExample 3.2. For the super-resolution setup, a popular approach is\nto impose sparsity in some known domain Ψ (e.g., wavelet), such that\ns = Ψr, where r is sparse. This boils down to an objective defined on\nr, which for the regularized least-squares formulation is given by\nLθo(r) = 1\n2∥x −HΨr∥2\n2 + ρ∥r∥0.\n(3.12)\nThe parameters of the objective in (3.12) are\nθo = {H, Ψ, ρ}.\n(3.13)\n20\nModel-Based Methods\nThe parameters H and Ψ in (3.13) follow from the underlying statistical\nmodel and the sparsity assumption imposed on the target. However,\nthe parameter ρ does not stem directly from the system model, and\nhas to be selected so that a solver based on (3.12) yields satisfactory\nestimates of the target.\nThe above examples shows how one can leverage domain knowledge\nto formulate an objective, which is dictated by the parameter vector θo.\nThey also demonstrate two key properties of model-based approaches:\n(i) that surrogate models can be quite unfaithful to the true data, since,\ne.g., the Gaussianity of w implies that x in (3.9) can take negative\nvalues, which is not the case for, e.g., image data; and (ii) that simplified\nmodels allow translating the task into a relatively simple closed-form\nobjective, as in (3.12). A similar Bayesian approaches can be used to\ntackle the dynamic system tracking setting of Example 2.2.\nExample 3.3. Traditional state estimation considers MSE recovery in\ndynamics that take the form of a state-space model, where\nst+1 = g(st, vt),\n(3.14a)\nxt = h(st, wt).\n(3.14b)\nThe noise sequences vt, wt are assumed to be i.i.d. in time. The objective\nat each time instance t is given by\nLθo(f) = E{∥st −ˆst∥2},\nˆst = f ({xτ}τ≤t) .\n(3.15)\nAn important special case of the state-space model in (3.14) is the\nlinear-Gaussian model, where\nst+1 = F st + vt,\n(3.16a)\nxt = Hst + wt.\n(3.16b)\nHere, the noise sequences vt, wt are zero-mean Gaussian signals, i.i.d.\nin time, with covariance matrices V , W , respectively. The parameters\nof the objective function (3.15) under the state-space model in (3.16)\nare thus\nθo = {F , H, V , W }.\n(3.17)\n3.1. OBJECTIVE FUNCTION\n21\nAn additional form of domain knowledge often necessary for for-\nmulating faithful objective functions is reliable modeling of hardware\nsystems. Such accurate modeling is core in the DoA estimation setting\nof Example 2.3.\nExample 3.4. A common treatment of DoA estimation considers a\nnon-Bayesian setup where the input is obtained using a uniform linear\narray with N half-wavelength spaced elements. In this case, assuming\nthat the d transmitted signals are narrowband and in the far-field of\nthe array, the received signal at time instance t is modeled as obeying\nthe following relationship\nxt = A(s)yt + wt.\n(3.18)\nIn (3.18), yt is a d × 1 vector whose entries are the source signals, wt\nis i.i.d. noise with covariance matrix W , and A(θ) = [a(s1), . . . , a(sd)]\nis the steering matrix, where\na(ψ) ≜[1, e−jπ sin(ψ), ..., e−jπ(N−1) sin(ψ)].\n(3.19)\nAs in Example 2.3, we use s to represent the d DoAs of the impinging\nsignals.\nSince the array is known to be uniform with half-wavelength spacing,\nthe steering vectors in (3.19) can be computed for any ψ. Thus, as the\nDoAs are assumed to be deterministic and unknown, the model param-\neters θo include the distributions of the sources yt and the noises wt.\nFor instance, when further imposing a zero-mean Gaussian distribution\non yt and wt,\nθo = {Y , W },\n(3.20)\nwith Y being the covariance matrix of yt. When the covariance matrix\nY is assumed to be diagonal, the sources are said to be non-coherent.\nIn order to formulate the maximum likelihood inference rule of s\ngiven {xt}T\nt=1, one has to model the temporal dependence of yt and wt.\nFor instance, in the simplistic case where both are assumed to be i.i.d.\nin time, by writing the covariance of xt as Σ(s) = A(s)Y AH(s) + W ,\n22\nModel-Based Methods\nthe maximum likelihood rule (3.7) becomes [32]\nfML(x) = arg min\ns∈\n\u0002\n−π\n2 , π\n2\n\u0003d\nT\nX\nt=1\nxH\nt Σ−1(s)xt + T log det (Σ(s))\n= arg min\ns∈\n\u0002\n−π\n2 , π\n2\n\u0003d tr\n \nΣ−1(s)\n \n1\nT\nT\nX\nt=1\nxtxH\nt\n!!\n+ log det (Σ(s)) .\n(3.21)\nThe formulation of the objectives in Examples 3.1-3.4 rely on full\ndomain knowledge, e.g., one has to know the prior ϕ(·), or the covariance\nmatrices and the location of the array elements in order to express the\nobjectives in (3.11), (3.15), and (3.21), respectively.\n3.2\nExplicit Solvers\nModel-based methods determine the parametric objective based on\ndomain knowledge, obtained from measurements and from understand-\ning of the underlying physics. Data and simulations are often used to\nestimate the parameters of the model, e.g., covariances of the noise\nsignals. Once the objective is fixed, setting the decision rule boils down\nto an optimization problem, where one often adopts highly-specific types\nof decision mappings whose structure follows from the optimization\nformulation. In some cases, one can even obtain a closed form explicit so-\nlution. These solvers arise in setups where the objective takes a relatively\nsimplified form, such that one can characterize the optimal mapping.\nExample 3.5. The minimal MSE estimate for the objective (3.15) is\nknown to be given by\nˆst = fMMSE({xτ}τ≤t) = E{st|{xτ}τ≤t}.\n(3.22)\nThis implies that the decision rule should recover the mean of the\nconditional probability of st|{xτ}τ≤t, which can be written using Bayes\nrule as\nPr(st|x1, . . . , xt) = Pr(xt|st, x1, . . . , xt−1) Pr(st|x1, . . . , xt−1)\nPr(xt|x1, . . . , xt−1)\n(a)\n= Pr(xt|st) Pr(st|x1, . . . , xt−1)\nPr(xt|x1, . . . , xt−1)\n.\n(3.23)\n3.2. EXPLICIT SOLVERS\n23\nHere, (a) holds under (3.14), and\nPr(xt|x1, . . . , xt−1) =\nZ\nPr(xt, st|x1, . . . , xt−1)dst\n=\nZ\nPr(xt|st, x1, . . . , xt−1) Pr(st|x1, . . . , xt−1)dst\n=\nZ\nPr(xt|st) Pr(st|x1, . . . , xt−1)dst.\n(3.24)\nSimilarly, we can write Pr(st|x1, . . . , xt−1) as\nPr(st|x1, . . . , xt−1) =\nZ\nPr(st, st−1|x1, . . . , xt−1)dst−1\n=\nZ\nPr(st|x1, . . . , xt−1, st−1) Pr(st−1|x1, . . . , xt−1)dst−1\n=\nZ\nPr(st|st−1) Pr(st−1|x1, . . . , xt−1)dst−1.\n(3.25)\nEquations (3.23)-(3.25), known as Chapman-Kolmogorov equations,\nshow how to obtain the optimal estimate by adaptively updating the\nposterior ˆpt ≜Pr(st|x1, . . . , xt) as t evolves. In particular, given ˆpt−1,\nthe Chapman Kolmogorov equations indicate how ˆpt is obtained via\nˆpt−1\n(3.25)\n⇒Pr(st|x1, . . . , xt−1)\n(3.24)\n⇒Pr(xt|x1, . . . , xt−1)\n(3.23)\n⇒\nˆpt. (3.26)\nWhile the above relationship indicates a principled solver, it is not\nnecessarily analytically tractable (in fact, the family of particle filters\n[33] is derived to tackle the challenges associated with computing (3.26)).\nFor the special case of a linear Gaussian state-space model as in\n(3.16), all the considered variables are jointly Gaussian, and thus we\ncan write\nst|x1, . . . , xt ∼N\n\u0010\nˆst|t, Σt|t\n\u0011\n.\n(3.27)\nSince st|st−1 ∼N(F st−1, V ) by (3.16), then (3.25) implies that\nst|x1, . . . , xt−1 ∼N\n\u0010\nˆst|t−1 = F ˆst−1|t−1,\nΣt|t−1 = F Σt−1|t−1F T + V\n\u0011\n.\n(3.28)\nSimilarly, since xt|st ∼N(Hst, W ) by (3.16), combining (3.28) and\n24\nModel-Based Methods\n(3.24) implies that\nxt|x1, . . . , xt−1 ∼N\n\u0010\nˆxt|t−1 = Hˆst|t−1,\nQt = HΣt|t−1HT + W\n\u0011\n.\n(3.29)\nFinally, substituting (3.28) and (3.29) into (3.23) results in\nˆst|t = ˆst|t−1 + Kt(xt −ˆxt|t−1),\n(3.30)\nΣt|t = Σt|t−1 −KtHΣt|t−1,\n(3.31)\nKt = Σt|t−1HT Q−1\nt .\n(3.32)\nThe matrix Kt is referred to as the Kalman gain. Equations (3.30)-\n(3.32) are known as the Kalman filter [34], which is one of the most\ncelebrated and widely-used algorithms in signal processing and control.\nExample 3.5 demonstrates how the modeling of a complex task using\na simplified linear Gaussian model, combined with the usage of a simple\nquadratic objective, results in an explicit solution, which here takes a\nlinear form. The resulting Kalman filter achieves the minimal MSE when\nthe state-space formulation it uses faithfully represents the underlying\nsetup, and its parameters, i.e., θo, are accurate. However, mismatches\nand approximation inaccuracies in the model and its parameters can\nnotably degrade performance.\nTo see this, we simulate the tracking of a two dimensional state vec-\ntor from noisy observations of its first entry obeying a linear Gaussian\nstate-space model, where the state evolution matrix is F =\n\"\n1\n0.1\n0\n1\n#\n.\nWhen the state-space model is fully characterized, the Kalman filter\nsuccessfully tracks the state, and we visualize the estimated first entry\ncompared with its true value and noisy observations in Fig. 3.1a. How-\never, when instead of the data being generated from a state-space model\nwith state evolution matrix F , it is generated from the same model\nwith the matrix rotated by 0.01 [rad], i.e., there is a slight mismatch in\nthe state-space model, then there is notable drift in the performance\nof the Kalman filter, as illustrated in Fig. 3.1b. These results showcase\nthe dependency of explicit solvers on accurate modeling.\n3.3. ITERATIVE OPTIMIZERS\n25\n(a) Kalman filter with full domain knowl-\nedge.\n(b) Kalman filter with rotated observa-\ntions.\nFigure 3.1: Tracking a dynamic system from noisy observations using the model-\nbased Kalman filter.\n3.3\nIterative Optimizers\nSo far, we have discussed model-based decision making, and saw that\nobjectives are often parameterized (we used θo to denote these objective\nparameters). We also noted that in some cases, the formulation of the\nobjective indicates how it can be solved, and even saw scenarios for which\na closed-form analytically tractable solution exists. However, closed-\nform solutions are often scarce, and typically arise only in simplified\nmodels. When closed-form solutions are not available, the model-based\napproach resorts to iterative optimization techniques.\nOptimization problems take the generic form of\nminimize\nL0(s),\nsubject to\nLi(s) ≤0,\ni = 1, . . . m,\nwhere s ∈Rn is the optimization variable, the function L0 : Rn 7→R is\nthe objective, and Li : Rn 7→R for i = 1, . . . , m are constraint functions.\nBy defining the constraint set S = {s : Li(s) ≤0, ∀i = 1, . . . m} ⊂Rn,\nwe can write the global solution to the optimization problem as\ns∗= arg min\ns∈S\nL0(s).\n(3.33)\nWhile we use the same notation (s) for the optimization variable and\nthe desired output of the inference rule, in the context of inference rule\ndesign the optimized variable is not necessarily the parameter being\ninferred.\n26\nModel-Based Methods\nAs the above formulation is extremely generic, it subdues many\nimportant families. One of them is that of linear programming, for which\nthe functions {Li(·)}m\ni=0 are all linear, namely\nLi(αs1 + βs2) = αLi(s1) + βLi(s2),\n∀α, β ∈R, s1, s2 ∈Rn.\nA generalization of linear programming is that of convex optimization,\nwhere for each i = 0, 1, . . . , m it holds that\nLi(αs1+βs2) ≤αLi(s1)+βLi(s2),\n∀α, β ∈R, s1, s2 ∈Rn. (3.34)\nIn particular, if (3.34) holds for i = 0, we say that the optimization\nproblem (3.33) has a convex objective, while when it holds for all i =\n1, . . . m we say that S is a convex set.\nConvex optimization theory provides useful iterative algorithms for\ntackling problems of the form (3.33). Iterative optimizers typically give\nrise to additional parameters which affect the speed and convergence rate\nof the algorithm, but not the actual objective being minimized. We refer\nto these parameters of the solver as hyperparameters, and denote them\nby θh. As opposed to the objective parameters θo (as in, e.g., (3.13)),\nthey often have no effect on the solution when the algorithm is allowed\nto run to convergence, and so are of secondary importance. But when\nthe iterative algorithm is stopped after a predefined number of iterations,\nthey affect the decisions, and therefore also the decision rule objective.\nDue to the surrogate nature of the objective, such stopping does not\nnecessarily degrade the evaluation performance. We next show how this\nis exemplified for several common iterative optimization methods.\n3.3.1\nFirst-Order Methods\nWe begin with unconstrained optimization, for which S = Rn, i.e.,\ns∗= arg min\ns\nL0(s).\n(3.35)\nWhen the objective is convex, L0(·) has a single minima, and thus if one\ncan iteratively update over s(k) and guarantee that L0(s(k+1)) < L0(s(k))\nthen this procedure is expected to converge to s∗. The family of iterative\noptimizers that operate this way are referred to as descent methods.\n3.3. ITERATIVE OPTIMIZERS\n27\nDescent methods that operate based on first-order derivatives of the\nobjective, i.e., using gradients, are referred to as first-order methods.\nArguably the most common first-order method is gradient descent,\nwhich dates back to Cauchy (in the 19th century).\nGradient Descent\nGradient descent stems from the first-order mul-\ntivariate Taylor series expansion of L0(·) around s(k), which implies\nthat\nL0(s(k) + ∆s) ≈L0(s(k)) + ∆sT ∇sL0(s(k)),\n(3.36)\nwhich holds when s(k) + ∆s is in the proximity of s(k), i.e., the norm\nof ∆s is bounded by some ϵ > 0. In this case, we seek the setting\nof ∆s for which the objective is minimized subject to ∥∆s∥2 ≤ϵ.\nThe right hand side of (3.36) is minimized under the constraint when\n∆s = −µ∇sL0(s(k)) by the Cauchy-Schwartz inequality, where µ is set\nto µ =\nϵ\n∥∇sL0(s(k))∥2 , guaranteeing that the norm of ∆s is bounded and\nthus the first-order Taylor series representation in (3.36) approximately\nholds. The resulting update equation, which is repeated over the iteration\nindex k = 0, 1, 2, . . ., is given by\ns(k+1) ←s(k) −µk∇sL0(s(k)),\n(3.37)\nwhere µk is referred to as the step size of the learning rate.\nIt is noted though that one must take caution to guarantee that\nwe are still in the proximity of s(k) where the approximation (3.36)\nholds. A possible approach is to select the step size µk at each iteration\nusing some form of line search, finding the one which maximizes the\ngap L0(s(k)) −L0(s(k+1)) via, e.g., backtracking [35, Ch. 9.2].\nProximal Gradient Descent\nGradient descent relies on the ability to\ncompute the gradients of L0(·). Consider an objective function that can\nbe decomposed into two functions as follows:\nL0(s) = g(s) + h(s),\n(3.38)\nwhere both g(·) and h(·) are convex, but only g(·) is necessarily differen-\ntiable. Such settings often represent regularized optimization, where h(·)\nrepresents the regularizing term. A common approach to implement a\n28\nModel-Based Methods\ndescent method which updates s(k) at each iteration k is to approximate\ng(·) with its second-order Taylor series approximation. Nonetheless, the\nsecond-order derivatives, i.e., the Hessian matrix, is not explicitly com-\nputed (hence the corresponding method is still a first-order method).\nInstead, the Hessian matrix is approximated as\n1\nµk I for some µk ≥0.\nThis results in\ng(s(k) + ∆s) ≈g(s(k)) + ∆sT ∇sg(s(k)) +\n1\n2µk\n∥∆s∥2\n2.\n(3.39)\nUsing the approximation in (3.39), we aim at finding s(k+1) =\ns(k) + ∆s by minimizing\ns(k+1) = arg min\nz\ng(s(k)) + (z −s(k))T ∇sg(s(k)) +\n1\n2µk\n∥z −s(k)∥2\n2 + h(z)\n= arg min\nz\n1\n2µk\n∥µk∇sg(s(k))∥2\n2 + (z −s(k))T ∇sg(s(k))\n+\n1\n2µk\n∥z −s(k)∥2\n2 + h(z) + g(s(k)) −\n1\n2µk\n∥µk∇sg(s(k))∥2\n2\n= arg min\nz\n1\n2µk\n\r\r\rz −\n\u0010\ns(k) −µk∇sg(s(k))\n\u0011\r\r\r\n2\n2 + h(z).\n(3.40)\nNext, we define the proximal mapping as\nproxϕ(y) ≜arg min\nz\n1\n2∥z −y∥2\n2 + ϕ(z).\n(3.41)\nThe resulting update equation, which is repeated over the iteration\nindex k = 0, 1, 2, . . ., is given by\ns(k+1) ←proxµk·h\n\u0010\ns(k) −µk∇sg(s(k))\n\u0011\n,\n(3.42)\nwhere we use proxµk·h to represent the proximal mapping in (3.41) with\nϕ(z) ≡µk · h(z). Equation (3.42) is the update equation of proximal\ngradient descent. Its key benefits stem from the following properties:\n• The proximal mapping can be computed analytically for many\nrelevant h(·) functions, even if they are not differentiable.\n• The proximal mapping is completely invariant of g(·) in (3.38).\n• The update equation in (3.42) coincides with conventional gradient\ndescent as in (3.37) when the term h(·) is constant.\n3.3. ITERATIVE OPTIMIZERS\n29\nExample 3.6 (ISTA). Recall the ℓ0 regularized objective in (3.12) used\nfor describing the super-resolution with sparse prior problem mentioned\nin Example 3.2, where for simplicity we focus on the setting where\nΨ = I, i.e., r = s. While this function is not convex, it can be relaxed\ninto a convex formulation, known as the LASSO objective, by replacing\nthe ℓ0 regularizer with an ℓ1 norm, namely:\nfLASSO(x) = arg min\ns\n1\n2∥x −Hs∥2 + ρ∥s∥1.\n(3.43)\nThe objective in (3.43) takes the form of (3.38) with h(s) ≡ρ∥s∥1 and\ng(s) ≡1\n2∥x −Hs∥2. In this case\n∇sg(s) = −HT (x −Hs).\n(3.44)\nThe proximal mapping is given by\nproxµk·h(y) = arg min\nz\n1\n2∥z −y∥2\n2 + µkρ∥z∥1 = Tµkρ(y),\n(3.45)\nwhere Tβ(·) is the soft-thresholding operation applied element-wise,\ngiven by\nTβ(x) ≜sign(x) max(0, |x| −β).\n(3.46)\nThis follows since ∥y −s∥2\n2 + 2β∥s∥1 = P(yi −si)2 + 2β|si|, and for\nscalars yi and si, it holds that\nd\ndsi\n\u0010\n(yi −si)2 + 2β|si|\n\u0011\n= 2 (si −yi + βsign(si)) .\n(3.47)\nWhen compared to zero, (3.47) yields si = Tβ(yi). The resulting formu-\nlation of the proximal gradient descent method is known as iterative\nsoft thresholding algorithm (ISTA) [36], and its update equation is given\nby\ns(k+1) ←Tµkρ\n\u0010\ns(k) + µkHT (x −Hs(k))\n\u0011\n.\n(3.48)\nExample 3.7 (Projected Gradient Descent). Consider an optimization\ncarried out over a closed set S ⊂Rn. Such optimization problems can\nbe re-formulated as\ns∗= arg min\ns∈S\nL0(s) = arg min\ns\nL0(s) + IS(s),\nIS(s) ≜\n\n\n\n0\ns ∈S\n∞\ns /∈S\n.\n30\nModel-Based Methods\nIn this case, the proximal mapping specializes into the projection oper-\nator denoted ΠS, since\nproxµk·IS(y) = arg min\nz\n1\n2∥z −y∥2\n2 + µkIS(z)\n= arg min\nz∈S\n∥z −y∥2\n2 ≡ΠS(y).\n(3.49)\nConsequently, the proximal gradient descent here coincides with pro-\njected gradient descent, i.e.,\ns(k+1) ←ΠS\n\u0010\ns(k) −µk∇sL0(s(k))\n\u0011\n.\n(3.50)\nFirst-order optimization procedures introduce additional hyperpa-\nrameters, namely θh = {µk}, which are often fixed to a single step-size\nto facilitate tuning, i.e., θh = µ. The exact setting of these hyper-\nparameters in fact affects the performance of the iterative algorithm,\nparticularly when it is limited in the maximal number of iterations it\ncan carry out. To illustrate this dependency, we evaluate ISTA for the\nrecovery of a signal with n = 1000 entries, of which only 5 are non-zero,\nfrom a noisy observations vector of comprised of 256 measurements\ntaken with a random Gaussian measurement matrix corrupted by noise\nwith variance σ2 = 0.01. We use two different step-sizes – µ = 0.1\n(Fig. 3.2a) and µ = 0.05 (Fig. 3.2b) – and limit the maximal number\nof ISTA iterations to 1000. Observing Fig. 3.2, which depicts both the\nrecovered signal as well as the convergence profile, i.e., the evolution of\nthe squared error over the iterations, we note that the setting of the\nhyperparameters of the optimization procedure has a dominant effect\non both the recovered signal and the convergence rate.\n3.3.2\nConstrained Optimization\nExample 3.7 shows one approach to tackle constrained optimization,\nby casting the problem as an unconstrained objective which results in\nprojected gradient descent as a first-order method. This is obviously\nnot the only approach to tackle constrained optimization. The more\nconventional approach is based on Lagrange multipliers and duality. To\ndescribe these methods, let us first repeat the considered constrained\n3.3. ITERATIVE OPTIMIZERS\n31\n(a) ISTA with step-size µ = 0.1.\n(b) ISTA with step-size µ = 0.05.\nFigure 3.2: The convergence profile (upper figures) and recovered signal (lower\nfigures) achieved by ISTA for recovering a sparse high-dimensional signal from noisy\nlow-dimensional observations.\noptimization problem (with additional equality constraints):\nminimize\nL0(s),\n(3.51)\nsubject to\nLi(s) ≤0,\ni = 1, . . . m,\nlj(s) = 0,\nj = 1, . . . r.\nThe classic method of Lagrange multipliers converts the constrained\noptimization problem (3.51) into a single objective with additional\nauxiliary variables η = [η1, . . . , ηm] and µ = [µ1, . . . , µr] with ηi ≥0\nand µjlj(s) = 0 for each i ∈1, . . . , m and j ∈1, . . . r, respectively. The\nLagrangian is defined as\nL(s, µ, η) ≜L0(s) +\nm\nX\ni=1\nηiLi(s) +\nr\nX\nj=1\nµjlj(s),\n(3.52)\nand the dual function is\nd(µ, η) ≜min\ns\nL(s, µ, η).\n(3.53)\nThe benefit of defining the dual function stems from the fact that for\nevery non-negative η1, . . . , ηm and for each s satisfying the constraints\n32\nModel-Based Methods\nin (3.51), i.e., s ∈S, it holds that\nL(s, µ, η) = L0(s) +\nm\nX\ni=1\nηiLi(s) +\nr\nX\nj=1\nµjlj(s)\n(a)\n= L0(s) +\nm\nX\ni=1\nηiLi(s)\n(b)\n≤L0(s),\n(3.54)\nwhere (a) and (b) follow since for each s ∈S it holds that lj(s) = 0 and\nLi(s) ≤0, respectively. Consequently,\nd(µ, η) = min\ns\nL(s, µ, η) ≤min\ns\nL0(s) ≤L0(s∗),\ni.e., for every non-negative η1, . . . , ηm, the dual function is a lower\nbound on the global minima of (3.51). Under some conditions (Slater’s\ncondition, KKT, see [35, Ch. 5]), the maxima of the dual coincides with\nthe global minima of the primal (3.51), and thus solving (3.51) can be\ncarried out by solving the dual problem\nmaximize\nd(µ, η),\n(3.55)\nsubject to\nηi ≥0,\ni = 1, . . . m.\n(3.56)\nA common optimization algorithm which is based on duality is\nthe alternating direction method of multipliers (ADMM) [37]. This\nalgorithm aims at solving optimization problems where the objectives\ncan be decomposed into two functions g(·) and h(·), with a constraint\nof the form\nˆs = arg min\ns\nmin\nv\ng(s) + h(v) + λ∥s −v∥2,\n(3.57)\ns.t.\nAv + Bs = c,\nfor some fixed A, B, c. ADMM tackles the regularzied objective in\n(3.57) by formulating a dual problem as in (3.55), which is solved by\nalternating optimization.\nWe next showcase a special case of ADMM applied for solving\nregularized objectives, such as that considered in the context of super-\nresolution in Example 3.2, for which (3.57) becomes\nˆs = arg min\ns\n1\n2∥x −Hs∥2 + ϕ(s).\n(3.58)\n3.3. ITERATIVE OPTIMIZERS\n33\nHowever, instead of solving (3.58), ADMM decomposes the objective\nusing variable splitting, formulating the objective as\nˆs = arg min\ns\nmin\nv\n1\n2∥x −Hs∥2 + ϕ(v) + λ∥s −v∥2,\n(3.59)\ns.t.\nv = s.\nThe solution to (3.59) clearly coincides with that of (3.58). ADMM\ntackles the regularzied objective in (3.59) by formulating a dual problem\nas in (3.55), as detailed in the following example.\nExample 3.8 (ADMM). Consider the constrained optimization prob-\nlem in (3.59). Since the formulation has only equality constraints, the\nLagrangian is\nL(s, v, µ) = 1\n2∥x −Hs∥2 + ϕ(v) + λ∥s −v∥2 + µT (s −v)\n(a)\n= 1\n2∥x −Hs∥2 + ϕ(v) + λ\n\r\r\r\rs −v + 1\n2λµ\n\r\r\r\r\n2\n−λ\n\r\r\r\r\n1\n2λµ\n\r\r\r\r\n2\n,\n(3.60)\nwhere (a) follows since for each y (where here y = s −v) it holds that\nλ∥y + 1\n2λµ∥2 = λ∥y∥2 + µT y + λ∥1\n2λµ∥2. Thus, by writing u =\n1\n2λµ,\nwe obtain the augmented Lagrangian\nL(s, v, u) = 1\n2∥x −Hs∥2 + ϕ(v) + λ∥s −v + u∥2 −λ∥u∥2,\n(3.61)\nand the dual is\nd(u) = min\ns,v L(s, v, u).\n(3.62)\nAs mentioned above, ADMM solves the optimization problem in an\nalternating fashion. Namely, for each iteration k, it first optimizes s\nto minimize the Lagrangian while keeping v and u fixed, after which\nit optimizes v to minimize the Lagrangian, and then optimizes u to\nmaximize the dual. The resulting update equation for s becomes\ns(k+1) = arg min\ns\nL(s, v(k), u(k))\n= arg min\ns\n1\n2∥x −Hs∥2 + λ∥s −v(k) + u(k)∥2.\n(3.63)\n34\nModel-Based Methods\nTaking the gradient of L(s, v(k), u(k)) with respect to s and comparing\nto zero yields\n−HT (x −Hs(k+1)) + 2λ(s(k+1) −v(k) + u(k)) = 0,\n(3.64)\nresulting in\ns(k+1) = (HT H + 2λI)−1(HT x + 2λ(v(k) −u(k))).\n(3.65)\nThe update equation for v is\nv(k+1) = arg min\nv\nL(s(k+1), v, u(k))\n= arg min\nv\nϕ(v) + λ∥s(k+1) −v + u(k)∥2\n= arg min\nv\n1\n2λϕ(v) + 1\n2∥v −(s(k+1) + u(k))∥2\n= prox 1\n2λ ·ϕ(s(k+1) + u(k)).\n(3.66)\nFinally, the auxiliary variable u, which should maximize the dual func-\ntion, is updated via gradient ascent with step-size\nµ\n2λ, resulting in\nu(k+1) = u(k) + µ\n2λ∇u=u(k)L(s(k+1), v(k+1), u)\n= u(k) + µ\n2λ∇u=u(k)\n\u0010\nλ∥s(k+1) −v(k+1) + u∥2 −λ∥u∥2\u0011\n= u(k) + µ\n\u0010\ns(k+1) −v(k+1)\u0011\n.\n(3.67)\nThe resulting ADMM optimization is summarized as Algorithm 1.\nAlgorithm 1: ADMM for Problem (3.59)\nInitialization: Fix λ, µ > 0. Initialize u(0), v(0)\n1 for k = 0, 1, . . . do\n2\nUpdate s(k+1) via\ns(k+1) ←(HT H + 2λI)−1(HT x + 2λ(v(k) −u(k))).\n3\nUpdate v(k+1) via v(k+1) ←prox 1\n2λ·ϕ(s(k+1) + u(k)).\n4\nUpdate u(k+1) via u(k+1) ←u(k) + µ\n\u0010\ns(k+1) −v(k+1)\u0011\n.\n5 end\nOutput: Estimate s(k).\n3.4. APPROXIMATION AND HEURISTIC ALGORITHMS\n35\nIn Example 3.8, the iterative solver introduces two hyperparameters,\ni.e., θh = [λ, µ], where λ is introduced in converting the objective in\n(3.58) into (3.59), while µ is used in the iterative minimization of (3.59).\n3.4\nApproximation and Heuristic Algorithms\nThe model-based methods discussed in Sections 3.2-3.3 are derived\ndirectly as solvers, which are either explicit or iterative, to a closed-\nform optimization problem. Nonetheless, many important model-based\nmethods, and particularly in signal processing related applications, are\nnot obtained directly as solutions to an optimization problem. When\ntackling an optimization problem which is extremely challenging to\nsolve in a computationally efficient manner (such as NP-hard problems),\none often resorts to computationally inexpensive methods which are\nnot guaranteed to minimize the objective. Common families of such\ntechniques include approximation algorithms, e.g., greedy methods and\ndynamic programming, as well as heuristic approaches [38]. Since signal\nprocessing applications are often applied in real-time on hardware-\nlimited devices, computational efficiency plays a key role in their design,\nand thus such sub-optimal techniques are frequently used.\nThe term approximation and heuristic algorithms encompasses a\nbroad range of diverse methods, whose main common aspect is the fact\nthat they are not directly derived by solving a closed-form optimization\nproblem. As a representative example of such algorithms in signal\nprocessing, we next detail the family of subspace methods for tackling\nthe DoA estimation problem in Example 3.4.\nExample 3.9 (Subspace Methods). Consider the DoA estimation of\nnarrowband sources formulated in Example 3.4 where the number of\nsources d is smaller than the number of array elements N, the sources\nare non-coherent, i.e., Y is diagonal, and the noise is white, namely\nW = σ2\nwI for some σ2\nw > 0. The maximum likelihood estimator detailed\nin (3.21) is typically computationally challenging to implement and\nrelies on the assumption that the signals are temporally uncorrelated. A\npopular alternative are subspace methods, which aim at recovering the\nDoAs s by dividing the covariance of the observations xt into distinct\n36\nModel-Based Methods\nsignal subspace and noise subspace.\nIn particular, under the above model assumptions, the covariance\nmatrix of the observations xt in (3.18) is given by\nE{xtxH\nt } = A(s)Y AH(s) + σ2\nwI.\n(3.68)\nNote that A(s)Y AH(s) is an N × N matrix of rank d < N, and\nthus has N −d eigenvectors corresponding to the zero eigenvalue,\nwhich are also the the eigenvectors corresponding to the N −d least\ndominant eigenvalues of (3.68). Let en be such an eigenvector, i.e.,\neH\nn A(s)Y AH(s)en = 0. Since Y is positive definite, it holds that\nAH(s)en = 0. Consequently, by letting EN be the (N −d) × N matrix\ncomprised of these N −d least dominant eigenvectors, it holds that for\neach entry of s = [s1, . . . , sd]:\naH(si)ENEH\nN a(si) =\n\r\rEH\nN a(si)\n\r\r2 = 0,\ni ∈1, . . . , d.\n(3.69)\nThus, subspace methods recover the DoAs by seeking the steering\nvectors that are orthogonal to the noise subspace EN.\nThe core equality of subspace methods in (3.69) is not derived from\nan optimization problem formulation, but is rather obtained from a\nset of arguments based on the understanding of the structure of the\nconsidered signals, which identifies the ability to decompose the input\ncovariance into orthogonal signal and noise subspaces. This relationship\ngives rise to several classic DoA estimation methods, including the\npopular multiple signal classification (MUSIC) algorithm [39].\nExample 3.10 (MUSIC). The MUSIC algorithm exploits the subspace\nequality (3.69) to identify the DoAs from the empirical estimate of the\ninput covariance. To that aim, one first uses the T snapshots to estimate\nthe covariance in (3.68) as\nCx = 1\nT\nT\nX\nt=1\nxtxH\nt .\n(3.70)\nThen, its eigenvalue decomposition is taken, from which the number of\nsources ˆd is estimated as the number of dominant eigenvalues (e.g., by\nthresholding), while the eigenvectors corresponding to the remaining\n3.4. APPROXIMATION AND HEURISTIC ALGORITHMS\n37\nN −ˆd eigenvalues are used to form the estimated noise subspace matrix\nˆEN.\nThe estimated noise subspace matrix ˆEN is used to compute the\nspatial spectrum, given by\nP(ψ) =\n1\naH(ψ) ˆEN ˆE\nH\nN aH(ψ)\n.\n(3.71)\nThe ˆd dominant peaks of P(ψ) are set as the estimated DoA angles ˆs.\nAn alternative subspace method is RootMUSIC [40].\nExample 3.11 (RootMUSIC). RootMUSIC recovers the number of\nsources ˆd and the estimated noise subspace matrix ˆEN in the same\nmanner as MUSIC. However, instead of computing the spatial spec-\ntrum via (3.71), it recovers the DoAs from the roots of a polynomial\nformulation representing (3.69).\nIn particular, RootMUSIC formulates the Hermitian matrix E =\nˆEN ˆE\nH\nN using its diagonal sum coefficients en via\nen =\nN−1−n\nX\ni=0\n[E]i,n+i,\nn ≥0,\n(3.72)\nwhere for n < 0, we set en = e∗\n|n|. This allows to approximate (3.69) as\na polynomial equation of order 2N −2 :\nD(z) =\nN−1\nX\ni=0\nN−1\nX\nj=0\n[a(ψ)]∗\ni [E]ij[a(ψ)]j\n=\nN−1\nX\ni=0\nN−1\nX\nj=0\n[E]ijzi−j =\nN−1\nX\nn=−(N−1)\nenzn,\n(3.73)\nwhere z = e−jπ sin(ψ). RootMUSIC identifies the DoAs from the roots of\nthe polynomial (3.73) and the roots map is viewed as the RootMUSIC\nspectrum. Since (3.73) has 2N −2 > d roots (divided into symmetric\npairs), while the roots corresponding to DoAs should have unit mag-\nnitude, the ˆd pairs of roots which are the closest to the unit circle are\nmatched as the ˆd sources DoAs [40].\n38\nModel-Based Methods\nExamples 3.10-3.11 showcase signal processing algorithms that are\nderived from revealing structures in the data that are identified based\non domain knowledge and imposed assumptions, yet are not obtained\nby directly tackling their corresponding objective function. In fact,\nthese DoA estimation algorithms do not rely on explicit knowledge of\nthe objective parameters θo in (3.20), but only on structures imposed\non these parameters, i.e., that of non-coherent sources. Furthermore,\nthey can operate with different number of snapshots T, where larger\nvalues of T allow to better estimate the covariance via (3.70). In ad-\ndition to estimating the desired s, the operation of the algorithms in\nExamples 3.10-3.11 provides a visual interpretable representation of\ntheir decision via the MUSIC spectrum in (3.71) and the root map of\nRootMUSIC.\nAn additional aspect showcased in the above examples is the de-\npendency on reliable domain knowledge and faithful hardware repre-\nsentation. In particular, the ability to model the DoA estimation setup\nvia (3.18) relies on the assumption that the sources are narrowband;\nthe fact that the signal can be decomposed into signal and noise sub-\nspace depends on the assumption that the sources are non-coherent;\nthe orthogonality of the steering vectors and the ability to compute\nthem for each candidate angle holds when one possesses an array which\nis calibrated and its elements are indeed uniformly spaced with half-\nwavelength spacing. This set of structural assumptions is necessary for\none to successfully recover DoAs using subspace methods.\nTo showcase the operation of subspace methods and their dependence\non the aforementioned assumptions, we evaluate both MUSIC and\nRootMUSIC for recovering d = 3 sources from T = 100 snapshots taken\nby an array with M = 8 half-wavelength spaced elements. When the\nsources are non-coherent (generated in an i.i.d. fashion, i.e., Y = I),\nthe MUSIC spectrum (Fig. 3.3a) exhibits clear peaks in the angles\ncorresponding to the DoAs, while RootMUSIC spectrum has roots lying\non the unit circle on these angles (Fig. 3.3b). However, when the sources\nare coherent (which we simulate using the same waveforms, such that\nY is singular), the resulting spectrum and root maps in Figs. 3.3c-3.3d\nno longer represent the true DoAs.\n3.4. APPROXIMATION AND HEURISTIC ALGORITHMS\n39\n75\n50\n25\n0\n25\n50\n75\nAngels [deg]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAmplitude\n(a) MUSIC spectrum, non-coherent sources.\n0°\n45°\n90°\n135°\n180°\n225°\n270°\n315°\nAngels [deg]\n0.5\n1.0\n1.5\n2.0\n(b) RootMUSIC spectrum, non-coherent\nsources.\n75\n50\n25\n0\n25\n50\n75\nAngels [deg]\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAmplitude\n(c) MUSIC spectrum, coherent sources.\n0°\n45°\n90°\n135°\n180°\n225°\n270°\n315°\nAngels [deg]\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n(d) RootMUSIC spectrum, coherent sources.\nFigure 3.3: The spectrum obtained by MUSIC and RootMUSIC for recovering\nd = 3 non-coherent (upper figures) and coherent (lower figures) sources located at\nangles −22◦, 12◦, 50◦.\n40\nModel-Based Methods\n3.5. SUMMARY\n41\n3.5\nSummary\n• The traditional model-based approach to design inference\nrules imposes statistical or structural models on the under-\nlying signals. The model is often a surrogate approximation\nadopted for simplicity and tractability.\n• Combining the loss measure and the model yields a risk\nfunction, which can be used to formulate an objective which\nguides the inference procedure.\n• The objective function is typically parameterized, where the\nparameters often arise from the imposed model. Different\nsetting of the objective parameters affect the optimization\nformulation, representing different tasks.\n• Once an objective is determined, it can lead to different\ninference mappings based on different approaches:\n– In some cases, the optimization problem can be ex-\nplicitly solved, and the decision rule is set as the\ncorresponding solution.\n– Often in practice, the optimization is tackled using\niterative optimizers, and thus the inference rule is an\niterative procedure, introducing additional parameters\nof the solver, referred to as hyperparameters.\n– Alternatively, one can utilize algorithms that are not\nnecessarily derived from the formulated problem, but\nrather based on exploiting structures in the models\nusing approximation and heuristic algorithms.\n• Model-based approaches are highly sensitive to the validity\nof the model and the accuracy of its parameters. Even when\nthe model is fully known, the hyperparameters of the solver\nand objective parameters that are often tuned by hand can\nhave a notable impact on performance and runtime.\n4\nDeep Learning\nSo far, we have discussed model-based decision making and optimization.\nThese approaches rely on a formulation of the system objective which\ntypically was obtained based on knowledge and approximations. While\nin many applications coming up with accurate and tractable models is\ndifficult, we are often given access to data describing the setup. In this\nchapter, we start discussing ML, and particularly deep learning, where\ndecision making is carried out using DNNs whose operation is learned\nalmost completely from data.\n4.1\nEmpirical Risk\nML systems learn their mapping from data. In a supervised setting, data\nis comprised of a training set consisting of nt pairs of inputs and their\ncorresponding labels, denoted D = {xi, si}nt\ni=1. This data is referred to\nas the training set. Since no mathematical model relating the input\nand the desired decision is imposed, the objective used for setting the\ndecision rule f(·) as in (2.1) is the empirical risk, given by\nLD(f) ≜1\nnt\nnt\nX\ni=1\nl(f, xi, si).\n(4.1)\n42\n4.1. EMPIRICAL RISK\n43\nFigure 4.1: Illustration of overfitting versus underfitting for different structures\nimposed on the domain of feasible mappings.\nWhile we focus our description in the sequel on supervised settings, ML\nsystems can also learn in an unsupervised manner. In such cases, the\ndata set D is comprised only of a set of examples {xi}nt\ni=1 ⊂X, and\nthe loss measure l is defined over F × X, instead of over F × X × S.\nSince there is no label to predict, unsupervised ML algorithms are often\nused to discover interesting patterns present in the given data. Common\ntasks in this setting include clustering, anomaly detection, generative\nmodeling, and compression.\nThe empirical risk in (4.1) does not require any assumptions to be\nimposed on the relationship between the context x and the desired\ndecision s, and it thus allows to judge decisions solely based on their\noutcome and their ability to match the available data. As opposed\nto the model-based case, where decision mappings can sometimes be\nderived by directly solving the optimization problem arising from the\nrisk formulation without initially imposing structure on the system,\nsetting a decision rule based on (4.1) necessitates restricting the domain\nof feasible mappings, also known as inductive bias. This stems from\nthe fact that one can usually form a decision rule which minimizes\nthe empirical loss of (4.1) by memorizing the data, i.e., overfit [30, Ch.\n2]. The selection of the inductive bias is crucial to the generalization\ncapabilities of the ML model. As illustrated in Fig. 4.1, allowing the\nsystem to implement arbitrary mappings can lead to overfitting, while\nrestricting the feasible mappings to ones which may not necessarily suit\nthe task is likely to result in underfitting.\n44\nDeep Learning\nFigure 4.2: Neuron illustration, where the lines represent the weights w and the\nadditive bias term b while the node represents the non-linear activation σ(·).\n4.2\nNeural Networks\nA leading strategy in ML, upon which deep learning is based, is to\nassume a highly-expressive generic parametric model on the decision\nmapping, while incorporating optimization mechanisms and regularizing\nthe empirical risk to avoid overfitting. In such cases, the decision box\nf is dictated by a set of parameters denoted θ, and thus the system\nmapping is written as fθ. In deep learning, fθ is a DNN, with θ being\nthe network parameters. Such highly-parametrized abstract systems\ncan effectively approximate any Borel measurable mapping, as follows\nfrom the universal approximation theorem [41, Ch. 6.4.1].\nWe next recall the basic formulation of DNNs, as well as some\ncommon architectures.\n4.2.1\nBasics of Neural Networks\nArtificial Neurons\nThe most basic building block of a neural network\nis the artificial neuron (or merely neuron). It is a mapping hθ : RN 7→R\nwhich takes the form\nhθ(x) = σ(wT x + b),\nθ = {w, b}.\n(4.2)\nThe neuron in (4.2), also illustrated in Fig. 4.2, is comprised of a\nparametric affine mapping (dictated by θ) followed by some non-linear\nelement-wise function σ : R 7→R referred to as an activation.\n4.2. NEURAL NETWORKS\n45\nStacking multiple neurons in parallel yields a layer. A layer with M\nneurons can be written as hθ : RN 7→RM and its mapping is given by\nhθ(x) = σ(W x + b),\nθ = {W , b},\n(4.3)\nwhere the activation σ(·) is applied element-wise.\nActivations\nActivation functions are often fixed, i.e., their mapping\nis not parametric. Some notable examples of widely-used activation\nfunctions include:\nExample 4.1 (ReLU). The rectified linear unit (ReLU) is an extremely\ncommon activation function, given by:\nσ(x) = max{x, 0}.\n(4.4)\nExample 4.2 (Leaky ReLU). A variation of the ReLU activation includes\nan additional parameter α < 1, typically set to 10−2, and is given by:\nσ(x) = max{x, αx}.\n(4.5)\nExample 4.3 (Sigmoid). Another common activation is the sigmoid,\nwhich is given by:\nσ(x) = (1 + exp(−x))−1.\n(4.6)\nThe importance of using activations stems from their ability to allow\nDNNs to realize non-affine mappings.\nMulti-Layered Perceptron\nWhile the layer mapping in (4.3) may be\nlimited in its ability to capture complex mappings, one can stack multiple\nlayers to obtain a more flexible family of parameterized mappings. Such\ncompositions are referred to as multi-layered perceptrons. Specifically,\na DNN fθ consisting of K layers {h1, . . . , hK} maps the input x to\nthe output ˆs = fθ(x) = hK ◦· · · ◦h1(x), where ◦denotes function\ncomposition. An illustration of a DNN with k = 3 layers (two hidden\nlayers and one output layer) is illustrated in Fig. 4.3. Since each layer\nhk is itself a parametric function, the parameters of the entire network\nfθ are the union of all of its layers’ parameters, and thus fθ denotes a\nDNN with parameters θ. In particular, by letting W k, bk denote the\n46\nDeep Learning\nFigure 4.3: Mutli-layered perceptron illustration with K = 3 layers. The lines\nrepresent the affine transformation (the weights and the additive bias terms in each\nlayer as in (4.7)); the green nodes represent the non-linear activations σ(·), while\nthe yellow nodes are the output layer.\nconfigurable parameters of the kth layer hk(·), the trainable parameters\nof the DNN are written as\nθ = {W k, bk}K\nk=1.\n(4.7)\nThe architecture of a DNN refers to the specification of its layers\n{hk}K\nk=1.\nOutput Layers\nThe choice of the output layer hK(·) is tightly coupled\nwith the task of the network, and more specifically, with the loss function\nL. In particular, the output layer dictates the possible outputs the\nparameteric mapping can realize. The following are commonly used\noutput layers based on the system task:\n• Regression tasks involve the estimation of a continuous-amplitude\nvector, e.g., S = Rd. In this case, the output must be allowed to\ntake any value in Rd, and thus a common output layer is a linear\nunit of width d, i.e.,\nhk(z) = W Kz + bK,\n(4.8)\nwhere the number of rows of W K and dimension of bK is set to\nd.\n• Detection is a binary form of classification, i.e., |S| = 2. In\nclassification tasks, one is typically interested in soft outputs, e.g.,\n4.2. NEURAL NETWORKS\n47\nPr(s|x), in which case the output is a probability vector over S.\nAs S is binary, a single output taking values in [0, 1], representing\nPr(s = s1|x), is sufficient. Thus, the typical output layer is a\nsigmoid unit as given in (4.6), and the output layer is given by\nhk(z) = σ(wT\nk z + bk).\n(4.9)\n• Classification in general allows any finite number of different\nlabels, i.e. |S| = d, where d is a positive integer not smaller than\ntwo. Here, to guarantee that the output is a probability vector\nover S, classifiers typically employ the softmax function (e.g. on\ntop of the output layer), given by:\nSoftmax(z) =\n\"\nexp(z1)\nPd\ni=1 exp(zi)\n, . . . ,\nexp(zd)\nPd\ni=1 exp(zi)\n#\n.\nThe resulting output layer is given by\nhK(z) = Softmax(W Kz + bK),\n(4.10)\nwhere the number of rows of W K and dimension of bK is set to d.\nDue to the exponentiation followed by normalization, the output\nof the softmax function is guaranteed to be a valid probability\nvector.\n4.2.2\nCommon Architectures\nUnlike model-based algorithms, which are specifically tailored to a given\nscenario, deep learning is model-agnostic. The unique characteristics of\nthe specific scenario are encapsulated in the weights that are learned\nfrom data. The parametrized inference rule, e.g., the DNN mapping, is\ngeneric and can be applied to a broad range of different problems. In\nparticular, the multi-layered perception can combine any of its input\nsamples in a parametric manner, and thus makes no assumption on the\nexistence of some underlying structure in the data.\nWhile standard DNN structures are model-agnostic and are com-\nmonly treated as black boxes, one can still incorporate some level\n48\nDeep Learning\nof domain knowledge in the selection of the specific network archi-\ntecture. We next review two common families of structured architec-\ntures: convolutional neural networks (CNNs) and recurrent neural net-\nworks (RNNs).\nConvolutional Neural Networks\nConvolutional layers are a special\ncase of (4.3). However, they are specifically tailored to preserve the\ntensor representation of certain data types, e.g., images, while utilizing\na reduced number of trainable parameters. The latter is achieved by\nconnecting each neuron to only a subset of the input variables (e.g.,\npixels) and by reusing weights between different neurons of the same\nlayer. The trainable parameters of convolutional layers are essentially a\nset of filters (typically two-dimensional), which slide over the spatial\ndimensions of the layer input.\nConvolution operators are commonly studied in the signal processing\nliterature in the context of time sequences, where one-dimensional\nfilters are employed. In the case of causal finite response filtering, a\none-dimensional convolution with kernel K[i] of size F applied to a\ntime sequence Z1[n], yields a signal\nZ2[n] =\nF−1\nX\ni=0\nZ1[n −i]K[i].\n(4.11)\nThe operation in (4.11) specializes the linear mapping of the perceptron\nby restricting the weights matrix to be Toeplitz.\nWhile (4.11) corresponds to the traditional signal processing for-\nmulation of convolutions, and constitutes the basis of one-dimensional\nconvolutional layers used in deep learning, the more common form of\nconvolutional layers generalizes (4.11) and considers tensor data, e.g.,\nimages, without reshaping them into vectors. To formulate the operation\nof such convolutional layers, consider a layer with an input tensor Z1 of\ndimensions H1 × W1 × D1 (height × width × depth). The convolution\nlayer is comprised of the following aspects:\n• Convolution kernel - the linear operation is carried out by a\nsliding kernel K of dimensions F × F × D1 × D2 (spatial extent\n*squared* × input depth × output depth).\n4.2. NEURAL NETWORKS\n49\n• Bias - as the transformation implemented is affine rather than\nlinear, and thus a bias term is added. These biases are typically\nshared among all kernels mapped to a given output channel, and\nthus the trainable parameter b is usually a D2 × 1 vector.\n• Zero padding - since the input is of a finite spatial dimension\n(i.e., H1 and W1), convolving an H1 × W1 image with an F × F\nkernel yields an (H1 −F + 1) × (W1 −F + 1) image. The natural\napproach to extend the dimensions is thus to zero-pad, i.e., add\nP zeros around the image, resulting in the output being an (H1 −\nF + 2P + 1) × (W1 −F + 2P + 1) image.\n• Stride - in order to reduce the dimensionality of the output image,\none can determine how the filter slides along the image, using\nthe stride hyperparameter S. For S = 1, the filter moves along\nthe image pixel-by-pixel. Increasing S implies that the filter skips\nS −1 pixels as it slides. Consequently, the output image is of\ndimensions (H1−F+2P\nS\n+ 1) × (W1−F+2P\nS\n+ 1).\nTo summarize, the output tensor Z2 is of size H2(= H1−F+2P\nS\n+1)×W2(=\nW1−F+2P\nS\n+ 1) × D2, and its entries are computed as\nZ2[n, m, l] =\nF−1\nX\ni=0\nF−1\nX\nj=0\nD1−1\nX\nd=0\nZ1[n · S + i −P, m · S + j −P, d]\n· K[i, j, d, l] + b[l],\n(4.12)\nwhere Z1[n, m, d] is set to zero for n /∈[0, H1 −1] and/or n /∈[0, W1 −1].\nRecurrent Neural Networks\nWhile CNNs can efficiently process spa-\ntial information in structured data such as images by combining neigh-\nboring features (e.g., pixels), RNNs are designed to handle sequential\ninformation, i.e., time sequences. Time sequences comprise of a sequen-\ntial order of samples. In this case, our data is a sequence of T samples,\ndenoted {xt}T\nt=1. The task is to map the inputs into a label sequence\n{st}T\nt=1. The sequential structure of the data implies that the inference\nof st should not be made based on xt solely, but should also depend on\npast inputs {xj}t−1\nj=1.\n50\nDeep Learning\nFigure 4.4: recurrent neural network illustration: (a) generic architecture; (b) as a\nneural layer. The symbol Z−1 represents a unit time delay.\nRecurrent parametric models maintain an internal state vector,\ndenoted ht, representing the memory of the system. Now, the parametric\nmapping is given by:\nˆst = fθ(xt, ht−1),\n(4.13)\nwhere the internal state also evolves via a learned parametric mapping\nht = gθ(xt, ht−1).\n(4.14)\nAn illustration of such a generic RNN is depicted in Fig. 4.4. The\nvanilla implementation of an RNN is the single hidden layer model\nillustrated in Fig 4.4b, in which xt and the latent ht−1 are first mapped\ninto an updated hidden variable ht using a fully-connected layer, after\nwhich ht is used to generate the instantaneous output ˆst using another\nfully-connected output layer. In this case, we can write\nht = gθ1(xt, ht−1);\nˆst = fθ2(ht);\nθ = [θ1, θ2].\n(4.15)\n4.3\nTraining\nGenerally speaking, ML methods and deep learning specifically operate\nwithout knowledge of a model relating the context and the desired\ndecision. They rely on data in order to tune the parameters of the\nmapping, i.e., the network parameters θ, to properly match the data.\nThis is where optimization traditionally comes into play in the context\nof ML. While in the previous chapter we discussed how optimization\ntechniques are used for decision making, in deep learning they are\n4.3. TRAINING\n51\ntypically applied to tune the parameters of the mapping. This procedure,\nwhere a data set D is used to determine the parameters θ, is referred to\nas training, and is the conventional role of optimization in deep learning.\n4.3.1\nTraining Formulation\nAs discussed in Section 4.1, ML systems learn their mapping from data.\nFor a loss function l(·), a parametric model fθ(·), and labeled data set\nD, we wish to find θ which minimizes the empirical risk of (4.1), which\nwe henceforth write as LD(θ) ≜LD(fθ). Often, a regularizing term is\nadded to reduce overfitting by imposing constraints on the values which\nthose weights can take. This is achieved by adding to the empirical\nrisk LD(θ) (that depends on the training set D) a regularization term\nΦ(θ) which depends solely on the parameter vector θ. The resulting\nregularized loss is thus\nLΦ(θ) = LD(θ) + λΦ(θ),\n(4.16)\nwhere λ > 0 is a hyperparameter representing the regularization\nstrength, thus balancing the contribution of the summed terms. Conse-\nquently, training can be viewed as an optimization problem\nθ∗= arg min\nθ\nLΦ(θ).\n(4.17)\nThe optimization problem in (4.17) is generally non-convex, indi-\ncating that actually finding θ∗is often infeasible. For this reason, we\nare usually just interested in finding a \"good\" set of the weights, and\nnot the optimal one, i.e., finding a \"good\" local minima of LΦ. Since\nneural networks are differentiable with respect to their weights and\ninputs (being comprised of a sequence of affine transformations and\ndifferentiable activations), a reasonable approach (and a widely adopted\none) to search for such local minima is via first-order methods. Such\nmethods involve the computation of the gradient\n∇θLΦ(θ) = ∇θLD(θ) + λ∇θΦ(θ)\n=\n1\n|D|\nX\n{xt,st}∈D\n∇θl(fθ, xt, st) + λ∇θΦ(θ).\n(4.18)\n52\nDeep Learning\nThe main challenges associated with computing the gradients stem\nfrom the first term in (4.18), and particularly:\n1. The data set D is typically very large, indicating that the sum-\nmation in (4.18) involves a massive amount of computations of\n∇θl(·) at all different training samples.\n2. Neural networks are given by a complex function of their parame-\nters θ, indicating that computing each ∇θl(·) may be challenging.\nDeep learning employs two key mechanism to overcome these challenges:\nthe difficulty in computing the gradients is mitigated due to the se-\nquential operation of neural networks via backporpagation, detailed\nin the following. The challenges associated with the data size are han-\ndled by replacing the gradients with a stochastic estimate, detailed in\nSubsection 4.3.3.\n4.3.2\nGradient Computation\nOne of the main challenges in optimizing complex highly-parameterized\nmodels using gradient-based methods stems from the difficulty in com-\nputing the empirical risk gradient with respect to each parameter, i.e.,\n∇θl(fθ, xi, si) in (4.25). In principle, the parameters comprising the\nentries of θ may be highly coupled, making the computation of the\ngradient a difficult task. Nonetheless, neural networks are not arbitrary\ncomplex models, but have a sequential structure comprised of a con-\ncatenation of layers, where each trainable parameter typically belongs\nto a single layer. This sequential structure facilitates the computation\nof the gradients using the backpropagation process.\nBackpropagation\nThe backpropagation method proposed in [42] is\nbased on the calculus chain rule. Suppose that one is given two multi-\nvariate functions such that y = g(x) and l(x) = f(y) = f(g(x)), where\nf : Rn 7→R and g : Rm 7→Rn. By the chain rule, it holds that\n∂l\n∂xi\n=\nn\nX\nj=1\n∂l\n∂yj\n∂yj\n∂xi\n.\n(4.19)\n4.3. TRAINING\n53\nThe formulation of the gradient computation via the chain rule is\nexploited to compute the gradients of the empirical risk of a multi-\nlayered neural network with respect to its weights in a recursive manner.\nTo see this, consider a neural network with K layers, given by\nfθ(x) = hK ◦· · · ◦h1(x),\n(4.20)\nwhere each layer hk(·) is comprised of a (non-parameterized) activation\nfunction σk(·) applied to an affine transformation with parameters θk =\n(W k, bk). Define ak as the output to the kth layer, i.e., ak = hk(ak−1),\nand zk as the affine transformation such that\nzk = W kak−1 + bk,\nak = σk(zk).\nNow, the empirical risk l = l(fθ, x, s) is a function of fθ(x) = hK ◦· · · ◦\nhk+1(σk(zk)). Consequently, we use the matrix version of the chain rule\nto obtain\n∇ak−1l = W T\nk\n\u0000∇zkl\n\u0001,\n(4.21a)\n∇W kl =\n\u0000∇zkl\n\u0001aT\nk−1,\n(4.21b)\n∇bkl = ∇zkl.\n(4.21c)\nFurthermore, since ak = σk(zk) it holds that\n∇zkL = ∇akl ⊙σ′\nk(zk),\n(4.21d)\nwhere ⊙denotes the element-wise product, and σ′\nk(·) is the element-wise\nderivative of the activation function σk(·). Equation (4.21) implies that\nthe gradients of the empirical risk with respect to (W k, bk) can be\nobtained by first evaluating the outputs of each layer, i.e., the vectors\n{zk, ak}, also known as the forward path. Then, the gradients of the loss\nwith respect to each layer’s output can be computed recursively from the\ncorresponding gradients of its subsequent layers via (4.21a) and (4.21d),\nwhile the desired weights gradients are obtained via (4.21b)-(4.21c).\nThis computation starts with the gradient of the loss with respect to\nthe DNN output, i.e., ∇fθ(x)L, which is dictated by the loss function,\ni.e., how L is computed for DNN output fθ(x). Then, this gradient is\nused to recursively update the gradients of the loss with respect to the\nparameters of the layers, going from the last layer (hK) to the first one\n(h1).\n54\nDeep Learning\nBackpropagation Through Time\nBackpropagation via (4.21) builds\nupon the fact that neural networks are comprised of a sequential opera-\ntion, allowing to compute the gradients of the loss with respect to the\nparameters of a given layer using solely the gradient of the loss with\nrespect to its output. This operation has an underlying assumption that\neach layer has its own distinct weights. However, backpropagation can\nalso be applied when weights are shared between layers. Since this is\neffectively the case in RNNs processing time sequences, the resulting\nadaptation of backporpagation is typically referred to as backpropaga-\ntion through time [43]; nonetheless, this procedure can be applied for\nany form of weight sharing between weights regardless of whether the\nnetwork is an RNN or if the data is a time sequence.\nTo see how backpropagation through time operates, let us again\nconsider a neural network with K layers as in (4.20) and use αk to\ndenote the features at the output of the kth layer. However, now there\nis some weight parameter wh that is shared by all layers, i.e., wh is an\nentry of θk for each k ∈{1, . . . , K}. To compute the derivative of the\nempirical risk with respect to wh, we note that\n∂l(fθ, x, s)\n∂wh\n= ∂l(fθ, x, s)\n∂ˆs\n∂ˆs\n\u0000 = fθ(x)\n\u0001\n∂αK\n∂αK\n∂wh\n.\n(4.22)\nSince the weight wh appears both in the Kth layer as well as in its\nsubsequent ones, it holds that\n∂αK\n∂wh\n= ∂hK(αK−1; wh)\n∂wh\n+ ∂hK(αK−1; wh)\n∂αK−1\n∂αK−1\n∂wh\n.\n(4.23)\nWe obtain a recursive equation relating ∂αK\n∂wh and ∂αK−1\n∂wh . If wh would\nhave been only a parameter of the Kth layer, then ∂αK−1\n∂wh\n= 0, and we are\nleft only with the first summand in (4.22). When there is weight sharing,\nwe can compute (4.23) recursively K times, eventually obtaining an\nexpression for the gradients\n∂αK\n∂wh\n= ∂hK(αK−1; wh)\n∂wh\n+\nK−1\nX\ni=1\n\n\nK\nY\nk=i+1\n∂hk(αk−1; wh)\n∂αk−1\n\n∂hi(αi−1; wh)\n∂wh\n.\n(4.24)\nPlugging (4.24) into (4.22) yields the expression for the desired gradi-\nents.\n4.3. TRAINING\n55\n4.3.3\nStochastic Gradients\nThe method referred to as (mini-batch) stochastic gradient descent\ncomputes gradients over a random subset of D, rather than the complete\ndata set. At each iteration index j, a mini-batch comprised of B samples,\ndenoted Dj, is randomly drawn from D, and is used to compute the\ngradient. The gradient estimate is thus given by\n∇θLDj(θ) = 1\nB\nX\n{xi,si}∈Dj\n∇θl(fθ, xi, si).\n(4.25)\nWhen Dj is drawn uniformly from all B-sized subsets of D in an i.i.d.\nfashion, (4.25) is a stochastic unbiased estimate of the true gradient\n∇θLD(θ).\nThe usage of stochastic gradients for gradient descent based opti-\nmization of θ, summarized as Algorithm 2 below, requires B gradient\ncomputations each time the parameters are updated, i.e., on each it-\neration. Since B is typically much smaller then the data set size |D|,\nstochastic gradients reduce the computational burden of training DNNs.\nAlgorithm 2: (Mini-batch) stochastic gradient descent\nInitialization: Fix number of iterations n; step sizes {µj}.\n1 Initialize θ0 randomly;\n2 for j = 0, 1, . . . , n −1 do\n3\nSample B different samples uniformly from D as Dj;\n4\nEstimate gradient as ˆ∇θL(θj) = ∇θLDj(θ) + λ∇θΦ(θj);\n5\nUpdate parameters via\nθj+1 ←θj −µj ˆ∇θL(θj).\n(4.26)\n6 end\nOutput: Trained parameters θn.\nThe term stochastic gradient descent is often used to refer to Algo-\nrithm 2 with mini-batch size B = 1, while for B > 1 it is referred to as\nmini-batch stochastic gradient descent. Each time the training procedure\ngoes over the entire data set, i.e., every ⌈|D|/B⌉iterations, are referred\nto as an epoch.\n56\nDeep Learning\n4.3.4\nUpdate Rules\nThe above techniques allow to compute an estimate of the gradient in\n(4.18) with relatively feasible computational burden. One can now use\nfirst order methods for tuning the DNN parameters θ, while replacing\nthe gradient ∇θLΦ(θ) with its stochastic estimate ˆ∇θL(θ). The most\nstraight-forward first-order optimizer which uses the stochastic gradients\nis stochastic gradient descent, detailed in Algorithm 2.\nThere are a multitude of variants and extensions of (4.26), which\nare known to improve the learning characteristics. Here we review the\nleading approaches which can be divided into momentum updates, which\nintroduce an additional additive term to (4.26), and adaptive learning\nrates, particularly the Adam optimizer [44], where (4.26) is further\nscaled in a different manner for each weight.\nMomentum\nMomentum, as it name suggests, encourages the opti-\nmization path to follow its current direction. As the direction of the\nprevious step is merely the difference θj −θj−1, momentum replaces\nthe vanilla update rule (4.26) with\nθj+1 −θj = −µj ˆ∇θL(θj) + βj (θj −θj−1) .\n(4.27)\nThe parameter βj in (4.27) is referred to as the damping factor, and\nshould take values in (0, 1). The setting of βj balances the contribution\nof the current direction (i.e., the momentum), and the current noisy\nstochastic gradient. In practice, the damping factor is typically set in\nthe range (0.9, 0.99), implying that momentum has a dominant impact\non the optimization path.\nAdam\nAdaptive learning rate methods use a different learning rate\nfor each weight. Here, the update equation (4.26) becomes\nθj+1 −θj = −µjvj ⊙ˆ∇θL(θj),\n(4.28)\nwhere vj is the adaption vector, whose dimensions are equal to those of\nθ.\nArguably the most widely-used adaptive update rule is the Adam\nmethod [44], which scales the gradient of each element with an estimate\n4.3. TRAINING\n57\nof its root-mean-squared value accumulated over the learning iterations.\nHere, weights that receive high gradients will have their effective learning\nrate reduced, while weights that receive small or infrequent updates\nwill have their effective learning rate increased. This is achieved by\nmaintaining an estimate of the (element-wise) sum-squared gradients\nin the vector\n˜vj = α˜vj−1 + (1 −α)\n\u0000 ˆ∇θL(θj)\n\u00012,\n(4.29)\nand a momentum term\nmj = α1mj−1 + (1 −α1) ˆ∇θL(θj).\n(4.30)\nThe resulting adaptation vector is set to\n[vj]i =\n[mj]i\nq\n[˜vj]i + ϵ\n,\n(4.31)\nwith α, α1, ϵ being hyperparameters. The full Adam algorithm proposed\nin [44], also includes a bias correction term not detailed here.\n58\nDeep Learning\n4.4\nSummary\n• ML methods set inference rules without relying on mathe-\nmatical modeling by using data to formulate an empirical\nestimate of the risk function as the design objective. This\nnecessitates the need to impose a parametric model on the\nmapping, which in deep learning is that of DNNs.\n• The generic DNN form is the multi-layered perceptron,\nwhich is a composition of parametric affine functions with\nintermediate non-linear activations, while the task affects\nthe output layer. Such architectures are model-agnostic, and\nthe specificities of the scenario which dictate the mapping\nare encapsulated in the parameters that are learned from\ndata.\n• DNN architectures such as CNNs and RNNs are designed\nfor structured data. Yet, like multi-layered perceptrons, they\nare still invariant of the underlying statistical model, and\nare often viewed as black boxes, where one can confidently\nassign operational meaning only to their input and output.\n• Training, i.e., the tuning of the parameters of DNNs to\nmatch a given data set based on a specified loss measure,\nis typically based on stochastic variations of first-order\noptimizers combined with backpropagation to compute the\ngradients.\n5\nModel-Based Deep Learning\nThe previous chapters focused on model-based optimization and deep\nlearning, which are often viewed as fundamentally different approaches\nfor setting inference rules. Nonetheless, both strategies typically use\nparametric mappings, i.e., the weights θ of DNNs and the parameters\n[θo, θh] of model-based methods, whose setting is determined based on\ndata and on knowledge of principled mathematical models. The core\ndifference thus lies in the specificity and the parameterization of the\ninference rule type: Model-based methods are knowledge-centric, e.g.,\nrely on a characterization of an underlying model. This knowledge allows\nmodel-based methods to employ inference mappings that are highly task-\nspecific, and usually involve a limited amount of parameters that one\ncan often set manually. Deep learning is data-centric, operating without\nspecifying a statistical model on the data, and thus uses model-agnostic\ntask-generic mappings that tend to be highly parametrized.\nThe identification of model-based methods and deep learning as two\nends of a spectrum of specificity and parameterization indicates the\npresence of a continuum, as illustrated in Fig. 5.1. In fact, many tech-\nniques lie in the middle ground, designing decision rules with different\nlevels of specificity and parameterization by combining some balance\n59\n60\nModel-Based Deep Learning\nof deep learning with model-based optimization [12], [45]–[47]. This\nchapter is dedicated to exploring methodologies residing in this middle\nground.\nThe learning of inference rules from data relies on three fundamental\npillars: an architecture, dictating the family of mappings one can set;\ndata which (in a supervised setting) comprises of pairs of inputs and\ntheir desired outputs; and a learning algorithm which uses the data to\ntune the architecture. The core of model-based deep learning as the\nfamily of middle ground methodologies between model-based inference\nand deep learning lies mostly in its revisiting of what type of inference\nrule is learned, namely, the architecture, and also how data is leveraged,\ni.e., the learning algorithm.\nSpecifically, model-based deep learning strategies are most suitable\nin the following settings:\n• Model deficiency, where one has only some limited level of do-\nmain knowledge, e.g., a partial characterization of the underlying\ndistribution P, as well as data.\n• Algorithm deficiency, where the task may be fully characterized\nmathematically, yet there is no efficient algorithm for tackling it.\nIn the context of the learning algorithm, the model deficient setting\ngives rise to a fundamental question: How to leverage the data to cope\nwith the fact that the available domain knowledge is partial? One can\nenvision two approaches – a model-centric approach which uses the\ndata to fit the missing domain knowledge, and a task-centric approach\nthat aims to directly learn the inference mapping without estimating\nthe missing model parameters. We thus commence this chapter by\ndiscussing what it is that must be learned in such cases, relating these\napproaches to the notions of generative and discriminative learning [28],\n[29] in Section 5.1.\nThe main bulk of the chapter is then dedicated to the architecture\npillar, whose design via model-based deep learning can tackle both\nmodel and algorithm deficiency. We categorize systematic frameworks\nfor designing decision mappings that are both knowledge and data-\ncentric as a form of hybrid model-based deep learning optimization:\n61\nFigure 5.1: Continuous spectrum of specificity and parameterization with model-based methods and deep learning constituting\nthe extreme edges of the spectrum.\n62\nModel-Based Deep Learning\nThe first strategy, coined learned optimizers [48] (Section 5.2), uses deep\nlearning automated tuning machinery to tune parameters of model-\nbased optimization conventionally tuned by hand. The second family\nof techniques, referred to as deep unfolding [12] (Section 5.3), converts\niterative model-based optimizers with a fixed number of iterations\ninto a DNN. We conclude with methods which augment model-based\noptimization with dedicated deep learning tools as a form of DNN-aided\noptimization. These include the replacement of a complex prior (coined\nDNN-aided priors, see Section 5.4), and the incorporation of DNNs for\ncarrying out internal computations (referred to as DNN-aided inference,\nsee Section 5.5) [46]. An illustration of this division is depicted in\nFig. 5.1.\n5.1\nLearning to Cope with Model Deficiency\nAs discussed above, model deficiency refers to settings where there\nis both a (possibly limited) data set D and partial characterization\nof the model P. Broadly speaking and following the terminology of\n[28], one can consider two main approaches for designing inference\nrules in such settings, where one has to cope with the fact that the\navailable domain knowledge is incomplete. These approaches arise from\nthe notions of generative learning and discriminative learning. We next\nrecall these ML paradigms in the context of data-driven inference with\npartial domain knowledge, after which we exemplify the differences and\ninterplay between these approaches using a dedicated example where\nboth learning paradigms are amenable to tractable analysis.\nGenerative Learning\nThe generative approach aims at designing the\ndecision rule f to minimize the generalization error LP(f) in (3.1),\ni.e., as in model-based inference. The available data D is thus used\nto estimate the data generating distribution P. Given the estimated\ndistribution, denoted by ˆPD, one then seeks the inference rule which\nminimizes the risk function with respect to ˆPD, i.e.,\nf∗= arg min\nf∈F\nL ˆPD(f).\n(5.1)\n5.1. LEARNING TO COPE WITH MODEL DEFICIENCY\n63\nDiscriminative Learning\nThe discriminative approach bypasses the\nneed to fit the underlying distribution, and uses data to directly tune\nthe inference rule based on the empirical risk LD(f) in (4.1). As such,\ndiscriminative learning encompasses conventional supervised training\nof DNNs discussed in Chapter 4. However, it can also accommodate\npartial domain knowledge as a form of model-based discriminative\nlearning. In particular, while the formulation in Chapter 4 is model-\nagnostic and allows the inference rule to take a broad range of mappings\nparametrized as abstract neural networks, model-based discriminative\nlearning leverages the available domain knowledge to determine what\nstructure the inference rule takes. Namely, the feasible set of inference\nrules is now constrained to a pre-determined parameterized structure,\ndenoted FP, that is known to be suitable for the problem at hand, and\nthe learning task becomes finding\nθ∗= arg min\nfθ∈FP\nLD(fθ).\n(5.2)\nThe additional constraint in (5.2) boosts the specificity and reduces the\nparameterization of the inference mapping compared with black-box\nmodel agnostic end-to-end learning.\nTo exemplify the differences and interplay between the generative\nand discriminative approach for hybrid model-based/data-driven in-\nference in an analytically tractable manner, we present the following\nexample, taken from [49], and illustrated in Fig. 5.2.\nExample 5.1. We consider an estimation task with the ℓ2 loss given\nin (2.6), where the risk function becomes the MSE as in (3.6). In this\nsetting, one has prior knowledge that the target s admits a Gaussian\ndistribution with mean µs and covariance Css, i.e. s ∼N(µs, Css),\nand that the measurements follow a linear model\nx = Hs + w,\nw ∼N(µ, σ2I).\n(5.3)\nIn (5.3), w is a Gaussian noise with mean µ and covariance matrix σ2I,\nand is assumed to be independent of s. Consequently, the observed x\nand the desired s obey a jointly Gaussian distribution P. The available\ndomain knowledge is partial in the sense that the parameters H and\n64\nModel-Based Deep Learning\nFigure 5.2: Illustration of the linear estimation of Example 5.1 obtained via\ngenerative learning (a) and discriminative learning (b).\nµ are unknown. Nonetheless, we are given access to a data set D =\n{xi, si}nt\ni=1 comprised of i.i.d. samples drawn from P.\nTo formulate the generative and discriminative estimators for the\nconsidered linear Gaussian setting, we define the sample first-order and\nsecond-order moments of (x, s) computed from D as\n¯x = 1\nnt\nnt\nX\ni=1\nxi,\n¯s = 1\nnt\nnt\nX\ni=1\nsi,\nˆCsx = 1\nnt\nnt\nX\ni=1\n(si −¯s)(xi −¯x)T ,\nˆCss = 1\nnt\nnt\nX\ni=1\n(si −¯s)(si −¯s)T ,\nˆCxx = 1\nnt\nnt\nX\ni=1\n(xi −¯x)(xi −¯x)T .\nThe generative approach uses data to estimate the missing domain\nknowledge parameters, i.e., it uses D to estimate the matrix H and the\nnoise mean µ. Since these parameters are considered to be deterministic\nand unknown, they are fitted to the data using the maximum likelihood\nrule. Letting P(x, s; H, µ) be the joint distribution of x and s for given\nH and µ, the log-likelihood can be written as\nlog P(xi, si; H, µ) = log P(xi|si; H, µ) + log P(si)\n= const −1\nσ2 ∥xi −Hsi −µ∥2\n2.\n(5.4)\n5.1. LEARNING TO COPE WITH MODEL DEFICIENCY\n65\nIn (5.4), const denotes a constant term, which is not a function of the\nunknown parameters H and µ. Since D is comprised of i.i.d. samples\ndrawn from the jointly Gaussian generative distribution P, the maximum\nlikelihood estimates are obtained from (5.4) as\nˆH, ˆµ = arg max\nH,µ\nnt\nX\ni=1\n∥xi −Hsi −µ∥2\n2.\n(5.5)\nThe solutions to (5.5) are given by [50, Section 3.3]\nˆµ = ¯x −ˆH¯s,\nˆH = ˆCxs ˆC\n−1\nss ,\n(5.6)\nwhere ˆCxs = ˆC\nT\nsx.\nThe estimates ˆH and ˆµ are then used to form the estimator in (5.1),\nas illustrated in Fig. 5.2(a). The estimated ˆPD is obtained from the\nlinear model\nx = ˆHs + ˜w,\n˜w ∼N(ˆµ, σ2I),\n(5.7)\nwhere we recall that s ∼N(µs, Css). Since the estimated distribution,\nˆPD, is a jointly Gaussian distribution, the solution of (5.1) is given by\nthe conditional expectation (3.6), i.e.,\nˆsg = f∗(x) = E(x,s)∼ˆPD{s|x}\n(a)\n= µs+Css ˆH\nT ( ˆHCss ˆH\nT +σ2I)−1(x−¯x−ˆH(µs−¯s))\n(b)\n= µs+( ˆH\nT ˆH+σ2C−1\nss )−1 ˆH\nT (x−¯x−ˆH(µs−¯s)).\n(5.8)\nHere, (a) follows from the estimated jointly Gaussian model (5.7),\nand (b) is obtained using the matrix inversion lemma (assuming that\nˆH\nT ˆH+σ2C−1\nss is invertible).\nThe discriminative approach leverages the partial domain knowl-\nedge regarding the underlying joint Gaussianity to set the structure of\nthe estimator, namely, that it should take a linear form. This implies\nthat FP is the set of parameterized mappings of the form\nfθ(x) = Ax + b,\nθ = {A, b}.\n(5.9)\nFor the considered parametric model, the available data is used to\ndirectly identify the parameters which minimize the empirical risk, as\n66\nModel-Based Deep Learning\nillustrated in Fig. 5.2(b), i.e.,\nθ∗= arg min\nfθ∈FP\n1\nnt\nnt\nX\ni=1\n∥si −fθ(xi)∥2\n2\n= arg min\nA,b\n1\nnt\nnt\nX\ni=1\n∥si −Axi −b∥2\n2,\n(5.10)\nwhich is solved by b∗= ¯s −A∗¯x with A∗= ˆCsx ˆC\n−1\nxx. The resulting\ndiscriminative learned estimator is given by\nˆsd = fθ∗(x) = ˆCsx ˆC\n−1\nxx(x −¯x) + ¯s.\n(5.11)\nThe estimator is in fact the sample linear minimal MSE (MMSE) es-\ntimate, obtained by plugging the sample-mean and sample covariance\nmatrices into the linear MMSE estimator.\nExample 5.1, which considers a simple yet common scenario of linear\nestimation with a partially-known measurement model, provides closed-\nform expressions for the suitable estimators attained via generative\nlearning and via discriminative learning. The resulting estimators in\n(5.8) and in (5.11) are generally different. Yet, they can be shown\nto coincide given sufficient number of data samples, i.e., they both\napproach the linear MMSE estimator when nt →∞. Nonetheless, in\nthe presence of model-mismatch, e.g., when the true P is not a linear\none, but is given by x = g(H, s) + w for some non-linear function\ng(·), the learned estimators differ also for nt →∞. In this case, the\ndiscriminative estimator is preferred as it approaches the linear MMSE\nestimator regardless of the underlying model, while the generative\nlearning approach, which is based on a mismatched statistical model,\nyields a linear estimate that differs from the LMMSE estimator, and is\nthus sub-optimal at nt →∞.\nThis behavior is empirically observed in Fig. 5.3. There, we compare\nthe generative and the discriminative learning estimators for signals in\nR30, when the target covariance has exponentially decaying off-diagonal\nentries, i.e., [Css]i,j = e−|i−j|\n5 . While the discriminative approach is\ninvariant of this setting, for the generative estimator we consider both\nthe case in which Css is accurately known as well as a mismatched case\n5.1. LEARNING TO COPE WITH MODEL DEFICIENCY\n67\n(a) MSE vs. SNR 1/σ2; nt = 100.\n(b) MSE vs. number of samples nt; σ = 0.3.\nFigure 5.3: MSE of discriminative versus generative learning of a linear estimator\nin a jointly Gaussian model with and without model mismatch, compared with the\nMMSE.\nwhere it is approximated as the identity matrix. Observing Fig. 5.3a, we\nnote that in the high signal-to-noise ratio (SNR) regime, all estimators\nachieve performance within a minor gap of the MMSE. However, in\nlower SNR values, the generative estimator, which fully knows Css,\noutperforms the discriminative approach due to its ability to incor-\nporate the prior knowledge of σ2 and of the statistical moments of s.\nNonetheless, in the presence of small mismatches in Css, the discrimi-\nnative approach yields improved MSE, indicating on its ability to better\ncope with modeling mismatches compared with generative learning. In\nFig. 5.3b we observe that the effect of model mismatch does not vanish\nwhen the number of samples increases, and the mismatched generative\nmodel remains within a notable gap from the MMSE, while both the\ndiscriminative learning estimator and the non-mismatched generative\none approach the MMSE as nt grows.\nThe above noted improved robustness of discriminative learning\nto model mismatch is exploited in model-based deep learning method-\nologies, detailed in the sequel. A repeated rationale which is shared\namong many of these methodologies leverages the available domain\nknowledge to identify a suitable model-based algorithm for the problem\nat hand, and then converts this algorithm into a trainable discriminative\narchitecture, where data is used to directly tune the selected solver. This\nrationale is useful for not only for the model deficit case, as discussed\n68\nModel-Based Deep Learning\nhere, but also for tackling algorithm deficiency. There, one can parame-\nterize an efficient model-based method that exists for a surrogate related\nmodel, and train it in a discriminative manner to be applicable for the\nproblem at hand.\nConsequently, discriminative learning dominates the training of ar-\nchitectures designed via learned optimization (Section 5.2) and deep\nunfolding (Section 5.3). Generative learning approaches are utilized in\nsome forms of model-based deep learning via DNN-aided priors and\nDNN-aided inference, as discussed in Sections 5.4-5.5. These allow to in-\ntegrate a pre-trained architecture into an existing model-based method,\nand can thus benefit from both the generative and discriminative ap-\nproaches.\n5.2\nLearned Optimization\nRecall that in Chapter 3, when we discussed decision making via model-\nbased optimization, we identified two main parameters of such solvers:\n• Objective parameters θo: These are parameters used in the formu-\nlation of the resulting optimization problem, which the decision\nmaking system is designed to solve. They typically arise from the\nmodel imposed on the task. However, some objective parameters,\ne.g., regularization coefficients, do not stem directly from the sys-\ntem model, and are selected so that a solver based on the objective\nachieves satisfactory results. In some cases (e.g., the Kalman filter\nexample discussed in Example 3.5), one can directly solve the\nresulting problem, i.e., mathematically derive arg minf Lθo(f). As\nwe discussed, the model-based objective Lθo is often a simplified\napproximated surrogate of the true system objective, and thus\nmodifying θo can better fit the solution to the actual problem\nbeing solved (and not just the one we can formulate).\n• Hyperparameters θh: Often in practice, even simplified surrogate\nobjectives do not give rise to a direct solution (such as the sparse\nrecovery example discussed in Example 3.6), but can be tackled\nusing iterative solvers. Iterative solvers are associated with their\n5.2. LEARNED OPTIMIZATION\n69\nown parameters (e.g., step-sizes and initialization), which are\nhyperparameters of the problems.\nTuning of the above parameters is traditionally a cumbersome task,\ninvolving manual configuration over extensive simulations. Learned\noptimization is a method to utilize deep learning techniques to facili-\ntate this aspect of optimization. In particular, learned optimizers use\nconventional model-based methods for decision making, while tuning\nthe parameters and hyperparameters of classic solvers via automated\ndeep learning training [48]. This form of model-based deep learning\nleverages data to optimize the optimizer. While learned optimization\nbypasses the traditional daunting effort of manually fitting the decision\nrule parameters, it involves the introduction of new hyperparameters of\nthe training procedure that must to be configured (typically by hand).\nLearned optimization effectively converts an optimizer into an ML\nmodel. Since automated tuning of ML models is typically carried out\nusing gradient-based first-order methods, a key requirement is for the\noptimizer to be differentiable, namely, that one can compute the gradient\nof its decision with respect to the parameter being optimized.\n5.2.1\nExplicit Solvers\nLearned optimization focuses on optimizing parameters conventionally\ntuned manually; these are parameters whose value does not follow from\nprior knowledge of the problem being solved, and thus their modification\naffects only the solver, and not the problem being solved. For model-\nbased optimizers based on explicit solutions, the parameters available\nare only those of the objective θo. Nonetheless, some of these parameters\nstem from the fact the objective is inherently surrogate to the actual\nproblem being solved, and thus require tuning, as shown in the following\nexample, based on the running example of tracking of dynamic systems.\nExample 5.2. Consider a dynamic system characterized by a linear\nGaussian state-space model as in (3.14), namely\nst+1 = F st + vt,\nvt ∼N(0, V )\n(5.12a)\nxt = Hst + wt,\nwt ∼N(0, W ).\n(5.12b)\n70\nModel-Based Deep Learning\nFor the filtering task (MSE estimation of st from {xτ}τ≤t), we know\nthat the optimal solution is the Kalman filter, given by the update\nequations:\nˆst|t = f({xτ}τ≤t) = F ˆst−1|t−1 + Kt(xt −HF ˆst−1|t−1),\n(5.13)\nwhere the Kalman gain is updated via\nKt = Σt|t−1HT \u0000HΣt|t−1HT + W\n\u0001−1,\n(5.14)\nand the second-order moments are updated as\nΣt|t−1 = F Σt−1|t−1F T + V ,\n(5.15a)\nΣt|t = Σt|t−1 −KtHΣt|t−1.\n(5.15b)\nIn such settings, the linear mappings F , H often arise from un-\nderstanding the physics of the problem. Nonetheless, in practice, one\ntypically does not have a concrete stochastic model for the noise signals,\nwhich are often introduced as a way to capture stochasticity, and thus\nV and W are often tuned by hand.\nGiven a data set comprised of nt trajectories of T observations along\nwith their corresponding states and actions, i.e., D = {{xi\nt, si\nt}T\nt=1}nt\ni=1,\none may set the trainable parameters to be θ = [V , W ], and write\nthe Kalman filter output (5.13) for a given setting as fθ({xτ}τ≤t) and\noptimize them via\nθj+1 = θj −ηj∇θLDj(fθj).\n(5.16)\nIn (5.16), Dj is a randomly selected mini-batch of D, and the loss is\nLD(fθ) =\n1\nT · |D|\n|D|\nX\ni=1\nT\nX\nt=1\n∥si\nt −fθ({xi\nτ}τ≤t)∥2\n2 + ϕ(θ),\n(5.17)\nwhere ϕ(·) is a regularizing term (e.g., a scalar multiple of the ℓ2 or ℓ1\nnorm).\nIn order to optimize θ = [V , W ] using (5.16), thus fitting the\nKalman filter to data, one must be able to compute the gradient of\nˆst|t with respect to both V and W . To show that these gradients can\nindeed be computed, let us focus on a scalar case (though we keep the\n5.2. LEARNED OPTIMIZATION\n71\nmultivariate notations, e.g., V and W , for brevity), where both xt and\nst are scalars. Here\n∂ˆst|t\n∂V\n= F ∂ˆst−1|t−1\n∂V\n+ ∂Kt\n∂V (xt −HF ˆst−1|t−1) −KtHF ∂ˆst−1|t−1\n∂V\n,\n(5.18)\nwhere, by (5.14), we have that\n∂Kt\n∂V\n=\n∂Kt\n∂Σt|t−1\n∂Σt|t−1\n∂V\n,\n(5.19)\nin which\n∂Kt\n∂Σt|t−1 is obtained from (5.14) and\n∂Σt|t−1\n∂V\n= F ∂Σt−1|t−1\n∂V\nF T + I.\n(5.20)\nNow, by (5.15) we have that\n∂Σt|t\n∂V\n= ∂Σt|t−1\n∂V\n−∂Kt\n∂V HΣt|t−1 −KtH ∂Σt|t−1\n∂V\n= ∂Σt|t−1\n∂V\n−\n∂Kt\n∂Σt|t−1\n∂Σt|t−1\n∂V\nHΣt|t−1 −KtH ∂Σt|t−1\n∂V\n(a)\n=\n \nI −\n∂Kt\n∂Σt|t−1\nHΣt|t−1 −KtH\n!\n∂Σt|t−1\n∂V\n(b)\n=\n \nI −\n∂Kt\n∂Σt|t−1\nHΣt|t−1 −KtH\n! \u0012\nF ∂Σt−1|t−1\n∂V\nF T + I\n\u0013\n,\n(5.21)\nwhere (a) holds since we assumed scalar quantities for brevity, and (b)\nis due to (5.20).\nNote that (5.21) formulates the computation of the gradient\n∂Σt|t\n∂V\nas a recursive relationship, implying that it can be computed recursively\nas a form of backpropagation through time. Then, the computed value\nis used to set ∂Kt\n∂V\nin (5.19), which is substituted into the gradient\nexpression in (5.18). Note that once ∂Kt\n∂V\nis obtained, (5.18) forms an\nadditional recursive relationship. While the above derivations are stated\nfor V , one can formulate similar recursions for W . Once these gradients\nare computed, they can be used to optimize θ from data via (5.16).\n72\nModel-Based Deep Learning\n(a) Kalman filter with mismatched com-\npared with full knowledge.\n(b) Kalman filter with learned parame-\nters compared with full knowledge.\nFigure 5.4: Tracking a dynamic system from noisy observations using the model-\nbased Kalman filter with mismatches and with objective parameters learned via\nlearned optimization.\nTo illustrate the ability to tune objective parameters by learning\nthrough an explicit solver via learned optimization, we next simulate a\nKalman filter for tracking a two-dimensional state vector from a noisy\nobservations of the first entry, as in the study reported in Fig. 3.1. Here,\na mismatched model corresponds to inaccurate knowledge of the state\nevolution noise covariance V , which degrades accuracy as shown in\nFig. 5.4a. Then, by treating V as a trainable parameter and converting\nthe Kalman filter into a discriminative model, one can learn its setting\nby backpropagating through the algorithm, yielding performance that\napproaches that achieved with full domain knowledge, as illustrated in\nFig. 5.4b, where we used a data set comprised of nt = 20 trajectories.\n5.2.2\nIterative Optimizers\nWhen the inference rule being learned is an iterative solver, one can\nuse data to automatically tune the hyperparameters θh. As opposed\nto the objective parameters θo, they often have only a minor effect on\nthe solution when the algorithm is allowed to run to convergence, and\nso are of secondary importance. But when the iterative algorithm is\nstopped after a predefined number of iterations, they affect the decisions,\nand therefore also the design objective. Additional parameters that can\nbe automatically tuned from data are objective parameters that do\nnot follow from the underlying statistical model, e.g., regularization\ncoefficients, which are conventionally tuned manually and not based on\n5.2. LEARNED OPTIMIZATION\n73\nsome principled modeling.\nThe main challenge with applying the same approach used in Ex-\nample 5.2 for iterative solvers stems from the challenge in taking their\ngradients. This is due to the fact that the output is not an explicit\nfunction of the input and the hyperparameters, but an implicit one,\ni.e., there is no closed-form expression for the inference rule output.\nMoreover, the number of iterations can vary between different inputs.\nNonetheless, one can still compute the gradients by letting the algorithm\nrun until it converges, and then taking the gradient of the loss computed\nover a data set, as shown in the following example:\nExample 5.3. Recall the ISTA, introduced in Example 3.6, as an itera-\ntive solver for the convex LASSO problem in (3.43), i.e.,\nˆs = arg min\ns\n1\n2∥x −Hs∥2 + ρ∥s∥1,\n(5.22)\nwhich is obtained as an ℓ1 convex relaxation of the super-resolution\nwith sparse prior problem. The iterations of ISTA are given by (3.48),\nnamely,\ns(k+1) ←Tβ=µρ\n\u0010\ns(k) + µHT (x −Hs(k))\n\u0011\n,\n(5.23)\nwhere Tβ(·) is the soft-thresholding operation applied element-wise:\nTβ(x) ≜sign(x) max(0, |x| −β).\n(5.24)\nNow, given a labeled data set D = {xi, si}nt\ni=1, we are interested\nin optimizing two hyperparameters of the algorithm: the step size µ\n(which is a parameter of the iterative solver) and the soft threshold\nβ = µρ (which is a parameter of the objective that does not follow from\nthe underlying model), i.e., θ = [µ, β].\nBy letting ˆsi = f(xi; θ) denote the converged output of ISTA applied\nto xt with setting θ, the empirical risk is given by\nLD(θ) =\n1\n|D|\nX\n(xi,si)∈D\n∥si −f(xi; θ)∥2\n2.\n(5.25)\nNow, in order to use (5.25) to optimize θ using first-order methods, we\nneed to be able to compute the gradient\n∇θf(xi; θ) =\nh∂s(Ki)\n∂µ\n,\n∂s(Ki)\n∂β\niT\n,\n(5.26)\n74\nModel-Based Deep Learning\nwhere Ki is the number of iterations required to achieve convergence.\nEssentially, we will treat Kt as fixed after convergence is achieved, and\nset θ to get the best performance out of these iterations.\nTo compute (5.26), we note that\n∂\n∂β Tβ(x) = −sign(x)1β<|x|,\n∂\n∂xTβ(x) = 1β<|x|,\n(5.27)\nwhere 1(·) is the indicator function. Using this and (5.23), we obtain\nthe (element-wise) recursion\n∂s(k+1)\n∂µ\n= 1β<|s(k)+µHT (x−Hs(k))| ⊙\n∂\n\u0010\ns(k) + µHT (x −Hs(k))\n\u0011\n∂µ\n= 1β<|s(k)+µHT (x−Hs(k))|\n⊙\n \nHT \u0010\nx −Hs(k)\u0011\n+\n \u0010\nI −µHT H\n\u0011 ∂s(k)\n∂µ\n!!\n.\n(5.28)\nSimilarly, we can compute the gradient with respect to β via\n∂s(k+1)\n∂β\n= −sign\n\u0000s(k) + µHT (x −Hs(k))\n\u00011β<|s(k)+µHT (x−Hs(k))|\n+ Tβ\n\u0010\ns(k) + µHT (x −Hs(k))\n\u0011\n⊙\n \u0010\nI −µHT H\n\u0011 ∂s(k)\n∂β\n!\n.\n(5.29)\nThe above expressions for the gradients are used to update the hyperpa-\nrameters based on the empirical risk (5.25). Once the hyperparameters\nare learned from data, inference follows using the conventional ISTA.\nLearned optimization of the hyperparameters of iterative solvers\nreplace the often tedious manual tuning of these hyperparameters with\nan automated data-driven pipeline. While Example 5.3 focuses on ISTA,\na similar derivation can be applied to alternative optimizers such as\nADMM, as we show next.\nExample 5.4. Let us consider the ADMM optimizer, which aims at\nsolving\nfMAP(x) = arg min\ns\n1\n2∥x −Hs∥2 + σ2ϕ(s),\n(5.30)\n5.2. LEARNED OPTIMIZATION\n75\nvia the iterative steps summarized as Algorithm 1. This iterative al-\ngorithm has two main hyperparameters, i.e., θh = {λ, µ}. Learned\noptimization allows to leverage a labeled data set D = {xi, si}nt\ni=1 to\nbest tune these hyperparameters from data, i.e., to set the learned\nparameters to be θ = θh. These parameters are tuned in an automated\nfashion based on the following optimization problem\nθ∗= arg min\nθ={λ,µ}\n1\nnt\nnt\nX\ni=1\n∥fθ(xi) −si∥2,\n(5.31)\nwhere fθ(·) is ADMM with hyperparameters θ.\nTo tackle (5.31) via first-order methods and automated deep learning\noptimization, we need to show that the iterative algorithm is differen-\ntiable with respect to θ. As seen in Section 5.2, this can be done by\ntreating the algorithm as a discriminative learning model while using\na variant of backpropagation through time when each of the iteration\nsteps is differentiable. In Algorithm 1, the only computation which has\nto be specifically examined is the proximal mapping\nproxλϕ(y) ≜arg min\nz\n1\n2∥z −y∥2\n2 + λϕ(z).\n(5.32)\nConsequently, in order to backpropagate through ADMM, thus using\ndeep learning tool to tune the hyperparameters of ADMM, one has to\nbe able to compute the gradient of (5.32) with respect to θ = θh. We\nnext show that this can be done, focusing on the hyperparameter λ ∈θh\nwhich explicitly appears in the formulation of the proximal mapping.\nWhen ϕ(·) is differentiable, and the proximal mapping is unique,\nthis derivative can be approximated using its definition\n∂proxλϕ(y)\n∂λ\n= 1\nϵ\n\u0010\n∂prox(λ+ϵ)ϕ(y) −∂proxλϕ(y)\n\u0011\n.\n(5.33)\nNow, we write z∗= proxλϕ(y) and z∗+ ∆z = prox(λ+ϵ)ϕ(y). Next, we\nnote that at z∗+ ∆z, the gradient of the proximal mapping argument\nmust be zero, being a global minima of the right hand side of (5.32),\nand thus\n∇z\n1\n2∥z −y∥2\n2 + (λ + ϵ) · ϕ(z)\n\f\f\f\nz=z∗+∆z\n= (λ + ϵ) · ∇ϕ(z∗+ ∆z) + z∗+ ∆z −y = 0.\n(5.34)\n76\nModel-Based Deep Learning\nNext, we approximate ∇ϕ(z∗+ ∆z) using its Taylor series approxi-\nmation around z∗, such that\n∇ϕ(z∗+ ∆z) ≈∇ϕ(z∗) + Hϕ(z∗)∆z,\n(5.35)\nwith Hϕ being the Hessian matrix. Substituting this into (5.34) yields\n(λ + ϵ) · (∇ϕ(z∗) + Hϕ(z∗)∆z) + z∗+ ∆z −y = 0\n⇒(λ∇ϕ(z∗) + z∗−y) + ϵ∇ϕ(z∗) + (λ + ϵ)Hϕ(z∗)∆z + ∆z = 0\n(a)\n⇒ϵ∇ϕ(z∗) + (λ + ϵ)Hϕ(z∗)∆z + ∆z = 0,\n(5.36)\nwhere (a) follows since z∗= proxλϕ(y). Consequently,\n∆z = −ϵ (I + (λ + ϵ)Hϕ(z∗))−1 ∇ϕ(z∗),\n(5.37)\nimplying that\n∂proxλϕ(y)\n∂λ\n= −(I + λHϕ(z∗))−1 ∇ϕ(z∗),\n(5.38)\nwhose computation depends on the differentiable function ϕ(·). Once\nthe hyperparameters are learned from data, inference follows using the\nconventional ADMM, i.e., Algorithm 1.\nThe gains of hyperparameter tuning via learned optimization ability\nare illustrated in Fig. 5.5. There, learned optimization of the hyperapa-\nrameters of ADMM is applied to recover a 200×1 sparse vector from 150\nnoisy compressed observations, obtained using a Gaussian measurement\nmatrix. In particular, ADMM runs until either convergence is achieved\n(difference of less than 10−2 between consecutive estimates) or until\n1000 iterations are exhausted. The hyperparameters are learned using\nthe Adam optimizer [44] with 60 epochs over a data set of 1000 labeled\nsamples. The results in Fig. 5.5 demonstrate the ability of learned\noptimization to notably facilitate hyperparameter tuning.\n5.3\nDeep Unfolding\nA relatively common and well-established methodology for combining\nmodel-based methods and deep learning is that of deep unfolding, also\nreferred to as deep unrolling [12]. Originally proposed in 2010 by Greger\n5.3. DEEP UNFOLDING\n77\nFigure 5.5: Sparse signal recovered using ADMM with hyperparameters optimized\nvia learned optimization (upper figure) compared with ADMM with manually tuned\nhyperparemetrs (lower figure).\nand LeCun for sparse recovery [10], deep unfolding converts iterative\noptimizers into trainable DNNs.\nAs the name suggests, the method relies on unfolding an iterative\nalgorithm into a sequential procedure with a fixed number of iterations.\nThen, each iteration is treated as a layer, with its trainable parameters\nθ being either only the hyperparameters θh, or also the objective\nparameters θo. An illustration of this approach is depicted in Fig. 5.6.\nConverting an iterative optimizer into a DNN facilitates optimizing\ndifferent parameters for each iteration, being transformed into trainable\nparameters of different layers. This is achieved by training the decision\nbox end-to-end, i.e., by evaluating the system output based on data.\nFor instance, letting K be the number of unfolded iterations, deep\nunfolding can learn iteration-dependent hyperparameters {θh\nk}K\nk=1 and\nobjective parameters {θo\nk}K\nk=1. This increases the parameterization and\nabstractness compared with learned optimization of iterative solvers,\nwhich typically reuses the learned hyperparameters and runs until\nconvergence (as in model-based optimizers). Nonetheless, for every\nsetting of {θo\nk}K\nk=1 and {θh\nk}K\nk=1, a deep unfolded system effectively\ncarries out its decision using K iterations of some principled iterative\n78\nModel-Based Deep Learning\nFigure 5.6: Deep unfolding outline illustration.\nsolver known to be suitable for the problem.\nThe above rationale specializes into three forms of deep unfolding,\nwhich notably vary in their specificity versus parameterization tradeoff\nillustrated in Fig. 5.1. We refer to these approaches, ordered from the\nmost task-specific to the most parametrized one, as Unfolded Hyper-\nparameters, Unfolded Objective Parameters, and Unfolded Abstracted\nOptimizers.\n5.3.1\nLearning Unfolded Hyperparameters\nDeep unfolded networks can be designed to improve upon model-based\noptimization in two main criteria: convergence speed and model ab-\nstractness. The former is achieved by the fact that the resulting system\noperates with a fixed number of iterations, which can be set to be much\nsmaller compared with that usually required to achieve convergence.\nThis is combined with the natural ability of deep unfolding to learn\niteration-dependent hyperparameters to enable accurate decisions to\nbe achieved within this predefined number of iterations, as exemplified\nnext:\n5.3. DEEP UNFOLDING\n79\nExample 5.5. Let us consider again the ADMM optimizer as in Exam-\nple 5.4. We are interested in being able to accurately carry out ADMM\nwith a low and fixed number of iterations by leveraging a labeled data\nset D = {xi, si}nt\ni=1.\nA deep unfolded ADMM is obtained by setting the decision to be ˆs =\nsK for some fixed K, and allowing each iteration to use hyperparameters\n[λk, µk], that are stacked into the trainable parameters vector θ. These\nhyperparameters are learned from data via solving the optimization\nproblem\nθ∗=\narg min\nθ={λk,µk}K\nk=1\n1\nnt\nnt\nX\ni=1\n∥fθ(xi) −si∥2.\n(5.39)\nSince, the iterative algorithm is differentiable with respect to θ, as\nshown in Example 5.4, the per-iteration hyperparameters can be learned\nbased on (5.39) via first-order methods and automated deep learning\noptimization.\nOnce the hyperparameters are learned from data, inference is carried\nout using K ADMM iterations with the per-iteration configured set-\ntings. The resulting architecture, as compared with the original ADMM\nand its version with learned optimization, is illustrated in Fig. 5.12.\nUnfolding ADMM and learning iteration-dependent hyperparameters\nnotably improves performance compared to conventional ADMM with\nfixed hyperparaemeters, as visualized in Fig. 5.7. There, both unfolding\ninto K = 4 as well as K = 12 layers are contrasted with conventional\nADMM with the same number of iterations, showcasing the notable\ngains of deep unfolding with learned hyperparameters in improving\nperformance withing a fixed number of iterations.\n5.3.2\nLearning Unfolded Objective Parameters\nExample 5.5 preserves the operation of ADMM with K iterations,\nas only the hyperparameters are learned. Nonetheless, one can also\ntransform iterative solvers into more abstract DNNs by also tuning the\nobjective of each iteration. Here, the trainable architecture jointly learns\nthe hyperparameters and the objective parameters θo per each iteration.\nThis can be viewed as if each iteration follows a different objective, such\nthat the output of the system after K such iterations most accurately\n80\nModel-Based Deep Learning\n(a) Unfolded ADMM (L-ADMM) versus conventional ADMM with\nK = 4 iterations.\n(b) Unfolded ADMM (L-ADMM) versus conventional ADMM with\nK = 12 iterations.\nFigure 5.7: Comparing unfolded ADMM with conventional ADMM with the same\nnumber of iterations for recovering a 200 × 1 sparse signal from 150 compressed noisy\nobservations. The unofolded ADMM is trained using 1000 labeled pairs.\n5.3. DEEP UNFOLDING\n81\nFigure 5.8: Learned iterative soft-thresholding algorithm architecture.\nmatches the desired value. As a result, the trained DNN can realize\na larger family of abstract mappings and is likely to deviate from the\nmodel-based optimizer, which serves here as a principled initialization\nfor the system, rather than its fixed structure.\nExample 5.6. Consider again the ISTA optimizer, which solves the\nconvex LASSO problem in (5.22) via the iterative update equations\nin (5.23). The learned ISTA (LISTA) DNN architecture, which is the\nfirst introduction of deep unfolding methodology [10], unfolds ISTA by\nfixing K iterations and replacing the update step in (3.48) with\ns(k+1) = Tβk\n\u0010\nW 1\nkx + W 2\nks(k)\u0011\n.\n(5.40)\nNote that for W 1\nk = µHT , W 2\nk = I −µHT H, and βk = µρ, (5.40)\ncoincides with the model-based ISTA. The trainable parameters θ =\n\u0002{W 1\nk, W 2\nk, βk}K\nk=1\n\u0003 are learned from data via end-to-end training, build-\ning upon the ability to backpropagate through ISTA iterations discussed\nin Example 5.3. An illustration of the unfolded LISTA is given in Fig. 5.8.\nWhile in Example 5.6 each layer has different parameters, one can\nenforce the parameters to be equal across layers; since the objective\nparameters are optimized, doing so still preserves the ability of deep\nunfolded networks to jointly match the solver and its objective to\nthe data. Fig. 5.9 compares ISTA to LISTA, in which the objective\nparameters are shared between different iterations, i.e., the trainable\nparameters are θ =\n\u0002W 1, W 2, {βk}K\nk=1\n\u0003. In Fig. 5.9, LISTA is trained\nfor K ∈{1, . . . , 13} iterations, and is contrasted with ISTA (where the\nobjective is not learned but set via W 1 = µHT , W 2 = I−µHT H) with\n82\nModel-Based Deep Learning\nFigure 5.9: MSE performance of LISTA and ISTA versus number of iterations K.\nthe same number of iterations for recovering a 200 × 1 4-sparse signal\nfrom 150 compressed noisy measurements, where the MSE is averaged\nover 1000 realizations. The results in Fig. 5.9 showcase the ability of\ndeep unfolding with learned objective parameters to notably improve\nperformance over conventional model-based optimization by jointly\nlearning a surrogate objective along with dedicated hyperparameters.\nFor additional examples on deep unfolding for jointly learning the\nobjective and the hyperparameters see examples in [51]–[53], and the\nsurveys in [12], [54].\n5.3.3\nLearning Unfolded Abstracted Optimizers\nThe third variant of the deep unfolding methodology uses an iterative\noptimizer to form an abstract DNN whose operation is inspired by the\nmodel-based optimizer rather than preserving or even specializing it.\nAn example for such an unfolding of an iterative optimization algorithm,\nwhich introduces further abstraction beyond reparameterization of the\nmodel-based iterations, is the DetNet architecture proposed in [25].\nThis algorithm is obtained by unfolding the projected gradient descent\niterations for detecting binary vectors in linear settings with Gaussian\nnoise. This corresponds to, e.g., symbol detection in multiple-input\nmultiple-output (MIMO) communications, where the constraint follows\nfrom the discrete nature of digital symbols.\n5.3. DEEP UNFOLDING\n83\nExample 5.7. Consider the optimization problem formulated by\nˆs = arg min\ns∈{±1}K ∥x −Hs∥2,\n(5.41)\nwhere H is a known matrix. While directly solving (5.41) involves an\nexhaustive search over the 2K possible combinations, it can be tackled\nwith affordable computational complexity using the iterative projected\ngradient descent algorithm. Let PS(·) denote the projection operator\ninto S, which for S = {±1}K is the element-wise sign function. Projected\ngradient descent iterates via\nˆs(k+1) = PS\n \nˆs(k) −ηk\n∂∥x −Hs∥2\n∂s\n\f\f\f\f\f\ns=ˆs(k)\n!\n= PS\n\u0010\nˆs(k) −ηkHT x + ηkHT Hˆs(k)\u0011\n(5.42)\nwhere ηk denotes the step size at iteration k, and ˆs0 is set to some\ninitial guess.\nDetNet builds upon the observation that (5.42) consists of two\nstages: gradient descent computation, i.e., gradient step ˆs(k) −ηkHT x+\nηkHT Hˆs(k), and projection, namely, applying PS(·). Therefore, each\nunfolded iteration is represented as two sub-layers: The first sub-layer\nlearns to compute the gradient descent stage by treating the step-size\nas a learned parameter and applying a fully-connected (FC) layer with\nReLU activation to the obtained value. For iteration index k, this results\nin\nz(k) =ReLU\n\u0010\nW 1,k\n\u0010\n(I+δ2,kHT H)ˆs(k−1)−δ1,kHT x\n\u0011\n+b1,k\n\u0011\nin which {W 1,k, b1,k, δ1,k, δ2,k} are learnable parameters. The second\nsub-layer learns the projection operator by approximating the sign\noperation with a soft sign activation proceeded by an FC layer, leading\nto\nˆs(k) = soft sign\n\u0010\nW 2,kz(k) + b2,k\n\u0011\n,\n(5.43)\nwhere the soft sign operation is applied element-wise and is given by\nsoft sign(x) =\nx\n1+|x|. Here, the learnable parameters are {W 2,k, b2,k}.\nThe resulting deep network is depicted in Fig. 5.10, in which the output\nafter K iterations is used as the estimated symbol vector by taking the\nsign of each element.\n84\nModel-Based Deep Learning\nFigure 5.10: DetNet illustration. Parameters in red fonts are learned in training,\nwhile those in blue fonts are externally provided.\nLet θ = {(W 1,k, W 2,k, b1,k, b2,k, δ1,k, δ2,k)}Q\nk=1 be the trainable pa-\nrameters of DetNet1. To tune θ, the overall network is trained end-to-end\nto minimize the empirical weighted ℓ2 norm loss over its intermediate\nlayers, given by\nL(θ) = 1\nnt\nnt\nX\ni=1\nK\nX\nk=1\nlog(k)∥si −ˆs(k)(xi; θ)∥2\n(5.44)\nwhere ˆsk(xi; θ) is the output of the kth layer of DetNet with parameters\nθ and input xi. This loss measure accounts for the interpretable nature\nof the unfolded network, in which the output of each layer is a further\nrefined estimate of s.\n5.4\nDNN-Aided Priors\nIn the previous section we introduced the strategy of deep unfolding for\ncombining principled mathematical models with data-driven DNNs [10].\nIn unfolded networks, the resultant inference system is a deep network\nwhose architecture imitates the operation of a model-based iterative\noptimizer, and we discussed three different unfolding approaches which\narise from the inherent parameterization of iterative algorithms. The\nnext sections introduce DNN-aided inference, which is a family of model-\nbased deep learning algorithms in which DNNs are incorporated into\nmodel-based methods. Here, inference is carried out using a traditional\n1The formulation of DetNet in [25] includes an additional sub-layer in each\niteration intended to further lift its input into higher dimensions and introduce\nadditional trainable parameters, as well as reweighing of the outputs of subsequent\nlayers. As these operations do not follow directly from unfolding projected gradient\ndescent, they are not included in the description here.\n5.4. DNN-AIDED PRIORS\n85\nmodel-based method, while some of the intermediate computations are\naugmented by DNNs, as illustrated in Fig. 5.11.\nThe main motivation of DNN-aided inference is to exploit the es-\ntablished benefits of model-based methods, in terms of performance,\ncomplexity, and suitability for the problem at hand. Deep learning is\nincorporated to mitigate sensitivity to inaccurate model knowledge,\nfacilitate operation in complex environments, limit computational bur-\nden, and enable application of the algorithm in new domains. In this\nsection, we focus on DNN-aided inference for facilitating optimization\nover complex and intractable signal domains (coined structure-agnostic\nDNN-aided inference in [46]).\nWhile model-based deep learning is mostly about converting model-\nbased algorithms into trainable architectures that can be learned end-to-\nend, namely, via discriminative learning, the family of DNN-aided priors\nscheme is typically concerned with generative learning, i.e., learning the\nstatistical model rather than the inference task. However, as opposed\nto conventional generative learning as exemplified in Section 5.1, where\nlearning is used to fill in missing parameters of an imposed system\nmodel, here the rationale is to exploit the abstractness of DNNs to\nimplicitly capture the underlying statistics, without having to explicitly\nmodel it.\nDNN-aided priors is thus a family of methods which use deep learn-\ning tools to implicitly learn structures and statistical properties of the\nsignal of interest, in a manner that is amenable to model-based opti-\nmization. These inference systems are particularly relevant for various\ninverse problems in signal processing, including denoising, sparse recov-\nery, deconvolution, and super resolution [13]. Tackling such problems\ntypically involves imposing some structure on the signal domain. This\nprior knowledge is then incorporated into a model-based optimization\nprocedure, which recovers the desired signal with provable performance\nguarantees.\nTo formulate this, we consider optimization problems which take on\nthe form\nˆs = arg max\ns\nL(s) = arg max\ns\ngx(s) + ϕ(s).\n(5.45)\nNote that (5.45) is stated as an unconstrained optimization whose\n86\nModel-Based Deep Learning\nFigure 5.11: DNN-aided inference illustration: a) a model-based algorithm com-\nprised of multiple iterations with intermediate model-based computations; b) A\ndata-driven implementation of the algorithm, where the specific model-based compu-\ntations are replaced with dedicated learned deep models.\nobjective is comprised of two terms – a data matching term gx(·), which\ndepends on the observations x, and a regularization term, or prior,\ndenoted ϕ(·), that is invariant of x.\nOptimization problems of the form (5.45) are tackled by various\nforms of deep unfolding, as discussed in Section 5.3. There, the op-\ntimization problem is known and ϕ(·) is given (up to perhaps some\nmissing parameters such as regularization coefficients); model-based\ndeep learning is thus employed to realize an effective solver, e.g., to cope\nwith algorithm deficiency. DNN-aided priors tackle a complementary\nchallenge, which stems from the difficulty in characterizing the prior\nterm ϕ(·), i.e., model deficiency. For instance, how does one capture the\nmarginal distribution of natural images? or of human speech?\nWe next discuss two approaches for designing inference rules via\nDNN-aided priors to cope with the challenge of characterizing prior\nterms for various types of data. The first, termed plug-and-play networks,\naugments iterative optimizers such as ADMM with a DNN to bypass\nthe need to express a proximal mapping. The second approach, coined\n5.4. DNN-AIDED PRIORS\n87\ndeep priors, employs deep generative networks for capturing the signal\nprior in a manner that is amenable to applying gradient-based iterative\noptimizers for the resulting optimization problem.\n5.4.1\nPlug-and-Play Networks\nPlug-and-play networks utilize deep denoisers as learned proximal map-\npings, which are key ingredients in many iterative optimization methods.\nHere, one uses DNNs to carry out an optimization procedure which\nrelies on a regularized objective without having to compute it, i.e., with-\nout expressing the desired signal domain in tractable form. We focus on\nthe combination of plug-and-play networks with ADMM optimization,\nthough this methodology can also be combined with other optimizers\nsuch a ISTA. We exemplify it for tackling the regularized objective of\n(3.58), i.e.,\nˆs = arg min\ns\n1\n2∥x −Hs∥2 + ϕ(s).\n(5.46)\nPlug-and-Play ADMM\nThe ADMM algorithm [37] was introduced in\nExample 3.8 and formulated in Algorithm 1 as an iterative optimizer\nseeking the saddle point of the primal-dual formulation of the optimiza-\ntion problem in (3.58). The key challenge in implementing Algorithm 1\nstems from the computation of the proximal mapping in Step 2. There\nare two main difficulties associated with this step:\n1. Explicit knowledge of the prior is required, which is often not\navailable.\n2. Even when one has a good approximation of ϕ(·), computing the\nproximal mapping may still be extremely challenging to carry out\nanalytically.\nNonetheless, the proximal mapping is invariant of the task. In\nparticular, as we show in the following example, it is the solution to a\ndenoiser for signals with prior ϕ(·).\n88\nModel-Based Deep Learning\nExample 5.8. Consider a denoising task of recovering s from its noisy\nmeasurements of the form\nx = s + w,\n(5.47)\nwhere w ∼N(0, σ2I). In this case, the MAP rule of Example 3.1 can\nbe applied, where the log conditional probability becomes\n−log p(x|s) = −log(2π)−k/2det(σ2I)−1/2 exp{−1\n2σ2 ∥x −s∥2}\n=\n1\n2σ2 ∥x −s∥2 + const.\n(5.48)\nConsequently, the MAP rule of (3.11) for the denoising task is given by\nˆsMAP = arg min\ns\n1\n2∥x −s∥2 + σ2ϕ(s)\n= proxσ2·ϕ(x).\n(5.49)\nIt is noted that even when the noise is not known to obey a Gaussian\ndistribution, modeling the denoiser task as (5.49) is still reasonable\nbeing the least-squares objective.\nDenoisers are common DNN models, which can be trained in an\nunsupervised manner (e.g., autoencoders) and are known to operate\nreliably on signal domains with intractable priors (e.g., natural images).\nOne can thus implement Algorithm 1 without having to specify the\nprior ϕ(·) by replacing Step 2 with a DNN denoiser, as illustrated in\nFig. 5.12.\nThe term plug-and-play [17] is used to describe decision mappings as\nin the above example where pre-trained models are plugged into model-\nbased optimizers without further tuning, as illustrated in Fig. 5.12(c).\nNonetheless, this methodology can also incorporate deep learning into\nthe optimization procedure by, e.g., unfolding the iterative optimiza-\ntion steps into a large DNN whose trainable parameters are those of\nthe smaller networks augmenting each iteration, as in [16], [55]. This\napproach allows to benefit from both the ability of deep learning to\nimplicitly represent complex domains, as well as the inference speed\nreduction of deep unfolding along with its robustness to uncertainty\nand errors in the model parameters assumed to be known. Nonetheless,\n5.4. DNN-AIDED PRIORS\n89\nthe fact that the iterative optimization must be learned from data in\naddition to the prior on S implies that typically larger amounts of\nlabeled data are required to train the system, compared to using the\nmodel-based optimizer.\n5.4.2\nDeep Priors\nThe plug-and-play approach avoids having to actually compute the prior\nϕ(·) by going directly to the optimization algorithm which builds upon\nthe regularized objective, and specifically selecting an optimizer where\nthe incorporation of ϕ(·) can be replaced with a standard denoiser (due\nto, e.g., variable splitting as in ADMM). An alternative approach to\naugment model-based solvers with pre-trained DNNs is the usage of deep\npriors [14]. As opposed to plug-and-play networks, which focus on the\nsolver that is based on the regularized optimization problem (5.45), e.g.,\non ADMM, deep priors use DNNs to replace the optimization problem\nwith a surrogate one. Here, deep learning tools are particularly employed\nto yield a surrogate optimization problem which is amenable to tackling\nusing gradient based approaches, while being a reliable approximation\nof the true (possibly intractable) optimization problem.\nGenerative Priors\nConsider again the super-resolution problem de-\ntailed in Example 3.1. However, let us assume that we have access to\na bijective mapping from some latent space Z to the signal space S,\ndenoted G : Z 7→S. In this case, assuming discrete-valued random\nvariables, the prior term ϕ(s) becomes\nϕ(s) = −log p(s) + const\n= −log p(z = G−1(s)) + const := ˜ϕ(z)|z=G−1(s).\n(5.50)\nWe can now write the regularized optimization problem as\nˆs = G(ˆz),\nwhere\nˆz = arg min\nz\ngx (G(z)) + ˜ϕ(z).\n(5.51)\n90\nModel-Based Deep Learning\nFigure 5.12: An illustration of the model-based deep learning strategies arising from the ADMM optimizer (Algorithm 1),\nwhere variables in red fonts represent trainable parameters: a) the model-based optimizer; b) a learned ADMM optimizer; c)\nplug-and-play ADMM; and d) deep unfolded ADMM.\n5.4. DNN-AIDED PRIORS\n91\nFor the MAP rule in (3.11), this becomes\nˆsMAP = G(zMAP),\nwhere\nzMAP = arg min\nz\n1\n2∥x −HG(z)∥2 + σ2 ˜ϕ(z).\n(5.52)\nThe rationale of generative priors is thus to find a mapping G(·) with\nwhich the surrogate optimization problem (5.51) is a faithful approxima-\ntion of the true optimization problem (5.45), e.g., one can employ (5.52)\nto seek the MAP rule (3.11). Specifically, when using deep learning for\ngenerative priors, the learned prior is differentiable, facilitating tackling\nthe surrogate problem via gradient based methods, as detailed next,\nwhere we focus our description on the approximation of the MAP rule\n(3.11).\nDeep Generative Priors\nWhen one has access to a mapping G(·)\nfor which the latent prior ˜ϕ(·) is computable and differentiable, the\nsurrogate regularized objective (5.52) can be used to recover the MAP\nestimate ˆsMAP. The learning of such mappings G(·) that map from\na known distribution into a complex signal distribution is what is\ncarried out by deep generative models, such as generative adversarial\nnetworks (GANs). Exploiting this property for regularized optimization\nwas proposed in [14]. Here, that latent variable z takes a zero-mean\nGaussian distribution with i.i.d. entries, and thus there exists λ > 0 for\nwhich\nσ2 ˜ϕ(z) = λ∥z∥2 + const.\n(5.53)\nDeep generative priors use a pre-trained DNN-based prior Gθ, typ-\nically a GAN trained over the domain of interest in an unsupervised\nmanner. Consequently, the design of DNN-aided priors commences with\ntraining a deep generative model for the considered data type. Once the\ntrained Gθ is available, one can formulate the surrogate optimization\nproblem as\nzMAP = arg min\nz\n1\n2∥x −HGθ(z)∥2 + λ∥z∥2 := arg min\nz\nL(z).\n(5.54)\nEven though the exact formulation of Gθ may be highly complex, one\ncan tackle (5.54) via gradient-based optimization, building upon the fact\n92\nModel-Based Deep Learning\nFigure 5.13: High-level overview of tackling (5.49) with a DNN-based prior. The\ngenerator network G is pre-trained to map Gaussian latent variables to plausible\nsignals in the target domain. Then signal recovery is done by finding a point in the\nrange of G that minimizes the reconstruction error via gradient-based optimization\nover the latent variable.\nthat DNNs allow simple computation of gradients via backpropagation,\ne.g.\nzk+1 = zk −ηk∇z=zkL(z).\n(5.55)\nNote that the gradient is computed in (5.55) not with respect to the\nweights (as done in conventional DNN training), but with respect to\nthe input of the network.\nIn summary, deep generative priors replace the constrained opti-\nmization over the complex input signal with tractable optimization over\nthe latent variable z, which follows a known simple distribution. This\nis achieved using a pre-trained DNN-based prior Gθ to map it into the\ndomain of interest. Inference is performed by minimizing L in the latent\nspace of Gθ via, e.g., (5.55). An illustration of the system operation is\ndepicted in Fig. 5.13.\n5.5\nDNN-Aided Inference\nThe last strategy we review combines conventional DNN architectures\nwith model-based optimization to enable the latter to operate reliably\nin complex domains. The rationale here is to preserve the objective and\n5.5. DNN-AIDED INFERENCE\n93\nstructure of a model-based decision mapping suitable for the problem\nat hand based on the available domain knowledge, while augmenting\ncomputations that rely on approximations and missing domain knowl-\nedge with model-agnostic DNNs. DNN-aided inference thus aims at\nbenefiting from the best of both worlds by accounting in a principled\nmanner for the available domain knowledge while using deep learning\nto cope with the elusive aspects of the problem description.\nUnlike the strategies discussed in the previous sections, which are\nrelatively systematic and can be viewed as recipe-style methodolo-\ngies, DNN-aided inference accommodates a broad family of different\ntechniques for augmenting model-based optimizers with DNNs. We\nnext discuss two representative DNN-aided inference approaches, with\nthe first replacing internal computations of a model-based method\nwith DNNs, while the latter adds an external DNN to correct internal\ncomputations. We exemplify these approaches based on our running\nexamples.Additional examples from other domains in signal processing\nand communications can be found in, e.g., [22], [56], [57].\n5.5.1\nReplacing Model-Based Computations with DNNs\nOften, model-based algorithms can be preferred based on some structural\nknowledge of the underlying model, despite the fact that its subtleties\nmay (and often are) unknown. In such cases, one can exploit domain\nknowledge of statistical structures to carry out model-based inference\nmethods in a data-driven fashion. These hybrid systems thus utilize\ndeep learning not for the overall inference task, but for robustifying and\nrelaxing the model-dependence of established model-based inference\nalgorithms designed specifically for the structure induced by the specific\nproblem being solved.\nWe next demonstrate how this rationale is translated into hybrid\nmodel-based/data-driven algorithms. We consider both the running\nexample of tracking of dynamic systems, based on [58], and that of DoA\nestimation, which is based on [59].\nExample 5.9. Consider a dynamic system characterized by a state-space\n94\nModel-Based Deep Learning\nmodel\nst+1 = f(st) + vt,\n(5.56a)\nxt = h(st) + wt.\n(5.56b)\nHere, f(·) and h(·) are possibly non-linear mappings, and the noise\nsequences vt, wt are zero-mean and i.i.d. in time. However, we do not\nhave a concrete stochastic model for the noise signals, which are often\nintroduced as a way to capture stochasticity, and thus V and W are\noften tuned by hand. Yet, we are given a data set comprised of nt\ntrajectories of T observations along with their corresponding states and\nactions, i.e., D = {{xi\nt, si\nt}T\nt=1}nt\ni=1. We focus on the filtering task, i.e.,\nMSE estimation of st from {xτ}τ≤t.\nClearly, if the noise is Gaussian with known moments and the\nfunctions f(·) and h(·) are linear, then the Kalman filter achieves the\nminimal MSE. Even when f(·) and h(·) are non-linear, one can still use\nvariants of the Kalman filter, such as the extended Kalman filter, in\nwhich the prediction stage is obtained by\nˆst|t−1 = f(ˆst−1),\n(5.57a)\nˆxt|t−1 = h(ˆst|t−1),\n(5.57b)\nand the state estimate still takes the form\nˆst = ˆst|t−1 + Kt · (xt −ˆxt|t−1).\n(5.58)\nRecall that in the conventional Kalman filter, the Kalman gain Kt is\ncomputed by tracking the second order moments of the predictions in\n(5.57); when the setting is non-linear, the extended Kalman filter (EKF)\ncomputes these moments using Taylor series approximations of f(·)\nand h(·). While this allows dealing with the non-linearity of f(·) and\nh(·) (though in a sub-optimal manner), one still has to cope with the\nunknown distribution of the noise signals. In fact, if the noise signals\nare not Gaussian, a linear estimator as in (5.58) (where Kt is not a\nfunction of the inputs of the system) is likely to result in degraded\nperformance.\nSince the dependency on the noise statistics in the Kalman filter is\nencapsulated in the Kalman gain Kt, its computation can be replaced\n5.5. DNN-AIDED INFERENCE\n95\nFigure 5.14: DNN augmented Kalman filter illustration. Here, the operation of the\nmodel-based Kalman filter is preserved in its division into prediction step (based on\n(5.57)) and update step (based on (5.58)), while deep learning is employed to learn\nfrom data to compute the Kalman gain, denoted Kθ.\nwith a trainable DNN, and thus (5.58) is replaced with\nˆst = ˆst|t−1 + Kθ(xt, ˆst−1) · (xt −ˆxt|t−1),\n(5.59)\nwhere Kθ(·) is a DNN with parameters θ. Since Kt is updated re-\ncursively, its learned computation is carried out with an RNN. One\ncan consider different architectures for implementing Kθ(·): the work\n[58] proposed two architectures, one that is based on a single RNN\nwith preceding and subsequent FC layers, and one that is based on\nthree RNNs. An alternative architecture was proposed in [60], where\ntwo RNNs were used to track the covariances of ˆst|t−1 and of ˆxt|t−1,\nrespectively, which allow to compute the model-based Kalman gain via\n(3.32). By letting f(·; θ) be the latent state estimate computed using\n(5.59) with parameters θ, the overall system is trained end-to-end via\nθ∗= arg min\nθ\n1\nntT\nnt\nX\ni=1\nT\nX\nt=1\n∥f(xi\nt; θ) −si\nt∥2\n2.\n(5.60)\nThe resulting system is illustrated in Fig. 5.14.\nThe DNN-aided Kalman filter of Example 5.9, referred to as Kalman-\nNet [58], learns to track dynamic systems from data while preserving the\noperation of the model-based Kalman filter and its variants. The ability\n96\nModel-Based Deep Learning\nFigure 5.15: Tracking a single trajectory using the DNN-aided KalmanNet compared\nwith the model-based EKF (reproduced from [58]).\nEKF\nUKF\nPF\nKalmanNet\nRNN\nMSE [dB]\n-6.432\n-5.683\n-5.337\n-11.284\n17.355\nRun-time[sec]\n5.440\n6.072\n62.946\n4.699\n2.291\nTable 5.1: MSE performance and run-time of the DNN-aided KalmanNet, end-to-\nend RNN, and the model-based EKF, UKF, and PF.\nto successfully track complex trajectories is illustrated in Fig. 5.15, which\nconsiders tracking of the three-dimensional Lorenz attractor chaotic\nsystem, where the model is trained using trajectories of length T = 200\nand evaluated on 3000 samples. The gains of DNN-aided inference are\nnot only in performance, but also in accuracy and run-time. This is\nshowcased in Table 5.1, where the DNN-aided KalmanNet is compared\nwith the direct application of an RNN trained with the same data, as\nwell as model-based variants of the Kalman filter deisnged for non-linear\nsettings – the EKF; unscented Kalman filter (UKF); and particle filter\n(PF). We observe in Table 5.1 that DNN-aided inference leverages data\nto simultaneously improve MSE along with inference speed, as well as\ngeneralization compared with conventional DNNs.\nAdditional gains of the DNN-aided inference based design of Exam-\nple 5.9 include the ability to train in an unsupervised manner, as well as\nthe extraction of uncertainty in the estimate. The former follows from\nthe fact that Kalman filter internally predicts the next observation as\nˆxt|t−1. Hence, one can train the model to produce accurate predictions\nof the future observations, exploiting its awareness of the state evolution\n5.5. DNN-AIDED INFERENCE\n97\nand observation functions. This is achieved by replacing (5.60) with [61]\nθ∗= arg min\nθ\n1\nntT\nnt\nX\ni=1\nT\nX\nt=2\n∥ˆxt|t−1(xi\nt−1; θ) −xi\nt∥2\n2,\n(5.61)\nwhere in (5.61) ˆxt|t−1(xi\nt−1; θ) is the input predicted at time step t\nbased on the inputs up to time t −1 using KalmanNet with parameters\nθ.\nThe second gain implies that the DNN augmented KalmanNet can\nrecover not only the state st, but also provide an estimate of the error\ncovariance matrix. This follows from the fact that the Kalman gain\nmatrix can be in some conditions mapped into the error covariance\nmatrix (Σt|t in (3.31)). In particular, when the state-space model has\nlinear observations H with full column rank, i.e., ˜H =\n\u0000HT H\n\u0001−1 exists,\nthen, filtering with Kalman gain Kt results in estimation with error\ncovariance [62, Thm. 1]\nΣt|t = (I −Kt · H) · ˜HHT (I −H · Kt)−1 HKtH ˜H.\n(5.62)\nConsequently, one can use (5.62) to map the learned Kalman gain\nKθ(xt, ˆst−1) into an estimate of the error covariance. This yields a\nreliable characterization of the error which is robust to mismatches in\nthe state-space model, as illustrated in Fig. 5.16, where KalmanNet\nproduces a better estimate of the uncertainty in its estimate compared\nwith a Kalman filter with both models operating with a mismatched\nstate evolution model.\nExample 5.9 is shown to overcome non-linearities and mismatches\nin the state-space model, outperforming the classical Kalman filter\nwhile retaining its data efficiency and interpretability. This is achieved\nby augmenting the operation of the Kalman filter with a dedicated\nDNN model for computing the specific part of the algorithm flow which\nencapsulates its domain knowledge sensitivity, while converting the\nalgorithm into a discriminative model that is trainable end-to-end.\nA similar rationale can be applied to augmenting subspace methods\nfor DoA estimation, detailed in Examples 3.9-3.11, overcoming their\nassociated limitations to, e.g., coherent and narrowband sources.\n98\nModel-Based Deep Learning\nFigure 5.16: Tracking a single trajectory along with the error regions predicted by\nthe model-based Kalman filter (upper plot) compared with KalmanNet (lower plot)\nwhen tracking based on a mismatched linear state-space model (reproduced from\n[62]).\nExample 5.10. Consider the DoA estimation setting detailed in Exam-\nple 2.3. As discussed in Example 3.9, when the sources are narrowband\nand non-coherent, the covariance of the observations can be decomposed\ninto orthogonal signal and noise subspaces, from which the DoAs can\nbe extracted via, e.g., RootMUSIC (Example 3.11).\nWhen the above assumptions do not necessarily hold, yet one has\naccess to a data set comprised of sequences of observations and their\ncorresponding DoA vectors, i.e., D = {xi\n1, . . . , xi\nTi, si}nt\ni=1, deep learn-\ning can be utilized to learn to compute a surrogate covariance which\nobeys the desired subspace decomposition. In this case, the empirical\ncovariance in (3.70) is replaced with a mapping Cθ(x1, . . . , xT ), where\nCθ(·) is a DNN with parameters θ. Since the DNN is applied to a time\nsequence whose duration T may vary between sequences, a possible\nimplementation uses RNNs [63]. Alternatively, one can set Cθ(·) to be\na convolutional autoencoder applied to the empirical autocorrelation of\nxt, as proposed in [59].\nWhile there is no ground truth surrogate covariance (i.e., a covari-\nance matrix that can be decomposed into orthogonal signal and noise\nsubspaces), the DNN can be trained as part of the RootMUSIC algo-\nrithm, i.e., to optimize the ability of the subspace method to decompose\n5.5. DNN-AIDED INFERENCE\n99\nFigure 5.17: RootMuSIC spectrum obtained when applying classic RootMUSIC\n(left) and its DNN-aided implementation (right) for recovering coherent sources at\nangles 12.5◦, 89◦. It is clearly observed that the DNN-aided version pushes roots that\ndo not correspond to true angles to be more distant from the unit circle (marked\nwith red arrows), thus reducing the chance that they will be mistaken for true DoAs.\nthe learned covariance into orthogonal signal and noise subspaces. By\nletting f(·; θ) be the DoAs estimated by applying RootMUSIC to the\ncovariance matrix computed using Cθ(·), the resulting architecture is\ntrained as a discriminative model via\nθ∗= arg min\nθ\n1\nnt\nnt\nX\ni=1\n∥f(xi\n1, . . . , xi\nTi; θ) −si\nt∥2\n2.\n(5.63)\nIn Example 5.10, the subspace method used to map the learned\nsurrogate covariance into an estimate of the DoAs is RootMUSIC.\nThe selection of this specific subspace method is not arbitrary, but is\nspecifically based on the fact that RootMUSIC translates the estimated\ncovariance into DoAs by seeking the roots of a polynomial, which is\na differentiable mapping. This differentiability is key when converting\nan algorithm into a discriminative model, as deep learning training\nmethods that are used to tune this model utilize gradient methods\nwhich inherently rely on differentiability of the mapping. For this reason,\nRootMUSIC is selected during training, as opposed to MUSIC, whose\ninference rule is based on the non-differentiable procedure of finding\npeaks in the spatial spectrum.\nExample 5.10 thus trains its DNN to produce surrogate covariances\nthat are decomposable into orthogonal subspaces using RootMUSIC.\n100\nModel-Based Deep Learning\n2\n4\n6\n8\nIndex\n0\n1000\n2000\n3000\n4000\n5000\n6000\nEigenvalues [ ]\n2\n4\n6\n8\nIndex\n0\n1\n2\n3\n4\n5\n6\nEigenvalues [ ]\n×105\nFigure 5.18: Eigenvalues of the covariance matrix obtained via conventional co-\nvariance estimation (left) and using an augmented DNN (right) for measurements\ncorresponding to three coherent sources. It is observed that the DNN-aided implemen-\ntation indeed yields a covariance matrix from which one can clearly identify the three\neigenmodes corresponding to the signal subspace, as opposed to the conventional\ncomputation.\nSpecifically, the architecture employed to compute Cθ(x1, . . . , xT ) is\ncomprised of a convolutional autoencoder with three conlvolutional\nlayers and three deconvolution layers with anti-rectifier activations,\nwhose input features are the empirical autocorrelation of x1, . . . , xT .\nThe trained DNN yields a surrogate covariance matrix with RootMUSIC\nspectrum from which one can clearly identify the DoAs, even when\ndealing with coherent sources, as visualized in Fig. 5.17. Once trained,\nthe resulting surrogate covariance mapping is decomposable into noise\nand signal subspaces, as demonstrated in Fig. 5.18, and consequently\ncan be combined with other subspace methods, such as MUSIC, as we\nshow in Fig. 5.19. This reveals a key gain of DNN-aided inference, and\nof model-based deep learning in general, in its ability to preserve the\ninterpretable nature of model-based algorithms and its associated gains\nwhile using deep learning to enable operation in domains where these\nclassic algorithms often fail.\n5.5.2\nExternal DNN Augmentation\nThe DNN-aided inference strategies detailed so far utilize model-based\nalgorithms to carry out inference, while replacing explicit domain-specific\n5.5. DNN-AIDED INFERENCE\n101\n75\n50\n25\n0\n25\n50\n75\nAngels [deg]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAmplitude\n(a) MUSIC spectrum.\n75\n50\n25\n0\n25\n50\n75\nAngels [deg]\n0.2\n0.4\n0.6\n0.8\n1.0\nAmplitude\n(b) DNN-aided MUSIC spectrum.\nFigure 5.19: The spectrum obtained by MUSIC when applied for recovering coherent\nsources located at angles −22◦, 12◦, 50◦.\nFigure 5.20: Neural augmentation illustration.\ncomputations with dedicated DNNs. An alternative approach utilizes the\ncomplete model-based algorithm for inference, i.e., without embedding\ndeep learning into its components, while using an external DNN for\ncorrecting some of its intermediate computations. An illustration of this\napproach is depicted in Fig. 5.20.\nThe main advantage in utilizing an external DNN for correcting\ninternal computations stems from its ability to notably improve the\nrobustness of model-based methods to inaccurate knowledge of the\nunderlying objective parameters. Since the model-based algorithm is\nindividually implemented, one must posses the complete domain knowl-\nedge it requires, and thus the external correction DNN allows the\n102\nModel-Based Deep Learning\nresulting system to overcome inaccuracies in this domain knowledge by\nlearning to correct them from data. Furthermore, the learned correc-\ntion term can improve the performance of model-based algorithms in\nscenarios where they are sub-optimal.\nThe design of external DNN augmented inference systems is com-\nprised of the following steps:\n1. Choose a suitable iterative model-based method for the problem\nof interest, and identify the information exchanged between the\niterations, along with the intermediate computations used to\nproduce this information.\n2. The information exchanged between the iterations is updated with\na correction term learned by a DNN. The DNN is designed to\ncombine the same quantities used by the model-based algorithm,\nonly in a learned fashion.\n3. The overall hybrid model-based/data-driven system is trained in\nan end-to-end fashion, where one considers not only the algorithm\noutputs in the loss function, but also the intermediate outputs of\nthe internal iterations.\nWe next demonstrate how these steps are carried out in order to\naugment smoothing in dynamic systems, as proposed in [64].\nExample 5.11. Consider again a linear Gaussian state-space model, i.e.,\n(5.56) where f(st) ≡F st, h(st) ≡Hst, and the noise signals vt and wt\nare Gaussian with covariance matrices V and W , respectively. Now, we\nare interested in recovering a sequence of T state variables {st}T\nt=1 from\nthe entire observed sequence {xt}T\nt=1. While the loss is still the MSE,\nthe setting is different from the filtering problem where one wishes to\nuse solely {xτ}τ≤t when recovering st. We focus on scenarios where\nthe state-space model, which is available to the inference system, is an\ninaccurate approximation of the true underlying dynamics.\nBy writing s = [s1, . . . , sT ]T , one can estimate by gradient descent\noptimization on the joint log likelihood function, i.e., by iterating over\ns(q+1) = s(q) + γ∇s log p\n\u0010\nx, s(q)\u0011\n(5.64)\n5.5. DNN-AIDED INFERENCE\n103\nwhere γ > 0 is a step-size. The state-space model (5.56) implies that\nthe joint probability density function (PDF) satisfies\np (x, s) = p (x|s) p (s) =\nY\nt\np(xt|st)p(st|st−1).\n(5.65)\nConsequently, it holds that\n∂\n∂st\nlog p (x, s)\n=\n∂\n∂st\nX\nτ\nlog p(xτ|sτ) +\nX\nτ\nlog p(sτ|sτ−1)\n=\n∂\n∂st\nlog p(xt|st) + ∂\n∂st\nlog p(st|st−1) + ∂\n∂st\nlog p(st+1|st)\n=\n∂\n∂st\n(xt −Hst)T W −1(xt −Hst) + ∂\n∂st\n(st −F st−1)T V −1(st −F st−1)\n+ ∂\n∂st\n(st+1 −F st)T V −1(st+1 −F st)\n= HT W −1 (xt −Hst) + −V −1 (st −F st−1) + F T V −1 (st+1 −F st) .\n(5.66)\nConsequently, the tth entry of the log likelihood gradient in (5.64),\nabbreviated henceforth as ∇(q)\nt , can be obtained as ∇(q)\nt\n= µ(q)\nSt−1→St +\nµ(q)\nSt+1→St + µ(q)\nXt→St, where the summands, referred to as messages, are\ngiven by\nµ(q)\nSt−1→St = −V −1 \u0010\ns(q)\nt\n−F s(q)\nt−1\n\u0011\n,\n(5.67a)\nµ(q)\nSt+1→St = F T V −1 \u0010\ns(q)\nt+1 −F s(q)\nt\n\u0011\n,\n(5.67b)\nµ(q)\nXt→St = HT W −1 \u0010\nxt −Hs(q)\nt\n\u0011\n.\n(5.67c)\nThe iterative procedure in (5.64), is repeated until convergence, and\nthe resulting s(q) is used as the estimate.\nThe gradient descent formulation in (5.64) is evaluated by the mes-\nsages (5.67), which in turn rely on accurate knowledge of the state-space\nmodel (5.56). To facilitate operation with inaccurate model knowledge\ndue to, e.g., (5.56) being a linear approximation of a non-linear setup,\none can introduce neural augmentation to learn to correct inaccurate\ncomputations of the log-likelihood gradients. This is achieved by using\n104\nModel-Based Deep Learning\nan external DNN to map the messages in (5.67) into a correction term,\ndenoted ϵ(q+1).\nThe learned mapping of the messages into a correction term operates\nin the form of a graph neural network (GNN). This is implemented by\nmaintaining an internal node variable for each variable in (5.67), denoted\nh(q)\nst for each s(q)\nt\nand hxt for each xt, as well as internal message variables\nm(q)\nV n→St for each message computed by the model-based algorithm\nin (5.67), i.e., V n ∈{St+1, St−1, Xt}. The node variables h(q)\nst\nare\nupdated along with the model-based smoothing algorithm iterations\nas estimates of their corresponding variables, while the variables hxt\nare obtained once from x via a neural network. The GNN then maps\nthe messages produced by the model-based Kalman smoother into its\ninternal messages via a neural network fe(·) which operates on the\ncorresponding node variables, i.e.,\nm(q)\nV n→St = fe\n\u0010\nh(q)\nvn , h(q)\nst , µ(q)\nV n→St\n\u0011\n(5.68)\nwhere h(q)\nxn ≡hxn for each q. These messages are then combined and\nforwarded into a gated recurrent unit (GRU), which produces the refined\nestimate of the node variables {h(q+1)\nst\n} based on their corresponding\nmessages (5.68). Finally, each updated node variable h(q+1)\nst\nis mapped\ninto its corresponding error term ϵ(q+1)\nt\nvia a fourth neural network,\ndenoted fd(·).\nThe correction terms {ϵ(q+1)\nt\n} aggregated into the vector ϵ(q+1) are\nused to update the log-likelihood gradients, resulting in the update\nequation (5.64) replaced with\ns(q+1) = s(q) + γ\n\u0010\n∇s log p\n\u0010\nx, s(q)\u0011\n+ ϵ(q+1)\u0011\n.\n(5.69)\nThe overall architecture is illustrated in Fig. 5.21.\nLet θ be the parameters of the GNN in Fig. 5.21. The hybrid\nsystem is trained end-to-end to minimize the empirical weighted ℓ2\nnorm loss over its intermediate layers, where the contribution of each\niteration to the overall loss increases as the iterative procedure progresses.\nIn particular, letting {(si, xi) = ([si\n1, . . . , si\nT ]T , [xi\n1, . . . , xi\nT ]T )}nt\ni=1 be\nthe training set, the loss function used to train the neural-augmented\n5.5. DNN-AIDED INFERENCE\n105\nFigure 5.21: Neural augmented smoother illustration.\nsmoother is given by\nθ∗= arg min\nθ\n1\nnt\nnt\nX\ni=1\nQ\nX\nq=1\nq\nQ∥si −ˆsq(xi; θ)∥2\n(5.70)\nwhere ˆsq(x; θ) is the estimate produced by the qth iteration, i.e., via\n(5.69), with parameters θ and input x.\n106\nModel-Based Deep Learning\n5.6\nSummary\n• Model-based deep learning encompasses a family of method-\nology that can generally be viewed as a middle ground\nbetween highly-specific model-based methods and highly-\nparameterized deep learning.\n• A repeated rationale in model-based deep learning focuses\non the conversion of model-based algorithms into trainable\narchitectures, and thus differentiability of the algorithm\nwith respect to its internal features is key.\n• A straight-forward methodology is learned optimization,\nwhich uses deep learning tools to tune objective and hyper-\nparameters from data.\n• The popular deep unfolding methodology encompasses sev-\neral different methods with varying levels of parameteri-\nzation, which are all based on data-driven tuning of an\niterative optimizer with a fixed number of iterations.\n• The strategy coined DNN-aided inference is not restricted\nto iterative optimizers, turning an algorithm into a trainable\narchitecture by augmenting specific internal computations\nof the algorithm with DNNs, such that the overall flow is\npreserved while being amenable to end-to-end learning. An\nalternative approach is external DNN augmentation, which\ninstead of replacing an internal computation, carries it out\nin parallel using both a model-based module and a DNN.\n• A form of model-based deep learning which uses generative\nlearning is DNN-aided priors, where the underlying statis-\ntical characterization is implicitly learned in a manner that\ncan be incorporated into a dedicated optimizer.\n6\nConclusions\nIn this monograph, we identified model-based optimization and data-\ndriven deep learning as distinct edges of a spectrum varying in pa-\nrameterization and specificity, and used this representation to provide\na mapping of methodologies for combining classical model-based and\ndata-driven inference via model-based deep learning. To conclude this\noverview, we pinpoint two fundamental questions – why should one\ndesign inference rules via model-based deep learning, and how to ap-\nproach such design tasks. To answer the first question, we summarize\nthe key advantages of model-based deep learning in Section 6.1. To\nclarify how should one approach a model-based deep learning design in\nlight of the proposed categorization, we present guidelines for selecting\na design methodology for a given application, intended to facilitate the\nderivation of future hybrid data-driven/model-based systems.\n6.1\nAdvantages of Model-Based Deep Learning\nThe combination of traditional handcrafted algorithms with data-driven\ntools via model-based deep learning brings forth several key advantages.\nThese can be divided based on their benchmark, whether model-based\nmethods or conventional deep learning.\n107\n108\nConclusions\nAdvantages over Classical Model-Based Methods\nCompared to\npurely model-based schemes, the integration of deep learning effectively\ntrades specificity for abstractness in a controllable fashion. Consequently,\nmodel-based deep learning facilitates inference in complex environments,\nwhere accurately capturing the underlying model in a closed-form\nmathematical expression may be infeasible. For instance, incorporating\nDNN-based implicit regularization was shown to enable super-resolution\nwithout having the impose a prior (Section 5.4), while integrating\nsubspace methods with DNNs enabled their operation in setups involving\ncoherent signals and few snapshots (Section 5.5).\nThe model-agnostic nature of deep learning also allows hybrid model-\nbased/data-driven inference to achieve improved resiliency to model\nuncertainty compared to inferring solely based on domain knowledge.\nFor example, augmenting the model-based Kalman with an RNN can\nimprove its performance when the state-space model does not fully\nreflect the true dynamics (Section 5.5).\nFinally, the fact that hybrid systems learn to carry out part of their\ninference based on data allows to infer with reduced delay and less com-\nputational complexity compared to the corresponding fully model-based\nmethods. This reduction in inference speed is prominent when learning\nto optimize iterative optimizers via learned optimization (Section 5.2),\nand more substantially when using deep unfolding techniques (Sec-\ntion 5.3). Such improved latency can also be obtained when augmenting\na model-based algorithm with a DNN by replacing a specific time-costly\ncomputation with a trainable model of relatively low inference speed.\nAdvantages over Conventional Deep Learning\nCompared to utilizing\nconventional DNN architectures for inference, model-based deep learning\ncan be viewed as using the operation of a task-specific algorithm as\nan inductive bias. Proper selection of an inductive bias is known in\nthe ML literature to facilitate learning. This is translated into a few\nkey advantages. First, the interleaving of deep learning into model-\nbased algorithms yields trainable architectures that are less prone to\noverfitting [30], and the lesser parameterization supports training with\nsmaller data sets compared with highly-parameterized model-agnostic\nDNNs. This gain also translates into improved generalization, as the\n6.2. CHOOSING A MODEL-BASED DEEP LEARNING STRATEGY109\ntrained architecture learns to carry out a suitable algorithm, rather than\njust map inputs to outputs based on the training data. Furthermore,\nwhen a parameterized architecture can be shown to specialize a model-\nbased algorithm suitable for the task at hand, e.g., when using deep\nunfolding with learned objective parameters (Section 5.3), then this\nconfiguration can be used as a principled initialization, again facilitating\nthe training procedure.\nAn additional core gain of model-based deep learning over conven-\ntional deep learning lies in the interpretability of the resulting inference\nrule. This interpretability indicates that one can assign a concrete\nmeaning to internal features of the inference rule, and understand\nthe operation and individual tasks of its internal building blocks. In-\nterpretability improves trustworthiness as one can understand how\ndecisions are made (as shown for, e.g., augmented subspace methods in\nSection 5.5), and is translated into operational gains. These include facil-\nitating training by properly penalizing internal features (as suggested in\nSection 5.3); robustness, particularly to noise, as this is often inherently\nsupported by model-based methods [65]; provide uncertainty and confi-\ndence measures [62]; and enable quick adaptation to variations in the\nunderlying statistical model [66]. Moreover, the fact that model-based\ndeep learning follows the operation of a model-based algorithms, which\nis often associated with theoretical performance guarantees, facilitate\nthe characterization of similar guarantees for some forms of model-based\ndeep learning, as was shown in [67]–[69].\n6.2\nChoosing a Model-Based Deep Learning Strategy\nThe aforementioned gains of model-based deep learning are shared\nat some level by all the different approaches presented in Chapter 5.\nHowever, each strategy is focused on exploiting a different advantage of\nhybrid model-based/data-driven inference, particularly in the context\nof signal processing oriented applications. Consequently, to complement\nthe mapping of model-based deep learning strategies and facilitate the\nimplementation of future application-specific hybrid systems, we next\nenlist the main considerations one should take into account when seeking\nto combine model-based methods with data-driven tools for a given\n110\nConclusions\nproblem.\nStep 1: Domain knowledge and data characterization:\nFirst, one\nmust ensure the availability of the two key ingredients in model-based\ndeep learning, i.e., domain knowledge and data. The former corresponds\nto what is known a priori about the problem at hand, in terms of statis-\ntical models and established assumptions, as well as what is unknown,\nor is based on some approximation that is likely to be inaccurate. The\nlatter addresses the amount of labeled and unlabeled samples one posses\nin advance for the considered problem, as well as whether or not they\nreflect the scenario in which the system is requested to infer in practice.\nStep 2: Identifying a model-based method:\nBased on the available\ndomain knowledge, the next step is to identify a suitable model-based\nalgorithm for the problem. This choice should rely on the portion of the\ndomain knowledge which is available, and not on what is unknown, as\nthe latter can be compensated for by integration of deep learning tools.\nThis stage must also consider the requirements of the inference system\nin terms of performance, complexity, and real-time operation, as these\nare encapsulated in the selection of the algorithm. The identification\nof a model-based algorithm, combined with the availability of domain\nknowledge and data, should also indicate whether model-based deep\nlearning mechanisms are required for the application of interest.\nStep 3: Implementation challenges:\nHaving identified a suitable\nmodel-based algorithm, the selection of the approach to combine it with\ndeep learning should be based on the understanding of its main imple-\nmentation challenges. Some representative issues and their relationship\nwith the recommended model-based deep learning approaches include:\n1. Missing domain knowledge - model-based deep learning can im-\nplement the model-based inference algorithm when parts of the\nunderlying model are unknown, or alternatively, too complex to\nbe captured analytically, by harnessing the model-agnostic nature\nof deep learning. In this case, the selection of the implementation\napproach depends on the format of the identified model-based\n6.2. CHOOSING A MODEL-BASED DEEP LEARNING STRATEGY111\nalgorithm: Iterative optimizers are systematically converted into\ntrainable architectures via deep unfolding (Section 5.3). Similarly,\nwhen the missing domain knowledge is represented as a complex\nsearch domain, DNN-aided priors (Section 5.4) can typically fa-\ncilitate optimization with implicitly learned regularizers. Finally,\nwhen the algorithm is divided into modules with a specific block\nidentified as encompassing the missing domain knowledge, one\ncan naturally augment its operation via DNN-aided inference\n(Section 5.5).\n2. Inaccurate domain knowledge - model-based algorithms are typi-\ncally sensitive to inaccurate knowledge of the underlying model\nand its parameters. In such cases, where one has access to a\ncomplete description of the underlying model up to some uncer-\ntainty, model-based deep learning can robustify the model-based\nalgorithm and learn to achieve improved accuracy. A candidate ap-\nproach to robustify model-based processing is by adding a learned\ncorrection term via external DNN augmentation (Section 5.5.2).\nAlternatively, when the model-based algorithm takes an iterative\nform, improved resiliency can be obtained by unfolding the algo-\nrithm into a DNN while learning either the hyperparameters or\nalso the objective parameters (Section 5.3).\n3. Inference speed - model-based deep learning can learn to im-\nplement iterative inference algorithms, which typically require a\nlarge amount of iterations to converge, with reduced inference\nspeed. This is achieved by by learning the hyperparameters of\nthe optimizer, either on a iteration-dependent manner with fixed\niterations via deep unfolding (Section 5.3) or by fully preserv-\ning the algorithm and tuning its hyperparameters via learned\noptimization (Section 5.2).\nThe aforementioned implementation challenges constitute only a\npartial list of the considerations one should account for when selecting a\nmodel-based deep learning design approach. Additional considerations\ninclude computational capabilities during both training as well as in-\nference; the need to handle variations in the statistical model, which\n112\nConclusions\nin turn translate to a possible requirement to periodically re-train the\nsystem; and the quantity and the type of available data. Nonetheless,\nthe above division provides systematic guidelines which one can utilize\nand possibly extend when seeking to implement an inference system\nrelying on both data and domain knowledge. Finally, we note that some\nof the detailed model-based deep learning strategies may be combined,\nand thus one can select more than a single design approach. For instance,\none can interleave DNN-aided inference via implicitly learned regular-\nization and/or priors, with deep unfolding of the iterative optimization\nalgorithm.\nAcknowledgements\nThe authors are grateful to Elad Sofer, Dor Haim Shmuel, Xiaoyong\nNi, Lital Dabush, and Arbel Yaniv for their help with the numerical\nexamples provided in this monograph. We would also like to thank\nStephen Boyd, Tirza Routtenberg, and Vishal Monga, for the helpful\ndiscussions leading to the formulation of the ideas summarized in this\nmonograph.\n113\nAppendices\nA\nNotations and Abbreviations\nMathematical Notation\nThe following mathematical notations are used:\nR\nThe set of real numbers\nR+\nThe set of non-negative real numbers\nE{·}\nThe stochastic expectation operator\nPr(·)\nThe probability measure\nlog(·)\nThe natural logarithm operator\nexp(·)\nThe natural exponent operator\ndet(·)\nThe determinant operator\nsign(·)\nThe sign operator\n∥· ∥\nThe ℓ2 norm operator\n∥· ∥p\nThe ℓp norm operator\n(·)T\nThe transpose operator\n(·)H\nThe Hermitian transpose operator\n⊙\nElement-wise (Hadamard) product\nI\nThe identity matrix\nN(µ, Σ)\nThe Gaussian distribution with mean µ and covariance Σ\nWe use boldface lower-case and upper-case letters for vectors and ma-\ntrices, respectively, i.e., x is a vector, while X is a matrix. Calligraphic\n115\n116\nNotations and Abbreviations\nletters, e.g., X, are used for sets.\nAbbreviations\nThe following acronyms and abbreviations are used in this monograph:\nADMM\nalternating direction method of multipliers\nAI\nartificial intelligence\nCNN\nconvolutional neural network\nDNN\ndeep neural network\nDoA\ndirection-of-arrival\nEKF\nextended Kalman filter\nEVD\neigenvalue decomposition\nGNN\ngraph neural network\nGRU\ngated recurrent unit\nISTA\niterative soft thresholding algorithm\nLISTA\nlearned ISTA\nMAP\nmaximum a-posteriori probability\nMSE\nmean-squared error\nML\nmachine learning\nPDF\nprobability density function\nReLU\nrectified linear unit\nRNN\nrecurrent neural network\nSGD\nstochastic gradient descent\nReferences\n[1]\nJ. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shannon, “A\nproposal for the Dartmouth summer research project on artificial\nintelligence, August 31, 1955,” AI magazine, vol. 27, no. 4, 2006,\npp. 12–12.\n[2]\nY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature,\nvol. 521, no. 7553, 2015, p. 436.\n[3]\nK. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification,”\nin Proceedings of the IEEE international conference on computer\nvision, pp. 1026–1034, 2015.\n[4]\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T.\nLillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D.\nHassabis, “Mastering the game of go without human knowledge,”\nNature, vol. 550, no. 7676, 2017, pp. 354–359.\n117\n118\nREFERENCES\n[5]\nO. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A.\nDudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev,\nJ. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T.\nCai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T.\nPohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine,\nC. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama,\nD. Wünsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K.\nKavukcuoglu, D. Hassabis, C. Apps, and D. Silver, “Grandmaster\nlevel in StarCraft II using multi-agent reinforcement learning,”\nNature, vol. 575, no. 7782, 2019, pp. 350–354.\n[6]\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hi-\nerarchical text-conditional image generation with clip latents,”\narXiv preprint arXiv:2204.06125, 2022.\n[7]\nT. Karras, S. Laine, and T. Aila, “A style-based generator ar-\nchitecture for generative adversarial networks,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition,\npp. 4401–4410, 2019.\n[8]\nOpenAI, “GPT-4 technical report,” arXiv preprint arXiv:2303.08774,\n2023.\n[9]\nY. Bengio, “Learning deep architectures for AI,” Foundations and\ntrends in Machine Learning, vol. 2, no. 1, 2009, pp. 1–127.\n[10]\nK. Gregor and Y. LeCun, “Learning fast approximations of\nsparse coding,” in International Conference on Machine Learning,\npp. 399–406, 2010.\n[11]\nJ. Chen and X. Ran, “Deep learning with edge computing: A\nreview,” Proceedings of the IEEE, 2019.\n[12]\nV. Monga, Y. Li, and Y. C. Eldar, “Algorithm unrolling: Inter-\npretable, efficient deep learning for signal and image processing,”\nIEEE Signal Processing Magazine, vol. 38, no. 2, 2021, pp. 18–44.\n[13]\nG. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis,\nand R. Willett, “Deep learning techniques for inverse problems in\nimaging,” IEEE Journal on Selected Areas in Information Theory,\nvol. 1, no. 1, 2020, pp. 39–56.\n[14]\nA. Bora, A. Jalal, E. Price, and A. G. Dimakis, “Compressed\nsensing using generative models,” in International Conference on\nMachine Learning, JMLR, pp. 537–546, 2017.\nREFERENCES\n119\n[15]\nY. Yang, J. Sun, H. Li, and Z. Xu, “ADMM-CSNet: A deep learn-\ning approach for image compressive sensing,” IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 42, no. 3, 2018,\npp. 521–538.\n[16]\nD. Gilton, G. Ongie, and R. Willett, “Neumann networks for in-\nverse problems in imaging,” IEEE Transactions on Computational\nImaging, vol. 6, 2019, pp. 328–343.\n[17]\nR. Ahmad, C. A. Bouman, G. T. Buzzard, S. Chan, S. Liu, E. T.\nReehorst, and P. Schniter, “Plug-and-play methods for magnetic\nresonance imaging: Using denoisers for image recovery,” IEEE\nSignal Processing Magazine, vol. 37, no. 1, 2020, pp. 105–116.\n[18]\nW. Dong, P. Wang, W. Yin, G. Shi, F. Wu, and X. Lu, “De-\nnoising prior driven deep neural network for image restoration,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 41, no. 10, 2018, pp. 2305–2318.\n[19]\nH. K. Aggarwal, M. P. Mani, and M. Jacob, “MoDL: Model-\nbased deep learning architecture for inverse problems,” IEEE\nTransactions on Medical Imaging, vol. 38, no. 2, 2018, pp. 394–\n405.\n[20]\nG. Mataev, P. Milanfar, and M. Elad, “DeepRED: Deep image\nprior powered by RED,” in Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision Workshops, 2019.\n[21]\nK. Zhang, L. V. Gool, and R. Timofte, “Deep unfolding network\nfor image super-resolution,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 3217–\n3226, 2020.\n[22]\nN. Shlezinger, N. Farsad, Y. C. Eldar, and A. J. Goldsmith,\n“ViterbiNet: A deep learning based Viterbi algorithm for sym-\nbol detection,” IEEE Transactions on Wireless Communications,\nvol. 19, no. 5, 2020, pp. 3319–3331.\n[23]\nN. Shlezinger, R. Fu, and Y. C. Eldar, “DeepSIC: Deep soft\ninterference cancellation for multiuser MIMO detection,” IEEE\nTransactions on Wireless Communications, vol. 20, no. 2, 2021,\npp. 1349–1362.\n120\nREFERENCES\n[24]\nE. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein,\nand Y. Be’ery, “Deep learning methods for improved decoding of\nlinear codes,” IEEE Journal of Selected Topics in Signal Process-\ning, vol. 12, no. 1, 2018, pp. 119–131.\n[25]\nN. Samuel, T. Diskin, and A. Wiesel, “Learning to detect,” IEEE\nTransactions on Signal Processing, vol. 67, no. 10, 2019, pp. 2554–\n2564.\n[26]\nH. He, C.-K. Wen, S. Jin, and G. Y. Li, “Model-driven deep\nlearning for MIMO detection,” IEEE Transactions on Signal\nProcessing, vol. 68, 2020, pp. 1702–1715.\n[27]\nM. Khani, M. Alizadeh, J. Hoydis, and P. Fleming, “Adaptive\nneural signal detection for massive MIMO,” IEEE Transactions\non Wireless Communications, vol. 19, no. 8, 2020, pp. 5635–5648.\n[28]\nA. Ng and M. Jordan, “On discriminative vs. generative classifiers:\nA comparison of logistic regression and naive Bayes,” Advances\nin neural information processing systems, vol. 14, 2001.\n[29]\nT. Jebara, Machine learning: discriminative and generative, vol. 755.\nSpringer Science & Business Media, 2012.\n[30]\nS. Shalev-Shwartz and S. Ben-David, Understanding machine\nlearning: From theory to algorithms. Cambridge university press,\n2014.\n[31]\nT. M. Cover and J. A. Thomas, Elements of information theory.\nJohn Wiley & Sons, 2012.\n[32]\nA. G. Jaffer, “Maximum likelihood direction finding of stochastic\nsources: A separable solution,” in IEEE International Conference\non Acoustics, Speech and Signal Processing, pp. 2893–2894, 1988.\n[33]\nM. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tuto-\nrial on particle filters for online nonlinear/non-Gaussian Bayesian\ntracking,” IEEE Transactions on Signal Processing, vol. 50, no. 2,\n2002, pp. 174–188.\n[34]\nR. E. Kalman, “A new approach to linear filtering and prediction\nproblems,” Journal of Basic Engineering, vol. 82, no. 1, 1960,\npp. 35–45.\n[35]\nS. P. Boyd and L. Vandenberghe, Convex optimization. Cambridge\nuniversity press, 2004.\nREFERENCES\n121\n[36]\nI. Daubechies, M. Defrise, and C. De Mol, “An iterative thresh-\nolding algorithm for linear inverse problems with a sparsity con-\nstraint,” Communications on Pure and Applied Mathematics: A\nJournal Issued by the Courant Institute of Mathematical Sciences,\nvol. 57, no. 11, 2004, pp. 1413–1457.\n[37]\nS. Boyd, N. Parikh, and E. Chu, Distributed optimization and sta-\ntistical learning via the alternating direction method of multipliers.\nNow Publishers Inc, 2011.\n[38]\nD. P. Williamson and D. B. Shmoys, The design of approximation\nalgorithms. Cambridge university press, 2011.\n[39]\nR. Schmidt, “Multiple emitter location and signal parameter\nestimation,” IEEE Transactions on Antennas and Propagation,\nvol. 34, no. 3, 1986, pp. 276–280.\n[40]\nA. Barabell, “Improving the resolution performance of eigenstructure-\nbased direction-finding algorithms,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\n1983.\n[41]\nI. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT\npress, 2016.\n[42]\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning\ninternal representations by error propagation,” California Univ\nSan Diego La Jolla Inst for Cognitive Science, Tech. Rep., 1985.\n[43]\nI. Sutskever, Training recurrent neural networks. University of\nToronto, 2013.\n[44]\nD. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” arXiv preprint arXiv:1412.6980, 2014.\n[45]\nT. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang, and\nW. Yin, Learning to optimize: A primer and a benchmark, arXiv\npreprint arXiv:2103.12828, 2021.\n[46]\nN. Shlezinger, J. Whang, Y. C. Eldar, and A. G. Dimakis, “Model-\nbased deep learning,” Proceedings of the IEEE, vol. 111, no. 5,\n2023, pp. 465–499.\n[47]\nN. Shlezinger, Y. C. Eldar, and S. P. Boyd, “Model-based deep\nlearning: On the intersection of deep learning and optimization,”\nIEEE Access, vol. 10, 2022, pp. 115 384–115 398.\n122\nREFERENCES\n[48]\nA. Agrawal, S. Barratt, and S. Boyd, “Learning convex optimiza-\ntion models,” IEEE/CAA Journal of Automatica Sinica, vol. 8,\nno. 8, 2021, pp. 1355–1364.\n[49]\nN. Shlezinger and T. Routtenberg, “Discriminative and generative\nlearning for linear estimation of random signals [lecture notes],”\nIEEE Signal Processing Magazine, 2023.\n[50]\nS. Theodoridis, Machine learning: a Bayesian and optimization\nperspective, 2nd ed. Elsevier Science & Technology, 2020.\n[51]\nO. Solomon, R. Cohen, Y. Zhang, Y. Yang, Q. He, J. Luo, R. J.\nvan Sloun, and Y. C. Eldar, “Deep unfolded robust PCA with ap-\nplication to clutter suppression in ultrasound,” IEEE Transactions\non Medical Imaging, vol. 39, no. 4, 2019, pp. 1051–1063.\n[52]\nY. Li, M. Tofighi, J. Geng, V. Monga, and Y. C. Eldar, “Efficient\nand interpretable deep blind image deblurring via algorithm un-\nrolling,” IEEE Transactions on Computational Imaging, vol. 6,\n2020, pp. 666–681.\n[53]\nG. Dardikman-Yoffe and Y. C. Eldar, “Learned SPARCOM: Un-\nfolded deep super-resolution microscopy,” Optics express, vol. 28,\nno. 19, 2020, pp. 27 736–27 763.\n[54]\nY. B. Sahel, J. P. Bryan, B. Cleary, S. L. Farhi, and Y. C. Eldar,\n“Deep unrolled recovery in sparse biological imaging: Achieving\nfast, accurate results,” IEEE Signal Processing Magazine, vol. 39,\nno. 2, 2022, pp. 45–57.\n[55]\nX. Wei, H. van Gorp, L. Gonzalez-Carabarin, D. Freedman, Y. C.\nEldar, and R. J. van Sloun, “Deep unfolding with normalizing\nflow priors for inverse problems,” IEEE Transactions on Signal\nProcessing, vol. 70, 2022, pp. 2962–2971.\n[56]\nN. Shlezinger, N. Farsad, Y. C. Eldar, and A. J. Goldsmith,\n“Learned factor graphs for inference from stationary time se-\nquences,” IEEE Transactions on Signal Processing, vol. 70, 2021,\npp. 366–380.\n[57]\nB. Luijten, R. Cohen, F. J. De Bruijn, H. A. Schmeitz, M. Mischi,\nY. C. Eldar, and R. J. Van Sloun, “Adaptive ultrasound beamform-\ning using deep learning,” IEEE Transactions on Medical Imaging,\nvol. 39, no. 12, 2020, pp. 3967–3978.\nREFERENCES\n123\n[58]\nG. Revach, N. Shlezinger, X. Ni, A. L. Escoriza, R. J. van Sloun,\nand Y. C. Eldar, “KalmanNet: Neural network aided Kalman\nfiltering for partially known dynamics,” IEEE Transactions on\nSignal Processing, vol. 70, 2022, pp. 1532–1547.\n[59]\nD. H. Shmuel, J. P. Merkofer, G. Revach, R. J. van Sloun, and N.\nShlezinger, “Deep root MUSIC algorithm for data-driven DoA es-\ntimation,” in IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2023.\n[60]\nG. Choi, J. Park, N. Shlezinger, Y. C. Eldar, and N. Lee, “Split-\nKalmanNet: A robust model-based deep learning approach for\nstate estimation,” IEEE Transactions on Vehicular Technology,\n2023, early access.\n[61]\nG. Revach, N. Shlezinger, T. Locher, X. Ni, R. J. van Sloun,\nand Y. C. Eldar, “Unsupervised learned Kalman filtering,” in\nEuropean Signal Processing Conference (EUSIPCO), 2022.\n[62]\nI. Klein, G. Revach, N. Shlezinger, J. E. Mehr, R. J. van Sloun,\nand Y. C. Eldar, “Uncertainty in data-driven Kalman filtering\nfor partially known state-space models,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 3194–3198, 2022.\n[63]\nJ. P. Merkofer, G. Revach, N. Shlezinger, and R. J. van Sloun,\n“Deep augmented MUSIC algorithm for data-driven DoA estima-\ntion,” in IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), IEEE, pp. 3598–3602, 2022.\n[64]\nV. Garcia Satorras, Z. Akata, and M. Welling, “Combining gener-\native and discriminative models for hybrid inference,” Advances\nin Neural Information Processing Systems, vol. 32, 2019.\n[65]\nO. Lavi and N. Shlezinger, “Learn to rapidly and robustly optimize\nhybrid precoding,” arXiv preprint arXiv:2301.00369, 2023.\n[66]\nT. Raviv, S. Park, O. Simeone, Y. C. Eldar, and N. Shlezinger,\n“Online meta-learning for hybrid model-based deep receivers,”\nIEEE Transactions on Wireless Communications, 2023, early\naccess.\n124\nREFERENCES\n[67]\nA. Shultzman, E. Azar, M. R. Rodrigues, and Y. C. Eldar, “Gen-\neralization and estimation error bounds for model-based neural\nnetworks,” in The International Conference on Learning Repre-\nsentations (ICLR), 2023.\n[68]\nW. Pu, Y. C. Eldar, and M. R. Rodrigues, “Optimization guar-\nantees for ISTA and ADMM based unfolded networks,” in IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pp. 8687–8691, 2022.\n[69]\nJ. Scarlett, R. Heckel, M. R. Rodrigues, P. Hand, and Y. C.\nEldar, “Theoretical perspectives on deep learning methods in\ninverse problems,” IEEE Journal on Selected Areas in Information\nTheory, vol. 3, no. 3, 2022, pp. 433–453.\n",
  "categories": [
    "eess.SP"
  ],
  "published": "2023-06-05",
  "updated": "2023-06-05"
}