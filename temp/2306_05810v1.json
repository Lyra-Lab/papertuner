{
  "id": "http://arxiv.org/abs/2306.05810v1",
  "title": "Explaining Reinforcement Learning with Shapley Values",
  "authors": [
    "Daniel Beechey",
    "Thomas M. S. Smith",
    "Özgür Şimşek"
  ],
  "abstract": "For reinforcement learning systems to be widely adopted, their users must\nunderstand and trust them. We present a theoretical analysis of explaining\nreinforcement learning using Shapley values, following a principled approach\nfrom game theory for identifying the contribution of individual players to the\noutcome of a cooperative game. We call this general framework Shapley Values\nfor Explaining Reinforcement Learning (SVERL). Our analysis exposes the\nlimitations of earlier uses of Shapley values in reinforcement learning. We\nthen develop an approach that uses Shapley values to explain agent performance.\nIn a variety of domains, SVERL produces meaningful explanations that match and\nsupplement human intuition.",
  "text": "Explaining Reinforcement Learning with Shapley Values\nDaniel Beechey 1 Thomas M. S. Smith 1\n¨Ozg¨ur S¸ims¸ek 1\nAbstract\nFor reinforcement learning systems to be widely\nadopted, their users must understand and trust\nthem. We present a theoretical analysis of explain-\ning reinforcement learning using Shapley values,\nfollowing a principled approach from game theory\nfor identifying the contribution of individual play-\ners to the outcome of a cooperative game. We call\nthis general framework Shapley Values for Ex-\nplaining Reinforcement Learning (SVERL). Our\nanalysis exposes the limitations of earlier uses\nof Shapley values in reinforcement learning. We\nthen develop an approach that uses Shapley values\nto explain agent performance. In a variety of do-\nmains, SVERL produces meaningful explanations\nthat match and supplement human intuition.\n1. Introduction\nReinforcement learning systems have potential for signif-\nicant impact in real-world applications.\nTo be widely\nadopted, it is useful for these systems to not only perform\nwell but also be explainable.\nMethods for explaining reinforcement learning can be cate-\ngorised as intrinsically interpretable or post-hoc. Intrinsi-\ncally interpretable approaches improve the transparency of\nmodels by substituting an opaque model with a more under-\nstandable one, such as a decision tree. This approach often\nleads to a reduction in representational power. In contrast,\npost-hoc methods hold no constraints on the complexity\nof the model, treating it as a black box. Reinforcement\nlearning systems with the largest potential to positively ben-\nefit society depend on function approximators with large\nrepresentational power, such as deep neural networks. We\ntherefore focus on post-hoc explanation methods.\nAn established, post-hoc explanation method for supervised\nlearning uses Shapley values (Shapley, 1953), a principled\n1Department of Computer Science, University of Bath, UK.\nCorrespondence to: Daniel Beechey <djeb20@bath.ac.uk>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\napproach from game theory for identifying the contribution\nof individual players to the outcome of a cooperative game.\nShapley values are the result of a rigorous mathematical for-\nmulation that satisfies four axioms of fairness. In supervised\nlearning, Shapley values explain a model by expressing the\ncontribution of individual features to the predictions of the\nmodel.\nWe analyse, from first principles, how Shapley values can be\nused to explain reinforcement learning. We make three main\ncontributions. First, we develop a theoretical framework for\nusing Shapley values in the context of reinforcement learn-\ning, showing that earlier uses of Shapley values in reinforce-\nment learning are incorrect or incomplete. Secondly, we\nconsider which aspects of reinforcement learning are impor-\ntant to explain, arguing that explaining agent performance is\nan important and overlooked element. Thirdly, we develop a\nprincipled approach that identifies the contributions of state\nfeatures to the performance of an agent.\nWe call this general framework Shapley Values for Ex-\nplaining Reinforcement Learning (SVERL). In a variety\nof domains, SVERL produces meaningful explanations that\nmatch and supplement human intuition.\n2. Background\nWe model the interaction of an agent with its environment\nas a Markov Decision Process (MDP), defined by the tu-\nple (S, A, p, r, γ, p0), where S denotes the set of states, A\nthe set of actions, p : S × A × S →[0, 1] the transition\ndynamics, r : S × A →R the reward function, γ ∈[0, 1]\nthe discount factor, and p0 : S →[0, 1] the initial state\ndistribution. At decision stage t, t ≥0, the agent observes\nthe current state of the environment, st ∈S, and executes\naction at ∈A(st). Consequently, the environment transi-\ntions to a new state, st+1 ∼p(·|st, at), and returns reward\nrt+1 whose expected value is r(st, at). The objective is to\nlearn a policy π that maximises the expected return Eπ[Gt],\nwhere Gt = P∞\nk=t γkrk+1. The policy can be stochastic,\nπ : S × A →[0, 1], or deterministic, π : S →A. A state-\nvalue function, V π(s), gives the expected return from state s\nwhen following policy π, V π(s) = Eπ[Gt|st = s]. A state-\naction value function, Qπ(s, a), gives the expected return\nfrom state s if the agent executes action a and follows policy\nπ thereafter, Qπ(s, a) = Eπ[Gt|st = s, at = a]. The opti-\n1\narXiv:2306.05810v1  [cs.LG]  9 Jun 2023\nExplaining Reinforcement Learning with Shapley Values\nmal state value function is denoted by V ∗and the optimal\nstate-action value function by Q∗.\nWe assume that an environment has a set of n state features,\nF = {0, . . . , n −1}, where we can decompose the state\nspace according to the state features, S = S0 × . . . × Sn−1,\nand each state can be represented as an ordered set: s =\n{si|si ∈Si}n−1\ni=0 . For example, in a classic gridworld do-\nmain, a state could be the agent’s location, with x and y\ncoordinates as state features. Let C ⊂F be a set of observ-\nable state features. Then a partial observation of a state is\nthe ordered set sC = {si|i ∈C}.\nShapley values assign the contributions of individual players\nto the outcome of a cooperative game (Shapley, 1953). They\nare the unique solution to a set of mathematical axioms\nthat specify fair distribution of credit across players. A\ncooperative game is defined by a set F of players and a\ncharacteristic value function v : 2|F| →R, where v (C)\nreturns the outcome of the game when played by some\ncoalition of players C ⊆F, with v(∅) = 0. The Shapley\nvalue of player i in the game (F, v) is:\nϕi (v) =\nX\nC⊆F\\{i}\n|C|! (|F| −|C| −1)!\n|F|!\n· δ (i, C) ,\n(1)\nwhere δ (i, C) = v (C ∪{i}) −v (C) is the marginal gain\nin characteristic value when player i joins coalition C. As\nan example, the employees of a company can be modelled\nas players in a game where profit is the characteristic value\nfunction.\nShapley values have been adopted in machine learning to\ndetermine the contribution of features to the predictions of\nsupervised learning models (Lipovetsky & Conklin, 2001).\nLet fF : X →Y be a supervised learning model defined over\na set of n features, F = {0, . . . , n−1}, such that X = X0 ×\n. . .×Xn−1 and each x ∈X can be represented as an ordered\nset, x = {xi|xi ∈Xi}n−1\ni=0 . Then Shapley values show the\ncontribution of feature xi ∈x to the target y = fF(x)\nfor the single point x. As an example, when predicting\nthe quality of wine using features such as acidity, pH, and\nalcohol (Cortez et al., 2009), the Shapley values show how\nmuch each feature contributes to the predicted quality of\na specific wine. This is done by modelling the prediction\nat x as a game, where the features {x0, . . . , xn−1} are the\nplayers and the target prediction y = fF(x) is the outcome\nof the game. Then the Shapley values ϕi(f, x), specifying\nthe contribution of feature xi to the prediction y = f(x),\nare computed using the characteristic value function:\nvf(C) := fC(x),\nwhere C ⊆F and fC(x) is the model’s prediction for the\nordered set xC = {xi|i ∈C}. The resulting Shapley values\nsatisfy fF(x) = vf(∅) + P\ni∈F ϕi(f, x).\nShapley values show each feature’s contribution to the\nchange in prediction when all features are known, fF(x),\ncompared to when no features are known, f∅(x) = vf(∅).\nIn game theory, the value of a game with no players is zero.\nHence v(∅) = 0. In supervised learning, the prediction\nwhen no features are known is the expected model predic-\ntion over the data distribution. Hence vf(∅) = Ep(x)[f(x)],\nwhere p(x) is the data distribution, the probability that a\nrandomly sampled point from X equals x.\nComputing Shapley values requires predictions, fC(x), to\nbe made for all subsets of features, C ⊆F. The original\napproach to approximating such predictions was to retrain\nthe model for all C ⊆F (ˇStrumbelj et al., 2009). With a\nlarge number of features, this is infeasible. An alternative\nmethod defines the prediction at x with subset of features C\nas:\nfC(x) = Ep(x′)\n\u0002\nfF(xC ∪x′\n¯C)\n\u0003\n,\n(2)\nwhere p(x′) is the data distribution (ˇStrumbelj &\nKononenko, 2010; 2014). Equation (2) can be approximated\nby marginalising over possible values for the unobserved\nfeatures ¯C = F \\ C. Assuming independent features and\nsampling n data points,\nfC(x) = lim\nn→∞\n1\nn\nX\nx′∼p(x′)\nfF(xC ∪x′\n¯C).\n(3)\nUsing Equation (3), an unbiased approximation algorithm\nfor calculating Shapley values samples a marginal gain:\nˆδ(i, C) = fF(xC∪{i} ∪x′\nC∪{i}) −fF(xC ∪x′\n¯C),\n(4)\nwhere the coalition C ⊆F \\ {i} is sampled proportional\nto the multinomial term in Equation (1) and x′ ∼p(x′).\nThe mean of these samples is the Shapley value in the limit\n(ˇStrumbelj & Kononenko, 2010). This algorithm does not\nrequire retraining the models. It is one of the approximations\nused in the popular python package SHAP (Lundberg &\nLee, 2017), which calculates Shapley values for an arbitrary\nmachine learning model. There are other approximations\nincluded in SHAP; they all approximate Equation (2) in\nsome way.\nEquation (2) is referred to as off-manifold. It makes the sim-\nplifying assumption that the features are independent. When\nfeatures are correlated, this assumption samples points\nxC ∪x′\n¯C that may not lie on the data manifold. Without this\nsimplifying assumption, the prediction at x with subset of\nfeatures C becomes:\nfC(x) = Ep(x′|xC) [fF(x′)] ,\n(5)\nwhere the conditional data distribution p(x′|xC) takes into\naccount the feature correlations (Frye et al., 2020). An on-\nmanifold sampling method that uses Equation (4) but now\n2\nExplaining Reinforcement Learning with Shapley Values\nsamples x′ ∼p(x′|xC) can then be used to approximate\nShapley values for models with correlated features.\nShapley values ϕi(f, x) provide the local contribution of\nfeatures to a prediction. The local contributions can be\ncombined to identify the global Shapley value for a feature,\nproducing the mean contribution of a feature to a model’s\npredictions: Φi(f) = Ep(x) [ϕi(f, x)]. If we consider a new\ncharacteristic value function, defined using a model’s loss ℓ,\nvℓ(C) := ℓ(f∅(x), y) −ℓ(fC(x), y), then global Shapley\nvalues can be interpreted as the contribution of feature i to\nthe model’s prediction accuracy (Covert et al., 2020):\nΦi(f) = Ep(x)\n\u0002\nϕi(vℓ, x)\n\u0003\n.\nIn reinforcement learning, earlier work has directly applied\nthe SHAP package to an agent’s policy (Rizzo et al., 2019;\nWang et al., 2020; He et al., 2021; Remman et al., 2022;\nLøver et al., 2021; Liessner et al., 2021) or state-value func-\ntion (Zhang et al., 2020; 2021) in an effort to explain re-\ninforcement learning in specific applications. This earlier\nwork implicitly assumes that the state features are inde-\npendent because SHAP implements only off-manifold ap-\nproximations. More importantly, this earlier work has not\nexplored the theoretical basis for what the resulting Shapley\nvalues mean in the context of reinforcement learning.\nIn the following sections, we present a theoretical and empir-\nical analysis of how Shapley values can be used to explain\nreinforcement learning, starting from first principles. We\nrefer to this general framework as Shapley Values for Ex-\nplaining Reinforcement Learning (SVERL).\n3. Using Shapley Values to Explain\nReinforcement Learning\nWe start by exploring the use of Shapley values to explain\nthe value function and the policy of an agent. Our analysis\nshows that (1) applying Shapley values to a value function\nproduces explanations that have no relation to the perfor-\nmance or behaviour of an agent, and (2) applying Shapley\nvalues to policies explains the contribution of state features\nto an agent’s decisions but not to its performance.\nShapley values applied to value functions. In order to\nuse Shapley values to explain an agent’s value function, we\nfollow the theory of on-manifold Shapley values in super-\nvised learning to propose the following characteristic value\nfunctions for V and Q:\nv\nˆV (C) := ˆV π\nC (s) =\nX\ns′∈S\npπ(s′|sC) ˆV π(s′)\n(6)\nv\nˆ\nQ (C) := ˆQπ\nC(s, a) =\nX\ns′∈S\npπ(s′|sC) ˆQπ(s′, a)\n(7)\nEquations (6) and (7) account for feature correlations by\nusing the conditional limiting state occupancy distribution\npπ(s′|sC), the probability of being in state s′ given that sC\nis observed and the agent is following policy π.\nShapley values resulting from Equation (6) satisfy v ˆV (F) =\nv ˆV (∅) + P\ni∈F ϕi(v ˆV , s). They show each feature’s contri-\nbution to the change in characteristic value when all state\nfeatures are observed, ˆV π(s), compared to when no state\nfeatures are observed, ˆV π\n∅(s). This observation also holds\nfor Equation (7) and all other characteristic value functions\nfor reinforcement learning presented in this paper.\nOne might expect the Shapley values resulting from Equa-\ntions (6) and (7) to relate to performance in some way, given\nthat a value function represents an agent’s prediction of\nhow well its policy performs. However, these characteristic\nvalue functions refer to the expected return of the agent’s\noriginal policy π. Not observing a state feature is likely\nto change an agent’s policy, which in turn changes the ex-\npected return. By never evaluating any change in policy, the\nfull consequences of removing state features are not being\nconsidered. Consequently, the resulting explanations do not\nmeaningfully relate to performance or behaviour.\nInstead, Shapley values applied to the value function ex-\nplain the contribution of each feature to the predictions of\nthe value function—but only under the assumption that all\nfeatures will be observed by the agent when acting in the\nenvironment. This is a subtle but important point. Shap-\nley values applied to the value function do not explain the\nagent’s performance; they explain the value function as a\npredictor—but without considering the impact of features\non behaviour.\nWe use two examples to illustrate the difference between\nexplaining the value function as a predictor and explaining\nagent performance. We use Equation (7) to apply Shap-\nley values to Q∗in Gridworld-A, shown in Figure 1a, and\nEquation (6) to apply Shapley values to V ∗in Tic-Tac-Toe.\nIn Gridworld-A, the optimal action is North (N) in each\nstate. Intuitively, if the optimal action is the same in all\nstates, then the contribution of each state feature to perfor-\nmance should be zero. However, Shapley values applied to\nQ∗(s, N), shown in Figure 2 (top panel), produce non-zero\ncontributions for the y state feature.\nTo explore why, consider the contribution of y in state 1.\nIf neither x nor y is known, the agent is equally likely to\nbe in states 1, 2, 3, or 4 (we are ignoring terminal states),\nwith Q∗(s, N) values of 8, 8, 9, and 9, respectively. Con-\nsequently, the predicted return is 8.5. Now consider the\nmarginal gain from observing y. If y is known to be 1 and\nx remains unknown, the agent is equally likely to be in\nstates 1 and 2, with Q∗(s, N) = 8 for both states, yielding\na predicted return of 8. Hence, the marginal gain in pre-\ndiction from observing y is 8 −8.5 = −0.5. Similarly, if\nx is known to be 1 and y is unknown, the agent is equally\n3\nExplaining Reinforcement Learning with Shapley Values\n1\n2\n4\nG\nG\nx\n3\ny\n(a) Gridworld-A\n1\n2\n3\n5\n4\nG\nG\nx\ny\n(b) Gridworld-B\n1\n2\n3\n5\n4\nG\nx\ny\n(c) Gridworld-C\nFigure 1. Deterministic gridworlds, with actions North, South,\nEast, and West. The numbers in each grid square show the state\nidentifier. The initial state is either state 1 or state 2, with equal\nprobability. The reward is −1 for each action and an additional\n+10 for transitions into a terminal state (G). The discount factor\nγ is 1. State features are the x and y coordinates. The red arrows\nshow the optimal action in each state.\nlikely to be in states 1 and 3, with Q∗(s, N) values of 8 and\n9, respectively, yielding a predicted return of 8.5. If y is\nalso known, then predicted return is Q∗(1, N) = 8. Hence,\nthe marginal gain in prediction when observing y is again\n8 −8.5 = −0.5. Both marginal gains are −0.5, resulting in\na Shapley value of −0.5 for y in state 1.\nIn contrast, the actual return from state 1 is 8, whatever\ncombination of features is observed, because the optimal\npolicy selects North in every state. Human intuition there-\nfore assigns a contribution of 0 because observing y does\nnot change the agent’s behaviour or expected return. Shap-\nley values applied to the value function is not capable of\ncapturing this relationship.\nIn Tic-Tac-Toe, there are 9 features, corresponding to each\nboard position, with possible values X, O or empty. Con-\nsider an agent (X) that uses V ∗to play optimally against an\nopponent (O) that follows a Minimax policy (Polak, 1989).\nThe reward is −1 for losing and 0 for drawing, the only pos-\nsible outcomes when playing against Minimax. In the state\nshown in Figure 3, the two squares marked by the opponent\ninform the agent that it needs to make a blocking move.\nIntuitively, we would expect the corresponding two state\nfeatures to impact the performance of the agent. However,\nthe feature contributions identified by applying Shapley val-\nues to V ∗are zero for every state feature. The reason is\nthat an optimal agent always draws, hence the optimal value\nfunction always predicts a return of zero, independently of\nwhich state features are observed. These Shapley values\nexplain the value function as a static predictor. They do\nnot consider that the value function depends on the policy,\nwhich would change in the absence of some state features.\n1\n2\n3\n4\n−0.5\n0.0\n0.5\n0.00\n0.00\n0.00\n0.00\n-0.50\n-0.50\n0.50\n0.50\nShapley Values Applied to Q∗(s, North)\nx\ny\n1\n2\n3\n4\n−0.5\n0.0\n0.5\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nSVERL-P\nState\nShapley Value\nFigure 2. Top panel: Shapley values applied to a state-action value\nfunction in Gridworld-A (Figure 1a). Bottom panel: SVERL-P,\npresented in Section 4, for the same domain. The optimal action is\nalways North so we intuitively expect contributions to performance\nto be zero for all state features in all states. This is accurately\ncaptured by SVERL-P but not by Shapley values applied to the\nvalue function.\nShapley values applied to policies. We follow the theory\nof on-manifold Shapley values in supervised learning to\npropose the following characteristic value function for a\npolicy ˜π : S →A that outputs actions:\nv˜π (C) := ˜πC(s) =\nX\ns′∈S\np˜π(s′|sC)˜π(s′),\n(8)\nand the following characteristic value function for a policy\nπ : S × A →[0, 1] that outputs action probabilities:\nvπ (C) := πC(a|s) =\nX\ns′∈S\npπ(s′|sC)π(a|s′).\n(9)\nEquations (8) and (9) account for feature correlations by\nusing the conditional limiting state occupancy distribution\npπ(s′|sC), as in Equations (6) and (7). We note that Equa-\ntion (8) is not valid in discrete action spaces because it is\nnot meaningful to sum discrete actions.\nThe characteristic value functions in Equations (8) and (9)\nproduce Shapley values that show the contribution of state\nfeatures, respectively, to the action selected by an agent\nand to the probability of selecting action a. Both values\nprovide information on the contributions of state features to\nthe decision made by the agent. This insight is valuable but\nwe argue that there is more to be understood and explained\nabout the decision. Specifically, these Shapley values reveal\nno insight into the importance of state features for an agent’s\nperformance.\n4\nExplaining Reinforcement Learning with Shapley Values\nX\nO\nO\nShapley Values\nApplied to V ∗\nX\nO\nO\nSVERL-P\n0.0\n0.1\n0.2\n0.3\nFigure 3. On the left: Shapley values applied to a state-value func-\ntion in a Tic-Tac-Toe state. On the right: SVERL-P, presented in\nSection 4, for the same state. Shapley values are represented using\na color scale projected onto each cell. There are 9 state features,\ncorresponding to each position on the board, with possible values\nX, O or empty. The agent plays as X against opponent O.\nAs an illustrative example, imagine an agent planning the\nshortest route through a city. The agent arrives at a junc-\ntion where turning left and turning right both result in an\noptimal route. Assume that the agent’s policy is to turn\nleft if it observes a road sign (a state feature), and to turn\nright otherwise. Shapley values applied to the agent’s policy\nwould assign a large contribution to the road sign, which\nis justified and improves our understanding of the agent’s\nbehaviour. The sign was indeed instrumental in the agent’s\ndecision to turn left. However, one would be incorrect to\nthen conclude that the sign is important for the agent to\nperform well. On the contrary, because turning left and\nturning right are both optimal, the sign contributes nothing\nto the agent’s performance. This insight can be gained only\nby considering the effect of removing state features on the\nagent’s performance. Therefore, we make a distinction be-\ntween explaining why the agent acted in a specific way and\nexplaining how features impact agent performance.\nThe contributions of state features to the value function or\nto the policy do not reveal insight into contributions to agent\nperformance. These two approaches consider either the\ncontributions to predicting expected return independent of\nbehaviour or the contributions to behaviour independent of\nexpected return. We have highlighted the limitations of both\napproaches. Next we propose an approach to explaining\nreinforcement learning by identifying contributions of state\nfeatures to agent performance.\n4. Explaining Agent Performance\nHere we provide a formulation of Shapley values to ex-\nplain the performance of a reinforcement learning agent.\nWe present two methods that explain either the local or the\nglobal contributions of state features to performance. Each\napproach reveals unique insight that improves understand-\ning. In both approaches, state features are removed from an\nagent’s observation for certain states, then the performance\nof the resulting policy is evaluated using expected return.\nWe call this approach SVERL-Performance (SVERL-P).\nLocal explanations. Local SVERL-P explains the contri-\nbutions of state features to performance from state s by\nconsidering removing state features from an agent’s obser-\nvation of state s. For some policy π : S × A →[0, 1] to be\nexplained, the local SVERL-P characteristic value function\nis given by:\nvlocal(C) := Eˆπ\n\" ∞\nX\nt=0\nγtrt+1|s0 = s\n#\n,\n(10)\nwhere ˆπ(at|st) =\n(\nπC (at|st)\nif st = s,\nπ(at|st)\notherwise.\nShapley values resulting from Equation (10) show the con-\ntribution of each feature to the change in performance when\nall state features are observed in state s, vlocal(F), compared\nto when no state features are observed in state s, vlocal(∅).\nIn most problems, state features are not independent, so we\nuse the theory for on-manifold Shapley values to propose\nsampling a from the agent’s policy given that it observes\nsC:\nπC(a|s) = Epπ(s′|sC) [π(a|s′)] ,\n(11)\nwhere we suggest the conditional data distribution in Equa-\ntion (5) becomes the conditional limiting state occupancy\ndistribution pπ(s′|sC).\nThe resulting explanations are specific to the policy used,\nwhich can be any possible policy, including a suboptimal\npolicy. One can interpret πC as the policy that best tries\nto match the behaviour of the original policy π given that\nfeatures are missing. Policy πC will not usually be able\nto perfectly mimic the behaviour of policy π. It is exactly\nthis difference in behaviour that causes the change in perfor-\nmance.\nGlobal explanations. Local SVERL-P considers the contri-\nbutions of state features to performance from a single state.\nIn addition to such local contributions, one may wish to un-\nderstand the contributions of state features to performance\nglobally. For example, in autonomous driving, a user may\nwish to understand which parts of an autonomous vehicle’s\nobservations are most important for driving performance,\nto focus resources on improving those parts of the road\nsystem. Some state features might contribute substantially\nto performance in certain states, such as breaking when\nobserving a human or pulling over when an ambulance ap-\nproaches, while road markings may be globally important\nby contributing to agent performance in many states.\n5\nExplaining Reinforcement Learning with Shapley Values\nTo quantify the global impact of state features on agent per-\nformance, we consider the effect of removing state features\nfrom every state in an environment. The corresponding\n(global) SVERL-P characteristic value function is as fol-\nlows:\nvglobal(C) := EπC\n\" ∞\nX\nt=0\nγtrt+1|s0 = s\n#\n.\n(12)\nEquation (12) produces Shapley values that show the con-\ntribution of state features to performance in state s and all\nfuture states that follow. These Shapley values are still\nconditioned on state and therefore not a truly global expla-\nnation method. To produce a fully global explanation, one\ncan marginalise over the state space using the limiting state\noccupancy distribution, producing global SVERL-P:\nΦi(vglobal) = Epπ(s)\n\u0002\nϕi\n\u0000vglobal, s\n\u0001\u0003\n.\n(13)\nEquation (13) gives the contribution of a state feature to the\nperformance of the agent in its environment. An alterna-\ntive is to marginalise over the initial state distribution p0,\nwhich would place undue attention on the initial states and\nis therefore less useful in infinite-horizon problems.\n5. Experiments\nWe present experimental results in a variety of domains. We\ncontrast SVERL-P with applying Shapley values to policies\nand to value functions, demonstrating the limitations of\nthe latter approaches. All Shapley values are calculated\nexactly, as described in Appendix C. The domains are fully\ndescribed in Appendix A.\nGridworld-B. We first consider Gridworld-B, shown in\nFigure 1b. Imagine an agent acting optimally: choosing\nEast (E) in state 1 and North (N) in every other state.\nConsider local explanations for specific states. Whatever the\nstate, if neither x nor y is known, the agent cannot know the\noptimal action with certainty but it knows that the optimal\naction is either N or E and that N is more likely than E.\nIn states 3 and 4, either the x or the y feature is sufficient for\nthe agent to take the optimal action N; in other words, x and\ny features make an equal contribution to agent performance.\nFurthermore, this contribution is rather small because, if\nneither feature is known, N is still the likely optimal action.\nIn state 1, the x feature is sufficient for the agent to take the\noptimal action E (because an optimal agent is never in state\n5). The y feature also improves the agent’s performance,\nbut by a smaller amount, because it increases the probability\nof the agent selecting the optimal action E. In sum, the x\nand y features contribute positively to agent performance,\nwith the x feature contributing more.\n1\n2\n3\n4\n0\n5\nShapley Value\n4.00\n0.33\n0.08\n0.08\n2.00\n-0.17\n0.08\n0.08\nSVERL-P\nx\ny\n1\n2\n3\n4\nState\n0\n5\nShapley Value\n-1.19\n0.31\n0.14\n0.14\n-0.52\n-1.02\n0.14\n1.14\nShapley Values Applied to V ∗\nFigure 4. Shapley values of x and y state features in Gridworld-B.\nTop panel: SVERL-P. Bottom panel: Shapley values applied to a\nvalue function.\nIn state 2, the x feature is sufficient for the agent to take the\noptimal action N while the y feature actually decreases the\nprobability of selecting the optimal action N (it increases\nthe probability of selecting the suboptimal action E). The x\nfeature therefore makes a positive contribution to agent per-\nformance while the y feature makes a negative contribution.\nLocal SVERL-P contributions are shown in the top panel in\nFigure 4. SVERL-P values align with our intuitive analysis\nof the domain. As expected, in states 3 and 4, both x and\ny contribute a small, equal amount to agent performance.\nAlso as expected, in state 1, x contributes more to perfor-\nmance than y. And we can now quantify the difference\nprecisely: x contributes exactly twice as much as y. In state\n2, Shapley values once again mirror our expectations, with\nx contributing positively to performance and y contributing\nnegatively—a reminder that a little bit of knowledge can be\na dangerous thing.\nNext, consider the global contribution of the two features.\nBased on the discussion above, the x feature positively\ncontributes larger amounts, more often, to the agent’s perfor-\nmance than the y feature. Therefore, we expect the global\ncontribution of the x feature to be larger than that of the\ny feature. These expectations align with global SVERL-P\ncontributions: 1.43 for x and 0.64 for y.\nSVERL-P has correctly and precisely expressed the local\nand global contribution of the features x and y to perfor-\nmance. It has done so in more detail and precision than our\nintuitive expectations, demonstrating the value of SVERL-P\neven in such a simple domain.\n6\nExplaining Reinforcement Learning with Shapley Values\n0\n0\n1\n0\n1\n2\n0\n1\n0\n1\n1\n1\nM1\nM2?\nM2?\n1\n2\n3\n4\ny\nSVERL-P\n0\n0\n1\n0\n1\n2\n0\n1\n0\n1\n1\n1\nM1\nM2?\nM2?\nShapley Values\nApplied to V π\n0\n0\n1\n0\n1\n2\n0\n1\n2\n0\n1\n1\n1\nM1\nM2\n1\n1\n2\n2\n3\n3\n4\n4\nx\ny\n0\n0\n1\n0\n1\n2\n0\n1\n2\n0\n1\n1\n1\nM1\nM2\n1\n2\n3\n4\nx\n−4.39\n0.00\n+4.39\n−8.62\n0.00\n+8.62\n−0.1\n0.00\n+0.1\n−0.09\n0.00\n+0.09\nFigure 5. Shapley value contributions for two successive states (top\nto bottom) of Minesweeper, represented as the color of each cell.\nOn the left: SVERL-P. On the right: Shapley values applied to a\nvalue function. The domain contains two mines, hidden from the\nagent. In the top state, the state features reveal the exact location of\none mine and two potential locations for the second mine, marked\nfor reference as “M1” and “M2?” respectively. The exact location\nof the second mine is then revealed in the second state, marked as\n“M2” for reference.\nMinesweeper. This is a relatively large domain, with ap-\nproximately 175,000 states, where it can be difficult to iden-\ntify how individual state features contribute to performance\nby reasoning alone. By using SVERL-P, we find local ex-\nplanations of performance that reveal novel insight into the\ntwo successive Minesweeper states shown in Figure 5.\nThe features in this domain are the 16 grid squares, with\npossible values 0, 1, 2, or unopened. Figure 5 shows that\none feature in particular (x = 4, y = 2) contributes substan-\ntially to performance in both states, with all other features\ncontributing relatively little in comparison. On further in-\nspection, we see that the feature (4, 2) is the only feature\nthat can exactly determine the location of M2. On the other\nhand, many features reveal the exact location of M1. To\nact optimally, the agent must determine the exact location\nof M2 so the feature (4, 2) is the most important one for\ncompleting the episode successfully.\nNotice the negative SVERL-P contributions for the squares\nwith possible mines. These are discussed in detail in Ap-\npendix B.\nTaxi. In the taxi domain (Dietterich, 1998), the agent picks\nup a passenger and drops them off at their destination. Re-\nwards are −1 for all actions, an additional +20 for dropping\na passenger at the correct destination, and an additional −10\nfor attempting to pick up or drop off the passenger at an\ninappropriate location. We examine the two states shown in\nFigure 6.\nIn the state shown on the top panel, to successfully com-\nplete the episode, the agent must first pick up the passenger.\nKnowledge of the passenger location is therefore vital and\nwe expect this feature to contribute a large amount to perfor-\nmance. This is captured by SVERL-P, as shown in Figure 6.\nConversely, until the passenger has been collected, we do\nnot expect the destination location to contribute positively\nto performance. Surprisingly, SVERL-P shows that ob-\nserving the destination location actually reduces the agent\nperformance. Upon closer review, we see that, in this state,\nobserving the destination location without the passenger\nlocation increases the probability of navigating towards the\ndestination, which is a suboptimal action.\nSVERL-P also shows that the x feature has a relatively low\ncontribution to performance compared to the y feature. Con-\nsider an agent that observes x = 4 but cannot observe its\ny coordinate. There are five possibilities for the value of y.\nOne of them, y = 1, would result in executing the pick-up\naction. Not being able to observe y increases the probability\nof choosing this action and earning a large negative reward,\nreducing the agent’s expected return. By observing y along\nwith x, the agent eliminates the possibility of inappropriate\nexecution of the pick-up action, leading to a large marginal\ncontribution to performance by y. Inappropriately execut-\ning the pick-up or drop-off action is highly detrimental to\nperformance. Features that decrease this probability are the\nlargest positive contributors to performance.\nIn the state shown on the lower panel in Figure 6, the pas-\nsenger is in the taxi, to be dropped off at location B. The\noptimal policy navigates to the drop-off location with the\npassenger in taxi. Intuitively, both the passenger and the\ndestination location are important, as shown by the SVERL-\nP contributions. The x and y state feature contributions\nare similar to those in the state discussed previously, for\nsimilar reasons—observing x often increases the probability\nof inappropriately executing the drop-off action, whereas\nobserving y decreases it.\nSVERL-P compared with Shapley values applied to\nvalue functions. The domains Gridworld-A and Tic-Tac-\nToe were used in Section 3 to demonstrate that applying\nShapley values to an agent’s value function does not explain\nagent performance. In contrast, local SVERL-P contribu-\ntions in these domains, shown in Figures 2 and 3, match our\nintuitive understanding of the contribution of state features\nto performance.\nAs a result of purposely choosing simple, illustrative ex-\namples, the examples in these two domains used either a\nconstant policy or a constant value function. MDPs with\nthese particular properties are uncommon. Our arguments,\n7\nExplaining Reinforcement Learning with Shapley Values\n-5.0\n0.0\n5.0\n0.51\n2.40\n3.74\n-0.85\n-0.98\n0.84\n-5.23\n1.15\nx\ny\nP\nD\nState Features\n-5.0\n0.0\n5.0\n0.09\n2.38\n1.38\n1.94\n-1.41\n0.97\n2.82\n0.14\nSVERL-P\nShapley Values\nApplied to V ∗\nShapley Value\nR\nY\nG\nB\nR\nY\nG\nB\nP\nP\nFigure 6. SVERL-P contributions contrasted with Shapley values\napplied to a value function for two states in the Taxi domain. State\nfeatures are the x and y coordinates of the taxi, passenger location\n(P), and destination location (D). The taxi location is marked with\na rectangle, the passenger location is marked with a p and the\ndestination location is circled. In the top state, the passenger is at\nlocation B and the destination is location G. In the bottom state,\nthe passenger is in the taxi and the destination is location B.\nhowever, are valid for any MDP. As an example, Figures 4\nto 6 show that, in all domains tested, SVERL-P gives dif-\nferent results than applying Shapley values to the value\nfunction. They include domains with varying policies and\nvalue functions. In Figure 7, we compare SVERL-P and\nShapley values applied to V ∗in every state of a randomly-\nconstructed gridworld with 80 states (Gridworld-D). The\nresults show a persistent difference between these two ap-\nproaches.\nSVERL-P compared with Shapley values applied to\npolicies. In Section 3, we introduced Shapley values ap-\nplied to an agent’s policy. We argued that they provided\ninsight which improved understanding of a decision but that\nfurther insight could be drawn by also considering the effect\nof state features on performance. We now illustrate our\nviewpoint by comparing local SVERL-P to Shapley values\napplied to a policy.\nConsider Gridworld-C, shown in Figure 1c. In this domain,\nif no state feature is known, the agent cannot know the\noptimal action with certainty but it knows that (1) it is either\nNorth, East or West, and (2) North is more likely than East\nor West. In states 2 and 5, neither observing x nor observing\ny reveals the optimal action. We have no natural intuition\non the importance of state features and must rely on Shapley\nvalues.\nShapley values applied to the optimal policy in every state\nare shown in Figure 8. For each state, the Shapley values are\n−1\n0\n1\nShapley Values Applied to V ∗\n−1\n0\n1\nSVERL-P\nx\ny\nFigure 7. SVERL-P for every state of Gridworld-D compared to\nShapley values applied to a value function. Shapley values were\nnormalised to fall between −1 and 1. Each blue cross denotes the\nx feature for a particular state and each orange cross the y feature.\npresented for the optimal action, a∗. In state 5, x contributes\nmore than y to the probability of choosing the optimal action\n(N). One might assume that x is therefore more important\nthan y for an agent to act optimally. However, this would\nbe incorrect. The local SVERL-P contributions, shown in\nthe top panel of Figure 8, reveal that in fact the x and y\nfeatures contribute equally to performance. The reason for\nthis difference is that, in state 5, x also contributes towards\nthe likelihood of selecting the worst action (E). Similarly, in\nstate 2, Shapley values applied to the policy show that both\nx and y contribute equally to the probability of selecting\nthe optimal action (N). However, local SVERL-P contri-\nbutions reveal that y actually contributes more than x to\nperformance. In this state, observing x but not y increases\nthe probability of selecting the worst action (W).\nBy applying Shapley values to policies without considering\nthe consequence on performance, one would draw incorrect\nor incomplete conclusions about the importance of state\nfeatures. By considering the contribution of state features\ntowards performance, SVERL-P provides additional insight\ninto agent behaviour.\n6. Discussion\nWe presented a theoretical and empirical analysis of us-\ning Shapley values for explaining reinforcement learning\n(SVERL), starting from first principles, and demonstrated\nthe limitations of existing work. We then developed SVERL-\nP, a method that uses Shapley values to explain agent perfor-\nmance. SVERL-P considers the consequences of removing\nfeatures by explicitly deriving an agent’s policy and quan-\ntifying the change in performance. Our results show that\nSVERL-P produces meaningful explanations in a variety of\nreinforcement learning problems, matching and supplement-\ning human intuition.\n8\nExplaining Reinforcement Learning with Shapley Values\n1\n2\n3\n4\n5\n0.0\n2.0\n4.0\n4.00\n0.17\n0.00\n1.25\n0.33\n4.00\n0.67\n0.50\n2.25\n0.33\nSVERL-P\nx\ny\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.44\n0.17\n0.00\n0.31\n0.25\n0.44\n0.17\n0.33\n0.47\n0.08\nShapley Values Applied to π(s, a∗)\nState\nShapley Value\nFigure 8. SVERL-P compared to Shapley values applied to a policy\nin Gridworld-C (Figure 1c). The plots show the Shapley values\nof the x and y state features for all states. SVERL-P gives the\ncontribution of state features towards performance while Shapley\nvalues applied to a policy give the contributions of state features\ntowards the likelihood of selecting the optimal action in each state.\nIn most real-world applications, it is computationally expen-\nsive to calculate the SVERL-P characteristic value functions\nexactly. So the characteristic value functions, and hence the\nShapley values, must be approximated. Here we outline an\napproximation algorithm for local SVERL-P based on the\non-manifold sampling approach from Shapley values in su-\npervised learning, which has been proven to converge to the\nShapley value in the limit (ˇStrumbelj & Kononenko, 2010;\nFrye et al., 2020). Analogous to the sample in Equation 4,\neach sample in the algorithm is a marginal gain:\nEπ1\n\" ∞\nX\nt=0\nγtrt+1|s0 = s\n#\n−Eπ2\n\" ∞\nX\nt=0\nγtrt+1|s0 = s\n#\n,\nwhere\nπ1(at|st) =\n(\nπ(at|s′)\nif st = s,\nπ(at|st)\notherwise,\nand\nπ2(at|st) =\n(\nπ(at|s′′)\nif st = s,\nπ(at|st)\notherwise.\nA new s′ is sampled from pπ(·|sC∪{i}), and a new s′′ is\nsampled from pπ(·|sC), whenever st = s. Each coalition\nC ⊆F \\ {i} is sampled proportional to the multinomial\nterm in the Shapley value calculation. The expected re-\nturns can be evaluated using a standard reinforcement learn-\ning method, such as Monte Carlo rollouts. This sampling\nmethod requires the learning of state occupancy distribu-\ntions pπ(·|sC) for all C ⊆F, which is not trivial. We\nsuggest taking inspiration from one of the on-manifold sam-\npling methods proposed by Frye et al. (2020). Importantly,\nit is likely that these distributions do not need to be learnt\nexactly because optimal policies usually visit only a small\nsubset of states in large domains.\nSVERL is a direct application of Shapley values using spe-\ncific characteristic value functions suitable for reinforcement\nlearning. All the theoretical guarantees of Shapley values\napply to SVERL. Similarly, any advancements in applying\nShapley values to supervised learning will apply directly to\nSVERL. For example, SVERL might be difficult to interpret\nin domains with thousands of features, such as robotics or\nvision. However, a method such as groupShapley (Jullum\net al., 2021), which finds the contribution of groups of fea-\ntures and was developed for supervised learning, could be\napplied to SVERL, offering computational advantages and\nsimplifying interpretation.\nAs with any feature-based explanation method, there is fur-\nther work, often psychological and sociological, to derive\nuseful explanations which improve a user’s understanding.\nIt is naturally human to interpret Shapley values subjectively,\noften developing beliefs and understanding that extend be-\nyond the quantitative information that they provide. These\ninterpretations will likely become more challenging and\nsubjective as the number of features increases. When one\nproceeds to develop this extended understanding, before act-\ning on it, they must first evaluate whether it is well founded.\nFor example, SVERL-P values allow us to say “this feature\ncontributed x amount to an agent’s performance”. One can\nhypothesise on why that feature contributed x but such hy-\npotheses must be tested. These tests depend on the task, ex-\nplanation and hypothesis. We suggest that future research fo-\ncuses on (1) the presentation, interpretation and explanatory\nuse of feature attribution techniques such as Shapley values,\nand (2) methods for evaluating the conclusions drawn from\nsuch interpretations. We provide an example in Appendix B.\nAcknowledgements\nThis work was supported by the UKRI Centre for Doctoral\nTraining in Accountable, Responsible and Transparent AI\n(ART-AI) [EP/S023437/1], the EPSRC Centre for Doctoral\nTraining in Digital Entertainment (CDE) [EP/L016540/1]\nand the University of Bath. This research made use of\nHex, the GPU Cloud in the Department of Computer Sci-\nence at the University of Bath. We thank our reviewers\nfor a constructive process and the members of the Bath Re-\ninforcement Learning Laboratory for their feedback. We\nthank Scarllette Ellis for her Minesweeper implementation.\nReferences\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI Gym,\n9\nExplaining Reinforcement Learning with Shapley Values\n2016.\nCortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J.\nModeling wine preferences by data mining from physic-\nochemical properties. Decision Support Systems, 47(4):\n547–553, 2009.\nCovert, I., Lundberg, S. M., and Lee, S.-I. Understand-\ning global feature contributions with additive importance\nmeasures. Advances in Neural Information Processing\nSystems, 33:17212–17223, 2020.\nDietterich, T. G. The MAXQ method for hierarchical rein-\nforcement learning. In ICML, volume 98, pp. 118–126,\n1998.\nFrye, C., de Mijolla, D., Begley, T., Cowton, L., Stanley, M.,\nand Feige, I. Shapley explainability on the data manifold.\nIn International Conference on Learning Representations,\n2020.\nHe, L., Aouf, N., and Song, B. Explainable deep rein-\nforcement learning for UAV autonomous path planning.\nAerospace Science and Technology, 118:107052, 2021.\nJullum, M., Redelmeier, A., and Aas, K. groupShapley:\nEfficient prediction explanation with Shapley values for\nfeature groups. arXiv preprint arXiv:2106.12228, 2021.\nLiessner, R., Dohmen, J., and Wiering, M. A. Explain-\nable reinforcement learning for longitudinal control. In\nICAART (2), pp. 874–881, 2021.\nLipovetsky, S. and Conklin, M. Analysis of regression in\ngame theory approach. Applied Stochastic Models in\nBusiness and Industry, 17(4):319–330, 2001.\nLøver, J., Gjærum, V. B., and Lekkas, A. M. Explainable\nAI methods on a deep reinforcement learning agent for\nautomatic docking. IFAC-PapersOnLine, 54(16):146–\n152, 2021.\nLundberg, S. M. and Lee, S.-I. A unified approach to inter-\npreting model predictions. Advances in Neural Informa-\ntion Processing Systems, 30, 2017.\nPolak, E. Basics of Minimax algorithms. In Nonsmooth\nOptimization and Related Topics, pp. 343–369. Springer,\n1989.\nRemman, S. B., Str¨umke, I., and Lekkas, A. M. Causal\nversus marginal Shapley values for robotic lever manipu-\nlation controlled using deep reinforcement learning. In\n2022 American Control Conference (ACC), pp. 2683–\n2690. IEEE, 2022.\nRizzo, S. G., Vantini, G., and Chawla, S. Reinforcement\nlearning with explainability for traffic signal control. In\n2019 IEEE Intelligent Transportation Systems Conference\n(ITSC), pp. 3567–3572. IEEE, 2019.\nShapley, L. S. A value for n-person games. 1953.\nˇStrumbelj, E. and Kononenko, I. An efficient explanation of\nindividual classifications using game theory. The Journal\nof Machine Learning Research, 11:1–18, 2010.\nˇStrumbelj, E. and Kononenko, I. Explaining prediction mod-\nels and individual predictions with feature contributions.\nKnowledge and Information Systems, 41:647–665, 2014.\nˇStrumbelj, E., Kononenko, I., and ˇSikonja, M. R. Explaining\ninstance classifications with interactions of subsets of\nfeature values. Data & Knowledge Engineering, 68(10):\n886–904, 2009.\nWang, Y., Mase, M., and Egi, M. Attribution-based salience\nmethod towards interpretable reinforcement learning. In\nAAAI Spring Symposium: Combining Machine Learning\nwith Knowledge Engineering (1), 2020.\nZhang, K., Xu, P., and Zhang, J. Explainable AI in deep\nreinforcement learning models: A SHAP method applied\nin power system emergency control. In 2020 IEEE 4th\nConference on Energy Internet and Energy System Inte-\ngration (EI2), pp. 711–716. IEEE, 2020.\nZhang, K., Zhang, J., Xu, P.-D., Gao, T., and Gao, D. W.\nExplainable AI in deep reinforcement learning models\nfor power system emergency control. IEEE Transactions\non Computational Social Systems, 9(2):419–427, 2021.\n10\nExplaining Reinforcement Learning with Shapley Values\nA. Domains\nGridworld-A, shown in Figure 1a, is a deterministic gridworld. The MDP state represents the grid square occupied\nby the agent and is described by two features, (x, y), the x and y coordinates of the agent on the grid. There are six\nstates, S = {(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3)}, two of which are goal states, G = {(1, 3), (2, 3)}. The initial state is\nsampled randomly from the southernmost squares, {(1, 1), (2, 1)}. The actions are North, East, South, and West. Reward\nis −1 for every action taken and an additional +10 for transitioning into a goal state, producing a shortest path problem.\nActions that attempt to transition an agent out of the grid do not change the state. Gridworld-B, shown in Figure 1b, and\nGridworld-C, shown in Figure 1c, are identical to Gridworld-A in all aspects other than the grid layout and the identity of\nthe goal states.\nGridworld-D is a deterministic 10 × 10 gridworld, containing 20 grid positions that are impassable blocks, selected\nuniformly randomly from among all grid positions. There is a single goal state, selected randomly, and fixed across episodes.\nThe initial state is selected randomly from among grid squares that are not impassable blocks or the goal. The domain is\nidentical to Gridworld-A in all other aspects.\nTic-Tac-Toe is a classic game played on a 3 × 3 grid, where two players take turns to place noughts (O) and crosses (X).\nWhen a player places three noughts or three crosses such that a straight line can be drawn through them, the game ends with\na win for the corresponding player. If the grid is full with no winner, the game is a draw. The state has nine features, with\neach feature representing a specific grid position, taking on values X, O, or empty. The agent plays as X and the opponent\nas O. The players have equal probability of playing first. The opponent’s policy is the Minimax algorithm (Polak, 1989).\nOptimal play against this opponent ends in a draw.\nTaxi is a classic reinforcement learning domain by Dietterich (1998). We used the implementation by OpenAI Gym (Brock-\nman et al., 2016). The domain has a grid with four locations, marked R(ed), G(reen), B(lue) and Y(ellow). There are four state\nfeatures: x ∈{1, 2, 3, 4, 5}, y ∈{1, 2, 3, 4, 5}, passenger-location ∈{R, G, B, Y, in-taxi}, and destination ∈{R, G, B, Y}.\nState features x and y represent the taxi’s location. Initial taxi location and destination are selected uniformly randomly. For\nan episode to terminate successfully, the taxi must navigate to the passenger location, pick-up the passenger, navigate to the\ndestination, and drop-off the passenger. At the beginning of an episode, the passenger location is randomly selected among\nR, G, B, and Y. Once the passenger has been collected, passenger location becomes in-taxi. The actions are north, south,\neast, west, pick-up, and drop-off. Pick-up action successfully picks up the passenger only when the taxi and the passenger is\nat the same grid location. Similarly, drop-off action successfully drops off the passenger when the passenger is in the taxi\nand the taxi is at the destination. The reward is −1 for each action, an additional +20 for delivering the passenger at the\ndestination, and −10 for unsuccessful execution of the pickup or the drop-off action.\nMinesweeper is an implementation of the classic game on a 4 × 4 grid. Each episode resets a grid that contains two hidden\nmines, each placed randomly. The state has 16 features, with each feature respresenting a specific grid square, taking on\nvalues 0, 1, 2 or unopened. Initially, all grid squares are unopened. At each decision stage, the agent selects an unopened\nsquare to reveal what is underneath. If it happens to be a number, that number represents the total number of mines in the\n(up to eight) squares directly surrounding the newly opened square. If the number is zero, all surrounding grid squares are\nrecursively revealed to reveal an area of zeros bordered by strictly positive numbers. The game ends when the agent opens a\nsquare with a mine or all squares that do not contain a mine are opened. There is only one reward signal: −20 whenever the\nagent reveals a mine. Therefore the highest return possible is 0. There is no incentive for the agent to complete a game in\nminimal time.\nB. Extended Analysis in Minesweeper\nIn the minesweeper example of Figure 5, SVERL-P contributions are negative for two unopened squares (M1 and M2) in the\nsecond state. The implication is that observing either state feature makes a negative contribution to the expected return. We\nhypothesise that, by becoming observable, these features increase the probability that the agent clicks on the corresponding\nsquares. Such an action would reveal the underlying mine and terminate the game with a large negative reward.\nIn Section 6, we suggested that humans are likely to naturally over-interpret SVERL-P contributions, developing hypotheses\nthat must be tested. This is one such example. The validity of our hypothesis can be tested by examining Shapley values\napplied to a policy that outputs action probabilities, introduced in Section 3. Figure 9 shows that the Shapley values for the\nprobability of selecting each unopened feature are positive, showing that, on average, observing that a square is unopened\npositively contributes towards the probability of selecting it.\n11\nExplaining Reinforcement Learning with Shapley Values\n0\n0\n1\n0\n1\n2\n0\n1\n0\n1\n1\n1\nM1\nM2?\nM2?\n1\n1\n2\n2\n3\n3\n4\n4\nx\ny\nState 1\n(4, 4)\n(4, 3)\n(3, 2)\n(4, 2)\n−0.2\n0.0\n0.2\n0.4\nShapley Values Applied to π\nπ(s, a44)\nπ(s, a43)\nπ(s, a32)\nπ(s, a42)\n0\n0\n1\n0\n1\n2\n0\n1\n2\n0\n1\n1\n1\nM1\nM2\n1\n1\n2\n2\n3\n3\n4\n4\nx\ny\nState 2\n(4, 4)\n(4, 3)\n(3, 2)\nState Feature\n−0.1\n0.0\n0.1\nShapley Values Applied to π\nFigure 9. Shapley values applied to a policy in two states of Minesweeper. Action axy denotes the action that opens grid square (x, y).\nThe plots show, for each available action, the Shapley values of the state features that correspond to unopened squares.\nNote the non-negative SVERL-P contribution of M1 in state 1 even though observing that a square is unopened increases\nthe probability of opening it. On closer inspection, Figure 9 reveals that observing that square (3, 2) is unopened increases\nthe probability of opening square (4, 2) (the optimal action) much more than it increases the probability of opening (3, 2).\nSVERL-P contributions revealed insight into how features contributed to performance but further analysis was required to\ninvestigate why features contributed to performance.\nC. Computing Shapley Values\nThis work presented four applications of Shapley values in reinforcement learning, under the SVERL framework: Shapley\nvalues applied to value functions, Shapley values applied to policies, local SVERL-P and global SVERL-P. Each of the\ndifferent Shapley values are computed using Equation (1), with their respective characteristic value functions computed using\nEquations (6) to (10) and (12). All of these characteristic value functions require the conditional limiting state occupancy\ndistributions, pπ(s′|sC), for every C ⊂F. We calculate each pπ(s′|sC) using Bayes’s rule:\npπ(s′|sC) = p(sC|s′)pπ(s′)\npπ(sC)\n=\np(sC|s′)pπ(s′)\nP\ns′∈S p(sC|s′)pπ(s′),\n(14)\nwhere the limiting state occupancy distribution pπ(s′) is approximated through interaction with the environment. Addition-\nally, if sC is a possible observation of s′, then p(sC|s′) = 1, else p(sC|s′) = 0. For example, in Gridworld-B, sC = {x = 1}\nis a possible observation of s′ = {x = 1, y = 3}, whereas sC = {x = 2} is not.\nAfter computing the conditional limiting state occupancy distributions using Equation (14), the characteristic value functions\nfor Shapley values applied to policies and Shapley values applied to value functions can be calculated directly using\nEquations (6) to (9). For the local and global SVERL-P characteristic values in Equations (10) and (12), first πC(a|s) must\nbe computed using Equation (11). Then the characteristic values, which are expected returns, can be computed using any\nstandard reinforcement learning algorithm. We used Monte Carlo roll outs.\nD. Code\nCode is available at https://github.com/bath-reinforcement-learning-lab/SVERL_icml_2023.\n12\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-06-09",
  "updated": "2023-06-09"
}