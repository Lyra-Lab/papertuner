{
  "id": "http://arxiv.org/abs/2306.06800v1",
  "title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing",
  "authors": [
    "Asaad Alghamdi",
    "Xinyu Duan",
    "Wei Jiang",
    "Zhenhai Wang",
    "Yimeng Wu",
    "Qingrong Xia",
    "Zhefeng Wang",
    "Yi Zheng",
    "Mehdi Rezagholizadeh",
    "Baoxing Huai",
    "Peilun Cheng",
    "Abbas Ghaddar"
  ],
  "abstract": "Developing monolingual large Pre-trained Language Models (PLMs) is shown to\nbe very successful in handling different tasks in Natural Language Processing\n(NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B\nparameters trained on 529GB of high-quality Arabic textual data. AraMUS\nachieves state-of-the-art performances on a diverse set of Arabic\nclassification and generative tasks. Moreover, AraMUS shows impressive few-shot\nlearning abilities compared with the best existing Arabic PLMs.",
  "text": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural\nLanguage Processing\nAsaad Alghamdi1,∗Xinyu Duan2,∗Wei Jiang2 Zhenhai Wang2 Yimeng Wu3\nQingrong Xia2 Zhefeng Wang2 Yi Zheng2 Mehdi Rezagholizadeh3 Baoxing Huai2\nPeilun Cheng1 Abbas Ghaddar3\n1 AI Cognitive Team, Tonomus\n2 Huawei Cloud Computing Technologies Co., Ltd.\n3 Huawei Technologies Co., Ltd.\n{asaad.alghamdi,eddie.chengpeilun}@neom.com\n{duanxinyu,jiangwei160,wangzhenhai1,yimeng.wu,xiaqingrong,wangzhefeng,\nzhengyi29,mehdi.rezagholizadeh,huaibaoxing,abbas.ghaddar}@huawei.com\nAbstract\nDeveloping monolingual large Pre-trained Lan-\nguage Models (PLMs) is shown to be very\nsuccessful in handling different tasks in Nat-\nural Language Processing (NLP). In this work,\nwe present AraMUS, the largest Arabic PLM\nwith 11B parameters trained on 529GB of high-\nquality Arabic textual data. AraMUS achieves\nstate-of-the-art performances on a diverse set\nof Arabic classification and generative tasks.\nMoreover, AraMUS shows impressive few-shot\nlearning abilities compared with the best exist-\ning Arabic PLMs.\n1\nIntroduction\nScaling-up Pre-trained Language Models (PLMs)\nhas led to astonishing performance gains on a\nvast variety of Natural Language Processing (NLP)\ntasks (Du et al., 2021; Zoph et al., 2022; Smith\net al., 2022). It has also opened new perspectives\nfor studying the opportunities and limitations of\nlarge PLMs (Raffel et al., 2019; Dale, 2021; Bom-\nmasani et al., 2021), as well as their social and\nethical impacts (Bender et al., 2021; Weidinger\net al., 2021; Tamkin et al., 2021; Rae et al., 2021a;\nSusnjak, 2022).\nAlthough for some languages such as English\nand Chinese, several PLMs with even more than\nhundred billions of parameters have been devel-\noped (Rae et al., 2021b; Chowdhery et al., 2022;\nZeng et al., 2021; Sun et al., 2021), little or no\nprogress has been made on this direction for many\nother languages including Arabic.1 While there\nhave recently been few attempts to develop multi-\nbillion parameters Arabic PLMs (Nagoudi et al.,\n2022a; Antoun et al., 2021b; Lakim et al., 2022),\n∗Equal contribution\n1Arabic is among top 10 most popular languages in the\nworld with 420M native speakers, and more than 25 popular\ndialects (Guellil et al., 2021).\nstill, their performances and abilities have not been\nwell investigated. The largest well-studied Arabic\nPLM has no more than 370M parameters (Nagoudi\net al., 2022b; Ghaddar et al., 2022).\nIn this work, we introduce AraMUS, an 11B pa-\nrameter encoder-decoder T5 (Raffel et al., 2019)\nmodel, which is pre-trained on 529GB of high-\nquality Arabic text (filtered out of 8.8TB). To the\nbest of our knowledge, AraMUS is the largest Ara-\nbic PLM in terms of pre-training data and model\nsize. Furthermore, it is the first time a multi-billion\nparameter Arabic PLM is systematically evaluated,\nagainst the existing state-of-the-art models, on a\ndiversified set of discriminative and generative\ntask models. More precisely, AraMUS achieves\nnew state-of-the-art performances of 79.8% on the\nALUE (Seelawi et al., 2021) benchmark, which\nis a collection of 8 discriminative tasks. In addi-\ntion, it significantly outperforms the best available\nencoder-decoder models on multiple generative\ntasks. Finally, AraMUS shows remarkable abil-\nities to maintain its performance under few-shot\nsettings.\n2\nRelated Work\nRecently, there has been a growing body of the\nliterature on very large-scale English PLMs by\nthoroughly studying different aspects of their scal-\ning. These efforts can be summarized into scal-\ning their pre-training data (Hoffmann et al., 2022)\nand model size (Dale, 2021; Rae et al., 2021b;\nSmith et al., 2022), designing efficient architec-\ntures (Zoph et al., 2022; Chowdhery et al., 2022)\nand pre-training objectives (Bajaj et al., 2022; Tay\net al., 2022), democratizing their access (Zhang\net al., 2022), and making them useful in real-world\napplications (Ouyang et al., 2022; Qu et al., 2023).\nBesides English, there have been multiple attempts\nto develop multilingual (Scao et al., 2022), as well\narXiv:2306.06800v1  [cs.CL]  11 Jun 2023\nas non-Anglocentric (Zeng et al., 2021; Sun et al.,\n2021; Shin et al., 2022) multi-billion PLMs.\nUnfortunately, the development of Arabic PLMs\ndoes not follow the same pace as that of English.\nThe earliest released Arabic PLMs (Antoun et al.,\n2020; Safaya et al., 2020) were based on the BERT-\nbase (as well as -large) architecture (Devlin et al.,\n2018) and pre-trained on less than 100GB of unfil-\ntered data. Successive works tried to improve Ara-\nbic BERT-base models performance by scaling up\nthe pre-training data up to 197GB and 167GB of un-\nfiltered Arabic text for MARBERT (Abdul-Mageed\net al., 2021) and CAMeLBERT (Inoue et al., 2021)\nrespectively. In addition, other works focused on\ndeveloping Arabic PLMs to support other archi-\ntectures like AraElectra (Antoun et al., 2021a),\nAraGPT (Antoun et al., 2021b), AraT5 (Nagoudi\net al., 2022b), and AraBART (Eddine et al., 2022)\nwhich are equivalent to English ELECTRA (Clark\net al., 2020), GPT (Radford et al., 2018), T5 (Raf-\nfel et al., 2019), and BART (Lewis et al., 2019)\nrespectively.\nRecently, Ghaddar et al. (2022) developed state-\nof-the-art Arabic BERT (JABER and SABER) and\nT5 models (AT5S and AT5B) by improving the\npre-training data quantitatively and qualitatively.\nMore precisely, they pre-trained Arabic BERT-\nbase/large and T5-small/base models on 115GB\nof high-quality Arabic text data (filtered out of\n514GB). AraGPT-Mega (Antoun et al., 2021b), Jas-\nmine (Nagoudi et al., 2022a), NOOR (Lakim et al.,\n2022) are the only existing multi-billion Arabic\nPLMs. These are decoder-only GPT models with\n1.5B, 6.7B, and 10B parameters respectively. How-\never, these aforementioned works suffer from the\nabsent (e.g. in AraGPT, NOOR) or limited (e.g.\nJasmine) comprehensive evaluation on NLP end-\ntasks. Moreover, some of these models (such as\nNOOR and Jasmine) are not publicly available for\ncustom evaluations.2 Evaluation is a key factor for\nunderstanding the strengths and limitations of these\nmodels, without which the progress of the Arabic\nNLP field is hindered.\n3\nAraMUS\n3.1\nPre-training Data\nWe mainly leverage all (up to July 2022) of the 90\nCommon Crawl 3 monthly web scrapes in order to\ncollect massive amount of Arabic textual data. This\n2We refer the reader to Appendix B.2 for detailed position-\ning of AraMUS against each of these three models.\n3https://commoncrawl.org\nis significantly larger compared to JABER (Ghad-\ndar et al., 2022), NOOR (Lakim et al., 2022), and\nJasmine (Nagoudi et al., 2022a), which use 10, 21,\nand 71 monthly CC shards, respectively. Then, we\napply aggressive noise filtering and deduplication,\nwhich give rise to 529GB of high-quality Arabic\ntext data. Nagoudi et al. (2022a) introduced the\nclosest comparable pre-training corpus size to us\nwith 413GB (22% smaller than ours) of Arabic\ntext data. Our data mainly differs in using 2.5\ntimes more CC data, while they used 3.8 times\nmore dialect data than ours. We refer the reader to\nAppendix A.1 for technical details regarding the\npre-training data collection.\n3.2\nModel and Implementation\nAraMUS follows the same encoder-decoder archi-\ntecture and configuration as T5-xxl (Raffel et al.,\n2019) model with 64k vocabulary size. We choose\nencoder-decoder T5 architecture because it was\nfound to deliver a good balance between the per-\nformance of the discriminative and generative\ntasks (Raffel et al., 2019; Tay et al., 2022), com-\npared to encoder-only BERT (discriminative tasks\nfocused) and decoder-only GPT (Radford et al.,\n2019) (generative tasks focused). AraMUS has\n11B parameters in total, which makes it the largest\nexisting Arabic T5 model. It was pre-trained using\n128 NVIDIA A100 GPUs for 2 months. Techni-\ncal details regarding implementation and hyper-\nparameters used for pre-training are listed in Ap-\npendix A.2.\n3.3\nEvaluation Protocol\nWe assess AraMUS by performing extensive fine-\ntuning experiments on a diverse set of NLP tasks.\nOn one side, we experiment on 8 tasks from the\nwell-established ALUE benchmark (Seelawi et al.,\n2021), which includes one regression (SVREG),\none multi-label classification (SEC), 4 single-\nsentence (MDD, FID, OOLD, and OHSD) and\n2 sentence-pair (MQ2Q and XNLI) classification\ntasks. On the generative tasks side, we evaluate on\nQuestion Answering (QA), Question Generation\n(QG), and Text Summarization (TS).\nWe compare AraMUS with state-of-the-art Ara-\nbic PLMs in the literature, including ARBERT,\nMARBERT, JABER (BERT-base), SABER, ALM-\n1.0 (BERT-large), AT5B and AraT5-base (T5-base).\nThe experimental protocol is designed to ensure\nthe diversity of the tasks, and the public availability\nof models. Most importantly, we make sure that\nModel\n#Params\nMQ2Q\nMDD\nSVREG\nSEC\nFID\nOOLD\nXNLI\nOHSD\nAvg.\nBERT-models\nARBERT\n163M\n74.7±0.1\n62.5±0.2\n83.5±0.6\n43.9±0.6\n85.3±0.3\n90.5±0.5\n70.8±0.5\n81.9±2.0\n74.1±0.6\nMARBERT\n163M\n69.1±0.9\n63.2±0.3\n88.0±0.4\n47.6±0.9\n84.7±0.4\n91.8±0.3\n63.3±0.7\n83.8±1.4\n73.9±0.7\nJABER\n135M\n75.1±0.3\n65.7±0.3\n87.4±0.7\n46.8±0.8\n84.8±0.3\n92.2±0.5\n72.4±0.7\n85.0±1.6\n76.2±0.7\nSABER\n369M\n77.7±0.4\n67.4±0.2\n89.3±0.3\n49.0±0.5\n86.1±0.3\n93.4±0.4\n75.9±0.3\n88.9±0.3\n78.5±0.3\nT5-models\nAT5B\n296M\n73.7±0.1\n64.7±0.2\n78.1±2.4\n43.8±0.7\n83.1±0.5\n90.0±0.4\n72.2±0.4\n81.2±2.1\n73.3±0.9\nAraT5-base\n289M\n70.5±2.1\n63.6±0.2\n80.8±1.3\n44.0±0.6\n82.3±0.4\n90.5±0.4\n72.5±1.5\n78.3±1.4\n73.0±1.0\nAraMUS\n11B\n80.7±0.1\n68.0±0.2\n89.8±0.3\n49.6±0.7\n86.6±0.4\n93.8±0.4\n82.9±0.2\n88.2±1.0\n79.9±0.2\nTable 1: DEV set performances and standard deviations over 5 runs on the ALUE benchmark.\nModel\n#Params\nMQ2Q\nMDD\nSVREG\nSEC\nFID\nOOLD\nXNLI\nOHSD\nAvg.\nDIAG\nJABER\n135M\n93.1\n64.1\n70.9\n31.7\n85.3\n91.4\n73.4\n79.6\n73.7\n24.4\nALM-1.0\n350M\n94.5\n65.1\n70.1\n35.3\n86.0\n91.7\n77.7\n85.7\n75.8\n30.2\nSABER\n369M\n93.3\n66.5\n79.2\n38.8\n86.5\n93.4\n76.3\n84.1\n77.3\n26.2\nAraT5-base\n282M\n91.3\n63.8\n65.9\n30.5\n82.3\n88.8\n68.2\n77.9\n71.1\n15.4\nAraMUS\n11B\n95.2\n67.5\n80.4\n41.6\n87.2\n95.5\n83.2\n87.4\n79.8\n42.0\nTable 2: Results of top-ranked models on the ALUE leaderboard.\ndatasets are of high quality, open-sourced, and sup-\nported by a well-established evaluation protocol.\nOur goal is to have a fair comparison between mod-\nels, as well as the credibility and reproducibility of\nthe results. A detailed description of fine-tuning\ndatasets, evaluation metrics, baselines, and imple-\nmentation details are available in Appendix B.\n3.4\nResults\nTable 1 shows the dev set results of the eight ALUE\ntasks with their average scores and standard devi-\nations of 5 runs. The baseline results are directly\nbrought from (Ghaddar et al., 2022) and they are\ndirectly comparable with AraMUS since we follow\nthe same evaluation protocol. Table 2 shows the\ntest set performances of the state-of-the-art models\non the ALUE leaderboard.\nAs we expect, AraMUS outperforms all other\nbaseline models on both dev and test sets and\nachieves a new state-of-the-art performances on\nALUE. While our average ALUE result is 1.4%\nbetter than the best baseline, SABER, the latter\noutperforms AraMUS on the OHSD dataset. On\nthe other hand, AraMUS significantly outperforms\nSABER by 2.5% on average and 3.3% on OHSD\nwhen comparing results on the leaderboard test. In-\nterestingly, this is roughly a similar performance\ngap (2.1%) on the English GLUE (Wang et al.,\n2018) between the English T5-xxl (Raffel et al.,\n2019) (11B parameters) and the well-trained En-\nglish Roberta-large (Liu et al., 2019) model.\nMoreover, we observe a huge gap of 13.8% be-\ntween AraMUS and SABER on the ALUE diag-\nnostic set. DIAG was specifically designed to eval-\nuate models’ abilities to capture complex linguis-\ntic phenomena in Arabic (Seelawi et al., 2021).\nThese observations clearly indicate that scaling the\nmodel with more data and parameters greatly im-\nproves the robustness and generalization abilities\nof Arabic PLMs. It is worth mentioning that our\nresults are in contrast with previous observations\nreported in (Nagoudi et al., 2022b; Ghaddar et al.,\n2022) that encoder-decoder T5 architecture Ara-\nbic models (e.g. AraT5-base and AT5B) signif-\nicantly underperform BERT models on discrimi-\nnative tasks. Our results suggest that, for Arabic,\nencoder-decoder models require more data and pa-\nrameters to catch up with encoder-only models on\ndiscriminative tasks.\nDev\nTest\nModel\nEM\nF1\nEM\nF1\nAraT5-base\n40.2±0.4\n61.4±0.8\n31.2\n65.7\nAT5B\n40.8±0.7\n61.6±1.1\n31.6\n67.2\nAraMUS\n49.8±1.1\n69.1±0.9\n35.3\n72.3\nTable 3: F1-score and Exact Match (EM) scores of T5-\nstyle models on the Question Answering (QA) task.\nWe further validate the performance of AraMUS\nby conducting an extensive set of experiments on\nthe ALUE benchmark under few-shot setting. Fig-\nure 1 shows AraMUS and the best publicly avail-\n8\n16\n32\n64\n128\n256\nSEC\n25\n30\n35\n40\n8\n16\n32\n64\n128\n256\nOHSD\n52.5\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n8\n16\n32\n64\n128\n256\nMDD\n10\n15\n20\n25\n30\n8\n16\n32\n64\n128\n256\nALUE\n40\n45\n50\n55\n60\n65\n70\nJABER\nSABER\nAraMUS\nFigure 1: Models performance on the dev set of 3 ALUE tasks and the ALUE average score in the few-shot setting.\nable Arabic PLMs (JABER and SABER) perfor-\nmances on 3 representative ALUE tasks (see the\nfull results in Table 7 of Appendix C) and the av-\nerage ALUE score. The 3 selected tasks are: SEC\nbecause it shows specific results; OHSD since with\nFID and OOLD they show similar result patterns,\nand MDD as a representative of trends observed\nfor tasks MQ2Q, SVREG, and XNLI.\nRouge1\nRouge2\nRougeL\nWikiLingua Dev\nAraT5-base\n25.0±0.2\n10.0±0.0\n22.4±0.2\nAT5B\n26.1±2.8\n10.5±1.6\n23.2±2.5\nAraMUS\n30.5±0.1\n13.2±0.1\n26.9±0.1\nWikiLingua Test\nAraT5-base\n25.1\n10.2\n22.5\nAT5B\n27.8\n11.5\n24.8\nAraMUS\n30.9\n13.5\n27.1\nEASC Test\nAraT5-base\n10.7\n2.7\n9.3\nAT5B\n12.6\n3.5\n11.3\nAraMUS\n16.1\n6.7\n13.3\nTable 4: T5-style models’ performances on the Text\nSummarization task.\nFirst, we notice that exceptionally on SEC, Ara-\nMUS performs on par with JABER and underper-\nforms SABER on many data points. We think that\nthis is because the text-to-text approach is not ef-\nfective for multi-label classification tasks under a\nfew-shot setting. Second, we observe that AraMUS\nhas a marginal gain compared to the best baseline\n(SABER) on some tasks like OHSD, e.g. 0.2%,\n1.0% and 6.0% on 8, 128, and 256 examples respec-\ntively. As for the remaining 4 tasks (represented by\nMDD), we observe that AraMUS significantly out-\nperforms both baselines by a large margin. Overall,\nAraMUS shows a consistent performance gain be-\ntween 4% to 6% when averaging the results on the\n8 ALUE tasks compared to SABER.\nModel\nDev\nTest\nAraT5-base\n6.7±0.1\n13.5\nAT5B\n8.1±0.1\n17.0\nAraMUS\n8.6±0.1\n17.4\nTable 5: Question Generation dev and test sets BLEU\nscore of T5-style models.\nFinally, we assess the text generation abilities of\nAraMUS by experimenting on 3 generative tasks\nin Table 3, 4 and 5. Overall, the observations are\nconsistent with the results obtained on ALUE, Ara-\nMUS reports the highest scores on all tasks and\nacross all metrics. More precisely, AraMUS sig-\nnificantly outperforms AT5B, the state-of-the-art\nArabic T5-base model, by 7.5% and 5.1% on QA\nF1 score dev and test sets respectively. Similarly,\nAraMUS has a gain of 4.4%, 4.1%, and 3.5% on\nTS dev, test, and EASC test rouge1 score respec-\ntively. However, gains are not always significant on\ngenerative tasks, as we observe a smaller margin\nof improvement of 0.5% and 0.4% and against the\nbest baseline on QG dev and test sets respectively.\n4\nConclusion\nIn this paper, we introduced AraMUS which is\nnot only the largest Arabic PLM in terms of pre-\ntraining data and model size, but also the first multi-\nbillion Arabic PLM to be extensively evaluated on\na wide range of NLP tasks. Since our work gives\nclues on the benefits and limitations of scaling up\ndata and model sizes, we hope that it will pave the\nway for the Arabic NLP community to focus on\nproblems that are beyond the reach of PLM scaling.\nLimitations\nWhile our model shows state-of-the-art results on\nmany discriminative and generative tasks, we can\nthink of the following main caveats of our work.\nFirst, the number of generative tasks that we eval-\nuate on is relatively small especially when con-\nsider that AraMUS is text-to-text encoder-decoder\nmodel. This is mainly because of the rarity of\nArabic generative datasets that are at the same time\nwell-established and open-source. Second, it would\nbe important to study how end-tasks performances\nis impacted when ablating the model size (e.g. 1-6\nbillion parameters models), pretraining data quan-\ntity or/and quality.\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7088–7105, Online. Association for Computational\nLinguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nArabert: Transformer-based model for arabic lan-\nguage understanding. In LREC 2020 Workshop Lan-\nguage Resources and Evaluation Conference 11–16\nMay 2020, page 9.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021a.\nAraELECTRA: Pre-training text discriminators for\nArabic language understanding. In Proceedings of\nthe Sixth Arabic Natural Language Processing Work-\nshop, pages 191–195, Kyiv, Ukraine (Virtual). Asso-\nciation for Computational Linguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021b.\nAragpt2: Pre-trained transformer for arabic language\ngeneration. In Proceedings of the Sixth Arabic Natu-\nral Language Processing Workshop, pages 196–207.\nPayal Bajaj, Chenyan Xiong, Guolin Ke, Xiaodong Liu,\nDi He, Saurabh Tiwary, Tie-Yan Liu, Paul Bennett,\nXia Song, and Jianfeng Gao. 2022. Metro: Efficient\ndenoising pretraining of large scale autoencoding lan-\nguage models with model generated signals. arXiv\npreprint arXiv:2204.06644.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nRobert Dale. 2021. Gpt-3: What’s it good for? Natural\nLanguage Engineering, 27(1).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\nBarret Zoph, Liam Fedus, Maarten Bosma, Zong-\nwei Zhou, Tao Wang, Yu Emma Wang, Kellie Web-\nster, Marie Pellat, Kevin Robinson, Kathy Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2021. Glam: Efficient scaling of language mod-\nels with mixture-of-experts.\nMoussa Kamal Eddine, Nadi Tomeh, Nizar Habash,\nJoseph Le Roux, and Michalis Vazirgiannis. 2022.\nArabart: a pretrained arabic sequence-to-sequence\nmodel for abstractive summarization. arXiv preprint\narXiv:2203.10945.\nMahmoud El-Haj, Udo Kruschwitz, and Chris Fox.\n2010. Using mechanical turk to create a corpus of\narabic summaries.\nIbrahim Abu El-Khair. 2016. 1.5 billion words Arabic\nCorpus. arXiv preprint arXiv:1611.04033.\nAbdelRahim Elmadany, El Moatez Billah Nagoudi, and\nMuhammad Abdul-Mageed. 2022. Orca: A challeng-\ning benchmark for arabic language understanding.\narXiv preprint arXiv:2212.10758.\nAbbas Ghaddar, Yimeng Wu, Sunyam Bagga, Ahmad\nRashid, Khalil Bibi, Mehdi Rezagholizadeh, Chao\nXing, Yasheng Wang, Duan Xinyu, Zhefeng Wang,\nBaoxing Huai, Xin Jiang, Qun Liu, and Phillippe\nLanglais. 2022.\nRevisiting Pre-trained Language\nModels and their Evaluation for Arabic Natural Lan-\nguage Understanding. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3135–3151, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nImane Guellil, Houda Saâdane, Faical Azouaou, Billel\nGueni, and Damien Nouvel. 2021. Arabic natural\nlanguage processing: An overview. Journal of King\nSaud University-Computer and Information Sciences,\n33(5):497–507.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in arabic pre-trained\nlanguage models. In Proceedings of the Sixth Arabic\nNatural Language Processing Workshop, WANLP\n2021, Kyiv, Ukraine (Virtual), April 9, 2021, pages\n92–104. Association for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summariza-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4034–4048,\nOnline. Association for Computational Linguistics.\nImad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol,\nMerouane Debbah, and Julien Launay. 2022. A holis-\ntic assessment of the carbon footprint of noor, a very\nlarge arabic language model. In Proceedings of Big-\nScience Episode\\# 5–Workshop on Challenges & Per-\nspectives in Creating Large Language Models, pages\n84–94.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74–81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gre-\ngory Diamos, Erich Elsen, David Garcia, Boris Gins-\nburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, and Hao Wu. 2018.\nMixed precision\ntraining. In In International Conference on Learning\nRepresentations.\nEl Moatez Billah Nagoudi, Muhammad Abdul-Mageed,\nAbdelRahim Elmadany, Alcides Alcoba Inciarte, and\nMd Tawkat Islam Khondaker. 2022a. Jasmine: Ara-\nbic gpt models for few-shot learning. arXiv preprint\narXiv:2212.10755.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and\nMuhammad Abdul-Mageed. 2022b. AraT5: Text-\nto-text transformers for Arabic language generation.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 628–647, Dublin, Ireland.\nAssociation for Computational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances\nin neural information processing systems, 32:8026–\n8037.\nXiaoye Qu, Yingjie Gu, Qingrong Xia, Zechang Li,\nZhefeng Wang, and Baoxing Huai. 2023.\nA sur-\nvey on arabic named entity recognition: Past, re-\ncent advances, and future trends.\narXiv preprint\narXiv:2302.03512.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJ Rae, G Irving, and L Weidinger. 2021a. Language\nmodelling at scale: Gopher, ethical considerations,\nand retrieval. DeepMind Blog.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021b. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. Kuisail at semeval-2020 task 12: Bert-cnn\nfor offensive speech identification in social media. In\nProceedings of the Fourteenth Workshop on Semantic\nEvaluation, pages 2054–2059.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nHaitham Seelawi, Ibraheem Tuffaha, Mahmoud Gzawi,\nWael Farhan, Bashar Talafha, Riham Badawi, Zyad\nSober, Oday Al-Dweik, Abed Alhakim Freihat, and\nHussein Al-Natsheh. 2021. Alue: Arabic language\nunderstanding evaluation.\nIn Proceedings of the\nSixth Arabic Natural Language Processing Workshop,\npages 173–184.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nSeongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong\nKim, HyoungSeok Kim, Boseop Kim, Kyunghyun\nCho, Gichang Lee, Woomyoung Park, Jung-Woo Ha,\net al. 2022. On the effect of pretraining corpora on\nin-context learning by a large-scale language model.\narXiv preprint arXiv:2204.13509.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao\nPang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yan-\nbin Zhao, Yuxiang Lu, et al. 2021. Ernie 3.0: Large-\nscale knowledge enhanced pre-training for language\nunderstanding and generation.\nTeo Susnjak. 2022. Chatgpt: The end of online exam\nintegrity? arXiv preprint arXiv:2212.09292.\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep\nGanguli. 2021. Understanding the capabilities, limi-\ntations, and societal impact of large language models.\narXiv preprint arXiv:2102.02503.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms.\narXiv preprint\narXiv:2205.05131.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, et al. 2021. Pangu-α: Large-\nscale autoregressive pretrained chinese language\nmodels with auto-parallel computation.\narXiv\npreprint arXiv:2104.12369.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Ab-\ndelhak Lakhouaja. 2019. Osian: Open source interna-\ntional arabic news corpus-preparation and integration\ninto the clarin-infrastructure. In Proceedings of the\nFourth Arabic Natural Language Processing Work-\nshop, pages 175–182.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-\nping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. 2022. Designing effective sparse expert mod-\nels. arXiv preprint arXiv:2202.08906.\nA\nPretraining\nA.1\nData Collection\nOur pre-training corpus is mainly sourced from\nthe publicly available web scrapes of the Common\nCrawl (CC) project. We downloaded 90 shards of\nCC monthly data ranging from May 2013 (the ear-\nliest available) up to July 2022. Also, we use an in-\nhouse collection of 47GB of Arabic dialect textual\ndata (DIALECT) in order to enhance the awareness\nof our model to Arabic dialects (Abdul-Mageed\net al., 2021). In addition, we include high-quality\nnews corpora such as NEWS (Zeroual et al., 2019)\nand El-KHAIR (El-Khair, 2016) which are com-\nmonly used in previous Arabic PLM works (Safaya\net al., 2020; Antoun et al., 2020; Nagoudi et al.,\n2022b; Ghaddar et al., 2022). Finally, we use 28GB\nof in-house Arabic data curated from different text\ngenres like literature, books, and Wikipedia.\nSource\nOriginal\nClean\nFiltering %\nCC\n8.7TB\n439GB\n95%\nDIALECT\n-\n47GB\n-\nNEWS\n21GB\n14GB\n34%\nEL-KHEIR\n16GB\n13GB\n19%\nOthers\n28GB\n16GB\n45%\nTotal\n8.8TB\n529GB\n94%\nTable 6: Size of the pre-training corpora before (Origi-\nnal) and after (Clean) applying data filtering and dedu-\nplication heuristics.\nAs it has been shown to be crucial for En-\nglish (Raffel et al., 2019), multilingual (Xue et al.,\n2021), and Arabic (Ghaddar et al., 2022) PLM\nend-tasks performance, we aggressively filter and\ndeduplicate the collected data using the heuristics\ndescribed in (Ghaddar et al., 2022). Table 6 shows\ndata sizes before and after applying the heuristics.\nWhile we discard 95% of CC data, it is still con-\nsidered, along with DIALECT, to form more than\n90% of our 529GB final pre-training corpus.\nA.2\nImplementation Details\nWe use the SentencePiece (Kudo and Richardson,\n2018) tokenizer in order to process text into sub-\ntokens. We train the tokenizer from scratch on our\npre-training corpus by setting the vocabulary size to\n64k, a value which is used commonly by previous\nArabic PLMs (Antoun et al., 2020; Ghaddar et al.,\n2022; Nagoudi et al., 2022a).\nFollowing (Raffel et al., 2019), we pre-train Ara-\nMUS on the Replace corrupted spans tasks with a\nrandom token probability of 15%. The pre-training\ncode is based on the PyTorch (Paszke et al., 2019)\nversion of the Megatron-LM library (Shoeybi et al.,\n2019). AraMUS is pre-trained on 16 sever, each\noccupied with 8 NVIDIA A100 GPUs with 80GB\nmemory. Model and data parallel sizes are set to 4\nand 32 respectively. The total batch size is 4096,\nwhich is based on the max batch size which can\nfit on a single GPU (32). To speed up the pre-\ntraining, we use mixed-precision training (Micike-\nvicius et al., 2018), except when calculating atten-\ntion softmax and when reducing gradients. We use\nthe Adafactor optimizer (Shazeer and Stern, 2018)\nwith an initial learning rate of 0.005, 10k warm-up\nsteps with the inverse square-root scheduler.\nB\nFinetuning\nB.1\nDatasets and Evaluation\nALUE (Seelawi et al., 2021) is a well-established\nbenchmark that consists of a collection of eight\nArabic NLU tasks. Although its datasets are rel-\natively small compared to the one of the English\nGLUE (Wang et al., 2018) benchmark, but it is sup-\nported by a public leaderboard with hidden test sets\nwhich ensures a fair comparison between models.\nFollowing (Seelawi et al., 2021), we report Pear-\nson correlation on SVREG, Jaccard on SEC, and\naccuracy on XNLI, and use the F1 score otherwise.\nWe also report the unweighted average sum over\nthe 8 tasks.\nAs for generative tasks, we follow (Ghaddar\net al., 2022) by considering 3 tasks for evaluation,\nas their datasets are fully open source. We use\nWikilingua (Ladhak et al., 2020) and EASC (El-Haj\net al., 2010) for TS, and the set of datasets used in\n(Abdul-Mageed et al., 2021; Nagoudi et al., 2022b)\nfor QA and QG. We follow (Ghaddar et al., 2022)\nfor splitting the data into train/dev/test, and report\nRouge scores (Lin, 2004) on TS, BLEU (Papineni\net al., 2002) on QG, and Exact Match (EM) and\nF1 score on QA. Therefore, AraMUS results can\nbe directly comparable with the baselines reported\nby (Ghaddar et al., 2022).\nB.2\nBaseline\nWe compared AraMUS with the state-of-the-art\nArabic PLMs that have been evaluated on publicly\navailable datasets, these include:\n• ARBERT and MARBERT are respectively\nMSA and Arabic Dialect BERT-base (Devlin\net al., 2018) models provided by\n(Abdul-\nMageed et al., 2021).\n• JABER and SABER are respectively BERT-\nbase and BERT-large models provided by\n(Ghaddar et al., 2022).\n• ALM-1.0 4 is a recently published Arabic\nBERT-large model.\n• AraT5-base and AT5B are Arabic T5-\nbase (Raffel et al., 2019) models provided by\n(Nagoudi et al., 2022b) and (Ghaddar et al.,\n2022) respectively.\nIt is worth mentioning that it was not possible to\ncompare AraMUS with its counterpart multi-billion\nArabic GPT models because:\nB.2.1\nNOOR\nNOOR (Lakim et al., 2022) is the largest exist-\ning Arabic PLM with 10B parameters. In their\nwork, the authors didn’t make their model publicly\navailable neither reported their results on public\ndatasets.\nB.2.2\nAraGPT-Mega\nAraGPT-Mega (Antoun et al., 2021b) has 1.5B pa-\nrameters and is publicly available for download.\nHowever, we tried to run in-house experiments\nwith this model but it didn’t perform well on many\ntasks. Most likely because it was only pre-trained\non 27GB of Arabic text, which is considered small\ncompared to the model size. Therefore, we pre-\nferred not to report weak results for this model.\nB.2.3\nJasmine\nJasmine (Nagoudi et al., 2022a) is an in-progress\nproject that aims to develop and evaluate a set of\nArabic GPT models up to 13B parameters. This in-\nprogress work was released at the time of writing\nour paper. The authors mentioned that the 13B\nmodel is still at early pre-training stage, while the\n6.7B version is only pre-trained for 143k steps.\nTherefore, their fully pre-trained Jasmine has 2.7B\nparameters only. This model is evaluated, in a few\nshot setting only, on a set of discriminative and\ngenerative tasks on the ARLUE (Abdul-Mageed\net al., 2021) and ARGEN (Nagoudi et al., 2022b)\nbenchmarks respectively. However, many of the\ndatasets in ARLUE and ARGEN have not been\npublicized yet (Elmadany et al., 2022; Ghaddar\net al., 2022). In addition, the authors didn’t open\nsource their model weights nor shared their code to\nreplicate their dataset splits.\nB.3\nImplementation Details\nWe used early stopping based on the perfor-\nmance of the dev sets during our extensive hyper-\nparameter search. We search the learning rate from\n4https://github.com/FlagAI-Open/FlagAI/tree/\nmaster/examples/ALM\nthe set of {5e-5, 1e-4, 2e-4, 1e-3}, batch size from\n{8, 16, 32, 64}, the learning rate scheduler from\n{constant, cosine}, and the dropout rate from {0.1,\n0.15, 0.2, 0.3}, and fixed the epoch number to a\nmaximum of 120 for all the experiments. Each fine-\ntuning experiment uses 4 NVIDIA A100 GPUs,\nwith the model parallel size set to 4.\nAfter finding the best hyper-parameters, we ran\nall the experiments 5 times and reported the aver-\nage score on the dev sets 5, in order to validate the\ncredibility of our results. For each ALUE task, we\nselected the best-performing model among the 5\nruns and used it for the ALUE leaderboard test sub-\nmission, and we computed the scores on generative\ntasks datasets.\nWe simulate a few-shot setting on the ALUE\ntasks by randomly sampling a subset of {8, 16, 32,\n64, 128, 256} examples of the training data. When\nthe number of classes is more than the number of\nsamples (e.g. MDD and SEC with 8 examples) we\nrandomly add one example for each missing class\nin order to ensure that each class has a represented\ndata point. All models are identically fine-tuned,\nand we report the average and standard deviation\nof 5 randomly selected folds.\nC\nFew-Shot Results\n5We use the MQ2Q dev set curated by (Ghaddar et al.,\n2022) to make our results compatible with their baselines.\nModel\nMQ2Q*\nMDD\nSVREG\nSEC\nFID\nOOLD\nXNLI\nOHSD\nAvg.\n8 Examples\nJABER\n50.0±15.8\n8.9±1.8\n18.8±17.5\n21.7±0.2\n56.7±13.5\n56.5±7.9\n35.7±2.3\n54.9±5.9\n37.9±8.1\nSABER\n53.5±6.9\n8.8±1.4\n34.2±09.6\n21.1±0.8\n63.0±11.2\n65.3±12.6\n35.5±1.9\n58.1±7.3\n42.4±6.5\nAraMUS\n60.2±3.7\n16.7±1.8\n54.5±8.7\n23.2±3.5\n69.0±2.8\n69.5±1.6\n35.8±1.1\n58.3±7.7\n48.4±3.9\n16 Examples\nJABER\n56.2±14.5\n7.9±1.1\n45.2±16.1\n24.3±3.0\n69.9±5.6\n68.0±12.5\n37.0±3.4\n53.0±5.7\n45.2±7.7\nSABER\n54.6±8.2\n9.0±2.1\n47.7±16.7\n21.6±1.9\n73.0±2.8\n80.3±7.9\n35.8±2.3\n57.7±8.0\n47.5±6.2\nAraMUS\n61.4±4.7\n20.4±1.9\n66.6±5.6\n25.5±4.8\n74.3±1.2\n82.3±1.7\n39.1±4.9\n59.1±7.5\n53.6±4.0\n32 Examples\nJABER\n66.9±3.3\n8.0±1.8\n63.7±11.7\n27.0±3.3\n72.1±3.9\n71.7±5.9\n38.7±2.9\n57.7±7.7\n50.7±5.1\nSABER\n63.3±6.6\n9.8±2.3\n72.3±9.3\n28.7±4.1\n74.5±1.4\n81.2±9.3\n37.4±1.4\n54.6±7.2\n52.7±5.2\nAraMUS\n69.2±4.3\n21.3±1.1\n74.5±3.6\n28.0±5.0\n74.8±2.6\n85.5±2.1\n45.3±3.7\n59.3±6.9\n57.2±3.7\n64 Examples\nJABER\n68.6±3.5\n11.0±1.9\n72.6±7.8\n31.5±1.5\n73.7±0.8\n77.0±2.7\n42.4±2.2\n58.8±8.4\n54.4±3.6\nSABER\n67.8±2.8\n12.8±1.9\n79.6±3.3\n34.8±1.7\n77.2±1.4\n87.0±2.1\n39.6±4.2\n61.4±7.4\n57.5±3.1\nAraMUS\n74.8±1.8\n22.3±1.0\n81.8±3.7\n31.5±1.8\n77.7±0.7\n89.6±1.5\n55.5±3.6\n64.0±8.7\n62.2±2.8\n128 Examples\nJABER\n70.0±1.5\n16.9±0.6\n80.5±1.3\n35.3±1.8\n76.4±1.1\n82.4±2.8\n44.6±1.0\n64.2±4.0\n58.8±1.8\nSABER\n72.1±0.9\n18.9±2.0\n83.6±2.0\n39.5±2.8\n78.3±1.3\n88.7±1.4\n44.8±4.0\n66.8±4.0\n61.6±2.3\nAraMUS\n77.5±1.1\n25.7±1.7\n84.1±0.9\n37.0±1.4\n78.6±0.5\n90.4±0.9\n63.6±1.5\n67.8±4.1\n65.6±1.5\n256 Examples\nJABER\n72.7±1.0\n22.4±0.6\n83.7±0.7\n39.3±0.8\n79.0±1.1\n84.9±1.0\n53.1±2.2\n62.5±6.2\n62.2±1.7\nSABER\n72.8±1.7\n25.5±1.9\n85.0±1.3\n42.2±0.5\n79.8±1.2\n89.6±0.7\n48.0±13.5\n70.6±1.3\n64.2±2.8\nAraMUS\n78.1±1.2\n30.2±0.8\n86.3±1.3\n41.1±0.7\n80.8±1.7\n92.3±0.9\n72.6±0.7\n71.2±3.4\n69.1±1.3\nTable 7: Dev ALUE performances across training set sizes. Underline figures indicates extra samples where added\nto ensure that each class is represented at least by one data point.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-06-11",
  "updated": "2023-06-11"
}