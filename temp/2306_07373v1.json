{
  "id": "http://arxiv.org/abs/2306.07373v1",
  "title": "EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing",
  "authors": [
    "Iker de la Iglesia",
    "Aitziber Atutxa",
    "Koldo Gojenola",
    "Ander Barrena"
  ],
  "abstract": "The utilization of clinical reports for various secondary purposes, including\nhealth research and treatment monitoring, is crucial for enhancing patient\ncare. Natural Language Processing (NLP) tools have emerged as valuable assets\nfor extracting and processing relevant information from these reports. However,\nthe availability of specialized language models for the clinical domain in\nSpanish has been limited.\n  In this paper, we introduce EriBERTa, a bilingual domain-specific language\nmodel pre-trained on extensive medical and clinical corpora. We demonstrate\nthat EriBERTa outperforms previous Spanish language models in the clinical\ndomain, showcasing its superior capabilities in understanding medical texts and\nextracting meaningful information. Moreover, EriBERTa exhibits promising\ntransfer learning abilities, allowing for knowledge transfer from one language\nto another. This aspect is particularly beneficial given the scarcity of\nSpanish clinical data.",
  "text": "EriBERTa:\nA Bilingual Pre-Trained Language Model for\nClinical Natural Language Processing\nIker de la Iglesia , Aitziber Atutxa , Koldo Gojenola , Ander Barrena\nHiTZ Basque Center for Language Technology\nUniversity of the Basque Country (UPV/EHU), Spain\n{first-name}.{last-name}@ehu.eus\nAbstract\nThe utilization of clinical reports for various sec-\nondary purposes, including health research and\ntreatment monitoring, is crucial for enhancing pa-\ntient care.\nNatural Language Processing (NLP)\ntools have emerged as valuable assets for extract-\ning and processing relevant information from these\nreports.\nHowever, the availability of specialized\nlanguage models for the clinical domain in Span-\nish has been limited.\nIn this paper, we introduce EriBERTa, a bilingual\ndomain-specific language model pre-trained on ex-\ntensive medical and clinical corpora. We demon-\nstrate that EriBERTa outperforms previous Spanish\nlanguage models in the clinical domain, showcas-\ning its superior capabilities in understanding med-\nical texts and extracting meaningful information.\nMoreover, EriBERTa exhibits promising transfer\nlearning abilities, allowing for knowledge transfer\nfrom one language to another. This aspect is partic-\nularly beneficial given the scarcity of Spanish clin-\nical data.\n1\nIntroduction\nExtracting and processing relevant information from clini-\ncal reports pose significant challenges due to their unstruc-\ntured nature and the domain-specific language used in health-\ncare. Natural Language Processing (NLP) techniques have\nemerged as powerful tools for tackling these challenges and\nunlocking the potential of clinical data.\nThe transformer architecture, introduced by Vaswani et al.\n[2017], has revolutionized NLP by enabling the training of\ndeep neural networks capable of capturing contextual rela-\ntionships between words and producing state-of-the-art re-\nsults across various tasks. This architecture has been suc-\ncessfully applied to numerous domains, including healthcare.\nIn the medical and clinical domains, the development of\ndomain-specific language models has further advanced the\nanalysis of clinical data.\nModels such as SciBERT [Belt-\nagy et al., 2019], BioBERT [Lee et al., 2020], and BioAL-\nBERT [Naseem et al., 2021] have been specifically designed\nand trained on biomedical literature to capture the intrica-\ncies of medical terminology and context. These models have\ndemonstrated remarkable performance in various biomedical\nNLP tasks, including named entity recognition, relation ex-\ntraction, and document classification in English. However,\nprogress in the Spanish language has been limited, primarily\ndue to the difficulty of obtaining clinical data and the result-\ning scarcity of available Spanish corpora. Nevertheless, mod-\nels such as roberta-base-biomedical-es [Carrino et al., 2021b]\nand bsc-bio-ehr-es [Carrino et al., 2022] have been developed\nand shown promising performances in Spanish.\nTo address the challenges posed by limited data availabil-\nity in the Spanish language, we present EriBERTa, a bilin-\ngual domain-specific language model tailored specifically for\nthe medical and clinical domains. EriBERTa is pre-trained\non medical and clinical corpora in both English and Span-\nish, enabling it to effectively capture the nuances of medical\nterminology and understand the context of clinical narratives.\nBy bridging the gap between English and Spanish models,\nEriBERTa opens up new opportunities for NLP applications\nin Spanish-speaking healthcare settings.\nIn this paper, firstly, we provide a detailed analysis of\nEriBERTa’s architecture. Secondly, we outline the training\nmethodology employed to pre-train EriBERTa on a large-\nscale medical and clinical corpus, enabling it to learn the in-\ntricacies of medical language. Lastly, we conduct compre-\nhensive evaluations of EriBERTa on benchmark datasets to\ndemonstrate its superiority in both monolingual and cross-\nlingual settings in Spanish, while having competitive perfor-\nmance in English datasets. The experimental findings high-\nlight the potential of EriBERTa to enhance the extraction of\nmeaningful insights from clinical data and contribute to ad-\nvancements in healthcare research and practice.\n2\nPre-Training EriBERTa\nThe EriBERTa model is a pre-trained transformer-based lan-\nguage model designed to enhance the performance of natural\nlanguage understanding tasks in the medical-clinical domain\nfor the Spanish language. It is based on the RoBERTa archi-\ntecture, which is a popular pre-trained model and a variant\nof BERT [Devlin et al., 2019]. EriBERTa uses the same to-\nkenizer and pre-training methods as RoBERTa, as described\nin the work of Liu et al. [2019]. This ensures that EriBERTa\nhas similar performance characteristics to RoBERTa but with\nthe additional advantage of being fine-tuned on English and\narXiv:2306.07373v1  [cs.CL]  12 Jun 2023\nSpanish medical-clinical data. We trained three variations of\nEriBERTa: one using public corpora, another using private\nhospital EHR documents, and a Longformer version based\non the one trained with private hospital data.\nIn this section, we will delve into the details of the pre-\ntraining and evaluation procedures utilized in our research.\nSpecifically, we will begin by describing the corpora used\nfor the Masked Language Modeling (MLM) pre-training and\nwill highlight the differences between the corpora used for\nboth EriBERTa versions. We will then proceed to explain the\nspecific pre-training procedures for each EriBERTa version.\nFinally, we will present the datasets used for evaluation, and\nprovide a detailed account of the evaluation setup for each\ndataset.\n2.1\nCorpora\nTo pre-train the EriBERTa model, we used a combination\nof publicly available and private medical-clinical corpora in\nSpanish and English. The selection of corpora was based on\ntheir relevance to the medical domain and the availability of\nhigh-quality data. However, obtaining medical and clinical\ncorpora can be challenging, especially in the Spanish lan-\nguage.\nOne of the main difficulties in obtaining medical and clini-\ncal corpora is the sensitive nature of the data. Medical records\ncontain highly personal and confidential information, which\nmakes it challenging to collect and share this type of data for\nresearch purposes. These challenges are compounded when\nit comes to the Spanish language. Compared to English, there\nare fewer medical and clinical corpora available in Spanish,\nwhich can make it challenging to obtain sufficient data for\npre-training language models. Furthermore, the quality and\nquantity of available Spanish corpora can be highly variable,\nwhich can impact the performance of pre-trained models.\nDespite the challenges of gathering medical and clinical\ncorpora, we have successfully assembled and curated a di-\nverse collection of relevant corpora in both Spanish and En-\nglish to pre-train the EriBERTa model. Table 1 provides more\ndetailed information on the language and size of each corpus\nused for pretraining.\nThe corpora we used for pretraining EriBERTa are:\n• MIMIC-III [Johnson et al., 2016]: English database\non ICU stays of over 40,000 patients between 2001 and\n2012. It contains information such as vital sign measure-\nments, laboratory test results, procedures, medications,\ncaregiver notes, and mortality, among others.\n• EMEA [Tiedemann, 2012]: A parallel corpus in English\nand Spanish consisting of documents from the European\nMedicines Agency.\n• ClinicalTrials1: Set of English documents on clinical\nstudies carried out worldwide.\n• PubMed: Contains abstracts and full texts of biomedi-\ncal literature from multiple NLM literary sources includ-\n1https://clinicaltrials.gov\nLang\nSource\nNo. Words\nMedical Corpus\nENG\nEMEA\n12M\nPubMed Abstracts\n968.4M\nClinical Trials\n127.4M\nES\nEMEA\n13.6M\nPubMed\n8.4M\nSNOMED-CT\n7.2M\nSPACCC\n350k\nUFAL\n10.5M\nWikipedia (Med)\n5.2M\nMedical Crawler\n918M\nClinical Corpus (EHR)\nENG\nMIMIC-III\n206M\nES\nPrivate Hospital Documents\n222M\nTable 1: Pre-training corpora used classified by document type and\nlanguage.\ning MEDLINE2, PubMed Central3, and Bookshelf4. We\nused abstracts for English and abstracts and full texts for\nSpanish.\n• SNOMED-CT [I.H.T.S.D.O, 2022]: Standardized and\nmultilingual medical vocabulary consisting of more\nthan 300,000 medical concepts, including categories\nsuch as body parts, clinical findings, and pharmaceuti-\ncal/biological products, among others. For this work, the\ndescriptions in Spanish associated with each term were\nused.\n• SPACCC [Intxaurrondo, 2018]: Spanish corpus created\nafter collecting 1,000 clinical cases from SciELO5, and\ncategorizing them based on structure and content into\nthose that were similar to real clinical texts and those\nthat were not suitable for this task.\n• UFAL [ ´UFAL, 2017]:\nMultilingual medical corpus\ncomposed of parallel corpora that have been collected\nover various projects.\n• Wikipedia Med: A Spanish corpus composed of en-\ntries collected from Wikipedia, filtered by scope, and\ncleaned.\n• Private Clinical Documents: Set of Spanish clinical\nnarrative from health centers. It is used exclusively on\nthe private version of EriBERTa.\n2https://www.nlm.nih.gov/medline/medline overview.html\n3https://www.ncbi.nlm.nih.gov/pmc/about/intro/\n4https://www.ncbi.nlm.nih.gov/books/\n5https://scielo.isciii.es/scielo.php\n• Medical Crawler [Carrino et al., 2021a]: A corpus\ncomprising over 3,000 URLs related to Spanish biomed-\nical and health domains, collected through web crawl-\ning.\nIt is used exclusively on the public version of\nEriBERTa.\nIt is important to note that the main difference between the\nprivate and public versions of EriBERTa is the type of clin-\nical documents used for pretraining. The private version of\nEriBERTa was trained using the Private Clinical Documents\ncorpus, but not the Medical Crawler corpus.\nIn contrast,\nthe public version of EriBERTa was trained using the Med-\nical Crawler corpus, but not the Private Clinical Documents\ncorpus. These differences in pretraining corpora may affect\nthe performance of the models on different tasks, particularly\nthose that involve hospital clinical documents.\nBalancing the Corpus\nAs shown in Table 1, the amount of resources in Spanish and\nEnglish for the private version of EriBERTa is imbalanced\n(1,313M words in English and 267M in Spanish). This im-\nbalance could result in English having too much weight when\ngenerating the tokenizer and pretraining the model, leading\nto poor performance in Spanish, as reported in Conneau et al.\n[2020]. Notably, the private EriBERTa version that uses the\ncrawler does not suffer from this imbalance and thus does not\nrequire balancing.\nTo address this issue, we employed the formula (1) pro-\nposed in Conneau and Lample [2019]. Here, ni and pi rep-\nresent the number of words and the frequency of occurrence\nof the language i, respectively, and qi denotes the probability\nthat a word in language i is sampled according to the multino-\nmial distribution. α is a parameter that controls the language\nsampling rate, with lower values reducing the sampling prob-\nability of the most represented languages and increasing the\nprobability of those with scarce resources. Based on the stud-\nies described in Conneau et al. [2020] for multilingual models\nwith few resources for some languages, we decided to balance\nthe corpus using a parameter of α = 0.3.\nqii = 1...N ; qi =\npα\ni\nP j = 1Npα\nj\nwhere pi =\nni\nPN\nj=1 nj\n(1)\nIn the corpus balancing calculations, we decided to omit\nthe section related to the EHR files because it was already\nbalanced: 206M in English and 222M in Spanish. Without\nconsidering these files and applying the formulas in (1), we\nobtain:\npes = 0.04 ; peng = 0.96 ; qes = 0.277 ; qeng = 0.723\n(2)\nTherefore, in order for the probabilities qes and qeng ob-\ntained in (2) to hold, we must multiply the Spanish texts by\n9.5 times, from 45.4M tokens to 438M tokens.\n2.2\nModel Pre-Training\nIn this section, we detail the pre-training process of our\nEriBERTa models, which are based on the RoBERTa-base\n[Liu et al., 2019] and Longformer [Beltagy et al., 2020] ar-\nchitectures. We first describe the tokenizer we used, which is\nthe same as RoBERTa, followed by the vocabulary and pre-\ntraining details. Finally, we will delve into the Longformer\nvariant pre-training details.\nOur tokenizer is based on Byte-Pair Encoding (BPE), in-\ntroduced by Sennrich et al. [2016], which is a data com-\npression technique used to represent a large set of sym-\nbols or words using a smaller vocabulary converting them\nto subwords, allowing the tokenizer to handle rare or out-\nof-vocabulary words. We defined a cased vocabulary with a\nsize of 64,000 that was chosen to accommodate the bilingual\nnature of the models and ensure sufficient coverage of both\nlanguages [Conneau et al., 2020].\nFor pre-training, we used the Masked Language Model-\ning (MLM) objective for all model variants, which involves\nrandomly masking tokens in the input sequence and training\nthe model to predict the original tokens based on the sur-\nrounding context [Devlin et al., 2019]. To generate the pre-\ntraining data, we concatenated all the raw text from the cor-\npora and then split the concatenated text into segments of the\nmaximum input length allowed by each model. We trained\nthe RoBERTa-based EriBERTa models from scratch for 125k\nsteps, with checkpoints saved every 2.5k steps. The model\nparameters and training hyperparameters are defined in Ta-\nble 2.\nTraining Hyperparameters\nNumber of Layers\n12\nBatch Size (tokens)\n2,083,840\nHidden Size\n768\nWeight Decay\n0.0\nFFN inner hidden size\n3,072\nMax Steps\n125k\nAttention heads\n12\nLearning Rate Decay\nLinear with warmup\nDropout\n0.1\nAdam ϵ\n1e-08\nAttention Dropout\n0.1\nAdam β1\n0.9\nWarmup Steps\n7.5k\nAdam β2\n0.99\nPeak Learning Rate\n2.683e-4\nGradient Clipping\n1\nTable 2: Parameter and hyperparameter details for EriBERTa mod-\nels.\nPre-Training EriBERTa-Longformer\nMedical and clinical documents often pose a challenge for\nmost transformer-based models due to their length which fre-\nquently exceeds the token limit of 512 or approximately 320\nwords. Various approaches have been proposed to address\nthis issue, including text summarization, truncation, or split-\nting by paragraphs [Sun et al., 2019]. However, these meth-\nods may not be suitable for tasks where global context is cru-\ncial, and the text cannot be altered, such as clinical section\ndetection.\nTo overcome this challenge, we introduce a Longformer\nvariant of EriBERTa, based on the Longformer architecture\n[Beltagy et al., 2020]. The Longformer extends the input se-\nquence length by using a sliding window mechanism up to\n4,096 tokens, enabling access to longer contexts in medical\ndocuments. The Longformer implementation also includes a\nglobal attention mechanism that attends to all tokens in the\ninput sequence, which can capture dependencies beyond the\nlocal context. It is important to note that the global attention\nmechanism must be manually configured for each task and is\nnot pre-trained.\nThe Longformer version of EriBERTa was initialized with\nthe pre-trained weights of the EriBERTa model trained with\nprivate clinical documents using the process defined by Belt-\nagy et al. [2020]. We further pre-trained the model using the\nMLM objective with the same corpus used for the original\nEriBERTa model. This step allowed the model to adapt to\nthe new attention system using the same hyperparameters de-\nscribed in Beltagy et al. [2020], as represented in Table 3.\nTraining Hyperparameters\nMax Steps\n65k\nBatch Size (tokens)\n262.144\nWarmup Steps\n500\nPeak Learning Rate\n3e-5\nLearning Rate Decay\nPower 3 polynomial\nWeight Decay\n0.01\nAdam ϵ\n1e-6\nAdam β1\n0.9\nAdam β2\n0.9\nTable 3: Parameter and hyperparameter details for EriBERTa Long-\nformer model.\n3\nFine-Tuning EriBERTa\nIn this section, we present the results of fine-tuning the\nEriBERTa models on multiple standard named entity recog-\nnition (NER) datasets in the medical domain for both English\nand Spanish languages. These datasets include annotations\nfor various types of medical entities, such as diseases, symp-\ntoms, and treatments. They are widely adopted, enabling us\nto better compare the performance of our models to existing\nones. Additionally, we include a set of datasets generated\nwith real hospital clinical documents to evaluate the effec-\ntiveness in real-world scenarios.\nWe first describe the datasets used for evaluation, followed\nby the experimental setup, which includes training and eval-\nuation procedures. We then report the performance of each\nmodel on the datasets. Overall, our results provide insights\ninto the effectiveness of EriBERTa and its Longformer vari-\nant in NER tasks and demonstrate their potential for advanc-\ning clinical NLP applications.\n3.1\nDatasets\nIn the context of English datasets, we utilized four commonly\nemployed datasets, which have been widely used by exist-\ning models in the field. Firstly, the BC5CDR dataset [Li\net al., 2016] comprises abstracts annotated with chemicals\n(BC5CDR-chem) and diseases (BC5CDR-disease). Secondly,\nthe JNLPBA dataset [Kim et al., 2004] is derived from MED-\nLINE 47 and consists of 2,000 manually annotated abstracts\nencompassing five categories: protein, DNA, RNA, cell line,\nand cell. Thirdly, the NCBI-disease corpus [Do˘gan et al.,\n2014] is specifically designed for disease recognition and\nconcept normalization. We focused solely on entity recogni-\ntion. Lastly, the BC4CHEM dataset [Krallinger et al., 2015],\nalso known as the CHEMDNER dataset, emphasizes chemi-\ncal text mining.\nIn the case of Spanish datasets, the availability of clini-\ncal datasets is relatively limited compared to the English lan-\nguage due to the challenges associated with acquiring such\ncorpora. However, we incorporated two publicly available\ndatasets specifically tailored to the clinical-medical domain.\nFirstly, the PharmaCoNER dataset [Gonzalez-Agirre et al.,\n2019] annotates mentions of drugs and substances in clini-\ncal texts. Secondly, the Cantemist-NER dataset [Miranda-\nEscalada et al., 2020a] consists of 1,301 files comprising clin-\nical case notes with annotations of tumor morphology men-\ntions.\nTo investigate the transfer learning capabilities between\nlanguages, we employed the DIANN dataset [Fabregat et al.,\n2018]. This dataset presents a bilingual setting where the\nsame document is available in both Spanish and English. The\nentities to be identified in this dataset are mentions of disabil-\nities, including terms such as “low vision” and “deafness”,\namong others. By applying a zero-shot context, we aim to\nevaluate the model’s capacity to transfer information effec-\ntively from one language to another.\nFinally, to better assess the suitability of the model for\ntasks involving real-world clinical documents, we evaluated\nit in private hospital datasets in both medical entity recogni-\ntion (MER), with 7 distinct medical entity types, and clinical\nsection classification where a clinical note must be classified\nin various sections like Present Illness, Exploration, Treat-\nment, etc.\nFinally, in order to evaluate the models’ applicability to\nreal-world clinical documents, we conducted evaluations on\nprivate hospital datasets. The evaluation encompassed two\nkey tasks: medical entity recognition (MER) and clinical sec-\ntion classification. For MER, we used a revisited version of\nthe dataset presented in Casillas et al. [2019] that focuses on\nidentifying 7 distinct medical entity types within the clinical\ndocuments. In addition, for clinical section classification, the\nobjective was to accurately classify clinical notes into vari-\nous sections, such as ’’Present Illness´´, ”Exploration´´ and\n”Treatment´´. For this task, we used 3 different datasets\nthat differ in the type of clinical note: semi-structured with\nsection headers, semi-structured without section headers, and\nunstructured. For the first two cases, we used the dataset pre-\nsented in Goenaga et al. [2021], for the last one we annotated\nthe corpora of the CodiEsp dataset [Miranda-Escalada et al.,\n2020b].\n3.2\nExperimental Results\nIn this section, we present the experimental setup and results\nof evaluating the EriBERTa language model in the medical-\nclinical domain. The evaluation consists of three subsections:\nSingle-Language Evaluation, where the model is tested on\nboth Spanish and English datasets; Transfer-Learning Eval-\nuation, where the model’s zero-shot capabilities are assessed\nin transferring knowledge between languages; and Evaluation\nin Real-World Clinical Tasks, where the model performance\nwith actual clinical notes from hospitals is analyzed. In or-\nder to evaluate the NER results, we employed the SeqEval\nlibrary [Nakayama, 2018], which calculates the precision, re-\ncall, and F1 scores for each entity type, as well as the mi-\ncro and macro averages. We conducted fine-tuning for both\nversions of EriBERTa, the private and public models, with\na batch size of 32 and a learning rate of 2.5e −5, 5e −5,\n7.5e −5 using a linear learning rate scheduler with a 2%\nwarm-up. The AdamW optimizer was employed, while the\nremaining hyperparameters were kept at their default values\nprovided by HuggingFace’s transformers library [Wolf et al.,\n2020].\nSingle Language Evaluation\nFor the Spanish language evaluation, we compared the perfor-\nmance of our EriBERTa models against two reference mod-\nels: a generic Spanish language model called BETO [Ca˜nete\net al., 2020] and the current state-of-the-art (SOTA) model for\nclinical text analysis in Spanish, BSC-BIO-EHR-ES [Carrino\net al., 2022]. Specifically, for the PharmaCoNER dataset, we\nexplored two approaches to examine the impact of note length\non model performance. The first approach involved providing\nthe model with a single sentence as input at a time, while the\nsecond approach entailed giving the full note as a single in-\nput. The former approach aligns with the methodology used\nfor the CANTEMIST dataset and is the one employed in Car-\nrino et al. [2022] for both datasets.\nThe results of this comparative analysis are presented in\nTable 4, which demonstrates that both versions of EriBERTa\noutperform the reference models in terms of performance\nmetrics.\nEriBERTa\nDataset\nMetric (Micro)\nBETO\nBSC-BIO-EHR-ES\nPrivate\nPublic\nCANTEMIST\n[by sentence]\nP\n81.36\n81.30\n82.92\n82.82\nR\n84.18\n86.00\n86.40\n86.51\nF1\n82.75\n83.59\n84.62\n84.62\nPharmaCoNER\n[by sentence]\nP\n87.12\n87.58\n89.58\n89.26\nR\n89.28\n90.73\n91.39\n91.47\nF1\n88.18\n89.13\n90.48\n90.35\nPharmaCoNER\n[full-text]\nP\n85.90\n85.26\n88.78\n89.40\nR\n88.75\n88.04\n90.64\n90.65\nF1\n87.30\n86.63\n89.70\n90.02\nTable 4: Performance comparison of EriBERTa models with refer-\nence models in Spanish medical tasks. The reported results for the\nEriBERTa model are the average score of 3 experiments with differ-\nent random seeds.\nIn order to assess the performance of EriBERTa in English\ndatasets, we conducted a comparison with several state-of-\nthe-art models commonly used in the field. Specifically, we\ncompared EriBERTa with SciBERT, BioBERT, and BioAL-\nBERT, which represents the current SOTA in English tasks.\nThe detailed results of this comparative analysis are presented\nin Table 5. Significantly, EriBERTa exhibits commendable\nperformance that aligns with two established language mod-\nels, SciBERT and BioBERT, despite the added complexity\nassociated with its bilingual nature and reduced pertaining\ncorpora.\nEriBERTa\nDataset\nMetric\nSciBERT\nBioBERT*\nBioALBERT*\nPrivate\nPublic\nNCBI-disease\nP\n-\n88.22\n97.18\n85.07\n85.82\nR\n-\n91.25\n97.18\n89.58\n88.85\nF1\n88.57\n89.71\n97.18\n87.27\n87.31\nBC5CDR-disease\nP\n-\n86.47\n99.27\n83.71\n82.76\nR\n-\n87.84\n96.33\n86.20\n87.27\nF1\n84.70*\n87.15\n97.78\n84.94\n84.95\nBC5CDR-chem\nP\n-\n93.68\n99.99\n91.80\n92.12\nR\n-\n93.26\n96.24\n92.94\n92.52\nF1\n92.51*\n93.47\n98.08\n92.37\n92.32\nBC4CHEMD\nP\n-\n92.80\n97.71\n90.40\n89.95\nR\n-\n91.92\n94.83\n90.13\n89.75\nF1\n-\n92.36\n96.25\n90.26\n89.85\nJNLPBA\nP\n-\n72.68\n86.23\n70.50\n70.76\nR\n-\n83.21\n81.90\n83.49\n83.71\nF1\n77.28\n77.59\n84.01\n76.44\n76.68\nTable 5: Performance comparison of EriBERTa models with refer-\nence models in English medical tasks. For the BioBERT and BioAL-\nBERT models, the best reported results among multiple model ver-\nsions are showcased for each dataset. Due to the absence of separate\nresults for the BC5CDR dataset in SciBERT, we utilized the reported\nresults provided by Wang et al. [2023]. The reported results for the\nEriBERTa model are the average score of 3 experiments with differ-\nent random seeds.\nTransfer Learning in Zero-Shot Scenarios Evaluation\nIn order to evaluate the zero-shot capabilities of the EriBERTa\nlanguage model, we conducted experiments in a cross-lingual\ntransfer learning scenario. The objective was to assess the\nmodel’s ability to transfer learned information from one lan-\nguage to another without any explicit training in the target\nlanguage. We selected a parallel dataset, DIANN, which con-\ntains documents available in both English and Spanish. By\nfine-tuning the EriBERTa model on one language and evalu-\nating its performance on the other, we investigated the extent\nto which the model could generalize and adapt its knowledge\nacross languages. The results of this zero-shot evaluation,\nshowcased in Table 6, provide insights into the model’s cross-\nlingual transfer learning capabilities and shed light on its po-\ntential for practical applications in multilingual medical text\nanalysis.\nWe also compared its performance with the models pre-\nsented by Goenaga et al. [2023].\nIn their study, they as-\nsessed the performance of the XLM-RoBERTa [Conneau et\nal., 2020] model and multiple FLAIR [Akbik et al., 2019]\napproaches in the Spanish zero-shot case.\nThe compara-\ntive analysis, presented in Table Table 7, highlights the best\nconfiguration for each approach, clearly demonstrating that\nEriBERTa surpasses the performance of these models. These\nfindings underscore the superior performance and efficacy of\nTraining Language\nZero-Shot Language\nTraining\nLanguage\nModel\nLR\nEpochs\ndev\ntest\ndev\ntest\nEN\nEriBERTa\n5e-5\n15\n89,43±0,51\n80,83±0,73\n68,93±4,12\n67,12±4,53\nEriBERTa-Public\n5e-5\n15\n89,06±0,69\n80,89±1,66\n76,03±3,93\n71,97±2,12\nES\nEriBERTa\n5e-5\n15\n83,30±1,30\n79,09±1,73\n70,12±2,04\n63,06±2,55\nEriBERTa-Public\n5e-5\n15\n86,79±0,52\n81,78±1,32\n75,21±6,10\n71,33±4,83\nTable 6: Performance comparison of EriBERTa in a zero-shot trans-\nfer learning scenario, evaluated on the DIANN dataset for English\nand Spanish. The results are presented as the mean and standard de-\nviation of 5 runs with different random seeds.\nEriBERTa in cross-lingual transfer learning, solidifying its\nposition as a leading language model in the medical domain\nfor the Spanish language.\nSystem\nPrecision\nRecall\nMicro F1-Score\nFLAIR ME\n50.64 (56,98)\n34,50 (44,54)\n41,04 (50,00)\nXLM-RoBERTa\n37,21 (50,00)\n6,37 (11,79)\n10,88 (19,08)\nEriBERTa Private\n67,51\n66,81\n67,12\nEriBERTa Public\n68.46\n76.24\n71,97\nTable 7: Results of the zero-shot evaluation of the EriBERTa mod-\nels and the state-of-the-art systems, trained on English data, on the\nSpanish test set, with post-processing results in parentheses.\nEvaluation in Real-World Clinical Tasks\nThe evaluation on private hospital datasets focused on two\nkey tasks: medical entity recognition (MER) and clinical\nsection classification. The evaluation results are presented\nin Table 8.\nIn this evaluation, we observed notable per-\nformance differences among the model variants.\nThe pri-\nvate versions, pre-trained on actual clinical Electronic Health\nRecords (EHRs), demonstrated superior performance com-\npared to the public version, which lacked such corpora in\nits pretraining.\nThis outcome highlights the significance\nof leveraging EHR data during pretraining to enhance the\nmodel’s understanding of clinical language and improve its\nperformance on clinical tasks.\nMoreover, we introduced the Longformer version, specifi-\ncally tailored to address the inherent length of clinical notes.\nGiven that clinical documents have substantial length, Long-\nformer’s attention mechanism yielded distinct advantages in\nhandling extensive text spans. Notably, the Longformer vari-\nant outperformed both the private and public base versions,\nparticularly in the clinical section classification task, which\nnecessitates a broader contextual scope compared to the MER\ntask, where local context assumes greater importance.\n4\nConclusion\nIn this study, we have introduced EriBERTa, a domain-\nspecific language model tailored for the medical and clini-\ncal domains, with a particular focus on bilingual capabilities.\nThrough performance evaluations on benchmark datasets,\nEriBERTa has exhibited exceptional proficiency in medical\nEriBERTa\nTask\nPrivate\nLongformerP rivate\nPublic\nMedical Entity Recognition (MER)\n89,42\n89,49\n89.10\nSections in Semi-Structured Discharge Reports\n84,66\n85,46\n82,61\nSections in Semi-Structured Discharge Reports\n[without section indicator]\n73,71\n74,74\n69,49\nSections in Unstructured Evolutionary Reports\n72,55\n72,62\n72,19\nTable 8: Evaluation results on private hospital datasets for medical\nentity recognition (MER) and clinical section classification tasks.\nentity recognition, clinical section classification, and other\npertinent tasks. Notably, EriBERTa has surpassed the perfor-\nmance of Spanish reference models and achieved comparable\nresults to two widely utilized English language models in this\ndomain. These results underscore its state-of-the-art perfor-\nmance and its potential to contribute significantly to advance-\nments in healthcare research, public health surveillance, and\nclinical decision-making.\nOne of the notable strengths of EriBERTa lies in its ca-\npacity for transfer learning between languages.\nThis at-\ntribute proves particularly advantageous in the case of Span-\nish, where the availability of clinical data is limited. Lever-\naging its transfer capability, EriBERTa can bridge the gap\ncaused by data scarcity and facilitate meaningful applications\nin clinical research and practice.\nMoving forward, our future work will focus on exploring\nthe advantageous applications of EriBERTa’s transfer learn-\ning capabilities, specifically harnessing the vast amount of\navailable English data.\nBy effectively leveraging this re-\nsource, we aim to overcome the challenges associated with\ndata scarcity and unlock further potential for impactful con-\ntributions in the field.\nAcknowledgements\nThis work was partially funded by the Spanish Min-\nistry of Science and Innovation (MCI/AEI/FEDER, UE,\nDOTT-HEALTH/PAT-MED\nPID2019-106942RB-C31),\nthe Basque Government (IXA IT1570-22), MCIN/AEI/\n10.13039/501100011033, European Union NextGeneration\nEU/PRTR (DeepR3,\nTED2021-130295B-C31),\nthe EU\nERA-Net CHIST-ERA, and the Spanish Research Agency\n(ANTIDOTE PCI2020-120717-2).\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul,\nStefan Schweter, and Roland Vollgraf.\nFlair: An easy-\nto-use framework for state-of-the-art nlp. In Proceedings\nof the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics (Demon-\nstrations), pages 54–59, 2019.\nIz Beltagy, Kyle Lo, and Arman Cohan.\nScibert: A pre-\ntrained language model for scientific text.\nIn Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-\n7, 2019, pages 3613–3618. Association for Computational\nLinguistics, 2019.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\nLong-\nformer:\nThe\nlong-document\ntransformer.\nCoRR,\nabs/2004.05150, 2020.\nCasimiro Pio Carrino, Jordi Armengol-Estap´e, Ona de Gib-\nert Bonet,\nAsier Guti´errez-Fandi˜no,\nAitor Gonzalez-\nAgirre, Martin Krallinger, and Marta Villegas.\nSpanish\nbiomedical crawled corpus: A large, diverse dataset for\nspanish biomedical language models, 2021.\nCasimiro\nPio\nCarrino,\nJordi\nArmengol-Estap´e,\nAsier\nGuti´errez-Fandi˜no, Joan Llop-Palao, Marc P`amies, Aitor\nGonzalez-Agirre, and Marta Villegas.\nBiomedical and\nclinical language models for spanish: On the benefits of\ndomain-specific pretraining in a mid-resource scenario,\n2021.\nCasimiro Pio Carrino, Joan Llop, Marc P`amies, Asier\nGuti´errez-Fandi˜no,\nJordi\nArmengol-Estap´e,\nJoaqu´ın\nSilveira-Ocampo,\nAlfonso Valencia,\nAitor Gonzalez-\nAgirre,\nand Marta Villegas.\nPretrained biomedical\nlanguage models for clinical NLP in Spanish.\nIn Pro-\nceedings of the 21st Workshop on Biomedical Language\nProcessing, pages 193–199, Dublin, Ireland, May 2022.\nAssociation for Computational Linguistics.\nArantza Casillas, Nerea Ezeiza, Iakes Goenaga, Alicia P´erez,\nand Xabier Soto. Measuring the effect of different types\nof unsupervised word representations on medical named\nentity recognition. International Journal of Medical Infor-\nmatics, 129:100–106, 2019.\nJos´e Ca˜nete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui\nHo, Hojin Kang, and Jorge P´erez. Spanish pre-trained bert\nmodel and evaluation data. In PML4DC at ICLR 2020,\n2020.\nAlexis Conneau and Guillaume Lample. Cross-lingual lan-\nguage model pretraining.\nIn Hanna M. Wallach, Hugo\nLarochelle, Alina Beygelzimer, Florence d’Alch´e-Buc,\nEmily B. Fox, and Roman Garnett, editors, Advances in\nNeural Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 7057–7067, 2019.\nAlexis Conneau,\nKartikay Khandelwal,\nNaman Goyal,\nVishrav\nChaudhary,\nGuillaume\nWenzek,\nFrancisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. Unsupervised cross-lingual repre-\nsentation learning at scale. In Dan Jurafsky, Joyce Chai,\nNatalie Schluter, and Joel R. Tetreault, editors, Proceed-\nings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 8440–8451. Association for Computational\nLinguistics, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding.\nIn Jill Burstein,\nChristy Doran, and Thamar Solorio, editors, Proceedings\nof the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computational\nLinguistics, 2019.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong Lu.\nNcbi disease corpus: A resource for disease name recog-\nnition and concept normalization. Journal of Biomedical\nInformatics, 47:1–10, 2014.\nHermenegildo Fabregat, Juan Mart´ınez-Romo, and Lourdes\nAraujo.\nOverview of the DIANN task: Disability an-\nnotation task.\nIn Paolo Rosso, Julio Gonzalo, Raquel\nMart´ınez, Soto Montalvo, and Jorge Carrillo de Albornoz,\neditors, Proceedings of the Third Workshop on Evalua-\ntion of Human Language Technologies for Iberian Lan-\nguages (IberEval 2018) co-located with 34th Conference\nof the Spanish Society for Natural Language Processing\n(SEPLN 2018), Sevilla, Spain, September 18th, 2018, vol-\nume 2150 of CEUR Workshop Proceedings, pages 1–14.\nCEUR-WS.org, 2018.\nIakes Goenaga, Xabier Lahuerta, Aitziber Atutxa, and Koldo\nGojenola.\nA section identification tool:\nTowards hl7\ncda/ccr standardization in spanish discharge summaries.\nJournal of Biomedical Informatics, 121:103875, 2021.\nIakes Goenaga, Edgar Andres, Koldo Gojenola, and Aitziber\nAtutxa. Advances in Monolingual and Crosslingual Au-\ntomatic Disability Annotation in Spanish. BMC Bioinfor-\nmatics, 2023.\nAitor Gonzalez-Agirre, Montserrat Marimon, Ander Intx-\naurrondo, Obdulia Rabal, Marta Villegas, and Martin\nKrallinger.\nPharmaconer: Pharmacological substances,\ncompounds and proteins named entity recognition track. In\nProceedings of The 5th Workshop on BioNLP Open Shared\nTasks, pages 1–10, Hong Kong, China, November 2019.\nAssociation for Computational Linguistics.\nI.H.T.S.D.O. SNOMED CT - Starter Guide. International\nHealth Terminology Standards Development Organisation,\nOnline, jun 2022.\nInstitute of Formal and Applied Linguistics ( ´UFAL). UFAL:\nMedical Corpus.\nhttps://ufal.mff.cuni.cz/ufal medical\ncorpus, 2017.\nAnder Intxaurrondo. Spaccc, November 2018. Funded by\nthe Plan de Impulso de las Tecnolog´ıas del Lenguaje (Plan\nTL).\nAlistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman\nLi-wei, Mengling Feng, Mohammad Ghassemi, Benjamin\nMoody, Peter Szolovits, Leo Anthony Celi, and Roger G\nMark. Mimic-iii, a freely accessible critical care database.\nScientific data, 3:160035, 2016.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka\nTateisi, and Nigel Collier. Introduction to the bio-entity\nrecognition task at jnlpba.\nIn Proceedings of the Inter-\nnational Joint Workshop on Natural Language Processing\nin Biomedicine and Its Applications, JNLPBA ’04, page\n70–75, USA, 2004. Association for Computational Lin-\nguistics.\nMartin Krallinger, Obdulia Rabal, Florian Leitner, Miguel\nVazquez, David Salgado, Zhiyong lu, Robert Leaman,\nYanan Lu, Donghong Ji, Daniel Lowe, Roger Sayle,\nRiza Batista-Navarro, Rafal Rak, Torsten Huber, Tim\nRockt¨aschel, S´ergio Matos, David Campos, Buzhou Tang,\nWang Qi, and Alfonso Valencia. The chemdner corpus of\nchemicals and drugs and its annotation principles. Journal\nof Cheminformatics, 7:S2, 03 2015.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim,\nSunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert:\na pre-trained biomedical language representation model\nfor biomedical text mining. Bioinform., 36(4):1234–1240,\n2020.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-\nHsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J\nMattingly, Thomas C Wiegers, and Zhiyong Lu. BioCre-\native V CDR task corpus: a resource for chemical disease\nrelation extraction. Database (Oxford), 2016, May 2016.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov.\nRoberta:\nA ro-\nbustly optimized BERT pretraining approach.\nCoRR,\nabs/1907.11692, 2019.\nAntonio Miranda-Escalada,\nEul`alia Farr´e,\nand Martin\nKrallinger. Named entity recognition, concept normaliza-\ntion and clinical coding: Overview of the cantemist track\nfor cancer text mining in spanish, corpus, guidelines, meth-\nods and results. IberLEF@ SEPLN, pages 303–323, 2020.\nAntonio Miranda-Escalada, Aitor Gonzalez-Agirre, Jordi\nArmengol-Estap´e, and Martin Krallinger. Overview of au-\ntomatic clinical coding: Annotations, guidelines, and so-\nlutions for non-english clinical cases at codiesp track of\nCLEF ehealth 2020. In Linda Cappellato, Carsten Eick-\nhoff, Nicola Ferro, and Aur´elie N´ev´eol, editors, Working\nNotes of CLEF 2020 - Conference and Labs of the Evalua-\ntion Forum, Thessaloniki, Greece, September 22-25, 2020,\nvolume 2696 of CEUR Workshop Proceedings. CEUR-\nWS.org, 2020.\nHiroki Nakayama.\nseqeval: A python framework for se-\nquence labeling evaluation, 2018. Software available from\nhttps://github.com/chakki-works/seqeval.\nUsman Naseem, Matloob Khushi, Vinay Reddy, Sakthivel\nRajendran, Imran Razzak, and Jinman Kim.\nBioalbert:\nA simple and effective pre-trained language model for\nbiomedical named entity recognition.\nIn International\nJoint Conference on Neural Networks, IJCNN 2021, Shen-\nzhen, China, July 18-22, 2021, pages 1–7. IEEE, 2021.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units. In\nProceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 1715–1725, Berlin, Germany, August 2016. Associ-\nation for Computational Linguistics.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How to\nfine-tune BERT for text classification? In Maosong Sun,\nXuanjing Huang, Heng Ji, Zhiyuan Liu, and Yang Liu, ed-\nitors, Chinese Computational Linguistics - 18th China Na-\ntional Conference, CCL 2019, Kunming, China, October\n18-20, 2019, Proceedings, volume 11856 of Lecture Notes\nin Computer Science, pages 194–206. Springer, 2019.\nJ¨org Tiedemann. Parallel data, tools and interfaces in opus.\nIn Nicoletta Calzolari (Conference Chair), Khalid Choukri,\nThierry Declerck, Mehmet Ugur Dogan, Bente Maegaard,\nJoseph Mariani, Jan Odijk, and Stelios Piperidis, edi-\ntors, Proceedings of the Eight International Conference on\nLanguage Resources and Evaluation (LREC’12), Istanbul,\nTurkey, may 2012. European Language Resources Associ-\nation (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In Advances in\nneural information processing systems, pages 5998–6008,\n2017.\nBenyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen,\nPrayag Tiwari, Zhao Li, and Jie fu. Pre-trained language\nmodels in biomedical domain: A systematic survey, 2023.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison,\nSam Shleifer, Patrick von Platen, Clara Ma, Yacine Jer-\nnite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander M.\nRush. Transformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online, October 2020. As-\nsociation for Computational Linguistics.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-06-12",
  "updated": "2023-06-12"
}