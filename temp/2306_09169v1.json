{
  "id": "http://arxiv.org/abs/2306.09169v1",
  "title": "Opportunities for Large Language Models and Discourse in Engineering Design",
  "authors": [
    "Jan Göpfert",
    "Jann M. Weinand",
    "Patrick Kuckertz",
    "Detlef Stolten"
  ],
  "abstract": "In recent years, large language models have achieved breakthroughs on a wide\nrange of benchmarks in natural language processing and continue to increase in\nperformance. Recently, the advances of large language models have raised\ninterest outside the natural language processing community and could have a\nlarge impact on daily life. In this paper, we pose the question: How will large\nlanguage models and other foundation models shape the future product\ndevelopment process? We provide the reader with an overview of the subject by\nsummarizing both recent advances in natural language processing and the use of\ninformation technology in the engineering design process. We argue that\ndiscourse should be regarded as the core of engineering design processes, and\ntherefore should be represented in a digital artifact. On this basis, we\ndescribe how foundation models such as large language models could contribute\nto the design discourse by automating parts thereof that involve creativity and\nreasoning, and were previously reserved for humans. We describe how\nsimulations, experiments, topology optimizations, and other process steps can\nbe integrated into a machine-actionable, discourse-centric design process.\nFinally, we outline the future research that will be necessary for the\nimplementation of the conceptualized framework.",
  "text": "Opportunities for Large Language Models and\nDiscourse in Engineering Design\nJan G¨opfert1,2*, Jann M. Weinand1, Patrick Kuckertz1,\nDetlef Stolten1,2\n1*Institute of Energy and Climate Research – Techno-economic Systems\nAnalysis (IEK-3), Forschungszentrum J¨ulich GmbH, J¨ulich, 52425,\nGermany.\n2Chair for Fuel Cells, RWTH Aachen University, Faculty of Mechanical\nEngineering, Aachen, 52062, Germany.\n*Corresponding author(s). E-mail(s): j.goepfert@fz-juelich.de;\nContributing authors: j.weinand@fz-juelich.de; p.kuckertz@fz-juelich.de;\nd.stolten@fz-juelich.de;\nAbstract\nIn recent years, large language models have achieved breakthroughs on a wide\nrange of benchmarks in natural language processing and continue to increase in\nperformance. Recently, the advances of large language models have raised interest\noutside the natural language processing community and could have a large impact\non daily life. In this paper, we pose the question: How will large language models\nand other foundation models shape the future product development process? We\nprovide the reader with an overview of the subject by summarizing both recent\nadvances in natural language processing and the use of information technology\nin the engineering design process. We argue that discourse should be regarded as\nthe core of engineering design processes, and therefore should be represented in\na digital artifact. On this basis, we describe how foundation models such as large\nlanguage models could contribute to the design discourse by automating parts\nthereof that involve creativity and reasoning, and were previously reserved for\nhumans. We describe how simulations, experiments, topology optimizations, and\nother process steps can be integrated into a machine-actionable, discourse-centric\ndesign process. Finally, we outline the future research that will be necessary for\nthe implementation of the conceptualized framework.\nKeywords: product development process, conceptual design, design methodology,\ndesign generation, natural language processing, foundation models, multi-modal models\n1\narXiv:2306.09169v1  [cs.CL]  15 Jun 2023\n1 Introduction\nLarge language models (LLMs) have transformed the field of natural language pro-\ncessing (NLP) and increasingly have an impact outside of academia. LLMs already\ndominate almost every benchmark in natural language understanding (e.g., [1]) and\ncurrent research focuses on extending them by means of other modalities such as\nimages, videos, or sensor signals [2, 3]. Many research fields, such as medicine [4] and\nchemistry [5], are discussing the future implications of these models for their field. The\nengineering sciences are a knowledge-intensive domain, which is likely to experience\ngreat progress via the adaptation of recent methods developed in the NLP community.\nIn this paper, we argue that foundation models such as LLMs can be used for cre-\native reasoning tasks in the engineering design process, complementing and integrating\nexisting computational methods such as topology optimization.\nFirst, we provide engineers with a summary of the recent advances in NLP and\noutline which aspects of engineering design have been digitized thus far (Section 2).\nIn Section 3, we place goal-oriented, argumentative discourse at the center of the\nproduct development process (see Figure 1) and propose making the reasoning steps\nexplicit in the form of a new digital artifact. On this basis, we describe how LLMs\nand multi-modal foundation models can assist in the design discourse (Section 4) and\noutline interesting directions of future research (Section 5). The presented ideas are\ntransferable to other contexts in which creativity and reasoning play an important\nrole, such as scientific discovery in general.\nGoal-oriented, argumenta\u0001ve discourse  \nAr\u0001facts\n(e.g., simula\u0001on \nresults, drawings, \nor 3D models)\nAc\u0001ons\n(e.g., conduc\u0001ng \nexperiments or \nsimula\u0001ons)\n…\n…\nFig. 1 The engineering design process depicted as a goal-oriented, argumentative discourse formed by\ninter- and intrapersonal communication in which machines can participate. As part of this discourse,\nexternal actions are invoked that in turn inform it.\n2\n2 Background\nAs this article bridges multiple domains, not all of which may be familiar to most\nreaders, we provide a thorough background for LLMs and foundation models as well\nas the digitization of engineering design.\n2.1 LLMs and foundation models\nIn its early years, NLP applications relied on hard-coded rules. When the body of\ndigitally-available text increased statistical methods became more prominent. Roughly\nfrom 2013 onward, NLP began to be dominated by machine-learning methods, in\nparticular deep learning ones. [6]\nBuilding on the distributional hypothesis, which states that words tend to have\nsimilar meanings if they occur in similar contexts [7], words (and other text units)\nhave been represented in dense multi-dimensional vector representations using self-\nsupervised learning [8–10]. These word embeddings allow for the similarity between\nwords to be calculated based on vector distances, and as they constitute a rich feature,\nthey have been a popular component of many NLP pipelines. However, these word\nembeddings have been static; that is, they are context-independent (e.g., the word\n‘seal’ has the same vector representation regardless of whether it refers to the animal\nor machine element). Contextual word embeddings based on language models resulted\nin significant improvements over a wide range of NLP benchmarks [11, 12].\nIn 2017, the transformer architecture was proposed [13], and has been the domi-\nnant neural network architecture for language models ever since, replacing previous\napproaches based on feed-forward models, recurrent neural networks, or long short-\nterm memory networks. The transformer architecture consists of an encoder and\ndecoder. Various large language models have been proposed utilizing either the encoder\n(e.g., the BERT family of models [12, 14]), decoder (e.g., the GPT [15–17], BLOOM\n[18] or LLaMA [19] family of models), or both (e.g., the T5 and UL2 model family\n[20–22]) of the transformer architecture. Language models are typically trained on\npredicting the next token or masked tokens in a sequence in which a token can be a\nword, character, or a sub-word unit, with most models using sub-word-tokenization.\nBecause models can be trained on this objective in a self-supervised setting, large\nunlabeled corpora (such as Wikipedia, book corpora, and Common Crawl data) can\nbe used for training.\nIncreasing the amount of data, parameters, and computation further, several emer-\ngent abilities of LLMs have been discovered [23]. Sufficiently large models are capable\nof in-context learning [17] and chain-of-thought reasoning [24]. Previously requiring\nfine-tuning on downstream tasks (e.g., named entity recognition or question answer-\ning), LLMs can now yield decent performance on new tasks by merely including a task\ndescription and few examples in the input [17]. Whether these abilities emerge sud-\ndenly in a sharp transition at a certain scale or in a gradual and predictable way is still\nsubject of scientific debate [25]. With instruction fine-tuning, the generated responses\nto a prompt aligned more with the user’s intent, removing the need for careful prompt\nselection [26]. To solve tasks beyond the capabilities of LLMs in isolation, they have\nbeen trained to use tools for which the input and output can be represented as text,\n3\nsuch as a calculator or Python console [27]. Further work on the agent-like behavior of\nLLMs combines reasoning and acting capabilities [28] or add self-reflection capacities\n[29].\nThe NLP community saw great advances in a wide range of benchmarks using\nLLMs. In addition to models operating on textual input alone, recently, multi-modal\nmodels that also process other modalities such as images, videos, and/ or sensor-\nsignals have become a focus of research [2, 3]. Abstracting the LLM concept to other\nmodalities, training procedures, and so forth, the term foundation model was coined\n[30].\n2.2 Computational engineering design\nToday, the digitization of engineering design is well advanced. In the past, techni-\ncal drawing was performed on drawing boards until software for computer-aided\ndesign (CAD) was developed in the second half of the twentieth century, which\nis generally adopted today. Currently, we utilize finite element analysis, topology\noptimization, design-support tools for additive manufacturing, and more. With\nmodel-based systems engineering and digital twins, the product development process\nbecame centered around digital models. Virtual and augmented reality enables visu-\nalization and interaction with designs. Due to the breadth of the field, only a brief\noverview of advances in computational engineering design can be given here, focusing\non design generation, design strategy learning, and NLP (in particular LLMs) for\nengineering design.\nGenerative adversarial networks, feedforward neural networks, variational autoen-\ncoders, as well as reinforcement learning systems have been used in design-related\ngeneration tasks such as topology optimization or shape synthesis based on visual\nmodalities (e.g., images, voxels, point clouds, etc.) [31]. Other work focuses on learn-\ning design strategies. Given a state in solving a truss design problem, Raina et al. [32]\npredicts what actions humans perform next. Gyory et al. [33, 34] analyze real time\ndata of design teams to suggest measures from a predefined list if the communication\nor action frequency appears to be too low.\nLexical databases [35–37] and stopword lists [38] for technological vocabulary\nand jargon have been proposed, as have engineering-related ontologies [39–41]. With\nontologies come knowledge graphs. However, there has been a lack of specialized engi-\nneering knowledge graphs thus far [42]. Only recently, Siddharth et al. [43] build a\nknowledge graph using patent claims. With a trend towards industry 4.0 and digi-\ntal twins, the subject of the semantic representation of technological knowledge will\nprobably be increasingly addressed in the future. NLP methods, which have been less\napplied in the engineering sciences compared to the biomedical and material ones,\nhave become increasingly popular recently. In design research, NLP has been applied\nto requirements extraction, ontology construction, patent analysis, and more [44].\nUsing foundation models such as LLMs or pre-trained multi-modal models in the\nengineering design process is a recent and unexplored topic. Several studies have exper-\nimented with using LLMs to provide designers with inspirational stimuli for ideation.\nIn two explorative studies, Zhu and Luo [45, 46] prompted GPT-2 and -3 to generate\n4\ndesign concepts (text-to-text) based on the description of either a concept, problem,\nor analogy in both a fine-tuning and few-shot learning setting. Similarly, Zhu et al.\n[47] fine-tune GPT-3 for bio-inspired design concept generation. Ma et al. [48] com-\npare design solutions generated with GPT-3 with crowdsourced ones. Other work has\nfocus on design concept evaluation combining a pre-trained language model (BERT)\nand image models in a multi-modal one [49, 50]. Song et al. [51] provided an extensive\noverview of multi-modal machine learning for engineering design. They outline possi-\nble applications, but focus on lower level tasks such as text-to-shape or shape-to-text\nsynthesis.\nOrthogonal to the prior work, we concentrate on the design process itself as a\ncomplex, iterative, and dynamic reasoning process and situate recent advances in NLP\nand machine learning in a superordinate framework.\n3 Depicting the design process as a goal-oriented,\nargumentative discourse\nWe have digital artifacts for shapes, assembly processes, stress distributions, flow\npatterns, and more. Until now, however, computer-aided engineering, as practiced\nin industry, has not included the creative and argumentative process of the product\ndevelopment process itself. In the following, we argue that this process could be\ndigitized and partially automatized next, and outline how this can be achieved.\nMany steps in the product development process are performed using computation\nand are not based on human thought alone. However, humans are needed to integrate\nthese computational processes, be they calculations, simulations, or optimizations,\ninto a meaningful superordinate product development process. Human thought and\nworld knowledge is required to reduce the solution space in advance and come up with\noriginal ideas that have not been modeled to be computationally accessible before.\nFor example, when a bicycle is designed, the starting point is not a blank slate but\nan idea of how a bicycle looks and how it has worked well for over a century. If a\nstandardized aerodynamic tube shape across bicycle manufactures is proposed, it is\nunlikely that this idea originated from a numerical optimization. Instead, background\nknowledge and the ability to think and reason are used. Solving engineering problems\nrequires an argumentative discourse. As such, argumentation is inherent to the prod-\nuct development process. Experiments and calculations, etc., inform the discourse\nto provide necessary information. Nevertheless, argumentation is rarely given a lot\nof attention, perhaps because it is hidden, as it is typically not made explicit in a\nreadable or visual representation.\nHaving described that a goal-driven, argumentative discourse is at the core of the\ndesign process, we argue that it should be represented as a digital artifact. Humans\ncommunicate, argue, and reason using natural language. Hence, the argumentative\ndiscourse can be largely represented in textual form. Many parts of the design process,\nhowever, cannot be represented as text. Therefore, we distinguish the design discourse\nfrom external actions (such as performing an experiment or simulation) and other\n5\nengineering artifacts (such as a drawing, or 3D model). We formulate that external\nactions are invoked from within the design discourse and in turn inform the discourse,\neither directly or indirectly, by yielding other engineering artifacts that inform the\ndesign discourse (see Figure 1).\nRepresenting the argumentative discourse as a digital artifact would improve the\ndocumentation of the design process. Instead of only archiving the results of pro-\ncess steps (e.g., CAD files or the results of simulation runs), the reasoning process is\ndocumented and hence archivable. For a past development process to be efficiently\nused for the development of a new product generation, past decisions and alternatives\nmust be accessible. Having the reasoning process explicitly documented makes past\ndesign decisions traceable. Furthermore, making the reasoning process explicit could\nimprove collective reasoning and therefore collaborative design. Finally, it would allow\nfor machines to participate in the reasoning process, which the next section covers.\n4 Foundation models as interlocutors in the design\ndiscourse\nIt is apparent that LLMs and other foundation models will (and already do) reshape\nhow we code, write, and search for information. Beyond that, we believe that they\ncan be directly involved in the creative and argumentative design process, and so\ncomplement generative design and topology optimization tools that are already able\nto automatize large parts of design exploration and optimization. An example of\na design discourse involving humans and machines is depicted in Figure 2. In the\nprevious section, we argued that explicitly representing the argumentative discourse\nof the design process as a digital artifact results in many advantages. One of these is\nthe potential for machines to participate in the reasoning process. LLMs and related\nmulti-modal models have several characteristics that suggest that they can be suc-\ncessfully applied for this purpose.\nLLMs and related multi-modal models can input and output natural language.\nThis facilitates their integration into the argumentative discourse, which to a large\nextent is conducted in natural language anyway, be it in intra- or interpersonal com-\nmunication. World knowledge is important both to interact with humans, where a\ncertain common ground is required, and for reducing the solution space and coming\nup with creative solutions. Preliminary evidence suggests that LLMs possess rich\nrepresentations of the world despite being trained on simple objectives [52]. It is\nevident that humans are constrained by their knowledge when designing by analogy\nor biomimicry. In contrast, LLMs can accumulate wide-ranging knowledge during\ntraining. LLMs of sufficient size display strong results on various tasks involving\nreasoning and are able to perform step-by-step reasoning [24]. Multi-modal models\ncan operate on various forms of design representations, which is important, because\nthroughout the product development process, designers utilize different types of rep-\nresentations of their designs (i.a., text, tables, sketches, and 3D models) [51]. Finally,\nmany tasks in the engineering design process cannot be solved by means of pure\nthought but require specialized engineering software and databases (i.a., CAD and\n6\nPrompt: Attached is the design of the current iteration of our aerodynamic road bike\nas well as the requirements, simulation results, and wind tunnel results. Please\noptimize the seat tube and seat stays.\nThought: What to optimize for?\nThought: Low aerodynamic drag has the highest priority. Other objectives are low\nweight, high comfort, high stiffness, low maintenance requirements.\nAction: Reinsuring by asking design team: Is this correct?\nPrompt: Yes.\nThought: What are the constraints?\nAction: Check given requirements file.\nThought: The bike must comply with the rules of the governing body for sports cycling\nto be used in professional races.\nAction: Look up rule book.\n...\nThought: Get inspiration from current aerodynamic road bike designs.\nAction: Web search\nThought: Get inspiration from patents.\nAction: Patent search\n...\nThought: How to reduce the aerodynamic drag?\nAction: Look up equation for aerodynamic drag.\nThought: Lowering the air density, frontal area, or drag coefficient reduces\naerodynamic drag. However, the air density is an environmental property which can not\nbe changed as part of the bicycle design.\nThought: The lower the seat stays the lower the frontal area.\nThought: Define the lowest allowed contact point with the seat tube as a boundary\ncondition.\nAction: Run multi-physics topology optimization to optimize seat stays for\naerodynamics.\n...\nThought: How can the drag of the seat tube be reduced?\nThought: Frame and tire should flow as seamlessly as possible into each other.\nThought: What problems does this solution have?\nThought: High pressure zone between tire and seat tube.\nThought: Is there a solution to this problem?\nThought: One might drag the outer shape of the seat tube close to the tire but\nenvelope it from behind to leave more room between the tire and the seat tube.\nAction: Check for patent infringements\nPrompt: Can you visualize this idea?\n...\n= Human\n= Machine\nFig. 2 An example of a design discourse. Note that the system outputs should clarify the ideas\npresented here and are not real outputs.\nsimulation software, patent and material databases). Recent work shows that LLMs\ncan be trained to interact with APIs (including the decision of when to call which\nAPI with which arguments) in a self-supervised setting [27].\nWe conclude that these models are fundamentally applicable to the purpose of\nassisting in the engineering design discourse. Nevertheless, a single call to a model will\nnot be of great utility; instead, the models must be embedded within a framework\nto solve complex engineering tasks. In the next section, we outline aspects of such a\nframework and simultaneously highlight promising future research directions.\n7\n5 Recommendations for future research\nMany puzzle pieces for the outlined transformation of the engineering design process\nare at hand. However, several aspects, which have not been sufficiently researched, are\nlikely to be necessary for a successful implementation of the proposed concept, namely:\nFormalizing the engineering design discourse within a framework of\nstages and components.\nAlthough approaches to interpret how outputs are formed in deep neural network\nmodels (such as probing [53]) exist, in practice, LLMs largely constitute black boxes.\nTo increase the interpretability and accuracy of LLMs, approaches such as scratch-\npad [54] or chain-of-thought prompting [24] steer the models towards the generation\nof intermediate steps. Complementary to the aforementioned approaches of generat-\ning intermediate steps, models can be guided to imitate certain patterns of thinking\nor to follow a logical flow by embedding calls to LLMs into a framework with a pre-\ndefined causal structure. In such frameworks “querying a language model becomes a\ncomputational primitive.” [55] For example, answering questions in steps adhering to\nformal logic yields interpretable reasoning traces and reduces the “hallucination” of\nfacts [55]. Similarly, formalizing the engineering design discourse within a framework\nof stages and recurring components will help safeguard the models and increases the\ndetail at which reasoning processes can be documented and verified. Furthermore, it\ncontributes to the clarity of the discourse and makes LLMs more controllable, in that\nhumans have more points of intervention, allowing them to provide more fine-grained\nguidance. For defining such a framework, the extensive literature on design research\ncan assist (e.g., [56, 57]).\nCreating machine-actionable interfaces for engineering software.\nWe noted that using external tools via APIs with LLMs is already an active area of\nresearch. The tools invoked in the design discourse could include specialized engineer-\ning software such as topology optimization. However, for today’s LLMs, the tools are\nrequired to provide a textual interface; that is, both inputs and outputs are repre-\nsented as text [27]. Therefore, the interfaces of specialized engineering software should\nbe adapted according to the models’ requirements. Note that the requirements for an\ninterface are bound to change with multi-modal models.\nLearning representations better corresponding to skills required for\nengineering.\nGood spatial imagination, solid engineering knowledge, and high heuristic competence\nare skills that correlate with higher quality engineering design solutions [56]. For foun-\ndation models to fulfill a supportive role in the engineering design process, the learned\nrepresentations should in turn correlate to a high degree with these skills. Therefore,\nthe proportion of technical documents (such as patents or specialist books) in the pre-\ntraining corpus can be increased. However, although LLMs compress large amounts of\nknowledge in their weights, a representation corresponding to good spatial imagina-\ntion and causal understanding of physical processes is unlikely to be learned from text\n8\nalone. With a trend towards multi-modal models, incorporating images, videos, and\nother modalities, we see this problem as likely to be addressed in the near future. Addi-\ntionally, strengthening the reasoning capabilities of LLMs is a promising and growing\nresearch direction.\nSpecifying evaluation metrics and datasets.\nPrior work has expressed the need for design-specific metrics for evaluating deep gen-\nerative models in design synthesis [45, 48, 51, 58]. Similarly, design-specific metrics are\nlikely to be required to evaluate the performance of frameworks to assist in the argu-\nmentative design discourse. In cases where ground truth is not required, more complex\nassessments, for which there is not yet a calculable metric, but for which human eval-\nuators are required, such as the assessment of usefulness and feasibility [48], could\npossibly be approximated by foundation models in the future. Furthermore, as par-\nticipation in the argumentative discourse of engineering design is a new task, there is\na need for public datasets to evaluate and compare models against.\n6 Conclusion\nWe propose conceiving the design process as a goal-oriented, argumentative discourse\non which foundation models can operate. Making the reasoning steps explicit in a\nnew digital artifact of the product development process could lead to improved docu-\nmentation and increased collaboration, and makes certain forms of machine assistance\npossible in the first place. We describe how LLMs and multi-modal foundation mod-\nels can assist in the design discourse and outline interesting directions of research,\nincluding structuring of the design discourse in stages and components, the provision\nof machine-actionable interfaces for specialized engineering software, the development\nof foundation models with learned representations that better correspond to the skills\nrequired in the design process, and the creation and publication of common metrics\nand datasets as a community effort. We do not intend to describe a theoretical frame-\nwork that is far from ever being implemented in practice, but believe that the product\ndevelopment process is about to change, with the aspects described in this article\nbeing part of its future.\nAcknowledgments.\nThe authors would like to thank the German Federal Govern-\nment, the German state governments, and the Joint Science Conference (GWK) for\ntheir funding and support as part of the NFDI4Ing consortium. Funded by the German\nResearch Foundation (DFG) – project number: 442146713. Furthermore, this work was\nsupported by the Helmholtz Association under the program “Energy System Design”.\nReferences\n[1] Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F.,\nLevy, O., Bowman, S.: SuperGLUE: A Stickier Benchmark for General-Purpose\n9\nLanguage Understanding Systems. In: Advances in Neural Information Pro-\ncessing Systems, vol. 32, pp. 3261–3275. Curran Associates, Inc., Vancou-\nver, BC, Canada (2019). https://papers.nips.cc/paper files/paper/2019/hash/\n4496bf24afe7fab6f046bf4923da8de6-Abstract.html\n[2] Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., Gao, J.: Vision-Language Pre-Training:\nBasics, Recent Advances, and Future Trends. Found. Trends Comput. Graph. Vis.\n14(3–4), 163–352 (2022) https://doi.org/10.1561/0600000105 . Publisher: Now\nPublishers, Inc.\n[3] Driess, D., Xia, F., Sajjadi, M.S.M., Lynch, C., Chowdhery, A., Ichter, B., Wahid,\nA., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P.,\nDuckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff,\nK., Zeng, A., Mordatch, I., Florence, P.: PaLM-E: An Embodied Multimodal\nLanguage Model (2023) arXiv:2303.03378 [cs.LG]\n[4] Moor, M., Banerjee, O., Abad, Z.S.H., Krumholz, H.M., Leskovec, J., Topol, E.J.,\nRajpurkar, P.: Foundation models for generalist medical artificial intelligence.\nNature 616(7956), 259–265 (2023) https://doi.org/10.1038/s41586-023-05881-4 .\nAccessed 2023-06-15\n[5] M. Hocky, G., D. White, A.: Natural language processing models that automate\nprogramming will transform chemistry research and teaching. Digital Discov-\nery 1(2), 79–83 (2022) https://doi.org/10.1039/D1DD00009H . Publisher: Royal\nSociety of Chemistry. Accessed 2023-06-15\n[6] Manning, C.D.: Human Language Understanding & Reasoning. Daedalus 151(2),\n127–138 (2022) https://doi.org/10.1162/daed a 01905\n[7] Harris, Z.S.: Distributional Structure. WORD 10(2-3), 146–162 (1954) https://\ndoi.org/10.1080/00437956.1954.11659520\n[8] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient Estimation of Word\nRepresentations in Vector Space (2013) arXiv:1301.3781 [cs.CL]\n[9] Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed Represen-\ntations of Words and Phrases and their Compositionality (2013) arXiv:1310.4546\n[cs.CL]\n[10] Pennington, J., Socher, R., Manning, C.: GloVe: Global Vectors for Word Rep-\nresentation. In: Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 1532–1543. Association for Compu-\ntational Linguistics, Doha, Qatar (2014). https://doi.org/10.3115/v1/D14-1162\n[11] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-\nmoyer, L.: Deep contextualized word representations (2018) arXiv:1802.05365\n[cs.CL]\n10\n[12] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In: Proceedings of the\n2019 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 4171–4186. Association for Computational Linguistics, Minneapolis,\nMinnesota (2019). https://doi.org/10.18653/v1/N19-1423\n[13] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I.: Attention is All you Need. In: Advances in Neu-\nral Information Processing Systems, vol. 30, pp. 5998–6008. Curran Associates,\nInc., Long Beach, CA, USA (2017). https://papers.nips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[14] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,\nM., Zettlemoyer, L., Stoyanov, V.: RoBERTa: A Robustly Optimized BERT\nPretraining Approach (2019) arXiv:1907.11692 [cs.CL]\n[15] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving Language\nUnderstanding by Generative Pre-Training (2018)\n[16] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nModels are Unsupervised Multitask Learners (2019)\n[17] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,\nHesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are\nFew-Shot Learners. In: Advances in Neural Information Processing Systems, vol.\n33, pp. 1877–1901. Curran Associates, Inc., virtual (2020). https://proceedings.\nneurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n[18] Workshop, B., Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow, D.,\nCastagn´e, R., Luccioni, A.S., Yvon, F., Gall´e, M., Tow, J., Rush, A.M., Bider-\nman, S., Webson, A., Ammanamanchi, P.S., Wang, T., Sagot, B., Muennighoff,\nN., Moral, A.V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Belt-\nagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P.O., Sanh, V., Lauren¸con, H.,\nJernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa,\nA., Aji, A.F., Alfassy, A., Rogers, A., Nitzav, A.K., Xu, C., Mou, C., Emezue,\nC., Klamm, C., Leong, C., Strien, D., Adelani, D.I., Radev, D., Ponferrada, E.G.,\nLevkovizh, E., Kim, E., Natan, E.B., De Toni, F., Dupont, G., Kruszewski, G.,\nPistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., John-\nson, I., Gonzalez-Dios, I., Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J.,\nFrohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K.,\nVon Werra, L., Weber, L., Phan, L., allal, L.B., Tanguy, L., Dey, M., Mu˜noz,\nM.R., Masoud, M., Grandury, M., ˇSaˇsko, M., Huang, M., Coavoux, M., Singh,\n11\nM., Jiang, M.T.-J., Vu, M.C., Jauhar, M.A., Ghaleb, M., Subramani, N., Kassner,\nN., Khamis, N., Nguyen, O., Espejel, O., Gibert, O., Villegas, P., Henderson, P.,\nColombo, P., Amuok, P., Lhoest, Q., Harliman, R., Bommasani, R., L´opez, R.L.,\nother: BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\n(2023) arXiv:2211.05100 [cs.CL]\n[19] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,\nE., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (2023)\narXiv:2302.13971 [cs.CL]\n[20] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\nY., Li, W., Liu, P.J.: Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer. Journal of Machine Learning Research 21(140), 1–67\n(2020)\n[21] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang,\nX., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen,\nX., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang,\nS., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi,\nE.H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V., Wei, J.: Scaling\nInstruction-Finetuned Language Models (2022) arXiv:2210.11416 [cs.LG]\n[22] Tay, Y., Dehghani, M., Tran, V.Q., Garcia, X., Wei, J., Wang, X., Chung, H.W.,\nShakeri, S., Bahri, D., Schuster, T., Zheng, H.S., Zhou, D., Houlsby, N., Met-\nzler, D.: UL2: Unifying Language Learning Paradigms (2023) arXiv:2205.05131\n[cs.CL]\n[23] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,\nD., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O.,\nLiang, P., Dean, J., Fedus, W.: Emergent Abilities of Large Language Models\n(2022) arXiv:2206.07682 [cs.CL]\n[24] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le,\nQ., Zhou, D.: Chain-of-Thought Prompting Elicits Reasoning in Large Language\nModels (2023) arXiv:2201.11903 [cs.CL]\n[25] Schaeffer, R., Miranda, B., Koyejo, S.: Are Emergent Abilities of Large Language\nModels a Mirage? (2023) arXiv:2304.15004 [cs.CL]\n[26] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller,\nL., Simens, M., Askell, A., Welinder, P., Christiano, P.F., Leike, J., Lowe, R.:\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35, 27730–27744 (2022)\n[27] Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Zettlemoyer, L.,\n12\nCancedda, N., Scialom, T.: Toolformer: Language Models Can Teach Themselves\nto Use Tools (2023) arXiv:2302.04761 [cs.CL]\n[28] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.: ReAct:\nSynergizing Reasoning and Acting in Language Models (2023) arXiv:2210.03629\n[cs.CL]\n[29] Shinn, N., Labash, B., Gopinath, A.: Reflexion: an autonomous agent with\ndynamic memory and self-reflection (2023) arXiv:2303.11366 [cs.AI]\n[30] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S., Bern-\nstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,\nD., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J.Q., Demszky, D.,\nDonahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Etha-\nyarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N.,\nGrossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E.,\nHong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karam-\ncheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M., Krishna, R.,\nKuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent,\nI., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S., Mitchell,\nE., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A.,\nNiebles, J.C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I.,\nPark, J.S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren,\nH., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., R´e, C., Sadigh, D., Sagawa, S.,\nSanthanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A.W.,\nTram`er, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S.M., Yasunaga,\nM., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng,\nL., Zhou, K., Liang, P.: On the Opportunities and Risks of Foundation Models\n(2022) arXiv:2108.07258 [cs.LG]\n[31] Regenwetter, L., Nobari, A.H., Ahmed, F.: Deep Generative Models in Engi-\nneering Design: A Review. Journal of Mechanical Design 144(071704) (2022)\nhttps://doi.org/10.1115/1.4053859 . Accessed 2023-06-14\n[32] Raina, A., Cagan, J., McComb, C.: Design Strategy Network: A Deep Hierar-\nchical Framework to Represent Generative Design Strategies in Complex Action\nSpaces. Journal of Mechanical Design 144(021404) (2021) https://doi.org/10.\n1115/1.4052566 . Accessed 2023-06-14\n[33] Gyory, J.T., Soria Zurita, N.F., Martin, J., Balon, C., McComb, C., Kotovsky,\nK., Cagan, J.: Human Versus Artificial Intelligence: A Data-Driven Approach to\nReal-Time Process Management During Complex Engineering Design. Journal of\nMechanical Design 144(2) (2021) https://doi.org/10.1115/1.4052488\n[34] Gyory, J.T., Kotovsky, K., McComb, C., Cagan, J.: Comparing the Impacts on\nTeam Behaviors Between Artificial Intelligence and Human Process Management\nin Interdisciplinary Design Teams. Journal of Mechanical Design 144(10) (2022)\n13\nhttps://doi.org/10.1115/1.4054723\n[35] Sarica, S., Luo, J., Wood, K.L.: TechNet: Technology semantic network based\non patent data. Expert Systems with Applications 142, 112995 (2020) https:\n//doi.org/10.1016/j.eswa.2019.112995\n[36] Jang, H., Jeong, Y., Yoon, B.: TechWord: Development of a technology lexical\ndatabase for structuring textual technology information based on natural lan-\nguage processing. Expert Systems with Applications 164, 114042 (2021) https:\n//doi.org/10.1016/j.eswa.2020.114042\n[37] Shi, F., Chen, L., Han, J., Childs, P.: A Data-Driven Text Mining and Semantic\nNetwork Analysis for Design Information Retrieval. Journal of Mechanical Design\n139(11) (2017) https://doi.org/10.1115/1.4037649\n[38] Sarica, S., Luo, J.: Stopwords in technical language processing. PLOS ONE 16(8),\n0254937 (2021) https://doi.org/10.1371/journal.pone.0254937\n[39] Morbach, J., Wiesner, A., Marquardt, W.: OntoCAPE—A (re)usable ontology for\ncomputer-aided process engineering. Computers & Chemical Engineering 33(10),\n1546–1556 (2009) https://doi.org/10.1016/j.compchemeng.2009.01.019\n[40] Booshehri, M., Emele, L., Fl¨ugel, S., F¨orster, H., Frey, J., Frey, U., Glauer, M.,\nHastings, J., Hofmann, C., Hoyer-Klick, C., H¨ulk, L., Kleinau, A., Knosala, K.,\nKotzur, L., Kuckertz, P., Mossakowski, T., Muschner, C., Neuhaus, F., Pehl, M.,\nRobinius, M., Sehn, V., Stappel, M.: Introducing the Open Energy Ontology:\nEnhancing data interpretation and interfacing in energy systems analysis. Energy\nand AI 5, 100074 (2021) https://doi.org/10.1016/j.egyai.2021.100074\n[41] Sanfilippo, E.M., Kitamura, Y., Young, R.I.M.: Formal ontologies in man-\nufacturing. Applied Ontology 14(2), 119–125 (2019) https://doi.org/10.3233/\nAO-190209\n[42] Han, J., Sarica, S., Shi, F., Luo, J.: Semantic Networks for Engineering Design: A\nSurvey. Proceedings of the Design Society 1, 2621–2630 (2021) https://doi.org/\n10.1017/pds.2021.523 . Publisher: Cambridge University Press\n[43] Siddharth, L., Blessing, L.T.M., Wood, K.L., Luo, J.: Engineering Knowledge\nGraph From Patent Database. Journal of Computing and Information Science in\nEngineering 22(2) (2021) https://doi.org/10.1115/1.4052293\n[44] Siddharth, L., Blessing, L., Luo, J.: Natural language processing in-and-for design\nresearch. Design Science 8, 21 (2022) https://doi.org/10.1017/dsj.2022.16\n[45] Zhu, Q., Luo, J.: Generative Pre-Trained Transformer for Design Concept Gen-\neration: An Exploration. Proceedings of the Design Society 2, 1825–1834 (2022)\nhttps://doi.org/10.1017/pds.2022.185 . Publisher: Cambridge University Press.\n14\nAccessed 2023-06-14\n[46] Zhu, Q., Luo, J.: Generative Transformers for Design Concept Generation. Jour-\nnal of Computing and Information Science in Engineering 23(4) (2023) https:\n//doi.org/10.1115/1.4056220\n[47] Zhu, Q., Zhang, X., Luo, J.: Biologically Inspired Design Concept Genera-\ntion Using Generative Pre-Trained Transformers. Journal of Mechanical Design\n145(041409) (2023) https://doi.org/10.1115/1.4056598 . Accessed 2023-06-14\n[48] Ma, K., Grandi, D., McComb, C., Goucher-Lambert, K.: Conceptual Design\nGeneration Using Large Language Models (2023) arXiv:2306.01779 [cs.CL]\n[49] Yuan, C., Marion, T., Moghaddam, M.: Leveraging End-User Data for Enhanced\nDesign Concept Evaluation: A Multimodal Deep Regression Model. Journal of\nMechanical Design 144(2) (2021) https://doi.org/10.1115/1.4052366\n[50] Song, B., Miller, S., Ahmed, F.: Attention-Enhanced Multimodal Learning for\nConceptual Design Evaluations. Journal of Mechanical Design 145(4) (2023)\nhttps://doi.org/10.1115/1.4056669\n[51] Song, B., Zhou, R., Ahmed, F.: Multi-modal Machine Learning in Engineering\nDesign: A Review and Future Directions (2023) arXiv:2302.10909 [cs.LG]\n[52] Li, K., Hopkins, A.K., Bau, D., Vi´egas, F., Pfister, H., Wattenberg, M.: Emergent\nWorld Representations: Exploring a Sequence Model Trained on a Synthetic Task\n(2023) arXiv:2210.13382 [cs.LG]\n[53] Belinkov, Y.: Probing classifiers: Promises, shortcomings, and advances. Compu-\ntational Linguistics 48(1), 207–219 (2022) https://doi.org/10.1162/coli a 00422\n[54] Nye, M., Andreassen, A.J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D.,\nDohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., Odena, A.: Show\nYour Work: Scratchpads for Intermediate Computation with Language Models\n(2021) arXiv:2112.00114 [cs.LG]\n[55] Creswell, A., Shanahan, M.: Faithful Reasoning Using Large Language Models\n(2022) arXiv:2208.14271 [cs.AI]\n[56] Fricke, G.: Successful individual approaches in engineering design. Research in\nEngineering Design 8(3), 151–165 (1996) https://doi.org/10.1007/BF01608350\n[57] Pahl, G., Beitz, W., Feldhusen, J., Grote, K.-H.: Engineering Design. Springer,\nLondon (2007). https://doi.org/10.1007/978-1-84628-319-2\n[58] Regenwetter, L., Srivastava, A., Gutfreund, D., Ahmed, F.: Beyond Statistical\nSimilarity: Rethinking Metrics for Deep Generative Models in Engineering Design\n(2023) arXiv:2302.02913 [cs.LG]\n15\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.CE"
  ],
  "published": "2023-06-15",
  "updated": "2023-06-15"
}