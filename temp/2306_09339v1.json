{
  "id": "http://arxiv.org/abs/2306.09339v1",
  "title": "From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management",
  "authors": [
    "Immanuel Trummer"
  ],
  "abstract": "Large language models have recently advanced the state of the art on many\nnatural language processing benchmarks. The newest generation of models can be\napplied to a variety of tasks with little to no specialized training. This\ntechnology creates various opportunities for applications in the context of\ndata management.\n  The tutorial will introduce participants to basic background on language\nmodels, discuss different methods to use language models, and give an overview\nand short demonstration of available libraries and APIs. Models for generating\nnatural language will be considered as well as models, such as GPT-3 Codex,\nwhich complete program code or generate code from natural language\ninstructions. Finally, the tutorial will discuss recent research in the\ndatabase community that exploits language models in the context of traditional\ndatabase systems or proposes novel system architectures that are based on them.\n  The tutorial is targeted at database researchers. No prior background on\nlanguage models is required. The goal of the tutorial is to introduce database\nresearchers to the latest generation of language models, and to their use cases\nin the domain of data management.",
  "text": "From BERT to GPT-3 Codex: Harnessing the Potential of Very\nLarge Language Models for Data Management\nImmanuel Trummer\nCornell University\nIthaca, NY\nitrummer@cornell.edu\nABSTRACT\nLarge language models have recently advanced the state of the\nart on many natural language processing benchmarks. The newest\ngeneration of models can be applied to a variety of tasks with\nlittle to no specialized training. This technology creates various\nopportunities for applications in the context of data management.\nThe tutorial will introduce participants to basic background on\nlanguage models, discuss different methods to use language models,\nand give an overview and short demonstration of available libraries\nand APIs. Models for generating natural language will be considered\nas well as models, such as GPT-3 Codex, which complete program\ncode or generate code from natural language instructions. Finally,\nthe tutorial will discuss recent research in the database community\nthat exploits language models in the context of traditional database\nsystems or proposes novel system architectures that are based on\nthem.\nThe tutorial is targeted at database researchers. No prior back-\nground on language models is required. The goal of the tutorial is to\nintroduce database researchers to the latest generation of language\nmodels, and to their use cases in the domain of data management.\nPVLDB Reference Format:\nImmanuel Trummer. From BERT to GPT-3 Codex: Harnessing the Potential\nof Very Large Language Models for Data Management. PVLDB, 15(12):\n3770 - 3773, 2022.\ndoi:10.14778/3554821.3554896\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://itrummer.github.io/lm4db/.\nMy name is GPT-3, I am a language model trained by OpenAI. I\ncan write stories, articles, poems, and even code. I am the most\npowerful language model in the world. I am the future of AI. .\nCompletion of Prompt “My name is GPT-3, I” by GPT-3 Codex\n1\nINTRODUCTION\nThe area of natural language processing (NLP) has recently been\nrevolutionized by the advent of large “language models”, trained\non huge quantities of unlabeled text [97]. Given sufficiently large\namounts of training data and parameters, such models can tackle a\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 15, No. 12 ISSN 2150-8097.\ndoi:10.14778/3554821.3554896\nbroad range of tasks with little to no specialized training [5]. The\nrange of applications for such models in the domain of databases is\nvast. It ranges from novel interfaces [11, 12, 32, 59, 69, 83, 88] to new\nsystem architectures [77, 84], based on data representations and\nprocessing mechanisms that are enabled by the latest generation of\nlanguage models. The goal of this tutorial is to introduce database\nresearchers to the possibilities offered by these models, to provide\npointers to libraries and APIs that make them accessible [60, 97],\nand to review recent research in the database community exploiting\nthem. The tutorial will cover language models that process and gen-\nerate natural language text [15, 18], as well as more recent models\nthat generate program code from natural language descriptions [9].\nIt will include examples and live demonstrations, providing atten-\ndees with an intuition for the scope of solvable problems.\nThe tutorial is aimed at database researchers. No prior back-\nground in language models or NLP is expected. The tutorial will\nstart with a short, high-level introduction to the Transformer [89], a\nnovel neural network architecture that has has enabled many of the\nrecent advances in NLP. Next, it will discuss Transformer-based lan-\nguage models and describe how they are pre-trained without super-\nvision on text or code. For model sizes in the hundreds of millions of\nparameters [15, 45, 52, 63], pre-training is typically followed by an-\nother (short) training phase on task-specific samples (“fine-tuning”).\nLanguage model sizes have continuously increased over the past\nyears, as illustrated in Figure 1 (note the logarithmic scale on the y-\naxis). The latest generation of language models with sizes in the hun-\ndreds of billions of parameters [9, 13, 17, 18, 27, 50, 64, 65, 73, 76, 103]\ncan often be used without further specialization (“prompting”). The\ntutorial will discuss and demonstrate both methods. Furthermore,\nit will provide pointers to libraries and APIs that allow using corre-\nsponding models. While an in-depth discussion of these APIs and\nlibraries is beyond the scope of this tutorial, attendees will receive\nan overview and pointers on how to choose the right framework\nfor their respective use case.\nFinally, the tutorial will discuss recent research in the database\ncommunity that exploits language models. The discussion will cover\nresearch on facilitating the use of traditional database systems\nvia such models (e.g., by advanced user interfaces [71, 75]). Also,\nit will include research that exploits language models to revise\nfundamental design decisions in database systems [26, 77, 84]. The\ntotal duration of the tutorial is 1.5 hours, including questions and\ndiscussions.\nThe reminder of this proposal is organized as follows. Section 2\ndescribes the topics covered in the tutorial in more detail. Section 3\ndescribes the organization and timeline of the tutorial. Section 4\nsummarizes the goals of the tutorial and describes the intended\naudience. Section 5 contrasts the tutorial content from other, recent\narXiv:2306.09339v1  [cs.DB]  15 Jun 2023\n2,019\n2,020\n2,021\n2,022\n100\n101\n102\n103\nMegatron-LM\nWu Dao 2\nBERT\nGPT-2\nTuring-NLG\nT5\nGPT-3\nMegatron-Turing NLG\nPaLM\nSwitch Transformer\nYear\n# Trainable Parameters (Billions)\nFigure 1: Evolution of parameter counts in language models.\ntutorials in the database community. Finally, Section 6 contains\nbiographical details on the presenter.\n2\nTOPICS COVERED\nThe tutorial will cover the following topics.\n2.1\nRise of the Transformer\nAt the heart of the NLP revolution is a novel neural network archi-\ntecture, the so-called Transformer [89]. The Transformer is nowa-\ndays the dominant architecture for various NLP tasks [97]. Be-\nyond NLP, it is increasingly being adopted in other domains such\nas computer vision [1, 16, 22, 24, 51, 53, 56, 101, 102, 106], audio\nanalysis [4, 8, 20, 21, 42, 55, 57, 66, 90, 91], and multi-modal data\nanalysis [6, 7, 14, 19, 29, 49, 62, 70, 72, 92, 104].\nThe tutorial will introduce the main ideas behind the Trans-\nformer model. In particular, it will discuss the concept of attention\nmechanisms [89]. The goal of this part is to give the audience an\nintuition for why Transformer models were able to advance the\nstate of the art in NLP, compared to prior methods such as recurrent\nneural networks [43]. Explanations will be kept at a relatively high\nlevel of abstraction. Hence, basic knowledge in machine learning\nwill be sufficient to follow this part.\n2.2\nPre-Trained Language Models\nCompared to prior architectures, the Transformer makes paralleliz-\ning the training process easier. In part, this has enabled the creation\nof very large language models. Such models are based on Trans-\nformer networks with hundreds of millions to hundreds of billions\nof trainable parameters.\nLanguage models are trained on tasks for which large amounts\nof training data are readily available. For instance, models such as\nBERT [15] learn to fill in obfuscated words in Web text (masked\nlanguage modeling). Models such as GPT-3 learn to complete text\nor code based on a prefix [18]. In all those cases, manual labeling\nof training data is not required. The tutorial will cover some of the\nmost important language models developed over the past years. In\nparticular, it will introduce BERT (one of the first language models\nproposed) and GPT-3. For the latter model, the tutorial will cover\nthe base version [18] (optimized for completing natural language\ntext) as well as the Codex variant [9] (optimized for generating\ncode from natural language instructions).\n2.3\nFine-Tuning and Prompting\nLanguage models provide the fundament for approaches that solve\nvarious tasks, related to natural language and code. Tradition-\nally, language models undergo a process called fine-tuning after\ntask-agnostic training. Fine-tuning specializes language models for\ndomain-specific tasks, using a small amount of task-specific training\ndata. Compared to training a new network from scratch, fine-tuning\nreduces the amount of training data and computational overheads\nvery significantly [28]. This is possible due to transfer learning [67],\nas generic knowledge about language can be transferred across\ndifferent tasks.\nFine-tuning has been the primary method of using language\nmodels until quite recently. As language models grew further in\nsize, it became apparent that providing task-specific instructions as\ninput, together with few or even no examples [5], is often sufficient\nto solve formerly unseen tasks. This insight has spurred significant\nresearch efforts, targeted at prompting. This term refers to the use\nof language models for new tasks by including instructions and\nexamples into the prompt, i.e. the input to be completed by the\nlanguage model. The tutorial will discuss fine-tuning briefly and\nfocus on prompting. It will provide an intuition for the potential\nof prompting using examples from the domains of text and code\ncompletion.\n2.4\nAPIs and Libraries\nLanguage models are nowadays available via various channels. This\nincludes libraries that facilitate using language models locally (e.g.,\nthe Huggingface Transformers library [97]). It also includes APIs\nthat enable remote use of language models that are not publicly\navailable (e.g., OpenAI’s GPT-3 model [18]).\nThe tutorial will introduce some of the most popular frameworks\nfor accessing language models. Specifically, the tutorial will give\nan overview of the Huggingface Transformers library. This library\nfacilitates tasks such as training and inference. Also, the tutorial\nwill include a demonstration based on OpenAI’s API. This API\nenables access to the GPT-3 series of language models, including\nTable 1: Tutorial organization overview.\nPart\nDuration\nWelcome and introduction\n5 min\nRise of the Transformer\n10 min\nPre-trained language models\n10 min\nFine-tuning and prompting\n10 min\nAPIs and libraries\n20 min\nApplications in data management\n25 min\nFinal discussion and conclusion\n10 min\nthe GPT-3 Codex model that generates code from natural language\ninstructions. The goal of the tutorial is not to cover any of those\nAPIs in depth. Instead, it aims at giving an intuition for the potential\nuse cases of each framework, as well as references for studying\nthem in more detail.\n2.5\nApplications in Data Management\nFinally, the tutorial will discuss novel applications of language\nmodels in the database area. This tutorial section will be split into\ntwo parts.\nFirst, the tutorial will introduce novel applications that facilitate\nthe use of traditional database management systems. Perhaps the\nmost classical use case for NLP in the context of database systems\nis text-to-SQL translation [23, 46, 68, 69, 94–96, 98–100, 105]. While\nlarger language models have significantly increased the accuracy on\nthat task, they also enable entirely new applications. Here, the tuto-\nrial will cover recent research leveraging language models for tasks\nsuch as data preparation and integration [2, 74, 75], fact checking\nfrom data [10, 25, 33–40, 81, 82], or database tuning [78–80, 85–87].\nSecond, the tutorial will discuss novel architectures for data\nprocessing systems that are enabled by the advent of large language\nmodels. The discussion will cover very recent research as well\nas potential research opportunities. Specifically, the tutorial will\ncover novel ways of representing data using language models (e.g.,\nby storing data as natural language facts [77] or by integrating\ndata within the language model [26]). Also, it will discuss the use\nof language models in the execution engine (e.g., to implement\noperators [74, 77] or to synthesize code for data processing [84]).\n3\nTUTORIAL ORGANIZATION\nTable 1 gives an overview of the tutorial parts, as well as their\nestimated duration. The tutorial organization is based on the topics\nintroduced in Section 2. The tutorial will use slides as well as several\ndemonstrations, illustrating the use of language models via different\nmethods. Questions and comments are welcome throughout the\ntutorial. The last ten minutes of the tutorial are specifically reserved\nfor questions and discussions, followed by concluding remarks.\n4\nGOALS AND AUDIENCE\nThe goal of this tutorial is to introduce the database community\nto the latest generation of language models. The primary focus\nis on enabling database researchers to apply language models to\nnew research problems in the context of data management. To that\npurpose, the tutorial will convey basic background knowledge on\nlanguage models, give an intuition for the scope of tasks to which\nlanguage models can be applied, as well as provide pointers to\nuseful APIs and libraries. Furthermore, the tutorial will discuss at\nlength existing and emerging applications of language models in\nthe database area.\nIn line with the goals of the tutorial, no prior background knowl-\nedge on language models is expected from the audience. Primarily,\nthe audience is expected to be familiar with database systems and\nrelational data processing methods. Some high-level background\non deep learning (at the level of an undergraduate course) is use-\nful for the first part of the tutorial (introducing the Transformer\narchitecture), even though not strictly required. The primary target\naudience for this tutorial are database researchers who are intrigued\nby the possibilities offered by language models, but have not yet\ndone research in this area.\n5\nRELATIONSHIP TO PRIOR TUTORIALS\nThe proposed tutorial connects but is complementary to prior tu-\ntorials in the database community. Several recent tutorials have\nfocused on specific problems in the database area that are solved\nvia NLP. Most notably, several recent tutorials [3, 41] discussed ap-\nproaches for text-to-SQL translation in detail. Other recent tutorials\ncovered approaches for automated fact checking [44], information\nextraction [58], or entity embedding [61]. The proposed tutorial is\ncomplementary to those prior events in (at least) two ways. First, it\ncovers very recent trends in the area of language models, including\nprompting and few-shot learning as well as code synthesis by lan-\nguage models. The underlying technologies, e.g. the GPT-3 Codex\nmodel, have appeared only recently and were not covered in prior\ntutorials. Second, the tutorial scope is defined less by a specific\nproblem than by a specific method (use of language models). It\naims at covering a wide range of possible applications, inspiring\nparticipants to apply language models to novel problems in their\narea of research.\nMore broadly, the proposed tutorial relates to prior events, con-\nnecting databases and machine learning topics [30, 31, 47, 48, 54, 93].\nThe suggested tutorial is however complementary, as it focuses on\none specific method from the area of machine learning.\n6\nPRESENTER\nImmanuel Trummer is assistant professor for computer science\nat Cornell University. He heads the Cornell database group and\npublishes at venues such as SIGMOD, VLDB, and AAAI. His re-\nsearch aims at making data management and data analysis more\nefficient and more user-friendly. Towards that goal, he often ap-\nplies language models and other methods from the area of artificial\nintelligence and machine learning. Most recently, he has applied lan-\nguage models to natural language query interfaces, data-driven fact\nchecking, database tuning, and code synthesis for data processing.\nHis papers were selected for “Best of VLDB”, “Best of SIGMOD”, for\nthe ACM SIGMOD Research Highlight Award, and for publication\nin CACM as CACM Research Highlight. His research is sponsored\nby NSF and by several Google Faculty Research Awards.\nREFERENCES\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić,\nand Cordelia Schmid. 2021. ViViT: A Video Vision Transformer. Proceedings\nof the IEEE International Conference on Computer Vision (2021), 6816–6826.\nhttps://doi.org/10.1109/ICCV48922.2021.00676 arXiv:2103.15691\n[2] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Ho-\njel, Immanuel Trummer, and Christopher Ré. 2023. Language Models En-\nable Simple Systems for Generating Structured Views of Heterogeneous Data\nLakes. CoRR abs/2304.0 (2023), 1–30. https://doi.org/10.48550/arXiv.2304.09433\narXiv:2304.09433\n[3] Fatma Åzcan, Abdul Quamar, Jaydeep Sen, Chuan Lei, and Vasilis Efthymiou.\n2020. State of the Art and Open Challenges in Natural Language Interfaces to\nData. Proceedings of the ACM SIGMOD International Conference on Management\nof Data (2020), 2629–2636. https://doi.org/10.1145/3318464.3383128\n[4] Alan Baade, Puyuan Peng, and David Harwath. 2022. MAE-AST: Masked Au-\ntoencoding Audio Spectrogram Transformer. In Proceedings of the Annual Con-\nference of the International Speech Communication Association, INTERSPEECH.\n2438–2442. https://doi.org/10.21437/Interspeech.2022-10961 arXiv:2203.16691\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Win-\nter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Processing Systems. 1877–1901.\narXiv:2005.14165\n[6] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, and Yuexian Zou. 2021.\nOn Pursuit of Designing Multi-modal Transformer for Video Grounding. In\nEMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Process-\ning, Proceedings. 9810–9823. https://doi.org/10.18653/v1/2021.emnlp-main.773\narXiv:2109.06085\n[7] Jiawei Chen and Chiu Man Ho. 2022. MM-ViT: Multi-Modal Video Transformer\nfor Compressed Video Action Recognition. In Proceedings - 2022 IEEE/CVF\nWinter Conference on Applications of Computer Vision, WACV 2022. 786–797.\nhttps://doi.org/10.1109/WACV51458.2022.00086 arXiv:2108.09322\n[8] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and\nShlomo Dubnov. 2022. Hts-At: a Hierarchical Token-Semantic Audio Trans-\nformer for Sound Classification and Detection. ICASSP, IEEE International Con-\nference on Acoustics, Speech and Signal Processing - Proceedings 2022-May (2022),\n646–650. https://doi.org/10.1109/ICASSP43922.2022.9746312 arXiv:2202.00874\n[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de\nOliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens\nWinter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,\nAlex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N Carr, Jan\nLeike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew\nKnight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,\nDario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\nEvaluating Large Language Models Trained on Code. CoRR abs/2107.0 (2021),\n1–35. arXiv:2107.03374 https://arxiv.org/abs/2107.03374\n[10] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang,\nShiyang Li, Xiyou Zhou, and William Yang Wang. 2019. TabFact: A Large-\nscale Dataset for Table-based Fact Verification. CoRR abs/1909.0 (2019), 1–17.\narXiv:1909.02164 http://arxiv.org/abs/1909.02164\n[11] Yiru Chen and Eugene Wu. 2022.\nPI2: End-to-end Interactive Visualiza-\ntion Interface Generation from Queries. Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data (2022), 1711–1725.\nhttps:\n//doi.org/10.1145/3514221.3526166 arXiv:2107.08203\n[12] Zui Chen, Ju Fan, Sam Madden, and Nan Tang. 2023. Symphony : Towards\nNatural Language Query Answering over Multi-modal Data Lakes. In CIDR.\n1–7.\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua\nMaynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury,\nJacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepa-\nssi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPaLM: Scaling Language Modeling with Pathways. CoRR abs/2204.0 (2022),\n1–87. arXiv:2204.02311 http://arxiv.org/abs/2204.02311\n[14] Yin Dai, Yifan Gao, and Fayu Liu. 2021. Transmed: Transformers advance\nmulti-modal medical image classification. Diagnostics 11, 8 (2021), 1–15. https:\n//doi.org/10.3390/diagnostics11081384 arXiv:2103.05940\n[15] Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nNAACL. 4171–4186. arXiv:1810.04805\n[16] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu\nYuan, Dong Chen, and Baining Guo. 2022. CSWin Transformer: A General\nVision Transformer Backbone with Cross-Shaped Windows. Proceedings of the\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition\n2022-June (2022), 12114–12124. https://doi.org/10.1109/CVPR52688.2022.01181\narXiv:2107.00652\n[17] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers:\nScaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal\nof Machine Learning Research 23, 1 (2022), 1–39. arXiv:2101.03961 http://arxiv.\norg/abs/2101.03961\n[18] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its Nature, Scope, Limits,\nand Consequences. Minds and Machines 30, 4 (2020), 681–694. https://doi.org/\n10.1007/s11023-020-09548-1\n[19] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-\nmodal Transformer for Video Retrieval. Lecture Notes in Computer Science\n(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in\nBioinformatics) 12349 LNCS (2020), 214–229.\nhttps://doi.org/10.1007/978-3-\n030-58548-8_13 arXiv:2007.10639\n[20] Yuan Gong, Yu An Chung, and James Glass. 2021. Ast: Audio spectrogram\ntransformer. Proceedings of the Annual Conference of the International Speech\nCommunication Association, INTERSPEECH 1 (2021), 56–60. https://doi.org/10.\n21437/Interspeech.2021-698 arXiv:2104.01778\n[21] Yuan Gong, Cheng I.Jeff Lai, Yu An Chung, and James Glass. 2022. SSAST:\nSelf-Supervised Audio Spectrogram Transformer. In Proceedings of the 36th\nAAAI Conference on Artificial Intelligence, AAAI 2022. 10699–10709.\nhttps:\n//doi.org/10.1609/aaai.v36i10.21315 arXiv:2110.09784\n[22] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,\nHervé Jégou, and Matthijs Douze. 2021. LeViT: a Vision Transformer in Con-\nvNet’s Clothing for Faster Inference. Proceedings of the IEEE International\nConference on Computer Vision (2021), 12239–12249. https://doi.org/10.1109/\nICCV48922.2021.01204 arXiv:2104.01136\n[23] Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and\nDongmei Zhang. 2019. Towards Complex Text-to-SQL in Cross-Domain Data-\nbase with Intermediate Representation. In ACL. 4524–4535. https://doi.org/10.\n18653/v1/p19-1444 arXiv:1905.08205\n[24] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua\nLiu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang,\nand Dacheng Tao. 2022. A Survey on Vision Transformer. IEEE Transactions on\nPattern Analysis and Machine Intelligence (2022), 1–23. https://doi.org/10.1109/\nTPAMI.2022.3152247 arXiv:2012.12556\n[25] Naeemul Hassan, Gensheng Zhang, Fatma Arslan, Josue Caraballo, Damian\nJimenez, Siddhant Gawsane, Shohedul Hasan, Minumol Joseph, Aaditya Kulka-\nrni, Anil Kumar Nayak, Vikas Sable, Chengkai Li, and Mark Tremayne. 2017.\nClaimBuster: the first-ever end-to-end fact-checking system. VLDB 10, 7 (2017),\n1–4.\n[26] Benjamin Heinzerling and Kentaro Inui. 2021. Language models as knowledge\nbases: On entity representations, storage capacity, and paraphrased queries. In\nEACL 2021. 1772–1791. arXiv:2008.09036\n[27] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\nhannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Train-\ning Compute-Optimal Large Language Models. CoRR abs/2203.1 (2022), 1–36.\narXiv:2203.15556 http://arxiv.org/abs/2203.15556\n[28] Neil Houlsby, Andrei Giurgiu, Stanisraw Jastrzçbski, Bruna Morrone, Quentin\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\n2019.\nParameter-efficient transfer learning for NLP. In ICML. 4944–4953.\narXiv:1902.00751\n[29] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020.\nPixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transform-\ners. CoRR abs/2004.0 (2020), 1–17. arXiv:2004.00849 http://arxiv.org/abs/2004.\n00849\n[30] Stratos Idreos and Tim Kraska. 2019. From auto-tuning one size fits all to\nself-designed and learned data-intensive systems. Proceedings of the ACM\nSIGMOD International Conference on Management of Data (2019), 2054–2059.\nhttps://doi.org/10.1145/3299869.3314034\n[31] Alekh Jindal and Matteo Interlandi. 2021. Machine learning for cloud data\nsystems: The progress so far and the path forward. Proceedings of the VLDB\nEndowment 14, 12 (2021), 3202–3205. https://doi.org/10.14778/3476311.3476408\n[32] Saehan Jo and Immanuel Trummer. 2023. Demonstration of ThalamusDB:\nAnswering Complex SQL Queries with Natural Language Predicates on Multi-\nModal Data. In SIGMOD. https://doi.org/10.1145/3555041.3589730\n[33] Saehan\nJo,\nImmanuel\nTrummer,\nWeicheng\nYu,\nDaniel\nLiu,\nXuezhi\nWang, Cong Yu, and Mehta Niyati. 2018.\nVerifying text summaries\nof relational data sets.\nhttps://arxiv.org/abs/1804.07686. , 16 pages.\narXiv:/arxiv.org/abs/1804.07686 [https:] https://arxiv.org/abs/1804.07686\n[34] Saehan Jo, Immanuel Trummer, Weicheng Yu, Xuezhi Wang, Cong Yu, Daniel\nLiu, and Niyati Mehta. 2019. Verifying text summaries of relational data sets.\nIn SIGMOD. 299–316.\n[35] Saehan Jo, Immanuel Trummer, Weicheng Yu, Xuezhi Wang, Cong Yu, Daniel\nLiy, and Niyati Mehta. 2019. AggChecker: a fact-checking system for text\nsummaries of relational data sets. VLDB 12, 12 (2019), 1938–1941.\n[36] Georgios Karagiannis, Mohammed Saeed, Paolo Papotti, and Immanuel Trum-\nmer. 2020. Scrutinizer: A mixed-initiative approach to large-scale, data-driven\nclaim verification. PVLDB 13, 12 (2020), 2508–2521.\n[37] Georgios Karagiannis, Mohammed Saeed, Paolo Papotti, and Immanuel Trum-\nmer. 2020. Scrutinizer: a mixed-initiative approach to large-scale, data-driven\nclaim verification [Extended Technical Report]. Technical Report. 1–14 pages.\n[38] Georgios Karagiannis, Mohammed Saeed, Paolo Papotti, and Immanuel Trum-\nmer. 2020. Scrutinizer: a system for fact-checking statistical claims.\n[39] Georgios Karagiannis, Mohammed Saeed, Paolo Papotti, and Immanuel Trum-\nmer. 2020. Scrutinizer: Fact Checking Statistical Claims. PVLDB 13, 12 (2020),\n2965–2968. https://doi.org/10.14778/3415478.3415520\n[40] Georgios Karagiannis, Immanuel Trummer, Saehan Jo, Shubham Khandelwal,\nXuezhi Wang, and Cong Yu. 2020. Mining an “anti-knowledge base” from\nWikipedia updates with applications to fact checking and beyond. PVLDB 13, 4\n(2020), 561–573.\n[41] George Katsogiannis-Meimarakis and Georgia Koutrika. 2021. A Deep Dive into\nDeep Learning Approaches for Text-to-SQL Systems. In SIGMOD. 2846–2851.\nhttps://doi.org/10.1145/3448016.3457543\n[42] Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh, and Gerhard Widmer.\n2022. Efficient Training of Audio Transformers with Patchout. In Proceedings\nof the Annual Conference of the International Speech Communication Associa-\ntion, INTERSPEECH. 2753–2757. https://doi.org/10.21437/Interspeech.2022-227\narXiv:2110.05069\n[43] Surafel M. Lakew, Mauro Cettolo, and Marcello Federico. 2018. A comparison\nof transformer and recurrent neural networks on multilingual neural machine\ntranslation. COLING 2018 - 27th International Conference on Computational\nLinguistics, Proceedings (2018), 641–652. arXiv:1806.06957\n[44] Laks V.S. Lakshmanan, Michael Simpson, and Saravanan Thirumuruganathan.\n2018. Combating fake news: A data management and mining perspective.\nPVLDB 12, 12 (2018), 1990–1993. https://doi.org/10.14778/3352063.3352117\n[45] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In ACL. 7871–7880. https://doi.org/10.18653/\nv1/2020.acl-main.703 arXiv:1910.13461\n[46] Fei Li and HV Jagadish. 2014. NaLIR: an interactive natural language interface\nfor querying relational databases. In SIGMOD. 709–712.\n[47] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI Meets Database: AI4DB and\nDB4AI. Proceedings of the ACM SIGMOD International Conference on Manage-\nment of Data (2021), 2859–2866. https://doi.org/10.1145/3448016.3457542\n[48] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. Machine learning for databases.\nProceedings of the VLDB Endowment 14, 12 (2021), 3190–3193. https://doi.org/\n10.14778/3476311.3476405\n[49] Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, and Alexey Gritsenko. 2021.\nVUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Mod-\neling. CoRR abs/2112.0 (2021), 1–19. arXiv:2112.05692 http://arxiv.org/abs/2112.\n05692\n[50] Opher\nLieber,\nOr\nSharir,\nBarak\nLenz,\nand\nYoav\nShoham.\n2021.\nJurassic-1: Technical details and evaluation.\nTechnical Report. 1–9\npages.\nhttps://uploads-ssl.webflow.com/60fd4503684b466578c0d307/\n61138924626a6981ee09caf6_jurassic_tech_paper.pdf\n[51] Xuefeng Liu, Longhui Wei, Qi Tian, Zhengsu Chen, Lingxi Xie, and Jianwei\nNiu. 2021. Visformer : The Vision-friendly Transformer. In ICCV. 589–598.\n[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining approach. CoRR abs/1907.1,\n1 (2019), 1–13. arXiv:1907.11692 https://arxiv.org/abs/1907.11692\n[53] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen\nLin, and Baining Guo. 2021. Swin Transformer. 2021 IEEE/CVF International\nConference on Computer Vision (ICCV) (2021), 9992–10002. https://ieeexplore.\nieee.org/document/9710580/\n[54] Jiaheng Lu, Yuxing Chen, Herodotos Herodotou, and Shivnath Babu. 2018.\nSpeedup your analytics: Automatic parameter tuning for databases and big\ndata systems. Proceedings of the VLDB Endowment 12, 12 (2018), 1970–1973.\nhttps://doi.org/10.14778/3352063.3352112\n[55] Wei-Tsung Lu, Ju-Chiang Wang, Minz Won, Keunwoo Choi, and Xuchen Song.\n2021.\nSpecTNT: a Time-Frequency Transformer for Music Audio.\nCoRR\nabs/2110.0 (2021), 1–8. arXiv:2110.09127 http://arxiv.org/abs/2110.09127\n[56] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye,\nYuan He, and Hui Xue. 2022. Towards Robust Vision Transformer. Proceedings of\nthe IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n2022-June (2022), 12032–12041. https://doi.org/10.1109/CVPR52688.2022.01173\narXiv:2105.07926\n[57] Xinhao Mei, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu\nWang. 2021. Audio Captioning Transformer. CoRR abs/2107.0 (2021), 1–5.\narXiv:2107.09817 http://arxiv.org/abs/2107.09817\n[58] Yu Meng, Jiaxin Huang, Jingbo Shang, and Jiawei Han. 2018. TextCube: Auto-\nmated construction and multidimensional exploration. Proceedings of the VLDB\nEndowment 12, 12 (2018), 1974–1977. https://doi.org/10.14778/3352063.3352113\n[59] Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Ré. 2022. Can\nFoundation Models Wrangle Your Data?\nPVLDB 16, 4 (2022), 738–746.\narXiv:2205.09911 http://arxiv.org/abs/2205.09911\n[60] OpenAI. 2021. https://openai.com/blog/openai-codex/.\n[61] Laurel Orr, Atindriyo Sanyal, Xiao Ling, Karan Goel, and Megan Leszczynski.\n2021. Managing ml pipelines: Feature stores and the coming wave of embedding\necosystems. Proceedings of the VLDB Endowment 14, 12 (2021), 3178–3181.\nhttps://doi.org/10.14778/3476311.3476402 arXiv:2108.05053\n[62] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. 2021. Multi-Modal Fusion\nTransformer for End-to-End Autonomous Driving. In Proceedings of the IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition. 7073–\n7083. https://doi.org/10.1109/CVPR46437.2021.00700 arXiv:2104.09224\n[63] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2020. Language Models are Unsupervised Multitask Learners. OpenAI\nBlog 1, 8 (2020), 1–9. http://static.cs.brown.edu/courses/cs146/assets/papers/\nlanguage_models_are_unsupervised_multitask_learners.pdf\n[64] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,\nFrancis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,\nEliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,\nGeorge van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang,\nAmelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme\nSutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,\nDomenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,\nMaria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas\nPajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson\nD’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan\nClark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew\nJohnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed\nLockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem\nAyoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving. 2022. Scaling Language Models: Methods, Analysis and\nInsights from Training Gopher. CoRR abs/2112.1 (2022), 1–120. arXiv:2112.11446\nhttp://arxiv.org/abs/2112.11446\n[65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. Journal of Machine\nLearning Research 21, 1 (2020), 5485—-5551. arXiv:1910.10683\n[66] Nicolae Cătălin Ristea, Radu Tudor Ionescu, and Fahad Shahbaz Khan. 2022.\nSepTr: Separable Transformer for Audio Spectrogram Processing. In Proceedings\nof the Annual Conference of the International Speech Communication Associa-\ntion, INTERSPEECH. 4103–4107. https://doi.org/10.21437/Interspeech.2022-249\narXiv:2203.09581\n[67] Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf.\n2019. Transfer Learning in Natural Language Processing. In ACL: Tutorials.\n15–18.\n[68] Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq\nMinhas, Ashish R Mittal, and Fatma Ozcan. 2016. ATHENA: An ontology-driven\nsystem for natural language querying over relational data stores. VLDB 9, 12\n(2016), 1209–1220.\n[69] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD:\nParsing Incrementally for Constrained Auto-Regressive Decoding from Lan-\nguage Models. In EMNLP. 9895–9901. https://doi.org/10.18653/v1/2021.emnlp-\nmain.779 arXiv:2109.05093\n[70] Yao Shen, Lei Wang, and Yue Jin. 2022. AAFormer: A Multi-Modal Transformer\nNetwork for Aerial Agricultural Images. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition Workshops, Vol. 2022-June. 1704–1710.\nhttps://doi.org/10.1109/CVPRW56347.2022.00177\n[71] Richard Shin and Benjamin Van Durme. 2021. Evaluating the Text-to-SQL\nCapabilities of Large Language Models. CoRR abs/2204.0, 1 (2021), 1–12. https:\n//arxiv.org/abs/2204.00498\n[72] Nina Shvetsova, Brian Chen, Andrew Rouditchenko, Samuel Thomas, Brian\nKingsbury, Rogerio Feris, David Harwath, James Glass, and Hilde Kuehne. 2022.\nEverything at Once - Multi-modal Fusion Transformer for Video Retrieval. In\nProceedings of the IEEE Computer Society Conference on Computer Vision and\nPattern Recognition. 19988–19997.\nhttps://doi.org/10.1109/CVPR52688.2022.\n01939 arXiv:2112.04446\n[73] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam\nRajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vi-\njay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie\nBernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston,\nSaurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to\nTrain Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.\nCoRR abs/2201.1 (2022), 1–44. arXiv:2201.11990 http://arxiv.org/abs/2201.11990\n[74] Sahaana Suri, Ihab Ilyas, Christopher Re, and Theodoros Rekatsinas. 2021.\nEmber: No-Code Context Enrichment via similarity-based keyless joins. PVLDB\n15, 3 (2021), 699–712. arXiv:arXiv:2106.01501v1\n[75] Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam\nMadden, and Mourad Ouzzani. 2021. Rpt: Relational pre-trained transformer\nis almost all you need towards democratizing data preparation. PVLDB 14, 8\n(2021), 1254–1261. https://doi.org/10.14778/3457390.3457391 arXiv:2012.02469\n[76] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-\nshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang\nLi, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali,\nYanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao,\nYanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,\nPranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kris-\nten Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire\nCui, Marian Croak, Ed Chi, and Quoc Le. 2022.\nLaMDA: Language Mod-\nels for Dialog Applications. CoRR abs/2201.0 (2022), 1–47. arXiv:2201.08239\nhttp://arxiv.org/abs/2201.08239\n[77] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel,\nand Alon Halevy. 2021. From natural language processing to neural databases.\nProceedings of the VLDB Endowment 14, 6 (2021), 1033–1039. https://doi.org/10.\n14778/3447689.3447706\n[78] Immanuel Trummer. 2021. Can deep neural networks predict data correlations\nfrom column names?. In https://arxiv.org/pdf/2107.04553.pdf. 1–12.\n[79] Immanuel Trummer. 2021. Database tuning using natural language processing.\nSIGMOD Record 50, 3 (2021), 27–28.\n[80] Immanuel Trummer. 2021. The case for nlp-enhanced database tuning: Towards\ntuning tools that “read the manual”. PVLDB 14, 7 (2021), 1159–1165.\nhttps:\n//doi.org/10.14778/3450980.3450984\n[81] Immanuel Trummer. 2021. Verifying text summaries of relational data sets.\n[82] Immanuel Trummer. 2021. WebChecker: Towards an Infrastructure for Efficient\nMisinformation Detection at Web Scale. IEEE Data Eng. Bull. 44, 3 (2021), 66–77.\n[83] Immanuel Trummer. 2022. BABOONS: Black-box optimization of data sum-\nmaries in natural language.\nPVLDB 15, 11 (2022), 2980 – 2993.\nhttps:\n//doi.org/10.14778/3551793.3551846\n[84] Immanuel Trummer. 2022. CodexDB: Synthesizing code for query processing\nfrom natural language instructions using GPT-3 Codex. PVLDB 15, 11 (2022),\n2921 – 2928. https://doi.org/10.14778/3551793.3551841\n[85] Immanuel Trummer. 2022. DB-BERT: a database tuning tool that “reads the\nmanual”. In SIGMOD. 190–203. https://doi.org/10.1145/3514221.3517843\n[86] Immanuel Trummer. 2022. Demonstrating DB-BERT: A Database Tuning Tool\nthat \"Reads\" the Manual. In SIGMOD. Association for Computing Machinery,\n2437–2440. https://doi.org/10.1145/3514221.3520171 arXiv:2112.10925\n[87] Immanuel Trummer. 2022. Towards NLP-Enhanced Data Profiling Tools. In\nCIDR. 1–1. https://www.cidrdb.org/cidr2022/papers/a55-trummer.pdf\n[88] Immanuel Trummer. 2023. Demonstrating NaturalMiner: Searching Large Data\nSets for Abstract Patterns Described in Natural Language. In SIGMOD. 139–142.\n[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Processing Systems. 5999–6009.\narXiv:1706.03762\n[90] Prateek Verma and Jonathan Berger. 2021. Audio Transformers:Transformer\nArchitectures For Large Scale Audio Understanding. Adieu Convolutions. CoRR\nabs/2105.0 (2021), 1–5. arXiv:2105.00335 http://arxiv.org/abs/2105.00335\n[91] Prateek Verma and Chris Chafe. 2021. A Generative Model for Raw Audio Using\nTransformer Architectures. In Proceedings of the 24th International Conference\non Digital Audio Effects, DAFx 2021. the authors, 230–237. https://doi.org/10.\n23919/DAFx51585.2021.9768298 arXiv:2106.16036\n[92] Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen,\nSer Nam Lim, and Yu Gang Jiang. 2022. M2TR: Multi-modal Multi-scale Trans-\nformers for Deepfake Detection. In ICMR 2022 - Proceedings of the 2022 Interna-\ntional Conference on Multimedia Retrieval. 615–623. https://doi.org/10.1145/\n3512527.3531415 arXiv:2104.09770\n[93] Abdul Wasay, Subarna Chatterjee, and Stratos Idreos. 2021. Deep Learning:\nSystems and Responsibility. In SIGMOD. 2867–2875. https://doi.org/10.1145/\n3448016.3457541\n[94] Ziyun Wei, Immanuel Trummer, and Connor Anderson. 2021. Robust voice\nquerying with muve: Optimally visualizing results of phonetically similar\nqueries. PVLDB 14, 11 (2021), 2397–2409.\nhttps://doi.org/10.14778/3476249.\n3476289\n[95] Ziyun Wei, Immanuel Trummer, and Anderson Connor. 2021. Demonstrating\nRobust Voice Querying with MUVE: Optimally Visualizing Results of Phoneti-\ncally Similar Queries. In SIGMOD. 2798–2802.\n[96] Nathaniel Weir, Andrew Crotty, Alex Galakatos, Amir Ilkhechi, Shekar Ra-\nmaswamy, Rohin Bhushan, Ugur Cetintemel, Prasetya Utama, Nadja Geisler,\nBenjamin Hättasch, Steffen Eger, and Carsten Binnig. 2019. DBPal: Weak Su-\npervision for Learning a Natural Language Interface to Databases. (2019), 1–4.\narXiv:1909.06182 http://arxiv.org/abs/1909.06182\n[97] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Lan-\nguage Processing. In EMNLP. 38–45. https://doi.org/10.18653/v1/2020.emnlp-\ndemos.6 arXiv:arXiv:1910.03771v5\n[98] Kuan Xuan, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. 2022.\nSeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising. In\nNAACL. 1845–1853. arXiv:2105.07911 http://arxiv.org/abs/2105.07911\n[99] Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan Li,\nand Dragomir R. Radev. 2020. SyntaxSqlnet: Syntax tree networks for complex\nand cross-domain text-to-SQL task. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2018. 1653–1663.\nhttps://doi.org/10.18653/v1/d18-1193 arXiv:1810.05237\n[100] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James\nMa, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R.\nRadev. 2020. Spider: A large-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, EMNLP 2018.\n3911–3921. https://doi.org/10.18653/v1/d18-1425 arXiv:1809.08887\n[101] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022. Scal-\ning Vision Transformers. Proceedings of the IEEE Computer Society Conference\non Computer Vision and Pattern Recognition 2022-June (2022), 12094–12103.\nhttps://doi.org/10.1109/CVPR52688.2022.01179 arXiv:2106.04560\n[102] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and\nJianfeng Gao. 2021. Multi-Scale Vision Longformer: A New Vision Transformer\nfor High-Resolution Image Encoding. Proceedings of the IEEE International\nConference on Computer Vision (2021), 2978–2988.\nhttps://doi.org/10.1109/\nICCV48922.2021.00299 arXiv:2103.15358\n[103] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuo-\nhui Chen, Christopher Dewan, Mona T Diab, Xian Li, Xi Victoria Lin, Todor\nMihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh\nKoura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT:\nOpen Pre-trained Transformer Language Models. CoRR abs/2205.0 (2022).\nhttps://doi.org/10.48550/arXiv.2205.01068\n[104] Yanan Zhang, Jiaxin Chen, and Di Huang. 2022. Cat-Det: Contrastively Aug-\nmented Transformer for Multimodal 3D Object Detection. In Proceedings of the\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition.\n898–907. https://doi.org/10.1109/CVPR52688.2022.00098 arXiv:2204.00325\n[105] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating\nStructured Queries from Natural Language using Reinforcement Learning. CoRR\nabs/1709.0, 1 (2017), 1–12. arXiv:1709.00103 http://arxiv.org/abs/1709.00103\n[106] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang,\nQibin Hou, and Jiashi Feng. 2021. DeepViT: Towards Deeper Vision Transformer.\nCoRR abs/2103.1 (2021), 1–12. arXiv:2103.11886 http://arxiv.org/abs/2103.11886\n",
  "categories": [
    "cs.DB"
  ],
  "published": "2023-06-15",
  "updated": "2023-06-15"
}