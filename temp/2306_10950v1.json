{
  "id": "http://arxiv.org/abs/2306.10950v1",
  "title": "Benchmarking Robustness of Deep Reinforcement Learning approaches to Online Portfolio Management",
  "authors": [
    "Marc Velay",
    "Bich-Liên Doan",
    "Arpad Rimmel",
    "Fabrice Popineau",
    "Fabrice Daniel"
  ],
  "abstract": "Deep Reinforcement Learning approaches to Online Portfolio Selection have\ngrown in popularity in recent years. The sensitive nature of training\nReinforcement Learning agents implies a need for extensive efforts in market\nrepresentation, behavior objectives, and training processes, which have often\nbeen lacking in previous works. We propose a training and evaluation process to\nassess the performance of classical DRL algorithms for portfolio management. We\nfound that most Deep Reinforcement Learning algorithms were not robust, with\nstrategies generalizing poorly and degrading quickly during backtesting.",
  "text": "Benchmarking Robustness of Deep Reinforcement\nLearning approaches to Online Portfolio\nManagement\nMarc Velay∗, Bich-Liˆen Doan∗, Arpad Rimmel∗, Fabrice Popineau∗and Fabrice Daniel†\n∗Universit´e Paris-Saclay, CNRS, CentraleSup´elec\nLaboratoire Interdisciplinaire des Sciences du Num´erique\n91190 Gif-sur-Yvette, France\nEmail: firstname.lastname@centralesupelec.fr\n† LUSIS\n5 cit´e Rougemont\n75002 Paris, France\nEmail: fabrice.daniel@lusis.fr\nAbstract—Deep Reinforcement Learning (DRL) approaches to\nOnline Portfolio Selection (OLPS) have grown in popularity in\nrecent years. The sensitive nature of training Reinforcement\nLearning agents implies a need for extensive efforts in market\nrepresentation, behavior objectives, and training processes, which\nhave often been lacking in previous works. We propose a training\nand evaluation process to assess the performance of classical DRL\nalgorithms for portfolio management. We found that most DRL\nalgorithms were not robust, with strategies generalizing poorly\nand degrading quickly during backtesting.\nIndex Terms—OLPS · DRL · Portfolio Management · Bench-\nmarking · Robustness\nI. INTRODUCTION\nImprovements in Reinforcement Learning (RL), specifically\nDeep Reinforcement Learning (DRL) have contributed to new\nstate-of-the-art performances in many fields from robotics [8],\nautonomous vehicles [13] and games [9]. These improvements\nhave also led to a broad application to fields previously\ndominated by heuristic approaches. Online Portfolio Selection\n(OLPS) has seen a large increase in the popularity of DRL\nmethods, leading to seemingly conclusive and positive results.\nOLPS is a dynamic management of financial assets, where\nwe have the opportunity to redistribute funds across assets\nregularly, a sequential decision-making problem. This allows\nthe managing algorithm to adapt to market changes with the\naim of outperforming index performance.\nHowever, evaluations of existing works often rely on limited\nmetrics and protocols, which may not be suitable for portfolio\nmanagement. Many works cannot be reproduced due to data\nunavailability or lack of experimental information. OLPS is\nunlike many other RL problems due to its high uncertainty\nThis work benefited from the support of the Chair ”Artificial intelligence ap-\nplied to credit card fraud detection and trading” led by CENTRALESUPELEC\nand sponsored by LUSIS. This work was performed using HPC resources\nfrom the ”M´esocentre” computing center of CentraleSup´elec, ´Ecole Normale\nSup´erieure Paris-Saclay and Universit´e Paris-Saclay supported by CNRS and\nR´egion ˆIle-de-France (https://mesocentre.universite-paris-scalay.fr/).\nand non-stationary nature. The financial market, used as the\nRL agent’s environment, has a disproportionate impact on\nthe proposed reward functions and lack of predictability.\nFurthermore, RL algorithms are very sensitive to hyperpa-\nrameter selection and initialization, which require additional\nevaluations and consideration. Many published results often\ncontain single-initialization results, which may misrepresent\nthe capabilities of an approach and yield poorer performance\nif deployed. Furthermore, traditional metrics only indicate\nthe performance of an algorithm during its conception phase,\nclose to its training data. We have found that agents tend to\noverfit, picking the same assets regardless of market variations,\nwhich reflect favorably in traditional metrics. However, when\nthe market evolves, the static behavior degrades. Evaluating\nthe robustness of algorithms and their capacity to adapt to\nuncertainty and out-of-distribution data gives greater insight\ninto their training quality and generalization capabilities.\nThe aim of this work is to provide a standardized compar-\nison process to assess portfolio management algorithms. The\nprocess provides reproducible results on the performance of\nthe management agents. Reinforcement Learning is a complex\ntask with many components sensitive to tuning and design\nchoices. The proposed training and evaluation setup measures\nthe robustness and generalization capabilities of our bench-\nmarked algorithms. We rely on public data that are freely\navailable and open-source implementations of DRL algorithms\nto obtain transparent comparisons of popular approaches to\nOLPS. We focus on evaluating the training quality of our\nagents and their robustness to never-seen market conditions.\nTo the best of our knowledge, there is no large comparison of\napproaches in this domain, with multi-dimensional evaluation\nof components’ contribution to performance.\nII. RELATED WORKS\nPrevious works have often focused on single Deep Re-\ninforcement Learning approaches, generally improving one\narXiv:2306.10950v1  [cs.LG]  19 Jun 2023\naspect of the problem. In this section, we compare different\nmethods that have been previously presented. We focus on the\nlearning algorithms used, the ways to represent the environ-\nment, how they modify the portfolio positions, and how they\ninfluence the learned behavior through rewards. These related\nworks provide the different components which we evaluated\nin our experiment.\nA. Learning Algorithms for OLPS\nTABLE I\nPOPULAR LEARNING ALGORITHMS\nDDPG\nPPO\nA2C\nSAC\n[1, 3, 5, 6, 7, 15, 16]\n[3, 7, 15]\n[3]\n[3]\nThe most popular learning algorithm from Table\nI is\nDeep Deterministic Policy Gradient (DDPG), an iteration upon\nthe Policy Gradient algorithm, which is an efficient way\nto include continuous states and actions. We found works\nwhich made use of more recent algorithms that also improve\nupon Policy Gradients, such as Proximal Policy Optimization\n(PPO), Advantage Actor-Critic (A2C), and Soft Actor-Critic\n(SAC). Some authors modified the algorithms to improve\ngeneralization capabilities through data augmentation [1, 6, 7],\nresulting in some performance improvements. Others proposed\npolicy ensembles [14], with the best-performing policy on\nrecent historic data controlling the portfolio for a given time.\nPPO is the only on-policy algorithm, learning from its recent\nexperiences. DDPG, A2C, and SAC are off-policy algorithms\nexploring many paths in the environment for each learning\niteration. This approach can avoid staying in local optima by\nhaving a broader spectrum of experiences. However, this keeps\nolder experiences relevant during learning. This is efficient for\nproblems with large spaces to explore and data availability\nto support such a process, where transitions are infrequently\nreplayed. OLPS has limited data, with roughly 2500 points\nfor 20 years of history. This is a limiting factor that may have\nmotivated some authors to augment their data. However, this\ncan also be mitigated by sampling subsets of assets, where\neach set represents a new distinct environment.\nPPO and SAC include mechanisms to increase gradient step\nstability, such as entropy regularization or constraining step\nsize to the neighborhood of previous weights. This may be\nuseful for high-uncertainty environments. DDPG and A2C do\nnot provide such mechanisms, however they provide the ability\nto evaluate an actor through multiple critics.\nB. Market Representations\nTABLE II\nPOPULAR MARKET REPRESENTATIONS\nCurrent Prices\nSliding Window\nSparse Window\nContext\n[3, 6, 14]\n[5, 7]\n[1, 11]\n[1, 11, 15, 16]\nEach representation of the market in Table II must contain\nenough information for an agent to select the best action.\nBased on MDPs, this should only contain information about\nthe current state as they are independent. This theoretically\nsound approach was tried in previous works and may include\nfinancial indicators, aggregates of past prices, current asset\nallocations, or asset correlations. However, modeling complex\ntime series often requires past data points for more accuracy,\nwhich can be done using sliding windows of past prices, which\nmay be considered unique states. This was improved by using\nsparse sliding windows, drastically reducing the state space.\nSome previous works argue that the financial market may\nbe a Partially Observable MDP, requiring additional contextual\ninformation to accurately select actions. Frequent uses include\nNLP encodings or historical price statistics. A rare approach\nis Technical Analysis Indicators used by human traders.\nC. Management Rewards\nTABLE III\nPOPULAR REWARDS\nDaily Returns\nEpisodic Returns\nRate of Return\nRisk\n[3, 6, 7, 11, 14, 15]\n[1]\n[5]\n[16]\nThe rewards in in Table\nIII control the desired behavior\nof agents. Different metrics can favor risk-taking, portfolio\nturnover, or reach a trade-off between the two. The most\npopular approach employed by previous works consists of\nmeasuring the net difference in portfolio value between two\ndays. This difference can be between consecutive days or a\ntotal difference over the trading period. However, this approach\nis nonstationary, as large changes in valuation between the start\nand end of a trading period can impact their value, leading to\nperceived changes in rewards while they are proportionally\nsimilar. To remedy this caveat, some works propose using a\nrate of returns or an average daily log return. An alternative\nuses a composite function that rewards gains but constrains\nthe turnover through regularization.\nD. Limitations\nMost of the previously cited works lack one or more\nmeasures to evaluate their algorithms fully. Some publications\nuse private data, which may never be reproduced due to their\nnature. Others lack information about selected assets or the\nperiods during which they have evaluated performance. The\nuse of public financial data, with clearly defined training\nand backtesting processes, may remedy this caveat and allow\nothers to reproduce our results.\nAll works evaluate their algorithms with classic financial\nmetrics, such as net returns, risk metrics, including Sharpe\nand Sortino ratios or Maximum DrawDown. While these\ndo well to evaluate portfolio management, we have found\nthat overfitting agents predicting the same allocation weights\nregardless of selected assets and market changes had good\nperformance measures in validation but would quickly become\nobsolete if deployed. Worse, many previous works only share\na single result for their approaches, lacking confidence mea-\nsures. Combined with the difficulties of training DRL agents,\nthe same algorithm implementation with the same data may\nyield completely different results if initialized with different\nseeds. While the published results may be encouraging, they\nmay not be representative of their validity for future periods.\nTraining multiple agents, with different initializations, for each\napproach would allow the computation of confidence metrics\nto validate the results.\nMost works are compared to classic portfolio optimization\nalgorithms, such as Mean-Variance Optimization, while few\ncompare to other DRL approaches. Combined with the lack of\nreproducibility from the first point, we cannot determine why\nwe should choose their approach compared to other solutions.\nThis motivates us to include a broad range of methods in this\nwork to determine the preferred use of learning algorithms,\nmarket representations, and behavior objectives.\nIII. DEEP REINFORCEMENT LEARNING FOR ONLINE\nPORTFOLIO SELECTION\nBased on previous works, we define our OLPS environment\nsuch that an agent allocates wealth across N assets over T\ntime steps. The actions directly impact the weights of the\nassets in the portfolio, such that each asset corresponds to\nan action dimension. We add the possibility for the agent to\nkeep liquidity to reduce portfolio volatility.\nat = {a ∈RN+1;\nN\nX\ni=0\nai = 1, ai ∈[0, 1]}\nFor each time step, after rebalancing the portfolio, we observe\na transition, composed of a reward and a new state. The\naim of training agents is to find the sequence of actions that\nmaximizes the rewards obtained during an episode.\nWe select a set of four rewards to optimize the behavior of\nthe algorithms based on previous works. Daily net returns are\nfrequently used and correspond most to the MDP framework.\nYet, it is complex to attribute a reward to each action, as it is\ninfluenced by previous allocations and price changes, leading\nto delayed payoffs. Episodic rewards evaluate management\nover T time steps, smoothing responsibility from individual\nactions to sequential decisions. Total Net Returns measures\nthe total wealth changes over the T periods. Episodic Rate\nof Returns is numerically more interesting, being stationary,\nwith a value range closer to RL recommendations. Finally,\nthe Sortino ratio, described in the experiments section, is a\nmeasure of risk, which may lead to more careful management.\nWe aimed to compare the influence of each reward function\non management capabilities.\nAgents must learn the optimal actions for given states to\nmaximize rewards. The choice of market representations in-\nfluences the quality of state-action-reward mapping. Represen-\ntations containing more information add complexity through\nhigher state dimensions. Using only market data, composed\nof Open, High, Low, and Adjusted Closing prices, and Vol-\nume, we select four market representations. The Markovian\nRepresentation is the default, using only the five normalized\nOHLCV values for each asset for a given time t. This results\nin a tensor of one dimension RN∗5. To add more information\nabout the recent fluctuations, we use a continuous Sliding\nWindow Representation, containing the normalized OHLCV\nvalues of the last month. This state is composed of a two-\ndimensional tensor of shape R21×N∗5. A solution to reduce the\nsubstantial state size of the windowed representation is to use\na sparse sliding window, as individual distant prices have less\ninfluence on coming variations. The Lagged Representation\nonly uses data from the past five open days and a day 2-3-4\nweeks ago, reducing the state dimensions to R8×N∗5. Finally,\nhuman traders have used technical analysis for the past decades\nto better understand the market. The Indicators Representation\nuses raw OHLCV values to compute a synthesis of past\nmarket conditions. We include the change since the last time\nstep, MACD, Bollinger bands bounds, RSI, CCI, DX, and a\nsmoothed 30 days moving average of closing prices for a state\ntensor of dimension RN∗8.\nFinally, to learn the state-action-reward mapping, we rely\non popular DRL algorithms. We select SAC, PPO, DDPG,\nand A2C based on their previously mentioned differences and\nbenefits.\nIV. EXPERIMENTS\nIn this section, we present market data acquisition and\nprocessing, algorithm details and training, and performance\nmeasures.\nA. Data Processing\nWe focused on S&P500 constituent stocks at the date of\nJanuary 2023, for the period from January 2010 to December\n2022. We reserved the years 2021-2022 for backtesting and\nused the remaining 11 years for training. We analyzed 500\nassets, from which we composed a 20-asset portfolio. During\ntraining, 20 stocks are randomly subsampled. Due to the\nevolving nature of the market, not all assets existed at the\nstart of the historical data. We excluded those missing from\nbeing sampled, resulting in a growing stock pool as dates\napproach the present. The training period was randomly split\ninto non-overlapping training and validation sets of 60 days.\nWe accounted for the range of historical data required for some\nmarket representations, such as sliding windows, to avoid\nleaking validation data into training.\nFor backtesting, we manually selected 20 assets from di-\nverse industries with moderate to high returns for the period.\nThis guarantees that every evaluation was done in the same\nuniverse, while limiting performance skewing from selecting\nonly top-performing assets.\nInstead of using raw Open, High, Low, and Close prices,\nwe normalized them to obtain stationary values. The values\nwe used in most market representations, excluding indicators,\nis xt =\npt\npt−1 −1, where p is the price. The starting value of\nthe portfolio was 100000USD.\nB. Training agents\nReinforcement Learning algorithms are very sensitive to\nboth initialization and hyperparameter selection. The first step\nis to find a configuration that converges after a given number\nof iterations. For each algorithm, we ran 100 trials with Optuna\nTPE sweeper, which randomly samples sets of parameters\naround the previous best runs. The samples vary for each ap-\nproach based on the RL algorithm used. However, the volume\nof parameters to set for the environment, neural networks, and\nlearning algorithms is very large. From experience and domain\nknowledge, we selected parameters with fixed, reasonable\nvalues, that were excluded from sweeps. This is also motivated\nby the substantial amount of compute time required to train a\nsingle trial in a sweep, ranging between 2 and 5 hours on a\nCPU with 40 cores. The market representations were set based\non reasonable values, such as past month sliding windows and\nmost frequently used financial indicators. This approach is\nmotivated by the less sensitive nature of the representations\nto small changes.\nDuring the hyperparameter sweep, we trained an agent\non the training set and evaluated it on the validation sets.\nTheir starting dates and lengths were shared across all trials\nto avoid bias through market conditions. The result is a set\nof hyperparameters for which training converges for a given\nalgorithm combination. We used this set to train multiple\nagents for the same algorithm using different initialization\nseeds. This training phase was longer than the trials, with\nno early stops. The best checkpoint on the validation set\nwas selected as representative. Limited by compute time,\nwe trained five agents for each algorithm, using different\ninitialization seeds [4]. The end result is 320 trained agents\nto backtest.\nC. Backtesting Evaluation\nWe backtest all our algorithms on the two-year period of\n2021-2022. We manually selected 20 assets from different\nsectors (see table IV). All agents, heuristic or DRL-based,\nwere evaluated on the same period. Some representations\nrequired historical data, which overlaps the training data.\nTABLE IV\nASSETS USED IN BACKTEST\nTech\nHealthcare\nIndustry\nFinance\nEnergy\nAAPL\nUNH\nALLE\nJPM\nXOM\nMSFT\nJNJ\nAME\nBAC\nCVX\nGOOG\nPFE\nBA\nWFC\nNEE\nAMZN\nABBV\nCAT\nMS\nCOP\nUsing the logged daily actions, we computed the per-\nformance and robustness metrics. For performance, we use\ntraditional finance metrics. The first is the portfolio returns,\ndefined as PR = Valuet −Valuet−n, with n the comparison\nperiod. Specifically, n = 1 the daily returns and n = T, T\nbeing the length of the backtesting period. Portfolio Returns\nare used as the foundation of most other metrics. From net\nreturns, we obtain the rate of returns, a metric independent\nof the initial funds, defined as RoR =\nValuet−Valuet−n\nValuet−n\n. The\nprevious metrics evaluate the profits, but investors are often\ninterested in risk management. We use the Sortino ratio based\non the distribution of losses. It only penalizes downward\nvolatility, defined as:\nSortino = PR −RFR\nσd\n, σd =\ns\n1\nn\nX\n<threshold\n(PR −RFR)2\nWith PR, the annual returns, RFR, the risk-free rate, and σd,\nthe distribution of downsides. For our backtest, the RFR and\ndownside threshold are 0%. The final metric is the Maximum\nDrawdown. It is defined as:\nMDD = Bottom value −Peak value\nPeak value\nIt measures the maximum value loss an agent incurred during\nthe period, where investors would generally discard algorithms\nwith MMD > 20%.\nWe define robustness as the ability of an agent to resist\ninternal and external variations and uncertainty. Algorithms\nshould be reliable in out-of-distribution data and volatile peri-\nods, which is expected to occur in unseen market conditions.\nWe focus on observing the behavior of agents over time [2],\nin worse-case scenarios [10, 12], and the relative performance\nbetween training and backtesting.\nThe first robustness metric is the Conditional Value at Risk.\nThe literature defines it as:\nCV aR =\n1\n1 −0.05\nZ V aR\n−1\nxp(x)dx\nWhere VaR, the Value at Risk, is the average 5%, a popular\nvalue, worse rate of returns, and x is a given rate lower\nthan this threshold. CVaR measures the expected returns in\nworse-case scenarios. We use this metric because we expect\nDRL approaches to lack some of the stability that is found in\ntraditional heuristic methods. Algorithms that manage returns\nwith low kurtosis and, thus higher CVaR are deemed to be\nmore stable.\nThe second robustness aspect to be evaluated is the gen-\neralization capabilities of the algorithms. We compare their\nperformance during training and backtesting and the perfor-\nmance trend over sequential periods of the backtest. Both rely\non the Information Ratio, a financial metric that describes an\ninvestment’s performance beyond the market’s returns. It is\nconsidered a measure of active management.\nIR = mean(PRt −IndexRt)\nstd(PRt −IndexRt) )\nWhere IndexR is the market’s index returns. The first use-case\nis an IR Quotient, comparing the management performance in\nthe validation set to the performance during the backtest:\nIRq = IRbacktest\nIRvalidation\nValues closer to 1 indicate the algorithm preserves its per-\nformance on out-of-distribution data. The second use case is\nthe IR trend, evaluating the relevance of our trained algo-\nrithms with regard to market condition shifts. We compare\nthe monthly performance of an agent to a static market index.\nFig. 1. Rate of Return\nThe aim is to measure how well the learned behavior stays\nrelevant over time. We define it as:\nIRtrend =\nTP\nt=0\n(t −T/2)(IRt −¯\nIR)\nTP\nt=0\n(t −T/2)2\nBoth metrics rely on a market index. While our assets belong\nto the S&P500, using its price as a point of comparison is\nunfair because we use an unrepresentative subset. Smaller\nportfolios are subject to more volatility, and hand-picking\nassets results in performance biases. The market index we\nuse is defined as a Mean-Variance Optimization allocation\nat t = 0 and rebalanced monthly during the period. This\napproach yields a more realistic base of comparison for our\nalgorithms.\nThe third robustness aspect is the stability of the algorithms.\nWe aim to distinguish one-off successes from actually learned\nbehaviors. Using multiple seeds trained per agent, we evaluate\nthe variance of metrics. Agents trained on the same data, with\nthe same input, should yield similar results. Therefore, the\nlower the interseed variance, the more robust the approach.\nV. RESULTS\nDuring the backtesting period, we gathered trajectories for\n64 combinations of algorithms. Each metric is the average and\nstandard deviation of performance over the 5 seeds. Due to the\nvolume, we have chosen to represent the 10 combinations with\nthe highest rates of returns, as well as the two worse, as a basis\nof comparison. The names are abbreviated versions of previous\napproaches, where ”default” are daily prices, ”lagged” are\nsparse windows, ”net” are daily returns and ”value” are value\nchanges over an episode.\nThe results were obtained following an hyperparameter\nsweep, where we selected the best performing set. These sets\nof parameters were trained for enough iterations to reach a\nplateau and the best checkpoint was chosen. Each algorithm\nwas trained to the best of our ability. From table V, we can\nsee that most top-performing approaches performed relatively\nclose to each other in terms of performance, with net returns in\nthe 23-28% range over two years. Only the worst-performing\napproach lost wealth, on average, compared to the 63 others\nhaving positive returns. Most approaches reliably reached\ncomparable results across seeds, with relatively low spread\nof performance. Most approaches beat out the Mean Variance\nOptimization algorithm by a fair margin.\nHowever, while most returns are interesting, their risk\nmanagement is lacking, reaching high volatility with regard\nto the returns, as denoted by the Sortino ratio. This volatility\nis confirmed by the Maximum DrawDown measures, which\nare close to the limit of what may be tolerated for risky\nmanagement.\nFrom Fig. 1, we evaluate the effect of each component\non algorithms’ performance. The highest impact comes from\nthe representations, where the daily prices performed best,\nfollowed closely by the continuous sliding window. Surpris-\ningly, the indicators and sparse windows lead to markedly\nworse performance. The reward functions had no discernible\nimpact on performance, with the average returns for each\nfunction being almost identical. However, RoR provided the\nbest performance in most cases, with a lower variance in\nresults, proving to be more reliable. Finally, the training\nalgorithms had a low impact, each outperforming others for\ngiven combinations of states and rewards. DDPG came out\nbetter in most configurations, but did result in the single worse\nperformance. We can conclude that an approach based on daily\nvalues, which aims to maximize episodic Rate of Returns using\nDDPG would generally yield the best results.\nThe robustness metrics indicate underlying problems for\nour algorithms. The IR Quotients for our agents lie around\n0.6, indicating a large degradation of performance between\nthe validation and backtesting performances. Agents learned\nto pick winning assets during the training phase, but these\nwere no longer the best performing at later dates. This aspect\nis confirmed by the IR Trend, which indicates a monthly\ndegradation in performance for the length of the backtest, as\nthe market shifts. Both metrics point to overfitting of action\npolicies, confirmed by analyzing allocations over a period,\nwith no variations in positions for some agents. Generalizable\nbehaviors should focus on recognizing patterns instead of\nremembering which assets were previously winning. We can\npoint to the learning efficiency of the algorithms, which are\nnot suited for the low volume of data available in OLPS.\nVI. CONCLUSION\nWe proposed a standardized training and evaluation process\nto evaluate the performance and robustness of a large scope of\nOLPS approaches. We found that most approaches performed\nrelatively close to one another. Yet, the two types of metrics\nTABLE V\nPERFORMANCE METRICS\nRank\nName\nRoR\nSortino\nMDD\nCVaR\nIR\nIR Trend\nIR Quotient\nMVO\n0.98\n-0.066\n-21.9%\n-\n-\n-\n-\n1\nppo default sortino\n1.28 ± 0.05\n0.94\n-18.9%\n-2204 ± 71\n0.05\n-0.0051 ± 0.002\n0.76 ± 0.29\n2\nddpg default net\n1.26 ± 0.06\n0.82\n-20.9%\n-2418 ± 133\n0.05\n-0.0071 ± 0.002\n0.49 ± 0.04\n3\na2c default net\n1.26 ± 0.02\n0.88\n-19.2%\n-2214 ± 51\n0.05\n-0.0068 ± 0.001\n0.55 ± 0.06\n4\nddpg default value\n1.25 ± 0.07\n0.90\n-19.3%\n-2124 ± 52\n0.05\n-0.0070 ± 0.003\n0.59 ± 0.12\n5\nddpg default sortino\n1.25 ± 0.05\n0.89\n-19.4%\n-2123 ± 101\n0.05\n-0.0047 ± 0.003\n0.60 ± 0.17\n6\nsac default net\n1.25 ± 0.07\n0.87\n-18.6%\n-2120 ± 121\n0.05\n-0.0047 ± 0.001\n0.62 ± 0.27\n7\na2c windowed value\n1.24 ± 0.05\n0.85\n-18.8%\n-2114 ± 207\n0.04\n-0.0055 ± 0.002\n0.56 ± 0.17\n8\nppo default value\n1.24 ± 0.08\n0.83\n-19.2%\n-2195 ± 86\n0.04\n-0.0061 ± 0.003\n0.51 ± 0.22\n9\nddpg windowed sortino\n1.23 ± 0.06\n0.85\n-19.2%\n-2071 ± 98\n0.04\n-0.0044 ± 0.001\n0.53 ± 0.19\n10\nddpg windowed ror\n1.23 ± 0.04\n0.85\n-18.2%\n-2067 ± 156\n0.04\n-0.0045 ± 0.002\n0.56 ± 0.09\n63\na2c indicators value\n1.09 ± 0.10\n0.31\n-24.7%\n-2064 ± 172\n0.02\n-0.0065 ± 0.001\n0.21 ± 0.29\n64\nddpg indicators value\n0.80 ± 0.09\n-0.93\n-33.1%\n-1698 ± 185\n-0.04\n0.0004 ± 0.004\n0.68 ± 0.88\nindicate opposing results. While the returns and risk manage-\nment were interesting, fairly beating out a popular allocation\nalgorithm, they do not generalize well to unseen data and their\nperformance degrades markedly over time. This highlights the\nlearning efficiency limits of popular algorithms applied to the\nOLPS problem. Further improvements to these aspects may\nyield more robust approaches that profitably manage portfolios\nfor longer lengths of time.\nREFERENCES\n[1]\nEric Benhamou et al. “Bridging the Gap Between\nMarkowitz Planning and Deep Reinforcement Learn-\ning”. In: ICAPS PRL (2020).\n[2]\nStephanie C. Y. Chan et al. “Measuring the Relia-\nbility of Reinforcement Learning Algorithms”. In: In-\nternational Conference on Learning Representations.\nDec. 20, 2019.\n[3]\nRicard Durall. Asset Allocation: From Markowitz to\nDeep Reinforcement Learning. July 14, 2022.\n[4]\nPeter Henderson et al. “Deep Reinforcement Learning\nThat Matters”. In: Proceedings of the AAAI Conference\non Artificial Intelligence 32.1 (Apr. 29, 2018). DOI: 10.\n1609/aaai.v32i1.11694.\n[5]\nZhengyao Jiang, Dixing Xu, and Jinjun Liang. “A Deep\nReinforcement Learning Framework for the Financial\nPortfolio Management Problem”. In: arXiv:1706.10059\n(July 16, 2017).\n[6]\nXinyi Li et al. “Optimistic Bull or Pessimistic Bear:\nAdaptive Deep Reinforcement Learning for Stock\nPortfolio Allocation”. In: arXiv:1907.01503 (June 20,\n2019).\n[7]\nZhipeng\nLiang\net\nal.\n“Adversarial\nDeep\nRein-\nforcement Learning in Portfolio Management”. In:\narXiv:1808.09940 (Nov. 17, 2018).\n[8]\nTimothy P. Lillicrap et al. “Continuous control with\ndeep reinforcement learning”. In: 4th International Con-\nference on Learning Representations. Ed. by Yoshua\nBengio and Yann LeCun. 2016.\n[9]\nVolodymyr Mnih et al. “Human-level control through\ndeep reinforcement learning”. In: Nature 518.7540\n(Feb. 26, 2015), pp. 529–533. DOI: 10/gc3h75.\n[10]\nJanosch Moos et al. “Robust Reinforcement Learning:\nA Review of Foundations and Recent Advances”. In:\nMachine Learning and Knowledge Extraction 4.1 (Mar.\n2022), pp. 276–315. DOI: 10.3390/make4010013.\n[11]\nUta\nPigorsch\nand\nSebastian\nSch¨afer.\n“High-\nDimensional\nStock\nPortfolio\nTrading\nwith\nDeep\nReinforcement Learning”. In: 2022 IEEE Symposium on\nComputational Intelligence for Financial Engineering\nand Economics (CIFEr) (Dec. 9, 2021).\n[12]\nAdarsh Subbaswamy, Roy Adams, and Suchi Saria.\n“Evaluating Model Robustness and Stability to Dataset\nShift”. In: International Conference on Artificial Intel-\nligence and Statistics. PMLR, Mar. 18, 2021, pp. 2611–\n2619.\n[13]\nSen Wang, Daoyuan Jia, and Xinshuo Weng. Deep Re-\ninforcement Learning for Autonomous Driving. May 19,\n2019. DOI: 10.48550/arXiv.1811.11329.\n[14]\nHongyang Yang et al. Deep Reinforcement Learning\nfor Automated Stock Trading: An Ensemble Strategy.\nSSRN Scholarly Paper ID 3690996. Rochester, NY:\nSocial Science Research Network, Sept. 11, 2020. DOI:\n10.2139/ssrn.3690996.\n[15]\nYunan Ye et al. “Reinforcement-Learning Based Port-\nfolio Management with Augmented Asset Movement\nPrediction States”. In: Proceedings of the AAAI Con-\nference on Artificial Intelligence 34.1 (Apr. 3, 2020),\npp. 1112–1119. DOI: 10.1609/aaai.v34i01.5462.\n[16]\nYifan Zhang et al. “Cost-Sensitive Portfolio Selection\nvia Deep Reinforcement Learning”. In: IEEE Transac-\ntions on Knowledge and Data Engineering PP (Mar. 10,\n2020), pp. 1–1. DOI: 10/gj6rzg.\n",
  "categories": [
    "cs.LG",
    "q-fin.PM"
  ],
  "published": "2023-06-19",
  "updated": "2023-06-19"
}