{
  "id": "http://arxiv.org/abs/2306.11535v1",
  "title": "Evolutionary Strategy Guided Reinforcement Learning via MultiBuffer Communication",
  "authors": [
    "Adam Callaghan",
    "Karl Mason",
    "Patrick Mannion"
  ],
  "abstract": "Evolutionary Algorithms and Deep Reinforcement Learning have both\nsuccessfully solved control problems across a variety of domains. Recently,\nalgorithms have been proposed which combine these two methods, aiming to\nleverage the strengths and mitigate the weaknesses of both approaches. In this\npaper we introduce a new Evolutionary Reinforcement Learning model which\ncombines a particular family of Evolutionary algorithm called Evolutionary\nStrategies with the off-policy Deep Reinforcement Learning algorithm TD3. The\nframework utilises a multi-buffer system instead of using a single shared\nreplay buffer. The multi-buffer system allows for the Evolutionary Strategy to\nsearch freely in the search space of policies, without running the risk of\noverpopulating the replay buffer with poorly performing trajectories which\nlimit the number of desirable policy behaviour examples thus negatively\nimpacting the potential of the Deep Reinforcement Learning within the shared\nframework. The proposed algorithm is demonstrated to perform competitively with\ncurrent Evolutionary Reinforcement Learning algorithms on MuJoCo control tasks,\noutperforming the well known state-of-the-art CEM-RL on 3 of the 4 environments\ntested.",
  "text": "Evolutionary Strategy Guided Reinforcement Learning via\nMultiBuffer Communication\nAdam Callaghan\nUniversity of Galway\nGalway, Ireland\nA.Callaghan10@nuigalway.ie\nKarl Mason\nUniversity of Galway\nGalway, Ireland\nkarl.mason@universityofgalway.ie\nPatrick Mannion\nUniversity of Galway\nGalway, Ireland\npatrick.mannion@universityofgalway.ie\nABSTRACT\nEvolutionary Algorithms and Deep Reinforcement Learning have\nboth successfully solved control problems across a variety of do-\nmains. Recently, algorithms have been proposed which combine\nthese two methods, aiming to leverage the strengths and mitigate\nthe weaknesses of both approaches.\nA central component of algorithms that combine Evolution-\nary Algorithms with Deep Reinforcement Learning has been the\n\"Shared Replay Buffer\". Deep Reinforcement Learning algorithms\nrequire batches of data to update policy networks. Since Evolu-\ntionary Algorithms encounter such data in excess, they can feed\nthe data produced from a variety of different behavioural policies\nto the Deep Reinforcement Learning model. Deep Reinforcement\nLearning in-turn seeks to bias the Evolutionary Algorithm to higher\nareas of fitness by introducing high-performing individuals into\nthe population periodically. This paradigm has produced several\nhighly successful algorithms.\nIn this paper we introduce a new Evolutionary Reinforcement\nLearning model built on this framework, combining a particular\nfamily of Evolutionary algorithm called Evolutionary Strategies\nwith the off-policy Deep Reinforcement Learning algorithm TD3.\nThe framework utilises a multi-buffer system instead of using a\nsingle shared replay buffer. The multi-buffer system allows for the\nEvolutionary Strategy to search freely in the search space of policies,\nwithout running the risk of overpopulating the replay buffer with\npoorly performing trajectories which limit the number of desirable\npolicy behaviour examples thus negatively impacting the potential\nof the Deep Reinforcement Learning within the shared framework.\nThe proposed algorithm is demonstrated to perform compet-\nitively with current Evolutionary Reinforcement Learning algo-\nrithms on MuJoCo control tasks, outperforming the well known\nstate-of-the-art CEM-RL on 3 of the 4 environments tested.\nKEYWORDS\nReinforcement Learning, Shared Replay Buffer, Evolutionary Algo-\nrithms, Evolutionary Strategies\n1\nINTRODUCTION\nReinforcement Learning (RL) achieved notable success over the last\nfew decades, from playing board games like chess [28] and GO!\n[23] at near human-expert levels, playing video games where the\nagent is fed pixel inputs of the game [15], to self driving cars [13].\nEvolutionary Algorithms (EAs) have existed concurrently, with\ndeep roots in function optimisation problems. EAs seek to find\nProc. of the Adaptive and Learning Agents Workshop (ALA 2023), Cruz, Hayes, Wang,\nYates (eds.), May 29-30, 2023, London, UK, https://alaworkshop2023.github.io/. 2023.\nthe optimal set of parameters (often belonging to some function-\napproximator) in order to minimise the a loss function. EAs have\nbeen applied to a variety of real world problems such as: video\ngame level generation [7], determining an optimal architecture\nfor a function approximator [24] and also the control problem of\nfinding a behavioural policy that allows an agent to perform a task\nsufficiently [20].\nIn general an EA maintains a population of one or more individ-\nuals. In the case of finding an optimal control policy, an individual\nor agent is usually represented by the weight vector of its pol-\nicy Neural Network. This Neural Network maps states to actions,\nand is used to determine the actions the agent performs in the\nenvironment. EA make use of genetic operators such as mutation\nand recombination to apply a selective pressure to the population\nleading to the emergence of fitter agents. In practice, knowing the\nfitness of the entire population at any point in time is a must. This\nresults in many evaluations of the policy networks causing \"low\nsample efficiency\".\nConversely, Deep Reinforcement Learning (DRL) generally con-\nsists of a single agent again represented as one (or in the case of\nactor-critic algorithms - two) Neural Networks. The performance\nof this agent is improved upon by performing Stochastic Gradient\nDescent on the Neural Networks using batches [15] of (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)\ndata points, where ğ‘is the action that was taken when the envi-\nronment was in state ğ‘ , resulting in the environment transitioning\nto state ğ‘ â€² and receiving back a scalar reward ğ‘Ÿ. The updates per-\nformed on the Neural Networks, either directly or indirectly, seek\nto increase the total reward gathered by the policy described by\nthe networks. Since data points can be reused it leads to a higher\nsample efficiency than EAs. However, in practice DRL algorithms\nhave been observed to be more fragile to hyper-parameter choice\nas seen in [4] where the difference between setting a particular\nhyper-parameter to 0.9999 instead of 0.99 can be the difference\nbetween the agent solving the problem completely or not at all.\nAlgorithms have been proposed which seek to unite these two\nframeworks while preserving the desirable traits of both [22]. Most\nnotably and related to the work in this paper, is the algorithm titled\nERL [9], where a general framework is described. The framework\ncan be summarised as follows: A Genetic Algorithm containing\nk agents and a single Reinforcement Learner are initialised. The\nagents in the Genetic Algorithm are evaluated and by means of\ntournament selection [14] the fitter agents are assigned a higher\nprobability of passing their genes through to the next generation.\nThe evaluation trajectories of the GA are stored in a replay buffer\nwhich is shared with the trajectories created by the RL agent. As\nsuch, the batches sampled from this replay buffer by the RL agent,\ncome from a range of different behavioural policies. The Reinforce-\nment Learner is periodically allowed to overwrite the weakest\narXiv:2306.11535v1  [cs.NE]  20 Jun 2023\nmember of the GA, in hopes of biasing the GA towards regions\nof search space with higher fitness. In turn, this allows the data\nthe GA generates and feeds to the Reinforcement Learner to be of\nhigher quality.\nThis paper proposes a new algorithm inspired by ERL but achiev-\ning higher results through two key differences.\nâ€¢ Firstly, GAs have been observed to cause catastrophic for-\ngetting when used in control problems [2]. As such we in-\nvestigate another family of EAs - Evolutionary Strategies.\nIn particular, we apply the algorithm titled ES (sometimes\nreferred to as openES to avoid confusion with the family of\nalgorithms - ES). [19]\nâ€¢ Secondly, we propose that the use of multiple replay buffers\ncan increase the performance of this algorithm. This is founded\non the claim that EAs often generate new individuals by\nrandomly sampling from nearby policies. Since the fitness\nlandscapes of many control problems are often extremely\nnon-smooth [25] it is unwise to assume that every policy\nnear to a \"good\" policy will in itself be \"good\". This can lead\nto a single replay buffer being unbalanced with respect to\ngood and bad trajectories. A simple solution would be to\ncompartmentalise the buffer into \"good\"/\"bad\" trajectories\nand sampling batches according to some desired ratio. In this\npaper a third buffer is also used where the Reinforcement\nLearner can store its own exploratory sequences.\n2\nBACKGROUND AND RELATED WORKS\nMarkov Decision Process: A Markov Decision Process (MDP)\nis used to convert a sequential control task into a mathematical\nformulation. MDPs are represented as the tuple (S,A,T ,R,ğ›¾). S is the\nset of all states the environment can be in. In the case of playing\nboard games, S would be the set of every possible configuration\nthe pieces can be in. A is the set of all actions the agent may take\nin a state. T is the transition dynamics of the environment. It is\na map S : (S, A, S) â†’(0, 1) giving the probability that taking an\naction in a state will result in the environment changing to another\nparticular state. R is the reward function, which gives the reward ğ‘Ÿ\nreceived by the agent at each timestep after performing an action.\nIt is usually assumed that these rewards take scalar values. ğ›¾is a\nhyper-parameter which takes values in the range [0, 1]. Its role is\nto balance the agentâ€™s interests towards acting to receive maximal\nrewards over a long period of time (when ğ›¾= 1) or acting to\nmaximise immediate rewards (when ğ›¾= 0).\nDynamic Programming [27] is used to find the behavioural poli-\ncies that maximise the agents cumulative rewards - often simply\ncalled the optimal policy.\nReinforcement Learning: When the transition dynamics T\nare unknown, dynamic programming cannot be used, instead RL\nmay be used to learn a policy. RL algorithms can be model-based\n[27] in that they learn an approximation of the dynamics and find\nan optimal policy based on this, or model-free where they learn an\noptimal policy by acting through trial and error in the environment.\nIn this paper we focus on model-free algorithms.\nModel-free RL can be divided further into on-policy and off-\npolicy. In on-policy RL, the agent must learn from batches of data\ngenerated by action selections under its own policy ğœ‹. On the other\nhand, off-policy RL is free to learn from batches of data collected\nfrom a completely different behavioural policy than the one it is\ncurrently following.\nOff-policy RL allows for an agent to store trajectories it generates\nin a Replay Buffer [15], which it can then sample batches from. The\nuse of a replay buffer helps to increase sample efficiency massively\nwhile additionally helping protect the agent from forgetting prior\nlearned traits - referred to as catastrophic forgetting.\nThe most widely used RL algorithms today are off-policy includ-\ning DQN, TD3 [5] (an improvement on DDPG [12] reducing the\nagent overestimating the value of actions) and SAC [6]. The com-\nmon representations of agents in RL include value-based agents\nwhich approximate the expected cumulative discounted rewards\nassociated to each state-pair and define the policy based on this,\nactor-based agents who learn the policy without the need of value\nestimates and finally actor-critic methods which approximate the\nvalue and in turn use this information to aid the learning of the\npolicy.\nEvolutionary Strategies: Evolutionary Strategies are one of\nthree main sub-families of Evolutionary Algorithms, along with\nGenetic Algorithms and Genetic Programming. While Evolutionary\nStrategies are often applied to numerical optimisation problems,\nhere we discuss how they can be applied to solving control prob-\nlems. An ES is instantiated by placing a parameterised probability\ndistribution over the search space - in this case the space of all\npolicy network parameters. While a range of probability distribu-\ntions could be selected, previous studies have demonstrated that\nusing a multivariate isotropic Gaussian with mean parameter ğœƒand\nstandard deviation ğœallows for significant run-time speed-ups [19].\nUnder this configuration, the algorithm holds fixed the standard de-\nviation of the distribution but performs approximate gradient ascent\non the mean parameter with respect to the policyâ€™s fitness func-\ntion ğ½(ğœƒ) by evaluating the fitness of policies selected through the\ndistribution in a method that closely resembles finite-differencing.\nHowever, Lehman et al. [11] clearly demonstrate that ES is sig-\nnificantly different to finite-difference methods and other point-\ngradient methods in general as it does not seek to place the mean at\nthe point of highest fitness, but rather centre the entire distribution\nover a region such that sampling from it leads to solutions with\nhigh fitness. This can lead to unexpected behaviours [11], where\nthe geometry of fitness landscapes can result in the mean param-\neter returned by ES having significantly lower performance than\npoints nearby it. ES doesnâ€™t succumb to the problem of gradient\ngaps. This is not true for RL. As ES is required to evaluate the poli-\ncies that are sampled from the distribution over entire episodes\nin order to generate gradient updates, a large amount of data is\ngenerated. Other evolutionary approaches such as GAs can be more\ndata efficient in this sense, as it is possible to maintain a smaller\npopulation of individuals to evaluate (pop-size=10) than ES which\ncan require hundreds of evaluations per gradient update. While\nthis can make GAs appear more appealing, it has been noted by\nBodnar et al. [2] that GAs can lead to undesirable traits such as\ncatastrophic forgetting of learned behaviours, while [20] claims\nGAs fail when a reward signal is either sparse or deceptive as is\noften the case in control problems. ES also have the property of\ndirectly approximating gradients as opposed to the more heuristic\nsearch technique observed in GAs.\nâˆ’8\nâˆ’6\nâˆ’4\nâˆ’2\n0\n2\n4\n6\n8\n0\n0.1\n0.2\n0.3\nğœƒ\nğ‘ƒ(ğœƒ| ğœ‡, ğœ)\nğ¹(ğœƒ)\nFigure 1: Evolutionary Strategies\nThe red curve represents a Gaussian probability density function (PDF)\nwith mean ğœ‡and standard deviation ğœover a 1D parameter space. The blue\ncurve represents the fitness of the policy defined by the weights ğœƒ. In\ncontrol problems this fitness function is unknown but can be sampled. ES\napplies approximate gradient ascent to the mean, effectively \"sliding\" the\ndistribution along the x-axis until it finds itself over a region of parameter\nspace where policy weights sampled according to ğ‘ƒ(ğœƒ| ğœ‡, ğœ) will have\nhigh fitness.\nEvolutionary Reinforcement Learning: Evolutionary Rein-\nforcement Learning is a fast emerging area combining RL and EA ap-\nproaches. The reason for its appeal is due to the the two approaches\nhaving almost directly opposing traits. RL often follows point gra-\ndients which can lead to problems of convergence to local optima,\nvanishing gradients and gradient gaps. Conversely, EAs such as\nGAs are gradient free, while ES do not follow point-gradients re-\nsulting in a different behaviour. Off-policy reinforcement learning\nalgorithms are more data efficient due to their ability to re-use\ndata from the Replay Buffer, however EAs often require the entire\npopulation to be evaluated on each generation resulting in much\nmore data. As such, methods have been proposed which seeks to\ncombine the strengths of the two approaches.\nKhadka and Tumer [9] propose a framework in which a GA\nruns alongside an RL (DDPG) agent. The GA can feed the RL its\nexcess data by sending its trajectories to the RL agentâ€™s replay buffer.\nThe RL agent can occasionally overwrite the weakest performing\nmember of the GA seeking to improve the overall average fitness of\nthe population. This leads to a cycle of the RL agent improving the\npopulation resulting in better performance during training. These\nresults demonstrate an increase in performance of the compound\nalgorithm over the two halves when they act separately.\nCEM-RL proposed by Pourchot and Sigaud [16] combines CEM\n(a method similar to ES, but without directly approximating gradi-\nents) with TD3. CEM maintains a population mean and standard\ndeviation and samples policies for evaluation in a similar way to\nES. However only the top 50% are used in updating both of the\npopulation parameters. In CEM-RL when the sampled policies have\nbeen generated, half are evaluated directly while the other half\nare updated following the TD3 actor update rule for ğ‘€time steps,\nbefore being evaluated. The algorithm then concludes similar to\nCEM with the top 50% overall being selected and used to update\nthe distribution parameters.\nAES-RL [10] builds on CEM-RL by introducing a multiple worker\nasynchronous update framework allowing for the algorithm to\nbenefit from temporal speed-ups and achieving state of the art\nresults in some environments.\nCHDRL [30] uses a hierarchical architecture to combine a global\noff-policy RL agent, with a local on-policy RL agent and a local\nEA (CEM). This builds on the reasoning that combining EA with\nRL is good due to their opposing properties, one step further by\nalso trying to make use of the good sample efficiency properties of\noff-policy RL while also benefiting from the stability properties of\non-policy RL.\nWhile ES is not exactly like a finite-difference algorithm as men-\ntioned earlier, it still closely resembles one. Shi et al. [21] combine\nthe gradient updates of a RL agent (DDPG) with those of a Finite-\nDifference algorithm (Augmented Random Search) to update the\nparameters of one single shared policy network.\nESAC [26] combines an EA with Soft Actor Critic [6] - a RL\nalgorithm which maximises both the cumulative reward of the\nagents but also the entropy of the action distributions. This leads\nto better exploration of the agent and thus improved results.\nThe increasing interest in Evolutionary Reinforcement Learning\nhas also lead to the development of platforms to aid researchers\nand developers implement these algorithms [1].\n3\nMETHODS\nIn this section we present our algorithm using a similar framework\nto ERL[9]. We explain the intuition behind the multibuffer system in\nthis application and provide pseudocode for our proposed algorithm,\nES-TD3Buffers.\n3.1\nMulti-Buffers\nThe framework first presented in ERL allowed for the genetic al-\ngorithm to send the trajectories its population of policies generate\nduring their evaluation phase to a single buffer. The RL agent then\nappends its own exploratory experience and samples a batch uni-\nformly from the buffer to update its parameters.\nWhile GAs (where the elite survives from generation to genera-\ntion) ensures the max score of the population will not decrease over\nthe iterations (in deterministic environments), no such guarantee is\nobserved with ES. Due to ES following a Monte Carlo gradient ap-\nproximation, it is easy for it to \"fall\" from peaks of high fitness if the\nlearning rate is set too high, while being extremely data inefficient\nif the learning rate is set too small. Pairing this with its abnormal\nsearch strategy which allows for the mean of the Gaussian distribu-\ntion to centre itself in areas of low fitness provided the surrounding\nareas have high fitness, runs the risk of the over-production of poor\nperforming trajectories. Carelessly appending all such trajectories\nto a single buffer runs the risk of pushing all \"good\" trajectories out\nof the buffer and leaving the RL agent with nothing but undesirable\nbehaviours to learn from. Intuitively, without having any examples\nof desirable behaviour, the agent runs the risk of learning the best\nof the undesirable behaviours.\nHence we propose a simple multibuffer approach. By compart-\nmentalising the replay buffer into \"Good\",\"Bad\" and \"Exploratory\"\npartitions, ES can append all its trajectories without the risk of\nnegatively impacting the RL agent. In this paper a very simple\nthreshold is used to determine whether a trajectory is considered\n\"Good\" or \"Bad\", namely we track the highest episodic fitness and\nuse 90% of this number as the threshold. Since the RL agent gen-\nerates exploratory trajectories on a timestep basis rather than one\ncomplete episode at a time, we propose that these data points are\nstores in the separate compartment as it is unknown if they can be\nconsidered \"Good\" or \"Bad\" until the episode terminates. During\nthe learning step the RL agent samples a ratio from each buffer.\nBuffer augmentation is a technique implemented in previous RL\nstudies [13], where a car was trained to safely overtake, a separate\nbuffer was utilised to store any crashes. As crashing during online\ntraining is extremely dangerous and costly, the use of a second\nbuffer minimised the risk of catastrophic forgetting as the policy\nimproved by reminding it periodically of actions that lead to crashes\n- penalised by heavily negative rewards. In another recent study,\nSadat Esmaeeli and Malek [18] improved on ERLâ€™s results by using\nan Elite buffer that was generated from the GAâ€™s trajectories, but\napplied data engineering techniques to allow only the most diverse\nsubset of good trajectories be used in the update step.\nFigure 2: ES-TD3Buffers Framework\nES begins by generating offspring by sampling from its\ndistribution. The resulting offspring are evaluated with their\ntrajectories sent to the corresponding compartment of the buffer.\nTD3 adds data to the noisy buffer when acting in the environment\nand samples a batch from all three buffers to perform its update.\nPeriodically ES and TD3 are compared, with the mean of ES being\nreplaced by TD3 if TD3 is outperforming it.\nAlgorithm 1 ES-TD3Buffers\nInitialise: TD3 actor ğœ‹\nInitialise: TD3 critics ğ‘„0, ğ‘„1\nInitialise: TD3 target critics Ë†ğ‘„0, Ë†ğ‘„1\nInitialise: ES mean ğœ‡and std ğœ\nInitialise: MultiBuffer: ğ›½ğº,ğ›½ğµ,ğ›½ğœ–with sampling ratio (ğ‘: ğ‘: ğ‘)\nSet ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘= âˆ’âˆ\nfor âˆdo\nfor M frames do\nâŠ²perform TD3 iterations\nReset env ğ‘ = ğ‘ 0\nğ‘’ğ‘_ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘= 0\nwhile Episode not terminated do\nğ‘â†ğœ‹(ğ‘ ) + ğœ–\nâŠ²Generate exploratory action\ntake action ğ‘in env, add (ğ‘ ,ğ‘,ğ‘ â€²,ğ‘Ÿ) to ğ›½ğœ–\nğ‘’ğ‘_ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘+=ğ‘Ÿ\nif All buffers contain K datapoints then\nSample batch from ğ›½ğº,ğ›½ğµ,ğ›½ğœ–, under ratio(ğ‘: ğ‘: ğ‘)\nUpdate ğœ‹and ğ‘„0, ğ‘„1 using TD3 update rule\nPeriodically soft-update Ë†ğ‘„0, Ë†ğ‘„1\nend if\nend while\nif ğ‘’ğ‘_ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘> ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘then\nğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘â†ğ‘’ğ‘_ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘\nend if\nend for\nğ‘‡ğ·3 â†ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’(ğœ‹)[0]\nfor g Generations do\nâŠ²perform ES iterations\nfor ğ‘–in n do\nâŠ²For each ES offspring\nSample noise ğ‘ğ‘–âˆ¼N (0, ğœ2)\nGenerate offspring ğ‘‹ğ‘–= ğœ‡+ ğ‘ğ‘–\nğ¹ğ‘–,ğ‘¡ğ‘Ÿğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦= ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’(ğ‘‹ğ‘–)\nif ğ¹ğ‘–> 0.9 âˆ—ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘then\nSend ğ‘¡ğ‘Ÿğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦to ğ›½ğº\nif ğ¹ğ‘–> ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘then\nğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘â†ğ¹ğ‘–\nend if\nelse\nSend trajectory to ğ›½ğµ\nend if\nend for\nğœ‡â†1\nğ‘›\nÃğ‘›\nğ‘–=1 ğ¹ğ‘–ğ‘ğ‘–\nğ¸ğ‘†â†ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’(ğœ‡)[0]\nend for\nif ğ‘‡ğ·3 > ğ¸ğ‘†then\nğœ‡â†ğœ‹\nend if\nend for\nReturn: ğœ‡\n3.2\nOverwrite Rule\nIn ERL, the use of a GA allowed for the RL policy to be periodically\nsubstituted for the lowest performing individual in the population.\nSince ES can be understood as maintaining a population of size\none, namely the policy described by ğœ‡, any attempt to insert the\nRL policy will result in a greater loss of information than the GA\nAlgorithm 2 Evaluate\nRequire: Env, policy ğœ‹\nReset Env ğ‘ â†ğ‘ 0\nğ¹ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ â†0\nğ‘‡ğ‘Ÿğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦= []\nwhile Env not terminated do\nğ‘â†ğœ‹(ğ‘ )\nTake action ğ‘in env, and observe reward ğ‘Ÿand next state ğ‘ â€²\nAppend (ğ‘ ,ğ‘,ğ‘ â€²,ğ‘Ÿ) to ğ‘‡ğ‘Ÿğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦\nğ¹ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ â†ğ¹ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ + ğ‘Ÿ\nğ‘ â†ğ‘ â€²\nend while\nReturn:ğ¹ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ,ğ‘‡ğ‘Ÿğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦\nsetting. In this paper we utilise a simple overwrite rule such that\nif the RL policy is performing better than the ES mean, then the\nES mean should be replaced with the RL policy parameters. In\npractice this was done by averaging the episodic scores of RL and\nES over a number of iterations as these algorithms can occasionally\nsignificantly decrease in performance between updates. Utilising\naverages will mitigate such effects in the comparison.\nAfter implementation we found that ES often settled on local\noptima, the use of the overwrite rule can be interpreted as providing\na means for ESâ€™s distribution to be re-centred over the search space\nof parameters if the RL agent finds an area with higher performance.\nThis then will allow ES to generate offspring in this \"fitter\" region\nof space and hence avoiding the replay buffers being filled with the\nsame low fitness trajectories for the remaining run time.\n3.3\nES-TD3Buffers\nTo implement our algorithm, a TD3 actor ğœ‹, dual critics ğ‘„0, ğ‘„1 and\ncorresponding target critics are initialised. Here we use Neural Net-\nworks as function approximators for each. Additionally an isotropic\nmultivariate Gaussian distribution with mean ğœ‡and standard de-\nviation ğœis centred over the origin of the space of all parameters\nof the actor network. Furthermore, a \"MultiBuffer\" consisting of 3\nempty buffers is initialised.\nPeriods of TD3 updates are applied to ğœ‹, ğ‘„0 and ğ‘„1 (with soft\nupdates applied to the target critics). This is followed by a fixed\nnumber of generations of sequential updates to the search distri-\nbution parameter ğœ‡, as described in Salimans et al. [19]. The two\naforementioned processes are then iterated until a stopping crite-\nria has been met (here fixed number of iterations). During each\ngeneration of ES, a population of actors are evaluated, which gen-\nerates trajectories of data. These data points are sorted into the\ncorrect replay buffer (\"Good\" or \"Bad\") by comparing the reward\ngathered over the entire episode to the current \"threshold\". The\nsimple \"threshold\" used in this paper is 90% of the highest recorded\nES episode to date. TD3 on the other hand, generates \"noisy\" data\nwhen selecting actions during training by adding Gaussian noise to\nthe TD3 policy. As such we place this data in the third replay buffer.\nAfter TD3 performs a \"noisy\" action in the environment, a batch of\ndata is drawn from the three buffers following a predefined ratio\n(see table 2). Gradient ascent is performed on the parameters of\nTD3s actor and critics as first described in Fujimoto et al. [5].\nAfter every iteration a comparison is made between the TD3 ac-\ntor and the policy defined by ğœ‡. If the TD3 actor ğœ‹is outperforming\nğœ‡, we re-centre the ES search distribution over ğœ‹. Pseudocode for\nthis can be found in Algorithm 1.\n4\nEXPERIMENTAL STUDY\nIn this section we evaluate the performance of the ES-TD3Buffers\nalgorithm. In particular we aim to answer the following questions:\nâ€¢ How does ES-TD3Buffers perform in control tasks compared\nto pre-existing Evolutionary Reinforcement Learning algo-\nrithms?\nâ€¢ What limitations does the algorithm have?\n4.1\nEnvironments\nWe used the well known OpenAI gym package which offers a\nvariety of continuous state and action space environments through\nthe MuJoCo physics simulator.\nThe majority of environments consist of controlling a multi-\njointed robot with the goal of performing a task such as running as\nfast as possible or staying upright for as long as possible. The action\nspace consists of the set of all force vectors that can be applied to\nthe joints.\nIn our experiments we used \"HalfCheetah\", \"Swimmer\", \"Ant\"\nand \"Walker2d\" as they are commonly analysed environments in\nthis area. Figure 3 illustrates these problem domains.\n4.2\nArchitecture\nIn our implementation we used feed-forward Neural Networks to\nrepresent all actors and critics. The actors consisted of 2 hidden\nlayers and 256 neurons per hidden layer. The number of output\nnodes was equal to the dimension of the action space for that\nparticular environment, similarly the number of input neurons\nwas equal to the dimension of the state space. The networks used\nhyperbolic tangent activation functions between all layers.\nThe critic consisted of 2x256 hidden layers, while the input\nlayerâ€™s number of neurons was set to the sum of the state space\ndimensionality plus the action space dimensionality. The output\nlayer consisted of a single neuron. The learning rate for the TD3\nactor and critic was set to 0.0003, while the ES used a learning rate\nof 0.001. For more details see Table 2.\n4.3\nResults\nWe compare our algorithm to several well known and commonly\nused RL and EC control algorithms in Table 1 .\nTD3 is reported as it is used as a building block within our ES-\nTD3Buffers algorithm. CEM is a common alternative to ES which\nfollows a very similar update rule where we only use the fittest K\noffspring to update our search distribution as opposed to using all\noffspring as in ES. Finally ERL and CEM-RL are reported as two of\nthe most influential examples of Evolutionary guided reinforcement\nlearning.\nThe ES-TD3Buffers is ran for a total of 20 iterations, where\neach iteration consists of 50 updates to the ES distribution mean,ğœ‡,\n100â€™000 TD3 timesteps and a chance for ğœ‡to be overwritten with ğœ‹.\nAll baselines shown are those reported in Lee et al. [10].\nFigure 3: MuJoCo Environments\nFrom left to right: Ant, HalfCheetah, Walker and Swimmer. The goal in all environments is for the creature to learn to move at a high\nvelocity.\nTable 1: Scores achieved on MuJoCo Environments. Baselines As Reported in Lee et al. [10]\nEnvironment\nStatistics\nTD3\nCEM\nERL\nCEM-RL\nES-TD3Buffers\nMean\n9630\n2940\n8684\n10725\n10793\nHalfCheetah\nStd.\n202\n353\n130\n397\n778\nMedian\n9606\n3045\n8675\n11539\n10862\nMean\n4027\n487\n3716\n4251\n4532\nAnt\nStd.\n403\n33\n673\n251\n999\nMedian\n4587\n506\n4240\n4310\n4367\nMean\n63\n351\n350\n75\n213\nSwimmer\nStd.\n9\n9\n8\n11\n119\nMedian\n47\n361\n360\n62\n174\nMean\n3808\n928\n2188\n4711\n2217\nWalker\nStd.\n339\n50\n240\n155\n1454\nMedian\n3882\n934\n2267\n4637\n1764\nIn the interest of fairness and clarity, we note that ES-TD3Buffers\nuses significantly more data than other reported algorithms. This\nis primarily due to the fact that ES requires significantly more data\nthan some other EAs including GAs and CEM as to approximate the\ngradients in high dimensional spaces we often require the number\nof offspring used to be in the range of hundreds to thousands.\nWe claim the increase in samples should not falsify ES-TD3Buffers\nclaim as a competitive algorithm under the reasoning provided by\nSalimans in his original paper [19]. Namely since ES does not re-\nquire backpropagation to update the parameters of the distribution,\nit can run in competitive time to other more data efficient gradient-\nbased algorithms. In the future works section, we discuss how\nparallel computing could be leveraged to speed this up further.\nFor comparative purposes, we ran our algorithm such that the\nTD3 half generates 2 million frames.\nThe results show ES-TD3Buffers performs comparably with the\nstate of the art - CEM-RL, across a variety of control tasks. Notably\nES-TD3Buffers is able to outperform CEM-RL on both HalfCheetah,\nAnt and Swimmer, while performing comparatively to ERL on\nWalker.\nWe note that while the average performance of our algorithm is\nhigh, the standard deviation is also. We expect that this is caused\nin part to the overwrite rule, which can take the small variance in\nTD3 runs -especially at the early stages - and choose to make vastly\ndifferent overwrites to the ES actor, which will ultimately affect the\nquality of data TD3 is trained on over the subsequent iterations. A\npossible solution to this which we will investigate is to implement\na softer-update rule.\nFurthermore, we note that we were unable to achieve the same\nresults with our version of TD3 on the Walker environment as\nthose reported in Lee et al. [10] and shown in Table 1. The Github\ncontaining the TD3 source code, which we used, does state hyper-\nparameters have been changed from the original implementation\nand we expect that this could be the reasoning for why our ES-\nTD3Buffers does not perform at least as well as the reported TD3\nresults on the Walker environment.\n4.4\nES vs TD3 Learning Dynamics\nIn this section we examine how TD3 and ES work together in ES-\nTD3Buffers to improve on the results they achieve individually\nacross the majority of MuJoCo environments. Figure 4 shows the\nindividual learning curves for both the ES and TD3 halves of our\nalgorithm, across the full 20 iterations on the HalfCheetah environ-\nment. An iteration consists of 50 generations of ES and 100â€™000 TD3\ntimesteps. The figure showcases the typical behaviour witnessed\nacross the other environments - Walker and Ant, however not\nSwimmer for reasons we discuss later. Most notably Figure 4 allows\nus to see how ES quickly converges to a local optimum by the end of\nthe 1st generation. In our experiments with ES we found it seldom\nescaped these local optima for a wide choice of hyperparameters.\nThis is evident by the results of the closely related CEM in Table 1,\nwhich performs poorly across HalfCheetah, Ant and Walker when\ncompared to the other algorithms. By allowing TD3 to overwrite\nthe mean ğœ‡of the Evolutionary Strategies search distribution with\nits own actor ğœ‹, we see how ES gets lifted out of the local optima of\nthe early generations. Most interestingly is that while ES struggles\nat the beginning of training on these environments, it usually can\nbe seen to exceed the converged TD3 performance over the last few\niterations (iterations 14 to 20 in Figure 4). In the case of Swimmer,\nES is capable of achieving much better results than TD3 when ran\nindividually, as seen again by the closely related CEM achieving\nthe best results on this environment in Table 1. Thus there are no\nearly overwrites performed by TD3 in that environment.\nFigure 4: ES and TD3 HalfCheetah Learning Curves\nES and TD3 both run for 20 iterations of their own update rule,\nand on completion of each iteration, a performance check is\ncarried out which can lead to ğœ‡being overwritten with ğœ‹. The\ncurves shown are smoothed with a moving window of size 10 to\nprovide a clearer visual aid\n5\nDISCUSSION\nWe have demonstrated that Evolutionary Strategies is a highly\neffective evolutionary method for ERL algorithms. The results pre-\nsented in this paper contribute to the existing literature in multiple\nrespects.\nFirstly, as reported in the Results section, ES is significantly more\ndata inefficient than GAs. This is due to the fact that GAs can be\nrun effectively with population sizes of â‰¤10. On the contrary, us-\ning Neural Networks as function approximators in ES often causes\nthe dimensionality of the search space to be very large. As ES is\nseeking to Monte Carlo approximate the gradient in a method simi-\nlar to finite-differencing, the number of samples required in high\ndimensional spaces should also be large.\nIn practice we found ES required the creation of 60 offspring per\ngeneration to achieve good performance. Since antithetic sampling\nis implemented, this is equivalent to 120 offspring in the population.\nThis already results in 10ğ‘¥more data being used than in the GA case.\nMethods such as E-ERL [29] have been proposed which reduces\nthe data usage of ERL by only allowing the GA component of the\nalgorithm to run when the RL component has converged to a local\noptimum. Such a technique could also reduce the data usage when\nES is used in place of a GA as per our algorithm.\nSecondly, the overwrite rule in this paper can result in a huge\nloss of information in the ES distribution. In the case of a genetic\nalgorithm, only a single member of the population is overwritten,\nbut due to the population being represented by the mean parameter\nin ES the population can be represented as having a cardinality\nof 1. Alternative overwrite rules have been explored such as that\nof Jung et al. [8], which avoids convergence to the same areas of\nthe search space by implementing a \"soft update\". A soft update in\nES-TD3Buffers could push the ES actor towards the RL in parameter\nspace, without setting it directly equal to the RL actor.\nThirdly, the original ES paper [19] demonstrated experimentally\nhow efficiently ES can run across parallel workers. The parallel\nversion of ES reduced training times of some MuJoco problems\nfrom 18 hours down to 10 minutes. As such, the excess of required\ndata could be justified. Due to the way in which our algorithm\nrequires trajectories to be shared between the ES offspring and the\nRL agent, our algorithm cannot be parallelised in the exact way in\nwhich Salimans et al. [19] did.\nHogwild! [17] is a platform which gained popularity in RL [15]\nby allowing a central reinforcement learner to share its parameters\namongst several parallel workers, and in return asynchronously re-\nceive gradient updates it should perform on the central parameters\nfrom each worker. A platform similar to this could be used in the\ncase of ES-TD3Buffers to store the RL agent and ES distributional\nparameters on the central worker while allowing ES to evaluate its\noffspring in parallel, communicating back the trajectories with the\ngradients.\nLastly, NSRA-ES [3] changed the definition of fitness in ES from\npurely episodic score to a linear combination of episodic score and\nnovelty with respect to some archive of previously encountered\nbehaviours. The coefficients of this linear combination could adap-\ntively change weight to further emphasise optimising episodic score\nwhen improvements were being made, while increasing the empha-\nsis on finding new behaviours when the episodic scores stopped\nimproving. NSRA-ES would be a perfect substitute for ES in ES-\nTD3Buffers and further encourage the discovery of more diverse\nbehaviours being added to the replay buffer.\n6\nCONCLUSION\nIn this paper we have presented a new algorithm called ES-TD3Buffers.\nThe ES-TD3Buffers algorithm demonstrates that Evolutionary Strate-\ngies is highly effective when combined with RL algorithms for\nEvolutionary Reinforcement Learning. A portion of the success of\nES-TD3Buffers is due to its unique multi-buffer architecture. Our\nalgorithm performs comparably to the current state-of-the-art Evo-\nlutionary Reinforcement Learning algorithm (CEM-RL) when tested\nTable 2: Hyperparameters\nHyperparameter\nDescription\nValue\nğœ\nES Standard Deviation\n0.005\nğœ‡\nES Mean Vector\nRandomly initialised near 0\nğ›¼ğ¸ğ‘†\nES Learning Rate\n0.001\nğ‘›\nNumber of ES Offspring\n60\nğ›¼ğ‘‡ğ·3\nTD3 Learning Rate\n0.0003\nğœ–\nExploratory Noise Added To TD3\nSampled from N (0 0.1)\nğœ\nTD3 soft update hyperparameter\n0.005\nK\nMin Number of TD3 Exploratory Timesteps Before Learning Starts\n25000\n(a,b,c)\nSampling Ratio of Good, Bad and Noisy DataPoints\n(0.5,0.2,0.3)\nğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘\nThreshold for trajectory being appended to \"good\" buffer\n0.9x(highest recorded fitness)\nM\nNumber of TD3 Frames Between ES Iterations\n100000\ng\nNumber of ES Generations Between TD3 Iterations\n50\non MuJoCo enviornments. ES-TD3Buffers provides a improvement\nof 184% on Swimmer, a 6.6% improvement on Ant and provides\ncompetitive performance on HalfCheetah when compared directly\nto CEM-RL. Our algorithm also shows that ES with TD3 works\nbetter in this compounded framework than they do separately, as\nour algorithm outperforms TD3 on 3 of the 4 environments tested,\nwith reasoning given in section 4.3 for the failure to improve on\nWalker. This is in thanks due to the RL agent helping ES escape\npoor local optima early in the training, which ultimately leads to\nbetter training data for the RL agent.\nACKNOWLEDGMENTS\nThis work was conducted with the financial support of the Sci-\nence Foundation Ireland Centre for Research Training in Artificial\nIntelligence under Grant No. 18/CRT/6223\nREFERENCES\n[1] Hui Bai, Ruimin Shen, Yue Lin, Botian Xu, and Ran Cheng. 2022. Lamarckian Plat-\nform: Pushing the Boundaries of Evolutionary Reinforcement Learning towards\nAsynchronous Commercial Games. IEEE Transactions on Games (2022).\n[2] Cristian Bodnar, Ben Day, and Pietro LiÃ³. 2020. Proximal distilled evolution-\nary reinforcement learning. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 3283â€“3290.\n[3] Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth\nStanley, and Jeff Clune. 2018. Improving exploration in evolution strategies for\ndeep reinforcement learning via a population of novelty-seeking agents. Advances\nin neural information processing systems 31.\n[4] MaÃ«l Franceschetti, Coline Lacoux, Ryan Ohouens, and Olivier Sigaud. 2022. Mak-\ning Reinforcement Learning Work on Swimmer. arXiv preprint arXiv:2208.07587\n(2022).\n[5] Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function ap-\nproximation error in actor-critic methods. In International conference on machine\nlearning. PMLR, 1587â€“1596.\n[6] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning with a\nstochastic actor. In International conference on machine learning. PMLR, 1861â€“\n1870.\n[7] Ming Jiang and Li Zhang. 2021. An Interactive Evolution Strategy based Deep\nConvolutional Generative Adversarial Network for 2D Video Game Level Pro-\ncedural Content Generation. In 2021 International Joint Conference on Neural\nNetworks (IJCNN). IEEE, 1â€“6.\n[8] Whiyoung Jung, Giseung Park, and Youngchul Sung. 2020. Population-guided\nparallel policy search for reinforcement learning. arXiv preprint arXiv:2001.02907\n(2020).\n[9] Shauharda Khadka and Kagan Tumer. 2018. Evolution-guided policy gradient in\nreinforcement learning. Advances in Neural Information Processing Systems 31\n(2018).\n[10] Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, and In So Kweon. 2020. An\nefficient asynchronous method for integrating evolutionary and gradient-based\npolicy search. Advances in Neural Information Processing Systems 33 (2020),\n10124â€“10135.\n[11] Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O Stanley. 2018. ES is more than\njust a traditional finite-difference approximator. In Proceedings of the Genetic and\nEvolutionary Computation Conference. 450â€“457.\n[12] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with\ndeep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).\n[13] Kexuan Lv, Xiaofei Pei, Ci Chen, and Jie Xu. 2022. A Safe and Efficient Lane\nChange Decision-Making Strategy of Autonomous Driving Based on Deep Rein-\nforcement Learning. Mathematics 10, 9 (2022), 1551.\n[14] Tom M Mitchell. 1997. Machine learning. Vol. 1. McGraw-hill New York.\n[15] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. 2015. Human-level control through deep reinforcement learning.\nnature 518, 7540 (2015), 529â€“533.\n[16] AloÃ¯s Pourchot and Olivier Sigaud. 2018. CEM-RL: Combining evolutionary and\ngradient-based methods for policy search. arXiv preprint arXiv:1810.01222 (2018).\n[17] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild!:\nA lock-free approach to parallelizing stochastic gradient descent. Advances in\nneural information processing systems 24 (2011).\n[18] Marzieh Sadat Esmaeeli and Hamed Malek. 2022. Evolutionary Deep Reinforce-\nment Learning Using Elite Buffer: A Novel Approach Towards DRL Combined\nwith EA in Continuous Control Tasks. arXiv e-prints (2022), arXivâ€“2209.\n[19] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017.\nEvolution strategies as a scalable alternative to reinforcement learning. arXiv\npreprint arXiv:1703.03864 (2017).\n[20] Eyal Segal and Moshe Sipper. 2022. Adaptive Combination of a Genetic Algorithm\nand Novelty Search for Deep Neuroevolution. arXiv preprint arXiv:2209.03618\n(2022).\n[21] Longxiang Shi, Shijian Li, Longbing Cao, Long Yang, Gang Zheng, and Gang\nPan. 2019. FiDi-RL: Incorporating Deep Reinforcement Learning with Finite-\nDifference Policy Search for Efficient Learning of Continuous Control. arXiv\npreprint arXiv:1907.00526 (2019).\n[22] Olivier Sigaud. 2022. Combining Evolution and Deep Reinforcement Learning\nfor Policy Search: a Survey. ACM Transactions on Evolutionary Learning (2022).\n[23] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural\nnetworks and tree search. nature 529, 7587 (2016), 484â€“489.\n[24] Kenneth O Stanley and Risto Miikkulainen. 2002. Evolving neural networks\nthrough augmenting topologies. Evolutionary computation 10, 2 (2002), 99â€“127.\n[25] Ryan Sullivan, Justin K Terry, Benjamin Black, and John P Dickerson. 2022. Cliff\nDiving: Exploring Reward Surfaces in Reinforcement Learning Environments.\narXiv preprint arXiv:2205.07015 (2022).\n[26] Karush Suri, Xiao Qi Shi, Konstantinos N Plataniotis, and Yuri A Lawryshyn.\n2020. Maximum mutation reinforcement learning for scalable control. arXiv\npreprint arXiv:2007.13690 (2020).\n[27] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Intro-\nduction (second ed.). The MIT Press. http://incompleteideas.net/book/the-book-\n2nd.html\n[28] Gerald Tesauro et al. 1995. Temporal difference learning and TD-Gammon.\nCommun. ACM 38, 3 (1995), 58â€“68.\n[29] Xiaoqiang Wu, Qingling Zhu, Qiuzhen Lin, Jianqiang Li, Jianyong Chen, and\nZhong Ming. 2022. An Efficient Evaluation Mechanism for Evolutionary Re-\ninforcement Learning. In International Conference on Intelligent Computing.\nSpringer, 41â€“50.\n[30] Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua Lu, and Chengqi\nZhang. 2020. Cooperative heterogeneous deep reinforcement learning. Advances\nin Neural Information Processing Systems 33 (2020), 17455â€“17465.\n",
  "categories": [
    "cs.NE",
    "cs.AI"
  ],
  "published": "2023-06-20",
  "updated": "2023-06-20"
}