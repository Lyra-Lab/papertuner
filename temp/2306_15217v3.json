{
  "id": "http://arxiv.org/abs/2306.15217v3",
  "title": "Unsupervised Episode Generation for Graph Meta-learning",
  "authors": [
    "Jihyeong Jung",
    "Sangwoo Seo",
    "Sungwon Kim",
    "Chanyoung Park"
  ],
  "abstract": "We propose Unsupervised Episode Generation method called Neighbors as Queries\n(NaQ) to solve the Few-Shot Node-Classification (FSNC) task by unsupervised\nGraph Meta-learning. Doing so enables full utilization of the information of\nall nodes in a graph, which is not possible in current supervised meta-learning\nmethods for FSNC due to the label-scarcity problem. In addition, unlike\nunsupervised Graph Contrastive Learning (GCL) methods that overlook the\ndownstream task to be solved at the training phase resulting in vulnerability\nto class imbalance of a graph, we adopt the episodic learning framework that\nallows the model to be aware of the downstream task format, i.e., FSNC. The\nproposed NaQ is a simple but effective unsupervised episode generation method\nthat randomly samples nodes from a graph to make a support set, followed by\nsimilarity-based sampling of nodes to make the corresponding query set. Since\nNaQ is model-agnostic, any existing supervised graph meta-learning methods can\nbe trained in an unsupervised manner, while not sacrificing much of their\nperformance or sometimes even improving them. Extensive experimental results\ndemonstrate the effectiveness of our proposed unsupervised episode generation\nmethod for graph meta-learning towards the FSNC task. Our code is available at:\nhttps://github.com/JhngJng/NaQ-PyTorch.",
  "text": "Unsupervised Episode Generation for Graph Meta-learning\nJihyeong Jung 1 Sangwoo Seo 1 Sungwon Kim 2 Chanyoung Park 1 2\nAbstract\nWe propose Unsupervised Episode Generation\nmethod called Neighbors as Queries (NAQ) to\nsolve the Few-Shot Node-Classification (FSNC)\ntask by unsupervised Graph Meta-learning. Do-\ning so enables full utilization of the information of\nall nodes in a graph, which is not possible in cur-\nrent supervised meta-learning methods for FSNC\ndue to the label-scarcity problem. In addition,\nunlike unsupervised Graph Contrastive Learning\n(GCL) methods that overlook the downstream task\nto be solved at the training phase resulting in vul-\nnerability to class imbalance of a graph, we adopt\nthe episodic learning framework that allows the\nmodel to be aware of the downstream task format,\ni.e., FSNC. The proposed NAQ is a simple but\neffective unsupervised episode generation method\nthat randomly samples nodes from a graph to\nmake a support set, followed by similarity-based\nsampling of nodes to make the corresponding\nquery set. Since NAQ is model-agnostic, any\nexisting supervised graph meta-learning meth-\nods can be trained in an unsupervised manner,\nwhile not sacrificing much of their performance\nor sometimes even improving them. Extensive\nexperimental results demonstrate the effective-\nness of our proposed unsupervised episode gen-\neration method for graph meta-learning towards\nthe FSNC task. Our code is available at: https:\n//github.com/JhngJng/NaQ-PyTorch.\n1. Introduction\nGraph-structured data are useful and widely applicable in\nthe real-world, thanks to their capability of modeling com-\nplex relationships between objects such as user-user rela-\ntionships in social networks and product networks, etc. To\nhandle tasks such as node classification on graph-structured\n1Department of Industrial & Systems Engineering, KAIST\n2Graduate School of Data Science, KAIST. Correspondence to:\nChanyoung Park <cy.park@kaist.ac.kr>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\ndata, Graph Neural Networks (GNNs) are widely used and\nhave shown remarkable performance (Kipf & Welling, 2017;\nVeliˇckovi´c et al., 2018). However, it is well known that\nGNNs suffer from poor generalization when only a small\nnumber of labeled samples are provided (Zhou et al., 2019;\nDing et al., 2020; Wang et al., 2022b).\nTo mitigate such issues inherent in the ordinary deep neural\nnetworks, few-shot learning methods have emerged, and the\ndominant paradigm was applying meta-learning algorithms\nsuch as MAML (Finn et al., 2017) and ProtoNet (Snell et al.,\n2017), which are based on the episodic learning frame-\nwork (Vinyals et al., 2016). Inspired by these methods,\nrecent studies proposed graph meta-learning methods (Zhou\net al., 2019; Ding et al., 2020; Huang & Zitnik, 2020; Wang\net al., 2022b) to solve the Few-Shot Node Classification\n(FSNC) task on graphs by also leveraging the episodic learn-\ning framework, which is the main focus of this study.\nDespite their effectiveness, existing supervised graph meta-\nlearning methods require abundant labeled samples from di-\nverse base classes for the training. As shown in Figure 1(a),\nsuch label-scarcity causes a severe performance drop of\nrepresentative methods (i.e., TENT (Wang et al., 2022b), G-\nMeta (Huang & Zitnik, 2020), ProtoNet (Snell et al., 2017),\nand MAML (Finn et al., 2017)) in FSNC. However, gath-\nering enough labeled data and diverse classes may not be\npossible, and is costly in reality. More importantly, as these\nmethods depend on a few labeled nodes from base classes,\nwhile not fully utilizing all nodes in the graph, they are\nalso vulnerable to noisy labels in base classes (Figure 1(b)).\nIn this respect, unsupervised methods are indispensable to\nfundamentally address the label-dependence problem of\nexisting supervised graph meta-learning methods.\nMost recently, TLP (Tan et al., 2022) empirically demon-\nstrated that a simple linear probing with node embeddings\npre-trained by Graph Contrastive Learning (GCL) methods\noutperforms existing supervised graph meta-learning meth-\nods in FSNC. This is because GCL methods tend to generate\ngeneric node embeddings, since all nodes in a graph are in-\nvolved in the training.\nHowever, despite the effectiveness of generic node embed-\ndings, we argue that they are vulnerable to class imbalance\nin the graph, which might lead to a significant performance\ndrop due to the lack of model generalizability resulting from\n1\narXiv:2306.15217v3  [cs.LG]  21 May 2024\nUnsupervised Episode Generation for Graph Meta-learning\nTENT\nG-Meta\nProtoNet\nMAML\n60\n65\n70\n75\nAmazon-Electronics\n Class% / Label%\n100%/100%\n80%/80%\n50%/50%\n20%/20%\n(a) Impact of the label-scarcity\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nLabel noise ratio p\n35\n45\n55\n65\n75\nAccuracy (%)\nAmazon-Electronics\nMAML\nProtoNet\nTENT\nG-Meta\n(b) Impact of the label noise\nProtoNet\nNaQ-Feat\n(Ours)\nBGRL\nSUGRL\nAFGRL\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\nMeta-\nLearning\nGCL\nAmazon-Electronics\nSettings\nOriginal\nPareto\nExtreme\n(c) Impact of the class imbalance\nFigure 1. (a): Impact of the label-scarcity on supervised graph meta-learning methods (‘Class%’: a rate of available base classes during\ntraining, ‘Label%’: a rate of available labeled samples for each class). (b): Impact of the (randomly injected) label noise p on supervised\ngraph meta-learning methods. (c): Impact of the class imbalance (‘Pareto’ setting: we kept nodes for top-20% head classes, while keeping\nonly 10 nodes for remaining classes; ‘Extreme’ setting: the only difference from the ‘Pareto’ setting is that we kept nodes only for top-5\nhead classes instead of top-20% classes). (5-way 1-shot)\nthe discrepancy in the objective between pre-training and\nfine-tuning (in downstream task) phase (Lu et al., 2021). If\nthe given graph mainly consists of nodes from the majority\nclasses, GCL methods have difficulty in learning embed-\ndings of nodes from the minority classes, which results in\npoor FSNC performance on such minority classes. On the\nother hand, as each episode in the episodic learning frame-\nwork provides the GNN encoder with the information about\nthe downstream task format (i.e., FSNC), meta-learning\nmethods are rather more robust to the class imbalance that\nmay exist in the given graph1. To corroborate our argu-\nment, we modified the original graph to simulate two class\nimbalance settings (i.e., ‘Pareto’ and ‘Extreme’), and eval-\nuated GCL methods (i.e., BGRL (Thakoor et al., 2022),\nSUGRL (Mo et al., 2022), AFGRL (Lee et al., 2022b))\nand meta-learning methods (i.e., ProtoNet and NAQ-FEAT\n(ours)) on the FSNC task (See Figure 1(c)). As expected,\nthe performance deterioration of GCL methods was more\nsevere than meta-learning methods under class imbalance\nsettings.\nTherefore, we argue that the FSNC performance can be\nfurther enhanced by unsupervised Graph Meta-learning,\nwhich can achieve the best of both worlds: 1) GCL that\nfully utilizes all nodes in a graph in an unsupervised manner,\nand 2) Meta-learning whose episodic learning framework is\naware of the downstream task format (i.e., FSNC).\nIn this work, we propose a simple yet effective unsupervised\nepisode generation method called Neighbors as Queries\n(NAQ), which enables unsupervised graph meta-learning,\nto benefit from the generalization ability of meta-learning\nmethods for the FSNC task, while fully utilizing all nodes\nin a graph. The main idea is to construct a support set by\nrandomly choosing nodes from the entire graph, and gen-\nerate a corresponding query set via sampling similar nodes\nbased on pre-calculated node-node similarity. It is important\n1Please refer to Section A.2.1 for more detail regarding how the\nepisodic learning allows the model to be robust to class imbalance.\nto note that our unsupervised episode generation method is\nmodel-agnostic, i.e., NAQ can be used to train any existing\nsupervised graph meta-learning methods in an unsupervised\nmanner directly or only with minor modifications.\nTo sum up, our contributions are summarized as follows:\n1. We present an unsupervised episode generation method,\ncalled NAQ, designed to solve the FSNC task via unsu-\npervised graph meta-learning. To our best knowledge,\nthis is the first study that focuses on the unsupervised\nepisode generation of graph meta-learning framework.\n2. NAQ is model-agnostic; that is, it can be used to train\nany existing supervised graph meta-learning methods in\nan unsupervised manner, while not sacrificing much of\ntheir performance or sometimes even improving them,\nwithout using any labeled nodes.\n3. Extensive experimental results demonstrate the effective-\nness of NAQ in the FSNC task and highlight the potential\nof the unsupervised graph meta-learning framework.\n2. Preliminaries\n2.1. Problem Statement\nLet G = (V, E, X) be a graph, where V, E ⊂V ×\nV, X ∈R|V|×d are a set of nodes, a set of edges, and a d-\ndimensional node feature matrix, respectively. We also use\nX to denote a set of node features, i.e., X = {xv : v ∈V}.\nLet C be a set of total node classes. Here, we denote the\nbase classes, a set of node classes that can be utilized during\ntraining, as Cb, and denote the target classes, a set of node\nclasses that we aim to predict in downstream tasks given a\nfew labeled samples, as Ct. Note that Cb ∪Ct = C and\nCb ∩Ct = ∅, and the target classes Ct are unknown during\ntraining. In common few-shot learning settings, the number\nof labeled nodes from classes of Cb is sufficient, while we\nonly have a few labeled nodes from classes of Ct in down-\nstream tasks. Now we formulate the ordinary supervised\nfew-shot node classification (FSNC) problem as follows:\n2\nUnsupervised Episode Generation for Graph Meta-learning\nDefinition 2.1 (Supervised FSNC). Given a graph G =\n(V, E, X), labeled data (XCb, YCb) and a model fθ trained\non (XCb, YCb), the goal of supervised FSNC is making\npredictions for xq ∈XCt (i.e. query set) based on a few\nlabeled samples (xs, ys) ∈(XCt, YCt) (i.e., support set)\nduring the testing phase.\nBased on this problem formulation, we can formulate the\nunsupervised FSNC problem as below. The only difference\nis that labeled nodes are not available during training.\nDefinition 2.2 (Unsupervised FSNC). Given a graph G =\n(V, E, X), unlabeled data X = XCb ∪XCt, and a model\nfθ trained on X, the goal of unsupervised FSNC is making\npredictions for xq ∈XCt (i.e., query set) based on a few\nlabeled samples (xs, ys) ∈(XCt, YCt) (i.e., support set)\nduring the testing phase.\nOverall, the goal of FSNC is to adapt well to unseen target\nclasses Ct only using a few labeled samples from Ct after\ntraining a model fθ on training data. In this work, we study\nhow to facilitate unsupervised Graph Meta-learning to solve\nthe FSNC task. More formally, we consider solving a N-\nway K-shot FSNC task (Vinyals et al., 2016), where N is\nthe number of distinct target classes and K is the number\nof labeled samples in a support set. Moreover, there are Q\nquery samples to be classified in each downstream task.\n2.2. Episodic Learning Framework\nWe follow the episodic training framework (Vinyals et al.,\n2016) that is formally defined as follows:\nDefinition 2.3 (Episodic Learning). Episodic learning is a\nlearning framework that utilizes a bundle of tasks {Tt}T\nt=1,\nwhere Tt = (STt, QTt), STt = {(xspt\nt,i , yspt\nt,i )}N×K\ni=1\nand\nQTt = {(xqry\nt,i , yqry\nt,i )}N×Q\ni=1 , instead of commonly used\nmini-batches in the stochastic optimization.\nBy mimicking the ‘format’ of the downstream task (i.e.,\nFSNC), the episodic learning allows the model to be aware\nof the task to be solved in the testing phase. Note that exist-\ning supervised meta-learning methods require a large num-\nber of labeled samples in the training set (XCb, YCb) and\na sufficient number of base classes |Cb| (i.e., diverse base\nclasses) to generate informative training episodes. However,\ngathering enough labeled data and diverse classes may not\nbe possible and is usually costly in the real world. As a\nresult, supervised methods fall short of utilizing all nodes in\nthe graph as they rely on a few labeled nodes, and thus lack\ngeneralizability.\nTherefore, we propose unsupervised episode generation\nmethods not only to tackle the label-scarcity problem caus-\ning a limited utilization of nodes in the graph, but also to\nbenefit from the episodic learning framework for down-\nstream task-aware learning of node embeddings, thereby\nRaw Feature-based Node-Node Similarity Calculation\nSimilarity-based Query Generation\nHigher\nLower\n𝑆𝑆𝒯𝒯𝑡𝑡\n𝑄𝑄𝒯𝒯𝑡𝑡\n𝑠𝑠1\n𝑠𝑠3\n𝑠𝑠2\n𝑠𝑠4\n𝑠𝑠5\n𝑞𝑞1,1\n𝑞𝑞3,1\n𝑞𝑞2,1\n𝑞𝑞4,1 𝑞𝑞5,1\n𝑞𝑞1,2\n𝑞𝑞3,2\n𝑞𝑞2,2\n𝑞𝑞4,2 𝑞𝑞5,1\n𝑞𝑞1,𝑄𝑄\n𝑞𝑞3,𝑄𝑄\n𝑞𝑞2,𝑄𝑄\n𝑞𝑞4,𝑄𝑄𝑞𝑞5,𝑄𝑄\nGenerated Task 𝒯𝒯𝑡𝑡\nSimilarity\nRandom Sampling Initial Support set 𝑆𝑆𝒯𝒯𝑡𝑡\nRandomly sampled 𝑇𝑇sets of 𝑁𝑁nodes\n…\n𝑣𝑣1\n𝑣𝑣2\n𝑣𝑣|𝒱𝒱|\n𝑣𝑣1\n1.0\n0.3\n⋯\n0.7\n𝑣𝑣2\n0.3\n1.0\n⋯\n0.6\n⋮\n⋮\n⋱\n⋮\n𝑣𝑣|𝒱𝒱|\n0.7\n0.6\n⋯\n1.0\nNode-Node Similarity Matrix 𝐒𝐒\nRaw Node Feature 𝑋𝑋\n𝑣𝑣1\n𝑣𝑣2\n𝑣𝑣3\n𝑣𝑣|𝒱𝒱|\n…\n𝑆𝑆𝒯𝒯1\n𝑆𝑆𝒯𝒯𝑇𝑇\nAssign distinct pseudo-labels\nFigure 2. Overview of the NAQ-FEAT.\nbeing robust to a class imbalance in a graph.\n3. Proposed Method\n3.1. Motivation: A Closer Look at Training Episodes\nIn the episodic learning framework, there are two essential\ncomponents: 1) support set that provides basic information\nabout the task to be solved, and 2) query set that enables the\nmodel to understand about how to solve the given task. For\nthis reason, the query set should share similar semantics\nwith the support set. Motivated by this characteristic of\nepisodic learning, we consider the similarity condition as\nthe key to our proposed query generation process. Note that\nin the ordinary supervised setting, the similarity condition is\neasily achievable, since labels of the support set and query\nset are known, and thus can be sampled from the same class.\nHowever, as our goal is to generate training episodes in an\nunsupervised manner, how to sample a query set that shares\nsimilar semantics with each support set is non-trivial.\n3.2. NAQ: Neighbors as Queries\nIn this work, we propose a simple yet effective query gener-\nation method, called Neighbors as Queries (NAQ), which\nleverages raw feature-level similar nodes as queries. The\noverview of NAQ can be found in Figure 2.\nSupport set generation.\nTo generate training episodes\n{Tt}T\nt=1, we start by randomly sampling T sets of N nodes\nfrom the entire graph for the support set generation. Next,\nwe assign pseudo-labels yt,i to each node xt,i ∈Tt, i.e.,\nSTt = {(xt,i, yt,i) | xt,i ∈V}N×K\ni=1\n. Note that we only\ngenerate 1-shot support set (i.e., K = 1) regardless of the\ndownstream task setting, to assure that randomly sampled N\nsupport set nodes (corresponding to ‘N-way’) are as much\ndistinguishable from one another as possible.\nQuery set generation. Then, we generate a corresponding\nquery set QTt with Top-Q similar nodes of each node xt,i in\nSTt based on a pre-calculated node-node similarity matrix\nS, and give them the same pseudo-label yt,i. Formally, we\ncan express this query generation process as follows:\nQTt =\n[\n(xt,i,yt,i)∈STt\nTop(Sxt,i, Q)\n(1)\n3\nUnsupervised Episode Generation for Graph Meta-learning\nwhere Sxt,i denotes a row of the similarity matrix S cor-\nresponding to the node xt,i, and Top(Sxt,i, Q) indicates a\nset of Q nodes corresponding to Q largest entries in Sxt,i\nexcluding xt,i itself.\nSimilarity Metric. For sampling ‘similar’ nodes to be used\nas queries, we used cosine similarity for node features such\nas bag-of-words, and Euclidean distance for the features\nsuch as word embeddings. Refer to Section A.3 in the\nAppendix for further discussions on the similarity metric.\n3.2.1. AN EXTENSION TO NAQ: NAQ-DIFF\nSince NAQ described above solely relies on the raw node\nfeature X, the structural information that is inherent in\ngraphs is overlooked, which plays an important role depend-\ning on the target domain. For example, in citation networks,\nsince the citation relationship between papers implies that\nthese papers usually share similar semantics (i.e., related\npaper topics), they have similar features even if their class\nlabels are different. Hence, considering structurally similar\nnodes as queries can be more beneficial than solely relying\non the feature-level similar nodes in such cases.\nHence, we present a variant of NAQ, called NAQ-DIFF,\nwhich utilizes structurally similar nodes found by gener-\nalized graph diffusion (Gasteiger et al., 2019) as queries.\nSpecifically, NAQ-DIFF leverages diffusion matrix S =\nΣ∞\nk=0θkTk as node-node similarity matrix, with weighting\ncoefficients θk, and the generalized transition matrix T. As\nedge weights of the diffusion matrix S can be interpreted\nas structural closeness, we can sample similar nodes of\neach support set node from S. It is important to note that\ncomputing the diffusion matrix does not require additional\ncomputation during training and can be readily calculated\nbefore the model training. The overview of NAQ-DIFF can\nbe found in Figure 10 in the Appendix, and detailed settings\nfor NAQ-DIFF can be found in Section A.4. Hereafter, we\ncall the former version of NAQ that is based on the raw\nfeatures as NAQ-FEAT, and the latter version that is based\non the graph structural information as NAQ-DIFF.\n3.3. Model Training with Episodes from NAQ\nIn this section, we explain how to train existing meta-\nlearning models with episodes generated by NAQ. Let\nTt = (STt, QTt) be a generated episode and Meta(Tt; θ) be\nany of existing graph meta-learning methods (e.g., MAML,\nProtoNet, G-Meta, etc.) with parameter θ. For simplicity\nof explanation, we used the same notation here even for\nmethods that use meta-batches like MAML. Regardless of\nwhether Tt is generated from NAQ or an ordinary supervised\nepisode generation, it follows the common format of Tt =\n(STt, QTt), where STt = {(xspt\nt,i , yspt\nt,i )}N×K\ni=1\nand QTt =\n{(xqry\nt,i , yqry\nt,i )}N×Q\ni=1 . That is, the only difference is whether\nyspt\nt,i and yqry\nt,i are annotated based on the ground-truth la-\nbel (supervised) or a psuedo-label (NAQ). Hence, any of\nMeta(Tt; θ) can be trained in the same way in an ‘unsu-\npervised manner with NAQ’ as ordinary supervised meta-\nlearning methods. The details are presented in Algorithm 1.\nAlgorithm 1 Training Meta-learner Meta( · ; θ) with NAQ\ninput Bundle of training episodes {Tt}T\nt=1, Graph Meta-\nlearner Meta( · ; θ), learning rate η.\nRandomly initialize the model parameter θ\nfor t = 1, · · · , T do\nStep 1: Calculate loss L by Meta(Tt; θ)\nStep 2: Update θ ←θ −η∇θL\nend for\noutput Meta(Tt; θ)\nRemark. Supervised TENT (Wang et al., 2022b) addi-\ntionally computes cross-entropy loss LCE over the entire\nlabeled data (XCb, YCb) in Step 1 of Algorithm 1. There-\nfore, when we train TENT with our NAQ, LCE is calculated\nover a single training episode.\nIt is important to note that since NAQ generates training\nepisodes based on all nodes in a graph, it enables existing\ngraph meta-learning methods to fully utilize all nodes in a\ngraph, while the supervised episode generation fails to do so\nas it depends on a few labeled nodes from base classes. The\ndetailed model training example in case of ProtoNet (Snell\net al., 2017) can be found in Section A.7 in the Appendix.\n3.4. Theoretical Insights\nIn this section, we provide some insights on conditions that\nenable NAQ to work within the episodic learning framework\nto justify our motivation of utilizing similar nodes as queries\ndescribed in Section 3.1. Specifically, we investigate the\nlearning behavior of MAML (Finn et al., 2017), which is\none of the most widely adopted meta-learning methods in\nthe perspective of ‘generalization error’ for a single episode\nduring the training phase. Since each of the existing graph\nmeta-learning methods has its own sophisticated architec-\nture, we only consider MAML here. The formal definition\nof the expected generalization error is as follows (Gareth\net al., 2013; Hastie et al., 2009).\nDefinition 3.1. Let S, Q, fS, f be a given training set, test\nset, an encoder trained on S, and the unknown perfect esti-\nmation, respectively. With an error measure L, for a given\npoint (x′, y′) ∈Q, an expected generalization error is de-\nfined as E[L\n\u0000y′, fS(x′)\n\u0001\n].\nBy assuming that y = f(x) + ϵ holds for an arbitrary input-\noutput pair (x, y) (E[ϵ] = 0, Var(ϵ) = σ2 < ∞) and an er-\nror measure L is the mean squared error, we can decompose\nexpected generalization error in Def. 3.1 as follows (Gareth\n4\nUnsupervised Episode Generation for Graph Meta-learning\nTable 1. Overall averaged FSNC accuracy (%) with 95% confidence intervals on product networks (Full ver. available at: Table 15)\nDataset\nAmazon-Clothing\nAmazon- Electronics\nSetting\n5 way\n10 way\nAvg.\nRank\n5 way\n10 way\n20 way\nAvg.\nRank\nBaselines\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\nMAML (Sup.)\n76.13±1.17\n84.28±0.87\n63.77±0.83\n76.95±0.65\n10.25\n65.58±1.26\n78.55±0.96\n57.31±0.87\n67.56±0.73\n46.37±0.61\n60.04±0.52\n9.33\nProtoNet (Sup.)\n75.52±1.12\n89.76±0.70\n65.50±0.82\n82.23±0.62\n7.25\n69.48±1.22\n84.81±0.82\n57.67±0.85\n75.79±0.67\n48.41±0.57\n67.31±0.47\n5.83\nTENT (Sup.)\n79.46±1.10\n89.61±0.70\n69.72±0.80\n84.74±0.59\n5.25\n72.31±1.14\n85.25±0.81\n62.13±0.83\n77.32±0.67\n52.45±0.60\n69.39±0.50\n4.00\nG-Meta (Sup.)\n78.67±1.05\n88.79±0.76\n65.30±0.79\n80.97±0.59\n7.75\n72.26±1.16\n84.44±0.83\n61.32±0.86\n74.92±0.71\n50.39±0.59\n65.73±0.48\n5.67\nGLITTER (Sup.)\n75.73±1.10\n89.18±0.74\n64.30±0.79\n77.73±0.68\n9.00\n66.91±1.22\n82.59±0.83\n57.12±0.88\n76.26±0.67\n49.23±0.57\n61.77±0.52\n7.00\nCOSMIC (Sup.)\n82.24±0.99\n91.22±0.73\n74.44±0.75\n81.58±0.63\n3.75\n72.61±1.05\n86.92±0.76\n65.24±0.82\n78.00±0.64\n58.71±0.57\n70.29±0.44\n3.00\nTLP-BGRL\n81.42±1.05\n90.53±0.71\n72.05±0.86\n83.64±0.63\n4.25\n64.20±1.10\n81.72±0.85\n53.16±0.82\n73.70±0.66\n44.57±0.54\n65.13±0.47\n8.67\nTLP-SUGRL\n63.32±1.19\n86.35±0.78\n54.81±0.77\n73.10±0.63\n11.50\n54.76±1.06\n78.12±0.92\n46.51±0.80\n68.41±0.71\n36.08±0.52\n57.78±0.49\n11.67\nTLP-AFGRL\n78.12±1.13\n89.82±0.73\n71.12±0.81\n83.88±0.63\n5.25\n59.07±1.07\n81.15±0.85\n50.71±0.85\n73.87±0.66\n43.10±0.56\n65.44±0.48\n9.00\nVNT\n65.09±1.23\n85.86±0.76\n62.43±0.81\n80.87±0.63\n10.50\n56.69±1.22\n78.02±0.97\n49.98±0.83\n70.51±0.73\n42.10±0.53\n60.99±0.50\n10.83\nNAQ-FEAT-Best (Ours)\n86.58±0.96\n92.27±0.67\n79.55±0.78\n86.10±0.60\n1.00\n76.46±1.11\n88.72±0.73\n69.59±0.86\n81.44±0.61\n61.05±0.59\n74.60±0.47\n1.00\nNAQ-DIFF-Best (Ours)\n84.40±1.01\n91.72±0.69\n73.39±0.79\n84.82±0.58\n2.25\n74.16±1.08\n87.09±0.75\n65.95±0.81\n79.13±0.60\n60.40±0.59\n73.75±0.42\n2.00\net al., 2013; Hastie et al., 2009):\nE[L\n\u0000y′, fS(x′)\n\u0001\n] =\n\u0000E[fS(x′)] −f(x′)\n\u00012\n+\n\u0000E\n\u0002\nfS(x′)2\u0003\n−E[fS(x′)]2\u0001\n+ σ2.\n(2)\nLet us consider the training process of MAML with an en-\ncoder fθ and a training episode T = (ST , QT ), where\nST\n= {(xspt\ni\n, yspt\ni\n)}N×K\ni=1\nand QT are the N-way K-\nshot support set and the query set, respectively. During\nthe inner-loop optimization, MAML produces fθ′, where\nθ′ = argminθ\nP\n(xspt,yspt)∈ST L\n\u0000yspt, fθ(xspt)\n\u0001\n.\nIf we regard the inner-loop optimization of MAML as a\ntraining process with training set S = ST , the outer-loop\noptimization (i.e., meta-optimization) as a testing process\nwith test set Q = QT , and the trained encoder fS = fST =\nfθ′, we can interpret that the meta-optimization actually\nreduces the generalization error in Eq. 2 over the query set\nQT with encoder fθ′ (Khodadadeh et al., 2019). With this\ninterpretation, we can re-write Eq. 2 as follows:\nE[L\n\u0000yqry, fθ′(xqry)\n\u0001\n] =\n\u0000E[fθ′(xqry)] −fT (xqry)\n\u00012\n+\n\u0000E\n\u0002\nfθ′(xqry)2\u0003\n−E[fθ′(xqry)]2\u0001\n+ σ2,\n(3)\nwhere fT is the unknown perfect estimation for T . With-\nout loss of generality, we considered a single query\n(xqry, yqry) ∈QT to derive Eq. 3. As Eq. 3 is used as\na loss function, an accurate calculation of Eq. 3 is essential\nfor a better model training on T (Khodadadeh et al., 2019).\nRemark. Let s := (xspt, yspt) ∈ST be a specific corre-\nsponding support set sample of the query q := (xqry, yqry)\nabove. Let ˜yspt, ˜yqry be the true labels of s, q, respec-\ntively. Note that the same new labels (i.e., yspt, yqry s.t.\nyspt=yqry) are assigned to each of xspt, xqry during the\ntraining episode generation (regardless of whether it is super-\nvised or not), to perform classification of N classes instead\nof classifying |C| classes (i.e., total number of classes in the\nentire dataset) in the training phase of the episodic learning\nframework. To get an accurate computation of Eq. 3, it is\nessential to assure that ˜yspt = ˜yqry holds. Otherwise, we\nhave yqry = fT (xqry) + ϵ + δ, where δ is an error resulting\nfrom ˜yspt ̸= ˜yqry, which may lead to a suboptimal solution\nwhen training with loss defined by Eq. 3.\nUnlike the ordinary supervised episode generation in which\ncase δ = 0 holds as condition that ˜yspt = ˜yqry is naturally\nsatisfied, our NAQ cannot guarantee δ = 0 since no label\ninformation is given (i.e., ˜yspt, ˜yqry are both unknown) dur-\ning its episode generation phase. Hence, we argue that it is\ncrucial to discover class-level similar query qNAQ for each\nsupport set sample sNAQ = (xspt\nNAQ, yspt\nNAQ) ∈ST 2 during the\nquery generation process of NAQ. If s and q are class-level\nsimilar, i.e., the difference between their corresponding true\nlabels ˜yspt\nNAQ, ˜yqry\nNAQ are small enough, we would have |δ| < ξ\nfor some small enough ξ > 0 so that we can successfully\ntrain encoder fθ.\nIn summary, the above analysis explains that discovering a\nquery that is class-level similar enough to a given support\nset sample is crucial for minimizing the training loss (i.e.,\nthe generalization error defined in Eq. 3), which eventually\nyields a better fθ. In this regard, NAQ works well within\nthe episodic learning framework, since NAQ generates\nclass-level similar query nodes using node-node similarity\ndefined based on the raw node feature (i.e., NAQ-FEAT) and\ngraph structural information (i.e., NAQ-DIFF).\nFurther discussions on why class-level similarity is sufficient\nfor unsupervised episode generation (Section A.1.1) and an\nempirical result that our NAQ can find class-level similar\nqueries (Section A.1.2) are provided in the Appendix.\n4. Experiments\nEvaluation Datasets.\nWe use five benchmark datasets\nthat are widely used in FSNC to comprehensively evalu-\nate the performance of our unsupervised episode genera-\n2Here, we use ST to denote the support set generated by NAQ.\nFor details, see ‘Support set generation’ process in Section 3.2.\n5\nUnsupervised Episode Generation for Graph Meta-learning\nTable 2. Overall averaged FSNC accuracy (%) with 95% confidence intervals on citation networks (Full ver. available at: Table 16, OOT:\nOut Of Time, which means that the training was not finished in 24 hours, OOM: Out Of Memory on NVIDIA RTX A6000)\nDataset\nCora-full\nDBLP\nSetting\n5 way\n10 way\n20 way\nAvg.\nRank\n5 way\n10 way\n20 way\nAvg.\nRank\nBaselines\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\nMAML (Sup.)\n59.28±1.21\n70.30±0.99\n44.15±0.81\n57.59±0.66\n30.99±0.43\n46.80±0.38\n9.67\n72.48±1.22\n80.30±1.03\n60.08±0.90\n69.85±0.76\n46.12±0.53\n57.30±0.48\n8.50\nProtoNet (Sup.)\n58.61±1.21\n73.91±0.93\n44.54±0.79\n62.15±0.64\n32.10±0.42\n50.87±0.40\n7.67\n73.80±1.20\n81.33±1.00\n61.88±0.86\n73.02±0.74\n48.70±0.52\n62.42±0.45\n4.33\nTENT (Sup.)\n61.30±1.18\n77.32±0.81\n47.30±0.80\n66.40±0.62\n36.40±0.45\n55.77±0.39\n4.50\n74.01±1.20\n82.54±1.00\n62.95±0.85\n73.26±0.77\n49.67±0.53\n61.87±0.47\n2.67\nG-Meta (Sup.)\n59.88±1.26\n75.36±0.86\n44.34±0.80\n59.59±0.66\n33.25±0.42\n49.00±0.39\n7.50\n74.64±1.20\n79.96±1.08\n61.50±0.88\n70.33±0.77\n46.07±0.52\n58.38±0.47\n7.00\nGLITTER (Sup.)\n55.17±1.18\n69.33±0.96\n42.81±0.81\n52.76±0.68\n30.70±0.41\n40.82±0.41\n11.50\n73.50±1.25\n75.90±1.19\nOOT\nOOT\nOOM\nOOM\n9.50\nCOSMIC (Sup.)\n62.24±1.15\n73.85±0.83\n47.85±0.77\n59.11±0.60\n42.25±0.43\n47.28±0.38\n6.33\n72.34±1.18\n80.83±1.03\n59.21±0.80\n70.67±0.71\n49.52±0.51\n59.01±0.42\n7.50\nTLP-BGRL\n62.59±1.13\n78.80±0.80\n49.43±0.79\n67.18±0.61\n37.63±0.44\n56.26±0.39\n3.17\n73.92±1.19\n82.42±0.95\n60.16±0.87\n72.13±0.74\n47.00±0.53\n60.57±0.45\n4.83\nTLP-SUGRL\n55.42±1.08\n76.01±0.84\n44.66±0.74\n63.69±0.62\n34.23±0.41\n52.76±0.40\n6.33\n71.27±1.15\n81.36±1.02\n58.85±0.81\n71.02±0.78\n45.71±0.49\n59.77±0.45\n8.17\nTLP-AFGRL\n55.24±1.02\n75.92±0.83\n44.08±0.70\n64.42±0.62\n33.88±0.41\n53.83±0.39\n7.17\n71.18±1.17\n82.03±0.94\n58.70±0.86\n71.14±0.75\n45.99±0.53\n60.31±0.45\n7.83\nVNT\n47.53±1.14\n69.94±0.89\n37.79±0.69\n57.71±0.65\n28.78±0.40\n46.86±0.40\n11.17\n58.21±1.16\n76.25±1.05\n48.75±0.81\n66.37±0.77\n40.10±0.49\n55.15±0.46\n11.17\nNAQ-FEAT-Best (Ours)\n66.30±1.15\n80.09±0.79\n52.23±0.73\n68.87±0.60\n44.13±0.47\n60.94±0.36\n1.33\n73.55±1.16\n82.36±0.94\n60.70±0.87\n72.36±0.73\n50.42±0.52\n64.90±0.43\n3.67\nNAQ-DIFF-Best (Ours)\n66.26±1.15\n80.07±0.79\n52.17±0.74\n69.34±0.63\n44.12±0.47\n60.97±0.37\n1.67\n76.58±1.18\n82.86±0.95\n64.31±0.87\n74.06±0.75\n51.62±0.54\n64.78±0.44\n1.17\ntion method: 1) Two product networks (Amazon-Clothing,\nAmazon-Electronics (McAuley et al., 2015)), 2) three ci-\ntation networks (Cora-Full (Bojchevski & G¨unnemann,\n2018), DBLP (Tang et al., 2008)) in addition to a large-\nscale dataset ogbn-arxiv (Hu et al., 2020). Detailed expla-\nnations of the datasets and their statistics are provided in\nSection A.5 in the Appendix.\nBaselines. We use six graph meta-learning models as base-\nlines, i.e., MAML (Finn et al., 2017), ProtoNet (Snell et al.,\n2017), G-Meta (Huang & Zitnik, 2020), TENT (Wang\net al., 2022b), GLITTER (Wang et al., 2022a), and COS-\nMIC (Wang et al., 2023b) to evaluate the performance of\nour proposed unsupervised episode generation methods, i.e.,\nNAQ-FEAT and NAQ-DIFF. In addition, three recent GCL\nbaselines, i.e., BGRL (Thakoor et al., 2022), SUGRL (Mo\net al., 2022) and AFGRL (Lee et al., 2022b), are included\nas they have shown remarkable performance on the FSNC\ntask without using labels (Tan et al., 2022). Lastly, we\ncompare with VNT (Tan et al., 2023) that uses a pretrained\ngraph transformer without labels and fine-tunes injected soft\nprompts to solve downstream FSNC task. For both NAQ-\nFEAT and NAQ-DIFF, we sampled Q = 10 queries for each\nsupport set sample to generate the training episodes. Details\non compared baselines and their experimental settings are\npresented in Section A.6 in the Appendix.\nEvaluation. For each dataset except for Amazon-Clothing,\nwe evaluate the performance of the models in 5/10/20-way,\n1/5-shot settings, i.e., six settings in total. For Amazon-\nClothing, as the validation set contains 17 classes, evalua-\ntions on 20-way cannot be conducted. Instead, the evalua-\ntion is done in 5/10-way 1/5-shot settings, i.e., four settings\nin total. In the validation and testing phases, we sampled 50\nvalidation tasks and 500 testing tasks for all settings with 8\nqueries each. For all the baselines, validation/testing tasks\nare fixed, and we use linear probing on frozen features to\nsolve each downstream task except for GLITTER and VNT\nas they use different strategies for solving downstream tasks.\nWe report average accuracy and 95% confidence interval\nover sampled testing tasks.\n4.1. Overall Performance Analysis\nThe overall results on five datasets are presented in Ta-\nble 1, 2, and 3. Note that since NAQ is model-agnostic,\nwe apply NAQ with all the supervised graph meta-learning\nmodels contained in our baselines, and report the best per-\nformance among them. We have the following observations.\nFirst, our proposed methods outperform the existing super-\nvised baselines. We attribute this to the episode generation\nstrategy of NAQ that allows the model to extensively utilize\nall nodes in the graph without reliance on node labels. It is\nworth noting that for each training episode while other su-\npervised methods use 5-shot support sets, our methods use\n1-shot support sets to ensure that the support set nodes are as\nmuch distinguishable from one another as possible. Hence,\nwe expect that our methods can be further improved if we\ndevelop methods to generate additional support set samples\nthat would make each support set even more distinguishable\nfrom one another, which we leave as future work.\nSecond, our proposed methods outperform methods utiliz-\ning the pre-trained encoder in an unsupervised manner (i.e.,\nGCL methods and VNT). Unlike these methods, by ap-\nplying the episodic learning framework for model training,\nour methods can capture information about the downstream\ntask ‘format’ during the model training, leading to generally\nbetter performance in the FSNC task.\nThird, NAQ-DIFF outperforms NAQ-FEAT in citation net-\nworks (Table 2). This result verifies our motivation for pre-\nsenting NAQ-DIFF in Section 3.2.1, which was to capture\nthe structural information instead of the raw node features in\ndomains where the structural information is more beneficial.\nOn the other hand, NAQ-FEAT outperforms NAQ-DIFF in\nproduct networks. This is because products in ‘also-viewed’\nor ‘bought-together’ relationships are not always similar or\nrelated in case of product networks (Zhang et al., 2022),\nimplying that discovering query sets based on ‘raw-feature’\n6\nUnsupervised Episode Generation for Graph Meta-learning\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n55\n60\n65\n70\n75\n80\n85\nAmazon-Clothing\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n45\n50\n55\n60\n65\n70\n75\nAmazon-Electronics\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n40\n45\n50\n55\n60\n65\nCora-Full\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n45\n50\n55\n60\n65\n70\n75\nDBLP\nAccuracy (%)\nSupervised\nNaQ-Feat\nNaQ-Diff\nFigure 3. Result of applying NAQ-FEAT and NAQ-DIFF to existing graph meta-learning models (5-way 1-shot).\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n45\n50\n55\n60\n65\n70\n75\n80\nAmazon-Clothing\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n30\n35\n40\n45\n50\n55\n60\nAmazon-Electronics\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n20\n25\n30\n35\n40\n45\nCora-Full\nMAML\nProtoNet\nTENT\nG-Meta GLITTER COSMIC\n20\n25\n30\n35\n40\n45\n50\nOOM\nDBLP\nAccuracy (%)\nFigure 4. Result of applying NAQ-FEAT and NAQ-DIFF to existing graph meta-learning models in higher way settings (Amazon-Clothing:\n10-way 1-shot, Others: 20-way 1-shot).\nsimilarity is more beneficial.\nLastly, NAQ outperforms other baselines on the ogbn-arxiv\ndataset, which is a large-scale dataset (Table 3). It is worth\nnoting that the performances of two variants of NAQ are\nat the best and second best in a more challenging one-shot\nsetting. One interesting observation is that NAQ-FEAT out-\nperforms NAQ-DIFF, even though ogbn-arxiv is a citation\nnetwork. We attribute this to the fact that the raw node\nfeatures of ogbn-arxiv are ‘embeddings’ extracted from the\nskip-gram model. This implies that high-quality node fea-\nture enables NAQ-FEAT to find high-quality queries, which\nleads to a better FSNC performance of NAQ-FEAT.\nIn summary, NAQ resolves the label-scarcity problem of\nsupervised graph meta-learning methods and achieve perfor-\nmance enhancement on FSNC tasks by providing training\nepisodes that contain both the information of all nodes in the\ngraph, and the information of the downstream task format\nto the model.\n4.2. Model-agnostic Property of NAQ\nIn this section, we verify that NAQ can be applied to any\nexisting graph meta-learning models while not sacrificing\nmuch of their performance.\nIn Figure 3 and 4, we observe that our methods retained\nor even improved the performance of existing graph meta-\nlearning methods across various few-shot settings with only\na few exceptions. Particularly, in higher way settings shown\nin Figure 4, which are more challenging, NAQ generally\noutperforms supervised methods. Therefore, we argue that\nour methods allow existing graph meta-learning models to\nbe trained to generate more generalizable embeddings with-\nout any use of label information thanks to the full utilization\nof all nodes in a graph.\nLastly, it is important to note again that the performances\nTable 3. Overall averaged FSNC accuracy (%) with 95% confi-\ndence intervals on ogbn-arxiv (NAQ base-model: ProtoNet, OOM:\nOut Of Memory on NVIDIA RTX A6000)\nDataset\nogbn-arxiv\nSetting\n5 way\n10 way\nBaselines\n1 shot\n5 shot\n1 shot\n5 shot\nMAML (Sup.)\n40.61±0.89\n58.75±0.89\n27.32±0.55\n43.87±0.56\nProtoNet (Sup.)\n43.34±1.01\n58.30±0.95\n28.17±0.60\n46.11±0.60\nTENT (Sup.)\n48.06±0.97\n63.45±0.88\n33.85±0.65\n48.14±0.59\nG-Meta (Sup.)\n41.06±0.87\n59.43±0.87\n27.20±0.53\n45.04±0.53\nGLITTER (Sup.)\n35.64±0.97\n34.51±0.85\n20.95±0.50\n21.84±0.47\nCOSMIC (Sup.)\n50.32±0.95\n63.54±0.80\n38.41±0.62\n49.31±0.51\nTLP-BGRL\n49.88±1.01\n69.10±0.82\n36.40±0.62\n56.15±0.54\nTLP-SUGRL\n49.25±0.97\n62.15±0.92\n32.87±0.61\n45.76±0.60\nTLP-AFGRL\nOOM\nOOM\nOOM\nOOM\nVNT\nOOM\nOOM\nOOM\nOOM\nNAQ-FEAT (Ours)\n54.09±1.03\n69.94±0.84\n41.61±0.68\n58.18±0.59\nNAQ-DIFF (Ours)\n51.45±1.04\n66.73±0.89\n39.27±0.67\n55.93±0.56\nof the supervised models reported in our experiments are\nonly achievable when they have access to all labeled sam-\nples of entire base classes and the given labeled samples\nin the base classes are clean. On the other hand, when\nthere is only a limited amount of labeled samples within\na limited number of base classes (Figure 1(a)) or there is\ninherent label noise in the base classes (Figure 1(b)), the\nperformance of supervised models severely drops, while our\nproposed unsupervised methods would not be affected at\nall. Furthermore, as will be demonstrated in Section 4.4,\nthe performance of NAQ can be improved by adjusting the\nnumber of queries.\n4.3. Regarding the Class Imbalance\nIn this section, we visualize t-SNE (Van der Maaten & Hin-\nton, 2008) embeddings of nodes that belong to the top-10\ntail classes. Doing so can further justify our motivation for\n7\nUnsupervised Episode Generation for Graph Meta-learning\n(a) Amazon-Electronics\n(b) Cora-Full\nFigure 5. The t-SNE plot of tail-class embeddings (base-model:\nProtoNet, NAQ: trained with 5-way 1-shot training episodes)\nusing unsupervised graph meta-learning on FSNC problems\nrather than using GCL methods.\nAs shown in Figure 5, we can observe that our NAQ can\nlearn clearly separable embeddings for tail-class nodes than\nGCL method BGRL. This result further supports our claim\nthat GCL methods have difficulty in learning embeddings\nof nodes from minority classes. Therefore, we can ver-\nify that additional downstream task ‘format’ information\nprovided by episodic learning is beneficial for learning tail-\nclass nodes when solving the FSNC problem. Further dis-\ncussions on why NAQ can attain robustness against the\nclass imbalance (Section A.2.1) and additional results on\nvarious dataset biases, such as structure or feature noise\n(Section A.2.2), are presented in the Appendix.\n4.4. Hyperparameter Sensitivity Analysis\nSo far, the experiments have been conducted with a fixed\nnumber of queries, Q = 10. In this section, we investigate\nthe effect of the number of queries on the performance of\nNAQ. To thoroughly explore the effect of the number of\nqueries on NAQ, we check the performance of NAQ with\nProtoNet by changing the number of queries Q ∈{1, 3, 5,\n7, 10, 13, 15, 17, 20, 30, 40, 100}. We have the follow-\ning observations from Figure 6: (1) In Amazon-Clothing,\nsince both NAQ-FEAT and NAQ-DIFF can discover highly\nclass-level similar queries (Figure 7), they exhibit an in-\ncreasing tendency in performance as Q increases. (2) In the\ncase of Amazon-Electronics, NAQ-FEAT shows a similar\ntendency as in Amazon-Clothing due to the same reason,\nwhile there is a slight performance drop when Q = 100. In\ncontrast, NAQ-DIFF shows clearly decreasing performance\nafter Q = 5, as its queries have relatively low class-level\nsimilarity (Figure 7). From the results above, we can con-\nclude that sampling a proper number of queries Q during the\nepisode generation phase is essential. Otherwise, a signifi-\ncant level of label noise in the generated episode might hin-\n1 3 5 7 1013151720\n30\n40\n100\n# of queries Q\n61\n66\n71\n76\n81\n86\nAccuracy (%)\nAmazon-Clothing\nAmazon-Electronics\nCora-Full\nDBLP\n(a) NAQ-FEAT\n1 3 5 7 1013151720\n30\n40\n100\n# of queries Q\n65\n70\n75\n80\nAccuracy (%)\nAmazon-Clothing\nAmazon-Electronics\nCora-Full\nDBLP\n(b) NAQ-DIFF\nFigure 6. Effect of the number of queries NAQ (5-way 1-shot, base-\nmodel: ProtoNet, star marker: maximal point)\nder the model training. (3) In the DBLP dataset, NAQ-FEAT\nshows a nearly consistent performance tendency, while the\nperformance of NAQ-DIFF can be enhanced by increas-\ning the number of queries for training. This is because\nNAQ-DIFF can sample more class-level similar queries than\nNAQ-FEAT (Figure 7). From this observation, we again\nvalidate the motivation of utilizing structural neighbors as\nqueries in such datasets (Section 3.2.1).\n5. Related Work\n5.1. Few-Shot Node Classification (FSNC)\nFew-shot learning (Vinyals et al., 2016; Finn et al., 2017;\nSnell et al., 2017) aims to classify unseen target classes with\nonly a few labeled samples based on the meta-knowledge ob-\ntained from training on abundant samples from base classes.\nGraph Meta-learning. There have been various studies\nto solve FSNC in graph-structured data. Meta-GNN (Zhou\net al., 2019) addresses the problem by directly applying\nMAML (Finn et al., 2017) on GNN, and GPN (Ding et al.,\n2020) uses ProtoNet (Snell et al., 2017) architecture with\nadjusted prototype calculation by considering node impor-\ntance. G-Meta (Huang & Zitnik, 2020) utilizes subgraph-\nlevel embeddings of nodes inside training episodes based\non both ProtoNet and MAML frameworks to enable scal-\nable and inductive graph meta-learning. TENT (Wang et al.,\n2022b) tries to reduce variances within training episodes\nthrough node-level, class-level, and task-level adaptations.\nMeta-GPS (Liu et al., 2022) utilizes various components of\nnetwork encoder, prototype-based parameter initialization,\nand S2 (scaling & shifting) transformation to solve FSNC\ntasks even on heterophilic graphs. GLITTER (Wang et al.,\n2022a) claims that the given entire graph structure is redun-\ndant for learning node embeddings within the meta-task so\nthat it tries to learn task-specific structure for each meta-\ntask. COSMIC (Wang et al., 2023b) applies a contrastive\nlearning scheme on meta-learning to obtain the intra-class\ngeneralizability with hard (unseen) node classes generated\nby similarity-sensitive mix-up to achieve high inter-class\n8\nUnsupervised Episode Generation for Graph Meta-learning\ngeneralizability.\nGraph Meta-learning for Label-scarcity Problem. There\nwere a few studies aiming to alleviate the label-scarcity prob-\nlem of graph meta-learning methods. TEG (Kim et al., 2023)\nutilizes equivariant neural networks to capture task-patterns\nshared among training episodes regardless of node labels,\nenabling the learning of highly transferable task-adaptation\nstrategies even with a limited number of base classes and\nlabeled nodes. Meanwhile, X-FNC (Wang et al., 2023a) ob-\ntains pseudo-labeled nodes via label propagation based on\nPoisson Learning, and optimizes the model based on infor-\nmation bottleneck to discard irrelevant information within\nthe augmented support set. Although these methods ex-\ntract useful meta-knowledge based on training episodes (i.e.,\nTEG) or from pseudo-labeled nodes (i.e., X-FNC), they still\nhighly depend on a few labeled nodes during the model\ntraining, and thus still fall short of utilizing the information\nof all nodes in the graph. As a result, their FSNC perfor-\nmance degrades as the number of labeled nodes and base\nclasses decreases (Wang et al., 2023a; Kim et al., 2023).\nUnsupervised FSNC. As existing graph meta-learning\nmethods suffer from the label-scarcity problem, there were\nseveral studies to handle the FSNC problem in an unsuper-\nvised manner. TLP (Tan et al., 2022) utilizes GCL methods\nto solve FSNC, and it has shown superior FSNC perfor-\nmance than graph meta-learning methods without labels.\nVNT (Tan et al., 2023) applies graph transformer on FSNC\nand solves downstream FSNC task by only fine-tuning ‘vir-\ntual’ nodes injected as soft prompts and the classifier with\ngiven a few-labeled samples in the downstream task. Most\nrecently, (Liu et al., 2024) analyse advantages of applying\nGCL on FSNC over graph meta-learning in two aspects: 1)\nutilization of graph augmentation, and 2) explicit usage of\nall nodes in a graph. Base on this analysis, they present\na GCL-based method named COLA that aims to combine\nGCL and meta-learning by constructing meta-tasks without\nlabels during the training phase, which is computationally\ncostly. Although it shares some similarities with our method\nNAQ, COLA focuses on GCL-based model while our NAQ\nfocuses on enabling unsupervised graph meta-learning.\n5.2. Unsupervised Meta-learning\nIn computer vision, several unsupervised meta-learning\nmethods exist that attempt to address the limitations of re-\nquiring abundant labels for constructing training episodes.\nMore precisely, UMTRA (Khodadadeh et al., 2019) and\nAAL (Antoniou & Storkey, 2019) are similar methods, mak-\ning queries via image augmentation on randomly sampled\nsupport set samples. In addition, AAL focuses on task\ngeneration, while UMTRA is mainly applied to MAML.\nOn the other hand, CACTUs (Hsu et al., 2018) aims to\nmake episodes based on pseudo-labels obtained from clus-\nter assignments, which come from features pre-trained in an\nunsupervised fashion. LASIUM (Khodadadeh et al., 2020)\ngenerates synthetic training episodes that can be combined\nwith existing models, such as MAML and ProtoNet, with\ngenerative models. Moreover, Meta-GMVAE (Lee et al.,\n2021) uses VAE (Kingma & Welling, 2013) with Gaussian\nmixture priors to solve the few-shot learning problem.\n6. Limitations & Future Work\nAlthough NAQ has proven its effectiveness for Few-Shot\nNode Classification (FSNC), it is crucial to acknowledge its\nlimitations, presented below, to stimulate future work.\n6.1. Computational Issue of NAQ-DIFF\nDue to some technical issues regarding sparse matrix mul-\ntiplication, we cannot even calculate the truncated approx-\nimation of the graph Diffusion for the dataset, which has\nmany edges (e.g., ogbn-products). This problem hinders\nthe applicability of NAQ-DIFF to large real-world datasets.\nHence, it will be promising to devise unsupervised episode\ngeneration methods that can fully leverage the structural\ninformation of graphs while reducing computational costs.\n6.2. Problem of Na¨ıve Support Set Generation.\nSince NAQ depends on na¨ıve random sampling for support\nset generation, there is a possibility that nodes having the\nsame label can be assigned to a distinct support set, which is\nan undesirable case. Although we sample 1-shot support sets\nto avoid the above problem, developing a more sophisticated\nsupport set generation method that mitigates the problem\nmentioned above and generates a K-shot (K > 1) support\nset will be valuable future work.\n7. Conclusion\nIn this study, we proposed NAQ, a novel unsupervised\nepisode generation algorithm that enables unsupervised\ngraph meta-learning. NAQ generates 1) support sets by\nrandom sampling from the entire graph, and 2) query sets\nby utilizing feature-level similar nodes (i.e., NAQ-FEAT)\nor structurally similar neighbors from graph diffusion (i.e.,\nNAQ-DIFF). As NAQ generates training episodes out of\nall nodes in the graph without any label information, it can\naddress the label-scarcity problem of supervised graph meta-\nlearning models. Moreover, generated episodes from NAQ\ncan be used for training any existing graph meta-learning\nmodels almost without modifications and even boost their\nperformance on the FSNC task. Extensive experimental\nstudies on various downstream task settings demonstrate the\nsuperiority and potential of NAQ.\n9\nUnsupervised Episode Generation for Graph Meta-learning\nAcknowledgment\nThis work was supported by the National Research Foun-\ndation of Korea(NRF) grant funded by the Korea govern-\nment(MSIT) (RS-2024-00335098), and supported by Na-\ntional Research Foundation of Korea(NRF) funded by Min-\nistry of Science and ICT (NRF-2022M3J6A1063021).\nImpact Statement\nThis paper proposes to advance the field of unsupervised\nMachine Learning on graph-structured data like social net-\nworks. Although our research might have many potential\nethical/societal impacts during its application, we believe\nno specific points should be emphasized here.\nReferences\nAntoniou, A. and Storkey, A. Assume, augment and learn:\nUnsupervised few-shot meta-learning via random labels\nand data augmentation. arXiv preprint arXiv:1902.09884,\n2019.\nBojchevski, A. and G¨unnemann, S. Deep gaussian em-\nbedding of graphs: Unsupervised inductive learning via\nranking. In International Conference on Learning Repre-\nsentations, 2018.\nChen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang,\nJ.-B. A closer look at few-shot classification. arXiv\npreprint arXiv:1904.04232, 2019.\nDing, K., Wang, J., Li, J., Shu, K., Liu, C., and Liu, H.\nGraph prototypical networks for few-shot learning on\nattributed networks. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge\nManagement, pp. 295–304, 2020.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning, volume 70, pp. 1126–1135. PMLR, 2017.\nGareth, J., Daniela, W., Trevor, H., and Robert, T. An\nintroduction to statistical learning: with applications in\nR. Springer, 2013.\nGasteiger, J., Weißenberger, S., and G¨unnemann, S. Dif-\nfusion improves graph learning. Advances in Neural\nInformation Processing Systems, 32, 2019.\nGrill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos,\nR., and Valko, M. Bootstrap your own latent: A new\napproach to self-supervised learning. NeurIPS, 2020.\nHastie, T., Tibshirani, R., and Friedman, J. The elements of\nstatistical learning: data mining, inference, and predic-\ntion, volume 2. Springer, 2009.\nHsu, K., Levine, S., and Finn, C. Unsupervised learning via\nmeta-learning. arXiv preprint arXiv:1810.02334, 2018.\nHu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B.,\nCatasta, M., and Leskovec, J. Open graph benchmark:\nDatasets for machine learning on graphs. arXiv preprint\narXiv:2005.00687, 2020.\nHuang, J., Du, L., Chen, X., Fu, Q., Han, S., and Zhang, D.\nRobust mid-pass filtering graph convolutional networks.\nIn Proceedings of the ACM Web Conference 2023, 2023.\nHuang, K. and Zitnik, M. Graph meta learning via local\nsubgraphs. Advances in Neural Information Processing\nSystems, 33, 2020.\nKhodadadeh, S., Boloni, L., and Shah, M. Unsupervised\nmeta-learning for few-shot image classification.\nAd-\nvances in neural information processing systems, 32,\n2019.\nKhodadadeh, S., Zehtabian, S., Vahidian, S., Wang, W., Lin,\nB., and B¨ol¨oni, L. Unsupervised meta-learning through\nlatent-space interpolation in generative models. arXiv\npreprint arXiv:2006.10236, 2020.\nKim, S., Lee, J., Lee, N., Kim, W., Choi, S., and Park, C.\nTask-equivariant graph few-shot learning. In Proceedings\nof the 29th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, pp. 1120—-1131, 2023.\nKingma, D. P. and Ba, J. L. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations, 2015.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKipf, T. N. and Welling, M. Semi-supervised classifica-\ntion with graph convolutional networks. In International\nConference on Learning Representations, 2017.\nLee, D. B., Min, D., Lee, S., and Hwang, S. J. Meta-gmvae:\nMixture of gaussian vae for unsupervised meta-learning.\nIn International Conference on Learning Representations,\n2021.\nLee, N., Hyun, D., Lee, J., and Park, C. Relational self-\nsupervised learning on graphs. In Proceedings of the\n31st ACM International Conference on Information &\nKnowledge Management, pp. 1054–1063, 2022a.\nLee, N., Lee, J., and Park, C.\nAugmentation-free self-\nsupervised learning on graphs. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 36,\npp. 7372–7380, 2022b.\n10\nUnsupervised Episode Generation for Graph Meta-learning\nLiu, H., Feng, J., Kong, L., Tao, D., Chen, Y., and Zhang, M.\nGraph contrastive learning meets graph meta learning: A\nunified method for few-shot node tasks. In Proceedings\nof the ACM Web Conference 2024, 2024.\nLiu, X., Ding, J., Jin, W., Xu, H., Ma, Y., Liu, Z., and Tang, J.\nGraph neural networks with adaptive residual. Advances\nin Neural Information Processing Systems, 2021.\nLiu, Y., Li, M., Li, X., Giunchiglia, F., Feng, X., and Guan,\nR. Few-shot node classification on attributed networks\nwith graph meta-learning. In Proceedings of the 45th in-\nternational ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval, pp. 471–481, 2022.\nLu, Y., Jiang, X., Fang, Y., and Shi, C. Learning to pre-\ntrain graph neural networks. In Proceedings of the AAAI\nconference on artificial intelligence, volume 35, pp. 4276–\n4284, 2021.\nMcAuley, J., Pandey, R., and Leskovec, J. Inferring net-\nworks of substitutable and complementary products. In\nProceedings of the 21th ACM SIGKDD international con-\nference on knowledge discovery and data mining, pp.\n785–794, 2015.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. Advances in neural informa-\ntion processing systems, 26, 2013.\nMo, Y., Peng, L., Xu, J., Shi, X., and Zhu, X. Simple unsu-\npervised graph representation learning. In Proceedings\nof the AAAI Conference on Artificial Intelligence (AAAI),\npp. 7797–7805, 2022.\nPage, L., Brin, S., Motwani, R., and Winograd, T. The\npagerank citation ranking: Bringing order to the web.\nTechnical report, Stanford InfoLab, 1999.\nRong, Y., Huang, W., Xu, T., and Huang, J. Dropedge:\nTowards deep graph convolutional networks on node clas-\nsification. In International Conference on Learning Rep-\nresentations, 2020.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks\nfor few-shot learning. Advances in Neural Information\nProcessing Systems, 30, 2017.\nTan, Z., Wang, S., Ding, K., Li, J., and Liu, H. Transductive\nlinear probing: A novel framework for few-shot node\nclassification. In Learning on Graphs Conference, 2022.\nTan, Z., Guo, R., Ding, K., and Liu, H. Virtual node tuning\nfor few-shot node classification. In Proceedings of the\n29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pp. 2177––2188, 2023.\nTang, J., Zhang, J., Yao, L., Li, J., Zhang, L., and Su, Z.\nArnetminer: extraction and mining of academic social\nnetworks. In Proceedings of the 14th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining, pp. 990–998, 2008.\nThakoor, S., Tallec, C., Azar, M. G., Azabou, M., Dyer,\nE. L., Munos, R., Veliˇckovi´c, P., and Valko, M. Large-\nscale representation learning on graphs via bootstrapping.\nIn International Conference on Learning Representations,\n2022.\nVan der Maaten, L. and Hinton, G. Visualizing data using\nt-sne. Journal of machine learning research, 9(11), 2008.\nVeliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,\nLio, P., and Bengio, Y. Graph attention networks. In\nInternational Conference on Learning Representations,\n2018.\nVinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.\nMatching networks for one shot learning. Advances in\nneural information processing systems, 29, 2016.\nWang, K., Shen, Z., Huang, C., Wu, C.-H., Dong, Y., and\nKanakia, A. Microsoft academic graph: When experts\nare not enough. Quantitative Science Studies, 2020.\nWang, S., Chen, C., and Li, J. Graph few-shot learning with\ntask-specific structures. In NeurIPS, 2022a.\nWang, S., Ding, K., Zhang, C., Chen, C., and Li, J. Task-\nadaptive few-shot node classification. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, pp. 1910–1919, 2022b.\nWang, S., Dong, Y., Ding, K., Chen, C., and Li, J. Few-shot\nnode classification with extremely weak supervision. In\nProceedings of the 16th International Conference on Web\nSearch and Data Mining, 2023a.\nWang, S., Tan, Z., Liu, H., and Li, J. Contrastive meta-\nlearning for few-shot node classification. In SIGKDD,\n2023b.\nWu, H., Wang, C., Tyshetskiy, Y., Docherty, A., Lu, K.,\nand Zhu, L. Adversarial examples on graph data: Deep\ninsights into attack and defense. IJCAI, 2019.\nZhang, C., Du, Y., Zhao, X., Han, Q., Chen, R., and Li,\nL. Hierarchical item inconsistency signal learning for\nsequence denoising in sequential recommendation. In\nProceedings of the 31st ACM International Conference\non Information and Knowledge Management, 2022.\nZhang, J., Zhang, H., Xia, C., and Sun, L. Graph-bert: Only\nattention is needed for learning graph representations.\narXiv preprint arXiv:2001.05140, 2020.\n11\nUnsupervised Episode Generation for Graph Meta-learning\nZhang, L. and Lu, H. A feature-importance-aware and ro-\nbust aggregator for gcn. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge\nManagement, pp. 1813–1822, 2020.\nZhou, F., Cao, C., Zhang, K., Trajcevski, G., Zhong, T., and\nGeng, J. Meta-gnn: On few-shot node classification in\ngraph meta-learning. In Proceedings of the 28th ACM\nInternational Conference on Information and Knowledge\nManagement, 2019.\n12\nUnsupervised Episode Generation for Graph Meta-learning\nA. Appendix\nA.1. Regarding ‘Class-level Similarity’\nA.1.1. WHY IS ‘CLASS-LEVEL SIMILARITY’ SUFFICIENT?\nIn Section 3.4, we justified our ‘similarity’ condition presented in Section 3.1 in terms of ‘class-level similarity’. In this\nsection, we provide an explanation on why considering the class-level similarity instead of the exact same class condition,\nwhich is in fact impossible because the class information is not given, is sufficient for the query generation process in NAQ,\nand further justify why our method outperforms supervised meta-learning methods.\nOverall, we conjecture that training a model via episodic learning with episodes generated from NAQ can be done\nsuccessfully not only because our methods enable the utilization of all nodes in a graph, but also because our methods\ngenerate sufficiently informative episodes that enable the model to learn the downstream task format. When we take a closer\nlook at the training process of an episodic learning framework, the model only needs to classify a small number (N-way)\nof classes in a single episode unlike the conventional training scheme requiring the model to classify total |C| classes in a\ngraph. For this reason, we do not have to strive for finding queries whose labels are exactly the same as their corresponding\nsupport set sample as in ordinary supervised episode generation. Therefore, finding class-level similar queries is sufficient\nfor generating informative training episodes.\nMoreover, if we can generate training episodes that have queries similar enough to the corresponding support set sample\nwhile being dissimilar to the remaining N −1 support set samples, we further conjecture that the episodes utilizing class-level\nsimilar queries from NAQ is even more beneficial than episodes generated in the ordinary supervised manner. This is because\nthe episodes generated by NAQ provide helpful information from different but similar classes while episodes generated in\nthe supervised manner merely provide the information within the same classes as support set sample. To further demonstrate\nthat NAQ has the ability to discover such class-level similar queries, empirical analysis is provided in Section A.1.2.\nAmazon-\nClothing\nAmazon-\nElectronics\nCora-Full\nDBLP\n20\n40\n60\n80\n100\nClass-level similarity (%)\nNaQ-Feat (Top-10)\nNaQ-Diff (Top-10)\nFigure 7. Averaged class-level similarity between each node and top-10 similar nodes found via NAQ-FEAT and NAQ-DIFF\nA.1.2. NAQ DISCOVERS ‘CLASS-LEVEL SIMILAR’ QUERIES\nIn this section, we provide an empirical evidence that NAQ can find class-level similar neighbors as queries for each support\nset sample (Figure 7), and we further analyze the experimental results of our methods based on that evidence.\nTo verify that queries found by NAQ are class-level similar, we measure the averaged class-level similarity between a node\nand its top-10 similar nodes found by NAQ-FEAT (raw feature similarity) and NAQ-DIFF (graph diffusion) in all four\ndatasets. The class-level similarity between two nodes is computed based on the similarity between their class centroids,\nwhere the centroid of class c is computed by ac = MEAN(P xi · I{yi = c}) with xi denoting the raw feature of node i and\nyi denoting the label of node i. The results are presented in Figure 7. In most cases, similar nodes found by NAQ-FEAT and\nNAQ-DIFF exhibit a high-level (∼80%) average class-level similarity. This result shows that NAQ-FEAT and NAQ-DIFF\ncan discover enough class-level similar nodes as queries for each support set sample.\nIn addition, we can further justify our arguments in Section 3.2.1 and the experimental results in Section 4.1 based on these\nresults. First, in Figure 7, we observe that we can sample more class-level similar queries by NAQ-DIFF than NAQ-FEAT in\ncitation networks (i.e., Cora-Full and DBLP), implying that considering graph structural information can be more beneficial\n13\nUnsupervised Episode Generation for Graph Meta-learning\nin citation networks for the reason described in Section 3.2.1. Second, since NAQ-DIFF can discover class-level similar\nqueries in the DBLP dataset, it shows superior performance in the DBLP dataset even though DBLP has a low homophily\nratio. Therefore, we emphasize again that discovering class-level similar queries is essential in generating informative\nepisodes. Third, we observe that the variant of NAQ with higher class-level similarity always performs better in the\ndownstream FSNC task, implying that making queries class-level similar to corresponding support set samples is directly\nrelated to the performance of NAQ.\nIn summary, we quantitatively demonstrated that NAQ indeed discovers class-level similar nodes without using label\ninformation, and showed that the experimental results for our methods align well with our motivation regarding the\nsupport-query similarity, presented in Section 3.1 and justified in Section 3.4.\nA.2. Regarding the Inherent Bias in Graphs\nAlthough in the main paper, we mentioned about the vulnerability of existing GCL methods to class imbalance, there\nexist other inherent bias that may exist in graphs, i.e., structure noise and feature noise. In this section, we provide\nfurther discussions on class imbalance (Section A.2.1) followed by additional results under structure and feature noise\n(Section A.2.2).\nA.2.1. FURTHER DISCUSSION ON CLASS IMBALANCE\nIn this section, we discuss in detail why existing graph meta-learning methods and our NAQ retains robustness against class\nimbalance in a graph. Even though we can conclude that task format information learned by episodic learning framework\nmakes the model to be robust against the class imbalance from various experimental results (See Figure 1(c) and Section 4.3),\nhere we delve deeper into which elements of the (ordinary) supervised or unsupervised episode generation (NAQ) contribute\nto the robustness against the class imbalance. In addition, we present empirical analysis to further support our claim that\nepisodic learning is beneficial to attain robustness against the class imbalance.\nSupervised Graph Meta-learning.\nIn the training episode generation step of the ordinary supervised meta-learning\nmethods, they first sample N-way classes in base classes Cb, then sample K-shot support set samples and Q queries within\neach of sampled classes. As a result, all classes in base classes are treated equally regardless of the number of samples they\ncontain. Therefore, with an aid of the task format information obtained via episodic learning, supervised graph meta-learning\ncan be robust to class imbalance in a graph.\nTable 4. Class-level similarity between each node from Top-p% tail classes in the graph and top-10 similar nodes found via NAQ-FEAT\nand NAQ-DIFF (Results of 100%: reported in Figure 7)\nDatasets\nAmazon-Clothing\nAmazon-Electronics\nCora-Full\nDBLP\ntop-p%\ntail classes\nNAQ-FEAT\nNAQ-DIFF\nNAQ-FEAT\nNAQ-DIFF\nNAQ-FEAT\nNAQ-DIFF\nNAQ-FEAT\nNAQ-DIFF\n10%\n∼78.7%\n∼75.2%\n∼72.3%\n∼48.2%\n∼69.7%\n∼77.9%\n∼66.6%\n∼75.1%\n20%\n∼81.3%\n∼78.2%\n∼74.1%\n∼51.6%\n∼70.7%\n∼77.6%\n∼68.3%\n∼78.0%\n50%\n∼81.7%\n∼80.7%\n∼77.8%\n∼53.0%\n∼74.6%\n∼81.8%\n∼70.4%\n∼80.9%\n80%\n∼80.8%\n∼79.0%\n∼78.9%\n∼52.5%\n∼77.8%\n∼84.6%\n∼71.9%\n∼82.1%\n100%\n∼81.6%\n∼78.8%\n∼81.9%\n∼52.7%\n∼79.8%\n∼86.0%\n∼73.5%\n∼83.0%\nUnsupervised Graph Meta-learning with NAQ. Since the class label information is not given to NAQ, addressing class\nimbalance is not trivial as in the supervised case described above. Instead, NAQ samples ‘class-level similar’ queries to the\nsupport set nodes from tail classes, which can help learning tail-class embeddings. To demonstrate that NAQ still finds\n‘class-level similar’ queries to the tail-class nodes, we measured the averaged class-level similarity between node of the\ntop-p% tail classes and top-10 similar nodes found by NAQ. Results can be found in Table 4. We observe that NAQ still\nfinds class-level similar enough queries even for the nodes from tail classes, especially in the dataset in which each variant\nof NAQ outperforms (i.e., NAQ-FEAT for product networks (Amazon-Clothing/Electronics), and NAQ-DIFF for citation\nnetworks (Cora-Full, DBLP)). For top-10% tail classes, queries found by NAQ-FEAT exhibit 78.67% / 72.29% class-level\nsimilarity in Amazon-Clothing / Amazon-Electronics, and queries found by NAQ-DIFF exhibit 77.89% / 75.05% class-level\nsimilarity in Cora-Full / DBLP. Therefore, we can conclude that ‘class-level similar’ queries found by NAQ are beneficial\nfor learning tail-class embeddings from the results of Table 4 and Section 4.3.\n14\nUnsupervised Episode Generation for Graph Meta-learning\nRole of the Episodic Learning Framework.\nTo empirically examine whether downstream task format information\nprovided by episodic learning helps attain robustness against the class imbalance in the graph or not, we observed the change\nin the quality of t-SNE embeddings of the top-10 tail-class nodes produced by NAQ-DIFF when N-way becomes larger (i.e.,\nN = 5 →20, more challenging training setting) in the Amazon-Electronics dataset.\nFigure 8. (Left): The t-SNE plot of tail-class embeddings produced by NAQ-DIFF trained with 5-way training episodes. (Right): The\nt-SNE plot of tail-class embeddings produced by NAQ-DIFF trained with 20-way training episodes (base-model: ProtoNet)\nAs we observed in Figure 5(a), NAQ-DIFF has difficulty in finding class-level similar queries (See Figure 7 and Table 4) due\nto the low average degree (∼2.06) of the Amazon-Electronics dataset, so that produces inferior tail-class embedding quality\ncompared to NAQ-FEAT in case of Amazon-Electronics. However, by training with more challenging episodes (i.e., 20-way\ntraining episodes), NAQ-DIFF can produce clearly separable tail-class node embeddings even in the Amazon-Electronics\ndataset. Therefore, we can conclude that downstream task ‘format’ information provided by episodic learning benefits\nlearning about minority classes.\n0.0 (Clean)\n0.1\n0.3\nRandom edge add ratio p\n55\n65\n75\nAccuracy (%)\nAmazon-Electronics\nNaQ-Feat\nNaQ-Diff\nProtoNet\nBGRL\n(a) Structure noise\n0.0 (Clean)\n0.1\n0.3\nFeature-flipped node ratio p\n55\n65\n75\nAccuracy (%)\nAmazon-Electronics\nNaQ-Feat\nNaQ-Diff\nProtoNet\nBGRL\n(b) Feature noise\nFigure 9. (a): Impact of the structure noise, (b): Impact of the feature noise (5-way 1-shot, NAQ base-model: ProtoNet)\nA.2.2. ADDITIONAL RESULTS ON THE INHERENT BIAS\nStructure Noise. Since structure noise in a graph is also a crucial inherent bias that is known to deteriorate the performance\nof GNNs, we also evaluated the FSNC performance when there are noisy edges in the given graph structure. To perturb\ngraph structure, we considered random edge addition, because adding edges are known to be a more effective attack (Wu\net al., 2019). We add random edges as much as p ∈{0.1, 0.3} of the number of edges in the original graph. By adjusting the\nrandom edge adding ratio p, we examined the impact of the structure noise on 5-way 1-shot FSNC performance. Results are\npresented in Figure 9(a). We observe that meta-learning methods are more robust than a GCL method, BGRL, which we\nattribute to the task format information learned by episodic learning framework. Moreover, NAQ-FEAT shows significantly\nbetter robustness compared with other baselines, as it only utilizes clean raw node feature instead of noisy structure for\ntraining episode generation.\n15\nUnsupervised Episode Generation for Graph Meta-learning\nFeature Noise.\nWe also examined the impact of the feature noise on the FSNC performance. After random sampling\np ∈{0.1, 0.3} nodes to be corrupted (Liu et al., 2021), we injected feature noise into sampled nodes by randomly flipping\n0/1 value on each dimension of the node feature X:.i from Bernoulli distribution with probability 1\nd\nPd\ni=1 X:.i (Zhang &\nLu, 2020; Huang et al., 2023). By adjusting the ratio of noisy nodes, we examined the impact of noisy features on 5-way\n1-shot FSNC performance. Results are presented in Figure 9(b). We observe that as more noise is added, BGRL shows a\nsignificant performance drop compared to meta-learning methods except for NAQ-FEAT, which we attribute again to the\ntask format information learned by episodic learning framework. As expected, as NAQ-FEAT relies on the node features for\nthe similarity computation, its performance drops as more feature noise is added. Thus, developing a more robust algorithm\nunder feature noise will be a promising direction for future work.\nA.3. Ablation Study: Similarity Metric in NAQ-FEAT\nAs discussed in the Section 3.2, the choice of the similarity metric is an important factor for NAQ-FEAT, since inappropriate\nchoice of the similarity metric can lead to the wrong selection of queries. To examine the impact of the similarity metric, we\nuse the cosine similarity and the negative Euclidean distance to measure the class-level similarity between each node and\ntop-10 similar nodes found by NAQ-FEAT (Table 5), as done in Section A.1.2. Note that Jaccard similarity is excluded when\nmeasuring class-level similarity since it cannot be applied to the continuous features. In addition, we evaluated the 5-way\n1-shot FSNC performance on each dataset when using the cosine similarity, Jaccard similarity, and the negative Euclidean\ndistance as the similarity metric (Table 6). Note that all hyperparameter settings of NAQ-FEAT other than the similarity\nmetric are identical.\nIn Table 5, we observe that using the cosine similarity as the similarity metric discovers more class-level similar nodes\nthan using the negative Euclidean distance. As a result, in Table 6, the FSNC accuracy when using the cosine similarity is\nsuperior to when using the negative Euclidean distance. Note that this is mainly due to the fact that the datasets used in this\nexperiment have bag-of-words node features, and thus the cosine similarity serves as a better metric. Therefore, we can\nconfirm that choosing an appropriate similarity metric is important.\nTable 5. Impact of the similarity metric on class-level similarity between each node and top-10 similar nodes found via NAQ-FEAT.\nDatasets\n(Feature type: bag-of-words)\nAvg. Class-level sim.\n(Cosine sim.)\nAvg. Class-level sim.\n(Neg. Euclidean dist.)\nAmazon-Clothing\n∼81.6%\n∼61.0%\nAmazon-Electronics\n∼81.9%\n∼64.6%\nCora-Full\n∼79.8%\n∼40.4%\nDBLP\n∼73.5%\n∼19.1%\nTable 6. Impact of the similarity metric on NAQ-FEAT (5-way 1-shot, base-model: ProtoNet)\nDatasets\n(Feature type: bag-of-words)\nFSNC Accuracy\n(Cosine sim.)\nFSNC Accuracy\n(Jaccard sim.)\nFSNC Accuracy\n(Neg. Euclidean dist.)\nAmazon-Clothing\n83.77%\n83.35%\n80.83%\nAmazon-Electronics\n76.46%\n76.63%\n70.68%\nCora-Full\n64.20%\n63.53%\n45.60%\nDBLP\n71.38%\n72.68%\n67.53%\nWhen comparing cosine similarity and Jaccard similarity, since they are similar metrics when measuring similarities in\nbag-of-words data, NAQ-FEAT with both similarity metrics shows similar FSNC performance over four datasets as shown in\nTable 6. Thus, we have the freedom to choose one of those two metrics when using NAQ-FEAT on data with bag-of-words\nfeatures. However, Jaccard similarity cannot be computed with continuous features, as we mentioned above. Hence, it will\nbe more beneficial to consider cosine similarity as a similarity metric due to its generality.\nLastly, note that we did not consider the learnable similarity metric since it requires node-node similarity calculation process\nper model update for episode generation, which is computationally burdensome. For this reason, we have not considered the\nlearnable metric since we pursued an episode generation method that can be performed before the training phase.\n16\nUnsupervised Episode Generation for Graph Meta-learning\nDiffusion-based Node-Node Similarity Calculation\nOriginal Graph\n𝒢𝒢\nRe-weighted Graph by Diffusion\n𝒢𝒢𝒢\nRandom Sampling Initial Support set 𝑆𝑆𝒯𝒯𝑡𝑡\n…\nRandomly sampled 𝑇𝑇sets of 𝑁𝑁nodes\n…\n𝑆𝑆𝒯𝒯1\n𝑆𝑆𝒯𝒯2\n𝑆𝑆𝒯𝒯𝑇𝑇\nSimilarity-based Query Generation\nHigher\nLower\n𝑆𝑆𝒯𝒯𝑡𝑡\n𝑄𝑄𝒯𝒯𝑡𝑡\n𝑠𝑠1\n𝑠𝑠3\n𝑠𝑠2\n𝑠𝑠4\n𝑠𝑠5\n𝑞𝑞1,1\n𝑞𝑞3,1\n𝑞𝑞2,1\n𝑞𝑞4,1\n𝑞𝑞5,1\n𝑞𝑞1,2\n𝑞𝑞3,2\n𝑞𝑞2,2\n𝑞𝑞4,2\n𝑞𝑞5,1\n𝑞𝑞1,𝑄𝑄\n𝑞𝑞3,𝑄𝑄\n𝑞𝑞2,𝑄𝑄\n𝑞𝑞4,𝑄𝑄\n𝑞𝑞5,𝑄𝑄\nGenerated Task 𝒯𝒯𝑡𝑡\nSimilarity\nFigure 10. Overview of the NAQ-DIFF. The only difference from NAQ-FEAT is that NAQ-DIFF utilizes graph diffusion instead of\nraw-feature-based similarity to get node-node similarity.\nA.4. Details on NAQ-DIFF\nFor NAQ-DIFF, we used Personalized PageRank (PPR) (Page et al., 1999)-based diffusion to obtain diffusion matrix S,\nwhere θP P R\nk\n= α(1 −α)k, with teleport probability α ∈(0, 1), as the weighting coefficient θk. In our experiments, α = 0.1\nis used to calculate PPR-based diffusion. Also, we used ˜Tsym = (wloop ·IN +D)−1/2(wloop ·IN +A)(wloop ·IN +D)−1/2,\nwith the self-loop weight wloop = 1, as transition matrix, where A ∈R|V|×|V| is an adjacency matrix of the graph G and D\nis a diagonal matrix whose entries Dii = P\nj Aij are each node’s degree.\nLast but not least, although there can be other approaches for capturing the graph structural information (e.g., using the\nadjacency matrix, or using a k-NN graph computed based on node embeddings learned by a GNN encoder during the\ntraining phase) (Lee et al., 2022a), we choose the graph diffusion as it captures more global information than the adjacency\nmatrix, and computationally more efficient than the k-NN approach.\nA.5. Details on Evaluation Datasets\nThe following is the details on evaluation datasets used in this work.\n• Amazon-Clothing (McAuley et al., 2015) is a product-product network, whose nodes are products from the category\n“Clothing, Shoes and Jewelry” in Amazon. Node features are constructed from the product descriptions, and edges\nwere created based on ”also-viewed” relationships between products. The node class is a low-level product category.\n• Amazon-Electronics (McAuley et al., 2015) is a network of products, whose nodes are products from the category\n“Electronics” in Amazon. Node features are constructed from the product descriptions, and edges represent the\n“bought-together” relationship between products. The node class is a low-level product category.\n• Cora-Full (Bojchevski & G¨unnemann, 2018) is a citation network, whose nodes are papers. The node features are\nconstructed from a bag-of-words representation of each node’s title and abstract, and edges represent the citation\nrelationship between papers. The node class is the paper topic.\n• DBLP (Tang et al., 2008) is a citation network, whose nodes are papers. Node features are constructed from their\nabstracts, and edges represent the citation relationship between papers. The node class is the venue where the paper is\npublished.\n• ogbn-arxiv (Hu et al., 2020) is a citation network, whose nodes are CS arXiv papers. Node features are constructed\nby averaging the embeddings of words in the title and abstract, where the word embeddings are obtained from the\nskip-gram model (Mikolov et al., 2013) over the MAG (Wang et al., 2020) corpus. Edges are citation relationships\nbetween papers, and the node class is 40 subject areas of arXiv CS papers.\nThe detailed statistics of the datasets can be found in Table 7. “Hom. ratio” denotes the homophily ratio of each dataset,\nand “Class split” denotes the number of distinct classes used to generate episodes in training (only for supervised settings),\nvalidation, and testing phase, respectively. For ogbn-arxiv, due to the GPU memory issue, graph diffusion calculation is\ndone as a truncated sum. Moreover, as node features in ogbn-arxiv are word embeddings, we used the negative Euclidean\ndistance as the similarity metric used for sampling query nodes in NAQ.\n17\nUnsupervised Episode Generation for Graph Meta-learning\nTable 7. Dataset statistics.\nDataset\n# Nodes\n# Edges\n# Features\n# Labels\nClass split\nHom. ratio\nAmazon-Clothing\n24,919\n91,680\n9,034\n77\n40/17/20\n0.62\nAmazon-Electronics\n42,318\n43,556\n8,669\n167\n90/37/40\n0.38\nCora-Full\n19,793\n65,311\n8,710\n70\n25/20/25\n0.59\nDBLP\n40,672\n288,270\n7,202\n137\n80/27/30\n0.29\nogbn-arxiv\n169,343\n1,166,243\n128\n40\n15/10/15\n0.43\nA.6. Details on Compared Baselines & Experimental Settings\nDetails on compared baselines are presented as follows.\n• MAML (Finn et al., 2017) aims to find good initialization for downstream tasks. It optimizes parameters via two-phase\noptimization. The inner-loop update finds task-specific parameters based on the support set of each task, and the\nouter-loop update finds a good parameter initialization point based on the query set.\n• ProtoNet (Snell et al., 2017) trains a model by building N class prototypes by averaging support samples of each class,\nand making each query sample and corresponding prototype closer.\n• G-Meta (Huang & Zitnik, 2020) obtains node embeddings based on the subgraph of each node in episodes, which\nallows scalable and inductive graph meta-learning.\n• TENT (Wang et al., 2022b) performs graph meta-learning to reduce the task variance among training episodes via\nnode-level, class-level, and task-level adaptations.\n• GLITTER (Wang et al., 2022a) aims to learn task-specific structures consisting of support set nodes and their relevant\nnodes, which have high node influence on them for each meta-training/test task since the given original graph structure\nis redundant when learning node embeddings in each meta-task.\n• COSMIC (Wang et al., 2023b) adopts contrastive learning scheme on graph meta-learning to enhance the intra-\nclass generalizability and similarity-sensitive mix-up which generates hard (unseen) node classes for the inter-class\ngeneralizability.\n• BGRL (Thakoor et al., 2022) applies BYOL (Grill et al., 2020) on graphs, so it trains the model by maximizing the\nagreement between an online embedding and a target embedding of each node, where each embedding is obtained\nfrom two differently augmented views.\n• SUGRL (Mo et al., 2022) simplifies architectures for effective and efficient contrastive learning on graphs, and trains\nthe model by concurrently increasing inter-class variation and reducing intra-class variation.\n• AFGRL (Lee et al., 2022b) applies BYOL architecture without graph augmentations. Instead of augmentations,\nAFGRL generates another view by mining positive nodes in the graph in terms of both local and global perspectives.\n• VNT (Tan et al., 2023) utilizes pretrained transformer-based encoder (Graph-Bert (Zhang et al., 2020)) as a backbone,\nand adapts to the downstream FSNC task by tuning injected ‘virtual’ nodes and classifier with given a few labeled\nsamples in the downstream task, then makes prediction on queries with such fine-tuned virtual nodes and classifier.\nFor meta-learning baselines except for GLITTER and COSMIC, we used a 2-layer GCN (Kipf & Welling, 2017) as the\nGNN encoder with the hidden dimension chosen from {64, 256}, and this makes MAML to be essentially equivalent\nto Meta-GNN (Zhou et al., 2019). Such choice of high hidden dimension size is based on (Chen et al., 2019), which\ndemonstrated that a larger encoder capacity leads to a higher performance of meta-learning model. For each baseline,\nwe tune hyperparameters for each episode generation method. In the case of GLITTER3 and COSMIC4, we adopted the\nsettings regarding the GNN encoder (e.g., number of layers and GNN model type) and hyperparameter settings reported in\ntheir official source code. For GCL baselines, we also used a 2-layer GCN encoder with the hidden dimension of size 256.\n3https://github.com/SongW-SW/GLITTER\n4https://github.com/SongW-SW/COSMIC\n18\nUnsupervised Episode Generation for Graph Meta-learning\nFor VNT, following the original paper, Graph-Bert (Zhang et al., 2020) is used as the backbone transformer model. As\nthe official code of VNT is not available, we tried our best to reproduce VNT with the settings presented in the paper of\nVNT and Graph-Bert. For all baselines, Adam (Kingma & Ba, 2015) optimizer is used. The tuned parameters and their\nranges are summarized in Table 8. Note that training TENT with NAQ was non-trivial, as it utilizes the entire labeled data\n(XCb, YCb) to compute cross-entropy loss along with episode-specific losses computed with training episodes per each\nupdate. Therefore, when we train TENT with NAQ, the cross-entropy loss was calculated over a single episode. For this\nreason, the superior performance of NAQ with TENT is especially noteworthy (See Figure 3 and 4) as it outperforms vanilla\nsupervised TENT even with much less data involved in each parameter update during the training phase.\nTable 8. Tuned hyperparameters and their range by baselines\nBaselines\nHyperparameters and Range\nMAML-like\n(MAML, G-Meta)\nInner step learning rate ∈{0.01, 0.05, 0.1, 0.3, 0.5},\n# of inner updates ∈{1, 2, 5, 10, 20}, Meta-learning rate ∈{0.001, 0.003}\nProtoNet-like\n(ProtoNet, TENT)\nLearning rate ∈{5 · 10−5, 10−4, 3 · 10−4, 5 · 10−4, 10−3, 3 · 10−3, 5 · 10−3}\nSelf-Supervised (TLP)\nLearning rate ∈{10−6, 10−5, 5 · 10−5, 10−4, 5 · 10−4, 10−3}\nProtoNet\nNaQ-Feat\n(Ours)\nBGRL\nSUGRL\nAFGRL\nVNT\n30\n35\n40\n45\n50\n55\n60\n65\nMeta-\nLearning\nGCL\nCora-Full\nProtoNet\nNaQ-Feat\n(Ours)\nBGRL\nSUGRL\nAFGRL\nVNT\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\nMeta-\nLearning\nGCL\nAmazon-Electronics\nSettings\nOriginal\nPareto\nExtreme\nFigure 11. Impact of the class imbalance (5-way 1-shot, NAQ-FEAT base-model: ProtoNet)\nDiscussion on VNT. Although we tried our best to reproduce VNT, we failed to achieve their reported performance\nespecially on Cora-Full, an evaluation dataset shared by VNT and our paper. This might be due to the random seed, dataset\nsplit, or the transformer architecture used in the experiment. However, we conjecture it will also suffer from the inherent bias\nin data such as class imbalance similar to GCL methods, as graph transformer-based model also learn generic embedding by\npretraining on a given graph. As an evidence, in Figure 11, we show the results on Cora-Full and Amazon-Electronics under\nthe same setting used to report results in Figure 1(c). We observe that the performance of VNT deteriorates under class\nimbalance like GCL methods.\nA.7. Model Training with Episodes from NaQ: ProtoNet Example\nIn this section, we explain how to train ProtoNet (Snell et al., 2017), which is one of the most widely used meta-learning\nmodels, with episodes generated by NAQ, as a detailed example of Algorithm 1. Let fθ be a GNN encoder, Tt be a generated\nepisode, and STt = {(xspt\nt,i , yspt\nt,i )}N×K\ni=1\nbe a randomly sampled support set, then a corresponding query set is generated as\nQTt = {(xqry\nt,i , yqry\nt,i )}N×Q\ni=1\n= NAQ(STt).\nMore precisely, we first obtain a prototype cj for each class j ∈{1, · · · , N} based on the support set ST as follows5:\ncj = 1\nK\nXK\ni=1 fθ(xspt\ni\n) · I{yspt\ni\n= j}\n(4)\nwhere I{yspt\ni\n= j} is an indicator function that is equal to 1 only if the label yi of xi is j, otherwise 0. Then, the probability\nof each query (xqry, yqry) ∈QT belonging to class j is computed as follows:\nP(yqry = j; xqry) =\nexp(−d(fθ(xqry), cj))\nP\nj′ exp(−d(fθ(xqry), cj′))\n(5)\n5To remove clutter, we drop the task subscript t from all notations from now on.\n19\nUnsupervised Episode Generation for Graph Meta-learning\nwhere d(·, ·) is a distance function. We use Euclidean distance in this work.\nThen, the parameter is updated as: θ ←θ −η∇θL(θ; qry), where η is the learning rate and L(θ; qry) is a loss given as:\nL(θ; qry) = −\n1\nN × Q\nX\n(xqry,yqry)∈QT log(P(yqry = j; xqry)).\n(6)\nA.8. Discussion on the Time Complexity of NAQ\nIn this section, we provide the time analysis of NAQ for generating training episodes. We measured the time spent for the\nsimilarity calculation in each dataset, and the time taken to generate all training episodes (i.e., 16,000 in total). The results\ncan be found in Table 9 and 10. Even though the datasets we used are not small, NaQ does not require significant time costs.\nMoreover, when we use NAQ, the time cost required for similarity calculation and episode generation is at least three times\nfaster than for ordinary supervised methods’ training episode generation.\nTable 9. Averaged elapsed time over 5 runs in seconds for node-node similarity calculation.\nDataset\nNAQ-FEAT\nNAQ-DIFF\nAmazon-Clothing\n1.7769\n2.7194\nAmazon-Electronics\n0.6538\n11.0443\nCora-Full\n0.0014\n1.3207\nDBLP\n0.0194\n9.8653\nTable 10. Averaged elapsed time over 5 runs in seconds for generating 16,000 training episodes.\nDataset\nNAQ-FEAT\nNAQ-DIFF\nSupervised\nAmazon-Clothing\n5.2117\n5.0242\n64.0850\nAmazon-Electronics\n6.1301\n5.9622\n64.4830\nCora-Full\n4.9786\n4.7484\n57.2931\nDBLP\n5.9689\n6.5407\n65.8119\nHowever, there are cases where it is challenging to perform similarity calculations at once due to GPU memory problems,\nif the size of the graph is too large. In such cases, we can calculate the node-node similarity by performing node-wise\ncalculation (NAQ-FEAT) or calculating graph diffusion as a truncated sum (NAQ-DIFF), where this process is required\nonly once for each dataset. Then, a list of Top-k (k << # of nodes) similar nodes for each node can be stored and used by\nloading them during the episode generation process. For example, in the case of the ogbn-arxiv dataset, which contains\nabout 160,000 nodes, we can calculate the Top-k similar nodes list with a capacity of ∼129.20 MiB in a short time of about\n150 seconds for NAQ-FEAT and 740 seconds for NAQ-DIFF, for k = 100. By using this Top-k similar nodes list, only\n2.2713 for NAQ-FEAT and 2.2266 for NAQ-DIFF seconds are spent on average (5 times) for generating total 16,000 training\nepisodes, which is faster than the supervised models’ average of 55.9989 seconds in the ogbn-arxiv dataset.\nMoreover, in the case of ogbn-products having 2,449,029 nodes, which is a very large-scale dataset, we can calculate such\nTop-100 similar nodes list in a short time of about 705 seconds by using batched node-node similarity calculation. Thus, our\nNAQ-FEAT can be scalable to very large graphs having million scale nodes. The following results presented in Table 11\ndemonstrate the effectiveness of our NAQ-FEAT in a very large-scale dataset.\nTable 11. Overall averaged FSNC accuracy (%) with 95% confidence intervals on very large-scale dataset (ogbn-products: having\n2,449,029 nodes, 61,859,140 edges, 47 classes (class split: 15/15/17), # of features: 100 (obtained by PCA on bag-of-words features),\nNAQ-FEAT base-model: ProtoNet)\nDataset\nogbn-products\nBaselines\n5-way 1-shot\n10-way 1-shot\nProtoNet (Sup.)\n43.50±1.20\n34.19±0.69\nCOSMIC (Sup.)\nOOM\nOOM\nTLP-BGRL\nOOM\nOOM\nTLP-SUGRL\n27.81±0.78\n18.72±0.52\nNAQ-FEAT (Ours)\n53.82±1.26\n43.84±0.77\nIt is worth noting that we only need one similarity calculation per dataset, which makes NAQ practical in reality.\n20\nUnsupervised Episode Generation for Graph Meta-learning\nA.9. Regarding Overlapping Queries in NAQ\nIn this section, we discuss the query overlapping problem of NAQ, where sampled query sets corresponding to each distinct\nsupport set have an intersection, which might hurt the FSNC performance of NAQ. Although we tried to prevent this problem\nby generating only a 1-shot support set as we mentioned in ‘Support set generation’ process in Section 3.2 (In other words,\nas each class contains only a 1-shot support node, the number of overlapping queries among classes can be minimized.),\nsuch query overlapping problem can happen and might be problematic for NAQ. To assess the severity of this problem, we\nmeasured the average query overlap ratio within training episodes generated by NaQ for each dataset. As shown in Table 12\nbelow, query overlap is generally very rare case.\nTable 12. Averaged query overlap ratio within 16,000 training episodes generated by NAQ\nDatasets\nAmazon-Clothing\nAmazon-Electronics\nCora-Full\nDBLP\nN-way\nNAQ-FEAT\nNAQ-DIFF\nNAQ-FEAT\nNAQ-DIFF\nNAQ-FEAT\nNAQ-DIFF\nNAQ-FEAT\nNAQ-DIFF\n5\n0.1573%\n0.9978%\n0.0871%\n11.1715%\n0.2206%\n0.4743%\n0.1826%\n0.0605%\n10\n0.3855%\n2.0769%\n0.2118%\n16.9618%\n0.5101%\n1.0138%\n0.4108%\n0.1389%\n20\n0.7834%\n4.0358%\n0.4457%\n21.4706%\n1.0221%\n2.0151%\n0.8559%\n0.3054%\nHowever, in the Amazon-Electronics dataset, which has a very low average degree (∼2.06), we observe non-negligible\noverlap ratio in the case of NAQ-DIFF, which uses graph Diffusion to find queries. To address this issue, we intentionally\ndropped overlapping queries in training episodes. Table 13 and 14 below show results of the effect of dropping overlapping\nqueries. ‘Overlap drop ver.’ means that we dropped overlapping queries after the episode generation process of NAQ.\nTable 13. Impact of dropping overlapping queries on FSNC performance (%) of NAQ-DIFF in Amazon-Electronics (base-model: ProtoNet)\nAmazon-Electronics\nSetting\nNAQ-DIFF\n(Original ver.)\nNAQ-DIFF\n(Overlap drop ver.)\n5-way 1-shot\n68.56±1.18\n69.77±1.17\n10-way 1-shot\n59.46±0.86\n61.98±0.86\n20-way 1-shot\n49.24±0.59\n52.15±0.60\nTable 14. Impact of dropping overlapping queries on FSNC performance (%) of NAQ-FEAT in Cora-Full (base-model: ProtoNet)\nCora-Full\nSetting\nNAQ-FEAT\n(Original ver.)\nNAQ-FEAT\n(Overlap drop ver.)\n5-way 1-shot\n64.20±1.11\n63.37±1.08\n10-way 1-shot\n51.78±0.75\n52.32±0.75\n20-way 1-shot\n40.11±0.45\n40.27±0.48\nFrom above results, we can conclude that removing query overlaps is a promising solution when query overlap is not\nnegligible like the case of NAQ-DIFF in Amazon-Electronics (see Table 13). However, when query overlap is negligible,\ndropping overlapping queries does not bring remarkable improvements (see Table 14).\nIn summary, the results in Table 12 and Table 14 demonstrate that the query overlapping problem of NAQ is generally\nnegligible in real-world datasets, and the results in Table 13 imply that dropping overlapping queries can be a promising\nsolution for some of the exceptional cases like NAQ-DIFF in Amazon-Electronics dataset.\nA.10. g-UMTRA: Augmentation-based Query Generation Method\nIn this section, we introduce our investigation method named g-UMTRA, utilizes graph augmentation to generate queries.\nIn computer vision, UMTRA (Khodadadeh et al., 2019) tried to apply MAML in an unsupervised manner by generating\nepisodes with image augmentations. With randomly sampled N support set nodes, UMTRA makes a corresponding query set\nthrough augmentation on the support set. Inspired by UMTRA (Khodadadeh et al., 2019), we devised an augmentation-based\nquery generation method called g-UMTRA. as an investigation method. g-UMTRA generates query set by applying graph\naugmentation on the support set. The method overview an be found in Figure 12.\n21\nUnsupervised Episode Generation for Graph Meta-learning\nOriginal Graph\n𝒢𝒢\nAugmented Graph\n𝒜𝒜(𝒢𝒢)\nSame \nEncoder\nNode Feature 𝑋𝑋\nAugmentation-based\nQuery Set Generation\nGNN 𝑓𝑓𝜃𝜃\nQuery set \nFeature 𝑋𝑋𝑄𝑄𝒯𝒯𝑡𝑡\nSupport Set Generation\nGNN 𝑓𝑓𝜃𝜃\nSupport set \nFeature 𝑋𝑋𝑆𝑆𝒯𝒯𝑡𝑡\n…\nRandomly sampled 𝑇𝑇sets of 𝑁𝑁nodes\n…\n𝑆𝑆𝒯𝒯1\n𝑆𝑆𝒯𝒯2\n𝑆𝑆𝒯𝒯𝑇𝑇\nRandom Sampling Initial Support set 𝑆𝑆𝒯𝒯𝑡𝑡\nFigure 12. Overview of g-UMTRA. First, we randomly sample T sets of N nodes from the entire graph and assign them distinct labels.\nThen, we generate support set features by using a GNN encoder with original structures and query set features with augmented structures.\nSpecifically, we first randomly sample T sets of N nodes from entire graph to generate {Tt}T\nt=1. Then for each task Tt,\nwe generate a N-way support set STt = {(xt,i, yt,i) | xt,i ∈V}N×1\ni=1 with distinct pseudo-labels yt,i for each xt,i, and their\ncorresponding query set QTt in the embedding space by applying graph augmentation.\nBy notating GNN encoder fθ as fθ(V; G), we can formally describe the query generation process of g-UMTRA as follows:\nXSTt = {\n\u0000fθ(xt,i; G), yt,i\n\u0001\n| (xt,i, yt,i) ∈STt},\nXQTt = {\n\u0000fθ(xt,i; A(G)), yt,i\n\u0001\n| (xt,i, yt,i) ∈STt},\n(7)\nwhere fθ(xt,i; G) is an embedding of node xt,i with the given graph G and a GNN encoder fθ, and A(·) is a graph\naugmentation function. For A(·), we can consider various strategies such as node feature masking (DropFeature) or\nDropEdge (Rong et al., 2020).\nNote that g-UMTRA is distinguished from UMTRA in the following two aspects: (1) g-UMTRA can be applied to any\nexisting graph meta-learning methods as it only focuses on episode generation, while UMTRA is mainly applied on MAML.\n(2) As described in Equation 7, g-UMTRA generates episodes as pair of sets (XSTj , XQTj ) that consist of embeddings.\nHence, its query generation process should take place in the training process, since augmentation and embedding calculation\nof GNNs depend on the graph structure. However, in UMTRA, image augmentation and ordinary convolutional neural\nnetworks are applied in instance-level, implying that the episode generation process of UMTRA can be done before training.\n60\n65\n70\n75\n80\n85\nAmazon-Clothing\n35\n40\n45\n50\n55\n60\n65\nCora-Full\nMAML\nProtoNet\nTENT\nG-Meta\n63\n68\n73\n78\nDBLP\nMAML\nProtoNet\nTENT\nG-Meta\n50\n55\n60\n65\n70\n75\nAmazon-Electronics\nAccuracy (%)\nSupervised\ng-UMTRA(DropEdge=0.5)\ng-UMTRA(DropEdge=0.5 & DropFeature=0.5)\nFigure 13. Performance comparison between supervised, g-UMTRA with DropEdge, and g-UMTRA with DropEdge and DropFeature on\nexisting graph meta-learning models (5-way 1-shot).\n22\nUnsupervised Episode Generation for Graph Meta-learning\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n75\n77\n79\n81\n83\nAmazon-Clothing\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n55\n57\n59\n61\n63\nCora-Full\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n63\n65\n67\n69\n71\n73\nDBLP\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n62\n64\n66\n68\n70\n72\n74\nAmazon-Electronics\nAccuracy (%)\nRatio p\nDropEdge only\nDropFeature only\nDropEdge + DropFeature\nFigure 14. Effect of augmentation function and its strength on g-UMTRA. (5-way 1-shot, base-model: ProtoNet)\nA.10.1. DRAWBACKS OF G-UMTRA\nAlthough g-UMTRA can show remarkable performance with some of base-models like ProtoNet (Snell et al., 2017)\n(See Figure 13), there are several drawbacks of g-UMTRA that limit its applicability in the real-world settings. First,\ng-UMTRA requires additional computation of augmented embedding by each update to make query set embeddings, which\nis time-consuming. Next, g-UMTRA cannot be model-agnostic, since it makes episode within the training phase due to\nthe graph augmentation for the query set generation. Thus, g-UMTRA requires inevitable modification on the training\nprocess of some existing models like G-Meta (Huang & Zitnik, 2020) and TENT (Wang et al., 2022b), up-to-date graph\nmeta-learning methods which are developed under the premise of utilizing supervised episodes, having mutually exclusive\nsupport set and query set. Lastly, g-UMTRA is also highly sensitive to the augmentation function choice and its strength\n(See Figure 14), similar to the original UMTRA.\nA.11. Additional Experimental Results\nImpact of the label-scarcity in Cora-Full. We additionally conducted the experiment about the label-scarcity problem\npresented in Figure 1(a) in the Cora-Full dataset. Similar to the result shown in Figure 1(a), supervised graph meta-learning\nmethods’ FSNC performance decreases as available labeled data and diversity of base classes decrease (See Figure 15(a)).\nImpact of the label noise in Cora-Full.\nWe also conducted the experiment regarding the label noise presented in\nFigure 1(b) in the Cora-Full dataset. Note that as Cora-Full has smaller size than Amazon-Electronics, we selected label\nnoise p ratio from {0, 0.1, 0.2, 0.3}. As shown in Figure 15(b), similar to the result in Figure 1(b), supervised meta-learning\nmethods’ FSNC performance is highly degraded as noise level increases.\nTENT\nG-Meta\nProtoNet\nMAML\n45\n50\n55\n60\n65\nCora-Full\nClass% / Label%\n100%/100%\n80%/80%\n50%/50%\n20%/20%\n(a) Impact of the label-scarcity in Cora-Full\n0.0\n0.1\n0.2\n0.3\nLabel noise ratio p\n35\n45\n55\n65\nAccuracy (%)\nCora-Full\nMAML\nProtoNet\nTENT\nG-Meta\n(b) Impact of the label noise in Cora-Full\nFigure 15. (a): Impact of the label-scarcity on supervised graph meta-learning methods, (b): Impact of the (randomly injected) label noise\np on supervised graph meta-learning methods. (5-way 1-shot)\n23\nUnsupervised Episode Generation for Graph Meta-learning\nTable 15. Overall averaged FSNC accuracy (%) with 95% confidence intervals on product networks (Full Version)\nDataset\nAmazon Clothing\nAmazon Electronics\nSetting\n5 way\n10 way\n5 way\n10 way\n20 way\nBase Model\nEpisode Generation\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\nMAML\nSupervised\n76.13±1.17\n84.28±0.87\n63.77±0.83\n76.95±0.65\n65.58±1.26\n78.55±0.96\n57.31±0.87\n67.56±0.73\n46.37±0.61\n60.04±0.52\nNAQ-FEAT (Ours)\n74.07±1.07\n86.49±0.86\n59.44±0.91\n75.99±0.70\n59.56±1.17\n74.85±1.03\n49.03±0.88\n70.47±0.73\n45.27±0.60\n62.36±0.51\nNAQ-DIFF (Ours)\n79.30±1.17\n86.81±0.82\n69.97±0.86\n79.74±0.68\n62.90±1.18\n78.37±0.90\n52.23±0.84\n68.77±0.76\n43.28±0.62\n59.88±0.51\nProtoNet\nSupervised\n75.52±1.12\n89.76±0.70\n65.50±0.82\n82.23±0.62\n69.48±1.22\n84.81±0.82\n57.67±0.85\n75.79±0.67\n48.41±0.57\n67.31±0.47\nNAQ-FEAT (Ours)\n83.77±0.96\n92.27±0.67\n76.08±0.81\n85.60±0.60\n76.46±1.11\n88.72±0.73\n68.42±0.86\n81.36±0.64\n58.80±0.60\n74.60±0.47\nNAQ-DIFF (Ours)\n78.64±1.05\n90.82±0.68\n71.75±0.81\n83.81±0.60\n68.56±1.18\n84.88±0.83\n59.46±0.86\n76.73±0.67\n49.24±0.59\n67.99±0.48\nTENT\nSupervised\n79.46±1.10\n89.61±0.70\n69.72±0.80\n84.74±0.59\n72.31±1.14\n85.25±0.81\n62.13±0.83\n77.32±0.67\n52.45±0.60\n69.39±0.50\nNAQ-FEAT (Ours)\n86.58±0.96\n91.98±0.67\n79.55±0.78\n86.10±0.60\n76.26±1.11\n87.27±0.81\n69.59±0.86\n81.44±0.61\n59.65±0.60\n74.09±0.46\nNAQ-DIFF (Ours)\n80.87±1.08\n90.53±0.71\n72.67±0.82\n84.54±0.61\n68.14±1.13\n83.64±0.80\n60.44±0.79\n76.03±0.67\n51.44±0.58\n68.37±0.49\nG-Meta\nSupervised\n78.67±1.05\n88.79±0.76\n65.30±0.79\n80.97±0.59\n72.26±1.16\n84.44±0.83\n61.32±0.86\n74.92±0.71\n50.39±0.59\n65.73±0.48\nNAQ-FEAT (Ours)\n85.83±1.03\n90.70±0.73\n73.45±0.84\n82.61±0.66\n74.49±1.15\n84.68±0.86\n61.18±0.83\n77.36±0.67\n55.35±0.60\n69.16±0.51\nNAQ-DIFF (Ours)\n82.27±1.10\n89.88±0.77\n71.48±0.86\n82.07±0.63\n69.62±1.20\n80.87±0.94\n58.71±0.80\n75.55±0.67\n49.06±0.58\n67.41±0.47\nGLITTER\nSupervised\n75.73±1.10\n89.18±0.74\n64.30±0.79\n77.73±0.68\n66.91±1.22\n82.59±0.83\n57.12±0.88\n76.26±0.67\n49.23±0.57\n61.77±0.52\nNAQ-FEAT (Ours)\n68.24±1.27\n76.91±1.00\n59.15±0.81\n77.19±0.65\n64.06±1.16\n80.25±0.86\n59.31±0.79\n74.65±0.67\n49.75±0.59\n65.30±0.51\nNAQ-DIFF (Ours)\n70.24±1.21\n82.48±0.83\n63.36±1.14\n80.41±0.62\n65.45±1.22\n80.33±0.87\n54.96±0.84\n71.10±0.72\n44.26±0.57\n60.20±0.50\nCOSMIC\nSupervised\n82.24±0.99\n91.22±0.73\n74.44±0.75\n81.58±0.63\n72.61±1.05\n86.92±0.76\n65.24±0.82\n78.00±0.64\n58.71±0.57\n70.29±0.44\nNAQ-FEAT (Ours)\n84.42±1.01\n91.73±0.69\n73.15±0.78\n84.74±0.58\n73.98±1.09\n87.08±0.75\n65.96±0.82\n79.11±0.60\n61.05±0.59\n73.73±0.42\nNAQ-DIFF (Ours)\n84.40±1.01\n91.72±0.69\n73.39±0.79\n84.82±0.58\n74.16±1.08\n87.09±0.75\n65.95±0.81\n79.13±0.60\n60.40±0.59\n73.75±0.42\nTLP\nBGRL\n81.42±1.05\n90.53±0.71\n72.05±0.86\n83.64±0.63\n64.20±1.10\n81.72±0.85\n53.16±0.82\n73.70±0.66\n44.57±0.54\n65.13±0.47\nSUGRL\n63.32±1.19\n86.35±0.78\n54.81±0.77\n73.10±0.63\n54.76±1.06\n78.12±0.92\n46.51±0.80\n68.41±0.71\n36.08±0.52\n57.78±0.49\nAFGRL\n78.12±1.13\n89.82±0.73\n71.12±0.81\n83.88±0.63\n59.07±1.07\n81.15±0.85\n50.71±0.85\n73.87±0.66\n43.10±0.56\n65.44±0.48\nVNT\n65.09±1.23\n85.86±0.76\n62.43±0.81\n80.87±0.63\n56.69±1.22\n78.02±0.97\n49.98±0.83\n70.51±0.73\n42.10±0.53\n60.99±0.50\nTable 16. Overall averaged FSNC accuracy (%) with 95% confidence intervals on product networks (Full Version, OOT: Out Of Time,\nwhich means that the training was not finished in 24 hours, OOM: Out Of Memory on NVIDIA RTX A6000)\nDataset\nCora-full\nDBLP\nSetting\n5 way\n10 way\n20 way\n5 way\n10 way\n20 way\nBase Model\nEpisode Generation\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\n1 shot\n5 shot\nMAML\nSupervised\n59.28±1.21\n70.30±0.99\n44.15±0.81\n57.59±0.66\n30.99±0.43\n46.80±0.38\n72.48±1.22\n80.30±1.03\n60.08±0.90\n69.85±0.76\n46.12±0.53\n57.30±0.48\nNAQ-FEAT (Ours)\n64.64±1.16\n74.31±0.94\n49.86±0.78\n64.88±0.64\n38.90±0.46\n53.87±0.43\n68.49±1.23\n77.31±1.08\n55.70±0.88\n67.94±0.82\n44.18±0.53\n56.50±0.48\nNAQ-DIFF (Ours)\n62.93±1.17\n76.48±0.92\n50.10±0.83\n63.50±0.66\n38.09±0.45\n54.08±0.41\n71.14±1.15\n79.47±1.01\n59.18±0.91\n70.19±0.78\n44.94±0.57\n58.68±0.47\nProtoNet\nSupervised\n58.61±1.21\n73.91±0.93\n44.54±0.79\n62.15±0.64\n32.10±0.42\n50.87±0.40\n73.80±1.20\n81.33±1.00\n61.88±0.86\n73.02±0.74\n48.70±0.52\n62.42±0.45\nNAQ-FEAT (Ours)\n64.20±1.11\n79.42±0.80\n51.78±0.75\n68.87±0.60\n40.11±0.45\n58.48±0.40\n71.38±1.17\n82.34±0.94\n58.41±0.86\n72.36±0.73\n47.30±0.53\n61.61±0.46\nNAQ-DIFF (Ours)\n65.30±1.08\n79.66±0.79\n51.80±0.78\n69.34±0.63\n40.76±0.49\n59.35±0.40\n73.89±1.15\n82.24±0.98\n59.43±0.79\n72.85±0.76\n48.17±0.52\n61.66±0.48\nTENT\nSupervised\n61.30±1.18\n77.32±0.81\n47.30±0.80\n66.40±0.62\n36.40±0.45\n55.77±0.39\n74.01±1.20\n82.54±1.00\n62.95±0.85\n73.26±0.77\n49.67±0.53\n61.87±0.47\nNAQ-FEAT (Ours)\n64.04±1.14\n78.48±0.79\n51.31±0.77\n67.09±0.62\n40.04±0.48\n56.15±0.40\n72.85±1.20\n80.91±1.00\n60.70±0.87\n71.98±0.79\n47.29±0.53\n61.01±0.46\nNAQ-DIFF (Ours)\n61.85±1.12\n77.26±0.84\n49.80±0.76\n67.65±0.63\n37.78±0.45\n56.55±0.41\n76.58±1.18\n82.86±0.95\n64.31±0.87\n74.06±0.75\n51.62±0.54\n63.05±0.45\nG-Meta\nSupervised\n59.88±1.26\n75.36±0.86\n44.34±0.80\n59.59±0.66\n33.25±0.42\n49.00±0.39\n74.64±1.20\n79.96±1.08\n61.50±0.88\n70.33±0.77\n46.07±0.52\n58.38±0.47\nNAQ-FEAT (Ours)\n65.79±1.21\n79.21±0.82\n48.90±0.80\n63.96±0.61\n40.36±0.46\n55.17±0.43\n70.08±1.24\n80.79±0.97\n57.98±0.87\n71.18±0.75\n45.65±0.52\n59.38±0.46\nNAQ-DIFF (Ours)\n62.96±1.14\n77.31±0.87\n47.93±0.79\n63.18±0.61\n37.55±0.46\n54.23±0.41\n70.39±1.20\n80.47±1.03\n57.55±0.85\n69.59±0.78\n44.56±0.52\n58.66±0.45\nGLITTER\nSupervised\n55.17±1.18\n69.33±0.96\n42.81±0.81\n52.76±0.68\n30.70±0.41\n40.82±0.41\n73.50±1.25\n75.90±1.19\nOOT\nOOT\nOOM\nOOM\nNAQ-FEAT (Ours)\n62.66±1.12\n76.40±0.87\n50.05±0.79\n67.66±0.61\n40.16±0.47\n57.13±0.42\n64.55±1.18\n78.54±1.10\nOOT\nOOT\nOOM\nOOM\nNAQ-DIFF (Ours)\n54.58±1.14\n70.59±0.93\n47.62±0.74\n64.58±0.65\n38.91±0.46\n52.70±0.41\n63.44±1.21\n75.79±1.06\nOOT\nOOT\nOOM\nOOM\nCOSMIC\nSupervised\n62.24±1.15\n73.85±0.83\n47.85±0.77\n59.11±0.60\n42.25±0.43\n47.28±0.38\n72.34±1.18\n80.83±1.03\n59.21±0.80\n70.67±0.71\n49.52±0.51\n59.01±0.42\nNAQ-FEAT (Ours)\n66.30±1.15\n80.09±0.79\n52.23±0.73\n68.63±0.61\n44.13±0.47\n60.94±0.36\n73.55±1.16\n82.36±0.94\n58.81±0.80\n71.14±0.70\n50.42±0.52\n64.90±0.43\nNAQ-DIFF (Ours)\n66.26±1.15\n80.07±0.79\n52.17±0.74\n68.95±0.60\n44.12±0.47\n60.97±0.37\n73.82±1.16\n82.29±0.94\n58.81±0.80\n71.10±0.70\n50.47±0.52\n64.78±0.44\nTLP\nBGRL\n62.59±1.13\n78.80±0.80\n49.43±0.79\n67.18±0.61\n37.63±0.44\n56.26±0.39\n73.92±1.19\n82.42±0.95\n60.16±0.87\n72.13±0.74\n47.00±0.53\n60.57±0.45\nSUGRL\n55.42±1.08\n76.01±0.84\n44.66±0.74\n63.69±0.62\n34.23±0.41\n52.76±0.40\n71.27±1.15\n81.36±1.02\n58.85±0.81\n71.02±0.78\n45.71±0.49\n59.77±0.45\nAFGRL\n55.24±1.02\n75.92±0.83\n44.08±0.70\n64.42±0.62\n33.88±0.41\n53.83±0.39\n71.18±1.17\n82.03±0.94\n58.70±0.86\n71.14±0.75\n45.99±0.53\n60.31±0.45\nVNT\n47.53±1.14\n69.94±0.89\n37.79±0.69\n57.71±0.65\n28.78±0.40\n46.86±0.40\n58.21±1.16\n76.25±1.05\n48.75±0.81\n66.37±0.77\n40.10±0.49\n55.15±0.46\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-06-27",
  "updated": "2024-05-21"
}