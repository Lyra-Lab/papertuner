{
  "id": "http://arxiv.org/abs/2306.15337v1",
  "title": "Homological Neural Networks: A Sparse Architecture for Multivariate Complexity",
  "authors": [
    "Yuanrong Wang",
    "Antonio Briola",
    "Tomaso Aste"
  ],
  "abstract": "The rapid progress of Artificial Intelligence research came with the\ndevelopment of increasingly complex deep learning models, leading to growing\nchallenges in terms of computational complexity, energy efficiency and\ninterpretability. In this study, we apply advanced network-based information\nfiltering techniques to design a novel deep neural network unit characterized\nby a sparse higher-order graphical architecture built over the homological\nstructure of underlying data. We demonstrate its effectiveness in two\napplication domains which are traditionally challenging for deep learning:\ntabular data and time series regression problems. Results demonstrate the\nadvantages of this novel design which can tie or overcome the results of\nstate-of-the-art machine learning and deep learning models using only a\nfraction of parameters.",
  "text": "Homological Neural Networks\nA Sparse Architecture for Multivariate Complexity\nYuanrong Wang 1 2 Antonio Briola 1 2 Tomaso Aste 1 2 3\nAbstract\nThe rapid progress of Artificial Intelligence re-\nsearch came with the development of increas-\ningly complex deep learning models, leading to\ngrowing challenges in terms of computational\ncomplexity, energy efficiency and interpretabil-\nity. In this study, we apply advanced network-\nbased information filtering techniques to design\na novel deep neural network unit characterized\nby a sparse higher-order graphical architecture\nbuilt over the homological structure of underly-\ning data. We demonstrate its effectiveness in two\napplication domains which are traditionally chal-\nlenging for deep learning: tabular data and time\nseries regression problems. Results demonstrate\nthe advantages of this novel design which can\ntie or overcome the results of state-of-the-art ma-\nchine learning and deep learning models using\nonly a fraction of parameters. The code and the\ndata are available at https://github.com/\nFinancialComputingUCL/HNN.\n1. Introduction\nComputational processes can be viewed as mapping opera-\ntions from points or regions in space into points or regions\nin another space with different dimensionality and proper-\nties. Neural networks process information through stacked\nlayers with different dimensions to efficiently represent the\ninherent structure of the underlying data. Uncovering this\nstructure is however challenging since it is typically an un-\nknown priori. Nevertheless, studying dependencies among\nvariables in a dataset makes it possible to characterise the\n1Department of Computer Science, University College London,\nLondon, UK 2UCL Centre for Blockchain Technologies, Lon-\ndon, UK 3Systemic Risk Centre, London School of Economics,\nLondon, United Kingdom. Correspondence to: Tomaso Aste\n<t.aste@ucl.ac.uk>.\nProceedings of the 2 nd Annual Workshop on Topology, Algebra,\nand Geometry in Machine Learning (TAG-ML) at the 40 th In-\nternational Conference on Machine Learning, Honolulu, Hawaii,\nUSA. 2023. Copyright 2023 by the author(s).\nstructural properties of the data and shape ad-hoc deep learn-\ning architectures on it. Specifically, the basic operation in\ndeep neural networks consists of aggregating input signals\ninto one output. This operation is most effective in scenar-\nios where the spatial organization of the variables is a good\nproxy for dependency. However, in several real-world com-\nplex systems, modelling dependency structures requires the\nusage of a complex network representation. Graph Neural\nNetworks have been introduced as one possible way to ad-\ndress this issue (Samek et al., 2021). However, they present\ntwo main limits: (i) they are designed for data defined on\nnodes of a graph (Yang et al., 2022), and (ii) they usually\nonly explicitly consider low-order interactions as geometric\npriors (edges connecting two nodes), ignoring higher-order\nrelations (triangles, tetrahedra, . . . ). Instead, dependency is\nnot simply a bi-variate relation between couples of variables\nand involves groups of variables with complex aggregation\nlaws.\nIn this work, we propose a novel deep learning architec-\nture that keeps into account higher-order interactions in the\ndependency structure as topological priors. Higher-order\ngraphs are networks that connect not only vertices with\nedges (i.e. low-order 1-dimensional simplexes) but also\nhigher-order simplexes (Torres & Bianconi, 2020). Indeed,\nany higher-order component can be described as a combina-\ntion of lower-order components (i.e. edges connecting two\nvertices, triangles connecting three edges, . . . ). The study\nof networks in terms of the relationship between structures\nat different dimensionality is a form of homology. In this\nwork, we propose a novel multi-layer deep learning unit\ncapable of fully representing the homological structure of\ndata and we name it Homological Neural Network (HNN).\nThis is a feed-forward unit where the first layer represents\nthe vertices, the second the edges, the third the triangles,\nand so on. Each layer connects with the next homological\nlevel accordingly to the network’s topology representing\ndependency structures of the underlying input dataset. Infor-\nmation only flows between connected structures at different\norder levels, and homological computations are thus ob-\ntained. Neurons in each layer have a residual connection to\na post-processing readout unit. HNN’s weights are updated\nthrough backward propagation using a standard gradient\ndescent approach. Given the higher-order representation of\n1\narXiv:2306.15337v1  [cs.LG]  27 Jun 2023\nHomological Neural Networks\nthe dependency structure in the data, this unit should pro-\nvide better computational performances than those of fully\nconnected multi-layer architectures. Furthermore, given the\nnetwork representation’s intrinsic sparsity, this unit should\nbe computationally more efficient, and results should be\nmore intuitive to interpret. We test these hypotheses by\nevaluating the HNN unit on two application domains tradi-\ntionally challenging for deep learning models: tabular data\nand time series regression problems.\nThis work builds upon a vast literature concerning com-\nplex network representation of data dependency structures\n(Costa-Santos et al., 2011; Moyano, 2017). Networks are\nexcellent tools for representing complex systems both math-\nematically and visually, they can be used for both qualita-\ntively describing the system and quantitatively modeling the\nsystem properties. A dense graph with everything connected\nwith everything else (complete graph) does not carry any\ninformation, conversely, too sparse representations are over-\nsimplifications of the important relations. There is a growing\nrecognition that, in most practical cases, a good represen-\ntation is provided by structures that are locally dense and\nglobally sparse. In this paper we use a family of network rep-\nresentations, named Information Filtering Networks (IFNs),\nthat have been proven to be particularly useful in data-driven\nmodeling (Tumminello et al., 2005; Barfuss et al., 2016;\nBriola & Aste, 2023). The proposed methodology exploits\nthe power of a specific class of IFNs, namely the Triangu-\nlated Maximally Filtered Graph (TMFG), which is a max-\nimally planar chordal graph with a clique-three structure\nmade of tetrahedra (Massara et al., 2017). The TMFG is\na good compromise between sparsity and density and it is\ncomputationally efficient to construct. It has the further\nadvantage of being chordal (every cycle of four or more\nvertices has a chord) which makes it possible to directly\nimplement probabilistic graphical modeling on its structure\n(Barfuss et al., 2016).\nThe rest of the paper is organised as follows. We first review,\nin Section 2, the relevant literature. Then, in Section 3, we\nintroduce a novel representation for higher-order networks,\nthe founding stone of HNNs. The design of HNN as a\nmodular unit of a deep learning architecture is discussed\nin Section 4. Application of HNN-based architectures to\ntabular data experiment on Penn Machine Learning Bench-\nmark and to multivariate time-series on solar-energy power\nand exchange-rates datasets are discussed in Section 5.1 and\nSection 5.2. Conclusions are provided in Section 6.\n2. Background literature\n2.1. Information Filtering Networks\nThe construction of sparse network representations of com-\nplex datasets has been a very active research domain during\nthe last two decades. There are various methodologies and\npossibilities to associate data with network representations.\nThe overall idea is that in a complex dataset, each variable\nis represented by a vertex in the network, and the interaction\nbetween variables is associated with the network structure.\nNormally such a network representation is constructed from\ncorrelations or (non-linear) dependency measures (i.e. mu-\ntual information) and the network is constructed in such\na way as to retain the largest significant dependency in\nits interconnection structure. These networks are known\nas Information Filtering Networks (IFN) with one of the\nbest-known examples being the Minimum Spanning Tree\n(MST) (Neˇsetˇril et al., 2001) built from pure correlations\n(Mantegna, 1998). The MST has the advantage of being\nthe sparsest connected network and of being the exact solu-\ntion for some optimization problems (Kruskal, 1956; Prim,\n1957). However, other IFNs based on richer topological em-\nbeddings, such as planar graphs (Tumminello et al., 2005;\nAste & Matteo, 2017) or clique trees and forests (Massara\net al., 2017; Massara & Aste, 2019), can extract more valu-\nable information and better represent the complexity of the\ndata. These network constructions have been employed\nacross diverse research domains from finance (Barfuss et al.,\n2016) to brain studies (Telesford et al., 2011), and psychol-\nogy (Christensen et al., 2016). In this paper, we use the\nTriangulated Maximally Filtered Graph (TMFG) (Massara\net al., 2017), which is a planar and chordal IFN. It has the\nproperty of being computationally efficient and it can yield\na sparse precision matrix with the structure of the network,\nthereby being a tool for L0-norm topological regularization\nin multivariate probabilistic models (Aste, 2022).\n2.2. Sparse neural networks\nRecent advances in artificial intelligence have exacerbated\nthe challenges related to models’ computational and energy\nefficiency. To mitigate these issues, researchers have pro-\nposed new architectures characterized by fewer parameters\nand sparse structures. Some of them have successfully re-\nduced the complexity of very large models to drastically\nimprove efficiency with negligible performance degradation\n(Ye et al., 2018; Molchanov et al., 2016; Lee et al., 2018;\nYu et al., 2017; Anwar et al., 2015; Molchanov et al., 2017;\nZhuo et al., 2018; Wang et al., 2018). Others have not only\nsimplified the architectures but also enhanced models’ ef-\nficacy, further demonstrating that fewer parameters yield\nbetter model generalization (Wu et al., 2020; Wen et al.,\n2016; Liu et al., 2015; 2017; Hu et al., 2016; Zhuang et al.,\n2018; Peng et al., 2019; Louizos et al., 2017).\nNonetheless, in the majority of literature, sparse topological\nconnectivity is pursued either after the training phase, which\nbears benefits only during the inference phase, or during the\nback-propagation phase which usually adds complexity and\nrun-time to the training. A very first attempt to solve these\n2\nHomological Neural Networks\nissues is represented by network-inspired pruning methods\nincorporated pruning at the earliest stage of the building\nprocess, allowing for the establishment of a foundational\ntopological architecture that can then be elaborated upon\n(Stanley & Miikkulainen, 2002; Hausknecht et al., 2014;\nMocanu et al., 2017). However, the most interesting so-\nlution is represented by Simplicial NNs (Ebli et al., 2020)\nand Simplicial CNNs (Yang et al., 2022). Indeed, these\narchitectures constitute the very first attempt to exploit the\ntopological properties of sparse graph representations to\ncapture higher-order data relationships. Despite their nov-\nelty, the design of these neural network architectures limits\nthem to pre-designed network data, without the possibility\nto easily scale to more general data types (e.g., tabular data\nand time series).\nIn this paper, we incorporate topological constraints within\nthe design phase of the network architecture, generating a\nmore intricate sparse topology derived from IFNs (Briola\net al., 2022; Briola & Aste, 2022; 2023; Vidal-Tomas et al.,\n2023).\n2.3. Deep Learning models for tabular data\nThroughout the previous ten years, conventional machine\nlearning algorithms, exemplified by gradient-boosted deci-\nsion trees (GBDT) (Chen & Guestrin, 2016), have predomi-\nnantly governed the landscape of tabular data modelling, ex-\nhibiting superior efficacy compared to deep learning method-\nologies. Although the encouraging results presented in the\nliterature (Shwartz-Ziv et al., 2018; Poggio et al., 2020; Pi-\nran et al., 2020), deep learning tends to encounter significant\nhurdles when implemented on tabular data. The works of\n(Arik & Pfister, 2019) and (Hollmann et al., 2022) claim\nto achieve comparable results to tree models, but they are\nall very large attention/transformer-based models. Indeed,\ntabular data manifest a range of peculiar issues such as non-\nlocality, data sparsity, heterogeneity in feature types, and\nan absence of a priori knowledge about underlying depen-\ndency structures. Therefore, tree ensemble methodologies,\nsuch as XGBoost, are still deemed as the optimal choice\nfor tackling real-world tabular data related tasks (Friedman,\n2001; Prokhorenkova et al., 2018; Grinsztajn et al., 2022).\nIn this work, we propose a much more efficient sparse deep-\nlearning model with similar results.\n2.4. Deep Learning models for multivariate time-series\nExisting research in multivariate time series forecasting can\nbe broadly divided into two primary categories: statistical\nmethods and deep learning-based methods. Statistical ap-\nproaches usually assume linear correlations among variables\n(i.e., time series) and use their lagged dependency to fore-\ncast through a regression, as exemplified by the vector auto-\nregressive model (VAR) (Zivot & Wang, 2003) and Gaussian\nprocess model (GP) (Roberts et al., 2012). In contrast, deep\nlearning-based methods, such as LSTNet (Lai et al., 2017)\nand TPA-LSTM (Shih et al., 2018), utilize Convolutional\nNeural Networks (CNN) to identify spatial dependencies\namong variables and combine them with Long Short-Term\nMemory (LSTM) networks to process the temporal informa-\ntion. Despite they have been widely applied across various\napplication domains, including finance (Lu et al., 2020) and\nweather data (Wan et al., 2019), these architectures do not\nexplicitly model dependency structures among variables,\nbeing strongly limited on the interpretability side.\nRecently, spatio-temporal graph neural networks (STGNNs)\n(Shao et al., 2022a;b) have attracted interest reaching state-\nof-the-art performances, as exemplified by MTGNN (Wu\net al., 2020). STGNNs integrate graph convolutional net-\nworks and sequential recurrent models, with the former\naddressing non-Euclidean dependencies among variables\nand the latter capturing temporal patterns. The inclusion of\nadvanced convolutional or aggregational layers accounting\nfor sparsity and higher-order interactions has resulted in\nfurther improvements of STGNNs (Wang & Aste, 2022;\nCalandriello et al., 2018; Chakeri et al., 2016; Rong et al.,\n2020; Hasanzadeh et al., 2020; Zheng et al., 2020; Luo et al.,\n2021; Kim & Oh, 2021). In this paper, we use the HNN\nunit as an advanced aggregational module to extract the\ndependency structure of variables from the temporal signals\ngenerated from LSTMs.\n3. A novel representation for higher order\nnetworks and its use for HNN construction\nThe representation of undirected graphs explicitly accounts\nfor the vertices and their connections through edges and,\ninstead, does not explicitly account for other, higher-order,\nstructures such as triangles, tetrahedra, and, in general, d-\ndimensional simplexes. Indeed, usually, an undirected graph\nis represented as a pair of sets, G = (V, E): the vertex set\nV = (v1, ..., vp) and the edge set E which is made of\npairs of edges (vi, vj). The associated graphical represen-\ntation is a network where vertices, represented as points,\nare connected through edges, represented as segments. This\nencoding of the structure accounts only for the edges skele-\nton of the network. However, in many real-world scenarios,\nhigher-order sub-structures are crucial for the functional\nproperties of the network and it is therefore convenient –\nand sometimes essential – to use a representation that ac-\ncounts for them explicitly.\nA simple higher-order representation can be obtained by\nadding triplets (triangles), quadruplets (tetrahedra), etc. to\nthe sets in G. However, the associated higher-order network\nis hard to handle both visually and computationally. In this\npaper, we propose an alternative approach, which consists\nof a layered representation that explicitly takes into account\n3\nHomological Neural Networks\n1\n2\n3\n4\n5\n6\n7\n5\n6\n1\n4\n4\n2\n3\n6\n4\n6\n7\n4\n4\n4,6\n4,6\n(a)\n(b)\nsimplexes\nseparators\nFigure 1. (a) Visual example of a higher order network made\nof 7 vertices, 11 edges, 6 triangles, and 1 tetrahedron. (b) This\nhigher-order network is a clique tree made of four cliques (max-\nimal cliques highlighted in the circles) connected through three\nseparators (the tick red edges). One can observe that the separator\nconstituted by the vertex ‘4’ has multiplicity 1, while the separator\nconstituted of the edge ’4-6’ has multiplicity 2 and indeed it ap-\npears twice.\nthe higher order sub-structures and their interconnections.\nSuch a representation is very simple, highly intuitive, of\npractical applicability as computational architecture, and,\nto the best of our knowledge, it has never been proposed\nbefore.\nThe proposed methodology is entirely based on a special\nclass of networks: chordal graphs. These networks are con-\nstituted only of cliques organized in a higher order tree-like\nstructure (also referred to as ‘clique tree’). This class of net-\nworks is very broad and it has many useful applications, in\nparticular for probabilistic modeling (Aste, 2022). A visual\nexample of a higher-order chordal network (a clique-tree),\nwith 7 vertices, 11 edges, 6 triangles, and 1 tetrahedron, is\nprovided in Figure 1. In the figure, the maximal cliques\n(largest fully-connected subgraphs) are highlighted and re-\nported, in the right panel, as clique-tree nodes. Such nodes\nare connected to each other with links that are sub-cliques\ncalled separators. Separators have the property that, if re-\nmoved from the network, they disconnect it into a number\nof components equal to the multiplicity of the separator\nminus one. In higher-order networks, cliques are the edge\nskeletons of simplexes. A 2-clique is a 1-dimensional\nsimplex (an edge); 3-clique is a 2-dimensional simplex (a\ntriangle); and so on with (d + 1)-cliques being the skeleton\nof d-dimensional simplexes.\nTo represent the complexity of a higher-order network, we\npropose to adopt a layered structure (i.e. the Hasse diagram)\nwhere nodes in layer d represent d-dimensional simplexes.\nThe structures start with the vertices in layer 0; then a couple\nof vertices connect to edges represented in layer 1; edges\nconnect to triangles in layer 2; triangles connect into tetra-\nhedra in layer 3, and so on. This is illustrated in Figure 2.\nSuch representation has a one-to-one correspondence with\nthe original network but shows explicitly the simplexes and\nsub-simplexes and their interconnection in the structure. All\ninformation about the network at all dimensions is explic-\nitly encoded in this representation including elements such\nas maximal cliques, separators, and their multiplicity (see\ncaption of Figure 2).\nFigure 2. Higher order homological representation of the chordal\ngraph in Figure 1 (reproduced in (a)). (b) Nodes in each layer,\nLd, represent the d-dimensional simplexes in the structure. The\nlinks between nodes in layers d and d + 1 are the connections\nbetween d to d + 1 simplexes in the network. The degree on the\nleft of nodes in Ld is always equal to d. The degree on the right\nof nodes in Ld can instead vary. The d-dimensional simplexes\nwith no connections towards d + 1 are the maximal cliques in the\nnetwork (i.e. the nodes in the clique tree in Figure 1(b)).\nFigure 3. The Homological Neural Network (HNN) unit is con-\nstructed by using as input layer 0 of the homological representation\nof the dependency structure (see Figure 2(b)) and then feeding for-\nward through the homological layers. The output is produced by a\nreadout unit that connects all neurons in the layers. The HNN is\nessentially a sparse MLP unit with residual connections.\nIt is worth noting the resemblance of this layered structure\nwith the layered architecture of deep neural networks. In-\n4\nHomological Neural Networks\ndeed, we leverage this novel higher-order network represen-\ntation as the neural network architecture of the HNN unit. In\nour experiments, the HNN is implemented from the TMFG\ngenerated from correlations. TMFG is computationally effi-\ncient, and can thus be used to dynamically re-configure the\nHNN according to changeable system conditions (Wang &\nAste, 2022). The HNN architecture is illustrated in Figure 3.\nEssentially it is made by the layered representation of Figure\n2 with the addition of the residual connections linking each\nneuron in each simplex layer to a final read-out layer. Such\nHNN is a sparse MLP-like neural network with extra resid-\nual connections and it can be employed as a modular unit. It\ncan directly replace fully connected MLP layers in several\nneural network architectures. In this paper, the HNN unit\nis implemented using the standard PyTorch deep learning\nframework, while the sparse connection between layers is\nobtained thorugh the “sparselinear”1 PyTorch library.\n4. Design of neural network architectures with\nHNN units for tabular data and time series\nstudies\nWe investigate the performances of HNN units in two tradi-\ntionally challenging application domains for deep learning:\ntabular data and time series regression problems. To pro-\ncess tabular data, the HNN unit can be directly fed with the\ndata and it can be constructed from correlations by using\nthe TMFG. In this case, the HNN unit acts as a sparsified\nMLP. This architecture is schematically shown in Figure 4.\nInstead, in spatio-temporal neural networks, the temporal\nlayers are responsible for handling temporal patterns of in-\ndividual series, whereas the spatial component learns their\ndependency structures. Consequently, the temporal part\nis usually modeled through the usage of recurrent neural\nnetworks (e.g. RNNs, GRUs, LSTMs), while the spatial\ncomponent employs convolutional layers (e.g. CNNs) or\naggregation functions (e.g. MLPs, GNNs).\nFigure 5 presents the spatio-temporal neural network archi-\ntecture employed in our multivariate time series experiments.\nThe architecture consists of an LSTM for the temporal en-\ncoding of each time series and a graph generation unit that\ntakes into account the correlation between different time\nseries. This unit models time series as nodes and pairwise\ncorrelations as edges by imposing the topological constraints\ntypical of the TMFG: planarity and chordality. The HNN\nis built based on the resulting sparse TMFG and aggregates\neach of the encoded time series from the LSTM, generating\nthe final output.\n1https://github.com/hyeon95y/SparseLinear\nFigure 4. HNN architecture for tabular data. The tabular data is\nprocessed by a Graph Generation Unit to construct a prior sparse\ngraph to represent spatial interdependencies between the feature\ncolumns. The prior graph guides the design of the HNN unit which\nthen processes and transforms the feature columns into the final\noutput.\nFigure 5. LSTM-HNN architecture for time-series data. The mul-\ntivariate time-series is processed by a Graph Generation Unit to\nconstruct a prior sparse graph to represent spatial interdependen-\ncies, and each of the multivariate time series is processed separately\nby LSTM in the Temporal Convolution Module to harness the tem-\nporal information. The prior graph guides the design of the HNN\nunit which then aggregates the single temporal representations\nfrom LSTMs into the final output.\n5. Results\n5.1. Tabular Data\nWe test performances of the HNN architecture on the Penn\nMachine Learning Benchmark (PMLB) (Romano et al.,\n2021) regression dataset. We select datasets with more\nthan 10,000 samples, and we split each of them into a 70%\ntraining and 30% testing set.\nThe R2 scores for the HNN architecture are reported in\nTable 1 for groups of PMLB regression datasets with dif-\nferent number of variables. We first compare HNN perfor-\nmances with the ones achieved by a Multi-Layer Perceptron\n(MLP) with same depth (i.e. 4 as imposed by the TMFG\nin the HNN’s building process). We also test a sparse MLP\n(MLP-HNN) with the same sparse structure of the HNN\nbut without the residual connections from each layer to\nthe final read-out layer, and a standard MLP with residual\nconnections to the final read-out layer, MLP-res. All these\narchitectures are optimized using gradient descent for the\n5\nHomological Neural Networks\nparameters and grid search for the hyper-parameters. It is\nevident from Table 1 that the HNN architecture largely out-\nperforms the other neural network models. It is also evident\nthat the homological structure of HNN is the main factor\nassociated with improved performances (indeed MLP-HNN\noutperforms MLP) while the residual connections between\nlayers are not the factor that makes HNN best performing\n(indeed MLP-res does not outperforms HNN).\nIt is commonly acknowledged that neural network models\ndo not perform well on tabular data (Borisov et al., 2021);\ntree-based models and the gradient boosting framework rep-\nresent instead the state-of-the-art (Shwartz-Ziv & Armon,\n2022; Grinsztajn et al., 2022). We therefore compare the\nHNN results with baseline models including Linear Regres-\nsion (LM), Random Forest (RF), Light Gradient Boosting\nMachine (LGBM), Extreme Gradient Boosting Machine\n(XGB) (Ke et al., 2017; Chen & Guestrin, 2016). The ex-\nperiments are performed on the same datasets using the\noptimization and tuning pipeline descirbed above.\nTable 2 and Figure 6 report the comparison between HNN\nresults and the machine learning methods on the PMLB\nregression datasets. We underline that HNN outperforms\ntraditional machine learning methods and nearly matches\nthe state-of-the-art. Furthermore, the relative performance\nof HNN improves with the number of variables, notably\nwith HNN obtaining equivalent and marginally better per-\nformance even than XGB for the datasets with large number\nof features (see Figure 6(d)).\nFigure 6. The R2 score from different models on PMLB (Penn\nMachine Learning Benchmarks) regression datasets.\n(a) All\ndatasets (46 datasets).\n(b) Datasets with number of variable\n∈[0, 20) (32 datasets). (c) Datasets with number of variable\n∈[20, 40) (8 datasets). (d) Datasets with number of variable\n∈[40, inf) (6 datasets).\n5.2. Multivariate Time-series Data\nThe HNN module can be used as a portable component\nalong with different types of neural networks to manage\nvarious input data structures and downstream tasks. In this\nSection, we apply HNN to process dependency structures\nin time series modelling after temporal dependencies are\nhandled through the LSTM architecture. We use two dif-\nferent datasets which have been extensively investigated in\nthe multivariate time-series literature (Wu et al., 2020): the\nsolar-energy dataset from the National Renewable Energy\nLaboratory, which contains the solar-energy power output\ncollected from 137 PV plants in Alabama State in 2007;\nand a financial dataset containing the daily exchange-rates\nrates of eight foreign countries including Australia, British,\nCanada, Switzerland, China, Japan, New Zealand, and Sin-\ngapore in the period from 1990 to 2016 (see Table 5 in\nAppendix for further details).\nAnalogously with the tabular data, we first compare the\noutcomes of LSTM-HNN with those obtained with adapted\nMLP units. Specifically, LSTM units plus an MLP (LSTM-\nMLP); LSTM units plus an MLP with added residual con-\nnections to the final read-out layer (LSTM-MLP-res); and\nLSTM units plus a sparse MLP of the same layout as\nHNN without residual connections (LSTM-MLP-HNN).\nWe then compare the LSTM-HNN results with traditional\nand state-of-the-art spatio-temporal models for multivariate\ntime-series problems: auto-regressive model (AR) (Zivot &\nWang, 2003); a hybrid model that exploits both the power\nof MLP and auto-regressive modelling (VARMLP) (Zhang,\n2003); a Gaussian process (GP) (Roberts et al., 2012); a\nrecurrent neural network with fully connected GRU hidden\nunits (RNN-GRU) (Wu et al., 2020); a LSTM recurrent neu-\nral network combined with a convolutional neural network\n(LSTNet) (Lai et al., 2017); a LSTM recurrent neural net-\nwork with attention mechanism (TPA-LSTM) (Shih et al.,\n2018); and a graph neural network with temporal and graph\nconvolution (MTGNN) (Wu et al., 2020).\nWe evaluate performances of the LSTM-HNN and com-\npare them with the ones achieved by benchmark method-\nologies by forecasting the solar-energy power outputs and\nthe exchange-rates values at different time horizons with\nperformances measured in terms of relative standard error\n(RSE) and correlation (CORR) (see Table 3). We underline\nthat LSTM-HNN significantly outperforms all MLP-based\nmodels. On solar-energy data, LSTM-HNN reduces RSE\nby 38%, 25%, 17%, and 36% from LSTM-MLP and 8%,\n7%, 3%, and 2% from LSTM-MLP-res across four horizons.\nOn exchange-rates data, LSTM-HNN reduces RSE by 23%,\n28%, 26%, and 13% from LSTM-MLP and 19%, 20%, 14%,\nand 10% from LSTM-MLP-res across four horizons.\nWe also notice that the residual connections from each layer\nto the final read-out layer are effective both in the HNN\n6\nHomological Neural Networks\n# variable ∈[0, 20)\n# variable ∈[20, 40)\n# variable > 40\nmean\n10th\n50th\n90th\nmean\n10th\n50th\n90th\nmean\n10th\n50th\n90th\nHNN\n0.70∗∗∗\n0.45\n0.78\n0.93\n0.75∗∗\n0.55\n0.78\n0.91\n0.89∗∗\n0.78\n0.92\n0.96\nMLP-HNN\n-9.64\n-8.97\n0.01\n0.82\n0.21\n-0.01\n0.03\n0.56\n0.55\n0.14\n0.54\n0.96\nMLP-res\n-5.14\n0.02\n0.79\n0.94\n0.40\n0.01\n0.27\n0.84\n0.32\n0.01\n0.19\n0.75\nMLP\n-7.18\n-0.63\n0.19\n0.87\n0.09\n-0.01\n-0.00\n0.22\n0.12\n-0.14\n-0.00\n0.50\nTable 1. R2 score from different models on PMLB regression dataset, with different number of variables. The best-performing average\nresult is highlighted in bold, and ∗denotes 1% significance, ∗∗for 0.1% and ∗∗∗for 0.001% respectively from paired T-test of the second\nbest performing model result against HNN result. We also report the 10% 50% and 90% quantiles.\n# variable ∈[0, 20)\n# variable ∈[20, 40)\n# variable > 40\nmean\n10th\n50th\n90th\nmean\n10th\n50th\n90th\nmean\n10th\n50th\n90th\nHNN\n0.70\n0.45\n0.78\n0.93\n0.75\n0.55\n0.78\n0.91\n0.89\n0.78\n0.92\n0.96\nXGB\n0.80∗\n0.52\n0.85\n0.95\n0.91\n0.83\n0.92\n0.98\n0.92\n0.88\n0.92\n0.96\nLGBM\n0.65\n0.00\n0.81\n0.95\n0.89\n0.81\n0.91\n0.97\n0.91\n0.87\n0.91\n0.95\nRF\n0.78\n0.47\n0.85\n0.98\n0.87\n0.76\n0.89\n0.98\n0.89\n0.85\n0.87\n0.95\nLM\n0.53\n0.12\n0.64\n0.90\n0.36\n0.01\n0.28\n0.74\n0.34\n0.11\n0.25\n0.65\nTable 2. R2 score from different models on PMLB regression dataset, with a number of variables between 0 and 20, between 20 and 40,\nand larger than 40. The best-performing average result is highlighted in bold, and ∗denotes 1% significance from paired T-test of the\nbest-performing model result against HNN result. We also report the 10% 50% and 90% quantiles. The absence of ∗indicates statistical\nequivalence between the best-performing and HNN.\nsolar-energy\nexchange-rates\nHorizon (days)\nHorizon (days)\nModel\nMetrics\n3\n6\n12\n24\n3\n6\n12\n24\nLSTM-HNN\nRSE\n0.190∗\n0.270∗\n0.354∗\n0.446∗\n0.022∗\n0.027∗∗\n0.040∗\n0.049∗\nCORR\n0.981\n0.964∗\n0.942∗\n0.902∗∗\n0.976∗∗∗\n0.968∗∗\n0.956∗\n0.938∗\nLSTM-MLP-HNN\nRSE\n0.207\n0.292\n0.365\n0.454\n0.028\n0.034\n0.046\n0.054\nCORR\n0.980\n0.959\n0.936\n0.893\n0.965\n0.957\n0.945\n0.928\nLSTM-MLP-res\nRSE\n0.245\n0.340\n0.409\n0.501\n0.031\n0.035\n0.052\n0.059\nCORR\n0.972\n0.944\n0.905\n0.898\n0.850\n0.829\n0.835\n0.828\nLSTM-MLP\nRSE\n0.307\n0.361\n0.425\n0.697\n0.029\n0.037\n0.054\n0.056\nCORR\n0.956\n0.937\n0.898\n0.723\n0.845\n0.838\n0.834\n0.824\nTable 3. Relative Standard Error (RSE) and CORR (correlation). The best-performing results in a given metric and horizon are highlighted\nin bold. In addition, a paired T-test has been performed, and the p-values for the LSTM-HNN against the second-best-performing model\n(i.e. the LSTM-MLP-res) in the given metrics and horizon are highlighted next to the best-performing results, where ∗denotes 1%\nsignificance, ∗∗for 0.1% and ∗∗∗for 0.001% respectively.\narchitecture (i.e. LSTM-HNN outperforms LSTM-MLP-\nHNN) and within the MPL models (i.e. LSTM-MLP-res\noutperforms LSTM-MLP). In order to illustrate the signif-\nicance of the gain, a paired t-test of LSTM-HNN against\nLSTM-MLP-res has been performed revealing that all differ-\nences are significant at 1% or better with the only exception\nfor the correlation at horizon 3 in the solar-energy output\ndata.\n7\nHomological Neural Networks\nsolar-energy\nexchange-rates\nHorizon (days)\nHorizon (days)\nModel\nMetrics\n3\n6\n12\n24\n3\n6\n12\n24\nLSTM-HNN\nRSE\n0.190\n0.270\n0.354\n0.446\n0.022\n0.027\n0.040\n0.049\nCORR\n0.981\n0.964\n0.942\n0.902\n0.976\n0.968\n0.956\n0.938\nMTGNN\nRSE\n0.177∗\n0.234∗∗\n0.310∗\n0.427∗\n0.019\n0.025\n0.034\n0.045\nCORR\n0.985\n0.972∗\n0.950∗\n0.903\n0.978\n0.970\n0.955\n0.937\nTPA-LSTM\nRSE\n0.180\n0.234\n0.323\n0.438\n0.017∗\n0.024\n0.034\n0.044\nCORR\n0.985\n0.974\n0.948\n0.908∗\n0.979∗\n0.970\n0.956\n0.938\nLSTNet-skip\nRSE\n0.184\n0.255\n0.325\n0.464\n0.022\n0.028\n0.035\n0.044\nCORR\n0.984\n0.969\n0.946\n0.887\n0.973\n0.965\n0.951\n0.935\nRNN-GRU\nRSE\n0.193\n0.262\n0.416\n0.485\n0.019\n0.026\n0.040\n0.062\nCORR\n0.982\n0.967\n0.915\n0.882\n0.978\n0.971\n0.953\n0.922\nGP\nRSE\n0.225\n0.328\n0.520\n0.797\n0.023\n0.027\n0.039\n0.058\nCORR\n0.975\n0.944\n0.851\n0.597\n0.871\n0.819\n0.848\n0.827\nVARMLP\nRSE\n0.192\n0.267\n0.424\n0.684\n0.026\n0.039\n0.040\n0.057\nCORR\n0.982\n0.965\n0.905\n0.714\n0.860\n0.872\n0.828\n0.767\nAR\nRSE\n0.243\n0.379\n0.591\n0.869\n0.022\n0.027\n0.035\n0.044\nCORR\n0.971\n0.926\n0.810\n0.531\n0.973\n0.965\n0.952\n0.935\nTable 4. Relative Standard Error and correlation. The best-performing results in a given metric and horizon are highlighted in bold. In\naddition, a paired T-test has been performed, and the p-values for the best-performing result against LSTM-HNN in the given metrics and\nhorizon are highlighted next to the best-performing results, where ∗denotes 1% significance, ∗∗for 0.1% and ∗∗∗for 0.001% respectively.\nThe absence of ∗indicates statistical equivalence between the best-performing and LSTM-HNN models. When LSTM-HNN is the\nbest-performing result, then the t-test is conversely performed against the second best-performing result.\nThe comparison between the results for LSTM-HNN and\nthe other benchmark models is reported in Table 4. Results\nreveal that LSTM-HNN consistently outperforms all three\nnon-RNN-based methods (AR, VARMLP and GP) on both\ndatasets. It also outperforms LSTNet-skip results. LSTM-\nHNN outperforms RNN-GRU for all datasets and horizons\nexcept for the correlation in the exchange rates at horizon\n6 where it returns an equivalent result accordingly with the\npaired t-test that was conducted between LSTM-HNN and\nthe best-performing model. LSTM-HNN is instead slightly\noutperformed by MTGNN in most results for solar-power\nand by TPA-LSTM in several results for exchange-rates.\nIt must be however noticed that these are massive deep-\nlearning models with a much larger number of parameters\n(respectively 1.5 and 2.5 times larger than LSTM-HNN for\nthe solar-energy datasets and 10 and 26 times larger for the\nexchange-rates datasets, see Table 6).\n6. Conclusion\nIn this paper we introduce Homological Neural Networks\n(HNNs), a novel deep-learning architecture based on a\nhigher-order network representation of multivariate data\ndependency structures. This architecture can be seen as a\nsparse MLP with extra residual connections and it can be\napplied in place of any fully-connected MLP unit in com-\nposite neural network models. We test the effectiveness of\nHNNs on tabular and time-series heterogeneous datasets.\nResults reveal that HNN, used either as a standalone model\nor as a modular unit within larger models, produces better\nresults than MLP models with the same number of neurons\nand layers. We compare the performance of HNN with\nboth fully-connected MLP, MLP sparsified with the HNN\nlayered structure, and fully-connected MLP with additional\nresidual connections and read-out unit. We design an ex-\nperimental pipeline that verifies that the sparse higher-order\nhomological layered structure on which HNN is built is the\nmain element that eases the computational process. Indeed,\nwe verify that the sparsified MLP with the HNN structure\n(MLP-HNN) over-performs all other MLP models. We\nalso verify that the residual links between layers and the\nreadout unit consistently improve HNN performances. No-\nticeably, although residual connections also improve fully-\nconnected MLP performances, results are still inferior to\nthe ones achieved by sparse MLP-HNN. We demonstrate\nthat HNNs’ performances are in line with state-of-the-art\n8\nHomological Neural Networks\nbest-performing computational models, however, it must\nbe considered that they have a much smaller number of\nparameters, and their processing architecture is easier to\ninterpret.\nIn this paper, we build HNNs from TMFG networks com-\nputed on pure correlations. TMFG are very convenient\nchordal network representations that are computationally\ninexpensive and provide opportunities for dynamically self-\nadjusting neural network structures. Future research work\non HNN will focus on developing an end-to-end dynamic\nmodel that addresses the temporal evolution of variable in-\nterdependencies. TMFG is only one instance of a large class\nof chordal higher-order information filtering networks (Mas-\nsara & Aste, 2019) which can be used as priors to construct\nHNN units. The exploration of this larger class of possible\nrepresentations is a natural expansion of the present HNN\nconfiguration and will be pursued in future studies.\nAcknowledgements\nAll the authors acknowledge the members of the University\nCollege London Financial Computing and Analytics Group\nfor the fruitful discussions on foundational topics related to\nthis work. All the authors acknowledge the ICML TAG-ML\n2023 workshop organising committee and the reviewers for\nthe useful comments that improved the quality of the paper.\nThe author, T.A., acknowledges the financial support from\nESRC (ES/K002309/1), EPSRC (EP/P031730/1) and EC\n(H2020-ICT-2018-2 825215).\nReferences\nAnwar, S., Hwang, K., and Sung, W. Structured pruning\nof deep convolutional neural networks. ACM Journal on\nEmerging Technologies in Computing Systems (JETC),\n13:1 – 18, 2015.\nArik, S. ¨O. and Pfister, T. Tabnet: Attentive interpretable\ntabular learning. ArXiv, abs/1908.07442, 2019.\nAste, T. Topological regularization with information filter-\ning networks. Information Sciences, arXiv:2005.04692,\n2022.\nAste, T. and Matteo, T.\nSparse causality network re-\ntrieval from short time series. Complex., 2017:4518429:1–\n4518429:13, 2017.\nBarfuss, W., Massara, G. P., Di Matteo, T., and Aste, T. Par-\nsimonious modeling with information filtering networks.\nPhysical Review E, 94(6), Dec 2016. ISSN 2470-0053.\ndoi: 10.1103/physreve.94.062306. URL http://dx.\ndoi.org/10.1103/PhysRevE.94.062306.\nBorisov, V., Leemann, T., Sessler, K., Haug, J., Pawelczyk,\nM., and Kasneci, G. Deep neural networks and tabular\ndata: A survey. IEEE transactions on neural networks\nand learning systems, PP, 2021.\nBriola, A. and Aste, T. Dependency structures in cryptocur-\nrency market from high to low frequency. 2022.\nBriola, A. and Aste, T. Topological feature selection: A\ngraph-based filter feature selection approach.\nArXiv,\nabs/2302.09543, 2023.\nBriola, A., Vidal-Tom’as, D., Wang, Y., and Aste, T.\nAnatomy of a stablecoin’s failure: the terra-luna case.\nArXiv, abs/2207.13914, 2022.\nCalandriello, D., Koutis, I., Lazaric, A., and Valko, M. Im-\nproved large-scale graph learning through ridge spectral\nsparsification. In ICML, 2018.\nChakeri, A., Farhidzadeh, H., and Hall, L. O. Spectral\nsparsification in spectral clustering. 2016 23rd Interna-\ntional Conference on Pattern Recognition (ICPR), pp.\n2301–2306, 2016.\nChen, T. and Guestrin, C. Xgboost: A scalable tree boosting\nsystem. In Proceedings of the 22nd acm sigkdd inter-\nnational conference on knowledge discovery and data\nmining, pp. 785–794, 2016.\nChristensen, D. L., Baio, J., Braun, K. V. N., Bilder, D. A.,\nCharles, J. M., Constantino, J. N., Daniels, J. L., Durkin,\nM. S., Fitzgerald, R. T., Kurzius-Spencer, M., Lee, L. C.,\nPettygrove, S., Robinson, C. C., Schulz, E. G., Wells, C.,\nWingate, M. S., Zahorodny, W. M., and Yeargin-Allsopp,\nM. Prevalence and characteristics of autism spectrum\ndisorder among children aged 8 years — autism and de-\nvelopmental disabilities monitoring network, 11 sites,\nunited states, 2012. MMWR Surveillance Summaries, 65:\n1 – 23, 2016.\nCosta-Santos, C., Bernardes, J., Antunes, L. F. C., and\nde Campos, D. A. Complexity and categorical analysis\nmay improve the interpretation of agreement studies using\ncontinuous variables. Journal of evaluation in clinical\npractice, 17 3:511–4, 2011.\nEbli, S., Defferrard, M., and Spreemann, G. Simplicial\nneural networks. arXiv preprint arXiv:2010.03633, 2020.\nFriedman, J. H. Greedy function approximation: a gradient\nboosting machine. Annals of statistics, pp. 1189–1232,\n2001.\nGrinsztajn, L., Oyallon, E., and Varoquaux, G. Why do tree-\nbased models still outperform deep learning on tabular\ndata? ArXiv, abs/2207.08815, 2022.\nHasanzadeh, A., Hajiramezanali, E., Boluki, S., Zhou, M.,\nDuffield, N. G., Narayanan, K. R., and Qian, X. Bayesian\n9\nHomological Neural Networks\ngraph neural networks with adaptive connection sampling.\nArXiv, abs/2006.04064, 2020.\nHausknecht, M. J., Lehman, J., Miikkulainen, R., and Stone,\nP. A neuroevolution approach to general atari game play-\ning. IEEE Transactions on Computational Intelligence\nand AI in Games, 6:355–366, 2014.\nHollmann, N., Muller, S., Eggensperger, K., and Hutter, F.\nTabpfn: A transformer that solves small tabular classifi-\ncation problems in a second. 2022.\nHu, H., Peng, R., Tai, Y.-W., and Tang, C.-K. Network trim-\nming: A data-driven neuron pruning approach towards ef-\nficient deep architectures. ArXiv, abs/1607.03250, 2016.\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma,\nW., Ye, Q., and Liu, T.-Y. Lightgbm: A highly efficient\ngradient boosting decision tree. In NIPS, 2017.\nKim, D. and Oh, A. H. How to find your friendly neighbor-\nhood: Graph attention design with self-supervision. In\nICLR, 2021.\nKruskal, J. B. On the shortest spanning subtree of a graph\nand the traveling salesman problem. 1956.\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling\nlong- and short-term temporal patterns with deep neural\nnetworks. The 41st International ACM SIGIR Conference\non Research & Development in Information Retrieval,\n2017.\nLee, N., Ajanthan, T., and Torr, P. H. S. Snip: Single-shot\nnetwork pruning based on connection sensitivity. ArXiv,\nabs/1810.02340, 2018.\nLiu, B., Wang, M., Foroosh, H., Tappen, M. F., and Pensky,\nM. Sparse convolutional neural networks. 2015 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pp. 806–814, 2015.\nLiu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang,\nC. Learning efficient convolutional networks through\nnetwork slimming. 2017 IEEE International Conference\non Computer Vision (ICCV), pp. 2755–2763, 2017.\nLouizos, C., Welling, M., and Kingma, D. P. Learning\nsparse neural networks through l0 regularization. ArXiv,\nabs/1712.01312, 2017.\nLu, W., Li, J., Li, Y., Sun, A., and Wang, J. A cnn-lstm-\nbased model to forecast stock prices. Complex., 2020:\n6622927:1–6622927:10, 2020.\nLuo, D., Cheng, W., Yu, W., Zong, B., Ni, J., Chen, H.,\nand Zhang, X. Learning to drop: Robust graph neural\nnetwork via topological denoising. Proceedings of the\n14th ACM International Conference on Web Search and\nData Mining, 2021.\nMantegna, R. N. Hierarchical structure in financial markets.\nThe European Physical Journal B - Condensed Matter\nand Complex Systems, 11:193–197, 1998.\nMassara, G. P. and Aste, T. Learning clique forests. ArXiv,\nabs/1905.02266, 2019.\nMassara, G. P., di Matteo, T., and Aste, T. Network filtering\nfor big data: Triangulated maximally filtered graph. J.\nComplex Networks, 5:161–178, 2017.\nMocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H.,\nGibescu, M., and Liotta, A. Scalable training of arti-\nficial neural networks with adaptive sparse connectivity\ninspired by network science. Nature Communications, 9,\n2017.\nMolchanov, D., Ashukha, A., and Vetrov, D. P.\nVaria-\ntional dropout sparsifies deep neural networks. ArXiv,\nabs/1701.05369, 2017.\nMolchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz,\nJ. Pruning convolutional neural networks for resource\nefficient transfer learning. ArXiv, abs/1611.06440, 2016.\nMoyano, L. G. Learning network representations. The\nEuropean Physical Journal Special Topics, 226:499–518,\n2017.\nNeˇsetˇril, J., Milkov´a, E., and Neˇsetˇrilov´a, H. Otakar boruvka\non minimum spanning tree problem translation of both the\n1926 papers, comments, history. Discrete mathematics,\n233(1-3):3–36, 2001.\nPeng, H., Wu, J., Chen, S., and Huang, J. Collaborative\nchannel pruning for deep networks.\nIn International\nConference on Machine Learning, 2019.\nPiran, Z., Shwartz-Ziv, R., and Tishby, N. The dual in-\nformation bottleneck. arXiv preprint arXiv:2006.04641,\n2020.\nPoggio, T., Banburski, A., and Liao, Q. Theoretical issues\nin deep networks. Proceedings of the National Academy\nof Sciences, 117(48):30039–30045, 2020.\nPrim, R. C. Shortest connection networks and some gener-\nalizations. Bell System Technical Journal, 36:1389–1401,\n1957.\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V.,\nand Gulin, A. Catboost: unbiased boosting with categori-\ncal features. Advances in neural information processing\nsystems, 31, 2018.\nRoberts, S. J., Osborne, M. A., Ebden, M., Reece, S., Gib-\nson, N. P., and Aigrain, S. Gaussian processes for time-\nseries modelling. 2012.\n10\nHomological Neural Networks\nRomano, J. D., Le, T. T., La Cava, W., Gregg, J. T., Gold-\nberg, D. J., Chakraborty, P., Ray, N. L., Himmelstein, D.,\nFu, W., and Moore, J. H. Pmlb v1.0: an open source\ndataset collection for benchmarking machine learning\nmethods. arXiv preprint arXiv:2012.00058v2, 2021.\nRong, Y., bing Huang, W., Xu, T., and Huang, J. Drope-\ndge: Towards deep graph convolutional networks on node\nclassification. In ICLR, 2020.\nSamek, W., Montavon, G., Lapuschkin, S., Anders, C. J.,\nand M¨uller, K.-R. Explaining deep neural networks and\nbeyond: A review of methods and applications. Proceed-\nings of the IEEE, 109:247–278, 2021.\nShao, Z., Zhang, Z., Wang, F., and Xu, Y. Pre-training\nenhanced spatial-temporal graph neural network for mul-\ntivariate time series forecasting. Proceedings of the 28th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, 2022a.\nShao, Z., Zhang, Z., Wei, W., Wang, F., Xu, Y., Cao, X.,\nand Jensen, C. S. Decoupled dynamic spatial-temporal\ngraph neural network for traffic forecasting.\nArXiv,\nabs/2206.09112, 2022b.\nShih, S.-Y., Sun, F.-K., and yi Lee, H. Temporal pattern\nattention for multivariate time series forecasting. Machine\nLearning, pp. 1–21, 2018.\nShwartz-Ziv, R. and Armon, A. Tabular data: Deep learning\nis not all you need. Inf. Fusion, 81:84–90, 2022.\nShwartz-Ziv, R., Painsky, A., and Tishby, N. Representation\ncompression and generalization in deep neural networks,\n2018.\nStanley, K. O. and Miikkulainen, R. Evolving neural net-\nworks through augmenting topologies. Evolutionary Com-\nputation, 10:99–127, 2002.\nTelesford, Q. K., Simpson, S., Burdette, J., Hayasaka, S.,\nand Laurienti, P. The brain as a complex system: Using\nnetwork science as a tool for understanding the brain.\nBrain connectivity, 1 (4):295–308, 2011.\nTorres, J. J. and Bianconi, G. Simplicial complexes: higher-\norder spectral dimension and dynamics.\nJournal of\nPhysics: Complexity, 1, 2020.\nTumminello, M., Aste, T., Di Matteo, T., and Mantegna,\nR. N. A tool for filtering information in complex systems.\nProceedings of the National Academy of Sciences, 102\n(30):10421–10426, 2005. ISSN 0027-8424. doi: 10.1073/\npnas.0500298102. URL https://www.pnas.org/\ncontent/102/30/10421.\nVidal-Tomas, D., Briola, A., and Aste, T. Ftx’s downfall\nand binance’s consolidation: The fragility of centralized\ndigital finance. SSRN Electronic Journal, 2023.\nWan, R., Mei, S., Wang, J., Liu, M., and Yang, F. Multi-\nvariate temporal convolutional network: A deep neural\nnetworks approach for multivariate time series forecast-\ning. Electronics, 2019.\nWang, D., Zhou, L., Zhang, X., Bai, X., and Zhou, J. Ex-\nploring linear relationship in feature map subspace for\nconvnets compression. ArXiv, abs/1803.05729, 2018.\nWang, Y. and Aste, T. Network filtering of spatial-temporal\ngnn for multivariate time-series prediction. Proceedings\nof the Third ACM International Conference on AI in Fi-\nnance, 2022.\nWen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. H. Learn-\ning structured sparsity in deep neural networks. ArXiv,\nabs/1608.03665, 2016.\nWu, Z., Pan, S., Long, G., Jiang, J., Chang, X., and Zhang, C.\nConnecting the dots: Multivariate time series forecasting\nwith graph neural networks. Proceedings of the 26th\nACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, 2020.\nYang, M., Isufi, E., and Leus, G. Simplicial convolutional\nneural networks. In ICASSP 2022-2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pp. 8847–8851. IEEE, 2022.\nYe, J., Lu, X., Lin, Z. L., and Wang, J. Z. Rethinking\nthe smaller-norm-less-informative assumption in channel\npruning of convolution layers. ArXiv, abs/1802.00124,\n2018.\nYu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V. I., Han,\nX., Gao, M., Lin, C.-Y., and Davis, L. S. Nisp: Prun-\ning networks using neuron importance score propagation.\n2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9194–9203, 2017.\nZhang, G. P. Time series forecasting using a hybrid arima\nand neural network model. Neurocomputing, 50:159–175,\n2003.\nZheng, C., Fan, X., Wang, C., and Qi, J. Gman: A graph\nmulti-attention network for traffic prediction.\nArXiv,\nabs/1911.08415, 2020.\nZhuang, Z., Tan, M., Zhuang, B., Liu, J., Guo, Y., Wu, Q.,\nHuang, J., and Zhu, J.-H. Discrimination-aware channel\npruning for deep neural networks. In Neural Information\nProcessing Systems, 2018.\n11\nHomological Neural Networks\nZhuo, H., Qian, X., Fu, Y., Yang, H., and Xue, X. Scsp:\nSpectral clustering filter pruning with soft self-adaption\nmanners. ArXiv, abs/1806.05320, 2018.\nZivot, E. and Wang, J. Vector autoregressive models for\nmultivariate time series. 2003.\n12\nHomological Neural Networks\nA. Appendix\nDataset\nNo. Features\nNo. Samples\nSample Rate\nsolar-energy\n137\n52560\n10 minutes\nexchange-rates\n8\n7588\n1 day\nTable 5. Multivariate time-series dataset statistics, including the number of features, number of samples and sample rate in the solar-\nenergy-energy and exchange-rates-rates datasets (Wu et al., 2020).\nsolar-energy\nexchange-rates\nLSTM-MLP\n452901\n13011\nLSTM-MLP-HNN\n509208\n13203\nLSTM-MLP-res\n509208\n13203\nLSTM-HNN\n239061\n12795\nTable 6. Number of parameters in each model in solar-energy and exchange-rates datasets, comparing the sparse LSTM-HNN with the\nfully connected models.\nsolar-energy\nexchange-rates\nLSTM-skip\n337112\n19478\nTPA-LSTM\n613987\n132172\nMTGNN\n347665\n337345\nLSTM-HNN\n239061\n12795\nTable 7. Number of parameters in each model in solar-energy and exchange-rates datasets, comparing the LSTM-HNN with respect to\nstate-of-art models.\n13\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-06-27",
  "updated": "2023-06-27"
}