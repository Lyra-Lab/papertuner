{
  "id": "http://arxiv.org/abs/2307.00008v1",
  "title": "Investigating Masking-based Data Generation in Language Models",
  "authors": [
    "Ed S. Ma"
  ],
  "abstract": "The current era of natural language processing (NLP) has been defined by the\nprominence of pre-trained language models since the advent of BERT. A feature\nof BERT and models with similar architecture is the objective of masked\nlanguage modeling, in which part of the input is intentionally masked and the\nmodel is trained to predict this piece of masked information. Data augmentation\nis a data-driven technique widely used in machine learning, including research\nareas like computer vision and natural language processing, to improve model\nperformance by artificially augmenting the training data set by designated\ntechniques. Masked language models (MLM), an essential training feature of\nBERT, have introduced a novel approach to perform effective pre-training on\nTransformer based models in natural language processing tasks. Recent studies\nhave utilized masked language model to generate artificially augmented data for\nNLP downstream tasks. The experimental results show that Mask based data\naugmentation method provides a simple but efficient approach to improve the\nmodel performance. In this paper, we explore and discuss the broader\nutilization of these data augmentation methods based on MLM.",
  "text": "Investigating Masking-based Data Generation in Language\nModels\nEd S. Ma∗\neddsma@outlook.com\nABSTRACT\nThe current era of natural language processing (NLP) has been\ndefined by the prominence of pre-trained language models since\nthe advent of BERT. A feature of BERT and models with similar\narchitecture is the objective of masked language modeling, in which\npart of the input is intentionally masked and the model is trained\nto predict this piece of masked information. Data augmentation is a\ndata-driven technique widely used in machine learning, including\nresearch areas like computer vision and natural language process-\ning, to improve model performance by artificially augmenting the\ntraining data set by designated techniques. Masked language mod-\nels (MLM), an essential training feature of BERT, have introduced a\nnovel approach to perform effective pre-training on Transformer\nbased models in natural language processing tasks. Recent studies\nhave utilized masked language model to generate artificially aug-\nmented data for NLP downstream tasks. The experimental results\nshow that Mask based data augmentation method provides a simple\nbut efficient approach to improve the model performance. In this\npaper, we explore and discuss the broader utilization of these data\naugmentation methods based on MLM.\nKEYWORDS\nnatural language processing, data augmentation, language models,\nneural networks\n1\nINTRODUCTION\nPre-trained language models (PLMs) have revolutionized the field\nof natural language processing, with BERT architectures stand-\ning out for their innovative design and impressive performance.\nThese Transformer [56] based models, based largely on transformer\narchitecture, use bi-directional representations to understand con-\ntext, thus pushing the limits of previous uni-directional models.\nA distinctive feature of BERT and similar models is the goal of\nmasked language modeling, in which part of the input is intention-\nally masked and the model is trained to predict this masked token.\nThis strategy simulates a fuller understanding of the context and\nrelationships between words, leading to a better understanding of\nlanguage nuances and meaning. As a result, BERT-like models have\nfound extensive application in a variety of NLP tasks, such as text\nclassification, sentiment analysis, and question answering, signifi-\ncantly pushing the boundaries of what machines can understand\nand accomplish in the realm of human language.\nNLP tasks requires a significant amount of high-quality anno-\ntated data for several critical reasons, primarily related to the in-\nherent complexity of human language and the need for machine\nlearning models to model it effectively understand, interpret and\ngenerate. Languages are extremely complicated and complex, with\n∗Independent project. Work in Progress. Contact: eddsma@outlook.com\ncountless nuances, exceptions and rules. Consisting of morpho-\nlogical, syntactic, semantic, and pragmatic aspects, they require\nunderstanding not only of the words and phrases, but also of con-\ntext, intent, and even cultural or social cues. In order for an NLP\nmodel to understand all of these elements and generate human-\nlike text, it must learn from a variety of examples that show these\ncharacteristics in multiple different contexts. This is where high-\nquality annotated data comes into play. They provide the models\nwith explicit labels or additional information that make it easier to\nunderstand the various features of the language.\nThe performance of machine learning models is highly depen-\ndent on the variety and amount of training data. Because these\nmodels learn by identifying patterns in the input data, a larger and\nmore diverse data set allows the models to be exposed to a wider\nrange of patterns and situations. This leads to better generalization\nability and the models can process new inputs more effectively.\nFor example, if you train an NLP model on annotated data from\ndifferent domains like literature, science, law, social media, etc., it\ncan understand and generate texts related to each of these domains.\nThe quality of the annotated data is important. Poorly annotated\ndata can mislead the model during training, resulting in suboptimal\nperformance or even completely wrong outputs. Accurate annota-\ntions are fundamental to supervised learning as they serve as the\nbasis for the model. They help models distinguish between differ-\nent elements of language, understand the relationships between\nwords, and understand the meaning and intent behind phrases or\nsentences. Therefore, high-quality annotated data plays a crucial\nrole in training robust and reliable NLP models. It provides the rich,\ndiverse, and concise input the models need to learn the complexities\nof the language, ensures their applicability in different domains and\nscenarios, and acts as an effective guide during the training process\nto optimize their performance.\nIt is often difficult and expensive to obtain quality annotated\ntext data in large volume. Traditional and expensive way is to hire\ncrowd workers with target language capability to annotate data. An\nexample is Amazon Mechanical Turk (AMT). AMT is a crowdsourc-\ning service that enables individuals and businesses to outsource\ntasks to a distributed workforce who can perform these tasks vir-\ntually. Based on requirements, workers (known as ‘Turkers‘) will\ngo through target data, manually annotate it as per instructions.\nOnce a worker completes a Human Intelligence Task (HIT), you\ncan review their work, approve or reject it based on the quality\nof the annotation, and then pay the worker. With its vast, diverse\nworkforce, AMT is often used to create large, annotated datasets\nfor NLP downstream tasks. However, this annotating method is\ncommon but very expensive. Therefore, researchers have started\nexploring annotating methods at cheaper costs. Some examples are\nmethods that are based on distant supervision. Unlabeled dialogue\ncorpora in the target domain can be easily curated from previous\narXiv:2307.00008v1  [cs.CL]  16 Jun 2023\nEd S. Ma\nFigure 1: Visual illustration of MLM in BERT. During pre-\ntraining, some tokens (about 10 – 15 percent) are masked\nrandomly (only one token is masked in this example) and the\nmodel is trained to predict the correct words for all masked\ntokens. The model will be updated based on the probability\ndistribution over vocabulary through MLM loss functions\nand backprobagation. Numbers are for illustration purposes\ninstead of real model output.\nconversation transcripts or collected via crowdsourcing [4, 5] with\nno additional cost on human labour.\nWith the rise of BERT (Bidirectional Encoder Representations\nfrom Transformers) [12] models in NLP, there has been a significant\nleap forward in the field’s capabilities and applications. BERT’s bi-\ndirectional approach, where it considers the context from both\nleft and right of a word, sets it apart from previous models. The\nexperiments on several NLP benchmark shows that it allows a more\naccurate understanding of the meaning of a word within its context,\nleading to improved performance in a variety of tasks such as\nsentiment analysis, named entity recognition, question answering,\nand others. There are several data augmentation methods that\nutilize these pre-trained language models in unsupervised manners,\nwith no human annotation needed.\n2\nRELATED WORK\nIn this section, we will introduce pre-trained language models and\nrecent data augmentation methods including data augmentation\nmethods with MLM.\n2.1\nPre-trained Language Models\nPre-trained language models such as BERT [12], RoBERTa [25],\nXLNet [64], BART [21], and T5 [43] have revolutionized the field of\nNatural Language Processing. These models rely on transformer-\nbased architectures and unsupervised learning, and they are trained\non large text corpora to learn useful representations of language.\nThey can then be fine-tuned on specific tasks with smaller amounts\nof task-specific data. The objective is masked language modeling is\nessential and prominent in these models,\nBERT [12], Bidirectional Encoder Representations from Trans-\nformers, was a pioneering pre-trained language model in the field. It\nis trained using a masked language modeling objective where some\npercentage of the input data is masked or hidden during training,\nand the model is trained to predict these masked words based on the\ncontext provided by the unmasked words. BERT’s MLM approach\nenables the model to learn a deep, bidirectional representation of\nthe sentence, rather than just predicting future words in the text\nlike traditional language models. This improves the model’s un-\nderstanding of the context and the semantic relationships between\nwords, leading to significant improvements on a wide range of NLP\ntasks. We show a visual illustration of MLM in BERT in Figure 1.\nSince BERT is a bi-directional model, the probability of predict-\ning the correct masked word is dependent on known surrounding\nwords in the context. RoBERTa [25], a variant of BERT developed\nby Meta, modifies the training process and hyperparameters of\nBERT, but still utilizes the MLM objective. RoBERTa extends the\ntraining time, uses larger mini-batches and learning rates, and re-\nmoves the next sentence prediction objective that BERT had. These\ntweaks led to improved performance, demonstrating the impact of\na well-thought-out training strategy.\nXLNet [64] is a another transformer-based model, combining\nthe advantages of BERT with autoregressive language models. It\nintroduces a permutation-based training objective that allows it\nto learn the dependency of words on both their preceding and\nsucceeding context, mitigating some of the limitations of BERT’s\nMLM approach where the predicted words don’t have an impact\non each other.\nT5 [43] Text-to-Text Transfer Transformer takes a unique ap-\nproach to its pre-training objective. Instead of employing the usual\nmasked language modeling, T5 treats every NLP problem as a text\ngeneration task. This is achieved by reframing tasks into a unified\ntext-to-text format where every task, be it translation, summariza-\ntion, or question answering, becomes a matter of generating the\ntarget text from the source text. During pre-training, T5 uses a\ndenoising autoencoder-style objective where random spans of text\nare masked out and the model learns to reconstruct the original\ntext. T5’s approach simplifies the application of the model to a\nwide variety of tasks, as it doesn’t require task-specific model ar-\nchitectures. Instead, differences between tasks are captured purely\nin the text input and output formats. As for performance, T5 has\nshown state-of-the-art results on a number of benchmark datasets\nacross different NLP tasks. Its scalability, coupled with the general\ntext-to-text approach, makes T5 a highly flexible and powerful tool\nfor NLP tasks, showcasing the potential of this unified framework.\nBART [21] Bidirectional and Auto-Regressive Transformers brings\na distinctive pre-training objective to the table. BART combines the\nbest of both worlds: the bidirectional context learning from BERT\nand the sequence-to-sequence nature of models like GPT. During\npre-training, BART corrupts the text by applying a noising function\n(i.e., by masking out random spans of text, similar to T5 [43]), and\nthen learns how to reconstruct the original uncorrupted text. This\nobjective is different from traditional masked language modeling\nas it forces the model to build an understanding of the entire sen-\ntence structure and not just predict masked tokens independently.\nBART has proven to be highly effective. It has achieved state-of-\nthe-art results on a variety of benchmark datasets, including the\nCNN/Daily [32] Mail summarization task, the SQuAD question\nInvestigating Masking-based Data Generation in Language Models\nanswering benchmark [45, 46], and others. By combining the ad-\nvantages of bidirectional and auto-regressive transformers, BART\nhas shown a robust ability to handle a wide range of NLP tasks,\nhighlighting the power of sequence-to-sequence and denoising\nautoencoder-style pre-training.\nPre-trained language models have had a profound impact on\nNLP, and the Masked Language Modeling objective has been a\ncritical part of their success. By learning to predict missing words\nin a sentence, models are able to gain a deeper understanding of\nlanguage context and semantics. This has resulted in significant\nperformance improvements across a wide range of NLP tasks, and\nit continues to drive progress in the field.\nMasked language modeling plays a crucial role in these notable\npre-trained language models. By learning to fill in the masked\nwords, language models gain a deep understanding of syntax, se-\nmantics, and contextual relationships. Masked language modeling\nserves as a self-supervised learning task that allows models to learn\nfrom vast amounts of unlabeled text data. This technique allows\npre-trained language models to capture the nuances of language\nand generate coherent and contextual responses when presented\nwith incomplete or ambiguous input. By incorporating masked lan-\nguage modeling, pre-trained language models have revolutionized\nvarious natural language processing tasks such as text generation,\nsentiment analysis, machine translation, and question answering.\n2.2\nData Augmentation\nData augmentation (DA) is a technique used to generate additional\ntraining data in situations where there is a lack of sufficient data.\nIt involves various methods, ranging from simple rule-based tech-\nniques to more advanced learnable generation-based methods. The\nprimary objective of data augmentation is to ensure that the gen-\nerated data is valid for the given task and belongs to the same\ndistribution as the original data [44]. This validity is determined\nby factors such as similar semantics in machine translation or the\nsame labels in text classification as the original data. Additionally,\naugmented data should also exhibit diversity to enhance the general-\nization of models for subsequent tasks. The diversity of augmented\ndata can be achieved through three categories of data augmentation\nmethods: paraphrasing, noising, and sampling.\nCategorization\nIn order to ensure validity, augmented data should also possess\ndiversity to enhance the generalization of models in subsequent\ntasks. This diversity refers to the range of variations within the\naugmented data. [22] introduces categorization of data augmenta-\ntion methods based on the diversity of resulting augmented data.\nParaphrasing-based methods generate augmented data that main-\ntains a limited semantic difference from the original data by making\nproper and controlled changes to sentences. The goal is to produce\naugmented data that conveys very similar information as the origi-\nnal. Noising-based methods introduce discrete or continuous noise\nto the data while ensuring its validity. The objective of these meth-\nods is to enhance the model’s robustness. Sampling-based methods,\non the other hand, understand the data distributions and generate\nnovel data samples from within these distributions. These methods\noutput more diverse data and cater to a wider range of requirements\nin downstream tasks, leveraging artificial heuristics and trained\nmodels.\nParaphrases are commonly observed in natural language, initi-\nated from early works in NLP [2, 28], serve as alternative expres-\nsions to convey identical information as the original form. Con-\nsequently, utilizing paraphrasing as a technique for data augmen-\ntation is a fitting approach. Paraphrasing encompasses multiple\nlevels, encompassing lexical paraphrasing, phrase paraphrasing,\nand sentence paraphrasing.\nBasic word-/sentence-level operations\nVarious basic word-level and sentence-level operations can be\nemployed to augment text data, including insertion, deletion, sub-\nstitution, and swapping. Insertion, additional words or phrases are\ninserted into the original text. By inserting different linguistic ele-\nments, such as synonyms or context-relevant terms, the extended\ndata can simulate different writing styles and increase the diversity\nof the data set. [37, 58] This variation can allow the model to learn\nbetter and generalize to different text types. Deletion, on the other\nhand, is the removal of specific words or phrases from the original\ntext. This technique helps the model focus on the most important\ninformation by eliminating non-essential elements. This operation\nhas been employed in various works such as [47, 58, 67] By training\non augmented data with strategically deleted content, the model\ncan become more resilient to noise and less dependent on specific\nphrases or words. Substitution is another powerful data augmenta-\ntion technique. Words are replaced by their synonyms or similar\nterms. By introducing variations in vocabulary, the model can learn\nto recognize and understand different word choices and expand its\nlanguage skills. [11, 27, 48, 57, 62] This technique is particularly use-\nful for improving the model’s ability to deal with out-of-vocabulary\nwords and to adapt to different linguistic expressions. Swapping is\nan augmentation technique that rearranges the order of words or\nphrases within a sentence. By rearranging the sequence of linguis-\ntic elements, the model can learn to understand different sentence\nstructures and syntactic variations. [17, 26, 47, 58, 68] This increases\nthe model’s flexibility and understanding of different sentence pat-\nterns, making it more effective in tackling sentence reordering or\nparaphrasing tasks.\nPLM backed methods\nAs mentioned in Section 2.1, the success of deep pre-trained\nlanguage models in recent years can be attributed to their ability\nto acquire extensive linguistic knowledge through pre-training.\nAs a result, these sophisticated pre-trained models have naturally\nbecome valuable resources for data augmentation purposes.\nAuto-regressive models have been explored in generating aug-\nmented data. For instance, [37] utilizes the pre-trained GPT based\nmodels to generate utterances and dialogue acts, respectively, en-\nsuring data quality through filtering. [39] applied DistilBERT on\noriginal sentences to generate synthetic sentences. Transformer de-\ncoder based model GPT-2 [42], in particular, has gained popularity\nas a model for generating augmented data. [68] employed GPT-2\nto generate significantly diversified augmented data for extreme\nmulti-label classification. Another GPT-2 backed data augmenta-\ntion method LAMBDA is introduced in [1]. They utilize GPT-2,\nwhich is pre-trained and fine-tuned on the training set, to generate\nlabeled augmented sentences. These augmented sentences are sub-\nsequently filtered using a classifier to maintain data quality. [20]\nEd S. Ma\napplies a similar method without the use of a filtering classifier.\n[40] employed a label-conditioned GPT-2 model to generate aug-\nmented data. [53] utilized GPT-2 to generate augmented data and\nsubsequently performed tokenization and splited them into sub-\nwords derived statistically, thereby avoiding vocabulary explosion\nin morphologically rich languages.\nTo obtain augmented data, some studies employ masked lan-\nguage models. For instance, [33] utilizes a masked language model\nto create both a corruption model and a reconstruction model. The\nmodel comprises a BERT-base pre-trained language model and\na classification layer. The BERT parameters are additionally pre-\ntrained on a vast Reddit dataset, ensuring a large-scale training\nprocess. The process involves generating data points that are ini-\ntially distant from the original data manifold using the corruption\nmodel, and subsequently using the reconstruction model to bring\nthose data points back to the original data manifold, resulting in\nthe final augmented data. Such decoding sampling methods have\nalso been utilized in [15] which first masks tokens or spans then\nfills in replaced tokens by pre-trained language models.\nObtaining unlabeled raw data can be relatively easy in certain\nscenarios, presenting an opportunity to convert such data into valid,\nusable data and significantly augment the overall dataset. [54] intro-\nduce a data augmentation approach involving fine-tuning BERT on\nthe original data, followed by utilizing the fine-tuned BERT to label\nunlabeled sentence pairs. These augmented data, along with the\ngold data, are then combined to train SBERT [49]. Data distillation\nhas been utilized as part of the self-training process [29], where the\nlabel of unlabeled data is determined using an iteratively updated\nteacher model. On question answering tasks, [63] applies a similar\nself-training method using a cross-attention-based teacher model to\ndetermine the label for each QA pair. SentAugment [13], a data aug-\nmentation method that leverages task-specific query embeddings\nfrom labeled data to retrieve sentences from a vast bank of bil-\nlions of unlabeled sentences obtained from web crawling. In some\ncases, existing models from other tasks are directly transferred\nto generate pseudo-parallel corpora. [30] leverages Wikipedia to\naccess a large volume of sentences and utilizes external informa-\ntion extraction package to extract triplets from these Wikipedia\nsentences. Recognizing BERT’s proficiency in object-property (OP)\nrelationship prediction and object-affordance (OA) relationship pre-\ndiction, [71] directly employs a fine-tuned BERT model to predict\nthe labels of the two samples. [59] explore data augmentation on\nsemantically-preserved continuous space using Transformers.\nUnsupervised methods\nUnsupervised methods for text data augmentation leverage un-\nlabeled data to enhance the quantity and quality of training data\nwithout relying on explicit or extra annotations or labels. Some\nunsupervised methods have been mentioned under word-/sentence-\nlevel operations, here we discuss methods not falling into those\ncategories. These methods aim to harness the inherent structure,\npre-training objectives and patterns present in the unlabeled data\nto generate additional training examples.\n”Mixup” methods take a different approach to data augmenta-\ntion by utilizing intermediate embeddings instead of generating\naugmented samples in natural language text. These methods in-\nvolve sampling data in vector space (manifold) based on existing\ndata, potentially resulting in samples with different labels than the\noriginal data. A prominent feature of this group of methods is no\nadditional human annotation is required to construct new training\nsamples, and resulting labels are automatically generated after the\n”mixing” process. The concept of Mixup was initially introduced in\ncomputer vision works [69]. Mixup has found wide application in\nnumerous recent studies. Building upon this work, [16] proposed\ntwo variations of Mixup specifically for sentence classification, per-\nforming sample interpolation in the word embedding space and in-\nterpolating the hidden states of sentence encoders. Both variations\ngenerate interpolated samples as augmented data. [60] propose a\ndata augmentation method integrating Mixup strategy with MLM\nthat involves converting a sentence from its one-hot representation\nto a controllable smoothed representation. [19] introduce a frame-\nwork for saliency map-informed textual data augmentation and\nregularization, which combines Dropout and Mixup techniques to\naddress the issue of overfitting in text learning.\nMixup methods for text classification are explored in [51, 52].\n[52] introduces Transformer based Mixup method, a combination\nof Mixup and transformer-based pre-trained architecture, which\nis evaluated on text classification datasets. [6] incorporates Mixup\ninto Named Entity Recognition (NER) to improve performance in\nNER tasks. [7] introduce MixText, which creates augmented train-\ning samples by interpolating text in hidden space. The method\nperforming mixing of hidden representations of examples in dif-\nferent Transformer layers. The resulting labels are determined by\nmixing samples.\nMachine translation has emerged as a popular method for data\naugmentation, leveraging its ability to naturally paraphrase text.\nWith the advancements in machine translation models, this tech-\nnique has found widespread application across various tasks. Re-\nsearchers have explored off-the-shelf machine translation models\non data augmentation with no additional human effort on annota-\ntions. Back-translation, involves translating the original text into\nother languages and then translating it back to the original lan-\nguage to generate augmented text. Unlike word-level methods,\nback-translation doe not merely replace individual words, but rather\nrewrites the entire sentence in a generated manner. [14, 61, 66] uti-\nlize English-French translation models, in both directions (known\nas round-trip translations), to perform back-translation on each\nsentence and generate paraphrases. Lowell also incorporates this\nmethod as one of the unsupervised data augmentation techniques.\nAdditionally, Zhang employs back-translation to obtain formal ex-\npressions of the original data in the context of style transfer tasks.\nBuilding upon the concept of vanilla back-translation, several\nstudies have introduced additional features and techniques to en-\nhance its effectiveness. In [70], a discriminator is employed to filter\nthe sentences obtained through back-translation. By applying this\nfiltering mechanism, the quality of the augmented data is signifi-\ncantly improved, as only sentences surpassing a certain threshold\nare retained. [34] explore various softmax temperature settings to\nensure diversity in the augmented data while preserving semantic\nmeaning. By carefully adjusting the temperature, they achieve a\nbalance between generating diverse examples and maintaining the\noriginal intent. Overall, these additional features and techniques\nbring improvements to the vanilla back-translation method, ensur-\ning diversity and enhancing the quality of the augmented data.\nDA with Mask tokens\nInvestigating Masking-based Data Generation in Language Models\nFigure 2: Example of augmented samples with mask tokens.\nThe top first sentence is the original sentence without any\naugmentation (The quick brown fox jumps over the lazy dog\nin the zoo). The rest five sentences are augmented sentences\nobtained by data augmentation with mask tokens in which\nan original word in the original sentence is replaced with a\nMASK token.\nMask tokens are a critical component of pre-trained language\nmodels like BERT [12]. With BERT architecture, a small percentage\nof the input tokens are randomly masked during pre-training. These\nmasked tokens are usually replaced with a special [MASK] token.\nWe present a visual example of MLM in Figure 2 for ease of under-\nstanding. Using mask tokens in pre-trained language models such\nas BERT enables effective language representation learning and\nallows the models to capture the contextual relationships between\nwords and phrases, resulting in improved performance on various\nNLP tasks. As a pre-training strategy, mask tokens are seen by the\nmodel during pre-training process. Mask tokens are pre-trained\nto preserve contextualized information (like other tokens) but not\nactually encountered in test-time examples. Intuitively, mask to-\nken can be used in sentences like other words without injecting\nundesired signals in sentences under the training scheme of BERT.\nWord replacement and back-translation have been demonstrated\nto be effective unsupervised data augmentation methods for short\nwritten text classification in previous studies [58, 61]. However, the\neffectiveness of these augmentation methods is diminished when\napplied to pre-trained models [50]. Additionally, the applicability\nof backt-ranslation is limited in our scenario, given that translating\nmulti-turn dialogue is considerably more challenging compared\nto short text. Recent work [65] has examined the effectiveness of\nreplacing a portion of original word tokens with [MASK] tokens\nas a data augmentation strategy in NLP. This technique allows for\nthe generation of extended examples in which certain words are\nescaped, providing the model with additional training instances.\n[65] introduce MaskAugment, a controllable data augmentation\nthat augments text input by leveraging the pre-trained mask token\nfrom BERT model on the task of dialog act tagging. [24] extend\nthis idea and propose a semi-supervised bootstrapping training\nmethod for dialog breakdown detection tasks. Both works utilize\nteacher-student learning scheme on samples with different proba-\nbilities of mask token replacement. By leveraging the pre-trained\nlanguage model’s ability to understand and contextualize mask\ntokens, this extension method introduces diversity and variation\ninto the training data without introducing unwanted bias or noise.\nThis DA approach has been shown to increase model robustness,\nimprove generalization to invisible data, and increase performance\non various downstream NLP tasks.\nCompared to the aforementioned DA methods, mask data aug-\nmentation appears to be better suited for pre-trained language mod-\nels with MLM training objectives and demonstrate effectiveness in\ndialog tasks. This method is intuitively simple and controllable as\nit only considers the position and probability of replacement. By\nmaintaining a low probability of masking [24, 65], the sentences\naugmented with mask tokens can retain the original meaning while\nundergoing natural changes.\nDA with adversarial training\nNLP data augmentation with adversarial training has emerged as\na promising technique to improve the robustness and performance\nof NLP models. Adversarial training involves generating adversarial\nexamples or perturbations that aim to deceive the model while\npreserving the original meaning or intent of the text. In the context\nof NLP, these adversarial examples can be created by applying\nvarious techniques but not limited to synonym substitution, word\ndeletion, or sentence modification to the original text.\n[9] constructs adversarial samples based on the original samples\nfollowing [8] and then applies two Mixup strategies. [38] combines\nback-translation with adversarial training. [31] introduce an off-the-\nshelf framework for developing data augmentation with adversarial\nattacks covering various attack recipes from literature[18, 23]. This\nintegration allows them to synthesize augmented examples that are\nboth diverse and informative, incorporating multiple transforma-\ntions to enrich the data. The goal of adversarial data augmentation\nis to expose the model to challenging and diverse examples that\nmimic real-world linguistic variations and potential attacks. By in-\ncorporating these adversarial examples into the training data, NLP\nmodels can learn to handle and generalize better to such variations,\nmaking them more robust and reliable in real-world scenarios. Ad-\nversarial training not only helps models overcome the limitations\nof overfitting but also enhances their ability to handle noisy or\nadversarial inputs.\n3\nDISCUSSION\n3.1\nNon-MLM Pre-trained Language Models\nNowadays, we are witnessing the rapid emergence of a diverse\narray of pre-trained language models that go beyond the traditional\ntraining objective of masked language modeling. Prominent exam-\nples include models such as T5 [43], Flan [10] and GPT [41, 42].\nT5/Flan-T5 deviate from the conventional approach of masked lan-\nguage modeling where individual tokens are masked. Instead, these\nmodels employ a different masking and prediction strategy that\noperates at the level of text spans. In this strategy, specific spans of\ntext are masked, and the models are trained to predict the correct\ncontent within those masked spans. These models are designed\nwith multifaceted training objectives aimed at enhancing different\naspects of language understanding and generation. In addition to\nEd S. Ma\nincorporating masked language modeling, these models can also\nintegrate various other objectives such as sequence classification,\ndocument retrieval, question-answering, and machine translation.\nBy encompassing this diverse range of training objectives, pre-\ntrained language models gain the ability to capture a comprehen-\nsive understanding of linguistic properties, semantic relationships,\nand contextual nuances. This advancement in training objectives\nnot only enhances the versatility and context-awareness of these\nlanguage models but also facilitates their application across a wide\nspectrum of natural language processing tasks and domains. As\nongoing research and innovation in this field continue to unfold,\nwe can expect the emergence of even more advanced pre-trained\nlanguage models, leading to significant advancements in natural\nlanguage understanding and generation across diverse domains.\nIncorporating mask token-based data augmentation into the train-\ning pipeline of models like T5 and GPT is potentially feasible and\nexplorable.\n3.2\nEmerging Large Language Models\nThe presence of large language models (LLM) like GPT-3 family\nmodels [3, 35, 36] and Llama [55] have brought both challenges and\nsignificant impacts on data augmentation methods.\nOne challenge is the scalability and computational demands\nthat come with data proliferation. Large language models require a\nlarge amount of training data to achieve their impressive perfor-\nmance. However, generating extended data at the same scale can\nbe computationally intensive and slow, requiring additional model\nprocessing and inference. On the other hand, the influence of large\nlanguage models on data augmentation is significant. These models\nhave extensive linguistic knowledge and can generate high-quality\nsynthetic examples using techniques such as conditional genera-\ntion or controlled generation. Consequently, data augmentation\ntechniques can benefit from integrating these models to generate\ndiverse and contextually relevant augmented data.\nAdditionally, large language models can serve as powerful tools\nfor data enrichment. By optimizing or adapting pre-trained models\nlike GPT-3 to specific downstream tasks, they can be used to gener-\nate advanced data tailored to the target task. This approach allows\nmodels to learn from extended data and potentially improve their\nperformance on the given task.\nMoreover, large language models can also be used for data en-\nrichment by leveraging their embeds or contextual features. These\nembeddings can provide valuable information about relationships\nand similarities between text samples and facilitate the develop-\nment of new data enhancement strategies that preserve semantic\nand syntactic properties.\nREFERENCES\n[1] Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George\nKour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020. Do Not\nHave Enough Data? Deep Learning to the Rescue!. In Proceedings of the AAAI\nConference on Artificial Intelligence. AAAI Press, New York, NY, USA, 7383–7390.\n[2] Regina Barzilay and Kathleen R. McKeown. 2001. Extracting Paraphrases from a\nParallel Corpus. In Proceedings of ACL. Association for Computational Linguistics,\nToulouse, France, 50–57. https://doi.org/10.3115/1073012.1073020\n[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. In\nProceedings of the 34th International Conference on Neural Information Processing\nSystems (Vancouver, BC, Canada) (NIPS’20). Curran Associates Inc., Red Hook,\nNY, USA, Article 159, 25 pages.\n[4] Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva,\nStefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ - A Large-\nScale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.\nIn Proceedings of EMNLP. Association for Computational Linguistics, Brussels,\nBelgium, 5016–5026. https://doi.org/10.18653/v1/D18-1547\n[5] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan,\nBen Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim,\nand Andy Cedilnik. 2019. Taskmaster-1: Toward a Realistic and Diverse Dialog\nDataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP). Association for Computational Linguistics,\nHong Kong, China, 4516–4525. https://doi.org/10.18653/v1/D19-1459\n[6] Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, and Diyi Yang. 2020.\nLocal Additivity Based Data Augmentation for Semi-supervised NER. In\narXiv:2010.01677. arXiv:2010.01677 [cs.CL]\n[7] Jiaao Chen, Zichao Yang, and Diyi Yang. 2020. MixText: Linguistically-Informed\nInterpolation of Hidden Space for Semi-Supervised Text Classification. In Pro-\nceedings of the 58th Annual Meeting of the Association for Computational Lin-\nguistics. Association for Computational Linguistics, Online, 2147–2157. https:\n//doi.org/10.18653/v1/2020.acl-main.194\n[8] Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019. Robust Neural Machine\nTranslation with Doubly Adversarial Inputs. In Proceedings of ACL. Association\nfor Computational Linguistics, Florence, Italy, 4324–4333. https://doi.org/10.\n18653/v1/P19-1425\n[9] Yong Cheng, Lu Jiang, Wolfgang Macherey, and Jacob Eisenstein. 2020. Ad-\nvAug: Robust Adversarial Augmentation for Neural Machine Translation. In\nProceedings of the 58th Annual Meeting of the Association for Computational Lin-\nguistics. Association for Computational Linguistics, Online, 5961–5970. https:\n//doi.org/10.18653/v1/2020.acl-main.529\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nYunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha\nChowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,\nDenny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned\nLanguage Models. In arXiv:2210.11416. arXiv:2210.11416 [cs.LG]\n[11] Guillaume Daval-Frerot and Yannick Weis. 2020. WMD at SemEval-2020 Tasks 7\nand 11: Assessing Humor and Propaganda Using Unsupervised Data Augmenta-\ntion. In Proceedings of the Fourteenth Workshop on Semantic Evaluation. Interna-\ntional Committee for Computational Linguistics, Barcelona (online), 1865–1874.\nhttps://doi.org/10.18653/v1/2020.semeval-1.246\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers). Association for Computational Linguistics, Minneapolis,\nMinnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423\n[13] Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael\nAuli, Veselin Stoyanov, and Alexis Conneau. 2021. Self-training Improves Pre-\ntraining for Natural Language Understanding. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies. Association for Computational Linguistics,\nOnline, 5408–5418. https://doi.org/10.18653/v1/2021.naacl-main.426\n[14] Alexander Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvinine-\njad, Shafiq Joty, Dragomir Radev, and Yashar Mehdad. 2021. Improving Zero\nand Few-Shot Abstractive Summarization with Intermediate Fine-tuning and\nData Augmentation. In Proc. of NAACL-HLT. Association for Computational\nLinguistics, Online, 704–717. https://doi.org/10.18653/v1/2021.naacl-main.57\n[15] Jun Gao, Changlong Yu, Wei Wang, Huan Zhao, and Ruifeng Xu. 2022. Mask-\nthen-Fill: A Flexible and Effective Data Augmentation Framework for Event\nExtraction. In Findings of the Association for Computational Linguistics: EMNLP\n2022. Association for Computational Linguistics, Abu Dhabi, United Arab Emi-\nrates, 4537–4544.\n[16] Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019. Augmenting Data with\nMixup for Sentence Classification: An Empirical Study. In arXiv:1905.08941.\narXiv:1905.08941\n[17] Phillip Howard, Gadi Singer, Vasudev Lal, Yejin Choi, and Swabha Swayamdipta.\n2022. NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer\nData Augmentation. In Findings of the Association for Computational Linguistics:\nEMNLP 2022. Association for Computational Linguistics, Abu Dhabi, United Arab\nEmirates, 5056–5072. https://aclanthology.org/2022.findings-emnlp.371\nInvestigating Masking-based Data Generation in Language Models\n[18] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is BERT Really\nRobust? A Strong Baseline for Natural Language Attack on Text Classification and\nEntailment. In Proceedings of the AAAI Conference on Artificial Intelligence. AAAI\nPress, New York, NY, USA, 8018–8025. https://doi.org/10.1609/aaai.v34i05.6311\n[19] Fanshuang Kong, Richong Zhang, Xiaohui Guo, Samuel Mensah, and Yongyi\nMao. 2022. DropMix: A Textual Data Augmentation Combining Dropout with\nMixup. In Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, Abu Dhabi,\nUnited Arab Emirates, 890–899.\n[20] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data Augmentation\nusing Pre-trained Transformer Models. In Proceedings of the 2nd Workshop on\nLife-long Learning for Spoken Language Systems. Association for Computational\nLinguistics, Suzhou, China, 18–26.\n[21] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics. Association for Computational\nLinguistics, Online, 7871–7880. https://doi.org/10.18653/v1/2020.acl-main.703\n[22] Bohan Li, Yutai Hou, and Wanxiang Che. 2022. Data augmentation approaches\nin natural language processing: A survey. AI Open 3 (2022), 71–90.\nhttps:\n//doi.org/10.1016/j.aiopen.2022.03.001\n[23] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger:\nGenerating Adversarial Text Against Real-world Applications. In 26th Annual\nNetwork and Distributed System Security Symposium, NDSS 2019. The Internet\nSociety.\n[24] Qian Lin and Hwee Tou Ng. 2022. A Semi-supervised Learning Approach with\nTwo Teachers to Improve Breakdown Identification in Dialogues. In Proceedings\nof the AAAI Conference on Artificial Intelligence. AAAI Press, Online, 11011–11019.\nhttps://doi.org/10.1609/aaai.v36i10.21349\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. In arXiv:1907.11692.\narXiv:1907.11692 [cs.CL]\n[26] Shayne Longpre, Yu Wang, and Chris DuBois. 2020. How Effective is Task-\nAgnostic Data Augmentation for Pretrained Transformers?. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020. Association for Computa-\ntional Linguistics, Online, 4401–4411. https://doi.org/10.18653/v1/2020.findings-\nemnlp.394\n[27] David Lowell, Brian Howard, Zachary C. Lipton, and Byron Wallace. 2021. Unsu-\npervised Data Augmentation with Naive Augmentation and without Unlabeled\nData. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational Linguistics, Online and Punta\nCana, Dominican Republic, 4992–5001. https://doi.org/10.18653/v1/2021.emnlp-\nmain.408\n[28] Nitin Madnani and Bonnie J. Dorr. 2010. Generating Phrasal and Sentential\nParaphrases: A Survey of Data-Driven Methods. Computational Linguistics 36, 3\n(Sept. 2010), 341–387. https://doi.org/10.1162/coli_a_00002\n[29] Lin Miao, Mark Last, and Marina Litvak. 2020. Twitter Data Augmentation for\nMonitoring Public Opinion on COVID-19 Intervention Measures. In Proceedings\nof the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020. Association for\nComputational Linguistics, Online. https://doi.org/10.18653/v1/2020.nlpcovid19-\n2.19\n[30] Sebastien Montella, Betty Fabre, Tanguy Urvoy, Johannes Heinecke, and Lina\nRojas-Barahona. 2020. Denoising Pre-Training and Data Augmentation Strategies\nfor Enhanced RDF Verbalization with Transformers. In Proceedings of the 3rd\nInternational Workshop on Natural Language Generation from the Semantic Web\n(WebNLG+). Association for Computational Linguistics, Dublin, Ireland (Virtual),\n89–99.\n[31] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020.\nTextAttack: A Framework for Adversarial Attacks, Data Augmentation, and\nAdversarial Training in NLP. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations. Association for\nComputational Linguistics, Online, 119–126. https://doi.org/10.18653/v1/2020.\nemnlp-demos.16\n[32] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing\nXiang. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs\nand Beyond. In Proceedings of the 20th SIGNLL Conference on Computational\nNatural Language Learning. Association for Computational Linguistics, Berlin,\nGermany, 280–290. https://doi.org/10.18653/v1/K16-1028\n[33] Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi. 2020.\nSSMBA: Self-\nSupervised Manifold Based Data Augmentation for Improving Out-of-Domain\nRobustness. In Proceedings of the 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP). Association for Computational Linguistics,\nOnline, 1268–1283. https://doi.org/10.18653/v1/2020.emnlp-main.97\n[34] Tim Nugent, Nicole Stelea, and Jochen L. Leidner. 2021. Detecting Environmental,\nSocial and Governance (ESG) Topics Using Domain-Specific Language Models\nand Data Augmentation. In Flexible Query Answering Systems, Troels Andreasen,\nGuy De Tré, Janusz Kacprzyk, Henrik Legind Larsen, Gloria Bordogna, and\nSławomir Zadrożny (Eds.). Springer International Publishing, Cham, 157–169.\n[35] OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt/.\n[36] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training\nlanguage models to follow instructions with human feedback. In arXiv:2203.02155.\narXiv:2203.02155 [cs.CL]\n[37] Baolin Peng, Chenguang Zhu, Michael Zeng, and Jianfeng Gao. 2021. Data Aug-\nmentation for Spoken Language Understanding via Pretrained Language Models.\nIn Proc. Interspeech 2021. 1219–1223. https://doi.org/10.21437/Interspeech.2021-\n117\n[38] Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Weizhu Chen, and Jiawei\nHan. 2021. Co{DA}: Contrast-enhanced and Diversity-promoting Data Aug-\nmentation for Natural Language Understanding. In International Conference on\nLearning Representations. https://openreview.net/forum?id=Ozk9MrX1hvA\n[39] Hugo Queiroz Abonizio and Sylvio Barbon Junior. 2020.\nPre-trained Data\nAugmentation for Text Classification. In Intelligent Systems, Ricardo Cerri and\nRonaldo C. Prati (Eds.). Springer International Publishing, Cham, 551–565.\n[40] Husam Quteineh, Spyridon Samothrakis, and Richard Sutcliffe. 2020. Textual Data\nAugmentation for Efficient Active Learning on Tiny Datasets. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguistics, Online, 7400–7410. https:\n//doi.org/10.18653/v1/2020.emnlp-main.600\n[41] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.\nImproving language understanding by generative pre-training.\n[42] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners.\n[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of\nMachine Learning Research 21, 140 (2020), 1–67.\n[44] Guillaume Raille, Sandra Djambazovska, and Claudiu Musat. 2020. Fast Cross-\ndomain Data Augmentation through Neural Sentence Editing. In arXiv:2003.10254.\narXiv:2003.10254 [cs.CL]\n[45] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t\nKnow: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers).\nAssociation for Computational Linguistics, Melbourne, Australia, 784–789. https:\n//doi.org/10.18653/v1/P18-2124\n[46] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natural Language Pro-\ncessing. Association for Computational Linguistics, Austin, Texas, 2383–2392.\nhttps://doi.org/10.18653/v1/D16-1264\n[47] Chetanya Rastogi, Nikka Mofid, and Fang-I Hsiao. 2020. Can We Achieve More\nwith Less? Exploring Data Augmentation for Toxic Comment Classification. In\narXiv:2007.00875. arXiv:2007.00875 [cs.CL]\n[48] Mehdi Regina, Maxime Meyer, and Sebastien Goutal. 2021. Text Data Augmen-\ntation: Towards better detection of spear-phishing emails. In arXiv:2007.02033.\narXiv:2007.02033 [cs.CL]\n[49] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP). Association for Computational\nLinguistics, Hong Kong, China, 3982–3992. https://doi.org/10.18653/v1/D19-1410\n[50] Sam Shleifer. 2019. Low Resource Text Classification with ULMFit and Back-\ntranslation. In arXiv:1903.09244. arXiv:1903.09244 [cs.CL]\n[51] Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu,\nand Maosong Sun. 2021. Better Robustness by More Coverage: Adversarial and\nMixup Data Augmentation for Robust Finetuning. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021. Association for Computational\nLinguistics, Online, 1569–1576. https://doi.org/10.18653/v1/2021.findings-acl.137\n[52] Lichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Philip Yu, and Lifang\nHe. 2020. Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks. In\nProceedings of the 28th International Conference on Computational Linguistics. In-\nternational Committee on Computational Linguistics, Barcelona, Spain (Online),\n3436–3440. https://doi.org/10.18653/v1/2020.coling-main.305\n[53] Balazs Tarjan, Gyorgy Szaszak, Tibor Fegyo, and Peter Mihajlik. 2020. Deep\nTransformer based Data Augmentation with Subword Units for Morphologically\nRich Online ASR. In arXiv:2007.06949. arXiv:2007.06949 [eess.AS]\n[54] Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. 2021.\nAugmented SBERT: Data Augmentation Method for Improving Bi-Encoders for\nPairwise Sentence Scoring Tasks. In Proceedings of NAACL-HLT. Association for\nComputational Linguistics, Online, 296–310.\n[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,\nEd S. Ma\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.\narXiv:2302.13971 [cs.CL]\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Proceedings of NIPS (Long Beach, California, USA) (NIPS’17). Curran\nAssociates Inc., Red Hook, NY, USA, 6000–6010.\n[57] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. SwitchOut:\nan Efficient Data Augmentation Algorithm for Neural Machine Translation. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, Brussels, Belgium, 856–\n861. https://doi.org/10.18653/v1/D18-1100\n[58] Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques for\nBoosting Performance on Text Classification Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nAssociation for Computational Linguistics, Hong Kong, China, 6382–6388. https:\n//doi.org/10.18653/v1/D19-1670\n[59] Xiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng, Weihua Luo, and Rong Jin.\n2022. Learning to Generalize to More: Continuous Semantic Augmentation for\nNeural Machine Translation. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Association\nfor Computational Linguistics, Dublin, Ireland, 7930–7944. https://doi.org/10.\n18653/v1/2022.acl-long.546\n[60] Xing Wu, Chaochen Gao, Meng Lin, Liangjun Zang, and Songlin Hu. 2022. Text\nSmoothing: Enhance Various Data Augmentation Methods on Text Classification\nTasks. In Proceedings of the 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers). Association for Computational Lin-\nguistics, Dublin, Ireland, 871–875. https://doi.org/10.18653/v1/2022.acl-short.97\n[61] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. 2020.\nUnsupervised Data Augmentation for Consistency Training. In Proceedings of\nthe 34th International Conference on Neural Information Processing Systems (Van-\ncouver, BC, Canada). Curran Associates Inc., Red Hook, NY, USA, Article 525,\n13 pages.\n[62] Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, and\nAndrew Y. Ng. 2017. Data Noising as Smoothing in Neural Network Language\nModels. In Proceedings of 5th International Conference on Learning Representations,\nICLR 2017. OpenReview.net, Toulon, France.\n[63] Yinfei Yang, Ning Jin, Kuo Lin, Mandy Guo, and Daniel Cer. 2021. Neural Retrieval\nfor Question Answering with Cross-Attention Supervised Data Augmentation.\nIn Proceedings of ACL-IJCNLP (Volume 2: Short Papers). Association for Computa-\ntional Linguistics, Online, 263–268. https://doi.org/10.18653/v1/2021.acl-short.35\n[64] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Lan-\nguage Understanding. In Proceedings of the 33rd International Conference on\nNeural Information Processing Systems. Curran Associates Inc., Red Hook, NY,\nUSA, Article 517, 11 pages.\n[65] Semih Yavuz, Kazuma Hashimoto, Wenhao Liu, Nitish Shirish Keskar, Richard\nSocher, and Caiming Xiong. 2020. Simple Data Augmentation with the Mask\nToken Improves Domain Adaptation for Dialog Act Tagging. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguistics, Online, 5083–5089. https:\n//doi.org/10.18653/v1/2020.emnlp-main.412\n[66] Adams Wei Yu, David Dohan, Quoc Le, Thang Luong, Rui Zhao, and Kai Chen.\n2018. Fast and Accurate Reading Comprehension by Combining Self-Attention\nand Convolution. In International Conference on Learning Representations.\n[67] Shujuan Yu, Jie Yang, Danlei Liu, Runqi Li, Yun Zhang, and Shengmei Zhao. 2019.\nHierarchical Data Augmentation and the Application in Text Classification. IEEE\nAccess 7 (2019), 185476–185485. https://doi.org/10.1109/ACCESS.2019.2960263\n[68] Danqing Zhang, Tao Li, Haiyang Zhang, and Bing Yin. 2020. On Data Augmenta-\ntion for Extreme Multi-label Classification. In arXiv:2009.10778. arXiv:2009.10778\n[69] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2018.\nmixup: Beyond Empirical Risk Minimization. In International Conference on\nLearning Representations.\n[70] Yi Zhang, Tao Ge, and Xu Sun. 2020. Parallel Data Augmentation for Formality\nStyle Transfer. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Linguistics, Online,\n3221–3228. https://doi.org/10.18653/v1/2020.acl-main.294\n[71] Zhenjie Zhao, Evangelos Papalexakis, and Xiaojuan Ma. 2020. Learning Physical\nCommon Sense as Knowledge Graph Completion via BERT Data Augmentation\nand Constrained Tucker Factorization. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Online, 3293–3298. https://doi.org/10.18653/v1/2020.\nemnlp-main.266\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2023-06-16",
  "updated": "2023-06-16"
}