{
  "id": "http://arxiv.org/abs/2307.01990v1",
  "title": "Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks",
  "authors": [
    "Kai Feng",
    "Yongqiang Zhao",
    "Seong G. Kong",
    "Haijin Zeng"
  ],
  "abstract": "This paper presents a deep learning-based spectral demosaicing technique\ntrained in an unsupervised manner. Many existing deep learning-based techniques\nrelying on supervised learning with synthetic images, often underperform on\nreal-world images especially when the number of spectral bands increases.\nAccording to the characteristics of the spectral mosaic image, this paper\nproposes a mosaic loss function, the corresponding model structure, a\ntransformation strategy, and an early stopping strategy, which form a complete\nunsupervised spectral demosaicing framework. A challenge in real-world spectral\ndemosaicing is inconsistency between the model parameters and the computational\nresources of the imager. We reduce the complexity and parameters of the\nspectral attention module by dividing the spectral attention tensor into\nspectral attention matrices in the spatial dimension and spectral attention\nvector in the channel dimension, which is more suitable for unsupervised\nframework. This paper also presents Mosaic25, a real 25-band hyperspectral\nmosaic image dataset of various objects, illuminations, and materials for\nbenchmarking. Extensive experiments on synthetic and real-world datasets\ndemonstrate that the proposed method outperforms conventional unsupervised\nmethods in terms of spatial distortion suppression, spectral fidelity,\nrobustness, and computational cost.",
  "text": "1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n \n*\nUnsupervised Spectral Demosaicing with \nLightweight Spectral Attention Networks \n \nKai Feng, Yongqiang Zhao, Seong G. Kong, and Haijin Zeng \nAbstract—This paper presents a deep learning-based spectral \ndemosaicing technique trained in an unsupervised manner. Many \nexisting deep learning-based techniques relying on supervised \nlearning with synthetic images, often underperform on real-world \nimages especially when the number of spectral bands increases.  \nAccording to the characteristics of the spectral mosaic image, this \npaper proposes a mosaic loss function, the corresponding model \nstructure, a transformation strategy, and an early stopping \nstrategy, which form \na complete \nunsupervised spectral \ndemosaicing framework. A challenge in real-world spectral \ndemosaicing is inconsistency between the model parameters and \nthe computational resources of the imager. We reduce the \ncomplexity and parameters of the spectral attention module by \ndividing the spectral attention tensor into spectral attention \nmatrices in the spatial dimension and spectral attention vector in \nthe channel dimension, which is more suitable for unsupervised \nframework. This paper also presents Mosaic25, a real 25-band \nhyperspectral \nmosaic \nimage \ndataset \nof \nvarious \nobjects, \nilluminations, and materials for benchmarking. Extensive \nexperiments on synthetic and real-world datasets demonstrate \nthat the proposed method outperforms conventional unsupervised \nmethods in terms of spatial distortion suppression, spectral \nfidelity, robustness, and computational cost. \n \nIndex Terms—spectral demosaicing, unsupervised learning, \nspectral imaging, spectral attention networks \nI. INTRODUCTION \npectral imaging acquires both spectral as well as spatial \ninformation simultaneously, which have been useful  for \napplications including vaccine detection [1], clinical \nmonitoring[2], and object tracking[3]–[5]. However, a large \ndevice size and slow imaging speed of spectral cameras often \nlimit practical applicability of spectral imaging. Snapshot \nspectral cameras based on compact micro spectral filter arrays \n(SFAs) [6], [7] have been utilized to overcome the limitation of \nconventional spectral imaging sensors. \nDemosaicing is essential to reconstruct a full spectral image \nfrom the signals measured using an SFA-based imager. Many \nexisting demosaicing techniques can be divided into \nunsupervised and supervised methods. Most unsupervised \nmethods rely on interpolation [11]–[16], and conventional mod- \n \nThis work was supported by the National Natural Science Foundation of \nChina (NSFC) under Grant 61771391, the Key R & D plan of Shaanxi Province \n2020ZDLGY07-11, the Science, Technology and Innovation Commission of \nShenzhen \nMunicipality \nunder \nGrants \nJCYJ20170815162956949 \nand \nJCYJ20180306171146740, the National Research Foundation of Korea under \nGrant NRF-2016R1D1A1B01008522, the Fundamental Research Funds for the \nCentral Universities. (Corresponding author: Yongqiang Zhao.)  \n   Kai Feng, Yongqiang Zhao are with the School of Automation, Northwestern \n  \n \nFig. 1. Demosaicing results at 841nm by supervised deep \ndemosaicing networks SpNet [8], MCAN[9], InNet [10], and \nthe proposed unsupervised deep demosaicing network on a real \nspectral mosaic image of a leaf captured by a 25-band SFA-\nbased spectral imager. \n \nel optimization [17]–[19]. These methods are robust  yet \nunderperform in the high-frequency region. Supervised \nmethods [8]–[10], [20]–[23]  mostly rely on paired mosaic \nimages and fully defined cubes to train deep neural networks. \nCurrent fully defined image cubes used in spectral demosaicing \nare mostly synthesized by the dataset images taken by other \nnon-snapshot spectral cameras. As spectral resolution \nincreases, spatial resolution of a single band image decreases, \nand therefore demosaicing networks rely more on spectral \ncorrelations. However, the spectral correlations in synthetic \nimages are likely inaccurate [24]. Because the real image is the \nweighted integral of the continuous waveband images, we \nsynthesize it as the weighted sum of the discrete waveband \nimages in the dataset. The spectral response of each waveband \nimage in the dataset is a narrow band function treated as a unit \nimpulse function [25]. Such reasons may cause the supervised \nmethods trained by synthetic images usually fail in real-world \nPolytechnical \nUniversity, \nXi’an \n710072, \nChina \n(e-mail: \n2018100620@mail.nwpu.edu.cn; zhaoyq@nwpu.edu.cn). \nSeong G. Kong is with the Department of Computer Engineering, Sejong \nUniversity, Seoul 05006, Korea (e-mail: skong@sejong.edu). \nHaijin Zeng is with the Image Processing and Interpretation imec research \ngroup \nat \nGhent \nUniversity, \n9000 \nGhent, \nBelgium \n(e-mail: \nHaijin.Zeng@UGent.be) \nS\n2 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \ncases. Fig. 1 shows demosaicing results of several \nrepresentative supervised spectral demosaicing techniques \ntrained using synthetic images. The supervised methods \nproduce global spatial distortions or artifact in the edge. Higher-\nquality datasets and better synthesis methods can alleviate the \nproblem, which is not trivial to achieve. This paper solves the \nstated problem by training neural network directly on real \nmosaic images in an unsupervised manner.  \nInspired by unsupervised equivariant imaging (EI) [26], we \npropose an Unsupervised Spectral Demosaicing (USD) \nframework including training method, model structure, \ntransformation enhancement strategy, and early stopping \nstrategy. With the USD framework, we do not need to prepare \npaired synthetic data and only use the real mosaic images to \ntrain deep networks. Specifically, we first design an \nunsupervised observation loss function based on the sampling \nmatrix of spectral mosaic image; To prevent the model from \ncollapsing, we propose a necessary condition on the model \nstructure: the interpolation branch; Considering that the \nperiodic sampling characteristic of the SFA pattern makes the \nnumber of intuitive shifting transformations in EI limited, we \nthen propose a mixed transformation strategy including \nshifting, rotation, resizing, and flipping to enhance the \nperformance of USD; To further detect the model overfitting in \nUSD, we also propose a self-evaluation index, which measures \nthe similarity between the periodic down sampled sub-images \nof each band image. Utilizing this index, we can automatically \nstop training at the appropriate epoch. \nAnother challenge with deep spectral demosaicing is \ninconsistency between the model parameters and computational \nresources of the imager. Demosaicing is essential in SFA-based \nspectral imaging that ideally needs to be implemented into the \ndevice [27]. In practice, however, snapshot spectral imaging \ndevices have limited memory spaces to store a large number of \nparameters [28]. Spectral attention is an effective technique in \nsupervised spectral demosaicing [9], but with a large amount of \nparameters and fails in USD. This paper presents a lightweight \nspectral attention (LSA) module with fewer parameters by \ndividing the spectral attention tensor into the spectral attention \nmatrixes among spatial dimension and the spectral attention \nvector among channel dimension. The proposed LSA not only \nreduces 99.8% of the parameters of the spectral attention \nmodule, but also improves the performance of USD. \nTo verify the validity of the proposed scheme, we built \nMosaic25, a real-world 25-band hyperspectral mosaic image \ndataset containing various objects, illuminations, and materials. \nThis dataset is expected to serve as a benchmark data in the \nfuture.  \nExtensive experiments were conducted on synthetic data and \na real-world 25-band hyperspectral mosaic image dataset to \ncompare the performances of the proposed technique with the \nstate-of-the-art spectral demosaicing methods. Experiment \nresults show that our proposed method outperforms \nconventional supervised and unsupervised techniques in terms \nof spatial distortion suppression, spectral fidelity, robustness, \nand computational cost.  \nMain contributions of the proposed demosaicing technique \nare: \n1) Presents an unsupervised spectral demosaicing (USD) \nframework including loss function, model structure, \ntransformation strategy, and early stopping strategy. \n2) Proposes a lightweight spectral attention module that \nsignificantly reduces the number of parameters and \nimproves the performance of USD. \n3) Develops Mosaic25, a real-world 25-band hyperspectral \nmosaic image dataset containing various objects, \nilluminations, and materials. \nII. SPECTRAL DEMOSAICING TECHNIQUES \nThis section introduces the spectral demosaicing problem, \nthe proposed unsupervised spectral demosaicing framework, \ntogether with the designed lightweight spectral attention \nmodule. \nA. Problem Overview \nA common spectral filter array is periodic. The mosaic \nsampling can be written as: \n1\nB\nb\nb\nb\nY\nX\nM\n=\n= ∑\n\n                               (1) \nwhere \nH W B\nX\n×\n×\n∈\nrepresents the fully defined spectral cube, \nwith H, W ,and B as the height, width, and band numbers of the \nimage, respectively, \nH W\nb\nX\n×\n∈\n,\n1,...,\nb\nB\n∀=\n represents the b-\nth band image, \nH W B\nM\n×\n×\n∈\ndenotes the mosaic sampling mask \nof SFA, \nb\nM\nis the b-th mask, and  \nH W\nY\n×\n∈\ndenotes the \nobserved spectral mosaic image. \n \nFig. 2. A schematic diagram of the sampling and demosaicing \nprocess of a spectral mosaic image. A 4×4 SFA pattern is shown \nas an example.  \n \nThe mosaic sampling is irreversible. The goal is to construct \na module, e.g., a deep demosaicing network, whose input is Y \nand the output is the estimated fully defined cube ˆX , as shown \nin the bottom part of Fig. 2. Previous supervised studies \n3 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nminimize the loss between the estimated ˆX  and ground truth \nX  to optimize the parameters θ  of the network. The final \noptimized parameters ˆθ can be formulated as: \n(\n)\n(\n)\nˆ\nˆ\narg min\n,\narg min\n( , ),\nX X\nF Y\nX\nl\nl\nθ\nθ\nθ\nθ\n=\n=\n       (2)  \nwhere l (·)  is the loss function for the fully defined spectral cube. \nIn most supervised studies, X is synthesized using data from \nother non-snapshot spectral cameras, so the paired Y-X can be \neasily obtained to make the above training process work. \n \nB. Unsupervised Spectral Demosaicing Framework \nMotivation. In general, it is not easy to obtain the real fully-\ndefined radiance spectral cube X. In CFA (Color Filter Array) \ndemosaicing, researchers \nbenefit \nfrom \nwell-established \ncommercial cameras to capture ground truth, i.e., fully defined \ncube, by shifting the sensor chip [29], which is expensive and \nimperfect for the SFA imager [30]. Also inspired by equivariant \nimaging [26], we shift the demosaiced cube output by the \nnetwork, then re-sample this demosaiced cube with the known \nsampling matrix and feed the new spectral mosaic image to the \nprevious network. The new demosaiced cube should be the \nsame as the previous demosaiced cube if this network \ndemosaics perfectly. Based on the above motivations, we \ndesign an unsupervised spectral demosaicing framework. Fig. 3 \nshows the overall schematic diagram of the USD framework. \nBasic training method. Here we describe the detailed \ntraining process of USD. First, we feed the real mosaic image \nY that has no paired spectral cube X to demosaicing network:  \n( ; )\nˆ\nF Y\nX\nθ\n=\n                               (3) \nwhere ˆX  is the first output cube of the demosaicing network F . \nThen we shift the first output of the network ˆX . Considering \nthe mosaic sampling matrix is periodic, we randomly shift the \nmosaic sampling matrix only at the range of one period, which \nis different from the original global shift operation in the \ninpainting task [26], the shifted cube ˆX ′   can be represented as: \nˆ\nˆ\n(\n, , )\nshift\nX\nT\nX i j\n′ =\n                           (4) \nwhere\n[\n]\n1\n1,\ni\nr\n∈\nand \n[\n]\n2\n1,\nj\nr\n∈\nrepresent the number of pixels to \nshift in the horizontal and vertical directions of the spatial \ndimension, 1r ,\n2r are the pattern size of SFA.  \nThen we take the ˆX ′  as pseudo fully defined cube and \nsample ˆX ′ to obtain the new pseudo spectral mosaic image \n(SMI) ˆY′: \n1\nˆ\nˆ\nB\nb\nb\nb\nY\nX\nM\n=\n′\n′\n= ∑\n\n                             (5) \nNow, we obtain the pseudo paired ˆY ′ - ˆX ′ . We feed ˆY′ to \nthe demosaicing network used before to obtain the second \noutput cube X ′: \nˆ\n(\n; )\nF Y\nX\nθ\n=\n′\n′\n                                (6) \n \nFig. 3. A schematic diagram of the proposed unsupervised spectral demosaicing (USD) framework. We feed the real spectral \nmosaic image (SMI) Y to spectral demosaicing model and obtain the first demosaiced cube ˆX . Then we transform and re-\nsample this cube to obtain the pseudo fully defined ground truth cube ˆX ′ and SMI ˆY′, and feed ˆY′to the previous model to \nobtain the second demosaiced cube X ′. Finally, with the mosaic loss between Y and ˆX , the cube loss between X ′ and ˆX ′ , we \ncan train spectral demosaicing model in an unsupervised manner.  \n4 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nThen, we can solve the following optimization problem for \nspectral demosaicing without the real fully defined spectral \ncube X : \n(\n)\n(\n)\n{\n}\n(\n)\n(\n)\n{\n}\nˆ\nˆ\nˆ\narg min\n,\n,\nˆ\nˆ\narg min\n(\n; ),\n( ; ),\ncube\nmosaic\ncube\nmosaic\nl\nX\nX\nl\nX Y\nl\nF Y\nX\nl\nF Y\nY\nθ\nθ\nθ\nα\nθ\nα\nθ\n′\n′\n=\n+\n=\n+\n′\n′\n\n      (7) \nwhere  \n( )\ncube\nl\n⋅ is the equivariant loss between shifted cubes, we \nuse Charbonnier L1 loss [31] to implement \n( )\ncube\nl\n⋅, \n( )\nmosaic\nl\n⋅is \nthe proposed mosaic loss function, hyper-parameter α  is used \nto control the weight of mosaic loss. \nMosaic loss. In the spectral demosaicing task, the known \npixels in a raw spectral mosaic image are valuable and reliable. \nWe hope the demosaiced cube can be sampled back to the raw \nspectral mosaic image, which is also a necessary condition for \nthe establishment of unsupervised imaging, therefore, we \nrepresent the mosaic loss as follows: \n(\n)\n1\n1\n1\nˆ\nˆ\n,\nB\nmosaic\nb\nb\nb\nl\nX Y\nX\nM\nY\nH\nW\n=\n=\n−\n×\n∑\n\n            (8) \nwhere M is the mosaic sampling mask of SFA, H, W are the \nheight and width of the raw spectral mosaic image, \n1⋅is the \nCharbonnier L1 loss. \nModel Structure. From (7), there exists a model collapse \nwhen the network only repeats the input mosaic image at the \nspectral dimension as output. This model can also make the loss \nvalue the lowest but without demosaicing. So we add the \ninterpolated cube that is non-mosaic to the output of the \nnetwork, as shown in Fig. 4. This interpolated cube is the main \nlow-frequency component. We encourage the network to \npredict the residual high-frequency component to avoid the \nabove model collapse. This paper uses classical weighted \nbilinear (WB) interpolation algorithm [16] which interpolates \neach sparse band independently so that the interpolated cube \nwill not have any periodic mosaic characteristic. \n \nFig. 4. A schematic diagram of the proposed model structure for \nUSD framework, where WB means the weighted interpolation. \n \nMixed Transformation Strategy. According to the conclusion \nin EI [26], the more the number of transformations  means the \nbetter performance of equivariant learning. Meanwhile the \nnumber of valid shift transformations in demosaicing is little, \nwhich is just r1×r2, where r1, r2 are the size of the SFA pattern. \nSo, we propose a mixed transformation enhancement strategy \nthat contains shift, flip, rotation, and resize operations to enrich \nthe number of transformations, as shown in Fig. 3.  \nEarly Stopping Strategy. In supervised training, we can \ndetect overfitting, stop the training procedure, and select a good \nmodel by computing the error between the output of the \nnetwork and the ground truth on validation dataset. In an \nunsupervised training with no ground truth, however, finding \nan early stopping criterion is not easy. So, we study overfitting \nof proposed unsupervised spectral demosaicing training. We \nfound that when the unsupervised spectral demosaicing model \noverfits, there will be periodic distortion in the space of the \noutput cube, as shown in Fig. 5. Therefore, we propose a self-\nevaluation index to measure the degree of periodic distortion in \neach band image of output cube. \n \n        \n \n6000-th epoch                     11000-th epoch \nFig. 5. The pseudo color results of the demosaicing model \nunder USD training of different training epochs. With the \nincrease in the number of training epochs, the model appears \noverfitting, which is manifested as periodic distortion in space. \n \nSince we found that the period of distortion is consistent with \nthe SFA pattern size, we first rearrange the b-th band image \nbx  \nof output cube using Inverse Pixel Shuffle (IPS) technology [32] \ninto r1r2 sub-images, then we calculate the variance between the \nglobal mean of these sub-images: \n  \n(\n(\n(\n)))\nb\nb\nv\nVar Mean IPS x\n=\n                     (9) \nConsidering this periodic distortion exists in each band \nimage, the self-evaluation index (SEI) of the whole cube is \ndefined as: \n1\n1\nB\nb\nb\nSEI\nv\nB\n=\n=\n∑\n                            (10) \nwhere B is the number of bands, in a non-redundancy SFA \npattern \n1\n2\nB\nr\nr\n=\n×\n. \nTherefore, we can detect whether the model overfits by \nsupervising the SEI change curve rather than observing the \noutput image of every training epoch manually. \nAlgorithm 1 shows the pseudo code of the USD algorithm. \nAlgorithm 1 Pseudo code of the USD algorithm in a \nPyTorch-like style. \n# SDModel is the deep spectral demosaicing model with an \ninterpolation branch \nwhile (1): \n      for y in minibatchs: # y is one minibatch that contains \nsome mosaic images \n            x = DMModel(y) \n            x_trans = random_transform(x) # this function \nrandomly selects the transformation from the shift, \nflip, rotation, and resize operations. \n            y_trans = mosaic_sampling(x_trans) # this function \nsamples the cube x_trans using the mosaic SFA \npattern \n            x_trans1 = SDModel(y_trans) \n            loss = CubeLoss(x_trans1, x_trans) \n      + MosaicLoss(x, y) \n5 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n            loss.backward() \n            update(SDModel.params) \nsei = SEI_compute(SDModel, yval) # yval are evaluation \nspectral mosaic images \n      if sei > sei_max: \n             break # overfitting happens, stop the training  \n \nC. Lightweight Spectral Attention Module \nMotivation. The spectral attention module has been proven \neffective in supervised spectral demosaicing [9]. The spectral \ncorrelation is distributed on the space and channel of deep \nfeature maps. The spectral attention module learns a 3D \nattention weight tensor for adjusting the spectral feature map. \nThis way introduces a large number of weight parameters, so \nwe call it heavyweight spectral module (HSA), and we found it \nfailed in USD. More details can be found in Section III.C. We \npropose a lightweight spectral attention (LSA) module that \nsplits the attention weight tensor into spectral attention 2D \nmatrixes in space and spectral attention 1D vector in channel, \nas shown in Fig. 6, aiming to reduce the number of learning \nparameters of the spectral module and make it useful in USD. \nSpectral attention in space. We first obtain the spectral \nattention matrix for each channel of the feature map. The \nnetwork architecture of spectral attention in space is like that of \nHSA in [9], but the input to this network is just a single channel \nfeature map. Each channel of the feature map shares the same \nattention network to obtain the dedicated spectral attention \nmatrix, which can reduce the number of parameters \nsignificantly. This process can be expressed as:  \n(\n(:,:, ),\n),\n{1,\n, }\ni\nsas\nsas\nAm\nf\nFm\ni\ni\nC\nθ\n=\n∈\n\n       (11) \nwhere \ni\nAm is the attention matrix of the i-th channel of feature \nmap Fm , \n( )\nsas\nf\n⋅is the function of spectral attention in space, \nsas\nθ\nare the learning weights of spectral attention in space. C is \nthe channel number of the feature map. \nSpectral attention in channel. Then we refer to the channel \nattention in [33] to generate the spectral attention vector in \nchannel Av  as follows: \n(\n,\n)\nsac\nsac\nAv\nf\nFm θ\n=\n                        (12) \nwhere \n( )\nsac\nf\n⋅is the function of spectral attention in channel, \nsac\nθ\nare the learning weights of spectral attention in channel. \nFinally, we multiply the initial feature map Fm by the \nattention matrixes Am and attention vector Av to obtain the \nrefined feature map ˆFm as follows: \nˆFm\nFm Am Av\n=\n⋅\n⋅\n                          (13) \nDiscussion of the number of parameters. The core network \narchitecture of LSA and HSA are all squeeze-excitation modes \n[33], so the parameters can be easily calculated from the \nnumber of input sizes and the reduction ratio d.  \nFor one feature map with C channels in spectral demosaicing \nnetworks of the 1\n2\nr\nr\n×\nSFA pattern, the number of parameters of \nits corresponding HSA is\n2\n2\n2\n1\n2\n2\n/ .\nr r C\nd   \nThe numbers of parameters of the corresponding spectral \nattention in the space and in the channel are \n2\n2\n1\n2\n2\n/\nr r\nd  and\n2\n2\n/\nC\nd , respectively \nThe total number of parameters of LSA is: \n2\n2\n2\n2\n2\n2\n1\n2\n1\n2\n2\n/\n2\n/\n2(\n) /\nr r\nd\nC\nd\nr r\nC\nd\n+\n=\n+\n     (14) \nTypically, C in deep neural networks is selected as a \nrelatively large number such as 64, the SFA pattern size 1\n2\nr\nr\n×\n \nin this paper is 5×5. Therefore, the design of LSA can reduces \nabout 99.8% parameters compared to HSA. \nIII. EXPERIMENT RESULTS \nA. Implementation Details \nWe replace heavyweight spectral attention (HSA) with  the \nproposed lightweight spectral attention (LSA) on the basis of \nMCAN [9] as the model for subsequent experiments. We \n \nFig. 6. A schematic diagram of the proposed lightweight spectral attention model structure. \n6 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nvalidate our algorithm in a challenging 25-band ( 1\n2\n5\nr\nr\n=\n=\n) \nsnapshot multispectral camera. The setup of the input layer, \noutput layer are modified according to the 5×5 pattern. We set \nthe reduction ratio d =4 in LSA, and the weight hyper-parameter \nα  =1 in loss.   \n \nB. Datasets and Metrics \nIn the experiments with synthetic data, we use hyperspectral \nimages \ndataset \ngenerated \nby \nthe \nInterdisciplinary \nComputational Vision Lab (ICVL) [34] to simulate snapshot \nmultispectral mosaic images, which can validate the \nperformance of our demosaicing algorithm qualitatively and \nquantitatively. We select 40 images as the training dataset and \n10 images as the testing dataset. We sampled the original cube \ninto a 25-band cube using spectral sensitive functions provided \nby the manufacturer IMEC. We use PSNR, SSIM [35], SAM \n[36], and ERGAS [37] as the performance evaluation metrics in \nthe experiments with synthetic data. \nIn the experiments with real data, we propose Mosaic25, a \nreal-world 25-band hyperspectral mosaic image dataset of \nvarious objects, illuminations, and materials. The images in our \ndataset are captured by one IMEC 25-band snapshot spectral \ncamera which spectrum ranges from 600nm~900nm. We \nselected 40 regions of interest from these mosaic images as a \ntraining dataset. Examples of these image patches are shown in \nFig. 8. Then we additionally provide another 17 full images \nwith size of 2045×1080 as a testing dataset in a variety of \nobjects and lighting conditions. The nature of the scenes ranges \nfrom indoor environments to outdoor environments. Our \ndataset is expected to serve as a common benchmark for \nspectral demosaicing. We use the no-reference spectral image \nevaluation metric proposed in [38] to show the performance \nquantitatively in the experiments with real data. \n \n \n \nFig. 7. PSNR values of the model with interpolation branch and \nthe model without interpolation at different epochs on ICVL \ndataset. \n \n \nFig. 8. Training examples of the proposed Mossaic25 Dataset, \ncaptured by a real IMEC 25-band snapshot spectral camera. Our \ndataset covers various objects, illuminations, and materials. \n \nWe randomly crop one patch from the training dataset to form \na training batch. The spatial size of the patch is 100×100. We \nset the learning rate as 1×10-4. \n \nC. Ablation Study \nThe significance of the proposed unsupervised spectral \ndemosaicing model structure. To verify the significance of the \nproposed model structure in USD, we train one model without \nand one model with an interpolation branch on the ICVL \ntraining dataset in the proposed unsupervised training manner \nand plot their PSNR values on the ICVL test dataset as a \nfunction of training epochs in Fig. 7. The model without \ninterpolation branch does not work, but the model with an \ninterpolation branch performs very well illustrating the \nsignificance of the interpolation branch in the USD model \nstructure.  \n \nEffectiveness of unsupervised early stopping strategy. To \nverify the effectiveness of the self-evaluation index (SEI) in \nunsupervised early stopping, we plot the PSNR value and SEI \nvalue of our model on the ICVL testing dataset as a function of \ntraining epochs in Fig. 9. As the number of epochs grow to 6500, \nthe PSNR begins to decrease, indicating the overfitting occurs. \nAt the same time, the SEI value begins to rise and fluctuate in a \nwide range. Therefore, we can use the SEI value to detect \nwhether the USD model overfits, and select an appropriate \nmodel. For the experiments with synthetic data on the ICVL \ndataset, we set up the maximum SEI as 2.1×10-7. For the \nexperiments with real data on the Mosaic25 dataset, we set up \nthe maximum SEI as 1×10-6.  \n \n7 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n \nFig. 9. PSNR and SEI value of proposed method at different \nepochs on ICVL dataset. The decrease in PSNR is in line with \nthe rise in SEI overall. \n \nComparison with different transformation strategies. To \nverify the effectiveness of the proposed mixed transformation \nstrategy, we compare the performance of the model with \ndifferent training strategies. Specifically, three models are \nexploited here. The first model is denoted ShiftTran, which only \nuses the shifting transformation strategy. The second model is \ndenoted MixTrans, which is trained by the proposed mixed \ntransformations strategy. The last model is trained by the \nclassical supervised training strategy, denoted Supervised. \nTable I shows the average results obtained by ShiftTran, \nMixTrans, and Supervised over all testing images in the ICVL \ndataset. From this table, we can observe mixed transformations \nstrategy is more effective than the shifting transformation \nstrategy and close to the classical supervised training strategy. \nTABLE I \nAVERAGE PERFORMANCE OF DIFFERENT TRAINING \nSTRATEGIES OVER ICVL TESTING DATASET \nMethods \nPSNR↑ \nSSIM↑ \nSAM↓ \nERGAS↓ \nUSD \nShiftTran \n46.67 \n0.9959 \n0.99 \n2.99 \nMixTrans \n48.89  \n0.9977  \n0.86  \n2.24  \nSupervised \n50.19  \n0.9983  \n0.75  \n1.93  \n \n \nFig. 10. PSNR values of the model with HSA and the model \nwith LSA at different training epochs on ICVL dataset. \n \nComparison with heavyweight spectral attention module. \nTo verify the efficiency of the proposed lightweight spectral \nattention module, we train the base model without the spectral \nattention module labelled as Base, the model with heavyweight \nspectral attention labelled as HSA to compare with our model \nlabelled as LSA here. Both models are trained under the USD \nframework. \nTABLE II \nAVERAGE PERFORMANCE OVER THE ICVL TESTING \nDATASET AND THE NUMBER OF MILLION PARAMETERS OF THE \nBASE MODEL WITH DIFFERENT SPECTRAL MODULE UNDER THE \nUSD FRAMEWORK  \nModel \nPSNR↑ \nSSIM↑ \nSAM↓ \nERGAS↓ \nParam(M) \nBase \n48.59  \n0.9975 \n0.87  \n2.33  \n0.411 \nHSA \n46.67  \n0.9964  \n1.08  \n2.85  \n3.166 \nLSA \n48.89  \n0.9977  \n0.86  \n2.24  \n0.416 \n \nTable II shows the average results over the ICVL testing \ndataset and the number of million parameters of Base, HSA, and \nLSA. The parameters of HSA are an order of magnitude more \nthan those of the Base, but its performance is worse, which \nshows that HSA fails under USD framework. Compared to HSA, \nLSA not only reduces the parameters, but also outperforms Base.     \nTABLE III \nDEMOSAICING PERFORMANCE (PSNR/SSIM/SAM/ERGAS) AT THREE TYPICAL SCENES AND THE AVERAGE PERFORMANCE \nOVER ALL TEST SCENES FOR DIFFERENT METHODS IN ICVL DATASET \nMethods \nNachal \nGrf \nBgu \nAverage of all \nPSNR SSIM SAM ERGAS PSNR SSIM SAM ERGAS PSNR SSIM SAM ERGAS PSNR SSIM SAM ERGAS \nWB \n36.42 0.964 1.35 \n6.25 \n37.44 0.989 1.33 \n7.36 \n35.18 0.988 1.60 \n6.67 \n39.12 0.982 1.47 \n6.52 \nItSD \n35.96 0.950 1.55 \n7.35 \n39.46 0.993 1.44 \n6.01 \n37.51 0.993 1.82 \n5.28 \n40.80 0.986 1.61 \n5.65 \nPPID \n37.87 0.971 1.19 \n5.59 \n40.11 0.994 1.11 \n5.42 \n38.33 0.994 1.31 \n4.62 \n41.92 0.990 1.23 \n4.81 \nOurs \n45.22 0.994 0.75 \n2.41 \n46.75 0.999 0.81 \n2.56 \n46.73 0.999 0.72 \n1.79 \n48.89 0.998 0.86 \n2.24 \nIdeal Value \n↑ \n↑1 \n↓0 \n↓0 \n↑ \n↑1 \n↓0 \n↓0 \n↑ \n↑1 \n↓0 \n↓0 \n↑ \n↑1 \n↓0 \n↓0 \n \n8 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nFig. 10 shows PSNR values of HSA and LSA at different \ntraining epochs on ICVL dataset. Overfitting occurs early in \nHSA, and the maximum value of PSNR is relatively low, which \nfurther proves that the proposed lightweight spectral attention \nmodule is more suitable for USD framework.  \n \nD. Comparisons on Synthetic Data \n  We then evaluate the proposed model (the base model with \nour lightweight spectral module) under the proposed USD \ntraining framework on the synthetic ICVL dataset. We \nconducted experiments to compare with state-of-the-art \nunsupervised and supervised methods. \nComparison \nmethods. \nThe \nunsupervised \nspectral \ndemosaicing methods for comparison are WB [16] , ANL [18], \nItSD [12], and PPID [13]. ANL is a non-local based method, \ntesting on the full ICVL image is very time-consuming, so we \njust evaluate ANL qualitatively on the 250×250 patch, which \nwill slightly decrease its performance. The supervised \ncomparison methods include representative deep neural \nnetwork based methods: SpNet [8], MCAN [9], and InNet [10] \nall retrained on the ICVL dataset. \nPerformance comparison on unsupervised methods. \nTable III lists the performance at three representative scenes \nand average performance over all ICVL testing scenes of three \ncomparison methods and our method. The proposed method \nsignificantly outperforms other competing methods on all \nevaluation metrics. Figs. 11 and 12 show the (23, 13, 5)-band \nresults in pseudo color and the radiance spectrum of the \nrepresentative points of the three scenes of all methods. Figs. \n13 and 14 show (1, 22, 12)-band results in pseudo color and the \nradiance spectrum of another three scenes. Our proposed \nmethod outperforms the other techniques for comparison, in \nboth recovery of spatial textures and spectral fidelity. \nPerformance comparison on supervised methods. Table \nIV lists the average quantitative indexes of supervised methods \nand our unsupervised method on ICVL dataset. In terms of \nPSNR and SSIM reflecting the spatial recovery performance, \nour method just has a small gap with the best MCAN, and is \n \nFig. 11. The local details of the pseudo color ground truth and demosaicing results of the conventional unsupervised methods \nand the proposed method for three ICVL scenes.  \n \nFig. 12. The radiance spectrum of the purple marked points in Fig. 11 of ground truth and demosaicing results of the \nconventional unsupervised methods and the proposed method. \n9 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nbetter than InNet. In terms of SAM reflecting the spectral \nrecovery performance, our method is only inferior to the best \nInNet. Our method is competitive even compared with the \nsupervised method on synthetic data. \n \n \nTABLE IV \nAVERAGE PERFORMANCE OF REPRESENTATIVE SUPERVISED \nMETHODS AND OUR UNSUPERVISED METHOD OVER ALL ICVL \nTESTING DATASET (THE BEST RESULT IS BOLD IN BLACK, AND \nTHE SECOND BEST RESULT IS UNDERLINED IN BLUE) \nMethods \nPSNR↑ \nSSIM↑ \nSAM↓ \nERGAS↓ \nSpNet \n49.26 \n0.9979 \n0.871 \n2.11 \nMCAN \n49.93 \n0.9982 \n0.865 \n1.97 \nInNet \n48.03 \n0.9972 \n0.764 \n2.46 \nOurs \n48.89 \n0.9977 \n0.861 \n2.24 \n \nE. Comparisons of Real Data \nWe then evaluate our method in the case that training and \ntesting data are all from the proposed real-world Mosaic25 \ndataset without ground-truth data, as compared with existing \nunsupervised methods and representative supervised methods. \n \nTABLE V \nNO-REFERENCE PERFORMANCE OF ALL COMPETING \nMETHODS AT THREE TYPICAL SCENES AND AVERAGE \nPERFORMANCE OVER REAL MOSAIC25 TESTING DATASET \nMethods \nCard \nGate1 \nGate2 \nAverage \nWB \n21.94  \n22.18  \n21.06 \n21.72  \nItSD \n20.33  \n19.60  \n19.95 \n19.91  \nPPID \n20.96  \n19.60  \n19.61 \n19.82  \nOurs \n19.46  \n19.57  \n19.41 \n19.72  \n \n \nFig. 13. The local details of the pseudo color ground truth and demosaicing results of conventional unsupervised methods \nand the proposed method for three ICVL scenes.  \n \nFig. 14. The radiance spectrum of the purple marked points in Fig. 13 of ground truth and demosaicing results of conventional \nunsupervised methods and the proposed method. \n10 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nPerformance comparison on unsupervised methods. We \ncompare the performance of all competing unsupervised \nmethods on the Mosaic25 real-world dataset. Table V shows the \nno-reference spectral image evaluation [38] results of three \nrepresentative scenes and the average result of overall testing \ndataset. Our method significantly outperforms other methods. \nFigs. 12 and 13 visually compares different pseudo color results \non six representative scenes obtained by all competing methods. \nOur method achieves better results than other methods and is \nrobust on real data.  \n \nPerformance \ncomparison \non \nsupervised \nmethods. \nAlthough we have explained in the Introduction section that the \nsupervised method trained on synthetic data does not perform \nwell in real data, we show the more results of the supervised \nmethod and our proposed unsupervised method in Fig. 17. The \nSpNet and MCAN show strong periodic distortion, and SpNet \nhas color distortion, indicating the loss of spectral information. \nThe InNet shows artifacts in high-frequency areas. Our method \noutperforms these representative supervised methods in terms \nof spectral fidelity, spatial details and distortion suppression. \n \nF. Running Times and Computational Cost \nDemosaicing is a fundamental step for an SFA-based camera, \nso the running time and computational cost of demosaicing \nmethod is an important evaluation dimension. We test the \nrunning times of all the comparison method and our method \nwhen demosaicing one 1300×1390 mosaic image on one \nIntel(R) Xeon(R) Gold 6240 CPU and NVIDIA RTX 2080Ti \n \nFig. 15. The local details of real-world raw mosaic and pseudo color demosaicing results of the unsupervised methods for \ncomparison and our method for three real scenes from Mosaic25. \n \nFig. 16. The local details of real-world raw mosaic and pseudo color demosaicing results of the unsupervised methods for \ncomparison and our method for three real scenes from Mosaic25. \n11 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nGPU. The number of million parameters and TFLOPs of deep \nmodels are also test. In Table VI, the running time are averaged \nover 50 times and the first image are not included to avoid the \nimpact of machine startup, the running time of the ANL method \nis obtained by reasoning the running time on a patch of 250×250 \nsize, InNet is based on the memory-consuming 3D convolution, \nand cannot get complete results at one time, so we test the image \nin blocks for InNet.  The CPU running speed of our method is \nthe second only to the fastest WB method. The GPU running \nspeed of SpNet, MCAN, and our proposed method are similar, \nand can reach more than 200 frames per second, but the \nparameters and TFLOPs of our proposed method are smaller. \nOur proposed method is more comprehensive and suitable for \ndeployment on real devices. \nTABLE VI \nRUNNING TIMES, NUMBER OF MILLION PARAMETERS, AND \nTFLOPS OF ALL COMPETING METHODS ON ONE 25-BAND \nSPECTRAL MOSAIC IMAGE OF SIZE 1300×1390 (THE BEST \nRESULT IS BOLD IN BLACK, AND THE SECOND BEST RESULT IS \nUNDERLINED IN BLUE. THE ‘\\’ MEANS THIS TERM IS NOT \nCOUNTED IN NON-DEEP LEARNING METHODS) \nPlatform Methods Running time(s) #Parameters \n(Million) \nTFLOPs \nCPU \nWB \n0.8 \n\\ \n\\ \nItSD \n73.1 \n\\ \n\\ \nPPID \n35.1 \n\\ \n\\ \nNL \n572400.0 \n\\ \n\\ \nOurs \n4.7 \n0.4  \n   0.5   \nGPU \nSpNet \n0.004 \n1.2  \n   2.3   \nMCAN \n0.003 \n3.2  \n   0.5   \nInNet \n7.0 \n0.9  \n  39.0   \nOurs \n0.004 \n0.4  \n   0.5   \n \nIV. CONCLUSION \nThis paper presents an unsupervised spectral demosaicing \nframework including the basic training method, model structure, \nenhancement strategy, early stopping strategy, and a \nlightweight attention network module. Together with the \neffective and efficient series of solutions proposed above, the \nproposed spectral demosaicing method holds promise for future \ndeployment into practical devices and the creation of a real \nspectral video dataset with high spectral-spatial-temporal \nresolution. Future research extension of this work includes \nunsupervised joint denoising and demosaicing [39], [40], no-\nreference image quality assessment [41] , super-resolution [42], \nand object detection [43], [44]. \nREFERENCES \n[1] \nA. Yin, Y. Wang, Y. Chen, K. Zeng, H. Zhang, and J. Mao, “SSAPN: \nSpectral-Spatial Anomaly Perception Network for Unsupervised \nVaccine Detection,” IEEE Transactions on Industrial Informatics, pp. \n1–12, 2022, doi: 10.1109/TII.2022.3195168. \n[2] \nJ. R. Bauer, A. A. Bruins, J. Y. Hardeberg, and R. M. Verdaasdonk, “A \nSpectral Filter Array Camera for Clinical Monitoring and Diagnosis: \nProof of Concept for Skin Oxygenation Imaging,” J. Imaging, vol. 5, \nno. 8, p. 66, Jul. 2019, doi: 10.3390/jimaging5080066. \n[3] \nF. Xiong, J. Zhou, and Y. Qian, “Material Based Object Tracking in \nHyperspectral Videos,” IEEE Transactions on Image Processing, vol. \n29, pp. 3719–3733, 2020, doi: 10.1109/TIP.2020.2965302. \n[4] \nL. Chen et al., “Object tracking in hyperspectral-oriented video with fast \nspatial-spectral features,” Remote Sensing, vol. 13, no. 10, p. 1922, \n2021. \n[5] \nL. Chen, Y. Zhao, J. C.-W. Chan, and S. G. Kong, “Histograms of \noriented mosaic gradients for snapshot spectral image description,” \nISPRS Journal of Photogrammetry and Remote Sensing, vol. 183, pp. \n79–93, Jan. 2022, doi: 10.1016/j.isprsjprs.2021.10.018. \n[6] \nJ. Jia, K. J. Barnard, and K. Hirakawa, “Fourier Spectral Filter Array for \nOptimal Multispectral Imaging,” IEEE Transactions on Image \nProcessing, vol. 25, no. 4, pp. 1530–1543, Apr. 2016, doi: \n10.1109/TIP.2016.2523683. \n[7] \nY. Monno, S. Kikuchi, M. Tanaka, and M. Okutomi, “A Practical One-\nShot Multispectral Imaging System Using a Single Image Sensor,” \nIEEE Transactions on Image Processing, vol. 24, no. 10, pp. 3048–\n3059, Oct. 2015, doi: 10.1109/TIP.2015.2436342. \n[8] \nT. A. Habtegebrial, G. Reis, and D. Stricker, “Deep Convolutional \nNetworks For Snapshot Hypercpectral Demosaicking,” in 2019 10th \nWorkshop on Hyperspectral Imaging and Signal Processing: Evolution \nin Remote Sensing (WHISPERS), Sep. 2019, pp. 1–5. doi: \n10.1109/WHISPERS.2019.8921273. \n[9] \nK. Feng, Y. Zhao, J. C.-W. Chan, S. G. Kong, X. Zhang, and B. Wang, \n“Mosaic Convolution-Attention Network for Demosaicing Multispectral \nFilter Array Images,” IEEE Transactions on Computational Imaging, \nvol. 7, pp. 864–878, 2021, doi: 10.1109/TCI.2021.3102052. \n[10] K. Shinoda, S. Yoshiba, and M. Hasegawa, “Deep demosaicking for \nmultispectral filter arrays,” arXiv:1808.08021 [eess], Oct. 2018, \n \nFig. 17. The local details of real-world raw mosaic and pseudo color demosaicing results of the supervised methods and our \nmethod for two real scenes from Mosaic25. \n12 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nAccessed: Feb. 18, 2021. [Online]. Available: \nhttp://arxiv.org/abs/1808.08021 \n[11] M. Gupta, V. Rathi, and P. Goyal, “Adaptive and Progressive \nMultispectral Image Demosaicking,” IEEE Transactions on \nComputational Imaging, vol. 8, pp. 69–80, 2022. \n[12] J. Mizutani, S. Ogawa, K. Shinoda, M. Hasegawa, and S. Kato, \n“Multispectral demosaicking algorithm based on inter-channel \ncorrelation,” in 2014 IEEE Visual Communications and Image \nProcessing Conference, Dec. 2014, pp. 474–477. doi: \n10.1109/VCIP.2014.7051609. \n[13] S. Mihoubi, O. Losson, B. Mathon, and L. Macaire, “Multispectral \nDemosaicing Using Pseudo-Panchromatic Image,” IEEE Trans. \nComput. Imaging, vol. 3, no. 4, pp. 982–995, Dec. 2017, doi: \n10.1109/TCI.2017.2691553. \n[14] S. Ogawa et al., “Demosaicking Method for Multispectral Images Based \non Spatial Gradient and Inter-channel Correlation,” in Image and Signal \nProcessing, Cham, 2016, pp. 157–166. doi: 10.1007/978-3-319-33618-\n3_17. \n[15] V. Rathi and P. Goyal, “Generic Multispectral Image Demosaicking \nAlgorithm and New Performance Evaluation Metric,” in Computer \nVision and Image Processing, Cham, 2022, pp. 45–57. doi: \n10.1007/978-3-031-11346-8_5. \n[16] J. Brauers and T. Aach, “A color filter array based multispectral \ncamera,” in 12. Workshop Farbbildverarbeitung, 2006. \n[17] G. Tsagkatakis, M. Bloemen, B. Geelen, M. Jayapala, and P. \nTsakalides, “Graph and Rank Regularized Matrix Recovery for \nSnapshot Spectral Image Demosaicing,” IEEE Trans. Comput. Imaging, \nvol. 5, no. 2, pp. 301–316, Jun. 2019, doi: 10.1109/TCI.2018.2888989. \n[18] L. Bian, Y. Wang, and J. Zhang, “Generalized MSFA Engineering With \nStructural and Adaptive Nonlocal Demosaicing,” IEEE Transactions on \nImage Processing, vol. 30, pp. 7867–7877, 2021, doi: \n10.1109/TIP.2021.3108913. \n[19] M. Kawase, K. Shinoda, and M. Hasegawa, “Demosaicking Using a \nSpatial Reference Image for an Anti-Aliasing Multispectral Filter \nArray,” IEEE Trans. on Image Process., vol. 28, no. 10, pp. 4984–4996, \nOct. 2019, doi: 10.1109/TIP.2019.2910392. \n[20] K. Dijkstra, J. van de Loosdrecht, L. R. B. Schomaker, and M. A. \nWiering, “Hyperspectral demosaicking and crosstalk correction using \ndeep learning,” Machine Vision and Applications, vol. 30, no. 1, pp. 1–\n21, Feb. 2019, doi: 10.1007/s00138-018-0965-4. \n[21] B. Arad et al., “NTIRE 2022 Spectral Demosaicing Challenge and Data \nSet,” in Proceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition, 2022, pp. 882–896. \n[22] S. Liu, Y. Zhang, J. Chen, L. K. Pang, and S. Rahardja, “A Deep Joint \nNetwork for Multispectral Demosaicking Based on Pseudo-\nPanchromatic Images,” IEEE Journal of Selected Topics in Signal \nProcessing, pp. 1–1, 2022, doi: 10.1109/JSTSP.2022.3172865. \n[23] P. Li et al., “Deep learning approach for hyperspectral image \ndemosaicking, spectral correction and high-resolution RGB \nreconstruction,” Computer Methods in Biomechanics and Biomedical \nEngineering: Imaging & Visualization, vol. 10, no. 4, pp. 409–417, Jul. \n2022, doi: 10.1080/21681163.2021.1997646. \n[24] S. P. Jaiswal, L. Fang, V. Jakhetiya, J. Pang, K. Mueller, and O. C. Au, \n“Adaptive Multispectral Demosaicking Based on Frequency-Domain \nAnalysis of Spectral Correlation,” IEEE Transactions on Image \nProcessing, vol. 26, no. 2, pp. 953–968, Feb. 2017, doi: \n10.1109/TIP.2016.2634120. \n[25] S. Mihoubi, “Snapshot multispectral image demosaicing and \nclassification,” Theses, Université de Lille, 2018. Accessed: Oct. 16, \n2022. [Online]. Available: https://hal.archives-ouvertes.fr/tel-01953493 \n[26] D. Chen, J. Tachella, and M. E. Davies, “Equivariant Imaging: Learning \nBeyond the Range Space,” presented at the Proceedings of the \nIEEE/CVF International Conference on Computer Vision, 2021, pp. \n4379–4388. Accessed: Nov. 10, 2022. [Online]. Available: \nhttps://openaccess.thecvf.com/content/ICCV2021/html/Chen_Equivaria\nnt_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.h\ntml \n[27] Y. Niu, J. Ouyang, W. Zuo, and F. Wang, “Low Cost Edge Sensing for \nHigh Quality Demosaicking,” IEEE Transactions on Image Processing, \nvol. 28, no. 5, pp. 2415–2427, May 2019, doi: \n10.1109/TIP.2018.2883815. \n[28] F. Iandola and K. Keutzer, “Keynote: small neural nets are beautiful: \nenabling embedded systems with small deep-neural- network \narchitectures,” in 2017 International Conference on Hardware/Software \nCodesign and System Synthesis (CODES+ISSS), 2017, pp. 1–10. doi: \n10.1145/3125502.3125606. \n[29] G. Qian, J. Gu, J. S. Ren, C. Dong, F. Zhao, and J. Lin, “Trinity of Pixel \nEnhancement: a Joint Solution for Demosaicking, Denoising and Super-\nResolution,” arXiv:1905.02538 [cs, eess], May 2019, Accessed: Dec. \n25, 2020. [Online]. Available: http://arxiv.org/abs/1905.02538 \n[30] E. L. Wisotzky, C. Daudkane, A. Hilsmann, and P. Eisert, \n“Hyperspectral Demosaicing of Snapshot Camera Images Using Deep \nLearning,” in Pattern Recognition, Cham, 2022, pp. 198–212. doi: \n10.1007/978-3-031-16788-1_13. \n[31] A. Bruhn, J. Weickert, and C. Schnörr, “Lucas/Kanade Meets \nHorn/Schunck: Combining Local and Global Optic Flow Methods,” \nInternational Journal of Computer Vision, vol. 61, no. 3, pp. 211–231, \nFeb. 2005, doi: 10.1023/B:VISI.0000045324.43199.43. \n[32] W. Shi et al., “Real-time single image and video super-resolution using \nan efficient sub-pixel convolutional neural network,” in Proceedings of \nthe IEEE conference on computer vision and pattern recognition, 2016, \npp. 1874–1883. \n[33] J. Hu, L. Shen, and G. Sun, “Squeeze-and-Excitation Networks,” p. 10. \n[34] B. Arad and O. Ben-Shahar, “Sparse Recovery of Hyperspectral Signal \nfrom Natural RGB Images,” in Computer Vision – ECCV 2016, Cham, \n2016, pp. 19–34. doi: 10.1007/978-3-319-46478-7_2. \n[35] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image \nquality assessment: from error visibility to structural similarity,” IEEE \nTransactions on Image Processing, vol. 13, no. 4, pp. 600–612, Apr. \n2004, doi: 10.1109/TIP.2003.819861. \n[36] F. A. Kruse et al., “The spectral image processing system (SIPS)—\ninteractive visualization and analysis of imaging spectrometer data,” \nRemote Sensing of Environment, vol. 44, no. 2, pp. 145–163, May 1993, \ndoi: 10.1016/0034-4257(93)90013-N. \n[37] L. Wald, “Quality of high resolution synthesised images: Is there a \nsimple criterion ?,” in Third conference ”Fusion of Earth data: merging \npoint measurements, raster maps and remotely sensed images”, Sophia \nAntipolis, France, 2000, pp. 99–103. Accessed: Aug. 23, 2022. \n[Online]. Available: https://hal.archives-ouvertes.fr/hal-00395027 \n[38] J. Yang, Y.-Q. Zhao, C. Yi, and J. C.-W. Chan, “No-reference \nhyperspectral image quality assessment via quality-sensitive features \nlearning,” Remote Sensing, vol. 9, no. 4, p. 305, 2017. \n[39] D. Chen, J. Tachella, and M. E. Davies, “Robust Equivariant Imaging: a \nfully unsupervised framework for learning to image from noisy and \npartial measurements,” arXiv:2111.12855 [cs, eess], Nov. 2021, \nAccessed: Dec. 13, 2021. [Online]. Available: \nhttp://arxiv.org/abs/2111.12855 \n[40] S. Guo, Z. Liang, and L. Zhang, “Joint Denoising and Demosaicking \nWith Green Channel Prior for Real-World Burst Images,” IEEE \nTransactions on Image Processing, vol. 30, pp. 6930–6942, 2021, doi: \n10.1109/TIP.2021.3100312. \n[41] N. Li, B. L. Teurnier, M. Boffety, F. Goudail, Y. Zhao, and Q. Pan, \n“No-Reference Physics-Based Quality Assessment of Polarization \nImages and Its Application to Demosaicking,” IEEE Transactions on \nImage Processing, vol. 30, pp. 8983–8998, 2021, doi: \n10.1109/TIP.2021.3122085. \n[42] B. Wang, C. Lu, D. Yan, and Y. Zhao, “Learning Pixel-Adaptive \nWeights for Portrait Photo Retouching.” arXiv, Dec. 07, 2021. doi: \n10.48550/arXiv.2112.03536. \n[43] B. Wang, Y. Zhao, and X. Li, “Multiple Instance Graph Learning for \nWeakly Supervised Remote Sensing Object Detection,” IEEE \nTransactions on Geoscience and Remote Sensing, vol. 60, pp. 1–12, \n2022, doi: 10.1109/TGRS.2021.3123231. \n[44] W. Zhou, L. Zhang, S. Gao, and X. Lou, “Gradient-Based Feature \nExtraction From Raw Bayer Pattern Images,” IEEE Transactions on \nImage Processing, vol. 30, pp. 5122–5137, 2021, doi: \n10.1109/TIP.2021.3067166. \n \n",
  "categories": [
    "eess.IV",
    "cs.CV"
  ],
  "published": "2023-07-05",
  "updated": "2023-07-05"
}