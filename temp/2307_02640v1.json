{
  "id": "http://arxiv.org/abs/2307.02640v1",
  "title": "Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts",
  "authors": [
    "Alexandrea K. Ramnarine"
  ],
  "abstract": "The massive collection of user posts across social media platforms is\nprimarily untapped for artificial intelligence (AI) use cases based on the\nsheer volume and velocity of textual data. Natural language processing (NLP) is\na subfield of AI that leverages bodies of documents, known as corpora, to train\ncomputers in human-like language understanding. Using a word ranking method,\nterm frequency-inverse document frequency (TF-IDF), to create features across\ndocuments, it is possible to perform unsupervised analytics, machine learning\n(ML) that can group the documents without a human manually labeling the data.\nFor large datasets with thousands of features, t-distributed stochastic\nneighbor embedding (t-SNE), k-means clustering and Latent Dirichlet allocation\n(LDA) are employed to learn top words and generate topics for a Reddit and\nTwitter combined corpus. Using extremely simple deep learning models, this\nstudy demonstrates that the applied results of unsupervised analysis allow a\ncomputer to predict either negative, positive, or neutral user sentiment\ntowards plastic surgery based on a tweet or subreddit post with almost 90%\naccuracy. Furthermore, the model is capable of achieving higher accuracy on the\nunsupervised sentiment task than on a rudimentary supervised document\nclassification task. Therefore, unsupervised learning may be considered a\nviable option in labeling social media documents for NLP tasks.",
  "text": "UNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY\nSOCIAL MEDIA POSTS\nAlexandrea K. Ramnarine\nSchool of Professional Studies\nNorthwestern University\nChicago, IL 60611\nalexandrearamnarine2021@u.northwestern.edu\nJuly 7, 2023\nABSTRACT\nThe massive collection of user posts across social media platforms is primarily untapped for artificial\nintelligence (AI) use cases based on the sheer volume and velocity of textual data. Natural language\nprocessing (NLP) is a subfield of AI that leverages bodies of documents, known as corpora, to train\ncomputers in human-like language understanding. Using a word ranking method, term frequency-\ninverse document frequency (TF-IDF), to create features across documents, it is possible to perform\nunsupervised analytics, machine learning (ML) that can group the documents without a human\nmanually labeling the data. For large datasets with thousands of features, t-distributed stochastic\nneighbor embedding (t-SNE), k-means clustering and Latent Dirichlet allocation (LDA) are employed\nto learn top words and generate topics for a Reddit and Twitter combined corpus. Using extremely\nsimple deep learning models, this study demonstrates that the applied results of unsupervised analysis\nallow a computer to predict either negative, positive, or neutral user sentiment towards plastic surgery\nbased on a tweet or subreddit post with almost 90% accuracy. Furthermore, the model is capable\nof achieving higher accuracy on the unsupervised sentiment task than on a rudimentary supervised\ndocument classification task. Therefore, unsupervised learning may be considered a viable option in\nlabeling social media documents for NLP tasks.\nKeywords Natural language processing · unsupervised analysis · social media · Twitter · Reddit · plastic surgery\n1\nIntroduction\nCosmetic plastic surgery is an expensive yet increasingly popular set of procedures for both men and women, especially\nconsidering the ease of accessibility to surgeons, patient testimonials, and procedure-specific information, such as\n“before-and-after” visual aids, afforded by the Internet. The Internet is virtually a bottomless trove of textual data that\nare easily created at velocities of millions of posts per second across social media platforms. The exponential adoption\nof social media, largely facilitated by the wide global distribution of smart phone technology, is a known disseminator\nof beauty standards that is highly targeted to billions of users daily. Cosmetic surgery becomes a quick, permanent fix in\nadopting sought after beauty standards set by celebrities, models and social media “influencers”, relative to longer term\nalternatives such as dieting and exercise, or temporary alternatives such as adoption of fashion and cosmetic trends.\nSocial media, while distributing information about plastic surgery procedures, also provides a setting for public social\ncommentary on the election of undergoing these surgeries. Users across many platforms are able to freely communicate\ntheir sentiments on a broad scale and even as granular as commenting on another individual’s posted surgical outcomes.\nDifferent social media platforms exist for different purposes, and thus attract and form distinct user bases that comprise\nthese Internet communities. Each text post is unique to a user, time-stamped, geo- located, and has the capability to\npossess multiple data formats including but not limited to links and images. Therefore, text posts from social media\nsites provide specific insight into the public’s opinion on cosmetic surgery. It is thus reasonable to assume that the text\nposts made on one platform can be used to distinguish user-derived text from other platforms.\narXiv:2307.02640v1  [cs.CL]  5 Jul 2023\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\nCurating massive corpora of text post documents from popular social media networks, Twitter and Reddit, is feasible\nwith the implementation of AI web scraping technology. NLP is then leveraged to process and mathematically transform\ntext to computationally understandable representations. ML methods can then identify patterns among the corpora that\nis an otherwise impossible task for a human to accomplish given the sheer volume of data. Deep learning (DL) methods,\nrelying on powerful and speedy neural network technology, are then poised to use the NLP-curated and ML-processed\ndata in order to accurately predict document class and user sentiment across the corpora. This study demonstrates that\nvery simple, regularized neural network architectures effectively use unsupervised NLP to answer an easy to ask yet\ndifficult to answer question, “how does the Internet feel about plastic surgery?”\n2\nLiterature Review\nOpinion mining, better known as sentiment analysis, is an application of NLP that is particularly suited to extracting\nand assessing human opinion from textual data over the Internet and social media networks [1]. While spoken language\noffers context surrounding feelings and opinions through auditory cues such as tone and pitch, written language often\nbroadly captures polarity in discussions, which can be leveraged by AI. Trained AI are able to detect polarity, whether\nnegative, positive, or neutral, based on word associations captured mathematically by distance metrics. Distance is able\nto represent and capture context, giving connotative rather than denotative meaning to the words that ultimately decide\nwhether a word is positive or negative [2]. Therefore, ranking word importance to use as term features for AI is critical\nto achieve high accuracy for sentiment analysis, particularly unsupervised sentiment assignment. This study adopts the\ninformation retrieval ranking function of TF-IDF, combining two methods by [3] and [4] to assign weights to important\nterms across a corpus of documents.\nTwo popular unsupervised analyses are utilized in this study to support analyst judgment for assigning sentiment to\nsocial media posts that lack these labels. [5] and [6] proposed the “k-means” method as an unsupervised algorithm to\ngroup observations based on minimizing variance, or the sum of squared deviations of points, to generate clusters of\npoints using centroided means. [7] formulated LDA, which uses Bayesian probabilistic modeling to generate topic\nrepresentations of NLP corpora based on their documents and associated terms. LDA therefore will ultimately support\nclustering analysis in generating labels for subsequent sentiment analysis.\nMore recently, [8] applied LDA to extract features of YouTube comments, proposing that semantic topic extraction\ncan directly aid in sentiment scoring of comments as “negative” or “positive” through an NLP-hybrid framework when\napplied to fuzzy lattice reasoning. In conjunction with unsupervised analysis, this application is useful in identifying\nuser groups within social media networks. [9] created an unsupervised approach to determine interests of social media\nusers based on tweet semantics. A ML survey of unsupervised learning applications to NLP specifically highlights\nclustering algorithms such as k-means to address categorical outputs when labeled training data is not available for\nanalytics.\n3\nMethods\n3.1\nData Acquisition\nA Python 3.8 development version of snscrape package was utilized to run command line bash scripts that scrape\ntop and relevant social media posts from chosen Reddit subreddits and Twitter hashtags through March 2021, which\nserve as the document categories. Reddit queries from three different subreddits, “PlasticSurgery”, “CosmeticSurgery”,\nand “BotchedSurgeries”, had a maximum of 8000 or 4000 result scrapes of the latest posts based on total reported\nposts on each subreddit’s webpage. Twitter queries for each of the following hashtags, “plasticsurgery”, “liposuction”,\n“lipinjections”, “botox”, and “nosejob”, had a maximum of 8000 result scrapes of top tweets as determined by Twitter’s\ninternal algorithm. Each scrape was saved as a JSON line file and subsequently read into respective Pandas dataframes.\nNull data were replaced with empty strings using NumPy. All Reddit and Twitter dataframes were concatenated\nrespectively.\n3.2\nData Pre-processing\nThe separate corpora dataframes were joined based on identification number, category, text, and title columns. Pre-\nprocessing steps for the combined corpus utilized a Python implementation of NLTK and custom functions to convert\ntext to lowercase, remove punctuation, remove emojis and other Unicode characters, tokenize each document, remove\nEnglish stop words, and stem each token. TF-IDF vectorization of the combined corpus employed additional pre-\nprocessing to drop duplicates and strip Unicode characters.\n2\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\n3.3\nUnsupervised Analysis\nThe Scikit-learn TF-IDF Vectorizer was set to generate unigrams and subsequently fit to the combined corpus after\nrandomly shuffling the dataframe. Scipy and Scikit-learn implementations of k-means using k of 8, 3, and 2, and a\n2-component t-SNE multidimensionality rescale using cosine distance and perplexity of 50.0 were applied to the TF-\nIDF matrix. Each method underwent at least ten initializations and between 300 and 1000 iterations before convergence.\nThe Scikit-learn implementation of LDA was used for topic modeling on the TF-IDF matrix, generating the top 20\nwords for 8, 3, and 2 topics. All visualizations were generated using MatPlotLib and Seaborn.\n3.4\nDeep Learning\nThe high-level Keras API on TensorFlow was utilized to build a Sequential dense neural network (DNN) with one input\nlayer using rectified linear unit (ReLU) activation, one dropout regularization layer, and one output layer using softmax\nactivation for both document category classification and sentiment analysis tasks. For sentiment analysis tasks, 1-D\ntemporal convolutional (1D-CNN) Sequential models were built with an input layer of 32 units and a 3x3 kernel, ReLU\nactivation and He uniform initialization, a 2x2 1-D max pooling layer, followed by a dropout and flatten layer feeding\nto a dense layer with 128 units before the final dense output layer. Each model was compiled using the Adam optimizer\nand a categorical cross-entropy loss function. After Scikit-learn 80% train, 20% test splitting of the TF-IDF matrix and\nlabels, the models were fit to shuffled data and trained over 15 epochs with an internal training validation split of 20\nFor classification labels, each of the eight document categories were represented as an integer. For sentiment labels,\nanalyst judgment of both the k-means clusters mapped into the t- SNE space and LDA topics was used to create three\nclasses corresponding to negative, positive, or neutral sentiment. All labels were converted to categorical vectors using\nTensorFlow.\nTraining and validation loss and accuracy were tracked over each epoch before evaluating each model on the test sets.\nScikit-learn classification reports of precision and recall, and confusion matrices were generated for each test instance.\n4\nResults\nAfter vectorization of the combined corpus, unsupervised analyses were performed to visualize the distribution of\ndocument categories. Figure 1 illustrates the most similar documents towards the middle of the t-SNE space, while\noutlier documents sparsely separate at the fringe of the main cluster.\nDocuments from Reddit are more similar to each other than documents from Twitter, primarily falling in the mid to\ntop right quadrant of the space while the Twitter documents cluster together along the bottom and left of the distribution.\nDocuments related generally to plastic surgery, sourced from the plastic surgery Twitter hashtag or the Plastic Surgery\nand Cosmetic Surgery subreddits, are the most similar to each other and fall within the middle of the distribution. About\nhalf of the Botched Surgery subreddit documents strongly form a smaller cluster away from the rest, however the second\nhalf is well interspersed within the other subreddits toward the center of the distribution.\nThe liposuction and lip injection documents are most dissimilar from each other of the Twitter-sourced hashtags,\nwhile the nose job hashtag is similar to the liposuction and half of the Botox hashtag. The other half of the Botox\nhashtag is more similar to the lip injection hashtag source. This half of the Botox hashtag, along with the lip injection\nhashtag, are more dissimilar than the general plastic surgery hashtag than the nose job and liposuction hashtags.\nThe outlier documents are distributed in smaller yet distinguishable groups. There are three nose job Twitter hashtag\noutlier groups where two are somewhat related to the larger group, but one is more related to the main Botox hashtag\ngroup. There are many liposuction tweets that are more closely related to the Reddit documents than to Twitter\ndocuments. There is one Botched Surgery subreddit outlier group that is more related to the liposuction tweets. Finally,\nthere are two strongly separated groups of mixed documents, however primarily comprised of Plastic Surgery and\nBotched Surgery subreddit documents. One of these falls within the main distribution but largely distanced in its entire\ncircumference from the general plastic surgery tweets and subreddit posts, and the second falls completely out of the\nmain distribution towards the most negative t-SNE 1 space, closer to a polarized outlier group of the twitter lip injection\nhashtag.\nCentroid clustering was applied to the t-SNE space using a k-means approach. The eight generated clusters highlight\nthe strongest outlier document groups, the differences among the twitter hashtags, and the similarities among both the\nReddit documents and the general plastic surgery documents, as illustrated in Figure 2.\nCluster 3 is a “catch all”, predominantly mapping to the Reddit documents and general plastic surgery related\ndocuments. Cluster 0 mapped directly to the Botched Surgery outlier group, cluster 1 to the Botox hashtag, cluster 7 to\n3\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\nFigure 1: t-SNE Dimensionality Reduction Mapped by Category\nthe nose job hashtag, and cluster 5 to the liposuction hashtag. Cluster 2 seemed to map to documents that fell directly\ncentral within the Botox and lip injection document space but were not sourced from either of those hashtags. Clusters\n3 and 6 highlight strong outlier groups in the t-SNE space, where the former maps to the outliers of the general plastic\nsurgery documents, and the latter maps to the fringe outliers of the liposuction hashtag tweets.\nIn order to stratify the documents into three balanced groups corresponding to positive, negative, or neutral sentiment,\nk-means clustering was performed again on the TF-IDF matrix using k equal to 3 and 2. Appendix A demonstrates\nthat the most significant differences between the documents using centroid clustering is between the main distribution\nand the large Botched Surgery outlier group. Therefore, LDA was employed to further support analyst judgement in\nunsupervised sentiment labeling. Figure 3 depicts the results of the top 20 terms for eight topics across the combined\ncorpus, corresponding to the eight document categories.\nTable 1: Selected k-means Top Terms\nCluster 0\nCluster 1\nCluster 2\nCluster 3\nCluster 4\nCluster 5\nCluster 6\nCluster 7\nremove\nbotch\nexcess skin\nevil\nevildick\nevilqueen\nexaggerate\nexboyfriend\ninject\nskincare\nfiller\ntreatment\nfacial\nantiaging\nbotox\nwrinkle\nfabulous\ngratitude\nmotivate\nfit\nwell\ncomfort\nlip goal\nlove\nthank\ngorgeous\nhappiness\namaze\nwonder\nplease\npretty\ncandylipz\ndelete\nban\nswollen\nswell\nevil\nexcess\nexboyfriend\ndidnt\nremove\nattack\nstubborn\nimprove\nproblem\neliminate\ndouble-chin\n(contains\nnumerical\nterms, like\ninjection\ndoses\nand\nphone\nnumbers)\nsurgeon\nsurgery\nbeauty\nmedic\ncosmetic\nrhinoplasty\nprocedure\npatient\nWhile the majority of the top terms can be considered neutral, there are a few that can be mapped back to the k-means\ntop term results, shown in Table 1, in order to assign sentiment labels to each k-means cluster. From Topic 4, “delete”\nand “improve”, and from Topic 8, “remove”, “addict”, and “stubborn”, are key terms indicative of negative sentiment if\nalso highlighted by k-means. Reducing the LDA to three or two topics still captures these negative connotation terms.\nTherefore, k-means clusters 0, 4, and 5 were assigned a negative sentiment label, clusters 1, 6, and 7 were assigned a\n4\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\nFigure 2: k-means Clustering Mapped to t-SNE Space\nneutral sentiment label, and clusters 2 and 3 were assigned a positive sentiment label. To correct for class imbalance,\nany Botched Surgery subreddit documents not assigned to negative sentiment were reassigned a negative label based on\nthat particular subreddit’s culture of mocking and shaming plastic surgery procedure outcomes subjectively deemed\npoor.\n4.1\nPredicting on Supervised versus Unsupervised Labels\nA very simple one-layer DNN architecture, utilizing 30% dropout regularization, was used to test supervised document\ncategory classification versus the unsupervised sentiment analysis. Appendix C illustrates that training accuracy\nincreases with epochs; however, validation accuracy stagnates. Training loss decreases with epochs, but validation loss\nincreases in both classification and sentiment analysis cases. Table 2 compares the performances between classification\nand sentiment analysis tasks on the combined corpus.\nTable 2: Dense Neural Network Performance\nTraining\nValidation\nTest\nTask\nAccuracy\nLoss\nAccuracy\nLoss\nAccuracy\nLoss\nClassification\n93.13\n0.1767\n78.16\n0.7555\n77.78\n0.7859\nSentiment\n97.77\n0.0473\n87.84\n0.4652\n87.12\n0.4768\nNote: Accuracy shown in percent.\nOverall, the model was able to achieve better performance on unsupervised sentiment analysis versus supervised\ndocument classification. Appendices D and E compare the harmonic mean and confusion matrices of the two learning\ntasks. For classification, there was a class imbalance for the lip injections Twitter hashtag, therefore the F1-score was\nlow and misclassification rate was high relative to the performance on the other labels. The model performed best on\ncorrectly classifying the nose job Twitter documents.\nFor sentiment analysis, although there was class imbalance skewed towards over-representation of the positive-labeled\ndocuments, this had no noticeable effect on model performance. The model performed best on predicting neutral\n5\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\nFigure 3: Top 20 Terms by LDA Topic Modeling\nsentiment and relatively worse on predicting negative sentiment, however the precision and recall metrics between the\nthree sentiments are similar. Almost all of the neutral documents were correctly predicted as such, and less than 20\n4.2\nSentiment Analysis\nGiven that a simple DNN could achieve near 90% accuracy on unsupervised sentiment analysis, experiments varying\ndropout regularization and use of a temporal convolutional neural network were conducted. Table 3 summarizes the\ntraining, validation, and test results of these experiments to predict sentiment.\nTable 3: Model Regularization and Architecture Experiments for Sentiment Prediction\nTraining\nValidation\nTest\nModel\nDropout\nAccuracy\nLoss\nAccuracy\nLoss\nAccuracy\nLoss\nDNN\n0\n97.33\n0.0567\n86.98\n0.5057\n86.27\n0.5227\nDNN\n0.3\n97.77\n0.0473\n87.84\n0.4652\n87.12\n0.4768\nDNN\n0.6\n96.41\n0.0830\n88.26\n0.3627\n87.53\n0.3729\n1D-CNN\n0\n98.64\n0.028\n87.59\n0.4912\n87.08\n0.5292\n1D-CNN\n0.3\n98.61\n0.0311\n87.12\n0.5203\n86.92\n0.5604\n1D-CNN\n0.6\n98.40\n0.0388\n88.80\n0.5048\n88.21\n0.5552\nNote: Dropout rate shown, accuracy shown in percent.\nIncreasing dropout rate in both model cases increases both validation and test accuracies overall. However, increasing\ndropout rate for the 1D-CNN does not improve validation or test loss compared to using no dropout regularization,\nand instead caused the validation loss to behave erratically over training epochs (Appendix G). Using dropout had no\nsubstantial effect on validation accuracy over epochs of the 1D-CNN. Appendix F illustrates that using high dropout\nrates for the DNN shrinks the gap between training and validation metrics at each epoch, notably shrinking validation\nloss despite the similar upwards trending loss over epochs in both zero and 60% dropout cases.\nAppendix H displays the test results of the sentiment analysis comparing dropout regularization between the two\nmodels. The 1D-CNN using 60% had the highest true classification rate of positive sentiment, while the 1D-CNN using\n30% dropout had the lowest. The DNN using 60% dropout had the best classification rate of negative sentiment, while\nthe DNN using no dropout performed relatively poorly on correctly classifying negative sentiment. The 1D-CNN using\n30% dropout correctly predicted neutral sentiment at the highest rate, and the DNN using no dropout correctly predicted\n6\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\nneutral sentiment at the lowest relative rate among the models. All models displayed almost negligible misclassification\nrates bidirectionally between negative and neutral sentiment.\n7\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\n5\nDiscussion\nWhile it is reasonable to assume that virtual communities formed over social media forums tend to attract like-minded\nopinions, this over-generalization may conflate the outlier user posts within each group. Herein, it is demonstrated that\napplying unsupervised dimensional reductions and clustering algorithms to an extremely large and heterogenous corpus\nof Twitter and Reddit text data is a viable option to capture user sentiment based on word rankings. In conjunction with\ntopic modeling, these methods may be employed to label noisy text data in a semi-supervised manner.\nThe Twitter and Reddit documents mostly separate in the t-SNE space, suggesting that the types of posts, and\ntherefore the user bases, can distinguish between the two social media networks. The relative homogeneity of the\nReddit to Twitter distribution supports the idea that Reddit posts, and therefore users, are somewhat more similar to\neach other that Twitter users. This is likely a function of subreddits being niche internet communities with users sharing\nmultiple posts within the same subreddit, and perhaps even between the three sampled plastic surgery subreddits since\nthere are no other major related communities that were found on Reddit pertaining to plastic or cosmetic surgery. It is a\nfair assumption that Twitter has a more heterogenous representation because hashtags do not act as niche communities\nthe way subreddits are structured to.\nGiven that the sourced tweets are stratified mainly by procedure related hashtags, it is unsurprising that non-invasive,\ninjection-based procedures cluster closely together, such as the lip injection and Botox clusters, while the invasive\nprocedures such as nose job and liposuction cluster together. k-means cluster 1 therefore must be representative of\nnon-invasive or injection- based procedures. That nose job and Botox documents are still relatively close in distance in\nthe t-SNE space indicates relatedness due to terms associated with facial procedures. Interestingly, these four hashtags\nfor multiple smaller outlier clusters in the t-SNE space, probably indicative of underlying sentiment distributions given\nthe k-means mapped analysis and strong predictive power of the neutral networks. Despite the biased sourcing used for\nthese tweets, the general plastic surgery Twitter hashtag documents almost uniformly span all of the procedure-specific\nTwitter documents in the t-SNE space.\nSurprisingly, the unsupervised generated labels seemingly allowed the simplest of neural networks to outperform a\nsupervised NLP task, which may suggest that the content of plastic surgery related documents sourced from Twitter\nand Reddit are better captured by analyst judgment sentiment and not by the empirical document source. This further\nsuggests that the term ranking methodology employed, together with topic modeling, generated strong indicators of\nsocial media user opinion, effectively grouping words based on cosine distance.\nUsing a temporal convolutional network, a model theoretically better suited to capturing high and low dimensionalities\nof sequential text data, showed negligible improvement over the DNN in terms of accuracy, loss, and sentiment true\nclassification rate. In general, both neural networks overfit the training data, averaging about 10% differences in\naccuracy between training and test instances. Training and validation instances indicate that both models would benefit\nfrom early stopping well before 10 epochs in order to achieve higher validation accuracy and lower validation loss; it is\nassumed that the test metrics would follow in suit.\nAll models had comparably high precision and recall for both neutral and positive sentiment, although the F1-scores\nfor negative sentiment prediction were not dramatically lower. Given absolute true classifications, the models overall\nwere able to distinguish negative from neutral sentiment very well. For each model, most of the misclassifications\noccurred between positive and negative sentiment, followed by positive and neutral sentiment. The top-ranking terms\ntherefore must strongly segregate neutral from other sentiment in the case of plastic surgery, indicative of volume of\nterms used and associated with the medical procedures rather than with user opinions of those procedures or their\noutcomes. That the models struggled most with misclassifications between positive and negative sentiment could be\nindicative of vernacular and colloquial usage of terms mixed with denotative usage, confounding learning and thus\nimpairing the decision boundary between these two sentiments. Additionally, it may be useful to use n-grams rather\nthan unigrams to better define terms, such as “beauty” and “change”, that could realistically fall into any sentiment\ncategory for plastic surgery depending on the context it is used in.\nThe high predictive capacities of these simple models indicate that favored NLP recurrent neural networks (RNNs),\nincluding gated recurrent unit networks and long short-term memory networks, may not perform that much better\nfor sentiment analysis of these social media sourced documents, given the abbreviated length of each document and\nthe frequently associated vanishing gradient problems with RNNs. While it may be interesting to pursue future work\nwith different model architectures, the results from the temporal convolutional network, considered an advancement to\nsimple RNNs given its ability to capture spatio-temporal information, indicate that it may be better to invest efforts in\ncuration, vectorization and thus representation of top terms, using fewer terms but a more polarizing vocabulary to\nmodel the data after. Additionally, it may be fruitful to capture a wider breadth of hashtags from Twitter, more posts\nfrom the subreddits, and even venture to other social media networks for relevant plastic surgery documents to expand\nthe user base, and thus opinion, representation.\n8\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS - JULY 7, 2023\nReferences\n[1] Rahul Kumar Singh, Manoj Kumar Sachan, and R. B. Patel. 360 degree view of cross-domain opinion classification:\na survey. Artificial Intelligence Review, 54(2):1385–1506, August 2020.\n[2] Thomas W. Miller. Web and network data science: Modeling techniques in predictive analytics, page 119–170.\nPearson Education, 2015.\n[3] H. P. Luhn. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of\nResearch and Development, 1(4):309–317, October 1957.\n[4] Karen Spärck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of\nDocumentation, 60(5):493–502, 2004.\n[5] E. Forgy. Cluster analysis of multivariate data: Efficiency versus interpretability of classification. Biometrics,\n21(3):768–769, 1965.\n[6] James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings\nof the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281–297. Oakland, CA,\nUSA, 1967.\n[7] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022,\n2003.\n[8] Hamed Jelodar, Yongli Wang, Mahdi Rabbani, Sajjad Bagheri Baba Ahmadi, Lynda Boukela, Ruxin Zhao, and\nRaja Sohail Ahmed Larik. A NLP framework based on meaningful latent-topic detection and sentiment analysis via\nfuzzy lattice reasoning on youtube comments. Multimedia Tools and Applications, 80(3):4155–4181, September\n2020.\n[9] Nacéra Bennacer Seghouani, Coriane Nana Jipmo, and Gianluca Quercini. Determining the interests of social\nmedia users: two approaches. Information Retrieval Journal, 22(1-2):129–158, July 2018.\n9\nSupplemental Materials: UNSUPERVISED SENTIMENT\nANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS\nAlexandrea K. Ramnarine\nSchool of Professional Studies\nNorthwestern University\nChicago, IL 60611\nalexandrearamnarine2021@u.northwestern.edu\nJuly 7, 2023\n1\narXiv:2307.02640v1  [cs.CL]  5 Jul 2023\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n1\nAppendix A\nk-means Clustering Mapped to t-SNE Space for k=3 & k=2\n2\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n3\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n2\nAppendix B\nTop 20 LDA Terms for 3 & 2 Topics\n4\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n3\nAppendix C\nDense Neural Network Training & Validation Metrics\n5\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n4\nAppendix D\nClassification Test Evaluation\n6\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n5\nAppendix E\nSentiment Analysis (30% Dropout) Test Evaluation\n7\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n6\nAppendix F\nDNN Dropout Regularization Comparison of Training & Validation Metrics\n8\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n7\nAppendix G\n1D-CNN Dropout Regularization Comparison of Training & Validation Metrics\n9\nUNSUPERVISED SENTIMENT ANALYSIS OF PLASTIC SURGERY SOCIAL MEDIA POSTS -\nJuly 7, 2023\n8\nAppendix H\nSentiment Analysis Test Results Comparing Model Architecture and Dropout Regularization\n10\n",
  "categories": [
    "cs.CL",
    "68T50"
  ],
  "published": "2023-07-05",
  "updated": "2023-07-05"
}