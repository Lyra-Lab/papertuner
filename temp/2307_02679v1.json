{
  "id": "http://arxiv.org/abs/2307.02679v1",
  "title": "A Study on the Impact of Face Image Quality on Face Recognition in the Wild",
  "authors": [
    "Na Zhang"
  ],
  "abstract": "Deep learning has received increasing interests in face recognition recently.\nLarge quantities of deep learning methods have been proposed to handle various\nproblems appeared in face recognition. Quite a lot deep methods claimed that\nthey have gained or even surpassed human-level face verification performance in\ncertain databases. As we know, face image quality poses a great challenge to\ntraditional face recognition methods, e.g. model-driven methods with\nhand-crafted features. However, a little research focus on the impact of face\nimage quality on deep learning methods, and even human performance. Therefore,\nwe raise a question: Is face image quality still one of the challenges for deep\nlearning based face recognition, especially in unconstrained condition. Based\non this, we further investigate this problem on human level. In this paper, we\npartition face images into three different quality sets to evaluate the\nperformance of deep learning methods on cross-quality face images in the wild,\nand then design a human face verification experiment on these cross-quality\ndata. The result indicates that quality issue still needs to be studied\nthoroughly in deep learning, human own better capability in building the\nrelations between different face images with large quality gaps, and saying\ndeep learning method surpasses human-level is too optimistic.",
  "text": "arXiv:2307.02679v1  [cs.CV]  5 Jul 2023\n1\nA Study on the Impact of Face Image Quality on\nFace Recognition in the Wild\nNa Zhang\nAbstract—Deep learning has received increasing interests in\nface recognition recently. Large quantities of deep learning meth-\nods have been proposed to handle various problems appeared in\nface recognition. Quite a lot deep methods claimed that they\nhave gained or even surpassed human-level face veriﬁcation\nperformance in certain databases. As we know, face image quality\nposes a great challenge to traditional face recognition methods,\ne.g. model-driven methods with hand-crafted features. However,\na little research focus on the impact of face image quality on\ndeep learning methods, and even human performance. Therefore,\nwe raise a question: Is face image quality still one of the\nchallenges for deep learning based face recognition, especially in\nunconstrained condition. Based on this, we further investigate this\nproblem on human level. In this paper, we partition face images\ninto three different quality sets to evaluate the performance of\ndeep learning methods on cross-quality face images in the wild,\nand then design a human face veriﬁcation experiment on these\ncross-quality data. The result indicates that quality issue still\nneeds to be studied thoroughly in deep learning, human own\nbetter capability in building the relations between different face\nimages with large quality gaps, and saying deep learning method\nsurpasses human-level is too optimistic.\nIndex Terms—Face recognition, Face image quality, Deep\nlearning\nI. INTRODUCTION\nWe all know that the accuracy of traditional face recognition\n(FR), e.g. Eigenfaces [1] and Fisherfaces [2], is greatly af-\nfected by face image quality problems, such as intraclass vari-\nations between enrollment and identiﬁcation stages. Using face\nimages with poor quality can actually degrade face recognition\nperformance. Non-standard lighting or pose and out-of-focus\nare among the main reasons responsible for the performance\ndegradation. That is why many quality enhancement methods\nwere proposed to try to improve the performance. For example,\nHassner et al. [3] used an off-the-shelf detector to detect\nfaces and facial landmarks, and then align the photo with a\ntextured, 3D model of a generic, reference face. Wang et al.\n[4] performed photometric normalization on face images. One\nsolution, where most researchers commit themselves, is to\nimprove the algorithm itself by making it robust to possible\ndegradation.\nAs the introduction of deep learning (DL) technique, suc-\ncessful development have been obtained on face recognition\n[5]–[10], especially in unconstrained environment, in which\nthe face images contain various face quality challenges, e.g.\npose variations, facial expression, varying illumination, large\nage gap, facial makeup, partial occlusions. Deep learning\nNa Zhang is with Lane Department of Computer Science and Electrical\nEngineering at West Virginia University, Morgantown, WV 26506-6109.\nbased face recognition methods can obtain much robust fea-\ntures and outperform the conventional face recognition meth-\nods with hand-craft features. Some of these methods claimed\nthat they have achieved human-level performance or even\nbetter in face veriﬁcation on the Labelled Faces in the Wild\n(LFW) [11] database. The gap between humans and machines\nseems become narrower.\nLFW database is a well-known, widely used, and challeng-\ning benchmark for face veriﬁcation evaluation, which contains\n13,233 face images of 5,749 subjects collected from the\nweb. Many deep learning based face recognition methods use\nthis database to evaluate their performance in unconstrained\ncondition. Even though existing face veriﬁcation accuracy is\nvery close to 100%, it still remains an argument that claiming\nsurpassing human-level face veriﬁcation performance is too\noptimistic. Liao et al. in [12] ﬁgured out that the existing\nstandard LFW protocol is very limited, only 3,000 positive and\n3,000 negative face pairs for classiﬁcation, and fails to fully\nexploit all the available data. Probably that is why some deep\nmethods can easily reach such high accuracies, even surpass\nthe human-level performance. N. Zhang and W. Deng [13]\nalso proposed several limitations on LFW, like that intraclass\nvariations and interclass similarity sometimes may be ignored\nby researchers, insufﬁcient matching pairs can not capture the\nreal difﬁculty of large-scale unconstrained face veriﬁcation\nproblem. Therefore, it is questionable to say that deep models\nhave touched the limit of LFW benchmark.\nFor traditional automatic face recognition systems, their\nperformance largely depends on the quality of the face images.\nGenerally speaking, face image quality can be used as a\nmeasure metric for their performance. In the early stage, most\nface images were obtained under controlled environment with\nproper lighting condition, frontal pose, neutral expression, no\nor less makeup and standard image resolution, e.g. photos\non ID cards. These faces own pretty high quality, thus it is\neasy for FR systems to achieve extremely high recognition\naccuracy. However, as the emergency of face data captured\nunder uncontrolled environment (e.g. face images crawled\nfrom Internet), these images with low quality signiﬁcantly\ndegrade recognition accuracy. Some researchers tend to seek\nfor more robust methods, thus deep learning based method\nwas brought in. Different with traditional methods which are\nmodel driven, deep learning methods are learning driven which\ncan automatically learn all kinds of faces with different quality\nproblems if enough data are fed into the network. It seems that\nface image quality become less important for the performance\nof deep learning based face recognition system. Besides little\nresearch specially study the impact of face image quality on\n2\nFig. 1. Pipeline of our approach.\ndeep learning methods.\nIt is well known that face recognition in unconstrained\ncondition is much more difﬁcult due to various changes in\nface images, e.g. pose variations, illumination changes, varying\nfacial expression, partial occlusion, low resolution, age varia-\ntions, heavy make-up, etc. Besides, high interclass similarity\nand large intraclass variation are still two big challenges for\nface recognition task. Although existing deep models have\nbeen trained very well for various quality changes of face\nimages, it is still much more challenging for deep models to\nrecognize faces with quite low quality. Therefore, we raise a\nquestion: Does the performance of deep learning based face\nrecognition system still depend on the face image quality? If\nnot, what is the challenge? If so, how it affects? Based on\nthis, we further investigate the impact of face image quality\non human performance, and the gap between deep learning\nand human.\nIn our previous research [14], we proposed that the face\nimage quality issue is still a grand challenge for deep learning\nmethods. In order to prove this, we developed new face\nrecognition protocols for cross-quality face identiﬁcation and\nveriﬁcation on two public databases, IJB-A [15] and Face-\nScrub [16], and four popular deep models were evaluated\nunder this settings. Based on this research, we asked human\nbeings to perform face veriﬁcation experiment on the faces\nin unconstrained environment by matching across different\nface image qualities and further investigate the impact of\nface image quality on human performance and the distance\nbetween human beings and deep learning methods. We also\nseek to expand previous comparisons [17]–[24] by performing\nface veriﬁcation on cross-quality face data in the wild. In our\nexperiment, we focus on face images of extremely difﬁcult\nlevels. These images are chosen from face pairs that the deep\nmodel fails to recognize successfully. The evaluation on human\nperformance in face veriﬁcation discloses that human beings\nshow a different performance with deep learning methods, and\nsaying surpassing human-level is still too optimistic.\nThe contributions of our work includes:\n• as an extension of research [14], we aim to examine the\nface recognition performance of deep learning and human\nbeings on cross-quality face images;\n• four pre-trained deep models with high reported accuracy\nare adopted to perform cross-quality face recognition\non two databases, IJB-A and FaceScrub; and the deep\nmodel with best recognition performance is chosen to be\ncompared with human beings;\n• human beings perform better than deep learning on\nface recognition by matching face images with different\nqualities, especially when the quality gap is large, which\nalso indicates that deep learning method still has a long\nway to surpass human.\nThe paper is organized as follows. In section II, we talk\nabout related work on face image quality assessment, human\nperformance in face recognition. In section III, we describe\nhow to choose the best model among four representative deep\nmodels. In section IV, the face veriﬁcation experiment is\nperformed by human. And section V gives an analysis on\nthe results. In section VI, some interesting discussion and\nconclusions are drawn.\nII. RELATED WORK\nA. Face Image Quality Assessment\nFace image quality is an important factor that apparently\naffect the performance of traditional face recognition. In\npractical recognition system, it is usual to choose multiple face\nimages for each subject, hence choosing face images with high\nquality is a good way to improve recognition accuracy. The ap-\nproved ISO/IEC standard 19794-5 [25] speciﬁed recommenda-\ntions for face photo taking for ID card, E-passport and related\napplications, including instructions for light condition, head\npose, facial expression, occlusion, and so on. Figure 2 shows\na few correct and incorrect illustration face images of ISO/IEC\n19794-5 standard [26]. Face images of bad quality which do\nnot accord with the requirements of the standards is a reason\nleading to face recognition performance degradation. ISO/IEC\n29794-5 [27] speciﬁes a few methodologies and approaches for\ncomputation of quantitative quality scores for facial images by\n3\nFig. 2. Illustration of face images by ISO/IEC 19794-5 standard. Top two rows: incorrect face photos, bottom row: correct face photo.\nintroducing facial symmetry, resolution and size, illumination\nintensity, brightness, contrast, color, exposure, sharpness, etc.\nRecently, a few face image quality assessment methods have\nbeen proposed. Most existing face image quality assessment\nmethods are based on the analysis of speciﬁc facial properties.\nYang et al. [28] introduced a face pose estimation method by a\nboosting regression algorithm to evaluate face image quality,\nand applied it in the best shot selection problem to choose\nthe most frontal face from a video sequence. Gao et al. [29]\ndeveloped a facial symmetry based method for face image\nquality assessment in which it applies the degree of facial\nasymmetry to quantify the face quality caused by non-frontal\nillumination and improper face pose. Nasrollahi and Moeslund\n[30] assesses face quality in video sequence by combining\nfour features (e.g. out-of-plan rotation, sharpness, brightness\nand resolution) using a local scoring system and weights.\nSang et al. [31] presented several methods for face image\nquality evaluation. It uses Gabor wavelets as basis features to\nestimate the facial symmetry and then evaluate the illumination\ncondition and facial pose. Sellahewa et al. [32] try to measure\nthe face image quality in terms of luminance distortion in com-\nparison to a speciﬁed reference face image. Wong et al. [33]\ndesigned a patch-based face image quality assessment method\nto choose the ’best’ subset of face images from multiple frames\nof video captured in uncontrolled conditions by quantifying\nthe similarity of a face image to a probabilistic face model,\nthe ’ideal’ face. Image characteristics that affect recognition,\nsuch as head pose, illumination, shadowing, motion blur and\nfocus change over the sequence, are taken into account. Long\nand Li [34] designed a quality assessment system to select\nthe best frame from the input video sequence by considering\nﬁve features including sharpness, brightness, resolution, head\npose and expression. The score of each feature is calculated\nseparately, and then the ﬁnal quality score is obtained by\nweight fusion of ﬁve scores. The image quality assessment\nmodel in [35] assesses the image quality by considering\nocclusion, face-to-camera distance, pose, expression, uneven\nillumination measure.\nMost of the methods mentioned above apply the artiﬁcially\ndeﬁned facial properties and empirically selected reference\nface images in their assessment process. Some others apply\ndifferent features, or strategies. Zhang and Wang [36] proposed\nthree asymmetry based face quality measures, which are based\non scale insensitive SIFT features. Bharadwaj et al. [37]\napplied Gist and HOG to classify face images into different\nquality categories that are derived from face matching perfor-\nmance. Raghavendra et al. [38] proposed a scheme for face\nquality estimation. It ﬁrst separates frontal faces from non-\nfrontal ones by pose estimation, and evaluate the image quality\nof frontal faces by analyzing its texture components using\nGrey Level Co-occurrence Matrix (GLCM), ﬁnally quantify\nthe quality using likelihood values obtained using Gaussian\nMixture Model (GMM). Chen et al. [39] proposed a simple\nand ﬂexible framework in which multiple feature fusion and\nlearning to rank are used.\nB. Human Performance in Face Recognition\nA lot researchers did pretty much work to evaluate human\nperformance in face recognition. O’Toole et al. [17] did a se-\nries of face veriﬁcation experiments on human and algorithms\n4\nin which the face images of each pair were taken under differ-\nent illumination conditions. They found that three algorithms\nsurpassed humans being performance by matching face pairs\npre-screened to be ”difﬁcult” and six algorithms surpassed\nhumans on ”easy” face pairs. Alice J. O’Toole et al. [20]\ncompared the performance of humans and machines in face\nidentiﬁcation task on frontal face images taken under different\nuncontrolled illumination conditions in both indoor and out-\ndoor settings and with natural variations in a person’s day-to-\nday appearance. In particular, they studied how human beings\nperform relative to machines as the level of difﬁcultly increases\nas the variations contributed, such as facial expression, partial\nocclusion, hair styles and so forth. They concluded that the\nsuperiority of machines over humans in the less challenging\nconditions may indicate that face recognition systems may be\nready for applications with comparable difﬁculty.\nKumar et al. [18] presented an evaluation of human perfor-\nmance on LFW dataset by following a procedure mentioned\nin paper [17]. They generated 6,000 image pairs and asked\n10 users to label two faces of each pair whether they belong\nto same person or not. The users were also asked to rate\ntheir conﬁdence when labelling. Human performance on LFW\nis 99.20%, 97.53% and 94.27% when users are shown the\noriginal images, tighter cropped images and inverse crops.\nHuman performance is really perfect when the participants are\nshown the original images. Due to lacking context information,\nthe performance drops when a tighter cropped version of face\nimages are given. It indicates that human can easily use context\ncues to recognize faces. Besides, the human performance is\nstill wonderful when they are just shown the inverse cropped\nversion (only context information is shown). P. Jonathon\nPhillips et al. [19] also did a similar work by matching frontal\nfaces in still and video face images in different difﬁculty\nlevels (e.g. good, challenging, very challenging). The result\nshowed that algorithms are consistently superior to humans\nfor frontal still faces with good quality, and humans are\nsuperior for video and challenging still faces. The result also\nindicated that humans can use non-face identity cues (e.g.\nhead, body. etc.) to recognize faces. Best-Rowden et al. [21]\nanalyzed the face recognition accuracies achieved by both\nmachines and humans on unconstrained face data, reported the\nhuman accuracy in still images via crowdsourcing on Amazon\nMechanical Turk, and ﬁrst reported human performance on\nvideo faces, the YouTube Faces database, which indicated\nthat humans are superior to machines, especially when videos\ncontain contextual cues in addition to the face image.\nZhou et al. [22] did a human face veriﬁcation test in\nreal-world environment on Chinese ID (CHID) benchmark,\nin which the data were collected ofﬂine and specialized on\nChinese people. The dataset contains a typical characteristic,\nage variation including intra-variation (i.e., same person with\ndifferent ages) and inter-variation (i.e., persons with different\nages). The experiment focused on cases their recognition\nsystem failed to recognize. The result showed that 90% cases\ncan be solved by human. Phillips et al. [23] expanded the\ncomparison between human and machine from still images\nand videos taken by digital single lens reﬂex cameras to digital\npoint and shoot cameras, Point and Shoot Face Recognition\nFig. 3.\nFace examples of High (top row), Middle (middle row), and Low\n(bottom row) quality sets from two databases: (a) IJB-A, (b) FaceScrub.\nChallenge (PaSC). They provided a human benchmark for\nverifying unfamiliar faces in unconstrained still images at\ntwo levels: challenging and extremely-difﬁcult. 100 different-\nidentity image-pair with the highest similarity scores and 100\nsame-identity image-pair with the lowest similarity scores\nwere selected and 30 users were asked to view two faces of\neach image-pair side by side and rate on a 1 to 5 scale. The\nresults demonstrated that, in extremely-difﬁcult level, human\nperformance shines relative to algorithms.\nAustin Blanton et al. [24] also made a comparison of per-\nformance between human and algorithms in face veriﬁcation\non the challenging IJB-A dataset, which includes varying\namounts of imagery, immutable attributes,e.g. gender, and\ncircumstantial attributes, e.g. occlusion, illumination, and pose.\nIn their experiment, the participants are asked to show how\nconﬁdent when they decide whether two given faces belong\nto same subjects or not with six options, which are Certain,\nLikely, Not Sure, Unlikely, Deﬁnitely Not, and Not Visible.\nThe result shows that even for the challenging images in IJB-\nA, face veriﬁcation is an easy task for humans.\nIn the past 10 years, pretty a lot researchers studied the\nperformance of humans and machines on face recognition\nand did all kinds of comparisons between them. In some\nscenarios, especially ”easy” cases, the algorithms perform\nbetter, and in other scenarios, like still images in ”difﬁcult”\nlevels with various variations and videos, the humans are\nbetter. As the fast development of deep learning technique\nin face recognition, the performance of deep models increase\nquickly. Quite a lot research reported the surpassing human-\nlevel performance on face recognition. Can deep learning\ntechnique really gain more excellent performance than human?\nIII. OUR APPROACH\nFig. 1 shows a whole pipeline of our approach. At the\nbeginning, we partition two popular public databases in the\nwild, IJB-A [15] and FaceScrub [16], into three quality sets\n(e.g. high quality, middle quality, low quality) separately\naccording the face image quality score. Four famous pre-\ntrained deep models, Light CNN [40], FaceNet [5], VGGFace\n[41], and CenterLoss [7], with high reported accuracy, are\nchosen to perform face recognition experiments, including\nface identiﬁcation and face veriﬁcation, on cropped faces\n5\nTABLE I\nFACE IMAGES DISTRIBUTION ON IJB-A AND FACESCRUB.\nQuality Set\n# Images\n# Subjects\nIJB-A\nHigh\n1,543\n500\nMiddle\n13,491\n483\nLow\n6,196\n489\nFaceScrub\nHigh\n10,089\n530\nMiddle\n10,444\n530\nLow\n362\n232\nof the two databases. After that, the deep model with best\nperformance among them is selected by evaluating their per-\nformance. And the face images that the best model fails to\nrecognize successfully are ﬁltered as the data to be used\nin our well-designed human veriﬁcation experiments. Human\nbeings are asked to perform face veriﬁcation experiment by\nmatching across different face image qualities and then the\nresult is evaluated to further examine whether face image\nquality changes can impact the performance of human beings,\nhow, and what is the gap between deep model and human. In\nthe experiment, we focus on extremely difﬁcult level of face\nimages, i.e., matching low to high quality sets. These images\nare chosen from face pairs that deep model fails to recognize\nsuccessfully.\nA. Face Image Quality\nAlthough LFW is very popular for face recognition in the\nwild, there still exists some limitations, like the standard\nLFW protocol contains limited number of pairs, which causes\ninsufﬁcient exploration on various quality issues, e.g. pose\nvariations, lighting condition, low resolution. Therefore, face\nimage quality changes maybe the key issue in unconstrained\nface recognition. In order to have a better understanding of\nthe face image quality, we are ﬁrst to examine the distribution\nof different face qualities in the data and the impact of the\ndistribution on face recognition performance.\nThe face image quality is evaluated by considering speciﬁc\nfacial properties, like resolution, pose angle, illumination pa-\nrameters, or occlusion. We adopt a method proposed in [39]\nto measure and quantify the quality of every face image. This\nmethod tries to compare the relative qualities of each face pairs\nand then use the relative relationship to train a ranking based\nmodel to learn the quality score. The generated quality score,\nwhich is between 0 and 100, is used as the indicator of face\nimage quality. The higher the quality score is, the better quality\nthe face image has. According to the score of face image, the\ndatabase is divided into three subsets, i.e., high quality, middle\nquality and low quality sets. In our study, high quality set is\nselected as the gallery set, and middle, low quality sets as\nprobe set separately, and then to perform face recognition on\nfour deep models.\nB. Database Preparation\nWe evaluate the performance of face recognition with\nmatching across different face image quality sets on two public\nface databases, IJB-A [15] and FaceScrub [16]. IJB-A, the\nIARPA Janus Benchmark A (IJB-A) database, is a publicly\navailable media in the wild dataset containing a total of 21,230\nface images of 500 subjects with manually localized face im-\nages. It is more challenging for face recognition. This dataset\ncontains full pose variation, joint use for face recognition\nand face detection benchmark, wider geographic variation of\nsubjects, protocols supporting both open-set identiﬁcation (1:N\nsearch) and veriﬁcation (1:1 comparison), an optional protocol\nthat allows modelling of gallery subjects and ground truth eye\nand nose locations. FaceScrub was created by building face\ndataset that detects faces in images returned from searching\nfor public ﬁgures on the Internet, followed by automatically\ndiscarding those not belonging to each queried person. It\ncomprises a total of 106,863 face images of 530 celebrities\nwith about 200 images per person. It contains 55,306 face\nimages of 265 males and 51,557 face images of 265 females.\nAll face images in both databases are estimated by the face\nimage quality assessment method [39] and quality scores are\ncalculated for each face image. According to these scores,\nwe divide the two databases into three different quality sets.\nTable I shows the distribution of three quality sets on the two\ndatabases. The quality score is between 0 and 100. Image\nquality scores in high quality set are greater than or equal to\n60. Scores in middle quality set are greater than or equal to\n30 and less than 60. And scores in low quality set are less\nthan 30. Fig. 3 gives some face examples of high, middle,\nand low quality sets from the two databases. Images with high\nquality are those frontal faces with high resolution, proper light\ncondition, no occlusion. Images with low quality are those\nwith big pose, dark light condition, or partial occlusion. And\nimages with middle quality are those cases between the two\nsituations.\nFor IJB-A database, we ﬁnd that quite a lot subjects in\nhigh quality set have less than three images. To ensure the\ngallery, i.e., high quality set, has enough target faces (at least\nthree), we choose a few images from middle quality sets\nwith higher scores to the high quality set. From Fig.4 (a),\nit is easy to notice that most subjects in high quality set\nhave three images. The middle quality set contains the most\nimages (63.55%), and low quality set also contains pretty\nmuch (29.19%). However, FaceScrub database owns many\nimages with pretty good quality, about 70% images with high\nquality and 25% with middle quality. In order to match the\nsize of IJB-A, a shortened version of FaceScrub is generated\nby randomly selecting images from each subject in high and\nmiddle quality sets. Finally, the subset of FaceScrub contains\na total of 20,895 images of 530 subjects as shown in table I.\nFrom Fig.4 (b), we can see the shortened FaceScrub still has\nquantities of face images with pretty good quality.\nC. Deep Models\nLight CNN [40], FaceNet [5], VGGFace [41], and Cen-\nterLoss [7] are four popular deep models that have reported\nvery high accuracies (LightCNN: 99.33%, FaceNet: 99.63%,\nVGGFace: 98.95%, and CenterLoss: 99.28%) on LFW for face\nveriﬁcation. Light CNN [40] is a light framework to learn a\n256-D face representation on the large-scale face data with\n6\nFig. 4. Distribution of High, Middle, and Low quality sets for each subject on (a) IJB-A and (b) FaceScrub databases. Best view in color\nmassive noisy labels. It is efﬁcient in computational costs\nand storage spaces. FaceNet [5] can directly learn a mapping\nfrom input face images to a compact 128-D Euclidean space\nin which the Euclidean distance indicates face similarity.\nVGGFace [41] is inspired by [42]. It is a ’very deep’ network\nwith a long sequence of convolutional layers. CenterLoss [7]\nuses two loss functions, softmax and center loss, to train the\ndeep model. The center loss can learn a center of deep features\nfor each class to reduce the intra-class variations and enlarge\nthe inter-class differences.\nD. Choose Model with Best Performance\nTo avoid any bias in training stage, we use the pre-trained\ndeep models to perform cross-quality face identiﬁcation and\n7\nFig. 5. CMC of face identiﬁcation experiments by matching different quality\nimages using (a) VGGFace, (b) Light CNN, (c) CenterLoss, and (d) FaceNet\non IJB-A. Best view in color\nFig. 6. CMC of face identiﬁcation experiments by matching different quality\nimages using (a) VGGFace, (b) Light CNN, (c) CenterLoss, and (d) FaceNet\non FaceScrub. Best view in color\nveriﬁcation experiments on three types (high, middle, and\nlow) of quality sets from IJB-A and FaceScrub databases. By\nevaluating the performance, the model with best performance\nis selected.\n1) Face Identiﬁcation: Face identiﬁcation aims to recognize\nthe person from a set of gallery face images and ﬁnd the\nmost similar one to the probe sample. For each database, we\ndesign three groups of experiments, and in each group the\nmatching faces is across different quality sets. The ﬁrst one\nis low to high matching in which low quality set is designed\nas query images and high quality set is gallery images. The\nsecond one is middle to high matching in which middle quality\nset is query images and high quality set is gallery images.\nAnd the third one is low to middle matching in which query\nFig. 7. ROC of face veriﬁcation experiment by matching (a) Low vs. High\nQuality on IJB-A, (b) Middle vs. High on IJB-A, (c) Low vs. High on\nFaceScrub, and (d) Middle vs. High on FaceScrub. Best view in color\nimages come from low quality set and gallery images are from\nmiddle quality set. Deep features of three quality sets from\nfour deep models on IJB-A and FaceScrub are extracted and\nCosine Similarity Score is adopted to calculate the similarity\nscore of each face pair. The performance of four models is\nmeasured by Cumulative Match Curve (CMC) [43] on two\ndatabases as shown in Fig.5 and Fig.6. It is easily to ﬁnd that\nthe performance of matching from middle to high quality set\nis much better than the other two matches for all deep models.\nThe performance of matching from low to middle is slightly\nbetter than that of matching from low to high for most cases.\nThe reason probably is that the difference between low and\nhigh quality faces is larger than the difference between low\nand middle quality faces. In general, VGGFace has the better\nresult than the other three models, and FaceNet performs the\nworst.\n2) Face Veriﬁcation: Face veriﬁcation aims to determine\nwhether a given pair of face images or videos belongs to the\nsame person or not. Considering that the performance of low\nto high and low to middle quality sets are nearly similar, only\nlow to high and low to middle cases are performed in face\nveriﬁcation experiment. Low and middle quality sets of each\ndatabase are set as query images separately and high quality\nset as gallery images. Finally, about 18,978 positive pairs and\n9,541,450 negative pairs in the case of matching low to high\nquality sets, and 41,642 positive pairs and 20,774,971 negative\npairs in the case of matching middle to high quality sets on\nIJB-A database are generated, and also 6,676 positive pairs\nand 3,645,542 negative pairs in the case of matching low to\nhigh quality sets, and 193,745 positive pairs and 105,175,771\nnegative pairs in the case of matching middle to high quality\nsets on FaceScrub database are generated.\nIn the face veriﬁcation experiment, we construct a similarity\nmatrix in which the row presents one query image, the column\n8\nTABLE II\nFACE VERIFICATION RERSULT ON FOUR DEEP MODELS.\nDatabase\nDeep Model\nLow vs. High\nMiddle vs. High\nFAR=0.01\n0.001\n0.0001\n0.01\n0.001\n0.0001\nIJB-A\nVGGFace\n0.605\n0.367\n0.194\n0.858\n0.675\n0.491\nLightCNN\n0.566\n0.402\n0.269\n0.905\n0.808\n0.678\nCenterLoss\n0.521\n0.313\n0.164\n0.859\n0.692\n0.499\nFaceNet\n0.257\n0.100\n0.033\n0.586\n0.330\n0.165\nGabor\n0.037\n0.006\n0.001\n0.200\n0.112\n0.064\nShortened FaceScrub\nVGGFace\n0.595\n0.389\n0.231\n0.837\n0.662\n0.468\nLight CNN\n0.503\n0.330\n0.148\n0.896\n0.811\n0.668\nCenterLoss\n0.493\n0.341\n0.215\n0.914\n0.814\n0.652\nFaceNet\n0.219\n0.075\n0.019\n0.633\n0.350\n0.162\nGabor\n0.022\n0.003\n0.001\n0.082\n0.027\n0.010\nindicates one gallery image and the value in the matrix shows\ncosine similarity score between two face images of the corre-\nsponding row and column. Simultaneously, a similarity mask\nmatrix is built in which the row still indicates one query image\nand the column indicates one gallery image. The difference\nbetween the two matrices is the values. In similarity mask\nmatrix, the values have only two types. -1 means that two face\nimages in the corresponding row and column is a positive pair\nand 127 means negative pair. We still adopt Cosine Similarity\nScore to show how similar two faces are and then calculate\nveriﬁcation accuracies with respect to FAR=0.01, 0.001 and\n0.0001 (FAR: false accept rate) as presented in table II, and\nalso give Receiver Operating Characteristic curves (ROC) in\nFig. 7. The result of veriﬁcation using Gabor feature is set as\na baseline to be compared. We can see that the performance of\nGabor feature is the worst. There is a big gap between Gabor\nfeatures and deep features. For matching middle to high quality\nsets experiment, Light CNN and CenterLoss has the best\nperformance on IJB-A and FaceScrub separately. And in low\nto high experiment, VGGFace performs best on FaceScrub,\nand better than others in FAR=0.01 case on IJB-A.\nBy analyzing the results of face identiﬁcation and veri-\nﬁcation experiments, we can see that, on IJB-A, VGGFace\nhas the best performance in low to high experiment, Light\nCNN is the best in middle to high experiment, and on\nFaceScrub, VGGFace gains the highest accuracy in low to\nhigh experiment, CenterLoss performs best in middle to high\nexperiment.\nIV. FACE VERIFICATION EXPERIMENT BY HUMAN\nIn this face veriﬁcation experiment, we use the best model\nchosen from previous face identiﬁcation and veriﬁcation ex-\nperiments, and try to ﬁnd the decision boundary for these\npositive and negative face pairs based on the best model. Then\nwe randomly select a certain number of face pairs that the\nbest model fails to recognize and perform human veriﬁcation\nexperiment on the selected face pairs.\nSince our goal is to examine how well the human per-\nformance on face veriﬁcation comparing to algorithms, we\nmainly focus on face veriﬁcation task in extremely difﬁcult\nlevel, matching low quality set to high quality set. From\nprevious experiments, it is easy to ﬁnd that VGGFace has the\ngreatest performance on IJB-A and FaceScrub databases in\nFig. 8.\nGenuine and impostor score distribution on IJB-A and FaceScrub.\n(a) genuine score distribution on IJB-A, (b) impostor score distribution on\nIJB-A, (c) genuine score distribution on FaceScrub, and (d) impostor score\ndistribution on FaceScrub.\nmatching low to high experiment. Hence we choose a number\nof face image pairs of low to high quality set on IJB-A and\nFaceScrub databases based on VGGFace model to do human\nface veriﬁcation experiment.\nA. Get Decision Boundary\nWe generate the statistical distributions of genuine and\nimpostor matching scores of all positive and negative pairs\non the two databases to ﬁnd the decision boundaries. Fig.\n8 shows the statistical distributions of genuine and impostor\nscores on both databases. And then the distributions are ﬁtted\nas Gaussian distribution illustrated in Fig. 9. Finally, the\nthresholds, 0.188 for IJB-A and 0.138 for FaceScrub, are easily\nobtained.\nB. Choose Genuine and Impostor Pairs\nBased on the thresholds, genuine and impostor pairs can\nbe easily selected. Those face images that VGGFace fails to\n9\nFig. 9. Genuine (blue line) and impostor (red line) matching score distribution\non (a) IJB-A and (b) FaceScrub. The threshold value is from the match score\nof green dot shows. Best view in color\nTABLE III\nDETAILS ON THREE GROUPS OF PARTICIPANTS.\nGroups\n# Male\n# Female\nIn Total\nDescription\nAll\n14\n6\n20\nGroup1\n2\n1\n3\nHave much experience on\nface quality\nGroup2\n2\n2\n4\nWorked on some facial\nimage analysis tasks\nGroup3\n10\n3\n13\nHave no background\nrecognize successfully are chosen, so the genuine pairs whose\nmatching scores are less than the threshold value and the\nimpostor pairs whose matching scores are greater than or equal\nto the threshold are ﬁltered from two databases. Since context\ninformation in face image can give people some useful cues\nto recognize the identities [18], the original images are not\ndirectly used in the experiment. We adopt a cropped version of\noriginal face images from VGGFace. Besides, those pairs that\nthe face images are wrongly or improperly aligned or cropped\nare manually removed to ensure that those pairs in the human\nexperiment do not contain some technical errors caused by the\nfactors that , not image quality. And then we randomly select\n100 positive pairs and 100 negative pairs from the cleaned\npairs, put them together and randomly permute them. Finally,\na total of 400 pairs for two databases are obtained. In this case,\nthe veriﬁcation rate of deep model VGGFace is 0% correct.\nC. Participants and Tool\nWe design a face veriﬁcation experiment performed by\nhumans. In the experiment, a total of 20 participants, 14 males\nand 6 females, are asked to view 400 face image pairs and give\ntheir choice on whether the two faces in each given pair belong\nto same person or not. A part of them (as indicated in table III)\nhave much experience on face image quality analysis, some\nones just know about it and others have no background. For\nconvenience, a tool is designed to assist participants during\nexperiment. Fig. 10 shows some samples of face pairs shown\nin the tool. Left is two positive pairs and right are two negative\nones.\nFig. 10.\nSamples of face images pairs: (a) two genuine pairs, and (b) two\nimpostor pairs.\nD. Experiment Procedure\n100 positive pairs and 100 negative pairs are randomly\nselected for each database. These 200 pairs are divided into\nfour subsets randomly with same size, i.e., 50 pairs. A total\nof eight subsets are generated in the end. All participants\nare asked to check the pairs one by one for each subset on\nthe designed tool and make the decision. After ﬁnishing one\nsubset, participants are advised to check next subset after a\npretty good rest which makes them work on this task with\nfull of energy. All participants have unrestricted time to ﬁnish\nthis experiment.\nV. EXPERIMENT RESULTS AND ANALYSIS\nAll participants are grouped into three sets as indicated\nin Table III according to their background on image quality\nanalysis. 3 persons (2 males and 1 female) have quite a\nlot experience on face quality understanding and analysis. 4\nindividuals (2 males and 2 females) have ever worked on\nrelated topics, and the remaining (10 males and 3 females)\nhave little background. We also analyzed all participants as one\ngroup. Most of them are students. Majority voting technique\nis adopted to deal with the ﬁnal results of these four groups. If\nthe number in the group is even, one subject in it will randomly\nremoved and just odd number of subjects are considered. Table\nIV and V gives the confusion matrix results including positive\nand negative accuracies in both actual and predicted cases on\nIJB-A and FaceScrub databases. ROC curves are also drawn\nin Fig. 11.\nBy analyzing the results, we can easily ﬁnd that the\nperformance of human on IJB-A and FaceScrub is more\nexcellent than VGGFace (best among the four deep models),\nalthough very high accuracy on LFW benchmark is achieved.\nThere still exists a clear gap between human performance and\nmachine recognition especially in the real-world setting. Real-\nworld face recognition has much more diverse criteria, like\nbig pose angle, poor illumination condition, and large facial\nocclusion, than we treated in previous recognition benchmarks.\nAnd data quality plays an important role in the performance\nof algorithms. Wider and more arbitrary range of changes\n10\nTABLE IV\nCONFUSION MATRIX RESULT ON IJB-A DATABASE.\nIJB-A:All\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n81%\n19%\n84%\nNegative\n13%\n87%\nIJB-A:Group1\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n93%\n7%\n92%\nNegative\n9%\n91%\nIJB-A:Group2\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n79%\n21%\n79.5%\nNegative\n20%\n80%\nIJB-A:Group3\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n65%\n35%\n76%\nNegative\n13%\n87%\nTABLE V\nCONFUSION MATRIX RESULT ON FACESCRUB DATABASE.\nFaceScrub: All\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n28%\n72%\n57%\nNegative\n14%\n86%\nFaceScrub:Group1\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n57%\n43%\n74.5%\nNegative\n8%\n92%\nFaceScrub:Group2\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n43%\n57%\n57%\nNegative\n29%\n71%\nFaceScrub:Group3\nPredicted\nAccuracy\nPositive\nNegative\nActual\nPositive\n19%\n81%\n49.5%\nNegative\n20%\n80%\nFig. 11.\nROC of human face veriﬁcation experiment (a) IJB-A and (b)\nFaceScrub.\nlike pose, illumination, expression, occlusion, resolution, age\nvariation, heavy make-up of face images are most common\nfactors which inﬂuence the system’s performance. However,\nit still lacks a sufﬁcient investigation on these cross factors,\nand also lacks an efﬁcient method to handle them clearly and\ncomprehensively. Large amount of face data with these factors\nare needed to assist us to build better models to improve\nrecognition performance.\nWe also ﬁnd that people who have much experience in face\nrecognition perform better than those who have not. What is\ninteresting is that people have higher accuracy in recognition\nof negative pairs than that of positive pairs. The reason may be\nthat it is hard for people to recognize that the two faces belong\nto same subject for positive pairs since the quality of face in\nquery set is much low, but for negative pairs, it is much easier\nto view two faces as negative (different persons). Besides, we\nﬁnd that the accuracies on FaceScrub are lower than IJB-A.\nThe reason may be that the quality of faces in query set (low\nquality set) on FaceScrub is much lower than that on IJB-A.\nThe quality scores of face images can also prove this.\nVI. DISCUSSION AND CONCLUSION\nIt is obvious that face image quality plays an important\nrole in model-driven face recognition systems. Faces with bad\nquality can directly degrade the accuracy of face recognition.\nThe main reason may be that most face recognition methods\nin the early stage try to build the models that are used to\nextract hand-craft features, and nearly all data are collected\nin controlled conditions with standard lighting, ﬁxed head\npose, proper facial expression, etc. These data fails to contain\nvarious or mixed qualities of face images. And the built models\nare sensitive to face quality changes. In order to improve the\naccuracy, some research focus on designing face image quality\nenhancement methods, like deblurring [44], pose correction\n[3], and photometric normalization [4]. Another solution is to\ndevelop more robust algorithm to possible degradation. The\nbrought of deep learning technique into face recognition ﬁeld\ngives an clear direction to further development.\nIn our previous research [14], we explored the impact of\nface image quality on deep learning based face recognition\nin unconstrained environment. Practically, the performance of\ndeep neural networks can be largely improved by feeding\nvarious of face data with different qualities in training stage.\nSince the deep networks have almost learnt all kinds of face\nimages with different qualities, they may keep in mind certain\nconnections between them on some level. Hence deep learn-\ning based face recognition system can obtained more robust\nfeatures than traditional face recognition methods. However, in\nfact, face image quality still has an inﬂuence on the accuracy of\nface recognition, although the deep networks have seen large\nquantities of face images. For example, in face identiﬁcation\nevaluation on four deep models, it is easy for deep models\nto identify the correct subject in matching faces from middle\nto high qualities, but difﬁcult in matching from low to high,\nwhich shows that deep models can recognize faces whose\nquality changes are big to some degrees, but not too huge.\nTherefore, more robust deep learning methods than existing\nones are still needed to be able to recognize faces with large\nquality gaps.\nThe inﬂuence of face image quality on human performance\nwere further explored. We designed a face veriﬁcation ex-\nperiment by human beings on cross-quality face data, IJB-\nA and FaceScrub, by matching from low to high qualities,\nwhich is the hardest one. The human performance on IJB-\nA and FaceScrub are more excellent than the best model,\nVGGFace. Human outperform deep learning methods largely.\nThe result indicts that there still exists a clear gap between\nhuman and machine performance in face recognition in un-\nconstrained environment. Human beings own the capability\n11\nin recognizing face images with large quality gaps. Besides,\nall participants were grouped into three categories according\nto their background on face image quality analysis, and the\nperformance of each group were analyzed too.\nREFERENCES\n[1] M. A. Turk and A. P. Pentland, “Face recognition using eigenfaces,” in\nComputer Vision and Pattern Recognition, 1991. Proceedings CVPR’91.,\nIEEE Computer Society Conference on.\nIEEE, 1991, pp. 586–591.\n[2] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces\nvs. ﬁsherfaces: Recognition using class speciﬁc linear projection,” Yale\nUniversity New Haven United States, Tech. Rep., 1997.\n[3] T. Hassner, S. Harel, E. Paz, and R. Enbar, “Effective face frontalization\nin unconstrained images,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2015, pp. 4295–4304.\n[4] B. Wang, W. Li, W. Yang, and Q. Liao, “Illumination normalization\nbased on weber’s law with application to face recognition,” IEEE Signal\nProcessing Letters, vol. 18, no. 8, pp. 462–465, 2011.\n[5] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed em-\nbedding for face recognition and clustering,” in 2015 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2015, pp.\n815–823.\n[6] C. Lu and X. Tang, “Surpassing human-level face veriﬁcation perfor-\nmance on lfw with gaussianface.” in AAAI, 2015, pp. 3811–3819.\n[7] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature\nlearning approach for deep face recognition,” in European Conference\non Computer Vision.\nSpringer, 2016, pp. 499–515.\n[8] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, “Sphereface: Deep\nhypersphere embedding for face recognition,” in The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), vol. 1, 2017.\n[9] H. Wang, Y. Wang, Z. Zhou, X. Ji, Z. Li, D. Gong, J. Zhou, and W. Liu,\n“Cosface: Large margin cosine loss for deep face recognition,” arXiv\npreprint arXiv:1801.09414, 2018.\n[10] J. Deng, J. Guo, and S. Zafeiriou, “Arcface: Additive angular margin\nloss for deep face recognition,” arXiv preprint arXiv:1801.07698, 2018.\n[11] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “La-\nbeled faces in the wild: A database for studying face recognition in\nunconstrained environments,” Technical Report 07-49, University of\nMassachusetts, Amherst, Tech. Rep., 2007.\n[12] S. Liao, Z. Lei, D. Yi, and S. Z. Li, “A benchmark study of large-\nscale unconstrained face recognition,” in Biometrics (IJCB), 2014 IEEE\nInternational Joint Conference on.\nIEEE, 2014, pp. 1–8.\n[13] N. Zhang and W. Deng, “Fine-grained lfw database,” in Biometrics\n(ICB), 2016 International Conference on.\nIEEE, 2016, pp. 1–6.\n[14] G. Guo and N. Zhang, “What is the challenge for deep learning in un-\nconstrained face recognition?” in Automatic Face & Gesture Recognition\n(FG 2018), 2018 13th IEEE International Conference on.\nIEEE, 2018,\npp. 436–442.\n[15] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen,\nP. Grother, A. Mah, and A. K. Jain, “Pushing the frontiers of uncon-\nstrained face detection and recognition: Iarpa janus benchmark a,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015, pp. 1931–1939.\n[16] H.-W. Ng and S. Winkler, “A data-driven approach to cleaning large\nface datasets,” in Image Processing (ICIP), 2014 IEEE International\nConference on.\nIEEE, 2014, pp. 343–347.\n[17] A. J. O’Toole, P. J. Phillips, F. Jiang, J. Ayyad, N. Penard, and\nH. Abdi, “Face recognition algorithms surpass humans matching faces\nover changes in illumination,” IEEE transactions on pattern analysis\nand machine intelligence, vol. 29, no. 9, 2007.\n[18] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, “Attribute\nand simile classiﬁers for face veriﬁcation,” in Computer Vision, 2009\nIEEE 12th International Conference on.\nIEEE, 2009, pp. 365–372.\n[19] P. J. Phillips and A. J. O’toole, “Comparison of human and computer\nperformance across face recognition experiments,” Image and Vision\nComputing, vol. 32, no. 1, pp. 74–85, 2014.\n[20] A. J. O’Toole, X. An, J. Dunlop, V. Natu, and P. J. Phillips, “Comparing\nface recognition algorithms to humans on challenging tasks,” ACM\nTransactions on Applied Perception (TAP), vol. 9, no. 4, p. 16, 2012.\n[21] L. Best-Rowden, S. Bisht, J. C. Klontz, and A. K. Jain, “Uncon-\nstrained face recognition: Establishing baseline human performance via\ncrowdsourcing,” in Biometrics (IJCB), 2014 IEEE International Joint\nConference on.\nIEEE, 2014, pp. 1–8.\n[22] E. Zhou, Z. Cao, and Q. Yin, “Naive-deep face recognition: Touching\nthe limit of lfw benchmark or not?” arXiv preprint arXiv:1501.04690,\n2015.\n[23] P. J. Phillips, M. Q. Hill, J. A. Swindle, and A. J. O’Toole, “Human\nand algorithm performance on the pasc face recognition challenge,” in\nBiometrics Theory, Applications and Systems (BTAS), 2015 IEEE 7th\nInternational Conference on.\nIEEE, 2015, pp. 1–8.\n[24] A. Blanton, K. C. Allen, T. Miller, N. D. Kalka, and A. K. Jain,\n“A comparison of human and automated face veriﬁcation accuracy on\nunconstrained image sets,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition Workshops, 2016, pp. 161–\n168.\n[25] “Iso/iec jtc 1/sc 37 n 506. biometric data interchange formats part 5:\nFace image data,” 2004.\n[26] I. 19794-5, “http://www.correlance.com/cms/en/iso19794-5.”\n[27] “Iso/iec tr 29794-5:2010 biometric sample quality – part 5: Face image\ndata (iso/iec tc jtc1/sc 37),” 2010.\n[28] Z. Yang, H. Ai, B. Wu, S. Lao, and L. Cai, “Face pose estimation and\nits application in video shot selection,” in Pattern Recognition, 2004.\nICPR 2004. Proceedings of the 17th International Conference on, vol. 1.\nIEEE, 2004, pp. 322–325.\n[29] X. Gao, S. Z. Li, R. Liu, and P. Zhang, “Standardization of face image\nsample quality,” in International Conference on Biometrics.\nSpringer,\n2007, pp. 242–251.\n[30] K. Nasrollahi and T. B. Moeslund, “Face quality assessment system in\nvideo sequences,” in European Workshop on Biometrics and Identity\nManagement.\nSpringer, 2008, pp. 10–18.\n[31] J. Sang, Z. Lei, and S. Z. Li, “Face image quality evaluation for\niso/iec standards 19794-5 and 29794-5,” in International Conference\non Biometrics.\nSpringer, 2009, pp. 229–238.\n[32] H. Sellahewa and S. A. Jassim, “Image-quality-based adaptive face\nrecognition,” IEEE Transactions on Instrumentation and measurement,\nvol. 59, no. 4, pp. 805–813, 2010.\n[33] Y. Wong, S. Chen, S. Mau, C. Sanderson, and B. C. Lovell, “Patch-based\nprobabilistic image quality assessment for face selection and improved\nvideo-based face recognition,” in Computer Vision and Pattern Recog-\nnition Workshops (CVPRW), 2011 IEEE Computer Society Conference\non.\nIEEE, 2011, pp. 74–81.\n[34] J. Long and S. Li, “Near infrared face image quality assessment\nsystem of video sequences,” in Image and Graphics (ICIG), 2011 Sixth\nInternational Conference on.\nIEEE, 2011, pp. 275–279.\n[35] X.-h. Chen and C.-z. Li, “Image quality assessment model based on\nfeatures and applications in face recognition,” in Signal Processing,\nCommunications and Computing (ICSPCC), 2011 IEEE International\nConference on.\nIEEE, 2011, pp. 1–4.\n[36] G. Zhang and Y. Wang, “Asymmetry-based quality assessment of face\nimages,” in International Symposium on Visual Computing.\nSpringer,\n2009, pp. 499–508.\n[37] S. Bharadwaj, M. Vatsa, and R. Singh, “Can holistic representations\nbe used for face biometric quality assessment?” in Image Processing\n(ICIP), 2013 20th IEEE International Conference on.\nIEEE, 2013, pp.\n2792–2796.\n[38] R. Raghavendra, K. B. Raja, B. Yang, and C. Busch, “Automatic face\nquality assessment from video using gray level co-occurrence matrix:\nAn empirical study on automatic border control system,” in Pattern\nRecognition (ICPR), 2014 22nd International Conference on.\nIEEE,\n2014, pp. 438–443.\n[39] J. Chen, Y. Deng, G. Bai, and G. Su, “Face image quality assessment\nbased on learning to rank,” IEEE signal processing letters, vol. 22, no. 1,\npp. 90–94, 2015.\n[40] X. Wu, R. He, Z. Sun, and T. Tan, “A light cnn for deep face\nrepresentation with noisy labels,” arXiv preprint arXiv:1511.02683,\n2015.\n[41] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition.”\nin BMVC, vol. 1, no. 3, 2015, p. 6.\n[42] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[43] R. M. Bolle, J. H. Connell, S. Pankanti, N. K. Ratha, and A. W.\nSenior, “The relation between the roc curve and the cmc,” in Automatic\nIdentiﬁcation Advanced Technologies, 2005. Fourth IEEE Workshop on.\nIEEE, 2005, pp. 15–20.\n[44] J. Pan, Z. Hu, Z. Su, and M.-H. Yang, “Deblurring face images with\nexemplars,” in European Conference on Computer Vision.\nSpringer,\n2014, pp. 47–62.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-07-05",
  "updated": "2023-07-05"
}