{
  "id": "http://arxiv.org/abs/2307.02900v2",
  "title": "Meta Federated Reinforcement Learning for Distributed Resource Allocation",
  "authors": [
    "Zelin Ji",
    "Zhijin Qin",
    "Xiaoming Tao"
  ],
  "abstract": "In cellular networks, resource allocation is usually performed in a\ncentralized way, which brings huge computation complexity to the base station\n(BS) and high transmission overhead. This paper explores a distributed resource\nallocation method that aims to maximize energy efficiency (EE) while ensuring\nthe quality of service (QoS) for users. Specifically, in order to address\nwireless channel conditions, we propose a robust meta federated reinforcement\nlearning (\\textit{MFRL}) framework that allows local users to optimize transmit\npower and assign channels using locally trained neural network models, so as to\noffload computational burden from the cloud server to the local users, reducing\ntransmission overhead associated with local channel state information. The BS\nperforms the meta learning procedure to initialize a general global model,\nenabling rapid adaptation to different environments with improved EE\nperformance. The federated learning technique, based on decentralized\nreinforcement learning, promotes collaboration and mutual benefits among users.\nAnalysis and numerical results demonstrate that the proposed \\textit{MFRL}\nframework accelerates the reinforcement learning process, decreases\ntransmission overhead, and offloads computation, while outperforming the\nconventional decentralized reinforcement learning algorithm in terms of\nconvergence speed and EE performance across various scenarios.",
  "text": "arXiv:2307.02900v2  [eess.SP]  9 Jul 2023\n1\nMeta Federated Reinforcement Learning for\nDistributed Resource Allocation\nZelin Ji, Graduate Student Member, IEEE, Zhijin Qin, Senior Member, IEEE, and Xiaoming Tao\nAbstract—In cellular networks, resource allocation is usually\nperformed in a centralized way, which brings huge computation\ncomplexity to the base station (BS) and high transmission\noverhead. This paper explores a distributed resource allocation\nmethod that aims to maximize energy efﬁciency (EE) while\nensuring the quality of service (QoS) for users. Speciﬁcally, in\norder to address wireless channel conditions, we propose a robust\nmeta federated reinforcement learning (MFRL) framework that\nallows local users to optimize transmit power and assign channels\nusing locally trained neural network models, so as to ofﬂoad\ncomputational burden from the cloud server to the local users,\nreducing transmission overhead associated with local channel\nstate information. The BS performs the meta learning procedure\nto initialize a general global model, enabling rapid adaptation\nto different environments with improved EE performance. The\nfederated learning technique, based on decentralized reinforce-\nment learning, promotes collaboration and mutual beneﬁts\namong users. Analysis and numerical results demonstrate that\nthe proposed MFRL framework accelerates the reinforcement\nlearning process, decreases transmission overhead, and ofﬂoads\ncomputation, while outperforming the conventional decentralized\nreinforcement learning algorithm in terms of convergence speed\nand EE performance across various scenarios.\nIndex Terms—Federated learning, meta learning, reinforce-\nment learning, resource allocation.\nI. INTRODUCTION\nThe inexorable progression of wireless networks is the\ntrend. The 3rd Generation Partnership Project (3GPP) has\nstandardized the access technique and physical channel model\nfor the ﬁfth-generation new radio (5G NR) network, which\nenables dynamic switching of user equipment (UE) between\nresource blocks (RBs) possessing varying bandwidths and\nsupports multiple subcarrier spacing [2], [3]. Building upon\nthe foundation established by 5G, the sixth generation (6G)\nand beyond networks aspire to provide the enhanced and\naugmented services of 5G NR, while transitioning toward\ndecentralized, fully autonomous, and remarkably ﬂexible user-\ncentric systems [4]. These emerging techniques impose more\nstringent requirements on decentralized resource allocation\nmethods, emphasizing the signiﬁcance of optimizing RB as-\nsignments to enhance the overall quality of service (QoS)\nwithin the systems.\nPart of this work was presented at the IEEE International Conference on\nCommunications 2022 [1].\nZelin Ji is with School of Electronic Engineering and Computer Sci-\nence, Queen Mary University of London, London E1 4NS, U.K. (email:\nz.ji@qmul.ac.uk).\nZhijin Qin and Xiaoming Tao are with Department of Electronic Engineer-\ning, Tsinghua University, Beijing, China. (email: qinzhijin@tsinghua.edu.cn;\ntaoxm@tsinghua.edu.cn).\nNevertheless, the fast variations and rapid ﬂuctuations in\nchannel conditions render conventional resource allocation\napproaches reliant on perfect channel state information (CSI)\nimpractical [5]. The inherent non-convexity of the resource\nallocation problem resulting from discrete resource block\nassociation necessitates computationally demanding solutions.\nFurthermore, the coupled variables further exacerbate the\ncomplexity of the problem. Traditionally, resource allocation\nproblems have been addressed through matching algorithms\nexecuted at the central base station (BS), resulting in sub-\nstantial computational burdens on the cloud server. All of the\naforementioned challenges require a brand-new optimization\ntool capable of effectively operating in unstable wireless\nenvironments.\nMachine learning (ML) methods, especially deep learning\n(DL) approaches, have become promising tools to address\nmathematically intractable and high-computational problems.\nHowever, artiﬁcial neural networks (NNs) require massive\namounts of training data, even for a simple binary classiﬁca-\ntion task. Moreover, the overﬁtting issue makes artiﬁcial NNs\nhard to adapt and generalize when facing new environments,\nhence requiring additional data to retrain the models and\naffecting the training data efﬁciency. Particularly, the fast\nchannel variations and the ﬂexible network structure in 5G\nbeyond network services restrict the application of conven-\ntional ML algorithms.\nTo enable fast and ﬂexible learning, meta learning has been\nproposed to enable the model to adapt to new tasks with faster\nconvergence speed by taking the input of experience from\ndifferent training tasks [6]–[8]. For instance, model-agnostic\nmeta-learning (MAML) [8] is a meta-learning technique that\ncan integrate prior experience and knowledge from the new\nenvironment, empowering the models with the ability to gen-\neralization and fast adaptation to new tasks. Another way to\nimprove data efﬁciency is to enable experience sharing among\nmodels, which is known as federated learning. By the periodic\nlocal model averaging at the cloud BS, federated learning\nenables the local users, to collectively train a global model\nusing their raw data while keeping these data locally stored\non the mobile devices [9]. In this paper, we focus on meta\nlearning enabled federated reinforcement learning, to improve\nthe performance of the reinforcement learning algorithm for\nresource allocation tasks in wireless communications.\nThrough the implementation of periodic local model aver-\naging at the cloud-based base station (BS), federated learning\nfacilitates collaborative training of a global model by enabling\nlocal users to utilize their respective raw data, which remains\nstored locally on their mobile devices [9]. This paper inves-\n2\ntigates the application of meta learning within the context of\nfederated reinforcement learning, with the aim of enhancing\nthe performance of the reinforcement learning algorithm in\nresource allocation tasks within wireless communication sys-\ntems.\nA. Related work\n1) Energy-Efﬁcient Resource Allocation for Cellular Net-\nworks: Presently, most cellular user equipment (UE) operates\non battery power, and the use of rate maximization-oriented\nalgorithms [10] may result in unnecessary energy consump-\ntion, which is unfavorable for the advancement of massive\ncapacity and connectivity in 5G and beyond communications.\nExisting literature on energy-efﬁcient resource allocation\nprimarily focuses on optimizing transmit power and channel\nassignment [11], [12]. Robat Mili et al. [11] concentrate\non maximizing energy efﬁciency (EE) for device-to-device\ncommunications. While numerous studies have investigated\nresource allocation in wireless communication systems, most\nof them rely on centralized approaches, which are considered\nas complex and not easily scalable [12]. In such centralized\napproaches, the central entity needs to obtain global channel\nstate information (CSI) to assign channels to UEs, leading\nto signiﬁcant communication overhead and latency. Conse-\nquently, distributed low-complexity algorithms are preferable\nover centralized ones.\nGame theory has been adopted for decentralized resource\nallocation [12]–[14]. However, these approaches typically as-\nsume a static radio environment and require multiple iterations\nfor UEs to converge to the Nash Equilibrium (NE) point. In\nthe practical environment, the performance of game theory\nbased algorithms is impacted by the rapid ﬂuctuations in the\nwireless channel. Yang et al. [13] and Dominic et al. [14]\nintegrate the game theory and stochastic learning algorithm\n(SLA) to enable local users to learn from past experience and\nadapt to channel variations. Nevertheless, game theory based\nalgorithms do not fully explore the advantages of collaboration\nand communication among users, potentially affecting system-\nlevel performance.\n2) Decentralized Reinforcement Algorithms in Wireless\nCommunications: A promising solution to address concerns\nregarding complexity and signaling cost concerns involves\nestablishing a decentralized framework for resource alloca-\ntion and extending the intelligent algorithms to encompass\ncooperative large-scale networks. The adoption of multi-agent\nreinforcement learning (MARL) algorithm presents an oppor-\ntunity to tackle the challenges associated with complexity and\nenhance the intelligence of local UEs. MARL algorithms rely\nsolely on real-time local information and observations, thereby\nsigniﬁcantly reducing communication overhead and latency.\nMathematically, MARL can be formulated as a Markov deci-\nsion process (MDP), where training agents observe the current\nstate of the environment at each step and determine an action\nbased on the current policy. Agents receive corresponding\nrewards that evaluate the immediate impact of the chosen\nstate-action pair. The policy updates are based on the received\nrewards and the speciﬁc state-action pair, and the environment\ntransitions to a new state subsequently. The application of\nMARL approaches in wireless communications has been\nextensive [15]–[17]. Wang et al. [16] have demonstrated that\nsuch a decentralized optimization approach can achieve near-\noptimal performance. However, local user equipment (UE)\ncannot directly access global environmental states, and UEs\nare unaware of the policies adopted by other UEs. Con-\nsequently, there is a possibility that UEs may select chan-\nnels already occupied by other UEs, leading to transmission\nfailures in the orthogonal frequency-division multiple access\n(OFDMA) based schemes.\n3) Reinforcement Algorithm for Jointly Resource Optimiza-\ntion: It is noted that the resource block association problem\nis a discrete optimization problem, which is usually solved\nby value-based methods, e.g., Q-learning, SARSA, and Deep\nQ-learning. Meanwhile, the transmit power is the continuous\nvariable, and only policy-based algorithm can deal with the\ncontinuous optimization. Hence, how to jointly optimize the\ntransmit power and channel assignment becomes a challenge.\nIn some work, the transmit power is approximated to discrete\npower levels, and the user can only transmit by these preset-\nting power levels [1], [18]. However, discrete transmit power\nwith large intervals means performance reduction.On the other\nhand, the complexity could be very high if the number of\npower levels is signiﬁcant. To address these concerns, Yuan\net al. [19] proposed a framework with a combination of value-\nbased network and policy-based network. Similarly, Hehe et\nal. [20] also proposed a combination framework with different\ncomponents to address the discrete user association problem\nand continuous power allocation problem. However, in such\nworks the different networks are trained simultaneously, which\nleads to an unstable framework and makes the NNs hard to\nbe trained and converge.\nB. Motivations and Contributions\n1) Federated Reinforcement Learning: The primary ob-\nstacle faced by MARL algorithms is the instability and\nunpredictability of actions taken by other user equipment\n(UEs), resulting in an unstable environment that affects the\nconvergence performance of MARL [21]. Consequently, a\npartially collaborative MARL structure with communication\namong UEs becomes necessary. In this structure, each agent\ncan share its reward, RL model parameters, action, and state\nwith other agents. Various collaborative RL algorithms may\nemploy different information-sharing strategies. For instance,\nsome collaborative MARL algorithms require agents to share\ntheir state and action information, while others necessitate the\nsharing of rewards. The training complexity and performance\nof a collaborative MARL algorithm are inﬂuenced by the data\nsize that each agent needs to share. This issue becomes severer\nwhen combining neural networks (NN) with reinforcement\nlearning. In a traditional centralized reinforcement algorithm,\ne.g., deep Q-network (DQN), the environment’s interactive\nexperiences and transitions are stored in the replay memory\nand utilized to train the DQN model. However, in multi-\nagent DQN, local observations fail to represent the global\nenvironment state, signiﬁcantly diminishing the effectiveness\n3\nof the replay memory. Although some solutions have been pro-\nposed to enable replay memory for MARL, these approaches\nlack scalability and fail to strike a suitable balance between\nsignaling costs and performance.\nTo address the issue of non-stationarity, it is necessary to\nensure the sharing of essential information among UEs, which\ncan be facilitated by federated learning [22]. Federated learn-\ning has demonstrated successful applications in tasks such\nas next-word prediction [23] and system-level design [24].\nSpeciﬁcally, federated reinforcement learning (FRL) enables\nUEs to individually explore the environment while collectively\ntraining a global model to beneﬁt from each other’s experi-\nences. In comparison to MARL approaches, the FRL method\nenables UEs to exchange their experiences, thereby enhancing\nconvergence performance [25]. This concept has inspired the\nwork of Zhang et al. [26] in improving WiFi multiple access\nperformance and Zhong et al. [27] in optimizing the placement\nof reconﬁgurable intelligent surfaces through the application\nof FRL.\n2) Meta Reinforcement Technique for Fast Adaptation and\nRobustness: Another main challenge of the reinforcement\nlearning algorithm is the demand for massive amounts of\ntraining data. Since the training data can only be acquired\nby interacting with the environment, the agent usually needs\na long-term learning process until it can learn from a good\npolicy. Moreover, using such a large amount of data to train an\nagent also may lead to overﬁtting and restrict the scalability of\nthe trained model. In the scope of the wireless environment,\nthe fast fading channels and unstable user distributions also\nput forward higher requirements on robustness and general-\nization ability. Particularly, the previous resource allocation\nalgorithms usually set a ﬁxed number of users, which makes\nthe algorithm lack scalability to various wireless environments\nin practical implementation.\nMeta learning is designed to optimize the model parameters\nusing less training data, such that a few gradient steps will\nproduce a rapid adaptation performance on new tasks. During\nthe meta learning training process, the model takes a little\ntraining data from different training tasks to initialize a general\nmodel, which reduces the model training steps signiﬁcantly.\nThe meta learning can be implemented in different ways.\nWang et al. [6] and Duan et al. [7] have applied recurrent\nNN and the long short-term memory to integrate the previous\nexperience into a hidden layer, and NNs have been adopted\nto learn the previous policy. Finn et al. [8] have leveraged the\nprevious trajectories to update the NNs, and further extended\nthe meta learning to reinforcement learning. In this paper, we\nconsider the meta learning for initializing the NNs for MARL.\nIn the scope of wireless communications, Yuan et al. [19]\nhave adopted the meta reinforcement learning for different\nuser distributions and conﬁrm that the meta reinforcement\nleaning is a better initialization approach and can achieve\nbetter performance in new wireless environments.\nAnother challenge caused by federated learning is the het-\nerogeneity in systems and the non-identical data distributions\nin RL may slow down or even diverge the convergence of\nthe local model. Inspired by the meta learning, Fallah et\nal. [28] have developed a combined model, in which the global\ntraining stage of the federated learning can be considered\nas the initialization of the model for meta learning, and\nthe personalized federated learning stage can be seen as\nthe adaptation stage for meta learning. Due to the similar\nmathematical expression, we can combine federated learning\nand meta learning naturally, so that training and adapting\nthe models from statistically heterogeneous local RL replay\nmemories. The aforementioned studies serve as valuable in-\nspiration for us to explore the application of meta learning and\nFRL in addressing the challenges of channel assignment and\npower optimization. By leveraging these techniques, we aim\nto distribute the computational load to local user equipment\n(UEs), reduce transmission overhead, and foster collaboration\namong UEs.\nThis paper introduces a novel framework that combines\nmeta learning and FRL for distributed solutions to the channel\nassignment and power optimization problem. To the best of\nour knowledge, this is the ﬁrst endeavor to integrate meta\nlearning and FRL in the context of resource allocation in\nwireless communications. The contributions of this paper are\nsummarized as follows:\n1) A meta federated reinforcement learning framework,\nnamed MFRL, is proposed to jointly optimize the channel\nassignment and transmit power. The optimization is per-\nformed distributed at local UEs to lower the computational\ncost at the BS and the transmission overhead.\n2) To improve the robustness of the proposed algorithm, we\nleverage the meta learning to initialize a general model,\nwhich can achieve fast adaptation to new resource allo-\ncation tasks and guarantee the robustness of the proposed\nMFRL framework.\n3) To address the joint optimization of the discrete and\ncontinuous variables, we redesign the action space for\nthe RL algorithm and design the corresponding proximal\npolicy optimization (PPO) network to optimize the real-\ntime resource allocation for each UE.\n4) To explore the collaboration among cellular users, we\npropose a global reward regarding the sum EE and the\nsuccessful allocation times for all UEs and apply the MFRL\nframework for enabling experience sharing among UEs.\nThe remainder of the paper is organized as follows. In Sec-\ntion II, the system model is presented and an EE maximization\nproblem is formulated. The meta federated reinforcement\nlearning algorithm is presented in Section III. The proposed\nMFRL framework is illustrated in Section IV. The numerical\nresults are illustrated in Section V. The conclusion is drawn\nin Section VI.\nII. SYSTEM MODEL\nIn this paper, we assume that the set of UEs is denoted as\nUE = {UE1, . . . , UEI}, where I is the total number of UEs.\nFor UEi, the binary channel assignment vector is given by\nρi = [ρi,1, . . . , ρi,n, . . . , ρi,N] , i ∈I, n ∈N, where N is the\nnumber of subchannels. The channel assignment parameter\nρi,n = 1 indicates that the n-th subchannel is allocated\nto UEi, otherwise ρi,n = 0. Each UE can only accesses\none channel, i.e., PN\nn=1 ρi,n = 1, ∀i ∈I. Meanwhile, we\n4\nconsider a system with OFDMA, which means a channel\ncan be accessed by at most one UE within a cluster, i.e.,\nPI\ni=1 ρi,n ∈{0, 1}, ∀n ∈N. In the case of each user\nequipment (UE), successful transmission with the base station\n(BS) is achieved when the UE accesses a speciﬁc subchannel\nwithout any other UEs within the same cluster accessing\nthe same subchannel. Consequently, if each UE is allocated\na channel that does not conﬂict with other UEs within the\ncluster, this allocation is considered a successful channel\nassignment.\nThe pathloss of a common urban scenario with no line of\nsight link between UEi and the BS can be denoted by [3]\nPLi,n = 32.4 + 20 log10 (fn) + 30 log10 (di,n) (dB),\n(1)\nwhere di,n represents the 3D distance between UEi and the\nBS, fn represents the carrier frequency for n-th subchannel.\nConsidering the small-scale fading, the overall channel gain\ncan be thereby denoted by\nhi,n =\n1\n10(P Li,n/10) ψmn,\n(2)\nwhere ψ is the log-normally distributed shadowing parameter.\nAccording to the aforementioned pathloss model, there is no\nline of sight between UEs and the BS, and mn represents\nthe Rayleigh fading power component of the n-th subchannel.\nHence, the corresponding signal-to-noise ratio (SNR) between\nthe BS and UEi transmitting over the n-th subchannel is\nrepresented as\nγi,n = ρi,nhi,npi\nNn\n,\n(3)\nwhere Nn = Wnσ2\nn represents the Gaussian noise power on\nthe n-th subchannel. The uplink EE for a successful channel\nassignment of UEi is given by\nui,n =\n(\nBWn\npi\nlog2 (1 + γi,n) ,\nif PN\nn=1 ρi,n = 1;\n0,\nelse.\n(4)\nwhere BWn = k×bn is the bandwidth of the n-th subchannel,\nk represents the number of subcarriers in each subchannel,\nand bn denotes the subcarriers spacing for n-th subchannel.\nMeanwhile, for the unsuccessful assignment, i.e., the UE\ncannot access any subchannel, the uplink rate is set to 0 as it\nis unacceptable for the OFDMA system.\nThe problem is formulated as\n(P0) maximize\n{ρ, p}\nI\nX\ni=0\nN\nX\nn=0\nui,n\n(5a)\nsubject to\npi ≤pmax, ∀i ∈I,\n(5b)\nγi,n > γmin, ∀i ∈I,\n(5c)\nN\nX\nn=1\nρi,n = 1, ∀i ∈I,\n(5d)\nXI\ni=1 ρi,n ∈{0, 1}, ∀n ∈N.\n(5e)\nwhere p = {p1, . . . , pI} denotes the transmit power vector\nof UEs, γmin represents the minimum SNR requirement to\nguarantee the QoS for UEs. Constraint (5d) and (5e) make the\nEE maximization problem a non-convex optimization problem\nand cannot be solved by mathematical convex optimization\ntools. In the literature, channel allocation problems are usu-\nally formed as linear sum assignment programming (LSAP)\nproblems. To solve this problem, local CSI or the UE related\ninformation, e.g., location and velocity should be uploaded to\nthe BS, then the centralized Hungarian algorithm [29] can be\ninvoked to solve the problem with computational complexity\nO\n\u0000I3\u0001\n. The computational complexity grows exponentially\nwith the number of UEs, and the mobility of UEs causes\nthe variable CSI, which means the high-complexity algorithm\nneeds to be executed frequently, leading to high transmission\noverhead and high computational pressure to the BS. More-\nover, due to the transmission latency, the current optimized\nresource allocation scheme by the BS may not be optimal for\nUEs anymore, and a distributed and low complexity resource\nallocation approach on the UE side is more than desired.\nAccording to the constraint (5d) and (5e), each UE can\nonly access one subchannel, and it is clear that the subchannel\nassignment is a discrete optimization problem. As aforemen-\ntioned concerns in Section I, it is hard to train different types\nof neural networks simultaneously. In another way, the discrete\nassignment problem can be described by different probabilities\nto choose different subchannels, and then one-dimensional\ndiscrete choice can be mapped to high-dimensional probability\ndistributions. Overall, the joint optimization problem can be\nsolved by a simple policy-based framework with a speciﬁc\noutput design.\nIII. PROPOSED META FEDERATED REINFORCEMENT\nLEARNING FOR RESOURCE ALLOCATION\nIn this section, we will ﬁrst introduce the proposed MFRL\nframework from an overall perspective. Then we will design\nthe NN structure to solve this EE maximization problem,\nand propose a meta reinforcement learning scheme for the\nNN initialization. We also demonstrate the meta-training and\nmeta-adapting algorithms in detail. Finally, we will present\nthe federated learning algorithm and procedures.\nThe proposed algorithm starts from the meta-training for\ninitializing the global generalized model at the BS. The initial\nmodel is meta-trained using the BS data set. After the initial\nglobal model is trained, it will be broadcast to the local\nUEs for adapting to the new environments. During the meta-\nadapting, i.e., the ﬁne-tuning process, the local models are\ntrained using a local database, i.e., local CSI, and the local\nmodels can be reunited as a global model so that the UEs\ncould learn the knowledge from the experiences of other UEs\nand improve the global EE. One popular way is to average the\ndistributed models and form a global model, which is called\nfederated learning [22]. After the local models are averaged\nby the BS, it would be broadcast to the local UEs which will\nﬁne-tune the global model and adapt to the local scenarios.\nThis process will be repeated until the meta-adaptation stage\nﬁnishes. The overall procedure is shown in Fig. 1\nA. Neural Network Structure Design\nAs the aforementioned description, the resource allocation\nproblem can be modeled as a multi-agent markov decision\n5\nBS \nDataset\nUE \nDataset\n3\n4\n5\nCentral \nBS\nCentral \nBS\n6\n2\n1\n3. Local model training (meta-adaptation).\n4. Local model updating.\n5. Model averaging at the central BS.\n6. Global model broadcasting.\n7. Repeat step 3-6.\n1. Meta-training for the model initialization at the BS.\n2. Pre-trained model broadcasting.\nMeta-adaptation \nand federated \nlearning process \n(step 3-7) \nMeta-training \nprocess (step 1 \nand 2)\ni\ni\nUE\n1\n1\nI\ni\ni\ni\nI\ni\ni\nh\nh\n=\n=\n= å\nå\nW\nW\nFig. 1. The proposed MFRL framework. The local models are uploaded and\naveraged periodically.\nprocess (MDP), which is mathematically expressed by a tuple,\n⟨I, O, A, R, P⟩, where I is the number of agents, N = 1\ndegenerates to a single-agent MDP, O is the combination\nset of all observation state, A = A0 × · · · × AI is the set\nof actions for each agent, R is the reward function, which\nis related to current observation Ot = {o0, . . . , oI} ∈O,\nAt = {a0, . . . , aI} ∈A, and Ot+1 ∈O. Transition prob-\nability function is deﬁned as P : O × A →P(O), with\nP(Ot+1|Ot, At) being the probability of transitioning into\nstate Ot+1 if the environment start in state Ot and take joint\naction At.\nOne of the challenges of using deep reinforcement learning\nalgorithms to solve the problem (P0) is that the resource\nallocation of the transmit power and subchannel association\nis the hybrid optimization of the continuous and discrete\nvariables. As the analysis above, the discrete subchannel asso-\nciation parameter can be described by different probabilities to\nchoose different subchannels, thus the discrete variable can be\nexpressed by probability distributions on subchannels, which\nis generated by a categorical layer. Meanwhile, continuous\npower optimization is performed by the Gaussian layer, where\nthe mean and variance of the transmit power can be trained.\nIn fact, any deep reinforcement learning algorithms with\ncontinuous action space can be applied for training the\nproposed network structure. Speciﬁcally, we apply the PPO\nalgorithm because of its ease of use and robustness, which\nmake it the default algorithm by OpenAI [30]. It is noted\nthat the NN architecture shares parameters between the policy\nand value function, so that the actor network and critic\nnetwork share the underlying features in the NN, and simplify\nthe meta learning initialization and model broadcast costs.\nThe corresponding network structure of the local models is\nillustrated in Fig. 2.\nIn this paper, we deﬁne the observation state at training step\nt for the UEs, which are considered as the agents in the MFRL\nframework, as ot,i = {{hi,n}∀n∈N, t} with dimension |oi|,\nwhere t represents the number of epoch. The variables t can be\ntreated as a low-dimensional ﬁngerprint information to contain\nthe policy of other agents [21], thus enhancing the stationary\nand the convergence performance of the MFRL algorithm.\nThe action at,i for the UEi including the subchannel and\nUE\nPPO brain\nActor\nInput state \ninformation\nFFN1\nFFN3\nGaussian \nLayer\nCatergorical\n Layer\nv Layer\nPower\nCritic\n( ,\n)\nm s\nA_loss\nC_loss\nSum_loss\nBP\nChannel\nFFN2\nSample\np\nρ\nClip\ntL\nCR\ntL\nO\nA\nR\nWireless Environment\n{ , }\nρ p\nFig. 2. The proposed PPO network structure for the MFRL framework.\nthe transmit power choice with dimension |a| = 2. The\nActor network contains a categorical layer with N neurons\nto determine which subchannel the local UE should access.\nThe continuous transmit power is optimized by a σ layer and a\nµ layer, and the power is sampled according to the probability\ndistribution N(µ, σ2).\nSince we aim to maximize the sum EE of the cellular\nnetwork, here we design a global reward rt, according to the\njoint action at such that encouraging collaboration of UEs.\nThe global reward at training step t can be deﬁned as\nrt =\n\n\n\nIP\ni=0\nri(t)\nif\nIP\ni=0\nρi,n ∈{0, 1}, ∀i ∈I, ∀n ∈N;\nIsuc−I\nI\n,\nOtherwise,\n(6)\nwhere Isuc denotes the number of UEs that satisﬁes the sub-\nchannel assignment constraints, i.e., PI\ni=0 ρi,n ∈{0, 1}, ∀n ∈\nN. For the assignment that fails to meet the subchannel\naccess requirements, a punishment is set to proportional to the\nnumber of failure UEs. Meanwhile, the reward for a successful\nsubchannel assignment is expressed by\nri(t) =\n(\nξui,n(t),\nif γi,n > γmin;\nξupmax\ni,n\n(t),\nOtherwise,\n(7)\nwhere ξ is a constant coefﬁcient, upmax\ni,n\n(t) denotes the EE by\nthe maximum transmit power, which means if the UE fails\nto meet the SNR constraint, it need to use the maximum\ntransmit power to avoid transmission failure. The success rate\nof UEi can be deﬁned as ηi = βi/T , where βi represents\nthe successful resource assignment counts for UEi, and T\nrepresents the number of resource allocation counts since the\ninitialization the system.1\nThe objective of the proposed MFRL framework is to enable\nUEs to learn a strategy that maximizes the discount reward,\nwhich can be expressed by\nR(τ) =\n∞\nX\nt=0\nξtrt,\n(8)\n1Please note that reward is designed as a sum of EE and the punishment,\nwhich makes it a dimensionless parameter and we only need to focus on the\nvalue of it.\n6\nwhere τ = (o0, a0, ..., oT +1) is a trajectory, T is the current\ntimestamp, ξ ∈(0, 1) represents the discount rate, which\ndenotes the impact of the future reward to the current action.\nB. Policy Gradient in Meta-training\nIn the previous work [1], [15], [16], the number of UEs in\neach cluster is ﬁxed, and the training and testing performance\nare implemented in the same environment. Particularly, the\nlocal model is trained by each UE individually for the MFRL\nalgorithm, which limits its application, making it hard to\nadapt to more complicated practical scenarios. The resource\nallocation model should have the ability to adapt and gener-\nalize to different wireless communication environments with\ndifferent cluster sizes. Hence, the meta reinforcement learning\nalgorithm can be considered to meet the requirement of the\ngeneralization.\nThe meta learning can be implemented in different ways,\nand in this paper we apply the MAML method for rein-\nforcement learning [8]. The meta-training process takes the\nexperience from different tasks, i.e., the resource allocation\nfor different cluster sizes, to initialize a model which can\nbe adopted by UEs in different scenarios and achieve fast\nadaptation. To take the number of UEs into account, the\nlocal observation should include the total number of UEs, i.e.,\not,i = {{hi,n}∀n∈N, I, t}. The task set of resource allocation\nfor UEs is deﬁned as T = {T Ik}, ∀k ∈K, where K is the\nnumber of tasks, Ik is the number of UEs for task k. The\nmeta-training process is implemented at the BS, which can\nuse the previous resource allocation experience for different\nnumber of UEs to meta-train an initial model.\nAt the end of each training epoch, the BS stores the\ntransitions ek\nt,i = {(ok\nt,i, ak\nt,i, rk\nt , ok\nt+1,i)|i = 0, 1, . . ., Ik −1}\nacquired from T Ik in the central dataset. The transitions\net,i = (ot,i, at,i, rt, ot+1,i) are sampled from B for calcu-\nlating the advantage function and the estimated state value\nfunction, which are introduced in the following paragraphs.\nThe objective function for training the reinforcement model\nis to maximize the expected reward for each trajectory as\nJ (πθ) = Eτ∼πθ(τ) [R(τ)] =\nZ\nτ\nP(τ|πθ)R(τ),\n(9)\nwhere\nπθ\nis\nthe\nparameterized\npolicy,\nP(τ|πθ)\n=\nP(o0) QT −1\nt=0 P(ot+1,i|ot,i, at,i)πθ(at,i|ot,i)\nrepresents\nthe probability of the trajectory τ, P(ot+1,i|ot,i, at,i) is the\nstate transformation probability, πθ(at,i|ot,i) is the action\nchoice probability, and P(o0) is the probability of the initial\nstate o0. To optimize the policy, the policy gradient needs to\nbe calculated, i.e., θj+1 = θj + α ∇θJ(πθ)|θj, where α is\nthe learning rate or the learning step.\nThe gradient of the policy can be expressed by a general\nform as\n∇θJ(πθ) = Eτ∼πθ(τ)\n\" T\nX\nt=0\n∇θ log πθ(at,i|ot,i)Φt,i\n#\n,\n(10)\nwhere Φt,i could be denoted as the action-value function\nQπθ(o, a) = Eτ∼πθ(τ) [R(τ)|o0 = o, a0 = a], which is the\nexpectation reward for taking action a at state o. Although\nwe can use the action-value function to evaluate the action is\ngood or bad, the action-value function Qπθ(o, a) relies on the\nstate and the action, which means an optimal policy under a\nbad state may have less action-value than an arbitrary action\nunder a better state. To address this issue, we need to eliminate\nthe inﬂuence caused by the state. First, we prove that the state\ninﬂuence elimination will not affect the value of the policy\ngradient [31].\nLemma 1 (Expected Grad-Log-Prob Lemma). Given P πθ is a\nparameterized probability distribution over a random variable\no, then Eo∼P πθ [∇θ log P πθ(o)] = 0.\nProof. For all probability distributions, we have\nZ\no\nP πθ(o) = 1.\n(11)\nTake the gradient of both side\n∇θ\nZ\no\nP πθ(o) = ∇θ1 = 0.\n(12)\nThus\nEo∼P πθ [∇θ log P πθ(o)]\n=\nZ\no\nP πθ(o)∇θ log P πθ(o)\n=\nZ\no\n∇θP πθ(o)\n= ∇θ\nZ\no\nP πθ(o)\n= 0.\nAccording\nto\nLemma\n1,\nwe\ncan\nderive\nthat\nfor\nany\nfunction\nb(ot)\nthat\nonly\ndepends\non\nthe\nstate,\nEa∼πθ [∇θ log πθ(a|o)b(o)] = 0. Hence, it would cause the\nsame expected value of the policy gradient ∇θJ(πθ) if we\nsubstitute the b(o) into the action-value function Qπθ(o, a).\nIn fact, we can use the state-value function V πθ(o) which\nrepresents whether the state is good for a higher reward or\nnot. Instead of comparing the action-value function Qπθ(o, a)\nof the action a directly, it is more reasonable to substitute\nthe inﬂuence of the state into the action-value function. We\ndeﬁne the substitution Aπθ(o, a) = Qπθ(o, a)−V πθ(o) as the\nadvantage function, which represents whether an action good\nor bad compared with other actions relative to the current\npolicy. Hence, the value function Φt,i can be also denoted as\nΦt,i = Qπθ(ot,i, at,i) −V πθ(ot,i) = Aπθ(ot,i, at,i).\n(13)\nC. Advantage Estimation and Loss Function Design\nAlthough we express the policy gradient by introducing the\nadvantage function, the challenge is, the action-value func-\ntion and the state-value function cannot be acquired directly\nfrom the experience et,i. Instead, the action-value function\ncan be expressed by the temporal difference form [32] as\nQπθ(ot,i, at,i) = rt + ξV πθ(ot+1,i). In deep reinforcement\nlearning approaches, NNs can be used to estimate the state-\nvalue function as ˆV πθ, then the estimated advantage function\n7\nˆAπθ(ot,i, at,i) = δV\nt,i = rt + ξ ˆV πθ(ot+1,i) −ˆV πθ(ot,i) can\nbe derived. However, the bias for this estimation is high,\nwhich restricts the training and convergence performance.\nTo overcome this issue, generalized advantage estimation\n(GAE) [31] can be applied to estimate the advantage function\nfor multi-steps and strike a tradeoff between the bias and\nvariance. The GAE advantage function is denoted by\nAGAE(ot,i, at,i) =\nT −t\nX\nl=0\n(λξ)lδV\nt+l,i,\n(14)\nwhere λ ∈(0, 1] is the discount factor for reducing the\nvariance of the future advantage estimation.\nThe actor network is optimized by maximising LAC =\nEτ∼πθ(τ)\n\u0002\nratiot,i × AGAE(ot,i, at,i)\n\u0003\n,\nwhere\nratiot,i\n=\nπθ(at,i|ot,i)\nπθold(at,i|ot,i) is the action step. However, too large action step\ncould lead to an excessively large policy update, hence we\ncan clip this step and restrict it. The clipped actor objective\nfunction is expressed by\nLClip\nt\n= min\n\u0000ratiot,i × AGAE(ot,i, at,i), g(ǫ, AGAE(ot,i, at,i))\n\u0001\n,\n(15)\nwhere\ng(ǫ, A) =\n(\n(1 + ǫ)A,\nA ≥0;\n(1 −ǫ)A\nA < 0,\n(16)\nin which the ǫ is a constant value representing the clip\nrange. The clip operation have been proved to improve the\nrobustness [30].\nThe loss LCR for the critic network is to minimize the gap\nbetween the estimated state-value function and discount sum\nreward, which can be expressed by\nLCR\nt\n=\n\r\r\rrt + ˆV πθ(ot+1,i) −ˆV πθ(ot,i)\n\r\r\r\n2\n.\n(17)\nCombining the objective of the actor network and critic\nnetwork, we can express the overall objective as\nL = arg min\nθ\nEt\nh\nLClip\nt\n−c1LCR\nt\n+ c2Et\ni\n,\n(18)\nwhere Et represents an entropy bonus to ensure sufﬁcient\nexploration, θ is the weights for the PPO network, c1 and c2\nare weight parameters for the estimation of value function and\nentropy, respectively. Then the initial model will be updated\nby the stochastic gradient (SG) ascent approach. The details\nof the meta-training algorithm is shown in Algorithm 1.\nD. Meta-Adapting Process\nUnlike the meta-training process where the BS stores the\ntransitions and uses these experiences to train a global model,\nthe local UE can train its own model based on its own ob-\nservations and experience during the meta-adaptation process.\nCompared with supervised learning which requires sufﬁcient\ndata set and pre-knowledge of the system, the proposed MFRL\nframework can train the local model with the local CSI data\nwhich is required by interacting with the environment, thus\nnot only ofﬂoading the computational pressure to the UEs,\nbut also lower the transmission overhead signiﬁcantly.\nAs the local models are inherited from the global model, the\nnetwork structure, the observation state space, the action, and\nAlgorithm 1 Meta-training algorithm.\n1: Input: The task set T = {T Ik}, ∀k ∈K, BS memory\nM, BS batch B;\n2: Initialize the PPO network θ;\n3: for each epoch t do\n4:\nfor each meta task k do\n5:\nThe\nBS\nacquire\nthe\nexperience\nek\nt,i\n=\n{(ok\nt,i, ak\nt,i, rk\nt , ok\nt+1,i)|i = 0, 1, . . . , Ik −1} from all\nUEs and store the transitions in central dataset M;\n6:\nend for\n7:\nSample the transitions in the BS batch B;\n8:\nUpdate the global PPO network by SG ascent with\nAdam: θ ←θ + αmeta∇θL;\n9: end for\n10: Return: Pre-trained global model θ.\nAlgorithm 2 Meta-adapting algorithm.\n1: Input: The pre-trained global model θ, number of UEs I,\nlocal memory Mi and batch Bi for each UE;\n2: Initialize the local models θ0,i ←θ, ∀i ∈I;\n3: for each epoch j do\n4:\nfor each D2D pair i do\n5:\nCollect set of trajectories Mi by running policy\nπj,i = π(θj,i) in the environment;\n6:\nCompute\nadvantage\nestimations\nAGAE(oj,i, aj,i)\nbased on current state-value function ˆV πθ(o) and\nreward rj;\n7:\nUpdate the PPO network by maximizing the objec-\ntive function:\nθj+1,i = arg max\nθi\n1\nT\nTP\nj=0\n\u0010\nLClip\nj\n−c1LCR\nj\n+ c2Ej\n\u0011\n;\n8:\nend for\n9: end for\nthe reward are deﬁned the same as Section III. Considering\nthat the i-th UE interacts with the environment at adapting\nepoch j, i.e., observes the state oj,i, and takes action according\nto current policy π(θj,i). Then the i-th UE receives the reward\nrj and observes the next state oj+1,i. The transition ej,i =\n(oj,i, aj,i, rj, oj+1,i) is stored in its local memory Mi which\ncan be sampled in the batch to train the local models. The\nadvantage is estimated using the GAE method and the loss\nfunction is the same as the meta-training process. The details\nof the meta-adapting process are described in Algorithm 2.\nE. Global Averaging of Local Models\nUnlike the meta-training process that the BS uses the\ncentralized replay memory that collects from all UEs to\ntrain the global model, the local UEs can only access their\nlocal memories during the meta-adaptation process, which\naffects the robustness of the local models when encountering\nunfamiliar scenarios. To enable the individual models at each\nUE can be beneﬁted from other UEs, the federated learning\ntechnique can be applied.\nThe local model is averaged to a global model, then the\nglobal model is broadcast to UEs and the UEs will continue to\n8\ntrain the new global model locally. By averaging the models,\neach UE is able to beneﬁt from the experience of other UEs,\nsince the weights direct correspond to the experience and\nmemory. Mathematically, the model averaging process at the\ncentral BS can be denoted as\nW =\nPI\ni=1 |Bi|W i\nPI\ni=1 |Bi|\n,\n(19)\nwhere |Bi| represents the number of number of elements in\nBi. The average algorithm shows that the averaged model will\nlearn more from the model with more training cases. However,\nin the proposed MFRL framework, we assume that UEs share\nthe team stage reward, which means the replay memory of\neach UE has an equivalent size. To ensure that the averaged\nmodel can beneﬁt from the model that caters to the needs of\nQoS, we further revised the averaging algorithm that considers\nthe success rate, which is denoted by\nˆ\nW =\nPI\ni=1 ηiW i\nPI\ni=1 ηi\n,\n(20)\nwhere ηi is the resource allocation success rate for UEi as\ndeﬁned in Section II.\nIV. NUMERICAL RESULTS\nWe consider a communication scenario underlying a single\ncellular network. For the meta-training process, we adopt the\nurban micro (street canyon) scenario in [3]. For the meta-\nadaptation process, the pre-trained models are trained and ﬁne-\ntuned in the indoor scenario, the urban macro scenario, and\nthe rural macro scenario. For all of the scenarios, the BS is\nﬁxed at the center of the considered square. We also adopt\nthe simulation assumptions in [3] to model the channels. To\nenable the mobility of UEs, we assume that the UEs can move\nwith the speed from 0 meters per second (m/s) to 1 m/s within\nthe square. Each subcarrier has ∆f = 2ψ · 15 kHz spacing,\nwhere ψ denotes an integer. A resource block usually consists\nof 12 consecutive subcarriers [2], hence we set the bandwidth\nset of the subchannels as [0.18, 0.18, 0.36, 0.36, 0.36, 0.72,\n0.72, 0.72, 1.44, 1.44] MHz. The rest of the parameters of the\nproposed simulation environment are listed in Table IV.\nThe network structure of local models is shown in Fig. 2.\nThe state information is fed in two fully connected feed-\nforward hidden layers, which contain 512 and 256 neurons\nrespectively. Then the PPO network diverges to actor networks\nand critic networks. The actor branch contains two layers for\nchannel choice and power optimization independently, while\nthe critic branch includes an additional hidden layer with 128\nneurons, following which is the value layer for estimating\nthe advantage function for the output of the actor network.\nThe meta-training rate for different number of users is 5e−7,\nwhile the learning rate for meta adaptation is 1e−6. The meta\nlearning rate is set relatively small to avoid the overﬁtting of\nthe meta model for some speciﬁc tasks. The weight for the\nloss of the value function c1 and entropy c2 are set as 0.5\nand 0.01, respectively. The sample batch size is 256, and the\ndiscount rate for the future reward ξ is set to 0.9. The discount\nTABLE I\nENVIRONMENT PARAMETERS\nParameter\nValue\nAntenna gain of the BS\n8dB\nAntenna gain of the UEs\n3dB\nNoise ﬁgure at the BS\n5dB\nNoise ﬁgure at the UEs\n9dB\nNumber of UEs I\n6\nNumber of UEs for different tasks in meta learning\n[2, 4, 8]\nNumber of subchannels N\n10\nHeight of antenna of the UEs\n1.5m\nNumber of subcarriers in a RB K\n12\nCarrier frequency fn, ∀n ∈N\n6GHz\nCellular transmit power range\n[0, 24]dBm\nMinimum SINR requirements for BS γC\nmin\n5 dB\nNoise power spectral density of indoor scenario\n-160 dBm/Hz\nNoise power spectral density of urban micro scenario\n-170 dBm/Hz\nNoise power spectral density of urban macro scenario\n-180 dBm/Hz\nNoise power spectral density of rural macro scenario\n-185 dBm/Hz\nShadowing distribution\nLog-normal\nPathloss and shadowing update\nEvery 100ms\nFast fading update\nEvery 1ms\n0\n20\n40\n60\n80\n100\nMeta\nepisodes\n0\n1\n2\n3\n4\n5\n6\n7\n8\nSum Reward for T\nasks\nSum Reward\nFig. 3.\nmeta-training reward over the meta-training episodes. The curve\nrepresents the sum reward the agent gets from different tasks.\nfactor for the advantage function λ = 0.98 in Eq. (11) is set\naccording to [30].\nTo verify the performance of the proposed MFRL frame-\nwork with the following benchmarks:\n1) MRL: Meta reinforcement learning benchmark. The lo-\ncal models are pre-trained and inherited from the global\nmodel, but the local models are not averaged by federated\nlearning.\n2) FRL: Federated reinforcement leanring benchmark. The\nlocal models are trained from the random initialization and\naveraged by the federated learning every 100 episodes.\n3) MFRL early: The early model of the proposed MFRL\nframework. The models are stored at half of the meta-\nadaptation period, i.e., store the local models at 500\nepisodes to evaluate the fast-adaptation performance of the\nproposed framework at the early stage.\n4) MARL: The multi-agent reinforcement learning bench-\n9\n0\n200\n400\n600\n800\n1000\nEpisodes\n0\n5\n10\n15\n20\n25\nT\nraining Reward in indoor scenario\nMFRL\nMRL\nFRL\n(a) Indoor scenario.\n0\n200\n400\n600\n800\n1000\nEpisodes\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nT\nraining Reward in urban scenario\nMFRL\nMRL\nFRL\n(b) Urban macro scenario.\n0\n200\n400\n600\n800\n1000\nEpisodes\n0\n2\n4\n6\n8\nT\nraining Reward in rural scenario\nMFRL\nMRL\nFRL\n(c) Rural macro scenario.\nFig. 4. Training performance comparison of the proposed algorithm and benchmarks in three different scenarios.\nloc 1\nloc 2\nloc 3\nloc 4\nloc 5\nloc 6\nloc 7\nloc 8\nloc 9\nloc 10\nDifferent\nuser\ndistributions\n0\n10\n20\n30\n40\n50\nT\nesting Energy Efficiency in indoor scenario (Mbits/J)\nMFRL\nMFRL_early\nMRL\nFRL\nMARL\n(a) Indoor scenario.\nloc 1\nloc 2\nloc 3\nloc 4\nloc 5\nloc 6\nloc 7\nloc 8\nloc 9\nloc 10\nDifferent\nuser\ndistributions\n0\n5\n10\n15\n20\n25\n30\nT\nesting Energy Efficiency in urban scenario (Mbits/J)\nMFRL\nMFRL_early\nMRL\nFRL\nMARL\n(b) Urban macro scenario.\nloc 1\nloc 2\nloc 3\nloc 4\nloc 5\nloc 6\nloc 7\nloc 8\nloc 9\nloc 10\nDifferent\nuser\ndistributions\n0\n5\n10\n15\n20\n25\nT\nesting Energy Efficiency in rural scenario (Mbits/J)\nMFRL\nMFRL_early\nMRL\nFRL\nMARL\n(c) Rural macro scenario.\nFig. 5. Testing snapshots of the proposed algorithm and benchmarks in three different scenarios.\nmark [15]. The local models are trained from random\ninitialization and are not averaged by the federated learning\ntechnique. Each UE learns the policy according to the local\nobservations and receives the global reward, but cannot\ncommunicate the model with the centralized cloud or other\nUEs.\nFig. 3 demonstrates the reward for different tasks (with\ndifferent amounts of users) during the meta-training process.\nParticularly, the meta reward is the sum of the reward of the\nresource allocation tasks for 2, 4, and 8 UEs in the urban\nmicro scenario. The increase in the meta reward demonstrates\nthe effectiveness of the meta-training. It is also noted that\nwith the meta-training step increasing over 100 episodes, the\nsum reward keeps stable. This is because the meta-training\nprocess is to train a global and generalized model which\ncan be adapted to different tasks, but the performance of the\ngeneralized model itself cannot be as well as the models for\nthe speciﬁc tasks.\nFig. 4 shows the training reward comparison over different\nepisodes of meta-training, from which we can see that the\nmeta-training could lead to faster convergence and higher\nrewards. Due to the punishment, the reward for all schemes\nis low at the beginning of the training period. With the\nexecution of the training progress, the proposed algorithms\nwith meta learning can achieve faster convergence and higher\ntraining reward, while the conventional benchmark needs more\niterations to ﬁnd the appropriate actions to converge. The\nimproved training performance veriﬁes the fast adaptation by\n0\n200\n400\n600\n800\n1000\nEpisodes\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nPolicy Entropy\nFRL\nMFRL\nFig. 6. Policy entropy of the MFRL and FRL schemes in the indoor scenario.\nthe meta learning is robust to different scenarios.\nTo further verify the robustness of the trained local models,\nwe set different simulation settings under each scenario. At\neach random testing user distribution, the system EE is aver-\naged by 100 testing steps with fast-fading channel updates.\nFig. 5 illustrates the testing performance for 10 random\nuser distributions. The proposed algorithm outperforms other\nreinforcement learning benchmarks in terms of average system\nEE. We also store the local models at 500 episodes to test\n10\nIndoor\nStreet Canyon\nUrban Macro\nRural Macro\nDifferent\nscenarios\n0\n5\n10\n15\n20\n25\n30\n35\n40\nT\nesting Energy Efficiency (Mbits/J)\nMFRL\nMFRL_early\nFRL\nMARL\nFig. 7. Testing averaged EE performance of 100 random user distributions\nover the number of model averaging times.\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber\nof\nUEs\n0\n10\n20\n30\n40\n50\nT\nesting Energy Efficiency (Mbits/J)\nMFRL\nMRL\nFRL\nFig. 8. Testing energy efﬁciency over the different number of users.\nthe performance of the algorithms at the early training stage.\nAs expected, the proposed MFRL framework outperforms the\nMRL and FRL algorithms. Moreover, even if MFRL early\nmodels are only trained half of the whole training period, they\nstill provide good performances compared with the models\nthat are not pre-trained, which veriﬁes the fast adaptation\nascendancy of the meta learning.\nTo evaluate the convergence speed and the stability of\nthe policy, and verify the fast adaptation performance of the\nproposed MFRL framework, we use the policy entropy as\nthe measure. The policy entropy is an dimensionless index\nin policy gradient based reinforcement learning algorithms, to\nmeasure the randomness of a policy. As shown in Fig. 6, the\nlower entropy of the MFRL algorithm veriﬁes that meta learn-\ning can speed up the training process and achieve convergence\nearlier. The MFRL framework also achieves a similar lower\nentropy and faster convergence compared with the benchmarks\nin other scenarios, and the results are omitted due to space\nlimitations.\nFig. 7 concludes the sum EE in different scenarios. The\nresults are averaged according to 100 random user distribu-\ntions. It is clear that the proposed MFRL framework achieves\nthe highest sum EE in all of the scenarios, which veriﬁes the\nrobustness of the proposed scheme. Additionally, although the\nmodels for the MFRL early benchmarks are trained half of\nthe whole adapting period, it still achieves better performance\ncompared with the FRL and MARL models. The MFRL\nframework and the FRL scheme enable the UEs to corporate\nwith each others and beneﬁt the local models, hence also\nimproving the overall system EE.\nFig. 8 shows the testing sum EE of the system over a\ndifferent number of users. Note that for different users, the\ntraining parameters may differ slightly for the best perfor-\nmance. It is obvious that as the number of UEs increases,\nmore subchannels can be accessed and the sum system EE can\nbe improved. However, the improvement slows down as the\nnumber of UEs increases, since the bandwidth of subchannels\nin the proposed scenario is not equal, and when the number\nof UEs is less than the subchannels, it would access the\nsubchannel with larger bandwidth for higher EE.\nV. CONCLUSION\nIn this paper, a distributed energy-efﬁcient resource allo-\ncation scheme was developed. The system energy efﬁciency\nwas maximized by jointly optimizing the channel assignment\nand the transmit power of user equipments. The formulated\nnon-convex problem was solved by the proposed robust meta\nfederated reinforcement learning framework to overcome the\nchallenge of the computational complexity at the base station\nand the transmission cost by the local data. Quantity analysis\nand numerical results showed that the meta training model\nhas good generalization ability under different scenarios,\neven if the scenarios and tasks are different. Meanwhile, the\ncombination of federated learning and meta learning with\nreinforcement learning enables the decentralized algorithm a\nbetter performance on convergence and robustness.\nREFERENCES\n[1] Z. Ji and Z. Qin, “Federated learning for distributed energy-efﬁcient\nresource allocation,” in Proc. IEEE Int. Conf. on Commun., Seoul,\nRepublic of Korea, May 2022, pp. 1–6.\n[2] Technical Speciﬁcation Group Radio Access Network; NR; Physical\nchannels and modulation; (Release 16), document 3GPP TS 38.211\nV16.6.0, 3rd Generation Partnership Project, Jun. 2021.\n[3] Technical Speciﬁcation Group Radio Access Network; Study on channel\nmodel for frequencies from 0.5 to 100 GHz (Release 16), 3GPP TR\n38.901 V16.1.0, 3rd Generation Partnership Project, Dec. 2019.\n[4] M. Rasti, S. K. Taskou, H. Tabassum, and E. Hossain, “Evolution toward\n6g multi-band wireless networks: A resource management perspective,”\nIEEE Wireless Commun., vol. 29, no. 4, pp. 118–125, Aug. 2022.\n[5] L. Liang, G. Y. Li, and W. Xu, “Resource allocation for D2D-enabled\nvehicular communications,” IEEE Trans. on Commun., vol. 65, no. 7,\npp. 3186–3197, Apr. 2017.\n[6] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo,\nR. Munos, C. Blundell, D. Kumaran, and M. Botvinick, “Learning to\nreinforcement learn,” arXiv preprint arXiv: 1611.05763, Jan. 2016.\n[7] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and\nP. Abbeel, “Rl2: Fast reinforcement learning via slow reinforcement\nlearning,” arXiv preprint arXiv: 1611.02779, Nov. 2016.\n[8] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for\nfast adaptation of deep networks,” arXiv preprint arXiv: 1703.03400,\nMar. 2017.\n11\n[9] J. Kang, Z. Xiong, D. Niyato, Y. Zou, Y. Zhang, and M. Guizani, “Reli-\nable federated learning for mobile networks,” IEEE Wireless Commun.,\nvol. 27, no. 2, pp. 72–80, Apr. 2020.\n[10] G. Song and Y. Li, “Utility-based resource allocation and scheduling\nin OFDM-based wireless broadband networks,” IEEE Commun. Mag.,\nvol. 43, no. 12, pp. 127–134, Dec. 2005.\n[11] M. Robat Mili, P. Tehrani, and M. Bennis, “Energy-efﬁcient power allo-\ncation in OFDMA D2D communication by multiobjective optimization,”\nIEEE Wireless. Commun. Lett., vol. 5, no. 6, pp. 668–671, Dec. 2016.\n[12] F. Meshkati, H. V. Poor, and S. C. Schwartz, “Energy-efﬁcient resource\nallocation in wireless networks,” IEEE Signal Processing Mag., vol. 24,\nno. 3, pp. 58–68, May 2007.\n[13] L. Yang, D. Wu, C. Yue, Y. Zhang, and Y. Wu, “Pricing-based channel\nselection for D2D content sharing in dynamic environments,” IEEE\nTrans. Wireless Commun., vol. 20, no. 4, pp. 2175–2189, Dec. 2021.\n[14] S. Dominic and L. Jacob, “Distributed resource allocation for D2D\ncommunications underlaying cellular networks in time-varying environ-\nment,” IEEE Commun. Lett., vol. 22, no. 2, pp. 388–391, Nov. 2018.\n[15] L. Liang, H. Ye, and G. Y. Li, “Spectrum sharing in vehicular networks\nbased on multi-agent reinforcement learning,” IEEE J. Sel. Areas\nCommun., vol. 37, no. 10, pp. 2282–2292, Aug. 2019.\n[16] L. Wang, H. Ye, L. Liang, and G. Y. Li, “Learn to compress CSI\nand allocate resources in vehicular networks,” IEEE Trans. Commun.,\nvol. 68, no. 6, pp. 3640–3653, Mar. 2020.\n[17] Z. Ji, Z. Qin, and C. G. Parini, “Reconﬁgurable intelligent surface aided\ncellular networks with device-to-device users,” IEEE Trans. Commun.,\nvol. 70, no. 3, pp. 1808–1819, Jan. 2022.\n[18] H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep reinforcement learning\nbased resource allocation for V2V communications,” IEEE Trans. Veh.\nTechnol., vol. 68, no. 4, pp. 3163–3173, Feb. 2019.\n[19] Y. Yuan, G. Zheng, K.-K. Wong, and K. B. Letaief, “Meta-reinforcement\nlearning based resource allocation for dynamic V2X communications,”\nIEEE Trans. Veh. Technol., vol. 70, no. 9, pp. 8964–8977, Jul. 2021.\n[20] M. He, Y. Li, X. Wang, and Z. Liu, “NOMA resource allocation method\nin IoV based on prioritized DQN-DDPG network,” EURASIP Journal\non Advances in Signal Processing, vol. 2021, no. 1, pp. 1–17, Dec.\n2021.\n[21] J. Foerster et al., “Stabilising experience replay for deep multi-agent\nreinforcement learning,” in Proc. Int. Conf. Mach. Learn., Jul. 2017,\npp. 1146–1155.\n[22] Z. Qin, G. Ye Li, and H. Ye, “Federated learning and wireless commu-\nnications,” IEEE Wireless Commun., pp. 1–7, Sep. 2021.\n[23] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen-\nstein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for\nmobile keyboard prediction,” arXiv preprint arXiv: 1811.03604, Feb.\n2019.\n[24] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,\nV. Ivanov, C. Kiddon, J. Koneˇcn´y, S. Mazzocchi, H. B. McMahan, T. V.\nOverveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards federated\nlearning at scale: System design,” arXiv preprint arXiv: 1902.01046,\nMar. 2019.\n[25] H. H. Zhuo, W. Feng, Y. Lin, Q. Xu, and Q. Yang, “Federated deep\nreinforcement learning,” arXiv preprint arXiv: 1901.08277, Feb. 2020.\n[26] L. Zhang, H. Yin, Z. Zhou, S. Roy, and Y. Sun, “Enhancing WiFi mul-\ntiple access performance with federated deep reinforcement learning,”\nin Proc. IEEE Veh. Technol. Conf., Nov. 2020, pp. 1–6.\n[27] R. Zhong, X. Liu, Y. Liu, Y. Chen, and Z. Han, “Mobile reconﬁg-\nurable intelligent surfaces for NOMA networks: Federated learning\napproaches,” arXiv preprint arXiv:2105.09462, Mar. 2021.\n[28] A. Fallah, A. Mokhtari, and A. Ozdaglar, “Personalized federated\nlearning with theoretical guarantees: A model-agnostic meta-learning\napproach,” in Proc. Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,\nvol. 33.\nCurran Associates, Inc., Dec. 2020, pp. 3557–3568.\n[29] H. W. Kuhn, “The hungarian method for the assignment problem,”\nNaval Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83–97, Mar.\n1955.\n[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv: 1707.06347,\nJul. 2017.\n[31] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-\ndimensional continuous control using generalized advantage estima-\ntion,” arXiv preprint arXiv: 1506.02438, Jun. 2015.\n[32] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT Press, 2018.\n",
  "categories": [
    "eess.SP",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2023-07-06",
  "updated": "2023-07-09"
}