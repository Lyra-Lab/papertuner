{
  "id": "http://arxiv.org/abs/2307.02991v1",
  "title": "ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation",
  "authors": [
    "Abhijeet Pendyala",
    "Justin Dettmer",
    "Tobias Glasmachers",
    "Asma Atamna"
  ],
  "abstract": "We present ContainerGym, a benchmark for reinforcement learning inspired by a\nreal-world industrial resource allocation task. The proposed benchmark encodes\na range of challenges commonly encountered in real-world sequential decision\nmaking problems, such as uncertainty. It can be configured to instantiate\nproblems of varying degrees of difficulty, e.g., in terms of variable\ndimensionality. Our benchmark differs from other reinforcement learning\nbenchmarks, including the ones aiming to encode real-world difficulties, in\nthat it is directly derived from a real-world industrial problem, which\nunderwent minimal simplification and streamlining. It is sufficiently versatile\nto evaluate reinforcement learning algorithms on any real-world problem that\nfits our resource allocation framework. We provide results of standard baseline\nmethods. Going beyond the usual training reward curves, our results and the\nstatistical tools used to interpret them allow to highlight interesting\nlimitations of well-known deep reinforcement learning algorithms, namely PPO,\nTRPO and DQN.",
  "text": "ContainerGym: A Real-World Reinforcement\nLearning Benchmark for Resource Allocation\nAbhijeet Pendyala1, Justin Dettmer1, Tobias Glasmachers1, and\nAsma Atamna (#)1\n1Ruhr-University Bochum, Bochum, Germany\nfirstname.lastname@ini.rub.de\nAbstract\nWe present ContainerGym, a benchmark for reinforcement learning\ninspired by a real-world industrial resource allocation task. The pro-\nposed benchmark encodes a range of challenges commonly encountered\nin real-world sequential decision making problems, such as uncertainty.\nIt can be configured to instantiate problems of varying degrees of diffi-\nculty, e.g., in terms of variable dimensionality. Our benchmark differs\nfrom other reinforcement learning benchmarks, including the ones aim-\ning to encode real-world difficulties, in that it is directly derived from a\nreal-world industrial problem, which underwent minimal simplification\nand streamlining. It is sufficiently versatile to evaluate reinforcement\nlearning algorithms on any real-world problem that fits our resource\nallocation framework. We provide results of standard baseline meth-\nods. Going beyond the usual training reward curves, our results and\nthe statistical tools used to interpret them allow to highlight interest-\ning limitations of well-known deep reinforcement learning algorithms,\nnamely PPO, TRPO and DQN.\nKeywords— Deep reinforcement learning, Real-world benchmark,\nResource allocation.\n1\nIntroduction\nSupervised learning has long made its way into many industries, but indus-\ntrial applications of (deep) reinforcement learning (RL) are significantly rare.\nThis may be for many reasons, like the focus on impactful RL success stories\nin the area of games, a lower degree of technology readiness, and a lack of\nindustrial RL benchmark problems.\nA RL agent learns by taking sequential actions in its environment, observ-\ning the state of the environment, and receiving rewards [15]. Reinforcement\nlearning aims to fulfill the enticing promise of training a smart agent that\n1\narXiv:2307.02991v1  [cs.LG]  6 Jul 2023\nsolves a complex task through trial-and-error interactions with the environ-\nment, without specifying how the goal will be achieved. Great strides have\nbeen made in this direction, also in the real world, with notable applications\nin domains like robotics [6], autonomous driving [10, 6], and control prob-\nlems such as optimizing the power efficiency of data centers [8], control of\nnuclear fusion reactors [4], and optimizing gas turbines [3].\nYet, the accelerated progress in these areas has been fueled by making\nagents play in virtual gaming environments such as Atari 2600 games [9], the\ngame of GO [14], and complex video games like Starcraft II [17]. These games\nprovided sufficiently challenging environments to quickly test new algorithms\nand ideas, and gave rise to a suite of RL benchmark environments. The use\nof such environments to benchmark agents for industrial deployment comes\nwith certain drawbacks. For instance, the environments either may not be\nchallenging enough for the state-of-the-art algorithms (low dimensionality\nof state and action spaces) or require significant computational resources\nto solve. Industrial problems deviate widely from games in many further\nproperties.\nPrimarily, exploration in real-world systems often has strong\nsafety constraints, and constitutes a balancing act between maximizing re-\nward with good actions which are often sparse, and minimizing potentially\nsevere consequences from bad actions. This is in contrast to training on a\ngaming environment, where the impact of a single action is often smaller,\nand the repercussions of poor decisions accumulate slowly over time. In ad-\ndition, underlying dynamics of gaming environments—several of which are\nnear-deterministic—may not reflect the stochasticity of a real industrial sys-\ntem. Finally, the available environments may have a tedious setup procedure\nwith restrictive licensing and dependencies on closed-source binaries.\nTo address these issues, we present ContainerGym, an open-source real-\nworld benchmark environment for RL algorithms. It is adapted from a digital\ntwin of a high throughput processing industry. Our concrete use-case comes\nfrom a waste sorting application. Our benchmark focuses on two phenomena\nof general interest: First, a stochastic model for a resource-filling process,\nwhere certain material is being accumulated in multiple storage containers.\nSecond, a model for a resource transforming system, which takes in the\nmaterial from these containers, and transforms it for further post-processing\ndownstream. The processing units are a scarce resource, since they are large\nand expensive, and due to limited options of conveyor belt layout, only a\nsmall number can be used per plant. ContainerGym is not intended to be a\nperfect replica of a real system but serves the same hardness and complexity.\nThe search for an optimal sequence of actions in ContainerGym is akin to\nsolving a dynamic resource allocation problem for the resource-transforming\nsystem, while also learning an optimal control behavior of the resource-filling\nprocess.\nIn addition, the complexity of the environment is customizable.\nThis allows testing the limitations of any given learning algorithm. This work\naims to enable RL practitioners to quickly and reliably test their learning\n2\nagents on an environment encoding real-world dynamics.\nThe paper is arranged as follows. Section 2 discusses the relevant liter-\nature and motivates our contribution. Section 3 gives a brief introduction\nto reinforcement learning preliminaries. In Section 4, we present the real-\nworld industrial control task that inspired ContainerGym and discuss the\nchallenges it presents. In Section 5, we formulate the real-world problem as\na RL one and discuss the design choices that lead to our digital twin. We\nbriefly present ContainerGym’s implementation in Section 6. We present\nand discuss benchmark experiments of baseline methods in Section 7, and\nclose with our conclusions in Section 8.\n2\nRelated Work\nThe majority of the existing open-source environments on which novel rein-\nforcement learning algorithms could be tuned can be broadly divided into\nthe following categories: toy control, robotics (MuJoCo) [16], video games\n(Atari) [9], and autonomous driving. The underlying dynamics of these en-\nvironments are artificial and may not truly reflect real-world dynamic condi-\ntions like high dimensional states and action spaces, and stochastic dynamics.\nTo the best of our knowledge, there exist very few such open-source bench-\nmarks for industrial applications. To accelerate the deployment of agents\nin the industry, there is a need for a suite of RL benchmarks inspired by\nreal-world industrial control problems, thereby making our benchmark envi-\nronment, ContainerGym, a valuable addition to it.\nThe classic control environments like mountain car, pendulum, or toy\nphysics control environments based on Box2D are stochastic only in terms\nof their initial state. They have low dimensional state and action spaces,\nand they are considered easy to solve with standard methods.\nAlso, the\n50 commonly used Atari games in the Arcade Learning Environment [1],\nwhere nonlinear control policies need to be learned, are routinely solved to\na super-human level by well-established algorithms. This environment, al-\nthough posing high dimensionality, is deterministic. The real world is not\ndeterministic and there is a need to tune algorithms that can cope with\nstochasticity. Although techniques like sticky actions or skipping a random\nnumber of initial frames have been developed to add artificial randomness,\nthis randomness may still be very structured and not challenging enough.\nOn the other hand, video game simulators like Starcraft II [17] offer high-\ndimensional image observations, partial observability, and (slight) stochas-\nticity. However, playing around with DeepRL agents on such environments\nrequires substantial computational resources. It might very well be overkill\nto tune reinforcement learning agents in these environments when the real\ngoal is to excel in industrial applications.\nAdvancements in the RL world, in games like Go and Chess, were achieved\n3\nby exploiting the rules of these games and embedding them into a stochastic\nplanner. In real-world environments, this is seldom achievable, as these sys-\ntems are highly complex to model in their entirety. Such systems are modeled\nas partially observable Markov decision processes and present a tough chal-\nlenge for learning agents that can explore only through interactions. Lastly,\nsome of the more sophisticated RL environments available, e.g., advanced\nphysics simulators like MuJoCo [16], offer licenses with restrictive terms of\nuse. Also, environments like Starcraft II require access to a closed-source\nbinary. Open source licensing in an environment is highly desirable for RL\npractitioners as it enables them to debug the code, extend the functionality\nand test new research ideas.\nOther related works, amongst the very few available open-source rein-\nforcement learning environments for industrial problems, are Real-world RL\nsuite [5] and Industrial benchmark (IB) [7] environments. Real-world RL-\nsuite is not derived from a real-world scenario, rather the existing toy prob-\nlems are perturbed to mimic the conditions in a real-world problem. The\nIB comes close in spirit to our work, although it lacks the customizability\nof ContainerGym and our expanded tools for agent behavior explainabil-\nity. Additionally, the (continuous) action and state spaces are relatively low\ndimensional and of fixed sizes.\n3\nReinforcement Learning Preliminaries\nRL problems are typically studied as discrete-time Markov decision processes\n(MDPs), where a MDP can be defined as a tuple ⟨S, A, p, r, γ⟩. At timestep\nt, the agent is in a state st ∈S and takes an action at ∈A. It arrives\nin a new state st+1 with probability p(st+1 | st, at) and receives a reward\nr(st, at, st+1). The state transitions of a MDP satisfy the Markov property\np(st+1 | st, at, . . . , s0, a0) = p(st+1 | st, at). That is, the new state st+1 only\ndepends on the current state st and action at. The goal of the RL agent\ninteracting with the MDP is to find a policy π : S →A that maximizes\nthe expected (discounted) cumulative reward. This optimization problem is\ndefined formally as follows:\narg max\nπ\nEτ∼π\n\nX\nt≥0\nγtr(st, at, st+1)\n\n,\n(1)\nwhere τ = (s0, a0, r0, s1, . . . ) is a trajectory generated by following π and\nγ ∈(0, 1] is a discount factor. A trajectory τ ends either when a maxi-\nmum timestep count T—also called episode1 length—is reached, or when a\nterminal state is reached (early termination).\n1We use the terms “episode” and “rollout” interchangeably in this paper.\n4\n4\nContainer Management Environment\nIn this section, we describe the real-world industrial control task that inspired\nour RL benchmark.\nIt originates from the final stage of a waste sorting\nprocess.\nThe environment consists of a solid material-transforming facility that\nhosts a set of containers and a significantly smaller set of processing units\n(PUs). Containers are continuously filled with material, where the material\nflow rate is a container-dependent stochastic process. They must be emptied\nregularly so that their content can be transformed by the PUs. When a\ncontainer is emptied, its content is transported on a conveyor belt to a free\nPU that transforms it into products. It is not possible to extract material\nfrom a container without emptying it completely. The number of produced\nproducts depends on the volume of the material being processed. Ideally,\nthis volume should be an integer multiple of the product’s size. Otherwise,\nthe surplus volume that cannot be transformed into a product is redirected\nto the corresponding container again via an energetically costly process that\nwe do not consider in this work. Each container has at least one optimal\nemptying volume: a global optimum and possibly other, smaller, local optima\nthat are all multiples of container-specific product size. Generally speaking,\nlarger volumes are better, since PUs are more efficient with producing many\nproducts in a series.\nThe worst-case scenario is a container overflow. In the waste sorting ap-\nplication inspiring our benchmark, it incurs a high recovery cost including\nhuman intervention to stop and restart the facility. This situation is unde-\nsirable and should be actively avoided. Therefore, letting containers come\nclose to their capacity limit is rather risky.\nThe quality of an emptying decision is a compromise between the num-\nber of potential products and the costs resulting from actuating a PU and\nhandling surplus volume. Therefore, the closer an emptying volume is to\nan optimum, the better. If a container is emptied too far away from any\noptimal volume, the costs outweigh the benefit of producing products.\nThis setup can be framed more broadly as a resource allocation problem,\nwhere one item of a scarce resource, namely the PUs, needs to be allocated\nwhenever an emptying decision is made. If no PU is available, the container\ncannot be emptied and will continue to fill up.\nThere are multiple aspects that make this problem challenging:\n• The rate at which the material arrives at the containers is stochas-\ntic. Indeed, although the volumes follow a globally linear trend—as\ndiscussed in details in Section 5, the measurements can be very noisy.\nThe influx of material is variable, and there is added noise from the\nsensor readings inside the containers. This makes applying standard\nplanning approaches difficult.\n• The scarcity of the PUs implies that always waiting for containers to fill\n5\nup to their ideal emptying volume is risky: if no PU is available at that\ntime, then we risk an overflow. This challenge becomes more prominent\nwhen the number of containers—in particular, the ratio between the\nnumber of containers and the number of PUs—increases. Therefore,\nan optimal policy needs to take fill states and fill rates of all containers\ninto account, and possibly empty some containers early.\n• Emptying decisions can be taken at any time, but in a close-to-optimal\npolicy, they are rather infrequent. Also, the rate at which containers\nshould be emptied varies between containers. Therefore, the distribu-\ntions of actions are highly asymmetric, with important actions (and\ncorresponding rewards) being rare.\n5\nReinforcement Learning Problem Formulation\nWe formulate the container management problem presented in Section 4 as\na MDP that can be addressed by RL approaches. The resulting MDP is an\naccurate representation of the original problem, as only mild simplifications\nare made. Specifically, we model one category of PUs instead of two, we\ndo not include inactivity periods of the facility in our environment, and we\nneglect the durations of processes of minor relevance. All parameters de-\nscribed in the rest of this section are estimated from real-world data (system\nidentification). Overall, the MDP reflects the challenges properly without\ncomplicating the benchmark (and the code base) with irrelevant details.\n5.1\nState and Action Spaces\n5.1.1\nState space\nThe state st of the system at any given time t is defined by the volumes vi,t\nof material contained in each container C i, and a timer pj,t for each PU Pj\nindicating in how many seconds the PU will be ready to use. A value of\nzero means that the PU is available, while a positive value means that it is\ncurrently busy. Therefore, st = ({vi,t}n\ni=1, {pj,t}m\nj=1), where n and m are the\nnumber of containers and PUs, respectively. Valid initial states include non-\nnegative volumes not greater than the maximum container capacity vmax,\nand non-negative timer values.\n5.1.2\nAction space\nPossible actions at a given time t are either (i) not to empty any container,\ni.e. to do nothing, or (ii) to empty a container C i and transform its content\nusing one of the PUs. The action of doing nothing is encoded with 0, whereas\nthe action of emptying a container C i and transforming its content is encoded\nwith the container’s index i. Therefore, at ∈{0, 1, . . . , n}, where n is the\n6\nnumber of containers. Since we consider identical PUs, specifying which PU\nis used in the action encoding is not necessary as an emptying action will\nresult in the same state for all the PUs, given they were at the same previous\nstate.\n5.2\nEnvironment Dynamics\nIn this section, we define the dynamics of the volume of material in the\ncontainers, the PU model, as well as the state update.\n5.2.1\nVolume dynamics\nThe volume of material in each container increases following an irregular\ntrend, growing linearly on average, captured by a random walk model with\ndrift. That is, given the current volume vi,t contained in C i, the volume at\ntime t + 1 is given by the function fi defined as\nfi(vi,t) = max(0, αi + vi,t + ϵi,t) ,\n(2)\nwhere αi > 0 is the slope of the linear upward trend followed by the volume\nfor C i and the noise ϵi,t is sampled from a normal distribution N(0, σ2\ni ) with\nmean 0 and variance σ2\ni . The max operator forces the volume to non-negative\nvalues.\nWhen a container C i is emptied, its volume drops to 0 at the next\ntimestep. Although the volume drops progressively to 0 in the real facil-\nity, empirical evidence provided by our data shows that emptying durations\nare within the range of the time-step lengths considered in this paper (60\nand 120 seconds).\n5.2.2\nProcessing unit dynamics\nThe time (in seconds) needed by a PU to transform a volume v of material\nin a container C i is linear in the number of products ⌊v/bi⌋that can be\nproduced from v. It is given by the function gi defined in Equation (3),\nwhere bi > 0 is the product size, βi > 0 is the time it takes to actuate the\nPU before products can be produced, and λi > 0 is the time per product.\nNote that all the parameters indexed with i are container-dependent.\ngi(v) = βi + λi⌊v/bi⌋.\n(3)\nA PU can only produce one type of product at a time. Therefore, it can be\nused for emptying a container only when it is free. Therefore, if all PUs are\nbusy, the container trying to use one is not emptied and continues to fill up.\n5.2.3\nState update\nWe distinguish between the following cases to define the new state st+1 =\n({vi,t+1}n\ni=1, {pj,t+1}m\nj=1) given the current state st and action at.\n7\nat = 0.\nThis corresponds to the action of doing nothing. The material\nvolumes inside the containers increase while the timers indicating the avail-\nability of the PUs are decreased according to:\nvi,t+1 = fi(vi,t),\ni ∈{1, . . . , n} ,\n(4)\npj,t+1 = max(0, pj,t −δ),\nj ∈{1, . . . , m} ,\n(5)\nwhere fi is the random walk model defined in Equation (2) and δ is the\nlength of a timestep in seconds.\nat ̸= 0.\nThis corresponds to an emptying action. If no PU is available,\nthat is, pj,t > 0 ∀j = 1, . . . , m, the updates are identical to the one defined\nin Equations (4) and (5). If at least one PU Pk is available, the new state\nvariables are defined as follows:\nvat,t+1 = 0 ,\n(6)\nvi,t+1 = fi(vi,t),\ni ∈{1, . . . , n}\\{at} ,\n(7)\npk,t+1 = gat(vat,t) ,\n(8)\npj,t+1 = max(0, pj,t −δ),\nj ∈{1, . . . , m}\\{k} ,\n(9)\nwhere the value of the action at is the index of the container C at to empty,\n‘\\’ denotes the set difference operator, fi is the random walk model defined\nin Equation (2), and gat is the processing time defined in Equation (3).\nAlthough the processes can continue indefinitely, we stop an episode after\na maximum length of T timesteps. This is done to make ContainerGym\ncompatible with RL algorithms designed for episodic tasks, and hence to\nmaximize its utility. When a container reaches its maximum volume vmax,\nhowever, the episode is terminated and a negative reward is returned (see\ndetails in Section 5.3).\n5.3\nReward Function\nWe use a deterministic reward function r(st, at, st+1) where higher values\ncorrespond to better (st, at) pairs. The new state st+1 is taken into account\nto return a large negative reward rmin when a container overflows, i.e. ∃i ∈\n{1, . . . , n}, vi,t+1 ≥vmax, before ending the episode.\nIn all other cases,\nthe immediate reward is determined only by the current state st and the\naction at.\nat = 0.\nIf the action is to do nothing, we define r(st, 0, st+1) = 0.\nat ̸= 0.\nIf an emptying action is selected while (i) no PU is available or (ii)\nthe selected container is already empty, i.e. vat,t = 0, then r(st, at, st+1) =\nrpen, where it holds rmin < rpen < 0 for the penalty.\nIf, on the other\n8\n0\n5\n10\n15\n20\n25\n30\n35\n40\nEmptying volume\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\nrpen\nv *\n1\nv *\n2\nv *\n3\nContainer C1-40\nFigure 1: Rewards received when emptying a container with three optima.\nThe design of the reward function fosters emptying late, hence allowing PUs\nto produce many products in a row.\nhand, at least one PU is available and the volume to process is non-zero\n(vat,t > 0), the reward is a finite sum of Gaussian functions centered around\noptimal emptying volumes v∗\nat,i, i = 1, . . . , pat, where the height of a peak\n0 < hat,i ≤1 is proportional to the quality of the corresponding optimum.\nThe reward function in this case is defined as follows:\nr(st, at, st+1) = rpen +\npat\nX\ni=1\n(hat,i −rpen) exp\n \n−\n(vat,t −v∗\nat,i)2\n2w2\nat,i\n!\n,\n(10)\nwhere pat is the number of optima for container C at and wat,i > 0 is the\nwidth of the bell around v∗\nat,i. The Gaussian reward defined in Equation (10)\ntakes its values in ]rpen, 1], the maximum value 1 being achieved at the ideal\nemptying volume v∗\nat,1 for which hat,1 = 1. The coefficients ha,i are designed\nso that processing large volumes at a time is beneficial, hence encoding a con-\nflict between emptying containers early and risking overflow. This tension,\ntogether with the limited availability of the PUs, yields a highly non-trivial\ncontrol task. Figure 1 shows an example of Gaussian rewards when emptying\na container with three optimal volumes at different volumes in [0, 40[.\n6\nContainerGym Usage Guide\nIn this section, we introduce the OpenAI Gym implementation2 of our bench-\nmark environment. We present the customizable parameters of the bench-\nmark, provide an outline for how the Python implementation reflects the\ntheoretical definition in Section 5, and show which tools we provide to ex-\ntend the understanding of an agent’s behavior beyond the achieved rewards.\nGym implementation.\nOur environment follows the OpenAI Gym [2]\nframework.\nIt implements a step()-function computing a state update.\n2The ContainerGym software is available on the following GitHub repository: https:\n//github.com/Pendu/ContainerGym.\n9\nThe reset()-function resets time to t = 0 and returns the environment to a\nvalid initial state, and the render()-function displays a live diagram of the\nenvironment.\nWe deliver functionality to configure the environment’s complexity and\ndifficulty through its parameters such as the number of containers and PUs,\nas well as the composition of the reward function, the overflow penalty rmin,\nthe sub-optimal emptying penalty rpen, the length of a timestep δ, and the\nlength of an episode T. We provide example configurations in our GitHub\nrepository that are close in nature to the real industrial facility.\nIn the spirit of open and reproducible research, we include scripts and\nmodel files for reproducing the experiments presented in the next section.\nAction explainability.\nWhile most RL algorithms treat the environment\nas a black box, facility operators want to “understand” a policy before deploy-\ning it for production. To this end, the accumulated reward does not provide\nsufficient information, since it treats all mistakes uniformly. Practitioners\nneed to understand which types of mistakes a sub-optimal policy makes. For\nexample, a low reward can be obtained by systematically emptying contain-\ners too early (local optimum) or by emptying at non-integer multiples of the\nproduct size. When a basic emptying strategy for each container is in place,\nlow rewards typically result from too many containers reaching their ideal\nvolume at the same time so that PUs are overloaded. To make the differ-\nent types of issues of a policy transparent, we plot all individual container\nvolumes over an entire episode. We further provide tools to create empirical\ncumulative distribution function plots over the volumes at which containers\nwere emptied over multiple episodes. The latter plots in particular provide\ninsights into the target volumes an agent aims to hit, and whether it does\nso reliably.\n7\nPerformance of Baseline Methods\nWe illustrate the use of ContainerGym by benchmarking three popular deep\nRL algorithms: two on-policy methods, namely Proximal Policy Optimiza-\ntion (PPO) [13] and Trust Region Policy Optimization (TRPO) [12], and one\noff-policy method, namely Deep Q-Network (DQN) [9]. We also compare the\nperformance of these RL approaches against a naive rule-based controller.\nBy doing so, we establish an initial baseline on ContainerGym. We use the\nStable Baselines3 implementation of PPO, TRPO, and DQN [11].\n7.1\nExperimental Setup\nThe algorithms are trained on 8 ContainerGym instances, detailed in Table 1,\nwhere the varied parameters are the number of containers n, the number\n10\nof PUs m and the timestep length δ.3\nThe rationale behind the chosen\nconfigurations is to assess (i) how the algorithms scale with the environment\ndimensionality, (ii) how action frequency affects the trained policies and\n(iii) whether the optimal policy can be found in the conceptually trivial\ncase m = n, where there is always a free PU when needed. This is only a\ncontrol experiment for testing RL performance. In practice, PUs are a scarce\nresource (m ≪n).\nThe maximum episode length T is set to 1500 timesteps during training\nin all experiments, whereas the initial volumes vi,0 are uniformly sampled in\n[0, 30], and the maximum capacity is set to vmax = 40 volume units for all\ncontainers. The initial PU states are set to free (pj,0 = 0) and the minimum\nand penalty rewards are set to rmin = −1 and rpen = −0.1 respectively. The\nalgorithms are trained with an equal budget of 2 (resp. 5) million timesteps\nwhen n = 5 (resp. n = 11) and the number of steps used for each policy\nupdate for PPO and TRPO is 6144. Default values are used for the remaining\nhyperparameters, as the aim of our work is to show characteristics of the\ntraining algorithms with a reasonable set of parameters. For each algorithm,\n15 policies are trained in parallel, each with a different seed ∈{1, . . . , 15},\nand the policy with the best training cumulative reward is returned for each\nrun.\nTo make comparison easier, policies are evaluated on a similar test\nenvironment with δ = 120 and T = 600.\n7.2\nResults and Discussion\nTable 1 shows the average test cumulative reward, along with its standard\ndeviation, for the best and median policies trained with PPO, TRPO, and\nDQN on each environment configuration. These statistics are calculated by\nrunning each policy 15 times on the corresponding test environment.\nPPO achieves the highest cumulative reward on environments with 5\ncontainers. DQN, on the other hand, shows the best performance on higher\ndimensionality environments (11 containers), with the exception of the last\nconfiguration.\nIt has, however, a particularly high variance.\nIts median\nperformance is also significantly lower than the best one, suggesting less\nstability than PPO. DQN’s particularly high rewards when n = 11 could\nbe due to a better exploration of the search space.\nAn exception is the\nlast configuration, where DQN’s performance is significantly below those\nof TRPO and PPO. Overall, PPO has smaller standard deviations and a\nsmaller difference between best and median performances, suggesting higher\nstability than TRPO and DQN.\nOur results also show that taking actions at a lower frequency (δ = 120)\nleads to better policies.\nDue to space limitations, we focus on analyzing\n3Increasing the timestep length δ should be done carefully. Otherwise, the problem\ncould become trivial. In our case, we choose δ such that it is smaller than the minimum\ntime it takes a PU to process the volume equivalent to one product.\n11\nTable 1: Test cumulative reward, averaged over 15 episodes, and its stan-\ndard deviation for the best and median policies for PPO, TRPO and DQN.\nBest and median policies are selected from a sample of 15 policies (seeds)\ntrained on the investigated environment configurations. The highest best\nperformance is highlighted for each configuration.\nConfig.\nPPO\nTRPO\nDQN\nn\nm\nδ\nbest\nmedian\nbest\nmedian\nbest\nmedian\n5\n2\n60\n38.55 ± 1.87\n29.94 ± 8.42\n37.35 ± 7.13\n4.93 ± 4.90\n29.50 ± 3.43\n1.57 ± 0.93\n5\n2\n120\n51.43 ± 3.00\n49.81 ± 1.56\n38.33 ± 8.36\n16.86 ± 2.78\n42.96 ± 2.80\n7.90 ± 4.72\n5\n5\n60\n37.54 ± 1.50\n30.64 ± 2.44\n33.62 ± 7.76\n7.36 ± 4.40\n23.98 ± 13.17\n1.96 ± 1.28\n5\n5\n120\n50.42 ± 2.57\n47.23 ± 1.89\n47.36 ± 2.07\n5.39 ± 4.38\n43.26 ± 3.73\n8.56 ± 4.12\n11\n2\n60\n31.01 ± 8.62\n23.25 ± 16.79\n26.62 ± 18.94\n4.26 ± 3.81\n42.63 ± 21.73\n11.84 ± 10.33\n11\n2\n120\n54.30 ± 8.48\n49.58 ± 13.54\n45.78 ± 18.42\n32.08 ± 18.04\n72.19 ± 16.20\n24.07 ± 13.68\n11\n11\n60\n27.87 ± 8.89\n17.57 ± 13.99\n15.66 ± 10.65\n4.90 ± 2.68\n28.32 ± 22.42\n8.49 ± 4.92\n11\n11\n120\n47.75 ± 10.78\n42.37 ± 13.25\n50.55 ± 11.08\n34.29 ± 16.00\n29.06 ± 13.10\n13.16 ± 8.32\nthis effect on configurations with n = 5 and m = 2. Figure 2 displays the\nvolumes, actions, and rewards over one episode for PPO and DQN when the\n(best) policy is trained with δ = 60 (left column) and with δ = 120.\nWe observe that with δ = 60, PPO tends to empty containers C1-60\nand C1-70 prematurely, which leads to poor rewards. These two containers\nhave the slowest fill rate. Increasing the timestep length, however, alleviates\nthis defect. The opposite effect is observed on C1-60 with DQN, whereas\nno significant container-specific behavior change is observed for TRPO, as\nevidenced by the cumulative rewards. To further explain these results, we\ninvestigate the empirical cumulative distribution functions (ECDFs) of the\nemptying volumes per container. Figure 3 reveals that more than 90% of\nPPO’s emptying volumes on C1-60 (resp. C1-70) are approximately within a\n3 (resp. 5) volume units distance of the third best optimum when δ = 60. By\nincreasing the timestep length, the emptying volumes become more centered\naround the global optimum. While no such clear pattern is observed with\nDQN on containers with slow fill rates, the emptying volumes on C1-60\nmove away from the global optimum when the timestep length is increased\n(more than 50% of the volumes are within a 3 volume units distance of the\nsecond best optimum). DQN’s performance increases when δ = 120. This\nis explained by the better emptying volumes achieved on C1-20, C1-70, and\nC1-80.\nNone of the benchmarked algorithms manage to learn the optimal policy\nwhen there are as many PUs as containers, independently of the environment\ndimensionality and the timestep length. When m = n, the optimal policy\nis known and consists in emptying each container when the corresponding\nglobal optimal volume is reached, as there is always at least one free PU.\nTherefore, achieving the maximum reward at each emptying action is the-\noretically possible. Figure 4a shows the ECDF of the reward per emptying\naction over 15 episodes of the best policy for each of PPO, TRPO and DQN\nwhen m = n = 11. The rewards range from rmin = −1 to 1, and the ratio of\n12\nnegative rewards is particularly high for PPO. An analysis of the ECDFs of\nits emptying volumes (not shown) reveals that this is due to containers with\nslow fill rates being emptied prematurely. TRPO achieves the least amount\nof negative rewards whereas all DQN’s rollouts end prematurely due to a con-\ntainer overflowing. Rewards close to rpen are explained by poor emptying\nvolumes, as well as a bad allocation of the PUs (r = rpen). These findings\nsuggest that, when m = n, it is not trivial for the tested RL algorithms\nto break down the problem into smaller, independent problems, where one\ncontainer is emptied using one PU when the ideal volume is reached.\nThe limitations of these RL algorithms are further highlighted when com-\npared to a naive rule-based controller that empties the first container whose\nvolume is less than 1 volume unit away from the ideal emptying volume.\nFigure 4b shows the ECDF of reward per emptying action obtained from\n15 rollouts of the rule-based controller on three environment configurations,\nnamely n = 5 and m = 2, n = 11 and m = 2, and n = 11 and m = 11. When\ncompared to PPO in particular (m = n = 11), the rule-based controller emp-\nties containers less often (17.40% of the actions vs. more than 30% for PPO).\nPositive rewards are close to optimal (approx. 90% in [0.75, 1]), whereas very\nfew negative rewards are observed. These stem from emptying actions taken\nwhen no PU is available. These findings suggest that learning to wait for\nlong periods before performing an important action may be challenging for\nsome RL algorithms.\nCritically, current baseline algorithms only learn reasonable behavior for\ncontainers operated in isolation.\nIn the usual m ≪n case, none of the\npolicies anticipates all PUs being busy when emptying containers at their\noptimal volume, which they should ideally foresee and prevent proactively\nby emptying some of the containers earlier.\nHence, there is considerable\nspace for future improvements by learning stochastic long-term dependencies.\nThis is apparently a difficult task for state-of-the-art RL algorithms. We\nanticipate that ContainerGym can contribute to research on next-generation\nRL methods addressing this challenge.\n8\nConclusion\nWe have presented ContainerGym, a real-world industrial RL environment.\nIts dynamics are quite basic, and therefore easily accessible.\nIt is easily\nscalable in complexity and difficulty.\nIts characteristics are quite different from many standard RL benchmark\nproblems. At its core, it is a resource allocation problem. It features stochas-\nticity, learning agents can get trapped in local optima, and in a good policy,\nthe most important actions occur only rarely. Furthermore, implementing\noptimal behavior requires planning ahead under uncertainty.\nThe most important property of ContainerGym is to be of direct indus-\n13\n0\n100\n200\n300\n400\n500\n600\n0\n20\n40\nVolume\n0\n100\n200\n300\n400\n500\n600\n1\n2\n3\n4\n5\nAction\n0\n100\n200\n300\n400\n500\n600\nTimestep\n0.0\n0.5\n1.0\nCumul. r : 38.70\nReward\nRollout of PPO on test environment\nC1-20\nC1-30\nC1-60\nC1-70\nC1-80\n0\n100\n200\n300\n400\n500\n600\n0\n20\n40\nVolume\n0\n100\n200\n300\n400\n500\n600\n1\n2\n3\n4\n5\nAction\n0\n100\n200\n300\n400\n500\n600\nTimestep\n0.0\n0.5\n1.0\nCumul. r : 52.49\nReward\nRollout of PPO on test environment\nC1-20\nC1-30\nC1-60\nC1-70\nC1-80\n0\n100\n200\n300\n400\n500\n600\n0\n20\n40\nVolume\n0\n100\n200\n300\n400\n500\n600\n1\n2\n3\n4\n5\nAction\n0\n100\n200\n300\n400\n500\n600\nTimestep\n0.0\n0.5\n1.0\nCumul. r : 30.42\nReward\nRollout of DQN on test environment\nC1-20\nC1-30\nC1-60\nC1-70\nC1-80\n0\n100\n200\n300\n400\n500\n600\n0\n20\n40\nVolume\n0\n100\n200\n300\n400\n500\n600\n1\n2\n3\n4\n5\nAction\n0\n100\n200\n300\n400\n500\n600\nTimestep\n0.0\n0.5\n1.0\nCumul. r : 45.53\nReward\nRollout of DQN on test environment\nC1-20\nC1-30\nC1-60\nC1-70\nC1-80\nFigure 2: Rollouts of the best policy trained with PPO (first row) and DQN\n(second row) on a test environment with n = 5 and m = 2. Left: training\nenvironment with δ = 60 seconds. Right: training environment with δ = 120.\nDisplayed are the volumes, emptying actions and non-zero rewards at each\ntimestep.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 5.77e-03\nC1-20\nPPO\nTRPO\nDQN\nglobal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 1.91e-03\nC1-60\nPPO\nTRPO\nDQN\nglobal opt.\nlocal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 3.57e-03\nC1-70\nPPO\nTRPO\nDQN\nglobal opt.\nlocal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 8.14e-03\nC1-80\nPPO\nTRPO\nDQN\nglobal opt.\nlocal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 5.77e-03\nC1-20\nPPO\nTRPO\nDQN\nglobal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 1.91e-03\nC1-60\nPPO\nTRPO\nDQN\nglobal opt.\nlocal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 3.57e-03\nC1-70\nPPO\nTRPO\nDQN\nglobal opt.\nlocal opt.\n0\n5\n10\n15\n20\n25\n30\n35\n40\ni\nl\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFill rate: 8.14e-03\nC1-80\nPPO\nTRPO\nDQN\nglobal opt.\nlocal opt.\nFigure 3: ECDFs of emptying volumes collected over 15 rollouts of the best\npolicy for PPO, TRPO, and DQN on a test environment with n = 5 and\nm = 2. Shown are 4 containers out of 5. Fill rates are indicated in volume\nunits per second. Top: training environment with δ = 60. Bottom: training\nenvironment with δ = 120. The derivatives of the curves are the PDFs of\nemptying volumes. Therefore, a steep incline indicates that the correspond-\ning volume is frequent in the corresponding density.\n14\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nReward\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nECDF of reward per emptying action\nPPO\nTRPO\nDQN\n(a)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nReward\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nECDF of reward per emptying action\nn = 5, m = 2\nn = 11, m = 2\nn = 11, m = 11\n(b)\nFigure 4: ECDF of the reward obtained for each emptying action over 15\nrollouts. (a): Best policies for PPO, TRPO and DQN, trained on an envi-\nronment with m = n = 11 and δ = 120. (b): Rule-based controller on three\nenvironment configurations.\ntrial relevance. At the same time, it is a difficult problem with the potential\nto trigger novel developments in the future. While surely being less chal-\nlenging than playing the games of Go or Starcraft II at a super-human level,\nContainerGym is still sufficiently hard to make state-of-the-art baseline al-\ngorithms perform poorly. We are looking forward to improvements in the\nfuture.\nTo fulfill the common wish of industrial stakeholders for explainable ML\nsolutions, we provide insights into agent behavior and deviations from op-\ntimal behavior that go beyond learning curves. While accumulated reward\nhides the details, our environment provides insights into different types of\nfailures and hence gives guidance for routes to further improvement.\nAcknowledgements.\nThis work was funded by the German federal min-\nistry of economic affairs and climate action through the “ecoKI” grant.\nReferences\n[1] Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learn-\ning environment: An evaluation platform for general agents. Journal of\nArtificial Intelligence Research 47, 253–279 (Jun 2013)\n[2] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J.,\nTang, J., Zaremba, W.: Openai gym (2016)\n[3] Compare, M., Bellani, L., Cobelli, E., Zio, E.: Reinforcement learning-\nbased flow management of gas turbine parts under stochastic fail-\nures. The International Journal of Advanced Manufacturing Technology\n99(9-12), 2981–2992 (Sep 2018)\n[4] Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese,\nF., Ewalds, T., Hafner, R., Abdolmaleki, A., de las Casas, D., Don-\n15\nner, C., Fritz, L., Galperti, C., Huber, A., Keeling, J., Tsimpoukelli,\nM., Kay, J., Merle, A., Moret, J.M., Noury, S., Pesamosca, F., Pfau,\nD., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli,\nP., Kavukcuoglu, K., Hassabis, D., Riedmiller, M.:\nMagnetic con-\ntrol of tokamak plasmas through deep reinforcement learning. Nature\n602(7897), 414–419 (Feb 2022)\n[5] Dulac-Arnold, G., Levine, N., Mankowitz, D.J., Li, J., Paduraru, C.,\nGowal, S., Hester, T.: An empirical investigation of the challenges of\nreal-world reinforcement learning. CoRR abs/2003.11881 (2020)\n[6] Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J.,\nKumar, V., Zhu, H., Gupta, A., Abbeel, P., Levine, S.: Soft actor-critic\nalgorithms and applications. CoRR abs/1812.05905 (2018)\n[7] Hein, D., Depeweg, S., Tokic, M., Udluft, S., Hentschel, A., Runkler,\nT.A., Sterzing, V.: A benchmark environment motivated by industrial\ncontrol problems. In: 2017 IEEE Symposium Series on Computational\nIntelligence (SSCI). IEEE (nov 2017)\n[8] Lazic, N., Lu, T., Boutilier, C., Ryu, M., Wong, E.J., Roy, B., Imwalle,\nG.: Data center cooling using model-predictive control. In: Proceed-\nings of the Thirty-second Conference on Neural Information Processing\nSystems (NeurIPS-18). pp. 3818–3827. Montreal, QC (2018)\n[9] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wier-\nstra, D., Riedmiller, M.A.: Playing atari with deep reinforcement learn-\ning. CoRR abs/1312.5602 (2013)\n[10] Osiński, B., Jakubowski, A., Zięcina, P., Miłoś, P., Galias, C., Homo-\nceanu, S., Michalewski, H.: Simulation-based reinforcement learning for\nreal-world autonomous driving. In: 2020 IEEE International Conference\non Robotics and Automation (ICRA). pp. 6411–6418 (2020)\n[11] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann,\nN.: Stable-baselines3: Reliable reinforcement learning implementations.\nJournal of Machine Learning Research 22(268), 1–8 (2021)\n[12] Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust\nregion policy optimization. In: Bach, F., Blei, D. (eds.) Proceedings of\nthe 32nd International Conference on Machine Learning. Proceedings\nof Machine Learning Research, vol. 37, pp. 1889–1897. PMLR (2015)\n[13] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Prox-\nimal policy optimization algorithms. CoRR abs/1707.06347 (2017)\n16\n[14] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den\nDriessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,\nLanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N.,\nSutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\nHassabis, D.: Mastering the game of go with deep neural networks and\ntree search. Nature 529(7587), 484–489 (Jan 2016)\n[15] Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction.\nMIT press (2018)\n[16] Todorov, E., Erez, T., Tassa, Y.: Mujoco: A physics engine for model-\nbased control. In: 2012 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems. pp. 5026–5033 (2012)\n[17] Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M., Dudzik,\nA., Chung, J., Choi, D.H., Powell, R., Ewalds, T., Georgiev, P., Oh,\nJ., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T.,\nAgapiou, J.P., Jaderberg, M., Vezhnevets, A.S., Leblond, R., Pohlen,\nT., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T.L., Gul-\ncehre, C., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., Wünsch,\nD., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Kavukcuoglu,\nK., Hassabis, D., Apps, C., Silver, D.: Grandmaster level in StarCraft\nII using multi-agent reinforcement learning. Nature 575(7782), 350–354\n(Oct 2019)\n17\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-07-06",
  "updated": "2023-07-06"
}