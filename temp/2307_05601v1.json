{
  "id": "http://arxiv.org/abs/2307.05601v1",
  "title": "Unsupervised Domain Adaptation with Deep Neural-Network",
  "authors": [
    "Artem Bituitskii"
  ],
  "abstract": "This report contributes to the field of unsupervised domain adaptation by\nproviding an analysis of existing methods, introducing a new approach, and\ndemonstrating the potential for improving visual recognition tasks across\ndifferent domains. The results of this study open up opportunities for further\nstudy and development of advanced methods in the field of domain adaptation.",
  "text": "Unsupervised Domain\nAdaptation with Deep\nNeural-Networks\nMSIAM M2, 2022-2023\nArtem Bitiutskii\nSupervisors: Massih-Reza Amini and Marianne Clausel\nGrenoble\narXiv:2307.05601v1  [cs.CV]  10 Jul 2023\nContents\n1\nIntroduction\n1\n1.1\nTheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2\nState-of-the-art\n4\n2.1\nUDA by Backpropagation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nSemantic Representations for UDA\n. . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nFixbi for UDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.4\nSpherical Space DA with Pseudo-label Loss . . . . . . . . . . . . . . . . . .\n8\n2.5\nDA with Invariant Representation Learning\n. . . . . . . . . . . . . . . . .\n10\n2.6\nDomain Adaptation for Segmentation with CBST . . . . . . . . . . . . . .\n11\n3\nContribution\n13\n4\nExperimental Setup\n15\n4.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2\nImplementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.3\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n5\nConclusion and Perspectives\n26\n6\nReferences\n27\nA Appendix\n29\nUnsupervised Domain Adaptation with Deep Neural-Networks\n1 | Introduction\nWe start with necessary theory and motivation to understand some essential definitions\nand ideas associated with domain adaptation.\n1.1 | Theory\nDomain adaptation is a subfield of machine learning that deals with the problem of\ntransferring knowledge learned from one domain to another related but different domain.\nIn real-world scenarios, it is common to encounter situations where the data distribution\nof the target domain differs significantly from the source domain used to train a model.\nThis can lead to a significant drop in the performance of the model on the target domain.\nTo understand clearly what domain adaptation is about, we should start with transfer\nlearning. For this purpose, we should dig deeper into the theory. In these articles [1] and\n[2] you can find a high level overview of the theory that connects with domain adaptation.\nLet’s start with the transfer learning definition and types that it consists of.\nDefinition 1.1. (Transfer learning) We consider a source data distribution S called the\nsource domain, and a target data distribution T called the target domain. Let XS × YS be\nthe source input and output spaces associated to S, and XT × YT be the target input and\noutput spaces associated to T. We use SX and TX to denote the marginal distributions of\nXS and XT , tS and tT to denote the source and target learning tasks depending on YS\nand YT, respectively. Then, transfer learning aims to help to improve the learning of the\ntarget predictive function fT : XT −→YT for tT using knowledge gained from S and tS ,\nwhere S = T.\nAccording to these papers ([1], [2]), transfer learning algorithms can be classified into\nthree categories based on the differences between the source and target tasks and domains:\ninductive, transductive, and unsupervised transfer learning.\n■Inductive transfer learning involves using labeled data from the source domain\nto train a model for a different, but related, target task in the target domain. In this\ncase, some labeled data from the target domain is required to fine-tune the model.\n■Transductive transfer learning, on the other hand, refers to using both labeled\ndata from the source domain and unlabeled data from the target domain to improve\nthe model’s performance on the target domain. In this case, the tasks remain the\nsame while the domains are different.\n■Unsupervised transfer learning involves adapting a model trained on the source\ntask to perform well on a related, but different target task in the target domain,\nwithout any labeled data in either the source or target domains.\nDomain adaptation is a type of transfer learning where the target task remains the same as\nthe source task, but the domain differs (the second type – transductive transfer learning).\nDepending on whether the feature spaces remain the same or differ, domain adaptation is\ncategorized into homogeneous and heterogeneous domain adaptation. Machine learning\ntechniques are commonly categorized based on the availability of labeled training data, such\nPage 1\nUnsupervised Domain Adaptation with Deep Neural-Networks\nas supervised, semi-supervised, and unsupervised learning. However, domain adaptation\nassumes the availability of data from both the source and target domains, making it\nambiguous to append one of these three terms to ”domain adaptation”. There are different\nways how these terms can be applied to domain adaptation, but we use the same as in [2].\n■Unsupervised domain adaptation refers to the case where both labeled source\ndata and unlabeled target data are available\n■Semi-supervised domain adaptation refers to the case where labeled source\ndata and some labeled target data are available\n■Supervised domain adaptation refers to the case where both labeled source and\ntarget data are available.\nUnsupervised Domain adaptation could be applied to a wide range of tasks in NLP [3, 4],\nin vision [5] and in many other applications where assigning labels to examples is tedious\nor impossible.\nThis report is more focused on studying unsupervised domain adaptation with using\ndeep neural-networks. Before move on to the practical part, it is important to discuss\ntheoretical analysis and guarantees that can be used in fields associated with transfer\nlearning. Thus, there are several methods that allow you to analyze the generalization\ngap in machine learning [6]. One of the most popular approaches is the model complexity\napproach, which estimates the generalization bound by measuring the complexity of the\nhypothesis set, such as Vapnik-Chervonenkis (VC) dimension and Rademacher complexity.\nAnother approach is to use the stability of the supervised learning algorithm in relation to\nthe datasets. Stability is a measure of how much a change in a data point in the training\nset can affect the output of the algorithm. Both of these approaches have been used to\nanalyze the generalization bounds of transfer learning algorithms.\nIt is equally important to discuss distributions and what experts mean by shift when\nanalyzing transfer learning algorithms. Distribution refers to the set of all possible values\nof a random variable, and a shift refers to a change in the distribution of the data between\nthe source and target domains. Understanding the shift in the distribution of the data is\ncrucial in developing effective transfer learning algorithms, as it enables the selection of\nappropriate techniques for adapting the model to the target domain.\nUnsupervised domain adaptation (UDA) is a type of supervised learning that involves\ntraining a model using labeled source data and applying it to unlabeled target data, where\nthe distributions of the two domains differ. Let the source domain be represented by\n(xS, yS) = (xS\nk, yS\nk )mS\nk=1 , and the target domain be represented by xT = (xT\nk )mT\nk=1. The number\nof observations in the source and target domains are denoted by mS and mT respectively.\nThe main challenge of domain adaptation is to develop a predictor that performs well\nin the target domain by leveraging the similarities between the two domains. One way\nto accomplish this is by making assumptions about how the joint distribution P(X, Y )\nchanges across the domains. In the case of covariate shift, the marginal distribution\nP(X) changes while the conditional distribution P(Y |X) remains the same. However, in\nreal-world scenarios, P(Y |X) may also change, requiring further assumptions. One such\nassumption is that the joint distribution can be factored into P(Y ) and P(X|Y ), allowing\nchanges in P(Y ) and P(X|Y ) to be addressed independently. The problem is then brokee\ntudied a cial10.5555/3241691.324170810.5555/3241691.3241ution of features and labels.\nPage 2\nUnsupervised Domain Adaptation with Deep Neural-Networks\n1.2 | Motivation\nUnsupervised domain adaptation (UDA) is a technique used in machine learning where a\nmodel is trained on labeled data from a source domain that has similar characteristics\nto the target domain, but where the target domain lacks labeled data. The goal is to\ncreate a model that will perform well on the target domain despite not having labeled\ndata from that domain. In UDA, the source and target domains are not directly related,\nso the model has to learn how to generalize across domains.\nThe first reason to be engaged in this field is a scarcity of data. It is known that collecting\nlabeled data in the target domain can be expensive and time-consuming. UDA allows us to\nuse the available labeled data in the associated source domain to learn representations that\ngeneralize well to the target domain without requiring additional labeled data. Minimizing\nthe discrepancy between domains, the model can learn more robust and transferable\nrepresentations, which leads us to the second reason – improved generalization and domain\nrobustness. The last reason is that UDA allows models to adapt to new environments. I\nreckon that this is a common situation in real applications, when models are trained on\nspecially prepared data, and then applied to all other data types.\nPage 3\nUnsupervised Domain Adaptation with Deep Neural-Networks\n2 | State-of-the-art\nIn this section, we discuss main purposes, approaches and algorithms that specialists in\nthe field of domain adaptation use in their research. In this section all figures are taken\nfrom the articles.\n2.1 | UDA by Backpropagation\nThe purpose of the article ”Unsupervised Domain Adaptation by Backpropagation” written\nby Yaroslav Ganin and Victor Lempitsky [7] is to tackle the problem of domain shift in\nmachine learning and to propose a solution to this problem using a neural-network model\nwith few standard layers and gradient reversal layer (GRL). The GRL makes the network\nto learn domain-invariant features by minimizing the difference between the distributions\nof the source and target domains. The architecture of the model is shown below (see\nFigure 1)\nFigure 1: The proposed architecture includes a deep feature extractor (shown in green),\na deep label predictor (shown in blue) and a domain classifier (shown in red). The domain\nclassifier is connected to the feature extractor via a gradient reversal layer, which multiplies\nthe incoming gradient by a negative constant during backpropagation-based training.\nThe gradient reversal layer ensures that the feature distributions over the two domains\nbecome as similar as possible (i.e., indistinguishable by the domain classifier), resulting in\ndomain-invariant features.\nThe authors introduce an architecture that predicts both the label y ∈Y and the domain\nlabel d ∈{0, 1} for each input x. The architecture consists of three parts: feature extractor\nf = Gf(x, θf), where θf is a vector that represents the parameters of all its layers; label\npredictor Gy that maps the features obtained after feature extractor to the label y, with\nθy representing its parameters; domain classifier Gd maps the same feature vector f to the\ndomain label d, with θd representing its parameters. The purpose to minimize the label\nprediction loss for the source domain and simultaneously make the features f invariant\nto the domain. To achieve this, the authors optimize the parameters θf of the feature\nmapping to maximize the loss of the domain classifier, however the parameters θd are\noptimized to minimize the loss of the domain classifier. The authors consider the loss\nPage 4\nUnsupervised Domain Adaptation with Deep Neural-Networks\nE(θf, θy, θd) =\nX\ni=1..N\ndi=0\nLy(Gy(Gf(xi; θf); θy), yi) −λ\nX\ni=1..N\nLd(Gd(Gf(xi; θf); θd), yi)\n=\nX\ni=0..N\ndi=0\nLi\ny(θf, θy) −λ\nX\ni=1..N\nLi\nd(θf, θd)\n(1)\nwhere Ly and Ld are label prediction and domain classification losses, respectively. (index\ni means the i-th example). It is considered the parameters ˆθf, ˆθy, ˆθd to gain a saddle point\n(ˆθf, ˆθy) = arg min\nθf,θy E(θf, θy, ˆθd)\nˆθd = arg max\nθd E(ˆθf, ˆθy, θd)\n(2)\nDuring learning, the trade-off between the two objectives that shape the features is\ncontrolled by the parameter λ. The following stochastic updates can find a saddle point\nθf ←θf −µ\n\u0012∂Li\ny\n∂θf\n−λ∂Li\nd\n∂θf\n\u0013\nθy ←θy −µ∂Li\ny\n∂θy\nθd ←θd −µ∂Li\nd\n∂θd\n(3)\nwhere µ is a learning rate. These updates are similar to SGD but with a −λ factor in the\nfirst update to prevent dissimilar features across domains. Therefore, the authors introduce\na GRL that acts as an identity transform during forward propagation but multiplies the\ngradient by −λ during backpropagation.\n2.2 | Semantic Representations for UDA\nNext, we continue with the article ”Learning Semantic Representations for Unsupervised\nDomain Adaptation” written by Xie, Shaoan, et al. [8] The main purpose of the article\nis to propose a new method for unsupervised domain adaptation that utilizes semantic\ninformation to learn domain-invariant representations. The authors propose a domain\nadaptation algorithm which is based on the idea of using an adversarial learning to learn\na feature representation that is invariant to domain shifts.\nPage 5\nUnsupervised Domain Adaptation with Deep Neural-Networks\nFigure 2: The authors use standard source classification loss with the domain adversarial\nloss to align distribution for two domains. It is showed that the performance of the domain\nadaptation method improved significantly on several benchmark datasets by aligning the\ncentroids. Global centroids Ck\nS and Ck\nT is maintained for each class at feature level.\nThe authors train a feature extractor and then use it to map the input data to a high-\ndimensional feature space, and a domain classifier that predicts the domain label of the\ninput data (see Figure 2). The feature extractor G is trained to confuse the domain\nclassifier D, while the domain classifier is trained to correctly predict the domain label. In\nthis way, the feature extractor is encouraged to learn features that are invariant to domain\nshifts, while still being discriminative for the task.\nFirst, the authors denote the cross entropy loss for the source domain as LC(XS, YS).\nThen, the discrepancy between source domain and target domain is supposed to be\nLDC(XS, XT) = d(XS, XT) = Ex∼DS[log(1 −D ◦G(x))] + Ex∼DT [log(D ◦G(x))]\n(4)\nMoreover, the authors introduce one more loss, which targets the semantic representation.\nCentroid alignment is used for this purpose. By computing the centroid for each class,\nboth correct and incorrect pseudo-labeled samples are utilized together:\nLSM(XS, YS, XT) =\nK\nX\nk=1\nΦ(Ck\nS, Ck\nT)\n(5)\nwhere Ck\nS, Ck\nT are centroids for each class and Φ(x, x′) = ∥x −x′∥2. This approach aims\nto cancel out the negative effects caused by inaccurate pseudo labels with accurate ones.\nThus, the authors get the following total loss\nL(XS, YS, XT) = LC(XS, YS) + λLDC(XS, XT) + γLSM(XS, YS, XT)\n(6)\nwhere λ and γ are responsible for the balance between the classification loss, domain\nconfusion loss and semantic loss. In the article, algorithm of moving average centroid\nalignment is presented that allows to align the centroids in same class but different\ndomains to achieve semantic transfer for UDA.\nPage 6\nUnsupervised Domain Adaptation with Deep Neural-Networks\n2.3 | Fixbi for UDA\nThe purpose of the article ”Fixbi: Bridging domain spaces for unsupervised domain\nadaptation” written by Jaemin Na, Heechul Jung et al. [9] is to propose a fixed ratio-based\nmixup method to address the problem of large domain discrepancies. The authors mix up\nimages and then fed them into neural networks to achieve greater reliability in learning\nfrom corrupted labels. It is proposed to use two predetermined mixup ratios λsd and λtd\nfor the source and target domain respectively. Denote input samples and their labels for\nsource and target domain as (xs\ni, ys\ni ) and (xt\ni, ˆyt\ni), the authors define mixup configurations\nin the following way:\n˜xst\ni = λxs\ni + (1 −λ)xt\ni\n˜yst\ni = λys\ni + (1 −λ)ˆyt\ni,\n(7)\nwhere λ ∈{λsd, λtd} and λsd + λtd = 1, ˆyt\ni is the pseudo-labels for the target samples.\nBy leveraging the fixed ratio-based mixup, it is constructed two neural networks with\ndifferent perspectives: the ”source-dominant model” (SDM) and the ”target-dominant\nmodel” (TDM) (see Figure 3).\nFigure 3: All parts of FixBi training model: (a) fixed ratio-based mixup, (b) confidence-\nbased learning and (c) consistency regularization.\nThe SDM provides robust supervision for the source domain but relatively weak supervision\nfor the target domain, while the TDM has strong supervision for the target domain but\nweaker supervision for the source domain. Thus, denoting p(y|˜xst\ni ) as a predicted class\ndistribution, it is defined fixed ratio-based mixup function\nLfm = 1\nB\nB\nX\ni=1\nˆyst\ni log(p(y|˜xst\ni )),\n(8)\nwhere ˆyst\ni = arg max p(y|˜xst\ni ) and B is the size of a mini-batch. In order to have connections\nbetween source and target domains, it is suggested to use a confidence-based learning\napproach whereby one model educates the other using positive pseudo-labels, or penalties\nitself using negative pseudo-labels. Positive pseudo-labels means labels which predictions\nare above a specific threshold, then the authors use them in training the second model by\nutilizing a conventional cross-entropy loss. Thus, denote p and q as distributions of two\nmodels, the authors get the following loss function\nPage 7\nUnsupervised Domain Adaptation with Deep Neural-Networks\nLbim = 1\nB\nB\nX\ni=1\n1(max(p(y|xt\ni) > τ)ˆyt\ni log(q(y|xt\ni)),\n(9)\nwhere ˆyt\ni = arg max p(y|xt\ni). In contrast, a negative pseudo-label refers to the top-1 label\npredicted by the network with a confidence below the threshold τ. The function of\nself-penalization is defined as follows:\nLsp = 1\nB\nB\nX\ni=1\n1(max(p(y|xt\ni) < τ)ˆyt\ni log(1 −p(y|xt\ni))\n(10)\nFurthermore, the threshold is changed adaptively during training. In addition, it is\nintroduced the following expression:\nLcr = 1\nB\nB\nX\ni=1\n∥p(y|˜xst\ni ) −q(y|˜xst\ni )∥2\n2\n(11)\nthat represents consistency regularization to guarantee a stable convergence during the\ntraining of both models.\n2.4 | Spherical Space DA with Pseudo-label Loss\nNow we are going to the next article ”Spherical Space Domain Adaptation with Robust\nPseudo-label Loss” written by Xiang Gu, Jian Sun, and Zongben Xu. [10] The authors\npropose a spherical space representation of data, which allows them to get more effective\nfeature extraction and better adaptation across domains. One approach associated with\nincreasing performance with differences in data distribution between source and target\ndomain is to use pseudo-labels. However, the use of pseudo-labels can be problematic in\nthe presence of noisy or incorrect labels. To tackle this problem, the authors map the\ndata to a high-dimensional sphere and introduce a new loss function, called the robust\npseudo-label loss, which is designed to address the problem of noisy or incorrect labels in\nthe target domain.\nFigure 4: Robust Spherical Domain Adaptation (RSDA) method proposed by the authors\nconsists of a feature extractor F, which is a deep convolutional network, used to extract\nfeatures that are then embedded onto a sphere, a spherical classifier and a discriminator\nthat predict class labels and domain labels, respectively. Target pseudo-labels and features\npassing through the Gaussian-uniform mixture model are used to estimate the posterior\nprobability of correct labeling.\nPage 8\nUnsupervised Domain Adaptation with Deep Neural-Networks\nIn the Figure 4 we can see spherical domain adaptation method. Domain invariant features\nare learned by adversarial training, entirely in the spherical feature space. The feature\nextractor F is utilized to normalize the features to map onto a sphere. The classifier C\nand discriminator D are defined in the spherical feature space, consisting of spherical\nperceptron layers and a spherical logistic regression layer (see Figure 5).\nFigure 5: Spherical neural network structure\nAlthough the use of a spherical element reduces feature dimension by one, it simplifies\nthe domain adaptation process by eliminating differences in norms. The authors define\nspherical adversarial training loss as follows:\nL = Lbas(F, C, D) + Lrob(F, C, ϕ) + γLent(F)\n(12)\nThis lost consists of three parts: basic loss, robust pseudo-label loss and conditional entropy\nloss. Let’s start with the first one. To align features, the authors utilize basic loss which is\ndefined as an adversarial domain adaptation loss:\nLbas(F, C, D) = Lsrc(F, C) + λLadv(F, D) + λ′Lsm(F),\n(13)\nwhere Lsrc is a cross-entropy loss for the source domain, Ladv is an adversarial training\nloss and Lsm is a semantic loss. The second one is conditional entropy loss which is used\nto keep the learned features away from the classification boundary:\nLent(F) = 1\nNt\nNt\nX\nj=1\nH(C(F(xt\nj))),\n(14)\nwhere H(·) denotes the entropy of a distribution. Additionally, the authors propose robust\npseudo-label loss to increase robustness of the model. Denote ˜yt\nj = arg maxk[C(F(xs\ni))]k\nas a pseudo-lebel of xt\nj where [·]k means the k-th element. To be ensured in precision of\npseudo-labels, it is assumed to use new random variable zj ∈{0, 1} for each pair (xt\nj, ˜yt\nj)\nthat specify the correctness of the data (1 is correct, 0 is not). Let the probability of\ncorrect labeling be Pϕ(zj = 1|xt\nj, ˜yt\nj) and ϕ to its parameters, then robust loss is defined as\nfollows:\nLrob(F, C, ϕ) = 1\nN0\nNt\nX\nj=1\nwϕ(xt\nj)J (C(F(xt\nj)), ˜yt\nj),\n(15)\nwhere N0 = PNt\nj=1 wϕ(xt\nj) and J (·, ·) is mean absolute error (MAE). The function wϕ(xt\nj)\nis defined using the posterior probability of correct labeling\nPage 9\nUnsupervised Domain Adaptation with Deep Neural-Networks\nwϕ(xt\nj) =\n(\nγj, if γj ≥0.5,\n0, otherwise,\n(16)\nwhere γj = Pϕ(zj = 1|xt\nj, ˜yt\nj). By utilizing a Gaussian-uniform mixture model in spherical\nspace based on pseudo-labels, the authors model the probability Pϕ(zj = 1|xt\nj, ˜yt\nj) as a\nfunction of the feature distance between the data and the center of the corresponding\nclass. Thus, samples from target domain with a probability of correct labeling below 0.5\ncan be discarded. For further details of computing posterior probability, please refer to\nthe article [10].\n2.5 | DA with Invariant Representation Learning\nNext, we move on to the article ”Domain Adaptation with Invariant Representation\nLearning: What Transformations to Learn?” written by Stojanov, Petar, et al. [11] The\nresearchers focus on the conditional shift scenario, where the data-generating process\nis utilized to (i) explain why two distinct encoding functions are required to infer the\nlatent representation, (ii) improve an implementation of these functions, and (iii) impose\nmeaningful structure on the latent representation Z to increase prediction accuracy in the\ntarget domain.\nFigure 6: Data-generating process under conditional shift\nLet’s consider the data-generating process shown in the Figure 6 to understand what\ninformation is required for learning. The label Y is generated first from its prior distribution\nP(Y ). Then, the invariant representation Z is generated from Y through P(Z|Y ), and X\nis generated from P(X|Z; θX), where θX represents the changing parameters of P(X|Y )\nacross domains. We can consider Z as a latent representation of our data. The variable\nθX may correspond to environment-specific changes that are irrelevant for predicting the\nclass Y . Generally speaking, Z is conditionally dependent on θX given X, although they\nmay be marginally independent. Therefore, to recover Z given X, the information of θX\nshould also be considered in the transformation (see detailed in the article to understand\nclearly how authors measure the influence of θX). The authors made two key observation\nassociated with the data-generating process. Firstly, the encoder function ϕ requires θX\nas an input in addition to X. Secondly, assuming that θX has minimal influence on the\nrelationship between X and Z, allowing us to use a single encoder ϕ(X, θX) instead of\ntwo separate encoders. A decoder function eϕ that restricts the influence of θX, acting as a\nregularizer on the encoder ϕ, in order to retain important semantic information.\nPage 10\nUnsupervised Domain Adaptation with Deep Neural-Networks\nFigure 7: Autoencoder framework used in this article\nThus, the authors proposed a domain-adaptation network, which is shown in the Figure 7,\nwhere θX ∈{θS\nX, θT\nX} parameters for source and target domains respectively.\n2.6 | Domain Adaptation for Segmentation with CBST\nThe main purpose of the paper ”Domain Adaptation for Semantic Segmentation via Class-\nBalanced Self-Training” written by Zou, Yang, et al. [12] propose a new UDA framework\nfor semantic segmentation based on iterative self-training procedure. A novel technique,\nreferred to as Class-Balanced Self-Training (CBST), has been suggested by the authors,\nwhich aims to adapt the segmentation model from the source domain to the target domain\nby leveraging unlabeled target data. In the Figure 8, the authors present a structure\nand the results of their deep self-training framework using two datasets: GTA 5 [13] and\nCityscapes [14].\nFigure 8: On the left side, the self-training framework for UDA is presented. On the\nright side, obtained results before and after adaptation for the Cityscapes dataset.\nThe CBST approach is based on two main components: a class-balancing strategy and\na self-training algorithm. The class-balancing strategy aims to address the problem of\nPage 11\nUnsupervised Domain Adaptation with Deep Neural-Networks\nclass imbalance between the source and target domains, which can negatively impact\nthe performance of the segmentation model. The authors change the loss function using\nparameters that determine the proportion of selected pseudo-labels due to balance the\nclass distribution during the self-training process. Furthermore, when the images in the\nsource and target domains are similar, spatial prior knowledge can be effectively utilized\nto adapt models. For this purpose, the authors count the class frequencies in the source\ndomain using Gaussian kernel. The experimental results show that the CBST approach\noutperforms several state-of-the-art unsupervised domain adaptation methods for semantic\nsegmentation.\nPage 12\nUnsupervised Domain Adaptation with Deep Neural-Networks\n3 | Contribution\nThe ”Contribution” section of the thesis highlights the developed new method called\nDannFixbi, which combines the Fixbi approach and the backpropagation approach. In an\nattempt to implement the state-of-the-art method, extensive research has been conducted,\nand existing approaches have been implemented. The decision to combine these two ap-\nproaches was based on their respective strengths and the potential for mutually beneficial\ninteraction between them. By incorporating the Fixbi technique, which addresses domain\nshift, and leveraging the benefits of backpropagation, DannFixbi aims to enhance the\nperformance and robustness of domain adaptation in the field of images. The development\nof DannFixbi represents an original contribution to the field.\nThis new method consists of two neural networks, which are trained using a modified\nversion of the Fixbi approach. To enhance the performance of this method, two domain\nclassifiers are added to each of these two networks. Two different approaches have been\nexplored for incorporating these domain classifiers.\nThe first approach involves adding a domain classifier to each neural network (see Figure\n9). During training, images obtained by mixing from the source and target domains with\npredefined mixup ratios are fed into these classifiers.\nFigure 9: DannFixbi architecture of the first approach.\nFor mixed images, the following loss functions is used:\nLdom = αLds( ˆX, Ys) + (1 −α)Ldt( ˆX, Yt),\n(17)\nwhere ˆX is mixed images, Ys is source domain labels, Yt is target domain labels and\nα ∈{λsd, λtd}. Lds and Ldt presents cross entropy loss for source and target domains,\nrespectively.\nThe second approach is to use a domain classifier for each net with images from the source\nand target domains without any mixing, similar to the backpropagation method (see\nFigure 10).\nPage 13\nUnsupervised Domain Adaptation with Deep Neural-Networks\nFigure 10: DannFixbi architecture of the second approach.\nThe second approach utilizes the following loss function for domain classification:\nLdom = Ld(Xst, Yst),\n(18)\nwhere Xst, Yst denotes source and target images and domain labels, respectively, and Ld is\na cross entropy loss.\nThe total loss for the new method is calculated as the sum of the loss from the Fixbi\nmethod and the domain loss described earlier:\nLtotal = βLfixbi + γLdom\n(19)\nHere, β and γ represent the weights assigned to the Fixbi loss and the domain loss,\nrespectively.\nThe values of these weights determine the relative importance of each\ncomponent in the overall loss calculation. Further, unless otherwise stated, the values\nof alpha and beta are assumed to be equal to 1. The Fixbi loss, denoted as Lfixbi, is\ncomposed of several summands described in the equations 8 – 11:\nLfixbi = Lfm + Lsp + 1{e > k} (Lbim + Lcr)\n(20)\nwhere e denotes a current epoch, and k is warm-up epochs. To establish independent\ncharacteristics for the two networks, it is introduced a warm-up period of k epochs. During\nthis phase, each network is trained separately using the fixed ratio-based mixup and\nself-penalization techniques. Once an enough amount of training has been completed,\nbidirectional matching loss is added, which helps networks train collaboratively, exchanging\nknowledge and benefiting from each other’s insights.\nThe new method, referred to as DannFixbi, demonstrates improved performance and\nrobustness in unsupervised domain adaptation for image analysis. This contribution\nrepresents a unique approach to UDA, offering valuable insights and potential for future\ndevelopments in this field. All the results obtained are presented in the ”Experiments”\nsection.\nPage 14\nUnsupervised Domain Adaptation with Deep Neural-Networks\n4 | Experimental Setup\nIn this part, we start with description of different datasets that are commonly used in\ntransfer learning. Then, we will continue with implementation details and experiments that\nhave been conducted. Code is available at https://github.com/Jetwev/domain-adaptation.\n4.1 | Datasets\nThe most popular datasets are Office-31, ImageCLEF-DA, Office-Home, DomainNet\nand VisDA-2017. Detailed discussion of each of them is given below:\n■Office-31 [15] consists of 4,110 images categorized into 31 classes, which are dis-\ntributed across three separate domains: Amazon (A), Webcam (W), and Dslr (D).\n■ImageCLEF-DA, utilized in [16], includes three distinct domains: Caltech-256 (C),\nImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P). There are 600 images in\neach domain and 50 images for each category.\n■Office-Home [17], includes four absolutely different domains: Artistic images (Ar),\nClip Art (Cl), Product images (Pr) and Real-World images (Rw). This dataset\ncontains 15 500 images in 65 object classes, which makes it more complex than\nOffice-31.\n■VisDA-2017 [18] consists of 12 classes shared between two very different domains:\nSynthetic and Real. It contains synthetic images (training set) and real-world images\n(test set). The dataset was designed to have a large domain gap, which makes it a\nchallenging benchmark for domain adaptation methods.\n■DomainNet [19] is a large-scale visual recognition dataset designed to evaluate\ndomain adaptation algorithms, which consists of almost 600 thousand images and\nincludes 345 classes.\nPage 15\nUnsupervised Domain Adaptation with Deep Neural-Networks\n4.2 | Implementation details\nAt the beginning, it was necessary to start with some approaches to check their performance\nand have a possibility to compare results. Four different methods described in the papers\nhave been chosen for the study:\n■Source only is a method where a model is trained solely on the source domain data\nwithout any adaptation to the target domain.\n■Domain-Adversarial Neural Network (Dann) is domain adaptation technique\nthat aims to learn a domain-invariant feature representation by aligning the feature\ndistributions of the source and target domains. The architecture of Dann consists\nof three components: a feature extractor network, a label predictor network, and a\ndomain classifier network, and is described in more details in section 2.1.\n■Moving Semantic Transfer Network (Mstn) The key idea behind Mstn is to\nadd semantic transfer loss to the Dann approach. In the section 2.2, it is proposed\nto use average centroid alignment for aligning the feature distributions of the source\nand target domains. The architecture is the same as in the Dann method.\n■Fixbi is the approach described in details in the section 2.3. The main idea is to train\ntwo neural networks, allowing models to learn from each other or on their own results.\nFor this purpose, the authors add bidirectional matching and self-penalization losses.\nCNN architectures. For all approaches, pretrained Resnet50 [20] is utilized as the\nbackbone network. The weights for the neural network can be downloaded from this\nlink. Resnet50 has been pretrained on large image datasets such as ImageNet, which\nmeans that the network has already learned to recognize a wide range of features in\nimages. Resnet50 is a convolutional neural network architecture consisting of 50 layers.\nThis is a variant of the Resnet family of neural networks, which are designed to solve\nthe vanishing gradient problem in deep neural networks. Resnet networks achieve this by\nusing short connections between layers, which allow gradients to move more easily during\nbackpropagation. Resnet50 is a widely used architecture in many articles, which makes it\na good choice for research.\nResnet50 is used as a Feature extractor in all considering methods. Label predictor is a sim-\nple network that consists of two fully connected layers (2048 →256 →number of classes).\nDomain classifier architecture represents several fully connected layers with a ReLU\nactivation function and dropouts between each two fully connected layers. Using dropouts\ncan reduce the sensitivity of the model to specific features in the input data and encourage\nthe model to learn more generalizable features. This can lead to better performance on\nnew, unseen data and can prevent overfitting.\nLearning rate schedulers. Learning rate is an important hyperparameter that deter-\nmines the step size at which the optimizer updates the model’s parameters during training.\nThere are many of them and it can be challenging to find the optimal learning rate, as\nsetting it too high can cause the model to diverge, while setting it too low can slow down\nthe learning process. Thus, in this study it is utilized two different learning rate schedulers:\nCustomLRScheduler and CosineAnnealingLR. The implementation of the first one follows\nthe rules that are described in [7]\nPage 16\nUnsupervised Domain Adaptation with Deep Neural-Networks\nηp =\nη0\n(1 + α · p)β ,\n(21)\nwhere p linearly increases from 0 to 1, and the values η0, α, and β are set to 0.01, 10, and\n0.75, respectively. The second one, CosineAnnealingLR is a popular learning rate scheduler\nutilized in deep learning. It systematically reduces the learning rate over multiple epochs\nin a cyclical manner. Initially, the learning rate starts at its maximum value and then\ngradually decreases until it reaches the minimum value. Upon reaching the minimum\nvalue, the cycle restarts, and the learning rate returns to its maximum value. This process\ncontinues until the end of the training, which is usually determined by the total number of\nepochs or a predefined stop criterion. By starting with a higher learning rate and gradually\ndecreasing it, the model can avoid getting stuck in local minima and converge to a better\nglobal minimum. The formula for the CosineAnnealingLR scheduler is:\nηt = ηmin + 1\n2(ηmax −ηmin)\n\u0012\n1 + cos\n\u0012 Tcur\nTmax\nπ\n\u0013\u0013\n,\n(22)\nwhere ηmax is your initial learning rate, ηmin – minimum learning rate value, Tcur is the\nnumber of epochs since the last start, Tmax – the total number of epochs. More detailed\ninformation about CosineAnnealingLR can be found here.\nOptimizers. This study uses two popular optimization algorithms - stochastic gradient\ndescent (SGD) and adaptive moment estimation (Adam). Both algorithms are commonly\nemployed in deep learning to optimize the parameters of a neural network and improve its\nperformance.\nSGD is a simple and popular optimization algorithm that updates the weights of a model\nin the direction of the negative gradient of the loss function. One limitation of SGD is that\nit can get stuck in local minima and struggle with noisy or sparse gradients. To tackle this\nproblem, several modifications can be used. In PyTorch, the SGD optimizer has several\nhyperparameters that can be tuned to improve its performance. The following parameters\n(except learning rate) are considered in this study:\n■Momentum is a hyperparameter that determines how much past gradients affect\nthe current gradient update. It helps to minimize the impact of the noise and\nfluctuations in the gradient updates. However, setting the momentum too high can\nalso lead to slower convergence.\n■Weight decay is a form of L2 regularization that adds a penalty term to the loss\nfunction during training. This penalty term is proportional to the square of the\nweights in the network, which encourages the model to use smaller weights and\nreduce overfitting.\n■Nesterov momentum is a variant of momentum that takes into account the\nmomentum term in the calculation of the gradient. This can help to reduce oscillations\nand improve convergence rates, especially in high-dimensional optimization problems.\nAdam is another optimization algorithm that is commonly used in deep learning. It is\nan extension of SGD. The key idea behind Adam is to maintain a separate adaptive\nlearning rate for each parameter in the network, based on estimates of the first and second\nPage 17\nUnsupervised Domain Adaptation with Deep Neural-Networks\nmoments of the gradients. This makes Adam more effective than SGD for optimization\nproblems with noisy or sparse gradients. However, it may not always be the best choice\nfor every task and model architecture, so it’s important to experiment with different\noptimization algorithms and settings to find the best approach for your specific problem.\nAdam is considered in this study with default parameters, more information about the\nimplementation and usage can be found at this link.\nPytorch Lightning. PyTorch Lightning is a lightweight PyTorch wrapper that allows\nusers to focus on the high-level design of their experiments and models, instead of dealing\nwith the low-level implementation details. It provides a structured way to organize PyTorch\ncode, making it easier to read and maintain.\nPyTorch Lightning offers a range of benefits that make it a good choice for deep learning\nresearches. Firstly, it offers a modular design that makes it easy to organize code. It gives\nyou a convenient and user-friendly interface to manage and run experiments. Moreover, all\nthese benefits can help to improve your productivity. Secondly, PyTorch Lightning makes\nit easier to scale models to multiple GPUs, which can significantly reduce training times\nfor large models. Finally, it is flexible and can be easily integrated with other PyTorch\nlibraries. Overall, PyTorch Lightning is an excellent choice for researchers who want to\nfocus on the research aspect of deep learning and leave the engineering components to the\nlibrary. More information can be found on the official website.\nWeights and Biases. Weights and Biases (WandB) is a platform that provides a suite\nof tools to help developers and data scientists track and visualize their machine learning\nexperiments. WandB makes it easy to log, keep track of your progress and compare\ndifferent experiments, visualize model performance, and collaborate with team members.\nOne of the main advantages of WandB is its integration with popular machine learning\nframeworks such as TensorFlow, PyTorch, and Keras. This means that you can easily log\nand track your model’s hyperparameters and performance metrics during training and\nevaluation. Moreover, WandB is a cloud-based platform, which means that users can access\ntheir experiments and data from anywhere with an internet connection and also share\nthem with colleagues and co-workers. For more detailed information, it is recommended\nto visit the official website.\nBatch size. Different domains in your dataset can contain different number of images\nthat makes your training process more complicated. To tackle this problem, it is proposed\ntwo approaches.\nThe first one is to find the ratio of the smaller dataset size to the larger one and concate-\nnate the smaller dataset multiple times to ensure that the number of batches is aligned\nduring the training loop. However, it is important to emphasize that with this approach,\noverfitting can occur if the appropriate number of epochs is not established. This is\nbecause the smaller dataset will be fed into the model more times than the larger one\n(depends on the ratio).\nThe second approach involves varying the number of images taken per batch for each\ndomain. Applying this approach, it becomes possible to avoid concatenating the smaller\ndataset multiple times, which effectively reduces the amount of memory consumed. It is\nPage 18\nUnsupervised Domain Adaptation with Deep Neural-Networks\ncrucial to carefully consider the number of images per batch, as choosing a value that is\neither too high or too low can have negative consequences.\nAugmentation techniques. Augmentation techniques for images are used to create\nvariations and increase the size of the training dataset by applying a series of transforma-\ntions. These techniques are widely employed in computer vision tasks, including image\nclassification, object detection, semantic segmentation, etc. Augmentation helps increase\nthe diversity of the dataset, leading to improved model generalization and robustness. Py-\nTorch provides a variety of image augmentation techniques. The following transformations\nare used in this research:\n■Normalize – normalizes the image by subtracting the mean value and dividing by\nthe standard deviation.\n■Resize is a function that allows you to resize an image to a specific size.\n■RandomCrop is a function that randomly crops a portion of the image.\n■CenterCrop is a transformation that allows you to perform a center crop on an\nimage.\n■RandomHorizontalFlip – randomly flips the image horizontally with a specified\nprobability.\n■RandomVerticalFlip - randomly flips the image vertically with a specified proba-\nbility.\n■RandomRotation is a function that randomly rotates the image by a given angle.\n■ColorJitter is a transformation that allows you to adjust the brightness, contrast,\nsaturation, and hue of the image.\n■ToTensor is a specific function in PyTorch that is used to convert an image into a\ntensor.\nThese augmentation techniques can be applied individually or combined sequentially using\nthe transforms.Compose function. More detailed information about transformations and\ntheir usage can be found here. It’s important to emphasize that the choice and combination\nof augmentation techniques depend on the specific task and dataset characteristics, and\ncareful selection of them are crucial to achieve optimal results.\nOmegaConf. OmegaConf is a library for Python that provides a convenient and flexible\nway to manage complex configurations in machine learning projects. It is designed to\nprovides a number of features that can help to simplify the configuration process. Here\nare some reasons why OmegaConf can be a good choice:\n■It is easy to use and allows developers to define nested configurations and easily\naccess and modify configuration values.\n■OmegaConf supports a wide range of configuration formats, including YAML and\nJSON. This makes it flexible and easy to integrate in your project.\nPage 19\nUnsupervised Domain Adaptation with Deep Neural-Networks\n■It supports type checking, which can help to catch configuration errors and improve\ncode quality.\nTo sum up, OmegaConf can be a good choice for Python developers who work on large\nand complex projects and want a flexible and powerful configuration system for their\napplications. Additional details can be found here.\nPage 20\nUnsupervised Domain Adaptation with Deep Neural-Networks\n4.3 | Experiments\nThe dataset Office-31 is used to test the approaches. This dataset consists of three\ndomains: Amazon (A) - 2817 images, Dslr (D) - 498 images and Webcam (W) - 795 images\n(see Figure 11).\nFigure 11: Different domains in dataset Office-31.\nThe Table 1 below highlights various key features of the dataset, including the number of\nclasses, image resolution, task and evaluation metric.\nTable 1: General information about the Office-31 dataset\nFeature\nDescription\nDataset Name\nOffice-31\nPurpose\nDomain adaptation and object recognition\nNumber of Classes\n31\nDomains\nAmazon, Webcam, DSLR\nImage Resolution\nVaries across domains and images\nImage Format\nJPG\nTask\nMulti-class classification\nEvaluation Metric\nClassification accuracy\nThe existence of dissimilar image quantities ensures us in the importance of utilizing one\nof the approaches discussed in the previous section in order to avoid any information loss.\nIn the first approach, where the smaller domain is concatenated, a batch size of 32 or 64\nis utilized for all experiments. The second approach takes into account the size of each\ndomain, and as a result, the batch sizes are utilized in the experiments according to the\nTable 2:\nPage 21\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 2: The table of batch sizes is organized such that the numerical values in the first\ncolumn correspond to the first letter, the second one to the second letter.\nBatch size\nAD\n45\n8\nAW\n45\n13\nDA\n8\n45\nDW\n20\n32\nWA\n13\n45\nWD\n32\n20\nFor the all methods, two kinds of optimizers are used: SGD and Adam. However, the\nsecond one shows worse results with default parameters than SGD with lr = 0.001, momen-\ntum = 0.9, weight decay = 0.0005. The CustomLRScheduler and CosineAnnealingLR are\nboth used as schedulers, but it has been found that the model performs better when using\nthe second one. Thus, all the following results have been obtained using the CosineAnneal-\ningLR scheduler. Furthermore, all methods give the best results with an approach using a\ndifferent number of images in each batch. As a result, this approach will be assumed by\ndefault, unless otherwise stated. For each two domains, at least three experiments were\nconducted for all methods, and the best results were selected.\nLet’s start with the first method – Source only. Here, the model is trained on the source\ndomain and then tested on the target domain. The obtained results are shown in Table 3\n(at the end of the 60th epoch).\nTable 3: Accuracy on Office-31 for the Source only method.\nSource\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\n1\n81.46\n76.43\n95.96\n60.33\n99.58\n64.45\n2\n81.04\n76.04\n95.7\n60.09\n99.37\n64.17\n3\n80.62\n76.04\n95.7\n59.38\n99.37\n64.13\naverage\n81.04\n76.17\n95.79\n59.93\n99.44\n64.25\nDann is an architecture that consists not only of a feature extractor and label predictor,\nbut also a domain classifier. This domain classifier helps to identify the domain of the\ninput data and allows the model to learn domain-invariant features. The Table 4 below\nclearly demonstrates that the results for each of the two domains are superior to those\nobtained using the simple Source only method. The results are obtained at the end of\nthe 60th epoch.\nTable 4: Accuracy on Office-31 for the Dann method.\nDann\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\n1\n83.13\n78.91\n96.35\n64.35\n100\n65.38\n2\n82.92\n78.65\n95.96\n63.88\n100\n64.91\n3\n82.71\n78.65\n95.83\n63.92\n99.79\n64.74\naverage\n82.92\n78.74\n96.05\n64.05\n99.93\n65.01\nMstn method is a complication of Dann by adding a semantic loss. To get this loss,\nwe add centroids for each class and utilize the algorithm described in the section 1.2.2.\nPage 22\nUnsupervised Domain Adaptation with Deep Neural-Networks\nIn the Table 5, you can see the results that are acquired at the end of the 60th epoch.\nThe quality of the results tends to suffer due to the significant influence of randomness.\nThe selection of pictures that are included in a batch determines the movements of the\ncentroids, ultimately influencing the overall quality to a significant extent.\nTable 5: Accuracy on Office-31 for the Mstn method.\nMstn\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\n1\n77.71\n72.53\n92.06\n33.38\n99.79\n51.07\n2\n76.88\n72.4\n91.8\n32.1\n99.79\n48.58\n3\n76.67\n71.48\n91.41\n34.09\n99.79\n48.65\naverage\n77.09\n72.14\n91.76\n33.19\n99.79\n49.43\nFixbi is a method that addresses the domain adaptation problem by training two neural\nnetworks that can help each other. As it is described in the article [9], we define λsd = 0.7\nand λtd = 0.3. In this method, we cannot use the approach with different batch sizes\ndue to the need to mix up images from source and target domain. Therefore, the second\napproach with concatenation is utilized. The model is trained for a combined duration of\n150 epochs, with the first 100 epochs designated as the warm-up period. It is important to\nnote that 150 epochs are used, not 200, because after the warm-up period the validation\nscore stabilizes and almost does not change. After the warm-up period, Lbim starts to be\napplied, which leads to a critical changing in the total accuracy. The sudden improvement\nin accuracy can occur in either a positive or negative direction, and is often heavily\ninfluenced by randomness. One possible explanation for this phenomenon is that the\nmodel may have already found a local minimum prior to the introduction of Lbim, and the\napplication of Lbim causes a sudden shift in gradients that propels the model out of the\ncurrent minimum and into a new one. Depending on the new minimum, this can result\nin either an improvement or a deterioration in the model’s performance. As we can see\nin the Figure 12, for Amazon (source) and Webcam (target) domains this method gives\nsignificant increase in accuracy, while for DSLR (source) and Amazon (target) it shows\nthe worst results.\nFigure 12: The total accuracy of Fixbi model on Amazon and Webcam domains.\nIn the Figure 13, you can see the separate accuracy of “source-dominant model” (SDM)\nPage 23\nUnsupervised Domain Adaptation with Deep Neural-Networks\nand “target-dominant model” (TDM) in case of Amazon (source) and Webcam (target)\ndomains.\nFigure 13: The separate accuracy of Fixbi “source-dominant model” (SDM) on the left\nside and “target-dominant model” (TDM) on the right side.\nThe results for each two domains of Office-31 dataset for Fixbi method are shown in Table\n6.\nTable 6: Accuracy on Office-31 for the Fixbi method.\nFixbi\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\n1\n76.88\n87.11\n94.66\n23.19\n94.38\n30.15\n2\n70.1\n86.72\n90.89\n15.23\n94.38\n26.03\n3\n77.08\n81.77\n88.54\n16.01\n92.92\n20.63\n4\n73.96\n81.38\n92.78\n17.8\n92.88\n23.45\naverage\n74.51\n84.25\n91.72\n18.06\n93.64\n25.07\nThe Fixbi method was selected for modification, wherein the ratios for SDM and TDM\nwere adjusted, and a domain classifier was added for mixup images. This modified method\nis named DannFixbi. λsd and λtd are set as 0.9 and 0.7, respectively. As it was mentioned\nbefore, it is used one of the two approaches described in ”Contribution” section with\ndomain classifiers for mixup images or separately for each domain. The Table 7 indicates\nthat certain domains exhibit an increase in accuracy as a result of these changes.\nTable 7: Accuracy on Office-31 for the DannFixbi method.\nDannFixbi\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\n1\n86.87\n85.81\n97.53\n65.27\n99.37\n63.04\n2\n86.46\n86.07\n97.35\n64.81\n98.98\n63.29\n3\n87.29\n86.07\n97.44\n63.53\n99.17\n62.93\naverage\n86.87\n85.98\n97.44\n64.54\n99.17\n63.09\nTable 8 presents all the obtained results. The DannFixbi method yields the highest\naccuracy for the A→D, A→W, D→A and D→W tasks, while the Dann method achieves\nthe best results for the W→D, and W→A tasks.\nPage 24\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 8: Accuracy on Office-31 for all methods.\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\nSource\n81.04\n76.17\n95.79\n59.93\n99.44\n64.25\nDann\n82.92\n78.74\n96.05\n64.05\n99.93\n65.01\nMstn\n77.09\n72.14\n91,76\n33.19\n99.79\n49.43\nFixbi\n74.51\n84.25\n91.72\n18.06\n93.64\n25.07\nDannFixbi\n86.87\n85.98\n97.44\n64.54\n99.17\n63.09\nAdditionally, it is worth noting that an overall assessment of the methods across all\ndomains can be obtained by calculating the average accuracy (see Table 9).\nTable 9: Average accuracy across all domains\nAvg\nSource\n79.44\nDann\n81.12\nMstn\n70.57\nFixbi\n64.54\nDannFixbi\n82.85\nTo sum up, the new introduced method called DannFixbi outperforms all other methods\nin visual recognition. The Appendix A provides additional results for each domain, allowing\nfor comparisons using the Wilcoxon signed-rank test to determine the statistical significance\nof the findings (see Tables 10 – 21). The new method demonstrates statistically significant\nresults in three tasks: A→D, A→W, and D→W, while outperforming all other methods\non average (Tables 22 – 23).\nPage 25\nUnsupervised Domain Adaptation with Deep Neural-Networks\n5 | Conclusion and Perspectives\nIn this thesis, the focus was on exploring and implementing methods related to unsupervised\ndomain adaptation. The Office-31 dataset was utilized for evaluating these methods and\nconducting a comprehensive comparison. The results obtained from the experiments were\nanalyzed, leading to the development of the new DannFixbi method that demonstrated\nthe best performance compared to all the other methods presented.\nThe Office-31 dataset provided a suitable benchmark for evaluating the effectiveness of\nvarious unsupervised domain adaptation techniques. By conducting experiments on this\ndataset, the performance of different methods could be objectively assessed and compared.\nThe analysis of the results shows the strengths and weaknesses of each method, allowing a\ndeeper understanding of their capabilities.\nBased on the comparative analysis, it was observed that the newly developed method\nshowcased the best results among all the presented methods. The success of the new\nmethod can be attributed to its ability to leverage the strengths of existing techniques.\nBy combining the back propagation method with domain classifiers and applying the\nFixbi approach, it is possible to identify common features in different domains and share\nknowledge and insights between networks. This collaborative approach to learning has led\nto higher performance and increased the overall effectiveness of the method.\nOverall, this thesis contributes to the field of unsupervised domain adaptation by providing\nan analysis of existing methods, introducing a new approach, and demonstrating the\npotential for improving visual recognition tasks across different domains. The results of\nthis study open up opportunities for further study and development of advanced methods\nin the field of domain adaptation.\nBy addressing the challenge of distribution mismatch between the labeled and unlabeled\ndata, we can note that advances in domain adaptation can significantly benefit other\nrelated domains specially semi-supervised learning. One line of research would be to study\nthe generalization performance of semi-supervised learning models that have been studied\nunder the cluster assumption [21]. Indeed, by explicitly considering the differences between\nthe source and target domains, domain adaptation techniques can enhance the model’s\nability to adapt to new, unseen data in the target domain and can hence provide strategies\nto handle domain shift and improve the generalization performance of the semi-supervised\nlearning model. Furthermore, by reducing the distribution mismatch between labeled\nand unlabeled data, domain adaptation methods can enable semi-supervised learning\nalgorithms to leverage the unlabeled data more effectively. Moreover, domain adaptation\nmethods often focus on learning robust representations that are less sensitive to noise\nand domain shifts. By leveraging such robust representations, semi-supervised learning\nalgorithms can become more resilient to label noise and improve their accuracy even with\nlimited labeled data. Domain adaptation is essentially a form of transfer learning, where\nknowledge learned from a source domain is transferred to a target domain. By studying\ndomain adaptation, we can gain insights into transfer learning techniques that can be\nbeneficial for semi-supervised learning scenarios like ranking [22]. These techniques can\nhelp leverage knowledge from a labeled source domain to improve the performance of a\nsemi-supervised learning model in the target domain.\nPage 26\nUnsupervised Domain Adaptation with Deep Neural-Networks\n6 | References\n[1] Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Youn`es Bennani.\nA survey on domain adaptation theory: learning bounds and theoretical guarantees.\narXiv preprint arXiv:2004.11829, 2020.\n[2] Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation.\nACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020.\n[3] Massih-Reza Amini. Interactive learning for text summarization. In Proceedings of\nthe PKDD/MLTIA Workshop on Machine Learning and Textual Information Access,\nLyon - France, 2000.\n[4] Jean-Fran¸cois Pessiot, Young-Min Kim, Massih-Reza Amini, and Patrick Gallinari.\nImproving document clustering in a learned concept space. Information Processing &\nManagement, 46(2):180–192, 2010.\n[5] Youshan Zhang. A survey of unsupervised domain adaptation for visual recognition,\n2021.\n[6] Zirui Wang.\nTheoretical guarantees of transfer learning.\narXiv preprint\narXiv:1810.05986, 2018.\n[7] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backprop-\nagation. In International conference on machine learning, pages 1180–1189. PMLR,\n2015.\n[8] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic represen-\ntations for unsupervised domain adaptation. In International conference on machine\nlearning, pages 5423–5432. PMLR, 2018.\n[9] Jaemin Na, Heechul Jung, Hyung Jin Chang, and Wonjun Hwang. Fixbi: Bridging\ndomain spaces for unsupervised domain adaptation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 1094–1103, 2021.\n[10] Xiang Gu, Jian Sun, and Zongben Xu. Spherical space domain adaptation with robust\npseudo-label loss. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9101–9110, 2020.\n[11] Petar Stojanov, Zijian Li, Mingming Gong, Ruichu Cai, Jaime Carbonell, and Kun\nZhang. Domain adaptation with invariant representation learning: What transforma-\ntions to learn? Advances in Neural Information Processing Systems, 34:24791–24803,\n2021.\n[12] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain\nadaptation for semantic segmentation via class-balanced self-training. In Proceedings\nof the European conference on computer vision (ECCV), pages 289–305, 2018.\n[13] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data:\nGround truth from computer games. In Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II\n14, pages 102–118. Springer, 2016.\nPage 27\nUnsupervised Domain Adaptation with Deep Neural-Networks\n[14] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,\nRodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 3213–3223, 2016.\n[15] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category\nmodels to new domains. In Computer Vision–ECCV 2010: 11th European Conference\non Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings,\nPart IV 11, pages 213–226. Springer, 2010.\n[16] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer\nlearning with joint adaptation networks. In International conference on machine\nlearning, pages 2208–2217. PMLR, 2017.\n[17] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Pan-\nchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 5018–5027,\n2017.\n[18] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and\nKate Saenko.\nVisda: The visual domain adaptation challenge.\narXiv preprint\narXiv:1710.06924, 2017.\n[19] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.\nMoment matching for multi-source domain adaptation. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1406–1415, 2019.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n[21] Yury Maximov, Massih-Reza Amini, and Zaid Harchaoui. Rademacher complexity\nbounds for a penalized multi-class semi-supervised algorithm. Journal of Artificial\nIntelligence Research, 61(1):761–786, jan 2018.\n[22] Nicolas Usunier, Massih-Reza Amini, and Cyril Goutte. Multiview semi-supervised\nlearning for ranking multilingual documents. In Proceedings of the 2011 European\nConference on Machine Learning and Knowledge Discovery in Databases - Volume\nPart III, ECML PKDD’11, page 443–458, Berlin, Heidelberg, 2011. Springer-Verlag.\nPage 28\nUnsupervised Domain Adaptation with Deep Neural-Networks\nA | Appendix\nThis appendix presents the results for each domain. In order to make comparisons, 15\nexperiments were conducted for each method within each area. The Wilcoxon rank test\nwas employed to analyze and assess the performance of the methods. The best-performing\nmethod for each experiment is denoted by bold values and statistical significance is denoted\nwith a star (*).\nTable 10: Accuracy on Office-31 for the source Amazon and the target Dslr domains.\nA→D\nA→D\nA→D\nA→D\nA→D\nSource\nDann\nMstn\nFixbi\nDannFixbi\n1\n81.67\n83.13\n77.71\n76.88\n86.87\n2\n81.04\n82.92\n76.88\n70.1\n86.46\n3\n81.04\n82.71\n76.67\n77.08\n87.29\n4\n80.21\n83.33\n75.63\n73.96\n84.79\n5\n79.79\n82.71\n75.21\n83.54\n84.38\n6\n79.17\n82.29\n74.79\n79.79\n85\n7\n79.97\n82.08\n73.33\n76.67\n85.21\n8\n80.75\n81.67\n73.33\n70.83\n85.73\n9\n81.07\n81.25\n78.98\n82.08\n86.61\n10\n79.9\n80.21\n75.97\n83.13\n84.41\n11\n79.12\n82.36\n76.14\n81.04\n85.58\n12\n79.47\n82.29\n76.83\n82.5\n85.42\n13\n80.17\n81.46\n74.79\n79.58\n86.4\n14\n81.51\n82.5\n76.68\n73.75\n85.08\n15\n80.11\n81.67\n74.48\n71.25\n84.22\navg\n80.33\n82.17\n75.83\n77.48\n85.56∗\nTable 11: Wilcoxon signed-rank test for the source Amazon and the target Dslr domains.\nA→D\nSource and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nDann and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nMstn and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nFixbi and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nPage 29\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 12: Accuracy on Office-31 for the source Amazon and the target Webcam domains.\nA→W\nA→W\nA→W\nA→W\nA→W\nSource\nDann\nMstn\nFixbi\nDannFixbi\n1\n76.43\n78.91\n72.53\n87.11\n85.81\n2\n76.04\n78.65\n72.4\n86.72\n86.07\n3\n76.04\n78.65\n71.48\n81.77\n86.07\n4\n76.45\n80.23\n70.23\n81.38\n83.07\n5\n76.28\n79.31\n71.11\n83.58\n84.64\n6\n75.44\n79.15\n73.81\n80.57\n85.29\n7\n76.88\n80.05\n71.55\n85.1\n82.81\n8\n75.92\n80.66\n69.55\n81.68\n85.55\n9\n77.22\n78.79\n70.54\n80.28\n84.51\n10\n76.09\n79.05\n71.21\n82.78\n85.42\n11\n78.7\n79.83\n69.36\n84.21\n84.9\n12\n75.67\n78.81\n70.28\n81.08\n82.29\n13\n75.9\n80.47\n71.67\n79.27\n83.33\n14\n76.71\n78.2\n71.72\n83.44\n82.68\n15\n77.65\n78.77\n72.1\n81.11\n81.64\navg\n76.49\n79.30\n71.30\n82.67\n84.27∗\nTable 13: Wilcoxon signed-rank test for the source Amazon and the target Webcam\ndomains.\nA→D\nSource and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nDann and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nMstn and DannFixbi\np(x > 3.4078) = 0.0117 ≤0.025\nFixbi and DannFixbi\np(x > 2.1583) = 0.0117 ≤0.025\nPage 30\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 14: Accuracy on Office-31 for the source Dslr and the target Webcam domains.\nD→W\nD→W\nD→W\nD→W\nD→W\nSource\nDann\nMstn\nFixbi\nDannFixbi\n1\n95.96\n96.35\n92.06\n94.66\n97.53\n2\n95.7\n95.96\n91.8\n90.89\n97.35\n3\n95.7\n95.83\n91.41\n88.54\n97.44\n4\n97.01\n94.92\n93.49\n92.78\n96.74\n5\n94.79\n96.48\n93.1\n93.1\n96.88\n6\n96.88\n94.4\n91.67\n94.4\n97.27\n7\n95.44\n95.96\n92.32\n91.69\n97.4\n8\n94.92\n96.22\n92.19\n93.53\n96.61\n9\n96.74\n96.35\n95.44\n92.29\n97.14\n10\n95.96\n95.05\n94.27\n94.42\n97.01\n11\n95.05\n94.79\n91.02\n96.88\n95.7\n12\n95.7\n96.61\n93.23\n91.92\n97.53\n13\n97.53\n95.57\n92.19\n92.55\n96.09\n14\n95.18\n97.01\n90.23\n93.53\n96.35\n15\n96.61\n96.09\n95.18\n94.82\n95.7\navg\n95.94\n95.84\n92.64\n93.07\n96.85∗\nTable 15: Wilcoxon signed-rank test for the source Dslr and the target Webcam domains.\nD→W\nSource and DannFixbi\np(x > 2.4422) = 0.0117 ≤0.025\nDann and DannFixbi\np(x > 2.7262) = 0.0117 ≤0.025\nMstn and DannFixbi\np(x > 3.3510) = 0.0117 ≤0.025\nFixbi and DannFixbi\np(x > 3.2942) = 0.0117 ≤0.025\nPage 31\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 16: Accuracy on Office-31 for the source Dslr and the target Amazon domains.\nD→A\nD→A\nD→A\nD→A\nD→A\nSource\nDann\nMstn\nFixbi\nDannFixbi\n1\n60.33\n64.35\n33.38\n23.19\n65.27\n2\n60.09\n63.88\n32.1\n15.23\n64.81\n3\n59.38\n63.92\n34.09\n16.01\n63.53\n4\n58.03\n62.6\n36.01\n17.8\n62.43\n5\n57.17\n63.36\n37.71\n43.54\n61.54\n6\n58.98\n62.31\n38.6\n25\n61.93\n7\n59.2\n62.48\n34.94\n20.06\n60.72\n8\n60.23\n61.11\n33.1\n25.5\n61.26\n9\n57.74\n63.18\n35.12\n26.1\n62.96\n10\n58.1\n63.94\n34.2\n29.58\n64.88\n11\n59.3\n62.48\n36.72\n23.22\n60.8\n12\n60.72\n62.9\n32.81\n20.7\n62.18\n13\n58.35\n63.72\n43.08\n24.57\n63.03\n14\n59.17\n63.6\n39.38\n23.08\n64.7\n15\n56.85\n62.22\n40.8\n42.12\n61.36\navg\n58.91\n63.07\n36.14\n25.05\n62.76\nTable 17: Wilcoxon signed-rank test for the source Dslr and the target Amazon domains.\nD→A\nSource and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nMstn and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nFixbi and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nDannFixbi and Dann\np(x > 1.0859) = 0.1425 > 0.1\nPage 32\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 18: Accuracy on Office-31 for the source Webcam and the target Dslr domains.\nW→D\nW→D\nW→D\nW→D\nW→D\nSource\nDann\nMstn\nFixbi\nDannFixbi\n1\n99.58\n100\n99.79\n94.38\n99.37\n2\n99.37\n100\n99.79\n94.38\n98.98\n3\n99.37\n99.79\n99.79\n92.92\n99.17\n4\n99.79\n100\n99.37\n92.88\n99.37\n5\n99.58\n99.79\n99.58\n94.79\n98.96\n6\n99.37\n100\n99.17\n93.12\n99.17\n7\n99.37\n99.58\n99.17\n95\n99.37\n8\n99.58\n99.58\n99.58\n92.5\n99.58\n9\n99.79\n99.79\n99.17\n92.92\n99.17\n10\n99.58\n100\n99.79\n92.08\n99.58\n11\n99.37\n99.79\n99.17\n95.83\n98.96\n12\n99.79\n99.58\n99.37\n94.79\n99.17\n13\n99.58\n100\n99.58\n93.96\n99.37\n14\n99.58\n99.79\n99.17\n91.87\n99.58\n15\n99.37\n99.58\n98.96\n92.71\n99.17\navg\n99.54\n99.82∗\n99.43\n93.61\n99.26\nTable 19: Wilcoxon signed-rank test for the source Webcam and the target Dslr domains.\nW→D\nSource and Dann\np(x > 2.8398) = 0.0117 ≤0.025\nMstn and Dann\np(x > 3.2374) = 0.0117 ≤0.025\nFixbi and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nDannFixbi and Dann\np(x > 3.3510) = 0.0117 ≤0.025\nPage 33\nUnsupervised Domain Adaptation with Deep Neural-Networks\nTable 20: Accuracy on Office-31 for the source Webcam and the target Amazon domains.\nW→A\nW→A\nW→A\nW→A\nW→A\nSource\nDann\nMstn\nFixbi\nDannFixbi\n1\n64.45\n65.38\n51.07\n30.15\n63.04\n2\n64.17\n64.91\n48.58\n26.03\n63.29\n3\n64.13\n64.74\n48.65\n20.63\n62.93\n4\n62.96\n64.7\n50.6\n23.45\n62.32\n5\n62.25\n63.42\n53.09\n25.82\n62.04\n6\n63.14\n64.91\n49.68\n23.69\n61.97\n7\n64.1\n64.03\n52.06\n35.58\n62.29\n8\n61.75\n63.81\n52.49\n27.7\n61.54\n9\n62.32\n63.74\n50.71\n30.11\n60.87\n10\n62.46\n64.1\n51.81\n24.08\n61.93\n11\n63\n64.38\n50.36\n35.83\n62.32\n12\n62.82\n62.57\n49.43\n30.29\n62.22\n13\n62.78\n63.99\n52.88\n21.16\n61.4\n14\n63.39\n62.82\n55.15\n25.92\n60.9\n15\n61.22\n63.07\n50.71\n25.85\n61.51\navg\n63.00\n64.04∗\n51.15\n27.09\n62.04\nTable 21: Wilcoxon signed-rank test for the source Webcam and the target Amazon\ndomains.\nW→A\nSource and Dann\np(x > 3.0670) = 0.0117 ≤0.025\nMstn and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nFixbi and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nDannFixbi and Dann\np(x > 3.4078) = 0.0117 ≤0.025\nTable 22: Accuracy on Office-31 for all methods.\nA→D\nA→W\nD→W\nD→A\nW→D\nW→A\nSource\n80.33 ± 0.8\n76.49 ± 0.8\n95.94 ± 0.8\n58.91 ± 1.2\n99.54 ± 0.2\n63.00 ± 0.9\nDann\n82.17 ± 0.8\n79.3 ± 0.8\n95.84 ± 0.7\n63.07 ± 0.9\n99.82 ± 0.2\n64.04 ± 0.8\nMstn\n75.83 ± 1.6\n71.3 ± 1.2\n92.64 ± 1.5\n36.14 ± 3.2\n99.43 ± 0.3\n51.15 ± 1.8\nFixbi\n77.48 ± 4.6\n82.67 ± 2.3\n93.07 ± 2.0\n25.05 ± 8.2\n93.61 ± 1.2\n27.09 ± 4.6\nDannFixbi\n85.56 ± 1.0\n84.27 ± 1.5\n96.85 ± 0.6\n62.76 ± 1.6\n99.26 ± 0.2\n62.04 ± 0.7\nTable 23: Average accuracy across all domains\nAvg\nSource\n79.04\nDann\n80.71\nMstn\n71.08\nFixbi\n66.49\nDannFixbi\n81.79\nPage 34\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2023-07-10",
  "updated": "2023-07-10"
}