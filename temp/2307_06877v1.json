{
  "id": "http://arxiv.org/abs/2307.06877v1",
  "title": "The complexity of non-stationary reinforcement learning",
  "authors": [
    "Christos Papadimitriou",
    "Binghui Peng"
  ],
  "abstract": "The problem of continual learning in the domain of reinforcement learning,\noften called non-stationary reinforcement learning, has been identified as an\nimportant challenge to the application of reinforcement learning. We prove a\nworst-case complexity result, which we believe captures this challenge:\nModifying the probabilities or the reward of a single state-action pair in a\nreinforcement learning problem requires an amount of time almost as large as\nthe number of states in order to keep the value function up to date, unless the\nstrong exponential time hypothesis (SETH) is false; SETH is a widely accepted\nstrengthening of the P $\\neq$ NP conjecture. Recall that the number of states\nin current applications of reinforcement learning is typically astronomical. In\ncontrast, we show that just $\\textit{adding}$ a new state-action pair is\nconsiderably easier to implement.",
  "text": "The complexity of non-stationary reinforcement learning\nChristos Papadimitriou\nColumbia University\nchristos@columbia.edu\nBinghui Peng\nColumbia University\nbp2601@columbia.edu\nJuly 14, 2023\nAbstract\nThe problem of continual learning in the domain of reinforcement learning, often called\nnon-stationary reinforcement learning, has been identified as an important challenge to the\napplication of reinforcement learning. We prove a worst-case complexity result, which we believe\ncaptures this challenge: Modifying the probabilities or the reward of a single state-action pair\nin a reinforcement learning problem requires an amount of time almost as large as the number\nof states in order to keep the value function up to date, unless the strong exponential time\nhypothesis (SETH) is false; SETH is a widely accepted strengthening of the P ̸= NP conjecture.\nRecall that the number of states in current applications of reinforcement learning is typically\nastronomical. In contrast, we show that just adding a new state-action pair is considerably\neasier to implement.\narXiv:2307.06877v1  [cs.LG]  13 Jul 2023\n1\nIntroduction\nReinforcement learning (RL) [SB18], the branch of machine learning seeking to create machines that\nreact to a changing environment so as to maximize long-term utility, has recently seen tremendous\nadvances through deep learning [SSS+17, SHS+18], as well as a vast expansion of its applicability\nand reach to many application domains, including board games, robotics, self-driving cars, control,\nand many more. As with most aspects of deep learning, one of the most important current challenges\nin deep RL lies in handling situations in which the model undergoes changes. Variably called non-\nstationary RL, continual RL, multi-task RL, or life-long RL, the problem of enabling RL to react\neffectively and gracefully to sequences of changes in the underlying Markov model has been identified\nas an important open problem in practice, see the prior work subsection for many references, and\n[KRRP22] for a recent survey of the challenge and the available remedies.\nWhen it becomes clear that a particular computational problem is difficult, the field of compu-\ntational complexity [PS98, Pap03, AB09] comes into play: the search for mathematical obstacles to\nthe efficient solution of problems. The identification of such obstacles is often informative about\nthe kinds of remedies one needs to apply to the problem. As far as we can tell, the computa-\ntional complexity of non-stationary RL (NSRL) has not been explored in the past; in contrast, see\n[CPP22] for an example of recent progress in identifying complexity obstacles in continual learning\nof classification tasks.\nIn this paper, we initiate the analysis of NSRL from the standpoint of computational complexity.\nWe consider finite horizon MDPs — it is easy to see that our results can be extended very easily\nto infinite horizon MDPs. We ask the following question: Suppose that we have already solved a\nfinite-horizon MDP, and that the MDP changes in some small way; how difficult is it to modify the\nsolution? If the solution we want to update is an explicit mapping from states to actions, then it\nis not hard to see that this is hopeless: a small local change can cause a large proportion of the\nvalues of this map to change1. However, recall that deep RL is not about computing explicitly\nthe optimum solution of the problem; it is about maintaining an implicit representation of a good\napproximation of the optimum solution. An efficient NSRL algorithm only needs to update the\nvalue or policy efficiently when visiting the state. Our results address precisely this aspect of the\ndifficulty.\nWe consider elementary local changes to the RL problem, which we believe capture well the\nnature of the NSRL problem: We choose a state-action pair and we modify somehow its parameters:\nthe reward, and the transition probability distribution. Our results hold for the most elementary\npossible change: We only modify two transition probabilities in this state-action pair.\n(Notice\nthat it is impossible to modify only one probability in a distribution...)\nWe prove that, under\nwidely accepted complexity assumptions to be explained soon, the amount of computation needed\nto update an ϵ-optimal value approximation in the face of such an elementary change is, in the worst\ncase, comparable to the number of states (the precise result is stated below). Since in the problems\ncurrently solved by deep RL the number of states of the underlying MDP is typically astronomical,\nsuch a prediction is bad indeed — it means that we essentially have to start all over because of a\nsmall change. Now, in deep learning we know well that a worst-case result is never the last word\non the difficulty of a problem. However, we believe that an alarming worst-case result, established\nfor an aspect of the problem which has been identified in practice to be a challenge, is a warning\nsign which may yield valuable hints about the corrective action that needs to be taken in order to\novercome the current bottleneck.\nWe complement this lower bound with a positive result for a different kind of change: adding a\n1For example, consider the extreme example where a change in an action increases the value of the next state,\nand this in turn changes the optimum actions in almost all other states.\n1\nnew action to a state. It turns out that this is a simpler problem, and an ϵ-approximate solution\ncan be updated in time polynomial in 1\nϵ and the horizon.\nRelated work\nNon-stationary MDPs have been studied extensively in recent years from the point of view of dy-\nnamic regret [AJO08, DGS14, OGA20, CSLZ20, ZCVJ22, LL19, TV20, WL21, DMP+21, MZZ+21];\nIn [MZZ+21] an algorithm with total regret eO(S1/3A1/3∆1/3HT 2/3) is provided, where T is the\ntotal number of iteration, ∆is the variational budget that measures the total change of MDP. An-\nother line of work focuses on the statistical problem of detecting the changes in the environment,\nsee [DSBBE06, BLH17, PKB20, OT21] , and [Pad21, KRRP22] for recent surveys; in particular,\n[Pad21] mentions the computational difficulty of the change problem addressed in this paper. Sev-\neral approaches to NSRL — e.g [WL21, MZZ+21] — resort to restarting the learning process if\nenough change has accumulated; our results suggest that, indeed, restarting may be preferable to\nupdating. Additional literature can be found at Appendix A.\nA brief overview of the main result\nOur main result (Theorem 3.1) states that, in the worst case, an elementary change in an MDP —\njust updating two transition probabilities in one action at one state of the MDP — requires time\n(SAH)1−o(1), where S is the number of states, A is the number of action and H is the horizon.\nThe proof is based on the Strong Exponential Time Hypothesis (SETH), which is a central con-\njecture in complexity, a refinement of P ̸= NP. SETH has many applications in graph algorithms\n[RVW13, AW14, BRS+18, Li21, DLW22], edit distance [BI15], nearest neighbor search [Rub18],\nkernel estimation [CS17, ACSS20] and many other domain; see [RW19] for a comprehensive survey.\nSETH states that, if the k-SAT problem (the Boolean satisfiability problem when each clause con-\ntains at most k literals) can be solved in time O(2ckn), then the limit of ck as k grows is one. Our\nwork is based on the important result of [ARW17] on the hardness, under SETH, of approximat-\ning the bichromatic Maximum Inner Product (Max-IP) problem. Subsequent work has improved\nthe approximation parameter [Rub18, Che20] and applied the technique to the Dynamic Coverage\nproblem [AAG+19, Pen21].\nWe reduce from the Max-IP problem, where we are given two collections of sets B1, . . . , Bn\nand C1, . . . , Cn, over a small universe [m] with m = no(1). It is known from [ARW17] that it is\nhard to distinguish between the following two scenaria: (a) Bi ⊆Cj for some i, j ∈[n], and (b)\n|Bi ∩Cj| ≤|Cj|/2log(n)1−o(1) for all i, j ∈[n]. That is, it is hard to tell the difference between the\ncase of a complete containment and the case of tiny intersections. The first step of our reduction is\nto construct a finite-horizon MDP such that the state of the first step (h = 1) corresponds to the\nsets B1, . . . , Bn and the state of the second step (h = 2) corresponds to the universe [m]. The state\nof the second step has either high reward or low reward, depending on the time t. By applying a\nsequence of changes to the state-action transition in the second step, based on the structure of the\nsets C1, . . . , Cn, one obtains a reduction from Max-IP establishing a lower bound of S2−o(1) for this\nsequence. However, since this sequence is of length S1+o(1) (because of the size of the Cj sets), we\nobtain an Ω(S1−o(1)) amortized lower bound for each step of the sequence, and this complete the\nreduction to the NSRL problem.\nThe construction so far yields an approximation ϵ that is very small (about S−o(1)). We need\na second stage of our construction to amplify ϵ to some constant such as 0.1. This is achieved\nby stacking multiple layers of the basic construction outlined above.\nFinally, by spreading the\nstate-actions across multiple steps, we improve the lower bound to Ω((SAH)1−o(1)).\n2\nThe complete proof can be found at Section 3.\n2\nPreliminary Definitions\nHere we shall define non-stationary MDPs. Let S be a state space (|S| = S), A an action space\n(|A| = A), H ∈Z+ the planning horizon. Next let T ∈Z+ be the number of rounds: The intention\nis that the MDP will repeated T times, with action parameters changed between rounds.\nA non-stationary finite horizon MDP is a set of T MDPs ({Sh, Ah, Pt,h, rt,h}t∈[T],h∈[H], sinit).\nSh ⊆S is the state space and Ah ⊆A is the action space at the h-th step (h ∈[H]), and\nPt,h : Sh×Ah →∆Sh+1 is the transition function, where ∆Sh is the set of all probability distributions\nover Sh, and rt,h : Sh × Ah →[0, 1] is the reward function at the h-th step of the t-th round\n(h ∈[H], t ∈[T]). We use sinit ∈S1 to denote the initial state.\nWe focus on deterministic non-stationary policies π = (π1, . . . , πT ), though our results apply\nfor randomized policies as well. Let πt = (πt,1, . . . , πt,H) be the policy of the t-th round (t ∈[T])\nand πt,h : Sh →Ah (h ∈[H]) be the decision at the h-th step. Given a policy π, the Q-value of a\nstate-action pair (s, a) ∈Sh × Ah at the t-round can be determined\nQπt\nt,h(s, a) = rt,h(s, a) + E\n\"\nH\nX\nℓ=h+1\nrt,h(st,ℓ, πt,ℓ(st,ℓ)) | st,h = s, at,h = a\n#\n∀s ∈Sh, a ∈Ah\nand the V -value\nV πt\nt,h(s) = E\n\" H\nX\nℓ=h\nrt,h(st,ℓ, πt,ℓ(st,ℓ)) | st,h = s\n#\n∀s ∈St,h.\nLet π∗\nt be the optimal policy at the t-round, and Q∗\nt , V ∗\nt be its Q-value and V -value. The goal is\nto maintain an ϵ-approximated value function. In particular, we require the algorithm to maintain\nan ϵ-approximated estimation Vt, of the initial state sinit, such that for all rounds t ∈[T],\n\f\fVt −V ∗\nt,1(sinit)\n\f\f ≤ϵ.\nUpdates.\nAll T MDPs of our definition must be solved, one after the other, despite the fact\nthat their parameters change from one round to the next. The updates are meant to be extremely\nsimple and local: For the t-th update, an adversary picks an arbitrary state-action pair (sh, ah) ∈\nSh × Ah, and changes the transition function from Pt−1,h(sh, ah) to Pt,h(sh, ah) and the reward\nfrom rt−1,h(sh, ah) to rt,h(sh, ah).\nIt also changes the transition function from Pt−1,h(sh, ah) to\nPt,h(sh, ah), such that these two distributions differ in exactly two states. That is, the change in the\ndistribution is the smallest kind imaginable: Two next states are chosen, and the probability mass of\nthe first is transferred to the second — obviously, two discrete distributions cannot differ in exactly\none probability.\nRemark 2.1. Implementing an elementary change of this kind takes constant time: If the distri-\nbution is represented in a tabular form, the two entries of the table are changed. It holds similarly\nwhen the MDP is accessed via a sampling oracle (a.k.a. the generative model), all one has to do is\nchange the output states.\nRemark 2.2. Notice that the kind of changes we consider is the simplest possible, and yet a sequence\nof such changes can simulate any desirable change. Hence, by showing in the next section that even\nthese changes are computationally intractable, we establish that NSRL is intractable.\n3\nIncremental action change.\nWe also consider a different type of NSRL, where the MDP changes\nonly through the introduction of a new action.2 The setup is similar to NSRL: we assume the initial\nMDP has S states, H steps but the action set is empty. Then in each round t ∈[T], a new state-\naction pair (sh, ah) is added to the MDP, together with its transition probability Ph(sh, ah) and\nreward rh(sh, ah). Note the crucial difference with NSRL is that the there is no change occurs on\nany existing state-action pair. There are a total of T rounds, and therefore, T state-action pairs at\nthe end. The incremental action model captures application scenario that involves explorations or\nexpansion of environments (e.g. incremental training).\n3\nHardness of NSRL\nThe main result is the following:\nTheorem 3.1 (Main result, hardness of NRSL). Let S, A, H, T be sufficiently large integers, the\nhorizon H ≥(SA)o(1). Then, unless SETH is false, there is no algorithm with amortized runtime\nO((SAH)1−o(1)) per update that can approximate the optimal value of a non-stationary MDP over a\nsequence of T updates. In particular, any algorithm with better runtime fails to distinguish between\nthese two cases:\n• The optimal policy has value at least H\n4 at some round t ∈[T];\n• The optimal policy has value at most\nH\n100 for all T rounds.\nOur result is based on the widely accepted Strong Exponential Time Hypothesis (SETH).\nConjecture 3.2 (Strong Exponential Time Hypothesis (SETH), [IP01]). For any ϵ > 0, there exists\nk ≥3 such that the k-SAT problem on n variables cannot be solved in time O(2(1−ϵ)n).\nNote that SETH is stronger than the P ̸= NP assumption, a strengthening that allows the proof\nof polynomial lower bounds on problems that have a polynomial-time algorithm — such as NSRL.\nThe starting point of our reduction is the following hardness result for the Bichromatic Maximum\nInner Product (Max-IP) problem, whose proof is based on the machinery of distributed PCP.\nTheorem 3.3 (Bichromatic Maximum Inner Product (Max-IP) [ARW17]). Let γ > 0 be any con-\nstant, and let n ∈Z+, m = no(1), w = 2(log(n))1−o(1). Given two collections of sets B = {B1, . . . , Bn}\nand C = {C1, . . . , Cn} over universe [m], satisfying |B1| = · · · = |Bn| = b and |C1| = · · · = |Cn| = c\nfor some b, c ∈[m]. Unless SETH is false, no algorithm can distinguish the following two cases in\ntime O(n2−γ):\n• YES instance.\nThere exists two sets B ∈B, C ∈C such that C ⊆B;\n• NO instance.\nFor every B ∈B and C ∈C, |B ∩C| ≤c/w.\nParameters.\nWe reduce Max-IP to NSRL. For any sufficiently large parameters S, A, H, T, let\nn = T 1/2−o(1) · (SAH)1/2\nand\nm = no(1)\nbe the input parameters of Max-IP. Given a Max-IP instance with sets B1, . . . , Bn and C1, . . . , Cn\nover a ground set [m], recall b, c ∈[m] are the size of set {Bi}i∈[n] and {Ci}i∈[n] . Let\nL = ⌈b/c⌉\nand\nN =\nSAH\n16L(log2(S) + 2).\n2Note the introduction of a new state can be achieved through a sequence of action additions.\n4\nWe shall divide {Bi}i∈[n] into K = n/N batches and each batch contains N sets. That is, {Bi}i∈[n] =\n{Bk,ν}k∈[K],ν∈[N]. In the proof, we assume the total number of updates SAH ≤T ≤poly(SAH),\ni.e., it is polynomially bounded.\n3.1\nConstruction of a hard instance\nWe first describe the MDP at the initial stage (t = 0), with state space {Sh}h∈[H], action space\n{Ah}h∈[H], transition function {Ph}h∈[H] and reward function {rh}h∈[H]. A (simplified) illustration\ncan be found at Figure 1. We omit the subscript of t = 0 for simplicity.\nFigure 1: A snapshot of the hard instance\nHorizon.\nWe divide the entire horizon into two phases\n[H] = H1 ∪H2,\nwhere\nH1 = [H/2]\nand\nH2 = [H/2 : H] .\nThe second phase is relatively simple and involves only two terminal states that provide rewards.\nThe first phase is more involved and determines the destination state.\nThe first phase contains L layers, and each layer contains H/2L steps\nH1 = H1,1 ∪· · · ∪H1,L,\nwhere\nH1,ℓ=\n\u0014\n(ℓ−1) · H\n2L + 1 : ℓ· H\n2L\n\u0015\n∀ℓ∈[L].\nThe layers are used for amplifying the difference between good and bad policies. The structure of\nthe MDP is for identical for each layer, except the last step at the last layer.\nFor each layer ℓ∈[L], we further divide it into G :=\nH\n2L(log2(S)+2) groups, and each group\ncontains log2(S) + 2 steps,\nH1,ℓ= H1,ℓ,1 ∪· · · ∪H1,ℓ,G\nwhere\nH1,ℓ,g =\n\u0014\n(ℓ−1) · H\n2L + (g −1)(log2(S) + 2) + 1 : (ℓ−1) · H\n2L + g · (log2(S) + 2)\n\u0015\n∀g ∈[G].\nWe set h(ℓ, g, τ) := (ℓ−1)(H/2L) + (g −1)(log2(S) + 2) + τ be the τ-step, at the g-th group\nof the ℓ-th layer, where τ ∈[log2(S) + 2], g ∈[G], ℓ∈[L]. For simplicity, we also write h(ℓ, g) =\nh(ℓ, g, log2(S) + 2) and h(ℓ) = h(ℓ, G) be the last step of each group and layer.\n5\nStates.\nThere are five types of states: terminal states, element states, set states, routing states\nand the pivotal state.\n• Terminal states. There are two terminal states st\n1 and st\n2, and they appear at every steps\nh ∈[H]. We use st\nh,1, st\nh,2 to denote the terminal states at Sh.\n• Element states. There are m element states {se\nu}u∈[m] that appear at every step h ∈H1 of\nphase one. We use se\nh,u to denote the u-th element state at Sh.\n• Set states. There are S/4 set states {sb\ni }i∈[S/4]. The set states only appear on the second\nlast step of each group Hℓ,g. In particular, for each layer ℓ∈[L], group g ∈[G], let sb\nh(ℓ,g)−1,i\ndenote the i-th (i ∈[S/4]) set state at Sh(ℓ,g)−1.\n• Pivotal state There is one pivotal state sp that appears at every step h ∈H1 of Phase 1,\ndenoted as sp\nh. The MDP start with the pivotal state, i.e., sinit := sp\n1.\n• Routing states. The routing states are used for reaching set states. There S/4 routing\nstates {sr\nα}α∈[S/4] that appear at the [2 : log2(S)]-th step of each group. In particular, at\nlayer ℓ∈[L], group g ∈[G], step τ ∈[2 : log2(S)], let {sr\nh(ℓ,g,τ),α}α∈[1:2τ−2] be the collection of\nrouting states at Sh(ℓ,g,τ).\nThe total number of possible states is at most 2 + m + S/4 + S/4 + 1 ≤S.\nActions\nThere are five types of actions. The terminal action at, the element actions ae, the set\nactions {ab\nj}j∈[A/2], the pivotal action {ap\n1, ap\n2} and the routing actions {ar\n1, ar\n2}. The total number\nof action is at most A/2 + 6 ≤A, and we assume these actions appear at every step h ∈[H].\nReward\nThe only state that returns non-zero reward is the terminal state {st\nh,1}h∈H2. Formally,\nwe set\nrh(s, a) = 0\nwhen\nh ∈H1\nand\nrh(s, a) =\n\u001a1\ns = st\nh,1\n0\notherwise\nwhen\nh ∈H2.\n(1)\nTransitions\nWe next specify the transition probability of the initial MDP.\n(a) Terminal states. The transition of terminal states is deterministic and always keeps the\nstate terminal, that is\nPh(st\nh,1, a) = 1{st\nh+1,1}\nand\nPh(st\nh,2, a) = 1{st\nh+1,2}\n∀h ∈[H −1], a ∈A.\n(2)\nHere we use 1{s} ∈∆Sh+1 to denote the one-hot vector that is 1 at state s and 0 otherwise.\nCombining with the definition of reward functions, the MDP guarantees that a policy receives H/2\nreward once it goes to the first terminal state st\nh,1 at some step h ∈H2. Meanwhile, it receives 0\nreward if it ever goes to the second terminal state st\nh,2.\n(b) Element states. At step h < H/2, for any element u ∈[m], the transition function of se\nh,u\nequals\nPh(se\nh,u, ae) =\n\u001a 1{sp\nh+1}\nh = h(ℓ) for some ℓ∈[L −1]\n1{se\nh+1,u}\notherwise\n(3)\nand\nPh(se\nh,u, a) = 1{se\nh+1,u},\n∀a ∈A\\{ae}.\n(4)\n6\nThat is, the element state se\nh,u always stays on itself, except at the end of each layer ℓ∈[L], it can\ngo to the pivotal state.\nAt the end of the first phase, the transition of element state is determined by the set C. In the\ninitialization stage (t = 0), let C0 ⊆[m] be an arbitrary set of size c and it would be replace later,\nlet\nPH/2(se\nH/2,u, ae) =\n(\n1{st\nH/2+1,1}\nu ∈C0\n1{st\nH/2+1,2}\nu /∈C0\nand\nPH/2(se\nH/2,u, a) = 1{st\nH/2+1,2},\n∀a ∈A\\{ae}.\nThat is, if the element u ∈C0, then it can go to a high reward terminal state st\nH/2+1,1; otherwise\nit goes to the no-reward terminal st\nH/2+1,2. Looking ahead, we would update the state-action pairs\n{(se\nH/2,u, ae)}u∈[m] according to sets {Ci}i∈[n] periodically.\n(c) Set states. The transition function of set states is determined by the sets {Bk,ν}k∈[K],ν∈[N].\nIn the initialization stage (t = 0), let {B0,ν}ν∈[N] be arbitrary sets of size b and they would be\nreplaced later in the update sequence. Recall that a set state would appear at the second last step\nof a group Hℓ,g, for some layer ℓ∈[L] and group g ∈[G]. Let\nN(g, i, j) := (g −1)(S/4)(A/2) + (i −1)(A/2) + j,\nand therefore,\n{N(g, i, j) : g ∈[G], i ∈[S/4], j ∈[A/2]} = [N].\nThe transition function of state-action pair (sb\nh(ℓ,g)−1,i, ab\nj) equals\nPh(ℓ,g)−1(sb\nh(ℓ,g)−1,i, ab\nj) = unif(se\nh(ℓ,g),u : u ∈B0,N(g,i,j))\n∀g ∈[G], i ∈[S/4], j ∈[A/2].\n(5)\nHere the RHS is the uniform distribution over the element states se\nh(ℓ,g),u for element u ∈B0,N(g,i,j).\nFor the rest of actions, it goes to the no-reward terminal st\nh(ℓ,g),2:\nPh(ℓ,g)−1(sb\nh(ℓ,g)−1,i, a) = 1{st\nh(ℓ,g),2}\n∀a ∈A\\{ae\nj}j∈[A/2]\n(d) Pivotal states. The pivotal state sp\nh appears at every step h ∈H1, and for h < H/2 −1,\nthe transition function equals\nPh(sp\nh, a) =\n\u001a1{sr\nh+1,1}\na = ap, h = h(ℓ, g, 1) for some ℓ∈[L], g ∈[G]\n1{sp\nh+1}\notherwise\n(6)\nThat is, the pivotal state stays on itself, except at the first step of Hℓ,g, it could go to the routing\nstate sr\nh(ℓ,g,2),1.\nAt the H/2-th step, it goes to the no-reward terminal st\nH/2+1,2,\nPH/2(sp\nH/2, a) = 1{st\nH/2+1,2}\n∀a ∈A.\n(e) Routing states. Recall {sr\nh(ℓ,g,τ),α}α∈[1:2τ−2] is the collection of routing states at the α-th\nstep (α ∈[2 : log2(S)]), g-th group (g ∈[G]) and ℓ-th layer (ℓ∈[L]).\nWhen τ ∈[2 : log2(S) −1], the transition function equals\nPh(ℓ,g,τ)(sr\nh(ℓ,g,τ),α, a) =\n\n\n\n\n\n1{sr\nh(ℓ,g,τ+1),2α−1}\na = ar\n1\n1{sr\nh(ℓ,g,τ+1),2α}\na = ar\n2\n1{st\nh(ℓ,g,τ+1),2}\notherwise\n,\n∀α ∈[2τ−2].\n(7)\n7\nIn other words, the routing state sr\nh(ℓ,g,τ),α goes to either sr\nh(ℓ,g,τ+1),2α−1 or sr\nh(ℓ,g,τ+1),2α−1, depending\non the choice of actions (unless it goes to the no-reward terminal sr\nh(ℓ,g,τ+1),2).\nWhen τ = log2(S), the routing state sr\nh(ℓ,g,log2(S)),α goes to the set state sb\nh(ℓ,g)−1,α (α ∈[S/4]),\nthat is,\nPh(ℓ,g,log2(S))(sr\nh(ℓ,g,log2(S)),α, a) = sb\nh(ℓ,g)−1,α,\n∀α ∈[S/4], a ∈A.\n(8)\nThe entire transition of routing states within a group works like a binary search tree: it comes from\nthe pivotal state and goes to one of the set states. We note that if S ≤A the construction could\nbe simplified: we can remove routing states and have a pivotal state directly go to set states. This\ncompletes the description of the initial MDP.\nUpdate sequence.\nWe next specify the sequence of updates to the MDP. The sequence of updates\nis divided into K = n/N stages, and each stage contains n-epochs.\nAt the beginning of each stage, the update occurs on the state-action pairs for set-states:\n{(sb\nh(ℓ,g)−1,i, ab\nj)}ℓ∈[L],g∈[G],i∈[S/4],j∈[A/2]\nConcretely, there is an initialization phase at the beginning of the k-th stage (k ∈[K]).\nLet\nt(k) ∈[T] be the end of initiazation phase, and the nature sets\nPt(k),h(ℓ,g)−1(sb\nh(ℓ,g)−1,i, ab\nj) = unif(se\nh(ℓ,g),u : u ∈Bk,N(g,i,j))\n∀ℓ∈[L], g ∈[G], i ∈[S/4], j ∈[A/2].\nEach stage contains n-epochs, and during each epoch, the update occurs on the state-action\npairs {(se\nH/2,u, ae)}u∈[m] of element state-action, in the H/2-th step. Let t(k, τ) ∈[T] be the end of\nk-th (k ∈[K]) stage and τ-th (τ ∈[n]) epoch. In the τ-th epoch (τ ∈[n]), for each element u ∈[m],\nthe transition function is updated to\nPt(k,τ),H/2(se\nH/2,u, ae) =\n(\n1{st\nH/2+1,1}\nu ∈Cτ\n1{st\nH/2+1,2}\nu /∈Cτ.\n(9)\nTo count the total number of updates, there are K = n/N stages. The initialization takes at\nmost O(SAHm) updates; there are n epochs, and each epoch contains at most 2m updates. Hence\nthe total number of updates equals (n/N) · O(SAHm + 2mn) ≈T.\n3.2\nAnalysis\nWe now proceed to prove Theorem 3.1. For any stage k ∈[K] and epoch τ ∈[n], we compute the\nV -value of the optimal policy. The proof can be found at the Appendix B\nLemma 3.4 (V -value, terminal states). At the end of stage k ∈[K] and epoch t ∈[n], for any\nstep h ∈[H], the V -value of optimal policy at terminal states satisfies V ∗\nt(k,τ),h(st\nh,1) = min{H + 1 −\nh, H/2} and V ∗\nt(k,τ),h(st\nh,2) = 0.\nLemma 3.5 (V -value, element states). At the end of stage k ∈[K] and epoch τ ∈[n], for any layer\nℓ∈[L] and any step h ∈H1,ℓ\n• For any element u ∈Cτ, V ∗\nt(k,τ),h(se\nh,u) = H/2; and\n• For any element u /∈Cτ, we have V ∗\nt(k,τ),h(se\nh,u) = V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1).\n8\nHere we take Vt(k,τ),H/2+1(sp\nH/2+1) := 0.\nLemma 3.6 (V -value, set states). At the end of stage k ∈[K] and epoch τ ∈[n], for each level\nℓ∈[L], group g ∈[G], we have\nV ∗\nt(k,τ),h(ℓ,g)−1(sb\nh(ℓ,g)−1,i)\n=\nmax\nj∈[A/2]\n\u001a|Cτ ∩Bk,N(g,i,j)|\nb\n· H\n2 +\n\u0012\n1 −|Cτ ∩Bk,N(g,i,j)|\nb\n\u0013\n· V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1)\n\u001b\nLemma 3.7 (V -value, pivotal state). At the end of stage k ∈[K] and epoch τ ∈[n], for each level\nℓ∈[L], the V -value of the pivotal state satisfies\nV ∗\nt(k,τ),h(ℓ−1)+1(sp\nh(ℓ−1)+1)\n= max\nν∈[N]\n\u001a|Cτ ∩Bk,ν|\nb\n· H\n2 +\n\u0012\n1 −|Cτ ∩Bk,ν|\nb\n\u0013\n· V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1)\n\u001b\n.\nAs a corollary, we can compute the V -value of the initial state.\nLemma 3.8 (V -value, initial state). Let κk,τ = maxν∈[N]\n|Cτ∩Bk,ν|\nb\n, then at the end of stage k and\nepoch τ ∈[n], one has\nV ∗\nt(k,τ),1(sinit) = (1 −(1 −κk,τ)L) · H\n2 .\nNow we can complete the proof of Theorem 3.1\nProof of Theorem 3.1. If the input of Max-IP is a YES instance, suppose Cτ ⊆Bk,ν for some\nτ ∈[n], k ∈[K], ν ∈[N]; then κk,τ = c/b = 1/L. By Lemma 3.8, the value of sinit at the end of\nepoch t satisfies\nV ∗\nt(k,τ),1(sinit) = (1 −(1 −κk,τ)L) · H\n2 = (1 −(1 −1/L)L) · H\n2 ≥H\n4 .\nIn the NO instance case, we have\nκk,τ ≤c/wb\nwhere\nw = 2log(n)1−o(1) = Ω(1),\nthen the value of sinit at the end of any stage k ∈[K], epoch τ ∈[n] is at most\nV ∗\nt(k,τ),1(sinit) = (1 −(1 −κk,τ)L) · H/2 ≤(1 −(1 −1/wL)L) · H\n2 ≤1\nw · H\n2 ≤H\n100.\nNow we bound the amortized runtime. By Theorem 3.3, assuming SETH, the total runtime of\nany NSRL algorithm should be at least n2−o(1), and therefore, the amortized runtime per update\nshould be at least n2−o(1)/T = (SAH)1−o(1) · T −o(1) ≈(SAH)1−o(1) when T = poly(SAH). This\ncompletes the proof.\nRemark 3.9. The statement of Theorem 3.1 asserts the decision version of NSRL requires (SAH)1−o(1)\ntime per update. The same lower bound translates directly to the task of maintaining an approximate\nV -value or maintaining an approximately optimal policy.\n9\n4\nIncremental action changes\nWhen the MDP changes only through the introduction of new actions, then we can maintain an\nϵ-approximation to value with amortized runtime that depends, polynomially, only on H and 1\nϵ\n(and not S).\nTheorem 4.1 (Efficient algorithm, incremental changes). There is an algorithm with amortized\nruntime eO(H5/ϵ3) per update that maintains an ϵ-approximation of the value over any sequence of\nT insertions of actions.\nThe approach is given as Algorithm 1. It combines the classic Q-value iteration with lazy updates\non V -value. For each new state-action pair (sh, ah), it constructs the empirical transition kernel\nusing samples from Ph(sh, ah). The newly added action could potentially affect the state value,\nand our algorithm propagates the change — lazily — to downstream states. That is, a change to\nV -value is triggered only if it significantly exceeds the previous estimate. The key mathematical\nintuition is the monotonicity of V -value under incremental action changes. The amortized runtime\nof Algorithm 1 is bounded because the Q-value of each state-action is updated rarely, at most\neO(H3/ϵ2 · H2/ϵ) = eO(H5/ϵ3) times, due to the sparsity of the empirical transition kernel and the\nlazy updates. The correctness of our algorithm follows from the standard Bernstein type bound and\na robust analysis of Q-value iteration. The detailed proof can be found at Appendix C.\nAlgorithm 1 Lazy updated Q-value iteration (Lazy-QVI)\n1: Initialize N ←H3 log3(SHT)/ϵ2, bVh(sh) ←0, eVh(sh) ←0, ∀sh ∈Sh, h ∈[H]\n2: procedure Insert(sh, ah)\n3:\nGenerate N samples {bsh+1,1, . . . , bsh+1,N} from Ph(sh, ah) and reward rh(sh, ah)\n4:\nbPh(sh, ah) ←unif{bsh+1,1, . . . , bsh+1,N}\n5:\nCall Propagate\n6: end procedure\n7: procedure Propagate\n8:\nfor h = H, H −1, . . . , 1 do\n9:\nfor state-action pair (sh, ah) ∈Sh × Ah do\n▷Update only if there is a change\n10:\nbQh(sh, ah) ←rh(sh, ah) + Esh+1∼bPh(sh,ah) eVh+1(sh+1)\n11:\nbVh(sh) ←maxah bQ(sh, ah)\n12:\nIf eVh(sh) ≤bVh(sh) −ϵ/4H then eVh(sh) ←bVh(sh)\n13:\nend for\n14:\nend for\n15: end procedure\nTheorem 4.1 provides an efficient algorithm for approximately optimal policy, one natural ques-\ntion is whether one can maintain the exact optimal policy (or value function) under incremental\naction changes. We give a negative answer, showing that T 1−o(1) runtime is necessary if one wants\nto maintain an O(1/T)-approximation to the value of optimal policy.\nTheorem 4.2 (Lower bound, exact optimal policy). Unless SETH is false, there is a sequence of\nT action insertions such that no algorithm with amortized runtime T 1−o(1) per update can maintain\nan O(1/T)-approximation to the value of optimal policy.\n10\n5\nDiscussion\nThe importance of a complexity result rests on its capacity to inform the development of new\nalgorithms. Our result seems to suggest that a successful heuristic approach to NSRL can alternate\nbetween additional exploration after each change in parameters and, when this brings diminishing\nbenefits, a restart from scratch. This is not unlike some of the approaches taken by some state-\nof-the-art applications [Pad21].\nBy further developing this and similar approaches, the current\nchallenge of NSRL may be eventually tamed. We also note that our negative result leaves open\nthe NSRL problem in the case of function approximation [JYWJ20, AJKS19]; we conjecture that a\nsimilar negative result may be provable in this case as well.\nReferences\n[AAG+19]\nAmir Abboud, Raghavendra Addanki, Fabrizio Grandoni, Debmalya Panigrahi, and\nBarna Saha. Dynamic set cover: improved algorithms and lower bounds. In Proceed-\nings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages\n114–125, 2019.\n[AB09]\nSanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cam-\nbridge University Press, 2009.\n[ACSS20]\nJosh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness\nfor linear algebra on geometric graphs. In 2020 IEEE 61st Annual Symposium on\nFoundations of Computer Science (FOCS), pages 541–552. IEEE, 2020.\n[AJKS19]\nAlekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning:\nTheory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages\n10–4, 2019.\n[AJO08]\nPeter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for\nreinforcement learning. Advances in neural information processing systems, 21, 2008.\n[ARW17]\nAmir Abboud, Aviad Rubinstein, and Ryan Williams. Distributed pcp theorems for\nhardness of approximation in p. In 2017 IEEE 58th Annual Symposium on Founda-\ntions of Computer Science (FOCS), pages 25–36. IEEE, 2017.\n[AW14]\nAmir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong\nlower bounds for dynamic problems. In 2014 IEEE 55th Annual Symposium on Foun-\ndations of Computer Science, pages 434–443. IEEE, 2014.\n[Bel57]\nRichard E Bellman. Dynamic programming. Princeton university press, 1957.\n[Ber12]\nDimitri Bertsekas. Dynamic programming and optimal control: Volume I, volume 1.\nAthena scientific, 2012.\n[BI15]\nArturs Backurs and Piotr Indyk.\nEdit distance cannot be computed in strongly\nsubquadratic time (unless seth is false). In Proceedings of the forty-seventh annual\nACM symposium on Theory of computing, pages 51–58, 2015.\n[BLH17]\nTaposh Banerjee, Miao Liu, and Jonathan P How. Quickest change detection ap-\nproach to optimal control in markov decision processes with model changes. In 2017\nAmerican control conference (ACC), pages 399–405. IEEE, 2017.\n11\n[BRS+18]\nArturs Backurs, Liam Roditty, Gilad Segal, Virginia Vassilevska Williams, and Nicole\nWein. Towards tight approximation bounds for graph diameter and eccentricities. In\nProceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,\npages 267–280, 2018.\n[Che20]\nLijie Chen. On the hardness of approximate and exact (bichromatic) maximum inner\nproduct. Theory of Computing, 16(4):1–50, 2020.\n[CPP22]\nXi Chen, Christos Papadimitriou, and Binghui Peng. Memory bounds for continual\nlearning. In 2022 IEEE 63th Annual Symposium on Foundations of Computer Science\n(FOCS), 2022.\n[CS17]\nMoses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in\nhigh dimensions. In 2017 IEEE 58th Annual Symposium on Foundations of Computer\nScience (FOCS), pages 1032–1043. IEEE, 2017.\n[CSLZ20]\nWang Chi Cheung, David Simchi-Levi, and Ruihao Zhu.\nReinforcement learning\nfor non-stationary markov decision processes: The blessing of (more) optimism. In\nInternational Conference on Machine Learning, pages 1843–1854. PMLR, 2020.\n[DGS14]\nTravis Dick, Andras Gyorgy, and Csaba Szepesvari. Online learning in markov deci-\nsion processes with changing cost sequences. In International Conference on Machine\nLearning, pages 512–520. PMLR, 2014.\n[DLW22]\nMina Dalirrooyfard, Ray Li, and Virginia Vassilevska Williams. Hardness of approx-\nimate diameter: Now for undirected graphs. In 2021 IEEE 62nd Annual Symposium\non Foundations of Computer Science (FOCS), pages 1021–1032. IEEE, 2022.\n[DMP+21]\nOmar Darwiche Domingues, Pierre Ménard, Matteo Pirotta, Emilie Kaufmann, and\nMichal Valko. A kernel-based approach to non-stationary reinforcement learning in\nmetric spaces. In International Conference on Artificial Intelligence and Statistics,\npages 3538–3546. PMLR, 2021.\n[DSBBE06]\nBruno C Da Silva, Eduardo W Basso, Ana LC Bazzan, and Paulo M Engel. Dealing\nwith non-stationary environments using context detection. In Proceedings of the 23rd\ninternational conference on Machine learning, pages 217–224, 2006.\n[FKQR21]\nDylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical\ncomplexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.\n[GAMK13]\nMohammad Gheshlaghi Azar, Rémi Munos, and Hilbert J Kappen. Minimax pac\nbounds on the sample complexity of reinforcement learning with a generative model.\nMachine learning, 91:325–349, 2013.\n[How60]\nRonald A Howard. Dynamic programming and markov processes. 1960.\n[IP01]\nRussell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of\nComputer and System Sciences, 62(2):367–375, 2001.\n[JAZBJ18]\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning\nprovably efficient? Advances in neural information processing systems, 31, 2018.\n12\n[JYWJ20]\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient\nreinforcement learning with linear function approximation. In Conference on Learning\nTheory, pages 2137–2143. PMLR, 2020.\n[KRRP22]\nKhimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards con-\ntinual reinforcement learning: A review and perspectives. Journal of Artificial Intel-\nligence Research, 75:1401–1476, 2022.\n[Li21]\nRay Li. Settling seth vs. approximate sparse directed unweighted diameter (up to\n(nu) nseth). In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory\nof Computing, pages 1684–1696, 2021.\n[LIT95]\nML LITTMAN. On the complexity of solving markov decision problems. Proceedings\nof the 11th International Comference on Uncertainty in Artificial Intelligence, 1995.\n[LL19]\nYingying Li and Na Li. Online learning for markov decision processes in nonstationary\nenvironments: A dynamic regret analysis.\nIn 2019 American Control Conference\n(ACC), pages 1232–1237. IEEE, 2019.\n[LS14]\nYin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solv-\ning linear programs in o(\n√\nrank) iterations and faster algorithms for maximum flow.\nIn 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pages\n424–433. IEEE, 2014.\n[LWC+20]\nGen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample\nsize barrier in model-based reinforcement learning with a generative model. Advances\nin neural information processing systems, 33:12861–12872, 2020.\n[MZZ+21]\nWeichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Basar.\nNear-optimal model-free reinforcement learning in non-stationary episodic mdps. In\nInternational Conference on Machine Learning, pages 7447–7458. PMLR, 2021.\n[OGA20]\nRonald Ortner, Pratik Gajane, and Peter Auer. Variational regret bounds for re-\ninforcement learning. In Uncertainty in Artificial Intelligence, pages 81–90. PMLR,\n2020.\n[OT21]\nMelkior Ornik and Ufuk Topcu. Learning and planning for time-varying mdps us-\ning maximum likelihood estimation.\nThe Journal of Machine Learning Research,\n22(1):1656–1695, 2021.\n[Pad21]\nSindhu Padakandla. A survey of reinforcement learning algorithms for dynamically\nvarying environments. ACM Computing Surveys (CSUR), 54(6):1–25, 2021.\n[Pap03]\nChristos H Papadimitriou. Computational complexity. In Encyclopedia of computer\nscience, pages 260–265. 2003.\n[Pen21]\nBinghui Peng. Dynamic influence maximization. Advances in Neural Information\nProcessing Systems, 34:10718–10731, 2021.\n[PKB20]\nSindhu Padakandla, Prabuchandran KJ, and Shalabh Bhatnagar.\nReinforcement\nlearning algorithm for non-stationary environments. Applied Intelligence, 50:3590–\n3606, 2020.\n13\n[PS98]\nChristos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algo-\nrithms and complexity. Courier Corporation, 1998.\n[PT87]\nChristos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision\nprocesses. Mathematics of operations research, 12(3):441–450, 1987.\n[Put14]\nMartin L Puterman. Markov decision processes: discrete stochastic dynamic program-\nming. John Wiley & Sons, 2014.\n[Rub18]\nAviad Rubinstein. Hardness of approximate nearest neighbor search. In Proceedings of\nthe 50th annual ACM SIGACT symposium on theory of computing, pages 1260–1268,\n2018.\n[RVW13]\nLiam Roditty and Virginia Vassilevska Williams. Fast approximation algorithms for\nthe diameter and radius of sparse graphs. In Proceedings of the forty-fifth annual\nACM symposium on Theory of computing, pages 515–524, 2013.\n[RW19]\nAviad Rubinstein and Virginia Vassilevska Williams. Seth vs approximation. ACM\nSIGACT News, 50(4):57–76, 2019.\n[SB18]\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.\nMIT press, 2018.\n[Sch13]\nBruno Scherrer. Improved and generalized upper bounds on the complexity of policy\niteration. Advances in Neural Information Processing Systems, 26, 2013.\n[SHS+18]\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\net al. A general reinforcement learning algorithm that masters chess, shogi, and go\nthrough self-play. Science, 362(6419):1140–1144, 2018.\n[SSS+17]\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,\nArthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al.\nMastering the game of go without human knowledge.\nnature, 550(7676):354–359,\n2017.\n[SWW+18]\nAaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye.\nNear-optimal\ntime and sample complexities for solving markov decision processes with a generative\nmodel. Advances in Neural Information Processing Systems, 31, 2018.\n[SWWY18]\nAaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye.\nVariance reduced value\niteration and faster algorithms for solving markov decision processes. In Proceedings\nof the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages\n770–787. SIAM, 2018.\n[SWYY20]\nAaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic\ntwo-player games with near-optimal time and sample complexity. In International\nConference on Artificial Intelligence and Statistics, pages 2992–3002. PMLR, 2020.\n[Tse90]\nPaul Tseng. Solving h-horizon, stationary markov decision problems in time propor-\ntional to log (h). Operations Research Letters, 9(5):287–297, 1990.\n14\n[TV20]\nAhmed Touati and Pascal Vincent. Efficient learning in non-stationary linear markov\ndecision processes. arXiv preprint arXiv:2010.12870, 2020.\n[VDBLL+21] Jan Van Den Brand, Yin Tat Lee, Yang P Liu, Thatchaphol Saranurak, Aaron Sidford,\nZhao Song, and Di Wang. Minimum cost flows, mdps, and ℓ1-regression in nearly\nlinear time for dense instances. In Proceedings of the 53rd Annual ACM SIGACT\nSymposium on Theory of Computing, pages 859–869, 2021.\n[WL21]\nChen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior\nknowledge: An optimal black-box approach. In Conference on Learning Theory, pages\n4300–4354. PMLR, 2021.\n[Ye05]\nYinyu Ye. A new complexity result on solving the markov decision problem. Mathe-\nmatics of Operations Research, 30(3):733–749, 2005.\n[Ye11]\nYinyu Ye.\nThe simplex and policy-iteration methods are strongly polynomial for\nthe markov decision problem with a fixed discount rate. Mathematics of Operations\nResearch, 36(4):593–603, 2011.\n[ZCVJ22]\nHuozhi Zhou, Jinglin Chen, Lav R Varshney, and Ashish Jagmohan. Nonstationary\nreinforcement learning with linear function approximation. Transactions on Machine\nLearning Research, 2022.\n15\nA\nAdditional related work\nComputational complexity of reinforcement learning\nThe computational complexity of\n(stationary) MDP has been a central topic across multiple disciplines. The study of MDP dates back\nto Bellman [Bel57] in 1950s, and since then, there is a long line of work concerning the computational\nefficiency of MDP [Tse90, LIT95, How60, Ye11, Sch13, Ye05, SWWY18, SWW+18, SWYY20, LS14,\nVDBLL+21, PT87]. The classical approaches include value iteration, policy iteration and linear\nprogramming, see [Put14, Ber12] for reference.\nFor a finite horizontal MDP with S states, A\nactions and H steps, the value iteration could return the optimal policy with runtime O(S2AH)\nthat is linear in the input size. If the algorithm could sample from the transition function (a.k.a. the\ngenerative model), then [GAMK13] provide an algorithm that returns an ϵ-approximation to the\nV -value with runtime eO(SAH3/ϵ2). For non-stationary MDP, it implies an algorithm with runtime\neO(S2AH + SAH3T/ϵ2) for ϵ-value approximation over a sequence of T updates. This is because\nthe algorithm could always re-compute from scratch, and it can sample the transition function in\nO(log(S)) time using a binary tree data structure, after reading the input initially.\nBesides computation complexity, a large number of work concern about the sample complexity\nin generative model (e.g. [GAMK13, LWC+20]) and regret in model-free RL (e.g. [JAZBJ18]), in\ntabular setting as well as functional approximation setting [FKQR21].\nB\nMissing proof from Section 3\nProof of Lemma 3.4. This is quite obvious, as the terminal state always stays on itself (Eq. (2)), the\nreward of st\nh,2 is always 0, while the reward of st\nh,1 is 0 in phase one and 1 in phase two (Eq. (1)).\nProof of Lemma 3.5. For an element u ∈Cτ, a policy could choose to never leave se\nu (Eq. (3)), and\nit receives the maximum H/2 reward (see Eq. (9)(1)). For an element u /∈Cτ, the policy needs to\nstay at se\nu until the end of layer ℓ(see Eq. (3)). While at the end of layer ℓ, it could move to pivotal\nstate sp\nh(ℓ)+1 or stay on itself (Eq. (4)). The later obtains strictly less reward, because the pivotal\nstate could always stay on itself (Eq. (6)), and the value V ∗\nt(k,τ),H/2(se\nH/2,u) = 0 at the end of phase\n1 (see Eq. (9)(1)). This completes the proof.\nProof of Lemma 3.6. The Q-value of choosing action ab\nj (j ∈[A/2]) equals\nQ∗\nt(k,τ),h(ℓ,g)−1(sb\nh(ℓ,g)−1,i, ab\nj)\n=\nX\nu∈[m]\nPr[sh(ℓ,g) = se\nh(ℓ,g),u] · V ∗\nt(k,τ),h(ℓ,g)(sb\nh(ℓ,g),u)\n=\nX\nu∈Cτ\nPr[sh(ℓ,g) = se\nh(ℓ,g),u] · H\n2 +\nX\nu∈[m]\\Cτ\nPr[sh(ℓ,g) = se\nh(ℓ,g),u] · V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1)\n= |Cτ ∩Bk,N(g,i,j)|\nb\n· H\n2 +\n\u0012\n1 −|Cτ ∩Bk,N(g,i,j)|\nb\n\u0013\n· V ∗\nt(k,τ),ℓ+1(sp\nh(ℓ)+1).\nThe first step follows from Bellman’s equation and the state-action pair (sb\nh(ℓ,g)−1,i, ab\nj) receives 0\nreward (Eq. (1)), the second step follows from Lemma 3.5, the last step follows from Eq. (5). The\nproof follows by taking the maximum over action {ab\nj}j∈[A/2].\nProof of Lemma 3.7. For each level ℓ∈[L], the transition functions of the pivotal state and routing\nstates guarantee that a policy can visit exactly one set state in Hℓ. To see this, it can visit at most\n16\none set state because the element state stays on itself till the end of the layer (Eq. (3)). Meanwhile,\nit can go to any set state ν ∈[N] with ν = N(g, i, j) for some i ∈[S/2] and g ∈[G], because it\ncan first go to the pivotal state sp\nh(ℓ,g−1)+1,0 at the beginning of group g, then move to sr\nh(ℓ,g)−1,i\nthrough routing states (see Eq. (7)(8)). Combining Lemma 3.6, we have\nV ∗\nt(k,τ),h(ℓ−1)+1(sp\nh(ℓ−1)+1)\n= max\ng∈[G] max\ni∈[S/4] V ∗\nt(k,τ),h(ℓ,g)−1(sb\nh(ℓ,g)−1,i)\n= max\ng∈[G] max\ni∈[S/4] max\nj∈[A/2]\n\u001a|Cτ ∩Bk,N(g,i,j)|\nb\n· H\n2 +\n\u0012\n1 −|Cτ ∩Bk,N(g,i,j)|\nb\n\u0013\n· V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1)\n\u001b\n= max\nν∈[N]\n\u001a|Cτ ∩Bk,ν|\nb\n· H\n2 +\n\u0012\n1 −|Cτ ∩Bk,ν|\nb\n\u0013\n· V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1)\n\u001b\n.\nThis completes the proof of the lemma.\nProof of Lemma 3.8. By Lemma 3.7, for any ℓ∈[L], we have\nV ∗\nt(k,τ),h(ℓ−1)+1(sp\nh(ℓ−1)+1)\n= max\nν∈[N]\n\u001a|Cτ ∩Bk,ν|\nb\n· H\n2 +\n\u0012\n1 −|Cτ ∩Bk,ν|\nb\n\u0013\n· V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1)\n\u001b\n= κk,τ · H\n2 + (1 −κk,τ)V ∗\nt(k,τ),h(ℓ)+1(sp\nh(ℓ)+1).\nSolving the above recursion, one has\nV ∗\nt(k,τ),1(sinit) = V ∗\nt(k,τ),1(sp\n1) =\nL\nX\nℓ=1\nκk,τ(1 −κk,τ)ℓ−1 · H\n2 = (1 −(1 −κk,τ)L) · H\n2 .\nThis completes the proof of the lemma.\nC\nMissing proof from Section 4\nWe first state the concentration bounds used in the paper.\nLemma C.1 (Hoeffding bound). Let X1, · · · , Xn be n independent bounded variables in [ai, bi]. Let\nX = Pn\ni=1 Xi, then we have\nPr[|X −E[X]| ≥t] ≤2 exp\n\u0012\n−\n2t2\nPn\ni=1(bi −ai)2\n\u0013\n.\nLemma C.2 (Bernstein bound). Let X1, · · · , Xn be n independent zero mean random variables and\n|Xi| ≤M. Let X = Pn\ni=1 Xi, σ = Pn\ni=1 E[X2\ni ] then we have\nPr[|X| ≥t] ≤2 exp\n\u0012\n−\n2t2\nMt/3 + σ2\n\u0013\n.\nIn particular, with probability at least 1 −δ, one has\n|X| ≤(M/3 + σ) · log(1/δ).\n17\nWe prove Algorithm 1 gives ϵ-approximation to both V -value and Q-value. For notation conve-\nnience, we drop the subscript of round number t in the proof.\nLemma C.3 (Value approximation). At the end of t-th update (t ∈[T]), for any step h ∈[H],\nstate sh ∈Sh and action ah, with probability at least 1 −(SHT)−ω(1), we have\n|V ∗\nh (sh) −bVh(sh)| ≤ϵ/2\nand\n|Q∗\nh(sh, ah) −bQh(sh, ah)| ≤ϵ\nProof. We prove the claim by induction. The base case of h = H holds trivially, as there is no error.\nSuppose the claim holds up to step h + 1, then for the h-th step, we have\nbQh(sh, ah) = rh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\neVh+1(sh+1)\n= rh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\nbVh+1(sh+1) ±\nϵ\n4H\n= rh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\n[V ∗\nh+1(sh+1)]\n+\nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1) −V ∗\nh+1(sh+1)] ±\nϵ\n4H ,\n(10)\nwhere the first step follows from the update rule of Algorithm 1, the second step holds since that\nthe propagate value eVh+1(sh+1) satisfies\n\f\f\fbVh+1(sh+1) −eVh+1(sh+1)\n\f\f\f ≤\nϵ\n4H ,\n∀sh+1 ∈Sh+1.\nWe bound the second term of Eq. (10) in terms of variance. Define\nσh(sh, ah)2 :=\nE\nsh+1∼Ph(sh,ah)[V ∗\nh+1(sh+1)2] −\n\u0012\nE\nsh+1∼Ph(sh,ah)[V ∗\nh+1(sh+1)]\n\u00132\n.\nBy Bernstein inequality, we have with probability at least 1 −(SHT)−ω(1),\n\f\f\f\f\f\nE\nsh+1∼bPh(sh,ah)\n[V ∗\nh+1(sh+1)] −\nE\nsh+1∼Ph(sh,ah)[V ∗\nh+1(sh+1)]\n\f\f\f\f\f ≲H +\n√\nNσh(sh, ah)\nN\n· log(SHT)\n≤ϵ2\nH2 +\nϵ\n16H3/2 · σh(sh, ah).\nPlugging into Eq. (10), we have\nbQh(sh, ah)\n= rh(sh, ah) +\nE\nsh+1∼Ph(sh,ah)[V ∗\nh+1(sh+1)] +\nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1) −V ∗\nh+1(sh+1)]\n±\nϵ\n16H3/2 · σh(sh, ah) ±\nϵ\n3H\n= Q∗\nh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1) −V ∗\nh+1(sh+1)] ±\nϵ\n16H3/2 · σh(sh, ah) ±\nϵ\n3H .\n(11)\nWe bound the V -value difference bVh(sh)−V ∗\nh (sh) and provide upper and lower bounds separately.\n18\nUpper bound bVh(sh)−V ∗\nh (sh). Let bπ be the policy induced by bQ, that is, for any state sℓ∈Sℓ,\nbπ(sℓ) = argmaxaℓbQℓ(sℓ, aℓ). Then for any state sh ∈Sh, one has\nbVh(sh) −V ∗\nh (sh)\n= bQh(sh, bπ(sh)) −Q∗\nh(sh, π∗(sh))\n= bQh(sh, bπ(sh)) −Q∗\nh(sh, bπ(sh)) + Q∗\nh(sh, bπ(sh)) −Q∗\nh(sh, π∗(sh))\n≤bQh(sh, bπ(sh)) −Q∗\nh(sh, bπ(sh))\n≤\nE\nsh+1∼bPh(sh,bπ(sh))\n[bVh+1(sh+1) −V ∗\nh+1(sh+1)] ±\nϵ\n16H3/2 σh(sh, bπ(sh)) +\nϵ\n3H ,\n(12)\nwhere the third step follows from the optimality of π∗, the fourth step follows from Eq. (11).\nFix the state sh ∈Sh, for any step ℓ∈[h : H] and state sℓ∈Sℓ, let bp(sℓ) be the probability\nthat policy ˆπ goes to state sℓ, starting from sh. Recurring Eq. (12), we obtain\nbVh(sh) −V ∗\nh (sh) ≤\nϵ\n16H3/2 ·\nH\nX\nℓ=h\nX\nsℓ∈Sℓ\nbp(sℓ)σℓ(sℓ, bπ(sℓ)) + ϵ\n3\n≤\nϵ\n16H\nv\nu\nu\nt\nH\nX\nℓ=h\nbp(sℓ)σℓ(sℓ, bπ(sℓ))2 + ϵ\n3.\n(13)\nHere the first step follows Eq. (12), the second step follows from Cauchy Schwarz inequality and\nP\nsℓ∈Sℓbp(sℓ) = 1 holds for any ℓ≥h.\nWe need the following two technical Lemmas.\nLemma C.4 (Connection with empirical variance). Define the empirical variance\nbσh(sh, ah)2 :=\nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1)2] −\n \nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1)]\n!2\nThen with probability at least 1 −(SHT)−ω(1), one has\n|σh(sh, ah)2 −bσh(sh, ah)2| ≤H.\nProof. First, by Hoeffding inequality, with probability at least 1 −(SHT)−ω(1), one has\n\f\f\f\f\f\nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1)2] −\nE\nsh+1∼Ph(sh,ah)[bVh+1(sh+1)2]\n\f\f\f\f\f ≤4H2√\nN log(SHT)\nN\n≤H/4.\nBy induction hypothesis, one has |bVh+1(sh+1) −Vh+1(sh+1)| ≤ϵ for any state sh+1 ∈Sh+1, and\ntherefore,\n\f\f\f\f\nE\nsh+1∼Ph(sh,ah)[bVh+1(sh+1)2 −Vh+1(sh+1)2]\n\f\f\f\f ≤2ϵH ≤H/4\nSimilarly, by Hoeffding bound, we have with probability at least 1 −(SHT)−ω(1),\n\f\f\f\f\f\nE\nsh+1∼bPh(sh,ah)\n[bVh+1(sh+1)] −\nE\nsh+1∼Ph(sh,ah)[bVh+1(sh+1)]\n\f\f\f\f\f ≤4\n√\nNH log(SHT)\nN\n≤ϵ\n19\nand by induction hypothesis,\n\f\f\f\f\nE\nsh+1∼Ph(sh,ah)[bVh+1(sh+1) −Vh+1(sh+1)]\n\f\f\f\f ≤ϵ\nCombining the above four inequalities, one can conclude the proof.\nLemma C.5 (Upper bound on empirical variance). We have\nH\nX\nℓ=h\nbp(sℓ)bσℓ(sℓ, bπ(sℓ))2 ≤3H2\nProof. We have\nH\nX\nℓ=h\nX\nsℓ∈Sℓ\nbp(sℓ)bσℓ(sℓ, bπ(sℓ))2\n=\nH\nX\nℓ=h\nX\nsℓ∈Sℓ\nbp(sℓ) ·\n\n\nE\nsℓ+1∼bPℓ(sℓ,bπ(sℓ))\n[bVℓ+1(sℓ+1)2] −\n \nE\nsℓ+1∼bPℓ(sℓ,bπ(sℓ))\n[bVℓ+1(sℓ+1)]\n!2\n\n≤\nH\nX\nℓ=h+1\nX\nsℓ∈Sℓ\nbp(sℓ)\n\nbVℓ(sℓ)2 −\n \nE\nsℓ+1∼bP(sℓ,bπ(sℓ))\n[bVℓ+1(sℓ+1)]\n!2\n+ 1\n=\nH\nX\nℓ=h+1\nX\nsℓ∈Sℓ\nbp(sℓ)\n\n\n \nE\nsℓ+1∼bP(sℓ,bπ(sℓ))\n[eVℓ+1(sℓ+1) + rℓ(sℓ, bπ(sℓ))]\n!2\n−\n \nE\nsℓ+1∼bP(sℓ,bπ(sℓ))\n[bVℓ+1(sℓ+1)]\n!2\n+ 1\n≤\nH\nX\nℓ=h+1\nX\nsℓ∈Sℓ\nbp(sℓ) · 2H · (1 + ϵ/H) + 1 ≤3H2.\nThe first step follows from the definition of empirical variance bσℓ. The second step is important and\nit holds due to the definition of visiting probability bpℓ, and we use the naive bound of bVH(sH) ≤1\nfor any state sH ∈SH in last step. The third step holds due to the definition of bVℓ(sℓ). The last\nstep holds due to\n\f\f\f\f\f\nE\nsℓ+1∼bP(sℓ,bπ(sℓ))\neVℓ+1(sℓ+1) + rℓ(sℓ, bπ(sℓ)) −bVℓ+1(sℓ+1)\n\f\f\f\f\f ≤1 + ϵ/H\nas |eVℓ+1(sℓ+1) −bVℓ+1(sℓ+1)| ≤ϵ/H and rℓ(sℓ, bπ(sℓ)) ≤1.\nCombining Lemma C.4, Lemma C.5 and Eq. (13), we have that\nbVh(sh) −V ∗\nh (sh) ≤\nϵ\n16H\nv\nu\nu\nt\nH\nX\nℓ=h\nbp(sℓ)σℓ(sℓ, bπ(sℓ))2 + ϵ\n3\n=\nϵ\n16H\nv\nu\nu\nt\nH\nX\nℓ=h\nbp(sℓ)bσℓ(sℓ, bπ(sℓ))2 + H2 + ϵ\n3\n≤\nϵ\n16H ·\np\n3H2 + H2 + ϵ\n3 ≤ϵ\n2\n(14)\n20\nLower bound bVh(sh) −V ∗\nh (sh). The proof for lower bound is similar. First, we have\nV ∗\nh (sh) −bVh(sh)\n= Q∗\nh(sh, π∗(sh)) −bQh(sh, bπ(sh))\n= Q∗\nh(sh, π∗(sh)) −bQh(sh, π∗(sh)) + bQh(sh, π∗(sh)) −bQh(sh, bπ(sh))\n≤Q∗\nh(sh, π∗(sh)) −bQh(sh, π∗(sh))\n≤\nE\nsh+1∼bPh(sh,π∗(sh))\n[V ∗\nh+1(sh+1) −bVh+1(sh+1)] +\nϵ\n16H3/2 σh(sh, π∗(sh)) +\nϵ\n3H ,\n(15)\nwhere the third step follows from the optimality of bπ, the fourth step follows from Eq. (11).\nUsing Hoeffding bound and the induction hypothesis |V ∗\nh+1(sh+1) −bVh+1(sh+1)| ≤ϵ, with prob-\nability at least 1 −(SHT)−ω(1), we have\nE\nsh+1∼bPh(sh,π∗(sh))\n[V ∗\nh+1(sh+1) −bVh+1(sh+1)] −\nE\nsh+1∼Ph(sh,π∗(sh))[V ∗\nh+1(sh+1) −bVh+1(sh+1)]\n≤2ϵ\n√\nN log(SHT)\nN\n≤\nϵ\n24H .\nPlug into Eq. (15), we have\nV ∗\nh (sh) −bVh(sh) ≤\nE\nsh+1∼Ph(sh,π∗(sh))[V ∗\nh+1(sh+1) −bVh+1(sh+1)]\n+\nϵ\n16H3/2 σh(sh, π∗(sh)) + 3ϵ\n8H .\nFix the state sh ∈Sh, for any step ℓ∈[h : H] and state sℓ∈Sℓ, let p∗(sℓ) be the probability\nthat policy π∗goes to state sℓ, starting from sh. Recurring the above equation, we obtain\nV ∗\nh (sh) −bVh(sh) ≤\nϵ\n16H3/2 ·\nH\nX\nℓ=h\nX\nsℓ∈Sℓ\np∗(sℓ)σℓ(sℓ, π∗(sℓ)) + 3ϵ\n8\n≤\nϵ\n16H\nv\nu\nu\nt\nH\nX\nℓ=h\np∗(sℓ)σℓ(sℓ, π∗(sℓ))2 + 3ϵ\n8 .\n≤\nϵ\n16H\n√\n3H2 + ϵ\n3 ≤ϵ/2.\n(16)\nWe use Cauchy Schwarz in the second step, the third step follows from the following Lemma (the\nproof is similar to Lemma C.5 and we omit it here).\nLemma C.6 (Upper bound on variance). We have\nH\nX\nℓ=h\np∗(sℓ)σℓ(sℓ, π∗(sℓ))2 ≤3H2.\nCombining Eq. (14) and Eq. (16), we conclude the proof for V -value.\n21\nFor Q-value, we have with probability at least 1 −(SHT)−ω(1),\nbQh(sh, ah) = rh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\neVh+1(sh+1)\n= rh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\nbVh+1(sh+1) ±\nϵ\n4H\n= rh(sh, ah) +\nE\nsh+1∼bPh(sh,ah)\nV ∗\nh+1(sh+1) ± ϵ/2 ±\nϵ\n4H\n= rh(sh, ah) +\nE\nsh+1∼Ph(sh,ah) V ∗\nh+1(sh+1) ± ϵ\n= Q∗\nh(sh, ah) ± ϵ.\nThe first step uses the update rule of Algorithm 1, the second step holds since |eVh+1(sh+1) −\nbVh+1(sh+1)| ≤ϵ/4H. The third follows from the guarantee of V -value, and the fourth step follows\nfrom Hoeffding bounds and the last step follows from Bellman equation. We finish the induction\nand complete the proof here.\nWe next bound the total update time of Algorithm 1.\nLemma C.7 (Total update time). The total update time of Algorithm 1 is at most eO(TH5/ϵ3) over\na sequence of T action insertions.\nProof. For each new action (sh, ah), the construction of bPh(sh, ah) takes eO(N) = eO(H3/ϵ2) time.\nThe major overhead comes from the Propagate part. First, note the propagated V -value eVh(sh) of\na state sh can change at most H/(ϵ/4H) = O(H2/ϵ) times. Next, for each state-action pair (sh, ah),\nthe Q-value bQh(sh, ah) = rh(sh, ah)+Esh+1∼bPh(sh,ah) eVh+1(sh+1) can change at most O(N ·H2/ϵ) =\neO(H5/ϵ3) times, because the support of bPh(sh, ah) has size at most N, and each estimate eVh+1(sh+1)\nchanges at O(H2/ϵ) times as stated above. The total number of state-action pair is bounded by T.\nWe conclude the proof.\nThe proof of Theorem 4.1 follows directly from Lemma C.3 and Lemma C.7.\nWe next prove the lower bound.\nProof of Theorem 4.2. Let n = T 1−o(1), m = no(1). We reduce from Max-IP with sets B1, . . . , Bn\nand C1, . . . , Cn defined over ground element [m]. The MDP contains H = 3 steps. There is one\nsingle initial state s1 at the first step h = 1. In the second step (h = 2), there are m = no(1) states\ns2,1, . . . , s2,m, and at the last step (h = 3), there are two states s3,1, s3,2.\nThe sequence of new actions is as follow. There is one action a3 for the last step, and the reward\nsatisfies r3(s3,1, a3) = 1 and r3(s3,1, a3) = 0, i.e., the reward is 1 for s3,1 and 0 for s3,2. There are n\nactions a1,1, . . . , a1,n for the initial state at the first step, and we have\nP1(s1, a1,i) = unif({s2,k : k ∈Bi})\nand\nr1(s1, a1,i) = 0\n∀i ∈[n].\nThe rest sequence divides into n epochs, and in the j-th epoch (j ∈[n]), there is one new action\na2,j for each state {s2,k}k∈[m]. Let δ = 1/4n. At the end of j-th epoch, t(j) ∈[T], the transition\nand the reward of the new action a2,j satisfies\nP2(sk, a2,j) =\n(\n(\nj\nn+1 + δ, 1 −\nj\nn+1 −δ)\nk ∈Cj\n(\nj\nn+1, 1 −\nj\nn+1)\nk /∈Cj\nand\nr2(sk, a2,j) = 0.\nIn summary, the total number of state-action pairs at the end is 2 + n + mn = n1+o(1) = T.\nFirst, a simple observation on the value function\n22\nLemma C.8. At the end of epoch j ∈[n], the optimal policy satisfies\n• V ∗\nt(j)(s3,1) = 1 and V ∗\nt(j)(s3,1) = 0\n• V ∗\nt(j)(s2,k) =\nj\nn+1 + δ when k ∈Cj and V ∗\nt(j)(s2,k) =\nj\nn+1 otherwise\n• V ∗\nt(j)(s1) =\nj\nn+1 + κj · δ, where κj = maxi∈[n]\n|Cj∩Bi|\nb\nProof. The first claim is trivial. The second claim holds since the action a2,j is the optimal choice by\nthe end of epoch j, and its value equals Q∗\nt(j)(s2,k, aj) =\nj\nn+1 + δ when k ∈Cj and Q∗\nt(j)(s2,k, aj) =\nj\nn+1 when k /∈Cj. For the last claim, for any i ∈[n], we have\nQ∗\nt(j)(s1, a1,i) =\nX\nk∈[m]\nPr[s2 = s2,k] · V ∗\nt(j)(s2,k)\n=\nX\nk∈Cj\nPr[s2 = s2,k] · V ∗\nt(j)(s2,k) +\nX\nk∈[m]\\Cj\nPr[s2 = s2,k] · V ∗\nt(j)(s2,k)\n= |Bi ∩Cj|\nb\n·\n\u0012\nj\nn + 1 + δ\n\u0013\n+\n\u0012\n1 −|Bi ∩Cj|\nb\n\u0013\n·\nj\nn + 1\n=\nj\nn + 1 + δ · |Bi ∩Cj|\nb\n.\nTaking maximum over i ∈[n], we have V ∗\nt(j)(s1) =\nj\nn+1 + κj · δ.\nHence, any algorithm that returns δ/b = O(1/mn) = O(1/T) approximation to optimal V -\nvalue could distinguish between YES/NO instance of Max-IP, and therefore, assuming SETH is\ntrue, there is no algorithm with n2−o(1)/T = T 1−o(1) amortized runtime per update.\n23\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.DS",
    "stat.ML"
  ],
  "published": "2023-07-13",
  "updated": "2023-07-13"
}