{
  "id": "http://arxiv.org/abs/2307.07699v1",
  "title": "Leveraging Large Language Models to Generate Answer Set Programs",
  "authors": [
    "Adam Ishay",
    "Zhun Yang",
    "Joohyung Lee"
  ],
  "abstract": "Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated\nexceptional performance in various natural language processing tasks and have\nshown the ability to solve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the application of\nvarious prompting techniques. In contrast, formal logic is adept at handling\ncomplex reasoning, but translating natural language descriptions into formal\nlogic is a challenging task that non-experts struggle with. This paper proposes\na neuro-symbolic method that combines the strengths of large language models\nand answer set programming. Specifically, we employ an LLM to transform natural\nlanguage descriptions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language descriptions into answer\nset programs in a step by step manner. Surprisingly, with just a few in-context\nlearning examples, LLMs can generate reasonably complex answer set programs.\nThe majority of errors made are relatively simple and can be easily corrected\nby humans, thus enabling LLMs to effectively assist in the creation of answer\nset programs.",
  "text": "Leveraging Large Language Models to Generate Answer Set Programs\nAdam Ishay1 , Zhun Yang1 , Joohyung Lee1,2\n1Arizona State University\n2Samsung Research\n{aishay, zyang90, joolee}@asu.edu\nAbstract\nLarge language models (LLMs), such as GPT-3 and GPT-4,\nhave demonstrated exceptional performance in various natu-\nral language processing tasks and have shown the ability to\nsolve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the ap-\nplication of various prompting techniques. In contrast, formal\nlogic is adept at handling complex reasoning, but translating\nnatural language descriptions into formal logic is a challeng-\ning task that non-experts struggle with. This paper proposes a\nneuro-symbolic method that combines the strengths of large\nlanguage models and answer set programming. Specifically,\nwe employ an LLM to transform natural language descrip-\ntions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language de-\nscriptions into answer set programs in a step by step man-\nner. Surprisingly, with just a few in-context learning exam-\nples, LLMs can generate reasonably complex answer set pro-\ngrams. The majority of errors made are relatively simple and\ncan be easily corrected by humans, thus enabling LLMs to\neffectively assist in the creation of answer set programs.\n1\nIntroduction\nTransformer-based large language models (LLMs) have\nrecently shown remarkable success in many downstream\ntasks, demonstrating their general reasoning capability\nacross diverse problems. However, while LLMs excel in\ngenerating System 1 thinking, they struggle with System 2\nthinking, resulting in output that is often inconsistent and\nincoherent (Nye et al. 2021). This is because LLMs are ba-\nsically trained to predict subsequent words in a sequence and\ndo not appear to have a deep understanding of concepts such\nas cause and effect, logic, and probability, which are essen-\ntial for reasoning.\nTo address the issue, Nye et al. (2021) propose a dual-\nsystem model that combines the strengths of LLMs and sym-\nbolic logic to achieve improved performance on reasoning\ntasks. They leverage an LLM to generate a System 1 pro-\nposal and employ symbolic computation to filter these pro-\nposals for consistency and soundness.\nWe are interested in situations where problems are de-\nscribed in natural language and solving them requires deep\nreasoning. A system needs to take into account linguistic\nvariability and be able to perform symbolic reasoning. We\ntake logic puzzles as the testbed as they are well-suited for\nthis purpose.\nWe first note that GPT-3 (Brown et al. 2020) and GPT-\n41 by themselves struggle with solving logic puzzles, de-\nspite various prompts we tried. On the other hand, we find\nthat they can convert the natural language descriptions of\nthe puzzles into declarative answer set programming lan-\nguages (Lifschitz 2008; Brewka, Niemel¨a, and Truszczynski\n2011) surprisingly well. Even the errors these LLMs make\nare mostly simple for humans to correct. We hope that our\nfinding will ease the efforts of writing answer set programs\nand expand the application of answer set programming to a\nbroader audience.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 offers a brief overview of related work on automated\nsolving of logic puzzles. Sections 3 and 4 delve into the pro-\nposed approach in detail. Section 5 presents experimental\nresults and performance evaluations of the approach. Sec-\ntion 6 shows more examples demonstrating the generaliz-\nability of our method.\nThe code is available at https://github.com/azreasoners/\ngpt-asp-rules.\n2\nPreliminaries\n2.1\nLarge Language Models (LLMs)\nLLMs have significantly improved natural language pro-\ncessing, achieving strong performance on a variety of tasks\nusing few-shot learning (Brown et al. 2020).\nHowever,\nLLMs remain weak at tasks that involve complex reason-\ning (Creswell, Shanahan, and Higgins 2022; Valmeekam\net al. 2022), and scaling model size alone is not enough\nto achieve good performance (Rae et al. 2021).\nIt has\nbeen shown that various prompting methods improve accu-\nracy on reasoning tasks (Wei et al. 2022; Zhou et al. 2022;\nCreswell, Shanahan, and Higgins 2022). Nye et al. (2021)\npresent a dual-system model which uses an LLM as a se-\nmantic parser and couples it with a custom symbolic mod-\nule to achieve performance gains on reasoning tasks. This\nframework combines the strengths of LLMs for parsing\ncomplex natural language and symbolic logic for handling\n1Throughout the paper, we use GPT-3 to refer to the “text-\ndavinci-003” model and GPT-4 to refer to the “gpt-4-0314” (re-\nleased March, 2023) model in the OpenAI API.\narXiv:2307.07699v1  [cs.AI]  15 Jul 2023\ncomplex reasoning. However, the authors had to use hand-\nengineered set of constraints for the latter part.\nTo our\nknowledge, our work is the first to use LLMs to generate\nlogic rules to solve complex reasoning tasks.\n2.2\nAutomated Logic Puzzle Solving\nWorks focused on solving logic puzzles typically involve a\nmapping from natural language to logic formalism. This\nprocess often includes problem simplification techniques,\nsuch as tailoring the puzzle to a specific domain, restrict-\ning natural language input to a certain form, or assuming\nadditional inputs like enumerated types. Lev et al. (2004)\nemploy a specialized automated multi-stage parsing process\nto convert natural language text into an intermediate form\ncalled Semantic Logic, which is then converted into First\nOrder Logic to finally evaluate on law school admissions\ntests (LSAT) and the Graduate Records Examination (GRE).\nShapiro (2011) manually encodes the “Jobs Puzzle” in a\nfew different logical formalisms and compare them. Puz-\nzler (Milicevic, Near, and Singh 2012) uses a general link\nparser to translate puzzles into to the Alloy language for\nsolving, primarily through an automated process, albeit with\nassumed types. LogicSolver (Nordstrom 2017) follows a\nsimilar approach to Puzzler but replaces Alloy with a cus-\ntom solver and conducts a more comprehensive evaluation.\nSeveral works utilize translations into the language of\nanswer set programming (ASP) (Lifschitz 2008; Brewka,\nNiemel¨a, and Truszczynski 2011).\nSchwitter (2013) ad-\ndresses the “Jobs Puzzle” by representing the problem using\ncontrolled natural language (Schwitter 2010), which can be\nfurther turned into ASP. Baral and Dzifcak (2012) employ a\nλ-calculus-based approach and trains a model that converts\na manually simplified version of natural language clues into\nASP rules for solving Zebra puzzle-type logic puzzles. Mi-\ntra and Baral (2015) train a maximum entropy-based model\nto extract relations for each clue, which are then converted\ninto a common ASP rule format, where a stable model corre-\nsponds to the puzzle solution. LGPSolver (Jabrayilzade and\nTekir 2020) uses DistilBERT, a transformer-based model, as\na classifier that can distinguish between representative rule\ntypes. With the clue classification, the authors use a hand-\ncrafted clue to Prolog translation (as opposed to ASP) and\ncompute the solution. The works mentioned involve some\ncombination of manual processing and/or brittle problem-\nspecific translations. Our work distinguishes itself by be-\ning both fully automated and featuring a general pipeline,\nleveraging the extensive translation capacity available from\nLLMs.\n2.3\nGenerate-Define-Test with ASP\nASP programs are typically written following the Generate-\nDefine-Test structure, which generates potential solutions\n(Generate) and eliminates invalid ones based on certain con-\nstraints (Test). The Generate portion usually includes choice\nrules, while the Test portion consists of a set of constraints\nthat prune out invalid solutions. An additional part of the\nprogram, the Define portion, includes necessary auxiliary\npredicates that are used in the Test portion.\n3\nMethod\nFigure 1: Flow of Generating Answer Set Programs from Logic\nPuzzle in English\nIn order to find a solution to a logic puzzle, we utilize\nGPT-3 to convert the puzzle into an answer set program\nso that the stable model (a.k.a answer set) encodes the so-\nlution.2\nAlthough GPT-3 exhibits strong capabilities, we\ndiscovered that it cannot generate a correct answer set pro-\ngram without being guided by carefully engineered prompts.\nThese prompts instructs GPT-3 to reliably extract constants\nand generate accurate predicates and rules. In this paper, we\ndetail our prompt engineering efforts.\nFigure 1 illustrates the structure of our pipeline, which uti-\nlizes GPT-3 step by step to generate an ASP program. Sim-\nilar to how a human would approach the task, our pipeline\nfirst extracts the relevant objects and their categories. Then,\nit generates a predicate that describes the relations among\nthe objects from different categories. Using the generated\ninformation, the pipeline further constructs an ASP program\nin the style of Generate-Define-Test.\nLet Fc and Fp denote the Constant Extraction and Predi-\ncate Generation steps in Figure 1. Let Fr1 and Fr2 represent\nthe two parts of the Rule Generation step, i.e., the Generate\npart and the Define&Test part, respectively. Our pipeline can\nbe modeled by the following equations that map a puzzle\nstory q to an ASP program Π = Πgenerate ∪Πdefine test.\nc =Fc(q)\np =Fp(q, c)\nΠgenerate =Fr1(c, p)\nΠdefine test =Fr2(q, c, p).\nHere, c and p denote extracted objects and generated pred-\nicates.\nEach step F∗is realized by GPT-3 with 2-shot\nprompting, i.e., only 2 examples in each prompt.\n3.1\nConstant Extraction\nThe first step in the pipeline is to extract constants or entities\nfrom the given story along with their corresponding cate-\ngories. To accomplish this, we invoke GPT-3 using Prompt\nC, which consists of three parts: instruction, examples, and\na query.\nPrompt C:\n2Though this section mostly mentions GPT-3, GPT-4 can be\nused instead.\n1\nGiven a problem, extract all different\nconstants and their categories in the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\". Here, the format of each constant\nis turned into either an integer or a string\nsurrounded by double quotes, e.g.,\"some name\".\n2\n3\nProblem 1:\n4\nConsider N-Queens Puzzle on a chessboard of\nsize 8x8. The goal is to assign 8 queens on\nthe chessboard so that no two queens can share\nthe same row, column, or diagonal.\n5\n6\nConstants:\n7\nindex_of_row: 1; 2; 3; 4; 5; 6; 7; 8.\n8\nindex_of_column: 1; 2; 3; 4; 5; 6; 7; 8.\n9\n10\nProblem 2:\n11\n\"Against the Grain\" offers hand-made wooden\nfurniture at reasonable prices. Each item is\nmade by an in-house employee. Using only the\nclues that follow, match each item to the\nemployee who crafted it, and determine its\nprice and the type of wood used to make it.\nRemember, as with all grid-based logic puzzles\n, no option in any category will ever be used\nmore than once.\n12\n1. Bonita’s piece costs $325.\n13\n2. The item made of poplar costs more than\nYvette’s piece.\n14\n3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n15\n4. The $275 item is either the piece made of\nash or Yvette’s item.\n16\n17\nConstants:\n18\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n19\nprice: 225; 275; 325.\n20\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n21\n22\nProblem 3:\n23\n<story>\n24\n25\nConstants:\nLine 1 provides a general instruction for the task of ex-\ntracting objects and directing GPT-3 to generate them in the\nform of “category: constant1; ...; constantn.” Then, two ex-\namples follow: Lines 6-8 for Problem 1 specified in Lines\n3-4, and Lines 17-20 for Problem 2 specified in Lines 10-\n15. By replacing Line 23 (⟨story⟩) with a new example story\nand invoking GPT-3 with the above prompt, a new list of\ncategories and constants for that story is generated, as with\nthe previous two examples.\nThe above two examples are chosen to cover two cases of\nobject extraction. For the N-Queens problem, the constants\n1, . . . , 8 are not described in the Problem 1 statement (Line\n4) but can be inferred. For the second puzzle, however, all\nconstants in Lines 18-20 are mentioned in the example story\nprovided in Lines 11-15.\nThe second puzzle is also intentionally selected to give an\nexample for GPT-3 so that certain constants (e.g., $225) can\nbe turned into valid integers (e.g., 225) so that arithmetic\ncan be applied correctly later when generating rules later on,\nwhile others should be surrounded by double quotes. We ex-\nperimented with various prompts to instruct GPT-3 to gener-\nate all non-numeric constants in lowercase and replace spe-\ncial characters with underscores. However, GPT-3 was un-\nable to strictly adhere to these instructions and consequently\nmade more errors.\n3.2\nPredicate Generation\nThe next step in the pipeline is to generate predicates p that\ndescribe the relations among the extracted constants. We use\nGPT-3 on the Prompt P below.\nPrompt P:\n1\nGiven a problem and some categorized constants\nof the form \"category: constant_1; constant_2;\n...; constant_n\", generate the minimum number\nof predicates to define the relations among\nthe categories of constants. Each generated\npredicate is of the form \"predicate(X1, X2,\n..., Xn)\" where X1, X2, ..., Xn are different\nvariables and each variable X belongs to one\nof the categories. For each category, there\nmust exist at least one variable of some\npredicate that belongs to this category.\n2\n3\nProblem 1:\n4\n(Lines 4-8 from Prompt C: Omitted)\n5\n6\nPredicates:\n7\n% The categories in Constants include\nindex_of_row and index_of_column. We use\ndifferent variables Ir and Ic to represent\nindex_of_row and index_of_column.\n8\n% We assign a queen at row Ir and column Ic,\nwhere Ir belongs to index_of_row and Ic\nbelongs to index_of_column.\n9\nassign(Ir, Ic)\n10\n11\nProblem 2:\n12\n(Lines 11-20 from Prompt C: Omitted)\n13\n14\nPredicates:\n15\n% The categories in Constants include employee,\nprice, and wood_type. We use different\nvariables E, P, and W to represent employee,\nprice, and wood_type.\n16\n% We match an employee E with price P and wood\ntype W, where E belongs to employee, P belongs\nto price, and W belongs to wood_type.\n17\nmatch(E, P, W)\n18\n19\nProblem 3:\n20\n<story>\n21\n22\nConstants:\n23\n<constants>\n24\n25\nPredicates:\nLine 1 is a general instruction describing the task of pred-\nicate generation, and that the generated predicates should\nfollow the form of “predicate(X1, ..., Xn)” where each Xi\nis a distinct variable that represents a category of constants.\nAgain, the two examples follow.\nLines 3–4 are a copy\nof the first example in Lines 3–8 of Prompt C (where we\nomit Lines 4–8 from Prompt C to reduce the space). Lines\n6–9 continue the first example, where it now generates the\npredicates with variables as arguments following the instruc-\ntion. It also contains two comments (starting with symbol\n%).\nThe first comment in Line 7 recalls the categories of\nconstants and assigns a different variable to each category.\nThe second comment in Line 8 gives the English reading of\nthe predicate and variables, and emphasizes the link between\neach variable and a category of constants. Similarly, Lines\n11–17 present the second example.\nNext, the story and constants are given for the third prob-\nlem and GPT-3 is prompted to generate the predicate for that\nexample, given the general instruction and the preceding two\nexamples.\nGiven the extracted constants c and generated predicates\np, the next step in the pipeline is to generate ASP rules Π,\nconsisting of the Generate part and the Define&Test part.\n3.3\nRule Generation: Generate\nThe Generate part of an ASP program defines all possi-\nble mappings of constants from different categories. This\nis done by choice rules.\nIn this step, an ASP program\nΠgenerate is obtained by calling GPT-3 with Prompt R1.\nPrompt R1:\n1\nGiven some categorized constants in the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\" and some predicates about the\nrelation among different categories of\nconstants, write ASP (Answer Set Programming)\nrules to generate the search space of possible\nrelations.\n2\n3\nConstants:\n4\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n5\nprice: 225; 275; 325.\n6\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n7\n8\nPredicates:\n9\n% The categories include employee, price, and\nwood_type. We use different variables E, P,\nand W to represent employee, price, and\nwood_type.\n10\n% We match an employee E with price P and wood\ntype W, where E belongs to employee, P belongs\nto price, and W belongs to wood_type.\n11\nmatch(E, P, W)\n12\n13\nASP Rules:\n14\n% Define the constants in each category.\n15\nemployee(\"Bonita\"; \"Yvette\"; \"Tabitha\").\n16\nprice(225; 275; 325).\n17\nwood_type(\"ash\"; \"poplar\"; \"sandalwood\").\n18\n19\n% For each employee E, it matches with exactly\n1 price P and 1 wood type W.\n20\n{match(E, P, W): price(P), wood_type(W)}=1 :-\nemployee(E).\n21\n22\nConstants:\n23\n<constants>\n24\n25\nPredicates:\n26\n<predicates>\n27\n28\nASP rules:\nIn the above prompt, ⟨constants⟩and ⟨predicates⟩are to\nbe replaced for a new example. GPT-3 generates facts and\nchoice rules following the last line of the prompt.\nThe task in this step is to write facts and choice rules based\non the generated constants and predicates. Since this step\ndoesn’t require the details of the story, we omit the story\nfrom the prompt to avoid unnecessary noisy information be-\ning included in the prompt. Each example only consists of\nconstants, predicates, and ASP rules to be generated, i.e.,\nfacts and choice rules.\nSimilar to the previous prompts, Line 1 is a general in-\nstruction, Lines 3–20 provide an example, and Lines 22–28\nare for the queried example. The example ASP rules in Lines\n14–20 contain comments (Lines 14 and 19), which will also\nbe generated for the queried example and help to gather se-\nmantic information before generating a rule.\n3.4\nRule Generation: Define and Test\nThe Define&Test part of an ASP program contains con-\nstraints that “weed out” the stable models that do not cor-\nrespond to valid answers. This step takes as input the puzzle\nstory q, constants c, and predicates p: semantically, the ASP\nrules represent the content in story q while, syntactically, the\nASP rules must be formed by the extracted constants c and\ngenerated predicates p. The ASP program Πdefine test is\nobtained by calling GPT-3 with Prompt R2.\nPrompt R2:\n1\nConsider the constraint in the following form\n2\n<C1>; <C2>; ...; <Cm> :- <L1>, <L2>, ..., <Ln>.\n3\nwhich says that if the conjunction \"<L1> and <\nL2> and ... and <Ln>\" is true, then the\ndisjunction of comparisons \"<C1> or <C2> or\n... or <Cm>\" must be true.\n4\n5\nOne can also add a restriction that \"exactly k\nof <C1>, <C2>, ..., <Cm> is true\" by using the\nfollowing form\n6\n{<C1>; <C2>; ...; <Cm>}=k :- <L1>, <L2>, ..., <\nLn>.\n7\n8\nGiven a problem, extract all constraints from\nthe clues in the problem using only the\nprovided constants and predicates.\n9\n10\nProblem 1:\n11\n\"Against the Grain\" offers hand-made wooden\nfurniture at reasonable prices. Each item is\nmade by an in-house employee. Using only the\nclues that follow, match each item to the\nemployee who crafted it, and determine its\nprice and the type of wood used to make it.\nRemember, as with all grid-based logic puzzles\n, no option in any category will ever be used\nmore than once.\n12\n1. Bonita’s piece costs $325.\n13\n2. The item made of poplar costs more than\nYvette’s piece.\n14\n3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n15\n4. The $275 item is either the piece made of\nash or Yvette’s item.\n16\n17\nConstants:\n18\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n19\nprice: 225; 275; 325.\n20\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n21\n22\nPredicates:\n23\n% The categories include employee, price, and\nwood_type. We use different variables E, P,\nand W to represent employee, price, and\nwood_type.\n24\n% We match an employee E with price P and wood\ntype W, where E belongs to employee, P belongs\nto price, and W belongs to wood_type.\n25\nmatch(E, P, W)\n26\n27\nConstraints:\n28\n% No option in any category will ever be used\nmore than once.\n29\n{E1=E2; P1=P2; W1=W2}=0 :- match(E1,P1,W1),\nmatch(E2,P2,W2), (E1,P1,W1)!=(E2,P2,W2).\n30\n31\n% 1. Bonita’s piece costs $325.\n32\nP=325 :- match(E,P,W), E=\"Bonita\".\n33\n34\n% 2. The item made of poplar costs more than\nYvette’s piece.\n35\nP1>P2 :- match(E1,P1,W1), match(E2,P2,W2), W1=\"\npoplar\", E2=\"Yvette\".\n36\n37\n% 3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n38\nP1=P2-50 :- match(E1,P1,W1), match(E2,P2,W2),\nE1=\"Tabitha\", W2=\"sandalwood\".\n39\n40\n% 4. The $275 item is either the piece made of\nash or Yvette’s item.\n41\n{W=\"ash\"; E=\"Yvette\"}=1 :- match(E,P,W), P=275.\n42\n43\n(Problem 2 omitted)\n44\n45\nProblem 3:\n46\n<story>\n47\n48\nConstants:\n49\n<constants>\n50\n51\nPredicates:\n52\n<predicates>\n53\n54\nConstraints:\nIn the above prompt,\n⟨story⟩is a new puzzle,\nand\n⟨constants⟩, ⟨predicates⟩are generated by GPT-3 for that\nstory using Prompt C and Prompt P in Section 3.1 and 3.2.\nLines 1–8 are a general instruction describing the task of\nΠdefine test generation and provides two rule forms for the\ntarget ASP rules. The first rule form\nC1; C2; . . . ; Cm ←L1, L2, . . . , Ln\nsays that “C1 or ... or Cm is true if L1 and ... and Ln are\ntrue.” Here, each Li is a literal and each Ci is a comparison\nin the input language of CLINGO, e.g., A > B, A = B + 3,\netc. The second rule form\n{C1; C2; . . . ; Cm} = k ←L1, L2, . . . , Ln\nadditionally restricts that “exactly k of {C1, . . . , Cm} must\nbe true.” In principle, the first rule form is enough to rep-\nresent various constraints. However, since the second rule\nform is syntactically closer to certain complex sentences re-\nlated to cardinality, e.g., “either ... or ...”, “neither ... nor ...”,\nor “no ... is ...”, etc, we found that GPT-3 works much better\nwhen we also include the second rule form.\n4\nOptional Enhancements to the Pipeline\nSection 3 presented a general pipeline that automatically\nwrites an ASP program for a puzzle in natural language us-\ning LLM. This section explains two optional enhancements\nthat strengthen its robustness.\n4.1\nConstant Formatting\nIn the Constant Extraction step (Section 3.1), GPT-3 may\nextract the names of the objects as they appear in the puz-\nzle story, such as $225, Sue Simpson, and 8:30 AM,\nwhich do not conform to the syntax of the input language of\nanswer set solver CLINGO. Also, GPT-3 applies arithmetic\ncomputations (e.g., L1=L2+3) to constants surrounded by\ndouble quotes (e.g., L2 is constant \"9 inches\") instead\nof constants that are integers (e.g., L2 is constant 9).\nA rule-based post-processing could be applied to turn\nthem into the right syntax, but alternatively, we employ\nGPT-3 to generate syntactically correct forms. We found\nthat this method requires significantly less efforts and is\nmore general because GPT-3 applies the constant formatting\ncorrectly even for unforeseen formats using some “common\nsense,” which is lacking in the rule-based approach. We use\nthe following prompt for this.\nThe Constant Formatting step is done by calling GPT-3\nwith the following prompt, where ⟨constants⟩at the end of\nthe prompt is replaced by the original (extracted) constants c\nobtained by the Constant Extraction step (Section 3.1). The\nGPT-3 response in this step is the updated constants c, serv-\ning as an input to other steps in the pipeline.\n1\nGiven categorized constants of the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\", format the category and constants\nsuch that:\n2\neach category consists of only lowercase\nletters and underscores, and\n3\neach constant is either an integer or a string\nsurrounded by double quotes, e.g., \"United\nStates\".\n4\n5\nThere are two ways below to format constants\nand we must use the same way for all constants\nof the same category.\n6\n1. Turn all constants of the same category into\nintegers with no space or special character.\n7\n2. Add double quotes around all constants of\nthe same category.\n8\nNote that the 1st way has a higher priority,\nmeaning that we must turn all constants of the\nsame category into integers whenever possible\n. For example, twice or second can be turned\ninto 2, September can be turned into 9,\nSeptember 5th can be turned into 5 if all\ndates are in September, but 9:30am can only be\nturned into \"9:30am\" since no integer can\nrepresent 9:30am.\n9\n10\nOriginal constants:\n11\nEmployees: Bonita; Yvette; Tabitha.\n12\nPrices: $225; $275; $325.\n13\nWood types: ash; poplar; sandalwood.\n14\n15\nFormatted constants:\n16\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n17\nprice: 225; 275; 325.\n18\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n19\n20\nOriginal constants:\n21\nmonths: January; April; October; December.\n22\ntimes: 8:30AM; 10:30AM; 2:30PM; 3:30PM.\n23\ndurations: 1 day; 3 days; 11 days; 12 days.\n24\n25\nFormatted constants:\n26\nmonth: 1; 4; 10; 12.\n27\ntime: \"8:30AM\"; \"10:30PM\"; \"2:30PM\"; \"3:30PM\".\n28\nduration: 1; 3; 11; 12.\n29\n30\nOriginal constants:\n31\n⟨constants⟩\n32\n33\nFormatted constants:\n4.2\nSentence Paraphrasing\nSometimes sentences may need to be paraphrased before an\nLLM can correctly generate rules from them. The Sentence\nParaphrasing step provides the opportunity to not only sim-\nplify or formalize the sentences from the original question\nbut also add the hidden information assumed to underlie the\nquestion. For example, the following sentence\n1\nOf the person who won the prize in\nbioengineering and Sue Simpson, one won in\n1976 and the other won in 1968.\nis one clue in the example question in Section 3. The cor-\nrect translation requires an LLM to turn the above sentence\ninto at least 3 ASP rules, which would be hard for the cur-\nrent LLMs (e.g., GPT-3). Instead, we can ask GPT-3 to first\nparaphrase such kind of sentence into simpler ones below.\n1\nThe person who won the prize in bioengineering\nand Sue Simpson are different.\n2\nThe person who won the prize in bioengineering\nwon in 1976 or won in 1968.\n3\nSue Simpson won in 1976 or won in 1968.\nThe Sentence Paraphrasing step is done by calling GPT-\n3 with the following prompt, where ⟨sentences⟩at the end\nof the prompt is replaced by the numbered sentences in the\nqueried puzzle story q, and the GPT-3 response in text is\nused to replace the original sentences in q. This prompt is\ndedicated to the logic puzzles from Puzzle Baron and only\nparaphrases one kind of sentence in the form “of A and B,\none is C and the other is D.”\n1\nCopy a sequence of numbered sentences.\n2\n3\nIf a sentence is of the form \"N. Of A and B,\none is C and the other is D\", replace it with\n3 sentences below.\n4\nN.1 A and B are different.\n5\nN.2 A is C or D.\n6\nN.3 B is C or D.\n7\n8\nFor every sentence, if it is not of the form \"N\n. Of ... and ...\", simply copy it without\nreplacement. An easy way to determine if a\nsentence is not of the above form is to check\nif its first word is not of.\n9\n10\nIn the following example, one sentence is of\nthe above form.\n11\nGiven:\n12\n1. The squad from Grenada ended with 2 silver\nmedals.\n13\n2. Of the team from Oman and the team that won\n10 silver medals, one finished with 2 gold\nmedals and the other finished with 1 gold\nmedal.\n14\nCopy:\n15\n1. The squad from Grenada ended with 2 silver\nmedals.\n16\n2.1 The team from Oman and the team that won 10\nsilver medals are different.\n17\n2.2 The team from Oman finished with 2 gold\nmedals or finished with 1 gold medal.\n18\n2.3 The team that won 10 silver medals finished\nwith 2 gold medals or finished with 1 gold\nmedal.\n19\n20\nIn the following example, no sentence is of the\nabove form.\n21\nGiven:\n22\n1. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n23\n2. The $275 item is either the piece made of\nash or Yvette’s item.\n24\nCopy:\n25\n1. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n26\n2. The $275 item is either the piece made of\nash or Yvette’s item.\n27\n28\nGiven:\n29\n⟨sentences⟩\n30\nCopy:\n5\nExperiments\nWe tested the above pipeline on the logic puzzles dataset\nfrom (Mitra and Baral 2015). Since the constants are pro-\nMethod\ntrain set\ntest set\n(Mitra and Baral 2015)\n–\n71%\nZero-shot GPT-3\n0%\n2%\nFew-shot GPT-3\n4%\n3%\nZero-shot GPT-4\n12%\n21%\nFew-shot GPT-4\n6%\n7%\nGPT-3 Generated ASP Rules\n86%\n81%\nGPT-4 Generated ASP Rules\n92%\n92%\nTable 1: Accuracy on 50 train and 100 test puzzles. GPT-3 refers\nto the model named“text-davinci-003” in the OpenAI API, while\nGPT-4 is the model named “gpt-4.”\nStep\nCount\nGPT-3\nGPT-4\nconstant formatting\n3\n1\nparaphrasing\n2\n4\nconstraint generation (syntax)\n3\n0\nconstraint generation (semantics)\n13\n3\nTable 2: Mistakes on 100 test puzzles at different pipeline steps.\nvided in the dataset as necessary information to solve each\npuzzle, we apply Constant Formatting directly on the given\nconstants to generate constants c.\nThe dataset consists of 50 training examples and 100 test-\ning examples. When designing our prompts, we only consult\nthe training examples and not the testing examples. Table 1\nshows the performance of our approach to zero-shot GPT-\n3/GPT-4, few-shot GPT-3/GPT-4, and a fully-supervised\nlearning system LOGICIA (Mitra and Baral 2015).\n3 In\nthe few-shot setting, we use the first two examples in the\ntraining set as the few-shot examples. GPT-3 with zero-shot\nand few-shot settings didn’t perform well, while zero-shot\nGPT-4 could solve 21% of the test puzzles correctly, which\nis significantly better than GPT-3’s performance. However,\nthis is much lower than our method’s 81%. Interestingly,\nwhile the few-shot setting slightly improves over the zero-\nshot for GPT-3, this is quite different with GPT-4.\nThis\nis likely because GPT-4 with the zero-shot setting was in-\nstructed to solve the puzzles in a step by step manner. How-\never, for the few-shot setting, the examples only include the\nproblem and solution, which may have discouraged GPT-4\nfrom working through the puzzles in steps.\nBesides the fact that the direct execution of the LLMs re-\nsults in low performance, it is hard to understand why they\nfail to solve puzzles; in other words, the results are hard to\ninterpret.\nOn the other hand, in our method, although the LLMs can\nstill produce incorrect ASP programs, the errors are much\nless and it is easy to identify the failures by examining the\ngenerated ASP programs. Table 2 summarizes the number\nof mistakes we encountered on 100 test data at each pipeline\nstep. Below we provide a representative example for each\n3For GPT-3/GPT-4, to avoid randomness, we use a temperature\nof 0 (deterministic) and a top P value of 1 (default setting).\ntype of errors from Table 2 and highlight the mistakes in\nred. 4 The examples are taken from GPT-3, although each\ntype also occurs with GPT-4, except for the syntax error.\n• Failure to convert certain constants into integers dur-\ning constant formatting.\nThe only mistake happens\nwhen, given constants “points: 181 points; 184 points;\n187 points; 190 points”, GPT-3 generated\n1\npoints:\n\"181 points\"; \"184 points\";\n2\n\"187 points\"; \"190 points\".\nwhile it should generate constants in integers below to be\nable to apply arithmetic operations.\n1\npoints: 181; 184; 187; 190.\n• Addition of wrong clues during paraphrasing.\nOne\nsuch error happens when paraphrasing the sentence “2.\nThe book published in 2011 is either Orlando’s project\nor Jan’s project”. GPT-3 generated the following three\nsentences.\n1\n% 2.1 The book published in 2011 is either\nOrlando’s project or Jan’s project.\n2\n% 2.2 Orlando’s project is the book\n3\npublished in 2011 or the 200 page\n4\nproject.\n5\n% 2.3 Jan’s project is the book\n6\npublished in 2011 or the 200 page\n7\nproject.\nThe first sentence is simply a copy of the original sentence\nwhile the last two should not be there since they cannot be\ninferred from the original sentence.\n• Syntax error during constraint generation.\nOne typ-\nical syntax error is an incorrect variable name in a gener-\nated rule. For example, given the sentence “The study on\nthe Liberty starts 2 months after Eunice’s project”, GPT-3\ngenerates the following rule\n1\nM=M1+2 :- match(S,M,Sh), match(S1,M1,Sh1),\nS=\"Eunice\", Sh=\"Liberty\".\nwhile the variable S in the second line should have been\nS1.\n• Semantic error during constraint generation.\nOne\ntypical semantic error is caused by a wrong equation. For\nexample, given the sentence “the $35,000 structure is 15\nsq ft smaller than the $29,000 home”, GPT-3 generated\n1\nS1=S2+15 :- match(C1,S1,P1), match(C2,S2,P2)\n, P1=35000, P2=29000.\nwhile the equation in the head should have been\nS1=S2-15.\nWhile our pipeline doesn’t achieve 100% accuracy on\ngenerated ASP programs, most failed puzzles only have one\n4We have provided a breakdown of the errors we encoun-\ntered on Github at https://github.com/azreasoners/gpt-asp-rules/\ntree/main/error analysis.\nmistake and such a mistake is easy to correct. This means\nthat our pipeline could serve as a good suggestion tool to\nprepare draft ASP programs for users. For example, com-\npared to designing all the ASP programs for 50+100 puzzles\nmanually, it would save a significant amount of time to only\ncheck the correctness of the automatically generated rules\nfor the programs that don’t have a single stable model.\n6\nMore Examples\nPrevious approaches that automate logic puzzle solving ei-\nther only predict constants and relations (Mitra and Baral\n2015) or treat rule generation as a classification problem on\na small set of rule templates (Jabrayilzade and Tekir 2020).\nIn comparison, our method is generative, where rules are\ngenerated in an open-ended manner under the guidance of a\nfew examples.\nWhile it’s hard to apply the previous methods to other\ndomains without substantial changes, applying our pipeline\nto new domains requires only minor adjustments on the\nprompts. To apply our pipeline to other domains, we make a\nslight adjustment by turning the last sentence in Line 11 of\nPrompt R2 into a numbered clue “0. No option in any cate-\ngory will ever be used more than once.”, since it was specific\nto grid logic puzzles.\nIn the following part of this section, we show how our\npipeline can be further applied to generate ASP programs\nfor Sudoku and the Jobs Puzzle.\n6.1\nSudoku\nIf we describe Sudoku problem with the following story q\n1\nThe Sudoku problem is a classic computer\nscience problem that involves filling a 9x9\ngrid with digits from 1 to 9.\n2\n1. The same number cannot appear twice in the\nsame row.\n3\n2. The same number cannot appear twice in the\nsame column.\n4\n3. The same number cannot appear twice in the 3\nx3 box. (You can use division / to check if 2\ncells are in the same box.)\nour pipeline generates the following ASP program Π.\n1\n% Define the constants in each category.\n2\nindex_of_row(1; 2; 3; 4; 5; 6; 7; 8; 9).\n3\nindex_of_column(1; 2; 3; 4; 5; 6; 7; 8; 9).\n4\nnumber(1; 2; 3; 4; 5; 6; 7; 8; 9).\n5\n% For each row Ir and column Ic, it assigns\nexactly 1 number N.\n6\n{assign(Ir, Ic, N): number(N)}=1 :-\nindex_of_row(Ir), index_of_column(Ic).\n7\n8\n% 1. The same number cannot appear twice in the\nsame row.\n9\n{N1=N2}=0 :- assign(Ir,Ic1,N1), assign(Ir,Ic2,\nN2), (Ic1,N1)!=(Ic2,N2).\n10\n11\n% 2. The same number cannot appear twice in the\nsame column.\n12\n{N1=N2}=0 :- assign(Ir1,Ic,N1), assign(Ir2,Ic,\nN2), (Ir1,N1)!=(Ir2,N2).\n13\n14\n% 3. The same number cannot appear twice in the\n3x3 box.\n15\n{N1=N2}=0 :- assign(Ir1,Ic1,N1), assign(Ir2,Ic2\n,N2), (Ir1/3,Ic1/3)=(Ir2/3,Ic2/3), (Ir1,\nIc1,N1)!=(Ir2,Ic2,N2).\nThis ASP program Π is almost correct except that the red\npart in Line 16 of Π should be\n1\n((Ir1-1)/3, (Ic1-1)/3) = ((Ir2-1)/3, (Ic2-1)/3)\nsince the row and column indices start from 1. This for-\nmula seems too difficult for GPT-3 to notice and generate\nunless some examples are provided . On the other hand, if\nwe slightly adjust Lines 7–8 of Prompt C (Section 3.1) to\nmake the indices start from 0, then the generated ASP pro-\ngram Π becomes correct as Lines 2–3 of Π are changed to\nthe following facts.\n1\nindex_of_row(0; 1; 2; 3; 4; 5; 6; 7; 8).\n2\nindex_of_column(0; 1; 2; 3; 4; 5; 6; 7; 8).\nGPT-4 also fails to generate the last rule correctly, al-\nthough it makes a different mistake.\n6.2\nJobs Puzzle\nThe Jobs Puzzle studied in (Schwitter 2013) asks one to as-\nsign 8 different jobs to 4 people while satisfying the given\nconstraints. The full puzzle q is shown below.\n1\n1. There are four people: Roberta, Thelma,\nSteve, and Pete.\n2\n2. Among them, they hold eight different jobs.\n3\n3. Each holds exactly two jobs.\n4\n4. The jobs are: chef, guard, nurse, telephone\noperator, police officer (gender not implied),\nteacher, actor, and boxer.\n5\n5. The job of nurse is held by a male.\n6\n6. The husband of the chef is the telephone\noperator.\n7\n7. Roberta is not a boxer.\n8\n8. Pete has no education past the ninth grade.\n9\n9. Roberta, the chef, and the police officer\nwent golfing together.\n10\nQuestion: Who holds which jobs?\nThis puzzle was considered a challenge for logical express-\nibility and automated reasoning (Shapiro 2011).\nTo apply our method to the Jobs Puzzle, some paraphras-\ning was needed before the Define&Test part of rule genera-\ntion. We manually paraphrased the above puzzle to the fol-\nlowing\n1\nThere are four people: Roberta, Thelma, Steve,\nand Pete. Among them, they hold eight\ndifferent jobs. Each holds exactly two jobs.\nThe jobs are: chef, guard, nurse, telephone\noperator, police officer (gender not implied),\nteacher, actor, and boxer.\n2\n5. The job of nurse is held by a male.\n3\n6. The husband of the chef is the telephone\noperator, which means the chef is a female and\nthe telephone operator is a male.\n4\n7. Roberta is not a boxer.\n5\n8. Pete has no education past the ninth grade.\n(Three jobs teacher, nurse, and police officer\nrequire higher education.)\n6\n9. Roberta, the chef, and the police officer\nwent golfing together, meaning they are\ndifferent people.\n7\n10.1 The same job cannot be assigned to\ndifferent people.\n8\n10.2 Roberta is a female.\n9\n10.3 Thelma is a female.\n10\n10.4 Steve is a male.\n11\n10.5 Pete is a male.\n12\n10.6 The actor is a male.\n13\nQuestion: Who holds which jobs?\nby turning clues 1–4 as background story, clarifying clues 6,\n8, and 9, and adding a few hidden clues numbered 10.X at\nthe end.\nAs for the prompts, we only need to update Line 1 of\nPrompt R1 to the following to allow for {...}=k in a rule.\n1\nGiven a problem as the background information,\nturn the provided categorized constants and\npredicates into rules.\n2\n3\nTasks:\n4\n1. Write a rule of the form \"category(\nconstant_1; constant_2; ...; constant_n)\" to\nrepresent each row of categorized constants in\nthe form \"category: constant_1; constant_2;\n...; constant_n\".\n5\n2. Write a rule of the form \"{predicate(X_1,\nX_2, ..., X_n): category_2(X_2), ...,\ncategory_n(X_n)}=k :- category_1(X_1)\" for\neach predicate of the form \"predicate(X_1, X_2\n, ..., X_n)\".\nFinally, GPT-3 generates the following ASP program:\n1\n% Task 1. Define the constants in each category\n.\n2\nperson(\"Roberta\"; \"Thelma\"; \"Steve\"; \"Pete\").\n3\njob(\"chef\"; \"guard\"; \"nurse\"; \"telephone\noperator\"; \"police officer\"; \"teacher\"; \"actor\n\"; \"boxer\").\n4\ngender(\"male\"; \"female\").\n5\n% Task 2. For each person P, there is exactly 2\njobs J and gender G.\n6\n{assign(P, J, G): job(J), gender(G)}=2 :-\nperson(P).\n7\n% Done tasks 1 and 2.\n8\n% 5. The job of nurse is held by a male.\n9\nG=\"male\" :- assign(P,J,G), J=\"nurse\".\n10\n% 6. The husband of the chef is the telephone\noperator, which means the chef is a female and\nthe telephone operator is a male.\n11\nG1=\"female\" :- assign(P1,J1,G1), assign(P2,J2,\nG2), J1=\"chef\", J2=\"telephone operator\".\n12\nG2=\"male\" :- assign(P1,J1,G1), assign(P2,J2,G2)\n, J1=\"chef\", J2=\"telephone operator\".\n13\n% 7. Roberta is not a boxer.\n14\nJ!=\"boxer\" :- assign(P,J,G), P=\"Roberta\".\n15\n% 8. Pete has no education past the ninth grade\n. (Three jobs teacher, nurse, and police\nofficer require higher education.)\n16\n{J=\"teacher\"; J=\"nurse\"; J=\"police officer\"}=0\n:- assign(P,J,G), P=\"Pete\".\n17\n% 9. Roberta, the chef, and the police officer\nwent golfing together, meaning they are\ndifferent people.\n18\nP1!=P2 :- assign(P1,J1,G1), assign(P2,J2,G2),\nassign(P3,J3,G3), P1=\"Roberta\", J2=\"chef\", J3\n=\"police officer\".\n19\nP2!=P3 :- assign(P1,J1,G1), assign(P2,J2,G2),\nassign(P3,J3,G3), P1=\"Roberta\", J2=\"chef\", J3\n=\"police officer\".\n20\nP1!=P3 :- assign(P1,J1,G1), assign(P2,J2,G2),\nassign(P3,J3,G3), P1=\"Roberta\", J2=\"chef\", J3\n=\"police officer\".\n21\n% 10.1 The same job cannot be assigned to\ndifferent people.\n22\n{P1=P2}=0 :- assign(P1,J,G1),\n23\nassign(P2,J,G2), (P1,G1)!=(P2,G2).\n24\n% 10.2 Roberta is a female.\n25\nG=\"female\" :- assign(P,J,G), P=\"Roberta\".\n26\n% 10.3 Thelma is a female.\n27\nG=\"female\" :- assign(P,J,G), P=\"Thelma\".\n28\n% 10.4 Steve is a male.\n29\nG=\"male\" :- assign(P,J,G), P=\"Steve\".\n30\n% 10.5 Pete is a male.\n31\nG=\"male\" :- assign(P,J,G), P=\"Pete\".\n32\n% 10.6 The actor is a male.\n33\nG=\"male\" :- assign(P,J,G), J=\"actor\".\nwhich is almost correct with a single mistake in translating\nclue 10.1. If we just replace this constraint (in red) with\n1\nP1=P2 :- assign(P1,J,G1), assign(P2,J,G2).\nthe corrected ASP program has exactly one stable model,\nwhich is the correct solution to the Jobs Puzzle.\nSimilarly, GPT-4 also failed to generate a completely cor-\nrect ASP program. It also couldn’t generate a correct rule for\nconstraint 10.1, and furthermore failed to produce the gen-\nder category in constant extraction step Prompt C), missing\n“gender:\n\"male\"; \"female\".”\n7\nConclusion\nLLMs are a relatively recent technology that have shown to\nbe disruptive. Despite their wide range of applications, their\nresponses are not always reliable and cannot be trusted.\nAutomatic rule generation is a difficult problem. How-\never, by using LLMs as a front-end to answer set program-\nming, we can utilize their linguistic abilities to translate nat-\nural language descriptions into the declarative language of\nanswer set programs. Unlike previous methods that use al-\ngorithmic or machine learning techniques, we find that a pre-\ntrained large language model with a good prompt can gener-\nate reasonably accurate answer set programs. We present a\npipeline with general steps that systematically build an ASP\nprogram in a natural way. This method not only leads to\nhigher accuracy but also makes the results interpretable.\nWe expect this type of work to expand the application of\nKR methods that may appear unfamiliar to non-experts. We\nalso anticipate that this pipeline will serve as a suggestion\ntool to help users prepare valid constants, useful predicates,\nor draft ASP programs.\nAcknowledgements\nWe are grateful to the anonymous referees for their useful\ncomments. This work was partially supported by the Na-\ntional Science Foundation under Grant IIS-2006747.\nReferences\nBaral, C., and Dzifcak, J. 2012. Solving puzzles described\nin english by automated translation to answer set program-\nming and learning how to do that translation. In Proceedings\nof the Thirteenth International Conference on Principles of\nKnowledge Representation and Reasoning, 573–577.\nBrewka, G.; Niemel¨a, I.; and Truszczynski, M. 2011. An-\nswer set programming at a glance. Communications of the\nACM 54(12):92–103.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems 33:1877–\n1901.\nCreswell, A.; Shanahan, M.; and Higgins, I.\n2022.\nSelection-inference:\nExploiting large language models\nfor\ninterpretable\nlogical\nreasoning.\narXiv\npreprint\narXiv:2205.09712.\nJabrayilzade, E., and Tekir, S. 2020. LGPSolver - solving\nlogic grid puzzles automatically. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020, 1118–\n1123.\nLev, I.; MacCartney, B.; Manning, C. D.; and Levy, R. 2004.\nSolving logic puzzles: From robust processing to precise se-\nmantics. In Proceedings of the 2nd Workshop on Text Mean-\ning and Interpretation, 9–16.\nLifschitz, V. 2008. What is answer set programming? In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, 1594–1597. MIT Press.\nMilicevic, A.; Near, J. P.; and Singh, R. 2012. Puzzler:\nAn automated logic puzzle solver. Technical report, Mas-\nsachusetts Institute of Technology (MIT).\nMitra, A., and Baral, C. 2015. Learning to automatically\nsolve logic grid puzzles. In Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 1023–1033.\nNordstrom, R. 2017. LogicSolver - Solving logic grid puz-\nzles with part-of-speech tagging and first-order logic. Tech-\nnical report, University of Colorado, Colorado Springs.\nNye, M.; Tessler, M.; Tenenbaum, J.; and Lake, B. M.\n2021. Improving coherence and consistency in neural se-\nquence models with dual-system, neuro-symbolic reason-\ning. Advances in Neural Information Processing Systems\n34:25192–25204.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoff-\nmann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.;\nYoung, S.; et al. 2021. Scaling language models: Meth-\nods, analysis & insights from training gopher. arXiv preprint\narXiv:2112.11446.\nSchwitter, R. 2010. Controlled natural languages for knowl-\nedge representation. In Coling 2010: Posters, 1113–1121.\nSchwitter, R. 2013. The jobs puzzle: Taking on the chal-\nlenge via controlled natural language processing.\nTheory\nand Practice of Logic Programming 13(4-5):487–501.\nShapiro, S. C. 2011. The jobs puzzle: A challenge for logi-\ncal expressibility and automated reasoning. In AAAI spring\nsymposium: logical formalizations of commonsense reason-\ning.\nValmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambham-\npati, S.\n2022.\nLarge language models still can’t plan\n(a benchmark for LLMs on planning and reasoning about\nchange). In NeurIPS 2022 Foundation Models for Decision\nMaking Workshop.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; brian ichter;\nXia, F.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2022. Chain of\nthought prompting elicits reasoning in large language mod-\nels. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho, K.,\neds., Advances in Neural Information Processing Systems.\nZhou, D.; Sch¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Bousquet, O.; Le, Q.; and Chi, E. 2022.\nLeast-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625.\nA\nPrompts in the Pipeline\nIn this section, we list all prompts used in our pipeline.\nPrompt C:\n1\nGiven a problem, extract all different\nconstants and their categories in the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\". Here, the format of each constant\nis turned into either an integer or a string\nsurrounded by double quotes, e.g., \"some name\n\".\n2\n3\nProblem 1:\n4\nConsider N-Queens Puzzle on a chessboard of\nsize 8x8. The goal is to assign 8 queens on\nthe chessboard so that no two queens can share\nthe same row, column, or diagonal.\n5\n6\nConstants:\n7\nindex_of_row: 1; 2; 3; 4; 5; 6; 7; 8.\n8\nindex_of_column: 1; 2; 3; 4; 5; 6; 7; 8.\n9\n10\nProblem 2:\n11\n\"Against the Grain\" offers hand-made wooden\nfurniture at reasonable prices. Each item is\nmade by an in-house employee. Using only the\nclues that follow, match each item to the\nemployee who crafted it, and determine its\nprice and the type of wood used to make it.\nRemember, as with all grid-based logic puzzles\n, no option in any category will ever be used\nmore than once.\n12\n1. Bonita’s piece costs $325.\n13\n2. The item made of poplar costs more than\nYvette’s piece.\n14\n3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n15\n4. The $275 item is either the piece made of\nash or Yvette’s item.\n16\n17\nConstants:\n18\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n19\nprice: 225; 275; 325.\n20\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n21\n22\nProblem 3:\n23\n<question: query>\n24\n25\nConstants:\nPrompt P:\n1\nGiven a problem and some categorized constants\nof the form \"category: constant_1; constant_2;\n...; constant_n\", generate the minimum number\nof predicates to define the relations among\nthe categories of constants. Each generated\npredicate is of the form \"predicate(X1, X2,\n..., Xn)\" where X1, X2, ..., Xn are different\nvariables and each variable X belongs to one\nof the categories. For each category, there\nmust exist at least one variable of some\npredicate that belongs to this category.\n2\n3\nProblem 1:\n4\nConsider N-Queens Puzzle on a chessboard of\nsize 8x8. The goal is to assign 8 queens on\nthe chessboard so that no two queens can share\nthe same row, column, or diagonal.\n5\n6\nConstants:\n7\nindex_of_row: 1; 2; 3; 4; 5; 6; 7; 8.\n8\nindex_of_column: 1; 2; 3; 4; 5; 6; 7; 8.\n9\n10\nPredicates:\n11\n% The categories in Constants include\nindex_of_row and index_of_column. We use\ndifferent variables Ir and Ic to represent\nindex_of_row and index_of_column.\n12\n% We assign a queen at row Ir and column Ic,\nwhere Ir belongs to index_of_row and Ic\nbelongs to index_of_column.\n13\nassign(Ir, Ic)\n14\n15\nProblem 2:\n16\n\"Against the Grain\" offers hand-made wooden\nfurniture at reasonable prices. Each item is\nmade by an in-house employee. Using only the\nclues that follow, match each item to the\nemployee who crafted it, and determine its\nprice and the type of wood used to make it.\nRemember, as with all grid-based logic puzzles\n, no option in any category will ever be used\nmore than once.\n17\n1. Bonita’s piece costs $325.\n18\n2. The item made of poplar costs more than\nYvette’s piece.\n19\n3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n20\n4. The $275 item is either the piece made of\nash or Yvette’s item.\n21\n22\nConstants:\n23\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n24\nprice: 225; 275; 325.\n25\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n26\n27\nPredicates:\n28\n% The categories in Constants include employee,\nprice, and wood_type. We use different\nvariables E, P, and W to represent employee,\nprice, and wood_type.\n29\n% We match an employee E with price P and wood\ntype W, where E belongs to employee, P belongs\nto price, and W belongs to wood_type.\n30\nmatch(E, P, W)\n31\n32\nProblem 3:\n33\n<question: query>\n34\n35\n<constants: query>\n36\n37\nPredicates:\nPrompt R1:\n1\nGiven some categorized constants in the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\" and some predicates about the\nrelation among different categories of\nconstants, write ASP (Answer Set Programming)\nrules to generate the search space of possible\nrelations.\n2\n3\nConstants:\n4\nnumber: 1; 2; 3; 4; 5; 6; 7; 8.\n5\n6\nPredicates:\n7\n% The categories include number. Note that we\nmust use different variables in each predicate\n.\n8\n% We assign a queen at row N1 and column N2,\nwhere N1 belongs to number and N2 belongs to\nnumber.\n9\nassign(N1, N2)\n10\n11\nASP Rules:\n12\n% Define the constants in each category.\n13\nnumber(1; 2; 3; 4; 5; 6; 7; 8).\n14\n% For each row N1, there is exactly 1 queen\nassigned at some column N2.\n15\n{assign(N1, N2): number(N2)}=1 :- number(N1).\n16\n17\nConstants:\n18\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n19\nprice: 225; 275; 325.\n20\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n21\n22\nPredicates:\n23\n% The categories include employee, price, and\nwood_type. We use different variables E, P,\nand W to represent employee, price, and\nwood_type.\n24\n% We match an employee E with price P and wood\ntype W, where E belongs to employee, P belongs\nto price, and W belongs to wood_type.\n25\nmatch(E, P, W)\n26\n27\nASP Rules:\n28\n% Define the constants in each category.\n29\nemployee(\"Bonita\"; \"Yvette\"; \"Tabitha\").\n30\nprice(225; 275; 325).\n31\nwood_type(\"ash\"; \"poplar\"; \"sandalwood\").\n32\n% For each employee E, it matches with exactly\n1 price P and 1 wood type W.\n33\n{match(E, P, W): price(P), wood_type(W)}=1 :-\nemployee(E).\n34\n35\n<constants: query>\n36\n37\n<predicates: query>\n38\n39\nASP rules:\nPrompt R2:\n1\nConsider the constraint in the following form\n2\n<C1>; <C2>; ...; <Cm> :- <L1>, <L2>, ..., <Ln>.\n3\nwhich says that if the conjunction \"<L1> and <\nL2> and ... and <Ln>\" is true, then the\ndisjunction of comparisons \"<C1> or <C2> or\n... or <Cm>\" must be true.\n4\n5\nOne can also add a restriction that \"exactly k\nof <C1>, <C2>, ..., <Cm> is true\" by using the\nfollowing form\n6\n{<C1>; <C2>; ...; <Cm>}=k :- <L1>, <L2>, ..., <\nLn>.\n7\n8\nGiven a problem, extract all constraints from\nthe clues in the problem using only the\nprovided constants and predicates.\n9\n10\nProblem 1:\n11\n\"Against the Grain\" offers hand-made wooden\nfurniture at reasonable prices. Each item is\nmade by an in-house employee. Using only the\nclues that follow, match each item to the\nemployee who crafted it, and determine its\nprice and the type of wood used to make it.\nRemember, as with all grid-based logic puzzles\n, no option in any category will ever be used\nmore than once.\n12\n1. Bonita’s piece costs $325.\n13\n2. The item made of poplar costs more than\nYvette’s piece.\n14\n3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n15\n4. The $275 item is either the piece made of\nash or Yvette’s item.\n16\n17\nConstants:\n18\nemployee: \"Bonita\"; \"Yvette\"; \"Tabitha\".\n19\nprice: 225; 275; 325.\n20\nwood_type: \"ash\"; \"poplar\"; \"sandalwood\".\n21\n22\nPredicates:\n23\n% The categories include employee, price, and\nwood_type. We use different variables E, P,\nand W to represent employee, price, and\nwood_type.\n24\n% We match an employee E with price P and wood\ntype W, where E belongs to employee, P belongs\nto price, and W belongs to wood_type.\n25\nmatch(E, P, W)\n26\n27\nConstraints:\n28\n% No option in any category will ever be used\nmore than once.\n29\n{E1=E2; P1=P2; W1=W2}=0 :- match(E1,P1,W1),\nmatch(E2,P2,W2), (E1,P1,W1)!=(E2,P2,W2).\n30\n31\n% 1. Bonita’s piece costs $325.\n32\nP=325 :- match(E,P,W), E=\"Bonita\".\n33\n34\n% 2. The item made of poplar costs more than\nYvette’s piece.\n35\nP1>P2 :- match(E1,P1,W1), match(E2,P2,W2), W1=\"\npoplar\", E2=\"Yvette\".\n36\n37\n% 3. Tabitha’s item costs 50 dollars less than\nthe piece made of sandalwood.\n38\nP1=P2-50 :- match(E1,P1,W1), match(E2,P2,W2),\nE1=\"Tabitha\", W2=\"sandalwood\".\n39\n40\n% 4. The $275 item is either the piece made of\nash or Yvette’s item.\n41\n{W=\"ash\"; E=\"Yvette\"}=1 :- match(E,P,W), P=275.\n42\n43\nProblem 2:\n44\nThe Winter Olympics have just wrapped up in\nNorway. Using only the clues that follow,\ndetermine the number of gold, silver and\nbronze medals won by each country. Remember,\nas with all grid-based logic puzzles, no\noption in any category will ever be used more\nthan once.\n45\n1. The four teams are the team from Bolivia,\nthe team that won 3 gold medals, the team that\nwon 6 silver medals, and the team from\nArgentina.\n46\n2. The team from Oman and the team that won 10\nsilver medals are different.\n47\n3. The team from Oman finished with 2 gold\nmedals or finished with 1 gold medal.\n48\n5. The squad that won 3 gold medals, the squad\nthat won 6 silver medals and the squad from\nBolivia were all different teams.\n49\n6. Neither the team from Argentina nor the\nsquad that won 2 silver medals is the squad\nthat won 4 gold medals.\n50\n8. The squad that won 2 gold medals is either\nthe squad that won 6 silver medals or the team\nfrom Grenada.\n51\n52\nConstants:\n53\ncountry: \"Argentina\"; \"Bolivia\"; \"Grenada\"; \"\nOman\".\n54\nsilver_medals: 2; 6; 10; 11.\n55\ngold_medals: 1; 2; 3; 4.\n56\n57\nPredicates:\n58\n% The categories include country, silver_medals\n, and gold_medals. We use different variables\nC, S, and G to represent country,\nsilver_medals, and gold_medals.\n59\n% We assign a country C with silver medals S\nand gold medals G, where C belongs to country,\nS belongs to silver_medals, and G belongs to\ngold_medals.\n60\nassign(C, S, G)\n61\n62\nConstraints:\n63\n% No option in any category will ever be used\nmore than once.\n64\n{C1=C2; S1=S2; G1=G2}=0 :- assign(C1,S1,G1),\nassign(C2,S2,G2), (C1,S1,G1)!=(C2,S2,G2).\n65\n66\n% 1. The four teams are the team from Bolivia,\nthe team that won 3 gold medals, the team that\nwon 6 silver medals, and the team from\nArgentina.\n67\n{C=\"Bolivia\"; G=3; S=6; C=\"Argentina\"}=1 :-\nassign(C,S,G).\n68\n69\n% 2. The team from Oman and the team that won\n10 silver medals are different.\n70\nC1!=C2 :- assign(C1,S1,G1), assign(C2,S2,G2),\nC1=\"Oman\", S2=10.\n71\n72\n% 3. The team from Oman finished with 2 gold\nmedals or finished with 1 gold medal.\n73\n{G=2; G=1}=1 :- assign(C,S,G), C=\"Oman\".\n74\n75\n% 5. The squad that won 3 gold medals, the\nsquad that won 6 silver medals and the squad\nfrom Bolivia were all different teams.\n76\nC1!=C2 :- assign(C1,S1,G1), assign(C2,S2,G2),\nassign(C3,S3,G3), G1=3, S2=6, C3=\"Bolivia\".\n77\nC2!=C3 :- assign(C1,S1,G1), assign(C2,S2,G2),\nassign(C3,S3,G3), G1=3, S2=6, C3=\"Bolivia\".\n78\nC1!=C3 :- assign(C1,S1,G1), assign(C2,S2,G2),\nassign(C3,S3,G3), G1=3, S2=6, C3=\"Bolivia\".\n79\n80\n% 6. Neither the team from Argentina nor the\nsquad that won 2 silver medals is the squad\nthat won 4 gold medals.\n81\n{C=\"Argentina\"; S=2}=0 :- assign(C,S,G), G=4.\n82\nC1!=C2 :- assign(C1,S1,G1), assign(C2,S2,G2),\nC1=\"Argentina\", S2=2.\n83\n84\n% 8. The squad that won 2 gold medals is either\nthe squad that won 6 silver medals or the\nteam from Grenada.\n85\n{S=6; C=\"Grenada\"}=1 :- assign(C,S,G), G=2.\n86\n87\nProblem 3:\n88\n<question: query>\n89\n90\n<constants: query>\n91\n92\n<predicates: query>\n93\n94\nConstraints:\nIn Prompt R2, since the maximum number of tokens (in-\ncluding the tokens in prompt and response) allowed by GPT-\n3 is 4096, we deleted 2 clues (numbered 4 and 7) in the ques-\ntion description of the second logic puzzle (lines 44-50) and\ntheir corresponding ASP rules (lines 63-85).\nB\nA Few Example Answer Set Programs\nGenerated by GPT-3\nB.1\nPuzzle 1 in test set\nGiven the question q below,\n1\nThe local foodie club met at Chez Martin last\nnight for their monthly meal. Match each\nperson to their choice of wine and entree, and\ndetermine how much each owed at the end of\nthe night. Remember, as with all grid-based\nlogic puzzles, no option in any category will\never be used more than once.\n2\n1. The person who had the port paid 1 dollar\nmore than Kurt.\n3\n2. Of the person who paid $25 and the person\nwho paid $24, one was Priscilla and the other\nhad the shiraz.\n4\n3. Of the person who paid $27 and Priscilla,\none had the chianti and the other had the port\n.\n5\n4. Isabel paid $25.\nour pipeline generates the following ASP program\n1\n% Define the constants in each category.\n2\nwine(\"chianti\"; \"port\"; \"riesling\"; \"shiraz\").\n3\nprice(24; 25; 26; 27).\n4\nname(\"Isabel\"; \"Kurt\"; \"Priscilla\"; \"Robin\").\n5\n% For each person N, it matches with exactly 1\nwine W and 1 price P.\n6\n{match(W, P, N): wine(W), price(P)}=1 :- name(N\n).\n7\n8\n% No option in any category will ever be used\nmore than once.\n9\n{W1=W2; P1=P2; N1=N2}=0 :- match(W1,P1,N1),\nmatch(W2,P2,N2), (W1,P1,N1)!=(W2,P2,N2).\n10\n11\n% 1. The person who had the port paid 1 dollar\nmore than Kurt.\n12\nP1=P2+1 :- match(W1,P1,N1), match(W2,P2,N2), W1\n=\"port\", N2=\"Kurt\".\n13\n14\n% 2.1 The person who paid $25 and the person\nwho paid $24 are different.\n15\nN1!=N2 :- match(W1,P1,N1), match(W2,P2,N2), P1\n=25, P2=24.\n16\n17\n% 2.2 The person who paid $25 was Priscilla or\nhad the shiraz.\n18\n{N=\"Priscilla\"; W=\"shiraz\"}=1 :- match(W,P,N),\nP=25.\n19\n20\n% 2.3 The person who paid $24 was Priscilla or\nhad the shiraz.\n21\n{N=\"Priscilla\"; W=\"shiraz\"}=1 :- match(W,P,N),\nP=24.\n22\n23\n% 3.1 The person who paid $27 and Priscilla are\ndifferent.\n24\nN1!=N2 :- match(W1,P1,N1), match(W2,P2,N2), P1\n=27, N2=\"Priscilla\".\n25\n26\n% 3.2 The person who paid $27 had the chianti\nor had the port.\n27\n{W=\"chianti\"; W=\"port\"}=1 :- match(W,P,N), P\n=27.\n28\n29\n% 3.3 Priscilla had the chianti or had the port\n.\n30\n{W=\"chianti\"; W=\"port\"}=1 :- match(W,P,N), N=\"\nPriscilla\".\n31\n32\n% 4. Isabel paid $25.\n33\nP=25 :- match(W,P,N), N=\"Isabel\".\nwhich has exactly 1 stable model and this stable model\nmatches with the solution.\n1\nmatch(\"chianti\",24,\"Priscilla\")\n2\nmatch(\"shiraz\",25,\"Isabel\")\n3\nmatch(\"riesling\",26,\"Kurt\")\n4\nmatch(\"port\",27,\"Robin\")\nB.2\nPuzzle 2 in test set\nGiven the question q below,\n1\nA group of friends has decided to try several\ndifferent weight-loss diets and exercises to\nsee who amongst them can lose the most weight\nin 3 months. Using only the clues below, match\neach person to their total number of pounds\nlost, and determine the diet type they used\nand their preferred type of exercise. Remember\n, as with all grid-based logic puzzles, no\noption in any category will ever be used more\nthan once.\n2\n1. Celia used the gluten-free diet.\n3\n2. The friend who lost 3 pounds used the low-\nfat diet.\n4\n3. Mandy lost 2 fewer pounds than Raymond.\n5\n4. Mandy lost 4 more pounds than Tom.\n6\n5. The dieter who used the vegan diet is either\nMandy or the friend who lost 3 pounds.\nour pipeline generates the following ASP program\n1\n% Define the constants in each category.\n2\npounds_lost(3; 5; 7; 9).\n3\ndiet(\"dairy-free\"; \"gluten-free\"; \"low-fat\"; \"\nvegan\").\n4\nname(\"Celia\"; \"Mandy\"; \"Raymond\"; \"Tom\").\n5\n% For each name N, it matches with exactly 1\npounds lost Pl and 1 diet D.\n6\n{match(N, Pl, D): pounds_lost(Pl), diet(D)}=1\n:- name(N).\n7\n8\n% No option in any category will ever be used\nmore than once.\n9\n{N1=N2; Pl1=Pl2; D1=D2}=0 :- match(N1,Pl1,D1),\nmatch(N2,Pl2,D2), (N1,Pl1,D1)!=(N2,Pl2,D2).\n10\n11\n% 1. Celia used the gluten-free diet.\n12\nD=\"gluten-free\" :- match(N,Pl,D), N=\"Celia\".\n13\n14\n% 2. The friend who lost 3 pounds used the low-\nfat diet.\n15\nPl=3 :- match(N,Pl,D), D=\"low-fat\".\n16\n17\n% 3. Mandy lost 2 fewer pounds than Raymond.\n18\nPl1=Pl2-2 :- match(N1,Pl1,D1), match(N2,Pl2,D2)\n, N1=\"Mandy\", N2=\"Raymond\".\n19\n20\n% 4. Mandy lost 4 more pounds than Tom.\n21\nPl1=Pl2+4 :- match(N1,Pl1,D1), match(N2,Pl2,D2)\n, N1=\"Mandy\", N2=\"Tom\".\n22\n23\n% 5. The dieter who used the vegan diet is\neither Mandy or the friend who lost 3 pounds.\n24\n{N=\"Mandy\"; Pl=3}=1 :- match(N,Pl,D), D=\"vegan\n\".\nwhich has exactly 1 stable model and this stable model\nmatches with the solution.\n1\nmatch(\"Tom\",3,\"low-fat\")\n2\nmatch(\"Celia\",5,\"gluten-free\")\n3\nmatch(\"Mandy\",7,\"vegan\")\n4\nmatch(\"Raymond\",9,\"dairy-free\")\nB.3\nVariants of Sudoku\nContinuing the process in Section 6, we can generate the\nASP program for variants of Sudoku by adding 1 or 2 more\nclues to the puzzle description q. Below are the newly gen-\nerated constraints for the added clues in each variant.\nAnti-Knight Sudoku\n1\n% 4. The same number cannot appear twice in a\nknight move.\n2\n{N1=N2}=0 :- assign(Ir1,Ic1,N1), assign(Ir2,Ic2\n,N2), |Ir1-Ir2|+|Ic1-Ic2|=3, (Ir1,Ic1,N1)!=(\nIr2,Ic2,N2).\nSudoku-X\n1\n% 4. The same number cannot appear twice among\nthe cells whose row index is equal to column\nindex.\n2\n{N1=N2}=0 :- assign(Ir1,Ic1,N1), assign(Ir2,Ic2\n,N2), Ir1=Ic1, Ir2=Ic2, (Ir1,Ic1,N1)!=(Ir2,Ic2\n,N2).\n3\n4\n% 5. The same number cannot appear twice among\nthe cells whose row and column indices sum up\nto 8.\n5\n{N1=N2}=0 :- assign(Ir1,Ic1,N1), assign(Ir2,Ic2\n,N2), Ir1+Ic1=8, Ir2+Ic2=8, (Ir1,Ic1,N1)!=(Ir2\n,Ic2,N2).\nOffset Sudoku\n1\n% 4. The same number cannot appear twice among\nthe cells with the same relative position in\n3*3 boxes.\n2\n{N1=N2}=0 :- assign(Ir1,Ic1,N1), assign(Ir2,Ic2\n,N2), Ir1\\3=Ir2\\3, Ic1\\3=Ic2\\3, (Ir1,Ic1,N1)\n!=(Ir2,Ic2,N2).\nC\nAdditional GPT-4 Analysis\nC.1\nRepresentative Example Errors (for GPT-4)\n• Failure to convert certain constants into integers dur-\ning constant formatting.\nGiven the constants: “times:\n8:00am; 9:00am; 10:00am; 11:00am”, GPT-4 generated\n1\ntimes(\"8:00am\"; \"9:00am\"; \"10:00am\";\n2\n\"11:00am\").\ninstead of the correct generation\n1\ntimes(8;9;10;11).\n• Addition of wrong clues during paraphrasing.\nGiven\nthe clue “2. The conductor working on June 12 is either\nthe conductor departing from Buttonwillow or Greg.”,\nGPT-4 generates the three sentences\n1\n2.1 The conductor working on June 12\nand\n2\nGreg are different.\n3\n2.2 The conductor working on June 12 is\neither the conductor departing from\nButtonwillow or Greg.\n4\nGreg is either the conductor departing\n5\nfrom Buttonwillow or the conductor\n6\nworking on June 12\nThe second sentence is a copy of the original, while 2.1\nand 2.3 cannot be inferred and are therefore wrong.\n• Semantic error during constraint generation.\nThe\nsentence “Vasquez will leave sometime after Macdonald.”\nis parsed by GPT-4 into\n1\nM1<M2 :- schedule(D1,M1,Du1), schedule(D2,M2\n,Du2), D1=\"Vasquez\", D2=\"Macdonald\".\nwhich is incorrect, the less than sign should be changed to\ngreater than:\n1\nM1>M2 :- schedule(D1,M1,Du1), schedule(D2,M2,\nDu2), D1=\"Vasquez\", D2=\"Macdonald\".\nThere are no syntax errors encountered with GPT-4.\nC.2\nError Subtypes\nWe further break down the paraphrasing error into two types,\n(p1) a sentence representing an exclusive disjunction is in-\ncorrectly translated into additional sentences. For example,\n“3. The card with an APR of 11% is either the card with the\n$4,000 credit limit or the one with the $20,000 credit limit.”,\nis incorrectly translated into\n1\n3.1 The card with an APR of 11% and the card\nwith the $4,000 credit limit are different.\n2\n3.2 The card with an APR of 11% is either the\ncard with the $4,000 credit limit or the one\nwith the $20,000 credit limit.\n3\n3.3 The card with the $4,000 credit limit is\neither the card with an APR of 11% or the one\nwith the $20,000 credit limit.\nand (p2) a sentence representing that four things are different\nis incorrectly translated into two incorrect sentences. For ex-\nample, the statement “ 5. The four people are Deep Shadow,\nthe superhero who started in 2007, the hero who started in\n2009 and Matt Minkle.”, is incorrectly translated into\n1\n5.1 Deep Shadow, the superhero who started in\n2007, the hero who started in 2009, and Matt\nMinkle are different.\n2\n5.2 Deep Shadow is either the superhero who\nstarted in 2007 or the hero who started in\n2009 or Matt Minkle.\n3\n5.3 The superhero who started in 2007 is either\nDeep Shadow or the hero who started in 2009\nor Matt Minkle.\n4\n5.4 The hero who started in 2009 is either Deep\nShadow or the superhero who started in 2007\nor Matt Minkle.\nConstraint Generation (semantic errors) is further broken\ninto four subtypes. The first (c1) has to do with an incorrect\ncomparison between times. For example, the statement “Tri-\ncia came in a half-hour after Ora.” is incorrectly translated\ninto\n1\n% 2. Tricia came in a half-hour after Ora.\n2\nT1=\"7:00am\", T2=\"7:30am\" :- order(C1,B1,T1),\norder(C2,B2,T2), C1=\"Ora\", C2=\"Tricia\".\n3\nT1=\"7:30am\", T2=\"8:00am\" :- order(C1,B1,T1),\norder(C2,B2,T2), C1=\"Ora\", C2=\"Tricia\".\n4\nT1=\"8:00am\", T2=\"8:30am\" :- order(C1,B1,T1),\norder(C2,B2,T2), C1=\"Ora\", C2=\"Tricia\".\nThe second (c2) is when an incorrect operator is used (e.g.\n“+” in place of “-”).\nFor example, the statement “% 1.\nVasquez will leave sometime after Macdonald.” is incor-\nrectly translated into\n1\nM1<M2 :- schedule(D1,M1,Du1), schedule(D2,M2,\nDu2), D1=\"Vasquez\", D2=\"Macdonald\".\nthe third (c3) is a disjunction in the head of a rule which\nshould not be there. For example, the statement “% 3. The\n11-year-old bird has a wingspan 8 inches shorter than Char-\nlie.” is incorrectly translated into\n1\nA=11, W=W1-8 :- assign(N,W,A), assign(N1,W1,A1)\n, N=\"Charlie\".\nand last (c4) belongs to semantic errors which do not fit\ninto any of the previous types and only occur once.\nError\nSubtype\nCount\nGPT-3\nGPT-4\nConstant Formatting\n3\n1\nParaphrasing\n2\n3\np1\n1\n3\np2\n1\n1\nCons. Gen. (syntax)\n3\n0\nCons. Gen. (semantics)\n13\n4\nc1\n3\n2\nc2\n4\n1\nc3\n3\n0\nc4\n3\n0\nTable 3: Errors on 100 test puzzles on GPT-3 and GPT-4, broken\ndown by subtype.\nTable 3 shows the count for the errors and error subtypes\nencountered for GPT-3 and GPT-4. We find that unlike GPT-\n3, GPT-4 does not make any syntax errors, however it in-\ncreases its rate of paraphrasing errors.\nC.3\nSudoku and Jobs Puzzle\nSudoku\nGPT-4 correctly generates all rules except for the\nthe constraints here:\n1\n% 1. The same number cannot appear twice in the\nsame row.\n2\n{Ir1=Ir2; Ic1=Ic2; N1=N2}=0 :- assign(Ir1,\nIc1,N1), assign(Ir2,Ic2,N2), (Ir1,Ic1,N1)!=(\nIr2,Ic2,N2), Ir1=Ir2.\n3\n% 2. The same number cannot appear twice in the\nsame column.\n4\n{Ir1=Ir2; Ic1=Ic2; N1=N2}=0 :- assign(Ir1,\nIc1,N1), assign(Ir2,Ic2,N2), (Ir1,Ic1,N1)!=(\nIr2,Ic2,N2), Ic1=Ic2.\n5\n% 3. The same number cannot appear twice in the\n3x3 box.\n6\n{Ir1=Ir2;\nIc1=Ic2; N1=N2}=0 :- assign(Ir1,Ic1,N1),\nassign(Ir2,Ic2,N2), (Ir1,Ic1,N1)!=(Ir2,Ic2,N2)\n, (Ir1-1)/3=(Ir2-1)/3, (Ic1-1)/3=(Ic2-1)/3.\nThe red portion should not be included, otherwise the pro-\ngram runs correctly.\nNote that GPT-4 generates the cor-\nrect (Ir1-1), (Ir2-2), (Ic1-1), and (Ic2-1) terms\nwhile GPT-3 does not.\nJobs Puzzle In the constant extraction step, GPT-4 fails\nto generate the gender category. From the problem\n1\nProblem 3:\n2\n1. There are four people: Roberta, Thelma,\nSteve, and Pete.\n3\n2. Among them, they hold eight different jobs.\n4\n3. Each holds exactly two jobs.\n5\n4. The jobs are: chef, guard, nurse, telephone\noperator, police officer (gender not\n6\nimplied), teacher, actor, and boxer.\n7\n5. The job of nurse is held by a male.\n8\n6. The husband of the chef is the telephone\noperator.\n9\n7. Roberta is not a boxer.\n10\n8. Pete has no education past the ninth grade.\n11\n9. Roberta, the chef, and the police officer\nwent golfing together.\n12\nQuestion: Who holds which jobs?\nIt produces:\n1\nperson: \"Roberta\"; \"Thelma\"; \"Steve\"; \"Pete\".\n2\njob: \"chef\"; \"guard\"; \"nurse\"; \"telephone\noperator\"; \"police officer\"; \"teacher\"; \"actor\n\"; \"boxer\"\nand is missing “gender:\n\"male\"; \"female\".”\nAlso, on prompt Prompt R1, GPT-4 produces the cor-\nrect output but then continues to produces constraints that it\nshould not:\n1\n% Task 1. Define the constants in each category\n.\n2\nperson(\"Roberta\"; \"Thelma\"; \"Steve\"; \"Pete\").\n3\njob(\"chef\"; \"guard\"; \"nurse\"; \"telephone\noperator\"; \"police officer\"; \"teacher\"; \"actor\n\"; \"boxer\").\n4\ngender(\"male\"; \"female\").\n5\n% Task 2. For each person P, they hold exactly\n2 jobs J and have a gender G.\n6\n{assign(P, J, G): job(J), gender(G)}=2 :-\nperson(P).\n7\n% Additional rules based on the problem\n8\nconstraints:\n9\n% 5.\nThe job of nurse is held by a\nmale.\n10\nassign(P, \"nurse\", \"male\") :-\nperson(P).\n11\n% 6.\nThe husband of the chef is the\n12\ntelephone operator.\n13\nassign(P1, \"chef\", \"female\"),\nassign(P2,\n14\n\"telephone operator\", \"male\") :-\n15\nperson(P1), person(P2).\n16\n% 7.\nRoberta is not a boxer.\n17\n:- assign(\"Roberta\", \"boxer\",\n).\n18\n% 8.\nPete has no education past the\nninth\n19\ngrade.\n20\n:- assign(\"Pete\", \"teacher\",\n).\n21\n% 9.\nRoberta, the chef, and the police\n22\nofficer went golfing together.\n23\nassign(\"Roberta\",\n, ), assign( ,\n\"chef\", ),\n24\nassign( , \"police officer\",\n) :-\n25\nperson().\nSince these constraints not supposed to be generated yet,\nthey are not appropriately prompted and hence GPT-4 pro-\nduces incorrect constraints. Apart from that, GPT-4 pro-\nduces a similar incorrect rule on rule 10.1 that GPT-3 pro-\nduces.\n1\n% 10.1 The same job cannot be assigned to\ndifferent people.\n2\n{P1=P2}=0 :- assign(P1,J1,G1), assign(P2,J2,G2)\n, J1=J2, P1!=P2.\nwhich should be\n1\nP1=P2 :- assign(P1,J,G1), assign(P2,J,G2).\n",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.SC"
  ],
  "published": "2023-07-15",
  "updated": "2023-07-15"
}