{
  "id": "http://arxiv.org/abs/2307.10188v1",
  "title": "Several categories of Large Language Models (LLMs): A Short Survey",
  "authors": [
    "Saurabh Pahune",
    "Manoj Chandrasekharan"
  ],
  "abstract": "Large Language Models(LLMs)have become effective tools for natural language\nprocessing and have been used in many different fields. This essay offers a\nsuccinct summary of various LLM subcategories. The survey emphasizes recent\ndevelopments and efforts made for various LLM kinds, including task-based\nfinancial LLMs, multilingual language LLMs, biomedical and clinical LLMs,\nvision language LLMs, and code language models. The survey gives a general\nsummary of the methods, attributes, datasets, transformer models, and\ncomparison metrics applied in each category of LLMs. Furthermore, it highlights\nunresolved problems in the field of developing chatbots and virtual assistants,\nsuch as boosting natural language processing, enhancing chatbot intelligence,\nand resolving moral and legal dilemmas. The purpose of this study is to provide\nreaders, developers, academics, and users interested in LLM-based chatbots and\nvirtual intelligent assistant technologies with useful information and future\ndirections.",
  "text": "Several categories of Large Language Models (LLMs): A Short\nSurvey\nSaurabh Pahune 1,†,‡, Manoj Chandrasekharan 2\n1\nCardinal Health, Dublin OH 43017, USA; Email: saurabh.pahune@cardinalhealth.com, Tel.:+1-901-691-7551\n2\nEmail: manoj.c@memphis.edu\nAbstract: Large Language Models (LLMs) have become effective tools for natural language process-\ning and have been used in many different ﬁelds. This essay offers a succinct summary of various\nLLM subcategories. The survey emphasizes recent developments and efforts made for various LLM\nkinds, including task-based ﬁnancial LLMs, multilingual language LLMs, biomedical and clinical\nLLMs, vision language LLMs, and code language models. The survey gives a general summary of the\nmethods, attributes, datasets, transformer models, and comparison metrics applied in each category\nof LLMs. Furthermore, it highlights unresolved problems in the ﬁeld of developing chatbots and\nvirtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and\nresolving moral and legal dilemmas. The purpose of this study is to provide readers, developers,\nacademics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies\nwith useful information and future directions. This survey sheds light on the possibilities of LLMs and\nlays the groundwork for additional study and advancement in the area by looking at the background,\nbeneﬁts, and drawbacks of LLMs generally as well as the implications of various LLM models.Thus\nthis paper offers signiﬁcant information and future directions. Our goal is to look at LLM’s history,\nthe advantages and disadvantages of LLMs in general, the types of various LLM models (eg: ﬁnance,\nclinical, multilingual, code, vision), and what all of this implies for the future\nKeywords: Natural language processing;large language models (LLM);ﬁnancial LLMs; multilin-\ngual language LLMs; biomedical and clinical LLMs; vision language LLMs;code language models;\ntransformer model;datasets;virtual intelligent assistant\n1. Introduction\nThe origins of the ﬁrst AI language models can be found in the early history of AI.\nOne of the oldest instances of an AI language model is the ELIZA language model, which\nmade its debut in 1966 at MIT[1,2].An LLM is a development of the language model idea\nin AI that signiﬁcantly increases the amount of data utilized for inference and training. As\na result, the AI model’s capabilities are greatly increased. An LLM normally includes at\nleast one billion parameters, while there isn’t a deﬁned size for the data set that must be\nused for training.A trained deep-learning model called a big language model can read and\nproduce text in a way that is similar to what a human would. Everything is accomplished\nbehind the scenes using a sizable transformer model. In 2017[3] “Attention is All You\nNeed,” to establish a transformer model (The ‘T’ in all the GPT models). It is based on the\nattention mechanism, dispensing with recurrence and convolutions entirely. Transformer\nlanguage models use a deep neural network architecture called a Transformer and they are\ntrained to predict either masked words (i.e. ﬁll-in-the-blank) or upcoming words in text[4].\nUszkoreit et al. describe the Transformer, a cutting-edge neural network design based on a\nself-attention mechanism that aims to be especially effective at interpreting language[5].\nTransformer language models have revolutionized the ﬁeld of natural language processing\n(NLP) since their introduction in 2018[6]. Transformer language models have received\nwidespread public attention, yet their generated text is often surprising even to NLP\nresearchers[4,7]. As per recent research, some of the top LLMs announced and released\nin the last few years (e.g. GPT-3/4[8], LLaMA[9], PaLM[10], MiniGPT-4[11], FinGPT[12],\nOPT[13], BERT[14], Bloomberggpt[15], BLOOM 176B[16], GPT NEO-X[17], RoBERTa [18],\nDolly2.0[19] ;)[10,13,20–22]. For applications ranging from web search and chatbots to\n1\n2 of 26\nmedical and ﬁnancial document analysis, many language models are employed in the\nbusiness[4,15,23].\nFigure 1. Distribution of language models\nAs per Figure 1. numerous language models have emerged. Language models with a\nlot of parameters and great processing power are collectively referred to as \"Large Language\nModels\" (LLM)[24]. Whereas A sort of language model known as a statistical language\nmodel (SLM) uses statistical methods to give probability values to word sequences in a\nlanguage. It is predicated on the notion that by examining the frequencies and patterns\nfound in a sizable corpus of text, it is possible to predict the likelihood of a speciﬁc word\nappearing in a speciﬁc situation[25].In a Neural Language Model (NLM), the probability\ndistribution of word sequences in a language is modeled using neural network topologies.\nNLMs are made to catch intricate word relationships and produce text that is appropriate for\nthe surrounding context[26,27]. The term \"Transformer Language Models\" (TLMs) is used\nto describe language models that especially make use of the Transformer architecture[3].\nThe term \"pre-trained language models\" (PLMs) refers to language models that have been\ntrained in an unsupervised fashion on sizable corpora before being adjusted for particular\ndownstream tasks. By extracting patterns and structures from massive volumes of text\ndata, these models learn representations of generic language. In recent years, in the ﬁeld\nof healthcare, there are various biomedical and clinical transformer models are available\nfor clinical concept extraction and medical relation[28]. BioBERT[29], ClinicalBERT[30],\nBioMegatron[31],GatorTron-base[32], GatorTron-medium[32], GatorTron-large [32]. In 2021,\nOne of the largest models in the world for reading comprehension and natural language\nFigure 2. A word cloud showing the frequency of terms used in the articles we reviewed.\n3 of 26\ninference, Megatron-Turing Natural Language Generation 530B was created by Nvidia\nand Microsoft to facilitate tasks like summarizing and content creation[33]. HuggingFace\nunveiled BLOOM last year, an open big language model that can produce text in over\na dozen programming languages in addition to 46 different natural languages and 13\nprogramming languages[34]. vision language models [35] a family of Visual Language\nModels (VLM) models that can be rapidly adapted to novel tasks using only a handful of\nannotated examples is an open challenge for multimodal machine learning research. Table1.\ncited the work as having been mentioned by the corresponding author on this language\nmodel ﬁeld.\nFigure 3. Mapping of visualize research articles\nIn Figure 3., by using connecting lines to visually represent the citations for this work,\nit demonstrates the connection. Useful research resources and project state are included in\nthis time-based citation network to arrange this review of the literature. The cited research\naddresses several topics, including model features, datasets, transformer models, and\nbenchmarks for assessing LLM performance.\nThe rest of the article is organized as follows. Section 2.1 presents prior related work\nthat has various versions of LLM models (Finance base, Clinical base, Vision base, Code-\nbase, Multilingual based)and tables respectively. In Section 3, deﬁne the conﬁguration of\nvarious models with billions of parameters and created taxonomy tables to discuss the\ndetailed methodology of LLM models such as (Benchmark and dataset, Dataset content,\nImplementation details etc.), A mapping of the articles used in this paper was also devel-\noped Figure 3. In Section 4 (Open Issues and Research Directions ), the open issues and\npotential future directions of LLM mdoels. The conclusions are described in Section 5.\nTable 1. Studies on different language models\nTypes of Language Models\nStudy\nStatistical language models (SLM)\nJelinek et al. [36], Rosenfeld et al. [37]\nPre-trained language models (PLM)\nMatthew et al.[38]\nLarge language models (LLM)\nKaplan et al.[39]\nNeural language models (NLM)\nBengio et al.[40]\nTransformer language models (TLM)\nVaswani et al.[3]\n4 of 26\n2. RELATED WORK\n2.1. Multilingual language-image model\nAn artiﬁcial intelligence model that can comprehend and produce both textual and\nvisual content across several languages is called a multilingual language-image model. In\norder to assess and provide useful information from both modalities, these models are par-\nticularly trained to process and understand the complicated relationships between words\nand visuals. Chen et al. [41] study PaLI (Pathways Language and Image model), a model\nthat extends this approach to the joint modeling of language and vision. Scheible et al.[42]\ndiscovered GottBERT is a pure German Language Model. AlexaTM 20B Soltan at al.[43]\ndemonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models,).pre-\ntrained on a mixture of denoising and Causal Language Modeling (CLM). Dossou et al.[44]\npresent AfroLM, a multilingual language model pretrained from scratch on 23 African\nlanguages (the largest effort to date) using our novel self-active learning framework. Scao\net al.[16] present BLOOM, a 176B-parameter open-access Multilingual language model\ndesigned.\nTable 2. Various multilingual language LLMs.\nModels\nStudy\nYear\nBLOOM\nScao et al.[16]\n2022\nAfroLM\nDossou et al.[44]\n2022\nAlexaTM 20B\nSoltan at al.[43]\n2022\nGottBERT\nScheible et al.[42]\n2020\nPaLI\nChen at al.[41]\n2022\n2.2. Clinical and biomedical transformer model\nA clinical and biomedical transformer model is a type of artiﬁcial intelligence that was\ncreated with the express purpose of processing and analyzing clinical and biomedical text\ndata. These transformer models make use of the architecture of the transformer, which\nhas excelled in jobs requiring natural language processing. Clinical notes, electronic health\nrecords, research articles, and other pertinent sources of clinical and biomedical data are\nincluded in the large-scale datasets used to train the clinical and biomedical transformer\nmodels. These models gain knowledge of the speciﬁc words, phrases, and ideas used in\nthe medical ﬁeld. Clinical and biomedical transformer models’ main goals are to derive\ninsightful information, carry out text categorization, entity recognition, relation extraction,\nquestion answering, and other activities particular to the clinical and biomedical area.\nClinical decision support, information retrieval, patient risk assessment, and automated\ndocumentation are just a few of the activities that they may help healthcare workers with.\nYang et al.[32] develop from scratch a large clinical language model – GatorTron – using\n>90 billion words of text (including >82 billion words of de-identiﬁed clinical text) and sys-\ntematically evaluate it on 5 clinical NLP tasks including clinical concept extraction, medical\nrelation extraction, semantic textual similarity, natural language inference (NLI), and medi-\ncal question answering (MQA). Lee et al.[29] introduce BioBERT (Bidirectional Encoder\nRepresentations from Transformers for Biomedical Text Mining), which is a domain-speciﬁc\nlanguage representation model pre-trained on large-scale biomedical corpora. Li et al[45]\nPresent Hi-BEHRT, a hierarchical Transformer-based model that can signiﬁcantly expand\nthe receptive ﬁeld of Transformers and extract associations from much longer sequences.\nUsing a multimodal large-scale linked longitudinal electronic health records. Wang et\nal.[46] propose an innovative causal inference model–InferBERT, by integrating the A\nLite Bidirectional Encoder Representations from Transformers (ALBERT). Large language\nmodels in health care As per Anmol et al.[47] has already been proposed that LLMs, such\nas ChatGPT, could have applications in the ﬁeld of health care due to the large volumes\nof free-text information available for training models.An LLM trained on more than 90\nbillion words of text from electronic health records (EHR)[28] Author Yang et al. develop a\n5 of 26\nscratch a large clinical language model GatorTron using more than 90 billion words of text.\nExisting biomedical and clinical transformer models for clinical concept extraction and med-\nical relation such as BioBERT[29], ClinicalBERT[30], BioMegatron[31],GatorTron-base[32],\nGatorTron-medium[32], GatorTron-large [32].\nSantosh et al.[48] propose PathologyBERT - a pre-trained masked language model\nwhich was trained on 347,173 histopathology specimen reports and publicly released in\nthe Huggingface1repository. Comprehensive experiments demonstrate that pre-training of\ntransformer model on pathology corpora yields performance improvements on Natural\nLanguage Understanding (NLU) and Breast Cancer Diagnose Classiﬁcation when com-\npared to nonspeciﬁc language models. Jaiswal et al.[49] intorduce RadBERT-CL which\nis\"Factually-Aware Contrastive Learning For Radiology Report Classiﬁcation.\" Also show\nthat the representations learned by RadBERT-CL can capture critical medical information in\nthe latent space. Gu et al.[14] accelerate research in biomedical and released state-of-the-art\npretrained and task-speciﬁc models for the community, and created a leaderboard featuring\nBLURB benchmark (Biomedical Language Understanding Reasoning Benchmark)). The\nauthor challenges, the major advantage of domain-speciﬁc pretraining from scratch stems\nfrom having an in-domain vocabulary. Peng et al.[50] introduce BLUE, a collection of\nresources for evaluating and analyzing biomedical natural language representation models.\nﬁnd that the BERT models pre-trained on PubMed abstracts and clinical notes see better\nperformance than do most state-of-the-art models. Beltagy et al.[51] SCIBERT leverages\nunsupervised pretraining on a large multi-domain corpus of scientiﬁc publications to\nimprove performance on downstream scientiﬁc NLP tasks. Alsentzer et al.[30] released\nClinical BERT models for clinical text: one for generic clinical text and another for discharge\nsummaries speciﬁcally. Also, demonstrate on several clinical NLP tasks that improve-\nments this system offers over traditional BERT and BioBERT. Shin et al.[31] come up with\nBioMegatron consider as large biomedical domain lanuage model. Which show consistent\nimprovements on benchmarks with larger BioMegatron model trained on a larger domain\ncorpus, contributing to our understanding of domain language model applications.\nTable 3. Various biomedical and clinical LLMs.\nModels\nStudy\nYear\nGatorTron-base\nYang et al.[32]\n2022\nBioBERT\nLee et al.[29]\n2020\nEntityBERT\nLin et al.[52]\n2021\nHi-BEHRT\nLi et al[45]\n2022\nInferBERT\nWang et al.[46]\n2021\nPathologyBERT\nSantosh et al.[48]\n2022\nPubMedBERT\nGu et al.[14]\n2021\nSciBERT\nBeltagy et al.[51]\n2019\nRadBERT\nYan et al.[53]\n2022\nClinicalBERT\nAlsentzer et al.[30]\n2019\nBlueBERT\nPeng et al.[50]\n2019\nBioMegatron\nShin et al.[31]\n2019\n2.3. Large language model for ﬁnance\nThese models can analyze and grasp complicated ﬁnancial text data efﬁciently by\nmaking use of deep learning techniques like transformer architectures. They can help with\njobs including compiling ﬁnancial reports, summarizing ﬁnancial documents, researching\ninvestments, managing portfolios, and analyzing ﬁnancial news. Financial professionals’\nability to make more educated, data-driven decisions may be improved by the use of large\nlanguage models in the ﬁeld. They can offer insights for investing plans, assist in identi-\nfying market trends, evaluate risk factors, and spot abnormalities. Wu et al.[15] present\nBloombergGPT (A large language model for ﬁnance), a 50 billion parameter language\nmodel that is trained on a wide range of ﬁnancial data. Author validates BloombergGPT on\n6 of 26\nstandard LLM benchmarks, open ﬁnancial benchmarks, and a suite of internal benchmarks\nthat most accurately reﬂect our intended usage. Scao et al.[16] present BLOOM, a 176B-\nparameter open-access language model designed and built. BLOOM is a decoder-only\nTransformer language model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total). Black et\nal.[17] introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model\ntrained on the Pile[54],evaluate its performance on a range of language-understanding,\nmathematics, and knowledge-based tasks. Araci et al.[55]introduce FinBERT language\nmodel based on BERT, to tackle NLP tasks in the ﬁnancial domain. Zhang et al.[13] present\nOpen Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers\nranging from 125M to 175B parameters. Yang et al.[12] present an open-source large\nlanguage model FinGPT, for the ﬁnance sector. FinGPT responds innovatively by lever-\naging pre-existing LLMs and ﬁne-tuning them to speciﬁc ﬁnancial applications. Xie at\nal.[56] discovers the PIXIU LLM model for Instruction Data and Evaluation Benchmark for\nFinance.\nTable 4. Various ﬁnance-based LLMs.\nModels\nStudy\nYear\nBloombergGPT\nWu et al.[15]\n2023\nGPT-NeoX\nBlack et al.[17]\n2022\nOPT\nZhang et al.[13]\n2022\nBLOOM-176B\nScao et al.[16]\n2022\nFinBERT\nAraci et al.[55]\n2019\nFinGPT\nYang et al.[12]\n2023\nPIXIU\nXie at al.[56]\n2023\n2.4. Classiﬁcations of vision language models\nArtiﬁcial intelligence models called \"vision language models\" are created to compre-\nhend and produce data from the combination of visual and linguistic inputs. These models\nseek to close the gap between the comprehension of images or other visual content and\nthat of natural language. These models are capable of carrying out a number of tasks,\nsuch as image captioning, visual question answering (VQA), image generation from text\ndescriptions, and image-text matching. For instance, a vision language model can produce\na caption for an image that accurately describes the image’s content. Similar to this, the\nmodel can offer pertinent responses or justiﬁcations when asked a text query regarding a\npicture. Alayrac et al.[57] comes with Flamingo, a family of Visual Language Models (VLM)\nmodels that can be rapidly adapted to novel tasks using only a handful of annotated exam-\nples is an open challenge for multimodal machine learning research. Monajatipoor et al.[58]\nVision-and-language (VL) models take image and text as input and learn to capture the\nassociations between them.Prior studies show that pre-trained VL models can signiﬁcantly\nimprove the model performance for downstream tasks such as Visual Question Answering\n(VQA). Ko et al.[59] present F-VLM, a simple open-vocabulary object detection method built\nupon Frozen Vision and Language Models.Where F-VLM simpliﬁes the current multi-stage\ntraining pipeline by eliminating the need for knowledge distillation or detection tailored\npretraining. Zhu et al.[11] projected MiniGPT-4 which is a enhancing vision-language\nunderstanding with advanced large language models. Hong et al.[60] propose a recurrent\nBERT model that is time-aware for use in of vision-and-language navigation(VLN). In this\npaper author propose a recurrent BERT model that is time-aware for use in VLN. Thrush et\nal.[61] present a novel task and dataset for evaluating the ability of vision and language\nmodels to conduct visio-linguistic compositional reasoning, which we call Winoground.\nWang et al.[62] propose a smaller and faster VL model, MiniVLM, which can be ﬁnetuned\nwith good performance on various downstream tasks like its larger counterpart. MiniVLM\n7 of 26\nconsists of two modules, a vision feature extractor and a transformer-based vision-language\nfusion module.\nTable 5. Various vision language LLMs.\nModels\nStudy\nYear\nFlamingo\nAlayrac et al.[57]\n2022\nBERTHop\nMonajatipoor et al.[58]\n2022\nF-VLM\nKuo et al.[59]\n2022\nMiniVLM\nWang et al.[62]\n2020\nVLN-BERT\nHong et al.[60]\n2021\nWinoground\nThrush et al.[61]\n2022\nMiniGPT-4\nZhu et al.[11]\n2023\n2.5. Classiﬁcations of code large language model (Code LLMs)\nLarge-scale language models have the potential to promote programming by pro-\nmoting code reuse, knowledge sharing, and developer collaboration. They can aid in\neliminating errors, automating repetitive coding processes, and accelerating the develop-\nment process. A code big language model is designed to help programmers with a variety\nof coding-related tasks. These models are capable of tasks like code completion, code\ngeneration, code summarization, and code translation and can comprehend the syntax,\nsemantics, and programming patterns of code. Luo et al.[63] In this paper introduced Wiz-\nardCoder, which empowers Code LLMs with complex instruction ﬁne-tuning, by adapting\nthe Evol-Instruct method to the domain of code. Nijkamp et al.[64] release a family of\nlarge language models up to 16.1B parameters, called CODEGEN, on natural language\nand programming language data. Jain et al.[65] present an approach to augment these\nlarge language models with post-processing steps based on program analysis and synthesis\ntechniques, that understand the syntax and semantics of programs. Wang et al. [66] present\nCodeT5,a uniﬁed pre-trained encoder-decoder Transformer model that better leverages the\ncode semantics conveyed from the developer-assigned identiﬁers.\nTable 6. Various code language models (Code LLMs)\nModels\nStudy\nYear\nWizardCoder\nLuo et al.[63]\n2023\nCodeGen\nNijkamp et al.[64]\n2022\nJigsaw\nJain et al.[65]\n2022\nCodeT5\nWang et al. [66]\n2021\n8 of 26\n3. Taxonomy tables of various LLM models\nTable 7. Classiﬁcations of multilingual language model\nLLM Model\nBenchmark and\nDataset\nDataset content\nImplementation\ndetails\nApplication\nVersions of Model\nPALI [41]\nWebLI[67],COCO[68],\nMultilingual\ncaptioning on\nCrossmodal-\n3600[69], VQA[70]\nNew image-text\ntraining set\ncontaining 10B\nimages and texts in\nover 100 languages\n-\nPathways\nLanguage and\nImage model\nPaLI-3B, PaLI-15B,\nPaLI-17B\nGottBERT[42]\nThe German data\nportion of the\nOSCAR measures\n145GB of text\ncontaining\napproximately 21.5\nbillion words in\napproximately 459\nmillion documents\n(one document per\nline)\nA vocabulary of\n52k subword\ntokens based on 40\nGB of randomly\nsampled\ndocuments of the\nGerman OSCAR\nportion.\n256 core TPU pod\nusing the RoBERTa\nBASE architecture\nA pure German\nLanguage Model\nGottBERT, German\nBERT, XLM\nRoBERTa, dbmz\nBERT[42]\nBLOOM [16]\nBLOOM is a\ndecoder-only\nTransformer\nlanguage model\nthat was trained on\nthe ROOTS\ncorpus[71], 2022),\nA composite\ncollection of 498\nHugging Face\ndatasets[72],\nHELM\nbenchmark[73]\nDataset\ncomprising\nhundreds of\nsources in 46\nnatural and 13\nprogramming\nlanguages (59)\n156 TFLOPs in our\nfastest\nconﬁguration with\nNVIDIA V100\nGPUs\nA 176B-Parameter\nOpen-Access\nMultilingual\nLanguage Model\nBLOOM-1B7,\nBLOOM-560M,\nBLOOM-1.1B,\nBLOOM-1.7B,\nBLOOM-1.7B,\nBLOOM-7.1B\n9 of 26\nTable 7 – continued from previous page\nAlexaTM\n20B[43]\nWikipedia and\nmC4 [70]\nData in 12\nlanguages,\nnamely, Arabic,\nEnglish, French,\nGerman, Hindi,\nItalian,\nJapanese,\nMarathi,\nPortuguese,\nSpanish, Tamil,\nand Telugu,\nPack sequences\nof tokens to\nproduce\nsequences of\napproximately\n1024 subword\nunits.\nTrained\nAlexaTM 20B\nfor 120 days on\n128 A100 GPUs\nA Large-Scale\nMultilingual\nSeq2seq Model\n-\n-\nAfroLM[44]\nLanguages\nCorpora Details\n[64,74,75],\nYOSM\ndataset[76]\nSeveral works\non sentiment\nanalysis have\nbeen done on\nhigh resource\nlanguages\nwhile low\nresources\nlanguages like\nYoruba and\nother African\nlanguages, data\ncomprised 1500\nmovie reviews\nthat were\nsourced from\nIMDB, Rotten\nTomatoes,\nLetterboxd,\nCinemapointer,\nand\nNollyrated[76]\nGoogle Cloud\nwith a single\n48GB NVIDIA\nA100 GPU\nA Self-Active\nLearning-based\nMultilingual\nPre-trained\nLanguage\nModel for 23\nAfrican\nLanguages\nAfroLM-Large,\nAfroLM-Large\n(w/AL) to\nAfroLM-Large\n(w/o AL),\nAfriBERTa-\nLarge[44]\n-\n10 of 26\nTable 8. Classiﬁcations of various clinical/biomedical transformer models\nLLM Model\nBenchmark and\nDataset\nDataset content\nImplementation\ndetails\nApplication\nVersions of Model\nGatorTron-base\n[32]\n5 clinical NLP\ntasks named entity\nrecognition [NER],\nmedical relation\nextraction (MRE),\nsemantic textual\nsimilarity (STS),\nnatural language\ninference(NLI),\nand medical\nquestion\nanswering (MQA),\nMedNLI[77], 2019\nn2c2[78], emrQA\nMedication [79]\nA total number of\n290,482,002 clinical\nnotes from\n2,476,628 patients\nwere extracted\nfrom the UF\nHealth Integrated\nData Repository\n(IDR), the\nenterprise data\nwarehouse of the\nUF Health\nsystem[32]\n992 A100 80G\nGPUs from 124\nNVIDIA DGX\nA large clinical\nlanguage model\nGatorTron-base,\nGatorTron-\nbase(1/4\ndata),GatorTron-\nmedium,\nGatorTron-large\nBioBERT [29]\nPre-trained on\nbiomedical domain\ncorpora (PubMed\nabstracts, English\nWikipedia,\nBooksCorpus and\nPMC full-text\narticles)\nNCBI Disease with\n6881 (Number of\nannotations),2010\ni2b2/VA disease\nwith 19665\n(Number of\nannotations) ,\nBC5CDR disease\nwith 12694\n(Number of\nannotations)\nEight NVIDIA\nV100 GPUs.\nA pre-trained\nbiomedical\nlanguage\nrepresentation\nmodel for\nbiomedical text\nmining\nBioBERT v1.0,\nBioBERT v1.1\nHi-BEHRT [45]\nModel\nperformance on\nincident risk\nprediction for four\ndiseases: heart\nfailure (HF),\ndiabetes, chronic\nkidney disease\n(CKD), and stroke.\nClinical Practice\nResearch Datalink\n(CPRD)[80],\nmyocardial\nfailure[81],\ndiabetes mellitus\n[82]\n2 GPUs [45]\nHierarchical\nTransformer-based\nmodel for accurate\nprediction of\nclinical events\nusing multimodal\nlongitudinal\nelectronic health\nrecords\nBEHRT, Hi-BEHRT\nInferBERT [46]\nTwo FAERS\ndatasets,\nAnalgesics-\ninduced acute liver\nfailure and\nTramadol-related\nmortalities\ndatasets\nFAERS dataset\nEffects: (Acute\nliver ﬁbrosis and\ncirrhosis, OR\nAcute liver failure\nand associated\ndisorders, OR\nCholestasis and\njaundice) AND\nDrugs by\nAND-groups:\n[Analgesics (Any\nRole)]” to extract\n45,773 [83]\none NVIDIA Tesla\nV100 GPU.\nA\nTransformer-Based\nCausal Inference\nFramework for\nEnhancing Phar-\nmacovigilance\n-\n11 of 26\nTable 8 – continued from previous page\nPathologyBERT[48]\nTwo\nnon-overlapping\ncorpora of\nhistopathology\nreports from\nEmory University\nHospital (EUH)\nEmory University\nInstitutional\nReview\nBoard(IRB), a total\nof 340,492\nunstructured\nHistopathology\nspecimens from\n67,136 patients\nwere extracted\nfrom the clinical\ndata warehouse of\nEmory University\nHospital (EUH)\nbetween the years\n1981 and 2021.\nGeForce Quadro\nRTX 6000 24 GB\nGPUs\nA New\nTransformer\nLanguage Model\nfor Pathology\nDomain(Note:\nPathologyBERT on\nCorpus II to\npredict 6 breast\ncancer diagnose\nseverity (invasive\nbreast cancer, high\nrisk lesion,\nborderline lesion,\nnon-breast cancer,\nbenign, and\nnegative)[48].\n-\nRadBERT [53]\nMIMIC-CXR\ndataset [84],\npseudo-labeled\nusing automatic\nlabeler [85]\nMIMIC-CXR\ndataset which\nconsists of 377, 110\nchest-Xray images\nof 227, 827 patients\nalong with their\ncorresponding\nde-identiﬁed\nradiology reports.\n-\nFactually-Aware\nContrastive\nLearning For\nRadiology Report\nClassiﬁcation\nRadBERT-CL\nPubMedBERT [14]\nBLURB: A\nComprehensive\nBenchmark for\nBiomedical NLP,\nBC5-\nchem[86],BC5-\ndisease[86],\nNCBI-disease[87],\nBC2GM[88],\nPubMedQA[89]\nBLUE mixes\nPubMed-based\nbiomedical\napplications (six\ndatasets such as\nBC5, ChemProt,\nand HoC) with\nMIMIC-based\nclinical\napplications (four\ndatasets such as\ni2b2 and MedNLI)\nOne DGX-2\nmachine with 16\nV100 GPUs\nIt was pretrained\nover PubMed\nabstracts and\ncomplete PubMed\nCentral articles\nand is an uncased\nBERT Base model\nBERT, RoBERTa,\nBioBERT, SciBERT,\nClinical- BERT ,\nand BlueBERT\nSciBERT[51]\nCorpus,\nEBM-NLP[90],\nSciERC[91],\nACL-ARC[92]\nTrain SCIBERT on\na random sample\nof 1.14M papers\nfrom Semantic\nscholar[93] corpus\nconsists of 18%\npapers from the\ncomputer science\ndomain and 82%\nfrom the broad\nbiomedical domain\nSingle TPU v3 with\n8 cores\nA Pretrained\nLanguage Model\nfor Scientiﬁc Text\n-\nClinicalBERT[30]\ni2b2 2006[94], i2b2\n2010[95], i2b2\n2012[96], i2b2\n2014[97]\nClinical NLP Task,\nMedNLI natural\nlanguage inference\ntask[98], Use\nclinical text from\nthe approximately\n2 million notes in\nthe MIMIC-III v1.4\ndatabase[99]\nA single GeForce\nGTX TITAN X 12\nGB GPU\nEvaluates\nrepresentations of\nclinical notes using\nbidirectional\ntransformers\n(ClinicalBERT)\nBio+Clinical BERT,\nClinical BERT [30]\n12 of 26\nTable 8 – continued from previous page\nBioMegatron[31]\nUsed downstream\nbiomedical\nbenchmark\ndatasets for\nNamed Entity\nRecognition(NER),\nRelation\nExtraction(RE),\nand Question\nAnswering(QA)\nBC5CDR[86] NER\ndataset annotated\ndisease,\nChemProt[100]\ndataset contains\nsentences from\nPubMed abstracts,\nBioASQ-7b factoid\ntask[101] is a\nbiomedical QA\ndataset\nBatch size of 64 per\nGPU with data\nparallelism on 16\nGPUs\nBioMegatron\nconsider as large\nbiomedical domain\nlanguage model\nBioMegatron-\n345m,\nBioMegatron-\n800m,\nBioMegatron-1.2b\nBlueBERT[50]\nUses both PubMed\ntext and\nde-identiﬁed\nclinical notes from\nMIMIC-III[99],\nPubMed\nabstract[102]\nModels were\ntrained with 5M\nsteps on the\nPubMed corpus\nand 0.2M steps\nand BLUE contains\nﬁve tasks with ten\ncorpora that cover\na broad range of\ndata quantities and\ndifﬁculties.\n(Sentence\nsimilarity, Named\nentity recognition,\nRelation extraction,\nDocument\nclassiﬁcation,\nInference task )\n-\nBiomedical\nLanguage\nUnderstanding\nEvaluation (BLUE)\nbenchmark to\nfacilitate research\nin the\ndevelopment of\npre-training\nlanguage\nrepresentations in\nthe biomedicine\ndomain\nBlueBERT-Base +\nUncased\n+PubMed,\nBlueBERT-Base +\nUncased+PubMed\n+ MIMIC-III\n13 of 26\nTable 9. Classiﬁcations of large language model for ﬁnance\nLLM Model\nBenchmark and\nDataset\nDataset content\nImplementation\ndetails\nApplication\nVersions of Model\nBloombergGPT[15]\nC4[103],\nFinPile[54], public\nﬁnancial NLP\nbenchmarks [104]\nColossal Clean\nCrawled Corpus\n(C4) gives us a\nvocabulary size of\n125,000, Dump of\nEnglish Wikipedia\nfrom July 1, 2022.\n8 NVIDIA 40GB\nA100 GPUs\nA large language\nmodel for ﬁnance\nBLOOM-\nstyle,BLOOM176B[16]\nGPT-NeoX [17]\nPile [54]\nIt has 22 data\nsources, coarsely\nbroken down into\n5 categories\n(Academic Writing,\nWeb-scrapes and\nInternet Resources,\nProse, Dialogue,\nMiscellaneous)\n8 NVIDIA\nA100-SXM4-40GB\nGPUs and\nconﬁgured with\ntwo AMD EPYC\n7532 CPUs\nAn Open-Source\nAutoregressive\nLanguage Model\nGPT-NeoX-20B\nOPT-175B[13]\nBookCorpus[105],\nMinhashLSH[106],\nRoBERTa\nCCNews[18]\nEight Transformer\nlanguage models\nranging from 125\nmillion to 175\nbillion parameters\nOn 992 80GB A100\nGPUs\nOpen Pre-trained\nTransformer\nLanguage Models\nOPT from 125M to\n175B (Ex:\nOPT-125M to\nOPT-175B)\nBLOOM-176B[16]\nROOTS corpus[71],\nA composite\ncollection of 498\nHugging Face\ndatasets[72],\nSuperGLUE[107],\nSTS datasets from\nMTEB[108], HELM\nbenchmark[73]\nROOTS corpus\n(dataset\ncomprising\nhundreds of\nsources in 46\nnatural and 13\nprogramming\nlanguages (59 in\ntotal))\n8 NVIDIA A100\n80GB GPUs\nA 176B-Parameter\nOpen-Access\nMultilingual\nLanguage Model\nBLOOM-\n560M,BLOOM-\n1B7, BLOOM-1.7B,\nBLOOM-\n3B,BLOOM-7.1B,\nBLOOMZ[109]\nFinBERT[55]\nFinancial\ncorpus(TRC2-\nﬁnancial)[110],\nFinancial\nPhraseBank[111],\nFiQA\nSentiment[112]\nFiQA Sentiment is\na dataset that was\ncreated for\nﬁnancial opinion\nmining and\nquestion\nanswering\nchallenge, use the\ndata for Task 1,\nwhich includes\n1,174 ﬁnancial\nnews headlines\nand tweets with\ntheir\ncorresponding\nsentiment score\nAmazon p2.xlarge\nEC2 instance with\none NVIDIA K80\nGPU, 4 vCPUs\nFinancial\nsentiment analysis\nwith pre-trained\nlanguage models\nFinBERT-task,\nFinBERT-domain\nFinGPT[12]\nAcademic datasets,\nNovel ﬁnancial\ndataset\nDifferent ﬁnancial\ndata sources, such\nas Financial News,\nCompany Fillings,\nSocial Media\nDiscussions, and\nCompany\nAnnouncements\n-\nAn open-source\nlarge language\nmodel (Financial\nsentiment analysis,\nFinancial Fraud\ndetection, Credit\nscoring, Portfolio\noptimization,\nFinancial\neducation)\nFinLLM\n14 of 26\nTable 9 – continued from previous page\nLLM Model\nBenchmark and\nDataset\nDataset content\nImplementation\ndetails\nApplication\nVersions of Model\nPIXIU [56]\nFinancial\nEvaluation\nBenchmark,\nﬁnancial\ninstruction tuning\ndataset FIT on\nvarious ﬁnancial\nNLP, FiQA-SA,\nFinancial Phrase\nBank (FPB),\nBenchmark FLARE\nFiQA-SA consist\n11,730 instructions\n(news\nheadlines,tweets),\nFPB has 48,450\nnews data types\n[56]\n8 A100 40GB\nGPUs.\nA Large Language\nModel, Instruction\nData and\nEvaluation\nBenchmark for\nFinance\nFinMA\n15 of 26\nTable 10. Classiﬁcations of vision language models\nLLM Model\nBenchmark and\nDataset\nDataset content\nImplementation\ndetails\nApplication\nVersions of Model\nFlamingo[57]\nMultiModal\nMassiveWeb\n(M3W) dataset,\nPairs of\nimage/video and\ntext, ALIGN [113],\nVQAv2, VATEX,\nVizWiz\nALIGN dataser\ncomposed of 1.8\nbillion images\npaired with alt-text,\nLTIP (Long Text\nand Image Pairs)\nwhich consists of\n312 million image\nand text pairs.\n-\nA Visual Language\nModel for\nFew-Shot Learning\nFlamingo-3B,\nFlamingo-9B,\nFlamingo\nBERTHop [58]\nChestX-ray14,\nMIMIC-CXR\nMIMIC-CXR labels\nare generated\nusing ChexPert[85]\nand NegBio[114]\nauto labelers.\nOpenI comprises\n3,996 reports and\n8,121 associated\nimages from 3,996\nunique patients\ncollected by\nIndiana University\nfrom multiple\ninstitutes.\n-\nAn Effective\nVision-and-\nLanguage Model\nfor Chest X-ray\nDisease Diagnosis\nPixelHop++,\nBlueBERT\nF-VLM [59]\nLVIS\nBenchmark[115]\nLVIS dataset which\ncontains a large\nand diverse set of\n1203 object\ncategories suitable\nfor\nopen-vocabulary\ndetection.\n-\nA simple\nopen-vocabulary\nobject detection\nmethod built upon\nFrozen Vision and\nLanguage Models.\nF-VLM-R50,\nF-VLM-R50x4,F-\nVLM-R50x16,F-\nVLM-R50x64\nMiniGPT-4 [11]\nConceptual\nCaption[116],\nSBU[117] and\nLAION[118]\nConceptual 12M\n(CC12M), a dataset\nwith 12 million\nimage-text pairs\nspeciﬁcally meant\nto be used for\nvision-and-\nlanguage\npre-training\n4 A100 (80GB)\nGPUs\nMiniGPT-4 aims to\nalign visual\ninformation from a\npretrained vision\nencoder with an\nadvanced large\nlanguage model\n(LLM)\n–\nMiniVLM [62]\nImage\nCaptioning[119],\nVQA[120], Natural\nLanguage Visual\nReasoning for Real\n(NLVR2)[121],\nCOCO image\ncaptioning task\nVQA is a\nrepresentation of\n[CLS] is used to\npredict the answer\nover a shared set of\n3129 answers with\na linear layer\n2 Intel(R) Xeon(R)\nCPU E5-2620 v4\n@2.10GHz\nA Smaller and\nFaster\nVision-Language\nModel\n-\nVLN-BERT[60]\nR2R[122],\nREVERIE[123]\nMatterport3D, a\nlarge-scale RGB-D\ndataset containing\n10,800 panoramic\nviews from 194,400\nRGB-D images of\n90 building-scale\nscenes[124]\nSingle NVIDIA\n2080Ti GPU\nA Recurrent\nVision-and-\nLanguage BERT\nfor Navigation\nVisualBERT, VL\nBERT\n16 of 26\nTable 10 – continued from previous page\nWinoground[61]\nWinoground\ndataset was\nhand-curated by\nfour expert\nannotators with\nextensive\nexperience in\nvision and\nlanguage research\nas well as\ncomputational\nlinguistics\nDataset has 1600\nimage-text pairs in\ntotal, with 800\ncorrect and 800\nincorrect pairings\n-\nProbing Vision and\nLanguage Models\nfor Visio-Linguistic\nCompositionality\n-\n17 of 26\nTable 11. Classiﬁcations of code language models (Code LLMs)\nLLM Model\nBenchmark and\nDataset\nDataset content\nImplementation\ndetails\nApplication\nVersions of Model\nWizardCoder [63]\nFour code\ngeneration\nbenchmarks:\nHumanEval[125],\nHumanEval+[126],\nMBPP[127], and\nDS-1000[128]\nInitialized it with\nthe 20K instruction-\nfollowing dataset\ncalled Code\nAlpaca[67].\nEvol-Instruct\ntechnique on this\ndataset consisting\nof 20,000 samples\nto produce evolved\ndata.\n-\nEmpowering Code\nLarge Language\nModels with\nEvol-Instruct\nWizardLM [129]\nCodeGen [64]\nTHEPILE,\nBIGQUERY, and\nBIGPYTHON.\nTHEPILE is an\n825.18 GiB English\ntext corpus\ncollected [54],\nBIGQUERY is a\nsubset of Google’s\npublicly available\nBigQuery dataset,\ndataset\nBIGPYTHON\ncontains a large\namount of data in\nthe programming\nlanguage, Python.\nGoogle’s TPU-v4\nhardware\nAn open LLM for\ncode with\nmulti-turn\nprogram synthesis\nCodeGen -MULTI ,\nCodeGen-NL,\nCodeGen-MONO\nJigsaw [65]\nPandasEval\ndataset, Hackathon\ndataset\nThis dataset\nconsists of 68\nPython Pandas\ntasks. Each task\ncan be solved\nusing a single line\nof code by\ncomposing at most\n2-3 Pandas\nfunctions,\nHackathon dataset\nconsists of 21\nPandas tasks; each\ntask\n-\nLarge Language\nModels meet\nProgram Synthesis\n-\nCodeT5[66]\nSearchNet[130]\nConsists of 99\nnatural language\nqueries with about\n4k expert relevance\nannotations of\nlikely results from\nCodeSearchNet\nCorpus. The\ncorpus contains\nabout 6 million\nfunctions from\nopen-source code\nspanning six\nprogramming\nlanguages (Go,\nJava, JavaScript,\nPHP, Python, and\nRuby)\n16 NVIDIA A100\nGPUs with 40G\nmemory.\nUniﬁed Pre-trained\nEncoder-Decoder\nModels for Code\nUnderstanding\nand Generation\nCodeBERT\n18 of 26\nTable 12. Detailed of several existing LLMs conﬁguration with Millions/ Billions of parameters.\nModel\nOptimizer and Layers\nModel size\nReference\nGPT-2\nAdam, 12 layers\n1.5 billion\n[131]\nGPT-3\nAdam, 96 layers\n175 billion\n[132,133]\nMicrosoft DialoGPT\n-\n147 million\n[134,135]\nBloombergGPT\nGELU, 70 layers\n50 billion\n[15]\nVicuna\n-\n13 billion\n[136]\nDolly2.0\n-\n12 billion\n[19]\nBLOOM\nAdam,70 layers\n176 billion\n[34]\nLLaMA\nAdamW,-\n65 billion\n[9]\nJurassic-1\n-\n178 billion\n[137]\nGLM\nAdamW,-\n130 billion\n[138]\nPaLM\nAdafactor,-\n540 billion\n[139]\nOPT 175B\nAdamW, 96\n175 billion\n[13]\nChinchilla\nAdam,80 layers\n70 billion\n[67]\nBERT-base\nAdam,12 layers\n100 million\n[14]\nBERT-large\nAdam,24 layers\n300 million\n[14]\nALBERT\nAdam ,12 layers\n12 million\n[140]\nRoBERTa base\nAdam,12 layers\n125 million\n[18]\nRoBERTa large\nAdam,24 layers\n355 million\n[18]\nMegatron-Turing NLG\n-\n530 billion\n[141]\nBioBERT\n-\n13.5 billion\n[29]\nClinicalBERT\nAdam\n1.28 billion\n[30,142]\nBioMegatron\nAdam,24\n1.2 billion\n[31]\nGatorTron-base\nAdam,24 layers\n345 million\n[32,143]\nGatorTron-medium\nAdam,48 layers\n3.9 billion\n[32,143]\nGatorTron-large\nAdam, 56 layers\n8.9 billion\n[32,143]\nGopher\nAdam,-\n280 billion\n[144]\nGPT-NeoX\nAdamW\n20 billion\n[17]\nBloom 176\nAdam,24 layers\n176 billion\n[16]\nPubMedBERT\n-\n110 million\n[89]\nAlexaTM 20B\n-,46layers\n19.75 billion\n[43]\nAfroLM-Large\n-,10layers\n264 million\n[44]\nHi-BEHRT\nAdam, layers\n264 million\n[45]\nPathologyBERT\nAdam, 12 Layers\n347 million\n[48]\nBioMegatron\nAdam, 24 Layers\n345 million\n[31]\nBioMegatron medium\nAdam, 36 Layers\n800 million\n[31]\nBioMegatron large\nAdam, 24 Layers\n1.2 billion\n[31]\nBloombergGPT\nAdam, 70 Layers\n50.6 billion\n[15]\nBLOOM-style\nAdam, 70 Layers\n50 billion\n[145]\nGPT-NeoX-20B\nAdam, 44 Layers\n20 billion\n[17]\nCODEGEN\n-\n16.1 billion\n[64]\nIn Table12. based on what we’ve seen, the billions to millions range. Dataset optimiza-\ntion is a crucial step in LLM models, particularly those with a large number of parameters,\nwith the goal of improving the model’s functionality and speed. To make sure the training\ndata is representative, diverse, and in line with the anticipated results, dataset optimization\nentails carefully choosing and preparing the training data. Researchers and programmers\ncan enhance the model’s capacity to comprehend and produce words, leading to more\nprecise and cogent responses, by optimizing the dataset. Basically, dataset optimization\nhelps LLM models reach their full potential by supplying high-quality training data that is\nin line with the particular tasks or objectives at hand.\n19 of 26\n4. Open Issues and Research Directions\nDue to the size of large language models, their deployment requires a high level of\ntechnical expertise, including a ﬁrm understanding of deep learning, transformer models,\ndistributed software, and hardware as well as ethical and legal issues arising from the\nliability and harm potential of such systems.\nMany professionals in the IT sector are working to support research and create tech-\nnologies that can open up access to broad language models, allowing customers and\ncompanies of all sizes to take advantage of them.\nIt is not clear how large clinical language models with billions of parameters can help\nmedical AI systems utilize unstructured electronic health records (EHRs) within the current\nlegal and ethical framework while ensuring privacy of patient information and accuracy of\nthe information provided[28].\nScaling and maintaining large language models can be difﬁcult and expensive. Build-\ning a foundational large language model often requires months of training time and millions\nof dollars [33].\nAnd because LLMs require a signiﬁcant amount of training data, developers and\nenterprises can ﬁnd it a challenge to access large-enough datasets to train such systems\nwhile ensuring data is collected ethically and with permission of the parties involved.\nMaintaining them by putting in place systems to ensure accurate and useful outputs at\nscale is also a signiﬁcant challenge.\n5. Conclusion\nIn this study, the most recent advances in large language models (LLMs) were show-\ncased and the key concepts, ﬁndings, and strategies for understanding and exploiting\nLLMs were presented. A wide range of issues are covered in this study, including model\nfeatures, datasets, transformer models, and LLM performance benchmarks. Recent studies\nhave focused on various LLM types, such as multilingual LLMs, biomedical and clinical\nLLMs, vision language LLMs, and code language models. This survey attempts to cover\nthe most recent research on LLMs and provides academics and engineers with a helpful\nresource.\nAcronyms\nBERT Bidirectional Encoder Representation From Transformers.\nBioBERT Bidirectional Encoder Representations from Transformers for Biomedical Text\nMining.\nBLUE Biomedical Language Understanding Evaluation.\nC4 Colossal Clean Crawled Corpus.\nLLaMA Large Language Model Meta AI.\nLLM Large Language Model.\nMTEB Massive text embedding benchmark.\nRoBERTa Robustly Optimized BERT approach.\n6. References\n1.\nSingh, S.; Thakur, H.K. Survey of various AI chatbots based on technology used. In Proceedings\nof the 2020 8th International Conference on Reliability, Infocom Technologies and Optimization\n(Trends and Future Directions)(ICRITO). IEEE, 2020, pp. 1074–1079.\n2.\nWeizenbaum, J. ELIZA—a computer program for the study of natural language communication\nbetween man and machine. Communications of the ACM 1966, 9, 36–45.\n20 of 26\n3.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin,\nI. Attention is all you need. Advances in neural information processing systems 2017, 30.\n4.\nChang, T.A.; Bergen, B.K. Language model behavior: A comprehensive survey. arXiv preprint\narXiv:2303.11504 2023.\n5.\nUszkoreit, J. Transformer: A novel neural network architecture for language understanding.\nGoogle AI Blog 2017, 31.\n6.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. Improving language understand-\ning by generative pre-training 2018.\n7.\nDevlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint arXiv:1810.04805 2018.\n8.\nGPT-4 is OpenAI’s most advanced system, producing safer and more useful responses. https:\n//openai.com/product/gpt-4. [Online; Accessed 06-17-2023].\n9.\nIntroducing LLaMA: A foundational, 65-billion-parameter large language model. https://ai.\nfacebook.com/blog/large-language-model-llama-meta-ai/. [Online; Accessed 06-17-2023].\n10.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung,\nH.W.; Sutton, C.; Gehrmann, S.; et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311 2022.\n11.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; Elhoseiny, M.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592 2023.\n12.\nYang, H.; Liu, X.Y.; Wang, C.D. FinGPT: Open-Source Financial Large Language Models. arXiv\npreprint arXiv:2306.06031 2023.\n13.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin,\nX.V.; et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n2022.\n14.\nGu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu, X.; Naumann, T.; Gao, J.; Poon, H.\nDomain-speciﬁc language model pretraining for biomedical natural language processing. ACM\nTransactions on Computing for Healthcare (HEALTH) 2021, 3, 1–23.\n15.\nWu, S.; Irsoy, O.; Lu, S.; Dabravolski, V.; Dredze, M.; Gehrmann, S.; Kambadur, P.; Rosenberg, D.;\nMann, G. Bloomberggpt: A large language model for ﬁnance. arXiv preprint arXiv:2303.17564\n2023.\n16.\nScao, T.L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow, D.; Castagné, R.; Luccioni, A.S.; Yvon,\nF.; Gallé, M.; et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv\npreprint arXiv:2211.05100 2022.\n17.\nBlack, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.; Golding, L.; He, H.; Leahy, C.;\nMcDonell, K.; Phang, J.; et al. Gpt-neox-20b: An open-source autoregressive language model.\narXiv preprint arXiv:2204.06745 2022.\n18.\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer,\nL.; Stoyanov, V. Roberta: A robustly optimized bert pretraining approach.\narXiv preprint\narXiv:1907.11692 2019.\n19.\nEDWARDS, B. A really big deal”—Dolly is a free, open source, ChatGPT-style AI model.\nhttps://arstechnica.com/information-technology/2023/04/a-really-big-deal-dolly-is-a-free-\nopen-source-chatgpt-style-ai-model/, 2023. [Online; Accessed 06-17-2023].\n20.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.D.; Dhariwal, P.; Neelakantan, A.; Shyam,\nP.; Sastry, G.; Askell, A.; et al. Language models are few-shot learners. Advances in neural\ninformation processing systems 2020, 33, 1877–1901.\n21.\nPeng, B.; Li, C.; He, P.; Galley, M.; Gao, J. Instruction tuning with gpt-4.\narXiv preprint\narXiv:2304.03277 2023.\n22.\nLopez-Lira, A.; Tang, Y. Can chatgpt forecast stock price movements? Return predictability and\nlarge language models. arXiv preprint arXiv:2304.07619 2023.\n23.\nLee, A. What are large language models used for. NVIDIA Blog 2023.\n24.\nZhao, W.X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.;\net al. A survey of large language models. arXiv preprint arXiv:2303.18223 2023.\n25.\nGao, J.; Lin, C.Y. Introduction to the special issue on statistical language modeling, 2004.\n26.\nMelis, G.; Dyer, C.; Blunsom, P. On the state of the art of evaluation in neural language models.\narXiv preprint arXiv:1707.05589 2017.\n27.\nBengio, Y.; Ducharme, R.; Vincent, P. A neural probabilistic language model. Advances in neural\ninformation processing systems 2000, 13.\n21 of 26\n28.\nYang, X.; Chen, A.; PourNejatian, N.; Shin, H.C.; Smith, K.E.; Parisien, C.; Compas, C.; Martin,\nC.; Costa, A.B.; Flores, M.G.; et al. A large language model for electronic health records. npj\nDigital Medicine 2022, 5, 194.\n29.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C.H.; Kang, J. BioBERT: a pre-trained biomedical\nlanguage representation model for biomedical text mining. Bioinformatics 2020, 36, 1234–1240.\n30.\nAlsentzer, E.; Murphy, J.R.; Boag, W.; Weng, W.H.; Jin, D.; Naumann, T.; McDermott, M. Publicly\navailable clinical BERT embeddings. arXiv preprint arXiv:1904.03323 2019.\n31.\nShin, H.C.; Zhang, Y.; Bakhturina, E.; Puri, R.; Patwary, M.; Shoeybi, M.; Mani, R. BioMegatron:\nLarger biomedical domain language model. arXiv preprint arXiv:2010.06060 2020.\n32.\nYang, X.; Chen, A.; PourNejatian, N.; Shin, H.C.; Smith, K.E.; Parisien, C.; Compas, C.; Martin,\nC.; Flores, M.G.; Zhang, Y.; et al. Gatortron: A large clinical language model to unlock patient\ninformation from unstructured electronic health records. arXiv preprint arXiv:2203.03540 2022.\n33.\nLEE, A. What Are Large Language Models Used For? https://blogs.nvidia.com/blog/2023/0\n1/26/what-are-large-language-models-used-for/, 2023. [Online; Accessed 06-17-2023].\n34.\nBigScience Blog. https://bigscience.huggingface.co/blog/bloom, 2023. [Online; Accessed\n06-17-2023].\n35.\nLee, S.; Kim, W.J.; Ye, J.C. LLM Itself Can Read and Generate CXR Images. arXiv preprint\narXiv:2305.11490 2023.\n36.\nJelinek, F. Statistical methods for speech recognition; MIT press, 1998.\n37.\nRosenfeld, R. Two decades of statistical language modeling: Where do we go from here?\nProceedings of the IEEE 2000, 88, 1270–1278.\n38.\nPeters, M.E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. “Deep\ncontextualized word representations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2018, New Orleans, Louisiana,USA.\n39.\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.B.; Chess, B.; Child, R.; Gray, S.; Radford, A.;\nWu, J.; Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361\n2020.\n40.\nBengio, Y.; Senécal, J.S. Adaptive importance sampling to accelerate training of a neural\nprobabilistic language model. Technical report, IDIAP, 2003.\n41.\nChen, X.; Wang, X.; Changpinyo, S.; Piergiovanni, A.; Padlewski, P.; Salz, D.; Goodman, S.;\nGrycner, A.; Mustafa, B.; Beyer, L.; et al. Pali: A jointly-scaled multilingual language-image\nmodel. arXiv preprint arXiv:2209.06794 2022.\n42.\nScheible, R.; Thomczyk, F.; Tippmann, P.; Jaravine, V.; Boeker, M. GottBERT: a pure German\nlanguage model. arXiv preprint arXiv:2012.02110 2020.\n43.\nSoltan, S.; Ananthakrishnan, S.; FitzGerald, J.; Gupta, R.; Hamza, W.; Khan, H.; Peris, C.; Rawls,\nS.; Rosenbaum, A.; Rumshisky, A.; et al. Alexatm 20b: Few-shot learning using a large-scale\nmultilingual seq2seq model. arXiv preprint arXiv:2208.01448 2022.\n44.\nDossou, B.F.; Tonja, A.L.; Yousuf, O.; Osei, S.; Oppong, A.; Shode, I.; Awoyomi, O.O.; Emezue,\nC.C. AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23\nAfrican Languages. arXiv preprint arXiv:2211.03263 2022.\n45.\nLi, Y.; Mamouei, M.; Salimi-Khorshidi, G.; Rao, S.; Hassaine, A.; Canoy, D.; Lukasiewicz, T.;\nRahimi, K. Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical\nevents using multimodal longitudinal electronic health records. IEEE Journal of Biomedical and\nHealth Informatics 2022.\n46.\nWang, X.; Xu, X.; Tong, W.; Roberts, R.; Liu, Z. InferBERT: a transformer-based causal inference\nframework for enhancing pharmacovigilance. Frontiers in Artiﬁcial Intelligence 2021, 4, 659622.\n47.\nArora, A.; Arora, A. The promise of large language models in health care. The Lancet 2023,\n401, 641.\n48.\nSantos, T.; Tariq, A.; Das, S.; Vayalpati, K.; Smith, G.H.; Trivedi, H.; Banerjee, I. PathologyBERT–\nPre-trained Vs. A New Transformer Language Model for Pathology Domain. arXiv preprint\narXiv:2205.06885 2022.\n49.\nJaiswal, A.; Tang, L.; Ghosh, M.; Rousseau, J.F.; Peng, Y.; Ding, Y. RadBERT-CL: factually-aware\ncontrastive learning for radiology report classiﬁcation. In Proceedings of the Machine Learning\nfor Health. PMLR, 2021, pp. 196–208.\n50.\nPeng, Y.; Yan, S.; Lu, Z. Transfer learning in biomedical natural language processing: an\nevaluation of BERT and ELMo on ten benchmarking datasets. arXiv preprint arXiv:1906.05474\n2019.\n22 of 26\n51.\nBeltagy, I.; Lo, K.; Cohan, A. SciBERT: A pretrained language model for scientiﬁc text. arXiv\npreprint arXiv:1903.10676 2019.\n52.\nLin, C.; Miller, T.; Dligach, D.; Bethard, S.; Savova, G. EntityBERT: Entity-centric masking strat-\negy for model pretraining for the clinical domain. Association for Computational Linguistics\n(ACL), 2021.\n53.\nYan, A.; McAuley, J.; Lu, X.; Du, J.; Chang, E.Y.; Gentili, A.; Hsu, C.N. RadBERT: Adapt-\ning transformer-based language models to radiology.\nRadiology: Artiﬁcial Intelligence 2022,\n4, e210258.\n54.\nGao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.;\nNabeshima, N.; et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027 2020.\n55.\nAraci, D. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint\narXiv:1908.10063 2019.\n56.\nXie, Q.; Han, W.; Zhang, X.; Lai, Y.; Peng, M.; Lopez-Lira, A.; Huang, J. PIXIU: A Large Language\nModel, Instruction Data and Evaluation Benchmark for Finance. arXiv preprint arXiv:2306.05443\n2023.\n57.\nAlayrac, J.B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Hasson, Y.; Lenc, K.; Mensch, A.; Millican,\nK.; Reynolds, M.; et al. Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems 2022, 35, 23716–23736.\n58.\nMonajatipoor, M.; Rouhsedaghat, M.; Li, L.H.; Jay Kuo, C.C.; Chien, A.; Chang, K.W. Berthop:\nAn effective vision-and-language model for chest x-ray disease diagnosis.\nIn Proceedings\nof the Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th\nInternational Conference, Singapore, September 18–22, 2022, Proceedings, Part V. Springer, 2022,\npp. 725–734.\n59.\nKuo, W.; Cui, Y.; Gu, X.; Piergiovanni, A.; Angelova, A. F-VLM: Open-Vocabulary Object\nDetection upon Frozen Vision and Language Models. arXiv preprint arXiv:2209.15639 2022.\n60.\nHong, Y.; Wu, Q.; Qi, Y.; Rodriguez-Opazo, C.; Gould, S. Vln bert: A recurrent vision-and-\nlanguage bert for navigation. In Proceedings of the Proceedings of the IEEE/CVF conference\non Computer Vision and Pattern Recognition, 2021, pp. 1643–1653.\n61.\nThrush, T.; Jiang, R.; Bartolo, M.; Singh, A.; Williams, A.; Kiela, D.; Ross, C. Winoground:\nProbing vision and language models for visio-linguistic compositionality. In Proceedings of the\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022,\npp. 5238–5248.\n62.\nWang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; Liu, Z. Minivlm: A smaller and\nfaster vision-language model. arXiv preprint arXiv:2012.06946 2020.\n63.\nLuo, Z.; Xu, C.; Zhao, P.; Sun, Q.; Geng, X.; Hu, W.; Tao, C.; Ma, J.; Lin, Q.; Jiang, D. WizardCoder:\nEmpowering Code Large Language Models with Evol-Instruct. arXiv preprint arXiv:2306.08568\n2023.\n64.\nNijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou, Y.; Savarese, S.; Xiong, C. Codegen:\nAn open large language model for code with multi-turn program synthesis. arXiv preprint\narXiv:2203.13474 2022.\n65.\nJain, N.; Vaidyanath, S.; Iyer, A.; Natarajan, N.; Parthasarathy, S.; Rajamani, S.; Sharma, R.\nJigsaw: Large language models meet program synthesis. In Proceedings of the Proceedings of\nthe 44th International Conference on Software Engineering, 2022, pp. 1219–1231.\n66.\nWang, Y.; Wang, W.; Joty, S.; Hoi, S.C. Codet5: Identiﬁer-aware uniﬁed pre-trained encoder-\ndecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 2021.\n67.\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; Casas, D.d.L.;\nHendricks, L.A.; Welbl, J.; Clark, A.; et al. Training compute-optimal large language models.\narXiv preprint arXiv:2203.15556 2022.\n68.\nChen, X.; Fang, H.; Lin, T.Y.; Vedantam, R.; Gupta, S.; Dollár, P.; Zitnick, C.L. Microsoft coco\ncaptions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 2015.\n69.\nThapliyal, A.V.; Pont-Tuset, J.; Chen, X.; Soricut, R. Crossmodal-3600: A massively multilingual\nmultimodal evaluation dataset. arXiv preprint arXiv:2205.12522 2022.\n70.\nXue, L.; Constant, N.; Roberts, A.; Kale, M.; Al-Rfou, R.; Siddhant, A.; Barua, A.; Raffel, C. mT5:\nA massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934\n2020.\n71.\nLaurençon, H.; Saulnier, L.; Wang, T.; Akiki, C.; Villanova del Moral, A.; Le Scao, T.; Von Werra,\nL.; Mou, C.; González Ponferrada, E.; Nguyen, H.; et al. The bigscience roots corpus: A 1.6\n23 of 26\ntb composite multilingual dataset.\nAdvances in Neural Information Processing Systems 2022,\n35, 31809–31826.\n72.\nLhoest, Q.; Villanova del Moral, A.; Jernite, Y.; Thakur, A.; von Platen, P.; Patil, S.; Chaumond, J.;\nDrame, M.; Plu, J.; Tunstall, L.; et al. Datasets: A Community Library for Natural Language\nProcessing. In Proceedings of the Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations; Association for Computational\nLinguistics: Online and Punta Cana, Dominican Republic, 2021; pp. 175–184. https://doi.org/\n10.18653/v1/2021.emnlp-demo.21.\n73.\nLiang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.;\nWu, Y.; Kumar, A.; et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n2022.\n74.\nAdelani, D.I.; Alabi, J.O.; Fan, A.; Kreutzer, J.; Shen, X.; Reid, M.; Ruiter, D.; Klakow, D.;\nNabende, P.; Chang, E.; et al. A few thousand translations go a long way! leveraging pre-trained\nmodels for african news translation. arXiv preprint arXiv:2205.02022 2022.\n75.\nOgueji, K.; Zhu, Y.; Lin, J. Small Data? No Problem! Exploring the Viability of Pretrained Multi-\nlingual Language Models for Low-resourced Languages. In Proceedings of the Proceedings\nof the 1st Workshop on Multilingual Representation Learning; Association for Computational\nLinguistics: Punta Cana, Dominican Republic, 2021; pp. 116–126. https://doi.org/10.18653/v1\n/2021.mrl-1.11.\n76.\nShode, I.; Adelani, D.I.; Feldman, A. yosm: A new yoruba sentiment corpus for movie reviews.\narXiv preprint arXiv:2204.09711 2022.\n77.\nHerlihy, C.; Rudinger, R. MedNLI is not immune: Natural language inference artifacts in the\nclinical domain. arXiv preprint arXiv:2106.01491 2021.\n78.\nWang, Y.; Fu, S.; Shen, F.; Henry, S.; Uzuner, O.; Liu, H.; et al. The 2019 n2c2/ohnlp track on\nclinical semantic textual similarity: overview. JMIR medical informatics 2020, 8, e23375.\n79.\nPampari, A.; Raghavan, P.; Liang, J.; Peng, J. emrqa: A large corpus for question answering on\nelectronic medical records. arXiv preprint arXiv:1809.00732 2018.\n80.\nHerrett, E.; Gallagher, A.M.; Bhaskaran, K.; Forbes, H.; Mathur, R.; Van Staa, T.; Smeeth, L. Data\nresource proﬁle: clinical practice research datalink (CPRD). International journal of epidemiology\n2015, 44, 827–836.\n81.\nConrad, N.; Judge, A.; Tran, J.; Mohseni, H.; Hedgecott, D.; Crespillo, A.P.; Allison, M.; Hem-\ningway, H.; Cleland, J.G.; McMurray, J.J.; et al. Temporal trends and patterns in heart failure\nincidence: a population-based study of 4 million individuals. The Lancet 2018, 391, 572–580.\n82.\nKuan, V.; Denaxas, S.; Gonzalez-Izquierdo, A.; Direk, K.; Bhatti, O.; Husain, S.; Sutaria, S.;\nHingorani, M.; Nitsch, D.; Parisinos, C.A.; et al. A chronological map of 308 physical and mental\nhealth conditions from 4 million individuals in the English National Health Service. The Lancet\nDigital Health 2019, 1, e63–e77.\n83.\nXingqiaoWang. DeepCausalPV-master. https://github.com/XingqiaoWang/DeepCausalPV-\nmaster, 2021. [Online; Accessed 06-17-2023].\n84.\nJohnson, A.E.; Pollard, T.J.; Greenbaum, N.R.; Lungren, M.P.; Deng, C.y.; Peng, Y.; Lu, Z.; Mark,\nR.G.; Berkowitz, S.J.; Horng, S. MIMIC-CXR-JPG, a large publicly available database of labeled\nchest radiographs. arXiv preprint arXiv:1901.07042 2019.\n85.\nIrvin, J.; Rajpurkar, P.; Ko, M.; Yu, Y.; Ciurea-Ilcus, S.; Chute, C.; Marklund, H.; Haghgoo, B.;\nBall, R.; Shpanskaya, K.; et al. Chexpert: A large chest radiograph dataset with uncertainty\nlabels and expert comparison. In Proceedings of the Proceedings of the AAAI conference on\nartiﬁcial intelligence, 2019, Vol. 33, pp. 590–597.\n86.\nLi, J.; Sun, Y.; Johnson, R.J.; Sciaky, D.; Wei, C.H.; Leaman, R.; Davis, A.P.; Mattingly, C.J.;\nWiegers, T.C.; Lu, Z. BioCreative V CDR task corpus: a resource for chemical disease relation\nextraction. Database 2016, 2016.\n87.\nDo˘gan, R.I.; Leaman, R.; Lu, Z. NCBI disease corpus: a resource for disease name recognition\nand concept normalization. Journal of biomedical informatics 2014, 47, 1–10.\n88.\nSmith, L.; Tanabe, L.K.; Kuo, C.J.; Chung, I.; Hsu, C.N.; Lin, Y.S.; Klinger, R.; Friedrich, C.M.;\nGanchev, K.; Torii, M.; et al. Overview of BioCreative II gene mention recognition. Genome\nbiology 2008, 9, 1–19.\n89.\nJin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.W.; Lu, X. Pubmedqa: A dataset for biomedical research\nquestion answering. arXiv preprint arXiv:1909.06146 2019.\n90.\nNye, B.; Li, J.J.; Patel, R.; Yang, Y.; Marshall, I.J.; Nenkova, A.; Wallace, B.C. A corpus with\nmulti-level annotations of patients, interventions and outcomes to support language processing\n24 of 26\nfor medical literature. In Proceedings of the Proceedings of the conference. Association for\nComputational Linguistics. Meeting. NIH Public Access, 2018, Vol. 2018, p. 197.\n91.\nLuan, Y.; He, L.; Ostendorf, M.; Hajishirzi, H. Multi-task identiﬁcation of entities, relations, and\ncoreference for scientiﬁc knowledge graph construction. arXiv preprint arXiv:1808.09602 2018.\n92.\nJurgens, D.; Kumar, S.; Hoover, R.; McFarland, D.; Jurafsky, D. Measuring the evolution of a\nscientiﬁc ﬁeld through citation frames. Transactions of the Association for Computational Linguistics\n2018, 6, 391–406.\n93.\nAmmar, W.; Groeneveld, D.; Bhagavatula, C.; Beltagy, I.; Crawford, M.; Downey, D.; Dunkel-\nberger, J.; Elgohary, A.; Feldman, S.; Ha, V.; et al. Construction of the literature graph in semantic\nscholar. arXiv preprint arXiv:1805.02262 2018.\n94.\nUzuner, Ö.; Luo, Y.; Szolovits, P. Evaluating the state-of-the-art in automatic de-identiﬁcation.\nJournal of the American Medical Informatics Association 2007, 14, 550–563.\n95.\nUzuner, Ö.; South, B.R.; Shen, S.; DuVall, S.L. 2010 i2b2/VA challenge on concepts, assertions,\nand relations in clinical text. Journal of the American Medical Informatics Association 2011, 18, 552–\n556.\n96.\nSun, W.; Rumshisky, A.; Uzuner, O. Evaluating temporal relations in clinical text: 2012 i2b2\nchallenge. Journal of the American Medical Informatics Association 2013, 20, 806–813.\n97.\nStubbs, A.; Uzuner, Ö. Annotating longitudinal clinical narratives for de-identiﬁcation: The\n2014 i2b2/UTHealth corpus. Journal of biomedical informatics 2015, 58, S20–S29.\n98.\nRomanov, A.; Shivade, C. Lessons from natural language inference in the clinical domain. arXiv\npreprint arXiv:1808.06752 2018.\n99.\nJohnson, A.E.; Pollard, T.J.; Shen, L.; Lehman, L.w.H.; Feng, M.; Ghassemi, M.; Moody, B.;\nSzolovits, P.; Anthony Celi, L.; Mark, R.G. MIMIC-III, a freely accessible critical care database.\nScientiﬁc data 2016, 3, 1–9.\n100. Krallinger, M.; Rabal, O.; Leitner, F.; Vazquez, M.; Salgado, D.; Lu, Z.; Leaman, R.; Lu, Y.; Ji,\nD.; Lowe, D.M.; et al. The CHEMDNER corpus of chemicals and drugs and its annotation\nprinciples. Journal of cheminformatics 2015, 7, 1–17.\n101. Tsatsaronis, G.; Balikas, G.; Malakasiotis, P.; Partalas, I.; Zschunke, M.; Alvers, M.R.; Weis-\nsenborn, D.; Krithara, A.; Petridis, S.; Polychronopoulos, D.; et al. An overview of the BIOASQ\nlarge-scale biomedical semantic indexing and question answering competition. BMC bioinfor-\nmatics 2015, 16, 1–28.\n102. Peters, M.E.; Ammar, W.; Bhagavatula, C.; Power, R. Semi-supervised sequence tagging with\nbidirectional language models. arXiv preprint arXiv:1705.00108 2017.\n103. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P.J.\nExploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer. Journal of\nMachine Learning Research 2020, 21, 1–67.\n104. Shah, R.; Chawla, K.; Eidnani, D.; Shah, A.; Du, W.; Chava, S.; Raman, N.; Smiley, C.; Chen,\nJ.; Yang, D. When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model\nfor Financial Domain. In Proceedings of the Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing; Association for Computational Linguistics: Abu\nDhabi, United Arab Emirates, 2022; pp. 2322–2335.\n105. Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; Fidler, S. Aligning\nbooks and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the Proceedings of the IEEE international conference on computer\nvision, 2015, pp. 19–27.\n106. Rajaraman, A.; Ullman, J.D. Mining of massive datasets; Cambridge University Press, 2011.\n107. Wang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; Bowman,\nS. Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems 2019, 32.\n108. Muennighoff, N.; Tazi, N.; Magne, L.; Reimers, N. MTEB: Massive text embedding benchmark.\narXiv preprint arXiv:2210.07316 2022.\n109. Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Biderman, S.; Scao, T.L.; Bari, M.S.; Shen,\nS.; Yong, Z.X.; Schoelkopf, H.; et al. Crosslingual generalization through multitask ﬁnetuning.\narXiv preprint arXiv:2211.01786 2022.\n110. Reuters Corpora (RCV1, RCV2, TRC2). https://trec.nist.gov/data/reuters/reuters.html, 2004.\n[Online; Accessed 06-17-2023].\n111. Malo, P.; Sinha, A.; Takala, P.; Korhonen, P.; Wallenius, J. FinancialPhraseBank-v1. 0, 2013.\n25 of 26\n112. Maia, M.; Handschuh, S.; Freitas, A.; Davis, B.; McDermott, R.; Zarrouk, M.; Balahur, A.\nWww’18 open challenge: ﬁnancial opinion mining and question answering. In Proceedings of\nthe Companion proceedings of the the web conference 2018, 2018, pp. 1941–1942.\n113. Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.T.; Parekh, Z.; Pham, H.; Le, Q.; Sung, Y.H.; Li, Z.; Duerig, T.\nScaling up visual and vision-language representation learning with noisy text supervision. In\nProceedings of the International conference on machine learning. PMLR, 2021, pp. 4904–4916.\n114. Peng, Y.; Wang, X.; Lu, L.; Bagheri, M.; Summers, R.; Lu, Z. NegBio: a high-performance tool for\nnegation and uncertainty detection in radiology reports. AMIA Summits on Translational Science\nProceedings 2018, 2018, 188.\n115. Gupta, A.; Dollar, P.; Girshick, R. Lvis: A dataset for large vocabulary instance segmentation. In\nProceedings of the Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2019, pp. 5356–5364.\n116. Changpinyo, S.; Sharma, P.; Ding, N.; Soricut, R. Conceptual 12m: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In Proceedings of the Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3558–3568.\n117. Ordonez, V.; Kulkarni, G.; Berg, T. Im2text: Describing images using 1 million captioned\nphotographs. Advances in neural information processing systems 2011, 24.\n118. Schuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk, R.; Mullis, C.; Katta, A.; Coombes, T.;\nJitsev, J.; Komatsuzaki, A. Laion-400m: Open dataset of clip-ﬁltered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114 2021.\n119. Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; Gao, J. Uniﬁed vision-language pre-training\nfor image captioning and vqa. In Proceedings of the Proceedings of the AAAI conference on\nartiﬁcial intelligence, 2020, Vol. 34, pp. 13041–13049.\n120. Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; Parikh, D. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answering. In Proceedings of the Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2017, pp. 6904–6913.\n121. Suhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; Artzi, Y. A corpus for reasoning about natural\nlanguage grounded in photographs. arXiv preprint arXiv:1811.00491 2018.\n122. Anderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.; Sünderhauf, N.; Reid, I.; Gould, S.; Van\nDen Hengel, A. Vision-and-language navigation: Interpreting visually-grounded navigation\ninstructions in real environments. In Proceedings of the Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018, pp. 3674–3683.\n123. Qi, Y.; Wu, Q.; Anderson, P.; Wang, X.; Wang, W.Y.; Shen, C.; Hengel, A.v.d. Reverie: Remote\nembodied visual referring expression in real indoor environments.\nIn Proceedings of the\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020,\npp. 9982–9991.\n124. Chang, A.; Dai, A.; Funkhouser, T.; Halber, M.; Niessner, M.; Savva, M.; Song, S.; Zeng, A.;\nZhang, Y. Matterport3D: Learning from RGB-D Data in Indoor Environments. International\nConference on 3D Vision (3DV) 2017.\n125. Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H.P.d.O.; Kaplan, J.; Edwards, H.; Burda, Y.;\nJoseph, N.; Brockman, G.; et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374 2021.\n126. Liu, J.; Xia, C.S.; Wang, Y.; Zhang, L. Is your code generated by chatgpt really correct? rigorous\nevaluation of large language models for code generation. arXiv preprint arXiv:2305.01210 2023.\n127. Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski, H.; Dohan, D.; Jiang, E.; Cai, C.; Terry,\nM.; Le, Q.; et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732\n2021.\n128. Lai, Y.; Li, C.; Wang, Y.; Zhang, T.; Zhong, R.; Zettlemoyer, L.; Yih, S.W.t.; Fried, D.; Wang, S.; Yu,\nT. DS-1000: A natural and reliable benchmark for data science code generation. arXiv preprint\narXiv:2211.11501 2022.\n129. Xu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao, C.; Jiang, D. Wizardlm: Empowering\nlarge language models to follow complex instructions. arXiv preprint arXiv:2304.12244 2023.\n130. Husain, H.; Wu, H.H.; Gazit, T.; Allamanis, M.; Brockschmidt, M. Codesearchnet challenge:\nEvaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 2019.\n131. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. Language models are\nunsupervised multitask learners. OpenAI blog 2019, 1, 9.\n132. Koubaa, A. GPT-4 vs. GPT-3.5: A concise showdown 2023.\n133. GPT-3. https://en.wikipedia.org/wiki/GPT-3#GPT-3.5, 2023. [Online; Accessed 06-17-2023].\n26 of 26\n134. Zhang, Y.; Sun, S.; Galley, M.; Chen, Y.C.; Brockett, C.; Gao, X.; Gao, J.; Liu, J.; Dolan, B. Dialogpt:\nLarge-scale generative pre-training for conversational response generation.\narXiv preprint\narXiv:1911.00536 2019.\n135. Microsoft Research-DialoGPT. https://www.microsoft.com/en-us/research/project/large-\nscale-pretraining-for-response-generation/, 2019. [Online; Accessed 06-17-2023].\n136. Padmanabha, N.H. A step-by-step guide to running Vicuna-13B Large Language Model on\nyour GPU / CPU machine. https://www.linkedin.com/pulse/step-by-step-guide-running-\nvicuna-13b-large-language-nischal/, 2023. [Online; Accessed 06-17-2023].\n137. Lieber, O.; Sharir, O.; Lenz, B.; Shoham, Y. Jurassic-1: Technical details and evaluation. White\nPaper. AI21 Labs 2021, 1.\n138. Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al.\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 2022.\n139. WIKIPEDIA. https://en.wikipedia.org/wiki/PaLM, 2022. [Online; Accessed 06-17-2023].\n140. Khadhraoui, M.; Bellaaj, H.; Ammar, M.B.; Hamam, H.; Jmaiel, M. Survey of BERT-base models\nfor scientiﬁc text classiﬁcation: COVID-19 case study. Applied Sciences 2022, 12, 2891.\n141. Smith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhandari, S.; Casper, J.; Liu, Z.; Prabhumoye,\nS.; Zerveas, G.; Korthikanti, V.; et al. Using deepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 2022.\n142. ClinicalBERT. https://huggingface.co/medicalai/ClinicalBERT. [Online; Accessed 06-17-2023].\n143. Hugging Face. https://huggingface.co/UFNLP/gatortron-base. [Online; Accessed 06-17-2023].\n144. Alford, A. Google Trains 280 Billion Parameter AI Language Model Gopher. https://www.\ninfoq.com/news/2022/01/deepmind-gopher/, 2022. [Online; Accessed 06-17-2023].\n145. Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas,\nD.; Hendricks, L.A.; Welbl, J.; Clark, A.; et al. An empirical analysis of compute-optimal large\nlanguage model training. Advances in Neural Information Processing Systems 2022, 35, 30016–\n30030.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2023-07-05",
  "updated": "2023-07-05"
}