{
  "id": "http://arxiv.org/abs/2307.11046v2",
  "title": "A Definition of Continual Reinforcement Learning",
  "authors": [
    "David Abel",
    "AndrÃ© Barreto",
    "Benjamin Van Roy",
    "Doina Precup",
    "Hado van Hasselt",
    "Satinder Singh"
  ],
  "abstract": "In a standard view of the reinforcement learning problem, an agent's goal is\nto efficiently identify a policy that maximizes long-term reward. However, this\nperspective is based on a restricted view of learning as finding a solution,\nrather than treating learning as endless adaptation. In contrast, continual\nreinforcement learning refers to the setting in which the best agents never\nstop learning. Despite the importance of continual reinforcement learning, the\ncommunity lacks a simple definition of the problem that highlights its\ncommitments and makes its primary concepts precise and clear. To this end, this\npaper is dedicated to carefully defining the continual reinforcement learning\nproblem. We formalize the notion of agents that \"never stop learning\" through a\nnew mathematical language for analyzing and cataloging agents. Using this new\nlanguage, we define a continual learning agent as one that can be understood as\ncarrying out an implicit search process indefinitely, and continual\nreinforcement learning as the setting in which the best agents are all\ncontinual learning agents. We provide two motivating examples, illustrating\nthat traditional views of multi-task reinforcement learning and continual\nsupervised learning are special cases of our definition. Collectively, these\ndefinitions and perspectives formalize many intuitive concepts at the heart of\nlearning, and open new research pathways surrounding continual learning agents.",
  "text": "A Definition of Continual Reinforcement Learning\nDavid Abel\ndmabel@google.com\nGoogle DeepMind\nAndrÂ´e Barreto\nandrebarreto@google.com\nGoogle DeepMind\nBenjamin Van Roy\nbenvanroy@google.com\nGoogle DeepMind\nDoina Precup\ndoinap@google.com\nGoogle DeepMind\nHado van Hasselt\nhado@google.com\nGoogle DeepMind\nSatinder Singh\nbaveja@google.com\nGoogle DeepMind\nAbstract\nIn a standard view of the reinforcement learning problem, an agentâ€™s goal is to\nefficiently identify a policy that maximizes long-term reward. However, this\nperspective is based on a restricted view of learning as finding a solution, rather\nthan treating learning as endless adaptation. In contrast, continual reinforcement\nlearning refers to the setting in which the best agents never stop learning. Despite\nthe importance of continual reinforcement learning, the community lacks a simple\ndefinition of the problem that highlights its commitments and makes its primary\nconcepts precise and clear. To this end, this paper is dedicated to carefully defining\nthe continual reinforcement learning problem. We formalize the notion of agents\nthat â€œnever stop learningâ€ through a new mathematical language for analyzing and\ncataloging agents. Using this new language, we define a continual learning agent as\none that can be understood as carrying out an implicit search process indefinitely,\nand continual reinforcement learning as the setting in which the best agents are all\ncontinual learning agents. We provide two motivating examples, illustrating that\ntraditional views of multi-task reinforcement learning and continual supervised\nlearning are special cases of our definition. Collectively, these definitions and\nperspectives formalize many intuitive concepts at the heart of learning, and open\nnew research pathways surrounding continual learning agents.\n1\nIntroduction\nIn The Challenge of Reinforcement Learning, Sutton states: â€œPart of the appeal of reinforcement\nlearning is that it is in a sense the whole AI problem in a microcosmâ€ [56]. Indeed, the problem\nfacing an agent that learns to make better decisions from experience is at the heart of the study of\nArtificial Intelligence (AI). Yet, when we study the reinforcement learning (RL) problem, it is typical\nto restrict our focus in a number of ways. For instance, we often suppose that a complete description\nof the state of the environment is available to the agent, or that the interaction stream is subdivided\ninto episodes. Beyond these standard restrictions, however, there is another significant assumption\nthat constrains the usual framing of RL: We tend to concentrate on agents that learn to solve problems,\nrather than agents that learn forever. For example, consider an agent learning to play Go: Once the\nagent has discovered how to master the game, the task is complete, and the agentâ€™s learning can\nstop. This view of learning is often embedded in the standard formulation of RL, in which an agent\ninteracts with a Markovian environment with the goal of efficiently identifying an optimal policy, at\nwhich point learning can cease.\nBut what if this is not the best way to model the RL problem? That is, instead of viewing learning\nas finding a solution, we can instead think of it as endless adaptation. This suggests study of the\ncontinual reinforcement learning (CRL) problem [47, 48, 25, 27], as first explored in the thesis by\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.11046v2  [cs.LG]  1 Dec 2023\nRing [46], with close ties to supervised never-ending [10, 39, 43] and continual learning [47, 48, 26,\n54, 41, 42, 49, 22, 30, 45, 4].\nDespite the prominence of CRL, the community lacks a clean, general definition of this problem. It is\ncritical to develop such a definition to promote research on CRL from a clear conceptual foundation,\nand to guide us in understanding and designing continual learning agents. To these ends, this paper is\ndedicated to carefully defining the CRL problem. Our definition is summarized as follows:\nThe CRL Problem (Informal)\nAn RL problem is an instance of CRL if the best agents never stop learning.\nThe core of our definition is framed around two new insights that formalize the notion of â€œagents\nthat never stop learningâ€: (i) we can understand every agent as implicitly searching over a set of\nhistory-based policies (Theorem 3.1), and (ii) every agent will either continue this search forever,\nor eventually stop (Remark 3.2). We make these two insights rigorous through a pair of logical\noperators on agents that we call generates and reaches that provide a new mathematical language for\ncharacterizing agents. Using these tools, we then define CRL as any RL problem in which all of the\nbest agents never stop their implicit search. We provide two motivating examples of CRL, illustrating\nthat traditional multi-task RL and continual supervised learning are special cases of our definition.\nWe further identify necessary properties of CRL (Theorem 4.1) and the new operators (Theorem 4.2,\nTheorem 4.3). Collectively, these definitions and insights formalize many intuitive concepts at the\nheart of continual learning, and open new research pathways surrounding continual learning agents.\n2\nPreliminaries\nWe first introduce key concepts and notation. Our conventions are inspired by Ring [46], the recent\nwork by Dong et al. [16] and Lu et al. [32], as well as the literature on general RL by Hutter [23, 24],\nLattimore [28], Leike [29], Cohen et al. [12], and Majeed [36].\nNotation.\nWe let capital calligraphic letters denote sets (ğ’³), lower case letters denote constants and\nfunctions (ğ‘¥), italic capital letters denote random variables (ğ‘‹), and blackboard capitals denote the\nnatural and real numbers (N, R, N0 = N âˆª{0}). Additionally, we let Î”(ğ’³) denote the probability\nsimplex over the set ğ’³. That is, the function ğ‘: ğ’³Ã— ğ’´â†’Î”(ğ’µ) expresses a probability mass\nfunction ğ‘(Â· | ğ‘¥, ğ‘¦), over ğ’µ, for each ğ‘¥âˆˆğ’³and ğ‘¦âˆˆğ’´. Lastly, we use Â¬ to denote logical negation,\nand we use âˆ€ğ‘¥âˆˆğ’³and âˆƒğ‘¥âˆˆğ’³to express the universal and existential quantifiers over a set ğ’³.\n2.1\nAgents and Environments\nWe begin by defining environments, agents, and related artifacts.\nDefinition 2.1. An agent-environment interface is a pair (ğ’œ, ğ’ª) of countable sets ğ’œand ğ’ªwhere\n|ğ’œ| â‰¥2 and |ğ’ª| â‰¥1.\nWe refer to elements of ğ’œas actions, denoted ğ‘, and elements of ğ’ªas observations, denoted ğ‘œ.\nHistories define the possible interactions between an agent and an environment that share an interface.\nDefinition 2.2. The histories with respect to interface (ğ’œ, ğ’ª) are the set of sequences of action-\nobservation pairs,\nâ„‹=\nâˆ\nÃ˜\nğ‘¡=0\n(ğ’œÃ— ğ’ª)ğ‘¡.\n(2.1)\nWe refer to an individual element of â„‹as a history, denoted â„, and we let â„â„â€² express the history\nresulting from the concatenation of any two histories â„, â„â€² âˆˆâ„‹. Furthermore, the set of histories\nof length ğ‘¡âˆˆN0 is defined as â„‹ğ‘¡= (ğ’œÃ— ğ’ª)ğ‘¡, and we use â„ğ‘¡âˆˆâ„‹ğ‘¡to refer to a history containing\nğ‘¡action-observation pairs, â„ğ‘¡= ğ‘0ğ‘œ1 . . . ğ‘ğ‘¡âˆ’1ğ‘œğ‘¡, with â„0 = âˆ…the empty history. An environment is\nthen a function from the set of all environments, â„°, that produces observations given a history.\nDefinition 2.3. An environment with respect to interface (ğ’œ, ğ’ª) is a function ğ‘’: â„‹Ã— ğ’œâ†’Î”(ğ’ª).\nThis model of environments is general in that it can capture Markovian environments such as Markov\ndecision processes (MDPs, Puterman, 2014) and partially observable MDPs (Cassandra et al., 1994),\nas well as both episodic and non-episodic settings. We next define an agent as follows.\n2\nDefinition 2.4. An agent with respect to interface (ğ’œ, ğ’ª) is a function ğœ†: â„‹â†’Î”(ğ’œ).\nWe let Î› denote the set of all agents, and let Î› denote any non-empty subset of Î›. This treatment of\nan agent captures the mathematical way experience gives rise to behavior, as in â€œagent functionsâ€\nfrom work by Russell and Subramanian [50]. This is in contrast to a mechanistic account of agency\nas proposed by Dong et al. [16] and Sutton [58]. Further, note that Definition 2.4 is precisely a\nhistory-based policy; we embrace the view that there is no real distinction between an agent and a\npolicy, and will refer to all such functions as â€œagentsâ€ unless otherwise indicated.\n2.2\nRealizable Histories\nWe will be especially interested in the histories that occur with non-zero probability as a result of the\ninteraction between a particular agent and environment.\nDefinition 2.5. The realizable histories of a given agent-environment pair, (ğœ†, ğ‘’), define the set of\nhistories of any length that can occur with non-zero probability from the interaction of ğœ†and ğ‘’,\nâ„‹ğœ†,ğ‘’= Â¯\nâ„‹=\nâˆ\nÃ˜\nğ‘¡=0\n(\nâ„ğ‘¡âˆˆâ„‹ğ‘¡:\nğ‘¡âˆ’1\nÃ–\nğ‘˜=0\nğ‘’(ğ‘œğ‘˜+1 | â„ğ‘˜, ğ‘ğ‘˜)ğœ†(ğ‘ğ‘˜| â„ğ‘˜) > 0\n)\n.\n(2.2)\nGiven a realizable history â„, we will refer to the realizable history suffixes, â„â€², which, when concate-\nnated with â„, produce a realizable history â„â„â€² âˆˆÂ¯\nâ„‹.\nDefinition 2.6. The realizable history suffixes of a given (ğœ†, ğ‘’) pair, relative to a history prefix\nâ„âˆˆâ„‹ğœ†,ğ‘’, define the set of histories that, when concatenated with prefix â„, remain realizable,\nâ„‹ğœ†,ğ‘’\nâ„\n= Â¯\nâ„‹â„= {â„â€² âˆˆâ„‹: â„â„â€² âˆˆâ„‹ğœ†,ğ‘’}.\n(2.3)\nWe abbreviate â„‹ğœ†,ğ‘’to Â¯\nâ„‹, and â„‹ğœ†,ğ‘’\nâ„\nto Â¯\nâ„‹â„, where ğœ†and ğ‘’are obscured for brevity.\n2.3\nReward, Performance, and the RL Problem\nSupported by the arguments of Bowling et al. [7], we assume that all of the relevant goals or purposes\nof an agent are captured by a deterministic reward function (in line with the reward hypothesis [57]).\nDefinition 2.7. We call ğ‘Ÿ: ğ’œÃ— ğ’ªâ†’R a reward function.\nWe remain agnostic to how the reward function is implemented; it could be a function inside of\nthe agent, or the reward functionâ€™s output could be a special scalar in each observation. Such\ncommitments do not impact our framing. When we refer to an environment we will implicitly mean\nthat a reward function has been selected as well. We remain agnostic to how reward is aggregated to\ndetermine performance, and instead adopt the function ğ‘£defined as follows.\nDefinition 2.8. The performance, ğ‘£: â„‹Ã— Î› Ã— â„°â†’[vmin, vmax] is a bounded function for fixed\nconstants vmin, vmax âˆˆR.\nThe function ğ‘£(ğœ†, ğ‘’| â„) expresses some statistic of the received future random rewards produced\nby the interaction between ğœ†and ğ‘’following history â„, where we use ğ‘£(ğœ†, ğ‘’) as shorthand for\nğ‘£(ğœ†, ğ‘’| â„0). While we accommodate any ğ‘£that satisfies the above definition, it may be useful to\nthink of specific choices of ğ‘£(ğœ†, ğ‘’| â„ğ‘¡), such as the average reward,\nlim inf\nğ‘˜â†’âˆ\n1\nğ‘˜Eğœ†,ğ‘’[ğ‘…ğ‘¡+ . . . + ğ‘…ğ‘¡+ğ‘˜| ğ»ğ‘¡= â„ğ‘¡],\n(2.4)\nwhere Eğœ†,ğ‘’[ Â· Â· Â· | ğ»ğ‘¡= â„ğ‘¡] denotes expectation over the stochastic process induced by ğœ†and ğ‘’\nfollowing history â„ğ‘¡. Or, we might consider performance based on the expected discounted reward,\nğ‘£(ğœ†, ğ‘’| â„ğ‘¡) = Eğœ†,ğ‘’[ğ‘…ğ‘¡+ ğ›¾ğ‘…ğ‘¡+1 + . . . | ğ»ğ‘¡= â„ğ‘¡], where ğ›¾âˆˆ[0, 1) is a discount factor.\nThe above components give rise to a simple definition of the RL problem.\nDefinition 2.9. An instance of the RL problem is defined by a tuple (ğ‘’, ğ‘£, Î›) as follows\nÎ›âˆ—= arg max\nğœ†âˆˆÎ› ğ‘£(ğœ†, ğ‘’).\n(2.5)\nThis captures the RL problem facing an agent designer that would like to identify an optimal agent\n(ğœ†âˆ—âˆˆÎ›âˆ—) with respect to the performance (ğ‘£), among the available agents (Î›), in a particular\nenvironment (ğ‘’). We note that a simple extension of this definition of the RL problem might instead\nconsider a set of environments (or similar alternatives).\n3\n3\nAgent Operators: Generates and Reaches\nWe next introduce two new insights about agents, and the logical operators that formalize them:\n1. Theorem 3.1: Every agent can be understood as searching over another set of agents.\n2. Remark 3.2: Every agent will either continue their search forever, or eventually stop.\nWe make these insights precise by introducing a pair of logical operators on agents: (1) a set of agents\ngenerates (Definition 3.4) another set of agents, and (2) a given agent reaches (Definition 3.5) an agent\nset. Together, these operators enable us to define learning as the implicit search process captured by\nthe first insight, and continual learning as the process of continuing this search indefinitely.\n3.1\nOperator 1: An Agent Basis Generates an Agent Set.\nThe first operator is based on two complementary intuitions.\nFrom the first perspective, an agent can be understood as searching over a space of representable\naction-selection strategies. For instance, in an MDP, agents can be interpreted as searching over the\nspace of policies (that is, the space of stochastic mappings from the MDPâ€™s state to action). It turns\nout this insight can be extended to any agent and any environment.\nThe second complementary intuition notes that, as agent designers, we often first identify the space\nof representable action-selection strategies of interest. Then, it is natural to design agents that search\nthrough this space. For instance, in designing an agent to interact with an MDP, we might be interested\nin policies representable by a neural network of a certain size and architecture. When we design\nagents, we then consider all agents (choices of loss function, optimizer, memory, and so on) that\nsearch through the space of assignments of weights to this particular neural network using standard\nmethods like gradient descent. We codify these intuitions in the following definitions.\nDefinition 3.1. An agent basis (or simply, a basis), Î›B âŠ‚Î›, is any non-empty subset of Î›.\nNotice that an agent basis is a choice of agent set, Î›. We explicitly call out a basis with distinct\nnotation (Î›B) as it serves an important role in the discussion that follows. For example, we next\nintroduce learning rules as functions that switch between elements of an agent basis for each history.\nDefinition 3.2. A learning rule over an agent basis Î›B is a function, ğœ: â„‹â†’Î›B, that selects a\nbase agent for each history.\nWe let Î£ denote the set of all learning rules over Î›B, and let Î£ denote any non-empty subset of\nÎ£. A learning rule is a mechanism for switching between the available base agents following each\nnew experience. Notice that learning rules are deterministic; while a simple extension captures the\nstochastic case, we will see by Theorem 3.1 that the above is sufficiently general in a certain sense.\nWe use ğœ(â„)(â„) to refer to the action distribution selected by the agent ğœ†= ğœ(â„) at any history â„.\nDefinition 3.3. Let Î£ be a set of learning rules over some basis Î›B, and let ğ‘’be an environment. We\nsay that a set Î› is ğšº-generated by Î›B in ğ‘’, denoted Î›B âŠ¢ğ‘’Î£ Î›, if and only if\nâˆ€ğœ†âˆˆÎ›âˆƒğœâˆˆÎ£âˆ€â„âˆˆÂ¯\nâ„‹ğœ†(â„) = ğœ(â„)(â„).\n(3.1)\nThus, any choice of Î£ together with a basis Î›B induces a family of agent sets whose elements can be\nunderstood as switching between the basis according to the rules prescribed by Î£. We then say that\na basis generates an agent set in an environment if there exists a set of learning rules that switches\nbetween the basis elements to produce the agent set.\nDefinition 3.4. We say a basis Î›B generates Î› in ğ‘’, denoted Î›B âŠ¢ğ‘’Î›, if and only if\nÎ›B âŠ¢ğ‘’Î£ Î›.\n(3.2)\nIntuitively, an agent basis Î›B generates another agent set Î› just when the agents in Î› can be\nunderstood as switching between the base agents. It is in this sense that we can understand agents\nas searching through a basisâ€”an agent is just a particular sequence of history-conditioned switches\nover a basis. For instance, let us return to the example of a neural network: The agent basis might\nrepresent a specific multilayer perceptron, where each element of this basis is an assignment to the\nnetworkâ€™s weights. The learning rules are different mechanisms that choose the next set of weights in\n4\nâ‡¤B\nâ‡¤\nâŒ«1\n...\n...\nâŒ«8\n(a) Generates\nâŒ«1\n...\n...\n(b) Sometimes Reaches\nFigure 1: A visual of the generates (left) and sometimes reaches (right) operators. (a) Generates: An\nagent basis, Î›B, comprised of three base agents depicted by the triangle, circle, and square, generates\na set Î› containing agents that can each be understood as switching between the base agents in the\nrealizable histories of environment ğ‘’. (b) Sometimes Reaches: On the right, we visualize ğœ†1 âˆˆÎ›\ngenerated by Î›B (from the figure on the left) to illustrate the concept of sometimes reaches. That is,\nthe agentâ€™s choice of action distribution at each history can be understood as switching between the\nthree basis elements, and there is at least one history for which the agent stops switchingâ€”here, we\nshow the agent settling on the choice of the blue triangle and never switching again.\nresponse to experience (such as gradient descent). Together, the agent basis and the learning rules\ngenerate the set of agents that search over choices of weights in reaction to experience. We present a\ncartoon visual of the generates operator in Figure 1(a).\nNow, using the generates operator, we revisit and formalize the central insight of this section: Every\nagent can be understood as implicitly searching over an agent basis. We take this implicit search\nprocess to be the behavioral signature of learning.\nTheorem 3.1. For any agent-environment pair (ğœ†, ğ‘’), there exists infinitely many choices of a basis,\nÎ›B, such that both (1) ğœ†âˆ‰Î›B, and (2) Î›B âŠ¢ğ‘’{ğœ†}.\nDue to space constraints, all proofs are deferred to Appendix B.\nWe require that ğœ†âˆ‰Î›B to ensure that the relevant bases are non-trivial generators of {ğœ†}. This\ntheorem tells us that no matter the choice of agent or environment, we can view the agent as a series\nof history-conditioned switches between basis elements. In this sense, we can understand the agent as\nif 1 it were carrying out a search over the elements of some Î›B. We emphasize that there are infinitely\nmany choices of such a basis to illustrate that there are many plausible interpretations of an agentâ€™s\nbehaviorâ€”we return to this point throughout the paper.\n3.2\nOperator 2: An Agent Reaches a Basis.\nOur second operator reflects properties of an agentâ€™s limiting behavior in relation to a basis. Given an\nagent and a basis that the agent searches through, what happens to the agentâ€™s search process in the\nlimit: does the agent keep switching between elements of the basis, or does it eventually stop? For\nexample, in an MDP, many agents of interest eventually stop their search on a choice of a fixed policy.\nWe formally define this notion in terms of an agent reaching a basis according to two modalities: an\nagent (i) sometimes or (ii) never reaches a basis.\nDefinition 3.5. We say agent ğœ†âˆˆÎ› sometimes reaches Î›B in ğ‘’, denoted ğœ†â‡\nğ‘’Î›B, if and only if\nâˆƒâ„âˆˆÂ¯\nâ„‹âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â€²) = ğœ†B(â„â„â€²).\n(3.3)\nThat is, for at least one realizable history, there is some base agent (ğœ†B) that produces the same action\ndistribution as ğœ†forever after. This indicates that the agent can be understood as if it has stopped its\nsearch over the basis. We present a visual of sometimes reaches in Figure 1(b). By contrast, we say\nan agent never reaches a basis just when it never becomes equivalent to a base agent.\nDefinition 3.6. We say agent ğœ†âˆˆÎ› never reaches Î›B in ğ‘’, denoted ğœ†â‡\n/\nğ‘’Î›B, iff Â¬(ğœ†â‡\nğ‘’Î›B).\n1We use as if in the sense of the positive economists, such as Friedman [19].\n5\nThe reaches operators formalize the intuition that, since every agent can be interpreted as if it were\nsearching over a basis, every agent will either (1) sometimes, or (2) never stop this search. Since (1)\nand (2) are simply negations of each other, we can now plainly state this fact as follows.\nRemark 3.2. For any agent-environment pair (ğœ†, ğ‘’) and any choice of basis Î›B such that Î›B âŠ¢ğ‘’{ğœ†},\nexactly one of the following two properties must be satisfied:\n(1) ğœ†â‡\nğ‘’Î›B,\n(2) ğœ†â‡\n/\nğ‘’Î›B.\n(3.4)\nThus, by Theorem 3.1, every agent can be thought of as implicitly searching over an agent basis, and\nby Remark 3.2, every agent will either (1) sometimes, or (2) never stop this search. We take this\nimplicit search process to be the behavioral signature of learning, and will later exploit this perspective\nto define a continual learning agent as one that continues its search forever (Definition 4.1). Our\nanalysis in Section 4.4 further elucidates basic properties of both the generates and reaches operators,\nand Figure 1 presents a cartoon visualizing the intuition behind each of the operators. We summarize\nall definitions and notation in a table in Appendix A.\nConsiderations on the Operators.\nNaturally, we can design many variations of both â‡\nğ‘’and âŠ¢ğ‘’. For\ninstance, we might be interested in a variant of reaches in which an agent becomes ğœ–-close to any of\nthe basis elements, rather than require exact behavioral equivalence. Concretely, we highlight four\naxes of variation that can modify the definitions of the operators. We state these varieties for reaches,\nbut similar modifications can be made to the generates operator, too:\n1. Realizability. An agent reaches a basis (i) in all histories (and thus, all environments), or (ii)\nin the histories realizable by a given (ğœ†, ğ‘’) pair.\n2. History Length. An agent reaches a basis over (i) infinite or, (ii) finite length histories.\n3. Probability. An agent reaches a basis (i) with probability one, or (ii) with high probability.\n4. Equality or Approximation. An agent reaches a basis by becoming (i) equivalent to a base\nagent, or (ii) sufficiently similar to a base agent.\nRather than define all of these variations precisely for both operators (though we do explore some in\nAppendix C), we acknowledge their existence, and simply note that the formal definitions of these\nvariants follow naturally.\n4\nContinual Reinforcement Learning\nWe now provide a precise definition of CRL. The definition formalizes the intuition that CRL captures\nsettings in which the best agents do not convergeâ€”they continue their implicit search over an agent\nbasis indefinitely.\n4.1\nDefinition: Continual RL\nWe first define continual learning agents using the generates and never reaches operators as follows.\nDefinition 4.1. An agent ğœ†is a continual learning agent in ğ‘’relative to Î›B if and only if the basis\ngenerates the agent (Î›B âŠ¢ğ‘’{ğœ†}) and the agent never reaches the basis (ğœ†â‡\n/\nğ‘’Î›B).\nThis means that an agent is a continual learner in an environment relative to Î›B if the agentâ€™s search\nover Î›B continues forever. Notice that an agent might be considered a continual learner with respect\nto one basis but not another; we explore this fact more in Section 4.4.\nThen, using these tools, we formally define CRL as follows.\nDefinition 4.2. Consider an RL problem (ğ‘’, ğ‘£, Î›). Let Î›B âŠ‚Î› be a basis such that Î›B âŠ¢ğ‘’Î›, and\nlet Î›âˆ—= arg maxğœ†âˆˆÎ› ğ‘£(ğœ†, ğ‘’). We say (ğ‘’, ğ‘£, Î›, Î›B) defines a CRL problem if âˆ€ğœ†âˆ—âˆˆÎ›âˆ—ğœ†âˆ—â‡\n/\nğ‘’Î›B.\nSaid differently, an RL problem is an instance of CRL just when all of the best agents are continual\nlearning agents relative to basis Î›B. This problem encourages a significant departure from how we\ntend to think about designing agents: Given a basis, rather than try to build agents that can solve\nproblems by identifying a fixed high-quality element of the basis, we would like to design agents that\ncontinue to update their behavior indefinitely in light of their experience.\n6\n4.2\nCRL Examples\nWe next detail two examples of CRL to provide further intuition.\nQ-Learning in Switching MDPs.\nFirst we consider a simple instance of CRL based on the standard\nmulti-task view of MDPs. In this setting, the agent repeatedly samples an MDP to interact with\nfrom a fixed but unknown distribution [64, 9, 2, 25, 20]. In particular, we make use of the switching\nMDP environment from Luketina et al. [33]. The environment ğ‘’consists of a collection of ğ‘›\nunderlying MDPs, ğ‘š1, . . . , ğ‘šğ‘›, with a shared action space and environment-state space. We refer\nto this environment-state space using observations, ğ‘œâˆˆğ’ª. The environment has a fixed constant\npositive probability of 0.001 to switch the underlying MDP, which yields different transition and\nreward functions until the next switch. The agent only observes each environment state ğ‘œâˆˆğ’ª, which\ndoes not reveal the identity of the active MDP. The rewards of each underlying MDP are structured\nso that each MDP has a unique optimal policy. We assume ğ‘£is defined as the average reward, and the\nbasis is the set of ğœ–-greedy policies over all ğ‘„(ğ‘œ, ğ‘) functions, for fixed ğœ–= 0.15. Consequently, the\nset of agents we generate, Î›B âŠ¢ğ‘’Î›, consists of all agents that switch between these ğœ–-greedy policies.\nNow, the components (ğ‘’, ğ‘£, Î›, Î›B) have been defined, we can see that this is indeed an instance\nof CRL: None of the base agents can be optimal, as the moment that the environment switches its\nunderlying MDP, we know that any previously optimal policy will no longer be optimal in the next\nMDP following the switch. Therefore, any agent that converges (in that it reaches the basis Î›B)\ncannot be optimal either for the same reason. We conclude that all optimal agents in Î› are continual\nlearning agents relative to the basis Î›B.\nWe present a visual of this domain in Figure 2(a), and conduct a simple experiment contrasting the\nperformance of ğœ–-greedy continual Q-learning (blue) that uses a constant step-size parameter of\nğ›¼= 0.1, with a convergent Q-learning (green) that anneals its step size parameter over time to zero.\nBoth use ğœ–= 0.15, and we set the number of underlying MDPs to ğ‘›= 10. We present the average\nreward with 95% confidence intervals, averaged over 250 runs, in Figure 2(b). Since both variants\nof Q-learning can be viewed as searching over Î›B, the annealing variant that stops its search will\nunder-perform compared to the continual approach. These results support the unsurprising conclusion\nthat it is better to continue searching over the basis rather than converge in this setting.\nContinual Supervised Learning.\nSecond, we illustrate the power of our CRL definition to capture\ncontinual supervised learning. We adopt the problem setting studied by Mai et al. [35]. Let ğ’³denote a\nset of objects to be labeled, each belonging to one of ğ‘˜âˆˆN classes. The observation space ğ’ªconsists\nof pairs, ğ‘œğ‘¡= (ğ‘¥ğ‘¡, ğ‘¦ğ‘¡), where ğ‘¥ğ‘¡âˆˆğ’³and ğ‘¦ğ‘¡âˆˆğ’´. Here, each ğ‘¥ğ‘¡is an input object to be classified and\nğ‘¦ğ‘¡is the label for the previous input ğ‘¥ğ‘¡âˆ’1. Thus, ğ’ª= ğ’³Ã—ğ’´. We assume by convention that the initial\n<1\n<2\n<3\nF\nvVd0rfE=\">AQ1nicjVhb9s2Fa7W+dm6Pe+EWFOgCJZAUO4LBOiaXvbQS5olbdEoCyiZtlIokZR\nSTxOw16GPQ3Y0/Zr9kP2b3ZIybZEOUMRKLO+Xj48fDwHDJBGtFMOM5/16/9/4H3504+POJ59+9vnNlV\ntfvMxYzkNyGLKI8dcBzkhE3IoqIjI65QTHAcReRWc7ir9qzPCM8qSAzFNyXGMxwkd0RALEP0QnyQnK6vO\nhqN/qN1wq8aqVf32Tm7d/NcfsjCPSLCGfZkeuk4lhiLmgYkaJzG62vr6M9HJ7iMcnUR8fPM5KWgqMY8z\nFNdpwNjybHckxYTASfFnWMJAmY51gQG8VYTHg2ymwE8wiAdmwjHGdKrBtiUgqyaRxAIxozTktZNE4zkgNV\nNgQ7KR2xRNgoy4MRHdtolEdRCmPZKMx+ypkgMIQZxOeRyAT9PRnGwUB2AkYOxU4ALX6WpDIeWSjC70ETf\nLQqykYZmrohuxoxHFMYOYTNtw5oKdvjmU81LJh0QEPYpowpHptNCwl5Dy9EORC2LqlnKDh68rjerUzhJMh\neqbmo6WgfEBI+pSCVHPNOv6QjCBc9JcQqjkpJD7j+8X0rEHm7bndgsTk+Y8jWYo1+nZPcf2Np0WLmoCtz\nx70Le9Xq8FHNZG9cBWz+4D6LaBqnNzt+1e13a7/TYsqsP6ju0O3KWDjkhSYXrbdkumPQG4D/w0dPpcu/U\nLA+2beR2HXgMWhOv41zHA4yngP1NE1in6rlbNvI2e/Bok+UQCJW9ASBcVxl1W/bOJ1TM7YF/0OxhAhnHyXi\nO3FTstgbw2PRMZN1L25tqKn0b/bm8qVYLCv0C1k1EAqfuWUen0TKfAM5nkuTMNVvnFa3k5pcjqPHTXml\nnKks23ipiSK2PncO8pYV/EcDNpzxtMK1wVb3YH60zvrCR1PhBkVjYDTboR9A3Rbdq8+9SZXRcJTseZ5myp\nAfdjuIYtj2NvSJ2lWSP8Mc2jQiCUq4x5MU8jpEYpoTMXdJj4BIXTIkyEUAiJkgnyuZoU5Z+fIp8lIwOR9h\nSqMnlmuesLzRFK0g9zix8SEpJxBpPrqVQc1Uf3HgBGp/JYqg/DyuO6/rHWq0J32pSKqOpdhA0e8VhceQ\nel/0AJ1d024cLBsAQAaZGxQwQyxYgeDjXyoemkmNQLnxpaCOljchoqfKZMhvIZ6bJN6X8jSl/Ucpfm\nPL9Ur5fmC4fQngVR96x7PgBgYK7qC9HnEshIwnhO+4KdTEAMrLuJCNO7orHLc8c8yKDxEujEUNHDFCEo\nJhCxVR4cC3Vl1v72LwIUCraokQmDURQ1TdB4QOB9wopbuearKOeNr0ofyH1PYE/D2bdV6FxBfzIDQMlaGg\np3hLGxIBFGx7+0/KCnoIo6IEtHhHBwIbzwJ7NyL70Qwhw8UT70TCpQZU1MCOMkXpOzRiEPylZR10uagE/\nDPBMshjOIASLJGeUsUecmucBAfHYQuJGTBWsf7DVNqdD1Deu6G3A1xR10GSdYc17IXcZBP6eXkZLw67AS\n+FMYrvEmal/DJiykdyn8DR8PQyUgpyBU7KsEFJL0CbkRZDltnjLGWZDurL6EFySwtZA15GUgOvwFLhTJq6\n7xKepVyd4/AZQbg1MlRNO3rUWcRDgfcnINoAnu4DGSajGd5AOA5rH6hxcs1XtDdWVCeQW+yr08w5Vflc\nnSY7lAGI1wEk7hdHyBsE4dSExglGzDSEMA0Dn43Wmoq9KQnjQwqm4ISqZFJTNTqvOuKVTptiZbfEJhUoJa\np0pi5j51FPi6ZyY/8NWSDAd1twyWrBE5cCfK41THifxu0W5gQpa8JaFQPoUNOW8bmNlGNfsHBFhVdt39a\nuhIRc41ifxh1WjoY0IuLOQT/SroUlrQd6O+BmKl1u1sWVnuhEOIZE+Us9m7lSKtaWa0tzacnvV+5KMO1xU\nJFmrTk37C9BaEzUvPfsgM2O4vaPeia0fLaJm/Dpix/vhKQBC6UEOny+YFbGLoMdhrJZNyST3AKtzdTPM\nIxjaYyTaO3dRVc5yEcqjtjRfALn2PQDKnWGfY4NsxSpZe67X5Ynfgwu+a1/t246W34fY2nBfd1XuHv5VX\n/xvWV9Y31h3LtfrWPet7a86tEJrbP1p/W39s/J65deV31f+KHXr1X/LvjSavxW/vofB2UZ1Q=</late\nxit><=\n(a) Switching MDP Visual\nQ-learning  (continual)\nQ-learning  (anneal)\n(b) Switching MDP Results\nFigure 2: A visual of a grid world instance of the switching MDPs problem (left) [33], and results from\nan experiment contrasting continual learning and convergent Q-learning (right). The environment\npictured contains ğ‘›distinct MDPs. Each underlying MDP shares the same state space and action\nspace, but varies in transition and reward functions, as indicated by the changing walls and rewarding\nlocations (stars, circles, and fire). The results pictured on the right contrast continual Q-learning (with\nğ›¼= 0.1) with traditional Q-learning that anneals its step-size parameter to zero over time.\n7\nlabel ğ‘¦0 is irrelevant and can be ignored. The agent will observe a sequence of object-label pairs,\n(ğ‘¥0, ğ‘¦0), (ğ‘¥1, ğ‘¦1), . . ., and the action space is a choice of label, ğ’œ= {ğ‘1, . . . , ğ‘ğ‘˜} where |ğ’´| = ğ‘˜.\nThe reward for each history â„ğ‘¡is +1 if the agentâ€™s most recently predicted label is correct for the\nprevious input, and âˆ’1 otherwise:\nğ‘Ÿ(ğ‘ğ‘¡âˆ’1ğ‘œğ‘¡) = ğ‘Ÿ(ğ‘ğ‘¡âˆ’1ğ‘¦ğ‘¡) =\n\u001a\n+1\nğ‘ğ‘¡âˆ’1 = ğ‘¦ğ‘¡,\nâˆ’1\nğ‘ğ‘¡âˆ’1 = otherwise.\n(4.1)\nConcretely, the continual learning setting studied by Mai et al. [35] supposes the learner will receive\nsamples from a sequence of probability distributions, ğ‘‘0, ğ‘‘1, . . ., each supported over ğ’³Ã— ğ’´. The\n(ğ‘¥, ğ‘¦) âˆˆğ’³Ã— ğ’´pairs experienced by the learner are determined by the sequence of distributions.\nWe capture this distributional shift in an environment ğ‘’that shifts its probability distribution over ğ’ª\ndepending on the history to match the sequence, ğ‘‘0, ğ‘‘1, . . ..\nNow, is this an instance of CRL? To answer this question precisely, we need to select a (Î›, Î›B) pair.\nWe adopt the basis Î›B = {ğœ†B : ğ‘¥â†¦â†’ğ‘¦ğ‘–, âˆ€ğ‘¦ğ‘–âˆˆğ’´} that contains each classifier that maps each object\nto each possible label. By the universal set of learning rules Î£, this basis generates the set of all\nagents that search over classifiers. Now, our definition says the above is an instance of CRL just when\nevery optimal agent never stops switching between classifiers, rather than stop their search on a fixed\nclassifier. Consequently, if there is an optimal classifier in Î›B, then this will not be an instance of\nCRL. If, however, the environment imposes enough distributional shift (changing labels, adding mass\nto new elements, and so on), then the only optimal agents will be those that always switch among the\nbase classifiers, in which case the setting is an instance of CRL.\n4.3\nRelationship to Other Views on Continual Learning\nThe spirit of continual learning has been an important part of machine learning research for decades,\noften appearing under the name of â€œlifelong learningâ€ [63, 62, 53, 55, 51, 3, 4], â€œnever-ending\nlearningâ€ [39, 43] with close ties to transfer-learning [61, 60], meta-learning [52, 17], as well as\nonline learning and non-stationarity [5, 40, 13, 6, 31]. In a similar vein, the phrase â€œcontinuing tasksâ€\nis used in the classic RL textbook [59] to refer explicitly to cases when the interaction between\nagent and environment is not subdivided into episodes. Continual reinforcement learning was first\nposed in the thesis by Ring [46]. In later work [47, 48], Ring proposes a formal definition of the\ncontinual reinforcement learning problemâ€”The emphasis of Ringâ€™s proposal is on the generality of\nthe environment: rather than assume that agents of interest will interact with an MDP, Ring suggests\nstudying the unconstrained case in which an agent must maximize performance while only receiving\na stream of observations as input. The environment or reward function, in this sense, may change\nover time or may be arbitrarily complex. This proposal is similar in spirit to general RL, studied\nby Hutter [24], Lattimore [28], Leike [29], and others [12, 37, 36] in which an agent interacts with\nan unconstrained environment. General RL inspires many aspects of our conception of CRL; for\ninstance, our emphasis on history-dependence rather than environment-state comes directly from\ngeneral RL. More recently, Khetarpal et al. [25] provide a comprehensive survey of the continual\nreinforcement learning literature. We encourage readers to explore this survey for a detailed history of\nthe subject.2 In the survey, Khetarpal et al. propose a definition of the CRL problem that emphasizes\nthe non-stationarity of the underlying process. In particular, in Khetarpal et al.â€™s definition, an agent\ninteracts with a POMDP in which each of the individual components of the POMDPâ€”such as the\nstate space or reward functionâ€”are allowed to vary with time. We note that, as the environment\nmodel we study (Definition 2.3) is a function of history, it can capture time-indexed non-stationarity.\nIn this sense, the same generality proposed by Khetarpal et al. and Ring is embraced and retained\nby our definition, but we add further precision to what is meant by continual learning by centering\naround a mathematical definition of continual learning agents (Definition 4.1).\n4.4\nProperties of CRL\nOur formalism is intended to be a jumping off point for new lines of thinking around agents and\ncontinual learning. We defer much of our analysis and proofs to the appendix, and here focus on\nhighlighting necessary properties of CRL.\n2For other surveys, see the recent survey on continual robot learning by Lesort et al. [30], a survey on\ncontinual learning with neural networks by Parisi et al. [42], a survey on transfer learning in RL by Taylor and\nStone [60], and a survey on continual image classification by Mai et al. [35].\n8\nTheorem 4.1. Every instance of CRL (ğ‘’, ğ‘£, Î›, Î›B) necessarily satisfies the following properties:\n1. If Î› â‰ Î›B âˆªÎ›âˆ—, then there exists a Î›â€²\nB such that (1) Î›â€²\nB âŠ¢ğ‘’Î›, and (2) (ğ‘’, ğ‘£, Î›, Î›â€²\nB) is not an\ninstance of CRL.\n2. No element of Î›B is optimal: Î›B âˆ©Î›âˆ—= âˆ….\n3. If |Î›| is finite, there exists an agent set, Î›â—¦, such that |Î›â—¦| < |Î›| and Î›â—¦âŠ¢ğ‘’Î›.\n4. If |Î›| is infinite, there exists an agent set, Î›â—¦, such that Î›â—¦âŠ‚Î› and Î›â—¦âŠ¢ğ‘’Î›.\nThis theorem tells us several things. The first point of the theorem has peculiar implications. We see\nthat as we change a single element (the basis Î›B) of the tuple (ğ‘’, ğ‘£, Î›B, Î›), the resulting problem can\nchange from CRL to not CRL. By similar reasoning, an agent that is said to be a continual learning\nagent according to Definition 4.1 may not be a continual learner with respect to some other basis.\nWe discuss this point further in the next paragraph. Point (2.) notes that no optimal strategy exists\nwithin the basisâ€”instead, to be optimal, an agent must switch between basis elements indefinitely. As\ndiscussed previously, this fact encourages a departure in how we think about the RL problem: rather\nthan focus on agents that can identify a single, fixed solution to a problem, CRL instead emphasizes\ndesigning agents that are effective at updating their behavior indefinitely. Points (3.) and (4.) show\nthat Î› cannot be minimal. That is, there are necessarily some redundancies in the design space of\nthe agents in CRLâ€”this is expected, since we are always focusing on agents that search over the\nsame agent basis. Lastly, it is worth calling attention to the fact that in the definition of CRL, we\nassume Î›B âŠ‚Î›â€”this suggests that in CRL, the agent basis is necessarily limited in some way.\nConsequently, the design space of agents Î› are also limited in terms of what agents they can represent\nat any particular point in time. This limitation may come about due to a computational or memory\nbudget, or by making use of a constrained set of learning rules. This suggests a deep connection\nbetween bounded agents and the nature of continual learning, as explored further by Kumar et al. [27].\nWhile these four points give an initial character of the CRL problem, we note that further exploration\nof the properties of CRL is an important direction for future work.\nCanonical Agent Bases.\nIt is worth pausing and reflecting on the concept of an agent basis. As\npresented, the basis is an arbitrary choice of a set of agentsâ€”consequently, point (1.) of Theorem 4.1\nmay stand out as peculiar. From this perspective, it is reasonable to ask if the fact that our definition\nof CRL is basis-dependant renders it vacuous. We argue that this is not the case for two reasons. First,\nwe conjecture that any definition of continual learning that involves concepts like â€œlearningâ€ and\nâ€œconvergenceâ€ will have to sit on top of some reference object whose choice is arbitrary. Second, and\nmore important, even though the mathematical construction allows for an easy change of basis, in\npractice the choice of basis is constrained by considerations like the availability of computational\nresources. It is often the case that the domain or problem of interest provides obvious choices of\nbases, or imposes constraints that force us as designers to restrict attention to a space of plausible\nbases or learning rules. For example, as discussed earlier, a choice of neural network architecture\nmight comprise a basisâ€”any assignment of weights is an element of the basis, and the learning rule ğœ\nis a mechanism for updating the active element of the basis (the parameters) in light of experience. In\nthis case, the number of parameters of the network is constrained by what we can actually build, and\nthe learning rule needs to be suitably efficient and well-behaved. We might again think of the learning\nrule ğœas gradient descent, rather than a rule that can search through the basis in an unconstrained\nway. In this sense, the basis is not arbitrary. We as designers choose a class of functions to act as the\nrelevant representations of behavior, often limited by resource constraints on memory or compute.\nThen, we use specific learning rules that have been carefully designed to react to experience in\na desirable wayâ€”for instance, stochastic gradient descent updates the current choice of basis in\nthe direction that would most improve performance. For these reasons, the choice of basis is not\narbitrary, but instead reflects the ingredients involved in the design of agents as well as the constraints\nnecessarily imposed by the environment.\n4.5\nProperties of Generates and Reaches\nLastly, we summarize some of the basic properties of generates and reaches. Further analysis of\ngenerates, reaches, and their variations is provided in Appendix C.\nTheorem 4.2. The following properties hold of the generates operator:\n9\n1. Generates is transitive: For any triple (Î›1, Î›2, Î›3) and ğ‘’âˆˆâ„°, if Î›1 âŠ¢ğ‘’Î›2 and Î›2 âŠ¢ğ‘’Î›3,\nthen Î›1 âŠ¢ğ‘’Î›3.\n2. Generates is not commutative: there exists a pair (Î›1, Î›2) and ğ‘’âˆˆâ„°such that Î›1 âŠ¢ğ‘’Î›2,\nbut Â¬(Î›2 âŠ¢ğ‘’Î›1).\n3. For all Î› and pair of agent bases (Î›1\nB, Î›2\nB) such that Î›1\nB âŠ†Î›2\nB, if Î›1\nB âŠ¢ğ‘’Î›, then Î›2\nB âŠ¢ğ‘’Î›.\n4. For all Î› and ğ‘’âˆˆâ„°, Î› âŠ¢ğ‘’Î›.\n5. The decision problem, Given (ğ‘’, Î›B, Î›), output True iff Î›B âŠ¢ğ‘’Î›, is undecidable.\nThe fact that generates is transitive suggests that the basic tools of an agent setâ€”paired with a set\nof learning rulesâ€”might be likened to an algebraic structure. We can draw a symmetry between an\nagent basis and the basis of a vector space: A vector space is comprised of all linear combinations\nof the basis, whereas Î› is comprised of all valid switches (according to the learning rules) between\nthe base agents. However, the fact that generates is not commutative (by point 2.) raises a natural\nquestion: are there choices of learning rules under which generates is commutative? We suggest that\na useful direction for future work can further explore an algebraic perspective on agents.\nWe find many similar properties hold of reaches.\nTheorem 4.3. The following properties hold of the reaches operator:\n1. â‡\nğ‘’and â‡\n/\nğ‘’are not transitive.\n2.â€œSometimes reachesâ€ is not commutative: there exists a pair (Î›1, Î›2) and ğ‘’âˆˆâ„°such that\nâˆ€ğœ†1âˆˆÎ›1 ğœ†1 â‡\nğ‘’Î›2, but âˆƒğœ†2âˆˆÎ›2 ğœ†2 â‡\n/\nğ‘’Î›1.\n3. For all pairs (Î›, ğ‘’), if ğœ†âˆˆÎ›, then ğœ†â‡\nğ‘’Î›.\n4. Every agent satisfies ğœ†â‡\nğ‘’Î› in every environment.\n5. The decision problem, Given (ğ‘’, ğœ†, Î›), output True iff ğœ†â‡\nğ‘’Î›, is undecidable.\nMany of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding\nwhether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate that\nthe majority of decision problems related to determining properties of arbitrary agent sets interacting\nwith unconstrained environments will be undecidable, though it is still worth making these arguments\ncarefully. Moreover, there may be interesting special cases in which these decision problems are\ndecidable (and perhaps, efficiently so). We suggest that identifying these special cases and fleshing\nout their corresponding efficient algorithms is an interesting direction for future work.\n5\nDiscussion\nIn this paper, we carefully develop a simple mathematical definition of the continual RL problem.\nWe take this problem to be of central importance to AI as a field, and hope that these tools and\nperspectives can serve as an opportunity to think about CRL and its related artifacts more carefully.\nOur proposal is framed around two new insights about agents: (i) every agent can be understood as\nthough it were searching over an agent basis (Theorem 3.1), and (ii) every agent, in the limit, will\neither sometimes or never stop this search (Remark 3.2). These two insights are formalized through\nthe generates and reaches operators, which provide a rich toolkit for understanding agents in a new\nwayâ€”for example, we find straightforward definitions of a continual learning agent (Definition 4.1)\nand learning rules (Definition 3.2). We anticipate that further study of these operators and different\nfamilies of learning rules can directly inform the design of new learning algorithms; for instance,\nwe might characterize the family of continual learning rules that are guaranteed to yield continual\nlearning agents, and use this to guide the design of principled continual learning agents (in the spirit\nof continual backprop by Dohare et al. [14]). In future work, we intend to further explore connections\nbetween our formalism of continual learning and some of the phenomena at the heart of recent\nempirical continual learning studies, such as plasticity loss [34, 1, 15], in-context learning [8], and\ncatastrophic forgetting [38, 18, 21, 26]. More generally, we hope that our definitions, analysis, and\nperspectives can help the community to think about continual reinforcement learning in a new light.\n10\nAcknowledgements\nThe authors are grateful to Michael Bowling, Clare Lyle, Razvan Pascanu, and Georgios Piliouras\nfor comments on a draft of the paper, as well as the anonymous NeurIPS reviewers that provided\nvaluable feedback on the paper. The authors would further like to thank all of the 2023 Barbados RL\nWorkshop participants and Elliot Catt, Will Dabney, Sebastian Flennerhag, AndrÂ´as GyÂ¨orgy, Steven\nHansen, Anna Harutyunyan, Mark Ho, Joe Marino, Joseph Modayil, RÂ´emi Munos, Evgenii Nikishin,\nBrendan Oâ€™Donoghue, Matt Overlan, Mark Rowland, Tom Schaul, Yannick Shroecker, Rich Sutton,\nYunhao Tang, Shantanu Thakoor, and Zheng Wen for inspirational conversations.\nReferences\n[1] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of\nplasticity in continual deep reinforcement learning. arXiv preprint arXiv:2303.07507, 2023.\n[2] David Abel, Yuu Jinnai, Yue Guo, George Konidaris, and Michael L. Littman. Policy and value\ntransfer in lifelong reinforcement learning. In Proceedings of the International Conference on\nMachine Learning, 2018.\n[3] Haitham Bou Ammar, Rasul Tutunov, and Eric Eaton. Safe policy search for lifelong reinforce-\nment learning with sublinear regret. In Proceedings of the International Conference on Machine\nLearning, 2015.\n[4] Megan M Baker, Alexander New, Mario Aguilar-Simon, Ziad Al-Halah, SÂ´ebastien MR Arnold,\nEse Ben-Iwhiwhu, Andrew P Brna, Ethan Brooks, Ryan C Brown, Zachary Daniels, et al. A\ndomain-agnostic approach for characterization of lifelong learning systems. Neural Networks,\n160:274â€“296, 2023.\n[5] Peter L Bartlett. Learning with a slowly changing distribution. In Proceedings of the Annual\nWorkshop on Computational Learning Theory, 1992.\n[6] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with\nnon-stationary rewards. Advances in Neural Information Processing Systems, 2014.\n[7] Michael Bowling, John D. Martin, David Abel, and Will Dabney. Settling the reward hypothesis.\nIn Proceedings of the International Conference on Machine Learning, 2023.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in Neural Information Processing Systems, 2020.\n[9] Emma Brunskill and Lihong Li. PAC-inspired option discovery in lifelong reinforcement\nlearning. In Proceedings of the International Conference on Machine Learning, 2014.\n[10] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom\nMitchell. Toward an architecture for never-ending language learning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, 2010.\n[11] Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. Acting optimally in\npartially observable stochastic domains. In Proceedings of the AAAI Conference on Artificiall\nIntelligence, 1994.\n[12] Michael K Cohen, Elliot Catt, and Marcus Hutter. A strongly asymptotically optimal agent in\ngeneral environments. arXiv preprint arXiv:1903.01021, 2019.\n[13] Travis Dick, AndrÂ´as GyÂ¨orgy, and Csaba Szepesvari. Online learning in Markov decision\nprocesses with changing cost sequences. In Procedings of the International Conference on\nMachine Learning, 2014.\n[14] Shibhansh Dohare, Richard S Sutton, and A Rupam Mahmood. Continual backprop: Stochastic\ngradient descent with persistent randomness. arXiv preprint arXiv:2108.06325, 2021.\n11\n[15] Shibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, and A Rupam\nMahmood. Loss of plasticity in deep continual learning. arXiv preprint arXiv:2306.13812,\n2023.\n[16] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple agent, complex environment:\nEfficient reinforcement learning with agent states. Journal of Machine Learning Research, 23\n(255):1â€“54, 2022.\n[17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-\ntion of deep networks. In Proceedings of the International Conference on Machine Learning,\n2017.\n[18] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive\nsciences, 3(4):128â€“135, 1999.\n[19] Milton Friedman. Essays in positive economics. University of Chicago press, 1953.\n[20] Haotian Fu, Shangqun Yu, Michael Littman, and George Konidaris. Model-based lifelong\nreinforcement learning with Bayesian exploration. Advances in Neural Information Processing\nSystems, 2022.\n[21] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical\ninvestigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint\narXiv:1312.6211, 2013.\n[22] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Con-\ntinual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028â€“1040,\n2020.\n[23] Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity.\narXiv preprint cs/0004001, 2000.\n[24] Marcus Hutter. Universal artificial intelligence: Sequential decisions based on algorithmic\nprobability. Springer Science & Business Media, 2004.\n[25] Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual rein-\nforcement learning: A review and perspectives. Journal of Artificial Intelligence Research, 75:\n1401â€“1476, 2022.\n[26] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks. Proceedings of the National Academy\nof Sciences, 114(13):3521â€“3526, 2017.\n[27] Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, and\nBenjamin Van Roy. Continual learning as computationally constrained reinforcement learning.\narXiv preprint arXiv:2307.04345, 2023.\n[28] Tor Lattimore. Theory of general reinforcement learning. PhD thesis, The Australian National\nUniversity, 2014.\n[29] Jan Leike. Nonparametric general reinforcement learning. PhD thesis, The Australian National\nUniversity, 2016.\n[30] TimothÂ´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and\nNatalia DÂ´Ä±az-RodrÂ´Ä±guez. Continual learning for robotics: Definition, framework, learning\nstrategies, opportunities and challenges. Information fusion, 58:52â€“68, 2020.\n[31] Yueyang Liu, Benjamin Van Roy, and Kuang Xu. A definition of non-stationary bandits. arXiv\npreprint arXiv:2302.12202, 2023.\n[32] Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and\nZheng Wen. Reinforcement learning, bit by bit. Foundations and Trends in Machine Learning,\n16(6):733â€“865, 2023. ISSN 1935-8237.\n12\n[33] Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and\nSatinder Singh. Meta-gradients in non-stationary environments. In Proceedings of the Confer-\nence on Lifelong Learning Agents, 2022.\n[34] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will\nDabney. Understanding plasticity in neural networks. In Proceedings of the International\nConference on Machine Learning, 2023.\n[35] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online\ncontinual learning in image classification: An empirical survey. Neurocomputing, 469:28â€“51,\n2022.\n[36] Sultan J Majeed. Abstractions of general reinforcement Learning. PhD thesis, The Australian\nNational University, 2021.\n[37] Sultan Javed Majeed and Marcus Hutter. Performance guarantees for homomorphisms beyond\nMarkov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence,\n2019.\n[38] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:\nThe sequential learning problem. In Psychology of learning and motivation, volume 24, pages\n109â€“165. Elsevier, 1989.\n[39] Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Bet-\nteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, et al. Never-ending\nlearning. Communications of the ACM, 61(5):103â€“115, 2018.\n[40] Claire Monteleoni and Tommi Jaakkola. Online learning of non-stationary sequences. Advances\nin Neural Information Processing Systems, 16, 2003.\n[41] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual\nlearning. 2018.\n[42] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual\nlifelong learning with neural networks: A review. Neural networks, 113:54â€“71, 2019.\n[43] Emmanouil Antonios Platanios, Abulhair Saparov, and Tom Mitchell. Jelly bean world: A\ntestbed for never-ending learning. arXiv preprint arXiv:2002.06306, 2020.\n[44] Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.\nJohn Wiley & Sons, 2014.\n[45] Matthew Riemer, Sharath Chandra Raparthy, Ignacio Cases, Gopeshh Subbaraj, Maximilian\nPuelma Touzel, and Irina Rish. Continual learning in environments with polynomial mixing\ntimes. Advances in Neural Information Processing Systems, 2022.\n[46] Mark B Ring. Continual learning in reinforcement environments. PhD thesis, The University of\nTexas at Austin, 1994.\n[47] Mark B Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77â€“104,\n1997.\n[48] Mark B Ring. Toward a formal framework for continual learning. In NeurIPS Workshop on\nInductive Transfer, 2005.\n[49] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Ex-\nperience replay for continual learning. Advances in Neural Information Processing Systems,\n2019.\n[50] Stuart J Russell and Devika Subramanian. Provably bounded-optimal agents. Journal of\nArtificial Intelligence Research, 2:575â€“609, 1994.\n[51] Paul Ruvolo and Eric Eaton. ELLA: An efficient lifelong learning algorithm. In Proceedings of\nthe International Conference on Machine Learning, 2013.\n13\n[52] Tom Schaul and JÂ¨urgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010.\n[53] JÂ¨urgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph. Reinforcement learning with\nself-modifying policies. In Learning to Learn, pages 293â€“309. Springer, 1998.\n[54] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework\nfor continual learning. In Proceedings of the International Conference on Machine Learning,\n2018.\n[55] Daniel L Silver. Machine lifelong learning: Challenges and benefits for artificial general\nintelligence. In Proceedings of the Conference on Artificial General Intelligence, 2011.\n[56] Richard S Sutton. Introduction: The challenge of reinforcement learning. In Reinforcement\nLearning, pages 1â€“3. Springer, 1992.\n[57] Richard S Sutton. The reward hypothesis, 2004. URL http://incompleteideas.net/rlai.\ncs.ualberta.ca/RLAI/rewardhypothesis.html.\n[58] Richard S Sutton. The quest for a common model of the intelligent decision maker. arXiv\npreprint arXiv:2202.13252, 2022.\n[59] Richard S Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,\n2018.\n[60] Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A\nsurvey. Journal of Machine Learning Research, 10(Jul):1633â€“1685, 2009.\n[61] Sebastian Thrun. Is learning the n-th thing any easier than learning the first? Advances in\nNeural Information Processing Systems, 1995.\n[62] Sebastian Thrun. Lifelong learning algorithms. Learning to Learn, 8:181â€“209, 1998.\n[63] Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. Robotics and autonomous\nsystems, 15(1-2):25â€“46, 1995.\n[64] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning:\na hierarchical Bayesian approach. In Proceedings of the International Conference on Machine\nlearning, 2007.\n14\nA\nNotation\nWe first provide a table summarizing all relevant notation.\nNotation\nMeaning\nDefinition\nğ’œ\nActions\nğ’ª\nObservations\nâ„‹ğ‘¡\nLength ğ‘¡histories\nâ„‹ğ‘¡= (ğ’œÃ— ğ’ª)ğ‘¡\nâ„‹\nAll histories\nâ„‹= Ãâˆ\nğ‘¡=0 â„‹ğ‘¡\nâ„\nA history\nâ„âˆˆâ„‹\nâ„â„â€²\nHistory concatenation\nâ„ğ‘¡\nLength ğ‘¡history\nâ„ğ‘¡âˆˆâ„‹ğ‘¡\nÂ¯\nâ„‹= â„‹ğœ†,ğ‘’\nRealizable histories\nÂ¯\nâ„‹= Ãâˆ\nğ‘¡=0\n\b\nâ„ğ‘¡âˆˆâ„‹ğ‘¡: Ãğ‘¡âˆ’1\nğ‘˜=0 ğ‘’(ğ‘œğ‘˜| â„ğ‘˜, ğ‘ğ‘˜)ğœ†(ğ‘ğ‘˜| â„ğ‘˜) > 0\t\nÂ¯\nâ„‹â„= â„‹ğœ†,ğ‘’\nâ„\nRealizable history suffixes\nÂ¯\nâ„‹â„= {â„â€² âˆˆâ„‹: â„â„â€² âˆˆâ„‹ğœ†,ğ‘’}\nğ‘’\nEnvironment\nğ‘’: â„‹Ã— ğ’œâ†’Î”(ğ’œ)\nâ„°\nSet of all environments\nğœ†\nAgent\nğœ†: â„‹â†’Î”(ğ’œ)\nÎ›\nSet of all agents\nÎ›\nSet of agents\nÎ› âŠ†Î›\nÎ›B\nAgent basis\nÎ›B âŠ‚Î›\nğ‘Ÿ\nReward function\nğ‘Ÿ: ğ’œÃ— ğ’ªâ†’R\nğ‘£\nPerformance\nğ‘£: â„‹Ã— Î› Ã— â„°â†’[vmin, vmax]\nğœ\nLearning rule\nğœ: â„‹â†’Î›B\nÎ£\nSet of all learning rules\nÎ£\nSet of learning rules\nÎ£ âŠ†Î£\nÎ›B âŠ¢ğ‘’Î£ Î›\nÎ£-generates\nâˆ€Î›âˆƒğœâˆˆÎ£âˆ€â„âˆˆÂ¯\nâ„‹ğœ†(â„) = ğœ(â„)(â„)\nÎ›B âŠ¢ğ‘’Î›\nGenerates\nâˆƒÎ£âŠ†Î£ Î›B âŠ¢ğ‘’Î£ Î›\nÎ›B |=Î£ Î›\nUniversally Î£-generates\nâˆ€Î›âˆƒğœâˆˆÎ£âˆ€â„âˆˆâ„‹ğœ†(â„) = ğœ(â„)(â„)\nÎ›B |= Î›\nUniversally generates\nâˆƒÎ£âŠ†Î£ Î›B |=Î£ Î›\nğœ†â‡\nğ‘’Î›B\nSometimes reaches\nâˆƒâ„âˆˆÂ¯\nâ„‹âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â€²) = ğœ†B(â„â„â€²)\nğœ†â‡\n/\nğ‘’Î›B\nNever reaches\nÂ¬(ğœ†â‡\nğ‘’Î›B)\nğœ†â–¡â‡\nğ‘’Î›B\nAlways reaches\nâˆ€â„âˆˆÂ¯\nâ„‹âˆƒğ‘¡âˆˆN0âˆ€â„â—¦âˆˆÂ¯\nâ„‹ğ‘¡:âˆ\nâ„âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â—¦â„â€²) = ğœ†B(â„â„â—¦â„â€²)\nTable 1: A summary of notation.\nB\nProofs of Presented Results\nWe next provide proofs of each result from the paper. Our proofs make use of some extra notation:\nwe use â‡’as logical implication, and we use ğ’«(ğ’³) to denote the power set of any set ğ’³. Lastly, we\nuse âˆ€ğ’œâŠ†ğ’³and âˆƒğ’œâŠ†ğ’³as shorthand for âˆ€ğ’œâˆˆğ’«(ğ’³) and âˆƒğ’œâˆˆğ’«(ğ’³) respectively.\n15\nB.1\nSection 3 Proofs\nOur first result is from Section 3 of the paper.\nTheorem 3.1. For any pair (ğœ†, ğ‘’), there exists infinitely many choices of a basis, Î›B, such that both\n(1) ğœ†âˆ‰Î›B, and (2) Î›B âŠ¢ğ‘’{ğœ†}.\nProof of Theorem 3.1.\nChoose a fixed but arbitrary pair (ğœ†, ğ‘’). Then, enumerate the realizable histories, â„‹ğœ†,ğ‘’, and\nlet â„1 denote the first element of this enumeration, â„2 the second, and so on.\nThen, we design a constructive procedure for a basis that, when repeatedly applied, induces\nan infinite enumeration of bases that satisfy the desired two properties. This constructive\nprocedure for the ğ‘˜-th basis will contain ğ‘˜+ 1 agents, where each agent is distinct from ğœ†,\nbut will produces the same action as the agent every ğ‘˜+ 1 elements of the history sequence,\nâ„1, â„2, . . ..\nFor the first (ğ‘˜= 1) basis, we construct two agents. The first, ğœ†1\nB, chooses the same action\ndistribution as ğœ†on each even numbered history: ğœ†1\nB(â„ğ‘–) = ğœ†(â„ğ‘–). Then, this agent will choose\na different action distribution on the odd length histories: ğœ†1\nB(â„ğ‘–+1) â‰ ğœ†(â„ğ‘–+1), for ğ‘–any even\nnatural number. The second agent, ğœ†2\nB will do the opposite to ğœ†1\nB: on each odd numbered\nhistory â„ğ‘–+1, ğœ†2\nB(â„ğ‘–+1) â‰ ğœ†(â„ğ‘–+1), but on every even numbered history, ğœ†2\nB(â„ğ‘–) = ğœ†(â„ğ‘–).\nObserve first that by construction, ğœ†â‰ ğœ†1\nB, and ğœ†â‰ ğœ†2\nB, since there exist histories where they\nchoose different action distributions. Next, observe that the basis, Î›B = {ğœ†1\nB, ğœ†2\nB}, generates\n{ğœ†} in ğ‘’through the following set of learning rules, Î£: given any realizable history, â„âˆˆâ„‹ğœ†,ğ‘’,\ncheck whether the history has an even or odd numbered index in the enumeration. If odd,\nchoose ğœ†1\nB, and if even, choose ğœ†2\nB.\nMore generally, this procedure can be applied for any ğ‘˜:\nÎ›ğ‘˜\nB = {ğœ†1\nB, . . . , ğœ†ğ‘˜+1\nB\n},\nğœ†ğ‘–\nB(â„) =\n\u001a\nğœ†(â„)\n[â„] == ğ‘–,\nâ‰ ğœ†(â„)\notherwise,\n(B.1)\nwhere we use the notation [â„] == ğ‘–to express the logical predicate asserting that the modulos\nof the index of â„in the enumeration â„1, â„2, . . . is equal to ğ‘–.\nFurther, â‰ ğœ†(â„) simply refers to any choice of action distribution that is unequal to ğœ†(â„).\nThus, for all natural numbers ğ‘˜â‰¥2, we can construct a new basis consisting of ğ‘˜base agents\nthat generates ğœ†in ğ‘’, but does not contain the agent itself. This completes the argument.\nâ–¡\nB.2\nSection 4 Proofs\nWe next present the proofs of results from Section 4.\nB.2.1\nTheorem 4.1: Properties of CRL\nWe begin with Theorem 4.1 that establishes basic properties of CRL.\nTheorem 4.1. Every instance of CRL (ğ‘’, ğ‘£, Î›, Î›B) satisfies the following properties:\n1. If Î› â‰ Î›B âˆªÎ›âˆ—, there exists a Î›â€²\nB such that (1) Î›â€²\nB âŠ¢ğ‘’Î›, and (2) (ğ‘’, ğ‘£, Î›, Î›â€²\nB) is not an\ninstance of CRL.\n2. No element of Î›B is optimal: Î›B âˆ©Î›âˆ—= âˆ….\n3. If |Î›| is finite, there exists an agent set, Î›â—¦such that |Î›â—¦| < |Î›| and Î›â—¦âŠ¢ğ‘’Î›.\n4. If |Î›| is infinite, there exists an agent set, Î›â—¦such that Î›â—¦âŠ‚Î› and Î›â—¦âŠ¢ğ‘’Î›.\n16\nWe prove this result in the form of three lemmas, corresponding to each of the four points of the\ntheorem (with the third lemma, Lemma B.3, covering both points 3. and 4.). Some of the lemmas\nmake use of properties of generates and reaches that we establish later in Appendix C.\nLemma B.1. For all instances of CRL (ğ‘’, ğ‘£, Î›, Î›B), if Î› â‰ Î›B âˆªÎ›âˆ—, then there exists a choice Î›â€²\nB\nsuch that (1) Î›â€²\nB âŠ¢ğ‘’Î›, and (2) (ğ‘’, ğ‘£, Î›, Î›â€²\nB) is not an instance of CRL.\nProof of Lemma B.1.\nRecall that a tuple (ğ‘’, ğ‘£, Î›, Î›B) is CRL just when all of the optimal agents Î›âˆ—do not reach\nthe basis. Then, the result holds as a straightforward consequence of two facts. First, we can\nalways construct a new basis containing all of the optimal agents, Î›â—¦\nB = Î›B âˆªÎ›âˆ—. Notice\nthat Î›â—¦\nB still generates Î› by property three of Theorem 4.2. Further, since both Î›B and Î›âˆ—\nare each subsets of Î›, and by assumption Î› â‰ Î›B âˆªÎ›âˆ—(so there is at least one sub-optimal\nagent that is not in the basis), it follows that Î›â—¦\nB âŠ‚Î›. Second, by Proposition C.15, we know\nthat every element ğœ†â—¦\nB âˆˆÎ›â—¦\nB will always reach the basis, ğœ†â—¦\nB â–¡â‡\nğ‘’Î›â—¦\nB. Therefore, in the tuple\n(ğ‘’, ğ‘£, Î›, Î›â—¦\nB), each of the optimal agents will reach the basis, and therefore this is not an\ninstance of CRL.\nâ–¡\nLemma B.2. No element of Î›B is optimal: Î›B âˆ©Î›âˆ—= âˆ….\nProof of Lemma B.2.\nThe lemma follows as a combination of two facts.\nFirst, recall that, by definition of CRL, each optimal agent ğœ†âˆˆÎ›âˆ—satisfies ğœ†âˆ—â‡\n/\nğ‘’Î›B.\nSecond, note that by Lemma B.11, we know that each ğœ†B âˆˆÎ›B satisfies ğœ†B â‡\nğ‘’Î›B.\nTherefore, since sometimes reaches (â‡\nğ‘’) and never reaches (â‡\n/\nğ‘’) are negations of one another,\nwe conclude that no basis element can be optimal.\nâ–¡\nBefore stating the next lemma, we note that points (3.) and (4.) of Theorem 4.1 are simply expansions\nof the definition of a minimal agent set, which we define precisely in Definition C.4 and Definition C.5.\nLemma B.3. For any instance of CRL, Î› is not minimal.\nProof of Lemma B.3.\nWe first show that Î› cannot be minimal. To do so, we consider the cases where the rank\n(Definition C.3) of Î› is finite and infinite separately.\n(Finite Rank Î›.)\nIf rank(Î›) is finite and minimal, then it follows immediately that there is no agent set of\nsmaller rank that generates Î›. By consequence, since Î›B âŠ‚Î› and Î›B âŠ¢ğ‘’Î›, we conclude that\nÎ› cannot be minimal. âœ“\n(Infinite Rank Î›.)\nIf rank(Î›) is infinite and minimal, then there is no proper subset of Î› that uniformly generates\nÎ› by definition. By consequence, since Î›B âŠ‚Î› and Î›B âŠ¢ğ‘’Î›, we conclude that Î› cannot be\nminimal. âœ“\nThis completes the argument of both cases, and we conclude that for any instance of CRL, Î›\nis not minimal.\nâ–¡\n17\nB.2.2\nTheorem 4.2: Properties of Generates\nNext, we prove basic properties of generates.\nTheorem 4.2. The following properties hold of the generates operator:\n1. Generates is transitive: For any triple (Î›1, Î›2, Î›3) and ğ‘’âˆˆâ„°, if Î›1 âŠ¢ğ‘’Î›2 and Î›2 âŠ¢ğ‘’Î›3,\nthen Î›1 âŠ¢ğ‘’Î›3.\n2. Generates is not commutative: there exists a pair (Î›1, Î›2) and ğ‘’âˆˆâ„°such that Î›1 âŠ¢ğ‘’Î›2,\nbut Â¬(Î›2 âŠ¢ğ‘’Î›1).\n3. For all Î› and pair of agent bases (Î›1\nB, Î›2\nB) such that Î›1\nB âŠ†Î›2\nB, if Î›1\nB âŠ¢ğ‘’Î›, then Î›2\nB âŠ¢ğ‘’Î›.\n4. For all Î› and ğ‘’âˆˆâ„°, Î› âŠ¢ğ‘’Î›.\n5. The decision problem, Given (ğ‘’, Î›B, Î›), output True iff Î›B âŠ¢ğ‘’Î›, is undecidable.\nThe proof of this theorem is spread across the next five lemmas below.\nThe fact that generates is transitive suggests that the basic tools of an agent setâ€”paired with a set\nof learning rulesâ€”might be likened to an algebraic structure. We can draw a symmetry between an\nagent basis and the basis of a vector space: A vector space is comprised of all linear combinations\nof the basis, whereas Î› is comprised of all valid switches (according to the learning rules) between\nthe base agents. However, the fact that generates is not commutative (by point 2.) raises a natural\nquestion: are there choices of learning rules under which generates is commutative? An interesting\ndirection for future work is to explore this style of algebraic analysis on agents.\nLemma B.4. Generates is transitive: For any triple (Î›1, Î›2, Î›3) and ğ‘’âˆˆâ„°, if Î›1 âŠ¢ğ‘’Î›2 and Î›2 âŠ¢ğ‘’Î›3,\nthen Î›1 âŠ¢ğ‘’Î›3.\nProof of Lemma B.4.\nAssume Î›1 âŠ¢ğ‘’Î›2 and Î›2 âŠ¢ğ‘’Î›3. Then, by Proposition C.4 and the definition of the generates\noperator, we know that\nâˆ€ğœ†2âˆˆÎ›2âˆƒğœ1âˆˆÎ£1âˆ€â„âˆˆÂ¯\nâ„‹ğœ†2(â„) = ğœ1(â„)(â„),\n(B.2)\nâˆ€ğœ†3âˆˆÎ›3âˆƒğœ2âˆˆÎ£2âˆ€â„âˆˆÂ¯\nâ„‹ğœ†3(â„) = ğœ2(â„)(â„),\n(B.3)\nwhere Î£1 and Î£2 express the set of all learning rules over Î›1 and Î›2 respectively. By definition\nof a learning rule, ğœ, we rewrite the above as follows,\nâˆ€ğœ†2âˆˆÎ›2âˆ€â„âˆˆÂ¯\nâ„‹âˆƒğœ†1âˆˆÎ›1 ğœ†2(â„) = ğœ†1(â„),\n(B.4)\nâˆ€ğœ†3âˆˆÎ›3âˆ€â„âˆˆÂ¯\nâ„‹âˆƒğœ†2âˆˆÎ›2 ğœ†3(â„) = ğœ†2(â„).\n(B.5)\nThen, consider a fixed but arbitrary ğœ†3 âˆˆÎ›3. We construct a learning rule defined over Î›1\nas ğœ1 : â„‹â†’Î›1 that induces an equivalent agent as follows. For each realizable history,\nâ„âˆˆ\nÂ¯\nâ„‹, by Equation B.5 we know that there is an ğœ†2 such that ğœ†3(â„) = ğœ†2(â„), and by\nEquation B.4, there is an ğœ†1 such that ğœ†2(â„) = ğœ†1(â„). Then, set ğœ1 : â„â†¦â†’ğœ†1 such that\nğœ†1(â„) = ğœ†2(â„) = ğœ†3(â„)\nSince â„and ğœ†3 were chosen arbitrarily, we conclude that\nâˆ€ğœ†3âˆˆÎ›3âˆ€â„âˆˆÂ¯\nâ„‹âˆƒğœ†1âˆˆÎ›1 ğœ†3(â„) = ğœ†1(â„).\nBut, by the definition of Î£, this means there exists a learning rule such that\nâˆ€ğœ†3âˆˆÎ›3âˆƒğœ1âˆˆÎ£1âˆ€â„âˆˆÂ¯\nâ„‹ğœ†3(â„) = ğœ1(â„)(â„).\nThis is exactly the definition of Î£-generation, and by Proposition C.4, we conclude Î›1 âŠ¢ğ‘’\nÎ›3.\nâ–¡\n18\nLemma B.5. Generates is not commutative: there exists a pair (Î›1, Î›2) and ğ‘’âˆˆâ„°such that\nÎ›1 âŠ¢ğ‘’Î›2, but Â¬(Î›2 âŠ¢ğ‘’Î›1).\nProof of Lemma B.5.\nThe result follows from a simple counterexample: consider the pair\nÎ›1 = {ğœ†ğ‘–: â„â†¦â†’ğ‘1},\nÎ›2 = {ğœ†ğ‘–: â„â†¦â†’ğ‘1, ğœ†ğ‘—: â„â†¦â†’ğ‘2}.\nNote that since ğœ†ğ‘–is in both sets, and Î›1 is a singleton, we know that Î›2 âŠ¢ğ‘’Î›1 in any\nenvironment. But, by Proposition C.6, we know that Î›1 cannot generate Î›2.\nâ–¡\nLemma B.6. For all Î› and pair of agent bases (Î›1\nB, Î›2\nB) such that Î›1\nB âŠ†Î›2\nB, if Î›1\nB âŠ¢ğ‘’Î›, then\nÎ›2\nB âŠ¢ğ‘’Î›.\nProof of Lemma B.6.\nThe result follows as a natural consequence of the definition of generates. Recall that Î›1\nB âŠ¢ğ‘’Î›\njust when,\nâˆƒÎ£1âŠ†Î£ Î›1\nB âŠ¢ğ‘’\nÎ£1 Î›\n(B.6)\nâ‰¡âˆƒÎ£1âŠ†Î£ âˆƒğœ1âˆˆÎ£1âˆ€â„âˆˆÂ¯\nâ„‹ğœ†(â„) = ğœ†ğœ1(â„)\nB\n(â„),\n(B.7)\nwhere again ğœ†ğœ1(â„)\nB\nâˆˆÎ›1\nB is the base agent chosen by ğœ1(â„). We use superscripts Î£1 and ğœ1 to\nsignify that ğœ1 is defined relative to Î›1\nB, that is, ğœ1 : â„‹â†’Î›1\nB âˆˆÎ£1.\nBut, since Î›1\nB âŠ†Î›2\nB, we can define Î£2 = Î£1 and ensure that Î›2\nB âŠ¢ğ‘’\nÎ£2 Î›, since the agent basis\nÎ›1\nB was already sufficient to generate Î›. Therefore, we conclude that Î›2\nB âŠ¢ğ‘’Î›.\nâ–¡\nLemma B.7. For all Î› and ğ‘’âˆˆâ„°, Î› âŠ¢ğ‘’Î›.\nProof of Lemma B.7.\nThis is a direct consequence of Proposition C.18.\nâ–¡\nLemma B.8. The decision problem, AGENTSGENERATE, Given (ğ‘’, Î›B, Î›), output True iff Î›B âŠ¢ğ‘’Î›,\nis undecidable.\nProof of Lemma B.8.\nWe proceed as is typical of such results by reducing AGENTSGENERATE from the Halting\nProblem.\nIn particular, let ğ‘šbe a fixed but arbitrary Turing Machine, and ğ‘¤be a fixed but arbitrary\ninput to be given to machine ğ‘š. Then, HALT defines the decision problem that outputs True\niff ğ‘šhalts on input ğ‘¤.\nWe construct an oracle for AGENTSGENERATE that can decide HALT as follows. Let (ğ’œ, ğ’ª)\nbe an interface where the observation space is comprised of all configurations of machine ğ‘š.\nThen, we consider a deterministic environment ğ‘’that simply produces the next configuration\nof ğ‘šwhen run on input ğ‘¤, based on the current tape contents, the state of ğ‘š, and the location\nof the tape head. Note that all three of these elements are contained in a Turing Machineâ€™s\nconfiguration, and that a single configuration indicates whether the Turing Machine is in a\nhalting state or not. Now, let the action space ğ’œconsist of two actions, {ğ‘no-op, ğ‘halt}. On\nexecution of ğ‘no-op no-op, the environment moves to the next configuration. On execution of\nğ‘halt, the machine halts. That is, we restrict ourselves to the singleton agent set, Î›, containing\n19\nthe agent ğœ†â—¦that outputs ğ‘halt directly following the machine entering a halting configuration,\nand ğ‘no-op otherwise:\nğœ†â—¦: â„ğ‘ğ‘œâ†¦â†’\n\u001a\nğ‘halt\nğ‘œis a halting configiration,\nğ‘no-op\notherwise.\n,\nÎ› = {ğœ†â—¦}.\nUsing these ingredients, we take any instance of HALT, (ğ‘š, ğ‘¤), and consider the singleton\nagent basis: Î›1\nB = {ğ‘no-op}.\nWe make one query to our AGENTSGENERATE oracle, and ask: Î›1\nB âŠ¢ğ‘’Î›. If it is True, then\nthe histories realizable by (ğœ†â—¦, ğ‘’) pair ensure that the single agent in Î› never emits the ğ‘halt\naction, and thus, ğ‘šdoes not halt on ğ‘¤. If it is False, then there are realizable histories in ğ‘’in\nwhich ğ‘šhalts on ğ‘¤. We thus use the oracleâ€™s response directly to decide the given instance of\nHALT.\nâ–¡\nB.2.3\nTheorem 4.3: Properties of Reaches\nWe find many similar properties hold for reaches.\nTheorem 4.3. The following properties hold of the reaches operator:\n1. â‡\nğ‘’and â‡\n/\nğ‘’are not transitive.\n2. Sometimes reaches is not commutative: there exists a pair (Î›1, Î›2) and ğ‘’âˆˆâ„°such that\nâˆ€ğœ†1âˆˆÎ›1 ğœ†1 â‡\nğ‘’Î›2, but âˆƒğœ†2âˆˆÎ›2 ğœ†2 â‡\n/\nğ‘’Î›1.\n3. For all pairs (Î›, ğ‘’), if ğœ†âˆˆÎ›, then ğœ†â‡\nğ‘’Î›.\n4. Every agent satisfies ğœ†â‡\nğ‘’Î› in every environment.\n5. The decision problem, Given (ğ‘’, ğœ†, Î›), output True iff ğœ†â‡\nğ‘’Î›, is undecidable.\nAgain, we prove this result through five lemmas that correspond to each of the above properties.\nMany of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding\nwhether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate\nthat the majority of decision problems related to determining properties of arbitrary agent sets will\nbe undecidable, though it is still worth making these arguments carefully. Moreover, there may be\ninteresting special cases in which these decision problems are decidable (and perhaps, efficiently\nso). Identifying these special cases and their corresponding efficient algorithms is another interesting\ndirection for future work.\nLemma B.9. â‡\nğ‘’and â‡\n/\nğ‘’are not transitive.\nProof of Lemma B.9.\nWe construct two counterexamples, one for each of â€œsometimes reachesâ€ (â‡\nğ‘’) and â€œnever\nreachesâ€ (â‡\n/\nğ‘’).\nCounterexample: Sometimes Reaches.\nTo do so, we begin with a tuple (ğ‘’, Î›1, Î›2, Î›3)\nsuch that both\nâˆ€Î›1âˆˆÎ›1 ğœ†1 â‡\nğ‘’Î›1,\nâˆ€Î›2âˆˆÎ›2 ğœ†2 â‡\nğ‘’Î›2.\nWe will show that there is an agent, ğœ†\n1 âˆˆÎ›1, such that ğœ†\n1 â‡\n/\nğ‘’\nÎ›3, thus illustrating that\nsometimes reaches is not guaranteed to be transitive. The basic idea is that sometimes reaches\nonly requires an agent stop its search on one realizable history. So, ğœ†1 â‡\nğ‘’Î›2 might happen\non some history â„, but each ğœ†2 âˆˆÎ›2 might only reach Î›3 on an entirely different history. As\na result, reaching Î›2 is not enough to ensure the agent also reaches Î›3.\nIn more detail, the agent sets of the counterexample are as follows. Let ğ’œ= {ğ‘1, ğ‘2} and\nğ’ª= {ğ‘œ1, ğ‘œ2}. Let Î›2 be all agents that, after ten timesteps, always take ğ‘2. ğœ†\n1 is simple: it\n20\nalways takes ğ‘1, except on one realizable history, â„â—¦, (and all of the realizable successors\nof â„â—¦, â„‹ğœ†,ğ‘’\nâ„â—¦), where it switches to taking ğ‘2 after ten timesteps. Clearly ğœ†1 â‡\nğ‘’\nÎ›2, since\nafter ten timesteps, we know there will be some ğœ†2 such that ğœ†\n1(â„â—¦â„â€²) = ğœ†2(â„â—¦â„â€²) for all\nrealizable history suffixes â„â€². Now, by assumption, we know that ğœ†2 â‡\nğ‘’Î›3. This ensures\nthere is a single realizable history â„such that there is an ğœ†3 where ğœ†2(â„â„â€²) = ğœ†3(â„â„â€²) for any\nrealizable suffix â„â€². To finish the counterexample, we simply note that this realizable â„can be\ndifferent from â„â—¦and all of its successors. For example, â„â—¦might be the history containing\nonly ğ‘œ1 for the first ten timesteps, while â„could be the history containing only ğ‘œ2 for\nthe first ten timesteps. Thus, this ğœ†1 never reaches Î›3, and we conclude the counterexample. âœ“\nCounterexample: Never Reaches.\nThe instance for never reaches is simple: Let ğ’œ=\n{ğ‘1, ğ‘2, ğ‘3}, and Î›1 = Î›3. Suppose all agents in Î›1 (and thus Î›3) only choose actions ğ‘1\nand ğ‘3. Let Î›2 be a singleton, Î›2 = {ğœ†2} such that ğœ†2 : â„â†¦â†’ğ‘2. Clearly, every ğœ†1 âˆˆÎ›1 will\nnever reach Î›2, since none of them ever choose ğ‘2. Similarly, ğœ†2 will never reach Î›3, since\nno agents in Î›3 choose ğ‘2. However, by Proposition C.15 and the assumption that Î›1 = Î›3,\nwe know âˆ€ğœ†1âˆˆÎ›1 ğœ†1 â–¡â‡\nğ‘’Î›3. This directly violates transitivity. âœ“\nThis completes the argument for all three cases, and we conclude.\nâ–¡\nLemma B.10. Sometimes reaches is not commutative: there exists a pair (Î›1, Î›2) and ğ‘’âˆˆâ„°such\nthat âˆ€ğœ†1âˆˆÎ›1 ğœ†1 â‡\nğ‘’Î›2, but âˆƒğœ†2âˆˆÎ›2 ğœ†2 â‡\n/\nğ‘’Î›1.\nProof of Lemma B.10.\nThe result holds as a straightforward consequence of the following counterexample. Consider\nthe pair of agent sets\nÎ›1 = {ğœ†ğ‘–: â„â†¦â†’ğ‘1},\nÎ›2 = {ğœ†ğ‘–: â„â†¦â†’ğ‘1, ğœ†ğ‘—: â„â†¦â†’ğ‘2}.\nNote that since ğœ†ğ‘–is in both sets, and Î›1 is a singleton, we know that ğœ†â‡\nğ‘’\nÎ›1 in any\nenvironment by Lemma B.11. But, clearly ğœ†ğ‘—never reaches Î›1, since no agent in Î›1 ever\nchooses ğ‘1.\nâ–¡\nLemma B.11. For all pairs (Î›, ğ‘’), if ğœ†âˆˆÎ›, then ğœ†â‡\nğ‘’Î›.\nProof of Lemma B.11.\nThe proposition is straightforward, as any ğœ†âˆˆÎ› will be equivalent to itself in behavior for all\nhistories.\nâ–¡\nLemma B.12. Every agent satisfies ğœ†â‡\nğ‘’Î› in every environment.\nProof of Lemma B.12.\nThis is again a direct consequence of Proposition C.18.\nâ–¡\nLemma B.13. The decision problem, AGENTREACHES, Given (ğ‘’, ğœ†, Î›), output True iff ğœ†â‡\nğ‘’Î›, is\nundecidable.\nProof of Lemma B.13.\nWe again proceed by reducing AGENTREACHES from the Halting Problem.\n21\nIn particular, let ğ‘šbe a fixed but arbitrary Turing Machine, and ğ‘¤be a fixed but arbitrary\ninput to be given to machine ğ‘š. Then, HALT defines the decision problem that outputs True\niff ğ‘šhalts on input ğ‘¤.\nWe construct an oracle for AGENTREACHES that can decide HALT as follows. Consider\nthe same observation space used in the proof of Lemma B.8: Let ğ’ªbe comprised of all\nconfigurations of machine ğ‘š. Then, sequences of observations are simply evolution of\ndifferent Turing Machines processing possible inputs. We consider an action space, ğ’œ=\n{ğ‘halted, ğ‘notâˆ’yet}, where agents simply report whether the history so far contains a halting\nconfiguration.\nThen, we consider a deterministic environment ğ‘’that simply produces the next configuration of\nğ‘šwhen run on input ğ‘¤, based on the current tape contents, the state of ğ‘š, and the location of\nthe tape head. Note again that all three of these elements are contained in a Turing Machineâ€™s\nconfiguration.\nUsing these ingredients, we take any instance of HALT, (ğ‘š, ğ‘¤), and build the singleton agent\nset Î›B containing only the agent ğœ†halted : â„â†¦â†’ğ‘halted that always reports the machine as\nhaving halted. We then consider whether the agent that outputs ğ‘notâˆ’yet indefinitely until ğ‘š\nreports halting, at which point the agent switches to ğ‘halted.\nWe make one query to our AGENTREACHES oracle, and ask: ğœ†â‡\nğ‘’Î›B. If it is True, then the\nbranching agent eventually becomes equivalent to ğœ†halted in that they both indefinitely output\nğ‘halted on at least one realizable history. Since ğ‘’is deterministic, we know this equivalence\nholds across all histories. If the query reports False, then there is no future in ğ‘’in which ğ‘š\nhalts on ğ‘¤, otherwise the agent would become equivalent to ğœ†halted. We thus use the oracleâ€™s\nresponse directly to decide the given instance of HALT.\nâ–¡\nC\nAdditional Analysis\nFinally, we present a variety of additional results about agents and the generates and reaches operators.\nC.1\nAdditional Analysis: Generates\nWe first highlight simple properties of the generates operator. Many of our results build around the\nnotion of uniform generation, a variant of the generates operator in which a basis generates an agent\nset in every environment. We define this operator precisely as follows.\nDefinition C.1. Let Î£ be a set of learning rules over some basis Î›B. We say that a set Î› is uniformly\nğšº-generated by Î›B, denoted Î›B |=Î£ Î›, if and only if\nâˆ€ğœ†âˆˆÎ›âˆƒğœâˆˆÎ£âˆ€â„âˆˆâ„‹ğœ†(â„) = ğœ(â„)(â„).\n(C.1)\nDefinition C.2. We say a basis Î›B uniformly generates Î›, denoted Î›B |= Î›, if and only if\nâˆƒÎ£âŠ†Î£ Î›B |=Î£ Î›.\n(C.2)\nWe will first show that uniform generation entails generation in a particular environment. As a\nconsequence, when we prove that certain properties hold of uniform generation, we can typically also\nconclude that the properties hold for generation as well, though there is some subtlety as to when\nexactly this implication will allow results about |= to apply directly to âŠ¢ğ‘’.\nProposition C.1. For any (Î›B, Î›) pair, if Î›B |= Î›, then for all ğ‘’âˆˆâ„°, Î›B âŠ¢ğ‘’Î›.\nProof of Proposition C.1.\nRecall that in the definition of uniform generation, Î›B |= Î›, we require,\nâˆƒÎ£âŠ†Î£âˆ€ğœ†âˆˆÎ›âˆƒğœâˆˆÎ£âˆ€â„âˆˆâ„‹ğœ†(â„) = ğœ(â„)(â„).\n(C.3)\nNow, contrast this with generates with respect to a specific environment ğ‘’,\nâˆƒÎ£âŠ†Î£âˆ€ğœ†âˆˆÎ›âˆƒğœâˆˆÎ£âˆ€â„âˆˆÂ¯\nâ„‹ğœ†(â„) = ğœ(â„)(â„).\n(C.4)\n22\nThe only difference in the definitions is that the set of histories quantified over is â„‹in the\nformer, and Â¯\nâ„‹= â„‹ğœ†,ğ‘’in the latter.\nSince Â¯\nâ„‹âŠ†â„‹for any choice of environment ğ‘’, we can conclude that when Equation C.3, it\nis also the case that Equation C.4 holds, too. Therefore, Î›B |= Î› â‡’Î›B âŠ¢ğ‘’Î› for any ğ‘’.\nâ–¡\nWe next show that the subset relation implies generation.\nProposition C.2. Any pair of agent sets (Î›small, Î›big) such that Î›small âŠ†Î›big satisfies\nÎ›big |= Î›small.\n(C.5)\nProof of Proposition C.2.\nThe result follows from the combination of two facts. First, that all agent sets generate\nthemselves. That is, for arbitrary Î›, we know that Î› âŠ¢ğ‘’Î›, since the trivial set of learning rules,\nÎ£tr = {ğœğ‘–: â„â†¦â†’ğœ†ğ‘–, âˆ€ğœ†ğ‘–âˆˆÎ›},\n(C.6)\nthat never switches between agents is sufficient to generate the agent set.\nSecond, observe that removing an agent from the generated set has no effect on the generates\noperator. That is, let Î›â€² = Î› \\ ğœ†, for fixed but arbitrary ğœ†âˆˆÎ›. We see that Î› âŠ¢ğ‘’Î›â€², since Î£tr\nis sufficient to generate Î›â€², too. By inducting over all removals of agents from Î›, we reach\nour conclusion.\nâ–¡\nNext, we establish properties about the sets of learning rules that correspond to the generates operator.\nProposition C.3. For any (Î›B, Î£, Î›) such that Î›B |=Î£ Î›, it holds that\n|Î›| â‰¤|Î£|.\n(C.7)\nProof of Proposition C.3.\nWe proceed toward contradiction, and assume |Î›| > |Î£|. Then, there is at least one learning\nrule ğœâˆˆÎ£ that corresponds to two or more distinct agents in Î›. Call this element ğœâ—¦, and\nwithout loss of generality let ğœ†1 and ğœ†2 be two distinct agents that are each generated by ğœâ—¦in\nthe sense that,\nğœ†1(â„) = ğœâ—¦(â„)(â„),\nğœ†2(â„) = ğœâ—¦(â„)(â„),\n(C.8)\nfor every â„âˆˆâ„‹. But, by the distinctness of ğœ†1 and ğœ†2, there must exist a history â„in\nwhich ğœ†1(â„) â‰ ğœ†2(â„). We now arrive at a contradiction as such a history cannot exist: By\nEquation C.8, we know that ğœ†1(â„) = ğœâ—¦(â„)(â„) = ğœ†2(â„) for all â„.\nâ–¡\nWe see that the universal learning rules, Î£, is the strongest in the following sense.\nProposition C.4. For any basis Î›B and agent set Î›, exactly one of the two following properties hold:\n1. The agent basis Î›B uniformly generates Î› under the set of all learning rules: Î›B |=Î£ Î›.\n2. There is no set of learning rules for which the basis Î£-uniformly generates the agent set:\nÂ¬âˆƒÎ£âŠ†Î£ Î›B |=Î£ Î›.\nProof of Proposition C.4.\nThe proof follows from the law of excluded middle. That is, for any set of learning rules\nÎ£, either it generates Î› or it does not. If it does generate Î›, by Lemma B.6 so does Î£. By\nconsequence, if Î£ does not generate Î›, neither do any of its subsets.\nâ–¡\nFurthermore, uniform generation is also transitive.\nTheorem C.5. Uniform generates is transitive: For any triple (Î›1, Î›2, Î›3), if Î›1 |= Î›2 and\nÎ›2 |= Î›3, then Î›1 |= Î›3.\n23\nProof of Theorem C.5.\nAssume Î›1 |= Î›2 and Î›2 |= Î›3. Then, by Proposition C.4 and the definition of the uniform\ngenerates operator, we know that\nâˆ€ğœ†2âˆˆÎ›2âˆƒğœ1âˆˆÎ£1âˆ€â„âˆˆâ„‹ğœ†2(â„) = ğœ1(â„)(â„),\n(C.9)\nâˆ€ğœ†3âˆˆÎ›3âˆƒğœ2âˆˆÎ£2âˆ€â„âˆˆâ„‹ğœ†3(â„) = ğœ2(â„)(â„),\n(C.10)\nwhere Î£1 and Î£2 express the set of all learning rules over Î›1 and Î›2 respectively. By definition\nof a learning rule, ğœ, we rewrite the above as follows,\nâˆ€ğœ†2âˆˆÎ›2âˆ€â„âˆˆâ„‹âˆƒğœ†1âˆˆÎ›1 ğœ†2(â„) = ğœ†1(â„),\n(C.11)\nâˆ€ğœ†3âˆˆÎ›3âˆ€â„âˆˆâ„‹âˆƒğœ†2âˆˆÎ›2 ğœ†3(â„) = ğœ†2(â„).\n(C.12)\nThen, consider a fixed but arbitrary ğœ†3 âˆˆÎ›3. We construct a learning rule defined over Î›1\nas ğœ1 : â„‹â†’Î›1 that induces an equivalent agent as follows. For each history, â„âˆˆâ„‹, by\nEquation C.12 we know that there is an ğœ†2 such that ğœ†3(â„) = ğœ†2(â„), and by Equation C.11,\nthere is an ğœ†1 such that ğœ†2(â„) = ğœ†1(â„). Then, set ğœ1 : â„â†¦â†’ğœ†1 such that ğœ†1(â„) = ğœ†2(â„) =\nğœ†3(â„). Since â„and ğœ†3 were chosen arbitrarily, we conclude that\nâˆ€ğœ†3âˆˆÎ›3âˆ€â„âˆˆâ„‹âˆƒğœ†1âˆˆÎ›1 ğœ†3(â„) = ğœ†1(â„).\nBut, by the definition of Î£, this means there exists a learning rule such that\nâˆ€ğœ†3âˆˆÎ›3âˆƒğœ1âˆˆÎ£1âˆ€â„âˆˆâ„‹ğœ†3(â„) = ğœ1(â„)(â„).\nThis is exactly the definition of Î£-uniform generation, and by Proposition C.4, we conclude\nÎ›1 |= Î›3.\nâ–¡\nNext, we show that a singleton basis only generates itself.\nProposition C.6. Any singleton basis, Î›B = {ğœ†}, only uniformly generates itself.\nProof of Proposition C.6.\nNote that generates requires switching between base agents. With only a single agent, there\ncannot be any switching, and thus, the only agent that can be described as switching amongst\nthe elements of the singleton set Î›B = {ğœ†} is the set itself.\nâ–¡\nC.1.1\nRank and Minimal Bases\nAs discussed in the paper, one natural reaction to the concept of an agent basis is to ask how we\ncan justify different choices of a basis. And, if we cannot, then perhaps the concept of an agent\nbasis is disruptive, rather than illuminating. In the main text, we suggest that in many situations, the\nchoice of basis is made by the constraints imposed by the problem, such as the available memory.\nHowever, there are some objective properties of different bases that can help us to evaluate possible\nchoices of a suitable basis. For instance, some bases are minimal in the sense that they cannot be\nmade smaller while still retaining the same expressive power (that is, while generating the same agent\nsets). Identifying such minimal sets may be useful, as it is likely that there is good reason to consider\nonly the most compressed agent bases.\nTo make these intuitions concrete, we introduce the rank of an agent set.\nDefinition C.3. The rank of an agent set, rank(Î›), is the size of the smallest agent basis that\nuniformly generates it:\nrank(Î›) = min\nÎ›BâŠ‚Î› |Î›B|\ns.t.\nÎ›B |= Î›.\n(C.13)\n24\nFor example, the agent set,\nÎ› = {ğœ†0 : â„â†¦â†’ğ‘0,\n(C.14)\nğœ†1 : â„â†¦â†’ğ‘1,\nğœ†2 : â„â†¦â†’\n\u001a\nğ‘0\n|â„| mod 2 = 0,\nğ‘1\n|â„| mod 2 = 1,\n},\nhas rank(Î›) = 2, since the basis,\nÎ›B = {ğœ†0\nB : â„â†¦â†’ğ‘0, ğœ†1\nB : â„â†¦â†’ğ‘1},\nuniformly generates Î›, and there is no size-one basis that uniformly generates Î› by Proposition C.6.\nUsing the notion of an agent setâ€™s rank, we now introduce the concept of a minimal basis. We suggest\nthat minimal bases are particular important, as they contain no redundancy with respect to their\nexpressive power. Concretely, we define a minimal basis in two slightly different ways depending on\nwhether the basis has finite or infinite rank. In the finite case, we say a basis is minimal if there is no\nbasis of lower rank that generates it.\nDefinition C.4. An agent basis Î›B with finite rank is said to be minimal just when there is no smaller\nbasis that generates it,\nâˆ€Î›â€²\nBâŠ‚Î› Î›â€²\nB |= Î›B â‡’rank(Î›â€²\nB) â‰¥rank(Î›B).\n(C.15)\nIn the infinite case, as all infinite rank bases will have the same effective size, we instead consider a\nnotion of minimiality based on whether any elements can be removed from the basis without changing\nits expressive power.\nDefinition C.5. An agent basis Î›B with infinite rank is said to be minimal just when no proper\nsubset of Î›B uniformly generates Î›B.\nâˆ€Î›â€²\nBâŠ†Î›B Î›â€²\nB |= Î›B â‡’Î›â€²\nB = Î›B.\n(C.16)\nNotably, this way of looking at minimal bases will also apply to finite rank agent bases as a direct\nconsequence of the definition of a minimal finite rank basis. However, we still provide both definitions,\nas a finite rank basis may not contain a subset that generates it, but there may exist a lower rank basis\nthat generates it.\nCorollary C.7. As a Corollary of Proposition C.2 and Definition C.4, for any minimal agent basis\nÎ›B, there is no proper subset of Î›B that generates Î›B.\nRegardless of whether an agent basis has finite or infinite rank, we say the basis is a minimal basis of\nan agent set Î› just when the basis uniformly generates Î› and the basis is minimal.\nDefinition C.6. For any Î›, a minimal basis of ğš²is any basis Î›B that is both (1) minimal, and (2)\nÎ›B |= Î›.\nA natural question arises as to whether the minimal basis of any agent set Î› is unique. We answer\nthis question in the negative.\nProposition C.8. The minimal basis of a set of agents is not necessarily unique.\nProof of Proposition C.8.\nTo prove the claim, we construct an instance of an agent set with two distinct minimal bases.\nLet ğ’œ= {ğ‘0, ğ‘1}, and ğ’ª= {ğ‘œ0}. We consider the agent set containing four agents. The first\ntwo map every history to ğ‘0 and ğ‘1, respectively, while the second two alternate between ğ‘0\n25\nand ğ‘1 depending on whether the history is of odd or even length:\nÎ› = {ğœ†0 : â„â†¦â†’ğ‘0,\n(C.17)\nğœ†1 : â„â†¦â†’ğ‘1,\nğœ†2 : â„â†¦â†’\n\u001a\nğ‘0\n|â„| mod 2 = 0,\nğ‘1\n|â„| mod 2 = 1,\nğœ†3 : â„â†¦â†’\n\u001a\nğ‘0\n|â„| mod 2 = 1,\nğ‘1\n|â„| mod 2 = 0,\n}.\nNote that there are two distinct subsets that each universally generate Î›:\nÎ›0,1\nB = {ğœ†0\nB, ğœ†1\nB},\nÎ›2,3\nB = {ğœ†2\nB, ğœ†3\nB}.\n(C.18)\nNext notice that there cannot be a singleton basis by Proposition C.6, and thus, both Î›0,1\nB and\nÎ›2,3\nB satisfy (1) |Î›0,1\nB | = |Î›2,3\nB | = rank(Î›), and (2) both Î›0,1\nB\n|= Î›, Î›2,3\nB\n|= Î›.\nâ–¡\nBeyond the lack of redundancy of a basis, we may also be interested in their expressive power. For\ninstance, if we compare two minimal bases, Î›1\nB and Î›2\nB, how might we justify which to use? To\naddress this question, we consider another desirable property of a basis: universality.\nDefinition C.7. An agent basis Î›B is universal if Î›B |= Î›.\nClearly, it might be desirable to work with a universal basis, as doing so ensures that the set of agents\nwe consider in our design space is as rich as possible. We next show that there is at least one natural\nbasis that is both minimal and universal.\nProposition C.9. The basis,\nÎ›â—¦\nB = {ğœ†: ğ’ªâ†’Î”(ğ’œ)},\n(C.19)\nis a minimal universal basis:\n1. Î›â—¦\nB |= Î›: The basis uniformly generates the set of all agents.\n2. Î›â—¦\nB is minimal.\nProof of Proposition C.9.\nWe prove each property separately.\n1. Î›â—¦\nB |= Î›\nFirst, we show that the basis is universal: Î›â—¦\nB |= Î›. Recall that this amounts to showing that,\nâˆ€ğœ†âˆˆÎ›âˆ€â„âˆˆâ„‹âˆƒğœ†â€²âˆˆÎ›â—¦\nB ğœ†(â„) = ğœ†â€²(â„).\n(C.20)\nLet ğœ†âˆˆÎ› and â„âˆˆâ„‹be fixed but arbitrary. Now, let us label the action distribution produced\nby ğœ†(â„) as ğ‘ğœ†(â„). Let ğ‘œrefer to the last observation contained in â„(or âˆ…if â„= â„0 = âˆ…). Now,\nconstruct the agent ğœ†â—¦\nB : ğ‘œâ†¦â†’ğ‘ğœ†(â„). By construction of Î›â—¦\nB, this agent is guaranteed to be a\nmember of Î›â—¦\nB, and furthermore, we know that ğœ†â—¦\nB produces the same output as ğœ†on â„. Since\nboth ğœ†and â„were chosen arbitrarily, the construction will work for any choice of ğœ†and â„,\nand we conclude that at every history, there exists a basis agent Î›â—¦\nB âˆˆÎ›â—¦\nB that produces the\nsame probability distribution over actions as any given agent. Thus, the first property holds. âœ“\n2. Î›â—¦\nB is minimal.\n26\nSecond, we show that Î›â—¦\nB is a minimal basis of Î›. Recall that since rank(Î›â—¦\nB) = âˆ, the\ndefinition of a minimal basis means that:\nâˆ€Î›BâŠ†Î›â—¦\nB Î›B |= Î› â‡’Î›B = Î›â—¦\nB.\n(C.21)\nTo do so, fix an arbitrary proper subset of Î›B âˆˆğ’«(Î›â—¦\nB). Notice that since Î›B is a proper\nsubset, there exists a non-empty set Î›B such that,\nÎ›B âˆªÎ›B = Î›â—¦\nB.\nNow, we show that Î›B cannot uniformly generate Î› by constructing an agent from Î›B. In\nparticular, consider the first element of Î›B, which, by construction of Î›â—¦\nB, is some mapping\nfrom ğ’ªto a choice of probability distribution over ğ’œ. Let us refer to this agentâ€™s output\nprobability distribution over actions as ğ‘. Notice that there cannot exist an agent in Î›â—¦\nB that\nchooses ğ‘, otherwise Î›B would not be a proper subset of Î›â—¦\nB. Notice further that in the set of\nall agents, there are infinitely many agents that output ğ‘in at least one history. We conclude\nthat Î›B cannot uniformly generate Î›, as it does not contain any base element that produces ğ‘.\nThe set Î›B was chosen arbitrarily, and thus the claim holds for any proper subset of Î›â—¦\nB, and\nwe conclude. âœ“\nThis completes the proof of both statements.\nâ–¡\nCorollary C.10. As a direct consequence of Proposition C.9, every universal basis has infinite rank.\nC.1.2\nOrthogonal and Parallel Agent Sets\nDrawing inspiration from vector spaces, we introduce notions of orthogonal and parallel agent bases\naccording to the agent sets they generate.\nDefinition C.8. A pair of agent bases (Î›1\nB, Î›2\nB) are orthogonal if any pair (Î›1, Î›2) they each\nuniformly generate\nÎ›1\nB |= Î›1,\nÎ›2\nB |= Î›2,\n(C.22)\nsatisfy\nÎ›1 âˆ©Î›2 = âˆ….\n(C.23)\nNaturally this definition can be modified to account for environment-relative generation (âŠ¢ğ‘’), or to be\ndefined with respect to a particular set of learning rules in which case two bases are orthogonal with\nrespect to the learning rule set just when they generate different agent sets under the given learning\nrules. As with the variants of the two operators, we believe the details of such formalisms are easy to\nproduce.\nA few properties hold of any pair of orthogonal bases.\nProposition C.11. If two bases Î›1\nB, Î›2\nB are orthogonal, then the following properties hold:\n1. Î›1\nB âˆ©Î›2\nB = âˆ….\n2. Neither Î›1\nB nor Î›2\nB are universal.\nProof of Proposition C.11.\nWe prove each property independently.\n1. Î›1\nB âˆ©Î›2\nB = âˆ…\nWe proceed toward contradiction. That is, suppose that both Î›1\nB is orthogonal to Î›2\nB, and that\nÎ›1\nB âˆ©Î›2\nB â‰ âˆ…. Then, by the latter property, there is at least one agent that is an element of\n27\nboth bases. Call this agent ğœ†â—¦\nB. It follows that the set Î›â—¦\nB = {ğœ†â—¦\nB} is a subset of both Î›1\nB and\nÎ›2\nB. By Proposition C.2, it follows that Î›1\nB |= Î›â—¦\nB and Î›2\nB |= Î›â—¦\nB. But this contradicts the fact\nthat Î›1\nB is orthogonal to Î›2\nB, and so we conclude. âœ“\n2. Neither Î›1\nB nor Î›2\nB are universal.\nWe again proceed toward contradiction. Suppose without loss of generality that Î›1\nB is\nuniversal. Then, we know Î›1\nB |= Î›. Now, we consider two cases: either Î›2\nB generates some\nnon-empty set, Î›2, or it does not generate any sets. If it generates a set Î›2, then we arrive at a\ncontradiction as Î›2 âˆ©Î› â‰ âˆ…, which violates the definition of orthogonal bases. If if does not\ngenerate a set, this violates the definition of a basis, as any basis is by construction non-empty,\nand we know that containing even a single element is sufficient to generate at least one agent\nset by Proposition C.6. Therefore, in either of the two cases, we arrive at a contradiction, and\nthus conclude the argument. âœ“\nThis concludes the proof of each statement.\nâ–¡\nCorollary C.12. For any non-universal agent basis Î›B, there exists an orthogonal agent basis, Î›â€ \nB.\nConversely, two agent bases Î›1\nB, Î›2\nB are parallel just when they generate the same agent sets.\nDefinition C.9. A pair of agent bases (Î›1\nB, Î›2\nB) are parallel if for every agent set Î›, Î›1\nB |= Î› if and\nonly if Î›2\nB |= Î›.\nProposition C.13. If two bases Î›1\nB, Î›2\nB are parallel, then the following properties hold:\n1. Both Î›1\nB |= Î›2\nB and Î›2\nB |= Î›1\nB.\n2. rank(Î›1\nB) = rank(Î›2\nB).\n3. Î›1\nB is universal if and only if Î›2\nB is universal.\nProof of Proposition C.13.\nWe prove each property separately.\n1. Both Î›1\nB |= Î›2\nB and Î›2\nB |= Î›1\nB.\nThe claim follows directly from the definition of parallel bases. An agent set Î› is uniformly\ngenerated by Î›1\nB if and only if it is uniformly generated by Î›2\nB. Since by Proposition C.2 we\nknow both Î›1\nB |= Î›1\nB and Î›2\nB |= Î›2\nB, we conclude that both Î›1\nB |= Î›2\nB and Î›2\nB |= Î›1\nB. âœ“\n2. rank(Î›1\nB) = rank(Î›2\nB).\nRecall that the definition of rank refers to the size of the smallest basis that uniformly generates\nit,\nrank(Î›) = min\nÎ›BâŠ‚Î› |Î›B|\ns.t.\nÎ›B |= Î›.\nNow, note that by property (1.) of the proposition, both sets uniformly generate each other.\nTherefore, we know that\nrank(Î›1\nB) â‰¤min \b\n|Î›1\nB|, |Î›2\nB|\t\n,\nrank(Î›2\nB) â‰¤min \b\n|Î›1\nB|, |Î›2\nB|\t\n,\nsince the smallest set that generates each basis is no larger than the basis itself, or the other\nbasis.\n28\n3. Î›1\nB is universal if and only if Î›2\nB is universal.\nThe claim again follows by combining the definitions of universality and parallel: If Î›1\nB\nis universal, then by definition of parallel bases, Î›2\nB must uniformly generate all the same\nagent sets including Î›, and therefore Î›2\nB is universal, too. Now, if Î›1\nB is not universal, then it\ndoes not uniformly generate Î›. By the definition of parallel bases, we conclude Î›2\nB does not\ngenerate Î› as well. Both directions hold for each labeling of the two bases without loss of\ngenerality, and we conclude. âœ“\nThis completes the argument for each property, and we conclude.\nâ–¡\nC.2\nAnalysis: Reaches\nWe now establish other properties of the reaches operator. Several of these results are based on a\nthird modality of the reaches operator: always reaches, in which an agent eventually reaches an agent\nbasis in all histories realizable in a given environment. We define this precisely as follows.\nDefinition C.10. We say agent ğœ†âˆˆÎ› always reaches Î›B, denoted ğœ†â–¡â‡\nğ‘’Î›B, if and only if\nâˆ€â„âˆˆÂ¯\nâ„‹âˆƒğ‘¡âˆˆN0âˆ€â„â—¦âˆˆÂ¯\nâ„‹ğ‘¡:âˆ\nâ„âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â—¦â„â€²) = ğœ†B(â„â„â—¦â„â€²).\n(C.24)\nThe nested quantifiers allows the agent to become equivalent to different base behaviors depending on\nthe evolution of the interaction stream. For example, in an environment that flips a coin to determine\nwhether ğ‘heads or ğ‘tails is optimal, the ğœ†might output ğ‘heads indefinitely if the coin is heads, but ğ‘tails\notherwise. In this case, such an agent will still always reach the basis Î›B = {ğœ†1\nB : â„â†¦â†’ğ‘heads, ğœ†2\nB :\nâ„â†¦â†’ğ‘tails}. Notice that we here make use of the notation, Â¯\nâ„‹ğ‘¡:âˆ\nâ„\n, which refers to all history suffixes\nof length ğ‘¡or greater, defined precisely as\nÂ¯\nâ„‹ğ‘¡:âˆ\nâ„\n= {â„â€² âˆˆÂ¯\nâ„‹â„: |â„â€²| â‰¥ğ‘¡}\n(C.25)\nWe first show that the always reaches operator implies sometimes reaches.\nProposition C.14. If ğœ†â–¡â‡\nğ‘’Î›, then ğœ†â‡\nğ‘’Î›.\nProof of Proposition C.14.\nAssume ğœ†â–¡â‡\nğ‘’Î›. That is, expanding the definition of always reaches, we assume\nâˆ€â„âˆˆÂ¯\nâ„‹âˆƒğ‘¡âˆˆN0âˆ€â„â—¦âˆˆÂ¯\nâ„‹ğ‘¡:âˆ\nâ„âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â—¦â„â€²) = ğœ†B(â„â„â—¦â„â€²).\n(C.26)\nFurther recall the definition of can reach ğœ†â‡Î›B is as follows\nâˆƒâ„âˆˆÂ¯\nâ„‹âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â€²) = ğœ†B(â„â„â€²).\n(C.27)\nThen, the claim follows quite naturally: pick any realizable history â„âˆˆ\nÂ¯\nâ„‹. By our initial\nassumption that ğœ†â–¡â‡\nğ‘’Î›, it follows (by Equation C.26) that there is a time ğ‘¡and a realizable\nhistory suffix â„â—¦for which\nâˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„â„â—¦â„â€²) = ğœ†B(â„â„â—¦â„â€²).\nBy construction of â„â„â—¦âˆˆÂ¯\nâ„‹â„, we know â„â„â—¦is a realizable history. Therefore, there exists a\nrealizable history, â„âˆ—= â„â„â—¦, for which âˆƒğœ†BâˆˆÎ›Bâˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†(â„âˆ—â„â€²) = ğœ†B(â„âˆ—â„â€²) holds. But this is\nexactly the definition of can reach, and therefore, we conclude the argument.\nâ–¡\nNext, we highlight the fact that every agent in a basis also reaches that basis.\nProposition C.15. For any agent set Î›, it holds that ğœ†â–¡â‡\nğ‘’Î› for every ğœ†âˆˆÎ›.\nProof of Proposition C.15.\n29\nThe proposition is straightforward, as any ğœ†âˆˆÎ› will be equivalent to itself in behavior for all\nhistories.\nâ–¡\nCorollary C.16. As a corollary of Proposition C.15, any pair of agent sets (Î›small, Î›big) where\nÎ›small âŠ†Î›big, satisfies\nâˆ€ğœ†âˆˆÎ›small ğœ†â–¡â‡\nğ‘’Î›big.\n(C.28)\nWe further show that, unlike sometimes and never reaches, always reaches is transitive.\nProposition C.17. Always reaches is transitive.\nProof of Proposition C.17.\nWe proceed by assuming that both âˆ€ğœ†1âˆˆÎ›1 ğœ†1 â–¡â‡\nğ‘’Î›2 and âˆ€ğœ†2âˆˆÎ›2 ğœ†2 â–¡â‡\nğ‘’Î›3 and show that it\nmust follow that âˆ€ğœ†1âˆˆÎ›1 ğœ†1 â–¡â‡\nğ‘’Î›3. To do so, pick a fixed but arbitrary ğœ†1 âˆˆÎ›1, and expand\nğœ†1 â–¡â‡\nğ‘’Î›2 as\nâˆ€â„âˆˆÂ¯\nâ„‹âˆƒğ‘¡âˆˆN0âˆ€â„â—¦âˆˆÂ¯\nâ„‹ğ‘¡:âˆ\nâ„âˆƒğœ†2âˆˆÎ›2âˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†1(â„â„â—¦â„â€²) = ğœ†2(â„â„â—¦â„â€²).\nNow, consider for any realizable history â„â„â—¦â„â€², we know that the corresponding ğœ†2 that pro-\nduces the same action distribution as ğœ†1 also satisfies ğœ†2 â–¡â‡\nğ‘’Î›3. Thus, there must exist some\ntime Â¯ğ‘¡at which, any realizable history Â¯â„Â¯â„â—¦, will satisfy âˆƒğœ†3âˆˆÎ›3 âˆ€Â¯â„â€²âˆˆÂ¯Â¯\nâ„‹ğœ†2( Â¯â„Â¯â„â—¦Â¯â„â€²) = ğœ†3( Â¯â„Â¯â„â—¦Â¯â„â€².\nBut then there exists a time (Â¯ğ‘¡), that ensures every ğœ†2 âˆˆÎ›2 will have a corresponding ğœ†3 âˆˆÎ›3\nwith the same action distribution at all subsequent realizable histories.\nTherefore,\nâˆ€â„âˆˆÂ¯\nâ„‹âˆƒğ‘¡â€²âˆˆN0âˆ€â„â—¦âˆˆÂ¯\nâ„‹ğ‘¡â€²:âˆ\nâ„\nâˆƒğœ†2âˆˆÎ›2âˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†1(â„â„â—¦â„â€²) =\nğœ†2(â„â„â—¦â„â€²)\n|      {z      }\nâˆƒğœ†3âˆˆÎ›3 =ğœ†3(â„â„â—¦â„â€²)\n.\nThus, rewriting,\nâˆ€â„âˆˆÂ¯\nâ„‹âˆƒğ‘¡â€²âˆˆN0âˆ€â„â—¦âˆˆÂ¯\nâ„‹ğ‘¡â€²:âˆ\nâ„\nâˆƒğœ†3âˆˆÎ›3âˆ€â„â€²âˆˆÂ¯\nâ„‹â„ğœ†1(â„â„â—¦â„â€²) = ğœ†3(â„â„â—¦â„â€²).\nBut this is precisely the definition of always reaches, and thus we conclude.\nâ–¡\nNext, we show two basic properties of the set of all agents: it uniformly generates all agent sets, and\nit is always reached by all agents.\nProposition C.18. For any ğ‘’, the set of all agents Î› (i) uniformly generates all other agent sets, and\n(ii) is always reached by all agents:\n(ğ‘–)\nâˆ€Î›âŠ†Î› Î› |= Î›,\n(ğ‘–ğ‘–)\nâˆ€ğœ†âˆˆÎ› ğœ†â–¡â‡\nğ‘’Î›.\n(C.29)\nProof of Proposition C.18.\n(i). âˆ€Î›âŠ†Î› Î› |= Î›\nThe property holds as a straightforward consequence of Proposition C.2: Since any set Î› is a\nsubset of Î›, it follows that Î› |= Î›. âœ“\n(ii). âˆ€ğœ†âˆˆÎ› ğœ†â–¡â‡\nğ‘’Î›\nThe property holds as a straightforward consequence of Proposition C.15: Since every agent\nsatisfies ğœ†âˆˆÎ›, it follows that ğœ†â‡\nğ‘’Î›. âœ“\nThis concludes the argument of both statements.\nâ–¡\n30\nC.3\nFigure: Set Relations in CRL\nFinally, in Figure 3 we present a visual depicting the set relations in CRL between an agent basis\nÎ›B, an agent set it generates Î›, and the three agent sets corresponding to those agents in Î› that (i)\nsometimes, (ii) never, or (iii) always reach the basis. First, we highlight that we visualize Î›B as\na subset of Î› since we define Î›B âŠ‚Î› in CRL (Definition 4.2). However, there can exist triples\n(Î›B, Î›, ğ‘’) such that Î›B âŠ¢ğ‘’Î›, but that Î›B is not a subset of Î›. Such cases are slightly peculiar, since\nit means that the basis contains agents that cannot be expressed by the agent set Î›. Such cases are not\nin line with our definition of CRL, so we instead opt to visualize Î›B as a subset of Î›. Next, notice\nthat the basis is a subset of both the agents that always reach the basis and the agents that sometimes\nreach the basisâ€”this follows directly from the combination of Proposition C.14 and point (3.) of\nTheorem 4.3. By similar reasoning from Proposition C.14, we know that the set of agents that always\nreaches Î›B is a subset of the agents that sometimes reach the basis. Further, since sometimes and\nnever reaches are negations of one another (Remark 3.2), observe that the two sets are disjoint, and\ntogether comprise the entirety of Î›. Lastly, we know that the set of optimal agents, Î›âˆ—, contains only\nagents that never reach the basis, and thus the set Î›âˆ—is disjoint from Î›B and the set of agents that\nsometimes reach Î›B.\nn\n7s0BNVFgdVeKVq1TI6gAC4MF7BjRZKTIFpZ4MxwdhnNzUOuLIUYoK/Qv+2T9W36HZJz3d3EyCbWDs/5eObw8NzIDYpEKr2/793v3JT3/281+898vxr379/m9+8GHv/tS5asyFC/DPMnLrwOuRCIz8VJLn\nYivi1LwNEjEV8H1Y+J/dSNKJfPsXN8V4jLli0zGMuQapK/mfCEyra4+2Nnf27cftv4w8w87I/85ufrwj9k8ysNVitlhwpW6mM0KfWl4qWYiGo8Hk+az3jCTnh4jVepvfF8pUThRhcrHT+8NDIrVlpkYcUmjCd\nJ/oaBvuQWXoPfz67NHGeOTA+EwYme7gbSM3OZ4xYqjvB8FSlXC+rIdEi16jqLg3WiHqZ9mlLWLEsRWw1sEpYCjbguv/yVZnUGA9UMi0SwV6ePmO0F0poLbNFb1KQ59eaB6qVXpR5LBRtIE92X694IvUdAyQR\na2t1y2pmBglYQc7LiJEVGK0vT/qzMhmKuORhOyvMUzB1jYZVSzb7+GDKhA572dSGZY5LaSq59aUfFHyYnX27wg1zpPaQNznUoV9q0ar5KkwEOPepGt0gDOe2kyrgM52JxMvClutbjVU/u0vtEFL9W1LKwzst\n3dXZ8q0ve+CKR+nhJBpwytQpiuZiyUL1e5VqoKYu4WparREyZltfTVkQpPiDf3adMrydsnSVaFnmb6YM4RYgusAlUoVQwBPFnkp4Uv2sVBihejJIwhMI5g/FRHsi9VKLVLEwRu7mB/J4EHt+4hwH5Ag1v\nvRP2hB76e0KaVnwviXq2aB+JhCvpTizu0sCjpICrxDazB2qMkfPz4K3QSaVIaGRM/BRokWJfZI3gumcTGRdB/EsShYiRX8dNmPeO5jR6EwqowY/Iim9NsNB3qciWmlhgp10PiQgwCz4MkpEkCEthFb2\nrjvYSEvpEZMfSemGbZBZiTh0iCHYRsR5G5pAd5/I23KJHvD3xAlKc8imCKsLpCWrElCnpidWVUNEKpFqLDchAhaBOy2CVG2iHITQBSqMvMb7FehZJnA7bMIhlW7RvW5gfH4Cow9g4amDmuhjBIhstX/kEulpq\nXcPoNqKRGJSLeCLJitouw07dM/ZxUDMznQ+2+cfRvhvQvHP2LIf3U0U/XjJnpMi/uHPdp5ZzgLEVxsn7bBcNmFwfYGIW9F0F+S1tD0/AEMWpVCph5pFURcLvlL5DAdg5qMiyk+NbTgXhU0aW3eWD83X/oW8X\nKQym9pvfrvXBORzCHyBmOE6L+buYNVKL2L+ZSevg/Ib2sgnxa3PCxDPb5MQrVZyKWmbSezxj4wxwHucZgYNRSigbSTieRiY8MoEZF87LGhYOzE0i8kMmHkulE1kBIwiplrvqrasbKGOfeGOc5ukGkz6j2\naS3JiGqNpowV4PyZeNEdol2GKoYLnOrw1FoFzlLwIlyhvuR2ea6q0NIs7iJV1VIvKO3DLeVFzlJ1qaWcNjbyoZthdf1DVCKfGwi1r4t5IzPb2yK9DM+sRFaKJ1XFCRo9iJey/cTPHESX7WASQeBlz/z\nEHr5k+qmdJBwTr2uys7tno4IZvlJ0n9Cl8mGriXE8tbEdUY34A2XmGZ1TrUI7cBUR/CJdnUZhQUbrnwmaO2KctKNA2eR8Q1OogNFHcb6icIt/wCFadDoVM2fy0zVQYPUNi6hEwQiArKvrzJXog9GsZOoh5ih\nqfwIvmYW7HS2z8a3zf0Etb1axD7G3yGrYJAj3LDZx1R3afbF72a5185iRZ7I8M5P5m5k5oXcLN0DrLyTDZhupqyxvsT54ZUrUKTGqXiDeq8Y0OxLjvpLrh3ZnJItGMqhLDQre80KMmkiqcr0Elm/+Wgw5hz5\nth2lUECnHiXoFfHZlDXarApq473OGhZTVxCqOlIk6aEz1GbaShnVmsImXUQSL9rCe73p7Tmn5D1ojNTQ9GlDRKd0mPsUBEH1jnX+W9dBKelpLashYWna4atOhm16IYM+h46SNAML/KVj7NpN31aZXwK9\nzk+NqHV8ASHPxz5cCZx/KIZI3mkdQ5vqaiDurTvel5d1elM4qOBEPkRy6Nv0I95WkOn5qnMmLq3p9f7ZKPebwpUal0I9qlek8EIv0ZU+xzFkD2XLOrM7D4CPisfEa5+BDf0Rr6thDiab2i0HGxY1h/0e4n4\n7D2ETXpci6SVvm8E6U8mSK804i3kW3jG0Dyj7+QoL1EsEyrDpjACwETeB2IBD6xPEdXkwk4L8zIT5eE+zuTj+Y2CAYSZpTjJUmSOsbdRO2XsSvBS5GhW/Ju0Gxn/jTDzD13u/Zp9fzPfxCiYlXlCf3t0aJejS\npR31YUnXZrHDa0HFa5FqozvlfrcRMAglXlmv3ocnJqLXNm+pfOSkw61B4+aLqcybczWG0Lur8VBZCSERUaTpjmebD+7FsRajSHFdbdPHe2gfmTL3mhe3wjyROMeL3iTk0lQvf2H0b0Ac2ebpPQNUaDsdn8nx\nm8inUygz3iwap5rOwK0wjMCxy7dF7QldRhzBNlz2Hu+N+n2RJKJHbIGiI1/Sl6VJkdPix0A/OkfUeicz0PEJKHs09AQY3C5nDgC1JT1D6kyzV6dEyxnQBXpBi3CBSE91VrCr6naXkQMi+C4sEFXPhaCn26wQ\nxC47SG2cNPy/M9urqkvmskY3JivmJXT0qIyN0Q7Fjv1jd/IPa6x+lMZn10KHy62KH/wYxaFs9+jknA4uR21gFOk8ROrSouy48KzCSWAB1ZaiIZIUyuvej/kK1U7EdHrvZCFfq3xodTHdgPN1EN5w3YWcWkrdgi\nJ9dZk+ndmC1aTLqKfZexRxOveR3X6nbZn9D1MUc1cT62NJEZhfVS0V24UtewI69VTtSrTgve2s6wdiz7M1iz6ZEaNrxcCaTylPtysgjJuf5ewPeLsBm9y61zu53uklGXfdbkOGtEpVZpMcQcNdTaPgm\nXPTs/JoLNcYimlF8Le2mkWYhu9prW32tQhc7zhCrweE5YrhOhYdsxzIqQCVO6oTg6f4QuXlM4IK8xS8jS3tBefD0yNasy+K+WmFN/4u6eigIBJG+ZjxrVqyQ2zsIVGr6UvKRfhrtJoMUgDyAXUkfYCUXI64uiP\nmE+kG6nkTIDMnX5FDbNLPFpompjYoR5C0UI8EDxexq1xWzZLqS8F3FNt3QilTD3mNdPUK9hXqADdWjmRvUs2SbuOtMsU1ByjHVeuOyrqMFvoWShBtqaeduUNPRJ4xy3jYFh03eumYW8RaEW6omZ27QTNHnzC\nbHraplpDjdZrDdWSt/O7ZN3vks1+58kUuC6/MEf3DZ2ImCs36mPdNtyUk/pBtb2oDWywpTL1NdqWT/vxsrUC9v2X0vbwBkgyXW2FkWb2KI8YzZ31UYdZ5M5algtr3on3YsiWBV3bmEgvqEar6sf5gxByG2e\nlZo98f2HB7kW1fkUwiXWYiXCZS8kiEaI/p18rUDGzBZ6WgmW5k1Athcd9poqlrdV98qpFsonOFYHw+df3CXsmTijBP17tMfV1Vbiv6h/0UNqpaIB/M7ykRkm7W1ZmebU/4MD032ISqksxX7p2bXe298\nB2ixOmYOuQDpUsXmVh/ZNGc48EBloRGF0u0s0XiB0YGrezrTieJH1ocx3o5mychB4Teyei3pXEX6rq6oOd2fD36PWHLw/2Zg/29r/4286jl/9yv1W/N/rD6E+j0az0d9Hj0ZPRyejl6NwdD369+g/o/e/e\nX+9eu8fDvruO/737d+Pep97x/8HAJ/d9A=</latexit>â‡¤\n{âŒ«2 â‡¤: âŒ« \n4 â‡¤B}\n{âŒ«2 â‡¤: âŒ« \n/\n4 â‡¤B}\n{âŒ«2 â‡¤: âŒ«â‡¤ \n4 â‡¤B}\nâ‡¤B\nâ‡¤â‡¤\nFigure 3: A depiction of the division of a set of agents Î› relative to a basis Î›B through the reaches\noperator in CRL.\n31\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-07-20",
  "updated": "2023-12-01"
}