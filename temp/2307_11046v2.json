{
  "id": "http://arxiv.org/abs/2307.11046v2",
  "title": "A Definition of Continual Reinforcement Learning",
  "authors": [
    "David Abel",
    "André Barreto",
    "Benjamin Van Roy",
    "Doina Precup",
    "Hado van Hasselt",
    "Satinder Singh"
  ],
  "abstract": "In a standard view of the reinforcement learning problem, an agent's goal is\nto efficiently identify a policy that maximizes long-term reward. However, this\nperspective is based on a restricted view of learning as finding a solution,\nrather than treating learning as endless adaptation. In contrast, continual\nreinforcement learning refers to the setting in which the best agents never\nstop learning. Despite the importance of continual reinforcement learning, the\ncommunity lacks a simple definition of the problem that highlights its\ncommitments and makes its primary concepts precise and clear. To this end, this\npaper is dedicated to carefully defining the continual reinforcement learning\nproblem. We formalize the notion of agents that \"never stop learning\" through a\nnew mathematical language for analyzing and cataloging agents. Using this new\nlanguage, we define a continual learning agent as one that can be understood as\ncarrying out an implicit search process indefinitely, and continual\nreinforcement learning as the setting in which the best agents are all\ncontinual learning agents. We provide two motivating examples, illustrating\nthat traditional views of multi-task reinforcement learning and continual\nsupervised learning are special cases of our definition. Collectively, these\ndefinitions and perspectives formalize many intuitive concepts at the heart of\nlearning, and open new research pathways surrounding continual learning agents.",
  "text": "A Definition of Continual Reinforcement Learning\nDavid Abel\ndmabel@google.com\nGoogle DeepMind\nAndr´e Barreto\nandrebarreto@google.com\nGoogle DeepMind\nBenjamin Van Roy\nbenvanroy@google.com\nGoogle DeepMind\nDoina Precup\ndoinap@google.com\nGoogle DeepMind\nHado van Hasselt\nhado@google.com\nGoogle DeepMind\nSatinder Singh\nbaveja@google.com\nGoogle DeepMind\nAbstract\nIn a standard view of the reinforcement learning problem, an agent’s goal is to\nefficiently identify a policy that maximizes long-term reward. However, this\nperspective is based on a restricted view of learning as finding a solution, rather\nthan treating learning as endless adaptation. In contrast, continual reinforcement\nlearning refers to the setting in which the best agents never stop learning. Despite\nthe importance of continual reinforcement learning, the community lacks a simple\ndefinition of the problem that highlights its commitments and makes its primary\nconcepts precise and clear. To this end, this paper is dedicated to carefully defining\nthe continual reinforcement learning problem. We formalize the notion of agents\nthat “never stop learning” through a new mathematical language for analyzing and\ncataloging agents. Using this new language, we define a continual learning agent as\none that can be understood as carrying out an implicit search process indefinitely,\nand continual reinforcement learning as the setting in which the best agents are all\ncontinual learning agents. We provide two motivating examples, illustrating that\ntraditional views of multi-task reinforcement learning and continual supervised\nlearning are special cases of our definition. Collectively, these definitions and\nperspectives formalize many intuitive concepts at the heart of learning, and open\nnew research pathways surrounding continual learning agents.\n1\nIntroduction\nIn The Challenge of Reinforcement Learning, Sutton states: “Part of the appeal of reinforcement\nlearning is that it is in a sense the whole AI problem in a microcosm” [56]. Indeed, the problem\nfacing an agent that learns to make better decisions from experience is at the heart of the study of\nArtificial Intelligence (AI). Yet, when we study the reinforcement learning (RL) problem, it is typical\nto restrict our focus in a number of ways. For instance, we often suppose that a complete description\nof the state of the environment is available to the agent, or that the interaction stream is subdivided\ninto episodes. Beyond these standard restrictions, however, there is another significant assumption\nthat constrains the usual framing of RL: We tend to concentrate on agents that learn to solve problems,\nrather than agents that learn forever. For example, consider an agent learning to play Go: Once the\nagent has discovered how to master the game, the task is complete, and the agent’s learning can\nstop. This view of learning is often embedded in the standard formulation of RL, in which an agent\ninteracts with a Markovian environment with the goal of efficiently identifying an optimal policy, at\nwhich point learning can cease.\nBut what if this is not the best way to model the RL problem? That is, instead of viewing learning\nas finding a solution, we can instead think of it as endless adaptation. This suggests study of the\ncontinual reinforcement learning (CRL) problem [47, 48, 25, 27], as first explored in the thesis by\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.11046v2  [cs.LG]  1 Dec 2023\nRing [46], with close ties to supervised never-ending [10, 39, 43] and continual learning [47, 48, 26,\n54, 41, 42, 49, 22, 30, 45, 4].\nDespite the prominence of CRL, the community lacks a clean, general definition of this problem. It is\ncritical to develop such a definition to promote research on CRL from a clear conceptual foundation,\nand to guide us in understanding and designing continual learning agents. To these ends, this paper is\ndedicated to carefully defining the CRL problem. Our definition is summarized as follows:\nThe CRL Problem (Informal)\nAn RL problem is an instance of CRL if the best agents never stop learning.\nThe core of our definition is framed around two new insights that formalize the notion of “agents\nthat never stop learning”: (i) we can understand every agent as implicitly searching over a set of\nhistory-based policies (Theorem 3.1), and (ii) every agent will either continue this search forever,\nor eventually stop (Remark 3.2). We make these two insights rigorous through a pair of logical\noperators on agents that we call generates and reaches that provide a new mathematical language for\ncharacterizing agents. Using these tools, we then define CRL as any RL problem in which all of the\nbest agents never stop their implicit search. We provide two motivating examples of CRL, illustrating\nthat traditional multi-task RL and continual supervised learning are special cases of our definition.\nWe further identify necessary properties of CRL (Theorem 4.1) and the new operators (Theorem 4.2,\nTheorem 4.3). Collectively, these definitions and insights formalize many intuitive concepts at the\nheart of continual learning, and open new research pathways surrounding continual learning agents.\n2\nPreliminaries\nWe first introduce key concepts and notation. Our conventions are inspired by Ring [46], the recent\nwork by Dong et al. [16] and Lu et al. [32], as well as the literature on general RL by Hutter [23, 24],\nLattimore [28], Leike [29], Cohen et al. [12], and Majeed [36].\nNotation.\nWe let capital calligraphic letters denote sets (𝒳), lower case letters denote constants and\nfunctions (𝑥), italic capital letters denote random variables (𝑋), and blackboard capitals denote the\nnatural and real numbers (N, R, N0 = N ∪{0}). Additionally, we let Δ(𝒳) denote the probability\nsimplex over the set 𝒳. That is, the function 𝑝: 𝒳× 𝒴→Δ(𝒵) expresses a probability mass\nfunction 𝑝(· | 𝑥, 𝑦), over 𝒵, for each 𝑥∈𝒳and 𝑦∈𝒴. Lastly, we use ¬ to denote logical negation,\nand we use ∀𝑥∈𝒳and ∃𝑥∈𝒳to express the universal and existential quantifiers over a set 𝒳.\n2.1\nAgents and Environments\nWe begin by defining environments, agents, and related artifacts.\nDefinition 2.1. An agent-environment interface is a pair (𝒜, 𝒪) of countable sets 𝒜and 𝒪where\n|𝒜| ≥2 and |𝒪| ≥1.\nWe refer to elements of 𝒜as actions, denoted 𝑎, and elements of 𝒪as observations, denoted 𝑜.\nHistories define the possible interactions between an agent and an environment that share an interface.\nDefinition 2.2. The histories with respect to interface (𝒜, 𝒪) are the set of sequences of action-\nobservation pairs,\nℋ=\n∞\nØ\n𝑡=0\n(𝒜× 𝒪)𝑡.\n(2.1)\nWe refer to an individual element of ℋas a history, denoted ℎ, and we let ℎℎ′ express the history\nresulting from the concatenation of any two histories ℎ, ℎ′ ∈ℋ. Furthermore, the set of histories\nof length 𝑡∈N0 is defined as ℋ𝑡= (𝒜× 𝒪)𝑡, and we use ℎ𝑡∈ℋ𝑡to refer to a history containing\n𝑡action-observation pairs, ℎ𝑡= 𝑎0𝑜1 . . . 𝑎𝑡−1𝑜𝑡, with ℎ0 = ∅the empty history. An environment is\nthen a function from the set of all environments, ℰ, that produces observations given a history.\nDefinition 2.3. An environment with respect to interface (𝒜, 𝒪) is a function 𝑒: ℋ× 𝒜→Δ(𝒪).\nThis model of environments is general in that it can capture Markovian environments such as Markov\ndecision processes (MDPs, Puterman, 2014) and partially observable MDPs (Cassandra et al., 1994),\nas well as both episodic and non-episodic settings. We next define an agent as follows.\n2\nDefinition 2.4. An agent with respect to interface (𝒜, 𝒪) is a function 𝜆: ℋ→Δ(𝒜).\nWe let Λ denote the set of all agents, and let Λ denote any non-empty subset of Λ. This treatment of\nan agent captures the mathematical way experience gives rise to behavior, as in “agent functions”\nfrom work by Russell and Subramanian [50]. This is in contrast to a mechanistic account of agency\nas proposed by Dong et al. [16] and Sutton [58]. Further, note that Definition 2.4 is precisely a\nhistory-based policy; we embrace the view that there is no real distinction between an agent and a\npolicy, and will refer to all such functions as “agents” unless otherwise indicated.\n2.2\nRealizable Histories\nWe will be especially interested in the histories that occur with non-zero probability as a result of the\ninteraction between a particular agent and environment.\nDefinition 2.5. The realizable histories of a given agent-environment pair, (𝜆, 𝑒), define the set of\nhistories of any length that can occur with non-zero probability from the interaction of 𝜆and 𝑒,\nℋ𝜆,𝑒= ¯\nℋ=\n∞\nØ\n𝑡=0\n(\nℎ𝑡∈ℋ𝑡:\n𝑡−1\nÖ\n𝑘=0\n𝑒(𝑜𝑘+1 | ℎ𝑘, 𝑎𝑘)𝜆(𝑎𝑘| ℎ𝑘) > 0\n)\n.\n(2.2)\nGiven a realizable history ℎ, we will refer to the realizable history suffixes, ℎ′, which, when concate-\nnated with ℎ, produce a realizable history ℎℎ′ ∈¯\nℋ.\nDefinition 2.6. The realizable history suffixes of a given (𝜆, 𝑒) pair, relative to a history prefix\nℎ∈ℋ𝜆,𝑒, define the set of histories that, when concatenated with prefix ℎ, remain realizable,\nℋ𝜆,𝑒\nℎ\n= ¯\nℋℎ= {ℎ′ ∈ℋ: ℎℎ′ ∈ℋ𝜆,𝑒}.\n(2.3)\nWe abbreviate ℋ𝜆,𝑒to ¯\nℋ, and ℋ𝜆,𝑒\nℎ\nto ¯\nℋℎ, where 𝜆and 𝑒are obscured for brevity.\n2.3\nReward, Performance, and the RL Problem\nSupported by the arguments of Bowling et al. [7], we assume that all of the relevant goals or purposes\nof an agent are captured by a deterministic reward function (in line with the reward hypothesis [57]).\nDefinition 2.7. We call 𝑟: 𝒜× 𝒪→R a reward function.\nWe remain agnostic to how the reward function is implemented; it could be a function inside of\nthe agent, or the reward function’s output could be a special scalar in each observation. Such\ncommitments do not impact our framing. When we refer to an environment we will implicitly mean\nthat a reward function has been selected as well. We remain agnostic to how reward is aggregated to\ndetermine performance, and instead adopt the function 𝑣defined as follows.\nDefinition 2.8. The performance, 𝑣: ℋ× Λ × ℰ→[vmin, vmax] is a bounded function for fixed\nconstants vmin, vmax ∈R.\nThe function 𝑣(𝜆, 𝑒| ℎ) expresses some statistic of the received future random rewards produced\nby the interaction between 𝜆and 𝑒following history ℎ, where we use 𝑣(𝜆, 𝑒) as shorthand for\n𝑣(𝜆, 𝑒| ℎ0). While we accommodate any 𝑣that satisfies the above definition, it may be useful to\nthink of specific choices of 𝑣(𝜆, 𝑒| ℎ𝑡), such as the average reward,\nlim inf\n𝑘→∞\n1\n𝑘E𝜆,𝑒[𝑅𝑡+ . . . + 𝑅𝑡+𝑘| 𝐻𝑡= ℎ𝑡],\n(2.4)\nwhere E𝜆,𝑒[ · · · | 𝐻𝑡= ℎ𝑡] denotes expectation over the stochastic process induced by 𝜆and 𝑒\nfollowing history ℎ𝑡. Or, we might consider performance based on the expected discounted reward,\n𝑣(𝜆, 𝑒| ℎ𝑡) = E𝜆,𝑒[𝑅𝑡+ 𝛾𝑅𝑡+1 + . . . | 𝐻𝑡= ℎ𝑡], where 𝛾∈[0, 1) is a discount factor.\nThe above components give rise to a simple definition of the RL problem.\nDefinition 2.9. An instance of the RL problem is defined by a tuple (𝑒, 𝑣, Λ) as follows\nΛ∗= arg max\n𝜆∈Λ 𝑣(𝜆, 𝑒).\n(2.5)\nThis captures the RL problem facing an agent designer that would like to identify an optimal agent\n(𝜆∗∈Λ∗) with respect to the performance (𝑣), among the available agents (Λ), in a particular\nenvironment (𝑒). We note that a simple extension of this definition of the RL problem might instead\nconsider a set of environments (or similar alternatives).\n3\n3\nAgent Operators: Generates and Reaches\nWe next introduce two new insights about agents, and the logical operators that formalize them:\n1. Theorem 3.1: Every agent can be understood as searching over another set of agents.\n2. Remark 3.2: Every agent will either continue their search forever, or eventually stop.\nWe make these insights precise by introducing a pair of logical operators on agents: (1) a set of agents\ngenerates (Definition 3.4) another set of agents, and (2) a given agent reaches (Definition 3.5) an agent\nset. Together, these operators enable us to define learning as the implicit search process captured by\nthe first insight, and continual learning as the process of continuing this search indefinitely.\n3.1\nOperator 1: An Agent Basis Generates an Agent Set.\nThe first operator is based on two complementary intuitions.\nFrom the first perspective, an agent can be understood as searching over a space of representable\naction-selection strategies. For instance, in an MDP, agents can be interpreted as searching over the\nspace of policies (that is, the space of stochastic mappings from the MDP’s state to action). It turns\nout this insight can be extended to any agent and any environment.\nThe second complementary intuition notes that, as agent designers, we often first identify the space\nof representable action-selection strategies of interest. Then, it is natural to design agents that search\nthrough this space. For instance, in designing an agent to interact with an MDP, we might be interested\nin policies representable by a neural network of a certain size and architecture. When we design\nagents, we then consider all agents (choices of loss function, optimizer, memory, and so on) that\nsearch through the space of assignments of weights to this particular neural network using standard\nmethods like gradient descent. We codify these intuitions in the following definitions.\nDefinition 3.1. An agent basis (or simply, a basis), ΛB ⊂Λ, is any non-empty subset of Λ.\nNotice that an agent basis is a choice of agent set, Λ. We explicitly call out a basis with distinct\nnotation (ΛB) as it serves an important role in the discussion that follows. For example, we next\nintroduce learning rules as functions that switch between elements of an agent basis for each history.\nDefinition 3.2. A learning rule over an agent basis ΛB is a function, 𝜎: ℋ→ΛB, that selects a\nbase agent for each history.\nWe let Σ denote the set of all learning rules over ΛB, and let Σ denote any non-empty subset of\nΣ. A learning rule is a mechanism for switching between the available base agents following each\nnew experience. Notice that learning rules are deterministic; while a simple extension captures the\nstochastic case, we will see by Theorem 3.1 that the above is sufficiently general in a certain sense.\nWe use 𝜎(ℎ)(ℎ) to refer to the action distribution selected by the agent 𝜆= 𝜎(ℎ) at any history ℎ.\nDefinition 3.3. Let Σ be a set of learning rules over some basis ΛB, and let 𝑒be an environment. We\nsay that a set Λ is 𝚺-generated by ΛB in 𝑒, denoted ΛB ⊢𝑒Σ Λ, if and only if\n∀𝜆∈Λ∃𝜎∈Σ∀ℎ∈¯\nℋ𝜆(ℎ) = 𝜎(ℎ)(ℎ).\n(3.1)\nThus, any choice of Σ together with a basis ΛB induces a family of agent sets whose elements can be\nunderstood as switching between the basis according to the rules prescribed by Σ. We then say that\na basis generates an agent set in an environment if there exists a set of learning rules that switches\nbetween the basis elements to produce the agent set.\nDefinition 3.4. We say a basis ΛB generates Λ in 𝑒, denoted ΛB ⊢𝑒Λ, if and only if\nΛB ⊢𝑒Σ Λ.\n(3.2)\nIntuitively, an agent basis ΛB generates another agent set Λ just when the agents in Λ can be\nunderstood as switching between the base agents. It is in this sense that we can understand agents\nas searching through a basis—an agent is just a particular sequence of history-conditioned switches\nover a basis. For instance, let us return to the example of a neural network: The agent basis might\nrepresent a specific multilayer perceptron, where each element of this basis is an assignment to the\nnetwork’s weights. The learning rules are different mechanisms that choose the next set of weights in\n4\n⇤B\n⇤\n⌫1\n...\n...\n⌫8\n(a) Generates\n⌫1\n...\n...\n(b) Sometimes Reaches\nFigure 1: A visual of the generates (left) and sometimes reaches (right) operators. (a) Generates: An\nagent basis, ΛB, comprised of three base agents depicted by the triangle, circle, and square, generates\na set Λ containing agents that can each be understood as switching between the base agents in the\nrealizable histories of environment 𝑒. (b) Sometimes Reaches: On the right, we visualize 𝜆1 ∈Λ\ngenerated by ΛB (from the figure on the left) to illustrate the concept of sometimes reaches. That is,\nthe agent’s choice of action distribution at each history can be understood as switching between the\nthree basis elements, and there is at least one history for which the agent stops switching—here, we\nshow the agent settling on the choice of the blue triangle and never switching again.\nresponse to experience (such as gradient descent). Together, the agent basis and the learning rules\ngenerate the set of agents that search over choices of weights in reaction to experience. We present a\ncartoon visual of the generates operator in Figure 1(a).\nNow, using the generates operator, we revisit and formalize the central insight of this section: Every\nagent can be understood as implicitly searching over an agent basis. We take this implicit search\nprocess to be the behavioral signature of learning.\nTheorem 3.1. For any agent-environment pair (𝜆, 𝑒), there exists infinitely many choices of a basis,\nΛB, such that both (1) 𝜆∉ΛB, and (2) ΛB ⊢𝑒{𝜆}.\nDue to space constraints, all proofs are deferred to Appendix B.\nWe require that 𝜆∉ΛB to ensure that the relevant bases are non-trivial generators of {𝜆}. This\ntheorem tells us that no matter the choice of agent or environment, we can view the agent as a series\nof history-conditioned switches between basis elements. In this sense, we can understand the agent as\nif 1 it were carrying out a search over the elements of some ΛB. We emphasize that there are infinitely\nmany choices of such a basis to illustrate that there are many plausible interpretations of an agent’s\nbehavior—we return to this point throughout the paper.\n3.2\nOperator 2: An Agent Reaches a Basis.\nOur second operator reflects properties of an agent’s limiting behavior in relation to a basis. Given an\nagent and a basis that the agent searches through, what happens to the agent’s search process in the\nlimit: does the agent keep switching between elements of the basis, or does it eventually stop? For\nexample, in an MDP, many agents of interest eventually stop their search on a choice of a fixed policy.\nWe formally define this notion in terms of an agent reaching a basis according to two modalities: an\nagent (i) sometimes or (ii) never reaches a basis.\nDefinition 3.5. We say agent 𝜆∈Λ sometimes reaches ΛB in 𝑒, denoted 𝜆⇝\n𝑒ΛB, if and only if\n∃ℎ∈¯\nℋ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ′) = 𝜆B(ℎℎ′).\n(3.3)\nThat is, for at least one realizable history, there is some base agent (𝜆B) that produces the same action\ndistribution as 𝜆forever after. This indicates that the agent can be understood as if it has stopped its\nsearch over the basis. We present a visual of sometimes reaches in Figure 1(b). By contrast, we say\nan agent never reaches a basis just when it never becomes equivalent to a base agent.\nDefinition 3.6. We say agent 𝜆∈Λ never reaches ΛB in 𝑒, denoted 𝜆⇝\n/\n𝑒ΛB, iff ¬(𝜆⇝\n𝑒ΛB).\n1We use as if in the sense of the positive economists, such as Friedman [19].\n5\nThe reaches operators formalize the intuition that, since every agent can be interpreted as if it were\nsearching over a basis, every agent will either (1) sometimes, or (2) never stop this search. Since (1)\nand (2) are simply negations of each other, we can now plainly state this fact as follows.\nRemark 3.2. For any agent-environment pair (𝜆, 𝑒) and any choice of basis ΛB such that ΛB ⊢𝑒{𝜆},\nexactly one of the following two properties must be satisfied:\n(1) 𝜆⇝\n𝑒ΛB,\n(2) 𝜆⇝\n/\n𝑒ΛB.\n(3.4)\nThus, by Theorem 3.1, every agent can be thought of as implicitly searching over an agent basis, and\nby Remark 3.2, every agent will either (1) sometimes, or (2) never stop this search. We take this\nimplicit search process to be the behavioral signature of learning, and will later exploit this perspective\nto define a continual learning agent as one that continues its search forever (Definition 4.1). Our\nanalysis in Section 4.4 further elucidates basic properties of both the generates and reaches operators,\nand Figure 1 presents a cartoon visualizing the intuition behind each of the operators. We summarize\nall definitions and notation in a table in Appendix A.\nConsiderations on the Operators.\nNaturally, we can design many variations of both ⇝\n𝑒and ⊢𝑒. For\ninstance, we might be interested in a variant of reaches in which an agent becomes 𝜖-close to any of\nthe basis elements, rather than require exact behavioral equivalence. Concretely, we highlight four\naxes of variation that can modify the definitions of the operators. We state these varieties for reaches,\nbut similar modifications can be made to the generates operator, too:\n1. Realizability. An agent reaches a basis (i) in all histories (and thus, all environments), or (ii)\nin the histories realizable by a given (𝜆, 𝑒) pair.\n2. History Length. An agent reaches a basis over (i) infinite or, (ii) finite length histories.\n3. Probability. An agent reaches a basis (i) with probability one, or (ii) with high probability.\n4. Equality or Approximation. An agent reaches a basis by becoming (i) equivalent to a base\nagent, or (ii) sufficiently similar to a base agent.\nRather than define all of these variations precisely for both operators (though we do explore some in\nAppendix C), we acknowledge their existence, and simply note that the formal definitions of these\nvariants follow naturally.\n4\nContinual Reinforcement Learning\nWe now provide a precise definition of CRL. The definition formalizes the intuition that CRL captures\nsettings in which the best agents do not converge—they continue their implicit search over an agent\nbasis indefinitely.\n4.1\nDefinition: Continual RL\nWe first define continual learning agents using the generates and never reaches operators as follows.\nDefinition 4.1. An agent 𝜆is a continual learning agent in 𝑒relative to ΛB if and only if the basis\ngenerates the agent (ΛB ⊢𝑒{𝜆}) and the agent never reaches the basis (𝜆⇝\n/\n𝑒ΛB).\nThis means that an agent is a continual learner in an environment relative to ΛB if the agent’s search\nover ΛB continues forever. Notice that an agent might be considered a continual learner with respect\nto one basis but not another; we explore this fact more in Section 4.4.\nThen, using these tools, we formally define CRL as follows.\nDefinition 4.2. Consider an RL problem (𝑒, 𝑣, Λ). Let ΛB ⊂Λ be a basis such that ΛB ⊢𝑒Λ, and\nlet Λ∗= arg max𝜆∈Λ 𝑣(𝜆, 𝑒). We say (𝑒, 𝑣, Λ, ΛB) defines a CRL problem if ∀𝜆∗∈Λ∗𝜆∗⇝\n/\n𝑒ΛB.\nSaid differently, an RL problem is an instance of CRL just when all of the best agents are continual\nlearning agents relative to basis ΛB. This problem encourages a significant departure from how we\ntend to think about designing agents: Given a basis, rather than try to build agents that can solve\nproblems by identifying a fixed high-quality element of the basis, we would like to design agents that\ncontinue to update their behavior indefinitely in light of their experience.\n6\n4.2\nCRL Examples\nWe next detail two examples of CRL to provide further intuition.\nQ-Learning in Switching MDPs.\nFirst we consider a simple instance of CRL based on the standard\nmulti-task view of MDPs. In this setting, the agent repeatedly samples an MDP to interact with\nfrom a fixed but unknown distribution [64, 9, 2, 25, 20]. In particular, we make use of the switching\nMDP environment from Luketina et al. [33]. The environment 𝑒consists of a collection of 𝑛\nunderlying MDPs, 𝑚1, . . . , 𝑚𝑛, with a shared action space and environment-state space. We refer\nto this environment-state space using observations, 𝑜∈𝒪. The environment has a fixed constant\npositive probability of 0.001 to switch the underlying MDP, which yields different transition and\nreward functions until the next switch. The agent only observes each environment state 𝑜∈𝒪, which\ndoes not reveal the identity of the active MDP. The rewards of each underlying MDP are structured\nso that each MDP has a unique optimal policy. We assume 𝑣is defined as the average reward, and the\nbasis is the set of 𝜖-greedy policies over all 𝑄(𝑜, 𝑎) functions, for fixed 𝜖= 0.15. Consequently, the\nset of agents we generate, ΛB ⊢𝑒Λ, consists of all agents that switch between these 𝜖-greedy policies.\nNow, the components (𝑒, 𝑣, Λ, ΛB) have been defined, we can see that this is indeed an instance\nof CRL: None of the base agents can be optimal, as the moment that the environment switches its\nunderlying MDP, we know that any previously optimal policy will no longer be optimal in the next\nMDP following the switch. Therefore, any agent that converges (in that it reaches the basis ΛB)\ncannot be optimal either for the same reason. We conclude that all optimal agents in Λ are continual\nlearning agents relative to the basis ΛB.\nWe present a visual of this domain in Figure 2(a), and conduct a simple experiment contrasting the\nperformance of 𝜖-greedy continual Q-learning (blue) that uses a constant step-size parameter of\n𝛼= 0.1, with a convergent Q-learning (green) that anneals its step size parameter over time to zero.\nBoth use 𝜖= 0.15, and we set the number of underlying MDPs to 𝑛= 10. We present the average\nreward with 95% confidence intervals, averaged over 250 runs, in Figure 2(b). Since both variants\nof Q-learning can be viewed as searching over ΛB, the annealing variant that stops its search will\nunder-perform compared to the continual approach. These results support the unsurprising conclusion\nthat it is better to continue searching over the basis rather than converge in this setting.\nContinual Supervised Learning.\nSecond, we illustrate the power of our CRL definition to capture\ncontinual supervised learning. We adopt the problem setting studied by Mai et al. [35]. Let 𝒳denote a\nset of objects to be labeled, each belonging to one of 𝑘∈N classes. The observation space 𝒪consists\nof pairs, 𝑜𝑡= (𝑥𝑡, 𝑦𝑡), where 𝑥𝑡∈𝒳and 𝑦𝑡∈𝒴. Here, each 𝑥𝑡is an input object to be classified and\n𝑦𝑡is the label for the previous input 𝑥𝑡−1. Thus, 𝒪= 𝒳×𝒴. We assume by convention that the initial\n<1\n<2\n<3\nF\nvVd0rfE=\">AQ1nicjVhb9s2Fa7W+dm6Pe+EWFOgCJZAUO4LBOiaXvbQS5olbdEoCyiZtlIokZR\nSTxOw16GPQ3Y0/Zr9kP2b3ZIybZEOUMRKLO+Xj48fDwHDJBGtFMOM5/16/9/4H3504+POJ59+9vnNlV\ntfvMxYzkNyGLKI8dcBzkhE3IoqIjI65QTHAcReRWc7ir9qzPCM8qSAzFNyXGMxwkd0RALEP0QnyQnK6vO\nhqN/qN1wq8aqVf32Tm7d/NcfsjCPSLCGfZkeuk4lhiLmgYkaJzG62vr6M9HJ7iMcnUR8fPM5KWgqMY8z\nFNdpwNjybHckxYTASfFnWMJAmY51gQG8VYTHg2ymwE8wiAdmwjHGdKrBtiUgqyaRxAIxozTktZNE4zkgNV\nNgQ7KR2xRNgoy4MRHdtolEdRCmPZKMx+ypkgMIQZxOeRyAT9PRnGwUB2AkYOxU4ALX6WpDIeWSjC70ETf\nLQqykYZmrohuxoxHFMYOYTNtw5oKdvjmU81LJh0QEPYpowpHptNCwl5Dy9EORC2LqlnKDh68rjerUzhJMh\neqbmo6WgfEBI+pSCVHPNOv6QjCBc9JcQqjkpJD7j+8X0rEHm7bndgsTk+Y8jWYo1+nZPcf2Np0WLmoCtz\nx70Le9Xq8FHNZG9cBWz+4D6LaBqnNzt+1e13a7/TYsqsP6ju0O3KWDjkhSYXrbdkumPQG4D/w0dPpcu/U\nLA+2beR2HXgMWhOv41zHA4yngP1NE1in6rlbNvI2e/Bok+UQCJW9ASBcVxl1W/bOJ1TM7YF/0OxhAhnHyXi\nO3FTstgbw2PRMZN1L25tqKn0b/bm8qVYLCv0C1k1EAqfuWUen0TKfAM5nkuTMNVvnFa3k5pcjqPHTXml\nnKks23ipiSK2PncO8pYV/EcDNpzxtMK1wVb3YH60zvrCR1PhBkVjYDTboR9A3Rbdq8+9SZXRcJTseZ5myp\nAfdjuIYtj2NvSJ2lWSP8Mc2jQiCUq4x5MU8jpEYpoTMXdJj4BIXTIkyEUAiJkgnyuZoU5Z+fIp8lIwOR9h\nSqMnlmuesLzRFK0g9zix8SEpJxBpPrqVQc1Uf3HgBGp/JYqg/DyuO6/rHWq0J32pSKqOpdhA0e8VhceQ\nel/0AJ1d024cLBsAQAaZGxQwQyxYgeDjXyoemkmNQLnxpaCOljchoqfKZMhvIZ6bJN6X8jSl/Ucpfm\nPL9Ur5fmC4fQngVR96x7PgBgYK7qC9HnEshIwnhO+4KdTEAMrLuJCNO7orHLc8c8yKDxEujEUNHDFCEo\nJhCxVR4cC3Vl1v72LwIUCraokQmDURQ1TdB4QOB9wopbuearKOeNr0ofyH1PYE/D2bdV6FxBfzIDQMlaGg\np3hLGxIBFGx7+0/KCnoIo6IEtHhHBwIbzwJ7NyL70Qwhw8UT70TCpQZU1MCOMkXpOzRiEPylZR10uagE/\nDPBMshjOIASLJGeUsUecmucBAfHYQuJGTBWsf7DVNqdD1Deu6G3A1xR10GSdYc17IXcZBP6eXkZLw67AS\n+FMYrvEmal/DJiykdyn8DR8PQyUgpyBU7KsEFJL0CbkRZDltnjLGWZDurL6EFySwtZA15GUgOvwFLhTJq6\n7xKepVyd4/AZQbg1MlRNO3rUWcRDgfcnINoAnu4DGSajGd5AOA5rH6hxcs1XtDdWVCeQW+yr08w5Vflc\nnSY7lAGI1wEk7hdHyBsE4dSExglGzDSEMA0Dn43Wmoq9KQnjQwqm4ISqZFJTNTqvOuKVTptiZbfEJhUoJa\np0pi5j51FPi6ZyY/8NWSDAd1twyWrBE5cCfK41THifxu0W5gQpa8JaFQPoUNOW8bmNlGNfsHBFhVdt39a\nuhIRc41ifxh1WjoY0IuLOQT/SroUlrQd6O+BmKl1u1sWVnuhEOIZE+Us9m7lSKtaWa0tzacnvV+5KMO1xU\nJFmrTk37C9BaEzUvPfsgM2O4vaPeia0fLaJm/Dpix/vhKQBC6UEOny+YFbGLoMdhrJZNyST3AKtzdTPM\nIxjaYyTaO3dRVc5yEcqjtjRfALn2PQDKnWGfY4NsxSpZe67X5Ynfgwu+a1/t246W34fY2nBfd1XuHv5VX\n/xvWV9Y31h3LtfrWPet7a86tEJrbP1p/W39s/J65deV31f+KHXr1X/LvjSavxW/vofB2UZ1Q=</late\nxit><=\n(a) Switching MDP Visual\nQ-learning  (continual)\nQ-learning  (anneal)\n(b) Switching MDP Results\nFigure 2: A visual of a grid world instance of the switching MDPs problem (left) [33], and results from\nan experiment contrasting continual learning and convergent Q-learning (right). The environment\npictured contains 𝑛distinct MDPs. Each underlying MDP shares the same state space and action\nspace, but varies in transition and reward functions, as indicated by the changing walls and rewarding\nlocations (stars, circles, and fire). The results pictured on the right contrast continual Q-learning (with\n𝛼= 0.1) with traditional Q-learning that anneals its step-size parameter to zero over time.\n7\nlabel 𝑦0 is irrelevant and can be ignored. The agent will observe a sequence of object-label pairs,\n(𝑥0, 𝑦0), (𝑥1, 𝑦1), . . ., and the action space is a choice of label, 𝒜= {𝑎1, . . . , 𝑎𝑘} where |𝒴| = 𝑘.\nThe reward for each history ℎ𝑡is +1 if the agent’s most recently predicted label is correct for the\nprevious input, and −1 otherwise:\n𝑟(𝑎𝑡−1𝑜𝑡) = 𝑟(𝑎𝑡−1𝑦𝑡) =\n\u001a\n+1\n𝑎𝑡−1 = 𝑦𝑡,\n−1\n𝑎𝑡−1 = otherwise.\n(4.1)\nConcretely, the continual learning setting studied by Mai et al. [35] supposes the learner will receive\nsamples from a sequence of probability distributions, 𝑑0, 𝑑1, . . ., each supported over 𝒳× 𝒴. The\n(𝑥, 𝑦) ∈𝒳× 𝒴pairs experienced by the learner are determined by the sequence of distributions.\nWe capture this distributional shift in an environment 𝑒that shifts its probability distribution over 𝒪\ndepending on the history to match the sequence, 𝑑0, 𝑑1, . . ..\nNow, is this an instance of CRL? To answer this question precisely, we need to select a (Λ, ΛB) pair.\nWe adopt the basis ΛB = {𝜆B : 𝑥↦→𝑦𝑖, ∀𝑦𝑖∈𝒴} that contains each classifier that maps each object\nto each possible label. By the universal set of learning rules Σ, this basis generates the set of all\nagents that search over classifiers. Now, our definition says the above is an instance of CRL just when\nevery optimal agent never stops switching between classifiers, rather than stop their search on a fixed\nclassifier. Consequently, if there is an optimal classifier in ΛB, then this will not be an instance of\nCRL. If, however, the environment imposes enough distributional shift (changing labels, adding mass\nto new elements, and so on), then the only optimal agents will be those that always switch among the\nbase classifiers, in which case the setting is an instance of CRL.\n4.3\nRelationship to Other Views on Continual Learning\nThe spirit of continual learning has been an important part of machine learning research for decades,\noften appearing under the name of “lifelong learning” [63, 62, 53, 55, 51, 3, 4], “never-ending\nlearning” [39, 43] with close ties to transfer-learning [61, 60], meta-learning [52, 17], as well as\nonline learning and non-stationarity [5, 40, 13, 6, 31]. In a similar vein, the phrase “continuing tasks”\nis used in the classic RL textbook [59] to refer explicitly to cases when the interaction between\nagent and environment is not subdivided into episodes. Continual reinforcement learning was first\nposed in the thesis by Ring [46]. In later work [47, 48], Ring proposes a formal definition of the\ncontinual reinforcement learning problem—The emphasis of Ring’s proposal is on the generality of\nthe environment: rather than assume that agents of interest will interact with an MDP, Ring suggests\nstudying the unconstrained case in which an agent must maximize performance while only receiving\na stream of observations as input. The environment or reward function, in this sense, may change\nover time or may be arbitrarily complex. This proposal is similar in spirit to general RL, studied\nby Hutter [24], Lattimore [28], Leike [29], and others [12, 37, 36] in which an agent interacts with\nan unconstrained environment. General RL inspires many aspects of our conception of CRL; for\ninstance, our emphasis on history-dependence rather than environment-state comes directly from\ngeneral RL. More recently, Khetarpal et al. [25] provide a comprehensive survey of the continual\nreinforcement learning literature. We encourage readers to explore this survey for a detailed history of\nthe subject.2 In the survey, Khetarpal et al. propose a definition of the CRL problem that emphasizes\nthe non-stationarity of the underlying process. In particular, in Khetarpal et al.’s definition, an agent\ninteracts with a POMDP in which each of the individual components of the POMDP—such as the\nstate space or reward function—are allowed to vary with time. We note that, as the environment\nmodel we study (Definition 2.3) is a function of history, it can capture time-indexed non-stationarity.\nIn this sense, the same generality proposed by Khetarpal et al. and Ring is embraced and retained\nby our definition, but we add further precision to what is meant by continual learning by centering\naround a mathematical definition of continual learning agents (Definition 4.1).\n4.4\nProperties of CRL\nOur formalism is intended to be a jumping off point for new lines of thinking around agents and\ncontinual learning. We defer much of our analysis and proofs to the appendix, and here focus on\nhighlighting necessary properties of CRL.\n2For other surveys, see the recent survey on continual robot learning by Lesort et al. [30], a survey on\ncontinual learning with neural networks by Parisi et al. [42], a survey on transfer learning in RL by Taylor and\nStone [60], and a survey on continual image classification by Mai et al. [35].\n8\nTheorem 4.1. Every instance of CRL (𝑒, 𝑣, Λ, ΛB) necessarily satisfies the following properties:\n1. If Λ ≠ΛB ∪Λ∗, then there exists a Λ′\nB such that (1) Λ′\nB ⊢𝑒Λ, and (2) (𝑒, 𝑣, Λ, Λ′\nB) is not an\ninstance of CRL.\n2. No element of ΛB is optimal: ΛB ∩Λ∗= ∅.\n3. If |Λ| is finite, there exists an agent set, Λ◦, such that |Λ◦| < |Λ| and Λ◦⊢𝑒Λ.\n4. If |Λ| is infinite, there exists an agent set, Λ◦, such that Λ◦⊂Λ and Λ◦⊢𝑒Λ.\nThis theorem tells us several things. The first point of the theorem has peculiar implications. We see\nthat as we change a single element (the basis ΛB) of the tuple (𝑒, 𝑣, ΛB, Λ), the resulting problem can\nchange from CRL to not CRL. By similar reasoning, an agent that is said to be a continual learning\nagent according to Definition 4.1 may not be a continual learner with respect to some other basis.\nWe discuss this point further in the next paragraph. Point (2.) notes that no optimal strategy exists\nwithin the basis—instead, to be optimal, an agent must switch between basis elements indefinitely. As\ndiscussed previously, this fact encourages a departure in how we think about the RL problem: rather\nthan focus on agents that can identify a single, fixed solution to a problem, CRL instead emphasizes\ndesigning agents that are effective at updating their behavior indefinitely. Points (3.) and (4.) show\nthat Λ cannot be minimal. That is, there are necessarily some redundancies in the design space of\nthe agents in CRL—this is expected, since we are always focusing on agents that search over the\nsame agent basis. Lastly, it is worth calling attention to the fact that in the definition of CRL, we\nassume ΛB ⊂Λ—this suggests that in CRL, the agent basis is necessarily limited in some way.\nConsequently, the design space of agents Λ are also limited in terms of what agents they can represent\nat any particular point in time. This limitation may come about due to a computational or memory\nbudget, or by making use of a constrained set of learning rules. This suggests a deep connection\nbetween bounded agents and the nature of continual learning, as explored further by Kumar et al. [27].\nWhile these four points give an initial character of the CRL problem, we note that further exploration\nof the properties of CRL is an important direction for future work.\nCanonical Agent Bases.\nIt is worth pausing and reflecting on the concept of an agent basis. As\npresented, the basis is an arbitrary choice of a set of agents—consequently, point (1.) of Theorem 4.1\nmay stand out as peculiar. From this perspective, it is reasonable to ask if the fact that our definition\nof CRL is basis-dependant renders it vacuous. We argue that this is not the case for two reasons. First,\nwe conjecture that any definition of continual learning that involves concepts like “learning” and\n“convergence” will have to sit on top of some reference object whose choice is arbitrary. Second, and\nmore important, even though the mathematical construction allows for an easy change of basis, in\npractice the choice of basis is constrained by considerations like the availability of computational\nresources. It is often the case that the domain or problem of interest provides obvious choices of\nbases, or imposes constraints that force us as designers to restrict attention to a space of plausible\nbases or learning rules. For example, as discussed earlier, a choice of neural network architecture\nmight comprise a basis—any assignment of weights is an element of the basis, and the learning rule 𝜎\nis a mechanism for updating the active element of the basis (the parameters) in light of experience. In\nthis case, the number of parameters of the network is constrained by what we can actually build, and\nthe learning rule needs to be suitably efficient and well-behaved. We might again think of the learning\nrule 𝜎as gradient descent, rather than a rule that can search through the basis in an unconstrained\nway. In this sense, the basis is not arbitrary. We as designers choose a class of functions to act as the\nrelevant representations of behavior, often limited by resource constraints on memory or compute.\nThen, we use specific learning rules that have been carefully designed to react to experience in\na desirable way—for instance, stochastic gradient descent updates the current choice of basis in\nthe direction that would most improve performance. For these reasons, the choice of basis is not\narbitrary, but instead reflects the ingredients involved in the design of agents as well as the constraints\nnecessarily imposed by the environment.\n4.5\nProperties of Generates and Reaches\nLastly, we summarize some of the basic properties of generates and reaches. Further analysis of\ngenerates, reaches, and their variations is provided in Appendix C.\nTheorem 4.2. The following properties hold of the generates operator:\n9\n1. Generates is transitive: For any triple (Λ1, Λ2, Λ3) and 𝑒∈ℰ, if Λ1 ⊢𝑒Λ2 and Λ2 ⊢𝑒Λ3,\nthen Λ1 ⊢𝑒Λ3.\n2. Generates is not commutative: there exists a pair (Λ1, Λ2) and 𝑒∈ℰsuch that Λ1 ⊢𝑒Λ2,\nbut ¬(Λ2 ⊢𝑒Λ1).\n3. For all Λ and pair of agent bases (Λ1\nB, Λ2\nB) such that Λ1\nB ⊆Λ2\nB, if Λ1\nB ⊢𝑒Λ, then Λ2\nB ⊢𝑒Λ.\n4. For all Λ and 𝑒∈ℰ, Λ ⊢𝑒Λ.\n5. The decision problem, Given (𝑒, ΛB, Λ), output True iff ΛB ⊢𝑒Λ, is undecidable.\nThe fact that generates is transitive suggests that the basic tools of an agent set—paired with a set\nof learning rules—might be likened to an algebraic structure. We can draw a symmetry between an\nagent basis and the basis of a vector space: A vector space is comprised of all linear combinations\nof the basis, whereas Λ is comprised of all valid switches (according to the learning rules) between\nthe base agents. However, the fact that generates is not commutative (by point 2.) raises a natural\nquestion: are there choices of learning rules under which generates is commutative? We suggest that\na useful direction for future work can further explore an algebraic perspective on agents.\nWe find many similar properties hold of reaches.\nTheorem 4.3. The following properties hold of the reaches operator:\n1. ⇝\n𝑒and ⇝\n/\n𝑒are not transitive.\n2.“Sometimes reaches” is not commutative: there exists a pair (Λ1, Λ2) and 𝑒∈ℰsuch that\n∀𝜆1∈Λ1 𝜆1 ⇝\n𝑒Λ2, but ∃𝜆2∈Λ2 𝜆2 ⇝\n/\n𝑒Λ1.\n3. For all pairs (Λ, 𝑒), if 𝜆∈Λ, then 𝜆⇝\n𝑒Λ.\n4. Every agent satisfies 𝜆⇝\n𝑒Λ in every environment.\n5. The decision problem, Given (𝑒, 𝜆, Λ), output True iff 𝜆⇝\n𝑒Λ, is undecidable.\nMany of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding\nwhether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate that\nthe majority of decision problems related to determining properties of arbitrary agent sets interacting\nwith unconstrained environments will be undecidable, though it is still worth making these arguments\ncarefully. Moreover, there may be interesting special cases in which these decision problems are\ndecidable (and perhaps, efficiently so). We suggest that identifying these special cases and fleshing\nout their corresponding efficient algorithms is an interesting direction for future work.\n5\nDiscussion\nIn this paper, we carefully develop a simple mathematical definition of the continual RL problem.\nWe take this problem to be of central importance to AI as a field, and hope that these tools and\nperspectives can serve as an opportunity to think about CRL and its related artifacts more carefully.\nOur proposal is framed around two new insights about agents: (i) every agent can be understood as\nthough it were searching over an agent basis (Theorem 3.1), and (ii) every agent, in the limit, will\neither sometimes or never stop this search (Remark 3.2). These two insights are formalized through\nthe generates and reaches operators, which provide a rich toolkit for understanding agents in a new\nway—for example, we find straightforward definitions of a continual learning agent (Definition 4.1)\nand learning rules (Definition 3.2). We anticipate that further study of these operators and different\nfamilies of learning rules can directly inform the design of new learning algorithms; for instance,\nwe might characterize the family of continual learning rules that are guaranteed to yield continual\nlearning agents, and use this to guide the design of principled continual learning agents (in the spirit\nof continual backprop by Dohare et al. [14]). In future work, we intend to further explore connections\nbetween our formalism of continual learning and some of the phenomena at the heart of recent\nempirical continual learning studies, such as plasticity loss [34, 1, 15], in-context learning [8], and\ncatastrophic forgetting [38, 18, 21, 26]. More generally, we hope that our definitions, analysis, and\nperspectives can help the community to think about continual reinforcement learning in a new light.\n10\nAcknowledgements\nThe authors are grateful to Michael Bowling, Clare Lyle, Razvan Pascanu, and Georgios Piliouras\nfor comments on a draft of the paper, as well as the anonymous NeurIPS reviewers that provided\nvaluable feedback on the paper. The authors would further like to thank all of the 2023 Barbados RL\nWorkshop participants and Elliot Catt, Will Dabney, Sebastian Flennerhag, Andr´as Gy¨orgy, Steven\nHansen, Anna Harutyunyan, Mark Ho, Joe Marino, Joseph Modayil, R´emi Munos, Evgenii Nikishin,\nBrendan O’Donoghue, Matt Overlan, Mark Rowland, Tom Schaul, Yannick Shroecker, Rich Sutton,\nYunhao Tang, Shantanu Thakoor, and Zheng Wen for inspirational conversations.\nReferences\n[1] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of\nplasticity in continual deep reinforcement learning. arXiv preprint arXiv:2303.07507, 2023.\n[2] David Abel, Yuu Jinnai, Yue Guo, George Konidaris, and Michael L. Littman. Policy and value\ntransfer in lifelong reinforcement learning. In Proceedings of the International Conference on\nMachine Learning, 2018.\n[3] Haitham Bou Ammar, Rasul Tutunov, and Eric Eaton. Safe policy search for lifelong reinforce-\nment learning with sublinear regret. In Proceedings of the International Conference on Machine\nLearning, 2015.\n[4] Megan M Baker, Alexander New, Mario Aguilar-Simon, Ziad Al-Halah, S´ebastien MR Arnold,\nEse Ben-Iwhiwhu, Andrew P Brna, Ethan Brooks, Ryan C Brown, Zachary Daniels, et al. A\ndomain-agnostic approach for characterization of lifelong learning systems. Neural Networks,\n160:274–296, 2023.\n[5] Peter L Bartlett. Learning with a slowly changing distribution. In Proceedings of the Annual\nWorkshop on Computational Learning Theory, 1992.\n[6] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with\nnon-stationary rewards. Advances in Neural Information Processing Systems, 2014.\n[7] Michael Bowling, John D. Martin, David Abel, and Will Dabney. Settling the reward hypothesis.\nIn Proceedings of the International Conference on Machine Learning, 2023.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in Neural Information Processing Systems, 2020.\n[9] Emma Brunskill and Lihong Li. PAC-inspired option discovery in lifelong reinforcement\nlearning. In Proceedings of the International Conference on Machine Learning, 2014.\n[10] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom\nMitchell. Toward an architecture for never-ending language learning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, 2010.\n[11] Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. Acting optimally in\npartially observable stochastic domains. In Proceedings of the AAAI Conference on Artificiall\nIntelligence, 1994.\n[12] Michael K Cohen, Elliot Catt, and Marcus Hutter. A strongly asymptotically optimal agent in\ngeneral environments. arXiv preprint arXiv:1903.01021, 2019.\n[13] Travis Dick, Andr´as Gy¨orgy, and Csaba Szepesvari. Online learning in Markov decision\nprocesses with changing cost sequences. In Procedings of the International Conference on\nMachine Learning, 2014.\n[14] Shibhansh Dohare, Richard S Sutton, and A Rupam Mahmood. Continual backprop: Stochastic\ngradient descent with persistent randomness. arXiv preprint arXiv:2108.06325, 2021.\n11\n[15] Shibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, and A Rupam\nMahmood. Loss of plasticity in deep continual learning. arXiv preprint arXiv:2306.13812,\n2023.\n[16] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple agent, complex environment:\nEfficient reinforcement learning with agent states. Journal of Machine Learning Research, 23\n(255):1–54, 2022.\n[17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-\ntion of deep networks. In Proceedings of the International Conference on Machine Learning,\n2017.\n[18] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive\nsciences, 3(4):128–135, 1999.\n[19] Milton Friedman. Essays in positive economics. University of Chicago press, 1953.\n[20] Haotian Fu, Shangqun Yu, Michael Littman, and George Konidaris. Model-based lifelong\nreinforcement learning with Bayesian exploration. Advances in Neural Information Processing\nSystems, 2022.\n[21] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical\ninvestigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint\narXiv:1312.6211, 2013.\n[22] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Con-\ntinual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028–1040,\n2020.\n[23] Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity.\narXiv preprint cs/0004001, 2000.\n[24] Marcus Hutter. Universal artificial intelligence: Sequential decisions based on algorithmic\nprobability. Springer Science & Business Media, 2004.\n[25] Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual rein-\nforcement learning: A review and perspectives. Journal of Artificial Intelligence Research, 75:\n1401–1476, 2022.\n[26] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks. Proceedings of the National Academy\nof Sciences, 114(13):3521–3526, 2017.\n[27] Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, and\nBenjamin Van Roy. Continual learning as computationally constrained reinforcement learning.\narXiv preprint arXiv:2307.04345, 2023.\n[28] Tor Lattimore. Theory of general reinforcement learning. PhD thesis, The Australian National\nUniversity, 2014.\n[29] Jan Leike. Nonparametric general reinforcement learning. PhD thesis, The Australian National\nUniversity, 2016.\n[30] Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and\nNatalia D´ıaz-Rodr´ıguez. Continual learning for robotics: Definition, framework, learning\nstrategies, opportunities and challenges. Information fusion, 58:52–68, 2020.\n[31] Yueyang Liu, Benjamin Van Roy, and Kuang Xu. A definition of non-stationary bandits. arXiv\npreprint arXiv:2302.12202, 2023.\n[32] Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and\nZheng Wen. Reinforcement learning, bit by bit. Foundations and Trends in Machine Learning,\n16(6):733–865, 2023. ISSN 1935-8237.\n12\n[33] Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and\nSatinder Singh. Meta-gradients in non-stationary environments. In Proceedings of the Confer-\nence on Lifelong Learning Agents, 2022.\n[34] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will\nDabney. Understanding plasticity in neural networks. In Proceedings of the International\nConference on Machine Learning, 2023.\n[35] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online\ncontinual learning in image classification: An empirical survey. Neurocomputing, 469:28–51,\n2022.\n[36] Sultan J Majeed. Abstractions of general reinforcement Learning. PhD thesis, The Australian\nNational University, 2021.\n[37] Sultan Javed Majeed and Marcus Hutter. Performance guarantees for homomorphisms beyond\nMarkov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence,\n2019.\n[38] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:\nThe sequential learning problem. In Psychology of learning and motivation, volume 24, pages\n109–165. Elsevier, 1989.\n[39] Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Bet-\nteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, et al. Never-ending\nlearning. Communications of the ACM, 61(5):103–115, 2018.\n[40] Claire Monteleoni and Tommi Jaakkola. Online learning of non-stationary sequences. Advances\nin Neural Information Processing Systems, 16, 2003.\n[41] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual\nlearning. 2018.\n[42] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual\nlifelong learning with neural networks: A review. Neural networks, 113:54–71, 2019.\n[43] Emmanouil Antonios Platanios, Abulhair Saparov, and Tom Mitchell. Jelly bean world: A\ntestbed for never-ending learning. arXiv preprint arXiv:2002.06306, 2020.\n[44] Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.\nJohn Wiley & Sons, 2014.\n[45] Matthew Riemer, Sharath Chandra Raparthy, Ignacio Cases, Gopeshh Subbaraj, Maximilian\nPuelma Touzel, and Irina Rish. Continual learning in environments with polynomial mixing\ntimes. Advances in Neural Information Processing Systems, 2022.\n[46] Mark B Ring. Continual learning in reinforcement environments. PhD thesis, The University of\nTexas at Austin, 1994.\n[47] Mark B Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77–104,\n1997.\n[48] Mark B Ring. Toward a formal framework for continual learning. In NeurIPS Workshop on\nInductive Transfer, 2005.\n[49] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Ex-\nperience replay for continual learning. Advances in Neural Information Processing Systems,\n2019.\n[50] Stuart J Russell and Devika Subramanian. Provably bounded-optimal agents. Journal of\nArtificial Intelligence Research, 2:575–609, 1994.\n[51] Paul Ruvolo and Eric Eaton. ELLA: An efficient lifelong learning algorithm. In Proceedings of\nthe International Conference on Machine Learning, 2013.\n13\n[52] Tom Schaul and J¨urgen Schmidhuber. Metalearning. Scholarpedia, 5(6):4650, 2010.\n[53] J¨urgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph. Reinforcement learning with\nself-modifying policies. In Learning to Learn, pages 293–309. Springer, 1998.\n[54] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework\nfor continual learning. In Proceedings of the International Conference on Machine Learning,\n2018.\n[55] Daniel L Silver. Machine lifelong learning: Challenges and benefits for artificial general\nintelligence. In Proceedings of the Conference on Artificial General Intelligence, 2011.\n[56] Richard S Sutton. Introduction: The challenge of reinforcement learning. In Reinforcement\nLearning, pages 1–3. Springer, 1992.\n[57] Richard S Sutton. The reward hypothesis, 2004. URL http://incompleteideas.net/rlai.\ncs.ualberta.ca/RLAI/rewardhypothesis.html.\n[58] Richard S Sutton. The quest for a common model of the intelligent decision maker. arXiv\npreprint arXiv:2202.13252, 2022.\n[59] Richard S Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,\n2018.\n[60] Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A\nsurvey. Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.\n[61] Sebastian Thrun. Is learning the n-th thing any easier than learning the first? Advances in\nNeural Information Processing Systems, 1995.\n[62] Sebastian Thrun. Lifelong learning algorithms. Learning to Learn, 8:181–209, 1998.\n[63] Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. Robotics and autonomous\nsystems, 15(1-2):25–46, 1995.\n[64] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning:\na hierarchical Bayesian approach. In Proceedings of the International Conference on Machine\nlearning, 2007.\n14\nA\nNotation\nWe first provide a table summarizing all relevant notation.\nNotation\nMeaning\nDefinition\n𝒜\nActions\n𝒪\nObservations\nℋ𝑡\nLength 𝑡histories\nℋ𝑡= (𝒜× 𝒪)𝑡\nℋ\nAll histories\nℋ= Ð∞\n𝑡=0 ℋ𝑡\nℎ\nA history\nℎ∈ℋ\nℎℎ′\nHistory concatenation\nℎ𝑡\nLength 𝑡history\nℎ𝑡∈ℋ𝑡\n¯\nℋ= ℋ𝜆,𝑒\nRealizable histories\n¯\nℋ= Ð∞\n𝑡=0\n\b\nℎ𝑡∈ℋ𝑡: Î𝑡−1\n𝑘=0 𝑒(𝑜𝑘| ℎ𝑘, 𝑎𝑘)𝜆(𝑎𝑘| ℎ𝑘) > 0\t\n¯\nℋℎ= ℋ𝜆,𝑒\nℎ\nRealizable history suffixes\n¯\nℋℎ= {ℎ′ ∈ℋ: ℎℎ′ ∈ℋ𝜆,𝑒}\n𝑒\nEnvironment\n𝑒: ℋ× 𝒜→Δ(𝒜)\nℰ\nSet of all environments\n𝜆\nAgent\n𝜆: ℋ→Δ(𝒜)\nΛ\nSet of all agents\nΛ\nSet of agents\nΛ ⊆Λ\nΛB\nAgent basis\nΛB ⊂Λ\n𝑟\nReward function\n𝑟: 𝒜× 𝒪→R\n𝑣\nPerformance\n𝑣: ℋ× Λ × ℰ→[vmin, vmax]\n𝜎\nLearning rule\n𝜎: ℋ→ΛB\nΣ\nSet of all learning rules\nΣ\nSet of learning rules\nΣ ⊆Σ\nΛB ⊢𝑒Σ Λ\nΣ-generates\n∀Λ∃𝜎∈Σ∀ℎ∈¯\nℋ𝜆(ℎ) = 𝜎(ℎ)(ℎ)\nΛB ⊢𝑒Λ\nGenerates\n∃Σ⊆Σ ΛB ⊢𝑒Σ Λ\nΛB |=Σ Λ\nUniversally Σ-generates\n∀Λ∃𝜎∈Σ∀ℎ∈ℋ𝜆(ℎ) = 𝜎(ℎ)(ℎ)\nΛB |= Λ\nUniversally generates\n∃Σ⊆Σ ΛB |=Σ Λ\n𝜆⇝\n𝑒ΛB\nSometimes reaches\n∃ℎ∈¯\nℋ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ′) = 𝜆B(ℎℎ′)\n𝜆⇝\n/\n𝑒ΛB\nNever reaches\n¬(𝜆⇝\n𝑒ΛB)\n𝜆□⇝\n𝑒ΛB\nAlways reaches\n∀ℎ∈¯\nℋ∃𝑡∈N0∀ℎ◦∈¯\nℋ𝑡:∞\nℎ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ◦ℎ′) = 𝜆B(ℎℎ◦ℎ′)\nTable 1: A summary of notation.\nB\nProofs of Presented Results\nWe next provide proofs of each result from the paper. Our proofs make use of some extra notation:\nwe use ⇒as logical implication, and we use 𝒫(𝒳) to denote the power set of any set 𝒳. Lastly, we\nuse ∀𝒜⊆𝒳and ∃𝒜⊆𝒳as shorthand for ∀𝒜∈𝒫(𝒳) and ∃𝒜∈𝒫(𝒳) respectively.\n15\nB.1\nSection 3 Proofs\nOur first result is from Section 3 of the paper.\nTheorem 3.1. For any pair (𝜆, 𝑒), there exists infinitely many choices of a basis, ΛB, such that both\n(1) 𝜆∉ΛB, and (2) ΛB ⊢𝑒{𝜆}.\nProof of Theorem 3.1.\nChoose a fixed but arbitrary pair (𝜆, 𝑒). Then, enumerate the realizable histories, ℋ𝜆,𝑒, and\nlet ℎ1 denote the first element of this enumeration, ℎ2 the second, and so on.\nThen, we design a constructive procedure for a basis that, when repeatedly applied, induces\nan infinite enumeration of bases that satisfy the desired two properties. This constructive\nprocedure for the 𝑘-th basis will contain 𝑘+ 1 agents, where each agent is distinct from 𝜆,\nbut will produces the same action as the agent every 𝑘+ 1 elements of the history sequence,\nℎ1, ℎ2, . . ..\nFor the first (𝑘= 1) basis, we construct two agents. The first, 𝜆1\nB, chooses the same action\ndistribution as 𝜆on each even numbered history: 𝜆1\nB(ℎ𝑖) = 𝜆(ℎ𝑖). Then, this agent will choose\na different action distribution on the odd length histories: 𝜆1\nB(ℎ𝑖+1) ≠𝜆(ℎ𝑖+1), for 𝑖any even\nnatural number. The second agent, 𝜆2\nB will do the opposite to 𝜆1\nB: on each odd numbered\nhistory ℎ𝑖+1, 𝜆2\nB(ℎ𝑖+1) ≠𝜆(ℎ𝑖+1), but on every even numbered history, 𝜆2\nB(ℎ𝑖) = 𝜆(ℎ𝑖).\nObserve first that by construction, 𝜆≠𝜆1\nB, and 𝜆≠𝜆2\nB, since there exist histories where they\nchoose different action distributions. Next, observe that the basis, ΛB = {𝜆1\nB, 𝜆2\nB}, generates\n{𝜆} in 𝑒through the following set of learning rules, Σ: given any realizable history, ℎ∈ℋ𝜆,𝑒,\ncheck whether the history has an even or odd numbered index in the enumeration. If odd,\nchoose 𝜆1\nB, and if even, choose 𝜆2\nB.\nMore generally, this procedure can be applied for any 𝑘:\nΛ𝑘\nB = {𝜆1\nB, . . . , 𝜆𝑘+1\nB\n},\n𝜆𝑖\nB(ℎ) =\n\u001a\n𝜆(ℎ)\n[ℎ] == 𝑖,\n≠𝜆(ℎ)\notherwise,\n(B.1)\nwhere we use the notation [ℎ] == 𝑖to express the logical predicate asserting that the modulos\nof the index of ℎin the enumeration ℎ1, ℎ2, . . . is equal to 𝑖.\nFurther, ≠𝜆(ℎ) simply refers to any choice of action distribution that is unequal to 𝜆(ℎ).\nThus, for all natural numbers 𝑘≥2, we can construct a new basis consisting of 𝑘base agents\nthat generates 𝜆in 𝑒, but does not contain the agent itself. This completes the argument.\n□\nB.2\nSection 4 Proofs\nWe next present the proofs of results from Section 4.\nB.2.1\nTheorem 4.1: Properties of CRL\nWe begin with Theorem 4.1 that establishes basic properties of CRL.\nTheorem 4.1. Every instance of CRL (𝑒, 𝑣, Λ, ΛB) satisfies the following properties:\n1. If Λ ≠ΛB ∪Λ∗, there exists a Λ′\nB such that (1) Λ′\nB ⊢𝑒Λ, and (2) (𝑒, 𝑣, Λ, Λ′\nB) is not an\ninstance of CRL.\n2. No element of ΛB is optimal: ΛB ∩Λ∗= ∅.\n3. If |Λ| is finite, there exists an agent set, Λ◦such that |Λ◦| < |Λ| and Λ◦⊢𝑒Λ.\n4. If |Λ| is infinite, there exists an agent set, Λ◦such that Λ◦⊂Λ and Λ◦⊢𝑒Λ.\n16\nWe prove this result in the form of three lemmas, corresponding to each of the four points of the\ntheorem (with the third lemma, Lemma B.3, covering both points 3. and 4.). Some of the lemmas\nmake use of properties of generates and reaches that we establish later in Appendix C.\nLemma B.1. For all instances of CRL (𝑒, 𝑣, Λ, ΛB), if Λ ≠ΛB ∪Λ∗, then there exists a choice Λ′\nB\nsuch that (1) Λ′\nB ⊢𝑒Λ, and (2) (𝑒, 𝑣, Λ, Λ′\nB) is not an instance of CRL.\nProof of Lemma B.1.\nRecall that a tuple (𝑒, 𝑣, Λ, ΛB) is CRL just when all of the optimal agents Λ∗do not reach\nthe basis. Then, the result holds as a straightforward consequence of two facts. First, we can\nalways construct a new basis containing all of the optimal agents, Λ◦\nB = ΛB ∪Λ∗. Notice\nthat Λ◦\nB still generates Λ by property three of Theorem 4.2. Further, since both ΛB and Λ∗\nare each subsets of Λ, and by assumption Λ ≠ΛB ∪Λ∗(so there is at least one sub-optimal\nagent that is not in the basis), it follows that Λ◦\nB ⊂Λ. Second, by Proposition C.15, we know\nthat every element 𝜆◦\nB ∈Λ◦\nB will always reach the basis, 𝜆◦\nB □⇝\n𝑒Λ◦\nB. Therefore, in the tuple\n(𝑒, 𝑣, Λ, Λ◦\nB), each of the optimal agents will reach the basis, and therefore this is not an\ninstance of CRL.\n□\nLemma B.2. No element of ΛB is optimal: ΛB ∩Λ∗= ∅.\nProof of Lemma B.2.\nThe lemma follows as a combination of two facts.\nFirst, recall that, by definition of CRL, each optimal agent 𝜆∈Λ∗satisfies 𝜆∗⇝\n/\n𝑒ΛB.\nSecond, note that by Lemma B.11, we know that each 𝜆B ∈ΛB satisfies 𝜆B ⇝\n𝑒ΛB.\nTherefore, since sometimes reaches (⇝\n𝑒) and never reaches (⇝\n/\n𝑒) are negations of one another,\nwe conclude that no basis element can be optimal.\n□\nBefore stating the next lemma, we note that points (3.) and (4.) of Theorem 4.1 are simply expansions\nof the definition of a minimal agent set, which we define precisely in Definition C.4 and Definition C.5.\nLemma B.3. For any instance of CRL, Λ is not minimal.\nProof of Lemma B.3.\nWe first show that Λ cannot be minimal. To do so, we consider the cases where the rank\n(Definition C.3) of Λ is finite and infinite separately.\n(Finite Rank Λ.)\nIf rank(Λ) is finite and minimal, then it follows immediately that there is no agent set of\nsmaller rank that generates Λ. By consequence, since ΛB ⊂Λ and ΛB ⊢𝑒Λ, we conclude that\nΛ cannot be minimal. ✓\n(Infinite Rank Λ.)\nIf rank(Λ) is infinite and minimal, then there is no proper subset of Λ that uniformly generates\nΛ by definition. By consequence, since ΛB ⊂Λ and ΛB ⊢𝑒Λ, we conclude that Λ cannot be\nminimal. ✓\nThis completes the argument of both cases, and we conclude that for any instance of CRL, Λ\nis not minimal.\n□\n17\nB.2.2\nTheorem 4.2: Properties of Generates\nNext, we prove basic properties of generates.\nTheorem 4.2. The following properties hold of the generates operator:\n1. Generates is transitive: For any triple (Λ1, Λ2, Λ3) and 𝑒∈ℰ, if Λ1 ⊢𝑒Λ2 and Λ2 ⊢𝑒Λ3,\nthen Λ1 ⊢𝑒Λ3.\n2. Generates is not commutative: there exists a pair (Λ1, Λ2) and 𝑒∈ℰsuch that Λ1 ⊢𝑒Λ2,\nbut ¬(Λ2 ⊢𝑒Λ1).\n3. For all Λ and pair of agent bases (Λ1\nB, Λ2\nB) such that Λ1\nB ⊆Λ2\nB, if Λ1\nB ⊢𝑒Λ, then Λ2\nB ⊢𝑒Λ.\n4. For all Λ and 𝑒∈ℰ, Λ ⊢𝑒Λ.\n5. The decision problem, Given (𝑒, ΛB, Λ), output True iff ΛB ⊢𝑒Λ, is undecidable.\nThe proof of this theorem is spread across the next five lemmas below.\nThe fact that generates is transitive suggests that the basic tools of an agent set—paired with a set\nof learning rules—might be likened to an algebraic structure. We can draw a symmetry between an\nagent basis and the basis of a vector space: A vector space is comprised of all linear combinations\nof the basis, whereas Λ is comprised of all valid switches (according to the learning rules) between\nthe base agents. However, the fact that generates is not commutative (by point 2.) raises a natural\nquestion: are there choices of learning rules under which generates is commutative? An interesting\ndirection for future work is to explore this style of algebraic analysis on agents.\nLemma B.4. Generates is transitive: For any triple (Λ1, Λ2, Λ3) and 𝑒∈ℰ, if Λ1 ⊢𝑒Λ2 and Λ2 ⊢𝑒Λ3,\nthen Λ1 ⊢𝑒Λ3.\nProof of Lemma B.4.\nAssume Λ1 ⊢𝑒Λ2 and Λ2 ⊢𝑒Λ3. Then, by Proposition C.4 and the definition of the generates\noperator, we know that\n∀𝜆2∈Λ2∃𝜎1∈Σ1∀ℎ∈¯\nℋ𝜆2(ℎ) = 𝜎1(ℎ)(ℎ),\n(B.2)\n∀𝜆3∈Λ3∃𝜎2∈Σ2∀ℎ∈¯\nℋ𝜆3(ℎ) = 𝜎2(ℎ)(ℎ),\n(B.3)\nwhere Σ1 and Σ2 express the set of all learning rules over Λ1 and Λ2 respectively. By definition\nof a learning rule, 𝜎, we rewrite the above as follows,\n∀𝜆2∈Λ2∀ℎ∈¯\nℋ∃𝜆1∈Λ1 𝜆2(ℎ) = 𝜆1(ℎ),\n(B.4)\n∀𝜆3∈Λ3∀ℎ∈¯\nℋ∃𝜆2∈Λ2 𝜆3(ℎ) = 𝜆2(ℎ).\n(B.5)\nThen, consider a fixed but arbitrary 𝜆3 ∈Λ3. We construct a learning rule defined over Λ1\nas 𝜎1 : ℋ→Λ1 that induces an equivalent agent as follows. For each realizable history,\nℎ∈\n¯\nℋ, by Equation B.5 we know that there is an 𝜆2 such that 𝜆3(ℎ) = 𝜆2(ℎ), and by\nEquation B.4, there is an 𝜆1 such that 𝜆2(ℎ) = 𝜆1(ℎ). Then, set 𝜎1 : ℎ↦→𝜆1 such that\n𝜆1(ℎ) = 𝜆2(ℎ) = 𝜆3(ℎ)\nSince ℎand 𝜆3 were chosen arbitrarily, we conclude that\n∀𝜆3∈Λ3∀ℎ∈¯\nℋ∃𝜆1∈Λ1 𝜆3(ℎ) = 𝜆1(ℎ).\nBut, by the definition of Σ, this means there exists a learning rule such that\n∀𝜆3∈Λ3∃𝜎1∈Σ1∀ℎ∈¯\nℋ𝜆3(ℎ) = 𝜎1(ℎ)(ℎ).\nThis is exactly the definition of Σ-generation, and by Proposition C.4, we conclude Λ1 ⊢𝑒\nΛ3.\n□\n18\nLemma B.5. Generates is not commutative: there exists a pair (Λ1, Λ2) and 𝑒∈ℰsuch that\nΛ1 ⊢𝑒Λ2, but ¬(Λ2 ⊢𝑒Λ1).\nProof of Lemma B.5.\nThe result follows from a simple counterexample: consider the pair\nΛ1 = {𝜆𝑖: ℎ↦→𝑎1},\nΛ2 = {𝜆𝑖: ℎ↦→𝑎1, 𝜆𝑗: ℎ↦→𝑎2}.\nNote that since 𝜆𝑖is in both sets, and Λ1 is a singleton, we know that Λ2 ⊢𝑒Λ1 in any\nenvironment. But, by Proposition C.6, we know that Λ1 cannot generate Λ2.\n□\nLemma B.6. For all Λ and pair of agent bases (Λ1\nB, Λ2\nB) such that Λ1\nB ⊆Λ2\nB, if Λ1\nB ⊢𝑒Λ, then\nΛ2\nB ⊢𝑒Λ.\nProof of Lemma B.6.\nThe result follows as a natural consequence of the definition of generates. Recall that Λ1\nB ⊢𝑒Λ\njust when,\n∃Σ1⊆Σ Λ1\nB ⊢𝑒\nΣ1 Λ\n(B.6)\n≡∃Σ1⊆Σ ∃𝜎1∈Σ1∀ℎ∈¯\nℋ𝜆(ℎ) = 𝜆𝜎1(ℎ)\nB\n(ℎ),\n(B.7)\nwhere again 𝜆𝜎1(ℎ)\nB\n∈Λ1\nB is the base agent chosen by 𝜎1(ℎ). We use superscripts Σ1 and 𝜎1 to\nsignify that 𝜎1 is defined relative to Λ1\nB, that is, 𝜎1 : ℋ→Λ1\nB ∈Σ1.\nBut, since Λ1\nB ⊆Λ2\nB, we can define Σ2 = Σ1 and ensure that Λ2\nB ⊢𝑒\nΣ2 Λ, since the agent basis\nΛ1\nB was already sufficient to generate Λ. Therefore, we conclude that Λ2\nB ⊢𝑒Λ.\n□\nLemma B.7. For all Λ and 𝑒∈ℰ, Λ ⊢𝑒Λ.\nProof of Lemma B.7.\nThis is a direct consequence of Proposition C.18.\n□\nLemma B.8. The decision problem, AGENTSGENERATE, Given (𝑒, ΛB, Λ), output True iff ΛB ⊢𝑒Λ,\nis undecidable.\nProof of Lemma B.8.\nWe proceed as is typical of such results by reducing AGENTSGENERATE from the Halting\nProblem.\nIn particular, let 𝑚be a fixed but arbitrary Turing Machine, and 𝑤be a fixed but arbitrary\ninput to be given to machine 𝑚. Then, HALT defines the decision problem that outputs True\niff 𝑚halts on input 𝑤.\nWe construct an oracle for AGENTSGENERATE that can decide HALT as follows. Let (𝒜, 𝒪)\nbe an interface where the observation space is comprised of all configurations of machine 𝑚.\nThen, we consider a deterministic environment 𝑒that simply produces the next configuration\nof 𝑚when run on input 𝑤, based on the current tape contents, the state of 𝑚, and the location\nof the tape head. Note that all three of these elements are contained in a Turing Machine’s\nconfiguration, and that a single configuration indicates whether the Turing Machine is in a\nhalting state or not. Now, let the action space 𝒜consist of two actions, {𝑎no-op, 𝑎halt}. On\nexecution of 𝑎no-op no-op, the environment moves to the next configuration. On execution of\n𝑎halt, the machine halts. That is, we restrict ourselves to the singleton agent set, Λ, containing\n19\nthe agent 𝜆◦that outputs 𝑎halt directly following the machine entering a halting configuration,\nand 𝑎no-op otherwise:\n𝜆◦: ℎ𝑎𝑜↦→\n\u001a\n𝑎halt\n𝑜is a halting configiration,\n𝑎no-op\notherwise.\n,\nΛ = {𝜆◦}.\nUsing these ingredients, we take any instance of HALT, (𝑚, 𝑤), and consider the singleton\nagent basis: Λ1\nB = {𝑎no-op}.\nWe make one query to our AGENTSGENERATE oracle, and ask: Λ1\nB ⊢𝑒Λ. If it is True, then\nthe histories realizable by (𝜆◦, 𝑒) pair ensure that the single agent in Λ never emits the 𝑎halt\naction, and thus, 𝑚does not halt on 𝑤. If it is False, then there are realizable histories in 𝑒in\nwhich 𝑚halts on 𝑤. We thus use the oracle’s response directly to decide the given instance of\nHALT.\n□\nB.2.3\nTheorem 4.3: Properties of Reaches\nWe find many similar properties hold for reaches.\nTheorem 4.3. The following properties hold of the reaches operator:\n1. ⇝\n𝑒and ⇝\n/\n𝑒are not transitive.\n2. Sometimes reaches is not commutative: there exists a pair (Λ1, Λ2) and 𝑒∈ℰsuch that\n∀𝜆1∈Λ1 𝜆1 ⇝\n𝑒Λ2, but ∃𝜆2∈Λ2 𝜆2 ⇝\n/\n𝑒Λ1.\n3. For all pairs (Λ, 𝑒), if 𝜆∈Λ, then 𝜆⇝\n𝑒Λ.\n4. Every agent satisfies 𝜆⇝\n𝑒Λ in every environment.\n5. The decision problem, Given (𝑒, 𝜆, Λ), output True iff 𝜆⇝\n𝑒Λ, is undecidable.\nAgain, we prove this result through five lemmas that correspond to each of the above properties.\nMany of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding\nwhether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate\nthat the majority of decision problems related to determining properties of arbitrary agent sets will\nbe undecidable, though it is still worth making these arguments carefully. Moreover, there may be\ninteresting special cases in which these decision problems are decidable (and perhaps, efficiently\nso). Identifying these special cases and their corresponding efficient algorithms is another interesting\ndirection for future work.\nLemma B.9. ⇝\n𝑒and ⇝\n/\n𝑒are not transitive.\nProof of Lemma B.9.\nWe construct two counterexamples, one for each of “sometimes reaches” (⇝\n𝑒) and “never\nreaches” (⇝\n/\n𝑒).\nCounterexample: Sometimes Reaches.\nTo do so, we begin with a tuple (𝑒, Λ1, Λ2, Λ3)\nsuch that both\n∀Λ1∈Λ1 𝜆1 ⇝\n𝑒Λ1,\n∀Λ2∈Λ2 𝜆2 ⇝\n𝑒Λ2.\nWe will show that there is an agent, 𝜆\n1 ∈Λ1, such that 𝜆\n1 ⇝\n/\n𝑒\nΛ3, thus illustrating that\nsometimes reaches is not guaranteed to be transitive. The basic idea is that sometimes reaches\nonly requires an agent stop its search on one realizable history. So, 𝜆1 ⇝\n𝑒Λ2 might happen\non some history ℎ, but each 𝜆2 ∈Λ2 might only reach Λ3 on an entirely different history. As\na result, reaching Λ2 is not enough to ensure the agent also reaches Λ3.\nIn more detail, the agent sets of the counterexample are as follows. Let 𝒜= {𝑎1, 𝑎2} and\n𝒪= {𝑜1, 𝑜2}. Let Λ2 be all agents that, after ten timesteps, always take 𝑎2. 𝜆\n1 is simple: it\n20\nalways takes 𝑎1, except on one realizable history, ℎ◦, (and all of the realizable successors\nof ℎ◦, ℋ𝜆,𝑒\nℎ◦), where it switches to taking 𝑎2 after ten timesteps. Clearly 𝜆1 ⇝\n𝑒\nΛ2, since\nafter ten timesteps, we know there will be some 𝜆2 such that 𝜆\n1(ℎ◦ℎ′) = 𝜆2(ℎ◦ℎ′) for all\nrealizable history suffixes ℎ′. Now, by assumption, we know that 𝜆2 ⇝\n𝑒Λ3. This ensures\nthere is a single realizable history ℎsuch that there is an 𝜆3 where 𝜆2(ℎℎ′) = 𝜆3(ℎℎ′) for any\nrealizable suffix ℎ′. To finish the counterexample, we simply note that this realizable ℎcan be\ndifferent from ℎ◦and all of its successors. For example, ℎ◦might be the history containing\nonly 𝑜1 for the first ten timesteps, while ℎcould be the history containing only 𝑜2 for\nthe first ten timesteps. Thus, this 𝜆1 never reaches Λ3, and we conclude the counterexample. ✓\nCounterexample: Never Reaches.\nThe instance for never reaches is simple: Let 𝒜=\n{𝑎1, 𝑎2, 𝑎3}, and Λ1 = Λ3. Suppose all agents in Λ1 (and thus Λ3) only choose actions 𝑎1\nand 𝑎3. Let Λ2 be a singleton, Λ2 = {𝜆2} such that 𝜆2 : ℎ↦→𝑎2. Clearly, every 𝜆1 ∈Λ1 will\nnever reach Λ2, since none of them ever choose 𝑎2. Similarly, 𝜆2 will never reach Λ3, since\nno agents in Λ3 choose 𝑎2. However, by Proposition C.15 and the assumption that Λ1 = Λ3,\nwe know ∀𝜆1∈Λ1 𝜆1 □⇝\n𝑒Λ3. This directly violates transitivity. ✓\nThis completes the argument for all three cases, and we conclude.\n□\nLemma B.10. Sometimes reaches is not commutative: there exists a pair (Λ1, Λ2) and 𝑒∈ℰsuch\nthat ∀𝜆1∈Λ1 𝜆1 ⇝\n𝑒Λ2, but ∃𝜆2∈Λ2 𝜆2 ⇝\n/\n𝑒Λ1.\nProof of Lemma B.10.\nThe result holds as a straightforward consequence of the following counterexample. Consider\nthe pair of agent sets\nΛ1 = {𝜆𝑖: ℎ↦→𝑎1},\nΛ2 = {𝜆𝑖: ℎ↦→𝑎1, 𝜆𝑗: ℎ↦→𝑎2}.\nNote that since 𝜆𝑖is in both sets, and Λ1 is a singleton, we know that 𝜆⇝\n𝑒\nΛ1 in any\nenvironment by Lemma B.11. But, clearly 𝜆𝑗never reaches Λ1, since no agent in Λ1 ever\nchooses 𝑎1.\n□\nLemma B.11. For all pairs (Λ, 𝑒), if 𝜆∈Λ, then 𝜆⇝\n𝑒Λ.\nProof of Lemma B.11.\nThe proposition is straightforward, as any 𝜆∈Λ will be equivalent to itself in behavior for all\nhistories.\n□\nLemma B.12. Every agent satisfies 𝜆⇝\n𝑒Λ in every environment.\nProof of Lemma B.12.\nThis is again a direct consequence of Proposition C.18.\n□\nLemma B.13. The decision problem, AGENTREACHES, Given (𝑒, 𝜆, Λ), output True iff 𝜆⇝\n𝑒Λ, is\nundecidable.\nProof of Lemma B.13.\nWe again proceed by reducing AGENTREACHES from the Halting Problem.\n21\nIn particular, let 𝑚be a fixed but arbitrary Turing Machine, and 𝑤be a fixed but arbitrary\ninput to be given to machine 𝑚. Then, HALT defines the decision problem that outputs True\niff 𝑚halts on input 𝑤.\nWe construct an oracle for AGENTREACHES that can decide HALT as follows. Consider\nthe same observation space used in the proof of Lemma B.8: Let 𝒪be comprised of all\nconfigurations of machine 𝑚. Then, sequences of observations are simply evolution of\ndifferent Turing Machines processing possible inputs. We consider an action space, 𝒜=\n{𝑎halted, 𝑎not−yet}, where agents simply report whether the history so far contains a halting\nconfiguration.\nThen, we consider a deterministic environment 𝑒that simply produces the next configuration of\n𝑚when run on input 𝑤, based on the current tape contents, the state of 𝑚, and the location of\nthe tape head. Note again that all three of these elements are contained in a Turing Machine’s\nconfiguration.\nUsing these ingredients, we take any instance of HALT, (𝑚, 𝑤), and build the singleton agent\nset ΛB containing only the agent 𝜆halted : ℎ↦→𝑎halted that always reports the machine as\nhaving halted. We then consider whether the agent that outputs 𝑎not−yet indefinitely until 𝑚\nreports halting, at which point the agent switches to 𝑎halted.\nWe make one query to our AGENTREACHES oracle, and ask: 𝜆⇝\n𝑒ΛB. If it is True, then the\nbranching agent eventually becomes equivalent to 𝜆halted in that they both indefinitely output\n𝑎halted on at least one realizable history. Since 𝑒is deterministic, we know this equivalence\nholds across all histories. If the query reports False, then there is no future in 𝑒in which 𝑚\nhalts on 𝑤, otherwise the agent would become equivalent to 𝜆halted. We thus use the oracle’s\nresponse directly to decide the given instance of HALT.\n□\nC\nAdditional Analysis\nFinally, we present a variety of additional results about agents and the generates and reaches operators.\nC.1\nAdditional Analysis: Generates\nWe first highlight simple properties of the generates operator. Many of our results build around the\nnotion of uniform generation, a variant of the generates operator in which a basis generates an agent\nset in every environment. We define this operator precisely as follows.\nDefinition C.1. Let Σ be a set of learning rules over some basis ΛB. We say that a set Λ is uniformly\n𝚺-generated by ΛB, denoted ΛB |=Σ Λ, if and only if\n∀𝜆∈Λ∃𝜎∈Σ∀ℎ∈ℋ𝜆(ℎ) = 𝜎(ℎ)(ℎ).\n(C.1)\nDefinition C.2. We say a basis ΛB uniformly generates Λ, denoted ΛB |= Λ, if and only if\n∃Σ⊆Σ ΛB |=Σ Λ.\n(C.2)\nWe will first show that uniform generation entails generation in a particular environment. As a\nconsequence, when we prove that certain properties hold of uniform generation, we can typically also\nconclude that the properties hold for generation as well, though there is some subtlety as to when\nexactly this implication will allow results about |= to apply directly to ⊢𝑒.\nProposition C.1. For any (ΛB, Λ) pair, if ΛB |= Λ, then for all 𝑒∈ℰ, ΛB ⊢𝑒Λ.\nProof of Proposition C.1.\nRecall that in the definition of uniform generation, ΛB |= Λ, we require,\n∃Σ⊆Σ∀𝜆∈Λ∃𝜎∈Σ∀ℎ∈ℋ𝜆(ℎ) = 𝜎(ℎ)(ℎ).\n(C.3)\nNow, contrast this with generates with respect to a specific environment 𝑒,\n∃Σ⊆Σ∀𝜆∈Λ∃𝜎∈Σ∀ℎ∈¯\nℋ𝜆(ℎ) = 𝜎(ℎ)(ℎ).\n(C.4)\n22\nThe only difference in the definitions is that the set of histories quantified over is ℋin the\nformer, and ¯\nℋ= ℋ𝜆,𝑒in the latter.\nSince ¯\nℋ⊆ℋfor any choice of environment 𝑒, we can conclude that when Equation C.3, it\nis also the case that Equation C.4 holds, too. Therefore, ΛB |= Λ ⇒ΛB ⊢𝑒Λ for any 𝑒.\n□\nWe next show that the subset relation implies generation.\nProposition C.2. Any pair of agent sets (Λsmall, Λbig) such that Λsmall ⊆Λbig satisfies\nΛbig |= Λsmall.\n(C.5)\nProof of Proposition C.2.\nThe result follows from the combination of two facts. First, that all agent sets generate\nthemselves. That is, for arbitrary Λ, we know that Λ ⊢𝑒Λ, since the trivial set of learning rules,\nΣtr = {𝜎𝑖: ℎ↦→𝜆𝑖, ∀𝜆𝑖∈Λ},\n(C.6)\nthat never switches between agents is sufficient to generate the agent set.\nSecond, observe that removing an agent from the generated set has no effect on the generates\noperator. That is, let Λ′ = Λ \\ 𝜆, for fixed but arbitrary 𝜆∈Λ. We see that Λ ⊢𝑒Λ′, since Σtr\nis sufficient to generate Λ′, too. By inducting over all removals of agents from Λ, we reach\nour conclusion.\n□\nNext, we establish properties about the sets of learning rules that correspond to the generates operator.\nProposition C.3. For any (ΛB, Σ, Λ) such that ΛB |=Σ Λ, it holds that\n|Λ| ≤|Σ|.\n(C.7)\nProof of Proposition C.3.\nWe proceed toward contradiction, and assume |Λ| > |Σ|. Then, there is at least one learning\nrule 𝜎∈Σ that corresponds to two or more distinct agents in Λ. Call this element 𝜎◦, and\nwithout loss of generality let 𝜆1 and 𝜆2 be two distinct agents that are each generated by 𝜎◦in\nthe sense that,\n𝜆1(ℎ) = 𝜎◦(ℎ)(ℎ),\n𝜆2(ℎ) = 𝜎◦(ℎ)(ℎ),\n(C.8)\nfor every ℎ∈ℋ. But, by the distinctness of 𝜆1 and 𝜆2, there must exist a history ℎin\nwhich 𝜆1(ℎ) ≠𝜆2(ℎ). We now arrive at a contradiction as such a history cannot exist: By\nEquation C.8, we know that 𝜆1(ℎ) = 𝜎◦(ℎ)(ℎ) = 𝜆2(ℎ) for all ℎ.\n□\nWe see that the universal learning rules, Σ, is the strongest in the following sense.\nProposition C.4. For any basis ΛB and agent set Λ, exactly one of the two following properties hold:\n1. The agent basis ΛB uniformly generates Λ under the set of all learning rules: ΛB |=Σ Λ.\n2. There is no set of learning rules for which the basis Σ-uniformly generates the agent set:\n¬∃Σ⊆Σ ΛB |=Σ Λ.\nProof of Proposition C.4.\nThe proof follows from the law of excluded middle. That is, for any set of learning rules\nΣ, either it generates Λ or it does not. If it does generate Λ, by Lemma B.6 so does Σ. By\nconsequence, if Σ does not generate Λ, neither do any of its subsets.\n□\nFurthermore, uniform generation is also transitive.\nTheorem C.5. Uniform generates is transitive: For any triple (Λ1, Λ2, Λ3), if Λ1 |= Λ2 and\nΛ2 |= Λ3, then Λ1 |= Λ3.\n23\nProof of Theorem C.5.\nAssume Λ1 |= Λ2 and Λ2 |= Λ3. Then, by Proposition C.4 and the definition of the uniform\ngenerates operator, we know that\n∀𝜆2∈Λ2∃𝜎1∈Σ1∀ℎ∈ℋ𝜆2(ℎ) = 𝜎1(ℎ)(ℎ),\n(C.9)\n∀𝜆3∈Λ3∃𝜎2∈Σ2∀ℎ∈ℋ𝜆3(ℎ) = 𝜎2(ℎ)(ℎ),\n(C.10)\nwhere Σ1 and Σ2 express the set of all learning rules over Λ1 and Λ2 respectively. By definition\nof a learning rule, 𝜎, we rewrite the above as follows,\n∀𝜆2∈Λ2∀ℎ∈ℋ∃𝜆1∈Λ1 𝜆2(ℎ) = 𝜆1(ℎ),\n(C.11)\n∀𝜆3∈Λ3∀ℎ∈ℋ∃𝜆2∈Λ2 𝜆3(ℎ) = 𝜆2(ℎ).\n(C.12)\nThen, consider a fixed but arbitrary 𝜆3 ∈Λ3. We construct a learning rule defined over Λ1\nas 𝜎1 : ℋ→Λ1 that induces an equivalent agent as follows. For each history, ℎ∈ℋ, by\nEquation C.12 we know that there is an 𝜆2 such that 𝜆3(ℎ) = 𝜆2(ℎ), and by Equation C.11,\nthere is an 𝜆1 such that 𝜆2(ℎ) = 𝜆1(ℎ). Then, set 𝜎1 : ℎ↦→𝜆1 such that 𝜆1(ℎ) = 𝜆2(ℎ) =\n𝜆3(ℎ). Since ℎand 𝜆3 were chosen arbitrarily, we conclude that\n∀𝜆3∈Λ3∀ℎ∈ℋ∃𝜆1∈Λ1 𝜆3(ℎ) = 𝜆1(ℎ).\nBut, by the definition of Σ, this means there exists a learning rule such that\n∀𝜆3∈Λ3∃𝜎1∈Σ1∀ℎ∈ℋ𝜆3(ℎ) = 𝜎1(ℎ)(ℎ).\nThis is exactly the definition of Σ-uniform generation, and by Proposition C.4, we conclude\nΛ1 |= Λ3.\n□\nNext, we show that a singleton basis only generates itself.\nProposition C.6. Any singleton basis, ΛB = {𝜆}, only uniformly generates itself.\nProof of Proposition C.6.\nNote that generates requires switching between base agents. With only a single agent, there\ncannot be any switching, and thus, the only agent that can be described as switching amongst\nthe elements of the singleton set ΛB = {𝜆} is the set itself.\n□\nC.1.1\nRank and Minimal Bases\nAs discussed in the paper, one natural reaction to the concept of an agent basis is to ask how we\ncan justify different choices of a basis. And, if we cannot, then perhaps the concept of an agent\nbasis is disruptive, rather than illuminating. In the main text, we suggest that in many situations, the\nchoice of basis is made by the constraints imposed by the problem, such as the available memory.\nHowever, there are some objective properties of different bases that can help us to evaluate possible\nchoices of a suitable basis. For instance, some bases are minimal in the sense that they cannot be\nmade smaller while still retaining the same expressive power (that is, while generating the same agent\nsets). Identifying such minimal sets may be useful, as it is likely that there is good reason to consider\nonly the most compressed agent bases.\nTo make these intuitions concrete, we introduce the rank of an agent set.\nDefinition C.3. The rank of an agent set, rank(Λ), is the size of the smallest agent basis that\nuniformly generates it:\nrank(Λ) = min\nΛB⊂Λ |ΛB|\ns.t.\nΛB |= Λ.\n(C.13)\n24\nFor example, the agent set,\nΛ = {𝜆0 : ℎ↦→𝑎0,\n(C.14)\n𝜆1 : ℎ↦→𝑎1,\n𝜆2 : ℎ↦→\n\u001a\n𝑎0\n|ℎ| mod 2 = 0,\n𝑎1\n|ℎ| mod 2 = 1,\n},\nhas rank(Λ) = 2, since the basis,\nΛB = {𝜆0\nB : ℎ↦→𝑎0, 𝜆1\nB : ℎ↦→𝑎1},\nuniformly generates Λ, and there is no size-one basis that uniformly generates Λ by Proposition C.6.\nUsing the notion of an agent set’s rank, we now introduce the concept of a minimal basis. We suggest\nthat minimal bases are particular important, as they contain no redundancy with respect to their\nexpressive power. Concretely, we define a minimal basis in two slightly different ways depending on\nwhether the basis has finite or infinite rank. In the finite case, we say a basis is minimal if there is no\nbasis of lower rank that generates it.\nDefinition C.4. An agent basis ΛB with finite rank is said to be minimal just when there is no smaller\nbasis that generates it,\n∀Λ′\nB⊂Λ Λ′\nB |= ΛB ⇒rank(Λ′\nB) ≥rank(ΛB).\n(C.15)\nIn the infinite case, as all infinite rank bases will have the same effective size, we instead consider a\nnotion of minimiality based on whether any elements can be removed from the basis without changing\nits expressive power.\nDefinition C.5. An agent basis ΛB with infinite rank is said to be minimal just when no proper\nsubset of ΛB uniformly generates ΛB.\n∀Λ′\nB⊆ΛB Λ′\nB |= ΛB ⇒Λ′\nB = ΛB.\n(C.16)\nNotably, this way of looking at minimal bases will also apply to finite rank agent bases as a direct\nconsequence of the definition of a minimal finite rank basis. However, we still provide both definitions,\nas a finite rank basis may not contain a subset that generates it, but there may exist a lower rank basis\nthat generates it.\nCorollary C.7. As a Corollary of Proposition C.2 and Definition C.4, for any minimal agent basis\nΛB, there is no proper subset of ΛB that generates ΛB.\nRegardless of whether an agent basis has finite or infinite rank, we say the basis is a minimal basis of\nan agent set Λ just when the basis uniformly generates Λ and the basis is minimal.\nDefinition C.6. For any Λ, a minimal basis of 𝚲is any basis ΛB that is both (1) minimal, and (2)\nΛB |= Λ.\nA natural question arises as to whether the minimal basis of any agent set Λ is unique. We answer\nthis question in the negative.\nProposition C.8. The minimal basis of a set of agents is not necessarily unique.\nProof of Proposition C.8.\nTo prove the claim, we construct an instance of an agent set with two distinct minimal bases.\nLet 𝒜= {𝑎0, 𝑎1}, and 𝒪= {𝑜0}. We consider the agent set containing four agents. The first\ntwo map every history to 𝑎0 and 𝑎1, respectively, while the second two alternate between 𝑎0\n25\nand 𝑎1 depending on whether the history is of odd or even length:\nΛ = {𝜆0 : ℎ↦→𝑎0,\n(C.17)\n𝜆1 : ℎ↦→𝑎1,\n𝜆2 : ℎ↦→\n\u001a\n𝑎0\n|ℎ| mod 2 = 0,\n𝑎1\n|ℎ| mod 2 = 1,\n𝜆3 : ℎ↦→\n\u001a\n𝑎0\n|ℎ| mod 2 = 1,\n𝑎1\n|ℎ| mod 2 = 0,\n}.\nNote that there are two distinct subsets that each universally generate Λ:\nΛ0,1\nB = {𝜆0\nB, 𝜆1\nB},\nΛ2,3\nB = {𝜆2\nB, 𝜆3\nB}.\n(C.18)\nNext notice that there cannot be a singleton basis by Proposition C.6, and thus, both Λ0,1\nB and\nΛ2,3\nB satisfy (1) |Λ0,1\nB | = |Λ2,3\nB | = rank(Λ), and (2) both Λ0,1\nB\n|= Λ, Λ2,3\nB\n|= Λ.\n□\nBeyond the lack of redundancy of a basis, we may also be interested in their expressive power. For\ninstance, if we compare two minimal bases, Λ1\nB and Λ2\nB, how might we justify which to use? To\naddress this question, we consider another desirable property of a basis: universality.\nDefinition C.7. An agent basis ΛB is universal if ΛB |= Λ.\nClearly, it might be desirable to work with a universal basis, as doing so ensures that the set of agents\nwe consider in our design space is as rich as possible. We next show that there is at least one natural\nbasis that is both minimal and universal.\nProposition C.9. The basis,\nΛ◦\nB = {𝜆: 𝒪→Δ(𝒜)},\n(C.19)\nis a minimal universal basis:\n1. Λ◦\nB |= Λ: The basis uniformly generates the set of all agents.\n2. Λ◦\nB is minimal.\nProof of Proposition C.9.\nWe prove each property separately.\n1. Λ◦\nB |= Λ\nFirst, we show that the basis is universal: Λ◦\nB |= Λ. Recall that this amounts to showing that,\n∀𝜆∈Λ∀ℎ∈ℋ∃𝜆′∈Λ◦\nB 𝜆(ℎ) = 𝜆′(ℎ).\n(C.20)\nLet 𝜆∈Λ and ℎ∈ℋbe fixed but arbitrary. Now, let us label the action distribution produced\nby 𝜆(ℎ) as 𝑝𝜆(ℎ). Let 𝑜refer to the last observation contained in ℎ(or ∅if ℎ= ℎ0 = ∅). Now,\nconstruct the agent 𝜆◦\nB : 𝑜↦→𝑝𝜆(ℎ). By construction of Λ◦\nB, this agent is guaranteed to be a\nmember of Λ◦\nB, and furthermore, we know that 𝜆◦\nB produces the same output as 𝜆on ℎ. Since\nboth 𝜆and ℎwere chosen arbitrarily, the construction will work for any choice of 𝜆and ℎ,\nand we conclude that at every history, there exists a basis agent Λ◦\nB ∈Λ◦\nB that produces the\nsame probability distribution over actions as any given agent. Thus, the first property holds. ✓\n2. Λ◦\nB is minimal.\n26\nSecond, we show that Λ◦\nB is a minimal basis of Λ. Recall that since rank(Λ◦\nB) = ∞, the\ndefinition of a minimal basis means that:\n∀ΛB⊆Λ◦\nB ΛB |= Λ ⇒ΛB = Λ◦\nB.\n(C.21)\nTo do so, fix an arbitrary proper subset of ΛB ∈𝒫(Λ◦\nB). Notice that since ΛB is a proper\nsubset, there exists a non-empty set ΛB such that,\nΛB ∪ΛB = Λ◦\nB.\nNow, we show that ΛB cannot uniformly generate Λ by constructing an agent from ΛB. In\nparticular, consider the first element of ΛB, which, by construction of Λ◦\nB, is some mapping\nfrom 𝒪to a choice of probability distribution over 𝒜. Let us refer to this agent’s output\nprobability distribution over actions as 𝑝. Notice that there cannot exist an agent in Λ◦\nB that\nchooses 𝑝, otherwise ΛB would not be a proper subset of Λ◦\nB. Notice further that in the set of\nall agents, there are infinitely many agents that output 𝑝in at least one history. We conclude\nthat ΛB cannot uniformly generate Λ, as it does not contain any base element that produces 𝑝.\nThe set ΛB was chosen arbitrarily, and thus the claim holds for any proper subset of Λ◦\nB, and\nwe conclude. ✓\nThis completes the proof of both statements.\n□\nCorollary C.10. As a direct consequence of Proposition C.9, every universal basis has infinite rank.\nC.1.2\nOrthogonal and Parallel Agent Sets\nDrawing inspiration from vector spaces, we introduce notions of orthogonal and parallel agent bases\naccording to the agent sets they generate.\nDefinition C.8. A pair of agent bases (Λ1\nB, Λ2\nB) are orthogonal if any pair (Λ1, Λ2) they each\nuniformly generate\nΛ1\nB |= Λ1,\nΛ2\nB |= Λ2,\n(C.22)\nsatisfy\nΛ1 ∩Λ2 = ∅.\n(C.23)\nNaturally this definition can be modified to account for environment-relative generation (⊢𝑒), or to be\ndefined with respect to a particular set of learning rules in which case two bases are orthogonal with\nrespect to the learning rule set just when they generate different agent sets under the given learning\nrules. As with the variants of the two operators, we believe the details of such formalisms are easy to\nproduce.\nA few properties hold of any pair of orthogonal bases.\nProposition C.11. If two bases Λ1\nB, Λ2\nB are orthogonal, then the following properties hold:\n1. Λ1\nB ∩Λ2\nB = ∅.\n2. Neither Λ1\nB nor Λ2\nB are universal.\nProof of Proposition C.11.\nWe prove each property independently.\n1. Λ1\nB ∩Λ2\nB = ∅\nWe proceed toward contradiction. That is, suppose that both Λ1\nB is orthogonal to Λ2\nB, and that\nΛ1\nB ∩Λ2\nB ≠∅. Then, by the latter property, there is at least one agent that is an element of\n27\nboth bases. Call this agent 𝜆◦\nB. It follows that the set Λ◦\nB = {𝜆◦\nB} is a subset of both Λ1\nB and\nΛ2\nB. By Proposition C.2, it follows that Λ1\nB |= Λ◦\nB and Λ2\nB |= Λ◦\nB. But this contradicts the fact\nthat Λ1\nB is orthogonal to Λ2\nB, and so we conclude. ✓\n2. Neither Λ1\nB nor Λ2\nB are universal.\nWe again proceed toward contradiction. Suppose without loss of generality that Λ1\nB is\nuniversal. Then, we know Λ1\nB |= Λ. Now, we consider two cases: either Λ2\nB generates some\nnon-empty set, Λ2, or it does not generate any sets. If it generates a set Λ2, then we arrive at a\ncontradiction as Λ2 ∩Λ ≠∅, which violates the definition of orthogonal bases. If if does not\ngenerate a set, this violates the definition of a basis, as any basis is by construction non-empty,\nand we know that containing even a single element is sufficient to generate at least one agent\nset by Proposition C.6. Therefore, in either of the two cases, we arrive at a contradiction, and\nthus conclude the argument. ✓\nThis concludes the proof of each statement.\n□\nCorollary C.12. For any non-universal agent basis ΛB, there exists an orthogonal agent basis, Λ†\nB.\nConversely, two agent bases Λ1\nB, Λ2\nB are parallel just when they generate the same agent sets.\nDefinition C.9. A pair of agent bases (Λ1\nB, Λ2\nB) are parallel if for every agent set Λ, Λ1\nB |= Λ if and\nonly if Λ2\nB |= Λ.\nProposition C.13. If two bases Λ1\nB, Λ2\nB are parallel, then the following properties hold:\n1. Both Λ1\nB |= Λ2\nB and Λ2\nB |= Λ1\nB.\n2. rank(Λ1\nB) = rank(Λ2\nB).\n3. Λ1\nB is universal if and only if Λ2\nB is universal.\nProof of Proposition C.13.\nWe prove each property separately.\n1. Both Λ1\nB |= Λ2\nB and Λ2\nB |= Λ1\nB.\nThe claim follows directly from the definition of parallel bases. An agent set Λ is uniformly\ngenerated by Λ1\nB if and only if it is uniformly generated by Λ2\nB. Since by Proposition C.2 we\nknow both Λ1\nB |= Λ1\nB and Λ2\nB |= Λ2\nB, we conclude that both Λ1\nB |= Λ2\nB and Λ2\nB |= Λ1\nB. ✓\n2. rank(Λ1\nB) = rank(Λ2\nB).\nRecall that the definition of rank refers to the size of the smallest basis that uniformly generates\nit,\nrank(Λ) = min\nΛB⊂Λ |ΛB|\ns.t.\nΛB |= Λ.\nNow, note that by property (1.) of the proposition, both sets uniformly generate each other.\nTherefore, we know that\nrank(Λ1\nB) ≤min \b\n|Λ1\nB|, |Λ2\nB|\t\n,\nrank(Λ2\nB) ≤min \b\n|Λ1\nB|, |Λ2\nB|\t\n,\nsince the smallest set that generates each basis is no larger than the basis itself, or the other\nbasis.\n28\n3. Λ1\nB is universal if and only if Λ2\nB is universal.\nThe claim again follows by combining the definitions of universality and parallel: If Λ1\nB\nis universal, then by definition of parallel bases, Λ2\nB must uniformly generate all the same\nagent sets including Λ, and therefore Λ2\nB is universal, too. Now, if Λ1\nB is not universal, then it\ndoes not uniformly generate Λ. By the definition of parallel bases, we conclude Λ2\nB does not\ngenerate Λ as well. Both directions hold for each labeling of the two bases without loss of\ngenerality, and we conclude. ✓\nThis completes the argument for each property, and we conclude.\n□\nC.2\nAnalysis: Reaches\nWe now establish other properties of the reaches operator. Several of these results are based on a\nthird modality of the reaches operator: always reaches, in which an agent eventually reaches an agent\nbasis in all histories realizable in a given environment. We define this precisely as follows.\nDefinition C.10. We say agent 𝜆∈Λ always reaches ΛB, denoted 𝜆□⇝\n𝑒ΛB, if and only if\n∀ℎ∈¯\nℋ∃𝑡∈N0∀ℎ◦∈¯\nℋ𝑡:∞\nℎ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ◦ℎ′) = 𝜆B(ℎℎ◦ℎ′).\n(C.24)\nThe nested quantifiers allows the agent to become equivalent to different base behaviors depending on\nthe evolution of the interaction stream. For example, in an environment that flips a coin to determine\nwhether 𝑎heads or 𝑎tails is optimal, the 𝜆might output 𝑎heads indefinitely if the coin is heads, but 𝑎tails\notherwise. In this case, such an agent will still always reach the basis ΛB = {𝜆1\nB : ℎ↦→𝑎heads, 𝜆2\nB :\nℎ↦→𝑎tails}. Notice that we here make use of the notation, ¯\nℋ𝑡:∞\nℎ\n, which refers to all history suffixes\nof length 𝑡or greater, defined precisely as\n¯\nℋ𝑡:∞\nℎ\n= {ℎ′ ∈¯\nℋℎ: |ℎ′| ≥𝑡}\n(C.25)\nWe first show that the always reaches operator implies sometimes reaches.\nProposition C.14. If 𝜆□⇝\n𝑒Λ, then 𝜆⇝\n𝑒Λ.\nProof of Proposition C.14.\nAssume 𝜆□⇝\n𝑒Λ. That is, expanding the definition of always reaches, we assume\n∀ℎ∈¯\nℋ∃𝑡∈N0∀ℎ◦∈¯\nℋ𝑡:∞\nℎ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ◦ℎ′) = 𝜆B(ℎℎ◦ℎ′).\n(C.26)\nFurther recall the definition of can reach 𝜆⇝ΛB is as follows\n∃ℎ∈¯\nℋ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ′) = 𝜆B(ℎℎ′).\n(C.27)\nThen, the claim follows quite naturally: pick any realizable history ℎ∈\n¯\nℋ. By our initial\nassumption that 𝜆□⇝\n𝑒Λ, it follows (by Equation C.26) that there is a time 𝑡and a realizable\nhistory suffix ℎ◦for which\n∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎℎ◦ℎ′) = 𝜆B(ℎℎ◦ℎ′).\nBy construction of ℎℎ◦∈¯\nℋℎ, we know ℎℎ◦is a realizable history. Therefore, there exists a\nrealizable history, ℎ∗= ℎℎ◦, for which ∃𝜆B∈ΛB∀ℎ′∈¯\nℋℎ𝜆(ℎ∗ℎ′) = 𝜆B(ℎ∗ℎ′) holds. But this is\nexactly the definition of can reach, and therefore, we conclude the argument.\n□\nNext, we highlight the fact that every agent in a basis also reaches that basis.\nProposition C.15. For any agent set Λ, it holds that 𝜆□⇝\n𝑒Λ for every 𝜆∈Λ.\nProof of Proposition C.15.\n29\nThe proposition is straightforward, as any 𝜆∈Λ will be equivalent to itself in behavior for all\nhistories.\n□\nCorollary C.16. As a corollary of Proposition C.15, any pair of agent sets (Λsmall, Λbig) where\nΛsmall ⊆Λbig, satisfies\n∀𝜆∈Λsmall 𝜆□⇝\n𝑒Λbig.\n(C.28)\nWe further show that, unlike sometimes and never reaches, always reaches is transitive.\nProposition C.17. Always reaches is transitive.\nProof of Proposition C.17.\nWe proceed by assuming that both ∀𝜆1∈Λ1 𝜆1 □⇝\n𝑒Λ2 and ∀𝜆2∈Λ2 𝜆2 □⇝\n𝑒Λ3 and show that it\nmust follow that ∀𝜆1∈Λ1 𝜆1 □⇝\n𝑒Λ3. To do so, pick a fixed but arbitrary 𝜆1 ∈Λ1, and expand\n𝜆1 □⇝\n𝑒Λ2 as\n∀ℎ∈¯\nℋ∃𝑡∈N0∀ℎ◦∈¯\nℋ𝑡:∞\nℎ∃𝜆2∈Λ2∀ℎ′∈¯\nℋℎ𝜆1(ℎℎ◦ℎ′) = 𝜆2(ℎℎ◦ℎ′).\nNow, consider for any realizable history ℎℎ◦ℎ′, we know that the corresponding 𝜆2 that pro-\nduces the same action distribution as 𝜆1 also satisfies 𝜆2 □⇝\n𝑒Λ3. Thus, there must exist some\ntime ¯𝑡at which, any realizable history ¯ℎ¯ℎ◦, will satisfy ∃𝜆3∈Λ3 ∀¯ℎ′∈¯¯\nℋ𝜆2( ¯ℎ¯ℎ◦¯ℎ′) = 𝜆3( ¯ℎ¯ℎ◦¯ℎ′.\nBut then there exists a time (¯𝑡), that ensures every 𝜆2 ∈Λ2 will have a corresponding 𝜆3 ∈Λ3\nwith the same action distribution at all subsequent realizable histories.\nTherefore,\n∀ℎ∈¯\nℋ∃𝑡′∈N0∀ℎ◦∈¯\nℋ𝑡′:∞\nℎ\n∃𝜆2∈Λ2∀ℎ′∈¯\nℋℎ𝜆1(ℎℎ◦ℎ′) =\n𝜆2(ℎℎ◦ℎ′)\n|      {z      }\n∃𝜆3∈Λ3 =𝜆3(ℎℎ◦ℎ′)\n.\nThus, rewriting,\n∀ℎ∈¯\nℋ∃𝑡′∈N0∀ℎ◦∈¯\nℋ𝑡′:∞\nℎ\n∃𝜆3∈Λ3∀ℎ′∈¯\nℋℎ𝜆1(ℎℎ◦ℎ′) = 𝜆3(ℎℎ◦ℎ′).\nBut this is precisely the definition of always reaches, and thus we conclude.\n□\nNext, we show two basic properties of the set of all agents: it uniformly generates all agent sets, and\nit is always reached by all agents.\nProposition C.18. For any 𝑒, the set of all agents Λ (i) uniformly generates all other agent sets, and\n(ii) is always reached by all agents:\n(𝑖)\n∀Λ⊆Λ Λ |= Λ,\n(𝑖𝑖)\n∀𝜆∈Λ 𝜆□⇝\n𝑒Λ.\n(C.29)\nProof of Proposition C.18.\n(i). ∀Λ⊆Λ Λ |= Λ\nThe property holds as a straightforward consequence of Proposition C.2: Since any set Λ is a\nsubset of Λ, it follows that Λ |= Λ. ✓\n(ii). ∀𝜆∈Λ 𝜆□⇝\n𝑒Λ\nThe property holds as a straightforward consequence of Proposition C.15: Since every agent\nsatisfies 𝜆∈Λ, it follows that 𝜆⇝\n𝑒Λ. ✓\nThis concludes the argument of both statements.\n□\n30\nC.3\nFigure: Set Relations in CRL\nFinally, in Figure 3 we present a visual depicting the set relations in CRL between an agent basis\nΛB, an agent set it generates Λ, and the three agent sets corresponding to those agents in Λ that (i)\nsometimes, (ii) never, or (iii) always reach the basis. First, we highlight that we visualize ΛB as\na subset of Λ since we define ΛB ⊂Λ in CRL (Definition 4.2). However, there can exist triples\n(ΛB, Λ, 𝑒) such that ΛB ⊢𝑒Λ, but that ΛB is not a subset of Λ. Such cases are slightly peculiar, since\nit means that the basis contains agents that cannot be expressed by the agent set Λ. Such cases are not\nin line with our definition of CRL, so we instead opt to visualize ΛB as a subset of Λ. Next, notice\nthat the basis is a subset of both the agents that always reach the basis and the agents that sometimes\nreach the basis—this follows directly from the combination of Proposition C.14 and point (3.) of\nTheorem 4.3. By similar reasoning from Proposition C.14, we know that the set of agents that always\nreaches ΛB is a subset of the agents that sometimes reach the basis. Further, since sometimes and\nnever reaches are negations of one another (Remark 3.2), observe that the two sets are disjoint, and\ntogether comprise the entirety of Λ. Lastly, we know that the set of optimal agents, Λ∗, contains only\nagents that never reach the basis, and thus the set Λ∗is disjoint from ΛB and the set of agents that\nsometimes reach ΛB.\nn\n7s0BNVFgdVeKVq1TI6gAC4MF7BjRZKTIFpZ4MxwdhnNzUOuLIUYoK/Qv+2T9W36HZJz3d3EyCbWDs/5eObw8NzIDYpEKr2/793v3JT3/281+898vxr379/m9+8GHv/tS5asyFC/DPMnLrwOuRCIz8VJLn\nYivi1LwNEjEV8H1Y+J/dSNKJfPsXN8V4jLli0zGMuQapK/mfCEyra4+2Nnf27cftv4w8w87I/85ufrwj9k8ysNVitlhwpW6mM0KfWl4qWYiGo8Hk+az3jCTnh4jVepvfF8pUThRhcrHT+8NDIrVlpkYcUmjCd\nJ/oaBvuQWXoPfz67NHGeOTA+EwYme7gbSM3OZ4xYqjvB8FSlXC+rIdEi16jqLg3WiHqZ9mlLWLEsRWw1sEpYCjbguv/yVZnUGA9UMi0SwV6ePmO0F0poLbNFb1KQ59eaB6qVXpR5LBRtIE92X694IvUdAyQR\na2t1y2pmBglYQc7LiJEVGK0vT/qzMhmKuORhOyvMUzB1jYZVSzb7+GDKhA572dSGZY5LaSq59aUfFHyYnX27wg1zpPaQNznUoV9q0ar5KkwEOPepGt0gDOe2kyrgM52JxMvClutbjVU/u0vtEFL9W1LKwzst\n3dXZ8q0ve+CKR+nhJBpwytQpiuZiyUL1e5VqoKYu4WparREyZltfTVkQpPiDf3adMrydsnSVaFnmb6YM4RYgusAlUoVQwBPFnkp4Uv2sVBihejJIwhMI5g/FRHsi9VKLVLEwRu7mB/J4EHt+4hwH5Ag1v\nvRP2hB76e0KaVnwviXq2aB+JhCvpTizu0sCjpICrxDazB2qMkfPz4K3QSaVIaGRM/BRokWJfZI3gumcTGRdB/EsShYiRX8dNmPeO5jR6EwqowY/Iim9NsNB3qciWmlhgp10PiQgwCz4MkpEkCEthFb2\nrjvYSEvpEZMfSemGbZBZiTh0iCHYRsR5G5pAd5/I23KJHvD3xAlKc8imCKsLpCWrElCnpidWVUNEKpFqLDchAhaBOy2CVG2iHITQBSqMvMb7FehZJnA7bMIhlW7RvW5gfH4Cow9g4amDmuhjBIhstX/kEulpq\nXcPoNqKRGJSLeCLJitouw07dM/ZxUDMznQ+2+cfRvhvQvHP2LIf3U0U/XjJnpMi/uHPdp5ZzgLEVxsn7bBcNmFwfYGIW9F0F+S1tD0/AEMWpVCph5pFURcLvlL5DAdg5qMiyk+NbTgXhU0aW3eWD83X/oW8X\nKQym9pvfrvXBORzCHyBmOE6L+buYNVKL2L+ZSevg/Ib2sgnxa3PCxDPb5MQrVZyKWmbSezxj4wxwHucZgYNRSigbSTieRiY8MoEZF87LGhYOzE0i8kMmHkulE1kBIwiplrvqrasbKGOfeGOc5ukGkz6j2\naS3JiGqNpowV4PyZeNEdol2GKoYLnOrw1FoFzlLwIlyhvuR2ea6q0NIs7iJV1VIvKO3DLeVFzlJ1qaWcNjbyoZthdf1DVCKfGwi1r4t5IzPb2yK9DM+sRFaKJ1XFCRo9iJey/cTPHESX7WASQeBlz/z\nEHr5k+qmdJBwTr2uys7tno4IZvlJ0n9Cl8mGriXE8tbEdUY34A2XmGZ1TrUI7cBUR/CJdnUZhQUbrnwmaO2KctKNA2eR8Q1OogNFHcb6icIt/wCFadDoVM2fy0zVQYPUNi6hEwQiArKvrzJXog9GsZOoh5ih\nqfwIvmYW7HS2z8a3zf0Etb1axD7G3yGrYJAj3LDZx1R3afbF72a5185iRZ7I8M5P5m5k5oXcLN0DrLyTDZhupqyxvsT54ZUrUKTGqXiDeq8Y0OxLjvpLrh3ZnJItGMqhLDQre80KMmkiqcr0Elm/+Wgw5hz5\nth2lUECnHiXoFfHZlDXarApq473OGhZTVxCqOlIk6aEz1GbaShnVmsImXUQSL9rCe73p7Tmn5D1ojNTQ9GlDRKd0mPsUBEH1jnX+W9dBKelpLashYWna4atOhm16IYM+h46SNAML/KVj7NpN31aZXwK9\nzk+NqHV8ASHPxz5cCZx/KIZI3mkdQ5vqaiDurTvel5d1elM4qOBEPkRy6Nv0I95WkOn5qnMmLq3p9f7ZKPebwpUal0I9qlek8EIv0ZU+xzFkD2XLOrM7D4CPisfEa5+BDf0Rr6thDiab2i0HGxY1h/0e4n4\n7D2ETXpci6SVvm8E6U8mSK804i3kW3jG0Dyj7+QoL1EsEyrDpjACwETeB2IBD6xPEdXkwk4L8zIT5eE+zuTj+Y2CAYSZpTjJUmSOsbdRO2XsSvBS5GhW/Ju0Gxn/jTDzD13u/Zp9fzPfxCiYlXlCf3t0aJejS\npR31YUnXZrHDa0HFa5FqozvlfrcRMAglXlmv3ocnJqLXNm+pfOSkw61B4+aLqcybczWG0Lur8VBZCSERUaTpjmebD+7FsRajSHFdbdPHe2gfmTL3mhe3wjyROMeL3iTk0lQvf2H0b0Ac2ebpPQNUaDsdn8nx\nm8inUygz3iwap5rOwK0wjMCxy7dF7QldRhzBNlz2Hu+N+n2RJKJHbIGiI1/Sl6VJkdPix0A/OkfUeicz0PEJKHs09AQY3C5nDgC1JT1D6kyzV6dEyxnQBXpBi3CBSE91VrCr6naXkQMi+C4sEFXPhaCn26wQ\nxC47SG2cNPy/M9urqkvmskY3JivmJXT0qIyN0Q7Fjv1jd/IPa6x+lMZn10KHy62KH/wYxaFs9+jknA4uR21gFOk8ROrSouy48KzCSWAB1ZaiIZIUyuvej/kK1U7EdHrvZCFfq3xodTHdgPN1EN5w3YWcWkrdgi\nJ9dZk+ndmC1aTLqKfZexRxOveR3X6nbZn9D1MUc1cT62NJEZhfVS0V24UtewI69VTtSrTgve2s6wdiz7M1iz6ZEaNrxcCaTylPtysgjJuf5ewPeLsBm9y61zu53uklGXfdbkOGtEpVZpMcQcNdTaPgm\nXPTs/JoLNcYimlF8Le2mkWYhu9prW32tQhc7zhCrweE5YrhOhYdsxzIqQCVO6oTg6f4QuXlM4IK8xS8jS3tBefD0yNasy+K+WmFN/4u6eigIBJG+ZjxrVqyQ2zsIVGr6UvKRfhrtJoMUgDyAXUkfYCUXI64uiP\nmE+kG6nkTIDMnX5FDbNLPFpompjYoR5C0UI8EDxexq1xWzZLqS8F3FNt3QilTD3mNdPUK9hXqADdWjmRvUs2SbuOtMsU1ByjHVeuOyrqMFvoWShBtqaeduUNPRJ4xy3jYFh03eumYW8RaEW6omZ27QTNHnzC\nbHraplpDjdZrDdWSt/O7ZN3vks1+58kUuC6/MEf3DZ2ImCs36mPdNtyUk/pBtb2oDWywpTL1NdqWT/vxsrUC9v2X0vbwBkgyXW2FkWb2KI8YzZ31UYdZ5M5algtr3on3YsiWBV3bmEgvqEar6sf5gxByG2e\nlZo98f2HB7kW1fkUwiXWYiXCZS8kiEaI/p18rUDGzBZ6WgmW5k1Athcd9poqlrdV98qpFsonOFYHw+df3CXsmTijBP17tMfV1Vbiv6h/0UNqpaIB/M7ykRkm7W1ZmebU/4MD032ISqksxX7p2bXe298\nB2ixOmYOuQDpUsXmVh/ZNGc48EBloRGF0u0s0XiB0YGrezrTieJH1ocx3o5mychB4Teyei3pXEX6rq6oOd2fD36PWHLw/2Zg/29r/4286jl/9yv1W/N/rD6E+j0az0d9Hj0ZPRyejl6NwdD369+g/o/e/e\nX+9eu8fDvruO/737d+Pep97x/8HAJ/d9A=</latexit>⇤\n{⌫2 ⇤: ⌫ \n4 ⇤B}\n{⌫2 ⇤: ⌫ \n/\n4 ⇤B}\n{⌫2 ⇤: ⌫⇤ \n4 ⇤B}\n⇤B\n⇤⇤\nFigure 3: A depiction of the division of a set of agents Λ relative to a basis ΛB through the reaches\noperator in CRL.\n31\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-07-20",
  "updated": "2023-12-01"
}