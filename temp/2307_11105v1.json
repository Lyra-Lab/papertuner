{
  "id": "http://arxiv.org/abs/2307.11105v1",
  "title": "Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games",
  "authors": [
    "Jonas Gillberg",
    "Joakim Bergdahl",
    "Alessandro Sestini",
    "Andrew Eakins",
    "Linus Gisslen"
  ],
  "abstract": "Going from research to production, especially for large and complex software\nsystems, is fundamentally a hard problem. In large-scale game production, one\nof the main reasons is that the development environment can be very different\nfrom the final product. In this technical paper we describe an effort to add an\nexperimental reinforcement learning system to an existing automated game\ntesting solution based on scripted bots in order to increase its capacity. We\nreport on how this reinforcement learning system was integrated with the aim to\nincrease test coverage similar to [1] in a set of AAA games including\nBattlefield 2042 and Dead Space (2023). The aim of this technical paper is to\nshow a use-case of leveraging reinforcement learning in game production and\ncover some of the largest time sinks anyone who wants to make the same journey\nfor their game may encounter. Furthermore, to help the game industry to adopt\nthis technology faster, we propose a few research directions that we believe\nwill be valuable and necessary for making machine learning, and especially\nreinforcement learning, an effective tool in game production.",
  "text": "Technical Challenges of Deploying Reinforcement\nLearning Agents for Game Testing in AAA Games\nJonas Gillberg1, Joakim Bergdahl2, Alessandro Sestini2, Andrew Eakins1, Linus Gissl´en2\n1Electronic Arts (EA), 2SEED - Electronic Arts (EA),\njonas.gillberg@dice.se, aeakins@europe.ea.com, {jbergdahl, asestini, lgisslen}@ea.com\nAbstract—Going from research to production, especially for\nlarge and complex software systems, is fundamentally a hard\nproblem. In large-scale game production, one of the main reasons\nis that the development environment can be very different from\nthe final product. In this technical paper we describe an effort to\nadd an experimental reinforcement learning system to an existing\nautomated game testing solution based on scripted bots in order\nto increase its capacity. We report on how this reinforcement\nlearning system was integrated with the aim to increase test\ncoverage similar to [1] in a set of AAA games including Battlefield\n2042 and Dead Space (2023). The aim of this technical paper is\nto show a use-case of leveraging reinforcement learning in game\nproduction and cover some of the largest time sinks anyone who\nwants to make the same journey for their game may encounter.\nFurthermore, to help the game industry to adopt this technology\nfaster, we propose a few research directions that we believe\nwill be valuable and necessary for making machine learning,\nand especially reinforcement learning, an effective tool in game\nproduction.\nIndex Terms—game testing, reinforcement learning, scripted\nbots, AAA games, game production\nI. INTRODUCTION\nAutomated testing is already a requirement in most modern,\nlarge-scale games. A case study is Battlefield V, which requires\ntesting of 601 different features amounting to around 0.5M\nhours of testing if done manually. This corresponds to ≈300\nwork-years and with every game in the franchise, this number\nis growing. Automated testing can cover many test cases, but\nthe capacity of the scripted solution is already reaching its limit\nand an increasing number of cases require more sophisticated\ntechniques for testing the game in depth. Common test solu-\ntions relying on script-based bots have several drawbacks: the\nbots do not learn from interactions, domain specific scripting\nexpertise is required, and some use-cases are hard or even\nimpossible to script for, to name a few. Recently, a promising\nresearch direction in game testing has evolved: it has been\nshown that agents trained via Reinforcement Learning (RL)\ncan bring value to automated game testing [1, 2, 3]. RL has\nthe capacity to leverage exploration to increase test coverage,\nfind exploits in gameplay, and detect imbalances (ranging\nfrom level design, and character design, to weapon design).\nCompared to classical solutions, RL allows for flexibility\nwhen introducing new game features as agents can easily be\nretrained instead of manually re-scripted. Another benefit of\nRL is that instead of requiring hand-authoring behaviors, they\ncan be generated in a more high-level abstraction through\nreward functions which often are more intuitive: instead of\nscripting an agent to solve a task, the user can simply give\nit rewards relevant to the task and the agent may figure out\nhow to solve it in an emergent fashion. Compared to many\nclassical solutions, RL does not require a navigation mesh\nfor pathfinding as the agent is controlled through the same\ncontroller inputs a player experiences. By not being bound\nto the navigation mesh, the agent enjoys a higher degree of\nfreedom leading to behaviors not attainable using a scripted\nsolution, which is useful for exploit discovery, for instance.\nAs straightforward as this approach might seem, there are\nmany hurdles to overcome before RL can be applied to\ngames, especially when combined with existing automated\nsystems that depend on scripting. Current RL research is\ncommonly pursued in simple environments that do not cover\nthe complexity and scope found in AAA game production.\nFundamental research is naturally needed to push the envelope,\nand toy problems and simplified environments certainly play\nan important role in this. However, it also means that bringing\ncurrent research into AAA games in a shippable form will\ncause unforeseen challenges to overcome. The use of RL\nagents alone should not be seen as means of replacing scripted\nbots altogether, but rather complement scripted bots in a\nsymbiotic fashion where they fall short.\nIn this paper, we describe an effort to augment scripted-\nonly automated game testing using RL, and all the technical\nand practical challenges that emerge from working with games\nin production. We first present current industry-standard test\nmethods based on bot scripting, then we highlight a few real-\nworld AAA games that are relevant to this study where clas-\nsical test solutions are impractical, followed by a description\nof how RL can be used to mitigate this. We present lessons\nlearned from applying RL to games in production and list\nchallenges that need to be overcome when doing so. Finally,\nwe outline related work on this topic and then conclude with\na discussion and future work directions.\nII. SCRIPTED BOTS FOR GAME TESTING\nAutoPlayers is an in-house developed scripted bot system\nthat is used for several internal games at Anonymized, created\nto address the growing scale of modern games and their\ncorresponding testing needs. In AutoPlayers, bots control game\ncharacters in a way very similar to how a player would in\n979-8-3503-2277-4/23/$31.00 ©2023 IEEE\narXiv:2307.11105v1  [cs.SE]  19 Jul 2023\nFig. 1: Visual scripting system for creating different behaviours. In this example, bots are tasked to defend capture points\nrandomly selected from a predefined set. At a random time interval, a new point is sampled and selected.\nan effort to generate test data that is as player representative\nas possible. The AI itself outputs action and movement in-\ntentions which are then translated into game specific inputs.\nIn many cases, the most useful translation is to game or\nengine specific action representations (see Figure 2), rather\nthan hardware inputs, meaning context specific translations are\nautomatically handled by the game. An alternative approach\nthat is sometimes applied is to perform contextual lookups of\nwhich corresponding hardware key currently would result in\nthe desired action and feed that key into the system.\nFig. 2: Overview of the integration of AutoPlayers into the\ninput handling system pre-existing in the game.\nScripted bots are especially useful in the many scenarios\nwhere a high degree of control is desired. The AutoPlayers\nbots are controlled through the scripting of high level inten-\ntions called Objectives. Objectives can either be generated by\ncode or provided from scripts as can be seen in the visual\nscripting example in Figure 1. Objectives control where the\nAutoPlayers go, but objective parameters also control how\nthey get there. If they should engage their enemies, remain\npassive, be invincible etc. Illustrating the typical granularity\nof these objectives, recurring type examples include: Move,\nDefend, Interact, Action, Attack, Follow and more high level\nones like Seek And Destroy, among others.\nWhen an AutoPlayers bot engages in combat with an enemy\nit will move and attack by relying on a predefined set of\nrules in code, based on metrics like enemy distance, effec-\ntive weapon range among others. This bundled autonomous\nbehavior concept is one of the key reasons setting up tests\nwith AutoPlayers is straight forward as only high level goals\nneed to be specified and the details of how to accomplish them\nis left to lower level systems.\nOf these systems, locomotion is one of the most important.\nLocomotion is the system that calculates the appropriate\nmovement controls to move the bot along a path of waypoints,\ntypically produced as the result of a query to the navigation\nsystem. In the AutoPlayers case, different versions of the\nlocomotion system are used depending on whether the bot is,\nfor example, moving on foot, piloting a helicopter or driving\na tank.\nOne of the main challenges when developing test bots is that\nthey need to always function throughout the games develop-\nment cycle, long before the game is stable and the feature set\nhas stabilized. As such, there is a constant trade-off between\nproperly testing features while at the same time not wasting\ndevelopment time on building bespoke test implementations\nfor a feature that might go through major changes or be cut\nentirely.\nIII. USE-CASES AND ENVIRONMENTS\nIn this section, we describe the games considered for this\nstudy, their properties, and how test solutions based on scripted\nbots may be impractical. Further, we define a practical recipe\nfor relaxing the constraints seen in the full production games\nto make the process of training RL agents in them easier and\nfaster by the means of test range and prototype environments.\nA. Production environments\nThe game engine used for both of the production environ-\nments is an in-house game engine called Frostbite. Both of\nthese games contained preexisting integrations of AutoPlayers.\nHere we describe the different environment variants that were\nused to evaluate our solution, combining AutoPlayers and RL.\nBattlefield 2042.\nBattlefield 2042 is a team based, large-\nscale, multiplayer oriented first-person shooter game. This\ntitle has a multi-modal gameplay structure where the player\ntries to defeat enemies either as on-the-ground soldiers, in\nground vehicles, or in aircrafts. The first use-case explored is\nhelicopter navigation, shown in Figure 3c. In order to validate\nand test that helicopters can be successfully piloted to reach all\nintended locations in the game world and that they do not get\nstuck, we seek to find a way of controlling them through con-\ntroller inputs. In pre-production, helicopters were very difficult\nto steer even for expert humans, leading to challenges when\nscripting control for them through the use of AutoPlayers. This\nis further exacerbated by the continuously changing parameters\ngoverning flight dynamics during development and the lack of\ntime to propery rewrite the control solution to compensate\nfor this. To manage this ever-changing control problem, one\nsolution is to train RL agents that automatically learn to control\nand navigate the vehicle through interactions with the game\nenvironment.\nDead Space (2023).\nDead Space is a slow-paced, single-\nplayer, third-person shooter game with a sci-fi horror setting.\nAn interesting characteristic of this title is that in some\ngameplay sections the player is tasked to navigate in zero\ngravity, illustrated in Figure 4b. In these sections, the player\nexperiences 6 degrees of freedom of movement, compared to\nthe 3 degrees in other areas where the player is bound to the\nfloor. This combined with the physics properties of the game\nlike inertia makes scripted control difficult and compensating\nfor these may be non-trivial.\nB. Test range environments\nInstead of training agents in the full production envi-\nronments, test ranges were constructed containing only the\nessential features of the full game, reducing aspects of the\ngame irrelevant to training. For instance, using a flat level with\nlower graphical fidelity by removing textures and collidable in-\ngame objects enabled execution of substantially more agents\nin the same environment at a higher rate while still staying true\nto the full game using the same helicopter representation. A\nvisual example of the differences between this test range and\nthe actual production environment can be seen in Figure 3b.\nAs RL is sensitive to out-of-distribution observations, it should\nbe noted that when using image data for training, textures\nand objects can not be removed as the visual interpretation\nof the game world would fundamentally change. A similar\ntest range was constructed for Dead Space, as seen in Figure\n4a, where the agent is tasked to navigate towards random\ntarget waypoints in a zero-gravity volume larger than the one\nexperienced in the full production setting seen in Figure 4b.\nC. Prototype environments\nThere are significant benefits when training your agents in\nthe same engine and game you are going to deploy them\nin. However, this is not always possible, especially early in\nproduction when the game is too unstable. To allow for easier\nsetup prototyping, we created a simpler prototype environment\nin Unity3D seen in Figure 3a, containing all key features that\nwere expected from the final environments.\nFor example, if the task is to fly a helicopter, the prototype\nenvironment should implement the same actions, although\nwithout exactly the same flight control parameters like thrust\npower, torque, turning radius, etc. For RL, these discrepancies\nare learnable, which was proven both in theory and practice\nwhen we transitioned to the full environment. Similarly, for\nthe observations, we used similar values available in the full\nenvironments from discussions with game team engineers.\nAs both game scenarios involve 3D navigation, the setup\ntuning for the Battlefield 2042 case proved to be useful when\nalso investigating Dead Space, and the prototype environment\nallowed us to quickly iterate on observation representations,\nrewards and neural network architectures, ultimately leading\nto a reusable algorithm setup that needed very little tuning\nwhen deployed in the test range environments for both games.\nIV. REINFORCEMENT LEARNING FOR GAME TESTING\nInspired by research demonstrating how RL can be used\nto improve game state coverage [1, 2], we combined RL with\nAutoPlayers bots to evaluate how efficiently the solution would\nbe in a production setting for solving a real problem: game\nstate coverage in hard-to-control vehicles and zero-gravity\nnavigation. In this section, we describe and motivate the main\nelements of the RL setup used in this work.\nA. Augmenting bots capacity with Reinforcement Learning\nGiven the navigation problems detailed in Section III, the\nidea was to replace the helicopter and the zero-gravity specific\nlocomotion code with a version that instead infers movement\ncontrols using an RL model. This allowed the agent to easily\nswitch between using AutoPlayers locomotion code while on\nfoot to RL based locomotion as soon as the agent entered the\npilot seat of a helicopter or zero-gravity sections. This could\nalso easily be extended by using different models for different\nvehicle types.\nTask Specification.\nIn order to train an agent to control\nhelicopters in Battlefield 2042, the agent is initially placed\non the ground inside the helicopter. From this position, we\ngenerate in-air target waypoints in a stepwise fashion requiring\nthe agent to learn how to lift from the ground without crashing.\nThe agent will then navigate between mid-air waypoints until\nultimately arriving at a final waypoint situated at some location\nback on the ground-level, necessitating successfully landing.\nFor Dead Space, the navigation problem is made easier due\nto the lack of gravity. Hence, we task the agent to merely\nnavigate between random waypoints in the play space.\nParallelization Implications and Algorithm Choice. Given\nthe parallelization constraints experienced when running com-\nplex games, low data collection rate inhibits the training pro-\ncess. There are different ways to mitigate this problem in prac-\ntice. In general, leveraging distributed training, it is beneficial\nto run as many parallel environments as your hardware allows.\n(a) Prototype environment\n(b) Test range environment\n(c) Production environment\nFig. 3: Example of environment setup going from prototype to production in Battlefield 2042: three different RL training\nenvironments.\n(a) Test range environment\n(b) Production environment\nFig. 4: Examples of test range and full production environment of Dead Space (2023).\nDepending on the game, further optimization methods may\nor may not be possible. For games fundamentally designed\naround multiplayer such as Battlefield 2042, the multiplayer\nconcept can be reused to allow for the instantiation of multiple\nagents per launched game process as long as the model is\nable to produce actions for each agent at every inference step.\nIn singleplayer oriented titles such as Dead Space, this is,\nhowever, impossible without additional engineering. As such,\nthe aforementioned techniques allowed us to run 250 agents\ndistributed over 5 game servers simultaneously in Battlefield\n2042 and merely 7 in Dead Space.\nIn this study, we evaluated a few algorithms with this in\nmind. On-policy methods such as Proximal Policy Optimiza-\ntion (PPO) rely on a continuous flow of fresh environment\ndata, effectively discarding seen samples at each model update\n[4]. PPO is a popular on-policy algorithm due to its stability\nand lower tuning requirements. Inspired by the TRPO trust\nregion policy optimization scheme, gradient backpropagation\nin PPO is bounded and never shifts the policy network too\nfar from the current policy, leading to fewer uncontrolled\nupdates. What makes PPO less viable is that, being an on-\npolicy algorithm, it requires a massive number of samples\nbefore it starts to converge to a high-performing policy.\nIn addition to PPO, we also explored Soft Actor-Critic\n(SAC) which is an off-policy algorithm that stores data that\ncome from different policies (e.g. past versions of the current\nmodel) for future model updates [5]. From this, learnable\ninformation encoded in previous experiences can be reused,\nleading to higher sample efficiency. As a result, the number\nof environment interactions needed when using SAC is sub-\nstantially lower than for on-policy algorithms. This, however,\ncomes with a caveat: SAC tends to be unstable like the\nmajority of off-policy algorithms [6]. Even if the number\nof required environment interactions is reduced, the training\npolicy is not guaranteed to converge to a good solution with\nhigh in-environment performance. As a result, it is possible\nthe model needs to be retrained multiple times, lowering the\ngains from improved sample efficiency. In the end, even given\nthe sample efficiency of SAC compared to PPO, the less stable\nresults from SAC made PPO more favorable.\nObservation Space. In the case of Battlefield 2042, simulat-\ning 250 helicopters over all game processes does not allow for\nefficient image based input generation as rendering viewports\nfor all of them is intractable. Further, when using image\ndata, the agent becomes sensitive to graphical changes in the\nenvironment, such as new textures, assets or rendering post\nprocessing common in the game production process. Hence,\nwe decided to construct an observation space based on a\nminimal set of game state features. This state is ego-centric to\nthe agent. Suppose that the world-space position of the agent\nat timestep t is defined as pt ∈R3, then the state includes\nthe relative positions of the current and next target waypoint\nwi\nt, wi+1\nt\n∈R3, distance to current waypoint dwi\nt = ∥wi\nt∥∈R,\nclosest distance to the line lt spanned by the vector from the\nprevious waypoint to the current, defined as:\ndlt = ∥(pt −wi−1\nt\n) × (wi\nt −wi−1\nt\n)∥\n∥wi\nt −wi−1\nt\n∥\n∈R,\n(1)\ndistance to ground dgt ∈R, velocity ˙pt ∈R3, acceleration\n¨pt ∈R3, quaternion rotation described by qt ∈R4, quaternion\nrotation difference ∆qt = qt −qt−1 ∈R4 since last rotation\nsample, and finally the z-axis angular alignment ϕt ∈R based\non the dot product between the unit vector representing the for-\nward direction of the agent and the normalized vector between\nthe agent and current waypoint projected onto the XY -plane.\nAll spatial values are normalized to a predefined bounding box\nencapsulating the entire game scene section that is playable to\nthe agent. As the zero-gravity navigation problem specification\nin Dead Space is similar to the helicopter case, the same\nobservation space was used with the omission of height from\nthe ground.\nAction Space. As the environments require fine control, we\nuse continuous actions to allow the agent to make variable\nadjustments when navigating. Each action is represented by a\nvector at = [a0\nt, ..., aN\nt ], where ai\nt ∈[−1, 1]. In the case of\nBattlefield 2042, 5 actions are used corresponding to throttle,\npitch, yaw, roll, and a fire action. The latter is treated as a\ncategorical action in the game side, which is triggered when\nthe network output reaches a threshold value of 0.5. For Dead\nSpace, we use a similar space of 5 actions controlling pitch,\nyaw and roll, but with forward and strafe movement in place\nof throttle and the fire action.\nReward Function. Due to the similarity between the tasks in\neach game, their corresponding reward functions are largely\nthe same. We used the AutoPlayers system to perform step-\nwise generation of valid target waypoints wi\nt\n∈\nR3 to\nconstruct a procedural navigation path described by the set\nW = {w0\nt , w1\nt , ..., wN−1\nt\n, wN\nt } of connected waypoints start-\ning from position w0\nt to an ultimate goal location wN\nt . Note,\nthat this sequence always starts and ends at the ground,\nrequiring the agent to both lift off, navigate midair and finally\nland. We use the reciprocal closest orthogonal distance dl to\nthe lines connecting consecutive waypoints following Equation\n1 to reward the agent for staying close to the shortest path\nbetween waypoints. To keep the agent facing the current\nwaypoint, we add a reward based on ϕ ∈[−1, 1]. For each\ntimestep, the agent gets an additional reward proportional to\nthe difference between the last and current measured distance\nto the active waypoint δw = dwi\nt−1 −dwi\nt, which effectively\npenalizes the agent for moving away from the waypoint.\nFinally, the agent receives a reward for arriving at the current\nwaypoint defined by 1(dwi\nt < ϵ), where 1(·) returns 1 when its\nargument is true and 0 otherwise and ϵ defines the configurable\nspecific threshold distance for arriving at the waypoint. The\nfinal reward is defined as:\nrt = α\ndlt\n+ βϕt + γδw + ψ1(dwi\nt < ϵ),\n(2)\nwhere α, β, γ and ψ are configurable reward shaping param-\neters. For the Battlefield 2042 use case, we require an addi-\ntional reward term stemming from the fundamental difference\nbetween navigating with and without gravity, compounded by\nthe fact that the helicopter is destroyed from collisions. As a\nresult, we penalize the agent when the dot product between\nits upwards facing vector and the world-space unit vector ˆz\nis ≤0, which corresponds to the helicopter flying sideways\nor inverted, resulting in flight instability and a high likelihood\nof crashing into the ground. This phenomenon is especially\ndetrimental when the agent is trying to land the helicopter at\nthe final target waypoint on the ground.\nModel Architecture. Besides requiring expensive rendering\noperations, the use of image data also imposes a higher degree\nof model complexity as it relies on convolutional layers for\nfeature extraction, which not only prolongs training but also\nincreases inference times and sensitivity to visual changes in\nthe environment. Following the previously defined observation\nrepresentation, we used a multilayer perceptron for encoding\nstates. The model was composed of two linear layers of sizes\n512 and 256 for PPO and 512 and 512 for SAC, each layer\nusing ReLU activations. Each model has an input layer of\nsize 87 corresponding to a 1D stack of 3 observations of\nsize 24 for both games. Each model is trained to produce\npolicies π(s; θ) ∼N|A|(µ, Σ) where µ and Σ are learned,\nand correspond to the first and second moments of each of\nthe 5 continuous actions.\nFramework for training RL agents.\nTo train the PPO\nand SAC agents, we utilize an in-house trainer built around\ndistribution and parallel data collection from multiple running\ngame environments, illustrated in Figure 5. The trainer makes\nuse of a standardized OpenAI Gym-like API for stepping the\nenvironments, thus enabling support for a wide spectrum of\nplatforms, including Frostbite, Unity3D, Unreal Engine, and\nother classical OpenAI environments [7]. Training is central-\nized following employing a client-server network topology\nwhere multiple clients are served action data upon inference\nfrom a server that hosts the neural network model. The\nclients return observation and reward data from each game,\ncollated following unique IDs reflecting each client-hosted\ngame process, allowing the server to batch data used to\nperform the gradient backpropagation needed to update the\nmodel. Further, the trainer produces models supported by\nthe Open Neural Network Exchange (ONNX) standard [8],\nallowing for embedded inference with the ONNX runtime [9].\nHaving a general, centralized training platform allows us to\neasily share models and training setups across teams.\nBy abstracting away the complexity of machine learning\nalgorithms, this modular library of building blocks enables\nrapid development and iteration. Further, by distributing data\ncollection across many processes and machines, it allows for a\nhigh-performing training procedure, lessening training time to\nallow for faster iteration. To allow for training in the mentioned\nproduction and test range environments, an API between the\ninternal training framework and AutoPlayers was set up to\nenable the communication of actions, rewards and observations\nto and from the AutoPlayers player-endpoint.\nB. Deployment\nTo use the trained RL models in-game, runtime support was\nneeded since there was no guarantee that the game clients\nalways would have access to the trainer for all potential\nenvironments and setups the agents could be used. This is\nespecially true when running training on various platforms and\nFig. 5: Architecture overview of the in-house RL frame-\nwork used for training agents in this work. The framework\ndistributes the collection of environment data over multiple\nclients, each in ownership of multiple game processes. The\ntraining server aggregates and batches data received from\neach client upon inference to update a central model. The\nclient-server abstraction allows for the training to either be\nrun locally or distributed over multiple machines, leveraging\nefficient communication over ZMQ network sockets.\ncloud solutions. The exported ONNX-model was embedded\nand baked with the rest of the game data and loaded by the\nhelicopter locomotion code when the agent enters a vehicle.\nThis ML locomotion collects observations in the same way,\nusing the same code, as was done during training by reading\nvalues from the AutoPlayers API and then feeding them to the\nONNX runtime. The output of the network is then interpreted\nas movement controls and injected as such together with other\nAutoPlayer generated inputs.\nGenerally, inference support in game engines is relatively\nnew and improving rapidly with respect to performance and\ncompatibility with other game systems. Integration with other\nsystems frequently requires operating on a batch size of 1\nwhich means that batching for speed-ups in inference is not\nalways possible. Inference run-time is critical for production,\nwhere current solutions for automated testing only require\n<100 µs for decision making, setting an upper feasibility\nboundary for inference time. The ONNX runtime provides\na good starting point, but for AAA games that would like\nto use ML models for many features, or for many agents, a\nbespoke high-performance inference library will probably be\nneeded. In our experiments, we were limited to fairly small\nmodels due to previously mentioned restrictions ranging from\n103 to 106 parameters in size. As inference support improves,\nthese models could be increased in complexity. Furthermore,\nquantization of model parameters is a good way of increasing\nperformance and reducing memory footprint but it was not\nused in our experiments.\nV. LESSONS LEARNED AND DISCUSSION\nIn general, RL integration comes with a set of requirements\non the game often not seen in other methods, and these\nneed to be fulfilled. Identify test cases early where RL could\nprovide value, as the implementation procedure of the method\ncomes with issues that might be easy to solve early in game\nproduction but virtually impossible at a later stage. In this\nsection, we list formulated suggestions and lessons learned\nduring this project. This list is not exhaustive, but it can\nprovide a starting point for preparing your game for RL\nintegration and bring awareness to potential causes of sub-\noptimal training performance you may encounter.\n• Use the most comprehensive and meaningful observation\nrepresentation up until simulation performance is de-\ngraded. For AAA games, every microsecond of CPU time\nmatters. In many cases, this means that computationally\nexpensive ways to collect data about the world such\nas raycast queries or vision data rendering can be pro-\nhibitively costly. Remember that data has to be collected\nnot only for the training procedure, but also for when\nthe in-game agent needs to use the model for inference\nat runtime. In addition, try to use the smallest possible\nmodel that does not adversely affect agent performance.\n• Start from the most stable version of the game, branch\nfrom that, and only start accepting upstream updates once\nsuccessful training has been established. This is especially\nimportant while iterating on the training setup, thus it\nis essential to have a plan for how to provide a stable\ntraining environment. Games under development are not\nstable, and both keeping up with and training agents\ndirectly in production code is virtually impossible.\n• Establish a plan for keeping the training setup alive by\ncontinuously testing it. Even if a model is successfully\ntrained early in development, the game will keep chang-\ning. As the game changes, it is almost certain that the\nmodel will have to be retrained at some point. This entails\ntuning hyperparameters, observation spaces and rewards,\nor recording new data in the case of Imitation Learning\n(IL) or just ascertaining that your training setup still\nworks.\n• As the game development process rapidly advances, make\na decision on what method to use beforehand as there\nmay be no time to fully explore alternatives. Given\nthe nature of the studied environments in this work, as\nIL generally requires less training time it could be a\nsensible alternative to RL. We made initial explorations\nusing Generative Adversarial Imitation Learning (GAIL)\nin Battlefield 2042, but given the state of the game,\ndemonstration recording was difficult as the helicopters\nwere still too hard to control and further user-facing\nimprovements were yet to be implemented [10]. Addi-\ntionally, any changes made to the game would make any\nalready recorded demonstration data obsolete.\n• Leverage the knowledge and efficiency of scripted solu-\ntions and only use RL when needed. We believe that the\nideal approach is to complement the automated scripting\nsystem with RL, rather than use a full end-to-end RL\nsolution to utilize existing knowledge and test procedures.\nThere are many test cases that require a level of control\nbest achieved through scripting, so it is more efficient to\nput such a system in place first before RL is applied to\nimprove certain parts of it.\n• Create your observation space with care. It is important to\nensure that every reward signal has an appropriate number\nof observations to which it can be assigned. Having too\nmany observations is preferable to having too few. RL\nrequires a solid environment definition and is particularly\nsensitive to poorly defined observation-spaces and reward\nsignals, and their relation to each other: any unexplained\nreward will significantly reduce training efficiency. For\ninstance, if a helicopter gets a negative reward when\ncolliding with the ground, make sure an observation\nthat either measure the distance to the ground, or any\nother indicator exists otherwise training will be undone\nif a seemingly random reward is propagated through the\nnetwork.\n• Make sure you are able to scale your game by run-\nning many instances of the game in parallel, and/or\nrunning many agents in each game process. Further,\nfast forwarding the game at a more than 1x simulation\nrate should allow for faster training. This requires that\nthe machine is actually capable of running the game\nfaster but even then the physics engine within the game\nis prone to failing, especially for validating collisions.\nWhen this was attempted for the helicopter scenario,\nthe parked helicopters would fall through the ground at\nthe start of the training session. In general, RL requires\nmany environment samples and choosing an algorithm for\ntraining the RL agent has to be done with consideration.\nOff-policy algorithms such as SAC can be more sample\nefficient than on-policy methods like PPO, but with the\ntrade-off of being more unstable [5, 6].\nVI. RELATED WORK\nThe goal of this paper is to provide guidance to engineers\nwhen applying RL to game testing in a production environ-\nment. The potential of deep reinforcement learning for video\ngame testing has been gaining interest from both the research\nand video game communities. Here, we review work from the\nliterature most related to our paper.\nA. Game testing\nRecently, automated playtesting and validation techniques\nhave been proposed as a way to reduce manual validation in\nlarge games. These works have primarily relied on classical\nhand-scripted AI [11, 12, 13] or proposed model-based testing,\nsuch as Iftikhar et al. [14] for platform games, and Lovreto\net al. [15] for mobile games [16, 17, 18, 19]. The work of\nMugrai et al. [20] is particularly noteworthy, as it developed\nan algorithm for mimicking human behavior in an effort to\nimprove the game design process as well as the validity of\nplaytesting results. However, when dealing with complex 3D\nenvironments these scripted or model-based techniques are\nnot readily applicable due to the high-dimensional state space\ninvolved and poor generalization performance.\nB. Machine learning and game testing\nRelated work on real-world applications regarding control-\nling autonomous helicopters with RL and expert data has\npreviously been studied in Abbeel et al. [21]. Recent research\nincludes the use of RL to augment automated playtesting [1].\nAlonso et al. [22] trained an RL agent to navigate a complex\n3D environment toward procedurally generated goals, while\nDevlin et al. [23] trained different agents and classification\nmodels to perform a Turing test to evaluate the human-\nlikeness of trained bots. Zheng et al. [24] proposed an on-\nthe-fly game testing framework that leverages evolutionary\nalgorithms, DRL, and multi-objective optimization to perform\nautomated game testing. Agarwal et al. [25] trained RL\nagents to perform automated playtesting in 2D side-scrolling\ngames along with visualizations for level design analysis, and\nGordillo et al. [2] used intrinsic motivation to train many\nagents to explore a 3D scenario with the aim of finding issues\nand oversights. Similar to the latter, Sestini et al. [3] proposed\nthe curiosity-conditioned proximal trajectories algorithm with\nwhich they test complex 3D game scenes with a combination\nof IL, RL, and curiosity driven exploration. Nevertheless,\nthe majority of the previously cited approaches were used\nin proof-of-concept games and, despite their complexity, still\nprovide a simplistic view of applying RL methods to an actual\ngame in production.\nC. RL in game production\nAt the same time, there are only a few examples in the\nliterature regarding RL in game production. Notable examples\nare: Harmer et al. [26] proposed a multi-action imitation\nlearning framework used in the game Battlefield 1; Iskander\net al. [27] trained multiple RL agents to play the multiplayer\ngame Roller Champions; Wei et al. [28] train a single complex\nagent in one of the most popular game Honor of Kings; finally,\nGTSophy [29], an RL policy trained to beat professional\nplayers in the game Gran Turismo 7, was recently shipped\nwithin the actual video game. Despite the already cited papers\ndiscussing RL, most do not address the challenges that a\ngame studio must face when applying these solutions for either\nvalidating their games or creating non-player characters.\nVII. CONCLUSION AND FUTURE WORK\nHerein we have described a cross-disciplinary and cross-\nteam experiment to bring a research prototype into AAA game\nproduction. The main finding of this paper is that indeed it\nis possible, although difficult, to bring RL into an existing\nautomated and scripted bot system. We have shown how RL\nagents can add capability to an already capable automated\ntesting system. Delivering RL capacity into the hands of QV\ntesters was achieved through extensive collaboration between\nresearchers, QV and the game teams.\nMachine learning offers a new way of creating behaviors\nthat can be used for testing. By talking to Quality Verification\n(QV), Quality Assurance (QA), and other experts in related\nfields, we were able to gather a lot of potential future work\nin that area. It can be divided into two different needs: the\nlearning capacity of the algorithms, and the technical aspects.\nIn areas of the model learning capacity, we see some important\nimprovements that most likely will require advancements in\nmachine learning to achieve. We also see that there are hurdles\nwhen having non-experts using ML. In most literature, part of\nthe impressive performance and results comes from the fact\nthat it has been trained and set up by a field expert and it\nmight not even be possible to otherwise do so without that\nperson. Overall, in order for ML/RL to be used by laymen\nsome usability improvements in algorithms would be desired.\nHere we list a few topics that we think would help increase\nadaptation in games and game production.\n• More stable and predictable RL algorithms: in a pro-\nduction setting there will be little time for parameter\ntuning, observation tuning, etc. It is desirable to have a\nrobust algorithm that arrives at a stable solution in many\ndifferent game iterations at a slower pace rather than to\nhave a faster but unstable algorithm that provides high\nvariance in performance which requires more training\niterations per version of the game.\n• Behaviour explainability: understanding a behavior, in\norder to correct it, is essential. When an RL agent gets\nconfused and ends up stuck in a critical place of the game\ndue to a bug, without explanation it would be hard to\ncorrect said bug.\n• Reward shaping: more intuitive ways of doing reward\nshaping. Most game developers lack the RL/ML expertise\nneeded to write good reward functions for specific testing\nbehaviors.\n• Transfer learning: games in production are constantly\nshifting, and improving how a model adapts to a\nnew game environment would significantly decrease re-\ntraining needs and speed up the process.\nMany of these problems are fundamental to machine learn-\ning in general, but hopefully this will give an indicator of\nwhere we see improvements can be made and what research\nis needed for adaptation to game production. Games and game\nproduction is a field that could heavily benefit from more user-\nfriendly and capable machine learning algorithms.\nREFERENCES\n[1] J. Bergdahl, C. Gordillo, K. Tollmar, and L. Gissl´en, “Augmenting\nautomated game testing with deep reinforcement learning,” in 2020 IEEE\nConference on Games (CoG).\nIEEE, 2020, pp. 600–603.\n[2] C. Gordillo, J. Bergdahl, K. Tollmar, and L. Gissl´en, “Improving\nplaytesting coverage via curiosity driven reinforcement learning agents,”\nin 2021 IEEE Conference on Games (CoG).\nIEEE, 2021, pp. 1–8.\n[3] A. Sestini, L. Gissl´en, J. Bergdahl, K. Tollmar, and A. D. Bag-\ndanov, “Ccpt: Automatic gameplay testing and validation with curiosity-\nconditioned proximal trajectories,” arXiv preprint arXiv:2202.10057,\n2022.\n[4] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” 2017. [Online]. Available:\nhttps://arxiv.org/abs/1707.06347\n[5] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning.\nPMLR, 2018,\npp. 1861–1870.\n[6] R. Sutton and A. Barto, Reinforcement Learning, second edition:\nAn Introduction, ser. Adaptive Computation and Machine Learning\nseries.\nMIT Press, 2018. [Online]. Available: https://books.google.co.\nuk/books?id=sWV0DwAAQBAJ\n[7] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016. [Online]. Available:\nhttps://arxiv.org/abs/1606.01540\n[8] J. Bai, F. Lu, K. Zhang et al., “Onnx: Open neural network exchange,”\nhttps://github.com/onnx/onnx, 2019.\n[9] O. R. developers, “Onnx runtime,” https://www.onnxruntime.ai, 2021.\n[10] J. Ho and S. Ermon, “Generative adversarial imitation learning,” Ad-\nvances in neural information processing systems, vol. 29, 2016.\n[11] S. Stahlke, A. Nova, and P. Mirza-Babaei, “Artificial players in the\ndesign process: Developing an automated testing tool for game level and\nworld design,” in Proceedings of the Annual Symposium on Computer-\nHuman Interaction in Play, 2020.\n[12] C. Holmg˚ard, M. C. Green, A. Liapis, and J. Togelius, “Automated\nplaytesting with procedural personas through MCTS with evolved\nheuristics,” IEEE Transactions on Games, 2018.\n[13] K. Chang, B. Aytemiz, and A. M. Smith, “Reveal-more: Amplifying\nhuman effort in quality assurance testing using automated exploration,”\nin IEEE Conference on Games, 2019.\n[14] S. Iftikhar, M. Z. Iqbal, M. U. Khan, and W. Mahmood, “An automated\nmodel based testing approach for platform games,” in ACM/IEEE\nInternational Conference on Model Driven Engineering Languages and\nSystems (MODELS), 2015.\n[15] G. Lovreto, A. T. Endo, P. Nardi, and V. H. Durelli, “Automated\ntests for mobile games: An experience report,” in 2018 IEEE Brazilian\nSymposium on Computer Games and Digital Entertainment, 2018.\n[16] S. Stahlke, A. Nova, and P. Mirza-Babaei, “Artificial playfulness: A\ntool for automated agent-based playtesting,” in Extended Abstracts of\nthe CHI Conference on Human Factors in Computing Systems, 2019.\n[17] C. Schaefer, H. Do, and B. M. Slator, “Crushinator: A framework to-\nwards game-independent testing,” in 2013 28th IEEE/ACM International\nConference on Automated Software Engineering, 2013.\n[18] C.-S. Cho, K.-M. Sohn, C.-J. Park, and J.-H. Kang, “Online game testing\nusing scenario-based control of massive virtual users,” in IEEE Interna-\ntional Conference on Advanced Communication Technology, 2010.\n[19] G. Xiao, F. Southey, R. C. Holte, and D. Wilkinson, “Software testing\nby active learning for commercial games,” in Conference on Artificial\nIntelligence, 2005.\n[20] L. Mugrai, F. Silva, C. Holmg˚ard, and J. Togelius, “Automated playtest-\ning of matching tile games,” in IEEE Conference on Games, 2019.\n[21] P. Abbeel, A. Coates, and A. Y. Ng, “Autonomous helicopter aerobatics\nthrough apprenticeship learning,” The International Journal of Robotics\nResearch, vol. 29, no. 13, pp. 1608–1639, 2010.\n[22] E. Alonso, M. Peter, D. Goumard, and J. Romoff, “Deep reinforcement\nlearning for navigation in AAA video games,” in International Joint\nConference on Artificial Intelligence (IJCAI), 2021.\n[23] S. Devlin, R. Georgescu, I. Momennejad, J. Rzepecki, E. Zuniga,\nG. Costello, G. Leroy, A. Shaw, and K. Hofmann, “Navigation turing\ntest (ntt): Learning to evaluate human-like navigation,” in International\nConference on Machine Learning.\nPMLR, 2021, pp. 2644–2653.\n[24] Y. Zheng, X. Xie, T. Su, L. Ma, J. Hao, Z. Meng, Y. Liu, R. Shen,\nY. Chen, and C. Fan, “Wuji: Automatic online combat game testing using\nevolutionary deep reinforcement learning,” in IEEE/ACM International\nConference on Automated Software Engineering, 2019.\n[25] S. Agarwal, C. Herrmann, G. Wallner, and F. Beck, “Visualizing AI\nplaytesting data of 2D side-scrolling games,” in IEEE Conference on\nGames, 2020.\n[26] J. Harmer, L. Gissl´en, J. del Val, H. Holst, J. Bergdahl, T. Olsson,\nK. Sj¨o¨o, and M. Nordin, “Imitation learning with concurrent actions\nin 3d games,” in 2018 IEEE Conference on Computational Intelligence\nand Games (CIG).\nIEEE, 2018, pp. 1–8.\n[27] N. Iskander, A. Simoni, E. Alonso, and M. Peter, “Reinforce-\nment learning agents for ubisoft’s roller champions,” arXiv preprint\narXiv:2012.06031, 2020.\n[28] H. Wei, J. Chen, X. Ji, H. Qin, M. Deng, S. Li, L. Wang, W. Zhang,\nY. Yu, L. Liu et al., “Honor of kings arena: an environment for\ngeneralization in competitive reinforcement learning,” arXiv preprint\narXiv:2209.08483, 2022.\n[29] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subrama-\nnian, T. J. Walsh, R. Capobianco, A. Devlic, F. Eckert, F. Fuchs et al.,\n“Outracing champion gran turismo drivers with deep reinforcement\nlearning,” Nature, vol. 602, no. 7896, pp. 223–228, 2022.\n",
  "categories": [
    "cs.SE",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-07-19",
  "updated": "2023-07-19"
}