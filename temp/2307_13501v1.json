{
  "id": "http://arxiv.org/abs/2307.13501v1",
  "title": "Deep Reinforcement Learning for Robust Goal-Based Wealth Management",
  "authors": [
    "Tessa Bauman",
    "Bruno Gašperov",
    "Stjepan Begušić",
    "Zvonko Kostanjčar"
  ],
  "abstract": "Goal-based investing is an approach to wealth management that prioritizes\nachieving specific financial goals. It is naturally formulated as a sequential\ndecision-making problem as it requires choosing the appropriate investment\nuntil a goal is achieved. Consequently, reinforcement learning, a machine\nlearning technique appropriate for sequential decision-making, offers a\npromising path for optimizing these investment strategies. In this paper, a\nnovel approach for robust goal-based wealth management based on deep\nreinforcement learning is proposed. The experimental results indicate its\nsuperiority over several goal-based wealth management benchmarks on both\nsimulated and historical market data.",
  "text": "Deep Reinforcement Learning for Robust Goal-Based Wealth\nManagement\nTessa Bauman∗, Bruno Gašperov, Stjepan Begušić, and Zvonko Kostanjčar\nUniversity of Zagreb, Faculty of Electrical Engineering and Computing,\nLaboratory for Financial and Risk Analytics (lafra.fer.hr),\nUnska 3, 10000 Zagreb, Croatia\nAbstract\nGoal-based investing is an approach to wealth management that prioritizes achieving spe-\ncific financial goals. It is naturally formulated as a sequential decision-making problem as it\nrequires choosing the appropriate investment until a goal is achieved. Consequently, reinforce-\nment learning, a machine learning technique appropriate for sequential decision-making, offers\na promising path for optimizing these investment strategies. In this paper, a novel approach\nfor robust goal-based wealth management based on deep reinforcement learning is proposed.\nThe experimental results indicate its superiority over several goal-based wealth management\nbenchmarks on both simulated and historical market data.\n1\nIntroduction\nGoal-based wealth management (GBWM), also known as goal-based investing [1], is a relatively new\nclass of approaches to wealth management that focus on attaining specific financial objectives (goals).\nAs opposed to more traditional approaches to wealth management, in which the notion of expected\nprofit and loss (PnL) plays a central role, GBWM revolves around maximizing the probability of goal\nattainment. Common investment goals include saving for college tuition, retirement, or purchasing\na home. Recent years have seen an uptick in the popularity of GBWM [2], particularly through\nthe use of target date funds (TDFs). TDFs, also known as life-cycle funds [3] or target-retirement\nfunds, are mutual funds or exchange-traded funds that provide investors with an asset allocation\naimed at fulfilling a target (goal) by a specified target date (e.g. a retirement date). Typically, as\nthe target date approaches, the asset allocation shifts towards a more conservative, i.e., less risky\nstrategy. During earlier time periods, there is a greater emphasis on investments in equities, while\nlater time periods are characterized by a higher concentration of investments in bonds. This pattern\nis typically illustrated by glide paths [4] - functions that show the percentage of wealth invested in\na certain type of asset over time. However, using fixed glide paths can be suboptimal; for example,\nin situations when the target date draws close and the goal is not yet accomplished, it is clear that\nmore risk should be taken on through larger positions in equity. This begets a new risk paradigm [5]\naccording to which risk is not directly associated with the volatility of underlying assets, as is the case\nin traditional portfolio optimization, but rather with the prospect of not achieving the investment\ngoal. Since GBWM involves making a series of investment decisions over time in fluctuating market\nconditions, each affecting the future position of the investor, it is naturally framed as a problem in\nsequential decision-making under uncertainty. Reinforcement learning (RL), due to its capacity to\ntackle sequential decision-making tasks in a data-driven fashion, offers a particularly promising path\nto GBWM, especially through its model-free and deep (DRL) algorithms. Multiple applications of\n(D)RL in quantitative finance exist, ranging from standard portfolio optimization [6,7], to market\nmaking1 [9–11] and optimal trade execution\n[12]. On the other hand, applications of (D)RL to\n∗Corresponding author: tessa.bauman@fer.hr\n1Unlike in GBWM, in market making, increasing levels of risk are typically incurred as the terminal time is\napproached [8], resulting in weaker inventory penalization.\n1\narXiv:2307.13501v1  [q-fin.PM]  25 Jul 2023\nGBWM are still very scarce, despite its vast potential for the field. In this paper, a novel approach\nfor GBWM based on DRL, with a focus on the robustness of the resulting strategies, is proposed. We\ndemonstrate its superior performance over several standard GBWM benchmarks on both simulated\nand historical market data.\n2\nRelated Work\n2.1\nDeterministic Glide Path\nA deterministic glide path is a simple approach for GBWM. While essentially a heuristic, it has\nbeen adopted by many investors due to its simplicity and intuitive appeal.\nNote that rules of\nthumb, such as 100-age, which suggests that an individual’s stock allocation should be equal to 100\nminus their age, are frequently used in retirement asset allocation [13, 14]. However, this strategy\nhas been criticized for its sole reliance on the time remaining to the target date [15, 16]. Despite\nthis, deterministic glide paths are frequently used for target date fund allocation as they provide\na straightforward and systematic approach to managing risk as the target date approaches 2. We\nfocus on the form:\nαt = 1 −t\nT ,\nwhere αt is the portfolio weight of the stock at time t and T is the target time.\n2.2\nMerton’s Constant\nIn Merton’s seminal work on lifetime portfolio selection [17], it is assumed that the riskless asset\nhas a constant rate of return r, while the price of the risky asset (St)t follows the dynamics dSt =\nµStdt + σStdZt. Here, Z(t) is a standard Brownian motion, µ is the expected rate of return and\nσ is the volatility of the underlying asset. Under this framework, it is demonstrated that, for an\ninvestor with a Constant Relative Risk Aversion (CRRA) utility3, it is optimal to maintain constant\nportfolio weights of each asset. The optimal weight of the risky asset equals:\nαt =\nµ −r\n(1 −γ)σ2 .\n2.3\nVariance Budgeting\nBruder et al. [18] describe an individual’s risk aversion by specifying the maximum cumulative\nportfolio variance they are willing to take on over the investment period, called the variance budget.\nThe authors consider this approach (restricted to the universe of two assets) and model their objective\nas maximizing the expected wealth at maturity αt = arg max E[XT ] subject to a predefined amount\nof risk\nR T\n0 α2\ntσ2\nt X2\nt dt ≤V 2, where V 2 is the total variance budget of the strategy from the start date\nto the target date. The optimal allocation strategy is given by:\nαt =\nV\nσt\n√\nTXt\n.\n2.4\nDynamic Programming\nDas et.\nal [16] developed a discrete-time dynamic programming algorithm to create a portfolio\ntrading strategy that maximizes the probability of an investor reaching its target wealth within a\npredetermined time frame. The approach utilizes portfolios from the efficient frontier, selecting one\nefficient portfolio at time step t to hold until the next period t + 1. A state space consisting of time\nand wealth is divided into individual states by a grid. States are evaluated based on how likely they\n2An\nexample\nis\ngiven\nby\nthe\nasset\nallocation\nof\nFidelity\nFreedom\nFunds\n(https://www.fidelity.com/\nmutual-funds/fidelity-fund-portfolios/freedom-funds).\n3The CRRA utility U is given by: U(x) = xγ/γ, γ < 1, where 1 −γ is the coefficient of relative risk aversion.\n2\nWT-1\nWT\nWT > WGOAL\n< WGOAL\n(µ1, σ1)\n(µ2, σ2)\nFigure 1: Simplified diagram of the dynamic programming approach\nare to lead to states in which the goal is reached. This evaluation is used to determine, at each time\nstep, the portfolio that is most likely to lead to states with larger values. Figure 1 is presented for\na better understanding of the method. At time T −1, the investor’s wealth is WT −1. The investor\nthen chooses the portfolio pair (µ, σ) from the efficient frontier that maximizes the probability of\nachieving the goal wealth at time T. The value of the state WT −1 is obtained and is then used\nto evaluate earlier states using backward recursion. In order to compute these probabilities, the\nevolution of the portfolio wealth needs to be modeled. The authors choose the Geometric Brownian\nmotion, remarking that the method is also applicable to other models.\n2.5\nReinforcement Learning-Based Approaches\nPendharkar and Cusatis [19] use RL to construct a two-asset personal retirement portfolio optimized\nfor long-term performance (i.e., a period of a decade or more). Yet the proposed approach uses\nneither goal-based reward criteria nor time as a state variable, making it difficult to classify it\nunder GBWM. Dixon and Halperin [20] propose two practical algorithms - G-Learner, a generative\nframework, and GIRL, an extension of G-Learning to the setting of inverse RL. The focus is on their\nuse for GBWM (optimization of retirement plans) in the context of robo-advising services. Another\nRL-based approach is provided by Das and Varma [21], where Q-learning is used to obtain the same\nstrategy as in their previous work using dynamic programming [16]. The comparative advantages of\nthe RL approach, particularly its superior handling of larger state and action spaces, are accentuated\nby the authors.\n3\nMethodology\nWe consider the GBWM framework as introduced in [16,21] and significantly expand it, primarily\nby a) putting forth a non-tabular approach based on deep neural networks (NNs) and b) introducing\nmore rigorous training and testing procedures with an emphasis on robustness and generalization.\n3.1\nMarkov Decision Process for Goal-Based Wealth Management\nThe underlying problem is modeled as a discrete-time Markov Decision Process (MDP) and then\napproached as an episodic RL task. An MDP is defined as a quintuple (S, A, P, R, γ), where S\nis the state space, A the action space, P : S × A × S 7→[0, 1] a transition probability function,\nR : S × A 7→R a reward function and γ ∈[0, 1] a discount factor.\n3.1.1\nState Space\nThe state at time t is defined as st = ( t\nT , Wt\nWG ), where Wt is the current total wealth, T the target\ndate, and WG the goal wealth. Wt = wBBt + wSSt, where Bt (St) is the price of the riskless (risky)\nasset at time t, which fluctuates in time, and wB (wS) the corresponding amounts held by the\ninvestor. We assume wB, wS ≥0, i.e., short (negative) positions are not allowed. Clearly, 0 ≤t\nT ≤1\nand 0 ≤\nWt\nWG < ∞(since Bt ≥0, St ≥0). The inclusion of\nWt\nWG , a simple measure of how close\nthe investor is (in relative terms) to achieving their goals, induces position-dependent behavior in\npolicies, which is in line with the GBWM risk paradigm. The state space variables are normalized\nwith respect to the reference (target) values.\n3\n3.1.2\nAction Space\nThe action at time t is given by: at = wS, where wS ∈[0, 1] represents the proportion of funds\ninvested in the risky asset St at time t. Since wS + wB = 1, the weight corresponding to the riskless\nasset is uniquely defined.\n3.1.3\nReward Function\nGBWM objectives are naturally framed as binary goals.\nConsequently, the RL reward is only\nreceived at the end of the episode, provided the goal is reached (rT = 1{WT ≥WG}). Otherwise, the\nrewards equal zero. This is a case of sparse rewards, which generally tend to be more difficult to\nlearn from.\n3.2\nDataset\nOur study uses a two asset model, which is a common approach in goal-based investing to balance\ncapital preservation and growth. While this approach somewhat limits the diversity of the portfolio,\nthis paper focuses on a simple model which allows a direct comparison with other common approaches\nin the literature. We employ a dataset of monthly returns of the S&P 500 index as the risky asset\nand bonds as the risk-free asset, made available by R. Shiller4. The dataset was split into a training\nset covering returns from 1901 to 1991 and a testing set covering returns from 1992 to 2022.\n3.3\nTraining Procedure\nFirst, each episode, representing 10 trading years, is split into 120 time steps of equal length,\neach corresponding to a single trading month. At the beginning of each time step, the investor\ndecides what percentage to invest in the (non-)risky asset and allocates the wealth accordingly.\nThis procedure is iterated until the target date is reached and the episode terminates. Considering\nthat historical data only provides a single trajectory, while simulating data from a multivariate\nnormal distribution with fixed parameters may detract from realism, we propose an alternative\ndata simulation mechanism, presented below. The goal here is to both extract as much as possible\nfrom the historical market data and provide the DRL agent with a sufficiently large training set.\nFor simplicity, it is assumed that the investor begins at state s0 = (0, 0.6), i.e., with the initial\ninvestment equal to 60% of the goal.\n3.3.1\nData Generation Procedure\nThe training dataset is represented by {R1, . . . , RNtrain}, where Rt = (Rbond\nt\n, Rstock\nt\n) contains\nbond and stock returns at time t. The trajectories used for training the DRL agent are generated\nby the following procedure:\n1. choose an index k ∈{n + 1, ..., Ntrain} randomly,\n2. use the series of returns {Rk−n, ..., Rk} to estimate the mean vector µ = (µbond, µstock)\nand the covariance matrix Σ, where n denotes the window size of returns used for estimation\n(n < k),\n3. sample the multivariate normal distribution N(µ, Σ) to obtain a trajectory.\nThe parameter n was set to 120 months, the same as the trajectory (and episode) length. The main\nidea behind this type of procedure is to generate a variety of trajectories emanating from different\ndistributions, with the goal of enhancing the robustness of the DRL agent by exposing it to varying\nconditions during training.\n4http://www.econ.yale.edu/~shiller/data.htm\n4\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nTime\n60%\n70%\n80%\n90%\n100%\nWealth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRisky asset weight\nFigure 2: Policy learned by the DRL agent. The agent was trained for 200000 episodes, 120 steps\neach, and the training wall-clock time was around 1 hour with the available computational resources.\n3.3.2\nAlgorithm and Network Architecture\nTabular RL methods (as seen in [21]) have difficulties in evaluating the values of rarely encountered\nstates. This makes it challenging for the RL agent to determine the optimal policy. DRL approaches,\nemploying function approximation, hence offer a more promising path. We use a state-of-the-art,\nactor-critic-based algorithm Proximal Policy Optimization (PPO) [22]. The objective function used\nby PPO is given by:\nJ(θ) = E [min (rθA(s, a), clip(rθ, 1 −ϵ, 1 + ϵ)A(s, a))] ,\nwhere θ denotes the policy parameters, E the empirical expectation, rθ the ratio of probabilities\nunder the new and old policies, ϵ a hyperparameter, and A the advantage function. The clipping\nfactor (clip) is used to prevent large policy changes. A feed-forward NN with 2 hidden layers, 6\nneurons each, is employed, with the ReLU activation function.\nThe discount factor γ is set to\n1, and the choice of the very small learning rate of 0.0001 was guided by the stochasticity of the\nenvironment. The Stable Baselines3 [23] library and PyTorch were used for the implementation.\n4\nResults\n4.1\nReinforcement Learning Policy\nThe resulting DRL policy is shown5 in Fig. 2. It depicts the state space and the actions selected\nby the agent in each state. If the agent is in a certain state, the color displayed on the heatmap\npresents the agent’s action.\nThe policy’s dependence on both wealth and time is evident.\nThe\nobtained policy is intuitive, easily interpretable, and satisfies properties associated with a glide path\ninvestment strategy, specifically taking on greater risk earlier on in the investment period. The\nagent uses a less risky portfolio when the current amount of wealth is sufficient or when there is still\nenough time to achieve the target. In contrast, a riskier portfolio is favored when the goal has not\nyet been reached despite the end of the episode drawing near.\n4.2\nPerformance Results\nThe analytical and numerical approaches presented in Section 2 were used as benchmarks to eval-\nuate the performance of the RL agent fairly. This ensures rigorous testing with as many as four\nbenchmarks, surpassing the number of benchmarks used in previous literature. The risk parameters\nrequired for determining Merton’s constant (risk aversion parameter γ) and the variance budget-\ning approach were chosen based on their performance on the training set. Figure 3 displays the\n5Since the original policy found by PPO is stochastic, its determinism is enforced by returning the mode of the\ndistribution over the action space instead of sampling from it.\n5\npercentage of successful episodes per value for each method, i.e., those that reached the goal for a\ncertain fixed parameter. Subfigure 3a shows the selection of the risk aversion parameter for Merton’s\nconstant, which was searched for in the interval [0.004, 0.05] to include all possible options for the\nconstant strategy. This interval was selected because γ ≤0.004 produces a stock-only portfolio, i.e.,\nα = 1, while values of γ ≥0.05 result in a bond-only portfolio, i.e., α = 0. The intermediate values\nof γ yield mixed portfolios. The best result on the training set was achieved with γ = 0.004. For the\nvariance budgeting approach, the optimal risk budget on the training set was v = 1.3%, as depicted\nin Subfigure 3b. The parameter was searched within the range of [0.001, 0.02] for the same reason\nas in the case of Merton’s constant.\n0.004 0.013 0.023 0.032 0.042\nRisk aversion\n20%\n25%\n30%\n35%\n40%\nSuccess rate\n = 0.004\n(a) Merton’s constant\n0.001 0.005 0.009 0.013 0.017\nVariance budget\n20%\n30%\n40%\n50%\n60%\nSuccess rate\nv = 0.013\n(b) Variance budgeting\nFigure 3: Choice of parameters for benchmark methods\nTo demonstrate the robustness of the trained RL agent, three distinct methods of testing were\nemployed, based on the use of a) real historical trajectories, b) simulated data, and c) bootstrapped\ndata.The aggregated results are shown in Table 1, with the following abbreviations: DG – Determin-\nistic glide path, MC – Merton’s constant, VB – Variance budgeting, DP – Dynamic programming,\nRL – Reinforcement learning.\nTable 1: Comparison of results - DRL vs benchmarks\nHistorical data\nSimulated data\nBootstrapped data\n(window size)\n(block size)\n36\n{24, 36, 48}\n60\n1\n{1,2,3}\n{4,5,6}\nDG\n54.3%\n70.5%\n70.3%\n69.7%\n79.2%\n76.9%\n75.1%\nMC\n67.0%\n71.8%\n72.3%\n67.0%\n80.7%\n77.4%\n74.6%\nVB\n68.6%\n74.1%\n74.1%\n70.6%\n87.2%\n83.5%\n80.1%\nDP\n74.4%\n73.7%\n73.8%\n74.7%\n88.9%\n84.4%\n80.7%\nRL\n77.5%\n76.7%\n76.4%\n75.3%\n90.8%\n86.3%\n82.3%\n4.2.1\nHistorical Market Data\nThe testing on historical market data was performed by using overlapping historical trajectories of\nmonthly returns from January 1991 to June 2022. The testing dataset includes 378 data points.\nConsidering that series of lengths 120 are needed (the number of months in 10 years), it is possible\nto generate 258 different (but overlapping) trajectories from the dataset. Table 1 shows the per-\ncentages of achieved targets for those time series. It is clear that the DRL agent outperforms all\nother benchmarks on historical market data. However, considering both the dependence between\n6\n0.02\n0.01\n0.00\n0.01\n0.02\n0.03\nMean return estimates\n0\n50\n100\n150\n200\nFrequency\nWindow size = 36\nWindow size = 60\nWindow size = 120\nFigure 4: Distribution of mean return estimates for each window size, together with Gaussian kernel\ndensity estimates.\n0\n20\n40\n60\n80\n100\n120\nTime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRisky asset weight\nDG\nMC\nDP\nVB\nRL\nFigure 5: Glide paths of different strategies\nindividual paths and the limited amount of historical data, it is essential to have another approach\nto test the robustness and efficacy of the obtained DRL agent.\n4.2.2\nSimulated Data\nThe data was acquired using the training procedure outlined in Subsection 3.3.1, with the only\ndifference being the use of historical market data from 1991 to 2022 as the underlying dataset.\nTo construct a testing set that encompasses a wider range of potential scenarios, simulations were\nperformed with different window sizes for parameter estimation in Step 2 of the procedure. Figure\n4 shows the distributions of mean return estimates for different window sizes. We note that the\nuse of shorter estimation windows results in an increase in the mean return variance. Consequently,\nreturns generated from distributions calibrated on smaller window sizes are more versatile, leading to\nmore diverse trajectories and, in turn, the enhanced robustness of the RL agent. Two fixed window\nsizes, 36 and 60, were considered, and an additional experiment was conducted in which a window\nsize was randomly chosen from the set {24, 36, 48} for every trajectory anew. For testing purposes,\n10000 trajectories were simulated for each setting. The Table shows the percentage of successful\nportfolio allocations, i.e., ones that led to achieving the goal. Again, the DRL model outperforms\nthe benchmark methods. Figure 5 displays the average proportion of the portfolio invested in the\nrisky asset in the case of the window length equal to 36. Each point on the graph represents the\naverage allocation of a given method at a specific time step. All of the obtained glide paths with\nthe sole exception of MC indicate risk reduction over time, as is typical for GBWM.\n7\n4.2.3\nBootstrapped Data\nThe bootstrap, originally presented by Efron [24], is a statistical method that involves generating\nnew data by simulating from an existing data set. It is a resampling approach in which multiple\nnew samples are generated from the original dataset with replacement, thereby creating a new\ndataset that captures the variability and uncertainty of the original data.\nThis technique was\nused to generate 10000 test trajectories from historical returns. Additionally, a variation of the\nbootstrap method known as block bootstrapping was employed. Block bootstrapping is specifically\ndesigned for time series data and involves sampling blocks of consecutive observations, preserving\nthe correlations within each block while still generating synthetic data. It was used to generate\n10000 trajectories, each one generated by first choosing a block size b from a given set of values,\nand then sampling from the testing set to acquire a 120-month-long series. If b = 1 the method\ndeteriorates to regular bootstrapping. For b = 2 a trajectory is obtained by sampling 60 blocks of\nconsecutive returns of two months. Three variations of block sizes were used for testing, and the\ncorresponding success rates are presented in Table 1.\nGiven that the agent was trained on data generated by the simulator, it has been ex-\nposed to a much wider range of market conditions than those in the historical test set. However,\nall of the considered methods (all the benchmark methods and the proposed approach) seem to\nperform better on out-of-sample simulated data than on historical market data.\nThis points to\nthe fact that historical market data likely exhibits more complex dynamics which cannot easily be\nreplicated by the simulation model — however, the simulation model allows us to generate a large\nnumber of trajectories, which is crucial in training the agent. Moreover, the results evidently testify\nto the fact that the proposed approach yields improved performance over the benchmarks in all of\nthe considered test cases, including historical data.\n5\nConclusion and Future Work\nIn this paper, a novel approach for goal-based wealth management based on deep reinforcement\nlearning is presented.\nThe results demonstrate that the proposed method outperforms multiple\nestablished benchmarks on both historical and simulated market data, using multiple testing pro-\ncedures. Our study provides evidence that the proposed approach can be a valuable addition to\nexisting wealth management strategies, as it offers improved performance and potential for prac-\ntical applications in various financial settings. Despite the challenges posed by the complexity of\ndecision-making processes in deep reinforcement learning, this paper presents a highly explainable\npolicy for the agent, indicating that progress is being made in improving the interpretability of these\nsystems. Future work should consider the following possibilities: First, the present work could be\nused in the context of regime-based asset allocation [25], for example by expanding the state space\nto include market regime-based features. This would require the development of a more complex\nfinancial market simulator capable of modeling non-stationary effects. Second, the approach might\nbe recast from the perspective of more sophisticated GBWM frameworks that take into account\nmultiple future goals [16,26]. Third, different risk preferences might be considered to pave the way\ntoward more personalized wealth management. Using a non-binary reward function, as opposed to\nsolely aiming to attain the predetermined target wealth, is expected to lead to more precise catering\nto investors’ preferences. Additionally, cash infusion during the investment period could also be\nincorporated into the framework, as is typical in retirement and goal-based investing [16]. Lastly,\ngeneralizations to multi-asset scenarios present a potentially fruitful further step, as multi-asset al-\nlocation provides a more diversified investment portfolio, reducing overall risk and increasing the\nlikelihood of achieving financial goals.\n5.0.1\nAcknowledgements\nThis work was supported in part by the Croatian Science Foundation under Project 5241, and in\npart by the European Regional Development Fund under Grant KK.01.1.1.01.0009 (DATACROSS).\n8\nReferences\n[1] Nevins, D.: Goals-based investing: Integrating traditional and behavioral finance. The Journal\nof Wealth Management, 6(4), 8–23 (2004), doi:10.3905/jwm.2004.391053\n[2] Iyer, A.K., Hoelscher, S.A. and Mbanga, C.L.: Target Date Funds, Drawdown Risk, and Central\nBank Intervention: Evidence during the COVID-19 Pandemic. Journal of Risk and Financial\nManagement 15(9), 408 (2022), doi:10.3390/jrfm15090408\n[3] Gomes, F.: Portfolio Choice Over the Life Cycle: A Survey. Annual Review of Financial Eco-\nnomics, 12, 277–304 (2020), doi:10.1146/annurev-financial-012820-113815\n[4] Blanchett, D. Dynamic allocation strategies for distribution portfolios: determining the optimal\ndistribution glide path. Journal of Financial Planning 20(12), 68–81 (2007).\n[5] Capponi, A.: Robo-Advising: Personalization and Goals-Based Investing. University of Cal-\nifornia Berkeley, https://cdar.berkeley.edu/sites/default/files/slides_capponi.pdf\n(2022).\n[6] Zhang, Z., Zohren, S., Roberts, J.: Deep Reinforcement Learning for Trading. The Journal of\nFinancial Data Science, 2 (2), 25–40 (2019), doi:10.48550/arXiv.1911.10107\n[7] Théate, T. and Ernst, D.: An Application of Deep Reinforcement Learning to Algorithmic\nTrading. Expert Systems with Applications, 173, (2021), doi:10.1016/j.eswa.2021.114632\n[8] Avellaneda, M. and Stoikov, S.: High-frequency trading in a limit order book. Quantitative\nFinance, 8(3), 217–224 (2008), doi:10.1080/14697680701381228\n[9] Spooner, T. and Savani, R.: Robust Market Making via Adversarial Reinforcement Learning.\nProceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-\n20), (2020), doi:10.48550/arXiv.2003.01820\n[10] Gašperov, B. and Kostanjčar Z.: Market Making With Signals Through Deep Reinforcement\nLearning. IEEE Access, 9, 61611–61622 (2021), doi:10.1109/ACCESS.2021.3074782\n[11] Gašperov, B. and Kostanjčar, Z.: Deep Reinforcement Learning for Market Making Under a\nHawkes Process-Based Limit Order Book Model. IEEE Control Systems Letters, 6, 2485–2490\n(2022), doi:10.1109/LCSYS.2022.3166446\n[12] Lin, S. and Beling, P.A.: A deep reinforcement learning framework for optimal trade execution.\nIn: Joint European Conference on Machine Learning and Knowledge Discovery in Databases,\npp. 223–240. Springer, Cham (2021), doi:10.1007/978-3-030-67670-4_14.\n[13] Hickman, K., Hunter, H., Byrd, J., Beck, J., and Terpening, W.:\nLife Cycle Investing,\nHolding Periods, and Risk. The Journal of Portfolio Management, 27(2), 101–111, (2001),\ndoi:10.3905/jpm.2001.319796\n[14] Bodie, Z., and Crane, D. B.: Personal Investing: Advice, Theory, and Evidence. Financial\nAnalysts Journal, 53(6), 13—23. (1997), doi:10.2139/ssrn.36158\n[15] Forsyth, P., Li, Y. and Vetzal, K.: Are target date funds dinosaurs? Failure to adapt can lead\nto extinction. arXiv preprint arXiv:1705.00543, (2017), doi:10.48550/arXiv.1705.00543\n[16] Das, S.R., Ostrov, D., Radhakrishnan, A. and Srivastav, D.: Dynamic portfolio allocation in\ngoals-based wealth management. Computational Management Science, 17(4), 613–640 (2020),\ndoi:10.1007/s10287-019-00351-7\n[17] Merton, R. C.:Lifetime portfolio selection under uncertainty: The continuous-time case. The\nReview of Economics and Statistics, 51, 247–257 (1969), doi:10.2307/1926560\n9\n[18] Bruder, B., Culerier, L., and Roncalli, T.: How to Design Target-Date Funds? Available at\nSSRN 2289099 (2012), doi:10.2139/ssrn.2289099\n[19] Pendharkar, P.C. and Cusatis, P.: Trading financial indices with reinforcement learning agents.\nExpert Systems with Applications, 103, 1–13 (2018), doi:10.1016/j.eswa.2018.02.032\n[20] Dixon, M. and Halperin, I.: G-learner and girl: Goal based wealth management with reinforce-\nment learning. arXiv preprint arXiv:2002.10990 (2020), doi:10.48550/arXiv.2002.10990\n[21] Das, S.R., Varma, S.: Dynamic Goals-Based Wealth Management using Reinforcement Learn-\ning. Journal Of Investment Management, 18(2), 1–20 (2020).\n[22] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O.: Proximal Policy Opti-\nmization Algorithms. arXiv preprint arXiv:1707.06347 (2017), doi:10.48550/arXiv.1707.06347\n[23] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M. and Dormann, N.:\nStable-\nBaselines3: Reliable Reinforcement Learning Implementations, Journal of Machine Learning\nResearch, 22(1), 12348–12355 (2021).\n[24] Efron, B.: Bootstrap methods: another look at the jackknife. The Annuals of Statistics, 7(1),\n1–26 (1979), doi:10.1214/aos/1176344552\n[25] Nystrup, P., Hansen, B.W., Madsen, H. and Lindström, E.,ž: Regime-based versus static asset\nallocation: Letting the data speak. The Journal of Portfolio Management, 42(1), 103–109 (2015),\ndoi:10.3905/jpm.2015.42.1.103\n[26] Capponi,\nA.\nand\nZhang,\nY.:\nGoal\nBased\nInvestment\nManagement.\nAvailable\nat\nSSRN: https://ssrn.com/abstract=4121931 or http://dx.doi.org/10.2139/ssrn.4121931\n(2022), doi:10.2139/ssrn.4121931\n10\n",
  "categories": [
    "q-fin.PM",
    "cs.LG"
  ],
  "published": "2023-07-25",
  "updated": "2023-07-25"
}