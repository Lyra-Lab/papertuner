{
  "id": "http://arxiv.org/abs/2307.15690v1",
  "title": "Benchmarking Offline Reinforcement Learning on Real-Robot Hardware",
  "authors": [
    "Nico Gürtler",
    "Sebastian Blaes",
    "Pavel Kolev",
    "Felix Widmaier",
    "Manuel Wüthrich",
    "Stefan Bauer",
    "Bernhard Schölkopf",
    "Georg Martius"
  ],
  "abstract": "Learning policies from previously recorded data is a promising direction for\nreal-world robotics tasks, as online learning is often infeasible. Dexterous\nmanipulation in particular remains an open problem in its general form. The\ncombination of offline reinforcement learning with large diverse datasets,\nhowever, has the potential to lead to a breakthrough in this challenging domain\nanalogously to the rapid progress made in supervised learning in recent years.\nTo coordinate the efforts of the research community toward tackling this\nproblem, we propose a benchmark including: i) a large collection of data for\noffline learning from a dexterous manipulation platform on two tasks, obtained\nwith capable RL agents trained in simulation; ii) the option to execute learned\npolicies on a real-world robotic system and a simulation for efficient\ndebugging. We evaluate prominent open-sourced offline reinforcement learning\nalgorithms on the datasets and provide a reproducible experimental setup for\noffline reinforcement learning on real systems.",
  "text": "Published as a conference paper at ICLR 2023\nBENCHMARKING OFFLINE REINFORCEMENT\nLEARNING ON REAL-ROBOT HARDWARE\nNico Gürtler1, Sebastian Blaes1, Pavel Kolev1, Felix Widmaier1, Manuel Wüthrich2,\nStefan Bauer3, Bernhard Schölkopf 1, and Georg Martius1\n1Max Planck Institute for Intelligent Systems∗\n2Harvard University\n3KTH Stockholm\nABSTRACT\nLearning policies from previously recorded data is a promising direction for real-\nworld robotics tasks, as online learning is often infeasible. Dexterous manipu-\nlation in particular remains an open problem in its general form. The combina-\ntion of offline reinforcement learning with large diverse datasets, however, has\nthe potential to lead to a breakthrough in this challenging domain analogously\nto the rapid progress made in supervised learning in recent years. To coordi-\nnate the efforts of the research community toward tackling this problem, we\npropose a benchmark including: i) a large collection of data for offline learn-\ning from a dexterous manipulation platform on two tasks, obtained with capa-\nble RL agents trained in simulation; ii) the option to execute learned policies\non a real-world robotic system and a simulation for efficient debugging. We\nevaluate prominent open-sourced offline reinforcement learning algorithms on\nthe datasets and provide a reproducible experimental setup for offline reinforce-\nment learning on real systems. Visit https://sites.google.com/view/\nbenchmarking-offline-rl-real for more details.\n1\nINTRODUCTION\nReinforcement learning (RL) (Sutton et al., 1998) holds great potential for robotic manipulation and\nother real-world decision-making problems as it can solve tasks autonomously by learning from\ninteractions with the environment. When data can be collected during learning, RL in combination\nwith high-capacity function approximators can solve challenging high-dimensional problems (Mnih\net al., 2015; Lillicrap et al., 2016; Silver et al., 2017; Berner et al., 2019). However, in many cases\nonline learning is not feasible because collecting a large amount of experience with a partially trained\npolicy is either prohibitively expensive or unsafe (Dulac-Arnold et al., 2020). Examples include\nautonomous driving, where suboptimal policies can lead to accidents, robotic applications where the\nhardware is likely to get damaged without additional safety mechanisms, and collaborative robotic\nscenarios where humans are at risk of being harmed.\nOffline reinforcement learning (offline RL or batch RL) (Lange et al., 2012) tackles this problem by\nlearning a policy from prerecorded data generated by experts or handcrafted controllers respecting\nthe system’s constraints. Independently of how the data is collected, it is essential to make the best\npossible use of it and to design algorithms that improve performance with the increase of available\ndata. This property has led to unexpected generalization in computer vision (Krizhevsky et al.,\n2012; He et al., 2016; Redmon et al., 2016) and natural language tasks (Floridi & Chiriatti, 2020;\nDevlin et al., 2018) when massive datasets are employed. With the motivation to learn similarly\ncapable decision-making systems from data, the field of offline RL has gained considerable attention.\nProgress is currently measured by benchmarking algorithms on simulated domains, both in terms of\ndata collection and evaluation.\n∗Correspondence to nico.guertler@tuebingen.mpg.de\n1\narXiv:2307.15690v1  [cs.LG]  28 Jul 2023\nPublished as a conference paper at ICLR 2023\nFigure 1: The TriFinger manipulation platform (Wüthrich et al., 2021; Bauer et al., 2022). Left:\nThe robot has 3 arms with 3 DoF each. The cube is constrained by a bowl-shaped arena, allowing for\nunattended data collection. Right: A cluster of these robots for parallel data collection and evaluation.\nYet, real-world data differs from simulated data qualitatively and quantitatively in several as-\npects (Dulac-Arnold et al., 2020). First, observations are noisy and sometimes faulty. Second,\nreal-world systems introduce delays in the sensor readings and often have different sampling rates\nfor different modalities. Third, the action execution can also be delayed and can get quantized by\nlow-level hardware constraints. Fourth, real-world environments are rarely stationary. For instance,\nin autonomous robotics, battery voltages might drop, leading to reduced motor torques for the same\ncontrol command. Similarly, thermal effects change sensor readings and motor responses. Abrasion\nchanges friction behavior and dust particles can change object appearances and sensing in general.\nFifth, contacts are crucial for robotic manipulation but are only insufficiently modeled in current\nphysics simulations, in particular for soft materials. Lastly, physical robots have individual variations.\nSince real-world data is different from simulated data, it is important to put offline RL algorithms to\nthe test on real systems. We propose challenging robotic manipulation datasets recorded on real robots\nfor two tasks: object pushing and object lifting with reorientation on the TriFinger platform (Wüthrich\net al., 2021). To study the differences between real and simulated environments, we also provide\ndatasets collected in simulation. Our benchmark of state-of-the-art offline RL algorithms on these\ndatasets reveals that they are able to solve the moderately difficult pushing task while their perfor-\nmance on the more challenging lifting task leaves room for improvement. In particular, there is a much\nlarger gap between the performance of the expert policy and offline-learned policies on the real system\ncompared to the simulated system. This underpins the importance of real-world benchmarks for\noffline RL. We furthermore study the impact of adding suboptimal trajectories to expert data and find\nthat all algorithms are ‘distracted’ by them, i.e., their success rate drops significantly. This identifies\nan important open challenge for the offline RL community: robustness to suboptimal trajectories.\nImportantly, a cluster of TriFinger robots is set up for evaluation of offline-learned policies for which\nremote access can be requested for research purposes. With our dataset and evaluation platform, we\ntherefore aim to provide a breeding ground for future offline RL algorithms.\n2\nTHE TRIFINGER PLATFORM\nWe use a robot cluster that was initially developed and build for the Real Robot Challenge in 2020\nand 2021 (Bauer et al., 2022). The robots that constitute the cluster are an industrial-grade adaptation\nof a robotic platform called TriFinger, an open-source hardware and software design introduced\nin Wüthrich et al. (2021), see Fig. 1.The robots have three arms mounted at a 120 degrees radially\nsymmetric arrangement with 3 degrees of freedom (DoF) each. The arms are actuated by outrunner\nbrushless motors with a 1:9 belt-drive, yielding high agility, low friction, and good force feedback\n(details on the actuator modules can be found in Grimminger et al. (2020)). Pressure sensors inside the\nelastic fingertips provide basic tactile feedback. The working area, where objects can be manipulated,\nis encapsulated by a high barrier to ensure that the object stays inside the arena even during aggressive\nmotions. This is essential for operation without human supervision. The robot is inside a closed\nhousing that is well lit by top-mounted LED panels making the images taken by three high-speed\nglobal shutter cameras consistent. The cameras are distributed between the arms to ensure objects are\nalways seen by any camera. We study dexterous manipulation of a cube whose pose is estimated by a\nvisual tracking system with 10 Hz.\n2\nPublished as a conference paper at ICLR 2023\nPPO\ntraining\ndata collection\noﬄine learning\ndomain\nrandomization\nreal\nTriFinger\nPybullet\nsimulation\nIsaac \nparallel simulation\nsim-\ndatasets\noﬄine RL\noﬄine RL\nreal-\ndatasets\nevaluation\nevaluation\nFigure 2: Overview of our approach. Policies are trained with domain randomization in a parallel\nsimulation using Isaac Gym (Makoviychuk et al., 2021) and then deployed in the PyBullet (Coumans\n& Bai, 2016) simulation and on the real system without fine-tuning to collect the datasets. We train\nstate-of-the-art offline RL algorithms on these datasets and evaluate them on the respective system,\ni.e, the simulator or the real-robot cluster.\nTo abstract away the low-level details, we developed a software with a simple Gym (Brockman et al.,\n2016) interface in Python that can interact with the robots at a maximal rate of 1 kHz in position control\nor torque control mode (see (Wüthrich et al., 2021) for details). We use a control frequency of 50 Hz\nand torque control for this work. We have a custom object tracking tool to provide position and orien-\ntation of a single colored cube in the environment, allowing algorithms to work without visual input.\nOn top of this interface, we have developed a submission system that allows users to submit jobs to a\ncluster of these robots for unattended remote execution. This setup was specifically adapted for ease\nof use in the offline RL setting and was extensively tested. We will provide researchers with access to\nthe robot cluster, which will allow them to evaluate the policies they trained on the dataset proposed\nherein. To ease development and study the fundamental differences between simulated and real world\ndata, we provide a corresponding simulated environment using PyBullet (Coumans & Bai, 2016).\nTo summarize, the hardware and software have the following three key properties: i) physically\ncapable of dexterous manipulation; ii) robust enough for running and evaluating learning methods,\nand iii) easy to use (robot hardware and simulator) and integrated in existing code frameworks.\n3\nTHE TRIFINGER DATASETS\nWe consider two tasks that involve a colored cube: pushing the cube to a target location and lifting\nthe cube to a desired location and orientation. To create behavioral datasets for these tasks on the\nTriFinger platform, we need expert policies, which we obtain using reinforcement learning in a\nparallel simulation environment with domain randomization. Fig. 2 visualizes the entire procedure –\nfrom training to data collection to offline learning. In this section, we describe the tasks and the data\ncollection while Sec. 4 is dedicated to benchmarking offline RL algorithms on the collected data.\n3.1\nDEXTEROUS MANIPULATION TASKS\nWe consider two tasks on the TriFinger platform that require dexterous manipulation of a cube:\nPush The goal is to move the cube to a target location. This task does not require the agent to align\nthe orientation of the cube; the reward is based only on the desired and the achieved position.\nLift\nThe cube has to be picked up and moved to a target pose in the air which includes position\nand orientation. This requires flipping the cube on the ground, obtaining a stable grasp, lifting it to a\ntarget location and rotating it to match the target orientation.\nFollowing prior work (Hwangbo et al., 2019; Allshire et al., 2022) we define the reward by applying a\nlogistic kernel k(x) = (b + 2) (exp(a∥x∥) + b + exp(−a∥x∥))−1 to the difference between desired\nand achieved position (for the Push task) or the desired and achieved corner points of the cube (for\n3\nPublished as a conference paper at ICLR 2023\nthe Lift task). The parameters a and b control the length scale over which the reward decays and how\nsensitive it is for small distances x, respectively. This yields a smooth, dense and bounded reward,\nsee Appendix B.3 for details. We define success in the pushing task as reaching the goal position with\na tolerance of 2 cm. For the lifting task we additionally require to not deviate more than 22 degrees\nfrom the goal orientation.\nWe note that pushing is more challenging than it may appear due to inelastic collisions between the\nsoft fingertips and the cube which are not modeled in rigid body physics simulators. Furthermore, the\nperformance of policies can be quite sensitive to the value of sliding friction. Lifting is, however,\neven more challenging as flipping the cube based on a noisy object-pose with a low refresh rate\nis error-prone, learning the right sequence of behaviors requires long-term credit assignment and\ndropping the cube often results in loosing all progress in an episode.\n3.2\nTRAINING EXPERT POLICIES\nWe train expert policies with online RL in simulation which we then use for data collection on the\nreal system (Fig. 2). We build on prior work which achieved sim-to-real transfer for a dexterous\nmanipulation task on the TriFinger platform (Allshire et al., 2022). This approach replicates the\nreal system in a fast, GPU-accelerated rigid body physics simulator (Makoviychuk et al., 2021) and\ntrains with an optimized implementation (Makoviichuk & Makoviychuk, 2022) of Proximal Policy\nOptimization (Schulman et al., 2017) with a high number of parallel actors. We furthermore adopt\ntheir choice of a control frequency of 50 Hz and use torque control to enable direct control of the\nfingers. The sensor input is the proprioceptive sensor information and the object pose. In order to\nobtain policies that are robust enough to work on the real system, domain randomization (Tobin et al.,\n2017; Mandlekar et al., 2017; Peng et al., 2018) is applied: for each episode, physics parameters like\nfriction coefficients are sampled from a distribution that is likely to contain the parameters of the real\nenvironment. Additionally, noise is added to the observations and actions to account for sensor noise\nand the stochasticity of the real robot. Furthermore, random forces are applied to the cube to obtain a\npolicy that is robust against perturbations, as in (Andrychowicz et al., 2020). The object and goal\nposes are represented by keypoints, i.e., the Cartesian coordinates of the corners of the cube. This\nchoice was empirically shown to accelerate training compared to separately encoding position and\norientation in Cartesian coordinates and a quaternion (Allshire et al., 2022).\nTo improve the robustness of the trained policies across the different robot instances and against other\nreal-world effects like tracking errors due to accumulating dust, we modified the implementation of\nAllshire et al. (2022) in several ways. While the original code correctly models the 10 Hz refresh\nrate of the object pose estimate, we found it beneficial to also simulate the delay between the time\nwhen the camera images are captured and when they are provided to the agent. This delay typically\nranges between 100 ms and 200 ms. We furthermore fully randomize the initial orientation of the\ncube and use a hand-crafted convex decomposition of the barrier collision mesh to avoid artifacts of\nthe automatic decomposition, to which the policy can overfit. For the pushing task we penalized rapid\nchanges in the cube position and orientation as sliding and flipping the cube transfers less well to the\nreal system than moving it in a slow and controlled manner. We observed that policies trained with\nRL in simulation tend to output oscillatory actions, which does not transfer well to the real system and\ncauses stronger wear effects (Mysore et al., 2021). To avoid this, we penalized changing the action\nwhich led to smoother movements and better performance. For the Lift task we additionally consider\na policy which was trained with an Exponential Moving Average on the actions as we observed that\nvibrations on the real robot can lead to slipping and dropping (see Fig. S14 (b)). These vibrations\nmight be caused by elastic deformations of the robot hardware and the complex contact dynamics\nbetween the soft fingertips and the cube that are not modeled in simulation. As also observed in\nWang et al. (2022), the performance on the real system varies significantly with training seeds. We\nevaluated 20 seeds on the real system and used the best one for data collection. More details on\ntraining in simulation can be found in the Appendix C.1.\n3.3\nDATA COLLECTION\nWe collected data for the pushing and lifting tasks both in a PyBullet (Coumans & Bai, 2016)\nsimulation (Joshi et al., 2020) and on the real system, as shown in Fig. 2. To ensure that the data\ncollection procedures are identical, we run the same code with the simulator backend and with the\n4\nPublished as a conference paper at ICLR 2023\nFigure 3: An example behavior of our expert policy on the lifting task. The robot is grasping and\nreorienting the object to reach the desired target location and orientation (transparent cube).\nreal-robot backend on six TriFinger platforms (Fig. 1). We observed that policies learned in simulation\nstruggle with retrieving the cube from the barrier on the real robot1. To alleviate this problem, we\nemploy a predefined procedure to push the cube away from the barrier between episodes. The\nresulting starting positions together with the target positions are visualized in Fig. S12. During data\ncollection, self-tests are performed at regular intervals to ensure that all robots are fully functional.\nFor the dataset to be useful to the community, it should be possible to evaluate offline-learned policies\non the real robots. As machine learning models can be computationally demanding, we wait for a\nfixed time interval between receiving a new observation and starting to apply the action based on this\nobservation. This time budget is allocated for running the policy without deviating from the control\nfrequency used for data collection. We choose 10 ms for the Push task and 2 ms for the Lift task as\nwe found training with bigger delays difficult. Note that our expert policies run in less than 1 ms.\nWe provide as much information about the system as possible in the observations. The robot state is\ncaptured by joint angles, angular velocities, recorded torques, fingertip forces, fingertip positions,\nfingertip velocities (both obtained via forward kinematics), and the ID of the robot. The object\npose is represented by a position, a quaternion encoding orientation, the keypoints, the delay of the\ncamera images for tracking and the tracking confidence. The observation additionally contains the\nlast action, which is applied during the fixed delay between observation and action. The desired and\nachieved goal are also included in the observation and contain either a position for the Push task or\nkeypoints for the Lift task. Some observations provide redundant information; however, we believe\nthis simplifies working with the datasets, as we provide user-friendly Python code that implements\nfiltering as well as automatically converting observations to a flat array. Moreover, we additionally\npublish a version of each dataset with camera images from three viewpoints. We believe that directly\nlearning from these image datasets is an exciting challenge for the community2 and that they are\nmoreover valuable in their own right as a large collection of robotic manipulation video footage.\nFor each task we consider pure expert data (Expert), mixed data recorded with a range of training\ncheckpoints (Mixed), a combination of 50% expert trajectories and 50% trajectories recorded with a\nweaker policy with additive Gaussian noise on the actions (Weak&Expert) and the 50% expert data\nin Weak&Expert (Half-Expert). We run the same policies in simulation and on the real system. An\nexemplary expert behavior for the Lift task is shown in Fig. 3. Episodes last for 15 s for the Push task\nand 30 s for the Lift task as reorienting, grasping and lifting the cube requires more time. For the Push\ntask, we collect 16 h of interaction for each dataset, corresponding to 3840 episodes and 2.8 million\ntransitions. For the more demanding Lift task, we collect 20 h of robot interactions corresponding to\n2400 episodes and 3.6 million transitions. The Half-Expert datasets contain the expert data that is\nincluded in the corresponding Weak&Expert datasets to isolate the effects of reducing the amount\nof expert data and adding suboptimal trajectories. The average success rates are provided next to\nthe offline learning results in Table 1 and 2 (see data column). In Appendix B, we give a detailed\nanalysis along with a specification (Table S4) and statistics (Table S5) of the offline RL datasets.\n4\nBENCHMARKING OFFLINE REINFORCEMENT LEARNING ALGORITHMS\nWe benchmark offline RL algorithms on pairs of simulated and real datasets and study the impact\nof data quality on their performance. We limit our evaluation to the best algorithms provided in the\n1Possibly because the object tracking performance is slightly worse at the barrier or the dynamics of the\noutstretched fingers aligns less well between simulation and reality.\n2At the time of writing we cannot benchmark on these datasets since the robot cluster does not provide\nGPU-access at the moment. This will likely change in the near future.\n5\nPublished as a conference paper at ICLR 2023\nPush\nLift\nBC\nCRR\nAWAC\nCQL\nIQL\n0.0\n0.5\n1.0\nSuccess / return\n BC\n CRR  AWAC  CQL\n IQL\n0.0\n0.5\n1.0\nSuccess / return\nFigure 4: Average normalized success rates and returns (light colors) on real robots. Each\nquantity is normalized by the dataset mean and averaged over all Push or Lift datasets.\nTable 1: Pushing: Success rate on the TriFinger-Push Datasets. ‘data’ denotes the mean over the\ndataset. Average and standard deviation over five training seeds. A star ∗indicates significance w.r.t.\nall other methods using Welch’s t-test with p < 0.05.\nPush-Datasets\ndata\nBC\nCRR\nAWAC\nCQL\nIQL\nSim-Expert\n0.95\n0.83 ± 0.02\n0.94 ± 0.04\n0.92 ± 0.03\n0.03 ± 0.01\n0.88 ± 0.04\nSim-Half-Expert\n0.95\n0.71 ± 0.05\n0.79 ± 0.05\n0.79 ± 0.02\n0.05 ± 0.02\n0.70 ± 0.06\nSim-Weak&Expert\n0.53\n0.53 ± 0.09\n0.88 ± 0.03\n0.83 ± 0.05\n0.17 ± 0.03\n0.66 ± 0.14\nSim-Mixed\n0.76\n0.53 ± 0.04\n0.09 ± 0.10\n0.84 ± 0.06∗0.02 ± 0.01\n0.69 ± 0.07\nReal-Expert\n0.92\n0.74 ± 0.05\n0.87 ± 0.07\n0.80 ± 0.03\n0.54 ± 0.13\n0.75 ± 0.08\nReal-Half-Expert\n0.92\n0.66 ± 0.08\n0.78 ± 0.04\n0.76 ± 0.10\n0.48 ± 0.08\n0.70 ± 0.08\nReal-Weak&Expert\n0.51\n0.48 ± 0.10\n0.84 ± 0.06∗\n0.69 ± 0.06\n0.14 ± 0.04\n0.68 ± 0.05\nReal-Mixed\n0.49\n0.29 ± 0.06\n0.30 ± 0.06\n0.61 ± 0.09\n0.02 ± 0.02\n0.66 ± 0.08\nopen-source library d3rlpy (Seno & Imai, 2021) to keep the required robot time for the evaluation\nof the trained policies manageable. Namely, we benchmark the following algorithms: BC (Bain &\nSammut, 1995; Pomerleau, 1991; Ross et al., 2011; Torabi et al., 2018), CRR (Wang et al., 2020),\nAWAC (Nair et al., 2020), CQL (Kumar et al., 2020), and IQL (Kostrikov et al., 2021b). We report\nresults for two sets of hyperparameters: The default values (except for CQL for which we performed a\ngrid search on Push-Sim-Expert as the default parameters did not learn) and the results of a grid search\non Lift-Sim-Weak&Expert (marked by †). Details about the hyperparamters and their optimization\nare in Appendix C.3. We think that the performance at the default hyperparameters is highly relevant\nfor offline RL on real data as optimizing the hyperparameters without a simulator is often infeasible.\n4.1\nRESULTS\nWe train with five different seeds for each algorithm and evaluate with a fixed set of randomly sampled\ngoals. Details about the policy evaluation on the simulated and real set-up can be found in Appendix\nD. We report success rates in the main text and returns in Appendix A.\nThe benchmarking results for the Push task are summarized in Table 1. Most of the offline RL\nalgorithms perform well but the performance on the real data is generally worse. An exception to this\npattern is CQL which performs poorly on the simulated Push task and gains some performance on\nthe real data, perhaps due to the broader data distribution of the stochastic real-world environment.\nAs expected BC performs well on the expert datasets but cannot exceed the dataset success rate\non the Weak&Expert data, unlike CRR, AWAC, and IQL. On the expert data, CRR and AWAC\nmatch the performance of the expert policy in simulation but fall slightly behind on the real data.\nInterestingly, the performance of CRR is not negatively impacted by weak trajectories (e.g. 84% on\nReal-Weak&Expert compared to 78% on Real-Half-Expert, where the latter only contains the expert\ndata portion). We provide insights into how the behavior policy compares to an offline-trained policy\nfor the Push task in Fig. 6 (b) and (c).\nThe more challenging Lift task separates the algorithms more clearly as summarized in Table 2. CQL\ndoes not reach a non-zero success rate at all, despite our best efforts to optimize the hyperparameters\n(see Appendix C.3). This is in line with the results reported in Mandlekar et al. (2021) where CQL\nfailed to learn on the datasets corresponding to more complex manipulation tasks. Kumar et al. (2021)\nfurthermore discusses the sensitivity of CQL to the choice of hyperparameters, especially on robotic\ndata. While the best algorithms come close to matching the performance of the expert on Lift-Sim-\nExpert, they fall short of reaching the dataset success rate on the real-robot data (Lift-Real-Expert).\n6\nPublished as a conference paper at ICLR 2023\nTable 2: Lifting: Success rate on the TriFinger-Lift Datasets. ‘data’ denotes the mean over the\ndataset. Average and standard deviation over five training seeds. Experiments with hyperparameters\noptimized on Sim-Weak&Expert are marked with a †. Stars ∗and ∗∗indicate significance w.r.t.\nsecond best Welch’s t-test with p < 0.05 and p < 0.01, respectively.\nLift-Datasets\ndata\nBC\nCRR\nAWAC\nCQL\nIQL\nSim-Expert\n0.87\n0.64 ± 0.00\n0.80 ± 0.03\n0.75 ± 0.04\n0.00 ± 0.00\n0.47 ± 0.06\nSim-Half-Expert\n0.88\n0.64 ± 0.02\n0.78 ± 0.02∗\n0.69 ± 0.05\n0.00 ± 0.00\n0.04 ± 0.01\nSim-Weak&Expert\n0.5\n0.16 ± 0.04\n0.43 ± 0.35\n0.55 ± 0.09\n0.00 ± 0.00\n0.24 ± 0.05\nSim-Mixed\n0.68\n0.01 ± 0.01\n0.19 ± 0.06\n0.41 ± 0.09∗∗0.00 ± 0.00\n0.00 ± 0.00\nSim-Expert†\n0.87\n0.80 ± 0.04\n0.84 ± 0.01\n0.84 ± 0.01\n0.01 ± 0.01\n0.80 ± 0.03\nSim-Half-Expert†\n0.88\n0.67 ± 0.09\n0.75 ± 0.08\n0.82 ± 0.02\n0.00 ± 0.01\n0.75 ± 0.05\nSim-Weak&Expert†\n0.5\n0.47 ± 0.03\n0.69 ± 0.04\n0.54 ± 0.03\n0.00 ± 0.00\n0.64 ± 0.05\nSim-Mixed†\n0.68\n0.01 ± 0.00\n0.20 ± 0.07\n0.32 ± 0.04∗0.00 ± 0.00\n0.01 ± 0.01\nReal-Smooth-Expert\n0.64\n0.26 ± 0.09\n0.52 ± 0.12∗\n0.34 ± 0.04\n0.00 ± 0.00\n0.24 ± 0.03\nReal-Expert\n0.66\n0.27 ± 0.09\n0.57 ± 0.07∗∗0.24 ± 0.04\n0.00 ± 0.00\n0.29 ± 0.09\nReal-Half-Expert\n0.68\n0.15 ± 0.01\n0.38 ± 0.08∗∗0.12 ± 0.08\n0.00 ± 0.00\n0.12 ± 0.06\nReal-Weak&Expert\n0.40\n0.02 ± 0.04\n0.17 ± 0.09\n0.03 ± 0.06\n0.00 ± 0.00\n0.11 ± 0.13\nReal-Mixed\n0.42\n0.00 ± 0.01\n0.18 ± 0.07∗∗0.02 ± 0.01\n0.00 ± 0.00\n0.03 ± 0.02\nReal-Expert†\n0.66\n0.28 ± 0.04\n0.54 ± 0.09\n0.31 ± 0.04\n0.00 ± 0.00\n0.48 ± 0.07\nReal-Half-Expert†\n0.68\n0.25 ± 0.07\n0.37 ± 0.09\n0.36 ± 0.06\n0.01 ± 0.01\n0.30 ± 0.06\nReal-Weak&Expert†\n0.40\n0.09 ± 0.04\n0.29 ± 0.07∗\n0.12 ± 0.06\n0.00 ± 0.00\n0.15 ± 0.03\nReal-Mixed†\n0.42\n0.00 ± 0.00\n0.18 ± 0.04∗∗∗0.01 ± 0.01\n0.00 ± 0.00\n0.02 ± 0.01\nLift-Sim-Expert\nLift-Sim-Half-Expert\nLift-Sim-Weak&Expert\n0\n100\n200\nepochs\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\n0\n100\n200\nepochs\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\n0\n100\n200\nepochs\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\nBC†\nCRR†\nAWAC†\nCQL†\nIQL†\nFigure 5: Success rates during offline RL training on the simulated Lift task. The black dashed\nline indicates the performance of the expert dataset, and the red dashed line depicts average success\nrate of the Weak&Expert dataset. Shaded areas indicate the interval between the 0.25 and 0.75\nquantiles. Learning curves for all experiments are shown in Appendix A.6.\nOn the Real-Smooth-Expert dataset the success rates are, on average, slightly lower, probably due\nto the non-Markovian behavior policy. The returns are generally higher, however, likely due to the\nsmoothed expert reaching higher returns (see Table S2).\nOn the Lift-Weak&Expert datasets all algorithms perform significantly worse than on the expert data.\nThis effect is most pronounced on the real robot data and calls the ability of the algorithms to make\ngood use of the 50% expert trajectories into question. To verify that the performance drop is not due\nto only half of the expert data being available, we also train solely on the expert trajectories contained\nin the Weak&Expert dataset. We refer to this dataset as Half-Expert. We find that the resulting policy\nperforms significantly better, ruling out that the performance drop on the Lift-Weak&Expert dataset\nis exclusively caused by a lack of data. Fig. 5 shows that this is true for the simulated Lift datasets as\nwell. We conclude that the benchmarked offline RL algorithms still exhibit BC-like behavior, i.e., they\nare distracted by suboptimal data. Their poor performance on the Mixed datasets further underpins\ntheir reliance on imitation of a consistent expert as all algorithms struggle to learn from the data\ncollected by a range of training checkpoints (see tables 1 and 2 and learning curves in section A.6).\nThe Lift datasets, in particular those recorded on the real robots, generally contain more useful\nsub-trajectories than apparent from the success rate, which is defined as achieving the goal pose at the\nend of an episode. For example, during 87% of all episodes in the Lift-Real-Expert dataset the goal\npose is achieved at some point but the best algorithm CRR achieves a success rate of only 57%. This\nsuggests that these datasets have an untapped potential for trajectory stitching. As the observations\n7\nPublished as a conference paper at ICLR 2023\n(a) Impact of noise\n(b) Speed of the cube\n(c) Momentary success\n0.0\n0.5\n1.0\n1.5\n2.0\nrelative noise scale\n0.00\n0.25\n0.50\n0.75\nsuccess rate\n0\n2\n4\n6\n8\nt [s]\n0\n2\n4\ncube speed [m/s]\n0\n5\n10\nt [s]\n0.0\n0.5\nmomentary success\nExpert\nBC†\nCRR†\nAWAC†\nCQL†\nIQL†\nFigure 6: Analysis of policy behavior. (a) Success rate for simulated lifting after training on data\nwith varying relative environment noise scale (1.0 corresponds to the noise level of the real system).\nThe dashed lines indicate the performance on the real system. (b) Average speed of the cube over\na pushing episode on the real system. Some offline RL algorithms learn to move the cube faster\nthan the expert. (c) Momentary success during a pushing episode on the real system: the fraction of\nepisodes during which the goal was achieved at a given point in time (trained on expert data).\nare furthermore noisy and do not contain estimates of the velocity of the cube, training recurrent\npolicies could potentially lead to an increase in performance.\nTo investigate the impact of noise on the performance of offline RL policies and the expert, we\ncollected datasets in simulation for up to twice the noise amplitudes measured on the real system.\nFig. 6 (a) shows that the performance of the expert and the offline RL policies degrades only slowly\nwhen increasing the noise scale, ruling out that noise is the sole explanation for the performance gap\nbetween simulated and real system. As delays in the observation and action execution are already\nimplemented in the simulated environment, we conclude that other factors like more complex contact\ndynamics and elastic deformations of the fingertips and robot limbs are likely causing the larger\nperformance gap between data and learned policies on the real robots.\nTo test how well the policies learned from the datasets generalize over instances of the robot hardware,\nwe evaluated on a hold-out robot which was not used for data collection. We did not see a significant\ndifference in performance (see Appendix A.3 for details on this experiment) suggesting that the\ndatasets cover enough variations in the robot hardware to enable generalization to unseen robots.\nIn summary, CRR and AWAC generally perform best on the proposed datasets with IQL also being\ncompetitive after hyperparameter optimization. The performance gap between expert and offline RL\nis in general bigger on the real system, perhaps due to more challenging dynamics.\n5\nRELATED WORK\nOffline RL: The goal of offline RL (Levine et al., 2020; Prudencio et al., 2022) is to learn effective\npolicies without resorting to an online interaction by leveraging large and diverse datasets covering a\nsufficient amount of expert transitions. This approach is particularly interesting if interactions with\nthe environment are either prohibitively costly or even dangerous.\nOffline RL faces a fundamental challenge, known as the Distributional Shift (DS) problem, originating\nfrom two sources. There is a distribution mismatch in training, as we use the behavioral data for\ntraining, but the learned policy would create a different state-visitation distribution. The second\nproblem is that during policy improvement, the learned policy requires an evaluation of the Q-function\non unseen actions (out of distribution). An over-estimation of the Q-value on these out-of-distribution\nsamples leads to learning of a suboptimal policy.\nSeveral algorithmic schemes were proposed for addressing the DS problem: i) constraining the\nlearned policy to be close to the behavior data (Fujimoto et al., 2019; Kumar et al., 2019b; Zhang\net al., 2021; Kostrikov et al., 2021a); ii) enforcing conservative estimates of future rewards (Kumar\net al., 2020; Yu et al., 2021; Cheng et al., 2022); and iii) model-based methods that estimate the\nuncertainty via ensembles (Janner et al., 2019; Kidambi et al., 2020).\nAdditionally, other approaches include: implicitly tackling the DS problem via (advantage-weighted)\nvariants of behavioral cloning (Nair et al., 2020; Wang et al., 2020; Chen et al., 2020; Fujimoto\n& Gu, 2021) or even completely bypassing it either by removing the off-policy evaluation and\n8\nPublished as a conference paper at ICLR 2023\nperforming a constrained policy improvement using an on-policy Q estimate of the behavior policy\n(Brandfonbrener et al., 2021), or by approximating the policy improvement step implicitly by learn-\ning on-data state value function (Kostrikov et al., 2021b). An orthogonal research line considers\ncombining importance sampling and off-policy techniques (Nachum et al., 2019b;a; Zhang et al.,\n2020; Xu et al., 2021), and recently (Chen et al., 2021; Janner et al., 2021) investigated learning an\noptimal trajectory distribution via transformer architectures.\nThe simplest strategy for learning from previously collected data is behavioral cloning (BC), which\nis fitting a policy to the data directly. Offline RL can outperform BC: i) on long-horizon tasks with\nmixed expert data, e.g., trajectories collected via an expert and a noisy-expert; and ii) with expert or\nnear expert data, when there is a mismatch between the initial and the deployment state distribution.\nRL for dexterous manipulation:\nReinforcement learning was recently successfully applied to\ndexterous manipulation on real hardware (OpenAI et al., 2018; 2019; Allshire et al., 2022; Wang et al.,\n2022). These results rely on training with online RL in simulation, however, and are consequently\nlimited by the fidelity of the simulator. While domain randomization (Tobin et al., 2017; Mandlekar\net al., 2017; Peng et al., 2018) can account for a mismatch between simulation and reality in terms of\nphysics parameters, it cannot compensate oversimplified dynamics. The challenges of real-world\nenvironments have been recognized and partly modeled in simulated environments (Dulac-Arnold\net al., 2020). To overcome the sim-to-real gap entirely, however, data from real-world interactions is\nstill required, in particular for robotics problems involving contacts.\nOffline RL datasets:\nWhile offline RL datasets with data from simulated environments, like\nD4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020), have propelled the field forward,\nthe lack of real-world robotics data has been recognized (Behnke, 2006; Bonsignorio & del Pobil,\n2015; Calli et al., 2015; Amigoni et al., 2015; Murali et al., 2019). Complex contact dynamics,\nsoft deformable fingertips and vibrations are particularly relevant for robotic manipulation but\nare not modeled sufficiently well in simulators used by the RL community (Todorov et al., 2012;\nMakoviychuk et al., 2021; Freeman et al., 2021). Recently, three small real-world datasets with\nhuman demonstrations for a robot arm with a gripper (using operational space control) have been\nproposed (Mandlekar et al., 2021). Two of them require only basic lifting and dropping while\nthe third, more challenging task could not be solved with the available amount of data. For more\nchallenging low-level physical manipulation, a dataset suitable for offline RL is still missing. We\ntherefore provide the first real-robot dataset for dexterous manipulation which is sufficiently large\nfor offline RL (one order of magnitude more data on real robots than prior work (Mandlekar et al.,\n2021)) and for which learned policies can easily be evaluated remotely on a real-robot platform.\nAffordable open-source platforms:\nOur hardware platform is open source. Other affordable\nrobotic open-source platforms are, for instance, a manipulator (Yang et al., 2019), a simple robotic\nhand and quadruped (Ahn et al., 2020). Since it is hard to set up and maintain such platforms, we\nprovide access to our real platform upon request, and hope that this will bring the field forward.\nRemote benchmarks: For mobile robotics, Pickem et al. (2017) propose the Robotarium, a remotely\naccessible swarm robotics research platform, and Kumar et al. (2019a) offer OffWorld gym consisting\nof two navigation tasks with a wheeled robot. Similarly, Duckietown (Paull et al., 2017) hosts the AI\nDriving Olympics (AI-DO-team, 2022).\n6\nCONCLUSION\nWe present benchmark datasets for robotic manipulation that are intended to help improving the\nstate-of-the-art in offline reinforcement learning. To record datasets, we trained capable policies using\nonline learning in simulation with domain randomization. Our analysis and evaluation on two tasks,\nPush and Lift, show that offline RL algorithms still leave room for improvement on data from real\nrobotic platforms. We identified two factors that could translate into increased performance on our\ndatasets: trajectory stitching and robustness to non-expert trajectories. Further, our analysis indicates\nthat noise and delay alone cannot explain the larger gap between dataset and offline RL performance\non real systems, underpinning the importance of real-robot benchmarks.\nWe invite the offline RL community to train their algorithms with the new datasets and test the\nempirical performance of the latest offline RL algorithms, e.g. (Kostrikov et al., 2021a; Xu et al.,\n2021; Kumar et al., 2021; Cheng et al., 2022), on real-robot hardware.\n9\nPublished as a conference paper at ICLR 2023\nAUTHOR CONTRIBUTIONS\nN.G., S.Bl., P.K., M.W., S.Ba., B.S., and G.M. conceived the idea, methods and experiments. G.M.\ninitiated the project, M.W., S.Ba. and B.S. conceived the robotic platform. F.W. implemented the\nlow-level robot control and parts of the submission system. N.G. trained the expert policies and\ncreated the datasets. N.G., S.Bl., and P.K. conducted the offline RL experiments, collected the results\nand analyzed them under the supervision of G.M. N.G. ran all experiments on the real systems. N.G.\nand F.W. wrote the software for downloading and accessing the datasets. N.G., S.Bl., P.K., F.W. and\nG.M. drafted the manuscript, and all authors revised it.\nACKNOWLEDGMENTS\nWe are grateful for the help of Thomas Steinbrenner in repairing and maintaining the robot cluster.\nMoreover, feedback by Arthur Allshire on training expert policies in simulation and by Huanbo Sun\non domain randomization proved valuable. We acknowledge the support from the German Federal\nMinistry of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B).\nGeorg Martius is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 –\nProject number 390727645. Pavel Kolev was supported by the Cyber Valley Research Fund and the\nVolkswagen Stiftung (No 98 571). We thank the anonymous reviewers for comments which helped\nimprove the presentation of the paper.\nREPRODUCIBILITY STATEMENT\nWe publish the datasets we propose as benchmarks (sections 3.3 and B) and provide access to the\ncluster of real TriFinger platforms (section 2) we used for data collection. Submissions to the cluster\ndo not require any robotics experience and can be made in the form of a Python implementation of\na RL policy (section B.5). A simulated version of the TriFinger platform (Joshi et al., 2020) and a\nlow-cost hardware variant are furthermore publicly available as open source (Wüthrich et al., 2021).\nFor offline RL training we moreover use open-source software (Seno & Imai, 2021). Finally, we\ndescribe our hyperparameter optimization in detail and provide the resulting hyperparameters in\nsection C.3.\nREFERENCES\nMichael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine, and\nVikash Kumar. Robel: Robotics benchmarks for learning with low-cost robots. In Leslie Pack\nKaelbling, Danica Kragic, and Komei Sugiura (eds.), Proceedings of the Conference on Robot\nLearning, volume 100 of Proceedings of Machine Learning Research, pp. 1300–1313. PMLR, 30\nOct–01 Nov 2020.\nAI-DO-team. The ai driving olympics (ai-do). https://www.duckietown.org/research/\nAI-Driving-olympics, 2022.\nArthur Allshire, Mayank MittaI, Varun Lodaya, Viktor Makoviychuk, Denys Makoviichuk, Felix\nWidmaier, Manuel Wüthrich, Stefan Bauer, Ankur Handa, and Animesh Garg. Transferring\ndexterous manipulation from GPU simulation to a remote real-world trifinger. In 2022 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pp. 11802–11809, 2022.\nFrancesco Amigoni, Emanuele Bastianelli, Jakob Berghofer, Andrea Bonarini, Giulio Fontana, Nico\nHochgeschwender, Luca Iocchi, Gerhard Kraetzschmar, Pedro Lima, Matteo Matteucci, Pedro\nMiraldo, Daniele Nardi, and Viola Schiaffonati. Competitions for benchmarking: Task and\nfunctionality scoring complete performance assessment. IEEE Robotics & Automation Magazine,\n22(3):53–61, 2015.\nOpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,\nJakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning\ndexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3–20,\n2020.\n10\nPublished as a conference paper at ICLR 2023\nMichael Bain and Claude Sammut. A framework for behavioural cloning. In Koichi Furukawa,\nDonald Michie, and Stephen H. Muggleton (eds.), Machine Intelligence 15, Intelligent Agents [St.\nCatherine’s College, Oxford, UK, July 1995], pp. 103–129. Oxford University Press, 1995.\nStefan Bauer, Manuel Wüthrich, Felix Widmaier, Annika Buchholz, Sebastian Stark, Anirudh\nGoyal, Thomas Steinbrenner, Joel Akpo, Shruti Joshi, Vincent Berenz, Vaibhav Agrawal, Niklas\nFunk, Julen Urain De Jesus, Jan Peters, Joe Watson, Claire Chen, Krishnan Srinivasan, Junwu\nZhang, Jeffrey Zhang, Matthew Walter, Rishabh Madan, Takuma Yoneda, Denis Yarats, Arthur\nAllshire, Ethan Gordon, Tapomayukh Bhattacharjee, Siddhartha Srinivasa, Animesh Garg, Takahiro\nMaeda, Harshit Sikchi, Jilong Wang, Qingfeng Yao, Shuyu Yang, Robert McCarthy, Francisco\nSanchez, Qiang Wang, David Bulens, Kevin McGuinness, Noel O’Connor, Redmond Stephen, and\nBernhard Schölkopf. Real robot challenge: A robotics competition in the cloud. In Douwe Kiela,\nMarco Ciccone, and Barbara Caputo (eds.), Proceedings of the NeurIPS 2021 Competitions and\nDemonstrations Track, volume 176 of Proceedings of Machine Learning Research, pp. 190–204.\nPMLR, 06–14 Dec 2022.\nSven Behnke. Robot competitions-ideal benchmarks for robotics research. In Proc. of IROS-2006\nWorkshop on Benchmarks in Robotics Research. Institute of Electrical and Electronics Engineers\n(IEEE), 2006.\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale\ndeep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\nFabio Bonsignorio and Angel P. del Pobil. Toward replicable and measurable robotics research [from\nthe guest editors]. IEEE Robotics & Automation Magazine, 22(3):32–35, 2015.\nDavid Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline RL without\noff-policy evaluation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 4933–4946.\nCurran Associates, Inc., 2021.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nBerk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar.\nBenchmarking in Manipulation Research: The YCB Object and Model Set and Benchmarking\nProtocols. arXiv:1502.03143, February 2015.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,\nAravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence\nmodeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan\n(eds.), Advances in Neural Information Processing Systems, volume 34, pp. 15084–15097. Curran\nAssociates, Inc., 2021.\nXinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. BAIL: best-action\nimitation learning for batch deep reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell,\nM.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,\npp. 18353–18363. Curran Associates, Inc., 2020.\nChing-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic\nfor offline reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba\nSzepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 3852–3878.\nPMLR, 17–23 Jul 2022.\nErwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics\nand machine learning. http://pybullet.org, 2016.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n11\nPublished as a conference paper at ICLR 2023\nGabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,\nand Todd Hester. An empirical investigation of the challenges of real-world reinforcement learning.\nCoRR, abs/2003.11881, 2020.\nLuciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds\nand Machines, 30(4):681–694, 2020.\nC. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem.\nBrax - a differentiable physics engine for large scale rigid body simulation, 2021. URL http:\n//github.com/google/brax.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep\ndata-driven reinforcement learning. CoRR, abs/2004.07219, 2020.\nScott Fujimoto and Shixiang (Shane) Gu. A minimalist approach to offline reinforcement learning.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances\nin Neural Information Processing Systems, volume 34, pp. 20132–20145. Curran Associates, Inc.,\n2021.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th\nInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning\nResearch, pp. 2052–2062. PMLR, 09–15 Jun 2019.\nFelix Grimminger, Avadesh Meduri, Majid Khadiv, Julian Viereck, Manuel Wüthrich, Maximilien\nNaveau, Vincent Berenz, Steve Heim, Felix Widmaier, Jonathan Fiene, Alexander Badri-Spröwitz,\nand Ludovic Righetti. An Open Torque-Controlled Modular Robot Architecture for Legged\nLocomotion Research. In International Conference on Robotics and Automation (ICRA), 2020.\nCaglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gómez, Konrad Zolna,\nRishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged: A\nsuite of benchmarks for offline reinforcement learning. Advances in Neural Information Processing\nSystems, 33:7248–7259, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 770–778, 2016.\nJemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen\nKoltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science\nRobotics, 4(26):eaau5872, 2019.\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-\nbased policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran\nAssociates, Inc., 2019.\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\nmodeling problem. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1273–1286.\nCurran Associates, Inc., 2021.\nShruti Joshi, Felix Widmaier, Vaibhav Agrawal, and Manuel Wüthrich. https://github.com/\nopen-dynamic-robot-initiative/trifinger_simulation, 2020.\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Model-\nbased offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and\nH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21810–21823.\nCurran Associates, Inc., 2020.\nIlya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning\nwith fisher divergence critic regularization. In Marina Meila and Tong Zhang (eds.), Proceedings\nof the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learning Research, pp. 5774–5783. PMLR, 2021a.\nURL http://proceedings.mlr.press/v139/kostrikov21a.html.\n12\nPublished as a conference paper at ICLR 2023\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\nq-learning. CoRR, abs/2110.06169, 2021b.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImageNet Classification with Deep\nConvolutional Neural Networks. In F Pereira, C J C Burges, L Bottou, and K Q Weinberger (eds.),\nAdvances in Neural Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc.,\n2012.\nAshish Kumar, Toby Buckley, John B Lanier, Qiaozhi Wang, Alicia Kavelaars, and Ilya Kuzovkin.\nOffworld gym: open-access physical robotics environment for real-world reinforcement learning\nbenchmark and research. arXiv preprint arXiv:1910.08639, 2019a.\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\nq-learning via bootstrapping error reduction. In Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11761–11771, 2019b.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\nreinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020.\nAviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for\noffline model-free robotic reinforcement learning. In Aleksandra Faust, David Hsu, and Gerhard\nNeumann (eds.), Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164\nof Proceedings of Machine Learning Research, pp. 417–428. PMLR, 2021.\nSascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement\nlearning, pp. 45–73. Springer, 2012.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\nreview, and perspectives on open problems. CoRR, abs/2005.01643, 2020.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In\nInternational Conference on Learning Representations (ICLR), 2016.\nDenys Makoviichuk and Viktor Makoviychuk. rl-games: A high-performance framework for rein-\nforcement learning. https://github.com/Denys88/rl_games, May 2022.\nViktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin,\nDavid Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance\ngpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.\nAjay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust\npolicy learning through active construction of physically-plausible perturbations. In IEEE Int’l\nConf. on Intelligent Robots and Systems (IROS), volume 16, 2017.\nAjay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-\nFei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning from offline\nhuman demonstrations for robot manipulation. In Aleksandra Faust, David Hsu, and Gerhard\nNeumann (eds.), Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164\nof Proceedings of Machine Learning Research, pp. 1678–1690. PMLR, 2021.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles\nBeattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane\nLegg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,\n518(7540):529–533, February 2015.\n13\nPublished as a conference paper at ICLR 2023\nAdithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh\nGupta, and Abhinav Gupta. PyRobot: An Open-source Robotics Framework for Research and\nBenchmarking. arXiv: 1906.08236, June 2019.\nSiddharth Mysore, Bassel Mabsout, Renato Mancuso, and Kate Saenko. Regularizing action policies\nfor smooth control with reinforcement learning. In 2021 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 1810–1816. IEEE, 2021.\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of\ndiscounted stationary distribution corrections. In Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 2315–2325, 2019a.\nOfir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. AlgaeDICE:\nPolicy gradient from arbitrary experience. CoRR, abs/1912.02074, 2019b.\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement\nlearning with offline datasets. CoRR, abs/2006.09359, 2020.\nOpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,\nJakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider,\nSzymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning\nDexterous In-Hand Manipulation. arXiv:1808.00177, August 2018.\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur\nPetron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas\nTezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei\nZhang. Solving Rubik’s Cube with a Robot Hand. arXiv:1910.07113, October 2019.\nFabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement\nlearning. In International Conference on Machine Learning, pp. 4045–4054. PMLR, 2018.\nLiam Paull, Jacopo Tani, Heejin Ahn, Javier Alonso-Mora, Luca Carlone, Michal Cap, Yu Fan\nChen, Changhyun Choi, Jeff Dusek, Yajun Fang, et al. Duckietown: an open, inexpensive and\nflexible platform for autonomy education and research. In 2017 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 1497–1504. IEEE, 2017.\nXue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of\nrobotic control with dynamics randomization. In 2018 IEEE international conference on robotics\nand automation (ICRA), pp. 3803–3810. IEEE, 2018.\nDaniel Pickem, Paul Glotfelter, Li Wang, Mark Mote, Aaron Ames, Eric Feron, and Magnus\nEgerstedt. The robotarium: A remotely accessible swarm robotics research testbed. In 2017 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 1699–1706. IEEE, 2017.\nDean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural\ncomputation, 3(1):88–97, 1991.\nRafael Figueiredo Prudencio, Marcos R. O. A. Maximo, and Esther Luna Colombini. A survey on\noffline reinforcement learning: Taxonomy, review, and open problems. CoRR, abs/2203.01387,\n2022.\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\nreal-time object detection. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 779–788, 2016.\nStéphane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In Geoffrey J. Gordon, David B. Dunson,\nand Miroslav Dudík (eds.), Proceedings of the Fourteenth International Conference on Artificial\nIntelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, volume 15 of\nJMLR Proceedings, pp. 627–635. JMLR.org, 2011.\n14\nPublished as a conference paper at ICLR 2023\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nTakuma Seno and Michita Imai. d3rlpy: An offline deep reinforcement learning library. CoRR,\nabs/2111.03788, 2021.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\nhuman knowledge. Nature, 550(7676):354–359, 2017.\nRichard S Sutton, Andrew G Barto, and Co-Director Autonomous Learning Laboratory Andrew G\nBarto. Reinforcement Learning: An Introduction. MIT Press, 1998.\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain\nrandomization for transferring deep neural networks from simulation to the real world. In 2017\nIEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23–30. IEEE,\n2017. doi: 10.1109/IROS.2017.8202133.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.\nIEEE, 2012.\nFaraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Jérôme Lang\n(ed.), Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,\nIJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 4950–4957. ijcai.org, 2018.\nQiang Wang, Francisco Roldan Sanchez, Robert McCarthy, David Cordova Bulens, Kevin McGuin-\nness, Noel O’Connor, Manuel Wüthrich, Felix Widmaier, Stefan Bauer, and Stephen J. Redmond.\nDexterous robotic manipulation using deep reinforcement learning and knowledge transfer for\ncomplex sparse reward-based tasks. Expert Systems, n/a(n/a):e13205, 2022.\nZiyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost Tobias Springenberg, Scott E. Reed,\nBobak Shahriari, Noah Y. Siegel, Çaglar Gülçehre, Nicolas Heess, and Nando de Freitas. Critic\nregularized regression. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020.\nManuel Wüthrich, Felix Widmaier, Felix Grimminger, Shruti Joshi, Vaibhav Agrawal, Bilal Ham-\nmoud, Majid Khadiv, Miroslav Bogdanovic, Vincent Berenz, Julian Viereck, Maximilien Naveau,\nLudovic Righetti, Bernhard Schölkopf, and Stefan Bauer. Trifinger: An open-source robot for\nlearning dexterity. In Proceedings of the 2020 Conference on Robot Learning (CoRL), volume\n155, pp. 1871–1882. PMLR, 2021.\nHaoran Xu, Xianyuan Zhan, Jianxiong Li, and Honglei Yin. Offline reinforcement learning with soft\nbehavior regularization. CoRR, abs/2110.07395, 2021.\nBrian Yang, Jesse Zhang, Vitchyr Pong, Sergey Levine, and Dinesh Jayaraman. Replab: A repro-\nducible low-cost arm benchmark platform for robotic learning. arXiv preprint arXiv:1905.07447,\n2019.\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.\nCOMBO: conservative offline model-based policy optimization. In Marc’Aurelio Ranzato, Alina\nBeygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in\nNeural Information Processing Systems 34: Annual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 28954–28967, 2021.\nChi Zhang, Sanmukh R. Kuppannagari, and Viktor K. Prasanna. BRAC+: improved behavior\nregularized actor critic for offline reinforcement learning. In Vineeth N. Balasubramanian and\nIvor W. Tsang (eds.), Asian Conference on Machine Learning, ACML 2021, 17-19 November 2021,\nVirtual Event, volume 157 of Proceedings of Machine Learning Research, pp. 204–219. PMLR,\n2021.\n15\nPublished as a conference paper at ICLR 2023\nRuiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDICE: Generalized offline estimation of\nstationary values. In 8th International Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n16\nPublished as a conference paper at ICLR 2023\nA\nADDITIONAL RESULTS\nThis section contains additional experimental results. In particular values for the average returns\nachieved by the algorithms are reported. These were omitted from the main text as the success rates\nare easier to interpret.\nA.1\nRETURNS FOR THE PUSH TASK\nWe report the returns achieved on the Push task in Table S1. Note that for the simulated datasets\nthe best performing algorithm in terms of return is not necessarily the same as the best performing\none in terms of success rates (see Table 1). For instance, AWAC has the highest average return on\nSim-Expert while CRR achieves a higher success rate. This has two reasons. First, the success is\nmeasured only at the final step of the episode, while the return is computed as the cumulative reward\nover time. Second, the return is dense as it is computed from the distance between cube and target,\nwhile the success rate is a sparse signal.\nTable S1: Push: Returns on the TriFinger-Push Datasets. ‘data’ denotes the mean over the dataset.\nAverage and standard deviation over five training seeds.\nPush-Datasets\ndata\nBC\nCRR\nAWAC\nCQL\nIQL\nSim-Expert\n674\n585 ± 19\n636 ± 20\n657 ± 14\n184 ± 23\n631 ± 18\nSim-Half-Expert\n674\n535 ± 18\n576 ± 17\n586 ± 14\n226 ± 12\n565 ± 22\nSim-Weak&Expert\n512\n460 ± 50\n613 ± 17\n603 ± 23\n311 ± 29\n543 ± 75\nSim-Mixed\n583\n460 ± 26\n205 ± 100\n636 ± 20∗138 ± 11\n588 ± 25\nReal-Expert\n660\n563 ± 34\n638 ± 16\n624 ± 17\n514 ± 37\n592 ± 29\nReal-Half-Expert\n660\n546 ± 29\n627 ± 21\n587 ± 46\n471 ± 26\n573 ± 23\nReal-Weak&Expert\n429\n387 ± 45\n622 ± 41∗\n568 ± 29\n346 ± 68\n555 ± 16\nReal-Mixed\n419\n335 ± 23\n373 ± 41\n569 ± 24\n206 ± 18\n600 ± 30\nA.2\nRETURNS FOR THE LIFT TASK\nIn this section, we report the returns achieved on the Lift task (Table S2). Interestingly, although IQL\nachieves a significantly higher return on the Real-Weak&Expert dataset than CRR, its final success\nrate is lower. An analysis of the rollout statistics reveals that IQL often moves the cube close to the\ngoal pose but not close enough to satisfy the success criteria (defined in section 3). CRR, on the other\nhand, on average deviates further from the goal pose than IQL but has a bigger fraction of rollouts\nin which it satisfies the success criteria. In summary IQL does better on average on this dataset but\nlacks precision in matching the goal pose.\nA.3\nEVALUATION ON THE HOLD-OUT ROBOT\nTo quantify how well the policies obtained with offline RL generalize to unseen hardware, we evaluate\nthem on a holdout robot which was not used for data collection. Table S3 shows the results for\nTriFinger-Push-Real-Expert and TriFinger-Lift-Real-Expert. The performance of the algorithms on\nthe hold-out robot is within the performance margin of the other robots, suggesting that there is no\nsignificant difference between the different robots.\nA.4\nIMPACT OF NOISE ON PERFORMANCE\nAs mentioned in section 4.1, we studied the impact of noise on the performance of the expert and\nthe considered offline RL algorithms by recording a sequence of datasets with increasing noise scale\nin simulation. A relative noise scale of 1 corresponds to the variance measured on the actions and\nobservations of the real system. Fig. S1 shows the success rate and return as a function of the noise\nscale up to a value of 2. As the performance of the expert and the offline RL policies degrades\nonly slowly when increasing noise, we conclude that the stochasticity of the real system cannot be\nthe only reason for the performance gap between the simulated and the real system. As delays in\n17\nPublished as a conference paper at ICLR 2023\nTable S2: Lift: Returns on the TriFinger-Lift Datasets. ‘data’ denotes the mean over the dataset.\nAverage and standard deviation over five training seeds.\nLift-Datasets\ndata\nBC\nCRR\nAWAC\nCQL\nIQL\nSim-Expert\n1334\n1129 ± 56\n1246 ± 10\n1280 ± 20∗\n163 ± 14\n1133 ± 41\nSim-Half-Expert\n1337\n1112 ± 39\n1231 ± 9\n1211 ± 19\n154 ± 13\n744 ± 40\nSim-Weak&Expert\n1133\n791 ± 43\n727 ± 447\n1103 ± 62\n164 ± 15\n943 ± 56\nSim-Mixed\n1173\n409 ± 9\n604 ± 79\n931 ± 75∗∗∗161 ± 8\n572 ± 24\nSim-Expert†\n1334\n1274 ± 17\n1245 ± 26\n1319 ± 11∗\n399 ± 56\n1286 ± 20\nSim-Half-Expert†\n1337\n1191 ± 33\n1153 ± 50\n1303 ± 14∗\n439 ± 26\n1229 ± 45\nSim-Weak&Expert†\n1133\n1040 ± 22\n1087 ± 49\n1120 ± 23\n467 ± 34\n1172 ± 34∗\nSim-Mixed†\n1173\n411 ± 35\n593 ± 93\n862 ± 53∗∗151 ± 9\n564 ± 31\nReal-Smooth-Expert\n1206\n915 ± 36\n1059 ± 54\n1031 ± 42\n143 ± 33\n1002 ± 19\nReal-Expert\n1064\n711 ± 92\n1014 ± 62∗\n820 ± 50\n283 ± 38\n901 ± 43\nReal-Half-Expert\n1064\n553 ± 77\n837 ± 58\n613 ± 107\n200 ± 7\n759 ± 101\nReal-Weak&Expert\n851\n346 ± 21\n633 ± 59\n397 ± 71\n298 ± 16\n827 ± 105∗\nReal-Mixed\n862\n272 ± 56\n631 ± 62\n346 ± 64\n207 ± 5\n608 ± 30\nReal-Expert†\n1064\n676 ± 47\n889 ± 39\n747 ± 44\n289 ± 33\n899 ± 40\nReal-Half-Expert†\n1064\n702 ± 83\n813 ± 57\n797 ± 54\n312 ± 65\n855 ± 51\nReal-Weak&Expert†\n851\n437 ± 47\n707 ± 20∗\n481 ± 54\n269 ± 33\n574 ± 67\nReal-Mixed†\n862\n288 ± 73\n634 ± 36\n361 ± 81\n192 ± 28\n596 ± 33\nTable S3: Evaluation on hold-out robot. Success rate on the Real-Expert datasets.\nDataset\nBC\nCRR\nAWAC\nCQL\nIQL\nPush-Real-Expert\n0.80 ± 0.04\n0.91 ± 0.08\n0.84 ± 0.06\n0.61 ± 0.05\n0.83 ± 0.09\nLift-Real-Expert\n0.29 ± 0.04\n0.64 ± 0.05∗∗0.31 ± 0.08\n0.00 ± 0.00\n0.24 ± 0.07\nthe observation and action execution are already implemented in the simulated environment, we\nhypothesize that other factors like more complex contact dynamics and elastic deformations of the\nfingertips and robot limbs are likely causing the larger performance gap between data and learned\npolicies on the real robots.\nAWAC and CRR perform consistently over a wide range of noise scales with a slight decrease\nin performance for high relative noise scales (probably due to a large variance of the estimated\nobject pose). BC and IQL seem to struggle with the narrow data distribution generated by a\ndeterministic environment but improve with increasing stochasticity. While the performance of BC\ndrops significantly again for large noise scales, IQL becomes competitive in this regime.\n0.0\n0.5\n1.0\n1.5\n2.0\nrelative noise scale\n0.00\n0.25\n0.50\n0.75\nsuccess rate\n0.0\n0.5\n1.0\n1.5\n2.0\nrelative noise scale\n500\n1000\nreturn\nExpert\nBC†\nCRR†\nAWAC†\nCQL†\nIQL†\nFigure S1: Success rate and return for simulated lifting for varying relative environment noise scales\n(1.0 corresponds to the noise level of the real system) when the algorithms were trained on data\nrecorded with that noise scale using the expert policy. The dashed lines indicate the performance on\nthe real system.\n18\nPublished as a conference paper at ICLR 2023\nA.5\nBAR PLOTS\nFig. S2 and Fig. S3 show bar plots that summarize the performance of the algorithms on the two\ntasks in the simulated and real environment. Before averaging results from different datasets, we\nnormalized the algorithm performance by the dataset performance, i.e., reaching the average success\nrate or return of the dataset corresponds to a value of 1.\nWhile AWAC (and IQL on the real data) can exceed the behavior policy on average on the Push task,\nall algorithms fall short of matching the dataset performance on the challenging Lift datasets. While\nsuccess rates on the Lift-Real datasets are particularly low, the returns indicate that CRR and IQL\nsignificantly outperform BC.\nSince the hyperparameter optimization was done on the Lift-Sim-Weak&Expert dataset, it has the\nbiggest impact on the performance on the Lift-Sim datasets. IQL in particular improves considerably\nthere. The increase in performance on the Lift-Real datasets is considerably smaller, however. This\nsuggests that optimizing the hyperparameters offline RL algorithms in simulation may have limited\nbenefits on real environments.\nPush-Sim\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized success\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized return\nPush-Real\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized success\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized return\nFigure S2: Normalized performance for the Push task: Success rates at the end of episodes (or\nreturns) were normalized to the dataset success rate (or mean return) and then averaged over all\ndatasets corresponding to a task (treating simulated and real data separately). Default hyperparameters\nwere used for the Push task.\n19\nPublished as a conference paper at ICLR 2023\nLift-Sim\ndefault hyperparameters\noptimized hyperparameters\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized success\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized success\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized return\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized return\nLift-Real\ndefault hyperparameters\noptimized hyperparameters\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized success\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized success\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized return\nBC\nCRR AWAC CQL\nIQL\n0.00\n0.25\n0.50\n0.75\n1.00\nNormalized return\nFigure S3: Normalized performance for the Lift task: Success rates at the end of episodes (or\nreturns) were normalized to the dataset success rate (or mean return) and then averaged over all\ndatasets corresponding to a task (treating simulated and real data separately).\n20\nPublished as a conference paper at ICLR 2023\nA.6\nLEARNING CURVES\nIn this section, we provide learning curves for the offline RL algorithms on all datasets. Since the\nevaluation of all training checkpoints on the real robots is prohibitively expensive and time-consuming,\nwe evaluate the checkpoints learned on the real data in the simulated environment. This gives an\nover-optimistic estimate of the learning performance on the real robots.\nFor the challenging Lift-Real-Weak&Expert task we performed a grid search for each algorithm\n(BC, CRR, AWAC, CQL, IQL) and selected the best hyperparameters. We present the grid search in\nTable S7, the corresponding optimal hyperparameters in Table S8, and the default hyperparameters\nin Table S9. We note that due to the poor performance of CQL, we expanded our grid search\nspecifically for this algorithm (on the Push-Sim-Expert data) and selected the corresponding optimal\nhyperparameters as its default parameters (see Figure S16). Our newly performed gridsearch, as\nmentioned above, was unfortunately not leading to improvements for CQL. Similar difficulties are\nreported in Kumar et al. (2021) and are tackled via case distinction and appropriate regularization\ntechniques. It would be interesting to test the preceding two algorithmic techniques on our datasets,\nonce their implementation is integrated in the D3RLPY library (Seno & Imai, 2021).\nWe proceed by presenting the learning curves for the offline RL algorithms on the datasets: for the\nLift task in section A.6.1 and for the Push task in section A.6.2. Shaded areas indicate the interval\nbetween the 0.25 and 0.75 quantiles.\n21\nPublished as a conference paper at ICLR 2023\nA.6.1\nLIFT DATASETS\nOptimized Hyperparameters†\nReal-Expert\nReal-Half-Expert\nReal-Weak&Expert\nReal-Mixed\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\nDefault Hyperparameters\nReal-Expert\nReal-Half-Expert\nReal-Weak&Expert\nReal-Mixed\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\nBC\nCRR\nAWAC\nCQL\nIQL\nFigure S4: Training curves for the Lift-Real datasets: Offline RL algorithms are trained on the\nreal datasets and are evaluated in the simulated environment. Success rates and returns for the\noptimized hyperparameters (in Table S8) and the default hyperparameters (in Table S9). The selected\nhyperparameters, by the grid-search procedure (in Table S7), optimize the algorithm’s final average\nreturn on Sim-Weak&Expert. The black dashed line shows the dataset performance of Real-Expert\nand the red dashed line the performance of the Real-Weak&Expert (first 3 columns) and Real-Mixed\n(last column).\n22\nPublished as a conference paper at ICLR 2023\nOptimized Hyperparameters†\nSim-Expert\nSim-Half-Expert\nSim-Weak&Expert\nSim-Mixed\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\nDefault Hyperparameters\nSim-Expert\nSim-Half-Expert\nSim-Weak&Expert\nSim-Mixed\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\nBC\nCRR\nAWAC\nCQL\nIQL\nFigure S5: Training curves for the Lift-Sim datasets: Offline RL algorithms are trained on a\nsimulated dataset and are evaluated in the simulated environment. Success rates and returns for the\noptimized hyperparameters (in Table S8) and the default hyperparameters (in Table S9). The selected\nhyperparameters, by the grid-search procedure (in Table S7), optimize the algorithm’s final average\nreturn on Sim-Weak&Expert. The black dashed line shows the dataset performance of Sim-Expert\nand the red dashed line the performance of the Sim-Weak&Expert (first 3 columns) and Sim-Mixed\n(last column).\n23\nPublished as a conference paper at ICLR 2023\nA.6.2\nPUSH DATASETS\nHere, we use the default hyperparameters in Table S9, where for the CQL algorithm we selected\nhyperparameters optimized by a grid search (see Figure S16 for histograms). The success rates and\nreturns are evaluated in the simulated environment.\nReal-Expert\nReal-Half-Expert\nReal-Weak&Expert\nReal-Mixed\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\nBC\nCRR\nAWAC\nCQL\nIQL\nFigure S6: Training curves for the Push-Real datasets: Offline RL algorithms are trained on the\nreal datasets and are evaluated in the simulated environment. The black dashed line shows the dataset\nperformance of Real-Expert and the red dashed line the performance of the Real-Weak&Expert (first\n3 columns) and Real-Mixed (last column).\nSim-Expert\nSim-Half-Expert\nSim-Weak&Expert\nSim-Mixed\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\nreturns\nBC\nCRR\nAWAC\nCQL\nIQL\nFigure S7: Training curves for the Push-Sim datasets: Offline RL algorithms are trained on a\nsimulated dataset and are evaluated in the simulated environment. The black dashed line shows the\ndataset performance of Sim-Expert and the red the performance of the Sim-Weak&Expert (first 3\ncolumns) and Sim-Mixed (last column).\n24\nPublished as a conference paper at ICLR 2023\nB\nDATASETS\nIn this section we provide additional details on the datasets we collected. Section B.1 summarizes\nthe data collection process, section B.2 contains a discussion of the different notions of success we\nreport, section B.3 is dedicated to the reward and the terminals, and section B.4 provides a detailed\nanalysis of the statistics of the datasets.\nB.1\nDATA COLLECTION\nData is collected by running jobs on a cluster of TriFinger platforms without human intervention.\nBefore the recording of each dataset the platforms were cleaned from particle dust to ensure consistent\nobject tracking performance. Each job consists of the following steps:\n1. Move fingers to initial state and sample new goal.\n2. Run one episode/rollout with the selected policy and store transitions.\n3. Move the cube away from the barrier with a pre-recorded trajectory of the fingers.\n4. Repeat from 1. unless the desired number of episodes per job is reached.\n5. Do a self-check including cameras, pose estimation and joints.\n6. Approximately reset the cube to the center of the arena.\nThe number of episodes collected per job is eight for the Push task (15 s per episode) and six for\nthe Lift task (30 s per episode). If a self-check fails, the corresponding robot is deactivated and the\nmaintainers are alerted via mail. Finally, the data from all episodes is combined in a single HDF5 file\nfollowing the conventions of D4RL (Fu et al., 2020).\nThe expert policies are obtained after the training in Isaac Gym (see section 3.2 and section C.1). The\nweak policies are training checkpoints at 210 · 106 training steps for pushing and 288 · 106 training\nsteps for lifting. Gaussian noise is added to the actions with an amplitude of 0.2 Nm and an update\nfrequency of 8 for pushing and 0.04 Nm with an update frequency of 4 for lifting.\nThe Mixed datasets are collected with policies from a range of training checkpoints where the total\nnumber of jobs was distributed as uniformly as possible over all training checkpoints. For pushing\n22 checkpoints up to 668 · 106 training steps were used while for lifting 59 training checkpoints\nup to 1721 · 106 training steps were considered. Fig. S8 shows the return and success rate of these\ncheckpoints on the real TriFinger cluster.\nB.2\nTHREE NOTIONS OF SUCCESS\nWe consider dexterous manipulation tasks which require the agent to match a goal position (for the\nPush task) or a goal pose (for the Lift task) with a tracked cube. The tolerance for goal achievement is\n2 cm for the position and 22 degrees for the orientation. We define momentary success as achieving\nthe goal at a single point in time, i.e., in an individual time step. This notion of success is rather weak,\nhowever, as achieving the goal pose for a short amount of time is significantly easier than maintaining\nit. For the pushing task in particular, it is much more likely to move through the goal by accident than\nto stabilize the cube after having reached it. When lifting the cube, it is quite challenging to maintain\na stable grasp due to the variance of the object pose estimate which introduces a considerable amount\nof noise to the observations. We therefore define success as achieving the goal at the end of the\nepisode which is only likely to happen when maintaining the goal pose for an extended period of\ntime.\nFrom the perspective of offline RL, however, it is highly relevant whether parts of the trajectories\nin the datasets lead to success, even if it is short-lived. As offline RL algorithms can, in principle,\ncombine information from several trajectories, even trajectories that do not end with goal achievement\nmay contain enough information to learn a successful policy. We therefore also report transient\nsuccess which indicates whether the goal has been achieved at any time during the episode and mean\nmomentary success which corresponds to the fraction of time steps during which the goal has been\nachieved.\n25\nPublished as a conference paper at ICLR 2023\nPush-Real-Mixed\nLift-Real-Mixed\n0\n2\n4\ntime steps\n1e8\n0\n200\n400\n600\nreturn\n0.0\n0.5\n1.0\n1.5\ntime steps\n1e9\n500\n1000\nreturn\n0\n2\n4\ntime steps\n1e8\n0.0\n0.2\n0.4\n0.6\n0.8\nsuccess rate\n0.0\n0.5\n1.0\n1.5\ntime steps\n1e9\n0.0\n0.2\n0.4\n0.6\n0.8\nsuccess rate\nFigure S8: Return and success rates of the checkpoints used for collecting the Real-Mixed datasets.\nNote that the checkpoints were obtained by training in Isaac Gym (see section 3.2 and section C.1)\nand were evaluated on the real system. The shaded areas indicate the interval between the 0.25 and\n0.75 quantiles of the return whereas the error bars represent the standard error of the mean of the\nsuccess rate.\nB.3\nREWARD AND TERMINALS\nAs already mentioned in section 3 of the main text, we use a logistic kernel Hwangbo et al. (2019);\nAllshire et al. (2022)\nk(x) =\nb + 2\nexp(a∥x∥) + b + exp(−a∥x∥)\n(S1)\nto define the reward where ∥·∥denotes the Euclidean norm. For the Push task, we apply it to the\ndifference between the achieved and the desired cube position. Since we also want to take the\norientation of the cube into account for the Lift task, we apply k separately to the differences between\nthe desired and achieved corner points (or keypoints) of the cube and average over the results Allshire\net al. (2022). We use a = 30 and b = 2. See Fig. S9 for a visualization of the reward function.\nBy default, the terminals in the dataset (a Boolean indicating whether a terminal state has been\nreached) are never set. We chose this default setting to avoid problems due to state aliasing: Episodes\n0.1\n0.0\n0.1\nx [m]\n0.00\n0.25\n0.50\n0.75\n1.00\nreward\nFigure S9: Reward as a function of the Euclidean distance between the desired and achieved position\n(for the Push task) or keypoint (for the Lift task). The shaded grey area corresponds to a cube centered\nat the goal and the green area corresponds to the goal achievement threshold (2 cm).\n26\nPublished as a conference paper at ICLR 2023\nTable S4: Overview of the TriFinger offline RL datasets.\ntask\ndataset\noverall duration [h]\n#episodes\n#transitions [106]\nepisode length [s]\nPush-\nSim-Expert\n16\n3840\n2.8\n15\nSim-Half-Expert\n8\n1920\n1.4\n15\nSim-Weak&Expert\n16\n3840\n2.8\n15\nSim-Mixed\n16\n3840\n2.8\n15\nReal-Expert\n16\n3840\n2.8\n15\nReal-Half-Expert\n8\n1920\n1.4\n15\nReal-Weak&Expert\n16\n3840\n2.8\n15\nReal-Mixed\n16\n3840\n2.8\n15\nLift-\nSim-Expert\n20\n2400\n3.6\n30\nSim-Half-Expert\n10\n1200\n1.8\n30\nSim-Weak&Expert\n20\n2400\n3.6\n30\nSim-Mixed\n20\n2400\n3.6\n30\nReal-Smooth-Expert\n20\n2400\n3.6\n30\nReal-Expert\n20\n2400\n3.6\n30\nReal-Half-Expert\n10\n1200\n1.8\n30\nReal-Weak&Expert\n20\n2400\n3.6\n30\nReal-Mixed\n20\n2400\n3.6\n30\nlast for a finite number of time steps H, and the RL objective is to maximize the discounted return\n(or cumulative reward)\nJ =\nH−1\nX\nt=0\nγtrt ,\n(S2)\nwhere γ denotes the discounting factor and rt the reward in step t. A Markov state therefore has to\nkeep track of how much time remains until the episode ends. The observation, on the other hand,\ndoes not contain this information. This omission is a deliberate choice as we want to obtain a policy\nwhich is independent of the time step. This makes it impossible, however, to accurately estimate\nthe value based on the observation if γ is too large. Intuitively, without access to the time step t, the\nagent cannot know whether a cube far away from the goal corresponds to a large expected return to\ngo because it is the beginning of an episode or a small expected return to go because there is no time\nleft to move the cube and accumulate reward. See Pardo et al. (2018) for a more detailed discussion\nof issues arising from training with a finite horizon.\nA practical solution to this problem is to not set the terminals and choose a gamma which is appropriate\nfor the time scale of the task (for offline learning we chose γ = 0.99). This choice hides the episodic\nnature of the task from the agent which results in good performance while avoiding a dependence of\nthe policy on time. It may, however, sacrifice optimality in some corner cases like dropping the cube\nclose to the end of the episode when there is not enough time to flip the cube over before lifting it\nagain. Nevertheless, as we provide a flag to set the terminals at the episode ends and as it is straight\nforward to augment the observation with the intra-episode time step, the datasets are also suitable for\nexperiments with time-dependent policies.\nB.4\nDATASET ANALYSIS\nOverview:\nWe provide an overview of the various dataset types and their properties in Table S4.\nStatistics:\nTable S5 summarizes the statistics of the TriFinger datasets. While the success rate and\nthe mean return constitute a limit to what can be achieved with pure imitation learning, the high\nnumbers for the transient success rate (fraction of episodes in which the goal was achieved at least in\none time step but not necessarily at the end of the episode) indicate that offline RL can potentially\noutperform the behavior policy significantly on these datasets.\nImpact of variations between robots:\nFig. S10 compares the success rates of the expert policies\non the individual robots used for data collection. While the performance differences between the\nrobot instances are significant, the expert policies perform reasonably well on all of them, achieving\nat least 80% success rate on the Push task and 60% on the Lift task on all robots.\n27\nPublished as a conference paper at ICLR 2023\nTable S5: Statistics of the proposed TriFinger offline RL datasets: TriFinger-Push and TriFinger-\nLift. Definitions of the success rates and the reward can be found in section B.2 and section B.3,\nrespectively.\nTriFinger-Push\nsuccess rate\nmean momentary\nsuccess rate\ntransient\nsuccess rate\nmean return\nSim-Expert\n0.95\n0.87\n0.95\n674\nSim-Half-Expert\n0.94\n0.86\n0.94\n667\nSim-Weak&Expert\n0.53\n0.48\n0.86\n512\nSim-Mixed\n0.76\n0.68\n0.77\n583\nReal-Expert\n0.92\n0.78\n0.98\n660\nReal-Half-Expert\n0.92\n0.78\n0.98\n660\nReal-Weak&Expert\n0.51\n0.43\n0.72\n429\nReal-Mixed\n0.49\n0.40\n0.63\n419\nTriFinger-Lift\nsuccess rate\nmean momentary\nsuccess rate\ntransient\nsuccess rate\nmean return\nSim-Expert\n0.87\n0.77\n0.97\n1334\nSim-Half-Expert\n0.88\n0.78\n0.98\n1337\nSim-Weak&Expert\n0.50\n0.44\n0.93\n1133\nSim-Mixed\n0.68\n0.60\n0.84\n1173\nReal-Smooth-Expert\n0.64\n0.53\n0.82\n1206\nReal-Expert\n0.67\n0.52\n0.87\n1064\nReal-Half-Expert\n0.68\n0.52\n0.86\n1064\nReal-Weak&Expert\n0.40\n0.30\n0.66\n851\nReal-Mixed\n0.42\n0.32\n0.65\n862\n1\n3\n5\n6\n7\n8\nrobot ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n(a) TriFinger-Push-Real-Expert\n1\n3\n5\n6\n7\n8\nrobot ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n(b) TriFinger-Lift-Real-Expert\nFigure S10: Success rates on the individual robots. The transitions were recorded with policies\ntrained with PPO in simulation. While there are significant differences between the success rates,\nthe expert policies achieve at least 80% and 60% on every robot for TriFinger-Push-Real-Expert and\nTriFinger-Lift-Real-Expert, respectively.\nDistribution of returns:\nFig. S11 shows the distribution of the episode returns for the datasets\ncollected on the real system. It reveals that the expert policy on the Push task performed more\nconsistently on the real robot than its counterpart for the Lift task. Qualitatively, this can be attributed\nto either (i) not being able to flip the cube to the approximately correct orientation, (ii) failing to\nestablish a stable grasp on the cube, or (iii) dropping it after already having lifted it. Flipping the\ncube on the ground might fail due to incomplete modeling of interactions between the fingertips and\nthe cube in the rigid body physics simulator or because it is sensitive to the value of sliding friction,\nthe noise on the object pose estimate makes it difficult to maintain a stable grasp after having lifted\nthe object.\n28\nPublished as a conference paper at ICLR 2023\nPush-Real-Expert\nLift-Real-Expert\nLift-Real-Smooth-Expert\n0\n200\n400\n600\nepisode return\n0\n500\n1000\n1500\nepisode return\n0\n500\n1000\n1500\nepisode return\nPush-Real-Weak&Expert\nLift-Real-Weak&Expert\n0\n200\n400\n600\nepisode return\n0\n500\n1000\n1500\nepisode return\nPush-Real-Mixed\nLift-Real-Mixed\n0\n200\n400\n600\nepisode return\n0\n500\n1000\n1500\nepisode return\nFigure S11: Distribution of the episode returns in the datasets recorded on the real system.\nFigure S12: Dataset visualization. Initial cube positions (blue) and goals (red) in the real datasets\nfor the Push task (left) and Lift task (right). Note that we avoid initial positions close to the barrier.\nInitial cube position and goal distributions:\nFig. S12 visualizes the distribution of the initial cube\nposition (blue) and the goal position (red) for the Push and Lift tasks. The goal positions are sampled\non the ground for the Push task and in the air for the Lift task. The distribution of the initial cube\nposition results from the reset procedure which removes the cube from the boundaries by moving the\nfingers along a pre-recorded trajectory.\nAction statistics:\nFig. S13 shows the distribution of actions of the expert policy that recorded\nTriFinger-Push-Real-Expert and of a policy learned from this data by AWAC.\nReset procedure:\nAs mentioned in section 3.3 of the main text, we removed the cube from the\nbarrier between episodes because we observed that the expert policies we trained in simulation\nstruggled with retrieving the cube from the barrier. Fig. S14a shows a histogram of the distance\nbetween the center of the cube and the arena center. While the reset procedure is sufficiently stochastic\n29\nPublished as a conference paper at ICLR 2023\nfinger 0, joint 0\nfinger 0, joint 1\nfinger 0, joint 2\nfinger 1, joint 0\nfinger 1, joint 1\nfinger 1, joint 2\n0.2\n0.0\n0.2\naction [Nm]\nfinger 2, joint 0\n0.2\n0.0\n0.2\naction [Nm]\nfinger 2, joint 1\n0.2\n0.0\n0.2\naction [Nm]\nfinger 2, joint 2\nPush-Real-Expert\nAWAC\nFigure S13: Distribution of action components, i.e., desired torques, for the expert policy that\nrecorded the TriFinger-Push-Real-Expert dataset and for a policy learned with AWAC from this\ndataset. Averaged over five offline RL seeds.\nto randomize the initial cube position (see Fig. S12), it clearly removes the cube from the barrier in\nthe majority of all resets. More precisely, only in 3% of all resets the cube center was within 6 cm of\nthe boundary (the cube has a width of 6.5 cm).\n0\n5\n10\n15\n20\ndistance cube center to arena center [cm]\n(a) Cube distribution after reset\n0\n500\n1000\n1500\ntime step\nfrequency of dropping\npure feedforward\nsmoothed\n(b) Dropping frequency\n0\n10\nt [s]\n0.0\n0.5\nmomentary success\n(c) Momentary success\nFigure S14: (a) Histogram of the distance between the center of the cube and the arena center after\nthe reset for the Lift-Real-Expert dataset. In the shaded gray area the cube can potentially touch the\nbarrier which begins at the vertical black line. (b) Dropping frequency over a real lift episode. ‘Pure\nfeedforward’ refers to directly applying the output of an MLP policy whereas ‘smoothed’ denotes a\npolicy with an Exponential Moving Average applied. (c) Comparison of the momentary success rate\nof a policy learned from Push-Real-Expert (AWAC in green) with the expert policy (PPO-Expert in\nblue) that recorded the data. The momentary success rate captures how often the policy achieved the\ngoal at a time step averaged over all episodes. The momentary success rate of the weak policy used\nin Push-Real-Weak&Expert is shown in orange.\nDropping frequency – pure feedforward vs. smoothed:\nOne reason for the lower success rate on\nthe Lift task as compared to the Push task is frequent dropping of the cube. This can be the result\nof vibrations of the fingers which can occur when applying policies trained in simulation on the\n30\nPublished as a conference paper at ICLR 2023\nreal robots or of shaking caused by the noisy pose estimate for the cube. We found that applying a\nlow-pass filter, more precisely an Exponential Moving Average, helps with both problems. Fig. S14b\nshows the frequency at which the cube is dropped over the course of a lifting episode. Dropping the\ncube is defined as reaching the height of the goal pose up to a tolerance of 2 cm at some point during\nthe episode and then dropping the cube flat on the ground (up to a tolerance of 1 cm). Smoothing the\nactions before applying them clearly helps with avoiding dropping, in particular later in the episode.\nNote, however, that the smoothing has to be applied already during training in simulation for the\npolicy to adapt to it. Despite the action smoothing the dropping rate does not reach zero. This can be\npartly attributed to unstable grasps that lead to slipping on the real robot.\nB.5\nUSING THE DATASETS\nWe provide an easy-to-use Python package that is compatible with a popular collection of offline RL\ndatasets Fu et al. (2020). To use the datasets, it is sufficient to clone the repository, instantiate the\ndesired environment and call a method that returns the dataset. The correct dataset is then downloaded\nautomatically. We also provide options for filtering the observations and for converting them to flat\narrays.\nThe following example code demonstrates how (parts of) a dataset can be loaded:\nimport gymnasium as gym\nimport numpy as np\nimport trifinger_rl_datasets\nenv = gym.make(\n\"trifinger-cube-push-real-expert-v0\",\ndisable_env_checker=True,\nvisualization=False,\n)\n# load data at timesteps specified in array\ndataset_part = env.get_dataset(indices=np.array([42, 1000, 5000]))\n# load data corresponding to a range of timesteps\ndataset_part = env.get_dataset(rng=(1000, 2000))\n# load the whole dataset\ndataset = env.get_dataset()\nFor installation instructions and further details we refer to the repository of the Python package at\nhttps://github.com/rr-learning/trifinger_rl_datasets.\nB.6\nSUBMITTING A POLICY TO THE ROBOT CLUSTER\nAccess to the robot cluster can be requested at https://webdav.tuebingen.mpg.de/\ntrifinger/. The policy has to be implemented in a GitHub repository following a fixed in-\nterface. We recommend adapting the example package available at https://github.com/\nrr-learning/trifinger-rl-example.\n31\nPublished as a conference paper at ICLR 2023\nTable S6: Performance of the expert policies in the Isaac Gym environment. The numbers for the\nsuccess rate and return are not directly comparable to those in the PyBullet simulator and on the real\nrobot for two reasons: (i) In Isaac Gym Lift episodes last for 15 s instead of 30 s, and (ii) the return in\nthe Isaac Gym environment is contains auxiliary reward terms as described in Allshire et al. (2022)\nand section 3.2.\nsuccess rate\nreturn\n#training steps\nPush\n0.99\n629\n0.83 · 109\nLift\n0.87\n589\n1.72 · 109\nC\nTRAINING\nC.1\nTRAINING EXPERT POLICIES IN SIMULATION\nIn addition to the modifications mentioned in section 3.2, we also ported training from Isaac Gym\n2 to the current version Isaac Gym 3 (Makoviychuk et al., 2021). We furthermore increased the\ntorque range from [−0.36 Nm, 0.36 Nm] to [−0.397 Nm, 0.397 Nm] to make sure the fingers are\nstrong enough to lift the cube when supporting it from below. To make the training environment\nresemble the data collection setting on the real robots, we furthermore implemented an adjustable\ndelay between when an observation is received and when the action based on this observation is\napplied for the first time. The success rates and returns reached after training are shown in Table S6.\nNote that the success rates we give for the lifting task on the real robot are not directly comparable to\nthe ones reported in Allshire et al. (2022) as our lifting task is more challenging. While Allshire et al.\n(2022) evaluate success after 60 s3, we evaluate after 30 s. As policies usually need several attempts\nto flip the cube to roughly the correct orientation and for picking it up, the success rate after 30 s is\nlower in general. We chose the shorter episode length because it is, in principle, sufficient to solve\nthe task and because we wanted to avoid episodes which consist, to a large part, of the cube being\nheld in place. Moreover, unlike Allshire et al. (2022), we do not push the cube to the center of the\narena before each episode but only remove it from the barrier.\nC.2\nTRAINING WITH OFFLINE RL ON THE DATASETS\nWe use the implementations of BC, CRR, AWAC, CQL and IQL provided by the open-source library\nD3RLPY (Seno & Imai, 2021). The code is available at https://github.com/takuseno/\nd3rlpy and the documentation can be found at https://d3rlpy.readthedocs.io/. For\nour experiments we used versions 1.1.0 and 1.1.1 of D3RLPY. The used hyperparameters and the\nperformed optimization are discussed in the next section.\nC.3\nHYPERPARAMETERS\nWe performed a grid search over hyperparameters for all algorithms as documented in Table S7. The\nhyperparameter setting with the highest performance in terms of final average return on Lift-Sim-\nWeak&Expert was selected, as listed in Table S8. In the paper, the results with optimized parameters\nare marked with a †. Otherwise, the default parameters were used, as listed in Table S9.\nThe rest of the section contains a detailed analysis of the grid search.\n3Personal communication\n32\nPublished as a conference paper at ICLR 2023\nTable S7: Hyperparameter grid search (on dataset Lift-Sim-Weak&Expert). Each algorithm is\ntrained with the same two seeds (sampled uniformly at random).\nAlgorithms\nParameters\nAWAC\n{(actor_learning_rate=R, critic_learning_rate=R) : R∈{1.5E-4, 3.0E-4, 6.0E-4}};\nbatch_size∈{256, 512}; lam∈{0.3, 1.0, 3.0}\nBC\nlearning_rate={1.5E-4; 3.0E-4; 6.0E-4}; batch_size∈{256, 512}\nCRR\n{(actor_learning_rate=R, critic_learning_rate=R) : R∈{1.5E-4, 3.0E-4, 6.0E-4}};\nbatch_size∈{256, 512}; beta∈{0.25, 1.0, 4.0}\nCQL\n{(actor_learning_rate=R, critic_learning_rate=3.0*R, initial_alpha=T[1],\nalpha_learning_rate={R if T[0] else 0}, temp_learning_rate=R, alpha_threshold=T[2]) :\nR∈{5.0E-5, 1.0E-4}, T∈{[true, 1.0, 1.0], [true, 1.0, 5.0], [true, 1.0, 10.0],\n[false, 0.3, 10.0], [false, 1.0, 10.0], [false, 3.0, 10.0]} };\nconservative_weight∈{2.5, 5.0, 10.0, 20.0}\nIQL\n{(actor_learning_rate=R, critic_learning_rate=R) : R∈{1.5E-4, 3.0E-4, 6.0E-4}};\nbatch_size∈{256, 512}; expectile∈{0.7, 0.8, 0.9}; weight_temp∈{3.0, 10.0}\nTable S8: Optimized Hyperparameters using grid search Table S7.\nAlgorithms\nParameters\nAWAC†\nactor_learning = critic_learning_rate = 0.00015; batch_size=256; lam=3.0\nBC†\nlearning_rate=0.00015; batch_size=512\nCRR†\nactor_learning = critic_learning_rate = 0.00015; batch_size=256; beta=1.0\nCQL†\nactor_learning_rate=0.0001; critic_learning_rate=0.0003; initial_alpha=1.0;\nconservative_weight=20.0; alpha_learning_rate=0.0; action_scaler: minmax\nIQL†\nactor_learning_rate = critic_learning_rate= 0.00015; batch_size=256;\nexpectile=0.9; weight_temp=3.0\nTable S9: Default hyperparameters. All algorithms except BC (with batch_size=100 and without a\ncritic) have batch_size=256 and n_critics=2. Note that due to bad performance we optimized CQL’s\nparameters on Push-Sim-Expert with an extensive grid search as shown in Fig. S16. For all other\nalgorithms, we used the default values of the implementation.\nAlgorithms\nParameters\nAWAC\nactor_learning = critic_learning_rate = 0.0003; batch_size=1024; lam=1.0\nBC\nlearning_rate=0.001; batch_size=100\nCRR\nactor_learning = critic_learning_rate = 0.0003; batch_size=256; beta=1.0\nCQL\nactor_learning_rate=0.0001; critic_learning_rate=0.0003; initial_alpha=1.0;\nconservative_weight=20.0; alpha_learning_rate=0.0; action_scaler: minmax\nIQL\nactor_learning_rate = critic_learning_rate= 0.0003; batch_size=256;\nexpectile=0.7; weight_temp=3.0\n33\nPublished as a conference paper at ICLR 2023\nIn Fig. S15 we present the returns and success rates for each of the hyperparameter settings for the\nLift task in simulation. We see that the different algorithms have very different sensitivity to their\nhyperparameters. For CRR a large fraction of parameters leads to good results. For AWAC the\nsensitivity is a bit higher. IQL seems to degrade more gracefully with changed parameters. For CQL\nwe were unable to find good hyperparameters, despite running 48 configurations. The performance\nof the individual runs over training time are shown in Fig. S17.\n0\n500\n1000\nreturn\n0.0\n0.5\n1.0\n1.5\n2.0\nfrequency\n0\n500\n1000\nreturn\n0\n2\n4\n6\nfrequency\n0\n500\n1000\nreturn\n0\n2\n4\n6\nfrequency\n0\n500\n1000\nreturn\n0\n10\n20\n30\nfrequency\n0\n500\n1000\nreturn\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nfrequency\n0.0\n0.5\n1.0\nsuccess_rate\n0.0\n0.5\n1.0\n1.5\n2.0\nfrequency\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess_rate\n0\n1\n2\n3\n4\n5\nfrequency\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess_rate\n0\n2\n4\n6\nfrequency\n0.0\n0.5\n1.0\nsuccess_rate\n0\n10\n20\n30\nfrequency\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess_rate\n0\n2\n4\n6\n8\nfrequency\nBC\nCRR\nAWAC\nCQL\nIQL\nFigure S15: Hyperparameter grid search: Histogram of returns and success rates for the Lift-Sim-\nWeak&Expert task.\nOn the Push task, we ran an even larger hyperparameter scan with 405 configurations for CQL, as\npresented in Fig. S16. Even for this much simpler task, we see that the majority of parameters yield\nlow success rates. In addition, we note that the training became quite unstable for alpha_lr > 0.0\nand due to this, we conducted our main grid-search with alpha_lr = 0.0. Then, we chose the best\nhyperparameter configuration as the default setting for CQL.\nReturns\nSuccess rate\n0\n1000\n2000\n3000\nreturn\n0\n20\n40\n60\n80\nfrequency\n0.0\n0.5\n1.0\nsuccess_rate\n0\n50\n100\n150\nfrequency\nParameters\nactor_lr ∈{3.0E-5, 1.0E-4, 3.0E-4};\ncritic_lr ∈{3.0E-5, 1.0E-4, 3.0E-4};\ntemp_lr ∈{3.0E-5, 1.0E-4, 3.0E-4};\ncons_weight ∈{1.0, 5.0, 10.0, 15.0, 20.0};\nbatch_size ∈{256, 1024, 4096};\nalpha_lr = 0.0;\nFigure S16: Hyperparameter grid search on the Push-Sim-Expert task for CQL. Shown is the\nhistogram of returns and success rates for the 405 hyperparameter settings, as defined on the right.\n34\nPublished as a conference paper at ICLR 2023\nReturns\nBC\nCRR\nAWAC\nCQL\nIQL\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\n0\n50\n100\n150\n200\nepochs\n0\n250\n500\n750\n1000\n1250\n1500\nreturns\nSuccess-rates\nBC\nCRR\nAWAC\nCQL\nIQL\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n0\n50\n100\n150\n200\nepochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nFigure S17: Lift-Sim-Weak&Expert. Hyperparameter grid search. Returns (top row) and success\nrate (bottom row). Every curve corresponds to one parameter configuration (averaged over 2 seeds).\nThe black dashed line shows the dataset performance of Sim-Expert and the red dashed line the\nperformance of the Sim-Weak&Expert.\nD\nPOLICY EVALUATION\nD.1\nEVALUATION ON THE REAL SYSTEM\nOn the real platforms, we report numbers for 48 trials for the Push task and 36 trials for the Lift task,\neach for 5 training seeds. Each algorithm and seed is evaluated on 5 to 6 robots with the same set\nof 6 (for the Lift task) to 8 (for the Push task) goals per robot (computed from the robot ID). The\nsuccess rate and return of each algorithm and seed is computed from the resulting trials. The mean\nand standard deviations of the success rates and returns of each algorithm is computed across seeds.\nD.2\nEVALUATION IN THE SIMULATOR\nIn the simulated environment, we perform 100 testing episodes per final policy for 5 independent\ntraining runs.\n35\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2023-07-28",
  "updated": "2023-07-28"
}