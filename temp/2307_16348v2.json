{
  "id": "http://arxiv.org/abs/2307.16348v2",
  "title": "Rating-based Reinforcement Learning",
  "authors": [
    "Devin White",
    "Mingkang Wu",
    "Ellen Novoseller",
    "Vernon J. Lawhern",
    "Nicholas Waytowich",
    "Yongcan Cao"
  ],
  "abstract": "This paper develops a novel rating-based reinforcement learning approach that\nuses human ratings to obtain human guidance in reinforcement learning.\nDifferent from the existing preference-based and ranking-based reinforcement\nlearning paradigms, based on human relative preferences over sample pairs, the\nproposed rating-based reinforcement learning approach is based on human\nevaluation of individual trajectories without relative comparisons between\nsample pairs. The rating-based reinforcement learning approach builds on a new\nprediction model for human ratings and a novel multi-class loss function. We\nconduct several experimental studies based on synthetic ratings and real human\nratings to evaluate the effectiveness and benefits of the new rating-based\nreinforcement learning approach.",
  "text": "Rating-Based Reinforcement Learning\nDevin White1, Mingkang Wu1, Ellen Novoseller2, Vernon J. Lawhern2, Nicholas Waytowich2,\nYongcan Cao1\n1University of Texas, San Antonio\n2DEVCOM Army Research Laboratory\nAbstract\nThis paper develops a novel rating-based reinforcement learn-\ning (RbRL) approach that uses human ratings to obtain hu-\nman guidance in reinforcement learning. Different from the\nexisting preference-based and ranking-based reinforcement\nlearning paradigms, based on human relative preferences over\nsample pairs, the proposed rating-based reinforcement learn-\ning approach is based on human evaluation of individual tra-\njectories without relative comparisons between sample pairs.\nThe rating-based reinforcement learning approach builds on\na new prediction model for human ratings and a novel multi-\nclass loss function. We finally conduct several experimental\nstudies based on synthetic ratings and real human ratings to\nevaluate the performance of the new rating-based reinforce-\nment learning approach.\nIntroduction\nWith the development of deep neural network theory and\nimprovements in computing hardware, deep reinforcement\nlearning (RL) has become capable of handling complex\ntasks with large state and/or action spaces (e.g., Go and Atari\ngames) and yielding human-level or better-than-human-\nlevel performance (Silver et al. 2016; Mnih et al. 2015).\nNumerous approaches, such as DQN (Mnih et al. 2015),\nDDPG (Lillicrap et al. 2015), PPO (Schulman et al. 2017),\nand SAC (Haarnoja et al. 2018) have been developed to ad-\ndress challenges such as stability, exploration, and conver-\ngence for various applications (Li 2019) such as robotic con-\ntrol, autonomous driving, and gaming. Despite the important\nand fundamental advances behind these algorithms, one key\nobstacle for the wide application of deep RL is the required\nknowledge of a reward function, which is often unavailable\nin practical applications.\nAlthough human experts could design reward functions\nin some domains, the cost is high because human experts\nneed to understand the relationship between the mission\nobjective and state-action values and may need to spend\nextensive time adjusting reward parameters and trade-offs\nnot to encounter adverse behaviors such as reward hack-\ning (Amodei et al. 2016). Another approach is to uti-\nlize qualitative human inputs indirectly to learn a reward\nfunction, such that humans guide reward function design\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nwithout directly handcrafting the reward. Existing work\non reward learning includes inverse reinforcement learning\n(IRL) (Ziebart et al. 2008), preference-based reinforcement\nlearning (PbRL) (Christiano et al. 2017), and the combina-\ntion of demonstrations and relative preferences, e.g. learning\nfrom preferences over demonstrations (Brown et al. 2019).\nExisting human-guided reward learning approaches have\ndemonstrated effective performance in various tasks. How-\never, they suffer from some key limitations. For example,\nIRL requires expert demonstrations and hence, cannot be di-\nrectly applied to tasks that are difficult for humans to demon-\nstrate. PbRL is a practical approach to learning rewards for\nRL, since it is straightforward for humans to provide accu-\nrate relative preference information. Yet, RL from pairwise\npreferences suffers from some key disadvantages. First, each\npairwise preference provides only a single bit of informa-\ntion, which can result in sample inefficiency. In addition, due\nto their binary nature, standard preference queries do not in-\ndicate how much better or worse one sample is than another.\nFurthermore, because preference queries are relative, they\ncannot directly provide a global view of each sample’s abso-\nlute quality (good vs. bad); for instance, if all choices shown\nto the user are of poor quality, the user cannot say, “A is bet-\nter than B, but they’re both bad!”. Thus, a PbRL algorithm\nmay be more easily trapped in a local optimum, and cannot\nknow to what extent its performance approaches the user’s\ngoal. Finally, PbRL methods often require strict preferences,\nsuch that comparisons between similar-quality or incompa-\nrable trajectories cannot be used in reward learning. While\nsome works use weak preference queries (Bıyık et al. 2020;\nBıyık, Talati, and Sadigh 2022), in which the user can state\nthat two choices are equally preferable, there is no way to\nspecify the quality (good vs. poor) of such trajectories; thus,\nvaluable information remains untapped.\nThe objective of this paper is to design a new rating-\nbased RL (RbRL) approach that infers reward functions\nvia multi-class human ratings. RbRL differs from IRL and\nPbRL in that it leverages human ratings on individual sam-\nples, whereas IRL uses demonstrations and PbRL uses rel-\native pairwise comparisons. In each query, RbRL displays\none trajectory to a human and requests the human to pro-\nvide a discrete rating. The number of rating classes can be\nas low as two, e.g. “bad” and “good”, and can be as high\nas desired. For example, when the number of rating classes\narXiv:2307.16348v2  [cs.LG]  29 Jan 2024\nis 5, the 5 possible human ratings could correspond to “very\nbad”, “bad”, “ok”, “good”, and “very good”. It is worth men-\ntioning that the statement “samples A and B are both rated\nas ‘good’ ” may provide more information than stating that\n“A and B are equally preferable”, which can be inferred by\nthe former. However, “A and B are equally preferable” may\nbe important information for fine-tuning. In addition, a per-\nson can also intentionally assign high ratings to samples that\ncontain rare states, which would be beneficial for address-\ning the exploration issue (Ecoffet et al. 2019) in RL. For\nboth PbRL and RbRL, obtaining good samples requires ex-\nploration, and both will suffer without any well-performing\nsamples.\nThe main contributions of this paper are as follows. First,\nwe propose a novel RbRL framework for reward function\nand policy learning from qualitative, absolute human eval-\nuations. Second, we design a new multi-class cross-entropy\nloss function that accepts multi-class human ratings as the\ninput. The new loss function is based on the computation\nof a relative episodic reward index and the design of a new\nmulti-class probability distribution function based on this in-\ndex. Third, we conduct several experimental studies to quan-\ntify the impact of the number of rating classes on the perfor-\nmance of RbRL, and compare RbRL and PbRL under both\nsynthetic and real human feedback. Our studies suggest that\n(1) too few or too many rating classes can be disadvanta-\ngeous, (2) RbRL can outperform PbRL under both synthetic\nand real human feedback, and (3) people find RbRL to be\nless demanding, discouraging, and frustrating than PbRL.\nRelated Work\nInverse Reinforcement Learning (IRL) seeks to infer re-\nward functions from demonstrations such that the learned\nreward functions generate behaviors that are similar to\nthe demonstrations. Numerous IRL methods (Ng, Rus-\nsell et al. 2000), such as maximum entropy IRL (Ziebart\net al. 2008; Wulfmeier, Ondruska, and Posner 2015), non-\nlinear IRL (Finn, Levine, and Abbeel 2016), Bayesian\nIRL (Levine, Popovic, and Koltun 2011; Choi and Kim\n2011, 2012), adversarial IRL (Fu, Luo, and Levine 2018),\nand behavioral cloning IRL (Szot et al. 2022) have been de-\nveloped to infer reward functions. The need for demonstra-\ntions often makes these IRL methods costly, since human\nexperts are needed to provide demonstrations.\nInstead of requiring human demonstrations, PbRL (Wirth\net al. 2017; Christiano et al. 2017; Ibarz et al. 2018; Liang\net al. 2022; Zhan, Tao, and Cao 2021; Xu et al. 2020; Lee,\nSmith, and Abbeel 2021; Park et al. 2022) leverages human\npairwise preferences over trajectory pairs to learn reward\nfunctions. Querying humans for pairwise preferences rather\nthan demonstrations can dramatically save human time. In\naddition, by leveraging techniques such as adversarial neu-\nral networks (Zhan, Tao, and Cao 2021), additional human\ntime can be saved by learning a well-performing model to\npredict human preference. Another benefit of PbRL is that\nhumans can provide preferences with respect to uncertainty\nto promote exploration (Liang et al. 2022). Despite these\nbenefits, PbRL can be ineffective, especially for complex\nenvironments, because pairwise preferences only provide\nrelative information rather than directly evaluating sample\nquality; while in some domains, sampled pairs may be se-\nlected carefully to infer global information, in practice, even\nif one sample is preferred over another, it does not necessar-\nily mean that this sample is good. People can also have dif-\nficulty when comparing similar samples, thus taking more\ntime and potentially yielding inaccurate preference labels.\nNotably, several works have sought to improve sample ef-\nficiency of PbRL; for instance, PEBBLE (Lee, Smith, and\nAbbeel 2021) considers off-policy PbRL, and SURF (Park\net al. 2022) explores data augmentations in PbRL. These\ncontributions are orthogonal to ours, as they could straight-\nforwardly be applied within our proposed RbRL framework.\nOther methods for learning reward functions from hu-\nmans include combining relative rankings and demonstra-\ntions, e.g. by inferring rewards via rankings over a pool of\ndemonstrations (Brown et al. 2020, 2019; Brown, Goo, and\nNiekum 2020) to extrapolate better-than-demonstrator per-\nformance from the learned rewards, or first learning from\ndemonstrations and then fine-tuning with preferences (Ibarz\net al. 2018; Bıyık et al. 2022). Finally, in the TAMER frame-\nwork (Knox and Stone 2009; Warnell et al. 2018; Celemin\nand Ruiz-del Solar 2015), a person gives positive (encourag-\ning) and negative (discouraging) feedback to an agent with\nrespect to specific states and actions, instead of over entire\ntrajectories. These methods generally take actions greedily\nwith respect to the learned reward, which may not yield an\noptimal policy in continuous control settings.\nProblem Formulation\nWe consider a Markov decision process without reward\n(MDP\\R) augmented with ratings, which is a tuple of the\nform (S, A, T, ρ, γ, n). Here, S is the set of states, A is\nthe set of possible actions, T : S × A × S →[0, 1] is a\nstate transition probability function specifying the probabil-\nity p(s′ | s, a) of reaching state s′ ∈S after taking action\na in state s, ρ : S →[0, 1] specifies the initial state distri-\nbution, γ is a discount factor, and n is the number of rating\nclasses. The learning agent interacts with the environment\nthrough rollout trajectories, where a length-k trajectory seg-\nment takes the form (s1, a1, s2, a2, . . . , sk, ak). A policy π\nis a function that maps states to actions, such that π(a | s) is\nthe probability of taking action a ∈A in state s ∈S.\nIn traditional RL, the environment would receive a reward\nsignal r : S × A →R, mapping state-action pairs to a nu-\nmerical reward, such that at time-step t, the algorithm re-\nceives a reward rt = r(st, at), where (st, at) is the state-\naction pair at time t. Accordingly, the standard RL prob-\nlem can be formulated as a search for the optimal policy\nπ∗, where π∗= arg maxπ\nP∞\nt=0 E(st,at)∼ρπ\nh\nγtr(st, at)\ni\n,\nat ∼π(·|st), and ρπ is the marginal state-action distribu-\ntion induced by the policy π. Note that standard RL assumes\nthe availability of the reward function r. When such a re-\nward function is unavailable, standard RL and its variants\nmay not be used to derive control policies. Instead, we as-\nsume that the user can assign any given trajectory segment\nτ = (s1, a1, . . . , sk, ak) a rating in the set {0, 1, . . . , n −1}\nindicating the quality of that segment, where 0 is the lowest\npossible rating, while n −1 is the highest possible rating.\nThe algorithm presents a series of trajectory segments σ\nto the human and receives corresponding human ratings. Let\nX := {(σi, ci)}l\ni=1 be the dataset of observed human rat-\nings, where ci ∈{0, . . . , n −1} is the rating class assigned\nto segment σi, and l is the number of rated segments con-\ntained in X at the given point during learning.\nNote that descriptive labels can also be given to the rating\nclasses. For example, for n = 4 rating classes, we can call\nthe rating class 0 “very bad”, the rating class 1 “bad”, the\nrating class 2 “good”, and the rating class 3 “very good”.\nWith n = 3 rating classes, we can call the rating class 0\n“bad”, the rating class 1 “neutral”, and class 2 “good”.\nRating-based Reinforcement Learning\nDifferent from the binary-class reward learning in Christiano\net al. (2017) that utilizes relative human preferences between\nsegment pairs, RbRL utilizes non-binary multi-class ratings\nfor individual segments. We call this a multi-class reinforce-\nment learning approach based on ratings.\nModeling Reward and Return\nOur approach learns a reward model ˆr : S × A →R\nthat predicts state-action rewards ˆr(s, a). We further de-\nfine ˆR(σ) := Pk\nt=1 γt−1ˆr(st, at) as the cumulative dis-\ncounted reward, or the return, of length-k trajectory segment\nσ. Larger ˆR(σ) corresponds to a higher predicted human rat-\ning for segment σ. Next, we define ˜R(σ) as a function map-\nping a trajectory segment σ to an estimated total discounted\nreward, normalized to fall in the interval [0, 1] based on the\ndataset of rated trajectory segments X:\n˜R(σ) =\nˆR(σ) −minσ′∈X ˆR(σ′)\nmaxσ′∈X ˆR(σ′) −minσ′∈X ˆR(σ′)\n.\n(1)\nNovel Rating-Based Cross-Entropy Loss Function\nTo construct a new (cross-entropy) loss function that can\ntake multi-class human ratings as the input, we need to es-\ntimate the human’s rating class predictions. In addition, the\nrange of the estimated rating class should belong to the in-\nterval [0, 1] for the cross-entropy computation. We here pro-\npose a new multi-class cross-entropy loss given by:\nL(ˆr) = −\nX\nσ∈X\n n−1\nX\ni=0\nµσ(i) log\n\u0000Qσ(i)\n\u0001\n!\n,\n(2)\nwhere X is the collected dataset of user-labeled segments,\nµσ(i) is an indicator that equals 1 when the user assigns\nrating i to trajectory segment σ, and Qσ(i) ∈[0, 1] is the\nestimated probability that the human assigns the segment σ\nto the ith rating class. Next, we will model the probabilities\nQσ(i) of the human choosing each rating class. Notably, we\ndo this without comparing the segment σ to other segments.\nModeling Human Rating Probabilities\nWe next describe our model for Qσ(i) based on the normal-\nized predicted returns ˜R(σ). To model the probability that σ\nbelongs to a particular class, we will first model separations\nbetween the rating classes in reward space.\nWe define rating class boundaries ¯R0, ¯R1, . . . , ¯Rn in the\nspace of normalized trajectory returns such that 0 := ¯R0 ≤\n¯R1 ≤. . . ≤¯Rn := 1. Then, if a segment σ has normal-\nized predicted return ˜R(σ) such that ¯Ri ≤˜R(σ) ≤¯Ri+1,\nwe wish to model that σ belongs to rating class i with the\nhighest probability.\nFor example, when the total number of rating classes is\nn = 4, we aim to model the lower and upper return bounds\nfor rating classes 0, 1, 2, and 3, which for instance, could\nrespectively correspond to “very bad”, “bad”, “good”, and\n“very good”. In this case, if 0 ≤˜R(σ) < ¯R1, then we would\nlike our model to predict that σ most likely belongs to class\n0 (“very bad”), while if ¯R2 ≤˜R(σ) < ¯R3, then our model\nshould predict that σ most likely belongs to class 2 (“good”).\nGiven the rating category separations ¯Ri, we model Qσ(i)\nas a function of the normalized predicted returns ˜R(σ):\nQσ(i) =\ne−k( ˜\nR(σ)−¯\nRi)( ˜\nR(σ)−¯\nRi+1)\nPn−1\nj=0 e−k( ˜\nR(σ)−¯\nRj)( ˜\nR(σ)−¯\nRj+1) ,\n(3)\nwhere k is a hyperparameter modeling human label noisi-\nness, and the denominator ensures that Pn−1\ni=0 Qσ(i) = 1,\ni.e. that the class probabilities sum to 1.\nTo gain intuition for Equation (3), note that when ˜R(σ) ∈\n( ¯Ri, ¯Ri+1), such that the predicted return falls within rating\nclass i’s predicted boundaries, then −( ˜R(σ) −¯Ri)( ˜R(σ) −\n¯Ri+1) ≥0 while −( ˜R(σ) −¯Rj)( ˜R(σ) −¯Rj+1) ≤0 for\nall j ̸= i. This means that Qσ(i) ≥Qσ(j), j ̸= i, so that\nthe model assigns category i the highest class probability,\nas desired. Furthermore, we note that Qσ(i) is maximized\nwhen ˜R(σ) = 1\n2( ¯Ri + ¯Ri+1), such that the predicted return\nfalls directly in the center of category i’s predicted range.\nAs ˜R(σ) becomes increasingly further from 1\n2( ¯Ri + ¯Ri+1),\nthe modeled probability Qσ(i) of class i monotonically de-\ncreases. These probability trends are illustrated in Figure 8\nin the Appendix. We next show how to compute the class\nboundaries ¯Ri, i = 1, . . . , n −1.\nModeling Boundaries between Rating Categories\nNext, we discuss how to model the boundaries between rat-\ning categories, 0 =: ¯R0 ≤¯R1 ≤. . . ≤¯Rn := 1. This re-\nquires selecting the range, or the upper and lower bounds of\n˜R, corresponding to each rating class. We determine these\nboundary values based on the distribution of ˜R(σ) for the\ntrajectory segments σ ∈X and the number of observed sam-\nples in X from each rating class. We select the ¯Ri values\nsuch that the number of training data samples that the model\nassigns to each modeled rating class matches the number of\nsamples in X that the human assigned to that rating class.\nNote that this does not require the predicted ratings based\non ˜R(σ) to match the human ratings for σ in the training\ndata X, but ensures that the proportions of segments in the\ntraining dataset X assigned to each rating class matches that\nin X. This matching in rating class proportions is desirable\nfor learning an appropriate reward function based on hu-\nman preference, since different humans could give ratings in\nsignificantly different proportions depending on their pref-\nerences and latent reward functions, as modeled by ˆR.\nTo define each ¯Ri so that the number of samples in each\nmodeled rating category reflects the numbers of ratings in\nthe human data, we first sort the estimated returns ˜R(σ) for\nall σ ∈X from lowest to highest, and label these sorted es-\ntimates as ˜R1 ≤˜R2 ≤· · · ≤˜Rl, where l is the cardinality\nof X. Denoting via kj the number of segments that the hu-\nman assigned to rating class j, j ∈{0, · · · , n −1}, we can\nthen model each category boundary ¯Ri, i /∈{0, n} (since\n¯R0 := 0 and ¯Rn := 1 by definition), as follows:\n¯Ri =\n˜Rkcum\ni−1 + ˜R1+kcum\ni−1\n2\n,\ni ∈{1, 2, . . . , n −1},\n(4)\nwhere kcum\ni\n:= Pi\nj=0 kj is the total number of segments that\nthe human assigned to any rating category j ≤i. When the\nuser has not assigned any ratings within a particular cate-\ngory, i.e., ki = 0 for some i, then we define the upper bound\nfor category i as ¯Rki+1 := ¯Rki.\nThis definition guarantees that when all normalized re-\nturn predictions ˜R(σ), σ ∈X, are distinct, then our model\nplaces k0 segments within interval [ ¯R0, ¯R1), ki segments\nwithin each interval ( ¯Ri, ¯Ri+1) for 1 ≤i ≤n−2, and kn−1\nsegments in ( ¯Rn−1, ¯Rn], and thus predicts that ki segments\nmost likely have rating i.\nSynthetic Experiments\nSetup\nWe conduct synthetic experiments based on the setup in Lee\net al. (2021) to evaluate RbRL relative to the PbRL base-\nline (Lee et al. 2021). The code can be found at https:\n//rb.gy/tdpc4y. The goal is to learn to perform a task by\nobtaining feedback from a teacher, in this case a synthetic\nhuman. For the PbRL baseline, we generate synthetic feed-\nback such that in each queried pair of segments, the seg-\nment with the higher ground truth cumulative reward is pre-\nferred. In contrast to the synthetic preferences between sam-\nple pairs in Lee et al. (2021), RbRL was given synthetic rat-\nings generated for individual samples, where these ratings\nwere given by comparing the sample’s ground truth return to\nthe ground truth rating class boundaries. For simplicity, we\nselected these ground truth rating class boundaries so that\nrating classes are evenly spaced in reward space.\nFor the synthetic PbRL experiments, we selected pref-\nerence queries using the ensemble disagreement approach\nin Lee et al. (2021). We extend this method to select rat-\ning queries for the synthetic RbRL experiments, designing\nan ensemble-based approach as in Lee et al. (2021) to se-\nlect trajectory segments for which to obtain synthetic rat-\nings. First, we train a reward predictor ensemble and obtain\nthe predicted reward for every candidate segment and en-\nsemble member. We then select the segment with the largest\nstandard deviation over the ensemble to receive a rating la-\nbel. We study the Walker and Quadruped tasks in Lee et al.\n(2021), with 1000 and 2000 synthetic queries, respectively.\nFor all synthetic experiments, the reward network param-\neters are optimized to minimize the cross entropy loss (2)\nbased on the respective batch of data via the computation\nof (3). We use the same neural network structures for both\nthe reward predictor and control policy and the same hyper-\nparameters as in Lee et al. (2021).\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n100\n200\n300\n400\n500\n600\n700\n800\n900\nMean Reward\nPreferences\nRatings (n=2)\nRatings (n=3)\nRatings (n=4)\nRatings (n=5)\nRatings (n=6)\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n100\n200\n300\n400\n500\nMean Reward\nPreferences\nRatings (n=2)\nRatings (n=3)\nRatings (n=4)\nRatings (n=5)\nRatings (n=6)\nFigure 1: Performance of RbRL in synthetic experiments for\ndifferent n, compared to PbRL: mean reward ± standard er-\nror over 10 runs for Walker (top) and Quadruped (bottom).\nResults\nFigure 1 shows the performance of RbRL for different num-\nbers of rating classes (i.e. values of n) and PbRL for two en-\nvironments from Lee et al. (2021): Walker and Quadruped.\nWe observe that a higher number of rating classes yields bet-\nter performance for Walker. In addition, RbRL with n = 5, 6\noutperforms PbRL. However, for Quadruped, while RbRL\nwith n = 2, 3 still outperforms PbRL, a higher number of\nrating classes decreases performance; this decrease may be\ncaused by the selection of rating class boundaries used to\ngenerate the synthetic feedback. The results indicate that\nRbRL is effective and can provide better performance than\nPbRL even if synthetic ratings feedback is generated using\nreward thresholds that are evenly distributed, without further\noptimization of their selection. We expect further optimiza-\ntion of the boundaries used to generate synthetic feedback\nto yield improved performance. For our experiments, we de-\nfined the rating boundaries by finding the maximum possible\nreward range for a segment and evenly dividing by the num-\nber of rating classes.\nHuman Experiments\nSetup\nWe conduct all human experiments by following a similar\nsetup to Christiano et al. (2017). In particular, our tests were\napproved by the UTSA IRB Office, including proper steps\nto ensure privacy and informed consent of all participants.\nIn particular, the goal is to learn to perform a given task\nby obtaining feedback from a teacher, in this case a human.\nDifferent from PbRL in Christiano et al. (2017), which asks\nhumans to provide their preferences between sample pairs,\ntypically in the form of short video segments, RbRL asks\nhumans to evaluate individual samples, also in the form of\nshort video segments, to provide their ratings, e.g., “segment\nperformance is good” or “segment performance is bad”.\nFor all human experiments, we trained a reward predictor\nby minimizing the cross entropy loss (2) based on the re-\nspective batch of data via the computation of (3). We used\nthe same neural network structures for both the reward pre-\ndictor and control policy and the same hyperparameters as\nin Christiano et al. (2017).\nRbRL with Different Numbers of Rating Classes\nTo evaluate the impact of the number of rating classes n\non RbRL’s performance, we first conduct tests in which\na human expert (an author on the study) provides ratings\nwith n = 2, . . . , 8 in the Cheetah MuJoCo environment. In\nparticular, three experiment runs were conducted for each\nn ∈{2, 3, . . . , 8}. Fig. 2 shows the performance of RbRL\nfor each n. It can be observed that RbRL performs better for\nn ∈{3, 4, . . . , 7} than for n ∈{2, 8}, indicating that al-\nlowing more rating classes is typically beneficial. However,\nan overly large number of rating classes n will lead to dif-\nficulties and inaccuracies in the human ratings, and hence\nn must be set to a reasonable value. Indeed, for smaller n,\none can more intuitively assign physical meanings to each\nn, whereas for overly large n, it becomes difficult to assign\nsuch physical meanings, and hence it will be more challeng-\ning for users to provide consistent ratings.\nn = 2\nn = 3\nn = 4\nn = 5\nn = 6\nn = 7\nn = 8\n0\n500\n1000\n1500\n2000\n2500\n3000\nMean Reward\nFigure 2: RbRL performance for different n in a human ex-\nperiment: performance in the Cheetah environment (mean ±\nstandard error over 3 experiment runs).\nRbRL Human User Study\nTo evaluate the effectiveness of RbRL for non-expert users,\nwe conducted an IRB-approved human user study. We con-\nducted tests on 3 of the OpenAI Gym MuJoCo Environ-\nments also used in Christiano et al. (2017): Swimmer, Hop-\nper and Cheetah. A total of 20 participants were recruited\n(7 for Cheetah, 7 for Swimmer, and 6 for Hopper). In our\nexperiments, we provided a single 1 to 2 second long video\nsegment to query users for each rating, while we provided\npairs of 1 to 2 second videos to obtain human pairwise pref-\nerences. For Cheetah, the goal is to move the agent to the\nright as fast as possible; this is the same goal encoded in the\ndefault hand-crafted environment reward. Similarly, the goal\nfor Swimmer matches that of the default hand-crafted envi-\nronment reward. However, for Hopper, we instructed users\nto teach the agent to perform a backflip, which differs from\nthe goal encoded by the default hand-crafted environment\nreward. We chose to study the back flip task to see how\nwell RbRL can learn new behaviors for which a reward is\nunknown. Thus, the performance of Cheetah and Swimmer\ncan be evaluated via the hand-crafted environment rewards,\nwhile the Hopper task cannot be evaluated via its hand-\ncrafted environment reward. For Hopper, the performance\nof RbRL will be evaluated based on evaluating the agent’s\nbehavior when running the learned policies from RbRL.\nDuring the user study, each participant performed two\ntests—one for RbRL and one for PbRL—in one of the three\nMuJoCo environments, both for n = 2 rating classes. To\neliminate potential bias, we assigned each participant a ran-\ndomized order in which to perform the PbRL and RbRL ex-\nperiment runs. Because the participants had no prior knowl-\nedge of the MuJoCo environments tested, we provided sam-\nple videos to show desired and undesired behaviors so that\nthe participants could better understand the task. Upon re-\nquest, the participants could also conduct mock tests before\nwe initiated human data collection. For each experiment run,\nthe participant was given 30 minutes to give rating/prefer-\nence labels. Once finished, the participant filled out a ques-\ntionnaire about the tested algorithm. The participant was\nthen given a 10 minute break before conducting the second\ntest and completing the questionnaire about the other algo-\nrithm. Afterwards, the participant completed a questionnaire\ncomparing the two algorithms. The questionnaires can be\nfound in the Appendix. Policy and reward learning occurred\nduring the 30 minutes in which the user answered queries,\nand then continued after the human stepped away until code\nexecution reached 4 million environment time-steps.\nPerformance\nFigure 3 shows the performance of PbRL\nand RbRL across the seven participants for the Cheetah and\nSwimmer tasks. We see that RbRL performs similarly to or\nbetter than PbRL. In particular, RbRL can learn quickly in\nboth cases, evidenced by the fast reward growth early dur-\ning learning. Figure 3 additionally displays results when an\nexpert (an author on the study) provided ratings and prefer-\nences for Cheetah and Swimmer. For consistency, the same\nexpert tested PbRL and RbRL in each environment. We ob-\nserve that for the expert trials, RbRL performs consistently\nbetter than PbRL given the same human time. These results\nsuggest that RbRL can outperform PbRL regardless of the\nuser’s environment domain knowledge. It can also be ob-\nserved that the RbRL and PbRL trials with expert users out-\nperform the trials in which feedback is given by non-experts.\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n0\n20\n40\n60\n80\n100\n120\n140\nMean Reward\nRatings (n=2)\nPreferences\nExpert Ratings (n=2)\nExpert Preferences\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n5\n0\n5\n10\n15\n20\nMean Reward\nRatings (n=2)\nPreferences\nExpert Ratings (n=2)\nExpert Preferences\nFigure 3: Performance of RbRL and PbRL in the human user\nstudy: Cheetah (top) and Swimmer (bottom). For non-expert\nusers, the plots show mean ± standard error over 7 users.\nThe expert results are each over a single experiment run.\nAlthough RbRL performs similarly to PbRL in the Chee-\ntah task, we observed that some participants performed very\npoorly in this environment, perhaps due to lack of under-\nstanding of the task. In the Appendix, the raw data of all\nparticipants for Cheetah and Swimmer is provided to show\nperformance under each individual participant’s guidance.\nFrom the individual results for Cheetah (RbRL), we can see\nthat one of the trials performs very poorly (with the final re-\nward less than −10). For all other tests, including both PbRL\nand RbRL, the final reward is in positive territory, usually\nmore than 20. Hence, it may be more meaningful to evalu-\nate the mean results for individuals who perform reasonably.\nFigure 4 shows the mean reward for the top 3 non-expert\nusers at different iterations for Cheetah and Swimmer. It can\nbe observed that RbRL consistently outperforms PbRL and\nlearns the goal faster than PbRL.\nTo compare PbRL and RbRL in the Hopper backflip task,\nwe ran the learned policies for the 6 participants to gener-\nate videos. Videos for the best learned policies from PbRL\nand RbRL can be found at rb.gy/nt1qm6, and indicate that\n(1) both RbRL and PbRL can learn the backflip, and (2) the\nbackflip learned via RbRL fits better with our understanding\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n0\n20\n40\n60\n80\n100\n120\n140\nMean Reward\nRatings (n=2)\nPreferences\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nMean Reward\nRatings (n=2)\nPreferences\nFigure 4: RbRL and PbRL performance for the top 3 (non-\nexpert) user study participants: mean reward ± standard er-\nror over the 3 experiment runs each for Cheetah (top) and\nSwimmer (bottom).\nof a desired backflip.\nUser Questionnaire Results\nThe previous results in this\nsection focus on evaluating the performance of RbRL and\nPbRL via the ground-truth environment reward (Cheetah\nand Swimmer) and the learned behavior (Hopper). To un-\nderstand how the non-expert users view their experience of\ngiving rating and preferences, we conduct a post-experiment\nuser questionnaire, shown in the last section of the Ap-\npendix. The questionnaire asked users for feedback about\ntheir experience supervising PbRL and RbRL and to com-\npare PbRL and RbRL. Figure 5 displays the normalized sur-\nvey results from the 20 user study participants. In particu-\nlar, the top subfigure of Figure 5 shows the participants’ re-\nsponses with respect to their separate opinions about PbRL\nand RbRL. These responses suggest that PbRL is more de-\nmanding and difficult than RbRL, leading users to feel more\ninsecure and discouraged than when using RbRL. The bot-\ntom subfigure of Figure 5 shows the survey responses when\nusers were asked to compare PbRL and RbRL; these results\nconfirm the above findings and also show that users per-\nceive themselves as completing the task more successfully\nwhen providing ratings (RbRL). One interesting observa-\ntion is that the participants prefer RbRL and PbRL equally,\nwhich differs from the other findings. However, one par-\nticipant stated that he/she preferred PbRL because PbRL is\nmore challenging, which is counter-intuitive. This suggests\nthat “liking” one algorithm more than the other is a very sub-\njective concept, making the responses for this question less\ninformative than those for the other survey questions.\nVery Low\nNeutral\nVery High\nResponse Value\nMental Demand \n(PbRL)\nMental Demand \n(RbRL)\nSuccess (PbRL)\nSuccess (RbRL)\nDifficulty (PbRL)\nDifficulty (RbRL)\nFrustration (PbRL)\nFrustration (RbRL)\nRbRL\nNeutral\nPbRL\nResponse Value\nMental Demand\nSuccess\nDifficulty\nFrustration\nPreference\nFigure 5: Participants’ responses to survey questions about\nRbRL and PbRL. The set of survey questions is detailed in\nthe Appendix. The blue bar indicates the median and the\nedges depict the 1st quartile (left) and 3rd quartile (right).\nHuman Time\nWe also conducted a quantitative analy-\nsis of human time effectiveness when humans were asked\nto give ratings and preferences. Figure 6 shows the aver-\nage number of human queries provided in 30 minutes for\nCheetah, Swimmer, Hopper, and for all three environments\ncombined. It can be observed that the participants can pro-\nvide more ratings than pairwise preferences in all environ-\nments, indicating that it is easier and more efficient to pro-\nvide ratings than to provide pairwise preferences. On aver-\nage, participants can provide approximately 14.03 ratings\nper minute, while they provide only 8.7 preferences per\nminute, which means that providing a preference requires\n62% more time than providing a rating. For Cheetah, provid-\ning a preference requires 100%+ more time than providing\na rating, which is mainly due to the need to compare video\npairs that are very similar. For Swimmer and Hopper, the\nenvironments and goals are somewhat more complicated.\nHence, providing ratings can be slightly more challenging,\nbut is still easier than providing pairwise preferences.\nDiscussion and Open Challenges\nOne key difference between PbRL and RbRL is the value of\nthe acquired human data. Because ratings in RbRL are not\nrelative, they have the potential to provide more global value\nCheetah\nSwimmer\nHopper\nTotal Queries\n0\n100\n200\n300\n400\n500\nNumber of Queries\n153\n292\n350\n261\n331\n467\n472\n421\nPbRL\nRbRL\nFigure 6: Number of queries provided in 30 minutes in our\nhuman user study (mean ± standard error).\nthan preferences, especially when queries are not carefully\nselected. For environments with large state-action spaces,\nratings can provide more value for reward learning. One lim-\nitation of ratings feedback is that the number of data samples\nin different rating classes can be very different, leading to\nimbalanced datasets. Reward learning in RbRL can be nega-\ntively impacted by this data imbalance issue (although our\nexperiments still show the benefits of RbRL over PbRL).\nHence, on-policy training with a large number of training\nsteps may not help reward learning in RbRL because the\ncollected human ratings data can become very unbalanced.\nWe expect that addressing the data imbalance issue would\nfurther improve RbRL performance.\nOne challenge for RbRL is that ratings may not be given\nconsistently during learning, especially considering users’\nattention span and fatigue level over time. Future work in-\ncludes developing mechanisms to quantify users’ consis-\ntency levels, the impact of user inconsistency, or solutions\nto user inconsistency. Another potential limitation of RbRL\nis that it learns a less refined reward function than PbRL be-\ncause RbRL does not seek to distinguish between samples\nfrom the same rating class. Hence, future work could inte-\ngrate RbRL and PbRL to create a multi-phase learning strat-\negy, where RbRL provides fast initial global learning while\nPbRL further refines performance via local queries based on\nsample pairs.\nOne open challenge is the lack of effective human in-\nterfaces in existing code bases. For example, in Lee et al.\n(2021), only synthetic human feedback is available. Al-\nthough a human interface is available for the algorithm\nin Christiano et al. (2017), the use of Google cloud makes\nit difficult to set up and operate efficiently. One of our future\ngoals is to address this challenge by developing an effec-\ntive human interface for reinforcement learning from human\nfeedback, including preferences, ratings, and their variants.\nAcknowledgements\nThe authors were supported in part by the Army Research\nLab under grant W911NF2120232, Army Research Office\nunder grant W911NF2110103, and Office of Naval Research\nunder grant N000142212474. We thank Feng Tao, Van Ngo,\nGabriella Forbis for their helpful feedback, code, and tests.\nReferences\nAmodei, D.; Olah, C.; Steinhardt, J.; Christiano, P.; Schul-\nman, J.; and Man´e, D. 2016. Concrete problems in AI safety.\narXiv preprint arXiv:1606.06565.\nBıyık, E.; Losey, D. P.; Palan, M.; Landolfi, N. C.; Shevchuk,\nG.; and Sadigh, D. 2022. Learning reward functions from\ndiverse sources of human feedback: Optimally integrating\ndemonstrations and preferences. The International Journal\nof Robotics Research, 41(1): 45–67.\nBıyık, E.; Palan, M.; Landolfi, N. C.; Losey, D. P.; Sadigh,\nD.; et al. 2020. Asking Easy Questions: A User-Friendly Ap-\nproach to Active Reward Learning. In Conference on Robot\nLearning, 1177–1190.\nBıyık, E.; Talati, A.; and Sadigh, D. 2022. APReL: A library\nfor active preference-based reward learning algorithms. In\nACM/IEEE International Conference on Human-Robot In-\nteraction (HRI), 613–617.\nBrown, D.; Coleman, R.; Srinivasan, R.; and Niekum, S.\n2020. Safe imitation learning via fast Bayesian reward in-\nference from preferences. In International Conference on\nMachine Learning, 1165–1177.\nBrown, D.; Goo, W.; Nagarajan, P.; and Niekum, S. 2019.\nExtrapolating beyond suboptimal demonstrations via inverse\nreinforcement learning from observations. In Proceedings\nof the International Conference on Machine Learning, 783–\n792.\nBrown, D. S.; Goo, W.; and Niekum, S. 2020. Better-than-\ndemonstrator imitation learning via automatically-ranked\ndemonstrations.\nIn Conference on Robot Learning, 330–\n359.\nCelemin, C.; and Ruiz-del Solar, J. 2015. COACH: Learning\ncontinuous actions from corrective advice communicated by\nhumans.\nIn 2015 International Conference on Advanced\nRobotics (ICAR), 581–586.\nChoi, J.; and Kim, K.-E. 2011. MAP inference for Bayesian\ninverse reinforcement learning. Advances in Neural Infor-\nmation Processing Systems, 24.\nChoi, J.; and Kim, K.-E. 2012. Nonparametric Bayesian in-\nverse reinforcement learning for multiple reward functions.\nAdvances in Neural Information Processing Systems, 25.\nChristiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.;\nand Amodei, D. 2017. Deep reinforcement learning from\nhuman preferences.\nAdvances in neural information pro-\ncessing systems, 30.\nEcoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and\nClune, J. 2019.\nGo-explore: a new approach for hard-\nexploration problems. arXiv preprint arXiv:1901.10995.\nFinn, C.; Levine, S.; and Abbeel, P. 2016. Guided cost learn-\ning: Deep inverse optimal control via policy optimization.\nIn Proceedings of the International Conference on Machine\nLearning, 49–58.\nFu, J.; Luo, K.; and Levine, S. 2018. Learning Robust Re-\nwards with Adverserial Inverse Reinforcement Learning. In\nInternational Conference on Learning Representations.\nHaarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor.\nIn Proceedings of\nthe International Conference on Machine Learning, 1861–\n1870.\nIbarz, B.; Leike, J.; Pohlen, T.; Irving, G.; Legg, S.; and\nAmodei, D. 2018. Reward learning from human preferences\nand demonstrations in Atari. Advances in Neural Informa-\ntion Processing Systems, 31.\nKnox, W. B.; and Stone, P. 2009.\nInteractively shap-\ning agents via human reinforcement: The TAMER frame-\nwork. In Proceedings of the fifth international conference\non Knowledge capture, 9–16.\nLee, K.; Smith, L.; Dragan, A.; and Abbeel, P. 2021. B-Pref:\nBenchmarking Preference-Based Reinforcement Learning.\nAdvances in Neural Information Processing Systems.\nLee, K.; Smith, L. M.; and Abbeel, P. 2021.\nPEBBLE:\nFeedback-Efficient Interactive Reinforcement Learning via\nRelabeling Experience and Unsupervised Pre-training. In\nInternational Conference on Machine Learning, 6152–\n6163.\nLevine, S.; Popovic, Z.; and Koltun, V. 2011. Nonlinear in-\nverse reinforcement learning with Gaussian processes. Ad-\nvances in Neural Information Processing Systems, 24.\nLi, Y. 2019.\nReinforcement learning applications.\narXiv\npreprint arXiv:1908.06973.\nLiang, X.; Shu, K.; Lee, K.; and Abbeel, P. 2022. Reward\nUncertainty for Exploration in Preference-based Reinforce-\nment Learning. In International Conference on Learning\nRepresentation.\nLillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;\nTassa, Y.; Silver, D.; and Wierstra, D. 2015. Continuous con-\ntrol with deep reinforcement learning. International Confer-\nence on Learning Representations.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje-\nland, A. K.; Ostrovski, G.; et al. 2015. Human-level control\nthrough deep reinforcement learning.\nNature, 518(7540):\n529–533.\nNg, A. Y.; Russell, S. J.; et al. 2000. Algorithms for inverse\nreinforcement learning. In Proceedings of the International\nConference on Machine Learning, 2.\nPark, J.; Seo, Y.; Shin, J.; Lee, H.; Abbeel, P.; and Lee,\nK. 2022.\nSURF: Semi-supervised Reward Learning with\nData Augmentation for Feedback-efficient Preference-based\nReinforcement Learning.\nIn International Conference on\nLearning Representations.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the\ngame of Go with deep neural networks and tree search. Na-\nture, 529(7587): 484.\nSzot, A.; Zhang, A.; Batra, D.; Kira, Z.; and Meier, F. 2022.\nBC-IRL: Learning Generalizable Reward Functions from\nDemonstrations. In The Eleventh International Conference\non Learning Representations.\nWarnell, G.; Waytowich, N.; Lawhern, V.; and Stone, P.\n2018.\nDeep TAMER: Interactive agent shaping in high-\ndimensional state spaces. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 32.\nWirth, C.; Akrour, R.; Neumann, G.; F¨urnkranz, J.; et al.\n2017. A survey of preference-based reinforcement learning\nmethods. Journal of Machine Learning Research, 18(136):\n1–46.\nWulfmeier, M.; Ondruska, P.; and Posner, I. 2015.\nMax-\nimum entropy deep inverse reinforcement learning. arXiv\npreprint arXiv:1507.04888.\nXu, Y.; Wang, R.; Yang, L.; Singh, A.; and Dubrawski, A.\n2020. Preference-based reinforcement learning with finite-\ntime guarantees. Advances in Neural Information Process-\ning Systems, 33: 18784–18794.\nZhan, H.; Tao, F.; and Cao, Y. 2021. Human-guided Robot\nBehavior Learning: A GAN-assisted Preference-based Re-\ninforcement Learning Approach. IEEE Robotics and Au-\ntomation Letters, 6(2): 3545–3552.\nZiebart, B. D.; Maas, A.; Bagnell, J. A.; and Dey, A. K.\n2008.\nMaximum entropy inverse reinforcement learning.\nProceedings of the AAAI Conference on Artificial Intelli-\ngence.\nRbRL Implementation Details\nAll tests and experiments of our RbRL and PbRL algorithms were trained for 4 million time-steps with a segment length of\n50. For the Walker environment, 1,000 synthetic labels were provided with the reward predictor being updated every 20,000\ntime-steps. For the Quadruped environment, 2,000 synthetic labels were provided with the reward predictor being updated every\n30,000 time-steps. All other hyper-parameters are the same as those in Lee et al. (2021).\nRaw Data for Agent Performance from Individual Participants in the User Study\nFigure 7 displays the individual reward curves for non-expert users who participated in the human user study, for the Cheetah\nand Swimmer tasks.\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n0\n20\n40\n60\n80\n100\n120\nMean Reward\n(a) Individual results for Cheetah (PbRL)\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n20\n0\n20\n40\n60\n80\n100\n120\n140\nMean Reward\n(b) Individual results for Cheetah (RbRL with n = 2)\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n10\n5\n0\n5\n10\n15\nMean Reward\n(c) Individual results for Swimmer (PbRL)\n0\n800k\n1.6e6\n2.4e6\n3.2e6\n4e6\nTimesteps\n5\n0\n5\n10\n15\n20\nMean Reward\n(d) Individual results for Swimmer (RbRL with n = 2)\nFigure 7: Results of individual runs in the human user study for the Cheetah and Swimmer environments (7 users in each case).\nModeling Rating Class Probabilities\nFigure 8 provides an intuitive illustration of the modeled class probabilities Qσ(i), as calculated via Equation (3), with specific\nrelevant parameter choices detailed in the figure caption.\nR0 = 0\nR1 = 0.2\nR2 = 0.4\nR3 = 0.6\nR4 = 0.8\nR5 = 1\nNormalized reward R( )\n0.0\n0.2\n0.4\n0.6\n0.8\nClass probability Q (i)\nClass i = 0\nClass i = 1\nClass i = 2\nClass i = 3\nClass i = 4\nFigure 8: RbRL’s modeled class probabilities Qσ(i). This figure illustrates Equation (3), evaluated with n = 5 classes, k =\n30, and the rating category separation parameters hard-coded at {0, 0.2, 0.4, 0.6, 0.8, 1}. We can see that the probability of\nbelonging to a class i is maximized when the normalized return ˜R(σ) is halfway between ¯Ri and ¯Ri+1, and decreases as ˜R(σ)\nmoves further from this point.\nQuestionnaire\nThe three-part questionnaire given to the user study participants appears next.\nUser Study Questionnaire \n \n \n \n \nInstructions: Mark the bin best reflecting your response when instructed to do so. \n \n \n \nRating-based feedback: \n \n1. How mentally demanding was the task? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n2. How successful were you in accomplishing what you were asked to do? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n3. How hard did you have to work to accomplish your level of performance? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n4. How insecure, discouraged, irritated, stressed, and annoyed were you? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n \n \n \n \nUser Study Questionnaire \n \n \n \n \nInstructions: Mark the bin best reflecting your response when instructed to do so. \n \n \n \nPreference-based feedback: \n \n1. How mentally demanding was the task? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n2. How successful were you in accomplishing what you were asked to do? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n3. How hard did you have to work to accomplish your level of performance? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n4. How insecure, discouraged, irritated, stressed, and annoyed were you? \n \n \n                                    Very low                         Neutral                      Very high \n \n \n \n \n \n \nUser Study Questionnaire \n \n \nInstructions: Mark the bin best reflecting your response when instructed to do so. \n \n \nComparison of preference-based and ratings-based feedback: \n \n1. Which was more mentally demanding? \n \n \n                                    Ratings                           Neutral                      Preferences  \n \n \n2. With which were you able to better accomplish the task? \n \n \n                                    Ratings                           Neutral                      Preferences  \n \n \n3. With which was it more difficult to accomplish the task? \n \n \n                                    Ratings                           Neutral                      Preferences  \n \n \n4. Which caused you to be more insecure, discouraged, irritated, stressed, and annoyed? \n \n \n                                    Ratings                           Neutral                      Preferences  \n \n \n5. Which did you like more? \n \n \n                                    Ratings                          Neutral                      Preferences  \n \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2023-07-30",
  "updated": "2024-01-29"
}