{
  "id": "http://arxiv.org/abs/2308.01649v1",
  "title": "MARLIM: Multi-Agent Reinforcement Learning for Inventory Management",
  "authors": [
    "Rémi Leluc",
    "Elie Kadoche",
    "Antoine Bertoncello",
    "Sébastien Gourvénec"
  ],
  "abstract": "Maintaining a balance between the supply and demand of products by optimizing\nreplenishment decisions is one of the most important challenges in the supply\nchain industry. This paper presents a novel reinforcement learning framework\ncalled MARLIM, to address the inventory management problem for a single-echelon\nmulti-products supply chain with stochastic demands and lead-times. Within this\ncontext, controllers are developed through single or multiple agents in a\ncooperative setting. Numerical experiments on real data demonstrate the\nbenefits of reinforcement learning methods over traditional baselines.",
  "text": "MARLIM: Multi-Agent Reinforcement Learning\nfor Inventory Management\nR´emi Leluc∗\nCMAP, ´Ecole Polytechnique\nTotalEnergies OneTech\nElie Kadoche†\nTotalEnergies OneTech\nAntoine Bertoncello†\nTotalEnergies OneTech\nS´ebastien Gourv´enec†\nTotalEnergies OneTech\nAbstract\nMaintaining a balance between the supply and demand of products by optimizing\nreplenishment decisions is one of the most important challenges in the supply chain\nindustry. This paper presents a novel reinforcement learning framework called\nMARLIM, to address the inventory management problem for a single-echelon\nmulti-products supply chain with stochastic demands and lead-times. Within\nthis context, controllers are developed through single or multiple agents in a\ncooperative setting. Numerical experiments on real data demonstrate the benefits\nof reinforcement learning methods over traditional baselines.\n1\nIntroduction\nInventory control [49] is one of the major problems in the supply chain industry. The main goal is to\nensure the right balance between the supply and demand of products by optimizing replenishment\ndecisions. More precisely, a controller observes the past demands and local information of the\ninventory and has to decide about the next ordering values. Accurate inventory management is key\nto running a successful product business with benefits ranging from better inventory accuracy and\ninsights to cost savings and avoidance of shortage and stock overflows.\nThe main issue of the supply chain is the environment uncertainty [64]. In an ideal world with\ndeterministic demand and lead-times, an inventory controller would be able to place a perfect order\nequal to the demand size at the right time. However, in practice, both the demands and lead-times are\nstochastic with potentially high volatility, making the inventory management problem hard to solve.\nIn most cases, the inventory controller may exceedingly or insufficiently order. The former case leads\nto paying unnecessary ordering and holding costs while the latter results in shortage costs which may\njeopardize the company’s performance.\nClassical methods for solving the inventory management problem rely on basic heuristics [55, 63] due\nto the complexity of the mathematical modeling of the inventory system. While these approaches are\neasy to implement, they are not able to capture the randomness of the demand and lead-times. Another\nway of solving the inventory management problem is dynamic programming [5]. Despite being\nefficient, this technique requires exact knowledge of the mathematical model of the inventory system,\nwhich becomes intractable for big companies with very large inventories. Inventory management\nmodels can quickly become too complex and time-consuming, leading to unworkable models [22].\n∗remi.leluc@gmail.com, Work done while being at TotalEnergies OneTech\n†{first.last}@totalenergies.com, TotalEnergies OneTech, 2 Place Jean Millier, 92400 Courbevoie, France\nReinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th Conference on Neural Information\nProcessing Systems (NeurIPS 2022).\narXiv:2308.01649v1  [cs.LG]  3 Aug 2023\nTo escape this curse of dimensionality, one may apply approximate dynamic programming [17, 23]\nwhich performs well in specific settings at the cost of strong assumptions and simplifications.\nSince the environment uncertainty is the major problem of inventory control [12], reinforcement\nlearning (RL) methods appear as a natural solution since they can model complex situations and\ngeneralize well in a data-driven manner. The reinforcement learning task [53] consists, for an\nautonomous agent (e.g., a robot), in learning the actions to be taken, from trials, in order to optimize\na quantitative signal over time. Such paradigm has reached tremendous success in games [35, 48,\n56], but the early applications of reinforcement learning to real-world tasks, e.g., robotics [29] or\nautonomous driving [37, 45] remain a challenge and an active field of research. Furthermore, most\nof the literature on inventory management systems is meant to be applied for companies which are\nspecialized in the retail industry where (i) the items of the inventory are meant to be sold and (ii)\nitem shortages may be addressed through back orders. However, many companies are interested in\ninventory management for factories and warehouses where the objective is to ensure the performance\nof a production line and avoid the drastic consequences of item shortages.\nThe main goal of this paper is to develop a novel reinforcement learning framework, called MARLIM,\nto address the inventory management problem for a single-echelon multi-products supply chain on a\nproduction line with stochastic demands and lead-times.\nRelated work. The stochastic inventory control problem is one of the most studied problems in\ninventory theory. It was initiated by the seminal works of [20] and [49] which presented the basic\nheuristics to solve the inventory management problem. The so-called (s,S) policy [63], which results\nfrom the economies of scale in procurement, is a widely used policy nowadays in practice. However\nsuch heuristic cannot capture the randomness of the demand and lead-times, making it difficult to\napply to realistic use cases. Later on, several authors developed the theory of inventory management\nand one may refer to the books of [40, 64] and [32] for an in-depth coverage of the topic. The\ndifferent state-of-the-art models for inventory management systems may be found in the recent survey\nof [12].\nIn the past few years, many comprehensive studies related to the integration of reinforcement learning\ntechniques to the inventory management problem were presented, starting with the work of [21]\nwhich considered production and distribution functions of global supply chain with multiple stages\nassuming a single item. Later on, different variations were derived with a particular focus on a supply\nchain with two echelons [28, 31], multiple echelons [11, 27, 30, 52] or multiple retailers [16, 25]. In\nall these studies, reinforcement learning methods have been implemented to specify near-optimal\nordering policies in the entire supply chain with different goals such as maximizing the profit (when\nconsidering retailers) or minimizing costs composed of either ordering and holding costs or holding\nand backorder costs but not all of them at the same time. Furthermore, all the mentioned studies are\ndealing with a simplified assumption of independent products with no interactions among them.\nIn order to handle multiple items and accurately model the inter-dependency between them, the\nproposed framework of this paper is based on the multi-agent reinforcement learning paradigm [36]\nwhere each item may be seen as a single agent. Such framework seems promising to improve the\ntraining efficiency as suggested by the recent survey of [61] and the two recent studies of multi-agent\nreinforcement learning applied to inventory management problems [2, 51]. In [2], the authors derive\na reinforcement strategy for a grocery retailer with multiple items and fixed delivery times where\nthe focus is on the items availability rather than their associated costs. Similarly the study of [51]\ndeals with a multiple echelons supply chain with predefined lead times whose aim is to maximise\nproduct sales and minimise wastage of perishable products. Conversely, the developed method of this\npaper is focused on the reduction of operating costs on a production line with stochastic demands and\nlead-times for real-world applications.\nThe framework of MARLIM is also related to the joint replenishment problem, i.e., when one\nconsiders the interdependency among different groups of products in a same order provided by a\nsingle supplier [26, 44, 57]. The objective is to optimize a global replenishment cost composed of\ninventory ordering and holding costs [42, 58] but does not take into account the losses induced by\nitem shortages. In contrast to these previous studies, the aim of the proposed method whose is not\nonly to reduce the global replenishment cost but also to avoid stock-outs of items.\n2\nContributions. The main contributions of this paper may be summarized as follows.\n• A novel reinforcement learning framework, called MARLIM, is developed to address the inventory\nmanagement problem for a single-echelon multi-products supply chain on a production line with\nstochastic demands and lead-times.\n• A methodology to train agents in different scenarios for fixed or shared capacity constraints with\nspecific handling of storage overflows is provided.\n• Various numerical experiments on real-world data demonstrate the benefits of the developed method\nover classical baselines.\nOutline. Section 2 presents the mathematical background of reinforcement learning and Section 3\ndeals with the inventory management model. The methodology and details of the developed supply\nchain environment are described in Section 4. Numerical experiments are performed in Section 5 to\nhighlight the relevance of the developed model on real-world data and Section 6 concludes the article\nwith further discussion.\n2\nPreliminaries on Reinforcement Learning\nMarkovian setting. Markov Decision Processes (MDP) [41] are a formalization of sequential\ndecision making, where actions influence not just immediate rewards, but also subsequent situations.\nConsider the classical framework of a MDP defined as a tuple M = ⟨S, A, p, r, γ, µ⟩, comprised of\na state space S, an action space A, a Markovian transition kernel p : S × A →∆(S) where ∆(S)\ndenotes the set of probability density functions over S, a reward function r : S × A →R, a discount\nfactor γ ∈(0, 1) and an initial-state distribution µ ∈∆(S). The reinforcement learning problem\nconsists of finding the best strategy or policy π : S →A in order to maximize the performance. The\ncriterion here is defined in terms of future rewards that can be expected and depend on the agent’s\nbehavior. The solution of a MDP is an optimal policy π∗that maximizes the value function V π in all\nthe states s ∈S over some policy set of interest Π: π⋆∈argmaxπ∈Π V π. Such optimal policy π⋆\nis guaranteed to exist thanks to the theorem of [6]. In practice, π⋆can be found through dynamic\nprogramming and Bellman equations [7, 9, 24] with different schemes, e.g., policy iteration [8, 10]\nor value iteration [39, 50]. Another way is to consider policy-gradient methods [4, 60] which use\na parameterized policy πθ with θ ∈Θ ⊂Rd and update the policy parameter θ on each step in the\ndirection of an estimate of the gradient of the performance with respect to the policy parameter.\nParametric policies. Given a parameter space Θ ⊆Rd, consider a class of smoothly parameterized\nstochastic policies ΠΘ = {πθ, θ ∈Θ} that are twice differentiable w.r.t. θ, i.e., for which the gradient\n∇θπ and the Hessian ∇2\nθπθ are defined everywhere and finite. When the action space is finite, a\npopular choice is to use Gibbs policies, a.k.a. softmax policies, for all s ∈S and a ∈A,\nπθ(a|s) =\nexp(θT ψ(s, a))\nP\na′∈A exp(θT ψ(s, a′))\n(1)\nwhere ψ : S × A →Rd is an appropriate feature-extraction function, often computed using a neural\nnetwork. When the action space is continuous S ⊂Rd, a popular choice is to use Gaussian policies\nso the policy can be defined as the normal probability density over a real-valued scalar action, with\nsome parametric mean µ : Θ × S →R and standard deviation σ : Θ × S →R+ that depend on the\nstate, for all s ∈S and a ∈A,\nπθ(a|s) =\n1\nσθ(s)\n√\n2π exp\n\u0012\n−(a −µθ(s))2\n2σθ(s)2\n\u0013\n.\n(2)\nMulti-Agent Reinforcement Learning (MARL). Originated from the seminal works of [47] and\n[34], Markov Games are the standard generalization of Markov Decision Processes as they capture the\ninteraction of multiple agents. A Markov game is defined as a tuple ⟨N, S, (Ai)i∈N , p, (ri)i∈N , γ⟩\nwhere N denotes the set of agents, S is the state space observed by all agents, Ai is the action\nspace of agent i. A = Q\ni∈N Ai denotes the joint action space, p : S × A →∆(S) is the tran-\nsition kernel, ri : S × A →R is the immediate reward function of agent i and γ ∈(0, 1) is\nthe discount factor. At time step t, each agent i ∈N selects an action a(i)\nt\nbased on the sys-\ntem state st. The system then transitions to state st+1 according to p and rewards each agent\ni with ri(st, at). The goal of agent i is to optimize its own long-term reward by finding the\n3\npolicy π(i) : S →∆(Ai) such that a(i)\nt\n∼π(i)(·|st).\nTherefore, the marginal value func-\ntion V (i) : S →R of agent i becomes a function of the joint policy π : S →A defined by\nπ(a|s) = Q\ni∈N π(i)(a(i)|s), V (i)(s) = E\nhP\nt≥0 γtri(st, at)\n\f\f\fa(i)\nt\n∼π(i)(·|st), s0 = s\ni\n. Thus, the\nsolution concept of a Markov game deviates from that of a MDP since the optimal performance of\neach agent is controlled not only by its own policy, but also the choices of all other players of the\ngame. The most common solution concept for Markov games is Nash equilibrium [3, 18].\nAs a standard learning goal for MARL, Nash equilibrium always exists for finite-space infinite-\nhorizon discounted Markov games [18], but may not be unique in general. Most of the multi-agent\nreinforcement learning algorithms are contrived to converge to such an equilibrium point, if it exists.\n3\nInventory Management Model\nDenote by N = {1, . . . , n} the discrete product space where i ∈N refers to the ith item in the\ninventory and n is the total number of items. At each time step, the inventory controller decides\nabout the order to take based on the current inventory level and the previous demands. This order\narrives in the inventory after a stochastic lead-time. After receiving the replenishment quantity of\ndifferent products, the demands are satisfied and the environment incurs inventory costs. The aim of\nthis Section is to describe the complete inventory management model from the inventory features and\ninventory costs to the inventory dynamics of the system.\n3.1\nInventory features\nThe different inventory features are the basis\nof the inventory management model as they\nmodel the state space and the dynamics of\nthe underlying Markov game. The inventory\nlevel xt ∈Rn is the on-hand inventory at\ntime t, i.e., the quantity of products sitting on\nthe shelf in the inventory. At each time step,\nthe inventory controller receives a stochas-\ntic demand signal δt ∈Rn and can order a\nquantity at ∈Rn of items. These quantities\narrive in the inventory after some lead-time\nτt ∈Rn. The inventory replenishment quan-\ntity ρt ∈Rn is the associated quantity of\nproducts which is on its way to the inventory,\ni.e., it is the quantity that will be added to the\ninventory level when the inventory check is\ndone at the next time step.\nSymbol\nFeature\nΛ(k)\nmaximum capacity of subspace k\nx(i)\nt\ninventory level of product i at time t\na(i)\nt\norder of product i at time t\nρ(i)\nt\nreplenishment of product i at time t\nλ(i)\nt\ntemporary level of product i at time t\nw(i)\nt\noverflow weight of product i at time t\nΩt,k\nstock overflow of subspace k at time t\nδ(i)\nt\ndemand of product i at time t\nτ (i)\nt\nlead-time of product i at time t\nβ(i)\nt\nbacklog of product i at time t\nC(i)\no\nOrdering cost of product i\nC(i)\nh\nHolding cost of product i\nC(i)\ns\nShortage cost of product i\nWhen receiving the replenishment quantity, the inventory levels are temporarily updated through\nλt ∈Rn and some storage overflow may happen. When this overflow event happens, denoted by Ωt,\nthe replenishment quantities are scaled using weights wt ∈Rn to ensure that the total inventory level\ndoes not exceed the maximum storage capacity Λ ∈N+. Finally, the demands are satisfied and the\ncontroller updates the inventory levels xt+1. When demand exceeds the available inventory level for\nan item, it yields a shortage cost computed through a backlog level βt+1 ∈Rn. When optimizing the\ndecisions relative to the inventory, the controller must take into account the different inventory costs.\n3.2\nInventory costs\nThe inventory costs may be classified into three categories [55]: ordering, holding and shortage.\n(i) Ordering costs: in the case of an ordering setting, these costs include the functioning costs,\nreception and tests costs, the salaries of the personnel, information systems costs and customs\ncosts. In the case of a production setting, these costs include the raw material costs, labor costs,\nfixed and variable overheads, e.g., rent of a factory or the energy consumption allocated for production.\n4\n(ii) Holding costs: these are associated with storing inventory that remains unused and may be divided\ninto two categories: financial and functional costs. The former represent the financial interest of the\nmoney invested in procuring the stocked products. The latter include the rent and maintenance of the\nrequired space, insurance costs, equipment costs, inter-warehouses transportation and obsolescence\ncosts.\n(iii) Shortage costs: when demand exceeds the available inventory for an item. The related costs fall\nin one of the two following categories: lost sales costs and backlogging costs. When considering lost\nsales, the unsatisfied demands are completely lost whereas with backlogging costs, there is a penalty\nshortage cost. Note that in the case where the stock is internal, the inventory shortage will induce the\nstop of production of therefore all the consequent costs.\nFor any item i ∈N, denote by C(i)\no , C(i)\nh\nand C(i)\ns\nthe unit ordering, holding and shortage costs\nrespectively. The different features of the inventory management system are summarized in the Table\nabove.\nRemark 1 (Costs and priority) The different unit costs are the keystone to accurately model different\nbehaviors among the items. Intuitively, items with high shortage cost correspond to critical items\nthat should not run out-of-stock. On the contrary, items with high ordering cost should be ordered\nsparingly. In general, an optimal controller should consider some reward function which leverages\nthese three factors to find the right balance between the different costs.\n3.3\nInventory Dynamics\nThe different relations between the inventory features are the mainstay to model a Markov game. First\nof all, there is a capacity constraint on the storage space and several structures are to be considered.\nOn the one hand, a natural assumption is to consider that each item i ∈N has a maximum capacity\nΛ(i) ∈N⋆. In practice, thanks to expert’s knowledge, an estimate of the mean demand of each item\nmay be available so that the capacity of each product can be upper bounded. On the other hand, for\neconomical reasons, the total storage space may be shared among all items. This model allows more\nflexibility as products can compete for storage space but it assumes that all items belong to a same\ncategory. A unified and more realistic setting is the following: the different products may be classified\ninto different types and they only compete for storage space inside their category (see Remark 2). In\nother words, the product space N can be decomposed into K different clusters of products, i.e.,\nN =\nK\n[\nk=1\nNk.\nEach product subspace Nk has a maximum storage capacity Λ(k) ∈N⋆. For the sake of clarity, all\nitems are assumed to have the same volume (see Remark 3). At each time step t ≥0, the available\nstorage space of each product subspace should be non-negative, i.e., for all k = 1, . . . , K\n∆(k)\nt\n= Λ(k) −\n X\ni∈Nk\nx(i)\nt\n!\n≥0.\n(3)\nThe replenishment quantity ρt depends on the previous orders a1, . . . , at and their associated lead-\ntimes. Indeed, the received quantity of product i at time t corresponds to all the previous orders a(i)\nj\nmade at time step j such that the receiving time (j + τj) obtained by adding the associated lead-time\nmatches the current time step t, i.e., for i = 1, . . . , n,\nρ(i)\nt\n=\nt\nX\nj=1\na(i)\nj 1{j+τ (i)\nj\n=t}.\nWhen receiving these replenishment quantities at time t, the inventory levels are temporarily updated\nas λ(i)\nt\n= x(i)\nt\n+ ρ(i)\nt\nand it may happen that the storage space of subspace Nk overflows which\ntranslates into the following event\nΩt,k =\n( X\ni∈Nk\nλ(i)\nt\n> Λ(k)\n)\n.\n5\nIn virtue of Eq.(3), on Ωt,k it holds P\ni∈Nk ρ(i)\nt\n> ∆(k)\nt\n. This means that when the stock overflow\nhappens in Nk then the received replenishment quantities of products in Nk exceed the available\nstorage space of Nk. In that case, the replenishment quantities are weighted to completely fill the\navailable storage space, i.e., some overflow weights w(i)\nt\nare chosen such that P\ni∈Nk w(i)\nt ρ(i)\nt\n= ∆(k)\nt\n.\nIn order to promote the replenishment of critical products (see Remark 4), the overflow weights are\nset to be proportional to the shortage cost of items. For all k = 1, . . . , K and i ∈Nk,\nw(i)\nt\n= 1Ωt,k\n \n∆(k)\nt\nC(i)\ns\nP\nj∈Nk C(j)\ns ρ(j)\nt\n!\n+ 1Ωc\nt,k.\n(4)\nSince the quantities of products are\nnon-negative integers, only the inte-\nger part of the weighted replenish-\nment quantities wtρt are added to the\ncurrent inventory levels xt. When\nthe demands exceed the available on-\nhand inventory levels, a backlog is\ncomputed to monitor the unmet de-\nmands and measure the associated\nshortage costs.\nItem i, time t\nlevel x(i)\nt\ntake order\naction a(i)\nt\nupdate\norders list\nreceive\nquantity ρ(i)\nt\noverflow\nweights w(i)\nt\n(x(i)\nt+1, β(i)\nt+1)\nlead-time\nτ (i)\nt\ncompute\nupdate\nt ←−t + 1\nFigure 1: Inventory Dynamics\nFor each item i ∈N, the inventory level and backlog are updated as\nx(i)\nt+1 =\n\u0010\nx(i)\nt\n+ ⌊w(i)\nt ρ(i)\nt ⌋−δ(i)\nt\n\u0011\n+ ,\nβ(i)\nt+1 = β(i)\nt\n+\n\u0010\nx(i)\nt\n+ ⌊w(i)\nt ρ(i)\nt ⌋−δ(i)\nt\n\u0011\n−\nwhere (·)+ = max(·, 0) and (·)−= max(−·, 0) denote the positive and negative parts respectively.\nThe inventory dynamics is summarized in Figure 1 above.\nRemark 2 (Product clusters) In practice, the different items of a warehouse are naturally classified\ninto several groups according to their storage conditions: size, weight and specific climatic conditions,\ne.g., temperature, pressure, humidity levels, ventilation and light.\nRemark 3 (Storage capacity) In the case where the total storage space is shared among all items\n(K = 1), the capacity constraint is fulfilled as soon as xT\nt 1n ≤Λ where Λ is the maximum capacity\nof the warehouse. To model different volumes of items, one can simply consider a vector of unit\nvolumes v ∈Rn where v(i) is the unit volume of product i and ensures that xT\nt v ≤Λ for all t ≥0.\nSimilarly, when dealing with K types of products, the capacity constraint of each subspace Nk\nbecomes P\ni∈Nk x(i)\nt v(i) ≤Λ(k) for all t ≥0.\nRemark 4 (Storage overflow) Different options are available to address the storage overflow of a\nproduct subspace Nk. If all items in Nk have equal priority levels, one could think of a uniform split\nof the available storage space of the form ∆(k)\nt\n/|Nk|. However, in practice, some items are more\nimportant than others and may even be critical for production. This is why, when the storage space\noverflows, the replenishment quantities should be weighted to favor the replenishment of products\nnon only with low inventory levels but also the ones associated to large shortage costs (see Eq.(4))\n4\nMARL for Inventory Management\nRecall that the goal of any MARL algorithm is to find a policy πθ maximizing the expected discounted\nreward V (πθ) = E\nhP\nt≥0 γtr(st, at)|at ∼πθ\ni\n. The goal of this Section is to derive the necessary\nformalism, namely the Markov games and associated rewards, in order to apply such MARL methods.\nMarkov games. The inventory management problem can be modeled as K different Markov games.\nConsider a fixed product subspace Nk with 1 ≤k ≤K. At time step t ≥0, the state of agent\ni ∈Nk is comprised of: the inventory level x(i)\nt , the replenishment quantity ρ(i)\nt , the lead time τ (i)\nt\nand backlogs β(i)\nt , i.e., s(i)\nt\n= (x(i)\nt , ρ(i)\nt , τ (i)\nt , β(i)\nt ) ∈Si\nk. The action of each agent concerns the\n6\nordering quantity a(i)\nt\n∈Ai\nk = J0, Λ(k)K. The joint state space and action space are respectively\ngiven by Sk = Q\ni∈Nk Si\nk and Ak = Q\ni∈Nk Ai\nk. The transition kernel is implicitly defined by all the\ninventory dynamics equations of Section 3.3.\nCosts and rewards. According to Section 3.2, at each time step t ≥0 and for a given product\nsubspace Nk, each agent i ∈Nk incurs some inventory costs C(i)\nt\ngiven by the combination of\nordering, holding and shortage costs. In other words, the overall inventory cost may be described as\nC(i)\nt\n= αo a(i)\nt C(i)\no\n| {z }\nordering\n+αh x(i)\nt C(i)\nh\n| {z }\nholding\n+αs β(i)\nt+1C(i)\ns\n|\n{z\n}\nshortage\nwhere αo, αh, αs ∈[0, 1] with αo + αh + αs = 1 are weighting coefficients that translate some\nexpert’s knowledge about the desired strategy. This expression translates the trade-off between the\ndifferent inventory costs. Hopefully, a trained agent will learn to order low quantities of products that\nare costly to stock and supply, while maintaining sufficient inventory levels to avoid stock-outs for\ncritical products associated to large shortage costs. This is validated in the numerical experiments of\nSection 5.\nThe inventory costs of each agent are associated to the single reward defined as: ri(st, at) = −Ci\nt.\nObserve that inside a product subspace Nk, the agents are working in a cooperative setting in\norder to optimize the average reward corresponding to rk(st, at) = P\ni∈Nk ri(st, at)/|Nk|. This\naverage reward model allows more heterogeneity among agents and facilitates the development\nof decentralized MARL algorithms [15, 62]. More subtly, the reward function is sometimes not\nsharable with others, as the preference may be private to each agent. Note that this setting finds broad\napplications in engineering systems as sensor networks [43], smart grid [14], intelligent transportation\nsystems [1], and robotics [13].\n5\nNumerical Experiments\nThis Section is dedicated to numerical experiments on real data. Various scenarios regarding the\ncapacity constraints of the warehouses are considered: (i) when the items in a product cluster have\ntheir own capacity constraints then it is enough to train a single RL agent and apply the resulting\nbehavior to all the items in that cluster; (ii) when the items compete for storage space then one may\napply some MARL algorithm to deal with the interdependency of items.\nStochastic Model. The demands and lead-times are assumed to be stochastic with stationary\ndistributions. The lead-times follow geometric distributions and the demands follow a mixed law\nof Poisson process with zero plateaus. More precisely, for each item i, the lead-time is given by\nτ (i) ∼G(pi) with parameter pi ∈(0, 1) and the demand distribution is given by\nδ(i) =\n\u001a\nP(µi)\nwith probability bi\n0\nwith probability 1 −bi\nIn other words, the demand process of item i is δ(i) ∼XiYi with Xi ∼B(bi) and Yi ∼P(µi).\nReal data. The data comes from a company with warehouses in different countries. The data is\nused to find the model parameters of the stochastic distributions using some maximum likelihood\nestimators [19, 38, 59]. For a fixed item i ∈N with historical data of demands (δ(i)\nt )t=1,...,Ni and\nlead-times (τ (i)\nt )t=1,...,Ti, the MLE estimators ˆbi, ˆµi and ˆpi are given by the empirical averages\nˆbi = 1\nNi\nNi\nX\nt=1\n1{δ(i)\nt\n>0},\nˆµi =\nPNi\nt=1 δ(i)\nt 1{δ(i)\nt\n>0}\nPNi\nt=1 1{δ(i)\nt\n>0}\n,\nˆpi =\nTi\nPTi\nt=1 τ (i)\nt\n.\nFor the case study, the total number of products is equal to n = 50 with time steps equal to months.\nThe history is ranging from 2002 to 2021 on different warehouses leading to a maximum number of\nhistorical data points equal to Nmax = 420. The horizon is set to T = 240 months and the methods\nare tested over 100 replications (independent random seeds). The different parameter values of each\nitem are summarized in Tables 8-12 in Appendix B.\nAgents battle. In order to evaluate the performance of the developed approach, different baselines\nare implemented along with the reinforcement learning methods.\n7\nFigure 2: Inventory level (blue) of an item (ID=21) over T = 120 months with different agents:\nMinMax (left), Oracle (center), PPO (right). The demand is plotted in red and the order actions are\nplotted in green. The safety stock MinMax agent is displayed in orange.\n• MinMax agents: a standard min-max strategy (s, S) from operation research. For these agents,\neach item has a safety stock κ = Φ−1(α)\np\n(µτσ2\nδ) + (µδστ)2, where Φ is the c.d.f. of N(0, 1),\nα ∈{0.90, 0.95, 0.99} represents the target service level and µ, σ2 are the mean and variance of the\ndemands and lead-times. The value of the service level is set to the classical value α = 0.90 and the\nmeans and variances are computed using the MLE estimators. As soon as the inventory level goes\nbelow the safety stock, the controller orders the maximum capacity of the corresponding item. Note\nthat such approach is easy to implement but may fail to anticipate spikes in the demand signal.\n• Oracle agents: such agents implement the following heuristic: given the knowledge of the mean µδ\nand variance σ2\nδ of the demand signal, it is natural to order, at each timestep, according to a random\nlaw with mean µδ and variance σ2\nδ. More precisely, the oracle agents have access to the estimated\nmean ˆµδ and standard deviation ˆσδ of their associated products and order at each time according to a\nnormal law N(ˆµδ, ˆσ2\nδ) which is clamped to fit the bounds of the action space.\n• MARL agents: the reinforcement learning agents are trained using Proximal Policy Optimization\n(PPO) algorithms [46] which are stable and effective policy gradient methods. When working with\ncapacity constraints per item, both discrete (Eq. (1)) and continuous (Eq.(2)) policies are considered,\ndenoted by PPO-D and PPO-C respectively. When the items compete for storage space, IPPO [54]\nwhich is an independent version of PPO for multi-agent frameworks is implemented. The different\nparameter configurations are given in Appendix A.\nResults. In the idea that similar items among a cluster share some intrinsic features, the question of\ngeneralization for a single agent is raised. For that matter, first consider a scenario with a cluster of\n5 items with their own capacity constraints and a single RL agent trained on an average item, i.e.,\na ”virtual” item whose features are given by taking the mean of the features of all the items in that\ncluster is trained. Then the learned behavior on the 5 items of the cluster is tested and the different\ncumulative costs are reported. Table 1 below presents the average cumulative costs in $ obtained over\n100 replications of the different methods for the 5 items over an horizon of T = 240 months.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n0\n48,554,986\n10,183,088\n4,863,202\n4,616,016\n1\n52,993,931\n16,917,389\n6,865,991\n6,385,378\n2\n70,467,282\n21,426,806\n8,727,215\n8,087,258\n3\n72,220,832\n12,722,887\n9,854,047\n5,280,345\n4\n79,235,272\n16,976,630\n8,801,628\n4,808,191\nTable 1: Average Cumulative cost in $ over 100 replications for different items over T = 240 months.\nID\n0\n1\n2\n3\n4\nMinMax\n39\n41\n42\n51\n49\nOracle\n10\n11\n13\n8\n10\nPPO-D\n0\n0\n0\n0\n0\nPPO-C\n0\n0\n0\n0\n0\nTable 2: Average Item Shortage over 100 replications for different items over T = 240 months.\n8\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTotal Timesteps (in millions)\n1e6\n225\n200\n175\n150\n125\n100\n75\n50\nAverage Reward\nAverage reward cluster \n1 with 5 agents\nIPPO\nOracle\nMinMax\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTotal Timesteps (in millions)\n1e6\n250\n225\n200\n175\n150\n125\n100\n75\n50\nAverage Reward\nAverage reward cluster \n2 with 10 agents\nIPPO\nOracle\nMinMax\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTotal Timesteps (in millions)\n1e6\n500\n400\n300\n200\n100\nAverage Reward\nAverage reward cluster \n3 with 20 agents\nIPPO\nOracle\nMinMax\nFigure 3: Learning curves for the different clusters N1, N2 and N3 where the mean and standard\ndeviation of IPPO are plotted in blue and the horizontal lines represent the average reward for the two\nbaselines Oracle (green) and MinMax (red) computed over 100 replications.\nRegarding the average cumulative costs, the clear winners are the RL-based methods. Indeed, the\nPPO methods are statistically better than the two other baselines with a cost reduction factor ranging\nfrom 8 (for item ID=1) to 16 (for item ID=4) compared to the standard (s, S)-strategy. Interestingly,\namong the PPO methods, the one with continuous actions presents the best performance. Another\nimportant result concerns the number of items shortages as shown by Table 2 above. Once again, the\nRL-based methods outperform the two standard baselines. Furthermore, the PPO agents learned an\noptimal strategy in terms of stock-outs since the number of item shortages is always equal to zero.\nSuch careful behavior is confirmed by Figure 2 where the evolution of the inventory levels is plotted,\nover an horizon of T = 120 months, for a particular item according to the different strategies. Note\nthat compared to the MinMax and Oracle agents, the inventory levels of the PPO agent are always\nabove the demand signal which ensure the avoidance of stock-outs. Interestingly, the RL agents have\nthe nice following interpretation: whenever there is either a demand spike or a demand plateau for\na long time, there is an incentive to order. Additional numerical results concerning all 50 different\nitems are available in Appendices C and D for the average cumulative costs and the average item\nshortages respectively.\nAnother scenario involving storage capacity constraint is considered with three different clusters\nN1, N2 and N3 composed of 5, 10 and 20 items respectively. The last cluster represents a more\ncomplex task as it involves many agents. As before, the different methods based on the average\ncumulative cost over a cluster and the average item shortages over a cluster are compared, both over\nan horizon of T = 240 months. For each cluster, one may take a look at the different training curves\nof the MARL agents. Such training is repeated on 10 replications and the results are displayed in\nFigure 3 which gathers the evolution of the normalized average reward along the training episodes.\nThe means and standard deviations of the average reward of the MARL agents are plotted in blue.\nFor ease of comparison, two horizontal lines corresponding to the normalized average reward of\nthe MinMax and Oracle agents are added. Thus, it allows to check the performance gain obtained\nwith the RL-based methods compared to standard baselines. Observe that for clusters N1 and N2,\nthe MARL agents quickly outperform the baselines in approximately 500, 000 timesteps but need\nmore than 1M timesteps on cluster N3 to surpass the Oracle agents. Concerning the test performance\nof the learned behaviors, the different Tables below show the average cumulative cost and average\nstock-outs over the different items, obtained over 100 replications and an horizon of T = 240 months.\nCluster\nN1\nN2\nN3\nMinMax\n49,219,195\n55,148,027\n63,319,299\nOracle\n29,596,024\n31,047,144\n38,002,027\nIPPO-C\n12,048,666\n7,905,165\n14,239,567\nTable 3: Average Cumulative cost in $ over differ-\nent clusters, computed over 100 replications for an\nhorizon of T = 240 months.\nCluster\nN1\nN2\nN3\nMinMax\n18\n18\n23\nOracle\n13\n9\n11\nIPPO-C\n3\n0\n2\nTable 4: Average Item shortages computed\nover 100 replications for an horizon of T =\n240 months.\nOnce again, the MARL-based agents present the best performance, both in terms of cost savings (see\nTable 3) and avoidance of shortages (see Table 4). Observe that for cluster N1, the MARL agents\nallows an overall cost reduction of 75% compared to the standard (s, S) strategy, 85% for cluster N2\nand about 78% for cluster N3. For each cluster, the details about the average costs and shortages of\nall items are available in Appendix E.\n9\n6\nConclusion & Discussion\nMaintaining the right balance between the supply and demand of products by optimizing replen-\nishment decisions is one of the most important challenges for inventory management systems. In\nthis paper, a rigorous methodological and practical reinforcement learning framework to address the\ninventory management problem for a single-echelon multi-products supply chain on a production\nline with stochastic demands and lead-times has been developed. The method has been illustrated\nwith extensive numerical experiments on real data for both single and multi-agents algorithms.\nThis problem is very close to real-world use cases. Not only does it handle stochastic demands\nand lead-times but is designed for deployment as an actual business solution on production lines.\nFuture work will focus on the statistical properties of the developed framework by further exploring\nlinks with mean-field approximation theory and the effects of exogenous variables in reinforcement\nlearning.\nAcknowledgements\nThe authors report financial support was provided by TotalEnergies SE. The authors have patent\nissued to TotalEnergies SE.\nReferences\n[1] Adler, J. L. and Blue, V. J. (2002). A cooperative multi-agent transportation management and\nroute guidance system. Transportation Research Part C: Emerging Technologies, 10(5-6):433–454.\n[2] Barat, S., Khadilkar, H., Meisheri, H., Kulkarni, V., Baniwal, V., Kumar, P., and Gajrani, M.\n(2019). Actor based simulation for closed loop control of supply chain using reinforcement learning.\nIn Proceedings of the 18th international conference on autonomous agents and multiagent systems,\npages 1802–1804.\n[3] Bas¸ar, T. and Olsder, G. J. (1998). Dynamic noncooperative game theory. SIAM.\n[4] Baxter, J. and Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of\nArtificial Intelligence Research, 15:319–350.\n[5] Bellman, R. (1966). Dynamic programming. Science, 153(3731):34–37.\n[6] Bellman, R. and Dreyfus, S. (1959). Functional approximations and dynamic programming.\nMathematical Tables and Other Aids to Computation, pages 247–251.\n[7] Bellman, R. and Kalaba, R. (1957). Dynamic programming and statistical communication theory.\nProceedings of the National Academy of Sciences of the United States of America, 43(8):749.\n[8] Bertsekas, D. P. (2011). Approximate policy iteration: A survey and some new methods. Journal\nof Control Theory and Applications, 9(3):310–335.\n[9] Bertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-dynamic programming, volume 5. Athena\nScientific Belmont, MA.\n[10] Bus¸oniu, L., Lazaric, A., Ghavamzadeh, M., Munos, R., Babuˇska, R., and De Schutter, B.\n(2012). Least-squares methods for policy iteration. Reinforcement learning, pages 75–109.\n[11] Chaharsooghi, S. K., Heydari, J., and Zegordi, S. H. (2008). A reinforcement learning model for\nsupply chain ordering management: An application to the beer game. Decision Support Systems,\n45(4):949–959.\n[12] Chaudhary, V., Kulshrestha, R., and Routroy, S. (2018). State-of-the-art literature review on\ninventory models for perishable products. Journal of Advances in Management Research.\n[13] Corke, P., Peterson, R., and Rus, D. (2005). Networked robots: Flying robot navigation using a\nsensor net. In Robotics research. The eleventh international symposium, pages 234–243. Springer.\n10\n[14] Dall’Anese, E., Zhu, H., and Giannakis, G. B. (2013). Distributed optimal power flow for smart\nmicrogrids. IEEE Transactions on Smart Grid, 4(3):1464–1475.\n[15] Doan, T., Maguluri, S., and Romberg, J. (2019). Finite-time analysis of distributed td (0) with\nlinear function approximation on multi-agent reinforcement learning. In International Conference\non Machine Learning, pages 1626–1635. PMLR.\n[16] Dogan, I. and G¨uner, A. R. (2015). A reinforcement learning approach to competitive ordering\nand pricing problem. Expert Systems, 32(1):39–48.\n[17] Fang, J., Zhao, L., Fransoo, J. C., and Van Woensel, T. (2013). Sourcing strategies in supply\nrisk management: An approximate dynamic programming approach. Computers & Operations\nResearch, 40(5):1371–1382.\n[18] Filar, J. and Vrieze, K. (2012). Competitive Markov decision processes. Springer Science &\nBusiness Media.\n[19] Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philosophical\nTransactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or\nPhysical Character, 222(594-604):309–368.\n[20] Fukuda, Y. (1964). Optimal policies for the inventory problem with negotiable leadtime.\nManagement Science, 10(4):690–708.\n[21] Giannoccaro, I. and Pontrandolfo, P. (2002). Inventory management in supply chains: a\nreinforcement learning approach. International Journal of Production Economics, 78(2):153–161.\n[22] Gijsbrechts, J., Boute, R. N., Van Mieghem, J. A., and Zhang, D. (2021). Can deep reinforcement\nlearning improve inventory management? performance on dual sourcing, lost sales and multi-\nechelon problems. Manufacturing & Service Operations Management.\n[23] Halman, N., Klabjan, D., Mostagir, M., Orlin, J., and Simchi-Levi, D. (2009).\nA fully\npolynomial-time approximation scheme for single-item stochastic inventory control with dis-\ncrete demand. Mathematics of Operations Research, 34(3):674–685.\n[24] Howard, R. A. (1960). Dynamic programming and markov processes.\n[25] Jiang, C. and Sheng, Z. (2009). Case-based reinforcement learning for dynamic inventory\ncontrol in a multi-agent supply-chain system. Expert Systems with Applications, 36(3):6520–6526.\n[26] Khouja, M. and Goyal, S. (2008). A review of the joint replenishment problem literature:\n1989–2005. European Journal of Operational Research, 186(1):1–16.\n[27] Kim, C., Jun, J., Baek, J., Smith, R., and Kim, Y.-D. (2005). Adaptive inventory control models\nfor supply chain management. The International Journal of Advanced Manufacturing Technology,\n26(9):1184–1192.\n[28] Kim, C. O., Kwon, I.-H., and Baek, J.-G. (2008). Asynchronous action-reward learning for\nnonstationary serial supply chain inventory control. Applied Intelligence, 28(1):1–16.\n[29] Kober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238–1274.\n[30] Kwak, C., Choi, J. S., Kim, C. O., and Kwon, I.-H. (2009). Situation reactive approach to\nvendor managed inventory problem. Expert Systems with Applications, 36(5):9039–9045.\n[31] Kwon, I.-H., Kim, C. O., Jun, J., and Lee, J. H. (2008). Case-based myopic reinforcement\nlearning for satisfying target service level in supply chain. Expert Systems with applications,\n35(1-2):389–397.\n[32] Levi, D. S., Chen, X., and Bramel, J. (2014). The logic of logistics: theory, algorithms, and\napplications for logistics management. Springer.\n11\n[33] Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gonzalez, J., Jordan, M.,\nand Stoica, I. (2018). Rllib: Abstractions for distributed reinforcement learning. In International\nConference on Machine Learning, pages 3053–3062. PMLR.\n[34] Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning.\nIn Machine learning proceedings 1994, pages 157–163. Elsevier.\n[35] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep\nreinforcement learning. nature, 518(7540):529–533.\n[36] Nguyen, T. T., Nguyen, N. D., and Nahavandi, S. (2020). Deep reinforcement learning for\nmultiagent systems: A review of challenges, solutions, and applications. IEEE transactions on\ncybernetics, 50(9):3826–3839.\n[37] Okuda, R., Kajiwara, Y., and Terashima, K. (2014). A survey of technical trend of adas and\nautonomous driving. In Technical Papers of 2014 International Symposium on VLSI Design,\nAutomation and Test, pages 1–4. IEEE.\n[38] Owen, A. B. (2001). Empirical likelihood. Chapman and Hall/CRC.\n[39] Pineau, J., Gordon, G., Thrun, S., et al. (2003). Point-based value iteration: An anytime\nalgorithm for pomdps. In IJCAI, volume 3, pages 1025–1032. Citeseer.\n[40] Porteus, E. L. (2002). Foundations of stochastic inventory theory. Stanford University Press.\n[41] Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming.\nJohn Wiley & Sons.\n[42] Qu, H., Wang, L., and Liu, R. (2015). A contrastive study of the stochastic location-inventory\nproblem with joint replenishment and independent replenishment. Expert Systems with Applica-\ntions, 42(4):2061–2072.\n[43] Rabbat, M. and Nowak, R. (2004). Distributed optimization in sensor networks. In Proceedings\nof the 3rd international symposium on Information processing in sensor networks, pages 20–27.\n[44] Salameh, M. K., Yassine, A. A., Maddah, B., and Ghaddar, L. (2014). Joint replenishment\nmodel with substitution. Applied Mathematical Modelling, 38(14):3662–3671.\n[45] Sallab, A. E., Abdou, M., Perot, E., and Yogamani, S. (2017). Deep reinforcement learning\nframework for autonomous driving. Electronic Imaging, 2017(19):70–76.\n[46] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347.\n[47] Shapley, L. S. (1953). Stochastic games. Proceedings of the national academy of sciences,\n39(10):1095–1100.\n[48] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L.,\nKumaran, D., Graepel, T., et al. (2018). A general reinforcement learning algorithm that masters\nchess, shogi, and go through self-play. Science, 362(6419):1140–1144.\n[49] Silver, E. A. and Peterson, R. (1985). Decision systems for inventory management and produc-\ntion planning, volume 18. Wiley.\n[50] Sondik, E. J. (1971). The optimal control of partially observable markov processes. Technical\nreport, Stanford Univ Calif Stanford Electronics Labs.\n[51] Sultana, N. N., Meisheri, H., Baniwal, V., Nath, S., Ravindran, B., and Khadilkar, H. (2020).\nReinforcement learning for multi-product multi-node inventory management in supply chains.\narXiv preprint arXiv:2006.04037.\n[52] Sun, R. and Zhao, G. (2012). Analyses about efficiency of reinforcement learning to supply\nchain ordering management. In IEEE 10th International Conference on Industrial Informatics,\npages 124–127. IEEE.\n12\n[53] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n[54] Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. In\nProceedings of the tenth international conference on machine learning, pages 330–337.\n[55] Toomey, J. W. (2000). Inventory management: principles, concepts and techniques, volume 12.\nSpringer Science & Business Media.\n[56] Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi,\nD. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using\nmulti-agent reinforcement learning. Nature, 575(7782):350–354.\n[57] Wang, L., He, J., and Zeng, Y.-R. (2012). A differential evolution algorithm for joint replenish-\nment problem using direct grouping and its application. Expert Systems, 29(5):429–441.\n[58] Wang, L., Shi, Y., and Liu, S. (2015). An improved fruit fly optimization algorithm and its\napplication to joint replenishment problems. Expert systems with Applications, 42(9):4310–4323.\n[59] White, H. (1982). Maximum likelihood estimation of misspecified models. Econometrica:\nJournal of the econometric society, pages 1–25.\n[60] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist\nreinforcement learning. Machine learning, 8(3):229–256.\n[61] Zhang, K., Yang, Z., and Bas¸ar, T. (2021). Multi-agent reinforcement learning: A selective\noverview of theories and algorithms. Handbook of Reinforcement Learning and Control, pages\n321–384.\n[62] Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018). Fully decentralized multi-agent\nreinforcement learning with networked agents. In International Conference on Machine Learning,\npages 5872–5881. PMLR.\n[63] Zheng, Y.-S. and Federgruen, A. (1991). Finding optimal (s, s) policies is about as simple as\nevaluating a single policy. Operations research, 39(4):654–665.\n[64] Zipkin, P. H. (2000). Foundations of inventory management.\n13\nAppendix\nAppendix A collects the technical details related to the implementation of Proximal Policy Optimiza-\ntion algorithms, namely the different loss functions and the associated hyper-parameters used for the\ntraining phase. Appendix B gathers the different item parameters of the real data. Appendices C and\nD present additional results concerning the cumulative costs and item shortages for single agents.\nSimilarly, Appendix E is dedicated to the detailed numerical results of MARL based methods.\nA\nProximal Policy Optimization algorithms\nA.1\nPPO methodology and model\nPPO is a model-free on-policy RL algorithm that works well for both discrete and continuous action\nspace environments. PPO utilizes an actor-critic framework, where there are two networks, an actor\n(policy network) and critic network (value function). Such algorithms are part of the family of policy\ngradient algorithms which use a parameterized action-selection policy πθ with θ ∈Θ ⊂Rd and\nupdate the policy parameter θ on each step in the direction of an estimate of the gradient of the\nperformance with respect to the policy parameter:\nθ ←θ + ηˆg,\nwhere η is the learning rate, ˆg = Et[∇θ log πθ(at|st) ˆAt] is a gradient estimate, ˆAt is an estimator of\nthe advantage function at timestep t and the expectation Et indicates the empirical average over a\nfinite batch of samples, in an algorithm that alternates between sampling and optimization.\nThe implementation of the Proximal Policy Optimization algorithms follows the one of the open-\nsource library RLlib [33]. Whereas standard policy gradient methods perform one gradient update\nper data sample, PPO enables multiple epochs of minibatch updates. According to [46], compared\nto TRPO methods, PPO algorithms are much simpler to implement, more general, and have better\nsample complexity (empirically). PPO’s clipped objective supports multiple SGD passes over the\nsame batch of experiences and RLlib’s PPO can scale out using multiple workers for experience\ncollection, and also with multiple GPUs for SGD.\nDenote by rt(θ) = πθ(at|st)/πθold(at|st) the probability ratio between old and new policies so that\nTRPO methods aim at optimizing the following surrogate objective: LTRPO(θ) = Et[rt(θ) ˆAt]. In\ncomparison, PPO methods rely on the following clipped objective function:\nLactor(θ) = Et\nh\nmin\nn\nrt(θ) ˆAt, clip(rt(θ), 1 −ε, 1 + ε) ˆAt\noi\n.\nThe motivation for this objective is as follows. The second term inside the min modifies the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of\nthe interval [1 −ε, 1 + ε]. By taking the minimum of the clipped and unclipped objective, the final\nobjective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this scheme,\nPPO methods only ignore the change in probability ratio when it would make the objective improve,\nand include it when it makes the objective worse. This very idea of clipping can also be applied to\nthe critic whose aim is to approximate the value function using some neural network, leading to the\nfollowing critic loss\nLcritic(θ) = Et\nh\nmax\nn\n(Vθ(st) −ˆVt)2, (Vθold(st) −ˆVt + clip(Vθ(st) −Vθold(st), −ε, +ε))2oi\nFurthermore, in addition to clipped surrogate objective, consider some KL divergence and entropy\nterms. For the KL term, this follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the current policy. Finally, the total objective can be augmented by adding an entropy\nbonus H to ensure sufficient exploration, as suggested in past work [35]. Overall, the goal is to\nmaximize the following objective function\nL(θ) = Lactor(θ) −c1Lcritic(θ) −c2Et[DKL(πθold(·|st), πθ(·|st))] + c3Et[H(πθ)(st)],\nwhere Lactor is the policy loss, Lcritic is the critic loss, DKL is the Kullback-Liebler divergence and\nH is the entropy bonus. The constants c1, c2 and c3 are the value-function loss coefficient, the KL\ncoefficient and the entropy coefficient.\n14\nA.2\nImplementation Details\nWhen working with storage constraints per item, it is enough to only train a few agents. The 50 items\nof the different warehouses are split in 2 groups according to the median of lead-time in order to train\nonly 2 average RL agent. Such agents are then tested on all the items in the corresponding group. In\norder to compare the effect of working with discrete or continuous actions spaces, two RL agents are\ntrained using Discrete actions (PPO-D1 and PPO-D2), and 2 RL agents using Continuous actions\n(PPO-C1 and PPO-C2). When working with items competing for storage space, MARL algorithms\nare used, suchg as IPPO and other variations with shared critic. Tables 5 and 6 gather all the details\nabout the different hyper parameters used for the traning of (MA)RL agents. In all the experiments,\nthe following parameters are fixed:\nParameter\nValue\nHORIZON\n200\nGAMMA\n0.99\nLEARNING RATE\n1e-4\nVF SHARE LAYERS\nFalse\nROLLOUT FRAGMENTLENGTH\n200\nBATCHMODE\ncomplete\nTRAIN BATCH SIZE\n8000\nSGD MINIBACTH SIZE\n250\nNUM SGD ITER\n20\nNORMALIZE ACTIONS\nTrue\nFCNET ACTIVATION\nrelu\nUSE CRITIC\nTrue\nGAE LAMBDA\n1\nKL COEFF\n2e-1\nKL TARGET\n1e-2\nENTROPY COEFF\n0.01\nCLIP PARAM\n0.3\nTable 5: Hyper parameters Details for Training\nThe following parameters are specific to the different RL agents:\nParameter\nPPO-D1\nPPO-D2\nPPO-C1\nPPO-D2\nFCNET HIDDEN\n[512, 512]\n[512, 512]\n[512, 512]\n[512, 512]\nGRAD CLIP\n40\n40\n40\n40\nLEARNING RATE\n1e-4\n1e-4\n1e-4\n2e-4\nVF SHARE LAYERS\nFalse\nFalse\nFalse\nFalse\nUSE GAE\nTrue\nTrue\nTrue\nTrue\nVF CLIP PARAM\n1e3\n1e4\n1e3\n5e2\nVF LOSS COEFF\n1\n1e-2\n1e-2\n1e-2\nTable 6: Hyper parameters Details for Training\nParameter\nIPPO-N1\nIPPO-N2\nIPPO-N3\nFCNET HIDDEN\n[512, 256]\n[512,256]\n[512,256]\nGRAD CLIP\n40\n20\n20\nLEARNING RATE\n5e-5\n2e-5\n2e-5\nVF SHARE LAYERS\nTrue\nTrue\nTrue\nUSE GAE\nFalse\nFalse\nFalse\nVF CLIP PARAM\n5e2\n5e2\n5e2\nVF LOSS COEFF\n1e-3\n1e-4\n1e-4\nTable 7: Hyper parameters Details for Training\n15\nB\nItem Hyperparameters\nThe data is used to find the model parameters of the stochastic distributions using some MLE\nestimators. For a fixed item i ∈N with historical data of demands (δ(i)\nt )t=1,...,Ni and lead-times\n(τ (i)\nt )t=1,...,Ti, the MLE estimators ˆbi, ˆµi and ˆpi are given by the empirical averages\nˆbi = 1\nNi\nNi\nX\nt=1\n1{δ(i)\nt\n>0}\nˆµi =\nPNi\nt=1 δ(i)\nt 1{δ(i)\nt\n>0}\nPNi\nt=1 1{δ(i)\nt\n>0}\nˆpi =\nTi\nPTi\nt=1 τ (i)\nt\nThe different Tables below summarize the parameter values for each item.\nID\nˆb\nˆµ\nˆp\nCo\nCh\nCs\n0\n0.33\n6.23\n0.12\n1,010\n57\n11,097\n1\n0.12\n17.33\n0.17\n1,092\n125\n11,800\n2\n0.21\n11.0\n0.17\n1,363\n159\n14,887\n3\n0.24\n9.04\n0.11\n1,125\n131\n12,881\n4\n0.17\n12.0\n0.11\n1,007\n119\n14,758\n5\n0.31\n6.87\n0.11\n1,174\n65\n15,954\n6\n0.17\n12.5\n0.12\n1,280\n104\n18,109\n7\n0.12\n17.25\n0.12\n1,220\n71\n14,450\n8\n0.18\n11.82\n0.19\n2,250\n269\n23,984\n9\n0.29\n8.46\n0.13\n1,356\n129\n13,998\nTable 8: Products Parameters and Costs for Items 0-9\nID\nˆb\nˆµ\nˆp\nCo\nCh\nCs\n10\n0.29\n8.08\n0.15\n1,597\n170\n21,512\n11\n0.11\n22.0\n0.14\n1,069\n100\n14,184\n12\n0.4\n8.25\n0.2\n1,020\n112\n15,244\n13\n0.17\n16.25\n0.15\n1,342\n149\n14,059\n14\n0.15\n20.59\n0.12\n1,080\n112\n11,352\n15\n0.08\n24.0\n0.41\n3,380\n298\n37,941\n16\n0.24\n9.83\n0.1\n1,857\n194\n23,874\n17\n0.12\n26.67\n0.14\n1,042\n107\n11,000\n18\n0.12\n20.0\n0.1\n1,360\n174\n17,718\n19\n0.4\n7.07\n0.12\n1,690\n215\n20,403\nTable 9: Products Parameters and Costs for Items 10-19\nID\nˆb\nˆµ\nˆp\nCo\nCh\nCs\n20\n0.08\n40.0\n0.15\n1,110\n95\n12,873\n21\n0.38\n9.5\n0.12\n1,270\n181\n13,578\n22\n0.08\n32.67\n0.1\n1,276\n184\n16,441\n23\n0.17\n23.58\n0.2\n1,170\n60\n15,026\n24\n0.14\n20.4\n0.11\n2,084\n270\n25,725\n25\n0.08\n40.0\n0.12\n1,371\n177\n14,804\n26\n0.11\n32.5\n0.09\n1,092\n152\n12,305\n27\n0.08\n32.0\n0.11\n2,104\n135\n24,015\n28\n0.11\n36.5\n0.15\n1,252\n139\n14,186\n29\n0.39\n7.81\n0.1\n1,792\n93\n25,279\nTable 10: Products Parameters and Costs for Items 20-29\n16\nID\nˆb\nˆµ\nˆp\nCo\nCh\nCs\n30\n0.28\n15.25\n0.1\n1,085\n77\n14,038\n31\n0.08\n40.0\n0.08\n1,445\n164\n14,742\n32\n0.19\n18.3\n0.11\n2,284\n304\n23,957\n33\n0.26\n19.73\n0.16\n1,142\n138\n13,926\n34\n0.17\n26.0\n0.15\n1,342\n196\n16,559\n35\n0.33\n10.39\n0.07\n1,851\n267\n19,626\n36\n0.14\n26.0\n0.15\n1,765\n225\n25,052\n37\n0.1\n35.67\n0.11\n2,070\n130\n20,910\n38\n0.12\n30.17\n0.09\n1,380\n204\n20,550\n39\n0.33\n6.58\n0.06\n4,470\n456\n51,326\nTable 11: Products Parameters and Costs for Items 30-39\nID\nˆb\nˆµ\nˆp\nCo\nCh\nCs\n40\n0.29\n18.14\n0.17\n1,689\n158\n22,534\n41\n0.08\n50.0\n0.1\n1,329\n141\n17,654\n42\n0.22\n20.77\n0.13\n2,312\n257\n29,177\n43\n0.44\n14.33\n0.1\n1,308\n100\n15,170\n44\n0.08\n70.0\n0.1\n1,308\n179\n13,718\n45\n0.17\n34.0\n0.18\n3,590\n328\n38,035\n46\n0.08\n108.0\n0.12\n1,011\n87\n10,134\n47\n0.19\n111.0\n0.12\n1,049\n100\n15,621\n48\n0.18\n97.88\n0.11\n1,851\n171\n20,451\n49\n0.08\n217.0\n0.11\n1,851\n114\n25,362\nTable 12: Products Parameters and Costs for Items 40-49\nC\nAverage Cumulative Costs\nC.1\nNumerical Results\nThe different Tables below present the full results of cumulative costs for the numerical experiments\nwith storage space constraints per item. The 50 items are split in 2 groups according to the median of\nlead-time in order to train only 2 average RL agent. Such agents are then tested on all the items in the\ncorresponding group.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n0\n48,554,986\n10,183,088\n4,863,202\n4,616,016\n1\n52,993,931\n16,917,389\n6,865,991\n6,385,378\n2\n70,467,282\n21,426,806\n8,727,215\n8,087,258\n3\n72,220,832\n12,722,887\n9,854,047\n5,280,345\n4\n79,235,272\n16,976,630\n8,801,628\n4,808,191\n5\n79,945,152\n17,793,765\n9,068,217\n4,270,027\n6\n94,856,066\n23,070,095\n6,725,252\n6,330,215\n7\n94,888,885\n23,194,601\n9,540,166\n4,599,449\n8\n96,678,620\n30,082,381\n15,010,899\n13,646,991\n9\n98,917,563\n15,738,319\n7,593,699\n7,229,608\nTable 13: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor Items 0-9.\n17\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n10\n110,582,575\n24,999,881\n9,564,232\n9,038,083\n11\n112,399,072\n42,910,634\n7,574,747\n6,628,323\n12\n115,603,447\n46,823,147\n6,853,628\n6,103,570\n13\n124,832,231\n39,888,919\n8,243,508\n7,665,907\n14\n152,157,505\n47,941,527\n6,445,518\n6,418,000\n15\n156,482,922\n64,905,614\n26,221,197\n18,708,044\n16\n162,805,503\n32,745,666\n15,653,937\n8,462,765\n17\n165,864,536\n74,037,166\n10,165,442\n9,631,728\n18\n167,274,515\n46,875,740\n12,546,177\n7,122,270\n19\n174,895,434\n56,821,488\n10,050,155\n9,706,447\nTable 14: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor Items 10-19.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n20\n182,267,502\n130,132,350\n33,743,915\n36,077,127\n21\n188,855,483\n66,602,602\n8,227,594\n7,704,383\n22\n209,851,096\n101,916,137\n18,964,499\n19,156,603\n23\n240,876,429\n119,152,696\n10,058,032\n9,181,690\n24\n253,729,788\n94,653,074\n18,857,686\n14,191,791\n25\n259,717,238\n154,572,208\n39,887,428\n41,316,087\n26\n275,654,944\n136,723,841\n21,806,460\n28,917,117\n27\n294,354,320\n13,3290,131\n25,792,146\n22,497,325\n28\n297,756,076\n195,971,759\n34,602,990\n39,975,903\n29\n317,748,541\n78,399,969\n13,360,829\n6,625,366\nTable 15: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor Items 20-29.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n30\n340,850,617\n118,356,448\n8,428,058\n5,674,420\n31\n346,677,835\n165,513,863\n38,211,495\n50,257,711\n32\n372,022,376\n140,849,611\n20,580,162\n13,426,053\n33\n379,189,787\n176,014,600\n17,857,314\n12,033,561\n34\n379,417,845\n187,467,258\n20,350,345\n18,721,088\n35\n429,201,653\n125,815,263\n15,920,620\n10,148,258\n36\n450,249,676\n197,138,780\n20,805,554\n21,535,008\n37\n467,942,296\n221,381,388\n40,233,046\n51,880,344\n38\n517,401,650\n217,431,556\n25,188,489\n35,934,573\n39\n529,054,715\n113,253,016\n35,899,378\n19,663,470\nTable 16: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor Items 30-39.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n40\n548,632,612\n274,992,151\n19,614,638\n14,265,377\n41\n614,478,713\n351,832,711\n91,357,970\n158,742,424\n42\n802,247,508\n318,672,823\n28,692,777\n22,605,438\n43\n826,810,789\n277,103,747\n13,143,955\n18,262,774\n44\n897,278,509\n593,855,529\n278,916,815\n434,302,595\n45\n1205,603,289\n856,462,764\n193,219,445\n184,847,412\n46\n1463,640,790\n1196,188,961\n1013,385,300\n1137,694,029\n47\n7013,730,126\n5018,246,734\n4692,465,941\n5153,720,918\n48\n7033,366,188\n4830,858,675\n4348,154,722\n4928,213,014\n49\n10354,442,106\n9275,492,362\n9137,883,131\n9309,276,387\nTable 17: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor Items 40-49.\n18\nC.2\nBarplots results\nSimilarly to Section C.1, the different barplots below present the full results of cumulative costs for\nthe numerical experiments with storage space constraints per item. The 50 items are split in 2 groups\naccording to the median of lead-time in order to train only 2 average RL agent. Such agents are then\ntested on all the items in the corresponding group.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo-D\nppo-C\nFigure 4: Average Cumulative Costs in $ ob-\ntained over 100 replications, horizon of T = 240\nmonths for Items 0-9.\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo-D\nppo-C\nFigure 5: Average Cumulative Costs in $ ob-\ntained over 100 replications, horizon of T = 240\nmonths for Items 10-19.\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo-D\nppo-C\nFigure 6: Average Cumulative Costs in $ ob-\ntained over 100 replications, horizon of T = 240\nmonths for Items 20-29.\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo-D\nppo-C\nFigure 7: Average Cumulative Costs in $ ob-\ntained over 100 replications, horizon of T = 240\nmonths for Items 30-39.\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo-D\nppo-C\nFigure 8: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor Items 40-49.\n19\nD\nAverage Item Shortages\nD.1\nNumerical results\nThe different Tables below present the full results of item shortages for the numerical experiments\nwith storage space constraints per item. The 50 items are split in 2 groups according to the median of\nlead-time in order to train only 2 average RL agent. Such agents are then tested on all the items in the\ncorresponding group.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n0\n39\n10\n0\n0\n1\n41\n11\n0\n0\n2\n42\n13\n0\n0\n3\n51\n8\n0\n0\n4\n49\n10\n0\n0\n5\n43\n10\n0\n0\n6\n47\n11\n0\n0\n7\n57\n12\n0\n0\n8\n38\n10\n0\n0\n9\n59\n9\n0\n0\nTable 18: Average Item shortages obtained\nover 100 replications, horizon of T = 240\nmonths for Items 0-9.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n10\n44\n11\n0\n0\n11\n69\n23\n0\n0\n12\n66\n26\n0\n0\n13\n76\n25\n0\n0\n14\n119\n37\n0\n0\n15\n37\n14\n0\n0\n16\n65\n13\n0\n0\n17\n132\n58\n2\n2\n18\n84\n22\n0\n0\n19\n71\n26\n0\n0\nTable 19: Average Item shortages obtained\nover 100 replications, horizon of T = 240\nmonths for Items 10-19.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n20\n125\n84\n16\n17\n21\n119\n41\n0\n0\n22\n108\n52\n3\n5\n23\n132\n69\n1\n1\n24\n95\n29\n0\n0\n25\n155\n88\n16\n16\n26\n200\n89\n7\n14\n27\n109\n46\n2\n4\n28\n182\n114\n15\n18\n29\n109\n26\n0\n0\nTable 20: Average Item shortages obtained\nover 100 replications, horizon of T = 240\nmonths for Items 20-29.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n30\n219\n69\n0\n0\n31\n209\n94\n14\n22\n32\n140\n48\n0\n0\n33\n234\n110\n4\n2\n34\n196\n95\n6\n5\n35\n197\n49\n0\n0\n36\n155\n66\n3\n3\n37\n193\n89\n9\n16\n38\n220\n86\n5\n11\n39\n90\n18\n0\n0\nTable 21: Average Item shortages obtained\nover 100 replications, horizon of T = 240\nmonths for Items 30-39.\nID\nMinMax\nOracle\nPPO-D\nPPO-C\n40\n209\n102\n2\n1\n41\n305\n172\n40\n75\n42\n240\n95\n3\n3\n43\n472\n141\n1\n5\n44\n575\n360\n167\n264\n45\n276\n186\n35\n33\n46\n1230\n986\n837\n945\n47\n3814\n2695\n2520\n2770\n48\n2959\n1989\n1778\n2030\n49\n3453\n3066\n3025\n3081\nTable 22: Average Item shortages obtained over 100 replications, horizon of T = 240 months for\nItems 40-49.\n20\nD.2\nBarplots results\nSimilarly to Section D.1, the different barplots below present the full results of item shortages for the\nnumerical experiments with storage space constraints per item. The 50 items are split in 2 groups\naccording to the median of lead-time in order to train only 2 average RL agent. Such agents are then\ntested on all the items in the corresponding group.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo-D\nppo-C\nFigure 9: Average Item shortages obtained over\n100 replications, horizon of T = 240 months for\nItems 0-9.\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo-D\nppo-C\nFigure 10: Average Item shortages obtained over\n100 replications, horizon of T = 240 months for\nItems 10-19.\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo-D\nppo-C\nFigure 11: Average Item shortages obtained over\n100 replications, horizon of T = 240 months for\nItems 20-29.\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo-D\nppo-C\nFigure 12: Average Item shortages obtained over\n100 replications, horizon of T = 240 months for\nItems 30-39.\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nItem ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo-D\nppo-C\nFigure 13: Average Item shortages obtained over 100 replications, horizon of T = 240 months for\nItems 40-49.\n21\nE\nAverage Cumulative Costs and Item Shortages for Multi-Agent cases\nE.1\nCluster N1\nID\nMinMax\nOracle\nIPPO-C\nN1-0\n56,413,703\n47,751,643\n21,747,138\nN1-1\n41,218,939\n22,017,147\n5,860,091\nN1-2\n28,734,942\n10,052,437\n9,674,691\nN1-3\n69,445,374\n49,567,318\n11,468,600\nN1-4\n50,283,017\n18,591,579\n11,492,810\nAverage\n49,219,195\n29,596,024\n12,048,666\nTable 23: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor cluster N1\nID\nMinMax\nOracle\nIPPO-C\nN1-0\n27\n25\n0\nN1-1\n18\n10\n1\nN1-2\n11\n4\n6\nN1-3\n22\n21\n5\nN1-4\n14\n5\n3\nAverage\n18\n9\n3\nTable 24: Average Item Shortages obtained over 100 replications, horizon of T = 240 months for\ncluster N1\n0\n1\n2\n3\n4\nItem ID Cluster \n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo\n0\n1\n2\n3\n4\nItem ID Cluster \n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo\nFigure 14: Average Cumulative Costs in $ and Item Shortages obtained over 100 replications, horizon\nof T = 240 months for cluster N1\nE.2\nCluster N2\nID\nMinMax\nOracle\nIPPO-C\nN2-0\n61,746,686\n37,154,240\n9,576,664\nN2-1\n43,195,470\n19,817,180\n4,755,388\nN2-2\n31,719,337\n8,849,048\n7,232,925\nN2-3\n49,873,610\n38,995,232\n7,999,544\nN2-4\n46,383,922\n17,470,651\n8,097,309\nN2-5\n90,361,502\n54,257,529\n6,974,378\nN2-6\n20,349,092\n8,573,083\n4,180,512\nN2-7\n27,434,488\n9,091,921\n9,855,567\nN2-8\n33,681,010\n11,145,783\n3,862,856\nN2-9\n146,735,155\n105,116,776\n16,516,513\nAverage\n55,148,027\n31,047,144\n7,905,165\nTable 25: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor cluster N2\n22\nID\nMinMax\nOracle\nIPPO-C\nN2-0\n33\n18\n1\nN2-1\n18\n8\n0\nN2-2\n14\n2\n0\nN2-3\n16\n14\n0\nN2-4\n13\n4\n0\nN2-5\n27\n16\n0\nN2-6\n11\n8\n0\nN2-7\n11\n2\n0\nN2-8\n14\n5\n0\nN2-9\n20\n13\n0\nAverage\n18\n9\n0\nTable 26: Average Item Shortages obtained over 100 replications, horizon of T = 240 months for\ncluster N2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nItem ID Cluster \n2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nItem ID Cluster \n2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo\nFigure 15: Average Cumulative Costs in $ and Item Shortages obtained over 100 replications, horizon\nof T = 240 months for cluster N2\nE.3\nCluster N3\nID\nMinMax\nOracle\nIPPO-C\nN3-0\n114,748,999\n110,936,731\n25,133,542\nN3-1\n76,876,589\n25,248,863\n13,012,784\nN3-2\n175,969,893\n156,867,720\n22,712,799\nN3-3\n61,746,686\n38,243,787\n11,097,544\nN3-4\n43,195,470\n14,974,424\n6,590,003\nN3-5\n31,719,337\n9,423,716\n12,452,378\nN3-6\n49,873,610\n35,928,930\n13,922,411\nN3-7\n46,383,922\n20,756,443\n7,299,909\nN3-8\n34,654,173\n9,843,404\n11,531,224\nN3-9\n90,361,502\n55,657,119\n13,186,238\nN3-10\n20,349,092\n8,662,162\n5,354,639\nN3-11\n41,008,234\n12,947,589\n10,195,819\nN3-12\n33,335,933\n12,451,461\n10,197,750\nN3-13\n27,434,488\n8,713,145\n8,518,849\nN3-14\n33,681,010\n10,451,041\n8,045,677\nN3-15\n106,676,800\n56,572,852\n11,317,268\nN3-16\n146,735,155\n126,683,713\n55,932,376\nN3-17\n46,569,841\n24,907,569\n14,623,596\nN3-18\n41,120,038\n9,885,961\n13,659,858\nN3-19\n43,945,213\n10,883,913\n10,006,678\nAverage\n63,319,299\n38,002,027\n14,239,567\nTable 27: Average Cumulative Costs in $ obtained over 100 replications, horizon of T = 240 months\nfor cluster N3\n23\nID\nMinMax\nOracle\nIPPO-C\nN3-0\n39\n33\n7\nN3-1\n19\n4\n0\nN3-2\n97\n63\n0\nN3-3\n33\n19\n1\nN3-4\n18\n5\n0\nN3-5\n14\n4\n6\nN3-6\n16\n14\n4\nN3-7\n13\n5\n0\nN3-8\n15\n2\n2\nN3-9\n27\n17\n4\nN3-10\n11\n6\n1\nN3-11\n9\n0\n0\nN3-12\n13\n3\n3\nN3-13\n11\n3\n0\nN3-14\n14\n4\n2\nN3-15\n55\n22\n0\nN3-16\n20\n18\n2\nN3-17\n19\n7\n7\nN3-18\n14\n2\n0\nN3-19\n16\n2\n3\nAverage\n23\n11\n2\nTable 28: Average Item Shortages obtained over 100 replications, horizon of T = 240 months for\ncluster N3\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\nItem ID Cluster \n3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Costs\nminmax\noracle\nppo\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\nItem ID Cluster \n3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Shortages\nminmax\noracle\nppo\nFigure 16: Average Cumulative Costs in $ and Item Shortages obtained over 100 replications, horizon\nof T = 240 months for cluster N3\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA"
  ],
  "published": "2023-08-03",
  "updated": "2023-08-03"
}