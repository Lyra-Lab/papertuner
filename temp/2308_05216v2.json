{
  "id": "http://arxiv.org/abs/2308.05216v2",
  "title": "High-dimensional reinforcement learning for optimization and control of ultracold quantum gases",
  "authors": [
    "Nicholas Milson",
    "Arina Tashchilina",
    "Tian Ooi",
    "Anna Czarnecka",
    "Zaheen F. Ahmad",
    "Lindsay J. LeBlanc"
  ],
  "abstract": "Machine-learning techniques are emerging as a valuable tool in experimental\nphysics, and among them, reinforcement learning offers the potential to control\nhigh-dimensional, multistage processes in the presence of fluctuating\nenvironments. In this experimental work, we apply reinforcement learning to the\npreparation of an ultracold quantum gas to realize a consistent and large\nnumber of atoms at microkelvin temperatures. This reinforcement learning agent\ndetermines an optimal set of thirty control parameters in a dynamically\nchanging environment that is characterized by thirty sensed parameters. By\ncomparing this method to that of training supervised-learning regression\nmodels, as well as to human-driven control schemes, we find that both machine\nlearning approaches accurately predict the number of cooled atoms and both\nresult in occasional superhuman control schemes. However, only the\nreinforcement learning method achieves consistent outcomes, even in the\npresence of a dynamic environment.",
  "text": "High-dimensional reinforcement learning for optimization and control of\nultracold quantum gases\nNicholas Milson1,∗Arina Tashchilina1, Tian Ooi1, Anna\nCzarnecka1, Zaheen F. Ahmad2, and Lindsay J. LeBlanc1†\n1Department of Physics, University of Alberta, Edmonton, Alberta, Canada and\n2Department of Computing Sciences, University of Alberta, Edmonton, Alberta, Canada\nMachine-learning techniques are emerging as a valuable tool in experimental physics, and among\nthem, reinforcement learning offers the potential to control high-dimensional, multistage processes\nin the presence of fluctuating environments. In this experimental work, we apply reinforcement\nlearning to the preparation of an ultracold quantum gas to realize a consistent and large number of\natoms at microkelvin temperatures. This reinforcement learning agent determines an optimal set\nof thirty control parameters in a dynamically changing environment that is characterized by thirty\nsensed parameters. By comparing this method to that of training supervised-learning regression\nmodels, as well as to human-driven control schemes, we find that both machine learning approaches\naccurately predict the number of cooled atoms and both result in occasional superhuman control\nschemes. However, only the reinforcement learning method achieves consistent outcomes, even in\nthe presence of a dynamic environment.\nI.\nINTRODUCTION\nAs a general tool, machine learning (ML) offers remarkable advantages in far-reaching domains, rang-\ning from large-language models to control-systems instrumentation. In the realm of scientific research, ML\npromises to improve the design, control, and optimization of experimental processes, particularly when these\nprocedures are high-dimensional, when uncontrolled environmental factors affect outcomes, and when sys-\ntematic optimization is implausible, leaving only intuition or trial and error [1]. The production of cold, dense\nensembles of neutral atoms for Bose-Einstein condensates (BECs), for example, involves many detailed steps,\neach with several free parameters [2]. While experimentalists have risen to the challenge to create systems\nthat consistently result in BECs, applications of ML to optimize magneto-optical traps [3, 4], evaporation\ncurves [5–9], and simultaneous laser and evaporative cooling [10, 11] show the potential for creating greater\nreliability for these systems.\nThese impressive results in atom-cooling applications are examples of supervised ML, in which a model\nlearns from labeled examples, aiming to predict or classify new instances of the same problem based on\nprior training. So far, these approaches used Gaussian processes [12] or deep neural networks [13] to apply a\nsupervised ML regression model (sometimes known as a surrogate function), then used the trained model to\nfind experimental control inputs that maximize the predicted output. These methods to find efficient cooling\nschemes demonstrated superior performance compared to human optimization and direct numerical optimiza-\ntion techniques, such as differential evolution [5]. However, in scenarios where optimal control parameters\ndepend on changing environmental conditions, these approaches are inadequate. A direct maximization via\nBayesian optimization [14] is unsuited, as the objective function varies with the environment. Furthermore,\nmodeling the output metric as a function of controllable and environmental factors and optimizing the model\nalone is likely insufficient, as previous investigations that only considered the controllable factors failed to\nyield consistent superhuman control parameter choices [3, 10]. Robustness of the found experimental control\ninputs is now also a consideration, as the usefulness of finding a single high-performing set is now highly\ndependent on how strongly the performance of this set changes with the dynamic environment.\nBeyond the scope of supervised ML, machine learning also includes two more general approaches: unsu-\npervised ML, which deals with finding patterns and structures in unlabeled data, through techniques such\nas clustering and dimensionality reduction; and reinforcement learning (RL), which represents a class of\nproblems where an agent interacts with an environment, learning to make sequential decisions to maximize\ncumulative rewards. While still a young subfield in physics, the power of reinforcement learning has been\n∗Corresponding author: nmilson@ualberta.ca\n† lindsay.leblanc@ualberta.ca\narXiv:2308.05216v2  [cond-mat.quant-gas]  30 Dec 2023\n2\ndemonstrated in the control of noisy and many-body quantum systems [15–20], as well as experimental\ncontrol and navigation in stochastic turbulent environments such as micro-sphere carrying optical tweezers\n[21] and microorganism simulating artificial nano-swimmers [22]. Despite this recent wave of RL successes,\nlive, autonomous optimization and control of atom-cooling apparatuses with RL remains largely unexplored,\nparticularly in the context of high-dimensional control-parameter and environmental-parameter spaces.\nIn this paper, we optimize a rubidium-87 atom cooling experiment using ML approaches that consider the\nenvironment the apparatus is in. We focus our work on the crucial initial production stages of a BEC, includ-\ning laser cooling and trapping, up to a high-field-gradient magnetic trap (MT) [23, 24]. Our high-dimensional\nML schemes include thirty measured environmental parameters, sensed at specific times throughout the ex-\nperimental sequence, in addition to thirty control parameters that are subject to ML optimization.\nWe\ndevelop an RL controller that determines optimal control parameters based on the current environmental\nconditions, and find that the overall atom number achieved exceeds that of all other methods. We specifically\ncompare this RL controller to a supervised regression model that maps the combined input space of control\nparameters and sensed environmental parameters to the number of atoms in the trap, and uses the model\nto find optimal control parameters that maximize atom number for a given environmental state. Our results\nshow that RL offers unique advantages to experimental control in high-dimensional systems, especially for\nits ability to react to drifts in the environmental conditions that have influence over the outcomes.\nII.\nMETHODS\nUltracold quantum gas experiments operate, generally, by varying a large number of parameters over\nseveral seconds to achieve laser cooling, atom trapping, and additional evaporative cooling, with each cycle\nculminating in a destructive measurement of the atom’s number and/or temperature. Here, we give a ML\nagent control over thirty parameters throughout the sequence, and supply the agent with measurements of\nthirty environmental parameters sensed during the previous cycle. Fundamentally, the question posed to the\ntwo agents we design is the same: given the current state of the environment, what is the ideal agent action\n(i.e., the settings of some controllable parameters) that maximizes the number of atoms imaged at the end\nof the cooling procedure?\nA.\nExperimental Apparatus\nFor this work, we use an experimental apparatus designed and built to create BECs of rubidium-87 [25]. To\nachieve BECs with large, consistent numbers of atoms, we find that the large, consistent numbers of atoms at\nthe magnetic-trapping (MT) stage of our sequence is key to success. Despite the best manual optimizations\nwe apply, we observe long- and short-term drifts in the atom number, and suspect environmental changes\nare responsible for the instabilities.\nOverall, our atom-cooling sequence begins with atoms under ultrahigh vacuum laser-cooled in a two-\ndimensional magneto-optical trap (2D-MOT), and then transferred to a separate chamber with a blue-\ndetuned push beam to be laser-cooled in a three-dimensional (3D) MOT. Next, the atoms undergo motion-\ninduced orientation cooling over a range of laser frequencies to cool them to sub-Doppler limit temperatures.\nSubsequently, the atoms are optically pumped into a magnetically trappable Zeeman state and transferred\nto a quadrupole magnetic field trap. Last, the MT is compressed by adiabatically ramping the magnetic field\ngradient to significantly increase atomic density in preparation for forced evaporative cooling. At this stage,\nwe release the atoms in time-of-flight and resonantly absorption-image the cloud to determine the number of\natoms. The temperature of the resulting ensemble is affected by the trap confinement. The ensemble trapped\nin the initial MT is 50 µK, and increases to 450 µK with compression of the MT.\nThroughout each stage in the cooling process, we have several degrees of control. Some examples include:\nlaser powers and frequencies controlled via acousto-optical modulators during the 2D- and 3D-MOT phases\naffect the cooling power; the position of the magnetic-trap centres may be moved via three sets of Helmholtz-\nconfiguration bias coils to align with the intersections of cooling beams to reduce heating when transitioning\nfrom one preparation stage to the next; or the atomic density can be adjusted by changing the current of\ngradient coils during the magnetic trapping phase. In total, we assign 30 such parameters to be controllable\nby our ML agents (see A for full list).\nTo inform the ML agents’ decision-making process, a monitoring system comprising a variety of sensors\n3\n2D + 3D MOT\n14.0 s\n20ms\n100 ms\nSub-\nDoppler\nOptical\npumping\nMagnetic trapping\n2 ms\nTOF + \nimaging\ntime\n3D trap laser freq., power\n3D bias coil currrents (x3)\n2D push power\n2D coil currents (x4)\n2D trap laser power\n2D repump laser power\n3D repump laser power\nImaging power, freq.\n3D vacuum pressure\nAmbient magnetic ﬁeld (3-axis)\n3D coil temperatures\nRb\n%\nAgent/\nControl\nAmbient temperature, humidity\nCamera image data\nEnvironment\nAtom number\nA29\nA30\nA5-20 \n2D laser power, polarization\n3D laser power, polarization\nA1\nA2\nA3\nA4\nA22 \nA21 \nA23 \nA25\nA24\nA26\nA27\nA28\nE1\nE2\nE3\nE4\nE5\nE24\nE25\nE29\nE30\nE6\nE7-18\nE22\nE23\nE27\nE28\npush beam power\nE26\nMagnetic trap currents\nE19-21\nFIG. 1: System schematic. Top: A set xc of agent-defined parameters (Ai) control the experimental\napparatus, while a set xe of environmental parameters (Ei) are measured during each experimental\nsequence. The apparatus consists of an oven-fed 2D-MOT (with agent-controlled coil currents and laser\npowers), which supplies a 3D-MOT (with agent-controlled laser powers and frequencies) via an\nagent-controlled push beam. At the 3D-MOT, several cooling steps precede the magnetic trapping of the\natoms, agent-controlled bias magnetic field coils are used to optimize the environment throughout the\nsequence. The atom number N is the reward and determined using data obtained by the camera, with\nagent-controlled imaging laser power and frequency. Bottom: The timing sequence for each experimental\ncycle, which is repeated after each destructive image of the atoms is recorded.\nis implemented to capture and track the relevant external parameters throughout the preparation stages.\nThese sensors enable us to construct a representation of the environment state to which the agent responds\nby selecting appropriate control parameters. Environmental parameters are read at the completion of each full\ncycle, with the control parameters held at fixed values. This approach decouples the subsequent environmental\nstate readings from the previously chosen control parameters. By doing so, the control parameter choices\nno longer affect the environment, thereby alleviating some of the complexity of this control problem. The\nenvironmental parameters monitored encompass a wide range of factors, including room temperature, room\nhumidity, magnetic trapping coil temperature, laser powers, magnetic field strengths, vacuum pressures, and\ncoil currents. Multiple sensors are strategically placed and configured to track a total of 30 environmental\nparameters (see B for full list).\nB.\nReinforcement-Learning-Based Agent Design\nRL tackles the optimization problem within a Markov decision process (MDP) framework by selecting\nactions to maximize long-term cumulative rewards. In general, actions influence the subsequent state of the\nenvironment, requiring agents to consider the impact of their choices on future rewards. In our case, we\ndesigned our system such that the observed outcomes are decoupled from previous control parameter actions,\nmaking it akin to a contextual bandit problem [26]. Contextual bandits can be viewed as an edge case of\nMDPs, where episodes consist of only a single step, or alternatively the discount factor is zero [27]. Despite\nthe reduced complexity resulting from the decoupling, we opt for a more powerful RL approach due to the\n4\nRL Agent\nActor\npolicy\nexperience replay buffer\nCritic\nprediction\nExperimental system\ncontrol\nvalue function\nenvironment\na\nSupervised ML Agent\nExperimental system\nOptimizer\nmodel\ncontrol\nvalue function\nenvironment\nb\nFIG. 2: a Schematic diagram of the actor-critic agent’s live control loop. The agent leverages the sensed\ncurrent environment as well as information about its own previous performance to output a continuous\nprobabilistic policy, from which control parameter actions are drawn. The control parameters are\nimplemented into our system, and the resulting cloud of cooled atoms is imaged. The weights and biases of\nthe actor-critic network are then adjusted depending on the number of atoms in the cloud. This loop may\nthen repeat. b Schematic diagram of regression based agent’s live control loop. The neural network\nregressor uses its bank of training data to makes a map between the concatenated control and environment\nparameter vectors, and the numbers of atoms imaged. The network is then partially optimized to find the\ncontrol parameters that maximize the outputted number of atoms, given the current environmental state.\nThese maximizing control parameters are implemented into our system, and the resulting cloud of cooled\natoms is imaged. With this additional experience tuple consisting of the new atom number and the control\nand environmental parameters that generated it, the network weights and biases are retrained and the loop\nmay repeat.\ncontinuously varying environment, the high dimensionality of the action space, our lack of knowledge about\nthe objective function’s true form, the need for relatively quick generation of control parameter actions, and\nthe potential for a partially observable environment leading to heteroscedasticity [28].\nUnlike models trained to predict the desired output metric based on inputs, including the control param-\neters, the philosophy of RL is to explicitly train an agent to learn an optimal policy, from which control\nparameter actions may be drawn. Here, we build our agent as an actor-critic neural network. Actor-critic\nnetworks are well-suited for this problem, as they generalize naturally to continuous action spaces and learn\nstochastic policies which allow our optimizing agent some degree of exploration. We show a schematic of the\ncontrol loop in figure 2a.\nThe sole input to this network is the environmental state vector, xe. The network output has two heads: the\nfirst head, known as the actor, seeks to output an optimal policy that chooses subsequent control parameter\nvalues, xc. Considering the continuous 30-dimensional control parameter space in this problem, the actor\nis learning a 30-dimensional normal distribution. As such, the actor head consists of 60 output nodes, half\nof which are learning-control-parameter means, while the other half are the associated variances. The mean\napproximating nodes are bounded using hyperbolic-tangent activation functions, allowing the output values\nto be compatible with our system’s hardware, while ensuring differentiability of the network through back-\npropagation. The variance-approximating nodes are followed by softplus activations, bounding the variances\n5\nto positive numbers while remaining differentiable. The other head, known as the critic, consists of a single\nnode. The critic learns a value function, which seeks to predict the resulting atom number while following the\nactor’s policy, given the input environmental state. The internal architecture of our actor-critic agent consists\nof four hidden layers, each comprising 128 neurons.\nThe network is initialized using the Glorot-Normal\nmethod [29], and network weight and bias updates are computed using the Adam algorithm [30]. To address\nthe issue of exploding gradients during training, we employ Scaled Exponential Linear Unit (SELU) activation\nfunctions for each hidden layer [31]. Through experimentation with various activation functions, we found\nthat SELU effectively mitigated the gradient instability. Furthermore, to prevent overfitting and promote\nsparsity in the network weights, we apply an L1-norm penalty term as regularization. This regularization\nconstraint not only helps prevent overfitting, but also has the potential to implicitly reduce the dimensionality\nof the problem [32].\nThe network’s critic node is optimized by iteratively adjusting the network’s weights and biases by mini-\nmizing the squared error\nδc = (log (N) −V (π)(xe))2,\n(1)\nwhere N is the measured atom number and V (π)(xe) is the critic’s value-function prediction, which indicates\nthe expected number of atoms (on a logarithmic scale) that the network predicts when control-parameter\nactions are chosen according to policy π, given the current environmental state xe.\nThe actor nodes are optimized according to the policy-gradient theorem [33].\nFor our case, in which\nsubsequent environmental parameters are decoupled from the previously chosen control parameters, the\npolicy-gradient method results in practice as the iterative minimization of actor loss\nδa = −log(π(xc|xe)) · (log (N) −V (π)(xe)),\n(2)\nwhere π(xc|xe) is the probability that control parameter action xc was chosen from the conditional proba-\nbilistic policy π(·|xe). The total non-L1-penalized loss function is thus a weighted sum of the two constituent\nloss functions\nL = αaδa + αcδc,\n(3)\nwhere the weighting coefficients αa,c are hyperparameters that can be thought of as different learning rates\nfor the actor and critic.\nDespite the comprehensive sensor array we deploy, the complex nature of atom cooling leads us to hy-\npothesize that the recorded environmental parameters may not fully capture the complete environmental\nstate. This limitation, known as the issue of partial observability, means that the agent has access to only a\nsubset of the pertinent information required for informed decision-making. Building upon prior research on\nautonomous curling robots [34], we enhance the actor-critic network by incorporating temporal information\nfrom the agent’s past performances. Specifically, we augment the environmental state with variables such\nas the previous atom number, the previous value function, and the probability of selecting the previous ac-\ntion based on the previous policy. Consequently, the agent can leverage not only the current environmental\nmeasurements but also its recent performance history when making decisions.\nThe vanilla policy gradient method used to train actor-critic models is generally an online algorithm, where\nthe network weights and biases are updated after every new experience tuple collected. Here, an experience\ntuple consists of the environmental parameters, control parameters, and the atom number: {xe, xc, log(N)}.\nTo ensure gradient-calculation stability, we maintain a small experience-replay buffer [35] for performing batch\nstochastic gradient descent. Each new experience tuple is appended to the beginning of the buffer, while\nthe oldest point is discarded. Additionally, to use the previously collected experiences from the supervised\nregression-based agent detailed below, we provide the actor-critic agent with a head start. We train the\nmodel on this bank of previously collected points as if they were being collected live, allowing the agent to\nbenefit from this initial training data.\nC.\nSupervised-Regression-Based Agent Design\nAs an alternative to RL, we implement a supervised ML technique, where a regression model maps a set\nof parameters to desired quantity, such as atom number; this class of ML is the one used in several previous\n6\nstudies investigating ultracold atom preparation [3–6, 8–11]. The universal function approximating models\ncommonly used for cold atom optimization are Gaussian processes [12] and artificial neural networks [13].\nHere, we use a deep, feed-forward, densely connected artificial neural network to control cycle-to-cycle control-\nparameter variations, which vary in reaction to environmental factors.\nThe input layer of the regression-based agent’s neural network accepts a vector x = (xc, xe) that concate-\nnates the control parameters and the environmental parameters, while the output layer predicts log (N). The\ninternal architecture of the network consists of four hidden layers, each with 128 neurons. Gaussian-Error\nLinear-Unit activation functions follow each hidden layer [36]. The network weights and biases are initialized\nusing the Glorot-normal method, and dynamically adjusted using the Adam algorithm. Performance evalua-\ntion is carried out using the Huber loss function [37]. To prevent overfitting, training is terminated when the\nmodel’s loss, validated on previously unseen data, no longer improves with subsequent epochs. It is important\nto note that, unlike the RL agent, we did not include additional elements in the environmental parameter\nvector to handle partial observability, such as information about the model’s previous performance. Including\nthese elements resulted in inferior control-parameter actions within our architecture.\nThe selection of control-parameter values in this scheme, as is common in similar experiments, relies on\nfinding the set that maximizes the network’s output. While neural networks may not in general possess explicit\nanalytical invertibility, approximate optimization methods can be used, such as Limited-memory BFGS [38],\nthe gradient-free Nelder-Mead [39] method, or probabilistic Bayesian optimization. In this experiment, we\nemploy the Nelder-Mead algorithm, as it yielded the best results given our time constraints.\nAlthough\nwe explored Bayesian optimization with Gaussian processes as priors, the computational feasibility was\nlimited due to the significant training time required for the Gaussian-process regressor: the computational\ncomplexity of the Gaussian-process regressor scales cubically with the number of points, making it impractical\nfor optimizing the neural network within a reasonable timeframe, i.e., before significant changes in the\nenvironmental state occur.\nWe select the control-parameter set that maximizes the atom number, given a specific environmental state,\nby partially optimizing the model while holding the environmental parameters constant in the current con-\nfiguration (Figure 2b). The control parameter set that maximizes the atom number, given an environmental\nparameter set, is\nx∗\nc = argmax\nxc\n{F(xc, xe)},\n(4)\nwhere xe denotes the current environmental state, and F(xc, xe) represents the network model approximating\nthe atom number as a function of control and environmental parameters.\nTo train the network, data is initially collected by sampling the control parameter space randomly. Once\n1000 such points have been collected, we proceed to construct the training set using an iterative feedback loop.\nIn this loop, the agent reads the current environmental state, adjusts the control parameters accordingly,\ncreates an atomic ensemble, and measures the number of atoms.\nThis experience tuple is added to the\ntraining data bank. By doing so, the network has an expanding training set to improve its model fitting.\nThe feedback loop continues until a training set of 7000 points is established. The size of this training data\nbank is large compared to similar experiments [3, 10], which is necessary because our model incorporates\nenvironmental parameters that our agent cannot directly control, meaning that many iterations are required\nto obtain a representative sampling of the combined control and environment space.\nIII.\nRESULTS\nA.\nReinforcement-Learning-Based Agent\nThe RL-based agent is initially trained on an offline-collected dataset and subsequently refined through\nlive training using the learned probabilistic policy. The training set ultimately comprises 10 005 experiences\ncollected over a period of approximately three months.\nTo evaluate the performance of the agent, we assess its actions by implementing the control-parameter\nactions deterministically, i.e., solely from the policy’s mean. For a comprehensive comparison, Figure 3a\nshows the agent’s actions with two alternative baselines: control parameters fixed at a human-optimized\nset, and fixed at the best set obtained from our regression-based agent. We assess the agent’s performance\nover a period of 160 experimental runs. This period is long enough to assess the capabilities of the agent,\n7\n2.0\n0.0\n-2.0\n-4.0\n4.0\ncritic prediction\nreinforcement learning\nregression-based: best\nmanual: best\n8.8\n8.6\n8.4\n8.2\n8.0\nLogarithmic atom number, log(N)\n3D-MOT cooling set-freq., f-f0 (MHz)\nValue function\nExample control parameter,  A24\n2.0\n0.0\n-2.0\n-4.0\n4.0\nReinforcement learning\nSupervised machine learning\na\nb\nc\nd\nFIG. 3: Machine learning performance. a In terms of the target atom-number value function, we compare\nthe reinforcement-learning actor-critic’s live control of the control parameters (top, dark green) vs. fixing\nthe control parameters at the best human-optimized set (red) and the best ever set from our\nregression-based agent (gold). Each run of these three approaches are performed one after another so that\nthe system is subjected to approximately the same environment for all three approaches, i.e., the RL agent\ntakes an action, followed by the fixed regression-agent derived action, followed by the human optimized\naction. Control parameters from the previous experimental run do not effect the current experiment, so the\nthree approaches act independently. Note here that a run for a given approach constitutes applying the\ncooling sequence, followed by extracting the resulting number of atoms from the TOF image. This pattern\nthen repeats 160 times. The critic’s prediction of the agent’s performance is also shown (light green). b\nMOT laser cooling frequency (control parameter A24) policy, showcasing the mean value (green), and one\nstandard deviation (gold). c Live performance of the supervised ML agent (green), noting that scale is\nexpanded compared to the RL results, in light of the large fluctuations in the performance. d Optimized\nMOT laser cooling frequency (control parameter A24) policy determined by the supervised ML agent.\nwhile being small relative to the size of the training set, so that any change to agent’s network weights\nwill have a negligible effect on performance. We find that the RL-based agent generates control parameter\nactions that outperform human optimization and the regression-based approach, and does so consistently\nand autonomously cycle-to-cycle. As an example of the control parameters used to generate this comparison,\nfigure 3b displays the MOT laser cooling frequency dimension of the multivariate policy.\nTo further assess the effectiveness of our actor-critic network, we examine the predictive capabilities of\nthe critic. Figure 3a additionally presents a comparison between the atom numbers obtained by following\nthe actor’s policy and the critic’s estimations. The critic’s ability to accurately predict the atom numbers\ngenerated by the actor’s policy serves as extra validation of the RL scheme, demonstrating the suitability of\nour environmental-state representation and network architecture.\n8\nB.\nSupervised-Regression-Based Agent\nWe retrain this agent on the full bank of experiences, including those collected by the RL agent’s ex-\nploration, thus resulting in a total data bank of 10 005 experiences. This experience data bank was then\nrandomly divided into training and validation sets using an 85:15 split. To ensure the robustness of this\napproach, we repeated the training process 500 times, each with different random partitions of the data. On\naverage, we observed a significant reduction in our loss metric when incorporating environmental parameters\ninto the model (Figure 4a). We further evaluated the predictive power of the model on a separate set of\ntesting points, where the control parameters were kept constant and atom-number fluctuations were driven\nby the environment only. Our model successfully predicted (varying) atom numbers over periods as long as\n2 hours, consisting of 160 iterations (Figure 4b).\nOur model successfully captures complex relationships between the control and environmental parameters,\nallowing it to accurately predict the corresponding number of atoms, more generally demonstrating that\nincorporating sensed environmental parameters into the model enhances its predictive performance.\nAfter training the regression-based agent and verifying its predictive capacity, we evaluate its performance\nin generating control-parameter actions in response to different environmental conditions. The effectiveness\nof the agent is assessed on two primary metrics: achieving a high overall atom number and maintaining\nconsistent and stable performance over time. To evaluate its live control capabilities, we grant the agent full\ncontrol of the control parameters for a continuous period of two hours. For comparison, we also assess the\nperformance by fixing the control parameters at the best configuration determined by human experts in our\nlaboratory.\nFigure 3c illustrates the results obtained from the agent’s live control and the human expert’s best set.\nWhile the agent occasionally achieves control parameter sets that surpass human performance, there remains\na significant variance in the resulting atom numbers, which is consistent with observations from similar\nexperiments. Over our 160 cycle showcase, the resulting log(N) generated by the agent has a variance of\n0.14, compared to fixing the control parameters at the human expert’s best set resulting in a log(N) variance\nof 0.01. In figure 3d, we show the agent’s chosen MOT laser cooling frequency as an example of the large\ncycle-to-cycle variability of the control parameters.\nThis variability limits the agent’s suitability for live\nadaptive control and emphasizes the necessity of RL techniques to address this challenge.\nAlthough regression-based agents may be inadequate for live control of atom cooling procedures, their\npredictive capability makes them valuable for experimental design purposes.\nBy leveraging this trained\nmodel, we can gain insights into the influence of different measured environmental parameters on the resulting\nnumber of atoms. This analysis helps experimental designers identify the critical components of the apparatus\nthat require improvement and optimization. To estimate parameter importance, we use a feature permutation\nalgorithm [40] (See C), and we present this metric for all parameters in figure 4c.\nIV.\nDISCUSSION\nIn a cold-atom experiment with a high-dimensional control-parameter space that is subject to a fluctuating\nenvironment, we find that optimizing control parameters with both a standard regression-based-ML protocol\nand a reinforcement-learning protocol result in larger numbers of atoms than achievable by our expert human\noptimizations. However, only the RL algorithm behaved more consistently than the human optimization.\nEven in what is likely an only partially observable environment, where the RL agent is not given the capability\nto fully counteract all the environment driven fluctuations, it is still able to outperform all other baselines\nconsistently.\nThe regression-based protocol exhibits strong predictive capability in estimating the atom number given\nthe current control and environmental parameters. Using optimization, it can effectively suggest high-quality\ncontrol parameter sets, although their variability limits their effectiveness for cycle-to-cycle optimization\ntasks.\nNevertheless, regression-based methods remain valuable in scenarios where continuous control is\nrequired in the presence of an influential environment, as they enable the extraction of estimations regarding\nthe importance of environmental parameters.\nWith careful selection of the network’s architecture and hyperparameters, the RL algorithms consistently\noutperform human optimization and our regression-based agent in generating control parameter actions. It\nis worth noting that due to the large environmental- and control-parameter spaces, reliable simulators for\ngenerating large training datasets are not available.\nAs a result, all iterations of the training loop were\n9\n0.0075 0.0100 0.0125 0.0150 0.0175 0.0200\nValidation loss\n0\n20\n40\n60\n80\nRealizations\n0.0105\n0.0153\nwith environment\nwithout environment\nE23\nEnvironmental parameter\nE21\nE19\nE20\nE14\nE18\nE10\nE26\nE11\nE13\nE15\nE8\nE17\nE7\nE16\nE27\nE22\nE28\nE3\nE24\nE9\nE12\nE6\nE29\nE1\nE4\nE30\nE2\nE5\nE25\nRelative importance\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\na\nc\n0\n40\n80\n120\n160\nRun number\n7.75\n8.00\n8.25\n8.50\nLogarithmic atom number, log(N)\nprediction with environment\nprediction without environment\nmeasured\nb\nFIG. 4: Supervised machine learning results. a Histograms of validation data loss, for models including\n(green) and excluding (gold) environmental parameters. The model is retrained 500 times with a different\nrandomly selected validation set each time. b Model prediction (gold) of drifting atom numbers (green).\nControl parameters are fixed, such that the drift is driven solely by environmental changes. The model’s\nprediction (red) is also shown when excluding the environment. c Importance of environmental parameters\nEi (see Appendix for parameter names). The five most important parameters (highlighted gold) include the\nroom temperature (E5) and humidity (E2), as well as the 3D-MOT cooling power (E25) and polarization\n(E30), and the 2D-MOT cooling power (E23).\nperformed live on the physical apparatus. Despite this challenge, the RL agent consistently generates high-\nquality control parameter actions. This suggests that when control and optimization of an atom cooling\napparatus is required, but large training datasets are not available, an RL policy-approximating agent could\nstill allow for the consistent production of high quality ensembles.\nOverall, while our findings suggest the applicability of RL actor-critic models in controlling the production\nof ultracold ensembles, we envision this as just the beginning of this burgeoning area of research. Additional\ncomputing power could allow more-expensive global optimization of neural network regression models, such\nas Bayesian optimization with Gaussian processes. This could enable regression-based approaches to achieve\nsimilar, or even superior, performance compared to RL. A high-performing supervised learning model, forming\na trustworthy mapping between the combined space of control and environmental parameters to resulting\natom number, would have the additional benefit of allowing experimentalists to see how control parameter\nsets predicted to be high-performing react to perturbations in the environmental parameters, thus giving\nan estimate of control parameter robustness.\nOther forms of RL such as Q-learning [41, 42] or Monte-\nCarlo policy gradients [43] may, too, hold potential for producing interesting results when applied to control\nand optimization of ultracold atom experiments. Additionally, discretization of the environment space to\nenable the application of standard contextual bandit solving algorithms like ϵ-greedy or gradient bandits is a\npotential avenue for investigation. Moreover, the incorporation of more complex reward schemes, including\nensemble temperature, cooling duty-cycle duration, and cloud shape, among others, could further enhance the\nperformance of such RL approaches. If stability is valued above all else, a reward scheme that is maximized\n10\nwhen the resulting atom number is equal to a target atom number could be employed. With a reward scheme\nconsidering phase space density along with atom number, and giving the agent control over the details of the\nevaporation trajectory, our RL protocol would naturally be extended to control the production of BEC. By\nintegrating these ML approaches with advanced environmental-state monitoring, such as magnetometry via\nthe Faraday effect [44] or even using a convolutional neural network to extract non-obvious environmental\nfactors reflected in the TOF images [45, 46], and implementing supplementary hardware configurations like\nadditional coils for magnetic-field compensation [47], RL control could emerge as a crucial tool in the toolkit\nof atomic physics research, and in other similarly high-dimensional experimental systems.\nNote added in preparation: While preparing this manuscript, we were made aware of a preprint demon-\nstrating reinforcement learning for a cold-atoms experiment [48].\nTABLE I: Agent controllable parametersparameters\nName\nNumber Stage\nImplementation\n2D coil 1\nA1\nMOT\ncurrent to 2D coil 1\n2D coil 2\nA2\nMOT\ncurrent to 2D coil 2\n2D coil 3\nA3\nMOT\ncurrent to 2D coil 3\n2D coil 4\nA4\nMOT\ncurrent to 2D coil 4\n3D x-bias\nA5\nMOT\ncurrent to x-axis bias coils\nA6\nSub-Doppler\nA7\nOptical pump\nA8\nMagnetic trap 1\nA9\nMagnetic trap 2\n3D y-bias\nA10\nMOT\ncurrent to y-axis bias coils\nA11\nSub-Doppler\nA12\nOptical pump\nA13\nMagnetic trap 1\nA14\nMagnetic trap 2\nA15\nImaging\n3D z-bias\nA16\nMOT\ncurrent to z-axis bias coils\nA17\nSub-Doppler\nA18\nOptical pump\nA19\nMagnetic trap 1\nA20\nMagnetic trap 2\n2D repump\nA21\nMOT\nlaser power, via AOM\n2D cooling\nA22\nMOT\nlaser power, via AOM\n2D push beam\nA23\nMOT\nlaser power, via AOM\n3D frequency\nA24\nMOT\nlaser frequency\n3D repump\nA25\nMOT\nlaser power, via AOM\n3D cooling\nA26\nMOT\nlaser power, via AOM\nSD start freq.\nA27\nSub-Doppler\nlaser frequency\nSD end freq.\nA28\nSub-Doppler\nlaser frequency\nOP power\nA29\nOptical pump\nlaser power, via AOM\nImaging freq.\nA30\nImaging\nlaser frequency\n11\nAppendix A: Control Parameters\nFor this investigation, we allow both the ML agents to control 30 parameters in the cooling sequence, as\nlisted in Table I, and identified by the letter A and a number. These parameters all play an important role\nin the cooling process.\nIn the 2D MOT, the magnetic field configuration is set by 2D-MOT coil currents A1-A4, while the efficiency\nof the cooling and re-pumping is controlled by the powers of the 2D-MOT laser cooling beams, A21 and A22.\nTransfer from the 2D- to 3D-MOT can be optimized by changing the acceleration of the atoms via push-beam\npower A23. Once in the 3D-MOT, laser frequency and powers are set by A24-A26, and the zero-point of the\nquadropole magnetic field may be shifted with x-, y-, and z-bias coil currents, A5, A10, and A16. Optimal\nvalues of the magnetic fields, laser powers and frequencies allows efficient cooling of a large range of velocity\nclasses via the optical molasses force, which depends on the intensities and detuning off resonance of the\nbeams.\nDuring sub-Doppler cooling, a range of atomic velocity classes are addressed via the start and end points\nof our laser’s frequency ramp, defined by A27 and A28, respectively. The bias magnetic fields during this\nstep, due to bias coil currents A6, A11, and A17, cancel out external fields.\nThe initial magnetic trap is preempted by an optical pumping beam, whose power is set by A29.\nA\nquantization axis is defined by the magnetic field along y, set via coil current A12, while additional bias fields\nalong x set via coil currents A7 and A18, cancel background fields. With the atoms in the MT, bias magnetic\nfields controlled by coil currents A8, A13, and A19 shift the zero point of the trap. When compressing the\ntrap, again the magnetic-gradient zero may be shifted this time with bias magnetic fields via coil currents\nA9, A14, and A20.\nFinally, the falling atoms are illuminated with the imaging beam of frequency A30, while the imaging\nquantization axis is defined by a magnetic field created by the y-bias coil current A15.\nAppendix B: Environmental Parameters\nIn this work, we monitor 30 environmental parameters relevant to our cooling process, labelled with E\nand a number (Table II). We monitor the majority of parameters at the end of each cooling cycle, which\ndecouples the measured environmental parameters from the chosen control parameter values. Exceptions to\nthis timing are E19-E21, which are measured during different stages of the cooling cycle (MOT for E19, MT\nfor E20, E21) via current transducer. To prevent interferences, we do not allow our agent to control the\nmaximum current or the current stability of our MOT and MT, and in this way, these parameters remain\nuncoupled from the control parameter actions of the agents.\nOur environmental measurements include E1, which measures the vacuum pressure of the oven chamber,\nwhich houses solid 87Rb used as a source, and is connected to the 2D-MOT chamber. The science chamber,\nwhere the 3D-MOT is located, is at too low a pressure to be measured by the ion pump. Higher vacuum\npressure may result in decreased trap lifetimes and inefficient cooling procedures.\nTwo probes are placed in our laboratory, each measuring room temperature and humidity. One is placed\nnext to our power supplies (E2 and E4), while the other is placed next to the 3D-MOT (E3 and E5). Temper-\nature and humidity both may have effects on the cooling procedure by changing the air index of refraction,\nslightly misaligning optical elements, and performance of the diode lasers. Furthermore, a thermocouple\nmeasures the temperature of the main magnetic coil used both in the 3D-MOT and MT. Changes in the coil\ntemperature would change the resistance of the circuit, and therefore could affect the performance of the\nmagnetic traps.\nUsing a vector magnetometer placed next to the 3D-MOT, we take multiple readings at the end of each\ncycle. Along the x−, y−, and z−axes respectively, we measure the magnetic field when the x-axis bias coils\nare active, alone, at a fixed current (E7, E11, E15), when the y-axis bias coils are active alone at a fixed\ncurrent (E8, E12, E16), when the z-axis bias coils are active alone at a fixed current (E9, E12, E17), and\nwhen the 3D-MOT/MT coils are active at a fixed current (E10, E14, E18). We use such a configuration to\ndetect unwanted external fields along all three Cartesian axes, as well as to detect potential issues with our\npower supplies.\nWe divert a small percentage of the 2D- and 3D-MOT’s repump and cooling light for monitoring of the\nbeam powers (E22-E25) and polarizations (E27-30). Drops in power and rotations of polarization would lead\nto inefficient cooling during both the MOT and sub-Doppler stages. The push-beam power is sensed after\n12\nTABLE II: Monitored environmental parameters\nName\nNumber Implementation\nVacuum pressure\nE1\nIon pump pressure gauge\nRoom temperature\nE2\nThermistor\nE3\nRoom humidity\nE4\nCapacitive humidity sensor,\nE5\nCoil temperature\nE6\nThermocouple\nmagnetic field x component\nE7\nMagnetometer\nE8\nE9\nE10\nmagnetic field y component\nE11\nMagnetometer\nE12\nE13\nE14\nmagnetic field z component\nE15\nMagnetometer\nE16\nE17\nE18\nMaximum MOT current\nE19\ncurrent transducer\nMaximum MT current\nE20\nMT current stability\nE21\n2D repump beam power\nE22\nPhotodiode\n2D cooling beam power\nE23\n3D repump beam power\nE24\n3D cooling beam power\nE25\npush beam power\nE26\n2D repump beam polarization\nE27\nPhotodiode pair &\n2D cooling beam polarization\nE28\npolarizing beamsplitter\n3D repump beam polarization\nE29\n3D cooling beam polarization\nE30\npassing through the entire vacuum chamber and exiting a window near the 3D-MOT. An underpowered push\nbeam fails to transfer the atoms optimally from the 2D-MOT to 3D-MOT, whereas if overpowered it can\ndestroy the MOT’s ability to effectively capture atoms.\nAppendix C: Parameter Importance Algorithm\nThe algorithm for estimating environmental parameter importance for our regression model is as follows:\n1. Train the model using an 85% randomly selected subset of the data bank, incorporating all environ-\nmental parameters.\n2. Evaluate the Huber loss for the predictions on the remaining 15% of the data, which serves as the\nvalidation set.\n3. Select an external parameter and randomly shuffle its values while keeping all other parameters un-\nchanged. Re-evaluate the Huber loss on the same validation set to quantify the increase in the loss\n13\nvalue, indicating the importance of the parameter.\n4. Repeat this process for each parameter column in the input array.\nWe repeat the algorithm 50 times, selecting different validation sets at random in each iteration.\nAppendix D: Estimating Atom Number From TOF Images\nAfter the compression of the MT, atoms are released from the trap and a destructive image is performed,\nallowing them a 10 ms TOF before they are illuminated with a F = 2 →F ′ = 3 resonant beam. Three\nimages are recorded by a CCD camera. The first image is timed to capture the atoms while they fall under\ngravity. The illuminating beam is partially absorbed by these atoms, casting a shadow on the camera. A\nsecond image is recorded after the atoms have fallen out of the field of view, with the absorbing beam on: this\nmeasures the nominal intensity of the absorbing beam. A third image is recorded next, with neither beam\nnor atoms, and thus captures the background illumination. By subtracting the third image from the first\ntwo, and using Beer’s extinction law along with the well-known absorption characteristics of the rubidium\natoms [23], we measure the number of atoms at the end of each experimental sequence.\nN =\nX\nnum pixels\nApixel\nσscs\nln CCD counts, no atoms\nCCD counts, with atoms,\n(D1)\nwhere Apixel is the area of single pixel, σscs is the resonant scattering cross-section, and the sum is performed\nover every pixel in camera’s field-of-view.\nWe show samples of experimental TOF images in figure 5. These images are processed composites of the\nthree raw images, allowing extraction of atom number estimates.\n0\n2000\n4000\n6000\n8000\n0\n2000\n4000\n6000\n0\n2000\n4000\n6000\n8000\n0.00\n0.25\n0.50\n0.75\n1.00\nOptical depth (normalized)\nx position ( m)\ny position ( m)\na\nb\nFIG. 5: Processed TOF images. Each image is within a 648 pixel by 488 pixel field of view, where the pixel\nsize is 14.8 µm. The colour scale shows the optical depth of the atoms in cloud, normalized to the\nmaximum. a Sample ensemble generated by a well performing control parameter set. We estimate 2.3 × 108\natoms (log N = 8.37) are present in this image. b Sample ensemble generated by a poor performing control\nparameter set. We estimate 3.37 × 107 (log N = 7.53) atoms are present in this image.\n14\nDATA AVAILABILITY\nThe data supporting the findings of this study are available within the article and supplementary informa-\ntion. The main dataset collected for the RL agent is publicly available at GitHub github.com/ultracoldYEG/\nAtom-Cooling-RL, along with a sample script training the RL agent offline in preparation for live deployment.\nAdditional data are available from the corresponding authors upon request.\nACKNOWLEDGEMENTS\nWe would like to thank Abilmansur Zhumabekov for his expertise and fruitful discussions, and Logan W.\nCooke, Benjamin D. Smith, and Taras Hrushevskyi for their work building and commissioning the apparatus.\nThis work was supported by the University of Alberta; the Natural Sciences and Engineering Research Council\n(NSERC), Canada (Grants No. RGPIN-2021-02884 and No. CREATE-495446-17); the Alberta Quantum\nMajor Innovation Fund; Alberta Innovates; the Canada Foundation for Innovation; and the Canada Research\nChairs (CRC) Program. We gratefully acknowledge that this work was performed on Treaty 6 territory, and\nas researchers at the University of Alberta, we respect the histories, languages, and cultures of First Nations,\nM´etis, Inuit, and all First Peoples of Canada, whose presence continues to enrich our vibrant community\nAUTHOR INFORMATION\nN.M., A.T., T.O., A.C., and L.J.L. built the physical experiment. N.M., A.T., T.O., and Z.F.A. designed\nthe optimization schemes. N.M., A.T., T.O., and A.C. collected training data. N.M., A.T., and L.J.L. wrote\nthe manuscript. All authors discussed the results and commented on the manuscript. The authors declare\nno competing interests.\n[1] Krenn, M., Erhard, M. & Zeilinger, A. Computer-inspired quantum experiments. Nature Reviews Physics 2,\n649–661 (2020).\n[2] Heck, R. et al. Remote optimization of an ultracold atoms experiment by experts and citizen scientists. Proceedings\nof the National Academy of Sciences 115, E11231–E11237 (2018).\n[3] Tranter, A. D. et al. Multiparameter optimisation of a magneto-optical trap using deep learning. Nature com-\nmunications 9, 4360 (2018).\n[4] Xu, S. et al. Maximizing the capture velocity of molecular magneto-optical traps with bayesian optimization.\nNew Journal of Physics 23, 063062 (2021).\n[5] Wigley, P. B. et al. Fast machine-learning online optimization of ultra-cold-atom experiments. Scientific reports\n6, 25890 (2016).\n[6] Nakamura, I., Kanemura, A., Nakaso, T., Yamamoto, R. & Fukuhara, T. Non-standard trajectories found by\nmachine learning for evaporative cooling of 87 rb atoms. Optics express 27, 20435–20443 (2019).\n[7] Wu, Y. et al. Active learning approach to optimization of experimental control. Chinese Physics Letters 37,\n103201 (2020).\n[8] Davletov, E. et al. Machine learning for achieving bose-einstein condensation of thulium atoms. Physical Review\nA 102, 011302 (2020).\n[9] Ma, J. et al. Bayesian optimization of bose-einstein condensation via evaporative cooling model. arXiv preprint\narXiv:2303.05358 (2023).\n[10] Barker, A. J. et al. Applying machine learning optimization methods to the production of a quantum gas. Machine\nLearning: Science and Technology 1, 015007 (2020).\n[11] Vendeiro, Z. et al. Machine-learning-accelerated bose-einstein condensation. Physical Review Research 4, 043216\n(2022).\n[12] Seeger, M. Gaussian processes for machine learning. International journal of neural systems 14, 69–106 (2004).\n[13] Mehta, P. et al. A high-bias, low-variance introduction to machine learning for physicists. Physics reports 810,\n1–124 (2019).\n[14] Snoek, J., Larochelle, H. & Adams, R. P.\nPractical bayesian optimization of machine learning algorithms.\nAdvances in neural information processing systems 25 (2012).\n15\n[15] Ding, Y., Chen, X., Magdalena-Benedito, R. & Mart´ın-Guerrero, J. D. Closed-loop control of a noisy qubit with\nreinforcement learning. Machine Learning: Science and Technology 4, 025020 (2023).\n[16] Bukov, M. et al. Reinforcement learning in different phases of quantum control. Physical Review X 8, 031086\n(2018).\n[17] Haug, T., Dumke, R., Kwek, L.-C., Miniatura, C. & Amico, L. Machine-learning engineering of quantum currents.\nPhysical Review Research 3, 013034 (2021).\n[18] C´ardenas-L´opez, F. A., Lamata, L., Retamal, J. C. & Solano, E. Multiqubit and multilevel quantum reinforcement\nlearning with quantum technologies. PloS one 13, e0200455 (2018).\n[19] Lamata, L. Basic protocols in quantum reinforcement learning with superconducting circuits. Scientific reports\n7, 1609 (2017).\n[20] Seif, A. et al. Machine learning assisted readout of trapped-ion qubits. Journal of Physics B: Atomic, Molecular\nand Optical Physics 51, 174006 (2018).\n[21] Praeger, M., Xie, Y., Grant-Jacob, J. A., Eason, R. W. & Mills, B. Playing optical tweezers with deep reinforce-\nment learning: in virtual, physical and augmented environments. Machine Learning: Science and Technology 2,\n035024 (2021).\n[22] Colabrese, S., Gustavsson, K., Celani, A. & Biferale, L. Flow navigation by smart microswimmers via reinforce-\nment learning. Physical review letters 118, 158004 (2017).\n[23] Ketterle, W., Durfee, D. & Stamper-Kurn, D. Making, probing and understanding Bose-Einstein condensates.\nIn Bose-Einstein Condensation in Atomic Gases, 67–176 (IOS Press, 1999).\n[24] Lin, Y.-J., Perry, A. R., Compton, R. L., Spielman, I. B. & Porto, J. V. Rapid production of 87Rb Bose-Einstein\ncondensates in a combined magnetic and optical potential. Phys. Rev. A 79, 063631 (2009).\n[25] Saglamyurek, E. et al. Storing short single-photon-level optical pulses in bose–einstein condensates for high-\nperformance quantum memory. New Journal of Physics 23, 043028 (2021).\n[26] Wang, C.-C., Kulkarni, S. R. & Poor, H. V. Bandit problems with side observations. IEEE Transactions on\nAutomatic Control 50, 338–355 (2005).\n[27] Sutton, R. S. & Barto, A. G. Reinforcement learning: An introduction (MIT press, 2018).\n[28] Le, Q. V., Smola, A. J. & Canu, S. Heteroscedastic gaussian process regression. In Proceedings of the 22nd\ninternational conference on Machine learning, 489–496 (2005).\n[29] Glorot, X. & Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proceedings\nof the thirteenth international conference on artificial intelligence and statistics, 249–256 (JMLR Workshop and\nConference Proceedings, 2010).\n[30] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n[31] Klambauer, G., Unterthiner, T., Mayr, A. & Hochreiter, S. Self-normalizing neural networks. Advances in neural\ninformation processing systems 30 (2017).\n[32] Candes, E. J., Wakin, M. B. & Boyd, S. P. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier\nanalysis and applications 14, 877–905 (2008).\n[33] Sutton, R. S., McAllester, D., Singh, S. & Mansour, Y. Policy gradient methods for reinforcement learning with\nfunction approximation. Advances in neural information processing systems 12 (1999).\n[34] Won, D.-O., M¨uller, K.-R. & Lee, S.-W. An adaptive deep reinforcement learning framework enables curling\nrobots with human-like performance in real-world conditions. Science Robotics 5, eabb9764 (2020).\n[35] Lin, L.-J.\nSelf-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine\nlearning 8, 293–321 (1992).\n[36] Hendrycks, D. & Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016).\n[37] Huber, P. J. Robust estimation of a location parameter. In Breakthroughs in statistics: Methodology and distri-\nbution, 492–518 (Springer, 1992).\n[38] Byrd, R. H., Lu, P., Nocedal, J. & Zhu, C. A limited memory algorithm for bound constrained optimization.\nSIAM Journal on scientific computing 16, 1190–1208 (1995).\n[39] Nelder, J. A. & Mead, R. A simplex method for function minimization. The computer journal 7, 308–313 (1965).\n[40] Fisher, A., Rudin, C. & Dominici, F. All models are wrong, but many are useful: Learning a variable’s importance\nby studying an entire class of prediction models simultaneously. J. Mach. Learn. Res. 20, 1–81 (2019).\n[41] Watkins, C. J. & Dayan, P. Q-learning. Machine learning 8, 279–292 (1992).\n[42] Gu, S., Lillicrap, T., Sutskever, I. & Levine, S. Continuous deep Q-learning with model-based acceleration. In\nInternational conference on machine learning, 2829–2838 (PMLR, 2016).\n[43] Lazaric, A., Restelli, M. & Bonarini, A. Reinforcement learning in continuous action spaces through sequential\nmonte carlo methods. Advances in neural information processing systems 20 (2007).\n[44] Budker, D., Kimball, D., Rochester, S., Yashchuk, V. & Zolotorev, M. Sensitive magnetometry based on nonlinear\nmagneto-optical rotation. Physical Review A 62, 043403 (2000).\n[45] K¨aming, N. et al.\nUnsupervised machine learning of topological phase transitions from experimental data.\nMachine Learning: Science and Technology 2, 035037 (2021).\n[46] Zhao, E. et al. Observing a topological phase transition with deep neural networks from experimental images of\nultracold atoms. Optics Express 30, 37786–37794 (2022).\n16\n[47] Li, J. et al. Bi-color atomic beam slower and magnetic field compensation for ultracold gases. AVS Quantum\nScience 4 (2022).\n[48] Reinschmidt, M., Fort´agh, J., G¨unther, A. & Volchkov, V. Reinforcement learning in ultracold atom experiments\n(2023). arxiv.org:2306.16764.\n",
  "categories": [
    "cond-mat.quant-gas",
    "physics.atom-ph"
  ],
  "published": "2023-08-09",
  "updated": "2023-12-30"
}