{
  "id": "http://arxiv.org/abs/2308.06739v1",
  "title": "Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks",
  "authors": [
    "David Junhao Zhang",
    "Mutian Xu",
    "Chuhui Xue",
    "Wenqing Zhang",
    "Xiaoguang Han",
    "Song Bai",
    "Mike Zheng Shou"
  ],
  "abstract": "Despite the rapid advancement of unsupervised learning in visual\nrepresentation, it requires training on large-scale datasets that demand costly\ndata collection, and pose additional challenges due to concerns regarding data\nprivacy. Recently, synthetic images generated by text-to-image diffusion\nmodels, have shown great potential for benefiting image recognition. Although\npromising, there has been inadequate exploration dedicated to unsupervised\nlearning on diffusion-generated images. To address this, we start by uncovering\nthat diffusion models' cross-attention layers inherently provide\nannotation-free attention masks aligned with corresponding text inputs on\ngenerated images. We then investigate the problems of three prevalent\nunsupervised learning techniques ( i.e., contrastive learning, masked modeling,\nand vision-language pretraining) and introduce customized solutions by fully\nexploiting the aforementioned free attention masks. Our approach is validated\nthrough extensive experiments that show consistent improvements in baseline\nmodels across various downstream tasks, including image classification,\ndetection, segmentation, and image-text retrieval. By utilizing our method, it\nis possible to close the performance gap between unsupervised pretraining on\nsynthetic data and real-world scenarios.",
  "text": "Free-ATM: Exploring Unsupervised Learning on\nDiffusion-Generated Images with\nFree ATtention Masks\nDavid Junhao Zhang1∗,\nMutian Xu2,4,\nChuhui Xue3, Wenqing Zhang3, Xiaoguang Han2,4, Song Bai3, Mike Zheng Shou1†\n1 Show Lab, National University of Singapore 2 SSE, CUHKSZ\n3 ByteDance 4 FNii, CUHKSZ\nAbstract\nDespite the rapid advancement of unsupervised learning in visual representation,\nit requires training on large-scale datasets that demand costly data collection,\nand pose additional challenges due to concerns regarding data privacy. Recently,\nsynthetic images generated by text-to-image diffusion models, have shown great\npotential for benefiting image recognition. Although promising, there has been\ninadequate exploration dedicated to unsupervised learning on diffusion-generated\nimages. To address this, we start by uncovering that diffusion models’ cross-\nattention layers inherently provide annotation-free attention masks aligned with\ncorresponding text inputs on generated images. We then investigate the problems of\nthree prevalent unsupervised learning techniques (i.e., contrastive learning, masked\nmodeling, and vision-language pretraining) and introduce customized solutions by\nfully exploiting the aforementioned free attention masks. Our approach is validated\nthrough extensive experiments that show consistent improvements in baseline\nmodels across various downstream tasks, including image classification, detection,\nsegmentation, and image-text retrieval. By utilizing our method, it is possible to\nclose the performance gap between unsupervised pretraining on synthetic data and\nreal-world scenarios.\n1\nIntroduction\nUnsupervised learning is a type of machine learning where models learn to identify patterns or\nstructures in data without explicit labels. In the past few years, several unsupervised learning\ntechniques have emerged, including contrastive learning [1, 2, 3, 4], masked modeling [5, 6], and\nvision-language pretraining [7, 8, 9], etc. Although these advancements have led to significant\nprogress in visual representation learning, the majority of them rely on pretraining on large-scale\ndatasets, such as ImageNet [10] which contains millions of images. However, manually building\na sizable dataset with decent richness and diversity is often time-consuming and costly. Moreover,\npresent-day concerns about data privacy and usage rights further complicated the acquisition of\nmassive data [11], creating additional obstacles to the development of unsupervised learning.\nTo overcome these challenges, using synthetic data for unsupervised pretraining presents itself as a\nlogical option, given its advantageous characteristics such as cost-effectiveness, virtually limitless\nscalability, enhanced control over data distribution, and improved data privacy and security.\n∗Work is partially done during an internship at ByteDance.\n†Corresponding Author.\nPreprint. Under review.\narXiv:2308.06739v1  [cs.CV]  13 Aug 2023\nFree Attention\nFree Attention\nA blue jay\nA girl and a dog are sitting on the beach\nFree Attention\nFree Attention\nA sunglass\nA corgi dog\nFigure 1: Visualization of diffusion-generated images with freely available attention masks.\nIn the computer vision area, there have been some attempts that leverage synthetic data for image\nrecognition tasks. Besnier et al. [12] and Zhao et al. [13] both employ BigGAN [14] to produce\ninformative images for training image classifiers. DatasetGAN [15] and BigDatasetGAN [16] adopt\nStyleGAN [17] and BigGAN [14] for generating images with pixel-wise labels for segmentation tasks.\nIn addition to using GANs, He et al. [18] finds that the revolutionary text-to-image diffusion models\nsuch as GLIDE [19] can generate not only high-quality but also diverse images in a customized label\nspace for benefiting image recognition. This recent study is noteworthy for firstly demonstrating\npromising results of image understanding using diffusion-generated images. Albeit promising, there\nhas been a lack of in-depth exploration focusing on unsupervised learning on diffusion-generated\ndata. We attempt to remedy this defect from the perspective of both diffusion-generated data and\nunsupervised models.\ni) In contrast to class-specific GANs that can only generate images of individual objects, text-to-image\ndiffusion models have the capability to produce diverse images featuring multiple objects by utilizing\ndifferent text tokens. More importantly, we find that the cross-attention layers of diffusion models\nnaturally provide semantic attention masks aligned with corresponding text inputs on generated\nimages without any manual annotations (also indicated in [20, 21]), which helps to locate each\nforeground object as shown in Fig. 1. ii) Next, in view of unsupervised models, we examine the\nshortcomings of three commonly used unsupervised learning schemes and investigate how the\nattention masks of diffusion-generated images can be used as a free resource to enhance unsupervised\nlearning on synthetic data.\nContrastive Learning (CL) aims to learn representations that can distinguish between positive pairs\n(similar examples) and negative pairs (dissimilar examples). Conventional techniques [1, 2, 3, 4, 22]\ntreat an image with a single object as a complete entity and conduct random crop and augmentations\nto get positive pairs at the image level. Yet, such a paradigm is not suitable for diffusion-generated\nimages containing multiple objects. As discussed earlier, the diversity of diffusion-generated images,\nwhich often include multiple instances, presents a significant challenge for traditional random crops\nin contrastive learning. This is due to the high risk of positive pairs being originated from distinct\ninstances, resulting in ambiguity in model training as the discriminate features of each instance are\npulled in (see top row of Fig. 2 (a)). To mitigate this issue, we leverage the free attention masks\nfrom diffusion generators to ensure that each positive pair comes from the same object. Meanwhile,\nnegative pairs are formed by selecting different instances of images based on their corresponding\nmasks (see bottom row of Fig. 2 (a)). Our approach aids in the acquisition of precise information by\nthe CL model.\nMasked Modeling (MM) recently achieves widespread success in various vision challenges by\nreconstructing masked visual patches [5, 6, 23]. A new study [24] proposes to identify the patches\nwhich are hard to reconstruct, and the study’s outcomes reveal that such patches are consistently\nlocated within foreground objects. By restoring these particular patches rather than randomly-masked\nones, the network can acquire more focused features. Inspired by this, as indicated in Fig. 2 (b), we\nemploy the freely available attention map from diffusion generators and present a balanced masking\ntechnique for masked modeling that gradually increases the masking ratio of foreground object\npatches. Our strategy enables the MM model to learn both universal and targeted representations.\nVision-and-Language Pretraining (VLP) is developed to jointly pretrain visual and language features\nusing image-text matching by learning to align and translate between the two modalities [7, 8, 9].\nVLP models predominantly rely on position features, such as those belonging to the objects of\ninterest in an image, to gain a better understanding of the relationships between words and objects.\n2\nInstance level\nImage level\ncrop\n(a) Contrastive Learning\n(c) VL-Pretraining\n(b) Mask Modeling\ncrop\nRandom\nOurs\nObject\ndetector\n～900ms\nThe girl is in block 5\nThe dog is in block 9\n～10ms\nAttention Position\nDetection\nFigure 2: Existing problems in current unsupervised learning frameworks and our solutions.\nNonetheless, locating these features through the use of bounding boxes predicted by object detection\nalgorithms can be a time-consuming process. Additionally, the quality of the position features is\nheavily dependent on the performance of the object detectors, which may ultimately limit the strength\nof VLP models. Fortunately, the attention maps from text-to-image diffusion models naturally align\neach text prompt with its corresponding object position. As shown in Fig. 2 (c), we apply attention\nmasks to supply position information without requiring the extra step of object detection. This not\nonly brings greater efficiency to VLP models, but also enhances their overall effectiveness.\nOur proposed frameworks, which rely on annotation-Free ATtention Masks from diffusion generators,\nare collectively referred to as Free-ATM. Extensive experiments show that our method of conducting\nunsupervised pretraining on diffusion-generated data consistently improves the baseline performance\non large-scale real-world benchmarks, including PASVOC [25], COCO [26], Cityspace [27], and\nADE20K [28], across various downstream tasks like image classification, detection, segmentation,\nand image-text retrieval. Besides, our Free-ATM achieves significantly better results compared\nto simply applying original unsupervised pretraining protocols on diffusion-generated data. By\nleveraging Free-ATM, the performance gap between unsupervised pretraining on synthetic data and\nreal-world scenarios can possibly be closed. Moreover, mixing synthetic data with real-world data\nfor pretraining can further boost performance.\n2\nRelated Work\nText-to-Image Diffusion Models. Diffusion models [29, 30, 31, 32, 33] have been making waves\nin the generative model’s landscape, primarily due to their unique approach to generating new data\nsamples. These models commence with a straightforward random noise and progressively denoise\nit through numerous steps with learned transformations until it mirrors a sample from the desired\ndata distribution. Recently, large-scale text-to-image diffusion models such as Stable Diffusion [34],\nImagen [35], and GLIDE [19] have made considerable strides and produced striking visual outcomes.\nRombach et al. proposed an approach where the diffusion process takes place in the latent space,\nutilizing a UNet [36] to predict noise and a VAE [37] decoder to convert the latent feature into pixel\nspace. This approach streamlined the text-to-image diffusion process and accelerated it, becoming a\npopular choice for diffusion models. Building on the latent diffusion model, Hertz et al. [20] discover\nthat the cross-attention map, which is employed for text-visual interaction in UNet, can accurately\nrepresent the foreground object when given the appropriate prompt. They leveraged this characteristic\nto manipulate images as per requirements by directly modifying the attention map. Similarly, Zhao et\nal. [21] use the UNet of the diffusion models as the core structure and extracted the cross-attention\nmap to boost the zero-shot segmentation performance.\nIn our study, we use the latent diffusion model to generate images in line with the text prompts and to\nextract the cross-attention map. Using human annotation-free sources, we investigate how attention\nmaps, serving as an additional resource, can enhance unsupervised learning on synthetic images.\nSynthetic Data from Generative Models. Generative Adversarial Networks (GANs) [38] have the\ncapability to produce highly realistic and superior quality images. There are numerous studies that\nutilize GANs to synthesize datasets akin to ImageNet [10]. Li et al. [16] present BigdatasetGAN, a\nmethod that generates a vast amount of images corresponding to ImageNet classes with pixel-level\n3\nlabels, but it necessitates additional human annotation. Jahanian et al. [39] propose a unique method\nthat employs a GAN to generate multiple views of an image, which are then used as positive pairs for\ncontrastive learning. Recently, diffusion models have been gaining precedence in the field of image\ngeneration. He et al. [18] demonstrate that innovative text-to-image diffusion models, such as GLIDE\n[19], can produce data in a custom label space for image recognition. Furthermore, works by Azizi et\nal. [40] and Mert Bulent et al. [41] employ Imagen [35] and Stable Diffusion [34] to generate images\nwith class labels, thereby improving supervised classification performance. Adding to this, Brandon\net al. utilize diffusion model inversion to augment images for classifying small datasets.\nRather than focusing on task-specific synthetic data manipulation, we delve into the broader realm of\nunsupervised learning on synthetic images. Notably, we leverage the overlooked attention masks,\nwhich can offer pixel-level labels without any need for human annotations, thus adding a new\ndimension to unsupervised learning.\nUnsupervised Learning. (i) Contrastive Learning is built upon the foundational idea of drawing\npositive pairs nearer while distancing negative pairs in the representational space. This method\nhas proven to be effective in learning visual representations without the need for labeled data\n[42, 43, 44, 45, 46, 47, 48, 49]. A significant advancement in this area is the SimCLR framework\n[1], which has substantially improved the quality of the learned representations via a non-linear\ntransformation head. MoCo [4], another impactful work, maintains a memory bank for a vast array\nof negative samples, and employs a momentum-based method for gentle updates, ensuring improved\nconsistency during learning. (ii) Masked Modeling [5, 50, 51, 52, 23], such as MAE [5], SimMIM\n[6], and iBOT [50], use masked patch reconstruction in combination with basic data augmentation to\neffectively learn robust representations. (iii) Vision-Language Pre-training (VLP), exemplified by\nmodels like CLIP [7], BLIP [8], and ViLT [9], seeks to boost performance on downstream vision and\nlanguage tasks by pretraining the model on large-scale image-text pairs to align visual and textual\nfeatures without any task-specific supervision.\nIn this work, we propose tailored approaches that leverage the freely available attention masks from\ndiffusion generators to enhance the above-mentioned three unsupervised learning frameworks.\n3\nMethod\n3.1\nAttention Mask from Text-to-Image Diffusion Model\nLatent Text-to-Image Diffusion model [34] is a generative model that uses a text prompt to create\nhigh-quality images through a controlled diffusion process. It first encodes the text prompt into a\nlatent space representation, then uses a diffusion process to gradually transform a noise input into the\nfinal image, guided by the encoded text. The model represents the joint understanding of textual and\nvisual data via cross-attention interaction in UNet. Specifically, in the single diffusion step and layer,\nthe text embedding is projected into key as K ∈RL×C and the visual noise is projected into query as\nQ ∈RH×W ×C, where L is the text sequence length, H, W are height and width of visual feature\nand C is the feature dimension. The cross-attention map is achieved by the multiplication of Q and\nK, resulting in\nA = Softmax\n\u0012QKT\n√\nd\n\u0013\n,\n(1)\nwhere A ∈RH×W ×L stands as the attention map, illustrating the relationship between textual and\nvisual elements. The attention map a ∈RH×W , corresponding to specific nouns such as ‘dog’ in\nan L length sentence, is selected. Following this, we compile the attention maps derived from every\nlayer and time step within the diffusion models. These maps are then resized and averaged to form a\nnew map. The visualizations are shown in Fig. 1.\n3.2\nPrompts Generation\nWe employ the ImageNet [10] label-space as a prompt cue to create synthetic images. It is vital\nfor the model to encounter a wide variety of images in order to learn universal representations\napplicable to various downstream tasks. A straightforward strategy to diversify synthetic images\nis to leverage a large language model to transform labels into sentences, effectively augmenting\nthe prompts. However, simply utilizing a large model like T5 [53], as suggested by He et al. [18],\n4\ncould result in unrealistic prompts, thereby producing unrealistic images. This may inadvertently\nwiden the gap between synthetic and real domains, leading to an inferior performance on downstream\ntasks. To generate more realistic and diverse images, we utilize GPT-3.5-turbo [54] to augment\nprompts. The augmentation templates vary based on the hierarchical level of classes in ImageNet,\nfor example, “[Class (with other class)] is/are [somewhere]” or “[Class ] with [other class] is/are\n[doing something] [somewhere]\".\n3.3\nUtilize Attention Masks for Unsupervised Learning\nUpon generating diverse images and a complimentary attention mask via augmented prompts used\nas input to diffusion models, we suggest modifications to three common unsupervised learning\nframeworks: Contrastive Learning, Masked Modeling, and Vision-Language Pretraining. These\nproposed adjustments fully exploit the attention masks to augment unsupervised learning, thereby\nenhancing performance on downstream tasks.\n3.3.1\nContrastive Learning\nWe adopt two widely used contrastive learning methods - SimCLR [1] and MoCo-v2 [22], to incor-\nporate the use of free attention masks. For ease of explanation, we use SimCLR as a representative\nexample, as depicted in Fig. 3 (a). As previously discussed in the introduction, using image-level\nfeatures can be problematic when an image contains multiple instances, such as a girl and a dog.\nThe augmented positive pairs, after random cropping, may contain different instances. This could\nnegatively impact network training, as the distinct instance features that should be differentiated are\ninstead conflated. To mitigate this, we use instance features based on the attention mask in place of\nimage features.\nSpecifically, given an image, we apply a series of augmentations and a random crop, resulting in\ntwo cropped images, denoted as x and x′. Concurrently, our instance attention masks are cropped in\nline with the image operation, yielding two sets of masks a1, · · · , aN, a′1, · · · , a′N, with each set\ncontaining N instances. These two image crops are then input into the encoder (e.g., ResNet50 [55]),\nresulting in outputs z = f(x), z′ = f(x′). At this stage, we utilize the features z ∈Rh×w×c, z′ ∈\nRh×w×c, which are derived prior to the last average pooling layer. Next, the m−th instance attention\nmasks am, a′m, resized to match the spatial resolution of the encoded features, are mapped to the\nfeatures z and z′, thereby applying attentive pooling. This process results in:\nzm =\n1\nP\ni,j ai,j\nh,w\nX\ni,j\nai,jzi,j,\n(2)\nand z′m ∈RC. Following the application of attention pooling, the features zm and z′m are tran-\nsitioned from the image level to the instance level. Subsequently, a straightforward Multilayer\nPerceptron (MLP) layer is applied to these features. For instance-level features, we redefine the\ncontrastive loss for zm as\nl = −log\nexp(sim(zm · z′m))\nexp(sim(zm · z′m)) + PN\nn=1[m̸=n] exp(sim(zm · zn))\n,\n(3)\nwhere sim (zm, z′m) =\nzm⊤·z′m\n|zm|·|z′m|. In this loss computation, we consider the features of the same\ninstance from the two crops as the positive pair, and the features of different instances as negative\npairs. In Equation (3), N signifies the total number of instances in the image. When extended to the\nbatch dimension, N can represent the total number of instances in the batch.\nFor MoCo-v2, the encoders for the two image crops are distinct. One encoder is updated by the\nExponential Moving Average (EMA) [4] of the other encoder. Furthermore, instance-level features,\nas opposed to image-level, are updated and stored in the memory bank. In addition to addressing\nthe issue where positive pairs may comprise different instances, this strategy enables every image to\nprovide a wealth of information. This greatly aids the network and allows for the learning of a more\ndiverse representation, given the fact that each image typically encompasses multiple instances.\n5\nIns. n Crop1\nIns. n Crop2\nEncoder\nEncoder\nIns. 1 Crop1\nIns. 1 Crop2\nAttention Map\nAttention Map\nAttentive Pooling\nthe dog is in block 9\n1 2 3\n4 5 6\n7 8 9\n9\n8\n7\n4 5 6\n1 2 3\nThe girl is in block 5,\nA girl and a dog are sitting on the beach\n(a) Instance Level Contrastive Learning\n(c) Vision-Language Pretraining with Attention Position\nTraining Epochs\n(b) Mask Modeling with Attention\nImage Encoder\nText Encoder\nFusion\nFigure 3: Adaptions for three frameworks to utilize free attention masks.\n3.3.2\nMasked Modeling\nAs indicated in [24], certain image patches can be challenging to reconstruct, and these often represent\nthe foreground objects in images. Prioritizing the reconstruction of these difficult patches aids the\nnetwork in learning a more discriminative representation. To accomplish this, Wang et al. [24]\nintroduce an additional teacher-student network to predict these difficult patches. However, deploying\nan extra teacher-student model will bring extra computation costs and complicate the learning process.\nIn contrast, our free attention mask naturally embodies the importance score of the foreground object\nmask. The scores of the attention map, ranging from low to high, correspond to patches from easy to\ndifficult. This allows us to discard the teacher-student model for identifying challenging patches.\nAn intuitive approach would be to mask the patches with the highest attention map scores and\nthen use pretraining from scratch to reconstruct the masked images. However, solely focusing on\nreconstructing the difficult patches may cause the network to overly concentrate on the foreground\nobject, which could be detrimental to learning a more universal representation of the entire image. To\nmitigate this, during the initial stages of training, we continue to mask the patches randomly. As the\ntraining epochs increase (Fig. 3 (b)), we gradually raise the ratio of masked patches determined by the\nhighest importance scores, and reduce the ratio of randomly selected masked patches. This balanced\napproach enables the network to learn both universal and targeted representations simultaneously.\n3.3.3\nVision-Language Pretraining.\nThe capacity for position grounding [56, 57] is vital for a vision-language model to excel in cross-\nmodality downstream tasks. Some studies strive to enhance this capability by incorporating bounding\nbox and region features as additional visual inputs during vision-language pretraining. However,\nobtaining these features and bounding boxes for the objects in the image necessitates the use of\na robust, pre-trained offline detection model, such as Fast-RCNN [58]. This process can be time-\nconsuming and often results in a significant increase in the parameters for the vision-language models.\nIn contrast, our synthetic data inherently includes the bounding box of the object (nouns). This is\nmade possible as we can readily transform the attention mask into a binary mask, with pixels marked\nas ‘1’ representing the foreground region, thereby allowing us to obtain the bounding box.\nRather than extracting regions using bounding boxes as inputs for the visual encoder, we employ\nposition-aware prompts, inspired by [59], which does not impose additional parameters or compu-\ntational demands on the vision-language model. As illustrated in Fig. 3 (c), we initially divide the\nimage into N blocks. After determining the bounding box of the object, we can identify the block\nin which the object’s center is located. Using this information, we generate position-aware prompts\nfollowing the template: “ The [O] is in block [P].”\n6\nSubsequently, we concatenate these prompts for all objects in the images with the original prompt.\nThis forms the input for the text encoder, enhancing the position-grounding ability of the vision-\nlanguage model. The model in focus, BILP, is adapted to our needs for Vision-Language Pretraining\n(VLP). We then conduct end-to-end training using conventional objectives. Consistent with the\nmethodologies outlined in [8, 9, 60], the training process involves the use of Language Modeling\n(LM) loss, Image-Text Matching (ITM) loss, and Image-Text Contrastive (ITC) loss. Note that the\nobject’s positional information is only required during the pre-training stage. For downstream tasks,\nwe evaluate the model using standard end-to-end methods, without the need for object information.\n4\nExperiments\nIn the process of pretraining for contrastive learning and masked modeling, we generate approximately\n1.2 million images using augmented prompts, a quantity that matches precisely the original ImageNet-\n1K [10] dataset for fair comparisons. For the supervised pretraining phase, we utilize real data tagged\nwith class labels. As for pretraining in the vision-language task, we select a subset from CC3M [61],\nwhich encompasses 0.3 million image-text pairs, and employ the original captions to generate images.\n4.1\nContrastive Learning\n4.1.1\nSimCLR\nPretraining. We employ ResNet50 [62] as the encoder for our pretraining. To maintain fairness\nin our comparisons, we adhere to the same 200 pretraining epochs across all settings, and all\nhyperparameters for training are aligned with those outlined in the original SimCLR paper[1]. As\noutlined in Tab. 1, various terms denote different pretraining methods. ‘Random init’ implies\nno pretraining, ’supervised’ refers to pretraining with ImageNet&labels, and ‘real’ indicates self-\nsupervised pretraining on ImageNet. ‘Synthetic’ stands for self-supervised pretraining on purely\nsynthetic images, while ‘synthetic w/ ours’ signifies our adapted method using synthetic images with\nfree attention masks. ‘Mix’ involves a blend of all masked synthetic and real images.\nObject Detection and Segmentation. The pretrained network is utilized to initialize our feature\nextractor. For object detection and instance segmentation in COCO [26], we, in accordance with\n[4], modify the Mask-RCNN [63] to be equipped with feature pyramid networks. The complete\nmodel is fine-tuned on the training dataset, following a standard 1x schedule (12 epochs), and we\nreport the results as bounding box AP (AP b) and instance mask AP(AP m). In the case of semantic\nsegmentation for Cityspace [27], we fine-tune the model over a span of 160 epochs, reporting the\nresults in terms of mIoU (mean Intersection over Union).\nResults. As shown in Tab. 1, self-supervised pretraining using purely synthetic data leads to a\nnoticeable performance discrepancy on the COCO and City Space datasets when compared to the\nuse of purely real data. However, with the introduction of the free attention mask, we observe a\nmarked improvement in results: an increase of 1.3% and 1.1% on the COCO dataset, and 0.8% on\nthe CitySpace dataset. This improvement narrows the gap with real data to an almost negligible\nlevel. These findings underscore the efficacy of our approach to instance-level contrastive learning,\nfacilitated by the use of free attention masks. Moreover, when combining both real and synthetic data,\nwe see not only an enhancement in performance but also a surpassing of the results achieved using\npurely real data. This is accomplished without incurring any costs associated with human annotation\nor data collection.\n4.1.2\nMoCo-v2\nPretraining. We employ ResNet-50 as the encoder and train it for a total of 200 epochs. During this\ntraining phase, we adopt a learning rate adjustment strategy wherein the learning rate is multiplied by\n0.1 at two specific points: the 120th and the 160th epochs. This adjustment is applied consistently\nacross all experimental settings. Aside from these specifics, all other training hyperparameters strictly\nadhere to the guidelines and recommendations provided in the original MoCo-v2 paper [22]. This\napproach ensures that our training process is rigorous and consistent with established best practices.\nObject Detection and Semantic Segmentation. In the context of object detection evaluation, we\nadhere to the standard protocol of fine-tuning a Faster R-CNN detector (with a C4 backbone) on the\n7\nTable 1: SimCLR [1] downstream results.\nCOCO\nCityspace\nAP b\nAP m\nmIoU\nrandom ini\n31.4\n28.5\n65.2\nsupervised\n39.2\n35.5\n74.2\nreal\n37.7\n34.2\n74.8\nsynthetic\n36.4\n33.1\n73.7\nsynthetic w/ ours 37.7 (+1.3) 34.2 (+1.1) 74.5(+0.8)\nmix w/ ours\n38.7\n34.8\n75.0\nTable 2: BLIP [8] image-text retrieval results.\nMS-COCO\ntr@1\nir@1\nreal\n58.1\n44.2\nVQGAN [64]\n35.6\n32.0\nDALL-E2 [65]\n44.5\n38.6\nStable [34]\n52.3\n40.9\nw/ ours\n54.9 (+2.6) 43.8 (+2.9)\nmix w/ ours\n60.8\n46.2\nTable 3: MoCo-v2 [22] downstream results.\nPASVOC\nCOCO\nCitysapce\nAP b\n50\nAP b\nmIoU\nrandom ini\n59.0\n31.4\n65.2\nsupervised\n81.6\n39.2\n74.2\nreal\n82.4\n39.8\n75.0\nsynthetic\n81.6\n37.9\n74.1\nsynthetic w/ ours 82.1 (+0.5) 38.3 (+0.4) 74.8 (+0.7)\nmix w/ ours\n82.6\n40.1\n75.3\nTable 4: MAE [5] downstream results.\nImageNet\nADE20K\nacc\nmIoU\nsupervised\n81.0\n47.4\nreal\n83.6\n48.1\nsynthetic\n82.7\n47.6\nsynthetic w/ ours 83.2 (+0.5) 48.0 (+0.4)\nmix w/ ours\n83.8\n48.5\nVOC trainval07+12 set, as per the 2x schedule mentioned in [49]. The evaluation is then carried out\non the VOC test2007 set. In evaluating COCO detection, our fine-tuning strategies remain consistent\nwith those applied in the previously discussed SimCLR framework. When it comes to semantic\nsegmentation in Cityspace, we utilize an FCN [66] model. This model is trained on the train-fine\ndataset over 40k iterations. The testing is then carried out on the validation dataset.\nResults. Tab. 3 reveals a pattern similar to that observed with SimCLR: when pretraining is conducted\nsolely on synthetic images, there is a noticeable performance gap compared to pretraining on real\nimages. Yet, interestingly, the results obtained using only synthetic images can rival and even\nsurpass those achieved through supervised learning on real images, a process that typically requires\nsubstantial human annotation and collection efforts. Furthermore, the application of free attention\nmasks enhances the performance on synthetic images to a level comparable with that on real data,\neffectively bridging the gap between synthetic and real images. When pretraining incorporates both\nsynthetic and real images, the results show an improvement over both supervised learning with real\ndata and self-supervised pretraining, all without the need for human labor.\n4.2\nMasked Modeling\nPretraining. We employ MAE [5], a representative masked modeling method. We follow its original\npretraining protocol for the ViT-B [67], spanning a total of 1600 epochs. The overall mask ratio is set\nat 75%. Additionally, we adopt a strategy to incrementally increase the ratio β of masked patches\naccording to the highest attention score, with a ceiling of 0.8, following a linear progression as epochs\nincreasing. Thus, a portion of the masked patches, specifically (β × 0.75) of them, are determined\nbased on the attention score. The remaining masked patches, which account for ((1 −β) × 0.75) of\nthe total, are selected at random.\nImage Classification and Semantic Segmentation. For classification tasks, we utilize the Masked\nAuto Encoder (MAE) [5]. We carry out end-to-end fine-tuning for 100 epochs on the ImageNet-1K\ntraining set and subsequently report the top-1 accuracy on the validation set. For segmentation tasks,\nwe use UperNet [68] as the segmentation head. This is then subjected to end-to-end fine-tuning on\nthe ADE20K [28] dataset for a total of 160k iterations.\n8\nResults. As shown in Tab. 4, MAE [5] achieves consistent trend results when compared to MoCo-v2.\nUtilizing free attention masks can assist in bridging the discrepancy between synthetic and real data.\nAdditionally, by exclusively employing synthetic data, which does not require human annotation or\ncollection costs, we can significantly outperform fully annotated supervised training (83.8 vs. 81.0).\n4.3\nVision Language Pretraining\nPretraining. We train the vision-language model following BLIP [8] on 0.3 M text-image pairs until\nthe loss converges.\nCOCO-Retrieval. We evaluate BLIP on the COCO datasets, focusing on image-to-text retrieval\n(TR) and text-to-image retrieval (IR). The pre-trained model is fine-tuned using both ITC and ITM\nloss functions. Additionally, we implement a re-ranking strategy to further refine the retrieval results.\nResults. Tab. 2 illustrates that with free attention mask, which provides position-aware prompts,\nenables the model to attain a substantial improvement over results obtained using purely synthetic\ndata. Furthermore, combining synthetic data with the mask and real data considerably enhances the\nresults, all without incurring any costs associated with human annotation or data collection.\n4.4\nAblation Study\nImage quality. Stable Diffusion is the most effective open-source model for text-to-image generation,\nsurpassing both VQGAN [64] and DALL-E2-LAION [65] in terms of visual quality. We have com-\npared the impact of pretraining on images synthesized by these three generators. Tab. 2 reveals that\nhigher image quality correlates with improved results. This suggests that as more powerful generators\nemerge in the future, the prospect of pretraining on synthetic data becomes increasingly promising.\nPASVOC\nbase\naugment\nAP 50\n81.0\n81.6\nTable 5: Prompts ablations.\nPrompts design. We assess the significance of augmented prompts\nin our study, simply employing a basic prompt, \"a photo of [class],\"\nfor comparison purposes. As indicated in Tab. 5, augmented\nprompts contribute significantly to the generation of a broader range\nof images, enhancing the pretraining process.\nMore data, more power. We examine the effectiveness of our framework powered by an increasing\nquantity of diffusion-generated images. Here we adopt MoCo-v2 [22] to incorporate our Free-ATM,\nand report the pretraining transfer performance on the PASVOC [25] dataset. The results, displayed\nin Tab. 4.4, demonstrate a consistent performance enhancement in line with the increase in synthetic\nimage count. This observation suggests that synthetic images can indeed scale in a manner similar to\nreal images.\nnumber of synthetic images\n0.5M\n1M\n1.5M\n2M\nAP b\n50\n81.3\n82.1\n82.4\n82.7\nTable 6: Using more synthetic images for our unsupervised pretraining framework.\nMoreover, by utilizing 1.5M synthetic images, our approach achieves equivalent performance to the\noriginal MoCo-v2, which is trained on 1M real-world images from ImageNet [10]. This finding\nhighlights that by increasing the amount of synthetic data, our method can effectively close the\nperformance gap between unsupervised pretraining on synthetic data and real-world scenarios.\nSupported by our Free-ATM, synthetic data can be used as a viable alternative to real-world data for\nunsupervised pretraining, which can be beneficial in scenarios where real-world data is scarce or\ninsufficient. In the future, we will keep exploring the upper bounds of our framework using more\nsynthetic data.\n5\nMore Visualizations of Attention Masks\nFig. 4 provides diverse visualizations of the freely-available attention masks from text-to-image\ndiffusion models, where the cross-attention maps between object instances and their corresponding\ntext prompts are decently aligned.\n9\nA man is riding a horse\nA seal and a golden fish\nA castle besides a waterfall\nA flower besides a lion\nman\nhorse\nfish\nseal\ncastle\nwaterfall\nlion\nflower\nFigure 4: Visualizations of Attention Masks.\n6\nConclusion\nWe have proposed Free-ATM, a technique that utilizes the readily available attention masks from\ndiffusion generators, to improve unsupervised learning on diffusion-generated images. Our method is\ndemonstrated to be effective through extensive experimental results across various downstream tasks\nand benchmarks. We anticipate that our approach will provide new directions for future research on\nleveraging synthetic images to tackle computer vision problems.\nReferences\n[1] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual\nrepresentations,” in ICML, 2020.\n[2] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo,\nM. Gheshlaghi Azar, B. Piot, k. kavukcuoglu, R. Munos, and M. Valko, “Bootstrap your own latent - a new\napproach to self-supervised learning,” in NeurIPS, 2020.\n[3] X. Chen and K. He, “Exploring simple siamese representation learning,” in CVPR, 2021.\n[4] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representation\nlearning,” in CVPR, 2020.\n[5] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision\nlearners,” in CVPR, 2022.\n[6] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, “Simmim: A simple framework for\nmasked image modeling,” in CVPR, 2022.\n[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark et al., “Learning transferable visual models from natural language supervision,” in ICML, 2021.\n[8] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation,” in ICML, 2022.\n[9] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language transformer without convolution or region\nsupervision,” in ICML, 2021.\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image\ndatabase,” in CVPR, 2009.\n[11] T. Orekondy, B. Schiele, and M. Fritz, “Towards a visual privacy advisor: Understanding and predicting\nprivacy risks in images,” in ICCV, 2017.\n10\n[12] V. Besnier, H. Jain, A. Bursuc, M. Cord, and P. Pérez, “This dataset does not exist: Training models from\ngenerated images,” in ICASSP, 2020.\n[13] B. Zhao and H. Bilen, “Synthesizing informative training samples with gan,” NeurIPS 2022 Workshop on\nSynthetic Data for Empowering ML Research, 2022.\n[14] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for high fidelity natural image\nsynthesis,” in ICLR, 2019.\n[15] Y. Zhang, H. Ling, J. Gao, K. Yin, J.-F. Lafleche, A. Barriuso, A. Torralba, and S. Fidler, “Datasetgan:\nEfficient labeled data factory with minimal human effort,” in CVPR, 2021.\n[16] D. Li, H. Ling, S. W. Kim, K. Kreis, A. Barriuso, S. Fidler, and A. Torralba, “Bigdatasetgan: Synthesizing\nimagenet with pixel-wise annotations,” in CVPR, 2022.\n[17] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,”\nTPAMI, 2021.\n[18] R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. QI, “IS SYNTHETIC DATA FROM\nGENERATIVE MODELS READY FOR IMAGE RECOGNITION?” in ICLR, 2023.\n[19] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, “Glide:\nTowards photorealistic image generation and editing with text-guided diffusion models,” in ICML, 2022.\n[20] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or, “Prompt-to-prompt image\nediting with cross attention control,” arXiv preprint arXiv:2208.01626, 2022.\n[21] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu, “Unleashing text-to-image diffusion models for visual\nperception,” arXiv preprint arXiv:2303.02153, 2023.\n[22] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum contrastive learning,” arXiv\npreprint arXiv:2003.04297, 2020.\n[23] H. Bao, L. Dong, S. Piao, and F. Wei, “BEit: BERT pre-training of image transformers,” in ICLR, 2022.\n[24] H. Wang, K. Song, J. Fan, Y. Wang, J. Xie, and Z. Zhang, “Hard patches mining for masked image\nmodeling,” in CVPR, 2023.\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes\n(voc) challenge,” IJCV, 2010.\n[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft\ncoco: Common objects in context,” in ECCV, 2014.\n[27] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in CVPR, 2016.\n[28] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba, “Semantic understanding of\nscenes through the ade20k dataset,” IJCV, 2019.\n[29] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in NeurIPS, 2020.\n[30] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” in ICLR, 2021.\n[31] J. Ho and T. Salimans, “Classifier-free diffusion guidance,” arXiv preprint arXiv:2207.12598, 2022.\n[32] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,” in NeurIPS, 2021.\n[33] Y. Shi, C. Xue, J. Pan, W. Zhang, V. Y. Tan, and S. Bai, “Dragdiffusion: Harnessing diffusion models for\ninteractive point-based image editing,” arXiv preprint arXiv:2306.14435, 2023.\n[34] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with\nlatent diffusion models,” in CVPR, 2022.\n[35] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB. Karagol Ayan, T. Salimans et al., “Photorealistic text-to-image diffusion models with deep language\nunderstanding,” in NeurIPS, 2022.\n[36] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmenta-\ntion,” in MICCAI, 2015.\n11\n[37] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.\n[38] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n“Generative adversarial networks,” Communications of the ACM, 2020.\n[39] A. Jahanian, X. Puig, Y. Tian, and P. Isola, “Generative models as a data source for multiview representation\nlearning,” in ICLR, 2022.\n[40] S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet, “Synthetic data from diffusion models\nimproves imagenet classification,” arXiv preprint arXiv:2304.08466, 2023.\n[41] M. B. Sariyildiz, K. Alahari, D. Larlus, and Y. Kalantidis, “Fake it till you make it: Learning transferable\nrepresentations from synthetic imagenet clones,” in CVPR, 2023.\n[42] P. Bachman, R. D. Hjelm, and W. Buchwalter, “Learning representations by maximizing mutual information\nacross views,” in NeurIPS, 2019.\n[43] O. J. Hénaff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. Eslami, and A. v. d. Oord, “Data-efficient\nimage recognition with contrastive predictive coding,” arXiv preprint arXiv:1905.09272, 2019.\n[44] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning via non-parametric instance\ndiscrimination,” in CVPR, 2018.\n[45] I. Misra and L. v. d. Maaten, “Self-supervised learning of pretext-invariant representations,” in CVPR,\n2020.\n[46] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive predictive coding,” arXiv\npreprint arXiv:1807.03748, 2018.\n[47] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, “Unsupervised embedding learning via invariant and\nspreading instance feature,” in CVPR, 2019.\n[48] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” in ECCV, 2019.\n[49] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li, “Dense contrastive learning for self-supervised visual\npre-training,” in CVPR, 2021.\n[50] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, “ibot: Image bert pre-training with\nonline tokenizer,” in ICLR, 2022.\n[51] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, “Data2vec: A general framework for\nself-supervised learning in speech, vision and language,” arXiv preprint arXiv:2202.03555, 2022.\n[52] C. Wei, H. Fan, S. Xie, C.-Y. Wu, A. Yuille, and C. Feichtenhofer, “Masked feature prediction for\nself-supervised visual pre-training,” in CVPR, 2022.\n[53] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring\nthe limits of transfer learning with a unified text-to-text transformer,” Journal of Machine Learning\nResearch, 2020.\n[54] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell et al., “Language models are few-shot learners,” in NeurIPS, 2020.\n[55] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016.\n[56] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang et al.,\n“Grounded language-image pre-training,” in CVPR, 2022.\n[57] Z. Liu, S. Stent, J. Li, J. Gideon, and S. Han, “Loctex: Learning data-efficient visual representations from\nlocalized textual supervision,” in ICCV, 2021.\n[58] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\n[59] A. J. Wang, P. Zhou, M. Z. Shou, and S. Yan, “Position-guided text prompt for vision-language pre-training,”\nin CVPR, 2023.\n[60] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark et al., “Learning transferable visual models from natural language supervision,” in ICML, 2021.\n[61] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A cleaned, hypernymed, image\nalt-text dataset for automatic image captioning,” in ACL, 2018.\n12\n[62] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016.\n[63] K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask r-cnn,” in ICCV, 2017.\n[64] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution image synthesis,” in\nCVPR, 2021.\n[65] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, “Zero-shot\ntext-to-image generation,” in ICML, 2021.\n[66] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in CVPR,\n2015.\n[67] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly et al., “An image is worth 16x16 words: Transformers for image recognition at\nscale,” in ICLR, 2021.\n[68] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Unified perceptual parsing for scene understanding,” in\nECCV, 2018.\n13\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-08-13",
  "updated": "2023-08-13"
}