{
  "id": "http://arxiv.org/abs/2308.07134v5",
  "title": "Language is All a Graph Needs",
  "authors": [
    "Ruosong Ye",
    "Caiqi Zhang",
    "Runhui Wang",
    "Shuyuan Xu",
    "Yongfeng Zhang"
  ],
  "abstract": "The emergence of large-scale pre-trained language models has revolutionized\nvarious AI research domains. Transformers-based Large Language Models (LLMs)\nhave gradually replaced CNNs and RNNs to unify fields of computer vision and\nnatural language processing. Compared with independent data samples such as\nimages, videos or texts, graphs usually contain rich structural and relational\ninformation. Meanwhile, language, especially natural language, being one of the\nmost expressive mediums, excels in describing complex structures. However,\nexisting work on incorporating graph problems into the generative language\nmodeling framework remains very limited. Considering the rising prominence of\nLLMs, it becomes essential to explore whether LLMs can also replace GNNs as the\nfoundation model for graphs. In this paper, we propose InstructGLM\n(Instruction-finetuned Graph Language Model) with highly scalable prompts based\non natural language instructions. We use natural language to describe\nmulti-scale geometric structure of the graph and then instruction finetune an\nLLM to perform graph tasks, which enables Generative Graph Learning. Our method\nsurpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets,\nunderscoring its effectiveness and sheds light on generative LLMs as new\nfoundation model for graph machine learning. Our code is open-sourced at\nhttps://github.com/agiresearch/InstructGLM.",
  "text": "EACL 2024\nLanguage is All a Graph Needs\nRuosong Ye1, Caiqi Zhang2, Runhui Wang1, Shuyuan Xu1, Yongfeng Zhang1\n1Department of Computer Science, Rutgers University, New Brunswick, US\n2Language Technology Lab, University of Cambridge, UK\nruosong.ye@rutgers.edu, cz391@cam.ac.uk, runhui.wang@rutgers.edu,\nshuyuan.xu@rutgers.edu, yongfeng.zhang@rutgers.edu\nAbstract\nThe emergence of large-scale pre-trained lan-\nguage models has revolutionized various AI\nresearch domains. Transformers-based Large\nLanguage Models (LLMs) have gradually re-\nplaced CNNs and RNNs to unify fields of com-\nputer vision and natural language processing.\nCompared with independent data samples such\nas images, videos or texts, graphs usually con-\ntain rich structural and relational information.\nMeanwhile, language, especially natural lan-\nguage, being one of the most expressive medi-\nums, excels in describing complex structures.\nHowever, existing work on incorporating graph\nproblems into the generative language model-\ning framework remains very limited. Consid-\nering the rising prominence of LLMs, it be-\ncomes essential to explore whether LLMs can\nalso replace GNNs as the foundation model\nfor graphs. In this paper, we propose Instruct-\nGLM (Instruction-finetuned Graph Language\nModel) with highly scalable prompts based on\nnatural language instructions. We use natu-\nral language to describe multi-scale geometric\nstructure of the graph and then instruction fine-\ntune an LLM to perform graph tasks, which\nenables Generative Graph Learning. Our\nmethod surpasses all GNN baselines on ogbn-\narxiv, Cora and PubMed datasets, underscor-\ning its effectiveness and sheds light on genera-\ntive LLMs as new foundation model for graph\nmachine learning. Our code is available at\nhttps://github.com/agiresearch/InstructGLM.\n1\nIntroduction\nPrior to the advent of Transformers (Vaswani et al.,\n2017), various artificial intelligence domains with\ndifferent inductive biases had diverse foundational\nmodel architectures. For instance, CNNs (LeCun\net al., 1995; Szegedy et al., 2016) were designed\nwith considerations for spatial invariance in images,\nleading to superior performance in computer vision\ntasks (Deng et al., 2009; Lin et al., 2014). Memory-\nenhanced models like RNNs (Elman, 1990) and\nLSTM (Hochreiter and Schmidhuber, 1997; Cho\net al., 2014) were widely used for handling sequen-\ntial data such as natural language (Sarlin et al.,\n2020) and audio (Chen et al., 2021). Graph Neural\nNetworks (GNNs) have long been the preferred\nchoice in graph learning due to their proficiency in\ncapturing topological information through message\npassing and aggregation mechanisms (Kipf and\nWelling, 2016; Veliˇckovi´c et al., 2017; Hamilton\net al., 2017; Han et al., 2023a).\nIn recent years, the AI community has witnessed\nthe emergence of numerous powerful pre-trained\nLarge Language Models (LLMs) (Devlin et al.,\n2018; Raffel et al., 2020; Brown et al., 2020; Tou-\nvron et al., 2023; Ouyang et al., 2022), which are\ndriving huge advancements and lead to the pursuit\nof Artificial General Intelligence (AGI) (Ge et al.,\n2023; Bubeck et al., 2023). Under this background,\nthere is a trend towards unification in model archi-\ntectures across different domains. Specifically, pre-\ntrained Transformers have demonstrated remark-\nable performance on various modalities, such as im-\nages (Dosovitskiy et al., 2020) and videos (Arnab\net al., 2021) in computer vision, text in natural lan-\nguage processing (Singh et al., 2021), structured\ndata in graph machine learning (Ying et al., 2021),\npersonalized data in recommender systems (Geng\net al., 2022), decision sequences in reinforcement\nlearning (Di Palo et al., 2023), and visual-text pairs\nin multimodal tasks (Radford et al., 2021). There\nhas even been Transformers capable of handling\ntwelve modalities (Zhang et al., 2023b).\nAlongside advancements in model architectures,\nthere is also a noteworthy trend towards the adop-\ntion of unified processing techniques for multi-\nmodal data. T5 (Raffel et al., 2020) established\na text-to-text framework, unifying all NLP tasks as\na sequence generation problem. Moreover, models\nlike CLIP (Radford et al., 2021) utilize image-text\npairs for multimodal tasks with the images cap-\ntioned by natural language. In the realm of rein-\narXiv:2308.07134v5  [cs.CL]  6 Feb 2024\nFigure 1: Illustration of the InstructGLM Framework. We fine-tune InstructGLM under a Multi-task Multi-prompt\ninstruction tuning framework, enabling it to solve various graph machine learning tasks with the structure information\npurely described by natural language.\nforcement learning, Di Palo et al. (2023) improves\nthe agent by employing natural language to de-\nscribe environmental states. P5 (Geng et al., 2022;\nHua et al., 2023; Xu et al., 2023) and its variants\n(Geng et al., 2023; Hua et al., 2024; Ji et al., 2024),\nfurther contributes to this trend by reformulating\nall personalized recommendation tasks as language\nmodeling tasks via prompts. The aforementioned\nworks collectively demonstrate that employing nat-\nural language for multimodal data representation\nhas emerged as a prominent and promising trend.\nHowever, in graph machine learning, such an\nexploration still remains limited. Existing methods\nthat utilize LLMs for graph can be roughly cate-\ngorized into two types: 1) Combining LLMs and\nGNNs, where the LLM acts as a feature extractor\nor data augmentation module to enhance the down-\nstream GNNs (He et al., 2023; Mavromatis et al.,\n2023; Zhao et al., 2023). These methods often\nrequire training multiple models, incurring signifi-\ncant computational overhead and tend to easily in-\nherit drawbacks of GNNs such as over-smoothing\n(Cai and Wang, 2020). 2) Purely relying on Trans-\nformers but necessitating novel designs of token\nembedding for nodes and edges (Kim et al., 2022)\nor creating complex graph attention modules to\nlearn structural information (Dwivedi and Bresson,\n2020; Nguyen et al., 2022). This type of method\ndemands local attention calculation on every node\nduring each optimization step, leading to consid-\nerable computation costs and thus limiting each\nnode’s scope to only 1-hop neighbors. Addition-\nally, the complex pipeline with special attention\nmechanisms or token representations prevents the\nmodel from directly observing and learning struc-\ntural information like GNNs, thus restricting fur-\nther improvement on performance.\nTo address the issues of LLM-based graph learn-\ning and bridge the gap between languages and\ngraphs, we propose InstructGLM (Instruction-\nfinetuned Graph Language Model). Given that\nLLMs have succeeded in many AI domains, we\naim to answer the question: Besides CNNs and\nRNNs, can LLMs also replace GNNs as the founda-\ntion model for graph machine learning? Intuitively,\nas one of the most expressive medium, natural lan-\nguage is adept at describing complex structures\nsuch that InstructGLM owns the following advan-\ntages over GNNs:\n1) Flexibility. A natural language sentence is\ncapable of effectively describing the connec-\ntivity at any desired hop level and intermediate\npaths without iterative message passing and\naggregation. Even multimodal features of the\nnodes and edges can be directly integrated\ninto natural language prompts, making natu-\nral language a very flexible medium to convey\nboth structure and content on the graph.\n2) Scalability.\nInjecting graph structure into\nmultiple natural language sentences enables\nmini-batch training and independent gradi-\nent propagation, which facilitates scalable dis-\ntributed training and low machine communi-\ncation overhead for massive graphs.\n3) Compatibility. With structure descriptions,\nInstructGLM is able to consistently reformu-\nlate various graph learning pipelines as lan-\nguage modeling tasks. This aligns well with\nthe LLM-based multimodal processing frame-\nwork, enabling the integration of graph learn-\ning with other AI domains, including vision,\nlanguage, and recommendation, to build uni-\nfied AI systems.\nIn this paper, we focus on node classification\nand link prediction—two of the most fundamental\ntasks for graph learning. Besides, self-supervised\nlink prediction can augment and enhance the node\nclassification performance. We design a series of\ngraph prompts for generative LLMs. Specifically,\nwe systematically employ natural language to de-\nscribe the graphs’ topological structures according\nto our prompts, making the graph structure clearly\nand intuitively provided to LLM without complex\npipelines tailored to graphs. Therefore, we can\nhandle graph tasks efficiently and succinctly by the\nvanilla Transformer architecture (Vaswani et al.,\n2017) and language modeling objective (Zhang and\nSabuncu, 2018) in a generative manner. Overall,\nour contributions can be summarized as:\n• Structural information is the most fundamental\ninformation for graphs, and our research shows\nthat this fundamental information can be effec-\ntively described by languages. To the best of our\nknowledge, we are the first to propose purely us-\ning natural language for graph structure represen-\ntation and conduct instruction tuning on genera-\ntive LLMs to solve graph problems. We eliminate\nthe requirement of designing specific complex at-\ntention mechanisms tailored for graphs. Instead,\nwe offer a concise and efficient natural language\nprocessing interface for graph learning, which ex-\nhibits high scalability to a unified multimodal and\nmultitask framework, aligning with the current\ntrend across other AI domains.\n• Inspired by various message passing mechanisms\nin GNNs, we have designed a series of rule-based,\nhighly scalable instruction prompts for general\ngraph structure representation and graph ML. Al-\nthough in this paper, our focus lies in exploring\ninstruction tuning on Large Language Models,\nthese prompts can also be utilized for zero-shot\nexperiments on LLMs.\n• We conduct self-supervised link prediction as an\ngeneric auxiliary task and further investigate its\ninfluence on the primary node classification task\nunder a multitask instruction tuning framework.\nThis investigation offers valuable insights into fu-\nture LLM-based multitask graph learning, high-\nlighting the importance of self-supervised link\nprediction in enhancing large language models’\nunderstanding of graph structures.\n• We implement extensive experiments on three\nwidely used graphs: ogbn-arxiv, Cora, PubMed.\nThe results demonstrate our InstructGLM out-\nperforms previous competitive GNN baselines\nand Transformers-based methods across all three\ndatasets, achieving the top-ranked performance.\nLLM envisions a technical paradigm where “ev-\nerything is tokenized”. Benefiting from LLM’s\npowerful expressive capability in representing\nraw data of various modality into text or non-text\ntokens, all types of node or edge features can\nessentially be transformed into LLM-compatible\ntokens, thereby reshaping both the graph struc-\nture and the graph attribute information into lan-\nguage tokens, showing the general applicability\nof our approach. Our experimental results vali-\ndate the effectiveness of InstructGLM under gen-\neral graph problem settings and emphasize the\ntrend of utilizing generative LLMs as the new\nfoundational model for graph machine learning.\n2\nRelated Work\n2.1\nGNN-based Methods\nGraph Neural Networks (GNNs) (Zhou et al., 2020;\nWu et al., 2020; Han et al., 2023a; Wu and Wang,\n2022) have been dominant in graph machine learn-\ning for a long period. Leveraging message passing\nand aggregation, GNNs excel in simultaneously\nlearning node features and graph topology. Overall,\nGNNs with various message passing mechanisms\ncan be categorized as spatial-based ones (Hamil-\nton et al., 2017; Veliˇckovi´c et al., 2017; Xu et al.,\n2018a; Monti et al., 2017) and spectral-based ones\n(Kipf and Welling, 2016; Defferrard et al., 2016;\nYadati et al., 2019). Inherently, GNNs easily suf-\nfer from over-smoothing (Cai and Wang, 2020),\nwith various regularization techniques such as Mix-\nHop, Jump Knowledge and EdgeDrop (Xu et al.,\n2018b; Abu-El-Haija et al., 2019; Rong et al., 2019)\nproposed to mitigate such an overfitting. Another\nmajor drawback of GNNs is their inability to di-\nrectly process non-numeric raw data such as text\nor images, requiring additional feature engineering\ntechniques like BoW, TF-IDF, or Skip-gram as a\npreprocessing step (Wang et al., 2021). Its lack of\ncompatibility with existing large-scale generative\nmodels presents a significant challenge for inte-\ngration with other AI domains such as vision and\nlanguage into a unified intelligent system.\n2.2\nTransformers-based Methods\nAttention-based Transformer models can be uti-\nlized for graph processing by representing nodes\nand edges as distinct tokens (Müller et al., 2023).\nHowever, it is computationally intensive for han-\ndling large-scale graphs and the global attention\nmechanism can not effectively capture the graph’s\ntopology (Kim et al., 2022). To mitigate the issue,\nsome methods incorporate graph structure informa-\ntion into attention matrices (Ying et al., 2021; Park\net al., 2022), while others restrict attention to local\nsubgraphs (Nguyen et al., 2022) or ingeniously de-\nsign graph orthogonal vectors for node and edge\ntokens (Kim et al., 2022). These newly designed\ncomplex pipelines result in indirect representation\nof graph structure and significantly increase the\nlearning difficulty. Zhang et al. (2021a) utilizes\nnatural language templates for biological concept\nlinking (Sokal and Crovello, 1970; Wang et al.,\n2023b). However, it can be difficult to be extended\nbeyond classification due to the use of encoder-only\nmodel (Liu et al., 2019). Additionally, its natural\nlanguage templates are not designed for general\ngraph learning thus not as expressive and flexible\nto serve as a foundation model for graph learning.\n2.3\nFuse GNN and Transformers\nGNNs excel at learning structure, while Transform-\ners are proficient in capturing multi-modality fea-\ntures. To combine the advantages of both, Chien\net al. (2021) and Duan et al. (2023) utilizes multi-\nscale neighborhood prediction and LoRA (Hu et al.,\n2021), respectively, to incorporate language models\nfor generating structure enhanced feature for down-\nstream GNNs. Mavromatis et al. (2023) employs\nGNNs to perform knowledge distillation on LMs,\nZhao et al. (2023) trains GNNs and LMs iteratively\nin a variational inference framework, while Rong\net al. (2020) attempts to replace attention heads\nwith GNNs to better capture global information.\nThe main drawback of the aforementioned meth-\nods is the lack of decoupling between Transformers\nand GNNs, results in training multiple models and\nincurs significant computational overhead (Nguyen\net al., 2022). Moreover, the model performance is\nstill susceptible to inherent issues of GNNs, such as\nover-smoothing (Yang et al., 2020) and the pipeline\nof multi-model training is usually very complex\ncompared to the simplicity of a single generative\nLLM framework.\n2.4\nLarge Language Model based Methods\nInspired by the remarkable zero-shot capabilities,\nleveraging LLMs in graph problems has attracted\nconsiderable attention. Existing works have in-\ncluded utilizing LLM to select the most suitable\ngraph processor based on the query (Zhang, 2023),\nemploying LLM’s zero-shot explanations for data\naugmentation to obtain advanced graph features\n(He et al., 2023), generating prompts and bench-\nmarks for graph construction, evaluation, biology\nand structural reasoning (Han et al., 2023b; Jiang\net al., 2023; Qian et al., 2023; Guo et al., 2023).\nThere are three works sharing similarities with ours.\nGuo et al. (2023) attempts to complete graph tasks\nby describing graphs. However, it uses complex for-\nmal languages like (Brandes et al., 2013; Himsolt,\n1997) but not flexible natural language. Wang et al.\n(2023a) and Chen et al. (2023b) both explore using\nnatural language with LLM for graph problems,\nwith (Wang et al., 2023a) focusing on mathemat-\nical problems on small graphs while (Chen et al.,\n2023b) concentrating on node classification in Text-\nAttributed Graphs (TAGs) (Hu et al., 2020). In com-\nparison, our natural language instruction prompts\nexhibit better scalability, applicable to both small\nand large graphs and not limited to specific graph\ntype. Besides, the three related works only ex-\nplored the basic capability of LLM for graph tasks\nin a zero-shot setting. Their performance does not\nsurpass GNN baselines for the most of time with\nthe model freezed, merely demonstrating the poten-\ntial of LLM as an optional candidate for graph tasks.\nBy contrast, we successfully bridge this gap by con-\nducting instruction tuning on generative LLMs with\nsimple prompts, achieving experimental results that\nsurpass all competitive GNN baselines.\n3\nInstructGLM\nIn this section, we introduce InstructGLM, a\nframework utilizing natural language to describe\nboth graph structure and meta features of node and\nedge for generative LLMs and further addressing\ngraph-related tasks by instruction-tuning. We start\nwith notation setup, followed by outlining the prin-\nciples behind the design of instruction prompts, and\nthen present a detailed illustration of the pipeline.\nFigure 2: Illustration of InstructGLM. We use graph prompts to describe each node’s multi-hop connectivity and\nmeta features in a scalable mini-batch manner, conveying graph structure concisely and intuitively by pure natural\nlanguage for learning. Subsequently, we instruct LLMs to generate responses for various graph tasks in a unified\nlanguage modeling pipeline. We also expand the LLM’s vocabulary by creating a new and unique token for each\nnode. More specifically, we set the graph’s inherent node feature vectors (e.g. BoW, OGB) as the embedding for\nthese new tokens (depicted as red vectors in the figure) and employ the LLM’s pre-trained embedding (depicted as\nblue vectors in the figure) for natural language tokens.\n3.1\nPreliminary\nFormally, a general graph can be represented as\nG = (V, A, E, {Nv}v∈V, {Ee}e∈E), where V is\nthe set of nodes, E ⊆V × V is the edge set,\nA ∈{0, 1}|V|×|V| is the adjacent matrix, Nv is\nthe node feature of v ∈V and Ee is the edge fea-\nture of e ∈E. It is worth noting that the node fea-\ntures and edge features can be in various modalities\nand in diverse forms. For example, node features\ncan be textual information in citation networks, vi-\nsual images in photography graphs, user profiles in\nsocial networks, and even video or audio signals\nin movie networks. Similarly, edge features can\nbe user friendships in social networks, or product\nreviews in user-item interaction graph of recom-\nmender systems, etc.\n3.2\nInstruction Prompt Design\nIn order to comprehensively convey the structure\ninformation of a graph and ensure the adaptability\nof the created instruction prompts to various types\nof graphs, we have systematically designed a set of\ngraph description prompts centered around a cen-\ntral node. We mainly consider the following three\nquestions when designing the prompts: i) What is\nthe largest hop level of neighbor information about\nthe central node in the prompt? ii) Does the prompt\ninclude meta node features or edge features? iii)\nFor prompts with large (≥2) hop level neighbors\nabout the central node, does the prompt encompass\ninformation about the intermediate nodes or paths\nalong the corresponding connecting route?\nRegarding question i), prompts can be classified\ninto two types: those exclusively contain 1-hop con-\nnection information, and those with a maximum\nof 2-hop or 3-hop connection details. Prior works\nhave shown that utilizing up to 3-hop connectiv-\nity is sufficient for excellent performance (Hamil-\nton et al., 2017; Veliˇckovi´c et al., 2017; Kipf and\nWelling, 2016), while information beyond 3-hop\ntypically owns a minor impact on improvement and\nmight even lead to negative effects (Zhang et al.,\n2021b; Cai and Wang, 2020). Therefore, the maxi-\nmum level of neighbor information included in the\nprompts is up to three. However, benefiting from\nthe flexibility of natural language, our designed\nprompts can actually accommodate structural in-\nformation of any hop level. Regarding question ii)\nand iii), there are two possible scenarios for each\nquestion, i.e., if or not to include the node or edge\nmeta features in the prompt, and if or not to include\nthe intermediate connecting paths in the prompt.\nWe then denote an instruction prompt as T (·)\nsuch that I = T (v, A, {Nv}v∈V, {Ee}e∈E) is the\ninput natural language sentence to LLM and v is\nthe central node of this prompt. For instance, the\nsimplest form of a graph description prompt con-\ntaining at most 2-hop neighbor information is:\nT (v, A) ={v} is connected with\n{[v2]v2∈Av\n2} within two hops.\nwhile its most detailed form which includes node\nfeatures, edge features and the corresponding inter-\nmediate paths should be:\nT (v,A, {Nv}v∈V, {Ee}e∈E) = {(v, Nv)} is\nconnected with {[(v2, Nv2)]v2∈Av\n2}\nwithin two hops through {[(v1, Nv1)]v1∈Av\n1}\nand featured paths {[(E(v,v1), E(v1,v2))]\nv1∈Av\n1 , v2∈Av1\n1 }, respectively.\nwhere Av\nk represents the list of node v’s k-hop\nneighbor nodes. Essentially, the above prompt\nshould contain all 2-hop paths with node and\nedge features like (v, Nv)\nE(v,v1)\n−→(v1, Nv1)\nE(v1,v2)\n−→\n(v2, Nv2) centering at node v. All our instruction\nprompts are summarized in Appendix D.\n3.3\nGenerative Instruction Tuning for Node\nClassification\nIn prompt engineering (Li and Liang, 2021; Lester\net al., 2021; Shin et al., 2020) or in-context learning\n(Dong et al., 2022), pretrained models are usually\nfrozen. Instruction Tuning (Wei et al., 2021; Chung\net al., 2022), however, directly conveys the require-\nments of downstream tasks to pretrained models by\nfusing the original input data with task-specific in-\nstructional prompts under the framework of multi-\nprompt training. This facilitates remarkably ef-\nfective fine-tuning, especially when coupled with\nhuman feedback (RLHF) (Ouyang et al., 2022). In-\nstruction Tuning has already become an indispens-\nable technique for fine-tuning the most powerful\nlarge language models.\nIn this paper, we propose InstructGLM as a\nmulti-prompt instruction-tuning framework tai-\nlored for graph learning. Specifically, We utilize\na generative large language model, either with an\nencoder-decoder or a decoder-only architecture, as\nthe backbone. And then we fuse all of our de-\nsigned instruction prompts, which are spanning at\ndifferent hop levels with diverse structural informa-\ntion, together as input to the LLM, enabling mutual\nenhancement among the instructions. By exclu-\nsively using natural language to depict graph struc-\ntures, we succinctly present the graph structure to\nthe LLM and provide a pure NLP interface for all\ngraph-related tasks, making them solvable via a uni-\nfied pipeline in generative manner. Worth noting\nthat we concentrate on solving node classification\ntask in this study. We train InstructGLM to strictly\ngenerate the category label in natural language, and\nthe prevalent Negative Log-Likelihood (i.e. NLL)\nLoss in language modeling are employed as our\nobjective function.\nGiven G = (V, A, E, {Nv}v∈V, {Ee}e∈E) and a\nspecific instruction prompt T ∈{T (·)}, we denote\nx and y as the LLM’s input and target sentence,\nrespectively. Then our pipeline can be formed as:\nPθ (yj | x, y<j) = LLMθ (x, y<j) ,\nx = Concatenate(P; I; Q)\nLθ = −\n|y|\nX\nj=1\nlog Pθ (yj | x, y<j)\nwhere I = T (v, A, {Nv}v∈V, {Ee}e∈E) is the\ngraph structure description centering at node v ∈V,\nL denotes the NLL loss, P and Q are the task-\nspecific instruction prefix and query. Specifically,\nfor node classification, we design P and Q for node\nclassification as follows: P = ‘Classify the central\nnode into one of the following categories: [<All\ncategory>]. Pay attention to the multi-hop link re-\nlationships between the nodes.’ and Q = ‘Which\ncategory should {v} be classified as?’. More de-\ntails of the pipeline are depicted in Figure 2.\nOur InstructGLM actually shares essential sim-\nilarities in mechanisms with various GNNs, thus\ninheriting their advantages. First, similar to Mix-\nHop (Abu-El-Haija et al., 2019), which performs\ngraph convolutions on subgraphs extracted at dif-\nferent hop levels, we mix prompts with diverse hop-\nlevel information during training. Second, Jumping\nKnowledge (Xu et al., 2018b) combines outcomes\nfrom different convolution layers via jump connec-\ntions, which is aligned with our prompts featuring\nintermediate information and high-hop-level neigh-\nbors. Additionally, due to LLM’s input length limit,\nsimilar to GraphSAGE (Hamilton et al., 2017), we\nconduct neighbor sampling for the central node\nwhen filling the prompts to form a mini-batch train-\ning. This operation also resembles graph regu-\nlarization techniques like DropEdge (Rong et al.,\n2019) for preventing over-smoothing (Chen et al.,\n2020a). Moreover, InstructGLM surpasses GNNs\nin expressiveness. Even a single graph description\nthat contains intermediate paths and k-hop neigh-\nbor information is equivalent to a k-layer GNN in\nexpressiveness. Therefore, InstructGLM can read-\nily accommodate the inductive bias of graph tasks\nwithout any alterations on LLM’s architecture and\npipeline. For instance, since our inputs are cen-\ntralized graph descriptions that directly exhibit the\ncorresponding multi-hop neighbors, self-attention\n(Vaswani et al., 2017) applied on such inputs can be\nseen as an advanced multi-scale weighted average\naggregation mechanism of GATs (Veliˇckovi´c et al.,\n2017; Li et al., 2021), facilitating InstructGLM to\neffectively grasp different neighbors’ varying im-\nportance to the central node.\n3.4\nAuxiliary Self-Supervised Link Prediction\nBoth SuperGAT (Kim and Oh, 2022) and DiffPool\n(Ying et al., 2018) introduce auxiliary link predic-\ntion task, thus successfully obtain better node rep-\nresentations and performance for node or graph\nclassification, demonstrating that model’s compre-\nhension of graph structure can be significantly en-\nhanced by such an auxiliary task. Inspired by them,\nalso to remove the restriction that our instruction\nprompts can only treat labeled training nodes as\ncentral nodes in single-task semi-supervised learn-\ning, we introduce self-supervised link prediction\nas a foundational auxiliary task for InstructGLM.\nGiven arbitrary hop level and central node, we ran-\ndomly select a neighbor or non-neighbor at this hop\nlevel as the candidate. Then we instruct our model\nto either discriminate whether there is a connec-\ntion at this hop level between the central node and\nthe candidate node (discriminative prompt) or di-\nrectly generate the correct neighbor in a generative\nmanner (generative prompt).\nGiven G = (V, A, E, {Nv}v∈V, {Ee}e∈E), the\npipeline of link prediction aligns exactly with node\nclassification.\nThe only distinction lies in the\nnewly designed task-specific prefix and two dif-\nferent query templates for it. Specifically, we de-\nsign P and Q for link prediction as follows: P =\n‘Perform link prediction for the central node. Pay\nattention to the multi-hop link relationships be-\ntween the nodes.’, Qgenerative = ‘Which other\nnode will be connected to {v} within {h} hop?’\nand Qdiscriminative = ‘Will {˜v} be connected to\n{v} within {h} hop?’, where v is the central node,\n˜v is the candidate node and h is the specified hop\nlevel. We enable arbitrary node to act as central\nnode via self-supervised link prediction and ensure\na multi-task multi-prompt framework.\n4\nExperiments\n4.1\nExperimental Setup\nIn this paper, we primarily utilize InstructGLM\nfor node classification, and also conduct self-\nsupervised link prediction as an auxiliary task.\nSpecifically, we select the following three popu-\nlar citation graphs: ogbn-arxiv (Hu et al., 2020),\nCora and PubMed (Yang et al., 2016), in which ev-\nery node represents an academic paper on a specific\ntopic, with its title and abstract included in raw text\nformat. We use accuracy as our metrics in all ex-\nperiments and employ the default numerical node\nembedding of the datasets to extend the LLM’s\nvocabulary by adding node-wise new tokens. Im-\nplementation details and elaborated dataset-specific\nstatistics are summarized in Appendix A and B.\n4.2\nMain Results\nOur results achieve single-model state-of-the-art\nperformance, surpassing all single graph learners\nacross all three datasets, including both representa-\ntive GNN models and graph Transformer models,\nwhich demonstrates the promising trend for large\nlanguage models to serve as the new foundation\nmodel for graph learning.\n4.2.1\nogbn-arxiv\nFor the ogbn-arxiv, we adopt the same data split as\nin the OGB open benchmark (Hu et al., 2020), i.e.\n54%/18%/28% for train/val/test splits, respectively.\nMethod\nOGB\nGIANT\nMLP\n55.50 ± 0.23\n73.06 ± 0.11\nGAMLP\n56.53 ± 0.16\n73.35 ± 0.08\nGraphSAGE\n71.19 ± 0.21\n74.35 ± 0.14\nGCN\n71.74 ± 0.29\n73.29 ± 0.01\nDeeperGCN\n71.92 ± 0.16\n–\nALT-OPT\n72.76 ± 0.00\n–\nUniMP\n73.11 ± 0.20\n–\nLEGNN\n73.37 ± 0.07\n–\nGAT\n73.66 ± 0.11\n74.15 ± 0.05\nAGDN\n73.75 ± 0.21\n76.02 ± 0.16\nRvGAT\n74.02 ± 0.18\n75.90 ± 0.19\nDRGAT\n74.16 ± 0.07\n76.11 ± 0.09\nCoarFormer\n71.66 ± 0.24\n–\nSGFormer\n72.63 ± 0.13\n–\nGraphormer\n72.81 ± 0.23\n–\nE2EG\n73.62 ± 0.14\n–\nFlan-T5-base\n73.51 ± 0.16\n74.45 ± 0.11\nFlan-T5-large\n74.67 ± 0.08\n74.80 ± 0.18\nLlama-7b\n75.70 ± 0.12\n76.42 ± 0.09\nTable 1: Results on ogbn-arxiv. We report accuracy\non GNNs (Top), Graph Transformers (Middle) and our\nInstructGLM with different backbones (Bottom).\nWe select top-ranked GNNs from the OGB\nLeaderboard1, including DRGAT, RevGAT, etc., as\nthe baselines (Zhang et al., 2022a; Hamilton et al.,\n2017; Kipf and Welling, 2016; Li et al., 2020; Han\net al., 2023a; Shi et al., 2020; Yu et al., 2022a;\nVeliˇckovi´c et al., 2017; Sun et al., 2020; Li et al.,\n2021; Zhang et al., 2023a). Several most power-\nful Transformer-based single-model graph learners\nlike Graphormer are also considered for compari-\nson (Kuang et al., 2021; Wu et al., 2023; Ying et al.,\n2021; Dinh et al., 2022).\nWe instruction-finetune Flan-T5 (Chung et al.,\n2022) and Llama-v1 (LoRA) (Touvron et al., 2023;\nHu et al., 2021) as the backbone for our In-\nstructGLM. The experimental results in Table 1\ndemonstrate that both models outperform all the\nGNNs and Transformer-based methods. Particu-\nlarly, when using Llama-v1-7b as the backbone\non the default OGB feature, our InstructGLM at-\ntains a 1.54% improvement over the best GNN\nmethod and a 2.08% improvement over the best\nTransformer-based method. Moreover, we also\nachieve new SoTA performance on another pop-\nular and advanced feature named GIANT (Chien\net al., 2021), which is enhanced by graph structure\ninformation via multi-scale neighborhood predic-\ntion task during preprocessing.\n4.2.2\nCora & PubMed\nIn terms of the compared methods for Cora and\nPubMed datasets (He et al., 2023), we select those\ntop-ranked GNNs from the two corresponding\nbenchmarks2 3 with 60%/20%/20% train/val/test\nsplits, including Snowball, RevGAT, etc. (Abu-\nEl-Haija et al., 2019; Pei et al., 2020; Wu et al.,\n2019; He et al., 2021; Bo et al., 2021; Chen et al.,\n2020b; Luan et al., 2022). Three most powerful\nTransformer-based single-model graph learners on\nthe two benchmarks, i.e., CoarFormer, Graphormer,\nand GT (Dwivedi and Bresson, 2020), are also con-\nsidered as baseline for comparison.\nWe instruction-finetune Flan-T5 and Llama-v1\n(LoRA) as the backbone for our InstructGLM.\nThe experimental results in Table 2 show that\nour InstructGLM outperforms all the GNNs and\nTransformer-based methods. Specifically, Instruct-\nGLM achieves a 1.02% improvement over the best\nGNN method and a 2.08% improvement over the\nbest Transformer-based method on Cora dataset,\n1stanford-ogbn-arxiv leaderboard\n2Cora-60-20-20-random leaderboard\n3PubMed-60-20-20-random leaderboard\nwhile also achieves a 3.18% improvement over the\nbest GNN and a 4.87% improvement over the best\nTransformer-based method on PubMed dataset.\nMethod\nCora\nPubMed\nMixHop\n75.65 ± 1.31\n90.04 ± 1.41\nGAT\n76.70 ± 0.42\n83.28 ± 0.12\nGeom-GCN\n85.27 ± 1.48\n90.05 ± 0.14\nSGC-v2\n85.48 ± 1.48\n85.36 ± 0.52\nGraphSAGE\n86.58 ± 0.26\n86.85 ± 0.11\nGCN\n87.78 ± 0.96\n88.90 ± 0.32\nBernNet\n88.52 ± 0.95\n88.48 ± 0.41\nFAGCN\n88.85 ± 1.36\n89.98 ± 0.54\nGCNII\n88.93 ± 1.37\n89.80 ± 0.30\nRevGAT\n89.11 ± 0.00\n88.50 ± 0.05\nSnowball-V3\n89.59 ± 1.58\n91.44 ± 0.59\nACM-GCN+\n89.75 ± 1.16\n90.96 ± 0.62\nGraphormer\n80.41 ± 0.30\n88.24 ± 1.50\nGT\n86.42 ± 0.82\n88.75 ± 0.16\nCoarFormer\n88.69 ± 0.82\n89.75 ± 0.31\nLlama-7b\n87.08 ± 0.32\n93.84 ± 0.25\nFlan-T5-base\n90.77 ± 0.52\n94.45 ± 0.12\nFlan-T5-large\n88.93 ± 1.06\n94.62 ± 0.13\nTable 2: Results on Cora and PubMed. We report accu-\nracy on GNNs (Top), Graph Transformers (Middle) and\nour InstructGLM with different backbones (Bottom).\n4.3\nAblation Study\nIn our experiments, two crucial operations con-\ntributing to the outstanding performance of In-\nstructGLM in node classification task are 1) multi-\nprompt instruction-tuning, which provides multi-\nhop graph structure information to the LLM, and\n2) the utilization of self-supervised link prediction\nas an auxiliary task. To validate the impact of the\ntwo key components on model performance, we\nconduct ablation experiments on all three datasets,\nthe results are shown in Table 3.\nRegarding the Hop Info column, Structure-Free-\nTuning indicates fine-tuning the model on titles and\nabstracts of the nodes, while 1-hop and Multi-hop\nmean that we utilize prompts that merely include\ninformation from 1-hop neighbors and prompts that\ninclude information from neighbors with higher\nhop levels, respectively. The experimental results\nshow that incorporating multi-hop information and\nincluding link prediction task can both enhance the\nmodel’s performance for node classification.\nHop Info\nLink Prediction\nogbn-arxiv\nCora\nPubMed\nLlama-v1-7b\nFlan-T5-base\nFlan-T5-base\nMulti-hop\nw/\n75.70%\n90.77%\n94.45%\nMulti-hop\nw/o\n75.37%\n87.27%\n94.35%\n1-hop\nw/o\n75.25%\n86.90%\n94.30%\nStructure-Free-Tuning\nw/o\n74.97%\n75.65%\n94.22%\nTable 3: Ablation Study Results. In particular, since Cora is equipped with the sparsest semantic feature (Bag of\nWords) among the three datasets (ogbn-arxiv with Skip-gram and PubMed with TF-IDF.), we can observe that\nintroducing multi-hop structural information provides the greatest performance gain on Cora.\n4.4\nInstruction Tuning at Low Label Ratio\nIn previous experiments, our data splits all ensured\na relatively high ratio of labeled training nodes. To\nfurther investigate the scalability and robustness\nof our InstructGLM, we conduct experiments on\nthe PubMed dataset using its another widely-used\nsplits with extremely low label ratio. Specifically,\nwe have only 60 training nodes available in this\nsetting thus the label ratio is 0.3%.\nMethod\nAccuracy\nGraphSAGE\n76.8 ± 0.9\nGAT\n79.0 ± 1.4\nSnowball\n79.2 ± 0.3\nGCN\n80.4 ± 0.4\nSuperGAT\n81.7 ± 0.5\nALT-OPT\n82.5 ± 1.7\nGRAND\n82.7 ± 0.6\nSAIL\n83.8 ± 0.1\nANS-GT\n79.6 ± 1.0\nNodeFormer\n79.9 ± 1.0\nSGFormer\n80.3 ± 0.6\nLlama-7b\n85.1 ± 0.6\nFlan-T5-base\n88.2 ± 0.3\nFlan-T5-large\n89.6 ± 0.4\nTable 4: Results on PubMed with 60 training nodes:\naccuracy on GNNs (Top), Graph Transformers (Middle)\nand InstructGLM with different backbones (Bottom).\nWe consider top-ranked GNNs from the cor-\nresponding leaderboard4, including SAIL, ALT-\nOPT, GRAND, etc., as the GNN baselines (Luan\net al., 2019; Kim and Oh, 2022; Feng et al., 2020;\nHan et al., 2023a; Yu et al., 2022b).\nWe also\ninclude the three most outstanding Transformer-\nbased graph learners under this dataset setting,\ni.e., ANS-GT, NodeFormer and SGFormer (Zhang\net al., 2022b; Wu et al., 2022, 2023). We then\ninstruction-finetune Flan-T5 and Llama as the back-\nbone for our InstructGLM. Experimental results\n4PubMed-Planetoid leaderboard\nin Table 4 show that InstructGLM outperforms\nall GNNs with an improvement of 5.8% against\nthe best GNN baseline. It also surpasses the best\nTransformer-based model by 9.3% and achieves\nnew SoTA performance on the leaderboard, demon-\nstrating the data-efficiency of InstructGLM.\n5\nConclusions and Future Work\nTo the best of our knowledge, this work is the\nfirst attempt to represent graph structure via nat-\nural language description and then further per-\nform instruction-tuning on generative LLMs for\ngraph learning tasks, demonstrating the huge po-\ntential of LLMs as the new foundation model\nfor graph ML. Our InstructGLM outperforms all\nsingle-model GNNs and Graph Transformers on\nogbn-arxiv, Cora and PubMed datasets.\nMore-\nover, benefiting from our highly scalable instruc-\ntion prompts and unified generative pipeline appli-\ncable to multi-modality data, InstructGLM can be\nreadily extended to valuable future works along\nfour directions: 1) Leveraging LLMs to generate\nimproved features like TAPE, SimTeG (He et al.,\n2023; Duan et al., 2023) and instruction prompts\n(Wei et al., 2022) for InstructGLM; 2) Enhancing\nInstructGLM with knowledge distillation (Mavro-\nmatis et al., 2023) and iterative training (Zhao et al.,\n2023) frameworks; 3) Deploying InstructGLM on\nmore graph tasks such as question answering on\nknowledge graphs (Chen et al., 2023a); 4) Extend-\ning InstructGLM to other languages beyond natu-\nral language under the premise that “everything is\ntokenized,” to include visual tokens, acoustic to-\nkens, other multi-modality tokens, or even domain\nspecific languages or tokens (Li et al., 2024) such\nas chemical languages. Detailed future works are\nsummarized in Appendix Section C. Overall, our\nInstructGLM provides a powerful NLP interface\nfor graph machine learning, with generative LLMs\nand natural language as the driving force, it further\ncontributes to the trend of unifying foundational\nmodel architecture and pipeline across multiple AI\ndomains for the AGI pursuit.\nLimitations\nThe primary limitation of our InstructGLM lies in\nthe input token limit of the large language model\n(LLM). For example, Flan-T5 can only accept\na maximum sentence input length of 512, while\nLlama allows for 2048. When dealing with large-\nscale graphs, the instruction prompts we construct\nmay not encompass all high-order neighbors within\na single natural language sentence due to the lim-\nitations of sentence length. The simplest solution\nto this problem is to construct multiple graph de-\nscription sentences for each training node (central\nnode) to enumerate all possible neighbors at corre-\nsponding hop level. However, this leads to a rapid\nincrease in the training data volume. In this work,\nlearning from GraphSAGE (Hamilton et al., 2017),\nwe repeatedly perform random sampling from the\nmulti-hop neighbor lists of the central node until\nthe sentence length reaches the input token limit\nto mitigate this issue. Despite our implementation\nachieving impressive results, we believe that im-\nproved neighbor sampling and selection strategies\ncan help InstructGLM better address graph-related\ntasks, especially in the context of applications in-\nvolving extremely large-scale graphs like knowl-\nedge graphs (Pan et al., 2023).\nEthics Statement\nOur method is proposed to provide a powerful nat-\nural language processing interface for graph ma-\nchine learning tasks. Under normal and appropriate\nusage circumstances, there is no obvious evidence\nor tendency that our method will lead to significant\nnegative societal impacts.\nReferences\nSami Abu-El-Haija, Bryan Perozzi, Amol Kapoor,\nNazanin Alipourfard, Kristina Lerman, Hrayr Haru-\ntyunyan, Greg Ver Steeg, and Aram Galstyan. 2019.\nMixhop: Higher-order graph convolutional architec-\ntures via sparsified neighborhood mixing. In interna-\ntional conference on machine learning, pages 21–29.\nPMLR.\nAnurag Arnab, Mostafa Dehghani, Georg Heigold,\nChen Sun, Mario Luˇci´c, and Cordelia Schmid. 2021.\nVivit: A video vision transformer. In Proceedings of\nthe IEEE/CVF international conference on computer\nvision, pages 6836–6846.\nDeyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen.\n2021. Beyond low-frequency information in graph\nconvolutional networks. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 35,\npages 3950–3957.\nUlrik Brandes, Markus Eiglsperger, Jürgen Lerner,\nand Christian Pich. 2013. Graph markup language\n(graphml).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nChen Cai and Yusu Wang. 2020.\nA note on over-\nsmoothing for graph neural networks. arXiv preprint\narXiv:2006.13318.\nDeli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and\nXu Sun. 2020a. Measuring and relieving the over-\nsmoothing problem for graph neural networks from\nthe topological view. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 34, pages\n3438–3445.\nI-Fan Chen, Brian King, and Jasha Droppo. 2021. Inves-\ntigation of training label error impact on rnn-t. arXiv\npreprint arXiv:2112.00350.\nJiao Chen, Luyi Ma, Xiaohan Li, Nikhil Thakurdesai,\nJianpeng Xu, Jason HD Cho, Kaushiki Nag, Evren\nKorpeoglu, Sushant Kumar, and Kannan Achan.\n2023a. Knowledge graph completion models are\nfew-shot learners: An empirical study of relation\nlabeling in e-commerce with llms. arXiv preprint\narXiv:2305.09858.\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding,\nand Yaliang Li. 2020b. Simple and deep graph con-\nvolutional networks. In International conference on\nmachine learning, pages 1725–1735. PMLR.\nZhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi\nWen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin,\nWenqi Fan, Hui Liu, et al. 2023b. Exploring the\npotential of large language models (llms) in learning\non graphs. arXiv preprint arXiv:2307.03393.\nEli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-\nFu Yu, Jiong Zhang, Olgica Milenkovic, and In-\nderjit S Dhillon. 2021. Node feature extraction by\nself-supervised multi-scale neighborhood prediction.\narXiv preprint arXiv:2111.00064.\nKyunghyun Cho, Bart Van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nMichaël Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. 2016. Convolutional neural networks on\ngraphs with fast localized spectral filtering. Advances\nin neural information processing systems, 29.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition, pages\n248–255. Ieee.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nNorman Di Palo, Arunkumar Byravan, Leonard Hasen-\nclever, Markus Wulfmeier, Nicolas Heess, and Mar-\ntin Riedmiller. 2023. Towards a unified agent with\nfoundation models. In Workshop on Reincarnating\nReinforcement Learning at ICLR 2023.\nTu Anh Dinh, Jeroen den Boef, Joran Cornelisse, and\nPaul Groth. 2022. E2eg: End-to-end node classi-\nfication using graph topology and text-based node\nattributes. arXiv preprint arXiv:2208.04609.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale.\narXiv preprint\narXiv:2010.11929.\nKeyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng\nYan, Wei Tsang Ooi, Qizhe Xie, and Junxian He.\n2023.\nSimteg: A frustratingly simple approach\nimproves textual graph learning.\narXiv preprint\narXiv:2308.02565.\nVijay Prakash Dwivedi and Xavier Bresson. 2020. A\ngeneralization of transformer networks to graphs.\narXiv preprint arXiv:2012.09699.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nWenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han,\nHuanbo Luan, Qian Xu, Qiang Yang, Evgeny Khar-\nlamov, and Jie Tang. 2020. Graph random neural\nnetworks for semi-supervised learning on graphs.\nAdvances in neural information processing systems,\n33:22092–22103.\nYingqiang Ge, Wenyue Hua, Kai Mei, jianchao ji, Jun-\ntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng\nZhang. 2023. OpenAGI: When LLM meets domain\nexperts. In Thirty-seventh Conference on Neural In-\nformation Processing Systems.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge,\nand Yongfeng Zhang. 2022. Recommendation as\nlanguage processing (rlp): A unified pretrain, person-\nalized prompt & predict paradigm (p5). In Proceed-\nings of the 16th ACM Conference on Recommender\nSystems, pages 299–315.\nShijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and\nYongfeng Zhang. 2023. VIP5: Towards multimodal\nfoundation models for recommendation. In The 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nadversarial nets. Advances in neural information\nprocessing systems, 27.\nJiayan Guo, Lun Du, and Hengyu Liu. 2023. Gpt4graph:\nCan large language models understand graph struc-\ntured data? an empirical evaluation and benchmark-\ning. arXiv preprint arXiv:2305.15066.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017.\nInductive representation learning on large graphs. Ad-\nvances in neural information processing systems, 30.\nHaoyu Han, Xiaorui Liu, Haitao Mao, MohamadAli\nTorkamani, Feng Shi, Victor Lee, and Jiliang Tang.\n2023a. Alternately optimized graph neural networks.\nIn International Conference on Machine Learning,\npages 12411–12429. PMLR.\nJiuzhou Han, Nigel Collier, Wray Buntine, and Ehsan\nShareghi. 2023b. Pive: Prompting with iterative veri-\nfication improving graph-based generative capability\nof llms. arXiv preprint arXiv:2305.12392.\nMingguo He, Zhewei Wei, Hongteng Xu, et al. 2021.\nBernnet: Learning arbitrary graph spectral filters via\nbernstein approximation. Advances in Neural Infor-\nmation Processing Systems, 34:14239–14251.\nXiaoxin He, Xavier Bresson, Thomas Laurent, and\nBryan Hooi. 2023. Explanations as features: Llm-\nbased features for text-attributed graphs.\narXiv\npreprint arXiv:2305.19523.\nMichael Himsolt. 1997. Gml: A portable graph file for-\nmat. Technical report, Technical report, Universitat\nPassau.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8).\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong,\nHongyu Ren, Bowen Liu, Michele Catasta, and Jure\nLeskovec. 2020. Open graph benchmark: Datasets\nfor machine learning on graphs. Advances in neural\ninformation processing systems, 33:22118–22133.\nWenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji,\nand Yongfeng Zhang. 2024. Up5: Unbiased foun-\ndation model for fairness-aware recommendation.\nEACL.\nWenyue Hua, Shuyuan Xu, Yingqiang Ge, and\nYongfeng Zhang. 2023. How to index item ids for\nrecommendation foundation models. SIGIR-AP.\nJianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua,\nYingqiang Ge, Juntao Tan, and Yongfeng Zhang.\n2024. Genrec: Large language model for genera-\ntive recommendation. ECIR.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Struct-\ngpt: A general framework for large language model\nto reason over structured data.\narXiv preprint\narXiv:2305.09645.\nDongkwan Kim and Alice Oh. 2022. How to find your\nfriendly neighborhood: Graph attention design with\nself-supervision. arXiv preprint arXiv:2204.04879.\nJinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho,\nMoontae Lee, Honglak Lee, and Seunghoon Hong.\n2022. Pure transformers are powerful graph learners.\nAdvances in Neural Information Processing Systems,\n35:14582–14595.\nThomas N Kipf and Max Welling. 2016.\nSemi-\nsupervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nWeirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei,\nand Bolin Ding. 2021. Coarformer: Transformer for\nlarge graph via graph coarsening.\nYann LeCun, Yoshua Bengio, et al. 1995. Convolu-\ntional networks for images, speech, and time series.\nThe handbook of brain theory and neural networks,\n3361(10):1995.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nGuohao Li, Matthias Müller, Bernard Ghanem, and\nVladlen Koltun. 2021. Training graph neural net-\nworks with 1000 layers. In International conference\non machine learning, pages 6437–6449. PMLR.\nGuohao Li, Chenxin Xiong, Ali Thabet, and Bernard\nGhanem. 2020. Deepergcn: All you need to train\ndeeper gcns. arXiv preprint arXiv:2006.07739.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nZelong Li, Wenyue Hua, Hao Wang, He Zhu, and\nYongfeng Zhang. 2024. Formal-LLM: Integrating\nFormal Language and Natural Language for Control-\nlable LLM-based Agents. arXiv:2402.00798.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu,\nMingde Zhao, Shuyuan Zhang, Xiao-Wen Chang,\nand Doina Precup. 2022. Revisiting heterophily for\ngraph neural networks. Advances in neural informa-\ntion processing systems, 35:1362–1375.\nSitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina\nPrecup. 2019. Break the ceiling: Stronger multi-\nscale deep graph convolutional networks. Advances\nin neural information processing systems, 32.\nCostas Mavromatis, Vassilis N Ioannidis, Shen Wang,\nDa Zheng, Soji Adeshina, Jun Ma, Han Zhao, Chris-\ntos Faloutsos, and George Karypis. 2023. Train your\nown gnn teacher: Graph-aware distillation on textual\ngraphs. arXiv preprint arXiv:2304.10668.\nFederico Monti, Davide Boscaini, Jonathan Masci,\nEmanuele Rodola, Jan Svoboda, and Michael M\nBronstein. 2017. Geometric deep learning on graphs\nand manifolds using mixture model cnns. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 5115–5124.\nLuis Müller, Mikhail Galkin, Christopher Morris, and\nLadislav Rampášek. 2023. Attending to graph trans-\nformers. arXiv preprint arXiv:2302.04181.\nDai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung.\n2022. Universal graph transformer self-attention net-\nworks. In Companion Proceedings of the Web Con-\nference 2022, pages 193–196.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2023. Unifying large\nlanguage models and knowledge graphs: A roadmap.\narXiv preprint arXiv:2306.08302.\nWonpyo Park, Woonggi Chang, Donggeon Lee, Juntae\nKim, and Seung-won Hwang. 2022. Grpe: Relative\npositional encoding for graph transformer. arXiv\npreprint arXiv:2201.12787.\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang,\nYu Lei, and Bo Yang. 2020. Geom-gcn: Geomet-\nric graph convolutional networks. arXiv preprint\narXiv:2002.05287.\nChen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and\nYong Liu. 2023. Can large language models em-\npower molecular property prediction? arXiv preprint\narXiv:2307.07443.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie,\nYing Wei, Wenbing Huang, and Junzhou Huang.\n2020. Self-supervised graph transformer on large-\nscale molecular data. Advances in Neural Informa-\ntion Processing Systems, 33:12559–12571.\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou\nHuang. 2019. Dropedge: Towards deep graph con-\nvolutional networks on node classification. arXiv\npreprint arXiv:1907.10903.\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Mal-\nisiewicz, and Andrew Rabinovich. 2020. Superglue:\nLearning feature matching with graph neural net-\nworks. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages\n4938–4947.\nYunsheng Shi, Zhengjie Huang, Shikun Feng, Hui\nZhong, Wenjin Wang, and Yu Sun. 2020. Masked\nlabel prediction: Unified message passing model\nfor semi-supervised classification. arXiv preprint\narXiv:2009.03509.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts.\narXiv preprint\narXiv:2010.15980.\nShikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormo-\nlabashi, Te-Lin Wu, Xuezhe Ma, and Nanyun Peng.\n2021. Com2sense: A commonsense reasoning bench-\nmark with complementary sentences. arXiv preprint\narXiv:2106.00969.\nRobert R Sokal and Theodore J Crovello. 1970. The\nbiological species concept: a critical evaluation. The\nAmerican Naturalist, 104(936):127–153.\nChuxiong Sun, Jie Hu, Hongming Gu, Jinpeng Chen,\nand Mingchuan Yang. 2020. Adaptive graph diffu-\nsion networks. arXiv preprint arXiv:2012.15024.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2818–2826.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017.\nGraph attention networks.\narXiv preprint\narXiv:1710.10903.\nHeng Wang, Shangbin Feng, Tianxing He, Zhaoxuan\nTan, Xiaochuang Han, and Yulia Tsvetkov. 2023a.\nCan language models solve graph problems in natural\nlanguage? arXiv preprint arXiv:2305.10037.\nQinyong Wang, Zhenxiang Gao, and Rong Xu. 2023b.\nExploring the in-context learning ability of large lan-\nguage model for biomedical concept linking. arXiv\npreprint arXiv:2307.01137.\nYangkun Wang, Jiarui Jin, Weinan Zhang, Yong Yu,\nZheng Zhang, and David Wipf. 2021. Bag of tricks\nfor node classification with graph neural networks.\narXiv preprint arXiv:2103.13355.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher\nFifty, Tao Yu, and Kilian Weinberger. 2019. Simpli-\nfying graph convolutional networks. In International\nconference on machine learning, pages 6861–6871.\nPMLR.\nNan Wu and Chaofan Wang. 2022. Gtnet: A tree-based\ndeep graph learning architecture.\narXiv preprint\narXiv:2204.12802.\nQitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and\nJunchi Yan. 2022. Nodeformer: A scalable graph\nstructure learning transformer for node classification.\nAdvances in Neural Information Processing Systems,\n35:27387–27401.\nQitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui\nZhang, Fan Nie, Haitian Jiang, Yatao Bian, and\nJunchi Yan. 2023.\nSimplifying and empowering\ntransformers for large-graph representations. arXiv\npreprint arXiv:2306.10759.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, Chengqi Zhang, and S Yu Philip. 2020. A com-\nprehensive survey on graph neural networks. IEEE\ntransactions on neural networks and learning sys-\ntems, 32(1):4–24.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie\nJegelka. 2018a. How powerful are graph neural net-\nworks? arXiv preprint arXiv:1810.00826.\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomo-\nhiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie\nJegelka. 2018b. Representation learning on graphs\nwith jumping knowledge networks. In International\nconference on machine learning, pages 5453–5462.\nPMLR.\nShuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2023.\nOpenp5: Benchmarking foundation models for rec-\nommendation. arXiv:2306.11134.\nNaganand Yadati, Madhav Nimishakavi, Prateek Yadav,\nVikram Nitin, Anand Louis, and Partha Talukdar.\n2019. Hypergcn: A new method for training graph\nconvolutional networks on hypergraphs. Advances in\nneural information processing systems, 32.\nChaoqi Yang, Ruijie Wang, Shuochao Yao, Shengzhong\nLiu, and Tarek Abdelzaher. 2020.\nRevisiting\nover-smoothing in deep gcns.\narXiv preprint\narXiv:2003.13663.\nZhilin Yang, William Cohen, and Ruslan Salakhudi-\nnov. 2016. Revisiting semi-supervised learning with\ngraph embeddings. In International conference on\nmachine learning, pages 40–48. PMLR.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and Tie-\nYan Liu. 2021. Do transformers really perform badly\nfor graph representation? Advances in Neural Infor-\nmation Processing Systems, 34:28877–28888.\nZhitao Ying, Jiaxuan You, Christopher Morris, Xiang\nRen, Will Hamilton, and Jure Leskovec. 2018. Hi-\nerarchical graph representation learning with differ-\nentiable pooling. Advances in neural information\nprocessing systems, 31.\nLe Yu, Leilei Sun, Bowen Du, Tongyu Zhu, and Weifeng\nLv. 2022a. Label-enhanced graph neural network for\nsemi-supervised node classification. IEEE Transac-\ntions on Knowledge and Data Engineering.\nLu Yu, Shichao Pei, Lizhong Ding, Jun Zhou, Longfei\nLi, Chuxu Zhang, and Xiangliang Zhang. 2022b.\nSail: Self-augmented graph contrastive learning. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 8927–8935.\nJiawei Zhang. 2023.\nGraph-toolformer:\nTo em-\npower llms with graph reasoning ability via\nprompt augmented by chatgpt.\narXiv preprint\narXiv:2304.11116.\nJiayou\nZhang,\nZhirui\nWang,\nShizhuo\nZhang,\nMegh\nManoj\nBhalerao,\nYucong\nLiu,\nDawei\nZhu, and Sheng Wang. 2021a.\nGraphprompt:\nBiomedical entity normalization using graph-based\nprompt templates. arXiv preprint arXiv:2112.03002.\nLei Zhang, Xiaodong Yan, Jianshan He, Ruopeng Li,\nand Wei Chu. 2023a. Drgcn: Dynamic evolving\ninitial residual for deep graph convolutional networks.\narXiv preprint arXiv:2302.05083.\nWentao Zhang, Zeang Sheng, Yuezihan Jiang, Yikuan\nXia, Jun Gao, Zhi Yang, and Bin Cui. 2021b. Eval-\nuating deep graph neural networks. arXiv preprint\narXiv:2108.00955.\nWentao Zhang, Ziqi Yin, Zeang Sheng, Yang Li, Wen\nOuyang, Xiaosen Li, Yangyu Tao, Zhi Yang, and\nBin Cui. 2022a. Graph attention multi-layer percep-\ntron. In Proceedings of the 28th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining,\npages 4560–4570.\nYiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hong-\nsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu\nYue. 2023b.\nMeta-transformer: A unified frame-\nwork for multimodal learning.\narXiv preprint\narXiv:2307.10802.\nZaixi Zhang, Qi Liu, Qingyong Hu, and Chee-Kong Lee.\n2022b. Hierarchical graph transformer with adaptive\nnode sampling.\nAdvances in Neural Information\nProcessing Systems, 35:21171–21183.\nZhilu Zhang and Mert Sabuncu. 2018. Generalized\ncross entropy loss for training deep neural networks\nwith noisy labels. Advances in neural information\nprocessing systems, 31.\nJianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian\nLiu, Rui Li, Xing Xie, and Jian Tang. 2023. Learning\non large-scale text-attributed graphs via variational\ninference. ICLR.\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan\nZhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,\nChangcheng Li, and Maosong Sun. 2020. Graph\nneural networks: A review of methods and applica-\ntions. AI open, 1:57–81.\nAPPENDIX\nA\nImplementation Details\nWe employ a multi-prompt instruction-tuning\nframework for all of our experiments and report\ntest accuracy as our metric. Also, we employ a\nsimple MLP over the default feature embedding\nof the node tokens to align their dimension with\nthe natural language word token embeddings. All\nof all our experiments are conducted on four 40G\nA100 GPUs.\nFor ogbn-arxiv dataset, we adopt the same\ndataset splits as in the OGB open benchmark (Hu\net al., 2020), which is 54%/18%/28%. It takes 3.5\nhours per epoch for Flan-T5-Large and 6 hours per\nepoch for Llama-7b during training. For Cora and\nPubMed datasets, we use the version that contains\nraw text information proposed in (He et al., 2023)\nand employ a 60%/20%/20% train/val/test splits for\nour experiments. It takes about 1.5 hours per epoch\nfor Flan-T5-Large (770M) and 2.5 hours per epoch\nfor Llama-v1-7b-LoRA (18M) during training.\nTo investigate InstructGLM’s performance un-\nder low-label-ratio training setting, following Yang\net al. (2016), we conduct further experiments on\nthe PubMed dataset with the fixed 20 labeled train-\ning nodes per class at a 0.3% label ratio, and it\ntakes about 5 minutes per epoch for Flan-T5-Large\nand 15 minutes per epoch for Llama-v1-7b during\ntraining due to limited labeled data.\nFor both normal setting and low-label-ratio set-\nting, the inference time is about 35ms on Flan-T5-\nLarge and 450ms on Llama-7b per graph prompt\nsentence.\nIn terms of hyper-parameter selection, we per-\nform grid search within the specified range for the\nfollowing parameters: (learning rate: 1e-5, 3e-5,\n8e-5, 1e-4, 3e-4, 1e-3), (batch size: 32, 64, 128,\n256, 512). We employed the AdamW (Loshchilov\nand Hutter, 2017) optimizer with a weight decay at\n0. All experiments are conducted with 4 epochs.\nB\nDataset Statistics\nThe detailed statistics of the datasets are shown in\nTable 5.\nC\nDetailed Discussions on Future Work\nPotential valuable future work can be explored\nalong three dimensions:\n• For TAGs, our experiments only used the de-\nfault OGB-feature embeddings. Future work can\nconsider using more advanced TAG-related em-\nbedding features such as LLM-based features\nlike TAPE (He et al., 2023) and SimTeG (Duan\net al., 2023). Additionally, leveraging LLM for\nChain-of-Thought (Wei et al., 2022), structure in-\nformation summary, and other data augmentation\ntechniques to generate more powerful instruction\nprompts will be a promising research direction\nfor graph language models.\n• InstructGLM can be integrated into frameworks\nlike GAN and GLEM (Goodfellow et al., 2014;\nZhao et al., 2023) for multi-model iterative train-\ning, or utilize off-the-shelf GNNs for knowl-\nedge distillation (Mavromatis et al., 2023). Also,\nclassic graph machine learning techniques like\nlabel reuse, Self-Knowledge Distillation (Self-\nKD), Correct & Smooth can further enhance the\nmodel’s performance.\n• Benefiting from the high flexibility and expres-\nsiveness of language and the highly scalable de-\nsign of our instruction prompts, InstructGLM can\nbe easily extended to various kinds of graphs and\nmodalities within a unified generative language\nmodeling framework, since “everything can be\ntokenized,” including texts, images, videos, au-\ndios and other modalities, and inserted into lan-\nguage prompts. Besides, our designed instruction\nprompts can be further used for inductive node\nclassification tasks. Furthermore, with only slight\nmodifications to the prompts, tasks such as graph\nclassification, intermediate node or path predic-\ntion, and even relation-based question answering\ntasks in knowledge graphs with rich edge features\ncan be effectively deployed.\nD\nInstruction Prompts\nWe present all of our designed instruction prompts.\nIt is worth noting that we follow the following\nconventions when numbering the prompts:\n• The length of each prompt number is 4.\n• The first digit represents the task index, where\n1 represents the node classification task and 2\nrepresents the link prediction task.\n• The second digit represents whether node fea-\ntures or edge features (such as text information)\nother than numerical feature embedding are used\nin the prompt. 1 means not used and 2 means\nused.\nDataset\n#Node\n#Edge\n#Class\nDefault Feature\n#Features\nogbn-arxiv\n169,343\n1,166,243\n40\nSkip-gram / GIANT\n128 / 768\nCora\n2,708\n5,429\n7\nBag of Words\n1433\nPubMed\n19,717\n44,338\n3\nTF-IDF\n500\nTable 5: Dataset Statistics\n• The third digit represents the maximum hop or-\nder corresponding to the structural information\nconsidered in this prompt. 1 represents only the\n1-hop neighbors are included, while 2 and 3 rep-\nresent the structural information including 2-hop\nand 3-hop neighbors, respectively.\n• The fourth digit represents whether the interme-\ndiate node information (i.e. the path) in the high-\norder connection is considered in this prompt. If\nthe digit is even, it means that the intermediate\nnode is considered, while an odd digit indicates\notherwise.\n• Specially, in node classiﬁcation task, we de-\nsigned a graph-structure-free prompt and num-\nbered it as 1-0-0-0.\nD.1\nNode Classiﬁcation\nTask-speciﬁc preﬁx:\nClassify the paper according to its topic into\none of the following categories:{{All Category\nList}}.\\n Node represents academic paper with a\nspecific topic, link represents a citation\nbetween the two papers. Pay attention to the\nmulti-hop link relationship between the nodes.\nPrompt ID: 1-1-1-1\nInput template:\n{{central node}} is connected with {{1-hop\nneighbor list}} within one hop. Which category\nshould {{central node}} be classified as?\nTarget template: {{category}}\nPrompt ID: 1-1-2-1\nInput template:\n{{central node}} is connected with {{2-hop\nneighbor list}} within two hops. Which category\nshould {{central node}} be classified as?\nTarget template: {{category}}\nPrompt ID: 1-1-2-2\nInput template:\n{{central node}} is connected with {{2-hop\nneighbor list}} within two hops through {{the\ncorresponding 1-hop intermediate node list}},\nrespectively. Which category should {{central\nnode}} be classified as?\nTarget template: {{category}}\nPrompt ID: 1-1-3-1\nInput template:\n{{central node}} is connected with {{3-hop\nneighbor list}} within three hops. Which category\nshould {{central node}} be classified as?\nTarget template: {{category}}\nPrompt ID: 1-1-3-2\nInput template:\n{{central node}} is connected with {{3-hop\nneighbor list}} within three hops through {{the\ncorresponding 2-hop intermediate path list}},\nrespectively. Which category should {{central\nnode}} be classified as?\nTarget template: {{category}}\nPrompt ID: 1-2-1-1\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{1-hop neighbor list attached with text\nfeature}} within one hop. Which category should\n({{central node}},{{text feature}}) be classified\nas?\nTarget template: {{category}}\nPrompt ID: 1-2-2-1\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{2-hop neighbor list attached with text\nfeature}} within two hops. Which category should\n({{central node}},{{text feature}}) be classified\nas?\nTarget template: {{category}}\nPrompt ID: 1-2-2-2\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{2-hop neighbor list attached with text\nfeature}} within two hops through {{the\ncorresponding 1-hop intermediate node list\nattached with text feature}}, respectively.\nWhich category should ({{central node}},{{text\nfeature}}) be classified as?\nTarget template: {{category}}\nPrompt ID: 1-2-3-1\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{3-hop neighbor list attached with text\nfeature}} within three hops. Which category\nshould ({{central node}},{{text feature}}) be\nclassified as?\nTarget template: {{category}}\nPrompt ID: 1-2-3-2\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{3-hop neighbor list attached with text\nfeature}} within three hops through {{the\ncorresponding 2-hop intermediate path list\nattached with text feature}}, respectively.\nWhich category should ({{central node}},{{text\nfeature}}) be classified as?\nTarget template: {{category}}\nPrompt ID: 1-0-0-0\nInput template:\n{{central node}} is featured with its {{text\nfeature}}. Which category should {{central node}}\nbe classified as?\nTarget template: {{category}}\nD.2\nLink Prediction\nTask-speciﬁc preﬁx:\nPerform Link Prediction for the central node:\\n\nNode represents academic paper with a specific\ntopic, link represents a citation between the two\npapers. Pay attention to the multi-hop link\nrelationship between the nodes.\nPrompt ID: 2-1-1-1\nInput template:\n{{central node}} is connected with {{1-hop\nneighbor list}} within one hop. Will {{candidate\nnode}} be connected with {{central node}} within\none hop?\nTarget template: {{yes/no}}\nPrompt ID: 2-1-1-2\nInput template:\n{{central node}} is connected with {{1-hop\nneighbor list}} within one hop. Which other node\nwill be connected to {{central node}} within one\nhop?\nTarget template: {{node_id}}\nPrompt ID: 2-1-2-1\nInput template:\n{{central node}} is connected with {{2-hop\nneighbor list}} within two hops. Will {{candidate\nnode}} be connected to {{central node}} within\ntwo hops?\nTarget template: {{yes/no}}\nPrompt ID: 2-1-2-2\nInput template:\n{{central node}} is connected with {{2-hop\nneighbor list}} within two hops through {{the\ncorresponding 1-hop intermediate node list}},\nrespectively. Will {{candidate node}} be\nconnected to {{central node}} within two hops\nthrough {{the specified 1-hop intermediate\nnode}}?\nTarget template: {{yes/no}}\nPrompt ID: 2-1-2-3\nInput template:\n{{central node}} is connected with {{2-hop\nneighbor list}} within two hops. Which other node\nwill be connected to {{central node}} within two\nhops?\nTarget template: {{node_id}}\nPrompt ID: 2-1-2-4\nInput template:\n{{central node}} is connected with {{2-hop\nneighbor list}} within two hops through {{the\ncorresponding 1-hop intermediate node list}},\nrespectively. Which other node will be connected\nto {{central node}} within two hops through {{the\nspecified 1-hop intermediate node}}?\nTarget template: {{node_id}}\nPrompt ID: 2-1-3-1\nInput template:\n{{central node}} is connected with {{3-hop\nneighbor list}} within three hops. Will\n{{candidate node}} be connected with {{central\nnode}} within three hops?\nTarget template: {{yes/no}}\nPrompt ID: 2-1-3-2\nInput template:\n{{central node}} is connected with {{3-hop\nneighbor list}} within three hops through {{the\ncorresponding 2-hop intermediate path list}},\nrespectively. Will {{candidate node}} be\nconnected to {{central node}} within three hops\nthrough {{the specified 2-hop intermediate\npath}}?\nTarget template: {{yes/no}}\nPrompt ID: 2-1-3-3\nInput template:\n{{central node}} is connected with {{3-hop\nneighbor list}} within three hops. Which other\nnode will be connected to {{central node}} within\nthree hops?\nTarget template: {{node_id}}\nPrompt ID: 2-1-3-4\nInput template:\n{{central node}} is connected with {{3-hop\nneighbor list}} within three hops through {{the\ncorresponding 2-hop intermediate path list}},\nrespectively. Which other node will be connected\nto {{central node}} within three hops through\n{{the specified 2-hop intermediate path}}?\nTarget template: {{node_id}}\nPrompt ID: 2-2-1-1\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{1-hop neighbor list attached with text\nfeature}} within one hop. Will ({{candidate\nnode}},{{candidate text feature}}) be connected\nto ({{central node}},{{text feature}}) within\none hop?\nTarget template: {{yes/no}}\nPrompt ID: 2-2-1-2\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{1-hop neighbor list attached with text\nfeature}} within one hop. Which other node will\nbe connected to ({{central node}},{{text\nfeature}}) within one hop?\nTarget template: {{node_id}}\nPrompt ID: 2-2-2-1\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{2-hop neighbor list attached with text\nfeature}} within two hops. Will ({{candidate\nnode}},{{candidate text feature}}) be connected\nto ({{central node}},{{text feature}}) within\ntwo hops?\nTarget template: {{yes/no}}\nPrompt ID: 2-2-2-2\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{2-hop neighbor list attached with text\nfeature}} within two hops through {{the\ncorresponding 1-hop intermediate node list\nattached with text feature}}, respectively. Will\n({{candidate node}},{{candidate text feature}})\nbe connected to ({{central node}},{{text\nfeature}}) within two hops through ({{the\nspecified 1-hop intermediate node attached with\ntext feature}})?\nTarget template: {{yes/no}}\nPrompt ID: 2-2-2-3\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{2-hop neighbor list attached with text\nfeature}} within two hops. Which other node will\nbe connected to ({{central node}},{{text\nfeature}}) within two hops?\nTarget template: {{node_id}}\nPrompt ID: 2-2-2-4\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{2-hop neighbor list attached with text\nfeature}} within two hops through {{the\ncorresponding 1-hop intermediate node list\nattached with text feature}}, respectively.\nWhich other node will be connected to ({{central\nnode}},{{text feature}}) within two hops through\n({{the specified 1-hop intermediate node attached\nwith text feature}})?\nTarget template: {{node_id}}\nPrompt ID: 2-2-3-1\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{3-hop neighbor list attached with text\nfeature}} within three hops. Will ({{candidate\nnode}},{{candidate text feature}}) be connected\nwith ({{central node}},{{text feature}}) within\nthree hops?\nTarget template: {{yes/no}}\nPrompt ID: 2-2-3-2\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{3-hop neighbor list attached with text\nfeature}} within three hops through {{the\ncorresponding 2-hop intermediate path list\nattached with text feature}}, respectively. Will\n({{candidate node}},{{candidate text feature}})\nbe connected to ({{central node}},{{text\nfeature}}) within three hops through {{the\nspecified 2-hop intermediate path attached with\ntext feature}}?\nTarget template: {{yes/no}}\nPrompt ID: 2-2-3-3\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{3-hop neighbor list attached with text\nfeature}} within three hops. Which other node\nwill be connected to ({{central node}},{{text\nfeature}}) within three hops?\nTarget template: {{node_id}}\nPrompt ID: 2-2-3-4\nInput template:\n({{central node}},{{text feature}}) is connected\nwith {{3-hop neighbor list attached with text\nfeature}} within three hops through {{the\ncorresponding 2-hop intermediate path list\nattached with text feature}}, respectively.\nWhich other node will be connected to ({{central\nnode}},{{text feature}}) within three hops\nthrough {{the specified 2-hop intermediate path\nattached with text feature}}?\nTarget template: {{node_id}}\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR",
    "cs.LG"
  ],
  "published": "2023-08-14",
  "updated": "2024-02-06"
}