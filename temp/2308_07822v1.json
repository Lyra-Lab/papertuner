{
  "id": "http://arxiv.org/abs/2308.07822v1",
  "title": "Deep reinforcement learning for process design: Review and perspective",
  "authors": [
    "Qinghe Gao",
    "Artur M. Schweidtmann"
  ],
  "abstract": "The transformation towards renewable energy and feedstock supply in the\nchemical industry requires new conceptual process design approaches. Recently,\nbreakthroughs in artificial intelligence offer opportunities to accelerate this\ntransition. Specifically, deep reinforcement learning, a subclass of machine\nlearning, has shown the potential to solve complex decision-making problems and\naid sustainable process design. We survey state-of-the-art research in\nreinforcement learning for process design through three major elements: (i)\ninformation representation, (ii) agent architecture, and (iii) environment and\nreward. Moreover, we discuss perspectives on underlying challenges and\npromising future works to unfold the full potential of reinforcement learning\nfor process design in chemical engineering.",
  "text": "©Q. Gao and A. M. Schweidtmann\nPage 1 of 14\nDeep reinforcement learning for process design: Review and perspective\nQinghe Gao1, Artur M. Schweidtmann1,∗\n1 Delft University of Technology, Department of Chemical Engineering, Van der Maasweg 9, Delft 2629 HZ,\nThe Netherlands\nAbstract: The transformation towards renewable energy and feedstock supply in the chemical industry\nrequires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer\nopportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine\nlearning, has shown the potential to solve complex decision-making problems and aid sustainable process\ndesign. We survey state-of-the-art research in reinforcement learning for process design through three\nmajor elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward.\nMoreover, we discuss perspectives on underlying challenges and promising future works to unfold the full\npotential of reinforcement learning for process design in chemical engineering.\nKeywords: Process synthesis, deep reinforcement learning, machine learning, chemical engineering,\nartificial intelligence, graph neural network\n1\nIntroduction\nThe chemical industry is facing a rapid paradigm shift towards a circular economy based on renewable\nenergy and feedstock supply1,2. This poses several challenges for conceptual process design due to\nthe increasing complexity of the design task, the lack of experienced engineers, and the pressure on\nimproving sustainability and profitability while shortening development times. Thus, there is a need for\nnew methodologies and tools that support engineers to design sustainable processes in a more efficient\nway.\nComputer-aid process design (CAPD) is widely used in process systems engineering (PSE) for con-\nceptual process design3,4. Particularly, derivative-based optimization algorithms are already available\nin commercial process simulation software to determine optimal design and operating variables. To\ndetermine optimal process structures, superstructure optimization methods are the de facto state of\nthe art, where possible process alternatives are modeled and subsequently selected by mixed-integer\noptimization methods5,6. While superstructure methods have been very successful in PSE, they also\nhave a number of shortcomings that limit industrial applications7, including (manual) setup of all process\nalternatives, the need to implement process models in an optimization environment, and the difficulties of\nsolving resulting mixed-integer nonlinear optimization problems (MINLPs). Also, artificial intelligence\n(AI) methods have been proposed for conceptual process design in the past8–10. For example, expert\nsystems were already proposed in the 1970s11,12. However, these heuristic-based systems were difficult\nCorresponding author: ∗A. M. Schweidtmann, E-mail: a.schweidtmann@tudelft.nl\narXiv:2308.07822v1  [cs.LG]  15 Aug 2023\nReinforcement learning in process design\n16.8.2023\nto maintain and extend. Also, recent generative modeling approaches for process design require large\ntraining data13,14 and are not the focus of this review.\nRecently, deep reinforcement learning (RL) has shown its potential to solve complex decision-making\nproblems at human-like or even superhuman level15–17. RL is a computational approach for goal-directed\nlearning and decision-making through the direct interaction of an agent with its environment18. Also,\nRL has seen its first applications in chemical engineering for process control19,20 and scheduling21,22.\nNotably, Yokogawa is even using RL to operate an industrial chemical process since 2022.\nRL has the potential to be a game-changer for process design because it can sequentially build up\nprocess topologies without pre-defined structures (i.e., it is a superstructure-free method), it can directly\nlearn through interactions with established simulation software, and it has a large learning capacity. So\nfar, RL for process design is at an early research stage (Technical Readiness Level of 3 to 4). There\nhave been a few first steps towards applying RL to absorption–stripping process23, energy systems\ndesign24,25, unit operation design26, separation process design27–34, solvent extraction process design35,\nsingle mixed-refrigerant process design36, and synthesis reaction process design37–39. We present a review\nand perspective on RL for conceptual process design.\n2\nState of the art\nThe general framework of RL in process design is shown in Figure 1. The agent learns to design processes\nby iteratively placing unit operations with design and operating variables, and simulating the resulting\nprocesses in the environment, ultimately obtaining the optimal policy π∗which designs optimal processes.\nMathematically, this problem can be formulated as Markov decision processes (MDP): M = {S, A, T, R}\nwith states s ∈S, actions a ∈A, the transition function T : S × A × S →[0, 1], and the reward function\nR : S × A × S →R. In the context of process design, the states s represent the flowsheet topology\nas well as all relevant design specifications, operating variables, thermodynamic stream data, flowrates,\nand compositions. The agent takes the current states s as input to take action a. The actions contain\ndiscrete choices (e.g., the selection of open streams, unit operation types, or number of stages) as well\nas continuous choices (e.g., the length of a reactor or operating flowrates), namely hybrid action space.\nUsually, decisions in process design are also hierarchical. For example, the agent first determines an\nopen stream to add a unit operation, then the type of unit operation, then design variables, and finally\noperating variables. After a new unit operation is added, the new flowsheet is simulated in an environment\n(e.g., a process simulation software). After finishing a flowsheet, a numerical reward R is returned to the\nagent. This corresponds to the objective for optimization. By repeating the design of multiple flowsheets\nand receiving corresponding rewards, the agent learns to design processes that maximize the reward.\nIn this section, we survey the RL state-of-the-art literature (summarized in Table 1) based on (i)\n©Q. Gao and A. M. Schweidtmann\nPage 2 of 14\nReinforcement learning in process design\n16.8.2023\nState\nReward\nAction\nEnvironment\nAgent\nNeural networks\nFlowsheet simulator\nStream table\n1. Select an open stream\n2. Add a new unit operation\n3. Select design/operation variables\nFigure 1: General framework of reinforcement learning for process design.\ninformation representation, (ii) agent architecture, and (iii) environment and reward.\n2.1\nInformation representation\nChemical processes comprise various information, such as process topology, thermodynamic states,\nflowrates, concentrations, design variables, operating variables, components, and underlying mechanistic\nknowledge. The meaningful representation of the chemical process information is critical for the learning\nand generalization of RL agents. For RL in process design, there are currently two methods for information\nrepresentations: Matrix23–37 and graph38,39.\nIn matrix-based representation, flowsheets are represented by fixed-size matrices. Within the flowsheet\nmatrix, the connectivity, stream compositions, thermodynamic stream data, and design variables are\nusually concatenated. For example, G¨ottl et al.29–31 represented flowsheets as 16 × 28 matrices, where\neach row represents a stream and encompasses four parts: {v, u, d, t}. vi has five entries, which describe\nthe molar fractions and total molar flowrate of stream i. ui stores the type of the unit operation that is\ndownstream of stream i as one-hot encoding. Furthermore, di stores the connectivity of unit operations\nand has sixteen entries (i.e., this corresponds to the adjacency matrix). Finally, ti has two entries: the\nfirst entry indicates whether the task is terminated (0 if not terminated), and the second entry indicates\nwhether stream i is still unused (0 if unused). Most of the previous publications use matrix representations\nof flowsheet states (c.f. Table 1).\nIn graph-based representation, flowsheets are represented by directed heterogeneous graphs. Flow-\nsheet graphs consist of nodes and edges. Unit operations are represented by nodes, also referred to as\n©Q. Gao and A. M. Schweidtmann\nPage 3 of 14\nReinforcement learning in process design\n16.8.2023\nTable 1: An overview of the reviewed literature and different choices of elements in RL for process design.\nWe use Repr. to indicate information representation. Furthermore, we utilize Dis., Cont., and Hier. to\ndenote discrete, continuous, and hierarchical action space of RL. within the decisions of RL, we use\nTopo., Des. and Oper. to represent actions involved in changing flowsheet topologies, selecting design\nvariables and operating variables, respectively.\nRef.\nRepr.\nAgent architecture\nEnvironment\nAction space\nDecisions\nDis.\nCont.\nHier.\nTopo.\nDes.\nOper.\n[27]\nMatrix\nActor-Critic (SAC)\nCOCO\n✓\n✓\n✓\n✓\n✓\n[28]\nMatrix\nActor-Critic (SAC)\nAspen Plus\n✓\n✓\n✓\n✓\n[23]\nMatrix\nActor-Critic ( - )\nAspen Plus\n✓\n✓\n[35]\nMatrix\nActor-Critic (PPO)\nShort-cut\n✓\n✓\n[29] [30] [31]\nMatrix\nActor-Critic (Two players)\nShort-cut\n✓\n✓\n✓\n[33][34]\nMatrix\nActor-Critic (PPO)\nShort-cut\n✓\n✓\n✓\n✓\n✓\n✓\n[25]\nMatrix\nActor-Critic (ACER)\nShort-cut\n✓\n✓\n[24]\nMatrix\nPolicy-based (Policy search)\nShort-cut\n✓\n✓\n[26]\nMatrix\nPolicy-based (PG)\nShort-cut\n✓\n✓\n✓\n[37]\nMatrix\nValue-based (DQN)\nIDAES\n✓\n✓\n[36]\nMatrix\nValue-based (DQN)\nUniSim\n✓\n✓\n✓\n[32]\nMatrix\nValue-based (Q-learning)\nShort-cut\n✓\n✓\n✓\n✓\n[38]\nGraph\nActor-Critic (PPO)\nShort-cut\n✓\n✓\n✓\n✓\n✓\n✓\n[39]\nGraph\nActor-Critic (PPO)\nDWSIM\n✓\n✓\n✓\n✓\n✓\n✓\nvertices v ∈V , and streams are represented by edges evw ∈E connecting two nodes v and w. Importantly,\nnode feature vector f v ∈F V , and edge feature vector f evw ∈F E are associated with each node and\nedge, respectively. Within the node feature vectors, types of unit operation, design specifications, and\noperating points are encoded. The edge feature vectors contain thermodynamic states, concentrations,\nand flowrates. In the past, only our previous works used graph representations of flowsheets for RL38,39.\nThe comparison between flowsheet matrices and flowsheet graphs is still an open research question in\nthe context of RL in process design. Flowsheet matrices are easier to implement than flowsheet graphs and\nare used by the majority of the literature as shown in Table 1. Flowsheet matrices are processed by RL\nagents using multilayer perceptrons (MLPs) or convolutional neural networks (CNNs). However, every\nflowsheet graph has N! different adjacency matrices. CNNs and MLPs are not permutation equivariance\nf(PT xP) ̸= PT f(x)P\nwhere P is a permutation matrix, x is the input matrix and f is a MLP or CNN40. This means that such\nmodels depend on the arbitrary order of rows/columns in the flowsheet matrix and thus, cannot generalize\nover flowsheet topologies. Also, the neighborhood of an entry in the matrix does not correspond to\nphysical connectivity which makes learning using MLPs/CNNs more difficult as it requires learning long-\nrange interactions. In contrast, (message-passing) graph convolutional networks (GCNs) are permutation\nequivariance and they learn from the actual connectivity of flowsheet graphs. Furthermore, MLPs and\nCNNs require fixed-size inputs, e.g., a pre-defined maximum number of unit operations and streams,\nwhile GCNs are size-independent41.\n©Q. Gao and A. M. Schweidtmann\nPage 4 of 14\nReinforcement learning in process design\n16.8.2023\n2.2\nAgent architecture\nRL agents consist of two components: A policy and a learning algorithm. The policy describes the\nbehavior of the agent, mapping the current state s into an action a: π(s) = a. It is parameterized\nby function approximators such as MLPs. The learning algorithm is used to continuously update the\npolicy based on the actions, states, and rewards. Depending on the learning algorithms, the agent can\nbe characterized into three types: Value-based, policy-based, and actor-critic-based (AC).\nValue-based agents learn a functional approximator of the value function (Vπ(s)) to take actions. The\nvalue function outputs the expected returns after the current process step t given a state s and a policy\nπ: Vπ(s) = Eπ [Gt|s], where returns Gt:\nGt = Rt+1 + γRt+2 + · · · + γkRt+k+1 =\n∞\nX\nk=0\nγkRt+k+1\n(1)\nwhere γk are the discount rates to determine the present value of future rewards, and k is the process step\nfrom t to the end of the episode. Similarly, we can also derive the state-action value function, namely the\nquality function Qπ, which calculates the expected returns given a state s and action a, following policy\nπ: Qπ(s, a) = Eπ [Gt|s, a]. Depending on the calculated V-/Q-value, different search algorithms such as\nbest-first search or nearest neighbors are used to choose the final action. In the context of RL in process\ndesign, three works32,36,37 deployed Q-learning based agent to perform process synthesis tasks. However,\ntraditional value-based agents can only take discrete actions, which hinders further development because\ncontinuous decision-making of design or operating variables is vital in process design tasks.\nPolicy-based agents directly learn a functional approximator of the policy function. Specifically, the\npolicy approximator πθ maps the current states s to the actions a: πθ(s) = a. And the optimal policy π∗\nis obtained by maximizing the expected return Eθ [Gt] through policy gradient or policy-search methods.\nIn the context of RL in process design, Sachio et al.26 and Perera et al.24 utilized policy gradient methods\nand policy-search methods to perform process design tasks, respectively. Compared to the value-based\napproach, the policy-based agent can handle both discrete and continuous actions. However, policy-based\nmethods are known for high variance and sub-optimal local solutions42.\nAC agents combine the advantages of value-based and policy-based methods. AC consists of an\nactor, working as a functional approximator of the policy function, and a critic, serving as a functional\napproximator of the value function. Therefore, AC agents explicitly optimize both value and policy\nfunctions and are able to process both discrete and continuous action spaces. In the context of RL\nin process design, different types of AC agents have been used such as Proximal Policy Optimization\n(PPO)34353839, Soft Actor-Critic (SAC)27,28, Two-player game29–31, and Sample Efficient Actor Critic\nwith Prioritized Experience Replay (ACER)25.\nThe choice of agent architecture for RL in process design is an open question. AC RL is deemed to be\n©Q. Gao and A. M. Schweidtmann\nPage 5 of 14\nReinforcement learning in process design\n16.8.2023\na viable option because it combines the advantages of value-based and policy-based and can handle both\ndiscrete and continuous decisions. Specifically, PPO is the most popular algorithm in process design tasks\nwith the advantage of less complicated implementation and a stable learning process. However, PPO is\nan on-policy algorithm which means the optimized policy is the same as the policy for action selection.\nTherefore, PPO is less data-efficient than off-policy algorithms, such as SAC and ACER, which may take\nless time and fewer training episodes.\n2.3\nEnvironment and reward\nThe environment simulates the processes and computes a reward as feedback to the agent. Selecting\nan appropriate accuracy level for the environment is a vital task for RL in process design and depends\non the task-specific requirements and modeling intent. There are two main levels of accuracy: Shortcut\nand rigorous simulators. Shortcut simulators utilize approximated process models to ensure tractability\nbut can be inaccurate. Rigorous simulators involve more accurate process models that require longer\ncomputation times, as previous studies indicated28,39. In the past, RL for process design has used multiple\nprocess simulation software including open-source (DWSIM, IDEAS), non-commercial (COCO), and\ncommercial (UniSim, Aspen Plus) alternatives. Additionally, Seidenberg et al.34 leveraged knowledge\ngraphs to retrieve information about the design task, process knowledge, and the current state of the\nprocess. Notably, this knowledge graph was part of a manually-implemented environment and not directly\naccessible to the RL agent. Thus, the agent also relied on a flowsheet matrix representation as states.\nPrevious research optimized towards a single economic objective23,25–32,34,38,39. Also, some works\nintegrate purity, recovery, power consumption, and product flow rate into scalar reward functions35–37.\n3\nPerspectives\nDespite the success of a few initial research works, there is still a long way from simple case studies\nto the adaption of RL for process design in the industry. In our view, the big research challenge is the\ngeneralization of RL models. The training phase of current RL frameworks is essentially used like a\nderivative-free optimization approach (e.g., a genetic algorithm) to optimize the process topology for\none particular case study. Thus, a re-training is needed for a new case study and the agent fails to\ntransfer its learning to new situations. In general, deep RL has a higher learning capacity than genetic\nalgorithms (e.g., learnable parameters). The derivative-free optimization approach with RL does not\nuse the full potential of RL. Generalization requires an extension of the information representation\nand agent architecture to account for process-relevant knowledge. This includes domain expertise, prior\nprocess data, and physical constraints which are typically employed by engineers when designing chemical\nprocesses. Integrating this information would allow the RL agent to see what ”drives the process” and\n©Q. Gao and A. M. Schweidtmann\nPage 6 of 14\nReinforcement learning in process design\n16.8.2023\nultimately unlock the full potential of RL by learning from multiple processes. We envision that RL will\ngeneralize (to some extent) and ultimately design processes at inference time. In this section, we provide\nour perspectives on underlying challenges and a number of other promising future works.\n3.1\nInformation representation\nInformation representation is critical for RL since it encapsulates the current state of the environment,\nwhich directly affects decision-making for agents. However, current information representations still\nlack mechanistic knowledge and relevant process information. Furthermore, it neglects the appropriate\nrepresentation of molecules. Integrating the above information will significantly benefit the RL agent\nin generalization. Numerous representation methods could be potentially incorporated into RL in pro-\ncess design for process and molecular information representation. For example, Simplified Flowsheet\nInput-Line Entry-System (SFILES)43, SFILES 2.044, eSFILES45, and knowledge graphs34 could be used\nto enhance process representations. Similarly, molecular descriptors46, molecular graphs47, SMILES48,\nknowledge graphs49, and hypergraphs50 could be used to encode molecular information.\n3.2\nAgent architecture\nIn this section, we identify the limitations and potential improvements of the current agent architecture.\n3.2.1\nIntegration of mechanistic knowledge\nCurrent RL algorithms are not sufficient to transfer knowledge into the new processes because RL\nagents have a limited understanding of mechanistic knowledge and physical properties. Future work\ncould consider implementing a physics-informed RL agent by encoding information-rich representations\nsuch as knowledge graphs or hypergraphs to inform the agent. Furthermore, fundamental concepts, such\nas thermodynamic driving forces (Gibb’s free energy), could be included in the RL agents. This allows\nthe agent to learn general concepts that can be translated into other problems because they are based\non physics.\n3.2.2\nIntegration of prior data\nRL for process design is currently initialized randomly, which can lead to suboptimal solutions, excessive\ntraining times, and frequent convergence issues. Meanwhile, there is a large number of existing digitized\nchemical process data from simulation files and images51, which can potentially accelerate the learning\nprocess of the agent. Transfer learning improves learning performance by transferring knowledge from\ndifferent but relevant domains52. In RL for process design, many works26,37,39 already leveraged transfer\nlearning to accelerate the learning process. However, in the current transfer learning setting, the agent\nis still not learning from existing chemical process information. Future work can consider leveraging\n©Q. Gao and A. M. Schweidtmann\nPage 7 of 14\nReinforcement learning in process design\n16.8.2023\nencoder-decoder models such as Variational Autoencoders (VAEs) or transformers to learn from existing\nflowsheets and then applying transfer learning to the agent.\n3.2.3\nStochastic decision-making\nConsidering the uncertainty of energy/feedstock prices and demand is a major challenge for renewable\nprocesses7. However, current RL agents ignore fluctuations in energy/feedstock prices and demand.\nFuture research could separate design and operating variables in the RL agent. This allows the inclusion of\nmultiple scenarios for flexible operation. Besides, additional encoders or actors can be included to process\nstochastic energy prices, demands, and raw material compositions as additional inputs at an operational\nlevel. Therefore, the agent can automatically select the operating variables based on stochastic energy\nprices and demand in two-stage stochastic programming settings.\n3.2.4\nConstrainted decision-making\nConstrained decision-making is crucial for RL in process design to ensure optimal and safe performance.\nHowever, standard RL agent frameworks cannot enforce constraints but include constraints as ”soft”\npenalties in the reward functions28,38,39. Future work should focus on integrating constraints directly\nin the agent structure. As a first step, an additional critic network could be built to account for safety\nconstraints, guiding RL agents to explore appropriate regions in policy optimization53.\n3.3\nEnvironment and reward\nIn this section, we offer our perspectives on the limitations of the environment and reward setup and\nprovide several suggestions for future work.\n3.3.1\nStandardized simulation interfaces\nRL agents frequently interact with process simulators during the training process and the interaction\nrelies on individual interfaces as Table 1 shows. However, current interfaces are usually simulator-specific,\nwhich means that a new interface needs to be implemented from scratch whenever a new process simulator\nis included. This process is highly repetitive and inefficient, especially for incorporating multi-fidelity\nprocess models. Future work could implement a standardized simulation interface that enables the agent\nto exchange data efficiently and uniformly between different process simulators. This interface could\npotentially make use of existing standards such as CAPE-OPEN54 and DEXPI+55.\n3.3.2\nMulti-fidelity process models\nCurrent research only leverages a single fidelity model for RL in process design tasks. However, RL\nagents greatly benefit from pre-training on low-fidelity process simulators and subsequent fine-tuning on\n©Q. Gao and A. M. Schweidtmann\nPage 8 of 14\nReinforcement learning in process design\n16.8.2023\nhigh-fidelity process simulators39. Therefore, future research can focus on developing an agent that can\ndynamically select between multiple fidelity models during training. Specifically, a probabilistic model\ncan be developed to guide the RL actor based on multi-fidelity critics to reduce training times and resolve\nconvergence issues.\n3.3.3\nMulti-objective rewards\nCurrent RL frameworks for process design are not suitable for sustainable process design because they\nare limited to a single objective function. In the future, the current agent structures could be extended\nto include multiple objectives. For example, the critic network could predict multiple rewards, which will\nbe processed by multi-objective optimization to generate corresponding weights for each objective (e.g.\neconomic, environmental, safety)56.\n3.4\nIntegrated molecular and process design\nCurrent RL frameworks lack the co-design of molecules. However, the design or selection of molecules is a\ncritical task in many process design tasks, e.g., co-design of working fluids, solvents, or products57. Also,\nRL has already been used for molecular design58. Therefore, future work should consider integrating\nthese concepts using RL.\n3.5\nIntegrated process control and design\nIntegrating process design and process control becomes increasingly relevant as renewable energy and\nfeedstock demand is fluctuating. However, current RL works only consider process design and control\nseparately or focused on a specific unit operation design and control26. Future research could integrate\nthe whole process design with process control through the RL.\n4\nConclusions\nWe reviewed the state-of-the-art RL in process design in terms of information representation, agent\narchitecture, environment and reward. RL has shown initial promising results for process design but\ncurrent frameworks show limited generalization capabilities. To unlock the full potential of RL, new con-\ncepts for meaningful information representation are required. Furthermore, the integration of mechanistic\nknowledge, existing process data, uncertainties, and constraints would be highly beneficial for optimal\ndecision-making. Finally, future RL frameworks could also integrate molecular design and process control\ninto the conceptual process design. Many research groups have begun investigating the fast-moving field\nof RL for process design. Looking ahead, we expect many advancements in the upcoming years, revealing\n©Q. Gao and A. M. Schweidtmann\nPage 9 of 14\nReinforcement learning in process design\n16.8.2023\nthe potential of RL for process design. We hope that this review and perspective will facilitate future\nresearch in this exciting field.\nReferences\n[1]\nSamir Isaac Meramo-Hurtado and ´Angel Dario Gonz´alez-Delgado. “Process synthesis, analysis, and\noptimization methodologies toward chemical process sustainability”. In: Industrial & Engineering\nChemistry Research 60.11 (2021), pp. 4193–4217.\n[2]\nElias Martinez-Hernandez. “Trends in sustainable process design—from molecular to global scales”.\nIn: Current Opinion in Chemical Engineering 17 (2017), pp. 35–41.\n[3]\nTomio Umeda. “Computer aided process synthesis”. In: Computers & Chemical Engineering 7.4\n(1983), pp. 279–309. doi: 10.1016/0098-1354(83)80016-7.\n[4]\nAlexandre C Dimian and Costin Sorin Bildea. Chemical process design: Computer-aided case\nstudies. John Wiley & Sons, 2008.\n[5]\nT.F. Yee and I.E. Grossmann. “Simultaneous optimization models for heat integration—II. Heat\nexchanger network synthesis”. In: Computers & Chemical Engineering 14.10 (1990), pp. 1165–1184.\ndoi: 10.1016/0098-1354(90)85010-8.\n[6]\nLuca Mencarelli et al. “A review on superstructure optimization approaches in process system\nengineering”. In: Computers & Chemical Engineering 136 (2020), p. 106808. doi: 10.1016/j.\ncompchemeng.2020.106808.\n[7]\nAlexander Mitsos et al. “Challenges in process optimization for new feedstocks and energy sources”.\nIn: Computers & Chemical Engineering 113 (2018), pp. 209–221. doi: 10.1016/j.compchemeng.\n2018.03.013.\n[8]\nVenkat Venkatasubramanian. “The promise of artificial intelligence in chemical engineering: Is it\nhere, finally?” In: AIChE Journal 65.2 (2018), pp. 466–478. doi: 10.1002/aic.16489.\n[9]\nJay H. Lee, Joohyun Shin, and Matthew J. Realff. “Machine learning: Overview of the recent\nprogresses and implications for the process systems engineering field”. In: Computers & Chemical\nEngineering 114 (2018), pp. 111–121. doi: 10.1016/j.compchemeng.2017.10.008.\n[10]\nArtur M. Schweidtmann et al. “Machine Learning in Chemical Engineering: A Perspective”. In:\nChemie Ingenieur Technik 93.12 (2021), pp. 2029–2039. doi: 10.1002/cite.202100083.\n[11]\nJeffrey J. Siirola and Dale F. Rudd. “Computer-Aided Synthesis of Chemical Process Designs.\nFrom Reaction Path Data to the Process Task Network”. In: Industrial & Engineering Chemistry\nFundamentals 10.3 (1971), pp. 353–362. doi: 10.1021/i160039a003.\n©Q. Gao and A. M. Schweidtmann\nPage 10 of 14\nReinforcement learning in process design\n16.8.2023\n[12]\nVladimir Mahalec and R.L. Motard. “Procedures for the initial design of chemical processing\nsystems”. In: Computers & Chemical Engineering 1.1 (1977), pp. 57–68. doi: 10.1016/0098-\n1354(77)80008-2.\n[13]\nGabriel Vogel, Lukas Schulze Balhorn, and Artur M. Schweidtmann. “Learning from flowsheets:\nA generative transformer model for autocompletion of flowsheets”. In: Computers & Chemical\nEngineering 171 (2023), p. 108162. doi: 10.1016/j.compchemeng.2023.108162.\n[14]\nJonas Oeing, Fabian Henke, and Norbert Kockmann. “Machine Learning Based Suggestions of\nSeparation Units for Process Synthesis in Process Simulation”. In: Chemie Ingenieur Technik 93.12\n(2021), pp. 1930–1936. doi: 10.1002/cite.202100082.\n[15]\nVolodymyr Mnih et al. Playing Atari with Deep Reinforcement Learning. 2013. doi: 10.48550/\nARXIV.1312.5602.\n[16]\nMichal Kempka et al. “ViZDoom: A Doom-based AI research platform for visual reinforcement\nlearning”. In: 2016 IEEE Conference on Computational Intelligence and Games (CIG). IEEE,\n2016. doi: 10.1109/cig.2016.7860433.\n[17]\nDavid Silver et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go\nthrough self-play”. In: Science 362.6419 (2018), pp. 1140–1144. doi: 10.1126/science.aar6404.\n[18]\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[19]\nJ.C. Hoskins and D.M. Himmelblau. “Process control via artificial neural networks and reinforce-\nment learning”. In: Computers & Chemical Engineering 16.4 (1992), pp. 241–251. doi: 10.1016/\n0098-1354(92)80045-b.\n[20]\nS.P.K. Spielberg, R.B. Gopaluni, and P.D. Loewen. “Deep reinforcement learning approaches for\nprocess control”. In: 2017 6th International Symposium on Advanced Control of Industrial Processes\n(AdCONIP). IEEE, 2017. doi: 10.1109/adconip.2017.7983780.\n[21]\nChristian D. Hubbs et al. “A deep reinforcement learning approach for chemical production schedul-\ning”. In: Computers & Chemical Engineering 141 (2020), p. 106982. doi: 10.1016/j.compchemeng.\n2020.106982.\n[22]\nYoung Hoon Lee and Seunghoon Lee. “Deep reinforcement learning based scheduling within produc-\ntion plan in semiconductor fabrication”. In: Expert Systems with Applications 191 (2022), p. 116222.\ndoi: 10.1016/j.eswa.2021.116222.\n[23]\nJunghui Chen and Fan Wang. “Cost reduction of CO2 capture processes using reinforcement\nlearning based iterative design: A pilot-scale absorption–stripping system”. In: Separation and\nPurification Technology 122 (2014), pp. 149–158. doi: 10.1016/j.seppur.2013.10.023.\n©Q. Gao and A. M. Schweidtmann\nPage 11 of 14\nReinforcement learning in process design\n16.8.2023\n[24]\nA.T.D. Perera et al. “Introducing reinforcement learning to the energy system design process”. In:\nApplied Energy 262 (2020), p. 114580. doi: 10.1016/j.apenergy.2020.114580.\n[25]\nCesare Caputo et al. “Design and planning of flexible mobile Micro-Grids using Deep Reinforcement\nLearning”. In: Applied Energy 335 (2023), p. 120707. doi: 10.1016/j.apenergy.2023.120707.\n[26]\nSteven Sachio et al. “Integrating process design and control using reinforcement learning”. In:\nChemical Engineering Research and Design 183 (2022), pp. 160–169. doi: 10.1016/j.cherd.\n2021.10.032.\n[27]\nLaurence Illing Midgley. “Deep Reinforcement Learning for Process Synthesis”. In: (2020). doi:\n10.48550/ARXIV.2009.13265. arXiv: 2009.13265 [cs.LG].\n[28]\nStephan C. P. A. van Kalmthout, Laurence I. Midgley, and Meik B. Franke. “Synthesis of separation\nprocesses with reinforcement learning”. In: (2022). doi: 10.48550/ARXIV.2211.04327. arXiv:\n2211.04327 [cs.LG].\n[29]\nQuirin G¨ottl, Dominik G. Grimm, and Jakob Burger. “Automated synthesis of steady-state con-\ntinuous processes using reinforcement learning”. In: Frontiers of Chemical Science and Engineering\n16.2 (2021), pp. 288–302. doi: 10.1007/s11705-021-2055-9.\n[30]\nQuirin G¨ottl et al. “Automated Flowsheet Synthesis Using Hierarchical Reinforcement Learning:\nProof of Concept”. In: Chemie Ingenieur Technik 93.12 (2021), pp. 2010–2018. doi: 10.1002/\ncite.202100086.\n[31]\nQuirin G¨ottl, Dominik G. Grimm, and Jakob Burger. “Using Reinforcement Learning in a Game-\nlike Setup for Automated Process Synthesis without Prior Process Knowledge”. In: Computer\nAided Chemical Engineering. Elsevier, 2022, pp. 1555–1560. doi: 10.1016/b978-0-323-85159-\n6.50259-1.\n[32]\nAhmad Khan and Alexei Lapkin. “Searching for optimal process routes: A reinforcement learning\napproach”. In: Computers & Chemical Engineering 141 (2020), p. 107027. doi: 10 . 1016 / j .\ncompchemeng.2020.107027.\n[33]\nAhmad A. Khan and Alexei A. Lapkin. “Designing the process designer: Hierarchical reinforcement\nlearning for optimisation-based process design”. In: Chemical Engineering and Processing - Process\nIntensification 180 (2022), p. 108885. doi: 10.1016/j.cep.2022.108885.\n[34]\nJ. Raphael Seidenberg, Ahmad A. Khan, and Alexei A. Lapkin. “Boosting autonomous process\ndesign and intensification with formalized domain knowledge”. In: Computers & Chemical Engi-\nneering 169 (2023), p. 108097. doi: 10.1016/j.compchemeng.2022.108097.\n[35]\nSiby Jose Plathottam et al. “Solvent extraction process design using deep reinforcement learning”.\nIn: Journal of Advanced Manufacturing and Processing 3.2 (2021). doi: 10.1002/amp2.10079.\n©Q. Gao and A. M. Schweidtmann\nPage 12 of 14\nReinforcement learning in process design\n16.8.2023\n[36]\nSam Kim, Mun-Gi Jang, and Jin-Kuk Kim. “Process design and optimization of single mixed-\nrefrigerant processes with the application of deep reinforcement learning”. In: Applied Thermal\nEngineering 223 (2023), p. 120038. doi: 10.1016/j.applthermaleng.2023.120038.\n[37]\nDewei Wang et al. “Reinforcement learning for automated conceptual design of advanced energy\nand chemical systems”. In: (2022). doi: 10.21203/rs.3.rs-2248780/v1.\n[38]\nLaura Stops et al. “Flowsheet generation through hierarchical reinforcement learning and graph\nneural networks”. In: AIChE Journal 69.1 (2022). doi: 10.1002/aic.17938.\n[39]\nQinghe Gao et al. “Transfer learning for process design with reinforcement learning”. In: Computer\nAided Chemical Engineering. Elsevier, 2023, pp. 2005–2010. doi: 10.1016/b978-0-443-15274-\n0.50319-x.\n[40]\nWilliam L. Hamilton. Graph Representation Learning. Springer International Publishing, 2020.\ndoi: 10.1007/978-3-031-01588-5.\n[41]\nJie Zhou et al. “Graph neural networks: A review of methods and applications”. In: AI Open 1\n(2020), pp. 57–81. doi: 10.1016/j.aiopen.2021.01.001.\n[42]\nOfir Nachum et al. “Bridging the Gap Between Value and Policy Based Reinforcement Learning”.\nIn: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran\nAssociates, Inc., 2017. url: https://proceedings.neurips.cc/paper_files/paper/2017/\nfile/facf9f743b083008a894eee7baa16469-Paper.pdf.\n[43]\nLo¨ıc d’Anterroches. Process Flowsheet Generation & Design through a Group Contribution Ap-\nproach. [CAPEC], Department of Chemical Engineering, Technical University of Denmark, 2005.\n[44]\nGabriel Vogel et al. “SFILES 2.0: an extended text-based flowsheet representation”. In: Optimiza-\ntion and Engineering (2023). doi: 10.1007/s11081-023-09798-9.\n[45]\nVipul Mann, Rafiqul Gani, and Venkat Venkatasubramanian. “Intelligent Process Flowsheet Syn-\nthesis and Design using Extended SFILES Representation”. In: Computer Aided Chemical Engi-\nneering. Elsevier, 2023, pp. 221–226. doi: 10.1016/b978-0-443-15274-0.50036-6.\n[46]\nRoberto Todeschini and Viviana Consonni. “Molecular descriptors”. In: Recent Advances in QSAR\nStudies (2010), pp. 29–102.\n[47]\nArtur M. Schweidtmann et al. “Graph Neural Networks for Prediction of Fuel Ignition Quality”.\nIn: Energy & Fuels 34.9 (2020), pp. 11395–11407. doi: 10.1021/acs.energyfuels.0c01533.\n[48]\nDavid Weininger. “SMILES, a chemical language and information system. 1. Introduction to\nmethodology and encoding rules”. In: Journal of Chemical Information and Computer Sciences\n28.1 (1988), pp. 31–36. doi: 10.1021/ci00057a005.\n©Q. Gao and A. M. Schweidtmann\nPage 13 of 14\nReinforcement learning in process design\n16.8.2023\n[49]\nYin Fang et al. “Molecular Contrastive Learning with Chemical Element Knowledge Graph”. In:\nProceedings of the AAAI Conference on Artificial Intelligence 36.4 (2022), pp. 3968–3976. doi:\n10.1609/aaai.v36i4.20313.\n[50]\nHiroshi Kajino. “Molecular Hypergraph Grammar with its Application to Molecular Optimization”.\nIn: (2018). doi: 10.48550/ARXIV.1809.02745. arXiv: 1809.02745 [cs.LG].\n[51]\nLukas Schulze Balhorn et al. “Flowsheet Recognition using Deep Convolutional Neural Networks”.\nIn: Computer Aided Chemical Engineering. Elsevier, 2022, pp. 1567–1572. doi: 10.1016/b978-0-\n323-85159-6.50261-x.\n[52]\nKarl Weiss, Taghi M. Khoshgoftaar, and DingDing Wang. “A survey of transfer learning”. In:\nJournal of Big Data 3.1 (2016). doi: 10.1186/s40537-016-0043-6.\n[53]\nQisong Yang et al. “Safety-constrained reinforcement learning with a distributional safety critic”.\nIn: Machine Learning 112.3 (2022), pp. 859–887. doi: 10.1007/s10994-022-06187-8.\n[54]\nM Jarke et al. “CAPE-OPEN: Experiences from a standardization effort in chemical industries”.\nIn: Proc. of 1st IEEE Conference on Standardisation and Innovation in Information Technology\n(SIIT 99)(Aachen, Germany). 1999, pp. 25–35.\n[55]\nM Theissen and M Wiedau. “DEXPI P&ID Specification”. In: Version 0.11 (2016).\n[56]\nChunming Liu, Xin Xu, and Dewen Hu. “Multiobjective Reinforcement Learning: A Comprehen-\nsive Overview”. In: IEEE Transactions on Systems, Man, and Cybernetics: Systems 45.3 (2015),\npp. 385–398. doi: 10.1109/tsmc.2014.2358639.\n[57]\nPhilipp Rehner, Johannes Schilling, and Andr´e Bardow. “Molecule superstructures for computer-\naided molecular and process design”. In: Molecular Systems Design & Engineering 8.4 (2023),\npp. 488–499. doi: 10.1039/d2me00230b.\n[58]\nMarcus Olivecrona et al. “Molecular de-novo design through deep reinforcement learning”. In:\nJournal of Cheminformatics 9.1 (2017). doi: 10.1186/s13321-017-0235-x.\n©Q. Gao and A. M. Schweidtmann\nPage 14 of 14\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2023-08-15",
  "updated": "2023-08-15"
}