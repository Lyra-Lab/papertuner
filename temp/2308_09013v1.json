{
  "id": "http://arxiv.org/abs/2308.09013v1",
  "title": "Deep-seeded Clustering for Unsupervised Valence-Arousal Emotion Recognition from Physiological Signals",
  "authors": [
    "Antoine Dubois",
    "Carlos Lima Azevedo",
    "Sonja Haustein",
    "Bruno Miranda"
  ],
  "abstract": "Emotions play a significant role in the cognitive processes of the human\nbrain, such as decision making, learning and perception. The use of\nphysiological signals has shown to lead to more objective, reliable and\naccurate emotion recognition combined with raising machine learning methods.\nSupervised learning methods have dominated the attention of the research\ncommunity, but the challenge in collecting needed labels makes emotion\nrecognition difficult in large-scale semi- or uncontrolled experiments.\nUnsupervised methods are increasingly being explored, however sub-optimal\nsignal feature selection and label identification challenges unsupervised\nmethods' accuracy and applicability. This article proposes an unsupervised deep\ncluster framework for emotion recognition from physiological and psychological\ndata. Tests on the open benchmark data set WESAD show that deep k-means and\ndeep c-means distinguish the four quadrants of Russell's circumplex model of\naffect with an overall accuracy of 87%. Seeding the clusters with the subject's\nsubjective assessments helps to circumvent the need for labels.",
  "text": "1\nDeep-seeded Clustering for Unsupervised\nValence-Arousal Emotion Recognition from\nPhysiological Signals\nAntoine Dubois1, Carlos Lima Azevedo*,2, Sonja Haustein3, and Bruno Miranda4\n1,2,3Technical University of Denmark, Department of Technology, Management and Economics, 2800 Kgs.\nLyngby, Denmark, E-mail: 1dubois@dtu.dk, 2climaz@dtu.dk (*corresponding author), 3sonh@dtu.dk\n4Physiology Institute - Lisbon School of Medicine, University of Lisbon, Lisboa, Portugal,\nE-mail:bruno.miranda@campus.ul.pt\nAbstract—Emotions play a significant role in the cognitive\nprocesses of the human brain, such as decision making, learning\nand perception. The use of physiological signals has shown to\nlead to more objective, reliable and accurate emotion recognition\ncombined with raising machine learning methods. Supervised\nlearning methods have dominated the attention of the research\ncommunity, but the challenge in collecting needed labels makes\nemotion recognition difficult in large-scale semi- or uncontrolled\nexperiments. Unsupervised methods are increasingly being ex-\nplored, however sub-optimal signal feature selection and la-\nbel identification challenges unsupervised methods’ accuracy\nand applicability. This article proposes an unsupervised deep\ncluster framework for emotion recognition from physiological\nand psychological data. Tests on the open benchmark data set\nWESAD show that deep k-means and deep c-means distinguish\nthe four quadrants of Russell’s circumplex model of affect with an\noverall accuracy of 87%. Seeding the clusters with the subject’s\nsubjective assessments helps to circumvent the need for labels.\nI. INTRODUCTION\nUnderstanding human emotions plays a vital role in under-\nstanding how humans behave [1]. Physiological signals have\nproven to be essential tools for investigating such emotions,\nrevealing the intricate connections between psychological\nstates and bodily responses: electroencephalography (EEG)\nunveils brainwave patterns linked to emotions; functional mag-\nnetic resonance imaging (fMRI) provide further insights into\nemotional processing in brain regions; electrodermal activity\n(EDA) measures sweat gland activity; electrocardiography\n(ECG) captures heart-related changes; and blood volume pulse\n(BVP) indicates changes in blood flow and cardiovascular ac-\ntivity related to emotional responses. These signals collectively\ndeepen our understanding of emotions, bridging neuroscience,\npsychology and body physiology [2].\nIn recent years, machine learning has improved automatic\nemotion recognition through physiological signals [3]. In par-\nticular, supervised learning models have identified emotions\nwith satisfactory performance [4], [5], [6], [7].\nSupervised models use pairs of a list of explanatory vari-\nables and a label to learn to recognise such labels. Affective\ncomputing researchers have used a still increasing range of\nphysiological measurements as explanatory variables such as\nface expression [8], voice pitch [9], EDA [10] or heartbeat\n[11]. As labels, researchers have used emotional stimuli [12],\n[11], subjective emotional assessments [13], [14] or other\nphysiological signals strongly associated with selected emo-\ntions [15]. However, physiological signals cannot entirely mea-\nsure an emotion due to its unobservable nature. Stimuli may\nfall short in isolating emotions, and psychological assessments\nare exposed to various biases [16], [17]. Despite these limita-\ntions, the last fifteen years have witnessed significant growth\nand accumulation of evidence on the potential of such methods\nand the reader is referred to [3], [2] for comprehensive reviews.\nSupervised models require sizeable labelled training sets\n[18]. Yet, collecting large labelled data sets for many partici-\npants is infeasible, especially in daily-living unconstrained sce-\nnarios [3], [2]. In addition, larger training set sizes and larger\nnumber of features can make complex emotional recognition\nharder [19]. To tackle this, dimensional reduction through\nauto-encoders has shown to reduce the need for labelled\nobservations [20], [21]. Yet, as accurate as these supervised\nmodels are, they are inoperative without labelled data.\nIn recent years, unsupervised learning has emerged as a\npromising alternative to the lack of reliable labelled data\nin emotion recognition [22], [23], [24], [25], [26]. So far,\nresearchers have most frequently used well-established algo-\nrithms such as k-means, Gaussian Mixture Models (GMM),\nand Hidden Markov models for binary classification [27], [28],\n[29]. Discerning stress received major attention [30], [31],\n[32], [33]. Furthermore, [34], [35], [36] showed recently that\nunsupervised learning can distinguish valence from arousal.\nThough promising, unsupervised learning suffers from three\nobstacles to their prominent use for emotion recognition.\nFirst, unsupervised models use no label, so it can be\nchallenging to directly identify which exact emotional states\nare being predicted. For example, when using clustering on\nphysiological data, one cannot say whether an identified cluster\ncontains stress or any other response without additional infor-\nmation. In other words, a cluster is not a category. Nonetheless,\nclusters can receive a pseudo-label by carefully initialising or\nseeding, the algorithm [37].\nSecond, the accuracy of unsupervised models remains vastly\narXiv:2308.09013v1  [cs.LG]  17 Aug 2023\n2\ninferior to supervised models for recognising emotions. Usu-\nally, researchers manually extract statistical features from\nphysiological signals before the classification exercise [38],\n[39]. However, no known feature, nor series of features,\nmay completely capture the useful information for obtaining\nrelevant clusters, thus motivating the use of deep cluster\nalgorithms that extract task-specific features and clusters. Mar-\ntinez et al. [20] and Aqajari et al. [21]’s deep auto-encoders\nautomatically extracted features in reduced dimensions from\nphysiological signals and extracted emotions using logistic\nclassifiers and k-nearest neighbours algorithms (k-NN). Such\nclassifiers trained on automatically extracted features showed\nto outperform those trained on manually selected statistical\nfeatures. Instead of deep auto-encoders, Yang et al. [40]\nand Yin et al. [41] stacked several shallow auto-encoders\nbefore using the emotion classifier. Furthermore, their feature\nextraction and classifiers were trained jointly as opposed to\n[21], [20].\nThird, while popular clustering algorithms such as k-means,\nDBSCAN, Affinity propagation, and BIRCH assign data points\nto mutually exclusive clusters, the boundary between emo-\ntions remains vague [42]. Consequently, clustering algorithms\nshould assign degrees of cluster membership to physiological\nresponses, as in GMM. The responses should receive degrees\nto belong to emotional states so that the intensity of responses\nis comparable.\nOn the one hand, supervised models trained on features\nextracted by a auto-encoder accurately recognise the emotions\nfor which they were trained, but they are dependent on the\nabundance of labelled data. On the other hand, unsupervised\nmodels do not require training labels, but have so far been\ntrained on manually extracted features.\nThe present article proposes deep seeded cluster algorithms\nthat overcome the above limitations of supervised and unsuper-\nvised learning for emotion recognition. In our proposed model,\na sequence-to-sequence auto-encoder [43] extracts features\nfrom physiological signals, and is trained jointly with a deep\nclustering algorithm for arousal and valence recognition. Seed-\ning the clusters with subjective assessment identifies the four\nquadrants of Russell’s circumplex model of affect. We test our\nproposed framework on EDA, BVP and skin temperature (ST)\nsignals recorded by a commercial wristband using both deep-\nseeded c-means and k-means clustering methods and compare\nit against seeded k- and c-means, with and without constrained\npre-training. The remainder of this article is organised as\nfollows: Section II describes the deep seeded k-means and\nc-means for signals. Section III demonstrates the efficiency of\nthe method on wearable sensors. Finally, Section V concludes.\nII. DEEP SEEDED CLUSTERING FOR SIGNALS\nDeep cluster algorithms emerged within the computer vi-\nsion community [44] to recognise unlabelled images. Their\nhigh performance comes from their ability to extract features\nspecific to the clustering task [45]. As boundaries between\ncategories are often fuzzy, Zhang et al. [46] proposed an\nextension with fuzzy clusters. We extend Caron’s deep seeded\nk-means and Zhang’s deep seeded c-means to cluster the\ndifferent states of a multidimensional signal.\nFigure 1 depicts the deep seeded k-means and c-means\nfor signals. First, the signal is pre-processed and sliced in\noverlapping windows of a fixed size δ. As window size δ is\nfixed throughout the article, the piece of signal xt:t+δ from t to\nt+δ is simply denoted xt for conciseness, for all t ≥0. Then,\na recurrent auto-encoder automatically extracts signal features.\nSimultaneously, the features are clustered around centroids\nseeded with subjective assessments. The following sections\npresent these components in detail.\nA. K-means and c-means\nClustering algorithms arrange data points by similarity. The\nwell-established k-means algorithm splits data points into K\nsimilarity groups called clusters.\nPoint xt is assigned to cluster Ck, with k ∈{1, . . . , K} and\nt ∈{0, . . . , T}, if xt is closer to the average characteristics\nof the points in cluster Ck. The average of a cluster is called\na centroid and denoted\nµk :=\n1\n|Ck|\nX\nt∈Ck\nxt\n(1)\nIf xt belongs to Ck, assignment vector st ∈{0, 1}K satisfies\nstk =\n(\n1,\nif k = argminK\nl=1||xt −µk||\n0,\notherwise\n(2)\nOne can then obtain the optimum clusters by successively\nupdating the assignment vectors and the centroids minimising\nthe k-means’ loss function\nLkm =\nT\nX\nt=0\nK\nX\nk=1\nstk||xt −µk||2\n(3)\nC-means generalises k-means by assigning data points to\nfuzzy clusters instead of mutually exclusive clusters. Accord-\ning to Zhang et al. [46], observation xt belongs to cluster k\nwith degree\nutk =\nexp\n\u0010\n−2\nγ\n||xt−µk||2\n1+||xt−µk||2\n\u0011\nPK\nl=1 exp\n\u0010\n−2\nγ\n||xt−µl||2\n1+||xt−µl||2\n\u0011\n(4)\nwith γ > 0. The higher is uik, the more xt belongs to cluster\nCk. Moreover, µk centroid of fuzzy cluster Ck is defined by\nµk =\nPT\nt=0 dtkutkxt\nPT\ns=0 dskusk\n(5)\nwhere\ndtk =\n||xt −µk|| + 2\n(||xt −µk|| + 1)2\n(6)\nC-means’ estimation steps are the same as those of k-means.\nSuccessively updating the assignment vectors from eq. 4 and\nthe centroids from eq. 5 and minimising the c-means’ loss\nLcm =\nT\nX\nt=0\nK\nX\nk=1\nutk||xt −µk||2\n(7)\n3\nRaw\nSignals\nDenoising\nResampling\nSlicing\nxt\nNeural Network\nzt\nNeural Network\nˆxt+1\nxt+1\nClustering Loss\n+\nReconstruction Loss\nSeeds\nEncoder\nDecoder\nPre-processing\nFig. 1.\nDiagram of the deep-seeded cluster algorithms based on k-means or c-means for signals. The definition of qtk and µk differentiate deep-seeded\nk-means from deep-seeded c-means.\nK-means and c-means proved their utility in countless do-\nmains. Nonetheless, if the data points do not contain informa-\ntive features, these methods fail to form meaningful clusters\n[47]. In our proposed method, we jointly estimate the c-\nmeans (and k-means for comparison) and an auto-encoder, to\ncategorise the discovered embedded features from the original\nsignal.\nB. Sequence-to-Sequence auto-encoder\nAn autoencoder is an artificial neural network (ANN) that\nlearns an efficient latent space, typically in a lower dimen-\nsional space, that is validated and refined by regenerating the\noriginal input. First, a function φ, called an encoder, maps the\nobservations (xi)n\ni=1 into a latent feature space. Second, the\nfunction ψ, called a decoder, maps back the latent vectors into\nthe observation space. Latent features zi := φ(xi) correctly\nencode observation xi if the decoded point ˆxi := ψ(zi) is\nclose enough to xi, i.e. ||xi −ˆxi|| < ϵ for some ϵ > 0.\nIn the state-of-the-art, φ and ψ are ANNs. The optimal\nweights of φ and ψ, respectively wφ and wψ, often minimise\nthe reconstruction loss\nLr =\nN\nX\ni=1\n||xi −ˆxi||2\nIn our case, we intend to extract features from the signal\nwindows of size δ, (xt)T\nt=0. In consequence, we use the\nsequence-to-sequence auto-encoder (S2S) [43] that is specifi-\ncally designed for signal feature extraction.\nContrary to the standard auto-encoders, S2S’s decoder ψ\nforecasts the next signal window [43]. In consequence, we say\nthat the latent vector zt = φ(xt) correctly encodes observation\nxt if ˆxt+1 = ψ(zt) is close enough to xt+1, i.e. ||xt+1 −\nˆxt+1|| < ϵ for some ϵ. Here, the optimal weights of φ and ψ,\nrespectively wφ and wψ, minimise the reconstruction loss\nLr =\nT −δ−1\nX\nt=0\n||xt+1 −ˆxt+1||2\n(8)\nThe optimal latent representation zt of data point xt is not\nunique. Indeed, if φ and ψ are optimal such that Lr is minimal,\nthen, for any bijection f, f ◦φ and ψ ◦f −1 are also optimal.\nConsequently, finding encoders and decoders minimising the\nreconstruction loss is insufficient. The latent representations\nneed to be specific to clustering. Deep cluster algorithms\njointly minimise reconstruction and clustering losses to obtain\na latent representation that is suitable for the clustering at\nstake.\nC. Deep-Clustering\nSignal windows of size δ, (xt)T\nt=1, admit infinitely many\nlatent representations. Nonetheless, deep-cluster algorithms\nbound the informative representations to form consistent clus-\nters.\nSeveral measures evaluate the consistency of clusters. We\nsay that clusters are optimal if the sum of distances between\ntheir elements and centroids (loss 3 or 7) is minimal over the\npossible latent representations. Conversely, specific features to\nthe k-means or c-means problem minimise loss 3 or 7.\nHowever, ztk = µk, for all t and k is a trivial solution for\nminimising k-means’ and c-means’ loss. Informative features\navoid this pitfall. In consequence, we say that clusters are\nconsistent if their clustering loss (3 or 7) is minimal over the\ninformative latent representations. Thus, the consistent features\nminimise the reconstruction loss 8 and clustering loss 3 or 7.\nEquivalently, the feature vectors are outputs of the encoder\nφ i.e ztk = φ(xt), and the predicted next signal windows are\nthe output of the decoder i.e. ˆxt+1 = ψ(ztk), the feature vec-\ntors are informative and specific when the consistent weights\nof φ and ψ minimise the total loss\nLtotal = Lr + Lc\n(9)\nwith Lc is k-means’ or c-means’ loss.\nK-means’ loss 3 and c-means’ loss 7 depend on the assign-\nments stk or memberships utk, and the centroids µk, which\ndepend itself on φ. Each deep-cluster algorithm’s iteration has\ntwo steps [45]. First, gradient descent actualises the weights\nof ψ and φ to minimise Ltotal. Second, Equation 2 updates\nthe assignment vectors for k-means, or Equation 4 updates the\nmemberships vectors for c-means. Third, Equation 1 updates\n4\nthe centroids µk for k-means or Equation 5 for c-means. Deep\ncluster algorithms repeat these three steps until convergence.\nDemonstrating deep cluster convergence is straightforward.\nFirst, each gradient descent’s iteration i updates the encoder\nand decoder from φi and ψi to φi+1 and ψi+1 such that\nLi+1\nr\n+\nT\nX\nt=0\nK\nX\nk=1\nqi\ntk||φi+1(xt) −µi\nk||2 ≤Li\nr\n+\nT\nX\nt=0\nK\nX\nk=1\nqi\ntk||φi(xt) −µi\nk||2\nwhere qi\nt is k-means’ assignment or c-means’ membership\nvector of observation t at iteration i. Second, k-means [48]\nor c-means [49] converge such that\nT\nX\nt=0\nK\nX\nk=1\nqi+1\ntk ||φi+1(xt)−µi+1\nk\n||2 ≤\nT\nX\nt=0\nK\nX\nk=1\nqi\ntk||φi+1(xt)−µi\nk||2\nIn consequence, the deep cluster algorithm converges such that\nLi+1\ntotal ≤Li\ntotal.\nTo summarise, deep cluster algorithms form consistent\nclusters by extracting informative and specific features from\nobservations. Nonetheless, the clusters’ contents remain un-\nknown.\nD. Seeding\nThough consistent, the clusters formed by the deep cluster\nalgorithms have no label. If a deep cluster algorithm assigns\nthe window of size δ of physiological signal xt to cluster k,\nno one can say what emotion the individual felt from t to t+δ.\nHowever, carefully initialising the algorithm can reveal what\nthe clusters represent. This method is called seeding [37] [50].\nWe propose a simple seeding. If individuals assess that\nemotion k∗was dominant from T1 to T2, physiological\nsignal windows (xt)T2\nt=T1 are likely to have recorded this\nemotion’s physiological response. For k-means clustering, we\ninitialise stk∗= 1, stk = 0 for all t ∈{T1, . . . , T2} and\nk ̸= k∗. However, we fix stk = 0 for all t /∈{T1, . . . , T2}\nand k ∈{1, . . . , K}. For c-means clustering, we initialise\nuk∗1 = 1, utk = 0 for all t ∈{T1, . . . , T2} and k ̸= k∗.\nThereby, emotion k∗defines the pseudo-label of cluster k∗.\nContrariwise, observations outside {T1, . . . , T2} do not belong\na priori more to one cluster than another. Thus, we initialise\nutk =\n1\nK for all t /∈{T1, . . . , T2} and k ∈{1, . . . , K}. This\noperation is repeated for all the dominant emotions reported\nby the individuals.\nOf course, emotion k∗may have been more intense around\nT1 than T2, or arisen outside this period. However, each\niteration of the algorithm adjusts stk∗or utk∗for all t inside\nand outside [T1, T2].\nFurthermore, the more emotion k∗is manifest, the higher\nc-means’ cluster membership utk∗is. Consequently, cluster\nmembership utk∗is interpreted as the intensity of emotion k∗\nat time t.\nE. Pre-training\nRecent parameter initialisation methods fasten network\nlearning, and performance [51]. Pre-training is a type of\ninitialisation method that brings the network’s parameters near\ntheir optimal value [52].\nTo improve emotion recognition performance, the S2S auto-\nencoder described in Subsection II-B can be trained to min-\nimise the reconstruction loss 8 before performing deep seeded\nk-means and c-means.\nThe latent representation resulting from the reconstruction\nloss minimisation is informative but risks being non-specific.\nTo bound the representation to be both informative and\nspecific, the S2S auto-encoder is trained to minimise the\ntotal loss 9. The centroids are updated, but assignments or\nmemberships initialised as described in Subsection II-D are\nnot updated. We call this initialisation method constrained pre-\ntraining.\nIII. EXPERIMENTAL EVALUATION\nIn this section, we test the algorithms introduced in Sec-\ntion II seeded k-means and seeded c-means, deep-seeded k-\nmeans and deep-seeded c-means to emotion recognition from\nEDA, BVP and skin temperature (ST) signals. The different\narchitectures were implemented in an open-source Python\ncode and compared using the WESAD benchmark data set. We\nalso test if constrained pre-training improves the performance.\nA. Data set\nThe data set Wearable Stress and Affect Detection (WE-\nSAD) [12] has become a benchmark for testing emotion\nrecognition methods. It includes data from a study to elicit\nthree different affective states (neutral, stress, amusement) to\n15 subjects with a mean age of 27.5 ± 2.4 years, where 12\nsubjects were male and the other three subjects were female.\nThe subjects were also followed a guided meditation to de-\nexcite them after the stress and amusement conditions.\nDuring the experiment, the volunteers wear the wristband\nEmpatica E41, measuring their EDA, BVP and ST. After each\nphase, the participants assess their valance and arousal levels\nthrough the Self-Assessment Manikin (SAM) questionnaire\n[53]. We slice the EDA, BVP and ST signals in overlapping\nwindows of range. Then, the deep-seeded cluster for signals\nalgorithm extract and cluster features from each window.\nThe clusters are beforehand seeded with the answers to the\nSAM questionnaire. Comparing the pseudo-label assigned to\nthe signal windows to experimental phase when they were\nrecorded gives a measure of the accuracy of our method.\nB. Signal pre-processing\nFeature extraction from raw EDA, BVP and ST signals is\nmore efficient after pre-processing. In our proposed method\nwe first apply a Savitzky-Golay filter to mitigate errors and\nremove outliers on the EDA and ST signals.\nThe EDA and ST signals were sampled at 4hz, while the\nBVP signal is sampled at 64hz, thus a zero-phase low-pass\n1https://www.empatica.com/en-eu/research/e4/\n5\nPhase\nValence\nArousal\nPresumed emotion\nA\n6.7\n2.5\nNeutral\nB\n7.5\n3\nAmusement\nC\n6.5\n2.3\nRelaxation\nD\n4.5\n6.8\nStress\nTABLE I\nAVERAGE VALENCE AND AROUSAL SCORE, AND PRESUMED EMOTIONS\nFOR EACH PHASE\nfilter up-sampled the EDA and ST signals to the length of\nthe BVP signal. Up-sampling the shorter signals instead of\ndown-sampling the longest signal avoids information loss.\nThirdly, min-max-normalisation was applied to the three\nsignals for speeding up the automatic feature extraction and\nimproving the clustering [54].\nFinally, the signals were sliced in overlapping windows of\nrange 20 and step 1.\nC. Practical implementation\nAfter testing different configurations, we opted for the\narchitecture in Figure 1. The encoder and decoder formed by\ntwo gated recurrent units (GRU) layers efficiently reconstruct\nthe EDA, BVP and ST signals while minimising the number\nof parameters. In addition, the three-neuron dense layer at the\ndecoder’s end helps reconstruct the original signals without\nsubstantially increasing the overall network complexity. More-\nover, the encoder and decoder’s recurrent layers are followed\nby normalisation layers. Standardising layers’ outputs, i.e.\n(x −µ)/σ, reduces the training time [55] and improves\nclustering [54] of the latent vectors. Furthermore, the encoder\nextracts 30 features. γ is fixed to 1/2 and the learning rate to\n5 × 10−5.\nBefore training, the deep k-means’ and deep c-means’ auto-\nencoders are trained during a single epoch. The constrained\npre-training lasts one or two epochs. The training follows pre-\ntraining with the total loss 9 for two epochs.\nD. Seeding\nAs described in Subsection II-D, we initialise the member-\nship function with subjective data.\nTable I shows the average arousal and valence scores mea-\nsured through SAM questionnaires. The average valence and\narousal are characteristic of stress, amusement and relaxation\n[42].\nFirst, phase C aroused relaxation as it has the lowest arousal.\nSecond, phase B aroused amusement as it has high arousal and\nvalence. Third, phase D aroused stress as it has high arousal\nbut low valence. In contrast, phase A corresponds to a neutral\nemotion.\nE. Results\nContrary to k-means, c-means and deep c-means return\ndegrees of cluster memberships. To measure the performance\nof this algorithm, we assign pseudo-label ˆyt to signal window\nxt if ˆyt is the cluster with the largest membership, i.e. ˆyt =\nargmaxK\nk=1utk. Then, the F1-score measures the accuracy\nof the pseudo labels against the WESAD stimuli. Comparing\nmodels with the F1-score is necessary because the durations\nof the stimuli are imbalanced. Table II displays the average\nF1-scores and overall accuracy across WESAD subjects.\nTable II shows that c-means has higher average F1-scores\nfor all the emotions but amusement than k-means. However,\nboth k-means and c-means display high discrepancy between\nthe benchmark dataset’s participants.\nBoth regard F1-score averages and standard deviation, deep\ncluster algorithms dominate k-means and c-means.\nNonetheless, the effect of constrained pre-training (CPT)\nis slight. CPT does not improve deep k-means for neutral\nemotion and stress recognition. Nonetheless, CPT deep k-\nmeans increases by 18% and 5% the recognition of amusement\nand relaxation while dividing the F1-score standard derivation\nby 3. Nonetheless, CPT for two epochs does not increase deep\nk-means performance.\nFurthermore, CPT hinders deep c-means from recognising\nneutral emotion and stress. However, it increased by 34% and\n14% amusement and relaxation recognition while dividing by\n8 the F1 score standard deviation. Thus, CPT improves deep\nk-means and c-means amusement and relaxation recognition.\nNonetheless, CPT for two epochs does not increase deep c-\nmeans performance.\nFinally, CPT deep k-means and c-means have comparable\noverall accuracy. However, CPT deep k-means slightly better\nidentifies stress, amusement and relaxation.\nTo conclude, deep k-means and c-means drastically improve\nemotion recognition compared to k-means and c-means. More-\nover, CPT increases deep k-means and c-means amusement\nand relaxation recognition. Nonetheless, understanding why\nk-means and c-means-based algorithms remain to be explored.\nIV. CONCLUSION\nThis article has introduced deep seeded k-means and c-\nmeans for physiological signals and a method to pre-train\ntheir auto-encoders. These algorithms automatically extract\nfeatures from physiological signals to form consistent clusters.\nMoreover, seeding the clusters with subjective assessments\nreveals the emotion they contain. Thanks to identification,\ncluster memberships formed by c-means-based algorithms\nrepresent the intensity of emotions.\nBenchmark tests reveal that deep seeded k-means and c-\nmeans recognise the four quadrants of the circumplex model of\naffect. Moreover, pre-training increases the distinction between\npositive and negative valence and reduces the discrepancy\nbetween benchmark’s individuals.\nNonetheless, a new auto-encoder architecture could improve\nthe performance of deep c-means and deep k-means. Finally,\nfuture work could use the deep seeded k-means or c-means\nalgorithm to reveal the effect of daily activities on mood.\nV. ACKNOLEDGEMENTS\nThis work was supported by the European Union’s Horizon\n2020 Research and Innovation Programme under the grant\nagreement No 945307 (eMOTIONAL Cities).\n6\nneutral\nstress\namusement\nrelaxation\naccuracy\nrandom\n30.52 (0)\n23.49 (0)\n16.58 (0)\n25.61 (0)\n25.00 (0)\nk-means\n49.22 (37)\n79.52 (32)\n39.02 (20)\n54.64 (23)\n56.60 (15)\ndeep k-means\n84.23 (9)\n94.18 (3)\n71.40 (16)\n82.22 (17)\n84.32 (7)\nCPT deep k-means\n85.60 (5)\n93.08 (3)\n84.33 (8)\n86.23 (6)\n87.20 (4)\n2-CPT deep k-means\n86.72 (4)\n92.69 (3)\n83.49 (9)\n87.00 (4)\n87.73 (4)\nc-means\n77.05 (18)\n82.64 (28)\n26.38 (27)\n64.97 (14)\n69.45 (12)\ndeep c-means\n97.19 (3)\n98.76 (1)\n61.90 (37)\n70.20 (40)\n88.44 (10)\nCPT deep c-means\n91.49 (3)\n91.70 (3)\n83.23 (5)\n80.36 (5)\n87.42 (3)\n2-CPT deep c-means\n91.32 (3)\n92.18 (3)\n83.49 (4)\n81.71 (5)\n87.86 (2)\nTABLE II\nF1-SCORE IN PERCENTAGE (STANDARD DEVIATION), CPT STANDS FOR CONSTRAINED PRE-TRAINING, 2-CPT INDICATES THAT CPT LASTED 2 EPOCHS\n(HIGHEST F1 SCORES PRINTED IN BOLD)\nREFERENCES\n[1] R. W. Picard, Affective computing. MIT press, 2000.\n[2] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang,\n“A review of emotion recognition using physiological signals,” Sensors,\nvol. 18, no. 7, p. 2074, 2018.\n[3] P. J. Bota, C. Wang, A. L. Fred, and H. P. Da Silva, “A review,\ncurrent challenges, and future possibilities on emotion recognition using\nmachine learning and physiological signals,” IEEE Access, vol. 7,\npp. 140990–141020, 2019.\n[4] S. Gedam and S. Paul, “Automatic stress detection using wearable\nsensors and machine learning: A review,” in 2020 11th International\nConference on Computing, Communication and Networking Technolo-\ngies (ICCCNT), pp. 1–7, IEEE, 2020.\n[5] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition using\nmulti-modal data and machine learning techniques: A tutorial and\nreview,” Information Fusion, vol. 59, pp. 103–126, 2020.\n[6] M. A. Hasnul, N. A. A. Aziz, S. Alelyani, M. Mohana, and A. A.\nAziz, “Electrocardiogram-based emotion recognition systems and their\napplications in healthcare—a review,” Sensors, vol. 21, no. 15, p. 5015,\n2021.\n[7] Y. Zhang, J. Wang, Y. Chen, H. Yu, and T. Qin, “Adaptive memory\nnetworks with self-supervised learning for unsupervised anomaly detec-\ntion,” IEEE Transactions on Knowledge and Data Engineering, 2022.\n[8] S. Li and W. Deng, “Deep facial expression recognition: A survey,” IEEE\ntransactions on affective computing, vol. 13, no. 3, pp. 1195–1215, 2020.\n[9] P. Shen, Z. Changjun, and X. Chen, “Automatic speech emotion recogni-\ntion using support vector machine,” in Proceedings of 2011 international\nconference on electronic & mechanical engineering and information\ntechnology, vol. 2, pp. 621–625, IEEE, 2011.\n[10] A. Anusha, P. Sukumaran, V. Sarveswaran, A. Shyam, T. J. Akl,\nS. Preejith, M. Sivaprakasam, et al., “Electrodermal activity based\npre-surgery stress detection using a wrist wearable,” IEEE journal of\nbiomedical and health informatics, vol. 24, no. 1, pp. 92–100, 2019.\n[11] T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using\ndynamical graph convolutional neural networks,” IEEE Transactions on\nAffective Computing, vol. 11, no. 3, pp. 532–541, 2018.\n[12] P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, and K. Van Laer-\nhoven, “Introducing wesad, a multimodal dataset for wearable stress\nand affect detection,” in Proceedings of the 20th ACM international\nconference on multimodal interaction, pp. 400–408, 2018.\n[13] R. W. Picard, E. Vyzas, and J. Healey, “Toward machine emotional in-\ntelligence: Analysis of affective physiological state,” IEEE transactions\non pattern analysis and machine intelligence, vol. 23, no. 10, pp. 1175–\n1191, 2001.\n[14] S. Katada, S. Okada, Y. Hirano, and K. Komatani, “Is she truly enjoying\nthe conversation? analysis of physiological signals toward adaptive\ndialogue systems,” in Proceedings of the 2020 International Conference\non Multimodal Interaction, pp. 315–323, 2020.\n[15] A. Jimenez-Molina, C. Retamal, and H. Lira, “Using psychophysiolog-\nical sensors to assess mental workload during web browsing,” Sensors,\nvol. 18, no. 2, p. 458, 2018.\n[16] E. Diener, R. E. Lucas, and C. N. Scollon, “Beyond the hedonic\ntreadmill: Revising the adaptation theory of well-being,” in The science\nof well-being, pp. 103–118, Springer, 2009.\n[17] C. R. Reynolds and L. A. Suzuki, “Bias in psychological assessment:\nAn empirical review and recommendations,” Handbook of Psychology,\nSecond Edition, vol. 10, 2012.\n[18] S. J. Raudys, A. K. Jain, et al., “Small sample size effects in statistical\npattern recognition: Recommendations for practitioners,” IEEE Trans-\nactions on pattern analysis and machine intelligence, vol. 13, no. 3,\npp. 252–264, 1991.\n[19] X. Zhu, C. Vondrick, C. C. Fowlkes, and D. Ramanan, “Do we need\nmore training data?,” International Journal of Computer Vision, vol. 119,\nno. 1, pp. 76–92, 2016.\n[20] H. P. Martinez, Y. Bengio, and G. N. Yannakakis, “Learning deep phys-\niological models of affect,” IEEE Computational intelligence magazine,\nvol. 8, no. 2, pp. 20–33, 2013.\n[21] S. A. H. Aqajari, E. K. Naeini, M. A. Mehrabadi, S. Labbaf, N. Dutt,\nand A. M. Rahmani, “pyeda: An open-source python toolkit for pre-\nprocessing and feature extraction of electrodermal activity,” Procedia\nComputer Science, vol. 184, pp. 99–106, 2021.\n[22] M. Neumann and N. T. Vu, “Improving speech emotion recognition with\nunsupervised representation learning on unlabeled speech,” in ICASSP\n2019-2019 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 7390–7394, IEEE, 2019.\n[23] J. O. Huelle, B. Sack, K. Broer, I. Komlewa, and S. Anders, “Unsuper-\nvised learning of facial emotion decoding skills,” Frontiers in human\nneuroscience, vol. 8, p. 77, 2014.\n[24] Z. Lian, J. Tao, B. Liu, and J. Huang, “Unsupervised representation\nlearning with future observation prediction for speech emotion recogni-\ntion,” arXiv preprint arXiv:1910.13806, 2019.\n[25] S. E. Eskimez, Z. Duan, and W. Heinzelman, “Unsupervised learning\napproach to feature analysis for automatic speech emotion recognition,”\nin 2018 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 5099–5103, IEEE, 2018.\n[26] J. Tervonen, S. Puttonen, M. J. Sillanp¨a¨a, L. Hopsu, Z. Homorodi,\nJ. Ker¨anen, J. Pajukanta, A. Tolonen, A. L¨ams¨a, and J. M¨antyj¨arvi,\n“Personalized mental stress detection with self-organizing map: From\nlaboratory to the field,” Computers in Biology and Medicine, vol. 124,\np. 103935, 2020.\n[27] M. Kang, S. Shin, J. Jung, and Y. Kim, “Stress classification using\nk-means clustering and heart rate variability from electrocardiogram,”\nInternational Journal of Biology and Biomedical Engineering, vol. 14,\npp. 251–254, 01 2021.\n[28] X. Zhuang, V. Rozgi´c, and M. Crystal, “Compact unsupervised eeg\nresponse representation for emotion recognition,” in IEEE-EMBS in-\nternational conference on Biomedical and Health Informatics (BHI),\npp. 736–739, IEEE, 2014.\n[29] E. Vildjiounaite, J. Kallio, J. M¨antyj¨arvi, V. Kyll¨onen, M. Lindholm,\nand G. Gimel’farb, “Unsupervised stress detection algorithm and exper-\niments with real life data,” in EPIA Conference on Artificial Intelligence,\npp. 95–107, Springer, 2017.\n[30] D. Huysmans, E. Smets, W. De Raedt, C. Van Hoof, K. Bogaerts,\nI. Van Diest, and D. Helic, “Unsupervised learning for mental stress\ndetection-exploration of self-organizing maps,” Proc. of Biosignals 2018,\nvol. 4, pp. 26–35, 2018.\n[31] C. Maaoui and A. Pruski, “Unsupervised stress detection from remote\nphysiological signal,” in 2018 IEEE International Conference on Indus-\ntrial Technology (ICIT), pp. 1538–1543, IEEE, 2018.\n7\n[32] Y. Wu, M. Daoudi, A. Amad, L. Sparrow, and F. D’Hondt, “Unsuper-\nvised learning method for exploring students’ mental stress in medical\nsimulation training,” in Companion Publication of the 2020 International\nConference on Multimodal Interaction, pp. 165–170, 2020.\n[33] K. Lai, S. N. Yanushkevich, and V. P. Shmerko, “Intelligent stress mon-\nitoring assistant for first responders,” IEEE Access, vol. 9, pp. 25314–\n25329, 2021.\n[34] S. Deldari, D. V. Smith, A. Sadri, and F. Salim, “Espresso: Entropy\nand shape aware time-series segmentation for processing heterogeneous\nsensor data,” Proceedings of the ACM on Interactive, Mobile, Wearable\nand Ubiquitous Technologies, vol. 4, no. 3, pp. 1–24, 2020.\n[35] A. Kumar, K. Sharma, and A. Sharma, “Genetically optimized fuzzy\nc-means data clustering of iomt-based biomarkers for fast affective\nstate recognition in intelligent edge analytics,” Applied Soft Computing,\nvol. 109, p. 107525, 2021.\n[36] Z. Liang, S. Oba, and S. Ishii, “An unsupervised eeg decoding system for\nhuman emotion recognition,” Neural Networks, vol. 116, pp. 257–268,\n2019.\n[37] S. Basu, A. Banerjee, and R. Mooney, “Semi-supervised clustering by\nseeding,” in In Proceedings of 19th International Conference on Machine\nLearning (ICML-2002, Citeseer, 2002.\n[38] S. Jerritta, M. Murugappan, R. Nagarajan, and K. Wan, “Physiological\nsignals based human emotion recognition: a review,” in 2011 IEEE\n7th international colloquium on signal processing and its applications,\npp. 410–415, IEEE, 2011.\n[39] A. Saxena, A. Khanna, and D. Gupta, “Emotion recognition and detec-\ntion methods: A comprehensive survey,” Journal of Artificial Intelligence\nand Systems, vol. 2, no. 1, pp. 53–79, 2020.\n[40] B. Yang, X. Han, and J. Tang, “Three class emotions recognition based\non deep learning using staked autoencoder,” in 2017 10th International\nCongress on Image and Signal Processing, BioMedical Engineering and\nInformatics (CISP-BMEI), pp. 1–5, IEEE, 2017.\n[41] Z. Yin, Y. Wang, W. Zhang, L. Liu, J. Zhang, F. Han, and W. Jin,\n“Physiological feature based emotion recognition via an ensemble deep\nautoencoder with parsimonious structure,” IFAC-PapersOnLine, vol. 50,\nno. 1, pp. 6940–6945, 2017.\n[42] J. A. Russell, “A circumplex model of affect.,” Journal of personality\nand social psychology, vol. 39, no. 6, p. 1161, 1980.\n[43] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\nwith neural networks,” Advances in neural information processing\nsystems, vol. 27, 2014.\n[44] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised learning of\ndeep representations and image clusters,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 5147–5156,\n2016.\n[45] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, “Towards k-\nmeans-friendly spaces: Simultaneous deep learning and clustering,” in\ninternational conference on machine learning, pp. 3861–3870, PMLR,\n2017.\n[46] R. Zhang, X. Li, H. Zhang, and F. Nie, “Deep fuzzy k-means with\nadaptive loss and entropy regularization,” IEEE Transactions on Fuzzy\nSystems, vol. 28, no. 11, pp. 2814–2824, 2019.\n[47] M. Cap´o, A. P´erez, and J. A. Lozano, “A cheap feature selection\napproach for the k-means algorithm,” IEEE transactions on neural\nnetworks and learning systems, vol. 32, no. 5, pp. 2195–2208, 2020.\n[48] L. Bottou and Y. Bengio, “Convergence properties of the k-means\nalgorithms,” Advances in neural information processing systems, vol. 7,\n1994.\n[49] R. J. Hathaway and J. C. Bezdek, “Recent convergence results for the\nfuzzy c-means clustering algorithms,” Journal of Classification, vol. 5,\nno. 2, pp. 237–247, 1988.\n[50] E. Bair, “Semi-supervised clustering methods,” Wiley Interdisciplinary\nReviews: Computational Statistics, vol. 5, no. 5, pp. 349–361, 2013.\n[51] C. A. de Sousa, “An overview on weight initialization methods for\nfeedforward neural networks,” in 2016 International Joint Conference\non Neural Networks (IJCNN), pp. 52–59, IEEE, 2016.\n[52] M. V. Narkhede, P. P. Bartakke, and M. S. Sutaone, “A review on weight\ninitialization strategies for neural networks,” Artificial intelligence re-\nview, vol. 55, no. 1, pp. 291–322, 2022.\n[53] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,\nT. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis;\nusing physiological signals,” IEEE transactions on affective computing,\nvol. 3, no. 1, pp. 18–31, 2011.\n[54] K. A. Doherty, R. Adams, and N. Davey, “Unsupervised learning with\nnormalised data and non-euclidean norms,” Applied Soft Computing,\nvol. 7, no. 1, pp. 203–210, 2007.\n[55] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n",
  "categories": [
    "cs.LG",
    "eess.SP"
  ],
  "published": "2023-08-17",
  "updated": "2023-08-17"
}