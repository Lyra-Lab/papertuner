{
  "id": "http://arxiv.org/abs/2308.09734v1",
  "title": "A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments",
  "authors": [
    "Sherif Abdelfattah",
    "Kathryn Kasmarik",
    "Jiankun Hu"
  ],
  "abstract": "Multi-objective Markov decision processes are a special kind of\nmulti-objective optimization problem that involves sequential decision making\nwhile satisfying the Markov property of stochastic processes. Multi-objective\nreinforcement learning methods address this problem by fusing the reinforcement\nlearning paradigm with multi-objective optimization techniques. One major\ndrawback of these methods is the lack of adaptability to non-stationary\ndynamics in the environment. This is because they adopt optimization procedures\nthat assume stationarity to evolve a coverage set of policies that can solve\nthe problem. This paper introduces a developmental optimization approach that\ncan evolve the policy coverage set while exploring the preference space over\nthe defined objectives in an online manner. We propose a novel multi-objective\nreinforcement learning algorithm that can robustly evolve a convex coverage set\nof policies in an online manner in non-stationary environments. We compare the\nproposed algorithm with two state-of-the-art multi-objective reinforcement\nlearning algorithms in stationary and non-stationary environments. Results\nshowed that the proposed algorithm significantly outperforms the existing\nalgorithms in non-stationary environments while achieving comparable results in\nstationary environments.",
  "text": "A Robust Policy Bootstrapping Algorithm\nfor Multi-objective Reinforcement\nLearning in Non-stationary Environments\nJournal Title\nXX(X):1–17\n©The Author(s) 2016\nReprints and permission:\nsagepub.co.uk/journalsPermissions.nav\nDOI: 10.1177/ToBeAssigned\nwww.sagepub.com/\nSAGE\nSherif Abdelfattah, Kathryn Kasmarik, and Jiankun Hu\nAbstract\nMulti-objective Markov decision processes are a special kind of multi-objective optimization problem that involves\nsequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement\nlearning methods address this kind of problem by fusing the reinforcement learning paradigm with multi-objective\noptimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics\nin the environment. This is because they adopt optimization procedures that assume stationarity in order to evolve\na coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach\nthat can evolve the policy coverage set while exploring the preference space over the defined objectives in an online\nmanner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage\nset of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two\nstate-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results\nshowed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments\nwhile achieving comparable results in stationary environments.\nKeywords\nMultiobjective Optimization, Reinforcement Learning, Non-stationary, Environment, Dynamics, Policy Bootstrapping,\nMarkov Decision Processes\nIntroduction\nIn reinforcement learning (RL), an agent learns from\ninteractions with the environment guided by a reward signal.\nThe objective of the learning process is to find a mapping\nfrom the environment’s state space to the action space that\nmaximizes the expected reward return (Sutton and Barto\n1998). Over the last decade, RL research has made many\nbreakthroughs such as playing computer games with human-\nlevel performance (Mnih et al. 2015) or beating human experts\nin complicated games such as Go (Silver et al. 2016). This has\nbeen achieved by maximizing a single reward function (e.g.,\nthe score in games). However, many real-world sequential\ndecision making problems involve multiple objectives that\nmay be in conflict with each other. Consider a search and\nrescue scenario in which an unmanned ground vehicle (UGV)\naims at optimizing multiple objectives including minimizing\nthe exposure chance to risks in the environment (i.e., fire or\nflood), maximizing the rescue ratio of trapped victims, and\nminimizing the overall search and rescue time. Similarly, a\ndrone in a patrolling scenario trying to maximize battery\nusability, maximize the patrolled area, and maximize the\ndetection rate of danger/enemy objects. These problems\nexhibit conflicting objectives that cannot be simultaneously\noptimized without a compromise. In reinforcement learning\nand control literature, such problems are known as multi-\nobjective Markov decision processes (MOMDPs).\nConventional reinforcement learning methods cannot\ndirectly tackle the MOMDP problem, as the defined\nobjectives are assumed to be in conflict and can not\nbe simultaneously optimized by a single policy. Rather,\nmulti-objective reinforcement learning (MORL) extends the\nconventional RL methodology through maximizing a vector\nof rewards instead of a single reward. Primarily, this is\nachieved by one of two MORL approaches: the single policy\napproach; and the multiple policy approach (Roijers et al.\n2013).\nIn the single policy approach, it is assumed that an optimal\nuser’s preference can be identified before solving the problem.\nConsequently, the multiple objectives can be transformed\ninto a single objective using the supplied user’s preference.\nThe main limitations of this approach lie in the difficulty\nof satisfying its main assumption in many practical multi-\nobjective problems. First, it may be impossible to find an\noptimal user’s preference beforehand. Secondly, it is difficult\nto deal with changes in the user’s preference in real-time,\nas it is necessary to optimize a different policy after each\npreference change. Changes in the user’s preference can\narise if the learning agent needs to deal with different users\n(such as in computer games or a personal assistant) or due\nto dealing with changes in the environment’s setup (e.g.,\nopponent actions) or in the objective space (e.g., new priorities\nor new objectives).\nSchool of Engineering and Information Technology,\nUniversity of New South Wales, Canberra, ACT, Australia\nCorresponding author:\nSherif Abdelfattah, School of Engineering and Information Technology,\nUniversity of New South Wales, Canberra, ACT, Australia\nEmail: sherif.abdelfattah@student.adfa.edu.au\nPrepared using sagej.cls [Version: 2017/01/17 v1.20]\narXiv:2308.09734v1  [cs.LG]  18 Aug 2023\n2\nJournal Title XX(X)\nAlternatively, the multiple policy approach addresses the\nMOMDP problem by finding a coverage set of optimal\npolicies that can satisfy any user’s preference in solving\nthe problem. This is achieved by performing an intensive\npolicy search procedure that evolves and orders policies based\non their performance across the defined objectives. While\novercoming the limitations of the single policy approach,\nthis comes with additional costs. First, this approach has\nhigher computational complexity as it requires an intensive\ninteraction with the environment to evolve a set of policies\ninstead of of a single one. Secondly, it assumes stationary\nenvironmental dynamics. This makes it inflexible to non-\nstationary environments, as changes in the environment will\ndemand re-optimization of the evolved policy coverage set.\nIn order to overcome these limitations in the current\nMORL methods in either dealing with changes in the\nuser’s preference, or with the non-stationary environments,\nwe propose a developmental multi-objective optimization\nmethod. The main hypothesis behind this method is that,\ndespite the existence of a large set of specialized policies\nfor every possible user’s preference, there is possibly a\nbounded set of steppingstone policies that can bootstrap any\nspecialized policy. In contrast to specialized policies greedily\noptimized for a specific user’s preference and environment,\nsteppingstone policies are dedicated to an interval of user\npreference. Targeting a coverage set of steppingstone policies\nthat covers the whole user preference space and can be used to\nbootstrap specialized policies, provides efficient optimization\nthat can work in an online manner and be robust to non-\nstationary environments as we show experimentally in this\npaper.\nThe contribution of this paper is threefold. First, we propose\na robust policy bootstrapping (RPB) algorithm that evolves\na convex coverage set (CCS) of steppingstone policies in an\nonline manner and utilizes the CCS to bootstrap specialized\npolicies in response to new user preferences or changes in the\nenvironmental dynamics. Second, we experimentally evaluate\neach design decision for the proposed algorithm in order\nto shed light on the configurable parts that can be changed\nfor different scenarios. Finally, we compare our proposed\nalgorithm with state-of-the-art multi-objective reinforcement\nlearning algorithms in stationary and non-stationary multi-\nobjective environments.\nThe remainder of this paper is organized as follows. The\nbackground section introduces the fundamental concepts\nand the problem definition. The related work section\nreviews relevant literature. The methodology section\nillustrates the proposed algorithm and its workflow. The\nexperimental design section describes the aim, assumptions,\nand methodology of the experiments. The results section\npresents the experimental results and discussion. Finally, the\nconclusion section concludes the work and identifies potential\nfuture work.\nBACKGROUND\nThis section introduces the fundamental concepts necessary\nfor the work and formulates the research problem.\nFigure 1. The policy search space of a bi-objective problem.\nThe set of non-dominated polices is represented by the red\ncircles, which is known as the Pareto front.\nMulti-objective Optimization\nThe problem of multi-objective optimization includes\nmultiple conflicting objectives that can not be simultaneously\noptimized without a compromise (Deb and Deb 2014). The\nmathematical formulation of such a problem can be as\nfollows:\nmax (R1 (π) , R2 (π) , . . . , RM (π))\n(1)\ns.t. gj (π) ≤0, j = 1, 2, . . . , J\nThe\naim\nis\nto\noptimize\nthe\nperformance\nof\nthe\nresultant\nsolutions\nover\na\nset\nof\nreward\nfunctions\n\b\nR1(π), R2(π), . . . , RM(π)\n\t\n, given that each function\nrepresents an objective om (m = 1, 2, . . . , M), the parameter\nπ ∈Π refers to the parameter configuration of the policy\n(decision variables) to be optimized over the search space\nΠ, while\n\b\ng1(π), g2(π), . . . , gJ(π)\n\t\nconstitutes the set of\nconstraint functions defined in the problem.\nPolicies have to be ranked and evaluated based on\nperformance dominance in order to reach the optimal\ncoverage set that can solve the problem while satisfying all\npossible user’s preferences.\nDefinition 0.1. Dominance: “A solution (A) dominates\nsolution (B) if (A) is better than (B) for at least one objective\nand is equal to (B) for all other objectives.” (Abdelfattah et al.\n2018)\nAdditional illustration of Definition 0.1 is presented in\nFigure 1, which shows a two-objective policy search space. It\ncan be noticed that policy (B) is dominated by policy (A). The\nset of red circles represents the set of non-dominated policies\nknown to as the Pareto front.\nThe user selects a policy from the set of non-dominated\npolicies based on his/her preference over the defined\nobjectives.\nDefinition 0.2. Preference: “A preference is defined as a\nweight vector with each weight element dedicated to a specific\nobjective ⃗w =\n\u0002\nw1, w2, . . . , wM\u0003T , such that the sum of all\nthe elements equals one PM\nm=1 wm = 1.” (Abdelfattah et al.\n2018)\nPrepared using sagej.cls\nSmith and Wittkopf\n3\nGiven a user’s preference, which constitutes a tradeoff\namong the defined objectives, a scalarization function can be\nused to formulate a combined objective to be optimized.\nDefinition 0.3. Scalarization Function: “A scalarization\nfunction h, transforms a vector of multiple objectives’ values\ninto a single objective scalar value given a preference as\nparameter o⃗w = h(⃗o, ⃗w).” (Abdelfattah et al. 2018)\nIn case of using of linear or piecewise linear scalarization\nfunctions (Eichfelder 2008), the non-dominated policies front\nis referred to as the convex hull (CH).\nDefinition 0.4. Convex Hull: “A convex hull is a subset of\nthe policy space (Π) that contains optimal policies that can\nmatch any user’s preference.” (Abdelfattah et al. 2018)\nCH(Π) =\nn\nπ : π ∈Π ∧∃⃗w ∀(π′ ∈Π) ⃗w · ⃗rπ ≥⃗w · ⃗rπ′o\nFor further illustration of the CH concept, Figure 2\nvisualizes the CH surface using linear scalarization functions\nover a two objective problem. The axes represent the\nnormalized reward functions for the two objectives. The\nsurface shaped by solid lines, which includes the set of non-\ndominated policies (i.e., red circles), represents the CH. The\nnon-convex surface shaped by solid and dashed line segments\nrepresents the Pareto front which contains all the red and blue\npoints. The non-dominated policy set falling within the CH\nis depicted as red circles. The blue circles refer to the set of\nnon-dominated policies that falls within the Pareto front and\noutside the CH. Dominated policies are represented by black\ncircles.\nFigure 2b depicts the scalarized reward front produced\nusing a linear scalarization function. Where each line\nrepresents a unique preference. The first weight component\n(w1) is shown on the x-axis and the second weight component\ncan be computed from the first component value given the\nsummation to one constraint (w2 = 1 −w1). The y-axis\nshows the scalarized reward value. In Figure 2b, the surface of\nthe bold black line segments, which forms a linear piecewise\nfunctions, represents the CH in this scenario.\nAs the CH surface can contain redundant policies (Roijers\net al. 2013), we can define a subset of it that contains the\nminimal number of unique policies that solve the problem, to\nbe the convex coverage set (CCS).\nDefinition 0.5. Convex Coverage Set: “A convex coverage\nset (CCS) is a subset of the CH that can provide for each\npreference (⃗w) a policy whose scalarized reward value is\nmaximal.” (Abdelfattah et al. 2018)\nCCS (Π) ⊆CH (Π) ∧\n(∀⃗w) (∃π)\n\u0010\nπ ∈CCS (Π) ∧∀(π′ ∈Π) ⃗w · ⃗rπ ≥⃗w · ⃗rπ′\u0011\nMulti-objective Markov Decision Processes\nA Markov decision process (MDP) is a sequential planning\nproblem in which the learning agent senses its current\nstate in the environment (st) at time t, performs an action\n(at) which results in transition to a new state (st+1) and\nreceives a reward/penalty (rt+1) for reaching this new\nstate (Papadimitriou and Tsitsiklis 1987). This paradigm\nis extended in multi-objective Markov decision processes\nby having multiple reward signals instead of a single one\nafter performing an action (Roijers et al. 2013). Figure\n3 shows a comparison between these two problems. The\ntuple\nD\nS, A, Pss′ , ⃗R, µ, γ\nE\nrepresents a MOMDP problem,\ngiven that S refers to the state space, the action space\nis A, Pss′= Pr(st+1 = s\n′|st = s, at = a) represents the\nstate transition distribution which may have a time varying\nparameters (dynamics) in non-stationary environments,\n⃗R ∈RM ∀R : S × A × S′ →r ∈R represents the rewards\nvector corresponding to M objectives, µ = Pr(s0) is the\ninitial state distribution, and finally, γ ∈[0, 1) represents the\ndiscounting factor that balances the priority of immediate\nversus future rewards during the learning process.\nAccordingly, the learning agent aims at maximization the\nexpected return of the scalarized reward signal. Initializing\nat time t and provided a user’s preference ⃗w, this scalarized\nreturn can be calculated as:\nR ⃗w\nt =\nT\nX\nl=0\nγlh (⃗rt+l+1, ⃗w)\n(2)\nsuch that T refers to the time horizon which approaches ∞in\nthe infinite time horizon situation.\nProblem Definition\nThe research problem in this paper can be formulated as:\nprovided a MOMDP problem setup\nD\nS, A, Pss′, ⃗R, µ, γ\nE\n, we\naim at finding the CCS under non-stationary dynamics of the\nenvironment (i.e., time varying parameters of state transition\ndistribution) that satisfies the minimal cardinality while\nmaximizing the scalarized reward return for any preference\nset given at T time horizon:\nmax R ⃗wi\nt\n=\nT\nX\nl=0\nγlh (⃗rt+l+1, ⃗wi)\n(3)\nmin |CCS|\ns.t. ⃗wi ∈W ∀⃗wi ∈RM ,\nM\nX\nm=1\nw m = 1\nwhere W is the set of all legitimate user’s preferences over\nthe defined objectives.\nRELATED WORK\nIn this section, we review related work in the MORL literature\nin order to highlight the contribution of this paper.\nThere are two main methods in MORL literature: single\npolicy methods; and multiple policy methods (Roijers et al.\n2013; Liu et al. 2015). Given a user’s preference defined\nprior to solving the problem, then a single policy can be\nlearned using a scalarized reward function utilizing any single\nobjective reinforcement learning algorithms. In contrast,\nmultiple policy methods search and prioritize the policy\nsearch space in order to reach the set of non-dominated\npolicies that can solve the problem given any possible user’s\npreference. We are going to review each of these methods as\nfollows.\nSingle Policy Methods Many single method techniques\nutilize scalarization functions in order to combine the multiple\nPrepared using sagej.cls\n4\nJournal Title XX(X)\n   \na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n(a)\na\nc\ne\nf\ng\nh\ni\n  \n  \n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(b)\nFigure 2. Comparing the Pareto front surface with the convex hull surface in a bi-objective scenario.(Abdelfattah et al. 2018).\nAgent\nEnvironment\n  \n  \n  \n(a)\nAgent\nEnvironment\n…\n(b)\nFigure 3. Comparing single objective Markov decision process (MDP) with the multi-objective variant known as multi-objective\nMarkov decision process (MOMDP). (a) Markov decision process (MDP). (b) Multi-objective Markov decision process (MOMDP)\n(Abdelfattah et al. 2018).\ndefined objectives into a single objective. Kasimbeyli et\nal. (Kasimbeyli et al. 2015) discussed the properties of\ndifferent scalarization functions in solving multi-objective\noptimization problems. Moffaert et al. (Moffaert et al. 2013)\nintroduced a variant of the Q-learning algorithm (Watkins\nand Dayan 1992) that utilizes the Chebyshev function for\nreward scalarization solving an MOMDP grid-world scenario.\nCastelletti et al. (Castelletti et al. 2013) used non-linear\nscalarization functions with a random exploration of the\nweight space for optimizing the workflow of irrigation\nresource management systems. Lizotte et al. (Lizotte et al.\n2010) introduced linear scalarization with an algorithm that\nadopts the classical value iteration algorithm in order to rank\nactions for finite state space problems. Perny and Weng (Perny\nand Weng 2010) utilized a linear programming method that\nincorporates Chebyshev scalarization to address a MOMDP\nproblem.\nAlternatively, other methods utilize linear programming\ntechniques that follow lexicographic ordering of objectives\n(Marchi and Oviedo 1992). Ogryczak, Perny, and Weng\n(Ogryczak et al. 2011) further developed the aforementioned\nmethod by adopting a regret technique instead of reward\nscalarization for action ranking. The regret value was derived\nfor each objective given an anchor point and the ranking\nis done based on the summed regret from all the defined\nobjectives. (Feinberg and Shwartz 1995; Altman 1999)\nproposed an alternative approach based on constrained\nmulti-objective optimization which transforms the MOMDP\nproblem into a MDP problem by focusing the optimization\nprocess on one objective while dealing with the remaining\nobjectives as constraints.\nMultiple Policy Methods Techniques of user’s preference\nelicitation learn the user’s preference gradually by sampling\nfrom different policy trajectories and observing the user’s\nfeedback in order to adjust future action selection (Mousseau\nand Pirlot 2015). One method of this group was proposed by\nAkrour et al. (Akrour et al. 2011) where the preference of\ndomain experts was acquired within the policy optimization\nusing an algorithm named preference-based policy learning\n(PPL). The algorithm assumed a defined parameterization of\nthe policy that can be sampled to draw different trajectories.\nThe preference of the domain expert is inferred implicitly\nby asking them to rank the performed trajectories, which is\nutilized to guide the next sampling iteration of trajectories.\nA similar approach was introduced by F¨urnkranz et al.\n(F¨urnkranz et al. 2012) which assumes that the Pareto front\nPrepared using sagej.cls\nSmith and Wittkopf\n5\nof optimal policies is found before questioning the expert’s\npreference.\nAnother group of methods adopts evolutionary optimiza-\ntion techniques (Abraham et al. 2005) for searching in the\npolicy space to tackle the MOMDP problem. An evolutionary\nalgorithm was proposed by Busa-Fekete et al. (Busa-Fekete\net al. 2014) for finding the Pareto front of non-dominated\npolicies. Afterwards, the learning agent will perform the\naction recommended by one of the policies in the Pareto front\nbased on the user’s feedback in ranking different sampled\ntrajectories.\nOther methods utilize systematic preference exploration\ntechniques to evolve a coverage set of policies that can solve\nthe MOMDP problem. These methods achieved comparable\nresults to the evolutionary optimization alternatives with\nmore efficient computation complexity (Roijers et al. 2013).\nRoijers, Whiteson, and Oliehoek (Roijers et al. 2014)\nintroduced an algorithm named Optimistic Linear Support\n(OLS) which evolves an approximate coverage set by\nsystematically investigating various user’s preferences over\nthe defined objectives. As an example, given a two-objectives\nscenario, the algorithm explores the two edge preferences (i.e.,\n[0.1, 0.9], [0.9, 0.1]) and generates two corresponding policies\nusing a reinforcement learning algorithm (i.e., Q-learning).\nThe performance of the evolved policies is contrasted using a\npredefined threshold parameter where the one that exceeds it\nwill be added to the final solution set. The algorithm will\ncontinue this procedure by further dividing the two edge\npreferences using a median point and repeating the same\nselection technique until no more policies exceeding the\nthreshold parameter can be found.\nPolicy lexicographic ordering has been explored by\nG´abor, Kalm´ar, and Szepesv´ari (G´abor et al. 1998) in their\nThreshold Lexicographic Ordering (TLO) algorithm which\nranks policies based on a predefined threshold value. For\neach sampled policy, the TLO algorithm performs the action\nthat achieves the maximum reward value exceeding another\npredefined threshold in any of the reward functions or the\nmaximum reward value if all actions are below the threshold.\nOLS and TLO algorithms have been widely adopted in\nmany relevant literature (Roijers et al. 2015; Geibel 2006;\nMossalam et al. 2016) to evolve convex coverage sets\nof non-dominated policies. Therefore, they form a good\nrepresentation for the current state-of-the-art methods and can\nclearly contrast the performance of our proposed algorithm.\nIn a previous work (Abdelfattah et al. 2018), we embarked\non investigating the non-stationary MOMDP environments\nby proposing a new benchmark environment which poses\nnon-stationary state transition dynamics and proposing a\nmulti-objective reinforcement learning algorithm based on\nthe fuzzy segmentation of the preference space. In this work,\nwe propose a generic and simpler algorithm and investigate\nthe different options and design decisions not explored in\nthe previous work. We utilize our previously introduced\nnon-stationary benchmark in the evaluation of the proposed\nalgorithm.\n  \n  \nFigure 4. Dividing the linear scalarization of two objectives into\nregions using the threshold value (φ).\nRobust Policy Bootstrapping For Solving\nMOMDPs\nThe proposed robust policy bootstrapping (RPB) algorithm\naims at maximizing the learning agent’s robustness to\nperturbations in the user’s preferences by evolving a coverage\nset of robust policies that can cope with these perturbations.\nAs mentioned previously, our main assumption is that while\nthere may be a large number of policies, each corresponding\nto a specific user’s preference, a smaller and finite number of\nrobust policies can offer the best steppingstone for any change\nin the preference. To find this coverage set of robust policies,\nwe assume a preference threshold value (φ) that divides the\nlinear scalarization of the defined reward functions into a\nnumber G of regions (see Figure 4). Then, for each region,\na single steppingstone policy is evolved that maximizes\na robustness metric, which measures the stability of its\nbehaviour.\nThis paper first describes the generic design of our\nmethodology. Then, We experimentally evaluate a number of\ndesign options to shed light on configurable parts that can be\nchanged in different applications.\nFigure 5 shows the generic flowchart of our proposed\nalgorithm. The flow starts after a change occur in the user’s\npreference represented by a weight vector over the defined\nreward functions. Then, the distance between the previous\nworking preference (−→\nw t−1) and the new one (−→\nw t) is measured\nusing a vector distance function. If the distance is less\nthan the significance threshold (φ), (which means that we\nare still in the same preference region) then the current\npolicy is used to respond to the new preference and policy\noptimization using the RL algorithm will continue. Otherwise,\nthe policy bootstrapping mechanism is activated. The first\nstep in this mechanism is to store the steppingstone policy\nfor the previous preference region in the CCS. To achieve\nthat we search in the existing CCS for a steppingstone policy\ndedicated to the previous preference region. If a steppingstone\npolicy is found, then it is compared with the current policy\non the basis of robustness value. Finally, the best one is\nstored in the CCS. Alternatively, in the case that there is\nno steppingstone policy found for the previous preference\nregion, then the current policy is directly assigned to it and\nsaved in the CCS. For each steppingstone policy pk, we\nstore three parameters\n\nπk, ⃗wk, βk\u000b\n. Where πk is the adopted\nPrepared using sagej.cls\n6\nJournal Title XX(X)\nreinforcement learning algorithm’s parameters (e.g., Q-values\nin Q-learning algorithm), ⃗wk is the preference corresponding\nto this policy, and βk is the robustness metric value. Finally,\nwe search in the CCS for the steppingstone policy with the\nminimum distance to the new preference. This policy is used\nto bootstrap the ensuing policy optimization procedure using\nthe reinforcement learning algorithm.\nAs\nwe\nare\ndealing\nwith\ngrid\nworld\nbenchmark\nenvironments, we adopt a scalarized version of Q-\nlearning (Abdelfattah et al. 2018; Watkins and Dayan 1992)\nfor solving the MOMDP task using scalarization given the\nweights vector as depicted in Algorithm 1. Q-learning has\nbeen successfully deployed to optimize policies for such\nenvironments effectively (Littman 2001; Greenwald et al.\n2003; Al-Tamimi et al. 2007; Dearden et al. 1998; Tsitsiklis\n1994). Accordingly, the policy parameters πk stored in the\nCCS are the Q(s, a) values for each state and action pairs.\nWe refer to this algorithm as SQ-L.\nIt has to be noted that the proposed RPB algorithm is not\nlimited to the use of Q-learning only as any reinforcement\nlearning algorithm (Szepesv´ari 2010) can be used instead of it\nwithout affecting the workflow. In other words, changing the\nreinforcement learning algorithm will only change the policy\nparameterization π. For example, changing Q-learning with a\npolicy gradient reinforcement learning algorithm (Sutton et al.\n2000) will change π from Q(s, a) values to weight matrices\nof a neural network.\nAlgorithm 1 Scalarized Q-Learning (SQ-L) (Abdelfattah et al.\n2018)\nRequire: A preference ⃗w, πinit\n1: if πinit = ϕ then\n2:\nInitialize Q(s, a) ∀s ∈S, a ∈A(s) arbitrarily\n3: else\n4:\nInitialize Q(s, a) ∀s ∈S, a ∈A(s) from πinit\n5: repeat\n6:\nfor each episode do\n7:\nInitialize S\n8:\nfor each time point t do\n9:\nTake at from st using policy derived from Q\n(e.g., ϵ-greedy) , observe ⃗rt+1, st+1\n10:\nCalculate scalarized reward ρ = ⃗w · ⃗rt+1\n11:\nQ(st, at) ←Q(st, at) +\nα [ρ + γmaxa′Q(st+1, a′) −Q(st, at)]\n12:\ns ←st+1\n13: until s is terminal\nThe pseudocode of the proposed robust policy bootstrap-\nping algorithm is presented in Algorithm 2.\nThere are three main design decisions that need to be\nconfigured in the previously described generic procedure as\nfollows.\n1. The distance function d (−→\nw t, −→\nw t−1) that measures the\ndifference between user preferences.\n2. The metric β(p) for measuring the robustness of\ngenerated policies.\n3. The value for the preference significance threshold\nparameter φ.\nAlgorithm 2 Robust Policy Bootstrapping (RPB)\nRequire: Preferences at times t and t −1 (−→\nw t, −→\nw t−1).\n1: dt := d (−→\nw t, −→\nw t−1)\n2: if dt > φ then\n3:\nset p∗equal to the policy with preference distance\nd (−→\nw ∗, −→\nw t−1) ≤φ from CCS\n4:\nif p∗= ∅then\n5:\nStore pt−1 in CCS\n6:\nelse if β(pt−1) > β(p∗) then\n7:\nReplace p∗with pt−1 in CCS\n8:\nRetrieve the best policy p′ = arg min\npk∈CCS\nh\nd\n\u0010−→\nw t, −→\nw k\u0011i\n9: Follow the Scalarized Q-Learning algorithm, SQ-\nL(−→\nw t, π′)\nWe are going to conduct a separate empirical analysis for\neach of these design decision in order to highlight the impact\nof different configurations.\nEXPERIMENTAL DESIGN\nIn this section, we present our experimental design for\nassessing the performance of the proposed algorithm.\nEnvironments\nIn this work, a benchmark of three MOMDP environments\nis utilized. This benchmark includes a resource gathering\nenvironment, deep-sea treasure environment, and a search\nand rescue environment. The former environments are well-\nestablished in relevant literature (Vamplew et al. 2011), while\nthe latter one was proposed in our previous work (Abdelfattah\net al. 2018). Latter environment has stochastic state transition\ndynamics necessary for examining performance in non-\nstationary environments. A graphical representation of these\nenvironments is shown in Figure 6.\nSearch and Rescue (SAR) Environment\nState Space: SAR environment is a grid-world with\ndimensions 9 × 9 that simulates a task in which the learning\nagent has to search for trapped victims while avoiding\nexposure to fire. Accordingly, the state representation is\ndefined as ⟨X, Y, F, O, H⟩, where X, Y indicate the latest\nposition features, and F, O, H are boolean flags for the\nexistence of fire, an obstacle, or victim in the current\nposition. As a stochastic feature of the state transition in\nthis environment, each victim has a random death time\nξi, i ∈{1, 2, 3, . . . , N} for N victims. The episode ends\nwhen all victims are dead or rescued.\nAction\nSpace:\nThe\naction\nspace\nis\nA =\n{MoveEast, MoveWest, MoveNorth, MoveSouth}\nwith one square per movement. Attempting to move into a\nlocation occupied by an obstacle fails (i.e., the agent remains\nat its current location and incurs a penalty).\nReward Functions:\nThis environment includes two\nmain objectives: minimizing the time to finish the task, and\nminimizing destruction by fire. Accordingly, there are two\ncorresponding reward functions ⃗r =\n\u0002\nrfire, r time\u0003\n, ⃗r ∈R2.\nEvery time the agent collides with a fire it gets a penalty of\nPrepared using sagej.cls\nSmith and Wittkopf\n7\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nYes \nNo \nNo \nYes \nThe user changes \nher preference \n𝒘ሬሬሬԦ𝒕 \n𝒅𝒕:= Distance (𝒘ሬሬሬԦ𝒕, 𝒘\nሬሬሬԦ𝒕−𝟏) \n𝒅𝒕> 𝝋 \nRL_Algorithm (𝒘ሬሬሬԦ𝒕 , 𝝅𝒕−𝟏) \n𝒑∗≔ 𝐚𝐫𝐠 \n𝒑∈{𝚷}\n𝑫𝒊𝒔𝒕𝒂𝒏𝒄𝒆𝒑(𝒘\nሬሬሬԦ 𝒑, 𝒘\nሬሬሬԦ𝒕−𝟏)< 𝝋 \n𝒑∗= ∅ \nYes \nNo \n𝐂𝐂𝐒←𝒑𝒕−𝟏 \n \n𝒑′ = 𝐚𝐫𝐠 𝐦𝐢𝐧\n𝒑𝒌∈{𝚷}\n 𝑫𝒊𝒔𝒕𝒂𝒏𝒄𝒆(𝒘\nሬሬሬԦ 𝒑,𝒘\nሬሬሬԦ𝒕) \nEND \n𝒑∗≔𝒑𝒕−𝟏 \n𝐂𝐂𝐒←𝒑∗ \nRL_Algorithm (𝒘ሬሬሬԦ𝒕 ,𝝅′) \n𝑹𝒐𝒃𝒖𝒔𝒕(𝒑𝒕−𝟏) > 𝑹𝒐𝒃𝒖𝒔𝒕(𝒑∗) \nFigure 5. A flowchart diagram describing the generic RPB algorithm workflow (Algorithm 2).\nPrepared using sagej.cls\n8\nJournal Title XX(X)\n(a)\n(b)\n(c)\nFigure 6. Graphical representation of the benchmark environments. (a) The search and rescue (SAR) environment. (b) The deep sea\ntreasure (DST) environment. (c) The resource gathering (RG) environment (Abdelfattah et al. 2018).\n−5 and 0 elsewhere, while there is a constant time penalty for\neach step of −1.\nDeep Sea Treasure (DST) Environment\nState Space: This grid-world environment has dimensions\nof 10 × 11. It simulates an undersea treasure search scenario\nin which a submarine agent is trying to maximize the found\ntreasure value. The state representation is defined by the\nfeatures of the current location ⟨X, Y ⟩. The episode ends\nwhen a treasure is found.\nAction Space: The learning agent can move in one of the\nfour directions at each step A = {Left, Right, Up, Down}\nwhere moving in directions that make the agent go outside\nthe gird will not change the latest state.\nReward Functions: The submarine has two conflicting\nobjectives of maximizing the resource reward while\nminimizing task’s time. The reward vector for these objectives\nis ⃗r =\n\u0002\nr time, r treasure\u0003\n, ⃗r ∈R2. The value of rtime is a\nconstant equal to −1 on every action, while treasure reward\nrtreasure will be unique for each treasure.\nResources Gathering (RG) Environment\nState Space:\nThe dimensions of this grid-world\nenvironment are 5 × 5. The learning agent aims at acquiring\nresources and returning back home without being attacked\nby an enemy. The representation of the state space is\n⟨X, Y, G, Y, E⟩, provided that X, Y are the features of the\ncurrent position, and G, Y, E are boolean flags referring to the\nexistence of gold, gem, or enemy in the current position. This\nenvironment has stochastic state transition with probability\nof enemy attack of 10%. After enemy attacks, the agent will\nlose any gathered resources and will be returned back to its\nhome. The episode ends when the agent safely returns to the\nhome location or when an attack occurs.\nAction\nSpace:\nThe\nlearning\nagent\nhas\nthe\nability to move to one of the four directions A =\n{MoveEast, MoveWest, MoveNorth, MoveSouth}\nby one step at a time.\nReward Functions: The learning agent has two objectives\nin this environment which are to maximize the reward\nvalue of collected resources and to minimize exposure\nto attacks from the enemy. These are represented as ⃗r =\n[ r resources, r enemy] , ⃗r ∈R2, where rresources equals 1 for\neach gathered resource, and every enemy attack will result in\na penalty renemy of −1.\nComparison Algorithms\nWe compare our proposed algorithm with three MORL\nalgorithms: 1) Optimistic Linear Support (OLS) (Roijers et al.\n2014), 2) Threshold Lexicographic Ordering (TLO) (G´abor\net al. 1998) and 3) Robust Fuzzy Policy Bootstrapping (RFPB)\n(Abdelfattah et al. 2018). In addition, we add the (SQ-L)\nalgorithm (see Algorithm 1) to the comparison as a baseline\nusing a random policy initialization after each preference\nchange. We explored an alternative policy initialization\napproach for the SQ-L algorithm that adopts the parameters\nof policy optimized for the last preference. We found that\ngiven a uniform preference sampling, there was no significant\ndifference between these two policy initialization approaches.\nAs both the OLS and TLO algorithms require an offline\nsimulation phase to evolve their coverage sets, we run them\nuntil convergence before comparing with our algorithm in the\nexecution phase. For the parameter configuration of the OLS,\nTLO, and RFPB algorithms we utilize the same configuration\nin (Roijers et al. 2014), (Geibel 2006), and (Abdelfattah\net al. 2018) respectively. For the proposed RPB algorithm\nwe conduct empirical analysis in order to compare the effect\nof different configurations.\nExperiments\nTo shed light on the design decisions and to assess\nthe performance of our RPB algorithm, we conduct five\nexperiments. These experiments include empirical analysis\nfor the three main design decisions: 1) preference significance\nthreshold, 2) robustness metric and 3) and distance\nfunction; and two additional experiments for contrasting the\nperformance of the proposed algorithm with state-of-the-art\nmethods in MORL literature in stationary and non-stationary\nenvironments.\nEmpirical\nAnalysis\nfor\nthe\nPreference\nSignificance\nThreshold (φ)\nAim:\nThis experiment aims at assessing the optimal\nvalue of the preference significance threshold (φ) in each\nexperimental environment.\nMethod: We evaluate 10 different threshold values that\ngradually increase from 0.05 to 0.5. For each run, we execute\nPrepared using sagej.cls\nSmith and Wittkopf\n9\nthe 10 uniformly sampled preferences in Table 1 giving\neach 800 episodes. We run 15 independent runs for each\nenvironment.\nEvaluation Criteria: We compare the reward loss values\nafter switching the preference for each threshold value over\nthe 15 independent runs. The one that achieves the lowest\nvalue on this metric is the best fit for the environment.\nEmpirical Analysis for the Robustness Metric (β)\nAim: The aim of this experiment is to assess the robustness\nmetric to use in the design of the RPB algorithm.\nMethod: For investigating the different variations of policy\nrobustness metrics, we evaluated five candidate metrics for\neach environment. We selected these metrics based relevant\nliterature for policy robustness evaluation (Bui et al. 2012;\nChow et al. 2015; Jamali et al. 2014; Deb and Gupta 2006;\nEhrgott et al. 2014; Jen 2003). The metrics are presented in\nTable 2. The first metric measures the stability of the policy’s\nbehaviour in terms of mean of rewards divided by standard\ndeviation. The second metric measures the dispersion of\na probability distribution (i.e., the observed reward values\nin our case), it is the ratio of variance to the mean and\nit is called index of dispersion (IoD). The third metric\nmeasures the degree of variability with respect to the mean\nof the population. It is calculated as the ratio between the\nstandard deviation and the mean and it is referred to as The\ncoefficient of variation (CV). The fourth metric is the entropy\nof the reward distribution. It is derived from the concepts\nof Information theory. Finally, the fifth metric is the regret,\ndefined as the difference between the reward mean of the\ncurrent policy and the reward mean of the optimum policy.\nWhile this metric has the potential to guide the policy search\ngiven a ground truth (optimum policy) it is not applicable in\nmany scenarios in which an optimum policy cannot be known\nprior solving the problem. For the last metric evaluation,\nwe utilize the policies generated by the OLS algorithm as\noptimum policies to compare with. We separate the regret\nmetric with a dotted line in Table 2 in order to indicate\nthe difference in its assumption compared to the rest of the\nmetrics.\nEvaluation Criteria: We compare the five metrics by\nrunning 15 independent runs for each metric. Each run\nincludes 800 episodes for each preference in the 9 sampled\npreferences in Table 1. Then, we calculate the median reward\nvalue for each preference and sum the medians to get one\nrepresentative value for each run. Finally, we average the 15\nindependent sums and visualize the average with standard\ndeviation bars.\nEmpirical\nAnalysis\nfor\nthe\nDistance\nFunction\n(d (−→\nw t−1, −→\nw t))\nAim: In this experiment, we evaluate the effectiveness\nof\nvarious\ndistance\nfunctions\nover\nall\nexperimental\nenvironments.\nMethod: We evaluate four well-known distance functions\nincluding the Euclidean, Hamming, Cosine, and Manhattan\ndistance functions. The equations for these functions are\nillustrated in Table 3\nEvaluation Criteria: We followed a similar evaluation\ncriteria as in the robustness metric empirical analysis. We run\n15 independent runs for each metric. Each run includes 800\nepisodes for each preference in the 9 sampled preferences in\nTable 1. Then, we calculate the median reward value for each\npreference and sum the medians to get one representative\nvalue for each run. Finally, we average the 15 independent\nsums and visualize the average with standard deviation bars.\nPerformance Evaluation in Stationary Environments\nAim: The aim of this experiment is to assess the impact of\nuser’s preference changes on each of the comparison methods\nwhile neutralizing the impact of the environment’s dynamics\nchanges by operating in a stationary environment. In this\nexperiment, the environment’s dynamics (i.e., distribution of\nobjects) is fixed during each run. Therefore, only the start\nlocation of the agent is randomized at the beginning of each\nrun. In addition, the two comparative MORL algorithms (OLS\nand TLO) assume a stationary environment setup, therefore,\nthis experiment guarantees a fair comparison by including the\nbest scenario for them.\nMethod: We compare the four algorithms based on a\nset of (9) random preferences sampled uniformly from the\ntwo-dimensional weight space for the three experimental\nenvironments. Table 1 presents the set of the randomly\nsampled preferences. We execute 30 runs each with a different\ninitial distribution of objects. Each preference is given a total\nof 800 episodes and the average reward value over the last\n50 episodes is used to compare the four algorithms. As the\nOLS and TLO algorithms are working in an offline mode, we\nfirstly, run their training procedures till convergence (evolving\nthe CCS), then, we compare them with the other algorithms.\nThe RPB algorithm is configured based on the outcomes of\nthe empirical analysis of each design configuration.\nEvaluation Criteria: We evaluate the performance of each\nalgorithm using two metrics: the average reward value over\nthe last 50 episodes for each preference; and the average\nreward loss after preference change. The former metric\nreflects the performance level of the generated policy for\neach algorithm in response to the user’s preference in terms\nof reward value after convergence. While the latter metric\nshows the robustness of each algorithm to changes in the\nuser’s preference in terms of reward loss.\nFor further illustration of these two metrics, Figure 7\nshows a diagram for the average reward signal over a single\npreference change from preference (A) to preference (B). The\nvalue Γc is the average reward value over the last 50 episodes\nof preference (A). While Γl is the average reward value over\nthe first 50 episodes of preference (B). We utilize Γc to show\nthe average reward value after convergence and (Γc −Γl) to\nshow the average loss value after the preference change from\n(A) to (B).\nPerformance Evaluation in Non-stationary Environments\nAim: This experiment aims at evaluating the impact of\nthe environment dynamics change, while using the same\npreference change pattern, therefore, the performance level\nof each comparison algorithm in non-stationary environments\ncan be well contrasted. In order to achieve that we allow the\nenvironment’s setup to change within each run by allowing\n25% of the objects (e.g., fire, victim, obstacle, treasure) to\nchange their locations randomly every 100 episodes.\nPrepared using sagej.cls\n10\nJournal Title XX(X)\nTable 1. The set of uniformly sampled user preferences utilized in the experimental design.\nPreference\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nw1\n0.66\n0.33\n0.28\n0.54\n0.68\n0.44\n0.88\n0.65\n0.48\nw2\n0.34\n0.67\n0.72\n0.46\n0.32\n0.56\n0.12\n0.35\n0.52\nTable 2. Robustness metrics utilized in the empirical analysis for (β) design decision\nMetric\nEquation\nStability\nµ\nσ\nIndex of dispersion (IoD)\nσ2\nµ\nCoefficient of variation (CV)\nσ\nµ\nEntropy\n−Pn\ni=1 P(xi) logb P(xi)\nRegret\nµπ∗−µπ\nTable 3. A list of distance functions evaluated in the empirical analysis\nDistance Function\nEquation\nEuclidean\nqPM\nm=1\n\u0000wm\nt−1 −wm\nt\n\u00012\nHamming\nPM\nm=1\n\u0002\nwm\nt−1 ̸= wm\nt\n\u0003\nCosine\nwt−1·wt\n∥wt−1∥∥wt∥\nManhattan\nPM\nm=1\n\f\fwm\nt−1 −wm\nt\n\f\f\nFigure 7. A diagram that that illustrates the two evaluation\nmetrics utilized in our experimental design. The average reward\nvalue over the last 50 episodes is represented by Γc. While the\naverage reward loss after change from preference (A) to\npreference (B) is represented by (Γc −Γl).\nMethod: We utilize the same method as in the stationary\nenvironment experiment.\nEvaluation Criteria: We use the same evaluation metrics\nas in the stationary environment experiment.\nRESULTS AND DISCUSSION\nIn this section, we are going to present and discuss the results\nof each of the five experiments included in the experimental\ndesign.\nEmpirical Analysis for the Preference\nSignificance Threshold (φ)\nFigure 8 shows the average reward loss after preference\nchange for the preference distance threshold parameter (φ)\nvalues for each experimental environment. For the SAR\nenvironment, the value of (0.25) was found to be the optimal\nas significant reward loss was observed for higher values.\nWhile for the DST and RG environments, the optimum value\nfor (φ) was found to be (0.15). These results indicate that\nthe there is no one optimum threshold value that can fit all\nscenarios. In other words, the preference threshold parameter\nneeds to be tuned for each different environment based on\nits dynamics and the defined reward functions. This finding\nopens the way for further future enhancements to the proposed\nalgorithm through ways that automatically tune this threshold\nparameter (our previously proposed algorithm (Abdelfattah\net al. 2018), while more complex, is able to do this). The\nRPB algorithm will utilize the identified φ values for each\nenvironment during comparison to other MORL algorithms.\nEmpirical Analysis for the Robustness Metric (β)\nFigure 9 shows the comparison results for the robustness\nmetrics. While the regret metric achieved the best results in\nterms of average reward value over the three experimental\nenvironment, it is not applicable in many scenarios in which\nthe CCS of policies is not defined beforehand. The stability\nmetric achieved the best overall results in comparison to\nthe other three remaining metrics. Thus, the RPB algorithm\nwill use the stability metric during comparison with other\nalgorithms.\nPrepared using sagej.cls\nSmith and Wittkopf\n11\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nWeight Threshold Values\n160\n140\n120\n100\n80\n60\n40\n20\nReward Loss\n(a)\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nWeight Threshold Values\n140\n120\n100\n80\n60\n40\n20\n0\nReward Loss\n(b)\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nWeight Threshold Values\n14\n12\n10\n8\n6\n4\n2\n0\nReward Loss\n(c)\nFigure 8. Empirical analysis results for determining the optimal (φ) threshold value of the RPM algorithm for each experimental\nenvironment. The boxplots represent the distribution of reward loss values after switching preferences for each threshold value. (a)\nResults for the SAR environment. (b) Results for the DST environment. (c) Results for the RG environment.\nEmpirical Analysis for the Distance Function\n(d (−→\nw t−1, −→\nw t))\nFigure 10 depicts the comparison results for different distance\nfunction and for each experimental environment.\nBased on the results it can be observed that the Euclidean\ndistance function achieved the best overall results among the\nother comparative functions. A justification to this finding\ncan be the representation of the user’s preferences in our\nmethodology as an Euclidean space of two dimensions\n(w1, w2) that are linearly separable. The RPB algorithm will\nutilize the Euclidean distance function during comparison to\nother algorithms.\nPerformance Evaluation in Stationary\nEnvironments\nWe are going to present and discuss the results for each\nexperimental environment for each of the two evaluation\nmetrics utilized. For the average reward value over the last 50\nepisodes metrics, Figure 11 shows a line plot that visualizes\nthe average reward value and its standard deviation over\n30 runs for each experimental environment and for each\ncomparison algorithm.\nWhile the results of the three environments differ in the\nmagnitude of the average reward values due to changes in\nreward functions, they reflect common implications. It can be\nnoticed that the OLS and TLO achieved the highest results\nin the average reward value across the three experimental\nenvironments, as the two algorithms evolved their CCS during\nthe their offline training phases, therefore they were able\nto respond with an optimal policy after each preference\nchange. While the RPB and RFPB algorithms started with\nan empty CCS as they work in an online manner, they\nachieved comparable results to the OLS and TLO algorithms\nand significantly (t-test: p-value < 0.05) outperformed the\nSQ-L algorithm configured to respond to each preference\nPrepared using sagej.cls\n12\nJournal Title XX(X)\nStability\nCV\nIoD\nRegret\nEntropy\nRobustness Metric\n80\n70\n60\n50\n40\n30\n20\n10\n0\nAverage Reward Value\n(a)\nStability\nCV\nIoD\nRegret\nEntropy\nRobustness Metric\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAverage Reward Value\n(b)\nStability\nCV\nIoD\nRegret\nEntropy\nRobustness Metric\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nAverage Reward Value\n(c)\nFigure 9. Empirical analysis results for determining the robustness metric to be used in the RPM algorithm for each experimental\nenvironment. (a) Results for the SAR environment. (b) Results for the DST environment. (c) Results for the RG environment. The error\nbars represent the standard deviation.\nchange with a randomly initialized policy. The RPB algorithm\nbenefits from the policy bootstrapping mechanism which\nenables it to build on previous experiences for each preference\nregion given the threshold value (φ). The advantage of the\nRPB algorithm is its simplicity, while the RFPB algorithm\nremoves requirement for the preference significance threshold\nparameter tuning.\nFor the average reward loss metric, Figure 12 depicts the\nresults of 30 runs for each experimental environment and\nfor each algorithm. As the RFPB achieved similar results\non the average reward metric to the RPB, we only report\nthe RPB results on the loss metric due to space constraints.\nSimilarly, the average reward loss after each preference\nchange emphasizes the findings from the average reward value\nmetric as both OLS and TLO algorithms benefited from their\nevolved CCS during the offline training phase to minimize\nthe reward loss after preference changes. While our RPB\nalgorithm achieved comparable results in comparison to the\nOLS and TLO algorithms and significantly outperformed the\nSQ-L algorithm working with random initialization strategy\nafter each preference change.\nThe results of the two metrics indicate that our RPB\nalgorithm can achieve comparable results to the OLS and\nTLO algorithms that require an offline training phase to\nevolve their CCS, while our algorithm works in an online\nmanner. Also, the RPB achieved a similar performance\nlevel compared to the RFPB algorithm, while it adopts a\nsimpler preference decomposition technique. Moreover, the\nRPM algorithm outperforms the SQ-L baseline algorithms\nrepresenting the single policy MORL approach that responds\nto each new preference by randomly initialized policy. While\nthe results show that the current MORL multiple policy\nalgorithms can deal with stationary environments through\noffline exhaustive search techniques, these environments are\nquite rare in practical problems. In the next experiment, we are\ngoing to assess the performance of the comparison algorithms\nunder non-stationary environments to contrast the advantage\nof proposed RPB algorithm.\nPerformance Evaluation in Non-stationary\nEnvironments\nFor the average reward value over the last 50 episodes\nmetric, Figure 13 presents the comparison results for each\nexperimental environment. Despite the variation in reward\nmagnitude across environments as a result of different reward\nfunctions for each environment, the results show that the\nRPB algorithm performed comparably to the RFPB algorithm,\nwhile it significantly outperformed (t-test: p-value < 0.05)\nPrepared using sagej.cls\nSmith and Wittkopf\n13\nEuclidean\nHamming\nCosine\nManhattan\nDistance Function\n80\n60\n40\n20\n0\nAverage Reward Value\n(a)\nEuclidean\nHamming\nCosine\nManhattan\nDistance Function\n0\n10\n20\n30\n40\n50\n60\nAverage Reward Value\n(b)\nEuclidean\nHamming\nCosine\nManhattan\nDistance Function\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nAverage Reward Value\n(c)\nFigure 10. Empirical analysis results for determining the distance function to be used in the RPM algorithm for each experimental\nenvironment. (a) Results for the SAR environment. (b) Results for the DST environment. (c) Results for the RG environment. The error\nbars represent the standard deviation.\nthe other comparative algorithms across all experimental\nenvironments. Mainly, this is due to its ability to evolve the\nCCS in an online manner, which enabled it to better cope\nwith changes in the environment’s dynamics in comparison\nto the OLS and TLO algorithms which failed to adapt\ndue to their outdated CCS evolved in an offline manner.\nAlso it can noticed that the SQ-L baseline, representing the\nsingle policy MORL approach, failed to adapt adequately\nas it can not accumulate knowledge from past experiences.\nAnother remark in these results is that at some situations\nthe OLS and TLO algorithms performed worse than the\nSQ-L baseline as their CCSs were greedy optimized for an\noutdated environment’s setup, consequently, this experience\nwas misleading during the new setup after changes in the\nenvironment’s dynamics.\nThe results of the average reward loss after preference\nchange metric are visualized in Figure 14. These results show\na similar message to the average reward value results as our\nRPB algorithm significantly outperformed (t-test: p-value\n< 0.05) the other comparative algorithms in minimizing the\nreward loss after preference changes. While the two state-of-\nthe-art multiple policy MORL algorithms (OLS and TLO)\nperformed insignificantly to the SQ-L baseline due to the\nlimitation to adapt with non-stationary dynamics in an online\nmanner.\nBased on the recent results, our RPB algorithm behaved\nin a more robust and adaptive manner in the non-stationary\nenvironments comparing to other comparative algorithms.\nReasons behind this finding can be summarized as follows.\nFirst, the RPB algorithm targets generic steppingstone policies\ninstead of specialized policies to specific environment setups\nin the case of the other algorithms, which makes its evolved\ncoverage set more robust to changes in the environment\ndynamics. Second, the ability of the RPB algorithm to\ncontinuously evaluating and enhancing the policy set which\ncannot be achieved in the comparison algorithms that depends\non offline and comprehensive policy search techniques.\nCONCLUSION AND FUTURE WORK\nThis paper proposed a novel algorithm that can robustly\naddress multi-objective sequential decision-making problems\nin non-stationary environments. This is achieved by evolving a\nrobust steppingstone policy coverage set in an online manner,\nthen utilizing this coverage set to bootstrap specialized\npolicies in response to changes in the user’s preference or\nin the environment dynamics. We compared our proposed\nalgorithm with state-of-the-art multi-objective reinforcement\nPrepared using sagej.cls\n14\nJournal Title XX(X)\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nUser Preference\n225\n200\n175\n150\n125\n100\n75\n50\n25\n0\nAvg Reward Over the Last 50 Episodes\nOLS\nTLO\nRPB\nRFPB\nSQ-L\n(a)\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nUser Preference\n0\n20\n40\n60\n80\n100\nAvg Reward Over the Last 50 Episodes\nOLS\nTLO\nRPB\nRFPB\nSQ-L\n(b)\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nUser Preference\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nAvg Reward Over the Last 50 Episodes\nOLS\nTLO\nRPB\nRFPB\nSQ-L\n(c)\nFigure 11. Average reward value and standard deviation over 30\nruns achieved by the four algorithms for each preference in the\nstationary environments. (a) the SAR environment. (b) the DST\nenvironment. (c) the RG environment.\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nLoss\n175\n150\n125\n100\n75\n50\n25\n0\nMagnitude in Reward\nOLS\nTLO\nRPB\nSQ-L\n(a)\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nLoss\n30\n25\n20\n15\n10\n5\n0\nMagnitude in Reward\nOLS\nTLO\nRPB\nSQ-L\n(b)\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nLoss\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\nMagnitude in Reward\nOLS\nTLO\nRPB\nSQ-L\n(c)\nFigure 12. Average reward loss after preference change with\nstandard deviation over 30 runs achieved by the four algorithms\nin the stationary environments. (a) the SAR environment. (b) the\nDST environment. (c) the RG environment.\nPrepared using sagej.cls\nSmith and Wittkopf\n15\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nUser Preference\n225\n200\n175\n150\n125\n100\n75\n50\n25\n0\nAvg Reward Over the Last 50 Episodes\nOLS\nTLO\nRPB\nRFPB\nSQ-L\n(a)\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nUser Preference\n0\n20\n40\n60\n80\n100\nAvg Reward Over the Last 50 Episodes\nOLS\nTLO\nRPB\nRFPB\nSQ-L\n(b)\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nUser Preference\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nAvg Reward Over the Last 50 Episodes\nOLS\nTLO\nRPB\nRFPB\nSQ-L\n(c)\nFigure 13. Average reward value and standard deviation over 30\nruns achieved by the four algorithms for each preference in the\nnon-stationary environments. (a) the SAR environment. (b) the\nDST environment. (c) the RG environment.\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nLoss\n200\n175\n150\n125\n100\n75\n50\n25\n0\nMagnitude in Reward\nOLS\nTLO\nRPB\nSQ-L\n(a)\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nLoss\n35\n30\n25\n20\n15\n10\n5\n0\nMagnitude in Reward\nOLS\nTLO\nRPB\nSQ-L\n(b)\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nLoss\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\nMagnitude in Reward\nOLS\nTLO\nRPB\nSQ-L\n(c)\nFigure 14. Average reward loss after preference change with\nstandard deviation over 30 runs achieved by the four algorithms\nin the non-stationary environments. (a) the SAR environment. (b)\nthe DST environment. (c) the RG environment.\nPrepared using sagej.cls\n16\nJournal Title XX(X)\nlearning algorithms over three different multi-objective\nenvironments using both stationary and non-stationary\ndynamics.\nWe experimentally analyzed the different design decisions\nfor the proposed algorithm in order to transparently\ndescribe the configuration setup and to indicate the possible\nconfigurable parts that can tailored for different application\nscenarios.\nIn order to contrast the performance of our proposed\nalgorithm, we compared it with two state-of-the-art MORL\nalgorithms and one baseline algorithm that is based on\nthe well known Q-learning algorithm. We evaluated the\ncomparison algorithms under both stationary and non-\nstationary environmental dynamics. During the stationary\nenvironments, the performance of the proposed algorithm was\ncomparable to the performance of the other algorithms. While\nin the non-stationary environments, the proposed algorithm\nsignificantly outperformed the other algorithms in terms of\nthe average reward value achieved as it adapted better under\nchanges in the environment dynamics.\nThe future extension for this work can be summarized into\nthree main points. First, we are going to explore adaptive\nstrategies for automatic preference exploration instead\nof the random strategy currently adopted. Unsupervised\nlearning-based generative models can be for this purpose.\nCandidate strategies can include adversarial generative\nnetworks (Goodfellow et al. 2014) and intrinsically motivated\nexploration (Merrick and Maher 2009). Second, we will\ninvestigate the impact of utilizing non-linear scalarization\nfunctions such as Chebyshev function, on the performance\nof the RPB algorithm. Finally, we are going to enhance the\ngeneralization ability of the RPB algorithm through adding a\ngeneric skill learning module, therefore, the generated skill\nset can be used to speed up the learning of specialized policies\nin different environments. We believe that equipping our\nproposed algorithm with the ability to learn generic skills\nwill facilitate the evolution of better policies and the transfer\nof learned knowledge across different environments.\nReferences\nAbdelfattah\nS,\nKasmarik\nK\nand\nHu\nJ\n(2018)\nEvolving\nrobust policy coverage sets in multi-objective markov\ndecision processes through intrinsically motivated self-play.\nFrontiers in Neurorobotics 12: 65.\nDOI:10.3389/fnbot.\n2018.00065.\nURL https://www.frontiersin.org/\narticle/10.3389/fnbot.2018.00065.\nAbraham A, Jain LC and Goldberg R (2005) Evolutionary Multiob-\njective Optimization: Theoretical Advances and Applications\n(Advanced Information and Knowledge Processing). Berlin,\nHeidelberg: Springer-Verlag. ISBN 1852337877.\nAkrour R, Schoenauer M and Sebag M (2011) Preference-\nBased Policy Learning. Berlin, Heidelberg: Springer Berlin\nHeidelberg.\nISBN 978-3-642-23780-5, pp. 12–27.\nDOI:\n10.1007/978-3-642-23780-5 11.\nURL http://dx.doi.\norg/10.1007/978-3-642-23780-5_11.\nAl-Tamimi A, Lewis FL and Abu-Khalaf M (2007) Model-\nfree q-learning designs for linear discrete-time zero-sum\ngames with application to h-infinity control.\nAutomatica\n43(3): 473 – 481. DOI:https://doi.org/10.1016/j.automatica.\n2006.09.019. URL http://www.sciencedirect.com/\nscience/article/pii/S0005109806004249.\nAltman E (1999) Constrained Markov decision processes, volume 7.\nCRC Press.\nBui LT, Abbass HA, Barlow M and Bender A (2012) Robustness\nagainst the decision-maker’s attitude to risk in problems with\nconflicting objectives.\nIEEE Transactions on Evolutionary\nComputation 16(1): 1–19.\nBusa-Fekete R, Sz¨or´enyi B, Weng P, Cheng W and H¨ullermeier E\n(2014) Preference-based reinforcement learning: evolutionary\ndirect policy search using a preference-based racing algo-\nrithm.\nMachine Learning 97(3): 327–351.\nDOI:10.1007/\ns10994-014-5458-8. URL https://doi.org/10.1007/\ns10994-014-5458-8.\nCastelletti A, Pianosi F and Restelli M (2013) A multiobjective\nreinforcement learning approach to water resources systems\noperation: Pareto frontier approximation in a single run. Water\nResources Research 49(6): 3476–3486.\nChow Y, Tamar A, Mannor S and Pavone M (2015) Risk-sensitive\nand robust decision-making: a cvar optimization approach. In:\nCortes C, Lawrence ND, Lee DD, Sugiyama M and Garnett R\n(eds.) Advances in Neural Information Processing Systems 28.\nCurran Associates, Inc., pp. 1522–1530.\nDearden R, Friedman N and Russell S (1998) Bayesian q-learning.\nIn: AAAI/IAAI. pp. 761–768.\nDeb K and Deb K (2014) Multi-objective Optimization. Boston, MA:\nSpringer US. ISBN 978-1-4614-6940-7, pp. 403–449. DOI:\n10.1007/978-1-4614-6940-7 15.\nURL http://dx.doi.\norg/10.1007/978-1-4614-6940-7_15.\nDeb K and Gupta H (2006) Introducing robustness in multi-objective\noptimization. Evolutionary computation 14(4): 463–494.\nEhrgott M, Ide J and Schobel A (2014) Minmax robustness for multi-\nobjective optimization problems. European Journal of Opera-\ntional Research 239(1): 17 – 31. DOI:http://dx.doi.org/10.1016/\nj.ejor.2014.03.013. URL http://www.sciencedirect.\ncom/science/article/pii/S0377221714002276.\nEichfelder G (2008) Adaptive scalarization methods in multiobjec-\ntive optimization, volume 436. Springer.\nFeinberg EA and Shwartz A (1995) Constrained markov decision\nmodels with weighted discounted rewards. Mathematics of\nOperations Research 20(2): 302–320. URL http://www.\njstor.org/stable/3690407.\nF¨urnkranz J, H¨ullermeier E, Cheng W and Park SH (2012)\nPreference-based reinforcement learning: a formal framework\nand a policy iteration algorithm.\nMachine Learning 89(1):\n123–156.\nDOI:10.1007/s10994-012-5313-8.\nURL http:\n//dx.doi.org/10.1007/s10994-012-5313-8.\nG´abor Z, Kalm´ar Z and Szepesv´ari C (1998) Multi-criteria\nreinforcement learning. In: ICML, volume 98. pp. 197–205.\nGeibel P (2006) Reinforcement learning for mdps with constraints.\nIn: ECML, volume 4212. Springer, pp. 646–653.\nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley\nD, Ozair S, Courville A and Bengio Y (2014) Generative\nadversarial nets.\nIn: Ghahramani Z, Welling M, Cortes C,\nLawrence ND and Weinberger KQ (eds.) Advances in Neural\nInformation Processing Systems 27. Curran Associates, Inc., pp.\n2672–2680.\nURL http://papers.nips.cc/paper/\n5423-generative-adversarial-nets.pdf.\nGreenwald A, Hall K and Serrano R (2003) Correlated q-learning.\nIn: ICML, volume 3. pp. 242–249.\nPrepared using sagej.cls\nSmith and Wittkopf\n17\nJamali N, Kormushev P, Ahmadzadeh SR and Caldwell DG\n(2014) Covariance analysis as a measure of policy robustness.\nIn: OCEANS 2014 - TAIPEI. pp. 1–5.\nDOI:10.1109/\nOCEANS-TAIPEI.2014.6964339.\nJen E (2003) Stable or robust? what’s the difference? Complexity\n8(3): 12–18.\nKasimbeyli R, Ozturk ZK, Kasimbeyli N, Yalcin GD and\nIcmen B (2015) Conic scalarization method in multiobjective\noptimization and relations with other scalarization methods. In:\nLe Thi HA, Pham Dinh T and Nguyen NT (eds.) Modelling,\nComputation and Optimization in Information Systems and\nManagement Sciences. Cham: Springer International Publishing.\nISBN 978-3-319-18161-5, pp. 319–329.\nLittman ML (2001) Friend-or-foe q-learning in general-sum games.\nIn: ICML, volume 1. pp. 322–328.\nLiu C, Xu X and Hu D (2015) Multiobjective reinforcement learning:\nA comprehensive overview. IEEE Transactions on Systems,\nMan, and Cybernetics: Systems 45(3): 385–398.\nLizotte DJ, Bowling MH and Murphy SA (2010) Efficient\nreinforcement learning with multiple reward functions for\nrandomized controlled trial analysis. In: Proceedings of the\n27th International Conference on Machine Learning (ICML-10).\nCiteseer, pp. 695–702.\nMarchi E and Oviedo JA (1992) Lexicographic optimality in\nthe multiple objective linear programming: The nucleolar\nsolution.\nEuropean Journal of Operational Research\n57(3): 355 – 359. DOI:https://doi.org/10.1016/0377-2217(92)\n90347-C.\nURL http://www.sciencedirect.com/\nscience/article/pii/037722179290347C.\nMerrick KE and Maher ML (2009) Motivated reinforcement\nlearning: curious characters for multiuser games. Springer\nScience & Business Media.\nMnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare\nMG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G\net al. (2015) Human-level control through deep reinforcement\nlearning. Nature 518(7540): 529–533.\nMoffaert KV, Drugan MM and Now´e A (2013) Scalarized multi-\nobjective reinforcement learning: Novel design techniques. In:\n2013 IEEE Symposium on Adaptive Dynamic Programming\nand Reinforcement Learning (ADPRL). pp. 191–199. DOI:\n10.1109/ADPRL.2013.6615007.\nMossalam H, Assael YM, Roijers DM and Whiteson S (2016)\nMulti-objective deep reinforcement learning. arXiv preprint\narXiv:1610.02707 .\nMousseau V and Pirlot M (2015) Preference elicitation and learning.\nEURO Journal on Decision Processes 3(1): 1–3. DOI:10.1007/\ns40070-015-0044-2. URL https://doi.org/10.1007/\ns40070-015-0044-2.\nOgryczak W, Perny P and Weng P (2011) On minimizing\nordered weighted regrets in multiobjective markov decision\nprocesses. In: Brafman RI, Roberts FS and Tsouki`as A (eds.)\nAlgorithmic Decision Theory. Berlin, Heidelberg: Springer\nBerlin Heidelberg. ISBN 978-3-642-24873-3, pp. 190–204.\nPapadimitriou CH and Tsitsiklis JN (1987) The complexity of\nmarkov decision processes.\nMathematics of Operations\nResearch 12(3): 441–450. DOI:10.1287/moor.12.3.441. URL\nhttps://doi.org/10.1287/moor.12.3.441.\nPerny P and Weng P (2010) On finding compromise solutions in\nmultiobjective markov decision processes. In: Proceedings of\nthe 2010 Conference on ECAI 2010: 19th European Conference\non Artificial Intelligence. Amsterdam, The Netherlands, The\nNetherlands: IOS Press. ISBN 978-1-60750-605-8, pp. 969–\n970. URL http://dl.acm.org/citation.cfm?id=\n1860967.1861159.\nRoijers DM, Vamplew P, Whiteson S and Dazeley R (2013) A\nsurvey of multi-objective sequential decision-making. Journal\nof Artificial Intelligence Research .\nRoijers DM, Whiteson S and Oliehoek FA (2014) Linear support\nfor multi-objective coordination graphs. In: Proceedings of\nthe 2014 International Conference on Autonomous Agents and\nMulti-agent Systems, AAMAS ’14. Richland, SC: International\nFoundation for Autonomous Agents and Multiagent Systems.\nISBN 978-1-4503-2738-1, pp. 1297–1304. URL http://dl.\nacm.org/citation.cfm?id=2615731.2617454.\nRoijers DM, Whiteson S and Oliehoek FA (2015) Point-based\nplanning for multi-objective pomdps. In: IJCAI. pp. 1666–1672.\nSilver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driess-\nche G, Schrittwieser J, Antonoglou I, Panneershelvam V,\nLanctot M et al. (2016) Mastering the game of Go with deep\nneural networks and tree search. Nature 529(7587): 484–489.\nSutton RS and Barto AG (1998) Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge.\nSutton RS, McAllester DA, Singh SP and Mansour Y (2000) Policy\ngradient methods for reinforcement learning with function\napproximation. In: Advances in neural information processing\nsystems. pp. 1057–1063.\nSzepesv´ari C (2010) Algorithms for reinforcement learning.\nSynthesis lectures on artificial intelligence and machine learning\n4(1): 1–103.\nTsitsiklis JN (1994) Asynchronous stochastic approximation and\nq-learning.\nMachine Learning 16(3): 185–202.\nDOI:10.\n1007/BF00993306. URL https://doi.org/10.1007/\nBF00993306.\nVamplew P, Dazeley R, Berry A, Issabekov R and Dekker E (2011)\nEmpirical evaluation methods for multiobjective reinforcement\nlearning algorithms. Machine Learning 84(1): 51–80. DOI:\n10.1007/s10994-010-5232-5. URL http://dx.doi.org/\n10.1007/s10994-010-5232-5.\nWatkins CJCH and Dayan P (1992) Q-learning. Machine Learning\n8(3): 279–292.\nDOI:10.1007/BF00992698.\nURL http:\n//dx.doi.org/10.1007/BF00992698.\nPrepared using sagej.cls\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2023-08-18",
  "updated": "2023-08-18"
}