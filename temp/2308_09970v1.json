{
  "id": "http://arxiv.org/abs/2308.09970v1",
  "title": "Tackling Vision Language Tasks Through Learning Inner Monologues",
  "authors": [
    "Diji Yang",
    "Kezhen Chen",
    "Jinmeng Rao",
    "Xiaoyuan Guo",
    "Yawen Zhang",
    "Jie Yang",
    "Yi Zhang"
  ],
  "abstract": "Visual language tasks require AI models to comprehend and reason with both\nvisual and textual content. Driven by the power of Large Language Models\n(LLMs), two prominent methods have emerged: (1) the hybrid integration between\nLLMs and Vision-Language Models (VLMs), where visual inputs are firstly\nconverted into language descriptions by VLMs, serving as inputs for LLMs to\ngenerate final answer(s); (2) visual feature alignment in language space, where\nvisual inputs are encoded as embeddings and projected to LLMs' language space\nvia further supervised fine-tuning. The first approach provides light training\ncosts and interpretability but is hard to be optimized in an end-to-end\nfashion. The second approach presents decent performance, but feature alignment\nusually requires large amounts of training data and lacks interpretability. To\ntackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal\nOptimization (IMMO), to solve complex vision language problems by simulating\ninner monologue processes, a cognitive process in which an individual engages\nin silent verbal communication with themselves. We enable LLMs and VLMs to\ninteract through natural language conversation and propose to use a two-stage\ntraining process to learn how to do the inner monologue (self-asking questions\nand answering questions). IMMO is evaluated on two popular tasks and the\nresults suggest by emulating the cognitive phenomenon of internal dialogue, our\napproach can enhance reasoning and explanation abilities, contributing to the\nmore effective fusion of vision and language models. More importantly, instead\nof using predefined human-crafted monologues, IMMO learns this process within\nthe deep learning models, promising wider applicability to many different AI\nproblems beyond vision language tasks.",
  "text": "Tackling Vision Language Tasks Through Learning Inner Monologues\nDiji Yang1, Kezhen Chen2, Jinmeng Rao2,\nXiaoyuan Guo2, Yawen Zhang2, Jie Yang2, Yi Zhang1\n1University of California, Santa Cruz, 2Mineral\n{dyang39,yiz}@ucsc.edu\n{kezhenchen, jinmengrao, xiaoyuanguo, yawenz, yangjie}@mineral.ai\nAbstract\nVisual language tasks such as Visual Question Answering\n(VQA) or Visual Entailment (VE) require AI models to com-\nprehend and reason with both visual and textual content.\nDriven by the power of Large Language Models (LLMs),\ntwo prominent methods have emerged: (1) the hybrid integra-\ntion between LLMs and Vision-Language Models (VLMs),\nwhere visual inputs are firstly converted into language de-\nscriptions by VLMs, serving as inputs for LLMs to gener-\nate final answer(s); (2) visual feature alignment in language\nspace, where visual inputs are encoded as embeddings and\nprojected to LLMs’ language space via further supervised\nfine-tuning. The first approach provides light training costs\nand interpretability but is hard to be optimized in an end-\nto-end fashion. The second approach presents decent perfor-\nmance, but feature alignment usually requires large amounts\nof training data and lacks interpretability.\nTo tackle this dilemma, we propose a novel approach,\nInner Monologue Multi-Modal Optimization (IMMO), to\nsolve complex vision language problems by simulating inner\nmonologue processes, a cognitive process in which an indi-\nvidual engages in silent verbal communication with them-\nselves. More specifically, we enable LLMs and VLMs to\ninteract through natural language conversation (i.e., inner\nmonologue) and propose to use a two-stage training process\nto learn how to do inner monologue (self-asking questions\nand answering questions). IMMO is evaluated on two popu-\nlar tasks and achieves competitive results compared with hy-\nbrid integration approaches, while it uses significantly less\ntraining data and provides greater interpretability compared\nwith embedding alignment approaches. The results suggest\nby emulating the cognitive phenomenon of internal dialogue,\nour approach can enhance reasoning and explanation abili-\nties, contributing to the more effective fusion of vision and\nlanguage models. More importantly, instead of using prede-\nfined human-crafted monologues, IMMO learns this process\nwithin the deep learning models, promising wider applica-\nbility to many different AI problems beyond vision language\ntasks.\n1\nIntroduction\nEvidence shows that explicitly using natural language as\nthe intermediate representation of reasoning is effective and\nessential for human cognition (Goldin-Meadow and Gen-\ntner 2003; Forbus, Liang, and Rabkina 2017; Crouse, Mc-\nPreprint\nFigure 1: Examples of multi-model inner monologue.\nFate, and Forbus 2018; Chen et al. 2020; Lee et al. 2019;\nZhang et al. 2023). Recently, large language models (LLMs)\nhave achieved substantial advancements. Notable models\nlike PaLM (Chowdhery et al. 2022), InstructGPT (Ouyang\net al. 2022), and LLaMA (Touvron et al. 2023) showcase\ntheir immense potential in the field of natural language pro-\ncessing and commonsense reasoning. Many researchers ex-\nplore using natural language as the intermediate represen-\ntation to bridge multiple modalities. For instance, various\nmodalities, including the vision modality, can be first pro-\njected to the natural language space, and then LLMs are uti-\nlized to perform multi-modality understanding via language\nprocessing. Two research directions to add visual inputs into\nlanguage space have been actively studied recently. The first\ndirection is the hybrid integration between vision-language\nmodels (VLMs) and LLMs (Yang et al. 2022; Salaberria\net al. 2023; Zhu et al. 2023a). Hybrid integration approaches\naim to enable LLMs to utilize VLMs in a zero-shot or few-\nshot manner, i.e., LLMs act as a central reasoner or planner\nand VLMs serve as tools or sensors. These models do not\nrequire heavy training costs and provide interpretability as\nthe model outputs from LLMs and VLMs are transparent.\nHowever, as LLMs do not access the visual inputs directly,\nthey may miss some visual details in the images. Also, most\nhybrid integration approaches merge LLMs and VLMs in a\ndiscrete space, which are hard to be optimized in an end-to-\narXiv:2308.09970v1  [cs.CL]  19 Aug 2023\nend fashion. The second direction is embedding alignment\n(Dai et al. 2023; Chen et al. 2023; Li et al. 2023a; Liu et al.\n2023). Visual information is transformed into visual embed-\ndings, which are then mapped onto the language embedding\nspace and employed as input embeddings for LLMs. Then,\nthe LLMs are tuned via supervised fine-tuning to fuse vi-\nsion and language information together to achieve decent\nperformance. However, learning the cross-modality embed-\nding alignment heavily rely on the training data. To get\ndecent performance, users need to prepare a large number\nof high-quality data containing accurate domain knowledge\nand complex reasoning, which requires heavy engineering\nefforts and high curation costs. Furthermore, during the in-\nference time, the entire model remains a black box, making\ninterpretation difficult (Gao et al. 2022). The heavy costs as-\nsociated with collecting cross-modality data, combined with\nlow interpretability, limit the applicability of this approach\nin many domains in practice.\nTo address the challenges, we introduce a novel approach,\nInner Monologue Multi-Modal Optimization (IMMO), to\nsolve complex vision language problems by simulating the\ninner monologue process, a cognitive process in which an in-\ndividual engages in silent verbal communication with them-\nselves. When people solve complicated reasoning problems,\nthey tend to use “inner monologue” by performing reasoning\nvia multi-turn self-conversations in their minds. Inner mono-\nlogue helps people organize their thoughts and work through\nthe optimal answers as a form of problem-solving (Cherney\n2023; Huang et al. 2022). Inspired by this process, we enable\nLLMs and VLMs to interact through natural language con-\nversation and propose to use supervised learning and rein-\nforcement learning to learn how to perform the inner mono-\nlogue. We choose one LLM as the Reasoner and one VLM\nas the Observer. Given a visual input and a visual reasoning\nproblem, the Observer perceives the visual information and\nabstracts it to a natural-language description. The Reasoner\ndecides whether the description has sufficient information to\nsolve the problem or generates an advanced question for the\nObserver to acquire more visual information. With multi-\nple turns, the Reasoner organizes the information and works\nthrough an answer. Figure 1 shows two examples of solving\nvisual questions with the inner monologue. To automatically\nlearn the process of the inner monologue, the entire system\nis optimized by a policy gradient method named Proximal\nPolicy Optimization (PPO) (Schulman et al. 2017).\nIn summary, our contributions are as follows:\n• Inspired by human cognition, we propose a novel\napproach IMMO for vision-and-language reasoning.\nIMMO includes a Reasoner model and one or more Ob-\nserver model(s) to communicate with each other in natu-\nral language, significantly improving their ability to solve\ncomplex problems together. IMMO can be trained effi-\nciently and has interpretability. This inner monologue-\nbased approach is flexible and can be adapted to other\nmodalities or models.\n• We propose a two-stage training process to let ob-\nserver(s) and Reasoner learn how to work together: first\nusing annotated multi-turn reasoning data to train the ini-\ntial models, then adopting reinforcement learning to fur-\nther improve the models. We created a new human-like\nmulti-turn inner monologue reasoning training corpus by\naugmenting existing VQA data with GPT-3.5 . We find\nthis low-cost training corpus effective.\n• We evaluate IMMO on two vision-language reasoning\ntasks. Experiments show that IMMO can achieve com-\npetitive results compared with hybrid integration ap-\nproaches, while it uses significantly less training data and\nprovides greater interpretability compared with embed-\nding alignment approaches.\n2\nRelated Works\nThe success of large pre-trained language models (LLMs)\nhas led to significant advancements in solving vision-\nlanguage problems by fusing visual representations into\nthe language space. The essence of these works is to al-\nlow LLMs to understand information from other modali-\nties, using their rich pre-trained language knowledge and the\nemerging ability (Wei et al. 2022a). Several recent works\nhave explored two research directions, embedding align-\nment and hybrid integration. Approaches with embedding\nalignment focus on projecting visual embeddings to the lan-\nguage space and fusing vision and language information\nvia supervised fine-tuning in the language space (Dai et al.\n2023; Chen et al. 2023; Li et al. 2023a; Liu et al. 2023).\nThese models learn the embedding projection from percep-\ntion models to LLMs with a large amount of high-quality\nvision-language instruction-tuning data. Despite the impres-\nsive performance, these models demand extensive engineer-\ning efforts to collect the training data and struggle with in-\nterpretability, given the difficulty for human to understand\nlatent embeddings and comprehend the reasoning process\nof deep models. Our approach focuses on converting vi-\nsual inputs to language descriptions while keeping decent\nperformance, which provides more interpretability and re-\nduces training costs significantly. Approaches involving hy-\nbrid integration convert visual inputs to language descrip-\ntions, such as image captions, via VLMs and solve prob-\nlems with LLMs (Yang et al. 2022; Salaberria et al. 2023).\nHowever, this approach may lead to captions that are ir-\nrelevant to the question. To address this problem, several\nworks adapt interactive multi-turn conversations to promote\nVLMs and LLMs interacting with each other and acquir-\ning more information (You et al. 2023; Zhu et al. 2023a).\nDespite these works providing more interpretability and ac-\ncessibility, they are usually in zero-shot or few-shot settings\nand have significantly lower performance compared to the\nembedding-alignment-based approaches. Our approach in-\ntroduces a novel framework to optimize hybrid integration\nsystems, which gives a more decent performance while pre-\nserving interpretability.\nAnother branch of research related to our work is multi-\nagent learning. Researchers in multi-agent collaborations\nhave studied communication and dialog between models\n(Foerster et al. 2016). Such interactions enable models to ex-\nchange information, clarify doubts, negotiate strategies, and\ncollaborate towards a common goal. They can share experi-\nFigure 2: The IMMO framework automatically acquires inner monologue capabilities through reinforcement learning. In each\ntraining epoch, the Reasoner (LLM) and Observer (VLM) are alternately designated as the actively trainable model, highlighted\nwith a pink hue and a fire icon, while the other model assumes the role of a static environmental representation, distinguished\nby a light blue shade and a snow icon. During the (2k)-th epoch, the Reasoner functions as the fixed environmental model,\nwhile the Observer becomes the focus of updates. Subsequently, in the (2k + 1)-th epoch, the roles shift, with the Reasoner\nnow undergoing updates as the active model, and the Observer assuming the position of the static environmental representation.\nPPO policy gradients are used for iterative updates of the model parameters.\nences, strategies, and insights gained from different perspec-\ntives. Multi-Agent Reinforcement learning has been studied\nin various applications such as the games of Go, robotics,\nand autonomous driving. Depending on whether the agents\nare fully collaborative, fully competitive, or a mix of the two,\none of the two approaches, Markov/stochastic games, and\nextensive-form games, are usually used (Zhang, Yang, and\nBas¸ar 2021). The RL method we proposed is motivated by\nthese earlier works, while our study focuses on agents that\ncan output natural language and facilitates model-to-model\ncommunication using natural language with the help of the\nrecent advance in large-scale language models.\n3\nApproach\nAn overview of the IMMO framework for solving com-\nplex vision and language problems is shown in Figure 2.\nOur framework contains two components: Reasoner and Ob-\nserver. The “inner monologue” process in the IMMO is de-\nscribed as follows.\nThe Observer takes images as the inputs and generates\ntextual descriptions to describe the key information it ob-\nserves. The Reasoner takes the generated textual descrip-\ntions and performs reasoning by either generating a new\nquery for the Observer or generating the final results of\nthe task. We choose an LLM as our Reasoner model and\na VLM as our Observer model. The objective of the Rea-\nsoner is to generate effective queries to obtain targeted in-\nformation and the objective of the Observer is to provide\ncorrect information based on the queries from the Reasoner.\nWith multi-turn querying-answering conversations between\nthe Reasoner and the Observer, the Reasoner gathers infor-\nmation to address the vision and language problems. Mean-\nwhile, the Observer receives the queries and perceives more\nvisual details from the image.\nIn this section, we start by presenting the IMMO frame-\nwork and then introduce the two-stage training approach for\nthe IMMO framework.\n3.1\nInner Monologue Multi-Modal Optimization\nIn the IMMO framework, the Reasoner and the Observer\nwork together to solve a problem. Initially, the Observer re-\nceives the Image I and generates a caption C to describe the\nbasic visual information in the image. A text container IM\nwill be used here to track the inner monologue, including the\ninitial caption C:\nIM0 = C = Observer(I)\n(1)\nAt the intermediate i-th turn (i ∈[1, t] where t is the\npredefined maximum conversation turn), the Reasoner and\nthe Observer will interact. Firstly, the Reasoner receives the\noriginal textual description of the problem/task P combined\nwith IMi−1 and generates a query Qi. Then, given Qi, the\nObserver will provide the answer Ai based on the image.\nThis process is shown as the equations 2, and 3.\nQi = Reasoner(P, IMi−1)\n(2)\nAi = Observer(I, Qi)\n(3)\nAt the end of each turn, both question and answer that are\ngenerated within the current turn will be added to the inner\nmonologue history. The IMi is defined as:\nIMi = C +\ni\nX\nj=0\n(Qj + Aj)\n(4)\nAfter the final conversation turn t, the Reasoner will pro-\nvide its prediction Af for the original problem P based on\nall the collected inner monologue:\nAf = Reasonser(P + IMt)\n(5)\nDuring the multi-turn iteration, the Observer only ac-\ncesses the input image and the most recent information\nquery generated by the Reasoner, while the Reasoner can\naccess the complete QA history at any given timestamp\nthrough the input prompt. Once the interaction reaches a pre-\ndefined number of turns, the system prompts LLM for the fi-\nnal prediction. Next, we describe how IMMO optimizes the\nReasoner and the Observer jointly.\n3.2\nTwo-Stage Training\nAlthough this multi-turn conversational framework can be\nused in a zero-shot manner through prompting, the col-\nlaboration between the Reasoner and the Observer is sub-\noptimal. The collaboration between the Reasoner and Ob-\nserver should be further improved. For example, the Rea-\nsoner needs to be familiar with the Observer’s capability\nin order to generate appropriate queries that the Observer\ncan answer. The Observer, on the other hand, should be op-\ntimized to extract correct visual information based on the\nqueries from the Reasoner.\nTo alleviate the aforementioned underperformed collab-\noration problem, IMMO uses a two-stage training process.\nFirst, high-quality inner monologue conversational data is\nused for supervised fine-tuning (SL) of both the Reasoner\nand the Observer. Second, reinforcement learning (RL) is\nused for further optimization. Figure 2 shows the overview\nof the IMMO framework, and only the system-level rein-\nforcement learning is illustrated.\nSupervised Human-prior Fine-tuning\nTo provide the\nReasoner and the Observer a better starting point for re-\ninforcement learning in the next stage, we employ super-\nvised fine-tuning, similar to the approach used in Instruct-\nGPT (Ouyang et al. 2022; Cruz Jr, Du, and Taylor 2017).\nOur training process focuses on imparting effective inner\nmonologue to the model, going beyond simple chit-chat or\nprompt-based zero-shot learning. To achieve this, we en-\nhance our pre-trained language model by introducing hu-\nman prior knowledge and reasoning patterns with super-\nvised fine-tuning. We utilize high-quality multi-turn conver-\nsational question-answering pairs annotated by humans as\nour training data. Both the Reasoner and the Observer are\ntrained on the human-annotated data as a warm-up process.\nIn order to impart human reasoning patterns to the lan-\nguage model, we employ the instruction fine-tuning method\n(Chung et al. 2022) in this training step. Inspired by Chains-\nof-Thought prompting (Wei et al. 2022b) in the reasoning\ntask, this stage trains the model to mimic the correct think-\ning path. Instead of directly answering the basic question,\nthe correct pattern involves reasoning through multiple turns\nof QA pairs and then arriving at the final answer.\nAlgorithm 1: IMMO Reinforcement Learning\nDataset: (Problem P, Image I, Ground Truth G) tuples\nReasoner: a pre-trained large language model\nObserver: a pre-trained vision-language model\nN: training epoch\nt: pre-defined max turns\nk: any none-negative integer\n1: for epoch = 1 to N do\n2:\nSet Reasoner as the active model M\n3:\nSet Observer as the environment model E\n4:\nif epoch = 2k then\n5:\nSet Observer as the active model M\n6:\nSet Reasoner as the environment model E\n7:\nend if\n8:\nSample (P, I, G) from the dataset\n9:\nC ←Observer(I)\n10:\nSet IM0 = C\n11:\nfor i = 1 to t do\n12:\nQi ←Reasoner(P, IMi−1)\n13:\nAi ←Observer(I, Qi)\n14:\nIMi = IMi−1 + Qi + Ai\n15:\nend for\n16:\nAf = Reasoner(P, IMt)\n17:\nReward ←R {Eq. 7}\n18:\nUpdate M using PPO\n19: end for\nReinforcement Learning\nIMMO uses a special alterna-\ntive training process for system-level reinforcement learn-\ning to jointly optimize multiple models while taking into ac-\ncount the dynamic interactions between models. Since the\nsystem involves two models, we use the alternating training\nstrategy to prevent issues that may arise from updating two\nmodels simultaneously, such as the imbalance of capabil-\nities of the Reasoner and the Observer (Goodfellow et al.\n2014). Specifically, at the 2k-th epoch where k is a non-\nnegative integer, we set the Observer as the active model\n(policy network) and the Reasoner as the environment model\nto provide feedback; at the 2k + 1-th epoch, we switch the\nReasoner to be active and change the Observer as the envi-\nronment model. During training, we only update the active\nmodel. Following the common approaches used in previous\nworks (Stiennon et al. 2020; Ziegler et al. 2019; Ouyang\net al. 2022; von Werra et al. 2020) for fine-tuning auto-\nregressive decoder-only generative model, we treat the ac-\ntive model as the policy networks. The active model is up-\ndated by PPO (Schulman et al. 2017) and the environment\nmodel remains frozen. Notably, the active model and the en-\nvironment model only affect which model will be updated,\nand the input/output of each model strictly follows the multi-\nturn framework as shown in Figure 2. The algorithm uses\nthe exact matching between the predicted answer Af and\nthe ground-truth answer G as the major reward factor:\nr(Af, G) =\n\u001a\n1\nif\nAf = G\n0\notherwise\n(6)\nThe final reward R shown in equation 7 also includes a\nKL penalty (Jaques et al. 2017) weighted by β to ensure that\nthe updated model M does not deviate too far from the well-\ntrained starting point M0 (Ziegler et al. 2019).\nR = r(Af, G) + βKL(M, M0)\n(7)\nThe training goal is to optimize the policy that maximizes\nthe expected reward. The overall training procedure is shown\nin Algorithm 1.\n4\nExperiment\nTo evaluate the effectiveness of IMMO for complex vision-\nlanguage reasoning, we conducted experiments on two\npopular tasks: Commonsense Visual Question Answering\n(VQA) and Visual Entailment (VE). Both tasks require mod-\nels to have commonsense knowledge and reasoning abilities.\nThis section first describes our implementation of the IMMO\nframework, then presents the details of these two tasks.\n4.1\nData and Implementation\nWe construct a new training corpus for supervised human-\nprior fine-tuning by utilizing the A-OKVQA (Schwenk et al.\n2022) dataset, which includes human-annotated reasoning\npaths labeled as rationale. We derive inner monologue from\nrationale. As demostrate in the Figure 3, by prompting GPT-\n3.5 in a zero-shot manner, we transform rationale into two-\nturn question-answering pairs. The results are then com-\nbined with 17k single-turn VQA samples. Each sample in\nthe training corpus contains a question, a choice list, two\nrounds of QA conversations, and the correct answer. At\nthe supervised fine-tuning stage, we optimize the autore-\ngressive LLM by performing the next token prediction task\nover this augmented corpus. At the reinforcement learning\nstage, the training is mainly based on the Transformers-\nReinforcement-Learning (TRL) solution (von Werra et al.\n2020) to wrap up the Hugging Face trainer\n(Wolf et al.\n2020). For different tasks (VQA or VE), reinforcement\nlearning is performed on task-specific training sets.\nOur proposed system uses the Vicuna-7b (Chiang et al.\n2023) language model and BLIP-2 (Li et al. 2023b) vision-\nlanguage model. To ensure computational efficiency, we em-\nployed the Low-rank adaptation (Lora) (Hu et al. 2021)\nto train only 0.06% of the Vicuna-7b model, which corre-\nsponds to 5 million parameters. Our experiments primarily\nfocus on the validation of the methodology. For broader ap-\nplicability, we chose a model that can be trained on a single\nNVIDIA A100-40G GPU or equivalent, instead of a more\npowerful but larger model. Task-specific prompts for both\nLLM and VLM were designed manually, inspired by prompt\ntemplates used by You et al. (2023); Liu et al. (2023).\nFigure 3: Example of converting human written declarative\nrationale to dialogue form reasoning path.\n4.2\nCommonsense Visual Question Answering\nWe conduct our experiments on the ScienceQA (SQA) (Lu\net al. 2022) dataset, which is a standard benchmark for com-\nmonsense visual question answering. It consists of 21k QA\npairs collected from elementary and high school courses,\nwith 48.7% of the questions including images, making it a\nVQA task. Success on this dataset requires appropriate com-\nmonsense knowledge as well as reasoning skills. We follow\nthe official train/validation/test split for all our experiments.\nBaselines\nTo study the impact of RL optimization and in-\nner monologue on model performance, we conduct experi-\nments with 3 baselines. As demonstrated in Table 1, different\ntraining methods and reasoning aids were examined, while\nwe use Vicuna-7B as LLM and BLIP-2 as captioning model\nin all cases. The first baseline is PICa (Yang et al. 2022),\nwhich first proposed LLM plus image captions for Knowl-\nedge Base VQA. For a fair comparison, instead of using the\ndefault GPT-3 as LLM in PICa, we let PICa use Vicuna as its\nLLM. The second baseline is Vicuna-16-shots, which incor-\nporates the Chain-of-Thoughts (CoT) prompting (Wei et al.\n2022b) with the Vicuna model. The third baseline is Vicuna-\nSL, which is LLM fined tuned with the training data follow-\ning instruction fine-tuning (Chung et al. 2022; Taori et al.\n2023). We also compare two training methods for IMMO:\nfew-shot learning (IMMO 16-shots) vs. two-stage training\ndescribed in Section 3.2 (IMMO SL+RL).\nResults\nFollowing the baseline setting, table 1 presents\nour results on the SQA test set. PICa, as a zero-shot base-\nline, achieves a modest accuracy of 54.3%. By using 16 in-\ncontext examples and CoT, Vicuna-16-shots attain an accu-\nracy of 68.6%. With the same LLM model and in-context ex-\namples, enabling inner monologue with only few-shot learn-\ning (IMMO-16-shots) improves the performance by 5.4%\n(from 68.6% to 74%). With the two-stage training, IMMO\nfurther improves and achieves 84.8% accuracy, which is\n6.5% over Vicuna-SL. Our experiment results highlight the\nrole of inner monologue in facilitating reasoning, while also\ndemonstrating how RL training can enhance the overall sys-\ntem’s capabilities.\n1All the experiments in the table are named after the methods,\nand the language models involved are all Vicuna-7B.\nMethod 1 Training\nReasoning Aids\nAccuracy(%)\nPICa\nZero-shot None\n54.3\nVicuna\n16-shots\nChain-of-Thought\n68.6\nIMMO\n16-shots\nInner-Monologue\n74.0\nVicuna\nSL\nChain-of-Thought\n78.3\nIMMO\nSL+RL\nInner-Monologue\n84.8\nTable 1: Results on ScienceQA.\nMethod\nAccuracy(%)\nEmb\nMiniGPT4 (Zhu et al. 2023b)\n35.1\nLLaVA (ZS) (Liu et al. 2023)\n40.3\nOFA (Wang et al. 2022)\n91.0\nNL\nIdealGPT (You et al. 2023)\n55.3\nVicuna-16-shots\n49.8\nVicuna-SL\n59.8\nIMMO\n65.7\nTable 2: Results on SNLI-VE. The Emb group includes em-\nbedding alignment approaches, while the NL group includes\nmethods using text to represent visual information.\n4.3\nVisual Entailment\nSNLI-VE (Xie et al. 2018) is a widely-used VE task built on\ntop of Stanford Natural Language Inference (SNLI) (Bow-\nman et al. 2015) and Flicker30k (Plummer et al. 2015) image\ndatasets. This task is designed as a classification problem for\nvision-language reasoning: identify whether the relationship\nbetween the given image premise and text hypothesis is en-\ntailment, neural, or contradiction.\nResult\nTable 2 shows the results on the SNLI-VE dev set.\nWe add another baseline: IdealGPT (You et al. 2023), a re-\ncent hybrid integration approach that utilizes the rich reason-\ning knowledge of GPT-3.5-175B. Among approaches that\nuse text to represent visual information, IMMO achieves\nthe best performance. With a much smaller LLM, our best-\nperforming checkpoints trained from Vicuna-7B achieved a\n10.4% improvement over IdealGPT (65.7% vs 55.3%).\nThe results of 3 embedding-based methods (MiniGPT4\n(Zhu et al. 2023b), LLaVA (Liu et al. 2023), and OFA (Wang\net al. 2022)) are also reported as a reference. Well-tuned\nembedding-based methods such as OFA work extremely\nwell on this dataset, illustrating the power of using a sin-\ngle model for end-to-end optimization. These results sug-\ngest that for tasks like SNLI-VE where natural language\nis not enough to describe necessary visual information, ap-\nproaches that convert images into text for LLM reason-\ning are sub-optimal. However, in real-world practice, many\ncompanies choose not to perform single-model-based end-\nto-end (embedding) optimization because interpretability is\nrequired or the training cost is too high. In such situations,\nhybrid integration like IMMO could be a good choice.\n5\nAblation Studies and Analysis\n5.1\nRepresentative cases\nFigure 4 displays instances of successful outcomes (a, b, and\nc) as well as an unsuccessful case (d) depicting the inter-\npretable inner monologues generated by IMMO. Example\n(b) shows LLM’s ability to compensate for VLM inaccura-\ncies (incorrect A2) through reasoning and available informa-\ntion. Moreover, the questioning path in (b) and (c) demon-\nstrate LLM’s vigilance in monitoring VLM responses, per-\nsisting in using subsequent questions Q2 to validate infor-\nmation after Q1, even when the information is enough to\nanswer the main question. On the other side, example (d)\nexposes how the VLM’s limited geographical background\nknowledge hinders LLM from arriving at an accurate an-\nswer. However, the erroneous visual information from VLM\nmisleads the LLM into an incorrect final prediction. These\nexamples illustrate the interpretability of IMMO.\n5.2\nComparison with additional VQA methods\nWe did some further analysis to compare representative so-\nlutions on the ScienceQA task (Table 3). The embedding\nalignment approach, LLaVA (Liu et al. 2023), performs\nwell; however, it requires extensive training data and lacks\ninterpretability. Hybrid integration such as IMMO, Chama-\nleon\n(Lu et al. 2023) and UnifiedQA (Khashabi et al.\n2020; Lu et al. 2022) employ modular architecture, enabling\nmodel-wise interpretability by access to individual outputs\nfrom sub-modules. Chameleon is based on GPT-4, which is\nnot publicly available, and poses constraints on its adop-\ntion and further fine-tuning. Compared with Chamaleon,\nIMMO achieved comparable performance with a signifi-\ncantly smaller model. UnifiedQA uses supervised training\nakin to our Vicuna-SL baseline, however, it falls short due to\nthe lack of system-wide optimization and information loss\nwhen converting images to captions. Compared with Uni-\nfiedQA, IMMO addresses these problems via inner mono-\nlogue and two-stage training, which significantly improves\nthe performance of the hybrid integration method. Further-\nmore, compared to Chamaleon and UnifiedQA, which offers\nsimple model-level interpretability, IMMO’s entire com-\nplex multi-round reasoning procedure is also transparent and\nhuman-readable.\nNotably, we can not rule out the possibility that black-box\nmodels like GPT-4 might have inadvertently or intention-\nally undergone training using publicly accessible test data.\nThus we list the performance of those black-box models here\nfor reference instead of as a baseline for a fair comparison.\nWe expect the proposed approach could be applied to other\nLLMs and VLMs such as GPT-4 and further improve their\nperformance.\n5.3\nThe Impact of Conversation Turns\nTo examine the impact of inner monologue turns on perfor-\nmance, we conduct ablation tests on ScienceQA using both\nfew-shot and trained approaches. Maintaining constant hy-\nperparameters, we evaluate turns ranging from 0 to 5, where\n0 means VLM only provides an initial caption. As shown in\nFigure 5, accuracy notably rises on the SQA test set from 0\n(d) Which country is highlighted?\nImage: a map a green triangle on it.\nChoices: [\"the Federated States of \nMicronesia\", \"Nauru\", \"the Marshall \nIslands\", \"Kiribati\"]\nQ1:  Is the Marshall Islands or Nauru \nhighlighted, or neither?\nA1: Nauru.\nQ2: Is the highlighted area a single island? \nA2: Yes.\nPredict: Nauru\nGround-truth answer: the Marshall \nIslands\n(b) In this food web, which organism \ncontains matter that eventually moves \nto the mushroom? The arrows in a food \nweb represent how matter moves \nbetween organisms in an ecosystem.\nImage: A map with various animals\nChoices: [\"grizzly bear\", \"parasitic jaeger\"]\nQ1: Is there any arrow between grizzly \nbear and mushroom?\nA1: Yes.\nQ2: Any other entities connected to the \nmushroom?\nA2: Lichen is connected with the \nmushroom.\nPredict: grizzly bear\nGround-truth answer: grizzly bear\n(a) Which property do these four \nobjects have in common?\nImage: Colorful toys on top of each other.\nChoices: [\"sticky\", \"slippery\", \"sour\"]\nQ1: How many toys?\nA1: 4.\nQ2: What is the name of each toy?\nA2: Honey, wet glue, ice pop, tape.\nPredict: sticky\nGround-truth answer: sticky\n(c) Hypothesis: dogs racing near \nracetrack.\nImage: Dog running by the fence.\nChoices: [\"Entailment\", \"Neutral\", \n\"Contradiction\"]\nQ1: Are all the dogs running in the same \ndirection? Yes or no?\nA1: Yes.\nQ2: Are the dogs wearing the racing vest \nwith a number or logo on it?\nA2: Yes.\nPredict: Entailment\nGround-truth answer: Entailment\nFigure 4: Success and failure examples of IMMO.\nLLaVa Chamaleon UnifiedQA IMMO\nInterpretable\n✗\n✓\n✓\n✓\nTrainable\n✓\n✗\n✓\n✓\nModel size\n13B\nGPT-4\n223M\n9B\nTuned param\n13B\n0\n223M\n5M\nData usage\n770K\n-\n17K\n25K\nSQA\n90.9\n86.5\n74.1\n84.8\nTable 3: Comparison with other ScienceQA approaches.\nto 2 turns, plateauing thereafter. Our analysis identifies SQA\nquestions as demanding less multi-hop reasoning than back-\nground knowledge. Thus, LLM’s primary learned strategy\ninvolves querying key facts initially, followed by confirma-\ntion or asking for side information. Also, more conversa-\ntions bring uncertainty to the interaction as LLM may ask\nless relevant questions after 3 turns or VLM brings incorrect\nvisual information. This trend is accentuated under the few-\nshot setting. Without training, LLM appears to be less robust\nto noise conversation, so the performance rapidly decreases\nafter 2 turns. It’s important to note that these findings are\nspecific to ScienceQA question patterns, underscoring the\nbest inner monologue turns are highly based on the dataset’s\ncharacteristics.\n6\nConclusion and Future Work\nInspired by cognitive modeling, we apply inner monologue,\na commonly seen human reasoning process, in the interac-\ntion between LLM and VLM. We proposed to learn which\nquestions to ask and how to answer questions during the\nmulti-round monologue using a two-stage training frame-\nwork together with a newly constructed training corpus from\nexisting VQA datasets. Our experiments demonstrated the\nability to learn how to do inner monologues, as well as\nthe effectiveness of acquiring information and reasoning\nTurns of Inner Monologue\nAccuracy (%)\n55\n60\n65\n70\n75\n80\n85\n90\n0\n1\n2\n3\n4\n5\nTrained\n16-shots\nFigure 5: Ablation study on different inner monologue turns\nusing on ScienceQA test set under few-shot and trained\nmanner.\nthrough inner monologues.\nThis paper is a first step towards this research direction,\nand there is much room for future improvement. Our cur-\nrent implementation promotes the reasoner querying certain\nturns, while an ideal reasoner should autonomously deter-\nmine whether to continue querying or end the inner mono-\nlogue with direct answers (for example, when adequate in-\nformation has been gathered or due to time/resource con-\nstraints). Our implementation only includes one observer,\nwhile it’s possible to include more observers with different\nmodalities or functionalities. Due to the resource limits, we\nused a synthetic way to generate supervised training data,\nwhile organizations with ample resources could hire hu-\nman annotators could provide more labeled data with higher\nquality. The reward function could also be further studied.\nMore importantly, our proposed approach is an automated\nway of generating intermediate steps Chain-of-Thought, and\nwe expect the concept of inner monologue can be widely ap-\nplied to a variety of use cases.\nReferences\nBowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.\n2015. A large annotated corpus for learning natural language\ninference. arXiv preprint arXiv:1508.05326.\nChen, F.; Han, M.; Zhao, H.; Zhang, Q.; Shi, J.; Xu, S.; and\nXu, B. 2023. X-LLM: Bootstrapping Advanced Large Lan-\nguage Models by Treating Multi-Modalities as Foreign Lan-\nguages. arXiv preprint arXiv:2305.04160.\nChen, K.; Huang, Q.; Palangi, H.; Smolensky, P.; Forbus,\nK.; and Gao, J. 2020. Mapping Natural-language Problems\nto Formal-language Solutions Using Structured Neural Rep-\nresentations. ICML.\nCherney, K. 2023. Everything to Know About Your Internal\nMonologue. Healthline.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; et al. 2022.\nPaLM: Scal-\ning Language Modeling with Pathways.\narXiv preprint\narXiv:2204.02311.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nCrouse, M.; McFate, C.; and Forbus, K. D. 2018. Learning\nfrom Unannotated QA Pairs to Analogically Disanbiguate\nand Answer Questions. Thirty-Second AAAI Conference.\nCruz Jr, G. V.; Du, Y.; and Taylor, M. E. 2017. Pre-training\nneural networks with human demonstrations for deep rein-\nforcement learning. arXiv preprint arXiv:1709.04083.\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang,\nW.; Li, B.; Fung, P.; and Hoi, S. 2023.\nInstructblip: To-\nwards general-purpose vision-language models with instruc-\ntion tuning. arXiv preprint arXiv:2305.06500.\nFoerster, J. N.; Assael, Y. M.; de Freitas, N.; and Whiteson,\nS. 2016. Learning to Communicate with Deep Multi-Agent\nReinforcement Learning. arXiv:1605.06676.\nForbus, K.; Liang, C.; and Rabkina, I. 2017. Representa-\ntion and Computation in Cognitive Models. Top Cognitive\nSystem.\nGao, F.; Ping, Q.; Thattai, G.; Reganti, A.; Wu, Y. N.; and\nNatarajan, P. 2022. Transform-Retrieve-Generate: Natural\nLanguage-Centric Outside-Knowledge Visual Question An-\nswering. In 2022 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 5057–5067.\nGoldin-Meadow, S.; and Gentner, D. 2003.\nLanguage in\nmind: Advances in the study of language and thought. MIT\nPress.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.\n2014. Generative adversarial nets. Advances in neural in-\nformation processing systems, 27.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\nof large language models. arXiv preprint arXiv:2106.09685.\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence,\nP.; et al. 2022.\nInner Monologue: Embodied Reasoning\nthrough Planning with Language Models.\narXiv preprint\narXiv:2207.05608.\nJaques, N.; Gu, S.; Bahdanau, D.; Hern´andez-Lobato, J. M.;\nTurner, R. E.; and Eck, D. 2017. Sequence tutor: Conser-\nvative fine-tuning of sequence generation models with kl-\ncontrol. In International Conference on Machine Learning,\n1645–1654. PMLR.\nKhashabi, D.; Min, S.; Khot, T.; Sabharwal, A.; Tafjord,\nO.; Clark, P.; and Hajishirzi, H. 2020. Unifiedqa: Crossing\nformat boundaries with a single qa system. arXiv preprint\narXiv:2005.00700.\nLee, K.; Palangi, H.; Chen, X.; Hu, H.; and Gao, J. 2019.\nLearning Visual Relation Priors for Image-Text Matching\nand Image Captioning with Neural Scene Graph Generators.\narXiv preprint arXiv:1909.09953.\nLi, B.; Zhang, Y.; Chen, L.; Wang, J.; Yang, J.; and Liu, Z.\n2023a. Otter: A multi-modal model with in-context instruc-\ntion tuning. arXiv preprint arXiv:2305.03726.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b.\nBlip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models.\narXiv preprint\narXiv:2301.12597.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual instruc-\ntion tuning. arXiv preprint arXiv:2304.08485.\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-\nC.; Tafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. Advances in Neural Information\nProcessing Systems, 35: 2507–2521.\nLu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu,\nY. N.; Zhu, S.-C.; and Gao, J. 2023. Chameleon: Plug-and-\nplay compositional reasoning with large language models.\narXiv preprint arXiv:2304.09842.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPlummer, B. A.; Wang, L.; Cervantes, C. M.; Caicedo, J. C.;\nHockenmaier, J.; and Lazebnik, S. 2015.\nFlickr30k enti-\nties: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE in-\nternational conference on computer vision, 2641–2649.\nSalaberria, A.; Azkune, G.; de Lacalle, O. L.; Soroa, A.; and\nAgirre, E. 2023. Image captioning for effective use of lan-\nguage models in knowledge-based visual question answer-\ning. Expert Systems with Applications, 212: 118669.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nSchwenk, D.; Khandelwal, A.; Clark, C.; Marino, K.; and\nMottaghi, R. 2022.\nA-okvqa: A benchmark for visual\nquestion answering using world knowledge. In Computer\nVision–ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part VIII, 146–\n162. Springer.\nStiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.;\nVoss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.\n2020.\nLearning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:\n3008–3021.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model.\nhttps:\n//github.com/tatsu-lab/stanford alpaca.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nvon Werra, L.; Belkada, Y.; Tunstall, L.; Beeching, E.;\nThrush, T.; and Lambert, N. 2020. TRL: Transformer Re-\ninforcement Learning. https://github.com/lvwerra/trl.\nWang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma, J.;\nZhou, C.; Zhou, J.; and Yang, H. 2022. Ofa: Unifying archi-\ntectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. In International Conference\non Machine Learning, 23318–23340. PMLR.\nWei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler,\nD.; et al. 2022a. Emergent abilities of large language mod-\nels. arXiv preprint arXiv:2206.07682.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022b. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint\narXiv:2201.11903.\nWolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.;\nXu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and\nRush, A. M. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning: System Demonstrations, 38–45. Online: Association for\nComputational Linguistics.\nXie, N.; Lai, F.; Doran, D.; and Kadav, A. 2018. Visual en-\ntailment task for visually-grounded language learning. arXiv\npreprint arXiv:1811.10582.\nYang, Z.; Gan, Z.; Wang, J.; Hu, X.; Lu, Y.; Liu, Z.; and\nWang, L. 2022. An empirical study of gpt-3 for few-shot\nknowledge-based vqa. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 36:3, 3081–3089.\nYou, H.; Sun, R.; Wang, Z.; Chen, L.; Wang, G.; Ayyubi,\nH.; Chang, K.-W.; and Chang, S.-F. 2023. IdealGPT: Iter-\natively Decomposing Vision and Language Reasoning via\nLarge Language Models. arXiv preprint arXiv:2305.14985.\nZhang, K.; Yang, Z.; and Bas¸ar, T. 2021. Multi-agent re-\ninforcement learning: A selective overview of theories and\nalgorithms. Handbook of reinforcement learning and con-\ntrol, 321–384.\nZhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and\nSmola, A. 2023. Multimodal chain-of-thought reasoning in\nlanguage models. arXiv preprint arXiv:2302.00923.\nZhu, D.; Chen, J.; Haydarov, K.; Shen, X.; Zhang, W.; and\nElhoseiny, M. 2023a. ChatGPT Asks, BLIP-2 Answers: Au-\ntomatic Questioning Towards Enriched Visual Descriptions.\narXiv preprint arXiv:2303.06594.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023b. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\nZiegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,\nA.; Amodei, D.; Christiano, P.; and Irving, G. 2019. Fine-\ntuning language models from human preferences.\narXiv\npreprint arXiv:1909.08593.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-08-19",
  "updated": "2023-08-19"
}