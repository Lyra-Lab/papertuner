{
  "id": "http://arxiv.org/abs/2308.11027v1",
  "title": "Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics",
  "authors": [
    "Zhuohang Li",
    "Chao Yan",
    "Xinmeng Zhang",
    "Gharib Gharibi",
    "Zhijun Yin",
    "Xiaoqian Jiang",
    "Bradley A. Malin"
  ],
  "abstract": "Deep learning continues to rapidly evolve and is now demonstrating remarkable\npotential for numerous medical prediction tasks. However, realizing deep\nlearning models that generalize across healthcare organizations is challenging.\nThis is due, in part, to the inherent siloed nature of these organizations and\npatient privacy requirements. To address this problem, we illustrate how split\nlearning can enable collaborative training of deep learning models across\ndisparate and privately maintained health datasets, while keeping the original\nrecords and model parameters private. We introduce a new privacy-preserving\ndistributed learning framework that offers a higher level of privacy compared\nto conventional federated learning. We use several biomedical imaging and\nelectronic health record (EHR) datasets to show that deep learning models\ntrained via split learning can achieve highly similar performance to their\ncentralized and federated counterparts while greatly improving computational\nefficiency and reducing privacy risks.",
  "text": "Split Learning for Distributed Collaborative\nTraining of Deep Learning Models in Health Informatics\nZhuohang Li, MS1, Chao Yan, PhD2, Xinmeng Zhang, BS1, Gharib Gharibi, PhD3,\nZhijun Yin, PhD1,2, Xiaoqian Jiang, PhD4, Bradley A. Malin, PhD1,2\n1Vanderbilt University, Nashville, TN;2Vanderbilt University Medical Center, Nashville, TN;\n3TripleBlind, Kansas City, MO;4UTHealth, Houston, TX\nAbstract\nDeep learning continues to rapidly evolve and is now demonstrating remarkable potential for numerous medical pre-\ndiction tasks. However, realizing deep learning models that generalize across healthcare organizations is challenging.\nThis is due, in part, to the inherent siloed nature of these organizations and patient privacy requirements. To address\nthis problem, we illustrate how split learning can enable collaborative training of deep learning models across dis-\nparate and privately maintained health datasets, while keeping the original records and model parameters private. We\nintroduce a new privacy-preserving distributed learning framework that offers a higher level of privacy compared to\nconventional federated learning. We use several biomedical imaging and electronic health record (EHR) datasets to\nshow that deep learning models trained via split learning can achieve highly similar performance to their centralized\nand federated counterparts while greatly improving computational efficiency and reducing privacy risks.\nIntroduction\nRecent advances in deep learning algorithms have enabled the development of neural networks with promising per-\nformance for a variety of healthcare data types that were previously considered challenging with traditional statistical\nmethods, including medical images [1], natural languages [2], and structured electronic health records (EHR) [3].\nDespite the remarkable progress that has been achieved, most current deep learning models in the healthcare domain\nare developed using data from only a single site. Yet it is evident that no single healthcare organization can collect\na sufficient amount of data on diverse populations, as well as variations in organizational practices, to represent the\ndistribution of the general patient population. Consequently, models developed from single-site data oftentimes lack\nsufficient generalizability and may perform poorly when applied to other sites [4]. To resolve this problem, one nat-\nural solution would be to enable disparate healthcare organizations to collaboratively train a model. However, such\ncollaborations have met numerous obstacles, ranging from concerns over data rights to patient privacy.\nOver the past several years, distributed learning has been investigated as a strategy to enable multiple data holders\nto contribute to the development of deep learning models while protecting the privacy of the underlying raw records.\nSpecifically, one of the most popular variants of distributed learning is federated learning (FL) [5, 6], which enables\ndata holders to contribute to the training of a learning model under the orchestration of a central server by exchanging\nonly focused model updates while maintaining private data locally. The ability to maintain the privacy of record-\nlevel data has drawn particular research interest from the healthcare community [7, 8]. Still, the canonical form of\nfederated learning requires model builders to reveal the details about their local models (i.e., model architecture and\nmodel parameters) and such information can be leveraged to make inferences about the privately maintained local data\nrecords. As a result, in untrustworthy environments, federated learning needs to be jointly implemented in concert\nwith additional mechanisms to enhance its privacy support. There are various approaches for doing so; some of\nthe popular methods include 1) differential privacy (DP) [9, 10] which leverages a randomization mechanism (e.g.,\nadditive Laplacian noise) to provide privacy guarantees for individual records for algorithms on aggregate databases,\n2) secure multiparty computation (SMC) [11, 12] a cryptographic solution to enable a set of parties to compute a joint\nfunction on their private data without revealing anything but the prescribed output and 3) homomorphic encryption\n(HE) [13, 14], which allows a party to compute certain mathematical operations on ciphertexts without decrypting\nthem. However, in practice, these additional privacy protection measures often come at the cost of either significantly\nharming model utility (e.g., predictive performance) or increasing computational complexity, which leads to long\nruntimes and costly computation in cloud computing environments.\nIn this paper, we investigate split learning (SL) as a new paradigm for multi-institutional collaborative learning of\ndeep neural networks across distributed health data. Similar to conventional distributed learning, the split learning\narXiv:2308.11027v1  [cs.LG]  21 Aug 2023\nTable 1: A qualitative comparison of different distributed learning schemes.\nPrivacy Implications\nFramework\nProtection over\nRaw Data\nProtection over\nModel Parameters\nProtection over\nModel Architecture\nModel Utility\nComputational Efficiency\nFL [5]\nYes\nNo\nNo\nHigh\nModerate\nFL+DP [10]\nYes\nVariable∗\nNo\nVariable∗\nModerate\nFL+SMC [12]\nYes\nNo\nNo\nHigh\nLow\nFL+SMC+HE [14]\nYes\nYes\nNo\nHigh\nLow\nSL [16]\nYes\nPartial†\nPartial†\nHigh\nHigh\n∗Depends on the choice of privacy parameters and it’s typically a trade-off between privacy and utility.\n† Associated with the relative location of the cut layer: only the shallow layers of the model (up to the cut layer) are protected from the server.\nframework is composed of a server (i.e., the coordinator) and multiple healthcare organizations (i.e., the data contribu-\ntors) [15]. Its uniqueness is that under the split learning setting, the deep neural network, also referred to as the global\nmodel, is divided into two sub-models (i.e., the client model and the server model) according to a specific layer known\nas the cut layer [16]. The healthcare organizations and the server hold only their portion of the model, which they do\nnot share with each other. At each training round, the healthcare organizations only train the first part of a deep neural\nnetwork and send, what the literature has called, the smashed data (i.e., the latent representations of raw data derived\nfrom the client model) to the server. The server then completes the rest of the forward propagation and computes the\nloss function without accessing the clients’ raw data. Finally, the training round is concluded with a backward pass,\nwhere the server sends back to the clients the computed gradients, which are then used to update the client model.\nThis process is equivalent to a training epoch in the centralized learning setting and will iterate until the global model\nconverges.\nTable 1 provides a qualitative comparison between different distributed learning frameworks. Notably, as is in the\nfederated learning setting, no sensitive raw data is shared during the split learning training process, which maintains\nthe privacy of patients’ data. Moreover, in split learning, neither the server nor the clients have complete knowledge\nof the global model’s architecture and weights. This is a major difference from federated learning, where the server\nhas full access to the client’s model. This notion of incompleteness in knowledge about the global model, combined\nwith the minimal information encoded in the smashed data, further reduces the risk of privacy leakage during training.\nThis is also beneficial to the server in scenarios where the server hopes not to reveal its developed model architecture,\nwhich may be considered proprietary. In addition to privacy benefits, split learning can also alleviate the computational\nburden on the healthcare organizations’ side by offloading part of the training process of the deep neural network to\nthe server (e.g., a data center) which typically has access to more computational power at a cheaper rate. In addition\nto the qualitative study, we further 1) provide a quantitative analysis of the split learning framework by conducting\nexperiments across three biomedical image datasets (PathMNIST [17], OrganAMNIST [18], and BloodMNIST [19])\nand two EHR datasets (eICU [20] and a private dataset from Vanderbilt University Medical Center [21]), 2) perform an\nanalysis to compare the privacy risk under the split learning and federated learning framework, and 3) investigate the\ntrade-off between privacy, model utility, and client-side model training efficiency in split learning. Our results suggest\nthat split learning can consistently achieve comparable performance as federated learning while providing enhanced\nprivacy and computational efficiency for the participating health organizations.\nMethod\nFederated Learning. Federated learning can be conducted among either a small group of organizations (cross-silo)\nor a large population of mobile devices (cross-device). In this paper, we focus on the cross-silo setting, since it is\nmore prevalent in the healthcare domain. As depicted in Figure1a, we assume that there are k clients who participate\nin the model training. Each client holds ni data samples (i ∈{1, 2, ..., k}). n = Pk\ni ni is the total data size. The\nobjective of the collaboration is to train a neural network Fw parameterized by weights w. At the beginning of the\nmodel training, each client initializes its local model wc\ni in parallel. For each training epoch, each client calculates\nthe loss L\n\u0000Fwc\ni (Xi), yi\n\u0001\nusing its own data Xi with labels yi in parallel. A local model update is computed through\ngradient descent and shared with the server: wc\ni ←wc\ni −η · ∂L\n∂wc\ni . Finally, at the end of each training epoch, the server\nupdates the global model weights by computing a weighted average of the local models, a process known as federated\naveraging [5]: w ←Pk\ni\nni\nn · wc\ni. This process is repeated until the global model converges.\n(a) Federated learning\n(b) Split learning\nFigure 1: Illustration of the federated learning and split learning frameworks.\nSplit Learning. As shown in Figure 1b, in the split learning setting, a neural network F is partitioned into two separate\nsub-models. The first is the client model hwc, which takes the raw data X and outputs latent representations of the\ndata known as the smashed data. The second is the server model fws, which makes predictions based on the smashed\ndata, i.e., F(X) = (fws ◦hwc)(X). The clients and the server only have access to their own part of the model and\ncannot access the other part.\nEach training step of a neural network can be described as a forward pass where the loss function is computed and\na backward pass where the model parameters are updated by back-propagating the error through gradients. In the\ncanonical version of split learning, at each forward pass, the client computes the smashed data hwc(x) and then shares\nthe smashed data along with its label to the server. The server makes a prediction of the smashed data using the server\nmodel to compute the loss L\n\u0010\nfws\u0000hwc(X)\n\u0001\n, y\n\u0011\n. In the backward pass, the server first updates its model according to\nws ←ws −η ·\n∂L\n∂ws and sends the gradients of the smashed data\n∂L\n∂hwc(X) to the client. The client then computes the\ngradients of the client model\n∂L\n∂wc and updates the model parameters accordingly i.e., wc ←wc −η ·\n∂L\n∂wc , where η is\nthe learning rate. Finally, in the multiple clients’ scenario, the client shares its model parameters with the next client\nwhere this process repeats. Note that the raw data is never shared in the process and the server only sees a compact\nrepresentation of the client’s private data (the smashed data). Moreover, the computational cost on the clients’ end\nis greatly reduced since the client is only responsible for the computation of the first part of the model. It should be\nrecognized that there are variants of split learning that do not require sharing labels or allow learning on vertically\npartitioned data [16].\nSpeeding up Split Learning with Federation. The canonical split learning framework processes each client in a\nsequential order, which can become very time-consuming if either the number of clients or the amount of data main-\ntained by each client is large. Inspired by federated learning, the split learning framework can be modified to enable\nclients to compute their updates in a parallel manner and thereby accelerate the learning process [22]. Specifically,\nat each round of training, k clients first compute their smashed data in parallel hwc\ni (Xi), i ∈{1, 2, ..., k} and send\nthe results to the server. Next, the server computes the loss Li\n\u0010\nfws\ni\n\u0000hwc\ni (Xi)\n\u0001\n, yi\n\u0011\nand the gradients ∂Li\n∂wc\ni and sends\nback the gradients to each individual clients. The server then updates its model by averaging the gradients from\nall clients: ws ←ws −η Pk\ni\nni\nn · ∂Li\n∂ws\ni . Similarly, the clients update their models via federated averaging, i.e.,\nwc ←wc −η Pk\ni\nni\nn · ∂Li\n∂wc\ni , with the help of a separate server referred to as the federated server. By allowing paral-\nlel computing on both the clients’ and the server’s side, this variant of split learning (named SplitFedv1 in [22]) can\nsignificantly speed up the training process. However, the drawback of this approach is that to protect the client’s data\nprivacy, it is required that the federated server is a trustworthy third party that does not collude with the main server. In\naddition, the federation process may negatively affect the utility of the converged model. Nevertheless, a low compu-\ntation latency is often desired for deploying distributed learning solutions to large-scale real-world applications. Thus,\nwe use this variant of split learning as the default in our experiments.\nEvaluation\nExperimental Setup\nTable 2: A summary of the datasets used in this study.\nDataset\nData Type\nTask\nLabel Distribution\nPathMNIST\nColon pathology\nMulti-class\n0\n1\n2\n3\n4\n5\n6\n7\n8\nClass\n0\n5\n10\n15\nPercentage (%)\nOrganAMNIST\nAbdominal CT\nMulti-class\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nClass\n0\n5\n10\n15\nPercentage (%)\nBloodMNIST\nBlood cell microscope\nMulti-class\n0\n1\n2\n3\n4\n5\n6\n7\nClass\n0\n10\n20\nPercentage (%)\neICU\nElectronic health record\nBinary-class\n0\n1\nClass\n0\n25\n50\n75\nPercentage (%)\nVUMC\nElectronic health record\nBinary-class\n0\n1\nClass\n0\n25\n50\n75\nPercentage (%)\nDatasets. We utilize five datasets to support evaluations\nin two types of settings - biomedical image classification\nand clinical concept predictions from structured EHR\ndata. Specfically, we apply the following three datasets\nfrom the MedMNIST [23] benchmark for biomedical\nimage classification tasks: (1) PathMNIST [17]: a colon\npathology dataset containing 100, 000 non-overlapping\nimage patches from hematoxylin & eosin stained histo-\nlogical images for classifying 9 types of tissues. We fol-\nlow the recommended train/test split in our experiment,\nresulting in a total number of 89, 996 images for train-\ning. An additional 7, 180 image patches collected from a\ndifferent clinical center are reserved for testing; (2) Or-\nganAMNIST [18]: a 2D image dataset cropped from\nthe axial view of 3D computed tomography (CT) im-\nages from Liver Tumor Segmentation Benchmark [18]\nfor performing classification of 11 body organs.\nThe\ndataset is partitioned into two disjoint sets, with 34, 581 images for training and 17, 778 for testing; and (3) Blood-\nMNIST [19]: a peripheral blood cell images dataset containing individual normal cells organized into 8 classes. The\ndataset is partitioned into training and testing sets, each with 11, 959 and 3, 421 images, respectively. For the EHR pre-\ndiction tasks, we use the following two datasets: (1) eICU [20]: a public EHR dataset containing more than 140, 000\npatients’ hospital visit records. The task is to predict the risk of being readmitted to the ICU in the next 15 days (i.e.,\nbinary classification) given the clinical activities during the current ICU stay. Following previous studies [24, 25], we\nconsider five types of events as features, including diagnosis, lab test, medication, physical exam, and treatment. We\nuse non-overlapping training and testing sets containing data from 30, 000 and 10, 000 patients, respectively; and (2)\nVUMC [21]: a private EHR dataset collected from Vanderbilt University Medical Center containing all adult inpatient\nvisits in 2019. The task is to predict whether the patient will be discharged the next day to home or other care facili-\nties. Visits shorter than 24h or of patients who died during hospitalization are excluded, resulting in a total number of\n26, 283 patients with an average age of 52.9. Table 2 provides a summary of the description and label distribution for\neach dataset.\nTable 3:\nModel architecture for\nbiomedical image classification.\nLayer\nKernel\nStride\nOutput\nConv2D\n3 × 3\n1 × 1\n16\nBatchNorm2D\n−\n−\n16\nConv2D\n3 × 3\n1 × 1\n16\nBatchNorm2D\n−\n−\n16\nMaxPool2D\n2 × 2\n2 × 2\n16\nConv2D\n3 × 3\n1 × 1\n64\nBatchNorm2D\n−\n−\n64\nConv2D\n3 × 3\n1 × 1\n64\nBatchNorm2D\n−\n−\n64\nConv2D\n3 × 3\n1 × 1\n64\nBatchNorm2D\n−\n−\n64\nMaxPool2D\n2 × 2\n2 × 2\n64\nFC\n−\n−\n128\nFC\n−\n−\n128\nFC\n−\n−\n# of classes\nDeep Learning Models. For biomedical image classification tasks, we rely on\na convolutional neural network containing five convolutional layers, two max-\npooling layers, and three fully-connected (FC) layers with ReLU activation. Ta-\nble 3 provides the details of the network architecture. The client owns the first\npart of the model, containing two convolutional layers and one max-pooling\nlayer. For the readmission prediction task on the eICU dataset, we apply the\nsame model architecture as described in [25]. The client model utilizes separate\nencoders for mapping different types of events into a same-length embedding\nsequence, which is then processed by a Transformer encoder. The server model\nis a two-layer fully-connected network for making final predictions. For the\ndischarge prediction task with the VUMC dataset, we use a four-layer fully-\nconnected network (2808 −64 −32 −32 −1) with ReLU activation, where the\nmodel is split after the first layer.\nCentralized/Distributed Learning Setting. In the centralized learning scenario, we train the deep learning model on\nthe entire training set for 50 epochs using Adam optimizer with a batch size of 256. By default, we use a learning rate\nof 10−4 and a weight decay of 10−5. Specially, for the readmission prediction task on the eICU dataset, we set the\nTable 4: A comparison of final performance measured on the test dataset for centralized learning (CL), federated\nlearning (FL), and split learning (SL) (reported with a 95% confidence interval).\nPathMNIST\nOrganAMNIST\nBloodMNIST\neICU\nVUMC\nAccuracy\nAUROC\nAccuracy\nAUROC\nAccuracy\nAUROC\nAUPRC\nF1\nKappa\nAUPRC\nF1\nKappa\nCL\n0.8346\n±0.0088\n0.9734\n±0.0071\n0.8773\n±0.0051\n0.9901\n±0.0010\n0.9379\n±0.0022\n0.9958\n±0.0005\n0.6389\n±0.0127\n0.6083\n±0.0057\n0.4735\n±0.0087\n0.7898\n±0.0058\n0.7147\n±0.0037\n0.6565\n±0.0042\nFL\n0.8225\n±0.0175\n0.9690\n±0.0043\n0.8689\n±0.0065\n0.9890\n±0.0010\n0.9165\n±0.0107\n0.9932\n±0.0012\n0.6384\n±0.0122\n0.5943\n±0.0101\n0.4675\n±0.0098\n0.7884\n±0.0019\n0.7122\n±0.0029\n0.6547\n±0.0035\nSL\n0.8127\n±0.0129\n0.9673\n±0.0069\n0.8538\n±0.0067\n0.9864\n±0.0012\n0.9066\n±0.0096\n0.9917\n±0.0013\n0.6331\n±0.0109\n0.6000\n±0.0074\n0.4693\n±0.0078\n0.7840\n±0.0028\n0.7116\n±0.0026\n0.6524\n±0.0033\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\nAccuracy\nAUROC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nAccuracy\nAUROC\n0\n10\n20\n30\n40\n50\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n(%)\nRelative Approximation Error\nAccuracy\nAUROC\n(a) PathMNIST\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\nAccuracy\nAUROC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nAccuracy\nAUROC\n0\n10\n20\n30\n40\n50\nNumber of Epochs\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n(%)\nRelative Approximation Error\nAccuracy\nAUROC\n(b) OrganAMNIST\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\nAccuracy\nAUROC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nAccuracy\nAUROC\n0\n10\n20\n30\n40\n50\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(%)\nRelative Approximation Error\nAccuracy\nAUROC\n(c) BloodMNIST\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSplit Learning\nAUPRC\nF1\nKappa\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nFederated Learning\nAUPRC\nF1\nKappa\n0\n10\n20\n30\n40\n50\nNumber of Epochs\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n(%)\nRelative Approximation Error\nAUPRC\nF1\nKappa\n(d) eICU\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSplit Learning\nAUPRC\nF1\nKappa\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFederated Learning\nAUPRC\nF1\nKappa\n0\n10\n20\n30\n40\n50\nNumber of Epochs\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n(%)\nRelative Approximation Error\nAUPRC\nF1\nKappa\n(e) VUMC\nFigure 2: Comparison of convergence by measuring the per-epoch performance on the test dataset for federated\nlearning and split learning.\nlearning rate to be 5 × 10−4. For the discharge prediction task on the VUMC dataset, we use a weight decay of 10−4.\nIn the case of federated and split learning scenarios, we randomly partition the training dataset into k disjoint subsets\nand assign them to each client. The parameters for the optimizers are set to be the same as in centralized learning.\nUnless mentioned otherwise, we use a default number of clients k = 5 in our experiments.\nMetrics. We use the following metrics for the biomedical image classification tasks: (1) Accuracy: the average\nclassification accuracy; and (2) AUROC: the average area under the receiver operating characteristic (ROC) curves of\neach class against the rest. For the EHR prediction tasks, we report the following measures: (1) AUPRC: the average\narea under the precision-recall curve; (2) F1: the average F1 score; and (3) Kappa: Cohen’s Kappa score, which\nmeasures the agreement between two raters on a classification task, defined as κ = po−pe\n1−pe , where po is the empirical\nprobability of agreement on the label assigned to any sample (i.e., the observed agreement ratio), and pe is the expected\nagreement when both raters assign labels randomly. The Kappa ranges from −1 to 1, with a larger value indicating a\nstronger agreement. For each setting in our experiment, we train the neural network on the training dataset with five\ndifferent random seeds and report its performance as measured on the hold-out testing dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0710 \n R2 score: 0.9636\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.1403 \n R2 score: 0.9842\nAUROC\n(a) PathMNIST\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.1159 \n R2 score: 0.9715\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.2833 \n R2 score: 0.9453\nAUROC\n(b) OrganAMNIST\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0229 \n R2 score: 0.9971\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.1337 \n R2 score: 0.9855\nAUROC\n(c) BloodMNIST\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0798 \n R2 score: 0.9757\nAUPRC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0260 \n R2 score: 0.9394\nF1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0833 \n R2 score: 0.9474\nKappa\n(d) eICU\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0798 \n R2 score: 0.9757\nAUPRC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0260 \n R2 score: 0.9394\nF1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSplit Learning\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFederated Learning\nCoefficient: 1.0833 \n R2 score: 0.9474\nKappa\n(e) VUMC\nFigure 3: Linear regression results of the per-epoch performance of federated learning vs. split leaning on 5 datasets.\nNumber of clients\n5\n10\n15\n20\nNumber of samples \n per client\n500\n1000\n1500\n2000\nAUPRC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n(a) Federated learning\nNumber of clients\n5\n10\n15\n20\nNumber of samples \n per client\n500\n1000\n1500\n2000\nAUPRC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n(b) Split learning\nNumber of clients\n5\n10\n15\n20\nNumber of samples \n per client\n500\n1000\n1500\n2000\nAUPRC\n−0.100\n−0.075\n−0.050\n−0.025\n0.000\n0.025\n0.050\n0.075\n0.100\n(c) Difference\nFigure 4: Sensitivity analysis\non the VUMC dataset.\nUtility Analysis\nPerformance. We conducted experiments using centralized learning, federated learn-\ning, and split learning under the same conditions for 50 epochs. To eliminate the ef-\nfects of overfitting in centralized learning, we assume early stopping is employed and\nreport the best performance measured on the test dataset. As shown in Table 4, the cen-\ntralized learning consistently achieved the highest performance across all five datasets,\nwhich can be considered the upper bound of the performance for distributed learning.\nWhile federated learning outperformed split learning slightly on four datasets (exclud-\ning eICU), the difference in performance between them was almost negligible (usually\n< 1%), with margins of error being similar.\nConvergence. Figure 2 compares the convergence of the global model between feder-\nated learning and split learning. The first and second rows plot the model performance\nmeasured on the test dataset after each epoch for split and federated learning, respec-\ntively. The third row plots the relative approximation error between federated and\nsplit learning, defined as δ = | vSL−vF L\nvF L\n| × 100%, where vSL and vF L denote the per-\nformance of split and federated learning model respectively. We observe that except\nfor the differences caused by the initialization at the beginning of training, both split\nand federated learning are able to converge at a similar rate. For some datasets (e.g.,\neICU and VUMC), split learning can converge even faster than federated learning.\nThis observation was further verified by the linear regression analysis shown in Fig-\nure 3, which demonstrates a high correlation for all datasets (typically with estimated\ncoefficients < 1.2 and R2 > 0.9).\nScalibility. To investigate the scalability of federated learning and split learning, we\nperform a sensitivity analysis on the VUMC dataset with different numbers of par-\nticipating clients as well as different numbers of training data samples per client. As\nshown in Figure 4, a similar pattern is observed for both federated learning and split\nlearning, where the performance of the resulting model (measured by AUPRC) contin-\nues to improve as the number of clients and the number of samples per client increases.\nMoreover, the differences in performance between federated learning and split learn-\ning as shown in Figure 4c (green for positive and orange for negative values) are neg-\nligible. This indicates that federated learning and split learning are both scalable to\nmore participating clients and a larger amount of training data.\nTable 5: Comparison on client-side model efficiency.\nDataset\nSplit Learning\nFederated Learning\n# of params\nFLOPs\n# of params\nFLOPs\nPathMNIST\n2,832\n1,726,208\n235,225\n7,591,817\nOrganAMNIST\n2,544\n1,531,520\n235,195\n7,397,387\nBloodMNIST\n2,832\n1,726,208\n235,096\n7,591,688\neICU\n1,556,475\n−∗\n1,558,556\n−∗\nVUMC\n179,776\n179,904\n186,049\n186,369\n∗Cannot be estimated since the input is heterogeneous clinical sequences with vari-\nable lengths and diverse features.\nEfficiency. Table 5 compares the efficiency on the\nclient’s end in terms of model parameters and the es-\ntimated floating point operations (FLOPs) required for\ncomputing a model forward pass. Our results indicate\nthat split learning consumes less memory and requires\nfewer computations on the client side compared to fed-\nerated learning across all five datasets. Notably, split\nlearning outperforms federated learning significantly in\nmedical image recognition tasks, reducing memory usage by approximately 99% and computational requirements by\n77%. However, the benefits of split learning in EHR tasks are marginal, likely due to the complexity of prediction\ntasks, the specific model architecture, and the choice of cut layer position used in our experiments.\nPrivacy Analysis\nBoth federated learning and split learning mitigate the systemic privacy risks from traditional centralized learning by\nembedding a data minimization principle in their design. Federated learning achieves this principle by aggregating\ninformation collected from multiple data records into a focused model update. By contrast, split learning limits the\namount of collected information about each data record by only sharing its smashed data.\nFor the simplicity of analysis, herein we assume that the number of dimensions being revealed is proportional to\nthe privacy risk, i.e., the amount of private information being leaked. We note that this simplified privacy model\nis by no means a rigorous measure of data privacy, but it can serve as a baseline with room for refinement in the\nfuture. More specifically, let us consider a client who contributes nc private data samples to participate in training.\nSuppose the federated learning model has Nw the total number of parameters (i.e., w ∈RNw) and the cut layer\nsize/smashed data dimension is d (i.e., hwc(x) ∈Rd). Then during each training round, the average number of\ndimensions revealed to the server per each training sample is Nw\nnc for federated learning and d for split learning, as\nillustrated in Figure 5a. Since the model update is for a fixed number of dimensions (i.e., the same dimension as the\nmodel parameters), federated learning benefits from having a large size of local training data to average out the privacy\nrisk. By contrast, the amount of information revealed for each data sample in split learning is invariant to the local\ndata size and only dependent on the client’s model architecture (i.e., the cut layer size). Since deeper layers tend to\nproduce more compact representations (i.e., smaller d), it is usually a trade-off between computational efficiency and\nprivacy for the client model in split learning. As such, federated learning appears to be more suitable for scenarios\nwhere every client possesses a sufficiently large quantity of data samples (≥Nw\nd ) but discourages clients with fewer\ndata from participating due to the higher privacy risk. In our setting, the minimum reasonable data size for federated\nlearning ( Nw\nd ) is 102, 2907, and 12176 for the biomedical image datasets, the VUMC dataset, and the eICU dataset,\nrespectively. However, since modern deep neural networks are experiencing exponential growth in size, the minimum\nrequired data size can easily scale up to a number that is very difficult to achieve in practice. For example, if we train\na ResNet-50 model [26] that contains over 23 million trainable parameters to recognize medical images and choose to\nsplit after the adaptive pooling layer (corresponds to a cut layer size of 512), we would need at least over 44000 data\nrecords from every site to justify the adoption of federated learning. Such a requirement is oftentimes unattainable\nfor healthcare applications, where a client with only a few patient records can also make a crucial contribution to the\nmodel. For instance, specialty healthcare facilities (e.g., smaller oncology practices) may have much fewer patients\ncompared to general hospitals, but they typically have a much more focused dataset in terms of certain diseases.\nTherefore, split learning becomes more desirable in these circumstances as it is more versatile and can provide the\nsame level of privacy benefits regardless of the size of the client’s local data.\nRisk of Inversion. Compared to federated learning, split learning further reduces the risks of model/gradient inversion\nattack [27, 28, 29, 30] by restricting model access for both parties. As is depicted in Figure 5b, learning can be seen\nas computing a function F on the client’s private data X to get result M. In federated learning, although the raw data\nX is not directly accessible to the server, the computation function F and the result M are both known to the server,\nwhich leaves the opportunity for the server to approximate an inverse function to infer X from M and break patient\nprivacy. Differently, split learning separates the function F (i.e., the model) into the client’s part FC and the server’s\nNumber of private\n training samples (nc)\nNumber of dimensions\n revealed per sample\n(Nw/d, d)\n(1, Nw)\nSL\nFL\n(a) Privacy budget per sample mea-\nsured by dimensionality\n(b) Privacy protection scheme: information within\nthe blue dashed line are kept secret from the server\nFigure 5: Conceptual comparison of privacy preservation in federated learning (FL) vs. split learning (SL).\npart FS and only the latter is revealed to the server. Thus, the server in split learning only has imperfect information\nabout the computing process, which makes it quite challenging to compute the exact inverse function.\nDiscussion\nDesign Trade-offs. There exist two major trade-offs in designing a deep learning model for split learning. (1) Privacy-\nutility trade-off: Split learning reflects the data minimization principle by allowing clients to extract and send only\nthe relevant features of their local data to the server, while keeping the rest of the data private. The client model\nserves as a compressor which helps to reduce the amount of information that is transmitted over the network while\npreserving the most important information for the task at hand. Using a small cut layer size further suppresses the\namount of information in the smashed data, but may negatively affect the model performance. Thus, one of the main\nobjectives of designing a split learning model is to choose a proper cut layer size to find a balance between keeping\nenough information to capture the important aspects of the data and reducing the amount of information to minimize\nthe privacy risk of releasing the smashed data. (2) Privacy-efficiency trade-off: the information bottleneck theory [31]\nsuggests that the output of deeper layers in a neural network contains more information about the task label and\nless information about the original input. As such, choosing to split at a deeper layer may result in less redundant\ninformation in the smashed data, thereby reducing the privacy risk. However, setting a shallow layer as the cut layer\nallows the client to have a more compact model and can thus give full scope to the advantages of split learning to\nreduce the memory consumption and computational burden on the client side. Overall, these tuning knobs in split\nlearning offer the user great flexibility in terms of configuring the privacy-utility-efficiency trade-off for specific tasks,\nwhereas federated learning lacks such customizability.\nLimitations. Despite the merits of split learning, there are several limitations we would like to highlight. First, the\ncurrent split learning framework only supports decentralized training of deep neural networks but does not support the\ntraining of traditional machine learning models, such as tree-based models. Second, although split learning largely\nrelieves the computational cost on the client side, the communication cost is increased compared to federated learning,\nas the client needs to communicate more frequently with the server for every mini-batch to perform gradient descent.\nTo address this, recent studies propose to utilize an intermediate edge server [32], asynchronous training scheme [33],\nor an automated software framework [34] to reduce the communication overheads in split learning. Additionally,\nservice providers who do not wish to claim intellectual property over the developed model can share the server-side\nmodel with the clients after training to eliminate the communication overheads at inference time. Third, the privacy\nbenefits of split learning heavily rely on the inaccessibility of the client model and thus might be vulnerable under a\nstronger threat model. For instance, in the insider attack scenario where the server is able to collude with one of the\nclients, the privacy protection over other clients’ data would be greatly diminished.\nFuture Directions. There are several notable problems that should be considered as future research. First, how can\nwe combine split learning with rigorous statistical privacy methods (e.g., differential privacy) or secure multiparty\ncomputation to achieve more rigorous privacy protection? Moreover, how can this be done with minimal sacrifice\nto model utility or significantly increasing the computational cost? Second, how can we design a more nuanced\nframework for privacy in split learning? Specifically, we need to quantify the amount of leaked private information via\nsmashed data and the potential privacy risks. Third, can we design a systematic framework to decide what the optimal\ncut layer size and location for a given neural network is to minimize the privacy risk while preserving most model\nutility? Fourth, how can we realize split learning on heterogeneous data types and varying data distributions?\nConclusion\nA lack of sufficient data to cover the distribution of the general population is one of the major impediments to de-\nveloping practical deep learning models for healthcare applications. In this work, we introduced split learning as a\nnew distributed learning paradigm for enabling multi-institutional collaborative development of deep learning models\nacross data silos without accessing raw patient data. Through both in-depth qualitative analysis as well as systematic\nquantitative experiments on five health datasets, we illustrated that split learning can achieve similar model utility as\nfederated learning while providing better client-side model efficiency, lower risk of inversion, enhanced protection\nover the model, and more flexible privacy protection over clients’ data. Our findings suggest that split learning is a\npromising alternative to federated learning for developing deep learning models without violating the privacy of the\ndata contributors in many healthcare tasks.\nAcknowledgments This research was sponsored in part by grant U54HG012510, the NIH Bridge2AI Center. BM\nhas been a paid consultant to TripleBlind AI, but for work unrelated to this investigation.\nReferences\n1. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, et al. Dermatologist-level classification of skin\ncancer with deep neural networks. Nature. 2017;542(7639):115-8.\n2. Wen A, Fu S, Moon S, El Wazir M, Rosenbaum A, Kaggal VC, et al. Desiderata for delivering NLP to accelerate\nhealthcare AI advancement and a Mayo Clinic NLP-as-a-service implementation. NPJ Digit Med. 2019;2(1):130.\n3. Landi I, Glicksberg BS, Lee HC, Cherng S, Landi G, Danieletto M, et al. Deep representation learning of elec-\ntronic health records to unlock patient stratification at scale. NPJ Digit Med. 2020;3(1):96.\n4. Cohen JP, Cao T, Viviano JD, Huang CW, Fralick M, Ghassemi M, et al. Problems in the deployment of machine-\nlearned models in health care. CMAJ. 2021;193(35):E1391-4.\n5. McMahan B, Moore E, Ramage D, Hampson S, y Arcas BA. Communication-efficient learning of deep networks\nfrom decentralized data. In: Artificial intelligence and statistics. PMLR; 2017. p. 1273-82.\n6. Rieke N, Hancox J, Li W, Milletari F, Roth HR, Albarqouni S, et al. The future of digital health with federated\nlearning. NPJ Digit Med. 2020;3(1):119.\n7. Kaissis GA, Makowski MR, R¨uckert D, Braren RF. Secure, privacy-preserving and federated machine learning in\nmedical imaging. Nature Machine Intelligence. 2020;2(6):305-11.\n8. Dayan I, Roth HR, Zhong A, Harouni A, Gentili A, Abidin AZ, et al. Federated learning for predicting clinical\noutcomes in patients with COVID-19. Nat Med. 2021;27(10):1735-43.\n9. Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, et al. Deep learning with differential\nprivacy. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security;\n2016. p. 308-18.\n10. Truex S, Liu L, Chow KH, Gursoy ME, Wei W. LDP-Fed: Federated learning with local differential privacy. In:\nProceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking; 2020. p.\n61-6.\n11. Constable SD, Tang Y, Wang S, Jiang X, Chapin S. Privacy-preserving GWAS analysis on federated genomic\ndatasets. In: BMC Med Inform Decis Mak. vol. 15. BioMed Central; 2015. p. 1-9.\n12. Bonawitz K, Ivanov V, Kreuter B, Marcedone A, McMahan HB, Patel S, et al. Practical secure aggregation for\nprivacy-preserving machine learning. In: proceedings of the 2017 ACM SIGSAC Conference on Computer and\nCommunications Security; 2017. p. 1175-91.\n13. Froelicher D, Troncoso-Pastoriza JR, Raisaro JL, Cuendet MA, Sousa JS, Cho H, et al.\nTruly privacy-\npreserving federated analytics for precision medicine with multiparty homomorphic encryption. Nat Commun.\n2021;12(1):5910.\n14. Hardy S, Henecka W, Ivey-Law H, Nock R, Patrini G, Smith G, et al. Private federated learning on vertically\npartitioned data via entity resolution and additively homomorphic encryption. arXiv preprint arXiv:171110677.\n2017.\n15. Gupta O, Raskar R. Distributed learning of deep neural network over multiple agents. Journal of Network and\nComputer Applications. 2018;116:1-8.\n16. Vepakomma P, Gupta O, Swedish T, Raskar R. Split learning for health: Distributed deep learning without sharing\nraw patient data. arXiv preprint arXiv:181200564. 2018.\n17. Kather JN, Krisam J, Charoentong P, Luedde T, Herpel E, Weis CA, et al. Predicting survival from colorectal\ncancer histology slides using deep learning: A retrospective multicenter study. PLoS Med. 2019;16(1):e1002730.\n18. Bilic P, Christ PF, Vorontsov E, Chlebus G, Chen H, Dou Q, et al. The liver tumor segmentation benchmark (lits).\narXiv. arXiv preprint arXiv:190104056. 2019.\n19. Acevedo A, Merino A, Alf´erez S, Molina ´A, Bold´u L, Rodellar J. A dataset of microscopic peripheral blood cell\nimages for development of automatic recognition systems. Data in Brief. 2020;30.\n20. Pollard TJ, Johnson AE, Raffa JD, Celi LA, Mark RG, Badawi O. The eICU Collaborative Research Database, a\nfreely available multi-center database for critical care research. Sci Data. 2018;5(1):1-13.\n21. Zhang X, Yan C, Malin BA, Patel MB, Chen Y. Predicting next-day discharge via electronic health record access\nlogs. Journal of the American Medical Informatics Association. 2021;28(12):2670-80.\n22. Thapa C, Arachchige PCM, Camtepe S, Sun L. Splitfed: When federated learning meets split learning. In:\nProceedings of the AAAI Conference on Artificial Intelligence. vol. 36; 2022. p. 8485-93.\n23. Yang J, Shi R, Wei D, Liu Z, Zhao L, Ke B, et al. MedMNIST v2-A large-scale lightweight benchmark for 2D\nand 3D biomedical image classification. Scientific Data. 2023;10(1):41.\n24. Choi E, Xu Z, Li Y, Dusenberry M, Flores G, Xue E, et al. Learning the graphical structure of electronic health\nrecords with graph convolutional transformer. In: Proceedings of the AAAI Conference on Artificial Intelligence.\nvol. 34; 2020. p. 606-13.\n25. Yang C, Westover MB, Sun J. ManyDG: Many-domain Generalization for Healthcare Applications. In: Proceed-\nings of the International Conference on Learning Representations; 2023. .\n26. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition; 2016. p. 770-8.\n27. Fredrikson M, Jha S, Ristenpart T. Model inversion attacks that exploit confidence information and basic coun-\ntermeasures. In: Proceedings of the 22nd ACM SIGSAC conference on computer and communications security;\n2015. p. 1322-33.\n28. Zhang Y, Jia R, Pei H, Wang W, Li B, Song D. The secret revealer: Generative model-inversion attacks against\ndeep neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition;\n2020. p. 253-61.\n29. Zhu L, Liu Z, Han S. Deep leakage from gradients. Advances in neural information processing systems. 2019;32.\n30. Li Z, Zhang J, Liu L, Liu J. Auditing privacy defenses in federated learning via generative gradient leakage. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2022. p. 10132-42.\n31. Tishby N, Zaslavsky N. Deep learning and the information bottleneck principle. In: IEEE Information Theory\nWorkshop. IEEE; 2015. p. 1-5.\n32. Turina V, Zhang Z, Esposito F, Matta I. Federated or split? a performance and privacy analysis of hybrid split and\nfederated learning architectures. In: Proceedings of the IEEE 14th International Conference on Cloud Computing.\nIEEE; 2021. p. 250-60.\n33. Duan Q, Hu S, Deng R, Lu Z. Combined federated and split learning in edge computing for ubiquitous intelligence\nin internet of things: State-of-the-art and future directions. Sensors. 2022;22(16):5983.\n34. Gharibi G, Patel R, Khan A, Gilkalaye BP, Vepakomma P, Raskar R, et al. An automated framework for distributed\ndeep learning–a tool demo. In: Proceedings of the IEEE 42nd International Conference on Distributed Computing\nSystems. IEEE; 2022. p. 1302-5.\n",
  "categories": [
    "cs.LG",
    "cs.CR"
  ],
  "published": "2023-08-21",
  "updated": "2023-08-21"
}