{
  "id": "http://arxiv.org/abs/2308.11924v1",
  "title": "Diverse Policies Converge in Reward-free Markov Decision Processe",
  "authors": [
    "Fanqi Lin",
    "Shiyu Huang",
    "Weiwei Tu"
  ],
  "abstract": "Reinforcement learning has achieved great success in many decision-making\ntasks, and traditional reinforcement learning algorithms are mainly designed\nfor obtaining a single optimal solution. However, recent works show the\nimportance of developing diverse policies, which makes it an emerging research\ntopic. Despite the variety of diversity reinforcement learning algorithms that\nhave emerged, none of them theoretically answer the question of how the\nalgorithm converges and how efficient the algorithm is. In this paper, we\nprovide a unified diversity reinforcement learning framework and investigate\nthe convergence of training diverse policies. Under such a framework, we also\npropose a provably efficient diversity reinforcement learning algorithm.\nFinally, we verify the effectiveness of our method through numerical\nexperiments.",
  "text": "Diverse Policies Converge in Reward-free\nMarkov Decision Processes\nFanqi Lin1, Shiyu Huang2[0000−0003−0500−0141], and Wei-Wei Tu2\nTsinghua University, Beijing, 100084, China\nlfq20@mails.tsinghua.edu.cn\n4Paradigm Inc., Beijing, 100084, China\n{huangshiyu,tuweiwei}@4paradigm.com\nAbstract. Reinforcement learning has achieved great success in many\ndecision-making tasks, and traditional reinforcement learning algorithms\nare mainly designed for obtaining a single optimal solution. However, re-\ncent works show the importance of developing diverse policies, which\nmakes it an emerging research topic. Despite the variety of diversity\nreinforcement learning algorithms that have emerged, none of them the-\noretically answer the question of how the algorithm converges and how\nefficient the algorithm is. In this paper, we provide a unified diversity re-\ninforcement learning framework and investigate the convergence of train-\ning diverse policies. Under such a framework, we also propose a provably\nefficient diversity reinforcement learning algorithm. Finally, we verify the\neffectiveness of our method through numerical experiments1.\nKeywords: Reinforcement learning · Diversity Reinforcement Learning\n· Bandit.\n1\nIntroduction\nReinforcement learning (RL) shows huge advantages in various decision-making\ntasks, such as recommendation systems [20,23], game AIs [3,10] and robotic\ncontrols [24,17]. While traditional RL algorithms can achieve superhuman per-\nformances on many public benchmarks, the obtained policy often falls into a\nfixed pattern. For example, previously trained agents may just overfit to a de-\ntermined environment and could be vulnerable to environmental changes [6].\nFinding diverse policies may increase the robustness of the agent [16,12]. More-\nover, a fixed-pattern agent will easily be attacked [21], because the opponent\ncan find its weakness with a series of attempts. If the agent could play the game\nwith different strategies each round, it will be hard for the opponent to identify\nthe upcoming strategy and it will be unable to apply corresponding attacking\ntactics [13]. Recently, developing RL algorithms for diverse policies has attracted\nthe attention of the RL community for the promising value of its application and\nalso for the challenge of solving a more complex RL problem [7,11,4].\n1 Access the code on GitHub: https://github.com/OpenRL-Lab/DiversePolicies\narXiv:2308.11924v1  [cs.LG]  23 Aug 2023\n2\nFanqi Lin et al.\nCurrent diversity RL algorithms vary widely due to factors like policy diver-\nsity measurement, optimization techniques, training strategies, and application\nscenarios. This variation makes comparison challenging. While these algorithms\noften incorporate deep neural networks and empirical tests for comparison, they\ntypically lack in-depth theoretical analysis on training convergence and algo-\nrithm complexity, hindering the development of more efficient algorithms.\nTo address the aforementioned issues, we abstract various diversity RL al-\ngorithms, break down the training process, and introduce a unified framework.\nWe offer a convergence analysis for policy population and utilize the contextual\nbandit formulation to design a more efficient diversity RL algorithm, analyz-\ning its complexity. We conclude with visualizations, experimental evaluations,\nand an ablation study comparing training efficiencies of different methods. We\nsummarise our contributions as follows: (1) We investigate recent diversity re-\ninforcement learning algorithms and propose a unified framework. (2) We give\nout the theoretical analysis of the convergence of the proposed framework. (3)\nWe propose a provably efficient diversity reinforcement learning algorithm. (4)\nWe conduct numerical experiments to verify the effectiveness of our method.\n2\nRelated Work\nDiversity Reinforcement Learning Recently, many researchers are com-\nmitted to the design of diversity reinforcement learning algorithms [7,19,11,4].\nDIYAN [7] is a classical diversity RL algorithm, which learns maximum en-\ntropy policies via maximizing the mutual information between states and skills.\nBesides, [19] trains agents with latent conditioned policies which make use of\ncontinuous low-dimensional latent variables, thus it can obtain infinite quali-\nfied solutions. More recently, RSPO [26] obtains diverse behaviors via iteratively\noptimizing each policy. DGPO [4] then proposes a more efficient diversity RL\nalgorithm with a novel diversity reward via sharing parameters between policies.\nBandit Algorithms The challenge in multi-armed bandit algorithm design is\nbalancing exploration and exploitation. Building on ϵ-greedy[22], UCB algorithms[1]\nintroduce guided exploration. Contextual bandit algorithms, like [18,14], im-\nprove modeling for recommendation and reinforcement learning. They demon-\nstrate better convergence properties with contextual information[5,14]. Extensive\nresearch[2] provides regret bounds for these algorithms.\n3\nPreliminaries\nMarkov Decision Process We consider environments that can be repre-\nsented as a Markov decision process (MDP). An MDP can be represented as\na tuple (S, A, PT , r, γ), where S is the state space, A is the action space and\nγ ∈[0, 1) is the reward discount factor. The state-transition function PT (s, a, s′) :\nS × A × S 7→[0, 1] defines the transition probability over the next state s′ af-\nter taking action a at state s. r(s, a) : S × A →R is the reward function\nStudy on Diverse Policies\n3\nTable 1: Comparison of different diversity algorithms.\nMethod Citation Policy Selection\nReward Calculation\nRSPO\n[26]\nIteration Fashion Behavior-driven / Reward-driven exploration\nSIPO\n[9]\nIteration Fashion\nBehavior-driven exploration\nDIAYN\n[7]\nUniform Sample\nI(s; z)\nDSP\n[25]\nUniform Sample\nI(s, a; z)\nDGPO\n[4]\nUniform Sample\nminz′̸=z DKL(ρπθ(s|z)||ρπθ(s|z′))\nOur work\nBandit Selection\nAny form mentioned above\ndenoting the immediate reward received by the agent when taking action a\nin state s. The discounted state occupancy measure of policy π is denoted as\nρπ(s) = (1 −γ) P∞\nt=0 γtP π\nt (s), where P π\nt (s) is the probability that policy π visit\nstate s at time t. The agent’ objective is to learn a policy π to maximize the\nexpected accumulated reward J(θ) = Ez∼p(z),s∼ρπ(s),a∼π(·|s,z)[P\nt γtr(st, at)]. In\ndiversity reinforcement learning, the latent conditioned policy is widely used. The\nlatent conditioned policy is denoted as π(a|s, z), and the latent conditioned critic\nnetwork is denoted as V π(s, z). During execution, the latent variable z ∼p(z) is\nrandomly sampled at the beginning of each episode and keeps fixed for the entire\nepisode. When the latent variable z is discrete, it can be sampled from a categor-\nical distribution with Nz categories. When the latent variable z is continuous, it\ncan be sampled from a Gaussian distribution.\n4\nMethodology\nIn this section, we will provide a theoretical analysis of diversity algorithms in\ndetail. Firstly, in section 4.1, we propose a unified framework for diversity al-\ngorithms, and point out major differences between diversity algorithms in this\nunified framework. Then we prove the convergence of diversity algorithms in\nsection 4.2. We further formulate the diversity optimization problem as a con-\ntextual bandit problem, and propose bandit selection in section 4.3. Finally,\nwe provide rigorous proof for regret bound of bandit selection in section 4.4.\n4.1\nA Unified Framework for Diversity Algorithms\nAlthough there has been a lot of work on exploring diversity, we find that these\nalgorithms lack a unified framework. So we propose a unified framework for\ndiversity algorithms in Algorithm 1 to pave the way for further research.\nWe use Div to measure the diversity distance between two policies and we\nabbreviate policy πθ(·|s, zi) as πi. Vector zi can be thought of as a skill unique\nto each policy πi. Moreover, we define U ∈RN×N as diversity matrix where\nUij = Div(πi, πj) and N denotes the number of policies.\nFor each episode, we first sample zi to decide which policy to update. Then we\n4\nFanqi Lin et al.\nAlgorithm 1 A Unified Framework for Diversity Algorithms\nInitialize: πθ(·|s, z); U ∈RN×N(Uij = Div(πi, πj))\nfor each episode do\nSample zi ∼SelectZ(U);\nGet trajectory τ from πi;\nGet rin = CalR(τ) and update U;\nStore tuple (s, a, s′, rin, zi) in replay buffer D;\nUpdate πi with D;\nend for\ninteract the chosen policy with the environment to get trajectory τ, which is used\nto calculate intrinsic reward rin and update diversity matrix U. We then store\ntuple (s, a, s′, rin, zi) in replay buffer D and update πi through any reinforcement\nlearning algorithm.\nHere we abstract the procedure of selecting zi and calculating rin as SelectZ\nand CalR functions respectively, which are usually the most essential differences\nbetween diversity algorithms. We summarize the comparison of some diversity\nalgorithms in Table 1. Now we describe these two functions in more detail.\nPolicy Selection. Note that we denote by p(z) the distribution of z. We can\ndivide means to select zi into three categories in general, namely iteration\nfashion, uniform sample and bandit selection:\n(1) Iteration fashion. Diversity algorithms such as RSPO [26] and SIPO [9]\nobtain diverse policies in an iterative manner. In the k-th iteration, policy πk will\nbe chosen to update, and the target of optimization is to make πk sufficiently\ndifferent from previously discovered policies π1, ..., πk−1. This method doesn’t\nensure optimal performance and is greatly affected by policy initialization.\n(2) Uniform sample. Another kind of popular diversity algorithm such as\nDIAYN [7] and DGPO [4], samples zi uniformly to maximize the entropy of p(z).\nDue to the method’s disregard for the differences between policies, it often leads\nto slower convergence.\n(3) Bandit selection. We frame obtaining diverse policies as a contextual\nbandit problem. Sampling zi corresponds to minimizing regret in this context.\nThis approach guarantees strong performance and rapid convergence.\nReward Calculation. Diversity algorithms differ in intrinsic reward calcula-\ntion. Some, like [4,7,19], use mutual information theory and a discriminator ϕ to\ndistinguish policies. DIAYN[7] emphasizes deriving skill z from the state s, while\n[19] suggests using state-action pairs. On the other hand, algorithms like [15,26]\naim to make policies’ action or reward distributions distinguishable, known as\nbehavior-driven and reward-driven exploration. DGPO[4] maximizes the mini-\nmal diversity distance between policies.\nStudy on Diverse Policies\n5\n4.2\nConvergence Analysis\nIn this section, we will show the convergence of diversity algorithms under a rea-\nsonable diversity target. We define P = {π1, π2, ..., πN} as the set of independent\npolicies, or policy population.\nDefinition 1. g : {π1, π2, ..., πN} →RN×N is a function that maps popula-\ntion P to diversity matrix U which is defined in section 4.1. Given a population\nP, we can calculate pairwise diversity distance under a certain diversity metric,\nwhich indicates that g is an injective function.\nDefinition 2. Note that in the iterative process of the diversity algorithm,\nwe update P directly instead of U. So if we find a valid U that satisfies the di-\nversity target, then the corresponding population P is exactly our target diverse\npopulation. We refer to this process of finding P backward as g−1.\nDefinition 3. f : RN×N →R is a function that maps U to a real number.\nWhile U measures the pairwise diversity distance between policies, f measures\nthe diversity of the entire population P. As the diversity of the population in-\ncreases, the diversity metric calculated by f will increase as well.\nDefinition 4. We further define δ-target population set Tδ = {g−1(U)|f(U) >\nδ, U ∈RN×N}. δ is a threshold used to separate target and non-target regions.\nThe meaning of this definition is that, during the training iteration process, when\nthe diversity metric closely related to U exceeds a certain threshold, or we say\nf(U) > δ, the corresponding population P is our target population.\nNote two important points: (1) The population meeting the diversity require-\nment should be a set, not a fixed point. (2) Choose a reasonable threshold δ that\nensures both sufficient diversity and ease of obtaining the population.\nTheorem 1. ( ∂f\n∂U )ij =\n∂f\n∂Uij =\n∂f\n∂Div(πi,πj) > 0, where i, j ∈{1, 2, 3, ..., N}.\nProof. f measures the diversity of the entire population P. When the diversity\ndistance between two policies in a population πi and πj increases, the overall\ndiversity metric f(U) will obviously increase.\nTheorem 2. We can find some special continuous differentiable f that, ∃ε > 0,\ns.t. ( ∂f\n∂U )ij > ε, where i, j ∈{1, 2, 3, ..., N}.\nProof. For example, we can simply define f(U) = P\ni̸=j Uij, where ( ∂f\n∂U )ij = 1.\nSo we can choose threshold 0 < ε < 1, then we can find ( ∂f\n∂U )ij > ε obviously. Of\ncourse, we can also choose other relatively complex f as the diversity metric.\nTheorem 3. There’s a diversity algorithm and a threshold ν > 0. Each time\nthe population P is updated, several elements in U will increase by at least ν in\nterms of mathematical expectation.\nProof. In fact, many existing diversity algorithms already have this property.\nSuppose we currently choose πi to update. For DIAYN [7], Div(πi, πj) and\nDiv(πj, πi)(∀j ̸= i) are increased in the optimization process. And for DGPO [4],\nsuppose policy πj is the closest to policy πi in the policy space, then Div(πi, πj)\nand Div(πj, πi) are increased as well in the optimization process. Apart from\nthese two, there are many other existing diversity algorithms such as [19,26,15]\n6\nFanqi Lin et al.\nthat share the same property. Note that we propose Theorem 3 from the per-\nspective of mathematical expectation, so we can infer that, ∃ν > 0, j ̸= i, s.t.\nDiv(π′i, πj) −Div(πi, πj) > ν, where policy π′i denotes the updated policy πi.\nAnd for k /∈{i, j}, we can assume Uik and Uki are unchanged for simplicity.\nTheorem 4. With an effective diversity algorithm and a reasonable diversity\nδ-target, we can obtain a diverse population P ∈Tδ.\nProof. We denote by P0 the initialized policy population, and we define f0 =\nf(g(P0)). Then ∃M ∈N, s.t. f0 + M · νε > δ. Given Theorem 2 and Theo-\nrem 3, we define PM as the policy population after M iterations, then we have\nf(g(PM)) > f0 + M · νε, which means we can obtain the δ-target policy pop-\nulation in up to M iterations. Or we can say that the diversity algorithm will\nconverge after at most M iterations.\nRemark. Careful selection of threshold δ is crucial for diversity algorithms.\nReasonable diversity goals should be set to avoid difficulty or getting stuck in the\ntraining process. This hyperparameter can be obtained through empirical exper-\niments or methods like hyperparameter search. In certain diversity algorithms,\nboth δ and P may change during training. For instance, in iteration fashion\nalgorithms (Section 4.1), during the k-th iteration, P = {π1, π2, ..., πk} with a\ntarget threshold of δk. If policy πk becomes distinct from π1, ..., πk−1, meeting\nthe diversity target, policy πk+1 is added to P and the threshold changes to\nδk+1.\n4.3\nA Contextual Bandit Formulation\nAs mentioned in Section 4.1, we can sample zi via bandit selection. In this\nsection, we formally define K-armed contextual bandit problem [14], and show\nhow it models diversity optimization procedure.\nAlgorithm 2 A Contextual Bandit Formulation\nInitialize: Arm Set A; Contextual Bandit Algorithm Algo\nfor t = 1, 2, 3, ... do\nObserve feature vectors xt,a for each a ∈A;\nBased on {xt,a}a∈A and reward in previous iterations, Algo chooses an arm at ∈\nA and receives reward rt,at;\nUpdate Algo with (xt,at, at, rt,at);\nend for\nWe show the procedure of the contextual bandit problem in Algorithm 2. In\neach iteration, we can observe feature vectors xt,a for each a ∈A, which are\nalso denoted as context. Note that context may change during training. Then,\nAlgo will choose an arm at ∈A based on contextual information and will receive\nreward rt,at. Finally, tuple (xt,at, at, rt,at) will be used to update Algo.\nWe further define T-Reward [14] of Algo as PT\nt=1 rt. Similarly, we define the\nStudy on Diverse Policies\n7\noptimal expected T-Reward as E[PT\nt=1 rt,a∗\nt ], where a∗\nt denotes the arm with\nmaximum expected reward in iteration t. To measure Algo’s performance, we\ndefine T-regret RT of Algo by\nRT = E[\nT\nX\nt=1\nrt,a∗\nt ] −E[\nT\nX\nt=1\nrt,at].\n(1)\nOur goal is to minimize RT .\nIn the diversity optimization problem, policies are akin to arms, and context is\nrepresented by visited states or ρπ(s). Note that context may change as policies\nevolve. When updating a policy, the reward is the difference in diversity metric\nbefore and after the update, linked to the diversity matrix U (Section 4.1). Our\nobjective is to maximize policy diversity, equivalent to maximizing expected\nreward or minimizing RT in contextual bandit formulation.\nHere’s an example to demonstrate the effectiveness of bandit selection. In\nsome cases, a policy πi may already be distinct enough from others, meaning that\nselecting πi for an update wouldn’t significantly affect policy diversity. To address\nthis, we should decrease the probability of sampling πi. Fixed uniform sampling\nfails to address this issue, but bandit algorithms like UCB[2] or LinUCB[14]\nconsider both historical rewards and the number of times policies have been\nchosen. This caters to our needs in such cases.\n4.4\nRegret Bound\nIn this section, we provide the regret bound for bandit selection in the diversity\nalgorithms.\nProblem Setting. We define T as the number of iterations. In each iteration\nt, we can observe N feature vectors xt,a ∈Rd and receive reward rt,at with\n∥xt,a∥≤1 for a ∈A and rt,at ∈[0, 1], where ∥· ∥means l2-norm, d denotes the\ndimension of feature vector and at is the chosen action in iteration t.\nLinear Realizability Assumption. Similar to lots of theoretical analyses\nof contextual bandit problems [1,5], we propose linear realizability assumption\nto simplify the problem. We assume that there exists an unknown weight vector\nθ∗∈Rd with ∥θ∗∥≤1 s.t.\nE[rt,a|xt,a] = xT\nt,aθ∗.\n(2)\nfor all t and a.\nWe now analyze the rationality of this assumption in practical diversity al-\ngorithms. Reward rt,a measures the changed value of overall diversity metric\n△f(U) of policy population P after an update. Suppose πi\nt is the policy corre-\nsponding to the feature vector xt,a in the iteration t. While xt,a encodes state\nfeatures of πi\nt, it can encode the diversity information of πi\nt as well. Therefore,\nwe can conclude that rt,a is closely related to xt,a. So given that xt,a contains\nenough diversity information, we can assume that the hypothesis holds.\n8\nFanqi Lin et al.\nTheorem 5. (Diversity Reinforcement Learning Oracle DRLO). Given a rea-\nsonable δ-target and an effective diversity algorithm, let the probability that the\npolicy population P reaches δ-target in T iterations be 1−϶δ,T . Then we have\nlimT →∞϶δ,T = 0.\nProof. This is actually another formal description of the convergence of diversity\nalgorithms which has been proved in Section 4.2. Experimental results [19,4] have\nshown that ϶δ,T will decrease significantly when T reaches a certain value.\nTheorem 6. (Contextual Bandit Algorithm Oracle CBAO). There exists a con-\ntextual bandit algorithm that makes regret bounded by O\n\u0012q\nTdln3(NTln(T)/η)\n\u0013\nfor T iterations with probability 1 −η.\nProof. Different contextual bandit algorithm corresponds to different regret bound.\nIn fact, we can use the regret bound of any contextual bandit algorithm here. The\nregret bound mentioned here is the regret bound of SupLinUCB algorithm [5].\nFor concrete proof of this regret bound, we refer the reader to [5].\nTheorem 7. For T iterations, the regret for bandit selection in diversity al-\ngorithms is bounded by O\n\u0010q\nTdln3( NT ln(T )(1−϶δ,T )\nη−϶δ,T\n)\n\u0011\nwith probability 1−η. Note\nthat limT →∞(η−϶δ,T ) = η > 0.\nProof. In diversity algorithms, the calculation of the regret bound is based on the\npremise that a certain δ-target has been achieved. Note that DRLO and CBAO\nare independent variables in this problem setting. Given 0 < η < 1, we define\nη1 = η−϶δ,T\n1−϶δ,T\n.\n(3)\nThen we have\n1 −η = (1−϶δ,T )(1 −η1).\n(4)\nThe implication of Equation 4 is that, for T iterations, with probability 1 −η,\nthe regret for bandit selection in diversity algorithms is bounded by\nO\n\u0012q\nTdln3(NTln(T)/η1)\n\u0013\n= O\n s\nTdln3(NTln(T)(1−϶δ,T )\nη−϶δ,T\n)\n!\n.\n(5)\nThe right-hand side of Equation 5 is exactly the regret bound we propose in\nTheorem 7.\n5\nExperiments\nThis section presents some experimental results about diversity algorithms. Firstly,\nfrom an intuitive geometric perspective, we demonstrate the process of policy\nevolution in the diversity algorithm. Then we compare the three policy selection\nmethods mentioned in Section 4.1 by experiments, which illustrates the high\nefficiency of bandit selection.\nStudy on Diverse Policies\n9\n(a)\n(b)\nFig. 1: (a) Policy evolution trajectory. We initialize three policies here, denoted\nby red, yellow, and green circles on the simplex. The darker the color of the\npolicy, the more iterations it has gone through, and the greater the diversity\ndistance between this policy and other policies is. Moreover, the blue circles on\nthe simplex denote the average state marginal distribution of policies ρ(s). (b)\nPolicy evolution process. We initialize three policies here as well, denoted by red,\ngreen, and blue dots on the simplex. The black dot denotes the average state\nmarginal distribution of policies ρ(s). Moreover, the contour lines in the figure\ncorrespond to the diversity metric I(s; z).\n5.1\nA Geometric Perspective on Policy Evolution\nTo visualize the policy evolution process, we use DIAYN [7] as our diversity algo-\nrithm and construct a simple 3-state MDP [8] to conduct the experiment. The set\nof feasible state marginal distributions is described by a triangle [(1, 0, 0), (0, 1, 0), (0, 0, 1)]\nin R3. And we use state occupancy measure ρπi(s) to represent policy πi. More-\nover, we project the state occupancy measure onto a two-dimensional simplex\nfor visualization.\nLet ρ(s) be the average state marginal distribution of all policies. Figure 1(a)\nshows policy evolution during training. Initially, the state occupancy measures\nof different policies are similar. However, as training progresses, the policies\nspread out, indicating increased diversity. Figure 1(a) highlights that diversity [8]\nensures distinct state occupancy measures among policies.\nWe use I(·; ·) to denote mutual information. The diversity metric in unsu-\npervised skill discovery algorithms is based on the mutual information of states\nand latent variable z. Furthermore, the mutual information can be viewed as\nthe average divergence between each policy’s state distribution ρ(s|z) and the\naverage state distribution ρ(s) [8]:\nI(s; z) = Ep(z)[DKL(ρ(s|z) ∥ρ(s))].\n(6)\nFigure 1(b) shows the policy evolution process and the diversity metric\nI(s; z). We find that the diversity metric increased gradually during the training\nprocess, which is in line with our expectation.\n10\nFanqi Lin et al.\n(a)\n(b)\nFig. 2: Comparison of different policy selection methods. (a) Training curves for\ndifferent numbers of policies with a fixed δ-target where δ = 0.8. (b) Training\ncurves for different δ-target with a fixed number of policies where N = 8.\n5.2\nPolicy Selection Ablation\nWe continue to use 3-state MDP [8] as the experimental environment. Whereas,\nin order to get closer to the complicated practical environment, we set specific δ-\ntarget and increased the number of policies. Moreover, when a policy that hasn’t\nmet the diversity requirement is chosen to update, we will receive a reward r = 1,\notherwise, we will receive a reward r = 0. We use I(s; z) as the diversity metric\nand use LinUCB[14] as our contextual bandit algorithm.\nFigure 2 shows the training curves under different numbers of policies and\ndifferent δ-target over six random seeds. The results show that bandit selection\nnot only always reaches the convergence fastest, but also achieves the highest\noverall diversity metric of the population when it converges. We now empirically\nanalyze the reasons for this result:\nDrawbacks of uniform sample. In many experiments, we observe that uni-\nform sample has similar final performance to bandit selection, but signifi-\ncantly slower convergence. This is because after several iterations, some policies\nbecome distinct enough to prioritize updating other policies. However, uniform\nsample treats all policies equally, resulting in slow convergence.\nDrawbacks of iteration fashion. In experiments, the iteration fashion con-\nverges quickly but has lower final performance than the other two methods. It’s\ngreatly affected by initialization. Each policy update depends on the previous\none, so poor initialization can severely impact subsequent updates, damaging\nthe overall training process.\nAdvantages of bandit selection. Considering historical rewards and balanc-\nStudy on Diverse Policies\n11\ning exploitation and exploration, bandit selection quickly determines if a policy\nis different enough to adjust the sample’s probability distribution. Unlike iter-\nation fashion, all policies can be selected for an update in a single iteration,\nmaking bandit selection not limited by policy initialization.\n6\nConclusion\nIn this paper, we compare existing diversity algorithms, provide a unified di-\nversity reinforcement learning framework, and investigate the convergence of\ntraining diverse policies. Moreover, we propose bandit selection under our\nproposed framework, and present the regret bound for it. Empirical results indi-\ncate that bandit selection achieves the highest diversity score with the fastest\nconvergence speed compared to baseline methods. We also provide a geometric\nperspective on policy evolution through experiments. In the future, we will focus\non the comparison and theoretical analysis of different reward calculation meth-\nods. And we will continually explore the application of diversity RL algorithms\nin more real-world decision-making tasks.\nReferences\n1. Auer, P.: Using confidence bounds for exploitation-exploration trade-offs. Journal\nof Machine Learning Research 3(Nov), 397–422 (2002)\n2. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed\nbandit problem. Machine learning 47(2), 235–256 (2002)\n3. Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi,\nD., Fischer, Q., Hashme, S., Hesse, C., et al.: Dota 2 with large scale deep rein-\nforcement learning. arXiv preprint arXiv:1912.06680 (2019)\n4. Chen, W., Huang, S., Chiang, Y., Chen, T., Zhu, J.: Dgpo: Discovering multi-\nple strategies with diversity-guided policy optimization. In: Proceedings of the\n2023 International Conference on Autonomous Agents and Multiagent Systems.\npp. 2634–2636 (2023)\n5. Chu, W., Li, L., Reyzin, L., Schapire, R.: Contextual bandits with linear payoff\nfunctions. In: Proceedings of the Fourteenth International Conference on Artificial\nIntelligence and Statistics. pp. 208–214. JMLR Workshop and Conference Proceed-\nings (2011)\n6. Ellis, B., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J.N., White-\nson, S.: Smacv2: An improved benchmark for cooperative multi-agent reinforce-\nment learning. arXiv preprint arXiv:2212.07489 (2022)\n7. Eysenbach, B., Gupta, A., Ibarz, J., Levine, S.: Diversity is all you need: Learning\nskills without a reward function. In: International Conference on Learning Repre-\nsentations (2018)\n8. Eysenbach, B., Salakhutdinov, R., Levine, S.: The information geometry of unsu-\npervised reinforcement learning. In: International Conference on Learning Repre-\nsentations (2021)\n9. Fu, W., Du, W., Li, J., Chen, S., Zhang, J., Wu, Y.: Iteratively learning novel\nstrategies with diversity measured in state distances. Submitted to ICLR 2023\n(2022)\n12\nFanqi Lin et al.\n10. Huang, S., Chen, W., Zhang, L., Li, Z., Zhu, F., Ye, D., Chen, T., Zhu, J.: Tikick:\nTowards playing multi-agent football full games from single-agent demonstrations.\narXiv preprint arXiv:2110.04507 (2021)\n11. Huang, S., Yu, C., Wang, B., Li, D., Wang, Y., Chen, T., Zhu, J.: Vmapd: Generate\ndiverse solutions for multi-agent games with recurrent trajectory discriminators.\nIn: 2022 IEEE Conference on Games (CoG). pp. 9–16. IEEE (2022)\n12. Kumar, S., Kumar, A., Levine, S., Finn, C.: One solution is not all you need:\nFew-shot extrapolation via structured maxent rl. Advances in Neural Information\nProcessing Systems 33, 8198–8210 (2020)\n13. Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P´erolat, J., Silver,\nD., Graepel, T.: A unified game-theoretic approach to multiagent reinforcement\nlearning. Advances in neural information processing systems 30 (2017)\n14. Li, L., Chu, W., Langford, J., Schapire, R.E.: A contextual-bandit approach to per-\nsonalized news article recommendation. In: Proceedings of the 19th international\nconference on World wide web. pp. 661–670 (2010)\n15. Liu, X., Jia, H., Wen, Y., Yang, Y., Hu, Y., Chen, Y., Fan, C., Hu, Z.: Unifying\nbehavioral and response diversity for open-ended learning in zero-sum games. arXiv\npreprint arXiv:2106.04958 (2021)\n16. Mahajan, A., Rashid, T., Samvelyan, M., Whiteson, S.: Maven: Multi-agent vari-\national exploration. arXiv preprint arXiv:1910.07483 (2019)\n17. Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M.,\nHoeller, D., Rudin, N., Allshire, A., Handa, A., et al.: Isaac gym: High performance\ngpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470\n(2021)\n18. May, B.C., Korda, N., Lee, A., Leslie, D.S.: Optimistic bayesian sampling in\ncontextual-bandit problems. Journal of Machine Learning Research 13, 2069–2106\n(2012)\n19. Osa, T., Tangkaratt, V., Sugiyama, M.: Discovering diverse solutions in deep rein-\nforcement learning by maximizing state–action-based mutual information. Neural\nNetworks 152, 90–104 (2022)\n20. Shi, J.C., Yu, Y., Da, Q., Chen, S.Y., Zeng, A.X.: Virtual-taobao: Virtualizing\nreal-world online retail environment for reinforcement learning. In: Proceedings of\nthe AAAI Conference on Artificial Intelligence. vol. 33, pp. 4902–4909 (2019)\n21. Wang, T.T., Gleave, A., Belrose, N., Tseng, T., Miller, J., Dennis, M.D., Duan, Y.,\nPogrebniak, V., Levine, S., Russell, S.: Adversarial policies beat professional-level\ngo ais. arXiv preprint arXiv:2211.00241 (2022)\n22. Watkins, C.J.C.H.: Learning from delayed rewards. Robotics & Autonomous Sys-\ntems (1989)\n23. Xue, W., Cai, Q., Zhan, R., Zheng, D., Jiang, P., An, B.: Resact: Reinforcing long-\nterm engagement in sequential recommendation with residual actor. arXiv preprint\narXiv:2206.02620 (2022)\n24. Yu, C., Yang, X., Gao, J., Yang, H., Wang, Y., Wu, Y.: Learning efficient multi-\nagent cooperative visual exploration. arXiv preprint arXiv:2110.05734 (2021)\n25. Zahavy, T., O’Donoghue, B., Barreto, A., Flennerhag, S., Mnih, V., Singh, S.:\nDiscovering diverse nearly optimal policies with successor features. In: ICML 2021\nWorkshop on Unsupervised Reinforcement Learning (2021)\n26. Zhou, Z., Fu, W., Zhang, B., Wu, Y.: Continuously discovering novel strategies via\nreward-switching policy optimization. In: International Conference on Learning\nRepresentations (2021)\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-08-23",
  "updated": "2023-08-23"
}