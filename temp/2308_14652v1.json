{
  "id": "http://arxiv.org/abs/2308.14652v1",
  "title": "Learning Visual Tracking and Reaching with Deep Reinforcement Learning on a UR10e Robotic Arm",
  "authors": [
    "Colin Bellinger",
    "Laurence Lamarche-Cliche"
  ],
  "abstract": "As technology progresses, industrial and scientific robots are increasingly\nbeing used in diverse settings. In many cases, however, programming the robot\nto perform such tasks is technically complex and costly. To maximize the\nutility of robots in industrial and scientific settings, they require the\nability to quickly shift from one task to another. Reinforcement learning\nalgorithms provide the potential to enable robots to learn optimal solutions to\ncomplete new tasks without directly reprogramming them. The current\nstate-of-the-art in reinforcement learning, however, generally relies on fast\nsimulations and parallelization to achieve optimal performance. These are often\nnot possible in robotics applications. Thus, a significant amount of research\nis required to facilitate the efficient and safe, training and deployment of\nindustrial and scientific reinforcement learning robots. This technical report\noutlines our initial research into the application of deep reinforcement\nlearning on an industrial UR10e robot. The report describes the reinforcement\nlearning environments created to facilitate policy learning with the UR10e, a\nrobotic arm from Universal Robots, and presents our initial results in training\ndeep Q-learning and proximal policy optimization agents on the developed\nreinforcement learning environments. Our results show that proximal policy\noptimization learns a better, more stable policy with less data than deep\nQ-learning. The corresponding code for this work is available at\n\\url{https://github.com/cbellinger27/bendRL_reacher_tracker}",
  "text": "Learning Visual Tracking and Reaching with Deep\nReinforcement Learning on a UR10e Robotic Arm\nColin Bellinger\nDigital Technologies, National Research Council of Canada\ncolin.bellinger@nrc-cnrc.gc.ca\nLaurence Lamarche-Cliche\nSoftware Engineering, Carleton University\nLaurenceLamarcheClic@cmail.carleton.ca\nAbstract\nAs technology progresses, industrial and scientific robots are increasingly being used in diverse\nsettings. In many cases, however, programming the robot to perform such tasks is technically com-\nplex and costly. To maximize the utility of robots in industrial and scientific settings, they require\nthe ability to quickly shift from one task to another. Reinforcement learning algorithms provide the\npotential to enable robots to learn optimal solutions to complete new tasks without directly repro-\ngramming them. The current state-of-the-art in reinforcement learning, however, generally relies\non fast simulations and parallelization to achieve optimal performance. These are often not possible\nin robotics applications. Thus, a significant amount of research is required to facilitate the efficient\nand safe, training and deployment of industrial and scientific reinforcement learning robots. This\ntechnical report outlines our initial research into the application of deep reinforcement learning on\nan industrial UR10e robot. The report describes the reinforcement learning environments created\nto facilitate policy learning with the UR10e, a robotic arm from Universal Robots, and presents\nour initial results in training deep Q-learning and proximal policy optimization agents on the de-\nveloped reinforcement learning environments. Our results show that proximal policy optimization\nlearns a better, more stable policy with less data than deep Q-learning. The corresponding code\nfor this work is available at https://github.com/cbellinger27/bendRL_reacher_tracker\n1\nIntroduction\nMotivated by increasing labour costs and potential improvements in efficiency, a wide variety of scien-\ntific laboratories and industries have adopted the use of robotic arms for production, maintenance and\nservice [BDCSTS21]. Robots are often ideal for repetitive, strenuous, and complex tasks that require\nspeed and precision. To achieve this, however, the current generation of industrial and scientific I robots\nrequire manual programming by robotics engineers. The programmer uses motion planning based on\nforward and inverse kinematics, where forward kinematics combine the direct measurement of all joint\norientations with the lengths of linkages connected to the joints to determine the end-effector position\nand inverse kinematics establish the values of joint positions required to place the end-effector at a\nnew location with a new orientation. To make the incorporation of industrial robots more inclusive,\nmodern systems enable skilled operators to manual manipulation of the robot via a control pendant.\nAlthough this can simplify the process, online manipulation via the control pendant presents its own\nchallenges and limitations [LQK21]. Small and medium-sized institutions face additional barriers to\nthe incorporation of robotic arms related to the cost of implementation and reprogramming the robot\nto handle evolving requirements [PNA+16].\nReinforcement learning (RL) is a form of machine learning, which is applicable to sequential\ndecision-making problems, such as navigating [CFFF19], control [YM22], complex games [SHS+18],\n1\narXiv:2308.14652v1  [cs.AI]  28 Aug 2023\netc. With RL, the agent learns a control policy to complete a task or achieve some goal. Importantly,\nthe learning setup does not require supervised learning and the task can evolve or be adapted as\nrequirements and environments change. As a result, RL offers a general alternative to task-specific\nrobot programming with the potential to simplify the adoption of industrial robots. On the other\nhand, many challenges exist in the applications of RL to real-world robotics. These include the re-\nquirement to ensure the safety of the robot and surroundings during learning, the need to define a\nreward function to effectively direct policy learning, and potentially long training times [SBS+18].\nFigure 1: UR10e Robot used in this work.\nIn this work, we evaluate the ability to learn\nimage-based reaching and tracking asks via re-\ninforcement learning on the UR10e robotic arm\nby Universal Robots.\nThe reaching and track-\ning tasks considered here are inspired by pre-\nvious work in [YM22] with a UR5 robot. The\nUR10e robot used in this work is shown in Figure\n1.\nIn our experiments, we evaluate the poten-\ntial of the off-policy, value-based method Deep\nQ-Learning (DQN), and the on-policy, policy\ngradient method Proximal Policy Optimization\n(PPO). Our results show that PPO learns a bet-\nter, more stable policy with less data than DQN.\nIn order to improve its performance, DQN re-\nquires a more sophisticated exploration strategy\nand non-uniform sampling of its replay buffer to\nfocus updates on the most informative experi-\nence.\n2\nBackground\nRL problems are formally described by a Markov decision process (MDP). An MDP is defined by the\ntuple (S, A, P, R, γ), where S and A are discrete or continuous action- and state-spaces, P is the state\ntransition dynamic which map state-action pairs at time t onto a distribution over next states at time\nt + 1, R is a reward function that emits an instantaneous reward at time t + 1 given the state-action\npair at time t, and γ ∈[0, 1) is a discount factor that balances the importance of immediate versus\nfuture rewards. Setting γ closer to 0 place greater importance on rewards accumulated in the near-\nterm. In RL, P and R are unknown to the agent. The states under the MDP setup satisfy the Markov\nProperty, which dictates that the future is conditionally independent of the past given the present\nstate. If this property does not hold, then the learner faces a partially observable Markov decision\nprocess (POMDP). In addition to the elements of the MDP, A POMDP contains an observation space\nΩ, which alone is not sufficiently informative to select the next action, and a state-action observation\nprobability function O(ot+1|at, st+1). When RL is applied to POMDPs, Ωand O are also unknown to\nthe agent. The agent receives an observation at each time step, t, rather than the underlying state of\nthe system. Image-based RL applications are often partially observable because a single image is not\nsufficient to encode the dynamics of the world and other details that are relevant to action selection.\nIn deep RL, POMDPs are typically handle with a combination of stacking a sufficiently long history\nof state-action pairs, a state encoder and/or a recurrent neural network [IZL+18].\nIn RL, the agent learns a policy, π, through sequential interactions with the environment (the world\noutside of the agent). At each time step, the policy maps the state to an action or distribution over\nactions π : st →at. The sequence of states, actions and rewards, (s0, a0, r1, s1, a1, r2, s2, ...) under a\npolicy π is denoted a trajectory or roll-out. The agent’s objective is to learn a policy π∗that maximizes\nthe return, G,\nπ∗= arg max\nπ\nE[G|π].\n(1)\n2\nThe return, G, is defined as:\nGt =\nt=T −1\nX\nt=0\n= γtrt+1,\n(2)\nwhere T is the length of a finite horizon MDP, or ∞for an infinite horizon MDPs. In the case of\ninfinite horizons γ ∈[0, 1). For more details see [SB18].\n3\nRelated Work\n3.1\nReinforcement Learning\nIn recent years, the combination of reinforcement learning with deep feature learning has produced\nimpressive advancements in the field of RL. Some noteworthy successes in deep RL include agents\nlearning to play Atari 2600 video games directly from images [MKS+15], learning to play and defeat\nthe world champion of Go [SHM+16], and learning to cool large industrial buildings [LPV+22].\nDeep Q-Learning (DQN) provided an early breakthrough in deep reinforcement learning [MKS+13].\nIt was the first deep RL algorithm capable of effectively learning directly from raw visual inputs. DQN\nis a value-based, off-policy RL algorithm that is suitable for discrete action spaces and discrete or\ncontinuous state spaces. DQN utilizes a deep neural network as its state-action value function (Q-\nfunction).\nMore specifically, given a state st, the neural network Q-function predicts the value of\ntaking each possible action a ∈A in st, where the value approximates the expected return starting\nfrom st, taking the action a, and thereafter following policy π. During training, the agent interacts\nwith the environment according to an ϵ-greedy policy. Specifically, 1−ϵ percent of the time, the agent\nselects an action according to its current policy, and ϵ percent of the time the agent selects a random\naction. The aim is to gain experience whilst balancing the exploration-exploitation trade-off. Tuples\nof experience are recorded in a fixed-length experience replay buffer and used to update the Q-function\naccording to the objective:\nLi(θi) = E(s,a,r,s′)\n\u0014\u0012\nyi −Q(s, a; θi)\n\u00132\u0015\n(3)\nwith\nyi = r + γ max\na′ Q(s′, a′; θ−),\n(4)\nwhere θ is the Q network parameters, θ−is the target network parameters and i is the current iteration\nof network updates. The parameters of the target network are copied from the Q network every τ\nstep or slowly shifted towards θ using soft Polyak updating. This, along with the use of the replay\nbuffer, improves the stability of the deep Q network. Other improvements to DQN include Double\nQ-Learning, which improves DQN’s overestimation action values under certain conditions [VHGS16]\nand Dueling DQN which generalizes learning across actions leading to better policy evaluation in the\npresence of many similar-valued actions [WSH+16], and prioritize experience replay which improves\nlearning by replaying important transitions more frequently [SQAS15]. A key advantage of off-policy\nmethods, such as DQN, is sample efficiency. In comparison to on-policy methods, off-policy methods\ngain sample efficiency through the reuse of data collected under previous versions of the policy and\nthe option to curate training batches such that experiences that produced more reward in the past are\nseen more frequently during training.\nProximal Policy Optimisation (PPO) is a policy gradient form of RL algorithm that has many of the\nadvantages of its predecessor, Trust Region Policy Optimization (TRPO), but is easier to implement,\nmore general, and has better sample efficiency [SWD+17]. Unlike DQN, PPO is suitable for discrete\nor continuous state- and action- spaces. A key feature of PPO is the use of clipping in the objective\nfunction to reduce the risk of large policy updates that lead to a collapse in performance. Policy\nupdates in PPO are conducted in an on-policy manner by collecting experience roll-outs under the\n3\ncurrent parameterized policy πθi and updating the policy as:\nθi+1 = arg max\nθ\nEs,a∼πθi\n\u0002\nL(s, a, θi, θ)\n\u0003\n(5)\nusing stochastic gradient ascent. The loss, L is:\nL(s, a, θi, θ) = min\n\u0012πθ(a|s)\nπθi(a|a)\nAπ\nθi(s, a), clip\n\u0000πθ(a|s)\nπθi(a|a)\n, 1 −ϵ, 1 + ϵ\n\u0001\nAπ\nθi(s, a)\n\u0013\n,\n(6)\nwhere A is an estimate of the advantage function, which measures whether or not an action is better\nthan the policy’s default behaviour, and ϵ is a small hyper-parameter that controls the maximum step\nsize of the policy update. A key advantage of PPO over many other RL algorithms is its robustness\nunder a wide range of hyper-parameters. Particularly in comparison with DQN, PPO can require\nsignificantly less hyper-parameter tuning. This is a beneficial feature for many RL applications, such\nas robotics.\nSoft Actor Critic (SAC) is an off-policy algorithm that is suitable for continuous state- and action-\nspaces [HZAL18]. In addition, there is a variation applicable to discrete action-spaces [Chr19]. SAC\nlearns a stochastic policy that is trained to maximize the expected return with entropy regularization.\nIn particular, it learns a parameterized policy πθ and Q-networks Qϕ1 and Qϕ2. SAC uses the clipped\ndouble-Q trick and takes the minimum Q-value of Qϕ1 and Qϕ2 for use in the loss function:\nL(ϕi, D) = E(s,a,r,s′,d)∼D\n\u0014\u0012\nQϕi(s, a) −y(r, s′, d)\n\u00132\u0015\n,\n(7)\nwhere D is the replay buffer, d indicates if s′ is terminal, and:\ny(r, s′, d) = r + γ(1 −d)\n\u0012\nmin Qϕtarget,j(s′, ˜a′) −αlogπθ(˜a′|s′)\n\u0013\n,\n˜a′ ∼πθ(·|s′),\n(8)\nwhere α controls the explore-exploit trade-off. The α value is the main hyperparameter that should be\ntuned in SAC. A larger α encourages more exploration. The policy network, πθ, acts to maximize the\nexpected future return plus expected future entropy. In this way, both future rewards and exploration\nare encouraged. In particular, SAC learns to maximize:\nV π(s) = Ea∼π\n\u0002\nQπ(s, a)\n\u0003\n+ αH(π(·|s)),\n(9)\nwhere H(·) is entropy. In practice, this is optimized using the reparameterization trick where a sample is\ndrawn from πθ(·|s) by computing a deterministic function of state, policy parameters, and independent\nnoise. In addition to the sample efficiency that comes with off-policy learning, SAC has the advantage\nof being relatively insensitive to hyperparameters and balances the exploration-exploitation trade-off\nthrough the use of entropy regularization.\n3.2\nReinforcement Learning with Physical Robots\nAdvancements in RL and the reduced costs of some robotics systems have resulted in a significant\nincrease in the number of RL researchers working on robotics applications.\nThis includes, for ex-\nample, object manipulations skills, [GHLL17, HZAL18], peg insertion [LZS+19], targeted throwing\n[GMKB17], and dexterous manipulation [ZGR+19]. Nonetheless, many significant challenges remain\nfor the widespread application of RL to robotics. These include issues such as image observation and\nmulti-format observation, sample efficiency, sim-to-real training, human-free learning, including safe\nand reset-free learning, generalization across tasks, and learning in the open world.\nIn relation to this work, there have been a few other applications of RL to versions of Universal\nRobots’ UR cobotic arms. This work takes direct inspiration from Yufeng and Mahmood [MKKB18,\nYM22] work on asynchronous RL for real-time control. Yufeng and Mahmood trained SAC RL to\ncontrol a UR5 robot to complete reaching and tracking tasks. In their experimental setup, the target\n4\nis rendered on a computer monitor in front of the robot. Rendering the target on a computer monitor\nhas many practical advantages, such as safe and reset-free learning, whilst maintaining some important\nreal-world robotics challenges. The state-space in this setup is composed of visual inputs from a camera\nmounted on the end-effector and the joint position of the arm. The combination of images and joint\ninformation reduces the risk of partial observability. In [MTR+17], RL is applied to learn a control\npolicy to enable a UR5 robot to complete the wire rope problem. The authors in [FTCG20], compared\nTrust Region Policy Optimization and Deep Q-Network with Normalized Advantage Functions to Deep\nDeterministic Policy Gradient and Vanilla Policy Gradient in simulation and with transfer to UR5.\nUR3e robots were the subject of RL learning the peg in the hole task in [BHPRAH20], pick and place\nin [GMLW22] and goal-based RL [PM21].\n4\nRL Environment\nIn the following experiments, a UR10e robot from Universal Robots learns to carry out reaching and\ntracking tasks from visual inputs. The inputs are captured by a Point Grey camera mounted on the\nend-effector of the robotic arm. Tracking and reaching are performed with respect to a digital target\nrendered on a 1920x1200 monitor fixed at a distance of 70 cm from the robot. The target is a red\ncircle on a white background. The setup is shown in Figure 1. In order to obtain results that would\ntransfer more easily to real-world applications, the background of the room is unobstructed and total\nlight in the room can vary due to changes in the outdoor light through the North facing windows. The\nspecifics of the physical reacher and tracker RL environment are described in the following sections.\nIn both the reacher and tracker tasks, the robotic arm has a fixed start position at the beginning\nof each episode. In the reacher task, the target location can be set to fixed or be randomly reset at the\nbeginning of each episode. Alternatively, in the tracker task, the target is initialized at the centre of\nthe monitor at the beginning of each episode. The target slowly drifts in one of 4 coordinate directions\nout from the centre of the monitor at a rate of 3 pixels per RL time step. The drift direction of the\ntarget is uniformly sampled at the beginning of each episode. The monitor frame boundaries reflect\nthe target by reversing the direction on one axis.\n4.0.1\nAction Space\nThe UR10e robotic arm has 6 degrees of freedom (base, shoulder, elbow and 3 wrist joints). The\naction space is composed of 10 discrete actions. These rotate one of of 5 joints either clockwise or\ncounterclockwise (the third wrist would only rotate the camera, and therefore it was not included in\nthe action space). Each joint’s rotation actions are set proportionally to the size of the joint, with\nlarger joints having slightly larger rotation actions. In particular, the base rotates 0.025 radians (rads),\nthe shoulder and elbow rotate 0.02 rads, and wrists 1 and 2 rotate 0.015 rads. The agent selects one\njoint to move per time step in either the positive or negative direction from its current position.\nFor safety purposes, prior to sending a selected action to the robot via the UR10e’s movej API\ncommand, the RL environment validates the outcome of the move. The validation checks if the move\nwill cause the arm to move out of the defined Cartesian limits. If the action is deemed to violate the\nlimits, the environment steps forward without the robot moving.\n4.0.2\nDynamics of the Environment\nThe environment is episodic. One time step in the environment includes one synchronous interaction\nbetween the agent and the environment. In particular, this involves: 1) the environment calculating\nthe current observation and reward, 2) sending the observation and reward to the agent, 3) the agent\nselecting the next action, 4) the agent sending the action to the environment, and 5) the environment\n5\nFigure 2: This figure displays the agent-environment-hardware interaction framework.\ncommanding the appropriate move of the robot. The interaction framework is visually presented in\nFigure 2.\nEach episode starts the robotic arm in a fixed joint position within the Cartesian boundaries. The\nepisode ends when the agent has achieved the goal. For training purposes, we truncated the episode\nafter 150 time steps if the agent as not achieved the goal.\nIf the agent selects an action that will trigger a protective stop due to a rotation or boundary\nlimit, the action is not initiated. Thus, the agent remains in its current position and state does not\nchange. An observation from this location is returned to the agent. All other actions cause the arm to\nmove a predefined amount via the movej command. The camera observation is made after the move\nis completed.\n4.0.3\nReward\nThe goal in each task is for the agent to learn to move camera on the end of the robotic arm as close\nas possible to the target and centre the target in the camera frame. A shaped rewards schemes is\nevaluated for this purpose. Arrival at the goal is determined by the size and centering of the target\nin the current observation frame. Given the frame size of 400x300 and that the target is included in\nthe observation, the maximum distance between the centre of the frame and the centre of the visible\ntarget is 250 pixels. The maximum radius of the target is 40 pixels and the minimum radius is 10\npixels. For the goal to be achieved, the target must have a radius greater than 30 pixels and be less\nthan 70 pixels from the centre of the frame.\nIn the shaped reward setting, the agent receives a reward of 20 for achieving the goal. A reward\nof -0.01 is given if the target is not in the frame. If the target is in the frame, but the goal has not\nbeen achieved, intermediate rewards between zero and one are given. The intermediate rewards are\nincreased based on how centred the target is and how close the camera is to the target. These are\ncalculated as:\ndist reward = −1 × dist to goal/max dist to goal\n(10)\nradius reward = (target radius −max radius)/(max radius −min radius)\n(11)\nreward = radius reward + dist reward\n(12)\nIf the agent selects an action that will trigger a protective stop due to a rotation or boundary limit,\n1 is subtracted from the reward in order to discourage the agent from hitting the limits.\n6\nSplit channels\nThreshold \nchannels\nMask\nImage \nprocessing\n-\nFind the radius of the circle \n-\nFind the distance from the centre of \nthe frame\nCalculate \nreward \n-\n = -1 x (dist_to_goal/max_goal_dist) +  \n(radius - max_radius)/(max_radius - min_radius) \nRt\n \nRt\nFigure 3: This figure displays the image processing steps to determine if the target is in the current\nframe, along with the size and location of the target if it is present.\n4.0.4\nImage Processing\nA significant challenge in RL with image observations is calculating the reward. In this setup, the\nreward is based on the size and centring of the target (red circle) in the image captured by the camera\non the robotic arm. Therefore, in order to compute the reward at each time step, the environment\nuses image processing to determine if the target is in the current frame. If the target is in the frame,\nthe environment determines the radius and location of the target to be used in the reward calculation\ndiscussed above.\nThere are multiple approaches that could be used to detect and quantify the target circle. In this\nwork, we use openCV’s HoughCircles function, which uses the Hough transform [DH72], to identify\ncircles in the observation image at each time step. The sensitivity and specificity of circle detection\nare controlled by four hyperparameters. In the initial experiments, the HoughCircles function was\noptimized on raw image observations.\nThe experimental setup includes many factors that can be\nexpected in open-world robotics, such as variable lighting, background noise containing a mix of\nobjects, textures and colours, and dynamically changing angles and distances between the camera\nand target. Each of these factors contributes to poor sensitivity and specificity with HoughCircles\nfunction. In addition, the background has circular objects that needed to be ignored during the target\nidentification for reward calculation. Therefore, the RL algorithm must be robust to some reward\nsignal noise.\nAs a result, a multi-step process to simplify the image observation by removing background details\nis used. This serves to significantly reduce false positives in the reward calculation. The steps are\npresented in Figure 3. First, the observation image is split into its three component channels (red,\ngreen and blue). Each channel has a threshold applied which maps all pixels, except those with nearly\ntotal presence, to total absence. The result of this is that most background details are removed, and in\nthe case of the blue and green channels, the target circle is also removed. This can be seen in the images\nafter the thresholding step in the figure. Next, the thresheld blue and green channels are applied as\nmasks to the thresheld red channel. The resulting red channel has nearly all of the background details\nremoved, with the target clearly intact. The final steps are to apply the HoughCircles function and\ncalculate the reward as described in the previous subsection.\n7\n5\nExperimental Setup\nWe assess the performance of PPO and DQN on the reacher and tracker environments1. In each case,\nthe agent is trained end-to-end from scratch. Each agent is evaluated over 3 independent trials. PPO is\ntrained over 40k time steps and DQN is trained over 60k time steps using the same CNN architecture.\nDQN uses a replay buffer of 20k and linear decay in the ϵ value from 1 to 0.01.\nThe results are graphically presented as the mean and standard error of reward per episode step.\nIn addition, the steps per episode over the course of learning is plotted as an alternative perspective\non performance. Fewer steps per episode indicate better performance.\nAll of the experiments utilize a HP Z6 G4 Workstation including Intel Xeon(R) Silver 4214R\nCPU @ 2.40GHz × 48 with the Ubuntu 22.04 operating system. Universal robot UR10e is running\nthe URSoftware version 5.12. Communication between the Linux desktop an the UR10e robot uses\nur rtde 1.5.5 software2.\n6\nResults and Discussion\nFigure 4 includes the learning curves for DQN and PPO on each environment in terms of mean reward\n(left) and mean steps per episode (right). The plots show that both RL algorithms improve their\npolicies over the course of training to increase the mean reward. After 40k steps, however, PPO has\na much higher mean reward than DQN on all three tasks. After an additional 20k of training, DQN\nis approximately equivalent on reaching and tracking, and remains much worse than PPO (with 40k\nof training) on the static reaching task. This suggests that epsilon greedy exploration and uniform\nsampling from DQN’s replay buffer are likely causing slow learning. Thus alternative strategies are\nneeded to improve exploration and focus policy updates on the most valuable experience.\nThe episode length plots demonstrate if, and how quickly, the agents are able to achieve the goal on\neach task. The most striking result is that although the reward results show that DQN learns to focus\non the target in the static reacher task, it fails to achieve the goal within the maximum episode length\nof 150 steps. Alternatively, PPO learns to achieve the goal in a mean of 110 steps and is still improving\nits policy when training was stopped. Given that PPO has superior performance, the remainder of the\nanalysis focuses on it.\nIn comparing the performance of PPO across the three environment types, the plots illustrate that\nthe agents are best at the static reacher, where the target is always located at the centre of the monitor.\nAlternatively, the reacher environment, where the target is randomly re-positioned at the beginning\nof each episode, is the most challenging of the three environments. The tracker environment, which\nhas the target start each episode at the centre of the monitor and slowly drifts away, falls between\nstatic reacher and reacher in terms of complexity for PPO. The ordering of performance is likely due\nto the fact that in static reacher and tracker, the target is always initialized in the same location. As\na result, the agent can quickly learn the general area that it should move towards at the beginning of\neach episode. Alternatively, because the target is randomly positioned at the beginning of each reacher\nepisode, the agent must search for and find the target before it can move it on the target. Due to the\nnature of on-policy learning, the agent may be biased to search areas where the target occurred in the\nmore recent updates and slowly forget pasted experience. In terms of training time, the clock time is\napproximately 3:30 hours for each agent.\nFigures 5, 6 and 7 show 3 episodes of evaluation rollouts for PPO agents at the end of training\non the static reacher, reacher and tracker environments. The roll-outs are depicted with two images\ncaptured on 5-second intervals. The embedded image is the observation received by the agent and the\nlarger image is from a separate camera used to record the robot and the target. For each environment,\n1The corresponding code for this work is available at https://github.com/cbellinger27/bendRL_reacher_tracker\n2https://pypi.org/project/ur-rtde/\n8\nFigure 4: DQN (top row) and PPO (bottom row) results. The left plots in each row depict the mean\nrewards on the tracking, reaching and static reaching environments. The right plots in each row present\nthe number of steps per episode in each environment.\neach episode commences with the agent looking away from the target and the monitor. Within the\nfirst 5 seconds, the agent locates the target, and once located the agent begins to move towards the\ntarget whilst keeping the target as centered as possible in its field of view. This is easiest in the static\nreacher environment where the target is always in the same location, and most challenging in the\nreacher environment.\nIn Figure 5, the pattern of the size and location of the target is very similar in each evaluation\nroll-out. This suggests that the PPO agent has learned a stable and robust policy. An additional\nsign of the agent’s efficacy on this task is that after just 20 seconds the target size is quite larger.\nAlternatively, in the reacher environment after 35 seconds the target still appears relatively small and\nsometimes poorly centered. More learning time is clearly needed to deal with the larger problem.\nFigure 7 highlights the fact that the complexity of the tracker environment is moderate compared\nto the static reacher and reacher. In particular, the agent consistently locates and centres the target\nwithin the first 5 seconds, similar to static reacher and unlike reacher, which sometimes takes more\nthan 5 seconds (depending on the target location.) Although the target is typically centered after 35\nseconds, it is not nearly as large as in the case of static reacher. This is attributed to the fact that\nthe agent must learn to approach and centre the drifting target. The precision of the agent is also\nlimited by size of the discrete movements in the environment. Once again, additional training time\nwould allow for further policy refinements on this task.\n6.1\nLimitations and Challenges\nThe implementation of the RL environment has some limitations, such as the synchronization of the\nagent and the environment. Moreover, from the agent’s perspective time advances in discrete steps.\nThis simplifies the tracker tasks because the target only moves as fast as the agent can select actions. In\nsubsequent versions of the environment, the target will drift at a rate that is independent of the agent’s\naction selection rate. As described above, for example, the agent faces a discrete action space that\nlimits the movement of the robot at each time step to a single joint and a fixed step size. Subsequent\nversions of the environment will add the option to use a continuous action space. A continuous action\nspace will provide numerous advantages, including the ability to learn a smooth control policy and\n9\nFigure 5: This figure displays three policy roll-outs for PPO on the static reacher environment after\ntraining is completed. It demonstrates that the agent learned to find the target and then navigate\ntoward the target while keeping the target centered in the field of view.\nFigure 6: This figure displays three policy roll-outs for PPO on the reacher environment after training\nis completed. It demonstrates that the agent learned to find the target and then navigate toward the\ntarget while keeping the target centered in the field of view.\na wider choice of additional RL algorithms. Finally, the observation space is composed of one image\ncaptured by the camera on the end-effector of the robotic arm. A single image state space, however,\nmay not include all of the information relevant to reacher and tracker tasks. In the case of tracker,\nfor example, a single image does not provide information about which direction the target is moving.\nThus, the problem can be considered partially observable [Kri16]. This can be handled by using the\ncombined recent history of state (or states and actions) as the input to the agent’s policy [MKS+13],\nor deep partially observable RL algorithms [IZL+18]. In addition, the physical robot has limits on its\njoint positions. Based on image states alone, however, the agent may not have sufficient information\nto learn these limits. The subsequent implementation of the environment will include the option to\ninclude both the current image and joint position in the state space.\nReward calculation from raw images is an open and challenging problem in RL. It will become\nrelevant as more RL is trained in the real-world. In this work, the reward is defined based on the\nsize and centering of the target in the frame. Unlike the previous work, the background in the image\nis complex, the lighting is inconsistent and the agent has the ability to look away from the monitor\ncontaining the target. As a result, image processing needs to be applied at each time step to determine\n10\nFigure 7: This figure displays three policy roll-outs for PPO on the tracker environment after training\nis completed. It demonstrates that the agent learned to find the target and then navigate toward the\ntarget while keeping the target centered in the field of view.\nif a reward should be given. This represents additional computational resources and requires problem-\nspecific engineering of the reward function. Moreover, due to the limitations of image processing under\nthese conditions, the rewards issued to the agent are noisy thereby increasing the learning challenge.\nDRL algorithms suffer from poor sample efficiency [Yu18]. Sample inefficiency occurs in two con-\ntexts: a) train RL policy from scratch requires the agent to collect a large number of experience tuples\n(state, action, reward, next state), and b) the agent must be trained from scratch multiple times in\norder to find the best hyper-parameters for the given task. During online training, the robot cannot\nbe used for other productive tasks. Future work will explore strategies for better robot utilization and\nsample efficiency.\nSample efficiency is improved with off-policy algorithms that allow for the reuse of experience\n[VHGS16], the prioritization of particularly rewarding experience[SQAS15], and the distribution of\nagent learning across multiple robots[GHLL17].\nIn this work, however, the sensitivity of DQN to\nhyper-parameters necessitated significantly more training time than PPO. More generally, sim-2-real\noffers the potential to initialize the RL via faster, lower-cost interactions with a simulated robot, and\nthen refine the policy on the real-world robot [TZC+18, YTB+20]. Similarly, model-based RL can be\nused for on- and off-policy RL to improve sample efficiency [NYA+18, YCI+20].\nOther improvements may stem from self-supervised learning to reduce the agent’s reliance on\nheavily engineered, problem-specific rewards [LPK+18, PG16, ITF+21] and the use of frame skipping\nand observation skipping to reduce observation and decision costs in predictable regions of the state\nspace [LSR17, BCT23].\n7\nConclusion\nThis work explored the potential of reinforcement learning algorithms to learn optimal control policies\nfor vision-based reacher and tracker tasks with a UR10e robotic arm. We describe the experimental\nsetting and results of online policy learning with DQN and PPO. Whereas much of the research\non vision-based RL is undertaken in highly controlled settings, here we train the agents in a complex\nphysical setting with variable light conditions. We find that in these conditions the image-based reward\ncalculations can be very sensitive to the background and variable light leading to a noisy reward signal.\nCareful image processing prior to reward calculation is required to minimize the risk of noise that can\nharm policy learning.\n11\nOur results show that proximal policy optimization learns a better, more stable policy with less\ndata than deep Q-learning. In order to improve its performance, we expect that DQN requires a more\nsophisticated exploration strategy and non-uniform sampling of its replay buffer to focus updates on\nthe most informative experience. Finally, we highlight general challenges and future directions for\nthe study of reinforcement learning in vision-based open-world robotics as: i) reducing the reliance\non heavily engineered, domain-dependent rewards, ii) lowering the dependence on computationally\nexpensive image processing for reward calculation, and iii) improving the robustness to the potential\nfor degraded learning due to a noisy reward signal resulting from imprecision in the imaged-based\nreward calculations.\nReferences\n[BCT23]\nColin Bellinger, Mark Crowley, and Isaac Tamblyn. Dynamic observation policies in ob-\nservation cost-sensitive reinforcement learning. arXiv preprint arXiv:2307.02620, 2023.\n[BDCSTS21]\nMar´ıa Teresa Ballestar, ´Angel D´ıaz-Chao, Jorge Sainz, and Joan Torrent-Sellens. Impact\nof robotics on manufacturing: A longitudinal machine learning perspective. Technolog-\nical Forecasting and Social Change, 162:120348, 2021.\n[BHPRAH20] Cristian C Beltran-Hernandez, Damien Petit, Ixchel G Ramirez-Alpizar, and Ken-\nsuke Harada. Variable compliance control for robotic peg-in-hole assembly: A deep-\nreinforcement-learning approach. Applied Sciences, 10(19):6923, 2020.\n[CFFF19]\nHao-Tien Lewis Chiang, Aleksandra Faust, Marek Fiser, and Anthony Francis. Learning\nnavigation behaviors end-to-end with autorl. IEEE Robotics and Automation Letters,\n4(2):2007–2014, 2019.\n[Chr19]\nPetros Christodoulou.\nSoft actor-critic for discrete action settings.\narXiv preprint\narXiv:1910.07207, 2019.\n[DH72]\nRichard O Duda and Peter E Hart. Use of the hough transformation to detect lines and\ncurves in pictures. Communications of the ACM, 15(1):11–15, 1972.\n[FTCG20]\nAndrea Franceschetti, Elisa Tosello, Nicola Castaman, and Stefano Ghidoni. Robotic\narm control and task training through deep reinforcement learning.\nCoRR,\nabs/2005.02632, 2020.\n[GHLL17]\nShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement\nlearning for robotic manipulation with asynchronous off-policy updates. In 2017 IEEE\ninternational conference on robotics and automation (ICRA), pages 3389–3396. IEEE,\n2017.\n[GMKB17]\nAli Ghadirzadeh, Atsuto Maki, Danica Kragic, and M˚arten Bj¨orkman. Deep predictive\npolicy training using reinforcement learning. In 2017 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages 2351–2358. IEEE, 2017.\n[GMLW22]\nNatanael Magno Gomes, Felipe Nascimento Martins, Jos´e Lima, and Heinrich W¨ortche.\nReinforcement learning for collaborative robots pick-and-place applications: A case\nstudy. Automation, 3(1):223–241, 2022.\n[HZAL18]\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic:\nOff-policy maximum entropy deep reinforcement learning with a stochastic actor. In\nInternational conference on machine learning, pages 1861–1870. PMLR, 2018.\n[ITF+21]\nJulian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, and Sergey\nLevine. How to train your robot with deep reinforcement learning: lessons we have\nlearned. The International Journal of Robotics Research, 40(4-5):698–721, 2021.\n12\n[IZL+18]\nMaximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep\nvariational reinforcement learning for pomdps. In International Conference on Machine\nLearning, pages 2117–2126. PMLR, 2018.\n[Kri16]\nVikram Krishnamurthy. Partially observed Markov decision processes. Cambridge uni-\nversity press, 2016.\n[LPK+18]\nSergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learn-\ning hand-eye coordination for robotic grasping with deep learning and large-scale data\ncollection. The International journal of robotics research, 37(4-5):421–436, 2018.\n[LPV+22]\nJerry Luo, Cosmin Paduraru, Octavian Voicu, Yuri Chervonyi, Scott Munns, Jerry Li,\nCrystal Qian, Praneet Dutta, Jared Quincy Davis, Ningjia Wu, et al. Controlling com-\nmercial cooling systems using reinforcement learning. arXiv preprint arXiv:2211.07357,\n2022.\n[LQK21]\nAndrew Lobbezoo, Yanjun Qian, and Hyock-Ju Kwon. Reinforcement learning for pick\nand place operations in robotics: A survey. Robotics, 10(3):105, 2021.\n[LSR17]\nAravind Lakshminarayanan, Sahil Sharma, and Balaraman Ravindran. Dynamic action\nrepetition for deep reinforcement learning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 31, 2017.\n[LZS+19]\nMichelle A Lee, Yuke Zhu, Krishnan Srinivasan, Parth Shah, Silvio Savarese, Li Fei-Fei,\nAnimesh Garg, and Jeannette Bohg. Making sense of vision and touch: Self-supervised\nlearning of multimodal representations for contact-rich tasks.\nIn 2019 International\nConference on Robotics and Automation (ICRA), pages 8943–8950. IEEE, 2019.\n[MKKB18]\nA Rupam Mahmood, Dmytro Korenkevych, Brent J Komer, and James Bergstra. Set-\nting up a reinforcement learning task with a real-world robot.\nIn 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pages 4635–4640.\nIEEE, 2018.\n[MKS+13]\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,\nDaan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning.\narXiv preprint arXiv:1312.5602, 2013.\n[MKS+15]\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al.\nHuman-level control through deep reinforcement learning.\nnature,\n518(7540):529–533, 2015.\n[MTR+17]\nRichard Meyes, Hasan Tercan, Simon Roggendorf, Thomas Thiele, Christian B¨uscher,\nMarkus Obdenbusch, Christian Brecher, Sabina Jeschke, and Tobias Meisen. Motion\nplanning for industrial robots using reinforcement learning. Procedia CIRP, 63:107–112,\n2017.\n[NYA+18]\nAnusha Nagabandi, Guangzhao Yang, Thomas Asmar, Ravi Pandya, Gregory Kahn,\nSergey Levine, and Ronald S Fearing. Learning image-conditioned dynamics models for\ncontrol of underactuated legged millirobots. In 2018 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages 4606–4613. IEEE, 2018.\n[PG16]\nLerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from\n50k tries and 700 robot hours. In 2016 IEEE international conference on robotics and\nautomation (ICRA), pages 3406–3413. IEEE, 2016.\n[PM21]\nRoman Parak and Radomil Matousek. Comparison of multiple reinforcement learning\nand deep reinforcement learning methods for the task aimed at achieving the goal. In\nMendel, volume 27, pages 1–8, 2021.\n13\n[PNA+16]\nMikkel Rath Pedersen, Lazaros Nalpantidis, Rasmus Skovgaard Andersen, Casper\nSchou, Simon Bøgh, Volker Kr¨uger, and Ole Madsen. Robot skills for manufacturing:\nFrom concept to industrial deployment. Robotics and Computer-Integrated Manufac-\nturing, 37:282–291, 2016.\n[SB18]\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT\npress, 2018.\n[SBS+18]\nNiko S¨underhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox, J¨urgen Leit-\nner, Ben Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, et al. The limits\nand potentials of deep learning for robotics. The International journal of robotics re-\nsearch, 37(4-5):405–420, 2018.\n[SHM+16]\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc\nLanctot, et al. Mastering the game of go with deep neural networks and tree search.\nnature, 529(7587):484–489, 2016.\n[SHS+18]\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai,\nArthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al.\nA general reinforcement learning algorithm that masters chess, shogi, and go through\nself-play. Science, 362(6419):1140–1144, 2018.\n[SQAS15]\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience\nreplay. arXiv preprint arXiv:1511.05952, 2015.\n[SWD+17]\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Prox-\nimal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[TZC+18]\nJie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven\nBohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped\nrobots. arXiv preprint arXiv:1804.10332, 2018.\n[VHGS16]\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with\ndouble q-learning.\nIn Proceedings of the AAAI conference on artificial intelligence,\nvolume 30, 2016.\n[WSH+16]\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Fre-\nitas. Dueling network architectures for deep reinforcement learning. In International\nconference on machine learning, pages 1995–2003. PMLR, 2016.\n[YCI+20]\nYuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sind-\nhwani. Data efficient reinforcement learning for legged robots. In Conference on Robot\nLearning, pages 1–10. PMLR, 2020.\n[YM22]\nYufeng Yuan and A. Rupam Mahmood. Asynchronous reinforcement learning for real-\ntime control of physical robots.\nIn 2022 International Conference on Robotics and\nAutomation (ICRA), pages 5546–5552, 2022.\n[YTB+20]\nWenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, and Sehoon Ha.\nLearning fast\nadaptation with meta strategy optimization. IEEE Robotics and Automation Letters,\n5(2):2950–2957, 2020.\n[Yu18]\nYang Yu. Towards sample efficient reinforcement learning. In IJCAI, pages 5739–5743,\n2018.\n[ZGR+19]\nHenry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar.\nDexterous manipulation with deep reinforcement learning: Efficient, general, and low-\ncost. In 2019 International Conference on Robotics and Automation (ICRA), pages\n3651–3657. IEEE, 2019.\n14\n",
  "categories": [
    "cs.AI",
    "cs.RO",
    "68T40",
    "I.2; I.4; J.2"
  ],
  "published": "2023-08-28",
  "updated": "2023-08-28"
}