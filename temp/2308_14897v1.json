{
  "id": "http://arxiv.org/abs/2308.14897v1",
  "title": "Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning",
  "authors": [
    "Hanhan Zhou",
    "Tian Lan",
    "Vaneet Aggarwal"
  ],
  "abstract": "Offline reinforcement learning aims to utilize datasets of previously\ngathered environment-action interaction records to learn a policy without\naccess to the real environment. Recent work has shown that offline\nreinforcement learning can be formulated as a sequence modeling problem and\nsolved via supervised learning with approaches such as decision transformer.\nWhile these sequence-based methods achieve competitive results over\nreturn-to-go methods, especially on tasks that require longer episodes or with\nscarce rewards, importance sampling is not considered to correct the policy\nbias when dealing with off-policy data, mainly due to the absence of behavior\npolicy and the use of deterministic evaluation policies. To this end, we\npropose DPE: an RL algorithm that blends offline sequence modeling and offline\nreinforcement learning with Double Policy Estimation (DPE) in a unified\nframework with statistically proven properties on variance reduction. We\nvalidate our method in multiple tasks of OpenAI Gym with D4RL benchmarks. Our\nmethod brings a performance improvements on selected methods which outperforms\nSOTA baselines in several tasks, demonstrating the advantages of enabling\ndouble policy estimation for sequence-modeled reinforcement learning.",
  "text": "Statistically Efficient Variance Reduction with Double\nPolicy Estimation for Off-Policy Evaluation in\nSequence-Modeled Reinforcement Learning\nHanhan Zhou\nThe George Washington University\nhanhan@gwu.edu\nTian Lan\nThe George Washington University\ntlan@gwu.edu\nVaneet Aggarwal\nPurdue University\nvaneet@purdue.edu\nAbstract\nOffline reinforcement learning aims to utilize datasets of previously gathered\nenvironment-action interaction records to learn a policy without access to the\nreal environment. Recent work has shown that offline reinforcement learning\ncan be formulated as a sequence modeling problem and solved via supervised\nlearning with approaches such as decision transformer. While these sequence-based\nmethods achieve competitive results over return-to-go methods, especially on tasks\nthat require longer episodes or with scarce rewards, importance sampling is not\nconsidered to correct the policy bias when dealing with off-policy data, mainly due\nto the absence of behavior policy and the use of deterministic evaluation policies. To\nthis end, we propose DPE: an RL algorithm that blends offline sequence modeling\nand offline reinforcement learning with Double Policy Estimation (DPE) in a unified\nframework with statistically proven properties on variance reduction. We validate\nour method in multiple tasks of OpenAI Gym with D4RL benchmarks. Our method\nbrings a performance improvements on selected methods which outperforms SOTA\nbaselines in several tasks, demonstrating the advantages of enabling double policy\nestimation for sequence-modeled reinforcement learning.\n1\nIntroduction\nMany real-world reinforcement learning (RL) problems, such as autonomous vehicle coordination\nand data-driven problems[1, 2, 3] are widely used to solve sequential decision-making problems\nnaturally in a way as the agent takes an action based on its observation, receives a reward from the\nenvironment, and then observes the next action[4, 5, 6], and so on [7, 8, 9]. Usually modeled as a\nMarkov Decision Process (MDP), the agent would take an action solely based on its current state\ninformation (which represents the whole trajectory history), so a scheme where sequences are divided\ninto each step and later solved with algorithms like Temporal Difference learning (TD-learning) [10]\nis proposed. This could be derived via Bellman Equations to solve RL problems mathematically.\nRecent advances in offline reinforcement learning (RL) algorithms provide a promising approach\nfor sequential decision-making tasks without the need for online interactions with an environment\n[11, 12]. This approach is particularly appealing when online interactions are costly or when there\nis an abundance of offline experiences available. Recent works have demonstrated that generative\nmodels [13, 14, 15] that are widely used in language and vision tasks can be applied to maximize\nthe likelihood of trajectories in an offline dataset without temporal difference learning [16], notably,\nPreprint. Under review.\narXiv:2308.14897v1  [cs.LG]  28 Aug 2023\nDecision Transformer (DT) [17], which uses the transformer architecture [18] for decision-making.\nSuch a pertaining paradigm in a supervised learning manner for RL can be considered known as\nReinforcement learning via Supervised Learning (RvS) [19, 20, 21]. Instead of learning a value-based\nalgorithm for decision-making, RvS-based methods often consider the learning task as a prediction\nproblem: to predict an action that will lead to a certain outcome or reward when given a sequence\nof past states and actions (e.g., using causal transformer architectures). These methods have gained\nsignificant attention due to their algorithmic and implementation simplicity while bringing a robust\nperformance on several offline-RL benchmarks.\nLearning an RvS policy πe requires off-policy learning since we need to estimate the expected return\nof the learned policy πe during training, from offline experiences/trajectories that are generated using\na different behavior policy πb. We note that online policy evaluation is usually expensive, risky, or\neven unethical for many real-world problems [22]. When the actual environment is not accessible,\nthese trajectories sampled by πb can be used to evaluate πt, also known as off-policy evaluation\n(OPE) [7]. An accurate OPE is crucial to evaluate and optimize a policy during training from offline\ndatasets, the concept of importance sampling (IS) rectifies the discrepancy between the distributions\nof the behavior policy πb and the evaluation policy πe [23]. IS-based off-policy evaluation methods\nhave also seen lots of interest recently, especially for short-horizon problems [24, 25], including\ncontextual bandits [26]. However, the application of IS to sequence modeling-based RvS methods is\ndifficult due to a number of challenges. The behavior policies for collecting experience/trajectory\ndata are often not available, while the evaluation policies in RvS methods are typically deterministic,\nmaking reweighting different experiences/trajectories inaccessible. Further, the variance of IS-based\napproaches tends to be too high to provide informative results, for long-horizon problems, since\nthe variance of the product of importance weights may grow exponentially as the horizon goes\nlong[27, 28].\nAlthough it is intuitively to assume that replacing the behavior policy with its empirical estimation\ncan harm the performance and increase the variance of a policy, recent works in several domains\nincluding multi-armed bandits[29] and off-policy evaluation [6, 30, 31] have shown that by applying\nan estimation of the behavior policy could improve the mean squared error of importance sampling\npolicy evaluation [32].\nIn this paper, we study a problem that when given a dataset of trajectories sampled by a behavior\npolicy and trajectories generated with sequence-modeling-based evaluation policy (in this paper we\nselect Decision Transformer to demonstrate our approach), to estimate both behavior policy and target\npolicy and then compute the importance sampling estimate which we call double policy estimation\nimportance sampling. We further provide a theoretical analysis on the properties of such estimators\nand show that this double policy estimation will reduce the variance of the target policy learned.\nSpecifically, we propose to introduce an asymptotic estimation for both behavior policy πb, which is\nused to sample and generate the dataset, and target evaluation policy, πt, which is the policy we are\nin an attempt to learn and correct, as double policy estimation, to calculate the likelihood ratio for all\nstate-action pairs in the off-policy data. Although it may seem that such an estimation would bring\neven worse performance as it introduces more uncertainties[33, 34], recent research in several domains\nincluding multi-armed bandits [32, 35], Monte Carlo integration [36], and causal inference [24] has\nshown this estimating behavior could potentially improve the mean squared error of importance\nsampling policy evaluation which partially motivates this design. Another direct motivation is that\nspecifically for many generation models based RvS methods like decision transformer in an offline\nreinforcement a common scenario is that both πb and πt are both inaccessible, which promotes a\ndesign for double policy estimation.\nWe prove that DPE can statistically lower the mean squared error of importance sampling OPE\nwith lower variance. We implement the proposed DPE on D4RL environments and compare DPE\nwith SOTA baselines including DT [17], RvS [19], CQL [37], BEAR [38], UWAC [39], BC [40],\nand IQL [41]. We empirically found double policy estimation based on importance sampling also\nbrings an improvement to the off-policy evaluation of the D4RL environment, where DPE achieves\nbetter performance than the original decision transformer on almost all datasets and outperforms\nthe state-of-the-art baselines over several datasets with further analysis discussing the effects and\nproperties of the proposed double policy estimator.\n2\n2\nBackground\n2.1\nMarkov Decision Process and Sequence-Based Method in Reinforcement Learning\nWe assume that the environment is a Markov decision process with a finite horizon and episodic\nnature, where the state space is denoted as S, the action space as A, and the environment possesses\ntransition probabilities represented by P, a reward function denoted as R, a horizon length of H, a\ndiscount factor of γ, and initial state distribution of d0 [42, 32]. A policy, denoted as π, is considered\nMarkovian if it maps the current state to a probability distribution over actions. In contrast, a policy is\nclassified as non-Markovian if its action distribution is dependent on past actions or states [43, 44]. We\nassume S and A are finite for simplicity and probability distributions are probability mass functions.\nIn off-policy policy evaluation, we are given a fixed evaluation policy, πe, and a data set of m\ntrajectories and the policies that generated them: D{ωi, π(i)\nb }m\ni=1 where ωi ∼π(i)\nb . We assume that\n∀{ωi, π(i)\nb } ∈D, π(i)\nb\nis Markovian, i.e., actions in D are independent of past states and actions gave\nthe immediately preceding state [45]. Sequence-based methods in reinforcement learning, which is\ntrained in reinforcement learning via supervised learning (RvS) manner such as Decision Transformer,\ntrain a model using supervised learning on a dataset with respect to trajectories to predict pD(a|s,R),\ni.e., given a cumulative reward R = P\nt γtrt to predict the probability of next action conditioning the\ncurrent state. Then at the deployment stage, the model takes actions conditioned on a desired target\nreturn value. Our goal is to design an off-policy estimator that takes D as input and estimates both\nbehavior policy πb and evaluation policy πe for enabling importance sampling in sequence modeling\nmethods.\nDecision Transformer processes a trajectory ω as a sequence consisting of 3 types of input to\nbe tokenized: the states, actions selected, and the return-to-go [18, 17]. Specifically, it learns a\ndeterministic model πDT(at|a−K,t,s−K,t,r−K,t) where −K denotes the past K sequences and is trained\nto predict the action token at timestamp t. During the evaluation, DT is given a desired reward\ng0 and the initial stage s0 at the beginning and executes the action it generates. Once an action\nat is generated and then executed, the next state st+1 ∼P(·|st, at) and reward rt = R(st, at) are\nobserved, together with the return-to-go gt+1 = rt −gt: this new sequence will be appended to the\nprevious input. The process is repeated until the terminal state. DT is then trained under standard l2\nloss as ∇θJ(πDT ) = 1\nK\nP\nk ∇θDT (ak −ˆa)2 in a supervised learning way.\n2.2\nImportance Sampling in Reinforcement Learning\nImportance Sampling (IS) is a method for reweighting returns generated by a behavior policy πb to\nproduce an unbiased estimate of the returns for the evaluation policy. To obtain a reliable numerical\nintegration of f as θ =\nR\nf(x)dx , assuming there is a family of sampling distributions, p(x; η),\nwith parameter η, that generates a random trajectory ω := (s0, a0, r0, · · · , sL−1, aL−1, rL−1) from\np(x; η0), where g(ω) := PL−1\nt=0 γtrt be the discounted return with preliminary fixed η0: an ordinary\nimportance sampling (OIS) method provides an estimator of θ in the form of ˜θ = 1\nn\nPn\ni=1\nf(xi)\np(x;η0).\nThen ˜θ is an unbiased estimator of θ and ˜θ is guaranteed to converge to θ as n goes to infinity\naccording to the strong law of large numbers [46].\nIn Monte Carlo problems with high-dimensional x, the target density p(x) can be writing in a\nchain-like decomposition as p(x) = p(x1) Qd\nt=2 p(x1|x1:t−1), where x[1:t] = (x1, · · · , xt). With a\nset of m trajectories and the policy that generated each trajectory, the IS off-policy estimate of v(πe)\nis: IS(πe, D) :=\n1\nm\nPm\ni=1 g(ω(i)) QL−1\nt=0\nπe(a(i)\nt\n|s(i)\nt\n)\nπb(a(i)\nt\n|s(i)\nt\n). We refer to this as the ordinary importance\nsampling (OIS) estimator which uses the true behavior policy and refer to πe(a|s)\nπb(a|s) as the OIS weight\nfor action a in state s. A standard approach to dealing with off-policy data is to correct the policy\nusing importance sampling (IS) by applying cumulative density ratios ν0:t [47, 29]. Then the policy\ngradient Z(θ) can be rewritten as an expectation over pπb and further estimated using an equivalent\nempirical expectation. The off-policy version of the classic REINFORCE algorithm [48] recognizes\nZ(θ) = E[ν0:H\nPH\nt=0 rt\nPH\nt=0 gt] (recall that E is understood as Epπb ) and uses the estimated policy\ngradient given by replacing E with En. Later works obtained a policy gradient in terms of Q-function\nas Z(θ) = E[PH\nt=0 ν0:tgtqt] [49].\n3\n3\nRelated Work\n3.1\nSequence-Based method in Reinforcement Learning\nMuch recent progress has been on formulating the offline decision-making procedure in offline\nreinforcement learning as a context-conditioned sequence modeling problem [16, 17]. Compared to\nthe temporal difference methods, these works consider a paradigm that utilizes predictive models to\ngenerate desired actions from the observation sequence and the task specification like a supervised\nlearning problem [20, 21, 19] rather than learning a Q-function or policy gradients. Specifically, the\nDecision Transformer model [17] trains the transformer architecture [18] as a model-free context-\nconditioned policy that takes the encoded reward-to-go, state, and action sequence as input to predict\nthe action for the next step, and the Trajectory Transformer [16] trains transformer that first discretizes\neach dimension of the input sequence and shows that beam search can be used to improve upon\nthe model-free performance. Various attempts have also been made to improve transformers in\nmulti-agent RL [50, 51, 52, 53] and other areas including meta RL [54, 5], and multi-task RL[55].\nHowever, these works do not consider the importance of sampling for offline reinforcement learning.\nOur work extended this area with the proposed double policy estimation and further improved the\nasymptotic variance of the ordinary method using the true sampling distribution.\n3.2\nImportance Sampling in Reinforcement Learning\nThe use of off-policy samples within reinforcement learning is a popular research area [56, 57, 58].\nMany of them rely on OIS or variants of OIS to correct for bias. The use of importance sampling\nensures unbiased estimates, but at the cost of considerable variance, as quantified by the ESS measure\n[59]. The problem of sampling error applies to any variant of importance sampling using OIS weights,\ne.g., weighted importance sampling and per-decision importance sampling [23], the doubly robust\nestimator [22], and the MAGIC estimator [60]. On-policy Monte Carlo policy evaluation is also\nsubject to sampling error, as it is a specific case of ordinary importance sampling where the behavior\npolicy and the evaluation policy are identical. Among these important sampling methods, [28] is the\nclosest work but considers estimated behavior policy where their behavior policy estimate comes\nfrom the same set of data used to compute the importance sampling estimate; while we estimate\nthe behavior policy to the training phase from the dataset and estimate the target policy from data\ngenerated from the target policy.\n4\nMethodology\nIn this section, we present the primary focus of our work: double policy estimation (DPE) importance\nsampling that corrects for sampling error in sequence modeling-based reinforcement learning. The\nkey idea is to obtain the maximum likelihood estimate of both behavior and evaluation policies\nˆπη\nb and ˆπψ\nt and use them for computing the DPE cumulative density ratio. We further analyze the\ntheoretical properties of DPE and prove that it is guaranteed to reduce the asymptotic variance of\npolicy parameters. A table of key notations with explanations is summarized in the Appendix.\n4.1\nDPE for sequence modeling-based reinforcement learning\nLet D be a set of off-policy trajectories of length H + 1 collected by a behavior policy πb, denoted\nby D = {ωi, ∀i} with each trajectory ωi = {(s0(i), a0(i), r0(i), · · · , sH(i), aH(i), rH(i))). For\nknown behavior policy πb and evaluation policy πθ\ne, OIS leverages the cumulative density ratio\nν0:t = Qt\nk=0 vk (with density ratio vk = πθ\ne(ak|sk)/πb(ak|sk)) to reweight the policy scores\ngt = ∇θ log πθ\ne(at|st), such that they are unbiased estimates of the evaluation policy πθ\ne. In the\noff-policy version of the classic REINFORCE algorithm [48], the policy gradient under OIS is\nrecognized as Z(θ) = E[ν0:Hq0:H\nPH\nt=0 gt], where qt:H = PH\ns=t rs is the return-to-go from step\nt to step H in trajectory ω (generated from behavior policy πb). OIS can be easily extended to its\nstep-wise form [61, 49] with Z(θ) = E[PH\nt=0 ν0:tqt:Hgt]. OIS has been commonly used in off-policy\nreinforcement learning.\nWe note that when RL is recast as an offline sequence modeling problem (such as Decision Trans-\nformer [17] and RvS [19]), it also relies on off-policy learning. However, there are three challenges\n4\npreventing OIS from being directly applied to sequence modeling-based RL. First, offline RL datasets\noften do not provide the actual behavior policy for collecting trajectories, making it impossible to\naccess πb in importance sampling. Second, sequence modeling-based RL usually are trained using a\ntransformer structure to represent evaluation policy and to generate deterministic action outputs [17].\nWe need to extend them to stochastic policies to obtain πe in importance sampling. Finally, OIS is\nknown to have a high variance [62], also known as high sampling error in importance sampling[28].\nMethods to reduce importance-sampling variance are needed for sequence modeling-based RL.\nTo this end, we propose two maximum likelihood estimators of (stochastic) behavior and evaluation\npolicies in sequence modeling-based RL, denoted by ˆπη\nb and ˆπψ\ne . A baseline return bξ\nt is further\nestimated (using a mean-square error loss) in sequence modeling-based RL and is leveraged to\nmitigate the variance in policy learning. Given a set D of m trajectories, the proposed DPE with\nrespect to the off-policy version of classic REINFORCE algorithm [48] is defined as:\nZDPE(θ|η, ψ, ξ, D) = E\n\"\u0010\nq0:H −bξ\n0\n\u0011 H\nY\nt=0\nπψ\ne (at|st)\nπη\nb (at|st)\n H\nX\nt=0\ngt\n!#\n.\n(1)\nDPE can also be applied to the step-wise form [61, 49], by replacing the density ratio vk with its\nestimator ˆvk = πθ\ne(ak|sk)/πb(ak|sk) and by subtracting the return baseline bξ\nt, i.e.,\nZDPE(θ|η, ψ, ξ, D) = E\n\" H\nX\nt=0\n(qt:H −bξ\nt)ˆv0:tgt\n#\n.\n(2)\nThe key idea of our DPE estimator for importance sampling is to leverage the maximum likelihood\nestimate of behavior and evaluation policies, denoted by ˆπη\nb and ˆπψ\nt respectively. We introduce the\nproposed maximum likelihood estimators for ˆπη\nb and ˆπψ\ne and minimum-mean-square estimator for bξ\nas following:\nMaximum likelihood estimator for behavior policy ˆπη\nb .\nWe consider estimating the ˆπb , with\nmaximum likelihood as ˆπη\nb := argmaxπb\nP\nω∈D\nP\nt logπb(a|ωt−n:t), so that it could provide a\nbehavior policy action probability estimation while the training of DT. Specifically, in this work, for\npolicy network estimator we consider learning πb from D as a Gaussian distribution over actions with\nmean and standard deviation estimated from a neural network.\nMaximum likelihood estimator for target policy ˆπψ\nt .\nOne key insight in this paper is that when\nassuming a Gaussian policy for target policy estimation, the estimator would be minimizing the mean-\nsquare error of action predictions, thus it is identical to sequence modeling-based RL like DT with\nMSE loss where its variance is this MSE specifically to each timestep while training. When obtaining\nthe target policy estimator, although for decision transformer πb is often not directly available and\nπb(a|s, R) cannot be used as this estimator, also estimating an ongoing learning method might be\nunstable and inefficient, we point out that this weight at specific timestep t can be considered as a\nGaussian distribution with a mean of ˆat and variance of the corresponding MSE. We explain why\nthis can serve as target policy estimation later in the main theorem in detail.\nMinimum-mean-square estimator for baseline bξ.\nSince bξ is trained to predict return-to-go by\nminimizing loss Pm\ni=1\nh\nqt:H −bξ\nt\ni2\n. This can be easily incorporated into sequence modeling-based\nReinforcement Learning like Decision Transformer.\nTraining sequence modeling based RL using DPE.\nWe summarize the general architecture of\nthe learning pipeline on Algorithm 1 of applying DPE to the sequence-modeling-based target policy\n(Decision Transformer). We first obtain an empirical estimator of the behavior policy πb prior to the\ntraining of the Decision Transformer in a warm-up phase. Then during the training phase, we acquire\nthe target policy estimator as a Gaussian distribution ˆaη\nt ∼N( ˆat, σ2) where ˆat is the mean generated\nfrom the decision transformer, ˆσ2 is the MSE that serves as variance from the loss calculated at a\nspecific timestamp. We present a pseudocode of the DPE training procedure in the appendix.\n5\n4.2\nProblem formulation and DPE Objective\nIn offline sequence modeling-based reinforcement learning, we are given a data set of m offline\ntrajectories ω = {(s0, a0, r0...)}, and the behavior policy π that is collected them. We denote the\ntrajectories that are generated by the decision transformer as ˆω = {( ˆs0, ˆa0, ˆr0...)}\nWe consider the following two joint objectives:\n\n\n\nH(πt) = −E\n\r\rP\nt log(2πeσ2\nt )\n\r\r ,\nL = −Eπtlogq(at)\n(3)\nwhere minimizing L −βH for πt, min(H(πt)) is to approximate the target policy decision trans-\nformer and L is to maximize the likelihood of at. We then choose πb(η) to maximize the likelihood\nand b(ξ) to minimize the squared error Pn\ni=1 w2 · (Gi −b(ξ))2 .\nNote that DPE objective can also be written as :\nDPE := 1\nm\nn\nX\ni=1\nq(ht)\nL−1\nY\nt=0\nˆπ(i)\nt (a(i)\nt |s(i)\nt )\nˆπ(i)\nb (a(i)\nt |s(i)\nt )\n= 1\nm\nn\nX\ni=1\nˆwπt(ht)\nˆwπb(ht)q(ht)\n(4)\nThe variance of ˜θ is given by δ2(f)/n, where δ2 = δ2(f) =\nR\n{\nf(x)\np(x;η0)−θ}2p(x; η0)dx, thus the\ndistribution of √n(˜θ −θ) converges to Normal distribution N(0, δ2) as n increases to infinity\naccording to central limit theorem.\n4.3\nTheoretical Properties of DPE\nWe analyze the asymptotic properties of the maximum likelihood estimator of behavior policy\nπˆη\nb (with optimal parameters ˆη), the maximum likelihood estimator of evaluation policy π ˆ\nψ\ne (with\noptimal parameters ˆψ), and the minimum mean-square error estimators of baseline bξ\nt (with optimal\nparameters ˆξ). We show that these estimators are able to reduce the variance of policy gradient\nestimates ZDPE. More precisely, for a given set of m off-policy trajectories D = {ωi, ∀i}, we\nconsider the gradient estimate ZDPE with DPE (in both per-episode form as Eq. (1) and per-step\nform as Eq. (2)), i.e.,\nZDPE = 1\nm\nm\nX\ni=1\n(q(i)\n0:H −b\nˆξ\n0)ˆv(i)\n0:H\n H\nX\nt=0\ng(i)\nt\n!\nand ZDPE = 1\nm\nm\nX\ni=1\nH\nX\nt=0\n(q(i)\nt:H −b\nˆξ\nt)ˆv(i)\n0:tg(i)\nt .\n(5)\nWe show that the variance Var(ZDPE) using optimal estimators ˆψ, ˆη and ˆξ is lower than the variance\nVar(ZOIS) using some ground truth ψ0, η0 and ξ0.\nWe begin with recognizing that both per-episode and per-step DPE can be consolidated using a\ngeneral form:\nZDPE = 1\nn\nn\nX\ni=1\nf(ωi; ˆψ)[G(ωi) −b(ˆξ)]\nP(ωi; ˆη)\n(6)\nNext, we show a few lemmas demonstrating some properties of the estimators ˆψ, ˆη, and ˆξ and then\nprove the variance reduction lemma.\nLemma 1. Let Fη = −1\nm\nPm\ni=1 ∂2\nη log P(ωi; ˆη0) be the Fisher Information Matrix. We have\n√m(ˆη −η0) =\n1\n√mF −1\nη\n·\nm\nX\ni=1\n∂η log P(ωi; η0) + O(1)\n(7)\nProof Sketch. Since ˆη is the maximum likelihood estimator that optimizes P(ωi; η), we have\n∂η\nPm\ni=1 log P(ωi; η) = 0 at η = ˆη. Expanding the left-hand side from η = η0 toward η = ˆη, we\nhave 0 = Pm\ni=1 ∂η log P(ωi; η0) + Pm\ni=1 ∂2\nη log P(ωi; ˆη0) · (ˆη −η0) + o(||ˆη −η0||2), which yields\nthe desired result by rearranging the terms and leveraging Fisher Information Matrix Fη.\n6\nLemma 2. Let Fξ = 1\nm\nPm\ni=1[∂ξb(ξ))]T · ∂ξb(ξ)). For linear baseline estimators b(ξ), we have\n√m(ˆξ −ξ0) =\n1\n√mF −1\nξ\n·\nm\nX\ni=1\n[G(ωi) −b(ξ0)] · ∂ξb(ξ0) + O(1)\n(8)\nProof\nSketch.\nSince\nˆξ\nis\nthe\nminimum\nmean-square-error\nestimator\noptimizing\nPm\ni=1 [G(ωi) −b(ξ)]2, we have ∂ξ\nPm\ni=1 [G(ωi) −b(ξ)]2 = 0. Expanding the left-hand side from\nξ = ξ0 toward ξ = ˆξ, we have 0 = ∂ξ\nPm\ni=1 [G(ωi) −b(ξ0)]2 + ∂2\nξ\nPm\ni=1 [G(ωi) −b(ξ)]2 (ˆξ −\nξ0) + o(||ˆξ −ξ0||2. It yields the desired result using the fact that b(ξ) is linear (thus ∂2\nξb(ξ) = 0) and\nusing the definition of Fξ.\nTheorem 1. The asymptotic variance of ZDPE, using optimal estimators ˆψ, ˆη, and ˆξ, is always less\nthan that of ZOIS using some ψ0, η0 and ξ0, i.e.,\nvar(ZDPE) = var(ZOIS) −var(VA) −var(VB)\n(9)\nwhere VA and VB are projections of {µi = f(ωi; ˆψ)[G(ωi) −b(ˆξ)]/P(ωi; ˆη), ∀i} onto the row\nspace of Sη = ∂η log P(ωi; η0) and Sξ = ∂ξb(ξ0), respectively.\nProof Sketch. We provide a sketch of the proof below and include the full proof in the appendix.\nStep 1: Define auxiliary function µi = µ(ωi; η, ψ, ξ) = f(ωi)[G(ωi)−b(ξ0)]\nP (ωi;η)\n, such that ZDPE (which\nis ˆθ in the notes with ZOIS being θ) can be written in Pn\ni=1 µ(xi; θ, ξ, η) −θ = 0. Then expand\nit from η0, ψ0, ξ0 to ˆη, ˆψ, ˆξ, to obtain √n(ˆθ −θ) =\n1\n√n\nPn\ni=1 µ(ωi; θ, ξ0, η0) + E(∂ηµ)(ˆη −η) +\nE(∂ξµ)√n(ˆξ −ξ) + O(1).\nStep 2: Rearranging the terms, plugging in Lemma 1 and Lemma 2, and using the fact of\nPn\ni=1 Sη\n′F −1\nη\nSη = 1 and Pn\ni=1 w2\ni Sξ\n′F −1\nξ\nSξ = 1, we obtain the equation below, where define Sξ\nand Seta here. Note that we use weights wi = 1 throughout the proof.\nStep 3, Recognize that Sξ and Sη are orthogonal. The two terms in C (i.e., A and B) can be viewed\nas projecting µi onto orthogonal row spaces of Sξ and Sη, respectively. Define these as VA and VB\nThe first term on the right hand side in\n√n(ˆθ −θ) =\n1\n√n\nn\nX\ni=1\n{µi −E(µiSη\n′)F −1\nη\n· Sη\n|\n{z\n}\nVA\n−E(µiSξ\n′) · w2\ni F −1\nξ\n}\n|\n{z\n}\nVB\n+O(1)\nis indeed OIS since √n(ˆθ −θ) =\n1\n√n\nPn\ni=1 µi.\nFrom Pythagorean relationship, we prove var(ZDPE) = var(ZOIS) −var(VA) −var(VB). The\ntheorem shows that the use of the DPE estimator always reduces the asymptotic variance of the\nestimator of OIS.\n5\nExperiments\nIn this section, we present an empirical study of applying Double Policy Estimator on Decision\nTransformer to verify the feasibility and effectiveness of our proposed method. We evaluate the\nperformance of our proposed algorithm on the continuous control tasks from the D4RL benchmark\nand compare it with several popular SOTA baselines. Furthermore, we analyze some critical properties\nto confirm the rationality of our motivation.\n5.1\nExperiment Setup\nWe empirically evaluate the performance of our proposed algorithm on the Gym Locomotion v2:\na series of continuous control tasks consisting of HalfCheetah, Hopper, and Walker2d\ndatasets from the D4RL offline reinforcement learning benchmark [63] with medium, medium-replay,\nand medium-expert datasets which include mixed and suboptimal trajectories. Specifically, Medium\ndataset includes 1 million timesteps generated by a “medium” policy that achieves approximately\n7\none-third of the score of an expert policy; Medium-Replay includes 25k-400k timesteps that\nare gathered from the replay buffer of an agent trained to the performance of a medium policy;\nMedium-Expert includes 1 million timesteps generated by the medium policy and then concate-\nnated with 1 million timesteps generated by an expert policy.\n5.2\nBaseline Selection\nWe compare our proposed algorithm to the following SOTA methods, where they aim to tackle the\ncurrent challenges in offline reinforcement learning from different perspectives: Decision Transformer\n(DT) [17], reward-conditioned behavioral cloning (RvS) [19], Conservative Q-Learning (CQL) [37],\nBEAR [38], UWAC [39], behavior cloning (BC), and Implicit Q Learning (IQL)[41]. CQL and IQL\nrepresent the state-of-the-art in model-free offline RL; RvS and DT represent the state-of-the-art in\nsequence-modeling-based supervised learning.\n5.3\nDPE weights implementation\nNote when proposing double policy estimation, there is no specific limitation on how πb and πt are\nestimated and how DPE weights are calculated. In this empirical section, we consider the following\nas one possible implementation: (1) We first apply CQL to train a neural network that generates\nmean and variances for Gaussian distributions as maximum likelihood estimation to obtain the\nestimated behavior policy ˆπb. (2) Then for each trajectory ωi we can calculate the estimated behavior\nweights as ˆwπb\ni\n= ˆπb(ai|ωi) (3) Next we train DT using l2 loss for updating each timestep, but\nwe record the MSE (ai −ˆai)2 as the variance, and ˆai as the mean for the Gaussian distribution,\ni.e. N(ai, (ai −ˆai)2) as target policy estimation. (4) There are multiple ways to calculate these\ntarget weights, e.g. cumulative distribution function (CDF): P(ai −β < ˆai ≤ai + β) where β is a\nprobability offset, or probability density function (PDF). In this empirical result, we consider using\nexponentiated clipped log-likelihood: exp(la(ˆa, (ai −ˆai)2)) with lˆa clipped at 0.05 and 0.995.\n5.4\nGeneral Performance\nDataset\nEnvironment\nDPE\nDT\nRvS\nCQL\nBEAR\nUWAC\nBC\nIQL\nmedium\nHalfCheetah\n45.4±0.3\n42.6±0.1\n41.6\n44.4\n41.7\n42.2\n43.1\n47.4\nHopper\n69.8±1.9\n67.6±1.0\n60.2\n58.8\n52.1\n50.9\n63.9\n66.3\nWalker\n77.9±0.8\n74.0±1.4\n71.7\n79.2\n59.1\n75.4\n77.3\n78.3\nmedium-\nreplay\nHalfCheetah\n40.5±1.5\n36.6±0.8\n38.0\n46.2\n38.6\n35.9\n4.3\n44.2\nHopper\n94.6±0.7\n79.4±7.0\n73.5\n48.6\n33.7\n25.3\n30.9\n94.5\nWalker\n83.5±1.2\n66.6±3.0\n60.6\n26.7\n19.2\n23.6\n36.9\n73.9\nmedium-\nexpert\nHalfCheetah\n82.5±5.8\n87.8±2.6\n92.2\n62.4\n53.4\n42.7\n59.9\n86.7\nHopper\n108.2±1.6\n107.6±1.8\n101.7\n104.6\n96.3\n44.9\n79.6\n91.5\nWalker\n93.7±6.2\n108.1±0.2\n106.0\n108.1\n40.1\n96.5\n36.6\n109.6\naverage\n77.34\n74.60\n71.72\n64.33\n48.24\n48.60\n48.06\n76.93\nTable 1: Overall performance of the normalized score of selected baselines on D4RL benchmark. All\nresults are evaluated on ’v2’ environments and datasets.\nWe first evaluate and compare the performance of the proposed method with all selected baselines\nin terms of average reward in Table 1, where 0 represents a random policy and 100 represents an\nexpert policy, with reward normalized per [63]. All results are averaged over 3 different seeds over\nthe final 10 evaluations, we put the full results including the error bar of all baselines in the appendix.\nOverall, we find DPE applied DT achieves better performance than the original decision transformer\non almost all datasets, and outperforms the state-of-the-art baselines over several datasets. Especially,\nin ‘medium-replay’ datasets that include mixed optimal and sub-optimal trajectories, our method\ncould bring a significant advancement in terms of reward. The finding that our proposed method\nattains competitive results stands in contrast to Decision Transformer which emphasizes the direct\nimprovements brought by applying double policy estimation.\n8\nm\nm-e\nm-r\nHooper\n0.00\n0.05\n0.10\n0.15\n0.20\nMSE\nBaseline\nDPE\nDT\n(a) Hopper-medium\nm\nm-e\nm-r\nHalfCheetah\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\nMSE\nBaseline\nDPE\nDT\n(b) HalfCheetah-medium\nm\nm-e\nm-r\nWalker2D\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nMSE\nBaseline\nDPE\nDT\n(c) Walker2D-medium\nFigure 1: MSE comparison with DT and DPE\n0.00\n0.25\n0.50\n0.75\n1.00\nProbability\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nDensity\npi_b\npi_t\n(a) Hopper-medium\n0.00\n0.25\n0.50\n0.75\n1.00\nProbability\n0\n1\n2\n3\nDensity\npi_b\npi_t\n(b) Hopper-medium-replay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\n0\n2\n4\n6\nDensity\npi_b\npi_t\n(c) Hopper-medium-expert\nFigure 2: Comparing Kernel Density Estimate of estimated πb and πt on Hopper datasets.\n5.5\nDiscussions\nTo demonstrate the actual effectiveness of reducing the variance, we also record the MSE from\nthe final evaluation stage of both DPE and DT for off-policy evaluation in Fig. 2, the results show\nthat using DPE weights could bring a generally lower MSE on all environments selected compared\nto DT, validating our efficiency on variance reduction. To visualize the source of effectiveness in\nthe double importance weights estimation we record the distribution of πb and πt on the ‘hopper’\nenvironment and provide a kernel density estimate plot in Fig. 3. The drastic difference from the\ntwo distributions could mean that the behavior policy estimated are acting as a correction weight to\noffset the probability sampling from the target policy distribution, leading to improved performance\nand reduced variance. As an example, an occasional sub-optimal trajectory that the target trajectory\nlearned with high probability could be corrected by the low probability from the estimated behavior\npolicy, making this a low-weight trajectory to learn from.\n5.6\nAblation Studies\nHalfCheetah\nHopper\nWalker2D\nEnvironment\n0.0\n0.1\n0.2\n0.3\nMSE\nWeighting Method\nCDF±0.1\nCDF±0.2\nPDF\nPDF-Clipped\nFigure 3: Ablations results on compar-\ning different probability sampling meth-\nods on estimated πb\nAccording to the object of DPE, the estimation of πb\nstill determines the target policy weights. In this sec-\ntion, we evaluate and compare several different ways to\ncalculate the exact probability generated from the esti-\nmated behavior distribution marking as CDF ±0.1, CDF\n±0.2, PDF, clipped PDF, and demonstrate the results over\nmedium-replay datasets in terms of MSE in Figure 3.\nWe see that despite some cases, most of the settings are\nsimilar regarding their prospective MSE, indicating that\nwhen a proper estimation of this Gaussian distribution is\nobtained, their method of sampling probability is not a\nmajor concern. Nevertheless, we find that using a clipped\nPDF for behavior probability selection brings the lowest\nMSE in general.\n9\n6\nLimitations and Social Impact\nThere are several opportunities for future work. First, our approach requires a warm-up phase prior to\nthe training of the decision transformer to obtain the estimated behavior policy. Also, as RvS methods\nperform poorly in stochastic environments as pointed out in [64], the currently proposed method\ncannot resolve such issues. We believe this work will result in positive social impacts as this will\nhelp to avoid unexpected behaviors from occasional unwanted trajectories from the dataset and make\nthe trained model more stable. However, this can potentially be used for making harmful decisions\nunder training with specific harmful and biased datasets.\n7\nConclusion\nIn this paper, we present DPE, a double policy estimation for importance sampling methods that are\nproven statistically efficient for variance reduction for Off-Policy Evaluation in Sequence-Modeled\nReinforcement Learning. Computing both the behavior policy estimate and target estimate from the\nsame set of data allows DPE to correct for the sampling error inherent to importance sampling with\nthe true behavior policy in the offline dataset. We evaluated DPE applied decision transformer across\nseveral benchmarks against current and SOTA works and showed that it demonstrated competitive\nperformances while improving the evaluation results of the Decision Transformer, especially on the\ndataset filled with sub-optimal trajectories, and confirming the effect of variance reduction through\nMSE comparison. Finally, we studied the possible cause for such improvements by visualizing the\ndensity of the estimated target policy and behavior policy.\nReferences\n[1] Xiaoling Luo, Xiaobo Ma, Matthew Munden, Yao-Jan Wu, and Yangsheng Jiang. A multisource data\napproach for estimating vehicle queue length at metered on-ramps. Journal of Transportation Engineering,\nPart A: Systems, 148(2):04021117, 2022.\n[2] Sihong He, Yue Wang, Shuo Han, Shaofeng Zou, and Fei Miao. A robust and constrained multi-agent\nreinforcement learning framework for electric vehicle amod systems. arXiv preprint arXiv:2209.08230,\n2022.\n[3] Min Hua, Cetengfei Zhang, Fanggang Zhang, Zhi Li, Xiaoli Yu, Hongming Xu, and Quan Zhou. Energy\nmanagement of multi-mode plug-in hybrid electric vehicle using multi-agent deep reinforcement learning.\nApplied Energy, 348:121526, 2023.\n[4] Xiaobo Ma, Abolfazl Karimpour, and Yao-Jan Wu. Statistical evaluation of data requirement for ramp\nmetering performance assessment. Transportation Research Part A: Policy and Practice, 141:248–261,\n2020.\n[5] Jingdi Chen and Tian Lan. Minimizing return gaps with discrete communications in decentralized pomdp,\n2023.\n[6] Kailash Gogineni, Yongsheng Mei, Peng Wei, Tian Lan, and Guru Venkataramani. Accmer: Accelerating\nmulti-agent experience replay with cache locality-aware prioritization. arXiv preprint arXiv:2306.00187,\n2023.\n[7] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[8] Jiayu Chen, Abhishek K Umrawal, Tian Lan, and Vaneet Aggarwal. Deepfreight: A model-free deep-\nreinforcement-learning-based algorithm for multi-transfer freight delivery. In Proceedings of the Interna-\ntional Conference on Automated Planning and Scheduling, volume 31, pages 510–518, 2021.\n[9] Yunfan Zhao, Qingkai Pan, Krzysztof Choromanski, Deepali Jain, and Vikas Sindhwani. Implicit two-tower\npolicies. arXiv preprint arXiv:2208.01191, 2022.\n[10] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:9–44,\n1988.\n[11] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\nmodeling problem. Advances in neural information processing systems, 34:1273–1286, 2021.\n10\n[12] Dan Zhang and Fangfang Zhou. Self-supervised image denoising for real-world images with context-aware\ntransformer. IEEE Access, 11:14340–14349, 2023.\n[13] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pages 1691–1703.\nPMLR, 2020.\n[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. 2018.\n[16] Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling\nproblem. In Neural Information Processing Systems, 2021.\n[17] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,\nAravind Srinivas, and Igor Mordatch.\nDecision transformer: Reinforcement learning via sequence\nmodeling. Advances in neural information processing systems, 34:15084–15097, 2021.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[19] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline\nrl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n[20] Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards - just map them to\nactions. ArXiv, abs/1912.02875, 2019.\n[21] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Wall Mutz, Wojciech Ja´skowski, and Jürgen Schmidhuber.\nTraining agents using upside-down reinforcement learning. ArXiv, abs/1912.02877, 2019.\n[22] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In\nInternational Conference on Machine Learning, pages 652–661. PMLR, 2016.\n[23] Doina Precup, Richard S. Sutton, and Satinder Singh. Eligibility traces for off-policy policy evaluation. In\nInternational Conference on Machine Learning, 2000.\n[24] Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects\nusing the estimated propensity score. Econometrica, 71(4):1161–1189, 2003.\n[25] Susan A Murphy, Mark J van der Laan, James M Robins, and Conduct Problems Prevention Research\nGroup. Marginal mean models for dynamic regimes. Journal of the American Statistical Association, 96\n(456):1410–1423, 2001.\n[26] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudık. Optimal and adaptive off-policy evaluation in\ncontextual bandits. In International Conference on Machine Learning, pages 3589–3597. PMLR, 2017.\n[27] Omer Gottesman, Yao Liu, Scott Sussex, Emma Brunskill, and Finale Doshi-Velez. Combining parametric\nand nonparametric models for off-policy evaluation. In International Conference on Machine Learning,\npages 2366–2375. PMLR, 2019.\n[28] Josiah Hanna, Scott Niekum, and Peter Stone. Importance sampling policy evaluation with an estimated\nbehavior policy. In International Conference on Machine Learning, pages 2605–2613. PMLR, 2019.\n[29] Josiah P Hanna and Peter Stone. Towards a data efficient off-policy policy gradient. In AAAI Spring\nSymposia, 2018.\n[30] Jingdi Chen, Lei Zhang, Joseph Riem, Gina Adam, Nathaniel D Bastian, and Tian Lan. Explainable\nlearning-based intrusion detection supported by memristors. In 2023 IEEE Conference on Artificial\nIntelligence (CAI), pages 195–196. IEEE, 2023.\n[31] Yongsheng Mei, Hanhan Zhou, and Tian Lan. Remix: Regret minimization for monotonic value function\nfactorization in multiagent reinforcement learning. arXiv preprint arXiv:2302.05593, 2023.\n[32] Lihong Li, Rémi Munos, and Csaba Szepesvári. Toward minimax off-policy value estimation. In Artificial\nIntelligence and Statistics, pages 608–616. PMLR, 2015.\n11\n[33] Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Robust multi-agent\nreinforcement learning with state uncertainty. Transactions on Machine Learning Research, 2023.\n[34] Sihong He, Lynn Pepin, Guang Wang, Desheng Zhang, and Fei Miao. Data-driven distributionally robust\nelectric vehicle balancing for mobility-on-demand systems under demand and supply uncertainties. In\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2165–2172.\nIEEE, 2020.\n[35] Yusuke Narita, Shota Yasui, and Kohei Yata. Efficient counterfactual learning from bandit feedback. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4634–4641, 2019.\n[36] Bernard Delyon and François Portier. Integral approximation by kernel smoothing. 2016.\n[37] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\nreinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n[38] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning\nvia bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.\n[39] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin\nGoh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint arXiv:2105.08140,\n2021.\n[40] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv\npreprint arXiv:1911.11361, 2019.\n[41] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning.\narXiv preprint arXiv:2110.06169, 2021.\n[42] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &\nSons, 2014.\n[43] Yongsheng Mei, Tian Lan, Mahdi Imani, and Suresh Subramaniam. A bayesian optimization framework\nfor finding local optima in expensive multi-modal functions. arXiv preprint arXiv:2210.06635, 2022.\n[44] Mridul Agarwal, Vaneet Aggarwal, and Tian Lan. Multi-objective reinforcement learning with non-linear\nscalarization. In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent\nSystems, pages 9–17, 2022.\n[45] Xiaobo Ma. Traffic Performance Evaluation Using Statistical and Machine Learning Methods. PhD thesis,\nThe University of Arizona, 2022.\n[46] Masayuki Henmi, Ryo Yoshida, and Shinto Eguchi. Importance sampling via the estimated sampler.\nBiometrika, 94(4):985–991, 2007.\n[47] Nathan Kallus and Masatoshi Uehara. Statistically efficient off-policy policy gradients. In International\nConference on Machine Learning, pages 5089–5100. PMLR, 2020.\n[48] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Reinforcement learning, pages 5–32, 1992.\n[49] Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In\nInternational Conference on Machine Learning, pages 1042–1051. PMLR, 2019.\n[50] Hanhan Zhou, Tian Lan, and Vaneet Aggarwal. Pac: Assisted value factorization with counterfactual\npredictions in multi-agent reinforcement learning. In Advances in Neural Information Processing Systems,\nvolume 35, pages 15757–15769. Curran Associates, Inc., 2022.\n[51] Jiayu Chen, Jingdi Chen, Tian Lan, and Vaneet Aggarwal. Scalable multi-agent covering option discovery\nbased on kronecker graphs. Advances in Neural Information Processing Systems, 35:30406–30418, 2022.\n[52] Yongsheng Mei, Hanhan Zhou, Tian Lan, Guru Venkataramani, and Peng Wei. Mac-po: Multi-agent\nexperience replay via collective priority optimization. In Proceedings of the 2023 International Conference\non Autonomous Agents and Multiagent Systems, pages 466–475, 2023.\n[53] Hanhan Zhou, Tian Lan, and Vaneet Aggarwal. Value functions factorization with latent state informa-\ntion sharing in decentralized multi-agent policy gradients. IEEE Transactions on Emerging Topics in\nComputational Intelligence, pages 1–11, 2023. doi: 10.1109/TETCI.2023.3293193.\n12\n[54] Jingdi Chen, Yimeng Wang, and Tian Lan. Bringing fairness to actor-critic reinforcement learning for\nnetwork utility optimization. In IEEE INFOCOM 2021-IEEE Conference on Computer Communications,\npages 1–10. IEEE, 2021.\n[55] Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Option-aware adversarial inverse reinforcement learning\nfor robotic control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages\n5902–5908. IEEE, 2023.\n[56] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Determin-\nistic policy gradient algorithms. In International conference on machine learning, pages 387–395. Pmlr,\n2014.\n[57] Sergey Levine and Vladlen Koltun. Guided policy search. In International conference on machine learning,\npages 1–9. PMLR, 2013.\n[58] Adam Elmachtoub, Vishal Gupta, and Yunfan Zhao. Balanced off-policy evaluation for personalized\npricing. In International Conference on Artificial Intelligence and Statistics, pages 10901–10917. PMLR,\n2023.\n[59] Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood ratio policy\ngradient. Advances in Neural Information Processing Systems, 23, 2010.\n[60] Philip S. Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement\nlearning. ArXiv, abs/1604.00923, 2016.\n[61] Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.\nFoundations and Trends® in Robotics, 2(1–2):1–142, 2013.\n[62] Carl Edward Rasmussen and Zoubin Ghahramani. Bayesian monte carlo. Advances in neural information\nprocessing systems, pages 505–512, 2003.\n[63] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[64] Keiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision transformers fail in\nstochastic environments. arXiv preprint arXiv:2205.15967, 2022.\n13\nA\nMathematical Details\nA.1\nNotations and Explanations\nWe use Table 1 to summarize the notations introduced in this paper and their corresponding explana-\ntions, more explanations can be found when first introduced in the main paper.\nNotation\nDefinition\ns\nstate space\na\naction space\nP\nenvironment transition probabilities\nr\nreward, reward function\nD\noffline dataset\nω\na trajectory\nH\nHorizon length\nπe\nevaluation policy\nπb\nbehaviour policy\nπt\ntarget policy\nν\ndensity ratio\nZ\nimportance weighed policy gradient\ng\npolicy score\nq\nreturn-to-go\nˆπb\nestimated behavior policy\nˆπt\nestimated target policy\nbη\nbaseline predicted return-to-go\nN(a, σ2)\nGaussian distribution, with mean of a and std of σ\nVar\nVariance Operator\nˆξ, ˆη, ˆψ\nestimators\nTable 2\nA.2\nDPE for sequence modeling-based reinforcement learning\nLet D be a set of off-policy trajectories of length H + 1 collected by a behavior policy πb, denoted\nby D = {ωi, ∀i} with each trajectory ωi = {(s0(i), a0(i), r0(i), · · · , sH(i), aH(i), rH(i))).\nWe first denote the approximated behavior policy with a Gaussian distribution as πb\n=\nargmaxπP(π|ω), and the approximated target policy as πt = ˆat + ˆσ2 ∗nk, where ˆat and ˆσ2is the\nmean and variance generated from the decision transformer, nk is Gaussian noise.\nA.3\nProof For Lemma 1\nLemma 3. Let Fη = −1\nm\nPm\ni=1 ∂2\nη log P(ωi; ˆη0) be the Fisher Information Matrix. We have\n√m(ˆη −η0) =\n1\n√mF −1\nη\n·\nm\nX\ni=1\n∂ηlogP(ωi; η0) + O(1)\n(10)\nProof.\nSince ˆη is the maximum likelihood estimator that optimizes P(ωi; η), we have\n∂η\nPm\ni=1 log P(ωi; η) = 0 at η = ˆη:\n14\n0\n=\nn\nX\ni=1\n∂ηlogP(ωi; η0) +\nn\nX\ni=1\n∂2\nηlogP(ωi; ˆη0)(ˆη −η0) + O(||ˆη −η||2)\nExpanding the right-hand side from η = η0 toward η = ˆη, we have\n√n(ˆη −η0)\n=\n1\n√n{−1\nn∂2\nηlogP(ωi; η0)(ˆη −η0)}−1 ·\nn\nX\ni=1\n∂ηlogP(ωi; η0) + O(1)\n√n(ˆη −η0)\n=\n1\n√n{−1\nn∂2\nηlogP(ωi; η0)(ˆη −η0)}−1 ·\nn\nX\ni=1\n∂ηlogP(ωi; η0) + O(1)\n√n(ˆη −η0)\n=\n1\n√nF −1\nη\n·\nn\nX\ni=1\n∂ηlogP(ωi; η0) + O(1)\n(11)\nwhich yields the desired result by rearranging the terms and leveraging the Fisher Information Matrix\nFη.\nA.4\nProof For Lemma 2\nLemma 4. Let Fξ = 1\nm\nPm\ni=1[∂ξb(ξ))]T · ∂ξb(ξ)). For linear baseline estimators b(ξ), we have\n√m(ˆξ −ξ0) =\n1\n√mF −1\nξ\n·\nm\nX\ni=1\n[G(ωi) −b(ξ0)] · ∂ξb(ξ0) + O(1)\n(12)\nProof. Since ˆξ is the minimum mean-square-error estimator optimizing Pm\ni=1 [G(ωi) −b(ξ)]2, we\nhave:\n∂ξ\nm\nX\ni=1\n[G(ωi) −b(ξ)]2 = 0\n(13)\nExpanding the right-hand side from ξ = ξ0 toward ξ = ˆξ, we have\n0 = ∂ξ\nm\nX\ni=1\n[G(ωi) −b(ξ0)]2 + ∂2\nξ\nm\nX\ni=1\n[G(ωi) −b(ξ)]2 (ˆξ −ξ0) + o(||ˆξ −ξ0||2\n0 =\nn\nX\ni=1\n∂ξw2\ni [G(ωi) −b(ξ)]2 +\nn\nX\ni=1\n∂2\nξw2\ni [G(ωi) −b(ξ)]2(ˆξ −ξ)2 + O(||(ˆξ −ξ)||2)\n0 =\nn\nX\ni=1\n(−I) · [G(ωi) −b(ξ)]2w2\ni · ∂ξb(ξ0) +\nn\nX\ni=1\n[w2\ni (∂ξb(ξ0))]2 −I[G(ωi) −b(ξ0) · w2\ni · ∂2\nξb(ξ0)]2(ˆξ −ξ) + O(1)\n√n(ˆξ −ξ) =\n1\n√n{ 1\nn\nn\nX\ni=1\n(∂2\nξb(ξ0))2w2\ni }−1 ·\nn\nX\ni=1\n[G(ωi) −b(ξ0)] · w2\ni ∂ξb(ξ0) + O(1)\n√n(ˆξ −ξ) =\n1\n√nF −1\nξ\n·\nn\nX\ni=1\n[G(ωi) −b(ξ0)]w2\ni ∂ξb(ξ0)\n(14)\nIt yields the desired result using the fact that b(ξ) is linear (thus ∂2\nξb(ξ) = 0) and using the definition\nof Fξ.\nProof\nSketch.\nSince\nˆξ\nis\nthe\nminimum\nmean-square-error\nestimator\noptimizing\nPm\ni=1 [G(ωi) −b(ξ)]2, we have ∂ξ\nPm\ni=1 [G(ωi) −b(ξ)]2 = 0. Expanding the left-hand side from\nξ = ξ0 toward ξ = ˆξ, we have 0 = ∂ξ\nPm\ni=1 [G(ωi) −b(ξ0)]2 + ∂2\nξ\nPm\ni=1 [G(ωi) −b(ξ)]2 (ˆξ −\nξ0) + o(||ˆξ −ξ0||2. It yields the desired result using the fact that b(ξ) is linear (thus ∂2\nξb(ξ) = 0) and\nusing the definition of Fξ.\n15\nA.5\nProof For Theorem 1\nTheorem 2. The asymptotic variance of ZDPE, using optimal estimators ˆψ, ˆη, and ˆξ, is always less\nthan that of ZOIS using some ψ0, η0 and ξ0, i.e.,\nvar(ZDPE) = var(ZOIS) −var(VA) −var(VB)\n(15)\nwhere VA and VB are projections of {µi = f(ωi; ˆψ)[G(ωi) −b(ˆξ)]/P(ωi; ˆη), ∀i} onto the row\nspace of Sη = ∂ηlogP(ωi; η0) and Sξ = ∂ξb(ξ0), respectively.\nProof. We define auxiliary function µi = µ(ωi; η, ψ, ξ) = f(ωi)[G(ωi)−b(ξ0)]\nP (ωi;η)\n, such that ZDPE (which\nis ˆθ in the notes with ZOIS being θ) can be written in\nn\nX\ni=1\nµ(ωi; θ, ξ, η) −θ = 0\nThen expand it from η0, ψ0, ξ0 to ˆη, ˆψ, ˆξ, we have\n0\n= 1\nnµ(ωi; θ, ξ0, η0) −(ˆθ −θ) + 1\nn\nn\nX\ni=1\n∂ηµ(ωi; θ, ξ0, η0)(η −ˆη) + 1\nn\nn\nX\ni=1\n∂ξµ(ωi; θ, ξ0, η0)(ξ −ˆξ)\n+ 1\nn\nn\nX\ni=1\n∂ξµ(ωi; θ, ξ0, η0)(ˆξ −ξ) + O(||ˆθ −θ||2 + ||ˆη −η||2 + ||ˆξ −ξ||2)\n√n(ˆθ −θ)\n=\n1\n√n\nn\nX\ni=1\nµ(ωi; θ, ξ0, η0) + E(∂ηµ)(ˆη −η) + E(∂ξµ)√n(ˆξ −ξ) + O(1)\n(16)\nRearranging the terms, plugging in Lemma 1 and Lemma 2, and using the fact of Pn\ni=1 Sη\n′F −1\nη\nSη =\n1 and Pn\ni=1 w2\ni Sξ\n′F −1\nξ\nSξ = 1, we obtain the equation below, where define Sξ and Sη here. Note\nthat we use weights wi = 1 throughout the proof. Recognize that Sξ and Sη are orthogonal. The\ntwo terms in VA and VB can be viewed as projecting µi onto orthogonal row spaces of Sξ and Sη,\nrespectively. Define these as VA and VB\n√n(ˆθ −θ)\n=\n1\n√n\nn\nX\ni=1\n{µi −E(µiSη\n′)F −1\nη\n· Sη\n|\n{z\n}\nVA\n−E(µiSξ\n′) · w2\ni F −1\nξ\n}\n|\n{z\n}\nVB\n+O(1)\n√n(ˆθ −θ)\n=\n1\n√n\nn\nX\ni=1\nµi + E{−f(ωi)[G(ωi) −b(ξ0)]\nP(ωi; η0)\n· ∂ηP(ωi; η0)\nP(ωi; η0)\n· √n(ˆη −η0)}\n+E{−\nf(ωi)\nP(ωi; η0)∂ξb(ξ0)} · √n(ˆξ −ξ) + O(1)\n√n(ˆθ −θ)\n=\n1\n√n\nn\nX\ni=1\nµi −E(µi · sη\n′) ·\n1\n√nF −1\nη\n·\nn\nX\ni=1\nsη\n−E{−\nf(ωi)\nP(ωi; η0)∂ξb(ξ0)} ·\n1\n√nF −1\nξ\nn\nX\ni=1\nw2\ni [G(ωi) −b(ξ0)]∂ξb(ξ0) + O(1)\n=\n1\n√n\nn\nX\ni=1\nµi −\n1\n√nE(µi · Sη\n′) · F −1\nη\n· Sη −\n1\n√n\nn\nX\ni=1\nE(µiSη\n′) · w2\ni F −1\nη\n· Sξ + O(1 +\n2\n√n)\n(17)\nRecall that\n\n\n\nPn\ni=1 Sη\n′F −1\nη\nSη = 1,\nPn\ni=1 w2\ni Sξ\n′F −1\nξ\nSξ = 1\n16\nWe have:\n√n(ˆθ −θ) =\n1\n√n\nn\nX\ni=1\n{µi −E(µiSη\n′)F −1\nη\n· Sη\n|\n{z\n}\nVA\n−E(µiSξ\n′) · w2\ni F −1\nξ\n}\n|\n{z\n}\nVB\n+O(1)\n(18)\n(19)\nWe note that Sξ\n′ and Sη\n′ are orthogonal, i.e.,\nn\nX\ni=1\nSηSξ = Sξ\nn\nX\ni=1\nSη = 0\nRecognize that Sξ and Sη are orthogonal. The two terms in Eq.(10) can be viewed as projecting µi\nonto orthogonal row spaces of Sξ and Sη, respectively. Define these as VA and VB The first term on\nthe right hand side in\n√n(ˆθ −θ) =\n1\n√n\nn\nX\ni=1\n{µi −E(µiSη\n′)F −1\nη\n· Sη\n|\n{z\n}\nVA\n−E(µiSξ\n′) · w2\ni F −1\nξ\n}\n|\n{z\n}\nVB\n+O(1)\nis indeed OIS since √n(ˆθ −θ) =\n1\n√n\nPn\ni=1 µi.\nFrom Pythagorean relationship, we prove\nvar(ZDPE) = var(ZOIS) −var(VA) −var(VB). The theorem shows that the use of the DPE es-\ntimator always reduces the asymptotic variance of the estimator of OIS.\nFrom Pythagorean relationship, we prove var(ZDPE) = var(ZOIS) −var(VA) −var(VB). The\ntheorem shows that the use of the DPE estimator always reduces the asymptotic variance of the\nestimator of OIS.\nAlgorithm 1 pseudocode for DPE\n1: Initiate ˆθ for πθ(a|s)\n2: for k = 0 to pretrain_steps do\n3:\nRandom Sample Trajectories: τ ∼D\n4:\nSample time index for each trajectory: h ∼τi[1, L]\n5:\nCalculate Loss: L(ˆθ) = P\nst,at,h logπθ(at|st)\n6:\nUpdate policy parameters: ˆθ = ˆθ + η∇θ(L(\nˆ\ntheta))\n7: end for\n8: initialize ˆπt, Decision Transformer\n9: for k = 0 to max_train_steps do\n10:\nRandom Sample Trajectories: τ ∼D\n11:\nSample time index for each trajectory: h ∼τi[1, L]\n12:\nGenerate Trajectories from DT: ˆa = DT(R, s, a, t)\n13:\nCalculate Loss for DT: L(˜θ) = 1\nN\nPN\ni=1( ˆai −ai)2\n14:\nEstimate ∇θJ(πt) = P\ni,t ∇θtlogπt(ai,t|sit)A((ai,t|sit))\n15:\nUpdate Decision Transformer: θπt = θπt + η∇θtJ(πθt)\n16: end for\n17: Return Decision Transformer πt\nB\nExperimental Details\nCode for experiments can be found on GitHub.\nB.1\nImplementation Details\nOur code is based on the original Decision Transformer[17] and CQL[37]. We summarize the\npseudocode for DPE training process in Algorithm 1. The hyperparameters used are shown below.\n17\nTable 3: Hyperparameters of DPE in experiments for D4RL Dataset.\nHyperparameter\nValue\nNumber of layers\n3\nNumber of attention heads\n1\nEmbedding dimension\n128\nNonlinearity function\nReLU\nBatch size\n64\nContext length K\n20 HalfCheetah, Hopper, Walker\nReturn-to-go conditioning\n6000 for HalfCheetah\n3600 for Hopper\n5000 for Walker\nDropout\n0.1\nLearning rate\n10−4\nGrad norm clip\n0.25\nWeight decay\n10−4\nLearning rate decay\nLinear warmup for first 105 training steps\n18\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.DC"
  ],
  "published": "2023-08-28",
  "updated": "2023-08-28"
}