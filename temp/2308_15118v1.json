{
  "id": "http://arxiv.org/abs/2308.15118v1",
  "title": "Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills",
  "authors": [
    "Mu-Tien Kuo",
    "Chih-Chung Hsueh",
    "Richard Tzong-Han Tsai"
  ],
  "abstract": "While large language models have made strides in natural language processing,\ntheir proficiency in complex reasoning tasks requiring formal language\ncomprehension, such as chess, remains less investigated. This paper probes the\nperformance of ChatGPT, a sophisticated language model by OpenAI in tackling\nsuch complex reasoning tasks, using chess as a case study. Through robust\nmetrics examining both the legality and quality of moves, we assess ChatGPT's\nunderstanding of the chessboard, adherence to chess rules, and strategic\ndecision-making abilities. Our evaluation identifies limitations within\nChatGPT's attention mechanism that affect its formal language comprehension and\nuncovers the model's underdeveloped self-regulation abilities. Our study also\nreveals ChatGPT's propensity for a coherent strategy in its gameplay and a\nnoticeable uptick in decision-making assertiveness when the model is presented\nwith a greater volume of natural language or possesses a more lucid\nunderstanding of the state of the chessboard. These findings contribute to the\ngrowing exploration of language models' abilities beyond natural language\nprocessing, providing valuable information for future research towards models\ndemonstrating human-like cognitive abilities.",
  "text": "Large Language Models on the Chessboard: A Study on ChatGPT’s Formal\nLanguage Comprehension and Complex Reasoning Skills\nMu-Tien Kuo1,2∗, Chih-Chung Hsueh1,2∗, Richard Tzong-Han Tsai2,3\n1Chingshin Academy, Taiwan\n2Center of GIS, Academia Sinica\n3National Central University, Taiwan\n{11035018, 11035038}@st.chjhs.tp.edu.tw\nthtsai@g.ncu.edu.tw\nAbstract\nWhile large language models have made strides in\nnatural language processing, their proficiency in\ncomplex reasoning tasks requiring formal language\ncomprehension, such as chess, remains less investi-\ngated. This paper probes the performance of Chat-\nGPT, a sophisticated language model by OpenAI in\ntackling such complex reasoning tasks, using chess\nas a case study. Through robust metrics examin-\ning both the legality and quality of moves, we as-\nsess ChatGPT’s understanding of the chessboard,\nadherence to chess rules, and strategic decision-\nmaking abilities. Our evaluation identifies limita-\ntions within ChatGPT’s attention mechanism that\naffect its formal language comprehension and un-\ncovers the model’s underdeveloped self-regulation\nabilities. Our study also reveals ChatGPT’s propen-\nsity for a coherent strategy in its gameplay and a\nnoticeable uptick in decision-making assertiveness\nwhen the model is presented with a greater vol-\nume of natural language or possesses a more lu-\ncid understanding of the state of the chessboard.\nThese findings contribute to the growing explo-\nration of language models’ abilities beyond natu-\nral language processing, providing valuable infor-\nmation for future research towards models demon-\nstrating human-like cognitive abilities.\n1\nIntroduction\nLarge Language Models (LLMs) have demonstrated the abil-\nity to deliver state-of-the-art performance in few-shot and\nzero-shot scenarios, rapidly expanding their capabilities with\nvery little data [Brown et al., 2020; Chowdhery et al., 2022;\nTouvron et al., 2023]. Recently, the reasoning abilities of\nLLMs have gained notable traction within the NLP research\ncommunity, as seen by the increased effort in crafting eval-\nuation benchmarks [Cobbe et al., 2021; Patel et al., 2021;\nShridhar et al., 2021; Yang et al., 2018] and reason-inducing\nprompting strategies [Wei et al., 2023; Zhou et al., 2023;\nWang et al., 2023; Huang and Chang, 2022]. Given the grow-\ning importance of LLMs, evaluating their complex reasoning\n*Equal contribution\nabilities in real-world applications is crucial, offering valu-\nable insights into a wide spectrum of tasks that necessitate\nthese abilities. In this research, we choose chess as a testing\nground to evaluate ChatGPT’s complex reasoning abilities.\nThe main research question we aim to address is: How well\ncan ChatGPT play chess, and what factors may affect its per-\nformance?\nOpenAI recently released ChatGPT1 (formally known as\nGPT-3.5), an instruction-tuned version of GPT-3 [Brown et\nal., 2020] which shares a similar training process with In-\nstructGPT [Ouyang et al., 2022]. ChatGPT is designed to\nunderstand task intent via natural language instructions and\nengage in multi-prompt coherent conversations, a feature that\ndistinguishes it from many other models. Its versatile appli-\ncability extends beyond the realm of linguistics, with uses\nin diverse fields where formal language application is req-\nuisite [Liu et al., 2023], including high-stake endeavors like\ndiscovering unknown causal relationships based on observed\ndata in the medical field [Tu et al., 2023]. This broad usage\ninvites an exploration into ChatGPT’s capabilities to compre-\nhend and infer formal language constructs accurately, with\nthe results shedding light on its limitations in identical sce-\nnarios and identifying areas for improvement.\nIn this research, we evaluate ChatGPT’s performance with-\nout additional fine-tuning. While there are other open-source\nLLMs available that support fine-tuning with specific task-\nrelated data, we chose ChatGPT for several reasons.\nAl-\nthough fine-tuning could potentially enhance a model’s per-\nformance in playing chess, our objective in this research is\nto assess the inherent reasoning capabilities and cognitive\nabilities of a widely-used, general-purpose LLM. Evaluating\nChatGPT without any specific fine-tuning allows us to gauge\nits base abilities and potential limitations when faced with\ncomplex, formal tasks like chess. This provides valuable in-\nsights into the model’s strengths and weaknesses, making it\nuseful for the broader AI research community.\nChess is an ideal evaluation tool for AIs due to its eas-\nily controllable enclosed state and a well developed suite of\ncomprehensive tools to evaluate player performance. With\nsimple rules and adequately complicated boards, chess is a\ntask that requires a high level of reasoning while being based\non simple prior knowledge.\nIt also provides a definitive\n1https://openai.com/blog/chatgpt\narXiv:2308.15118v1  [cs.CL]  29 Aug 2023\nenvironment in which the exact state of the board is inter-\npretable through only formal language such as move nota-\ntions. Chess is also found to be correlated to cognitive skills\nsuch as perception, memory, decision-making, and knowl-\nedge comprehension [Simon and Chase, 1988; Gobet, 1998;\nBurgoyne et al., 2016; Pinheiro, 2017], providing insight\nfor complicated real world applications that are of high\nstakes and also require sophisticated reasoning skills in con-\njunction with knowledge based decisions such as planning\nbusiness strategies or medical consultation [Swami, 2013;\nObasola et al., 2022].\nIn this research, we evaluate ChatGPT’s cognitive abilities\nwith chess through a chess game conversation, where after an\ninitial prompt instructs the model to play, players exchange\nmoves one message at a time. Through this process, we as-\nsess ChatGPT’s capacity to comprehend complex scenarios\nand to retain information throughout the entire exchange. We\ndefine a baseline experiment that provides the minimal infor-\nmation required for a human to play chess (Section 2) and\nexplore how incorporating different information and prompt-\ning strategies may affect ChatGPT’s performance (Section 3).\nWe also conduct further analysis on whether ChatGPT has a\nconsistent strategy among its games (Section 4). Finally, we\ndiscuss ChatGPT’s abilities on complex cognitive tasks, ana-\nlyze the limitations of natural language trained LLMs’ limita-\ntions on processing formal language and discuss whether the\nmodel has an ”intent” when making moves (Section 5). Our\nresearch provides the following contributions:\n1. We propose a diverse set of metrics that comprehen-\nsively evaluate the dimensions of ChatGPT’s move va-\nlidity and quality, allowing for a thorough analysis of its\nchess-playing abilities.\n2. We evaluate how providing information in prompts or\nallowing the model to reason in natural language may\nimpact ChatGPT’s performance in handling tasks that\nrequire complex formal language comprehension.\n3. We hypothesize on the limitations of natural language\ntrained attention that leads to increased forgetfulness and\ninconsistencies in formal language, shedding light on\npotential areas for improvement in future LMs.\n2\nBaseline Experiment\nTo evaluate ChatGPT’s chess-playing abilities, we designed\na baseline experiment in which we provided the minimum\namount of information required for a human to play chess.\nSpecifically, we instructed ChatGPT to play chess as the\nblack player and provided white’s first move.\nTo ensure\nconsistency in opponent difficulty, we chose to play against\nthe state-of-the-art computer chess engine Stockfish 15.1,\na widely recognized and powerful chess engine known for\nits high level of play.\nSince the Standard Algebraic No-\ntation (SAN) is commonly used to communicate moves in\nchess, we utilized this notation to exchange moves with\nChatGPT. We conducted all our experiments with the model\ngpt-3.5-turbo-0301 and follow the parameters used in\nSt¨ockl [2021] with a temperature of 1 and top-p of 0.9. A\ntotal of 1000 games were played against Stockfish, serving as\nthe baseline for future experiments.\n2.1\nBaseline Procedure\nWe initiated each game with a new chat instance and provided\nChatGPT with the following prompt:\nI want you to act as a rival chess player. I will start\nas white, and we will say our moves in reciprocal\norder. After my first message, I will just write my\nmove. Please don’t explain your decision and just\nreply with your move.\n[White’s first move].\nSince chess openings can lead to substantial variance in the\nsubsequent moves, we aimed to exclude rare openings that\ncould lead to edge-case games. To achieve this, we randomly\nselected one of the top four engine moves (e4, d4, Nf3, and\ne3) for each game, thereby ensuring an even opening dis-\ntribution.\nUpon receiving the model’s move, we checked\nwhether the move was legal. If it wasn’t, we regenerated\nthe response until a legal move was provided or if 10 ille-\ngal moves were made consecutively, in which case the game\nwas terminated. We recorded Stockfish’s evaluation of the ad-\nvantage of white’s position and sampled a response from the\ntop three moves provided by Stockfish. The game continued\nuntil it either met standard chess end criteria (e.g., checkmate\nor stalemate) or was terminated due to ten consecutive illegal\nmoves.\n2.2\nEvaluation Metrics\nIn chess, generating high-quality moves is substantially more\ndifficult than generating legal moves. Good quality moves re-\nquire skills such as sophisticated board comprehension, mem-\nory, and future planning. In this study, we therefore evaluate\nLLMs’ chess performance in two dimensions: legality and\nquality. Legality evaluates the model’s ability to generate le-\ngal moves (i.e., moves that comply with chess rules), while\nquality evaluates how good a move is in terms of increasing\nthe player’s positional advantage.\nGiven a series of games G where\nGi = (Pi,1, Pi,2, ..., Pi,ni)\nPi,j =\n\u001a1,\nModel made any number of illegal attempts\n0,\nNo illegal attempts were made\na series n where ni is the count of moves ChatGPT made in\ngame i, a series r where rj\ni is the amount of illegal moves\nChatGPT attempted before making a legal move on game i’s\njth move, we define the metrics as follows:\nWe measure validity using two metrics, the Illegal Move\nRatio (IMR) and Retries Before Legal Move (RBLM). IMR\nrepresents validity at the attempt level, calculating the ratio\nof illegal moves to total moves. Game i’s IMR at move t is\ndefined as follows (Game i’s IMR is defined as IMR(i, ni)):\nIMR(i, t) = Σt\nj=1Pi,j\nt\nRBLM captures the average count of illegal moves Chat-\nGPT makes before making a valid move. Game i’s RBLM\nat move t is defined as follows (Game i’s RBLM is defined as\nRBLM(i, ni)):\nRBLM(i, t) = Σt\nj=1rj\ni\nΣt\nj=1Pi,j\nAs making an illegal move is extremely uncommon among\nhuman chess players, we argue that there is a substantial\nthreshold of incoherence required to make such a mistake.\nTherefore, IMR presents an isolated figure that only studies\nthe distributions of these catastrophic attempts. RBLM scores\nare designed to represent how spread the model’s next options\nare. A high RBLM indicates that the model has a wide range\nof moves deemed viable, which indicates uncertainty in the\nmodel, while low RBLM indicates a limited amount of con-\nsidered moves and higher certainty (see Section 5.3 for more\ndetail). Although one may argue that IMR and RBLM can be\ncombined into a single metric (total illegal attempts over total\nattempts), this would fail to separate games that often make\nshort bursts of illegal moves from games that suffer from a\nfew long sequences of illegal moves. Our two-metric sys-\ntem enables further model motive interpretation, allowing for\na more detailed analysis of why models may fail to comply\nwith chess rules.\nIn assessing move quality, we employ Stockfish’s advan-\ntage evaluation function, which quantifies white’s positional\nadvantage in centipawns (one hundredth of a pawn’s value).\nA positive value signifies a favorable position for white while\na negative value a favorable position for black. Given the\nprogressive increase in evaluation throughout the course of\na game (as Stockfish constantly outperforms ChatGPT), it is\nflawed to compute the mean Board Evaluation (BE) across\nall games, as experiments that utilize prompts that result in\nlonger game durations will invariably yield higher average\nevaluations.\nConsequently, we restrict our analysis to the\nmean BE on the 20th move (since at least 10% of games in\neach variation reach this checkpoint) within each variation.\nThe average BE over all games can be found in Appendix A.\nWe also take the Games’ Length (GL) into consideration.\nAlthough typically the length of games indicates very lit-\ntle information in chess games, most of the games Chat-\nGPT played were terminated due to ten consecutive illegal\nmoves. As noted in the following sections, ChatGPT’s per-\nformance tends to decay as the length of the game increases,\nwe therefore record GL to provide insight into the average\nrequired length of a game to have ChatGPT fail to generate\nlegal moves.\nIn future sections, we use these metrics to evaluate Chat-\nGPT’s performance in playing chess and assess the impact of\nvarious prompting strategies on its ability to play the game.\n2.3\nBaseline Performance\nThe baseline experiments reveal an underwhelming perfor-\nmance by ChatGPT, as detailed in Table 1. ChatGPT failed to\nsecure a win in any games and recorded a high IMR, generat-\ning an illegal move every four moves. The quality of moves,\nassessed by how often ChatGPT’s moves improved black’s\nadvantage, was consistently poor with black rarely gaining\nan advantage over white. Upon observing the model’s IMR\nand RBLM, we found that both consistently increased as the\nlength of the games extended. This suggests that the length\nof games impacts the model’s understanding of the board\nstate and its adherence to the rules of chess. This observa-\ntion resonates with findings from Bang et al., [2023], where\nincreased inconsistencies and forgetfulness were detected in\nIMR\nRBLM\nGL\nBE\n0.26\n6.78\n18.79\n253.1\nTable 1: Baseline Performance. Lower IMR and RBLM re-\nflect better legality; higher GL signifies prolonged games; and\nhigher BE indicates poorer move quality.\nIMR\nRBLM\nGL\nBE\nBaseline\n0.26\n6.78\n18.79\n253.1\nInt-Illegal\n0.27\n6.86\n18.07\n278.6\nInt-Rules\n0.33\n7.52\n13.15\n364.53\nTable 2: Initial Prompt Variations’ Results\nprolonged conversations. We hypothesize that this trend may\nstem from two key aspects: ChatGPT’s limited capability of\nretaining previous conversational context, and the model’s\ndifficulty in handling intricate game scenarios. We term the\nformer as attention decay, referring to the model’s declining\nability to reference and incorporate past conversational con-\ntent into its responses over the course of an extended dia-\nlogue, and explore how prompting effects the impact of at-\ntention decay on the model.\n3\nIncorporating Alternate Prompts\nGiven ChatGPT’s sub-optimal performance in move validity\nand high game-termination rates, we explore the potential of\nusing prompts to enhance the model’s ability to generate valid\nchess moves. Prior research recognizes prompting as a cost-\neffective method to improve LLMs’ performance, sometimes\neven outperforming fine-tuned models [Reynolds and Mc-\nDonell, 2021; Webson and Pavlick, 2022; Kojima et al., 2023;\nWei et al., 2023].\nIn this section, we aim to determine\nwhether prompts that provide clearer instructions or assistive\ninformation can improve ChatGPT’s ability to generate legal\nmoves and impact its move quality. To achieve this objective,\nwe devise and implement variations of the baseline proce-\ndure employing different prompting strategies. We evaluate\nthe effectiveness of these prompt variations by recording 400\ngames per variation, adhering to the baseline procedure for\nall steps unless specified otherwise.\n3.1\nInvestigating Initial Prompt Variations\nWe explored the impact of altering the initial prompt in two\nways. The first variation, labeled as Int-Illegal, involved ap-\npending the message ”Please do not make illegal moves” to\nthe original prompt. This variation tested whether ChatGPT\nhad learned conceptual functions [Reynolds and McDonell,\n2021] that would help it avoid illegal moves. The second vari-\nation, termed Int-Rules, involved including a concise sum-\nmary of the rules of chess within the initial prompt.\nThe\nobjective of this variation was to test whether an in-prompt\nversion of rules would increase the model’s attention to gen-\nerating legal moves.\nResults: The Int-Illegal variation yielded results that were\nnearly identical to the baseline procedure. However, when\nasked about chess rules, ChatGPT demonstrated a perfect un-\nderstanding by accurately reciting every rule multiple times.\nThis indicates that relying solely on natural language hints\nis insufficient to improve model performance in chess. On\nthe other hand, the Int-Rules variation resulted in a noticeable\nperformance drop compared to the baseline, with a significant\ndecrease in the average game length. We speculate that the in-\nclusion of the rule tokens in the prompt diluted the attention\nreceived by the board state tokens, thereby compromising the\nmodel’s ability to comprehend the board effectively. Our ex-\nperiments revealed that ChatGPT failed to effectively utilize\nthe provided chess rules, regardless of whether they were in-\ncluded as model memory or in the prompt. Furthermore, re-\ninforcing the importance of rules did not lead to better model\nperformance.\n3.2\nInvestigating Move Prompt Variations\nWe investigated the effectiveness of adding information to\nmove prompts to enhance ChatGPT’s ability. To this end, we\nconducted two experimental variations of the baseline proce-\ndure that incorporate additional information in move prompts.\nThe first variation, named Move-Repeat, involves append-\ning every move made in the game to the end of the move\nprompt. This experiment aims to reduce the impact of Chat-\nGPT’s attention decay by increasing the appearances of to-\nkens that date back further in the game. The second varia-\ntion, termed Move-IlgRem, provided a reminder to ChatGPT\nwhenever it made illegal moves. In this variation, we supplied\nChatGPT with a list of its previous illegal attempts during that\nmove and informs it that those are illegal, aiming to reduce\ngame terminations by preventing ChatGPT from making the\nsame mistakes repeatedly.\nResults: The Move-Repeat variation yielded considerable\nimprovements over the baseline in all metrics except IMR.\nWe observed considerable enhancements in GL and BE, sug-\ngesting that Move-Repeat enables the model to generate a\nmore concrete understanding of the board, mitigating the im-\npact of attention decay and resulting in substantially longer\ngames. Interestingly, the model attempts more illegal moves\nbut requires fewer moves before reaching a legal solution. We\nspeculate that this might be a form of model ”intent,” which\nwe define as signs of the model reducing move candidates and\nshowing more faith or determination towards a certain move.\nOn the contrary, Move-IlgRem demonstrated extremely\npoor chess abilities.\nAlthough the model tended to avoid\nmoves deemed illegal, the staggering RBLM suggests a\nstrong sense of uncertainty. We hypothesize that this is due to\nthe model’s inability to differentiate game moves from moves\nin reminders, resulting in drastic drops in board comprehen-\nsion, causing high RBLM and short games.\nInterestingly,\nMove-Repeat and Move-IlgRem variations only show effects\nif the information is included throughout the entire conversa-\ntion. If the information is only appended after the latest move,\nboth variations exhibit baseline-like results. This finding in-\ndicates that repetition itself may not be enough, and constant\nrepetition might be required to achieve compelling improve-\nments.\n3.3\nReasoning in Natural Language\nRecent work has shown that allowing LLMs to reason in nat-\nural language can substantially enhance model performance,\nIMR\nRBLM\nGL\nBE\nBaseline\n0.26\n6.78\n18.79\n253.1\nMove-Repeat\n0.31\n5.82\n23.97\n284.99\nMove-IlgRem\n0.23\n9.33\n12.96\n314.38\nTable 3: Move Prompt Variations’ Results. Lower IMR and RBLM\nreflect better legality; higher GL signifies prolonged games; and\nhigher BE indicates poorer move quality.\nBaseline\nMove: [Stockfish’s move]\nExample\nMove: Nd7\nMove-\nMove: [Stockfish’s Move],\nRepeat\nPrevious Moves: [Previous Move]\nExample\nMove: Nf6,\nPrevious Moves: 1. Nf3 d5 2. d4 e6 3. g3 Bd6\n4. c4 c6 5. Bg2\nMove-\nMove: [Stockfish’s move] (moves [Illegal\nIlgRem\nmoves made] are illegal).\nExample\nMove: Nd7 (moves b2, c5 are illegal).\nTable 4: All variations of the Move Prompts\nboth in a few-shot and a zero-shot manner [Wei et al., 2023;\nZhou et al., 2023; Kojima et al., 2023]. The improvements\nthese methods bring are most often observed on a limited set\nof benchmarks, namely arithmetic, commonsense, and sym-\nbolic reasoning tasks [Wei et al., 2023; Zhou et al., 2023;\nWang et al., 2023; Kojima et al., 2023]. As these bench-\nmarks are relatively straightforward compared to chess, we\ntested the extent to which allowing models reasoning in natu-\nral language can improve ChatGPT’s chess abilities.\nTo this end, we designed variations where models were\nencouraged to reason in natural language before making\ntheir move. Another gpt-3.5-turbo-0301 instance was\ngiven the model’s response and eight shots of examples to ex-\ntract the final move in the format of the SAN notation. The\nsentence was not injected if the model had already learned\nto emulate this behavior. We conducted three experiments:\nRsn-Simple, Rsn-CoT, and Rsn-DropCoT. In Rsn-Simple,\nthe model was asked to ”analyze the board and explain your\nmove” and was offered the most recent analysis as an ex-\nample.\nRsn-CoT and Rsn-DropCoT followed Kojima et\nal. [2023] in encouraging the model to reason with Chain\nof Thought (CoT) [Wei et al., 2023] in a zero-shot manner.\nIn the initial prompt, an instruction was included to ”provide\na step-by-step analysis”, and an additional message Let’s\nthink step by step. was inserted before the model’s\nexplanation. We also investigated whether removing prior\nreasoning in the conversation affects model performance by\nconducting two versions of CoT: Rsn-CoT (which keeps up to\neight instances of prior reasoning) and Rsn-DropCoT (which\nonly keeps the most recent one).\nResults: In all variations involving natural language rea-\nsoning (NL reasoning), we observed significant decrements\nin RBLM, indicating that NL reasoning also helps LLMs gen-\nerate ”intent.” However, the presence of intent does not neces-\nIMR\nRBLM\nGL\nBE\nBaseline\n0.26\n6.78\n18.79\n253.1\nRsn-Simple\n0.34\n5.84\n18.11\n412.26\nRsn-CoT\n0.37\n5.82\n18.45\n492.4\nRsn-DropCoT\n0.4\n5.31\n19.56\n525.34\nDsc-Base\n0.47\n5.02\n19.3\n763.11\nTable 5: NL Reasoning & Board Description Variations’ Results.\nLower IMR and RBLM reflect better legality; higher GL signifies\nprolonged games; and higher BE indicates poorer move quality.\nsarily correlate with better game performance. Allowing NL\nreasoning significantly impaired the model’s move quality,\nwhich we attribute to the excessive amount of erroneous in-\nformation in the model’s reasoning, analysis, and game state\ndescription. This, in turn, misled the model into formulat-\ning strategies based on model hallucinations. Interestingly,\nKojima et al., [2023]’s prompting strategies led to worse\nmove legality and quality compared to Rsn-Simple. Upon\nclose examination of the dialogues within Rsn-Simple and\nRsn-DropCoT, we identify a major distinction in the amount\nof spurious information. Rsn-Simple responses tend to be\nshort, containing only one to two sentences. In contrast, Rsn-\nDropCoT responses typically consists of an evaluation of the\nopponent’s move, a list of plausible moves along with the\nramifications of each, and a decision of what the models be-\nlieves is the best course of action. This however, introduces\nan a considerably greater amount of erroneous information,\nexacerbating the model’s susceptibility to illegitimate infor-\nmation and resulting in the decreased legality.\nAdditionally, we observed the ”one-shot contamination”\neffect discussed by Reynolds and McDonell [2021], where\nRsn-DropCoT performed comparatively worse than Rsn-\nCoT. Rsn-CoT may have better performance due to it allow-\ning models to see a more diverse set of explanations. Al-\nthough most explanations are incorrect, the exposure to more\ndiverse information may bring performance gains similar to\nhow sampling multiple responses improves model perfor-\nmance in Wang et al., [2023]. Intuitively, Rsn-DropCoT ex-\nhibited a more consistent strategy (indicated by lower RBLM)\ncompared to Rsn-CoT, suggesting that having only a sin-\ngle response better retains consistency within the model’s\ndecision-making process. Future research should continue to\nexplore how different types of NL reasoning impact model\nperformance in complex tasks like chess.\n3.4\nDescribing State in Natural Language\nGiven that ChatGPT is primarily trained on natural language\ndata, a compelling research question arises regarding the ex-\ntent to which substituting formal language with natural lan-\nguage can enhance ChatGPT’s capacity for intricate reason-\ning.\nTherefore, we designed an experiment to investigate\nwhether formal language is the main factor contributing to\nChatGPT’s unsatisfactory chess abilities. To this extent, we\ncrafted a prompt that supplements a natural language board\ndescription on each move, providing information about each\npiece’s location and relation. This message is appended af-\nter the phrase \"After my move, the board state\nis a follows:\nboard state\" in the move prompt.\nWe began by describing white’s state, including details about\nthe quantity of each piece type and each piece’s location and\nrelation with other pieces in a prompt such as follows.\nWhite has [quantity] [piece-type] left.\nA [piece-type] is on [square], can capture [tar-\ngets], can be captured by [attackers], and is de-\nfended by [defenders].\n...\nAn additional message is added behind pawns that are an en\npassant target. We last specify whether white has kingside\nand queenside castling rights. The description, in the same\nformat, is then repeated for black. After the description, we\nask ChatGPT to make its next move. Due to input token lim-\nitations, we only retained the most recent description of the\nchess board in the conversation. Implementing the variation\ndescribed above, we conduct the experiment Dsc-Base.\nResults: The results of Dsc-Base was surprisingly under-\nwhelming. As shown in table 5, this variation had the highest\nIMR and BE across all experiments, demonstrating a substan-\ntially worse chess performance. However, lower RBLM indi-\ncated a stronger decision making intent by the model. Upon\ninspecting the model’s responses, we did not find a high vol-\nume of erroneous information like those in the NL reasoning\nvariation. In fact, the responses instead were relatively plain\nand contained only the model’s moves made in the SAN no-\ntation. We posit that the drop in IMR and move quality can\nbe attributed to the model’s failure to effectively apply its\nlearned chess cognitive functions from SAN notation to the\nnatural language board descriptions. As most chess games on\nthe internet are recorded in the SAN notation, we propose that\nthe conceptual functions that play chess [Reynolds and Mc-\nDonell, 2021] in ChatGPT is more active when the input is in\nthe SAN format instead of a much more general format (i.e.,\nnatural language). Further evidence supporting this notion is\nthe model’s consistent choice to make the move in the SAN\nnotation without user specification. As for the lower RBLM,\nthe model’s trait of having a stronger intent when given nat-\nural language inputs remain unchanged, thus resulting in the\nlower RBLM value.\n4\nAnalyzing ChatGPT’s Strategic Behavior\nWe next analyze whether a consistent behavior can be ob-\nserved in ChatGPT. Building upon the faithfulness concept\nin evaluating NLP systems’ explainability [Jacovi and Gold-\nberg, 2020; DeYoung et al., 2020], we draw inspiration from\nJacovi and Goldberg [2020] and assess the consistency of\nChatGPT’s moves as an indicator of its strategic behavior.\n4.1\nIllegal Move Diversity\nOne of the most significant issues in ChatGPT’s chess perfor-\nmance is its propensity for making illegal moves. The root\ncauses of these illegal moves may be attributed to two op-\nposed reasons. The first being that the model may resort to\ngenerating arbitrary moves due to a lack of clear direction.\nThe other may be an exhibition of the model’s strong ”in-\ntent” to achieve an objective (for instance, mirroring the hu-\nman thought process of capturing the opponent’s high-value\nBaseline\nInt-Illegal\nInt-Rules\n0.51\n0.5\n0.44\nMove-Repeat\nMove-IlgRem\nRsn-Simple\n0.64\n0.06\n0.63\nRsn-CoT\nRsn-DropCoT\nDsc-Base\n0.59\n0.63\n0.6\nTable 6: Average MRS per Variation\npieces) and, in doing so, it may overlook the rules of the\ngame. To discern the tendency of the model’s performance,\nwe introduce the Move Repetition Score (MRS). This score\nquantifies the similarity between ChatGPT’s illegal moves.\nThe MRS for each game is calculated as follows:\nMRS =\nΣn\ni=1Σci\nj=1( cj\ni\nai )2\nn\nwhere n is the count of moves where ChatGPT attempted il-\nlegal moves, ai is the count of attempts of illegal moves on\nmove i, ci is the count of unique illegal moves ChatGPT at-\ntempted on move i, and cj\ni, 1 ≤j ≤ci is the model’s total\nattempts of the jth unique move on move i. We then calcu-\nlate the average of all games’ MRS to obtain each variation’s\nMRS.\nResults: The MRS displays considerable variation across\ndifferent iterations, indicating that the model’s thought pro-\ncess is influenced by prompting. In general, variations that\ninvolve more natural language (both in board description\nand model reasoning) elicit more consistent illegal moves,\nthereby suggesting that retaining any amount of natural lan-\nguage in the conversation history can enhance the model’s\ncapability to pursue a goal consistently. The elevated MRS\nobserved in the Move-Repeat variation is noteworthy. As the\nMove-Repeat variation theoretically provides a more accu-\nrate board representation, we posit that allowing the model\nto perceive the board with fewer misrepresentations also en-\nables it to make more strategically consistent moves. Vari-\nations without NL reasoning (i.e., Baseline and Int-Illegal)\nresulted in the model demonstrating more erratic attempts,\nbut still perform better than variations that have distractions\n(i.e., Int-Rules and Move-IlgRem). The Int-Rules variation,\nwhich incorporates information that we find is distracting to\nthe model, produced more arbitrary outcomes. The Move-\nIlgRem variation is an exception to the correlation between\nMRS and RBLM. Due to its design to deliberately avoid il-\nlegal move repetition, the significant drop in MRS indicates\nthe model’s attempts to avoid making the same illegal moves,\nbut the extremely high RBLM demonstrates an exorbitant\namount of randomness in the model’s moves, demonstrating\nthe severe impact the illegal move reminders have on Chat-\nGPT’s board comprehension.\n4.2\nGame Level Performance Evaluation\nIn this subsection, we aim to further investigate ChatGPT’s\nbehavior in chess games by conducting a manual analysis of\nthe game conversations. Our analysis focuses on assessing\nthe quality of ChatGPT’s moves, insights, and suggestions\nto gain a deeper understanding of its chess-playing capabil-\nities. We randomly selected 50 games from the Rsn-Simple\nvariation and truncated 30-70% of their moves (the percent-\nage for each game was decided randomly). For each game,\nwe prompted ChatGPT to simulate a skillful chess player\nand find black’s best move. The model’s response was then\nevaluated according to three criteria: Alignment, Insight, and\nSuggestions. Alignment measures whether one of ChatGPT’s\nmoves matches a move actually played in the original game.\nInsight measures the correctness of ChatGPT’s analysis of\nthe board (e.g., potential threats, strategies or possibilities\nfor checkmate). Suggestions evaluates whether all of Chat-\nGPT’s suggested moves are among the top four moves rec-\nommended by Stockfish for that particular board position.\nThe results of our evaluation revealed that only 9 out of the\n50 games exhibited proper alignment, 16 demonstrated accu-\nrate insight, and 39 had valid suggestions. The low alignment\nscores substantiate the significant role that model reasoning\nplays in the differences observed between the model’s behav-\nior in the baseline and Rsn-Simple experiments. This is at-\ntributed to ChatGPT’s reasoning process in this experiment,\nwhich since ChatGPT typically makes a move first and then\nprovides an explanation, closely mirrors that of the baseline\nexperiments. Further enhancing this argument, little correla-\ntion was found between the acceptance of insight and sug-\ngestions (Pearson’s r = 0.05), indicating that the insight pro-\nvided after moves does not influence the moves the model\nmakes. The poor insight scores align with the observations in\nSection 3.3, where the model tends to hallucinate board infor-\nmation such as achievements and threats, resulting in incor-\nrect strategic analysis. However, ChatGPT performed rela-\ntively better in terms of suggestions, with a significant propor-\ntion of games suggesting moves that Stockfish ranked among\nthe best four. This is consistent with the acceptable perfor-\nmance observed in the baseline experiment’s move quality.\nOverall, our manual analysis of ChatGPT’s games supports\nour previous arguments and is consistent with our statistical\ncalculations, highlighting the reasoning process’s impact on\nmodel decisions and the model’s challenges in generating ac-\ncurate insights.\n5\nDiscussion\n5.1\nChatGPT’s Performance Overview\nDespite ChatGPT’s demonstrated proficiency in natural lan-\nguage processing, it displays substantial limitations when it\ncomes to playing chess. In the 3200 games played during\nour experiment, ChatGPT failed to secure any victories. Fur-\nthermore, only 1.59% of games concluded naturally in ac-\ncordance with the standard rules of chess. Early termina-\ntions frequently occurred at ChatGPT’s second or third move,\nand games’ IMR and BE continuously increased through-\nout games, illustrating its struggle to both adhere to the\ngame’s rules and navigate the increasing strategic complexity\nas games progressed. The average game length across all ex-\nperimental variations was significantly lower than the human\naverage of 74.28 moves per game, as documented by Deleo\nand Guven [2022]. This disparity highlights the considerable\ngap between ChatGPT’s performance and human expertise in\nchess.\nAlthough GPT models like ChatGPT are trained to mem-\norize domain-specific information, such as chess rules, our\nexperiments reveal a clear challenge for ChatGPT in apply-\ning these rules effectively.\nIn the context of chess, every\ngame introduces unique strategic and positional situations,\nrequiring a dynamic application of chess rules.\nThe high\nIMR and RBLM across all variations underscore ChatGPT’s\ndifficulty in dynamically applying these memorized rules to\nnovel, complex scenarios. This observation persisted even\nwhen clearer board representations were provided, suggest-\ning that the high IMR may stem more from issues with rule\nadherence than from a lack of board state comprehension.\nThese findings raise critical concerns about deploying\nChatGPT in high-stakes contexts that demand the accurate\napplication of a comprehensive rule set, such as providing\nmedical diagnostics or legal interpretations. The model’s ob-\nserved inability to effectively self-regulate, despite possess-\ning an understanding of the rules, questions its reliability in\nsuch scenarios. Our study, although rooted in the context of\nchess, sheds light on potential limitations of ChatGPT and\nsimilar models when tasked with complex situations that ne-\ncessitate formal logic and strategic planning, enabling further\nresearch to better understand and address these limitations.\n5.2\nLimitations of ChatGPT’s Self-Attention\nMechanism in Chess Gameplay\nChatGPT’s self-attention mechanism plays a crucial role in its\nperformance, especially in chess gameplay. Our experiments\nreveal two critical limitations of transformer-based LMs like\nChatGPT when trained on natural language.\nThe first limitation is related to the increase in IMR and\nRBLM over the course of a game. As highlighted by [St¨ockl,\n2021], GPT-2 models are found to devote less attention to\nSAN notation tokens that are farther away from the latest in-\nput. Since a complete game memory is paramount for models\nto accurately track the board state [Toshniwal et al., 2022], we\npostulate that ChatGPT’s disproportionate attention alloca-\ntion might be the cause of a significant portion of its mistakes.\nThis limitation is evident across all variations that depend on\nformal language but don’t actively reinforce the game state,\nwhich presents a challenge to the effectiveness of LLMs in\ntasks requiring extended conversation memory.\nThe second limitation pertains to the tendency of natural\nlanguage trained LLMs to neglect formal language where\ntokens are used in an unconventional manner.\nMaynez et\nal. [2020] noted that LMs typically remain indifferent to\nnoises or artifacts in training data, which we argue may also\napply to formal languages like chess notations.\nThis is-\nsue is particularly evident in the Int-Rules variation, where\ndespite the introduction of helpful data, ChatGPT’s perfor-\nmance dropped substantially. We hypothesize that this may\nbe due to the model shifting its focus towards the rules,\nthereby reducing the attention allocated to the game board.\nThese identified limitations, while challenging, also pro-\nvide valuable insights for future research. For instance, ad-\ndressing the second limitation might involve frequent repe-\ntition of formal language sequences, potentially leading to\nmore substantial improvements in game performance. Our\nfindings is a first step towards investigating techniques such\nas token repetition’s impact on model performance, laying the\nground work for future work to explore how we can mitigate\nthe impact of disproportionate attention allocation.\n5.3\nIntent Behind LLMs’ Decisions\nDo LLMs actually exhibit strategies or ”intent” in their game-\nplay, or are they simply attempting to randomly predict le-\ngal moves?\nOur investigation into this issue involves the\nmodel’s RBLM and MRS, which we find a striking corre-\nlation between evidenced by a Pearson correlation coefficient\nof r = −0.86. Our findings corroborate that a decreased\nRBLM is indicative of the model contemplating fewer moves.\nThis, in turn, signifies a heightened degree of confidence in\nthe selection of moves at the level of output token probabil-\nity distribution. Therefore, when we observe low RBLM and\nhigh MRS, we can confidently infer that both increased natu-\nral language in the conversation and providing better board\nrepresentation enhance the model’s ”intent.” The effect of\nboard representation is especially noteworthy, as no natural\nlanguage clues were provided in these cases, making it im-\npossible for the model to exclude moves for the purpose of\nmaintaining a consistent narrative. However, it is important\nto bear in mind that ”intent,” as we define it here, doesn’t nec-\nessarily equate to better move quality—it simply means that\nthe model is making decisions in a non-random manner. We\nencourage future work to conduct detailed examination of the\nmodel’s decisions across multiple moves to evaluate the pres-\nence of a consistent, long-term strategy.\n6\nConclusion\nIn summary, our investigation reveals that despite its excep-\ntional capabilities in natural language processing, ChatGPT\nfaces considerable challenges with complex reasoning tasks\ninvolving formal language, as evidenced by its chess game-\nplay performance.\nThe model’s attention mechanism ex-\nhibits limitations in adequately recognizing tokens used in\nformal language, resulting in a suboptimal understanding of\nthe game board. Interestingly, our findings indicate that con-\nsistent repetition of relevant information throughout a conver-\nsation can partially alleviate this limitation. Yet, despite Chat-\nGPT’s capacity to learn and internalize rules, the model strug-\ngles with self-regulation, which neither in-prompt instruc-\ntions nor improved board comprehension appear to enhance.\nAdditionally, we find that the model’s decision-making focus,\nor ”intent,” can be strengthened by allowing NL reasoning,\nproviding NL chessboard descriptions or enabling a clearer\nrepresentation of the game board. Future research could ex-\namine how this disproportionate attention allocation impacts\nother tasks that involve formal language and necessitate com-\nplex cognitive processing.\nIn conclusion, while ChatGPT\nstands as a remarkable advancement in artificial intelligence,\nit continues to face significant limitations, especially in non-\nlinguistic contexts. These findings highlight the necessity for\nfurther refinement before ChatGPT, and models of its kind,\ncan be considered reliable tools for practical applications re-\nquiring complex cognition akin to human abilities.\nAcknowledgments\nWe extend our acknowledgement to Mr. Cheng-Chi Lu for his\nmathematical consultations and astute insights, which have\ngreatly enhanced the clarity and precision of this work. His\nexpertise has been a valuable asset in the crafting of this pa-\nper. We would also like to express our profound gratitude\nto Ms. Yi-Pin Lin, whose guidance and unwavering support\nhave been instrumental in this research.\nReferences\n[Bang et al., 2023] Yejin\nBang,\nSamuel\nCahyawijaya,\nNayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy\nLovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A\nmultitask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity.\narXiv\npreprint arXiv:2302.04023, 2023.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al.\nLanguage models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877–1901, 2020.\n[Burgoyne et al., 2016] Alexander P. Burgoyne, Giovanni\nSala, Fernand Gobet, Brooke N. Macnamara, Guillermo\nCampitelli, and David Z. Hambrick. The relationship be-\ntween cognitive ability and chess skill: A comprehensive\nmeta-analysis. Intelligence, 59:72–83, 2016.\n[Chowdhery et al., 2022] Aakanksha\nChowdhery,\nSharan\nNarang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles\nSutton,\nSebastian Gehrmann,\net al.\nPalm:\nScal-\ning language modeling with pathways.\narXiv preprint\narXiv:2204.02311, 2022.\n[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Moham-\nmad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman. Training\nverifiers to solve math word problems, 2021.\n[DeLeo and Guven, 2022] Michael DeLeo and Erhan Gu-\nven. Learning Chess and NIM with Transformers. Interna-\ntional Journal on Natural Language Computing, 11(5):1–\n15, October 2022.\n[DeYoung et al., 2020] Jay\nDeYoung,\nSarthak\nJain,\nNazneen Fatema Rajani, Eric Lehman, Caiming Xiong,\nRichard Socher, and Byron C. Wallace.\nEraser:\nA\nbenchmark to evaluate rationalized nlp models, 2020.\n[Gobet, 1998] Fernand Gobet. Chess players’ thinking revis-\nited. Swiss Journal of Psychology, 57, 01 1998.\n[Huang and Chang, 2022] Jie Huang and Kevin Chen-Chuan\nChang. Towards reasoning in large language models: A\nsurvey, 2022.\n[Jacovi and Goldberg, 2020] Alon Jacovi and Yoav Gold-\nberg. Towards faithfully interpretable NLP systems: How\nshould we define and evaluate faithfulness?\nIn Pro-\nceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4198–4205, Online,\nJuly 2020. Association for Computational Linguistics.\n[Kojima et al., 2023] Takeshi Kojima, Shixiang Shane Gu,\nMachel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023.\n[Liu et al., 2023] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue\nZhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu,\nXiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and\nBao Ge. Summary of chatgpt/gpt-4 research and perspec-\ntive towards the future of large language models, 2023.\n[Maynez et al., 2020] Joshua\nMaynez,\nShashi\nNarayan,\nBernd Bohnet, and Ryan McDonald. On faithfulness and\nfactuality in abstractive summarization. In Proceedings of\nthe 58th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1906–1919, Online, July 2020.\nAssociation for Computational Linguistics.\n[Obasola et al., 2022] Oluwaseun Ireti Obasola, Alison An-\nnet Kinengyere, Devind Peter, Diston Chiweza, Amanda\nRoss-White, and Christina Godfrey. Perceptions, experi-\nences, and attitudes of health care professionals regard-\ning the role of librarians in fostering evidence-based health\npractice: a systematic review protocol. JBI Evidence Syn-\nthesis, 20(1):181–188, 2022.\n[Ouyang et al., 2022] Long Ouyang, Jeff Wu, Xu Jiang,\nDiogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. Training lan-\nguage models to follow instructions with human feedback,\n2022.\n[Patel et al., 2021] Arkil Patel, Satwik Bhattamishra, and\nNavin Goyal. Are NLP Models really able to Solve Sim-\nple Math Word Problems?\nIn Proceedings of the 2021\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies, pages 2080–2094, Online, 2021. Associa-\ntion for Computational Linguistics.\n[Pinheiro, 2017] Marcia Pinheiro. Skills for chess. IJIER,\n504, 04 2017.\n[Reynolds and McDonell, 2021] Laria Reynolds and Kyle\nMcDonell. Prompt programming for large language mod-\nels: Beyond the few-shot paradigm. In Extended Abstracts\nof the 2021 CHI Conference on Human Factors in Com-\nputing Systems, pages 1–7, 2021.\n[Shridhar et al., 2021] Mohit Shridhar, Xingdi Yuan, Marc-\nAlexandre Cˆot´e, Yonatan Bisk, Adam Trischler, and\nMatthew Hausknecht. Alfworld: Aligning text and em-\nbodied environments for interactive learning, 2021.\n[Simon and Chase, 1988] Herbert\nSimon\nand\nWilliam\nChase.\nSkill in chess.\nComputer chess compendium,\npages 175–188, 1988.\n[St¨ockl, 2021] Andreas St¨ockl. Watching a language model\nlearning chess. In Proceedings of the International Confer-\nence on Recent Advances in Natural Language Processing\n(RANLP 2021), pages 1369–1379, Held Online, Septem-\nber 2021. INCOMA Ltd.\n[Swami, 2013] Sanjeev Swami. Executive functions and de-\ncision making: A managerial review. IIMB Management\nReview, 25(4):203–212, 2013.\n[Toshniwal et al., 2022] Shubham Toshniwal, Sam Wise-\nman, Karen Livescu, and Kevin Gimpel.\nChess as a\nTestbed for Language Model State Tracking.\nProceed-\nings of the AAAI Conference on Artificial Intelligence,\n36(10):11385–11393, June 2022.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gau-\ntier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models, 2023.\n[Tu et al., 2023] Ruibo Tu, Chao Ma, and Cheng Zhang.\nCausal-discovery performance of chatgpt in the context of\nneuropathic pain diagnosis, 2023.\n[Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schu-\nurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves\nchain of thought reasoning in language models, 2023.\n[Webson and Pavlick, 2022] Albert\nWebson\nand\nEllie\nPavlick. Do prompt-based models really understand the\nmeaning of their prompts?\nIn Proceedings of the 2022\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies, pages 2300–2344, Seattle, United States,\nJuly 2022. Association for Computational Linguistics.\n[Wei et al., 2023] Jason Wei, Xuezhi Wang, Dale Schuur-\nmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-thought prompting elicits\nreasoning in large language models, 2023.\n[Yang et al., 2018] Zhilin Yang, Peng Qi, Saizheng Zhang,\nYoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for di-\nverse, explainable multi-hop question answering, 2018.\n[Zhou et al., 2023] Denny Zhou, Nathanael Sch¨arli, Le Hou,\nJason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-\nmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi.\nLeast-to-most prompting enables complex reasoning in\nlarge language models, 2023.\nA\nFull Experiment Data\nBaseline\nInt-Illegal\nInt-Rules\n88.38\n84.38\n90.84\nMove-Repeat\nMove-IlgRem\nRsn-Simple\n148.24\n71.7\n145.64\nRsn-CoT\nRsn-DropCoT\nDsc-Base\n166.79\n194.12\n293.35\nTable 7: Average BE (Full Game) per Variation\nFigure 1: Average IMR by Move\nFigure 2: Average RBLM by Move\nFigure 3: Remaining Games by Move\nFigure 4: Board Evaluation by Move\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-08-29",
  "updated": "2023-08-29"
}