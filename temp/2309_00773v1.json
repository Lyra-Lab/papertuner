{
  "id": "http://arxiv.org/abs/2309.00773v1",
  "title": "Deep Reinforcement Learning in Surgical Robotics: Enhancing the Automation Level",
  "authors": [
    "Cheng Qian",
    "Hongliang Ren"
  ],
  "abstract": "Surgical robotics is a rapidly evolving field that is transforming the\nlandscape of surgeries. Surgical robots have been shown to enhance precision,\nminimize invasiveness, and alleviate surgeon fatigue. One promising area of\nresearch in surgical robotics is the use of reinforcement learning to enhance\nthe automation level. Reinforcement learning is a type of machine learning that\ninvolves training an agent to make decisions based on rewards and punishments.\nThis literature review aims to comprehensively analyze existing research on\nreinforcement learning in surgical robotics. The review identified various\napplications of reinforcement learning in surgical robotics, including\npre-operative, intra-body, and percutaneous procedures, listed the typical\nstudies, and compared their methodologies and results. The findings show that\nreinforcement learning has great potential to improve the autonomy of surgical\nrobots. Reinforcement learning can teach robots to perform complex surgical\ntasks, such as suturing and tissue manipulation. It can also improve the\naccuracy and precision of surgical robots, making them more effective at\nperforming surgeries.",
  "text": "Deep Reinforcement Learning in Surgical \nRobotics: Enhancing the Automation Level \n \nCheng Qian# and Hongliang Ren* \n#Department of Electrical Engineering and Information Technology, Technical University of \nMunich, Germany \n*Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong \n \nAbstract:  \nSurgical robotics is a rapidly evolving field that is transforming the landscape of surgeries. \nSurgical robots have been shown to enhance precision, minimize invasiveness, and alleviate \nsurgeon fatigue. One promising area of research in surgical robotics is the use of reinforcement \nlearning to enhance the automation level. Reinforcement learning is a type of machine learning \nthat involves training an agent to make decisions based on rewards. This literature review aims \nto comprehensively analyze existing research on reinforcement learning in surgical robotics. \nThe review identified various applications of reinforcement learning in surgical robotics, \nincluding pre-operative, intra-body, and percutaneous procedures, listed the typical studies, \nand compared their methodologies and results. The findings show that reinforcement learning \nhas great potential to improve the autonomy of surgical robots. Reinforcement learning can \nteach robots to perform complex surgical tasks, such as suturing and tissue manipulation. It can \nalso improve the accuracy and precision of surgical robots, making them more effective at \nperforming surgeries. \nKey Words:  \nSurgical robotics, reinforcement learning, surgical autonomy, tissue manipulation, \npercutaneous procedures, suturing  \nI. Introduction \nThe use of surgical robots has significantly increased in the last decade, driven by the need for \nprecision, safety, and efficiency in surgeries [1]. Since the appearance of da Vinci robotic-\nassisted surgical system in 2000 [3], surgical robots have proven to help perform minimally \ninvasive surgeries (MIS), providing better visualization, higher precision, and reduced \ninvasiveness, and helping reduce surgeons' fatigue [2]. However, the full potential of surgical \nrobots has yet to be realized, and there is still a need to improve their autonomy. In the last \ndecade, more and more studies have been conducted on autonomous surgical robots [4]. To \nachieve autonomy in surgery, it is crucial for robots to understand the surgical task objectives, \nperceive complex physical environments, and autonomously make decisions. One of the biggest \nchallenges in the autonomy of surgical robots is the high variance of surgical tasks [13], which is \nhard to address by explicitly modeling and planning. Therefore, Artificial Intelligence (AI) \nsolutions emerged due to the model-free property and learning capability. \nDeep Reinforcement Learning (DRL), a deep learning-based planning method that allows \nrobots to learn from interaction with environments in a semi-supervised fashion without a pre-\ndefined model, is one of the most promising approaches [5]. DRL has been increasingly \nhighlighted in recent years, since its success in Atari [7]. It has demonstrated the possibility of \nenabling intelligent agents to outperform human experts in multiple fields. Compared to \nconventional planning methods, DRL provides advantages, e.g., end-to-end learning, complex \ndecision-making, generalization, and transferability, handling uncertainties, and continuous \nlearning. These properties enable DRL to handle high-dimensional inputs from cameras and \nsensors in surgeries, apply its acquired knowledge and skills to different patients, handle \nunforeseen variations, and continuously learn and refine their performance during surgery \nprocedures. For these reasons, in the context of surgical robots, DRL provides a powerful \nmodel-free framework and a set of tools for learning various complicated surgical tasks with \ncomplex physical environments, which are hard to model [6]. Many studies have utilized DRL \non robots under abundant surgical scenarios, e.g., ultrasound scanning, cutting and sewing, \ntissue retraction, needle steering, and catheterization. Currently, there have been some reviews \non DRL in the scope of medical imaging, e.g., radiation therapy, image registration [8], health \ncare application, e.g., clinical decision support [9, 10, 12], medicine, e.g., medicine treatment or \ndevelopment [11]. However, there still lacks a review specifically on the applications of DRL in \nmedical robots. To fill this gap, this chapter will present a literature review highlighting the \ntypical state-of-art works in the past 5 years (2018-2023) that utilize DRL in autonomous \nsurgical robots, and comparing their methodologies, limitations, and results. We divide these \nworks into three categories according to their access modes, namely: \n1) Extra-body skin-interfaced procedure \n2) Intra-body procedure \n3) Percutaneous procedure, \nwhich are three main procedures that we find the combination of DRL and surgical robot is \nmainly applied to. The various surgical tasks learned by the robot in this review are illustrated \nin Fig. 1, including steerable needle planning in keyhole neurosurgery, needle insertion in \nophthalmic microsurgery, neck vessel and spine US scanning, tissue cutting and retraction, and \nwound suturing. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 1. The seven different robotic surgical tasks contained in this review \n \nTo exhaust the published review articles of the concerning fields and extract the most relevant \nones, we searched keywords on the database and excluded the irrelevant articles. The articles \nextraction pipeline is shown in Fig. 2. We also counted the number of studies that applied DRL \nOphthalmic \nmicrosurgery \nNeck vessel US \nscanning \nSpine US scanning \nWound suturing \nKeyhole neurosurgery \nTissue retraction \nTissue cutting \nin medical scenarios in the last 5 years. In Fig. 3, the number of articles on the application of \nDRL in medical imaging, medical robotics, and dynamic treatment regime in recent 5 years are \nlisted.  \n \nWe can see that the number of studies combining DRL with different medical fields has quickly \nemerged in recent years, which indicates a growing trend.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 2. Articles selection pipeline with keywords \"surgical robot\", ‚Äúautonomous‚Äù, and ‚Äúdeep \nreinforcement learning‚Äù according to PRISMA [81] \nDatabase search \nn = 1070 articles \nn = 265 articles \nn = 39 articles \nn = 26 articles \nn = 805 articles excluded \nReview or Survey = 186 \nEarlier than 2000 = 323 \nNot focus on surgical robot = 232 \nFocus on environment simulation = 64 \nn = 226 articles excluded \nFocus on surgeon training = 15 \nFocus on development = 38 \nNot focus on deep reinforcement learning = 93 \nNot focus on autonomous robotic surgery = 76 \nn = 13 articles excluded \nFull text unavailable = 6 \nLack of experiments = 7 \n \n \nFig. 3. Statistics of the number of publications on RL in three main medical applications \nin PubMed in the past 5 years. The combination of DRL with autonomous surgical \nrobots and other medical fields have a rising trend in the last 5 years. \nThe rest of the chapters are organized as follows: Section 2 briefly introduces the fundamental \ntheories in RL. Sections 3, 4, and 5 discuss the latest work of DRL in the fields of pre-operative \nscanning, intra-body surgery, and Image-guided autonomous robotic surgery, respectively. \n \nII. Basics of Reinforcement Learning \nBefore discussing the state-of-art works of DRL in surgical robotics fields, we will first give a \ngeneral introduction to the fundamental knowledge in Reinforcement learning (RL). Learning \nthrough interaction with the environment is the essence of RL [14], which means an agent learns \nto take action through rewards and penalties and refines its policy accordingly.  \nThe fundamental of RL includes five essential elements: agent, environment, action, state, and \nreward. In the context of surgical robots, an example of it can be illustrated in Fig. 4, where the \nrobot (agent) works at the surgical site of a human body (environment), moving the probe to \nfind a feasible scan plane for the sacrum, and obtaining the current position information of \nprobe via real-time ultrasound (US) images. At each time step, the robot possibly gets a positive \nor negative reward based on the current US image, which guides the robot toward the standard \nscan plane.  \n \n0\n10\n20\n30\n40\n2018\n2019\n2020\n2021\n2022\nNumber of publications on RL in medical applications in PubMed in \nthe past 5 years\nMedical Imaging\nMedical Robotics\nDynamic Treatment Regime\n \n \n \n \n \n \n \n \n \n \n \nFig. 4. An illustration of agent-environment interaction in RL under the context of \nsurgical robots \n[Markov Decision Process] \nMarkov Decision Process (MDP) is always used to formally describe the above-mentioned \nagent-environment interaction, which consists of [17, 18]: \nÔÇ∑ \nState Space (S): The set of possible states the agent can be. Each state represents a \nparticular configuration or situation in the environment. In surgical robot scenarios, the \nstate is often chosen as the robot's pose and target. \nÔÇ∑ \nAction Space (ùíú): The set of possible actions that the agent can take. Actions are the \nchoices available to the agent in each state. Actions can be discrete or continuous \ndepending on the accuracy requirements. It is preferred to be chosen as continuous in \nsome safety-critical scenarios, which need high accuracy of control, e.g., in intra-body \nsurgery. \nÔÇ∑ \nTransition (ùíØ): The transition probabilities that describe the dynamics of the \nenvironment. They specify the probability of transitioning from one state ùë† to another \nstate ùë†‚Ä≤, ùëñf the agent takes action ùëé, which is represented as ùëá(ùë†‚Ä≤|ùë†, ùëé). It includes \nimportant information about the robot-environment interaction, e.g., the interaction \nbetween the US probe and tissue. \nÔÇ∑ \nReward (R): The immediate feedback that the agent receives from the environment for \nits action. It quantifies the desirability or value associated with transitions between \nEnvironment \nAgent \nAction(a) \nState(s) \nReward(r) \n  \nstates. The reward function is typically denoted as ùëü= ùëÖ(ùë†‚Ä≤, ùëé, ùë†). It is commonly \ndesigned to guide the robot to achieve its goal. For example, for standard scan plane \nnavigation in robotic ultrasound scanning, the reward is commonly designed to be the \npose improvement of the probe to the target pose. \nÔÇ∑ \nDiscount factor (Œ≥): A value between 0 and 1 that determines the importance of future \nrewards compared to immediate rewards. It determines the preference of the agent for \nimmediate rewards or long-term cumulative rewards.  \nGiven ùëÄùê∑ùëÉ(ùëÜ, ùê¥, ùëá, ùëÖ, ùõæ), the agent chooses the action at state ùë† with the observation ùëú it receives \naccording to the policy ùúã(ùëé|ùë†). When the policy is deterministic, ùúã is a mapping from state ùë† to \naction ùëé; when the policy is stochastic, ùúã represents the possibility of selecting action ùëé at state ùë†.  \nThe goal of RL is to find an optimal policy ùúã‚àó that maximizes the expectation of cumulative \nreturn, which is denoted as: \nùúã‚àó= ùëöùëéùë•\nùúã\nùîº[‚àë\nùõæ ùëüùë°]\nùëá\nùë°=0\n , \nwhere ùëüùë° is the reward at time ùë°, and ùëá is the time horizon. \nTo be noticed that sometimes the full state information is not available for the agent, but only a \npart of it, instead. The agent has to predict the state information given an observation. For \nexample, the ultrasound scanning robot has to detect its current position according to the real-\ntime US image. In this case, the process is a partially observable MDP (POMDP) [29]. And the \nset of the state information that is observable for the agent is called Observation. In this case, the \npolicy ùúã is dependent on observation ùëú instead of state ùë†. \nBesides, in this review, only model-free RL algorithms are focused on. Therefore, the transition \nis assumed to be unknown. \n \nIII. Deep Reinforcement Learning in Surgical \nRobotics \nIn this section, we will highlight the state-of-art studies of DRL for surgical robotics applications \nand discuss them in three parts: pre-operative scanning, intra-body surgery, and percutaneous \nsurgery. We will focus on how they formulate the problem in a DRL-based framework and \ndifferent methodologies applied to augment RL to meet some surgery-specific requirements, \nsuch as risk analysis. \n[Pre-operative Procedure] \nSurgical images are obtained using various imaging modalities and ultrasound (US) scanning is \nthe one that has been widely studied in combination with robotics. Over the past two decades, \nresearchers have begun exploring the potential of robotics in applying US scanning. By \nequipping the robot arm with a probe, the robot can move the probe to perform US scanning on \nthe patient. The accuracy, consistency, skill and maneuverability of robotic manipulators can be \nused to improve the acquisition and utility of real-time ultrasounds [25]. However, to obtain \nhigh-quality ultrasound images, it is crucial to navigate the US probe to the correct scan plane \n[26] and maintain reasonable and consistent probe-skin contact force [27], as illustrated in Fig. 5. \nTherefore, standard scan plane localization and contact force control are two main challenges in \nrobotic US and so far, there have been several studies utilizing DRL to address them. In Table 1, \nthe methodologies, metrics, and results used in the 9 reviewed papers in this section are listed. \na. Standard Scan Plane Navigation \nA standard scan plane in ultrasound imaging refers to a recommended imaging plane or view \ncommonly used for a specific anatomical structure or diagnostic purpose. Finding the \nappropriate scan planes is crucial to obtaining good-quality US images. To enable autonomous \nrobotic ultrasound scanning, the robot should be capable of detecting its own position and \nfinding the way toward the standard scan plane of the specific anatomy with the real-time US \nimage it obtains.  \nHase et al. [28] proposed a framework in 2020 to train the robot to autonomously navigate to \nthe standard scan plane of the sacrum with the information of a sequence of history US images \nwith Deep Q-Network (DQN) [7]. The agent is trained on the 2D US images acquired by grid \ncovering and moves with 2-DOF actions, namely moving forward or backward. When moving \ncloser to or further from the desired scan plane, the agent receives positive or negative rewards. \nA binary classifier that determines whether the robotic probe is at the standard scan plane, \ndepending on the current US image, is used to make the agent stop at the correct position.  \nOne of the limitations in [28] is that the probe is assumed to find the scan plane by moving in a \n2D space, which is unavailable in real US scanning scenarios, where the relative pose between \nthe probe and sacrum is not static. Therefore, in Li et al. [31], the state and action space is \ndesigned to be the 6D-pose and -twist of the probe so that the learned policy is no longer \nrestricted to the collected data. The agent receives a reward proportional to the pose \nimprovement of the probe at each time step. Besides, the quality of the US image is also \nconsidered in this work by evaluating the pixel-wise confidence and giving corresponding \nrewards to the agent. \nAn agent that can recognize different anatomies and find the nearest one according to its \ncurrent position can be advantageous for its flexibility in a real US scanning scenario. In Li et al. \n[33], an agent is trained to find three different spinal anatomies with a given standard view \nrecognizer for different spinal anatomies. \nReal US images as observations given to the agent can be noisy, which makes the agent hard to \npredict the real state correctly. One of the methods addressing this issue is presented in Bi, et al. \n[39], which navigates the agent to the scan plane of the neck vessel. The study segments the US \nimages with a pre-trained U-Net [40] in advance and provides the segmented mask to the agent, \nwhere the area of interest has pixel values of one, while other areas have values of zero. \nCompared to real US images, it is much easier for the agent to extract information from \nsegmented masks, which only contain binary values. \nBesides, Li, et al. [41] and Milletari, Fausto, Vighnesh Birodkar, and Michal Sofka [42] proposed \nDRL-based frameworks for training an agent that guides a novice operator to find the standard \nscan planes in transesophageal echocardiography and chest sonography, respectively.  \nb. Pose and Force Control \nThe way in which the ultrasound probe is positioned and controlled can have a significant \nimpact on both the quality of the resulting ultrasound images and the overall safety of the \nrobotic ultrasound system. It is essential to carefully control the pose and force used when \noperating the probe, as any errors or inconsistencies can compromise the quality of the imaging \nand potentially cause harm to the patient or the system itself. A system can ensure imaging \nquality while minimizing potential risks or complications by taking a deliberate approach to \nprobe control. Unlike rigid objects, force control of the US probe should consider the \ncompliance of the patient body. Besides, errors caused by target movement have to be also \ncompensated. Both of them are hard to be accurately modeled. However, by taking advantage \nof DRL's model-free and end-to-end properties, the control of the US probe can be solved \nwithout explicitly modeling.  \nIn Ning, et al. [42], an agent is trained with Proximal Policy Optimization (PPO) [21] to \nautonomously control the pose and force of the US probe with a force-to-displacement \nadmittance controller. The agent has to provide proper 2D input command for the controller, \nnamely the desired torque of the US probe in long- and short-horizontal-direction, as illustrated \nin Fig. 5, to keep the vertical between the probe and the scanned surface with suitable contact \nforce. A 6-D force sensor is attached to the robot end-effector to give the force feedback to the \nagent. A positive reward is given when the vertical contact force is suitable and the horizontal \ncontact force is small enough, which means the probe is approximately vertical to the scanned \nsurface. A similar work is done in [46], however, with an inverse RL method to study the \nreward function. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 5. The robot moves the probe to find the standard scan plane while keeping the contact \nforce in a suitable range. \nDiffering from [42] and [46], in Ning, et al. [44], the scene image captured by a RGB camera is \nalso provided to the agent as observation. From the scene image, the agent can extract the \ninformation of both its own pose and the target pose. In the study [45], a convolutional \nautoencoder (CAE) and a reward prediction network are employed to achieve two objectives \nsimultaneously. Firstly, the CAE is used to decrease the dimensionality of the observation \nspace, allowing for more efficient data processing. Secondly, the reward prediction network \nencodes force and ultrasound image information into the scene image, enhancing the resulting \nimage's quality. By utilizing these techniques, the researchers improved the overall efficiency of \nthe system. \n[Intra-body Procedure] \nRecently, flexible surgical robotic systems have been developed to improve intra-body surgery \nin the narrow areas of the human body. However, the teleoperation of surgical robots can be \nexhausting and needs long-term training time. More and more researchers have been working \non the possibility of automating difficult surgical handling tasks, e.g., tissue cutting, suturing, \nknot tying, and tissue retraction, to reduce the surgeons‚Äô workload [64]. However, the large \nquantity of soft tissue in surgeries, including organs, blood vessels, and muscles, possess \ninherent compliance and deformability, making their manipulation challenging and requiring \nmodeling and planning with high accuracy and complexity. Therefore, some studies have tried \nContact  \n Force \nt \nto unleash the model-free property of DRL in automating the tissue manipulation tasks in MIS, \nincluding tensioning, suturing and retraction. In Table 2, the methodologies, metrics, and \nresults used in the 8 reviewed papers in this section are listed. \n \nRef. \nDescription \nAlgorithm \nObservation \nDOF \nof \nAction \nReward \nResult \n[28] \nNavigation towards the \nstandard scan plane of sacrum \nDQN \nSequential US \nimages \n1 \n+ moving closer  \n- moving further \nPolicy correctness of 79.53% \nand reachability of 82.91% \n[31] \nNavigation towards the \nstandard scan plane of the \nspine with consideration of US \nimage quality \nDQN \nSequential US \nimages \n6 \n+ pose improvement or \nimage quality improvement \n- unallowable pose \n92% and 46% success rate in \nintra- and inter-patient \nsettings, respectively \n[33] \nNavigation towards different \nstandard scan planes of spinal \nanatomies \nDQN \nSequential US \nimages \n6 \n+ pose improvement or \nimage quality improvement \n- \nunallowable pose \nPose error ~ 5.18mm/5.25¬∞ \nfor intra-patient settings; \nPose error ~ 2.87mm/17.49¬∞ \nfor inter-patient settings. \n[39] \nNavigation towards the \nstandard scan plane of carotid \nvessels \nA2C \nSequential \nsegmented US \nimages + \nSequential \nvessel area \nchanges \n3 \n+ vessel area improvement  \n- too small vessel area \n91.5% and 80% success rate \nin simulated and real \nenvironment, respectively \n[41] \nGuidance for novice operators \nin moving TEE probe towards \nthe standard scan plane of heart \nwith pressure awareness \nDQN \nSequential US \nimages \n3 \n+ pose improvement \n- unallowable pose \nPose error of 2.72 mm / \n2.69¬∞ and 8.15 mm / 5.58¬∞ \nwithout and with pressure \nawareness, respectively \n[42] \nGuidance for novice operators \nin obtaining correct US images \nof anatomy of interest \nDQN \nSequential US \nimages \n4 \n+ moving closer \n- moving further \n86.1% success rate in giving \ncorrect guidance \n[43] \nForce control between probe \nand phantom \nPPO \n6-D contact \nforce \n2 \n+ small horizontal force \n- big horizontal force or too \nbig or too small vertical force \nDifference of skin area in US \nimages within 3‚Äâ¬±‚Äâ0.4% from \nthe hand-free scanning \napproach \n[44] \nForce control between probe \nand phantom \nPPO \nSingle encoded \nRBG scene \nimage \n3 \n+ moving closer to the target \nsurface, good US image \nquality, correct relative \nposition \n- otherwise \n93% success rate in getting \nfeasible US images \n[48] \nForce control between probe \nand phantom \nPPO + \nInverse RL \nContact force \nand torque, \nand \ncorresponding \nlinear and \nangular speed \n6 \nReward shaping via inverse \nRL \nPosture error of 2.3¬±1.3¬∞and \n1.9¬±1.2¬∞ in X and Y axis \ncompared to manual \noperation \n \nTable 1. The formulation, methodologies and results of the reviewed papers in the section on pre-operative planning\na. Tensioning \nRobotic surgery has revolutionized the medical field, and an electric knife is an effective tool for \ncutting and removing thin tissues. However, the electric knife alone may not be enough to cut \neffectively when it comes to deformable soft tissue. This is because soft tissue needs to be held \nin tension to be cut most effectively. Therefore, a second tool is required to pinch and tension \nthe material while cutting. This technique is illustrated in Fig. 6, which demonstrates the use of \ntwo tools cooperatively to cut soft tissue. The first tool, the knife, cuts the tissue while the \nsecond tool, which pinches and tensions the material, helps the knife cut more effectively. This \ntechnique is particularly important in robotic surgery, where precision is critical, and using \nmultiple tools can help ensure the surgery is successful. To let the robot autonomously assist the \nsurgeon in cutting. The robot has to learn the tensioning policies for different cutting contours.  \n  \n \n \n \n \n \n \n \nFig. 6. The blue manipulator pinches the grey point and tensions the tissue, while the green \nmanipulator is responsible for cutting (e.g., in Endoscopic Submucosal Dissection, ESD) \nIn Thananjeyan, et al. [50], a finite-element model is first developed for simulating the \ndeformation and cutting of tissue. Then, considering the kinematics constraints of the surgical \nrobot arm, the cutting outline is divided into several subdivision segments in advance. The \nagent is lastly trained in Trust region policy optimization (TRPO) [22] to learn the optimal \ntensioning policy to minimize the cutting error with a single fixed pinch point. The agent \nreceives sparse rewards at the end of each episode according to the final cutting error. \nThere needs to be more than a single tensioning point to assist cutting, when the cutting pattern \nis complex, for example, the cutting contours are zigzag and have to be divided into many \nsegments. Therefore, in [51, 52], an improved pipeline is proposed to address this limitation. \nSpecifically, a pinch point is chosen for each cutting segment instead of the whole contour and \nthe agent learns different tensioning policies for each pinch point in a similar way as in [50]. \nCompared to [50], the improved method shows more accurate and robust performance, when \nhandling complex cutting contours. \nb. Suturing \nSuturing is a critical step in wound closure during surgeries and in robot-assisted surgeries. \nHowever, robotic suturing can be laborious for novice operators. A collaborative robot that \nautonomously assists surgeons in performing some sub-tasks in robotic surgeries can effectively \noperators‚Äô fatigue. So far, utilizing DRL on surgical collaborative robots can learn how to \nautonomously collaborate with surgeons in teleoperated suturing process, as illustrated in Fig. \n7. In Table 3, the methodologies, metrics, and results used in the 9 reviewed papers in this \nsection are listed. \nIn Varier, et al. [53], an agent is trained to use an assistive Patient Side Manipulator (PSM) to \npull the needle, translate it to the next suture point and hand it to the surgeon after the needle is \ninserted through the tissue with the main PSM by the operator, as illustrated in Fig. 7. To \naddress varying suture styles of users, the users are instructed to perform a running suture \nwithout a collaborative robot. The trajectory of a single hand-off task is collected and an \nalgorithm is designed to generate sparse rewards on the trajectory. Then, the agent imitates the \noperator‚Äôs hand-off trajectory by maximizing the cumulative collected reward. \n \n \n \n \n \nFig. 7. The main manipulator (left) is operated by the surgeon, which inserts the needle through \nthe tissue, while the assistive manipulator (right) pulls the needle out and hands it to the main \nmanipulator. \n \nHowever, in [53], the state of the agent is respect to a fixed frame, which makes the learned \npolicy strongly depend on the selection of the frame. In Chiu, et al. [55], an improved method is \nproposed to address this issue by designing action spaces as being respect to the ego-centric \nframe, which means the policy depends on the relative position and orientation of the assistive \nPSM relative to the main PSM and therefore can be directly applied to different robot \nconfigurations.  \nc. Retraction \nAnother kind of tissue manipulation task in robotic surgery is tissue retraction. I.e., to uncover \nthe underlying anatomical region, the tissue is repeatedly held and pulled back in MIS [57]. To \nautonomously perform the tissue retraction task, the robot has to find the position of the tissue, \nmove closer to it and grasp it to the target position. \nIn Pore, et al. [58], a robot learns to approach the tumor from its initial position and retract it to \nthe target position from human demonstrations. The position of the tumor is assumed to be \nknown from the pre-operative data. The agent gets the reward based on whether it moves closer \nto the tumor or target position before or after grasping. Human demonstrations are collected to \nenable imitation learning. The agent is trained with Generative adversarial imitation learning \n(GAIL) and PPO, where PPO acts as the action generator. \n Safety is always the priority in surgeries, especially in tissue retraction, where the robot directly \ninteracts with the tissue. However, due to the model-free property of DRL, the safety of the \nlearned policy is always hard to verify, which leads to significant potential risks in surgery. In \nanother work by Pore, et al. [60], a framework for robotic tissue retraction incorporates the \nsafety constraints during the DRL training with formal verification, which adds a penalty term \nin the reward function for unsafe actions and evaluates the safety of the learned policy [61]. The \nproposed method shows a large reduction in the safety violation rate, compared to [58]. \nIn [58] and [60], the agents are assumed to have access to the full-state information, e.g., the \nrobot joint angles and tissue position, which makes the policy largely depend on the accuracy of \nstate extraction and lack robustness against the patient movements. In Scheikl, et al., [62], a \nvision-based framework for robotic tissue retraction is proposed. The agent is trained with the \nsimulated RGB scene image and a translation model is trained to translate the observation \nfunction in simulation to the one in reality using domain adaptation. The trained agent achieves \na success rate of 50% in real surgical scenarios. \n \n[Percutaneous Procedure] \nPercutaneous techniques are increasingly used in many surgical scenarios, including \nneurosurgery and ophthalmic surgery. The practical advantages include lower complexity rates \nand faster recovery time. It involves the precise insertion of a thin, hollow needle into specific \nanatomical structures for diagnostic, therapeutic, or monitoring purposes. This technique \nallows surgeons to access internal body tissues, organs, or vessels without the need for open \nsurgery, minimizing patient discomfort, reducing the risk of complications, and promoting \nfaster recovery. \na. Needle Insertion \nNeedle insertion surgery is common in various medical fields, including keyhole neurosurgery \n[65] and ophthalmic surgery [66]. Conventionally, a rigid needle is commonly used in these \nprocedures. There has been some research on DRL-based needle path planning. In Keller et al. \n[73], an agent is trained to control the yaw, pitch, and depth of the needle to achieve the goal \nposition in ophthalmic surgery with optical coherence tomography (OCT) image as observation. \nIn Gao, et al. [74], an agent is trained to provide a remote center of motion (RCM) [75] \nrecommendation in brain surgery. The author considers three aspects to evaluate the quality of \nRCM, namely clinical obstacle avoidance (COA), mechanically inverse kinematics (MIK) and \nmechanically less motion (MLM) and the reward function is also designed based on these three \naspects. \nHowever, it can be challenging for rigid needles to find safe trajectories to insert toward the \ntarget without touching some critical anatomies, e.g., blood vessels, especially when the \nstructure is complicated. Therefore, steerable needles have attracted much attention in the last \ndecade due to their flexibility. Accurate path planning is one of the most crucial factors for \nsuccessful steerable needle insertion, where the tissue-needle interaction has to be considered. \nIn Lee, et al. [68], an agent is trained to perform pre-operative path planning for steerable \nneedles in keyhole neurosurgery with DQN. The environment is simulated by segmenting 2D \nMRI images into obstacles and obstacle-free areas. The agent controls the bevel direction \nrotation and insertion depth to insert the needle toward the target area. The agent is rewarded \nwhen achieving the goal and punished when entering an unsafe area, e.g., a blood vessel. In \nKumar, et al. [69] and Segato, et al. [72, 79], similar frameworks are proposed, however, with 3D \nMRI images to enable 3D path planning and continuous actions. \nPre-operative path planning provides initial planning, including the insertion point and a rough \nglobal plan. However, due to unexpected anatomical movements or needle-tissue interactions, \nthe pre-planned path can be violated and therefore, intra-operative replanning is needed. \nFurthermore, it is also essential for the surgeon to easily detect the risk potential (possibility of \nthe needle entering unsafe areas) of the re-planned path. In Tan, et al. [70], a universal \ndistributional Q-learning (UDQL) [71] based training framework is proposed to enable fast \nreplanning and risk management. In UDQL, the expected Q-value is parameterized with a \nvalue distribution, so that only a distribution with a high mean Q-value and low variance can \nbe considered a safe plan.\n \nRef. \nDescription \nAlgorithm \nObservation \nDOF \nof \nAction \nReward \nResult \n[50] \nTensioning policy for the selected \npinch point \nTRPO \nCutting \ntrajectory and \nfiducial points \nlocations \n2 \nSparse reward according to \nthe final cutting error, when \nepisode ends \nImprovement of 43.3% \ncompared to non-tension \nbaseline in term of cutting \nerror \n[51,  \n52] \nAn improved pipeline enabling \nselecting multiple pinch points \nfor different cutting segments \nTRPO \nCutting \ntrajectory and \nfiducial points \nlocations \n2 \nSparse reward according to \nthe final cutting error, when \nepisode ends \nImprovement of 50.6% \ncompared to non-tension \nbaseline in term of cutting \nerror \n[53] \nAutonomous collaborative \nneedle hand-off task of PSM \nQ-Learning \n3-D Position of \nrobotic tip \n3 \nSparse reward according to \nthe data points on user-\ndefined trajectory \nDissimilarity between learned \ntrajectory and reference \ntrajectory with mean and \nstandard deviation of \n[2.857mm, 1.488mm, \n0.774mm] and [3.388mm, \n2.286mm, 1.808mm] \n[55] \nAutonomous collaborative \nneedle hand-off task of PSM in \nego-centric spaces \nDDPG + \nBC \nRelative \nposition and \nquaternion \n6 \n+ reaching target pose \n- collision  \n97% and 73.3% success rate in \nsimulation and real-world \nenvironment, respectively \n[58] \nRobotic tissue retraction learning \nfrom expert demonstration \nPPO + \nGAIL \ngripper state, \nend-effector \nlocation, \n3 \n+ moving closer to tumor or \ntarget position \n+ moving further from tumor \nor target position \nAverage tumor exposure \npercentage of 84% and 90% in \nsimulation and real-world \nenvironment, respectively \ntarget location \n[60] \nRobotic tissue retraction \nconsidering safety properties \nPPO +  \nFormal \nVerification \nGripper state, \nend-effector \nlocation, \ntarget location \n3 \n+ moving closer to tumor or \ntarget position \n+ moving further from tumor \nor target position or violating \nsafety constraints \nSafety violation rate of 3.07% \nand violation rate reduction \nof 24%, compared to non-\nsafety method \n[62] \nRobotic tissue retraction with \nsim-to-real \nPPO +  \nDCL \nSequential \ntranslated \nscene image \n2 \n+ moving closer to tumor or \ntarget position \n+ moving further from tumor \nor target position \n50% success rate in real world \nenvironment with raw \ncamera images as input \n \nTable 2. The formulation, methodologies and results of the reviewed papers in the section of intra-body surgery.\nb. Catheterization \nCatheterization is one of the most commonly used procedures in endovascular intervention. \nThe catheter is guided to the target of the disease in the vasculature along with treatments such \nas stenting, embolization, and ablation, as illustrated in Fig. T8. However, the guidance of the \ncatheter is not trivial. The surgeon needs to manipulate the catheter with limited 2D \nfluoroscopic information and minimize unwanted excessive tissue contacts. Due to the \ndifficulty of manually operating the catheter, robot-assisted catheterization has been researched \nin the last decade and DRL is one of the promising methods for its path or trajectory planning. \nIn Chi, et al. [76], an agent is trained to optimize the catheterization trajectory demonstrated by \nthe experts. The expert demonstrations are first parameterized with dynamic motion primitives \n(DMP). The agent adjusts the parameters of DMP to optimize the trajectory, guiding the \ncatheter tip towards the desired vessel plane while matching the trajectory with the vessel \ncenterline as much as possible. The agent is trained with Path Integral (ùëÉùêº2) [77] algorithm, \nwhich is a robust RL implementation based on trajectory rollouts. The environment is based on \nvascular models with flow simulation. Furthermore, Chi, et al. [78] proposed a closed-loop \ncatheter control framework based on GAIL to imitate the expert catheterization demonstration. \nAn electromagnetic (EM) tracking sensor is attached to the catheter tip to take into account its \nreal-time pose to enable intra-operative control. The agent is trained to imitate the five-motion \nprimitive of the expert‚Äôs hand demonstration, namely pull, push, clockwise rotation, anti-\nclockwise rotation, and stand-by. Besides, in Omisore, et al. [80], DRL is utilized to tune the \nparameters of a PID controller in real time, to let it adapt to different blood flow settings. \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 9. The catheter is guided to the target position in vessel. The blue dot line is the planned \npath. \nRef. \nDescription \nAlgorithm \nObservation \nDOF \nof \nAction \nReward \nResult \n[73] \nNeedle path planning for \nOphthalmic microsurgery \nDDPGfD \nHeight maps of \ntwo corneal \nsurfaces  \n3 \nSparse reward when reaching \nthe target position \nPerforation-free percent depth of \n84.75% ¬± 4.91% \n[74] \nRCM recommendation for keyhole \nneuro surgery \nPPO \nTarget position, \nRobot joints \nposition \n2 \nSparse reward according to \npositioning accuracy, solvable \ninverse kinematics and \nmechanical joint motion \n93% success rate of finding \noptimal RCM \n[68] \nNavigation of steerable needle \ntowards the target position in brain \nin 2D space  \nDQN \n2D map based on \nsegmented MRI \nimages \n2 \n+ position improvement \n93.6% success rate of achieving \ntarget position \n[69] \nNavigation of steerable needle \ntowards the target position in brain \nin 3D space  \nDDPG \n3D map based on \nsegmented MRI \nimages \n2 \n+ achieve goal or in safe area \n- in unsafe area \nOutperforms RRT* under \ndifferent quantiles of constraints \n[72] \nNavigation of steerable needle \ntowards the target position in brain \nin 3D space  \nGAIL \n3D map based on \nsegmented MRI \nimages \n6 \n+ achieve goal  \n- obstacle collision \nAverage targeting error of 1.34 ¬± \n0.52 mm in position and 3.16 ¬± \n1.06 degrees in orientation  \n[79] \nNavigation of steerable needle \ntowards the target position in brain \nin 3D space  \nGA3C \nSequential frames \nof 3D map based \non segmented MRI \nimages \n6 \n+ achieve goal  \n- obstacle collision \nOutperforms RRT* under \ndifferent quantiles of constraints \n \nTable 3. The formulation, methodologies and results of the reviewed papers in the section of percutaneous surgery. \n[76] \nOptimization of catheterization \ntrajectory obtained from \ndemonstration \nùëÉùêº2 \nCatheter tip pose, \nTarget position \n2 \n+ position improvement,  \n vessel centerline alignment \nAverage targeting error of \n2.92mm and path length of \n258.67mm \n[78] \nNavigation of catheter tip to the \ntarget position \nGAIL+ PPO \nCatheter tip pose, \nTarget position \n3 \n+ position improvement \n- obstacle collision \n82.4% success rate of achieving \ntarget position \n[80] \nAdaptation of PID controller \nparameters for catheterization \nDQN \nCatheter tip pose \n3 \n+ position improvement \nAverage error of 0.003 ¬± 0.0058 \nmm with respect to setting point \n[Conclusion] \nThis literature review has provided an overview of the application of Deep Reinforcement \nLearning (DRL) in surgical robots. We divided the state-of-art works that applied DRL on \nsurgical robots into three main fields: skin-interfaced, intra-body, and percutaneous, discuss \nhow they formulate the problem based on RL-framework, and compare their methodologies \nand limitations. Based on the outstanding performance of DRL in these works, the integration \nof DRL algorithms into surgical robotic systems has the potential to revolutionize the field of \nrobotic-assisted surgery by enhancing the autonomy and decision-making capabilities of these \nsystems.  \nThe technology of DRL is in its youth and still suffers from some limitations, e.g., low data \nefficiency, expensive to train in the real world, and lack of safety guarantee. Looking forward, \nfurther research is needed to refine and optimize DRL algorithms for surgical applications. This \nincludes the following points: Firstly, more efficient training methodologies. Currently, most \nDRL algorithms are very sample inefficient compared to other deep learning methods. \nSecondly, a more accurate simulation environment. In surgical scenarios, there exist a lot of \ndeformable bodies interaction, which are much harder to simulate, compared to rigid bodies \ninteraction. Thirdly, addressing safety concerns. Safety is always the priority in surgeries. This \ncould include risk analysis or interpretability of the model. Lastly, conducting clinical trials to \nevaluate the effectiveness and reliability of DRL-based surgical robots because, so far, few DRL-\nbased robots have been tested in real clinical scenarios. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n[1] Peters, Brian S., et al. \"Review of emerging surgical robotic technology.\" Surgical endoscopy 32 (2018): \n1636-1655. \n[2] Gomes, Paula, ed. Medical robotics: Minimally invasive surgery. Elsevier, 2012. \n[3] Freschi, Cinzia, et al. \"Technical review of the da Vinci surgical telemanipulator.\" The International \nJournal of Medical Robotics and Computer Assisted Surgery 9.4 (2013): 396-406. \n[4] Moustris, George P., et al. \"Evolution of autonomous and semi‚Äêautonomous robotic surgical systems: a \nreview of the literature.\" The international journal of medical robotics and computer-assisted surgery 7.4 \n(2011): 375-392. \n[5] Ibarz, Julian, et al. \"(Dogangil, Davies et al. 2010, Moustris, Hiridis et al. 2011, Freschi, Ferrari et al. \n2013, Peters, Armijo et al. 2018).\" The International Journal of Robotics Research 40.4-5 (2021): 698-721. \n[6] Nguyen, Hai, and Hung La. \"Review of deep reinforcement learning for robot manipulation.\" 2019 \nThird IEEE International Conference on Robotic Computing (IRC). IEEE, 2019. \n[7] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint \narXiv:1312.5602 (2013). \n[8] Zhou, S. Kevin, et al. \"Deep reinforcement learning in medical imaging: A literature review.\" Medical \nimage analysis 73 (2021): 102193. \n[9] Yu, Chao, et al. \"Reinforcement learning in healthcare: A survey.\" ACM Computing Surveys \n(CSUR) 55.1 (2021): 1-36. \n[10] Coronato, Antonio, et al. \"Reinforcement learning for intelligent healthcare applications: A \nsurvey.\" Artificial Intelligence in Medicine 109 (2020): 101964. \n[11] Jonsson, Anders. \"Deep reinforcement learning in medicine.\" Kidney diseases 5.1 (2019): 18-22. \n[12] Liu, Siqi, et al. \"Reinforcement learning for clinical decision support in critical care: comprehensive \nreview.\" Journal of medical Internet research 22.7 (2020): e18477. \n[13] Haidegger, Tam√°s. \"Autonomy for surgical robots: Concepts and paradigms.\" IEEE Transactions on \nMedical Robotics and Bionics 1.2 (2019): 65-76. \n[14] Arulkumaran, Kai, et al. \"Deep reinforcement learning: A brief survey.\" IEEE Signal Processing \nMagazine 34.6 (2017): 26-38. \n[15] Hua, Jiang, et al. \"Learning for a robot: Deep reinforcement learning, imitation learning, transfer \nlearning.\" Sensors 21.4 (2021): 1278. \n[16] Zhang, Tianhao, et al. \"Deep imitation learning for complex manipulation tasks from virtual reality \nteleoperation.\" 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018. \n[17] Sigaud, Olivier, and Olivier Buffet, eds. Markov decision processes in artificial intelligence. John \nWiley & Sons, 2013. \n[18] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018. \n[19] Watkins, Christopher JCH, and Peter Dayan. \"Q-learning.\" Machine learning 8 (1992): 279-292. \n[20] Kakade, Sham M. \"A natural policy gradient.\" Advances in neural information processing systems 14 \n(2001). \n[21] Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint \narXiv:1707.06347 (2017). \n[22] Schulman, John, et al. \"Trust region policy optimization.\" International conference on machine \nlearning. PMLR, 2015. \n[23] Yu, Yang. \"Towards Sample Efficient Reinforcement Learning.\" IJCAI. 2018. \n[24] Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International \nconference on machine learning. PMLR, 2016. \n[25] Priester, Alan M., Shyam Natarajan, and Martin O. Culjat. \"Robotic ultrasound systems in \nmedicine.\" IEEE transactions on ultrasonics, ferroelectrics, and frequency control 60.3 (2013): 507-523. \n[26] Baumgartner, Christian F., et al. \"SonoNet: real-time detection and localisation of fetal standard scan \nplanes in freehand ultrasound.\" IEEE transactions on medical imaging 36.11 (2017): 2204-2215. \n[27] Virga, Salvatore, et al. \"Automatic force-compliant robotic ultrasound screening of abdominal aortic \naneurysms.\" 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2016. \n[28] Hase, Hannes, et al. \"Ultrasound-guided robotic navigation with deep reinforcement learning.\" 2020 \nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020. \n[29] Spaan, Matthijs TJ. \"Partially observable Markov decision processes.\" Reinforcement learning: State-\nof-the-art (2012): 387-414. \n[30] Baisero, Andrea, and Christopher Amato. \"Learning complementary representations of the past using \nauxiliary tasks in partially observable reinforcement learning.\" Proceedings of the 19th International \nConference on Autonomous Agents and MultiAgent Systems. 2020. \n[31] Li, Keyu, et al. \"Autonomous navigation of an ultrasound probe towards standard scan planes with \ndeep reinforcement learning.\" 2021 IEEE International Conference on Robotics and Automation (ICRA). \nIEEE, 2021. \n[32] Karamalis, Athanasios, et al. \"Ultrasound confidence maps using random walks.\" Medical image \nanalysis 16.6 (2012): 1101-1112. \n[33] Li, Keyu, et al. \"Image-guided navigation of a robotic ultrasound probe for autonomous spinal \nsonography using a shadow-aware dual-agent framework.\" IEEE Transactions on Medical Robotics and \nBionics 4.1 (2021): 130-144. \n[34] Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint arXiv:1511.05952 (2015). \n[35] Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep reinforcement learning with double q-\nlearning.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 30. No. 1. 2016. \n[36] Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" International \nconference on machine learning. PMLR, 2016. \n[37] Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image \nrecognition.\" arXiv preprint arXiv:1409.1556 (2014). \n[38] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" 2009 IEEE conference on \ncomputer vision and pattern recognition. Ieee, 2009. \n[39] Bi, Yuan, et al. \"VesNet-RL: Simulation-based reinforcement learning for real-world us probe \nnavigation.\" IEEE Robotics and Automation Letters 7.3 (2022): 6638-6645. \n[40] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for \nbiomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention‚Äì\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III \n18. Springer International Publishing, 2015. \n[41] Li, Keyu, et al. \"RL-TEE: Autonomous Probe Guidance for Transesophageal Echocardiography Based \non Attention-Augmented Deep Reinforcement Learning.\" IEEE Transactions on Automation Science and \nEngineering (2023). \n[42] Milletari, Fausto, Vighnesh Birodkar, and Michal Sofka. \"Straight to the point: Reinforcement \nlearning for user guidance in ultrasound.\" Smart Ultrasound Imaging and Perinatal, Preterm and \nPaediatric Image Analysis: First International Workshop, SUSI 2019, and 4th International Workshop, \nPIPPI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, \nProceedings 4. Springer International Publishing, 2019. \n[43] Ning, Guochen, Xinran Zhang, and Hongen Liao. \"Autonomic robotic ultrasound imaging system \nbased on reinforcement learning.\" IEEE Transactions on Biomedical Engineering 68.9 (2021): 2787-2797. \n[44] Ning, Guochen, Xinran Zhang, and Hongen Liao. \"Autonomic robotic ultrasound imaging system \nbased on reinforcement learning.\" IEEE Transactions on Biomedical Engineering 68.9 (2021): 2787-2797. \n[45] Badrinarayanan, Vijay, Alex Kendall, and Roberto Cipolla. \"Segnet: A deep convolutional encoder-\ndecoder architecture for image segmentation.\" IEEE transactions on pattern analysis and machine \nintelligence 39.12 (2017): 2481-2495. \n[46] Ning, Guochen, et al. \"Inverse-Reinforcement-Learning-Based Robotic Ultrasound Active \nCompliance Control in Uncertain Environments.\" IEEE Transactions on Industrial Electronics (2023). \n[47] Ng, Andrew Y., and Stuart Russell. \"Algorithms for inverse reinforcement learning.\" Icml. Vol. 1. \n2000. \n[48] Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint \narXiv:1509.02971 (2015). \n[49] Mahvash, Mohsen, et al. \"Modeling the forces of cutting with scissors.\" IEEE Transactions on \nBiomedical Engineering 55.3 (2008): 848-856. \n[50] Thananjeyan, Brijen, et al. \"Multilateral surgical pattern cutting in 2d orthotropic gauze with deep \nreinforcement learning policies for tensioning.\" 2017 IEEE international conference on robotics and \nautomation (ICRA). IEEE, 2017. \n[51] Nguyen, Thanh, et al. \"A new tensioning method using deep reinforcement learning for surgical \npattern cutting.\" 2019 IEEE international conference on industrial technology (ICIT). IEEE, 2019. \n[52] Nguyen, Ngoc Duy, et al. \"Manipulating soft tissues by deep reinforcement learning for autonomous \nrobotic surgery.\" 2019 IEEE International Systems Conference (SysCon). IEEE, 2019. \n[53] Varier, Vignesh Manoj, et al. \"Collaborative suturing: A reinforcement learning approach to automate \nhand-off task in suturing for surgical robots.\" 2020 29th IEEE international conference on robot and \nhuman interactive communication (RO-MAN). IEEE, 2020. \n[54] M√ºller, Meinard. \"Dynamic time warping.\" Information retrieval for music and motion (2007): 69-84. \n[55] Chiu, Zih-Yun, et al. \"Bimanual regrasping for suture needles using reinforcement learning for rapid \nmotion planning.\" 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021. \n[56] Shahkoo, Amin Abbasi, and Ahmad Ali Abin. \"Deep reinforcement learning in continuous action \nspace for autonomous robotic surgery.\" International Journal of Computer Assisted Radiology and \nSurgery 18.3 (2023): 423-431. \n[57] Patil, Sachin, and Ron Alterovitz. \"Toward automated tissue retraction in robot-assisted \nsurgery.\" 2010 IEEE International Conference on Robotics and Automation. IEEE, 2010. \n[58] Pore, Ameya, et al. \"Learning from demonstrations for autonomous soft-tissue retraction.\" 2021 \nInternational Symposium on Medical Robotics (ISMR). IEEE, 2021. \n[59] Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural \ninformation processing systems 29 (2016). \n[60] Pore, Ameya, et al. \"Safe reinforcement learning using formal verification for tissue retraction in \nautonomous robotic-assisted surgery.\" 2021 IEEE/RSJ International Conference on Intelligent Robots and \nSystems (IROS). IEEE, 2021. \n[61] Corsi, Davide, et al. \"Formal verification for safe deep reinforcement learning in trajectory \ngeneration.\" 2020 Fourth IEEE International Conference on Robotic Computing (IRC). IEEE, 2020. \n[62] Scheikl, Paul Maria, et al. \"Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable \nObject Manipulation for Robot-Assisted Surgery.\" IEEE Robotics and Automation Letters 8.2 (2022): 560-\n567. \n[63] Wang, Mei, and Weihong Deng. \"Deep visual domain adaptation: A survey.\" Neurocomputing 312 \n(2018): 135-153. \n[64] H. Mayer et al., \"Automation of Manual Tasks for Minimally Invasive Surgery,\" Fourth International \nConference on Autonomic and Autonomous Systems (ICAS'08), Gosier, France, 2008, pp. 260-265, doi: \n10.1109/ICAS.2008.16. \n[65] Reisch, Robert, et al. \"The keyhole concept in neurosurgery.\" World neurosurgery 79.2 (2013): S17-e9. \n[66] Berry, Scott, and Kristin Ondecko Ligda. Ophthalmic surgery. Springer New York, 2015. \n[67] Swaney, Philip J., et al. \"A flexure-based steerable needle: high curvature with reduced tissue \ndamage.\" IEEE Transactions on Biomedical Engineering 60.4 (2012): 906-909. \n[68] Lee, Yonggu, et al. \"Simulation of robot-assisted flexible needle insertion using deep Q-\nnetwork.\" 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC). IEEE, 2019. \n[69] Kumar, Jayesh, Chinmay Satish Raut, and Niravkumar Patel. \"Automated Flexible Needle Trajectory \nPlanning for Keyhole Neurosurgery Using Reinforcement Learning.\" 2022 IEEE/RSJ International \nConference on Intelligent Robots and Systems (IROS). IEEE, 2022. \n[70] Tan, Xiaoyu, et al. \"Robot-assisted flexible needle insertion using universal distributional deep \nreinforcement learning.\" International journal of computer assisted radiology and surgery 15 (2020): 341-\n349. \n[71] Tan, Xiaoyu, et al. \"Robust path planning for flexible needle insertion using Markov decision \nprocesses.\" International Journal of Computer Assisted Radiology and Surgery 13 (2018): 1439-1451. \n[72] Segato, Alice, et al. \"Inverse reinforcement learning intra-operative path planning for steerable \nneedle.\" IEEE Transactions on Biomedical Engineering 69.6 (2021): 1995-2005. \n[73] Keller, Brenton, et al. \"Optical coherence tomography-guided robotic ophthalmic microsurgery via \nreinforcement learning from demonstration.\" IEEE Transactions on Robotics 36.4 (2020): 1207-1218. \n[74] Gao, Huxin, et al. \"Remote-center-of-motion recommendation toward brain needle intervention \nusing deep reinforcement learning.\" 2021 IEEE International Conference on Robotics and Automation \n(ICRA). IEEE, 2021. \n[75] Aksungur, Serhat. \"Remote center of motion (RCM) mechanisms for surgical \noperations.\" International Journal of Applied Mathematics Electronics and Computers 3.2 (2015): 119-126. \n[76] Chi, Wenqiang, et al. \"Trajectory optimization of robot-assisted endovascular catheterization with \nreinforcement learning.\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems \n(IROS). IEEE, 2018. \n[77] Theodorou, Evangelos, Jonas Buchli, and Stefan Schaal. \"A generalized path integral control \napproach to reinforcement learning.\" The Journal of Machine Learning Research 11 (2010): 3137-3181. \n[78] Chi, Wenqiang, et al. \"Collaborative robot-assisted endovascular catheterization with generative \nadversarial imitation learning.\" 2020 IEEE International conference on robotics and automation (ICRA). \nIEEE, 2020. \n[79] Segato, Alice, et al. \"Ga3c reinforcement learning for surgical steerable catheter path planning.\" 2020 \nIEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020. \n[80] Omisore, Olatunji Mumini, et al. \"A novel sample-efficient deep reinforcement learning with episodic \npolicy transfer for pid-based control in cardiac catheterization robots.\" arXiv preprint \narXiv:2110.14941 (2021). \n[81] Page, Matthew J., et al. \"The PRISMA 2020 statement: an updated guideline for reporting systematic \nreviews.\" International journal of surgery 88 (2021): 105906. \n \n \n \n \n \n",
  "categories": [
    "cs.RO"
  ],
  "published": "2023-09-02",
  "updated": "2023-09-02"
}