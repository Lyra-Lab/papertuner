{
  "id": "http://arxiv.org/abs/2309.01909v1",
  "title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
  "authors": [
    "Chayan Banerjee",
    "Kien Nguyen",
    "Clinton Fookes",
    "Maziar Raissi"
  ],
  "abstract": "The inclusion of physical information in machine learning frameworks has\nrevolutionized many application areas. This involves enhancing the learning\nprocess by incorporating physical constraints and adhering to physical laws. In\nthis work we explore their utility for reinforcement learning applications. We\npresent a thorough review of the literature on incorporating physics\ninformation, as known as physics priors, in reinforcement learning approaches,\ncommonly referred to as physics-informed reinforcement learning (PIRL). We\nintroduce a novel taxonomy with the reinforcement learning pipeline as the\nbackbone to classify existing works, compare and contrast them, and derive\ncrucial insights. Existing works are analyzed with regard to the\nrepresentation/ form of the governing physics modeled for integration, their\nspecific contribution to the typical reinforcement learning architecture, and\ntheir connection to the underlying reinforcement learning pipeline stages. We\nalso identify core learning architectures and physics incorporation biases\n(i.e., observational, inductive and learning) of existing PIRL approaches and\nuse them to further categorize the works for better understanding and\nadaptation. By providing a comprehensive perspective on the implementation of\nthe physics-informed capability, the taxonomy presents a cohesive approach to\nPIRL. It identifies the areas where this approach has been applied, as well as\nthe gaps and opportunities that exist. Additionally, the taxonomy sheds light\non unresolved issues and challenges, which can guide future research. This\nnascent field holds great potential for enhancing reinforcement learning\nalgorithms by increasing their physical plausibility, precision, data\nefficiency, and applicability in real-world scenarios.",
  "text": "1\nA Survey on Physics Informed Reinforcement\nLearning: Review and Open Problems\nC. Banerjee, Member, IEEE, K. Nguyen, Member, IEEE, C. Fookes, Senior Member, IEEE,\nM. Raissi, Senior Member, IEEE,\nAbstract—The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This\ninvolves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore\ntheir utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information,\nas known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning\n(PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare\nand contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing\nphysics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the\nunderlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e.\nobservational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better\nunderstanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the\ntaxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and\nopportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research.\nThis nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility,\nprecision, data efficiency, and applicability in real-world scenarios.\nIndex Terms—Physics-informed, Reinforcement Learning, Machine learning, Neural Network, Deep Learning\n✦\n1\nINTRODUCTION\nThrough trial-and-error interactions with the environment,\nReinforcement Learning (RL) offers a promising approach to\nsolving decision-making and optimization problems. Over\nthe past few years, RL has accomplished impressive feats\nin handling difficult tasks, in such domains as autonomous\ndriving [119, 16], locomotion control [99, 129], robotics [71,\n94], continuous control [5, 6, 7], and multi-agent systems and\ncontrol [39, 15]. A majority of these successful approaches\nare purely data-driven and leverage trial-and-error to freely\nexplore the search space. RL methods work well in simu-\nlations, but they struggle with real-world data because of\nthe disconnection between simulated setups and the com-\nplexities of real world systems. Major RL challenges [33],\nthat are consistently addressed in latest research includes\nsample efficiency [91, 9], high dimensional continuous state\nand action spaces [34, 118], safe exploration [41, 48], multi-\nobjective and well-defined reward function [65, 10], perfect\nsimulators and learned model [27, 96] and policy transfer\nfrom offline pre-training [72, 131].\nWhen it comes to machine learning, incorporating math-\nematical physics into the models can lead to more mean-\ningful solutions. This approach, known as physics-informed\nmachine learning, helps neural networks learn from in-\ncomplete physics information and imperfect data more\nefficiently, resulting in faster training times and better\n•\nC. Banerjee, K. Nguyen, and C. Fookes are with Queensland University\nof Technology, Australia. Maziar Raissi is with University of Colorado\nBoulder, USA. E-mail: {c.banerjee, k.nguyenthanh, c.fookes}@qut.edu.au,\nmaziar.raissi@colorado.edu\ngeneralization. Additionally, it can assist in tackling high\ndimensionality applications and ensure that the resulting\nsolution is physically sound and follows the underlying\nphysical law [60, 8, 52]. Among the various sub-fields of\nML, RL is the natural candidate for incorporating physics\ninformation since most RL-based solutions deal with real-\nworld problems and have an explainable physical structure.\nRecent research has seen substantial improvement in\naddressing the RL challenges by incorporating physics in-\nformation in the training pipeline. For example, PIRL ap-\nproaches seek to use physics to reduce high-dimensional\ncontinuous states with intuitive representations and better\nsimulation. A low-dimensional representation adhering to\nphysical model PDEs is learned in [45], while [12] uses fea-\ntures from a supervised surrogate model. Learning a good\nworld model is a quicker and safer alternative to training RL\nagents in the real world. [103] incorporate physics into the\nnetwork for better world models, and [128] utilize a high-\nlevel specification robot morphology and physics for rapid\nmodel identification.\nA well-defined reward function is crucial for successful\nreinforcement learning, PIRL approaches also seek to incor-\nporate physical constraints into the design for safe learning\nand more efficient reward functions. For example, in [68] the\ndesigned reward incorporates IMU sensor data, imbibing in-\nertial constraints, while in [75] the physics informed reward\nis designed to satisfy explicit operational targets. To ensure\nsafe exploration during training and deployment, works\nsuch as [133, 141] learn a data-driven barrier certificate\nbased on physical property-based losses and a set of unsafe\nstate vectors.\narXiv:2309.01909v1  [cs.LG]  5 Sep 2023\n2\nThere are several lines of PIRL research dedicated to\nexploring more efficient exploration of the search space\nand effective policy deployment for real-world systems.\nSome approaches were developed to improve simulators\nfor sample efficiency and better sim to real transfer [1, 81].\nCarefully selecting task-specific state representations [59,\n51], reward functions [13, 14], and action spaces [124, 141]\nhas been shown to improve both the time to convergence\nand performance. To sum it up, integrating underlying\nphysics about the learning task structure has been found\nto improve performance and accelerate convergence.\nPhysics-informed Reinforcement Learning (PIRL) has\nbeen a growing trend in the literature, as demonstrated\nin the increasing number of papers published in this area\nover the past six years, as shown in Figure 1. The bar chart\nindicates that this field is gaining more attention, and we\ncan anticipate even more in the future.\n2023\n2022\n2021\n2020\n2019\n2018\n0\n10\n20\nNo. of PIRL paper published\nFigure 1: PIRL papers published over years. This statistic\ngraph the exponential growth of PIRL papers over last six\nyears.\nOur contributions in this paper are summarized as follows:\n1) Taxonomy: We propose a unified taxonomy to investi-\ngate what physics knowledge/processes are modeled,\nhow they are represented, and the strategies to incor-\nporate them into RL approaches.\n2) Algorithmic Review: We present state-of-the-art ap-\nproaches on the physics information guided/ physics\ninformed RL methods, using unified notations, sim-\nplified functional diagrams and discussion on latest\nliterature.\n3) Training and evaluation benchmark Review: We analyze the\nevaluation benchmarks used in the reviewed literature,\nthus presenting popular evaluation and benchmark\nplatforms/ suites for understanding the popular trend\nand also for easy reference.\n4) Analysis: We delve deep into a wide range of model\nbased and model free RL applications over diverse\ndomains. We analyze in detail how physics information\nis integrated into specific RL approaches, what physical\nprocesses have been modeled and incorporated, and\nwhat network architectures or network augmentations\nhave been utilized to incorporate physics.\n5) Open Problems: We summarize our perspectives on the\nchallenges, open research questions and directions for\nfuture research.\nTable 1: A list of abbreviations used in this article.\nAbbreviations\nFSA\nFinite State Automata\nFEA\nFinite Element Analysis\nCFD\nComputational Fluid Dynamics\nMDP\nMarkov Decision Process\nMBRL\nModel based Reinforcement Learning\nMFRL\nModel Free Reinforcement Learning\nCBF\nControl Barrier Function\nCBC\nControl Barrier Certificate\nNBC\nNeural Barrier Certificate\nCLBF\nControl Lyapunov Barrier Function\nNBC\nNeural Barrier Certificate\nDFT\nDensity Functional Theory\nAC\nActor Critic\nMPC\nModel Predictive Control\nDDP\nDifferential Dynamic Programming\nNPG\nNatural Policy Gradient\nTL\nTemporal Logic\nDMP\nDynamic Movement Primitive\nWBTG\nWhole Body Trajectory Generator\nDPG\nDeterministic Policy Gradient\nDPPO\nDistributed proximal Policy optimization\nABM\nAdjoint based method\nAPG\nAnalytic Policy Gradient\nWBIC\nWhole Body Impulse Controller\nLNN\nLagrangian Neural Network\nDifference to other survey papers:\nGeorge et al. [60] provided one of the most comprehen-\nsive reviews on machine learning (ML) in the context of\nphysics-informed (PI) methods, but approaches in the RL\ndomain has not been discussed. The work by Hao et al. [52]\nalso provided an overview of physics-informed machine\nlearning, where the authors briefly touch upon the topic of\nPIRL. Another recent study by Eesserer et al. [35] show-\ncased the use of prior knowledge to guide reinforcement\nlearning (RL) algorithms, specific to robotic applications.\nThe authors categorize knowledge into three types: expert\nknowledge, world knowledge, and scientific knowledge.\nOur paper offers a focused and comprehensive review\nspecially on the RL approaches that utilize the structure,\nproperties, or constraints unique to the underlying physics\nof a process/system. Our scope of application domains is\nnot limited to robotics, but also spanning to motion con-\ntrol, molecular structure optimization, safe exploration, and\nrobot manipulation.\nThe rest of this paper is organized as follows. In Sec-\ntion § 2, we provide a brief overview of the Physics informed\nML paradigm. In Section § 3, we present RL fundamentals/\nframework in § 3.1 and provide a definition with an intu-\nitive introduction to PIRL in\n§ 3.2. Most importantly we\nintroduce a comprehensive taxonomy in\n§ 3.3 threading\ntogether physics information types, PIRL methods that im-\nplement those information and RL pipeline as a backbone.\nLater in § 3.4 we present and elaborate on two additional\ncategories: Learning architecture and Bias, through which\nthe implementation side of the literature is explained more\nprecisely. In Section § 4 we present an elaborate review and\nanalysis of latest PIRL literature. In Section § 5, we discuss\nthe different open problems, challenges and research direc-\ntions that may be addresses in future works by interested\nresearchers. Finally Section § 6 concludes the paper.\n3\n2\nPHYSICS-INFORMED\nMACHINE\nLEARNING\n(PIML): AN OVERVIEW\nThe aim of PIML is to merge mathematical physics models\nand observational data seamlessly in the learning process.\nThis helps to guide the process towards finding a physically\nconsistent solution even in complex scenarios that are par-\ntially observed, uncertain, and high-dimensional [62, 52, 26].\nAdding physics knowledge to machine learning models has\nnumerous benefits, as discussed in [62, 89]. This information\ncaptures the vital physical principles of the process being\nmodeled and brings following advantages\n1) Ensures that the ML model is consistent both physically\nand scientifically.\n2) Increases data efficiency in model training, meaning\nthat the model can be trained with fewer data inputs.\n3) Accelerates the model training process, allowing mod-\nels to converge faster to an optimal solution.\n4) Increases the generalizability of trained models, en-\nabling them to make better predictions for scenarios\nthat were not seen during the training phase.\n5) Enhances the transparency and interpretability of mod-\nels, making them more trustworthy and explainable.\nAccording to literature, there are three strategies for inte-\ngrating physics knowledge or priors into machine learning\nmodels: observational bias, learning bias, and inductive\nbias.\nObservational bias: This approach uses multi-modal\ndata that reflects the physical principles governing their\ngeneration [82, 61, 77, 132]. The deep neural network (DNN)\nis trained directly on observed data, with the goal of cap-\nturing the underlying physical process. The training data\ncan come from various sources such as direct observations,\nsimulation or physical equation-generated data, maps, or\nextracted physics data induction.\nLearning bias: One way to reinforce prior knowledge\nof physics is through soft penalty constraints. This ap-\nproach involves adding extra terms to the loss function\nthat are based on the physics of the process, such as\nmomentum or conservation of mass. An example of this\nis physics-informed neural networks (PINN), which com-\nbine information from measurements and partial differ-\nential equations (PDEs) by embedding the PDEs into the\nneural network’s loss function using automatic differentia-\ntion [60]. Some prominent examples of soft penalty based\napproaches includes statistically constrained GAN [127],\nphysics-informed auto-encoders [37] and encoding invari-\nances by soft constraints in the loss function InvNet [110].\nInductive\nbiases:\nCustom\nneural\nnetwork-induced\n’hard’ constraints can incorporate prior knowledge into\nmodels. For instance, Hamiltonian NN [47] draws inspi-\nration from Hamiltonian mechanics and trains models to\nrespect exact conservation laws, resulting in better induc-\ntive biases. Lagrangian Neural Networks (LNNs) [25] in-\ntroduced by Cranmer et al. can parameterize arbitrary La-\ngrangians using neural networks, even when canonical mo-\nmenta are unknown or difficult to compute. Meng et al. [90]\nuses a Bayesian framework to learn functional priors from\ndata and physics with a PI-GAN, followed by estimating\nthe posterior PI-GAN’s latent space using the Hamiltonian\nMonte Carlo (HMC) method. Additionally, DeepONets [82]\nnetworks are used in PDE agnostic physical problems.\n3\nPHYSICS-INFORMED REINFORCEMENT LEARN-\nING: FUNDAMENTALS, TAXONOMY AND EXAMPLES\nIn this section, we will explain how physics information can\nbe integrated into reinforcement learning applications.\n3.1\nRL fundamentals\nFigure 2: Agent-environment framework, of RL paradigm.\nHere the reward generating function and the system/ plant\nis abstracted as the environment. And the control policy (e.g.\na DNN) and the learning algorithm, forms the RL agent.\nRL algorithms use reward signals from the environment\nto learn the best strategy to solve a task through trial\nand error. They effectively solve sequential decision-making\nproblems that follow the Markov Decision Process (MDP)\nframework. In the RL paradigm, there are two main players:\nthe agent and the environment. The environment refers to\nthe world where the agent resides and interacts. Through\nagent-environment interactions the agent perceives the state\nof the world and decides on the appropriate action to take.\nThe agent-environment RL framework, see Fig. 2, is a\nlarge abstraction of the problem of goal-directed learning\nfrom interaction [115]. The details of control apparatus, sen-\nsors, and memory are abstracted into three signals between\nthe agent and the environment: the control/ action, the state\nand the reward. Though typically, the agents computes the\nrewards, but by the current convention anything that cannot\nbe changed arbitrarily by the agent is considered outside of\nit and hence the reward function is shown as a part of the\nenvironment.\nMDP is typically represented by the tuple (S, A, R, P, γ),\nwhere S represents the states of the environment, A rep-\nresents set of actions that the RL agent can take. Reward\nfunction may be typically represented as R(st+1, at) a func-\ntion of next state and current action. The function generates\nthe reward due to action induced state transition from st to\nst+1. P(st+1|st, at) is the environment model that returns\nthe probability of transitioning to state st+1 from st. Finally\nthe discount factor γ ∈[0, 1], determines the amount of\nemphasis given to the immediate rewards relative to that\nof future rewards.\nThe RL framework typically organizes the agent’s inter-\nactions with the environment into episodes. In each episode,\nthe agent starts at a particular initial state s1 sampled from\nan initial distribution p(s1), which is part of the state space\nS of the MDP. At each timestep t, the agent observes the\ncurrent state st ∈S and samples an action at ∈A from\nits latest policy πϕ(at|st) based on the state st, where ϕ\nrepresents the policy parameters. The action space of the\nMDP is denoted by A.\n4\n(a) Online RL (model-free)\n(b) Off-policy RL (model-free)\n(c) Offline RL\n(d) Off-policy RL (model-based)\nFigure 3: Typical RL architectures, based on model use and\ninteraction with the environment.\nNext, the agent applies the action at into the envi-\nronment, which results in a new state st+1 given by the\ndynamics of the MDP, i.e., st+1 ∼p(st+1|st, at). The agent\nalso receives a reward rt = R(st+1, at), which can be\nconstrued as the desirability of a certain state transition\nfrom the context of the given task. The above process is\nrepeated up to a certain time horizon T, which may also be\ninfinite. The agent-environment interaction is recorded as a\ntrajectory, and the closed-loop trajectory distribution for the\nepisode t = 1, · · · , T can be represented by,\npϕ(τ) =pϕ(s1, a1, s2, a2, · · · , sT , aT , sT +1)\n(1)\n=p(s1)\nT\nY\nt=1\nπϕ(at|st)p(st+1|st, at),\n(2)\nwhere τ = (s1, a1, s2, a2, · · · , sT , aT , sT +1) represents the\nsequence of states and control actions. The objective is to\nfind an optimal policy represented by the parameter,\nϕ∗= arg maxϕ Eτ∼pϕ(τ)\nh\nT\nX\nt=1\nγtR(at, st+1)\ni\n|\n{z\n}\nJ (ϕ)\n,\n(3)\nwhich maximizes the objective function J (ϕ), γ is a parame-\nter called discount factor, where 0 ≤γ ≤1. γ determines the\npresent value of the future reward, i.e., a reward received at\nk timesteps in the future is worth only γk−1 times what it\nwould be worth if received immediately.\nModel-free and model-based RL: In RL, algorithms can\nbe classified based on whether the environment model is\navailable during policy optimization. The environment dy-\nnamics are represented as p(st+1, rt) = Pr(st+1, rt|st, at),\nwhich means that given a state and action, the environment\nmodel can predict the state transition and the corresponding\nreward. Access to an environment model allows the agent to\nplan and choose between options and also improves sample\nefficiency compared to model-free approaches. However,\nthe downside is that the environment’s groundtruth model\nis typically not available, and learning a perfect model of\nthe real world is challenging. Additionally, any bias in the\nlearned model can lead to good performance in the learned\nmodel but poor performance in the real environment.\nOnline, Off-policy and Offline RL: Online RL algorithms,\ne.g. PPO, TRPO, and A3C, optimize policies by using only\ndata collected while following the latest policy, creating an\napproximator for the state or action value functions, used to\nupdate the policy. Off-policy RL algorithms, e.g. SAC, TD3\nand IPNS, involve the agent updating its policy and other\nnetworks using data collected at any point during training.\nThis data is stored in a buffer called the experience replay\nbuffer and is in the form of tuples. Mini-batches are sampled\nfrom the buffer and used for the training process. Offline RL\nalgorithms use a fixed dataset called D collected by a policy\nπζ to learn the optimal policy. This allows for the use of\nlarge datasets collected previously.\nCombining model-free/model-based with online/off-\npolicy/offline categorization, typical RL architectures can be\npresented as Fig. 3.\n3.2\nPIRL: Introduction\n3.2.1\nDefinition\nThe concept of physics-informed RL involves incorporating\nphysics structures, priors, and real-world physical variables\ninto the policy learning or optimization process. Physics\ninduction helps improve the effectiveness, sample efficiency\nand accelerated training of RL algorithms/ approaches,\nfor complex problem-solving and real-world deployment.\nDepending on the specific problem or scenario, different\nphysics priors can be integrated using various RL methods\nat different stages of the RL framework, see Fig. 4.\n3.2.2\nIntuitive introduction to physics priors in RL\nPhysics priors come in different forms, like intuitive phys-\nical rules or constrains, underlying mathematical/ guiding\nequations and physics simulators, to name a few. Here we\ndiscuss a couple of intuitive examples. In [128], the physical\n5\nPolicy:\nValue function:\nRL Agent\nSystem dynamics:\nENVIRONMENT\nState\nReward\nAction\nReward function:\nState Design\nAction Regulation\nReward Design\nAugment Policy or Value N/W\nAugment Simulator/ Model\nSEC. 4.1.1\nSEC. 4.1.2\nSEC. 4.1.3\nSEC. 4.1.4\nSEC. 4.1.5\nFigure 4: Map of physics incorporation (PI) in the conventional Reinforcement Learning (RL) framework.\ncharacteristics of the system were utilized as priors. The\nhigh-level specifications of a robot’s morphology such as\nthe number and connectivity structure of links were used\nas physics priors. This feature based representation of the\nsystem dynamics enabled rapid model identification in this\nmodel based RL setup. In another example, pertaining to\nadaptive cruise control problem, [59] (see Fig.5), physics\ninformation in the form of “jam-avoiding distance” (based\non desired physical parameters e.g. velocity and acceler-\nation constraints, minimum jam avoiding distance etc.) is\nincluded in state space input to the RL agent. Physics info.\nincorporation results in a RL controller which performs with\nless collisions and enables more equidistant travel.\nFigure 5: An illustrative example of physics incorporation\nin RL application, [59]. Here the RL agent is fed with an\nadditional state variable: jam avoiding distance, which is\nbased on desired physical parameters and primary state\nvariables.\n3.3\nPIRL Taxonomy\n3.3.1\nPhysics information (types): representation of physics\npriors\nThere are different types/ forms of physics information,\ne.g. mathematical representation of the physical system like\nPDE/ODE and physics enriched simulators. Based on the\ntype of the physics information representation, works can\nbe typically categorized as follows.\n1) Differential and algebraic equations (DAE): Many works\nuse system dynamics representations, such as par-\ntial/ordinary differential equations (PDE/ ODE) and\nboundary conditions (BC), as physics priors primarily\nthrough PINN and other special networks. For example\nin transient voltage control application [40], a PINN\nis trained using PDE of transient process. The PINN\nlearns a physical constraint which it transfers to the loss\nterm of the RL algorithm.\n2) Barrier certificate and physical constraints (BPC): It is im-\nperative to regulate agent exploration in safety-critical\napplications of reinforcement learning. One way it is\naddressed in recent research is through the use of\noptimization-based control theoretic constraints. Use\nof concepts like control Lyapunov function (CLF)[74,\n23], barrier certificate/ barrier function (BF), control\nbarrier function/ certificate (CBF/ CBC) [19, 11] is\nmade in recent safety critical RL applications. Barrier\ncertificate is generally used to establish a safe set of\ndesired states for a system. A control barrier function\nis then employed to devise a control law that keeps the\nstates within the safety set. In certain scenarios barrier\nfunctions are represented as NNs and learned through\ndata driven approaches [141, 140]. In above control\ntheoretic approaches the system dynamics either par-\ntial or learnable and safety sets represent the primary\nphysical information. For more details on CBFs refer [2].\nAdditionally safety in the learning process may also be\nensured by incorporating physical constraints into the\nRL loss [76, 17].\n3) Physics parameters, primitives and physical variables (PPV):\nPhysics values extracted/ derived from the environ-\n6\nment or system has been directly used by RL agents in\nform of physics parameters [113], dynamic movement\n(physics) primitives [3], physical state [59] and physical\ntarget [75]. For example in [75], the reward is created to\nmeet two physical objectives/ targets: operation cost\nand self-energy sustainability. In an adaptive cruise\ncontrol problem [59], authors use desired physical pa-\nrameters e.g. velocity and acceleration constraints and\nminimum jam avoiding distance, as a state space input.\n4) Offline data and representation (ODR): For the improve-\nment simulator based training, especially during sim-\nto-real transfer, non-task- specific-policy data collected\nfrom real robot has been used to train RL agents in of-\nfline setting along with simulators [46] and as hardware\ndata to seed simulators [81].\nAnother popular way of extracting physics information\nfrom environment is learning physically relevant low\ndimensional representation from observations [45, 12].\nFor example, in [45], PINN is used to extract physi-\ncally relevant information about the hidden state of the\nsystem, which is further used to learn a Q-function for\npolicy optimization.\n5) Physics simulator and model (PS): Simulators provide a\neasy way of experimenting with RL algorithms without\nexposing the agent e.g. a robot to the wear and tear of\nthe real environment.\nApart from serving as test-beds for RL algorithms, sim-\nulators are also used alongside RL algorithms to impart\nphysical correctness or physics awareness in the data\nor training process. For example in order to improve\nmotion capture and imitation of given motion clips, [20]\nhave used rigid body physics simulations to solve the\nrigid body poses closely following the motion capture\nclip. In [42], using a physics simulator, a residual agent\nis able to learn how to improve user input in order to\nachieve a task while staying true to the original input\nand expert-recorded trajectories.\nIn the MBRL setting the system model can be: 1)\ncompletely known, 2) partially known or 3) completely\nunknown. RL algorithms typically addresses the last\ntwo types, since it deals with environments whose\ndynamics is complex and difficult to ascertain through\nclassical approaches. In such cases a DNN based data-\ndriven approach is generally utilized to learn the sys-\ntem model completely or enrich the existing partial or\nbasic model of the environment. In [51] a data driven\nsurrogate traffic flow model is learned that generates\nsynthetic data. This data is later used by the agent in an\noffline learning process, followed by an online control\nprocess. In [103] learns environment and reward mod-\nels by using Lagrangian NNs [25]. LNNs are models\nthat are able to Lagrangian functions straight from data\ngathered from agent-environment interactions.\n6) Physical properties (PPR): Fundamental knowledge re-\ngarding the physical structure or properties pertaining\nto a system has been used in a number of works. For\nexample system morphology, system symmetry [54]\n3.3.2\nPIRL methods: physics prior augmentations to RL\nPIRL methods highlights and discusses about the different\ncomponents of the typical RL paradigm e.g. state space,\naction space, reward function and agent networks (policy\nand value function N/W), that has been directly modified/\naugmented through the incorporation of physics informa-\ntion.\n1) State design: This category is concerned with the ob-\nserved state space of the environment or model. The\nPIRL approaches, typically modifies or expands the\nstate representation in order to make it more instruc-\ntive. Works include state fusion using additional infor-\nmation from environment [59] and other agents [112],\nstate as extracted features from robust representation\n[12], learned surrogate model generated data as state\n[51] and state constraints [138].\n2) Action regulation: This pertains to modifying the ac-\ntion value, which is often achieved through PIRL ap-\nproaches that impose constraints on the action value to\nensure safety protocols are implemented [76, 19].\n3) Reward design: It concerns approaches that induce\nphysics information through effective reward design\nor augmentation of existing reward functions with\nbonuses or penalties [28, 83].\n4) Augment policy or value N/W: These PIRL approaches in-\ncorporate physics principles via methods like, adjusting\nthe update rules and losses of the policy [4, 87], value\nfunctions [93, 98] and making direct changes to their\nunderlying network structure [14]. Works with novel\nphysics based losses [92, 130] and constraints for policy\nor value function learning [40] are also included.\n5) Augment simulator or model:\nThis category encom-\npasses those works that develops improved simulators\nthrough incorporation of underlying physics knowl-\nedge thereby allowing for more accurate simulation\nof real-world environments. Works include physics\nbased augmentation of DNN based learnable models\nfor accurate system model learning [70, 103], improved\nsimulators for sim-to-real transfer [46, 81] and physics\ninformed learning for partially known environment\nmodel [78].\n3.3.3\nRL Pipeline\nA typical RL pipeline can be represented into four functional\nstages namely, the problem representation, learning strategy,\nnetwork design, training and trained policy deployment.\nThese stages are elaborated as follows:\n1. Problem Representation: In this stage, a real-world prob-\nlem is modeled as a Markov Decision Process (MDP)\nand thereby described using formal RL terms. The main\nchallenge is to choose the right observation vector,\ndefine the reward function, and specify the action space\nfor the RL agent so that it can perform the specified task\nproperly.\n2. Learning strategy: In this stage, the decisions are made\nregarding the type of agent-environment interaction\ne.g. in terms of environment model use, learning ar-\nchitecture and the choice of RL algorithm.\n3. Network design: Here the finer details of the learning\nframework are decided and customized where needed.\nDecisions are made regarding the type of constituent\nunits (e.g. layer types, network depth etc.) of underly-\ning Policy and value function networks.\n7\nFigure 6: Deep Reinforcement Learning Pipeline. Here the problem is first modeled as a MDP, clearly defining the state,\naction and reward spaces. Followed by selecting the RL algorithm as Learning strategy and then selecting/ designing the\npolicy and/or value networks in network design stage. Finally the agent is trained using default/ custom loss function in\ntraining stage and finally deployed.\n4. Training: The policy and allied networks are trained\nin this stage. It also represents training augmentation\napproaches like Sim-to-real, that helps is reducing dis-\ncrepancy between simulated and real worlds.\n5. Trained policy deployment: At this stage the policy is\ncompletely trained and is deployed for solving the\nconcerned task.\nFigure 7: PIRL taxonomy and further categories. Physics\ninformation (types), the RL methods that incorporate them\nand the underlying RL pipeline constitutes the PIRL-\ntaxonomy, see Fig. 9. bias (sec. 3.4.1) and Learning architec-\nture (sec. 3.4.2) are two additional categories which has been\nintroduced to better explain the implementation of PIRL.\n3.4\nFurther categorization\nIn this section we introduce a couple of additional catego-\nrizations: Bias and Learning architecture. These categories\nare not part of the taxonomy that we have discussed in\nthe previous section, see Fig.7. They provide an additional\nperspective to the PIRL approaches presented here.\n3.4.1\nBias\nPI approaches in ML paradigm, mentions of different kind\nof biases or categories of methods of physics incorporation\nin ML models. In order to relate to that existing taxonomy\nused in PIML methods, in Table 2 and Table 3., we include\ncorresponding bias categories to each of the PIRL entries.\n3.4.2\nLearning architecture\nWe also categorize PIRL algorithms based on the alterations\nthat they introduce to the conventional RL learning archi-\ntecture to incorporate physics information/ priors. As listed\nand discussed below they help us understand the PIRL\nmethods from an architectural point of view. In the literature\nreview section we use the aid of such learning architecture\ncategories to group and discuss the PIRL methods.\n1) Safety filter:\nThis category includes approaches that\nhas a PI based module which regulates the agent’s\nexploration ensuring safety constraints, for reference\nsee Fig. 8(a). In this typical architecture the safety-filter\nmodule takes action at from RL agent πφ, and state\ninformation (st) and refines the action, giving ˜at.\n2) PI reward: This category includes approaches where\nphysics information is used to modify the reward\nfunction, see Fig.8(b) for reference. Here the PI-reward\nmodule augments agent’s extrinsic reward (rt) with a\nphysics information based intrinsic component, giving\n˜rt.\n3) Residual learning: Residual RL is an architecture which\ntypically consists of two controllers: a human designed\ncontroller and a learned policy [58]. In PIRL setting the\narchitecture consists of a physics informed controller\nπψ along with the data-driven DNN based policy πφ,\ncalled residual RL agent, see Fig. 8(c).\n4) Physics embedded network: In this category physics infor-\nmation e.g. system dynamics is directly incorporated in\nthe policy or value function networks, see Fig.8(d) for\nreference.\n5) Differentiable\nsimulator:\nHere\nthe\napproaches\nhave\nuse differentiable physics simulators, which are non-\nconventional/ or adapted simulators and explicitly pro-\nvides loss gradients of simulation outcome w.r.t. control\naction, see Fig.8(e) for reference.\n6) Sim-to-Real: In Sim-to-real architecture, the agent is first\ntrained on a simulator or source domain and is later\ntransferred to a target domain for deployment. In cer-\ntain cases the transfer is followed by fine-tuning at the\ntarget domain, see Fig.8(f) for reference.\n7) Physics variable: This architecture encompasses all those\napproaches where physical parameters, variables or\nprimitives are introduced to augment components (e.g.\nstates and reward) of the RL framework. For reference\nsee Fig.8(g).\n8) Hierarchical RL: This category includes hierarchical and\ncurriculum learning based approaches, Fig.8(h) for ref-\n8\nFigure 8: Typical RL architectures with physics information incorporation (a) Safety filter (b) PI-reward (c) Residual agent\n(d) Physics embedded network (e) Differentiable simulator (f) Sim-to-Real (g) Physics variable (h) Hierarchical RL (i) Data\naugmentation (j) PI model identification. To keep illustrations simple, we have not included ancillary networks e.g. value\nfunction networks above.\nerence. In a hierarchical RL (HRL) setting a long hori-\nzon decision making task is broken into simpler sub-\ntasks autonomously. In curriculum learning a com-\nplex task is solved by learning to solve a series of\nincreasingly difficult tasks. In both HRL and CRL\nphysics is typically incorporated into all the policy\n(including meta and sub-policies) and value networks.\nApproaches here are mostly extensions of physics-\nembedded networks (Fig.8(d)), as used in non-HRL/\nCRL settings.\n9) Data augmentation: This category includes approaches\nwhere the input state is replaced with a different or\naugmented form of it, e.g. low dimensional represen-\ntation so as to derive special and physically relevant\nfeatures out of it. See Fig.8(i) for reference. In this typi-\ncal architecture, the state vector st+1 is transformed into\nan augmented representation zt+1. Physically relevant\nfeatures are then extracted from it and used by the RL\nagent (πφ).\n10) PI model identification: This architecture represents those\nPIRL approaches, especially in data-driven MBRL set-\nting where physics information is directly incorporated\ninto the model identification process. For reference see\nFig.8(j).\n4\nPIRL: REVIEW AND ANALYSIS\nIn this section we provide a indepth review of latest works\nin PIRL, followed by a review of the popular datasets.\nWe also include an analysis of the algorithms and their\nderivatives, and discuss crucial insights.\n4.1\nAlgorithmic review\nWe provide a detailed overview of the PIRL approaches as\nidentified by our literature review in Table 2 and Table 3. We\nhave structured our discussion according to the methods of\nthe introduced taxonomy (see § 3.3) since they form a bridge\nbetween the physics information sources and practical ap-\nplications. We also use learning architecture categories as\nintroduced in 3.4.2, to better explain the PIRL methods.\n4.1.1\nState design:\nVehicular traffic control applications have used physics pri-\nors to design the state representations. While controlling\nconnected automated vehicles (CAVs), [112] proposed the\nuse of surrounding information from downstream vehicles\nand roadside geometry, by embedding them in the state\nrepresentation, see Fig. 10. The physics-informed state fu-\nsion approach integrates received information as DRL state\n(input features) i.e. for the ith CAV, DRL state is given\nas st\ni =\n\u0002et\ni, ϕt\ni, δq−t\ni , δd−t\ni , kt\ni\n\u0003\n, which are deviation values,\n(from left): lateral, angular, weighed equilibrium spacing\nand speed, and road curvature information.\nJurj et al. [59] makes use of physical information like jam-\navoiding distance to train RL agent, in order to improve col-\nlision avoidance of vehicles with adaptive cruise control. In\nramp metering control, [51] utilizes an offline-online policy\ntraining process, where the offline training data consists of\nhistorical data and synthetic data generated from a physical\ntraffic flow model.\n9\nFigure 9: Taxonomy, the diagram connects PI types with PIRL methods and then to the RL pipeline backbone. The\nconnection thickness represents the quantity of work done which corresponds to those components/ categories.\nFigure 10: Example of state design, through physics incor-\nporation. Distributed control framework for connected au-\ntomated vehicles [112]. Here information from downstream\nvehicles and roadway geometry information are incorpo-\nrated as physics prior knowledge through state fusion.\nIn [12] a physics informed graphical representation-\nenabled, global graph attention (GGAT) network is trained\nto model power flow calculation process. Informative fea-\ntures are then extracted from the GGAT layer (as repre-\nsentation N/W) and transferred used in the policy training\nprocess. While [45], uses PINNs based on thermal dynamics\nof buildings for learning better heating control strategies.\nDealing with aircraft conflict resolution problem, [139] com-\nposed intruder’s information e.g. speed and heading angle\ninto an image state representation. This image now consti-\ntutes of the physics prior and serves as the input feature\nfor RL based learning. In [138], the authors proposed a safe\nreinforcement learning algorithm using barrier functions for\ndistributed MPC nonlinear multi-robot systems, with state\nconstraints. [95], incorporates trained model alongside con-\ntrol barrier certificates, which restrict policies and prohibits\nexploration of the RL agent into certain undesirable sections\nof the state space. In case of a safety breach due to non-\nstationarity, the Lyapunov stability conditions ensures the\nre-establishment of safety.\n4.1.2\nAction regulation:\nFigure 11: Example of action regulation, using physics\npriors. In [141], a barrier certification system receives RL\ncontrol policy generated control actions and refines them\nsequentially using a barrier certificate to satisfy operational\nconstraints.\nMany safety critical applications have used physics\nbased constraints and other information in action regula-\ntion. These kind of approaches can be categorized under\nshielded RL/ safety filter, where a type of safety shield or\nbarrier function is employed to check the actions.\nFor safe power system control [141] proposes a frame-\nwork for learning a stabilizing controller that satisfies pre-\ndefined safety regions, see Fig. 11. Combining a model-free\ncontroller and a barrier-certification system, using a NN\nbased barrier function, i.e. neural barrier certificate (NBC).\nGiven a training set they learn a NBC Bϵ(x) and filtered\n10\n(regulated) control action Fψ\nu , jointly holding the following\ncondition\n(∀x ∈S0, Bϵ(x) ≤0) ∧(∀x ∈Su, Bϵ(x) > 0)\n∧(∀x ∈x|Bϵ(x) = 0, Lf(x,uRL)Bϵ(x) < 0)\nwhere Lf(x,uRL)Bϵ(x) is the Lie derivative of Bϵ(x), and\nϕ, ϵ are NN parameters. S0, Su are set of initial states and\nunsafe states respectively.\n[19] introduces a hybrid approach of MFRL and MBRL\nusing CBF, with provision of online learning of unknown\nsystem dynamics. It assumes availability of a set of safe\nstates. In a MARL setting, [11] introduced cooperative\nand non-cooperative CBFs in a collision-avoid problem,\nwhich includes both cooperative agents and obstacles. Also\nin MARL setting, [17] proposed efficient active voltage\ncontroller of photovoltaics (PVs) enabled with shielding\nmechanism. Which ensures safe actions of battery energy\nstorage systems (BESSs) during training. [136] deals with\ncontrolling a district cooling system (DCS), with complex\nthermal dynamic model and uncertainties from regulation\nsignals and cooling demands. The proposed safe controller\na hybrid of barrier function and DRL and helps avoid\nunsafe explorations and improves training efficiency. [76]\nproposed a safe RL framework for adaptive cruise control,\nbased on a safety-supervision module. The authors used the\nunderlying system dynamics and exclusion-zone require-\nment to construct a safety set, for constraining the learning\nexploration.\nIn a highway motion planning setting for autonomous\nvehicles[124] proposed a CBF-DRL hybrid approach. Cer-\ntain works like [13] and [14] have introduced multiple\nphysics based artifacts to ensure safe learning in au-\ntonomous agents. Both of them used residual control based\narchitecture merging physical model and data driven con-\ntrol. Additionally it also leverages physics model guided re-\nward. [14] extends the work by [13] and introduces physics\nmodel guided policy and value network editing in addition\nto the physics based reward. In [32], the authors integrate\nlearning a task space policy with a model based inverse\ndynamics controller, which translates task space actions\ninto joint-level controls. This enables the RL policy to learn\nactions in task space.\n4.1.3\nReward design:\nIn sim-to-real setting [113] proposed a reward specification\nframework based on composing probabilistic periodic costs\non basic forces and velocities, see Fig. 12. The framework\ndefines a parametric reward function for common robotic\n(bipedal) gaits. Dealing with periodic robot behavior, the\nabsolute time reward function is here defined in terms of a\ncycle time variable ϕ (which cycles over time period of [0, 1],\nas R(s, ϕ). The updated reward function as given below, is\ndefined as a biased sum of n reward components Ri(s, ϕ),\neach capturing a desired robot gait characteristic.\nR(s, ϕ) = β + ΣRi(s, ϕ), where\nRi(s, ϕ) = ci × Ii(ϕ) × qi(s)\neach Ri(s, ϕ) is a product of phase-coefficient ci, phase\nindicator Ii(ϕ) and phase reward measurement qi(s).\nIn [18], the authors introduced a RL-PIDL hybrid frame-\nwork, to learn MFGs, which generalize well and manage\nFigure 12: Example of physics incorporation in reward\ndesign. In [113] a reward function design framework was\nintroduced, that describe robot gaits as a periodic phase\nsequence such that each of which rewards or penalizes a\nparticular physical system measurement.\ncan complex multi-agent systems applications. The physics\nbased reward component (= evolution of population den-\nsity/ mean-field state) is approximated using PINN. To bet-\nter mimic natural human locomotion [68], designed reward\nfunction based on physical and experimental information:\ntrajectory optimization rewards, and bio-inspired rewards.\nIn a similar task of imitation of human motion but from\nmotion clip, [20] proposes a physics-based controller using\nDRL. A rigid body physics simulator is used to solve rigid\nbody poses that closely follows the motion capture (mocap)\nclip frames. In a similar work [100], a data driven RL\nframework was introduced for training control policies for\nsimulated characters. Refernce motions are used to define\nimitation reward and the task goal defines task specific\nreward.\n[75] leverages a federated MADRL approach for energy\nmanagement in multi-microgrid settings. The reward is de-\nsigned to satisfy two physical targets: operation cost and self\nenergy sufficiency. [135] proposed a DRL based method for\nreconstruction of flow fields from noisy data. Physical con-\nstraints like momentum equation, pressure Poisson equa-\ntion and boundary conditions are used for designing the\nreward function. [134] proposed physics based reward shap-\ning for wireless navigation applications. They used a cost\nfunction augmented with physically motivated costs like\ncosts for link-state monotonicity, for angle of arrival direc-\ntion following, and for SNR increasing. In single molecule\n3D structure optimization problem, [22] used physics based\nDFT calculation is used as reward function, for physically\ncorrect structural prediction. In [74], the authors used tem-\nporal logic through a finite state automata (FSA), control\nLyapunov and barrier function for ensuring effective and\nsafe RL in complex environments.The FSA simultaneously\nprovides rewards, objectives and safety constraints to the\nframework components.\nAddressing the problem of dexterous manipulation of\nobjects in virtual environments, [42] trained the agent in a\nresidual setting using hybrid model-free RL-IL approach.\nUsing a physics simulator and a pose estimation reward the\nagent learns to refine the user input to achieve a task while\n11\nkeeping the motion close to the input and the expert demon-\nstrations. [83] tackles physically valid 3D pose estimation\nfrom egocentric video. The authors utilized a combination of\nkinematics and dynamics approach, whereby the residual of\nthe action against a learned kinematics model is outputted\nby the dynamics-based model. In [56],the authors proposed\ninclusion of physics based intrinsic reward for improved\npolicy optimization of RL algorithms.\nIn the context of predicting interfacial area in two-phase\nflow, [28] proposed. The two-phase flow physics informa-\ntion is infused into the underlying MDP framework, which\nis then uses RL strategies to describe behavior of flow\ndynamics. The work introduces multiple rewards based on\nphysical interfacial area transport models, other physical\nparameters and data. In a work concerning optimization of\nnuclear fuel assembly [101], the authors introduce a reward\nshaping approach in RL optimization, which is based on\nphysical tactics used by fuel designers. These tactics include\nmoving fuel rods in assembly to meet certain constraints\nand objectives.\nA number of works have used physics through multiple\nPIRL methods. Apart from reward design they have infused\nphysics information through state design [112] and action\nregulation [14, 13, 124]. They have been discussed in previ-\nous sections and hence not repeated.\n4.1.4\nAugment simulator or model:\nFigure 13: Example, augmentation of learnable model using\nphysics information. The figure shows system dynamics\nlearning network structured using a LNN [103] and next\nstate calculations using Ralston’s method. Here the PINN\n(LNN) based dynamics model and reward model , are\nlearned via data-driven method.\nIn MBRL setting, using structure of underlying physics,\nand building upon Lagrangian neural network (LLN) [25],\nRamesh et al.[103] learned the system model via data-\ndriven approach, see Fig. 13. Concerning systems obeying\nLagrangian mechanics, the state consists of generalized co-\nordinates q and velocities ˙q. Lagrangian, which is a scalar is\ndefined as\nL(q, ˙q, t) = T (q, ˙q) −V(q)\nwhere T (q, ˙q) is kinetic energy and V(q) is the potential\nenergy. And so the Lagrangian equation of motion can be\nwritten as\nτ = M(q)¨q + C(q, ˙q) ˙q + G(q), where\n¨q = M −1(q)(τ −C(q, ˙q) ˙q −G(q))\nwhere C(q, ˙q) ˙q is Coriolis term, G(q) is gravitational term\nand τ is motor torque. In the NN implementation, separate\nnetworks are used for learning V(q) and L(q), leveraging\nwhich the acceleration (¨q) quantity is generated. The output\nstate derivative ( ˙q, ¨q) is then integrated using 2nd-order\nRunge-Kutta to compute next state.\nConcerning a sim-to-real setting, in [46] authors train\na recurrent neural network on the differences between\nrobotic trajectories in simulated and actual environments.\nThis model is further used to improve the simulator. For\nimproved transfer to real environment, [81] collected hard-\nware data (positions and calculated system velocities) to\nseed the simulator, for training control policies. [1] proposes\na framework for autonomous manufacturing of acoustic\nmeta-material, while leveraging physics informed RL and\ntransfer learning. A physics guided simulation engine is\nused to train the agent in source task and then fine-tuned\nin a data-driven fashion in the target task.\n[88] introduced a PINN based gravity model for training\nof dynamically informed RL agents. [106] uses surrogate\nmodels that capture primary physics of the system, as a\nstarting point of training DRL agent. In a curriculum learn-\ning setting, they train an agent to first track limit cycles in a\nvelocity space for a representative non-holonomic system\nand then further trained on a small simulation dataset.\n[128] combines linear dynamic models of physical systems\nwith optimism driven exploration. Here the features for\nthe linear models obtained from robot morphology and the\nexploration is done using MPC.\nA number of works introduced novel models are better\nrepresentations of real world physics and serves as better\nsimulators and ensures effective sim to real transfers. [108]\nintroduced learnable physics models which supports accu-\nrate predictions and efficient generalization across distinct\nphysical systems. Concerning dynamic control with par-\ntially known underlying physics (governing laws), [78] pro-\nposed a physics informed learning architecture, for environ-\nment model. ODEs and PDEs serves as the primary source\nof physics for these models. [121] uses entity abstraction to\nintegrate graphical models, symbolic computation and NNs\nin a MBRL agent. The framework presents object-centric\nperception, prediction and planning which helps agents to\ngeneralize to physical tasks not encountered before. [70] pro-\nposes a context aware dynamics model which is adaptable\nto change in dynamics. They break the problem of learning\nthe environment dynamics model into two stages: learning\ncontext latent vector and predicting next state conditioned\non it.\nIn micro-grid power control problem, [111] combines\nmodel-based analytical proof and reinforcement learning.\nHere model-based derivations are used to narrow the learn-\ning space of the RL agen, reducing training complexity\nsignificantly. In visual model based RL, [121] models a scene\nin terms of entities and their local interactions, thus better\ngeneralizing to physical task the learner has not seen before.\n12\nSimilar to learning entity abstractions, in [70] the authors\ntackles the challenge of learning a generalizable global\nmodel through: learning context latent vector, capturing\nlocal dynamics and predicting next state conditioned on\nthe encoded vector. Addressing dynamic control problem\nin MBRL setting, [78] leveraged physical laws (in form\nof canonical ODE/ PDE) and environmental constraints\nto mitigate model bias issue and sample inefficiency. In\nautonomous driving safe ramp merging problem, [120] em-\nbedded probabilistic CBF in RL policy in order to learn safe\npolicies, that also optimize the performance of the vehicle.\nTypically CBFs need good approximation of car’s model.\nHere the probabilistic CBF is used as an estimate of the\nmodel uncertainty.\n[22] incorporates physics through reward design as well\nas through simulator augmentation, and has been discussed\nin previous section.\n4.1.5\nAugment policy and/or value N/W:\nFigure 14: Example, augmentation of policy using physics\ninformation. In [4], given an observation st from the en-\nvironment, a neural dynamic policy generates w i.e. the\nweights of basis function and g which is a goal for the robot,\nfor a function fθ. This function is then used by an open\nloop controller to generate a set of actions from the robot\nto execute in the environment and collect next states and\nrewards to train the policy.\nIn [4], Bahl et al. proposes Neural Dynamic Policies\n(NDP) where they incorporate dynamical system as a dif-\nferentiable layer in the policy network, see Fig. 14. In NDP,\na NN Φ takes an input state (st) and predicts parameters\nof the dynamical system (i.e. (w, g) ). Which are then used\nto solve second-order differential equation ¨y = α(β(g −\ny) −˙(y)) + f(x), to obtain system states (y, ˙y, ¨y)., which\nrepresents the behavior of the dynamic system, given a state\ngoal g. Here α, β are global parameters allowing critical\ndamping of system and f is a non-linear forcing function\nwhich primarily captures the shape of trajectory. Depending\non robot’s coordinate system an inverse controller may also\nbe used to convert y to a, i.e. a = Ω(y, ˙y, ¨y) . The NDPs thus\ncan be defined as\nπ(a|s; θ) ≜Ω(DE(Φ(s; θ))), where\nDE(w, g) →{y, ˙y, ¨y}\nhere DE(w, g) represents solution of the differential equa-\ntion.\nExtending this work to hierarchical deep policy learn-\ning framework, [3] introduced H-NDP which forms a cur-\nriculum by learning local dynamical system-based policies\non small state-space region and then refines them into\nglobal dynamical system based policy. Given the accurate\ndynamics and constraint of the system [140] introduces\ncontrol barrier certificates into actor-critic RL framework, for\nlearning safe policies in dynamical systems. [87] proposes\na method for generating highly agile and visually guided\nlocomotion behaviors. They leverage MFRL while using\nmodel based optimization of ground reaction forces, as a\nbehavior regularizer.\nIn [31] proposes an approach of safe exploration using\nCLBF without explicitly employing any dynamic model.\nThe approach approximate the RL critic as a CLBF, from\ndata samples and parameterized with DNNs. Both the ac-\ntor and critic satisfies reachability and safety guarantees.\n[93] combines PINN with RL, where the value function is\ntreated as a PINN to solve Hamilton-Jacobi-Bellman (HJB)\nPDE. It enables the RL algorithm to exploit the physics of\nenvironment aswell as optimal control to improve learning\nand convergence.\n[98] proposes an optimization method for freeform\nnanophotonic devices, by combining adjoint based methods\n(ABM) and RL. In this work the value network is initialized\nwith adjoint gradient predicting network during initializa-\ntion of RL process. Cao et al. [14] have used physics model\nto influence reward function, as well as edit policy and value\nnetworks as necessary. The work has been mentioned before\nin reward design.\nTo improve policy optimization, [92] used differentiable\nsimulators to directly compute the analytic gradient of the\npolicy’s value function w.r.t. the actions generated by it.\nThis gradient information is used to monotonically improve\nthe policy’s value function. Gao et al. [40] proposes a tran-\nsient voltage control approach, by integrating physical and\ndata-driven models of power system. They also uses the\nconstraint of the physical model on the data-driven model\nto speed up convergence. A PINN trained using PDE of\ntransient process acts as the physical model and contributes\ndirectly to the loss of the RL algorithm.\nXu et al. [130] presents an efficient differentiable sim-\nulator (DS) with a new policy training algorithm which\ncan effectively leverage simulation gradients. The learning\nalgorithm alleviates issues inherent in DS while allowing\nmany physical environments to be run in parallel. [17]\nincorporates physics through action regulation and penalty\nsignal to agent, and has been discussed in previous section.\nIn MBRL setting, [85] leverage differentiable physics-\nbased simulation and differentiable rendering. By compar-\ning raw observations between simulated and real world, the\ninitial learned system model is continually updated, pro-\nducing a more physically consistent model. In data center\n(DC) cooling control application, [123] proposed a lifelong-\nRL approach under evolving DC environment. It leverages\nphysical laws of thermodynamics and the system and mod-\nels the DC thermal transition and power usage through\ndata collected online. Utilizing learned state transition and\nreward models it accelerates online adaptation.\nWorking with a nominal system model, [23] presented\nan RL framework where the agent learns model uncertainty\nin multiple general dynamic constraints, e.g. CLF and CBF,\nthrough data-driven training. A quadratic program then\nsolves for the control that satisfies the safety constraints\nunder learned model uncertainty.\n13\nTable 2: Summary of PIRL literature - Model Free.\nRef.\nYear\nContext/ Application\nRL Algorithm\nLearning arch.\nBias\nPhysics information\nPIRL methods\nRL pipeline\n[20]\n2018\nMotion capture\nPPO\nPhysics reward\nLearning\nPhysics simulator\nReward design\nProblem representation\n[100]\n2018\nMotion control\nPPO [109]\nPhysics reward\nLearning\nPhysics simulator\nReward design\nProblem representation\n[46]\n2018\nPolicy optimization\nPPO\nSim-to-Real\nObservational\nOffline data\nAugment simulator\nTraining\n[81]\n2018\nPolicy optimization\nNPG [126] (C)∗\nSim-to-Real\nObservational\nOffline data\nAugment simulator\nTraining\n[22]\n2019\nMolecular structure optimization\nDDPG\nPhysics reward\nLearning\nDFT (PS)\nReward design\nProblem representation\nAugment simulator\nTraining\n[74]\n2019\nSafe exploration and control\nPPO\nResidual RL\nLearning\nCBF, CLF, FSA/TL (BPC)\nReward design\nProblem representation\nAugment policy\nLearning strategy\n[4]\n2020\nDynamic system control\nPPO\nPhy. embed. N/W\nInductive\nDMP (PPV)\nAugment policy\nNetwork design\n[42]\n2020\nDexterous manipulations\nPPO\nResidual RL\nObservational\nPhysics simulator\nReward design\nProblem representation\n[83]\n2020\n3D Ego pose estimation\nPPO\nPhysics reward\nLearning\nPhysics simulator\nState, Reward design\nProblem representation\n[3]\n2021\nDynamic system control\nPPO\nHierarchical RL\nInductive\nDMP (PPV)\nAugment policy\nNetwork design\n[87]\n2021\nDynamic system control\nPPO\nHierarchical RL\nLearning\nWBIC (PPV)\nAugment policy\nLearning strategy\n[1]\n2021\nManufacturing\nSARSA [116]\nSim-to-Real\nObservational\nPhysics engine\nAugment simulator\nTraining\n[113]\n2021\nDynamic system control\nPPO\nPhy. variable\nLearning\nPhysics parameters\nReward design\nProblem representation\n[76]\n2021\nSafe exploration and control\nNFQ [104]\nSafety filter\nLearning\nPhysical constraint\nAction regulation\nProblem representation\n[59]\n2021\nSafe cruise control\nSAC\nPhy. variable\nObservational\nPhysical state (PPV)\nState design\nProblem representation\n[92]\n2021\nPolicy optimization\nDPG (C)\nDiff. Simulator\nLearning\nPhysics simulator\nAugment policy\nLearning strategy\n[101]\n2021\nOptimization, nuclear engineering\nDQN, PPO\nPhysics reward\nLearning bias\nPhysical properties (PPR)\nReward design\nProblem representation\n[139]\n2021\nAir-traffic control\nPPO\nData augmentation\nObservational\nRepresentation (ODR)\nState design\nProblem representation\n[124]\n2022\nMotion planner\nPPO + AC [67]\nSafety filter\nLearning\nCBF (BPC)\nAction regulation\nProblem representation\nReward design\n[17]\n2022\nActive voltage control\nTD3 (C)\nSafety filter\nLearning\nPhysical constraints\nPenalty function\nProblem representation\nAction regulation\n[28]\n2022\nInterfacial structure prediction\nDDPG\nOff-policy\nLearning\nPhysics model\nReward design\nProblem representation\n[40]\n2022\nTransient voltage control\nDQN\nPINN loss\nLearning\nPDE (DAE)\nAugment policy\nLearning strategy\n[45]\n2022\nBuilding control\nQ-learning (C)\nData augment\nObservational\nRepresentation (ODR)\nState design\nProblem representation\n[51]\n2022\nTraffic control\nQ-Learning\nData augment\nObservational\nPhysics model\nState design\nProblem representation\n[88]\n2022\nSafe exploration and control\nSAC\nSim-to-Real\nObservational\nPhysics model\nAugment simulator\nTraining\n[56]\n2022\nDynamic system control\nSAC (etc.)\nPhysics reward\nLearning\nBarrier function\nReward design\nProblem representation\n[130]\n2022\nPolicy Learning\nActor-critic (C)\nDiff. Simulator\nLearning\nPhysics simulator\nAugment policy\nLearning strategy\n[13]\n2023\nSafe exploration and control\nDDPG\nResidual RL\nLearning\nPhysics model\nReward design\nProblem representation\nAction regulation\n[14]\n2023\nSafe exploration and control\nDDPG\nResidual RL\nInductive\nPhysics model\nReward design\nProblem representation\nAction regulation\nInductive\nN/W editing (Aug. pol.)\nNetwork design\n[12]\n2023\nRobust voltage control\nSAC\nData augment\nObservational\nRepresentation (ODR)\nState design\nProblem representation\n[18]\n2023\nMean field games\nDDPG\nPhysics reward\nLearning\nPhysics model\nReward design\nProblem representation\n[133]\n2023\nSafe exploration and control\nPPO (C)\nSafety filter\nLearning\nNBC (BPC)\nAugment policy\nTraining\n[141]\n2023\nPower system stability enhancement\nCustom\nSafety filter\nLearning\nNBC (BPC)\nAction regulation\nProblem representation\n[31]\n2023\nSafe exploration and control\nAC (C)\nSafety filter\nLearning\nCLBF [107, 29] (BPC)\nAugment value N/W\nTraining\n[112]\n2023\nConnected automated vehicles\nDPPO\nPhysics variable\nObservational\nPhysical state (PPV)\nState design\nProblem representation\nLearning\nReward design\n[68]\n2023\nMusculoskeletal simulation\nSAC (C)\nPhysics variable\nLearning\nPhysical value\nReward design\nProblem representation\n[75]\n2023\nEnergy management\nMADRL(C)\nPhysics variable\nLearning\nPhysical target\nReward design\nProblem representation\n[93]\n2023\nPolicy optimization\nPPO\nPhy. embed N/W\nInductive\nPDE (DAE)\nAugment value N/W\nNetwork design\n[135]\n2023\nFlow field reconstruction\nA3C\nPhysics reward\nLearning\nPhysical constraints\nReward design\nProblem representation\n[98]\n2023\nFreeform nanophotonic devices\nϵ−greedy Q\nPhy. embed N/W\nInductive\nABM\nAugment value N/W\nNetwork design\n[106]\n2023\nDynamic system control\nDPG\nCurriculum learning\nLearning\nPhysics model\nAugment simulator\nTraining\n[111]\n2023\nEnergy management\nTD3\nSim-to-Real\nObservational\nPhysics model\nAugment simulator\nLearning strategy\n[134]\n2023\nRobot wireless navigation\nPPO\nPhysics reward\nLearning\nPhysical value\nReward design\nProblem representation\nC∗represents custom versions of the adjacent conventional algorithms.\n4.2\nReview of simulation/ evaluation benchmarks\nIn Table 4, we present the different training and evaluation\nbenchmarks that has been used in the reviewed PIRL litera-\nture. We list the important insights from the table:\n1. A majority works dealing with dynamic control have\nused OpenAI Gym [128], Safe Gym [133], MuJoCo [121,\n142], Pybullet [31] and Deep mind control suite envi-\nronments [108, 103], which are standard benchmarks in\nRL . Works dealing specifically with traffic management\nhave used platforms like SUMO [124] and CARLA\n[120].\n2. Works dealing with power and voltage management\nproblems have used IEEE distribution system bench-\nmarks [17, 40] to evaluate proposed algorithms. Alter-\nnatively in some works MATLAB/ SIMULINK plat-\nform is also used for training or evaluating RL agents\n[111]\n3. One crucial observation is that a huge number of work\nhave used customized or adapted environments for\ntraining and evaluation and have not used conventional\nenvironments [74, 24, 84].\n4.3\nAnalysis\n4.3.1\nResearch trend and statistics\nUse of RL algorithms:\nAs is evident from Fig.15 (a),\nPPO[109] and its variants are the most preferred RL algo-\nrithm, followed by DDPG [114]. Among the comparatively\nnew algorithms SAC[49] is preferred over TD3[38].\nTypes of physics priors used: In Fig.15 (b), we can see\nthat physics information takes the form of physics simulator,\n14\nTable 3: Summary of PIRL literature - Model based\nRef.\nYear\nContext/ Application\nAlgorithm\nLearning arch.\nBias\nPhysics information\nPIRL method\nRL pipeline\n[128]\n2016\nExploration and control\n-\nModel learning\nObservational\nSys. morphology (PPR)\nAugment model\nLearning strategy\n[108]\n2018\nDynamic system control\n-\nModel learning\nInductive\nPhysics model\nAugment model\nLearning strategy\n[95]\n2019\nSafe navigation\n-\nSafety filter\nLearning\nCBC (BPC)\nAction regulation\nProblem representation\n[19]\n2019\nSafe exploration and control\nTRPO, DDPG\nResidual RL\nLearning\nCBF (BPC)\nAction regulation\nProblem representation\n[121]\n2020\nControl (visual RL)\n-\nModel learning\nObservational\nEntity abstraction (ODR)\nAugment model\nLearning strategy\n[70]\n2020\nDynamic system control\n-\nModel learning\nObservational\nContext encoding (ODR)\nAugment model\nLearning strategy\n[23]\n2020\nSafe exploration and control\nDDPG [114]\nSafety filter\nLearning\nCBF, CLF, QP (BPC)\naugment policy\nLearning strategy\n[78]\n2021\nDynamic system control\nDyna + TD3(C)∗\nModel identification\nLearning\nPDE/ ODE, BC (DAE)\nAugment model\nLearning strategy\n[32]\n2021\nDynamic system control\nPPO\nResidual-RL\nLearning\nPhysics model\nAction regulation\nProblem representation\n[11]\n2021\nMulti agent collision avoidance\nMADDPG (C)\nSafety filter\nLearning\nCBF (BPC)\nAction regulation\nProblem representation\n[85]\n2022\nDynamic system control\nTD3(C)\nSim-to-Real\nLearning\nPhysics simulator\nAugment policy\nLearning strategy\n[120]\n2022\nTraffic control\nAC[86]\nSafety filter\nLearning\nCBF (BPC)\nAugment model\nLearning strategy\n[140]\n2022\nSafe exploration and control\nDDPG\nSafety filter\nLearning\nCBC (BPC)\nAugment policy\nLearning strategy\n[138]\n2022\nDistributed MPC\nAC [57]\nSafety filter\nLearning\nCBF (BPC)\nState design\nProblem representation\n[103]\n2023\nDynamic system control\nDreamer [50]\nPhy. embed. N/W\nInductive\nPhysics model\nAugment model\nNetwork design\n[24]\n2023\nSafe exploration and control\n-\nSafety filter\nLearning\nCBF (BPC)\nAugment model\nLearning strategy\n[54]\n2023\nAttitude control\n-\nPhy. embed N/W\nInductive\nSystem symmetry (PPR)\nAugment model\nNetwork design\n[123]\n2023\nData center cooling\nSAC\nModel identification\nLearning\nPhysics laws (PPR)\nAugment model\nLearning strategy\n[136]\n2023\nCooling system control\nDDPG\nResidual RL\nLearning\nCBF (BPC)\nAction regulation\nProblem representation\nC∗represents custom versions of the adjacent conventional algorithms.\nsystem models, barrier certificates and physical constraints,\nin a majority of works. PI types “Barrier certificate con-\nstraints and physical constraint” and “Physics simulator and\nmodels” dominates in more that 60% of works in “Action\nregulation” and “Augment policy and value N/W” PIRL\nmethods.\nLearning architecture and bias: In Fig.15 (c) we visu-\nalize the relationship between PIRL learning architectures\n(sec: 3.4.2) and the three biases through which physics is\ntypically incorporated in PIML approaches. In architectures\n“PI reward” and “safety filter”, physics is incorporated\nstrictly through “learning bias”, signifying the heavy use\nof constraints, regularizers and specialized loss functions.\nWhile “Physics embedded network” incorporates physics\ninformation through “inductive bias”, i.e. through imposi-\ntion of hard constraints through use specialized and custom\nphysics embodied networks.\nApplication domains: In Fig.15 (d) almost 85% of the\napplication problem dealt with PIRL approaches relates\nto controller or policy design. “Miscellaneous control” in-\ncludes optimal policy/ controller learning approaches for\ndifferent application sectors like energy management [75,\n111] and data-center cooling [123], and accounts to majority\nof applications. “Safe control and exploration”, includes\nthose works concerning with safety critical systems, ensur-\ning safe exploration and policy learning, accounts for 25%.\n“Dynamic control”, includes control of dynamic systems,\nincluding robot systems and amounts to about 23% of all\nworks surveyed. Other specific applications include opti-\nmization/ prediction [22, 28], motion capture/simulation\n[124, 20] and improvement of general policy optimization\napproaches [46, 81] through physics incorporation.\n4.3.2\nRL challenges addressed\nIn this section we will discuss and elaborate on how recent\nphysics incorporation in RL algorithms have addressed\ncertain open problems of the RL paradigm.\n1) Sample efficiency: RL approaches need a huge number\nof agent-environment interaction and related data to\nwork. One effective way of dealing with this problem\nis to use a surrogate for the real environment in the\nform of a simulator or learned model via data-driven\napproaches.\nPIRL approaches incorporate physics to augment simu-\nlators thus reducing the sim-to-real gap, thereby bring-\ning down online evaluation cycles [85, 1]. Also physics\nincorporation during system identification or model\nlearning phase in MBRL help reduce sample efficiency\nthrough learning a truer to real environment using\nlesser training samples [108, 121].\n2) Curse of dimensionality: RL algorithms become less effi-\ncient both in training and performing on environment\ndefined with high-dimensional and continuous state\nand action spaces, known as the ’curse of dimensional-\nity’. Typically dimensionality reduction techniques are\nused to encode the large state or action vectors into low\ndimensional representations. The RL algorithm is then\ntrained in this low dimensional setting.\nPIRL approaches extract underlying physics informa-\ntion from environment through learning physically rel-\nevant low dimensional representation from high di-\nmensional observation or state space [45, 12]. In [45],\na PINN is utilized to extract physically relevant infor-\nmation about the system’s hidden state, which is then\nused to learn a Q-function for policy optimization.\n3) Safety exploration: Safe reinforcement learning involves\nlearning control policies that guarantee system per-\n15\n(a)\n(b)\n(c)\n(d)\nFigure 15: Statistical analysis of PIRL literature. (a) Statistic\nof type of RL algorithms used, (b) Statistic of PI types used\nin each PIRL method, (c) Statistic of PIRL learning architec-\ntures and related biases, (d) Statistic of PIRL applications in\ndifferent domains.\nformance and respect safety constraints during both\nexploration and policy deployment.\nIn safety-critical applications using reinforcement learn-\ning, it’s crucial to regulate agent exploration. Control\nLyapunov function (CLF)[74, 23], barrier certificate/\nbarrier function (BF), control barrier function/ certifi-\ncate (CBF/ CBC) [19, 11] are commonly used concepts.\nBarrier certificates define safe states, while control bar-\nrier functions ensure states stay in the safety set. These\napproaches are typically used for systems with partial\nor learnable dynamics model and generally a known\nset of safe states/ actions.\n4) Partial observability or imperfect measurement: Partial ob-\nservability is a setting where due to noise, missing\ninformation, or outside interference, an RL agent is un-\nable to obtain the complete states needed to understand\nthe environment.\nPIRL approaches modify or enhance the state represen-\ntation to provide more useful information, in cases of\nmissing or inadequate information. This may involve\nstate fusion, which incorporates additional physics or\ngeographical information from the environment [59] or\nother agents [112].\n5) Under-defined reward function: Defining the reward func-\ntion is critical in creating MDPs and ensuring the ef-\nfectiveness and efficiency of RL algorithms. However,\nsince they are created by humans, there is a risk of them\nbeing under-defined and not guiding the RL algorithm\neffectively in policy optimization.\nPIRL\napproaches\nintroduce\nphysics\ninformation\nthrough effective reward design or augmentation of\nexisting reward functions with bonuses or penalties\n[28, 83, 42, 113]. For example, in a sim-to-real setting,\n[113] proposed a framework for specifying rewards that\ncombines probabilistic costs associated with primary\nforces and velocities. The framework creates a paramet-\nric reward function for common robotic gaits, in biped\nrobots.\n5\nOPEN CHALLENGES AND RESEARCH DIREC-\nTIONS\n5.1\nHigh Dimensional Spaces\nA large number of real world tasks deals with high di-\nmensional and continuous state and action spaces. One\npopular method to address this high dimensionality issue is\nto compress the state space (or action space) vectors into low\ndimensional vectors. A PI based approach may learn high\nquality environment representations using deep networks\nand extract physically relevant low dimensional features\nfrom them.\nBut learning a compressed and informative latent space\nfrom high dimensional continuous state (or action) space\nstill remains a hurdle. Also learning physically relevant\nrepresentation is still an open problem. Future research\nshould address this issue and try to devise approaches that\nhelps to incorporate or take guidance of underlying physics\nduring representation learning or feature extraction, so as to\nmake them both informative and physically pertinent.\n5.2\nSafety in Complex and Uncertain Environments\nIn the realm of safe reinforcement learning, striking a\nbalance between the complexity of the environment and\nensuring safety is always a challenge. Current physics in-\nformed approaches uses different control theoretic concepts\ne.g. CBFs to ensure safe exploration and learning of the RL\n16\nTable 4: Summary of PIRL training/ evaluation benchmarks.\nSimulator/ platform\nSpecific environment/ system name\nReference\nOpenAI Gym\nPusher, Striker, ErgoReacher\n[46]\nOpenAI Gym\nMountain Car, Lunar Lander (continuous)\n[56]\nOpenAI Gym\nCart-Pole, Pendulum (simple and double)\n[128]\nOpenAI Gym\nCart-pole\n[13]\nOpenAI Gym\nCart-pole and Quadruped robot\n[14]\nOpenAI Gym\nCartPole, Pendulum\n[78]\nOpenAI Gym\nInverted Pendulum (pendulum −v0),\n[19]\nOpenAI Gym\nMountain car (cont.), Pendulum, Cart pole\n[140]\nOpenAI Gym\nSimulated car following [53]\nMuJoCo\nAnt, HalfCheetah, Humanoid, Walker2d\n[93]\nHumanoid standup, Swimmer, Hopper\nInverted and Inverted Double Pendulum (v4)\nMuJoCo\nCassie-MuJoCo-sim [105]\n[113, 32]\n6 DoF Kinova Jaco [44]\n[4, 3]\nMuJoCo\nHalfCheetah, Ant,\n[70]\nCrippledHalfCheetah, and SlimHumanoid [142]\nMuJoCo\nBlock stacking task [55]\n[121]\nOpenAI Gym\nCartPole, Pendulum\nOpenSim-RL[64]\nL2M2019 environment\n[68]\nSafety gym [137]\nPoint, car and Doggo goal\n[133]\n-\nCart pole swing up, Ant\n[130]\n-\nHumanoid, Humanoid MTU\n-\nAutonomous driving system\n[18]\nDeep control suite [117]\nPendulum, Cartpole, Walker2d\n[108]\nAcrobot, Swimmer, Cheetah\n-\nJACO arm (real world)\nDeep control suite\nReacher, Pendulum, Cartpole,\n[103]\nCart-2-pole, Acrobot,\nCart-3-pole and Acro-3-bot\n-\nRabbit[21]\n[23]\nMARL env. [80]\nMulti-agent particle env.\n[11]\nADROIT[102]\nShadow dexterous hand\n[42]\n-\nFirst-Person Hand Action Benchmark[43]\nMuJoCo\nDoor opening, in-hand manipulation,\ntool use and object relocation\nSUMO[79], METANET[69]\n-\n[51]\nSUMO\n-\n[124]\nCARLA[30]\n-\n[120]\nGazebo[66]\nQuadrotor (IF750A)\n[54]\nIEEE Distribution\nIEEE 33-bus and 141-bus distribution networks\n[17]\nsystem benchmarks\nIEEE 33-node system\n[12, 17]\nIEEE 9-bus standard system\n[40]\n-\nCustom (COMSOL based)\n[1]\n-\nCustom (DFT based)\n[22]\n-\nCustom (based on [122])\n[45]\n-\nCustom (based on [63])\n[59]\n-\nCustom\n[75, 88, 28]\n-\nCustom\n[98, 112, 134]\n-\nCustom\n[135, 136, 141]\n-\nCustom\n[74, 24, 84]\n-\nCustom\n[123, 18]\nOpen AI Gym\nCustom (based on geometries of Nuclear reactor)\n[101]\nMATLAB-Simulink\nCustom\n[111, 138]\n-\nCustom [143]\n[92]\nMATLAB\nCruise control\n[76]\nPygame\nCustom\n[139]\n-\nCustom (Unicycle, Car-following)\n[36]\n-\nBrushbot, Quadrotor (sim)\n[95]\nPhantom manipulation platform\n[81]\nPybullet\n2 finger gripper\ngym-pybullet-drones[97]\n[31]\nPybullet\nFranka Panda, Flexiv Rizon (also real world robots)\n[85]\nNimblePhysics[125],\nRedner[73] (Differentiable sim.)\n-\nCustom MOCAP\n[20, 100, 83]\nagent. But these approaches are limited by the approximated\nmodel of the system and the prior knowledge about safe\nstate sets. There has been a lot of research for better system\nidentification or model learning through physics incorpora-\ntion. But most works do not generalize well to different tasks\nand environments. To summarize, future works should ad-\ndress these crucial research goals: 1) model agnostic safe\nexploration and control using RL agents in complex and\nuncertain environments and 2) devise generalized approach\nof incorporating physics in data-driven Model learning.\n5.3\nChoice of physics prior\nChoice of the physics prior is very crucial for the PIRL\nalgorithm. But such choice is difficult and requires extensive\nstudy of the system and may vary extensively from one\ncase to another even in same domains. To enhance efficacy,\ndevising a comprehensive framework with physics informa-\ntion to manage novel physical tasks is preferable rather than\ndealing with tasks individually.\n5.4\nEvaluation and bench-marking platform\nCurrently, PIRL doesn’t have comprehensive benchmarking\nand evaluation environments to test and compare new\nphysics approaches before induction. This limitation makes\nit challenging to assess the quality and uniqueness of new\nworks.\nAdditionally, most PIRL works rely on customized envi-\nronments related to a particular domain, making it difficult\nto compare PIRL algorithms fairly. Moreover, PIRL applica-\ntion cases are diverse, and the physics information chosen is\nspecific to a domain, requiring extensive study and domain\nexpertise to understand and compare such works.\n6\nCONCLUSIONS\nThis paper presents a state-of-the-art reinforcement learn-\ning paradigm, known as physics-informed reinforcement\nlearning (PIRL). By leveraging both data-driven techniques\nand knowledge of underlying physical principles, PIRL is\ncapable of improving the effectiveness, sample efficiency\nand accelerated training of RL algorithms/ approaches, for\ncomplex problem-solving and real-world deployment. We\nhave created two taxonomies that categorize conventional\nPIRL methods based on physics prior/information type\nand physics prior induction (RL methods), providing a\nframework for understanding this approach. To help readers\ncomprehend the physics involved in solving RL tasks, we\nhave included various explanatory images from recent pa-\npers and summarized their characteristics in Tables 2 and 3.\nAdditionally, we have provided a benchmark-summary ta-\nble 4 detailing the training and evaluation benchmarks used\nfor PIRL evaluation. Our objective is to simplify the complex\nconcepts of existing PIRL approaches, making them more\naccessible for use in various domains. Finally, we discuss\nthe limitations and unanswered questions of current PIRL\nwork, encouraging further research in this area.\n7\nACKNOWLEDGMENT\nThis research was partly supported by the Advance Queens-\nland Industry Research Fellowship AQIRF024-2021RD4.\nREFERENCES\n[1]\nMd Ferdous Alam et al. “A physics-guided rein-\nforcement learning framework for an autonomous\nmanufacturing system with expensive data.” In: 2021\nAmerican Control Conference (ACC). 2021, pp. 484–490.\n[2]\nAaron D Ames et al. “Control barrier functions: The-\nory and applications.” In: 2019 18th European control\nconference (ECC). 2019, pp. 3420–3431.\n[3]\nShikhar Bahl, Abhinav Gupta, and Deepak Pathak.\n“Hierarchical neural dynamic policies.” In: arXiv\npreprint arXiv:2107.05627 (2021).\n[4]\nShikhar Bahl et al. “Neural dynamic policies for end-\nto-end sensorimotor learning.” In: Advances in Neural\nInformation Processing Systems 33 (2020), pp. 5058–\n5069.\n[5]\nChayan Banerjee, Zhiyong Chen, and Nasimul No-\nman. “Boosting Exploration in Actor-Critic Algo-\nrithms by Incentivizing Plausible Novel States.” In:\narXiv preprint arXiv:2210.00211 (2022).\n17\n[6]\nChayan Banerjee, Zhiyong Chen, and Nasimul No-\nman. “Improved soft actor-critic: Mixing prioritized\noff-policy samples with on-policy experiences.” In:\nIEEE Transactions on Neural Networks and Learning\nSystems (2022).\n[7]\nChayan Banerjee et al. “Optimal Actor-Critic Policy\nWith Optimized Training Datasets.” In: IEEE Trans-\nactions on Emerging Topics in Computational Intelligence\n6.6 (2022), pp. 1324–1334.\n[8]\nChayan Banerjee et al. Physics-Informed Computer Vi-\nsion: A Review and Perspectives. 2023. arXiv: 2305 .\n18035 [eess.IV].\n[9]\nGabriel Barth-Maron et al. “Distributed distribu-\ntional deterministic policy gradients.” In: arXiv\npreprint arXiv:1804.08617 (2018).\n[10]\nSerena Booth et al. “The perils of trial-and-error\nreward design: misdesign through overfitting and\ninvalid task specifications.” In: AAAI Conference on\nArtificial Intelligence. Vol. 37. 5. 2023, pp. 5920–5929.\n[11]\nZhiyuan Cai et al. “Safe multi-agent reinforcement\nlearning through decentralized multiple control bar-\nrier functions.” In: arXiv preprint arXiv:2103.12553\n(2021).\n[12]\nDi\nCao\net\nal.\n“Physics-informed\nGraphical\nRepresentation-enabled\nDeep\nReinforcement\nLearning for Robust Distribution System Voltage\nControl.” In: IEEE Transactions on Smart Grid (2023).\n[13]\nHongpeng Cao et al. “Physical Deep Reinforce-\nment Learning Towards Safety Guarantee.” In: arXiv\npreprint arXiv:2303.16860 (2023).\n[14]\nHongpeng Cao et al. “Physical Deep Reinforce-\nment Learning: Safety and Unknown Unknowns.”\nIn: arXiv preprint arXiv:2305.16614 (2023).\n[15]\nCi Chen et al. “Off-policy learning for adaptive opti-\nmal output synchronization of heterogeneous multi-\nagent systems.” In: Automatica 119 (2020), p. 109081.\nISSN: 0005-1098.\n[16]\nJianyu Chen, Bodi Yuan, and Masayoshi Tomizuka.\n“Model-free deep reinforcement learning for urban\nautonomous driving.” In: 2019 IEEE intelligent trans-\nportation systems conference (ITSC). 2019, pp. 2765–\n2771.\n[17]\nPengcheng Chen et al. “Physics-Shielded Multi-\nAgent Deep Reinforcement Learning for Safe Active\nVoltage Control with Photovoltaic/Battery Energy\nStorage Systems.” In: IEEE Transactions on Smart Grid\n(2022).\n[18]\nXu Chen, Shuo Liu, and Xuan Di. “A Hybrid\nFramework of Reinforcement Learning and Physics-\nInformed Deep Learning for Spatiotemporal Mean\nField Games.” In: 2023 International Conference on\nAutonomous Agents and Multiagent Systems. 2023,\npp. 1079–1087.\n[19]\nRichard Cheng et al. “End-to-end safe reinforcement\nlearning through barrier functions for safety-critical\ncontinuous control tasks.” In: AAAI conference on\nartificial intelligence. Vol. 33. 01. 2019, pp. 3387–3395.\n[20]\nNuttapong Chentanez et al. “Physics-based motion\ncapture imitation with deep reinforcement learning.”\nIn: 11th ACM SIGGRAPH Conference on Motion, Inter-\naction and Games. 2018, pp. 1–10.\n[21]\nChristine Chevallereau et al. “Rabbit: A testbed for\nadvanced control theory.” In: IEEE Control Systems\nMagazine 23.5 (2003), pp. 57–79.\n[22]\nYoungwoo Cho et al. “Physics-guided reinforcement\nlearning for 3D molecular structures.” In: Workshop\nat the 33rd Conference on Neural Information Processing\nSystems (NeurIPS). 2019.\n[23]\nJason Choi et al. “Reinforcement learning for safety-\ncritical control under model uncertainty, using con-\ntrol lyapunov functions and control barrier func-\ntions.” In: arXiv preprint arXiv:2004.07584 (2020).\n[24]\nMax H Cohen and Calin Belta. “Safe exploration\nin model-based reinforcement learning using con-\ntrol barrier functions.” In: Automatica 147 (2023),\np. 110684.\n[25]\nMiles Cranmer et al. “Lagrangian neural networks.”\nIn: arXiv preprint arXiv:2003.04630 (2020).\n[26]\nSalvatore Cuomo et al. “Scientific Machine Learn-\ning through Physics-Informed Neural Networks:\nWhere we are and What’s next.” In: arXiv preprint\narXiv:2201.05624 (2022).\n[27]\nMark Cutler, Thomas J Walsh, and Jonathan P How.\n“Real-world reinforcement learning via multifidelity\nsimulators.” In: IEEE Transactions on Robotics 31.3\n(2015), pp. 655–671.\n[28]\nZhuoran Dang and Mamoru Ishii. “Towards stochas-\ntic modeling for two-phase flow interfacial area pre-\ndictions: A physics-informed reinforcement learning\napproach.” In: International Journal of Heat and Mass\nTransfer 192 (2022), p. 122919.\n[29]\nCharles Dawson et al. “Safe nonlinear control using\nrobust neural lyapunov-barrier functions.” In: Con-\nference on Robot Learning. PMLR. 2022, pp. 1724–1735.\n[30]\nAlexey Dosovitskiy et al. “CARLA: An open urban\ndriving simulator.” In: Conference on robot learning.\nPMLR. 2017, pp. 1–16.\n[31]\nDesong Du et al. “Reinforcement Learning for Safe\nRobot Control using Control Lyapunov Barrier Func-\ntions.” In: arXiv preprint arXiv:2305.09793 (2023).\n[32]\nHelei Duan et al. “Learning task space actions\nfor bipedal locomotion.” In: 2021 IEEE International\nConference on Robotics and Automation (ICRA). 2021,\npp. 1276–1282.\n[33]\nGabriel Dulac-Arnold et al. “Challenges of real-\nworld reinforcement learning: definitions, bench-\nmarks and analysis.” In: Machine Learning 110.9\n(2021), pp. 2419–2468.\n[34]\nGabriel Dulac-Arnold et al. “Deep reinforcement\nlearning in large discrete action spaces.” In: arXiv\npreprint arXiv:1512.07679 (2015).\n[35]\nJulian EEßerer et al. “Guided Reinforcement Learn-\ning: A Review and Evaluation for Efficient and Ef-\nfective Real-World Robotics.” In: IEEE Robotics &\nAutomation Magazine (2022).\n[36]\nYousef Emam et al. “Safe reinforcement learning\nusing robust control barrier functions.” In: IEEE\nRobotics and Automation Letters 99 (2022), pp. 1–8.\n[37]\nN Benjamin Erichson, Michael Muehlebach, and\nMichael W Mahoney. “Physics-informed autoen-\ncoders for Lyapunov-stable fluid flow prediction.”\nIn: arXiv preprint arXiv:1905.10866 (2019).\n18\n[38]\nScott Fujimoto, Herke Hoof, and David Meger. “Ad-\ndressing function approximation error in actor-critic\nmethods.” In: International conference on machine learn-\ning. PMLR. 2018, pp. 1587–1596.\n[39]\nBolin Gao and Lacra Pavel. “On Passivity, Rein-\nforcement Learning, and Higher Order Learning in\nMultiagent Finite Games.” In: IEEE Transactions on\nAutomatic Control 66.1 (2021), pp. 121–136.\n[40]\nJiemai Gao et al. “Transient Voltage Control Based\non Physics-Informed Reinforcement Learning.” In:\nIEEE Journal of Radio Frequency Identification 6 (2022),\npp. 905–910.\n[41]\nJavier Garcia and Fernando Fernández. “Safe explo-\nration of state and action spaces in reinforcement\nlearning.” In: Journal of Artificial Intelligence Research\n45 (2012), pp. 515–564.\n[42]\nGuillermo Garcia-Hernando, Edward Johns, and\nTae-Kyun Kim. “Physics-based dexterous manipula-\ntions with estimated hand poses and residual rein-\nforcement learning.” In: 2020 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS).\n2020, pp. 9561–9568.\n[43]\nGuillermo Garcia-Hernando et al. “First-person hand\naction benchmark with rgb-d videos and 3d hand\npose annotations.” In: IEEE conference on computer\nvision and pattern recognition. 2018, pp. 409–419.\n[44]\nDibya Ghosh et al. “Divide-and-conquer reinforce-\nment learning.” In: arXiv preprint arXiv:1711.09874\n(2017).\n[45]\nGargya Gokhale, Bert Claessens, and Chris Develder.\n“PhysQ: A Physics Informed Reinforcement Learn-\ning Framework for Building Control.” In: arXiv\npreprint arXiv:2211.11830 (2022).\n[46]\nFlorian Golemo et al. “Sim-to-real transfer with\nneural-augmented robot simulation.” In: Conference\non Robot Learning. PMLR. 2018, pp. 817–828.\n[47]\nSamuel Greydanus, Misko Dzamba, and Jason Yosin-\nski. “Hamiltonian neural networks.” In: Advances in\nneural information processing systems 32 (2019).\n[48]\nShangding Gu et al. “A review of safe reinforce-\nment learning: Methods, theory and applications.”\nIn: arXiv preprint arXiv:2205.10330 (2022).\n[49]\nTuomas Haarnoja et al. “Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with\na stochastic actor.” In: International conference on ma-\nchine learning. PMLR. 2018, pp. 1861–1870.\n[50]\nDanijar Hafner et al. “Dream to control: Learning\nbehaviors by latent imagination.” In: arXiv preprint\narXiv:1912.01603 (2019).\n[51]\nYu Han et al. “A physics-informed reinforcement\nlearning-based strategy for local and coordinated\nramp metering.” In: Transportation Research Part C:\nEmerging Technologies 137 (2022), p. 103584.\n[52]\nZhongkai Hao et al. “Physics-Informed Machine\nLearning: A Survey on Problems, Methods and Ap-\nplications.” In: arXiv preprint arXiv:2211.08064 (2022).\n[53]\nChaozhe R He, I Ge Jin, and Gábor Orosz. “Data-\nbased fuel-economy optimization of connected au-\ntomated trucks in traffic.” In: 2018 Annual American\nControl Conference (ACC). 2018, pp. 5576–5581.\n[54]\nJunchang Huang et al. “Symmetry-informed Rein-\nforcement Learning and Its Application to the Atti-\ntude Control of Quadrotors.” In: IEEE Transactions on\nArtificial Intelligence (2023).\n[55]\nMichael Janner et al. “Reasoning about physical in-\nteractions with object-oriented prediction and plan-\nning.” In: arXiv preprint arXiv:1812.10972 (2018).\n[56]\nJiazhou Jiang, Minyue Fu, and Zhiyong Chen.\n“Physics informed intrinsic rewards in reinforcement\nlearning.” In: 2022 Australian & New Zealand Control\nConference (ANZCC). 2022, pp. 74–69.\n[57]\nYi Jiang et al. “Cooperative adaptive optimal out-\nput regulation of nonlinear discrete-time multi-agent\nsystems.” In: Automatica 121 (2020), p. 109149.\n[58]\nTobias Johannink et al. “Residual reinforcement\nlearning for robot control.” In: 2019 International\nConference on Robotics and Automation (ICRA). 2019,\npp. 6023–6029.\n[59]\nSorin Liviu Jurj et al. “Increasing the safety of adap-\ntive cruise control using physics-guided reinforce-\nment learning.” In: Energies 14.22 (2021), p. 7572.\n[60]\nGeorge Em Karniadakis et al. “Physics-informed ma-\nchine learning.” In: Nature Reviews Physics 3.6 (2021),\npp. 422–440.\n[61]\nAli Kashefi, Davis Rempe, and Leonidas J Guibas. “A\npoint-cloud deep learning framework for prediction\nof fluid flow fields on irregular geometries.” In:\nPhysics of Fluids 33.2 (2021), p. 027104.\n[62]\nK Kashinath et al. “Physics-informed machine learn-\ning: case studies for weather and climate modelling.”\nIn: Philosophical Transactions of the Royal Society A 379\n(2021), pp. 1–36.\n[63]\nArne Kesting et al. “Jam-avoiding adaptive cruise\ncontrol (ACC) and its impact on traffic dynamics.” In:\nTraffic and Granular Flow’05. Springer. 2007, pp. 633–\n643.\n[64]\nŁukasz Kidzi´nski et al. “Learning to run challenge:\nSynthesizing physiologically accurate motion using\ndeep reinforcement learning.” In: The NIPS’17 Com-\npetition: Building Intelligent Systems. Springer. 2018,\npp. 101–120.\n[65]\nW Bradley Knox et al. “Reward (mis) design for\nautonomous driving.” In: Artificial Intelligence 316\n(2023), p. 103829.\n[66]\nNathan Koenig and Andrew Howard. “Design and\nuse paradigms for gazebo, an open-source multi-\nrobot simulator.” In: 2004 IEEE/RSJ international con-\nference on intelligent robots and systems (IROS)(IEEE\nCat. No. 04CH37566). Vol. 3. 2004, pp. 2149–2154.\n[67]\nVijay Konda and John Tsitsiklis. “Actor-critic algo-\nrithms.” In: Advances in neural information processing\nsystems 12 (1999).\n[68]\nSoroush Korivand, Nader Jalili, and Jiaqi Gong.\n“Inertia-Constrained Reinforcement Learning to En-\nhance Human Motor Control Modeling.” In: Sensors\n23.5 (2023), p. 2698.\n[69]\nApostolos Kotsialos et al. “Traffic flow modeling\nof large-scale motorway networks using the macro-\nscopic modeling tool METANET.” In: IEEE Transac-\ntions on intelligent transportation systems 3.4 (2002),\npp. 282–292.\n19\n[70]\nKimin Lee et al. “Context-aware dynamics model for\ngeneralization in model-based reinforcement learn-\ning.” In: International Conference on Machine Learning.\nPMLR. 2020, pp. 5757–5766.\n[71]\nSergey Levine et al. “End-to-end training of deep\nvisuomotor policies.” In: The Journal of Machine Learn-\ning Research 17.1 (2016), pp. 1334–1373.\n[72]\nSergey Levine et al. “Offline reinforcement learning:\nTutorial, review, and perspectives on open prob-\nlems.” In: arXiv preprint arXiv:2005.01643 (2020).\n[73]\nTzu-Mao Li et al. “Differentiable monte carlo ray\ntracing through edge sampling.” In: ACM Transac-\ntions on Graphics (TOG) 37.6 (2018), pp. 1–11.\n[74]\nXiao Li and Calin Belta. “Temporal logic guided safe\nreinforcement learning using control barrier func-\ntions.” In: arXiv preprint arXiv:1903.09885 (2019).\n[75]\nYuanzheng Li et al. “Federated multiagent deep re-\ninforcement learning approach via physics-informed\nreward for multimicrogrid energy management.” In:\nIEEE Transactions on Neural Networks and Learning\nSystems (2023).\n[76]\nYutong Li et al. “Safe reinforcement learning using\nrobust action governor.” In: Learning for Dynamics and\nControl. PMLR. 2021, pp. 1093–1104.\n[77]\nZongyi Li et al. “Fourier neural operator for paramet-\nric partial differential equations.” In: arXiv preprint\narXiv:2010.08895 (2020).\n[78]\nXin-Yang Liu and Jian-Xun Wang. “Physics-informed\nDyna-style model-based deep reinforcement learn-\ning for dynamic control.” In: Royal Society A 477.2255\n(2021), p. 20210618.\n[79]\nPablo Alvarez Lopez et al. “Microscopic traffic simu-\nlation using sumo.” In: 2018 21st international confer-\nence on intelligent transportation systems (ITSC). 2018,\npp. 2575–2582.\n[80]\nRyan Lowe et al. “Multi-agent actor-critic for mixed\ncooperative-competitive environments.” In: Advances\nin neural information processing systems 30 (2017).\n[81]\nKendall Lowrey et al. “Reinforcement learning for\nnon-prehensile manipulation: Transfer from simula-\ntion to physical system.” In: 2018 IEEE International\nConference on Simulation, Modeling, and Programming\nfor Autonomous Robots (SIMPAR). 2018, pp. 35–42.\n[82]\nLu Lu et al. “Learning nonlinear operators via Deep-\nONet based on the universal approximation theo-\nrem of operators.” In: Nature Machine Intelligence 3.3\n(2021), pp. 218–229.\n[83]\nZhengyi Luo et al. “Kinematics-guided reinforce-\nment learning for object-aware 3d ego-pose estima-\ntion.” In: arXiv preprint arXiv:2011.04837 (2020).\n[84]\nMichael Lutter et al. “Differentiable physics mod-\nels for real-world offline model-based reinforcement\nlearning.” In: 2021 IEEE International Conference on\nRobotics and Automation (ICRA). 2021, pp. 4163–4170.\n[85]\nJun Lv et al. “SAM-RL: Sensing-Aware Model-Based\nReinforcement Learning via Differentiable Physics-\nBased Simulation and Rendering.” In: arXiv preprint\narXiv:2210.15185 (2022).\n[86]\nHaitong Ma et al. “Model-based constrained rein-\nforcement learning using generalized control bar-\nrier function.” In: 2021 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS). 2021,\npp. 4552–4559.\n[87]\nGabriel B Margolis et al. “Learning to jump from\npixels.” In: arXiv preprint arXiv:2110.15344 (2021).\n[88]\nJohn Martin and Hanspeter Schaub. “Reinforcement\nlearning and orbit-discovery enhanced by small-\nbody physics-informed neural network gravity mod-\nels.” In: AIAA SCITECH 2022 Forum. 2022, p. 2272.\n[89]\nChuizheng Meng et al. “When Physics Meets Ma-\nchine Learning: A Survey of Physics-Informed Ma-\nchine Learning.” In: arXiv preprint arXiv:2203.16797\n(2022).\n[90]\nXuhui Meng et al. “Learning functional priors and\nposteriors from data and physics.” In: Journal of\nComputational Physics 457 (2022), p. 111073.\n[91]\nVolodymyr\nMnih\net\nal.\n“Playing\natari\nwith\ndeep reinforcement learning.” In: arXiv preprint\narXiv:1312.5602 (2013).\n[92]\nMiguel Angel Zamora Mora et al. “Pods: Policy\noptimization via differentiable simulation.” In: Inter-\nnational Conference on Machine Learning. PMLR. 2021,\npp. 7805–7817.\n[93]\nAmartya Mukherjee and Jun Liu. “Bridging Physics-\nInformed\nNeural\nNetworks\nwith\nReinforcement\nLearning: Hamilton-Jacobi-Bellman Proximal Pol-\nicy\nOptimization\n(HJBPPO).”\nIn:\narXiv\npreprint\narXiv:2302.00237 (2023).\n[94]\nMichael Neunert et al. “Continuous-discrete rein-\nforcement learning for hybrid control in robotics.”\nIn: Conference on Robot Learning (2020), pp. 735–751.\n[95]\nMotoya Ohnishi et al. “Barrier-certified adaptive re-\ninforcement learning with applications to brushbot\nnavigation.” In: IEEE Transactions on robotics 35.5\n(2019), pp. 1186–1205.\n[96]\nBła˙zej Osi´nski et al. “Simulation-based reinforcement\nlearning for real-world autonomous driving.” In:\n2020 IEEE international conference on robotics and au-\ntomation (ICRA). 2020, pp. 6411–6418.\n[97]\nJacopo Panerati et al. “Learning to fly—a gym en-\nvironment with pybullet physics for reinforcement\nlearning of multi-agent quadcopter control.” In: 2021\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS). 2021, pp. 7512–7519.\n[98]\nChaejin Park et al. “Physics-informed reinforce-\nment learning for sample-efficient optimization of\nfreeform nanophotonic devices.” In: arXiv preprint\narXiv:2306.04108 (2023).\n[99]\nXue Bin Peng et al. “DeepLoco: Dynamic locomotion\nskills using hierarchical deep reinforcement learn-\ning.” In: ACM Transactions on Graphics (TOG) 36.4\n(2017), pp. 1–13.\n[100]\nXue Bin Peng et al. “Deepmimic: Example-guided\ndeep reinforcement learning of physics-based char-\nacter skills.” In: ACM Transactions On Graphics (TOG)\n37.4 (2018), pp. 1–14.\n[101]\nMajdi I Radaideh et al. “Physics-informed reinforce-\nment learning optimization of nuclear assembly de-\nsign.” In: Nuclear Engineering and Design 372 (2021),\np. 110966.\n[102]\nAravind\nRajeswaran\net\nal.\n“Learning\ncomplex\ndexterous manipulation with deep reinforcement\n20\nlearning and demonstrations.” In: arXiv preprint\narXiv:1709.10087 (2017).\n[103]\nAdithya\nRamesh\nand\nBalaraman\nRavindran.\n“Physics-Informed\nModel-Based\nReinforcement\nLearning.” In: Learning for Dynamics and Control\nConference. PMLR. 2023, pp. 26–37.\n[104]\nMartin Riedmiller. “Neural fitted Q iteration–first ex-\nperiences with a data efficient neural reinforcement\nlearning method.” In: 16th European Conference on\nMachine Learning. Springer. 2005, pp. 317–328.\n[105]\nAgility robotics. Cassie-mujoco-sim. Year Published/\nLast Updated. URL: https://github.com/osudrl/\ncassie-mujoco-sim.\n[106]\nColin\nRodwell\nand\nPhanindra\nTallapragada.\n“Physics-informed\nreinforcement\nlearning\nfor\nmotion control of a fish-like swimming robot.” In:\nScientific Reports 13.1 (2023), pp. 1–17.\n[107]\nMuhammad\nZakiyullah\nRomdlony\nand\nBayu\nJayawardhana. “Stabilization with guaranteed safety\nusing control Lyapunov–barrier function.” In: Auto-\nmatica 66 (2016), pp. 39–47.\n[108]\nAlvaro Sanchez-Gonzalez et al. “Graph networks\nas learnable physics engines for inference and con-\ntrol.” In: International Conference on Machine Learning.\nPMLR. 2018, pp. 4470–4479.\n[109]\nJohn Schulman et al. “Proximal policy optimiza-\ntion algorithms.” In: arXiv preprint arXiv:1707.06347\n(2017).\n[110]\nViraj Shah et al. “Encoding invariances in deep gen-\nerative models.” In: arXiv preprint arXiv:1906.01626\n(2019).\n[111]\nBuxin She et al. “Inverter PQ Control with Trajectory\nTracking Capability for Microgrids Based on Physics-\ninformed Reinforcement Learning.” In: IEEE Transac-\ntions on Smart Grid (2023).\n[112]\nHaotian Shi et al. “Physics-informed deep reinforce-\nment\nlearning-based\nintegrated\ntwo-dimensional\ncar-following control strategy for connected auto-\nmated vehicles.” In: Knowledge-Based Systems 269\n(2023), p. 110485.\n[113]\nJonah Siekmann et al. “Sim-to-real learning of all\ncommon bipedal gaits via periodic reward com-\nposition.” In: 2021 IEEE International Conference on\nRobotics and Automation (ICRA). 2021, pp. 7309–7315.\n[114]\nDavid Silver et al. “Deterministic policy gradient\nalgorithms.” In: International conference on machine\nlearning. Pmlr. 2014, pp. 387–395.\n[115]\nRichard S Sutton and Andrew G Barto. Reinforcement\nlearning: An introduction. MIT press, 2018.\n[116]\nRichard S Sutton and Andrew G Barto. “Reinforce-\nment learning: an introduction MIT Press.” In: Cam-\nbridge, MA 22447 (1998).\n[117]\nYuval Tassa et al. “Deepmind control suite.” In: arXiv\npreprint arXiv:1801.00690 (2018).\n[118]\nChen Tessler et al. “Action assembly: Sparse imita-\ntion learning for text based games with combinato-\nrial action spaces.” In: arXiv preprint arXiv:1905.09700\n(2019).\n[119]\nMarin\nToromanoff,\nEmilie\nWirbel,\nand\nFabien\nMoutarde. “End-to-end model-free reinforcement\nlearning for urban driving using implicit affor-\ndances.” In: IEEE/CVF conference on computer vision\nand pattern recognition. 2020, pp. 7153–7162.\n[120]\nSoumith Udatha, Yiwei Lyu, and John Dolan. “Safe\nReinforcement Learning with Probabilistic Control\nBarrier Functions for Ramp Merging.” In: arXiv\npreprint arXiv:2212.00618 (2022).\n[121]\nRishi Veerapaneni et al. “Entity abstraction in visual\nmodel-based reinforcement learning.” In: Conference\non Robot Learning. PMLR. 2020, pp. 1439–1456.\n[122]\nEvangelos Vrettos et al. “Experimental demonstra-\ntion of frequency regulation by commercial build-\nings—Part I: Modeling and hierarchical control de-\nsign.” In: IEEE Transactions on Smart Grid 9.4 (2016),\npp. 3213–3223.\n[123]\nRuihang Wang et al. “Phyllis: Physics-Informed Life-\nlong Reinforcement Learning for Data Center Cool-\ning Control.” In: 14th ACM International Conference on\nFuture Energy Systems. 2023, pp. 114–126.\n[124]\nXiao Wang. “Ensuring safety of learning-based mo-\ntion planners using control barrier functions.” In:\nIEEE Robotics and Automation Letters 7.2 (2022),\npp. 4773–4780.\n[125]\nKeenon Werling et al. “Fast and feature-complete dif-\nferentiable physics for articulated rigid bodies with\ncontact.” In: arXiv preprint arXiv:2103.16021 (2021).\n[126]\nRonald J Williams. “Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning.” In: Machine learning 8 (1992), pp. 229–256.\n[127]\nJin-Long Wu et al. “Enforcing statistical constraints\nin generative adversarial networks for modeling\nchaotic dynamical systems.” In: Journal of Computa-\ntional Physics 406 (2020), p. 109209.\n[128]\nChris Xie et al. “Model-based reinforcement learning\nwith parametrized physical models and optimism-\ndriven exploration.”\nIn:\n2016 IEEE international\nconference on robotics and automation (ICRA). 2016,\npp. 504–511.\n[129]\nZhaoming Xie et al. “Feedback control for cassie with\ndeep reinforcement learning.” In: 2018 IEEE/RSJ In-\nternational Conference on Intelligent Robots and Systems\n(IROS) (2018), pp. 1241–1246.\n[130]\nJie Xu et al. “Accelerated policy learning with par-\nallel differentiable simulation.” In: arXiv preprint\narXiv:2204.07137 (2022).\n[131]\nMengjiao Yang and Ofir Nachum. “Representation\nmatters: Offline pretraining for sequential decision\nmaking.” In: International Conference on Machine\nLearning. PMLR. 2021, pp. 11784–11794.\n[132]\nYibo Yang and Paris Perdikaris. “Conditional deep\nsurrogate models for stochastic, high-dimensional,\nand multi-fidelity systems.” In: Computational Me-\nchanics 64.2 (2019), pp. 417–434.\n[133]\nYujie Yang et al. “Model-Free Safe Reinforcement\nLearning through Neural Barrier Certificate.” In:\nIEEE Robotics and Automation Letters (2023).\n[134]\nMingsheng Yin et al. “Generalizable Wireless Nav-\nigation\nthrough\nPhysics-Informed\nReinforcement\nLearning in Wireless Digital Twin.” In: arXiv preprint\narXiv:2306.06766 (2023).\n21\n[135]\nMustafa Z Yousif et al. “Physics-guided deep re-\ninforcement learning for flow field denoising.” In:\narXiv preprint arXiv:2302.09559 (2023).\n[136]\nPeipei Yu, Hongcai Zhang, and Yonghua Song. “Dis-\ntrict cooling system control for providing regula-\ntion services based on safe reinforcement learning\nwith barrier functions.” In: Applied Energy 347 (2023),\np. 121396.\n[137]\nZhaocong Yuan et al. “Safe-control-gym: A uni-\nfied benchmark suite for safe learning-based con-\ntrol and reinforcement learning.” In: arXiv preprint\narXiv:2109.06325 (2021).\n[138]\nXinglong Zhang et al. “Barrier Function-based Safe\nReinforcement Learning for Formation Control of\nMobile Robots.” In: 2022 International Conference on\nRobotics and Automation (ICRA). 2022, pp. 5532–5538.\n[139]\nPeng Zhao and Yongming Liu. “Physics informed\ndeep reinforcement learning for aircraft conflict reso-\nlution.” In: IEEE Transactions on Intelligent Transporta-\ntion Systems 23.7 (2021), pp. 8288–8301.\n[140]\nQingye Zhao, Yi Zhang, and Xuandong Li. “Safe\nreinforcement learning for dynamical systems using\nbarrier certificates.” In: Connection Science 34.1 (2022),\npp. 2822–2844.\n[141]\nTianqiao\nZhao,\nJianhui\nWang,\nand\nMeng\nYue.\n“A Barrier-Certificated Reinforcement Learning Ap-\nproach for Enhancing Power System Transient Sta-\nbility.” In: IEEE Transactions on Power Systems (2023).\n[142]\nWenxuan Zhou, Lerrel Pinto, and Abhinav Gupta.\n“Environment Probing Interaction Policies.” In: Inter-\nnational Conference on Learning Representations. 2018.\n[143]\nSimon Zimmermann et al. “Puppetmaster: robotic\nanimation of marionettes.” In: ACM Transactions on\nGraphics (TOG) 38.4 (2019), pp. 1–11.\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2023-09-05",
  "updated": "2023-09-05"
}