{
  "id": "http://arxiv.org/abs/2309.02712v1",
  "title": "Unveiling the frontiers of deep learning: innovations shaping diverse domains",
  "authors": [
    "Shams Forruque Ahmed",
    "Md. Sakib Bin Alam",
    "Maliha Kabir",
    "Shaila Afrin",
    "Sabiha Jannat Rafa",
    "Aanushka Mehjabin",
    "Amir H. Gandomi"
  ],
  "abstract": "Deep learning (DL) enables the development of computer models that are\ncapable of learning, visualizing, optimizing, refining, and predicting data. In\nrecent years, DL has been applied in a range of fields, including audio-visual\ndata processing, agriculture, transportation prediction, natural language,\nbiomedicine, disaster management, bioinformatics, drug design, genomics, face\nrecognition, and ecology. To explore the current state of deep learning, it is\nnecessary to investigate the latest developments and applications of deep\nlearning in these disciplines. However, the literature is lacking in exploring\nthe applications of deep learning in all potential sectors. This paper thus\nextensively investigates the potential applications of deep learning across all\nmajor fields of study as well as the associated benefits and challenges. As\nevidenced in the literature, DL exhibits accuracy in prediction and analysis,\nmakes it a powerful computational tool, and has the ability to articulate\nitself and optimize, making it effective in processing data with no prior\ntraining. Given its independence from training data, deep learning necessitates\nmassive amounts of data for effective analysis and processing, much like data\nvolume. To handle the challenge of compiling huge amounts of medical,\nscientific, healthcare, and environmental data for use in deep learning, gated\narchitectures like LSTMs and GRUs can be utilized. For multimodal learning,\nshared neurons in the neural network for all activities and specialized neurons\nfor particular tasks are necessary.",
  "text": "1 \n \nUnveiling the frontiers of deep learning: innovations shaping diverse domains \n \nShams Forruque Ahmed1,*, Md. Sakib Bin Alam2, Maliha Kabir1, Shaila Afrin1, Sabiha Jannat \nRafa1, Aanushka Mehjabin1, Amir H. Gandomi3,4* \n \n1 Science and Math Program, Asian University for Women, Chattogram 4000, Bangladesh  \n2 Data Science and Artificial Intelligence, Asian Institute of Technology, Chang Wat Pathum Thani 12120, Thailand \n3 Faculty of Engineering & Information Technology, University of Technology Sydney, NSW, 2007, Australia \n4 University Research and Innovation Center (EKIK), Óbuda University, 1034 Budapest, Hungary \n \n*Corresponding authors: shams.ahmed@auw.edu.bd, shams.f.ahmed@gmail.com (Shams Forruque Ahmed); \ngandomi@uts.edu.au (Amir H. Gandomi) \n \nAbstract \nDeep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, \nrefining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data \nprocessing, agriculture, transportation prediction, natural language, biomedicine, disaster management, \nbioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it \nis necessary to investigate the latest developments and applications of deep learning in these disciplines. However, \nthe literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus \nextensively investigates the potential applications of deep learning across all major fields of study as well as the \nassociated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, \nmakes it a powerful computational tool, and has the ability to articulate itself and optimize, making it effective in \nprocessing data with no prior training. Given its independence from training data, deep learning necessitates massive \namounts of data for effective analysis and processing, much like data volume. To handle the challenge of compiling \nhuge amounts of medical, scientific, healthcare, and environmental data for use in deep learning, gated architectures \nlike LSTMs and GRUs can be utilized. For multimodal learning, shared neurons in the neural network for all activities \nand specialized neurons for particular tasks are necessary. \n \nKeywords: Deep learning; Deep learning architecture; Natural language processing; Computer vision; Deep neural \nnetwork  \n \n1. Introduction  \nDeep learning is a method for constructing computational models composed of numerous processing layers in \norder to investigate and learn the demonstrations of multiple abstraction data. Utilizing these pathways, improvements \nhave been made in various sectors, including audio-visual technology, better recognition and detection, genomics, \nproteomics, biomedicine, drug discovery, environment, and security. Deep learning unravels and identifies the \nunderlying structures within massive datasets utilizing several algorithms, such as back-propagation, to learn and \napply changes to given conditions as a machine would [1].  Deep learning exhibits advantages over earlier machine \nlearning and artificial intelligence algorithms that lack the ability to analyze natural raw data. By utilizing methods \n2 \n \nlike representation learning, a system is able to take raw data as input and determine the necessary patterns for analysis. \nDeep learning takes into account many layers of representation data, each of which has an effect on the other. it is \nessential to examine the recent advancements and applications of deep learning in the potential fields, including audio-\nvisual data processing [2][3][4], agriculture [5][6][7][8], transportation prediction [9][10][11], natural language \n[12][13], biomedicine [14][15], disaster management [16][17], bioinformatics [18][19][20], healthcare [21][22], drug \ndesign [23][24], genomics [25], face recognition [26][27] and ecology [28][29], in order to understand the present \nstate of deep learning. Numerous surveys have been conducted on the analysis of medical image segmentation, \nreviewing different analysis techniques. While many types of analysis were discussed, the technical details around \nthem were not [30]. For instance, Litjens et al. [31] adopted a broad approach to medical image segmentation and \nreviewed many different subfields, which did not help to limit the topic. Hesamian et al. [32] shed light on the machine \nlearning and artificial intelligence techniques utilized in recent biomedical image research, focusing on their structure \nand methodology, and evaluating their benefits and limitations. However, they did not discuss the difficulties of black \nboxes and the inaccessibility of complex neural networks to human cognition.  \nSeveral studies [33–36] evaluated features of deep learning-based audio-visual data processing, such as text \nanalysis, activity recognition, and face recognition, without exploring the difficulties of black boxes or data privacy \nissues. Particularly, in the sampling of text and picture data, the processing of personally identifiable information is \nsubject to numerous legal constraints that are not adequately clarified. Sorting and analyzing biomedical data are \ncrucial challenges in the health industry since it is so complicated, varied, and dispersed. Health records, picture \nrecords, genomes, proteomics, transcriptomics, sensory data, and texts are just a few examples of the many types of \ndata produced by the biomedical sector. To accurately forecast, represent, analyze, and sort this data, deep learning-\nbased data mining approaches have proven to be both effective and fast. The usage of spiking neural networks (SNNs), \nwhich are constructed to emulate the information processing methods in biological systems, was reviewed by Pfeiffer \net al. [37]. However, the study failed to address the issues posed by these systems, such as the fact that massive \nvolumes of data input cause delays and fail to incorporate uncertainties. Many other reviews concentrating on deep \nlearning techniques for health data overlook the challenges posed by poor data quality and excessive data volume [38]. \nPrivacy problems of health data and genomic data are commonly disregarded in data mining approaches in \nbiomedicine and bioinformatics [39,40]. Yuan et al. [41] reviewed conventional neural network and deep learning \napproaches, which pertain to the development of environmental remote sensing procedures, and their applications in \nenvironmental monitoring. Nevertheless, the authors did not explain how to deal with issues like incomplete or \ninaccurate data, complex domains, inaccurate models, or the difficulty of using a multidisciplinary approach to merge \ndisciplines.  \nThe numerous research areas surveyed by Kamilaris et al. [42] applied deep learning approaches to address \nissues in agricultural and food production. The researchers found that deep learning can outperform conventional \nmethods in terms of providing accurate results. Another study [43] on plant diseases reported that deep neural network \n(DNN) models are effective in simulating a warning mechanism for specific diseases. Unfortunately, data collection, \ndata volume, data structure, as well as domain complexity and modelling were not addressed in either of these studies. \nMoreover,  these reviews barely explored the issue of data representation, despite the fact that reliable data is essential \n3 \n \nfor productive research. Based on a survey of the current literature, there has not been a comprehensive assessment of \nthe applications of deep learning that discusses the most prevalent obstacles associated with deploying deep learning \nin each of these domains across a wide variety of industries. Therefore, this review aims to combine a wide range of \ntopics, including information technology, biomedical research, health, the environment, and agriculture. In addition, \na review of common obstacles encountered in most disciplines is conducted while employing deep learning methods. \nThis report can be used as a single resource for studying and learning several applications of deep learning in a range \nof sectors, as well as for gaining an understanding of potential implementation issues with deep learning. This study \nwill facilitate the work of academics by collecting several evaluations of applications from various fields and the \ncommon deep learning challenges in these domains. \n \n2. Deep learning frameworks \nDeep learning techniques can be referred to as a representation learning method, which has many layers of \nrepresentation. These representation components can be created by building non-linear elements that can successively \nchange the degree of representation into a subsequent abstract level. Deep learning can perform very complicated \nfunctions by combining these modifications. There are many deep learning frameworks that are used to develop \nsolutions for complex problems, such as Caffe, Caffe2, MXNet, Computational Network Tool Kit (CNTK), Torch, \nTheano, and TensorFlow, which are discussed in this section [44]. The advantages and disadvantages of the DL \nframeworks are summarized in Table 1. \n \nCaffe \nCaffe stands for convolutional architecture for rapid feature embedding. It is a free and open-source DL \nframework that allows its users to explore complex structures. This library was created in C++ by the BVLC centre \nand is often used in Matlab and Python [45]. One study found that Caffe can handle dealing with more than forty \nmillion pictures each day only with a Titan GPU or K40 [46]. In Caffe, data are received by data layers. Because of \nthe standard picture formats of these data layers (.gif, .tiff, .jpg, .jpeg, .png, .pdf), Hierarchical Data Format (HDF5) \nand efficient databases are acceptable. Moreover, in another research, it was found that if Caffe can be incorporated \nwith cuDNN, then it enhances the productivity rate by 36% without using much memory [47]. \n \nCaffe2 \nCaffe2 is an advanced version of Caffe created by Yangqing Jia, the same person who created Caffe. After \nYangqing Jia began operating for Facebook, they collaborated with Facebook and NVIDIA to create the Caffe2 \nframework, which is based on Caffe [48]. Caffe2 addresses several of Caffe's shortcomings, including vast dispersed \ntraining, deployment in mobile systems, and additional equipment support for quantized processing. NVIDIA is an \nexcellent support system for Caffe2, which includes Python and C++ APIs [49]. This enables it to prototype and refine \nprojects fast. \n \nMXNet \n4 \n \nMXNet is a multilingual deep learning framework that has been made to achieve speed and flexibility [50]. Pedro \nDomingos and a team of scientists created MXNet, which is also an addition of DMLC (Distributed (Deep) Machine \nLearning Common). This framework is highly scalable with a small memory footprint. It can operate on a vast range \nof platforms, which include GPU systems and mobile devices and can perform a numerical calculation with a short \ncode of Python and R for dispersed networks and GPU. It supports both imperative and symbolic programming, which \nuse NDArray API and Symbol API, respectively [51]. \n \nComputational Network Tool Kit (CNTK) \nCNTK is a deep learning framework that features Python API and is built on top of C++ code, which was \ndeveloped by Microsoft [52]. This framework portrays neural networks as a sequential computational process. CNTK \ncan readily combine some of the prominent architectures like convolutional neural networks (CNNs), Feed-forward \nDNNs, and recurrent neural networks (RNNs/LSTMs). C# and BrainScript are supported by CNTK, which both offer \nincreased and minimal APIs for simplicity of usage and versatility. In a study, CNTK was evaluated using a system \nof several GPUs in a thoroughly linked four-layer neural network [53] and showed to outperform Caffe, TensorFlow, \nTheano, and Torch [54]. \n \nTorch/PyTorch \nTorch is a DL framework written in the Lua programming language. A tensor or an array is used in many of the \nfunctions in the torch. These functions include memory exchange, indexing, slicing, and resizing [55]. The torch was \ndesigned by Facebook in 2017 and employs dynamic graphics to manage variable-length deliverables. PyTorch is the \nPython version of Torch [56]. It is written in Python, C, and CUDA and features acceleration libraries from Intel and \nNVIDIA. This feature has aided PyTorch's rapid growth and adoption among academic communities. \n \nTheano \nTheano is a Python deep learning framework that acts as a compiler for numerical expressions and allows \ndevelopers and users to evaluate their mathematical process via NumPy's syntax [57]. This package was created at the \nlab of the University of Montreal. Some packages have been designed to increase the capabilities of Theano’s, such \nas Pylearn2 [58], Blocks [59], Lasagne [60], and Keras [61]. Although Theano’s development team actively developed \nand expanded its infrastructure, its development ended in 2017. \n \nTensorFlow \nTensorFlow is a deep learning framework that employs an individual data flow graph to define all mathematical \noperations to provide exceptional throughput [62]. It is an open-source framework that was developed by the Google \nbrain team. TensorFlow creates huge computational networks, wherein every node in the graph refers to a \nmathematical function and the edges indicate node interaction [63]. This data flow graph specifically conducts \ninteraction between a large part of the computational system, allowing for concurrent execution of separate \n5 \n \ncalculations or the deployment of numerous devices to run partition operations [64]. TensorFlow constructs and \nexecutes these graphs using APIs from multiple programming languages, including Python, C++, and Java.\n6 \n \nTable 1. Advantages and disadvantages of the DL frameworks, with brief descriptions of each \nDL frameworks \nBrief description \nAdvantages \nDisadvantages \nCaffe \nCaffe represents convolutional architecture \nfor rapid feature embedding. It is a freely \navailable DL framework that may be used \nto analyze complicated networks. \n− The code is ideally suited for research due to its \nflexibility. \n− Caffe nicely functions with CNN architecture. \n− It allows the adjustment of training models without \nwriting perfect code. \n− Caffe does not have good documentation. \n− It has poor and cumbersome performance in \nbig neural networks and RNN architecture. \n− Its development has become slow. \nCaffe2 \nCaffe2 is an improved variant of Caffe. It \nfixes a number of issues with Caffe, \nincluding scalability, portability, and the \nlack of hardware support for quantized \nprocessing. \n− Caffe2 is a cross-platform deep learning framework. It \ncan be used in mobile networking systems or edge \ncomputing systems. \n− Caffe2 is said to be supported by Amazon, Intel, \nQualcomm, and NVIDIA because of its scalability and \nrobustness in operation. \n− Compatibility with Caffe2 can be \nchallenging for new users. \n− It does not support dynamic graph \ncomputations. \n \nMXNet \nMXNet is a fast and flexible deep learning \nframework developed for several languages. \nIt has a small memory footprint while still \nbeing extremely scalable. \n− It supports high-level programming languages such as \nC++, R, Python, Scala, JavaScript, Perl, Go, Julia, \nMatlab, and Wolfram. \n− It supports dynamic graph computations. \n− Open Neural Network Exchange (ONNX) format also \nworks in MXNet, which makes it simple to switch \nbetween other DL frameworks and libraries. \n− It is not compatible with some APIs. \n \nCNTK \nAs a deep learning framework, CNTK is \nwritten in C++ and supports a Python API. \nCNTK makes it simple to combine popular \nnetwork architectures including CNNs, \nFeed-forward DNNs, and RNNs/LSTMs. \n− CNTK provides dependable and superior functionality. \n− It's compatible with Azure Cloud, which is supported by \nMicrosoft. \n− The control and use of resources are both efficient. \n− It is hard for beginners. \n− As it’s significantly new, it does not have vast \ncommunity support. \n \n7 \n \nTorch/PyTorch \nThe Torch framework for deep learning was \ndeveloped in Lua. \nMany Torch operations depend on either a \ntensor or an array. \n− Torch contains a lot of effective extensions and support \n− It provides GPU support that is both quick and effective. \n− It is research-friendly. \n \n− Only the smaller scale projects can be run on \nthis \n− Lua has become a mid-level programming \nlanguage thus it is not popular anymore. \nTheano \nTheano is a numerical expression compiler \nfor the Python deep learning framework \nNumPy, enabling both programmers and \ndata scientists to assess the efficacy of their \nown mathematical procedures in the context \nof the framework. \n− Theano is a cross-platform framework and open source. \n− It provides support in GPU and mathematical calculations \ncan be done faster. \n− It makes RNNs efficient because of symbolic API. \n \n− No longer under development. \n− Low-level API. \n− Theano is tough to utilize explicitly for \nconstructing deep learning models. \n \nTensorFlow \nThe TensorFlow deep learning system \nexpresses all mathematical operations using \na single data flow graph, resulting in \nextremely high throughput. The Google \nBrain team developed this open-source \nplatform to help advance artificial \nintelligence. \n− TensorFlow is an open-source, popular and swiftly \nchanging framework available. \n− It has a powerful numerical library that acts as the \nfoundation for deep learning research. \n− Efficiently work with numerical expressions which have \na multidimensional array. \n− It has great computational versatility across machines \nand large datasets. \n− A low-level API makes generating DL \nmodels tough. \n \n \n8 \n \n3. Recent advancements and applications of deep learning \nDeep learning has addressed challenges that were considered to be impossible a few years back. Specifically, it \nhas been successful in handling issues that conventional machine algorithms failed to do; such as deep learning has \nrevolutionized image and video analysis where traditional methods often struggle with variations in lighting, angles, \nand backgrounds. Because of its exceptional data handling feature, it has piqued the interest of professionals who are \nbombarded with all kinds of data. The advancement and application of deep learning (Fig. 1) have increased over the \nyears. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 1. Applications of deep learning in potential sectors  \n \nNatural language processing is one of the fields that has benefited significantly from deep learning, which is \nused in the various domains of natural language processing like the translation of audio and machines [65]. In 2015, \n9 \n \nGoogle debuted the word lens identification engine based on deep learning. The best feature of a word lens is that it \ncan read the texts instantaneously and convert them into the target language [66]. Another field where deep learning \nis being immensely used is transportation. As the world population is increasing, a smart transportation system is in \ndemand. In the transportation sector, deep learning is being used in Destination Prediction [67], Demand Prediction \n[68,69], Traffic Flow Prediction [70,71], [72], Travel Time Estimation [73,74], Predicting Traffic Accident Severity \n[75,76], Predicting the Mode of Transportation [77], Trajectory Clustering  [78,79], Navigation [80,81], Demand \nServing [82,83], and Traffic Signal Control [84,85]. Implementation of DL in the bioinformatics domain has increased \nsubstantially over the years. In this area, DL has been mostly used to predict the structure of proteins [86,87], gene \nregulations and expressions [88,89], precision medicine [90,91],  biomedical imaging [92,93],  localization of cells \n[94,95], clustering [96,97], classification of proteins [98,99] and so on. Techniques of  DL have become one of the \nkey elements in many multimedia systems [100]. Systems like Convolutional Neural Networks (CNNs) have \npresented noteworthy outcomes in various tasks from the real world, such as the detection of objects and processing \nof visual data, including images and videos. There have been numerous advancements, including the development of \nevent detection from sports videos [101], new techniques like recurrent convolution networks (RCNs) for video \nprocessing [102], the use of intermediate CNN layers [97], and Gated Recurrent Unit for improving the sparsity and \nlocality in modules. \n \n3.1. Speech and audio processing \nSpeech and audio have been proven as two important modes of communication in human history. Deep learning \ntechnologies have made significant progress in the field of speech processing. Unlike traditional speech enhancement \ntechniques that rely on a statistical model, deep learning models are data-driven. Deep learning is among the most \nimportant technologies nowadays and, thus, merits its own study. Mapping-based and masking-based are the types of \nDL approaches that are used in speech enhancement [103]. Moreover, DL methods for speech emotion recognition \nhave several advantages over conventional machine learning methods, such as identifying complex systems and no \nrequirement for manual extraction of the features [104]. Also, deep learning has the ability to handle a large number \nof unlabeled data and a proclivity for obtaining low-level characteristics from provided raw data. Furthermore, many \nacademics have abandoned traditional signal processing approaches for sound production due to the emergence of \ndeep learning algorithms. Deep learning techniques have accomplished eloquent voice generation, audio textures, and \nmelodies from simulated instruments [105]. Deep neural networks have shown excellent progress in audio processing. \n \n3.1.1. Speech enhancement \nSpeech enhancement is being utilized in video conferencing, hearing aids, microphones, audiovisual screening \nand so on. In order to have a good speech enhancement system, pattern mining is needed [106]. Processing background \nnoise is considered a good speech augmentation method. There are different types of conventional machine learning \napproaches available to filter and eliminate the additional noise from the voice signals. In recent years, DL methods \nhave proven beneficial for speech augmentation. The deep neural network (DNN) is the most general technique that \nhas been used to remove noises from large datasets [107]. However, research shows that DNN struggles to adapt to \n10 \n \nnew voice corpora in low signal-to-noise ratio (SNR) settings [108]. Accordingly, an alternative solution is to utilize \na lower frameshift in short-time speech processing that can substantially improve cross-corpus generalization. \nNoise suppression is a well-developed field of signal processing [109], yet it still heavily relies on the precise \ntweaking of estimator techniques and variables. In a study, hybrid deep learning technologies were proposed to \neliminate noise from the background [110]. In this technique, a great emphasis is placed on minimizing complications \nwhile obtaining highly improved speech. This method proves more viable than a standard lowest mean squared error \nspectrum estimator.  \nIn order to achieve a significant speech enhancement performance, a large DNN is needed, which is computer- \nand memory-intensive [111]. Therefore, making such a speech enhancement system is challenging to deploy because \nof timing constraints and minimal resources of hardware. One study introduced two compression models for DNN-\nbased speech enhancements that mainly include quantization to minimize model size based on clustering, sparse \nregularization, and repeated pruning [112]. Moreover, experimental results indicate that this method decreases the size \nof four distinct models by a substantial amount without affecting their enhancing performance. \n \n3.1.2. Speech emotion recognition (SER) \nSpeech emotion recognition (SER) refers to identifying the underlying emotion in a text or speech regardless of \nthe semantic content. Implementation of deep learning in SER has made it possible to detect emotion in a speech in \nreal-time situations [113]. Several researches have aimed to enhance the SER technique using deep learning \nmethods. In a study, a dynamic SER identification system was developed based on aural and visual data processing \n[114]. In this system, a total of 88 characteristics (Mel Frequency Cepstral Coefficients (MFCC), filter bank energies \n(FBEs)) are employed to minimize the earlier retrieved features using principal component analysis (PCA) [115]. A \ncomparison of deep learning flow versus conventional machine learning flow techniques for SER is shown in Fig. 2.  \n \n11 \n \n \nFig. 2. Traditional machine learning vs deep learning flow process (modified from [116]) \n \nSurekha [117] utilized GMM classifiers in the SER system to detect five emotions by applying the Berlin \nEmotional Speech database. An acoustic feature, which is called the MFCC with the Teager Energy Operator (TEO), \nwas used as a prosodic. This approach was employed in another study where 13 MFCC special features derived from \nthe audio data were utilized to identify seven emotions [118]. In this method, the Logistic Model Tree (LMT) method \nwas used, which has a 70% accuracy rate. Nevertheless, there are some challenges related to SER that needs to be \naddressed. For instance, an issue with emotional speech datasets is annotation ambiguity [119]. In a specific task like \npicture classification, a car will always be labeled as a car, yet in an emotional discourse, asserting something strongly \nmay be categorized as anger. This bias in categorization complicates the work while also limiting the ability to \ncombine datasets and generate emotional supersets. \n \n3.2. Transportation prediction \nRecently, researchers have been using deep learning techniques in the intelligent transportation system. \nPreviously, analytical or statistical methods were used to solve problems in this domain. These enhancements have \naided traffic management and planning, raised transit road safety and security, reduced maintenance costs, improved \npublic transportation and ride-sharing firm performance, and propelled driverless car development to a new level. \nZheng et al. [120] presented a DL model that automatically extracts fundamental properties of traffic flow data using \nhybrid and multiple layer architectures. They designed an attention-based ConvLSTM module that extracts spatial \nand short-term temporal features using a CNN and LSTM network. By automatically employing various weights to \n12 \n \nflow sequences at different periods, the attention method was correctly constructed to discern the relevance of the \nsequences at several intervals. In order to investigate long-term temporal features, the researchers proposed a \nbidirectional-LSTM (Bi-LSTM) architecture that extracts regular (daily and weekly) periodical features in order to \ngrab the variance trend of traffic flow in prior and posterior orientation. However, only relatively small and simple \nroad networks were considered in the study, which are more complicated and large scale. As a result, CNN and Bi-\nLSTM methods might be unable to completely utilize traffic flow's complex and dynamic properties. \nA multistep prediction model built on attention mechanism-based Convolution Neural Network-LSTM was \nproposed by Vijayalakshmi et al. [121]. To increase model accuracy, the suggested technique employs the spatial and \ntime-based features of data, which are retrieved by CNN and LSTM. This method facilitates the detection of short-\nterm traffic characteristics (e.g. speed), which is essential for calculating future flow value. Weather and other \nconditions like accidents and road closure data could be taken into account for the reliability of the suggested \ntechnique. Abdollahi et al. [122] proposed a method for forecasting travel time using a multi-step method, which \nbegins with the removal of both temporal and spatial outliers. To increase prediction accuracy, reduce overfitting risks, \nand achieve a more robust learner, the researchers employed a deep stacked autoencoder to present lower dimensions’ \nfeatures. To forecast travel times, a multi-layered perceptron was trained. While the proposed technique showed to be \ncapable of capturing traffic dynamics in general, it was not successful when heavy snow was present or another \nuncommon occurrence that had a substantial influence on travel times prevailed. To overcome this problem, several \ndeep architectures and representation learning algorithms could be implemented, followed by a performance \ncomparison to find an effective method. \n \n3.3. Agriculture \nThe successful application of deep learning has spread to many fields, including agriculture. Some specific \nproblems in this field that can be explored with deep learning methods include land cover categorization, plant \nclassification, fruit counting, and crop type classifications. In a survey, Kamilaris and Prenafeta-Boldú [123] found \nthat deep learning outperforms frequently-used image processing approaches in terms of accuracy in the agricultural \ndomain. Many methods based on deep learning have been developed recently for identifying leaf stress in plants. \nRamcharan et al. [124] offered a cassava vegetable-based transfer learning approach. An Inception V3 model was \nused on a sample of 15,000 photos, and the approach outperformed more common models based on machine learning, \nsuch as KNN and support vector machine (SVM), with an accuracy of 93%. For certain diseases (CBSD, BLS, GMD), \nusing the leaflet rather than the full leaf increased diagnostic accuracy. However, using whole leaf photos enhanced \naccuracies for other diseases like CMD and RMD. \nInstead of evaluating the entire leaf, Arnal Barbedo  [125] employed a CNN to categorize specific diseases and \nspots on plant leaves. This revealed a number of illnesses that all damage the same leaf. In addition, the author used \ndeep learning to recognize specific lesions and spots in 14 plant species. The models in this study were trained using \na pre-trained GoogLeNet CNN. How many pictures of disease symptoms would be needed for the neural network to \nlearn their characteristics could not be determined by the study. In all situations, a few hundred photos appeared to be \nsufficient to produce credible findings, but this quantity must be approached with caution. \n13 \n \nLin et al. [126] applied an RGB sensor along with a UNet-CNN to classify and identify cucumber powdery \nmildew-affected leaves. Because the loss value of the affected pixels in this study was lower than the non-affected \npixels, the experiment presented a loss function (Binary-Cross-Entropy) to tenfold the loss values. The semantic \nsegmentation CNN model segmented the sick powdery mildew on cucumber leaf pictures with a mean pixel accuracy \nvalue of 96.08%. However, it is necessary to take images in a controlled setting rather than in an open field. \nFurthermore, a lack of appropriate amount and diversity of the datasets, where symptoms produced by other conditions \nwere not included, may hinder the effectiveness of DL methods. \nDL approaches have recently been implemented in smart fish farms. A deep learning method was demonstrated \nto better differentiate variations in traits, classes, and the environment, allowing it to extract target fish attributes from \na picture captured in an uncontrolled underwater environment [127]. On the well-known LifeCLEF-14 and LifeCLEF-\n15 fish datasets, CNN outperformed conventional techniques with classification accuracies of over 90% [128]. General \ndeep structures in the experiment should be fine-tuned to increase the efficacy to identify vital information in the \nfeature space of interest, thus reducing the requirement for vast volumes of annotated data.   \n \n3.4. Natural language processing (NLP) \nDeep learning algorithms are rapidly being implemented in NLP research. Popular ML algorithms, such as SVM, \nand logistic regression, trained on sparse and high-dimensional features have been the foundation of ML approaches \nfor NLP issues for decades. On a range of NLP tasks, neural networks based on dense vector representations have \nrecently surpassed classical models. This trend has been fueled by significant advancements in word embedding and \nDL approaches [65]. \n \n3.4.1. \nParaphrase identification \nThe task of identifying whether two statements written in natural language have semantic meanings that are \ncomparable is known as identification. When two sentences have the same meaning, they are referred to be \nparaphrases. This methodology is a fundamental approach in a variety of data mining approaches that has tremendous \npotential in a variety of domains, including plagiarism detection, machine translation, and others. Various techniques \nof paraphrase identification have been presented, which are distributed into two categories: similarity-based and \nclassification-based methods [129]. Alzubi et al. [130] applied a collaborative adversarial network (CAN) in order to \ndiscover the problem of paraphrasing. A common feature extractor is included in CAN to help increase the association \nbetween phrases in the recurrent neural network model. Within a word pair, the extractor looks for related features. \nThe integration of adversarial networks with collaborative learning mostly enhances the generator and discriminator \nworking together. The model outperforms the baseline MaLSTM model as well as several baseline approaches. Hunt \net al. [131] investigated a wide range of machine learning approaches for modeling the problem of paraphrase \nidentification and alternative input encoding schemes. Since RNN outperformed the other models, the researchers \ndeveloped a method based on the LSTM, an RNN variation. RNN works by using the output of past time sequences \nto create the output for next time sequences. This fits in NLP because it can be illustrated as a sequence of tokens, and \nthe appearance of the next token is generally influenced by the preceding tokens.  \n14 \n \n \n3.4.2. Machine translation \nNLP's most well-known application is machine translation, which entails translating texts from one to another \nlanguage using mathematical and computational procedures. The translation is challenging for humans since it \nrequires knowledge of morphology, syntax, and semantics, as well as a full understanding and evaluation of cultural \nsensitivity for the target language and its associated communities. Ahmed et al. [132] advocated that attention methods \ncan be used to encode a language from input to output instead of employing a huge number of recurrent and \nconvolutional layers. The following three principles motivate the use of \"self-attention\" methods over traditional \nlayers: decreasing the complexity of calculations needed for each layer; limiting sequential training steps; and lastly, \nreducing the length of the path between input and output and its influence on the learning of long-range relationships, \nwhich is important in several sequencing tasks.  \nJohnson et al. [132] presented that a single, basic (but huge) NN could be applied for translating a number \n(minimum 12) of several languages into each other. This NN automatically recognizes the source languages and uses \njust one input token to determine the output languages. When numerous language tokens are supplied, the method \nshowed to be capable of interpreting multilingual inputs and producing mixed outputs, at least in part, sometimes even \nin languages similar to but not identical to those chosen. The performance of such zero-shot translation is frequently \ninsufficient to be practical, as the basic pivoting strategy quickly outperforms it.  \nA deep-attention technique was introduced by Zhang et al. [133] for a neural machine translation (NMT) system. \nThis model incorporates many stacked attention layers, each of which pays attention to a corresponding encoder layer, \nto advise what should be transmitted or repressed from the encoder layer, resulting in learnt distributed representations \nthat are suitable for high-level translation tasks. English-French, NIST-English-Chinese, and WMT14 English-\nGerman translation skims were used in this experiment. The technique could be implemented on other tasks like \nsummarization and could adapt to more complex attention models. Aside from the work listed above, other academics \nhave suggested a variety of high-performance designs. In the English-German and Chinese-English translation \nsystems, Zhang et al. [134] introduced the Variational NMT approach, which has a novel approach to modeling \ntranslation problems. The results demonstrated that it performed better than the baseline technique (NMT). Fast-\nForward Connection for RNN (LSTM) was developed by Zhou et al. [135], which allows for a deeper network in \nimplementation and, hence, greater performance. \n \n3.4.3. Sentiment analysis \nUsers today create massive volumes of data in a large and dynamic fashion, as the number of internet \ntechnologies grows. The number of people who use social media on a regular basis is continuously expanding. In this \nsense, sentiment analysis appears to be a useful technique for automating the extraction of insights from user-generated \ndata. Deep learning algorithms have recently been presented for several sentiment analysis applications, with the \nexisting results [136]. For extracting aspect opinion target expressions (OTEs), Al-Smadi et al. [137] suggested \ncombining BiLSTM with a CRF method for classifying, as well as for classifying aspect-sentiment polarity. An aspect-\nbased (LSTM) was used, where the aspect opinion target expressions were applied as attention expressions. A two-\n15 \n \nstage attention structure was created by Ma et al. [138] that involves paying attention to both the words that make up \nthe target expression and the complete phrase. The author also used extended LSTM to construct a common sense \nmodel for target aspect-based emotion detection, which can use external knowledge. However, these methods were \nunable to represent several aspects of sentences, and the explicit location contexts of words were not investigated.  \nYuan et al. [139] suggested a Domain-Attention-Model (DAM) by utilizing an attention mechanism to model \nfeature-level tasks for multi-domain emotion categorization. Domain and sentiment modules are the two components \nthat makeup DAM. The domain module uses BiLSTM to forecast the domain where texts belong, and the sentiment \nmodule uses another BiLSTM with an attention method to choose the key aspects connected to the domain. To forecast \nthe polarity of the sentences, the vector derived from the sentiment module is input to a soft-max classifier. In contrast \nto earlier multi-domain sentiment classification algorithms, the proposed methodology can pull out the most \ndistinctive features from the hidden layers, reducing the required number of labeled samples. \nTo predict multimodal attitudes in tweets, F. Chen et al. [140] suggested a weakly-supervised multimodal DL \n(WS-MDL) method. To produce multimodal prediction scores and sentiment consistency scores, the method employs \nCNN and dynamic CNN. The classic text-based sentiment/emotion analysis technique has evolved into multimodal \nsentiment analysis compound models due to the huge quantity of data available on social media in multiple formats, \nsuch as videos, audio, and photographs for expressing emotion on these sites. In future work, the order of the emotion \nicon levels might be further investigated, and this may be included as a constraint to the proposed WS-MDL technique. \nFor sentiment categorization, researchers are also mixing several DL algorithms. Combining LSTM with CNN, \nHuang et al. [141] suggested an architecture with one CNN layer and two LSTM layers placed on top of CNN. \nSpecifically, the CNN is used to collect important local text properties, which are then input into a two-layered LSTM \nmodel. The researchers utilized a pre-trained word2vecmodel. The suggested model can generate a sentence \nrepresentation for sentence categorization by extracting context-dependent features. Hassan and Mahmood [142] \ndeveloped the ConvLstm architecture, which merges LSTM and CNN for categorizing brief texts on top of the \nword2vec. Local information loss may be reduced, and long-term dependencies can be captured using the suggested \ndesign.  \n \n3.4.4. Question answering \nQuestion answering (QA) is the process of extracting words, phrases, or sentences from documents that are \nrelevant. In response to a request, QA delivers this information in a logical manner. The approaches are similar to the \nsummarizing methods that are presented in the next section. Wang et al. [143] utilized an attention-based LSTM to \nassociate the questions with answers containing paragraphs. By mapping the full text, a self-matching attention process \nwas employed to develop the machine representations. The position and boundaries of responses were predicted using \npointer networks. The networks utilized attention pooling vector representation of passages, and also the words were \nexamined to model the crucial tokens or phrases. Modeling more than one aspect simultaneously with the attention \nmechanism would be an interesting addition to the experiment. Using Wikipedia as the knowledge source, Mozannar \net al. [144] investigated the topic of open domain factual Arabic QA and proposed an approach composed of A BERT \n16 \n \ndocument reader and hierarchical TF-IDF document retriever. They also introduced a dataset (ARCD). However, \nARCD’s questions were created with certain paragraphs in mind; without that context, they might seem ambiguous. \nConvolutional Neural Networks were compressed with LSTM in [145] in an attempt to speed up processing. For \nthe decomposition of fully connected layers in LSTM and CNN, the researchers suggested applying different \ndecomposition algorithms and regression techniques. Tensor Regression layers replace the Flattening and Fully \nConnected layers in the final section of the method. The Tensor Contraction layer compresses the flow of features \nbetween the layers to further compress the parameter. Determining the rank is an NP hard problem in the low-rank \ndecomposition, and their method is still constrained in this area by inserted hyper-parameters. Yu et al. [146] used a \ntree-LSTM method for capturing a language's linguistic structures. The study created a semantic tree for each question \nin the dataset, with every node referring to a single LSTM unit and the root node representing the sequence. To boost \nreasoning capacity, this approach can divide problems into several logical phrases. The representational ability of the \nnetwork could be improved in future.  \nNarasimhan and Schwing  [147] suggested an end-to-end approach that combines image and question \ncharacteristics acquired from a CNN and LSTM, using a multi-layered perceptron (MLP). They gathered this \ninformation from other sources in order to better respond to the question. The fact embedding and MLP output are \nthen put into a scoring function. The function calculates the cosine similarity between the inputs, in this case, to \nguarantee that the fact is useful in answering the image-question pair. The approach might be used on unstructured \ninformation sources, such as online text corpora, in addition to structured knowledge bases. \n \n3.4.5. Summarization \nAutomatic Text Summarization (ATS) is a significant topic due to the vast volume of textual data that is rapidly \ngrowing nowadays on the internet and some other sources. Three ATS approaches are available: abstractive, \nextractive, and hybrid [148]. To create a summary, the extractive technique chooses and incorporates the most \nsignificant sentences from the source text. In contrast, the abstractive approach transforms the input content into an \nintermediate representation before generating a summary using unique terms. The hybrid approach incorporates both \nthe extractive and abstractive processes. L. Chen and Nguyen  [149] proposed an ATS technique for summarizing sign \ndocuments based on a Reinforcement Learning (RL) technique and an encoder-extractor network architecture's RNN \nmodel. A sentence-level encoding approach is used to choose the key features, then summary sentences are retrieved. \nHowever, the method has a chance of suffering from large variances since it uses an approximation in the RL method \ntraining objective function. \nClustering, unsupervised NNs, and topic modeling were used to create an Arabic summarization approach in this \nstudy [150]. The researchers also presented ensemble learning models and neural network models to combine the \ninformation produced by the topic space. Specifically, the ELM-AE method and k-mean approach were used to \naccomplish document clustering on a large sample of Arabic documents. The LDA technique was implemented to \ndetermine the topic space associated with each cluster, then the discovered subject space was used to generate a \nnumerical document representation. For learning unsupervised features from texts, this format was utilized as the input \nfor various NN and ensemble methods. For the variation of the information contained in the final summary, the learnt \n17 \n \ncharacteristics were utilized to rank phrases by following a graph-based model, and key phrases were picked by \napplying the redundancy removal components. A few more unsupervised NN methods, for instance, stacked-auto-\nencoder and RBM, could be included in future studies to improve the robustness of the suggested technique. \nAnother study combined text summarization embedding spaces (TSS), sentiment analysis embedding space \n(SAS), and opinion summarizer modules [151]. SAS employs an LSTM-based RNN to reap the benefits of sequential \nprocessing. In order to improve word-level embedding, TSS applies a wide range of statistical and linguistic \nknowledge variables to extract a relevant set of sentences from a large number of texts. Additionally, TSS uses RBM. \nOSM is divided into two steps, namely sentence categorization and sentence identification, which work together to \nprovide a relevant summary. This was the first experiment in which RBM and RNN–LSTM were paired for predicting \nsentiment polarity. One of the limitations of the proposed method is that it fails to obtain the difference between active \nand passive sentences. \n \n3.5. Biomedicine \nA plethora of biological and medical data, including information about medical imaging, biological sequences, \nand protein structures, has been accumulated in recent decades due to the advancements in high-throughput \ntechnology. Consequently, deep learning has been extensively applied to biomedical data in biomedicine. For \ninstance, CNNs are widely employed in the field of biomedical image processing due to their extraordinary capacity \nto assess spatial features. CNNs have a lot of potential in omics analysis [40] and the study of biological signals [152], \neven though sequencing data employing CNNs are not very common. RNN-based architectures, on the other hand, \nare designed for sequential data and are more frequently utilized for transcriptome analysis [153] and in dynamic \nbiomedical signals [154]. Hence, the focus on deep learning is increasing in the field of biomedical information, and \nit is possible that each paradigm may soon find new implementations. \n \n3.5.1. Prediction of protein structure \nProtein structural analysis now relies heavily on cryo-electron microscopy (cryo-EM) [155], which has made \natomic resolution possible. However, on all but the purest density maps that have a resolution of less than 2.5 \nangstroms, estimating the structural trace of a protein remains difficult [156]. A  DL model proposed by Si et al. [157] \npredicted the alpha carbon atoms throughout the core structure of proteins using a series of cascaded convolutional \nneural networks (C-CNNs). Specifically, C-CNN is a revolutionary deep learning architecture consisting of several \nCNNs, each of which predicts a particular feature of a protein's structure. In order to create a comprehensive prediction \nmap, this model combines the predictions of alpha carbon atoms, core structure, and secondary structure elements. \nUsing a semantic image classifier, the cascaded CNN was trained on a large number of generated concentration maps. \nWith only a suggested threshold value needed per protein concentration map, this procedure was automatic and \nsignificant. To create the primary core trace with alpha carbon placements, a customized tabu-search path walking \nalgorithm was applied. The alpha-helix secondary structural elements were further enhanced by a helix-refinement \ntechnique. Finally, to create full protein structures, a unique quality assessment-based creative approach was employed \nto successfully align protein sequences into alpha carbon traces, using 50 trial maps with resolutions between 2.6 and \n18 \n \n4.4 angstroms. In terms of the proportion of connecting alpha carbon atoms, the proposed model produced core traces \nthat were more comprehensive (88.9%), thus outperforming the Phoenix-based structure construction method with an \naccuracy of around 66.8%. By including additional protein structural details in the C-CNN or training the networks \nwith experimental data, further study may enhance this research area. \nOne of the most important issues in computational biology is the identification of microRNAs (miRNAs), which \nare critical in post-transcriptional expression.  The typical length of miRNAs is between 20 and 23 base pairs [158]. \nBecause it can be challenging to differentiate between miRNA-encoding regions from other non-coding and pseudo \nsequences that are identical in length, the majority of earlier research suggested employing candidate miRNAs for \nreliable identification. There have been several proposals for traditional machine-learning-based categorization \ntechniques, but they suffer from constrained performance and required constant feature engineering.  \nIn order to understand sequence patterns and folding structure, Park et al. [159] introduced deepMiRGene that \nutilizes RNNs, particularly LSTM networks. The most significant contribution of this suggested approach is that it \ndoes not involve any rigorous manual feature development. By utilizing end-to-end DL, this approach eliminates the \nneed for extensive domain expertise and instead utilizes simple preprocessing. However, it is challenging to use an \nLSTM network right away due to the palindromic secondary structure of microRNA. To solve this problem, \ndeepMiRGene employs a novel learning method in which the secondary structure from the input sequence is \nsegmented into front and back streams.  Additionally, deepMiRGene performed better than all other options in terms \nof sensitivity and specificity on testing datasets and with higher sensitivity accuracies of 89%, 91%, and 88% in the \nmethod's three datasets. Even though there were significant disparities between the features of the various species, \ndeepMiRGene also performed best when using cross-species data, thus demonstrating the potential for identifying \ninherent traits. \n \n3.5.2. Genomic sequencing and gene expression analysis \nLarge-scale gene expression profiling has been frequently utilized in the characterization of cellular states in \nresponse to various illness circumstances, genetic mutations, and so on [160] Creating a compilation consisting of \nthousands of samples of gene expression profiles is still quite extortionate, despite the fact that the expense of entire \ngene expression profiles has been gradually declining. The computational strategy used by the LINCS program, \nhowever, currently relies on linear regression, which restricts its accuracy because it cannot detect complicated \ncomplex correlations among gene expressions.   \nChen et al. [161] suggested using a DL technique called DGEX to predict the expression of key genes from the \nlandmark gene profile expression. The model was trained using the microarray-based GEO dataset, which contains \naround 111,100 expression profiles, and its performance was evaluated against other approaches. With a 15.33% \nproportional improvement, deep learning greatly exceeded LR in terms of the average absolute error of mean through \nall of the genes. Deep learning outperformed LR in 99.97% of the key genes as per the gene-gene comparison \ninvestigation. A separate RNA-Seq-based GTEx dataset with around 20,000 expression profiles was also used to test \nthe performance of the trained model. \n \n19 \n \n3.5.3. Medical image classification and segmentation \nDeep learning strategies have been utilized in studies segmenting cerebral tumors as a result of their achievement \nin general image analysis disciplines, including the classification of images [162] and semantic identification [163]. \nIn particular, CNNs were deployed for the Multimodal Brain Tumor Image Screening Challenge to identify scans of \ncerebral tumors [164]. Conditional random fields (CRFs) and fully CNNs (FCNNs) in a single framework were \nproposed by Zhao et al. [165] to develop a unique deep learning-based identification approach for cerebral malignancy. \nThe model was created to produce a malignancy segment that results in accurate identification of both appearance and \nconcentration. Instead of employing CRFs as a step after FCNN post-processing, the authors utilized CRF-RNN to \nconstruct CRFs, making it simple to train both the frameworks as a single deep network. In three steps, the combined \ndeep learning model was trained by employing segments and pixel patches. FCNNs were trained in the first step using \nimage patches, while picture slices were used to train the next CRF-RNN in the second stage. In the third stage, the \nentire network was tuned using image slices. The unification of FCNNs and CRF-RNN, which acquired an accuracy \nof 88%, could increase segment resilience to model training factors, such picture patch size and training picture patch \ncount, as per the experimental data. Therefore, using 3D CRF as a step of post-processing could enhance malignancy \nidentification performance.  \nGliomas are the most prevalent and deadly type of cerebral tumor, with an extremely low survival probability in \nthe maximum grade [166]. Planning the timeline of treatment is consequently essential for expanding the quality of \nlife for cancer patients. The assessment of these tumors is frequently done using magnetic resonance imaging (MRI), \nbut the volume of data generated by MRI makes it impossible to manually segment the images in a timely manner. \nAccordingly, this restricts the use of profound quantitative assessments in clinical practice. The enormous spatial and \nfunctional heterogeneity among cerebral malignancies makes automatic segmentation a difficult task, hence \ndependable and automated segmentation approaches are needed.   \nAn automated segmentation technique based on CNN was suggested by Pereira et al. [167] to explore kernels \nfor glioma segmentation in the images derived from MRI. As a result of the network's smaller number of weights, \nusing small kernels enabled the design of more intricate architectures while also helping to prevent overfitting. The \nintensity normalization as an essential step for pre-processing was also utilized. While this is considered uncommon \nin CNN-based segmentation approaches, it proved to be quite successful when combined with data augmentation for \nsegmenting brain tumors in images collected from MRI. By simultaneously receiving the top spot for the full, core, \nand expanding areas in the DSC measure (88%, 83%, and 77%), the proposed methodology demonstrated to be \nlegitimate in the Cerebral Tumor Classification Challenge database. \n \n3.6. Bioinformatics \nThe natural innate immune components known as antimicrobial peptides (AMPs) are commonly the focus of \nnovel therapeutic developments due to the increment of antibiotic-resistant bacteria. Nowadays, wet-lab researchers \nfrequently use machine learning techniques to find promising applicants. For instance, Veltri et al. [168] proposed a \nDL model to identify antibacterial activity by generating a neural network model that uses a core sequence composition \nand contains CCN and RNN layers. A comprehensive training and testing dataset was utilized that incorporates the \n20 \n \nmost recent antibacterial peptide information. In contrast to the current approaches, the proposed novel deep neural \nnetwork (DNN) classifier performed better AMP identification. Through the implementation of DNN and RNN layers, \nthe dependency on prior feature generation has decreased. Besides, a reduced-alphabet representation demonstrated \nthat adequate AMP identification can be preserved using 9 different kinds of amino acids based on the embedded \nparameters. The DNN model performed the best in terms of MCC, auROC, and ACC and achieved a superior accuracy \nof 91.01% compared to other models, such as AntiBP2 (89.37%) and CAMP Database RF model (87.57%). Thus, the \nsuggested model eliminates the dependency on domain experts for feature creation by employing a deep network \nmodel that automatically extracts expert-free characteristics. \nNowadays, it is possible to measure DNA methylation at a single-cell level due to recent technological \nadvancements. To facilitate genome-wide analysis, strategies to predict unknown methylation patterns are necessary \nbecause existing procedures are constrained by insufficient Chg. coverage. Angermueller et al. [169] developed \nDeepCpG, which is a computational method focusing on DNN to make a prediction regarding the methylation stages \nof a single cell and model the origins of DNA methylation dispersion. DeepCpG utilizes relationships between patterns \nof DNA sequences, stages of methylation, and nearby CpG sites both within and between cells. The extraction of \nuseful characteristics and model training parts are not separated in the proposed method, rather DeepCpG is built on \na modular architecture and develops predicted sequences of DNA and patterns of methylation through data-based \nlearning. Notably, DeepCpG performed better than RF trained DNA and CpG characteristics. After training both the \nRF and DeepCpG models using only DNA sequence information, the highest relative improvements in the accuracy \nof 80% and 83% were achieved, respectively.  \nThe detection of enhancer-promoter interactions (EPIs) is vital for human development. However, the majority \nof computational techniques currently being used depend on a range of genetic data that are unfortunately often not \naccessible, particularly for a specific cell line [170]. Sequence-based computational approaches, as an option, are only \nlikely to be utilized at the genome size. A novel deep learning technique name EPIVAN was introduced by Hong et \nal. [171] that allows lengthy EPIs to be predicted only from genomic sequences. Pre-trained DNA vectors were utilized \nto encode regulators and promoters in order to investigate the essential sequential properties. Subsequently, a one-\ndimensional convolutional model and deep neural units were used to identify both local and global characteristics to \nmake a prediction regarding EPIs in different cell lines. The effectiveness of EPIVAN was compared to that of the \nneural network models, such as SPEID, EPIANN, and SIMCNN. Each model applied the identical test and training \nset to every cell line. On six cell lines, the proposed model’s results and the four predictors were provided in terms of \nAUPR and AUROC. EPIVAN demonstrated the strongest AUROC of any model, with an accuracy ranging from \n96.5% to 98.5%.  According to test findings on six cell lines, EPIVAN outperformed the existing models, showed the \ncapability of being utilized as a pre-trained model for further transfer learning, and demonstrated strong transfer \nability. \nPrediction of the functionality of enzymes is an important step in constructing new enzymes and diagnosing \ndisorders connected to enzymes, which is a major task in bioinformatics [172]. Several studies have generally \nconcentrated on predicting the mechanism of the monofunctional enzyme. However, the amount of multi-functional \nenzymes is continuously increasing, necessitating the development of novel computational techniques [173]. Zou et \n21 \n \nal. [174] proposed mlDEEPre, a deep learning network specifically designed to predict the functionality of multi-\nfunctional enzymes. Using an auto label issuing threshold and a new transfer function linked with the interaction \nbetween several labels, mlDEEPre could predict multi-functional enzymes reliably and quickly. The proposed multi-\nlabel model surpassed all other approaches, as it correctly predicted 97.6% of all the observed primary categories in \nthe test dataset with an SD of 0.27. The performance of SVM-NN with an accuracy of 84.7% seemed somewhat better \nthan that of mlDEEPre (82.6%) and GA (80.8%), despite the fact that SVM-NN was worthy of predicting such \ninfrequent class labels generated by unbalanced training samples. Comprehensive tests further demonstrated that \nmlDEEPre outperformed the other techniques in determining the kind of functional enzyme that has been utilized, as \nwell as the primary class prediction throughout many parameters. As mlDEEPre and DEEPre are flexible, mlDEEPre \ncould be effortlessly merged into DEEPre, allowing the enhanced DEEPre to handle different functional predictions \nwithout the need for human interference. \n \n3.7. Disaster management  \nIn terms of disaster or natural calamity management, uncertainty, lack of resources in the affected areas, and \ndynamic environmental changes are the major characteristics of natural catastrophes. The inability to predict outcomes \nindicates that catastrophic effects on persons and property during several disasters cannot be foreseen with a level of \naccuracy that is reasonable [175]. Big data is a technology paradigm that enables researchers to efficiently analyze \nenormous amounts of data that are made accessible by current practices [176]. Making the most of big data is possible \nusing a variety of technological and scientific strategies and equipment. Recent advancements in big data and IoT \ntechnologies open up a wide range of opportunities for disaster management systems in order to obtain leading \nassistance, guidance, as well as better observations and ideas for precise and suitable decision-making.  \nIn order to resolve catastrophic challenges, Anbarasan et al. [177] introduced concepts and strategies for \nidentifying flood catastrophes based on IoT, big data, and CDNN. Big data derived from the flood catastrophe was \nused as the source of the input data first. Afterwards, the Hadoop Distributed File System (HDFS) was used to decrease \nthe high frequency data. After excluding high frequency data, the data were preprocessed via missing value \napproximation and a normalizing technique. A conjunction of attributes was utilized to construct the pattern centered \non the pre-processed data. In the final step, the created parameters were sent into the CDNN classifier, which \ncategorized them into two categories as the probabilities and infeasibility of a flood emerging. The performance of \nthe suggested system was analyzed in terms of precision, accuracy, recall, F-score, specificity, and sensitivity in \ncomparison to the existing systems, DNN and artificial neural network (ANN). The comparison findings clearly \ndemonstrate that the CDNN method has a greater level of accuracy than the present approach with 93.2% accuracy, \n92.23% precision, 90.36 % recall, and 91.28% F-score with 500 data points utilizing CDNN. The results described \nabove can be improved using DNN as well as ANN models. In conclusion, the detection system performed better than \nother top methods currently in use, and in the future, the work presented here could be improved using IoT devices \nthat have even longer sensor ranges at lower costs and using cutting-edge algorithms at each stage of the flood \nidentification process.  \n22 \n \nDisasters caused by fire do have a negative impact on the environment, society, and economy. Early fire detection \nand an automated response are vital and essential for disaster management schemes in order to minimize these \ndamages. Fire detection at an early stage in disaster management systems while monitoring public spaces, woods, and \nnuclear power plants can prevent ecological, monetary, and social harm [178]. The movements of entities with a \nretardant appearance and varied atmospheric factors make early detection a hard proposition [179]. Subsequently, an \nimproved accuracy algorithm that reduces the number of false alerts in the aforementioned conditions is required. In \norder to accomplish this, Khan et al. [180] investigated deep neural networks and developed a prominent architecture \nfor the early detection of fire through surveillance for efficient disaster management systems. Another preferable \ncriterion was to notify the disaster management system after a successful fire detection and include the representative \nframes. A system of flexible prioritization was developed for the monitoring system's camera nodes, taking into \naccount the contents they were capable of visualizing. Two datasets were the major focus in this experiment in which \nthe first dataset indicated further improvement by increasing the accuracy from 93% to 94% and decreasing false \npositives from 0.11% to 0.9%. Despite producing false negatives of around 21%, the approach maintained a stronger \nbalance, enabling the methodology to more accurately achieve fire identification. The findings for the second dataset \n[181] were gathered using a different set of metrics, including recall, precision, and F-score, in order to properly \nconduct the performance. Results produced by using deep features and optimizing the fire prediction model are \ncompared and contrasted, showing that the detection approach and model are feasible with a maximum score of 82% \nfor precision, 98% for recall, and 89% for F-score.  Despite evaluating the sustainability of the proposed approach \nutilizing noise intrusions, scaling, and rotations, each image comprised of fire and showed a high accuracy ranging \nfrom 89% to 99%. Considering the potential of such occurrences occurring under surveillance, the results indicate that \nthe CNN-based model could detect fires at an early stage in a range of circumstances, even when images are blurry. \nThe relevance of the framework for successful fire disaster management was supported by experimental results, which \nalso confirm the high accuracy of the fire detection technique compared to cutting-edge methods. \nOver the past few years, social media networks have contributed to managing natural catastrophes. In order to \ninterpret better messages and extract relevant information from social media, text data mining techniques employing \nstandard machine learning approaches have been constructed. These techniques tend to be distinct and are challenging \nto generalize for different categorizations. Therefore, considering the crisis management efforts related to hurricanes, \nManzhu et al. [33] investigated the ability of a convolutional neural-based deep learning model to classify the trending \ncatastrophic topics from Twitter. SVM and LR are two conventional machine learning techniques that were evaluated \nagainst the logistic regression of the CNN model. The outcomes of the experiment demonstrate that CNN models \nconsistently outperformed SVM and LR models in terms of accuracy for both types of assessment scenarios. \nMoreover, the CNN classifier surpassed the classification methods of SVM (63-72%) and LR (44-60%), achieving an \naccuracy of up to 81% among all datasets. In order to categorize tweets posted during a later event, the evaluation was \ncarried out using CNN based on Twitter data from prior events. Results of the experiment demonstrated that while \nSVM and LR's accuracy dramatically decreased, CNN maintained a steady performance. This suggests that the CNN \nmodel could be trained in advance using Twitter data from previous events to categorize new occurrences for \n23 \n \nsituational awareness. However, CNN took longer to learn than SVM and LR because there were more parameters to \nconsider, which makes it difficult to employ CNN for web-based learning. \nMultimedia big data present several opportunities and research prospects due to the quick and explosive \nproliferation of digital data throughout social media and the Internet [182]. Due to its effects on society and \ngovernment, disaster management programs have become very popular recently. The multiple correspondence \nanalysis (MCA) method has been widely employed in a variety of data mining jobs because it is efficient in capturing \ncorrelations between variables and classes [183]. To improve the final classification outcomes and reduce the \ncomplexity across various data modalities, MCA has been utilized for multimodal data fusion. For example, Pouyanfar \net al. [184] introduced a multimedia big data framework built on cutting-edge deep learning methods, aiming to \nmanage disasters by analyzing and mining content. The proposed fusion model, which is based on the MCA technique \nand considers the correlations between data modalities and final classes, was applied to combine the results of both \nmodels. On the basis of the disaster dataset that was gathered, the suggested multimodal framework was assessed and \ncontrasted with a number of cutting-edge single modality and fusion methodologies. In comparison to the baseline \napproaches, the results showed that both the visual model and fusion model were effective. On this difficult dataset, \nan accuracy of 73% was attained by the suggested MCA-based fusion for the final multi-class classification. \n \n3.8. Drug discovery and toxicology \nEmerging contaminants (ECs), which pose a serious risk to human health due to their detrimental effect on the \nendocrine system, include over eighty thousand endocrine-disrupting chemicals (EDCs). To determine the possible \nimpacts of EDCs, numerous in vitro techniques have been developed, such as signaling pathway studies, ligand-\nbinding protein, reporter gene experiments, and cell proliferation [185]. While in vitro methods are often more \neconomical and quicker than in vivo trials, it is still impractical to analyze thousands of molecules in a reasonable \namount of time [186]. This has resulted in the usage of alternative computational methods experiencing exponential \ngrowth. To anticipate the toxicological effects of chemicals, the QSAR model is a possible substitute for in vitro \ntechniques.   \nA  DL-QSAR model was evaluated by Heo et al. [187] to give predictions regarding the impacts of EDCs on the \nendocrine system, particularly estrogen receptor (ER) and sex-hormone binding globulin (SHBG). For the \nclassification and forecasting of probable EDCs, DL-QSAR models were created and three distinct DL algorithms, \nnamely SAE, DBN, and DNN, were employed. The suggested models’ performance was assessed using validation \nmetrics by comparing the classification prediction models with traditional machine learning classifiers, such as LR, \nSVM, and MLR. Results showed that the DL-QSAR algorithm outperformed traditional machine learning based \nQSAR models. Accordingly, the DNN-QSAR model adequately described the vast majority of EDCs' qualitative \nresponses to tests. Compared to the LR and SAE-QSAR models (accuracies of 86.49% and 89.91%, respectively), the \nDNN-QSAR model achieved an accuracy of 90%. As a result, DNN was more effective for evaluating qualitative \nresponses since it could translate dense chemical identifiers into multidimensional space regions. Additionally, by \novercoming multicollinearity and overfitting issues, DNN-QSAR exhibited great performance in the field of \n24 \n \ncomputational chemistry. As a result, it was determined that DL can effectively utilize the qualitative characteristics \nof the EDCs. \nEffective methods for assessing potential EDCs are majorly required. An evaluation platform was established by \nZhang et al. [186] established an evaluation platform by employing three different machine learning models, such as \nSVM, linear discriminant analysis (LDA), and classification and regression trees (CART), for the purpose of \nidentifying EDCs through estrogen receptors. The model was modified using a total of 440 compounds, and 109 new \ncompounds were added for the screening of EDCs. Their predictive capabilities were evaluated by contrasting the \nscreening results with those anticipated by the models derived from classification. The most accurate model was found \nto use an SVM classifier, which correctly identified agonists at an accuracy level of 76.6% and antagonists with an \naccuracy of 75% on the test set, including an average predicted accuracy of 75.2%. The overall projected accuracy \nconfirmed by the screening of the EDC assay was 87.57%, illustrating the effectiveness of a synthetic alert for EDCs \nwith ER agonistic or antagonistic actions.  \nThe fundamentals of organic chemistry are retrosynthesis and reaction prediction. Retrosynthetic analysis is a \ntechnique in which targeted molecules are converted into simpler parent compounds [188]. This process has two \nrelated tasks: reaction prediction, which predicts how a group of reactants will react to produce; and planning the best \npossible sequence of reaction prediction steps with the least amount of expense, energy, and waste [189]. However, \nreactivity conflicts occur when reaction rules overlook the molecular environment, which is why they frequently fail. \nSegler et al. [190] constructed a neural network model to predict which transformation rules will be most frequently \napplied to the involved molecule. The model exhibited an accuracy of 95% in retrosynthesis and 97% in response \nprediction. During retrosynthesis, the single-layer neural network was 78% accurate and had an MRR of 87%, \nindicating that the model is extremely capable of ranking the real reactions. Thus, the rule-based expert system and \nthe logistic regression are both significantly outperformed by the neural network models.  \nDrugs are often the cause of detrimental results, such as accidents, injuries, and enormous medical expenses. \nClinicians can build suitable treatment plans and make effective judgments with the aid of accurate drug-drug \ninteraction (DDI) predictions. Recently, numerous AI-based methods for DDI prediction have been proposed. \nHowever, the majority of currently used techniques give less consideration to possible connections between DDI \nprocesses and other multidimensional data, such as targets and enzymes. A multimodal DNN for DDI events \nprediction was suggested by Lyu et al. [191] in order to generate drug multimodal representations in MDNN. To \nexamine the complementary aspects of the drug's heterogeneous representations, a multimodal neuronal layer was \nalso developed. Several multi-class classification evaluation metrics, including accuracy, F1 score, precision, and \nrecall, were used to assess the prediction performance. The proposed model MDNN achieved the most stable \nperformance and outperformed DDIMDL by 0.08% on accuracy, 7.1% on F1 score, 1.5% on precision, and 10% on \nrecall. The MDNN model achieved an accuracy of around 99%, according to the comparative study with other models \nused in different journals, such as DDIMDL [192]. The MDNN model's superior performance can be ascribed to its \nexploration of both the cross-modality embedding representations of the heterogeneous data and the drug topological \nintegrating expressions in the drug network. This effectively demonstrates how the use of structural information and \n25 \n \nmultimodal features can increase the prediction accuracy of drug-drug interaction irrespective of current drugs or \nnewly-launched FDA-approved drugs and provides a solid, trustworthy basis for research on DDI prediction. \nDrug co-prescription can be safer and more productive if the effects of drug-drug interactions (DDIs) are \naccurately predicted [193]. There is still potential for improvement in prediction performance despite the many \ncomputational methods that have been presented to anticipate the impact of DDIs [194]. These methods attempt to \nmake it easier to find these interactions in vivo or in vitro. A deep learning model was proposed by Lee et al. [195] to \nmore precisely predict the impact of DDIs as well as to employ autoencoders and a feed-forward deep network to \npredict the therapeutic effects of DDIs. The experiments utilizing just GSP or TSP or combined GSP and TSP did not \nproduce tests with satisfactory classification accuracy. However, integrating TSP and GSP improved classification \naccuracy to 97-97.5%.  Additionally, the suggested model outperformed standard techniques like SVM (80-83%) and \nRandom Forest (75-91%). The results showed that TSP and GSP improved prediction accuracy in comparison to SSP \nalone, and that the autoencoder outperformed PCA in reducing the parameters of each profile. Hence, the proposed \ndeep learning model provided a more precise prediction of DDIs and their therapeutic effects.  \n \n3.9. Partial differential equations (PDEs) \nFor the purpose of multi-scale modeling, model predictive control, and computational complexity, Raissi et al. \n[196] suggested a physics-based neural network that is effective in incorporating any genuine physiological principles \nthat regulate a particular set of data. The resulting methodologies demonstrated a number of encouraging findings for \na broad range of computer science issues, opening the way for giving deep learning the potency of mathematical \nphysics. The goal was to derive the whole spatial and temporal Schrödinger equation response. For the representation \nof the underlying function, a five-layer DNN with 100 neurons in each layer and nonlinear activation was used. In \ngeneral, the neural network needed to have enough approximate capacity to handle the function's predicted complex \nnature. The findings showed the position of the two data snapshots utilized for training as well as the precise solution. \nThe formation of the solution between the two given snapshots is significantly different due to the complicated \nnonlinear dynamics of the equation. Whether or not the training data was polluted with noise, the method was still \nable to reliably identify the ambiguous parameters despite these variations and the substantial time delay between the \ntwo training images.  \nUncertainty assessment and surrogate modeling are commonly viewed as supervised learning problems for PDE \nsystems, with varying data pairs being used during the training process [197]. The generation of such emulation is \nproblematic for small data, posing challenges for deep learning algorithms developed for the big data context. Even \nthough these types of models have demonstrated excellent predictive potential in high dimensions, they are incapable \nof addressing the data limits that the PDE model suggests. The system of the physical model equations was included \nin the loss functions using a methodology presented by Zhu et al. [198]. Without any labeled training data, the \nsubsequent physics-constrained deep learning algorithms were able to predict outcomes on a level with data-driven \nmodels while conforming to the limitations of the given challenge. In order to solve PDEs, generate alternative models, \nand quantify uncertainty, the latter study used convolutional encoder-decoder neural networks along with a conditional \ncirculation generative model. In order to solve PDEs, the methodology contrasted convolutional decoder networks \n26 \n \n(CDNs) with completely connected dense networks using input data from a non-linear Darcy law correction. After \nresolving SPDE, a comparison between data-driven surrogate DDS and surrogate PCS was made. Last but not least, \na probabilistic surrogate with a reverse KL formulation was constructed. Two optimizers, L-BFGS and ADAM, were \nalso utilized. For PCS, 300 epochs were produced in iterations, while for DDS, 200 epochs were produced along with \n8-30 pictures. Therefore, the suggested physics-restricted alternative sources regularly outperformed data-driven \nalternative sources in terms of generalization performance.  \n \n3.10. Financial fraud detection \nFinancial fraud is a type of thievery whereby a business entity or anyone illegally takes money or assets to profit \nfrom it. Since the advent of new technologies, there has been a massive rise in financial fraud involving all aspects of \nthe business world. For instance, financial fraud victims in the United States individually lost an average of $1,090, \naccumulating over $3.2 billion in 2017 [199]. The selection procedure for practitioners is frequently known as \nsafeguards of fraud analysis, assessment, or detection by information security specialists and associations. Some \nmethods for detecting anomalies include decision trees, logistic regression, SVM, and others. However, these methods \nare constrained as they are supervised algorithms that rely on labels to determine transaction validity [200].  \nSeveral studies [201–203] revealed that developing measures based on fraud estimations has been an infuriating \nprocedure that is extortionate and futile when tackling composite non-linear data, has inefficient memory utilization, \nlengthy calculation, and generates abundant false alarms. Financial fraud detection is still a strenuous task because of \nthe constant changes in fraudulent behavior, the absence of a mechanism to track fraudulent transactions' information, \ncertain limitations of the existing detection techniques, and highly skewed datasets [204]. Therefore, optimization of \nprevious procedures and novel approaches are necessary to increase fraud detection rates [205,206]. As a result, deep \nlearning, a subset of modern artificial intelligence, is one of the promising techniques that has achieved recognition in \nrecent years. DL also facilitates computational models with multiple processing layers to adapt data representation to \ndifferent abstraction levels.  \nCredit card fraud is one of the most pervasive forms of financial fraud in which a hacker gains unauthorized \naccess to a legit card without the cardholder's consensus. The simultaneous expansion of e-commerce and the internet \nhas resulted in a significant increase in credit card use, which has resulted in an unusual increase in credit card theft \nin recent years. For instance, a report by the Boston Consulting Group showed that North American financial \ninstitutions lost $3 billion in 2017 due to credit card fraud. Despite industry attempts to combat economic fraud, the \nNilson Report site revealed that the global financial losses exceeded $24.71 billion in 2016 and $27.69 billion in 2017 \nattributed to credit card fraud [207]. This vast number of fraudulent occurrences may erode consumer confidence, \ndestabilize economies, and elevate people's living costs. As a result, several studies have applied deep learning to \ndetect credit card fraud. Heryadi and Warnars (2018) built many deep learning paradigms for credit card fraud \nidentification and inspected the implications of an imbalance between deceit and non-fraud data. They investigated \nthe impacts using CNN, CNN-LSTM Hybrid, and Long Short-term Memory (LSTM) models. Based on Area Under \nthe ROC Curve (AUC) as a performance metric, CNN secured the maximum value, thenceforth SLSTM and CNN-\n27 \n \nLSTM. However, the inequitable dataset could not help in using training accuracy as the primary criterion to identify \nthe best model.  \nMost studies have recently used LSTM networks to approach credit card fraud detection. The hidden units in the \nLSTM have feedback connections linked to discrete time steps, contrasting conventional feedforward neural networks. \nThis enables the learning of long-term sequence dependencies and the transaction label prediction premised on \nprevious transaction sequences. Benchaji et al. (2021) devised a fraud detection model using a sequence classifier that \nutilizes LSTM networks to track individual cardholders' behavior. LSTMs were intended to overcome vanishing and \nexploding gradients issues during conventional RNN training. All the changes made by the forget gate, input gate, and \noutput gate are stored in the LSTM unit's memory cells. The three gates control the incoming and outgoing data, and \nthe cell can store values for an indefinite amount of time. The proposed model relies on the Keras deep learning \nframework that enables fast experimentation with deep neural networks.  \nOver 80 million credit card transactions labeled as fraudulent or legitimate were used to assess the efficacy of \nthe deep learning process reported by Roy et al. [209]. This study assessed the performance of various DL algorithms \nbased on their parameters, class imbalance, scalability, and sensitivity analysis. The researchers used LSTM, Gated \nRecurrent Unit, RNN, and ANN in a distributed cloud computing environment. The findings confirm that the LSTM \ntechnique achieved the best performance and reveal that each of the four topologies' execution expanded with the \nincreased network size. However, the study failed to specify the conditions under which the model functions cease to \nameliorate, model sensitivity, and limit of the network size. \nAlghofaili et al. [204] initiated an LSTM-based fraud detection technique that strives to enhance current detection \ntactics while simultaneously increasing the detection accuracy of large amounts of complex data. The system was \ntested with a unique dataset of credit card frauds. Simultaneously, the outputs are analogized to an auto-encoder model \nand machine learning technologies in a pre-existing deep learning model to detect suspicious financial activities and \nalert the appropriate authorities, allowing them to take appropriate action. Although some machine learning techniques \nhave demonstrated promising upshots, they do not detect new patterns or deal with large amounts of data to improve \naccuracy after reaching a saturation point. LSTM worked brilliantly in the trials, reaching 99.95% accuracy in less \nthan a minute. The unique applicability of LSTM in prediction is ensured by prior experience and the correlation \nbetween prediction outputs and historical input. Many-state memory cells and gates are incorporated into the \nframework. Due to its vital role in ensuring that data is transmitted unmodified, the cell's state is at the core of the \ninformation-transfer process [210]. \nSeveral studies used other deep learning frameworks productively for a similar cause. To illustrate, Pandey [211] \nintroduced the H2O deep learning framework in credit card forgery detection. The pattern of the model enables \nmultiple algorithms to aggregate as modules, and their outputs can integrate to improve the final output accuracy. \nConcurrently, the efficiency of the algorithms rises as the dataset length grows while using this framework; thus, \nadding more algorithms with equivalent formats and datasets can elevate this model. Pumsirirat and Yan [212] \npropound an autoencoder-based DL model for credit card fraud identification. The model concentrates on fraud cases \nthat cannot be recognized through prior knowledge or supervised learning. Concurrently, the authors suggested a novel \nrestricted Boltzmann machine (RBM), which contains visible and hidden layers, and auto-encoder model for altering \n28 \n \nordinary transactions to detect anomalies. The output accurately depicted the root mean squared error, the area under \nthe curve, and the mean squared error with a 96.03% accuracy. \nAnother type of financial fraud is tax fraud, the malicious act of falsifying a tax return document to reduce \nsomeone's taxation. Low fiscal revenues are weakening governmental investment [213]. When it comes to the property \nacquisition tax, Lee [214] implemented a reliable sampling technique for tax-paying citizens. The data from 2,228 \nreturns were fed into autoencoder, a well-known unsupervised deep learning technique, to calculate the potential tax \nshortfalls for each return based on an estimate of the rebuilding faults. The sorted reconstruction ratings are compatible \nwith practical context, implying that the defects can be exploited to identify suspicious payers for auditing in a cost-\nefficient strategy. Utilizing the recommended strategy in real-world tax administration can reinforce the self-\nassessment acquisition tax system.  \nLópez et al. [215] researched tax fraud detection in the context of individual income tax filings. Neural networks \napplication enabled the taxpayers' segmentation and the possible assessment of a particular taxpayer attempting to \navoid taxes. The neural network outcome categorizes whether a taxpayer is deceitful or not as well as reveals a \ntaxpayer's proclivity for unethical activities. In other words, it classifies individuals based on their potential of \nconvicting and also calculates the propensity of tax fraud per taxpayer. The chosen model excelled over other tax \nfraud detection algorithms with an accuracy of 84.3%. It would be interesting to see this concept applied to additional \ntaxes in the future. \n \n3.11. Computer vision  \nTo a broader extent, computer vision is an interdisciplinary branch of computer science study that examines how \ncomputers can quickly recognize digital images and movies. The predominant processes include extracting, analyzing, \nand comprehending relevant information autonomously about a specific picture or a series of images [216]. Computer \nvision implies the algorithmic and theoretical roots for spontaneous ocular comprehension. The growth of computer \nvision is reliant on the computer technology system, focusing on image quality enhancement or image classification \n[217]. Researchers often use the terms interchangeably in the basic approaches because of the overlapping with image \nprocessing. However, the core aim of computer vision is to develop models, data extraction, and information from \nimages. On the other hand, image rectification deals with applying computational alterations to pictures, such as \nsharpening and contrast, among other things [218]. Deep learning algorithms recently tackled major computer vision \ntasks like face recognition, object detection, human pose estimation, action recognition, activity recognition, and fruit \ndefect detection [219]. \n \n3.11.1. Object detection \nObject detection has attracted massive attention as a vital function in computer vision, in which CNN has made \nsubstantial headway [220]. Object detection is, in essence, the first step in several computer vision applications, \nincluding logo detection, face recognition, pedestrian identification, and video analysis. One-stage detectors (YOLO \nand its variants) and two-stage detectors (region-based CNN (R-CNN)) are the two primary families of object \ndetection deep learning frameworks [221]. One-stage detectors do not need the cascaded zone classification step to \n29 \n \nmake qualitative predictions of objects on every position of feature maps. Two-stage detectors, on the other hand, \nhave a proposal generator that makes a handful of suggestions before feature extraction. When dealing with \ncomplicated models, deep neural architectures are superior to simple ones [222]. Using the weight-sharing principle, \nCNN is a feed-forward neural network. CNNs are not precise for minor data, but produce high accuracy for vast image \ndatasets [223]. However, to execute computer vision tasks, CNNs require large labeled datasets. It is a fusion of \nmultiple functions and integration that shows how they intersect. The CNN layered structure for object detection is \nshown in Fig. 3.  \n \n \nFig. 3. Layered CNN structure for object detection (reprinted with the permission of Elsevier from [224]) \n \nFor object detection, deep R-CNNs have been widely deployed. Ouyang et al. [225] suggested a deformable \ndeep CNN for generic object detection. This constrained pooling layer in the recommended deep design simulates the \ndeformation of object portions with geometric compensation and limitation. The researchers introduced diverse \nmodels by changing the training process, net topologies, and altering some vital components in the detection pipeline. \nThis strategy exceptionally optimized the model’s averaging efficacy. The proposed technique elevated the mean \naveraged precision by RCNN from 31 to 50.3, exceeding GoogLeNet, the winner of the ILSVRC2014, by 6.1%. The \nextensive experimental evaluation also provides a detailed component analysis. Doulamis and Doulami [226] explored \na deep model's representation capacities in a semi-supervised scenario. The method overcame deep learning's crucial \ndrawbacks by utilizing unsupervised data to construct the network, and then fine-tuning the data using a gradient \ndescent optimization process. Unsupervised learning appreciates more abstract representations to reduce the input \ndata, thus optimizing the model's stability, accuracy, and reliability. In addition, an adaptive approach allows \nmodifying the model dynamically to the digital photogrammetry conditions. \n \n3.11.2. Face recognition \nGiven the enormous diversity of face images present in the real world, face recognition is amongst the most \ncomplex biometrics attributes to use in open circumstances [227]. Face recognition structures are frequently comprised \nof several components, such as an image, face detection, face alignment, face representation, and face matching. \nResearchers have focused on specialized approaches for each face variation owing to the feature's sturdy complexity \nin unconstrained environments. Many new face recognition architectures have been presented in tandem with deep \nlearning advances, and some even come near to human performance. Extensive research with acceptable outcomes has \n30 \n \nbeen conducted on the issues of illumination, position, and expression [228]. However, when dealing with noisy \nphotos, the precision of most methods deteriorates dramatically. Humans can often have a hard time distinguishing an \nidentity from a seriously noisy face. \nZadeh et al. [229] designed a local model with convolutional specialists constraints (CMLs) for facial landmark \ndetection by adding a set of appearance prototypes of diverse poses and expressions. A pivotal part of the CE-CLMs \nis the Convolutional Experts Network (CEN), a new local sensor that blends neural structures with expertise in an \nend-to-end framework. The findings suggest that launching from iteratively-stated CEN network weights while \nexerting Menpo trained data alone does not yield competent outcomes. However, applying plain datasets of the \nCurriculum Learning paradigm [230] and switching to Menpo training data gave satisfactory results. Deng et al. [231] \nsuggested UV-Generative adversarial networks, a painstakingly built system that unifies global and local adversarial \ndeep CNNs to develop an identity-preserving facial UV closure framework for pose-invariant visual recognition. \nCombining posture augmentation during training and pose discrepancy depletion throughout testing, the protocol \nattained a state-of-the-art affirmation accuracy of 94.05%.  \nTo adversarially train the identity-distilled attributes, Y. Liu et al. [232] built an autoencoder system with \nminimal direction via facial identities. The model showed to be capable of generating identity-distilled features and \nextracting concealed identity-dispelled features that can preserve complementary knowledge, such as backdrop \nclutters and intra-personal variances. The primary limitation of deep learning approaches is that they must be learned \non extremely big datasets with enough variety to generalize to unknown samples. Recently, numerous large-scale face \ndatasets, including images of faces in the wild, have been made available to train CNN models [233,234]. The studies \ndemonstrate that neural networks may get trained as classifiers and can reduce dimensionality.  \n \n3.11.3. Action and activity recognition \nHuman activity recognition (HAR) plays a vital part in today's society because of its potential to assimilate \nextensive advanced information from raw sensor data about human actions and activities [36]. To identify sensor \nacquired data, early studies primarily applied naive bias, decision trees, SVM, and other classic machine learning \nmethodologies [235,236]. Researchers have recently transitioned from traditional handcrafting to deep learning \nmethods for HAR, especially with the advent and efficient application of deep learning algorithms. Handcrafted model \nrepresentation cannot tackle complicated cases due to their limitations. However, given the success of DL models in \nspeech recognition, NLP, image classification, and other domains, transferring them to the HAR field is a new research \nconcept for pattern identification [237,238].  \nOrdóñez et al. [239] reported a classifier that can classify 5 movements and 27 hand gestures using CNN and \nLSTM. The methodology surpasses deep non-recurrent networks by an average of 4% and outperformed some \nprevious findings by 9%. Results suggest that this model can be used with homogenous sensor modalities and can \nfuse multi-modal sensors for better execution. To reliably classify subjects or activities, Lin et al. [238] introduced a \nnovel iterative CNN technique using autocorrelation pre-processing rather than the conventional micro-Doppler image \npre-processing. An iterative DL framework was used in the proposed method to define and extract features \nautomatically. In addition to outperforming feature-based methods employing micro-Doppler pictures, the proposed \n31 \n \niterative CNNs, followed by random forests, also outperform classification methods employing various types of \nsupervised classifiers.  \nEven though the preceding models may distinguish human activities in general, the entire network structure is \nquite complex. Furthermore, the large number of parameters has a massive computational cost in these models, which \nis thus challenging to employ in situations where high real-time performance is required. Agarwal and Alam [240] \nimplemented a lightweight deep learning paradigm for HAR on a Raspberry Pi3. The overall accuracy of this model \non the WISDM dataset was 95.78% using a simplistic RNN incorporating the LSTM technique. Despite its excellent \naccuracy and simplicity, the conceptual framework was evaluated on a single dataset with only six activities, which \ndoes not guarantee that it is generalizable. To address the drawbacks, Xia et al. [241] introduced a unique deep LSTM-\nCNN for HAR that can extricate activity features and categorize them automatically using only a few factors. The \nnetwork was tested with three of the most often utilized publicly available datasets. The analysis showed that the \nnetwork not only has fewer parameters with great precision but also has fast convergence speed with sufficient \ngeneralization aptitude. In many computer vision applications, deep learning has outperformed older methodologies \ndue to the capacity of DL algorithms to learn characteristics from raw data, depleting the need for constructed feature \ndetectors and descriptors. \n \n3.11.4. Human pose estimation (HPE) \nThe goal of the HPE problem, which has existed for centuries, is to determine how well sensor inputs can be \nused to estimate human posture. The HPE system attempts to determine a human’s body position from still images or \nmoving video. It is an important research field because it involves various applications, including activity recognition, \naction detection, human tracking, movies, virtual reality, sports motion, video surveillance, human-computer \ncommunication, medical assistance, and self-driving analysis. HPE is challenging because of the wide variety of body \ntypes, self-obscuring stances, and complex environments that might occur from the great degree of freedom of \ninterconnected joints and limbs [242]. Toshev and Szegedy [243] initially tried to instruct an AlexNet-like DNN to \ndetermine joint points from complete pictures without utilizing any models or part detectors. A multi-stage refining \nregressor cascade architecture improved the performance and modified the cropped images from the previous step. \nSimilarly, Pfister et al. [244] used a string of concatenated frames to implement an AlexNet-like network to forecast \nthe human stance in films. The use of joints alone without the context is not robust; however, turning them into \nnumerical-joint positions from heatmap supervision monitoring can benefit both approaches. \nYang et al. [245] developed a Pyramid Residual Module (PRM) to increase DCNN stability across levels to swap \nthe Hourglass network's residual module. This method exhibited considerably enhanced performance compared to the \nearlier state-of-the-art procedures. Papandreou et al. [246] developed a box-free, multi-tasking ResNet-based network \nfor pose evaluation and occurrence classification. In real-time, the ResNet-based network can forecast joint heatmaps \nof all critical spots of all people, as well as their relative displacements. Then, the most confident detection is \ngrouped, adopting a decoding strategy focused on a tree-structured kinematic graph. Furthermore, the network \nacquired a greater average precision and above 5% absolute advancement over the former top-performing approach \n32 \n \non a similar dataset. Other studies [247–250] mainly focused on moderately constrained directions, such as 3D HPE, \nRGB-D-based action identification, body parts-based HPE, and monocular-based HPE model-based HPE.  \n \n3.12. Ecology \nThe endless data stream offers ecologists a new challenge: finding or developing the analytical models required \nfor data extraction from the massive amounts of video feeds and streaming photos [251]. On the other hand, obtaining \nusable footage in maritime zones to attain acceptable computing performance provides a unique set of obstacles \ncompared to terrestrial situations. Variable water purity, obstruction due to schooling fish, complicated background \nformations, and diminished light with increased depth are some environmental complications that might interfere with \nclear footage in aquatic ecosystems [252]. Though certain elements may affect the image and video quality, deep \nlearning techniques have evinced effectiveness in a variety of marine implementations. Deep learning algorithms are \ncurrently pirouetted around marine environments to automate the classification of certain species. Not surprisingly, \nthe most common use of deep learning is identifying species from recordings of sounds or images or videos. These \ninvestigations already cover a wide variety of organisms, from bacteria and protozoa to plants, insects, and vertebrates, \nboth living and extinct, and from microscopic to planetary levels [210,253,262,254–261].  \nDeep learning's capacity to comprehend features from data is what deems it so powerful. Unsupervised \nalgorithms have no specified output and are frequently used as a systematic appliance to find patterns in data, minimize \ndimensions, and classify groupings [251]. Ditria et al. [263] emulated the speed and precision of deep learning \napproaches opposite to human counterparts for verifying fish abundance in underwater video clips and photos to test \nits adequacy and relevance. They designed three prototypes using Mask R-CNN, an object detection framework to \nlocate the aspired species called luderick (Girella tricuspidata). In single image test datasets, the machine topped \nmarine specialists by 7.1% and citizen scientists by 13.4% and outperformed in video datasets by 1.5% and 7.8%, \nrespectively. These results confirm that deep learning is a better tool to evaluate abundance with stable results and is \nportable across survey locations than humans. \nAnother vast potential of deep learning is to detect disease symptoms similar to existing applications in the \nmedical sector. For example, CNNs can identify tree defoliation and crop illnesses. Rather than accounting for each \nfeature independently through explicit modeling, Kälin et al. [264] built a joint distribution of techniques, applying 5-\nfold cross-validation for the preliminary investigation to eradicate any train-test split bias. Defoliation frequencies \nwere distributed evenly among all 5 folds. On one of the datasets, this technique performed only 0.9% less than a \ngroup of human experts. This protocol can spot malnutrition, scars, and the presence of apparent diseases in wild \nplants and animals. Although species-specific models might forecast tree stress in a better way, the study does not \nhave enough training data for each species to build a robust algorithm. In consequence, complex situations, including \nabundantly defoliated trees or a green canopy with trivial defoliation, can produce errors. \nDeep learning can also automate attribute recognition from herbaria and natural photos, including leaf position, \nvein structure, bloom color, and leaf shape. Linking properties detected by deep learning to databases can pave the \nway for new datasets to investigate plant diversity research [265]. Using algorithms based on deep learning to automate \nthe recognition and extraction of characteristics is a novel field of study. Immense efforts are being made for the \n33 \n \ndigitalization of herbaria to streamline access and conserve sensitive specimens. In the United States, the iDigBio \nportal, a federally supported primary collector of museum specimen records, has over 1.8 million georeferenced and \nphotographed vascular plant specimens [266]. Carranza-Rojas et al. [254] integrated deep learning algorithms into \nenormous herbarium picture sets as a first contribution in this regard, by analyzing over 260,000 visuals of herbarium \nsheets comprising over 1,204 distinct species. The method achieved a top-one species recognition rate of 80% and a \ntop-five efficiency of 90%. However, due to the vast differences in the visual guise (e.g. inviolable color discrepancy \nand the 3D object's alteration), the researchers discovered that it is now impractical to transmit specialized information \nfrom a herbarium to field recognition.  \nHumans can abstract widely between topics and make accurate judgments with minimal information. However, \ndeep learning algorithms have limited abstraction and reasoning capabilities. Only a few researches have looked into \ncombining different plant organs or perspectives to improve accuracy [267,268]. Owing to the high effort required to \ncollect and label the datasets, completely automated species identification remains a long way off. The excellence of \nan automatic detection model is determined not only by the quantity but also by the caliber of the given training data \n[269]. On the contrary, most of the reviewed research indicates a deficit in the available qualifying data.  \n \n3.13. Fluid dynamics \nFluid dynamics is an area of applied science dealing with the flow of liquids and gases. Three conservation laws \ndefine fluid dynamics: mass conservation, linear momentum conservation, and energy conservation. Computational \nfluid dynamics (CFD) is a set of numerical methods for providing a rough solution to fluid dynamics and thermal \ndifficulties. CFD is not a science in itself but rather a means of applying numerical analysis principles to heat and \nmass transfer [270]. It provides an extensive analysis of airflow motion, contaminant transport, and heat transfer in \nenclosed places. CFD helps with wind flow and contamination dispersal analysis surrounding buildings in \nmetropolitan surroundings, but still confronts several difficulties concerning computing cost and accuracy.  \nThe neoteric success of deep neural networks (DNNs) has been facilitated by a high abundance of computational \npower that takes advantage of the multi-layer architecture. Not long ago, DNNs gained notoriety in turbulence \nmodeling or, comprehensively, in high-dimensional area, complex dynamical systems [271]. Fonda et al. [272] \nscrutinized heat transport characteristics of turbulent Rayleigh–Bénard convection in horizontally expanded systems \nby using deep-learning frameworks. The researchers applied the deep CNN trained method to measure the heat \ntransfer's fraction and time variations. The slowly evolving turbulent superstructures received special attention because \nthey are larger than the height of the convection layer. The strategy trains a deep CNN with a U-shaped configuration \nthat has a contraction branch and an expansion branch, simplifying the complicated 3D superstructure in the midplane \nlayer to a temporal planar network. As a result, data compression happens when the maximum Rayleigh number is \nmore than five magnitudes. This shows deep learning's utility to parameterize convection in global models of stellar \nand atmospheric convection. The U-specialized net's architecture requires fairly minimal training datasets that proved \nto be the most effective for ridge extraction, especially for noisier data at higher Rayleigh numbers. Nonetheless, the \nstudy did not provide information on the network's efficiency at lower Rayleigh numbers.  \n34 \n \nAnother study conducted by Daw et al. [273] applied extensive fluid flow simulations to climatology and \nturbulence to forecast turbulent flow by identifying exceptionally nonlinear phenomena from spatiotemporal velocity \nfields. Specifically, they introduced adaptable spectral filters along with a specific U-net for assertion.  The concept \nexhibits substantial declines in error prediction compared to state-of-the-art baselines. Most crucially, this procedure \naccurately previses over physical entities that satisfy favorable physical attributes, such as the conservation of mass, \nand also simulates the acute turbulent kinetic energy field and spectra for precise predictions. Another novel approach \nin this discipline is model recognition of reduced-order fluid dynamics process applying deep learning, which is a \ncomputational simulation-based method to create mathematical patterns of dynamic physical systems.  \nReduced-order modeling (ROM) is one of the effective system identification strategies for reducing the complex \nand excessive dimensional size of discrete dynamical systems. Air pollution modeling, nonlinear large-scale systems, \noptimal control, ocean modeling, form optimization, neutron difficulties, sensor placement optimization, porous media \nproblems, aerospace, multiscale fracture, and shallow water are some examples of its usage [274–282]. Subsequently, \nWang et al. [280] developed a ROM initially by combining a deep learning algorithm (LSTM) with suitable orthogonal \ndecomposition (POD) methodologies. The results revealed that the DL ROM (DLROM) can capture sophisticated \nfluid dynamics with a CPU cost of less than 1/1000.  \nWhile DLROM offers better potentiality in prediction than earlier ROMs [283], deep learning frequently \ndemands the availability of massive training data that greatly surpass the network's parameters. When the probability \ndistributions of the novel input data deviate from those of the training data, the resulting models are typically \nacceptable for interpolation but may not be adequate for extrapolation. Nevertheless, the effects of expanding this \nmethod to variable parametric obstacles, such as the real-time response to natural disasters, are unknown. \nTable 2 provides a summary of the reviewed studies on the uses of deep learning in various sectors. \n \n35 \n \nTable 2. Overview of the surveyed studies conducted on the applications of deep learning  \nApplications \nAlgorithms/ \nModels \nObjective \nOutcome \nRemarks \nRef. \nTransportation \nPrediction \nAttention based \nConvLSTM, \nBi-LSTM \nExtracts fundamental \nproperties of traffic flow data \nusing hybrid and multiple \nlayer architectures \nThe combination of attention \nConvLSTM and Bi-LSTM \nperformed better than the existing \nmodels. \nThey considered a relatively small and simple road \nnetwork. Therefore, CNN and Bi-LSTM methods \nmight be unable to completely utilize traffic flow's \ncomplex and dynamic properties. \n[120] \nAttention-based CNN-\nLSTM \nTraffic flow forecasting \nThe accuracy of the model was \nfound 99% \nWeather and other factors such as accidents and road \nclosures can be factored into the model to enhance it. \n[121] \nDeep stacked \nautoencoder to present \nfeatures in a lower \ndimension \nPredict the travel times \nShowed better performance than \napplying Deep NN in the initial \ntraining data. \nWasn’t capable of capturing traffic dynamics during \nheavy snow. \n[122] \nAgriculture \nTransfer learning to \ntrain Deep \nConvolution NN \nPlant leaf stress detection \nAchieved 93% accuracy \nFor certain diseases (CBSD, BLS, GMD), using the \nleaflet rather than the full leaf increased diagnostic \naccuracy. However, using whole leaf photos enhanced \naccuracies for others (CMD and RMD). \n[124] \nMulti-layer CNN \nDistinguish between healthy \nand stressed mango leaves \nAchieved 97.13% accuracy \nEmploying a new activation function in place of \nSoftmax can improve CNN's performance \n[284] \nCNN \nPlant disease identification \nMildly diseased images were \ndifficult to identify but accuracies \nwere higher for other cases. \nIn all situations, a few hundred photos appeared to be \nsufficient to produce credible findings, but this \nquantity must be approached with caution. \n[125] \nUNet-CNN \nClassify and identify \ncucumber powdery mildew-\naffected leaves \nCNN model segmented the sick \npowdery mildew on cucumber leaf \npictures with a mean pixel accuracy \nvalue of 96.08%. \nLack of appropriate amount and diversity of the \ndatasets.  \n[126] \n36 \n \nCNN \nFish species classification \nAchieved accuracies of over 90% \nGeneral deep structures in the experiment should be \nfine-tuned to increase the efficacy to identify vital \ninformation in the feature space of interest, in order to \nreduce the requirement for vast volumes of annotated \ndata.   \n[128] \nNatural language \nprocessing \nCollaborative \nadversarial network \nParaphrase identification \nThe model outperforms the baseline \nMaLSTM model \nShows excellent potential using the CAN for \nparaphrase identification \n[130] \nRNN \n(LSTM) \nand \nseveral other models \nParaphrase identification \nRNN (LSTM) outperforms the \nothers \nRNN shows significant performance in NLP \n[131] \nWeighted Transformer \nMachine Translation \nOutperformed state-of-the-art \nmethods \nThe model ignores the modeling of relations among \ndifferent modules. \n[285] \nSingle Neural Machine \nTranslation  \nMachine Translation \nThe method works reliably on \ngoogle scale production setting \nThe performance of zero-shot translation is frequently \ninsufficient to be practical, as the basic pivoting \nstrategy quickly outperforms it. \n[132] \nDeep-attention model \nMachine Translation \nIn comparison to the best methods \ncurrently available, deep attention \nperforms exceptionally well \nThe technique could be implemented on other tasks \nlike summarization and it could adapt to more \ncomplex attention models. \n[133] \nBiLSTM with CRF \nSentiment analysis \n(extracting aspect opinion \ntarget expression) \nOutperformed the existing research \nUnable to represent several aspects of sentences and \ndid not investigate the explicit location contexts of \nwords \n[137] \nLSTM \nSentiment Analysis \nOutperformed the existing research \nUnable to represent several aspects of sentences and \ndid not investigate the explicit location contexts of \nwords \n[138] \nDomain attention \nmodel \nMulti-domain \nsentiment \ncategorization \nEvaluated on multiple datasets and \nshowed better performance than the \nstate-of-the-art techniques \nCan pull out the most distinctive features from the \nhidden layers, reducing the number of labeled samples \nrequired. \n[139] \n37 \n \nWeakly-Supervised \nMultimodal Deep \nLearning (WS-MDL) \nPrediction \nof \nmultimodal \nattitudes in tweets \nBetter performance than supervised \nand other weakly supervised \nmodels. \nThe order of the emotion icon levels might be further \ninvestigated, and this may be included as a constraint \nto the proposed WS-MDL technique. \n[140] \nCNN, LSTM \nSentiment classification \nAchieved accuracy of 87% \nIn future work, bag-of-word and word embedding \ntechniques can be combined \n[141] \nConvLstm (merged \nCNN and LSTM) \nSentiment analysis \nPerformed better than existing \nworks with less number of \nparameters \nLocal information loss may be reduced, and long-term \ndependencies can be captured using the suggested \ndesign \n[142] \nAttention-based LSTM \nQuestion answering \nBetter performance than baseline \napproaches \nModeling more than one aspect simultaneously with \nthe attention mechanism would be an interesting \naddition to the experiment \n[143] \nHierarchical TF-IDF \ndocument retriever \nand a BERT document \nreader \nQuestion answering \nSignificant improvement of the \nbaseline techniques for the Arabic \nlanguage \nThey introduced a dataset (ARCD). However, \nARCD’s questions were created with certain \nparagraphs in mind; without that context, they might \nseem ambiguous. \n[144] \nCNN, LSTM \nVisual Question answering \nWill be helpful to study the \nreduction of the network model size. \nDetermining the rank is an NP hard problem in the \nlow-rank decomposition, and their method is still \nconstrained in this area by inserting hyper-parameters \n[145] \nTree-LSTM \nVisual Question answering \nBetter performance than existing \nHie and Deeper LSTM \nThe representational ability of the network could be \nimproved in future. \n[146] \nCNN, LSTM \nVisual Question answering \nOutperformed the existing studies by \n5% \nThe approach might be used on unstructured \ninformation sources, such as online text corpora, in \naddition to structured knowledge bases \n[147] \nReinforcement \nLearning technique \nand an encoder-\nSummarization \nPerformed better than baseline Bi-\nLSTM \nThe model may suffer from large variance since they \nuse an approximation in the Reinforcement Learning \nmethod training objective function \n[149] \n38 \n \nextractor network \narchitecture's RNN \nsequence model \nAutoencoder, \nEnsemble technique \nSummarization \nEnsemble learning models showed \nsignificant improvements in Rouge \nrecall \nand \ngood \nF-measure \noutcomes. \nA few more unsupervised NN techniques, such as \nRestricted Boltzmann Machines and stacked-auto-\nencoders, could be included in the experiment to \nimprove the robustness of the suggested technique. \n[150] \nRNN-LSTM, RBM \nSummarization \nFirst experiment where RBM and \nRNN–LSTM have been paired for \npredicting sentiment polarity. \nDoesn’t get the difference between active and passive \nsentences. \n \nBiomedicine \nCascaded-CNN \n(C-\nCNN) \nPrediction of alpha carbon \natoms throughout the core \nstructure of proteins  \nC-CNN \noutperformed (88.9%) the Phoenix-\nbased structure construction method \n(66.8%) \nAdding protein structural details for training the \nnetworks can enhance the model  \n[157] \ndeepMiRGene \n(RNN+LSTM) \nPrediction \nof \nstructural \ncharacteristics of precursor \nmiRNAs \ndeepMiRGene \nperformed \nbetter \nhaving accuracy between 88%-91% \nThe most important contribution was the elimination \nof rigorous manual feature development. \n[159] \nDNN \nPrediction of gene expression \ninterference \nOutperformed LR in 99.97% of the \nkey genes \nDemonstrated high accuracy than the linear regression \nmodel \n[161] \nFCNNs, CRF, RNN \nCerebral \nmalignancy \nidentification \nThe integration of FCNNs and CRF-\nRNN acquired an accuracy of 88% \n2D CNNs lack the essential capabilities to fully utilize \n3D information from MR \n[165] \nCNN \nIdentifications of kernels for \ngliomas segmentation in the \nimages derived from MRI \n88%, 83%, 77% accuracy acquired  \nCNN model demonstrated to be legitimate in the \nCerebral Tumor Classification database \n[167] \nBioinformatics \nDNN, RNN \nIdentification of AMPs \nBest \nperformance \nhaving \nan \naccuracy of 91.01% \nEliminated the dependency on domain experts for \nfeature creation by employing a deep network model \n[168] \n39 \n \n \nDeepCpG \nSingle-cell DNA methylation \nprediction \nObtained accuracy of 83% \nThe strength of the DeepCpG extracting predictive \nsequence from large (<1000 bp) DNA sequence.  \n[169] \n  \nEPIVAN \nIdentification of enhancer-\npromoter interactions \naccuracy ranging from 96.5% to \n98.5% in different datasets \nThe model can be utilized in transfer learning \n[171] \n \nmlDEEPre \nPrediction \nof \nMultiple \nEnzyme Functions \n97.6% of accuracy with a standard \ndeviation of 0.27 \nmlDEEPre can be effortlessly merged into DEEPre to \nhandle functional predictions.  \n[174] \nDisaster \nManagement \nSystems \nCDNN algorithm \nFlood catastrophe \nidentification \nHigher accuracy (93.2%) level than \nthe current approach of DNN and \nANN \nThe model can be improved with IoT-based devices \nusing cutting-edge algorithms at each stage of flood \nidentification  \n[177] \nCNN-based \narchitecture  \nEarly fire detection \nHigh accuracy of from 89% to 99% \nCapable of early-stage fire detection with high \naccuracy and in addition to providing an automated \nresponse \n[180] \nSVM, LR, CNN \nclassifier \nCNN-based deep learning \nalgorithm to classify the \ntrending catastrophic topics \nfrom social media \nSVM (63%-72%), LR (44%-60%), \nCNN (81%) \n \n \nCNN took longer to learn than SVM and LR as there \nwere more parameters to consider making it difficult \nto employ for web-based learning \n[33] \nRNN, CNN, MCA-\nbased model \nDisaster information \nmanagement \nMinimal accuracy 73% \nMCA based model can include more textual and \nmetadata to optimize the final categorization results  \n[184] \nDrug \ndiscovery \nand toxicology \nDNN-QSAR model \n(SAE, DBN, DNN)  \nPrediction of impacts of \nEDCs on the endocrine \nsystem, particularly (SHBG) \nand (ER). \nThe accuracy of DBN-QSAR was \n90%, \nDNN was more effective for evaluating qualitative \nresponses  \n[187] \nSVM, LDA, CART \nIdentifying EDCs through the \nER  \nOverall \nprojected \naccuracy \nconfirmed by the screening of EDC \nassay was 87.57% \nThe estrogenic activity of 109 compounds was \npredicted using the best model, SVM. \n[186] \n40 \n \nCNN \nReaction prediction and \nRetrosynthesis  \nThe model achieved a 95% \nretrosynthesis accuracy and a 97% \nresponse prediction accuracy \nThe model demonstrated the extreme capability of \nranking the real reactions precisely \n[190] \nMultimodal Deep \nNeural Network \n(MDNN) \nTo build a connection \nbetween DDI and other \ntargets and enzymes. \nDDIMDL outperformed by \nachieving around a 99% of accuracy \nrate \nDemonstrated the impact of structural information \nand multimodal features on prediction accuracy  \n[191] \nAutoencoder (SSP, \nTSP, GSP) \nPredict the therapeutic effects \nof drug-drug interaction \nclassification \naccuracy \nbetween \n97%–97.5% \nTherapeutic implications of the DDIs predicted \nshould be verified \n[195] \nPartial Differential \nEquations \n5-layer deep neural \nnetwork \nBuilding multi-physics/multi-\nscale modeling \nFor noise-free training data, the \nerror in predicting unknown \nparameters was 0.023% and \n0.006%, respectively.  \nCapable of accurately identifying the unknown \nparameters despite variations and the significant time \ninterval between the two training images. \n[196] \nDeep residual network \n(ResNet) \nSolving SBVPs with high-\ndimensional uncertainty \nDNNs predicted the mean with less \nthan 1.35% relative L2 error  \nTrained DNNs were found to transition effectively to \ninputs from non-distribution data \n[286] \nCNN based encoder-\ndecoder network \nSolving SPDE and \nuncertainty assessment tasks \n300 epochs and 200 epochs for PCS \nand DDS respectively along with 8 \nto 12 images \nSurrogate model PCS outperformed data-driven DDS \nand demonstrated the capability of incorporating \nprediction uncertainty \n[198] \nFinancial fraud \ndetection \n \nLTSM \nTo catch consumer behavior \ndetecting credit card fraud \nEnabled fast experimentation with \ngood accuracy \nCan be compared with other variants of RNN such as \nBidirectional Many-to-Many and plain neural work \n[207] \nLTSM, RNN, ANN \nEvaluating the effectiveness \nof deep learning techniques \nin financial fraud detection \nLSTM technique achieved the best \nperformance \nDid not specify the network size and sensitivity \n[209] \nLSTM \nCredit card fraud detection \nDetected suspicious financial \nactivities and alerted the appropriate \nauthorities with 99.95% accuracy \nIt can study even complex data structures and adjust \nto changed fraud trends dynamically \n[204] \n41 \n \nH2O framework \nCredit card fraud detection \nEnabled multiple algorithms to \naggregate as modules, and their \noutputs can integrate to improve the \nfinal output accuracy \nAdding more algorithms with equivalent formats and \ndatasets can elevate this model \n[211] \nAutoencoder and \nRBM \nAltering ordinary \ntransactions to detect \nanomalies \nWorked with 96.03% accuracy \nShould use real credit card fraud occurrence with \nmassive quantities of data \n[212] \nMultilayer Perceptron \nneural network (MLP) \nTax fraud detection \nClassified individuals based on \nconvicting \npotentiality \nand \ncalculates the propensity of tax fraud \nper taxpayer with 84.3% accuracy \nCan facilitate tax department with effective decision-\nmaking regarding strategic planning \n[215] \nComputer Vision \nCNN \nGeneric object detection \nThe proposed method increased the \nRCNN's mean average precision \nfrom 31 to 50.3. It also exceeds \nGoogLeNet, the winner of the \nILSVRC2014, by 6.1% \nThe addition of a def-pooling layer provides the \nmodel with a richer set of options for handling \ndeformations and incorporating deep architectures \n[225] \nSemi-Supervised \nOvercoming \nthe \ncrucial \ndrawbacks \nof \nthe \nobject \ndetection model \nTranslated the input data into more \ncompact and abstract \nrepresentations, which enhanced the \nmodel's convergence, stability, and \nperformance \nAllows modifying the model dynamically to the \ndigital photogrammetry conditions. \n[226] \nGlobal adversarial \ndeep CNNs \nFace recognition model \nProtocol attained 94.05% state-of-\nthe-art verification accuracy \nCan generate identity-distilled features and also \nextract concealed identity-dispelled features but lacks \ngeneralization \n[231] \n42 \n \nCNN and LSTM \nHuman activity recognition \nOutperformed previous models by \nup to 9% \nThe framework is useful with similar sensor \nmodalities, and it can combine them for enhanced \nperformance \n[239][\n211][\n211][\n211][\n241][\n241][\n241][\n241][\n241] \nLSTM \nDecreasing the parameters \nand computational cost of the \npreceding action and activity \nrecognition model \nThe overall accuracy of this project \nwas 95.78% on the WISDM dataset \nIt was tested on enough datasets to guarantee its \ngeneralizability \n[240] \nAutoencoder \nBuilding an autoencoder \nsystem with minimal \nsupervision via face identities \nThe model was able to generate \nidentity-distilled features and also \nextract concealed identity-dispelled \nfeatures \nDid not specify the exact dependence on range \nresolution \n[215][\n215][\n215][\n215][\n245][\n245][\n245][\n245][\n245] \nResNet based network  \nHuman pose recognition  \nThe network could forecast joint \nheatmaps and relative displacements \nof all key points for all people in \nreal-time \nNotwithstanding its simplicity, it offered state-of-art \noutcomes against the challenging benchmark \n[246] \n43 \n \nEcology \nMask R-CNN \nTo compare the deep learning \nalgorithms for determining \nfish abundance with human \ncounterparts \nThe deep learning algorithm \noutperformed marine specialists by \n7.1% and citizen scientists by \n13.4% \nEvaluated abundance with stable results and is more \nportable across survey locations than humans \n[263] \nCNN \nTree defoliation \nidentification \nIdentified tree defoliation 0.9% less \naccurately than a group of human \nexperts \nCan produce errors during complex situations due to \ndata deficiency \n[264] \nDNN and Transfer \nlearning \nTo automate characteristic \nrecognition and extraction. \nThe top-one species identification \naccuracy was 80%, while the top-\nfive accuracy was 90% \nIt is not possible anymore owing to vast differences in \nvisual appearance \n[254] \nCNN \nDirectly identifying ant \ngenera from the profile, head, \nand dorsal views of ant \nphotos \nGained over 80% accuracy in top-1 \nclassification and over 90% in top-3 \nclassification while reducing total \nclassification error \nContributes novel understanding of ensembles for \nmulti-view structured data and transfer learning \nprocesses for probing commonalities in multi-view \nCNNs \n[268][\n211][\n211][\n211][\n241][\n241][\n241][\n241][\n241] \nCNN \nTesting the efficiency of deep \nlearning in classifying \nhabitus images. \nCNN has high accuracy and can be \nutilized for many classification jobs \nTaxonomic resolution and classification recall both \nhave room to improve. Network structure, image \nquality, or the total class number to be predicted \n[288] \nFluid Dynamics  \n \nDeep CNN \nTo scrutinize heat transport \ncharacteristics of turbulent \nRayleigh–Bénard convection \nSimplified the complicated 3D \nsuperstructure in the midplane layer \nDid not provide information on the network's \nefficiency at lower Rayleigh numbers \n[272] \n44 \n \nPhysics-guided neural \nnetworks (PGNN) \nTo forecast turbulent flow \n \nShowed significant improvements in \nerror prediction over state-of-the-art \nbenchmarks \nPrecisely simulate the turbulent kinetic energy field \nand spectrum that are vital for accurate turbulent flow \nprediction \n[273] \nDLROM and POD \nDescribing the conservation \nof mass and momentum in \nfluids \nComparison with earlier ROMs \nclarified that the DLROM has better \npotentiality in the prediction \nThe consequences of applying this approach to \ntransitioning parametric constraints, such as real-time \nresponse to natural disasters, are uncertain. \n[280]  \nROMs and POD \nTo conduct a comparative \nanalysis using three ROM \napplied to a biological model \nComparison of POD-DEIM and \nGappy POD solutions revealed \nsimilar levels of accuracy \nThe model did not converge when the number of MPE \npoints was low \n[274] \nPOD and ROM \nUsing an efficient adjoint \ntechnique to optimally collect \ntargeted observations \nWhen compared to the high fidelity \nmodel, the size of the problem is \ndecreased by a factor of 200 \nIt ensures that the sensors are positioned at the \noptimum distance from one another \n[212]  \n45 \n \nBased on our prior discussion, Table 3  represents the relationship among the mentioned fields i.e. how deep learning \ntechniques on vision, audio, and NLP helps the other concrete applications, such as transportation, agriculture, \nbioinformatics, and ecology. \n \nTable 3. How deep learning techniques on vision, audio, and NLP helps the other concrete applications \nApplication Area \nDL Techniques \nBenefits and Applications \nTransportation \nObject Detection \n- Enhancing autonomous driving by identifying pedestrians, \nvehicles, and road signs \n- Real-time surveillance for safer navigation \n- Traffic flow analysis and optimization \nImage Segmentation \n- Identifying road lanes and obstacles for self-driving cars \n- Accurate mapping and localization using satellite imagery \nAgriculture \nCrop Monitoring \n- Identifying crop diseases, pests, and nutrient deficiencies \nthrough image analysis \n- Precision agriculture and yield prediction \nObject Recognition \n- Differentiating between various plant species and weeds for \ntargeted intervention \nDisaster Management \nImage Analysis \n- Aerial imagery interpretation to assess disaster extent and \ndamage \n- Identifying trapped survivors in disaster areas \nSentiment Analysis \n- Analyzing social media data for real-time disaster response \nand understanding public sentiment \nDrug Discovery \nMolecular Structure \n- Predicting molecular interactions and drug-binding sites for \ndrug design \n- Accelerating drug discovery process \nImage Analysis \n- Analyzing medical images to identify potential drug \ncandidates and understand their effects \nToxicology \nToxicity Prediction \n- Predicting toxicity of chemicals and compounds, aiding risk \nassessment \n- Reducing animal testing through computational models \nBioinformatics \nSequence Analysis \n- Analyzing genetic data for disease prediction and \npersonalized medicine \n- Identifying genetic markers for various conditions \nProtein Structure \n- Predicting protein structures to understand their functions \nand interactions \n- Advancing drug discovery and understanding diseases \n46 \n \nEcology \nImage Classification \n- Identifying and tracking wildlife species for biodiversity \nmonitoring \n- Detecting habitat changes and invasive species \nSound Classification \n- Monitoring ecosystem health by analyzing animal calls and \nenvironmental sounds \nFluid Dynamics \nImage Analysis \n- Analyzing fluid flow patterns in images or simulations \n- Identifying turbulence and vortices for better predictions \nData Analysis \n- Processing large volumes of flow data for insights and \npredictions \n \n4. Challenges and benefits of deep learning applications \nDeep learning is an incredibly important computational tool due mostly to its accuracy in data prediction and \nanalysis. Since DL does not require the use of previously processed data, the input of raw data can be computed \nthrough branched layers that separately analyze the data and represent it to the next layer for further processing [287]. \nDeep learning is useful when dealing with enormous data since it can extract information from it, eliminating the need \nfor data training and the associated computational expenditures. It is also notable for its expression and optimization \ncapabilities, which make it so effective at processing data without training [288]. As the volume and complexity of \navailable data continue to rise at an exponential rate, deep learning is gaining recognition as a crucial tool for more \nefficient data collection and analysis [289]. \nSeveral challenges of deep learning applications are common across various fields. Because of its independence \nfrom training data, deep learning demands rigorous data collection for proper analysis and processing of such a large \nnumber of data. Hence, medical, research, healthcare, and environmental data are challenging for large-scale data \ncompilation, reducing the efficacy of deep learning [290]. Particularly of concern are data quality and structure, as \ndata from health, research, and environmental studies are highly heterogeneous, full of ambiguity and noise, and often \nincomplete, which frequently presents problems for the model. Another issue with deep learning is that it typically \nassumes inputs to be static vectors and cannot readily incorporate time variables as inputs. Owing to the complex \nrelationship of signals across time, samples are irregular, which influences the model’s performance, particularly when \ndealing with health data [291–296].  \nDomain complexity, such as diverse data and insufficient information, and black boxes, the complexity of \nalgorithms that aren't understood, offer challenges for the system and inhibit the growth of data comprehension \n[287,297,298]. Some studies have found that using multimodal data improves the accuracy of the results, while others \nreport that the heterogeneous nature of the data makes it difficult for the program to implement the necessary \nmechanisms [299–302]. It is also challenging for deep learning algorithms to overcome the problem of label omission \nin many datasets [38]. \n \n47 \n \n5. Conclusions  \nTo effectively forecast and analyze data at all levels, deep learning uses cutting-edge methods, combining ideas \nfrom machine learning, deep neural networks, and artificial intelligence to construct models that utilize data \nrepresentations at each level. This review investigated numerous deep learning application pathways as well as the \nframework and modeling for each unique deep learning application across a variety of fields. Additionally, some of \nthe most significant and commonly experienced obstacles and technical issues associated with using deep learning \nwere discussed. Research domains might specialize in addressing specific difficulties and issues by analyzing the \ncommon obstacles encountered by deep learning applications across numerous fields. Data volume, quality, modeling, \ndomain complexity, and representation are some of the issues that deep learning applications have in common with \nother disciplines.  \nWhen processing massive amounts of data, it is best to employ a gated architecture, such as LSTM or GRU units, \nfor extracting persistent information. In a neural network designed for multimodal learning, some of the neurons are \nused for all tasks, while others are trained to perform specific tasks. The irregularity problem in temporal sequence \nsimilarity measurement is addressed by a suggested approach that uses dynamic time warming. For labeling \nchallenges, data can be implicitly labeled by applying acquired knowledge to fresh datasets for the same task or by \nutilizing an autoencoder for a variant architecture, such that transfer learning can be accomplished. Methods like \nknowledge distillation, which condenses the data learnt from a complex model so that is it a simpler and easier to \nexecute, and attention mechanisms, which employ an understanding of historical data to forecast results, are also being \nutilized to improve interpretability. Recent advances in quick solutions to current difficulties point to a promising \nfuture for deep learning techniques in multi-field applications. Future research should focus on better understanding \nthe issues related to the construction of adequate datasets for deep learning models, including but not limited to data \nquality, volume, domain complexity, and privacy. \n \nReferences  \n[1] \nY. Lecun, Y. Bengio, G. Hinton, Deep learning, Nature. 521 (2015) 463–444. \nhttps://doi.org/10.1038/nature14539. \n[2] \nD. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, J. Jensen, An overview of deep-learning-\nbased audio-visual speech enhancement and separation, IEEE/ACM Trans. Audio, Speech, Lang. Process. \n29 (2021) 1368–1396. \n[3] \nS.-Y. Chuang, H.-M. Wang, Y. Tsao, Improved lite audio-visual speech enhancement, IEEE/ACM Trans. \nAudio, Speech, Lang. Process. 30 (2022) 1345–1359. \n[4] \nH. Ilyas, A. Javed, K.M. Malik, AVFakeNet: A unified end-to-end Dense Swin Transformer deep learning \nmodel for audio-visual deepfakes detection, Appl. Soft Comput. (2023) 110124. \n[5] \nE. Raei, A.A. Asanjan, M.R. Nikoo, M. Sadegh, S. Pourshahabi, J.F. Adamowski, A deep learning image \nsegmentation model for agricultural irrigation system classification, Comput. Electron. Agric. 198 (2022) \n106977. \n[6] \nN. Ullah, J.A. Khan, L.A. Alharbi, A. Raza, W. Khan, I. Ahmad, An Efficient Approach for Crops Pests \nRecognition and Classification Based on Novel DeepPestNet Deep Learning Model, IEEE Access. 10 \n(2022) 73019–73032. \n[7] \nM. Sami, S.Q. Khan, M. Khurram, M.U. Farooq, R. Anjum, S. Aziz, R. Qureshi, F. Sadak, A deep learning-\nbased sensor modeling for smart irrigation system, Agronomy. 12 (2022) 212. \n48 \n \n[8] \nP.K. Kashyap, S. Kumar, A. Jaiswal, M. Prasad, A.H. Gandomi, Towards Precision Agriculture: IoT-\nenabled Intelligent Irrigation Systems Using Deep Learning Neural Network, IEEE Sens. J. (2021). \n[9] \nN. Nagaraj, H.L. Gururaj, B.H. Swathi, Y.-C. Hu, Passenger flow prediction in bus transportation system \nusing deep learning, Multimed. Tools Appl. 81 (2022) 12519–12542. \n[10] \nR. Wang, Y. Zhang, G. Fortino, Q. Guan, J. Liu, J. Song, Software escalation prediction based on deep \nlearning in the cognitive internet of vehicles, IEEE Trans. Intell. Transp. Syst. 23 (2022) 25408–25418. \n[11] \nM. Yu, Construction of regional intelligent transportation system in smart city road network via 5G network, \nIEEE Trans. Intell. Transp. Syst. (2022). \n[12] \nT.-X. Sun, X.-Y. Liu, X.-P. Qiu, X.-J. Huang, Paradigm shift in natural language processing, Mach. Intell. \nRes. 19 (2022) 169–183. \n[13] \nJ. Guo, Deep learning approach to text analysis for human emotion detection from big data, J. Intell. Syst. \n31 (2022) 113–126. \n[14] \nM.M. Li, K. Huang, M. Zitnik, Graph representation learning in biomedicine and healthcare, Nat. Biomed. \nEng. (2022) 1–17. \n[15] \nL. Jiang, D. Chen, Z. Cao, F. Wu, H. Zhu, F. Zhu, A two-stage deep learning architecture for radiographic \nstaging of periodontal bone loss, BMC Oral Health. 22 (2022) 106. \n[16] \nC. Liu, S.M.E. Sepasgozar, Q. Zhang, L. Ge, A novel attention-based deep learning method for post-disaster \nbuilding damage classification, Expert Syst. Appl. 202 (2022) 117268. \n[17] \nA.M. Vinod, D. Venkatesh, D. Kundra, N. Jayapandian, Natural Disaster Prediction by Using Image Based \nDeep Learning and Machine Learning, in: Second Int. Conf. Image Process. Capsul. Networks ICIPCN \n2021 2, Springer, 2022: pp. 56–66. \n[18] \nJ. Cai, G. Xiao, R. Su, GC6mA-Pred: a deep learning approach to identify DNA N6-methyladenine sites in \nthe rice genome, Methods. 204 (2022) 14–21. \n[19] \nM.U. Rehman, H. Tayara, K.T. Chong, DL-m6A: Identification of N6-methyladenosine Sites in Mammals \nusing deep learning based on different encoding schemes, IEEE/ACM Trans. Comput. Biol. Bioinforma. \n(2022). \n[20] \nJ. Jeevanandam, D. Agyei, M.K. Danquah, C. Udenigwe, Food quality monitoring through bioinformatics \nand big data, in: Futur. Foods, Elsevier, 2022: pp. 733–744. \n[21] \nA. Tahmassebi, J. Martin, A. Meyer-Baese, A.H. Gandomi, An Interpretable Deep Learning Framework for \nHealth Monitoring Systems: A Case Study of Eye State Detection using EEG Signals, in: 2020 IEEE Symp. \nSer. Comput. Intell., IEEE, 2020: pp. 211–218. \n[22] \nA. Kumar, M. Ramachandran, A.H. Gandomi, R. Patan, S. Lukasik, R.K. Soundarapandian, A deep neural \nnetwork based classifier for brain tumor diagnosis, Appl. Soft Comput. 82 (2019) 105528. \n[23] \nF. Palazzesi, A. Pozzan, Deep learning applied to ligand-based de novo drug design, Artif. Intell. Drug Des. \n(2022) 273–299. \n[24] \nM. Pandey, M. Fernandez, F. Gentile, O. Isayev, A. Tropsha, A.C. Stern, A. Cherkasov, The \ntransformational role of GPU computing and deep learning in drug discovery, Nat. Mach. Intell. 4 (2022) \n211–221. \n[25] \nP. Millán Arias, F. Alipour, K.A. Hill, L. Kari, DeLUCS: Deep learning for unsupervised clustering of DNA \nsequences, PLoS One. 17 (2022) e0261531. \n[26] \nZ. Li, X. Lei, S. Liu, A lightweight deep learning model for cattle face recognition, Comput. Electron. \nAgric. 195 (2022) 106848. \n[27] \nR. Hammouche, A. Attia, S. Akhrouf, Z. Akhtar, Gabor filter bank with deep autoencoder based face \nrecognition system, Expert Syst. Appl. (2022) 116743. \n[28] \nT. Ott, U. Lautenschlager, GinJinn2: Object detection and segmentation for ecology and evolution, Methods \n49 \n \nEcol. Evol. 13 (2022) 603–610. \n[29] \nN. Li, H. Huijser, Y. Xi, M. Limniou, X. Zhang, M.Y.C.A. Kek, Disrupting the disruption: A digital \nlearning HeXie ecology model, Educ. Sci. 12 (2022) 63. \n[30] \nY. Zhang, J.M. Gorriz, Z. Dong, Deep learning in medical image analysis, J. Imaging. 7 (2021) NA. \nhttps://doi.org/10.3390/jimaging7040074. \n[31] \nG. Litjens, T. Kooi, B.E. Bejnordi, A. Arindra, A. Setio, F. Ciompi, M. Ghafoorian, J.A.W.M. Van Der \nLaak, B. Van Ginneken, I.S. Clara, A Survey on Deep Learning in Medical Image Analysis, Med. Image \nAnal. 42 (2017) 60–88. https://doi.org/10.1016/j.media.2017.07.005. \n[32] \nM.H. Hesamian, W. Jia, X. He, P. Kennedy, Deep Learning Techniques for Medical Image Segmentation: \nAchievements and Challenges, J. Digit. Imaging. 32 (2019) 582–596. https://doi.org/10.1007/s10278-019-\n00227-x. \n[33] \nM. Yu, Q. Huang, H. Qin, C. Scheele, C. Yang, Deep learning for real-time social media text classification \nfor situation awareness–using Hurricanes Sandy, Harvey, and Irma as case studies, Int. J. Digit. Earth. 12 \n(2019) 1230–1247. https://doi.org/10.1080/17538947.2019.1574316. \n[34] \nM. Wang, W. Deng, Deep face recognition: A survey, Neurocomputing. 429 (2021) 215–244. \nhttps://doi.org/10.1016/j.neucom.2020.10.081. \n[35] \nM. Masud, G. Muhammad, H. Alhumyani, S.S. Alshamrani, O. Cheikhrouhou, S. Ibrahim, M.S. Hossain, \nDeep learning-based intelligent face recognition in IoT-cloud environment, Comput. Commun. 152 (2020) \n215–222. https://doi.org/10.1016/j.comcom.2020.01.050. \n[36] \nJ. Wang, Y. Chen, S. Hao, X. Peng, L. Hu, Deep learning for sensor-based activity recognition: A survey, \nPattern Recognit. Lett. 119 (2019) 3–11. https://doi.org/10.1016/j.patrec.2018.02.010. \n[37] \nM. Pfeiffer, T. Pfeil, Deep Learning With Spiking Neurons: Opportunities and Challenges, Front. Neurosci. \n12 (2018). https://doi.org/10.3389/fnins.2018.00774. \n[38] \nC. Xiao, E. Choi, J. Sun, Review Opportunities and challenges in developing deep learning models using \nelectronic health records data: a systematic review, J. Am. Med. Informatics Assoc. 25 (2018) 1419–1428. \nhttps://doi.org/10.1093/jamia/ocy068. \n[39] \nY. Li, C. Huang, L. Ding, Z. Li, Y. Pan, X. Gao, Deep learning in bioinformatics: Introduction, application, \nand perspective in the big data era, Methods. 166 (2019) 4–21. https://doi.org/10.1016/j.ymeth.2019.04.008. \n[40] \nC. Cao, F. Liu, H. Tan, D. Song, W. Shu, W. Li, Y. Zhou, X. Bo, Z. Xie, Deep Learning and Its \nApplications in Biomedicine, Genomics, Proteomics Bioinforma. 16 (2018) 17–32. \nhttps://doi.org/10.1016/j.gpb.2017.07.003. \n[41] \nQ. Yuan, H. Shen, T. Li, Z. Li, S. Li, Y. Jiang, H. Xu, W. Tan, Q. Yang, J. Wang, J. Gao, L. Zhang, Deep \nlearning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ. 241 (2020) \n111716. https://doi.org/10.1016/j.rse.2020.111716. \n[42] \nA. Kamilaris, F.X. Prenafeta-Boldú, Deep learning in agriculture: A survey, Comput. Electron. Agric. 147 \n(2018) 70–90. https://doi.org/10.1016/j.compag.2018.02.016. \n[43] \nK.P. Ferentinos, Deep learning models for plant disease detection and diagnosis, Comput. Electron. Agric. \n145 (2018) 311–318. https://doi.org/10.1016/j.compag.2018.01.009. \n[44] \nR. Elshawi, A. Wahab, A. Barnawi, S. Sakr, DLBench: a comprehensive experimental evaluation of deep \nlearning frameworks, Cluster Comput. 24 (2021) 2017–2038. https://doi.org/10.1007/s10586-021-03240-4. \n[45] \nY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell, Caffe: \nConvolutional Architecture for Fast Feature Embedding, (2014). \n[46] \nY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell, Caffe: \nConvolutional architecture for fast feature embedding, MM 2014 - Proc. 2014 ACM Conf. Multimed. \n(2014) 675–678. https://doi.org/10.1145/2647868.2654889. \n[47] \nS. Shi, Q. Wang, P. Xu, X. Chu, Benchmarking state-of-the-art deep learning software tools, Proc. - 2016 \n50 \n \n7th Int. Conf. Cloud Comput. Big Data, CCBD 2016. (2017) 99–104. \nhttps://doi.org/10.1109/CCBD.2016.029. \n[48] \nY. Deng, Deep learning on mobile devices: a review, Https://Doi.Org/10.1117/12.2518469. 10993 (2019) \n52–66. https://doi.org/10.1117/12.2518469. \n[49] \nR. Xu, F. Han, Q. Ta, Deep learning at scale on NVIDIA V100 accelerators, Proc. PMBS 2018 Perform. \nModel. Benchmarking Simul. High Perform. Comput. Syst. Held Conjunction with SC 2018 Int. Conf. High \nPerform. Comput. Networking, Storage Anal. (2019) 23–32. https://doi.org/10.1109/PMBS.2018.8641600. \n[50] \nA. Jain, A.A. Awan, H. Subramoni, D.K. Panda, Scaling TensorFlow, PyTorch, and MXNet using \nMVAPICH2 for high-performance deep learning on Frontera, Proc. DLS 2019 Deep Learn. Supercomput. - \nHeld Conjunction with SC 2019 Int. Conf. High Perform. Comput. Networking, Storage Anal. (2019) 76–\n83. https://doi.org/10.1109/DLS49591.2019.00015. \n[51] \nA. Dakkak, C. Li, S.G. De Gonzalo, J. Xiong, W.M. Hwu, TrIMS: Transparent and isolated model sharing \nfor low latency deep learning inference in function-as-a-service, IEEE Int. Conf. Cloud Comput. CLOUD. \n2019-July (2019) 372–382. https://doi.org/10.1109/CLOUD.2019.00067. \n[52] \nF. Seide, A. Agarwal, CNTK, (2016) 2135–2135. https://doi.org/10.1145/2939672.2945397. \n[53] \nD. Yu, K. Yao, Y. Zhang, The Computational Network Toolkit [Best of the Web], IEEE Signal Process. \nMag. 32 (2015) 123–126. https://doi.org/10.1109/MSP.2015.2462371. \n[54] \nW. Dai, D. Berleant, Benchmarking contemporary deep learning hardware and frameworks: A survey of \nqualitative metrics, Proc. - 2019 IEEE 1st Int. Conf. Cogn. Mach. Intell. CogMI 2019. (2019) 148–155. \nhttps://doi.org/10.1109/COGMI48466.2019.00029. \n[55] \nL. Liu, Y. Wu, W. Wei, W. Cao, S. Sahin, Q. Zhang, Benchmarking deep learning frameworks: Design \nconsiderations, metrics and beyond, Proc. - Int. Conf. Distrib. Comput. Syst. 2018-July (2018) 1258–1269. \nhttps://doi.org/10.1109/ICDCS.2018.00125. \n[56] \nY. Wu, W. Cao, S. Sahin, L. Liu, Experimental Characterizations and Analysis of Deep Learning \nFrameworks, Proc. - 2018 IEEE Int. Conf. Big Data, Big Data 2018. (2019) 372–377. \nhttps://doi.org/10.1109/BIGDATA.2018.8621930. \n[57] \nA. Shatnawi, G. Al-Bdour, R. Al-Qurran, M. Al-Ayyoub, A comparative study of open source deep learning \nframeworks, 2018 9th Int. Conf. Inf. Commun. Syst. ICICS 2018. 2018-Janua (2018) 72–77. \nhttps://doi.org/10.1109/IACS.2018.8355444. \n[58] \nI.J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra, F. Bastien, \nY. Bengio, Pylearn2: a machine learning research library, (2013). \n[59] \nB. van Merriënboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-Farley, J. Chorowski, Y. Bengio, \nBlocks and Fuel: Frameworks for deep learning, (2015). https://doi.org/10.48550/arxiv.1506.00619. \n[60] \nX. Miao, W. Zhang, Y. Shao, B. Cui, L. Chen, C. Zhang, J. Jiang, Lasagne: A Multi-Layer Graph \nConvolutional Network Framework via Node-aware Deep Architecture, IEEE Trans. Knowl. Data Eng. \n(2021) 1–1. https://doi.org/10.1109/TKDE.2021.3103984. \n[61] \nJ. Moolayil, An Introduction to Deep Learning and Keras, Learn Keras Deep Neural Networks. (2019) 1–16. \nhttps://doi.org/10.1007/978-1-4842-4240-7_1. \n[62] \nJ. Han, E. Shihab, Z. Wan, S. Deng, X. Xia, What do Programmers Discuss about Deep Learning \nFrameworks, Empir. Softw. Eng. 25 (2020) 2694–2747. https://doi.org/10.1007/S10664-020-09819-\n6/FIGURES/22. \n[63] \nJ. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasudevan, D. Moore, B. Patton, A. Alemi, M. Hoffman, \nR.A. Saurous, TensorFlow Distributions, (2017). https://doi.org/10.48550/arxiv.1711.10604. \n[64] \nB. Pang, E. Nijkamp, Y.N. Wu, Deep Learning With TensorFlow: A Review:, \nHttps://Doi.Org/10.3102/1076998619872761. 45 (2019) 227–248. \nhttps://doi.org/10.3102/1076998619872761. \n51 \n \n[65] \nT. Young, D. Hazarika, S. Poria, E. Cambria, Recent trends in deep learning based natural language \nprocessing [Review Article], IEEE Comput. Intell. Mag. 13 (2018) 55–75. \nhttps://doi.org/10.1109/MCI.2018.2840738. \n[66] \nA. Anushya, Google Lens as an Image Classifier, Int. J. Sci. Res. Comput. Sci. Appl. Manag. Stud. \nIJSRCSAMS. 8 (2019). \n[67] \nJ. Xu, J. Zhao, R. Zhou, C. Liu, P. Zhao, L. Zhao, Predicting Destinations by a Deep Learning based \nApproach, IEEE Trans. Knowl. Data Eng. 33 (2021) 651–666. \nhttps://doi.org/10.1109/TKDE.2019.2932984. \n[68] \nF. Rodrigues, I. Markou, F.C. Pereira, Combining time-series and textual data for taxi demand prediction in \nevent areas: A deep learning approach, Inf. Fusion. 49 (2019) 120–129. \nhttps://doi.org/10.1016/J.INFFUS.2018.07.007. \n[69] \nV. Varghese, M. Chikaraishi, J. Urata, Deep Learning in Transport Studies: A Meta-analysis on the \nPrediction Accuracy, J. Big Data Anal. Transp. 2020 23. 2 (2020) 199–220. https://doi.org/10.1007/S42421-\n020-00030-Z. \n[70] \nW. Wang, Y. Bai, C. Yu, Y. Gu, P. Feng, X. Wang, R. Wang, A network traffic flow prediction with deep \nlearning approach for large-scale metropolitan area network, IEEE/IFIP Netw. Oper. Manag. Symp. Cogn. \nManag. a Cyber World, NOMS 2018. (2018) 1–9. https://doi.org/10.1109/NOMS.2018.8406252. \n[71] \nY. Wu, H. Tan, L. Qin, B. Ran, Z. Jiang, A hybrid deep learning based traffic flow prediction method and its \nunderstanding, Transp. Res. Part C Emerg. Technol. 90 (2018) 166–180. \nhttps://doi.org/10.1016/J.TRC.2018.03.001. \n[72] \nA. Telikani, A.H. Gandomi, K.-K.R. Choo, J. Shen, A cost-sensitive deep learning based approach for \nnetwork traffic classification, IEEE Trans. Netw. Serv. Manag. (2021). \n[73] \nL. Tran, M.Y. Mun, M. Lim, J. Yamato, N. Huh, C. Shahabi, DeepTRANS, Proc. VLDB Endow. 13 (2020) \n2957–2960. https://doi.org/10.14778/3415478.3415518. \n[74] \nS. Xu, R. Zhang, W. Cheng, J. Xu, MTLM: a multi-task learning model for travel time estimation, \nGeoinformatica. (2020) 1–17. https://doi.org/10.1007/S10707-020-00422-X/TABLES/2. \n[75] \nH. Ren, Y. Song, J. Wang, Y. Hu, J. Lei, A Deep Learning Approach to the Citywide Traffic Accident Risk \nPrediction, IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC. 2018-Novem (2018) 3346–3351. \nhttps://doi.org/10.1109/ITSC.2018.8569437. \n[76] \nZ. Yang, W. Zhang, J. Feng, Predicting multiple types of traffic accident severity with explanations: A \nmulti-task deep learning framework, Saf. Sci. 146 (2022) 105522. \nhttps://doi.org/10.1016/J.SSCI.2021.105522. \n[77] \nJ.J.Q. Yu, Travel Mode Identification with GPS Trajectories Using Wavelet Transform and Deep Learning, \nIEEE Trans. Intell. Transp. Syst. 22 (2021) 1093–1103. https://doi.org/10.1109/TITS.2019.2962741. \n[78] \nD. Yi, J. Su, C. Liu, W.H. Chen, Trajectory clustering aided personalized driver intention prediction for \nintelligent vehicles, IEEE Trans. Ind. Informatics. 15 (2019) 3693–3702. \nhttps://doi.org/10.1109/TII.2018.2890141. \n[79] \nC. Wang, J. Li, Y. He, K. Xiao, C. Hu, Segmented Trajectory Clustering-Based Destination Prediction in \nIoVs, IEEE Access. 8 (2020) 98999–99009. https://doi.org/10.1109/ACCESS.2020.2998063. \n[80] \nG. Manogaran, M. Alazab, Ant-Inspired Recurrent Deep Learning Model for Improving the Service Flow of \nIntelligent Transportation Systems, IEEE Trans. Intell. Transp. Syst. 22 (2021) 3654–3663. \nhttps://doi.org/10.1109/TITS.2020.3037902. \n[81] \nH.A. Ibrahim, A.T. Azar, Z.F. Ibrahim, H.H. Ammar, A Hybrid Deep Learning Based Autonomous Vehicle \nNavigation and Obstacles Avoidance, Adv. Intell. Syst. Comput. 1153 AISC (2020) 296–307. \nhttps://doi.org/10.1007/978-3-030-44289-7_28. \n[82] \nA. Bhattacharjee, A.D. Chhokra, Z. Kang, H. Sun, A. Gokhale, G. Karsai, BARISTA: Efficient and scalable \nserverless serving system for deep learning prediction services, Proc. - 2019 IEEE Int. Conf. Cloud Eng. \n52 \n \nIC2E 2019. (2019) 23–33. https://doi.org/10.1109/IC2E.2019.00-10. \n[83] \nJ. Ke, H. Zheng, H. Yang, X. (Michael) Chen, Short-term forecasting of passenger demand under on-\ndemand ride services: A spatio-temporal deep learning approach, Transp. Res. Part C Emerg. Technol. 85 \n(2017) 591–608. https://doi.org/10.1016/J.TRC.2017.10.016. \n[84] \nT. Chu, J. Wang, L. Codeca, Z. Li, Multi-agent deep reinforcement learning for large-scale traffic signal \ncontrol, IEEE Trans. Intell. Transp. Syst. 21 (2020) 1086–1095. \nhttps://doi.org/10.1109/TITS.2019.2901791. \n[85] \nC. Chen, H. Wei, N. Xu, G. Zheng, M. Yang, Y. Xiong, K. Xu, Z. Li, Toward a thousand lights: \nDecentralized deep reinforcement learning for large-scale traffic signal control, AAAI 2020 - 34th AAAI \nConf. Artif. Intell. (2020) 3414–3421. https://doi.org/10.1609/AAAI.V34I04.5744. \n[86] \nJ. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. \nŽídek, A. Potapenko, A. Bridgland, C. Meyer, S.A.A. Kohl, A.J. Ballard, A. Cowie, B. Romera-Paredes, S. \nNikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. \nPacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A.W. Senior, K. Kavukcuoglu, P. Kohli, \nD. Hassabis, Highly accurate protein structure prediction with AlphaFold, Nat. 2021 5967873. 596 (2021) \n583–589. https://doi.org/10.1038/s41586-021-03819-2. \n[87] \nM. Torrisi, G. Pollastri, Q. Le, Deep learning methods in protein structure prediction, Comput. Struct. \nBiotechnol. J. 18 (2020) 1301–1310. https://doi.org/10.1016/J.CSBJ.2019.12.011. \n[88] \nJ. Zrimec, C.S. Börlin, F. Buric, A.S. Muhammad, R. Chen, V. Siewers, V. Verendel, J. Nielsen, M. Töpel, \nA. Zelezniak, Deep learning suggests that gene expression is encoded in all parts of a co-evolving \ninteracting gene regulatory structure, Nat. Commun. 2020 111. 11 (2020) 1–16. \nhttps://doi.org/10.1038/s41467-020-19921-4. \n[89] \nR. Singh, J. Lanchantin, G. Robins, Y. Qi, DeepChrome: deep-learning for predicting gene expression from \nhistone modifications, Bioinformatics. 32 (2016) i639–i648. \nhttps://doi.org/10.1093/BIOINFORMATICS/BTW427. \n[90] \nD. Grapov, J. Fahrmann, K. Wanichthanarak, S. Khoomrung, Rise of deep learning for genomic, proteomic, \nand metabolomic data integration in precision medicine, Omi. A J. Integr. Biol. 22 (2018) 630–636. \nhttps://doi.org/10.1089/OMI.2018.0097/ASSET/IMAGES/LARGE/FIGURE4.JPEG. \n[91] \nV.S. Parekh, M.A. Jacobs, Deep learning and radiomics in precision medicine, \nHttps://Doi.Org/10.1080/23808993.2019.1585805. 4 (2019) 59–72. \nhttps://doi.org/10.1080/23808993.2019.1585805. \n[92] \nJ.S. Duncan, M.F. Insana, N. Ayache, Biomedical Imaging and Analysis in the Age of Big Data and Deep \nLearning, Proc. IEEE. 108 (2020) 3–10. https://doi.org/10.1109/JPROC.2019.2956422. \n[93] \nM. Dai, G. Xiao, L. Fiondella, M. Shao, Y.S. Zhang, Deep learning-enabled resolution-enhancement in \nmini- and regular microscopy for biomedical imaging, Sensors Actuators A Phys. 331 (2021) 112928. \nhttps://doi.org/10.1016/J.SNA.2021.112928. \n[94] \nH. Rizk, M. Torki, M. Youssef, CellinDeep: Robust and Accurate Cellular-Based Indoor Localization via \nDeep Learning, IEEE Sens. J. 19 (2019) 2305–2312. https://doi.org/10.1109/JSEN.2018.2885958. \n[95] \nH. Rizk, M. Youssef, MonoDCell: A ubiquitous and low-overhead deep learning-based indoor localization \nwith limited cellular information, GIS Proc. ACM Int. Symp. Adv. Geogr. Inf. Syst. (2019) 109–118. \nhttps://doi.org/10.1145/3347146.3359065. \n[96] \nD. Jia, C. Chen, C. Chen, F. Chen, N. Zhang, Z. Yan, X. Lv, Breast Cancer Case Identification Based on \nDeep Learning and Bioinformatics Analysis, Front. Genet. 12 (2021). \nhttps://doi.org/10.3389/FGENE.2021.628136. \n[97] \nM.R. Karim, O. Beyan, A. Zappa, I.G. Costa, D. Rebholz-Schuhmann, M. Cochez, S. Decker, Deep \nlearning-based clustering approaches for bioinformatics, Brief. Bioinform. 22 (2021) 393–415. \nhttps://doi.org/10.1093/BIB/BBZ170. \n[98] \nL. Guo, S. Wang, M. Li, Z. Cao, Accurate classification of membrane protein types based on sequence and \n53 \n \nevolutionary information using deep learning, BMC Bioinformatics. 20 (2019) 1–17. \nhttps://doi.org/10.1186/S12859-019-3275-6/TABLES/12. \n[99] \nN. Du, J. Shang, Y. Sun, Improving protein domain classification for third-generation sequencing reads \nusing deep learning, BMC Genomics. 22 (2021) 1–13. https://doi.org/10.1186/S12864-021-07468-\n7/FIGURES/9. \n[100] H. Greenspan, B. Van Ginneken, R.M. Summers, Guest Editorial Deep Learning in Medical Imaging: \nOverview and Future Promise of an Exciting New Technique, IEEE Trans. Med. Imaging. 35 (2016) 1153–\n1159. https://doi.org/10.1109/TMI.2016.2553401. \n[101] G. Tsagkatakis, M. Jaber, P. Tsakalides, M. Jaber, P. Tsakalides, Goal!! Event detection in sports video, \nElectron. Imaging. 29 (2017) 15–20. https://doi.org/10.2352/ISSN.2470-1173.2017.16.CVAS-344. \n[102] J. Donahue, L.A. Hendricks, M. Rohrbach, S. Venugopalan, S. Guadarrama, K. Saenko, T. Darrell, Long-\nTerm Recurrent Convolutional Networks for Visual Recognition and Description, in: IEEE Trans. Pattern \nAnal. Mach. Intell., 2017: pp. 677–691. https://doi.org/10.1109/TPAMI.2016.2599174. \n[103] A.R. Yuliani, M.F. Amri, E. Suryawati, A. Ramdan, H.F. Pardede, Speech Enhancement Using Deep \nLearning Methods: A Review, J. Elektron. Dan Telekomun. 21 (2021) 19. \nhttps://doi.org/10.14203/JET.V21.19-26. \n[104] J. Byrd, Z.C. Lipton, What is the effect of importance weighting in deep learning?, 36th Int. Conf. Mach. \nLearn. ICML 2019. 2019-June (2019) 1405–1419. \n[105] A. Natsiou, S. O’Leary, Audio representations for deep learning in sound synthesis: A review, (2022) 1–8. \nhttps://doi.org/10.1109/aiccsa53542.2021.9686838. \n[106] W. Widodo, M. Nugraheni, I.P. Sari, Z. Yao, A. Chen, H. Xie, D.S. Maylawati, Y.J. Kumar, F.B. Kasmin, \nM.A. Ramdhani, An idea based on sequential pattern mining and deep learning for text summarization, J. \nPhys. Conf. Ser. 1402 (2019) 077013. https://doi.org/10.1088/1742-6596/1402/7/077013. \n[107] T.G. Kang, J.W. Shin, N.S. Kim, DNN-based monaural speech enhancement with temporal and spectral \nvariations equalization, Digit. Signal Process. 74 (2018) 102–110. \nhttps://doi.org/10.1016/J.DSP.2017.12.002. \n[108] A. Pandey, D. Wang, On Cross-Corpus Generalization of Deep Learning Based Speech Enhancement, \nIEEE/ACM Trans. Audio Speech Lang. Process. 28 (2020) 2489–2499. \nhttps://doi.org/10.1109/TASLP.2020.3016487. \n[109] Y. Xu, J. Du, L.R. Dai, C.H. Lee, A regression approach to speech enhancement based on deep neural \nnetworks, IEEE/ACM Trans. Audio Speech Lang. Process. 23 (2015) 7–19. \nhttps://doi.org/10.1109/TASLP.2014.2364452. \n[110] J.M. Valin, A hybrid DSP/Deep learning approach to real-time full-band speech enhancement, 2018 IEEE \n20th Int. Work. Multimed. Signal Process. MMSP 2018. (2018). \nhttps://doi.org/10.1109/MMSP.2018.8547084. \n[111] N. Das, S. Chakraborty, J. Chaki, N. Padhy, N. Dey, Fundamentals, present and future perspectives of \nspeech enhancement, Int. J. Speech Technol. 24 (2021) 883–901. https://doi.org/10.1007/S10772-020-\n09674-2/TABLES/6. \n[112] K. Tan, D. Wang, Towards Model Compression for Deep Learning Based Speech Enhancement, \nIEEE/ACM Trans. Audio Speech Lang. Process. 29 (2021) 1785–1794. \nhttps://doi.org/10.1109/TASLP.2021.3082282. \n[113] M. Lech, M. Stolar, C. Best, R. Bolia, Real-Time Speech Emotion Recognition Using a Pre-trained Image \nClassification Network: Effects of Bandwidth Reduction and Companding, Front. Comput. Sci. 2 (2020) 14. \nhttps://doi.org/10.3389/FCOMP.2020.00014/BIBTEX. \n[114] H. Aouani, Y. Ben Ayed, Speech Emotion Recognition with deep learning, Procedia Comput. Sci. 176 \n(2020) 251–260. https://doi.org/10.1016/J.PROCS.2020.08.027. \n[115] F. Noroozi, M. Marjanovic, A. Njegus, S. Escalera, G. Anbarjafari, Audio-Visual Emotion Recognition in \n54 \n \nVideo Clips, IEEE Trans. Affect. Comput. 10 (2019) 60–75. https://doi.org/10.1109/TAFFC.2017.2713783. \n[116] R.A. Khalil, E. Jones, M.I. Babar, T. Jan, M.H. Zafar, T. Alhussain, Speech Emotion Recognition Using \nDeep Learning Techniques: A Review, IEEE Access. 7 (2019) 117327–117345. \nhttps://doi.org/10.1109/ACCESS.2019.2936124. \n[117] S.R. Bandela, T.K. Kumar, Stressed speech emotion recognition using feature fusion of teager energy \noperator and MFCC, 8th Int. Conf. Comput. Commun. Netw. Technol. ICCCNT 2017. (2017). \nhttps://doi.org/10.1109/ICCCNT.2017.8204149. \n[118] A.A.A. Zamil, S. Hasan, S.M. Jannatul Baki, J.M. Adam, I. Zaman, Emotion detection from speech signals \nusing voting mechanism on classified frames, 1st Int. Conf. Robot. Electr. Signal Process. Tech. ICREST \n2019. (2019) 281–285. https://doi.org/10.1109/ICREST.2019.8644168. \n[119] R. Jahangir, Y.W. Teh, F. Hanif, G. Mujtaba, Deep learning approaches for speech emotion recognition: \nstate of the art and research challenges, Multimed. Tools Appl. 80 (2021) 23745–23812. \nhttps://doi.org/10.1007/S11042-020-09874-7/FIGURES/17. \n[120] H. Zheng, F. Lin, X. Feng, Y. Chen, A Hybrid Deep Learning Model with Attention-Based Conv-LSTM \nNetworks for Short-Term Traffic Flow Prediction, IEEE Trans. Intell. Transp. Syst. 22 (2021). \nhttps://doi.org/10.1109/TITS.2020.2997352. \n[121] B. Vijayalakshmi, K. Ramar, N.Z. Jhanjhi, S. Verma, M. Kaliappan, K. Vijayalakshmi, S. Vimal, Kavita, U. \nGhosh, An attention-based deep learning model for traffic flow prediction using spatiotemporal features \ntowards sustainable smart city, Int. J. Commun. Syst. 34 (2021). https://doi.org/10.1002/dac.4609. \n[122] M. Abdollahi, T. Khaleghi, K. Yang, An integrated feature learning approach using deep learning for travel \ntime prediction, Expert Syst. Appl. 139 (2020). https://doi.org/10.1016/j.eswa.2019.112864. \n[123] A. Kamilaris, F.X. Prenafeta-Boldú, Deep learning in agriculture: A survey, Comput. Electron. Agric. 147 \n(2018). https://doi.org/10.1016/j.compag.2018.02.016. \n[124] A. Ramcharan, K. Baranowski, P. McCloskey, B. Ahmed, J. Legg, D.P. Hughes, Deep learning for image-\nbased cassava disease detection, Front. Plant Sci. 8 (2017). https://doi.org/10.3389/fpls.2017.01852. \n[125] J.G. Arnal Barbedo, Plant disease identification from individual lesions and spots using deep learning, \nBiosyst. Eng. 180 (2019). https://doi.org/10.1016/j.biosystemseng.2019.02.002. \n[126] K. Lin, L. Gong, Y. Huang, C. Liu, J. Pan, Deep learning-based segmentation and quantification of \ncucumber powdery mildew using convolutional neural network, Front. Plant Sci. 10 (2019). \nhttps://doi.org/10.3389/fpls.2019.00155. \n[127] X. Yang, S. Zhang, J. Liu, Q. Gao, S. Dong, C. Zhou, Deep learning for smart fish farming: applications, \nopportunities and challenges, Rev. Aquac. 13 (2021). https://doi.org/10.1111/raq.12464. \n[128] A. Salman, A. Jalal, F. Shafait, A. Mian, M. Shortis, J. Seager, E. Harvey, Fish species classification in \nunconstrained underwater environments based on deep learning, Limnol. Oceanogr. Methods. 14 (2016). \nhttps://doi.org/10.1002/lom3.10113. \n[129] A. Altheneyan, M.E.B. Menai, Evaluation of State-of-the-Art Paraphrase Identification and Its Application \nto Automatic Plagiarism Detection, Int. J. Pattern Recognit. Artif. Intell. 34 (2020). \nhttps://doi.org/10.1142/S0218001420530043. \n[130] J.A. Alzubi, R. Jain, A. Kathuria, A. Khandelwal, A. Saxena, A. Singh, Paraphrase identification using \ncollaborative adversarial networks, J. Intell. Fuzzy Syst. 39 (2020). https://doi.org/10.3233/JIFS-191933. \n[131] E. Hunt, R. Janamsetty, C. Kinares, C. Koh, A. Sanchez, F. Zhan, M. Ozdemir, S. Waseem, O. Yolcu, B. \nDahal, J. Zhan, L. Gewali, P. Oh, Machine learning models for paraphrase identification and its applications \non plagiarism detection, in: Proc. - 10th IEEE Int. Conf. Big Knowledge, ICBK 2019, 2019. \nhttps://doi.org/10.1109/ICBK.2019.00021. \n[132] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Viégas, M. Wattenberg, G. \nCorrado, M. Hughes, J. Dean, Google’s Multilingual Neural Machine Translation System: Enabling Zero-\nShot Translation, Trans. Assoc. Comput. Linguist. 5 (2017). https://doi.org/10.1162/tacl_a_00065. \n55 \n \n[133] B. Zhang, D. Xiong, J. Su, Neural Machine Translation with Deep Attention, IEEE Trans. Pattern Anal. \nMach. Intell. 42 (2020). https://doi.org/10.1109/TPAMI.2018.2876404. \n[134] B. Zhang, D. Xiong, J. Su, H. Duan, M. Zhang, Variational neural machine translation, in: EMNLP 2016 - \nConf. Empir. Methods Nat. Lang. Process. Proc., 2016. https://doi.org/10.18653/v1/d16-1050. \n[135] J. Zhou, Y. Cao, X. Wang, P. Li, W. Xu, Deep Recurrent Models with Fast-Forward Connections for Neural \nMachine Translation, Trans. Assoc. Comput. Linguist. 4 (2016). https://doi.org/10.1162/tacl_a_00105. \n[136] A. Yadav, D.K. Vishwakarma, Sentiment analysis using deep learning architectures: a review, Artif. Intell. \nRev. 53 (2020). https://doi.org/10.1007/s10462-019-09794-5. \n[137] M. Al-Smadi, B. Talafha, M. Al-Ayyoub, Y. Jararweh, Using long short-term memory deep neural networks \nfor aspect-based sentiment analysis of Arabic reviews, Int. J. Mach. Learn. Cybern. 10 (2019). \nhttps://doi.org/10.1007/s13042-018-0799-4. \n[138] Y. Ma, H. Peng, T. Khan, E. Cambria, A. Hussain, Sentic LSTM: a Hybrid Network for Targeted Aspect-\nBased Sentiment Analysis, Cognit. Comput. 10 (2018). https://doi.org/10.1007/s12559-018-9549-x. \n[139] Z. Yuan, S. Wu, F. Wu, J. Liu, Y. Huang, Domain attention model for multi-domain sentiment \nclassification, Knowledge-Based Syst. 155 (2018). https://doi.org/10.1016/j.knosys.2018.05.004. \n[140] F. Chen, R. Ji, J. Su, D. Cao, Y. Gao, Predicting Microblog Sentiments via Weakly Supervised Multimodal \nDeep Learning, IEEE Trans. Multimed. 20 (2018). https://doi.org/10.1109/TMM.2017.2757769. \n[141] Q. Huang, R. Chen, X. Zheng, Z. Dong, Deep sentiment representation based on CNN and LSTM, in: Proc. \n- 2017 Int. Conf. Green Informatics, ICGI 2017, 2017. https://doi.org/10.1109/ICGI.2017.45. \n[142] A. Hassan, A. Mahmood, Deep Learning approach for sentiment analysis of short texts, in: 2017 3rd Int. \nConf. Control. Autom. Robot. ICCAR 2017, 2017. https://doi.org/10.1109/ICCAR.2017.7942788. \n[143] Y. Wang, M. Huang, L. Zhao, X. Zhu, Attention-based LSTM for aspect-level sentiment classification, in: \nEMNLP 2016 - Conf. Empir. Methods Nat. Lang. Process. Proc., 2016. https://doi.org/10.18653/v1/d16-\n1058. \n[144] H. Mozannar, K. El Hajal, E. Maamary, H. Hajj, Neural arabic question answering, in: ACL 2019 - 4th \nArab. Nat. Lang. Process. Work. WANLP 2019 - Proc. Work., 2019. https://doi.org/10.18653/v1/w19-4612. \n[145] Z. Bai, Y. Li, M. Woźniak, M. Zhou, D. Li, DecomVQANet: Decomposing visual question answering deep \nnetwork via tensor decomposition and regression, Pattern Recognit. 110 (2021). \nhttps://doi.org/10.1016/j.patcog.2020.107538. \n[146] D. Yu, X. Gao, H. Xiong, Structured Semantic Representation for Visual Question Answering, in: Proc. - \nInt. Conf. Image Process. ICIP, 2018. https://doi.org/10.1109/ICIP.2018.8451516. \n[147] M. Narasimhan, A.G. Schwing, Straight to the facts: Learning knowledge base retrieval for factual visual \nquestion answering, in: Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes \nBioinformatics), 2018. https://doi.org/10.1007/978-3-030-01237-3_28. \n[148] W.S. El-Kassas, C.R. Salama, A.A. Rafea, H.K. Mohamed, Automatic text summarization: A \ncomprehensive survey, Expert Syst. Appl. 165 (2021). https://doi.org/10.1016/j.eswa.2020.113679. \n[149] L. Chen, M. Le Nguyen, Sentence selective neural extractive summarization with reinforcement learning, in: \nProc. 2019 11th Int. Conf. Knowl. Syst. Eng. KSE 2019, 2019. https://doi.org/10.1109/KSE.2019.8919490. \n[150] N. Alami, M. Meknassi, N. En-nahnahi, Y. El Adlouni, O. Ammor, Unsupervised neural networks for \nautomatic Arabic text summarization using document clustering and topic modeling, Expert Syst. Appl. 172 \n(2021). https://doi.org/10.1016/j.eswa.2021.114652. \n[151] A. Abdi, S. Hasan, S.M. Shamsuddin, N. Idris, J. Piran, A hybrid deep learning architecture for opinion-\noriented multi-document summarization based on multi-feature fusion, Knowledge-Based Syst. 213 (2021). \nhttps://doi.org/10.1016/j.knosys.2020.106658. \n[152] P. Mirowski, D. Madhavan, Y. LeCun, R. Kuzniecky, Classification of patterns of EEG synchronization for \nseizure prediction, Clin. Neurophysiol. 120 (2009) 1927–1940. https://doi.org/10.1016/j.clinph.2009.09.002. \n56 \n \n[153] B. Lee, S. Park, J. Baek, S. Yoon, DeepTarget: End-to-end learning framework for MicroRNA target \nprediction using deep recurrent neural networks, ACM-BCB 2016 - 7th ACM Conf. Bioinformatics, \nComput. Biol. Heal. Informatics. (2016) 434–442. https://doi.org/10.1145/2975167.2975212. \n[154] A. Petrosian, D. Prokhorov, R. Homan, R. Dasheiff, D. Wunsch, Recurrent neural network based prediction \nof epileptic seizures in intra- and extracranial EEG, Neurocomputing. 30 (2000) 201–218. \nhttps://doi.org/10.1016/S0925-2312(99)00126-5. \n[155] X. chen Bai, G. McMullan, S.H.W. Scheres, How cryo-EM is revolutionizing structural biology, Trends \nBiochem. Sci. 40 (2015) 49–57. https://doi.org/10.1016/j.tibs.2014.10.005. \n[156] A. Bartesaghi, C. Aguerrebere, V. Falconieri, S. Banerjee, L.A. Earl, X. Zhu, N. Grigorieff, J.L.S. Milne, G. \nSapiro, X. Wu, S. Subramaniam, Atomic Resolution Cryo-EM Structure of β-Galactosidase, Structure. 26 \n(2018) 848-856.e3. https://doi.org/10.1016/j.str.2018.04.004. \n[157] D. Si, S.A. Moritz, J. Pfab, J. Hou, R. Cao, L. Wang, T. Wu, J. Cheng, Deep Learning to Predict Protein \nBackbone Structure from High-Resolution Cryo-EM Density Maps, Sci. Rep. 10 (2020) 1–22. \nhttps://doi.org/10.1038/s41598-020-60598-y. \n[158] W. Kang, M.R. Friedländer, Computational prediction of miRNA genes from small RNA sequencing data, \nFront. Bioeng. Biotechnol. 3 (2015) 1–14. https://doi.org/10.3389/fbioe.2015.00007. \n[159] S. Park, S. Min, H. Choi, S. Yoon, deepMiRGene: Deep Neural Network based Precursor microRNA \nPrediction, (2016). \n[160] P. Baldi, P. Sadowski, Understanding dropout, Adv. Neural Inf. Process. Syst. (2013) 1–9. \nhttps://doi.org/10.17744/mehc.25.2.xhyreggxdcd0q4ny. \n[161] Y. Chen, Y. Li, R. Narayan, A. Subramanian, X. Xie, Gene expression inference with deep learning, \nBioinformatics. 32 (2016) 1832–1839. https://doi.org/10.1093/bioinformatics/btw074. \n[162] B.A. Krizhevsky, I. Sutskever, G.E. Hinton, Cnn实际训练的, Commun. ACM. 60 (2012) 84–90. \n[163] Z. Liu, X. Li, P. Luo, C.C. Loy, X. Tang, Semantic image segmentation via deep parsing network, Proc. \nIEEE Int. Conf. Comput. Vis. 2015 Inter (2015) 1377–1385. https://doi.org/10.1109/ICCV.2015.162. \n[164] D. Zikic, Y. Ioannou, M. Brown, A. Criminisi, Segmentation of Brain Tumor Tissues with Convolutional \nNeural Networks, Proc. MICCAI-BRATS. (2014) 36–39. \n[165] X. Zhao, Y. Wu, G. Song, Z. Li, Y. Zhang, Y. Fan, A deep learning model integrating FCNNs and CRFs for \nbrain tumor segmentation, Med. Image Anal. 43 (2018) 98–111. \nhttps://doi.org/10.1016/j.media.2017.10.002. \n[166] P. Khani, F. Nasri, F. Khani Chamani, F. Saeidi, J. Sadri Nahand, A. Tabibkhooei, H. Mirzaei, Genetic and \nepigenetic contribution to astrocytic gliomas pathogenesis, J. Neurochem. 148 (2019) 188–203. \n[167] M.M. Thaha, K.P.M. Kumar, B.S. Murugan, S. Dhanasekeran, P. Vijayakarthick, A.S. Selvi, Brain Tumor \nSegmentation Using Convolutional Neural Networks in MRI Images, J. Med. Syst. 43 (2019) 1240–1251. \nhttps://doi.org/10.1007/s10916-019-1416-0. \n[168] D. Veltri, U. Kamath, A. Shehu, Deep learning improves antimicrobial peptide recognition, Bioinformatics. \n34 (2018) 2740–2747. https://doi.org/10.1093/bioinformatics/bty179. \n[169] C. Angermueller, H.J. Lee, W. Reik, O. Stegle, DeepCpG: Accurate prediction of single-cell DNA \nmethylation states using deep learning, Genome Biol. 18 (2017) 1–14. https://doi.org/10.1186/s13059-017-\n1189-z. \n[170] J. van Arensbergen, B. van Steensel, H.J. Bussemaker, In search of the determinants of enhancer-promoter \ninteraction specificity, Trends Cell Biol. 24 (2014) 695–702. https://doi.org/10.1016/j.tcb.2014.07.004. \n[171] Z. Hong, X. Zeng, L. Wei, X. Liu, Identifying enhancer-promoter interactions with neural network based on \npre-trained DNA vectors and attention mechanism, Bioinformatics. 36 (2020) 1037–1043. \nhttps://doi.org/10.1093/bioinformatics/btz694. \n57 \n \n[172] B. Hoffmann, M. Beck, G. Sunder-Plassmann, W. Borsini, R. Ricci, A. Mehta, Nature and prevalence of \npain in Fabry disease and its response to enzyme replacement therapy - A retrospective analysis from the \nFabry outcome survey, Clin. J. Pain. 23 (2007) 535–542. https://doi.org/10.1097/AJP.0b013e318074c986. \n[173] L. De Ferrari, S. Aitken, J. van Hemert, I. Goryanin, EnzML: Multi-label prediction of enzyme classes using \nInterPro signatures, BMC Bioinformatics. 13 (2012). https://doi.org/10.1186/1471-2105-13-61. \n[174] Z. Zou, S. Tian, X. Gao, Y. Li, mlDEEPre: Multi-functional enzyme function prediction with hierarchical \nmulti-label deep learning, Front. Genet. 10 (2019) 1–10. https://doi.org/10.3389/fgene.2018.00714. \n[175] A.. Sutanta, H.; Bishop, I.D.B.; Rajabifard, Integrating spatial planning and disaster risk reduction at the \nlocal level in the context of spatially enabled government. In Spatially Enabling Society Research, Emerging \nTrends and Critical Assessment, Abbas, R., Joep, C., Kalantari, M., Kok, B., Eds.; Leuven Univ. Press \nLeuven, Belgium. (2010) 55–68. \n[176] I.A.T. Hashem, I. Yaqoob, N.B. Anuar, S. Mokhtar, A. Gani, S. Ullah Khan, The rise of “big data” on cloud \ncomputing: Review and open research issues, Inf. Syst. 47 (2015) 98–115. \nhttps://doi.org/10.1016/j.is.2014.07.006. \n[177] M. Anbarasan, B.A. Muthu, C.B. Sivaparthipan, R. Sundarasekar, S. Kadry, S. Krishnamoorthy, D.J. \nSamuel, A.A. Dasel, Detection of flood disaster system based on IoT, big data and convolutional deep \nneural network, Comput. Commun. 150 (2020) 150–157. https://doi.org/10.1016/j.comcom.2019.11.022. \n[178] T. Celik, H. Demirel, H. Ozkaramanli, M. Uyguroglu, Fire detection using statistical color model in video \nsequences, J. Vis. Commun. Image Represent. 18 (2007) 176–185. \nhttps://doi.org/10.1016/j.jvcir.2006.12.003. \n[179] I. Mehmood, M. Sajjad, S.W. Baik, Mobile-cloud assisted video summarization framework for efficient \nmanagement of remote sensing data generated by wireless capsule sensors, 2014. \nhttps://doi.org/10.3390/s140917112. \n[180] K. Muhammad, J. Ahmad, S.W. Baik, Early fire detection using convolutional neural networks during \nsurveillance for effective disaster management, Neurocomputing. 288 (2018) 30–42. \nhttps://doi.org/10.1016/j.neucom.2017.04.083. \n[181] S. Verstockt, T. Beji, P. De Potter, S. Van Hoecke, B. Sette, B. Merci, R. Van De Walle, Video driven fire \nspread forecasting (f) using multi-modal LWIR and visual flame and smoke data, Pattern Recognit. Lett. 34 \n(2013) 62–69. https://doi.org/10.1016/j.patrec.2012.07.018. \n[182] M.L. Shyu, S.C. Chen, Emerging Multimedia Research and Applications, IEEE Multimed. 22 (2015) 11–13. \nhttps://doi.org/10.1109/MMUL.2015.84. \n[183] S. Pouyanfar, Y. Yang, S.C. Chen, M.L. Shyu, S.S. Iyengar, Multimedia big data analytics: A survey, ACM \nComput. Surv. 51 (2018). https://doi.org/10.1145/3150226. \n[184] S. Pouyanfar, Y. Tao, H. Tian, S.C. Chen, M.L. Shyu, Multimodal deep learning based on multiple \ncorrespondence analysis for disaster management, World Wide Web. 22 (2019) 1893–1911. \nhttps://doi.org/10.1007/s11280-018-0636-4. \n[185] Q. Zhang, M. Lu, X. Dong, C. Wang, C. Zhang, W. Liu, M. Zhao, Potential estrogenic effects of \nphosphorus-containing flame retardants, Environ. Sci. Technol. 48 (2014) 6995–7001. \nhttps://doi.org/10.1021/es5007862. \n[186] Q. Zhang, L. Yan, Y. Wu, L. Ji, Y. Chen, M. Zhao, X. Dong, A ternary classification using machine \nlearning methods of distinct estrogen receptor activities within a large collection of environmental \nchemicals, Sci. Total Environ. 580 (2017) 1268–1275. https://doi.org/10.1016/j.scitotenv.2016.12.088. \n[187] S.K. Heo, U. Safder, C.K. Yoo, Deep learning driven QSAR model for environmental toxicology: Effects of \nendocrine disrupting chemicals on human health, Environ. Pollut. 253 (2019) 29–38. \nhttps://doi.org/10.1016/j.envpol.2019.06.081. \n[188] P.A. Wender, R. V. Quiroz, M.C. Stevens, Function through Synthesis-Informed Design, Acc. Chem. Res. \n48 (2015) 752–760. https://doi.org/10.1021/acs.accounts.5b00004. \n58 \n \n[189] S. V. Ley, D.E. Fitzpatrick, R.J. Ingham, R.M. Myers, Organic synthesis: March of the machines, Angew. \nChemie - Int. Ed. 54 (2015) 3449–3464. https://doi.org/10.1002/anie.201410744. \n[190] M.H.S. Segler, M.P. Waller, Neural-Symbolic Machine Learning for Retrosynthesis and Reaction \nPrediction, Chem. - A Eur. J. 23 (2017) 5966–5971. https://doi.org/10.1002/chem.201605499. \n[191] T. Lyu, J. Gao, L. Tian, Z. Li, P. Zhang, J. Zhang, MDNN: A Multimodal Deep Neural Network for \nPredicting Drug-Drug Interaction Events, IJCAI Int. Jt. Conf. Artif. Intell. (2021) 3536–3542. \nhttps://doi.org/10.24963/ijcai.2021/487. \n[192] Y. Deng, X. Xu, Y. Qiu, J. Xia, W. Zhang, S. Liu, A multimodal deep learning framework for predicting \ndrug-drug interaction events, Bioinformatics. 36 (2020) 4316–4322. \nhttps://doi.org/10.1093/bioinformatics/btaa501. \n[193] H. Gao, J.M. Korn, S. Ferretti, J.E. Monahan, Y. Wang, M. Singh, C. Zhang, C. Schnell, G. Yang, Y. \nZhang, O.A. Balbin, S. Barbe, H. Cai, F. Casey, S. Chatterjee, D.Y. Chiang, S. Chuai, S.M. Cogan, S.D. \nCollins, E. Dammassa, N. Ebel, M. Embry, J. Green, A. Kauffmann, C. Kowal, R.J. Leary, J. Lehar, Y. \nLiang, A. Loo, E. Lorenzana, E. Robert McDonald, M.E. McLaughlin, J. Merkin, R. Meyer, T.L. Naylor, M. \nPatawaran, A. Reddy, C. Röelli, D.A. Ruddy, F. Salangsang, F. Santacroce, A.P. Singh, Y. Tang, W. \nTinetto, S. Tobler, R. Velazquez, K. Venkatesan, F. Von Arx, H.Q. Wang, Z. Wang, M. Wiesmann, D. \nWyss, F. Xu, H. Bitter, P. Atadja, E. Lees, F. Hofmann, E. Li, N. Keen, R. Cozens, M.R. Jensen, N.K. Pryer, \nJ.A. Williams, W.R. Sellers, High-throughput screening using patient-derived tumor xenografts to predict \nclinical trial drug response, Nat. Med. 21 (2015) 1318–1325. https://doi.org/10.1038/nm.3954. \n[194] A. Gottlieb, G.Y. Stein, Y. Oron, E. Ruppin, R. Sharan, INDI: A computational framework for inferring \ndrug interactions and their associated recommendations, Mol. Syst. Biol. 8 (2012) 1–12. \nhttps://doi.org/10.1038/msb.2012.26. \n[195] G. Lee, C. Park, J. Ahn, Novel deep learning model for more accurate prediction of drug-drug interaction \neffects, BMC Bioinformatics. 20 (2019) 1–8. https://doi.org/10.1186/s12859-019-3013-0. \n[196] M. Raissi, P. Perdikaris, G.E. Karniadakis, Physics-informed neural networks: A deep learning framework \nfor solving forward and inverse problems involving nonlinear partial differential equations, J. Comput. Phys. \n378 (2019) 686–707. https://doi.org/10.1016/j.jcp.2018.10.045. \n[197] A.G. Wilson, H. Nickisch, Kernel interpolation for scalable structured Gaussian processes (KISS-GP), 32nd \nInt. Conf. Mach. Learn. ICML 2015. 3 (2015) 1775–1784. \n[198] Y. Zhu, N. Zabaras, P.S. Koutsourelakis, P. Perdikaris, Physics-constrained deep learning for high-\ndimensional surrogate modeling and uncertainty quantification without labeled data, J. Comput. Phys. 394 \n(2019) 56–81. https://doi.org/10.1016/j.jcp.2019.05.024. \n[199] R.E. Morgan, B. Statistician, Financial Fraud in the United States, 2017, (2021). \nhttps://doi.org/10.17226/23492. \n[200] S. Ounacer, H. Ait, E. Bour, Y. Oubrahim, M.Y. Ghoumari, M. Azzouazi, Using isolation forest in anomaly \ndetection: The case of credit card transactions, Period. Eng. Nat. Sci. 6 (2018) 394–400. \nhttps://doi.org/10.21533/PEN.V6I2.533.G312. \n[201] A.M. Mubalaike, E. Adali, Deep Learning Approach for Intelligent Financial Fraud Detection System, \nUBMK 2018 - 3rd Int. Conf. Comput. Sci. Eng. (2018) 598–603. \nhttps://doi.org/10.1109/UBMK.2018.8566574. \n[202] S. Makki, Z. Assaghir, Y. Taher, R. Haque, M.S. Hacid, H. Zeineddine, An Experimental Study With \nImbalanced Classification Approaches for Credit Card Fraud Detection, IEEE Access. 7 (2019) 93010–\n93022. https://doi.org/10.1109/ACCESS.2019.2927266. \n[203] S. Sanober, I. Alam, S. Pande, F. Arslan, K.P. Rane, B.K. Singh, A. Khamparia, M. Shabaz, An Enhanced \nSecure Deep Learning Algorithm for Fraud Detection in Wireless Communication, (2021). \nhttps://doi.org/10.1155/2021/6079582. \n[204] Y. Alghofaili, A. Albattah, M.A. Rassam, A Financial Fraud Detection Model Based on LSTM Deep \nLearning Technique, Https://Doi.Org/10.1080/19361610.2020.1815491. 15 (2020) 498–516. \n59 \n \nhttps://doi.org/10.1080/19361610.2020.1815491. \n[205] D. Sculley, M.E. Otey, M. Pohl, B. Spitznagel, J. Hainsworth, Y. Zhou, Detecting adversarial \nadvertisements in the wild, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. (2011) 274–282. \nhttps://doi.org/10.1145/2020408.2020455. \n[206] S.H. Li, D.C. Yen, W.H. Lu, C. Wang, Identifying the signs of fraudulent accounts using data mining \ntechniques, Comput. Human Behav. 28 (2012) 1002–1013. https://doi.org/10.1016/J.CHB.2012.01.002. \n[207] I. Benchaji, S. Douzi, B. El Ouahidi, Credit card fraud detection model based on LSTM recurrent neural \nnetworks, J. Adv. Inf. Technol. 12 (2021) 113–118. https://doi.org/10.12720/JAIT.12.2.113-118. \n[208] Y. Heryadi, H.L.H.S. Warnars, Learning temporal representation of transaction amount for fraudulent \ntransaction recognition using CNN, Stacked LSTM, and CNN-LSTM, 2017 IEEE Int. Conf. Cybern. \nComput. Intell. Cybern. 2017 - Proc. 2017-Novem (2018) 84–89. \nhttps://doi.org/10.1109/CYBERNETICSCOM.2017.8311689. \n[209] A. Roy, J. Sun, R. Mahoney, L. Alonzi, S. Adams, P. Beling, Deep learning detecting fraud in credit card \ntransactions, 2018 Syst. Inf. Eng. Des. Symp. SIEDS 2018. (2018) 129–134. \nhttps://doi.org/10.1109/SIEDS.2018.8374722. \n[210] I. Md. Sanzidul, S.M. Sadia Sultana, S. Abujar, S.A. Hossain, Sequence-to-sequence Bangla Sentence \nGeneration with LSTM Recurrent Neural Networks, Procedia Comput. Sci. 152 (2019) 51–58. \nhttps://doi.org/10.1016/J.PROCS.2019.05.026. \n[211] Y. Pandey, Credit card fraud detection using deep learning, Undefined. (2017). \nhttps://doi.org/10.26483/IJARCS.V8I5.3514. \n[212] A. Pumsirirat, L. Yan, Credit Card Fraud Detection using Deep Learning based on Auto-Encoder and \nRestricted Boltzmann Machine, Int. J. Adv. Comput. Sci. Appl. 9 (2018) 18–25. \nhttps://doi.org/10.14569/IJACSA.2018.090103. \n[213] D. De Roux, B. Pérez, A. Moreno, M. Del Pilar Villamil, C. Figueroa, Tax fraud detection for under-\nreporting declarations using an unsupervised machine learning approach, Proc. ACM SIGKDD Int. Conf. \nKnowl. Discov. Data Min. (2018) 215–222. https://doi.org/10.1145/3219819.3219878. \n[214] C. Lee, Deep learning-based detection of tax frauds: an application to property acquisition tax, Data \nTechnol. Appl. (2021). https://doi.org/10.1108/DTA-06-2021-0134/FULL/XML. \n[215] C.P. López, M.J.D. Rodríguez, S. de L. Santos, Tax Fraud Detection through Neural Networks: An \nApplication Using a Sample of Personal Income Taxpayers, Futur. Internet 2019, Vol. 11, Page 86. 11 \n(2019) 86. https://doi.org/10.3390/FI11040086. \n[216] I.H. Sarker, Ai-based modeling: Techniques, applications and research issues towards automation, \nintelligent and smart systems, SN Comput. Sci. 3 (2022) 158. \n[217] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G.V. Hernandez, L. Krpalkova, D. Riordan, J. \nWalsh, Deep learning vs. traditional computer vision, in: Adv. Comput. Vis. Proc. 2019 Comput. Vis. Conf. \n(CVC), Vol. 1 1, Springer, 2020: pp. 128–144. \n[218] X. Wu, D. Sahoo, S.C.H. Hoi, Recent advances in deep learning for object detection, Neurocomputing. 396 \n(2020) 39–64. https://doi.org/10.1016/J.NEUCOM.2020.01.085. \n[219] R. Nithya, B. Santhi, R. Manikandan, M. Rahimi, A.H. Gandomi, Computer Vision System for Mango Fruit \nDefect Detection Using Deep Convolutional Neural Network, Foods. 11 (2022) 3483. \n[220] A. Behera, Z. Wharton, A. Keidel, B. Debnath, Deep cnn, body pose, and body-object interaction features \nfor drivers’ activity monitoring, IEEE Trans. Intell. Transp. Syst. 23 (2020) 2874–2881. \n[221] S.S. Sumit, J. Watada, A. Roy, D.R.A. Rambli, In object detection deep learning methods, YOLO shows \nsupremum to Mask R-CNN, in: J. Phys. Conf. Ser., IOP Publishing, 2020: p. 42086. \n[222] B. Manifold, S. Men, R. Hu, D. Fu, A versatile deep learning architecture for classification and label-free \nprediction of hyperspectral images, Nat. Mach. Intell. 3 (2021) 306–315. \n60 \n \n[223] G. Latif, S.E. Abdelhamid, R.E. Mallouhy, J. Alghazo, Z.A. Kazimi, Deep Learning Utilization in \nAgriculture: Detection of Rice Plant Diseases Using an Improved CNN Model, Plants. 11 (2022) 2230. \n[224] A.R. Pathak, M. Pandey, S. Rautaray, Application of Deep Learning for Object Detection, Procedia Comput. \nSci. 132 (2018) 1706–1717. https://doi.org/10.1016/J.PROCS.2018.05.144. \n[225] W. Ouyang, X. Zeng, X. Wang, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang, Z. Wang, H. Li, K. Wang, J. Yan, \nC.C. Loy, X. Tang, DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural \nNetworks, IEEE Trans. Pattern Anal. Mach. Intell. 39 (2017) 1320–1334. \nhttps://doi.org/10.1109/TPAMI.2016.2587642. \n[226] N. Doulamis, A. Doulamis, Semi-supervised deep learning for object tracking and classification, 2014 IEEE \nInt. Conf. Image Process. ICIP 2014. (2014) 848–852. https://doi.org/10.1109/ICIP.2014.7025170. \n[227] M. Taskiran, N. Kahraman, C.E. Erdem, Face recognition: Past, present and future (a review), Digit. Signal \nProcess. 106 (2020) 102809. \n[228] Y. Guo, L. Zhang, Y. Hu, X. He, J. Gao, MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face \nRecognition, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes \nBioinformatics). 9907 LNCS (2016) 87–102. https://doi.org/10.1007/978-3-319-46487-9_6. \n[229] A. Zadeh, Y.C. Lim, T. Baltrušaitis, L.P. Morency, Convolutional experts constrained local model for 3D \nfacial landmark detection, Proc. - 2017 IEEE Int. Conf. Comput. Vis. Work. ICCVW 2017. 2018-Janua \n(2017) 2519–2528. https://doi.org/10.1109/ICCVW.2017.296. \n[230] V. Cirik, E. Hovy, L.-P. Morency, Visualizing and Understanding Curriculum Learning for Long Short-\nTerm Memory Networks, Undefined. (2016). \n[231] J. Deng, S. Cheng, N. Xue, Y. Zhou, S. Zafeiriou, UV-GAN: Adversarial Facial UV Map Completion for \nPose-Invariant Face Recognition, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (2018) \n7093–7102. https://doi.org/10.1109/CVPR.2018.00741. \n[232] Y. Liu, F. Wei, J. Shao, L. Sheng, J. Yan, X. Wang, Exploring Disentangled Feature Representation beyond \nFace Identification, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (2018) 2080–2089. \nhttps://doi.org/10.1109/CVPR.2018.00222. \n[233] A. Nech, I. Kemelmacher-Shlizerman, Level Playing Field for Million Scale Face Recognition, Proc. - 30th \nIEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017. 2017-Janua (2017) 3406–3415. \nhttps://doi.org/10.1109/CVPR.2017.363. \n[234] A. Bansal, A. Nanduri, C.D. Castillo, R. Ranjan, R. Chellappa, UMDFaces: An Annotated Face Dataset for \nTraining Deep Networks, IEEE Int. Jt. Conf. Biometrics, IJCB 2017. 2018-Janua (2016) 464–473. \nhttps://doi.org/10.1109/BTAS.2017.8272731. \n[235] A. Jain, V. Kanhangad, Human Activity Classification in Smartphones Using Accelerometer and Gyroscope \nSensors, IEEE Sens. J. 18 (2018) 1169–1177. https://doi.org/10.1109/JSEN.2017.2782492. \n[236] N. Jalloul, F. Poree, G. Viardot, P. L Hostis, G. Carrault, Activity Recognition Using Complex Network \nAnalysis, IEEE J. Biomed. Heal. Informatics. 22 (2018) 989–1000. \nhttps://doi.org/10.1109/JBHI.2017.2762404. \n[237] M.O. Mario, Human activity recognition based on single sensor square HV acceleration images and \nconvolutional neural networks, IEEE Sens. J. 19 (2019) 1487–1498. \nhttps://doi.org/10.1109/JSEN.2018.2882943. \n[238] Y. Lin, J. Le Kernec, S. Yang, F. Fioranelli, O. Romain, Z. Zhao, Human Activity Classification with Radar: \nOptimization and Noise Robustness with Iterative Convolutional Neural Networks Followed with Random \nForests, IEEE Sens. J. 18 (2018) 9669–9681. https://doi.org/10.1109/JSEN.2018.2872849. \n[239] F.J. Ordóñez, D. Roggen, Y. Liu, W. Xiao, H.-C. Chao, P. Chu, Deep Convolutional and LSTM Recurrent \nNeural Networks for Multimodal Wearable Activity Recognition, Sensors 2016, Vol. 16, Page 115. 16 \n(2016) 115. https://doi.org/10.3390/S16010115. \n[240] P. Agarwal, M. Alam, A Lightweight Deep Learning Model for Human Activity Recognition on Edge \n61 \n \nDevices, Procedia Comput. Sci. 167 (2020) 2364–2373. https://doi.org/10.1016/J.PROCS.2020.03.289. \n[241] K. Xia, J. Huang, H. Wang, LSTM-CNN Architecture for Human Activity Recognition, IEEE Access. 8 \n(2020) 56855–56866. https://doi.org/10.1109/ACCESS.2020.2982225. \n[242] Y. Chen, Y. Tian, M. He, Monocular human pose estimation: A survey of deep learning-based methods, \nUndefined. 192 (2020). https://doi.org/10.1016/J.CVIU.2019.102897. \n[243] A. Toshev, C. Szegedy, DeepPose: Human Pose Estimation via Deep Neural Networks, Proc. IEEE Comput. \nSoc. Conf. Comput. Vis. Pattern Recognit. (2013) 1653–1660. https://doi.org/10.1109/CVPR.2014.214. \n[244] T. Pfister, K. Simonyan, J. Charles, A. Zisserman, Deep convolutional neural networks for efficient pose \nestimation in gesture videos, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. \nNotes Bioinformatics). 9003 (2015) 538–552. https://doi.org/10.1007/978-3-319-16865-4_35. \n[245] W. Yang, W. Ouyang, X. Wang, J. Ren, H. Li, X. Wang, 3D Human Pose Estimation in the Wild by \nAdversarial Learning, Undefined. (2018) 5255–5264. https://doi.org/10.1109/CVPR.2018.00551. \n[246] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler, K. Murphy, Towards Accurate \nMulti-person Pose Estimation in the Wild, Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, \nCVPR 2017. 2017-Janua (2017) 3711–3719. https://doi.org/10.1109/CVPR.2017.395. \n[247] X. Perez-Sala, S. Escalera, C. Angulo, J. Gonzàlez, A Survey on Model Based Approaches for 2D and 3D \nVisual Human Pose Recovery, Sensors (Basel). 14 (2014) 4189. https://doi.org/10.3390/S140304189. \n[248] P. Wang, W. Li, P. Ogunbona, J. Wan, S. Escalera, RGB-D-based human motion recognition with deep \nlearning: A survey, Comput. Vis. Image Underst. 171 (2018) 118–139. \nhttps://doi.org/10.1016/J.CVIU.2018.04.007. \n[249] W. Gong, X. Zhang, J. Gonzàlez, A. Sobral, T. Bouwmans, C. Tu, E.H. Zahzah, Human Pose Estimation \nfrom Monocular Images: A Comprehensive Survey, Sensors (Basel). 16 (2016). \nhttps://doi.org/10.3390/S16121966. \n[250] N. Sarafianos, B. Boteanu, B. Ionescu, I.A. Kakadiaris, 3D Human pose estimation: A review of the \nliterature and analysis of covariates, Comput. Vis. Image Underst. 152 (2016) 1–20. \nhttps://doi.org/10.1016/J.CVIU.2016.09.002. \n[251] J.J. Valletta, C. Torney, M. Kings, A. Thornton, J. Madden, Applications of machine learning in animal \nbehaviour studies, Anim. Behav. 124 (2017) 203–220. https://doi.org/10.1016/J.ANBEHAV.2016.12.005. \n[252] R. Mandal, R.M. Connolly, T.A. Schlacher, B. Stantic, Assessing fish abundance from underwater video \nusing deep neural networks, Proc. Int. Jt. Conf. Neural Networks. 2018-July (2018). \nhttps://doi.org/10.1109/IJCNN.2018.8489482. \n[253] B.D. Satoto, I. Utoyo, R. Rulaningtyas, E.B. Khoendori, An improvement of Gram-negative bacteria \nidentification using convolutional neural network with fine tuning, TELKOMNIKA (Telecommunication \nComput. Electron. Control. 18 (2020) 1397–1405. https://doi.org/10.12928/TELKOMNIKA.V18I3.14890. \n[254] J. Carranza-Rojas, H. Goeau, P. Bonnet, E. Mata-Montero, A. Joly, Going deeper in the automated \nidentification of Herbarium specimens, BMC Evol. Biol. 17 (2017) 181. https://doi.org/10.1186/S12862-\n017-1014-Z/FIGURES/7. \n[255] E. Schuettpelz, P.B. Frandsen, R.B. Dikow, A. Brown, S. Orli, M. Peters, A. Metallo, V.A. Funk, L.J. Dorr, \nApplications of deep convolutional neural networks to digitized natural history collections, Biodivers. Data \nJ. 5 E21139. 5 (2017) e21139-. https://doi.org/10.3897/BDJ.5.E21139. \n[256] M. Valan, K. Makonyi, A. Maki, D. Vondráček, F. Ronquist, Automated Taxonomic Identification of \nInsects with Expert-Level Accuracy Using Effective Feature Transfer from Convolutional Networks, Syst. \nBiol. 68 (2019) 876–895. https://doi.org/10.1093/SYSBIO/SYZ014. \n[257] O.L.P. Hansen, J.C. Svenning, K. Olsen, S. Dupont, B.H. Garner, A. Iosifidis, B.W. Price, T.T. Høye, \nSpecies-level image classification with convolutional neural network enables insect identification from \nhabitus images, Ecol. Evol. 10 (2020) 737–747. https://doi.org/10.1002/ECE3.5921. \n62 \n \n[258] S. Villon, D. Mouillot, M. Chaumont, E.S. Darling, G. Subsol, T. Claverie, S. Villéger, A Deep learning \nmethod for accurate and fast identification of coral reef fishes in underwater images, Ecol. Inform. 48 (2018) \n238–244. https://doi.org/10.1016/J.ECOINF.2018.09.007. \n[259] X. Liu, H. Song, Automatic identification of fossils and abiotic grains during carbonate microfacies analysis \nusing deep convolutional neural networks, Sediment. Geol. 410 (2020) 105790. \nhttps://doi.org/10.1016/J.SEDGEO.2020.105790. \n[260] S. Younis, C. Weiland, R. Hoehndorf, S. Dressler, T. Hickler, B. Seeger, M. Schmidt, Taxon and trait \nrecognition from digitized herbarium specimens using deep convolutional neural networks, \nHttps://Doi.Org/10.1080/23818107.2018.1446357. 165 (2018) 377–383. \nhttps://doi.org/10.1080/23818107.2018.1446357. \n[261] M.S. Norouzzadeh, A. Nguyen, M. Kosmala, A. Swanson, M.S. Palmer, C. Packer, J. Clune, Automatically \nidentifying, counting, and describing wild animals in camera-trap images with deep learning, Proc. Natl. \nAcad. Sci. U. S. A. 115 (2018) E5716–E5725. https://doi.org/10.1073/PNAS.1719367115/-\n/DCSUPPLEMENTAL. \n[262] R.P. de Lima, K.F. Welch, J.E. Barrick, K.J. Marfurt, R. Burkhalter, M. Cassel, G.S. Soreghan, \nCONVOLUTIONAL NEURAL NETWORKS AS AN AID TO BIOSTRATIGRAPHY AND \nMICROPALEONTOLOGY: A TEST ON LATE PALEOZOIC MICROFOSSILS, Palaios. 35 (2020) 391–\n402. https://doi.org/10.2110/PALO.2019.102. \n[263] E.M. Ditria, S. Lopez-Marcano, M. Sievers, E.L. Jinks, C.J. Brown, R.M. Connolly, Automating the \nAnalysis of Fish Abundance Using Object Detection: Optimizing Animal Ecology With Deep Learning, \nFront. Mar. Sci. 7 (2020) 429. https://doi.org/10.3389/FMARS.2020.00429/BIBTEX. \n[264] U. Kälin, N. Lang, C. Hug, A. Gessler, J.D. Wegner, Defoliation estimation of forest trees from ground-\nlevel images, BioRxiv. (2018) 441733. https://doi.org/10.1101/441733. \n[265] P.S. Soltis, Digitization of herbaria enables novel research, Am. J. Bot. 104 (2017) 1281–1284. \nhttps://doi.org/10.3732/AJB.1700281. \n[266] C.G. Willis, E.R. Ellwood, R.B. Primack, C.C. Davis, K.D. Pearson, A.S. Gallinat, J.M. Yost, G. Nelson, \nS.J. Mazer, N.L. Rossington, T.H. Sparks, P.S. Soltis, Old Plants, New Tricks: Phenological Research Using \nHerbarium Specimens, Trends Ecol. Evol. 32 (2017) 531–546. https://doi.org/10.1016/J.TREE.2017.03.015. \n[267] S.H. Lee, C.S. Chan, P. Remagnino, Multi-Organ Plant Classification Based on Convolutional and \nRecurrent Neural Networks, IEEE Trans. Image Process. 27 (2018) 4287–4301. \nhttps://doi.org/10.1109/TIP.2018.2836321. \n[268] A.C.R. Marques, M.M. Raimundo, E.M.B. Cavalheiro, L.F.P. Salles, C. Lyra, F.J. Von Zuben, Ant genera \nidentification using an ensemble of convolutional neural networks, PLoS One. 13 (2018) e0192011. \nhttps://doi.org/10.1371/JOURNAL.PONE.0192011. \n[269] J. Wäldchen, P. Mäder, Machine learning for image based species identification, Methods Ecol. Evol. 9 \n(2018) 2216–2225. https://doi.org/10.1111/2041-210X.13075. \n[270] O. Zikanov, Essential computational fluid dynamics, second edition., JOHN WILEY & Sons, [Place of \npublication not identified], 2019. \n[271] J. Nathan Kutz, Deep learning in fluid dynamics, (2022). https://doi.org/10.1017/jfm.2016.803. \n[272] E. Fonda, A. Pandey, J. Schumacher, K.R. Sreenivasan, Deep learning in turbulent convection networks, \nProc. Natl. Acad. Sci. U. S. A. 116 (2019) 8667–8672. https://doi.org/10.1073/PNAS.1900358116/-\n/DCSUPPLEMENTAL. \n[273] A. Daw, A. Karpatne, W. Watkins, J. Read, V. Kumar, Physics-guided Neural Networks (PGNN): An \nApplication in Lake Temperature Modeling, (2017). \n[274] G. Dimitriu, R. Ştefănescu, I.M. Navon, Comparative numerical analysis using reduced-order modeling \nstrategies for nonlinear large-scale systems, J. Comput. Appl. Math. 310 (2017) 32–43. \nhttps://doi.org/10.1016/J.CAM.2016.07.002. \n63 \n \n[275] F. Fang, C.C. Pain, I.M. Navon, A.H. Elsheikh, J. Du, D. Xiao, Non-linear Petrov–Galerkin methods for \nreduced order hyperbolic equations and discontinuous finite element methods, J. Comput. Phys. 234 (2013) \n540–559. https://doi.org/10.1016/J.JCP.2012.10.011. \n[276] F. Fang, C.C. Pain, I.M. Navon, D. Xiao, An efficient goal-based reduced order model approach for targeted \nadaptive observations, Int. J. Numer. Methods Fluids. 83 (2017) 263–275. \nhttps://doi.org/10.1002/FLD.4265. \n[277] M. Diez, E.F. Campana, F. Stern, Design-space dimensionality reduction in shape optimization by \nKarhunen–Loève expansion, Comput. Methods Appl. Mech. Eng. 283 (2015) 1525–1544. \nhttps://doi.org/10.1016/J.CMA.2014.10.042. \n[278] A. Manzoni, F. Salmoiraghi, L. Heltai, Reduced Basis Isogeometric Methods (RB-IGA) for the real-time \nsimulation of potential flows about parametrized NACA airfoils, Comput. Methods Appl. Mech. Eng. 284 \n(2015) 1147–1180. https://doi.org/10.1016/J.CMA.2014.11.037. \n[279] M. Alotaibi, V.M. Calo, Y. Efendiev, J. Galvis, M. Ghommem, Global–local nonlinear model reduction for \nflows in heterogeneous porous media, Comput. Methods Appl. Mech. Eng. 292 (2015) 122–137. \nhttps://doi.org/10.1016/J.CMA.2014.10.034. \n[280] Z. Wang, D. Xiao, F. Fang, R. Govindan, C.C. Pain, Y. Guo, Model identification of reduced order fluid \ndynamics systems using deep learning, Int. J. Numer. Methods Fluids. 86 (2018) 255–268. \nhttps://doi.org/10.1002/FLD.4416. \n[281] A. Lozovskiy, M. Farthing, C. Kees, E. Gildin, POD-based model reduction for stabilized finite element \napproximations of shallow water flows, J. Comput. Appl. Math. 302 (2016) 50–70. \nhttps://doi.org/10.1016/J.CAM.2016.01.029. \n[282] J. Oliver, M. Caicedo, A.E. Huespe, J.A. Hernández, E. Roubin, Reduced order modeling strategies for \ncomputational multiscale fracture, Comput. Methods Appl. Mech. Eng. 313 (2017) 560–595. \nhttps://doi.org/10.1016/J.CMA.2016.09.039. \n[283] J. Kou, W. Zhang, Data-driven modeling for unsteady aerodynamics and aeroelasticity, Prog. Aerosp. Sci. \n125 (2021) 100725. \n[284] U.P. Singh, S.S. Chouhan, S. Jain, S. Jain, Multilayer Convolution Neural Network for the Classification of \nMango Leaves Infected by Anthracnose Disease, IEEE Access. 7 (2019). \nhttps://doi.org/10.1109/ACCESS.2019.2907383. \n[285] N.S.K.R.S. Karim Ahmed, Weighted Transformer Network for Machine Translation, (n.d.). \n[286] S. Karumuri, R. Tripathy, I. Bilionis, J. Panchal, Simulator-free solution of high-dimensional stochastic \nelliptic partial differential equations using deep neural networks, J. Comput. Phys. 404 (2020) 1–63. \nhttps://doi.org/10.1016/j.jcp.2019.109120. \n[287] N. Rusk, Deep learning, Nat. Methods. 13 (2015) 35. https://doi.org/10.1038/nmeth.3707. \n[288] T. Wang, C.K. Wen, H. Wang, F. Gao, T. Jiang, S. Jin, Deep learning for wireless physical layer: \nOpportunities and challenges, China Commun. 14 (2017) 92–111. \nhttps://doi.org/10.1109/CC.2017.8233654. \n[289] M. Shyu, S. Chen, S.S. Iyengar, a Survey on Deep Learning Techniques, Strad Res. 7 (2020). \nhttps://doi.org/10.37896/sr7.8/037. \n[290] R. Miotto, F. Wang, S. Wang, X. Jiang, J.T. Dudley, Deep learning for healthcare : review , opportunities \nand challenges, Brief. Bioinform. 19 (2018) 1236–1246. https://doi.org/10.1093/bib/bbx044. \n[291] A. Rajkomar, E. Oren, K. Chen, A.M. Dai, N. Hajaj, M. Hardt, P.J. Liu, X. Liu, J. Marcus, M. Sun, P. \nSundberg, H. Yee, K. Zhang, Y. Zhang, G. Flores, G.E. Duggan, J. Irvine, Q. Le, K. Litsch, A. Mossin, J. \nTansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig, S.L. Volchenboum, K. Chou, M. Pearson, S. \nMadabushi, N.H. Shah, A.J. Butte, M.D. Howell, C. Cui, G.S. Corrado, J. Dean, Scalable and accurate deep \nlearning with electronic health records, Npj Digit. Med. 1 (2018) 1–10. https://doi.org/10.1038/s41746-018-\n0029-1. \n64 \n \n[292] I.M. Baytas, C. Xiao, X. Zhang, F. Wang, A.K. Jain, J. Zhou, Patient subtyping via time-aware LSTM \nnetworks, Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. Part F1296 (2017) 65–74. \nhttps://doi.org/10.1145/3097983.3097997. \n[293] A.N. Jagannatha, H. Yu, Structured prediction models for RNN based sequence labeling in clinical text, \nEMNLP 2016 - Conf. Empir. Methods Nat. Lang. Process. Proc. (2016) 856–865. \nhttps://doi.org/10.18653/v1/d16-1082. \n[294] T. Pham, T. Tran, D. Phung, S. Venkatesh, DeepCare: A deep dynamic memory model for predictive \nmedicine, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics). \n9652 LNAI (2016) 30–41. https://doi.org/10.1007/978-3-319-31750-2_3. \n[295] M.K. Awasthi, S. Sarsaiya, S. Wainaina, K. Rajendran, S.K. Awasthi, T. Liu, Y. Duan, A. Jain, R. Sindhu, \nP. Binod, A. Pandey, Z. Zhang, M.J. Taherzadeh, Techno-economics and life-cycle assessment of biological \nand thermochemical treatment of bio-waste, Renew. Sustain. Energy Rev. 144 (2021) 110837. \nhttps://doi.org/10.1016/j.rser.2021.110837. \n[296] E. Choi, M.T. Bahadori, A. Schuetz, W.F. Stewart, J. Sun, Doctor AI: Predicting Clinical Events via \nRecurrent Neural Networks., JMLR Workshop Conf. Proc. 56 (2016) 301–318. \n[297] Z.C. Lipton, The mythos of model interpretability, Commun. ACM. 61 (2018) 35–43. \nhttps://doi.org/10.1145/3233231. \n[298] P.W. Koh, P. Liang, Understanding black-box predictions via influence functions, 34th Int. Conf. Mach. \nLearn. ICML 2017. 4 (2017) 2976–2987. \n[299] S.C. Huang, A. Pareek, S. Seyyedi, I. Banerjee, M.P. Lungren, Fusion of medical imaging and electronic \nhealth records using deep learning: a systematic review and implementation guidelines, Npj Digit. Med. 3 \n(2020). https://doi.org/10.1038/s41746-020-00341-z. \n[300] A.M. Alaa, M. Weisz, M. van der Schaar, Deep Counterfactual Networks with Propensity-Dropout, (2017). \n[301] N. Razavian, J. Marcus, D. Sontag, Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests, \n(2016) 1–27. \n[302] R. Henao, J.T. Lu, J.E. Lucas, J. Ferranti, L. Carin, Electronic health record analysis via deep poisson factor \nmodels, J. Mach. Learn. Res. 17 (2016) 1–32. \n \n \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE",
    "68T07"
  ],
  "published": "2023-09-06",
  "updated": "2023-09-06"
}