{
  "id": "http://arxiv.org/abs/2309.06896v1",
  "title": "Domain-Aware Augmentations for Unsupervised Online General Continual Learning",
  "authors": [
    "Nicolas Michel",
    "Romain Negrel",
    "Giovanni Chierchia",
    "Jean-François Bercher"
  ],
  "abstract": "Continual Learning has been challenging, especially when dealing with\nunsupervised scenarios such as Unsupervised Online General Continual Learning\n(UOGCL), where the learning agent has no prior knowledge of class boundaries or\ntask change information. While previous research has focused on reducing\nforgetting in supervised setups, recent studies have shown that self-supervised\nlearners are more resilient to forgetting. This paper proposes a novel approach\nthat enhances memory usage for contrastive learning in UOGCL by defining and\nusing stream-dependent data augmentations together with some implementation\ntricks. Our proposed method is simple yet effective, achieves state-of-the-art\nresults compared to other unsupervised approaches in all considered setups, and\nreduces the gap between supervised and unsupervised continual learning. Our\ndomain-aware augmentation procedure can be adapted to other replay-based\nmethods, making it a promising strategy for continual learning.",
  "text": "MICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n1\nDomain-Aware Augmentations for\nUnsupervised Online General Continual\nLearning\nNicolas Michel\nnicolas.michel@esiee.fr\nRomain Negrel\nromain.negrel@esiee.fr\nGiovanni Chierchia\ngiovanni.chierchia@esiee.fr\nJean-François Bercher\njf.bercher@esiee.fr\nUniv Gustave Eiffel\nCNRS\nLIGM\nF-77454 Marne-la-Vallée, France\nAbstract\nContinual Learning has been challenging, especially when dealing with unsupervised\nscenarios such as Unsupervised Online General Continual Learning (UOGCL), where the\nlearning agent has no prior knowledge of class boundaries or task change information.\nWhile previous research has focused on reducing forgetting in supervised setups, recent\nstudies have shown that self-supervised learners are more resilient to forgetting. This\npaper proposes a novel approach that enhances memory usage for contrastive learning in\nUOGCL by defining and using stream-dependent data augmentations together with some\nimplementation tricks. Our proposed method is simple yet effective, achieves state-of-\nthe-art results compared to other unsupervised approaches in all considered setups, and\nreduces the gap between supervised and unsupervised continual learning. Our domain-\naware augmentation procedure can be adapted to other replay-based methods, making it\na promising strategy for continual learning.\n1\nIntroduction\nContinual Learning (CL) is the ability to learn from a continuously evolving stream of data\nwhile accommodating shifts in distribution over time. Recent years have witnessed numer-\nous attempts to simulate such an environment for image classification, including domain\nand class-incremental learning scenarios [18]. While much of the prior research has been fo-\ncused on a fully supervised scenario that assumes specific prior knowledge, unsupervised CL\nmethods operate under more challenging circumstances where there is no task boundary or\nthe total number of classes available. This work focuses on a more realistic learning scenario\nwhere only one pass over non-iid, unlabeled data is allowed without prior task knowledge,\n© 2023. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\nThis work has received support from Agence Nationale de la Recherche (ANR) for the project APY, with ref-\nerence ANR-20-CE38-0011-02. This work was granted access to the HPC resources of IDRIS under the allocation\n2022-AD011012603 made by GENCI.\narXiv:2309.06896v1  [cs.LG]  13 Sep 2023\n2\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\ntask change information, or known number of classes during training. This setup is known\nas Unsupervised Online General Continual Learning (UOGCL) [3] and only a handful of ap-\nproaches have been designed to address it. STAM [31] employs a patch-based online cluster-\ning with novelty detection and expandable memory. SCALE [33] leverages a pseudo-labeled\ncontrastive loss and knowledge distillation with a fixed memory to learn data representation.\nBy design, both STAM and SCALE strongly focus on reducing forgetting.\nAlthough forgetting is widely recognized as the main issue in CL environments, self-\nsupervised learners have been found to be exceptionally resilient to forgetting compared\nto cross-entropy trained models [9, 11, 25]. Additionally, several studies demonstrate that\nreplay-based methods can easily take advantage of memory data more efficiently. One way\nis to use implementation tricks for reviewing memory data [23, 27], and another is to train for\nmultiple iterations for each batch [4, 27]. Similarly, some methods have obtained state-of-\nthe-art results while training using memory data only [28, 29]. Previous observations indicate\nthat replay-based self-supervised learners might not need anti-forgetting mechanisms to cope\nwith UOGCL. Rather, a promising strategy would be to learn more efficiently from memory\ndata.\nThis paper focuses on replay-based methods showing the best performances in online\nCL. We introduce a novel replay-based method that improves memory utilization with con-\ntrastive loss by combining stream-dependent data augmentations with implementation tricks\nfor UOGCL. Despite its simplicity, our method performs better than other unsupervised\nmethods in all evaluation setups. Additionally, the proposed Domain-Aware Augmentation\nprocedure could easily be integrated into other replay-based approaches with minor adapta-\ntions to improve their performance as well.\nThe paper is structured as follows: Section 2 presents related work. Section 3 describes\nthe training procedure, the strategy used to improve memory usage, and our new Domain-\nAware Augmentation framework for replay-based methods. Section 4 relates our experi-\nments and eventually, section 5 concludes the paper.\nDomain Aware Aug\nMany-view batch\nStandard Aug\nContrastive Training\nFigure 1: Overview of the Domain-Aware Augmentation procedure. From left to right,\nunlabeled images are sampled from stream S and memory M to create the incoming batch\nB. This batch is augmented to obtain a many-view batch BI. Here BI is composed of 2\nstandard augmentations and 3 DAA. Images from S are used to create DAA for every image\nin B. The model then learns image representation by minimizing the contrastive loss defined\nin eq. 1. Best viewed in color.\n2\nRelated work\nThis section defines learning strategies related to the work presented here.\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n3\n2.1\nOnline General Continual Learning\nIn the following, we define the considered CL setups.\nOnline Continual Learning (OCL) addresses the problem of learning from a contin-\nuous stream of data. Formally, we consider a sequential learning setup with a sequence\n{T1,··· ,TK} of K tasks, and Dk = (Xk,Yk) the corresponding data-label pairs. In CL, we\noften assume that for any value k1,k2 ∈{1,··· ,K}, if k1 ̸= k2 then we have Yk1 ∩Yk2 = /0\nand the number of classes in each task is the same. Contrary to standard CL, in OCL only\none pass over the data is allowed. This setup has been studied mostly in a fully supervised\nscenario [1, 2, 14, 26, 27, 29, 30].\nOnline General Continual Learning (OGCL) imposes further constraints on the al-\nready challenging task of Continual Learning.\nIn this setup, the learning model is not\nprovided with any prior information about the training environment, including task-ids,\ntask boundaries, number of classes per task, total classes, and tasks. While previous re-\nsearch efforts have mainly focused on developing methods for the supervised OGCL scenario\n[1, 26, 27, 30], only a limited number of approaches have been proposed for the unsuper-\nvised case. Among them, STAM [31] and SCALE [33] were recently introduced to address\nthe challenges of Unsupervised Online General Continual Learning (UOGCL).\nReplay based methods. In replay-based CL methods, a memory buffer stores a subset\nof the past training samples. As the learning model encounters a new batch of data from the\ndata stream, a corresponding batch is retrieved from the memory, and the model is trained on\nthe combined set of both the stream and memory batches. During the interval between two\nconsecutive stream batches, the memory is updated with the most recent data from the stream\nbatch. Replay-based methods have been widely developed in CL [1, 3, 14, 27, 28, 30, 33].\nContrastive Learning.\nContrastive Learning has become a widely used technique to\nlearn image representations [7, 17]. The essential principle underlying this approach is to\ntrain a model that maps similar data samples (referred to as positives) into closer proximity\nin a feature space while pushing dissimilar data samples (referred to as negatives) away\nfrom each other. In situations where labeled data is not available, augmentations of the same\nimage are treated as positives, while all other images are considered negatives. Mai et al. [26]\nused contrastive learning in the supervised scenario, and unsupervised contrastive learning\nwas used recently for UOGCL by Yu et al. [33]. It was also adapted to a semi-supervised\nscenario by Michel et al. [28].\nAverage Accuracy. Average Accuracy (AA) is the standard metric used in Continual\nLearning. It measures the overall accuracy of a model at each task, averaged across all tasks\nlearned up to that point. In this paper, we focus exclusively on the final Average Accuracy as\nour evaluation metric [18, 27], which is equivalent to the accuracy of the model at the end of\ntraining. By using only the final Average Accuracy, we can get a clear picture of how well\na model has performed over the entire learning process. This allows a fair comparison of\ndifferent approaches to Continual Learning and provides a consistent performance measure.\n3\nMethod Definition\nIn this section, we define our method. First, we describe our training procedure. Second,\nwe discuss the impact of key hyper-parameters, and last, we introduce a new augmentation\nstrategy for continual learning.\n4\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n3.1\nTraining procedure\nIn the following, we define the training procedure of our method.\nMany-view batch. We propose an extension to the multi-view batch concept as de-\nscribed by Khosla et al. [20] that involves using more than two augmentations. Specifically,\nwe define the many-view batch as the union of p augmentations for a batch B such that\nBI = B Sp\ni=1 Aug(B), where p is the number of augmentations and I represents the indices\nover BI. To train the model on many-view batches, we adapt the SupCon loss for unsuper-\nvised scenarios by treating every augmentation of the same input image as having the same\nlabel. We formulate this approach as minimizing the Multi-View Contrastive (MVCont) loss,\ndefined as follows:\nLMVCont(BI,θ) = −∑\ni∈I\n1\n|P(i)| ∑\np∈P(i)\nlog\nezi·zp/τ\n∑\na∈I\\{i}\nezi·za/τ\n(1)\nHere, P(i) = {j ∈I\\{i} | yj = yi} represents the set of images having the same input source\nas input i, ZI = {zi}i∈I, fθ denotes the learnable model with parameters θ, and zi = fθ(xi)\nrepresents the feature vector of the input image xi.\nExperience Replay with Contrastive Learning. We propose to combine Experience\nReplay (ER) [30] with unsupervised contrastive learning on a many-view batch by mini-\nmizing LMVCont defined in equation 1. Similar to ER, we mitigate forgetting by using a\nfixed sized memory that is filled following a reservoir sampling strategy [32] and a random\nretrieval. The overall training procedure is detailed in Algorithm 1.\nAlgorithm 1 Proposed Training Procedure\nInput: Data stream S; Augmentation procedure Aug(.); Model fθ(.)\nOutput: Model fθ; Memory M\nM ←{}\n▷Initialize memory\nfor Bs ∈S do\n▷Data stream\nfor q iterations do\n▷Memory iterations\nBm ←Retrieve(M)\n▷Retrieve data from memory\nB ←Bs ∪Bm\n▷Combined Batch\nBI ←B Sp\ni=1 Aug(B)\n▷Many-view batch\nθ ←SGD(LMVCont(fθ(BI),θ))\n▷Loss defined in 1\nM ←MemoryUpdate(Bs,M)\nreturn: θ; M\n3.2\nImproving memory usage\nIn the following, we discuss several strategies to improve memory usage in the training\nprocedure defined in Algorithm 1. Experimental results regarding such tricks are presented\nin Table 1.\nLarger Memory batch size |Bm|. One common hyper-parameter impacting the perfor-\nmance of replay-based methods is the memory batch size |Bm|, the amount of data retrieved\nfrom memory when encountering a new stream batch. As the size of |Bm| increases, the\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n5\nmodel will be exposed to memory data more frequently, which can lead to overfitting. How-\never, in UOGCL, we found that increasing |Bm| results in steadily increasing performances.\nMore Memory Iterations q. In Algorithm 1, q represents the number of memory it-\nerations, indicating how often the model will be exposed to memory data during training.\nAs q increases, the model will have more opportunities to learn from the memory data and\npotentially improve its performance on the task at hand. This technique has been applied in\nprevious works [1] with supervised methods with the risk of overfitting to the current task.\nIn UOGCL, we observe little overfitting.\nMore augmentations. Using more data augmentation can improve the learning process\nin online continual learning scenarios. It helps the model learn better by enabling it to see\nthe same data from different perspectives, recognize patterns, and generalize. Augmenta-\ntion also generates new training samples from existing ones, making the model adaptable\nto evolving data distributions. In that sense, increasing the value of p, the number of views\nin the many-view batch can similarly increase performances. However, standard augmenta-\ntions like random crop and color-jitter are limited as they do not use external information.\nFor example, a random crop augmentation only has a limited number of crops and through-\nout training, the model is likely to be trained on every variation of augmented memory data,\nencouraging overfitting. This phenomenon is exacerbated when using multiple memory iter-\nations. Therefore, more sophisticated augmentations are presented in section 3.3.\n3.3\nDomain Aware Augmentations (DAA)\nAs introduced in section 3.2, traditional data augmentation can be limited for replay meth-\nods. This section proposes a framework for stronger domain-aware augmentations that lever-\nages stream information. This allows the model to view memory data through an unlimited\namount of perspectives along training.\nDAA framework We define a DAA as an augmentation that combines an input image\nxi with a domain-related image xd, resulting in an augmented version of xi denoted as xa =\nDAA(xi,xd) via the DAA procedure. In replay-based approaches, xi comes from the current\nbatch B, while xd comes from the stream.\nDomain-Aware Mixup (DAM). Mixup has been introduced in 2018 [35] in the super-\nvised scenario as a new augmentation technique that linearly interpolates between two data-\nlabel pairs. Recently, mixup has been adapted to the CL setting [10, 25]. Notably, in LUMP\n[25], Madaan et al. introduced mixup strategies between memory and stream images to cre-\nate new images for replay-based unsupervised CL. For xi ∈M from memory and xd ∈S\nfrom stream the author trained a model on xa = λ · xi + (1 −λ) · xd. Notably, the obtained\nimages are considered as entirely new images. In this work, we define DAM by construct-\ning augmented images xa = λ ·xi +(1−λ)·xd, however, mixup-generated images are used\nas views of the original image. Additionally, we use λ ∼U(0.5,1), xi ∈B, xd ∈S and\nxa = DAM(xi,xd). The interpolation factor is set such that the augmented image xa has at\nleast half of its information coming from the input image xi. This strategy is inspired by the\nSMOTE [6] oversampling strategy.\nDomain-Aware CutMix (DAC). CutMix is another augmentation technique [34], which\nbears similarities with mixup. Likewise to the DAM adaptation we consider xi ∈B and\nxd ∈S to create xa, a new view of xi such that xa = M⊙xi+(1−M)⊙xd with M ∈{0,1}W×H\na binary mask where W and H are the width and the height of the image. 1 is a binary\nmask filled with ones, ⊙is the Hadamard product and λ ∼U(0.5,1). The binary mask is\nconstructed according to the bounding box coordinates B = (rx,ry,rw,rh) which correspond\n6\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\nto the region to crop from xd and integrate into xi. Following the work proposed by [34] we\nsample the bounding box for a given λ value according to:\nrx ∼U(0,W), rw = W\np\n1−λ\nrh ∼U(0,H), rh = H\np\n1−λ\n(2)\nAs with DAM we use λ ≥0.5 to ensure that a significant part of the original image is present\nin the augmented version.\nDomain-Aware Style (DAS). Style transfer is the transfer of non-semantic visual infor-\nmation from one image xd to another image xi to create the resulting image xa, with content\nfrom xi and style from xd. The original style transfer method proposed by [12] relies on a\nslow optimization process which cannot reasonably be applied as a data augmentation proce-\ndure. [19] proposed a method based on instance normalization that can compute and transfer\nany style from any image efficiently, but has to be pre-trained beforehand. A model pre-\ntrained on MS-COCO [24] is used to transfer the style from xd ∈S to xi ∈B. The obtained\nimage is considered as another view of xi such that xa = DAS(xi,xd).\n4\nExperiments\nIn this section, we first describe our setup: evaluation protocol, datasets used, baseline meth-\nods considered for comparisons, and implementation details; before presenting our experi-\nmental results.\n4.1\nEvaluation Protocol\nSince we focus on UOGCL, the training procedure defined in Algorithm 1 outputs a trained\nencoder fθ(.) and a subset of images M. An extra transfer-learning step is required for\nclassification. For a fair comparison, we use only the images stored in memory M at the\nend of training for transfer learning. This is equivalent to adding an extra step for labeling\nmemory after training. As it in common in representation learning [7, 11, 27] we consider\nthe trained model fθ(.) as being the succession of a feature extractor hθr(.) and a projection\nhead gθp(.) such that fθ(.) = gθp(hθr(.)). For the transfer learning step, the representations\nobtained from hθr(.) are used, as described in Algorithm 2.\nAlgorithm 2 Proposed Evaluation procedure\nInput: Data stream S; Memory M; Augmentation procedure Aug(.); Feature extractor\nhθr(.); Projection head gθp(.); Nearest Class Mean classifier φω(.)\nOutput: End-to-end classifier φω(hθr(.))\nTraining Phase:\nθr, M ←Train(Bs,Aug(.), fθ(.))\n▷Train as in Algorithm 1 with fθ(.) = gθp(hθr(.))\nTesting Phase:\nR ←hθr(M)\nω ←TrainNCM(ω, R)\n▷Train a Nearest Class Mean classifier on representations.\nreturn: ω; θr\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n7\nCIFAR10\nCIFAR100\nTiny IN\n10\n34.7±1.8\n11.3±0.4\n8.8±0.04\nMemory\n20\n36.3±2.7\n11.8±1.0\n10.1±0.2\nbatch size\n50\n41.1±2.0\n16.8±1.0\n13.2±0.5\n|Bm|\n100\n42.9±0.1\n19.2±0.5\n15.2±0.3\n200\n43.2±2.3\n21.2±0.9\n16.7±0.5\n|Bm| = 200\n|Bm| = 200\n|Bm| = 200\n1\n43.2±2.3\n21.2±0.9\n16.7±0.5\nMemory\n2\n44.0±1.5\n23.1±0.2\n17.2±0.6\niterations\n3\n44.0±2.0\n23.0±0.3\n18.3±0.3\nq\n4\n45.2±2.7\n23.8±0.4\n17.6±0.2\n5\n42.6±1.9\n24.0±0.4\n18.1±0.5\nq = 1\nq = 4\nq = 1\nq = 4\nq = 1\nq = 4\n1\n43.2±2.3\n45.2±2.7\n21.2±0.9\n23.8±0.4\n16.7±0.5\n17.6±0.2\nNumber\n2\n44.4±0.5\n42.4±2.0\n24.6±0.7\n24.6±1.0\n17.2±0.6\n18.8±0.6\nof\n3\n45.6±1.4\n41.8±5.0\n25.7±0.4\n25.9±0.6\n18.0±0.4\n18.7±0.4\nviews\n4\n45.3±1.7\n41.5±5.7\n26.4±0.2\n26.3±0.3\n17.9±0.1\n18.6±0.0\np\n5\n45.6±1.0\n39.0±6.1\n26.7±0.3\n27.3±0.7 18.2±0.4 19.1±0.2\n6\n45.7±1.0\n40.0±7.7\n26.8±0.5\n26.8±0.1\n18.1±0.4\n18.5±0.9\nTable 1: Impact of |Bm|, q and p on the final AA (%) for CIFAR10, CIFAR100 and Tiny Im-\nageNet. The top part shows performances for |Bm| ∈[10,200], p = 1, q = 1. The middle part\nshows performances for q ∈[1,5], |Bm| = 200, p = 1. The bottom part show performances\nfor p ∈[1,6], q ∈{1,5}, |Bm| = 200. The performances are obtained by following algorithm\n2. We use standard augmentations described in section 4.4. Each experiment is run 3 times\nand their average and standard deviation are displayed. The best results are displayed in\nbold.\n4.2\nDatasets\nWe use variations of standard image classification datasets [21, 22] to build continual learn-\ning environments.\nThe original datasets are split into several tasks of non-overlapping\nclasses. Specifically, we experimented on split-CIFAR10, split-CIFAR100 and split-Tiny\nImageNet. In this paper, we omitted the split- suffix for simplicity. CIFAR10 contains\n50,000 32x32 train images and 10,000 test images and is split into 5 tasks containing 2\nclasses each for a total of 10 distinct classes. CIFAR100 contains 50,000 32x32 train im-\nages and 10,000 test images and is split into 10 tasks containing 10 classes each for a total of\n100 distinct classes. Tiny ImageNet is a subset of the ILSVRC- 2012 classification dataset\nand contains 100,000 64x64 train images as well as 10,000 test images and is split into 20\ntasks containing 10 classes each for a total of 200 distinct classes.\n4.3\nBaselines\nIn the following, we describe considered baselines. While proposing an unsupervised ap-\nproach, we compare our method to supervised an unsupervised baselines to better demon-\nstrate its efficiency. For methods using replay strategies, we add the suffix -ER to the name\nand use reservoir sampling [32] for memory update and random retrieval. fine-tuned: Su-\npervised lower bound corresponding to training using a cross entropy loss in a continual\nlearning setup without precautions to avoid forgetting.\noffline: Supervised upper bound. The model is trained without any CL specific constraints.\nExperience Replay (ER) [30]: ER is a supervised memory based technique using reservoir\nsampling [32] for memory update and random retrieval. The model is trained using cross-\n8\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\nentropy.\nSupervised Contrastive Replay (SCR) [26]: Replay-based method trained using the Sup-\nCon loss [20].\nER-ACE [5]: Replay based method using an Asymmetric Cross Entropy to overcome fea-\nture drift.\nGSA [15]: Replay-based method dealing with cross-task class discrimination with a rede-\nfined loss objective using Gradient Self Adaptation.\nGDumb [29]: Simple method that stores data from the stream in memory, with the con-\nstraint of having a balanced class selection. At inference time, the model is trained offline\non memory data.\nSimCLR-ER [7]: Memory-based approach where the model is trained using the unsuper-\nvised contrastive loss of SimCLR. The memory management strategy is the same as the one\nused in ER.\nBYOL-ER [13]: Memory-based approach where the model is trained using the loss defined\nin BYOL. The memory management strategy is the same as the one used in ER.\nSimSiam-ER [8]: Memory-based approach where the model is trained using the loss de-\nfined in SimSiam. The memory management strategy is the same as the one used in ER.\nLUMP [25]: Replay-based approach where every image in the batch is a mixup between\nmemory and stream image. The model is trained using the unsupervised contrastive loss.\nOriginally proposed in a non-online scenario, this method was adapted to the UOGCL.\nSCALE [33]: Replay-based method using a pseudo-labeled contrastive loss. While very\nrecent, the code is not available for this method and we had to report the available perfor-\nmances from the original paper.\nSTAM [31]: A method designed for UOGCL using an expandable memory, patch-based\nclustering and novelty detection.\n4.4\nImplementation details\nWe train a ResNet-18 [16] from scratch for every experiment. The projection layer for con-\ntrastive approaches is a MLP with 1 hidden layer of size 512, ReLU activation, and output\nsize of 128. Memory batch size for replay-based methods is 200 and stream batch size for\nany method is 10. Our method uses an SGD optimizer with a fixed learning rate of 0.1. For\nall methods, a small hyperparameter search is conducted, and best parameters are kept for\ntraining. The search includes learning rate and optimizer. Temperature for contrastive losses\nis set to 0.07. For standard augmentations, we use random crop, colo jitter, random flip, and\ngrayscale. Offline methods are trained for 50 epochs with the same optimizer, model, and\naugmentation procedure as other methods. Unsupervised methods are evaluated using NCM\non memory data at the end of training following sec 4.1. For each experiment, the order of\nthe labels for the training sequence is generated randomly.\n4.5\nResults\nIn what follows, we present our experimental results, highlighting the main figures and char-\nacteristics that demonstrate the interest and relevance of our approach.\nScaling memory parameters. Memory parameters described in 3.2 can have a signifi-\ncant impact on performances. While expanding the amount of data retrieved from memory\n|Bm| continuously improves performances, it cannot exceed memory size. Similarly, we\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n9\nobserve that increasing the amount of augmentation p also results in an increase in perfor-\nmances for all datasets. However, larger values of memory iteration q do not scale well\nfor p ≥5 while considerably increasing computation. Therefore, we set q = 1 for our final\nmethod and scale with the number of augmentations rather than the number of iterations.\nHowever, experimenting with larger values of q could lead to even higher performances.\nFinal AA. We report the final AA on table 2 for all methods. Our approach outperforms\nevery other unsupervised method for UOGCL, on all considered setups. Notably, Ours -\n(7,1,0,0,0), which corresponds to training with (p,q) = (7,1) demonstrate that training\nwith more augmentations can considerably help training in UOGCL. Such results experi-\nmentally demonstrate the efficiency of focusing on memory usage rather than minimizing\nforgetting. We cannot report performances for STAM on Tiny IN since the author did not\ngive corresponding parameters for this dataset and CIFAR100 parameters gave poor perfor-\nmances.\nImpact of DAA. To disentangle the impact of DAA compared to standard augmenta-\ntion, we present in table 2 the results of our method with (p,q) = (7,1), namely Ours −\n(7,1,0,0,0) and the results of our method with (p,q) = (4,1) and 1 DAS, 1 DAM, 1 DAC;\nnamely Ours−(4,1,1,1,1). It can be seen that for the same number of augmentations over-\nall, using DAA gives better performances in all considered scenarios.\nComparison to supervised methods.\nSince very few methods have been designed\nfor UOGCL, we also implemented some typical supervised methods for OGCL. Results\ndisplayed in table 2 show that for small memory sizes, our method can achieve perfor-\nmances close to SCR, a state-of-the-art supervised technique. Specifically, on CIFAR10 with\nM = 200, our method performs only 1.5% below SCR. We conjecture that this results from\nself-supervised methods being less sensitive to overfitting, which is especially important for\nsmaller memory sizes.\nCIFAR10\nCIFAR100\nTiny ImageNet\nMethod\nM=200\nM=500\nM=2k\nM=5k\nM=2k\nM=5k\nM=10k\nSupervised\noffline\n86.1±5.7\n53.0±1.8\n42.3±3.9\nfine-tuned\n16.6±2.3\n3.6±0.7\n1.4±0.1\nER [30]\n41.46±3.41\n52.93±4.39\n31.37±0.69\n39.22±1.11\n11.33±1.17\n19.4±2.26\n25.93±3.02\nGDUMB [29]\n34.06±1.81\n41.42±1.25\n15.74±0.61\n25.53±0.44\n7.08±0.39\n13.79±0.76\n22.35±0.23\nSCR [26]\n49.16±3.02\n60.28±1.21\n37.79±0.95\n47.31±0.34\n19.76±0.24\n28.80±0.51\n34.28±0.28\nER-ACE [5]\n45.25±2.85\n53.10±2.70\n33.32±1.14\n40.60±1.55\n21.71±0.34\n27.27±0.95\n32.57±1.0\nGSA [15]\n52.03±2.14\n61.30±2.35\n38.77±1.07\n48.21±0.99\n19.35±0.72\n27.58±0.74\n34.72±0.82 -\nUnsupervised\nSTAM\n30.54±0.8\n8.39±0.4\n-\nSCALE [33]\n32±1⋆\n22±0.1⋆\n-\nLUMP [25]\n24.96±1.72\n25.34±1.06\n7.42±0.57\n7.18±0.5\n4.15±0.5\n4.55±0.68\n5.41±0.19\nSimSiam-ER [8]\n27.73±1.18\n30.59±1.21\n6.91±0.37\n7.47±0.11\n5.69±0.32\n6.49±0.41\n6.9±0.52\nBYOL-ER [13]\n29.43±0.55\n29.30±1.01\n9.39±0.52\n10.35±0.61\n5.07±0.39\n6.19±0.26\n6.59±0.38\nSimCLR-ER [7]\n43.20±2.30\n48.81±0.78\n21.2±0.9\n23.62±0.54\n12.84±0.7\n16.7±0.5\n17.97±0.14\nOurs (7,1,0,0,0) 45.68±2.38\n52.89±0.57\n27.27±0.13\n31.32±0.64\n13.16±0.37\n17.9±0.58\n20.21±0.13\nOurs (4,1,1,1,1) 48.09±1.22 56.02±1.34\n29.02±0.77 33.19±0.9\n14.79±0.49 20.35±0.02 22.06±0.37\nTable 2: Final AA (%) for all methods on CIFAR10, CIFAR100 and Tiny ImageNet and\nvarying memory sizes M. For our method, we reported two set of (p,q,#DAM,#DAC,#DAS)\nwhere #DAM, #DAC, #DAS are the number of DAM, DAC and DAS respectively. Lines\ncorresponding to our method show that 1) using more augmentations can easily improve\nperformances 2) more improvement is achieved using DAA. Each experiment is run 5 times\nand their average value and standard deviation are reported. The best result and are displayed\nin bold. Starred values are values reported from the original paper.\n10\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n5\nConclusion\nIn this paper, we addressed the problem of Unsupervised Online General Continual Learning\nfrom the perspective of improving memory usage whereas current state-of-the-art methods\npropose to cope with catastrophic forgetting. We demonstrated that data augmentation can\nbe enhanced for replay-based methods and proposed a new augmentation strategy, Domain\nAware Augmentations, designed for continual learning. We showed the efficiency of fo-\ncusing on memory usage rather than minimizing forgetting: with such an approach, we not\nonly surpassed current unsupervised approaches to UOGCL but also narrowed the gap be-\ntween supervised and unsupervised methods for Online General Continual Learning. Our\nexperiments show that better memory utilization by augmentations implies higher compu-\ntation costs. As these calculations can be parallelized, the impact on training time remains\nmanageable. Lastly, it should be pointed out that the proposed approach could be adapted\nto other memory-based methods, with small changes, making it a promising strategy for\ncontinual learning.\nReferences\n[1] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Cac-\ncia, Min Lin, and Lucas Page-Caccia. Online Continual Learning with Maximal In-\nterfered Retrieval. In Advances in Neural Information Processing Systems, volume 32,\n2019.\n[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample\nselection for online continual learning. Advances in Neural Information Processing\nSystems, 32, 2019.\n[3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-\nara. Dark experience for general continual learning: a strong, simple baseline. In\nAdvances in Neural Information Processing Systems, volume 33, pages 15920–15930,\n2020.\n[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Simone Calderara. Rethinking\nexperience replay: a bag of tricks for continual learning. In 2020 25th International\nConference on Pattern Recognition (ICPR), pages 2180–2187. IEEE, 2021.\n[5] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eu-\ngene Belilovsky.\nNew insights on reducing abrupt representation change in online\ncontinual learning, 2022.\n[6] Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer.\nSMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence\nResearch, 2002.\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple\nframework for contrastive learning of visual representations. In International confer-\nence on machine learning, pages 1597–1607. PMLR, 2020.\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n11\n[8] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 15750–15758, 2021.\n[9] MohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf Aljundi, and Eugene\nBelilovsky. Probing representation forgetting in supervised and unsupervised continual\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16712–16721, 2022.\n[10] Arthur Douillard, Alexandre Ramé, Guillaume Couairon, and Matthieu Cord. Dytox:\nTransformers for continual learning with dynamic token expansion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9285–\n9295, 2022.\n[11] Enrico Fini, Victor G. Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek\nAlahari, and Julien Mairal. Self-Supervised Models are Continual Learners. In 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n[12] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image Style Transfer Using\nConvolutional Neural Networks. In Computer Vision and Patern Recognition (CVPR),\n2016.\n[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond,\nElena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad\nGheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised\nlearning. Advances in neural information processing systems, 33:21271–21284, 2020.\n[14] Yiduo Guo, Bing Liu, and Dongyan Zhao. Online Continual Learning through Mutual\nInformation Maximization. In Proceedings of the 39th International Conference on\nMachine Learning, 2022.\n[15] Yiduo Guo, Bing Liu, and Dongyan Zhao. Dealing with cross-task class discrimination\nin online continual learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11878–11887, 2023.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 770–778, 2016.\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum con-\ntrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n[18] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira.\nRe-evaluating\ncontinual learning scenarios:\nA categorization and case for strong baselines.\narXiv:1810.12488, 2018.\n[19] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive\ninstance normalization. In Proceedings of the IEEE international conference on com-\nputer vision, pages 1501–1510, 2017.\n12\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n[20] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip\nIsola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning.\nAdvances in Neural Information Processing Systems, 33:18661–18673, 2020.\n[21] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. Univer-\nsity of Toronto, 2009.\n[22] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3,\n2015.\n[23] Guoqiang Liang, Zhaojie Chen, Zhaoqiang Chen, Shiyu Ji, and Yanning Zhang.\nNew insights on relieving task-recency bias for online class incremental learning.\narXiv:2302.08243, 2023.\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra-\nmanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In european conference on computer vision (ECCV), 2014.\n[25] Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Rep-\nresentational continuity for unsupervised continual learning. arXiv:2110.06976, 2021.\n[26] Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner. Supervised contrastive re-\nplay: Revisiting the nearest class mean classifier in online class-incremental contin-\nual learning. In Conference on Computer Vision and Pattern Recognition Workshop\n(CVPRW), 2021.\n[27] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner.\nOnline continual learning in image classification: An empirical survey. Neurocomput-\ning, 2022.\n[28] Nicolas Michel, Romain Negrel, Giovanni Chierchia, and Jean-Fmnçois Bercher. Con-\ntrastive learning for online semi-supervised general continual learning. In 2022 IEEE\nInternational Conference on Image Processing (ICIP). IEEE, 2022.\n[29] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach\nthat questions our progress in continual learning. In european conference on computer\nvision (ECCV), 2020.\n[30] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory\nWayne. Experience Replay for Continual Learning. In Advances in Neural Information\nProcessing Systems, 2019.\n[31] James Smith, Cameron Taylor, Seth Baer, and Constantine Dovrolis. Unsupervised\nprogressive learning and the stam architecture. arXiv:1904.02021, 2019.\n[32] Jeffrey S. Vitter. Random sampling with a reservoir. ACM Transactions on Mathemat-\nical Software, 1985.\n[33] Xiaofan Yu, Yunhui Guo, Sicun Gao, and Tajana Rosing. Scale: Online self-supervised\nlifelong learning without prior knowledge. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023.\nMICHEL, NEGREL, CHIERCHIA, BERCHER: DOMAIN-AWARE AUGMENTATIONS\n13\n[34] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with local-\nizable features. In Proceedings of the IEEE/CVF international conference on computer\nvision, 2019.\n[35] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup:\nBeyond empirical risk minimization. arXiv:1710.09412, 2017.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-09-13",
  "updated": "2023-09-13"
}