{
  "id": "http://arxiv.org/abs/2309.07909v2",
  "title": "DiffAug: Enhance Unsupervised Contrastive Learning with Domain-Knowledge-Free Diffusion-based Data Augmentation",
  "authors": [
    "Zelin Zang",
    "Hao Luo",
    "Kai Wang",
    "Panpan Zhang",
    "Fan Wang",
    "Stan. Z Li",
    "Yang You"
  ],
  "abstract": "Unsupervised Contrastive learning has gained prominence in fields such as\nvision, and biology, leveraging predefined positive/negative samples for\nrepresentation learning. Data augmentation, categorized into hand-designed and\nmodel-based methods, has been identified as a crucial component for enhancing\ncontrastive learning. However, hand-designed methods require human expertise in\ndomain-specific data while sometimes distorting the meaning of the data. In\ncontrast, generative model-based approaches usually require supervised or\nlarge-scale external data, which has become a bottleneck constraining model\ntraining in many domains. To address the problems presented above, this paper\nproposes DiffAug, a novel unsupervised contrastive learning technique with\ndiffusion mode-based positive data generation. DiffAug consists of a semantic\nencoder and a conditional diffusion model; the conditional diffusion model\ngenerates new positive samples conditioned on the semantic encoding to serve\nthe training of unsupervised contrast learning. With the help of iterative\ntraining of the semantic encoder and diffusion model, DiffAug improves the\nrepresentation ability in an uninterrupted and unsupervised manner.\nExperimental evaluations show that DiffAug outperforms hand-designed and SOTA\nmodel-based augmentation methods on DNA sequence, visual, and bio-feature\ndatasets. The code for review is released at\n\\url{https://github.com/zangzelin/code_diffaug}.",
  "text": "DiffAug: Enhance Unsupervised Contrastive Learning\nwith Domain-Knowledge-Free Diffusion-based Data Augmentation\nZelin Zang 1 2 3 Hao Luo 2 4 Kai Wang 3 Panpan Zhang 3 Fan Wang 2 4 Stan.Z Li 1 Yang You 3\nAbstract\nUnsupervised Contrastive learning has gained\nprominence in fields such as vision, and biology,\nleveraging predefined positive/negative samples\nfor representation learning. Data augmentation,\ncategorized into hand-designed and model-based\nmethods, has been identified as a crucial compo-\nnent for enhancing contrastive learning. However,\nhand-designed methods require human expertise\nin domain-specific data while sometimes distort-\ning the meaning of the data. In contrast, gen-\nerative model-based approaches usually require\nsupervised or large-scale external data, which has\nbecome a bottleneck constraining model train-\ning in many domains. To address the problems\npresented above, this paper proposes DiffAug, a\nnovel unsupervised contrastive learning technique\nwith diffusion mode-based positive data genera-\ntion. DiffAug consists of a semantic encoder and a\nconditional diffusion model; the conditional diffu-\nsion model generates new positive samples condi-\ntioned on the semantic encoding to serve the train-\ning of unsupervised contrast learning. With the\nhelp of iterative training of the semantic encoder\nand diffusion model, DiffAug improves the repre-\nsentation ability in an uninterrupted and unsuper-\nvised manner. Experimental evaluations show that\nDiffAug outperforms hand-designed and SOTA\nmodel-based augmentation methods on DNA se-\nquence, visual, and bio-feature datasets. The code\nfor review is released at https://github.\ncom/zangzelin/code_diffaug.\n1AI Lab, Research Center for Industries of the Future, Westlake\nUniversity, China 2DAMO Academy, Alibaba Group 3National\nUniversity of Singapore 4Hupan Lab, Zhejiang Province. Corre-\nspondence to: Stan.Z Li <Stan.Z.Li@westlake.edu.cn>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\n1. Introduction\nContrastive learning, as shown by many studies (He et al.,\n2020b; Chen et al., 2020; Cui et al., 2021; Wang & Qi, 2022;\nAssran et al., 2022; Zang et al., 2023), has become important\nin areas like vision (He et al., 2021; Zang et al., 2022b), nat-\nural language processing (Rethmeier & Augenstein, 2023),\nand biology (Yu et al., 2023; Krishnan et al., 2022). The key\nto contrastive learning (CL) lies in designing appropriate\ndata augmentation methods. Many studies (Tian et al., 2020;\nZhang & Ma, 2022; Peng et al., 2022; Zhang et al., 2023b)\nhave found that data augmentation helps CL by making it\nmore robust and preventing model collapse problems.\nData augmentation falls into two main types: hand-designed\nmethods and model-based methods (Xu et al., 2023). Hand-\ndesigned methods require humans to understand the mean-\ning of the data and then change the input features while\nmaintaining or extending that meaning. For example, sev-\neral methods in visual tasks (such as color change (Yan\net al., 2022), random cropping (Cubuk et al., 2020), and\nrotation (Maharana et al., 2022)) and DNA sequence repre-\nsentation tasks (such as mutations, insertion, and noise (Lee\net al., 2023)) are used to aid in model training. However,\nthe problem is that the above techniques must be more data-\nspecific. For some data (genes or proteins or others), it isn’t\neasy to understand due to the complexity of its meaning.\nConsequently, it isn’t easy to design a good augmentation\nstrategy. Semantics-independent augmentation methods\nsuch as adding noise (Huang et al., 2022) and random hid-\ning (Theodoris et al., 2023) are used, but only sometimes\nwith significant results. Another issue with hand-designed\nmethods is their inability to subtly alter the semantics of the\ndata. For instance, a minor mutation in a DNA sequence can\nlead to significant semantic changes, akin to a gene mutation\n(as illustrated in Figure. 1). As a result, more positive/neg-\native samples are needed to distribute these risks to obtain\na stable representation. It is also challenging to train CL\nmodels with fewer samples for certain domains where data\nacquisition is costly, such as biology.\nGiven the challenges mentioned earlier, model-based meth-\nods (generative models) based on deep learning are used\nto create better data. In the vision domain, techniques us-\ning VAE (Kingma & Welling, 2014), GAN (Goodfellow\n1\narXiv:2309.07909v2  [cs.LG]  25 May 2024\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nAugmented Image\nAugmented Image\nHand-\nDesigned Aug.\nNeed Semantic Priori\nRepresentation Backbone\nModel-\nBased Aug.\nNeed External\nData/Model/Label\nAugmented Image\nDiffAug\nDiffusion\nLabel\nExtract\nDiffAug\nEncoder\nNot Need \n Data/Model/Label & Priori\nAGC TGA\n...\n...\nA\nC\nTG\n...\n...\nA\nT\nImage Data\nDNA Data\nAGC TGA\n...\n...\nA\nC\nTG\n...\n...\nA\nT\nImage Data\nDNA Data\nAGC TGA\n...\n...\nA\nC\nTG\n...\n...\nA\nT\nImage Data\nDNA Data\nAGC\nGA\n...\n...\nA\nC\nTG\n...\n...\nT\nAGC TG\n...\n...\nA\nC\nG\n...\n...\nA\nT\nAG\nC\nT\nGA\n...\n...\nA\nC\nTG\n...\n...\nA\nT\nAG\nC\nTG\nA\n...\n...\nA\nC\nTG\n...\n...\nA\nT\nAG\nC\nTGA\n...\n...\nA\nC\nTG\n...\n...\nT\nA\nG\nC\nT\nG\nA\n...\n...\nA\nC\nTG\n...\n...\nA\nT\nAugmented DNA (mutations)\nAugmented DNA\nAugmented DNA\nG\nC\nA\nT\nRepresentation Backbone\nRepresentation Backbone\n Hand-Designed Methods\n(May specific to data domain)\n(a)\nModel-Based Methods\n(Specific to data domain)\n(b)\nProposed DiffAug\n(Not specific to data domain)\n(c) \nA\nFigure 1. Comparison of DiffAug with existing augmentation strategy. (a) Hand-designed augmentation is based on human priori that\ngenerate new data with different feature but semantically similar semantic. (b) Model-based augmentation methods generate new data\nwith the same labels by training generative models with large amount of data, labels. These methods often require large amounts of data\nand target specific data domains. (c) DiffAug attempts to reduce the dependence on external data and prior knowledge through iterative\ntraining with encoders and diffusion. Expanding the application areas of unsupervised CL.\net al., 2014), and diffusion models (Ho et al., 2020; Nichol\n& Dhariwal, 2021; Saharia et al., 2022; Nichol et al., 2022;\nRamesh et al., 2022) have been developed to improve model\ntraining. For supervised learning, several studies have re-\nceived attention. Du et al. (2023) proposed the DREAM-\nOOD framework, which uses diffusion models to gener-\nate photo-realistic outliers from in-distribution data for im-\nproved OOD detection. Zhang et al. (2023a) developed\nthe Guided Imagination Framework (GIF) using generative\nmodels like DALL-E2 and Stable Diffusion for dataset ex-\npansion, enhancing accuracy in both natural and medical im-\nage datasets. The detailed related works are in Appendix.A.\nHowever, there are concerns about these methods, espe-\ncially about their diversity and how well they generalize.\nMoreover, most of these generative models are trained with\nsupervision or need much external data. This makes them\nless suitable for areas like DNA sequence and bio-feature\ndata (in Figure. 1). This leads to an important question:\nIs it possible to design a data augmentation framework\nto enhance unsupervised CL in different domains without\nrequiring expert knowledge or additional data?\nWe introduce DiffAug, a novel diffusion mode-based pos-\nitive data generation technique for unsupervised CL to ad-\ndress the posed problem. DiffAug eliminates the need for\ntraining labels. Instead, we employ a semantic estimator to\ngauge the semantics of the input data, subsequently guiding\nthe augmentation process. At its core, DiffAug operates\nthrough two synergistic modules: a semantic encoder and a\ndiffusion generator. Utilizing a soft contrastive loss, the se-\nmantic encoder crafts latent representations that act as guid-\ning vectors for the diffusion generator. This generator then\nmethodically produces augmented data in the input space,\nensuring varying levels of semantic consistency based on\nthe guiding vectors and specific adjustable hyperparameters.\nWe demonstrate DiffAug pioneering on DNA sequence and\nbiometric datasets, and pioneering on highly competitive\nvisual datasets. Our findings indicate that the proposed\nmethod can produce sensible data augmentations, subse-\nquently enhancing the performance of unsupervised CL that\nutilizes these augmentations. Notably, DiffAug performs\nsuperior classification and clustering tasks compared to all\nbenchmark methods. The primary contributions of this pa-\nper are: (a) We introduce DiffAug, a novel unsupervised\nCL technique with diffusion mode-based positive data gen-\neration. DiffAug’s data augmentation improves traditional\ndomain-specific hand-designed data augmentation strategy.\n(b) DiffAug operates independently of external data or man-\nually designed rules. Its versatility allows for integration\nwith various models, encompassing domains like vision or\nbiology studies. (b) The experimental results show the ef-\nficacy of DiffAug in enhancing the performance of CL in\ndifferent tasks. This suggests that DiffAug can generate\npositive sample data unsupervised, which in turn promotes\nthe development of unsupervised learning techniques.\n2. Methods\nIn the context of unsupervised data augmentation, the train-\ning dataset providing potential semantic categories is de-\nnoted as Dt = {xi}N\ni=1, where N is the size of the training\n2\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nGenerator \nEncoder\nshared weight\nGenerator \nEncoder \nEncoder \nEncoder \nTrain Encoder\nTrain Diffusion Model\nB-Step:\nGenerative\nModeling\nA-Step:\nSemanticity\nModeling\nGenerator \nIterative Training\n❄\n❄\n❄\n(d) Augmentation Generating\n(c) Generative Modeling (M-Step)\n(b) Semanticity Modeling (E-Step)\n(a) Training Strategies\nGenerate Positive\nSamples\nFigure 2. The DiffAug framework and training strategy. DiffAug includes a semantic encoder Enc(·|θ) and a diffusion generator\nGen(·|ϕ). (a) shows how Enc(·|θ) and Gen(·|ϕ) are interative trained. (b) and (c) show how to calculate the loss functions. (d) shows\nhow to generate new augmentation data with the trained model.\nset. To boost the training efficiency of unsupervised con-\ntrastive learning with positive samples generated by the\ndiffusion model, a novel framework called DiffAug is pro-\nposed.\n2.1. Preliminaries of Contrastive Learning\nContrastive Learning. Contrastive learning learns visual\nrepresentation via enforcing the similarity of the positive\npairs and enlarging distance of negative pairs. Formally,\nloss is defined as, Lcl =\n−logQ\n\u0000zi, z+\ni\n\u0001\n+log[Q\n\u0000zi, z+\ni\n\u0001\n+\nX\nz−\ni ∈V −\nQ\n\u0000zi, z−\ni\n\u0001\n] (1)\nwhere zi is the low dimensional embedding zi = Enccl(xi),\nQ\n\u0000zi, z+\ni\n\u0001\nindicates the similarity between positive pairs\nwhile Q\n\u0000zi, z−\ni\n\u0001\nis the similarity between negative pairs.\nFor the traditional scheme, in the computer vision do-\nmain, data augmentation methods such as random crop-\nping (Cubuk et al., 2020) or data Mixup (Zhang et al., 2017)\nare used to generate new positive data. The negative samples\nv−\ni are sampled from negative distribution V −.\nSoft Contrastive Learning. To address the performance\ndegradation due to view noise in contrastive learning\nand to accomplish unsupervised learning on smaller scale\ndatasets, Zang et al. (2023) designed soft contrastive learn-\ning, which smoothes sharp positive and negative sample\npair labels by evaluating the credibility of the sample pairs.\nConsider the loss form for multiple positive samples and\nmultiple negative samples as,\nLscl(yc,yj,zc,zj)=−\nB\nX\nj=1\n{P(yc, yj)log(Q(zc, zj))+\n(1−P(yc, yj)) log (1−Q(zc, zj))},\nP(a, b) =\n\u00001 + Hij\n\u0000eβ −1\n\u0001\u0001\nQ(a, b),\n(2)\nwhere the yi, zi are the high dimensional embedding and\nlow dimensional embedding yi, zi = Enc(xi). The P(a, b)\nis soft learning weight and calculated by the positive/neg-\native pair indicator Hcj. The hyper-parameter β ∈[0, 1]\nintroduces prior knowledge of data augmentation relation-\nship Hcj into the model training. Details of contrastive and\nsoft contrastive learning are in Appendix. B.\n2.2. DiffAug Design Details and Training Strategies\nDiffAug Framework. DiffAug accomplishes the tasks of\npositive sample generation and data representation by iterat-\ning the two modules over each other (in Figure. 2). DiffAug\nconsists of two main modules, a semantic encoder Enc(·|θ)\nand a diffusion generator Gen(·|ϕ), where θ and ϕ are model\nparameters. The Enc(·|θ) maps the input data xi to the\ndiscriminative latent space vi, and the generator Gen(·|ϕ)\ngenerates new data with a semantic vector vi. Similar to the\nExpectation maximization algorithm (Gupta et al., 2011),\nthe semantic encoder Enc(·|θ) and the diffusion generator\nGen(·|ϕ) are trained in turn by two different loss functions\n(see Figure. 2(a) and Figure. 2(b)).\nSemanticity Modeling (A-Step). In the semanticity model-\ning step, given a central data xc, we generate a background\nset Bc = {x1, · · · , xj, · · · , xNb},\n(\nxj ∼Dt\nif Hcj = 0\nxj ∼Aug(xc)\nif Hcj = 1\n(3)\nwhere Nb is the number of background data points. The\nHcj = 0 indicates xj is sampled from the dataset Dt, and\nxc and xj are negative pair. Meanwhile, Hcj = 1 indicates\nxc and xj are positive pair and xj is sampled from data\naugmentation. For details, new positive data are generated\nby the diffusion model according to DDPM (Ho et al., 2020),\nxj = Gen(δ, zc|ϕ∗), yc, zc = Enc(xc|θ∗),\n(4)\nwhere Gen(δ, zc|ϕ∗) is the generation process of the diffu-\nsion model, and the generating details are in Eq. (7). The\n3\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nTable 1. Comparison of Linear probing results on DNA sequence datasets. The compared methods including SOTA DNA sequence\nmethods (DNA-BERT (Ji et al., 2021), NT (Dalla-Torre et al., 2023), Hyena (Nguyen et al., 2023)) and contrastive methods with\nhuman-designed DNA-augmentation. The ‘AVE’ represents the average performance. The best results are marked in bold.\nDatasets\nGenomic Benchmarks (Greˇsov´a et al., 2023)\nMoEnEn\nCoIn\nHuWo\nHuEnCo\nHuEnEn\nHuEnRe\nHuNoPr\nHuOcEn\nAVE\nCNN\n69.0\n87.6\n93.0\n58.6\n69.5\n93.3\n84.6\n68.0\n76.7\nDNA-BERT\n69.4\n92.3\n96.3\n74.3\n81.1\n87.7\n85.8\n73.1\n82.5\nNT\n70.2\n90.0\n92.3\n71.5\n80.8\n87.9\n84.0\n77.2\n81.7\nHyena\n80.9\n89.0\n96.4\n73.2\n88.1\n88.1\n94.6\n79.2\n86.2\nSSL+Translocation\n83.8\n88.2\n95.5\n73.8\n77.4\n88.2\n84.7\n52.5\n80.5\nSSL+RC\n84.5\n88.3\n95.8\n71.9\n82.3\n86.8\n91.0\n74.4\n84.3\nSSL+Insertion\n80.9\n89.8\n96.6\n73.7\n83.4\n87.1\n91.8\n77.3\n85.0\nSSL+Mixup\n80.9\n89.4\n96.2\n73.2\n85.2\n88.6\n91.6\n77.9\n85.4\nDiffAug\n86.0(+1.5) 94.9(+2.6) 96.8(+0.2) 74.0(-0.3)\n94.9(+6.8) 91.8(+3.2) 94.5(-0.1)\n79.9(+0.7)\n89.1(+2.9)\nAlgorithm 1 The DiffAug Training Algorithm:\nInput: Data: Dt = {xc}N\ni=1, Learning rate: η, E or M State: S,\nBatch size: B, Network parameters: θ, ϕ,\nOutput: Updateed Parameters: θ, ϕ.\n1: while b = 0; b < [|X|/B]; b++ do\n2:\nxc ∼Dt;\n# Sample the centering data\n3:\nyc, zc ←Enc(xc|θ); # Generate frozen condition vector\n4:\nif S==B-Step then\n5:\nL1 ←Ldf(xc, SG(zc)|ϕ) by Eq. (6);\n6:\nϕ←ϕ −η ∂L1\n∂ϕ ,\n# Calculate diffusion loss\n7:\nelse\n8:\nBc\n=\n{x1, · · · , xB|xj ∼Dt if Hij\n=\n0; xj ∼\nAug(xc) else }; # Generate/sample data\n9:\nY = {y1, · · · , zj, · · · , yB},\n10:\nZ = {z1, · · · , zj, · · · , zB}, yj, zj = Enc(xj|θ)\n11:\nL2 ←Lscl(Y, Z) by Eq. (2);\n12:\nθ←θ −η ∂L2\n∂θ\n# Calculate scl loss\n13:\nend if\n14: end while\nδ ∼N(0, 1) is the random initialized data, and zc is a con-\nditional vector. The ∗in ϕ∗and θ∗means the parameter is\nfrozen. To avoid unstable positive samples from untrained\ngenerative models, training starts exclusively with tradi-\ntional data augmentation tools, and then, the data generated\nby DiffAug is replaced with data generated by DiffAug,\nwith a replacement probability of the hyperparameter λ, an\noversized λ introduce toxicity, which we will discuss in\nSec. 3.6. We update the parameter of the semantic encoder\nwith the soft contrastive learning loss,\nθ = θ −η\nX\nxj∈Bc\n{Lscl(yc, zc, yj, zj)} ,\nwhere yj, zj = Enc(xj|θ),\n(5)\nwhere the η is the learning rate, and the Lscl(yc, zc, yj, zj)\nis in Eq. (2).\nGenerative Modeling (B-Step). In the generative modeling\nstep, the conditional diffusion generator Gen(·|ϕ) is trained\nby the vanilla diffusion loss Ldf(xc, zc|ϕ) (Ho et al., 2020),\nϕ=ϕ−η\nT\nX\nt=1\nn\r\rδ −gϕ\n\u0000√¯αtext\nc+\n√\n1 −¯αt, t, zc\n\u0001\r\r2\n2\no\n, (6)\nwhere the conditional vector zc is generated from the seman-\ntic encoder in Eq.(4). The gϕ(·) is the conditional diffusion\nneural network. The αt is the noise parameter in the diffu-\nsion process, and ¯αt = 1 −αt. The ext\nc is the intermediate\ndata in the diffusion process, and the ex0\nc = xc. T is the time\nstep of the generation process. When gϕ(·) is trained, the\ndetailed generating process is,\nGen(δ, zc|ϕ∗)=\n\u001a\nex0 | ext−1 =\n1\n√αt\n\u0000ext−ˆα\n\u0001\n+σtN(0, 1),\n\u001b\n,\nˆα = 1 −αt\n√1−¯αt\ngϕ(ext, t, z∗\nc),\n(7)\nwhere t ∈{T,· · ·,1}, the gϕ(·) is a neural network approxi-\nmator intended to predict δ with ex and the condition vector\nz∗\nc.\nAugmentation Generation. Given the trained semantic\nencoder Enc(·|θ) and diffusion generator D(·), and DiffAug\ngenerate new augmented data x+\ni from any input data xi.\nx+\ni = Gen(δ|zi), yi, zi = Enc(xi).\n(8)\nMeanwhile, DiffAug’s semantic encoder can be seen as a\nfeature extractor. It is considered to have good discrimina-\ntive performance because it is trained simultaneously as the\ndiffusion generator.\n3. Results\nWe conduct experiments on various datasets, including DNA\nsequences, vision, and bio-feature datasets. We aim to\ndemonstrate that Diffaug can operate effectively and fa-\ncilitate improvements across diverse domains.\n4\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nTable 2. Comparison of the Linear probing results on Bio-feature dataset. dimensional reduction (DR) methods and contrastive\nlearning methods are list in the table. The DR methods are widely used on Bio-feature analysis including TopoAE (Moor et al., 2019),\nPaCMAP (Wang et al., 2022), and hNNE (Sarfraz et al., 2022). CL methods wtih mixup augmentation including Simclr (Chen et al.,\n2020), BYOL (Grill et al., 2020), MoCo (He et al., 2020b), and DLME (Zang et al., 2022b).\nMethod Type\nGA1457\nSAM561\nMC1374\nHCL500\nWARPARIOP\nPROT579\nAVE\nTopoAE\nDR\n74.6\n72.4\n61.3\n56.0\n73.0\n88.3\n70.9\nPaCMAP\nDR\n85.3\n83.7\n61.3\n36.2\n76.9\n87.2\n71.7\nhNNE\nDR\n77.4\n83.8\n62.3\n62.2\n72.5\n82.4\n73.4\nSimclr+Mixup\nCL\n84.8\n82.4\n62.3\n61.7\n74.3\n74.7\n73.3\nBYOL+Mixup\nCL\n82.8\n73.3\n60.9\n61.3\n72.6\n70.5\n70.2\nMoCo+Mixup\nCL\n84.2\n83.1\n70.2\n61.7\n82.8\n84.2\n77.7\nDLME+Mixup\nCL\n85.7\n83.6\n71.4\n62.3\n83.5\n84.5\n78.5\nDiffAug\nCL\n92.7(+7.0)\n89.3(+5.5)\n71.8(+0.4)\n64.7(+2.4)\n84.8(+1.3)\n91.2(+2.9)\n82.4(+3.9)\nTable 3. Comparison of Linear probing results on vision\ndataset. The SimC.+Mix. and Mo.V2+Mix. are SimCLR and Mo-\nCoV2 with Mixup data augmentation which processed by Zhang\net al. (2022). The SimC./Mo.V2+VAE/GAN means SimCLR/Mo-\nCoV2 with VAE/GAN generative model as data augmentation.\nDatasets\nCF10\nCF100\nSTL10\nTINet\nSimCLR\n89.6\n60.3\n89.0\n45.2\nMo.V2\n86.7\n56.1\n89.1\n47.1\nBYOL\n92.0\n62.7\n91.8\n46.1\nSimSiam\n91.6\n64.7\n89.4\n43.0\nDINO\n91.8\n67.4\n91.7\n44.2\nSimC.+Mixup\n90.9\n62.9\n89.6\n—\nMo.V2+Mixup\n91.5\n62.7\n90.1\n—\nSimC.+VAE\n89.6\n64.2\n91.7\n46.0\nMo.V2+VAE\n89.3\n65.9\n91.2\n43.3\nSimC.+GAN\n90.0\n64.3\n89.9\n44.6\nMo.V2+GAN\n91.1\n62.9\n91.2\n43.6\nDiffAug\n93.4(+1.6) 69.9(+2.5) 92.5(+0.8) 49.7(+2.1)\n3.1. Comparations on DNA Sequence Datasets\nFirst, we demonstrate the efficacy of DiffAug in improving\nDNA sequence representation and classification. DNA se-\nquence representation is challenging for contrastive learning\nbecause one cannot easily design data augmentation man-\nually by visualizing and understanding the data. Lee et al.\n(2023) explores how various natural genetic alterations can\nenhance model training performance.\nTest Protocols. Our experiments utilize the Genomic Bench-\nmarks (Greˇsov´a et al., 2023), encompassing datasets that\ntarget regulatory elements (such as promoters, enhancers,\nand open chromatin regions) from three model organisms:\nhumans, mice, and roundworms1. We adopted a method-\nology akin to that of Hyena-DNA (Nguyen et al., 2023)\nfor evaluating linear-test performance.\nTo mitigate the\n1https://github.com/ML-Bioinfo-\nCEITEC/genomic benchmarks.\ninfluence of the pre-training dataset, we exclusively em-\nployed the training data from Genomic Benchmarks for\nself-supervised pre-training and fine-tuning, followed by\nan evaluation of the test dataset. The comparison includes\nvarious methods such as CNN (as per Genomic Bench-\nmarks), DNA-BERT (Ji et al., 2021), NT (Dalla-Torre et al.,\n2023), and Hyena (Nguyen et al., 2023). Both DiffAug and\n‘SSL+’ leverage Hyena-tiny2 backbone, replacing Hyena’s\npre-training approach with a unique pre-training method-\nology. ‘SSL+’ means the model is pre-trained with Sim-\nCLR (Chen et al., 2020) with the augmentation of natural\nDNA augmentation strategies in (Lee et al., 2023). Further\ndetails on the dataset are provided in Appendix C.\nAnalysis. DiffAug outperforms competing methods in eight\nevaluations across four datasets, achieving performance im-\nprovements ranging up to 6.8%. Notably, DiffAug demon-\nstrates several significant benefits, particularly in classifica-\ntion metrics: (a) DiffAug enhances the sequence model’s\nperformance on classification accuracy, surpassing tradi-\ntional DNA augmentation approaches. (b) By learning dis-\ntributional knowledge from the training data, DiffAug facil-\nitates data augmentation with minimal human intervention,\npotentially enabling the generation of more stable enhanced\nsamples.\n3.2. Comparations on Bio-feature Datasets\nNext, we benchmark DiffAug against SOTA unsupervised\ncontrastive learning (CL) methods and traditional dimen-\nsional reduction (DR) methods in bio-feature datasets. Un-\nlike DNA sequence data, bio-feature datasets are the format\nfor most proteomics(Suhre et al., 2021), genomics (Bus-\ntamante et al., 2011), and transcriptomics(Aldridge & Te-\nichmann, 2020) data. The dataset contains an equal-length\nvector per sample, and each element in the vector represents\na gene, protein abundance, or bio-indicator. The data of\ntraining time consumption is in the Table 11.\n2https://huggingface.co/LongSafari/hyenadna-tiny-1k-seqlen\n5\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nTable 4. Ablation study of the semantic encoder includes DiffAug’s encoder is necessary and can efficiently generate conditional\nvectors. Linear-tests performance of different ablation setups on on vision dataset and biological dataset.\nDatasets\nVision Datasets\nBio-feature Datasets\nCF10\nCF100\nSTL10\nTINet\nGA1457\nSAM561\nMC1374\nHCL500\nA1. Gen(·) + Sup. Condition\n93.4\n70.9\n92.9\n45.9\n92.5\n89.6\n71.1\n63.9\nA2. Gen(·) + Rand. Condition\n34.2\n10.4\n30.1\n7.3\n10.5\n16.9\n13.9\n10.0\nA3. Gen(·) + Enc(·|θ) (DiffAug)\n93.4\n69.9\n92.5\n49.7\n92.7\n88.3\n71.8\n64.7\nShip\nTruck\nAirp.\nHorse\nAug1\n(a) CF10\nShip\nTruck\nAirp.\nMonk.\n(b) STL10\nAug2\nAug3\nAug1\nAug2\nAug3\nAug1\nAug2\nAug3\nAug1\nAug2\nAug3\nOri\nOri\nOri\nOri\n(c) MCA\n(d) HCL\nOri1\nOri2\nAug1 Aug2 Aug3\nOri1\nOri2\nAug1 Aug2 Aug3\nOri1\nOri2\nAug1 Aug2 Aug3\nOri1\nOri2\nAug1 Aug2 Aug3\nClass: label 3\nClass: label 5\nClass: label 6\nClass: label 7\n0\n1.0\n2.0\n-1.0\n-2.0\n0\n1.0\n2.0\n-1.0\n-2.0\nFigure 3. The display of original and generated images illustrates that DiffAug generates semantically similar augmented data.\nThe ‘Ori’ means original data and Aug1, Aug2 and Aug3 are augmentated data. For bio-feature data, we use violin plots to plot the\ndistribution of features.\nTest protocols. Our experiments are conducted using a vari-\nety of bio-feature datasets, namely GA1457 (Rouillard et al.,\n2016), SAM (Weber & Robinson, 2016), MC1374 (Han\net al., 2018), HCL500 (Han et al., 2020), PROT579 (Sun\net al., 2022), and WARPARIOP. To evaluate the effective-\nness of our DiffAug, we adopted a linear Support Vector Ma-\nchine (SVM) performance assessment akin to the method-\nologies described in Wang et al. (2022) and Sarfraz et al.\n(2022). In this assessment, dataset embeddings are split,\nallocating 90% for training purposes and the remaining 10%\nfor testing. The comparative results are shown in Table 2.\nFurther details regarding this experimental setup can be\nfound in the Appendix E. The training regimen for DiffAug\nis structured as follows: an initial A-Step of 330 epochs, fol-\nlowed by an B-Step of 330 epochs and concluding with a fi-\nnal A-Step of 340 epochs. Comprehensive training specifics,\nalong with the evolution of accuracy throughout training,\nare depicted in Figure 7.\nAnalysis. DiffAug consistently surpasses all other methods\nacross eight evaluations spanning four datasets, registering\na performance enhancement between 0.4% and 7.0% over\nits counterparts. (a) It is worth noting that the benefits of\nDiffAug are not limited to DNA sequence data. It also\nexcels in areas such as bio-feature and has broader applica-\ntions in traditional bioanalysis. (b) Data processed through\nDiffAug exhibits reduced overlap among distinct groups,\nfacilitating enhanced classification. This suggests DiffAug\ndelineates more explicit boundaries between data categories,\nculminating in more precise outcomes. Corresponding ev-\nidence is demonstrated in Figure 4. (c) Experiments with\nDNA sequence and bio-feature data have demonstrated that\nDiffAug is versatile and an important complement to other\nunsupervised learning techniques. Searching for effective\ndata augmentation strategies in biology has been difficult.\n3.3. Comparations on Vision Datasets\nNext, we benchmark DiffAug against the SOTA unsuper-\nvised comparative learning (CL) method on a visual dataset.\nThis field is much more active and has seen the emergence\nof excellent human-designed data augmentation methods.\nWe focus on data augmentation techniques that can be used\nfor unsupervised CL. Therefore, the comparison does not\ninclude some labeling-based methods (Nichol et al., 2021;\nHe et al., 2023; Trabucco et al., 2023; Zhang et al., 2023a).\nTest\nProtocols.\nExperiments\nare\nperformed\non\nCIFAR-10 [CF10] and CIFAR-100 [CF100] (Krizhevsky\net al., 2009), STL10 (Coates et al., 2011), TinyIma-\ngeNet [TINet] (Le & Yang, 2015) dataset. Notably, we\ndid not use a larger dataset in our experiments. It is because\nthe proposed method aims to efficiently utilize data to train\nmodels when data is limited (or expensive). Simple augmen-\n6\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nTable 5. Ablation study of scl loss function and training strategy. The classifier accuracy of each setting is displayed in this table. Soft\ncontrastive learning is improved with typical contrast learning, and AB-Step training is more stable.\nDatasets\nVision Datasets\nBio-feature Datasets\nCF10\nCF100\nSTL10\nTINet\nGA1457\nSAM561\nMC1374\nHCL500\nB1. SimCLR\n89.6\n60.3\n89.0\n45.2\n84.8\n82.4\n62.3\n61.7\nB2. DiffAug w/o Ldf\n91.3\n66.1\n90.1\n44.9\n89.1\n82.1\n59.3\n62.3\nB3. DiffAug w/o Lscl\n92.7\n68.4\n90.9\n45.1\n89.2\n82.4\n69.2\n61.3\nB4. DiffAug Syn. Training\n92.9\n69.7\n92.7\n45.3\n90.1\n89.6\n68.1\n62.3\nB5. DiffAug AB Training\n93.4\n69.9\n92.5\n49.7\n92.7\n88.3\n71.8\n64.7\nCF10\nDiffAug(ACC=93.4)\nDLME(ACC=91.3)\nBYOL(ACC=91.0)\nCF100\nBYOL(ACC=62.7)\nHCL500\nMC1374\nDiffAug(ACC=71.8)\nDLME+Mixup(ACC=71.4)\nPaCMAP(ACC=61.3)\nDiffAug(ACC=69.2)\nDLME+Mixup(ACC=62.3)\nPaCMAP(ACC=36.2)\nDiffAug(ACC=69.9)\nDLME(ACC=66.1)\nFigure 4. The scatter visualization of representation indicates DiffAug’s encoder learns cleaner embedding. The colors represent\ndifferent categories; there are 100 categories in CF100; we used the superclasses label provided by (Deng et al., 2021).\ntation techniques can train models without fully adequate\nand well-sampled data. We followed a procedure similar to\nSimCLR (Chen et al., 2020) for the Linear-test performance\nassessment. The baseline of SimCLR (Chen et al., 2020),\nBYOL (Grill et al., 2020), MoCo v2 (He et al., 2020a), and\nSimSiam (Chen & He, 2021) is from Peng et al. (2022).\nThe results of SimCLR and MoCoV2 with Mixup data aug-\nmentation (SimC.+Mix. and Mo.V2+Mix.) are from Zhang\net al. (2022). Since we did not find the corresponding GAN\nand VAE as a baseline for data augmentation, we tested\nthe corresponding results ourselves. For DiffAug, its se-\nmantic encoder served as the CL backbone, trained using\nDiffAug-augmented images. Comparative results are shown\nin Table 3. Details of the experimental setup are in Ap-\npendix D. The training strategy of DiffAug is A-Step: 200\nepochs →B-Step: 400 epoch →A-Step: 800 epoch. The\ndata of training time consumption is in the Table 9.\nAnalysis. From Table 3, it is evident that DiffAug consis-\ntently outperforms SOTA methods across all datasets. It\nsurpasses other techniques by at least 0.8% in five out of the\nfour projects. This showcases the effectiveness of DiffAug’s\ndata augmentation. (a) Beyond hand-designed augmentation\nmethods. DiffAug’s versatility indicates that its approach\nis on par with, or even better than, traditional hand-crafted\nmethods. The encoder in DiffAug produces robust features.\n(b) Beyond Mixup improved CL methods. DiffAug outper-\nforms the Mixup improved CL method of typical contrast\nlearning methods, and additionally, models trained using\nDiffAug-generated data and contrast learning methods bring\nsome improvement. (c) For datasets with many classes, like\nCF100 and TINet, DiffAug’s encoder might only sometimes\ncapture some local detail. Still, augmented data is crucial in\nguiding CL to produce better results.\n3.4. DiffAug Effectiveness Analysis\nEffectiveness analysis of diffusion generator. The diffu-\nsion module generates new positive data by inputting the\nprovided condition vector. To demonstrate that our Dif-\nfAug works appropriately, we show the generation results\nfor both the image and bio-feature datasets (in Figure. 3).\nA more detailed implementation and more results are in\nthe Appendix D and Appendix E. We can observe that the\ngenerated data retains semantic similarity to the original\ndata. For example, the objects described in the image data\nare consistent, while the gene distribution is also consistent.\nAt the same time, the generated data is not simply copied\n7\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nSimilarity\nSimilarity\nSimilarity\n(a) Mixup\n(b) Crop\n(c) DiffAug\nSimilarity\nSimilarity\nSimilarity\n(d) Mixup\n(f) DiffAug\n1\n0\n0.5\n1\n0\n0.5\n1\n0\n0.5\n1\n0\n0.5\n1\n0\n0.5\n1\n0\n0.5\nVision Dataset\nCF10\nBiological Dataset\nGA1457\np\np\np\np\np\np\n(e) Dimensional\nExchange\nFigure 5. Hist plot of the cosine similarity between original data\nand the augmentation data in latent space indicate that DiffAug\ngenerates semantically smooth augmentations. For the image\ndata, we compared similar mixups with random cropping. For\nbio-feature datasets, we compared same-label Mixup and random\ndimension swapping.\nbut varied without changing the semantic information.\nIn addition, we computed the cos-similarity of the original\naugmented sample in latent space to explore further the se-\nmantic differences between the newly generated and original\ndata. As depicted in Figure. 5, DiffAug’s similarity distribu-\ntion is smoother and broader. In comparison, Mixup tends\nto produce augmentations that are very similar semantically,\nwhile methods like cropping might introduce data with se-\nmantically distinct noise samples. In addition, we computed\nthe cos-similarity of the original augmented sample in latent\nspace to explore further the semantic differences between\nthe newly generated and original data. As depicted in Fig-\nure. 5, DiffAug’s similarity distribution is smoother and\nbroader. In comparison, Mixup tends to produce augmenta-\ntions that are very similar semantically, while methods like\ncropping might introduce data with semantically distinct\nnoise samples.\nEffectiveness analysis of semantic encoder. Next, we\nconfirm that the semantic encoder of DiffAug works well\nby visualizing the representation of DiffAug and baseline\nmethods (in Figure. 4). The t-SNE (Van der Maaten & Hin-\nton, 2008) is used to analyze the BYOL, DLME, and Dif-\nfAug embedding on CF10, CF100, MC1374, and HCL500\ndatasets. The results show that DiffAug’s encoder learns\ncleaner embedding than the baseline methods in Figure. 4,\nDiffAug E means the first A-Step’s results of the DiffAug.\nBy comparing DiffAug E and DiffAug, we observe that the\naugmented data further improves the embedding quality, sig-\nnificantly enhancing the depiction of local structures. The\nsame conclusion is shown in Figure. 7.\n3.5. Ablation Study and Effectiveness of Component\nAblation study of the semantic encoder. In the ablation\nstudy presented in Table. 4, we consider three configura-\ntions: A1 and A2 confirm the significance of DiffAug’s\nsemantic encoder by ablating it in two ways. A1 directly\nuses supervised one hot label as the conditional, bypassing\nthe condition vectors generated by the unsupervised neural\nnetwork. A2 employs random conditional vectors instead\nof those the encoder produces. A3 means the proposed Dif-\nfAug method. The results from these experiments can be\nfound in Table 4. We observe that the average performance\nof A1 is highest due to the access to the label. And not\naccessing the label at all brings a huge performance drop.\nThe results in A3 illustrate that DiffAug’s performance is\ncomparable to the fully supervised condition, demonstrating\nits ability to model supervised annotation within an unsu-\npervised framework.\nAblation study of training strategy and scl loss func-\ntion. For Ablation in Table. 5, B1 means that the model\nis trained by SimCLR (Chen et al., 2020). B2 omits the\ndiffusion loss and trains the encoder with only the soft CL\nloss. B3 omits the soft CL loss and trains the encoder with\nInfoNCE loss. B4 and B5 talk about the training strategy\nof DiffAug. B4 denotes training the model by integrating\ntwo loss functions, i.e., mixing A-Step and B-Step to update\nthe parameters of both networks simultaneously through a\nsingle forward propagation.B5 denotes the default training\nstrategy, which trains the model by alternating the two loss\nfunctions. The results from these experiments can be found\nin Table 5. First, we observe that either replacing the scl\nloss or replacing the diff model (B2 or B3) brings about per-\nformance degradation, which implies that the two modules\nof DiffAug work in conjunction with each other. Second,\nwe observe that on some datasets, the performance of the\ntwo training strategies (B4 and B5) is comparable, but on\nothers, the EM method demonstrates higher stability. We\nattribute this to the fact that the difficulty of diffusion model\ntraining varies from data to data, and simultaneous training\nmay result in the two modules being unable to match at all\ntimes, bringing about instability in training. However, the\nE-M training approach avoids this problem.\n3.6. Hyperparametric Analysis and Toxicity Analysis\nFinally, we investigate the performance improvement and\npotential toxicity of the DiffAug method through hyper-\nparametric analysis. The hyperparameter λ determines how\noften the model generated by DiffAug affects the train-\ning of the semantic encoder. Introducing the augmentation\ndata (λ = 0) brings the method back to traditional CL\nmethods, while too much (λ = 1) will lead to the encoder\n8\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\ncrashing. To demonstrate this, we tested the model perfor-\nmance of different λ counterparts on two visual datasets\n(CF10, CF100) and two bio-feature datasets (SAM561 and\nMC1374). As shown in Figure. 3, the change in perfor-\nmance brought about by λ is consistent across datasets.\nSpecifically, setting λ = 0.1 or λ = 0.15 provides the most\nsignificant gain. We believe that λ = 0.1 may be a suitable\ndefault setting for most datasets.\n4. Conclusion\nIn summary, we presented DiffAug, an innovative con-\ntrastive learning framework that leverages diffusion-based\naugmentation to enhance the robustness and generalization\nof unsupervised learning. Unlike many existing methods,\nDiffAug operates independently of prior knowledge or ex-\nternal labels, positioning it as a versatile augmentation tool\nwith notable performance in vision and life sciences. Our\ntests reveal that DiffAug consistently boosts classification\nand clustering accuracy across multiple datasets.\nAcknowledgements\nThis work was supported by the National Science and\nTechnology Major Project (No.2022ZD0115101), the Na-\ntional Natural Science Foundation of China Project (No.\nU21A20427), and Project (No.WU2022A009) from the Cen-\nter of Synthetic Biology and Integrated Bioengineering of\nWestlake University. We thank the Westlake University HPC\nCenter for providing computational resources. This work\nwas done when Zelin Zang was intern at DAMO Academy.\nThis work was supported by Alibaba Group through Alibaba\nResearch Intern Program.\nThis research is supported by the National Research Founda-\ntion, Singapore under its AI Singapore Programme (AISG\nAward No: AISG2-PhD-2021-08-008). Yang You’s research\ngroup is being sponsored by NUS startup grant (Presiden-\ntial Young Professorship), Singapore MOE Tier-1 grant,\nByteDance grant, ARCTIC grant, SMI grant (WBS num-\nber: A-8001104-00-00), Alibaba grant, and Google grant\nfor TPU usage.\nImpact Statements\nThe introduction of DiffAug, our novel approach to unsuper-\nvised contrastive learning, presents an opportunity to reflect\non the ethical considerations inherent in the advancement of\nmachine learning technologies. A key aspect of DiffAug is\nits reliance on a conditional diffusion model for the genera-\ntion of new, positive data samples. This process, rooted in\nunsupervised learning, underscores our commitment to min-\nimizing human intervention and, by extension, the potential\nfor human bias in the initial stages of data handling. By\nautomating the generation of training data, we aim to reduce\nthe subjective influences that may arise from hand-designed\nmethods, thus promoting a more objective and equitable\ndevelopment of machine learning models. Furthermore,\nthe iterative training process of the semantic encoder and\ndiffusion model in DiffAug is designed to ensure that the\ngenerated data remains true to the original dataset’s seman-\ntic integrity, thereby upholding the principles of fairness and\ntransparency in AI.\nThe deployment of DiffAug is poised to have a transfor-\nmative effect on a wide array of sectors, leveraging the\nuntapped potential of unsupervised learning to interpret\ncomplex datasets across disciplines. In healthcare, for ex-\nample, DiffAug’s ability to enhance representation learning\nwithout extensive labeled datasets could revolutionize the\nearly detection and diagnosis of diseases, making health-\ncare more accessible and efficient. The fundamental shift\ntowards unsupervised learning, exemplified by DiffAug,\nalso heralds a new era of innovation in which the reliance\non large, labeled datasets is significantly reduced, thereby\ndemocratizing access to advanced machine learning tech-\nnologies.\nReferences\nAldridge, S. and Teichmann, S. A. Single cell transcrip-\ntomics comes of age. Nature Communications, 11(1):\n4307, 2020.\nAntoniou, A., Storkey, A., and Edwards, H. Data aug-\nmentation generative adversarial networks, 2017. URL\nhttps://arxiv.org/abs/1711.04340.\nAssran, M., Caron, M., Misra, I., Bojanowski, P., Bordes,\nF., Vincent, P., Joulin, A., Rabbat, M., and Ballas, N.\nMasked siamese networks for label-efficient learning. In\nECCV, pp. 456–473. Springer, 2022.\nBesnier, V., Jain, H., Bursuc, A., Cord, M., and P´erez,\nP.\nThis dataset does not exist:\nTraining models\nfrom generated images.\nIn ICASSP 2020, pp. 1–\n5. IEEE, 2020.\ndoi:\n10.1109/ICASSP40776.2020.\n9053146.\nURL https://doi.org/10.1109/\nICASSP40776.2020.9053146.\nBlanco-Gonzalez, A., Cabezon, A., Seco-Gonzalez, A.,\nConde-Torres, D., Antelo-Riveiro, P., Pineiro, A., and\nGarcia-Fandino, R. The role of ai in drug discovery: chal-\nlenges, opportunities, and strategies. Pharmaceuticals,\n16(6):891, 2023.\nBotton, A., Barberi, G., and Facco, P. Data augmentation to\nsupport biopharmaceutical process development through\ndigital models—a proof of concept. Processes, 10(9):\n1796, 2022.\n9\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nBrock, A., Donahue, J., and Simonyan, K. Large scale\nGAN training for high fidelity natural image synthesis.\nIn ICLR 2019, 2019. URL https://openreview.\nnet/forum?id=B1xsqj09Fm.\nBustamante, C. D., De La Vega, F. M., and Burchard, E. G.\nGenomics for the world. Nature, 475(7355):163–165,\n2011.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A Sim-\nple Framework for Contrastive Learning of Visual Repre-\nsentations. arXiv:2002.05709 [cs, stat], June 2020. URL\nhttp://arxiv.org/abs/2002.05709.\narXiv:\n2002.05709.\nChen, X. and He, K. Exploring simple siamese representa-\ntion learning. In CVPR, pp. 15750–15758, 2021.\nCoates, A., Ng, A., and Lee, H. An analysis of single-\nlayer networks in unsupervised feature learning. In Pro-\nceedings of the fourteenth international conference on\nartificial intelligence and statistics, pp. 215–223. JMLR\nWorkshop and Conference Proceedings, 2011.\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In CVPR, pp. 702–703, 2020.\nCui, J., Zhong, Z., Liu, S., Yu, B., and Jia, J. Parametric\ncontrastive learning. In ICCV, pp. 715–724, 2021.\nDalla-Torre, H., Gonzalez, L., Mendoza-Revilla, J., Car-\nranza, N. L., Grzywaczewski, A. H., Oteri, F., Dallago,\nC., Trop, E., de Almeida, B. P., Sirelkhatim, H., et al. The\nnucleotide transformer: Building and evaluating robust\nfoundation models for human genomics. bioRxiv, pp.\n2023–01, 2023.\nDat, P. T., Dutt, A., Pellerin, D., and Qu´enot, G. Classi-\nfier training from a generative model. In 2019 Interna-\ntional Conference on Content-Based Multimedia Index-\ning (CBMI), pp. 1–6, 2019. doi: 10.1109/CBMI.2019.\n8877479.\nDeng, D., Chen, G., Hao, J., Wang, Q., and Heng, P.-A. Flat-\ntening sharpness for dynamic gradient projection memory\nbenefits continual learning. Advances in Neural Informa-\ntion Processing Systems, 34:18710–18721, 2021.\nDhariwal, P. and Nichol, A. Q.\nDiffusion models\nbeat gans on image synthesis.\nIn Ranzato, M.,\nBeygelzimer, A., Dauphin, Y. N., Liang, P., and\nVaughan, J. W. (eds.), NeurIPS 2021, pp. 8780–\n8794,\n2021.\nURL\nhttps://proceedings.\nneurips.cc/paper/2021/hash/\n49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.\nhtml.\nDu, X., Sun, Y., Zhu, X., and Li, Y. Dream the impossible:\nOutlier imagination with diffusion models, 2023.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY.\nGenerative adversarial nets.\nIn Ghahramani, Z.,\nWelling, M., Cortes, C., Lawrence, N., and Weinberger,\nK. (eds.), NeurIPS, volume 27. Curran Associates,\nInc.,\n2014.\nURL\nhttps://proceedings.\nneurips.cc/paper/2014/file/\n5ca3e9b122f61f8f06494c97b1afccf3-Paper.\npdf.\nGreˇsov´a, K., Martinek, V., ˇCech´ak, D., ˇSimeˇcek, P., and\nAlexiou, P. Genomic benchmarks: a collection of datasets\nfor genomic sequence classification. BMC Genomic Data,\n24(1):25, 2023.\nGrill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos, R.,\nand Valko, M. Bootstrap Your Own Latent: A New Ap-\nproach to Self-Supervised Learning. arXiv:2006.07733\n[cs, stat], June 2020.\nURL http://arxiv.org/\nabs/2006.07733. arXiv: 2006.07733.\nGupta, M. R., Chen, Y., et al. Theory and use of the em al-\ngorithm. Foundations and Trends® in Signal Processing,\n4(3):223–296, 2011.\nHalko, N., Martinsson, P.-G., and Tropp, J. A. Finding\nstructure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions, 2010.\nHan, X., Wang, R., Zhou, Y., Fei, L., Sun, H., Lai, S.,\nSaadatpour, A., Zhou, Z., Chen, H., Ye, F., et al. Mapping\nthe mouse cell atlas by microwell-seq. Cell, 172(5):1091–\n1107, 2018.\nHan, X., Zhang, L., Zhou, K., and Wang, X. Progan: Pro-\ntein solubility generative adversarial nets for data aug-\nmentation in dnn framework. Computers & Chemical\nEngineering, 131:106533, 2019.\nHan, X., Zhou, Z., Fei, L., Sun, H., Wang, R., Chen, Y.,\nChen, H., Wang, J., Tang, H., Ge, W., et al. Construction\nof a human cell landscape at single-cell level. Nature,\n581(7808):303–309, 2020.\nHaque, A.\nEC-GAN: low-sample classification us-\ning semi-supervised algorithms and gans (student\nabstract).\nIn AAAI, pp. 15797–15798, 2021.\nURL\nhttps://ojs.aaai.org/index.php/AAAI/\narticle/view/17895.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep Residual\nLearning for Image Recognition. arXiv:1512.03385 [cs],\nDecember 2015. URL http://arxiv.org/abs/\n1512.03385. arXiv: 1512.03385.\n10\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738,\n2020a.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momen-\ntum Contrast for Unsupervised Visual Representation\nLearning. arXiv:1911.05722 [cs], March 2020b. URL\nhttp://arxiv.org/abs/1911.05722.\narXiv:\n1911.05722.\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Girshick, R.\nMasked autoencoders are scalable vision learners. CVPR,\n2021.\nHe, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S.,\nand Qi, X. Is synthetic data from generative models ready\nfor image recognition?, 2022. URL https://arxiv.\norg/abs/2210.07574.\nHe, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai,\nS., and Qi, X. Is synthetic data from generative models\nready for image recognition? ICLR, 2023.\nHo, J. and Salimans, T.\nClassifier-free diffusion guid-\nance, 2022.\nURL https://arxiv.org/abs/\n2207.12598.\nHo, J., Jain, A., and Abbeel, P.\nDenoising diffusion\nprobabilistic models.\nIn Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.),\nNeurIPS, 2020.\nURL https://proceedings.\nneurips.cc/paper/2020/hash/\n4c5bcfec8584af0d967f1ab10179ca4b-Abstract.\nhtml.\nHuang, H.-H., Rao, H., Miao, R., and Liang, Y. A novel\nmeta-analysis based on data augmentation and elastic data\nshared lasso regularization for gene expression. BMC\nbioinformatics, 23(Suppl 10):353, 2022.\nJahanian, A., Puig, X., Tian, Y., and Isola, P.\nGen-\nerative models as a data source for multiview repre-\nsentation learning.\nIn ICLR, 2022.\nURL https:\n//openreview.net/forum?id=qhAeZjs7dCL.\nJi, Y., Zhou, Z., Liu, H., and Davuluri, R. V. Dnabert:\npre-trained bidirectional encoder representations from\ntransformers model for dna-language in genome. Bioin-\nformatics, 37(15):2112–2120, 2021.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. In Bengio, Y. and LeCun, Y. (eds.), ICLR,, 2014.\nURL http://arxiv.org/abs/1312.6114.\nKrishnan, R., Rajpurkar, P., and Topol, E. J. Self-supervised\nlearning in medicine and healthcare. Nature Biomedical\nEngineering, pp. 1–7, 2022.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nLe, Y. and Yang, X. Tiny imagenet visual recognition chal-\nlenge. CS 231N, 7(7):3, 2015.\nLee, N. K., Tang, Z., Toneyan, S., and Koo, P. K. Evoaug:\nimproving generalization and interpretability of genomic\ndeep neural networks with evolution-inspired data aug-\nmentations. Genome Biology, 24(1):105, 2023.\nLi, B., Han, Z., Li, H., Fu, H., and Zhang, C. Trustworthy\nlong-tailed classification. arXiv: Learning, 2021.\nLi, M. and Zhang, W. Phiaf: prediction of phage-host inter-\nactions with gan-based data augmentation and sequence-\nbased feature fusion. Briefings in Bioinformatics, 23(1):\nbbab348, 2022.\nMaharana, K., Mondal, S., and Nemade, B. A review: Data\npre-processing and data augmentation techniques. Global\nTransitions Proceedings, 3(1):91–99, 2022.\nMcGibbon, M., Money-Kyrle, S., Blay, V., and Houston,\nD. R. Scorch: improving structure-based virtual screening\nwith machine learning classifiers, data augmentation, and\nuncertainty estimation. Journal of Advanced Research,\n46:135–147, 2023.\nMoon, K. R. and van Dijk. Visualizing structure and transi-\ntions in high dimensional biological data. Nature biotech-\nnology, 37(12):1482–1492, 2019.\nMoor, M., Horn, M., Rieck, B., and Borgwardt, K. Topo-\nlogical Autoencoders. arXiv preprint arXiv:1906.00722,\n2019.\nNguyen, E., Poli, M., Faizi, M., Thomas, A. W., Wornow,\nM., Birch-Sykes, C., Massaroli, S., Patel, A., Rabideau,\nC. M., Bengio, Y., et al. Hyenadna: Long-range genomic\nsequence modeling at single nucleotide resolution. In\nThirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\nP., McGrew, B., Sutskever, I., and Chen, M.\nGlide:\nTowards photorealistic image generation and editing\nwith text-guided diffusion models.\narXiv preprint\narXiv:2112.10741, 2021.\nNichol, A. Q. and Dhariwal, P. Improved denoising dif-\nfusion probabilistic models. In Meila, M. and Zhang,\nT. (eds.), ICML, volume 139 of Proceedings of Ma-\nchine Learning Research, pp. 8162–8171. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/\nnichol21a.html.\n11\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nNichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P.,\nMishkin, P., McGrew, B., Sutskever, I., and Chen, M.\nGLIDE: towards photorealistic image generation and edit-\ning with text-guided diffusion models. In Chaudhuri,\nK., Jegelka, S., Song, L., Szepesv´ari, C., Niu, G., and\nSabato, S. (eds.), ICML, volume 162 of Proceedings of\nMachine Learning Research, pp. 16784–16804. PMLR,\n2022. URL https://proceedings.mlr.press/\nv162/nichol22a.html.\nPeng, X., Wang, K., Zhu, Z., Wang, M., and You, Y. Craft-\ning better contrastive views for siamese representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 16031–\n16040, 2022.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents, 2022. URL https://arxiv.org/abs/\n2204.06125.\nRazavi, A., van den Oord, A., and Vinyals, O. Generating\ndiverse high-fidelity images with VQ-VAE-2. In Wallach,\nH. M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc,\nF., Fox, E. B., and Garnett, R. (eds.), NeurIPS 2019, pp.\n14837–14847, 2019. URL https://proceedings.\nneurips.cc/paper/2019/hash/\n5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.\nhtml.\nRethmeier, N. and Augenstein, I. A primer on contrastive\npretraining in language processing: Methods, lessons\nlearned, and perspectives. ACM Computing Surveys, 55\n(10):1–17, 2023.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B.\nHigh-resolution image synthesis with la-\ntent diffusion models. In CVPR 2022, pp. 10674–10685.\nIEEE, 2022.\ndoi: 10.1109/CVPR52688.2022.01042.\nURL https://doi.org/10.1109/CVPR52688.\n2022.01042.\nRouillard, A. D., Gundersen, G. W., Fernandez, N. F., Wang,\nZ., Monteiro, C. D., McDermott, M. G., and Ma’ayan,\nA. The harmonizome: a collection of processed datasets\ngathered to serve and mine knowledge about genes and\nproteins. Database, 2016, 2016.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Den-\nton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi,\nS. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J.,\nand Norouzi, M. Photorealistic text-to-image diffusion\nmodels with deep language understanding, 2022. URL\nhttps://arxiv.org/abs/2205.11487.\nSainburg, T., McInnes, L., and Gentner, T. Q.\nPara-\nmetric UMAP embeddings for representation and semi-\nsupervised learning. arXiv:2009.12981 [cs, q-bio, stat],\nApril 2021. URL http://arxiv.org/abs/2009.\n12981. arXiv: 2009.12981.\nSarfraz, S., Koulakis, M., Seibold, C., and Stiefelhagen,\nR. Hierarchical nearest neighbor graph embedding for\nefficient dimensionality reduction. In CVPR, pp. 336–345,\n2022.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A.,\nMullis, C., Wortsman, M., Schramowski, P., Kundurthy,\nS., Crowson, K., Schmidt, L., Kaczmarczyk, R., and\nJitsev, J.\nLaion-5b: An open large-scale dataset for\ntraining next generation image-text models, 2022. URL\nhttps://arxiv.org/abs/2210.08402.\nSuhre, K., McCarthy, M. I., and Schwenk, J. M. Genet-\nics meets proteomics: perspectives for large population-\nbased studies. Nature Reviews Genetics, 22(1):19–37,\n2021.\nSun, Y., Selvarajan, S., Zang, Z., Liu, W., Zhu, Y., Zhang,\nH., Chen, W., Chen, H., Li, L., Cai, X., et al. Artificial\nintelligence defines protein-based classification of thyroid\nnodules. Cell discovery, 8(1):85, 2022.\nSzubert, B., Cole, J. E., Monaco, C., and Drozdov, I.\nStructure-preserving visualisation of high dimensional\nsingle-cell datasets. Scientific Reports, 9(1):8914, June\n2019. ISSN 2045-2322.\nTanaka, F. H. K. d. S. and Aranha, C. Data augmentation\nusing gans, 2019. URL https://arxiv.org/abs/\n1904.09135.\nTheodoris, C. V., Xiao, L., Chopra, A., Chaffin, M. D.,\nAl Sayed, Z. R., Hill, M. C., Mantineo, H., Brydon, E. M.,\nZeng, Z., Liu, X. S., et al. Transfer learning enables\npredictions in network biology. Nature, pp. 1–9, 2023.\nTian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and\nIsola, P. What makes for good views for contrastive\nlearning? NeurIPS, 33:6827–6839, 2020.\nTrabucco, B., Doherty, K., Gurinas, M., and Salakhutdinov,\nR. Effective data augmentation with diffusion models.\narXiv preprint arXiv:2302.07944, 2023.\nTran, T., Pham, T., Carneiro, G., Palmer, L. J., and Reid,\nI. D.\nA bayesian data augmentation approach for\nlearning deep models. In Guyon, I., von Luxburg, U.,\nBengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S.\nV. N., and Garnett, R. (eds.), NeurIPS 2017, pp. 2797–\n2806,\n2017.\nURL\nhttps://proceedings.\nneurips.cc/paper/2017/hash/\n076023edc9187cf1ac1f1163470e479a-Abstract.\nhtml.\n12\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nVan der Maaten, L. and Hinton, G. Visualizing data using\nt-sne. JMLR, 9(11), 2008.\nWang, X. and Qi, G.-J. Contrastive learning with stronger\naugmentations. TPAMI, 45(5):5549–5560, 2022.\nWang, Y., Huang, H., Rudin, C., and Shaposhnik, Y. Un-\nderstanding how dimension reduction tools work: An\nempirical approach to deciphering t-sne, umap, trimap,\nand pacmap for data visualization. JMLR, 22(201):1–\n73, 2022. URL http://jmlr.org/papers/v22/\n20-1061.html.\nWeber, L. M. and Robinson, M. D. Comparison of clustering\nmethods for high-dimensional single-cell flow and mass\ncytometry data. Cytometry Part A, 89(12):1084–1096,\n2016.\nWickramaratne, S. and Mahmud, M. S. Conditional-gan\nbased data augmentation for deep learning task classifier\nimprovement using fnirs data. Frontiers in Big Data, 4:\n659146, 07 2021. doi: 10.3389/fdata.2021.659146.\nXiong, W., He, Y., Zhang, Y., Luo, W., Ma, L., and Luo,\nJ. Fine-grained image-to-image transformation towards\nvisual recognition. In CVPR 2020, June 2020.\nXu, M., Yoon, S., Fuentes, A., and Park, D. S. A comprehen-\nsive survey of image augmentation techniques for deep\nlearning. Pattern Recognition, pp. 109347, 2023.\nYamaguchi,\nS.,\nKanai,\nS.,\nand Eda,\nT.\nEffec-\ntive data augmentation with multi-domain learning\ngans.\nIn AAAI 2020, pp. 6566–6574. AAAI Press,\n2020.\nURL https://ojs.aaai.org/index.\nphp/AAAI/article/view/6131.\nYan, Z., Xu, L., Suzuki, A., Wang, J., Cao, J., and Huang,\nJ. Rgb color model aware computational color naming\nand its application to data augmentation. In 2022 IEEE\nInternational Conference on Big Data (Big Data), pp.\n1172–1181. IEEE, 2022.\nYu, T., Cui, H., Li, J. C., Luo, Y., Jiang, G., and Zhao, H.\nEnzyme function prediction using contrastive learning.\nScience, 379(6639):1358–1363, 2023.\nZang, Z., Cheng, S., Lu, L., Xia, H., Li, L., Sun, Y., Xu, Y.,\nShang, L., Sun, B., and Li, S. Z. Evnet: An explainable\ndeep network for dimension reduction. TVCG, 2022a.\nZang, Z., Li, S., Wu, D., Wang, G., Wang, K., Shang, L.,\nSun, B., Li, H., and Li, S. Z. Dlme: Deep local-flatness\nmanifold embedding. ECCV, pp. 576–592, 2022b.\nZang, Z., Shang, L., Yang, S., Wang, F., Sun, B., Xie, X.,\nand Li, S. Z. Boosting novel category discovery over\ndomains with soft contrastive learning and all-in-one clas-\nsifier. ICCV, 2023.\nZhang, H., Ciss´e, M., Dauphin, Y. N., and Lopez-Paz, D.\nmixup: Beyond empirical risk minimization.\nCoRR,\nabs/1710.09412, 2017. URL http://arxiv.org/\nabs/1710.09412.\nZhang, J. and Ma, K. Rethinking the augmentation module\nin contrastive learning: Learning hierarchical augmenta-\ntion invariance with expanded views. In CVPR 2022, pp.\n16650–16659, 2022.\nZhang, S., Liu, M., Yan, J., Zhang, H., Huang, L., Yang,\nX., and Lu, P. M-mix: Generating hard negatives via\nmulti-sample mixing for contrastive learning. In KDD,\npp. 2461–2470, 2022.\nZhang, Y., Chen, W., Ling, H., Gao, J., Zhang, Y., Torralba,\nA., and Fidler, S. Image gans meet differentiable ren-\ndering for inverse graphics and interpretable 3d neural\nrendering. In ICLR, 2021a.\nZhang, Y., Ling, H., Gao, J., Yin, K., Lafleche, J., Barriuso,\nA., Torralba, A., and Fidler, S. Datasetgan: Efficient\nlabeled data factory with minimal human effort. In CVPR,\npp. 10145–10155, 2021b.\nZhang, Y., Zhou, D., Hooi, B., Wang, K., and Feng, J.\nExpanding small-scale datasets with guided imagination,\n2023a.\nZhang, Y., Zhu, H., and Yu, S. Adaptive data augmentation\nfor contrastive learning. ICASSP 2023, pp. 1–5, 2023b.\nZheng, Z., Zheng, L., and Yang, Y. Unlabeled samples gen-\nerated by GAN improve the person re-identification base-\nline in vitro. In ICCV 2017, pp. 3774–3782. IEEE Com-\nputer Society, 2017. doi: 10.1109/ICCV.2017.405. URL\nhttps://doi.org/10.1109/ICCV.2017.405.\nZheng, Z., Le, N. Q. K., and Chua, M. C. H. Maskdna-\npgd: An innovative deep learning model for detecting\ndna methylation by integrating mask sequences and ad-\nversarial pgd training as a data augmentation method.\nChemometrics and Intelligent Laboratory Systems, 232:\n104715, 2023.\n13\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nAppendix\nContents\nA. Appendix: Related Works\nGenerative Models\nGenerative models have been the subject of growing interest and rapid advancement. Earlier methods,\nincluding VAEs (Kingma & Welling, 2014) and GANs (Goodfellow et al., 2014), showed initial promise generating realistic\nimages, and were scaled up in terms of resolution and sample quality (Brock et al., 2019; Razavi et al., 2019). Despite\nthe power of these methods, many recent successes in photorealistic image generation were the result of diffusion models\n(Ho et al., 2020; Nichol & Dhariwal, 2021; Saharia et al., 2022; Nichol et al., 2022; Ramesh et al., 2022). Diffusion\nmodels have been shown to generate higher-quality samples compared to their GAN counterparts (Dhariwal & Nichol,\n2021), and developments like classifier free guidance (Ho & Salimans, 2022) have made text-to-image generation possible.\nRecent emphasis has been on training these models with internet-scale datasets like LAION-5B (Schuhmann et al., 2022).\nGenerative models trained at internet-scale (Rombach et al., 2022; Saharia et al., 2022; Nichol et al., 2022; Ramesh et al.,\n2022) have unlocked several application areas where photorealistic generation is crucial.\nSynthetic Image Data Generation\nTraining neural networks on synthetic data from generative models was popularized\nusing GANs (Antoniou et al., 2017; Tran et al., 2017; Zheng et al., 2017). Various applications for synthetic data generated\nfrom GANs have been studied, including representation learning (Jahanian et al., 2022), inverse graphics (Zhang et al.,\n2021a), semantic segmentation (Zhang et al., 2021b), and training classifiers (Tanaka & Aranha, 2019; Dat et al., 2019;\nYamaguchi et al., 2020; Besnier et al., 2020; Xiong et al., 2020; Wickramaratne & Mahmud, 2021; Haque, 2021). More\nrecently, synthetic data from diffusion models has also been studied in a few-shot setting (He et al., 2022). These works\nuse generative models that have likely seen images of target classes and, to the best of our knowledge, we present the first\nanalysis for synthetic data on previously unseen concepts. (Du et al., 2023) proposed the DREAM-OOD framework, which\nuses diffusion models to generate photo-realistic outliers from in-distribution data for improved OOD detection. By learning\na text-conditioned latent space, it visualizes imagined outliers directly in pixel space, showing promising results in empirical\nstudies. (Zhang et al., 2023a) developed the Guided Imagination Framework (GIF) using generative models like DALL-E2\nand Stable Diffusion for dataset expansion, enhancing accuracy in both natural and medical image datasets.\nSynthetic Biology Data Generation\nThe realm of synthetic biology has witnessed a surge in the utilization of data-driven\napproaches, particularly with the advent of advanced computational models. The generation of synthetic biological data has\nbeen instrumental in predicting protein structures (McGibbon et al., 2023). The use of Generative Adversarial Networks\n(GANs) has also found its way into this domain, aiding in the c reation of synthetic DNA sequences (Zheng et al., 2023;\nLi & Zhang, 2022; Han et al., 2019) and simulating cell behaviors (Botton et al., 2022). Furthermore, the integration of\nmachine learning with synthetic biology has paved the way for innovative solutions in drug discovery (Blanco-Gonzalez\net al., 2023; McGibbon et al., 2023). Unlike the synthetic image data generation, where models have often seen images\nof target classes, synthetic biology data generation often grapples with the challenge of generating data for entirely novel\nbiological entities. This presents a unique set of challenges and opportunities, pushing the boundaries of what synthetic data\ncan achieve in the realm of biology.\nB. Appendix: Details of Contrastive Learning and Soft Contrastive Learning\nB.1. The t-kernel similarity in soft contrastive learning\nTo map the high-dimensional embedding vector to a probability value, a kernel function S(·) is used. In this paper, we use\nthe t-distribution kernel function Sν(·) because it exposes the degrees of freedom and allows us to adjust the closeness of\nthe distribution in the dimensionality reduction mapping (Li et al., 2021). The t-distribution kernel function is defined as\nfollows,\nS(zi, zj) = Γ ((ν + 1)/2)\n\u00001 + ∥zi −zj∥2\n2/ν\n\u0001−ν+1\n2 /√νπΓ (ν/2),\n(9)\nwhere Γ(·) is the Gamma function. The degrees of freedom ν control the shape of the kernel function. The different degrees\nof freedom (νy, νz) is used in Ry and Rz for the dimensional reduction mapping.\n14\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nB.2. Why Soft Contrastive Learning is a softened version of Contrastive Learning\nLemma\nB.1.\nLet\nLcl\n=\n−log Q\n\u0000zi, z+\ni\n\u0001\n+\nlog\nh\nQ\n\u0000zi, z+\ni\n\u0001\n+ P\nz−\ni ∈V −Q\n\u0000zi, z−\ni\n\u0001i\nand\nLp\ncl\n=\n−PNK+1\nj=1\nn\nHij log Qij + (1 −Hij) log ˙Qij\no\nThen limx→∞Lcl −Lp\ncl = 0 .\nProof. We start with LCL = −log\nexp(S(zi,zj))\nPNK\nk=1 exp(S(zi,zk)) (Eq. (3)), then\nLCL = log NK −log\nexp(S(zi, zj))\n1\nNK\nPNK\nk=1 exp(S(zi, zk))\n.\nWe are only concerned with the second term that has the gradient. Let (i, j) are positive pair and (i, k1), · · · , (i, kN) are\nnegative pairs. The overall loss associated with point i is:\n−log\nexp(S(zi, zj))\n1\nNK\nPNK\nk=1 exp(S(zi, zk))\n= −\n\"\nlog exp(S(zi, zj)) −log\n1\nNK\nNK\nX\nk=1\nexp(S(zi, zk))\n#\n= −\n\"\nlog exp(S(zi, zj)) −\nNK\nX\nk=1\nlog exp(S(zi, zk)) +\nNK\nX\nk=1\nlog exp(S(zi, zk)) −log\n1\nNK\nNK\nX\nk=1\nexp(S(zi, zk))\n#\n= −\n\"\nlog exp(S(zi, zj)) −\nNK\nX\nk=1\nlog exp(S(zi, zk)) + log ΠNK\nk=1 exp(S(zi, zk)) −log\n1\nNK\nNK\nX\nk=1\nexp(S(zi, zk))\n#\n= −\n\"\nlog exp(S(zi, zj)) −\nNK\nX\nk=1\nlog exp(S(zi, zk)) + log\nΠNK\nk=1 exp(S(zi, zk))\n1\nNK\nPNK\nk=1 exp(S(zi, zk))\n#\nWe focus on the case where the similarity is normalized, S(zi, zk) ∈[0, 1]. The data i and data k is the negative samples, then\nS(zi, zk) is near to 0, exp(S(zi, zk)) is near to 1, thus the\nΠ\nNK\nk=1 exp(S(zi,zk))\n1\nN\nPNK\nk=1 exp(S(zi,zk)) is near to 1, and log\nΠ\nNK\nk=1 exp(S(zi,zk))\n1\nN\nPNK\nk=1 exp(S(zi,zk))\nnear to 0. We have\nLCL ≈−\n\"\nlog exp(S(zi, zj)) −\nNK\nX\nk=1\nlog exp(S(zi, zk))\n#\nWe denote ij and ik by a uniform index and use Hij to denote the homology relation of ij.\nLCL ≈−\n\"\nlog exp(S(zi, zj)) −\nNK\nX\nk=1\nlog exp(S(zi, zk))\n#\n≈−\n\nHij log exp(S(zi, zj)) −\nNK\nX\nj=1\n(1 −Hij) log exp(S(zi, zj))\n\n\n≈−\n\n\nNK+1\nX\nj=1\n{Hij log exp(S(zi, zj)) + (1 −Hij) log{exp(−S(zi, zj))}}\n\n\nwe define the similarity of data i and data j as Qij = exp(S(zi, zj)) and the dissimilarity of data i and data j as\n˙Qij = exp(−S(zi, zj)).\n15\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nLCL ≈−\n\n\nNK+1\nX\nj=1\nn\nHij log Qij + (1 −Hij) log ˙Qij\no\n\n\nThe proposed SCL loss is a smoother CL loss:\nThis proof tries to indicate that the proposed SCL loss is a smoother CL loss. We discuss the differences by comparing\nthe two losses to prove this point. the forward propagation of the network is, zi = H(ˆzi), ˆzi = F(xi), zj = H(ˆzj), ˆzj =\nF(xj). We found that we mix y and ˆz in the main text, and we will correct this in the new version. So, in this section\nzi = H(yi), yi = F(xi), zj = H(yj), yj = F(xj) is also correct.\nLet H(·) satisfy K-Lipschitz continuity, then dz\nij = k∗dy\nij, k∗∈[1/K, K], where k∗is a Lipschitz constant. The difference\nbetween LSCL loss and LCL loss is,\nLCL −LSCL ≈\nX\nj\n\u0014\u0000Hij −[1 + (eα −1)Hij]κ\n\u0000dy\nij\n\u0001\u0001\nlog\n \n1\nκ\n\u0000dz\nij\n\u0001 −1\n! \u0015\n.\n(10)\nBecause the α > 0, the proposed SCL loss is the soft version of the CL loss. if Hij = 1, we have:\n(LCL −LSCL)|Hij=1 =\nX\u0014\u0000(1 −eα)κ\n\u0000k∗dz\nij\n\u0001\u0001\nlog\n \n1\nκ\n\u0000dz\nij\n\u0001 −1\n! \u0015\n(11)\nthen:\nlim\nα→0(LCL −LSCL)|Hij=1 = lim\nα→0\nX\u0014\u0000(1 −eα)κ\n\u0000k∗dz\nij\n\u0001\u0001\nlog\n \n1\nκ\n\u0000dz\nij\n\u0001 −1\n! \u0015\n= 0\n(12)\nBased on Eq.(12), we find that if i, j is neighbor (Hij = 1) and α →0, there is no difference between the CL loss LCL and\nSCL loss LSCL. When if Hij = 0, the difference between the loss functions will be the function of dz\nij. The CL loss LCL\nonly minimizes the distance between adjacent nodes and does not maintain any structural information. The proposed SCL\nloss considers the knowledge both comes from the output of the current bottleneck and data augmentation, thus less affected\nby view noise.\nDetails of Eq. (10). Due to the very similar gradient direction, we assume ˙Qij = 1 −Qij. The contrastive learning loss is\nwritten as,\nLCL ≈−\nX\n{Hij log Qij + (1 −Hij) log (1 −Qij)}\n(13)\nwhere Hij indicates whether i and j are augmented from the same original data.\nThe SCL loss is written as:\nLSCL = −\nX\n{Pij log Qij + (1 −Pij) log (1 −Qij)}\n(14)\nAccording to Eq. (4) and Eq. (5), we have\nPij = Rijκ(dy\nij) = Rijκ(yi, yj), Rij =\n\u001a eα\nif H(xi, xj) = 1\n1\notherwise\n,\nQij = κ(dz\nij) = κ(zi, zj),\n(15)\nFor ease of writing, we use distance as the independent variable, dy\nij = ∥yi −yj∥2, dz\nij = ∥zi −zj∥2.\n16\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nThe difference between the two loss functions is:\nLCL −LSCL\n= −\nX\u0014\nHij log κ\n\u0000dz\nij\n\u0001\n+ (1 −Hij) log\n\u00001 −κ\n\u0000dz\nij\n\u0001\u0001\n−Rijκ\n\u0000dy\nij\n\u0001\nlog κ\n\u0000dz\nij\n\u0001\n−\n\u00001 −Rijκ\n\u0000dy\nij\n\u0001\u0001\nlog\n\u00001 −κ\n\u0000dz\nij\n\u0001\u0001 \u0015\n= −\nX\u0014\u0000Hij −Rijκ\n\u0000dy\nij\n\u0001\u0001\nlog κ\n\u0000dz\nij\n\u0001\n+\n\u00001 −Hij −1 + Rijκ\n\u0000dy\nij\n\u0001\u0001\nlog\n\u00001 −κ\n\u0000dz\nij\n\u0001\u0001 \u0015\n= −\nX\u0014\u0000Hij −Rijκ\n\u0000dy\nij\n\u0001\u0001\nlog κ\n\u0000dz\nij\n\u0001\n+\n\u0000Rijκ\n\u0000dy\nij\n\u0001\n−Hij\n\u0001\nlog\n\u00001 −κ\n\u0000dz\nij\n\u0001\u0001 \u0015\n= −\nX\u0014\u0000Hij −Rijκ\n\u0000dy\nij\n\u0001\u0001 \u0000log κ\n\u0000dz\nij\n\u0001\n−log\n\u00001 −κ\n\u0000dz\nij\n\u0001\u0001\u0001 \u0015\n=\nX\u0014\u0000Hij −Rijκ\n\u0000dy\nij\n\u0001\u0001\nlog\n \n1\nκ\n\u0000dz\nij\n\u0001 −1\n! \u0015\n(16)\nSubstituting the relationship between Hij and Rij, Rij = 1 + (eα −1)Hij, we have\nLCL −LSCL =\nX\u0014\u0000Hij −[1 + (eα −1)Hij]κ\n\u0000dy\nij\n\u0001\u0001\nlog\n \n1\nκ\n\u0000dz\nij\n\u0001 −1\n! \u0015\n(17)\nWe assume that network H(·) to be a Lipschitz continuity function, then\n1\nK H(dz\nij) ≤dy\nij ≤KH(dz\nij)\n∀i, j ∈{1, 2, · · · , N}\n(18)\nWe construct the inverse mapping of H(·) to H−1(·),\n1\nK dz\nij ≤dy\nij ≤Kdz\nij\n∀i, j ∈{1, 2, · · · , N}\n(19)\nand then there exists k∗:\ndy\nij = k∗dz\nij\nk∗∈[1/K, K]\n∀i, j ∈{1, 2, · · · , N}\n(20)\nSubstituting the Eq.(20) into Eq.(17).\nLCL −LSCL =\nX\u0014\u0000Hij −[1 + (eα −1)Hij]κ\n\u0000k∗dz\nij\n\u0001\u0001\nlog\n \n1\nκ\n\u0000dz\nij\n\u0001 −1\n! \u0015\n(21)\nC. Appendix: Details of DNA Sequence Experiments\nC.1. Experimental Setups and Datasets Information\nIn our research, we utilized the ’genomic-benchmarks’ dataset, a comprehensive collection of curated sequence classification\ndatasets specifically designed for genomic studies. This repository encompasses a range of datasets derived from both novel\ncompilations mined from publicly accessible databases and existing datasets gathered from published studies. It focuses on\nregulatory elements such as promoters, enhancers, and open chromatin regions from three model organisms: humans, mice,\nand roundworms. Accompanying these datasets is a simple convolutional neural network provided as a baseline model,\nenabling researchers to benchmark their algorithms effectively. The entire collection is made available as a Python package,\nfacilitating easy integration with popular deep learning libraries and serving as a valuable resource for the genomics research\ncommunity.\n17\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nThe initiative behind the ’genomic-benchmarks’ dataset aims to address the critical need for standardized benchmarks in\ngenomics, akin to the role that carefully curated benchmarks have played in advancing other biological fields, notably\ndemonstrated by AlphaFold’s success in protein folding. By offering a structured and easily accessible set of benchmarks,\nthis collection not only promotes comparability and reproducibility in machine learning applications within genomics but\nalso lowers the entry barrier for researchers new to this domain. Consequently, it fosters a healthy competitive environment\nthat is likely to spur innovation and discovery in genomic research, paving the way for significant advancements in the\nunderstanding and annotation of genomes.\nTable 6. GenomicBenchmarks Dataset Metadata\nName\nAcronyms\nNum. Seqs\nNum. Classes\nMedian Len\nStd\ndummy mouse enhancers ensembl\nMoEnEn\n1,210\n2\n2,381\n984.4\ndemo coding vs intergenomic seqs\nCoIn\n100,000\n2\n200\n0\ndemo human or worm\nHuWo\n100,000\n2\n200\n0\nhuman enhancers cohn\nHuEnCo\n27,791\n2\n500\n0\nhuman enhancers ensembl\nHuEnEn\n154,842\n2\n269\n122.6\nhuman ensembl regulatory\nHuEnRe\n289,061\n3\n401\n184.3\nhuman nontata promoters\nHuNoPr\n36,131\n2\n251\n0\nhuman ocr ensembl\nHuOcEn\n174,756\n2\n315\n108.1\nD. Appendix: Details of Vision Experiments\nD.1. Dataset Setups\nExperiments are performed on CIFAR-10 [CF10]3 and CIFAR-1004 [CF100] (Krizhevsky et al., 2009), STL105 (Coates\net al., 2011), TinyImageNet6 [TINet] (Le & Yang, 2015) dataset.\nTo compare with the two different baseline methods, the setting of the dataset is shown in Table. 7.\nTable 7. Dataset setting of linear-test Performance.\nDataset\nTrain Split\nTest Split\nTrain Samples\nTest Samples\nClasses\nCF10\nTrain\nTest\n50,000\n10,000\n10\nCF100\nTrain\nTest\n50,000\n10,000\n100\nSTL10\nTrain + Unlabeled\nTest\n5,000+100,000\n8,000\n10\nTINet\nTrain\nTest\n100,000\n100,000\n200\nTable 8. Dataset setting of clustering test.\nDataset\nTrain & Test Split\nTrain & Test Samples\nClasses\nCF10\nTrain+Test\n60,000\n10\nCF100\nTrain+Test\n60,000\n20\nSTL10\nTrain+Test\n13,000\n10\nTIN\nTrain\n100,000\n200\nD.2. Baseline Methods and Implementation Details\nThe contrastive learning methods, including SimCLR (Chen et al., 2020), MOCO v2 (He et al., 2020b), BYOL (Grill et al.,\n2020), SimSiam (Chen & He, 2021), and DLME (Zang et al., 2022b) are chosen for comparison. The SimC.+Mix. and\nMoCo.+Mix. are SimCLR and MoCoV2 with Mixup data augmentation which processed by (Zhang et al., 2022). The\n3https://www.cs.toronto.edu/ kriz/cifar.html\n4https://www.cs.toronto.edu/ kriz/cifar.html\n5https://cs.stanford.edu/ acoates/stl10/\n6https://www.kaggle.com/c/tiny-imagenet\n18\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nSimC.+Dif. and MoCo.+Mix. are SimCLR and MoCoV2 with DiffAug data augmentation. Improvements over the best\nbaseline are shown in parentheses.\nFor the Linear-test performance assessment, we followed a procedure similar to SimCLR (Chen et al., 2020). We evaluated\nthe model’s representations linearly on top of the frozen features. This ensures that the quality of the representations is\nattributed only to the pre-training task, without any influence from potential fine-tuning. We used the ResNet-50 (He et al.,\n2015) backbone as the encoder and a standard diffusion backbone as diffusion model (in code below). In contrast, for\nDiffAug, its semantic encoder served as the contrastive learning backbone, trained using DiffAug-augmented images. For\nthe kMeans clustering evaluation, we extracted feature vectors from the models, leaving out the top classification layer. We\nthen applied kMeans clustering to these features. The primary metric for evaluation was clustering accuracy.\nListing 1. DiffusionModel for Vision Task\n1\nclass DiffusionModelVision(nn.Module):\n2\ndef __init__(self, c_in=3, c_out=3, time_dim=256):\n3\nsuper().__init__()\n4\nself.time_dim = time_dim\n5\nself.remove_deep_conv = remove_deep_conv\n6\nself.inc = DoubleConv(c_in, 16)\n7\nself.down1 = Down(16, 32)\n8\nself.sa1 = SelfAttention(32)\n9\nself.down2 = Down(32, 64)\n10\nself.sa2 = SelfAttention(64)\n11\nself.down3 = Down(64, 64)\n12\nself.sa3 = SelfAttention(64)\n13\nself.up1 = Up(128, 32)\n14\nself.sa4 = SelfAttention(32)\n15\nself.up2 = Up(64, 16)\n16\nself.sa5 = SelfAttention(16)\n17\nself.up3 = Up(32, 16)\n18\nself.sa6 = SelfAttention(16)\n19\nself.outc = nn.Conv2d(16, c_out, kernel_size=1)\n20\ndef pos_encoding(self, t, channels):\n21\ninv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=one_param(self)\n.device).float() / channels))\n22\npos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n23\npos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n24\npos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n25\nreturn pos_enc\n26\n27\ndef forward(self, x, t):\n28\nt = t.unsqueeze(-1)\n29\nt = self.pos_encoding(t, self.time_dim)\n30\nreturn self.unet_forwad(x, t)\nOur training strategy is as follows: A-step: 200 epochs →B-Step: 400 epoch →A-Step: 800 epoch. Continued training will\nfurther improve performance, but we did not increase the amount of computation due to computational resource constraints.\nThe time loss of the method does improve due to the use of the diffusion model. However, on small datasets, this boost is\nacceptable. In this way at the same time DiffAug gives the possibility to accomplish unsupervised comparison learning\ntraining on small datasets.\nTable 9. Details of the training process in vision dataset.\nCF10\nν\nLearning Rate\nWeight Decay\nBatch Size\nGPU\npix\nTraining Time\nCF10\n1\n0.001\n1e-6\n256\n1*V100\n32×32\n7.1 hours\nCF100\n2\n0.001\n1e-6\n256\n1*V100\n32×32\n7.2 hours\nSTL10\n5\n0.001\n1e-6\n256\n1*V100\n96×96\n15.1 hours\nTINet\n3\n0.001\n1e-6\n256\n1*V100\n64×64\n20.6 hours\n19\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nSTL10 (96*96*3)\nCF10 (32*32*3)\nFigure 6. The display of original and generated images illustrates that DiffAug generates semantically similar augmented images.\nOri means original image and Aug1, Aug2 and Aug3 are augmentated images. More detailed results are in the appendix.\nD.3. Data Augmentation of the Compared Methods\nBYOL augmentation.\nThe BYOL augmentation method is a hand-designed method. It is composed of four parts: random\ncropping, left-right flip, color ji\n• Random cropping: A random patch of the image is selected, with an area uniformly sampled between 8% and 100% of\nthat of the original image, and an aspect ratio logarithmically sampled between 3/4 and 4/3. This patch is then resized\nto the target size of 224 × 224 using bicubic interpolation.\n• Optional left-right flip.\n• Color jittering: The brightness, contrast, saturation, and hue of the image are shifted by a uniformly random offset\napplied to all the pixels of the same image. The order in which these shifts are performed is randomly selected for each\npatch.\n• Color dropping: An optional conversion to grayscale. When applied, the output intensity for a pixel (r, g, b) corresponds\nto its luma component, computed as 0.2989r + 0.5870g + 0.1140b1.\nSimCLR augmentation.\n• Random Cropping: This involves taking a random crop of the image and then resizing it back to the original size. This\ncan be seen as a combination of zooming and spatial location changes.\n• Random Flipping: Randomly flip the image horizontally.\n• Color Distortion: Apply a random color distortion. In the SimCLR paper, they use a combination of random brightness,\nrandom contrast, random saturation, and random hue changes. The strength of these distortions is controlled by a factor.\n• Gaussian Blur: Apply a random Gaussian blur to the image. The extent of blurring is controlled by a factor.\n20\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nMoCo v2 augmentation.\nFor MoCo v2, the data augmentations are similar to those used in SimCLR, but there might be\nslight differences in implementation details. Here are the main augmentations used in MoCo v2:\n• Random Cropping: This involves taking a random crop of the image and then resizing it back to the original size.\nRandom Flipping: Randomly flip the image horizontally.\n• Color Jitter: Randomly change the brightness, contrast, saturation, and hue of the image.\n• Gaussian Blur: Apply Gaussian blur to the image with a certain probability.\n• Solarization: This is an augmentation introduced in MoCo v2. It inverts pixel values above a threshold, which can\ncreate a unique visual effect.\nMAE augmentation.\nThe core idea behind MAE is to mask out parts of an image and then train an autoencoder to\nreconstruct the original image from the masked version. This is somewhat analogous to the masked language modeling task\nused in models like BERT for NLP, where parts of the text are masked out and the model is trained to predict the masked\nwords.\nE. Appendix: Details of Biology Experiments\nE.1. Dataset Setups\nExperiments are performed on biological datasets, including MC13747 (Han et al., 2018), GA14578 (Rouillard et al., 2016),\nSAM9 (Weber & Robinson, 2016), , and HCL50010 (Han et al., 2020) datasets.\nTo ensure a fair comparison, we first embed the data into a 2D space using the method under evaluation. We then assess the\nmethod’s performance through 10-fold cross-validation. Classification accuracy is determined by applying a linear SVM\nclassifier in the latent space, while clustering accuracy is gauged using k-means clustering in the same space. Further details\nabout the datasets, baseline methods, and evaluation metrics can be found in Table 10.\nTable 10. Datasets information of simple manifold embedding task\nDataset\nTrain Samples\nTest Samples\nInput Dimension\nClass Number in label\nMC1374\n24,000\n6,000\n1,374\n98\nGA1457\n8,510\n2,127\n1,457\n49\nSAM561\n69,491\n17,373\n561\n52\nHCL500\n48,000\n12,000\n500\n45\nE.2. Baseline Methods and Implementation Details\nDimension reduction methods that have been widely used on biological analyze are compared, including kPCA (Halko et al.,\n2010), Ivis (Szubert et al., 2019), PHATE (Moon & van Dijk, 2019), PUMAP (Sainburg et al., 2021), PaCMAP (Wang et al.,\n2022), DMTEV (Zang et al., 2022a) and hNNE (Sarfraz et al., 2022).\nFor DiffAug, both the semantic encoder Enc(·), and the diffusion generator Gen(·), are implemented using a Multi-Layer\nPerceptron (MLP). Their respective architectures are defined as: Enc(·): [-1, 500, 300, 80]. The Gen(·): is defined below,\n7https://bis.zju.edu.cn/MCA/\n8https://maayanlab.cloud/Harmonizome/gene/GAST\n9https://github.com/abbioinfo/CyAnno\n10https://db.cngb.org/HCL/\n21\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\nListing 2. DiffusionModel for Biology Task\n1\nclass AE(nn.Module):\n2\ndef __init__( self,in_dim, mid_dim=2000, time_step=1000,):\n3\nsuper().__init__()\n4\nself.enc1 = self.diff_block(in_dim, mid_dim)\n5\nself.enc2 = self.diff_block(in_dim, mid_dim)\n6\nself.enc3 = self.diff_block(in_dim, mid_dim)\n7\nself.enc4 = self.diff_block(in_dim, mid_dim)\n8\n9\nself.dec1 = self.diff_block(in_dim, mid_dim)\n10\nself.dec2 = self.diff_block(in_dim, mid_dim)\n11\nself.dec3 = self.diff_block(in_dim, mid_dim)\n12\nself.dec4 = self.diff_block(in_dim, mid_dim)\n13\nself.time_encode = nn.Embedding(time_step, in_dim)\n14\n15\ndef diff_block(in_dim, mid_dim):\n16\nreturn nn.Sequential(\n17\nnn.LeakyReLU(), nn.InstanceNorm1d(in_dim),\n18\nnn.Linear(in_dim, mid_dim), nn.LeakyReLU(),\n19\nnn.InstanceNorm1d(mid_dim), nn.Linear(mid_dim, in_dim),)\n20\n21\ndef forward(self, input, time, cond=None):\n22\ninput_shape = input.shape\n23\nif len(input.size()) > 2:\n24\ninput = input.view(input.size(0), -1)\n25\nti = self.time_encode(time)\n26\ncd = self.cond_model(cond).reshape(input.shape[0], -1)\n27\nee1 = self.enc1(input + ti + cd)\n28\nee2 = self.enc2(ee1 + ti+ cd) + ee1\n29\nee3 = self.enc3(ee2 + ti+ cd) + ee1 + ee2\n30\nee4 = self.enc4(ee3 + ti+ cd) + ee1 + ee2 + ee3\n31\n32\ned1 = self.dec1(ee4 + ti+ cd)\n33\ned2 = self.dec2(ed1 + ti+ cd) + ee3 + ed1\n34\ned3 = self.dec3(ed2 + ti+ cd) + ee2 + ed1 + ed2\n35\ned4 = self.dec4(ed3 + ti+ cd) + ee1 + ed1 + ed2 + ed3\n36\nreturn ed4.reshape(input_shape)\nTo assess the efficacy of the proposed methods, following (Wang et al., 2022; Sarfraz et al., 2022), we utilized linear\nSVM performance to evaluate the performance of differences methods. For the linear SVM evaluation, embeddings were\npartitioned with 90% designated for training and 10% for testing; the training set facilitated the linear SVM training, while\nthe test set yielded the performance metrics. Detailed specifics of this configuration are elaborated in the Table 11.\nTable 11. Details of the training process in biological dataset.\nCF10\nν\nLearning Rate\nWeight Decay\nBatch Size\nGPU\nTraining Time\nMC1374\n1\n0.0001\n1e-6\n300\n1*V100\n4.2 hours\nGA1457\n1\n0.0001\n1e-6\n300\n1*V100\n4.6 hours\nSAM561\n1\n0.0001\n1e-6\n300\n1*V100\n12.1 hours\nHCL500\n0.1\n0.0001\n1e-6\n300\n1*V100\n20.1 hours\n22\nDomain-Knowledge-Free Diffusion-based Data Augmentation Can Enhance Unsupervised Contrastive Learning\n0\n200\n400\n600\n800\n1000\nEpochs\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\nLoss SCL\nLoss SCL vs. Epochs\nLoss SCL\nEstep\nMstep\nEstep\n0\n200\n400\n600\n800\n1000\nEpochs\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nLoss DIFF\nLoss DIFF vs. Epochs\nLoss DIFF\nEstep\nMstep\nEstep\n0\n200\n400\n600\n800\n1000\nEpochs\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nAccuracy vs. Epochs\nAccuracy\nEstep\nMstep\nEstep\nFigure 7. Training curves on the GA1457 dataset, including two Esteps and one Mstep. We can observe that the new generated data\nimproves the correctness of E step.\n23\n",
  "categories": [
    "cs.LG",
    "cs.CE",
    "cs.CV"
  ],
  "published": "2023-09-10",
  "updated": "2024-05-25"
}