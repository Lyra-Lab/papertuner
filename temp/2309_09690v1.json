{
  "id": "http://arxiv.org/abs/2309.09690v1",
  "title": "Do learned speech symbols follow Zipf's law?",
  "authors": [
    "Shinnosuke Takamichi",
    "Hiroki Maeda",
    "Joonyong Park",
    "Daisuke Saito",
    "Hiroshi Saruwatari"
  ],
  "abstract": "In this study, we investigate whether speech symbols, learned through deep\nlearning, follow Zipf's law, akin to natural language symbols. Zipf's law is an\nempirical law that delineates the frequency distribution of words, forming\nfundamentals for statistical analysis in natural language processing. Natural\nlanguage symbols, which are invented by humans to symbolize speech content, are\nrecognized to comply with this law. On the other hand, recent breakthroughs in\nspoken language processing have given rise to the development of learned speech\nsymbols; these are data-driven symbolizations of speech content. Our objective\nis to ascertain whether these data-driven speech symbols follow Zipf's law, as\nthe same as natural language symbols. Through our investigation, we aim to\nforge new ways for the statistical analysis of spoken language processing.",
  "text": "DO LEARNED SPEECH SYMBOLS FOLLOW ZIPF’S LAW?\nShinnosuke Takamichi, Hiroki Maeda, Joonyong Park, Daisuke Saito, and Hiroshi Saruwatari\nThe University of Tokyo, Japan.\nABSTRACT\nIn this study, we investigate whether speech symbols, learned\nthrough deep learning, follow Zipf’s law, akin to natural language\nsymbols. Zipf’s law is an empirical law that delineates the frequency\ndistribution of words, forming fundamentals for statistical analysis\nin natural language processing. Natural language symbols, which\nare invented by humans to symbolize speech content, are recognized\nto comply with this law. On the other hand, recent breakthroughs\nin spoken language processing have given rise to the development\nof learned speech symbols; these are data-driven symbolizations of\nspeech content. Our objective is to ascertain whether these data-\ndriven speech symbols follow Zipf’s law, as the same as natural\nlanguage symbols. Through our investigation, we aim to forge new\nways for the statistical analysis of spoken language processing.\nIndex Terms— speech analysis, Zipf’s law, generative spoken\nlanguage model, speech representation\n1. INTRODUCTION\nZipf’s law, a well-known empirical principle, delineates the fre-\nquency of occurrence of elements within a dataset [1]. Specifically,\nwhen the occurrence frequency of an element ranks as the k-th high-\nest within a dataset, it equates to 1/k of the frequency of the most\noccurring element.\nThis law is observed to be applicable across\nvarious data domains, with natural language symbols (e.g., words),\nfollow this pattern [2]. To illustrate, the third most frequent word\nin an English document, “and”, appears approximately one-third as\noften as the most frequent word, “the”1. In the practice of analyzing\nthe occurrence frequency of natural language symbols within a text\ncorpus, one can denote the frequency rank as r and its respective oc-\ncurrence frequency as fr. Consequently, the following relationship\nis given by Zipf’s law:\nfr = ar−η,\n(1)\nwhere, a and η are model parameters. The text corpus follows Zipf’s\nlaw when η ≈1, and follows a power law otherwise. In essence,\nZipf’s law is a specific type of power law. When following a power\nlaw, the log-log plot of rank against frequency appears linear.\nBy examining the adherence to or deviation from Zipf’s law,\none can analyze the distinct characteristics of a text corpus. This\nanalysis finds notable applications in natural language processing,\nas illustrated by the following examples [3]:\nInfants’ language acquisition: The vocabulary that 2- to 4-year-\nolds acquire tends to gravitate towards high-frequency words, a phe-\nnomenon indicated by a rank-frequency distribution that is convex,\nrather than linear [4].\nThis work is supported by JSPS KAKENHI 21H04900 (practical exper-\niment) and Moonshot R&D Grant Number JPMJPS2011 (algorithm develop-\nment).\n1https://www.cs.cmu.edu/˜cburch/words/top.html\nWriting system variations across languages: Writing systems\nsignificantly differ from one language to another, ranging from\nsound-based phonograms to semantic-based logograms.\nTanaka\nobserved that as notation shifts from phonogram to logogram,\nthe rank-frequency distribution transitions to follow a power law;\nphonographic languages display a convex, while logographic lan-\nguages demonstrate a linear on a plot [3].\nQuantification of communication effort: Zipf’s law is also rec-\nognized as the principle of least effort [5]. This principle stipulates\nthat the utilization of frequently used words minimizes speaking and\nlistening efforts during human communication. Viewing communi-\ncation through this perspective allows for the potential measurement\nof the naturalness exhibited in communication, including emergent\ncommunication [6] and machine-generated communication [7].\nMeanwhile, recent advancements in deep learning, such as\nself-supervised learning, have facilitated the discovery of discrete\nsymbol representations [8]–[10]. These representations are learned\nin a data-driven manner from speech.\nIn contrast to traditional\nsignal-processing-based representations, such as mel-spectrograms,\nthe symbol sequences learned through these methods captures a\nwealth of phonetic and semantic contents. While natural language\nsymbols were crafted by humans to encode the contents of speech,\nthe learned speech symbols can be seen as their data-driven counter-\nparts, that means, symbols crafted in a data-driven manner.\nWith this context in mind, we are led to a research question:\n“Does Zipf’s law, which holds true for natural language symbols,\nalso apply to the learned speech symbols?”\nVerifying this hypothesis could potentially unveil the capability to\nextend the statistical analysis techniques, traditionally employed in\nnatural language analysis, to the spoken language analysis. More-\nover, since this methodology bypasses the necessity for transcrip-\ntions, it might forge a path towards textless analysis applicable to a\ndiverse range of sounds, encompassing non-verbal vocalizations and\nnon-speech sounds (further details are elaborated in Section 5).\nIn this study, we conduct experiments to address the posed ques-\ntion, utilizing the generative spoken language model (GSLM) [8], a\nvariant of speech symbol representation methodology. Initially, we\nperform a foundational experiment to ascertain whether the learned\nspeech symbols follow Zipf’s law. Leveraging a speech corpus com-\nprising paired text and speech, we analyze symbol rank-frequency\ndistributions by correlating them with the accompanying text. Sub-\nsequently, we explore whether our textless analysis identifies devia-\ntion in non-textual contents of speech. Specifically, we aim to cap-\nture variations in language fluency between native and non-native\nspeech utterances. Our efforts endeavor to pave a way for compre-\nhensive speech analysis.\narXiv:2309.09690v1  [cs.CL]  18 Sep 2023\nSpeech2unit\n(encoder)\nUnit2speech\n(decoder)\nInput\nwaveform\nResynthesized\nwaveform\nDiscrete symbols\nFig. 1. Generative spoken language model.\n2. GENERATIVE SPOKEN LANGUAGE MODEL (GSLM)\nFigure 1 illustrates the GSLM [8], an analysis-synthesis system\noperating through discrete speech symbols. This system is struc-\ntured into three modules: speech2unit, unit language model, and\nunit2speech. However, this study focuses solely on the utilization of\nthe speech2unit. This specific module integrates a pre-trained self-\nsupervised learning (SSL) model, featuring technologies such as\ncontrastive predictive coding [11], wav2vec2.0 [12], HuBERT [13],\nin conjunction with a k-means clustering model. The SSL model\nremains fixed, while the k-means clustering model is trained using a\nspeech corpus. Within this setup, the speech2unit module transforms\na speech waveform into a sequence of discrete symbols.\nSymbols are generated at intervals of 20 ms, this interval\ngenerally shorter than the phoneme duration.\nConsequently, the\nspeech2unit frequently predicts identical symbols. To prevent this\nredundancy, we have opted to consolidate consecutive identical\nsymbol sequences into a single representation. For instance, if the\nspeech2unit outputs a sequence like [3, 3, 3, 50, 200, 200] (where the\nnumbers indicate symbol indices), we simplify this to [3, 50, 200].\nIt’s important to note that the speech2unit, particularly the k-means\nclustering model, is highly sensitive to language of input speech.\nHence, it’s imperative that the model and the encoded speech align\nin terms of language.\n3. METHODOLOGY\nWe delineate two methodologies employing GSLM speech symbols\nto explore Zipf’s law.\nExamining the applicability of Zipf’s law to speech symbols.\nThis methodology is conducted utilizing text-speech pairs. In the\ncase of text, we employ natural language symbols, represented ei-\nther as words or as character n-grams. Words are extracted through\nmorphological analysis to discern the original form of the words,\nwhile character n-grams involve sequences of n consecutive charac-\nters. For speech, we obtain speech symbols as outlined in Section 2,\ncalculate speech symbol n-grams, that is, sequences of n consecu-\ntive speech symbols. We then calculate the ratio of the lengths of\nnatural language symbol sequences to speech symbol sequences to\ndetermine the value of n. To describe this intuitively, n signifies the\naverage number of speech symbols corresponding to a single natural\nlanguage symbol. We then assess whether Zipf’s law holds true for\nspeech symbol n-grams within text adhering to the law.\nIdentifying non-textual deviations from Zipf’s law. As ex-\nplained in Section 1, identifying deviations from this law can be a\nmethod to pinpoint non-standard word usage. In this methodology,\nwe explore to detect non-standard speech patterns based on rank-\nfrequency distribution. By comparing distributions of standard and\nnon-standard speech, we aim to identify deviations in non-standard\nspeech from the standard speech.\n4. EXPERIMENTAL EVALUATION\n4.1. Experimental condition\nWe utilized HuBERT [13], which was trained on LibriSpeech [14],\nas the SSL model in the GSLM speech2unit. The language-specific\nk-means clustering models were trained using JSUT/JVS [15], J-\nKAC [16], and J-MAC [17] for Japanese, and LibriSpeech for En-\nglish. The number of classes was set to 200 for both languages.\nThe models, implemented using the fairseq toolkit [18], are pub-\nlicly available23. The speech sampling frequency was set at 16 kHz,\nand speech symbols were extracted every 20 ms. Given that high-\nfrequency and low-frequency items often deviate from Zipf’s law[3],\nwe estimated model parameters a and η based only on the top 0.1 %\nto 10 % of the frequencies. These model parameters were deter-\nmined using the least square method. For comparative analysis, we\nfixed η at 1.0 while estimating only a, indicating a strict adherence\nto Zipf’s law in the rank-frequency distribution. To reduce the data\nsize of figures, we thinned the data to be plotted.\nTo verify Zipf’s law in Section 4.2, we utilized approximately\n7, 600 Japanese utterances from JSUT [15] and 13, 000 English ut-\nterances from LJSpeech [14]. For word tokenization, MeCab4 and\nNLTK5 served as morphological analyzers. In the character n-gram\nanalysis, we used a set of characters encompassing Chinese/Japanese\ncharacters and marks for Japanese, and lowercase alphabets, sym-\nbols (e.g., ”:”, ”?”), and whitespace for English. The average number\nof characters per word was 1.6 (ja) and 5.1 (en), while the average\nnumber of speech symbols per character stood at 5.7 (ja) and 1.9\n(en). Moreover, the average number of speech symbols per word\nwas 8.9 (ja) and 9.0 (en). The value of n in both character n-gram\nand speech symbol n-gram was set to the ceiling of these values, for\ninstance, ⌈1.6⌉= 2 for Japanese characters.\nFor identifying non-textual deviation in Section 4.3, we em-\nployed native and Japanese-accented English utterances from UME-\nERJ6. The corpus contained about 20 native and 200 non-native\nspeakers, each reading approximately 300–500 English sentences.\nThese sentences varied between speakers but maintained a balanced\nphoneme distribution.\nNon-native speakers were assigned a lan-\nguage fluency score on a five-point scale. Based on these scores, we\ncategorized non-native speakers into three groups: low- (score <\n3.0), mid- (3.0 ≤score < 3.5), and high-level (score ≥3.5).\nThese groups consisted of 74, 66, and 46 speakers, respectively. We\naggregated speech symbols separately for each non-native speaker\ngroup and the native speakers. To equalize data size across groups,\nwe randomly selected 10, 000 utterances per group. For symbol en-\ncoding, we used the English GSLM speech2unit.\n4.2. Verifying Zipf’s law of speech symbols\nWe verify the law via three steps: word, character n-gram, and\nspeech symbol n-gram.\n4.2.1. Word\nFirst, we verify that the words in the corpora we used adhere to\nZipf’s law. Figure 2 illustrates the rank-frequency distributions in\n2https://huggingface.co/nonmetal/gslm-japanese\n(Japanese)[19]\n3https://github.com/facebookresearch/fairseq/\ntree/main/examples/textless_nlp/gslm (English)\n4https://taku910.github.io/mecab/\n5https://www.nltk.org/\n6https://research.nii.ac.jp/src/en/UME-ERJ.html\n100\n101\n102\n103\n104\nRank\n100\n101\n102\n103\n104\nCount\n=1.000\n=0.951\n100\n101\n102\n103\n104\nRank\n100\n101\n102\n103\n104\n=1.000\n=0.944\nFig. 2. Word rank-frequency distributions. left: Japanese, right:\nEnglish.\n100\n101\n102\n103\n104\n105\nRank\n100\n101\n102\n103\n104\n105\nCount\n=1.000 (n=2)\n=0.855 (n=2)\nn=2\nn=4\nn=6\nn=2\nn=4\nn=6\n100\n101\n102\n103\n104\n105\nRank\n100\n101\n102\n103\n104\n105\n=1.000 (n=6)\n=0.797 (n=6)\nFig. 3.\nCharacter n-gram rank-frequency distributions.\nleft:\nJapanese, right: English. n = 2 in Japanese and n = 6 in En-\nglish correspond a word.\nboth Japanese and English. These distributions appear to be linear\nand the value of η is close to 1.0. Therefore, we can say that the\nwords follow Zipf’s law, and that the corpora used have the uni-\nversal statistics in terms of the word distribution. A minor observa-\ntion, as highlighted in previous studies [3], is that the high-frequency\n(rank < 20) and low-frequency (103 < rank) items deviate from\nthe regression line.\n4.2.2. Character n-gram\nNext, we explore another natural language symbol: the character n-\ngram. Figure 3 shows these distributions. When analyzed for the\nsame value of n, differences between languages become evident. In\nJapanese, the distribution attains linear at n = 2, whereas in English,\nit displays convex. The English distribution gradually transitions to\na linear shape as n increases, and at n = 6, it remains convex but\nclosely resembles a linear trend. At n = 2 for Japanese and n = 6\nfor English that correspond to a word, the distributions of character\nn-grams more closely follow a power law, rather than Zipf’s law.\n4.2.3. Speech symbol n-gram\nFinally, we explore whether speech symbols follow Zipf’s law by\ncomparing the results with those of words and character n-grams.\nFigure 4 shows these distributions.\nDifference between languages. When analyzed for the same\nvalue of n, we find that the distributions are almost identical. These\nresults suggest that the distributions of consecutive speech symbols,\nwhich indicate the frequency of speech segment use, are language-\nindependent, at least between Japanese and English. The distribu-\n100\n101\n102\n103\n104\n105\nRank\n100\n101\n102\n103\n104\n105\nCount\n=1.000 (n=9)\n=0.453 (n=9)\n100\n101\n102\n103\n104\n105\nRank\n100\n101\n102\n103\n104\n105\n=1.000 (n=9)\n=0.516 (n=9)\nn=2\nn=6\nn=9\nn=2\nn=6\nn=9\nFig. 4. Speech symbol n-gram rank-frequency distributions. left:\nJapanese, right: English. n = 6 in Japanese and n = 2 in English\ncorrespond to a character. n = 9 corresponds a word.\ntions for English are slightly shifted upwards compared to those for\nJapanese. This discrepancy is due to the difference in data size; the\nEnglish corpus is twice as large as the Japanese corpus.\nComparison in n corresponding to a character. n = 6 for\nJapanese and n = 2 for English corresponds to a character. We ob-\nserve that distributions where n corresponds to a character maintain\nshapes similar to those of character 1-gram. Specifically, the dis-\ntribution for n = 6 in Japanese is nearing linearity (but not η =\n1.0), while the distribution for n = 2 in English is convex (Al-\nthough not illustrated in Figure 3, the distributions of character 1-\ngrams resemble those of the 2-gram distributions: they are linear for\nJapanese and convex for English.). This difference can be traced\nback to differences in writing systems, as discussed in Section 1.\nNamely, Japanese and English are more akin to logographic and\nphonographic languages, respectively. The closer a symbol is to rep-\nresenting semantic information rather than phonetic one, the more\nlinear the distribution becomes. Our findings suggest that this trend\nholds true for speech symbols and that the speech symbol n-gram\nmight statistically reflect the sound or meaning of a character. The\nability to attain character statistics without relying on characters also\nfacilitates analysis based on Zipf’s law (and power law) without in-\nvolving characters.\nComparison in n corresponding to a word. n = 9 signifies for\nboth Japanese and English. As n escalates, the distributions gravi-\ntate towards linear (not η = 1.0). We theorize that this phenomenon\noccurs because, as noted previously, n-grams tend to represent se-\nmantic information as n increases. η not being 1.0, but the distribu-\ntion at 9-grams is linear, mirroring the word distribution observed in\nFigure 2. This suggests that our methodology holds promise for sta-\ntistically analyzing features associated with words without utilizing\nwords themselves.\n4.3. Identifying non-textual deviation\nFigure 5 displays the speech symbol n-gram (n = 1, 3, 5, 7) fre-\nquencies in both native and non-native speakers. We delve into the\ndistinctions between native and non-native speakers based on the\ndata presented in the figure.\nLinearization by increasing n. The distribution tends to lin-\nearize as n increases, a trend observed in both native and non-native\nspeakers. Since the corpus utilized in this experiment employs the\nsame reading text for both groups, language fluency does not influ-\nence the linguistic content. Consequently, the linearization is antici-\npated to be driven by the textual information (particularly the seman-\ntic information hypothesized in Section 4.2), rather than language\n100\n102\n104\n100\n101\n102\n103\n104\nn = 1\nlow\nmid\nhigh\nnative\n100\n102\n104\n100\n101\n102\n103\n104\nn = 3\nlow\nmid\nhigh\nnative\n100\n102\n104\n100\n101\n102\n103\n104\nn = 5\nlow\nmid\nhigh\nnative\n100\n102\n104\n100\n101\n102\n103\n104\nn = 7\nlow\nmid\nhigh\nnative\nRank\nCount\nFig. 5. Speech symbol n-gram frequency of native and non-native speech. “low,” “mid,” and “high” indicate English fluency level of non-\nnative speakers. For clear illustration, we choose line plots rather than scatter plots.\nfluency.\nDeviation of non-native speakers. When we examine the de-\nviations of non-native speakers compared to native speakers, we no-\ntice deviations correlated with language fluency. Specifically, non-\nnative speakers tend to use high-frequency symbols more frequently.\nThese findings indicate that Zipf’s law (and the power law) can be\nemployed to discern differences between standard and non-standard\nspeech with regards to language fluency. Further exploration is re-\nquired to ascertain the specific nature of these deviations. Intrigu-\ningly, speakers with high-level proficiency diverge more from na-\ntive speakers. This counters the intuitive expectation that individuals\nwith low-level fluency would deviate more significantly.\n5. CONCLUSION\nIn this paper, we investigated whether speech symbols follow Zipf’s\nlaw. Through our experiments, we determined that: 1) speech sym-\nbol n-grams corresponding to a word follow a power law rather than\nZipf’s law, and 2) non-textual deviations in non-standard speech can\nbe identified through the power law of the rank-frequency distribu-\ntions.\nOur research paves the way for textless analysis methods appli-\ncable to various audio data. The following are possible directions for\nfuture research.\n• Language development. As outlined in Section 1, the Zipf’s (or\npower) law can potentially aid in analyzing infants’ vocabulary\nacquisition. Despite the existing challenges in robust automatic\nspeech recognition of infants’ voices [20], our approach can en-\ncode voices into symbols without transcriptions. This enables\nstatistical analyses utilizing voices exclusively.\n• Animal voices and non-speech audio.\nZipf’s law has been\ndemonstrated to apply to symbolic audio, including animal\ncalls [21] and music scores [22]. While current research often\nrelies on human-invented symbols and annotations, our method\ncould potentially extend to audio sources beyond human speech\nwithout using the human-invented symbols and annotations.\nGeneral-purpose audio representation models [23] could be em-\nployed for this purpose.\n• Emergent speech communication. The analysis of emergent\nlanguages constitutes a significant area of research, with objec-\ntives including 1) fostering language-based machine-machine\ncommunication (i.e., communication between artificial intelli-\ngence) [7], [24], and 2) evaluating the extent to which these\ncommunications mirror statistics of human-to-human communi-\ncations [6], [25]. Given that Zipf’s law (and related principles,\nsuch as the Zipf’s law of abbreviation [26]) embodies the princi-\nple of least effort in human communication, it may offer a way\nto explore machine-machine communication [27]. Our method\nholds the potential to forge a way in examining speech-based\nmachine-machine communication, potentially benefitting devel-\nopments of exploring human spoken language emergence and\nhuman-machine communication.\nReferences\n[1]\nG. K. Zipf, Human Behaviour and the Principle of Least Ef-\nfort. 1949.\n[2]\nS. T. Piantadosi, “Zipf’s word frequency law in natural lan-\nguage: A critical review and future directions,” Psychonomic\nBulletin & Review, vol. 21, no. 5, pp. 1112–1130, 2009.\n[3]\nK. Tanaka-Ishii, Statistical Universals of Language: Mathe-\nmatical Chance vs. Human Choice [Japanese Edition]. 2021.\n[4]\nL. Elena, S. Doroth´e, and T. Michael, “Two-year-old chil-\ndren’s production of multiword utterances: A usage-based\nanalysis,” Cognitive Linguistics, vol. 20, no. 3, 2009.\n[5]\nG. M. Linders and M. M. Louwerse, “Zipf’s law revisited:\nSpoken dialog, linguistic units, parameters, and the principle\nof least effort,” Psychon Bull Rev, 2023.\n[6]\nS. Kottur, J. Moura, S. Lee, et al., “Natural language does not\nemerge ‘naturally’ in multi-agent dialog,” in Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Lan-\nguage Processing, 2017, pp. 2962–2967.\n[7]\nS. Havrylov and I. Titov, “Emergence of language with\nmulti-agent games: Learning to communicate with se-\nquences of symbols,” in Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing Sys-\ntems, ser. NIPS’17, 2017, pp. 2146–2156.\n[8]\nK. Lakhotia, E. Kharitonov, W.-N. Hsu, et al., “On generative\nspoken language modeling from raw audio,” Transactions of\nthe ACL, vol. 9, pp. 1336–1354, 2021.\n[9]\nJ. Park, S. Takamichi, T. Nakamura, et al., “How generative\nspoken language modeling encodes noisy speech: Investiga-\ntion from phonetics to syntactics,” in INTERSPEECH, 2023.\n[10]\nZ. Borsos, R. Marinier, D. Vincent, et al., AudioLM: A\nlanguage modeling approach to audio generation, arXiv\n2209.03143, 2022.\n[11]\nA. van den Oord, Y. Li, and O. Vinyals, Representation\nlearning with contrastive predictive coding, arXiv preprint,\narXiv:1807.03748, 2018.\n[12]\nA. Baevski, Y. Zhou, A. Mohamed, et al., “wav2vec 2.0: A\nframework for self-supervised learning of speech representa-\ntions,” in Proc. NIPS, vol. 33, 2020, pp. 12 449–12 460.\n[13]\nW.-N. Hsu, B. Bolte, Y.-H. H. Tsai, et al., “HuBERT: Self-\nsupervised speech representation learning by masked pre-\ndiction of hidden units,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 29, pp. 3451–3460,\n2021.\n[14]\nV. Panayotov, G. Chen, D. Povey, et al., “Librispeech: An\nasr corpus based on public domain audio books,” in Proc.\nICASSP, 2015, pp. 5206–5210.\n[15]\nS. Takamichi, R. Sonobe, K. Mitsui, et al., “JSUT and JVS:\nFree Japanese voice corpora for accelerating speech synthe-\nsis research,” Acoustical Science and Technology, vol. 41,\nno. 5, pp. 761–768, 2020.\n[16]\nW. Nakata, T. Koriyama, S. Takamichi, et al., “Audiobook\nSpeech Synthesis Conditioned by Cross-Sentence Context-\nAware Word Embeddings,” in Proc. SSW, 2021, pp. 211–215.\n[17]\nS. Takamichi, W. Nakata, N. Tanji, et al., “J-MAC: Japanese\nmulti-speaker audiobook corpus for speech synthesis,” in\nProc. Interspeech, 2022, pp. 2358–2362.\n[18]\nM. Ott, S. Edunov, A. Baevski, et al., “fairseq: A fast, exten-\nsible toolkit for sequence modeling,” in Proceedings of the\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics, 2019, pp. 48–53.\n[19]\nJ. Park, S. Takamichi, T. Nakamura, et al., “Analysis of de-\ngraded noisy voice and application to other languages us-\ning generative spoken language model,” in Annual meeting\nof acoustic society of Japan, in Japanese, 2023.\n[20]\nC. Gebauer, L. Rumberg, H. Ehlert, et al., “Exploiting Diver-\nsity of Automatic Transcripts from Distinct Speech Recog-\nnition Techniques for Children’s Speech,” in Proc. INTER-\nSPEECH, 2023, pp. 4578–4582.\n[21]\nA. Kershenbaum, V. Demartsev, D. E. Gammon, et al.,\n“Shannon entropy as a robust estimator of zipf’s law in ani-\nmal vocal communication repertoires,” Methods in Ecology\nand Evolution, vol. 12, no. 3, pp. 553–564, 2021.\n[22]\nJ. I. Perotti and O. V. Billoni, “On the emergence of zipf ’s\nlaw in music,” Physica A: Statistical Mechanics and its Ap-\nplications, vol. 549, p. 124 309, 2020.\n[23]\nD. Niizumi, D. Takeuchi, Y. Ohishi, et al., “BYOL for\nAudio: Self-supervised learning for general-purpose audio\nrepresentation,” in International Joint Conference on Neural\nNetworks (IJCNN), 2021.\n[24]\nA. Lazaridou, A. Peysakhovich, and M. Baroni, “Multi-\nagent cooperation and the emergence of (natural) language,”\nin ICLR, 2017.\n[25]\nL. Harding Graesser, K. Cho, and D. Kiela, “Emergent lin-\nguistic phenomena in multi-agent communication games,” in\nProceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-\nIJCNLP), 2019, pp. 3700–3710.\n[26]\nG. K. Zipf, The psycho-biology of language: An introduction\nto dynamic philology. 2013.\n[27]\nR. Ueda and K. Washio, “On the relationship between Zipf’s\nlaw of abbreviation and interfering noise in emergent lan-\nguages,” in Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning: Student Research Workshop, 2021, pp. 60–70.\n",
  "categories": [
    "cs.CL",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2023-09-18",
  "updated": "2023-09-18"
}