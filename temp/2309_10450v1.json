{
  "id": "http://arxiv.org/abs/2309.10450v1",
  "title": "Unsupervised speech enhancement with diffusion-based generative models",
  "authors": [
    "Berné Nortier",
    "Mostafa Sadeghi",
    "Romain Serizel"
  ],
  "abstract": "Recently, conditional score-based diffusion models have gained significant\nattention in the field of supervised speech enhancement, yielding\nstate-of-the-art performance. However, these methods may face challenges when\ngeneralising to unseen conditions. To address this issue, we introduce an\nalternative approach that operates in an unsupervised manner, leveraging the\ngenerative power of diffusion models. Specifically, in a training phase, a\nclean speech prior distribution is learnt in the short-time Fourier transform\n(STFT) domain using score-based diffusion models, allowing it to\nunconditionally generate clean speech from Gaussian noise. Then, we develop a\nposterior sampling methodology for speech enhancement by combining the learnt\nclean speech prior with a noise model for speech signal inference. The noise\nparameters are simultaneously learnt along with clean speech estimation through\nan iterative expectationmaximisation (EM) approach. To the best of our\nknowledge, this is the first work exploring diffusion-based generative models\nfor unsupervised speech enhancement, demonstrating promising results compared\nto a recent variational auto-encoder (VAE)-based unsupervised approach and a\nstate-of-the-art diffusion-based supervised method. It thus opens a new\ndirection for future research in unsupervised speech enhancement.",
  "text": "arXiv:2309.10450v1  [cs.CV]  19 Sep 2023\nUNSUPERVISED SPEECH ENHANCEMENT WITH DIFFUSION-BASED GENERATIVE MODELS\nBern´e Nortier, Mostafa Sadeghi, Romain Serizel\nUniversit´e de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France\nABSTRACT\nRecently, conditional score-based diffusion models have gained sig-\nniﬁcant attention in the ﬁeld of supervised speech enhancement,\nyielding state-of-the-art performance. However, these methods may\nface challenges when generalising to unseen conditions. To address\nthis issue, we introduce an alternative approach that operates in an\nunsupervised manner, leveraging the generative power of diffusion\nmodels. Speciﬁcally, in a training phase, a clean speech prior distri-\nbution is learnt in the short-time Fourier transform (STFT) domain\nusing score-based diffusion models, allowing it to unconditionally\ngenerate clean speech from Gaussian noise. Then, we develop a\nposterior sampling methodology for speech enhancement by com-\nbining the learnt clean speech prior with a noise model for speech\nsignal inference. The noise parameters are simultaneously learnt\nalong with clean speech estimation through an iterative expectation-\nmaximisation (EM) approach. To the best of our knowledge, this\nis the ﬁrst work exploring diffusion-based generative models for\nunsupervised speech enhancement, demonstrating promising re-\nsults compared to a recent variational auto-encoder (VAE)-based\nunsupervised approach and a state-of-the-art diffusion-based super-\nvised method. It thus opens a new direction for future research in\nunsupervised speech enhancement.\nIndex Terms— Unsupervised speech enhancement, diffusion-\nbased models, expectation-maximisation, posterior sampling.\n1. INTRODUCTION\nOver the past decade, the speech enhancement (SE) task has been\nextensively investigated, and numerous novel approaches have been\nproposed that greatly leverage the advancements and efﬁcacy of deep\nneural network (DNN) architectures [1]. The majority of these ap-\nproaches are based on supervised (discriminative) learning of a DNN\nover training pairs of clean and noisy speech signals, covering dif-\nferent speakers, noise types, and signal-to-noise ratio (SNR) values.\nSuch an approach depends heavily on the number and diversity of\ntraining samples and noise conditions, and thus generalisation to un-\nseen (out-of-domain) environments cannot be guaranteed.\nUnsupervised SE based on deep generative models presents an\nalternative approach with improved generalisation performance [2–\n4]. In contrast to purely supervised methods, the generative-based\n(unsupervised) framework learns the statistical distribution of clean\nspeech signals and uses it as a prior distribution for inferring the\ntarget signal from its noisy observation. In these methods, VAE [5]\nhas been commonly used as a generative clean speech prior, which\nThis work was supported by the French National Research Agency (ANR)\nunder the project REAVISE (ANR-22-CE23-0026-01). Experiments presented in\nthis paper were carried out using the Grid’5000 testbed, supported by a scien-\ntiﬁc interest group hosted by Inria, and including CNRS, RENATER, and several\nuniversities as well as other organizations (see https://www.grid5000.fr).\nis combined with a non-negative matrix factorization (NMF)-based\nobservation model to estimate clean speech following a statistical\nEM framework.\nRecently, diffusion-based generative models have emerged as a\npowerful and state-of-the-art framework to model complex data dis-\ntributions [6,7]. These models learn an implicit distribution by esti-\nmating the score, i.e., the gradient of the log probability density (with\nrespect to data). This is done by gradually diffusing data samples\ninto noise and then learning a score approximating model that can\nreverse the noising process for different noise scales. The forward\nprocess of corrupting data is modelled as a stochastic differential\nequation (SDE), which can be reversed and yields a corresponding\nreverse SDE that depends only on the score of the perturbed data\nand may easily be solved numerically. Diffusion-based models have\nbeen widely applied to the SE task in a supervised way [8–12] by\nincorporating noisy speech signals in the diffusion process as condi-\ntioning information.\nIn this paper, we develop an unsupervised speech enhance-\nment framework leveraging diffusion-based generative models as\ndata-driven priors.\nSpeciﬁcally, in a training step, the statistical\ncharacteristics of clean speech signals are learnt in the complex\nSTFT domain through the use of a score-based diffusion model. At\ntest time, we perform posterior sampling by combining the learnt\nimplicit clean speech prior with a parametric statistical model for\nnoise to infer the clean speech signal. The noise parameters are\nestimated alongside the clean speech signal by following an iterative\nEM-based approach. To our knowledge, this is the ﬁrst work that\nproposes using diffusion-based generative models for unsupervised\nSE, and explores their potential.\nWe conduct experiments com-\nparing the proposed framework with a VAE-based unsupervised\napproach [3] as well as a state-of-the-art diffusion-based super-\nvised method [11]. The results demonstrate the effectiveness and\npromising performance of the proposed diffusion-based unsuper-\nvised approach, paving the path for future research in this direction.\nThe rest of the paper is organised as follows: Section 2 reviews\nscore-based diffusion modelling and VAE-based SE as two closely\nrelated problems to our work. The proposed speech generative mod-\nelling and enhancement frameworks are detailed in Section 3. Ex-\nperimental results are then presented in Section 4, followed by a\nconclusion and suggestions for future lines of work in Section 5.\n2. BACKGROUND\n2.1. Score-based diffusion models\nDiffusion models are a state-of-the-art class of probabilistic genera-\ntive models that have recently achieved remarkable performance in\ngenerating high-quality samples in different applications [7]. These\nmodels transform an unknown data distribution p0 to a tractable prior\ndistribution, usually N(0, I), by gradually adding noise to training\ndata in a forward process. Then, in a reverse process, a parame-\nterised model is learnt to iteratively generate samples starting from\nnoise and transform these into samples from the unknown data distri-\nbution. This action of smoothly injecting noise into training samples\nmay be described by a SDE. Speciﬁcally, consider a diffusion pro-\ncess {st}t∈[0,1], indexed by a continuous time-step variable t, which\nsolves the following general linear SDE\ndst = f(st)dt + g(t)dw,\n(1)\nwhere w denotes a standard Wiener process, the vector-valued f is\nthe drift coefﬁcient term, and the scalar function g is the diffusion\ncoefﬁcient. Here, the forward process transforms a clean training\nsample s0 = s to a noise sample s1, whose distribution converges\nto p1 ∼N(0, I). Under some light regularity conditions [13], the\nSDE in (1) also has an associated reverse-time SDE:\ndst = [f(st)dt −g(t)2∇st log pt(st)]dt + g(t)d¯w,\n(2)\nwhere ¯w is a standard Wiener process running backwards in time,\ndt is an inﬁnitesimal negative time-step, and ∇st log pt(st) is called\nthe score function. In practice, the score is approximated by a time-\ndependent neural network (NN) Sθ∗(st, t) ≈∇st log pt(st), called\nthe score model, where θ∗denotes the learnt weights of the NN.\nBy plugging the score model in (2), we can solve the resulting SDE\nusing a variety of solvers to sample from the unknown data distribu-\ntion [7]. In this paper, we make use of the Predictor-Corrector (PC)\nsampler [7].\n2.2. VAE-based unsupervised speech enhancement\nPrevious work on unsupervised SE use VAE to learn the prior distri-\nbution of clean speech signals, which is then combined with an ob-\nservation model to estimate clean speech in a statistical framework.\nSpeciﬁcally, in the STFT domain, a latent variable-based generative\nmodel is assumed as pθ(s, z) = pθ(s|z)pθ(z), where s denotes the\nSTFT representation of clean speech and z represents the associated\n(latent) low-dimensional embedding. Some parameterised Gaussian\nforms for the generative distributions are usually assumed, whose\nparameters are learnt over clean speech data, following the evidence\nlower-bound optimisation principle [5].\nFor SE, it is assumed that x = s + n, where x, s, and n denote\nSTFT representations of noisy (mixture) speech, clean speech, and\nbackground noise, respectively. The likelihood pφ(x|s) is usually a\nproper complex Gaussian distribution NC with mean s, whose vari-\nance is parameterised with a low-rank NMF factorisation. SE then\namounts to inferring the latent variable z associated with s from x,\nwhich necessitates learning the NMF parameters, denoted φ, via an\nEM process formulated below\nmax\nφ\nEpφ(z|x) {log pφ(x|z)} .\n(3)\nThis could be solved using, e.g., the variational EM procedure de-\nveloped in [3,14], which approximates pφ(z|x).\n3. PROPOSED FRAMEWORK\n3.1. Diffusion-based speech generative modeling\nFollowing [11], we work with the complex-valued STFT representa-\ntions of speech signals and apply an exponential amplitude transfor-\nmation to balance the heavy-tailed distribution of STFT amplitudes.\nLike VAE, the diffusion-based generative model is indepen-\ndently deﬁned for each time-frequency (TF) bin.\nTherefore, as\ndone in [11], all the vector-valued variables st in boldface contain\nﬂattened TF representations of speech signals. For concrete instan-\ntiations of the forward and reverse SDE ( (1) and (2) respectively),\nwe use an alternative form of the well-known variance-preserving\nstochastic differential equation (VESDE) [15] inspired by [11], and\nadapt it to obtain the drift and diffusion coefﬁcients as follows\nf(st) = −γst,\ng(t) = σmax\n\u0010σmax\nσmin\n\u0011tr\n2 log\n\u0010σmax\nσmin\n\u0011\n,\n(4)\nwhere γ is a constant parameter, and σmin and σmax are parameters\ndeﬁning the noise schedule of the Wiener process. The SDE in (1)\nthen has the perturbation kernel deﬁned below, which allows one to\nsample st directly given s0\np0t(st|s0) = NC(δts0, σ(t)2I),\n(5)\nwhere δt = e−γt and the variance term σ(t)2 is given by\nσ(t)2 =\nσ2\nmin\n\u0010\n(σmax/σmin)2t −δt\n2\u0011\nlog(σmax/σmin)\nγ + log(σmax/σmin)\n.\n(6)\nTo learn the NN parameters θ, a weighted Fisher divergence [15] be-\ntween the true and approximated score is solved, which, after some\nmathematical manipulation, leads to the following training objec-\ntive [11]\nθ∗= argmin\nθ\nEt,s,ζ,st|s\nh\n∥Sθ(st, t) +\nζ\nσ(t)∥2\n2\ni\n,\n(7)\nwhere ζ ∼NC(0, I), i.e., complex-valued Gaussian noise.\n3.2. Diffusion-based unsupervised speech enhancement\nWe now describe the unsupervised SE framework based on diffusion-\nbased generative models.\nThe prior clean speech distribution\np = p(s) is unknown, but can be obtained by training a diffusion-\nbased generative model as described in Section 3.1, yielding an\nimplicit prior, as opposed to the explicit VAE-based speech prior\nmodelling framework. This implicit diffusion-based speech prior\nonly allows for iterative sampling, without an explicit density form.\nAs such, the SE procedure adopted in VAE-based modelling cannot\ndirectly be used for diffusion-based learnt speech priors. Assuming\nthe same observation model as before, i.e., x = s + n, and NMF-\nbased likelihood parameterisation, we here propose to sample from\nthe following intractable posterior distribution to estimate the clean\nspeech s directly\npφ(s|x) ∝pφ(x|s)pθ∗(s),\n(8)\nwhere θ∗denotes the diffusion model’s pretrained, and thus ﬁxed,\nparameters. We model the noise by n ∼NC(0, diag(vec(WH)))\nwhere W, H are low-rank matrices with non-negative entries and\nrank r and vec(WH) denotes the vectorised form of WH. The\nlikelihood pφ(x|s) then writes as pφ(x|s) = NC(s, diag(vφ)),\nwhere vφ\n= vec(WH).\nLearning the NMF parameters, i.e.,\nφ = {W, H}, is done by solving\nmax\nφ\nEpφ(s|x) {log pφ(x|s)} .\n(9)\nAn overview of the proposed Unsupervised Diffusion-Based Speech\nEnhancement (UDiffSE) approach is provided in Algorithm 1. The\nfollowing sections detail the E-step and M-step.\nAlgorithm 1 UDiffSE\n1: φ0 = {W0, H0}\n2: for k = 1, . . . , K do\n3:\nˆs ∼pφk−1(s|x)\n⊲(E-Step)\n4:\nφk ←argmaxφ log pφ(x|ˆs)\n⊲(M-Step)\n5: end for\n6: return ˆs\n3.2.1. E-Step\nGiven a current estimate of φ, the E-step (posterior sampling) en-\ntails the generation of speech samples from the posterior distribution\npφ(s|x) to approximate the expectation in (9). This is done via the\nconstruction of a stochastic process {st|x}t∈[0,1] by conditioning\nthe original process {st}t∈[0,1] on the observation x to obtain an es-\ntimate ˆs ∼pφ(s|x). To this end, we modify the reverse SDE (2) as\nfollows\ndst =\nh\nf(st)dt −g(t)2∇st log pt(st|x)\ni\ndt + g(t)d¯w\n=\nh\nf(st)dt −g(t)2(∇st log pt(x|st)+∇st log pt(st))\ni\ndt\n+ g(t)d¯w\n(10)\nwhere again the score function can be approximated by Sθ∗(st, t).\nHowever, the conditional score function ∇st log pφ(x|st) is, in fact,\nintractable to compute in closed form due to its dependence on time.\nThat is,\npφ(x|st) =\nZ\npφ(x|s0)pt0(s0|st)ds0,\n(11)\nwhere pt0(s0|st) ∝p0t(st|s0)p(s0) is intractable. As an approx-\nimation, we follow [16] and assume an uninformative prior p(s0),\nwhich, along with (5), results in\n˜pt0(s0|st) = NC\n\u0010s0\nδt , σ(t)2\nδ2\nt\nI\n\u0011\n.\n(12)\nPlugging this approximation in (11) gives us the following noise-\nperturbed pseudo-likelihood\n˜pφ(x|st) ∼NC\n\u0010st\nδt , σ(t)2\nδ2\nt\nI + diag(vφ)\n\u0011\n.\n(13)\nThe conditional reverse process is then approximated as\ndst =\nh\nf(st)dt −g(t)2Sθ∗(st, t)\ni\ndt + g(t)d¯w\n−g(t)2∇st log ˜pφ(x|st)dt.\n(14)\nThis is exactly the unconditional reverse process (2) for sampling\nclean speech, plus an additional term which imposes data consis-\ntency. We use the change of variables formula and take the gradient\nto compute ∇stlog˜p(x|st), the noise-perturbed pseudo-likelihood\nscore as\n∇st log ˜p(x|st) = 1\nδ t\nhσ(t)2\nδ2\nt\nI + diag(vφ)\ni−1\n(st\nδt −x).\n(15)\nLastly, we introduce an additional weighting parameter λ to the\npseudo-likelihood as in [16] to balance the effect of the mixture\nsignal on the estimated sample. We experimentally observed that\nperforming the full posterior reverse step at each iteration enforces\nAlgorithm 2 Posterior sampling (E-step) of UDiffSE\nRequire: N, x, ℓ, λ\n1: s1 ∼NC(x, I), ∆τ ←\n1\nN\n2: for i = N, . . . , 1 do\n3:\nτ ←\ni\nN\n4:\nζc ∼NC(0, I)\n⊲(Corrector)\n5:\nsτ ←sτ + ǫτSθ∗(sτ, τ) + √2ǫτζc\n6:\nζp ∼NC(0, I)\n⊲(Predictor)\n7:\nsτ ←sτ −fτ∆τ + g2\nτSθ∗(sτ, t)∆τ + gτ\n√\n∆τζp\n8:\nif i ≡0 (mod ℓ) then\n⊲(Posterior)\n9:\n∇sτ log ˜p(x|sτ) ←1\nδτ\nhσ2\nτ\nδ2τ\nI + diag(vφ)\ni−1\n(sτ\nδτ −x)\n10:\nsτ ←sτ + λg2\nτ∇sτ log ˜p(x|sτ)\n11:\nend if\n12: end for\n13: return ˆs = s0\nstrongly the data consistency condition, causing the sample to con-\nverge to the mixture signal. To prevent this, we only perform the\nposterior step every ℓiterations. We solve the reverse SDE using\na PC sampler [7] - a numeric sampler consisting of a discretisation\nof (2) - the predictor - followed by a Langevin sampling step - the\ncorrector - to ‘correct‘ the marginal at time t. The overall E-step is\nsummarised in Algorithm 2. The variable τ denotes discrete time-\nstep in [0, 1]. For simplicity, we employ the shorthand στ, fτ, gτ for\nσ(τ), f(τ), g(τ), respectively.\n3.2.2. M-Step\nHaving obtained a clean speech estimate ˆs in the E-step, we now con-\nsider updating the noise parameters φ = {W, H} via (9), approxi-\nmating the expectation with a Monte-Carlo average using s ←ˆs:\nφ ←argmax\n{W,H}≥0\nlog pφ(x|ˆs)\n= argmin\n{W,H}≥0\n(x −ˆs)H(x −ˆs)\nvφ\n+ log(vφ),\n(16)\nwhere (·)H denotes the conjugate transpose operation, and the divi-\nsion is done element-wise. The above problem can be solved using\ndifferent algorithms, e.g., the multiplicative update rules [17,18].\n4. EXPERIMENTS\nIn this section, we provide a performance evaluation of our pro-\nposed UDiffSE framework as compared against an unsupervised\nspeech enhancement approach based on recurrent VAE (RVAE)1 [3,\n14], as well as a state-of-the-art diffusion-based supervised SE\nmethod, called score-based generative model for speech enhance-\nment (SGMSE+)2 [11].\nEvaluation Metrics. To measure the quality of the enhanced speech\nsignals, we use standard instrumental evaluation metrics, including\nthe scale-invariant signal-to-distortion ratio (SI-SDR) in dB [19], the\nextended short-time objective intelligibility (ESTOI) measure [20]\nranging in [0, 1], and the perceptual evaluation of speech qual-\nity (PESQ) score [21] ranging in [−0.5, 4.5].\nWe also use the\n1https://github.com/XiaoyuBIE1994/DVAE_SE/\n2https://github.com/sp-uhh/sgmse\nTable 1: Speech enhancement results under both matched and mismatched conditions. ‘S’: supervised, ‘U’: unsupervised. Bold and italicised\nindicate the best and second best performances, respectively.\nMethod\nType\nSI-SDR (dB)\nPESQ\nESTOI\nSIG-MOS\nBAK-MOS\nOVR-MOS\nInput (WSJ0-QUT)\n-\n-2.60 ± 0.17\n1.83 ± 0.02\n0.50 ± 0.01\n4.04 ± 0.01\n2.93 ± 0.02\n3.13 ± 0.01\nRVAE [3,14]\nU\n4.39 ± 0.21\n2.20 ± 0.02\n0.59 ± 0.01\n3.88 ± 0.02\n3.32 ± 0.02\n3.13 ± 0.02\nUDiffSE (Ours)\nU\n4.80 ± 0.23\n2.21 ± 0.02\n0.63 ± 0.01\n4.33 ± 0.01\n3.74 ± 0.02\n3.74 ± 0.02\nSGMSE+ [11]\nS\n9.41 ± 0.18\n2.66 ± 0.02\n0.77 ± 0.01\n4.48 ± 0.01\n4.51 ± 0.01\n4.19 ± 0.01\nInput (TCD-TIMIT)\n-\n-8.74 ± 0.29\n1.84 ± 0.02\n0.35 ± 0.01\n3.52 ± 0.02\n2.22 ± 0.03\n2.68 ± 0.01\nRVAE [3,14]\nU\n1.44 ± 0.31\n2.02 ± 0.02\n0.35 ± 0.01\n3.08 ± 0.03\n3.18 ± 0.02\n2.61 ± 0.02\nUDiffSE (Ours)\nU\n0.37 ± 0.25\n2.01 ± 0.02\n0.41 ± 0.01\n3.91 ± 0.01\n2.88 ± 0.03\n3.08 ± 0.02\nSGMSE+ [11]\nS\n-3.97 ± 0.41\n2.04 ± 0.03\n0.38 ± 0.01\n3.79 ± 0.02\n3.43 ± 0.02\n3.13 ± 0.02\nDNS-MOS [22], a non-intrusive speech quality metric, which pro-\nvides scores for the speech quality (SIG), background noise quality\n(BAK), and overall quality (OVRL) of speech. For all the metrics,\nhigher values indicate improved performance.\nDatasets. To learn the clean speech prior model, we train on the\n‘si tr s‘ subset of the Wall Street Journal (WSJ) corpus [23], which\namounts to roughly 25 hours of data. The STFT is computed using\na window size of 510, a hop-length of 128 (≈75% overlap), and a\nHann window, which gives F = 256 frequency bins. All signals\nhave a sampling rate of 16kHz. To ensure similarity across samples\nof different length during training, subsamples are randomly selected\nfrom a STFT transform so that we get T = 256 time frames with\nstart and end positions randomly generated during training.\nFor performance evaluation, we use the WSJ0-QUT dataset cre-\nated by [14], comprising 651 synthetic mixtures (about 1.5 hours of\nnoisy speech data) which uses clean speech signals from the ‘si et\n05‘ subset of WSJ dataset and noise signals from the QUT-NOISE\ncorpus [24]. These include Caf´e, Home, Street, and Car and have\nSNR values of −5 dB, 0 dB, and 5 dB. We also evaluate generalisa-\ntion capability of different methods in mismatched conditions by us-\ning pre-computed noisy versions of the TCD-TIMIT data presented\nin [25]. This set contains noise types Living Room (from the second\nCHiME challenge [26]), White, Car, and Babble (from the RSG-10\ncorpus [27]) with SNR values of −5 dB, 0 dB, and 5 dB and. This\nyields 540 test speech signals (or approximately 45 minutes).\nStochastic Differential Equation. The SDE in (4) has parameter\nvalues γ = 1.5, σmin = 0.05, σmax = 0.5. To avoid instabilities\naround 0, we adopt standard practice and set a minimum process\ntime with tmin = 0.03.\nModels architecture. We adapt the SGMSE+ architecture devel-\noped in [10], which is based on a multi-resolution U-Net structure,\nby zeroing out their x term and adapting the channels. RVAE con-\nsists of an encoder-decoder architecture composing bidirectional\nlong short-term memory (BLSTM) networks. For both RVAE and\nSGMSE+, we use the pretrained models that are available in their\nassociated public code repositories.\nTraining setup. We train the score model Sθ∗for 220 epochs using\nan Adam optimiser with a learning rate of 0.0001 and a batch size\nof 16. Our loss is an exponential moving average of the network’s\nweights, initialised with a decay of 0.999.\nEM settings. The reverse process in (14) is solved using a PC sam-\npler with step size ǫτ := (στ/2)2. The number of reverse sampling\nsteps is set to N = 30. The posterior update step is performed every\nℓ= 2 steps, and the NMF variances matrices have rank r = 4. For\neach sample, we perform 5 EM iterations. We observe that perform-\ning the same denoising procedure over b samples in parallel and then\naveraging the result yields much better performance; we thus set the\nbatch size to b = 4. The weighting parameter λ is set to 1.5. These\nparameter choices are motivated by an extensive set of experimental\nstudies provided in the Supplementary Material.\nResults. We report our SE results in Table 1. Competing methods\nare evaluated in the matched and mismatched cases. Inspecting the\nresults, we can make a number of conclusions: As may be expected,\nthe supervised framework outperforms its unsupervised counterpart\nin the matched case, but at the cost of utilising labelled data. Our\nUDiffSE framework outperforms the alternative unsupervised RVAE\non almost all metrics under both matched and mismatched condi-\ntions. In particular, it achieves much higher ESTOI, SIG-MOS, and\nOVR-MOS scores than RVAE, which is more noticeable in the mis-\nmatched condition.\nFurthermore, the proposed UDiffSE method outperforms the su-\npervised SGMSE+ framework for both the ESTOI and SIG-MOS\nmetrics in the mismatched condition, with a comparable OVR-MOS\nscore. While all three frameworks have very similar PESQ results\nin the mismatched case, the unsupervised methods signiﬁcantly out-\nperform SGMSE+ in terms of SI-SDR (by more than 4 dB). The\nperformance of UDiffSE on the TCD-TIMIT dataset showcases its\ncapacity to generalise to unseen data, which could possibly imply\nthat it has learnt a good representation of general clean speech as the\nunderlying prior. Supplementary material, including audio samples,\nis available online. 3\n5. CONCLUSION\nIn this paper, we introduce UDiffSE, an unsupervised generative-\nbased framework to solve the SE task by learning an implicit prior\ndistribution over clean speech data. We do this by deﬁning a con-\ntinuous diffusion process in the STFT domain in the form of a con-\nditional SDE, and imposing an NMF-based parameterised additive\nnoise model. An EM approach is developed to simultaneously gen-\nerate clean speech and learn the noise parameters. An approxima-\ntion of the likelihood term in the E-step then yields a tractable pos-\nterior sampling procedure. This method outperforms an unsuper-\nvised VAE-based approach to SE for almost all metrics in matched\nand mismatched test conditions, while showcasing better generalisa-\ntion performance than a state-of-the-art diffusion-based supervised\nmethod. UDiffSE does, however, have the disadvantage of being\ntime-consuming, which originates from the complexity of the re-\nverse diffusion process. Future works include speeding up the re-\nverse process, utilising the recent advancements in diffusion-based\nimage generation, and developing more efﬁcient noise models.\n3https://github.com/joanne-b-nortier/UDiffSE\n6. REFERENCES\n[1] D. Wang and J. Chen, “Supervised speech separation based\non deep learning: An overview,” IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, vol. 26, no. 10, pp.\n1702–1726, 2018.\n[2] Y. Bando, M. Mimura, K. Itoyama, K. Yoshii, and T. Kawa-\nhara, “Statistical speech enhancement based on probabilistic\nintegration of variational autoencoder and non-negative ma-\ntrix factorization,” in International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2018, pp. 716–720.\n[3] X. Bie, S. Leglaive, X. Alameda-Pineda, and L. Girin, “Unsu-\npervised speech enhancement using dynamical variational au-\ntoencoders,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 30, pp. 2993–3007, 2022.\n[4] Y. Bando, K. Sekiguchi, and K. Yoshii,\n“Adaptive neu-\nral speech enhancement with a denoising variational autoen-\ncoder.,” in Interspeech, 2020, pp. 2437–2441.\n[5] D. P. Kingma and M. Welling,\n“Auto-encoding variational\nbayes,” in Proc. ICLR, April 2014.\n[6] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Gan-\nguli, “Deep unsupervised learning using nonequilibrium ther-\nmodynamics,” in ICML, 2015.\n[7] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Er-\nmon, and B. Poole, “Score-based generative modeling through\nstochastic differential equations,” in International Conference\non Learning Representations, 2021.\n[8] Y.-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard, C. Yu, and Y.\nTsao, “Conditional diffusion probabilistic model for speech\nenhancement,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), 2022, pp. 7402–\n7406.\n[9] J. Serr`a, S. Pascual, J. Pons, R. O. Araz, and D. Scaini, “Uni-\nversal speech enhancement with score-based diffusion,” arXiv\npreprint arXiv:2206.03065, 2022.\n[10] S. Welker, J. Richter, and T. Gerkmann, “Speech enhancement\nwith score-based generative models in the complex STFT do-\nmain,” in Proc. Interspeech 2022, 2022, pp. 2928–2932.\n[11] J. Richter, S. Welker, J.-M. Lemercier, B. Lay, and T.\nGerkmann,\n“Speech enhancement and dereverberation with\ndiffusion-based generative models,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2023.\n[12] H. Yen, F. G. Germain, G. Wichern, and J. Le Roux, “Cold dif-\nfusion for speech enhancement,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\n2023, pp. 1–5.\n[13] B. D. Anderson,\n“Reverse-time diffusion equation models,”\nStochastic Processes and their Applications, vol. 12, no. 3, pp.\n313–326, 1982.\n[14] S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud, “A\nrecurrent variational autoencoder for speech enhancement,” in\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2020.\n[15] Y. Song and S. Ermon, “Generative modeling by estimating\ngradients of the data distribution,” in Advances in Neural In-\nformation Processing Systems, 2019, pp. 11895–11907.\n[16] X. Meng and Y. Kabashima, “Diffusion model based posterior\nsampling for noisy linear inverse problems,”\narXiv preprint\narXiv:2211.12343, 2022.\n[17] C. F´evotte, N. Bertin, and J.-L. Durrieu, “Nonnegative matrix\nfactorization with the itakura-saito divergence: With applica-\ntion to music analysis,” Neural computation, vol. 21, no. 3, pp.\n793–830, 2009.\n[18] M. Sadeghi and X. Alameda-Pineda,\n“Robust unsupervised\naudio-visual speech enhancement using a mixture of varia-\ntional autoencoders,”\nin IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2020, pp.\n7534–7538.\n[19] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “SDR–\nhalf-baked or well done?,” in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2019.\n[20] J. Jensen and C. H. Taal, “An algorithm for predicting the in-\ntelligibility of speech masked by modulated noise maskers,”\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 24, no. 11, pp. 2009–2022, 2016.\n[21] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hek-\nstra, “Perceptual evaluation of speech quality (PESQ)-a new\nmethod for speech quality assessment of telephone networks\nand codecs,” in IEEE international conference on acoustics,\nspeech, and signal processing. Proceedings (ICASSP), 2001,\nvol. 2, pp. 749–752.\n[22] C. K. Reddy, V. Gopal, and R. Cutler,\n“DNSMOS P. 835:\nA non-intrusive perceptual objective speech quality metric to\nevaluate noise suppressors,” in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2022,\npp. 886–890.\n[23] J. Garofolo, D. Graff, D. Paul, and D. Pallett, “CSR-I (WSJ0)\ncomplete LDC93S6B,”\nWeb Download. Philadelphia: Lin-\nguistic Data Consortium, vol. 83, 1993.\n[24] D. Dean, A. Kanagasundaram, H. Ghaemmaghami, M. H. Rah-\nman, and S. Sridharan, “The QUT-NOISE-SRE protocol for\nthe evaluation of noisy speaker recognition,” in Proceedings of\nInterspeech, 2015, pp. 3456–3460.\n[25] A. H. Abdelaziz et al., “NTCD-TIMIT: A new database and\nbaseline for noise-robust audio-visual speech recognition.,” in\nInterspeech, 2017, pp. 3752–3756.\n[26] E. Vincent, J. Barker, S. Watanabe, J. Le Roux, F. Nesta, and\nM. Matassoni, “The second ’CHiME’ speech separation and\nrecognition challenge: Datasets, tasks and baselines,” in IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2013, pp. 126–130.\n[27] H. J. Steeneken and F. W. Geurtsen, “Description of the RSG-\n10 noise database,” report IZF, vol. 3, pp. 1988, 1988.\nThis figure \"figure1.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2309.10450v1\n",
  "categories": [
    "cs.CV",
    "cs.SD",
    "eess.AS",
    "eess.SP",
    "stat.ML"
  ],
  "published": "2023-09-19",
  "updated": "2023-09-19"
}