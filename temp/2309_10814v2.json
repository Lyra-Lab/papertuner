{
  "id": "http://arxiv.org/abs/2309.10814v2",
  "title": "Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning",
  "authors": [
    "Tianhua Zhang",
    "Jiaxin Ge",
    "Hongyin Luo",
    "Yung-Sung Chuang",
    "Mingye Gao",
    "Yuan Gong",
    "Xixin Wu",
    "Yoon Kim",
    "Helen Meng",
    "James Glass"
  ],
  "abstract": "How can we perform computations over natural language representations to\nsolve tasks that require symbolic and numeric reasoning? We propose natural\nlanguage embedded programs (NLEP) as a unifying framework for addressing\nmath/symbolic reasoning, natural language understanding, and instruction\nfollowing tasks. Our approach prompts a language model to generate full Python\nprograms that define functions over data structures which contain natural\nlanguage representations of structured knowledge. A Python interpreter then\nexecutes the generated code and prints the output. Despite using a task-general\nprompt, we find that this approach can improve upon strong baselines across a\nrange of different tasks including math and symbolic reasoning, text\nclassification, question answering, and instruction following. We found that\nthe generated programs are interpretable since they outline the exact reasoning\nprocess followed by the program interpreter.",
  "text": "Natural Language Embedded Programs for\nHybrid Language Symbolic Reasoning\nTianhua Zhang†∗, Jiaxin Ge♢∗, Hongyin Luo‡∗, Yung-Sung Chuang‡, Mingye Gao‡,\nYuan Gong‡, Xixin Wu†, Yoon Kim‡, Helen Meng†, James Glass‡\n†The Chinese University of Hong Kong, ‡Massachusetts Institute of Technology, ♢Peking University\ntzhang@se.cuhk.edu.hk, aomaru@stu.pku.edu.cn, hyluo@mit.edu\nAbstract\nHow can we perform computations over natu-\nral language representations to solve tasks that\nrequire symbolic and numeric reasoning? We\npropose natural language embedded programs\n(NLEP) as a unifying framework for address-\ning math/symbolic reasoning, natural language\nunderstanding, and instruction following tasks.\nOur approach prompts a language model to gen-\nerate full Python programs that define functions\nover data structures which contain natural lan-\nguage representations of structured knowledge.\nA Python interpreter then executes the gener-\nated code and prints the output. Despite using a\ntask-general prompt, we find that this approach\ncan improve upon strong baselines across a\nrange of different tasks including math and\nsymbolic reasoning, text classification, ques-\ntion answering, and instruction following. We\nfound that the generated programs are inter-\npretable since they outline the exact reasoning\nprocess followed by the program interpreter.\n1\nIntroduction\nSolving complex language tasks often requires per-\nforming computations over natural language repre-\nsentations. For language-based reasoning, chain-\nof-thought prompting (CoT; Wei et al., 2022) has\nemerged as a promising approach for surfacing\nthe symbolic reasoning capabilities of large lan-\nguage models (LLMs). However, certain types\nof computations (e.g., arithmetic) are unnatural\nto perform in pure language space, and hence\npresent difficulties for LLMs. General-purpose\nprogramming languages, on the other hand, pro-\nvide convenient abstractions as well as predefined\nlibraries and functions for natively implementing\nmany types of symbolic computations, and there\n∗\nEqual contribution.\nCorrespondence to Hongyin\nLuo at hyluo@mit.edu. Work done during Jiaxin Ge vis-\niting MIT. Source code is available at https://github.com/\nluohongyin/LangCode.\nhas been much recent work on interleaving pro-\ngram calls within CoT-style reasoning to extend the\ncapabilities of LLMs. While promising, existing\nmethods are generally limited to narrow types of\ntasks such as math and symbolic reasoning (Chen\net al., 2022; Cai et al., 2023; Gao et al., 2023), sim-\nple API calling (Schick et al., 2023; Paranjape et al.,\n2023; Liang et al., 2023a), and database accessing\n(Cheng et al., 2022). These works moreover rely on\ntask-specific prompts which are hard to generalize\nacross datasets.\nThis work describes a task-general approach for\ncombining the language-based reasoning capabili-\nties of LLMs with symbolic computations enabled\nby the use of programs. Specifically, we prompt\nLLMs to generate natural language embedded pro-\ngrams (NLEPs), which are fully executable Python\nprograms containing appropriate package import-\ning, structured natural language representations of\nknowledge, function definitions for problem solv-\ning, and response printing. The generated NLEP\nis then executed using a Python interpreter that\ncaptures the standard output of the program as the\nresponse. An example of an NLEP generated by\nGPT-4 is shown in Figure 1.\nNLEPs use code as a scaffold to reason over\nnatural language representations of data and knowl-\nedge. This makes our approach different from\nToolFormer (Schick et al., 2023) and language\nmodel as tool maker (LATM; Cai et al., 2023),\nwhich instead use language as the scaffold and inter-\nleave API calls within natural language sentences\nduring LLM generation. Compared to program-\nof-thought (PoT; Chen et al., 2022) and program\naided language models (PAL; Gao et al., 2023),\nwhich mainly focus on math and symbolic prob-\nlems, NLEPs handles natural language, math, and\nsymbolic tasks with significantly better generaliza-\ntion ability based on task-general prompts. Com-\npared to existing neuro-symbolic augmentations\napproaches needing new logical language (Li et al.,\narXiv:2309.10814v2  [cs.CL]  29 Mar 2024\n \n # Step 1: Import necessary libraries\n from datetime import datetime\n # Step 2: Define a dictionary storing the knowledge about US presidents\n us_presidents = {\n     \"Dwight D. Eisenhower\": {\"birth_date\": \"1890-10-14\", \"term_start\": \"1953-01-20\"},\n     \"John F. Kennedy\":      {\"birth_date\": \"1917-05-29\", \"term_start\": \"1961-01-20\"},\n     \"Lyndon B. Johnson\":    {\"birth_date\": \"1908-08-27\", \"term_start\": \"1963-11-22\"},\n     \"Richard Nixon\":        {\"birth_date\": \"1913-01-09\", \"term_start\": \"1969-01-20\"},\n     \"Gerald Ford\":          {\"birth_date\": \"1913-07-14\", \"term_start\": \"1974-08-09\"},\n     \"Jimmy Carter\":         {\"birth_date\": \"1924-10-01\", \"term_start\": \"1977-01-20\"},\n     \"Ronald Reagan\":        {\"birth_date\": \"1911-02-06\", \"term_start\": \"1981-01-20\"},\n     \"George H. W. Bush\":    {\"birth_date\": \"1924-06-12\", \"term_start\": \"1989-01-20\"},\n     \"Bill Clinton\":         {\"birth_date\": \"1946-08-19\", \"term_start\": \"1993-01-20\"},\n     \"George W. Bush\":       {\"birth_date\": \"1946-07-06\", \"term_start\": \"2001-01-20\"},\n     \"Barack Obama\":         {\"birth_date\": \"1961-08-04\", \"term_start\": \"2009-01-20\"},\n     \"Donald Trump\":         {\"birth_date\": \"1946-06-14\", \"term_start\": \"2017-01-20\"},\n     \"Joe Biden\":            {\"birth_date\": \"1942-11-20\", \"term_start\": \"2021-01-20\"},\n }\n # Step 3: Define a function that finds the presidents who are elected after a certain year and born on a certain day \nof the week\n def find_presidents(presidents, year, day_of_week):\n     result = []\n     for president, info in presidents.items():\n         birth_date = datetime.strptime(info[\"birth_date\"], \"%Y-%m-%d\")\n         term_start = datetime.strptime(info[\"term_start\"], \"%Y-%m-%d\")\n         if birth_date.strftime(\"%A\") == day_of_week and term_start.year > year:\n             result.append(president)\n     return result\n # Step 4: Print an answer in natural language using the knowledge and function defined above\n year = 1950\n day_of_week = \"Monday\"\n presidents = find_presidents(us_presidents, year, day_of_week)\n if presidents:\n     print(\"The US presidents who were elected after 1950 and born on Mondays are: \" + \", \".join(presidents) + \".\")\n else:\n     print(\"There are no US presidents who were elected after 1950 and born on Mondays.\")\nTool\nUsing\nStructured\nknowledge\nFunction\ndefinition\nPrint\nanswer\nThe US presidents who were elected after 1950 and born on Mondays are: Gerald Ford, Ronald Reagan, Bill Clinton.\nInstruction:\nWho are the US presidents elected after 1950 and born on Monday?\nStdout:\nNLEP generated by GPT-4\nFigure 1: A generated NLEP correctly answers the given question while ChatGPT-4 obtains an incorrect answer (link). This\nNLEP uses the date-weekday conversion tool in the datetime package, constructs structured knowledge about US presidents,\nimplements a selection function, and outputs natural language responses depending on the function output. A more detailed\ncomparison between NLEP and ChatGPT-4 code interpreter is shown in Figure 5.\n2023) and predefined solvers (Pan et al., 2023),\nNLEP uses a general programming interface thus\ncan handle a significantly wider range of tasks\nwith more flexible programming elements includ-\ning packages, databases, and APIs.\nExperiments across math and symbolic reason-\ning, question answering and instruction following,\nand text classification tasks demonstrate that (1)\nNLEPs conducts accurate reasoning on both struc-\ntured and unstructured tasks and inputs; (2) NLEP’s\nstep-by-step, meta prompting strategy can signif-\nicantly improve the prompt efficiency across dif-\nferent tasks. As a result, we conclude that pro-\ngramming language prompting with NLEP is more\ncapable and generalizable than existing natural lan-\nguage and neuro-symbolic prompting strategies.\n2\nApproach: NLEP Prompting\nAn NLEP is a program containing both program-\nming code and natural language. NLEPs use natu-\nral language in several different ways. First, it uses\nnatural language comments to guide step-by-step\nprogram generation. Second, language is used to\nrepresent structured knowledge through Python’s\nnative data structures (e.g., dictionaries and lists).\nFinally, an NLEP uses language to print fluent re-\nsponses to the user input by constructing a standard\noutput string containing references to program vari-\nables.\nThe hybrid language-symbolic design of NLEP\nenables generalized problem solving for natural\nlanguage, math, symbolic reasoning, and API call-\ning tasks, which have traditionally been tackled\nby separate mechanisms. This approach combines\nthe benefits of language-based reasoning with pro-\ngram synthesis: comments and knowledge in nat-\nural language improve program generation, while\nthe structured/symbolic reasoning powered by pro-\ngram interpreters provides more accurate compu-\ntations than would have been obtained via direct\ndecoding from LLMs.\nAn example of an NLEP for answering a ques-\ntion is shown in Figure 5. In the generated program,\neach section is preceded by comments in natural\nlanguage, and the defined counting function uses\nknowledge stored in a key-value dictionary (which\nitself is generated from GPT-4’s internal knowl-\nedge) to find the correct answer. Finally, the answer\nis printed through a natural language response. In\nthis example, we generated 5 independent NLEPs\nand found that they achieve 100% accuracy, com-\npared to 60% for ChatGPT-4 and 40% GPT-4 API.\nNLEP structure. More generally, each NLEP\ncontains four sections: (1) importing necessary li-\nbraries, (2) defining variables containing structured\nknowledge, (3) implementing problem-solving\nfunctions, and (4) printing the response in natural\nlanguage. Instead of providing direct solutions for\neach task, we guide the model to arrive at a solution\nfollowing this four-step process. The structured\nand modularized programming design disentangles\nthe knowledge extraction and reasoning steps, in\ncontrast to PoT (Chen et al., 2022), where different\nsegments may intertwine. As show in the exam-\nple in Figure 1, an NLEP answers the question by\nconstructing a structured knowledge dictionary con-\ntaining the birthday and start date of the US presi-\ndents. To recognize the weekdays, the program uti-\nlizes pre-defined functions in the datetime pack-\nage. The selected answers are stored in a list and\nthen embedded into an output template. The NLEP\nalso handles the situation when no answer is found.\nThe correct answer is then printed by the NLEP.\nTask-general demonstration prompts. As is\nstandard in chain-of-thought prompting (Nye et al.,\n2021; Wei et al., 2022), our approach uses demon-\nstration prompts for NLEP generation. However,\nunlike previous approaches our demonstrations are\nnot task-specific. The unified NLEP structure mo-\ntivates models to solve different problems in the\nsame disentangled four steps, bypassing the ne-\ncessity for task-specific, in-domain examples to\nexplicitly teach the models for a particular task.\nFor example, for all classification tasks we con-\nsider we use the same demonstration prompt (de-\nrived from SST2). Similarly, we use mostly the\nsame prompt for our math and symbolic reasoning\ntasks. This task-general prompt is similar in spirit\nto zero-shot chain-of-thought prompting (Kojima\net al., 2023) which adds a task-agnostic prompt\n(“Let’s think step-by-step”) to elicit the rea-\nsoning capabilities of LLMs in a task-agnostic way.\nThe prompts used for the various tasks are given\nin Table 1, and the exact prompts are given in Ap-\npendix D. In summary, we use 4 different demon-\nstration prompts across 16 tasks, each of which\nworks well within a task category. Thus, while the\nproposed method is not fully task-agnostic in the\nstrictest sense of the term, it is still more flexible\nthan previous approaches that combine program\nsynthesis with chain-of-thought prompting (Chen\net al., 2022; Gao et al., 2023), which use examples\nfrom the dataset to craft prompts.\nProgrammatic reasoning for natural language\nunderstanding. Prior works on combining pro-\ngram synthesis with LLM-based reasoning have\ngenerally focused on math and symbolic reasoning\ntasks (Chen et al., 2022; Gao et al., 2023), and it\nhas not been clear how such methods could be ex-\ntended to address natural language understanding\n(NLU) tasks. We show that NLEPs can be straight-\nforwardly extended to text classification tasks.\nFor question answering, we apply NLEP prompt-\ning and the target output is constructed by the gen-\nerated programs. Classification tasks, on the other\nhand, are handled by a different type of NLEP con-\nsisting of a decision tree since standard program-\nbased reasoning may not seamlessly translate to\ntext-based classification and yield optimal perfor-\nmance. Each node of the decision tree is anno-\ntated by a simple natural language sentence, and\nthe Yes/No decisions at each node are handled in\na zero-shot way by an entailment classifier, which\nhas in general been shown to be an effective ap-\nproach to zero-shot text classification (Obamuyide\nand Vlachos, 2018; Condoravdi et al., 2003; Ge\net al., 2023). Concretely, given the tree we com-\npute the entailment score between the input and\nthe language description associated with each node\nand traverse the decision tree until a leaf node is\nreached. We emphasize that the topology of the\ntree and the textual description of each node is gen-\nerated by the prompted LLM only once per bench-\nmark. The decision maker per question is indeed\nthe smaller entailment models for tree traversal.\nConsequently, NLEP leverages LLMs to efficiently\nconstruct decision trees, assisting smaller models in\nmaking informed decisions and augmenting overall\nperformance. The demonstration prompt for clas-\nsification tasks is given by a manually constructed\nexample from SST2 (Wang et al., 2018) for dif-\nferent benchmarks. We find that this prompt can\ngenerate NLEPs containing sensible decision trees\nfor various classification tasks without requiring\ntask-specific examples. An example of the gener-\nated program and the corresponding decision tree\nis shown in Figure 2.\n3\nExperiments\nWe evaluate natural language embedded programs\n(NLEPs) on 16 tasks across three broad task cate-\ngories. The tasks and corresponding prompts are\nsummarized in Table 1.\n    # Step 1: Define the criteria of the decision tree.\n    criterions = {\n        'is_positive': ‘My feeling is positive',\n        'is_sad': 'I feel sad',\n        'is_angry': 'I feel angry',\n        'is_afraid': 'I feel afraid',\n        'is_happy': 'I feel happy',\n        'is_love': 'I feel love',\n        'is_surprised': 'I feel surprised'\n    }\n    # Step 2: Define the balanced decision tree for this classification task\n    tree = {\n        'root': 'is_positive',\n        'is_positive': {'yes': 'is_happy', 'no': 'is_sad'},\n        'is_happy': {'yes': 'I feel happy', 'no': 'is_love'},\n        'is_love': {'yes': 'I feel love', 'no': 'is_surprised'},\n        'is_surprised': {'yes': 'I feel surprised', 'no': 'I feel happy'},\n        'is_sad': {'yes': 'I feel sad', 'no': 'is_angry'},\n        'is_angry': {'yes': 'I feel angry', 'no': 'is_afraid'},\n        'is_afraid': {'yes': 'I feel afraid', 'no': 'I feel sad'}\n    }\nInput Prompt\n# Task: Emotion classification\n# Description: I wrote a sentence about my feeling, determine which emotion do I feel.\n# Possible emotions: ['sad', 'happy', 'love', 'angry', 'afraid', 'surprised']\n# Generated decision Tree:\nis_positive\nis_happy\nis_sad\n“happy”\nis_love\n“love”\nis_surprised\n“happy”\n“surprised”\n“sad”\nis_angry\n“angry”\nis_afraid\n“angry”\n“sad”\nYES\nYES\nYES\nYES\nYES\nYES\nYES\nNO\nNO\nNO\nNO\nClassification accuracy of zero-shot methods (%)\n GPT-3: 42.7   6-class RoBERTa-Ent: 49.2   Decision Tree RoBERTa-Ent: 54.5\nNO\nNO\nNO\n  RoBERTa_Ent(\n      f“{is_angry} is entailed by {input}”\n  )\n  Ent.\n  Con.\nYES\n  Ent.\n  Con.\nNO\nEntailment\nClassification\nFigure 2: A decision tree structure generated within an NLEP for emotion classification based on task description using an\nexample program for SST2 as the prompt. The branching of each node is decided by a RoBERTa (Liu et al., 2019) text entailment\nmodel. This language-based decision tree generated by an NLEP outperforms GPT-3 and entailment-based multi-class prediction\n(Ge et al., 2023) without needing any task-specific examples (i.e., exemplars specific to the emotion classification dataset).\nMath and symbolic reasoning tasks include\nTracking Shuffled Objects, Dyck Language, Word\nSorting and Chinese Remainder Theorem from\nBigBench (Srivastava et al., 2023), Scheduling\nMeeting task from Cai et al. (2023), GSM-Hard1\nbenchmark of math word problems from Gao et al.\n(2023), and Game of 24 (Yao et al., 2023a). We\nuse two examples for all tasks except for Game of\n24, for which we applied a word sorting example\nto elicit stronger game-playing reasoning ability.\nThe exact NLEP prompts we used are given in\nAppendix D.1 and D.2.\nQuestion answering and instruction follow-\ning tasks include the StrategyQA (Geva et al.,\n2021a), TruthfulQA (Lin et al., 2022), and Vicu-\nnaQA (Chiang et al., 2023) benchmarks. Strate-\ngyQA requires models to answer multi-hop ques-\ntions with “Yes” or “No”. TruthfulQA and Vicu-\nnaQA contain questions and instructions requiring\nfree-form responses. VicunaQA also allows us to\ntest how NLEPs perform in the popular instruction-\nfollowing setting. The evaluation metrics on ques-\ntion answering focus on accuracy, relevance, and\nfactuality of the generated answers. The prompts in\nAppendix D.1 are used for StrategyQA. For Truth-\nfulQA and VicunaQA, we added an example with\na longer response to encourage more detailed re-\nsponse generation.\nText classification tasks includes tasks that re-\n1We opted for GSM-Hard to mitigate potential data con-\ntamination from GSM8K. The numbers in GSM-Hard are\nlarge, less likely to have been encountered by LLMs during\ntraining, and hence augmenting the task complexity. However,\nsince GSM-Hard is automatically constructed, there are cases\nwhere the answers are not reasonable or the questions appear\npeculiar. We detail the analysis in Appendix B.\nDomain\nTask\nPrompt\nMath and Symbolic\nReasoning\nTracking Shuffled Objects (7)\nD.1\nDyck Language\nD.1\nWord Sorting\nD.1\nChinese Remainder Theorem\nD.1\nScheduling Meeting\nD.1\nGSM-Hard\nD.1\nGame of 24\nD.2\nQuestion Answering\nStrategyQA\nD.1\nTruthfulQA\nD.3\nVicunaQA\nD.3\nText Classification\nSST2\nD.4\nCola\nD.4\nEmotion-Classification\nD.4\nAmazon Review\nD.4\nHate-Speech\nD.4\nSocial Bias Frame\nD.4\nTable 1: Summary descriptions of the various tasks considered\nin this work.\nquire understanding of both natural language inputs\nand labels. We evaluate NLEP on movie-review\nclassification (SST2; Socher et al., 2013), linguistic\nacceptance (COLA; Warstadt et al., 2019), emotion\nclassification (Saravia et al., 2018), amazon review\n(Ni et al., 2019), hate speech detection (de Gibert\net al., 2018), and stereotypes recognition (Sap et al.,\n2019). We use the prompts in Appendix D.1 for\nmodel-free classification. For decision tree genera-\ntion, the prompts in Appendix D.4 are applied.\n3.1\nMath and Symbolic Reasoning\nWe compare NLEP prompting with chain-of-\nthought (CoT; Wei et al., 2022), program-of-\nthought (PoT; Chen et al., 2022), and LLMs as\ntool makers (LATM; Cai et al., 2023). We also\ncompare against tree-of-thought (ToT; Yao et al.,\n2023a) on the Game of 24 benchmark, where ToT\noutperforms CoT by a significant margin (but re-\nquires many more calls to the LLM). We evalu-\nate CoT and PoT with both task-general and task-\nspecific demonstrations. Since LATM needs in-\ndomain input-output pairs to create tools, we only\nreport the results with task-specific LATM.\nTask-general prompting.\nFor task-general\nprompts we use two examples as the in-context\ndemonstration for the math and symbolic reason-\ning benchmarks (see Table 1 and Appendix D). For\nCoT, we present two examples with intermediate\nreasoning represented in natural language rather\nthan as programs. Our task-general PoT implemen-\ntation takes the math and symbolic reasoning lines\nsimilar as (Chen et al., 2022) and (Gao et al., 2023),\nbut without the step-by-step programming scheme\nin NLEP as an ablation.\nTask-specific prompting baselines. We report\nthe task-specific prompting performance as an “up-\nper bound” for each task. For CoT, we use the\nsame prompting settings (from 3 to 8-shot) adopted\nin previous studies (Cobbe et al., 2021; Cai et al.,\n2023; Fu et al., 2023). For PoT, we use the same\nin-context examples as in the task-specific CoT ex-\namples, but provide intermediate reasoning steps\nin Python code. On the GSM-Hard benchmark,\nwe adopt the demonstrations (9-shot) for GSM8K\nused in (Chen et al., 2022). For the Chinese Re-\nmainder Theorem and Scheduling Meeting bench-\nmarks, we construct the in-context examples with\nthe first three successful instances of task-general\nPoT. For LATM, we evaluate its performance on\nTracking Shuffled Objects (7) using the provided\ntool and cite the results for other tasks from (Cai\net al., 2023). Details are shown in Appendix E.\nProgram synthesis approaches (PoT and NLEP)\nmay sometimes generate non-executable programs\nif lack task-specific programming demonstration.\nFor both approaches, we select certain benchmarks\nto resample up to three additional programs if the\nreturned program failed at execution. Since this\ncondition is triggered only if program execution\nfails, there is no label leakage. We discuss this\nfurther in Section 4 and provide results details in\nAppendix B.\n3.1.1\nResults\nWe show the main results of NLEP prompting on\nsix math and symbolic reasoning tasks in Table 2.\nAn example of NLEP generated for solving a Dyck\nlanguage problem is shown in Figure 3(a).\nGPT-4 Results. Among the three approaches\nemploying task-general prompts, NLEP demon-\nstrates superior performance over CoT across 5\nof 6 tasks and outperforms PoT on 4 of 6 tasks.\nAdditionally, NLEP achieves equivalent perfor-\nmance to PoT on Word Sorting benchmark. The\nlarge performance gap between NLEP and CoT\nsuggests that programmatic reasoning can enable\nmore accurate answers. Compared to PoT, NLEP\nachieves significantly higher average accuracy, es-\npecially on the Dyck Language (66.4%→91.6%)\nand the complex Chinese Remainder Theorem\n(84.4%→97.2%) tasks. On GSM-Hard, we con-\nfirm the same phenomenon discovered by (Gao\net al., 2023) where language does not further bene-\nfit the calculation accuracy with GPT-4.\nNLEP also achieves comparable performance to\ntask-specific, few-shot prompting methods. No-\ntably, our method achieves the best performance on\nTracking Shuffled Objects (7) and Dyck Language,\nand outperforms task-specific CoT on many bench-\nmarks. On the Word Sorting benchmark, NLEP\nonly fails on one instance where the input word\nsequence contains “steelmake\" and GPT-4 auto-\nmatically corrected it to “steelmaker\". We find that\nthe high scores of task-specific PoT on Word Sort-\ning and Chinese Remainder Theorem come from\nthe generally applicable programming code from\nthe in-context demonstrations.\nGPT-3.5 Results. We observe a significant de-\ncline in performance with GPT-3.5 for all reasoning\napproaches across nearly all benchmarks. However\nNLEP still achieves the best average performance,\nexhibiting significant improvement on 5 of 6 tasks\nover both task-specific and task-general CoT base-\nlines. NLEP exceeds task-general PoT notably\nacross 4 tasks and demonstrates comparable per-\nformance on the Word Sorting benchmark. On the\nDyck Language benchmark, program-based strate-\ngies (PoT and NLEP with task-general prompts)\nfailed to accomplish the problem without task-\nspecific examples, highlighting the need for strong\nbackbone LLMs.\nGame of 24 results. Table 3 shows the results\non the challenging Game of 24 task from (Yao\net al., 2023a). Our approach also surpasses the ora-\ncle setup of IO/CoT, which calculates the success\nrate of IO/CoT by considering the best of 100 sam-\nples for each instance. However, unlike ToT which\nrequires in-context demonstrations for each decom-\nposed sub-task, NLEP prompting achieves a sig-\nnificant performance gain over ToT (b=1) without\nrequiring a computationally expensive multi-chain\ninference procedure.\nGPT-4\nGPT-3.5-Turbo\n(a) Task-Specific\n(b) Task-General\n(c) Task-Specific\n(d) Task-General\nTasks / Method\nCoT\nPoT\nLATM\nCoT\nPoT\nNLEP\nCoT\nPoT\nCoT\nPoT\nNLEP\nTracking Shuffled Objects\n100.0\n100.0\n100.0\n81.2\n98.4\n100.0\n68.0\n6.8\n51.2\n88.4\n74.4\nDyck Language\n63.6†\n60.8\n87.5†\n39.6\n66.4\n91.6\n20.4†\n28.4\n38.0\n4.0\n7.2\nWord Sorting\n90.9†\n100.0\n99.1†\n84.4\n99.6\n99.6\n59.2†\n100.0\n75.2\n100.0\n99.6\nChinese Remainder Theorem\n0.0†\n100.0\n100.0†\n0.0\n84.4\n97.2\n0.0†\n100.0\n0.0\n72.4\n96.4\nScheduling Meeting\n55.6†\n75.2\n100.0†\n82.8\n85.2\n93.2\n18.9†\n33.6\n39.6\n49.2\n85.6\nGSM-Hard\n57.4\n74.1\n–\n54.9\n69.3\n67.7\n45.0\n63.4\n42.8\n52.2\n54.1\nAverage\n61.3\n85.0\n97.3\n57.2\n83.9\n91.6\n35.3\n55.4\n41.1\n61.0\n69.6\nTable 2: Performance on math and symbolic reasoning tasks with both task-specific and task-general demonstration prompts.\n† stands for results from (Cai et al., 2023). LATM results are not available for GSM-Hard benchmark as it is hard to derive a\ngenerally applicable tool function for all test cases.\nPrompt\nMethod\nAccuracy (%)\nTask-specific\nCoT\n4\nToT (b = 1)\n45\nToT (b = 5)\n74\nTask-general\nPoT\n52\nNLEP\n66\nTable 3: Performance on the Game of 24 benchmark. CoT\nand ToT stand for chain-of-thought (Wei et al., 2022) and\ntree-of-thought (Yao et al., 2023a) prompting respectively. †\nshows the results from (Yao et al., 2023a).\n3.2\nQA and Instruction Following\nStrategyQA. Experiment results are presented in\nTable 4. With GPT-4, NLEP achieves the best per-\nformance under the task-general prompt setting\nand is competitive with the task-specific CoT. With\nGPT-3.5, although the scores of code-based strate-\ngies decrease more than CoT (PoT: 18.4%, NLEP:\n20.1%, task-general CoT: 10.5%, task-specific CoT:\n10.1%), NLEP still exceeds PoT by a significant\nmargin. An example of output is shown in 3(b).\nTruthfulQA. We also evaluate how NLEP\nprompting influences the factuality of question an-\nswering with the TruthfulQA benchmark (Lin et al.,\n2022). A fine-tuned GPT-3 model is applied for\nautomatic scoring. In this experiment, we compare\nthe vanilla auto-regressive text generation method\nagainst NLEP. Traditionally, such question answer-\ning tasks have been solved only with black-box\nlanguage model without explicit symbolic compu-\ntations due to the complexity of test questions.\nThe results are shown in Table 5. With GPT-\n4, the truth score of NLEP prompting strategy is\nclose to standard LLM-based generation, while the\ninformativeness score is higher. However, perfor-\nmance degrades significant with GPT-3.5-Turbo,\nindicating a strong dependence on the program-\nming ability of the underlying language model.\nVicunaQA. Results on the VicunaQA bench-\nmark are shown in Figure 4, where we follow the\nstandard approach and evaluate the answers using\nGPT-4. We find that GPT-4 prefers its own gen-\nerations, which are generally more detailed than\nGPT-3.5-Turbo and NLEP responses. To control\nfor the bias due to response lengths, we also as-\nsess all responses without the requirement about\ndetails using another evaluation prompt. The evalu-\nation prompts with and without the requirement on\ndetails is shown in Appendix F.1 and F.2.\nAs we demonstrate in Figure 4, this assessment\nleads to different results on GPT-4. After remov-\ning the detail requirement in the automatic scor-\ning pipeline, NLEP achieves better performance.\nThis suggests that NLEP can help GPT-4 gener-\nate accurate, factual, and relevant responses. How-\never, human-generated programs for pretraining the\nGPT-4 models usually do not embed long pieces\nof natural language. As a result, the responses\ngenerated by NLEP have a limited level of detail.\n3.3\nText Classification\nFinally, we evaluate whether NLEPs can be applied\nto solve text classification tasks that have tradition-\nally been difficult for pure program synthesis-based\napproaches. As discussed in section 2, we manu-\nally construct a decision tree NLEP for SST2 and\nuse it as a prompt to guide GPT models to generate\ndecision trees for other tasks only with task and la-\nbel descriptions. We highlight that the decision tree\nis generated once per benchmark. Subsequently,\nall decision-making for each specific question is\ncarried out with RoBERTa/DeBERTa to perform\nthe tree traversal re-using the generate tree topol-\nogy. An example input and output NLEP generated\nby GPT-4 for emotion classification is shown in\nFigure 2.\n(a) Dyck Langauge:\n{{}((<>)\n# Step 1: Import necessary built-in libraries\n# No need to import\n# Step 2: Define necessary functions that generally solve this type of problem\ndef complete_sequence(sequence):\n    open_brackets = ['{', '(', '<']\n    close_brackets = ['}', ')', '>']\n    stack = []\n    for char in sequence:\n        if char in open_brackets:\n            stack.append(char)\n        elif char in close_brackets:\n            if len(stack) == 0:\n                return \"Invalid sequence\"\n            else:\n                stack.pop()\n    while len(stack) != 0:\n        open_bracket = stack.pop()\n        sequence += close_brackets[open_brackets.index(open_bracket)]\n    return sequence\n# Step 3: Define constant variables for the task\nsequence = '{{}((<>)'\n# Step 4: Print an answer in natural language.\ncompleted_sequence = complete_sequence(sequence)\nprint(f\"To complete the sequence, we need to close all the open brackets in the correct order. The completed \nsequence is {completed_sequence}.\")\nNLEP generated by GPT-4\nStdout:\nTo complete the sequence, we need to close all the open brackets in the correct order. The \ncompleted sequence is {{}((<>))}.\nTask-General PoT:\n{{}((<>)({\nFigure 3: NLEP generated for solving Dyck language problem. The instruction is “Complete the rest of the sequence, making\nsure that the parentheses are closed properly.” An example for StrategyQA is outlined in Figure 6.\nsetting\nGPT-4\nGPT-3.5-Turbo\nTask-specific\nTask-general\nTask-specific\nTask-general\nCoT\nCoT\nPoT\nNLEP (ours)\nCoT\nCoT\nPoT\nNLEP (ours)\nStrategyQA\n81.7\n78.6\n68.6\n81.2\n71.6\n68.1\n50.2\n61.1\nTable 4: Performance on the StrategyQA benchmark. The experimental setup is the same as in Table 2. Note that LLMs do not\nalways generate “Yes” or “No”. and we only predict the “Yes” label if the “Yes” string is generated explicitly. See Appendices\nD.1 and E for implementation details.\nFoundation Model\nMode\nTrue\nInfo\nTrue * Info\nGPT-4\nText\n76.01\n97.55\n73.56\nNLEP\n75.76\n99.63\n75.40\nGPT-3.5-Turbo\nText\n68.91\n98.90\n67.93\nNLEP\n61.69\n97.18\n59.00\nTable 5: Performance of GPT-4 and GPT-3.5-Turbo on the\nTruthfulQA benchmark.\nWe compare NLEP against two baseline meth-\nods. Our first baseline uses the zero-shot classifica-\ntion method proposed in (Ge et al., 2023) (“multi-\nclass prompting”). This method uses the same\nentailment models but makes the prediction with-\nout the tree structure. Our second baseline asks\na human expert to design a decision tree for each\ntask also based on the SST-2 example. The re-\nsults shown in Table 6 show that NLEP generated\nby GPT-4 outperforms multi-class prompting and\nhuman-generated tree baselines on most datasets.\nModel-free NLEP. We also tried using the task-\ngeneral prompt shown in D.1 to generate NLEPs\nthat directly use programs to solve these tasks.\nThese programs do not need any neural models and\nare hence very efficient (e.g., finishing the entire\nvalidation set in about 2 seconds on CPUs). The re-\nsults can be found in Table 6 (“Model-free NLEP”).\nWhile not achieving the performance of entailment-\nbased methods, the generated NLEP significantly\noutperforms random baselines, suggesting that this\nmay be a viable approach for quickly extracting\nsimple and interpretable classifiers from LLMs.\nModel\nMethod\nPerformance (Num. Classes)\ncola (2)\nemotion (6)\namazon (5)\nhsd (2)\nsbic (3)\nAverage\nRoBERTa\nMulti-class Prompting\n65.87\n49.2\n33.31\n67.78\n52.99\n53.83\nHuman-Generated Tree\n69.03\n22.20\n26.88\n64.85\n58.37\n48.27\nNLEP w/ GPT-3.5\n56.66\n35.1\n33.46\n67.36\n38.25\n46.17\nNLEP w/ GPT-4\n68.94\n54.5\n38.88\n70.92\n55.95\n57.65\nDeBERTa\nMulti-class Prompting\n53.50\n51.93\n37.01\n67.78\n59.08\n53.86\nHuman-Generated Tree\n69.22\n32.15\n33.00\n72.18\n55.02\n52.31\nNLEP w/ GPT-3.5\n49.66\n39.00\n36.18\n70.29\n52.49\n49.52\nNLEP w/ GPT-4\n68.36\n55.4\n40.2\n70.08\n59.68\n58.74\nNone\nModel-free NLEP w/o Tree\n69.13\n40.55\n25.76\n59.62\n37.63\n46.54\nTable 6: Zero-shot performance of different human and LLM-generated classification schemes. The GPT-4 generated decision\ntrees consistently exhibit significant improvement. For model-free NLEP, generated code can be executed on the entire validation\nset in 2 seconds and notably surpasses the random baseline, with cola notably matching the state-of-the-art performance. Results\non SST2 benchmark is outlined in Appendix C.\nTasks / Methods\nCodeLlama7b\nNLEP-CodeLlama7b\nCodeLlama13b\nClaude2\nTask-Specific\nTask-General\nZero-Shot\nTask-General\nTask-General\nPoT\nPoT\nNLEP\nNLEP\nPoT\nNLEP\nPoT\nNLEP\nTracking Shuffled Objects\n95.6\n21.2\n30.0\n84.4\n23.2\n23.2\n93.6\n96.0\nDyck Language\n15.2\n0.8\n0.8\n1.2\n2.0\n2.8\n33.6\n60.4\nWord Sorting\n78.0\n98.0\n93.2\n98.4\n95.6\n97.6\n99.6\n99.6\nChinese Remainder Theorem\n100.0\n0.0\n18.8\n0.0\n1.2\n27.2\n40.4\n60.4\nScheduling Meeting\n32.0\n4.0\n24.8\n34.4\n5.6\n8.8\n10.4†\n24.0†\nTable 7: Performance on five reasoning tasks adopted by Cai et al. (2023): (a) Prompting CodeLlama-7b-Instruct (Rozière et al.,\n2023) with task-specific and task-general demonstrations. (b) We train CodeLlama-7b (Rozière et al., 2023) with out-of-domain\nNLEP-format examples and report the zero-shot performance. (c) Prompting CodeLlama-13b-Instruct (Rozière et al., 2023) with\ntask-general demonstrations. (d) Prompting Claude-2 with task-general demonstrations by API. The demonstration examples\nremain consistent with these outlined in Table 2. † indicates human evaluation.\n4\nDiscussion\nExecution failures and retries. While the task-\ngeneral PoT and NLEP lack programming demon-\nstrations for the target task, GPT-4 in general is\nable to generate bug-free programs as presented\nin Appendix B Table 11. Notably, both PoT and\nNLEP obtain execution error rate of 0 on Tracking\nShuffled Objects (7) and Word Sort tasks. One ad-\nvantage of the program synthesis approaches such\nas PoT and NLEP is that non-executable programs\ncan be identified and filtered. This gives LLMs\nthe chance to “self-correct\" and generate new an-\nswers, and we take advantage of this in our math\nand symbolic reasoning tasks by generating up to\nthree programs if there is an execution failure on\ncertain benchmarks. (For fair comparison we ap-\nply this reattempting scheme to PoT as well). We\nablate on this mechanism in Appendix B, Tables\n8, 9 and 11. Besides effectively reducing the exe-\ncution error as presented in Table 11, these retries\ngreatly enhance the reasoning accuracy. In partic-\nular, 12% and 15.6% improvement is observed on\nthe Chinese Remainder Theorem and the Schedul-\ning Meeting tasks in Table 8(b). In this work we\nonly experiment extra retries with larger tempera-\ntures for diverse sampling and leave more advanced\n“self-correction\" algorithms (e.g., those that make\nuse of error messages (Cai et al., 2023; Hu et al.,\n2023)) for future work.\nDifferent foundation LLMs for NLEP. The\nperformance of task-general PoT and NLEP with\nmore foundation models is reported in Table 7.\nWe provide detailed analysis in Appendix B. Over-\nall, NLEP demonstates superior performance com-\npared to task-general PoT, particularly excelling in\ncomplex tasks such as the Chinese Remainder The-\norem. We note the trend of performance decline\non the reasoning tasks as the abilities of underling\nLLMs decreased from GPT-4, Claude-2, GPT-3.5-\nTurbo to CodeLlama (Rozière et al., 2023). How-\never, this observation is not limited to NLEP, it also\napplies to other prevalent reasoning approaches\nlike PoT. For example, on the Dyck Language task,\nGPT-3.5-Turbo only achieves 7.2% (4.0%) accu-\nracy with NLEP (PoT) prompting, while GPT-4\nModel\n# NLEP >Text\nDetail\n% Score\n% Length\nBias\nGPT-4\n23.75\nyes\n93.08\n72.72\nno\n105.06\n26.67\nGPT-3.5\n38.75\nyes\n101.22\n3.13\n-Turbo\nno\n102.50\n10.34\n0\n20\n40\n60\n80\n100\nNumber of Games\nGPT-4\n(detail)\nGPT-4\n(no detail)\nGPT-3.5-Turbo\n(detail)\nGPT-3.5-Turbo\n(no detail)\n12.5\n17.5\n70.0\n57.5\n28.75\n13.75\n56.25\n20.0\n23.75\n50.0\n27.5\n22.5\nComparison of Model Performances\nWin\nTie\nLose\nFigure 4: Automatic evaluations of NLEP against standard\nLLM-based generation with different models. # NLEP >Text\nmeans that the % of NLEP responses containing more tokens\nthan the baseline. Detail means if the evaluation metric con-\nsiders details and response lengths. Score stands for the scores\nreceived by NLEP divided by the baseline scores (>100 means\nNLEP is better). Win, tie, and lose stand for the % of evalua-\ntion cases resulting in each category. Length Bias shows how\nmuch the evaluation pipeline prefers longer or shorter answers\n(lower means fairer, introduced in Appendix F.3).\nimproves these figures to 91.6% (66.4%). It indi-\ncates that strong programming ability of underlying\nLLMs is vital to generate accurate responses and\nattain satisfactory performance. Surprisingly, zero-\nshot CodeLlama-7b (Rozière et al., 2023) trained\nusing NLEP-style data (without in-domain exam-\nples) demonstrates superiority on Tracking Shuf-\nfled Objects (7) benchmark over NLEP prompted\nGPT-3.5 and Word Sorting benchmark over task-\ngeneral CoT prompted GPT-3.5, even with signifi-\ncantly fewer parameters. It shows the potential for\neffective training of compact large language mod-\nels, enabling them to achieve performance levels\ncomparable to those of extremely large models.\n5\nRelated Work\nLarge language models for reasoning. State-of-\nthe-art LLMs (OpenAI, 2022, 2023; Touvron et al.,\n2023; Zeng et al., 2022) have shown very strong\nperformance on complicated reasoning tasks, in-\ncluding commonsense (Geva et al., 2021b), math\n(Cobbe et al., 2021), symbolic reasoning (Suzgun\net al., 2022), and programming (Austin et al., 2021;\nChen et al., 2021). Tackling such tasks with LLMs\noften requires prompting them with demonstrations\nthat elicit their reasoning capabilities. (Wei et al.,\n2022) proposed chain-of-thought prompting tech-\nnique that encourages language to generate answers\nstep-by-step. (Wang et al., 2022) found that self-\nconsistency can further improve the performance\nof chain of thoughts reasoning ability. (Kojima\net al., 2023) discovered that LLMs can perform rea-\nsoning without any demonstrations through adding\nthe incantation “Let’s think step-by-step\".\nTree of thoughts (Yao et al., 2023a) and graph of\nthoughts (Yao et al., 2023b; Besta et al., 2023)\nwere proposed to tackle tasks that require more\ncomplicated reasoning processes. These improved\nreasoning methods apply chain of thoughts as\nthe atomic reasoning step but organize reasoning\n“chains” through more advanced mechanisms.\nPrograms and tools. Previous studies have found\nthat some limitations of LLMs can be overcome\nby combining program synthesis techniques with\nprompt-based learning. Program of thoughts (Chen\net al., 2022) and program aided language model\n(Gao et al., 2023) both translate mathematical ques-\ntions to equations and use the python interpreter\nto ensure the correctness of the calculations. An-\nother line of related work for enabling LLMs to use\ntools is through interleaving API calls during LLM\ngeneration (Qin et al., 2023; Liang et al., 2023b;\nMialon et al., 2023; Tang et al., 2023). APIs can aid\nmany tasks that are challenging for LLMs by pro-\nviding tailored tools (e.g., calculators, search) that\ncan solve specific tasks. Toolformer (Schick et al.,\n2023) addresses reasoning tasks by using prede-\nfined tools, and LLMs as tool makers (LATM) can\nimplement functions solving a class of tasks based\non few-shot examples (Cai et al., 2023). With these\nsolutions, the correctness of a prediction can be en-\nsured if correct API is called and correct inputs are\nselected. Existing works on combining program\nsynthesis and tool usage with LLMs generally rely\non task-specific prompts, in contrast to the more\ntask-general prompt explored in the present work.\n6\nConclusion\nThis work describes natural language embedded\nprograms (NLEP), which flexibly combine natural\nlanguage reasoning with program synthesis within\nprompt-based learning to tackle a variety of tasks.\nOur experiments demonstrate that NLEPs expand\nthe scope of applications that can be addressed by\nprogram synthesis by more closely incorporating\nnatural language during code generation.\nAcknowledgement\nThis research was supported by the Center for Per-\nceptual and Interactive Intelligence (CPII) Ltd un-\nder the Innovation and Technology Commission’s\nInnoHK Scheme.\nLimitation\nWe found that the NLEP prompts are not suit-\nable for generating long-form natural language re-\nsponses. Experimental results on VicunaQA show\nthat most responses generated by NLEP prompting\nhave fewer tokens than responses obtained from\nusual LLM generation. This feature is expected, be-\ncause most naturally-occurring programs (on which\nthe LLMs were pretrained) do not contain large\nchunks of natural language. Future work could\nconsider incorporating (potentially synthetically\ngenerated) programs with longer-form natural lan-\nguage within the pretraining set to enable the ap-\nplication of NLEP to more involved NLG tasks. In\nthe context of prompt-based approaches without\nparameter adjustments, the design of prompts can\naffect performance. While we aimed to mitigate\nthis issue by expanding the scope of evaluation\nbenchmarks and minimizing random factors, we\ndid not engage in extensive prompt variation analy-\nsis. Further exploration into the impact of prompt\nvariations remains an avenue for enhancing the ro-\nbustness of reasoning approaches.\nEthical Statement\nThis work intends to design a accurate and inter-\npretable reasoning framework for language that en-\ntails more transparent and responsible LLM appli-\ncations. However, the program generation method\nis more capable to handle different tasks in both\nareas of natural and program languages, infecting\nboth humans and computing systems. As a re-\nsult, we believe program generation models need\nstronger alignment and careful management.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\ning elaborate problems with large language models.\narXiv preprint arXiv:2308.09687.\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\nand Denny Zhou. 2023. Large language models as\ntool makers. arXiv preprint arXiv:2305.17126.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022.\nProgram of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\net al. 2022. Binding language models in symbolic\nlanguages. In The Eleventh International Conference\non Learning Representations.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nCleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-\nhard Stolle, and Daniel G. Bobrow. 2003. Entailment,\nintensionality and text understanding. In Proceedings\nof the HLT-NAACL 2003 Workshop on Text Meaning,\npages 38–45.\nOna de Gibert, Naiara Perez, Aitor García-Pablos, and\nMontse Cuadros. 2018. Hate speech dataset from\na white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2),\npages 11–20, Brussels, Belgium. Association for\nComputational Linguistics.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng,\nand Tushar Khot. 2023. Chain-of-thought hub: A\ncontinuous effort to measure large language models’\nreasoning performance.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. Pal: Program-aided language\nmodels. In International Conference on Machine\nLearning, pages 10764–10799. PMLR.\nJiaxin Ge, Hongyin Luo, Yoon Kim, and James Glass.\n2023.\nEntailment as robust self-learner.\narXiv\npreprint arXiv:2305.17197.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021a. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021b. Did Aris-\ntotle Use a Laptop? A Question Answering Bench-\nmark with Implicit Reasoning Strategies. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nYi Hu, Haotong Yang, Zhouchen Lin, and Muhan Zhang.\n2023. Code prompting: a neural symbolic method\nfor complex reasoning in large language models.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nZiyang Li, Jiani Huang, and Mayur Naik. 2023. Scallop:\nA language for neurosymbolic programming. Pro-\nceedings of the ACM on Programming Languages,\n7(PLDI):1463–1487.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming\nGong, and Nan Duan. 2023a. TaskMatrix.AI: Com-\npleting Tasks by Connecting Foundation Models with\nMillions of APIs. arXiv:2303.16434.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming\nGong, and Nan Duan. 2023b. Taskmatrix.ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023.\nFaithful chain-of-\nthought reasoning.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Jus-\ntifying recommendations using distantly-labeled re-\nviews and fine-grained aspects. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 188–197.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2021. Show your work: Scratchpads for intermediate\ncomputation with language models.\nAbiola Obamuyide and Andreas Vlachos. 2018. Zero-\nshot relation classification as textual entailment. In\nProceedings of the First Workshop on Fact Extraction\nand VERification (FEVER), pages 72–78, Brussels,\nBelgium. Association for Computational Linguistics.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Yang Wang. 2023. Logic-lm: Empower-\ning large language models with symbolic solvers\nfor faithful logical reasoning.\narXiv preprint\narXiv:2305.12295.\nBhargavi Paranjape, Scott Lundberg anbd Sameer Singh,\nHannaneh\nHajishirzi,\nLuke\nZettlemoyer,\nand\nMarco Tulio Ribeiro. 2023. ART: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels. arXiv:2303.09014.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish\nBhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-\nhan Xiong, Alexandre Défossez, Jade Copet, Faisal\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\nThomas Scialom, and Gabriel Synnaeve. 2023. Code\nllama: Open foundation models for code.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A Smith, and Yejin Choi. 2019. Social bias\nframes: Reasoning about social and power implica-\ntions of language. arXiv preprint arXiv:1911.03891.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3687–3697, Brussels, Belgium. Association\nfor Computational Linguistics.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\net al. 2023. Beyond the imitation game: Quanti-\nfying and extrapolating the capabilities of language\nmodels.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, , and Jason Wei. 2022. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\narXiv preprint arXiv:2210.09261.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca:\nGeneralized tool learning for language models with\n3000 simulated cases.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nYao Yao, Zuchao Li, and Hai Zhao. 2023b.\nBe-\nyond chain-of-thought, effective graph-of-thought\nreasoning in large language models. arXiv preprint\narXiv:2305.16582.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nA\nAdditional Examples\nA detailed comparison between NLEP and\nChatGPT-4 is shown in Figure 5. An example of\ngenerated NLEP for StrategyQA problem is out-\nlined in Figure 6.\nB\nAdditional Results and Analysis on\nMath and Symbolic Reasoning\nWe present the detailed experimental results of\nmath and symbolic reasoning tasks in Tables 8 to\n9, with execution failure analysis in Table 11. The\nsignificance test analysis is outlined in Table 10.\nGPT Results. We report the results of Table 2\nwith more details in Table 8. The effect of extra re-\ntries described in Section 4 is highlighted with (→).\nThe detailed experimental results on the Game of\n24 benchmark is listed in Table 9.\nSignificance Test. We report the significance\ntest in Table 10 for task-general PoT and NLEP\nprompted GPT-4 results. Due to API cost hamper,\nwe run significance test using bootstrap test:\nhttps://github.com/neubig/util-scripts/\nblob/master/paired-bootstrap.py.\nThe\nhyperparameters are configured to the default set-\nting: num_samples=10000, sample_ratio=0.5\nBesides, to make the results more reliable and\nreproducible, we performed the major experiments\nusing a temperature of 0 as reported in Table 8,\nand assessed NLEP across a diverse range of tasks\nto show its general capabilities.\nResults with Different Fundation Models. To\ninvestigate the effect of NLEP on different large\nlanguage models, we report the results with Claude-\n22 and CodeLlama (Rozière et al., 2023) in Ta-\nble 7. Following the guidance of the instruction-\nfollowing models3, we employ a chat session to\ninclude task-specific and task-general prompts as\nprevious turns by interleaving the “user\" and “assis-\ntant\" messages with a system message “Provide an-\nswers in Python\" at the beginning for CodeLlama-\n7b-Instruct and CodeLlama-13b-Instruct. Hence,\nwe only treat bug-free Python programs that have\nthe desired results after execution as correct an-\nswers, regardless of natural language outputs since\nwe explicitly prompt CodeLlama to generate the an-\nswer in Python. Unlike the prominent performance\nof GPT-4, the positive impact of NLEP with CodeL-\nlama is diminished due to the much smaller model\nsize and greatly reduced programming capability.\n2https://www.anthropic.com/api\n3https://github.com/facebookresearch/codellama\nAlthough NLEP prompting outperforms the task-\ngeneral PoT by a large margin on Chinese Remain-\nder Theorem and Scheduling Meeting benchmarks,\na non-negligible performance gap is observed be-\ntween NLEP and task-specific PoT on most tasks.\nWe notice a decline or performance on Scheduling\nMeeting bechmark of 13b model over 7b. Interest-\ningly, we found that CodeLlama-13b prefers to use\ntools (built-in library) more for solving Scheduling\nMeeting problems while CodeLlama-7b tends to\ndo it directly “by hand”. However, the tool use of a\nless-proficient model may result in more possible\nexecution errors. For instance, 113 over 125 exe-\ncution errors (total 250 test cases) of CodeLlama-\n13b is Error:\n‘datetime.time’ object has\nno attribute ‘overlaps’ while CodeLlama-7b\nonly has 3 execution errors. The performance with\nClaude-2 is more prominent, with NLEP demon-\nstrates superior performance on Dyck Language,\nScheduling Meeting and Chinese Remainder Theo-\nrem benchmarks.\nTo further investigate the benefits of NLEP, we\nfine-tune a CodeLlama-7b (Rozière et al., 2023)\nmodel using NLEP-style instances, resulting in\na variant that we term NLEP-CodeLlama. Note\nthat our training corpus does not include specific\nevaluation tasks.\nDuring the evaluation phase,\nwe adopt zero-shot prompting strategy, where the\nmodel is provided with only test instances without\nin-context demonstrations. As presented in Table\n7(b), zero-shot NLEP-CodeLlama exhibits consis-\ntent performance improvement on 5 of 6 tasks. The\nonly exception is the Chinese Remainder Theo-\nrem benchmark, which is notably more complex in\nnature. Remarkably, zero-shot NLEP-CodeLlama\ndemonstrates superior performance on Word Sort-\ning benchmark when compared to task-general CoT\nprompted GPT-3.5-Turbo, and outperforms NLEP\nprompted GPT-3.5-Turbo on Tracking Shuffled Ob-\njects (7) benchmark, despite a considerably lower\nparameter size.\nLimitation of GSM-Hard Benchmark. We\nopted for GSM-Hard (Gao et al., 2023) to mitigate\npotential data contamination from GSM8K. The\nnumbers in GSM-Hard are large, less likely to have\nbeen encountered by LLMs during training, and\nhence augmenting the task complexity. However,\nsince GSM-Hard is automatically constructed by\nreplacing small numbers in GSM-8K with large\nones, there are peculiar cases where sometimes\nmodels may refuse to reply or automatically solve\nit. An example is as follows: the original ques-\nTracking Shuffled\nObjects (7)\nDyck\nLanguage\nWord\nSorting\nChinese\nRemainder Theorem\nScheduling\nMeeting\nGSM-Hard\n(a) Task-Specific Prompting: GPT-4\nCoT\n100.0\n63.6†\n90.9†\n0.0†\n55.6†\n57.4\nPoT\n100.0\n60.8\n100.0\n100.0\n75.2\n74.1\nLATM\n100.0\n87.5†\n99.1†\n100.0†\n100.0†\n-\n(b) Task-General Prompting: GPT-4\nCoT\n81.2\n39.6\n84.4\n0.0\n82.8\n54.9\nPoT\n98.4\n66.4\n99.6\n76.4 (→84.4)\n84.4 (→85.2)\n69.3\nNLEP (Ours)\n100.0\n91.6\n99.6\n85.2 (→97.2)\n77.6 (→93.2)\n67.7\n(c) Task-Specific Prompting: GPT-3.5-Turbo\nCoT\n68.0\n20.4†\n59.2†\n0.0†\n18.9†\n45.0\nPoT\n6.8\n28.4\n100.0\n100.0\n33.6\n63.4\n(d) Task-General Prompting: GPT-3.5-Turbo\nCoT\n51.2\n38.0\n75.2\n0.0\n39.6\n42.8\nPoT\n88.4\n4.0\n100.0\n58.4 (→72.4)\n46.8 (→49.2)\n39.0 (→52.2)\nNLEP (Ours)\n74.4\n7.2\n99.6\n94.8 (→96.4)\n75.2 (→85.6)\n50.9 (→54.1)\nTable 8: Performance on six reasoning tasks. † stands for results from LATM (Cai et al., 2023). Results with † and of LATM are\nreported on the last 240 instances with the first 10 instances as training and validation sets for tool making according to LATM’s\ndesign. LATM is not appropriate for GSM-Hard benchmark as it is hard to derive a generally applicable tool function for all\ntest cases. We mainly experiment LATM with GPT-4 as the tool maker since (Cai et al., 2023) found that GPT-3.5 fails in all 5\ntrials on hard tasks like Tracking Shuffled Objects (5). If the generated tool is not general enough or only suitable for training\nsamples, the tool using phase will fail. We perform experiments using GPT-4 and GPT-3.5-Turbo with a temperature of 0 for all\nsettings except PoT and NLEP on GSM-Hard in (b) which use a temperature of 0.5 to increase the sampling diversity. Since\ntask-general PoT and NLEP lack task-specific programming instruction, they may generate non-executable Python programs.\nWe select some settings and give each instance failed at execution up to three additional trials with temperature=0.4 to diversify\nthe possible outputs. No label leakage is involved in this process as only the success of execution is used as a judgement. We\nreport the results with these extra retries on execution failures in (→). The highest score among each sub-table (a), (b), (c) and\n(d) is underlined and the best result for each task is in bold.\nsetting\nTask-Specific\nTask-General\nIO\nCoT\nIO\n(best of 100)\nCoT\n(best of 100)\nToT\n(b=1)\nToT\n(b=5)\nPoT\nNLEP (ours)\nGame of 24\n7.3†\n4.0†\n33.0†\n49.0†\n45.0†\n74.0†\n52 (→52)\n63 (→66)\nTable 9: Performance on Game of 24 benchmark. † stands for results from (Yao et al., 2023a).\ntion in GSM8K is “Claire makes a 3 egg omelet\nevery morning for breakfast. How many dozens\nof eggs will she eat in 4 weeks?” The number 3\nis replaced with 6022727 and the corresponding\nquestion in GSM-Hard is “Claire makes a 6022727\negg omelet every morning for breakfast. How many\ndozens of eggs will she eat in 4 weeks?” The output\nof task-general PoT aligns with the ground-truth,\nwhich is 6022727 × 7 × 4/12 = 14053029.667,\na decimal number. However, GPT-4 with NLEP\nprompting regards the number of dozens should\nbe an integer, and consequently returns the result\nas 6022727 × 7 × 4//12 = 14053029. This gives\nadditional challenges in evaluation, and we thus\nmainly evaluate the former five reasoning bench-\nmarks using more models in Table 7, which might\nserve as more reliable performance indicators.\nExecution Failure Analysis. We present the\nexecution failure statistics of code-based reasoning\nstrategies in Table 11. The effect of extra retries de-\nscribed in Section 4 is highlighted with (→). Note\nthat different from task-specific PoT with demon-\nstrations showing how to return the desired outputs\nin Python program, e.g.,\n# Python code ,\nr e t u r n\nans\nAlice = \" s t r i k e r \"\nBob = \" r i g h t\nwinger \"\nC l a i r e = \" l e f t\nwinger \"\nDave = \" benchwarmer \"\nEve = \" goalkeeper \"\nFred = \" c e n t e r\nm i d f i e l d e r \"\nGertrude = \" c h e e r l e a d e r \"\nEve ,\nC l a i r e = Claire ,\nEve\nGertrude ,\nAlice = Alice ,\nGertrude\nFred ,\nBob = Bob ,\nFred\nDave ,\nFred = Fred ,\nDave\nFred ,\nBob = Bob ,\nFred\nBob ,\nEve = Eve ,\nBob\nClaire ,\nAlice = Alice ,\nC l a i r e\nans = Gertrude\nTasks/Evaluation\nAccuracy\nSignificance Test\nPoT\nNLEP\nNLEP win/lose/tie\n95% Confidence Interval\nP-value\nTracking Shuffled Objects (7)\n98.4\n100.0\n86.8/0.0/13.2\n[1.000,1.000]\n[0.960,1.000]\n0.132\nDyck Languages\n66.4\n91.6\n100.0/0.0/0.0\n[0.864,0.960]\n[0.576,0.744]\n0.000\nWord Sorting\n99.6\n99.6\n0.0/0.0/100.0\n[0.984,1.000]\n[0.984,1.000]\n/\nChinese Remainder Theorem\n84.4\n97.2\n100.0/0.0/0.0\n[0.944,1.000]\n[0.776,0.904]\n0.000\nScheduling Meeting\n85.2\n93.2\n98.5/0.7/0.8\n[0.888,1.976]\n[0.784,0.912]\n0.015\nGSM-Hard\n69.3\n67.7\n0.92/88.8/20.0\n[0.642,0.712]\n[0.657,0.728]\nPoT better 0.112\nTable 10: We report the significance test results for task-general PoT and NLEP prompted GPT-4 in Table 2. Due to\nAPI cost hamper, we run significance test using bootstrap test: https://github.com/neubig/util-scripts/blob/master/\npaired-bootstrap.py. The hyperparameters are configured to the default setting: num_samples=10000, sample_ratio=0.5.\nGPT-4\nGPT-3.5-Turbo\nTask-Specific\nTask-General\nTask-Specific\nTask-General\nPoT\nPoT\nNLEP\nPoT\nPoT\nNLEP\nTrack Shuffled Objects (7)\n0\n0\n0\n233\n26\n24\nDyck Language\n16\n24\n10\n32\n81\n26\nWord Sort\n0\n0\n0\n0\n0\n0\nChinese Remainder Theorem\n0\n32 (→0)\n37 (→6)\n0\n46 (→7)\n4 (→0)\nScheduling Meeting\n0\n3 (→0)\n43 (→0)\n2\n15 (→2)\n36 (→0)\nGSM-Hard\n17\n6\n8\n31\n464 (→145)\n95 (→13)\nTable 11: Execution failure statistics on six math and symbolic reasoning tasks. Results with extra reties are reported in (→). For\ntask-specific PoT, we report the execution error statistics with None as the return value of safe_execute() function following\nthe source code of (Chen et al., 2022): https://github.com/wenhuchen/Program-of-Thoughts/blob/main/tool.py. It\nincludes instances where the generated programs do not contain the required line of code: ans={desired_result}, which are\nexplicitly required in the instruction and given in the prompt demonstration. Under this scenario, we cannot capture the execution\nresults of task-specific PoT.\nwe need to design rules to extract the target an-\nswers from the execution results of task-general\nPoT and NLEP since they are allowed to generate\nfree-from outputs. For example, given the gener-\nated programs,\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\n# No need\nto\nimport\n# Step\n2:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nt h a t\ng e n e r a l l y\nsolve\nt h i s\ntype\nof\nproblem\ndef\nswap_positions ( p o s i t i o n s ,\nswaps ) :\nf o r\nswap\nin\nswaps :\np o s i t i o n s [ swap [ 0 ] ] ,\np o s i t i o n s [ swap [ 1 ] ] =\np o s i t i o n s [ swap [ 1 ] ] ,\np o s i t i o n s [ swap [ 0 ] ]\nr e t u r n\np o s i t i o n s\n# Step\n3:\nDefine\nc o n s t a n t\nv a r i a b l e s\nf o r\nthe\nt a s k\np o s i t i o n s = {\n\" Alice \" :\n\" s t r i k e r \" ,\n\"Bob \" :\n\" r i g h t\nwinger \" ,\n\" C l a i r e \" :\n\" l e f t\nwinger \" ,\n\" Dave \" :\n\" benchwarmer \" ,\n\" Eve \" :\n\" goalkeeper \" ,\n\" Fred \" :\n\" c e n t e r\nm i d f i e l d e r \" ,\n\" Gertrude \" :\n\" c h e e r l e a d e r \"\n}\nswaps = [\n( \" Eve \" ,\n\" C l a i r e \" ) ,\n( \" Gertrude \" ,\n\" Alice \" ) ,\n( \" Fred \" ,\n\"Bob \" ) ,\n( \" Dave \" ,\n\" Fred \" ) ,\n( \" Fred \" ,\n\"Bob \" ) ,\n( \" Bob \" ,\n\" Eve \" ) ,\n( \" C l a i r e \" ,\n\" Alice \" )\n]\n# Step\n4:\nP r i n t\nan answer\nin\nn a t u r a l\nlanguage .\nf i n a l _ p o s i t i o n s\n= swap_positions ( p o s i t i o n s ,\nswaps )\np r i n t ( f \" At\nthe\nend\nof\nthe\nmatch ,\nGertrude\ni s\np l a y i n g\n{ f i n a l _ p o s i t i o n s [ ’ Gertrude ’ ] } . \" )\nwe need to extract “striker\", the target answer,\nfrom the execution results “At the end of the match,\nGertrude is playing striker.\".\nAlthough task-specific PoT explicitly instructs\nthe model to generate a required line of code\nans={desired_result} to capture the final result\nby adding\n# Python code ,\nr e t u r n\nans\nin the instruction and explicitly provides task-\nspecific examples in the prompt, e.g.,\n# Python code ,\nr e t u r n\nans\n. . .\n. . .\nans = Gertrude\nwe found that GPT models sometimes fail to\naccomplish the requirement. On Tracking Shuf-\nfled Objects (7) benchmark, only 17 programs\nout of 250 test cases generated by GPT-3.5 suc-\ncessfully contain the required line of code, i.e.,\nans={desired_result}, which explains the huge\nnumber of failures (233). In addition to already\nhaving task-specific programming examples, the\ninability to distinguish between the erroneous pro-\ngrams and lack of required line of code is another\nreason why we do not apply the error retries on\ntask-specific PoT.\nNLEP v.s. Task-general PoT Compared to PoT,\nNLEP offers structured and modularized Python\nblocks. This disentanglement of knowledge ex-\ntraction from problem solving facilitates enhanced\ninterpretability and controllability. In contrast, PoT\nmay interwine different components, particularly\nblending knowledge and reasoning steps. An il-\nlustrative example is provided below: the question\nfrom GSM-Hard is Toula went to the bakery and\nbought various types of pastries. She bought 3\ndozen donuts which cost 5804528 per dozen, 2\ndozen mini cupcakes which cost 80 per dozen, and\n6 dozen mini cheesecakes for 55 per dozen. How\nmuch was the total cost? The generated Python\nblock of NLEP is\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\n# No need\nto\nimport\n# Step\n2:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nt h a t\ng e n e r a l l y\nsolve\nt h i s\ntype\nof\nproblem\ndef\nc a l c u l a t e _ t o t a l _ c o s t ( items ) :\nt o t a l _ c o s t\n= 0\nf o r\nitem\nin\nitems :\nt o t a l _ c o s t\n+= item [ ’ q u a n t i t y ’ ]\n* item [ ’ price\n’ ]\nr e t u r n\nt o t a l _ c o s t\n# Step\n3:\nDefine\nc o n s t a n t\nv a r i a b l e s\nf o r\nthe\nt a s k\nitems = [\n{\" name \" :\n\" donuts \" ,\n\" q u a n t i t y \" :\n3 ,\n\" p r i c e \" :\n5804528} ,\n{\" name \" :\n\" mini\ncupcakes \" ,\n\" q u a n t i t y \" :\n2 ,\n\" p r i c e \" :\n80} ,\n{\" name \" :\n\" mini\ncheesecakes \" ,\n\" q u a n t i t y \" :\n6 ,\n\"\np r i c e \" :\n55} ,\n]\n# Step\n4:\nP r i n t\nan answer\nin\nn a t u r a l\nlanguage .\nt o t a l _ c o s t\n=\nc a l c u l a t e _ t o t a l _ c o s t ( items )\np r i n t ( f \" The\nt o t a l\nc o s t\nof\nthe\np a s t r i e s\nt h a t\nToula\nbought\nfrom\nthe\nbakery\ni s\n${ t o t a l _ c o s t } .\nThis\ni n c l u d e s\nthe\nc o s t\nof { items [ 0 ] [ ’ q u a n t i t y ’ ] }\ndozen { items [ 0 ] [ ’ name ’ ] }\na t\n${ items [ 0 ] [ ’ price\n’ ] }\nper\ndozen ,\n{ items [ 1 ] [ ’ q u a n t i t y ’ ] }\ndozen {\nitems [ 1 ] [ ’ name ’ ] }\na t\n${ items [ 1 ] [ ’ price ’ ] }\nper\ndozen ,\nand { items [ 2 ] [ ’ q u a n t i t y ’ ] }\ndozen { items\n[ 2 ] [ ’ name ’ ] }\na t\n${ items [ 2 ] [ ’ price ’ ] }\nper\ndozen\n. \\ nThe\nc o r r e c t\nanswer\ni s\n${ t o t a l _ c o s t } . \" )\nThe code returned by PoT is\ndonuts = 3 * 5804528\ncupcakes = 2 * 80\ncheesecakes = 6 * 55\nt o t a l _ c o s t\n= donuts + cupcakes + cheesecakes\nNum\ncola\nemotion\namazon\nhsd\nsbic\nInternal Node\n4\n7\n8\n4\n3\nLeaf Node\n4\n8\n10\n5\n4\nLabel\n2\n6\n5\n2\n3\nTable 12: Statistics of NLEP generated decision trees for each\nbenchmark in Table 6.\np r i n t ( t o t a l _ c o s t )\nans = Gertrude\nC\nAdditional Results on Text\nClassification\nStatistics of Decision Trees. We list the statics\nof the generated decision tree by NLEP prompting\nin Table 12. Each internal decision node in the\ngenerated tree has two branches, “yes” and “no” ,\nindicating whether it satisfies the requirement or\nnot.\nClassification Results on SST2 Benchmark.\nSince we use the trees derived from SST2 to prompt\nthe LLM for the classification tasks, it would be\ninappropriate to use these examples for SST2 itself.\nFor SST2, we thus use an automatically generated\ndecision tree for the CoLA task to prompt GPT-4\nto generate a new tree for SST2. As shown in Table\n13, the automatically generated tree matches the\nperformance of the SST2 decision tree created by\nthe authors.\nSST2 Performance\nModel-Free\n66.17\nRoBERTa-Manual\n83.03\nRoBERTa-Automatic\n87.36\nDeBERTa-Manual\n84.06\nDeBERTa-Automatic\n93.49\nTable 13: Performance of manually crafted vs. generated\ndecision trees on SST2.\n \n # Step 1: Import necessary built-in libraries\n # No need to import\n \n # Step 2: Define knowledge about the secretary-general of United Nations\n secretaries_general = {\n     \"Trygve Lie\":              \"Europe\",\n     \"Dag Hammarskjöld\":        \"Europe\",\n     \"U Thant\":                 \"Asia\",\n     \"Kurt Waldheim\":           \"Europe\",\n     \"Javier Pérez de Cuéllar\": \"South America\",\n     \"Boutros Boutros-Ghali\":   \"Africa\",\n     \"Kofi Annan\":              \"Africa\",\n     \"Ban Ki-moon\":             \"Asia\",\n     \"António Guterres\":        \"Europe\"\n }\n # Step 3: Define a function that counts the number of secretary-general not \nfrom Europe\n def count_non_european(secretaries_general):\n     return len([region for region in secretaries_general.values() if region !\n= \"Europe\"])\n # Step 4: Print an answer in natural language\n non_european_count = count_non_european(secretaries_general)\n print(f\"Out of all the Secretaries-General of the United Nations, \n{non_european_count} of them are not from Europe.\")\nTool\nUsing\nStructured\nknowledge\nFunction\ndefinition\nPrint\nanswer\nOut of all the Secretaries-General of the United Nations, 5 of them are not from Europe.\nStdout:\nInstruction:\nHow many secretaries-general of United Nations are not from Europe?\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\n100\n60\n40\nGPT-4: NLEP\nGPT-4: ChatGPT\nGPT-4: OpenAI API (T = 0.5)\nAnswering the question with\nfive independent generations\nNLEP generated by GPT-4:\nUser\nWrong answer\nFigure 5: NLEP answering a question which requires numeric reasoning of structured knowledge. ChatGPT-4 code interpreter\n(currently the advanced data analysis option) constantly prefers to answer this question with plain natural language.\n# Step 1: Import necessary built-in libraries\n# No need to import\n# Step 2: Define dictionaries storing the rappers' information\nrappers = {\n    \"Lil Wayne\": {\n        \"real_name\": \"Dwayne Michael Carter Jr.\",\n        \"grammy_awards\": 5\n    },\n    \"Jay-Z\": {\n        \"real_name\": \"Shawn Corey Carter\",\n        \"grammy_awards\": 23\n    }\n}\n# Step 3: Define a function that checks if a rapper has over quadruple the Grammy awards of another\ndef check_quadruple_awards(rappers, rapper1, rapper2):\n    if rappers[rapper2][\"grammy_awards\"] > 4 * rappers[rapper1][\"grammy_awards\"]:\n        return \"yes\"\n    else:\n        return \"no\"\n# Step 4: Print an answer using the defined function and varible\nanswer = check_quadruple_awards(rappers, \"Lil Wayne\", \"Jay-Z\")\nprint(answer)\nyes\n(b) StrategyQA: Lil Wayne similar real name rapper has over quadruple Wayne’s Grammy awards?\nStdout:\nNLEP generated by GPT-4\n… As of now, Lil Wayne has won 5 Grammy awards, while Lil Twist has not won any. Thus, \nLil Twist does not have over quadruple Wayne’s Grammy awards. So the answer is no.\nFew-shot CoT:\nFigure 6: NLEP generated for solving StrategyQA problem. The instruction is “Answer the question with yes or no.”\nD\nPrompts for Task-General Strategies\nD.1\nPrompts for Table 2 and 4\nWe list the prompts for the task-general chain-of-\nthought (CoT), our implementation of program-of-\nthoughts (PoT) and the proposed natural language\nembedded programs (NLEP) strategies in the fol-\nlowing code segments. They share the same two\nNLEP examples (one is for natural language rea-\nsoning and the other is for mathematical reasoning)\nbut with different forms of intermediate reasoning\nsteps (e.g., code, text etc.) to evaluate the average\nperformance of different strategies.\nPrompt for task-general chain-of-thought\n(CoT) in Table 2 and 4. The detailed intermediate\nnatural language reasoning chains are generated by\nprompting GPT-4 given the input and target output.\n’ ’ ’\nAnswer\nthe\nproblem\nbased on\nthe\ngiven\ni n s t r u c t i o n\nand\ni n p u t .\n###\nI n s t r u c t i o n :\nI d e n t i f y\nthe\nodd one\nout .\n###\nInput :\nTwitter ,\nInstagram ,\nTelegram\n### Answer :\nLet ’ s\nt h i n k\ns t e p\nby\ns t e p .\n1 .\nS t a r t\nby\nu n d e r s t a n d i n g\nthe\nt a s k\ni n s t r u c t i o n .\nThe\nt a s k\ni s\nto\ni d e n t i f y\nthe\nodd one\nout\nfrom a\ngiven\nl i s t .\n2 .\nLook\na t\nthe\ni n p u t .\nThe\ni n p u t\nc o n s i s t s\nof\nt h r e e\nitems :\nTwitter ,\nInstagram ,\nand Telegram .\n3.\nI d e n t i f y\nwhat\nt h e s e\nitems\nare .\nT w i t t e r\nand\nInstagram\nare\ns o c i a l\nmedia\nplatforms ,\nwhile\nTelegram\ni s\na messaging\napp .\n4 .\nCompare\nthe\nitems\nto\nf i n d\nthe\nodd one\nout .\nT w i t t e r\nand\nInstagram\nare\np r i m a r i l y\nused\nf o r\ns h a r i n g\ninformation ,\nimages ,\nand\nvideos . On the\no t h e r\nhand ,\nTelegram\ni s\nmainly\nused\nf o r\ni n s t a n t\nmessaging\nand\nvoice −over −IP\ns e r v i c e .\n5 .\nDetermine\nthe\nodd one\nout\nbased on\nthe\ncomparison\n.\nIn\nt h i s\ncase ,\nTelegram\ni s\nthe\nodd one\nout\nbecause\ni t\ns e r v e s\na\nd i f f e r e n t\nprimary\nf u n c t i o n\ncompared\nto\nT w i t t e r\nand\nInstagram .\n6.\nFormulate\nthe\nt a r g e t\noutput .\nThe\nt a r g e t\noutput\nshould\nc l e a r l y\ns t a t e\nt h a t\nTelegram\ni s\nthe\nodd one\nout\nand\nprovide\nthe\nreason why\ni t\ni s\nso .\nThe\nreason\nbeing\nt h a t\nT w i t t e r\nand\nInstagram\nare\ns o c i a l\nmedia\np l a t f o r m s\nmainly\nf o r\ns h a r i n g\ninformation ,\nimages ,\nand\nvideos\nwhile\nTelegram\ni s\na cloud −\nbased\ni n s t a n t\nmessaging\nand\nvoice −over −IP\ns e r v i c e .\nThe\nc o r r e c t\nanswer\ni s\nTelegram .\n###\nI n s t r u c t i o n :\nUse\nthe\ngiven\ndata\nto\nc a l c u l a t e\nthe\nmedian .\n###\nInput :\n[2 ,\n3 ,\n7 ,\n8 ,\n10]\n### Answer :\nLet ’ s\nt h i n k\ns t e p\nby\ns t e p .\n1.\nS t a r t\nby\nu n d e r s t a n d i n g\nthe\ntask ,\nwhich\ni s\nto\nc a l c u l a t e\nthe\nmedian\nof\na\ngiven\ndata\ns e t .\nThe median\ni s\nthe\nmiddle\nvalue\nin\na\nsorted ,\nascending\nor\ndescending ,\nl i s t\nof\nnumbers .\n2.\nLook\na t\nthe\ngiven\ninput ,\nwhich\ni s\na\nl i s t\nof\nnumbers :\n[2 ,\n3 ,\n7 ,\n8 ,\n1 0 ] .\n3.\nNotice\nt h a t\nthe\nl i s t\ni s\na l r e a d y\ns o r t e d\nin\nascending\norder .\nI f\ni t\nwasn ’\nt ,\nthe\nf i r s t\ns t e p\nwould be\nto\ns o r t\ni t .\n4.\nUnderstand\nt h a t\nto\nf i n d\nthe\nmedian , we need\nto\nf i n d\nthe\nmiddle\nvalue .\nI f\nthe\nl i s t\nhas\nan odd number\nof\no b s e r v a t i o n s ,\nthe\nmedian\ni s\nthe\nmiddle\nnumber .\nI f\nthe\nl i s t\nhas\nan even number\nof\no b s e r v a t i o n s ,\nthe\nmedian\ni s\nthe\naverage\nof\nthe\ntwo middle\nnumbers .\n5 .\nCount\nthe\nnumber\nof\nvalu es\nin\nthe\nl i s t .\nThere\nare\n5 values ,\nwhich\ni s\nan odd number ,\nso\nthe\nmedian\nw i l l\nbe\nthe\nmiddle\nvalue .\n6 .\nI d e n t i f y\nthe\nmiddle\nvalue .\nSince\nt h e r e\nare 5\nvalues ,\nthe\nmiddle\nvalue\ni s\nthe 3 rd\nvalue when counting\nfrom\ne i t h e r\nend .\n7 .\nFind\nthe 3 rd\nvalue\nin\nthe\nl i s t ,\nwhich\ni s\n7 .\n8 .\nConclude\nt h a t\nthe\nmedian\nof\nthe\ngiven\ndata\ns e t\ni s\n7 .\nThe\nc o r r e c t\nanswer\ni s\n7 .\n’ ’ ’\nPrompt for task-general NLEP in Table 2 and\n4. The intermediate program reasoning chains are\ngenerated by prompting GPT-4 given the input and\ntarget output.\nWrite a bug−f r e e\nPython\nprogram\nt h a t\ncan\ng e n e r a t e\nthe\nanswer\nto\nthe\ngiven\ni n s t r u c t i o n\nwhen\nc o r r e c t l y\nexecuted .\n###\nI n s t r u c t i o n :\nI d e n t i f y\nthe\nodd one\nout .\n###\nInput :\nTwitter ,\nInstagram ,\nTelegram\n### Python\nprogram :\n‘ ‘ ‘\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\nfrom\nc o l l e c t i o n s\nimport\nOrderedDict\n# Step\n2:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nt h a t\ng e n e r a l l y\nsolve\nt h i s\ntype\nof\nproblem\ndef\nfind_odd_one_out ( s e r v i c e s ,\ni n p u t _ s e r v i c e s ) :\nd e s c r i p t i o n s\n= [ s e r v i c e s [ s e r v i c e ]\nf o r\ns e r v i c e\nin\ni n p u t _ s e r v i c e s ]\nf o r\nd e s c r i p t i o n\nin\nd e s c r i p t i o n s :\ni f\nd e s c r i p t i o n s . count ( d e s c r i p t i o n ) == 1:\nr e t u r n\ni n p u t _ s e r v i c e s [ d e s c r i p t i o n s . index\n( d e s c r i p t i o n ) ]\nr e t u r n\nNone\n# Step\n3:\nDefine\nc o n s t a n t\nv a r i a b l e s\nf o r\nthe\nt a s k\ns e r v i c e s = OrderedDict ( [\n( \" T w i t t e r \" ,\n\" a\ns o c i a l\nmedia\np l a t f o r m\nmainly\nf o r\ns h a r i n g\ninformation ,\nimages and\nvideos \" ) ,\n( \" Instagram \" ,\n\" a\ns o c i a l\nmedia\np l a t f o r m\nmainly\nf o r\ns h a r i n g\ninformation ,\nimages and\nvideos\n\" ) ,\n( \" Telegram \" ,\n\" a cloud −based\ni n s t a n t\nmessaging\nand\nvoice −over −IP\ns e r v i c e \" ) ,\n] )\ni n p u t _ s e r v i c e s = [ \" T w i t t e r \" ,\n\" Instagram \" ,\n\" Telegram\n\" ]\n# Step\n4:\nP r i n t\nan answer\nin\nn a t u r a l\nlanguage .\nodd_one_out = find_odd_one_out ( s e r v i c e s ,\ni n p u t _ s e r v i c e s )\ni f\nodd_one_out :\no t h e r _ s e r v i c e s = [ s e r v i c e\nf o r\ns e r v i c e\nin\ni n p u t _ s e r v i c e s\ni f\ns e r v i c e\n!=\nodd_one_out ]\np r i n t ( f \" The odd one\nout\ni s\n{ odd_one_out } .\n{\no t h e r _ s e r v i c e s [ 0 ] }\nand { o t h e r _ s e r v i c e s [ 1 ] }\nare\n{ s e r v i c e s [ o t h e r _ s e r v i c e s [ 0 ] ] }\nwhile {\nodd_one_out }\ni s\n{ s e r v i c e s [ odd_one_out ] } . \\\nnThe\nc o r r e c t\nanswer\ni s\n{ odd_one_out } . \" )\n‘ ‘ ‘\n###\nI n s t r u c t i o n :\nUse\nthe\ngiven\ndata\nto\nc a l c u l a t e\nthe\nmedian .\n###\nInput :\n[2 ,\n3 ,\n7 ,\n8 ,\n10]\n### Python\nprogram :\n‘ ‘ ‘\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\n# No need\nto\nimport\n# Step\n2:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nt h a t\ng e n e r a l l y\nsolve\nt h i s\ntype\nof\nproblem\ndef\nc a l c u l a t e _ m e d i a n ( data ) :\ndata . s o r t ( )\nl e n g t h = len ( data )\ni f\nl e n g t h % 2 == 0:\nr e t u r n\n( data [ l e n g t h / / 2 ]\n+ data [ l e n g t h / / 2\n−\n1 ] )\n/\n2\ne l s e :\nr e t u r n\ndata [ l e n g t h / / 2 ]\n# Step\n3:\nDefine\nc o n s t a n t\nv a r i a b l e s\nf o r\nthe\nt a s k\ndata = [2 ,\n3 ,\n7 ,\n8 ,\n10]\n# Step\n4:\nP r i n t\nan answer\nin\nn a t u r a l\nlanguage .\nmedian = c a l c u l a t e _ m e d i a n ( data )\np r i n t ( f \"To f i n d\nthe\nmedian\nof\na\ndata\nset , we need\nto\nar r a n g e\nthe\ndata\nin\nascending\norder\nand\nthen\nf i n d\nthe\nmiddle\nvalue .\nIn\nt h i s\ncase ,\nthe\ngiven\ndata\ni s\na l r e a d y\narranged\nin\nascending\norder .\nSince\nt h e r e\nare\n{ len ( data ) }\nvalues\nin\nthe\ndata\nset ,\nthe\nmedian\nw i l l\nbe\nthe\nmiddle\nvalue ,\nwhich\ni s\nthe\n{ len ( data ) / / 2\n+ 1} rd\nvalue .\nHence ,\nthe\nmedian\nof\nthe\ngiven\ndata\ns e t\ni s\n{ median } . \\ nThe\nc o r r e c t\nanswer\ni s\n{ median } . \" )\n‘ ‘ ‘\nPrompt for our task-general program-of-\nthoughts (PoT) implementation in Table 2 and\n4. The intermediate program reasoning chains are\nobtained from NLEP. We keep the key implemen-\ntation steps of Python programs but eliminate the\nstep-by-step programming instructions and detailed\nnatural language elaboration as an ablation.\nWrite a bug−f r e e\nPython\nprogram\nt h a t\ncan\ng e n e r a t e\nthe\nanswer\nto\nthe\ngiven\ni n s t r u c t i o n\nwhen\nc o r r e c t l y\nexecuted .\n###\nI n s t r u c t i o n :\nI d e n t i f y\nthe\nodd one\nout .\n###\nInput :\nTwitter ,\nInstagram ,\nTelegram\n### Python\nprogram :\n‘ ‘ ‘\ns o f t w a r e s = {\n’ s o c i a l\nmedia ’ :\n[ ’ t w i t t e r ’ ,\n’ instagram ’ ] ,\n’ communication ’ :\n[ ’ telegram ’ ]\n}\nf o r\ngenre ,\napps\nin\ns o f t w a r e s . items ( ) :\ni f\nlen ( apps ) == 1:\np r i n t ( apps [ 0 ] )\n‘ ‘ ‘\n###\nI n s t r u c t i o n :\nUse\nthe\ngiven\ndata\nto\nc a l c u l a t e\nthe\nmedian .\n###\nInput :\n[2 ,\n3 ,\n7 ,\n8 ,\n10]\n### Python\nprogram :\n‘ ‘ ‘\ndata = [2 ,\n3 ,\n7 ,\n8 ,\n10]\ndata . s o r t ( )\nl e n g t h = len ( data )\ni f\nl e n g t h % 2 == 0:\np r i n t ( ( data [ l e n g t h / / 2 ]\n+ data [ l e n g t h / / 2\n−1 ] )\n/\n2)\ne l s e :\np r i n t ( data [ l e n g t h / / 2 ] )\n‘ ‘ ‘\nD.2\nPrompts for Table 3\nThe Game of 24 task is much more challenging and\nwe replace the first example in Appendix D.1 with\na word sorting example to elicit stronger reasoning\nability.\nPrompt for task-general NLEP in Table 3.\nThe intermediate program reasoning chains are gen-\nerated by prompting GPT-4 given the input and\ntarget output.\nWrite a bug−f r e e\nPython\nprogram\nt h a t\ncan\ng e n e r a t e\nthe\nanswer\nto\nthe\ngiven\ni n s t r u c t i o n\nwhen\nc o r r e c t l y\nexecuted .\n###\nI n s t r u c t i o n :\nArrange\nthe\nf o l l o w i n g\nwords\nto make\nthe\nl o n g e s t\np o s s i b l e\nword .\n###\nInput :\nthe ,\nhad ,\nnot ,\nbeen\n### Python\nprogram :\n‘ ‘ ‘\n#\nS e c t i o n\n1:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nand\nc a l c u l a t e\ni n t e r m e d i a t e\nv a r i a b l e s\ndef\nlongest_word ( words ) :\nfrom\ni t e r t o o l s\nimport\np e r m u t a t i o n s\nall_words = [ ’ ’ . j o i n ( p )\nf o r\np in\np e r m u t a t i o n s\n( ’ ’ . j o i n ( words ) ) ]\nall_words . s o r t ( key=len ,\nr e v e r s e =True )\nwith\nopen ( ’ english_words . txt ’ )\nas\nw o r d _ f i l e :\n#\nAssuming you have a\nl i s t\nof\ne n g l i s h\nwords\nenglish_words = s e t ( word . s t r i p ( ) . lower ( )\nf o r\nword\nin\nw o r d _ f i l e )\nf o r\nword\nin\nall_words :\ni f\nword . lower ( )\nin\nenglish_words :\nr e t u r n\nword\nr e t u r n\nNone\n#\nS e c t i o n\n2:\nDefine\nc o n s t a n t\nv a r i a b l e s\nwords = [ \" the \" ,\n\" had \" ,\n\" not \" ,\n\" been \" ]\n#\nS e c t i o n\n3:\nI n s e r t\nv a r i a b l e s\nin\nt e x t\no u t p u t s\nusing\nf −s t r i n g s .\nl o n g e s t = longest_word ( words )\ni f\nl o n g e s t :\np r i n t ( f \" The\nl o n g e s t\nword\nt h a t\ncan be made from\nthe\nl e t t e r s\nin\nthe\nwords\n\\ \" { ’ ,\n’ . j o i n ( words\n) } \\ \"\ni s\n\\ \" { l o n g e s t } \\ \" . \" )\n‘ ‘ ‘\n###\nI n s t r u c t i o n :\nUse\nthe\ngiven\ndata\nto\nc a l c u l a t e\nthe\nmedian .\n###\nInput :\n[2 ,\n3 ,\n7 ,\n8 ,\n10]\n### Python\nprogram :\n‘ ‘ ‘\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\n# No need\nto\nimport\n# Step\n2:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nt h a t\ng e n e r a l l y\nsolve\nt h i s\ntype\nof\nproblem\ndef\nc a l c u l a t e _ m e d i a n ( data ) :\ndata . s o r t ( )\nl e n g t h = len ( data )\ni f\nl e n g t h % 2 == 0:\nr e t u r n\n( data [ l e n g t h / / 2 ]\n+ data [ l e n g t h / / 2\n−\n1 ] )\n/\n2\ne l s e :\nr e t u r n\ndata [ l e n g t h / / 2 ]\n# Step\n3:\nDefine\nc o n s t a n t\nv a r i a b l e s\nf o r\nthe\nt a s k\ndata = [2 ,\n3 ,\n7 ,\n8 ,\n10]\n# Step\n4:\nP r i n t\nan answer\nin\nn a t u r a l\nlanguage .\nmedian = c a l c u l a t e _ m e d i a n ( data )\np r i n t ( f \"To f i n d\nthe\nmedian\nof\na\ndata\nset , we need\nto\na r r a n g e\nthe\ndata\nin\nascending\norder\nand\nthen\nf i n d\nthe\nmiddle\nvalue .\nIn\nt h i s\ncase ,\nthe\ngiven\ndata\ni s\na l r e a d y\narranged\nin\nascending\norder .\nSince\nt h e r e\nare\n{ len ( data ) }\nval ues\nin\nthe\ndata\nset ,\nthe\nmedian\nw i l l\nbe\nthe\nmiddle\nvalue ,\nwhich\ni s\nthe\n{ len ( data ) / / 2\n+ 1} rd\nvalue .\nHence ,\nthe\nmedian\nof\nthe\ngiven\ndata\ns e t\ni s\n{ median } . \" )\n‘ ‘ ‘\nPrompt for our task-general program-of-\nthoughts (PoT) implementation in Table 3. The\nintermediate program reasoning chains are ob-\ntained from NLEP. We keep the key implemen-\ntation steps of Python programs but eliminate the\nstep-by-step programming instructions and detailed\nnatural language elaboration as an ablation.\nWrite a bug−f r e e\nPython\nprogram\nt h a t\ncan\ng e n e r a t e\nthe\nanswer\nto\nthe\ngiven\ni n s t r u c t i o n\nwhen\nc o r r e c t l y\nexecuted .\n###\nI n s t r u c t i o n :\nArrange\nthe\nf o l l o w i n g\nwords\nto make\nthe\nl o n g e s t\np o s s i b l e\nword .\n###\nInput :\nthe ,\nhad ,\nnot ,\nbeen\n### Python\nprogram :\n‘ ‘ ‘\ndef\nlongest_word ( words ) :\nfrom\ni t e r t o o l s\nimport\np e r m u t a t i o n s\nall_words = [ ’ ’ . j o i n ( p )\nf o r\np in\np e r m u t a t i o n s\n( ’ ’ . j o i n ( words ) ) ]\nall_words . s o r t ( key=len ,\nr e v e r s e =True )\nwith\nopen ( ’ english_words . txt ’ )\nas\nw o r d _ f i l e :\n#\nAssuming you have a\nl i s t\nof\ne n g l i s h\nwords\nenglish_words = s e t ( word . s t r i p ( ) . lower ( )\nf o r\nword\nin\nw o r d _ f i l e )\nf o r\nword\nin\nall_words :\ni f\nword . lower ( )\nin\nenglish_words :\nr e t u r n\nword\nr e t u r n\nNone\nwords = [ \" the \" ,\n\" had \" ,\n\" not \" ,\n\" been \" ]\nl o n g e s t = longest_word ( words )\ni f\nl o n g e s t :\np r i n t ( l o n g e s t )\n‘ ‘ ‘\n###\nI n s t r u c t i o n :\nUse\nthe\ngiven\ndata\nto\nc a l c u l a t e\nthe\nmedian .\n###\nInput :\n[2 ,\n3 ,\n7 ,\n8 ,\n10]\n### Python\nprogram :\n‘ ‘ ‘\ndata = [2 ,\n3 ,\n7 ,\n8 ,\n10]\ndata . s o r t ( )\nl e n g t h = len ( data )\ni f\nl e n g t h % 2 == 0:\np r i n t ( ( data [ l e n g t h / / 2 ]\n+ data [ l e n g t h / / 2\n−1 ] )\n/\n2)\ne l s e :\np r i n t ( data [ l e n g t h / / 2 ] )\n‘ ‘ ‘\nD.3\nPrompts for NLEP in Table 5 and Figure\n4\nFor experiments in TruthfulQA and VicunaQA,\nwe added the following example into the NLEP\nprompts shown in Appendix D.1 to encourage gen-\nerating more detailed responses:\n# Write a bug−f r e e\nPython\nprogram\nt h a t\ncan\ng e n e r a t e\nthe\nanswer\nto\nthe\ngiven\ni n s t r u c t i o n\nwhen\nc o r r e c t l y\nexecuted . Do not\nask\nf o r\nuser\ni n p u t .\nFor\nr e a s o n i n g\ntasks ,\nd e f i n e\nf u n c t i o n s\nf i r s t\nand\nthen\nd e f i n e\nv a r i a b l e s .\nFor\nknowledge\ni n t e n s i v e\ntasks ,\nd e f i n e\nv a r i a b l e s\nbefore\nd e f i n i n g\nf u n c t i o n s . Do not\nd e f i n e\nany\nv a r i a b l e\nt h a t\nd i r e c t l y\ns t o r e s\nthe\nf i n a l\nanswer .\nI f\nt h e r e\ni s\na\nknowledge\nd e f i n i t i o n\nstep ,\nuse\nd i c t i o n a r i e s\nto\ns t o r e\nboth\nthe\nknowledge and\nd e t a i l e d\ne x p l a n a t i o n .\n###\nI n s t r u c t i o n :\nDiscuss\nthe\ncauses\nof\nthe\nGreat\nDepression\n###\nInput :\nNone\n### Python\nprogram :\n‘ ‘ ‘\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\n# No need\nto\nimport\n# Step\n2:\nDefine\nd i c t i o n a r i e s\ns t o r i n g\nd e t a i l e d\nknowledge\nabout\nthe\ng r a t\nd e p r e s s i o n\ndepression_name = \" The Great\nDepression \"\nd e p r e s s i o n _ p e r i o d = \"1929 −1939\"\nd e p r e s s i o n _ c o u n t r i e s = \" the\nUnited\nS t a t e s\nand\nc o u n t r i e s\naround\nthe\nworld \"\nd e p r e s s i o n _ c a u s e s = {\n\" Stock\nMarket\nCrash\nof\n1929\":\n\" In\nOctober\nof\n1929 ,\nthe\nstock\nmarket\nexperienced\na\ns i g n i f i c a n t\nf a l l\nt h a t\nwiped\nout\nm i l l i o n s\nof\ni n v e s t o r s .\nThis\nevent\ni s\nc o n s i d e r e d\nby\nmany to\nbe\nthe\ni n i t i a l\nt r i g g e r\nof\nthe\nGreat\nDepression . \" ,\n\" Overproduction \" :\n\" During\nthe\n1920 s ,\nmany\ni n d u s t r i e s\nproduced more goods\nthan\nconsumers\nwanted\nor\ncould\na f f o r d .\nThis\nu l t i m a t e l y\nled\nto\na\nd e c l i n e\nin\ndemand\nf o r\ngoods ,\ncausing\njob\nloss ,\nlower wages ,\nand\nb u s i n e s s\nf a i l u r e . \" ,\n\" High\nT a r i f f s\nand War Debts \" :\n\" P r o t e c t i o n i s t\nt r a d e\np o l i c i e s\nin\nthe\nform\nof\nhigh\nt a r i f f s\nled\nto\na\nd e c l i n e\nin\ng l o b a l\ntrade ,\nas\no t h e r\nc o u n t r i e s\nr e t a l i a t e d\nwith\nt a r i f f s\nof\nt h e i r\nown .\nA d d i t i o n a l l y ,\nmany\nc o u n t r i e s\nwere\ns t r u g g l i n g\nto\nrepay\nwar\ndebts ,\nwhich\nled\nto\neconomic\ni n s t a b i l i t y . \" ,\n\" Bank\nF a i l u r e s \" :\n\"As demand\nf o r\ngoods\ndeclined ,\nmany banks\nbegan\nto\nf a i l ,\ncausing\na\nl o s s\nof\nconfidence\nin\nthe\nbanking\nsystem .\nThis\nled\nto\na\nmassive\nwithdrawal\nof money from\nbanks ,\ncausing\neven more banks\nto\nf a i l . \" ,\n\" Drought\nConditions \" :\n\" The Dust Bowl was a\nse ve re\ndrought\nand\ndust\nstorm\nt h a t\nh i t\nthe\nGreat\nP l a i n s\nregi on\nof\nthe\nUnited\nS t a t e s\nin\nthe\n1930 s .\nThis\nhad a\ns i g n i f i c a n t\nimpact\non\na g r i c u l t u r e ,\ncausing many farmers\nto\nl o s e\nt h e i r\nland\nand\nl i v e l i h o o d s\nwhich\nworsened\nthe\ne f f e c t s\nof\nthe\nd e p r e s s i o n . \"\n}\n# Step\n3:\nDefine\nn e c e s s a r y\nf u n c t i o n s\nt h a t\ng e n e r a l l y\nsolve\nt h i s\ntype\nof\nproblem\n# Do not\nneed\nto\nd e f i n e\nf u n c t i o n s\n# Step\n4:\nP r i n t\nthe\nanswer and\ne x p l a i n\nin\nn a t u r a l\nlanguage by\nc a l l i n g\nthe\ni n f o r m a t i o n\nin\nthe\nd e f i n e d\nknowledge\nd i c t i o n a r y\n‘ depression_causes\n‘\np r i n t ( f \"{ depression_name } was a\np eri od\nof\neconomic\nd e c l i n e\nt h a t\nl a s t e d\nfrom { d e p r e s s i o n _ p e r i o d } ,\nmaking\ni t\nthe\nlongest −l a s t i n g\nd e p r e s s i o n\nin\nmodern\nh i s t o r y .\nI t\na f f e c t e d\nnot\nonly {\nd e p r e s s i o n _ c o u n t r i e s } ,\ncausing\ns u b s t a n t i a l\ns o c i a l\nand economic\nupheaval . \\ n \" )\np r i n t ( f \" There\nwere\ns e v e r a l\nmajor\ncauses\nof {\ndepression_name } ,\nwhich\ni n c l u d e : \\ n \" )\n#\nL i s t\ncauses\nand\ne x p l a n a t i o n s\nin\n‘ depression_causes\n‘\nwith\na\nfor −loop .\nf o r\ni ,\n( cause ,\nd e s c r i p t i o n )\nin\nenumerate (\nd e p r e s s i o n _ c a u s e s . items ( ) ,\n1) :\np r i n t ( f \"{ i } .\n{ cause } −{ d e s c r i p t i o n }\\ n \" )\np r i n t ( f \" Overall ,\n{ depression_name } was caused by a\ncombination\nof\nf a c t o r s ,\ni n c l u d i n g\neconomic ,\nenvironmental ,\nand\np o l i t i c a l\nf a c t o r s .\nI t s\nimpact was widespread ,\na f f e c t i n g\nm i l l i o n s\nof\npeople\naround\nthe\nworld . \" )\n‘ ‘ ‘\n###\nI n s t r u c t i o n :\nI d e n t i f y\nthe\nodd one\nout .\n###\nInput :\nTwitter ,\nInstagram ,\nTelegram\n### Python\nprogram :\n‘ ‘ ‘\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\nfrom\nc o l l e c t i o n s\nimport\nOrderedDict\n# Step\n2:\nDefine\nd i c t i o n a r i e s\ns t o r i n g\nd e t a i l e d\nknowledge\nabout\nthe\nmain\nf u n c t i o n\nof\neach\na p p l i c a t i o n\ns e r v i c e s = {\n\" T w i t t e r \" :\n\" a\ns o c i a l\nmedia\np l a t f o r m\nmainly\nf o r\ns h a r i n g\ninformation ,\nimages and\nvideos \" ,\n\" Instagram \" :\n\" a\ns o c i a l\nmedia\np l a t f o r m\nmainly\nf o r\ns h a r i n g\ninformation ,\nimages and\nvideos \" ,\n\" Telegram \" :\n\" a cloud −based\ni n s t a n t\nmessaging\nand\nvoice −over −IP\ns e r v i c e \" ,\n}\n# Step\n3:\nDefine\na\nf u n c t i o n\nt h a t\nf i n d s\nthe\nd i f f e r e n t\na p p l i c a t i o n\ndef\nfind_odd_one_out ( s e r v i c e s ,\ni n p u t _ s e r v i c e s ) :\nd e s c r i p t i o n s\n= [ s e r v i c e s [ s e r v i c e ]\nf o r\ns e r v i c e\nin\ni n p u t _ s e r v i c e s ]\nf o r\nd e s c r i p t i o n\nin\nd e s c r i p t i o n s :\ni f\nd e s c r i p t i o n s . count ( d e s c r i p t i o n ) == 1:\nr e t u r n\ni n p u t _ s e r v i c e s [ d e s c r i p t i o n s . index\n( d e s c r i p t i o n ) ]\nr e t u r n\nNone\n# Step\n4:\nP r i n t\nthe\nanswer\nin\nn a t u r a l\nlanguage by\nc a l l i n g\nthe\ni n f o r m a t i o n\ns t o r e d\nin\n‘ s e r v i c e s ‘\nand\nthe\nd e f i n e d\nf u n c t i o n\n‘ find_odd_one_out ‘\ni n p u t _ s e r v i c e s = [ \" T w i t t e r \" ,\n\" Instagram \" ,\n\" Telegram\n\" ]\nodd_one_out = find_odd_one_out ( s e r v i c e s ,\ni n p u t _ s e r v i c e s )\ni f\nodd_one_out :\no t h e r _ s e r v i c e s = [ s e r v i c e\nf o r\ns e r v i c e\nin\ni n p u t _ s e r v i c e s\ni f\ns e r v i c e\n!=\nodd_one_out ]\np r i n t ( f \" The odd one\nout\ni s\n{ odd_one_out } .\n{\no t h e r _ s e r v i c e s [ 0 ]}\nand { o t h e r _ s e r v i c e s [ 1 ] }\nare\n{ s e r v i c e s [ o t h e r _ s e r v i c e s [ 0 ] ] }\nwhile {\nodd_one_out }\ni s\n{ s e r v i c e s [ odd_one_out ] } . \" )\n‘ ‘ ‘\n###\nI n s t r u c t i o n :\nC a l c u l a t e\nthe\nt o t a l\ns u r f a c e\narea\nof\na cube\nwith\na\ns i d e\nl e n g t h\nof 5 cm .\n###\nInput :\nNone\n### Python\nprogram :\n‘ ‘ ‘\n# Step\n1:\nImport\nn e c e s s a r y\nb u i l t −in\nl i b r a r i e s\n# No need\nto\nimport\n# Step\n2:\nDefine\na\nf u n c t i o n\nt h a t\nc a l c u l a t e\nthe\ns u r f a c e\narea\nof\ncubes\ndef\nc a l c u l a t e _ s u r f a c e _ a r e a ( s i d e _ l e n g t h ) :\nr e t u r n\n6 * ( s i d e _ l e n g t h\n** 2)\n# Step\n3:\nDefine\nd i c t i o n a r i e s\ns t o r i n g\nthe\ncube\ni n f o r m a t i o n\ncube = {\n\" s i d e _ l e n g t h \" :\n5\n# Side\nl e n g t h\nof\nthe\ncube\n}\n# Step\n4:\nP r i n t\na\nstep −by−s t e p\nc a l c u l a t i o n\nanswer\nin\nn a t u r a l\nlanguage\nusing\nthe\ndefined\nf u n c t i o n\nand\nv a r i b l e\ns i d e _ l e n g t h = cube [ \" s i d e _ l e n g t h \" ]\ns u r f a c e _ a r e a = c a l c u l a t e _ s u r f a c e _ a r e a ( s i d e _ l e n g t h )\np r i n t ( f \" The\ns u r f a c e\narea\nof\na cube\ni s\nfound by\nc a l c u l a t i n g\nthe\narea\nof\none\nof\ni t s\nf a c e s\nand\nm u l t i p l y i n g\ni t\nby\ns i x\n( s i n c e\na cube\nhas\ns i x\nf a c e s ) .\nThe\narea\nof\na cube\nface\ni s\nsimply\ni t s\ns i d e\nl e n g t h\nsquared . \\ n \" )\np r i n t ( f \" Thus\nf o r\nt h i s\np a r t i c u l a r\ncube : \" )\np r i n t ( f \" Surface\nArea = 6 x ( Side\nLength ) \\ ^ 2 \" )\np r i n t ( f \"\n= 6 x ({ s i d e _ l e n g t h } cm) \\ ^ 2 \" )\np r i n t ( f \"\n= 6 x { s i d e _ l e n g t h **2} cm \\ ^ 2 \" )\np r i n t ( f \"\n= { s u r f a c e _ a r e a } cm \\ n \" )\np r i n t ( f \" The\nt o t a l\ns u r f a c e\narea\nof\nt h i s\ncube\ni s\n{\ns u r f a c e _ a r e a }\nsquare\nc e n t i m e t e r s . \" )\n‘ ‘ ‘\n‘ ‘ ‘\nD.4\nPrompts for Table 6 and 13\nWe use the following prompt for the entailment-\nbased NLEP results in Table 6. The model-free\nresult uses the NLEP prompt shown in D.1.\n\"\"\" Write a Python\nf u n c t i o n\nt h a t\nc o n s t r u c t s\na\nd e c i s i o n\nt r e e\naccording\nto\nthe\ngiven\nexamples\nt h a t\ncan\ng e n e r a t e\nthe\nc o r r e c t\nl a b e l\nof\nthe\ngiven\nc l a s s i f i c a t i o n\nt a s k . \" \" \"\n###\nA v a i l a b l e\nf u n c t i o n s\n( shared\nf o r\na l l\nt a s k s ) :\n# Returns\nwhether\nthe\nh y p o t h e s i s\nin\ne n t a i l e d\nby\nthe\npremise .\ndef\ne n t a i l m e n t ( hypothesis ,\npremise ,\nmodel ,\nt o k e n i z e r\n) :\np r o p o s i t i o n = f ’{ h y p o t h e s i s }\ni s\ne n t a i l e d\nby {\npremise }. ’\ni n p u t s = t o k e n i z e r ( p r o p o s i t i o n ,\nr e t u r n _ t e n s o r s\n=\" pt \" ,\nt r u n c a t i o n =True ,\npadding=True ,\nmax_length =128)\no u t p u t s = model (** i n p u t s ) [ ’ l o g i t s ’ ] [ 0 ]\ne n t _ l a b e l =\ni n t ( o u t p u t s [ 0 ] > o u t p u t s [ 2 ] )\ni f\ne n t _ l a b e l == 1:\nr e t u r n\n’ yes ’\ne l s e :\nr e t u r n\n’no ’\n# Use\nthe\nc o n s t r u c t e d\nd e c i s i o n\nt r e e\nto\np r e d i c t\nthe\nl a b e l\nof\nthe\ns en t e n ce .\ndef\nt r e e _ p r e d i c t ( sentence ,\nc r i t e r i o n s ,\ntree ,\nmodel ,\nt o k e n i z e r ) :\nnode = t r e e [ ’ root ’ ]\nwhile\nnode\nnot\nin POSSIBLE_CLASSES :\ne n t _ l a b e l = e n t a i l m e n t ( c r i t e r i o n s [ node ] ,\nsentence ,\nmodel ,\nt o k e n i z e r )\nnode = t r e e [ node ] [ e n t _ l a b e l ]\nr e t u r n\nnode\n### Task :\nMovie review\nc l a s s i f i c a t i o n\n###\nP o s s i b l e\nc l a s s e s :\n[ p o s i t i v e ,\nn e g a t i v e ]\n### Examples :\n\"\"\"\n−c o n t a i n s\nno wit ,\nonly\nl a b o r e d\ngags\n−[ The movie\ni s\nwise | The movie\ni s\nnot\nwise | 1 ] ,\n[\nthe\ns t o r y\ni s\nfun | the\ns t o r y\ni s\nnot\nboring\n| 1 ] ,\n[ the\nreview\ni s\np o s i t i v e | the\nreview\ni s\nn e g a t i v e | 1 ]\n−t h a t\nloves\ni t s\nc h a r a c t e r s\nand communicates\nsomething\nr a t h e r\nb e a u t i f u l\nabout human\nn a t u r e\n−[ The\nc h a r a c t e r s\nare\nl o v e l y | The\nc h a r a c t e r s\nare\nawful | 0 ] ,\n[ the\ns c r i p t\ni s\ntouching | the\ns c r i p t\ni s\ndry | 0 ] ,\n[ the\nreview\ni s\np o s i t i v e |\nthe\nreview\ni s\nn e g a t i v e | 0 ]\n−on\nthe\nworst\nrevenge −of −the −nerds\nc l i c h e s\nthe\nfilmmakers\ncould\ndredge up\n−[ The movie\ni s\nnovel | The movie\ni s\nmostly\np l a t i t u d e s | 1 ] ,\n[ the\nreview\ni s\nn e g a t i v e | 1 ]\n−are\nmore\ndeeply\nthought\nthrough\nthan\nin\nmost\nr i g h t\n−t h i n k i n g\nf i l m s\n−[ The takeaway\nof\nthe\nmovie\ni s\nprofound | The\nidea\nof\nthe\nmovie\ni s\nshallow | 0 ] ,\n[ the\nreview\ni s\np o s i t i v e | the\nreview\ni s\nn e g a t i v e\n| 0 ]\n\"\"\"\n###\nDefine\np o s s i b l e\nc l a s s e s\nPOSSIBLE_CLASSES = [ ’ p o s i t i v e ’ ,\n’ negative ’ ]\n### Python\nprogram :\n‘ ‘ ‘\ndef\ng e t _ d e c i s i o n _ t r e e ( sentence ,\nmodel ,\nt o k e n i z e r ) :\n# Step\n1:\nd e f i n e\nc r i t e r i o n s\nof\nthe\nd e c i s i o n\nt r e e\n.\nc r i t e r i o n s\n= [\n’ This\nmovie\ni s\ni n t e r e s t i n g ’ ,\n’ The movie\nhas a good\ns c r i p t ’ ,\n’ The\nc h a r a c t e r s\nare awsome ’ ,\n’ This\nmovie\ni s\nwise ’\n]\n# Step\n2:\nd e f i n e\nthe\nDecision\nTree\nf o r\nc l a s s i f i c a t i o n\nt r e e = {\n’ root ’ :\n0 ,\n0:\n{ ’ yes ’ :\n1 ,\n’no ’ :\n3} ,\n1:\n{ ’ yes ’ :\n’ p o s i t i v e ’ ,\n’no ’ :\n2} ,\n2:\n{ ’ yes ’ :\n’ p o s i t i v e ’ ,\n’no ’ :\n’ negative ’} ,\n3:\n{ ’ yes ’ :\n’ p o s i t i v e ’ ,\n’no ’ :\n’ negative ’}\n}\nr e t u r n\nc r i t e r i o n s ,\nt r e e\n‘ ‘ ‘\nWhen we test the SST-2 performance based on\na generated Cola decision tree in Table 13, we use\nthe following prompt:\nWrite a Python\nf u n c t i o n\nt h a t\nc o n s t r u c t s\na\nd e c i s i o n\nt r e e\naccording\nto\nthe\ngiven\nexamples\nt h a t\ncan\ng e n e r a t e\nthe\nc o r r e c t\nl a b e l\nof\nthe\ngiven\nc l a s s i f i c a t i o n\nt a s k .\n###\nA v a i l a b l e\nAPIs ( shared\nf o r\na l l\nt a s k s ) :\n# Returns\nwhether\nthe\nh y p o t h e s i s\nin\ne n t a i l e d\nby\nthe\npremise .\ndef\ne n t a i l m e n t ( hypothesis ,\npremise ,\nmodel ,\nt o k e n i z e r\n) :\np r o p o s i t i o n = f ’{ h y p o t h e s i s }\ni s\ne n t a i l e d\nby {\npremise }. ’\ni n p u t s = t o k e n i z e r ( p r o p o s i t i o n ,\nr e t u r n _ t e n s o r s\n=\" pt \" ,\nt r u n c a t i o n =True ,\npadding=True ,\nmax_length =128)\no u t p u t s = model (** i n p u t s ) [ ’ l o g i t s ’ ] [ 0 ]\ne n t _ l a b e l =\ni n t ( o u t p u t s [ 0 ] > o u t p u t s [ 2 ] )\ni f\ne n t _ l a b e l == 1:\nr e t u r n\n’ yes ’\ne l s e :\nr e t u r n\n’no ’\n# Use\nthe\nc o n s t r u c t e d\nd e c i s i o n\nt r e e\nto\np r e d i c t\nthe\nl a b e l\nof\nthe\ns en t e n ce .\ndef\nt r e e _ p r e d i c t ( sentence ,\nc r i t e r i o n s ,\ntree ,\nmodel ,\nt o k e n i z e r ) :\nnode = t r e e [ ’ root ’ ]\nwhile\nnode\nnot\nin POSSIBLE_CLASSES :\ne n t _ l a b e l = e n t a i l m e n t ( c r i t e r i o n s [ node ] ,\nsentence ,\nmodel ,\nt o k e n i z e r )\nnode = t r e e [ node ] [ e n t _ l a b e l ]\nr e t u r n\nnode\n### Task :\nGrammar\nc o r r e c t n e s s\nc l a s s i f i c a t i o n\n###\nP o s s i b l e\nc l a s s e s :\n[ ’ accpetable ’ ,\n’ unacceptable ’ ]\n###\nDefine\np o s s i b l e\nc l a s s e s\nPOSSIBLE_CLASSES = [ ’ accpetable ’ ,\n’ unacceptable ’ ]\n###\nDecision\nTree\nLogic :\n−I f\nverbs\nare\nnot\nc o r r e c t l y\nc o n s t r u c t e d ,\nthe\ns e n t e n c e\ni s\nimmediately\nl a b e l e d\nas\nu n a c c e p t a b l e\n.\n−I f\nverbs\nare\nc o r r e c t :\nThe\nt r e e\nthen\nchecks\ni f\nthe\ns en t e nc e\nhas\nc o r r e c t\np u n c t u a t i o n\n−I f\ni n c o r r e c t ,\nl a b e l\nthe\ns e nt e n c e\nas\nu n a c c e p t a b l e\n−I f\nc o r r e c t :\nThe next\nc r i t e r i o n\nto\nbe\na s s e s s e d\ni s\nthe\ns u b j e c t −verb\nagreement .\n−I f\ns u b j e c t\nand\nverb\ndisagree ,\nl a b e l\nthe\ns e nt e n ce\nas\nu n a c c e p t a b l e .\n−I f\nthey\nagree ,\ncheck\nf o r\ns e nt e n c e\nfragments .\n−I f\nthe\ns en t e n ce\ni s\na fragment ,\nl a b e l\ni t\nas\nu n a c c e p t a b l e .\n−I f\ni t\ni s\nnot\na\ns en t e n ce\nfragment ,\nl a b e l\nthe\ns en t e n ce\nas\na c c e p t a b l e .\n### Python\ncode\nf o r\nthe\nd e c i s i o n\nt r e e :\n‘ ‘ ‘ python\ndef\ng e t _ d e c i s i o n _ t r e e ( sentence ,\nmodel ,\nt o k e n i z e r ) :\n# Step\n1:\nd e f i n e\nc r i t e r i o n s\nof\nthe\nd e c i s i o n\nt r e e\nc r i t e r i o n s\n= {\n’ c o r r e c t _ v e r b s ’ :\n’The\nverbs\nare\nc o r r e c t l y\nc o n s t r u c t e d\nin\nthe\nsentence ’ ,\n’ c o r r e c t _ p u n c t u a t i o n ’ :\n’ The\ns e nt e n c e\ni s\npunctuated\nc o r r e c t l y ’ ,\n’ subject_verb_agreement ’ :\n’ The\ns u b j e c t\nand\nverb\nagree\nin\nthe\nsentence ’ ,\n’ no_sentence_fragments ’ :\n’ The\ns e nt e n c e\ni s\nnot\na fragment ’ ,\n}\n# Step\n2:\nd e f i n e\nthe\nbalanced\nd e c i s i o n\nt r e e\nf o r\nt h i s\nc l a s s i f i c a t i o n\nt a s k\nt r e e = {\n’ root ’ :\n’ c o r r e c t _ v e r b s ’ ,\n’ c o r r e c t _ v e r b s ’ :\n{ ’ yes ’ :\n’\nc o r r e c t _ p u n c t u a t i o n ’ ,\n’no ’ :\n’\nunacceptable ’} ,\n’ c o r r e c t _ p u n c t u a t i o n ’ :\n{ ’ yes ’ :\n’\nsubject_verb_agreement ’ ,\n’no ’ :\n’\nunacceptable ’} ,\n’ subject_verb_agreement ’ :\n{ ’ yes ’ :\n’\nno_sentence_fragments ’ ,\n’no ’ :\n’\nunacceptable ’} ,\n’ no_sentence_fragments ’ :\n{ ’ yes ’ :\n’ acceptable\n’ ,\n’no ’ :\n’ unacceptable ’}\n}\nr e t u r n\nc r i t e r i o n s ,\nt r e e\n‘ ‘ ‘\nThe input format of target tasks is\n### Task :\nGrammar\nc o r r e c t n e s s\nc l a s s i f i c a t i o n\n###\nP o s s i b l e\nc l a s s e s :\n[ acceptable ,\nu n a c c e p t a b l e ]\nE\nImplementation Details for\nTask-Specific Strategies\nWe detail the few-shot chain-of-thought (CoT) and\nprogram-of-thought (PoT) prompting under the\ntask-specific setting in Tables 2 and 4:\n• Tracking Shuffled Objects (7). We use the\nsame 3-shot examples as used by previous\nwork for both task-specific CoT and PoT. The\nthree examples are related to Tracking Shuf-\nfled Objects (3) and the models need to learn\nfrom demonstrations and generalize to seven\nobjects test cases. The difference between\nCoT and PoT lies on the format of intermedi-\nate reasoning: CoT adopts natural language as\nthe reasoning chains while we transform the\nthought process into concise Python code for\nPoT.\n• Dyck Language. We cite the results of CoT\nfrom LATM (Cai et al., 2023) and transform\nthe reasoning steps of the 3-shot examples\nused by previous chain-of-thought work into\nPython code for PoT. In order to evaluate the\ngeneralization ability of program-of-thought\nprompting, we try to avoid directly giving gen-\nerally applicable Python program that can be\nused for all test instances.\n• Word Sorting. We cite the results of CoT\nfrom LATM (Cai et al., 2023) and transform\nthe reasoning steps of the 3-shot examples\nused by previous chain-of-thought work into\nPython code for PoT. Since the task can be\neffectively resolved by just few lines of code,\ni.e., read in the given input and use sorted()\nfunction, e.g.,\n# Python code ,\nr e t u r n\nans\nwords = [ ’ oven ’ ,\n’ costume ’ ,\n’ c o u n t e r p a r t ’ ]\nans = \"\n\" . j o i n ( s o r t e d ( words ) )\nit can be regarded that the generally applicable\ntool is already given in the input prompt.\n• Chinese Remainder Theorem. We cite the\nresults of CoT from LATM (Cai et al., 2023).\nWe build the in-context examples (3-shot)\nwith the first three successful instances of task-\ngeneral PoT as we construct the Python rea-\nsoning chains from the generated programs of\ntask-general PoT with GPT-4. Indeed, for this\ncomplicated task, the provided program in the\ndemonstration can also be regarded as a gener-\nally applicable tool. That’s a main reason why\ntask-specific PoT can obtain 100% accuracy\non this benchmark.\n• Scheduling Meeting. We cite the results of\nCoT from LATM (Cai et al., 2023). We build\nthe in-context examples (3-shot) with the first\nthree successful instances of task-general PoT\nas we construct the Python reasoning chains\nfrom the generated programs of task-general\nPoT with GPT-4.\nHowever, unlike giving\nthe “ground-truth\" problem solving tool for\nChinese Remainder Theorem, the provided\nPython reasoning chains can only derive the\ncorrect answer for each specific demonstra-\ntion question but can not be directly applied\nto all scenarios due to the complexity of the\nproblem. We hope to compare this setup with\nChinese Remainder Theorem and evaluate the\nperformance of task-specific PoT on compli-\ncated tasks through different in-context learn-\ning demonstrations.\n• GSM-Hard. We use the same 8-shot exam-\nples as used by previous work on GSM8K\ndataset for CoT GSM-Hard. For PoT, we\nadopt the 9-shot examples on GSM8K dataset\nfrom program-of-thought (Chen et al., 2022)\ncontaining concise Python code as reasoning\nchains.\n• StrategyQA. We remove 1 example that ap-\npears in the development set from the 6-shot\ndemonstration of previous work (Lyu et al.,\n2023) for CoT. As PoT is not designed and ap-\nplied for natural language question answering\ntask, we did not reproduce task-specific PoT\nresults on StrategyQA benchmark.\nF\nEvaluation Prompts for VicunaQA\nWe have two metrics for VicunaQA. The first met-\nric assesses the level of details and biases to long\nresponses generated by GPT-4, while the other met-\nric does not ask for details.\nF.1\nEvaluation prompt asking for details.\nprompt = f ’ ’ ’ [ Question ] \\ n{ q u e s _ s t r }\n[ The\nS t a r t\nof\nA s s i s t a n t\n1 ’ s Answer ] \\ n{ gp t 4_ r e s }\n\\ n [ The End of\nA s s i s t a n t\n1 ’ s Answer ]\n[ The\nS t a r t\nof\nA s s i s t a n t\n2 ’ s Answer ] \\ n{ t a r g e t _ r e s }\n\\ n [ The End of\nA s s i s t a n t\n2 ’ s Answer ]\n[ System ]\nWe would\nl i k e\nto\nr e q u e s t\nyour\nfeedback on\nthe\nperformance\nof two AI\na s s i s t a n t s\nin\nresponse\nto\nthe\nuser\nq u e s t i o n\nd i s p l a y e d\nabove . \\ nPlease\nr a t e\nthe\nh e l p f u l n e s s ,\nrelevance ,\naccuracy ,\nl e v e l\nof\nd e t a i l s\nof\nt h e i r\nr e s p o n s e s .\nEach\na s s i s t a n t\nr e c e i v e s\nan\no v e r a l l\nscore\non a\ns c a l e\nof 1 to\n10 ,\nwhere a\nhigher\nscore\ni n d i c a t e s\nb e t t e r\no v e r a l l\nperformance . \\ nPlease\nf i r s t\noutput\na\ns i n g l e\nl i n e\nc o n t a i n i n g\nonly two\nvalu es\ni n d i c a t i n g\nthe\ns c o r e s\nf o r\nA s s i s t a n t\n1 and\n2 ,\nr e s p e c t i v e l y .\nThe two\ns c o r e s\nare\ns e p a r a t e d\nby a\nspace .\nIn\nthe\nsubsequent\nline ,\np l e a s e\nprovide\na\ncomprehensive\ne x p l a n a t i o n\nof\nyour\ne v a l u a t i o n ,\navoiding\nany\np o t e n t i a l\nb i a s\nand\nen s ur i n g\nt h a t\nthe\norder\nin\nwhich\nthe\nr e s p o n s e s\nwere\np r e s e n t e d\ndoes\nnot\na f f e c t\nyour\njudgment . ’ ’ ’\nF.2\nEvaluation prompt not asking for details.\nprompt = f ’ ’ ’ [ Question ] \\ n{ q u e s _ s t r }\n[ The\nS t a r t\nof\nA s s i s t a n t\n1 ’ s Answer ] \\ n{ gp t 4_ r e s }\n\\ n [ The End of\nA s s i s t a n t\n1 ’ s Answer ]\n[ The\nS t a r t\nof\nA s s i s t a n t\n2 ’ s Answer ] \\ n{ t a r g e t _ r e s }\n\\ n [ The End of\nA s s i s t a n t\n2 ’ s Answer ]\n[ System ]\nWe would\nl i k e\nto\nr e q u e s t\nyour\nfeedback on\nthe\nperformance\nof two AI\na s s i s t a n t s\nin\nresponse\nto\nthe\nuser\nq u e s t i o n\nd i s p l a y e d\nabove . \\ nPlease\nr a t e\nthe\nr e l e v a n c e\nand\naccuracy\nof\nt h e i r\nr e s p o n s e s .\nEach\na s s i s t a n t\nr e c e i v e s\nan\no v e r a l l\nscore\non a\ns c a l e\nof 1 to\n10 ,\nwhere a\nhighe r\nscore\ni n d i c a t e s\nb e t t e r\no v e r a l l\nperformance . \\\nnPlease\nf i r s t\noutput\na\ns i n g l e\nl i n e\nc o n t a i n i n g\nonly\ntwo\nvalues\ni n d i c a t i n g\nthe\ns c o r e s\nf o r\nA s s i s t a n t\n1 and\n2 ,\nr e s p e c t i v e l y .\nThe two\ns c o r e s\nare\ns e p a r a t e d\nby a\nspace .\nIn\nthe\nsubsequent\nli n e ,\np l e a s e\nprovide\na\ncomprehensive\ne x p l a n a t i o n\nof\nyour\ne v a l u a t i o n ,\navoiding\nany\np o t e n t i a l\nb i a s\nand\nen s ur i n g\nt h a t\nthe\norder\nin\nwhich\nthe\nr e s p o n s e s\nwere\np r e s e n t e d\ndoes\nnot\na f f e c t\nyour\njudgment . Do not\nb i a s\non\ne i t h e r\nlonger\nor\ns h o r t e r\nanswers . ’ ’ ’\nF.3\nCalculation of Length Bias\nSuppose we have N evaluation cases, each receiv-\ning 2 candidate responses. A GPT-4 scorer decides\nthe winner between the candidates. a stands for the\nnumber of cases where a candidate response with\nmore tokens wins. The length bias is calculated by\nlb = | a\nN −0.5| ∗2\n(1)\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-09-19",
  "updated": "2024-03-29"
}