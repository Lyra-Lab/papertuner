{
  "id": "http://arxiv.org/abs/2309.12626v1",
  "title": "Construction contract risk identification based on knowledge-augmented language model",
  "authors": [
    "Saika Wong",
    "Chunmo Zheng",
    "Xing Su",
    "Yinqiu Tang"
  ],
  "abstract": "Contract review is an essential step in construction projects to prevent\npotential losses. However, the current methods for reviewing construction\ncontracts lack effectiveness and reliability, leading to time-consuming and\nerror-prone processes. While large language models (LLMs) have shown promise in\nrevolutionizing natural language processing (NLP) tasks, they struggle with\ndomain-specific knowledge and addressing specialized issues. This paper\npresents a novel approach that leverages LLMs with construction contract\nknowledge to emulate the process of contract review by human experts. Our\ntuning-free approach incorporates construction contract domain knowledge to\nenhance language models for identifying construction contract risks. The use of\na natural language when building the domain knowledge base facilitates\npractical implementation. We evaluated our method on real construction\ncontracts and achieved solid performance. Additionally, we investigated how\nlarge language models employ logical thinking during the task and provide\ninsights and recommendations for future research.",
  "text": "Construction contract risk identification based on \nknowledge-augmented language model \nSaika Wonga, Chunmo Zhenga, Xing Sua, *, Yinqiu Tangb\n \naCollege of Civil Engineering and Architecture, Zhejiang University, Hangzhou, \nChina \nbPowerChina Huadong Engineering Corporation Limited, Hangzhou, China \n \nAbstract \n \nContract review is an essential step in construction projects to prevent potential losses. \nHowever, the current methods for reviewing construction contracts lack effectiveness \nand reliability, leading to time-consuming and error-prone processes. While large \nlanguage models (LLMs) have shown promise in revolutionizing natural language \nprocessing (NLP) tasks, they struggle with domain-specific knowledge and addressing \nspecialized issues. This paper presents a novel approach that leverages LLMs with \nconstruction contract knowledge to emulate the process of contract review by human \nexperts. Our tuning-free approach incorporates construction contract domain \nknowledge to enhance language models for identifying construction contract risks. The \nuse of a natural language when building the domain knowledge base facilitates practical \nimplementation. We evaluated our method on real construction contracts and achieved \nsolid performance. Additionally, we investigated how large language models employ \nlogical thinking during the task and provide insights and recommendations for future \nresearch. \n \nKeywords: Large language models, Construction contract risk, Knowledge \naugmentation, Knowledge database \n \n \nHighlights \nl \nThe study uncovers the mechanism of “logical thinking” of large language \nmodels during construction contract risk identification. \nl \nThe construction contract knowledge-augmented framework formalizes the \nintegration of vectorized domain knowledge into large language models. \nl \nThe construction contract risk knowledge base established in natural language \nfacilitates its practical implementation. \n \n \n \n* Corresponding author, E-mail address: xsu@zju.edu.cn \n1. \nIntroduction \nConstruction contracts are the foundation for relationships among project \nstakeholders, protecting their rights and interests throughout the project's lifespan. \nContractual risks remain a long-standing and significant concern for all parties involved \nin a construction project, and failure to identify these risks in the contract clauses may \nresult in disputes, posing a risk of project loss. According to a recent report (Victoria et \nal., 2022), the global average values and durations of disputes are $52.6M and 15.4 \nmonths, respectively.  \nContract review involves several important tasks, such as identifying and modifying \nambiguous clauses (Artan Ilter and Bakioglu, 2018), clarifying vaguely stated \nrequirements (Hassan and Le, 2020; ul Hassan et al., 2020; ul Hassan and Le, 2021), \nand correcting inaccurately referenced specifications (Hamie and Abdul-Malak, 2018). \nCurrently, the construction industry mainly relies on manual review due to the lack of \nsophisticated and reliable automated methods for identifying construction contract risks \n(CCRI). However, studies (Lee et al., 2019; Moon et al., 2021) have shown that this \nlabor-intensive approach is error-prone and time-consuming.  It heavily relies on the \nexpertise and experience of contract experts who have to review numerous contract \nclauses within a limited timeframe. \nMany researchers have suggested various methods that utilizes computer to aid the \nreview process, which primarily involves rule-based natural language processing (NLP) \nmethods (Kim et al., 2020) and machine learning (ML) algorithms (Choi and Lee, 2022) \n(Candaş and Tokdemir, 2022). However, due to the complex language comprehension \nrequired for reviewing contract clauses, the existing approaches each suffer from \ndifferent limitations. For instance, rule-based NLP requires comprehensive rules to pre-\ndefined, which is not only inflexible but also poses a significant challenge to cover all \nsituations (Hassan and Le, 2021). In the case of applying ML algorithms (e.g., support \nvector machine), it requires a substantial amount of high-quality data to train a model \nfor a specific task. This demands considerable time investment for training, as well as \ndata collection, cleaning, and labeling. Meanwhile, its inability to effectively capture \nsemantic and contextual information significantly limits the performance of text \nunderstanding and processing (Hassan and Le, 2021; Yang et al., 2022). \nTo mitigate these problems, researchers have proposed the application of language \nmodels (LMs), to enhance the capture of semantic information from contract text \n(Mohamed Hassan et al., 2022; Shuai, 2023). For example, Fu’s research team (Fu et \nal., 2023) fine-tuned a language model, DeBERTa, for construction contracts semantic \nannotation. Meanwhile, due to the emergence of LLMs and their distinct performance, \ne.g., in-context learning, improved text comprehension, and the ability to summarize \nlong paragraphs (Zhao et al., 2023), researchers have begun to consider the use of LLMs \nfor processing contract text. Xue et al. (Xue et al., 2022) used BART to summarize \nconstruction contracts, maintaining critical information for better interpretation of the \ncontracts. \nHowever, LLMs pre-trained in the general domain are not directly applicable for \ndomain-specific tasks. As highlighted in several studies, using LLMs in specialized \nfields often leads to poor performance due to diluted domain-specific training data and \nissues that arise during the training processes (Ji et al., 2023; Kim et al., 2022; Mialon \net al., 2023; Peng et al., 2023). Imperfect representation learning is one such issue, \nwhere the encoders learn incorrect correlations between different parts of the training \ndata, leading to erroneous generation that deviates from the input. Similarly, direct \napplication of LLMs to process construction contracts may lead to unprofessional and \nincorrect outputs, i.e., hallucination (Huang et al., 2023). As a result, the utilization of \nLLMs in the construction contract management field is not as widespread as in many \nother fields (Cui et al., 2023). \nMotivated by the aforementioned challenges and aiming to explore how to enhance \nLLMs to more closely emulate human experts for CCRI, we have adopted the idea of \naugmenting LLMs with external knowledge (Gao et al., 2022; Guu et al., 2020; Peng \net al., 2023). We refer to this external knowledge, usually acquired from domain experts, \nas the factual knowledge and the expert knowledge. In this paper, we introduce a novel \nknowledge-augmented CCRI methodology. It augments an LLM with factual \nknowledge and the knowledge of contract experts, without fine-tuning the language \nmodel. This significantly alleviates the hallucination phenomenon in domain \nknowledge-intensive task. This research provides insights for utilizing LLMs with a \nthinking paradigm similar to that of human experts in identifying contract risks. \nSimultaneously, this contributes to the body of knowledge in the automated contract \nreview, which could substantially decrease the time spent on contract reviews and \nminimize risks during construction projects. Although we introduce this method in the \ncontext of identifying construction contract risks, it is a versatile approach that can be \nincorporated into a broad range of construction contract text processing and \nmanagement tasks.  \n2. \nRelated works \n2.1 Natural language processing in construction contract \nNLP is an effective means for computers to understand and solve a wide range of \ntasks involving human language. Common applications include information retrieval, \ninformation extraction, text classification, text generation, question answering etc. The \nfinal goal of NLP is to enable human languages to be processed automatically by \nmachines (Otter et al., 2019). In the past, many NLP methods and Machine Learning \n(ML) algorithms based on statistical and probabilistic calculations have been proposed \nand widely used to process and analyze text data. This field has become a popular area \ndue to the large amount of textual data generated during construction projects in recent \nyears (Ding et al., 2022; Yang et al., 2022; Zhang et al., 2023). \nConstruction contract review is a key subject of study in construction automation \nscenario. Automating general tasks such as contract quality review and identifying \ncommon patterns in contracts can improve efficiency and quality. The construction \ncontract text processing can be divided into rule-based NLP and ML-based (Salama and \nEl-Gohary, 2016). Clauses or relevant information extraction after pre-defined rule \nmatching is a common pattern for rule-based NLP methodology (Al Qady and Kandil, \n2010; Liu et al., 2014; Zhang and El-Gohary, 2016). Lee et al. (Lee et al., 2019) \ndeveloped a rule-based automatic model to detect contract clauses that disadvantage \ncontractors and extract critical information automatically. They first define the risk \nclauses, construct a domain lexicon, and design information extraction rules to achieve \nthe identification of risk clauses and extraction of relevant information. Lee et al. (Lee \net al., 2020) proposed rule-based NLP to build a model containing text pre-processing, \nsyntactic analysis, and semantic analysis for checking the omission of contractor-\nfriendly clauses in contracts. Despite the impressive results, the scalability of the \nmethod is hindered due to its dependence on the manual crafting of rules, which seems \ntedious and may not function efficiently with other standards contracts. Kim et al. (Kim \net al., 2020) proposed a rule-based NLP approach applying syntactic rules to increase \nthe accuracy of matching the Subject, Verb, Object (SVO) of each sentence and \nsemantic rules to pre-defined risk categories for extracting potentially dangerous \nclauses from the contract documents. Padhy et al. (Padhy et al., 2021) suggested that \nusing common phrases from exculpatory clauses as rules to extract exculpatory clauses \nin any contract, and eventually extract all clauses that contain one or more common \nphrases in the exculpatory clauses. The paper also stated that automatically extracting \nthe desired clauses using ML requires a large amount of training data. \nObtaining high-quality training data from construction contracts is time-consuming \nand labor-intensive. This is the reason for most of the rule-based NLP studies opt for \nsuch approach. Studies that construct corresponding rules can yield good results in \nterms of accuracy and interpretability. However, their dependence on predefined \nvocabulary and rules diminishes the flexibility of their application, suffer from the issue \nof poor generalization, where some knowledge is difficult to describe through rules, \nand it is both impossible and ineffective to develop all possible rules. Further, the high \nperformance comes at the expense of considerable human effort. With the advancement \nin the construction industry, construction contracts are becoming increasingly complex. \nConsequently, the need for construction contract risk identification cannot be satisfied \nsolely by pure NLP applications. Instead, further research should focus on pre-trained \nneural network models (Hassan et al., 2021). \nIn respect to ML-based method, commonly used ML models include support vector \nmachine (SVM) (Candaş and Tokdemir, 2022; Hassan and Le, 2021; Lee and Yi, 2017; \nUl Hassan et al., 2020; Yang et al., 2022), naïve bayes (NB) (Candaş and Tokdemir, \n2022; Hassan and Le, 2021, 2020; ul Hassan et al., 2020), k-nearest neighbors (KNN), \nand hidden Markov model (HMM) (Candaş and Tokdemir, 2022; ul Hassan et al., 2020; \nul Hassan and Le, 2021). All of which have been applied simultaneously around the \nabove-mentioned studies and compared in terms of performance. For instance, ul \nHassan and Le (Hassan and Le, 2020) utilized ML to develop a dependable \nclassification model with a recall of 95%. The model effectively distinguished between \nrequirement and non-requirement text within contracts.  \nWith the advancement of deep learning (DL), this technic has also been used in the \nanalysis of construction contract. Recurrent neural network (RNN) and long short-term \nmemory (LSTM) have been employed for this purpose. RNN is used to train a model \nfor DB requirement classification, and the research has yielded that the reliability of the \nclassification depends on the text vectorization method (ul Hassan and Le, 2021). Choi \nand Lee (Choi and Lee, 2022) proposed to apply a bi-LSTM algorithm to train a risk \nlevel ranking (RLR) model to automatically analyze the key risk clauses in the \ninvitation to bid (ITB) at the bidding stage. In addition, the study also mentioned that \nalthough the accuracy of text classification models has improved, it does not necessarily \nimply that the model comprehends the text at the same semantic level as a human would.  \nAlthough ML methods offer flexibility and efficiency, most studies face a \nsignificant limitation of the availability of training data. Collecting high-quality data is \ncrucial for training a competent model, but it poses a challenge, particularly with \ndomain-specific documents such as construction contracts. This challenge is further \namplified by the sensitive corporate data often contained within contract information, \nmaking it even more difficult to acquire of high-quality contract training data. Table 1 \nlists some representative studies using rule-based NLP, ML-based, and DL-based \nmethod in processing contract textual data. \n \nMethod \nDescription \nTask \nLiterature reviews \nRule-based NLP \nmethodology (semantic \nmapping rules, pattern \nmatching, lexicon-based \nmethod) \nUtilized NLP and a shallow parser for the extraction of semantic \nknowledge from construction contract documents. \nExtracting concept relations from \ncontracts for automatic contract \nreview and management \n(Al Qady and Kandil, 2010) \n9/22/23 1:14:00 PM \nA rule-based mechanism to map textual elements to ontology entities, \nenhancing the interpretation process of impact factors in the text. \nIdentification of impact factors in \nconstruction claim \n(Niu and Issa, 2013) \nEmployed semantic and syntactic rules to extract detrimental clauses for \ncontractor. \nPoisonous clauses identification \nfor automatic clauses detection \n(Lee et al., 2019) \nUsed NLP techniques to transform complex and lengthy contract \ndocuments into a simplified form for accurate Subject, Verb, Object \n(SVO) tagging for risk prediction and management. \nPotentially hazardous clauses \nextraction \n(Kim et al., 2020) \nEmployed a rule-based NLP methodology to build a proactive risk \nassessment model, analyzing unstructured text data through syntactic \nanalysis, and semantic analysis to identify missing contractor-friendly \nclauses. \nContractor-friendly clauses \nidentification for automatic \nclauses detection \n(Lee et al., 2020) \nDeveloped an NLP model that analyzes the syntactic, lexical, and \nsemantic aspects of natural language in construction contracts to identify \nexculpatory clauses automatically. \nExculpatory clauses identification \n(Padhy et al., 2021) \nML-based methodology \nFour classification models SVM, ANN, kNN, and NB are used for \nunstructured text combined with digital data of bid documents for bid \nrisk prediction. \nUncertainty risks classification \nbased on pre-bid clarification \ninformation \n(Lee and Yi, 2017) \nUsing linear and non-linear algorithms, feature extraction, \ntransformation, and selection, with predictive models such as logistic \nregression (LR), decision tree (DT), and random forest (RF), to ascertain \nthe probability of successful contract execution. \nIdentification of contract \nexecution according to financial \nand non-financial parameters \n(Valpeters et al., 2018) \nFour different machine learning algorithms were tested and compared for \nclassifying contractual text into requirement and non-requirement text. \nContract requirements \nclassification based on \nsubcontract discipline \n(Hassan and Le, 2020) \nUsing NLP and various supervised machine learning approaches, \nincluding NB, SVM, Logistic Regression, kNN, DT, and FNN, to \ndevelop an automated framework for classifying text describing project \nrequirements into distinct classes. \nContract requirements \nclassification \n(Ul Hassan et al., 2020) \nTable 1. Studies of rule-based NLP, ML-based, and DL-based methods for construction contract text processing. \n \n \nUtilizing NLP and supervised ML, including convolutional neural \nnetwork and recurrent neural network, to automate subcontract drafting, \ncategorizing requirements into three stages: design, construction, and \nO&M. \nContract requirements \nclassification based on \nsubcontract discipline \n(Hassan and Le, 2021) \nTwo models are proposed to automatically analyze critical risk clauses in \nInvitation to Bid (ITB) documents: a Semantic Analysis (SA) model, \nwhich is a rule-based approach using NLP to extract key risk clauses, \nand a Risk Level Ranking (RLR) model, which is a train-based approach \nthat applies bi-LSTM to rank the risk impact of each clause. \nITB risk clause classification \n(Choi and Lee, 2022) \nSeveral supervised ML classifiers were used to automate the \nidentification of vague terms in construction contract conditions. \nAmbiguous contract terms \nidentification \n(Candaş and Tokdemir, \n2022) \nML models were used to classify text in both general and particular \nconditions of contracts to measure the functions of contracts. \nContractual function classification \n(Yang et al., 2022) \nDL-based methodology \n(the primary model used \nwas BERT) \nFine-tuned BERT to classify near-miss reports in construction projects. \nSafety reports classification \n(Fang et al., 2020) \nDevelop a named entity recognition model based on bidirectional long \nshort-term memory architecture for construction specification review. \nConstruction contract named \nentity recognition \n(Moon et al., 2021) \nDetecting contractual risk information from construction specifications \nusing BERT. \nContract clause classification \n(Moon et al., 2022) \nConstruction contract text summarization using different LLMs, like \nBART. \nContract text summarization \n(Xue et al., 2022) \nFine-tuned BERT to classify obligation sentences in the context of EPC \ncontracts for mitigating the unilateral contractual change risk. \nContract clause classification \n(Shuai, 2023) \nFine-tuned DeBERTa for construction contracts semantic annotation to \nhelp measuring the complexity of contractual function. \nContract clause annotation \n(Fu et al., 2023) \n2.2 Knowledge augmented language model \nIn recent years, LMs have made significant progress, due to the availability of \nextensive datasets and the improvement of computer power. This progress has greatly \ninfluenced the field of NLP, with the emergence of several notable LLMs1 such as GPT \n(Brown et al., 2020), LLaMa (Touvron et al., 2023). As the LMs have scaled up in size, \nthey have demonstrated emergent abilities including in-context learning, step-by-step \nreasoning, and instruction following. These capabilities, which may not be as \npronounced in smaller LMs, have enabled LLMs to excel in zero-shot and few-shot \ntasks. They have also showcased exceptional proficiency in more complex endeavors \nsuch as mathematical problem-solving, question answering (Zhao et al., 2023). Recent \nstudies have highlighted the outstanding capabilities of LLMs on question answering \nand logic reasoning. The study (OpenAI, 2023) suggested that GPT-4 outperforms \nexisting pre-train language models on all benchmarks, the highest accuracy up to 96.3%, \nfor various question-answering and commonsense reasoning. Furthermore, LLMs have \nproven to be highly efficient on knowledge-intensive tasks and have even outperformed \nhuman experts in certain domains.  \nRegardless of the distinguished performance of the LLMs, its limitations lead to \nphenomena such as “making something out of nothing”. This is the reason why it has \nnot been widely used in many specialized fields that require a high degree of \nprofessionalism and logical rigor, such as law. This phenomenon summarized as \nhallucination. Hallucination can be categorized as intrinsic and extrinsic according to \ntheir nature (Liang et al., 2022). In current research, many methods have been proposed \nto alleviate this problem, including fine-tuning, prompt engineering, and data \naugmentation.  \nFine-tuning is a straightforward and highly effective method of addressing the \nhallucination, as it involves training the model with the knowledge of a specialized \ndomain and tailoring it to suit the desired application (Huang et al., 2023; Thoppilan et \nal., 2022; Tinn et al., 2023). For instance, both the Lawyer LLaMA (Huang et al., 2023) \nand the ChatLaw (Cui et al., 2023) fine-tuning the LLaMA model by injecting legal \ndomain knowledge including relevant law articles related to legal issue. These \nmodifications successfully overcame the models’ inability to leverage the legal \nknowledge to resolve domain-specific issues. However, fine-tuning a language model, \nparticularly one with a large number of parameters, is not a trivial and costly task. \nHence, to leverage the robust capabilities of the LLMs, studies suggested that \nspecialized methods of prompting are required to guide them in recalling the knowledge \nthey encapsulate, a process referred to as prompt engineering. In addition, the LLMs \nhave also shown the abilities to tackle reasoning problems by specific instructions like \nChain-of-Thought (Wei et al., 2022), Tree-of-Thoughts (Yao et al., 2023).  \nThe data augmentation on the other hand, also known as knowledge augmentation, \n \n1 Note: In existing research, there is currently no uniform definition of the LLM on the minimum parameter scale \nin either academia or industry. In this research, we mainly consider language models larger than 10B in size as \nLLM. \nis a method that incorporates external knowledge to improve the LLMs understanding \nand generation capabilities. Recent works have explore leveraging the structure data in \nthe form of knowledge graph (KG), providing external knowledge to the LLMs for \ninference and interpretability. Some researchers have proposed to incorporate KGs into \nLLMs during the pre-training stage, which can help LLMs learn knowledge from KGs \n(Liu et al., 2021; Yu et al., 2021). Others researchers proposed retrieving knowledge \nfrom the KGs for LLMs during the reasoning stage (Sen et al., 2023; Hu et al., 2022). \nGiven the KGs are challenging to construct and continuously evolving, they pose \ndifficulties for existing methods aimed at generating new facts and representing \nknowledge. Consequently, there are studies representing knowledge using vector \nembeddings that retain semantically meaningful information (Minaee et al., 2021).  \nVector embeddings represent data points in a multi-dimensional space and are \ncommonly used in ML to characterize features of objects, such as text, images, or even \nentities. It typically consists of a set of numbers that form a multidimensional numerical \nspace, wherein each dimension of a vector represents a different feature or attribute. \nFor example, in textual data, one dimension might represent the frequency of \noccurrence of each word. By defining different features, it is possible to represent \ntextual data as multi-dimensional vectors. This representation has the advantage that \nsimilar content will be closer in the vector space, so that similarity calculations, such \nas cosine similarity, can be used to quantify the similarity of the vectors. Compared \nwith KGs in terms of semantic search, the vector similarity search, which locates \nvectors closest to a given query vector, offers more accurate and contextualized \nsearching over data. Studies have started to use vector embeddings to represent and \nretrieve external knowledge in an effort to improve the performance of language model \n(Lazaridou et al., 2022; Nye et al., 2021). Peng et al. (Peng et al., 2023) proposed an \nLLM-augmenter, which utilizes similarity search to retrieve external knowledge from \nWikipedia, and links the retrieved knowledge to the relevant context. They reported that \nthe augmenter significantly improves the factuality of the answers. He et al. (He et al., \n2022) used Wikipedia as an external knowledge base, applying cosine similarity to \nselect the passages most similar to the input query sentences as knowledge relevant to \nthe question. The results of the study show that incorporating additional knowledge \nsurpasses methods that do not. This superiority is evident in terms of common-sense \nreasoning and temporal reasoning, and the faithfulness of the responses increases \ndramatically, especially as the scale of the model increases. Knowledge is critical for \nequipping statistics-based models, and vector embeddings can facilitate the \ndevelopment of an efficient retrieval enhancement system. Capitalizing on efficient \nsemantic search capability, this study proposes a novel method for identify risks in \ncontract clauses by incorporating knowledge highly relevant to the risk clauses into \nLLMs, not only aligning the models’ behaviors with the preferences of human experts \nenables but also increasing trustworthiness of the answers provided by the LLMs. As a \nresult, enhances its capabilities to identify risks in contract clauses. \n \n3. \nMethodology \nIn this section, we introduce the knowledge augmented language model based CCRI \nmethodology. It consists of four parts: building a domain-specific knowledge base, \nretrieving project clauses, retrieving clause-review pairs, and developing appropriate \nprompts. Given a construction contract clause, the goal of CCRI is to identify \ncontradictions or omissions in the context based on a given checkpoint. The overall \narchitecture of our approach is depicted in Fig. 1. \nFig. 1. The framework of knowledge augmented LLM for CCRI. \n3.1 Constitution of domain-specific knowledge base \nThe storage of data is one of the key tasks. Before storing the contract text into the \nknowledge base, it needs to be vectorized. To ensure the effectiveness of semantic \nsearch, the text should be parsed into text chunks with similar attributes. In the context \nof the construction contracts, the text is parsed into chunks based on the content of the \nclauses, specifically, each section. For instance, the content of \"condition precedent\" \nand \"payment\" will be treated as two separate text chunks. Longer sections can be \ndivided into shorter paragraphs while maintaining semantic coherence. Each chunk is \nthen vectorized using a text vectorizer, represented as an n-dimensional vector of \nfloating-point numbers. The vectorized text chunks and their corresponding \nembeddings are subsequently stored in the knowledge base. \nThe organization of data for the two knowledge bases is illustrated in Fig. 2. Each \nknowledge base is designed for specific purposes and has unique schemas. Both \nknowledge bases contain an \"ID\" and \"Embeddings\". The \"ID\", an integer number, is \nthe primary key for each row of data, while the \"Embeddings\", a floating-point vector \ndenotes the vector representation of each clause. The project clause base is responsible \nfor storing and providing contract clauses for risk identification. It adds the fields \n\"Clause_type\" and \"Clauses\" to its schema. \"Clause_type\" records the type of clause, \nspecifically the name of the section (when the subsection is omitted) or the subsection \nof each clause. \"Clauses\" represents the detailed content of each clause. For example, \nin subsection 4.1 \"Condition Precedent\", \"4.1 Condition Precedent\" is the \n\"Clause_type\" and the corresponding clauses \"The Contract shall come into full force \nand effect on the Date… waived by the Owner\" are the \"Clauses\". The \"Embeddings\" \nare then the vector representations of the relevant clauses. On the other hand, the expert \nknowledge base is responsible for storing the risky clauses identified in past contract \nreviews, along with their review results. Its schema includes three fields: \"Checkpoints\", \n\"Clauses\", and \"Reviews\". \"Checkpoints\" stores the contract risk identification \nrequirements of an organization, tailored to different contract characteristics and risk \npreferences. Similar to the project contract base, \"Clauses\" stores specific clauses. \nHowever, these clauses are those that have been identified as risky in the past projects. \n\"Reviews\" records the results of the identification of such clauses, providing \nexplanations as to why certain clauses are considered risky. \nFig. 2. Process of building a construction contract knowledge base. \n \nIt is essential to define the scope covered by these checkpoints, specifically \ndelineating the key risk categories within the construction contract. In general, \ncheckpoints should cover areas such as: Contractual Basics, Permissions and Transfers, \nPayment and Financials, Time and Schedule, Warranty and Performance, Liabilities and \nPenalties, Suspensions and Terminations, Site Condition, Claims and Variations, and \nDesign and Subcontracting. Subsequently, based on the nature of the identification and \ntask requirements, the composition of the checkpoints can be further specified. The \nextent of a checkpoint is determined by the nature of the check, which can be abstract \nor concrete, such as identifying specific amounts or dates. The final checkpoint is drawn \nbased on the task requirements, which can be categorized into key point summaries, \nelement extraction, and logical reasoning. For instance, an abstract logical reasoning \ncheckpoint might state, \"The conditions precedent should be waived by mutual \nagreement between the Project Company and the Contractor.\" Conversely, a concrete \nelement extraction checkpoint might query, \"What is the proportion of the advance \npayment?\" The format of a checkpoint can differ, from declarative statements, as \nillustrated in the prior examples, to interrogative sentences, and within these sentences, \nit is vital to incorporate technical terms specific to the domain. \nIn the expert knowledge base, there is a one-to-many relationship between \n\"Checkpoints\" and \"Reviews\", where a single checkpoint can correspond to multiple \nreviews. This is because, while the descriptions of clauses in different contracts may \nvary, these clauses may present similar risks. For clauses with the same risks, the same \ncheckpoint is used to review these clauses. For example, if two risk clauses \"8.1 \nCommencement of Works; The Financial Closing has occurred in terms of the \nimplementation agreement…\" and \"5.1 Notice to Proceed; The Commencement Date \nshall be the date stated in a written notice…\" have been identified as risky for the same \ncheckpoint \"The Financial Closing Date shall have occurred before the Commencement \nDate\" in different contracts, then these two risk clauses and their respective reviews \nwill share the same checkpoint in the expert knowledge base. \nWith respect to the composition of reviews, it can be structured into three main \nparts: 1. Conclusion; 2. Content of the relevant clauses; 3. Rationale. For instance, \"A \nparticular provision appears to be in conflict with the checkpoint. As per section 8.1 \ntitled 'commencement of works', specifically in sub-clause (a), it is mentioned that the \nobligation can be 'waived or deferred (with or without conditions) by the Employer', \nwhich prescribes that the Financial Closing should transpire prior to the \nCommencement Date. This implies that the requirement for the Financial Closing to \nprecede the Commencement Date can be overruled by the Employer. Consequently, it \nis not definitively ascertainable that the Financial Closing Date will precede the \nCommencement Date.\" The review first directly provides a conclusion regarding the \npotential conflict between the clause and the checkpoint. Subsequently, it elaborates on \nthe specific content of the relevant clause, and finally, it presents the basis for the \njudgement based on the clause's content. \n3.2 Project clauses retrieval \nClause retrieval is a process designed to fetch project contract clauses that are \nhighly associated with a given checkpoint from the pre-built project contract base (as \ndepicted in Fig. 3). This process addresses the challenge posed by the fact that the \nclauses containing key information may be scattered across different locations, often \ninvolving cross-page situations. For example, Clause IV is a time provision that \ndescribes advance payment, yet the definition of advance payment is elaborated in \nClause I. Clause I and Clause IV appear in different locations within the contract. The \nprocedure begins with a manually collected checkpoint, derived from a set of checklists \ncreated by construction contract experts. These checklists focus on major inspection \ntopics. For example, a checkpoint related to the inspection topic of “Precondition of \nCommencement” might include statements such as “Receipt of advance payment is not \none of the commencement preconditions” or “The Owner’s provision of right of access \nto the site is not one of the commencement preconditions”.  \nBefore searching for relevant clauses within the project contract base, the \ncheckpoint undergoes vectorization using a text vectorizer, such as the sentence-\ntransformers model provided by Hugging Face (Reimers and Gurevych, 2019), or the \ntext-embedding-ada-002 model provided by OpenAI2.  After vectorization, a semantic \nsearch is performed to retrieve the appropriate clauses. This involves calculating the \nvector similarity between the clauses and the checkpoint. The similarity metric can be \nbased on either cosine similarity or Euclidean distance, both of which should yield the \nsame ranking of results.  \n \nFig. 3. Process of retrieving factual knowledge and expert knowledge. \n3.3 Clause-review pairs retrieval \nThis step aims to acquire the case clauses and their corresponding reviews from the \nexpert knowledge base. The case clauses are risky clauses collected from the past \nproject contracts and the reviews are statements explaining the reason. To retrieve the \n \n2 Public API available at: https://openai.com/api/.  \nclause-review pairs, we conduct a hybrid search operation, instead of pure semantic \nsearch, on the expert knowledge base using the checkpoint and the retrieved project \nclauses. The checkpoint retains its original text format, while the retrieved project \nclauses are transformed into a vector representation, composed of long segment \nmultiple clause i. As shown in Fig. 3, after retrieving top k project clauses associated \nwith checkpoint, according to the order of the similarity scores, we merge all the clause \ni into a singular text and then vectorize it. \nTo retrieve the case clauses relevant to the same checkpoint used to retrieve the \nproject clauses, we first match the same checkpoint in the expert knowledge base. This \nnarrows down the pool of clauses to those most pertinent to the checkpoint, ensuring \nthat the retrieved case clauses possess the type of risk that the project clauses are \npotentially covering. After that, the top k case clauses can be retrieved by calculating \nthe vector similarity between the project clauses and the case clauses. Finally, the most \nrelevant case clauses and their reviews are extracted based on the similarity score. \nConsider the following example: when analyzing the checkpoint \"The condition \nprecedent should be waived by mutual agreement...\" and the corresponding project \nclauses \"The Contract shall come into full force and effect on the Date...\", which \nprovide some details regarding the financial close. There are four distinct case clauses: \n\"The Employer may issue the Notice to Proceed at any time and the Commencement \nDate shall occur...\", \"The Commencement Date shall be the date stated in a written \nnotice to the Contractor from the Employer...\", \"The Company shall issue an NTP if the \nConditions Precedent are defined...\", and \"Commencement Date refers to the date when \nthe Contractor receives the Notice to Proceed from the Project Company...\". The initial \ntwo clauses pertain to the checkpoint \"The Financial Closing Date shall have occurred \nbefore the Commencement Date\", whereas the latter two are associated with \"The \nconditions precedent should only be waived by mutual agreement between the Project \nCompany and the Contractor\". Since the project clauses include references to the \nfinancial close, it is evident that using vector similarity for retrieval will lead to the \nextraction of clauses connected to the financial close risks, specifically the first two \nclauses. Nevertheless, the risk present in these two clauses arises from the financial \nclose itself, rather than the condition precedent being waived through mutual agreement. \nConsequently, these clauses fail to offer insights related to mutual agreement and \ntherefore do not contribute to the language model's understanding of the core of the \ncheckpoint. To this end, it is necessary to first match the case clauses that have similar \nrisks to the project clauses based on the checkpoint using vector similarity calculation. \nAt this point, clause-review pairs related to the checkpoint can be extracted as the expert \nknowledge to enhance the logical reasoning of the language models. \n3.4 Prompt development \nDifferent prompts for the same question can produce diverse answers, which makes \nit challenging to find the “right” prompt to exploit the capabilities of the LLM. \nLeveraging the in-context learning capabilities of LLMs, we present a two-stage \nprompting process for generating robust, stable, and trustworthy results (as depicted in \nFig. 4). The first step begins with a question-answering prompt. The content of the \nprompt is based on the information retrieved in the previous knowledge retrieval stages. \nMultiple distinct suggestions are generated by feeding the prompt into the LLM \nindependently several times. In the second step, we use a zero-shot response selection \nprompt and adopt the idea of majority voting to select the most suitable and promising \nsuggestion. The selection is based on the input context, including the checkpoint and \nspecific conditions. This method employs self-consistent sampling rather than greedy \ndecoding. It improves the accuracy of inference by generating a number of different \nanswers and then applying majority voting. This response selection prompt is repeated \nmultiple times independently, similar to the first step. The final answer for the risk \nidentification is determined by the suggestion that receives the most votes.  \nFig. 4. An illustration of the two-stage prompting \nFig.5 shows the overall structure of the Question-Answering prompt. In tackling \nthe challenge of LLMs struggling to determine the appropriate perspective or logical \nthinking for identifying relevant risks, we provide clause-review pairs as input-output \nexemplars. These exemplars serve as a guide for the LLMs’ thinking process, \nemploying the method of few-shot prompting instead of zero-shot.  \n  \nFig. 5. The structure of Question-Answering prompt. \nThe details of the Question-Answering prompt and Response Selection prompt are \nillustrated in Fig. 6. We have opted for multiple-choice questions as the format for our \nquestions. If the answer is \"contradict\" or \"not found,\" we consider the given particular \ncondition as risky. Here, \"contradict\" implies that the specific condition is in \ncontradiction with the checkpoint, while \"not found\" indicates potential risks to the \nintegrity of the clauses. The contents within curly brackets, such as clause_1 or \nreview_1, represent the information retrieved during the project clauses retrieval and \nclause-review pair retrieval steps. After providing exemplars, we requested the LLMs \nto concisely summarize the given information concisely and complete the subsequent \ntasks based on the summary, which helps LLMs to better understand the intention of \nthe exemplars by summarizing what was provided or learned. Conversely, the Response \nSelection prompt is a standard prompt comprised of an Instruction and an Input context, \nwhere checkpoint and specific condition are identical to the Question-Answering \nprompt, and “choice i” denotes the result of the previous step.  \n \n \nFig. 6. The framework of Question-Answering and Response Selection prompt \nAccording to (Yao et al., 2023), we can formalize the above intact processes for \ninto formula (1) and (2). We use (1) to illustrate the procedure of acquiring suggestions \nusing few-shot prompting with various specific conditions, where 𝑃! represent pre-\ntrained language model with parameters 𝜃, in other words a LLM, x as the input, and \n𝑦\" as the intermediate outputs through LLM with i times (i = 1, 2, …, n). As mentioned \nin (Wang et al., 2022), more samples will result in a consistently better performance. \nWe recommend the value of i is at a minimum of 5. promptIO(x) denote an input-output \nprompting which the input x involves instructions of the risk identification task, i.e., \ntask description, input-output exemplars of Question and Answer pairs form as \ndemonstrations for the LLM, and the input context, namely checkpoint and specific \nconditions. \n𝑦\" ∼𝑃!% 𝑦\" ∣∣𝑝𝑟𝑜𝑚𝑝𝑡#$(𝑥) /\n(1)  \nAfter getting 𝑦\", we use (2) to acquire the final output 𝑦1, i.e. the final answer, \nthrough selecting the most reasonable intermediate output from a set of 𝑦\", where Y \nrepresent a set of 𝑦\", and 𝑝𝑟𝑜𝑚𝑝𝑡%&'(𝑥, 𝑌) denote that a selection prompting which the \ninput x only contains the task description and the input context. \n𝑦1 ∼𝑃!% 𝑦1 ∣∣𝑝𝑟𝑜𝑚𝑝𝑡%&'(𝑥, 𝑌) /\n(2) \n4. \nCase illustration \nThis section present examples that showcase the effectiveness of our approach in \ncomparison to the standard prompt. Our approach involves using clause-review pairs \nas the expert knowledge and the context of the clauses requiring risk identification as \nthe factual knowledge along side our prompts. The standard prompt, on the other hand, \nwas developed according to instructions provided by OpenAI's official resources, only \nrelying on the factual knowledge.  All responses are generated using the GPT4.0 model3. \n4.1 Data preparation \nWe adopted EPC contracts collected from construction projects as our test data. \nOne hundred clauses are extracted from the sample contracts and used for testing on \nthe designated checkpoints, with 50 clauses from each. These selected clauses either \ncontained information that need verification or had superfluous information not \nrequired for the checkpoints. The first and second contracts contain 22 and 7 clauses, \nrespectively, relating to the designation of checkpoints. In addition, we prepared 20 \nclause-review pairs from other contracts, and the corresponding review results are \nprovided by human experts.  \nThe checkpoints used in this demonstration are: \"The Financial Closing Date shall \nhave occurred before the Commencement Date\" and “The conditions precedent should \nonly be waived by mutual agreement between the Project Company and the Contractor”. \nFurther, the specific conditions used for this risk identification are composed of two \nparts: one consists of clauses related to the main elements of the checkpoint, namely \nthe Commencement Date, Financial Closing Date, and Condition Precedents; the other \npart consists of a randomly selected assortment of clauses unrelated to the checkpoint. \nFurthermore, the clause-review pairs are put together by contract experts who identify \nrelevant clauses from other commercial contracts based on the above checkpoints first, \nthen provide the corresponding review results. All the clauses and reviews are saved in \nCSV format, following the data schema mentioned in section 3.1. For more details, \nplease refer to this repository4. \n4.2 Data storage and retrieve \nOne way of leveraging the knowledge base to store and retrieve clauses data is to \ndeploy it through a Docker in local environment. We utilized Milvus5, an open-source \nand highly scalable vector database that supporting vector embeddings, to facilitate the \nstorage and to index the massive vector embeddings. 48 specific conditions and 8 case \nclause-review pairs associated to the corresponding checkpoints were saved as the \n \n3 https://openai.com/gpt-4 \n4 https://github.com/HarrisSK/Knowledge-augmented-LLMs-for-construction-contract-risk-identification/ \n5 Milvus is accessible at: https://milvus.io/.  \n“specific_conditions” and “clause_review” respectively into three collections, and the \ndata schema as well as the unit of text followed section 3.1. Since we use OpenAI's \nembedding model, we set the dimension of the embedding to its output value of 1536. \n To retrieve related specific conditions and case clause-review pairs, Milvus \nprovides an efficient indexing approach, HNSW (Hierarchical Navigable Small World \nMap), to rapidly access the target location, as well as supports the hybrid search. We \nset the index building and search parameters to its recommended values (i.e. Maximum \ndegree of the node = 48, Search scope = 500). After having all the parameters set, we \nimplement Euclidean distance metric to compare the checkpoint to each clause as well \nas to the retrieved specific conditions to each case clause (as illustrated in Fig. 7 and \nFig. 8). We have extracted top 5 clauses out of “specific_condition_1” collection and \ntop 3 clause-review pairs out of “clause_review” collection according to the Euclidean \ndistance score. \n \nFig. 7. Project clauses retrieval algorithm. \n \nFig. 8. Clause-review pairs retrieval algorithm. \n4.3 Identification result comparison \nAfter conducting the necessary steps to determine the specific conditions required \nfor identification, we proceeded to compare the performance of our prompt architecture \nwith and without its use. When not using our architecture, we followed the official \nOpenAI prompt engineering instruction6 and created standard prompts that included \nbasic element such as instruction, context, output format, etc. The structure of the \nstandard prompt is depicted in Fig. 9. All identifications were carried out in new \nconversations, without any historical conversations provided. This means the GPT-4 \nmodel did not have access to any information that may interfere with the task. In the \ncase of using our prompt method, we initially posed the same query five times, utilizing \nthree different clause-review pairs. Then, we apply the voting prompt, executing the \nsame process five times based on the results of the initial queries. To obtain a range of \nsuggestions, we recommend using a temperature value around 0.2-0.4 if temperature \nsampling is employed. The outcome with the highest number of votes was recognized \nas the risk identification result for the clause. For a fair comparison, we also used the \nstandard prompt five times for the same query. The entire conversation is available in \nthe repository. \n \n6 The official instructions for prompt engineering with ChatGPT: https://platform.openai.com/docs/guides/gpt-\nbest-practices \n \nFig. 9. Architecture of standard prompt for CCRI. \nTable 2 and Table 3 presents the risk identification results of the two clauses for \ncheckpoint 1 and checkpoint 2, respectively. The left-hand side is the response using \nstandard prompt, while the right-hand side shows the response of our method. The \nidentification results from both cases demonstrate that our method alignment with the \nexpert reviews. For \"Clause 1,\" the identification result should be that it is contradictory \nto the “Checkpoint 1”, as the Financial Close can be waived by the Employer, \npotentially resulting in the Financial Closing Date not occurring before the \nCommencement Date. For \"Clause 2,\" the identification result should be that it entails \nwith the “Checkpoint 2”, as the statement in “Clause 2” allows a party to unilaterally \nwaive certain provisions, but these provisions are limited to those protecting its own \nrights and interest, without harming the other party. Thus, it does not violate the \nrequirement for mutual consent to waive a condition precedent.  \nThe result from identifying “Clause 1” and \"Clause 2” failed to align with those of \nthe human expert reviews when using standard prompt and not providing the clause-\nreview pairs. In the case of “Clause 1”, the LLM only grasped the overall clause \nstructure without a deep analysis of its content. The risk associated with this clause, \nwhich allows the Employer to waive the Financial Close, was not identified, leading to \nthe incorrect conclusion that “Clause 1” entails with the “Checkpoint 1”. Similarly, for \n“Clause 2”, the LLM only assessed compliance with the “Checkpoint 2” based on the \nlogic of “if the waiver is made without mutual consent.” Contradictions with the \n“Checkpoint 2” arise when the sentence contradicts this logic. The issue here is that the \nLLM fails to fully grasp the intended meaning of the clause. \nCheckpoint 1: The Financial Closing Date shall have occurred before the \nCommencement Date.  \nClause 1: “Commencement Date” means the date on which the conditions below \nhave been satisfied or waived in accordance with the following sentence. The below \ncondition (i) may be waived by the Employer under the condition that Contractor has \nreceived payment of the milestone defined as payment no. 2 in Schedule 4 (as down-\npayment, i.e. regardless of the conditions established for milestone no. 2), the below \nconditions (iii) and (vi) may be waived by the Employer and the below conditions \n(ii), (iv) and (v) may be waived by the Contractor: \n(i) Financial Close; \n(ii) the Contractor has received the down-payment from the Employer in the amount \nof 5% of the Contract Price as established as payment no,1 in Schedule 4; \n(iii) not used; … \nStandard: Condition situation: Entail. \nExplanation: The specific condition does \nentail the checkpoint. The checkpoint \nstates that the Financial Closing Date \nshould occur before the Commencement \nDate. In the specific condition, it is \ndetailed that the \"Commencement Date\" \nis the date on which certain conditions \nhave been satisfied or waived, and one of \nthose conditions is the Financial Close. \nThis essentially means the same thing as \nthe Financial Closing Date. Thus, it is \nclear that the Financial Closing Date (or \nthe Financial Close) is intended to occur \nbefore the Commencement Date, which \nis in alignment with the checkpoint. \nOurs: The specific condition contradicts \nthe checkpoint. According to the provided \nspecific conditions, the requirement for \nFinancial Closing (stated as condition (i)) \ncan be waived by the Employer under \ncertain circumstances. Therefore, it is not \ndefinite that the Financial Closing Date \nwill occur before the Commencement \nDate. The answer is therefore [A] \ncontradicts. \nTable 2. An excerpt of risk identification result for checkpoint 1. \nCheckpoint 2: The conditions precedent should only be waived by mutual agreement \nbetween the Project Company and the Contractor. \nClause 2: “Effective Date” means the date upon which the EPC Contractor receives \na notice from the Owner requiring the EPC Contractor to proceed with the Works \n(the “Full Notice to Proceed”). The Full Notice to Proceed shall not be issued until \neach of the following conditions precedent has been satisfied, or waived by the \nOwner (in the case of articles 5.1(a), 5.1(b) and 5.1(e) of this Agreement), by the EPC \nContractor (in the case of articles 5.1(c) and 5.1(f) of this Agreement) and by both \nParties (in the case of articles 5.1(d) and 5.1(g) of this Agreement): (a) the Owner has \nreceived a certificate in respect of each of the following entities signed by a duly \nauthorised officer of such entity:… \nStandard: \nCondition \nsituation: \nContradict. Explanation: The checkpoint \ncondition specifies that the conditions \nprecedent should be waived by mutual \nagreement between the Project Company \nand the Contractor. However, in the \nspecific \ncondition, \nthe \nconditions \nprecedent are to be waived by the Owner \nfor articles 5.1(a), 5.1(b), and 5.1(e), by \nthe EPC Contractor for articles 5.1(c) \nand 5.1(f), and by both parties for articles \n5.1(d) and 5.1(g). This creates a \ncontradiction as the parties involved in \nwaiving the conditions precedent in the \nspecific condition do not align with the \nparty involvement in the checkpoint \ncondition. \nOurs: \nExplanation: \nThe \nspecific \ncondition entails the checkpoint. The \ncondition explicitly states that certain \nconditions can be waived by the Owner \n(representing the Project Company), \nothers can be waived by the EPC \nContractor, and a few can be waived by \nboth parties. This distribution of authority \naligns with the checkpoint's requirement \nthat waivers should be made by mutual \nagreement, ensuring the interests of both \nparties. However, it also allows for \nindividual waivers where the conditions \nsolely concern the party providing the \nwaiver, ensuring no compromise on their \ninterests. The answer is therefore [B] \nentails. \nTable 3. An excerpt of risk identification result for checkpoint 2. \n5. Discussion \nThe proposed method aims to develop an automatic pipeline for contract review \nand risk detection based on knowledge augmented LLM. We demonstrated the \nproficiency of our method with selected practical construction contracts. In order to \nadopt our method in contracts of a wider domain, we offer some insights and \napprehensions that may arise during the implementation of the method in real-world \nscenarios. \n5.1 “Logical thinking” of LLM \nWe observe from our experiment that accurate risk review conclusion comes with \nsubtle human intervention. It is crucial to note that LLM has a higher tendency to follow \nthe logical reasoning of human-provided demonstrations. Especially in contract review \ndomain which requires higher level of alignment with experts, it is essential to carefully \nprovide the demonstrations (clause-review pairs as in this paper). \nBased on our experimental observations, there are two primary factors influence the \nefficacy of providing the clause-review pair to the LLM in emulating the thinking \npattern of the contract experts, namely the quantity of clause-review pairs and the \ndegree of similarity between the case clause and the risk clause. Regarding the former, \nwhen the provided case clause closely resembles the clause that requires scrutiny, both \na single clause-review pair and three pairs can prompt the LLM to emulate the expert's \nthinking pattern in assessing risk clauses, implying that the LLM will review related \nclauses based on similar risk preferences. We also observed that even if discrepancies \nexist in the risk rationale for the same checkpoint among multiple similar clause-review \npairs, the LLM's ability to mimic the experts’ thinking pattern remains. As for the latter, \nwhen only one clause-review pair is provided and the case clause within that pair is not \nhighly similar to the risk clause needing examination, the clause-review pair does not \nsufficiently stimulate the LLM to adopt the appropriate analytical mindset. Taking \nclause 1 from the case illustration as an example, when a less correlated clause-review \npair is provided and the Euclidean distance between the case clause and clause 1 is 0.31, \nthe LLM fails to accurately identify the inherent risks in clause 1. This might be due to \nthe provided clause-review pair emphasizes the risk arising from the absence of the \ndescription stating \"The Financial Closing Date shall have occurred before the \nCommencement Date\", rather than from the provision allowing the employer to waive, \nwhich results in the risk. \n Risks in construction contracts frequently appear in sentences with such subtle \ndescriptive variations. Identifying such risks requires the ability to apply specific \nknowledge to certain circumstances. It is important to note that while LLMs can mimic \nlogical reasoning, they do not truly “understand” or “think” about the topics the same \nway humans do. In the first example presented in section 4, the occurrence of the risk \nis related to risk preference, and the difference in responses depends on whether \nexternal knowledge of risk preference is provided. For a general language model \nwithout specific risk preferences, the first answer is reasonable. The risk of this clause \nlies in the preference level of \"or\" in \"satisfied or waived.\" In the response utilizing our \nmethod, we provide a review pair with similar clauses. The clause-review pair, serving \nas knowledge enhancement, emphasizes the risk preference for \"or\" and highlights the \nrisk caused by the uncertainty brought about by \"or\" to some extent.  \nHumans excel at learning from examples. Confronted with an unfamiliar issue, we \noften reason and derive solutions by comparing it with familiar situations. The results \nof our method suggest that LLMs demonstrate a similar ability, leading some to \npostulate that they exhibit a form of  “logical thinking”. However, the perceived \"logical \nthinking\" of LLMs is merely a result of recognizing and generating patterns from a \nmassive dataset. While they lack human-like cognition and reasoning abilities, they can \nemulate logical reasoning to a significant extent based on the data and the appropriate \nparadigm, namely the provided knowledge and the prompt (P. Liu et al., 2023). Just as \ndata inherently sets the upper bound of a model's potential, the inclusion of informed \nprofessional analysis and well-defined risk preferences shape the judgment of a \nlanguage model in risk identification. As a result, the construction of a comprehensive \nand precise knowledge base is fundamental to this endeavor. Furthermore, the \ncontinuous accumulation of risk-related clauses and expert analyses through practice \ncontributes to sustainably performance improvement. \nTraditionally, supervised learning-based models predict outputs based on inputs. In \ncontrast, prompt-based learning models use language models to predict the probability \nof a text sequence. In this new paradigm, the initial input undergoes transformation \nthrough a template, resulting in a \"prompt\" with unfilled slots. The language model then \nprobabilistically fills these slots to obtain the final output. Such an approach, with well-\ncrafted prompts, enables the language model to flexibly perform few-shot or zero-shot \nlearning, and even emulate \"logical thinking.\" Therefore, for the language model to \nproduce trustworthy and human-interpretable responses, prompts with sufficient \ncontext, clarity, and specificity are essential, particularly for highly relevant knowledge \nin downstream tasks. \n5.2 Evaluation criterion for CCRI tasks \nAlthough the risk review using our method can greatly improve the contract review \nefficiency, there doesn’t exist a comprehensive evaluation metrics for evaluating the \nLLM generated content for contract review task. The establishment of such evaluation \nmetrics faces a major challenge due to the diverse and complex evaluation standard in \nreal-world scenarios. \nCurrently, the evaluation primarily relies on the feedback of human experts, using \nit as the ground truth. The expert assesses the response based on its logic, complexity, \naccuracy, and traceability. Specifically, they determine whether the response actually \nrefers to the provided clauses, if its logic is sound, and if it accurately identifies the \npoint of contradiction. To improve the efficiency of evaluations and mitigate the human \nsubjective bias, one promising approach is to establish an iterative, automated expert \nevaluation system. Potential methods include using a knowledge graph for explicit \nknowledge inference (Zhang et al., 2020; Zheng et al., 2023). The identification results \nare compared with inference outcomes obtained from the knowledge graph, enabling \nthe evaluation of answers generated by large language models from a logical inference \nperspective. On the other hand, it is also feasible to combine multiple language models, \nallowing them to serve as an evaluator that assesses answers for accuracy based on \nspecific criteria. Answers that deviate from the standards, along with the corresponding \nadjustment strategies, are returned to the language models responsible for identifying \nrisks, which then re-generate them. For instance, Yang’s team (Y. Liu et al., 2023) used \nthe LLMs along with a particular prompting paradigm to act as the evaluator for \nassessing natural language generation outputs. Their results show that the proposed \nmethod outperforms all previous methods. Furthermore, the establishment of relevant \nbenchmark datasets is crucial for comprehensive performance evaluation of these tasks. \nExamples of benchmark datasets in other domains can be referenced in the works of \n(Chalkidis et al., 2022). \n6. \nConclusion \nThe paper presents a knowledge augmented language model methodology for CCRI, \nwhich integrates construction contract knowledge into LLMs. This approach \nincorporates domain-specific knowledge, contract clauses, and expert reviews to \nimprove the efficiency and accuracy of contract-specific knowledge retrieval.  The \nexperiment results demonstrate the effectiveness of the method and provide valuable \nperspectives for discussion. \nWhile the presented method performs well in the experiment, it has not yet been \nimplemented in real-world scenarios that may involve a larger domain knowledge base. \nOn one hand, a larger knowledge base could enhance the language model's \ncomprehension by providing more prior knowledge. On the other hand, it may pose \nchallenges in identifying the best few-shot learning materials. Therefore, it is \nrecommended to collaborate with industrial practitioners to further investigate this \ndirection. \n7. Data availability \nAll data are available at https://github.com/HarrisSK/Knowledge-augmented-LLMs-\nfor-construction-contract-risk-identification/. \n \nAcknowledgement \nThis research was funded by the National Natural Science Foundation of China (grant \nno. 71971196). \n \nReference \nAl Qady, M., Kandil, A., 2010. Concept relation extraction from construction \ndocuments using natural language processing. Journal of construction engineering and \nmanagement 136, 294–302. \nArtan Ilter, D., Bakioglu, G., 2018. Modeling the relationship between risk and dispute \nin subcontractor contracts. Journal of Legal Affairs and Dispute Resolution in \nEngineering and Construction 10, 04517022. \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, \nA., Shyam, P., Sastry, G., Askell, A., others, 2020. Language models are few-shot \nlearners. Advances in neural information processing systems 33, 1877–1901. \nCandaş, A.B., Tokdemir, O.B., 2022. Automating Coordination Efforts for Reviewing \nConstruction Contracts with Multilabel Text Classification. Journal of Construction \nEngineering and Management 148, 04022027. \nChalkidis, I., Jana, A., Hartung, D., Bommarito, M., Androutsopoulos, I., Katz, D.M., \nAletras, N., 2021. LexGLUE: A benchmark dataset for legal language understanding in \nEnglish. arXiv preprint arXiv:2110.00976. \nChoi, S.-W., Lee, E.-B., 2022. Contractor’s risk analysis of engineering procurement \nand construction (EPC) contracts using ontological semantic model and Bi-long short-\nterm memory (LSTM) technology. Sustainability 14, 6938. \nCui, J., Li, Z., Yan, Y., Chen, B., Yuan, L., 2023. Chatlaw: Open-source legal large \nlanguage model with integrated external knowledge bases. arXiv preprint \narXiv:2306.16092. \nDing, Y., Ma, J., Luo, X., 2022. Applications of natural language processing in \nconstruction. \nAutomation \nin \nConstruction \n136, \n104169. \nhttps://doi.org/10.1016/j.autcon.2022.104169 \nFang, W., Luo, H., Xu, S., Love, P.E., Lu, Z., Ye, C., 2020. Automated text \nclassification of near-misses from safety reports: An improved deep learning approach. \nAdvanced Engineering Informatics 44, 101060. \nFu, Y., Xu, C., Zhang, L., Chen, Y., 2023. Control, coordination, and adaptation \nfunctions in construction contracts: A machine-coding model. Automation in \nConstruction 152, 104890. \nGao, J., Xiong, C., Bennett, P., Craswell, N., 2022. Neural Approaches to \nConversational Information Retrieval. \nGuu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M., 2020. Retrieval augmented \nlanguage model pre-training, in: International Conference on Machine Learning. \nPMLR, pp. 3929–3938. \nHamie, J.M., Abdul-Malak, M.-A.U., 2018. Model language for specifying the \nconstruction contract’s order-of-precedence clause. Journal of Legal Affairs and \nDispute Resolution in Engineering and Construction 10, 04518011. \nHassan, F.U., Le, T., 2021. Computer-assisted separation of design-build contract \nrequirements to support subcontract drafting. Automation in Construction 122, 103479. \nhttps://doi.org/10.1016/j.autcon.2020.103479 \nHassan, F.U., Le, T., Lv, X., 2021. Addressing Legal and Contractual Matters in \nConstruction Using Natural Language Processing: A Critical Review. J. Constr. Eng. \nManage. 147, 03121004. https://doi.org/10.1061/(ASCE)CO.1943-7862.0002122 \nHassan, F. ul, Le, T., 2020. Automated requirements identification from construction \ncontract documents using natural language processing. Journal of Legal Affairs and \nDispute Resolution in Engineering and Construction 12, 04520009. \nHe, H., Zhang, H., Roth, D., 2022. Rethinking with retrieval: Faithful large language \nmodel inference. arXiv preprint arXiv:2301.00303. \nHu, Z., Xu, Y., Yu, W., Wang, S., Yang, Z., Zhu, C., Chang, K.-W., Sun, Y., 2022. \nEmpowering Language Models with Knowledge Graph Reasoning for Question \nAnswering. arXiv preprint arXiv:2211.08380. \nHuang, Q., Tao, M., An, Z., Zhang, C., Jiang, C., Chen, Z., Wu, Z., Feng, Y., 2023. \nLawyer LLaMA Technical Report. arXiv preprint arXiv:2305.15062. \nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., \nFung, P., 2023. Survey of hallucination in natural language generation. ACM \nComputing Surveys 55, 1–38. \nKim, Y., Bang, S., Sohn, J., Kim, H., 2022. Question answering method for \ninfrastructure damage information retrieval from textual data using bidirectional \nencoder representations from transformers. Automation in construction 134, 104061. \nKim, Y., Lee, J., Lee, E.-B., Lee, J.-H., 2020. Application of natural language \nprocessing (NLP) and text-mining of big-data to engineering-procurement-construction \n(EPC) bid and contract documents, in: 2020 6th Conference on Data Science and \nMachine Learning Applications (CDMA). IEEE, pp. 123–128. \nLazaridou, A., Gribovskaya, E., Stokowiec, W., Grigorev, N., 2022. Internet-\naugmented language models through few-shot prompting for open-domain question \nanswering. arXiv preprint arXiv:2203.05115. \nLee, J., Ham, Y., Yi, J.-S., Son, J., 2020. Effective risk positioning through automated \nidentification of missing contract conditions from the contractor’s perspective based on \nFIDIC contract cases. Journal of Management in Engineering 36, 05020003. \nLee, J., Yi, J.-S., 2017. Predicting project’s uncertainty risk in the bidding process by \nintegrating unstructured text data and structured numerical data using text mining. \nApplied Sciences 7, 1141. \nLee, J., Yi, J.-S., Son, J., 2019. Development of automatic-extraction model of \npoisonous clauses in international construction contracts using rule-based NLP. Journal \nof Computing in Civil Engineering 33, 04019003. \nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., \nNarayanan, D., Wu, Y., Kumar, A., 2022. Holistic evaluation of language models. \narXiv preprint arXiv:2211.09110. \nLiu, H., Gegov, A., Stahl, F., 2014. Categorization and Construction of Rule Based \nSystems, in: Mladenov, V., Jayne, C., Iliadis, L. (Eds.), Engineering Applications of \nNeural Networks, Communications in Computer and Information Science. Springer \nInternational Publishing, Cham, pp. 183–194. https://doi.org/10.1007/978-3-319-\n11071-4_18 \nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G., 2023. Pre-train, prompt, \nand predict: A systematic survey of prompting methods in natural language processing. \nACM Computing Surveys 55, 1–35. \nLiu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., Zhu, C., 2023. Gpteval: Nlg evaluation using \ngpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. \nLiu, Y., Wan, Y., He, L., Peng, H., Philip, S.Y., 2021. Kg-bart: Knowledge graph-\naugmented bart for generative commonsense reasoning, in: Proceedings of the AAAI \nConference on Artificial Intelligence. pp. 6418–6425. \nMialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, \nB., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., 2023. Augmented language models: a \nsurvey. arXiv preprint arXiv:2302.07842. \nMinaee, S., Kalchbrenner, N., Cambria, E., Nikzad, N., Chenaghlu, M., Gao, J., 2021. \nDeep learning--based text classification: a comprehensive review. ACM computing \nsurveys (CSUR) 54, 1–40. \nMohamed Hassan, H.A., Marengo, E., Nutt, W., 2022. A BERT-Based Model for \nQuestion Answering on Construction Incident Reports, in: International Conference on \nApplications of Natural Language to Information Systems. Springer, pp. 215–223. \nMoon, S., Chi, S., Im, S.-B., 2022. Automated detection of contractual risk clauses from \nconstruction specifications using bidirectional encoder representations from \ntransformers \n(BERT). \nAutomation \nin \nConstruction \n142, \n104465. \nhttps://doi.org/10.1016/j.autcon.2022.104465 \nMoon, S., Lee, G., Chi, S., Oh, H., 2021. Automated construction specification review \nwith named entity recognition using natural language processing. Journal of \nConstruction Engineering and Management 147, 04020147. \nNiu, J., Issa, R.R., 2013. Conceptualizing methodology for building an ontology for \nconstruction claim knowledge, in: Computing in Civil Engineering (2013). pp. 492–\n499. \nNye, M., Andreassen, A.J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, \nD., Lewkowycz, A., Bosma, M., Luan, D., 2021. Show your work: Scratchpads for \nintermediate computation with language models. arXiv preprint arXiv:2112.00114. \nOpenAI, 2023. GPT-4 Technical Report. arXiv prepint arXiv:2303.08774. \nOtter, D.W., Medina, J.R., Kalita, J.K., 2019. A Survey of the Usages of Deep Learning \nin Natural Language Processing. \nPadhy, J., Jagannathan, M., Kumar Delhi, V.S., 2021. Application of natural language \nprocessing to automatically identify exculpatory clauses in construction contracts. \nJournal of Legal Affairs and Dispute Resolution in Engineering and Construction 13, \n04521035. \nPeng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., \nChen, W., 2023. Check your facts and try again: Improving large language models with \nexternal knowledge and automated feedback. arXiv preprint arXiv:2302.12813. \nReimers, N., Gurevych, I., 2019. Sentence-bert: Sentence embeddings using siamese \nbert-networks. arXiv preprint arXiv:1908.10084. \nSalama, D.M., El-Gohary, N.M., 2016. Semantic Text Classification for Supporting \nAutomated Compliance Checking in Construction. J. Comput. Civ. Eng. 30, 04014106. \nhttps://doi.org/10.1061/(ASCE)CP.1943-5487.0000301 \nSen, P., Mavadia, S., Saffari, A., 2023. Knowledge graph-augmented language models \nfor complex question answering. \nShen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y., 2023. Hugginggpt: Solving ai \ntasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580. \nShuai, B., 2023. A rationale-augmented NLP framework to identify unilateral \ncontractual change risk for construction projects. Computers in Industry 149, 103940. \nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, \nA., Bos, T., Baker, L., Du, Y., 2022. Lamda: Language models for dialog applications. \narXiv preprint arXiv:2201.08239. \nTinn, R., Cheng, H., Gu, Y., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H., \n2023. Fine-tuning large neural language models for biomedical natural language \nprocessing. Patterns 4. \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, \nB., Goyal, N., Hambro, E., Azhar, F., 2023. Llama: Open and efficient foundation \nlanguage models. arXiv preprint arXiv:2302.13971. \nul Hassan, F., Le, T., 2021. Computer-assisted separation of design-build contract \nrequirements to support subcontract drafting. Automation in Construction 122, 103479. \nul Hassan, F., Le, T., Tran, D.-H., 2020. Multi-class categorization of design-build \ncontract requirements using text mining and natural language processing techniques, in: \nConstruction Research Congress 2020. American Society of Civil Engineers Reston, \nVA, pp. 1266–1274. \nUl Hassan, F., Le, T., Tran, D.-H., 2020. Multi-Class Categorization of Design-Build \nContract Requirements Using Text Mining and Natural Language Processing \nTechniques, in: Construction Research Congress 2020. Presented at the Construction \nResearch Congress 2020, American Society of Civil Engineers, Tempe, Arizona, pp. \n1266–1274. https://doi.org/10.1061/9780784482889.135 \nValpeters, M., Kireev, I., Ivanov, N., 2018. Application of machine learning methods \nin big data analytics at management of contracts in the construction industry, in: \nMATEC Web of Conferences. EDP Sciences, p. 01106. \nVictoria, P., Kevin, M., Gary, K., Jordi, R., Paul, M., 2022. Global Construction \nDisputes Report. \nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, \nD., 2022. Self-consistency improves chain of thought reasoning in language models. \narXiv preprint arXiv:2203.11171. \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., \n2022. Chain-of-thought prompting elicits reasoning in large language models. \nAdvances in Neural Information Processing Systems 35, 24824–24837. \nXue, X., Hou, Y., Zhang, J., 2022. Automated Construction Contract Summarization \nUsing Natural Language Processing and Deep Learning, in: ISARC. Proceedings of the \nInternational Symposium on Automation and Robotics in Construction. IAARC \nPublications, pp. 459–466. \nYang, J., Chen, Y., Yao, H., Zhang, B., 2022. Machine learning–driven model to \nanalyze particular conditions of contracts: A multifunctional and risk perspective. \nJournal of Management in Engineering 38, 04022036. \nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan, K., 2023. \nTree of thoughts: Deliberate problem solving with large language models. arXiv \npreprint arXiv:2305.10601. \nYu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y., Ren, X., Yang, Y., Zeng, M., \n2021. Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question \nanswering. arXiv preprint arXiv:2110.04330. \nZhang, D., Wang, H., Ding, Y., 2020. A review of inference methods based on \nknowledge graph. Fuzzy Systems and Data Mining VI 492–513. \nZhang, J., El-Gohary, N.M., 2016. Extending Building Information Models \nSemiautomatically Using Semantic Natural Language Processing Techniques. J. \nComput. \nCiv. \nEng. \n30, \nC4016004. \nhttps://doi.org/10.1061/(ASCE)CP.1943-\n5487.0000536 \nZhang, Q., Xue, C., Su, X., Zhou, P., Wang, X., Zhang, J., 2023. Named entity \nrecognition for Chinese construction documents based on conditional random field. \nFrontiers of Engineering Management 10, 237–249. https://doi.org/10.1007/s42524-\n021-0179-8 \nZhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, \nJ., Dong, Z., 2023. A survey of large language models. arXiv preprint \narXiv:2303.18223. \nZheng C.M., Wong, S.K., Su, X., Tang, Y.Q., 2023. A knowledge representation \napproach \nfor \nconstruction \ncontract \nknowledge \nmodeling. \narXiv \nprepint \narXiv:2309.12132. \n \n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2023-09-22",
  "updated": "2023-09-22"
}