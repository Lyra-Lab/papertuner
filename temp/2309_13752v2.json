{
  "id": "http://arxiv.org/abs/2309.13752v2",
  "title": "Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning",
  "authors": [
    "Hongyan Zhou",
    "Yao Liang"
  ],
  "abstract": "The current learning process of deep learning, regardless of any deep neural\nnetwork (DNN) architecture and/or learning algorithm used, is essentially a\nsingle resolution training. We explore multiresolution learning and show that\nmultiresolution learning can significantly improve robustness of DNN models for\nboth 1D signal and 2D signal (image) prediction problems. We demonstrate this\nimprovement in terms of both noise and adversarial robustness as well as with\nsmall training dataset size. Our results also suggest that it may not be\nnecessary to trade standard accuracy for robustness with multiresolution\nlearning, which is, interestingly, contrary to the observation obtained from\nthe traditional single resolution learning setting.",
  "text": " \n \nHongyan Zhou, Yao Liang \n \n  \nAbstract‚ÄîThe current learning process of deep learning, \nregardless of any deep neural network (DNN) architecture \nand/or learning algorithm used, is essentially a single \nresolution training. We explore multiresolution learning \nand show that multiresolution learning can significantly \nimprove robustness of DNN models for both 1D signal and \n2D signal (image) prediction problems. We demonstrate \nthis improvement in terms of both noise and adversarial \nrobustness as well as with small training dataset size. Our \nresults also suggest that it may not be necessary to trade \nstandard accuracy for robustness with multiresolution \nlearning, which is, interestingly, contrary to the observation \nobtained from the traditional single resolution learning \nsetting.  \n \n \nIndex \nTerms‚ÄîMultiresolution \ndeep \nlearning, \nRobustness, \nWavelet, 1D/2D signal classification \n \n \nI. INTRODUCTION \neep neural network (DNN) models have achieved \nbreakthrough performance on a number of challenging \nproblems in machine learning, being widely adopted \nfor increasingly complex and large-scale problems including \nimage classification, speech recognition, language translation, \ndrug discovery, and self-driving vehicles (e.g., [1], [2], [3], [4]). \nHowever, it has been revealed that these deep learning models \nsuffer from brittleness, being highly vulnerable to even small \nand imperceptible perturbations of the input data known as \nadversarial examples (see the survey [5] and the references \ntherein).  Remarkably, a recent study has shown that perturbing \neven one pixel of an input image could completely fool DNN \nmodels [6].  Robustness in machine learning is of paramount \nimportance in both theoretical and practical perspectives.  \n \nH. Zhou is with the Department of Computer and Information Science, \nIndiana University Purdue University, Indianapolis, IN 46202 USA (e-mail: \nzhou94@iu.edu).   \n \nThis intrinsic vulnerability of DNN presents one of the most \ncritical challenges in DNN modeling, which has gained \nsignificant attention. Existing efforts on improving DNN \nrobustness can be classified into three categories: (1) Brute-\nforce adversarial training, i.e., adding newly discovered or \ngenerated adversarial inputs into training data set (e.g., [7, 8]), \nwhich dramatically increases training data size; (2) modifying \nDNN architecture, by adding more layers/subnetworks or \ndesigning robust DNN architecture (e.g., [9, 10]); and (3) \nmodifying \ntraining \nalgorithm \nor \nintroducing \nnew \nregularizations (e.g., [11, 12]). Unlike those existing works, we \ntake a different direction. We focus on how to improve the \nlearning process of neural networks in general. The observation \nis that the current learning process, regardless of DNN \narchitecture and/or learning algorithm (e.g., backpropagation) \nused, is essentially a single resolution training. It has been \nshown that for shallow neural networks (SNN) this traditional \nsingle resolution learning process is limited in its efficacy, \nwhereas multiresolution learning can significantly improve \nmodels‚Äô robustness and generalization [13, 14]. We are \ninterested in investigating if such multiresolution learning \nparadigm can also be effective for DNN models as well to \naddress their found vulnerability. In particular, the original \nmultiresolution learning was mainly focused on 1-demisional \n(1D) signal prediction, which needs to be extended for 2-\ndemisioal (2D) signal prediction tasks such as image \nclassification. \n \nIn this work, we show that the multiresolution learning \nparadigm is effective for deep convolutional neural networks \n(CNN) as well, particularly in improving the robustness of DNN \nmodels. We further extend the original multiresolution learning \nto 2D signal (i.e., image) prediction problem and demonstrate its \nefficacy. Our exploration also shows that with multiresolution \nlearning it may not be necessary to have the so-called tradeoff \nY. Liang is with the Department of Computer and Information Science, \nIndiana University Purdue University, Indianapolis, IN 46202 USA (e-\nmail: yaoliang@iu.edu). Corresponding author.   \n \n \nImproving Robustness of Deep Convolutional \nNeural Networks via Multiresolution Learning\nD \narXiv:2309.13752 [cs.LG] 22 Sep 2023 \n \n \n2 \n \nbetween the accuracy and robustness, which seems contrary to \nthe recent findings of [15] in traditional single resolution learning \nsetting.  \nII. MULTIRESOLUTION LEARNING \nA. The main idea \nThe multiresolution learning paradigm for neural network \nmodels was originally proposed in [13, 14], and was \ndemonstrated in applications to VBR video traffic prediction \n[16, 17, 18]. It is based on the observation that the traditional \nlearning process, regardless of the neural network architectures \nand learning algorithms used, is basically a single-resolution \nlearning paradigm. From multiresolution analysis framework \n[19] in wavelet theory, any signal can have a multiresolution \ndecomposition in a tree-like hierarchical way. This systematic \nwavelet representation of signals directly uncovers underlying \nstructures and patterns of individual signals at their different \nresolution levels, which could be hidden or much complicated \nat the finest resolution level of original signals. Namely, any \ncomplex signal can be decomposed into a simplified \napproximation (i.e., a coarser resolution) signal plus a ‚Äúdetail‚Äù \nsignal which is the difference between the original and the \napproximation signals. A low pass filter L is used to generate \nthe coarse approximation signal (i.e., Low frequency \ncomponent) while a high pass filter H is used to generate the \ndetail signal (i.e., High frequency component) as shown in \nEquations (1) and (2), respectively, where ùëì! denotes the signal \napproximation at resolution level i (i √é Z and i ‚â• 0), and ùëë! \ndenotes the detail signal at resolution level i. This \ndecomposition process can be iterated on approximation signals \nuntil an appropriate level of approximation is achieved. For the \nsake of notation convenience, ùêø\" (Layer 0) denotes the original \nsignal ùëì# at the finest resolution level m, whereas ùêø! and ùêª! \ndenote approximation ùëì#$!  and detail ùëë#$!  signals, \nrespectively, at Layer i of signal decomposition. \nùëì!$% = ùë≥ùëì!,   ùëñ= 1, 2, ‚Ä¶ , ùëö.                            (1) \nùëë!$% = ùëØùëì!,   ùëñ= 1, 2, ‚Ä¶ , ùëö.                           (2) \nThis signal decomposition is reversable, meaning that a \ncoarser resolution approximation plus its corresponding detail \nsignal can reconstruct a finer resolution version of the signal \nrecursively without information loss.  One level of signal \nreconstruction is as follows: \nùëì! = ùëì!$%‚ü®+‚ü©ùëë!$% , ùëñ= 1, 2, ‚Ä¶ , ùëö.                     (3) \nwhere operator ‚ü®+‚ü© denotes the reconstruction operation. Fig. 1 \nshows the decomposition hierarchy of signal ùëì!  from the \noriginal finest resolution level m to resolution level m-4. As we \ncan see, by applying low pass filter L and high pass filter H on \nsignal ùêø\"(ùëì#), we obtain ùêø% and ùêª%. Recursively, ùêø% is further \ndeconstructed into ùêø& and ùêª&, and so on. \n \nFig. 1. Illustration of decomposition structure of 1D signal ùëì! from \nthe original finest resolution to lower resolution levels. \nUtilizing the mathematical framework of multiresolution \nanalysis [19], multiresolution learning [13, 14] explores and \nexploits the different signal resolutions in neural network learning, \nwhere the entire training process of neural network models is \ndivided into multiple learning phases associated with different \nresolution approximation signals, from the coarsest version to \nfinest version. This is in contrast with the traditional learning \nprocess widely employed in today‚Äôs deep learning, where only the \noriginal finest resolution signal is used in the entire training \nprocess. The main idea of multiresolution learning is that for \nmodels to learn any complex signal better, it would be more \neffective to start training with a significantly simplified \napproximation version of the signal with a lot of details removed, \nthen progressively adding more and more details and thus refining \nthe neural network models as the training proceeds. This paradigm \nembodies the principle of divide and conquer applied to \ninformation science.  \n \nIn multiresolution learning, discrete wavelet transform (DWT) \ncan be employed for signal decomposition, whereas inverse DWT \n(IDWT) can be used to form simplified approximation signals via \nthe replacement of the detail signal by zero signal in the signal \nreconstruction. To build a neural network model for a given task of \nsampled signal ùëì#, multiresolution learning can be outlined as \nfollows [13, 14].  \n \nFirst, decompose the original signal ùëì# to obtain ùëì#$%, ùëì#$&, \n‚Ä¶, etc., based on which one reconstructs multiresolution versions \nof training data, where each different resolution approximation \nshould maintain the dimension of the original signal that matches \nthe input dimension of neural network model. Namely, reconstruct \nk multiresolution versions ùëü! (ùëñ= 1, 2, ‚Ä¶ , ùëò) of the original signal \ntraining data, where the representation of training data ùëü!  at \nresolution version i is formed as follows:      \nùëü!\n= 7 ùëì#,                                                                                       \nùëñ= 1\n \n(‚Ä¶ ((ùëì#$!'%‚ü®+‚ü©0#$!'%)‚ü®+‚ü©0#$!'&)‚ü®+‚ü©‚Ä¶ ‚ü®+‚ü©0#$%),\nùëñ> 1\n  \n                                                                                                      \n(4)   \nwhere 0( indicates a zero signal at resolution level j in \nmultiresolution analysis.  \n \n \n3 \n \n \nFig. 2. Illustration of the information content of 1D signal training data at \nfive different resolution levels, which can be organized into five \ninformation layers. The top layer (Layer 0) corresponds to original signal \nat the finest resolution level m, whereas the bottom layer (Layer 4) \nindicates the information content contained in the generated training data \nat the coarsest resolution level m ‚Äì 4. The coarser resolution, the less \ndetailed information it contains.  \n \nFig. 2 illustrates how information content varies in k \nmultiresolution signal training data generated by (4) from the \noriginal (finest resolution) training data for k = 5. \n \nSecond, the training process of a neural network model is \ndivided into a sequence of learning phases. Let ùê¥!(ùëü!)  be a training \nphase conducted on training data version ùëü! of a coarser resolution \ni with any given learning algorithm. Let A‚ÜíB indicate that A \nactivity is carried out in advance of B activity. Thus, k-level \nmultiresolution learning (k >1) is formulated as an ordered \nsequence of k learning phases associated with the sequence of \napproximation subspaces in multiresolution analysis, which \nsatisfies ùê¥!'%(ùëü!'%) ‚Üí ùê¥!(ùëü!) (i=1, 2, ‚Ä¶, k-1) during the entire \ntraining process of a model.  \n \nIt can be seen that the multiresolution learning paradigm forms \nan ordered sequence of k learning phases stating with an \nappropriate courser resolution version of training data ùëü) , \nproceeding to the finer resolution versions of training data. The \nfinest resolution of training data originally given will be used in the \nlast learning phase ùê¥% (ùëü%). The total number of multiresolution \nlevels k is to be chosen by the user for a given task at hand.  Also, \nthe traditional (single resolution) learning process is just a special \ncase of the multiresolution learning where the total multiresolution \nlevels k=1.    \n \nB. New Extension \nWhile the original multiresolution learning [13, 14] was \nproposed for 1D signal prediction problems, it can be naturally \nextended for 2D signal prediction problems, such as image \nrecognition. In this work, the original multiresolution learning \nparadigm is extended for 2D signal problems as follows. First, \n2D multiresolution analysis is employed.  Basically, 2D \nwavelet decomposition and its reconstruction are applied to \nconstruct coarser resolution approximation versions for 2D \nsignal (training data). For example, 2D DWT can be performed \niterating two orthogonal (horizontal and vertical) 1D DWT. For \nany 2D signal (image) at a given resolution level, four sub-band \ncomponents are obtained by 2D DWT: the low-resolution \napproximation (sub-band LL) of the input image and the other \nthree sub-bands containing the details in three orientations \n(horizontal LH, vertical HL, and diagonal HH) for this image \ndecomposition. \n \nLet ùëì#, a sampled 2D signal at a given resolution level m, be \ndenoted as ùêøùêø\" (Layer 0) in 2D signal decomposition hierarchy. \nThe coarser approximation ùëì#$%obtained by 2D DWT, is then \ndenoted as ùêøùêø%, with a one-quarter size of that of the original \nùëì#; the other three sub-bands ùëë+,\n#$%, ùëë,+\n#$%, ùëë,,\n#$% are denoted \nas ùêøùêª%, ùêªùêø% and ùêªùêª%, generated through two orthogonal 1D \nDWT in the order (L, H), (H, L) and (H, H), respectively. \nSimilarly, this 2D signal decomposition process of ùëì! (i=m, m-\n1, ‚Ä¶, m-k+1) can be iterated until an appropriate coarser \napproximation ùêøùêø)  (i.e., ùëì#$) ) is achieved, as illustrated in \nFig. 3. Again, this 2D signal decomposition is reversable.  One \nlevel of 2D signal reconstruction is as follows: \n ùëì! = ùëì!$%‚ü®+‚ü©ùëë+,\n!$%‚ü®+‚ü© ùëë,+\n!$%‚ü®+‚ü© ùëë,,\n!$%,   ùëñ= 1, 2, ‚Ä¶ , ùëö.       (5) \nWe can rewrite (5) as: \nùêøùêø( = ùêøùêø('%‚ü®+‚ü©ùêøùêª('%‚ü®+‚ü© ùêªùêø('%‚ü®+‚ü© ùêªùêª('%, \n             ùëó= 0, 1, ‚Ä¶ , ùëö‚àí1.                                                    (6)                    \n \nFig. 3. Illustration of three-level wavelet decomposition of 2D signal \nùëì! (ùêøùêø\") from the given resolution level m. \n \nTo construct multiresolution versions of training data, we \ncreate one additional intermediate resolution level for each \nlevel of 2D decomposition to reduce information difference \nbetween resolutions, as shown in the second row in Fig. 4. This \nis one significant difference from the multiresolution learning \nprocedure for 1D signal problem. Hence, the representation of \nmultiresolution training data ùëü!\n&- at resolution level i is \nconstructed as follows: \nùëü!\n&- =\n=\nùëì#,                                                                                                ùëñ= 1\n(‚Ä¶ ((ùêøùêø)‚ü®+‚ü©0))‚ü®+‚ü©0)$%) ‚Ä¶‚ü®+‚ü© 0%),                          ùëñ= 2ùëò+ 1\n(‚Ä¶ ((ùêøùêø)‚ü®+‚ü©ùêøùêª)‚ü®+‚ü©ùêªùêø)‚ü®+‚ü©0,,\n) )‚ü®+‚ü©0)$%) ‚Ä¶ ‚ü®+‚ü© 0%),   ùëñ= 2\u0000\u0000              \n(7) \nwhere 0( = 0+,\n( ‚ü®+‚ü©0,+\n( ‚ü®+‚ü©0,,\n(  , and integer k > 0.  \n \nFig. 4 illustrates the information content of 2D signal training \ndata at five different resolutions. The coarser resolution version \nof training data, the less detailed information it contains. Fig. 5 \ngives an example of four different resolution training data \nconstructed from (7).  \n \n \n \n4 \n \n \nFig. 4. Illustration of the information content of 2D signal training data \nat five different resolution levels, indicated by arrows from the coarsest \nresolution to the finest resolution. The coarser resolution, the less \ndetailed information it contains. \n \n \nFig. 5. An example of a sound sample of log-mel spectrogram \nrepresented in four resolution levels: (a) r#\n$% , (b) r$\n$%, (c) r&\n$%, and (d) \nr'\n$%. \n \nC. Multiresolution Learning Process \nAll the multiresolution training data versions are of the same \ndimension size but associated with different detailed levels of \nthe given signal. For 1D signals, each level of wavelet \ndecomposition will create one new version of coarser resolution \ntraining data, while for 2D signals, each level of wavelet \ndecomposition will create two new versions of coarser \nresolution training data. As the version index i of training data \nùëü!/ùëü!\n&-  increases, the ùëü!/ùëü!\n&-  contain less details, which \nfacilitates to start model training with a significantly simplified \nsignal version even though the original signal could be \nextremely complex. Furthermore, one has the flexibility to \nselect an appropriate wavelet transform and basis for signal \ndecomposition and a total multiresolution levels k in \nmultiresolution learning for a given task.  After each learning \nphase in multiresolution learning, the resulting intermediate \nweights of the DNN model are saved and used as initial weights \nof the model in the next learning phase in this dynamic training \nprocess, until the completion of the final learning phase with \nthe original finest resolution training data ùëü%/ùëü%\n&- (ùëì#). \n \nIII. EXPERIMENTS AND ANALYSES \nWe conduct thorough experiments with three open datasets \nFSDD [20], ESC-10 [21] and CIFAR-10 [22] of audio/image to \nsystematically study and compare the multiresolution learning with \nthe traditional single resolution learning on deep CNN models. \nParticularly, the problem of speech recognition is casted, \nrespectively, to 1D signal classification on data set FSDD, and 2D \nimage classification on data set ESC-10, to illustrate our extended \nmultiresolution learning.  The following subsections describe the \ndata sets used, our experiment setup, and results and evaluation.  \nA. Datasets and Preprocessing \nThe FSDD [20] is a simple audio/speech dataset consisting \nof sample recordings of spoken digits in wav files at 8kHz. \nFSDD is an open dataset, which means it will grow over time \nas new data are contributed. In this work, the version used \nconsists of six speakers, 3,000 recordings (50 of each digit per \nspeaker), and English pronunciations. Even though the \nrecordings are trimmed so that they have near minimal silence \nat the beginning and end, there are still a lot of zero values in \nthe raw waveform of the recordings. \n \nTo apply DNN models to speech recognition, some \npreprocessing steps are necessary for raw speech samples, \nwhich include cropping, time scaling, and amplitude scaling. \nFirst, in the cropping step, all contiguous zero values at the \nbeginning and the end of recordings should be removed, to \nreserve the significant signal part. However, that means the \nrecordings can have different lengths after cropping. Second, in \nthe time scaling step, a fixed-length L, a number near the mean \nvalue of lengths of all sample recordings, is chosen as the \ndimension of input layer of DNN models; each sample \nrecording of variable length is then either extended or \ncontracted to get the same length L. Third, in the amplitude \nscaling step, the amplitude of each recording is normalized to a \nrange of [-1,1]. Fig. 6 shows six examples of three different \npreprocessing steps. The leftmost shows the original signals \nbefore processing. After cropping all the zero values in the \nbeginning and end, signals are shown in the middle part. The \nrightmost gives the final preprocessing results after scaling time \nsteps and amplitude. Scaling time helps obtain a unified signal \nsize as model input size while scaling amplitude reduces the \ninfluence of different loudness levels since loudness is not our \nfocus. \n \nThe ESC-10 dataset [21] contains sounds that are recorded in \nnumerous outdoor and indoor environments. Each sound \nsample recording is of 5 second duration. The sampling rate of \neach recording is 44100 Hz. The dataset consists of a total of \n400 recordings, which are divided into 10 major categories. \nThey are sneezing, dog barking, clock ticking, crying baby, \ncrowing rooster, rain, sea waves, fire crackling, helicopter, and \nchainsaw. Each category contains 40 samples. The dataset \ncomes with 5 folds for cross-validation.  \n \n \n \n5 \n \n \nFig. 6. Illustration of preprocessing steps. The left-most column shows \nthe original signals. The middle column shows obtained signals after \ncropping all the zero values at the beginning and end of recordings. \nThe rightmost column shows the results after both scaling time and \nscaling amplitude. \n \nTo apply multiresolution learning on the 2D CNN for speech \nrecognition, log-scaled mel-spectrograms (i.e., images) were \nfirst extracted from all sound recordings with a window size of \n1024, hop length of 512, and 60 mel-bands. Fig. 7 illustrates the \ngenerated spectrogram of two examples. \n \nFig. 7. Spectrograms extracted from a clip (dog bark) and a clip (rain). \n \nSome environmental sound clips have periodic features. For \nexample, we can hear bark several times or tick several times in \na single sample recording. Also, considering the very limited \nnumber of samples available for training, the spectrograms \nwere split into 50% overlapping segments of 41 frames (short \nvariant, segments of approx. 950 ms) [21], but without \ndiscarding silent segments. One example is shown in Fig. 8. The \ntop image shows the original spectrogram extracted from a clip \n(dog bark), while the second, third, and fourth images illustrate \nthe first three short segments split from the original \nspectrogram, where the overlaps about 50% with neighboring \nshort segments can be clear seen. \n \n \nFig. 8. Spectrogram clipping. The first image shows the original \nspectrogram extracted from a clip (dog bark). The second, third, and \nfourth images illustrate the first 3 short segments split from the original \nspectrogram. \n \nThe CIFAR-10 dataset [22] contains 50,000 training and \n10,000 testing 32x32 color images of 10 classes. Both training \nset and testing set are normalized with mean and standard \ndeviation respectively. Considering the small size of images in \nCIFAR-10, we apply multiresolution learning with three levels \nof only transform original training images into another two \nresolutions, which means resolutions ùíìùüè\nùüêùë´, ùíìùüê\nùüêùë´ and ùíìùüë\nùüêùë´ . will be \nfed into the training network. Fig. 820 shows several sample \nexamples of the transformation results before applying \nnormalization method. \n \nFig. 9. Examples of a few sample representations of CIFAR-10 at three \ndifferent resolution levels. \nB. Experiment Setup \nFor CIFAR-10 and ESC-10, we always train models with \n100% of the training data. For FSDD data set, two groups of \nexperiments are designed. The first group is the normal \nexperiments using 100% available training data set aside to \nconstruct CNN models. In the second group, we attempt to \nevaluate CNN models‚Äô performance using reduced size of \ntraining data on FSDD. To do so, instead of using 100% of the \ntraining data, a much smaller portion of training data is applied \n \n \n6 \n \nin model learning. Validation data and testing data are reserved \nas the same as in normal experiments. For example, for FSDD, \n2000 records of total 3000 records are used as training data in \nthe first group of normal experiments, whereas in the second \ngroup (i.e., reduced training data) of experiments, only 20% of \nthose 2000 records of data are applied as training data for \nconstructing CNNs. \n \nTABLE I \nTHE CNN MODEL ARCHITECTURE ADOPTED ON FFSD \n \nLayer \n# Params \nConv 1D 128, 2 / Maxpooling 1D \n384 \nConv 1D 128, 2 / Maxpooling 1D \n32,896 \nConv 1D 128, 2 / Maxpooling 1D \n32,896 \nConv 1D 256, 2 / Maxpooling 1D \n65,792 \nConv 1D 256, 2 / Maxpooling 1D \n131,328 \nConv 1D 256, 2 / Maxpooling 1D \n131,328 \nConv 1D 256, 2 / Maxpooling 1D \n131,328 \nConv 1D 256, 2 / Maxpooling 1D \n131,328 \nConv 1D 256, 2 / Maxpooling 1D \n131,328 \nConv 1D 512, 2 / Maxpooling 1D \n262,656 \nConv 1D 512, 1 \n262,656 \nDense 128 \n917,632 \nDense 10 \n1,290 \nTotal # Params  \n2,232,842 \n \nDNN architecture and setup for FSDD. With FSDD data set, \nsound waveforms are directly taken as input to a deep CNN \nmodel. Reported in recent work of SampleCNN [23], a CNN \nmodel that takes raw waveforms as input can achieve the \nclassification performance to be in a par with the use of log-mel \nspectrograms as input to DNN models, provided that a very \nsmall filter size (such as two or three) is used in all \nconvolutional layers. The CNN architecture given in \nSampleCNN is adopted in our experiments with FSDD. Table I \nillustrates the CNN architecture and its number of parameters \nfor each layer. The portion of training/validation/testing data is \n64%, 16% and 20% respectively for the group of normal \nexperiments. \n \nIn our multiresolution learning for DNN models on both data \nsets FFSD and ESC-10, Haar wavelet transform is adopted to \nbuild multiresolution training data. Three groups of \nmultiresolution learning DNN models are constructed on FFSD \nwith four, five and six levels of multiresolution learning \nprocesses separately. To make sure DNN models get sufficient \ntraining and at the same time prevent overfitting, 500 total \nepochs and an early stopping scheme are applied. To have fair \ncomparisons of multiresolution learning (ML) versus \ntraditional learning (TL), for experiments with ML, the \nmaximum epoch number for each individual resolution training \nphase is divided evenly from the total 500 epochs. The input \nsize of the data is (16000, 1). Stochastic gradient descent (SGD) \noptimizer is adopted with a learning rate of 0.02 and momentum \nof 0.2 and batch size of 23. The weight initializer is set as a \nnormal distribution with a standard deviation of 0.02 and a \nmean of 0. In addition, a 0.001 L2 norm weight regularizer is \napplied. The ReLU (Rectified Linear Unit) activation function \nis used for each convolutional layer and dense layer except the \noutput layer using the softmax function. It should be noted that \ndifferent slope value of ReLU function is used for different \nresolution level i of training data, as shown in (8). The general \nrule is that the finer resolution of the training data is, the smaller \nthe slope value is [14], where the finest resolution learning \nphase, the slope value is 1 (i.e., the normal ReLU). The normal \nReLU is also used for TL.  \nùë†ùëôùëúùëùùëí= 0.85 + 0.15ùëñ                               (8) \nDNN architecture and setup for ESC-10. We design our new \nCNN model architecture for ESC-10 but adopt the same \nsegmentation and voting scheme as in Piczak [24]. Our CNN \nmodel contains 13 layers in total as shown in Table II. The first \nconvolutional ReLU layer consists of 80 filters of rectangular \nshape (3√ó3 size). Maxpooling layer is applied after every 2 \nconvolutional layers with a default pool shape of 2√ó2 and stride \nof 1√ó1. The second to the sixth convolutional ReLU layer \nconsist of 80 filters of rectangular shape (2√ó2 size). Further \nprocessing is applied through 3 dense (fully connected hidden) \nlayers of 1000 ReLUs, 500 ReLUs and 500 ReLUs, \nrespectively, with a softmax output layer. One dropout layer \nwith dropout rate 0.5 are added after the third dense layer. \n \nTABLE II \nTHE CNN MODEL ARCHITECTURE DESIGNED FOR ESC-10 \n \nLayers \n# Params \nConv2D 80, (3,3) \n800 \nConv2D 80, (2,2) \n25,680 \nMaxpool2D  \n- \nConv2D 80, (2,2) \n25,680 \nConv2D 80, (2,2) \n25,680 \nMaxpool2D  \n- \nConv2D 80, (2,2) \n25,680 \nConv2D 80, (2,2) \n25,680 \nMaxpool2D  \n- \nDense 1000 \n1,201,000 \nDense 500 \n500,500 \nDense 500 \n250,500 \nDense 10 \n5,010 \nTotal # Params \n2,089,210 \n \nWith the input size (60, 41, 1) for ESC-10, training is \nperformed using SGD with a learning rate of 0.02, momentum \n0.2, and batch size of 64. The same as the setup for the \nexperiments with FSDD, 500 total epochs, and an early \nstopping scheme are applied. Again, for experiments with \nmultiresolution learning, the maximum epoch number for each \nindividual resolution training phase is obtained by dividing the \ntotal 500 epochs evenly. Also, 0.001 L2 norm weight \nregularizer is applied for two convolutional layers and dense \nlayers. Different slope values of ReLU function are also applied \nfollowing (8) for different resolution level i of training data. The \n \n \n7 \n \nmodel is evaluated in a 5-fold cross-validation regime with a \nsingle fold used as an intermittent validation set. That is, the \ntraining/validation/test data split ratio is 0.6/0.2/0.2 in the group \nof normal experiments. This cross-validation scheme leads to \n20 different combinations of model construction and testing. \nSince each clip is segmented into 20 short pieces, the absolute \nnumber \nof \ntraining/validation/test \nshort \nsegments \nis \n4800/1600/1600 respectively. Final predictions for clips are \ngenerated a probability-voting scheme. \n \nDNN architecture and setup for CIFAR-10. We use wide \nResidual Networks [25] as our main network. Experiments are \nimplemented on a popular variant WRN-28-10 which has a \ndepth of 28 and a multiplier of 10 containing 36.5M parameters. \nSGD optimizer with Nesterov momentum and a global weight \ndecay of 5x10-4. 200 training epochs are divided into 4 phases \n(60, 60, 40, 40 epochs respectively) to apply different learning \nrates (0.1, 0.02, 0.004, 0.0008) while batch size 128 is kept \nsame. There is no validation set for training and data \naugmentation is not used. Instead, we evaluate top-1 error rates \nbased on 5 runs with different seeds. While training ML2 \nmodels,  ùíìùüê\nùüêùë´ is the input in phase 1 and phase 2 while ùíìùüè\nùüêùë´ in \nphase 3 and phase 4. When we train ML3 models,  ùíìùüë\nùüêùë´ is the \ninput in phase 1,  ùíìùüê\nùüêùë´ is the input in phase 2 while ùíìùüè\nùüêùë´ in phase \n3 and phase 4. \nC. Results and Evaluation \nWith FSDD. The deep CNN models are evaluated based on \nCNN models constructed by the traditional single resolution \nlearning versus the multiresolution learning using 15 different \nseeds for the generation of initial random weights of CNN \nmodels. To evaluate the noise robustness of trained deep CNN \nmodels on FSDD, random noise is then added to the test data. \nThe noise set is generated using a normal probabilistic \ndistribution with a zero mean and a scale of 0.75, with 5% noise \ndensity of the original sample length of 16000. \n \nIn the second group of experiments with small training data, \nwe construct new CNN models by reducing the training data to \nonly 20% of the original training data set. The evaluation results \nare shown in Table III, in which the accuracy for each model \ncategory is the average recognition accuracy over 15 \nconstructed models using arbitrary seeds for models‚Äô random \nweight initialization.    \n \nTABLE III \nAVERAGE CLASSIFICATION ACCURACY AND RELATIVE IMPROVEMENT RATIO OF ML CNNS (4, 5, 6 LEVELS) OVER TL CNNS UNDER TWO GROUPS \nOF EXPERIMENTS ON FSDD DATASET (100% TRAINING DATA INDICATES THE FULL TRAINING DATA USED, WHILE 20% TRAINING DATA INDICATES \nONLY 20% OF THE FULL TRAINING DATA USED) \n \n \nTraining \ndata \nNoise \nadded in \ntesting \ndata \nTL \nML \n4 Levels \n5 Levels \n6 Levels \nAcc. \nAcc. \nImprov. \nAcc. \nImprov. \nAcc. \nImprov. \n100% \nN \n83.93% \n89.10% \n6.16% \n88.19% \n5.07% \n85.62% \n2.01% \n100% \nY \n31.51% \n44.30% \n40.60% \n44.44% \n41.04% \n53.42% \n69.54% \n20% \nN \n53.82% \n66.03% \n22.69% \n64.51% \n19.86% \n54.03% \n0.39% \n20% \nY \n25.97% \n41.21% \n58.68% \n36.82% \n41.78% \n39.73% \n52.98% \nIn Table. 3 and Fig. 10, as we can see, ML models always \noutperform TL models no matter how many total levels of \nmultiresolution learning phases are employed in the learning \nprocess. To evaluate these constructed DNN models‚Äô \nperformance on noisy testing data, random noise is added to test \ndata. The performance of TL models degrades much more as \nwe can see that the improvement ratios of ML models (with \neither 4-, or 5-, or 6- levels of multiresolution learning process) \nover TL model are all over 10%, which is significantly higher \nthan that on clean test data. This indicates that CNN models \nconstructed by the traditional learning are not as robust as CNN \nmodels constructed by multiresolution learning with respect to \nnoise.  \n \nWhen the amount of training data is reduced in the second \ngroup of experiments, the accuracy of TL models decreases \nmore than that of ML models, indicating that CNN models \nconstructed by TL are more vulnerable compared to CNN \nmodels constructed by ML in small training data setting. The \nhighest improvement over TL models is obtained by ML \nmodels in the category of 4 levels of multiresolution learning, \nwhich is 22.69%.  The last row in Table III shows that, when \nreduced training data size and noise attack are combined, ML \nmodels \ndemonstrate \nmore \nsubstantial \nperformance \nimprovements over TL models, illustrating the benefits of \nmultiresolution learning for CNN models in robustness \nenhancement with respect to noise or/and small training data.  \n \n \n \n8 \n \n \nFig. 10. Accuracy result comparison of traditional learning (single \nresolution) versus 4, 5, and 6 levels of multiresolution learning on \nFSDD. The mean accuracy for each category is given by triangle. \n(a)100% training data; (b)100% training data + noise; (c) 20% training \ndata; (d) 20% training data + noise. \n \nWith ESC-10. The model is evaluated in a 5-fold cross-\nvalidation regime with a single fold used as an intermittent \nvalidation set. Since each clip is segmented into 20 short pieces, \nthe absolute number of training/validation/test short segments \nis 4800/1600/1600 respectively. Final predictions for clips are \ngenerated using a probability-voting scheme. Two sets of \nexperiments are implemented with unnormalized training data \nand normalized training data respectively. \n \nWe use Deepfool [8], an effective tool to systematically \nevaluate the adversarial robustness of CNN models on ESC-10. \nDeepfool is an algorithm to efficiently compute the minimum \nperturbations that are needed to fool DNN models on image \nclassification tasks, and reliably quantify the robustness of these \nDNN classifiers using robustness metric œÅ, where œÅ of a \nclassifier M is defined as follows [9]: \nùúåH234(ùëÄ) = \n%\n|ùíü| ‚àë\n‚Äñùíë9(ùíô)‚Äñ!\n‚Äñùíô‚Äñ!\n=‚ààùíü\n                                     (9) \nwhere ùíëL(ùíô)  is the estimated minimal perturbation obtained \nusing Deepfool, and ùíü denotes the test set. Basically, Deepfool \nis applied to the test data for each CNN model to generate \nadversarial perturbations. An average robustness value can be \ncomputed over the generated perturbations and original test \ndata. The Adversarial Robustness Toolbox [25] is used to \nimplement Deepfool. \n \nWith ESC-10 (unnormalized training data). As shown in \nTable IV, under normal accuracy experiments, we can see that \nML models of 2 and 3 levels only slightly perform better in \nprobability voting (PV) setting comparing to TL model. In \naddition, lower average test accuracy is obtained with ML \nmodels of 5 levels than TL models, which shows a degradation \nof 2.3%.   \n \nTABLE IV \nAVERAGE CLASSIFICATION ACCURACY AND RELATIVE IMPROVEMENT RATIO OF ML MODELS (2, 3, 4, 5 LEVELS) OVER TL MODELS ON ESC-10 \nWITH UNNORMALIZED TRAINING DATA USING SP (SHORT SEGMENT + PROBABILITY VOTING) \n \nVoting \nscheme \nTL \nML \n2 Levels  \n3 Levels \n4 Levels \n5 Levels \nAcc. \nAcc. \nImp. \nAcc \nImp. \nAcc \nImp. \nAcc \nImp. \nSP \n84.06% \n84.94% \n1.05%  84.25% \n 0.23% \n84.06% \n0.00% \n82.13% \n-2.30% \n \n \n \nTABLE V \nAVERAGE DEEPFOOL ROBUSTNESS VALUE ùúå OF ML (2, 3, 4, 5 LEVELS) AND TL UNDER EACH GROUP OF EXPERIMENTS ON ESC-10 WITH \nUNNORMALIZED TRAINING DATA \n \nTL \nML \n2 Levels  \n3 Levels \n4 Levels \n5 Levels \nùúå \nùúå \nImp. \nùúå \nImp. \nùúå \nImp. \nùúå \nImp. \n0.277 \n0.297 \n7.22%  \n0.295 \n 6.50% \n0.303 \n9.39% \n0.296 \n6.86% \n \n \n \n \n \n \n \n \n9 \n \nTABLE VI \nAVERAGE CLASSIFICATION ACCURACY AND RELATIVE IMPROVEMENT RATIO OF ML MODELS (2, 3, 4, 5 LEVELS) OVER TL MODELS ON ESC-10 \nDATASET WITH NORMALIZED TRAINING DATA USING SP (SHORT SEGMENT + PROBABILITY VOTING) \n \nVoting \nscheme \nTL \nML \n2 Levels  \n3 Levels \n4 Levels \n5 Levels \nAcc. \nAcc. \nImp. \nAcc \nImp. \nAcc \nImp. \nAcc \nImp. \nSP \n82.00% \n81.56% \n-0.54%  81.88%  -0.15% 83.13% \n1.38% \n83.25% \n1.52% \n \nTABLE VII \nAVERAGE DEEPFOOL ROBUSTNESS VALUE ùúå OF ML (2, 3, 4, 5 LEVELS) AND TL UNDER EACH GROUP OF EXPERIMENTS ON ESC-10 DATASET WITH \nNORMALIZED TRAINING DATA \n \nTL \nML \n2 Levels  \n3 Levels \n4 Levels \n5 Levels \nùúå \nùúå \nImp. \nùúå \nImp. \nùúå \nImp. \nùúå \nImp. \n2.782 \n3.757 \n35.05%  \n4.381 \n 57.48% \n4.711 \n69.34% \n4.993 \n79.48% \n \nWe compute robustness value œÅ for each TL and ML model. \nAs illustrated in Table IV, when full training data are used, TL \nmodels get the lowest average œÅ value 0.277 compared to all \ndifferent ML models, which means the perturbation needed to \nattack the TL models is smaller than that to the ML models. In \nother words, the CNN models constructed by multiresolution \nlearning obtain better robustness against adversarial examples. \nSpecifically, the average of œÅ value of the ML (4) models is \nabout 9.4% higher than the average œÅ value of the TL models. \nThe similar average œÅ values of the ML (2, 3, 5) models are \nobtained, which are about 6.5% to 7.2% higher than the average \nœÅ value of the TL models. Fig. 11 shows results from both Table \nIV and Table V. \n \nFig. 11. Accuracy (left plot) and Deepfool robustness (right plot) result \ncomparison of traditional learning(W1) versus 2, 3, 4 and 5 levels of \nmultiresolution learning (W2, W3, W4, W5) on ESC-10 with \nunnormalized training data. \n \nWith ESC-10 (normalized training data). Experiments were \nalso conducted with normalized input data, which fall in range \n[0,1] based on original spectrogram value, to explore a new \nrobustness attack tool ‚Äì AutoAttack, in addition to Deepfool. \nAutoAttack consists of four attacks. They are APGD-CE (auto \nprojected gradient descent with cross-entropy loss), APGD-\nDLR (auto projected gradient descent with difference of logits \nratio loss), FAB (Fast Adaptive Boundary Attack) and Square \nAttack. APGD is a white-box attack aiming at any adversarial \nexample within an Lp-ball, FAB minimizes the norm of the \nperturbation necessary to achieve a misclassification, while \nSquare Attack is a score-based black-box attack for norm \nbounded perturbations which uses random search and does not \nexploit any gradient approximation [1]. In our experiments, \nattack batch size 1000, epsilon 8/255 and L2 norm are applied. \nOne special requirement of AutoAttack is softmax function in \nthe last dense layer should be removed, which results in \ndifferent sets of standard test accuracy as shown in Table VI \nand Table VIII (before attack). As shown in Table VI, we can \nsee that ML (4) & ML (5) models slightly perform better \ncomparing to TL model. In addition, lower average test \naccuracy is obtained with ML (2) levels than TL models, which \nshows a small degradation of 0.54%. Overall performance \nshows that normalizing data does not help improve \nclassification accuracy. \n \nFig. 12. Accuracy (left plot) and Deepfool robustness (right plot) result \ncomparison of traditional learning(W1) versus 2, 3, 4 and 5 levels of \nmultiresolution learning (W2, W3, W4, W5) on ESC-10 with \nnormalized training data. \n \nWe further examine average Deepfool robustness value œÅ for \neach TL and ML model on normalized training data. As \nillustrated in Table VII, TL models get the lowest average œÅ \nvalue 2.782 compared to all different ML models. In general, \nhigher robustness value œÅ corresponds to ML with more \nresolution levels. Specifically, the average of œÅ value of the ML \n(5) models is about 79.5% higher than the average œÅ value of \nthe TL models. And ML (2) models achieve 35.0% higher value \n \n \n10 \n \nœÅ compared to TL models. In general, robustness value œÅ \nobtained with experiments on normalized training data is \nsignificantly higher than that on unnormalized training data. \nThe results also show that the CNN models constructed by \nmultiresolution learning demonstrate significant improvement \non robustness against adversarial examples compared to TL \nmodels with normalized training data. Fig. 12 shows results \nfrom Table VI and Table VII.\n \nTABLE VIII \nAUTOATTACK (ùúÄ= 8/255) EXPERIMENT RESULTS. AVERAGE CLASSIFICATION ACCURACY AND RELATIVE IMPROVEMENT RATIO OF ML MODELS \n(2, 3, 4, 5 LEVELS) OVER TL MODELS ON ESC-10 DATASET WITH NORMALIZED INPUT TRAINING DATA IN PHASE OF BOTH BEFORE ATTACK AND AFTER \nATTACK \n \nPhase \n(AutoAttack) \nTL \nML \n2 Levels  \n3 Levels \n4 Levels \nAcc. \nAcc. \nImp. \nAcc \nImp. \nAcc \nImp. \nBefore  \n74.69% \n77.19% \n3.35%  77.19% \n 3.35% \n77.75% \n4.10% \nAfter  \n59.46% \n61.71% \n3.78% \n61.60% \n3.60% \n62.83% \n5.67% \n \n \nFig. 13. Accuracy boxplot before applying AutoAttack (left plot) and \nAccuracy boxplot after applying AutoAttack (right plot) result \ncomparison of traditional learning(W1) versus 2, 3, 4 and 5 levels of \nmultiresolution learning (W2, W3, W4) on ESC-10 with normalized \ninput data. \n \nTable VIII and Fig. 13 illustrate AutoAttack experiment \nresults. As we can see, removing softmax function causes some \ngeneral performance degradation on standard test accuracy \ncomparing to results in Table IV. In other words, this illustrates \nthe effectiveness of softmax function. Different from Table IV, \nall ML models outperform TL models under Autoattack \nstandard test accuracy setting (Phase -- before Autoattack). ML \n(4) models achieve the best improvement of 4.1% in \ncomparison to TL models. After applying Autoattack, ML (4) \nmodels still achieve the best performance as opposed to all \nother models. Hence, we can draw the same overall trend as \nDeepfool experiments -- the ML models with more resolution \nlevels tend to achieve better robustness performance. \n \nWith CIFAR-10. The average classification accuracy results \non CIFAR-10 with WRN-28-10 models are shown in Table IX. \nResults indicate that TL models slightly outperform ML2 \nmodels while performance of ML3 models get a degradation of \n1.31% comparing to TL models.  \nTo evaluate the robustness of ML versus TL models, we also \napply Deepfool [8] as described in section III.C. The average \nrobustness values are reported in Table X. As we know, under \nthe same setting, the higher robustness value indicates the better \nresistance to Deepfool perturbation. We find that both ML2 and \nML3 models outperform TL models significantly with \nimprovement ratio of 14.06% and 20.48% respectively. This \ntrend can also be seen for experiments on ESC-10 dataset. The \nmultiresolution learning models with more resolution levels, in \ngeneral, achieve better Deepfool robustness. \n \nTo thoroughly investigate the robustness improvement by \nmultiresolution learning, another black-box attack tool, one-\npixel-attack [26], is further adopted in our evaluation. The one-\npixel-attack tool is a method for generating one-pixel \nadversarial perturbations based on differential evolution (DE). \nWe apply the untargeted attack for our models. 1000 samples \nout of all the successfully predicted images will be chosen \nrandomly to test robustness. Once a perturbated sample is \npredicted as a wrong class, we call a success attack. The overall \naverage attack success rate for models is obtained with 5 runs \nof different seeds. The results are presented in Table XI. A \nlower attack success rate always implies that the trained model \nis more robust. From Table XI, it can be seen that TL models \nachieve the highest attack success rate 26.0% within 1000 \nattacked samples. At the same time, ML2 models achieve the \nlowest attack success rate 24.1%, a reduction of 7.3% of attack \nsuccess rate compared to TL models. ML3 models also obtain \na similar result as ML2 models. To sum up, both Deepfool \nattack and one-pixel attack results consistently indicate that ML \nmodels are more robust than TL models under the same setting. \n \n \n \n \n \n \n \n \n \n11 \n \nTABLE IX \nAVERAGE CLASSIFICATION ACCURACY AND RELATIVE IMPROVEMENT RATIO OF ML MODELS (2, 3 LEVELS) OVER TL MODELS ON CIFAR-10 \nDATASET \n \nTraining \ndata \nTL \nML \n2 Levels  \n3 Levels \nAcc. \nAcc. \nImp. \nAcc \nImp. \n100% \n94.86% \n94.78% \n-0.08%   \n93.62% \n-1.31%  \n \nTABLE X \nAVERAGE DEEPFOOL ROBUSTNESS VALUE ùúå AND RELATIVE IMPROVEMENT RATIO OF ML MODELS (2, 3 LEVELS) OVER TL MODELS ON CIFAR-10 \nDATASET \n \nTraining \ndata \nTL \nML \n2 Levels  \n3 Levels \nRobustness ùúå \n(\n)\nRobustness ùúå \n(\n)\nImp. \nRobustness ùúå \n(\n)\nImp. \n100% \n8.579 \n9.785 \n14.06%   \n10.336 \n20.48%  \n \nTABLE XI \nAVERAGE ONE-PIXEL ATTACK SUCCESS RATE (AS RATE) AND RELATIVE IMPROVEMENT RATIO OF ML MODELS (2, 3 LEVELS) OVER TL MODELS ON \nCIFAR-10 DATASET \n \nTraining \ndata \nTL \nML \n2 Levels  \n3 Levels \nAS rate \nAS rate \nImp. \nAS rate \nImp. \n100% \n26.00% \n24.10% \n7.30%   \n24.40% \n6.15%  \n \nTABLE XII\nAVERAGE MULTI-RESOLUTION ROBUSTNESS ATTACK SUCCESS RATE (AS RATE) ON DIFFERENT COEFFICIENTS AND RELATIVE IMPROVEMENT RATIO \nOF ML MODELS (2, 3 LEVELS) OVER TL MODELS ON CIFAR-10 DATASET \n \nCoefficient \nTL \nML \n2 Levels  \n3 Levels \nAS rate \nAS rate \nImp. \nAS rate \nImp. \nLH \n7.80% \n6.41% \n17.83%   \n3.99% \n48.85% \nHL \n9.43% \n7.81% \n17.18% \n4.34% \n53.98% \nHH \n3.41% \n2.48% \n27.28% \n0.81% \n76.25% \nTotal \n20.63% \n16.70% \n19.05% \n9.14% \n55.70% \n \nFig. 14. Multi-resolution robustness attack examples. For each original image, one detailed coefficient subset among LH, HL or HH is replaced \nby that of another random image of different category to generate adversarial images.\n \n \n12 \n \nLast, we propose our multi-resolution robustness attack \nmethod. To implement this attack, we apply 2D haar wavelet \ntransformation on correct predicted test images and obtain \napproximation (LL), horizontal detail (LH), vertical detail (HL) \nand diagonal detail (HH) coefficients respectively. Then, one \ncomponent among LH, HL and HH of each image is replaced \nrandomly with the same detail component from another image \nof different category to generate adversarial images. The \ncomparison between generated adversarial images and original \nimages is shown in Fig. 13. As we can see, replacing coefficient \ndoesn‚Äôt affect when human identify different categories. \nThrough 5 runs of attack, we obtain robustness results \nillustrated in Table XII. Overall, ML2 (AS rate 16.7%) and \nML3 (AS rate 9.14%) both significantly outperform TL (AS \nrate 20.63%) while identifying perturbated images. It shows \nthat replacing HL or LH coefficients makes it harder for models \nto identify perturbated images, while replacing HH coefficients \nis less harmful. This makes sense since HH coefficients, in \ngeneral, contain detailed information of images which is less \ncritical. In summary, TL model shows the least robustness \nwhile test images are perturbated by multi-resolution robustness \nattack compared to ML2 and ML3 models. \nIV. DISCUSSIONS \nA.  Wavelet CNNs. \nSeveral works are reported on wavelet-based CNNs to \nincorporate DWT in CNNs, which can be basically classified \ninto two categories. The first category is to use wavelet \ndecomposition as a preprocessing of data to obtain a fixed level \nof lower resolution training data, to be employed to construct a \nCNN model using traditional learning process.  For example, \nWavelet-SRNet [27] is such a wavelet-based CNN for face \nsuper resolution. Zhao [28] focuses on experiments in ECG (1D \nsignal) classification task using deep CNN with wavelet \ntransform, where, again, wavelet transform is only used as a \ndata preprocessing tool to obtain the filtered ECG signal. The \nsecond category includes schemes that embed DWT into CNN \nmodels mainly for image restoration tasks (e.g., deep \nconvolutional framelets [29], MWCNN [30]). Our approach of \nmultiresolution learning for deep CNN is different from those \nwavelet-based CNN schemes in literature in the following key \naspects: First, the overall training of CNN model is a \nprogressive process involving multiple different resolution \ntraining data from coarser versions to finer versions. In contrast, \nwhen DWT is only used for data preprocessing, the CNN \ntraining is still traditional learning, a repeated process over the \nsingle resolution training data.  Second, our approach is very \nflexible, where users can easily select a different wavelet basis \nand a total levels of wavelet decomposition for a given task. In \ncontrast, embedded DWT into CNN means that the wavelet \ntransformation and basis used as well as the total levels of \ndecomposition are ‚Äòhard coded‚Äô in the model, making any \nchange difficult and error prone. \n \n1 LP: Long segment + Probability voting.  \nB.  Accuracy performance comparison with other schemes on \nESC-10.   \nESC-10 is a popular sound classification dataset which has \nbeen used by many researchers. The performance of our \nproposed multiresolution learning CNN is compared with other \nexisting various schemes on ESC-10 using traditional learning, \nas shown in Table XIII, where our result of 5-level \nmultiresolution learning is used.   \nTABLE XIII \n ENVIRONMENT SOUND CLASSIFICATION PERFORMANCE COMPARISON \nWITH OTHER EXISTING MODELS ON ESC-10 DATASET \n \nYear [References] \nMethod \nAccuracy  \n2015[24] \nPiczakCNN + SP \n78.3% \n2015[24] \nPiczakCNN + LP1 \n80% \n2017[31] \nSB-CNN (DA) \n79% \n2017[31] \nSB-CNN \n73% \n2019[32] \nCNN2 \n73% \n2019[32] \nTDSN2 \n56% \n2019[33] \nANN2 + Cascade \n64.5% \n2023[This paper] \nML CNN + SP \n83.25% \n \nWhile our CNN model adopts the same segmentation and \nvoting scheme as in the baseline work of PiczakCNN [24], our \nnetwork scale is greatly reduced. PiczakCNN has about 25M \ntrainable parameters, whereas our CNN only has about 2M \nparameters. That means that our CNN model has drastically \nsaved computational resource and training time. At the same \ntime, a better recognition result is achieved by our approach \nwith much fewer model parameters. We note that we do not \napply data augmentation (DA) in our study, while √ó10 data \naugmentations for PiczakCNN [24] and √ó5 data augmentations \nfor SB-CNN [31] were used respectively. As we can see, our \nmultiresolution learning CNN under short segment/probability \nvoting setting (SP) outperforms all the other schemes/models \nlisted in Table XIII, even though our ML CNN only used about \n9.1% and 16.7% amount of training data compared to data \naugmentations used in PiczakCNN and SB-CNN, respectively. \nC.  Accuracy performance comparison with other schemes on \nCIFAR-10. \nIn recent years, the state-of-the-art performance on CIFAR-\n10 has continued to improve, with new models and techniques \nthat achieve increasingly lower error rates. In this work, we do \nnot apply complex data augmentation method except the \ncrop/flip on training data to focus on the effect of \nmultiresolution learning. Table XIV presents the comparison of \nsome classification results for this setting. As we can see, our \nmultiresolution learning approach has achieved the higher \naccuracy than other schemes except for the work of [37] which \nimproved SGD by ensembling a subset of the weights in late \nstages of learning to augment the standard models. \n2 Unclear whether data augmentation was used or not.  \n \n \n13 \n \n \nTABLE XIV \nIMAGE CLASSIFICATION PERFORMANCE COMPARISON WITH OTHER \nEXISTING MODELS WITHOUT APPLYING DATA AUGMENTATION ON \nCIFAR-10 DATASET \n \nYear [References] \nNetwork \nAccuracy  \n2021[34] \nWRN-28-10 \n86.09% \n2020[35] \nMobilenet V2 \n92.4% \n2018[36] \nNiN \n91.54% \n2021[37] \nWRN-28-10 \n96.46% \n2021[38] \nWRN-28-10 \n86.09% \n2023[This paper] \nWRN-28-10 \n94.78% \n \nD.  Tradeoff between accuracy and robustness \nRecent studies show that robustness may be at odds with \naccuracy in traditional learning setting (e.g., [15], [39]). \nNamely, getting robust models may lead to a reduction of \nstandard accuracy. It is also stated in [15] that though \nadversarial training is effective, this comes with certain \ndrawbacks like an increase in the training time and the potential \nneed for more training data. The multiresolution deep learning, \nhowever, can overcome these drawbacks easily because it only \ndemands the same computing epochs and the same dataset as \ntraditional learning, as demonstrated in our experiments. Both \n[15] and [39] claim that adversarial vulnerability is not \nnecessarily tied to the standard training framework but is rather \na property of the dataset. This may perhaps need further study \nand evidence, since under robustness attack different training \nframeworks (e.g., traditional learning vs. multiresolution \nlearning) could bring totally distinct results. A hypothesis is \nraised in [15], [39] that the biggest price of being adversarial \nrobust is the reduction of standard accuracy. However, our \nresults seem to suggest that multiresolution learning could \nsignificantly improve robustness with no or little tradeoff of \nstandard accuracy.  \nV. CONCLUSIONS AND FUTURE WORK \nIn this paper, we present a new multiresolution deep learning \nfor CNNs. To the best of our knowledge, this work represents \nthe first study of its kind in exploring multiresolution learning \nin deep learning technology. We showed that multiresolution \nlearning can significantly improve the robustness of deep CNN \nmodels for both 1D and 2D signal classification problems, even \nwithout applying data augmentation. We demonstrated this \nrobustness improvement in terms of random noise, reduced \ntraining data, and in particular well-designed adversarial attacks \nusing \nmultiple \nsystematic \ntools \nincluding \nDeepfool, \nAutoAttack, and one-pixel-attack. In addition, we have also \nproposed our systematic multi-resolution attack method for the \nevaluation of robustness. Our multiresolution deep learning is \nvery general and can be readily applied to other DNN models. \nOur multi-resolution attack method can also be applied in \ngeneral for robustness attack and adversarial training. On \ncontrary to the recent observation in traditional single \nresolution learning setting, our results seem to suggest that it \nmay not be necessary to trade standard accuracy for robustness \nwith multiresolution learning, a definite interesting and \nimportant topic for further future research. We also plan to \nfurther investigate our approach on large-scale problems \nincluding ImageNet, and to explore various wavelet bases \nbeyond Haar wavelet in the multiresolution learning. \n \nREFERENCES \n \n[1] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep Learning,‚Äù Nature, vol. 521, \nno. \n7553, \npp. \n436‚Äì444, \nMay \n2015, \ndoi: \nhttps://doi.org/10.1038/nature14539. \n[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImageNet classification \nwith deep convolutional neural networks,‚Äù Communications of the ACM, \nvol. 60, no. 6, pp. 84‚Äì90, May 2012, doi: https://doi.org/10.1145/3065386. \n[3] A. Graves, A. -r. Mohamed and G. Hinton, \"Speech recognition with deep \nrecurrent neural networks,\" 2013 IEEE International Conference on \nAcoustics, Speech and Signal Processing, Vancouver, BC, Canada, 2013, \npp. 6645-6649, doi: 10.1109/ICASSP.2013.6638947. \n[4] K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual Learning for Image \nRecognition,\" 2016 IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: \n10.1109/CVPR.2016.90.  \n[5] N. Akhtar and A. Mian, \"Threat of Adversarial Attacks on Deep Learning \nin Computer Vision: A Survey,\" in IEEE Access, vol. 6, pp. 14410-14430, \n2018, doi: 10.1109/ACCESS.2018.2807385.  \n[6] J. Su, D. V. Vargas and K. Sakurai, \"One Pixel Attack for Fooling Deep \nNeural Networks,\" in IEEE Transactions on Evolutionary Computation, \nvol. 23, no. 5, pp. 828-841, Oct. 2019, doi: 10.1109/TEVC.2019.2890858. \n[7] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and Harnessing \nAdversarial Examples,‚Äù arXiv.org, 2014. https://arxiv.org/abs/1412.6572 \n[8] S. -M. Moosavi-Dezfooli, A. Fawzi and P. Frossard, \"DeepFool: A Simple \nand Accurate Method to Fool Deep Neural Networks,\" 2016 IEEE \nConference on Computer Vision and Pattern Recognition (CVPR), Las \nVegas, NV, USA, 2016, pp. 2574-2582, doi: 10.1109/CVPR.2016.282. \n[9] J. Gao, B. Wang, Z. Lin, W. Xu, and Y. Qi, ‚ÄúDeepCloak: Masking Deep \nNeural \nNetwork \nModels \nfor \nRobustness \nAgainst \nAdversarial \nSamples,‚Äù arXiv:1702.06763 \n[cs], \nApr. \n2017, \nAvailable: \nhttps://arxiv.org/abs/1702.06763 \n[10] M. Guo, Y. Yang, R. Xu, Z. Liu and D. Lin, \"When NAS Meets \nRobustness: In Search of Robust Architectures Against Adversarial \nAttacks,\" 2020 IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR), Seattle, WA, USA, 2020, pp. 628-637, doi: \n10.1109/CVPR42600.2020.00071.  \n[11] Y. Guo, Q. Li, and H. Chen, ‚ÄúBackpropagating Linearly Improves \nTransferability of Adversarial Examples,‚Äù arXiv.org, Dec. 07, 2020. \nhttps://arxiv.org/abs/2012.03528 \n[12] C. Lyu, K. Huang and H. -N. Liang, \"A Unified Gradient Regularization \nFamily for Adversarial Examples,\" 2015 IEEE International Conference \non Data Mining, Atlantic City, NJ, USA, 2015, pp. 301-309, doi: \n10.1109/ICDM.2015.84.  \n[13] Y. Liang and E. W. Page, \"Multiresolution learning paradigm and signal \nprediction,\" in IEEE Transactions on Signal Processing, vol. 45, no. 11, \npp. 2858-2864, Nov. 1997, doi: 10.1109/78.650113. \n[14] Y. Liang and X. Liang, \"Improving signal prediction performance of \nneural networks through multiresolution learning approach,\" IEEE \nTransactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. \n36, no. 2, pp. 341-352, April 2006, doi: 10.1109/TSMCB.2005.857092.  \n[15] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry, \n‚ÄúRobustness May Be at Odds with Accuracy,‚Äù arXiv.org, Sep. 09, 2019. \nhttps://arxiv.org/abs/1805.12152 \n[16] Y. Liang, ‚ÄúMultiresolution learning paradigm and high-speed network \ntraffic prediction,‚Äù Ph.D. dissertation, Dept. Comput. Sci., Clemson \nUniv., Clemson, SC, 1997.  \n[17] Y. Liang, ‚ÄúReal-Time VBR video traffic prediction for dynamic \nbandwidth allocation,‚Äù IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., \nvol. 34, no. 1, pp. 32‚Äì47, Feb. 2004. \n[18] Y. Liang and M. Han, ‚ÄúDynamic Bandwidth Allocation Based on Online \nTraffic Prediction for Real-Time MPEG-4 Video Streams,‚Äù EURASIP \n \n \n14 \n \nJournal on Advances in Signal Processing, Volume 2007, No. 1, Article \nID 87136, 10 pages, doi:10.1155/2007/87136, 2007. \n[19] S. G. Mallat, \"A theory for multiresolution signal decomposition: the \nwavelet representation,\" in IEEE Transactions on Pattern Analysis and \nMachine Intelligence, vol. 11, no. 7, pp. 674-693, July 1989, doi: \n10.1109/34.192463. \n[20] Z. Jackson, C. Souza, J. Flaks, Y. Pan, H. Nicolas, and A. Thite, \n‚ÄúJakobovski/free-spoken-digit-dataset: v1.0.8,‚Äù Zenodo, Aug. 09, 2018. \nhttps://zenodo.org/record/1342401  \n[21] K. J. Piczak, ‚ÄúESC: Dataset for Environmental Sound Classification,‚Äù in \nProceedings of the 23rd ACM international conference on Multimedia, \nNew \nYork, \nNY, \nUSA, \n2015, \npp. \n1015‚Äì1018, \ndoi: \n10.1145/2733373.2806390 \n[22] A. Krizhevsky, ‚ÄúCIFAR-10 and CIFAR-100 datasets,‚Äù Toronto.edu, 2009. \nhttps://www.cs.toronto.edu/~kriz/cifar.html \n[23] J. Lee, J. Park, K. Kim, and J. Nam, ‚ÄúSampleCNN: End-to-End Deep \nConvolutional Neural Networks Using Very Small Filters for Music \nClassification,‚Äù Applied Sciences, vol. 8, no. 1, p. 150, Jan. 2018, doi: \nhttps://doi.org/10.3390/app8010150.  \n[24] K. J. Piczak, \"Environmental sound classification with convolutional \nneural networks,\" 2015 IEEE 25th International Workshop on Machine \nLearning for Signal Processing (MLSP), Boston, MA, USA, 2015, pp. 1-\n6, doi: 10.1109/MLSP.2015.7324337. \n[25] ‚ÄúWelcome to the Adversarial Robustness Toolbox ‚Äî Adversarial \nRobustness \nToolbox \n1.7.0 \ndocumentation,‚Äù adversarial-robustness-\ntoolbox.readthedocs.io. \nhttps://adversarial-robustness-\ntoolbox.readthedocs.io/en/latest/ \n[26] J. Su, D. V. Vargas and K. Sakurai, \"One Pixel Attack for Fooling Deep \nNeural Networks,\" in IEEE Transactions on Evolutionary Computation, \nvol. 23, no. 5, pp. 828-841, Oct. 2019, doi: 10.1109/TEVC.2019.2890858. \n[27] H. Huang, R. He, Z. Sun and T. Tan, \"Wavelet-SRNet: A Wavelet-Based \nCNN for Multi-scale Face Super Resolution,\" 2017 IEEE International \nConference on Computer Vision (ICCV), Venice, Italy, 2017, pp. 1698-\n1706, doi: 10.1109/ICCV.2017.187.  \n[28] Y. Zhao, J. Cheng, P. Zhan, and X. Peng, ‚ÄúECG Classification Using Deep \nCNN Improved by Wavelet Transform,‚Äù Computers, Materials & \nContinua, \nvol. \n64, \nno. \n3, \npp. \n1615‚Äì1628, \n2020, \ndoi: \nhttps://doi.org/10.32604/cmc.2020.09938.  \n[29] Y. Han and J. C. Ye, \"Framing U-Net via Deep Convolutional Framelets: \nApplication to Sparse-View CT,\" in IEEE Transactions on Medical \nImaging, \nvol. \n37, \nno. \n6, \npp. \n1418-1429, \nJune \n2018, \ndoi: \n10.1109/TMI.2018.2823768.  \n[30] P. Liu, H. Zhang, W. Lian and W. Zuo, \"Multi-Level Wavelet \nConvolutional Neural Networks,\" in IEEE Access, vol. 7, pp. 74973-\n74985, 2019, doi: 10.1109/ACCESS.2019.2921451. \n[31] J. Salamon and J. P. Bello, \"Deep Convolutional Neural Networks and \nData Augmentation for Environmental Sound Classification,\" in IEEE \nSignal Processing Letters, vol. 24, no. 3, pp. 279-283, March 2017, doi: \n10.1109/LSP.2017.2657381.  \n[32] A. Khamparia, D. Gupta, N. G. Nguyen, A. Khanna, B. Pandey and P. \nTiwari, \"Sound Classification Using Convolutional Neural Network and \nTensor Deep Stacking Network,\" in IEEE Access, vol. 7, pp. 7717-7727, \n2019, doi: 10.1109/ACCESS.2018.2888882. \n[33] B. da Silva, A. W. Happi, A. Braeken, and A. Touhafi, ‚ÄúEvaluation of \nClassical Machine Learning Techniques towards Urban Sound \nRecognition on Embedded Systems,‚Äù Applied Sciences, vol. 9, no. 18, p. \n3885, Sep. 2019, doi: https://doi.org/10.3390/app9183885. \n[34] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. Mann, \n‚ÄúData Augmentation Can Improve Robustness,‚Äù arXiv:2111.05328 [cs, \nstat], Nov. 2021, Available: https://arxiv.org/abs/2111.05328  \n[35] M. Ayi and M. El-Sharkawy, \"RMNv2: Reduced Mobilenet V2 for \nCIFAR10,\" 2020 10th Annual Computing and Communication Workshop \nand Conference (CCWC), Las Vegas, NV, USA, 2020, pp. 0287-0292, doi: \n10.1109/CCWC47524.2020.9031131. \n[36] Y. Pang, M. Sun, X. Jiang and X. Li, \"Convolution in Convolution for \nNetwork in Network,\" in IEEE Transactions on Neural Networks and \nLearning Systems, vol. 29, no. 5, pp. 1587-1597, May 2018, doi: \n10.1109/TNNLS.2017.2676130.  \n[37] J. von Oswald, S. Kobayashi, A. Meulemans, C. Henning, B. F. Grewe, \nand J. Sacramento, ‚ÄúNeural networks with late-phase weights,‚Äù arXiv.org, \nApr. 11, 2022. https://arxiv.org/abs/2007.12927  \n[38] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. Mann, \n‚ÄúFixing \nData \nAugmentation \nto \nImprove \nAdversarial \nRobustness,‚Äù arXiv:2103.01946 \n[cs], \nOct. \n2021, \nAvailable: \nhttps://arxiv.org/abs/2103.01946  \n[39] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry, \n‚ÄúAdversarial Examples Are Not Bugs, They Are Features,‚Äù arXiv.org, \nAug. 12, 2019. https://arxiv.org/abs/1905.02175 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-09-24",
  "updated": "2023-09-28"
}