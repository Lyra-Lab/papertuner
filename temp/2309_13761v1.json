{
  "id": "http://arxiv.org/abs/2309.13761v1",
  "title": "Text Classification: A Perspective of Deep Learning Methods",
  "authors": [
    "Zhongwei Wan"
  ],
  "abstract": "In recent years, with the rapid development of information on the Internet,\nthe number of complex texts and documents has increased exponentially, which\nrequires a deeper understanding of deep learning methods in order to accurately\nclassify texts using deep learning techniques, and thus deep learning methods\nhave become increasingly important in text classification. Text classification\nis a class of tasks that automatically classifies a set of documents into\nmultiple predefined categories based on their content and subject matter. Thus,\nthe main goal of text classification is to enable users to extract information\nfrom textual resources and process processes such as retrieval, classification,\nand machine learning techniques together in order to classify different\ncategories. Many new techniques of deep learning have already achieved\nexcellent results in natural language processing. The success of these learning\nalgorithms relies on their ability to understand complex models and non-linear\nrelationships in data. However, finding the right structure, architecture, and\ntechniques for text classification is a challenge for researchers. This paper\nintroduces deep learning-based text classification algorithms, including\nimportant steps required for text classification tasks such as feature\nextraction, feature reduction, and evaluation strategies and methods. At the\nend of the article, different deep learning text classification methods are\ncompared and summarized.",
  "text": "TEXT CLASSIFICATION: A PERSPECTIVE OF DEEP LEARNING\nMETHODS\nA PREPRINT\nZhongwei Wan\nDepartment of Aritificial Intelligence\nUniversity of Chinese Academic of Science\nBeijing, China\n11612201@mail.sustech.edu.cn\nOctober 28, 2020\nABSTRACT\nIn recent years, with the rapid development of information on the Internet, the number of complex texts\nand documents has increased exponentially, which requires a deeper understanding of deep learning\nmethods in order to accurately classify texts using deep learning techniques, and thus deep learning\nmethods have become increasingly important in text classification. Text classification is a class of\ntasks that automatically classifies a set of documents into multiple predefined categories based on\ntheir content and subject matter. Thus, the main goal of text classification is to enable users to extract\ninformation from textual resources and process processes such as retrieval, classification, and machine\nlearning techniques together in order to classify different categories. Many new techniques of deep\nlearning have already achieved excellent results in natural language processing. The success of these\nlearning algorithms relies on their ability to understand complex models and non-linear relationships\nin data. However, finding the right structure, architecture, and techniques for text classification is a\nchallenge for researchers. This paper introduces deep learning-based text classification algorithms,\nincluding important steps required for text classification tasks such as feature extraction, feature\nreduction, and evaluation strategies and methods. At the end of the article, different deep learning\ntext classification methods are compared and summarized.\nKeywords Text Classification · Machine Learning · Deep Learning\n1\nIntroduction\nText classification is a classical problem in natural language processing. The task is to assign predefined categories to\na given sequence of texts. In recent years, the study of text classification has become increasingly important due to\nthe rapid growth of social networks, blogs and forums, and the increase in the size of online academic libraries. As a\nresult, text classification is widely used in information retrieval systems and search engine applications. At the same\ntime, text classification can also be used for email and SMS spam filtering. Most of the text classification techniques\ninclude feature extraction of text, data reduction and deep learning model selection, and model evaluation. Also, text\nclassification systems can classify text by its size, such as document level, paragraph level, sentence level, and clause\nlevel [1].\nBefore deep learning became the dominant model, traditional machine learning had a wide range of applications in text\nclassification, such as using ensemble learning techniques like boosting and bagging for text classification and analysis\n[2]. At the same time, [3] used simple logistic regression to classify textual information for information retrieval using\nsimple logistic regression. [4] uses a Naive Bayesian classifier to classify documents because Naive Bayes uses less\nmemory and computation, and is the classifier that is used more often by traditional machine learning.\nTherefore, this review consists of several parts, the first part will briefly introduce several deep learning algorithms\nfor feature extraction in text classification tasks, such as Word2Vec [5], Global Vectors for Word Representation\narXiv:2309.13761v1  [cs.CL]  24 Sep 2023\narXiv Template\nA PREPRINT\n(GloVe) [6], and several word embedding algorithms. In the second part, we will also briefly introduce several data\nreduction algorithms that may be used in traditional machine learning-based text classification tasks, such as Principal\nComponent Analysis (PCA) [7], Linear Discriminant Analysis (LDA) [8], which can improve the Accuracy in traditional\nmachine learning text classification tasks, in some cases.We will focus on several conventional deep learning-based text\nclassification algorithms, such as LSTM [9], GRU [5], and several state of the art attention models, such as Transformer\n[10], and improved versions based on Transformer such as XL-Net [11], Bert [12], and several improved models of\nBert, among others. Finally, Several common model evaluation techniques are very essential in text classification tasks,\nsuch as accuracy, Fb score, receiver operating characteristics (ROC), and area under the ROC curve (AUC).\nFigure 1: Pipeline of Text classification [1]\n2\nFeature extraction\nWord embedding is a key technique in the feature extraction process of text classification. Although we have used\nTokenizer before the feature extraction task to divide sentences into words and count the number of occurrences of\neach word, as well as to generate syntactic word representations, this process does not capture the semantic information\nbetween words and words. This problem can cause serious problems in the model’s understanding of the semantic\ninformation in sentences. For example, the n-gram model does not find word-to-word similarities. So google researchers\nin the journal NIPS solved this problem by word vector embedding. [5] is one of the foundational papers of word2vec,\npresented by Tomas Mikolov of Google. The paper proposed two word2vec model structures, CBOW and Skip-gram.\nAnd [13] is another foundational paper on word2vec. The Skip-gram model is described in detail, including the specific\nform of the model and two feasible training methods, Hierarchical Softmax and Negative Sampling. [6] presented\nGloVe, an improved technique of word2vec, using global matrix decomposition (LSA) and local content windows\n(Word2vec) to fully utilize statistical information to train the model using elements with non-zero frequencies in the\nword co-occurrence matrix.\nFrom the three papers on word embedding presented by the above researchers, word embedding is a feature learning\ntechnique in which each word from a vocabulary is mapped to an X-dimensional vector. Two of the key techniques,\nword2vec and GloVe, have been successfully used in deep learning techniques. In word2vec, the training goal of the\nSkip-gram model is to find word representations that are useful for predicting words in the context of a sentence or\ndocument. And the formula is:\n1\nT\nT\nX\nt=1\nX\n−c≤j≤cj̸=0\nlog p (wt+j | wt)\n(1)\nThe CBOW model uses the contextual words to predict the central words, and the diagram of the two models is shown\nbelow.\n3\nFeature reduction\nIn this section, we will briefly introduce possible feature reduction techniques that can be used in text classification tasks.\nMany text sequences in term-based vector models consist of many complex features, and therefore many researchers\nuse machine learning-based feature reduction techniques to reduce the feature space size and thus the temporal and\nspatial complexity of the model. We will briefly introduce two common dimensionality reduction techniques such as\nPCA [7], LDA [8] in the following.\n2\narXiv Template\nA PREPRINT\nFigure 2: The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding\nwords given the current word[5].\nFigure 3: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their capital\ncities[62]\n3.1\nPrincipal Component Analysis (PCA)\nThe core idea of PCA is to reduce dimensionality by finding approximate subspaces of the data distribution. n-\ndimensional features are mapped by PCA to the k-dimension, which is a new orthogonal feature, also known as\na principal component, that is reconstructed from the original n-dimensional features. k-dimensional features are\nreconstructed from the original n-dimensional features. It is closely related to the data itself. The first new axis is\nchosen to be the direction with the greatest variance in the original data, the second new axis is chosen to be the one\nwith the greatest variance in the plane orthogonal to the first axis, and the third axis is the one with the greatest variance\nin the plane orthogonal to the 1st and 2nd axes. By analogy, n such axes can be obtained. The new axes are obtained\nin this way. Finally, by eigenvalue decomposition or SVD decomposition of the covariance matrix of the data, the\neigenvector corresponding to the first K largest eigenvalues of the desired dimension is obtained, and multiplied with\nthe original data matrix to obtain the dimensionality-reduced features.\nCov(X, Y ) = E[(X −E(X))(Y −E(Y ))] =\n1\nn −1\nn\nX\ni=1\n(xi −¯x) (yi −¯y)\n(2)\nA = UΣV T\n(3)\n3.2\nLinear Discriminant Analysis (LDA)\nLDA differs from the unsupervised learning technique of PCA in that it is a supervised dimensionality reduction\ntechnique, which means that each sample of the dataset has a category output.The core idea of LDA is that we want to\n3\narXiv Template\nA PREPRINT\nproject the data in a low dimension, and after projection we want the projection points of each category of data to be as\nclose to each other as possible, and the distance between the category centers of the different categories of data to be as\nlarge as possible. Since we are projecting multiple categories to a low dimension, the projected low dimensional space\nis not a straight line, but a hyperplane. Suppose we project a low-dimensional space of dimension d, corresponding to a\nbase vector of (w1,w2,...wn). The core formula of LDA is as follows, Sw is defined as a class scatter matrix and Sb is\ndefined as an interclass scatter matrix, so the optimization function of the LDA dimensionality reduction algorithm is as\nfollows.\narg max\n|\n{z\n}\nW\nJ(W) =\nQ\ndiag W T SbW\nQ\ndiag W T SwW\n(4)\nSb =\nk\nX\nj=1\nNj (µj −µ) (µj −µ)T\n(5)\nSw =\nk\nX\nj=1\nSwj =\nk\nX\nj=1\nX\nk∈Xj\n(x −µj) (x −µj)T\n(6)\n4\nDeep Learning models\nDeep learning models have achieved state-of-the-art results in many fields and are in many ways superior to traditional\nmachine learning algorithms. Traditional machine learning algorithms require a feature extraction and classifier selection\nprocess, which increases the cost of labor. Deep learning models are end-to-end training models for many computer\nvision and natural language tasks. In many tasks, deep learning models have much better fitting and generalization\ncapabilities than traditional machine learning algorithms, and we will introduce the most common deep learning\nbackbone models, LSTM [9], GRU [5], and Transformer [10], which are mainly used in text classification tasks. ELMO\n[14], GPT [15], bert [12], GPT2 [16], XL-net [11] and other state of the art deep learning models.\n4.1\nLSTM and GRU\nRecurrent neural networks (RNNs) are the most commonly used neural network architectures for text data mining and\nclassification, especially for serial data such as textual information. Thus, RNNs have advantages in classifying text,\nstring, and sequential data, and in considering information from previous nodes on a temporal order basis.\n4.1.1\nLong Short Time memory(LSTM)\nLSTM is an improved model of RNN, which, compared to the original RNN, better overcomes the gradient disappearance\nproblem through structures such as input gates, memory cells, forgetting gates, output gates, and hidden units, thereby\nmaintaining long-term dependence. As shown in Figure 4, the forgetting gate controls whether the information from the\ncell memory of the previous time step is passed to the current time step, while the input gate controls how the input\ninformation of the current time step flows into the memory cell of the current time step through the candidate memory\ncell. This design can cope with the gradient decay problem in cyclic neural networks and better capture the dependency\nof large time step distances in time series. Therefore, the basic formula for the LSTM model is as follows.\n4\narXiv Template\nA PREPRINT\n4.1.2\nGate Recurrent Unit(GRU)\nCompared to LSTM, GRU is a simplified variant of LSTM with a reset gate and an update gate. As shown in figure 4,\nthe state of the previous time step is discarded when the element in the reset gate is close to 0, and the hidden state\nof the previous time step is retained when the element in the reset gate is close to 1. The update gate controls how\nthe hidden state should be updated by the candidate hidden state that contains information about the current time step.\nFinally, the hidden state of the current time step is a combination of the update gate of the current time step and the\nhidden state of the previous time step. The structure of GRU is as follows.\nFigure 4: The cell of LSTM and GRU\n4.2\nELMo\nELMo is a pre-training model based on Bi-direction LSTM. In previous word2vec work, each word corresponds to only\none word vector, but it is not useful for polysemous words with complex semantic information. Therefore, in ELMo,\nthe pre-trained model is no longer just a correspondence of vectors. When using ELMo, a sentence or a paragraph is\nentered into the model, and the model inferred the word vector for each word based on the context. The formula for the\nELMo model is as follows.\np (t1, t2, . . . , tN) =\nN\nY\nk=1\np (tk | t1, t2, . . . , tk−1)\n(7)\np (t1, t2, . . . , tN) =\nN\nY\nk=1\np (tk | tk+1, tk+2, . . . , tN)\n(8)\nPN\nk=1\n\u0010\nlog p\n\u0010\ntk | t1, . . . , tk−1; Θx, ⃗ΘLST M, Θs\n\u0011\n+ log p\n\u0010\ntk | tk+1, . . . , tN; Θx, ←−\nΘ LST M, Θs\n\u0011\u0011\n(9)\nFigure 5: The model of ELMo\nEquation 7 computes the objective function to be learned by the forward LSTM language model, while Equation 8\ncomputes the objective function for the backward LSTM language model. The Bi-directional LSTM language model\n5\narXiv Template\nA PREPRINT\nused by ELMo combines the forward and backward formulas to maximize the forward and backward maximum\nlikelihood probabilities, as shown in Equation 9. Thus, ELMo’s model, shown in Fig. 9, has two improvements on the\noriginal LSTM, the first being the use of a multilayer LSTM and the second being the addition of a backward language\nmodel. For the backward language model, it is similar to the forward language model in that the backward context is\ngiven to predict the forward context. When ELMo’s model completes the pre-training task, it can be used for other\nNLP tasks, or the Bi-directional LSTM can be fine-tuned and the representation of the task can be improved. This is a\ntransfer learning method that can be used for NLP text classification tasks.\n4.3\nTransformer\nAttention is all you need. The Transformer model architecture, which has recently gained great interest among\nresearchers, is based on a total attention model and has validity in the areas of language, vision, and reinforcement\nlearning. Particularly in the field of natural language, the Transformer model has gone beyond RNN and has taken\nstate of the art effects in several NLP tasks. The most important part of the Transformer model is the self-attention\nmechanism, which can be viewed as a graph-like induction bias that connects all the markers in a sequence through\nassociation-based pooling operations. Based on self-attention, the researchers proposed a multiheaded attention\nmechanism, a position-wise feed-forward network, layer normalization modules and residual connectors. Input to the\nTransformer model is often a tensor of shape (batch size, sequence length). The input first passes through an embedding\nlayer that converts each one-hot token representation into a d dimensional embedding, And than an positional encodings\nis added to the aforementioned new tensor. The formula for the Transformer is as follows.\nXA = LayerNorm(MultiheadSelfAttention (X)) + X\n(10)\nXB = Layer Norm(PositionFFN (XA)) + XA\n(11)\nAttention (Q, K, V ) = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(12)\nwhere, Q, K, V are linear transformations applied on the the temporal dimension of the input sequence. dk is the\ndimension of the vector in the multiheaded attention model and is multiplied by 1/\n√\ndk to counteract the fact that when\ndk is large, the size of the dot product increases, thus pushing the softmax function into the region of its very small\ngradient. Thus, Transformer achieves the effect of state of the art on the sequence to sequence translation task. At the\nsame time, for the text classification task, the use of transformer’s encoder model coupled with the multilayer perceptron\nnetwork (MLP) for the classification regression task also achieves good results.\nFigure 6: The model of Transformer\n4.4\nGenerative Pre-Training (GPT)\nOpen AI proposes the Generative Pre-Training (GPT) model, a language comprehension task that uses a semi-supervised\napproach to processing. The GPT task is to learn a generic language representation, which can then be fine-tuned\n6\narXiv Template\nA PREPRINT\nfor use in many downstream tasks, such as natural language processing tasks like text classification. The approach\nto unsupervised text processing is to maximize the great likelihood of the language model, hence the Transformer’s\ndecoder language model is used in the paper. Unlike Transformer’s encoder, the Mask multiple-head attention model in\nthe decoder model uses a mask that allows the model to notice only previous sequences during pre-training. Thus, the\nmulti-layer structure of GPT applies multi-headed self-attention to process the input text plus a feedforward network of\nlocation information, and the output is a conceptual distribution of words.\nSince the GPT uses a one-directional Transformer model, the model can only see the words above. The training process\nis to add Positional Encoding to the N-word word vector of the sentence and input it into the Transformer mentioned\nabove, and the N outputs predict the next word at that location. The formula for the model is as follows.\nL1(U) =\nX\nlog P (ui | ui−k, . . . , ui−1; Θ)\n(13)\nh0 = UWe + Wp\n(14)\nhl = transformerblock (hl−1) ∀i ∈[1, n]\n(15)\nP(u) = Softmax\n\u0000hnW T\ne\n\u0001\n(16)\nFigure 7: The model of GPT\n4.5\nBi-directional Encoder Representation from Transformer(Bert)\nBERT is very similar to GPT in that it is a two-stage Transformer-based training model, divided into Pre-Training and\nFine-Tuning stages. The parameters in this model are fine-tuned to adapt it to different downstream tasks. However, GPT\nuses a unidirectional Transformer, while BERT uses a bidirectional Transformer, which means no Mask operation is\nrequired. In addition, BERT uses the Encoder in the Transformer model, while GPT uses the Decoder in the Transformer\nmodel, so the pre-training methods are different for the two models. In addition, BERT uses the Masked Language\nModel (MLM) pre-training method and the Next Sentence Prediction (NSP) pre-training method, which can be trained\nat the same time.\nIn order to distinguish between two sentences, BERT adds a Segment Embedding to be learned during pre-training, in\naddition to Positional Encoding. In this way, BERT’s input consists of a word vector, a position vector, and a segment\nvector that are added together. In addition, the two sentences are distinguished from each other using <SEP> tags. The\nembedding is as follow.\nFigure 8: The Embedding of Bert\n7\narXiv Template\nA PREPRINT\nBERT’s Fine-Tuning phase is not much different from GPT. Because of the bi-directional Transformer, the auxiliary\ntraining target used by GPT in the Fine-Tuning phase, i.e., the language model, has been abandoned. In addition, the\noutput vector for classification prediction has been changed from the output position of the last word in GPT to the\nposition of <CLS> at the beginning of a sentence.\nFigure 9: The fine-tune of Bert\n4.6\nXL-Net\nXLNet combines two pre-trained model ideas, Bert and GPT, to present a state-of-the art deep learning model in the\nfield of natural language processing. From the above, we can see that GPT is a typical autoregressive language model,\nwhich has the disadvantage of not being able to use the above and below information at the same time. Bert, on the\nother hand, belongs to the Autoencoder Language Model, where Bert randomly Masks out a portion of the words in\nthe input sequence, and then one of the main tasks of the pre-training process is to predict the Masks based on the\ncontextual words. Therefore, XLNet needs to improve on Bert because the first pre-training stage takes the training\nmode of introducing [Mask] markers to Mask off some words, while the fine-tuning stage does not see such forced\nMask markers, so the two stages have inconsistent usage patterns, which may lead to some performance loss. Another\nis that Bert assumes in the first pre-training phase that multiple words are Masked out of the sentence, that there is no\nrelationship between the Masked out words, that they are conditionally independent, and that sometimes there is a\nrelationship between the words, which XLNet takes into account.\nBased on the autoregressive model, XLNet introduces the training objective of the Permutation Language Model in the\npre-training phase, assuming that the sequence is x1,x2,x3,x4, the word to be predicted is x3, and ContextBefore is\nx1,x2. To take into account the content of ContextAfter, an After the random permutation operation, the sequences\nx4,x2,x3,x1 are obtained and the model is input. In this operation, XLNet is able to take into account both the context\nand the content. This part of the improvement is achieved through the mask operation in Transformer Attention. We\nrefer to the literature for the detailed implementation process. The main idea of mask attention is as follow:\nFigure 10: The mask attention of XLNet\n4.7\nGPT-2\nGPT2 is an enhanced version of GPT, and it is based on GPT-2 with the following improvements: GPT-2 collects\na larger and more extensive dataset. At the same time, the quality of this dataset is ensured by retaining pages that\n8\narXiv Template\nA PREPRINT\nhave high-quality content. Second, GPT-2 increased the number of Transformer stack layers to 48, the hidden layer\ndimension to 1600, and the number of parameters to 1.5 billion. Third, GPT-2 increased the vocabulary to 50257, the\nmaximum context size from 512 to 1024, and the batch size from 512 to 1024. Self-attention is followed by the addition\nof a standardization layer; the initialization method of the residual layer is changed, and so on.\n5\nConlusion\nIn natural language processing tasks, deep learning-based text classification is a very important research direction.\nWith the development of the Internet and smart phones, the accurate classification of text and analysis of its content\nhas become the frontier in the field of natural language processing. The rapid progress of deep learning has largely\nreplaced the traditional machine learning methods. In this paper, we first introduce feature extraction methods for\ntext classification, such as word2vec, and GloVE, which is a method of mutual inference between central words and\ncontextual words. After that, we briefly introduce some feature reduction methods applied to text classification. We\nfocus on deep learning based text classification models. These two-stage pre-training models consist of a pre-training\nprocess and a two-stage model that is based on the pre-training process. fine tuned process. Today, these deep learning\nbased pre-trained models are the mainstay of natural language processing tasks. These models have been fine-tuned and\nhave yielded excellent results in the field of text classification.\nReferences\n[1] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura E. Barnes, and Donald E.\nBrown. Text classification algorithms: A survey. CoRR, abs/1904.08067, 2019. URL http://arxiv.org/abs/\n1904.08067.\n[2] Naoki Abe and Hiroshi Mamitsuka. Query learning strategies using boosting and bagging. In Jude W. Shavlik,\neditor, Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998), Madison,\nWisconsin, USA, July 24-27, 1998, pages 1–9. Morgan Kaufmann, 1998.\n[3] Wei Chen, Xiaoshen Xie, Jiale Wang, Biswajeet Pradhan, Haoyuan Hong, Dieu Tien Bui, Zhao Duan, and\nJianquan Ma. A comparative study of logistic model tree, random forest, and classification and regression\ntree models for spatial prediction of landslide susceptibility. CATENA, 151:147 – 160, 2017. ISSN 0341-\n8162. doi:https://doi.org/10.1016/j.catena.2016.11.032. URL http://www.sciencedirect.com/science/\narticle/pii/S0341816216305136.\n[4] Ray R. Larson.\nIntroduction to information retrieval.\nJ. Assoc. Inf. Sci. Technol., 61(4):852–853, 2010.\ndoi:10.1002/asi.21234. URL https://doi.org/10.1002/asi.21234.\n[5] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector\nspace. In Yoshua Bengio and Yann LeCun, editors, 1st International Conference on Learning Representations,\nICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013. URL http://arxiv.\norg/abs/1301.3781.\n[6] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation.\nIn Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting\nof SIGDAT, a Special Interest Group of the ACL, pages 1532–1543. ACL, 2014. doi:10.3115/v1/d14-1162. URL\nhttps://doi.org/10.3115/v1/d14-1162.\n[7] Sasan Karamizadeh, S. Abdullah, A. A. Manaf, Mazdak Zamani, and Alireza Hooman. Principal component\nanalysis. In Encyclopedia of Biometrics, 2009.\n[8] J. Hérault and B. Ans. Réseau de neurones à synapses modifiables: décodage de messages sensoriels composites\npar apprentissage non supervisé et permanent. 1984.\n[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997.\ndoi:10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\nIn Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9\nDecember 2017, Long Beach, CA, USA, pages 5998–6008, 2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.\n9\narXiv Template\nA PREPRINT\n[11] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. CoRR, abs/1906.08237, 2019. URL http:\n//arxiv.org/abs/1906.08237.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.\n04805.\n[13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations\nof words and phrases and their compositionality. In Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra-\nmani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8,\n2013, Lake Tahoe, Nevada, United States, pages 3111–3119, 2013. URL http://papers.nips.cc/paper/\n5021-distributed-representations-of-words-and-phrases-and-their-compositionality.\n[14] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.\n[15] A. Radford. Improving language understanding by generative pre-training. 2018.\n[16] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\n10\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-09-24",
  "updated": "2023-09-24"
}