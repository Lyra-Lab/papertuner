{
  "id": "http://arxiv.org/abs/2309.16564v1",
  "title": "Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings",
  "authors": [
    "Gregory Scafarto",
    "Madalina Ciortan",
    "Simon Tihon",
    "Quentin Ferre"
  ],
  "abstract": "Unsupervised learning allows us to leverage unlabelled data, which has become\nabundantly available, and to create embeddings that are usable on a variety of\ndownstream tasks. However, the typical lack of interpretability of unsupervised\nrepresentation learning has become a limiting factor with regard to recent\ntransparent-AI regulations. In this paper, we study graph representation\nlearning and we show that data augmentation that preserves semantics can be\nlearned and used to produce interpretations. Our framework, which we named\nINGENIOUS, creates inherently interpretable embeddings and eliminates the need\nfor costly additional post-hoc analysis. We also introduce additional metrics\naddressing the lack of formalism and metrics in the understudied area of\nunsupervised-representation learning interpretability. Our results are\nsupported by an experimental study applied to both graph-level and node-level\ntasks and show that interpretable embeddings provide state-of-the-art\nperformance on subsequent downstream tasks.",
  "text": "Journal of Machine Learning Research\nAugment to Interpret: Unsupervised and Inherently\nInterpretable Graph Embeddings\nGregory Scafarto\ngregory.scafarto@euranova.eu\nMadalina Ciortan\nmadalina.ciortan@euranova.eu\nSimon Tihon\nsimon.tihon@euranova.eu\nQuentin Ferr´e\nquentin.ferre@euranova.eu\nEuranova, Brussels, Belgium\nAbstract\nUnsupervised learning allows us to leverage unlabelled data, which has become abun-\ndantly available, and to create embeddings that are usable on a variety of downstream tasks.\nHowever, the typical lack of interpretability of unsupervised representation learning has be-\ncome a limiting factor with regard to recent transparent-AI regulations. In this paper, we\nstudy graph representation learning and we show that data augmentation that preserves\nsemantics can be learned and used to produce interpretations. Our framework, which we\nnamed INGENIOUS, creates inherently interpretable embeddings and eliminates the need\nfor costly additional post-hoc analysis. We also introduce additional metrics addressing\nthe lack of formalism and metrics in the understudied area of unsupervised-representation-\nlearning interpretability. Our results are supported by an experimental study applied to\nboth graph-level and node-level tasks and show that interpretable embeddings provide\nstate-of-the-art performance on subsequent downstream tasks.1\nKeywords: interpretability; unsupervised learning; graph embedding; contrastive loss\n1. Introduction\nGraphs are powerful data representations that model objects as well as the relationships\nbetween them. Much like images, they are often complex to process. A popular solution is\nto project such data into a latent space, thus creating vector embeddings, which can later be\nused by any classical machine learning training and inference pipeline. These embeddings\ncould be learned with supervision, but labelled data is often difficult to obtain. This has\nresulted in an increased interest in the learning of semantic-preserving embeddings, which\ncan later be used for a variety of downstream tasks. There are numerous ways to learn graph\nrepresentations (Hamilton et al., 2017) without label supervision, but using graph neural\nnetworks (GNN) has become the go-to approach. In particular, graph contrastive learning\n(GCL) aims at creating graph embeddings without supervision by leveraging augmented\nviews of available samples to extract meaningful information.\nThis approach provided\nstate-of-the-art results when applied to diverse domains ranging from chemistry (Duvenaud\net al., 2015) to social sciences (Fan et al., 2019).\nChen et al. (2020) demonstrated that the use of data augmentation in self-supervised\ncontrastive learning can lead to the creation of semantic-preserving embeddings. Similar re-\n1. Our code is available at https://github.com/euranova/Augment_to_Interpret.\n© G. Scafarto, M. Ciortan, S. Tihon & Q. Ferr´e.\narXiv:2309.16564v1  [cs.LG]  28 Sep 2023\nScafarto Ciortan Tihon Ferr´e\nsults have been achieved on graph data by leveraging carefully crafted augmentation (You\net al., 2020). Despite promising results, the underlying GNNs act as black-boxes. They\nare difficult to interpret, debug and trust, which raises the question: What input features\ndo GNNs focus on when generating representations?\nUsing edge and node dropping as\ngraph augmentation can lead to the creation of sub-graphs that retain discriminative se-\nmantic information while reaching better results (You et al., 2020) than other augmentation\nschemes. Interestingly, this objective correlates with the aim of GNN interpretability, which\nis to identify highly influential sparse subsets of nodes and edges with the largest impact\non the model’s behaviour (Luo et al., 2020). This observation leads us to the hypothesis\nthat a well-learned augmentation can serve to produce interpretations of the information\nembedded in GNN representations. Our main contributions follow.\n• We propose INGENIOUS (INherently INterpretable Graph and Node Unsupervised\nembeddings), a framework that generalises over existing approaches based on learned\naugmentation (Suresh et al., 2021; Gao et al., 2022; Miao et al., 2022) and we introduce\nnew losses and a module to produce useful and interpretable embeddings.\n• We show that embeddings produced by INGENIOUS are relevant to a variety of down-\nstream tasks, matching or exceeding results obtained by state-of-the-art approaches.\n• We show that carefully designed learned graph augmentation can be used to produce\ninterpretations.\nWe assess their quality along different axes introduced by Nauta\net al. (2023): correctness, completeness, continuity, compactness and coherence. To\nthis end, we introduce new metrics.\n• We conduct a hyperparameter study, highlighting the importance of sparsity when\nusing augmented views as interpretations.\nTo our knowledge, this is the first attempt at assessing the link between learned aug-\nmentation and interpretability. We characterise it on graph-level and node-level tasks. The\nstructure of the paper is as follows: We first position our work in Section 2 and present our\nframework in detail in Section 3. In Section 4, we assess the utility and interpretability of\nthe framework by introducing necessary metrics and we perform an in-depth study of the\nproperties of the framework. Finally, we discuss limitations and conclude.\n2. Related Work\nPrevious works on other modalities have shown the superiority of augmentation-based con-\ntrastive learning (Chen et al., 2020). However, adapting these works so that they work\nreliably on graph data raised new challenges, such as defining the right augmentation tech-\nniques amongst existing augmentation techniques (Ding et al., 2022). Early methods, in-\ncluding GraphCL (You et al., 2020) and InfoCL (Wang et al., 2022b), focus on random\nperturbations produced by dropping edges or nodes. Other methods like graphMVP (Liu\net al., 2022) and MICRO-Graph (Subramonian, 2021) use domain knowledge to perform\nbetter than random augmentation. More recent approaches such as AD-GCL (Suresh et al.,\n2021) and MEGA (Gao et al., 2022) use a multilayer perceptron (MLP) as an augmentation\nAugment to Interpret\nlearner. They create augmented views by learning to drop low-relevance edges. Similarly,\nRGCL (Li et al., 2022) adds a repulsive dynamic to the augmented views.\nIn parallel, considerable work has been done to interpret graph neural networks post\ntraining. A backpropagation of gradient-like signals (Simonyan et al., 2014), perturbations\nof the input graph (Ying et al., 2019; Luo et al., 2020), and the training of a surrogate\nmodel (Vu and Thai, 2020) are examples of such techniques. These post-hoc techniques\nhave been shown to be biased and unrepresentative of the true interpretations (Rudin,\n2019), leading to an increased interest in inherently interpretable models (Miao et al.,\n2022; Feng et al., 2022). Furthermore, the aforementioned methods focus on interpreting\npredictions of models learned with supervision. Zheng et al. (2022) have proposed USIB,\nthe first post-hoc method to interpret unsupervised graph representations, but USIB faces\nthe same faithfulness limitation as other post-hoc techniques.\nGiven existing limitations and the power of augmentation-based learning, we investigate\nhow augmentation learning can faithfully and intrinsically provide interpretability.\n3. Materials and Methods\nIn this section, we present INGENIOUS. We begin with a general overview and then provide\na description of the augmentation and learning processes.\n3.1. Framework\nWe propose an inherently interpretable framework that produces graph representations\nin an unsupervised manner by leveraging learned graph augmentation, as shown in Fig-\nure 1. Learned edge-dropping augmentation has been used both in contrastive learning by\nAD-GCL (Suresh et al., 2021) and MEGA (Gao et al., 2022), two recent state-of-the-art ap-\nproaches, and in interpretability by GSAT (Miao et al., 2022) and PGExplainer (Luo et al.,\n2020). Inspired by said contrastive approaches, INGENIOUS is trained with a contrastive\nloss, following a dual-branch approach as recommended by Chen et al. (2020). INGENIOUS\nis based on two modules: an embedding module that produces node embeddings (ϕu) and\ngraph embeddings (ϕ), and an edge-selection module (θ) that produces semantic-preserving\nsub-graphs by stochastically dropping uninformative edges. In this study, the embedding\nmodule is implemented as a graph neural network (GNN) encoder and the edge-selection\nmodule as an MLP, as described in Section 4.1.\nThe embedding module first generates an embedding ϕu for each node u of the in-\nput graph. Using these node embeddings, the edge-selection module θ produces a keeping\nprobability puv ∈[0, 1] for each edge (u, v) of the graph. Then, the Gumbel-max reparam-\neterisation trick is used to partially remove edges according to their keeping probability\npuv in a differentiable manner. Each edge of an augmented view therefore has a weight\nwuv ∈[0, 1].\nThis edge-selection process is applied twice to obtain two positively augmented views. A\nnegatively augmented view is also produced by giving each edge the flipped weight 1 −wuv\nfrom the first positively augmented view. The stochastic aspect of this data augmentation\nincreases the robustness of the model, as it shows more diverse data to the model. Ad-\nditionally, this aspect is needed to apply the dual-branch contrastive loss, since this loss\nnecessitates at least two distinct augmented views for each graph.\nScafarto Ciortan Tihon Ferr´e\nFigure 1: Schema of the INGENIOUS framework, as described in the text.\nEach augmented view of the input graph is then embedded using once again2 the em-\nbedding module ϕ.\nFinally, a simclr loss is back-propagated to pull the embeddings of\ncorresponding positively augmented views together while keeping the embeddings of other\ngraphs’ positively augmented views distant in the embedding space. The negatively aug-\nmented view is also used as a part of the final loss, as described below.\nAt inference time, the Gumbel-max sampling is no longer used. Keeping probabilities\npuv are therefore not used as probabilities anymore but are used directly as weights on edges\nof the original graph, i.e. wuv = puv, in order for the augmentation to be deterministic.\nThe full-graph embedding is defined as the embedding of the augmented view, to be used in\nany downstream task. This augmented view serves as the interpretation of the embedding.\n3.1.1. Embedding Module and Augmented-Graph Generation\nIn this work, we consider attributed graphs G = (V, E) where V is the set of nodes and\nE ⊂{(u, v)|u ∈V, v ∈V } is the set of edges of the graph. By giving an arbitrary order\nto V , each node can be represented by a single integer u ∈N. G can then be described\nby its node-feature matrix X ∈R|V |×d and its adjacency matrix A ∈R|V |×|V |, with |V |\nthe total number of nodes in the graph, d the number of node features, Xui the value of\nthe ith feature of the uth node u, and Auv = 1 if (u, v) ∈E else 0. Node embeddings can\nbe computed by the embedding module as zu = ϕu (A, X).\nGraph embeddings can be\ncomputed as z = ϕ (A, X) = Pooling({zu ∀u ∈V }), with Pooling an aggregation function\non sets of vectors, such as the mean or the sum.\nAugmented views of a graph G can be produced in two different ways given the embed-\nding zu of each of its nodes u. The first way, which was described briefly above, is this: The\nedge-selection module θ produces a keeping probability puv = θ([zu, zv]) for each edge (u, v),\nwith [·, ·] the concatenation operator. Then, edges are sampled following the Gumbel-max\nreparameterization (Gumbel, 1954), giving each edge its weight\nwuv = Sigmoid ((Logit(puv) + ϵuv)/τ)\n(1)\n2. When a module includes batch normalisation layers, it is tricky to use this same module for distinct\nsteps in a model, because the input distribution of the batch normalisation layers may be different for\neach step the module is used for. This aspect was not considered in previous approaches and may have\nperturbed their results and conclusions. More details are given in supplementary materials.\nAugment to Interpret\nwith Logit the inverse function of the Sigmoid function, ϵuv = log(αuv)−log(1−αuv) a ran-\ndom noise defined using αuv ∼Uniform(0, 1), and τ ∈R+ a temperature hyperparameter\nwe set to 1. The higher the value of τ, the more uniform the weight distribution.\nThe second way is to produce a keeping probability pu = θ(zu) for each node, sample\nnodes by giving them a weight wu = Gumbel(pu) with Gumbel the Gumbel-max trick\ndescribed above, and then lift the weights from nodes to edges following wuv = wu · wv.\nAlthough both could be used for both graph and node embeddings, in our approach\nwe use the first way for graph embeddings and the second way for node embeddings, since\nit works best empirically. These edge weights are then used to weigh messages during the\nmessage-passing operation of the GNN encoder. At train time, independent samplings are\nused to generate two distinct positively augmented views. The negatively augmented view is\nobtained by flipping the first positively augmented view, that is, each edge of the negatively\naugmented view has a weight wuv = 1−w+\nuv, with w+\nuv the weight of the edge (u, v) in the first\npositively augmented view. Finally, the two positively augmented views and the negatively\naugmented view are embedded by the embedding module, which produces embeddings z+\n1 ,\nz+\n2 and z−. At inference time, the deterministic process described previously is used.\n3.1.2. Losses\nIn this section, we define the INGENIOUS loss. It is composed of four losses with diverse\nobjectives.\nThe first loss is defined as the dual-branch approach of the simclr loss (Chen et al.,\n2020), which leverages two augmented views rather than an anchor and an augmented\nview, as proposed by Zhu et al. (2021).\nThis is the main loss of the framework as it\nstructures the embedding space.\nSpecifically, given augmented-view-embedding triplets\ncoming from N distinct graphs and represented as {Z+\n1 , Z+\n2 , Z−} ⊂RN×D, with D the\nembedding dimension, we first normalise each embedding so that it has an L2-norm of 1.\nThen, the loss is defined as\nLsimclr(Z+\n1 , Z+\n2 ) = −1\n2N\nN\nX\ni=1\n\u0010\nlog(li(Z+\n1 , Z+\n2 )) + log(li(Z+\n2 , Z+\n1 ))\n\u0011\n(2)\nli(Z+\n1 , Z+\n2 ) =\nexp(sim(Z+\n1i, Z+\n2i)/β)\nPN\nk=1\n\u0010\nexp(sim(Z+\n1i, Z+\n2k)/β) + I[k̸=i] exp(sim(Z+\n1i, Z+\n1k)/β)\n\u0011\n(3)\nwith sim(·, ·) a similarity operation such as the dot product, I[condition] an indicator function\nwith a value of 1 when the condition is met, else 0, and β a hyperparameter we set to 0.07\nas in Chen et al. (2020).\nThe second loss, which we call the negative loss, complements the first loss in structuring\nthe latent space. It leverages the negatively augmented views of the normalised triplets:\nL−(Z+\n1 , Z−) =\nN\nX\ni=1\nLsimclr(Z∗\ni , Z∗\ni )\n(4)\nwith Z∗\ni = {Z+\n1i, Z−\ni } ∈R2×D. The motivation behind this idea stems from the observation\nthat the simclr loss has an attracting effect between corresponding augmented-view em-\nbeddings and a repelling effect between everything else. If corresponding augmented-view\nScafarto Ciortan Tihon Ferr´e\nembeddings are always an embedding and itself, there is no attracting effect anymore as\nsomething cannot attract itself. Thus, only the repelling effect of the simclr loss has an\neffect, which is a repelling effect between the positively-augmented-view embedding and\nthe negatively-augmented-view embedding.\nThe third loss is the information loss from GSAT. It is used as regularisation. This loss\nminimises the mutual information between the positively augmented view and its original\ngraph anchor following the graph information bottleneck (GIB) principle (Miao et al., 2022).\nIt favours sparser augmented views as it prevents the model from collapsing and selecting\nall edges as important. The information loss is defined as:\nLinfo(G, P) =\n1\n|E|\nX\n(u,v)∈E\npuv log puv\nr\n+ (1 −puv) log 1 −puv\n1 −r\n(5)\nwith P = {puv∀(u, v) ∈E} the keeping probabilities of all edges, G = (V, E) a batch of\ngraphs as one big graph, |E| the total number of edges and r ∈[0, 1] a hyperparameter\nthat represents the prior of the probability to keep any given edge. Similarly to GSAT, the\nhyperparameter r is initially set to 0.9 and gradually decays to the tuned value of 0.7. As\nproposed in GSAT, the weights wuv are used in practice instead of the probabilities puv.\nThe fourth and last loss is the watchman loss. Wang et al. (2022a) have shown that\nedge-dropping augmentation can degrade the quality of the latent space. Additionally, we\nhave observed that learned-augmentation-based methods sometimes have unstable training.\nPreliminary experiments supporting this claim are given in supplementary materials. The\nwatchman loss mitigates these issues by forcing the embeddings to contain graph-level\ninformation.\nIn detail, a watchman module ψ uses augmented-view embeddings Z+\n1 to\npredict Λk(Gi)∀i, the top k eigenvalues of the Laplacian matrix of each graph Gi composing\nthe batch G. The loss is defined as\nLWm(G, Z+\n1 ) = 1\nN\nN\nX\ni=1\nMSE(ψ(Z+\n1i), Λk(Gi))\n(6)\nwith MSE(·, ·) the squared euclidean distance between two vectors. This loss is meaningful\nfor graph-embedding tasks only, as node embeddings have no reason to contain information\nabout the full graph.\nFinally, the INGENIOUS loss is defined on a batch G of N graphs as:\nL = Lsimclr(Z+\n1 , Z+\n2 ) + L−(Z+\n1 , Z−) + Linfo(G, W) + λWm · LWm(G, Z+\n1 )\n(7)\nwith W = {wuv∀(u, v) ∈E} the edge weights of the first positively augmented view and\nλWm a hyperparameter we set to 0 for node-embedding tasks and to 0.3 for graph-embedding\ntasks, based on preliminary empirical results.\n4. Experiments\nIn this section, we evaluate INGENIOUS both in terms of utility and interpretability by\ncomparing its performance to the performance of its unsupervised competitors AD-GCL and\nMEGA on graph-classification tasks and we conduct an ablation study on the loss terms.\nAugment to Interpret\nFigure 2: Downstream ACC and interpretability AUC. INGENIOUS outperforms its unsu-\npervised competitors, AD-GCL and MEGA.\nHowever, these competitors have not been extended to handle node-classification tasks, and\nto the best of our knowledge, there is no competitor that uses learned augmentation for\nthese tasks. For both graph and node classification tasks, we use GSAT not as a direct\ncompetitor, as it is a supervised model, but as an upper bound on the obtainable utility.\n4.1. Experimental Setup\nDatasets\nFor graph classification, we use two synthetic datasets (BA2Motifs (Luo et al.,\n2020), SPMotifs.5 (Wu et al., 2022)) and one real-world dataset (Mutag (Morris et al.,\n2020)). For node classification, we use two synthetic datasets (Tree-grid, Tree-cycle (Ying\net al., 2019)) and one real-world dataset (Cora (McCallum et al., 2000)). All of them are\nused with a batch size of 256 except for Cora which is used with a batch size of 128. For\nMutag and BA2Motifs, we randomly split the data into train (80%), validation (10%) and\ntest (10%) sets. For the other datasets, we use the same split as in their original papers.\nModel\nFor the embedding module ϕ, we use a GIN (Xu et al., 2019) model with 3 layers.\nFor graph-classification tasks, a final pooling layer is added to the model. For the edge-\nselection module θ, we use a 2-layer MLP. For the watchman module ψ, we use a 3-layer\nMLP. We use a learning rate of 1e-3 and a dropout ratio of 0.3, and we train the whole on\n150 epochs. For all approaches, the metrics defined in Sections 4.2 and 4.3 are measured\non the model that obtains the best downstream ACC (see Section 4.2) measured on the\nvalidation set, amongst all epochs, as in GSAT. All results are averaged on 3 random seeds.\n4.2. Utility\nThe utility of embeddings learned without supervision is traditionally (Suresh et al., 2021)\napproximated by the performance of simple models trained on these embeddings and their\nlabels across various downstream tasks. Similarly, we feed the representations to be evalu-\nated to a linear model and we train the latter on the classification task associated with the\navailable labels. The final metric is the accuracy of the model, which we call downstream\nACC. As visible in Figure 2, the accuracy of INGENIOUS is similar to the accuracy of its\ncompetitors (AD-GCL and MEGA) in the case of graph classification. Furthermore, it is\nclose to the supervised upper bound (GSAT) despite being unsupervised.\nScafarto Ciortan Tihon Ferr´e\n4.3. Interpretability\nThe idea of interpreting representations can be defined as finding informative parts of\nthe input that led to its representation. Like most graph-interpretability methods (Yuan\net al., 2020), INGENIOUS focuses on topology and produces sub-graphs as interpretations.\nExamples of such interpretations are visible in Figure 3. Interpretability and how to measure\nit has been extensively studied in the supervised setting (Nauta et al., 2023), but little work\nhas focused on interpretability for unsupervised representation learning. Monotony and\nlocalisation metrics have been introduced by (Wickstrøm et al., 2023; Zheng et al., 2022),\nbut most of the major categories introduced by the taxonomy proposed by Nauta et al.\n(2023) have not been covered yet. Building on previous work, we introduce five metrics -\nFaithfulness, Opposite faithfulness, Wasserstein distance, Bi-modality and Interpretability\nAUC -, all based on desirable characteristics for interpretations.\nCorrectness and Completeness\nCorrectness (how much the interpretation truly de-\nscribes the behaviour of the model) and completeness (how much of the model behaviour\nis described by the interpretation) (Nauta et al., 2023) are studied by evaluating whether\nan interpretation aligns with the edges that truly impact the embedding.\nThe canoni-\ncal approach to evaluating it is the deletion (resp. insertion) experiment introduced by\nRISE (Petsiuk et al., 2018), which involves removing elements in order (resp.\nopposite\norder) of importance and studying the impact on model outputs. Usually, this impact is\nmeasured as a difference in predicted scores. However, in the unsupervised setting, outputs\nare not predicted scores, but embeddings. Therefore, we propose to extend the metric by\nmeasuring this impact as the Euclidean distance between the embedding of the original\ngraph and the embeddings of the perturbed graphs.\nTo evaluate correctness, we use an adapted faithfulness metric. Given a graph, we delete\nits edges in decreasing order of importance weights given by the edge-selection module. To\nachieve this, we group edges of similar importance into bins. In our experiments, we use\n30 bins. Then, we successively remove bins of edges from the graph in decreasing order\nof importance, going from the full graph to a graph without any edge.\nThe perturbed\ngraphs are then embedded using the embedding module on its own, taking the importance\nweights wuv of the remaining edges into account during the message-passing operation of the\nmodule. The final metric is the area under the curve (AUC) of dissimilarity scores across\ndifferent percentages of edges removed. For graph classification, the dissimilarity score is\nFigure 3: Illustrative examples of interpretations produced by INGENIOUS on four\nBA2Motifs graphs selected randomly. Nodes included in the class-specific motifs\nare shown in red. Edge importance weights produced by INGENIOUS are repre-\nsented through edge thickness. Therefore, thick edges between red nodes indicate\nthat INGENIOUS focuses on the informative motifs.\nAugment to Interpret\nTable 1: Faithfulness and Wasserstein results. A higher faithfulness gap means the inter-\npretations are more faithful, and a higher Wasserstein gap means the embedding\nspace is more continuous in terms of interpretations.\nMethod\nFaithfulness Gap\nWasserstein Gap\nBA2Motifs\nMutag\nSPMotifs.5\nBA2Motifs\nMutag\nSPMotifs.5\nGSAT\n.34 ± .00\n.21 ± .08\n.06 ± .02\n.04 ± .00\n.02 ± .00\n.04 ± .00\nAD-GCL\n.01 ± .13\n.03 ± .09\n.08 ± .06\n.10 ± .01\n.01 ± .00\n.03 ± .00\nMEGA\n.06 ± .02\n.39 ± .12\n.68 ± .03\n.04 ± .01\n.01 ± .00\n.01 ± .00\nINGENIOUS\n.60 ± .02\n.41 ± .18\n.45 ± .03\n.06 ± .01\n.01 ± .00\n.05 ± .01\nINGENIOUS - Info\n.68 ± .02\n.48 ± .03\n.48 ± .06\n.05 ± .02\n.01 ± .00\n.05 ± .01\nINGENIOUS - Negative\n.13 ± .02\n.18 ± .00\n.22 ± .06\n.02 ± .01\n.01 ± .00\n.02 ± .01\nINGENIOUS - Negative - Info\n.61 ± .10\n.15 ± .09\n.32 ± .00\n.03 ± .01\n.01 ± .00\n.03 ± .00\nthe Euclidean distance between the embeddings of the pruned graphs and the embedding of\nthe initial graph. For node classification, we perform this experiment for each node, taking\nonly the k-neighbourhood (k being the depth of the model) of that node into consideration,\nand we take the average dissimilarity score. In order to ease comparisons between models,\nthese dissimilarity scores are normalised by scaling, to be equal to 1 when all edges are\nremoved. Removing important edges should change the embedding. Hence, removing them\nfirst should result in a larger AUC.\nTo evaluate completeness, we use the opposite faithfulness. It seeks to assess the com-\nplementary statement to faithfulness: Are the edges marked as unimportant truly unim-\nportant? To measure it, we apply the same procedure as for the faithfulness metric, except\nedges are removed in increasing order of predicted importance. In theory, removing unim-\nportant edges should not impact the embedding. The AUC should therefore be small.\nTo summarise the results intuitively, we focus on the faithfulness gap, that is, the dif-\nference in value between the faithfulness and the opposite faithfulness. A larger difference\nis better. Results are presented in Table 2. Faithfulness and opposite faithfulness results\nare presented in supplementary material. The model we propose has a clear advantage,\nachieving a high faithfulness gap in all cases, even higher than the supervised GSAT model.\nThese results demonstrate the superior correctness and completeness of INGENIOUS.\nContinuity\nContinuity refers to the smoothness of the interpretability function (Nauta\net al., 2023). We seek to evaluate whether inputs for which the model response is similar,\nthat is, inputs with similar embeddings, also have similar interpretations. However, much\nlike faithfulness, no metrics have been defined in the literature to evaluate this in an un-\nsupervised setting. To evaluate it anyway, we need to define (dis)similarity, both between\nembeddings and between interpretations. While embeddings lie in a Euclidean space, where\nthe L2-norm can be used as a dissimilarity measure, the interpretations lie in the space of\ngraphs. There is no consensual method to compare any two graphs or sub-graphs. Indeed,\na large variety of methods have been proposed (McCabe et al., 2021). Amongst them, we\nchoose to use the node-degree-based Wasserstein distance as a dissimilarity measure, as\nit is amongst the simplest to compute and the most intuitive. In line with the literature\non graph interpretability and augmentation, which focuses solely on graph topology, this\nScafarto Ciortan Tihon Ferr´e\nmeasure disregards node features. It also has the advantage of enabling its use with any\ngraph dataset. The continuity metric we propose could be adapted with any other similarity\nmetric with minor changes, but the adaptation would not significantly alter its theoretical\nanalysis. The study of possible adaptations is therefore left for future work.\nFormally, given a graph G, we obtain its interpretation Gexp by applying a hard-\nthreshold.\nWe keep a fixed percentage of the most important edges, arbitrarily set to\n10%. However, for datasets for which there is a prior on the number of important edges for\na downstream task, we use that prior number instead of 10%.\nThe Wasserstein distance (WD) between two graphs G1 and G2 serves as the dissimi-\nlarity between their interpretations and is defined as dW (G1, G2) = W1(ν(Gexp\n1\n), ν(Gexp\n2\n)),\nwith ν(G) the distribution of node degrees in G and W1 the 1-Wasserstein metric. Let π(G)\nbe the local neighbourhood of a graph G, defined as the 10% closest graphs of the dataset\nin the embedding space according to the Euclidean distance. To evaluate the continuity of\ninterpretations, we compare two values:\n• W local\n1\n= Ei\nh\nEGj∈π(Gi) [dW (Gi, Gj)]\ni\n, the expected WD between a graph and another\ngraph in the local neighbourhood of the first one;\n• W global\n1\n= Ei [Ej [dW (Gi, Gj)]], the expected WD between any two graphs.\nTo save time, the expected values are computed on a subset of 6 batches of graphs,\ni.e. 1536 graphs. A big difference between these two values would be a good indicator\nof continuity, as it would mean that graphs that are similar in the embedding space have\ninterpretations that are more similar to each other than to the interpretations of other\ngraphs. To summarise the results intuitively, we focus on the difference W global\n1\n−W local\n1\n; a\nlarger gap indicates a stronger continuity. We report these differences in Table 2. We see the\nadvantage provided by the negative term of the INGENIOUS loss to produce an embedding\nspace with continuous interpretations, as visible by the higher scores of INGENIOUS and\nINGENIOUS - Info compared to INGENIOUS - Negative and INGENIOUS - Negative -\nInfo, on BA2Motifs and SPMotifs.5. As opposed to Mutag, these datasets have clear distinct\nmotifs by class and therefore most benefit from the repulsive effect of the negative loss.\nCompactness and readability\nPrevious work has shown that sparsity is an important\nfactor in human understanding (Nauta et al., 2023), as it is hindered by human cognitive\ncapacity limitations, and that interpretations should be sparse to not overwhelm the user.\nMoreover, for inherently interpretable methods as INGENIOUS, sparsity is related to faith-\nfulness as non-sparse interpretations would mean that all features are used by the model\nto produce the embeddings. Nevertheless, we argue that sparsity, alone, can be mislead-\ning, since it can favour interpretations with globally lower weights. An ideal interpretation\nshould be bi-modal, with weights of 1 for important edges and 0 for unimportant ones.\nWe, however, relax this assumption and consider that an interpretation is easy to read if\nthe distribution of weights has two modes. To estimate this, we approximate the density\nfunction of edge weights by using a histogram with ten bins between 0 and 1. We also define\nthe sparsity index as the average of the edge importance weights.\nFigure 4 shows that on simple datasets such as BA2Motifs, all methods produce sparse\nand bimodal interpretations, even without regularisation. However, more complex datasets\nAugment to Interpret\nFigure 4: Density function of edge weights. Optimal interpretations should have two density\npeaks and maximal separation. INGENIOUS results in importance values that\nare much sparser and closer to extreme values.\nsuch as Mutag and SPMotifs.5 highlight the need for regularisation. On Mutag for example,\nadding the regularisation lowers the proportion of scores between 0.9 and 1 from 90% to\n30%. Both AD-GCL and GSAT tend to collapse on high value and narrow-support densities,\nas opposed to MEGA which has importance scores closer to extreme values.\nCoherence\nCoherence assesses the alignment with domain knowledge. Sometimes, in the\nsupervised setting, human prior can be used to define expected interpretations that can be\ncompared with generated interpretations. In the literature, this is referred to as the clicking\ngame (Wickstrøm et al., 2023).\nHowever, it relies on the prior that the model focuses\non the same features as the domain expert would, which is a strong assumption (Petsiuk\net al., 2018).\nIn an unsupervised setting, it is even harder to define relevant expected\ninterpretations for embeddings. A way to approach this is to use synthetic datasets only;\nwe use BA2Motifs, SPMotifs.5, Tree-grid and Tree-cycle. In these datasets, base graphs\nare generated from the same distribution. Then, different motifs are added to each random\ngraph (Wu et al., 2022; Ying et al., 2019; Luo et al., 2020) depending on the dataset. For\ngraph classification datasets, the motif also depends on the label of the graph. Therefore,\nwe hypothesise that the base graph can be considered as noise and that all the distinctive\ninformation of each graph is in the motifs.\nUnder this hypothesis, any good unsupervised embedding method should rely on the\nmotifs only. This is partially validated by the good accuracy shown in Section 4.2, as it is\nimpossible to have good accuracy on the synthetic datasets if the embeddings contain no\ninformation about the motifs. The motifs can therefore be used as expected interpretations.\nTo measure how close a produced interpretation is to the expected one, we consider an\nedge-classification problem. An edge is labelled as important if it is part of the expected in-\nterpretation. The predicted probability of an edge being important is given by the produced\nimportance weight of that edge. The area under the ROC curve is reported in Figure 2. In\nthis paper, we call this metric the interpretability AUC.\nIn terms of interpretability AUC, INGENIOUS outperforms its unsupervised competi-\ntors. The supervised approach GSAT is better as expected. The ablation study shows that\nboth regularisation losses are needed to reach a better interpretability AUC, although their\nadvantage is less significant on node classification tasks. This could find an explanation in\nthe link between interpretability and sparsity. We study this link in Section 4.4.\nScafarto Ciortan Tihon Ferr´e\n4.4. In-Depth Analysis of the Sparsity\nImpact of the sparsity on faithfulness\nIn addition to being a desirable character-\nistic for interpretations, sparsity impacts contrastive learning by increasing the difference\nbetween the augmented views. Figure 5 shows a significant positive correlation between\nopposite faithfulness (red dots) and sparsity, while there is a negative correlation between\nfaithfulness (blue dots) and sparsity. This suggests there may be a link between higher spar-\nsity and better faithfulness gap. This conclusion applies to all graph-classification tasks.\nFor node-classification tasks, the sparsity has a smaller range of values and presents no\ncorrelation with the faithfulness but still a small correlation with the opposite faithfulness.\nFurther results with a random baseline are presented in supplementary materials. Overall,\nthe faithfulness gap is improved by the sparsity.\nFigure 5: (Opposite) faithfulness with respect to the sparsity index. A lower sparsity re-\nsults in a lower opposite faithfulness (lower is better) and a marginally higher\nfaithfulness, which means a better faithfulness gap. The grey area represents the\n95% confidence interval for a smoothed conditional mean.\nImpact of hyperparameters on sparsity\nThis section deals with the impact of the\ntemperature of the Gumbel-max sampling and the impact of the edge-keeping-probability\nprior r. A high temperature drives the sampling probability closer to a uniform distribution,\nwhich increases the diversity of augmented views. Figure 13 shows that sparsity decreases\nwith an increasing temperature value until it reaches a plateau. On the synthetic dataset\nBA2Motifs, a temperature sweet-spot for interpretability AUC is visible at a temperature\nequal to 20; too high or too low of a temperature is harmful.\nHowever, for the other\nexperiments of this paper, to ensure the comparability of our results with those of GSAT,\nwe use the same temperature value as in GSAT, i.e. τ = 1, even though this choice might\nnot result in optimal performances.\nThe first column of Figure 7 shows that r is positively correlated with the sparsity\nof INGENIOUS. Lower r leads to a better faithfulness gap, meaning more complete and\ncorrect interpretations.\nSparser augmented views lead the model to use fewer edges to\nproduce embeddings, which results in a slight drop in downstream ACC. This phenomenon\nis less marked on simple datasets where all methods reach a perfect downstream ACC. This\ntrade-off between downstream ACC and interpretability has already been documented in\nprevious work (Jacovi and Goldberg, 2020). Similarly, the sparsity positively impacts the\nAugment to Interpret\nFigure 6: Sparsity and interpretability AUC on BA2Motifs with different temperatures. A\nsweet spot is visible at 20; too high or too low temperatures worsen the results.\ncontinuity of the interpretability function, as can be seen by the increasing Wasserstein\ngap. This impact, however, is less strong on Mutag. In INGENIOUS, the sparsity can be\ncontrolled to an extent through regularisation and hyperparameter tuning.\nFigure 7: Analysis of the hyperparameter r. A lower r hyperparameter can improve the\nsparsity, resulting in interpretations that are more correct, complete and robust,\nwith only a slight impact on the downstream ACC.\n5. Limitations and Future Work\nLike most GNN-interpretability methods (Luo et al., 2020; Ying et al., 2019; Yuan et al.,\n2020), we limit our interpretability analysis to topology. However, early results in sup-\nplementary materials show the possibility of extending INGENIOUS to node features. As\ndefined by Doshi-Velez and Kim (2018), we limit our study to a functionally grounded eval-\nuation (which does not require human subjects). Both human-grounded and application-\ngrounded evaluations, which are more specific and costly, are left as future work.\n6. Conclusion\nWe unify previous works on augmentation-based graph representation learning to introduce\na new unsupervised embedding approach. Our method, INGENIOUS, introduces new losses\nfor better regularisation. We show their positive impact through an ablation study. Aiming\nfor interpretability, we introduce new metrics to evaluate the quality of interpretations\nof graph representations learned without supervision, building upon previous work in the\nsupervised setting. Thanks to the approach and metrics we propose, we show that it is\nScafarto Ciortan Tihon Ferr´e\npossible to produce inherently interpretable embeddings that are also useful for various\ndownstream tasks and that augmented views, when sparse, can be used as interpretations.\nINGENIOUS compares favourably with state-of-the-art unsupervised models in terms of\nutility of the embeddings and correctness, completeness, continuity and readability of the\ninterpretations. By tuning a reduced set of two hyperparameters, the user can manage\nthe sparsity/utility trade-off. Our work paves the way for novel approaches of intrinsically\ninterpretable unsupervised embeddings for modalities as complex as graph data.\nAugment to Interpret\nReferences\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frame-\nwork for contrastive learning of visual representations. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, ICML’20. JMLR.org, 2020.\nKaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph\nlearning: A survey. SIGKDD Explor. Newsl., 24(2):61–77, dec 2022. ISSN 1931-0145.\nFinale Doshi-Velez and Been Kim.\nConsiderations for evaluation and generalization in\ninterpretable machine learning. Explainable and interpretable models in computer vision\nand machine learning, pages 3–17, 2018.\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy\nHirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for\nlearning molecular fingerprints. In Advances in Neural Information Processing Systems,\nvolume 28. Curran Associates, Inc., 2015.\nWenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph\nneural networks for social recommendation. In The World Wide Web Conference, WWW\n’19, page 417–426, New York, NY, USA, 2019. Association for Computing Machinery.\nISBN 9781450366748.\nAosong Feng, Chenyu You, Shiqiang Wang, and Leandros Tassiulas. Kergnns: Interpretable\ngraph neural networks with graph kernels. CoRR, abs/2201.00491, 2022.\nHang Gao, Jiangmeng Li, Wenwen Qiang, Lingyu Si, Fuchun Sun, and Changwen Zheng.\nBootstrapping informative graph augmentation via a meta learning approach. In Proceed-\nings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-\n22, pages 3001–3007. International Joint Conferences on Artificial Intelligence Organiza-\ntion, 7 2022. Main Track.\nE. J. Gumbel. Statistical theory of extreme values and some practical applications. lectures\nby emit j. gumbel. national bureau of standards, washington, 1954. 51 pp. diagrams. 40\ncents. The Aeronautical Journal, 58(527):792–793, 1954.\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs:\nMethods and applications. IEEE Data Eng. Bull., 40(3):52–74, 2017.\nAlon Jacovi and Yoav Goldberg. Towards faithfully interpretable NLP systems: How should\nwe define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 4198–4205, Online, July 2020. Association\nfor Computational Linguistics.\nSihang Li, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Let invariant ratio-\nnale discovery inspire graph contrastive learning. In ICML, 2022.\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang.\nPre-training molecular graph representation with 3d geometry. In International Confer-\nence on Learning Representations, 2022.\nScafarto Ciortan Tihon Ferr´e\nDongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and\nXiang Zhang. Parameterized explainer for graph neural network. Advances in Neural\nInformation Processing Systems, 33, 2020.\nStefan McCabe, Leo Torres, Timothy LaRock, Syed Arefinul Haque, Chia-Hung Yang,\nHarrison Hartle, and Brennan Klein. netrd: A library for network reconstruction and\ngraph distances. Journal of Open Source Software, 6:2990, June 2021.\nAndrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automat-\ning the construction of internet portals with machine learning. Information Retrieval, 3:\n127–163, 2000.\nSiqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic\nattention mechanism. International Conference on Machine Learning, 2022.\nChristopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and\nMarion Neumann.\nTudataset: A collection of benchmark datasets for learning with\ngraphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+\n2020), 2020.\nMeike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt,\nJ¨org Schl¨otterer, Maurice van Keulen, and Christin Seifert. From anecdotal evidence to\nquantitative evaluation methods: A systematic review on evaluating explainable AI. ACM\nComputing Surveys, feb 2023.\nVitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explana-\ntion of black-box models. In British Machine Vision Conference, 2018.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions\nand use interpretable models instead. Nature Machine Intelligence, 1:206–215, 05 2019.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional net-\nworks: Visualising image classification models and saliency maps. In 2nd International\nConference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16,\n2014, Workshop Track Proceedings, 2014.\nArjun Subramonian. Motif-driven contrastive learning of graph representations. Proceedings\nof the AAAI Conference on Artificial Intelligence, 35(18):15980–15981, May 2021.\nSusheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation\nto improve graph contrastive learning. In Advances in Neural Information Processing\nSystems, 2021.\nMinh N. Vu and My T. Thai. Pgm-explainer: Probabilistic graphical model explanations\nfor graph neural networks. In Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual, 2020.\nHaonan Wang, Jieyu Zhang, and Wei Huang. Augmentation-free graph contrastive learning\nwith performance guarantee. ArXiv, abs/2204.04874, 2022a.\nAugment to Interpret\nHaoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu.\nRethinking minimal sufficient\nrepresentation in contrastive learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 16041–16050, 2022b.\nKristoffer K. Wickstrøm, Daniel J. Trosten, Sigurd Løkse, Ahc`ene Boubekki, Karl øyvind\nMikalsen, Michael C. Kampffmeyer, and Robert Jenssen. Relax: Representation learning\nexplainability. Int. J. Comput. Vision, 131(6):1584–1610, mar 2023. ISSN 0920-5691.\nYing-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering\ninvariant rationales for graph neural networks. CoRR, abs/2201.12872, 2022.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph\nneural networks? In International Conference on Learning Representations, 2019.\nRex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. GNN ex-\nplainer: A tool for post-hoc explanation of graph neural networks. CoRR, abs/1903.03894,\n2019.\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen.\nGraph contrastive learning with augmentations. In Advances in Neural Information Pro-\ncessing Systems, volume 33, pages 5812–5823. Curran Associates, Inc., 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji.\nExplainability in graph neural\nnetworks: A taxonomic survey.\nIEEE transactions on pattern analysis and machine\nintelligence, PP, 2020.\nQinghua Zheng, Jihong Wang, Minnan Luo, Yaoliang Yu, Jundong Li, Lina Yao, and Xi-\naojun Chang. Towards explanation for unsupervised graph-level representation learning,\n2022.\nYanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An Empirical Study of Graph Contrastive\nLearning. arXiv.org, September 2021.\nScafarto Ciortan Tihon Ferr´e\nAppendix A. Reusing a Module with Batch Normalisation\nAll our competitors use approximately the same GIN architecture for their experimenta-\ntion (Suresh et al., 2021; Gao et al., 2022; Miao et al., 2022). Specifically, they all use batch\nnormalisation layers, and so do we. However, considering the context it is used in, we argue\nthat the way it should be used to be mathematically correct is not as straightforward as the\nway it is used in these approaches. This may have unexpected effects on their results and\nconclusions. Therefore, we propose a batch-norm switch to fix the issue, and we use it for\nour approach. As this is not within the scope of this paper, only a preliminary experiment\nis presented to demonstrate the relevance of a switch in practice. Further investigation into\nalternative methods to address the problem and the resulting effects on performances is left\nfor future work.\nA.1. Theoretical Analysis\nIn more detail, all mentioned approaches have one point in common: they use the same\nneural network to embed both the original graph (raw) and an augmented view of it (aug).\nHowever, the original graph and its augmented view come from different distributions, with\nsignificant differences. For example, the augmented view usually has lower node degrees, as\nonly a subset of the original graph edges is kept. Ultimately, this difference has an impact\non the input of the batch normalisation layers of this neural network.\nAs a reminder, the batch normalisation layer uses the batch mean and variance to\nnormalise its input, and stores a running mean of these values to be used at test time as\nan estimate of said values. That is, the normalisation performed at train time is meant to\nbe roughly equivalent to the normalisation performed at inference time. In that way, given\nan input, it produces roughly the same output, whether it is in train or in inference mode.\nThis is important as the end of the network is trained on that output.\nIn our case, the mean and variance of the input of the layer are different whether the\ninput of the network is a raw graph (µraw, σraw) or an augmented view of it (µaug, σaug).\nAt train time, the normalisation will therefore be different in both cases. However, at our\ncompetitors’ inference time, the estimate to perform the normalisation is something close to\nthe average of the real values (1\n2(µraw + µaug), 1\n2(σraw + σaug)), for both the original graph\nand its augmented view. As a result, if the means and variances are very different, the\noutputs will not be similar in any way to the ones at train time. The final output of the\nnetwork will therefore be highly unpredictable.\nA.2. Batch-Norm Switch and Preliminary Experiment\nA way to avoid this issue is to have two different running means: one for the original graph\ndistribution, and one for the augmented graph distribution. We call this solution a batch-\nnorm switch, as it switches between running means. With a batch-norm switch, the training\nof the model remains the same as without the switch, but the inference is mathematically\ncorrect. To assess the relevance of the switch, a quick experiment is presented hereunder,\nbased on the code provided by GSAT authors for the BA2Motifs dataset with a GIN\narchitecture, using seed 0 only.\nAugment to Interpret\nIn GSAT, the issue is mitigated by having augmented views close to the original graphs,\ni.e. removing on average less than half the edges. It is further mitigated by an early stopping\ncriterion based on a metric on the validation set, which is unlikely to give good results if\nthe network produces unpredictable results. The early stopping will therefore tend to select\nepochs for which the issue has no negative impact.\nFor our analysis, we change these two parameters. First, we run the experiments for\n500 epochs instead of 100 and show the full curves observed during training rather than the\nfinal result of the selected epoch. Second, for half the experiments, we use a final r value of\n0.1 instead of 0.5. This means that starting from epoch 80, roughly 10% of edges are kept\nrather than 50%. Note the first 40 epochs are not affected by this second change, as GSAT\nadopt a step decay for r. The result is shown in Figure 8, measured on the test set.\n(a) Interpretability AUC\n(b) Downstream ACC\n(c) Avg. Background Attention Weights\n(d) Avg. Signal Attention Weights\nFigure 8: Comparison of GSAT with a batch-norm switch (orange and teal) and without a\nbatch-norm switch (red and dark blue) on BA2Motifs. The final value of r is set\nto 0.1 (dark blue and orange) or 0.5 (teal and red). A smoothing effect is applied\nto ease the analysis. Non-smoothed results are shown in transparent.\nAs can be seen, the downstream ACC is greatly improved by the switch. On the other\nhand, the interpretability AUC is improved by the switch when interpretations are sparse,\nbut it is not as good when interpretations are not sparse.\nWe think it is because the\nunexpected effect aligns with some dataset properties. For example, it could favour highly\nScafarto Ciortan Tihon Ferr´e\nconnected interpretations, which is good in the case of the BA2Motifs dataset but may not\nbe for other datasets.\nAdditionally, we can see that the model with a switch and sparse interpretations (the\norange curves) has on average almost null attention weights on non-important edges (0.0016\non average at the end), but significant weights on important edges (0.12 on average at\nthe end). This indicates that the model misses important edges, but never includes non-\nimportant edges. For example, it could select length-3 cycles from class 1 and nothing from\nclass 0, as shown in Figure 9. This could be considered a perfect interpretation, but it\nobtains a not-so-good interpretability AUC, given the expected interpretations include the\nlength-5 cycles for each class.\nFigure 9: Interpretations of GSAT with a batch-norm switch and final r at 0.1 at epoch\n370 for class 0 (top) and class 1 (bottom). Red nodes are part of the motif, and\nbold lines are predicted as important.\nA deeper analysis could be done to verify our hypotheses, challenge the batch-norm\nswitch against alternatives, analyse results obtained with a batch-norm switch on each of\nour competitors or even challenge the BA2Motifs dataset itself, but this is out of the scope\nof this paper and is therefore left for future work.\nAugment to Interpret\nFigure 10: Ranking according to the average importance score given by the feature-\naugmentation module to each feature. It shows that noisy features are given\nless importance on average.\nAppendix B. Beyond Topology-Based Interpretations\nOur results indicate that there is a link between contrastive graph augmentation and in-\nterpretability. In such approaches, however, augmentations only focus on edges, and thus\ninterpretations only highlight topology. A natural question is whether these conclusions\nstand for node features too. This section presents some early work on the matter.\nWe enhance the framework with an additional 2-layer MLP ν, trained to learn im-\nportance weights for node features. To make these weights understandable, we apply the\nfeature augmentations in the input space, before being processed by the encoder.\nThe augmented feature matrix ( e\nX) is obtained as e\nX = (ν(X)+ϵ)·X and with ϵ sampled\nas indicated in Section 3.1 with ν(X) a weight per node per feature, i.e. ν(X) ∈RN×d.\nWe add an information loss on the importance scores to regularise the optimization,\nusing the same r as the one for the topology information loss.\nAs no ground truth is\navailable for feature importance, we artificially add 20% of noisy features. We then average\nthe importance score obtained by each feature on the test set and rank them. On Mutag,\nwith our complete loss, we obtain the following results:\nFigure 10 shows that the noisy features are given less importance on average, thus\ndemonstrating an intrinsic feature selection ability which can be used as a feature impor-\ntance scheme.\nAppendix C. Impact of Sparsity on Human Readability\nFigure 11(a) show some interpretations where important scores and unimportant ones are\nclose. While truly important edges are highlighted, it is harder to distinguish them than in\nFigure 11(b)\nScafarto Ciortan Tihon Ferr´e\n(a) Sparse importance weights\n(b) Bi-modal importance weights\nFigure 11: Examples of importance weights. A min-max normalisation is applied for clarity.\nOriginal weight limits are shown as the title of the figures.\nAppendix D. Ablation Study of the Watchman Module\nOur framework introduces a watchman module, with the purpose to stabilize the optimiza-\ntion. We observe in Table 4 that the presence of this module prevents quick drops in both\ninterpretability AUC and downstream ACC, except for a few rare cases (as visible in Table 4\nand Table 2).\nThis pattern is either positive or not significantly different across datasets, except for\na few: INGENIOUS - Negative on BA2Motifs and certain methods on SPMotifs.5 which\nsignificantly decrease due to conflicting optimization objectives, however we advise to always\nuse the watchman for our regularised loss INGENIOUS (L) as it improves the results on all\ndatasets.\nInterestingly, the loss which benefits the most from the watchman is the simple unreg-\nularised simclr loss (L-info-negative), probably because in this case, the problem is under-\nconstrained.\nIt acts as a regularizer by enforcing the recovery of eigenvalues from the\nembeddings. This result shows the importance of regularisations of any kind for contrastive\nlearning.\nWe conclude that while performance diminishes with overfitting as we just discussed,\nboth the watchman and early stopping help alleviate this.\nMoreover, defining the right number of training epochs is challenging in unsupervised\nsettings. Our training curves (Figure 12) show that these unsupervised frameworks are\nprone to overfitting. This module can attenuate or delay this overfitting.\nAugment to Interpret\nFigure 12: Example of training curves, giving interpretability AUC and downstream accu-\nracy. Light blue is run with Watchman, dark blue without it. Those curves are:\n(A) for GSAT on Mutag, (B) for INGENIOUS on Mutag, (C) for INGENIOUS\non BA2Motifs, (D) for the simple double augmentation simclr without regulari-\nsation on Mutag. The watchman helps prevent overtraining, on both supervised\n(GSAT) and unsupervised (INGENIOUS). The simple double-aug loss benefits\ntoo, showing that any form of regularisation is good.\nScafarto Ciortan Tihon Ferr´e\nAppendix E. Schematics and Other Figures\nFigure 13: Principle of the Wasserstein distance metric. Our goal is to quantify the con-\ntinuity of the interpretability functions. Therefore we evaluate whether similar\ngraph embeddings obtain similar interpretations. Graph embedding proximity\ncan be quantified by any vector similarity metric (we used Euclidian distance)\nand interpretation similarity can be quantified with any graph similarity metric\n(we use the Wasserstein distance between degree distributions).\nAugment to Interpret\nFigure 14: Examples of faithfulness curves on one graph of the BA2Motifs datasets. Clock-\nwise starting from the top left: AD-GCL, GSAT, INGENIOUS, MEGA. We\nshow the faithfulness (blue), faithfulness by removing edges according to ground\ntruth (green), shuffled faithfulness (orange), random faithfulness (purple) and\nopposite faithfulness (red).\nScafarto Ciortan Tihon Ferr´e\nAppendix F. Full Tables\nWm.\nMethods\nFaithfulness Gap\nWasserstein Gap\nBA2Motifs\nMutag\nSPMotifs.5\nBA2Motifs\nMutag\nSPMotifs.5\nGSAT\n.34 ± .00\n.21 ± .08\n.06 ± .02\n.04 ± .00\n.02 ± .00\n.04 ± .00\nAD-GCL\n.01 ± .13\n.03 ± .09\n.08 ± .06\n.10 ± .01\n.01 ± .00\n.03 ± .00\nMEGA\n.06 ± .02\n.39 ± .12\n.68 ± .03\n.04 ± .01\n.01 ± .00\n.01 ± .00\nINGENIOUS\n.39 ± .05\n.24 ± .01\n.50 ± 0.00\n.06 ± .00\n.01 ± .00\n.06 ± 0.00\nINGENIOUS - Info\n.54 ± .03\n.58 ± .04\n.46 ± .07\n.02 ± .00\n.01 ± .00\n.05 ± .01\nINGENIOUS - Negative\n.13 ± .02\n.19 ± .02\n.23 ± .04\n.02 ± .00\n.00 ± .00\n.02 ± .00\nINGENIOUS - Negative - Info\n.58 ± .13\n.17 ± .13\n.27 ± .08\n.04 ± .03\n.00 ± .00\n.02 ± .00\n✓\nGSAT\n.25 ± .03\n.06 ± .10\n−.03 ± .01\n.04 ± .00\n.02 ± .00\n.04 ± .00\n✓\nINGENIOUS\n.60 ± .02\n.41 ± .18\n.45 ± .03\n.06 ± .01\n.01 ± .00\n.05 ± .01\n✓\nINGENIOUS - Info\n.68 ± .02\n.48 ± .03\n.48 ± .06\n.05 ± .02\n.01 ± .00\n.05 ± .01\n✓\nINGENIOUS - Negative\n.13 ± .02\n.18 ± .00\n.22 ± .06\n.02 ± .01\n.01 ± .00\n.02 ± .01\n✓\nINGENIOUS - Negative - Info\n.61 ± .10\n.15 ± .09\n.32 ± .00\n.03 ± .01\n.01 ± .00\n.03 ± .00\nTable 2: Table of full results. A check in the Wm column means the model uses the watch-\nman loss. A higher faithfulness gap means the interpretations are more faithful,\na higher wasserstein gap means the embedding space is more continuous in terms\nof interpretations.\nWm.\nLoss\nDownstream ACC\nIntepretability AUC\nCora\nTree-cycle\nTree-grid\nCora\nTree-cycle\nTree-grid\nGSAT\n.610 ± .07\n.985 ± .01\n.984 ± .01\n.000 ± .00\n.630 ± .17\n.844 ± .01\nL\n.572 ± .06\n.848 ± .06\n.954 ± .02\n.000 ± .00\n.232 ± .11\n.532 ± .28\nL - Info\n.593 ± .05\n.777 ± .11\n.952 ± .03\n.000 ± .00\n.256 ± .09\n.588 ± .34\nL - Negative\n.575 ± .06\n.951 ± .01\n.868 ± .08\n.000 ± .00\n.293 ± .01\n.549 ± .04\nL - Negative - Info\n.606 ± .04\n.856 ± .02\n.887 ± .07\n.000 ± .00\n.319 ± .08\n.702 ± .07\nTable 3: Downstream ACC and interpretability AUC on node classification. Higher is bet-\nter\nAugment to Interpret\nWm.\nLoss\nDownstream ACC\nIntepretability AUC\nBA2Motifs\nMutag\nSPMotifs.5\nBA2Motifs\nMutag\nSPMotifs.5\nGSAT\n1 ± .00\n.921 ± .02\n.393 ± .02\n.998 ± .00\n.843 ± .15\n.895 ± .01\nAD-GCL\n1 ± .00\n.902 ± .02\n.337 ± .00\n.378 ± .06\n.416 ± .19\n.472 ± .03\nMEGA\n1 ± .00\n.886 ± .01\n.335 ± .00\n.459 ± .28\n.566 ± .46\n.506 ± .03\nL\n1 ± .00\n.900 ± .01\n.366 ± .01\n.910 ± .04\n.745 ± .11\n.467 ± .01\nL - Info\n.990 ± .00\n.889 ± .03\n.361 ± .02\n.860 ± .07\n.606 ± .23\n.491 ± .10\nL - Negative\n1 ± .00\n.887 ± .01\n.340 ± .02\n.745 ± .24\n.546 ± .04\n.494 ± .02\nL - Negative - Info\n1 ± .00\n.895 ± .01\n.337 ± .01\n.462 ± .38\n.478 ± .23\n.494 ± .04\n✓\nGSAT\n1 ± .00\n.912 ± .00\n.389 ± .01\n.999 ± .00\n.904 ± .04\n.882 ± .03\n✓\nL\n.993 ± .01\n.900 ± .03\n.363 ± .00\n.959 ± .01\n.771 ± .16\n.510 ± .06\n✓\nL - Info\n1 ± .00\n.863 ± .04\n.360 ± .02\n.900 ± .02\n.563 ± .20\n.499 ± .07\n✓\nL - Negative\n.995 ± .01\n.895 ± .01\n.338 ± .00\n.148 ± .00\n.620 ± .03\n.581 ± .03\n✓\nL - Negative - Info\n.997 ± .01\n.903 ± .01\n.337 ± .00\n.583 ± .30\n.650 ± .18\n.498 ± .04\nTable 4: Downstream ACC and interpretability AUC on graph classification Higher is bet-\nter.\nWm.\nLoss\nRandom faithfulness\nSparsity\nBA2Motifs\nMutag\nSPMotifs.5\nBA2Motifs\nMutag\nSPMotifs.5\nGSAT\n.840 ± .04\n.731 ± .03\n.708 ± .07\n.883 ± .01\n.812 ± .14\n.874 ± .00\nAD-GCL\n.606 ± .02\n.774 ± .03\n.679 ± .01\n.608 ± .08\n.955 ± .05\n1 ± .00\nMEGA\n.486 ± .01\n.638 ± .02\n.614 ± .01\n.566 ± .13\n.549 ± .07\n.275 ± .05\nL\n.875 ± .02\n.804 ± .03\n.649 ± .01\n.523 ± .03\n.897 ± .11\n.180 ± .01\nL - Info\n.714 ± .09\n.666 ± .08\n.651 ± .03\n.266 ± .12\n.191 ± .20\n.166 ± .01\nL - Negative\n1.020 ± .04\n.859 ± .01\n.647 ± .06\n.991 ± .00\n.980 ± .00\n.714 ± .24\nL - Negative - Info\n.897 ± .13\n.879 ± .02\n.641 ± .06\n.417 ± .13\n.958 ± .07\n.657 ± .29\n✓\nGSAT\n.817 ± .02\n.822 ± .01\n.538 ± .05\n.855 ± .04\n.893 ± .00\n.848 ± .00\n✓\nL\n.719 ± .01\n.783 ± .02\n.636 ± .01\n.402 ± .04\n.591 ± .31\n.205 ± .06\n✓\nL - Info\n.743 ± .03\n.670 ± .09\n.606 ± .02\n.288 ± .04\n.269 ± .23\n.168 ± .01\n✓\nL - Negative\n.877 ± .03\n.808 ± .02\n.663 ± .07\n.840 ± .02\n.969 ± .00\n.808 ± .31\n✓\nL - Negative - Info\n.767 ± .10\n.789 ± .05\n.594 ± .02\n.402 ± .09\n.971 ± .04\n.466 ± .10\nTable 5: Random faithfulness and sparsity on graph classification. For random faithfulness,\nhigher is better and for sparsity lower is better.\nWm.\nLoss\nfaithfulness\nOpposite faithfulness\nCora\nTree-cycle\nTree-grid\nCora\nTree-cycle\nTree-grid\nGSAT\n.756 ± .01\n.728 ± .21\n.960 ± .03\n.550 ± .07\n.761 ± .05\n.691 ± .01\nL\n.747 ± .03\n.839 ± .07\n.965 ± .07\n.628 ± .13\n.699 ± .08\n.707 ± .05\nL - Info\n.752 ± .03\n.767 ± .03\n.868 ± .06\n.611 ± .10\n.680 ± .17\n.584 ± .11\nL - Negative\n.732 ± .01\n.853 ± .08\n.902 ± .03\n.531 ± .06\n.846 ± .05\n.610 ± .15\nL - Negative - Info\n.783 ± .04\n.780 ± .04\n.847 ± .01\n.477 ± .03\n.608 ± .05\n.598 ± .11\nTable 6: Faithfulness and opposite faithfulness on node classification.\nFor faithfulness,\nhigher is better and for opposite faithfulness lower is better.\nScafarto Ciortan Tihon Ferr´e\nWm.\nLoss\nRandom faithfulness\nSparsity\nCora\nTree-cycle\nTree-grid\nCora\nTree-cycle\nTree-grid\nGSAT\n.693 ± .06\n.814 ± .12\n.911 ± .02\n.708 ± .01\n.827 ± .04\n.801 ± .00\nL\n.727 ± .02\n.823 ± .13\n.877 ± .04\n.754 ± .06\n.629 ± .08\n.639 ± .06\nL - Info\n.727 ± .03\n.757 ± .07\n.816 ± .10\n.711 ± .04\n.481 ± .03\n.565 ± .11\nL - Negative\n.718 ± .03\n.818 ± .08\n.849 ± .01\n.669 ± .04\n.827 ± .01\n.609 ± .12\nL - Negative - Info\n.714 ± .04\n.735 ± .06\n.816 ± .03\n.740 ± .01\n.564 ± .10\n.592 ± .05\nTable 7: Random Faithfulness and Sparsity on node classification. For random faithfulness,\nhigher is better and for sparsity lower is better.\nWm.\nMethod\nW1 global\nW1 local\nBA2Motifs\nMutag\nSPMotifs.5\nBA2Motifs\nMutag\nSPMotifs.5\nGSAT\n.065 ± .02\n.097 ± .00\n.080 ± .00\n.019 ± .01\n.071 ± .01\n.036 ± .00\nAD-GCL\n.240 ± .04\n.099 ± .00\n.117 ± .03\n.134 ± .02\n.085 ± .00\n.082 ± .02\nMEGA\n.139 ± .04\n.128 ± .01\n.115 ± .00\n.095 ± .03\n.113 ± .01\n.099 ± .01\nINGENIOUS\n.154 ± .01\n.099 ± .00\n.113 ± .01\n.093 ± .02\n.087 ± .01\n.044 ± .01\nINGENIOUS - Info\n.144 ± .01\n.096 ± .01\n.105 ± .02\n.122 ± .01\n.084 ± .00\n.047 ± .01\nINGENIOUS - Negative\n.123 ± .01\n.100 ± .00\n.132 ± .01\n.101 ± .01\n.095 ± .00\n.103 ± .01\nINGENIOUS - Negative - Info\n.144 ± .02\n.100 ± .00\n.117 ± .01\n.098 ± .02\n.092 ± .00\n.092 ± .02\n✓\nGSAT\n.055 ± .01\n.097 ± .00\n.080 ± .00\n.012 ± .00\n.077 ± .00\n.035 ± .00\n✓\nINGENIOUS\n.143 ± .01\n.099 ± .00\n.095 ± .01\n.080 ± .01\n.084 ± .00\n.044 ± .00\n✓\nINGENIOUS - Info\n.151 ± .01\n.100 ± .00\n.094 ± .02\n.098 ± .01\n.087 ± .00\n.042 ± .00\n✓\nINGENIOUS - Negative\n.136 ± .00\n.101 ± .00\n.116 ± .01\n.107 ± .02\n.089 ± .00\n.088 ± .02\n✓\nINGENIOUS - Negative - Info\n.140 ± .00\n.101 ± .00\n.110 ± .01\n.105 ± .01\n.087 ± .00\n.074 ± .01\nTable 8: Global and local Wasserstein distances on graph embeddings.\nWm.\nMethod\nFaithfulness\nOpposite Faithfulness\nBA2Motifs\nMutag\nSPMotifs.5\nBA2Motifs\nMutag\nSPMotifs.5\nGSAT\n.843 ± .02\n.778 ± .05\n.702 ± .06\n.497 ± .02\n.565 ± .04\n.643 ± .03\nAD-GCL\n.622 ± .07\n.779 ± .00\n.707 ± .02\n.609 ± .06\n.747 ± .09\n.626 ± .05\nMEGA\n.531 ± .01\n.827 ± .04\n.898 ± .01\n.467 ± .02\n.436 ± .09\n.214 ± .02\nINGENIOUS\n.831 ± .03\n.830 ± .03\n.832 ± 0.01\n.432 ± .05\n.588 ± .04\n.325 ± 0.01\nINGENIOUS - Info\n.823 ± .01\n.864 ± .03\n.817 ± .06\n.274 ± .04\n.279 ± .03\n.356 ± .03\nINGENIOUS - Negative\n.978 ± .05\n.834 ± .02\n.744 ± .03\n.844 ± .03\n.637 ± .00\n.506 ± .07\nINGENIOUS - Negative - Info\n.912 ± .04\n.838 ± .04\n.746 ± .01\n.323 ± .12\n.664 ± .10\n.474 ± .10\n✓\nGSAT\n.782 ± .02\n.747 ± .07\n.511 ± .04\n.523 ± .02\n.684 ± .04\n.548 ± .05\n✓\nINGENIOUS\n.856 ± .01\n.862 ± .07\n.797 ± .02\n.257 ± .01\n.443 ± .12\n.345 ± .01\n✓\nINGENIOUS - Info\n.884 ± .02\n.819 ± .07\n.796 ± .02\n.200 ± .01\n.336 ± .03\n.316 ± .04\n✓\nINGENIOUS - Negative\n.801 ± .00\n.803 ± .01\n.738 ± .02\n.663 ± .02\n.617 ± .02\n.518 ± .08\n✓\nINGENIOUS - Negative - Info\n.893 ± .05\n.782 ± .03\n.730 ± .01\n.279 ± .08\n.629 ± .08\n.409 ± .02\nTable 9: Faithfulness depending on experiments. For faithfulness, higher is better, for op-\nposite faithfulness, lower is better.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-09-28",
  "updated": "2023-09-28"
}