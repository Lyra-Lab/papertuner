{
  "id": "http://arxiv.org/abs/2310.00552v1",
  "title": "Siamese Representation Learning for Unsupervised Relation Extraction",
  "authors": [
    "Guangxin Zhang",
    "Shu Chen"
  ],
  "abstract": "Unsupervised relation extraction (URE) aims at discovering underlying\nrelations between named entity pairs from open-domain plain text without prior\ninformation on relational distribution. Existing URE models utilizing\ncontrastive learning, which attract positive samples and repulse negative\nsamples to promote better separation, have got decent effect. However,\nfine-grained relational semantic in relationship makes spurious negative\nsamples, damaging the inherent hierarchical structure and hindering\nperformances. To tackle this problem, we propose Siamese Representation\nLearning for Unsupervised Relation Extraction -- a novel framework to simply\nleverage positive pairs to representation learning, possessing the capability\nto effectively optimize relation representation of instances and retain\nhierarchical information in relational feature space. Experimental results show\nthat our model significantly advances the state-of-the-art results on two\nbenchmark datasets and detailed analyses demonstrate the effectiveness and\nrobustness of our proposed model on unsupervised relation extraction.",
  "text": "Siamese Representation Learning for Unsupervised\nRelation Extraction\nGuangxin Zhanga and Shu Chena;*\naSchool of IoT Engineering, Jiangnan University, China\nAbstract.\nUnsupervised relation extraction (URE) aims at discov-\nering underlying relations between named entity pairs from open-\ndomain plain text without prior information on relational distribution.\nExisting URE models utilizing contrastive learning, which attract\npositive samples and repulse negative samples to promote better sep-\naration, have got decent effect. However, fine-grained relational se-\nmantic in relationship makes spurious negative samples, damaging\nthe inherent hierarchical structure and hindering performances. To\ntackle this problem, we propose Siamese Representation Learning\nfor Unsupervised Relation Extraction – a novel framework to simply\nleverage positive pairs to representation learning, possessing the ca-\npability to effectively optimize relation representation of instances\nand retain hierarchical information in relational feature space. Ex-\nperimental results show that our model significantly advances the\nstate-of-the-art results on two benchmark datasets and detailed anal-\nyses demonstrate the effectiveness and robustness of our proposed\nmodel on unsupervised relation extraction. We have released our code\nat https://github.com/gxxxzhang/siamese-ure.\n1\nIntroduction\nRelation Extraction (RE) is the task of extracting semantic relation\nbetween entity pair from raw text. For example, given the sentence\n“ChatGPT is created by OpenAI, a research organization dedicated\nto creating and promoting friendly AI that benefits humanity”, and\nthe entity pair (ChatGPT, OpenAI), RE model can predict the pre-\ndefine relationship “created_by” and extract the corresponding triplet\n(ChatGPT, created_by, OpenAI) for downstream tasks, such as web\nsearch [35], knowledge base construction [1] and question answering\n[5]. Existing RE methods which are restricted to specific relation types\nhave achieved good performance with annotated data. Nevertheless,\nwith the rapid emergence of large, domain-specific text corpora (e.g.,\nsports news, social media content, scientific publications) and new\nrelation types in the real world, these methods face many challenges.\nOn the one hand manually establishing and maintaining the ever-\ngrowing relation require expert knowledge and are time-consuming,\non the other hand these methods are hard to scale up to newly emerged\nrelations. Unsupervised relation extraction is promising and received\nwidespread concern since it does not require prior information on\nrelation distribution to reduce the reliance on labeled data and can\ndiscover new relation types in raw text.\nTraditional unsupervised relation extraction approaches are based\non variational autoencoder (VAE) architecture [20, 27, 28, 37]. These\nmethods train the relation extraction model as an encoder that gen-\n∗Corresponding Author. Email: kjcuse@jiangnan.edu.cn\nerates relation classifications. A decoder is trained along with the\nencoder to reconstruct the encoder input based on the encoder gen-\nerated relation classifications. However, joint training for two net-\nworks (encoder and decoder) and requiring the exact number of re-\nlation classes during the training period of the encoder make model\nunstable. Different from treating relations as latent variables, the\nclustering-based approaches learn semantic relational representation\nfrom high-dimensional embeddings and adopt unsupervised cluster-\ning algorithms to recognize relation classes in feature space. In this\nprocess, the main challenge is how to learn semantic representation\nof instances in the relational feature space.\nElsahar et al. [10] extracts KB types and NER tags of entities as\nwell as re-weighted word embeddings from sentences, then adopts\nPrincipal Component Analysis (PCA) to reduce feature dimensional-\nity that can alleviate the problem of features sparsity, and finally uses\nHierarchical Agglomerative Clustering (HAC) to cluster the feature\nrepresentations. Because integrating word embeddings in a rule-based\nway, the method heavily rely on hand-craft features and make many\nsimplifying assumptions and its feature space is lack of semantic infor-\nmation. Liu et al. [18] formulate URE using a structural causal model\nand conduct Element Intervention to eliminate spurious correlations,\nwhich intervenes on the context and entities respectively to obtain the\nunderlying causal effects of them and learn the causal effects through\ninstance-wise contrastive learning.\nThe core idea of instance-wise contrastive learning is to pull\ntogether the representations within positive instances while push-\ning apart negative ones. However, negative examples are com-\nmonly sampled from the batch or training data at random due\nto the lack of ground-truth annotations. In relation extraction, the\nrelational semantic tend to be more fine-grained and based on\na potential hierarchical structure [38]. For example, the relations\n“per:stateorprovinces_of_residence”, “per:countries_of_residence”\nand “per:cities_of_residence” in one benchmark dataset share the\nsame parent semantic on /people/residence, which means that they\nbelong to the same semantic cluster from a hierarchical perspective.\nNaturally, instances may have highly similar semantics in a batch\nbut contrastive learning pushes these representations apart as long as\nthey are from different original instances, regardless of their seman-\ntic similarities. To alleviate the dilemma which instance-wise con-\ntrastive learning unreasonably pushes apart those sentence pairs that\nare semantically similar, Liu et al. [19] propose hierarchical exemplar\ncontrastive learning. The model leverages Hierarchical Propagation\nClustering to obtain hierarchical exemplars from relational feature\nspace and further utilizes exemplars to hierarchically update relational\nfeatures of sentences and is optimized by performing both instance and\narXiv:2310.00552v1  [cs.CL]  1 Oct 2023\n......\n[E1] ChatGPT [/E1] is created by [E2] OpenAI [/E2], a research organization …... that benefits humanity.\n         [CLS]  [E1]  ChatGPT [/E1] is created by [E2] OpenAI [/E2], a research organization …... [SEP]\nDeep Transformer (BERT)\n......\n......\nh\nRelation Instance Encoder\nRelational Semantic Clustering\nSiamese Representation Learning\nstop-grad \nBackprop\nsimilar semantic\nclustering loss\nz\n z'\nh\nNon-linear\nMapping\ng\nNon-linear\nMapping\ng\nNon-linear\nMapping\nd\nSimilarity\nRepresentation \n Loss\nFigure 1: Overall architecture of our model. Instances will be fed into encoder with different dropout masks z and z′ to obtain feature pairs, then\ntransmitted into Non-linear Mapping g(·) and d(·) respectively. We use ones to predict other ones to optimize encoder. Besides, in relational\nsemantic clustering, we mining nearest neighbors of each sample with semantic clustering loss.\nexemplar-wise contrastive learning through Hierarchical Exemplar\nContrastive Loss and propagation clustering iteratively. Nonetheless,\nthe method still utilize traditional instance-wise contrastive learning\nloss to retain the local smoothness in relational feature space dur-\ning training period and use an iterative way to obtain hierarchical\nexemplars will cause error accumulation [24].\nIn this paper, we attempt to propose a Siamese network architec-\nture, only using positive pairs for representation learning, to eliminate\nadverse effect of spurious negative samples. Siamese networks are\nable to learn similarity metrics of relations from labeled data of pre-\ndefined relations, and then transfer the relational knowledge to identify\nnovel relations in unlabeled data [33]. Therefore, we intend to use\nSiamese architecture to construct positive pairs and learn relation\nrepresentations in unsupervised setting. For the most part, owing to\nlack of labeled data, the network is easy to collapse (i.e., all outputs\n“collapsing” to a constant). To avoid that ,we introduce entity type,\nwhich provide a strong inductive bias for relation extraction [28], as\nprior information. Furthermore, followed by Chen and He [7], we\nuse the analogous network architecture to further prevent to collapse.\nTo recognize different relations, we through traditional unsupervised\nclustering algorithms (e.g., k-means clustering algorithm) to clus-\nter learned representation in relational feature space. Nevertheless,\nnaively applying these clustering algorithms on the obtained features\ncan lead to cluster degeneracy [6]. We propose relational semantic\nclustering module that mining nearest neighbors of each instance in\nfeature space. The module can support model learn more discrim-\ninative representations under semantic clustering loss, so that the\nlearned representation is cluster-friendly and obtain better clustering\nperformance.\nOur main contributions are the following: (1) We propose a novel\nrepresentation learning framework for unsupervised relation extrac-\ntion. Furthermore, our model is much simpler than existing self-\nsupervised learning models that apply empirical data augmentation\nand complicated network architecture. (2) We explore Relational\nSemantic Clustering module, encoding the relational semantic into\nthe representations via unsupervised clustering. We efficiently con-\nduct representation learning and unsupervised clustering in a unified\nframework. (3) We conduct extensive experiments on two datasets and\nachieves better performance than the existing state-of-the-art meth-\nods. Meanwhile, our ablation analysis shows the impacts of different\nmodules in our framework.\n2\nModel\nWe aim at developing a joint model that leverages the beneficial prop-\nerties of self-supervised learning to improve unsupervised relation\nextraction. As illustrated in Figure 1, our model consists of three\nmodules: Relation Instance Encoder, Siamese Representation Learn-\ning and Relational Semantic Clustering. The encoder module uses\ninstances as input which are composed of natural language sentences\nand entity pairs, and then employs the pre-trained model to output\nentity-level feature pair sets H and H′ for all instances. The learn-\ning module is structured as Siamese architecture and takes pair sets\nrespectively as input. We use similarity representation loss, which\nmeasure the cosine similarity of positive pairs, to enforces the re-\nlational feature of instances that have similar semantics to be more\nclose in feature space. In clustering module, we mining nearest neigh-\nbors of each sample in relational feature space to learn discriminative\nrepresentations under semantic clustering loss that can yield better\nclustering performance.\n2.1\nRelation Instance Encoder\nThe Relation Instance Encoder aims to obtain relational features from\ninstances. We use instances X as inputs that each xi ∈X is com-\nposed of a sentence si = {w1, .., wn} with n words, hi = (hs\ni, he\ni )\nrepresenting the start and end position of head entity, and ti = (ts\ni, te\ni )\nrepresenting the start or end position of tail entity in the sentence.\nSame as the previous work, named entities in the sentences have\nbeen recognized in advance. We employ pre-trained BERT [8] model,\nwhich has strong performance on extracting contextual information,\nas encoder f to map relation instance xi to embedding. However,\nBERT always induces a non-smooth anisotropic semantic space of\nsentences [16] , which is easier make model to collapse. To this end,\nwe add entity types as prior information in head and tail entities as\n[hs\ni], [he\ni ], [ts\ni], [te\ni ] and inject them to each instance xi:\nxi = [w1, . . . , [hs\ni], . . . , wi, . . . , [he\ni ], . . . , [ts\ni], . . . , [te\ni ], . . . , wn]\n(1)\nthen get the token embedding:\ne1, ..., en = fθ(x1, ..., xn)\n(2)\nwhere θ is the learnable parameters in the encoder. We use the embed-\ndings with position of [hs\ni] and position of [ts\ni] as outputs to obtain the\nentity-level feature hi ∈2 · Rd:\nhi = ehead ⊕etail\n(3)\nWe follow the data augmentation used in SimCSE [11] to construct\npositive pairs. Specifically, we only feed the same input xi to the\nencoder twice with different dropout masks, which placed on fully-\nconnected layers as well as attention probabilities, and we can obtain\ntwo different embeddings as positive pairs. We denote positive pair\nas hi = fθ (xi; z) and h′\ni = fθ (xi; z′), where z and z′ is the\ndifferent dropout masks.\n2.2\nSiamese Representation Learning\nWe use Siamese network which can naturally introduce inductive\nbiases for modeling invariance to learn relational similarity metrics.\nHowever, Siamese networks suffers from the problem of model col-\nlapse, where the model converges to a constant value and the samples\nall mapped to a single point. Besides, it is difficult to learn a reason-\nable distance in feature space without negative samples. Followed by\nChen and He [7], we attempt to use the similar approach to address\nthis issue. As shown in Figure 1, positive feature pair hi and h′\ni of one\ninstance are processed by the same encoder network f with different\ndropout masks z and z′. Then we use the MLP g(·) to map feature\npair to gi and g′\ni respectively:\ngi, g′\ni = gϕ\n\u0000hi, h′\ni\n\u0001\n(4)\nwhere ϕ is the learnable parameters in the non-linear mapping net-\nwork. And then, the non-linear mapping network d(·) is applied on\none side, and we denote the feature as d′\ni = dψ(g′\ni), ψ is the learnable\nparameters. Meanwhile, a stop-gradient operation is applied on the\nother side. The model maximizes the similarity between both sides\nto learn relation representation under similarity representation loss.\nSimilarity Representation Loss\nGiven a training set X = {x1, x2, . . . , xn} of n instances, Relation\nInstance Encoder can obtain two augmented relational features for\neach input sentences by feed the same input to the encoder twice\nwith different dropout masks. In this process, we obtain feature sets\nH = {h1, h2, . . . , hn} and H′ = {h′\n1, h′\n2, . . . , h′\nn}. We mini-\nmize negative cosine similarity between positive pair:\nLRl = 1\n2\n\u0002\nD\n\u0000d, stopgrad(g′)\n\u0001\n+ D\n\u0000d′, stopgrad(g)\n\u0001\u0003\n(5)\nwhere stopgrad(·) is use stop-grad strategy that gradient does not back-\npropagate. The cost is described as a symmetrized form since pairs\nfrom the Siamese network. For each part of the loss, we minimize\ntheir negative cosine similarity:\nD(d, g′) = −1\nn\nn\nX\ni=1\ndi\n∥di∥2 ·\ng′\ni\n∥g′\ni∥2\n(6)\nfor a mini-batch of N instances,where i ∈[1, N] and ∥·∥2 is ℓ2-norm.\n2.3\nRelational Semantic Clustering\nOne of the main obstacles for our model is difficult learn a discrim-\ninative representation without negative samples. Be enlightened by\nVan Gansbeke [30], we assume that in a excellent relational feature\nspace, each sample with their nearest neighbors have similar rela-\ntional semantic and belong to the same relation class. We propose\nRelational Semantic Clustering to learn discriminative relational rep-\nresentations and conduct clustering and representation learning in a\nunified framework. Specifically, for each instance xi, we mine its K\nnearest neighbors according to each representation hi and define the\nset Nxi as the output features of neighboring samples corresponding\nto each xi in the dataset. Then, we use similar semantic clustering\nloss to attract instances and their neighboring samples to approach\neach other. Simultaneously, different instances are separated in feature\nspace.\nSimilar Semantic Clustering Loss\nLike adaptive clustering [34], we aim to learn a clustering function\nΦσ - parameterized by a neural network. The neural network classifies\neach instance xi and its mined neighbors Nxi together with the learn-\nable parameters σ. The function Φσ terminates in a softmax function\nto perform a soft assignment over the clusters C = {1, . . . , C}, with\nΦσ (xi) ∈[0, 1]C. The probability of instance xi being assigned\nto cluster c is denoted as Φc\nσ(xi). We learn the weights of Φσ by\nminimizing the following objective:\nLCl = −1\n|X|\nX\nxi∈X\nX\nk∈Nxi\nlog ⟨Φσ(xi), Φσ(k)⟩+ λ\nX\nc∈C\nΦ′c\nσ log Φ′c\nσ\n(7)\nwhere ⟨·⟩denotes the dot product operator. The first term in Equation 7\nimposes Φσ to make consistent predictions for each instance xi and its\nneighboring samples Nxi. However, the dot product will be maximal\nwhen the predictions are one-hot (confident) and assigned to the same\ncluster (consistent). To avoid Φσ from assigning all samples to a single\ncluster, we include an entropy term(the second term in Equation 7):\nΦ′c\nσ =\n1\n|X|\nX\nxi∈X\nΦc\nσ(xi).\n(8)\nwhich spreads the predictions uniformly across the clusters C and\nencourages the classifier to scatter a set of instances into different\nclasses.\n2.4\nIterative Joint Training\nIn early training period, we only optimize LRl in Siamese Repre-\nsentation Learning module to drive instances with similar semantic\nget closer in feature space. After several warm-up epochs, instances\nhave acquired reasonable semantic representations. We introduce Re-\nlational Semantic Clustering module to further refine the semantic\nrepresentations and enable them to be more discriminative. In sum-\nmary, our overall objective is:\nL = LRl + ηLCl\n(9)\nFigure 2: The relation distribution in the NYT+FB dataset. The or-\ndinate represents the number of sentences in each relation type in\nthe dataset.The x-axis is the relations sorted according to the number\nof sentences contained. For ease of observation, the x-axis label is\nomitted.\nwhere η is a loss coefficient. Our approach involves the combined\nutilization of the Siamese Representation Learning module and the\nRelational Semantic Clustering module through an iterative procedure.\nThis joint usage enables model to achieve a well-separated representa-\ntion of distinct instances in the learned feature space while preserving\nlocal invariance for each individual instance.\n3\nExperiments\nIn this section, we first describe two relation extraction datasets for\ntraining and evaluating the proposed method, then detail the baseline\nmodels for comparison, and then expound the implementation details\nand hyperparameter configuration, finally we conduct a comprehen-\nsive and detailed analysis of our model.\n3.1\nDatasets and Evaluation Metrics\nDatasets.\nFollowing previous work [28, 19], we conduct experi-\nments on two relation extraction datasets – NYT+FB [20] and TA-\nCRED [39] with different constructing settings. The former is gener-\nated via distant supervision while the latter is manually annotated cor-\npus, which is extremely challenging to model. The NYT+FB dataset\nis obtained by using Freebase to label the corpus of the New York\nTimes corpus. That is, if the entity pair that appears in a sentence also\nappears in Freebase [4], then this sentence is automatically labeled\nas the relation stored by Freebase. After filtering out some sentences\nusing syntactic patterns, there are 2 million sentences in the dataset, of\nwhich 41,000 are labeled with meaningful relations1. Of the 41,000\ntagged sentences, 20% are used as validation set, and 80% are used as\ntest set. The TACRED dataset is a large-scale crowd-sourced relation\nextraction dataset following the TAC KBP relation schema that covers\n42 relation types. We remove the instances labeled as no_relation\nand use the remaining 21,773 instances including 41 relation types\nfor training and evaluation.\nEvaluation Metrics.\nB-cube (B3) [3], V-measure [26] and Adjusted\nRand Index (ARI) [13] are used as evaluation metrics for different\nmodels. Specifically, B3 contains the precision and recall metrics to\ncorrespondingly measure the correct rate of putting each sentence\nin its cluster or clustering all samples into a single class, which are\ndefined as follows:\nB3\nPrec. = E\nX,Y P(g(X) = g(Y )|c(X) = c(Y ))\nB3\nRec. = E\nX,Y P(c(X) = c(Y )|g(X) = g(Y ))\nTable 1: Hyper-parameter values used in our experiments.\nHyper-parameters\nvalue\noptimizer\nSGD\nlearning rate\n1e-5\nweight_decay\n1e-4\nmomentum\n0.9\nbatch size\n64\nwarm-up epochs L\n5\ndropout rate r\n0.1\nnumber of nearest neighbors K\n20\nloss coefficient η\n0.5\nThen B3 F1 is computed as the harmonic mean of the precision and\nrecall.\nV-measures contains the homogeneity and completeness, which\nis analogous to B3 precision and recall. These two metrics penalize\nsmall impurities in a relatively “pure” cluster more harshly than in\nless “pure” ones:\nVHomo. =1 −H(c(X)|g(X))/H(c(X))\nVComp. =1 −H(g(X)|c(X))/H(g(x))\nARI is a normalization of the Rand Index, which measures the\nagreement degree between the cluster and golden distribution. This\nmetric ranges in [-1,1]. The larger the value, the more consistent the\nclustering result is with the real situation.\n3.2\nBaselines\nTo evaluate the effectiveness of our method, we select the following\nunsupervised relation extraction models for comparison with standard\nevaluation metrics: 1) rel-LDA [36], a generative model that consid-\ners the unsupervised relation extraction as a topic model.We choose\nthe full rel-LDA with a total number of 8 features for comparison\nin our experiment. 2) March [20], a VAE-based model learned by\nself-supervised signal of entity link predictor. 3) UIE [27],a discrim-\ninative model that adopts additional regularization to guide model\nlearning. And it has different versions according to the choices of\ndifferent relation encoding models (e.g., PCNN). We report the results\nof two versions—UIE-PCNN and UIE-BERT (i.e., using PCNN and\nBERT as the relation encoding models) with the highest performance.\n4) EType [28], a simple and effective method relying only on entity\ntypes. The same link predictor as in March [20] is employed and two\nadditional regularizers are used. 5) SelfORE [12],a self-supervised\nframework that bootstraps to learn a contextual relation representa-\ntion through adaptive clustering and pseudo label. 6) EIURE [18],\na contrastive learning framework that intervenes on the context and\nentities respectively to obtain the underlying causal effects of them.\n7) HiURE [19], is the state-of-the-art method that derive hierarchical\nsignals from relational feature space using cross hierarchy attention\nand effectively optimize relation representation of sentences under\nexemplar-wise contrastive learning.\n3.3\nImplementation Details\nIn order to do a fair comparison with baseline method, we adopted\nthe setting by clustering all samples into 10 relation super-classes.\nIn the process of training of our model, we used the development\nset to manually search part of the hyper-parameters, Table 1 shows\nour best parameter settings. In our implementation, we adopt the\npre-trained Bert-Base-Cased model to initialize parameters for\nTable 2: Main results on two relation extraction datasets.The results of all baseline are reproduced in liu et al. [19], MLPs refers to two mapping\nnetworks in Representation Learnin module and Semantic Clustering refers to Relational Semantic Cluster module in our model.\nDataset\nModel\nB3\nV-measure\nARI\nF1\nPrec.\nRec.\nF1\nHom.\nComp.\nNYT+FB\nrel-LDA[36]\n29.1±2.5\n24.8±3.2\n35.2±2.1\n30.0±2.3\n26.1±3.3\n35.1±3.5\n13.3±2.7\nMarch[20]\n35.2±3.5\n23.8±3.2\n67.1±4.1\n27.0±3.0\n18.6 ±1.8\n49.6±3.1\n18.7±2.6\nUIE-PCNN[27]\n37.5±2.9\n31.1±3.0\n47.4±2.8\n38.7±3.2\n32.6±3.3\n47.8±2.9\n27.6±2.5\nUIE-BERT[27]\n38.7±2.8\n32.2±2.4\n48.5±2.9\n37.8±2.1\n32.3±2.9\n45.7±3.1\n29.4±2.3\nEType[28]\n41.9±2.0\n31.3±2.1\n63.7±2.0\n40.6±2.2\n31.8±2.5\n56.2±1.8\n32.7±1.9\nSelfORE[12]\n41.4±1.9\n38.5±2.2\n44.7±1.8\n40.4±1.7\n37.8±2.4\n43.3±1.9\n35.0±2.0\nEIURE[18]\n43.1±1.8\n48.4±1.9\n38.8±1.8\n42.7±1.6\n37.7±1.5\n49.2±1.6\n34.5±1.4\nHiURE[19]\n44.3±0.5\n39.9±0.6\n49.8±0.5\n44.9±0.4\n40.0±0.5\n51.2±0.4\n38.3±0.6\nOur w/o MLPs\n40.5±0.6\n35.9±0.5\n46.5±0.8\n43.3±0.4\n38.2±0.2\n50.0±0.5\n31.0±0.5\nOur w/o Semantic Clustering\n41.9±0.3\n36.8±0.3\n48.8±0.4\n44.7±0.8\n39.3±0.2\n51.8±0.9\n32.1±0.7\nOur\n44.9±0.4\n39.5±0.3\n52.1±0.7\n45.7±0.6\n40.0±0.3\n53.2±0.8\n39.6±0.3\nTACRED\nrel-LDA[36]\n35.6±2.6\n32.9±2.5\n38.8±3.1\n38.0±3.5\n33.7±2.6\n43.6±3.7\n21.9±2.6\nMarch[20]\n38.8±2.9\n35.5±2.8\n42.7±3.2\n40.6±3.1\n36.1±2.7\n46.5±3.2\n25.3±2.7\nUIE-PCNN[27]\n41.4±2.4\n44.0±2.7\n39.1±2.1\n41.3±2.3\n40.6±2.2\n42.1±2.6\n30.6±2.5\nUIE-BERT[27]\n43.1±2.0\n43.1±1.9\n43.2±2.3\n49.4±2.1\n48.8±2.1\n50.1±2.5\n32.5±2.4\nEType[28]\n49.3±1.9\n51.9±2.1\n47.0±1.8\n53.6±2.2\n52.5±2.1\n54.8±1.9\n35.7±2.1\nSelfORE[12]\n47.6±1.7\n51.6±2.0\n44.2±1.9\n52.1±2.2\n51.3±2.0\n52.9±2.3\n36.1±2.0\nEIURE[18]\n52.2±1.4\n57.4±1.3\n47.8±1.5\n58.7±1.2\n57.7±1.4\n59.7±1.7\n38.6±1.1\nHiURE[19]\n55.8±0.4\n57.8±0.3\n54.0±0.5\n59.7±0.6\n57.6±0.5\n61.9±0.6\n40.5±0.4\nOur w/o MLPs\n53.6±0.8\n45.6±0.6\n65.0±1.2\n59.5±0.7\n51.9±0.5\n69.8±1.1\n44.0±0.9\nOur w/o Semantic Clustering\n56.1±0.3\n47.7±0.2\n68.1±0.8\n64.1±0.7\n56.2±0.8\n74.6±1.3\n48.4±0.6\nOur\n59.5±0.6\n49.4±0.4\n74.9±0.8\n66.7±0.8\n58.1±0.6\n78.5±0.9\n50.6±0.5\nFigure 3: Influence of the used number of neighbors K.\nRelation Instance Encoder and set dropout rate r = 0.1 to generate\npositive pairs. The output entity-level features hi and h′\ni possess the\ndimension of 2 · Rd, where Rd = 768. For Siamese Representation\nLearning, we use Non-linear Mapping g(·) and d(·) in our network\nand use SGD with 1e-5 learning rate to optimize the loss. The g(·)\nhas layer normalization (LN) [2] applied to each fully-connected (fc)\nlayer, including its output fc. Its output fc has no ReLU. This MLP\nhas 3 layers. The d(·) has LN applied to its hidden fc layers. Its output\nfc does not have LN or ReLU. This MLP has 2 layers. For Relation\nSemantic Clustering, we set warm-up epochs L = 5 and number of\nnearest neighbors of each instance K = 20. In the evaluation period,\nwe simply adopt the pre-trained models for representation extraction,\nthen cluster the evaluation instances based on these representations.\n3.4\nResults\nWe summarize the performances of our method and seven baseline\nmodels in Table 2. All of these models are evaluated on identical\ntest set to show their performance. From the experimental results,\nwe can see that our method significantly outperforms baselines. For\nNYT+FB dataset, compared with the previous SOTA model, our\nmethod improves V-measure F1 by 1.6%, and ARI by 5.7%, but\nthe B3 F1 score is only by 0.6%. The reason why the performance\ngain is minuscule in NYT+FB is that the dataset contains numerous\nwrongly labeled instances in the train and test sets. These instances are\nunable to reflect the real performance of the model, and the number of\nsamples in different relations is very unbalanced. As shown in Figure\n2, the relation distribution is similar to a long-tailed distribution.\nBesides, most relations only have several samples that make a lot\nof noisy nearest neighbors to hurt performance when use mining\nnearest neighbors in Relation Semantic Clustering. For TACRED,\ncompared with the previous SOTA model, our method improves B3 F1\nby 3.7%, V-measure F1 by 7.0%, and ARI by 10.1%. It is worth noting\nthat the score of precision and recall in B3 seems to be extremely\ndisequilibrium. By definition, precision measure the correct rate of\nputting each sample in its cluster and recall measure the correct rate\nof clustering all samples into a single class. Therefore, the results\nindicate that most of the samples from corresponding relations are\nclustered in the same cluster.\nAblation Study.\nTo study the contribution of different components\nin the proposed method, we conduct an ablation study on each com-\nponent. For fair comparisons, the other settings remain the same as\nthe main model. From Table 2, we can see that in both NYT+FB and\nTACRED, the model’s performance is degraded if any component\nis removed, indicating that both modules are important for the final\nmodel performance.\n3.5\nDetailed Analysis\nHyperparameter Analysis.\nOn account of the importance of two\nhyperparameters dropout rate r and number of nearest neighbors K\nwhich is in the encoder module and cluster module respectively, we\nconduct a detail analysis on them. Firstly, to further study the role of\ndropout rate in relation instance encoder for data augmentation, we try\nout different rates and report the performance of B3 F1 on NYT+FB\nand TACRED. As shown in Table 3, we observe that all the variants\nunderperform the default dropout rate r = 0.1 of Transformers [31].\nUsing small dropout rate will introduce small divergence so that\nit is difficult for our model to learn discriminative representation,\nTable 3: Effects of different dropout rates r on the NYT+FB and\nTACRED development sets.\nDataset / r\n0.01\n0.05\n0.1\n0.15\n0.2\n0.5\nNYT+FB\n41.3\n42.1\n44.3\n44.1\n43.8\n41.5\nTACRED\n50.4\n52.1\n58.7\n57.6\n57.1\n51.1\nwhile large dropout rate will make more noise and prejudice similar\nsemantic information. Secondly, we study the influence of number of\nnearest neighbors K in cluster module for mining nearest neighbors\nand report the accuracy and F1 score on two datasets. As shown in\nFigure 3, the accuracy (left),which is the correct rate of that nearest\nneighbors with their corresponding samples are all come from same\nrelation, is gradually decrease with the increase of K. However, the\nresult of B3 F1 score (right) is not very sensitive to the value and even\nremain perform well when increasing K to 50, despite the increasing\nvalue will introduce more noise. This is beneficial, since we do not\nhave to fine-tune the value on new raw text.\nVisualization of Relation Representations.\nIn this experiment, to\nintuitively show the effectiveness of our model to learn representa-\ntions in relational feature space, we visualize the representations of\nthe instances in TACRED datasets with t-SNE [29] and randomly se-\nlect 4 relations from the test set. As shown in Figure 4, we color each\ninstance according to its ground-truth relation label and we can ob-\nserve that the proposed model without Relation Semantic Clustering\n(left) gives general results and does not provide discriminative cluster\nassignments. For example, the instances with black and red colors\nmay have similar syntactic or surface features and clustering them di-\nrectly will lead to a poor result. When we use clustering module in our\nmodel, model with full module (right) can learn more discrimintaive\nfeatures and each relation is mostly separate from others.\n；\n乡\n．\n“\n父．．＆\n.\n4\n人．\n？\n4\n4\n今\nr\ns\ni\n.\n•\n•\n \n．．\n．\n \n,\n \n．\n．\n \n．．\n．\n ．\n．\n \n••\n•\n ．\n \n．\n \n,g·\n·\n·\n•\n••\n \nL\n •••\n \n．\n \ng\n·\n \n·\n卢\n．\n．．．\n 一．\n资\n.\n.\n \n、\n:\n5\n.\n \n•\n \n`\n:\n：.\n \n扩\n．．\n．\n女\n：\n ．．．\n.\ng-\n”“\n ...\n \n.\n ~．一\n位．\n，』．＂\"\n．\n \n.\nt\n .•\n•\n$\n冷\n｀\n『\n岱·\n冷心．\n书\n.＾···\n俨\n. ．,. 七\n叠\nt\n．\n兮\n•\n•\n “\n今．．．\n.\n`\"\n ．\n．\n．\n ．\n．\n `\n”\n、\n,.”^\n．\n心.\n*”\n;\n．\n1\n．\n蜕\n．．\n \n．\n \n久\n·，\n为\nr\n占\n上\nFigure 4: Visualizing contextualized entity-level features after t-SNE\ndimension reduction on TACRED.\nAnalysis on Clustering Results.\nDue to the number of clusters\nis lower than the number of true relations, different relations are\nlikely to be clustered into same relation class. We attempt to have\na detailed analysis on clustering results from TACRED to further\nverify whether different relation types in same cluster group have\nsimilar semantic or not. Specifically, we select the largest and smallest\ncluster group, the two most typical groups, which contain the most\nand least number of samples, to conduct detailed analysis. We find\nthe top 5 real relations that appear most frequently in each of these\ntwo cluster groups. The 5 relation types in the former are: “per:\ncities_of_residence, per: countries_of_residence, per: origin, per:\nstateorprovinces_of_residence, per: city_of_death”; The 5 relation\ntypes in the latter are: “org: country_of_headquarters, org: members,\norg: stateorprovince_of_headquarters, org: member_of, org: parents.\nThe findings of the analysis have led to the conclusion that these\nrelation types in the same cluster have analogous relational semantics\nand a potential hierarchical structure. Furthermore, we count the\nFigure 5: Statistics of samples in clusters and classes from the largest\nand smallest group.\nexact number of samples based on their true relation types in each\ncluster group. As shown in Figure 5, in the largest cluster group\n(left), the number of samples from different relation types is nearly\nand these samples occupy the vast majority of their corresponding\nrelation types, which indicate that the cluster is a fine super-class. On\nthe contrary, the most frequent relation type dominants the smallest\ncluster group (right), while other types only have one or few samples\nin this cluster, which indicate that the cluster is very pure and have\nless small impurities.\n4\nRelated Work\nSelf-supervised Learning\nSelf-supervised learning enables AI sys-\ntems to learn from orders of magnitude more data, which is important\nto recognize and understand patterns of more subtle, less common\nrepresentations of the world. Specifically, self-supervised learning\ntries to learn an encoder that extracts generic feature representations\nfrom unlabeled datasets. Early work focuses on solving different artifi-\ncially designed pretext tasks that does not require any supervision and\ncan be easily constructed on the dataset, such as predicting neighbor\nwords [21], generating neighbor sentences [14] for textual data, and\ndenoising [32], colorization [15], adversarial generative models [9]\nfor image data. Nevertheless, the feature representations are tailored\nto the specific pretext tasks with limited generalization.\nRecent self-supervised learning algorithms mainly solve an instance\ndiscrimination task. In these algorithms, instance-wise contrastive\nlearning with InfoNCE loss function [23] is prominent. Instance-CL\ntreats each instance in the dataset and its augmentations as an indepen-\ndent pair and tries to pull together the representations within each pair\nwhile pushing apart different pairs . Consequently, different instances\nare well-separated in the learned feature space with local invariance\nbeing preserved for each instance. Although Instance-CL may implic-\nitly group similar instances together, it pushes representations apart\nas long as they are from different original instances, regardless of\ntheir semantic similarities. Thereby, the implicit grouping effect of\nInstance-CL is less stable and more data-dependent, giving rise to\nworse representations in some cases [17, 25].\nOpen Relation Extraction\nOpen relation extraction has received\nmore attention in recent years and many efforts have been under-\ntaken to exploring methods for it, due to the ability to extract new\nemerging relation types. The first line of research is Open Informa-\ntion Extraction , in which relation phrases are extracted directly to\nrepresent different relation types. However, using surface forms to\nrepresent relations results in an associated lack of generality since\nmany surface forms can express the same relation. Another explo-\nration is Relation Discovery, aims at discovering unseen relation types\nfrom open-domain text. Relation discovery can be divided into two\ndifferent approaches: 1) cluster the relation representations learned\nfrom the instances, or 2) make more assumptions as learning signals\nto discover better relational representations.\nThe variational autoencoder (VAE) based approaches are under\nunsupervised setting. Marcheggiani and Titov [20] first propose the\nvariational autoencoder method on unsupervised relation extraction.\nThe model utilize the encoder extracts the semantic relation from\nhand-crafted features of the sentence and the decoder tries to predict\none of the two entities given the relation and the other entity with a\ngeneral triplet scoring function [22]. However, Simon et al. [27] point\nout that the aforementioned method severely suffer from the instability,\nand they also propose two regularizers to guide the learning procedure.\nBut the fundamental cause of the instability is still undiscovered. On\nthis basis, Tran et al. [28] demonstrate that by using only named\nentities to induce relation types can achieve better performance. Yuan\net al. [37] assume that these classifications are a latent variable so\nthey are required to follow a pre-defined prior distribution which\nresults in unstable training and overcome this limitation by using the\nclassifications as an intermediate variable instead of a latent variable.\nIn clustering-based approaches, the supervised learning model [33, 38,\n40] are restricted by labeled data despite achieving good performance.\nIn unsupervised setting, Yao et al. [36] proposed Rel-LDA model\n,using a generative model inspired by LDA to cluster sentences: each\nrelation defines a distribution over a high-level handcrafted set of\nfeatures describing the relationship between the two entities in the text\n(e.g. the dependency path). Hu et al. [12] proposed SelfORE which\nencodes relational feature space in a self-supervised method that\nbootstraps relational feature signals by leveraging adaptive clustering\nand classification iteratively.\n5\nConclusion\nIn this work, we investigate the deficiencies of the contrastive learning\non unsupervised relation extraction and propose a similarity-based\nrepresentation learning method, which can learn well semantic of\ninstances to effectively improve the performance with unsupervised\nclustering. In addition, we further obtain discriminative feature repre-\nsentations through relational semantic clustering. Owing to the fact\nthat our model is straightforward and efficient, we believe that our\napproach easily admits extensions to different open-domain texts.\nHowever, our model still has some shortcomings. Similar to other un-\nsupervised relation extraction methods, our model is unable to handle\ninstances where entity pairs appearing in a sentence do not exhibit any\nrelation. We leave these problems as future work and look forward to\nseeking possible solutions from a broader perspective.\nAcknowledgements\nWe would like to thank the referees for their comments, which helped\nimprove this paper considerably.\nReferences\n[1]\nRabah A Al-Zaidy and C Lee Giles, ‘Extracting semantic relations for\nscholarly knowledge base construction’, in 2018 IEEE 12th international\nconference on semantic computing (ICSC), pp. 56–63. IEEE, (2018).\n[2]\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton, ‘Layer nor-\nmalization’, arXiv preprint arXiv:1607.06450, (2016).\n[3]\nAmit Bagga and Breck Baldwin, ‘Entity-based cross-document corefer-\nencing using the vector space model’, in COLING 1998 Volume 1: The\n17th International Conference on Computational Linguistics, (1998).\n[4]\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie\nTaylor, ‘Freebase: a collaboratively created graph database for structur-\ning human knowledge’, in Proceedings of the 2008 ACM SIGMOD in-\nternational conference on Management of data, pp. 1247–1250, (2008).\n[5]\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt, ‘Discover-\ning latent knowledge in language models without supervision’, arXiv\npreprint arXiv:2212.03827, (2022).\n[6]\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze,\n‘Deep clustering for unsupervised learning of visual features’, in Pro-\nceedings of the European conference on computer vision (ECCV), pp.\n132–149, (2018).\n[7]\nXinlei Chen and Kaiming He, ‘Exploring simple siamese representation\nlearning’, in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 15750–15758, (June 2021).\n[8]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,\n‘BERT: Pre-training of deep bidirectional transformers for language\nunderstanding’, in Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers),\npp. 4171–4186, Minneapolis, Minnesota, (June 2019). Association for\nComputational Linguistics.\n[9]\nJeff Donahue and Karen Simonyan, ‘Large scale adversarial representa-\ntion learning’, in Proceedings of the 33rd International Conference on\nNeural Information Processing Systems, pp. 10542–10552, (2019).\n[10]\nHady Elsahar, Elena Demidova, Simon Gottschalk, Christophe Gravier,\nand Frederique Laforest, ‘Unsupervised open relation extraction’, in\nThe Semantic Web: ESWC 2017 Satellite Events: ESWC 2017 Satellite\nEvents, Portorož, Slovenia, May 28–June 1, 2017, Revised Selected\nPapers 14, pp. 12–16. Springer, (2017).\n[11]\nTianyu Gao, Xingcheng Yao, and Danqi Chen, ‘SimCSE: Simple con-\ntrastive learning of sentence embeddings’, in Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing, pp.\n6894–6910, Online and Punta Cana, Dominican Republic, (November\n2021). Association for Computational Linguistics.\n[12]\nXuming Hu, Lijie Wen, Yusong Xu, Chenwei Zhang, and S Yu Philip,\n‘Selfore: Self-supervised relational feature learning for open relation ex-\ntraction’, in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pp. 3673–3682, (2020).\n[13]\nLawrence Hubert and Phipps Arabie, ‘Comparing partitions’, Journal of\nclassification, 2, 193–218, (1985).\n[14]\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Anto-\nnio Torralba, Raquel Urtasun, and Sanja Fidler, ‘Skip-thought vectors’,\nin Proceedings of the 28th International Conference on Neural Informa-\ntion Processing Systems-Volume 2, pp. 3294–3302, (2015).\n[15]\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich, ‘Learning\nrepresentations for automatic colorization’, in Computer Vision–ECCV\n2016: 14th European Conference, Amsterdam, The Netherlands, October\n11–14, 2016, Proceedings, Part IV 14, pp. 577–593. Springer, (2016).\n[16]\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and\nLei Li, ‘On the sentence embeddings from pre-trained language models’,\nin Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 9119–9130, (2020).\n[17]\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi, ‘Prototypical\ncontrastive learning of unsupervised representations’, arXiv preprint\narXiv:2005.04966, (2020).\n[18]\nFangchao Liu, Lingyong Yan, Hongyu Lin, Xianpei Han, and Le Sun,\n‘Element intervention for open relation extraction’, in Proceedings of\nthe 59th Annual Meeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp. 4683–4693, Online, (August\n2021). Association for Computational Linguistics.\n[19]\nShuliang Liu, Xuming Hu, Chenwei Zhang, Shu’ang Li, Lijie Wen,\nand Philip Yu, ‘HiURE: Hierarchical exemplar contrastive learning for\nunsupervised relation extraction’, in Proceedings of the 2022 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 5970–5980, Seattle,\nUnited States, (July 2022). Association for Computational Linguistics.\n[20]\nDiego Marcheggiani and Ivan Titov, ‘Discrete-state variational autoen-\ncoders for joint discovery and factorization of relations’, Transactions\nof the Association for Computational Linguistics, 4, 231–244, (2016).\n[21]\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey\nDean, ‘Distributed representations of words and phrases and their compo-\nsitionality’, in Proceedings of the 26th International Conference on Neu-\nral Information Processing Systems-Volume 2, pp. 3111–3119, (2013).\n[22]\nMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel, ‘A three-way\nmodel for collective learning on multi-relational data’, in Proceedings\nof the 28th International Conference on International Conference on\nMachine Learning, pp. 809–816, (2011).\n[23]\nAaron van den Oord, Yazhe Li, and Oriol Vinyals, ‘Representa-\ntion learning with contrastive predictive coding’, arXiv preprint\narXiv:1807.03748, (2018).\n[24]\nAshokkumar Palanivinayagam and Sureshkumar Nagarajan, ‘An opti-\nmized iterative clustering framework for recognizing speech’, Interna-\ntional Journal of Speech Technology, 23, 767–777, (2020).\n[25]\nSenthil Purushwalkam and Abhinav Gupta, ‘Demystifying contrastive\nself-supervised learning: Invariances, augmentations and dataset biases’,\nAdvances in Neural Information Processing Systems, 33, 3407–3418,\n(2020).\n[26]\nAndrew Rosenberg and Julia Hirschberg, ‘V-measure: A conditional\nentropy-based external cluster evaluation measure’, in Proceedings of\nthe 2007 Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning (EMNLP-\nCoNLL), pp. 410–420, Prague, Czech Republic, (June 2007). Association\nfor Computational Linguistics.\n[27]\nÉtienne Simon, Vincent Guigue, and Benjamin Piwowarski, ‘Unsuper-\nvised information extraction: Regularizing discriminative approaches\nwith relation distribution losses’, in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pp. 1378–\n1387, (2019).\n[28]\nThy Thy Tran, Phong Le, and Sophia Ananiadou, ‘Revisiting unsuper-\nvised relation extraction’, in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pp. 7498–7505, Online,\n(July 2020). Association for Computational Linguistics.\n[29]\nLaurens van der Maaten and Geoffrey Hinton, ‘Visualizing data using\nt-sne’, Journal of Machine Learning Research, 9, 2579–2605, (2008).\n[30]\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis,\nMarc Proesmans, and Luc Van Gool, ‘Scan: Learning to classify images\nwithout labels’, in Computer Vision–ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X, pp.\n268–285. Springer, (2020).\n[31]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin, ‘Attention\nis all you need’, in Advances in Neural Information Processing Systems,\neds., I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, volume 30. Curran Associates, Inc.,\n(2017).\n[32]\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine\nManzagol, ‘Extracting and composing robust features with denoising\nautoencoders’, in Proceedings of the 25th international conference on\nMachine learning, pp. 1096–1103, (2008).\n[33]\nRuidong Wu, Yuan Yao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin,\nLeyu Lin, and Maosong Sun, ‘Open relation extraction: Relational\nknowledge transfer from supervised data to unsupervised data’, in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP), pp. 219–228, Hong Kong,\nChina, (November 2019). Association for Computational Linguistics.\n[34]\nJunyuan Xie, Ross Girshick, and Ali Farhadi, ‘Unsupervised deep em-\nbedding for clustering analysis’, in Proceedings of the 33rd International\nConference on International Conference on Machine Learning-Volume\n48, pp. 478–487, (2016).\n[35]\nChenyan Xiong, Russell Power, and Jamie Callan, ‘Explicit semantic\nranking for academic search via knowledge graph embedding’, in Pro-\nceedings of the 26th international conference on world wide web, pp.\n1271–1279, (2017).\n[36]\nLimin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum,\n‘Structured relation discovery using generative models’, in Proceedings\nof the 2011 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1456–1466, (2011).\n[37]\nChenhan Yuan and Hoda Eldardiry, ‘Unsupervised relation extraction:\nA variational autoencoder approach’, in Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Processing, pp.\n1929–1938, Online and Punta Cana, Dominican Republic, (November\n2021). Association for Computational Linguistics.\n[38]\nKai Zhang, Yuan Yao, Ruobing Xie, Xu Han, Zhiyuan Liu, Fen Lin,\nLeyu Lin, and Maosong Sun, ‘Open hierarchical relation extraction’,\nin Proceedings of the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 5682–5693, Online, (June 2021). Association for Com-\nputational Linguistics.\n[39]\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christo-\npher D. Manning, ‘Position-aware attention and supervised data improve\nslot filling’, in Proceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 35–45, Copenhagen, Denmark,\n(September 2017). Association for Computational Linguistics.\n[40]\nJun Zhao, Tao Gui, Qi Zhang, and Yaqian Zhou, ‘A relation-oriented\nclustering method for open relation extraction’, in Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Process-\ning, pp. 9707–9718, (2021).\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2023-10-01",
  "updated": "2023-10-01"
}