{
  "id": "http://arxiv.org/abs/2310.00637v1",
  "title": "Knowledge Engineering using Large Language Models",
  "authors": [
    "Bradley P. Allen",
    "Lise Stork",
    "Paul Groth"
  ],
  "abstract": "Knowledge engineering is a discipline that focuses on the creation and\nmaintenance of processes that generate and apply knowledge. Traditionally,\nknowledge engineering approaches have focused on knowledge expressed in formal\nlanguages. The emergence of large language models and their capabilities to\neffectively work with natural language, in its broadest sense, raises questions\nabout the foundations and practice of knowledge engineering. Here, we outline\nthe potential role of LLMs in knowledge engineering, identifying two central\ndirections: 1) creating hybrid neuro-symbolic knowledge systems; and 2)\nenabling knowledge engineering in natural language. Additionally, we formulate\nkey open research questions to tackle these directions.",
  "text": "Knowledge Engineering using Large Language\nModels\nBradley P. Allen #\nUniversity of Amsterdam, Amsterdam, NL\nLise Stork #\nVrije Universiteit Amsterdam, Amsterdam, NL\nPaul Groth #\nUniversity of Amsterdam, Amsterdam, NL\nAbstract\nKnowledge engineering is a discipline that fo-\ncuses on the creation and maintenance of processes\nthat generate and apply knowledge. Traditionally,\nknowledge engineering approaches have focused on\nknowledge expressed in formal languages.\nThe\nemergence of large language models and their cap-\nabilities to effectively work with natural language,\nin its broadest sense, raises questions about the\nfoundations and practice of knowledge engineer-\ning. Here, we outline the potential role of LLMs in\nknowledge engineering, identifying two central dir-\nections: 1) creating hybrid neuro-symbolic know-\nledge systems; and 2) enabling knowledge engin-\neering in natural language. Additionally, we for-\nmulate key open research questions to tackle these\ndirections.\n2012 ACM Subject Classification Computing methodologies →Natural language processing, Comput-\ning methodologies →Machine learning, Computing methodologies →Philosophical/theoretical found-\nations of artificial intelligence, Software and its engineering →Software development methods\nKeywords and Phrases knowledge engineering, large language models\nDigital Object Identifier 10.1234/0000000.00000000\nReceived 2023-07-14 Accepted To be completed by Dagstuhl editorial office Published To be completed\nby Dagstuhl editorial office\n1\nIntroduction\nKnowledge engineering (KE) is a discipline concerned with the development and maintenance of\nautomated processes that generate and apply knowledge [4, 93]. Knowledge engineering rose to\nprominence in the nineteen-seventies, when Edward Feigenbaum and others became convinced\nthat automating knowledge production through the application of research into artificial intelli-\ngence required a domain-specific focus [32]. The period from the mid-nineteen-seventies into the\nnineteen-eighties saw the knowledge engineering of rule-based expert systems for the purposes of\nthe automation of decision making in business enterprise settings. By the early nineteen-nineties,\nhowever, it became clear that the expert systems approach, given its dependence on manual\nknowledge acquisition and rule-based representation of knowledge by highly skilled knowledge\nengineers, resulted in systems that were expensive to maintain and difficult to adapt to chan-\nging requirements or application contexts.\nFeigenbaum argued that, to be successful, future\nknowledge-based systems would need to be scalable, globally distributed, and interoperable [34].\nThe establishment of the World Wide Web and the emergence of Web architectural principles\nin the mid-nineteen-nineties provided a means to address these requirements. Tim Berners-Lee\nargued for a \"Web of Data\" based on linked data principles, standard ontologies, and data sharing\nprotocols that established open standards for knowledge representation and delivery on and across\nthe Web [11]. The subsequent twenty years witnessed the development of a globally federated\nopen linked data \"cloud\" [13], the refinement of techniques for ontology engineering [51], and\nmethodologies for the development of knowledge-based systems [86]. During the same period,\n© Bradley P. Allen, Lise Stork, Paul Groth;\nlicensed under Creative Commons Attribution 4.0 International (CC BY 4.0)\nTransactions on Graph Data and Knowledge, Vol. VOL, Issue ISS, Article No. ART NO., pp. ART:1–ART:19\nTransactions on Graph Data and Knowledge\nTGDK Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany\narXiv:2310.00637v1  [cs.AI]  1 Oct 2023\nXX:2\nKnowledge Engineering using Large Language Models\nincreasing use of machine learning and natural language processing techniques led to new means\nof knowledge production through the automated extraction of knowledge from natural language\ndocuments and structured data sources [26, 68]. Internet-based businesses in particular found\nvalue in using such technologies to improve access to and discovery of Web content and data [43].\nA consensus emerged around the use of knowledge graphs as the main approach to knowledge\nrepresentation in the practice of knowledge engineering in both commercial and research arenas,\nproviding easier reuse of knowledge across different tasks and a better developer experience for\nknowledge engineers [45].\nMore recently, the increase in the availability of graphical processing hardware for fast mat-\nrix arithmetic, and the exploitation of such hardware to drive concurrent innovations in neural\nnetwork architectures at heretofore unseen scales [106], has led to a new set of possibilities for\nthe production of knowledge using large language models (LLMs). LLMs are probabilistic mod-\nels of natural language, trained on very large corpora of content, principally acquired from the\nWeb. Similiar to previous approaches to language modelling , given a sequence of tokens LLMs\npredict a probable next sequence of tokens based on a learned probability distribution of such\nsequences. However, presumably due to the vast amount of content processed in learning and the\nlarge size and architecture of the neural networks involved, LLMs exhibit remarkable capabilities\nfor natural language processing that far exceed earlier approaches [60].\nThese capabilities include the ability to do zero- or few-shot learning across domains [20], to\ngeneralize across tasks, including the ability to perform domain-independent question answering\nintegrating large amounts of world knowledge [77], to generate text passages at human levels of\nfluency and coherence [28, 96], to deal gracefully with ambiguity and long-range dependencies in\nnatural language [104], and to reduce or even eliminate the need for manual feature engineer-\ning [98]. LLMs also exhibit the ability to generate and interpret structured and semi-structured\ninformation, including programming language code [6, 100], tables [46, 53], and RDF metadata\n[106, 58, 7]. The generalization of language models (termed \"foundation models\" by some) to\nother modalities including images and audio have led to similarly significant advances in image\nunderstanding [23, 117], image generation [38, 79, 83], speech recognition, and text-to-speech\ngeneration [78, 105]. Such capabilities have prompted a significant amount of research and devel-\nopment activity demonstrating potential applications of LLMs [66, 84, 54]. However, the means\nof incorporating LLMs into structured, controllable, and repeatable approaches to developing and\nfielding such applications in production use are only just beginning to be considered in detail [73].\nThis paper engages with the question of how LLMs can be effectively employed in the context\nof knowledge engineering. We start by examining the different forms that knowledge can take,\nboth as inputs for constructing knowledge systems and as outputs of such systems. We argue\nthat the distinction between knowledge expressed in natural language (or other evolved, naturally\noccurring modalities such as images or video) and knowledge expressed in formal languages (for\nexample, as knowledge graphs or rules), sheds light how LLMs can be brought to bear on the\ndevelopment of knowledge systems.\nBased on this perspective, we then describe two potential paths forward. One approach in-\nvolves treating LLMs as components within hybrid neuro-symbolic knowledge systems. The other\napproach treats LLMs and prompt engineering [57] as a standalone approach to knowledge engin-\neering1, using natural language as the primary representation of knowledge. We then enumerate a\nset of open research problems in the exploration of these paths. These problems aim to determine\nthe feasibility of and potential approaches to using LLMs with existing KE methodologies, as well\n1 As defined by [57], prompt engineering is finding the most appropriate prompt or input text to an LLM to\nhave it solve a given task.\nB. P. Allen, L. Stork, P. Groth\nXX:3\nas the development of new KE methodologies centered around LLMs and prompt engineering.\n2\nForms of knowledge and their engineering\nIn the history of the computational investigation of knowledge engineering, knowledge has been\noften treated primarily as symbolic expressions. However, as [39] noted, knowledge is actually\nencoded in a variety of media and forms, most notably in natural language (e.g. English) but also\nin images, video, or even spreadsheets. This fact becomes even more apparent when looking at\ninstitutional knowledge practices that have developed over centuries, for example, in the sciences\nor archives [44]. We now illustrate this point by describing the many ways in which knowledge\nmanifests itself in the context of biodiversity informatics.\n2.1\nThe multimodal richness of knowledge: an example from biodiversity\nsciences\nThe ultimate goal of biodiversity science is to understand species evolution, variation, and distri-\nbution, but finds applications in a variety of other fields such as climate science and policy. At its\nheart is the collection and observation of organisms, providing evidence for deductions about the\nnatural world [59]. Such knowledge is inherently multimodal in nature, most commonly appearing\nin the form of images, physical objects, tree structures and sequences, i.e., molecular data.\nHistorically, organism sightings have been carefully logged in handwritten field diaries to de-\nscribe species behaviour and environmental conditions. Detailed drawings and later photographs\nwere made to capture colour, organs and other knowledge about an organism’s traits used for\nidentification, which is best conveyed visually but which is challenging to preserve in natural\nspecimens. These manuscripts are housed, together with the physical zoological specimens and\nherbaria which they describe, in museums and collection facilities across the world. Both the mul-\ntimodal nature of these knowledge sources as well as their distributed nature hamper knowledge\nintegration and synthesis.\nMetadata describes the specimen’s provenance: where specimens were found, who found them,\nand provides an attempt at identifying the type of organism (such as the preserved squid specimen\nshown in Figure 1). Such knowledge is paramount, as it allows researchers to understand resources\nwithin the context in which they were produced, enabling researchers to carry out ecological\nstudies such as distribution modeling over time.\nFor a systematic comparison of the multitude of resources available, the biodiversity sciences\nhave had a long-standing tradition of developing information standards [67]. From Linnaeus’\nSystema naturae mid 18th century as well as his formal introduction of zoological nomenclature,\ntaxonomists have started categorizing natural specimens according to tree-like hierarchical struc-\ntures. The process is challenging, given that biologist up until this day do not have a full picture\nof all living organisms on earth, and incomplete, naturally evolved and fuzzy knowledge is not\neasily systematized.\nThe development of digital methods has opened up new pathways for comparison and ana-\nlysis. Gene sequencing technology has lead biologist to the genetic comparison of species, by the\ncalculation of ancestry and construction of evolutionary tree structures in the study of phylo-\ngeny [50]. More importantly, digital methods allowed the transfer of analog resources, such as\nspecimen collection scans [14] and metadata, to the digital world. Such techniques have furthered\nformalisation and thereby interoperability of collected data through the use of Web standards,\nsuch as globally unique identifiers for species names [72] as well as shared vocabularies for data\nintegration across collections [10]. The Global Biodiversity Information Facility (GBIF) and their\ndata integration toolkit serves as a great example of such integration efforts [97, 81]. Currently,\nTGDK\nXX:4\nKnowledge Engineering using Large Language Models\nFigure 1 A specimen of the Loligo vulgaris Lamarck, 1798 species from the Naturalis–Zoology and\nGeology catalogues.a Images free of known restrictions under copyright law (Public Domain Mark 1.0).\na https://bioportal.naturalis.nl/nl/specimen/RMNH.MOL.5009890\nthere is a large emphasis on linking up disparate digital resources in the creation of an inter-\nconnected network of digital collection objects on the Web, linked up with relevant ecological,\nenvironmental and other related data in support of machine actionability (i.e., the ability of com-\nputational systems to find, access, interoperate, and reuse data with minimal intervention) for an\narray of interdisciplinary tasks such as fact-based decision-making and forecasting [41].Using data\nstandards for describing and reasoning over collection data can aid researchers counter unwanted\nbiases via transparency. However, making data comply with data standards can also lead to\noversimplification or reinterpretation [71].\nMachine learning and knowledge engineering strategies can help to (semi-)automatically ex-\ntract and structure biodiversity knowledge according [102, 91], for instance using state-of-the-art\ncomputer vision or natural language processing techniques as well as crowd-sourcing platforms\nfor the annotation of field diaries and other collection objects with formal language [92, 29]. Nev-\nertheless, a bottleneck in the digitization of collections and their use for machine actionability is\nthe amount of work and domain expertise required for the formalisation of such knowledge, and\nthe extraction from unstructured texts, images and video’s. Historical resources, i.e. handwritten\ntexts, pose an additional challenge, as they are exceptionally challenging to interpret within the\ncurrent scientific paradigm [107].\nThe variety and usefulness of different forms of knowledge both natural and formal and the\nchallenges they pose is not limited to the biodiversity domain as described above. We see the\nsame diversity happening in law [82], medicine [16, 21] and even self-driving vehicles [9]. To\nsummarize:\ndomain knowledge is often best represented in a variety of modalities, i.e., images, taxonomies,\nor free text, each modality with its own data structure and characteristics which should be\npreserved, and no easy way of integrating, interfacing with or reasoning over multimodal\nknowledge in a federated way exists;\nprovenance of data is paramount in understanding knowledge within the context in which it\nwas produced;\nfuzzy, incomplete, or complex knowledge is not easily systematized;\nusing data standards for describing and reasoning over collection data can aid researchers\ncounter unwanted biases via transparency;\nmaking data comply with data standards can lead to oversimplification or reinterpretation;\nB. P. Allen, L. Stork, P. Groth\nXX:5\nthe production of structured domain knowledge, for instance from images or free text, requires\ndomain expertise, and is therefore labour intensive and costly;\nknowledge evolves, and knowledge-based systems are required to deal with updates in their\nknowledge bases.\n2.2\nKE as the transformation of knowledge expressed in natural language\ninto knowledge expressed in a formal language\nThis sort of rich and complex array of modalities for the representation of knowledge has tra-\nditionally posed a challenge to knowledge engineers [33]. Much of the literature on knowledge\nengineering methodology has focused on the ways in which knowledge in these naturally-occurring\nforms can be recast into a structured symbolic representation, e.g., using methods of knowledge\nelicitation from subject matter experts [88], for instance by the formulation of competency ques-\ntions for analysing application ontologies [12]. One way to think about this is as the process\nof expressing knowledge presented in a natural, humanly evolved language in a formally-defined\nlanguage. This notion of the transformation of natural language into a formal language as a\nmeans of enabling effective reasoning has a deep history rooted in methodologies developed by\nanalytical philosophers of the early twentieth century [24, 69], but dating even further back to\nLiebniz’s lingua rationalis [35] and the thought of Ramón Lull [37]. Catarina Dutilh Novaes [69]\nhas argued that formal languages enable reasoning that is less skewed by bias and held beliefs,\nan effect achieved through de-semantification, i.e., the process of replacing terms in a natural\nlanguage with symbols that can be manipulated without interpretation using a system of rules\nof transformation. Coupled with sensorimotor manipulation of symbols in a notational system,\npeople can reason in a manner that outstrips their abilities unaided by such a technology.\nWhile Dutilh Novaes’ analysis focuses on this idea of formal languages as a cognitive tool used\nby humans directly, e.g. through the manipulation of a system of notation using paper and pencil,\nshe notes that this manipulation of symbols is the route to the mechanization of reasoning through\ncomputation. When externally manifested as a function executed by a machine through either\ninterpretation by an inference engine, or through compilation into a machine-level language,\nthis approach of formalization yields the benefits of reliability, greater speed and efficiency in\nreasoning.\nThis idea captures precisely the essence of the practice of knowledge engineering: Starting from\nsources of knowledge expressed in natural language and other modalities of human expression,\nthrough the process of formalization [51, 95], knowledge engineers create computational artifacts\nembodying this knowledge. These computational artifacts then enable us to reason using this\nknowledge in a predictable, efficient, and repeatable fashion. This is done either by proxy through\nthe action of autonomous agents, or in the context of human-mediated decision-making processes.\n2.3\nLLMs as a general-purpose technology for transforming natural\nlanguage into formal language\nUntil recently, there have been two ways in which this sort of formalization could be performed:\nthrough the manual authoring of symbolic/logical representations, e.g., as in the traditional notion\nof expert systems [34], or through the use of machine learning and natural language processing\nto extract such representations automatically from natural language text [61]. But what has\nbecome evident with the emergence of LLMs, with their capabilities for language learning and\nprocessing, is that they provide a new and powerful type of general purpose tool for mapping\nTGDK\nXX:6\nKnowledge Engineering using Large Language Models\nbetween natural language2 and formal language, as well as other modalities. LLMs have shown\nstate-of-the-art performance on challenging NLP tasks such as relation extraction [5] or text\nabstraction/summarization [114], and have been used to translate between other modalities, such\nas images and text (called vision-language models [119, 77]) in computer vision tasks, or from\nnatural language to code [113, 47], in which a pretrained task-agnostic language model can be\nzero-shot and few-shot transferred to perform a certain task [20, 52]. If one accepts the position\nthat KE can be generally described as the process of transforming knowledge in natural language\ninto knowledge in formal language, then it becomes clear that LLMs provide an advance in our\nability to perform knowledge engineering tasks.\n3\nThe use of LLMs in the practice of knowledge engineering: two\nscenarios\nGiven the above discussion, the natural question that arises is: what might be the utility and\nimpact of the use of LLMs for the transformation of natural language into formal language, when\napplied in the context of the practice of knowledge engineering?\nWhen LLMs emerged as a new technology in the mid-2010s, two views of the relationship\nbetween LLMs and knowledge bases (KBs) were put forward. One was the LLM can be a useful\ncomponent for various processes that are part of a larger knowledge engineering workflow (i.e.\n\"LMs for KBs\" [3]); the other was that that the LLM is a cognitive artifact that can be treated as\na knowledge base in and of itself (i.e., \"LMs as KBs\" [75]). We exploit this dichotomy to formulate\na pair of possible future scenarios for the use of LLMs in the practice of KE. One is to use LLMs as\na technology for or tool in support of implementing knowledge tasks that have traditionally been\nbuild using older technologies such as rule bases and natural language processing (NLP). Another\nis to use LLMs to remove the need for knowledge engineers to be fluent in a formal language,\ni.e., by allowing knowledge for a given knowledge task to be expressed in natural language, and\nthen using prompt engineering as the primary paradigm for the implementation of reasoning and\nlearning. We now explore each of these scenarios in turn, and consider the open research problems\nthat they raise.\n3.1\nLLMs as components or tools used in knowledge engineering\nWe illustrate the first scenario through reference to CommonKADS [86], a structured method-\nology that has been used by knowledge engineers since the early 2000’s. CommonKADS is the\nrefinement of an approach to providing a disciplined approach to the development of knowledge\nsystems. This approach saw initial development in the nineteen-eighties as a reaction to both the\nad-hoc nature of early expert systems development [111] and to the frequency of failures in the\ndeployment of expert systems in an organizational context [34]. Stemming from early work on\nmaking expert systems development understandable and repeatable [42], CommonKADS is dis-\ntinguished from methodologies more focused on ontology development (e.g., NeON [94], Kendall\nand McGuinness’s \"Ontology 101\" framework [51], and Presutti’s ontology design patterns [76])\nin that it provides practical guidance for specification and implementation of knowledge systems\ncomponents in a broader sense. It attempts to provide a synoptic guide to the full scope of activ-\nities involved in the practice of KE, and show how it relates to the activities of the organization\n2 Again, we note that natural language should be read to include all modalities. Hence, the term “foundation\nmodel”[15] to refer to LLMs.\nB. P. Allen, L. Stork, P. Groth\nXX:7\nin which that engineering is taking place. As such, in the context of this paper we can use it as\na framework to explore for what tasks and in what ways LLMs can be used for KE.\nSome tasks identified by CommonKADS as part of the KE process may remain largely un-\nchanged by the use of LLMs. These include knowledge task identification and project organiz-\national design. But others can involve the use of LLMs. LLMs can assist knowledge engineers\nand/or knowledge providers in the performance of knowledge engineering tasks. They can also\nbe a means for the implementation of modules performing knowledge-intensive tasks. Examples\nof these uses include the following:\nKnowledge acquisition and elicitation LLMs can be used to support knowledge acquisition and\nelicitation in a given domain of interest. Engineers can create prompts that target specific\naspects of the domain, using the responses as a starting point for building the knowledge base.\nDialogs between LLMs trained using such prompts and knowledge providers, the subject mat-\nter experts, can support the review, validation, and refinement of the acquired knowledge [8].\nKnowledge organization LLMs can be used to organize the acquired knowledge into a coherent\nstructure using natural language, making it easy to understand and update. Prompt engineer-\ning can be used to develop a set of prompts that extract formal language using the LLM, e.g.,\nfor text to graph generation [40] or vice versa [18, 2]. Moreover, LLMs are used for program\nsynthesis [113, 47], the generation of metadata [56] or for fusing knowledge graphs [118].\nData augmentation LLMs can be used to generate synthetic training data to aid in testing the\nknowledge system by evaluating its performance on instances of the specific task [116].\nTesting and refinement Feedback from subject matter experts and users can be used to prompt\nan LLM to refine the natural language knowledge base and improve the system’s accuracy\nand efficiency through self-correction of prompts and tuning of the LLM model settings as\nneeded to optimize the system’s performance [110].\nMaintenance LLMs can be used to monitor new information and trends, and to then propose\nnew prompts integrating those updates into the knowledge base.\nConsider the CommonKADS knowledge task hierarchy shown in Figure 2. Synthetic knowledge-\nintensive tasks, e.g. design or configuration, are amenable to generative approaches [109]; analytic\nknowledge-intensive tasks can involve LLM components within a hybrid neuro-symbolic know-\nledge system.\nA shortcoming of using CommonKADS for our purposes, however, is that it predates the\nwidespread use of machine learning and statistical natural language processing in KE. A number\nof architectural approaches have since been developed that extend the CommonKADS concepts\nof a knowledge-intensive task type hierarchy and knowledge module templates. These include\nmodeling the fine-grained data flows and workflows associated with knowledge systems that com-\nbine components that ingest, clean, transform, aggregate and generate data, as well as generate\nand apply models built using machine learning [103, 19, 27, 31, 101]. These architectures are\nput forward as providing a general framework for composing heterogeneous tools for knowledge\nrepresentation and inference into a single integrated hybrid neuro-symbolic system. The design\npattern notations put forward in recent work [103, 101, 31] treat data, models, and symbolic\nrepresentations as the inputs and outputs of components composed into a variety of knowledge\nsystem design patterns. Generalizing these into natural language and formal language inputs and\noutputs can provide a simple way to extend these design notations to accommodate both LLMs\nas well as a richer set of knowledge representations.\nTGDK\nXX:8\nKnowledge Engineering using Large Language Models\nFigure 2 Hierarchy of knowledge-intensive task types from CommonKADS ([86], p.125)\n3.2\nKnowledge engineering as prompt engineering\nGiven that LLMs enable knowledge modeling in natural language, it is conceivable that the\nprogramming of knowledge modules could take place entirely in natural language. Consider that\nprompt programming is \"finding the most appropriate prompt to allow an LLM to solve a task\"\n[57]. One can through this lens view knowledge engineering as the crafting of dialogues in which\na subject matter expert (SME) arrives at a conclusion by considering the preceding context and\nargumentation [80, 109, 89, 60]. This framing of knowledge engineering as prompt engineering is\nthe second scenario we wish to explore.\nFrom the perspective of the CommonKADS knowledge-intensive task type hierarchy, this\nwould involve a redefinition of the types and hierarchy to use LLMs and prompt programming\ndesign patterns, e.g. as described in [57]. Several aspects of this redefinition could include:\nNatural language inference LLMs can be used to build natural language inference engines that\nuse the organized knowledge to perform the specific task by taking input queries and generate\noutput using prompt engineering to guide the LLM towards generating accurate inferences,\ne.g. using zero- or few-shot chain-of-thought design patterns. The benefit here is that the gap\nbetween the knowledge engineer, knowledge provider (the subject matter expert) and the user\nis smaller since a translation to a formal language (the language of the engineer) is no longer\nrequired.\nKnowledge-intensive task execution through human/machine dialog LLMs can be used to a\nconversational interface that allows users to interact with the knowledge system and receive\ntask-specific support.\nTesting and refinement through human/machine dialog Feedback from subject matter experts\nand users can be used to prompt an LLM to refine the natural language knowledge base and\nimprove the system’s accuracy and efficiency through self-correction of prompts and tuning\nB. P. Allen, L. Stork, P. Groth\nXX:9\nof the LLM model settings as needed to optimize the system’s performance.\nOne possible benefit of this approach would be that the barrier to adoption of knowledge\nengineering as a practice could be lowered significantly. Knowledge elicitation could be conducted\nentirely within natural language, meaning that subject matter experts without training in formal\nknowledge representations could perform these tasks directly. However, this approach assumes\nthat predictable inference [101] using natural language is satisfactory. The propensity of current\nLLMs to \"hallucinate\", i.e., to confabulate facts, is an obstacle to the realization of this idea\n[48]. Multiple efforts have been devoted to the creation of prompt programming patterns that\naddress this issue, ranging from chain-of-thought approaches [108] to retrieval-assisted generation,\ni.e. the augmentation of LLMs with authoritative document indexes and stores [84, 65]. Recent\nwork [73] has described ways in which knowledge graphs as a formal language can be integrated\nwith natural language and LLM-based language processing and reasoning to provide knowledge\nsystems architectures that directly address this issue. [115] surveys work in this direction.\n4\nOpen research questions\nUsing the scenarios outlined above, we can identify a number of open research questions to\nbe addressed to realize either or both of these two possible approaches to the use of LLMs in\nknowledge engineering. These questions touch on three general areas: the impact of LLMs on\nthe methodologies used to build knowledge systems, on the architectural design of knowledge\nsystems incorporating and/or based on LLMs, and on the evaluation of such systems. For each\nof these open questions, we provide a link back to the biodiversity scenario discussed in Section\n2.1 denoted by a \n.\n4.1\nMethodology\n4.1.1\nHow can knowledge engineering methodologies best be adapted to\nuse LLMs?\nHow can we harmoniously meld the considerable body of work on knowledge engineering meth-\nodologies [51, 36, 76, 94, 87, 85, 90] with the new capabilities presented by LLMs?\nSchreiber’s conceptualization of knowledge engineering as the construction of different aspect\nmodels of human knowledge [86], as discussed above, offers a framework for further elaboration.\nThe distinctive characteristics of LLMs, coupled with prompt engineering, present unique chal-\nlenges and opportunities for building agents within a knowledge system, one that is consistent\nwith the CommonKADS approach.\nWhile the role definitions within KE methodologies might mostly remain the same, the skills\nrequired for knowledge engineers will need morphing to adapt to the LLM environment. This\nevolution of roles calls for an extensive investigation into what these new skills might look like,\nand how they can be cultivated. Additionally, the adaptability of the various knowledge-intensive\ntask type hierarchies described by CommonKADS and its descendants in the literature on hybrid\nneuro-symbolic systems (e.g., as described in [19]) to accommodate LLMs is another fertile area\nfor exploration.\nLLM-based applications, likened to synthetic tasks within these knowledge engineering frame-\nworks, raise compelling research questions regarding accuracy and the prevention of hallucina-\ntions. LLM-based applications have a lower bar to reach with respect to notions of accuracy\nand avoidance of hallucinations, but still must provide useful and reliable guidance to users and\npractitioners.\nTGDK\nXX:10\nKnowledge Engineering using Large Language Models\n\n Connecting back to the biodiversity domain, answering these questions would provide\nguidance on the appropriate methodology to adopt when developing a new specimen curation\nand collection knowledge management system that needs to deal with multimodal assets like\nhandwritten text or images.\n4.1.2\nHow do principles of content and data management apply to prompt\nengineering?\nApplying content and/or data management principles to collections of prompts and prompt tem-\nplates, integral to work with LLMs, is an area ripe for exploration. Properly managing these\nresources could improve efficiency and guide the development of improved methodologies in know-\nledge engineering. This calls for a rigorous investigation of current data management practices,\ntheir applicability to LLMs, and potential areas of refinement. Ensuring the reproducibility of\nLLM engineering from a FAIR data standpoint [112] is a crucial yet complex challenge. De-\nveloping and validating practices and protocols that facilitate easy tracing and reproduction of\nLLM-based processes and outputs is central to this endeavour.\n\n Addressing this challenge will aid researchers in applying LLM engineering in a FAIR way.\nDoing so is critical for biodiversity research and science in general where precision, reproducibility\nand provenance are key for knowledge discovery and research integrity.\n4.1.3\nWhat are the cognitive norms that govern the conduct of KE?\nA crucial area of inquiry involves the identification and understanding of cognitive norms, as\ndescribed by Menary [62], that govern the practice of knowledge engineering. Cognitive norms\nare established within a human community of practice as a way of governing the acceptable use of\n\"external representational vehicles to complete a cognitive task\" [63]. As the consumer adoption\nof LLM technology has progressed, we see a great deal of controversy about when and how it\nis appropriate to use, e.g. in the context of education or the authoring of research publications.\nUnderstanding how these norms shape the use of LLMs in this context is an under-explored field\nof study. By unravelling the interplay between these cognitive norms and LLM usage, we can\ngain valuable insights into the dynamics of knowledge engineering practices and possibly foster\nmore effective and responsible uses of LLMs.\n\n In the biodiversity sciences, this means understanding the cognitive norms specific to the\ndomain, to understand how LLMs can be used in a way that respects the domain’s practices and\nstandards.\n4.1.4\nHow do LLMs impact the labor economics of KE?\nA related but distinct question pertains to the impact of LLMs on the economic costs associ-\nated with knowledge engineering. The introduction and application of LLMs in this field may\nsignificantly alter the economic landscape, either by driving costs down through automation and\nefficiency or by introducing new costs tied to system development, maintenance, and oversight.\nThoroughly exploring these economic implications can shed light on the broader effects of integ-\nrating LLMs into knowledge engineering.\nThe realm of labor economics as it pertains to hybrid or centaur systems [1], is another\narea ripe for investigation. Understanding how the deployment of these systems influences labor\ndistribution, skill requirements, and job roles could provide valuable input into the planning\nand implementation of such technologies. Additionally, it could reveal the potential societal and\neconomic impacts of this technological evolution.\nB. P. Allen, L. Stork, P. Groth\nXX:11\n\n Developments for LLM-based KE can help mitigate labour of knowledge experts in the\nbiodiversity sciences, for instance by the development of more efficient KE workflows for the\ndigitization of museum specimens or manuscripts.\n4.2\nArchitecture\n4.2.1\nHow can hybrid neuro-symbolic architectural models incorporate\nLLMs?\nDesign patterns for hybrid neuro-symbolic systems, as described in [103], offer a structured ap-\nproach to comprehend the flow of data within a knowledge system.\nAdapting this model to\naccount for the differences between natural and formal language could significantly enhance our\nability to trace and manage data within knowledge systems. A salient research question emerging\nfrom this scenario pertains to the actual process of integrating LLMs into knowledge engineer-\ning data processing flows [27]. Understanding the nuances of this process will involve a deep\nexamination of the shifts in methodologies, practices, and the potential re-evaluations of existing\nknowledge engineering paradigms. The perspective of KE enabled by LLMs as focused on the\ntransformation of natural language into formal language provides insights that can be used to\nimprove the motivation for hybrid neuro-symbolic systems; e.g., [19] references [17] in using dual\nprocess theories of reasoning (i.e. the \"System 1/System 2\" model described in [49]) as a motiva-\ntion for hybridization in knowledge systems, but more recent analyses [69, 64] cast doubt on the\nvalidity of such models, and point to more nuanced perspectives that provide a better grounding\nfor the benefits of hybridization.\n\n Addressing these questions would shed light on tasks for which hybridization using LLMs\nwould prove favourable, e.g., image classification of species.\n4.2.2\nHow can prompt engineering patterns support reasoning in natural\nlanguage?\nOne fundamental question that arises is how prompt engineering patterns can be utilized to facil-\nitate reasoning in natural language. Exploring this topic involves understanding the mechanics of\nthese patterns and their implications on natural language processing capabilities of LLMs. This\nline of research could open new possibilities for enhancing the functionality and efficiency of these\nmodels.\nA related inquiry concerns the structure, controllability, and repeatability of reasoning facil-\nitated by LLMs. Examining ways to create structured, manageable, and reproducible reasoning\nprocesses within these models could significantly advance our capacity to handle complex know-\nledge engineering tasks and improve the reliability of LLMs.\nThe interaction of LLMs and approaches to reasoning based on probabilistic formalisms is also\nan underexplored area of research. A particularly evocative effort in this area is that described\nin [113], which describes the use of LLMs to transform natural language into programs in a prob-\nabilistic programming language, which can then be executed to support reasoning in a particular\nproblem domain. We note that this work provides an excellent example of the knowledge en-\ngineering as the transformation of natural language into formal language perspective and of the\nimpact of LLMs in advancing that perspective. Investigating how to automatically generate and\nassess other nuanced forms of knowledge within LLMs could lead to a more refined understanding\nof these models and their capabilities.\n\n Given that biodiversity knowledge is often best represented in a variety of modalities each\nwith their own data structures and characteristics, research may explore how LLMs can act as\nTGDK\nXX:12\nKnowledge Engineering using Large Language Models\nnatural language interfaces to such multimodal knowledge bases.\n4.2.3\nHow can we manage bias, trust and control in LLMs using\nknowledge graphs?\nTrust, control, and bias in LLMs, especially when these models leverage knowledge graphs, are\ncritical areas to explore. Understanding how to detect, measure, and mitigate bias, as well as\nestablish trust and exert control in these models, is an essential aspect of ensuring ethical and\nresponsible use of LLMs. Furthermore, investigating methods to update facts in LLMs serving\nas knowledge graphs is a crucial area of research. Developing strategies for efficient and reliable\nfact updating could enhance the accuracy and usefulness of these models.\nAnother key question involves understanding how we can add provenance to statements pro-\nduced by LLMs. This line of research could prove vital in tracking the origin of information\nwithin these models, thus enhancing their reliability and usability. It opens the door to more\nrobust auditing and validation practices in the use of LLMs.\n\n Addressing this challenge can help biodiversity researchers detect and mitigate biases, as\nuse of LLMs might further exacerbate knowledge gaps, e.g., groups of individuals omitted from\nhistorical narratives in archival collections. Moreover, novel update mechanisms can aid research-\ners to reliably update facts or changing knowledge structures learned by LLMs, for instance when\ndomain knowledge evolves.\n4.2.4\nIs extrinsic explanation sufficient?\nA significant area of interest pertains to how we can effectively address the explainability of\nanswers generated using LLMs [30]. This exploration requires a deep dive into the functioning\nof LLMs and the mechanisms that govern their responses to prompts. Developing a thorough\nunderstanding of these processes can aid in creating transparency and trust in LLMs, as well as\nfostering their effective use.\nThe need for explanation in LLMs also leads to the question of whether extrinsic explanation\nis sufficient for the purposes of justifying a knowledge system’s reasoning, as argued in general for\nthe intelligibility of knowledge systems by Cappelen and Devers [22], or if intrinsic explainability\nis a necessary requirement [55]. This question calls for a thoughtful exploration of the value and\nlimitations of both extrinsic and intrinsic explanation methodologies, and their implications for\nthe understanding and usage of LLMs. An exciting research avenue arises from the work of Tiddi\n[99], concerning explainability with formal languages. The exploration of this topic could reveal\nsignificant insights into how we can leverage formal languages to enhance the explainability of\nLLMs. This could pave the way for new methods to increase transparency and intelligibility in\nthese models.\n\nIn the sciences in general, answering these questions would aid explainability of LLM-\ngenerated answers via curated facts, increasing transparency and trust.\n4.2.5\nHow can LLMs support the engineering of hybrid human/machine\nknowledge systems?\nAnother topic of interest involves exploring the potential of hybrid systems that combine human\ncognition with machine capabilities within a dialogical framework [64, 70]. As an exciting example\nof the possibilities for new approaches to human/machine collaboration in this vein, we point to\nthe recent results reported by [74] on the creation of conversational agents that simulate goal-\ndirected human conversation and collaboration on tasks. One can imagine coupling LLM-based\nB. P. Allen, L. Stork, P. Groth\nXX:13\nagents with human interlocutors working collaboratively in this manner on specific knowledge-\nintensive tasks. Understanding how to develop these types of systems, and what their implications\nmight be for the practice of knowledge engineering presents a fertile research line. It requires the\ncareful analysis of human-machine interaction, the study of system design principles, and the\ninvestigation of their potential impact.\n\n Research in this avenue can help mitigate the workload of the knowledge expert, for instance\nin the elicitation of domain knowledge, or crowdsourcing of annotations from unstructured sources\nsuch as herbaria or manuscripts.\n4.3\nEvaluation\n4.3.1\nHow do we evaluate knowledge systems with LLM components?\nThe first point of interest involves the evaluation of knowledge-based systems, with a focus bey-\nond just logic. This area calls for innovative methodologies to assess the system’s capacity to\nmanage and utilize knowledge efficiently, going beyond traditional logical evaluations. This topic\nof evaluation naturally extends to the question of how we evaluate ontologies and design pat-\nterns within knowledge engineering. Evaluating these aspects would require a deep dive into the\nstructures and mechanisms underpinning these elements, potentially leading to the development\nof refined evaluation metrics and methodologies.\nInterestingly, the long-standing paradigm of machine learning evaluation, relying on bench-\nmarking against a standard train/test dataset, seems to falter in the era of LLMs [25]. This\npresents an intriguing challenge for researchers and engineers alike. It is quite possible that tradi-\ntional methods may need to be significantly buttressed by methodologies and supporting tools for\nthe direct human evaluation of knowledge system performance. This has implications concern-\ning the cost and speed of evaluation processes, encouraging the rethink of current approaches to\nperhaps develop new strategies that balance accuracy, cost-effectiveness, and timeliness. Reima-\ngining evaluation methodologies in this new context could provide transformative insights into\nhow we can gain confidence in the reliability engineering of knowledge systems that use LLMs.\n\n Developments in this direction may aid biodiversity researchers to get a better under-\nstanding of the real-world efficacy of employing knowledge-based systems with LLM components\nin their institutions. One can think of improving access to collections, knowledge discovery, or\naccuracy in describing institutional knowledge.\n4.3.2\nWhat is the relationship between evaluation and explainability?\nLastly, there is an inherent dependency of evaluation on effective solutions for explainability\nwithin knowledge systems. Understanding this relationship could help in the creation of more\ncomprehensive evaluation models that take into account not only the performance of a system\nbut also its explainability.\n5\nSummary\nIn this paper, we have advocated for a reconsideration of the practice and methodology of know-\nledge engineering in light of the emergence of LLMs. We argued that LLMs allow naturally-\noccurring and humanly-evolved means of conveying knowledge to be brought to bear in the\nautomation of knowledge tasks. We described how this can enhance the engineering of hybrid\nneuro-symbolic knowledge systems, and how this can make knowledge engineering possible by\npeople who do not necessarily have the experience of recasting natural language into formal,\nstructured representation languages. Both of these possibilities will involve addressing a broad\nTGDK\nXX:14\nKnowledge Engineering using Large Language Models\nrange of open questions, which we have attempted to outline above. Given the rapid pace of the\ndevelopment of this area of research, it is our earnest hope that the coming months and years\nwill yield results shedding light on these questions.\nAcknowledgements\nThis work was partially supported by the EU’s Horizon Europe research\nand innovation programme within the ENEXA project (grant Agreement no. 101070305).\nB. P. Allen, L. Stork, P. Groth\nXX:15\nReferences\n1Zeynep Akata, Dan Balliet, Maarten De Rijke,\nFrank Dignum, Virginia Dignum, Guszti Eiben,\nAntske Fokkens, Davide Grossi, Koen Hindriks,\nHolger Hoos, et al.\nA research agenda for hy-\nbrid intelligence: augmenting human intellect with\ncollaborative, adaptive, responsible, and explain-\nable artificial intelligence. Computer, 53(8):18–28,\n2020.\n2Dimitrios Alivanistos, Selene Báez Santamaría,\nMichael Cochez, Jan Christoph Kalo, Emile van\nKrieken, and Thiviyan Thanapalasingam. Prompt-\ning as probing: Using language models for know-\nledge base construction.\nIn Sneha Singhania,\nTuan-Phong Nguyen, and Simon Razniewski, edit-\nors, LM-KBC 2022 Knowledge Base Construction\nfrom Pre-trained Language Models 2022, CEUR\nWorkshop\nProceedings,\npages\n11–34.\nCEUR-\nWS.org, 2022.\n3Badr AlKhamissi, Millicent Li, Asli Celikyilmaz,\nMona Diab, and Marjan Ghazvininejad. A review\non language models as knowledge bases. arXiv pre-\nprint arXiv:2204.06031, 2022.\n4Bradley\nP\nAllen,\nFilip\nIlievski,\nand\nSaurav\nJoshi.\nIdentifying\nand\nconsolidating\nknow-\nledge engineering requirements.\narXiv preprint\narXiv:2306.15124, 2023.\n5Christoph Alt, Marc Hübner, and Leonhard Hen-\nnig. Fine-tuning pre-trained transformer language\nmodels to distantly supervised relation extraction.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1388–1398, 2019.\n6Jacob Austin, Augustus Odena, Maxwell Nye,\nMaarten Bosma, Henryk Michalewski, David Do-\nhan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc\nLe, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\n7Agnes Axelsson and Gabriel Skantze. Using large\nlanguage models for zero-shot natural language\ngeneration from knowledge graphs. arXiv preprint\narXiv:2307.07312, 2023.\n8Stephen Bach, Victor Sanh, Zheng Xin Yong, Al-\nbert Webson, Colin Raffel, Nihal V. Nayak, Ab-\nheesht Sharma, Taewoon Kim, M Saiful Bari,\nThibault Fevry, Zaid Alyafeai, Manan Dey, An-\ndrea\nSantilli,\nZhiqing\nSun,\nSrulik\nBen-david,\nCanwen\nXu,\nGunjan\nChhablani,\nHan\nWang,\nJason Fries, Maged Al-shaibani, Shanya Sharma,\nUrmish Thakker,\nKhalid Almubarak,\nXiangru\nTang, Dragomir Radev, Mike Tian-jian Jiang,\nand Alexander Rush.\nPromptSource:\nAn in-\ntegrated development environment and reposit-\nory for natural language prompts.\nIn Proceed-\nings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics:\nSystem\nDemonstrations, pages 93–104, Dublin, Ireland,\nMay 2022. Association for Computational Lin-\nguistics. URL: https://aclanthology.org/2022.\nacl-demo.9, doi:10.18653/v1/2022.acl-demo.9.\n9Claudine Badue, Rânik Guidolini, Raphael Vivac-\nqua Carneiro, Pedro Azevedo, Vinicius B Cardoso,\nAvelino Forechi, Luan Jesus, Rodrigo Berriel, Thi-\nago M Paixao, Filipe Mutz, et al.\nSelf-driving\ncars: A survey. Expert Systems with Applications,\n165:113816, 2021.\n10Steve\nBaskauf,\nRoger\nHyam,\nStanley\nBlum,\nRobert A Morris, Jonathan Rees, Joel Sachs,\nGreg Whitbread, and John Wieczorek.\nTdwg\nstandards documentation specification.\nTech-\nnical report, Biodiversity Information Standards\n(TDWG), 2017.\n11Tim Berners-Lee, James Hendler, and Ora Lassila.\nThe semantic web. Scientific american, 284(5):34–\n43, 2001.\n12Camila Bezerra, Fred Freitas, and Filipe Santana.\nEvaluating ontologies with competency questions.\nIn 2013 IEEE/WIC/ACM International Joint\nConferences on Web Intelligence (WI) and Intel-\nligent Agent Technologies (IAT), volume 3, pages\n284–285. IEEE, 2013.\n13Christian Bizer, Tom Heath, Kingsley Idehen,\nand Tim Berners-Lee.\nLinked data on the web\n(ldow2008).\nIn Proceedings of the 17th inter-\nnational conference on World Wide Web, pages\n1265–1266, 2008.\n14Vladimir Blagoderov, Ian J Kitching, Laurence\nLivermore, Thomas J Simonsen, and Vincent S\nSmith. No specimen left behind: industrial scale\ndigitization of natural history collections. ZooKeys,\n209:133–146, 2012.\n15Rishi Bommasani and et al.\nOn the opportun-\nities and risks of foundation models.\nCoRR,\nabs/2108.07258,\n2021.\nURL: https://arxiv.\norg/abs/2108.07258, arXiv:2108.07258, doi:10.\n48550/arXiv.2108.07258.\n16Stephen\nBonner,\nIan\nP\nBarrett,\nCheng\nYe,\nRowan Swiers, Ola Engkvist, Andreas Bender,\nCharles Tapley Hoyt, and William L Hamilton. A\nreview of biomedical datasets relating to drug dis-\ncovery: a knowledge graph perspective. Briefings\nin Bioinformatics, 23(6):bbac404, 2022.\n17Grady Booch, Francesco Fabiano, Lior Horesh,\nKiran Kate, Jonathan Lenchner, Nick Linck, An-\ndreas Loreggia, Keerthiram Murgesan, Nicholas\nMattei, Francesca Rossi, et al. Thinking fast and\nslow in ai.\nIn Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 35, pages\n15042–15046, 2021.\n18Antoine\nBosselut,\nHannah\nRashkin,\nMaarten\nSap, Chaitanya Malaviya, Asli Celikyilmaz, and\nYejin\nChoi.\nCOMET:\nCommonsense\ntrans-\nformers for automatic knowledge graph construc-\ntion.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Lin-\nguistics, pages 4762–4779, Florence, Italy, July\n2019. Association for Computational Linguistics.\nURL: https://aclanthology.org/P19-1470, doi:\n10.18653/v1/P19-1470.\n19Anna\nBreit,\nLaura\nWaltersdorfer,\nFajar\nJ\nEkaputra, Marta Sabou, Andreas Ekelhart, An-\ndreea Iana, Heiko Paulheim, Jan Portisch, Artem\nRevenko, Annette ten Teije, et al. Combining ma-\nchine learning and semantic web:\nA systematic\nmapping study. ACM Computing Surveys, 2023.\n20Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared D Kaplan,\nPrafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nTGDK\nXX:16\nKnowledge Engineering using Large Language Models\nAmanda Askell, et al. Language models are few-\nshot learners. Advances in neural information pro-\ncessing systems, 33:1877–1901, 2020.\n21Tiffany J. Callahan,\nIgnacio J. Tripodi,\nHar-\nrison Pielke-Lombardo, and Lawrence E. Hunter.\nKnowledge-based biomedical data science. Annual\nReview of Biomedical Data Science, 3(1):23–41,\n2020. doi:10.1146/annurev-biodatasci-010820-\n091627.\n22Herman Cappelen and Josh Dever. Making AI in-\ntelligible: Philosophical foundations. Oxford Uni-\nversity Press, 2021.\n23Mathilde Caron,\nHugo Touvron,\nIshan Misra,\nHervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-\nsupervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on com-\nputer vision, pages 9650–9660, 2021.\n24André W Carus.\nCarnap and twentieth-century\nthought: Explication as enlightenment. Cambridge\nUniversity Press, 2007.\n25Yupeng Chang, Xu Wang, Jindong Wang, Yuan\nWu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan\nYi, Cunxiang Wang, Yidong Wang, et al. A sur-\nvey on evaluation of large language models. arXiv\npreprint arXiv:2307.03109, 2023.\n26Ronan Collobert, Jason Weston, Léon Bottou,\nMichael Karlen, Koray Kavukcuoglu, and Pavel\nKuksa. Natural language processing (almost) from\nscratch.\nJournal of machine learning research,\n12(ARTICLE):2493–2537, 2011.\n27Enrico Daga and Paul Groth. Data journeys: ex-\nplaining ai workflows through abstraction.\nSe-\nmantic Web, Preprint:1–27, 2023.\n28Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bi-\ndirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n29Chris Dijkshoorn, Mieke HR Leyssen, Archana\nNottamkandath, Jasper Oosterman, Myriam C\nTraub, Lora Aroyo, Alessandro Bozzon, Wan J\nFokkink, Geert-Jan Houben, Henrike Hovelmann,\net al. Personalized nichesourcing: Acquisition of\nqualitative annotations from niche communities. In\nUMAP Workshops, 2013.\n30Filip Karlo Došilović, Mario Brčić, and Nikica\nHlupić.\nExplainable artificial intelligence:\nA\nsurvey.\nIn 2018 41st International convention\non information and communication technology,\nelectronics and microelectronics (MIPRO), pages\n0210–0215. IEEE, 2018.\n31Fajar J Ekaputra, Majlinda Llugiqi, Marta Sabou,\nAndreas Ekelhart, Heiko Paulheim, Anna Breit,\nArtem Revenko, Laura Waltersdorfer, Kheir Ed-\ndine Farfar, and Sören Auer. Describing and or-\nganizing semantic web and machine learning sys-\ntems in the swemls-kg. In European Semantic Web\nConference, pages 372–389. Springer, 2023.\n32Edward A Feigenbaum.\nThe art of artificial in-\ntelligence: Themes and case studies of knowledge\nengineering. In Proceedings of the Fifth Interna-\ntional Joint Conference on Artificial Intelligence,\nvolume 2. Boston, 1977.\n33EDWARD A. FEIGENBAUM. Knowledge engin-\neering. Annals of the New York Academy of Sci-\nences, 426(1 Computer Cult):91–107, November\n1984. doi:10.1111/j.1749-6632.1984.tb16513.x.\n34Edward A Feigenbaum. A personal view of expert\nsystems: Looking back and looking ahead. Know-\nledge Systems Laboratory, Department of Com-\nputer Science, Stanford . . . , 1992.\n35Dov M Gabbay and John Woods. The rise of mod-\nern logic: from Leibniz to Frege. Elsevier, 2004.\n36Aldo Gangemi and Valentina Presutti. Ontology\ndesign patterns. In Handbook on ontologies, pages\n221–243. Springer, 2009.\n37Clark Glymour, Kenneth M Ford, and Patrick J\nHayes. Ramón lull and the infidels. AI Magazine,\n19(2):136–136, 1998.\n38Ian\nGoodfellow,\nJean\nPouget-Abadie,\nMehdi\nMirza,\nBing Xu,\nDavid Warde-Farley,\nSherjil\nOzair, Aaron Courville, and Yoshua Bengio. Gen-\nerative adversarial networks. Communications of\nthe ACM, 63(11):139–144, 2020.\n39Paul Groth, Aidan Hogan, Lise Stork, Kather-\nine Thornton, and Vrandečić Denny.\nKnowledge\ngraphs vs. other forms of knowledge representa-\ntion. Dagstuhl Reports, 12(9):101–105, 2023.\n40Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan\nZhang, David Wipf, and Zheng Zhang. Cyclegt:\nUnsupervised graph-to-text and text-to-graph gen-\neration\nvia\ncycle\ntraining.\narXiv\npreprint\narXiv:2006.04702, 2020.\n41Alex R Hardisty, Elizabeth R Ellwood, Gil Nelson,\nBreda Zimkus, Jutta Buschbom, Wouter Addink,\nRichard K Rabeler, John Bates, Andrew Bent-\nley, José AB Fortes, et al. Digital extended spe-\ncimens: Enabling an extensible network of biod-\niversity data records as integrated digital objects\non the internet. BioScience, 72(10):978–987, 2022.\n42Frederick Hayes-Roth, Donald A Waterman, and\nDouglas\nB\nLenat.\nBuilding\nexpert\nsystems.\nAddison-Wesley Longman Publishing Co., Inc.,\n1983.\n43James Hendler, Fabien Gandon, and Dean Alle-\nmang.\nSemantic web for the working ontologist:\nEffective modeling for linked data, RDFS, and\nOWL. Morgan & Claypool, 2020.\n44Birger Hjørland. What is knowledge organization\n(ko)?\nKO Knowledge Organization, 35(2-3):86–\n101, 2008.\n45Aidan Hogan, Eva Blomqvist, Michael Cochez,\nClaudia d’Amato, Gerard de Melo, Claudio Gu-\ntierrez, Sabrina Kirrane, José Emilio Labra Gayo,\nRoberto Navigli, Sebastian Neumaier, et al. Know-\nledge graphs. ACM Computing Surveys (CSUR),\n54(4):1–37, 2021.\n46Madelon Hulsebos, Kevin Hu, Michiel Bakker,\nEmanuel Zgraggen, Arvind Satyanarayan, Tim\nKraska, Çagatay Demiralp, and César Hidalgo.\nSherlock: A deep learning approach to semantic\ndata\ntype\ndetection.\nIn\nProceedings\nof\nthe\n25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 1500–\n1508, 2019.\n47Naman Jain,\nSkanda Vaidyanath,\nArun Iyer,\nNagarajan Natarajan, Suresh Parthasarathy, Sri-\nram Rajamani, and Rahul Sharma. Jigsaw: Large\nlanguage models meet program synthesis. In Pro-\nceedings of the 44th International Conference on\nSoftware Engineering, pages 1219–1231, 2022.\nB. P. Allen, L. Stork, P. Groth\nXX:17\n48Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, An-\ndrea Madotto, and Pascale Fung. Survey of hal-\nlucination in natural language generation.\nACM\nComputing Surveys, 55(12):1–38, 2023.\n49Daniel Kahneman. Thinking, fast and slow. mac-\nmillan, 2011.\n50Paschalia Kapli, Ziheng Yang, and Maximilian J\nTelford.\nPhylogenetic tree building in the gen-\nomic age.\nNature Reviews Genetics, 21(7):428–\n444, 2020.\n51Elisa F Kendall and Deborah L McGuinness. On-\ntology engineering. Morgan & Claypool Publishers,\n2019.\n52Takeshi Kojima, Shixiang Shane Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213, 2022.\n53Keti\nKorini\nand\nChristian\nBizer.\nColumn\ntype annotation using chatgpt.\narXiv preprint\narXiv:2306.00745, 2023.\n54Tiffany\nH\nKung,\nMorgan\nCheatham,\nArielle\nMedenilla, Czarina Sillos, Lorie De Leon, Camille\nElepaño, Maria Madriaga, Rimel Aggabao, Giezel\nDiaz-Candido, James Maningo, et al. Performance\nof chatgpt on usmle: Potential for ai-assisted med-\nical education using large language models. PLoS\ndigital health, 2(2):e0000198, 2023.\n55Himabindu Lakkaraju, Dylan Slack, Yuxin Chen,\nChenhao Tan,\nand Sameer Singh.\nRethink-\ning\nexplainability\nas\na\ndialogue:\nA\npracti-\ntioner’s\nperspective.\nCoRR,\nabs/2202.01875,\n2022. URL: https://arxiv.org/abs/2202.01875,\narXiv:2202.01875.\n56Wanhae Lee, Minki Chun, Hyeonhak Jeong, and\nHyunggu\nJung.\nToward\nkeyword\ngeneration\nthrough large language models.\nIn Companion\nProceedings of the 28th International Conference\non Intelligent User Interfaces, IUI ’23 Compan-\nion, page 37–40, New York, NY, USA, 2023. As-\nsociation for Computing Machinery. doi:10.1145/\n3581754.3584126.\n57Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\nJiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey\nof prompting methods in natural language pro-\ncessing.\nACM Computing Surveys, 55(9):1–35,\n2023.\n58Michela Lorandi and Anya Belz.\nData-to-text\ngeneration for severely under-resourced languages\nwith gpt-3.5:\nA bit of help needed from google\ntranslate. arXiv preprint arXiv:2308.09957, 2023.\n59Arthur MacGregor. Naturalists in the field: col-\nlecting, recording and preserving the natural world\nfrom the fifteenth to the twenty-first century,\nvolume 2. Brill, 2018.\n60Kyle Mahowald, Anna A Ivanova, Idan A Blank,\nNancy Kanwisher,\nJoshua B Tenenbaum,\nand\nEvelina Fedorenko.\nDissociating language and\nthought in large language models: a cognitive per-\nspective. arXiv preprint arXiv:2301.06627, 2023.\n61Jose L. Martinez-Rodriguez, Aidan Hogan, and\nIvan Lopez-Arevalo. Information extraction meets\nthe semantic web:\nA survey.\nSemantic Web,\n11(2):255–335, February 2020.\ndoi:10.3233/sw-\n180333.\n62Richard Menary. Writing as thinking. Language\nsciences, 29(5):621–632, 2007.\n63Richard Menary. Dimensions of mind. Phenomen-\nology and the Cognitive Sciences, 9:561–578, 2010.\n64Hugo Mercier and Dan Sperber.\nThe enigma of\nreason. Harvard University Press, 2017.\n65Grégoire Mialon, Roberto Dessì, Maria Lomeli,\nChristoforos Nalmpantis, Ram Pasunuru, Roberta\nRaileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, et al.\nAugmen-\nted language models:\na survey.\narXiv preprint\narXiv:2302.07842, 2023.\n66Bonan Min,\nHayley Ross,\nElior Sulem,\nAmir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar\nSainz, Eneko Agirre, Ilana Heintz, and Dan Roth.\nRecent advances in natural language processing\nvia large pre-trained language models: A survey.\nACM Comput. Surv., jun 2023.\nJust Accepted.\ndoi:10.1145/3605943.\n67Staffan Müller-Wille. Names and numbers:“data”\nin classical natural history, 1758–1859.\nOsiris,\n32(1):109–128, 2017.\n68Zara\nNasar,\nSyed\nWaqar\nJaffry,\nand\nMuhammad\nKamran\nMalik.\nNamed\nentity\nrecognition\nand\nrelation\nextraction:\nState-of-\nthe-art.\nACM\nComputing\nSurveys\n(CSUR),\n54(1):1–39, 2021.\n69Catarina Dutilh Novaes. Formal languages in lo-\ngic: A philosophical and cognitive analysis. Cam-\nbridge University Press, 2012.\n70Catarina Dutilh Novaes.\nThe dialogical roots of\ndeduction: Historical, cognitive, and philosophical\nperspectives on reasoning. Cambridge University\nPress, 2020.\n71Alexandra Ortolja-Baird and Julianne Nyhan. En-\ncoding the haunting of an object catalogue: on the\npotential of digital technologies to perpetuate or\nsubvert the silence and bias of the early-modern\narchive.\nDigital Scholarship in the Humanities,\n37(3):844–867, 2022.\n72Roderic DM Page. Biodiversity informatics: the\nchallenge of linking data and the role of shared\nidentifiers. Briefings in bioinformatics, 9(5):345–\n354, 2008.\n73Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen,\nJiapu Wang, and Xindong Wu. Unifying large lan-\nguage models and knowledge graphs: A roadmap.\narXiv preprint arXiv:2306.08302, 2023.\n74Joon Sung Park, Joseph C O’Brien, Carrie J Cai,\nMeredith Ringel Morris, Percy Liang, and Mi-\nchael S Bernstein.\nGenerative agents:\nInteract-\nive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\n75Fabio Petroni, Tim Rocktäschel, Patrick Lewis,\nAnton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as know-\nledge bases?\narXiv preprint arXiv:1909.01066,\n2019.\n76Valentina Presutti, Enrico Daga, Aldo Gangemi,\nand Eva Blomqvist. extreme design with content\nontology design patterns. In Proc. Workshop on\nOntology Patterns, pages 83–97, 2009.\nTGDK\nXX:18\nKnowledge Engineering using Large Language Models\n77Alec Radford, Jong Wook Kim, Chris Hallacy,\nAditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al.\nLearning transferable visual\nmodels from natural language supervision. In In-\nternational conference on machine learning, pages\n8748–8763. PMLR, 2021.\n78Alec Radford, Jong Wook Kim, Tao Xu, Greg\nBrockman, Christine McLeavey, and Ilya Sut-\nskever. Robust speech recognition via large-scale\nweak supervision.\nIn International Conference\non Machine Learning, pages 28492–28518. PMLR,\n2023.\n79Aditya Ramesh, Prafulla Dhariwal, Alex Nichol,\nCasey Chu, and Mark Chen.\nHierarchical text-\nconditional image generation with clip latents.\narXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n80Laria Reynolds and Kyle McDonell. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, pages 1–7, 2021.\n81Tim Robertson, Markus Döring, Robert Gural-\nnick, David Bloom, John Wieczorek, Kyle Braak,\nJavier Otegui, Laura Russell, and Peter Desmet.\nThe gbif integrated publishing toolkit: facilitating\nthe efficient publishing of biodiversity data on the\ninternet. PloS one, 9(8):e102623, 2014.\n82Víctor\nRodríguez-Doncel\nand\nElena\nMontiel-\nPonsoda. Lynx: Towards a legal knowledge graph\nfor multilingual europe.\nLaw Context: A Socio-\nLegal J., 37:175, 2020.\n83Robin Rombach, Andreas Blattmann, Dominik\nLorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion\nmodels. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition,\npages 10684–10695, 2022.\n84Timo\nSchick,\nJane\nDwivedi-Yu,\nRoberto\nDessì,\nRoberta Raileanu,\nMaria Lomeli,\nLuke\nZettlemoyer,\nNicola\nCancedda,\nand\nThomas\nScialom. Toolformer: Language models can teach\nthemselves to use tools, 2023. arXiv:2302.04761,\ndoi:10.48550/arXiv.2302.04761.\n85Guus Schreiber. Knowledge engineering. Founda-\ntions of Artificial Intelligence, 3:929–946, 2008.\n86Guus Schreiber, Hans Akkermans, Anjo Anjewi-\nerden, Nigel Shadbolt, Robert de Hoog, Walter\nVan de Velde, and Bob Wielinga. Knowledge en-\ngineering and management:\nthe CommonKADS\nmethodology. MIT press, 2000.\n87Guus\nSchreiber\nand\nLora\nAroyo.\nPrinciples\nfor\nknowledge\nengineering\non\nthe\nweb.\nIn\nAAAI Spring Symposium:\nSymbiotic Relation-\nships between Semantic Web and Knowledge En-\ngineering, pages 78–82, 2008.\n88Nigel R Shadbolt, Paul R Smart, J Wilson, and\nS Sharples. Knowledge elicitation. Evaluation of\nhuman work, pages 163–200, 2015.\n89Murray Shanahan. Talking about large language\nmodels. arXiv preprint arXiv:2212.03551, 2022.\n90Steffen Staab and Rudi Studer. Handbook on on-\ntologies. Springer Science & Business Media, 2010.\n91Lise Stork.\nKnowledge extraction from archives\nof natural history collections. PhD thesis, Ph. D.\nDissertation, Leiden University, 2021.\n92Lise Stork, Andreas Weber, Eulàlia Gassó Miracle,\nFons Verbeek, Aske Plaat, Jaap van den Herik,\nand Katherine Wolstencroft. Semantic annotation\nof natural history collections. Journal of Web Se-\nmantics, 59:100462, 2019.\n93Rudi Studer, V Richard Benjamins, and Dieter\nFensel.\nKnowledge engineering:\nPrinciples and\nmethods.\nData & knowledge engineering, 25(1-\n2):161–197, 1998.\ndoi:10.1016/S0169-023X(97)\n00056-6.\n94Mari Carmen Suárez-Figueroa, Asunción Gómez-\nPérez, and Mariano Fernández-López. The neon\nmethodology for ontology engineering.\nIn Onto-\nlogy engineering in a networked world, pages 9–34.\nSpringer, 2011.\n95Mari Carmen Suárez-Figueroa, Asunción Gómez-\nPérez, Enrico Motta, and Aldo Gangemi. Introduc-\ntion: Ontology engineering in a networked world.\nSpringer, 2012.\n96Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Se-\nquence to sequence learning with neural networks.\nAdvances in neural information processing sys-\ntems, 27, 2014.\n97Anders Telenius.\nBiodiversity information goes\npublic: Gbif at your service.\nNordic Journal of\nBotany, 29(3):378–381, 2011.\n98Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert\nrediscovers the classical nlp pipeline.\narXiv pre-\nprint arXiv:1905.05950, 2019.\n99Ilaria Tiddi and Stefan Schlobach.\nKnowledge\ngraphs as tools for explainable machine learning:\nA survey. Artificial Intelligence, 302:103627, 2022.\n100Priyan Vaithilingam, Tianyi Zhang, and Elena L\nGlassman.\nExpectation vs. experience: Evaluat-\ning the usability of code generation tools powered\nby large language models.\nIn Chi conference on\nhuman factors in computing systems extended ab-\nstracts, pages 1–7, 2022.\n101Michael van Bekkum, Maaike de Boer, Frank van\nHarmelen, André Meyer-Vitali, and Annette ten\nTeije. Modular design patterns for hybrid learning\nand reasoning systems: a taxonomy, patterns and\nuse cases.\nApplied Intelligence, 51(9):6528–6546,\n2021.\n102M.G.J. van Erp. Accessing natural history: Dis-\ncoveries in data cleaning, structuring, and re-\ntrieval.\nPhD thesis, Tilburg University, 2010.\nSeries: TiCC Ph.D. Series Volume: 13.\n103Frank Van Harmelen and Annette ten Teije.\nA\nboxology of design patterns for hybrid learn-\ning\nand\nreasoning\nsystems.\narXiv\npreprint\narXiv:1905.12389, 2019.\n104Ashish Vaswani,\nNoam Shazeer,\nNiki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is\nall you need. Advances in neural information pro-\ncessing systems, 30, 2017.\n105Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang\nZhang, Long Zhou, Shujie Liu, Zhuo Chen, Yan-\nqing Liu, Huaming Wang, Jinyu Li, et al. Neural\ncodec language models are zero-shot text to speech\nsynthesizers.\narXiv preprint arXiv:2301.02111,\n2023.\n106Haohan Wang and Bhiksha Raj. On the origin of\ndeep learning.\narXiv preprint arXiv:1702.07800,\n2017.\nB. P. Allen, L. Stork, P. Groth\nXX:19\n107Andreas\nWeber,\nMahya\nAmeryan,\nKatherine\nWolstencroft, Lise Stork, Maarten Heerlien, and\nLambert Schomaker. Towards a digital infrastruc-\nture for illustrated handwritten archives. In Digital\nCultural Heritage: Final Conference of the Marie\nSkłodowska-Curie Initial Training Network for Di-\ngital Cultural Heritage, ITN-DCH 2017, Olimje,\nSlovenia, May 23–25, 2017, Revised Selected Pa-\npers, pages 155–166. Springer, 2018.\n108Jason\nWei,\nXuezhi\nWang,\nDale\nSchuurmans,\nMaarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al.\nChain-of-thought prompting\nelicits reasoning in large language models.\nAd-\nvances in Neural Information Processing Systems,\n35:24824–24837, 2022.\n109Justin D Weisz, Michael Muller, Jessica He, and\nStephanie Houde.\nToward general design prin-\nciples for generative ai applications. arXiv preprint\narXiv:2301.05578, 2023.\n110Jules White, Sam Hays, Quchen Fu, Jesse Spencer-\nSmith, and Douglas C. Schmidt. Chatgpt prompt\npatterns for improving code quality,\nrefactor-\ning, requirements elicitation, and software design,\n2023.\narXiv:2303.07839, doi:10.48550/arXiv.\n2303.07839.\n111Bob J Wielinga, A Th Schreiber, and Jost A\nBreuker.\nKads: A modelling approach to know-\nledge engineering. Knowledge acquisition, 4(1):5–\n53, 1992.\n112Mark\nD.\nWilkinson,\nMichel\nDumontier,\nIJs-\nbrand\nJan\nAalbersberg,\nGabrielle\nAppleton,\nMyles Axton, Arie Baak, Niklas Blomberg, Jan-\nWillem Boiten,\nLuiz Bonino da Silva Santos,\nPhilip E. Bourne, Jildau Bouwman, Anthony J.\nBrookes, Tim Clark, Mercè Crosas, Ingrid Dillo,\nOlivier Dumon, Scott Edmunds, Chris T. Evelo,\nRichard\nFinkers,\nAlejandra\nGonzalez-Beltran,\nAlasdair J.G. Gray, Paul Groth, Carole Goble, Jef-\nfrey S. Grethe, Jaap Heringa, Peter A.C ’t Hoen,\nRob Hooft,\nTobias Kuhn,\nRuben Kok,\nJoost\nKok, Scott J. Lusher, Maryann E. Martone, Al-\nbert Mons, Abel L. Packer, Bengt Persson, Phil-\nippe Rocca-Serra, Marco Roos, Rene van Schaik,\nSusanna-Assunta Sansone, Erik Schultes, Thierry\nSengstag, Ted Slater, George Strawn, Morris A.\nSwertz, Mark Thompson, Johan van der Lei, Erik\nvan Mulligen, Jan Velterop, Andra Waagmeester,\nPeter Wittenburg, Katherine Wolstencroft, Jun\nZhao, and Barend Mons. The FAIR guiding prin-\nciples for scientific data management and steward-\nship.\nScientific Data, 3(1), March 2016.\ndoi:\n10.1038/sdata.2016.18.\n113Lionel Wong, Gabriel Grand, Alexander K Lew,\nNoah D Goodman, Vikash K Mansinghka, Jacob\nAndreas, and Joshua B Tenenbaum. From word\nmodels to world models: Translating from natural\nlanguage to the probabilistic language of thought.\narXiv preprint arXiv:2306.12672, 2023.\n114Qianqian\nXie,\nJennifer\nAmy\nBishop,\nPrayag\nTiwari, and Sophia Ananiadou.\nPre-trained lan-\nguage models with domain knowledge for biomed-\nical extractive summarization.\nKnowledge-Based\nSystems, 252:109460, 2022.\n115Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, and\nErik Cambria.\nLogical reasoning over natural\nlanguage as knowledge representation: A survey,\n2023.\narXiv:2303.12023, doi:10.48550/arXiv.\n2303.12023.\n116Kang Min Yoo, Dongju Park, Jaewook Kang,\nSang-Woo Lee, and Woomyeong Park. Gpt3mix:\nLeveraging large-scale language models for text\naugmentation. arXiv preprint arXiv:2104.08826,\n2021.\n117Jiahui\nYu,\nZirui\nWang,\nVijay\nVasudevan,\nLegg\nYeung,\nMojtaba\nSeyedhosseini,\nand\nYonghui\nWu.\nCoca:\nContrastive\ncaptioners\nare image-text foundation models. arXiv preprint\narXiv:2205.01917, 2022.\n118Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia\nGeng, Yufeng Huang, Yajing Xu, Wenting Song,\nand Huajun Chen.\nStructure pretraining and\nprompt tuning for knowledge graph transfer.\nIn\nProceedings of the ACM Web Conference 2023,\nWWW ’23, page 2581–2590, New York, NY, USA,\n2023. Association for Computing Machinery. doi:\n10.1145/3543507.3583301.\n119Kaiyang Zhou, Jingkang Yang, Chen Change Loy,\nand Ziwei Liu.\nLearning to prompt for vision-\nlanguage models. International Journal of Com-\nputer Vision, 130(9):2337–2348, 2022.\nTGDK\n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2023-10-01",
  "updated": "2023-10-01"
}