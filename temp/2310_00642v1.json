{
  "id": "http://arxiv.org/abs/2310.00642v1",
  "title": "From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning with Contextual Information",
  "authors": [
    "Zhendong Shi",
    "Xiaoli Wei",
    "Ercan E. Kuruoglu"
  ],
  "abstract": "The problem of how to take the right actions to make profits in sequential\nprocess continues to be difficult due to the quick dynamics and a significant\namount of uncertainty in many application scenarios. In such complicated\nenvironments, reinforcement learning (RL), a reward-oriented strategy for\noptimum control, has emerged as a potential technique to address this strategic\ndecision-making issue. However, reinforcement learning also has some\nshortcomings that make it unsuitable for solving many financial problems,\nexcessive resource consumption, and inability to quickly obtain optimal\nsolutions, making it unsuitable for quantitative trading markets. In this\nstudy, we use two methods to overcome the issue with contextual information:\ncontextual Thompson sampling and reinforcement learning under supervision which\ncan accelerate the iterations in search of the best answer. In order to\ninvestigate strategic trading in quantitative markets, we merged the earlier\nfinancial trading strategy known as constant proportion portfolio insurance\n(CPPI) into deep deterministic policy gradient (DDPG). The experimental results\nshow that both methods can accelerate the progress of reinforcement learning to\nobtain the optimal solution.",
  "text": "From Bandits Model to Deep Deterministic Policy\nGradient, Reinforcement Learning with\nContextual Information\nZhendong Shi1*, Ercan Kuruo˘glu1*† and Xiaoli Wei1†\n1Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen,\nChina.\n*Corresponding author(s). E-mail(s): shizd20@mails.tsinghua.edu.cn;\nkuruoglu@sz.tsinghua.edu.cn;\nContributing authors: xiaoli wei@sz.tsinghua.edu.cn;\n†These authors contributed equally to this work.\nAbstract\nThe problem of how to take the right actions to make profits in sequential pro-\ncess continues to be difficult due to the quick dynamics and a significant amount\nof uncertainty in many application scenarios. In such complicated environments,\nreinforcement learning (RL), a reward-oriented strategy for optimum control, has\nemerged as a potential technique to address this strategic decision-making issue.\nHowever, reinforcement learning also has some shortcomings that make it unsuit-\nable for solving many financial problems, excessive resource consumption, and\ninability to quickly obtain optimal solutions, making it unsuitable for quantita-\ntive trading markets. In this study, we use two methods to overcome the issue\nwith contextual information: contextual Thompson sampling and reinforcement\nlearning under supervision which can accelerate the iterations in search of the\nbest answer. In order to investigate strategic trading in quantitative markets,\nwe merged the earlier financial trading strategy known as constant proportion\nportfolio insurance (CPPI) into deep deterministic policy gradient (DDPG). The\nexperimental results show that both methods can accelerate the progress of\nreinforcement learning to obtain the optimal solution.\nKeywords: contextual information, Thompson sampling, reinforcement learning,\nconstant proportion portfolio insurance\n1\narXiv:2310.00642v1  [cs.LG]  1 Oct 2023\n1 Introduction\nCompared with traditional trading methods, quantitative trading is widely known\nfor its features of high-frequency, algorithmic, and automated trading, which is difficult\nto achieve by human beings in complex and dynamic stock market [11, 30]. In the\nquantitative market, massive noisy signals from stochastic trading behaviors and all\nkinds of unforeseeable social events make it difficult to predict the market state [15, 23].\nHuman traders can easily be affected by these events which would make the irrational\ndecisions of trading nearly inevitable [13, 24]. Therefore, different financial individuals\nand institutes from different research fields have started to explore more effective ways\nfor handling these problems.\nDubey and Pentland [12] proposed an algorithm for the MAB problem based on\nthe symmetric α-stable distribution [27]. The approach demonstrated success through\naccurate assumptions and a normalized iterative process. The α-stable distribution is\na family of distributions characterized by heavy tails.\nMotivated by the presence of asymmetric characteristics in various real life data\n[20] and the success in reinforcement learning and other directions due to the introduc-\ntion of asymmetry [4], in previous work, Shi et al. [28] propose a statistic model, for\nwhich the reward distribution is both heavy-tailed and asymmetric, named asymmetric\nalpha-Thompson sampling algorithm. With the theorems for heavy-tailed distribu-\ntion [6], variance analysis [8] and estimation methods for both symmetric and skewed\nα-stable distribution [19], Shi et al. [28] analyse the performances of symmetric α-\nThompson sampling algorithm and symmetric α-Thompson sampling algorithm by\nBayesian regret bound. In the general sequential decision making algorithm, asymmet-\nric information[16] is regraded as a single parameter derived from interdependence,\ncommon knowledge, higher order beliefs and so on. Our algorithm differs from other\nalgorithms in that it assumes that the reward follows an asymmetric distribution, while\nothers changing the their structures or using expert supervision to handle asymmetric\ninformation. Therefore, the range of our application is wider and more flexible.\nOver the past years, with the development of artificial intelligence techniques,\nreinforcement learning (RL) has emerged as an efficient method for making decisions in\ndynamic environments with uncertainties [2]. The principle behind RL is the Markov\ndecision process (MDP). Through interacting with the environment, the RL agent,\ni.e. the decision maker, will iteratively update its strategy according to the rewards,\nwhich can be treated as guidance toward the expected target and the goal of the RL\nagent is hence to maximize the total reward [29]. Following the MDP, researchers from\nfinancial fields have tried to build their own specifically designed RL architecture to\ncope with different financial problems. A deep RL method combined with knowledge\ndistillation was proposed to improve the training reliability in the trading of currency\npairs [31]. To investigate the stock portfolio selection problem, a hypergraph-based RL\nmethod was designed to learn the policy function of generating appropriate trading\nactions [21]. Besides, a policy-based RL framework for stock portfolio management was\nintroduced and its performance was also compared with other trading strategies [32].\nThe advantage of reinforcement learning in the financial field lies in its ability to\ncope with very complex market environments and uncertainties, adapt to constantly\nchanging market conditions, and improve trading efficiency and profitability through\n2\ncontinuous learning and adjustment of strategies. However, reinforcement learning\nalso has limitations, such as requiring a large amount of computing resources and\ndata, requiring good model and algorithm design, as well as stable data sources. In\naddition, due to the complexity of the financial market itself, reinforcement learning\nalso faces many challenges in practice, such as overfitting, data sparsity and other\nproblems, which need to be comprehensively considered and optimized in combination\nwith practical application scenarios.\nIn this study, we use two methods to overcome the issue with contextual informa-\ntion: contextual Thompson sampling and a specific method to speed up reinforcement\nlearning iterations in search of the best answer. In order to investigate strategic trad-\ning in quantitative markets, we merged the earlier financial trading strategy known as\nconstant proportion portfolio insurance (CPPI) into deep deterministic policy gradi-\nent (DDPG) for investigating strategic trading in quantitative markets, respectively\nfor studying how this novel architecture will behave in quantitative markets.\nThe Bandits model is a simplified reinforcement learning algorithm that has been\nwidely applied in the financial field. By adjusting the distribution function to be\nheavy-tailed distribution and adding contextual information, we can make the bandits\nmodel more suitable for dealing with specific financial problems. Proportion portfolio\ninsurance is a specific financial strategy designed for individuals with different risk\npreferences.\nThe rest of this work is organized as follows. Section 2 introduces the background\nknowledge, in which we discuss the similarities and differences between the contextual\nbandits model and reinforcement learning. The algorithms represent how contextual\nbandits work and how the DDPG specifically combined with CPPI strategy are shown\nin Section 3. Further, the numerical experiment and results are implemented and\nanalyzed in Section 4. Finally, the conclusions are drawn in Section 5.\n2 Background Knowledge\nIn this section, we map the evolution from the bandits model to reinforcement\nlearning and the similarities and differences between various algorithms.\n2.1 Multi-Armed Bandit Problem\nAssume that an agent has a choice of multiple slot machines from which to choose\nfor each round one’s draw and recording of the payouts. If no two slot machines are\nprecisely alike, we can gather some statistical data from each machine after several\nspins of the operations, and then choose the one that offers the largest projected\npayout.\nThe process of learning is indexed by t ∈[T]. The entire number of rounds, denoted\nby the symbol T, is known beforehand. The agent selects an arm from at ∈[N], and\nin each round of t ∈[T], rat(t) is observed from that arm. Rewards are distributed\nindividually for each arm n ∈[N] from a distribution Dn with a mean µn = EDn[r].\nThe optimal arm(s) is(are) denoted as n∗, and the associated arm(s) is(are) denoted\nas the biggest expected reward, µ⋆= maxn ∈[N] µn.\n3\nFig. 1 From bandits model to reinforcement learning\nThe regret R(T), which measures the discrepancy between the ideal total reward\nan agent can obtain and the total reward they actually receive, is used to measure\nperformance.\nR(T) = µ⋆T −\nT\nX\nt=0\nµat.\n(1)\n2.2 Thompson Sampling Algorithm for Multi-Armed Bandit\nProblem\nA variety of exploration algorithms have been proposed, including ϵ-greedy algo-\nrithm, UCB algorithm and Thompson sampling. ϵ-greedy algorithm[18] makes use of\nboth exploitations to take advantage of prior knowledge and exploration in order to\nsearch for new solutions, whereas the UCB algorithm[7] simply draws the arm that\nhas the largest empirical estimate of reward up to that point plus some term that is\ninversely related to the number of times the arm was played.\nUnder the assumption that for each arm n ∈[N], the reward distribution is\nDn parametrized by θn ∈Θ (µn may not be an appropriate parameter) and that\nthe parameter has a prior probability distribution p (θn). The reward distribution is\ndefined as follows: Thompson’s sampling algorithm updates the prior distribution of\nθn as a function of the observed reward of n, and then chooses the arm according to\nthe posterior probability derived from the reward under the arm n.\nThrough Bayes rule,\np(θ|x) = p(x|θ)p(θ)\np(x)\n=\np(x|θ)p(θ)\nR\np(x|θ\n′)p(θ\n′)dθ\n′.\n(2)\nwhere θ is the parameter and x is the observation. p(θ|x) is the posterior distribution,\np(x|θ) is likelihood function, p(θ) is the prior distribution.\nIn each round, t ∈[T], the agent draws the ˆn(t) parameter for each arm n ∈[N]\naccording the posterior distribution of the parameters given the prior rewards up to\n4\ntime t, rrrn(t −1) = {r(1)\nn , r(2)\nn , · · · , r(kn(t−1))\nn\n}, where kn(t) is how many times the arm\nn has been pulled up at time t:\nˆθn(t) ∼p(θn|rrrn(t −1)) ∝p(rrrn(t −1)|θn)p(θn).\n(3)\nThrough the parameters ˆθn (t) drawn from each arm, the agent chooses the arm at\nwith the highest mean return from the posterior distribution, receives the return rat.\nat = arg max\nn∈[N]\nµn(θn(t))\n(4)\nIn order to make a comparison with the symmetric case, we will use the Bayesian\nRegret [25] for the performance measure. The estimated regret with respect to the\npriors is Bayesian Regret (BR). Denoting the parameters on the set of arms as θ =\n{θ1, ..., θN} and their corresponding product distribution as D = Q\ni Di, the Bayesian\nRegret is expressed in the following way.\nBR(T, π) = Eθ∼D[R(T)]\n(5)\n2.3 Alpha-Stable Distribution\nIn many application scenarios, binary distribution and Normal distribution cannot\naccurately show the characteristics of the data set. The α-stable distribution is a\ntype of probability distribution that has a wide range of applications in various areas\nsuch as finance and signal processing. In finance, the α-stable distribution is used to\nmodel fluctuations in asset prices and returns. It is particularly useful in modeling\nextreme events, such as stock market crashes or sudden changes in currency exchange\nrates. In signal processing, the α-stable distribution is used for modeling noise in\ncommunication systems. It helps to understand how different types of interference can\naffect the quality of signal transmission.\nThe alpha-stable distribution is an important non-Gaussian distribution that is\noften used to model both impulsive and skewed data. It has a non-analytic density\nand therefore, usually is described with the characteristic function. We say a random\nvariable X is Sα(β, δ, σ) if X has characteristic function:\nE[eiuX] = exp(−σα |u|α (1 + iβsign(u)\n(|σu|1−α −1)) + iuδ)\n(6)\nα is used to indicate the impulsiveness of the distribution, parameter β corresponds\nto the skewness, γ is the scale parameter and µ is the mean, which is closely related\nwith the location parameter delta via µ = δ −βσ tan(πα/2). The shape parameter α\nmust be in the interval (0, 2]. When α > 1, the mean of the distribution exists and is\nequal to µ, so this paper only concentrates on the interval α ∈(1,2) since the stable\ndistribution will degenerate to Gaussian distribution when α is equal to 2.\nIf E[X2] = ∞, then X or its distribution has a heavy tail. X or its distribution\nhas a heavy tail of order α, where α ∈(1, 2), if limt→∞(|t|α)P(|X| > t) = C < ∞,\nE|X|p < ∞⇐⇒p < α. When large and unexpected jumps occur between several\n5\nFig. 2 Stable distribution\nrelatively small observations, it is reasonable to suspect a heavy tail distribution.\nStable distributions are as fundamental for heavy tail models as Gaussian distributions\nare for distributions with a finite second order moment.\n2.4 Thompson Sampling Algorithm with Alpha-Stable\nDistribution\nThe Thompson Sampling (TS) method received attention for its strong empirical\nevidence. This has led to more research on the algorithm’s theoretical analysis from\na Bayesian standpoint. Specifically, Russo[26] demonstrated the optimality of TS for\nGaussian reward distributions. Korda[17] later expanded on these findings to include\na wider range of exponential family reward distributions.\nThe family of symmetric α-stable distributions, known for their extremely heavy\ntails, were studied by Dubey and Pentland[12] in Thompson Sampling. The study\nobtained the first polynomial regret bounds, independent of the problem, for Thomp-\nson Sampling when using symmetric α-stable densities.\nOn the basis of Dubey and Pentland’s work, Shi et al. [28] extended the algorithm\nto α-stable distributions through Gibbs Sampler and updating formula for α, β, σ, δ.\nCompared with symmetric α-Thompson algorithm, asymmetric α-Thompson algo-\nrithm can not only cover the asymmetry in data (which is very common in social\ndata), but also greatly improve the accuracy of reward distribution assumptions by\niterating on the four parameters of α stable distribution.\nWith the theorems for heavy-tailed distribution[6] and estimation methods for\nboth symmetric and skewed α-stable distribution[19], Shi et al. [28] analyse the perfor-\nmances of symmetric α-Thompson sampling algorithm and symmetric α-Thompson\nsampling algorithm by Bayesian regret bound.\nThe subsequent algorithms in this paper are based on asymmetric α-Thompson\nalgorithm and are extensions of this algorithm.\n2.5 Contextual Information\nContextual data refers to information that offers a perspective on an event, person,\nor thing by revealing how different pieces of data interrelate, resulting in a more\n6\ncomprehensive understanding of the subject. Such pertinent facts can be utilized to\nanalyze behavior patterns, optimizing user experience.\nFor instance, companies may analyze sales data and incorporate information about\ntraffic or weather conditions to gain a deeper understanding of the factors that impact\ntheir sales. In the realm of big data, information without context often lacks practical\nvalue. By incorporating contextual information, organizations can make more informed\nand accurate decisions at a higher level.\nAs for the value in the algorithm, we use context data to show the action mode\nwhen it is too complex to be covered by a single set of data. Many factors and their\nrelationships need to be considered for description of the action mode of these things,\nwhich is the dimension upgrading processing of simple data.\n2.6 Thompson Sampling for Contextual Bandits\nThe Contextual Multi-Armed Bandit (CMAB), also known as the Contextual Ban-\ndit, is a useful variation of the multi-armed bandit problem. This scenario assumes\nthat the agent observes an N-dimensional context or feature vector prior to selecting\nan arm at each iteration. In other words, the goal of the learner in CMAB is not only\nto maximize reward but also to learn the relationship between the feature vectors and\nthe rewards, in order to make better decisions in the future. This is especially useful\nin real world applications such as personalized recommendations, where the context\nmay be the user’s past behaviour, interests and demographics.\nSome recent studies have also considered the non-linear relationship between the\nactions’ expected reward and their contexts, such as Neural Bandit, LINUCB [9],\nand Contextual Thompson Sampling [1]. These algorithms typically assume a linear\nrelationship between an action’s expected reward and its context.\nCMAB algorithms have found application in a variety of real-world problem set-\ntings, including healthcare, computer network routing, and finance, among others.\nCMAB approaches are also useful in computer science and machine learning for hyper-\nparameter tuning and algorithmic choices in supervised learning and reinforcement\nlearning.\nIn multi-armed bandit (MAB) problems, a random reward ri(t) with an unknown\nmean θi(t) is assumed. In the contextual MAB problem, it is assumed that θi(t) =\nθtbi(t), where θi(·) is an arbitrary function and bi(t) is a context vector. Specifically,\nit’s a Linear contextual MAB problems if we assume that θtbi(t) is linear in bi(t):\nθtbi(t) = bi(t)T µ, i = 1, ..., N\n(7)\nThe key advantages of linear contextual bandits is their simplicity and efficiency.\nThey can handle large datasets with many features and do not require complicated\noptimization techniques. Additionally, they can operate with incomplete information,\nwhich is useful when data is missing or incomplete.\n2.7 Non-Linear Contextual Bandits\nIn contextual bandits, a non-linear function is used to model the relationship\nbetween the context and the expected reward, rather than a linear function. However,\n7\nthe function is only partially specified, which allows for flexibility and adaptation to\ndifferent contexts. The parameters of the function are learned from data through a\nprocess of trial and error, using a limited subset of the available context information[3].\nθtbi(t) = bi(t)T µ + v(t), i = 1, ..., N\n(8)\nIf v(t) = 0, the contextual multi-armed bandit (MAB) problem reduces to a linear\ncontextual MAB. When v(t) is not zero but unrelated to the action, it is referred to\nas a semi-parametric MAB problem.\nIf v(t) depends on the action, then the reward distribution becomes completely\nnon-parametric, and the problem is defined as adversarial. In adversarial contextual\nbandits[3], an “adversary” is introduced to the environment and tries to actively\nmanipulate the reward signals received by the agent. Under changing conditions, the\nagent must learn a policy that is robust to the adversary’s manipulations. This is often\nachieved through a process of exploration and adaptation, where the agent continually\nadjusts its behavior to minimize the impact of the adversary’s actions.\nContextual adversarial bandits are simplified versions of complete reinforcement\nlearning problems with n arms. The adversarial contextual bandits[14] avoid the com-\nplexity of full RL by teaching the agent to act in only one situation with n different\npossible actions or options.\n2.8 Deep Deterministic Policy Gradient(DDPG)\nA sequential decision-making problem in the scenario can be described as a stochas-\ntic game, which can be defined by a set of key elements S, A, P, R ,γ⟩. At time t,\nunder a shared state St ∈S, each agent takes its action a ∈A simultaneously. The\naction a leads the environment changes according to the dynamics P : S ×AAA →∆(S).\nAfter that each agent receives its individual reward ri according to its reward function\nRi : S × AAA × S →R. γ is the discount factor that represents the value of time.\nIn our setting, we have one agent trading in quantitative markets using different\nstrategies. Its goal is to maximize individual return while keeping a certain degree\nof diversity among the portfolios managed by each agent since we want to allocate\nrisks. The state space shared by our agents is the raw historical closing prices of all\nstocks. At each step, our agent will output an action of choosing several stocks with\nthe amount of operation (i.e., buy, sell or hold).\nWe adopt the DDPG [21] to train our agents. This approach is specifically estab-\nlished for the implementation in the scenario of quantitative trading, presented as\nFig. 3. DDPG is a version of actor-critic method considering a continuous action set\nwith a deterministic policy, where each agent has an actor network πθ parameterized\nby θ and a critic network Qϕ parameterized by ϕ. Each agent learns its optimal policy\nby updating the parameters of its policy networks to directly maximize the objec-\ntive function, i.e., the cumulative discounted return, J(θi) = Es∼P,a∼πθ[P\nt≥0 γtRi\nt]\nand the direction to take steps by agent i can be presented as the gradient of the\n8\n1\n\n1\nQ\nFinancial \nSignals\nActors\nCritics\nTest Phase\nTraining Phase\n1\n1\n( )\nJ\n\n\n\nN\n\nN\nQ\n(\n)\nN\nN\nJ\n\n\n\n...\n...\n...\n1\n2\nN\no\no\no\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\nN\na\na\na\n\n\n\n\n\n\n\n\n\n\n\n\nOutput\n2\n\n2\nQ\nFig. 3 A schematic of DDPG in the quantitative market environment.\ncumulative discounted return, shown as follow equations:\n∇θiJ(θi) = Es∼D\n\u0014\n∇θi log πθi(ai|s) · ∇aiQϕi(s, a)|a=πθ(s)\n\u0015\n(9)\nwhere D is the experience replay buffer containing tuples (s, s′, a, r) that are stored\nthroughout training. The centralized critic networks are updated by approximating\nthe true action-value function using temporal-difference learning,\nL(ϕi) = Es,a,r,s′\n\u0014\n(Qϕi(s, a) −y)2\n\u0015\n,\ny = ri + γQϕ′\ni(s′, a′)|a′\nj=πθ′\ni(s)\n(10)\nwhere Qϕ′\ni and πθ′\ni are target networks with delayed parameters ϕ′\ni and θ′\ni like in\ndeep Q-network method [22]. The purpose of introducing target networks is to ease\nthe moving target problem in deep reinforcement learning and stabilize the off-policy\nlearning procedure.\nThe core idea of DDPG is to centralize training while execution in a decentralized\nmanner. The centralized critic networks Q utilize past action. When execution, only\nthe actor network is used to generate policy. This technique serves as a cure to the\nnon-stationarity problem in MARL.\n9\n2.9 Strategies of Constant Proportion Portfolio\nInsurance(CPPI)\nIn many reinforcement learning frameworks, we have access to data from the system\noperated by its predecessor controller, but we do not have access to a precise simulator\nof the system. For this reason, we want the agent to learn as much as possible from\nthe demonstration data prior to running on the actual system. In the pre-training\nphase, the goal is to learn how to imitate the demonstrator with a value function that\nsatisfies Bellman’s equation.\nCPPI is a type of portfolio insurance in which the investor sets a floor based on\ntheir asset, then structures asset allocation around the trading decision [5]. As shown\nin Fig. 4, the total asset A is separated into two parts, the protection floor F and the\ncushion C, in which the floor F is the minimum guarantee used for protecting the\nbasis of the total asset and the multiple cushions k ∗C is supposed to be used as the\nrisky asset E,\nE = k ∗C = k ∗(A −F),\n(11)\nwhere the risk factor k indicates the measurement of the risk and a higher value\ndenotes a more aggressive trading strategy.\nTotal \nAsset \n(A)\nProtection \nFloor (F)\nCushion (C)\n = A - F\nRisky Asset (E) \n= k * C\nCPPI strategy\nAgent i\nProtection \nFloor (F)\ni\n\nFig. 4 The principle of CPPI strategy for agent i.\n10\nFor pre-training to have any effect, supervised loss is crucial. As the demonstration\ndata necessarily covers a narrow portion of the state space and does not take all\npossible actions, many state-actions have never been taken and have no data to ground\nthem to realistic values. To obtain the new loss function, we add a large margin\nclassifier loss.\nJE(Q) = maxa∈A[Q(s, a) + l(aE, a)] −Q(s, aE)\n(12)\nwhere aE is the action the expert demonstrator took in state s and l(aE, a) is a margin\nfunction that is 0 when a = aE and positive otherwise.\nThe overall loss is used to combine losses to update the network:\nJ(Q) = JDDP G(Q) + λJE(Q)\n(13)\nwhere λ parameter shows the weighting between the losses.\n3 Algorithms\n3.1 Asymmetric Alpha-Thompson Sampling with Contextual\nBandits\n3.1.1 Parameter Setting\nAssume the mean of reward θtbi(t) and bi(t) is linear: θtbi(t) = [bi(t)]T µ, i =\n1, ..., N and µ is unknown. Assuming the expectation ri(t) = [bi(t)]T µ (linear com-\nponent that does not change with time, depending on action ) +v(t) (nonparametric\ncomponent over time, possibly depending on history information, but not action\ndependent)\nThe distribution of v(t) (disturbance term) is assumed unknown, if v(t) = 0, lin-\near Contextual Multi-Armed Bandit; otherwise, if v(t) also depends on the action,\nAdversarial Contextual Multi-Armed Bandits.\nThese models are part of generalized linear bandits that have binary rewards,\nrepresented as A for the set of arms. The algorithm is an extension of Thompson\nSampling that assumes Bernoulli reward distributions. It models the expected reward\nat each time step using a logistic function, denoted as µ, that depends on the context\nx and a parameter vector θ in RD. Specifically, the probability of receiving a reward\nof 1 for selecting arm a at time t is represented by pa,t = µ(θaT xt).\nThen we apply an extended version of Bayesian Thompson Sampling. This method\nmaintains a posterior distribution over arm parameters, which is updated based on\nobserved rewards. At each time step, an arm with the highest probability of achieving\nthe highest expected reward is selected.\nThis algorithm balances exploration and exploitation, and can achieve near-optimal\ncumulative regret. It has shown good performance in a variety of contexts, out-\nperforming other popular algorithms such as UCB and epsilon-greedy. In Bayesian\nterminology, exploration can be seen as the process of updating the prior distribu-\ntion based on new data or information. This corresponds to the Bayesian notion of\nupdating beliefs based on evidence. Exploitation, on the other hand, can be seen as\nthe process of using the current knowledge or belief (i.e., the prior) to make decisions\n11\nthat maximize the expected reward. This corresponds to the Bayesian notion of using\nthe posterior distribution to make decisions that maximize the expected utility.\nIn a news recommendation system, the algorithm selects one article from a discrete\nset of articles A in each trial at time t = 1, 2, . . . , and the reward is obtained when\nthe user clicks on the recommended article (1 if the user clicks, 0 otherwise).\nConsidering contextual bandit problems for article recommendation, articles and\nusers are characterized by contextual attributes, such as genre and popularity for\narticles, or age and gender for users. At each trial t, the learner observes the current\nuser ut, the set of available articles A, and their respective contexts xt,a, which are\nd-dimensional summaries of both the user and article contexts.\nThe objective at each time step in contextual bandit problems for article recom-\nmendation is to recommend an article to the current user (i.e., select an arm from the\nset A), and the subsequent user action, whether they click or not, results in a binary\nreward of 1 or 0. The correlation between the reward and the contextual attributes\nis mediated by a coefficient vector θ∗, which reflects the user’s preferences concerning\ndifferent article attributes.\nThis model can be used in a variety of scenarios, such as news recommendation,\nwhere the articles are characterized by their topics and authors and the users by their\nreading history and demographic information. The learner aims to balance exploration\nof unfamiliar articles with exploitation of previously successful recommendations to\nmaximize cumulative rewards.\nTable 1 Parameter Defination\nParameter\nMeanings\nut\ncurrent user\na\navailable article\nxa,t\ncorresponding contexts\nrt\nreward\nθ⋆\ncoefficient vector showing the relationship\nThe baseline tendency in user’s clicking behavior can change unexpectedly due to\ndifferent users visiting at each time, and even for the same user, the clicking tendency\ncan modify based on their mood or schedule, which cannot be captured as contextual\ninformation. Hence, the probability of the user clicking on an article is assumed to be\nlinearly associated with the contextual information of the article and user, given this\nbaseline tendency.\n3.1.2 Algorithm for Asymmetric Alpha-Thompson Sampling with\nLinear Contextual Bandits\nThe main difference between contextual multi-armed bandits algorithm and non-\ncontextual one is that the linear relationship leads to the uncertainty of θ. For non-\ncontextual bandits algorithms, each arm has a certain parameter θ, while for contextual\n12\nbandits algorithms, the parameters θ of arms for agents with different preferences are\ndifferent.\nAlgorithm 1 Contextual Thompson Sampling\nSet B = Id, r = 0d, d is the dimension of context vector.\n1: for each iteration t ∈[T] do\n2:\nCompute µ(t) = B−1r.\n3:\nSample\nˆ\nµ(t) from Distribution N(µ(t), v2B−1)\n4:\nPull arm a(t) = argmaxi∈[N]θi(t)T µ(t) and get reward ra(t)(t).\n5:\nfor each arms n ∈[N] do\n6:\nCompute π(t) = P(a(t) = i|Ft−1)\n7:\nend for\n8:\nUpdate B and r:\n9:\nB ←−B + (θa(t)(t) −θ(t))(θa(t)(t) −θ(t))T + PN\nk=1 πi(t)(θk(t) −θ(t))(θk(t) −\nθ(t))T\n10:\nr ←−r + 2 ∗(θa(t)(t) −θ(t))ra(t)(t)\n11: end for\nIn general, we assume that reward obeys normal distribution. At this time,\naccording to the conjugation of Bayesian formula, we get the formula:\nθ = (BT B + I)−1BT µ\n(14)\nAs for asymmetric alpha-stable distribution, we need the new Bayesian inference\nformula.\nThe steps of the algorithm can be explained as follows:\na Estimate initial parameter for α, β and σ and prior distribution p(δ) (Line 1)\nb In each round, use the sampled parameter vectors to calculate the expected reward\nfor each arm (Line 3)\nc Select the arm with the highest expected reward (Line 4)\nd Decide whether accept the drawn theta from prior distribution through Metropo-\nlis–Hastings algorithm and the relationship between prior and posterior distribution\n(Line 9)\ne Get reward distirbution which is decided by new theta (Line 11),\nf Update the posterior distribution for all parameters, in the linear condition, the\nchange is shown by the update of B and r. (Line 13 - 16)\n13\nAlgorithm 2 Contextual Asymmetric α-Thompson Sampling\nSet B = Id, r = 0d, where d is the dimension of context vector. Arms n ∈[N], priors\nα, β, σ for each arm, auxiliary variable y\n1: estimate all α, β, σ by empirical characteristic function method and deduce prior\ndistribution p(δ)\n2: for each iteration t ∈[T] do\n3:\nCompute µ(t) = B−1r.\n4:\nPull arm a(t) = argmaxi∈[N]θi(t)T µ(t) and get reward ra(t)(t).\n5:\nfor each arms n ∈[N] do\n6:\ndraw θn(t) from prior distribution\n7:\nfor dimension j ∈[d] do\n8:\nGenerate u from a Uniform(0,1)\n9:\nIf\nu<p(ˆθj\nn(t)|α, β, σ,rrrj\nn(t))p(θj\nn(t)|ˆδj\nn(t))/(p(θj\nn(t)|α, β, σ,rrrj\nn(t))\np(ˆθj\nn(t)|θj\nn(t))) then θj\nn(t + 1) = ˆθj\nn(t); otherwise, θj\nn(t + 1) = θj\nn(t)\n10:\nend for\n11:\nCompute Pn(t) =\nR +∞\nθa(t) p(z|θn)dz\n12:\nend for\n13:\nUpdate B and r:\n14:\nB ←−B +(θa(t)(t)−θ(t))(θa(t)(t)−θ(t))T +PN\nk=1 Pk(t)/(PN\nk=1 Pk(t))(θk(t)−\nθ(t))(θk(t) −θ(t))T\n15:\nr ←−r + 2 ∗(θa(t)(t) −θ(t))ra(t)(t)\n16:\nUpdate posterior distribution p(δn(t+1)) p(αn(t+1)) p(βn(t+1)) p(σn(t+1))\nby equations\n17: end for\n3.1.3 Algorithm for Asymmetric Alpha-Thompson Sampling with\nSemi-Parametric Contextual Bandits\nThe central concept behind conditioning is that the non-stationarity of the rewards\ndoes not vary significantly across arms. Therefore, centering the context around the\nmean for each arm does not alter the problem of selecting the arm with the highest\nexpected reward. This enables the construction of an estimator for µj that is robust\nto the effect of vj(t), while simultaneously utilizing user affinity information through\nthe creation of a graph.\nIn this model, conditioning is a method that accounts for the variation in rewards\nacross different contexts and helps to reduce exploration time by exploiting the knowl-\nedge gained from exploring other similar contexts. By conditionally centering the\ncontext around the mean of each arm, we can focus on the differences between the\narms and choose the one that is most likely to yield the highest reward.\nThe graph structure can be utilized to model user affinity information by con-\nnecting users with similar preferences, thereby facilitating the transfer of knowledge\nbetween users in the form of reward information. This approach can lead to faster\nlearning and improved performance, as it allows the algorithm to exploit previously\nacquired knowledge rather than starting from scratch in each new context.\n14\nAlgorithm 3 Semi-Contextual Thompson Sampling\nFix λ > 0 Bj(1) = λljjId, yj(1) = 0d, d is the dimension of context vector.\n1: for each iteration t ∈[T] do\n2:\nObserve jt\n3:\nfor each iteration j ∈[N] do\n4:\nif j ̸= jt then\n5:\nUPDATE Bj(t + 1) ←−Bj(t), µj(t + 1) ←−µj(t), and yj(t + 1) ←−\nyj(t)\n6:\nelse\n7:\nˆ\nµj(t) ←−µj(t) −Bj(t)−1 P\nk̸=j λljkµk(t)\n8:\nΓj(t) ←−Bj(t) + λ2 P\nk̸=j l2\njkBk(t)−1\n9:\nSample µj(t) from N(\nˆ\nµj(t), v2\nj Γj(t)−1\n10:\nPull arm a(t) = argmaxi∈[N]θi(t)T µj(t) and get reward ra(t),j(t).\n11:\nπi(t) ←−P(a(t) = i|Ft−1)i ∈[N]\n12:\nθ(t) ←−PN\ni=1 πi(t)θi(t) and X ←−thetaa(t)(t) −θ(t)\n13:\nUpdate B, y and µ:\n14:\nBj(t+1) ←−Bj(t)+(θa(t)(t)−θ(t))(θa(t)(t)−θ(t))T +PN\nk=1 πi(t)(θk(t)−\nθ(t))(θk(t) −θ(t))T\n15:\nyj(t+1) ←−yj(t)+2X(t)ra(t),j(t), and µj(t + 1) = Bj(t+1)−1yj(t+1)\n16:\nend if\n17:\nend for\n18: end for\nThe main difference with the Asymmetric α-Thompson Sampling is shown by Line\n7-8. It is important to note that the proposed estimator ˆµj(t) and the subsequent\nThompson sampling step are both local, in the sense that they are only executed\nfor user jt at each time step, and not for all users simultaneously. This approach is\nmotivated by the fact that we typically lack updated information about other users\nat time t.\nThe idea of local updates is a natural consequence of the information available at\neach time step, as only the current user and their corresponding context are observed.\nBy focusing on the local user, we can effectively utilize the available information and\ntailor the recommendations to the user’s preferences. Through the proposed estimator\nµj(t), a combination of single user based semi-parametric contextual and asymmetric\nalpha-stable assumption is derived.\nThe main difference with the Asymmetric α-Thompson Sampling is shown by Line\n8-9. With the assumption that the context around the mean for each arm does not\nalter the problem of selecting the arm with the highest expected reward, we have\nminimized the impact of terms v(t) through new estimator µj(t).\n15\nAlgorithm 4 Semi-Contextual Asymmetric α-Thompson Sampling\nFix λ > 0 Bj(1) = λljjId, yj(1) = 0d, d is the dimension of context vector. Arms n ∈\n[N], priors α, β, σ for each arm, auxiliary variable y\n1: estimate all α, β, σ by empirical characteristic function method and deduce prior\ndistribution p(δ)\n2: for each iteration t ∈[T] do\n3:\nObserve jt\n4:\nfor each iteration j ∈[N] do\n5:\nif j ̸= jt then\n6:\nUpdate Bj(t+1) ←−Bj(t), µj(t + 1) ←−µj(t), and yj(t+1) ←−yj(t)\n7:\nelse\n8:\nˆ\nµj(t) ←−µj(t) −Bj(t)−1 P\nk̸=j λljkµk(t)\n9:\nSample µj(t) from N(\nˆ\nµj(t), v2\nj Γj(t)−1\n10:\nPull arm a(t) = argmaxi∈[N]θi(t)T µj(t) and get reward ra(t),j(t).\n11:\ndraw θn(t) from prior distribution\n12:\nGenerate u from a Uniform(0,1)\n13:\nIf\nu<p(ˆθj\nn(t)|α, β, σ,rrrj\nn(t))p(θj\nn(t)|ˆδj\nn(t))/(p(θj\nn(t)|α, β, σ,rrrj\nn(t))\np(ˆθj\nn(t)|θj\nn(t))) then θj\nn(t + 1) = ˆθj\nn(t); otherwise, θj\nn(t + 1) = θj\nn(t)\n14:\nCompute Pn(t) =\nR +∞\nθa(t) p(z|θn)dz\n15:\nUpdate B and r:\n16:\nBj(t + 1)\n←−\nBj(t) + (θa(t)(t) −θ(t))(θa(t)(t) −θ(t))T\n+\nPj(t)/(PN\nk=1 Pk(t))(θk(t) −θ(t))(θk(t) −θ(t))T\n17:\nyj(t+1) ←−yj(t)+2X(t)ra(t),j(t), and µj(t + 1) = Bj(t+1)−1yj(t+1)\n18:\nUpdate posterior distribution p(δn(t + 1)) p(αn(t + 1)) p(βn(t + 1))\np(σn(t + 1)) by equations\n19:\nend if\n20:\nend for\n21: end for\n3.1.4 Algorithm for Asymmetric Alpha-Thompson Sampling with\nAdversarial Contextual Bandits\nAdversarial contextual bandits can be regarded as a simplified RL problem, in\nwhich an adversary modifies the reward function based on the actions taken by the\nlearner. This can be seen as a form of exploration-exploitation trade-off, as the learner\ndecides between exploring new actions and exploiting previously successful actions,\nwhile the adversary tries to prevent the learner from obtaining high rewards.\nDespite its simplicity, the adversarial contextual bandit problem is highly relevant\nin practical applications, such as online advertisement, where the reward function is\naffected by user behavior and external factors, making it difficult to achieve optimal\nperformance.\nIn more complex environments, the dependence between the observed contextual\ninformation and the chosen action of the agent might not be independent, and can\n16\nbe modeled as a Markov decision process (MDP) in a reinforcement learning (RL)\nproblem.\nIn an MDP, the agent observes a state st at time step t and then selects an action\nat based on their policy. The environment responds to the action by transitioning to\na new state st+1 and providing the agent with a reward rt+1.\nWe consider a simple extension of our analysis to contextual episodic Markov\ndecision process (MDP) with unknown but deterministic transitions, denoted by\nM = MDP(S, A, H, P, r)\n(15)\nThe player interacts with this contextual episodic MDP as follows. In each episode\nt = 1, ..., T, a context x1\nt ∈S1 ⊂S is picked arbitrarily by an adversary.\nThe goal of MDP is to optimize the expected cumulative rewards:\nE\nT\nX\nt=1\nH\nX\nh=1\nrh\nt\n(16)\nIt is known that the optimal policy can be derived from the Q function of the MDP.\nQh(xh, ah) = E[rh|xh, ah] + maxah+1Qh+1(xh+1, ah+1)\n(17)\nFor simplicity, we assume that QH+1() = 0.\nThe regret of an MDP algorithm at each time step t is defined as:\nREGRETt = maxQ1(x1, a1) −E\nH\nX\nh=1\nrh\nt\n(18)\nThe flowchart of MDP-contextual bandits can be broken down into the following\nsteps:\na The algorithm starts by estimating the parameters and initializing the state, action,\nand reward history (Line 1)\nb At each time step, the algorithm receives a context, calculates the expected reward\nfor each arm and selects an action based on the current policy (Line 9)\nc After selecting an action, the algorithm receives a reward and a new context. The\nreward and context are used to update the state of the MDP (Line 10-12)\nd The algorithm updates the postrior distribution of parameters and value function\nQ of the MDP based on the current state and the rewards received (Line 13-14)\ne The algorithm improves the policy based on the updated value function. (Line 15)\nThe main difference with Semi-Contextual α-Thompson Sampling is the Q function\nwhich makes future actions have an impact on current choices.\n17\nAlgorithm 5 MDP-Contextual Asymmetric α-Thompson Sampling\nArms n ∈[N], priors α, β, σ for each arm, auxiliary variable y\n1: estimate all α, β, σ by empirical characteristic function method and deduce prior\ndistribution p(δ)\n2: for each iteration t ∈[T] do\n3:\nObserve jt ∈S\n4:\nfor each iteration j ∈S do\n5:\nDraw θj(t) p(|Sj,t−1)\n6:\nif j ̸= jt then\n7:\nUpdate Bj(t+1) ←−Bj(t), µj(t + 1) ←−µj(t), and yj(t+1) ←−yj(t)\n8:\nelse\n9:\nrefer to Algorithm 4\n10:\nUpdate B and r:\n11:\nBj(t + 1)\n←−\nBj(t) + (θa(t)(t) −θ(t))(θa(t)(t) −θ(t))T\n+\nPj(t)/(PN\nk=1 Pk(t))(θk(t) −θ(t))(θk(t) −θ(t))T\n12:\nyj(t+1) ←−yj(t)+2X(t)ra(t),j(t), and µj(t + 1) = Bj(t+1)−1yj(t+1)\n13:\nUpdate posterior distribution p(δn(t + 1)) p(αn(t + 1)) p(βn(t + 1))\np(σn(t + 1)) by equations\n14:\nUpdate\nQ\nfunction\nQh(xh, ah)\n=\nE[rh|xh, ah]\n+\nmaxah+1Qh+1(xh+1, ah+1)\n15:\nend if\n16:\nepisode t using greedy algorithm π(θ(t)) of ah = ah(θ(t), jh\nt )\n17:\nend for\n18:\nObserve trajectory [jt, at, rt]\n19: end for\n3.2 Deep Deterministic Policy Gradient with Contextual\nInformation\nThe Thompson sampling algorithm is only suitable for environments with discrete\nactions and states. In this section, we demonstrate a reinforcement learning algorithm\ncalled DDPG that is also based on the AC framework and can adapt to continuous\naction and state environments. In response to the slow iteration speed of the DDPG\nalgorithm and the inability to achieve fast transactions, we also design CPPI-DDPG\nalgorithm to balance exploration and exploitation. CPPI strategy is used for providing\ncontextual information and accelerate the iteration speed of DDPG.\nTo investigate the randomness of the dynamic stock market, we adopt the following\ntuple ⟨s, a, r, s′⟩to represent the MDP:\nState s = [p, h, b]: a vector that includes D kinds of stock price p ∈RD\n+, share\nh ∈ZD\n+, and the remaining balance b ∈RD\n+.\nAction a: An action set for K agents.\nReward r(s, a, s′): The reward for taking action a given state s and the transition\nto the new state s′.\n18\nStrategy π(s, a): the strategy of an agent. The basic idea of the policy gradient\nalgorithm is to use a parameterized probability distribution πθ(a|s) = P(a|s; θ) to\nrepresent the policy.\nBased on the settings above, we specifically designed a novel loss function according\nto the CPPI strategies. On the one hand, our ultimate target is to maximize the\nbenefits brought by the sum of strategies. On the other hand, we need each agent\nto consider its own specific situation for ensuring that the increase or decrease of\nthe overall benefits will not have much influence on its own decisions. At the same\ntime, in order to avoid all agents moving towards the same strategy, we need to set\nthe correlation between agents as part of the loss function to achieve the purpose of\nportfolio selection. The loss function for agent i can be expressed as,\nL(ϕi) = λEs,a,r,s′\n\u0014\n(Qϕi(s, a) −y)2\n\u0015\n+ (1 −λ)\nK\nX\ni=1,i≤j\nCorr(ai, aj)2\n(19)\nwhere ai is the action vector showing the positional confidence vector of agent i\nunder the restriction of strategy CPPI, and λ is the hyperparameter that controls the\nequilibrium.\nAlgorithm 6 DDPG with quantitative trading strategy\n1: Initialize Qϕi, πθi, Qϕ′\ni, πθ′\ni.\n2: while training not finished do\n3:\nInitialize initial state s and a random process N for\naction exploration.\n4:\nfor each episode do\n5:\nFor each agent select action ai = πθi(s) + Nt\n6:\nExecute joint action a and observe reward r and\nnext state s′.\n7:\nStore experience ⟨s, a, r, s′⟩in replay buffer D.\n8:\nSample a minibatch of K experiences from D.\n9:\nfor each agent do\n10:\nAdjust J(Q) using CPPI, then update the\ncritic Qϕi by minimizing Eq.(19).\n11:\nUpdate the actor πθi using Eq.(9).\n12:\nend for\n13:\nUpdate target networks for each agent:\nϕ′\ni ←τϕi + (1 −τ)ϕ′\ni,\nθ′\ni ←τθi + (1 −τ)θ′\ni.\n14:\nend for\n15: end while\n19\n4 Experiments\n4.1 Dataset and Settings\nIn our experiment, the operation of algorithm with contextual information is\nmodeled, the object is to minimize the regret bound, with the side information.\nTo demonstrate the impact of contextual information, we compared this algorithm\nwith the original Asymmetric-TS using the dataset from previous experiments with\nside information. For synthetic asymmetric, we generated contexts xt,a ∈R10 from\nalpha-stable distributions for all arms.\nWhen dealing with stock prices, there are different approaches to consider when\nusing side information. These approaches depend on the type of information required\nto make accurate predictions in the financial domain. At present, the application of RL\nin quantitative trading in academia can be roughly divided into four types: Portfolio\nManagement, Single asset trading signal, Execution, and Option hedging. Portfolio\nManagement is generally low-frequency trading and Execution is generally based on\nhigh-frequency tick level data strategies.\nFor low-frequency trading data, shares are listed Exchange through Tushare in\nShenzhen Stock using Python had been chosen as risk assets. Price to earnings ratio,\nprice to book ratio, price to sales ratio, price to cash ratio, operating revenue growth\nrate, operating profit growth rate, sales net profit margin, gross profit margin, pre-\nvious period rise and fall, and circulating market value are selected as contextual\ninformation.\nFor high-frequency tick level data, two datasets with different numbers of stocks\nincluded in the S&P 500 index are selected. We obtain stock data that consist of\nopening, high, low, closing, and volume values from Yahoo Finance and use a smaller\ndataset to compare with other methods in detail. As output data for training, the first\n70% of the data in this interval is used as training data, and the last 30% is used as\ntesting data.\nTogether with the cash as the risk-free asset, the investment products to be man-\naged may exponentially increase. The data in the training set is from January.1st 2018\nto December.31st 2020 while the data in the testing set is from January.1st 2021 to\nDecember.31st 2021. In order to better fit the actual situation of the market, we have\nimposed restrictions on the data such as non-negative remaining balance and trans-\naction cost. We initialize our cash and aim to get the highest profits with the trading\nstrategies mentioned above.\nWhen dealing with recommendation data, we choose the MovieLen 100K dataset.\nThe version of the MovieLen 100K dataset includes several features with the “-ratings”\nsuffix, including: “movieid”: A unique identifier for the rated movie; “userrating”:\nThe score given by the user for the movie on a five-star scale; “usergender”: The\ngender of the user who made the rating, with “true” values corresponding to male\n“bucketizeduserage”: Buckets of age values of the user who made the rating and so on.\nOther versions of the MovieLens dataset may include additional features such as movie\ngenres, year of release, and user occupation. These features can be used to create more\ncomplex recommendation systems that take contextual information into consideration\nwhen making movie recommendations.\n20\n4.2 Experiments for Asymmetric Alpha-Thompson Sampling\nwith Linear Contextual Bandits\nThe main difference between contextual multi-armed bandits algorithm and non-\ncontextual one is that the linear relationship leads to the uncertainty of θ.\nThe experimental results show that the context information can have a great\nimpact on the regret bound, and help to extract the information faster. At the same\ntime, due to the complex relationship between internal factors, it is more unstable\nthan the model without context information.\nFor synthetic data with contextual information, asymmetric α-Thompson algo-\nrithm performs much better than symmetric α-Thompson sampling with Linear\nContextual Bandits as the dataset is generated based on the asymmetric α-stable dis-\ntribution. For stock price with contextual information, the symmetric α-Thompson\nsampling with Linear Contextual Bandits performs better as contextual information\nis extremely important for stock prices. For recommendation data, the asymmetric α-\nThompson algorithm performs relatively better than symmetric α-Thompson sampling\nwith Linear Contextual Bandits.\nFig. 5 Regret for asymmetric data with contextual information\n21\nFig. 6 Regret for stock selection with contextual information\nFig. 7 Regret for recommendation data with contextual information\n22\n4.3 Experiments for Asymmetric Alpha-Thompson Sampling\nwith Adversarial Contextual Bandits\nAdversarial Contextual Bandits involve the impact of the agent’s own actions on\nfuture choices and even the environment, which can cause fluctuations in its Regret\nBound. In order to reflect the impact of actions on the overall environment, an inter-\nference term needs to be added to the existing data. This enables the evaluation of\nalgorithms in the context of Adversary Contextual Bandits, where the algorithm’s\ndecisions not only affect the user receiving the recommendation but also impact the\nrecommendation environment for the next user. The originally optimized action may\nactually lead to an increase in the Regret Bound. To better illustrate the optimization\nprocess, we compare the Regret Bound during each iteration in the table.\nThe number on the left represents the winning rate of the algorithm on the vertical\naxis, while the number on the right represents the winning rate of the algorithm on\nthe horizontal axis. The table shows the accuracy of different algorithms compared\nto other algorithms in dynamic systems. The AC-TS algorithm suitable for MDP,\nalthough not performing as well as reinforcement learning algorithms such as DQL\nand QL, is superior to CB-TS algorithm that does not provide feedback with action\nchanges.\nTable 2 Average Wins for different reinforcement learning\nRL\nQL\nDQL\nSARSA\nCB-TS\nAC-TS\nQL\n-\n62:38\n55:45\n63:37\n54:46\nDQL\n38 : 62\n-\n40:60\n48:52\n48:52\nSARSA\n45 : 55\n60:40\n-\n63:37\n51:49\nCB-TS\n37:63\n52:48\n37:63\n-\n42:58\nAC-TS\n46:54\n52:48\n49:51\n58:42\n-\navg wins(%)\n55.1\n45.2\n53.4\n40.5\n48\nFig. 8 Performance of Adversarial Contextual bandits\n23\nThese figures show that the variation process of the regret bound of each algorithm\nduring the iteration process. Due to the impact of actions on the environment, there\nare often fluctuations in the early stages of the iteration. The poor performance of CB-\nTS is because it cannot cover the impact of actions. Sometimes the choice of actions\ncan actually lead to fluctuations in Regret Bound, but in the long run, the impact\nof actions is a factor that needs to be considered in dynamic systems. The lower the\nRegret Bound, the closer it is to the theoretical optimal action selection.\n4.4 Portfolio Management\nTo study how each agent has made a series of trading decisions over time in the\ntest phase, we visualize the general trading behavior for each agent on 100 shares\nwith DDPG, CPPI-DDPG, and AD-TS, respectively. As shown in Fig. 9, the thermo-\ndynamic diagram presents how the agents with different strategies choose to allocate\nthe asset. The agents with DDPG prefer the relatively uniform allocation while the\nassets allocated by those with CPPI-DDPG and AD-TS are more sparse. The sparsity\nof CPPI-DDPG stems from its strategy itself, as its aversion to risk prevents it from\nincorporating high-risk stocks into its investment portfolio. The sparsity of AD-TS\nmainly stems from the discontinuity of its actions and states, which cannot be freely\nselected like the DDPG algorithm.\nFig. 9 The asset allocations with DDPG, CPPI-DDPG, and AD-TS strategies .\nThe changes of the total assets with different trading strategies over time in short\nterm are present in Fig. 10. In the short term, the advantages of the AD-TS algorithm\nare more obvious, and based on the assumption of the reward distribution function,\nits iteration speed is faster than other algorithms. The CPPI-DDPG algorithm, due\nto its model based nature, outperforms the DDPG algorithm in the short term under\nsupervised conditions.\nAs for long-term performance shown in Fig. 11, the advantage of DDPG not being\nrestricted by supervision is reflected, and it has the best performance. Due to a lack\nof exploration of the external environment, DQN has fallen into suboptimal solutions\nand has adopted a long-term strategy of not trading.\nFinally, we compare the performance of our trading strategies to that of Universal\nPortfolios (UP), DQN and DDPG through Annual Return (AR), Sharpe Ratio (SR),\nand Maximum Drawdown (MaxD, namely the maximum portfolio value loss from the\npeak to the bottom).The performance of AR, SR and MaxD are given in Table 3.\n24\nFig. 10 The short-term performances of portfolios with different strategies (The metrics of Total\nAsset and Time Step: 103 RMB and Day).\nFig. 11 The long-term performances of portfolios with different strategies.\nThe UP is a common portfolio method, which makes optimal decisions through the\ncalculation of the correlation of different stock returns. However, it cannot cope with\nreal-time data and performs poorly in the test set. The problem of DQN is the lack\nof exploration ability. There are too many uncertain factors in the stock market for\nthe strategy obtained by single agent. CPPI-DDPG and AD-TS can also degenerate\ninto the classic DDPG strategy under specific parameters, and they can adjust their\nparameters according to the investors’ individual risk preferences.\n25\nTable 3 Comparison of Different Strategies\nStrategy\nAR\nSR\nMaxD\nUP\n3.36%\n9.2%\n3.48%\nDQN\n6.47%\n8.3%\n6.35%\nDDPG\n8.22%\n11.7%\n4.79%\nCPPI-DDPG\n7.76%\n23.5%\n3.39%\nAD-TS\n9.68%\n17.8%\n4.5%\n4.5 Execution\nExecution is generally based on high-frequency tick level data strategies. We test\nalgorithmic trading methods using two datasets with different numbers of stocks\nincluded in the S&P 500 index. We obtain stock data that consist of opening, high,\nlow, closing, and volume values from Yahoo Finance.\nFig. 12 The short-term performances of executions with different strategies.\nIt can be observed that during the Execution, the trading frequency of stocks shifts\nfrom low frequency to high frequency, and the impact of the combination between\ndifferent stocks is reduced. From the short-term and long-term performance, the effi-\nciency and final results of reinforcement learning such as DDPG are not as good as\nour algorithms.\nDue to its continuous state and continuous actions, DDPG has an advantage in\ninvestment portfolios. In high-frequency trading, reinforcement learning such as DDPG\nstill requires supervision (such as CPPI) to improve its efficiency.\n26\nFig. 13 The long-term performances of executions with different strategies.\n5 Conclusion\nIn this article, we introduced the derivation of algorithms from MAB to DDPG.\nMAB, contextual bandit, Q-learning, and DDPG. In actual production, we need to\nfirst think about our own assumptions about the problem, and then choose a suitable\nmodel based on this assumption.\nFor issues in the financial sector, compared with the common gambling machine\nalgorithms such as epsilon greedy, Thompson sampling does not need to manually\nadjust hyper-parameter, and can better adapt to different problems and reward distri-\nbution. Compared to Thompson sampling, reinforcement learning algorithms require\nmore iterations to achieve the same benefits in multi-arm bandits problems. Because\nThompson sampling can converge faster through model updates under the Bayesian\nframework. In addition, reinforcement learning algorithms may lead to a decrease in\nshort-term benefits due to excessive exploration, while Thompson sampling controls\nthe balance between exploration and utilization through the uncertainty of Bayesian\nmodels, which can better balance short-term and long-term benefits.\nBy applying certain supervisory conditions to reinforcement learning, we can avoid\nexcessive exploration in the early stages of learning, thereby improving exploration\nefficiency. In this article, we chose the CPPI strategy to ensure the efficiency of rein-\nforcement learning in the short term, but it is significantly worse in the long term. In\npractical operation, we need to understand the specific situation and then choose or\nadjust the algorithm based on the conditions.\nIn the future of the work, we would like to adopt a Bayesian framework for the\nsequential learning [10] and model improvement as agents face more data and decisions.\n27\n6 Declarations\nEthical Approval\nThis paper belongs to the research of basic algorithms and can not be applied to\nhuman or animal studies, there are no ethical issues.\nCompeting interests\nAll authors disclosed no relevant relationships.\nAuthors’ contributions\nShi Zhendong and Ercan Kuruo˘glu wrote the main manuscript text, Xiaoli Wei\nwrote the theorems part. All authors reviewed the manuscript and participated in\nthe research on the algorithm to be expanded in this manuscript, asymmetric alpha\nThompson sampling.\nFunding\nThis study did not receive support from funding.\nAvailability of data and materials\nWe have provided specific sources for the all data generated or analysed during this\nstudy. The synthesized data was generated using Python through Chamber’s research.\nThe social data that support the findings of this study are openly available in the\ntushare package in Python and MovieLen 100K dataset.\nReferences\n[1] Agrawal S, Goyal N (2013) Thompson sampling for contextual bandits with linear\npayoffs. International conference on machine learning pp 127–135\n[2] An B, Sun S, Wang R (2022) Deep reinforcement learning for quantitative trading:\nChallenges and opportunities. IEEE Intelligent Systems 37(2):23–26\n[3] Auer P, Cesa-Bianchi N, Freund Y, et al (1995) Gambling in a rigged casino:\nThe adversarial multi-armed bandit problem. Proceedings of IEEE 36th annual\nfoundations of computer science pp 322–331\n[4] Baisero A, Amato C (2021) Unbiased Asymmetric Actor-Critic for Partially\nObservable Reinforcement Learning. The Computing Research Repository\n[5] Balder S, Brandl M, Mahayni A (2009) Effectiveness of cppi strategies under\ndiscrete-time trading. Journal of Economic Dynamics and Control 33(1):204–220\n[6] Bubeck S, Cesa-Bianchi N, Lugosi G (2013) Bandits with heavy tail. IEEE\nTransactions on Information Theory 59(11):7711–7717\n28\n[7] Capp´e O, Garivier A, Maillard OA, et al (2013) Kullback-Leibler upper confidence\nbounds for optimal sequential allocation. The Annals of Statistics p 1516–1541\n[8] Chen Y, So HC, Kuruoglu EE (2016) Variance analysis of unbiased least lp-norm\nestimator in non-gaussian noise. Signal Processing 122:190–203\n[9] Chu W, Li L, Reyzin L, et al (2011) Contextual bandits with linear payoff\nfunctions. Proceedings of the Fourteenth International Conference on Artificial\nIntelligence and Statistics pp 208–214\n[10] Costagli M, Kuruo˘glu EE (2007) Image separation using particle filters.\nDigital Signal Processing 17(5):935–946.\nhttps://doi.org/https://doi.org/10.\n1016/j.dsp.2007.04.003, URL https://www.sciencedirect.com/science/article/pii/\nS1051200407000590, special Issue on Bayesian Source Separation\n[11] Daniels MG, Farmer JD, Gillemot L, et al (2003) Quantitative model of price\ndiffusion and market friction based on trading as a mechanistic random process.\nPhysical Review Letters 90(10):108–102\n[12] Dubey A, Pentland A (2019) Thompson Sampling on Symmetric alpha-Stable\nBandits. International Joint Conference on Artificial Intelligence\n[13] Fleming WH, Pang T (2004) An application of stochastic control theory to\nfinancial economics. SIAM Journal on Control and Optimization 43(2):502–531\n[14] Gopalan A, Mannor S, Mansour Y (2014) Thompson sampling for complex online\nproblems. International conference on machine learning pp 100–108\n[15] Guo X, Lai TL, Shek H, et al (2017) Quantitative trading: Algorithms, Analytics,\nData, Models, Optimization. CRC Press\n[16] Gupta A (2014) Dynamic sequential decision problems with asymmetric informa-\ntion: Some existence results. University of Illinois at Urbana-Champaign\n[17] Korda N, Kaufmann E, Munos R (2013) Thompson sampling for 1-dimensional\nexponential family bandits. Advances in neural information processing systems 26\n[18] Korte B, Lov´asz L (1984) Greedoids-a structural framework for the greedy\nalgorithm. Progress in combinatorial optimization pp 221–243\n[19] Kuruoglu EE (2001) Density parameter estimation of skewed/spl alpha/-stable\ndistributions. IEEE Transactions on signal processing 49(10):2192–2201\n[20] Kuruoglu EE (2003) Analytical representation for positive /spl alpha/-stable den-\nsities. In: 2003 IEEE International Conference on Acoustics, Speech, and Signal\nProcessing, 2003. Proceedings. (ICASSP ’03)., pp VI–729\n29\n[21] Li X, Cui C, Cao D, et al (2022) Hypergraph-based reinforcement learning\nfor stock portfolio selection. 2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) pp 4028–4032\n[22] Mnih V, Kavukcuoglu K, Silver D, et al (2015) Human-level control through deep\nreinforcement learning. Nature 518(7540):529–533\n[23] Moskowitz TJ, Ooi YH, Pedersen LH (2012) Time series momentum. Journal of\nfinancial economics 104(2):228–250\n[24] Pham H (2009) Continuous-time Stochastic Control and Optimization with\nFinancial Applications, vol 61. Springer Science & Business Media\n[25] Russo D, Van R (2014) Learning to optimize via posterior sampling. Mathematics\nof Operations Research 39(4):1221–1243\n[26] Russo D, Van R (2014) Learning to optimize via posterior sampling. Mathematics\nof Operations Research 39(4):1221–1243\n[27] Samorodnitsky G, Taqqu M (1997) Stable non-gauss/an random processes.\nEconometric Theory 13:133–142\n[28] Shi Z, Kuruoglu E, Wei X (2022) Thompson sampling on asymmetric α-stable\nbandits. In Proceedings of the 15th International Conference on Agents and\nArtificial Intelligence 3:434–441\n[29] Sutton RS, Barto AG (2018) Reinforcement learning: An introduction. MIT press\n[30] Thakkar A, Chaudhari K (2021) A comprehensive survey on deep neural networks\nfor stock market: The need, challenges, and future directions. Expert Systems\nwith Applications 177:114800\n[31] Tsantekidis A, Passalis N, Tefas A (2021) Diversity-driven knowledge distillation\nfor financial trading using deep reinforcement learning. Neural Networks 140:193–\n202\n[32] Zhang H, Jiang Z, Su J (2021) A deep deterministic policy gradient-based strategy\nfor stocks portfolio management. 2021 IEEE 6th International Conference on Big\nData Analytics (ICBDA) pp 230–238\n30\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "93A16",
    "I.2.11; G.3"
  ],
  "published": "2023-10-01",
  "updated": "2023-10-01"
}