{
  "id": "http://arxiv.org/abs/2310.01717v2",
  "title": "Ensemble Distillation for Unsupervised Constituency Parsing",
  "authors": [
    "Behzad Shayegh",
    "Yanshuai Cao",
    "Xiaodan Zhu",
    "Jackie C. K. Cheung",
    "Lili Mou"
  ],
  "abstract": "We investigate the unsupervised constituency parsing task, which organizes\nwords and phrases of a sentence into a hierarchical structure without using\nlinguistically annotated data. We observe that existing unsupervised parsers\ncapture differing aspects of parsing structures, which can be leveraged to\nenhance unsupervised parsing performance. To this end, we propose a notion of\n\"tree averaging,\" based on which we further propose a novel ensemble method for\nunsupervised parsing. To improve inference efficiency, we further distill the\nensemble knowledge into a student model; such an ensemble-then-distill process\nis an effective approach to mitigate the over-smoothing problem existing in\ncommon multi-teacher distilling methods. Experiments show that our method\nsurpasses all previous approaches, consistently demonstrating its effectiveness\nand robustness across various runs, with different ensemble components, and\nunder domain-shift conditions.",
  "text": "Published as a conference paper at ICLR 2024\nENSEMBLE DISTILLATION FOR\nUNSUPERVISED CONSTITUENCY PARSING\nBehzad Shayegh1,∗\nYanshuai Cao2\nXiaodan Zhu3,4\nJackie C.K. Cheung5,6\nLili Mou1,6\n1Dept. Computing Science, Alberta Machine Intelligence Institute (Amii), University of Alberta\n2Borealis AI\n3Dept. Electrical and Computer Engineering, Queen’s University\n4Ingenuity Labs Research Institute, Queen’s University\n5Quebec Artificial Intelligence Institute (MILA), McGill University\n6Canada CIFAR AI Chair\nthe.shayegh@gmail.com\nyanshuai.cao@borealisai.com\nxiaodan.zhu@queensu.ca\njcheung@cs.mcgill.ca\ndoublepower.mou@gmail.com\nABSTRACT\nWe investigate the unsupervised constituency parsing task, which organizes words\nand phrases of a sentence into a hierarchical structure without using linguistically\nannotated data. We observe that existing unsupervised parsers capture different\naspects of parsing structures, which can be leveraged to enhance unsupervised\nparsing performance. To this end, we propose a notion of “tree averaging,” based\non which we further propose a novel ensemble method for unsupervised parsing.\nTo improve inference efficiency, we further distill the ensemble knowledge into a\nstudent model; such an ensemble-then-distill process is an effective approach to\nmitigate the over-smoothing problem existing in common multi-teacher distilling\nmethods. Experiments show that our method surpasses all previous approaches,\nconsistently demonstrating its effectiveness and robustness across various runs,\nwith different ensemble components, and under domain-shift conditions.1\n1\nINTRODUCTION\nConstituency parsing is a well-established task in natural language processing (NLP), which inter-\nprets a sentence and induces its constituency tree, a syntactic structure representation that organizes\nwords and phrases into a hierarchy (Chomsky, 1967). It has wide applications in various downstream\ntasks, including semantic role labeling (Mohammadshahi & Henderson, 2023) and explainability of\nAI models (Tenney et al., 2019; Wu et al., 2022). Traditionally, parsing is accomplished by super-\nvised models trained with linguistically annotated treebanks (Charniak, 2000), which are expensive\nto obtain and may not be available for low-resource scenarios. Also, these supervised parsers often\nunderperform when encountering domain shifts. This motivates researchers to explore unsupervised\nmethods as they eliminate the need for annotated data.\nTo address unsupervised parsing, researchers have proposed various heuristics and indirect su-\npervision signals. Clark (2001) employs context distribution clustering to induce a probabilistic\ncontext-free grammar (PCFG; Booth, 1969). Klein & Manning (2002) define a joint distribution\nfor sentences and parse structures, the latter learned by expectation–maximization (EM) algorithms.\nSnyder et al. (2009) further extend unsupervised parsing to the multilingual setting with bilingual\nsupervision.\nIn the deep learning era, unsupervised parsing techniques keep advancing. Cao et al. (2020) uti-\nlize linguistic constituency tests (Chomsky, 1967) as heuristics, evaluating all spans as potential\nconstituents for selection. Li & Lu (2023) modify each span based on linguistic perturbations and\nobserve changes in the contextual representations of a masked language model; according to the\nlevel of distortion, they determine how likely the span is a constituent. Maveli & Cohen (2022) use\nrules to train two classifiers with local features and contextual features, respectively, which are fur-\nther refined in a co-training fashion. Another way to obtain the parsing structure in an unsupervised\n⋆Work partially done during Mitacs internship at Borealis AI.\n1Code available at https://github.com/MANGA-UOFA/ED4UCP\n1\narXiv:2310.01717v2  [cs.CL]  26 Apr 2024\nPublished as a conference paper at ICLR 2024\nCompound PCFG\n100\nDIORA\n55.8\n100\nS-DIORA\n58.1\n63.6\n100\nCompound\nDIORA\nS-DIORA\nPCFG\nDIORA✩\n100\nDIORA✧\n74.1\n100\nDIORA✻\n74.3\n74.9\n100\nDIORA✩\nDIORA✧\nDIORA✻\nTable 1: Correlation analysis of unsupervised parsers. Numbers are the F1 score of one model\nagainst another on the Penn Treebank dataset (Marcus et al., 1993). The left table considers three\nheterogeneous models (Compound PCFG, DIORA, and S-DIORA), whereas the right table consid-\ners three runs (✩, ✧, and ✻) of the same model. All their F1 scores against the groundtruth fall\nwithin the range of 59–61, thus providing a controlled experimental setting.\nway is to treat it as a latent variable and train it in downstream tasks, such as text classification (Li\net al., 2019), language modeling (Shen et al., 2019; Kim et al., 2019b), and sentence reconstruc-\ntion (Drozdov et al., 2019; Kim et al., 2019a). Overall, unsupervised parsing is made feasible by\nsuch heuristics and indirect supervisions, and has become a curious research direction in NLP.\nIn our work, we uncover an intriguing phenomenon of low correlation among different unsupervised\nparsers, despite their similar overall F1 scores (the main evaluation metric for parsing), shown in\nTable 1. While Williams et al. (2018) have shown low self-consistency in early latent-tree models,\nwe go further and show the correlation among different models is even lower than restarts of the same\nmodel. This suggests that existing unsupervised parsers capture different aspects of the structures,\nand our insight is that combining these parsers may leverage their different expertise to achieve\nhigher performance for unsupervised parsing.\nTo this end, we propose an ensemble method for unsupervised parsing. We first introduce a notion of\n“tree averaging” based on the similarity of two constituency trees. Given a few existing unsupervised\nparsers, referred to as teachers,2 we then propose to use a CYK-like algorithm (Kasami, 1966;\nYounger, 1967; Manacher, 1978; Sennrich, 2014) that utilizes dynamic programming to search for\nthe tree that is most similar to all teachers’ outputs. In this way, we are able to obtain an “average”\nparse tree, taking advantage of different existing unsupervised parsers.\nTo improve the inference efficiency, we distill our ensemble knowledge into a student model. In\nparticular, we choose the recurrent neural network grammar (RNNG; Dyer et al., 2016) with an\nunsupervised self-training procedure (URNNG; Kim et al., 2019b), following the common practice\nin unsupervised parsing (Kim et al., 2019a; Cao et al., 2020). Our ensemble-then-distill process is\nable to mitigate the over-smoothing problem, where the standard cross-entropy loss encourages the\nstudent to learn an overly smooth distribution (Wen et al., 2023b). Such a problem exists in common\nmulti-teacher distilling methods (Wu et al., 2021), and would be especially severe when the teachers\nare heterogeneous, signifying the importance of our approach.\nWe evaluated our ensemble method on the Penn Treebank (PTB; Marcus et al., 1993) and SU-\nSANNE (Sampson, 2002) corpora. Results show that our approach outperforms existing unsuper-\nvised parsers by a large margin in terms of F1 scores, and that it achieves results comparable to\nthe supervised counterpart in the domain-shift setting. Overall, our paper largely bridges the gap\nbetween supervised and unsupervised constituency parsing.\nIn short, the main contributions of this paper include: 1) We reveal an intriguing phenomenon that\nexisting unsupervised parsers have diverse expertise, which may be leveraged by model ensembles;\n2) We propose a notion of tree averaging and utilize a CYK-like algorithm that searches for the av-\nerage tree of existing unsupervised parsers; and 3) We propose an ensemble-then-distill approach to\nimprove inference efficiency and to alleviate the over-smoothing problem in common multi-teacher\ndistilling approaches.\n2\nAPPROACH\n2.1\nUNSUPERVISED CONSTITUENCY PARSING\nIn linguistics, a constituent refers to a word or a group of words that function as a single unit in\na hierarchical tree structure (Chomsky, 1967). In the sentence “The quick brown fox jumps over\n2Our full approach involves training a student model from the ensemble; thus, it is appropriate to use the\nterm teacher for an ensemble component.\n2\nPublished as a conference paper at ICLR 2024\nthe lazy dog,” for example, the phrase “the lazy dog” serves as a noun phrase constituent, whereas\n“jumps over the lazy dog” is a verb phrase constituent. In this study, we address unsupervised\nconstituency parsing, where no linguistic annotations are used for training. This reduces human\nlabor and is potentially useful for low-resource languages. Following most previous work in this\ndirection (Cao et al., 2020; Maveli & Cohen, 2022; Li & Lu, 2023), we focus on unlabeled, binary\nparse trees, in which each constituent has a binary branching and is not labeled with its syntactic\nrole (such as a noun phrase or a verb phrase).\nThe standard evaluation metric for constituency parsing is the F1 score, which is the harmonic mean\nof precision and recall (Shen et al., 2018; Zhang et al., 2021):\nP = |C(Tpred) ∩C(Tref)|\n|C(Tpred)|\n, R = |C(Tpred) ∩C(Tref)|\n|C(Tref)|\n, F1 = 2 PR\nP + R\n(1)\nwhere Tpred and Tref are predicted and reference trees, respectively, and C(T) is the set of con-\nstituents in a tree T.\n2.2\nA NOTION OF AVERAGING CONSTITUENCY TREES\nIn our study, we propose an ensemble approach to combine the expertise of existing unsupervised\nparsers (called teachers), as we observe they have low correlation among themselves despite their\nsimilar overall F1 scores (Table 1).\nTo accomplish ensemble binary constituency parsing, we need to define a notion of tree averaging;\nthat is, our ensemble output is the average tree that is most similar to all teachers’ outputs. Inspired\nby the evaluation metric, we suggest the average tree should have the highest total F1 score compared\nwith different teachers. Let s be a sentence and Tk be the kth teacher parser. Given K teachers, we\ndefine the average tree to be\nAvgTree(s, {Tk}K\nk=1) = arg max\nT ∈T (s)\nK\nX\nk=1\nF1(T, Tk(s))\n(2)\nwhere T (s) is all possible unlabeled binary trees on sentence s, and Tk(s) is the kth teacher’s output.\nIt is emphasized that only the trees of the same sentence can be averaged. This simplifies the F1\nscore of binary trees, as the denominators for both precision and recall are 2|s| −1 for a sentence\nwith |s| words, i.e., |C(Tpred)| = |C(Tref)| = 2|s| −1. Thus, Eqn. (2) can be rewritten as:\nAvgTree(s, {Tk}K\nk=1) = arg max\nT ∈T (s)\nK\nX\nk=1\nF1(T, Tk(s)) = arg max\nT ∈T (s)\nK\nX\nk=1\n|C(T) ∩C(Tk(s))|\n2|s| −1\n(3)\n= arg max\nT ∈T (s)\nX\nc∈C(T )\nK\nX\nk=1\n1[c ∈C(Tk(s))]\n|\n{z\n}\nHitCount(c,{Tk(s)}K\nk=1)\n(4)\nHere, we define the HitCount function to be the number of times that a constituent c appears in the\nteachers’ outputs. In other words, Eqn. (4) suggests that the average tree should be the one that hits\nthe teachers’ predicted constituents most.\nDiscussion on MBR decoding. Our work can be seen as minimum Bayes risk (MBR) decod-\ning (Bickel & Doksum, 2015). In general, MBR yields an output that minimizes an expected error\n(called Bayes risk), defined according to the task of interest. In our case, the error function can\nbe thought of as −P\nc∈C(T ) HitCount(c, {Tk(s)}K\nk=1), and minimizing such-defined Bayes risk is\nequivalent to maximizing the total hit count in Eqn. (4).\nHowever, our MBR approach significantly differs from prior MBR studies in NLP. In fact, MBR has\nbeen widely applied to text generation (Kumar & Byrne, 2004; Freitag et al., 2022; Suzgun et al.,\n2023), where a set of candidate output sentences are obtained by sampling or beam search, and the\nbest one is selected based on a given error function, e.g., the dissimilarity against others; such an\nMBR method is selective, meaning that the output can only be selected from a candidate set. On\nthe contrary, our MBR is generative, as the sentence’s entire parse space T (s) will be considered\n3\nPublished as a conference paper at ICLR 2024\nduring the arg max process in (4). This follows Petrov & Klein (2007) who search for the global\nlowest-risk tree in the task of supervised constituency parsing. Here, the global search is feasible\nbecause the nature of tree structures facilitates efficient exact decoding with dynamic programming,\ndiscussed in the next subsection.\n2.3\nOUR CYK VARIANT\nAs indicated by Eqn. (4), our method searches for the constituency tree with the highest total hit\ncount of its constituents in the teachers’ outputs. We can achieve this by a CYK (Kasami, 1966;\nYounger, 1967)-like dynamic programming algorithm, because an optimal constituency parse struc-\nture of a span—a continuous subsequence of a sentence—is independent of the rest of the sentence.\nGiven a sentence s, we denote by sb:e a span starting from the bth word and ending right before the\neth word. Considering a set of teachers {Tk}K\nk=1, we define a recursion variable\nHb:e =\nmax\nT ∈T (sb:e)\nX\nc∈C(T )\nHitCount(c, {Tk(s)}K\nk=1)\n(5)\nwhich is the best total hit count for this span.3 We also define Lb:e to be the corresponding, locally\nbest parse structure, unambiguously represented by the set of all constituents in it.\nThe base cases are Hb:b+1 = K and Lb:b+1 = {sb:b+1} for b = 1, · · · , |s|, suggesting that the best\nparse tree of a single-word span is the word itself, which appears in all teachers’ outputs and has a\nhit count of K.\nFor recursion, we see a span sb:e will be split into two subspans sb:j and sj:e for some split position j,\nbecause our work focuses on binary constituency parsing. Given j, the total hit count for the span\nsb:e is the summation over those of the two subspans sb:j and sj:e, plus its own hit count. To obtain\nthe best split, we need to vary j from b to e (exclusive), given by\nj∗\nb:e = arg max\nb<j<e\n\u0002\nHb:j + Hj:e + HitCount(sb:e, {Tk(s)}K\nk=1)\n\u0003\n(6)\nwhere the hit count is a constant for arg max and can be omitted in implementation. Then, we have\nHb:e = Hb:j∗\nb:e + Hj∗\nb:e:e + HitCount(sb:e, {Tk(s)}K\nk=1)\n(7)\nLb:e = Lb:j∗\nb:e ∪Lj∗\nb:e:e ∪{sb:e}\n(8)\nEqn. (8) essentially groups two sub-parse structures Lb:j∗\nb:e and Lj∗\nb:e:e for the span sb:e. This can be\nrepresented as the union operation on the sets of constituents.\nThe recursion terminates when we have computed L1:|s|+1, which is the best parse tree for the\nsentence s, maximizing overall similarity to the teachers’ predictions and being our ensemble output.\nIn Appendix A, we summarize our ensemble procedure in pseudocode and provide an illustration.\n2.4\nENSEMBLE DISTILLATION\nIn our work, we further propose an ensemble distilling approach that trains a student parser from\nan ensemble of teachers. This is motivated by the fact that the ensemble requires performing infer-\nence for all teacher models and may be slow. Specifically, we choose the recurrent neural network\ngrammar (RNNG; Dyer et al., 2016) as the student model, which learns shift–reduce parsing op-\nerations (Aho & Johnson, 1974) along with language modeling using recurrent neural networks.\nThe choice of RNNG is due to its unsupervised refinement procedure (URNNG; Kim et al., 2019b),\nwhich treats syntactic structures as latent variables and uses variational inference to optimize the\njoint probability of syntax and language modeling, given some unlabeled text. Such a self-training\nprocess enables URNNG to significantly boost parsing performance.\nConcretely, we treat the ensemble outputs as pseudo-groundtruth parse trees and use them to\ntrain RNNG with cross-entropy loss. Then, we apply URNNG for refinement, following previous\nwork (Kim et al., 2019a; Cao et al., 2020).\n3Note that, in Eqns. (5)–(8), Tk(s) should not be Tk(sb:e), because the hit count is based on the teachers’\nsentence-level parsing.\n4\nPublished as a conference paper at ICLR 2024\nDiscussion on union distillation. An alternative way of distilling knowledge from multiple teachers\nis to perform cross-entropy training based on the union of the teachers’ outputs (Wu et al., 2021),\nwhich we call union distillation. Specifically, the cross-entropy loss between a target distribution t\nand a learned distribution p is −P\nx t(x) log p(x), which tends to suffer from an over-smoothing\nproblem (Wei et al., 2019; Wen et al., 2023a;b): the machine learning model will predict an overly\nsmooth distribution p to cover the support of t; if otherwise p(x) is zero but t(x) is non-zero for\nsome x, the cross-entropy loss would be infinity. Such an over-smoothing problem is especially\nsevere in our scenario, as will be shown in Section 3.4, because our multiple teachers are hetero-\ngeneous and have different expertise (Table 1). By contrast, our proposed method is an ensemble-\nthen-distill approach, which first synthesizes a best parse tree by model ensemble and then learns\nfrom the single best tree given an input sentence.\n3\nEXPERIMENTS\n3.1\nDATASETS\nWe evaluated our approach on the widely used Penn Treebank (PTB; Marcus et al., 1993) dataset,\nfollowing most previous work (Shen et al., 2019; Kim et al., 2019a; Cao et al., 2020; Maveli &\nCohen, 2022; Li & Lu, 2023). We adopted the standard split: 39,701 samples in Sections 02–21\nfor training, 1,690 samples in Section 22 for validation, and 2,412 samples in Section 23 for test. It\nis emphasized that we did not use linguistic annotations in the training set, but took the unlabeled\nsentences to train teacher unsupervised parsers and to distill knowledge into the student.\nIn addition, we used the SUSANNE dataset (Sampson, 2002) to evaluate model performance in a\ndomain-shift setting. Since it is a small, test-only dataset containing 6,424 samples in total, it is not\npossible to train unsupervised parsers directly on SUSANNE, which on the other hand provides an\nideal opportunity for domain-shift evaluation.\nWe adopted the standard evaluation metric, the F1 score of unlabeled constituents, as has been\nexplained in Section 2.1. We used the same evaluation setup as Kim et al. (2019a), ignoring punctu-\nation and trivial constituents, i.e., single words and the whole sentence. We reported the average of\nsentence-level F1 scores over the corpus.\n3.2\nCOMPETING METHODS\nOur ensemble approach involves the following classic or state-of-the-art unsupervised parsers as our\nteachers, which are also baselines for comparison.\n• Ordered Neurons (Shen et al., 2019), a neural language model that learns syntactic structures\nwith a gated attention mechanism;\n• Neural PCFG (Kim et al., 2019a), which utilizes neural networks to learn a latent probabilistic\ncontext-free grammar;\n• Compound PCFG (Kim et al., 2019a), which improves the Neural PCFG by adding an addi-\ntional sentence-level latent representation;\n• DIORA (Drozdov et al., 2019), a deep inside–outside recursive auto-encoder that marginalizes\nlatent parse structures during encoder–decoder training;\n• S-DIORA (Drozdov et al., 2020), a variant of DIORA that only considers the single most\nprobable tree during unsupervised training;\n• ConTest (Cao et al., 2020), which induces parse trees by rules and heuristics inspired by con-\nstituency tests (McCawley, 1998); and\n• ContexDistort (Li & Lu, 2023), which induces parsing structures from pretrained masked lan-\nguage models—in particular, the BERT-base model (Devlin et al., 2019) in our experiments—\nbased on contextual representation changes caused by linguistic perturbations.\nTo combine multiple teachers, we consider several alternatives:\n• Selective MBR, which selects the lowest-risk constituency tree among a given candidate set\n(Section 2.2). In particular, we consider teachers’ outputs as the candidates, and we have\nSelectiveMBR(s, {Tk}K\nk=1) = arg maxT ∈{Tk(s)}K\nk=1\nPK\nk=1 F1(T, Tk(s)). This differs from\n5\nPublished as a conference paper at ICLR 2024\nMethod\nMean±Std\nRun 1\n+RNNG\n+URNNG\n1\nLeft branching\n8.7\n–\n–\n–\n2\nRight branching\n39.5\n–\n–\n–\n3\nOrdered Neurons (Shen et al., 2019)\n44.3±6.0\n44.8\n45.4\n45.3\n4\nNeural PCFG (Kim et al., 2019a)\n51.0±1.7\n48.4\n48.9\n51.1\n5\nCompound PCFG (Kim et al., 2019a)\n55.5±2.4\n60.1\n60.5\n67.4\n6\nDIORA (Drozdov et al., 2019)\n58.9±1.8\n55.4\n58.6\n62.3\n7\nS-DIORA (Drozdov et al., 2020)\n57.0±2.1\n56.3\n59.4\n62.2\n8\nConTest (Cao et al., 2020)\n62.9±1.6\n65.9\n64.6\n68.5\n9\nContexDistort (Li & Lu, 2023)\n47.8±0.9\n48.8\n48.5\n50.8\n10\nUnion distillation\n–\n–\n65.6\n65.4\n11\nSelective MBR\n66.3±0.6\n66.7\n68.6\n71.5\n12\nOur ensemble (corresponding run)\n70.4±0.6\n70.5\n69.7\n71.7\n13\nOur ensemble (worst teacher across runs)\n69.4\n–\n69.1\n70.0\n14\nOur ensemble (best teacher across runs)\n71.9\n–\n71.1\n72.8\n15\nOracle\n83.3\n–\n76.0\n76.0\nTable 2: F1 scores on PTB. Teacher models’ results are given by our five runs of replication (de-\ntailed in Appendix E) for a fair comparison. Due to the limit of computing resources, we trained\nRNNG/URNNG with the first run only. The oracle refers to the highest possible F1 score of a binary\ntree, as the groundtruth tree may not be binary.\nour MBR approach, which is generative and performs the arg max over the entire binary tree\nspace, shown in Eqn. (2).\n• Union distillation, which trains a student from the union of the teachers’ outputs (Section 2.4).\nFor hyperparameters and other setups of previous methods (all teacher and student models), we used\ndefault values mentioned in either papers or codebases. It should be emphasized that our proposed\nensemble approach does not have any hyperparameters, thus not requiring any tuning.\n3.3\nMAIN RESULTS\nResults on PTB. Table 2 presents the main results on the PTB dataset, where we performed five\nruns of replication either by loading original authors’ checkpoints or by rerunning released code-\nbases. Our replication results are comparable to those reported in previous papers, inventoried in\nAppendix E, showing that we have successfully established a foundation for our ensemble research.\nWe first evaluate our ensembles of corresponding runs (Row 12), which is a fair comparison against\nteacher models (Rows 3–9). Without RNNG/URNNG distillation, our method outperforms the best\nteacher (Row 8) by 7.5 points in terms of F1 scores, showing that our ensemble approach is highly\neffective and justifying the proposed notion of tree averaging for unsupervised parsing.\nIt is also possible to have an ensemble of the best (or worst) teachers, one per each model across\ndifferent runs, as the teacher models are all validated by a labeled development set. We observe\nthat the ensemble of the best (or worst) teachers achieves slightly higher (or lower) scores than\nthe ensemble of the teachers in corresponding runs, which is intuitive. However, the gap between\nthe best-teachers ensemble and worst-teachers ensemble is small (Rows 13 vs. 14), showing that\nour approach is not sensitive to the variance of teachers. Interestingly, the ensemble of the worst\nteachers still outperforms the best single teacher by a large margin of 6.5 F1 points.\nWe compare our ensemble approach with selective MBR (Row 11), which selects a minimum-risk\ntree from the teachers’ predictions. As shown, selective MBR outperforms all the teachers too,\nagain verifying the effectiveness of our tree-averaging formulation. However, its performance is\nworse than our method (Row 12), which can be thought of as generative MBR that searches the\nentire tree space using a CYK-like algorithm.\nThen, we evaluate the distillation stage of our approach, which is based on Run 1 of each model. We\nobserve our RNNG and URRNG follow the same trend as in previous work that RNNG may slightly\n6\nPublished as a conference paper at ICLR 2024\nhurt the performance, but its URNNG refinement4 yields a performance boost. It is also noted that\nURNNG’s boosting effect on our approach is less significant than that on previous models, which is\nreasonable because our ensemble (w/o RNNG or URNNG) has already achieved a high performance.\nOverall, we achieve an F1 score of 72.8 in Row 14, being a new state of the art of unsupervised\nparsing and largely bridging the gap between supervised and unsupervised constituency parsing.\nWe compare our ensemble-then-distill approach with union distillation (Row 10), which trains from\nthe union of the teachers’ first-run outputs. As expected in Section 2.4, union distillation does not\nwork well; its performance is worse than that of the best teacher (Run 1 of Row 8), suggesting that\nmultiple teachers may confuse the student and hurt the performance. Rather, our approach requires\nall teachers to negotiate a most agreed tree, thus avoiding confusion during the distilling process.\nMethod\nRun 1\n+RNNG\n+URNNG\n1\nLeft branching\n6.9\n–\n–\n2\nRight branching\n26.9\n–\n–\n3\nOrdered Neurons\n32.4\n33.1\n33.1\n4\nNeural PCFG\n44.2\n46.1\n48.6\n5\nCompound PCFG\n43.0\n43.4\n46.5\n6\nDIORA\n35.9\n42.2\n44.0\n7\nS-DIORA\n37.5\n43.3\n42.4\n8\nConTest\n38.8\n46.9\n47.3\n9\nContexDistort\n41.2\n39.7\n41.1\n10\nSelective MBR\n47.4\n48.4\n48.5\n11\nOur ensemble\n50.3\n49.1\n48.8\n12\nPTB-supervised\n–\n50.1\n49.8\n13\nSUSANNE oracle\n69.8\n–\n–\nTable 3: F1 scores in the domain-shift setting\nfrom PTB to SUSANNE. Note that all mod-\nels were trained on PTB, including RNNGs\nand URNNGs. Since our approach is highly\nrobust, we only considered the models of the\nfirst run on PTB in this experiment.\nResults on SUSANNE. Table 3 presents parsing\nperformance under a domain shift from PTB to SU-\nSANNE. We directly ran unsupervised PTB-trained\nmodels on the test-only SUSANNE corpus without\nfinetuning. This is a realistic experiment to examine\nthe models’ performance in an unseen low-resource\ndomain.\nWe see both selective MBR (Row 10) and our\nmethod (Row 11) outperform all teachers (Rows 3–\n9) in the domain-shift setting, and that our approach\noutperforms selective MBR by 3 points. The results\nare consistent with the PTB experiment.\nFor the ensemble-distilled RNNG and URNNG\n(Row 11), the performance drops slightly, proba-\nbly because the performance of our ensemble ap-\nproach without RNNG/URNNG is saturating and\nclose to the PTB-supervised model (Row 12), whose\nRNNG/URNNG distillation also yields slight per-\nformance drop. Nevertheless, our RNNG and URNNG (Row 11) outperform all the baselines in\nall settings. Moreover, the inference of our student model does not require querying the teachers,\nand is 18x faster than the ensemble method (Appendix B.1). Thus, the ensemble-distilled model is\nuseful as it achieves competitive performance and high efficiency.\n3.4\nIN-DEPTH ANALYSIS\nDenoising vs. utilizing different expertise. A curious question raised from the main results is why\nour ensemble approach yields such a substantial improvement. We have two plausible hypotheses:\n1) The ensemble approach merely smooths out the teachers’ noise, and 2) The ensemble approach\nis able to utilize different expertise of heterogeneous teachers.\nWe conducted the following experiment to verify the above hypotheses. Specifically, we compare\ntwo settings: the ensemble of three runs of the same model and the ensemble of three heterogeneous\nmodels. We picked the runs and models such that the two settings have similar performance. This\nsets up a controlled experiment, as the gain obtained by the ensemble of multiple runs suggests a\ndenoising effect, whereas a further gain obtained by the ensemble of heterogeneous models suggests\nthe effect of utilizing different expertise.\nWe repeated the experiment for seven groups with different choices of models and show results in\nFigure 1. As seen, the ensemble of different runs always outperforms a single run, showing that the\ndenoising effect does play a role in the ensemble process. Moreover, the ensemble of heterogeneous\nmodels consistently leads to a large add-on improvement compared with the ensemble of multiple\nruns; the results convincingly verify that different unsupervised parsers learn different aspects of the\nlanguage structures, and that our ensemble approach is able to utilize such different expertise.\n4URNNG is traditionally used as a refinement procedure following a noisily trained RNNG (Kim et al.,\n2019a; Cao et al., 2020). If URNNG is trained from scratch, it does not yield meaningful performance and may\nbe even worse than right-branching (Kim et al., 2019b). Thus, we excluded URNNG from our teachers.\n7\nPublished as a conference paper at ICLR 2024\nGroup 1\nGroup 2\nGroup 3\nGroup 4\nGroup 5\nGroup 6\nGroup 7\n20\n30\n40\n50\n60\n70\nContexDistort\n46.6\nContexDistort\n48.7\nContexDistort\n48.8\n48.9\nOrdered Neurons\n44.8\nNeural PCFG\n48.4\nContexDistort\n48.8\n57.3\nOrdered Neurons\n32.9\nOrdered Neurons\n44.8\nOrdered Neurons\n50.0\n50.3\nOrdered Neurons\n32.9\nContexDistort\n46.6\nNeural PCFG\n48.4\n55.3\nCompound PCFG\n53.9\nCompound PCFG\n55.4\nCompound PCFG\n60.1\n60.2\nCompound PCFG\n53.9\nDIORA\n55.4\nS-DIORA\n60.0\n66.3\nDIORA\n55.4\nDIORA\n59.4\nDIORA\n60.3\n63.7\nCompound PCFG\n53.9\nS-DIORA\n58.9\nDIORA\n60.3\n66.8\nS-DIORA\n56.3\nS-DIORA\n58.9\nS-DIORA\n60.0\n65.0\nCompound PCFG\n55.4\nS-DIORA\n58.9\nDIORA\n59.4\n67.4\nConTest\n61.6\nConTest\n61.8\nConTest\n62.3\n65.0\nCompound PCFG\n60.0\nS-DIORA\n60.1\nConTest\n62.3\n70.9\nDenoising\neffect\nNeural PCFG\n48.4\nNeural PCFG\n52.1\nNeural PCFG\n52.6\n55.5\nAdditional\nboost\ngained by\nutilizing\nexpertise\nContexDistort\n48.7\nOrdered Neurons\n50.0\nNeural PCFG\n52.6\n61.6\nEnsemble of different runs\nEnsemble of different models\nFigure 1: Effect of denoising vs. utilizing different expertise. Results are the F1 scores on the PTB\ntest set. The italic blue annotation is an interpretation of the plot.\nApproach\nMean Entropy\nStd\nUnion distillation\n11.42\n0.09\nOur ensemble distillation\n4.93\n0.12\nBinarized-groundtruth supervision\n2.26\n0.12\nTable 4: The mean and standard deviation of the prediction entropy for distilled/supervised RNNGs.\nOver-smoothing in multi-teacher knowledge distillation. As discussed in Section 2.4, union\ndistillation is prone to the over-smoothing problem, where the student learns an overly smooth,\nwide-spreading distribution. This is especially severe in our setting, as our student learns from\nmultiple heterogeneous teachers.\n1\n2\n3\n4\n5\n6\n7\nNumber of teachers in ensemble\n35\n40\n45\n50\n55\n60\n65\n70\nEnsemble F1 score on PTB test\nBest-performing combination\nAverage over all combinations\nWorst-performing combination\nFigure 2: Ensemble performance with differ-\nent numbers of teachers. The lines are best-\nperforming, average, and worst-performing\ncombinations.\nThese results are averaged\nover five runs available in the experiments\nconducted for Table 2. The gray shades are\nthe best and worst runs.\nThe over-smoothing problem can be verified by\nchecking the entropy, −P\nx p(x) log p(x), of a\nmodel’s predicted distribution p.\nIn Table 4, we\nreport the mean and standard deviation of the en-\ntropy5 across five runs. Results clearly show that\nthe union distillation leads to a very smooth distri-\nbution (very high entropy), which also explains its\nlow performance (Table 2).\nOn the contrary, our\nensemble-then-distill approach yields much lower\nentropy, providing strong evidence of the alleviation\nof the over-smoothing problem.\nAnalyzing the number of teachers. In our main\nexperiment (Table 2), we perform an ensemble of\nseven popular unsupervised parsers. We would like\nto analyze the performance of ensemble models with\ndifferent numbers of teachers,6 and results are shown\nin Figure 2.\nWe see a consistent trend that more teachers lead to\nhigher performance. Profoundly, the top dashed line\nsuggests that, even if we start with a strong teacher, adding weaker teachers also improves, or at\nleast does not hurt, the performance. Further, the decrease in the width of gray shades (deviations\nof best and worst runs) suggests that more teachers also lead to lower variance. Overall, this anal-\nysis conclusively shows that, with a growing number of teachers, our ensemble approach not only\nimproves performance, but also makes unsupervised parsing more robust.\nAdditional results. We present supplementary analyses in the appendix. B.1: Inference efficiency;\nB.2: Performance by sentence lengths; and B.3: Performance by different constituency types.\n5The entropy of each run is averaged over 2,412 samples. The calculation of entropy is based on the\ncodebase of Kim et al. (2019b), available at https://github.com/harvardnlp/urnng\n6We have 27 −1 combinations, which are tractable because our CYK algorithm is efficient.\n8\nPublished as a conference paper at ICLR 2024\n4\nRELATED WORK\nUnsupervised syntactic structure discovery carries a long history and has attracted much attention\nin different ages (Klein, 2005; Shen et al., 2019; Li & Lu, 2023). Its significance lies in the potential\nto help low-resource domains (Kann et al., 2019) and its important role in cognitive science, such\nas understanding how children learn language (Bod, 2009). Peng et al. (2011) show unsupervised\nconstituency parsing methods are not limited to linguistics but can also be used to parse motion-\nsensor data by treating it as a language. This approach finds an abstraction of motion data and leads\nto a better understanding of the signal semantics.\nUnsupervised syntactic structure discovery can be divided into different tasks: unsupervised con-\nstituency parsing, which organizes the phrases of a sentence in a hierarchical manner (Chomsky,\n1967); unsupervised dependency parsing (Nivre, 2010; Naseem et al., 2010; Han et al., 2020),\nwhich determines the syntactic relation between the words in a sentence; and unsupervised chunk-\ning, which aims at segmenting a text into groups of syntactically related words in a flattened struc-\nture (Deshmukh et al., 2021; Wu et al., 2023).\nOur work falls in the category of unsupervised constituency parsing. Previous work has proposed\nvarious heuristics and indirect supervisions to tackle this task (Snyder et al., 2009; Kim et al., 2019a;\nDrozdov et al., 2019; Shi et al., 2019), as mentioned in Section 1. In our work, we propose to build\nan ensemble model to utilize the expertise of different unsupervised parsers.\nMinimum Bayes risk (MBR) decoding minimizes a Bayes risk (i.e., expected loss) during infer-\nence (Bickel & Doksum, 2015). For example, machine translation systems may generate a set of\ncandidate outputs, and define the risk as the dissimilarity between one candidate output and the rest;\nMBR decoding selects the lowest-risk candidate translation that is most similar to others (Kumar &\nByrne, 2004; Freitag et al., 2022). Similar approaches are applied to other decoding tasks, such as\nspeech recognition (Gibson & Hain, 2006), text summarization (Suzgun et al., 2023), text-to-code\ntranslation (Shi et al., 2022), and dependency parsing (Smith & Smith, 2007). For constituency\nparsing, Titov & Henderson (2006) formulate the task under the MBR framework, and Petrov &\nKlein (2007) extend it to state-split PCFGs.\nIn this work, we develop a novel generative MBR method for ensemble constituency parsing that\nsearches the entire binary tree space by an efficient CYK-like dynamic programming, significantly\ndiffering from common MBR approaches that perform selection on a candidate set.\nKnowledge distillation (KD) is commonly used to train a small student model from a large teacher\nmodel (Sun et al., 2019; Jiao et al., 2020). Evidence show that the teacher’s predicted probability\ncontains more knowledge than a groundtruth label and can better train the student model (Hinton\net al., 2015; Wen et al., 2023b).\nInterestingly, KD is originally proposed to train a small model from an ensemble of teachers (Bucilu˘a\net al., 2006; Hinton et al., 2015). They address simple classification tasks and use either voting or\naverage ensembles to train the student. A voting ensemble is similar to MBR, but only works\nfor classification tasks; it cannot be applied to structure prediction (e.g., sequences or trees). An\naverage ensemble takes the average of probabilities; thus, it resembles union distillation, which is\nthe predominant approach for multi-teacher distillation in recent years (Wu et al., 2021; Yang et al.,\n2020). However, these approaches may suffer from the over-smoothing problem when teachers are\nheterogeneous (Section 2.4). In our work, we propose a novel MBR-based ensemble method for\nmulti-teacher distillation, which largely alleviates the over-smoothing problem and is able to utilize\ndifferent teachers’ expertise.\n5\nCONCLUSION\nIn this work, we reveal an interesting phenomenon that different unsupervised parsers learn dif-\nferent expertise, and we propose a novel ensemble approach by introducing a new notion of “tree\naveraging” to leverage such heterogeneous expertise. Further, we distill the ensemble knowledge\ninto a student model to improve inference efficiency; the proposed ensemble-then-distill approach\nalso addresses the over-smoothing problem in multi-teacher distillation. Overall, our method shows\nconsistent effectiveness with various teacher models and is robust in the domain-shift setting, largely\nbridging the gap between supervised and unsupervised constituency parsing. We will discuss future\nwork in Appendix D.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nAlfred V. Aho and Stephen C. Johnson. LR parsing. CSUR, 6(2):99–124, 1974.\nPeter J Bickel and Kjell A Doksum. Mathematical Statistics: Basic Ideas and Selected Topics. CRC\nPress, 2015.\nRens Bod. From exemplar to grammar: A probabilistic analogy-based model of language learning.\nCognitive Science, 33(5):752–793, 2009.\nTaylor L. Booth. Probabilistic representation of formal languages. In Scandinavian Workshop on\nAlgorithm Theory, 1969.\nCristian Bucilu˘a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, pp.\n535–541, 2006.\nSteven Cao, Nikita Kitaev, and Dan Klein. Unsupervised parsing via constituency tests. In EMNLP,\npp. 4798–4808, 2020.\nEugene Charniak. A maximum-entropy-inspired parser. In NAACL, pp. 132–139, 2000.\nNoam Chomsky. Syntactic Structures. The Hague: Mouton, 1967.\nAlexander Clark. Unsupervised induction of stochastic context-free grammars using distributional\nclustering. In CoNLL, 2001.\nAnup Anand Deshmukh, Qianqiu Zhang, Ming Li, Jimmy Lin, and Lili Mou. Unsupervised chunk-\ning as syntactic structure induction with a knowledge-transfer approach. In Findings of EMNLP,\npp. 3626–3634, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT, pp. 4171–4186, 2019.\nAndrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. Unsupervised\nlatent tree induction with deep inside-outside recursive auto-encoders. In NAACL-HLT, pp. 1129–\n1141, 2019.\nAndrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim O’Gorman, Mohit Iyyer, and Andrew Mc-\nCallum. Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside re-\ncursive autoencoders. In EMNLP, pp. 4832–4845, 2020.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network\ngrammars. In NAACL-HLT, pp. 199–209, 2016.\nMarkus Freitag, David Grangier, Qijun Tan, and Bowen Liang. High quality rather than high model\nprobability: Minimum Bayes risk decoding with neural metrics. TACL, 10:811–825, 2022.\nMatthew Gibson and Thomas Hain. Hypothesis spaces for minimum Bayes risk training in large\nvocabulary speech recognition. In INTERSPEECH, pp. 2406–2409, 2006.\nWenjuan Han, Yong Jiang, Hwee Tou Ng, and Kewei Tu. A survey of unsupervised dependency\nparsing. In COLING, pp. 2522–2533, 2020.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun\nLiu. TinyBERT: Distilling BERT for natural language understanding. In Findings of EMNLP, pp.\n4163–4174, 2020.\nKatharina Kann, Anhad Mohananey, Samuel R. Bowman, and Kyunghyun Cho. Neural unsuper-\nvised parsing beyond English. In Proceedings of Workshop on Deep Learning for Low-Resource\nNatural Language Processing, pp. 209–218, 2019.\nTadao Kasami. An efficient recognition and syntax-analysis algorithm for context-free languages.\nCoordinated Science Laboratory Report, 1966.\n10\nPublished as a conference paper at ICLR 2024\nYoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for\ngrammar induction. In ACL, pp. 2369–2385, 2019a.\nYoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and G´abor Melis. Unsuper-\nvised recurrent neural network grammars. In NAACL-HLT, pp. 1105–1117, 2019b.\nDan Klein. The Unsupervised Learning of Natural Language Structure. Stanford University, 2005.\nDan Klein and Christopher D. Manning. A generative constituent-context model for improved gram-\nmar induction. In ACL, pp. 128–135, 2002.\nShankar Kumar and William Byrne. Minimum Bayes-risk decoding for statistical machine transla-\ntion. In HLT-NAACL, pp. 169–176, 2004.\nBowen Li, Lili Mou, and Frank Keller. An imitation learning approach to unsupervised parsing. In\nACL, pp. 3485–3492, 2019.\nJiaxi Li and Wei Lu.\nContextual distortion reveals constituency: Masked language models are\nimplicit parsers. In ACL, pp. 5208–5222, 2023.\nGlenn K. Manacher. An improved version of the Cocke-Younger-Kasami algorithm. Computer\nLanguages, 3(2):127–133, 1978.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\nNickil Maveli and Shay Cohen. Co-training an unsupervised constituency parser with weak super-\nvision. In Findings of ACL, pp. 1274–1291, 2022.\nJames D. McCawley. The Syntactic Phenomena of English. University of Chicago Press, 1998.\nAlireza Mohammadshahi and James Henderson. Syntax-aware graph-to-graph transformer for se-\nmantic role labelling. In RepL4NLP, pp. 174–186, 2023.\nTahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. Using universal linguistic knowl-\nedge to guide grammar induction. In EMNLP, pp. 1234–1244, 2010.\nJoakim Nivre. Dependency parsing. Language and Linguistics Compass, 4(3):138–152, 2010.\nHuan-Kai Peng, Pang Wu, Jiang Zhu, and Joy Ying Zhang. Helix: Unsupervised grammar induction\nfor structured activity recognition. In ICDM, pp. 1194–1199, 2011.\nSlav Petrov and Dan Klein. Improved inference for unlexicalized parsing. In HLT-NAACL, pp.\n404–411, 2007.\nGeoffrey Sampson. English for the computer: The SUSANNE corpus and analytic scheme. Com-\nputational Linguistics, 28(1):102–103, 2002.\nRico Sennrich. A CYK+ variant for SCFG decoding without a dot chart. In Proceedings of the\nWorkshop on Syntax, Semantics and Structure in Statistical Translation, pp. 94–102, 2014.\nBehzad Shayegh, Yuqiao Wen, and Lili Mou. Ensemble-based unsupervised discontinuous con-\nstituency parsing by tree averaging. arXiv preprint arXiv:2403.00143, 2024.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron Courville. Neural language modeling by\njointly learning syntax and lexicon. In ICLR, 2018.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating\ntree structures into recurrent neural networks. In ICLR, 2019.\nFreda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang.\nNatural\nlanguage to code translation with execution. In EMNLP, pp. 3533–3546, 2022.\nHaoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen Livescu. Visually grounded neural syntax\nacquisition. In ACL, pp. 1842–1861, 2019.\n11\nPublished as a conference paper at ICLR 2024\nDavid A. Smith and Noah A. Smith. Probabilistic models of nonprojective dependency trees. In\nEMNLP-CoNLL, pp. 132–140, 2007.\nBenjamin Snyder, Tahira Naseem, and Regina Barzilay. Unsupervised multilingual grammar induc-\ntion. In ACL, pp. 73–81, 2009.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model\ncompression. In EMNLP-IJCNLP, pp. 4323–4332, 2019.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. Follow the wisdom of the crowd: Effective\ntext generation via minimum Bayes risk decoding. In Findings of ACL, pp. 4265–4293, 2023.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In ACL,\npp. 4593–4601, 2019.\nIvan Titov and James Henderson. Loss minimization in parse reranking. In EMNLP, pp. 560–567,\n2006.\nBolin Wei, Shuai Lu, Lili Mou, Hao Zhou, Pascal Poupart, Ge Li, and Zhi Jin. Why do neural dialog\nsystems generate short and meaningless replies? A comparison between dialog and translation.\nIn ICASSP, pp. 7290–7294, 2019.\nYuqiao Wen, Yongchang Hao, Yanshuai Cao, and Lili Mou. An equal-size hard EM algorithm for\ndiverse dialogue generation. In ICLR, 2023a.\nYuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level\nknowledge distillation. In ACL, pp. 10817–10834, 2023b.\nYuqiao Wen, Behzad Shayegh, Chenyang Huang, Yanshuai Cao, and Lili Mou. Ebbs: An ensemble\nwith bi-level beam search for zero-shot machine translation. arXiv preprint arXiv:2403.00144,\n2024.\nAdina Williams, Andrew Drozdov, and Samuel R. Bowman. Do latent tree learning models identify\nmeaningful structure in sentences? TACL, 6:253–267, 2018.\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang. One teacher is enough? Pre-trained language\nmodel distillation from multiple teachers. In Findings of ACL-IJCNLP, pp. 4408–4413, 2021.\nZijun Wu, Zi Xuan Zhang, Atharva Naik, Zhijian Mei, Mauajama Firdaus, and Lili Mou. Weakly\nsupervised explainable phrasal reasoning with neural fuzzy logic. In ICLR, 2022.\nZijun Wu, Anup Anand Deshmukh, Yongkang Wu, Jimmy Lin, and Lili Mou. Unsupervised chunk-\ning with hierarchical RNN. arXiv preprint arXiv:2309.04919, 2023.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with two-stage\nmulti-teacher knowledge distillation for web question answering system. In WSDM, pp. 690–698,\n2020.\nDaniel H. Younger. Recognition and parsing of context-free languages in time n3. Information and\nControl, 10(2):189–208, 1967.\nYu Zhang, Houquan Zhou, and Zhenghua Li. Fast and accurate neural CRF constituency parsing.\nIn IJCAI, pp. 4046–4053, 2021.\n12\nPublished as a conference paper at ICLR 2024\nA\nOUR CYK VARIANT\nIn this appendix, we provide a step-by-step illustration of our CYK-based ensemble algorithm in-\ntroduced in Section 2.3.\nConsider four teachers predicting the trees in the first row of Figure 3. The hit count of each span is\nshown in the second row. For example, the span (w1w2) hits 3 times, namely, Teachers 1–3.\nThe initialization of the algorithm is to obtain the total hit count for a single word, which is simply\nthe same as the number of teachers because every word appears intact in every teacher’s prediction.\nThe initialization has five cells in a row, and is omitted in the figure to fit the page width.\nFor recursion, we first consider the constituents of two words, denoted by l = 2. A constituent’s\ntotal hit count, denoted by Hb:e in Eqn. (5), inherits those of its children, plus its own hit count. In\nthe cell of l = 2, b = 1, for example, H1:3 = 4 + 4 + 3 = 11, where 3 is the hit count of the span\n(w1w2), shown before.\nFor the next step of recursion, we consider three-word constituents, i.e., l = 3. For example, the\nspan w1w2w3 has two possible tree structures (w1(w2w3)) and ((w1w2)w3). The former leads to a\nTeacher 1\nTeacher 2\nTeacher 3\nTeacher 4\nTree\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\n✧\nw1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5\nHitCount\n4\n1\n1\n1\n1\n1\n3\n0\n3\n1\n4\n4\n4\n4\n4\nw1\nw2\nw3\nw4\nw5\nb = 1\nb = 2\nb = 3\nb = 4\nl = 2\n11\n11\n08\n11\n08\n11\n11\n08\n11\n09\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\nw1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5\nl = 3\n16\n16\n16\n16\n16\n16\n11\n08\n11\n09\n11\n08\n11\n09\n11\n08\n11\n09\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\nw1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5\nl = 4\n23\n23\n21\n16\n16\n16\n16\n16\n16\n11\n08\n11\n09\n11\n08\n11\n09\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\nw1\nw2\nw3\nw4\nw5 w1\nw2\nw3\nw4\nw5\nl = 5\n31\n23\n21\n16\n16\n16\n11\n08\n11\n09\n4\n4\n4\n4\n4\nw1\nw2\nw3\nw4\nw5\nFigure 3: Step-by-step illustration of our CYK algorithm, showing the dynamic changes in the H\nalong with the construction of the corresponding optimal binary constituency tree.\n13\nPublished as a conference paper at ICLR 2024\nAlgorithm 1 Our CYK Variant\n1: input: s, {Ti}K\ni=1\n2: for b ←1 to |s| do\n▷Base cases\n3:\nHb:b+1 = K\n4:\nLb:b+1 = {sb:b+1}\n5: end for\n6: for l ←2 to |s| do\n▷Iterate over different lengths of constituents\n7:\nfor b ←1 to |s| −l + 1 do\n▷Iterate over different possible constituents of length l\n8:\ne ←b + l\n9:\nj∗\ns:b ←arg max\nb<j<e\n(Hb:j + Hj:e+ HitCount(sb:e, {Ti(s)}K\ni=1))\n▷The gray term need not be implemented as it is a constant in j\n10:\nHb:e ←Hb:j∗\ns:b + Hj∗\ns:b:e + HitCount(sb:e, {Ti(s)}K\ni=1)\n11:\nLb:e ←Lb:j∗\ns:b ∪Lj∗\ns:b:e ∪{sb:e}\n12:\nend for\n13: end for\n14: return L1:|s|+1\ntotal hit count of 13, whereas the latter leads to 16. Therefore, ((w1w2)w3) is chosen, with the best\ntotal hit count H1:4 = 16.\nThe process is repeated until we have the best parse tree of the whole sentence, which is l = 5 for\nthe 5-word sentence in Figure 3.\nWe provide the pseudocode for the process in Algorithm 1.\nB\nSUPPLEMENTARY ANALYSES\nB.1\nINFERENCE EFFICIENCY\nInference Time (ms)\nModel\nw/ GPU\nw/o GPU\nTeachers\nON\n35\n130\nNeural PCFG\n610\n630\nCompound PCFG\n560\n590\nDIORA\n30\n30\nS-DIORA\n110\n140\nConTest\n4,300\n59,500\nContexDestort\n1,890\n11,110\nOur ensemble\nCYK part\n6\n6\nTotal\n7,541\n72,136\nStudent\nRNNG\n410\n410\nTable 5: Per-sample inference time (in mil-\nliseconds) on the PTB test.\nWe propose to distill the ensemble knowledge into a\nstudent model to increase the inference efficiency.\nWe conducted an analysis on the inference time\nof different approaches, where we measured the\nrun time using 28 Intel(R) Core(TM) i9-9940X\n(@3.30GHz) CPUs with or without GPU (Nvidia\nRTX Titan). Table 5 reports the average time elapsed\nfor performing inference on one sample7 of the PTB\ntest set, ignoring loading models, reading inputs, and\nwriting outputs.\nIn the table, the total inference time of our ensem-\nble model is the summation of all the teachers and\nthe CYK algorithm. As expected, an ensemble ap-\nproach is slow because it has to perform inference\nfor every teacher. However, our CYK-based ensem-\nble algorithm is extremely efficient and its inference\ntime is negligible compared with the teacher models.\nThe RNNG student model learns the knowledge from the cumbersome ensemble, and is able to\nperform inference efficiently with an 18x and 175x speedup with and without GPU, respectively.\nThis shows the necessity of having knowledge distillation on top of the ensemble. Overall, RNNG\nachieves comparable performance to its ensemble teacher (Tables 2 and 3) but drastically speeds up\nthe inference, being a useful model in practice.\n7The average time was computed on 100 samples of the PTB test set, due to the slow inference of certain\nteachers without GPU.\n14\nPublished as a conference paper at ICLR 2024\n1-10\n11-20\n21-30\n31-40\n41-50\n51-60\nSentence length (# words)\n30\n40\n50\n60\n70\nAverage F1 score on PTB test\nOrderedNeurons\nNeuralPCFG\nCompoundPCFG\nDIORA\nS-DIORA\nConTest\nContexDistort\nEnsemble\nFigure 4: Performance by sentence lengths. F1 scores are averaged over five different runs.\nB.2\nPERFORMANCE BY SENTENCE LENGTHS\nFigure 4 illustrates the parsing performance on sentences of varying lengths. The result shows that\nexisting unsupervised parsers have different levels of susceptibility to long sentences. For example,\nContexDistort shows notable robustness to the length, whereas the performance of Ordered Neu-\nrons drops significantly when the sentences are longer. Our ensemble method achieves both high\nperformance and robustness across different lengths.\nB.3\nPERFORMANCE BY CONSTITUENCY LABELS\nIn this work, we see different unsupervised parsers learn different patterns (Table 1), and their ex-\npertise can be utilized by an ensemble approach (Section 3.4). From the linguistic point of view,\nwe are curious about whether there is a relation between such different expertise and the linguistic\nconstituent labels (e.g., noun phrases and verb phrases).\nWith this motivation, we report in Figure 5 the breakdown performance by constituency labels,\nwhere the most common five labels—namely, noun phrases, propositional phrases, verb phrases,\nsimple declarative clauses, and subordinating conjunction clauses—are considered, covering 95%\nNP\nVP\nPP\nS\nSBAR\n0\n20\n40\n60\n80\nOrdered Neurons\nOrdered Neurons\nOrdered Neurons\nOrdered Neurons\nOrdered Neurons\nNeural PCFG\nNeural PCFG\nNeural PCFG\nNeural PCFG\nNeural PCFG\nCompound PCFG\nCompound PCFG\nCompound PCFG\nCompound PCFG\nCompound PCFG\nDIORA\nDIORA\nDIORA\nDIORA\nDIORA\nS-DIORA\nS-DIORA\nS-DIORA\nS-DIORA\nS-DIORA\nConTest\nConTest\nConTest\nConTest\nConTest\nContexDistort\nContexDistort\nContexDistort\nContexDistort\nContexDistort\nOur ensemble\nOur ensemble\nOur ensemble\nOur ensemble\nOur ensemble\nFigure 5: Performance by constituency labels on the PTB test set. Results are measured by re-\ncall, because the predicted parse trees are unlabeled; thus, precision and F1 scores cannot be com-\nputed (Drozdov et al., 2019). Bars and gray intervals are the mean and standard deviation, respec-\ntively, over five runs.\n15\nPublished as a conference paper at ICLR 2024\nTeachers:\n        Neural PCFG, DIORA, ContexDistort\n    /\\  Compound PCFG, S-DIORA, Ordered Neurons, ConTest\n /\\   Our ensemble\n      Groundtruth constituents\n⥎\nIt\n's\nhuge\nIt\n's\nhuge\nFigure 6: A case study, which shows that voting selects more commonly agreed structures.\nSome\nanalysts\nsaw\nthe\npayment\nas\nan\neffort\nalso\nto\ndispel\ntakeover\nspeculation\nSome\nanalysts\nsaw\nthe\npayment\nas\nan\neffort\nalso\nto\ndispel\ntakeover\nspeculation\nTeachers:     Compound PCFG      DIORA      ConTest\n/\\  Agreed among all teachers\n ⥎   Groundtruth constituents\n  /\\    Our ensemble\nFigure 7: A case study, where the ensemble outperforms all the teachers and achieves 100% recall\nover the groundtruth constituents.\nof the cases in the PTB test set. Notice that the predicted constituency parse trees are still unla-\nbeled (without tags like noun phrases), whereas the groundtruth constituency labels are used for\ncollecting the statistics. Consequently, only recall scores can be calculated in per-label performance\nanalysis (Drozdov et al., 2019; Kim et al., 2019a; Cao et al., 2020).\nAs seen, existing unsupervised parsers indeed exhibit variations in the performance of different\nconstituency labels. For example, ConTest achieves high performance of prepositional phrases,\nwhereas DIORA works well for clauses (including simple declarative clauses and subordinating\nconjunction clauses); for noun phrases, most models perform similarly. By contrast, our ensemble\nmodel achieves outstanding performance similar to or higher than the best teacher in each category.\nThis provides further evidence that our ensemble model utilizes different teachers’ expertise.\nC\nCASE STUDIES\nIn this section, we present case studies to show how the ensemble improves the performance. In\nparticular, Figure 6 illustrates teachers’ performance, their ensemble output, and groundtruth for the\nsentence “It’s huge.” This example represents how voting over local structures may result in correct\nstructure detection. True constituents have a higher chance to appear in the majority of the teach-\ners’ outputs. This phenomenon extends to longer sentences and more complex structures. Figure 7\npresents an example where the ensemble outperforms all its teachers, hitting all the groundtruth con-\nstituents, which never happens in any teacher. Note that in this example, every constituent captured\nby the ensemble appears in at least two out of three teachers.\nFigure 8 illustrates a more interesting behavior of the ensemble, where it recovers a true constituent\nnever seen in any teacher’s output, drawn in dotted purple in the bottom figure. It happens in complex\nstructures when teachers agree on some local structures but do not agree over the entire sentence. In\nthat case, the ensemble eventually picks the agreed structures and fills the gaps with the remaining\noptions.\nD\nFUTURE WORK\nFuture work may be considered from both linguistic and machine learning perspectives. The pro-\nposed ensemble method largely bridges the gap between supervised and unsupervised parsing of the\nEnglish language. A future direction is to address unsupervised linguistic structure discovery in low-\nresource and multilingual settings (Shayegh et al., 2024). Regarding the machine learning aspect,\nour work demonstrates the importance of addressing the over-smoothing problem in multi-teacher\ndistillation, and we expect our ensemble-then-distill approach can be extended to different data\n16\nPublished as a conference paper at ICLR 2024\nFutures traders\nsay\nthe\nS&P\nwas\nsignaling\nthat\nthe\ndow\ncould\nfall\nas\nmuch\nas\n200\npoints\nFutures traders\nsay\nthe\nS&P\nwas\nsignaling\nthat\nthe\ndow\ncould\nfall\nas\nmuch\nas\n200\npoints\nTeachers:\n        Neural PCFG\n        Ordered Neurons\n        ContexDistort\n/\\  Agreed among all teachers\n  /\\    Our ensemble\n ⥎   Recovered groundtruth\n - -  Missed groundtruth\n ...  Discovered groundtruth\nFigure 8: A case study, where the ensemble recovers a true constituent never seen in any teacher’s\noutput.\ntypes, such as sequences and graphs, with proper design of data-specific ensemble methods (Wen\net al., 2024).\nACKNOWLEDGMENTS\nWe would like to thank all reviewers and chairs for their valuable and constructive comments. The\nresearch is supported in part by the Natural Sciences and Engineering Research Council of Canada\n(NSERC), a Mitacs Accelerate project, the Amii Fellow Program, the Canada CIFAR AI Chair Pro-\ngram, an Alberta Innovates Program, and the Digital Research Alliance of Canada (alliancecan.ca).\nWe also thank Yongchang Hao for providing advice on the algorithms.\n17\nPublished as a conference paper at ICLR 2024\nE\nINVENTORY OF TEACHER MODELS\nOur experiments involve seven existing unsupervised parsers as teachers, each of which has five runs\neither based on authors’ checkpoints or by our replication using authors’ codebases. We show the\ndetails in Table 6, where we also quote the mean F1 scores and, if available, max F1 scores reported\nin respective papers. Overall, we have achieved similar performance to previous work, which shows\nthe success of our replication and establishes a solid foundation for our ensemble research.\nRun\nSource\nF1\nOrdered Neurons\nmean F1 = 47.7, max F1 = 49.4 reported in Shen et al. (2019)\n1\nOur replication using the original codebase8 (seed = 0017)\n44.8\n2\nOur replication using the original codebase8 (seed = 0031)\n32.9\n3\nParsed data available in Kim et al. (2019a)9\n50.0\n4\nOur replication using the original codebase8 (seed = 7214)\n47.8\n5\nOur replication using the original codebase8 (seed = 1111)\n45.9\nNeural PCFG\nmean F1 = 50.8, max F1 = 52.6 reported in Kim et al. (2019a)\n1\nOur replication using the original codebase9 (seed = 3435)\n48.4\n2\nParsed data available in the original codebase9\n52.6\n3\nOur replication using the original codebase9 (seed = 1234)\n52.1\n4\nOur replication using the original codebase9 (seed = 1313)\n52.3\n5\nOur replication using the original codebase9 (seed = 5555)\n49.8\nCompound PCFG\nmean F1 = 50.8, max F1 = 52.6 reported in Kim et al. (2019a)\n1\nParsed data available in the original codebase9\n60.1\n2\nOur replication using the original codebase9 (seed = 3435)\n53.9\n3\nOur replication using the original codebase9 (seed = 1234)\n55.4\n4\nOur replication using the original codebase9 (seed = 0887)\n53.2\n5\nOur replication using the original codebase9 (seed = 0778)\n55.0\nDIORA\nmean F1 = 56.8 reported in Drozdov et al. (2019)\n1\nThe mlp-softmax checkpoint available on the original codebase10\n55.4\n2\nOur replication using the original codebase10 (seed = 0035)\n59.4\n3\nOur replication using the original codebase10 (seed = 0074)\n60.3\n4\nOur replication using the original codebase10 (seed = 1313)\n60.5\n5\nOur replication using the original codebase10 (seed = 5555)\n58.9\nS-DIORA\nmean F1 = 57.6, max F1 = 64.0 reported in Drozdov et al. (2020)\n1\nOur replication using the original codebase11(seed = 1943591871)\n56.3\n2\nOur replication using the original codebase11 (seed = 0315)\n60.0\n3\nOur replication using the original codebase11 (seed = 0075)\n58.9\n4\nOur replication using the original codebase11 (seed = 1313)\n54.7\n5\nOur replication using the original codebase11 (seed = 442597220)\n54.9\nConTest\nmean F1 = 62.8, max F1 = 65.9 reported in Cao et al. (2020)\n1\nA checkpoint provided by the authors through personal email\n65.9\n2\nOur replication using the original codebase12 (id = 0)\n61.6\n3\nParsed data provided by the authors through personal email\n62.3\n4\nOur replication using the original codebase12 (id = 1)\n63.0\n5\nOur replication using the original codebase12 (id = 2)\n61.8\nContexDistort13\nF1 = 49.0 reported in Li & Lu (2023)\n1\nOur replication using the original codebase14 on 10th layer of “bert-base-cased”\n48.8\n2\nOur replication using the original codebase14 on 12th layer of “bert-base-cased”\n46.6\n3\nOur replication using the original codebase14 on 11th layer of “bert-base-cased”\n48.7\n4\nOur replication using the original codebase14 on 8th layer of “bert-base-cased”\n46.9\n5\nOur replication using the original codebase14 on 9th layer of “bert-base-cased”\n48.1\nTable 6: F1 scores are on PTB test for different teachers in different runs. Note that the runs were\nrandomly shuffled for the randomized experiment.\n8https://github.com/yikangshen/Ordered-Neurons\n9https://github.com/harvardnlp/compound-pcfg\n10https://github.com/iesl/diora\n11https://github.com/iesl/s-diora\n12https://github.com/stevenxcao/constituency-test-parser\n13Given a pretrained language model, ContexDistort is a deterministic algorithm. Therefore, we used differ-\nent layers of the language model as runs to obtain different results.\n14https://github.com/jxjessieli/contextual-distortion-parser\n18\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-10-03",
  "updated": "2024-04-26"
}