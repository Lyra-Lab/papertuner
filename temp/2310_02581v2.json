{
  "id": "http://arxiv.org/abs/2310.02581v2",
  "title": "Online Estimation and Inference for Robust Policy Evaluation in Reinforcement Learning",
  "authors": [
    "Weidong Liu",
    "Jiyuan Tu",
    "Xi Chen",
    "Yichen Zhang"
  ],
  "abstract": "Reinforcement learning has emerged as one of the prominent topics attracting\nattention in modern statistical learning, with policy evaluation being a key\ncomponent. Unlike the traditional machine learning literature on this topic,\nour work emphasizes statistical inference for the model parameters and value\nfunctions of reinforcement learning algorithms. While most existing analyses\nassume random rewards to follow standard distributions, we embrace the concept\nof robust statistics in reinforcement learning by simultaneously addressing\nissues of outlier contamination and heavy-tailed rewards within a unified\nframework. In this paper, we develop a fully online robust policy evaluation\nprocedure, and establish the Bahadur-type representation of our estimator.\nFurthermore, we develop an online procedure to efficiently conduct statistical\ninference based on the asymptotic distribution. This paper connects robust\nstatistics and statistical inference in reinforcement learning, offering a more\nversatile and reliable approach to online policy evaluation. Finally, we\nvalidate the efficacy of our algorithm through numerical experiments conducted\nin simulations and real-world reinforcement learning experiments.",
  "text": "Online Estimation and Inference for Robust Policy\nEvaluation in Reinforcement Learning\nWeidong Liu∗Jiyuan Tu† Xi Chen‡ Yichen Zhang§\nAbstract\nReinforcement learning has emerged as one of the prominent topics attracting attention in\nmodern statistical learning, with policy evaluation being a key component. Unlike the tradi-\ntional machine learning literature on this topic, our work emphasizes statistical inference for\nthe model parameters and value functions of reinforcement learning algorithms. While most\nexisting analyses assume random rewards to follow standard distributions, we embrace the con-\ncept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier\ncontamination and heavy-tailed rewards within a unified framework. In this paper, we develop\na fully online robust policy evaluation procedure, and establish the Bahadur-type representation\nof our estimator. Furthermore, we develop an online procedure to efficiently conduct statisti-\ncal inference based on the asymptotic distribution. This paper connects robust statistics and\nstatistical inference in reinforcement learning, offering a more versatile and reliable approach to\nonline policy evaluation. Finally, we validate the efficacy of our algorithm through numerical\nexperiments conducted in simulations and real-world reinforcement learning experiments.\nKeywords: Statistical inference; dependent samples; online policy evaluation; robust inference;\nBahadur representation.\n∗School of Mathematical Sciences and MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University\n†School of Mathematics, Shanghai University of Finance and Economics\n‡Stern School of Business, New York University.\n§Daniels School of Business, Purdue University\n1\narXiv:2310.02581v2  [stat.ML]  1 Mar 2025\n1\nIntroduction\nReinforcement learning has offered immense success and remarkable breakthroughs in a variety of\napplication domains, including autonomous driving, precision medicine, recommendation systems,\nand robotics (to name a few, e.g., Murphy, 2003; Kormushev et al., 2013; Mnih et al., 2015; Shi\net al., 2018). From recommendation systems to mobile health (mHealth) intervention, reinforcement\nlearning can be used to adaptively make personalized recommendations and optimize intervention\nstrategies learned from retrospective behavioral and physiology data. While the achievements of\nreinforcement learning algorithms in applications are undisputed, the reproducibility of its results\nand reliability is still in many ways nascent.\nThose recommendation and health applications enjoy great flexibility and affordability due to\nthe development of reinforcement algorithms, despite calling for critical needs for a reliable and\ntrustworthy uncertainty quantification for such implementation. The reliability of such implementa-\ntions sometimes plays a life-threatening role in emerging applications. For example, in autonomous\ndriving, it is critical to avoid deadly explorations based upon some uncertainty measures in the\ntrial-and-error learning procedure.\nThis substance also extends to other applications including\nprecision medicine and autonomous robotics. From the statistical perspective, it is important to\nquantify the uncertainty of a point estimate with complementary hypothesis testing to reveal or\njustify the reliability of the learning procedure.\nPolicy evaluation plays a cornerstone role in typical reinforcement learning (RL) algorithms\nsuch as Temporal Difference (TD) learning. As one of the most commonly adopted algorithms for\npolicy evaluation in RL, TD learning provides an estimator of the value function iteratively with\nregard to a given policy based on samples from a Markov chain. In large-scale RL tasks where the\nstate space is infinitely expansive, a typical procedure to provide a scalable yet efficient estimation\nof the value function is via linear function approximation. This procedure can be formulated in a\nlinear stochastic approximation problem (Sutton, 1988; Tsitsiklis and Van Roy, 1997; Sutton et al.,\n2009; Ramprasad et al., 2023), which is designed to sequentially solve a deterministic equation\nAθ = b by a matrix-vector pair of a sequence of unbiased random observations of (At, bt) governed\nby an ergodic Markov chain.\n2\nThe earliest and most prototypical stochastic approximation algorithm is the Robbins-Monro\nalgorithm introduced by Robbins and Monro (1951) for solving a root-finding problem, where the\nfunction is represented as an expected value, e.g., E[f(θ)] = 0. The algorithm has generated pro-\nfound interest in the field of stochastic optimization and machine learning to minimize a loss function\nusing random samples. When referring to an optimization problem, its first-order condition can\nbe represented as E[f(θ)] = 0, and the corresponding Robbins-Monro algorithm is often referred\nto as first-order methods, or more widely known as stochastic gradient descent (SGD) in machine\nlearning literature. It is well established in the literature that its averaged version (Ruppert, 1988;\nPolyak and Juditsky, 1992), as an online first-order method, achieves optimal statistical efficiency\nwhen estimating the model parameters in statistical models, which apparently kills the interest in\ndeveloping second-order methods that use additional information to help the convergence. That\nbeing said, it is often observed in practice that first-order algorithms entail significant accuracy\nloss on non-asymptotic convergence as well as severe instability in the choice of hyperparameters,\nspecifically, the stepsizes (a.k.a. learning rate). In addition, the stepsize tuning further compli-\ncates the quantification of uncertainty associated with the algorithm output. Despite the known\ndrawbacks above, first-order stochastic methods are historically favored in machine learning tasks\ndue to their computational efficiency while they primarily focus on estimation. On the other hand,\nwhen the emphasis of the task lies on statistical inference, certain computation of the second-order\ninformation is generally inevitable during the inferential procedure, which shakes the supremacy of\nfirst-order methods over second-order algorithms.\nIn light of that, we propose a second-order online algorithm which utilizes second-order infor-\nmation to perform the policy evaluation sequentially. Meanwhile, our algorithm can be used for\nconducting statistical inference in an online fashion, allowing for the characterization of uncertainty\nin the estimation of the value function. Such a procedure generates no extra per-unit computation\nand storage cost beyond O(d2), which is at least the same as typical first-order stochastic methods\nfeaturing statistical inference (see, e.g., Chen et al., 2020 for SGD and Ramprasad et al., 2023\nfor TD). More importantly, we show theoretically that the proposed algorithm converges faster\nin terms of the remainder term compared with first-order stochastic approximation methods, and\nrevealed significant discrepancies in numerical experiments. In addition, the proposed algorithm is\n3\nfree from tuning stepsizes, which has been well-established as a substantial criticism of first-order\nalgorithms.\nAnother challenge to the reliability of reinforcement learning algorithms lies in the modeling\nassumptions. Most algorithms in RL have been in the optimism in the face of uncertainty paradigm\nwhere such procedures are vulnerable to manipulation (see some earlier exploration in e.g., Everitt\net al., 2017; Wang et al., 2020). In practice, it is often unrealistic to believe that rewards on the\nentire trajectory follow exactly the same underlying model. Indeed, non-standard behavior of the\nrewards happens from time to time in practice. The model-misspecification and presence of outliers\nare indeed very common in an RL environment, especially that with a large time horizon T. It\nis of substantial interest to design a robust policy evaluation procedure. In pursuit of this, our\nproposed algorithm uses a smoothed Huber loss to replace the least-squares loss function used in\nclassical TD learning, which is tailored to handle both outliers and heavy-tailed rewards. To model\noutlier observations of rewards in reinforcement learning, we bring the static α-contamination model\n(Huber, 1992) to an online setting with dependent samples. In a static offline robust estimation\nproblem, one aims to learn the distribution of interest, where a sample of size n is drawn i.i.d. from\na mixture distribution (1−αn)P +αnQ, and Q denotes an arbitrary outlier distribution. We adopt\nrobust estimation in an online environment, where the observations are no longer independent, and\nthe occurrence time of outliers is unknown. In contrast to the offline setting, future observations\ncannot be used in earlier periods in an online setting.\nTherefore, in the earlier periods, there\nis very limited information to help determine whether an observation is an outlier. In addition\nto this discrepancy between the online decision process and offline estimation, we further allow\nthe outlier reward models to be potentially different for different time t instead of being from a\nfixed distribution, and such rewards may be arbitrary and even adversarially adaptive to historical\ninformation. In addition to the outlier model, our model also incorporates rewards with heavy-\ntailed distributions. This substantially relaxes the boundedness condition on the reward functions\nthat is common in policy evaluation literature.\nWe summarize the challenges and contributions of this paper in the following facets.\n• We propose a fully online method that leverages dependent samples to simultaneously per-\n4\nform policy evaluation and conduct statistical inference on the model parameters and value\nfunctions. Furthermore, we build a Bahadur-type representation of the proposed estimator,\nwhich includes the main term corresponding to the asymptotic normal distribution and a\nhigher-order remainder term. Moreover, it shows that our algorithm matches the offline or-\nacle and converges strictly faster to the asymptotic distribution than that of a prototypical\nfirst-order stochastic method such as TD learning.\n• Compared to existing reinforcement learning literature, our proposed algorithm features an\nonline generalization of the αn-contamination model where the rewards contain outliers or\narbitrary corruptions. Our proposed algorithm is robust to adversarial corruptions which can\nbe adaptive to the trajectory, as well as heavy-tailed distribution of the rewards. Due to the\nexistence of outliers, we use a smooth Huber loss where the thresholding parameter is carefully\nspecified to change over time to accommodate the online streaming data. From a theoretical\nstandpoint, a robust policy evaluation procedure forces the update step from θt to θt+1 to be\na non-linear function of θt, which brings in additional technical challenges compared to the\nanalysis of classical TD learning algorithms (see e.g., Ramprasad et al., 2023) based on linear\nstochastic approximation.\n• Our proposed algorithm is based on a dedicated averaged version of the second-order method,\nwhere in each iteration a surrogate Hessian is obtained and used in the update step. This\nsecond-order information enables the proposed algorithm to be free from stepsize tuning\nwhile still ensuring efficient implementation. Furthermore, our proposed algorithm stands\nout distinctly from conventional first-order stochastic approximation approaches which fall\nshort of attaining the optimal offline remainder rate. On the other hand, while deterministic\nsecond-order methods do excel in offline scenarios, they lack the online adaptability crucial\nfor real-time applications.\n1.1\nRelated Works\nConducting statistical inference for model parameters in stochastic approximation has attracted\ngreat interest in the past decade, with a building foundation of the asymptotic distribution of the\n5\naveraged version of stochastic approximation first established in Ruppert (1988); Polyak and Judit-\nsky (1992). The established asymptotic distribution has been brought to conduct online inference.\nFor example, Fang et al. (2018) presented a perturbation-based resampling procedure. Chen et al.\n(2020) proposed two online procedures to estimate the asymptotic covariance matrix to conduct in-\nference. Chen et al. (2021c) studied inference for zeroth-order stochastic approximation. Shi et al.\n(2021a) developed online inference procedures for high-dimensional problems. Other than those\nfocused on the inference procedures, Shao and Zhang (2022) established the rate of convergence in\nthe remainder term, and utilize it to establish Berry-Esseen bounds of the averaged SGD estima-\ntor. Givchi and Palhang (2015); Mou et al. (2021) analyzed averaged SGD with Markovian data.\nSecond-order stochastic algorithms were analyzed in Ruppert (1985); Schraudolph et al. (2007);\nByrd et al. (2016) and applied to TD learning in Givchi and Palhang (2015).\nUnder the online decision-making settings including bandit algorithms and reinforcement learn-\ning, a few existing works focused on statistical inference of the model parameters or uncertainty\nquantification of value functions. Deshpande et al. (2018); Hadad et al. (2021); Zhang et al. (2021,\n2022) studied statistical inference with adaptively sampled data. Zhan et al. (2021); Chen et al.\n(2021a,b); Dimakopoulou et al. (2021); Chen et al. (2022); Han et al. (2022) proposed online in-\nference procedures for bandit algorithms particularly. Shen et al. (2024) studied optimal policy\nevaluation and constructed valid confidence intervals for the value of the optimal policy.\nFor\nreinforcement learning algorithms, Thomas et al. (2015) proposed high-confidence off-policy eval-\nuation based on Bernstein inequality. Hanna et al. (2017) presented two bootstrap methods to\ncompute confidence bounds for off-policy value estimates. Dai et al. (2020); Feng et al. (2021)\nconstruct confidence intervals for value functions based on optimization formulations, and Jiang\nand Huang (2020) derived a minimax value interval, both with i.i.d. sample. Shi et al. (2021b)\nproposed inference procedures for Q functions in RL via sieve approximations. Hao et al. (2021)\nstudied multiplier bootstrap algorithms to offer uncertainty quantification for exploration in fitted\nQ-evaluation. Syrgkanis and Zhan (2023) studied a re-weighted Z-estimator on episodic RL data\nand conducted inference on the structural parameter.\nThe most relevant literature to ours is Ramprasad et al. (2023), who studied a bootstrap online\nstatistical inference procedure under Markov noise using a quadratic SGD and demonstrated its\n6\napplication in the classical TD (and Gradient TD) algorithms in RL. Our proposed procedure and\nanalysis differ in at least two aspects. First, our proposed estimator is a Newton-type second-order\napproach that enjoys a faster convergence and optimality in the remainder rate. In addition, we\nshow both analytically and numerically that the computation cost of our procedure is typically\nlower than Ramprasad et al. (2023) for an inference task. Second, our proposed algorithm is a\nrobust alternative to TD algorithms, featuring a non-quadratic loss function to handle the potential\noutliers and heavy-tailed rewards. There exists limited RL literature on either outliers or heavy-\ntailed rewards. Recent works Li and Sun (2023); Zhu et al. (2024) studied online linear stochastic\nbandits and offline reinforcement learning in the presence of heavy-tailed rewards. However, their\nstudies do not apply to online statistical inference.\n1.2\nPaper Organization and Notations\nThe remainder of this paper is organized as follows. In Sections 2 and 3, we present and discuss our\nproposed algorithm for robust policy evaluation in reinforcement learning. Theoretical results on\nconvergence rates, asymptotic normality, and the Bahadur representation are presented in Section\n3. In Section 4, we develop an estimator for the long-run covariance matrix to construct confidence\nintervals in an online fashion and provide its theoretical guarantee. Simulation experiments are\nprovided in Section 5 to demonstrate the effectiveness of our method. Concluding remarks are\ngiven in Section 6. All proofs are deferred to the supplementary material.\nFor every vector v = (v1, ..., vd)⊤, denote |v|2 =\nqPd\nl=1 v2\nl , |v|1 = Pd\nl=1 |vl|, and |v|∞=\nmax1≤l≤d |vl|. For simplicity, we denote Sd−1 and Bd as the unit sphere and unit ball in Rd centered\nat 0. Moreover, we use supp(v) = {1 ≤l ≤d | vl ̸= 0} as the support of the vector v. For every\nmatrix A ∈Rd1×d2, define ∥A∥= sup|v|2=1 |Av|2 as the matrix operator norms, Λmax(A) and\nΛmin(A) as the largest and smallest singular values of A respectively.\nThe symbols ⌊x⌋(⌈x⌉)\ndenote the greatest integer (the smallest integer) not larger than (not less than) x. We denote\n(x)+ = max(0, x). For two sequences an, bn, we say an ≍bn when an = O(bn) and bn = O(an)\nhold at the same time. We say an ≈bn if limn→∞an/bn = 1. For a sequence of random variables\n{Xn}∞\nn=1, we denote Xn = OP(an) if there holds limC→∞lim supn→∞P(|Xn| > Can) = 0, and\n7\ndenote Xn = oP(an) if there holds limC→0 lim supn→∞P(|Xn| > Can) = 0. Lastly, the generic\nconstants are assumed to be independent of n and d.\n2\nOnline Robust Policy Evaluation in Reinforcement Learning\nWe first review the Least-squares temporal difference methods in RL. Consider a 4-tuple (S, U, P, R).\nHere S = {1, 2, ..., N} is the global finite state space, U is the set of control (action), P is the transi-\ntion kernel, and R is the reward function. One of the core steps in RL is to estimate the cumulative\nreward J∗(which is also called the state value function) for a given policy:\nJ∗(s) = E\nh ∞\nX\nk=0\nγkR(sk)\n\f\f\f s0 = s\ni\n,\nwhere γ ∈[0, 1) is a given discount factor and s ∈S is any state. Here {sk} denote the environment\nstates which are usually modeled by a Markov chain. In real-world RL applications, the state space\nis often very large such that one cannot directly compute value estimates for every state in the\nstate space. A common approach in the modern RL is to approximate the value function J∗(·),\ni.e., let\neJ(s, θ) = θ⊤ϕ(s) =\nd\nX\nl=1\nθlϕl(s),\nbe a linear approximation of J∗(·), where θ = (θ1, ..., θd)⊤∈Rd contains the model parameters, and\nϕ(s) = (ϕ1(s), ..., ϕd(s))⊤∈Rd is a set of feature vectors that corresponds to the state s ∈S. Here\nwe write ϕl = (ϕl(1), ..., ϕl(N))⊤, 1 ≤l ≤d are linearly independent vectors in RN and d ≪N.\nThat is, we use a low dimensional linear approximation (with the basis vectors {ϕ1, ..., ϕd}) for\nJ∗(·). Let the matrix Φ = (ϕ1, ..., ϕd) and then eJ = Φθ.\nThe state value function J∗satisfies the Bellman equation\nJ∗(s) = R(s) + γE\nh ∞\nX\nk=1\nγk−1R(sk)\n\f\f\fs0 = s\ni\n= R(s) + γ\nN\nX\ns′=1\npss′J∗(s′),\n(1)\nwhere s′ is the next state transferred from s. Let P = (pss′)N×N be the probability transition\nmatrix and define the Bellman operator T by T (Q) = R + γPQ. The state function J∗is the\n8\nunique fixed point of the Bellman operator T . When J∗is replaced by eJ = Φθ, the Bellman\nequation may not hold.\nThe classical temporal difference attempts to find θ such that eJ(s, θ) well approximates the\nvalue function J∗(s). Particularly, the TD algorithm updates\nθt+1 = θt −ηtϕ(st)[(ϕ⊤(st) −γϕ⊤(st+1))θt −R(st)],\nt ≥0,\n(2)\nwhere ηt is the step size which often requires careful tuning. We next illustrate the key point that\n(2) leads to a good estimator such that eJ(s, θt) is close to J∗(s). It can be shown that θt converges\nto an unknown population parameter θ∗that minimizes the expected squared difference of the\nBellman equation,\nθ∗= argmin\nu∈Rd E|ϕ⊤(s)u −(R(s) + γϕ⊤(s′)θ∗)|2;\n(3)\nIt is easy to see that such θ∗exists and satisfies the following first-order condition,\nEϕ(s)[(ϕ⊤(s) −γϕ⊤(s′))θ∗−R(s)] = 0.\n(4)\nUnder the condition that H := Eϕ(s)(ϕ⊤(s) −γϕ⊤(s′)) is positive definite in the sense that\nx⊤Hx > 0 for all x ∈Rd; see Tsitsiklis and Van Roy (1997), the solution of (4) writes\nθ∗= (Eϕ(s)(ϕ⊤(s) −γϕ⊤(s′)))−1Eϕ(s)R(s).\nThe estimation equation (2) is a first-order stochastic algorithm that converges to the stochastic\nroot-finding problem (4) using a sequence of observations {st, rt}t≥1. In this paper, we refer to it\nas the Least-squares temporal difference estimator. See Kolter and Ng (2009) for details on the\nproperties of the Least-squares TD estimator.\nLeast-squares-based methods are oftentimes criticized due to their sensitivity to outliers in\ndata. When there may exist outliers in some observations of the reward R(s), it is a natural call\nfor interest to design a robust estimator of θ∗.\nFollowing the widely-known classical literature\n(Huber, 1964; Charbonnier et al., 1994, 1997; Hastie et al., 2009) , we replace the square loss | · |2\n9\nby a smoothed (Pseudo) Huber loss fτ(x) = τ 2(\np\n1 + (x/τ)2 −1), parametrized by a thresholding\nparameter τ. We define a similar fixed point equation with the smoothed Huber loss by\nθ∗\nτ = argmin\nu∈Rd Efτ\n\u0000ϕ⊤(s)u −(R(s) + γϕ⊤(s′)θ∗\nτ)\n\u0001\n.\n(5)\nIn this section, when we motivate the algorithm, we assume that the fixed point θ∗\nτ exists1. As\nthe thresholding parameter τ goes to infinity, the objective equation in (5) is close to the least-\nsquares loss in (3), and θ∗\nτ should be close to θ∗. When τ tends to 0, the problem (5) becomes\nθ∗\n0 = argminu∈Rd E|ϕ⊤(s)u−(R(s)+γϕ⊤(s′)θ∗\n0)|, with a nonsmooth least absolute deviation (LAD)\nloss, which is out of the scope of this paper. In this paper, we carefully specify τ to balance the\nstatistical efficiency and the effect of potential outliers in an online fashion (see Theorem 1).\nBy the first-order condition of (5), we obtain a similar estimation function as (4),\nE\n\u0002\nϕ(s)gτ\n\u0000(ϕ⊤(s) −γϕ⊤(s′))θ∗\nτ −R(s)\n\u0001\u0003\n= 0,\n(6)\nwhere the function gτ(x) = f′\nτ(x) is the differential of the smoothed Huber loss fτ(x). Instead of\nusing the first-order iteration with the estimation equation (6) as in TD (2), we propose a Newton-\ntype iterative estimator, which avoids the tuning of the learning rate. Newton-type estimators are\noften referred to as second-order methods when discussed in convex optimization. Nonetheless, the\nequation (5) is just for illustration purposes, which cannot be directly optimized since θ∗\nτ appears\nin the objective function for minimization. On the contrary, our proposed method is a Newton-type\nmethod for solving the root-finding problem (6) using a sequence of the observations.\nIn this paper, we model two types of noise in observed rewards. The first is the classical Huber\ncontamination model (Huber, 1992, 2004), where an αn-fraction of rewards comes from arbitrary\ndistributions. The second is the heavy-tailed model, where the reward function may admit heavy-\ntailed distributions. In the following section, we show that our proposed estimator is robust to the\naforementioned two noises.\n1Here θ∗\nτ is used for the motivation only. Our algorithm and theory do not depend on the existence of θ∗\nτ.\n10\n3\nOnline Newton-type Method for Parameter Estimation\nIn the following, we introduce an online Newton-type method for estimating the parameter θ∗in the\npresence of outliers and heavy-tailed noise in the reward function R(s). For ease of presentation,\nwe denote the observations by Xi = ϕ(si), Zi = ϕ(si) −γϕ(si+1), and bi = R(si), where si is the\nstate at time i. Here (Xi, Zi, bi) ∈Rd × Rd × R for each observation in the sample. Our objective\nis to propose an online estimator by the estimation function (6), which can be rewritten as\nE\n\u0002\nXgτ(Z⊤θ∗\nτ −b)\n\u0003\n= 0,\n(7)\nbased on a sequence of dependent observations {(Xi, Zi, bi)}i≥1.\nAt iteration n + 1, our proposed Newton-type estimator updates bθn+1 by\nbθn+1 =\n1\nn + 1\nn\nX\ni=0\nbθi −c\nH\n−1\nn+1\n1\nn + 1\nn\nX\ni=0\nXi+1gτi+1(Z⊤\ni+1bθi −bi+1).\n(8)\nHere c\nHn+1 is an empirical information matrix of the estimation equation (7), as\nc\nHn+1 =\n1\nn + 1\nn\nX\ni=0\nXi+1Z⊤\ni+1g′\nτi+1(Z⊤\ni+1bθi −bi+1).\n(9)\nwhere τi is the thresholding parameter in the Huber loss. We let τn tend to infinity to eliminate\nthe bias generated by the smoothed Huber loss.\nIt is noteworthy to mention that the matrix c\nHn+1 is not the Hessian matrix of the objective\nfunction on the right-hand side of (5). As discussed in the previous section, (5) cannot be directly\noptimized, nor do they lead to M-estimation problems.\nIndeed, our proposed update (8) is a\nNewton-type method to find the root of (7) using a sequence of observations {(Xi, Zi, bi)}i≥1. In\nthe following section, we will show the use of the matrix c\nHn+1 helps for desirable convergence\nproperties.\nIt should be noted that (8) can be implemented efficiently in a fully-online manner, i.e., without\nstorage of the trajectory of historical information. Specifically, we write (8) as bθn+1 = θn+1 −\nc\nH\n−1\nn+1Gn+1, with each of the item on the right-hand side being a running average. It is easy to see\n11\nAlgorithm 1 Robust Online Policy Evaluation (ROPE).\nInput: Online streaming data {(Xi, Zi, bi) | i ≥1}\n1: Compute c\nH\n−1\nn0 and Gn0 according to (12) respectively.\n2: for n = n0 + 1, n0 + 2, . . . do\n3:\nSpecify the thresholding parameter τn.\n4:\nCompute θn = (n −1)θn−1/n + bθn−1/n, and (c\nH\n−1\nn , Gn) by (10) and (11).\n5:\nUpdate the parameter by\nbθn = θn −c\nH\n−1\nn Gn.\n6: end for\nthat the averaged estimator θn+1 =\n1\nn+1(nθn + bθn) and the vector\nGn+1 =\n1\nn + 1\nn\nX\ni=0\nXi+1gτi+1(Z⊤\ni+1bθi −bi+1)\n(10)\n=\nn\nn + 1Gn +\n1\nn + 1Xn+1gτn+1(Z⊤\nn+1bθn −bn+1)\ncan both be updated online. In addition, the inverse c\nH\n−1\nn+1 can be directly and efficiently computed\nonline by the inverse recursion formulation. By the Sherman-Morrison formula, we have\nc\nH\n−1\nn+1 =n + 1\nn\nc\nH\n−1\nn −n + 1\nn2\nc\nH\n−1\nn Xn+1\n×\nh 1\nnZ⊤\nn+1c\nH\n−1\nn Xn+1 + {g′\nτn+1(Z⊤\nn+1bθn −bn+1)}−1i−1\nZ⊤\nn+1c\nH\n−1\nn .\n(11)\nHere we note that both terms 1\nnZ⊤\nn+1c\nH\n−1\nn Xn+1 and {g′\nτn+1(Z⊤\nn+1bθn −bn+1)}−1 inside the brackets\non the right-hand side of (11) are scalars in R1, not matrices.\nThe complete algorithm is presented in Algorithm 1. We refer to it as the Robust Online Policy\nEvaluation (ROPE). Compared with existing stochastic approximation algorithms for TD learning\n(Durmus et al., 2021; Mou et al., 2021; Ramprasad et al., 2023), our ROPE algorithm does not\nneed to tune the step size.\nSince performing iterations of c\nH\n−1\nn+1 in (11) requires an initial invertible c\nHn0, we shall compute\n12\nfrom the first n0 samples that\nc\nHn0 = 1\nn0\nn0\nX\ni=1\nXiZ⊤\ni g′\nτ0(Z⊤\ni bθ0 −bi),\nGn0 = 1\nn0\nn0\nX\ni=1\nXigτ0(Z⊤\ni bθ0 −bi),\n(12)\nwhich serves as the initial quantities of (10)–(11) in order to compute the forthcoming iterations.\nHere bθ0 is a given initial parameter, and τ0 is a specified initial threshold level.\n3.1\nConvergence Rate of ROPE\nBefore illustrating how to conduct statistical inference on the parameter θ∗, we first provide theo-\nretical results for our proposed ROPE method.\nTo characterize the weak dependence of the sequence of environment states, we use the concept\nof ϕ-mixing. More precisely, we assume {si, i ≥1} is a sequence of ϕ-mixing dependent variables,\nwhich satisfy\n|P(B|A) −P(B)| ≤ϕ(k)\n(13)\nfor all A ∈Fn\n1 , B ∈F∞\nn+k and all n, k ≥1, where Fb\na = σ(si, a ≤i ≤b). The ϕ-mixing dependence\ncovers the irreducible and aperiodic Markov chain, which is typically used to model the states\nsampling in reinforcement learning (RL) (Rosenblatt, 1956; Rio, 2017, and others). The mixing\nrate ϕ(k) identifies the strength of dependence of the sequence.\nIn this paper, we assume the\nfollowing condition on the mixing rate.\nCondition (C1). The sequence {(Xi, Zi)} is a stationary ϕ-mixing sequence, where the mixing\nrate satisfies ϕ(k) = O(ρk) for some 0 < ρ < 1.\nCondition (C1) holds when {si} is an irreducible and aperiodic Markov chain with finite state\nspace (Tsitsiklis and Van Roy, 1997). While the main techniques of our analysis could be extended\nto accommodate infinite and even continuous state spaces, we restrict our study to finite state\nspaces, since our proposed robust estimator targets θ∗, the fixed point of the projected Bellman\nequation. The deviation of this fixed point from that of the original Bellman equation, known as the\napproximation error, is well understood in the literature (Tsitsiklis and Van Roy, 1997; Bhandari\net al., 2018) for finite state spaces in TD learning. In contrast, the approximation error under\n13\ncontinuous state spaces introduce additional challenges. Since our primary focus is inference for\nonline policy evaluation under heavy-tailed rewards and outliers, we concentrate on establishing\nthe statistical properties of the proposed estimator in the finite-state setting.\nIn addition, we also assume the following boundedness condition on X = ϕ(s) that is commonly\nrequired by the RL literature (Sutton and Barto, 2018; Ramprasad et al., 2023).\nCondition (C2). There exists a constant CX > 0 such that max\n\b\n|ϕ(s)|2\n\f\f s ∈S\n\t\n≤CX.\nCondition (C2) imposes a uniform bound on the feature space that is independent of d, which\nensures that our theoretical bounds scale appropriately with the feature dimension.\nNext, we\nassume the matrix H = E[XZ⊤] has a bounded condition number.\nCondition (C3). There exists a constant c > 0 such that c ≤Λmin(H) ≤Λmax(H) ≤c−1, where\nΛmax and Λmin denote the largest and the smallest singular values, respectively.\nWhen {si} is an irreducible and aperiodic Markov chain, Tsitsiklis and Van Roy (1997) proved\nthat H is positive definite, i.e., x⊤Hx > 0 for all x ̸= 0. This indicates that H is nonsingular and\nhence (C3) holds. Finally, we provide conditions on the temporal difference error Z⊤θ∗−b.\nCondition (C4). The temporal-difference error Z⊤θ∗−b comes from the distribution (1−αn)P +\nαnQ, where αn ∈[0, 1).\nThe distribution Q is an arbitrary distribution, and P satisfies that\nEP\n\u0002\n|Z⊤θ∗−b|1+δ\f\fX, Z\n\u0003\n≤Cb holds uniformly for some constants δ > 0 and Cb > 0.\nWe assume that the temporal-difference error comes from the Huber contamination model. The\noutlier distribution Q can be arbitrary and the true distribution P has (1 + δ)-th order of moment\n(δ > 0), which does not necessarily indicate a variance exists when δ ∈(0, 1). Therefore, our\nassumption largely extends the boundedness condition of the reward function R(s) in RL literature\n(Sutton and Barto, 2018; Ramprasad et al., 2023). In Condition (C4), αn controls the ratio of\noutliers among n samples.\nParticularly, αn = mn/n means that there are mn outliers among\nn samples {(Xi, Zi, bi), 1 ≤i ≤n}. Given the above conditions, by selecting the thresholding\nparameter τi as defined in (8), we can obtain the convergence rate for all the intermediate iterates\nbθi.\n14\nTheorem 1. Suppose that (C1) to (C4) hold and the thresholding parameter τi = Cτ max(1, iβ1/(log i)β2)\n(where β1 ∈[0, 1), β2 ≥0, and Cτ > 0). Assume n0 is sufficiently large and the initial value\n|bθ0 −θ∗|2 ≤c0 for some c0 < 1. Then for every ν > 0, there exist constants C, c > 0 which are\nindependent of the dimension d such that\nP\n\u0010\n∩n\ni=n0\n\b\n|bθi −θ∗|2 ≥C\n√\ndei\n\t\u0011\n≥1 −cn−ν\n0 ,\nwhere\nen = αnτn + τ −min(δ,2)\nn\n+\ns\nτ (1−δ)+\nn\nlog n\nn\n+ τn log2 n\nn\n+ 1\n√\nd\n(c0)2n−n0.\n(14)\nHere δ is defined in the moment condition in (C4).\nThe error bounds en in (14) contains five terms. In the sequel, we will discuss each of these\nterms individually. The first term αnτn comes from the impact of outliers among the samples. Here\nthe contamination ratio αn = mn/n should tend to zero; otherwise, it is impossible to obtain a\nconsistent estimator for θ∗. The second term τ −min(δ,2)\nn\nis due to the bias from smoothed Huber’s\nloss for mean estimation. The third and the fourth term in (14) is the classical statistical rate due to\nthe variance and boundedness incurred by thresholding, respectively. These two terms commonly\nappear in the Bernstein-type inequalities (see, e.g. Bennett, 1962). As we can see from the third\nterm, the convergence rate has a phase transition between the regimes of finite variance δ > 1 and\ninfinite variance 0 < δ ≤1 (see also Corollary 3 below). This phenomenon has also been observed\nin different estimation problems with Huber loss; see Sun et al. (2020) and Fan et al. (2022). In\nthe presence of a higher moment condition, specifically δ ≥5, we can eliminate the fourth term by\napplying a more delicate analysis (See Proposition 5 below and Proposition 8 in the supplementary\nmaterial for more details). The last term represents how quickly the proposed iterative algorithm\nconverges. Given an initial value bθ0 with an error c0 < 1, the proposed second-order method enjoys\na local quadratic convergence, i.e., the error evolves from c0 to (c0)2n−n0 after n−n0 iterations. This\nterm decreases super-exponentially fast, a characteristic convergence rate often encountered in the\nrealm of second-order methods (see, e.g., Nesterov, 2003). Specifically, when the sample size, which\nis equivalent to the number of iterations, is reasonably large, the last term (c0)2n−n0 = O(1/√n) is\n15\ndominated by the statistical error in the other terms of (14). The assumption on the initial error\n|bθ0 −θ∗|2 ≤c0 for some c0 < 1 is mild. A valid initial value bθ0 can be obtained by the solution\nto the following estimation equation Pn0\ni=0 Xigτ(Z⊤\ni θ −bi) = 0 using a subsample with fixed size\nn0 before running Algorithm 1 with a specified constant threshold τ. The above equation can be\nsolved efficiently by classical first-order root-finding algorithms.\nTo highlight the relationship between the convergence rate and the outlier rewards, we present\nthe following corollary which gives a clear statement on how the rate depends on the number of\noutliers mn.\nCorollary 2. Suppose the conditions in Theorem 1 hold with δ ≥2. Let the thresholding parameter\nτi = Cτiβ with some β, Cτ > 0 and n0 tend to infinity. We have\n|bθn −θ∗|2 = OP\n \n√\nd\n\u0010r\nlog n\nn\n+ log2 n\nn1−β + mn\nn1−β +\n1\nn2β\n\u0011!\n,\nwhere mn is the number of outliers among the samples of size n.\nThe corollary shows that, when there are mn = o(n1−β) outliers, our ROPE can still estimate\nθ∗consistently. Note that β in τi is the exponent specified by the practitioner. A smaller β allows\nmore outliers mn among the samples. Furthermore, when there are mn = o(n\n1\n2 −β) outliers and\n1\n4 ≤β ≤1\n2, the ROPE estimator achieves the optimal rate |bθn −θ∗|2 = OP\n\u0010q\nd log n\nn\n\u0011\n, up to a\nlogarithm term.\nThe next corollary indicates the impact of the tail of rewards R(s) on the convergence rate.\nFor a clear presentation, we discuss the impact under the case without outliers, i.e., αn = 0.\nCorollary 3. Suppose the conditions in Theorem 1 hold with the contamination rate αn = 0 and\nlet n0 tend to infinity.\n• When δ ∈(0, 1], we specify τi = Cτ(i/ log i)1/(1+δ). Then\n|bθn −θ∗|2 = OP\n\u0012√\nd\n\u0010log n\nn\n\u0011\nδ\n1+δ\n\u0013\n.\n16\n• When δ > 1, we specify τi = Cτ(i/ log i)1/2. Then\n|bθn −θ∗|2 = OP\n\u0012√\nd\n\u0010log n\nn\n\u00111/2\u0013\n.\nCorollary 3 illustrates a sharp phase transition between the regimes of infinite variance δ ∈(0, 1]\nand finite variance δ > 1, and such transition is smooth and optimal. When δ ∈(0, 1], the optimal\nchocie of τi depends on δ. In practice, when δ is unknown, a classical inference method on the tail\nbehavior, such as the Hill estimator (Hill, 1975), can be employed to estimate δ. This estimation\ncan be conducted during an initial pilot stage of data collection using a constant thresholding\nparameter τ0.\nThe rate of convergence established in Corollary 3 matches the offline oracle with independent\nsamples in the estimation of linear regression model (Sun et al., 2020), ignoring the logarithm term.\nA similar corollary can be established for this phase transition of the convergence rate when the\ncontamination rate αn > 0.\n3.2\nAsymptotic Normality and Bahadur Representation\nIn the following theorem, we give the Bahadur representation for the proposed estimator bθn. The\nBahadur representation provides a more refined rate for the estimation error. Moreover, the asymp-\ntotic normality can be established by applying the central limit theorem to the main term in the\nrepresentation.\nTheorem 4. Suppose the conditions in Theorem 1 hold, and (C4) holds with δ ≥5. Let n0 tend\nto infinity. We have for any nonzero v ∈Rd with |v|2 ≤1,\nv⊤(bθn −θ∗) =v⊤H−1 1\nn\nX\ni/∈Qn\nXi(Z⊤\ni θ∗−bi)\n(15)\n+ OP\n\u0010√\ndτnαn + de2\nn−1 log2 n +\n√\ndτ −2\nn\n+\n√\nd\n(nτ 2n)2/5\n\u0011\n.\nHere en is defined in (14), and Qn denotes the index set of the outliers. Moreover, if the contami-\n17\nnation rate αn = o(1/(√nτn)) and n1/4 = o(τn), then\n√n\nσv\nv⊤(bθn −θ∗) d−→N(0, 1), where σ2\nv = v⊤H−1Σ(H⊤)−1v,\n(16)\nas n →∞. Here\nΣ =\n∞\nX\nk=−∞\nE\n\u0002\nX0X⊤\nk (Z⊤\n0 θ∗−b0)(Z⊤\nk θ∗−bk)\n\u0003\n.\n(17)\nTheorem 4 provides a Bahadur-type representation for the proposed estimator bθn, decompos-\ning it into a leading term representing an asymptotic linear expansion in (15) and a higher-order\nremainder term. This decomposition is instrumental in establishing asymptotic distribution in (16)\nand thereby enabling the construction of valid confidence intervals in an online setting, as demon-\nstrated in Section 4. Furthermore, in our framework, it facilitates the comparison of second-order\nconvergence rates of the proposed method with those of first-order methods, as we will discuss in\nRemark 1 below, and may serve as a foundation for Berry–Esseen-type bounds on the convergence\nrate of the distribution, which we leave as a promising direction for further investigation. Addi-\ntionally, this representation explicitly characterizes the orders of both the leading and remainder\nterms in relation to the model’s dimensionality. While the Bahadur representation was originally\nintroduced in the context of quantile regression, we adopt its use following the recent literature on\nHuber-type loss problems (Sun et al., 2020), motivated by its role in robustness in reinforcement\nlearning.\nTo achieve asymptotic normality, additional conditions αn = o(1/(√nτn)) and n1/4 = o(τn)\nare required on the specification of the thresholding parameters τn. These conditions easily hold\nwhen the practitioner specifies τn = Cτnβ with β > 1/4 and the number of outliers satisfies\nmn = o(n1/2−β).\nWe further note that to establish the Bahadur representation in the above\ntheorem, we require the sixth moment to exist (i.e., δ = 5). This condition may be weakened and\nwe leave this theoretical question to future investigation. It is worth noting that even for the case\nwhere there is no contamination, i.e., αn = 0, our result is still new. To our knowledge, there is no\nliterature that establishes the Bahadur representation for the TD method. To highlight the rate of\nconvergence concisely in the remainder term, we provide the following corollary under αn = 0.\n18\nProposition 5. Suppose the conditions of Theorem 4 hold with αn = 0 and τi = Cτiβ for β ≥3/4\nand some Cτ > 0, we have for any nonzero v ∈Rd with |v|2 ≤1,\nv⊤(bθn −θ∗) = v⊤H−1 1\nn\nn\nX\ni=1\nXi(Z⊤\ni θ∗−bi) + OP\n\u0010d log n\nn\n\u0011\n.\nIn this proposition, we specify β ≥3/4 to ensure that the remainder term possesses an order of\nOP\n\u0000d log n/n\n\u0001\n. Notably, this result is not a direct corollary of Theorems 1 and 4, since the rate of\nen−1 in (14) of Theorem 1 is not sharp enough to guarantee the term de2\nn−1 log2 n in (15) converges\nas fast as O\n\u0000d log n/n\n\u0001\nin Proposition 5. To reach this fast rate of the remainder, we establish an\nimproved rate of Theorem 1 under the stronger assumptions, achieved by eliminating the fourth\nterm of (14). The detailed formal result of the improved rate of en is relegated to Proposition 8 in\nthe supplementary material.\nRemark 1 (Acceleration of Convergence). We discuss the order of the remainder term in the\nBahadur representation and further demonstrate that our proposed method converges strictly faster\nto the asymptotic distribution than that of a prototypical first-order stochastic method such as TD\nlearning. Existing analysis on the TD learning (Ramprasad et al., 2023) does not provide a Bahadur\nrepresentation. Therefore, we compare it with its i.i.d. version instead. For estimators obtained\nwith i.i.d. sample using SGD, the proof of Polyak and Juditsky (1992) indicates that the remainder\nterm of SGD with learning cn−η, 0 < η ≤1 is at the order of OP(1/n1−η/2 + 1/nη), which is upper\nbounded by OP(n−2\n3 ), when the dimension d is fixed. Our proposed method achieves a strictly faster\nrate of remainder, O(n−1 log n).\n4\nEstimation of Long-Run Covariance Matrix and Online Statis-\ntical Inference\nIn the above section, we provide an online Newton-type algorithm to estimate the parameter θ∗.\nAs we can see in Theorem 4, the proposed estimator has an asymptotic variance with a sandwich\nstructure H−1Σ(H⊤)−1. To conduct statistical inference simultaneously with ROPE method we\nproposed in Section 3, we propose an online plug-in estimator for this sandwich structure. Note\n19\nthat the online estimator c\nH\n−1\nn\nof H−1 is proposed and utilized in (11) of the estimation algorithm.\nIt remains to construct an online estimator for Σ in (17).\nDeveloping an online estimator of Σ with dependent samples is intricate compared to the case\nof independent ones. As the samples are dependent, the covariance matrix Σ is an infinite sum of\nthe series of time-lag covariance matrices. To estimate the above long-run covariance matrix Σ, we\nfirst rewrite the definition of Σ in (17) into the following form\nΣ =E\n\u0002\nX0X⊤\n0 (Z⊤\n0 θ∗−b0)2\u0003\n+\n−1\nX\nk=−∞\nE\n\u0002\nX0X⊤\nk (Z⊤\n0 θ∗−b0)(Z⊤\nk θ∗−bk)\n\u0003\n+\n∞\nX\nk=1\nE\n\u0002\nX0X⊤\nk (Z⊤\n0 θ∗−b0)(Z⊤\nk θ∗−bk)\n\u0003\n.\nNext, we replace each term E\n\u0002\nX0X⊤\nk (Z⊤\n0 θ∗−b0)(Z⊤\nk θ∗−bk)\n\u0003\nin (17) with its empirical counterpart\nusing the n samples. Meanwhile, to handle outliers, we replace (Z⊤\ni θ∗−bi) by gτi(Z⊤\ni bθi−1 −bi).\nIn addition, the infinite sum in (17) is not feasible for direct estimation. Nonetheless, Condition\n(C1) on the mixing rate of {(Xt, Zt)} allows us to approximate it by effectively estimating the\nfirst ⌈λ log n⌉terms only, where λ is a pre-specified constant. In summary, we can construct the\nfollowing estimator for the covariance matrix Σ,\nbΣn = 1\nn\nn\nX\ni=1\nXiX⊤\ni\n\u0000gτi(Z⊤\ni bθi−1 −bi)\n\u00012\n(18)\n+ 1\nn\nn\nX\ni=1\n⌈λ log i⌉∧(i−1)\nX\nk=1\nXiX⊤\ni−kgτi(Z⊤\ni bθi−1 −bi)gτi−k(Z⊤\ni−kbθi−k−1 −bi−k)\n+ 1\nn\nn\nX\ni=1\n⌈λ log i⌉∧(i−1)\nX\nk=1\nXi−kX⊤\ni gτi−k(Z⊤\ni−kbθi−k−1 −bi−k)gτi(Z⊤\ni bθi−1 −bi).\nTo perform a fully-online update for the estimator bΣn, we define\nSj =\nj\nX\ni=1\nX⊤\ni gτi(Z⊤\ni bθi−1 −bi),\nj ≥1,\n20\nand the above equation (18) can be rewritten as,\nbΣn = 1\nn\nn\nX\ni=1\nXiX⊤\ni g2\nτi(Z⊤\ni bθi−1 −bi) + 1\nn\nn\nX\ni=1\nXigτi(Z⊤\ni bθi−1 −bi)(Si−1 −Si−⌈λ log i⌉∧(i−1))\n+ 1\nn\nn\nX\ni=1\n(Si−1 −Si−⌈λ log i⌉∧(i−1))⊤X⊤\ni gτi(Z⊤\ni bθi−1 −bi).\nIn practice, at the n-th step, we keep only the terms {Sn, ..., Sn−⌈λ log n⌉∧(n−1)} in memory, and the\ncovariance matrix can be updated by\nbΣn = 1\nn\nh\nXnX⊤\nn g2\nτn(Z⊤\nn bθn−1 −bn) + Xngτn(Z⊤\nn bθn−1 −bn)(Sn−1 −Sn−⌈λ log n⌉∧(n−1))\n+ (Sn−1 −Sn−⌈λ log n⌉∧(n−1))⊤X⊤\nn gτn(Z⊤\nn bθn−1 −bn)\ni\n+ n −1\nn\nbΣn−1.\n(19)\nThe proposed estimator bΣn complements the estimator c\nH\n−1\nn\nin (11) to construct an estimator of\nthe sandwich form H−1Σ(H⊤)−1 that appears in the asymptotic distribution in Theorem 4.\nSpecifically, we can construct the confidence interval for the parameter v⊤θ∗using the asymp-\ntotic distribution in (16) in the following way. For any unit vector v, a confidence interval with\nnominal level (1 −ξ) is\nh\nv⊤bθn −q1−ξ/2bσv, v⊤bθn + q1−ξ/2bσv\ni\n,\n(20)\nwhere bσ2\nv = v⊤c\nH\n−1\nn bΣn(c\nH\n⊤\nn )−1v and q1−ξ/2 denotes the (1 −ξ/2)-th quantile of a standard normal\ndistribution.\nRemark 2. We provide some insights into the computational complexity of our inference procedure.\nBoth estimators c\nH\n−1\nn\nand bΣn can be computed in an O(d2) per-iteration computation cost, as\ndiscussed in (11). In addition, the computation bσ2\nv = v⊤c\nH\n−1\nn bΣn(c\nH\n⊤\nn )−1v requires three vector-\nmatrix multiplication with the same computation complexity O(d2). On the contrary, the online\nbootstrap method proposed in Ramprasad et al. (2023) has a per-iteration complexity of at least\nO(Bd), where the resampling size B in the bootstrap is usually much larger than d in practice.\nThe construction in (20) serves also for the uncertainty quantification of the value function\nϕ(s)⊤θ∗if one specifies v = ϕ(s). Furthermore, the constructed confidence interval can be utilized\n21\nto implement early stopping, reducing costs associated with data collection. One potential approach\nis to adopt the early stopping rule outlined in Xia et al. (2023), which is based on predefined\ntolerance probability, stopping checkpoints and target accuracy level. This allows the robust policy\nevaluation algorithm to terminate efficiently once the desired precision is achieved, optimizing\nresource usage.\nIn the following theorem, we provide the theoretical guarantee of the confidence interval con-\nstruction by showing that our online estimator of the covariance matrix, bΣn, is consistent.\nTheorem 6. Suppose the conditions in Theorem 4 hold. The covariance estimator bΣn defined in\n(19) satisfies\n∥bΣn −Σ∥= OP\n \n√\ndτ 2\nnαn +\n√\ndτ −1\nn\n+ τn\nr\nd log n\nn\n+ dτ 2\nn log2 n\nn\n!\n.\nTheorem 6 provides an upper bound on the estimation error bΣn−Σ. To achieve the consistency\non bΣn, we specify the thresholding parameter τn = Cτnβ for 1/4 < β < 1/2, such that\n∥bΣn −Σ∥= OP\n\u0012√\ndτ 2\nnαn +\n√d log n\nn1/2−β\n\u0013\n.\nIn this case, as long as the fraction of outliers αn satisfies αn = o(1/(n2β√\nd)), we obtain a consistent\nestimator of the matrix Σ. In other words, our proposed robust algorithm allows o(n1−2β) outliers,\nignoring the dimension. We summarize the result in the following corollary.\nCorollary 7. Suppose the conditions of Theorem 4 hold, and the fraction of outliers satisfies\nαn = o(1/(n2β√\nd)), where we specify τi = Cτiβ and 1/4 < β < 1/2. Then given a vector v ∈Rd\nand a pre-specified confidence level 1 −ξ, we have\nlim\nn→∞P\n\u0010\nv⊤θ∗∈\nh\nv⊤bθn −q1−ξ/2bσv, v⊤bθn + q1−ξ/2bσv\ni\u0011\n= 1 −ξ,\nwhere bσ2\nv = v⊤c\nH\n−1\nn bΣn(c\nH\n⊤\nn )−1v and q1−ξ/2 denotes the (1 −ξ/2)-th quantile of a standard normal\ndistribution.\n22\n5\nNumerical Experiments\nIn this section, we assess the performance of our ROPE algorithm in numerical experiments. We\nconstruct all confidence intervals with a nominal coverage of 95%, and compare our method with\nthe online bootstrap method with the linear stochastic approximation proposed in Ramprasad et al.\n(2023), which we refer to as LSA.\n5.1\nParameter Inference for Infinite-Horizon MDP\nIn the first experiment, we focus on an infinite-horizon Markov Decision Process (MDP) setting.\nSpecifically, we create an environment with a state space of size 50 and an action space of size 5.\nThe dimension of the features is fixed at 10. The transition probability kernel of the MDP, the\nstate features are randomly generated from N(0p, Ip), and the policy under evaluation is generated\nby picking an action uniformly from the action space for each state. By employing the Bellman\nequation (1), we can compute the expected rewards at each state under the policy. To introduce\nvariability, we add noise to the expected rewards, drawn from different distributions such as the\nstandard normal distribution and t distribution with a degree of freedom of 2.25. The main objective\nof this experiment is parameter inference. Particularly, we construct a confidence interval for the\nfirst coordinate of the true parameter θ∗to study the effect of the first feature on the value function.\nWe set the thresholding level as τi = C(i/(log i)2)β where β is a positive constant. We begin\nwith an investigation of the influence of the parameters C and β on the coverage probability and\nwidth of the confidence interval in our ROPE algorithm. Figure 1 displays the results, revealing\nthat the performance is relatively robust to variations in C and β, irrespective of the type of noise\napplied. Notably, although our theoretical guarantees are based on the noises with a 6-th order\nmoment (See Theorem 4), the experiment indicates that our method exhibits a wider range of\napplicability.\nIn subsequent experiments, we specify C = 0.5 and β = 1/3 for the ROPE algorithm and\ncompare it with the LSA method. Alongside the coverage probability and width of the confidence\ninterval, we also assess the average running time for both methods.\nThe findings are depicted\nin Figure 2.\nIt is evident that, under a light-tailed noise that admits normal distribution, the\n23\nFigure 1: Coverage probability (the first column) and the width of confidence interval (the second\ncolumn) of ROPE of various C and β. We specify the noise distribution as standard normal (the\nfirst row) and Student’s t2.25 (the second row).\ntwo methods yield comparable results, with the ROPE method consistently outperforming LSA in\nterms of the confidence interval width. When the noise exhibits heavy-tailed characteristics, the\nconfidence interval width of LSA is notably larger than that of ROPE. Additionally, the runtime\nof ROPE is much shorter than that of LSA.\nIn the LSA method, the step size is determined by the expression αi−η, which relies on two\npositive hyperparameters α and η. In this experiment, we investigate the sensitivity of LSA method\non these parameters. For comparison, we also present the result of our ROPE algorithm, which\ndoes not require any hyperparameter tuning. In this particular experiment, we specify the noise\ndistribution to be standard normal, and directly apply the online Newton step on the square\nloss (as opposed to the pseudo-Huber loss). Notably, Figure 3 illustrates that the LSA method is\n24\nsensitive to the parameter α. Specifically, when α takes on larger values, the LSA method generates\nsignificantly wider confidence intervals than that of LSA with well-tuned hyperparameters, which\nis undesirable in practical applications. Meanwhile, our ROPE method always generates confidence\nintervals with a valid width and comparable coverage rate.\n5.2\nValue Inference for FrozenLake RL Environment\nIn this section, we consider the FrozenLake environment provided by OpenAI gym, which involves\na character navigating an 8 × 8 grid world. The character’s objective is to start from the first tile\nof the grid and reach the end tile within each episode. If the character successfully reaches the\ntarget tile, a reward of 1 is obtained; otherwise, the reward is 0. In our setup, we generate the state\nfeatures uniformly from [0, 1]p, with a dimensionality of p = 4. The policy under evaluation is pre-\ntrained using Q-learning, and the true parameter can be explicitly computed using the transition\nprobability matrix. Under a contaminated reward model, we introduce random perturbations by\nreplacing the true reward with a value uniformly sampled from the range [0, 100] with probability\nα ∈{0, n−1, 0.05n−1/2}. Our goal is to infer the initial state value.\nFollowing the approach in Section 5.1, we proceed to examine the sensitivity of ROPE to\nthe thresholding parameters C and β. We present the results in Figure 4, and observe that the\nperformance of ROPE is influenced by both the contamination rate αn and the chosen thresholding\nparameter. Specifically, when αn is small, a larger thresholding parameter tends to yield better\noutcomes.\nHowever, as αn increases, the use of a large thresholding parameter can negatively\nimpact performance.\nSubsequently, we set the values C = 0.5, β = 1/3 when αn = 0 or n−1, and C = 0.1, β = 1/3\nwhen αn = 0.05n−1/2 for ROPE algorithm, and compare its performance with that of the LSA\nmethod. Figure 5 displays the results, which clearly demonstrates that our ROPE method consis-\ntently outperforms LSA in terms of coverage rates, CI widths, and running time. The advantage\nof ROPE becomes more pronounced as the contamination rate αn increases.\nIn the next experiment, we investigate the impact of the initialization value n0 on the per-\nformance of the ROPE algorithm. We consider the corruption rate 0.05n−3/4, while varying the\n25\nFigure 2: Coverage probability (the first column), the width of confidence interval (the second col-\numn), and computing time (the third column) of ROPE and LSA. We specify the noise distribution\nas standard normal (the first row) and Student’s t2.25 (the second row).\nFigure 3: Coverage probability (left), and the width of confidence interval (right) of ROPE and\nLSA of various α and η. We specify the noise distribution as standard normal.\n26\nFigure 4: Coverage probability (the first row) and the width of confidence interval (the second row)\nof ROPE of various C and β. We set the contamination rate to be 0 (the first column), n−1 (the\nsecond column), and 0.05n−1/2 (the third column), respectively.\nnumber of episodes in the initialization by {5, 10, 20}.\nEach episode contains an average of 38\nobservations, leading to corresponding values of n0 approximately {190, 380, 760}. The parameters\nare set to C = 0.5, β = 1/3. The results in Figure 6 indicate that the value of n0 has a minimal\neffect on ℓ2-errors and coverage rates.\n5.3\nOnline Policy Evaluation on MIMIC-III Dataset\nIn this section, we demonstrate the effectiveness of the proposed ROPE algorithm by applying it\nto the Medical Information Mart for Intensive Care version III (MIMIC-III) database (Johnson\net al., 2016). MIMIC-III is a freely accessible database containing de-identified critical care data\ncollected from 2001 to 2012 across six ICUs at a Boston teaching hospital. Following Komorowski\net al. (2018), we select the data pertaining to adult sepsis patients and extract a set of 43 features\nto characterize each patient, including demographics, Elixhauser premorbid status, vital signs, and\nlaboratory values. We then apply feature clustering, resulting in 752 distinct states. Each state\nis assigned a feature vector, computed as the mean of the features within that state. We follow\n27\nFigure 5: Coverage probability (the first row), the width of confidence interval (the second row),\nand computing time (the third row) of ROPE and LSA. We set the contamination rate to be 0\n(the first column), n−1 (the second column), and 0.05n−1/2 (the third column), respectively.\n28\nFigure 6: The ℓ2 error (left) and coverage probability (right) of ROPE with different initialization\nusing 5, 10, 20 episodes. We set the contamination rate to be 0.05n−3/4.\nthe approach of Komorowski et al. (2018) and Prasad et al. (2017) to assign rewards to each state\nbased on patient health metrics and mortality outcomes. The reward at the end of each episode\nreflects hospital mortality, while the rewards assigned to intermediate observations incorporate\nphysiological parameters such as heart rate, respiratory rate, and arterial pH (Prasad et al., 2017).\nThe terminal reward has a substantially larger magnitude than the intermediate rewards, naturally\ninducing a heavy-tailed reward distribution. This design is common in healthcare applications,\nas it emphasizes critical but infrequent outcomes of great clinical importance. Our objective is to\nperform on-policy evaluation and construct confidence intervals for the value function corresponding\nto the initial state.\nFigure 7: The width of confidence interval (the first column), computational time (the second\ncolumn), and MSPBE (the third column) of ROPE and LSA for the MIMIC-III dataset.\nThe final processed data contains 20943 episodes, comprising a total 278598 observations. We\n29\ncompare the LSA method with our ROPE method using various (C, β) pairs: (0.5, 0), (0.1, 1/3),\nand (0.5, 1/3), where ROPE is initialized with 100 episodes. The discount factor is set to γ =\n0.99, and the experiment is repeated 100 times.\nSince the true value is unknown in real-data\nanalysis, we report only the evolution of confidence interval width and computational time as the\nnumber of episodes increases. Additionally, we present a box plot of the Mean-Squared Projected\nBellman Error (MSPBE) in the final episode. The results, shown in Figure 7, demonstrates that\nour proposed ROPE method outperforms LSA across all metrics. In particular, we observe that the\nunrobustified LSA method yields considerably wider confidence intervals than our ROPE method,\nand the dispersion of MSPBE values is also larger for LSA. These findings underscore the necessity\nof robust method in the presence of heavy-tailed noise.\n6\nConcluding Remarks\nThis paper introduces a robust online Newton-type algorithm designed for policy evaluation in\nreinforcement learning under the influence of heavy-tailed noise and outliers. We demonstrate the\nestimation consistency as well as the asymptotic normality of our estimator. We further establish\nthe Bahadur representation and show that it converges strictly faster than that of a prototypical\nfirst-order method such as TD. To further conduct statistical inference, we propose an online\napproach for constructing confidence intervals. Our experimental results highlight the efficiency\nand robustness of our approach when compared to the existing online method.\nWhile our current work focuses on TD learning for on-policy evaluation, our robust framework\ncan be extended to Gradient Temporal-Difference (GTD) algorithms for off-policy evaluation. There\nis also potential to apply our approach to multi-step TD methods and TD(λ), though this may\nintroduce challenges due to increased dependencies. Additionally, we identify an extension to fitted-\nQ evaluation as a promising direction for future research. Beyond policy evaluation, extending\nour methodology to the reinforcement learning algorithms with policy optimization (e.g., deep\nQ-learning) presents significant potential but also introduces challenges, which we leave as open\nquestions for future investigation.\n30\nReferences\nBennett, G. (1962), “Probability inequalities for the sum of independent random variables,” Journal\nof the American Statistical Association, 57, 33–45.\nBerkes, I. and Philipp, W. (1979), “Approximation theorems for independent and weakly dependent\nrandom vectors,” The Annals of Probability, 7, 29–54.\nBhandari, J., Russo, D., and Singal, R. (2018), “A Finite Time Analysis of Temporal Difference\nLearning With Linear Function Approximation,” in Conference On Learning Theory.\nBillingsley, P. (1968), Convergence of Probability Measures, Wiley Series in Probability and Math-\nematical Statistics, Wiley.\nByrd, R. H., Hansen, S. L., Nocedal, J., and Singer, Y. (2016), “A stochastic quasi-Newton method\nfor large-scale optimization,” SIAM Journal on Optimization, 26, 1008–1031.\nCharbonnier, P., Blanc-Feraud, L., Aubert, G., and Barlaud, M. (1994), “Two deterministic half-\nquadratic regularization algorithms for computed imaging,” in Proceedings of 1st International\nConference on Image Processing.\n— (1997), “Deterministic edge-preserving regularization in computed imaging,” IEEE Transactions\non Image Processing, 6, 298–311.\nChen, H., Lu, W., and Song, R. (2021a), “Statistical inference for online decision making: In a\ncontextual bandit setting,” Journal of the American Statistical Association, 116, 240–255.\n— (2021b), “Statistical inference for online decision making via stochastic gradient descent,” Jour-\nnal of the American Statistical Association, 116, 708–719.\nChen, X., Lai, Z., Li, H., and Zhang, Y. (2021c), “Online Statistical Inference for Stochastic\nOptimization via Kiefer-Wolfowitz Methods,” arXiv preprint arXiv:2102.03389.\n— (2022), “Online statistical inference for contextual bandits via stochastic gradient descent,”\narXiv preprint arXiv:2212.14883.\n31\nChen, X., Lee, J. D., Tong, X. T., and Zhang, Y. (2020), “Statistical inference for model parameters\nin stochastic gradient descent,” The Annals of Statistics, 48, 251–273.\nDai, B., Nachum, O., Chow, Y., Li, L., Szepesv´ari, C., and Schuurmans, D. (2020), “Coindice:\nOff-policy confidence interval estimation,” Advances in Neural Information Processing Systems.\nDeshpande, Y., Mackey, L., Syrgkanis, V., and Taddy, M. (2018), “Accurate inference for adaptive\nlinear models,” in International Conference on Machine Learning, PMLR, pp. 1194–1203.\nDimakopoulou, M., Ren, Z., and Zhou, Z. (2021), “Online multi-armed bandits with adaptive\ninference,” Advances in Neural Information Processing Systems, 34, 1939–1951.\nDoukhan, P., Massart, P., and Rio, E. (1994), “The functional central limit theorem for strongly\nmixing processes,” in Annales de l’Institut Henri Poincar´e, Probabilit´es et Statistiques, vol. 30,\npp. 63–82.\nDurmus, A., Moulines, E., Naumov, A., Samsonov, S., and Wai, H.-T. (2021), “On the stability of\nrandom matrix product with Markovian noise: Application to linear stochastic approximation\nand TD learning,” in Proceedings of Thirty Fourth Conference on Learning Theory.\nEveritt, T., Krakovna, V., Orseau, L., and Legg, S. (2017), “Reinforcement learning with a cor-\nrupted reward channel,” in Proceedings of the 26th International Joint Conference on Artificial\nIntelligence.\nFan, J., Guo, Y., and Jiang, B. (2022), “Adaptive Huber regression on Markov-dependent data,”\nStochastic Processes and their Applications, 150, 802–818.\nFang, Y., Xu, J., and Yang, L. (2018), “Online bootstrap confidence intervals for the stochastic\ngradient descent estimator,” Journal of Machine Learning Research, 19, 1–21.\nFeng, Y., Tang, Z., Zhang, N., and Liu, Q. (2021), “Non-asymptotic confidence intervals of off-policy\nevaluation: Primal and dual bounds,” in International Conference on Learning Representations.\nFreedman, D. A. (1975), “On tail probabilities for martingales,” The Annals of Probability, 3,\n100–118.\n32\nGivchi, A. and Palhang, M. (2015), “Quasi newton temporal difference learning,” in Proceedings of\nthe Sixth Asian Conference on Machine Learning.\nHadad, V., Hirshberg, D. A., Zhan, R., Wager, S., and Athey, S. (2021), “Confidence intervals for\npolicy evaluation in adaptive experiments,” Proceedings of the National Academy of Sciences,\n118, e2014602118.\nHan, Q., Sun, W. W., and Zhang, Y. (2022), “Online statistical inference for matrix contextual\nbandit,” arXiv preprint arXiv:2212.11385.\nHanna, J., Stone, P., and Niekum, S. (2017), “Bootstrapping with models: Confidence intervals for\noff-policy evaluation,” in Proceedings of the AAAI Conference on Artificial Intelligence.\nHao, B., Ji, X., Duan, Y., Lu, H., Szepesvari, C., and Wang, M. (2021), “Bootstrapping fitted\nq-evaluation for off-policy inference,” in International Conference on Machine Learning.\nHastie, T., Tibshirani, R., Friedman, J. H., and Friedman, J. H. (2009), The Elements of Statistical\nLearning: Data Mining, Inference, and Prediction, vol. 2, Springer.\nHill, B. M. (1975), “A simple general approach to inference about the tail of a distribution,” Ann.\nStatist., 3, 1163–1174.\nHuber, P. J. (1964), “Robust estimation of a location parameter,” The Annals of Mathematical\nStatistics, 35, 73–101.\n— (1992), “Robust estimation of a location parameter,” in Breakthroughs in Statistics, Springer.\n— (2004), Robust Statistics, vol. 523, John Wiley & Sons.\nJiang, N. and Huang, J. (2020), “Minimax value interval for off-policy evaluation and policy opti-\nmization,” Advances in Neural Information Processing Systems.\nJohnson, A. E. W., Pollard, T. J., Shen, L., et al. (2016), “MIMIC-III, a freely accessible critical\ncare database,” Scientific Data, 3, 160035.\n33\nKolter, J. Z. and Ng, A. Y. (2009), “Regularization and feature selection in least-squares temporal\ndifference learning,” in Proceedings of the 26th Annual International Conference on Machine\nLearning.\nKomorowski, M., Celi, L. A., Badawi, O., et al. (2018), “The Artificial Intelligence Clinician learns\noptimal treatment strategies for sepsis in intensive care,” Nature Medicine, 24, 1716–1720.\nKormushev, P., Calinon, S., and Caldwell, D. G. (2013), “Reinforcement learning in robotics:\nApplications and real-world challenges,” Robotics, 2, 122–148.\nLi, X. and Sun, Q. (2023), “Variance-aware robust reinforcement learning with linear function\napproximation under heavy-tailed rewards,” arXiv e-prints, arXiv:2303.05606.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015), “Human-level control through deep\nreinforcement learning,” Nature, 518, 529–533.\nMou, W., Pananjady, A., Wainwright, M. J., and Bartlett, P. L. (2021), “Optimal and\ninstance-dependent guarantees for Markovian linear stochastic approximation,” arXiv e-prints,\narXiv:2112.12770.\nMurphy, S. A. (2003), “Optimal dynamic treatment regimes,” Journal of the Royal Statistical\nSociety: Series B: Statistical Methodology, 65, 331–355.\nNesterov, Y. (2003), Introductory Lectures on Convex Optimization: A Basic Course, vol. 87,\nSpringer Science & Business Media.\nPolyak, B. T. and Juditsky, A. B. (1992), “Acceleration of stochastic approximation by averaging,”\nSIAM Journal on Control and Optimization, 30, 838–855.\nPrasad, N., Cheng, L.-F., Chivers, C., Draugelis, M., and Engelhardt, B. E. (2017), “A Reinforce-\nment Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units,” in 33rd\nConference on Uncertainty in Artificial Intelligence.\n34\nRamprasad, P., Li, Y., Yang, Z., Wang, Z., Sun, W. W., and Cheng, G. (2023), “Online bootstrap\ninference for policy evaluation in reinforcement learning,” Journal of the American Statistical\nAssociation, 118, 2901–2914.\nRio, E. (2017), Asymptotic Theory of Weakly Dependent Random Processes, vol. 80, Springer.\nRobbins, H. and Monro, S. (1951), “A stochastic approximation method,” The Annals of Mathe-\nmatical Statistics, 22, 400–407.\nRosenblatt, M. (1956), “A central limit theorem and a strong mixing condition,” Proceedings of the\nNational Academy of Sciences, 42, 43–47.\nRuppert, D. (1985), “A Newton-Raphson Version of the Multivariate Robbins-Monro Procedure,”\nThe Annals of Statistics, 13, 236–245.\n— (1988), “Efficient estimations from a slowly convergent Robbins-Monro process,” Tech. rep.,\nCornell University Operations Research and Industrial Engineering.\nSchraudolph, N. N., Yu, J., and G¨unter, S. (2007), “A stochastic quasi-Newton method for on-\nline convex optimization,” in Proceedings of the Eleventh International Conference on Artificial\nIntelligence and Statistics.\nShao, Q.-M. and Zhang, Z.-S. (2022), “Berry–Esseen bounds for multivariate nonlinear statistics\nwith applications to M-estimators and stochastic gradient descent algorithms,” Bernoulli, 28,\n1548–1576.\nShen, Y., Cai, H., and Song, R. (2024), “Doubly robust interval estimation for optimal policy\nevaluation in online learning,” Journal of the American Statistical Association, 1–20.\nShi, C., Fan, A., Song, R., and Lu, W. (2018), “High-dimensional A-learning for optimal dynamic\ntreatment regimes,” The Annals of Statistics, 46, 925–957.\nShi, C., Song, R., Lu, W., and Li, R. (2021a), “Statistical inference for high-dimensional models via\nrecursive online-score estimation,” Journal of the American Statistical Association, 116, 1307–\n1318.\n35\nShi, C., Zhang, S., Lu, W., and Song, R. (2021b), “Statistical inference of the value function\nfor reinforcement learning in infinite-horizon settings,” Journal of the Royal Statistical Society.\nSeries B: Statistical Methodology, 84, 765–793.\nSun, Q., Zhou, W.-X., and Fan, J. (2020), “Adaptive Huber Regression,” Journal of the American\nStatistical Association, 115, 254–265.\nSutton, R. (1988), “Learning to predict by the method of temporal differences,” Machine Learning,\n3, 9–44.\nSutton, R. and Barto, A. (2018), Reinforcement Learning, second edition: An Introduction, Adap-\ntive Computation and Machine Learning series, MIT Press.\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv´ari, C., and Wiewiora, E.\n(2009), “Fast gradient-descent methods for temporal-difference learning with linear function ap-\nproximation,” in Proceedings of the 26th Annual International Conference on Machine Learning.\nSyrgkanis, V. and Zhan, R. (2023), “Post-Episodic Reinforcement Learning Inference,” arXiv e-\nprints, arXiv:2302.08854.\nThomas, P., Theocharous, G., and Ghavamzadeh, M. (2015), “High confidence policy improve-\nment,” in International Conference on Machine Learning, PMLR.\nTsitsiklis, J. and Van Roy, B. (1997), “An analysis of temporal-difference learning with function\napproximation,” IEEE Transactions on Automatic Control, 42, 674–690.\nVershynin, R. (2010), “Introduction to the non-asymptotic analysis of random matrices,” arXiv\npreprint arXiv:1011.3027.\nWang, J., Liu, Y., and Li, B. (2020), “Reinforcement learning with perturbed rewards,” in Pro-\nceedings of the AAAI conference on Artificial Intelligence.\nXia, E., Khamaru, K., Wainwright, M. J., and Jordan, M. I. (2023), “Instance-Dependent Confi-\ndence and Early Stopping for Reinforcement Learning,” Journal of Machine Learning Research,\n24, 1–43.\n36\nZhan, R., Hadad, V., Hirshberg, D. A., and Athey, S. (2021), “Off-policy evaluation via adap-\ntive weighting with data from contextual bandits,” in Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, pp. 2125–2135.\nZhang, K., Janson, L., and Murphy, S. (2021), “Statistical inference with M-estimators on adap-\ntively collected data,” Advances in Neural Information Processing Systems, 34, 7460–7471.\n— (2022), “Statistical inference after adaptive sampling in non-Markovian environments,” arXiv\npreprint arXiv:2202.07098.\nZhu, J., Wan, R., Qi, Z., Luo, S., and Shi, C. (2024), “Robust offline reinforcement learning with\nheavy-tailed rewards,” in International Conference on Artificial Intelligence and Statistics.\n37\n7\nAppendix\n7.1\nExperiment of the Effect of Thresholding parameters\nIn this section, we extend the analysis from the infinite-horizon Markov Decision Process setting\ndiscussed in Section 5.1 by examining the effect of thresholding parameter τ on our ROPE method.\nAs stated in Theorem 1, the thresholding parameter is defind as τ = Cτ max(1, iβ1/(log i)β2), and is\ndetermined by three factors Cτ, β1 and β2. In our experiments, we investigate how variations in each\nof these factors affect performance. We adopt the ideal parameter triple (Cτ, β1, β2) = (0.5, 1/3, 2/3)\nand add only t2.25 noise to the reward. In each experiment, one parameter is varied while keep\nthe other two remain fixed, and we report the corresponding coverage probability and confidence\ninterval width. The results are presented in Figure 8 below.\nFigure 8: Coverage probability (the first row) and the width of confidence interval (the second row)\nROPE under different thresholding parameters. We vary Cτ (the first column), β1 (the second\ncolumn) and β2 (the third column) while other factors are held constant.\nFrom our experimental results, we observe that the factors Cτ, β1 and β2 have a relatively\nlimited impact on the performance under t2.25 noise. In particular, as illustrated in the first the\nthird columns of Figure 8, a smaller thresholding parameter leads to a narrower confidence interval\n38\nwidth when β1 is held constant. Moreover, the results in the second column indicate that the\nconfidence interval width does not vary monotonically with respect to β1, with β1 = 1/3 emerging\nas the optimal choice.\n7.2\nTechnical Lemmas\nLemma 1. Let Y1, ..., Yn be ϕ-mixing sequence with ϕ(k) = O(ρk). Assume that |Yi| ≤M and\nEYi = 0. Then for any fixed ν > 0 and 1 ≤k ≤n, we have some constant C such that\nP\n \f\f\f\nPk\ni=1 Yi\nk\n\f\f\f ≥C\nr\nlog n\nk\n+ C log2 n\nk\n!\n= O(n−ν).\nProof. We divide the k-tuple (1, . . . , k) into mk different subsets H1, . . . , Hmk, where mk = ⌈k/⌈λ log n⌉⌉.\nHere |Hi| = ⌈λ log n⌉for 1 ≤i ≤mk −1, and |Hmj| ≤⌈λ log n⌉. λ is a sufficiently large constant\nwhich will be specified later. Then we have mk ≈k/(λ log n). Without loss of generality, we assume\nthat mk is an even integer.\nLet ξl =\n1\n⌈λ log n⌉\nP\ni∈H2l−1 Yi, then we know |ξl| ≤M and E[ξl] = 0 holds.\nFor all B1 ∈\nσ(ξ1, . . . , ξl) and B2 ∈σ(ξl+1), there holds |P(B2|B1) −P(B2)| ≤ϕ(⌈λ log n⌉) for all l. By Theorem\n2 of Berkes and Philipp (1979), there exists a sequence of independent variables ηl, l ≥1 with ηl\nhaving the same distribution as ξl, and\nP\n\u0010\n|ξl −ηl| ≥6ϕ(⌈λ log n⌉)\n\u0011\n≤6ϕ(⌈λ log n⌉).\nFor any ν > 0, we can take λ ≥(ν + 1)/| log ρ| so that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nl=1\n(ξl −ηl)\n\f\f\f ≥C1n−ν\u0011\n≤P\n\u0010\f\f\f 2\nmk\nmk/2\nX\nl=1\n(ξl −ηl)\n\f\f\f ≥6ϕ(⌈λ log n⌉)\n\u0011\n≤3mkϕ(⌈λ log n⌉) ≤C1n−ν,\n(21)\nwhere C1 > 0 is some constant. Next, we bound the variance of ξl. From equation (20.23) of\n39\nBillingsley (1968) we know that for arbitrary i, j, there is\n\f\fE[YiYj] −E[Yi]E[Yj]\n\f\f ≤2\np\nϕ(|i −j|)\nq\nE[Y 2\ni ]E[Y 2\nj ] ≤2ρ|i−j|/2M2.\nThen we compute that\nVar(ηl) = Var(ξl) =\n1\n⌈λ log n⌉2\nh\nX\ni,j∈H2l−1\n{E(YiYj) −E(Yi)E(Yj)}\ni\n(22)\n≤\n2\n⌈λ log n⌉2\n⌈λ log n⌉\nX\ni,j=1\nρ|i−j|/2M2\n≤2⌈λ log n⌉\n⌈λ log n⌉2\n\u0010\n2\n1 −√ρ −1\n\u0011\nM2 ≤\n4\n(1 −√ρ)⌈λ log n⌉M2.\nNotice that ηl’s are all uniformly bounded by M, we can apply Bernstein’s inequality (Bennett,\n1962) and yield\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nl=1\nηl\n\f\f\f ≥t\n\u0011\n≤2 exp\n\u0010\n−\nmkt2/4\nVar(ηl)/2 + Mt/3\n\u0011\n≤exp\n\u0010\n−\nmk⌈λ log n⌉t2/4\n2M2/(1 −√ρ) + ⌈λ log n⌉Mt/3\n\u0011\n.\nWe take t = C(\np\nlog n/k + log2 n/k) for some C large enough (note that k ≈mk⌈λ log n⌉). Then\nwe have that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nl=1\nηl\n\f\f\f ≥t\n\u0011\n= O(n−ν).\n(23)\nCombining (21) and (23) we have that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nl=1\nξl\n\f\f\f ≥C\nr\nlog n\nk\n+ C log2 n\nk\n\u0011\n= O(n−ν).\n(24)\nNext we denote eξ =\n1\n⌈λ log n⌉\nP\ni∈H2l Yi. We can similarly show that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nl=1\neξl\n\f\f\f ≥C\nr\nlog n\nk\n+ C log2 n\nk\n\u0011\n= O(n−ν).\n(25)\n40\nCombining (24) and (25) we prove the desired result.\nLemma 2. Let Y 1, . . . , Y n be random vectors in Rp. Let Gi = σ(Y 1, . . . , Y i). Assume that\nE[Y i|Gi−1] = 0,\nmax\n1≤i≤n sup\nv∈Sp |v⊤Y i| ≤1,\nsup\nv∈Sp\nn\nX\ni=1\nVar[v⊤Y i|Gi−1] ≤bn.\nThen for every ν > 0, there is\nP\n\u0010\f\f\nn\nX\ni=1\nY i\n\f\f\n2 ≥C\n\u0000d log n +\np\ndbn log n\n\u0001\u0011\n= O(n−νd),\nfor some C sufficiently large.\nProof. Let N be the 1/2-net of the unit ball Sd−1. By Lemma 5.2 of Vershynin (2010) we know\nthat |N| ≤5d. Then we have that\n\f\f\f\nn\nX\ni=1\nY i\n\f\f\f\n2 = sup\nv∈Sd−1\n\f\f\f\nn\nX\ni=1\nv⊤Y i\n\f\f\f\n≤sup\nev∈N\n\f\f\f\nn\nX\ni=1\nev⊤Y i\n\f\f\f +\nsup\n|v−ev|2≤1/2\n\f\f\f\nn\nX\ni=1\n(v −ev)⊤Y i\n\f\f\f,\n⇒\n\f\f\f\nn\nX\ni=1\nY i\n\f\f\f\n2 ≤2 sup\nev∈N\n\f\f\f\nn\nX\ni=1\nev⊤Y i\n\f\f\f.\nBy Theorem 1.6 of Freedman (1975) we know that\nsup\nev∈Sd−1 P\n\u0010\f\f\nn\nX\ni=1\nev⊤Y i\n\f\f ≥C1\n\u0000d log n +\np\ndbn log n\n\u0001\u0011\n= O(n−(ν+log 5)d)\n41\nfor some large constant C′ > 0. Therefore\nP\n\u0010\f\f\nn\nX\ni=1\nY i\n\f\f\n2 ≥2C1\n\u0000d log n +\np\ndbn log n\n\u0001\u0011\n≤P\n\u0010\nsup\nev∈N\n\f\f\nn\nX\ni=1\nev⊤Y i\n\f\f ≥C1\n\u0000d log n +\np\ndbn log n\n\u0001\u0011\n≤5d sup\nev∈N\nP\n\u0010\f\f\nn\nX\ni=1\nev⊤Y i\n\f\f ≥C1\n\u0000d log n +\np\ndbn log n\n\u0001\u0011\n= O(n−νd),\nwhich proves the lemma.\nLemma 3. Let Y be a d × d random matrix with E[Y ] = 0, ∥Y ∥≤1, F be the σ-field generated\nby Y . Then for any σ-field G, there holds\nE\nh\n∥E[Y |G]∥\ni\n≤d\np\n2πϕ,\nwhere\nϕ =\nsup\nA∈F,B∈G\n|P(AB) −P(A)P(B)|.\nProof. By elementary inequality of matrix, we know that\n|Y |∞≤∥Y ∥≤1,\nand\n∥Y ∥≤∥Y ∥F .\nFor every element Yi,j of Y , by Lemma 4.4.1 of Berkes and Philipp (1979) we have that\nE\nh\f\fE[Yij|G]\n\f\f2i\n≤E\nh\f\fE[Yij|G]\n\f\f\ni\n≤2πϕ.\nTherefore, we have that\nE\nh\n∥E[Y |G]∥\ni\n≤E\nh\n∥E[Y |G]∥F\ni\n,\n≤\nn\nE\nh\n∥E[Y |G]∥2\nF\nio1/2\n≤\nn\nE\nh\nd\nX\ni,j=1\n\f\fE[Yij|G]\n\f\f2io1/2\n≤\np\nd22πϕ = d\np\n2πϕ.\n42\nThe proof is complete.\nLemma 4. (Approximation of pseudo-Huber gradient) The gradient of pseudo-Huber loss gτ(x)\nsatisfies\n|gτ(x) −x| ≤1\n2τ −δ|x|1+δ,\nfor δ ∈(0, 2]\n|g′\nτ(x) −1| ≤5\n2τ −1−δ|x|1+δ\nfor δ ∈(0, 1]\n(26)\nuniformly for |x| ≤τ. And\n|gτ(x) −x| ≤|x|,\n|g′\nτ(x) −1| ≤1,\n(27)\nholds for all x.\nProof. It is easy to compute that\ngτ(x) =\nx\np\n1 + x2/τ 2 ,\ng′\nτ(x) =\n1\n(1 + x2/τ 2)3/2 .\nIt is not hard to see that\n|gτ(x) −x| ≤|x|,\n|g′\nτ(x) −1| ≤1.\nThis proves (27). Furthermore, when |x| ≤τ, we have\n|gτ(x) −x| =\nx2/τ 2\n(1 +\np\n1 + x2/τ 2)\np\n1 + x2/τ\n· |x|\n≤1\n2τ −δ|x|1+δ\nfor all 0 ≤δ ≤2. Similarly,\n|g′\nτ(x) −1| =x2/τ 2(2 +\np\n1 + x2/τ 2 + x2/τ 2)\n(1 +\np\n1 + x2/τ 2)(1 + x2/τ 2)3/2\n≤5\n2τ −1−δ|x|1+δ,\nfor |x| ≤τ. The proof is complete.\n43\nLemma 5. For any sequence {ak} where ak = (log k)β1/kβ2 and β1 ≥0, there hold\nn0an0 +\nn\nX\nk=n0+1\nak ≤\n\n\n\n\n\n\n\nCnan log n\nif β2 ≤1;\nC(log n)β1/nβ2−1\n0\nif β2 > 1.\nHere the constant C only depends on the parameter β2.\nProof. Directly compute that\nn0an0 +\nn\nX\nk=n0+1\nak ≤(log n)β1\u0010 n0\nnβ2\n0\n+\nn\nX\nk=n0+1\n1\nkβ2\n\u0011\n.\n(28)\nWe know that\nn\nX\nk=n0+1\n1\nkβ2 ≤\nZ n\nn0\n1\nxβ2 dx =\n1\n1 −β2\n\u0010\nn1−β2 −n1−β2\n0\n\u0011\n.\nWhen β2 < 1, (28) can be bounded by\nn0an0 +\nn\nX\nk=n0+1\nak ≤(log n)β1\u0010\nn1−β2\n0\n+\n1\n1 −β2\n(n1−β2 −n1−β2\n0\n)\n\u0011\n≤(log n)β1n1−β2\u0010\n−\nβ2\n1 −β2\n\u0010n0\nn\n\u00111−β2 +\n1\n1 −β2\n\u0011\n≤\n1\n1 −β2\nnan.\nWhen β2 > 1, (28) can be bounded by\nn0an0 +\nn\nX\nk=n0+1\nak ≤(log n)β1\u0010\nβ2\nβ2 −1 ×\n1\nnβ2−1\n0\n−\n1\nβ2 −1 ×\n1\nnβ2−1\n\u0011\n≤\nβ2\nβ2 −1(log n)β1\n1\nnβ2−1\n0\n.\nWhen β2 = 1, (28) can be bounded by\nn0an0 +\nn\nX\nk=n0+1\nak ≤(log n)β1\u00001 + log n\n\u0001\n≤2nan log n.\n44\n7.3\nProof of Results in Section 2\nProof of Theorem 1. For each k ≥n0, when δ > 0, we denote\nek = αkτk + τ −min(δ,2)\nk\n+\ns\nτ (1−δ)+\nk\nlog k\nk\n+ log2 kτk\nk\n+ 1\n√\nd\n(c0)2k−n0,\n(29)\nwhere δ′ = min{2, δ}. Given a sufficiently large constant Ψ > 0, we define the event\nEk =\nn\n|bθk −θ∗|2 ≤Ψ\n√\ndek\no\n∩\nn\n∥c\nHk −H∥≤C0Ψ\n√\ndek log k\no\n∩\nn\f\f\f1\nk\nk\nX\ni=1\nXigτi(Z⊤\ni θ∗−bi)\n\f\f\f\n2 ≤C0\n√\ndek\no\n,\n(30)\nwhere the C0 is a constant that does not depend on Ψ. We will show that P(Ec\nk, ∩k−1\ni=n0Ei) ≤C0k−ν,\nwhere ν > 1 is some constant and C0 > 0 is some constant only depends on ν. Let us prove it by\ninduction on k. By the choice of initial parameter bθ0, we know (30) holds for k = n0. For n = k+1,\nfrom (8) we have that\nbθk+1 −θ∗=\n1\nk + 1\nk+1\nX\ni=1\n(bθi−1 −θ∗) −c\nH\n−1\nk+1\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni bθi−1 −bi) −Xigτi(Z⊤\ni θ∗−bi)\n\t\n−c\nH\n−1\nk+1\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\n.\n(31)\nFor the second term on the right-hand side of (31), we have that\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni bθi−1 −bi) −Xigτi(Z⊤\ni θ∗−bi)\n\t\n=\n1\nk + 1\nk+1\nX\ni=1\nXiZ⊤\ni g′\nτi(Z⊤\ni θ∗−bi)(bθi−1 −θ∗)\n+\n1\nk + 1\nk+1\nX\ni=1\nZ 1\n0\n(1 −t)Xig′′\nτi\n\b\nZ⊤\ni (θ∗+ t(bθi−1 −θ∗)) −bi\n\t\n(Z⊤\ni (bθi−1 −θ∗))2dt\n=\n1\nk + 1\nk+1\nX\ni=1\nXiZ⊤\ni g′\nτi(Z⊤\ni θ∗−bi)(bθi−1 −θ∗) +\n1\nk + 1\nk+1\nX\ni=1\nO(1)|bθi−1 −θ∗|2\n2\n45\n=\n1\nk + 1\nk+1\nX\ni=1\nXiZ⊤\ni g′\nτi(Z⊤\ni θ∗−bi)(bθi−1 −θ∗) + O(1)\n1\nk + 1\nk+1\nX\ni=1\n|bθi−1 −θ∗|2\n2,\nwhere the second equality holds because of the fact that |g′′\nτi(x)| ≤3/τi = O(1), the last equality\nuses the inductive hypothesis. Substitute it into (31), we have\nbθk+1 −θ∗=(I −c\nH\n−1\nk+1H)\n1\nk + 1\nk+1\nX\ni=1\n(bθi−1 −θ∗) −c\nH\n−1\nk+1\n1\nk + 1\nk+1\nX\ni=1\n(Ai −H)(bθi−1 −θ∗)\n−c\nH\n−1\nk+1\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\n+ O(1)\n1\nk + 1\nk+1\nX\ni=1\n|bθi−1 −θ∗|2\n2,\n(32)\nwhere Ai = XiZ⊤\ni g′\nτi(Z⊤\ni θ∗−bi) and H = E[XZ⊤].\nUnder event ∩k\ni=n0Ei, by Lemma 5 there exists a constant C1 > 0 such that\nP\n\u0010\n1\nk + 1\nk+1\nX\ni=1\n|bθi−1 −θ∗|2\n2 ≥C1Ψ2de2\nk log k, ∩k\ni=n0Ei\n\u0011\n≥P\n\u0010\n1\nk + 1\nk+1\nX\ni=1\n|bθi−1 −θ∗|2\n2 ≥\n1\nk + 1\nk\nX\ni=0\nΨ2de2\ni , ∩k\ni=n0Ei\n\u0011\n= 0.\n(33)\nSimilarly, by Lemma 5 there exists a constant C2 > 0 such that\nP\n\u0010\n1\nk + 1\nk+1\nX\ni=1\n|bθi−1 −θ∗|2 ≥C2Ψ\n√\ndek log k, ∩k\ni=n0Ei\n\u0011\n≥P\n\u0010\n1\nk + 1\nk+1\nX\ni=1\n|bθi−1 −θ∗|2 ≥\n1\nk + 1\nk\nX\ni=0\nΨ\n√\ndei, ∩k\ni=n0Ei\n\u0011\n= 0.\n(34)\nBy Lemma 6, (34) and (42) we know that under event ∩k\ni=n0Ei, for every ν > 0 there exist\nconstants C1, C3 > 0 (which only depend on ν), such that\nP\n\u0010\f\f\f(I −c\nH\n−1\nk+1H)\n1\nk + 1\nk\nX\ni=0\n(bθi −θ∗)\n\f\f\f\n2 ≥C1Ψ2de2\nk log2 k, ∩k\ni=n0Ei\n\u0011\n≤P\n\u0010\n∥c\nH\n−1\nk+1∥· ∥c\nHk+1 −H∥·\n1\nk + 1\nk\nX\ni=0\n|bθi −θ∗|2 ≥C1Ψ2de2\nk log2 k, ∩k\ni=n0Ei\n\u0011\n≤C3(k + 1)−ν.\n46\nBy Lemma 8 we know that\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(Ai −H)(bθi−1 −θ∗)\n\f\f\f\n2 ≥C1(αk + Ψ2de2\nk log k), ∩k\ni=n0Ei\n\u0011\n≤C3(k + 1)−ν.\n(35)\nIn this part, we mainly consider the case where δ > 0 and τk can be arbitrary. A special case\nwhere δ > 4 and\nq\ni/ log3 i = O(τi) will be presented in Proposition 8 below. By Lemma 7 we have\nthat\nP\n\u0010\f\f\fc\nH\n−1\nk+1\n1\nk + 1\nk+1\nX\ni=1\n\b\nXig(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C1\n√\ndek+1, ∩k\ni=n0Ei\n\u0011\n≤C3(k + 1)−ν;\nBy substituting the above inequalities into (32), we have that\nP\n\u0010\n|bθk+1 −θ∗|2 ≥Ψ\n√\ndek+1, ∩k\ni=n0Ei\n\u0011\n≤P\n\u0010\n|bθk+1 −θ∗|2 ≥C1\n√\ndek+1 + C1αk + 3C1Ψ2de2\nk log2 k, ∩k\ni=n0Ei\n\u0011\n≤3C3(k + 1)−ν,\n(36)\nwhere Ψ can be taken to be sufficiently large, given C1 fixed. Here the second inequality holds\nbecause\n√\ndek can be sufficiently small for k ≥n0 by taking n0 sufficiently large. Meanwhile, as\nαk ≤(k + 1)αk+1/k ≤(n0 + 1)αk+1/n0 = o(τk+1αk+1) = o(ek+1), the last two terms both have\norder of o(ek+1).\nObviously, the rest two events of (30) are contained in (36).\nTherefore, the\ninductive hypothesis that P(Ec\nk+1, ∩k\nj=n0Ej) ≤3C3(k + 1)−ν, is proved.\nFor k ≥n0, there holds\nP\n\u0010\n∩k\ni=n0 Ei\n\u0011\n≥P\n\u0010\nEn0\n\u0011\n−\nk\nX\ni=n0+1\nP\n\u0010\nEc\ni , ∩i−1\nj=n0Ej\n\u0011\n≥1 −\nk\nX\ni=n0+1\n3C3i−ν ≥1 −3C3\nν −1\n1\nnν−1\n0\n,\nwhich proves the theorem.\nLemma 6. (Bound of the pseudo-Huber Hessian matrix) Under the same assumptions as in The-\n47\norem 1, there exists uniform constants C and c, such that the Hessian matrix c\nHk+1 satisfies\nP\n\u0010\n∥c\nHk+1 −H∥≥CΨ\n√\ndek log k, ∩k\ni=n0Ei\n\u0011\n≤c(k + 1)−νd.\nProof. We note that\n∥c\nHk+1 −H∥≤\n1\nk + 1\n\r\r\r\nX\ni∈Qk\nXiZig′\nτi(Z⊤\ni bθi−1 −bi) −H\n\r\r\r +\n1\nk + 1\n\r\r\r\nX\ni/∈Qk\nXiZig′\nτi(Z⊤\ni bθi−1 −bi) −H\n\r\r\r.\nFor the first term, there is\n\r\r\rXZ⊤g′\nτ(Z⊤θ −b)\n\r\r\r\n=\nsup\nu,v∈Sd−1 u⊤XZ⊤v|g′\nτ(Z⊤θ −b)| ≤M2,\nholds for all θ and τ ≥1. Therefore we have that\n1\nk + 1\n\r\r\r\nX\ni∈Qk\nXiZig′\nτi(Z⊤\ni bθi−1 −bi) −H\n\r\r\r\n(37)\n≤\n1\nk + 1\nX\ni∈Qk\n∥XiZig′\nτi(Z⊤\ni bθi−1 −bi)∥+\n1\nk + 1\nX\ni∈Qi\n∥H∥≤2M2αk.\nFor the second term, we first prove that\nP\n\u0010\r\r\r\n1\nk + 1\nX\ni/∈Qk\n(Ai −E[Ai])\n\r\r\r ≥C1\nr\nd log k\nk + 1\n\u0011\n= O((k + 1)−νd).\n(38)\nLet N be the 1/4-net of the unit ball Sd−1. By Lemma 5.2 of Vershynin (2010) we know that\n|N| ≤9d. Then we have that\n\r\r\r\n1\nk + 1\nX\ni/∈Qk\n(Ai −E[Ai])\n\r\r\r =\nsup\nu,v∈Sd−1\n\f\f\f\n1\nk + 1\nX\ni/∈Qk\nu⊤(Ai −E[Ai])v\n\f\f\f\n≤\nsup\neu,ev∈N\n\f\f\f\n1\nk + 1\nX\ni/∈Qk\neu⊤(Ai −E[Ai])ev\n\f\f\f\n+ sup\neu∈N\nsup\n|v−ev|2≤1/4\n\f\f\f\n1\nk + 1\nX\ni/∈Qk\neu⊤(Ai −E[Ai])(v −ev)\n\f\f\f\n2\n48\n+ sup\nv∈Sd−1\nsup\n|u−eu|2≤1/4\n\f\f\f\n1\nk + 1\nX\ni/∈Qk\n(u −eu)⊤(Ai −E[Ai])v\n\f\f\f\n2\n⇒\n\r\r\r\n1\nk + 1\nX\ni/∈Qk\n(Ai −E[Ai])\n\r\r\r ≤\n2 sup\neu,ev∈N\n\f\f\f\n1\nk + 1\nX\ni/∈Qk\neu⊤(Ai −E[Ai])ev\n\f\f\f.\nApplying Lemma 1 to every\n1\nk+1\nP\ni/∈Qk eu⊤(Ai −E[Ai])ev we can obtain that\nsup\neu,ev∈N\nP\n\u0010\f\f\f\n1\nk + 1\nX\ni/∈Qk\neu⊤(Ai −E[Ai])ev\n\f\f\f\n2 ≥C1\nr\nd log k\nk + 1\n\u0011\n= O((k + 1)−(ν+log 9)d)\nfor some constant C1 > 0. Therefore\nP\n\u0010\r\r\r\n1\nk + 1\nX\ni/∈Qk\n(Ai −E[Ai])\n\r\r\r ≥2C1\nr\nd log k\nk + 1\n\u0011\n≤P\n\u0010\nsup\neu,ev∈N\n\f\f\f\n1\nk + 1\nX\ni/∈Qk\neu⊤(Ai −E[Ai])ev\n\f\f\f\n2 ≥C1\nr\nd log k\nk + 1\n\u0011\n≤92d sup\neu,ev∈N\nP\n\u0010\f\f\f\n1\nk + 1\nX\ni/∈Qk\neu⊤(Ai −E[Ai])ev\n\f\f\f\n2 ≥C1\nr\nd log k\nk + 1\n\u0011\n= O((k + 1)−νd),\nwhich proves (38). Next we prove that when (X, Z, b) are normal data, for every τ > 0,\n\r\r\rE[XZ⊤g′\nτ(Z⊤θ∗−b)] −E[XZ⊤]\n\r\r\r ≤C2τ −δ′.\nIndeed, denote ϵ = Z⊤θ∗−b, we have that\n\r\r\rE[XZ⊤g′\nτ(Z⊤θ∗−b)] −E[XZ⊤]\n\r\r\r\n≤sup\nu,v E[u⊤XZ⊤v|g′\nτ(ϵ) −1|]\n≤sup\nu,v E[u⊤XZ⊤vI(|ϵ| > τ) + 5\n2u⊤XZ⊤vτ −1−δ′′|ϵ|1+δ′′I(|ϵ| ≤τ)]\n≤sup\nu,v E\n\u0002\nu⊤XZ⊤vP(|ϵ| > τ|X, Z)\n\u0003\n+ 5\n2τ −1−δ′′ sup\nu,v E\n\u0002\nu⊤XZ⊤vE[|ϵ|1+δ′′|X, Z]\n\u0003\n≤C2τ −1−δ′′ ≤C2τ −δ′,\nwhere the third line uses Lemma 4, and δ′′ = min(1, δ). Then clearly we have that 1 + δ′′ ≥δ′.\n49\nTherefore, by the choice of τi and Lemma 5, we have\n\r\r\r\n1\nk + 1\nX\ni/∈Qk\nE[Ai] −H\n\r\r\r ≤\n1\nk + 1\nX\ni/∈Qk\n\r\r\rE[Ai] −H\n\r\r\r ≤C2τ −δ′\nk+1.\n(39)\nUnder event ∩k\ni=n0Ei, from (34) we know there is\n\r\r\r\n1\nk + 1\nX\ni/∈Qk\nAi −c\nHk\n\r\r\r =\n\r\r\r\n1\nk + 1\nX\ni/∈Qk\nXiZ⊤\ni\n\b\ng′\nτi(Z⊤\ni θ∗−bi) −g′\nτi(Z⊤\ni bθi−1 −bi)\n\t\r\r\r\n≤M2\n1\nk + 1\nX\ni/∈Qk\n|bθi−1 −θ∗|2 ≤C3Ψ\n√\ndek log k,\n(40)\nTherefore combining (38), (39) and (40) we have that\n.P\n \n1\nk + 1\n\r\r\r\nX\ni/∈Qk\nXiZig′\nτi(Z⊤\ni bθi−1 −bi) −H\n\r\r\r ≥C4\n\u0010r\nd log k\nk + 1 + τ −δ′\nk+1 + Ψ\n√\ndek log k\n\u0011!\n=O((k + 1)−νd).\n(41)\nCombining (37) and (41), we have that\nP\n \n∥c\nHk+1 −H∥≥C5\n\u0010\nαk +\nr\nd log k\nk + 1 + τ −δ′\nk+1 +\n√\ndek log k\n\u0011!\n= O((k + 1)−νd),\nwhich proves the lemma.\nAs a corollary, we can show that under event ∩k\ni=n0Ei,\n∥c\nH\n−1\nk+1∥= (Λmin(c\nHk+1))−1 ≤\n\u0000Λmin(H) −∥c\nHk+1 −H∥\n\u0001−1 ≤2/Λmin(H).\n(42)\nLemma 7. Under the same assumptions as in Theorem 1, for every ν > 0, there exist constants\nC and c such that:\n50\ni) If δ > 0 and τk can be arbitrary, then\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C\n√\ndek+1\n\u0011\n≤c(k + 1)−ν.\nHere ek+1 is defined in (29).\nii) If δ > 1 and\nq\nk/ log3 k = O(τk), then\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C\n√\ndek+1\n\u0011\n≤c(k + 1)−min{ν,(δ−1)/3}.\nHere ek+1 is defined in (65).\nProof. Proof of i). We prove the bound coordinate-wisely. When (X, Z, b) has no outlier, for the\nl-th coordinate, we first prove that for every τ > 0, there is\n|E[Xlgτ(Z⊤θ∗−b)]| ≤C1τ −δ′.\nIndeed, ϵ = Z⊤θ∗−b, we have that\n|E[Xlgτ(ϵ)]| ≤|E[Xlϵ]| +\n\f\fE[Xl|ϵ −gτ(ϵ)|I(|ϵ| ≤τ)]\n\f\f +\n\f\fE[Xl|gτ(ϵ) −ϵ|I(|ϵ| > τ)]\n\f\f\n≤1\n2τ −δ′\f\fE[Xl|ϵ|1+δ′I(|ϵ| ≤τ)]\n\f\f +\n\f\fE[Xl|ϵ|I(|ϵ| > τ)]\n\f\f\n≤1\n2τ −δ′\f\fE\n\u0002\nXlE[|ϵ|1+δ′I(|ϵ| ≤τ)|X, Z]\n\u0003\f\f +\n\f\fE\n\u0002\nXlE[|ϵ|I(|ϵ| > τ)|X, Z]\n\u0003\f\f\n≤C1τ −δ′.\nTherefore, by the choice of τi and Lemma 5, we have\n\f\f\f\n1\nk + 1\nk+1\nX\ni=1\nE[Xi,lgτi(Z⊤\ni θ∗−bi)]\n\f\f\f ≤\n1\nk + 1\nk+1\nX\ni=1\n\f\f\fE[Xi,lgτi(Z⊤\ni θ∗−bi)]\n\f\f\f ≤C2τ −δ′\nk+1.\n(43)\n51\nNext, when all data are normal, we prove the rate of\n\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXi,lgτi(ϵi) −E[Xi,lgτi(ϵi)]\n\t\f\f\f.\nWe basically rehash the proof in Lemma 1.\nCase 1: δ ∈(0, 1]. We divide the k-tuple (1, . . . , k) into mk different subsets H1, . . . , Hmk, where\nmk = ⌈k/⌈λ log k⌉⌉. Here |Hi| = ⌈λ log k⌉for 1 ≤i ≤mk −1, and |Hmj| ≤⌈λ log k⌉. Then we have\nmk ≈k/(λ log k). Without loss of generality, we assume that mk is an even integer.\nLet Yi = Xi,lgτi(ϵi) −E[Xi,lgτi(ϵi)] and ξq =\n1\n⌈λ log k⌉\nP\ni∈H2q−1 Yi, then we know |ξq| ≤2Mτk+1\n(since Xi,lgτi(ϵ) ≤|X|2τi ≤Mτk+1) and E[ξl] = 0 holds. For all B1 ∈σ(ξ1, . . . , ξq) and B2 ∈\nσ(ξq+1), there holds |P(B2|B1) −P(B2)| ≤ϕ(⌈λ log k⌉) for all q. By Theorem 2 of Berkes and\nPhilipp (1979), there exists a sequence of independent variables ηl, l ≥1 with ηl having the same\ndistribution as ξl, and\nP\n\u0010\n|ξq −ηq| ≥6ϕ(⌈λ log k⌉)\n\u0011\n≤6ϕ(⌈λ log k⌉).\nFor any ν > 0, we can take λ ≥(ν + 1)/| log ρ| so that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nq=1\n(ξq −ηq)\n\f\f\f ≥C3k−ν\u0011\n≤P\n\u0010\f\f\f 2\nmk\nmk/2\nX\nq=1\n(ξq −ηq)\n\f\f\f ≥6ϕ(⌈λ log k⌉)\n\u0011\n≤3mkϕ(⌈λ log k⌉) ≤C3k−ν,\n(44)\nwhere C3 > 0 is some constant. Next, we bound the variance of ξq. From equation (20.23) of\nBillingsley (1968) we know that for arbitrary i, j, there is\n\f\fE\n\u0002\nYiYj\n\u0003\f\f ≤2\np\nϕ(|i −j|)\nq\nE[Y 2\ni ]E[Y 2\nj ] ≤2C4ρ|i−j|/2M2τ 1−δ\nk+1.\n52\nThen we compute that\nVar(ηq) = Var(ξq) =\n1\n⌈λ log k⌉2\nh\nX\ni,j∈H2q−1\n{E(YiYj)}\ni\n≤\n2C4\n⌈λ log k⌉2\n⌈λ log k⌉\nX\ni,j=1\nρ|i−j|/2M2τ 1−δ\nk+1\n≤2⌈λ log k⌉C4\n⌈λ log k⌉2\n\u0010\n2\n1 −√ρ −1\n\u0011\nM2τ 1−δ\nk+1 ≤\n4C4\n(1 −√ρ)⌈λ log k⌉M2τ 1−δ\nk+1.\nNotice that ηq’s are all uniformly bounded by 2Mτk+1, we can apply Bernstein’s inequality (Bennett,\n1962) and yield\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nq=1\nηq\n\f\f\f ≥t\n\u0011\n≤2 exp\n\u0010\n−\nmkt2/4\nVar(ηq)/2 + Mτkt/3\n\u0011\n≤exp\n\u0010\n−\nmk⌈λ log k⌉t2/4\n2M2Cϵτ 1−δ\nk+1/(1 −√ρ) + 2⌈λ log k⌉Mτk+1t/3\n\u0011\n.\nWe take t = C(\nq\nτ 1−δ\nk+1 log k/(k + 1) + τk+1 log2 k/(k + 1)) for some C large enough (note that\nk ≈mk⌈λ log k⌉). Then we have that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nq=1\nηq\n\f\f\f ≥t\n\u0011\n= O(k−ν).\n(45)\nCombining (44) and (45) we have that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nq=1\nξq\n\f\f\f ≥C\ns\nτ 1−δ\nk+1 log k\nk + 1\n+ C log2 kτk+1\nk + 1\n\u0011\n= O(k−ν).\n(46)\nNext we denote eξ =\n1\n⌈λ log k⌉\nP\ni∈H2q Yi, then we can similarly show that\nP\n\u0010\f\f\f 2\nmk\nmk/2\nX\nq=1\neξq\n\f\f\f ≥C\ns\nτ 1−δ\nk+1 log k\nk + 1\n+ C log2 kτk+1\nk + 1\n\u0011\n= O(k−ν),\n(47)\n53\nCombining (46) and (47) we can prove that\nP\n \f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXi,lgτi(ϵi) −E[Xi,lgτi(ϵi)]\n\t\f\f\f ≥C\n\u0010\ns\nτ 1−δ\nk+1 log k\nk + 1\n+ log2 kτk+1\nk + 1\n\u0011!\n= O(k−ν).\n(48)\nBy combining it with (43) we have that\nP\n \f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C\n\u0010√\ndτ −δ′\nk+1 +\ns\ndτ 1−δ\nk+1 log k\nk + 1\n+\n√\nd log2 kτk+1\nk + 1\n\u0011!\n= O(k−ν).\nCase 2: δ > 1. Similarly, we can obtain the rate\nP\n \f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXi,lgτi(ϵi) −E[Xi,lgτi(ϵi)]\n\t\f\f\f ≥C\n\u0010r\nlog k\nk + 1 + log2 kτk+1\nk + 1\n\u0011!\n= O(k−ν),\nby directly plugging τk into (48). Then we have that\nP\n \f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C\n\u0010\nτ −δ′\nk+1 +\nr\nlog k\nk + 1 + log2 kτk+1\nk + 1\n\u0011!\n= O(k−ν).\nProof of ii).\nWhen δ > 1 and\nq\nk/ log3 k = O(τk), we define the thresholding level eτ =\nCτ\n√\nk + 1/ log3/2 k, and consider the truncated random variables Xigτi(ϵi)I(|ϵi| ≤eτ). Here Cτ\nis a sufficiently large constant. Then we have that\nP\n\u0010 k+1\nX\ni=1\nXigτi(ϵi) ̸=\nk+1\nX\ni=1\nXigτi(ϵi)I(|ϵi| ≤eτ)\n\u0011\n≤P\n\u0010\n∪k+1\ni=1 {Xigτi(ϵi) ̸= Xigτi(ϵi)I(|ϵi| ≤eτ)}\n\u0011\n≤(k + 1)\nmax\n1≤i≤k+1 P\n\u0010\n|ϵi| > eτ\n\u0011\n= O((k + 1)−(δ−1)/3).\n(49)\nSimilarly as in (43), we have that\n\f\f\f\n1\nk + 1\nk+1\nX\ni=1\nE[Xi,lgτi(ϵi)I(|ϵi| ≤eτ)]\n\f\f\f ≤\n1\nk + 1\nk+1\nX\ni=1\n\f\f\fE[Xi,lgτi(ϵi)I(|ϵi| ≤eτ)]\n\f\f\f ≤C(τ −δ′\nk+1 + eτ −δ).\n(50)\n54\nSimilarly as in the proof of (48), for every ν > 0, there exists C > 0 such that\nP\n \f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXi,lgτi(ϵi)I(|ϵi| ≤eτ) −E[Xi,lgτi(ϵi)I(|ϵi| ≤eτ)]\n\t\f\f\f ≥C\n\u0000r\nlog k\nk + 1 + log2 keτ\nk + 1\n\u0001\n!\n= O(k−ν)\n(51)\nBy the choice of eτ, we know that\nr\nlog k\nk + 1 + log2 keτ\nk + 1 = O\n\u0010r\nlog k\nk + 1\n\u0011\n.\nCombining (49), (50) and (51) we have that\nP\n \f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C\n√\nd\n\u0010\nτ −δ′\nk+1 + eτ −δ +\nr\nlog k\nk + 1\n\u0011!\n= O((k + 1)−min{ν,(δ−1)/3}).\nWhen there are αk fraction of outliers, denote Qk as the index set, we have\n\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2\n≤\n1\nk + 1\n\f\f\f\nX\ni∈Qk\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 +\n1\nk + 1\n\f\f\f\nX\ni/∈Qk\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2\n=O\n\u0000√\ndαkτk+1\n\u0001\n+\n1\nk + 1\n\f\f\f\nX\ni/∈Qk\n\b\nXigτi(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2,\nwhich proves the lemma.\nLemma 8. (Bound of the mixed term) Under the same assumptions as in Theorem 1, for every\nν > 0, there exist constants C and c, such that\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(Ai −H)(bθi−1 −θ∗)\n\f\f\f\n2 ≥C(αk + Ψ2de2\nk log k), ∩k\ni=n0Ei\n\u0011\n≤c(k + 1)−ν.\n55\nProof. Firstly, from (39) and Lemma 5, under the event ∩k\ni=n0Ei, we can bound the term\n\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(E[Ai] −H)(bθi−1 −θ∗)\n\f\f\f\n2\n≤\n1\nk + 1\nk+1\nX\ni=1\n∥E[Ai] −H∥· |bθi−1 −θ∗|2\n≤\n1\nk + 1\nk+1\nX\ni=1\nCΨτ −δ′\ni\nei−1 = O(τ −δ′\nk+1\n√\ndek log k) = O(\n√\nde2\nk log k).\n(52)\nFor ease of notation, we first consider the case when there is no outlier. Similar as in the proof of\nLemma 1, for each k, we evenly divide the tuple {n0, . . . , k} into mk subsets H1, . . . , Hmk, where\nmk = ⌈(k −n0)/⌈λ log k⌉⌉. Here |Hq| = ⌈λ log k⌉for 1 ≤q ≤mk −1, and |Hmk| ≤⌈λ log k⌉. λ is a\nsufficiently large constant which will be specified later. Then we have mk ≈(k −n0)/(λ log k). We\nfurther denote H0 = {1, . . . , n0}. Without loss of generality, we assume that mk is an even integer.\nFor each i ∈{n0, . . . , k}, suppose i ∈Hli, we construct the following random variable\neθi = 1\ni\nli−2\nX\nq=0\nX\nj∈Hq\n(bθj −θ∗) + H−1 1\ni\nli−2\nX\nq=0\nX\nj∈Hq\nXj+1gτj+1(Z⊤\nj+1bθj −bj+1).\n(53)\nWhen lj = 1, we take the sum for the terms in H0. For i ∈H0, we take eθi = bθi −θ∗= bθ0 −θ∗.\nThen by Lemma 9 below we have that,\n‘\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])(bθi−1 −θ∗) −\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])eθi−1\n\f\f\f\n2 ≥CΨ2de2\nk, ∩k\ni=n0Ei\n\u0011\n=O((k + 1)−ν).\n(54)\nMoreover, from the proof of Lemma 9, we can obtain that under ∩k\ni=n0Ei, there is\n|bθi −θ∗−eθi|2 ≤Ψ\n√\ndei.\n(55)\nIt left to bound the term\n1\nk+1\nPk\nj=0(Ai −E[Ai])eθi.\nTo be more precise, we define the σ-field\n56\nGl = σ((Xi, Zi, bi) : i ∈∪2l+1\nj=1 Hj), and construct the random variable\nξl =\nX\ni∈H2l+1\n(Ai+1 −E[Ai+1])eθi\nThen by (55) we know that\nP\n\u0010\n∪i\nn\n|eθi|2 ≥2Ψ\n√\ndei\no\n, ∩k\nj=n0Ej\n\u0011\n= O((k + 1)−ν).\nWe further set\neξl =\nX\ni∈H2l+1\n(Ai+1 −E[Ai+1])eθiI\nn\n|eθi|2 ≤2Ψ\n√\ndei\no\n,\nthen there is\nP\n\u0010 mk/2\nX\nl=1\nξl ̸=\nmk/2\nX\nl=1\neξl, ∩k\ni=n0Ei\n\u0011\n= O(k−ν).\n(56)\nNotice that {eξl −E[eξl|Gl−1], l ≥1} are martingale differences, and there is E[eθi|Gl−1] = eθi for\ni ∈H2l+1. Therefore we have\nE[eξl|Gl−1] =\nX\ni∈H2l+1\nE[(Ai+1 −E[Ai+1])|Gl−1]eθiI\nn\n|eθi|2 ≤2Ψ\n√\ndei\no\n.\nBy Lemma 3 there is\nE\nh\r\rE[Ai+1 −E[Ai+1]|Gl−1]\n\r\r\ni\n≤Cd\np\nϕ(⌈λ log k⌉) = O(k−ν−2),\nfor some λ large enough. Then Markov’s inequality yields\nP\n\u0010\f\f\f1\nk\nmk/2\nX\nl=1\nE[eξl|Gl−1]\n\f\f\f\n2 ≥C\n√\ndek\nk\n, ∩k\ni=n0Ei\n\u0011\n= O(k−ν).\n(57)\n57\nIt is direct to verify that\nsup\nv∈Sd−1 |v⊤(eξl −E[eξl|Gl−1])| ≤CΨλ log k,\nsup\nv∈Sd−1\nmk/2\nX\nl=1\nVar[|v⊤eξl||Gl−1] ≤\nsup\nv∈Sd−1\nmk/2\nX\nl=1\nE[|v⊤eξl|2|Gl−1] ≤CΨ2kde2\nk log k.\nThen we can apply Lemma 2 and yield\nP\n\u0010\f\f\f\nmk/2\nX\nl=1\n(eξl −E[eξl|Gl−1])\n\f\f\f\n2 ≥CΨ(d log2 k + dek\nq\nk log2 k)\n\u0011\n= O(k−νd).\nCombining it with (57) and (56), we have that\nP\n\u0010\f\f\f\n1\nk + 1\nmk/2\nX\nl=1\nξl\n\f\f\f\n2 ≥CΨ\n\u0010d log2 k\nk\n+ dek\ns\nlog2 k\nk\n\u0011\n, ∩k\ni=n0Ei\n\u0011\n= O((k + 1)−ν)\nA similar result holds for the even term. Note that d log2 k\nk\n+ dek\nq\nlog2 k\nk\n= O(de2\nk), and together\nwith (54) we have\nP\n\u0010\f\f\f\n1\nk + 1\nk\nX\nj=0\n(Ai+1 −E[Ai+1])(bθi −θ∗)\n\f\f\f\n2 ≥CΨ2de2\nk log k, ∩k\ni=n0Ei\n\u0011\n= O((k + 1)−ν).\nWhen taking the outliers into consideration, we have that\n\f\f\f\n1\nk + 1\nk\nX\nj=0\n(Ai+1 −E[Ai+1])(bθi −θ∗)\n\f\f\f\n2\n=\n\f\f\f\n1\nk + 1\nX\nj∈Qk\n(Ai+1 −E[Ai+1])(bθi −θ∗)\n\f\f\f\n2 +\n\f\f\f\n1\nk + 1\nX\nj /∈Qk\n(Ai+1 −E[Ai+1])(bθi −θ∗)\n\f\f\f\n2\n=\n\f\f\f\n1\nk + 1\nX\nj /∈Qk\n(Ai+1 −E[Ai+1])(bθi −θ∗)\n\f\f\f\n2 + O\n\u0000αk),\nunder the event ∩k\ni=n0Ei, which proves the lemma by combining it with (52).\n58\nLemma 9. Let eθi be defined in (53), for every ν > 0, there exist constants C and c, such that\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])(bθi−1 −θ∗) −\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])eθi−1\n\f\f\f\n2 ≥CΨ2de2\nk, ∩k\ni=n0Ei\n\u0011\n≤c(k + 1)−ν.\nProof. Under event ∩k\ni=n0Ei, using Lemma 5, we have\n\f\fbθi −θ∗−eθi\n\f\f\n2\n=\n\f\f\f1\ni\ni−1\nX\nj=(li−2)⌈λ log k⌉+n0+1\n(bθj −θ∗)\n\f\f\f\n2 +\n\f\f\fH−1 1\ni\ni−1\nX\nj=(li−2)⌈λ log k⌉+n0+1\nXj+1gτj+1(Z⊤\nj+1bθj −bj+1)\n\f\f\f\n2\n+\n\f\f\f(c\nH\n−1\ni−1 −H−1)1\ni\ni−1\nX\nj=0\nXj+1gτj+1(Z⊤\nj+1bθj −bj+1)\n\f\f\f\n2\n=\n\f\f\f(c\nH\n−1\ni−1 −H−1)1\ni\ni−1\nX\nj=0\nXj+1gτj+1(Z⊤\nj+1bθj −bj+1)\n\f\f\f\n2\n+\n\f\f\fH−1 1\ni\ni−1\nX\nj=(li−2)⌈λ log k⌉+n0+1\nXj+1gτj+1(Z⊤\nj+1θ∗−bj+1)\n\f\f\f\n2 + O\n\u0010ei log k\ni\n\u0011\n.\n(58)\nFor the first term, by Lemma 6 and (42) we have that under event ∩k\ni=n0Ei,\n∥c\nH\n−1\ni−1 −H−1∥≤∥c\nH\n−1\ni−1∥· ∥c\nHi−1 −H∥· ∥H−1∥≤CΨ\n√\ndei.\nOn the other hand, there is\n\f\f\f1\ni\ni−1\nX\nj=0\nXj+1gτj+1(Z⊤\nj+1bθj −bj+1)\n\f\f\f\n2\n≤\n\f\f\f1\ni\ni−1\nX\nj=0\nXj+1gτj+1(Z⊤\nj+1θ∗−bj+1)\n\f\f\f\n2 + M2 1\ni\ni−1\nX\nj=0\n|bθj −θ∗|2 ≤CΨ\n√\ndei log i.\n59\nTherefore, we have that\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])(bθi−1 −θ∗) −\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])eθi−1\n=\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])H−1 1\ni\ni−1\nX\nj=(li−2)⌈λ log k⌉+n0+1\nXj+1gτj+1(Z⊤\nj+1θ∗−bj+1) + O(Ψ2de2\ni )\n=\n1\nk + 1\nk+1\nX\ni=1\nn\ni\nX\nj=(li−2)⌈λ log k⌉+n0+1\n1\nj (Aj −E[Aj])\no\nH−1Xigτi(Z⊤\ni θ∗−bi) + O(Ψ2de2\ni ).\n(59)\nDenote\nY i =\nn\ni\nX\nj=(li−2)⌈λ log k⌉+n0+1\n1\nj (Aj −E[Aj])\no\nH−1Xigτi(Z⊤\ni θ∗−bi).\nIt is direct to verify that\n|E[Yi]|2 ≤\nv\nu\nu\ntE\n\r\r\r\ni\nX\nj=(li−2)⌈λ log k⌉+n0+1\n1\nj (Aj −E[Aj])\n\r\r\r\n2\nE\n\f\fH−1Xigτi(Z⊤\ni θ∗−bi)\n\f\f2\n2\n=O\n\u0010√\nd log k\ni\nτ (1−δ)+/2\ni\n\u0011\n= O(\n√\nde2\ni ).\n(60)\nTherefore, it left bound\n\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(Yi −E[Yi])\n\f\f\f\n2.\nTo this end, we further denote\nξl =\nX\ni∈H4l+1\n(Y i −E[Y i]),\nWe define the σ-field Gl = σ((Xi, Zi, bi) : i ∈∪4l+1\nj=1 Hj), by Lemma 3 we have that\nE\nh\f\f\fE[ξl|Gl−1]\n\f\f\f\n2\ni\n= O\n\u0010√\ndτl log k\nlkν+2\n\u0011\n.\n60\nTherefore, by Markov’s inequality\nP\n\u0010\f\f\f1\nk\nmk/4\nX\nl=1\nE[ξl|Gl−1]\n\f\f\f\n2 ≥C\n√\ndτk log k\nk2\n, ∩k\ni=n0Ei\n\u0011\n= O(k−ν).\n(61)\nIt is direct to verify that\nsup\nv∈Sd−1 |v⊤(ξl −E[ξl|Gl−1])| ≤C\n√\nd log k,\nsup\nv∈Sd−1\nmk/2\nX\nl=1\nVar[|v⊤ξl||Gl−1] ≤\nsup\nv∈Sd−1\nmk/2\nX\nl=1\nE[|v⊤ξl|2|Gl−1]\n≤C1\nmk/4\nX\nl=1\ndτ (1−δ)+\nl\nlog2 k/l2 ≤Cd log3 k.\nThen we can apply Lemma 2 and yield\nP\n\u0010\f\f\f\nmk/4\nX\nl=1\n(ξl −E[ξl|Gl−1])\n\f\f\f\n2 ≥C(d3/2 log2 k + d log2 k)\n\u0011\n= O(k−νd).\nCombining it with (60) and (61), we have that\nP\n\u0010\f\f\f\n1\nk + 1\nmk/2\nX\nl=1\nξl\n\f\f\f\n2 ≥C d3/2 log2 k\nk\n, ∩k\ni=n0Ei\n\u0011\n= O((k + 1)−ν)\nA similar result holds for the average of ξl = P\ni∈H4l+q(Y i −E[Y i]), where q = 0, 2, 3. Substitute\nit into (59), we have that\nP\n\u0010\f\f\f\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])(bθi−1 −θ∗) −\n1\nk + 1\nk+1\nX\ni=1\n(Ai −E[Ai])eθi−1\n\f\f\f\n2 ≥CΨ2de2\nk, ∩k\ni=n0Ei\n\u0011\n=O((k + 1)−ν),\nwhich proves the lemma.\n61\n7.4\nProof of Results in Section 3.2\nProof of Theorem 4. From the proof of Theorem 1, as n0 tends to infinity, we can obtain that\nbθn −θ∗= H−1 1\nn\nX\ni/∈Qn\nXigτi(Z⊤\ni θ∗−bi) + OP\n\u0010√\ndτnαn + de2\nn−1 log2 n\n\u0011\n,\n(62)\nwhere H = E[XZ⊤]. Denote ϵi = Z⊤\ni θ∗−bi, we first bound the term\n\f\f\f 1\nn\nX\ni/∈Qn\nXi(gτi(ϵi) −ϵi)\n\f\f\f\n2\n(63)\n≤\n\f\f\f 1\nn\nX\ni/∈Qn\nXi(gτi(ϵi) −ϵi) −1\nn\nX\ni/∈Qn\nE[Xi(gτi(ϵi) −ϵi)]\n\f\f\f\n2 + 1\nn\nX\ni/∈Qn\n\f\f\fE[Xi(gτi(ϵi) −ϵi)]\n\f\f\f\n2\n=\n\f\f\f 1\nn\nX\ni/∈Qn\nXi(gτi(ϵi) −ϵi) −1\nn\nX\ni/∈Qn\nE[Xi(gτi(ϵi) −ϵi)]\n\f\f\f\n2 + O(\n√\ndτ −2\nn ),\nwhere the last inequality uses the moment assumption on ϵ and similar argument as in (43). To\nbound the first term, we directly compute its variance and use Chebyshev’s inequality.\nMore\nprecisely, by equation (20.23) of Billingsley (1968), for arbitrary i, j and each coordinate l, there is\n\f\f\fE\nh\b\nXi,l(gτi(ϵi) −ϵi) −E[Xi,l(gτi(ϵi) −ϵi)]\n\t\b\nXj,l(gτj(ϵj) −ϵj) −E[Xj,l(gτj(ϵj) −ϵj)]\n\ti\f\f\f\n≤2ρ|i−j|/2q\nE[X2\ni,l(gτi(ϵi) −ϵi)2]E[X2\nj,l(gτj(ϵj) −ϵj)2]\n≤2C0ρ|i−j|/2\n1\nτ 2\ni τ 2\nj\nq\nE[|ϵi|6]E[|ϵj|6] ≤C1\nρ|i−j|/2\nτ 2\ni τ 2\nj\n.\nTherefore we have that for every unit vector v ∈Sd−1, there is\nVar\nh 1\nn\nX\ni/∈Qn\nXi(gτi(ϵi) −ϵi)\ni\n≤C1\nn2\nn\nX\ni,j=1\nρ|i−j|/2\nτ 2\ni τ 2\nj\n≤C1\nn2\nn\nX\ni=1\n2\nτ 2\ni\n\u0010\nn\nX\nj=0\nρj/2\u0011\n≤C2\n1\nnτ 2n\n,\n62\nfor some constant C2 > 0. Therefore, by Chebyshev’s inequality, we have that\n\f\f\f 1\nn\nX\ni/∈Qn\nXi(gτi(ϵi) −ϵi) −1\nn\nX\ni/∈Qn\nE[Xi(gτi(ϵi) −ϵi)]\n\f\f\f\n2 = OP\n\u0010\n√\nd\n(nτ 2n)2/5\n\u0011\n.\n(64)\nCombining (62), (63) and (64), given a unit vector v ∈Sd−1, we have\nv⊤(bθn −θ∗) =v⊤H−1 1\nn\nX\ni/∈Qn\nXiϵi + OP\n\u0010√\ndτnαn + de2\nn−1 log2 n +\n√\ndτ −2\nn\n+\n√\nd\n(nτ 2n)2/5\n\u0011\n,\n= 1\nn\nX\ni/∈Qn\nv⊤H−1Xiϵi + oP\n\u0010 1\n√n\n\u0011\n,\nwhere the remainder becomes oP(1/√n) under some rate constraints. Here the main term is the\naverage of strict stationary and strong mixing sequence v⊤H−1Xiϵi. By Lemma 10, the conditions\nin Theorem 1 of Doukhan et al. (1994) is fulfilled. Then we can apply Theorem 1 of Doukhan et al.\n(1994) and yield\n√n\nσv\n(bθn −θ∗) →N(0, 1),\nwhere\nσ2\nv =\n∞\nX\ni=−∞\nCov\n\u0000v⊤H−1X0ϵ0, v⊤H−1X⊤\ni ϵi\n\u0001\n=v⊤H−1Σ(H⊤)−1v,\nand Σ is defined in (17). Therefore the theorem is proved.\nLemma 10. Let the random variable X satisfies E[|X|1+δ] for some δ > 1, and ϕ(k) = O(ρk)\n(where ρ < 1). Then we have that\nZ 1\n0\nϕ−1(u)Q2\nX(u)du < ∞,\nwhere ϕ−1(u) is the inverse function of ϕ(k), i.e. , ϕ−1(u) = log u/ log ρ, and QX(u) = inf{t :\nP(|X| ≥t) ≤u}.\n63\nProof. By Markov’s inequality, we have that\nP(|X| ≥QX(u)) ≤u ≤\nE[|X|1+δ]\n(QX(u))1+δ ,\n⇒QX(u) ≤\n\u0010E[|X|1+δ]\nu\n\u00111/(1+δ)\n.\nTherefore, we have that\nZ 1\n0\nϕ−1(u)Q2\nX(u)du ≤\nZ 1\n0\nlog u\nlog ρ\n\u0010E[|X|1+δ]\nu\n\u00112/(1+δ)\ndu < ∞,\nas long as δ > 1, which proves the lemma.\nProposition 8. Suppose the conditions in Theorem 1 hold. When δ > 4 and\nq\ni/ log3 i = O(τi),\nfor every ν > 0, there exists constants C, c > 0 such that\nP\n\u0010\n∩n\ni=n0\n\b\n|bθi −θ∗|2 ≥C\n√\ndei\n\t\u0011\n≥1 −cn−min{ν,(δ−4)/3}\n0\n,\nwhere\nen = αnτn +\nr\nlog n\nn\n+ 1\n√\nd\n(c0)2n−n0.\n(65)\nHere δ is defined in the moment condition in (C4).\nProof. When δ > 4 and\n√\nk = O(τk), we denote\nek = αkτk +\nr\nlog k\nk\n+ 1\n√\nd\n(c0)2k−n0,\nfor k ≥n0. We continue from equation (33) in the proof of Theorem 1.\nBy ii) of Lemma 7, there is\nP\n\u0010\f\f\fc\nH\n−1\nk+1\n1\nk + 1\nk+1\nX\ni=1\n\b\nXig(Z⊤\ni θ∗−bi)\n\t\f\f\f\n2 ≥C1\n√\ndek+1, ∩k\ni=n0Ei\n\u0011\n≤C3(k + 1)−(δ−1)/3.\n64\nThen we have that\nP\n\u0010\n|bθk+1 −θ∗|2 ≥Ψ\n√\ndek+1, ∩k\ni=n0Ei\n\u0011\n≤3C3(k + 1)−min{ν,(δ−1)/3},\nwhich yields\nP\n\u0010\n∩k\ni=n0 Ei\n\u0011\n≥1 −\nk\nX\ni=n0+1\n3C3i−min{ν,(δ−1)/3}\n≥1 −3C3 max\nn\n1\nν −1,\n2\nδ −4\no\nn−min{ν−1,(δ−4)/3}\n0\n.\nTherefore, the theorem is proved.\nProof of Proposition 5. By Proposition 8 and Theorem 4, we know that bθn admits the Bahadur\nrepresentation\nv⊤(bθn −θ∗) = v⊤H−1 1\nn\nX\ni/∈Qn\nXi(Z⊤\ni θ∗−bi) + OP\n\u0010\nde2\nn−1 log2 n +\n√\ndτ −2\nn\n+\n√\nd\n(nτ 2n)2/5\n\u0011\n,\nwhere αn = 0 and en−1 is given in (65). When τi = Ciβ for β ≥3/4 we have that the remainder\nterm has an order OP(d log n/n) when n0 →∞, which proves the proposition.\n7.5\nProof of Results in Section 4\nProof of Theorem 6. For simplicity we denote Γk = Cov(X0ϵ0, X−kϵ−k), Y i,k = Xigτi(ϵi)X⊤\ni−kgτi−k(ϵi−k).\nThen by (17) we know\nΣ = Γ0 +\n∞\nX\nk=1\n(Γk + Γ⊤\nk ).\nUnder event (30), for k = 0, . . . , ⌈λ log n⌉, we have that\n\r\r\r 1\nn\nn\nX\ni=⌈ek/λ⌉\nY i,k −1\nn\nn\nX\ni=⌈ek/λ⌉\nXi−kgτi−k(Z⊤\ni−kbθi−k−1 −bi−k)X⊤\ni gτi(Z⊤\ni bθi−1 −bi)\n\r\r\r\n=O\n\u0010 1\nn\nn\nX\ni=⌈ek/λ⌉\nτi|bθi −θ∗|2\n\u0011\n= OP\n\u0010√\ndτnen\n\u0011\n.\n65\nTherefore we have that\n\r\r\rbΣn −\n\u0010 1\nn\nn\nX\ni=1\nY i,0 + 1\nn\nn\nX\ni=1\n⌈λ log i⌉\nX\nk=1\n(Y i,k + Y ⊤\ni,k)\n\u0011\r\r\r = OP\n\u0010√\ndτnen\n\u0011\n.\n(66)\nWe first prove\n\r\rΣ −\n\u0000Γ0 +\n⌈λ log n/2⌉\nX\nk=1\n(Γk + Γ⊤\nk )\n\u0001\r\r = O(n−ν),\n(67)\nfor some ν > 0. By equation (20.23) of Billingsley (1968), for each k, there is\n∥Γk∥=\nsup\nu,v∈Sd−1 E\nh\n|u⊤X0ϵ0v⊤X−kϵ−k|\ni\n≤2\np\nϕ(|k|)\nsup\nu,v∈Sd−1\nq\nE[|u⊤X0ϵ0|2]E[|v⊤X−kϵ−k|2] = O(ρ|k|/2).\nTherefore we can obtain that\n\r\rΣ −\n\u0000Γ0 +\n⌈λ log n/2⌉\nX\nk=1\n(Γk + Γ⊤\nk )\n\u0001\r\r\n≤2\n∞\nX\nk=⌈λ log n/2⌉+1\n∥Γk∥= O(\n∞\nX\nk=⌈λ log n/2⌉+1\nρ|k|/2) = O(ρ⌈λ log n⌉/4) = O(n−ν),\nfor λ ≥4ν/| log ρ|, which proves (67). Next we prove that for k = 0, . . . , ⌈λ log n⌉, there holds\n\r\r\r 1\nn\nn\nX\ni=⌈ek/λ⌉\nY i,k −1\nn\nX\ni,i−k/∈Qn\nE[Y i,k]\n\r\r\r\n=\n\r\r\r 1\nn\nX\ni,i−k/∈Qn\nY i,k −1\nn\nX\ni,i−k/∈Qn\nE[Y i,k]\n\r\r\r + OP\n\u0000τ 2\nnαn\n\u0001\n=OP\n\u0010\nτ 2\nnαn +\ns\nd log2 n\nn\n+ dτ 2\nn log2 n\nn\n\u0011\n.\n(68)\nBy the proof of Lemma 6, we know that\n\r\r\r 1\nn\nX\ni,i−k/∈Qn\nY i,k −1\nn\nX\ni,i−k/∈Qn\nE[Y i,k]\n\r\r\r ≤2 sup\nu,v∈N\n\f\f\f 1\nn\nX\ni,i−k/∈Qn\nu⊤(Y i,k −E[Y i,k])v\n\f\f\f,\n66\nwhere N is a 1/4-net of Sd−1. For k = 0, . . . , ⌈λ log n⌉−1, Denote eFb\nk,a = σ(Y i,k, a ≤i ≤b), then\nby (13) we know that\n|P(B|A) −P(B)| ≤ϕ((j −k)+),\nfor all A ∈eFn\nk,1, B ∈eF∞\nk,n+j for all n, j ≥0. We basically rehash the proof in Lemma 1 and obtain\nthat\nsup\nu,v∈N\nP\n \f\f\f 1\nn\nX\ni,i−k/∈Qn\nu⊤(Y i,k −E[Y i,k])v\n\f\f\f ≥C\n\u0010\ns\nd log2 n\nn\n+ dτ 2\nn log2 n\nn\n\u0011!\n= O(n−(γ+2 log 9)d).\nHere the only difference is that Var(ηl) = O(1) in (22), and u⊤Y i,kv is bounded by τ 2\nn. Therefore\n(68) can be proved.\nLast, we prove that\n\r\r\r 1\nn\nX\ni,i−k/∈Qn\nE[Y i,k] −Γk\n\r\r\r = O\n\u0010\nαn + τ −2\nn\n\u0011\n(69)\nTo see this, we compute that\n\r\r\rE[Y i,k] −Γk\n\r\r\r\n≤\n\r\r\rE[Xigτi(ϵi)X⊤\ni−kgτi−k(ϵi−k)] −E[X0ϵ0X−kgτi−k(ϵ−k)]\n\r\r\r\n+\n\r\r\rE[XiϵiX⊤\ni−kgτi−k(ϵi−k)] −E[X0ϵ0X−kϵ−k]\n\r\r\r\nFor the second term,\n\r\r\rE[XiϵiX⊤\ni−kgτi−k(ϵi−k)] −E[X0ϵ0X−kϵ−k]\n\r\r\r\n=\nsup\nu,v∈Sd−1\n\f\f\fE[u⊤XiϵiX⊤\ni−kv\n\b\ngτi−k(ϵi−k) −ϵi−k\n\t\n]\n\f\f\f\n≤\nsup\nu,v∈Sd−1\nq\nE[|u⊤Xiϵi|2]E[|X⊤\ni−kv\n\b\ngτi−k(ϵi−k) −ϵi−k\n\t\n|2] ≤C1τ −2\ni−k,\nfor some constants C1 > 0. Therefore (69) is proved.\n67\nCombining (66), (67), (68) and (69) we have that\n∥bΣn −Σ∥\n≤\n\r\r\rbΣn −\n\u0010 1\nn\nn\nX\ni=1\nY i,0 + 1\nn\nn\nX\ni=1\n⌈λ log i⌉\nX\nk=1\n(Y i,k + Y ⊤\ni,k)\n\u0011\r\r\r +\n\r\rΣ −\n\u0000Γ0 +\n⌈λ log n/2⌉\nX\nk=1\n(Γk + Γ⊤\nk )\n\u0001\r\r\n+2\n⌈λ log n/2⌉\nX\nk=0\n\r\r\r 1\nn\nn\nX\ni=⌈ek/λ⌉\nY i,k −1\nn\nX\ni,i−k/∈Qn\nE[Y i,k]\n\r\r\r + 2\n⌈λ log n/2⌉\nX\nk=0\n\r\r\r 1\nn\nX\ni,i−k/∈Qn\nE[Y i,k] −Γk\n\r\r\r\n+2\n⌈λ log n⌉\nX\nk=⌈λ log n/2⌉+1\n\r\r\r 1\nn\nn\nX\ni=⌈ek/λ⌉\nY i,k −1\nn\nX\ni,i−k/∈Qn\nE[Y i,k]\n\r\r\r + 2\n⌈λ log n⌉\nX\nk=⌈λ log n/2⌉+1\n\r\r\r 1\nn\nX\ni,i−k/∈Qn\nE[Y i,k] −Γk\n\r\r\r\n+2\n⌈λ log n⌉\nX\nk=⌈λ log n/2⌉+1\n\r\r\rΓk\n\r\r\r\n=\nOP\n\u0010√\ndτnen + n−ν + τ 2\nnαn +\ns\nd log2 n\nn\n+ dτ 2\nn log2 n\nn\n+ αn + τ −2\nn\n\u0011\n=\nOP\n\u0010√\ndτ 2\nnαn +\n√\ndτ −1\nn\n+ τn\nr\nd log n\nn\n+ dτ 2\nn log2 n\nn\n\u0011\n,\nwhich proves the theorem.\n68\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2023-10-04",
  "updated": "2025-03-01"
}