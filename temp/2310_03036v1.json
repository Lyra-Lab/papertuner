{
  "id": "http://arxiv.org/abs/2310.03036v1",
  "title": "A quantum system control method based on enhanced reinforcement learning",
  "authors": [
    "Wenjie Liu",
    "Bosi Wang",
    "Jihao Fan",
    "Yebo Ge",
    "Mohammed Zidan"
  ],
  "abstract": "Traditional quantum system control methods often face different constraints,\nand are easy to cause both leakage and stochastic control errors under the\ncondition of limited resources. Reinforcement learning has been proved as an\nefficient way to complete the quantum system control task. To learn a\nsatisfactory control strategy under the condition of limited resources, a\nquantum system control method based on enhanced reinforcement learning\n(QSC-ERL) is proposed. The states and actions in reinforcement learning are\nmapped to quantum states and control operations in quantum systems. By using\nnew enhanced neural networks, reinforcement learning can quickly achieve the\nmaximization of long-term cumulative rewards, and a quantum state can be\nevolved accurately from an initial state to a target state. According to the\nnumber of candidate unitary operations, the three-switch control is used for\nsimulation experiments. Compared with other methods, the QSC-ERL achieves close\nto 1 fidelity learning control of quantum systems, and takes fewer episodes to\nquantum state evolution under the condition of limited resources.",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nA Quantum System Control Method Based on Enhanced\nReinforcement Learning\nWenjie Liu1,2 · Bosi Wang1 · Jihao Fan3 · Yebo Ge1 · Mohammed\nZidan4\nReceived: date / Accepted: date\nAbstract Traditional quantum system control meth-\nods often face different constraints, and are easy to\ncause both leakage and stochastic control errors un-\nder the condition of limited resources. Reinforcement\nlearning has been proved as an efficient way to com-\nplete the quantum system control task. To learn a sat-\nisfactory control strategy under the condition of lim-\nited resources, a quantum system control method based\non enhanced reinforcement learning (QSC-ERL) is pro-\nposed. The states and actions in reinforcement learning\nare mapped to quantum states and control operations in\nquantum systems. By using a new enhanced neural net-\nworks, reinforcement learning can quickly achieve the\nmaximization of long-term cumulative rewards, and a\nquantum state can be evolved accurately from an ini-\ntial state to a target state. According to the number\nof candidate unitary operations, the three-switch con-\ntrol is used for simulation experiments. Compared with\nother methods, the QSC-ERL achieves close to 1 fi-\ndelity learning control of quantum systems, and takes\n\u0000 Wenjie Liu\nwenjiel@163.com\nBosi Wang\nbosi@nuist.edu.cn\n1 School of Computer and Software, Nanjing University\nof Information Science and Technology, Nanjing 210044,\nChina\n2 Engineering Research Center of Digital Forensics Min-\nistry of Education, Nanjing 210044, China\n3 School of Electronic and Optical Engineering, Nanjing\nUniversity of Science and Technology, Nanjing 210094,\nChina\n4 Hurghada Faculty of Computers and Artificial Intelli-\ngence, South Valley University, Egypt\nfewer episodes to quantum state evolution under the\ncondition of limited resources.\nKeywords quantum system control · reinforcement\nlearning · quantum computing · machine learning ·\nneural networks\nMathematics Subject Classification (2020) 81Q93 ·\n81P68 · 68T07\n1 Introduction\nQuantum system control is one of the keys to the de-\nvelopment of quantum information technology, which\nhas been applied in many fields, such as multi-photon\ninterference measurement (Vedaie et al. 2018), quan-\ntum error correction (Fosel et al. 2018), quantum states\npreparation (Bukov et al. 2018). Since most quantum\nsystems cannot meet the two constraint conditions, one\nis strong regular free Hamiltonian, the other is that in-\nteraction Hamiltonian are fully connected (Meng and\nCong 2022), it is difficult to implement active manipu-\nlation or control. In order to manipulate quantum sys-\ntems to a ideal performance, different control methods\nhave been developed (Patsch et al. 2020; An et al. 2021;\nTorosov et al. 2021). For a quantum system with lim-\nited control resources, it is a challenge to effectively\nand accurately control quantum states evolution under\nperturbation.\nTraditional learning algorithms (such as gradient al-\ngorithms (Chakrabarti and Rabitz 2007; Roslund and\nRabitz 2009), genetic algorithms (Tsubouchi and Mo-\nmose 2008) have shown excellent control effects under\nspecific experimental environment. But in practical, the\nquantum system to be manipulated usually has differ-\nent restrictions. There is a class of quantum system\narXiv:2310.03036v1  [cs.ET]  30 Sep 2023\n2\nWenjie Liu1,2 et al.\ncontrol problem with limited control resources. In this\ncase, the gradient algorithms are not suitable for solving\nthe above problems, and the genetic algorithms need a\nlot of experimental data to optimize the control perfor-\nmance that complicates the resolution of the problem.\nWith the advent of quantum information technol-\nogy and the upsurge of machine learning (Abualigah et\nal. 2021; Abualigah et al. 2021; Abualigah et al. 2021;\nAbualigah et al. 2021), many researchers have found\nthat machine learning can effectively help to find the\noptimal strategy to solve the control problem of quan-\ntum systems (Chunlin et al. 2012; Chen et al. 2013;\nPalittapongarnpim et al. 2017). In particular, studies on\nquantum system control based on reinforcement learn-\ning have been increasing gradually. Reinforcement learn-\ning (Fang et al. 2020) interacts with the environment\nin the form of rewards and punishments. Vedaie et al.\n(2018) applied reinforcement learning to realize multi-\nphoton interference measurement. Cardenas-Lopez et\nal. (2018) proposed a protocol for quantum reinforce-\nment learning, which does not require coherent feed-\nback during the learning process and can be imple-\nmented in a variety of quantum systems. Fosel et al.\n(2018) showed how a network-based “agent” can dis-\ncover a complete quantum error correction method to\nprotect qubits from noise. In addition, Bukov et al.\n(2018) used reinforcement learning to prepare the de-\nsired quantum states. They also successfully used Q-\nlearning (Watkins et al. 1992) to control quantum sys-\ntems (Bukov 2018). Yu et al. (2019) used quantum rein-\nforcement learning to make a qubit “agent” adapt to the\nunknown quantum system “environment” to achieve\nmaximum overlap. Niu et al. (2019) used deep reinforce-\nment learning and proposed a quantum control frame-\nwork for fast and high-fidelity quantum gate control\noptimization. Zhang et al. (2019) successfully used re-\ninforcement learning algorithm to solve a class of quan-\ntum state control problems, and made a theoretical\nanalysis. However, the above methods have high re-\nquirements on hardware resources in practical and are\nnot effective for solving a class of resource-constrained\nquantum system control problems.\nIn order to complete the evolution of quantum states\nquickly and efficiently under the condition of insuffi-\ncient hardware conditions and limited numbers and types\nof unitary operations that can be used, a quantum sys-\ntem control method based on enhanced reinforcement\nlearning (QSC-ERL) is proposed. The quantum sys-\ntem control problem under the condition of limited re-\nsources is modeled using reinforcement learning algo-\nrithm. By using a proposed enhanced neural networks,\nreinforcement learning can more quickly achieve the\nmaximization of long-term cumulative rewards, and a\nquantum state can be evolved accurately from the ini-\ntial state to the target state. The simulation experi-\nment is implemented by Python programming language\nand Linalg tool library. The result shows that compared\nwith other methods, the QSC-ERL can achieve high fi-\ndelity learning control of quantum systems, and takes\nfewer episodes to achieve quantum state evolution un-\nder the condition of limited resources.\nThe main contributions of this paper are: (1) Vari-\nous reinforcement learning algorithms are used for vali-\ndate the effectiveness and generality of quantum system\ncontrol methods based on reinforcement learning. (2)\nA quantum system control method based on enhanced\nreinforcement learning (QSC-ERL) is proposed to effi-\nciently solve the control problem of quantum systems\nwith limited control resources.\nThe rest of this paper is structured as follows. In\nSec. II, we briefly overview the preliminaries about quan-\ntum system control and reinforcement learning. In Sec.\nIII, we model the quantum system control problem and\npresent our novel method. In Sec. IV and V, we respec-\ntively show the results of simulation experiments and\ndraw our conclusions.\n2 Preliminaries\n2.1 Learning control of quantum systems\nLearning control methods are powerful for solving quan-\ntum system control problems (Ma and Chen 2020). The\nlearning methods are often optimized by multiple iter-\nations to realize the evolution of qubits from an ini-\ntial state to the desired target state. In this paper,\nthe task of quantum system control is set as the quan-\ntum pure state transition control problem of n-order\nquantum system. For the free Hamiltonian H0 of n-\norder quantum system, its eigenstate can be defined as\nD = {|ϕi⟩}N\ni=1. The quantum state to be evolved\n\f\fψ(t)\n\u000b\nof a controlled system can be extended according to the\neigenstates in set D:\n\f\fψ(t)\n\u000b\n=\nN\nX\ni=1\nci(t) |ψi⟩,\n(1)\nwhere the complex number ci(t) satisfies PN\ni=1 |ci(t)|2 =\n1.\nIn order to achieve the active control of the quantum\nsystem, the control Hamiltonian Hc is introduced into\nthe control u(t) ∈L2(R), which is independent of time\nand interacts with the quantum system. The\n\f\fψ(t=0)\n\u000b\ncan be redefined as |ψ0⟩. The C(t) = (Ci(t))N\ni=1 evolves\nA Quantum System Control Method Based on Enhanced Reinforcement Learning\n3\naccording to the Schr¨odinger equation:\n\u001a\nι¯h ˙C(t) = [A + u(t)B]C(t)\nC(t = 0) = C0\n,\n(2)\nwhere ι = √−1,\nC0 = (c0i)N\ni=1,\nc0i = ⟨φi | ψ0⟩,\nPN\ni=1 |c0i|2 = 1, ¯h is the reduced Planck constant, and\nthe matrices A and B correspond to the free Hamilto-\nnian H0 and the controlled Hamiltonian Hc of the quan-\ntum system respectively. U(t1→t2) represents an unitary\noperation for any state\n\f\fψ(t1)\n\u000b\nof the quantum system.\nThe\n\f\fψ(t2)\n\u000b\n= U(t1→t2)\n\f\fψ(t1)\n\u000b\nof the quantum system is\nthat the quantum state\n\f\fψ(t1)\n\u000b\nevolves from time t = t1\nto time t = t2. In addition, U(t1→t2) can also be defined\nas U(t), t ∈[t1, t2].\nIn fact, if the quantum systems evolve freely without\ncontrol resources limited, it can also arrive at the tar-\nget state from an initial state. However, there are two\nunfavorable problems in this way of free evolution con-\ntrol: One is that it is difficult to satisfy the conditions\nin practice, and will waste a lot of control resources\nto evolve from an initial state to the desired target\nstate. The other is that free evolutionary control has\nno certain control law, and is unable to be determined\nwhen the quantum system reaches the target state. Our\nstudy mainly aims at solving a class of control resource-\nlimited quantum system control problem.\n2.2 Quantum control landscapes\nThe quantum control landscapes (Chakrabarti and Ra-\nbitz 2007) has provided a theoretical basis for analyz-\ning the learning control problem of quantum systems,\nwhich can be defined as the mapping between the con-\ntrol Hamiltonian and the correlation value of the con-\ntrol performance function. The task of quantum sys-\ntem control can be defined as a problem of maximizing\nthe target performance function. In other words, it can\nbe transformed into a problem of maximizing the state\ntransition probability from the initial state to the de-\nsired target state. For the state transition control prob-\nlem, the quantum control transition can be defined as\nJ(u) =tr(U(ε,T )|ψinitial⟩\n⟨ψinitial|U †\n(ε,T )|ψtarget⟩⟨ψtarget|),\n(3)\nwhere tr(·) is the trace operation, U † is the ad-joint of\nU, |ψinitial⟩is the initial quantum state, |ψtarget⟩is the\ndesired target quantum state.\nIn this paper, it is assumed that the control set\n{uj, j = 1, 2, . . . , m} allowed to operate in a controlled\nquantum system can be given in advance, where each\ncontrol uj corresponds to an unitary operation Uj. The\ngoal of learning control is to evolve control from the ini-\ntial state |ψinitial⟩to the desired target state |ψtarget⟩,\nand learn a global optimal control sequence u∗:\nu∗= arg max\nu\nJ(u).\n(4)\n2.3 Reinforcement learning\nReinforcement learning (Fang et al. 2020) is described\nby Markov Decision Process (MDP), which is usually\ndefined by the quadruple ⟨S, A, P, R⟩. The S is the\nset of states, A is the set of actions, and the state\ns ∈S, the action a ∈A. The state transition function\nP (s, a, s′) represents the probability of state transition.\nThe R (s, a, s′) represents the reward value function.\nP (s, a, s′) and R (s, a, s′) only depend on the current\nstate s and action a that have nothing to do with other\nhistorical states and actions. The MDP which adopts\nthe discount criterion is denoted as M = (S, A, P, γ, R),\nwhere γ is the discount factor.\nReinforcement learning agents learn by interacting\nwith external environment. Specifically, the agent ob-\nserves the state st ∈S at each discrete time step t ∈\n[0, T], where T is the end time, and selects an action\nat ∈A used for transitioning the state st ∈S to the\nnext state st+1 ∈S with the probability p. After per-\nforming an action, the agent is usually given a scalar\nreward signal rt+1, which reflects how good or bad the\naction was. The learning process mentioned above is\nrepeated continuously until the agent can learn an op-\ntimal strategy, which is a mapping from the state space\nS to the action set A.\nQ-learning proposed by Watkins et al. (1992) is an\noffline reinforcement learning algorithm, and is described\nin Algorithm 1. The iteration of the Q-value func-\ntion and the strategy selection are independent of each\nother. The approximation goal of Q-learning can be de-\nfined as r + γ maxa′ Q (s′, a′). The agent can choose ac-\ntions according to the greedy algorithm or other non-\noptimal strategies.\n3 Methods\n3.1 Problem modeling\nThe two-level quantum system (D’Alessandro and Dahleh\n2001) is representative in filed of quantum system con-\ntrol. The spin 1/2 system is one of the typical two-level\nquantum systems for theoretical and practical research.\nThe state |ψ⟩of the spin 1/2 system can be defined as:\n|ψ⟩= cos θ\n2|0⟩+ etϕ sin θ\n2|1⟩,\n(5)\n4\nWenjie Liu1,2 et al.\nAlgorithm 1 The Q-Table learning algorithm\n1: Randomly initialize the Q table;\n2: for episode = 1, M do\n3:\nRandomly initialize the s state;\n4:\nfor step = 1, T do\n5:\nSelect an action a according to the Q table;\n6:\nExecute action a, receive reward r, enter state s′;\n7:\nQ(s, a) =\nQ(s, a) + α(r + γ maxa′∈A Q(s′, a′) −Q(s, a));\n8:\ns←s′;\n9:\nend for\n10: end for\nwhere θ ∈[0, π] and ϕ ∈[0, 2π] represent the polar and\nphase angles respectively. A point ⃗a on the unit sphere\ncan be defined as\n⃗a = (x, y, z) = (sin θ cos ϕ, sin θ sin ϕ, cos θ).\n(6)\nThe aim is to design the control of two-level quantum\nsystem based on reinforcement learning. In the follow-\ning, the problem of quantum system control based on\nreinforcement learning is modeled and described.\nThe agent in reinforcement learning learns through\ncontinuous interaction with the environment. Specific\nto the quantum system environment, our method di-\nvides the state space of the quantum system into a finite\ndiscrete set of states S. Set A = {uj, j = 1, 2, . . . , m} is\ndefined as a limited set of executable actions (unitary\noperations) in a quantum environment. Specifically, for\nthe three-switch control, the m is set to 3. Whenever\nthe agent performs action a and the state is transformed\nfrom s to s′, it will receive the feedback value, and using\nthe fidelity as the reward:\nr =\n\n\n\n10, fidelity ≤0.5\n100, 0.5 < fidelity ≤0.7\n10000, fidelity > 0.7\n.\n(7)\nThe goal of reinforcement learning is to obtain an opti-\nmal method π∗and the global optimal control sequence\nu∗as Eq. (4).\nFor quantum systems, the agent of reinforcement\nlearning obtains the optimal method by maximizing\nthe long-term cumulative reward in the process of in-\nteracting with the environment of quantum systems.\nTherefore, the agent also needs to constantly interact\nwith the external environment and learns through trial\nand error. Specifically, the permitted controls at each\ncontrol step for any quantum state are U1 (no control),\nU2 (positive impulse control), and U3 (negative impulse\ncontrol), which is defined as follows:\nU1 = e−ιIz π\n15 ,\nU2 = e−ι(Iz+0.5Ix) π\n15 ,\nU3 = e−ι(Iz−0.5Ix) π\n15 ,\n(8)\nwhere Iz = 1\n2\n\u0012 1 0\n0 −1\n\u0013\n, Ix = 1\n2\n\u0012 0 1\n1 0\n\u0013\n. The state of the\nquantum system in evolutionary control will be limited\nby the three-switch control. The agent of reinforcement\nlearning will learn under the norms of the three-switch\ncontrol in interactive learning with the environment of\nthe quantum system. It is mainly embodied in the ac-\ntion selection of the agent in any quantum system state.\nUnder the three-switch control, each action can be per-\nformed by the agent is U1, U2 and U3.\nUnder the above control conditions, a global opti-\nmal control method is obtained by using proposed rein-\nforcement learning algorithm to minimize the number\nof control sequences, so that the spin 1/2 system can\nreach the target state from the initial state.\n3.2 Enhanced reinforcement learning\nIn order to improve the learning efficiency of Q learn-\ning algorithm (Watkins and Dayan 1992) without prior\nknowledge, it is important to improve the foresight abil-\nity of the learning agent. But it brings the following two\nproblems in practice: 1) The state space increases, caus-\ning the “dimensionality disaster”, which greatly reduces\nthe learning efficiency; 2) The visible space of the learn-\ning agent is reduced, making the agent’s search process\nmore blind.\nTo solve the above two problems, a new enhanced\nreinforcement learning algorithm is proposed. The en-\nhanced reinforcement learning shown in Fig. 1 consists\nof a quantitative Q table and a qualitative V value\nheuristic function obtained by enhanced neural net-\nwork. And the description of the algorithm is shown in\nAlgorithm 2. As the action a is executed, the reward r\nis obtained, state s and state s′ will change accordingly.\nThe agent trains a enhanced neural network for learn-\ning a table space, which can gradually form a heuristic\nfunction to guide the agent to efficiently obtain optimal\nstrategies for the evolution of quantum states.\n3.2.1 Heuristic function based on enhanced neural\nnetwork\nTo build a generalization and foresight capacity to avoid\nthe blind behavior of the agent, a heuristic function\nbased on enhanced neural network is proposed. The\nQ-table in enhanced reinforcement learning is updated\nwith the execution of actions. At the same time, the\nenhanced neural network shown in Fig. 2 is trained,\nand a V value fitting surface is gradually developed.\nThe heuristic function based on enhanced neural net-\nwork thereby shows up to guide the optimization and\nupdating of the new quantum system control method\nA Quantum System Control Method Based on Enhanced Reinforcement Learning\n5\nFig. 1 An overview of enhanced reinforcement learning: the orange rectangle is given by the environment. St and St+1 are\ninput into the enhanced neural network which is abbreviated to E network. The algorithm selects Q∗\nt according to at from\nQStand maxQt+1 from QSt+1 respectively. Then calculating the loss for updating the enhanced neural network between “the\nblue rectangles”.\nAlgorithm 2 The enhanced reinforcement learning al-\ngorithm\n1: Initialize the Q table randomly;\n2: Initialize the neural network as a qualitative layer;\n3: for episode = 1,M do\n4:\nInitialize parameters s, ε0, γ, λ, α, β, e = 0;\n5:\nfor step = 1,T do\n6:\nGenerate ε ∈[0, 1) randomly;\n7:\nif ε ≤1 −ε0 then\n8:\nSelect action a = arg maxQ(s, a), a ∈A;\n9:\ne = γλe +\n∂\n∂w VNN(s)\n10:\nelse\n11:\nSelect action a randomly;\n12:\nif a == arg maxb∈A Q(s, b) then\n13:\ne = γλe +\n∂\n∂w VNN(s)\n14:\nelse\n15:\ne = 0;\n16:\nend\n17:\nend\n18:\nExecute action a, get reward r, the next state s′;\n19:\nGet VNN(s) and VNN (s′) from neural network;\n20:\nUpdate the enhanced neural network:\nw = w + β(r(s, a) + γVNN(s′) −VNN(s))e;\n21:\nF (s, a, s′) = γVNN (s′) −VNN(s);\n22:\nQ(s, a) = Q(s, a) + α(r(s, a)+\nF(s, a, s′) + γ max Q(s′, a′) −Q(s, a))\n23:\ns ←s′;\n24:\nend for\n25: end for\nbased on enhanced reinforcement learning (QSC-ERL).\nInspired by common convolutional neural network (Gu\net al. 2018) and residual neural network (He et al. 2016),\nthe enhanced neural network can make full use of the\nextracted features. The state s is the input of the neu-\nral network, and the Q values got by the probability\nof actions is the output, where N is the number of ac-\ntions. In order to obtain the nonlinear characteristics\nmore comprehensively, the Leaky ReLU is selected as\nactivation function to give all negative values a non-zero\nslope. The heuristic function F (s, a, s′) participating in\nthe update of the Q table takes s and s′ as input and\ngets the V value output which is defined as VNN(s) and\nVNN (s′) in state s and s′ respectively. And the heuris-\ntic function is defined as\nF (s, a, s′) = γVNN (s′) −VNN(s).\n(9)\n3.2.2 The parameters updating method\nTo build an effective parameters updating method, and\naccelerate training speed of QSC-ERL, the eligibility\ntrace (Singh and Suttun 1996) is introduced. The error\nobtained by updating can be passed back several steps\nto speed up the learning of the enhanced neural net-\nwork and provide an effective inspiration for the whole\nalgorithm.\nThe learning of Q table can be defined as\nQ(s, a) =Q(s, a) + α[r(s, a, s′) + F(s, a, s′)\n+ γ max\na′ Q(s′, a′) −Q(s, a)],\n(10)\n6\nWenjie Liu1,2 et al.\nFig. 2 The enhanced neural network architecture: For each\nstate s fed into the network, the network extracts features\nand outputs Q values.\nand the updating of V values can be defined as\nV (s) = max\na\nQ(s, a).\n(11)\nWhen the agent performs a non-greedy action, its next\nstate s′ often does not obtain the largest Q value. The\nQSC-ERL will update the current state-action paired\nQ value according to the V value of the next state\nobtained by the greedy strategy. For updating the en-\nhanced neural network, when the agent requires the V\nvalue according to the greedy strategy, the eligibility\ntrace is also updated. When the agent performs a non-\ngreedy strategy, the eligibility trace is set as 0, prevent-\ning the error from propagating backward.\nTo update the weights of the enhanced neural net-\nwork, the gradient descent method is adopted which\ncan be defined as\n∆wt =β(r(st) + γVNN(st+1) −VNN(st))×\nt\nX\nk=0\n(γλ)tk ∂\n∂wVNN(sk),\n(12)\nwhere β is the learning rate, 0 < β < 1, λ is the eligi-\nbility trace coefficient, 0 < λ < 1.\nThe agent updates the weight of the neural net-\nwork through the difference value r (st)+γVNN (st+1)−\nVNN (st) between the next predicted V value of state s\nand the current target V value. The difference value\ncan be used for updating the V value in other state. If\nthe eligibility trace is defined as\net =\nt\nX\nk=0\nγλ ∂\n∂wVNN(sk) = γλet−1 + ∂\n∂wVNN(st),\n(13)\nEq. (12) can be rewritten as\n∆wt = β(r(st) + γVNN(st+1) −VNN(st))et.\n(14)\nIt is easy for modifying the weights from the hidden\nlayer of the neural network to the output layer, and\nthen modify the weights from the input layer to the\nhidden layer through the back propagation.\nThe QSC-ERL is carried out synchronously in the\nlearning of Q-Table and the enhanced neural network.\nThe Q-Table based reinforcement learning can obtain\nmore accurate results, but the speed of learning is slow.\nThe enhanced neural network is not accurate enough,\nbut it has better generalization performance. In the ini-\ntial stage of learning, the effect is not obvious. But with\ncontinuous learning, by using the parameters updating\nmethod, the enhanced neural network is gradually es-\ntablished the trend information, and the convergence\nspeed can be greatly improved.\n4 Simulation experiments\n4.1 Settings\nSince it is difficult to verify the validity and efficiency of\nthe algorithm in real quantum computers, the realiza-\ntion of the experiment is inseparable from the quantum\ncontrol landscapes (Chakrabarti and Rabitz 2007). The\nsimulation experiment is implemented by Python pro-\ngramming language and Linalg tool library. Full train-\ning for a given scenario can be achieved on a single\nCPU+GPU workstation (CPU: Intel Xeon Gold 5218,\nGPU: GeForce RTX 2080 Ti 11G). The state space of\nthe quantum system will be reconstructed from the ini-\ntial state sinitial = |ψinitial⟩to the target state starget =\n|ψtarget⟩. The state set is S = {si = |ψi⟩, i = 1, 2, . . . , n},\nand the executable action set is A = {aj = Uj, j =\n1, 2, . . . , m◦}. For the spin 1/2 system, the initial state\nis set as |ψinitial⟩(θ = (π/60), ϕ = (π/30)), and the tar-\nget state is |ψtarget⟩(θ = (41π/60), ϕ = (29π/30)). The\nEq. (3) and Eq. (5) can be utilized to construct the\nwhole quantum simulation environment. The setting of\nthe reward in QSC-ERL is according to the Eq. (7).\nHere is the parameter settings shown as table 1.\n4.2 Evaluation index\nFidelity is a evaluation index to measure the distance\nbetween density operators. It allows us to compare how\nA Quantum System Control Method Based on Enhanced Reinforcement Learning\n7\nTable 1 The parameter settings of the QSC-ERL\nName\nValue\nmaximum episode\n500\nlearning rate\n0.01\nreward decay\n0.9\ne greedy\n0.99\nmemory size\n2000\nthe state of the system at any given moment is differ-\nent from the initial state, or how the state of a sys-\ntem is different from a reference state. It allows us to\nmeasure quantitatively how different two states really\nare. For two density matrices ρ, σ it is generalized as\nthe largest fidelity between any two purifications of the\ngiven states. And the fidelity function can be defined\nas\nF(ρ, σ) = (tr\nq√ρσ√ρ)2,\n(15)\nwhere ρ and σ are the density matrix of source infor-\nmation and target information respectively.\n4.3 Results and analysis\nThe simulation experiments is carried out under the\nthree switch control paradigm. The goal of the experi-\nment is to control the spin 1/2 system from the initial\nstate |ψinitial⟩to the target state |ψtarget⟩. The main\npurpose is to explore the effectiveness of reinforcement\nlearning algorithm for solving quantum control prob-\nlem.\nTherefore, the simulation experiments is divided for\ntwo parts: one is that the tabular Q-learning (TQL)\n(Sutton and Barto 2018), deep Q-learning (DQL) (Mnih\net al. 2015) and policy gradient (PG) (Sutton et al.\n2000) are applied to explore the effectiveness of rein-\nforcement learning algorithm for solving quantum con-\ntrol problem. The other is that the NN-QSC (Fosel et al.\n2018) and the DRL-QSC (An and Zhou 2019) are com-\npared for verifying that the proposed QSC-ERL per-\nformed better than its peers. The parameters of the\nreinforcement learning algorithms involved in the ex-\nperiment are set as follows: For all state-action paired,\nthe Q value is initialized to 0, the discount factor is\nγ = 0.99, the learning rate is α = 0.1, and the action\nselection probability is initialized to 1/3.\nFig. 3 shows the comparison of fidelity between al-\ngorithms, where the X-axis is the number of episode,\nand the Y-axis is fidelity. It can be seen from Fig. 3\nthat reinforcement learning has certain effects on solv-\ning the quantum system control problems. Since the\nTable 2 The comparison of the number of episodes between\nalgorithms\nName\nEpisodes\nFidelity\nTQL\n452\n0.73\nPG\n311\n0.99\nDQL\n135\n0.99\nNN-QSC\n171\n0.99\nDRL-QSC\n60\n0.99\nQSC-ERL\n42\n0.99\nTQL algorithm can not converge rapidly during train-\ning, the fidelity is the lowest. The PG algorithm has\nbetter convergence, but only a little bit at one episode.\nThe other four methods have good results. The DQL al-\ngorithm adopted convolutional neural network to guide\nthe learning of the Q learning algorithm. Although it\nsolves the problem of not being able to update Q table\nwell when there are many actions, it is difficult for sim-\nple neural networks to learn the useful features of quan-\ntum systems. If want to get a high fidelity between the\nfinal state and the target state, it needs to train with\nmore episodes and data set. Due to the high correla-\ntion between states in the training process, the NN-\nQSC and the DRL-QSC may fall into local optimum\nor be difficult to converge. Our QSC-ERL use the en-\nhanced neural network to effectively make use of the\ndifferential features before and after the evolution of\nthe quantum state. By introducing the eligibility trace\nto update parameters, The QSC-ERL can quickly find\nthe optimal control strategy of the quantum system.\nTable 2 shows the number of episodes when the fidelity\ncan get the maximum, and total number of episodes is\nset to 500. The data is taken from the average value of\n100 experiments. It represents that the ability of algo-\nrithms can make the quantum system from the initial\nstate to the desired target state. The experimental re-\nsults show that most methods converge after training\nand make the quantum system from the initial state to\nthe desired target state. Specifically, for the TQL, the\nmaximum of the fidelity is 0.73, and others can reach\n0.99 after total training. The PG requires about 311\nepisodes and the DQL requires about 135 episodes. It\nmeans that the reinforcement learning based on neu-\nral network has the better performance in some degree\nthan the common RL algorithm. The NN-QSC requires\nabout 171 episodes to control the evolution of the quan-\ntum system from the initial state to the target state\nwhile the DRL-QSC requires about 60 episodes, and\nthe QSC-ERL requires the 42 episodes. So our proposed\nQSC-ERL algorithm is faster than the NN-QSC and the\nDRL-QSC for controlling the evolution of the quantum\nsystem from the initial state to the target state.\n8\nWenjie Liu1,2 et al.\nFig. 3 The comparison of fidelity between algorithms. (a)Fidelity of the TQL algorithm. (b) Fidelity of the PG algorithm.\n(c) Fidelity of the DQL algorithm. (d) Fidelity of the NN-QSC algorithm. (e) Fidelity of the DRL-QSC algorithm. (f) Fidelity\nof the QSC-ERL algorithm.\n5 Conclusion\nIn this paper, a quantum system control method based\non enhanced reinforcement learning (QSC-ERL) is pro-\nposed to achieve the learning control of the spin 1/2 sys-\ntem. A satisfactory control strategy is obtained through\nenhanced reinforcement learning so that the quantum\nsystem can be evolved accurately from the initial state\nto the target state. Compared with other methods, our\nmethod can achieve the quantum system control with\nhigh fidelity, and improve the control efficiency of quan-\ntum systems.\nIt should be noted that our method is sufficient\nfor the evolution of quantum state in spin 1/2 system.\nOther difficult quantum control problems include quan-\ntum error correction based on bosonic codes (Michael et\nal. 2016) and quantum state preparation in the single-\nphoton manifold (Vrajitoarea et al. 2020). And it is a\nvaluable work to conduct a study on providing solu-\ntions by using learning theories (Li et al. 2018; Zhang\nand Wang 2020) and neural network (Xu et al. 2019;\nHu et al. 2020), which is also one of our next research.\nAcknowledgements The authors would like to thank the\nanonymous reviewers and editors for their comments that im-\nproved the quality of this paper. This work is supported by\nthe National Natural Science Foundation of China (62071240,\n61802175), the Natural Science Foundation of Jiangsu Province\n(BK20171458), and the Priority Academic Program Develop-\nment of Jiangsu Higher Education Institutions (PAPD).\nDeclarations\nConflict of interest The authors declare that they\nhave no conflict of interest.\nEthical statement Articles do not rely on clinical tri-\nals.\nHuman and animal participants All submitted m-\nanuscripts containing research which does not involve\nhuman participants and/or animal experimentation.\nReferences\n1. Abualigah L, Diabat A, Mirjalili S, Abd Elaziz\nM, Gandomi AH (2021a) The arithmetic opti-\nmization algorithm. Computer Methods in Ap-\nplied Mechanics and Engineering 376:113609,DOI\nhttps://doi.org/10.1016/j.cma.2020.113609\n2. Abualigah L, Diabat A, Sumari P, Gandomi A\n(2021b) Applications, deployments, and integration\nof internet of drones (iod): A review. IEEE Sensors\nJournal PP:1–1, DOI 10.1109/JSEN.2021.3114266\n3. Abualigah L, Elsayed Abd Elaziz M, Sumari P,\nGeemZW, Gandomi A (2021c) Reptile search al-\ngorithm (rsa): A nature-inspired meta-heuristic\noptimizer.\nExpert\nSystems\nwith\nApplications\n191:116158, DOI 10.1016/j.eswa.2021.116158\n4. Abualigah\nL,\nYousri\nD,\nAbd\nElaziz\nM,\nEwees\nAA,\nAlqaness\nMA,\nGandomi\nAH\n(2021d)\nAquila\noptimizer:\nA\nnovel\nmeta-\nheuristic\noptimization\nalgorithm.\nComputers\nA Quantum System Control Method Based on Enhanced Reinforcement Learning\n9\n&\nIndustrial\nEngineering\n157:107250,\nDOI\nhttps://doi.org/10.1016/j.cie.2021.107250\n5. An Z, Zhou D (2019) Deep reinforcement learning\nfor quantum gate control. EPL (Europhysics Let-\nters) 126(6):60002\n6. An Z, Song HJ, He QK, Zhou D (2021) Quantum\noptimal control of multilevel dissipative quantum\nsystems with reinforcement learning. Physical Re-\nview A 103(1):012404\n7. Bukov M (2018) Reinforcement learning for au-\ntonomous preparation of floquet-engineered states:\nInverting the quantum kapitza oscillator. Physical\nReview B 98(22):224305\n8. Bukov\nM,\nDay\nAG,\nSels\nD,\nWeinberg\nP,\nPolkovnikov A, Mehta P (2018) Reinforcement\nlearning in different phases of quantum control.\nPhysical Review X 8(3):031086\n9. C´ardenas-L´opez\nFA,\nLamata\nL,\nRetamal\nJC,\nSolano E (2018) Multiqubit and multilevel quan-\ntum reinforcement learning with quantum tech-\nnologies. PloS one 13(7):e0200455\n10. Chakrabarti R, Rabitz H (2007) Quantum con-\ntrol landscapes. International Reviews in Physical\nChemistry 26(4):671–735\n11. Chen C, Dong D, Li HX, Chu J, Tarn TJ (2013)\nFidelity-based probabilistic q-learning for control\nof quantum systems. IEEE transactions on neural\nnetworks and learning systems 25(5):920–933\n12. Chunlin C, Frank J, Daoyi D (2012) Hybrid control\nof uncertain quantum systems via fuzzy estimation\nand quantum reinforcement learning. In: Proceed-\nings of the 31st Chinese Control Conference, IEEE,\npp 7177-7182\n13. D’Alessandro D, Dahleh M (2001) Optimal control\nof two-level quantum systems. IEEE Transactions\non Automatic Control 46(6):866–876\n14. Fang W, Pang L, Yi W (2020) Survey on the appli-\ncation of deep reinforcement learning in image pro-\ncessing. Journal on Artificial Intelligence 2(1):39–58\n15. F¨osel T, Tighineanu P, Weiss T, Marquardt F\n(2018) Reinforcement learning with neural net-\nworks for quantum feedback. Physical Review X\n8(3):031084\n16. Gu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai\nB, Liu T, Wang X, Wang G, Cai J, et al. (2018)\nRecent advances in convolutional neural networks.\nPattern Recognition 77:354–377\n17. He K, Zhang X, Ren S, Sun J (2016) Deep residual\nlearning for image recognition. In: Proceedings of\nthe IEEE conference on computer vision and pat-\ntern recognition, pp 770–778\n18. Hu B, Zhao H, Yang Y, Zhou B, Raj ANJ\n(2020) Multiple faces tracking using feature fu-\nsion and neural network in video. INTELLI-\nGENT AUTOMATION AND SOFT COMPUT-\nING 26(6):1549–1560\n19. Li Z, Zhang J, Zhang K, Li Z (2018) Visual track-\ning with weighted adaptive local sparse appearance\nmodel via spatio-temporal context learning. IEEE\nTransactions on Image Processing 27(9):4478–4489\n20. Ma H, Chen C (2020) Several developments in\nlearning control of quantum systems. In: 2020 IEEE\nInternational Conference on Systems, Man, and\nCybernetics (SMC), IEEE, pp 4165–4172\n21. Meng F, Cong S (2022) Control design for state\ntransition of open quantum system. In: Journal\nof Physics: Conference Series, IOP Publishing, vol\n2183, p 012005\n22. Michael MH, Silveri M, Brierley R, Albert VV,\nSalmilehto J, Jiang L, Girvin SM (2016) New class\nof quantum error-correcting codes for a bosonic\nmode. Physical Review X 6(3):031006\n23. Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Ve-\nness J, Bellemare MG, Graves A, Riedmiller M,\nFidjeland AK, Ostrovski G, et al. (2015) Human-\nlevel control through deep reinforcement learning.\nnature 518(7540):529–533\n24. Niu MY, Boixo S, Smelyanskiy VN, Neven H (2019)\nUniversal quantum control through deep reinforce-\nment learning. npj Quantum Information 5(1):1–8\n25. Palittapongarnpim P, Wittek P, Sanders BC (2017)\nRobustness of learning-assisted adaptive quantu-\nmenhanced metrology in the presence of noise. In:\n2017 IEEE International Conference on Systems,\nMan, and Cybernetics (SMC), IEEE, pp 294–299\n26. Patsch S, Maniscalco S, Koch CP (2020) Simula-\ntion of open-quantum-system dynamics using the\nquantum zeno effect. Physical Review Research\n2(2):023133\n27. Roslund J, Rabitz H (2009) Gradient algorithm ap-\nplied to laboratory quantum control. Physical Re-\nview A 79(5):053417\n28. Singh SP, Sutton RS (1996) Reinforcement learning\nwith replacing eligibility traces. Machine learning\n22(1):123–158\n29. Sutton RS, Barto AG (2018) Reinforcement learn-\ning: An introduction. MIT press\n30. Sutton RS, McAllester DA, Singh SP, Mansour Y\n(2000) Policy gradient methods for reinforcement\nlearning with function approximation. In: Advances\nin neural information processing systems, pp 1057–\n1063\n31. Torosov BT, Shore BW, Vitanov NV (2021) Co-\nherent control techniques for two-state quantum\nsystems: A comparative study. Physical Review A\n103(3):033110\n10\nWenjie Liu1,2 et al.\n32. Tsubouchi M, Momose T (2008) Rovibrational\nwavepacket manipulation using shaped midinfrared\nfemtosecond pulses toward quantum computation:\nOptimization of pulse shape by a genetic algorithm.\nPhysical Review A 77(5):052326\n33. Vedaie SS, Palittapongarnpim P, Sanders BC\n(2018) Reinforcement learning for quantum metrol-\nogy via quantum control. In: 2018 IEEE Photon-\nics Society Summer Topical Meeting Series (SUM),\nIEEE, pp 163–164\n34. Vrajitoarea A, Huang Z, Groszkowski P, Koch J,\nHouck AA (2020) Quantum control of an oscillator\nusing a stimulated josephson nonlinearity. Nature\nPhysics 16(2):211–217\n35. Watkins CJ, Dayan P (1992) Q-learning. Machine\nlearning 8(3-4):279–292\n36. Xu F, Zhang X, Xin Z, Yang A (2019) Investiga-\ntion on the chinese text sentiment analysis based\non convolutional neural networks in deep learning.\nComput Mater Contin 58(3):697–709\n37. Yu S, Albarr´an-Arriagada F, Retamal JC, Wang\nYT, Liu W, Ke ZJ, Meng Y, Li ZP, Tang JS, Solano\nE, et al. (2019) Reconstruction of a photonic qubit\nstate with reinforcement learning. Advanced Quan-\ntum Technologies 2(7-8):1800074\n38. Zhang XM, Wei Z, Asad R, Yang XC, Wang X\n(2019) When does reinforcement learning stand out\nin quantum control? a comparative study on state\npreparation. npj Quantum Information 5(1):1–7\n39. Zhang Y, Wang Z (2020) Hybrid malware detection\napproach with feedback-directed machine learning.\nInformation Sciences 63(139103):1–139103\n",
  "categories": [
    "cs.ET",
    "cs.AI",
    "quant-ph"
  ],
  "published": "2023-09-30",
  "updated": "2023-09-30"
}