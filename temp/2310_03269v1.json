{
  "id": "http://arxiv.org/abs/2310.03269v1",
  "title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction",
  "authors": [
    "Zeyuan Wang",
    "Qiang Zhang",
    "Keyan Ding",
    "Ming Qin",
    "Xiang Zhuang",
    "Xiaotong Li",
    "Huajun Chen"
  ],
  "abstract": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, but they fall short in comprehending biological sequences\nsuch as proteins. To address this challenge, we propose InstructProtein, an\ninnovative LLM that possesses bidirectional generation capabilities in both\nhuman and protein languages: (i) taking a protein sequence as input to predict\nits textual function description and (ii) using natural language to prompt\nprotein sequence generation. To achieve this, we first pre-train an LLM on both\nprotein and natural language corpora, enabling it to comprehend individual\nlanguages. Then supervised instruction tuning is employed to facilitate the\nalignment of these two distinct languages. Herein, we introduce a knowledge\ngraph-based instruction generation framework to construct a high-quality\ninstruction dataset, addressing annotation imbalance and instruction deficits\nin existing protein-text corpus. In particular, the instructions inherit the\nstructural relations between proteins and function annotations in knowledge\ngraphs, which empowers our model to engage in the causal modeling of protein\nfunctions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein\nand human language understanding.",
  "text": "Preprint. Under review.\nINSTRUCTPROTEIN: ALIGNING HUMAN AND PROTEIN\nLANGUAGE VIA KNOWLEDGE INSTRUCTION\nZeyuan Wang1,2,3\nQiang Zhang1,2∗\nKeyan Ding1,2\nMing Qin,1,2,3\nXiang Zhuang1,2\nXiaotong Li1,2\nHuajun Chen1,2,3∗\n1College of Computer Science and Technology, Zhejiang University\n2ZJU-Hangzhou Global Scientific and Technological Innovation Center\n3AZFT Joint Lab for Knowledge Engine\n{yuanzew,qiang.zhang.cs,dingkeyan,qinandming}@zju.edu.cn\n{zhuangxiang, 3190104904,huajunsir}@zju.edu.cn\nABSTRACT\nLarge Language Models (LLMs) have revolutionized the field of natural language\nprocessing, but they fall short in comprehending biological sequences such as\nproteins. To address this challenge, we propose InstructProtein, an innovative\nLLM that possesses bidirectional generation capabilities in both human and protein\nlanguages: (i) taking a protein sequence as input to predict its textual function\ndescription and (ii) using natural language to prompt protein sequence generation.\nTo achieve this, we first pre-train an LLM on both protein and natural language\ncorpora, enabling it to comprehend individual languages. Then supervised instruc-\ntion tuning is employed to facilitate the alignment of these two distinct languages.\nHerein, we introduce a knowledge graph-based instruction generation framework\nto construct a high-quality instruction dataset, addressing annotation imbalance\nand instruction deficits in existing protein-text corpus. In particular, the instruc-\ntions inherit the structural relations between proteins and function annotations in\nknowledge graphs, which empowers our model to engage in the causal modeling\nof protein functions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein and\nhuman language understanding.\n1\nINTRODUCTION\nThe landscape of Natural Language Processing (NLP) research, and indeed the broader Artificial\nIntelligence (AI) community, has recently been revolutionized by generative Large Language Models\n(LLMs) (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), such as ChatGPT (Ouyang\net al., 2022). The expansion of parameter size and training corpora has empowered these models\nto acquire versatile, general-purpose data representations that seamlessly transcend linguistic tasks\nencompassing comprehension and generation in a multitude of languages. Beyond natural languages\n(a.k.a., human languages), recent investigations have illuminated the potential of these LLMs to serve\nas a versatile interface for processing multimodal data, including but not limited to images, videos\nand speech (Chen et al., 2021; Reed et al., 2022; Gong et al., 2023; Huang et al., 2023).\nHowever, general LLMs fall short of capturing the intricate realm of biological sequences, a domain\nabundant with its own unique linguistic nuances. For example, existing LLMs like ChatGPT cannot\nunderstand biological sequences when they are asked to predict the family of proteins (see Figure 1).\nThe biological sequences, particularly proteins, represent a distinctive facet of what could be referred\nto as “life language”, exerting a significant influence on signal transduction pathways, enzymatic\ncatalysis, and gene regulation (Lee & Yaffe, 2016; Huber, 2001; Südhof, 1995; Durek & Walther,\n2008; Luzarowski et al., 2021; Jiang et al., 2022).\n∗Corresponding author.\n1\narXiv:2310.03269v1  [q-bio.BM]  5 Oct 2023\nPreprint. Under review.\nTo unlock the potential within LLMs for deciphering proteins, researchers have put rich efforts into\ndeveloping protein language models (PLMs) (Alley et al., 2019; Elnaggar et al., 2021; Rives et al.,\n2021; Rao et al., 2021; Lin et al., 2023). These specialized models are tailored to ingest amino acid\nsequences as inputs, predict protein functionalities, or even design de novo proteins. Notwithstanding,\nit is crucial to highlight that while PLMs exhibit competence in comprehending amino acid sequences,\nthey are unable to grasp the complexities of human languages. A recent research trend (Abdine\net al., 2023; Luo et al., 2023) has explored models that accept both protein sequences and textual\ndescriptions as input, aiming to enhance the protein function prediction ability. Nevertheless, these\nendeavors to align the realms of protein and human languages are unidirectional and remain in their\nnascent stages; they fall short of being able to generate protein sequences based on textual instructions.\nIn essence, there exists an unaddressed void in the current landscape of LLMs, wherein the ability to\nswiftly traverse between human and protein languages.\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001b\u0016\u001a\u0015\u0019\nInstruction: I would like a protein that has bifunctional inhibitor/plant lipid\ntransfer protein/seed storage helical domain.\nOutput (ChatGPT): Creating a custom protein with specific functional \ndomains would typically involve genetic engineering and molecular biology \ntechniques, and it's a complex and specialized process that requires a \nlaboratory and expertise in the field. \nOutput (InstructProtein): One of the protein that meets the demand is\nMASVKSSSSSSSSSFISLLLLILLVIVLQSQVIE...\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0014\u0019\u0013\u001b\u001e\u0016\u001c\u0012\u0019\u0013\u001a\u0019\u0015\nInstruction: MFTGGGTIALIERLATSWLTAIRLILSWHPIHAPNRNQ...\nWhat family is the protein in?\nOutput (ChatGPT): The provided protein sequence is a string of amino \nacids, and based on the sequence alone, it is not possible to definitively \ndetermine the family or function of the protein.\nOutput (InstructProtein): The protein is in marek disease virus, lorf3 family.\nFigure 1: An example of bidirectional generation\nby LLMs between human and protein languages.\nChatGPT fails to provide an accurate response\nwhile the proposed InstructProtein offers a rea-\nsonable solution.\nTo enable an LLM to adeptly comprehend both\nhuman and protein languages, we contend that\nthe limitations imposed by existing models pri-\nmarily stem from their training corpora. No-\ntably, many existing models are trained on either\nhuman languages or protein sequences, render-\ning them proficient in only one of these linguis-\ntic realms. This unilateral training approach\nis insufficient to imbue an LLM with a com-\nprehensive vocabulary encompassing both lan-\nguages. Moreover, it is important to recognize\nthat the existing protein-text corpus used in pre-\nvious studies (Luo et al., 2023; Abdine et al.,\n2023; Xu et al., 2023; Taylor et al., 2022) has its\nlimitations. (1) The imbalance of annotations:\nResearchers tend to focus on well-studied pro-\nteins, leading to a significant disparity in the\navailability of annotations (Kustatscher et al.,\n2022). Training LLMs directly on such a corpus\nintroduces model bias, which ultimately results\nin suboptimal performance. (2) The absence\nof instructional signals: Protein-related textual\ncontent is primarily comprised of descriptive\nnarratives, often devoid of instructional signals specifically designed for training LLMs. This inherent\ndisparity obstructs a holistic understanding of a wide range of tasks, ultimately resulting in subpar\nzero-shot performance (Wei et al., 2022a). In short, the fundamental hurdle of current LLMs\ninvolves curating an elaborate training corpus that seamlessly bridges the gap between human\nand protein languages.\nIn this work, we introduce InstructProtein, a pioneering study that aligns human and protein\nlanguages through knowledge instruction, leading to the first LLM with bidirectional generation\ncapabilities between these two languages. Specifically, to equip LLMs with the ability to understand\nprotein language, InstructProtein adopts a two-step training approach. It initiates with pre-training on\nprotein and natural language corpora, followed by finetuning with the established protein knowledge\ninstruction dataset. To construct such an instruction dataset, we first transform raw protein-text\ncorpora into a structured knowledge graph (KG). Inspired by the idea of chain-of-thoughts, we enrich\nKG with knowledge causal modeling, which involves establishing causal relationships between\ntriples, indicating causality within annotations. We then propose a debiased sampling strategy to\nselect KG triples, effectively addressing the issue of annotation imbalance. Finally, we mimic\nKG completion tasks, leverage general LLMs to convert KG triples into instructions, and conduct\nsupervised instruction tuning. Extensive experiments have demonstrated that the introduced protein\nknowledge instructions significantly improve the performance of LLMs on protein understanding and\ndesign tasks. Our contributions can be summarized as follows:\n1. We propose InstructProtein, an innovative LLM that enables bidirectional generation between\nprotein and human languages, effectively filling the gap between the two languages.\n2\nPreprint. Under review.\n37.1%\n34.6%\n8.01%\n7.77%\n6.42%\n6.15%\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\n\u0016\u0015\u0014\u0013\n\u0012\u001d\u0011\u0010\u000f\u0018\u000e\r\u0014\u0014\u0016\f\n\u0016\u000b\u0015\u0013\n\u001f\u0010\u001a\u001a\u000e\n\t\t\u0010\u000f\u000e\u0017\u0010\u0017\b\u000f\u0019\t\u0010\n\u0007\u0006\u0013\n\u0005\u0004\u0003\u001a\u0010\u0004\u0018\n\u0002\u0015\u0013\n\u001f\u0010\u001a\u001a\u000e\u0017\u0010\u0017\b\u000f\u0019\t\u0010\n\u0002\u0015\u0013\n\u0001\u0010\u0003\u000f\u0010\u001d\u0010\n\u0007\u0013\nFigure 2: We visualized the top-5 subcel-\nlular location categories and their respec-\ntive proportions, in comparison to the\nleast frequently used annotations, which\naccounted for only 0.000224%.\nModels\nPrediction\nCytoplasm\nNucleus\nCell membrane\nOthers\nOPT\n2\n115\n1691\n0\nLLaMA\n0\n1806\n2\n0\nGalactica\n1807\n1\n0\n0\nAlpaca\n1808\n0\n0\n0\nTable 1: The results of querying existing LLMs for\nfactual knowledge. We prompt LLMs to predict sub-\ncellular location, but their results are biased to a certain\ncategory, which suggests that these LLMs have been\ncontaminated by annotation imbalance.\n2. We introduce a protein instruction generation framework with knowledge graphs, resulting\nin the first high-quality protein instruction dataset for tuning LLMs.\n3. The InstructProtein outperforms state-of-the-art LLMs by a substantial margin, serving as a\npioneering step toward text-guided protein function prediction and sequence design.\n2\nA CLOSER LOOK AT ANNOTATION IMBALANCE\nMuch of life science research is dedicated to unraveling the biological functions of proteins. While\ncertain proteins, such as the well-studied tumor suppressor p53 (Dolgin, 2017), have undergone\nextensive investigation, there still exist tens of thousands of proteins remain categorized as under-\nstudied. This phenomenon implies an imbalance in protein function annotation. To clearly illustrate\nthis problem, we take the subcellular location as an example, and show its annotation distribution in\nFigure 2. The results reveal a notable concentration of research attention on proteins residing in the\ncytoplasm, while other subcellular locations lack comprehensive labeling and study.\nThe annotation imbalance has a detrimental effect on the performance of existing LLMs. To demon-\nstrate this, we collect the same number of proteins in each subcellular location category from\nUniProtKB (Consortium, 2019), resulting in 1,808 proteins in total, and prompt LLMs to predict the\nsubcellular location. The outcomes of LLMs are presented in Table 1, from which one can observe\nthat these LLMs are biased in a certain category, due to the annotation imbalance in the training\ncorpus of LLMs.\n3\nINSTRUCTPROTEIN\nThis section presents the methodological details of InstructProtein. We first pre-train it in a self-\nsupervised manner on natural language corpus and protein sequence datasets respectively, and then\nconduct supervised tuning using the created knowledge instruction dataset.\n3.1\nMULTILINGUAL PRE-TRAINING\nInstructProtein is designed to comprehend both the protein and human languages. An intuitive\napproach involves incrementally pre-training an LLM using the protein corpus P and text sequences\nT . Given an unsupervised corpus of tokens X = {x1, x2, . . . , xn} ∈P ∪T , the training objective\nof a generative LLM (e.g., OPT (Zhang et al., 2022a)) is defined as\nL(X) =\nX\ni\nlog P(xi|xi−k, . . . , xi−1; θ),\n(1)\nwhere the prediction of each token depends on previous tokens x<i, k is the context window size,\nand the conditional probability P is modeled using a neural network parameterized by θ.\n3.2\nINSTRUCTION TUNING\nAfter pre-training, the model acquires an extensive comprehension of both natural language and\nprotein sequences; however, it still falls short in achieving alignment between these two different\nlanguages. We fill this gap through supervised instruction tuning.\n3\nPreprint. Under review.\nRaw Document\nKnowledge Graph\nwith KCM\nTriples\nDebiased Sampler\nInstruction\nInstruction\nSeed Task\nRaw Document\nInstruction\n(Task)\n(Input)\n(Output)\n(Task)\n(Input)\n(Output)\n(Task)\n(Input)\n(Output)\n(a)\n(b)\n(c)\nProp. Dist.\nSeq. Dist.\nKG Completion\n(p,  r,  t)\nFigure 3: Overview of Instruction generation methods. The red text represents the fields that rely\non internal knowledge of LLMs. (a) Given a set of seed tasks, prompting an LLM to produce new\ninstruction data.(b) Utilizing LLMs to generate the instruction data corresponding to the contents in\nraw documents. (c) The proposed knowledge graph (KG)-based instruction generation framework.\nWe first construct a KG with knowledge causal modeling (KCM), and introduce a debiased sampler to\npick the informative triples, which are then translated into instruction data through the use of LLMs\nin conjunction with KG completion tasks.\n3.2.1\nKNOWLEDGE INSTRUCTION GENERATION\nWe propose an instruction generation method based on knowledge graphs (KGs) and LLMs, aiming\nto construct a factual, logical, diverse, and well-balanced protein instruction dataset. Figure 3\nillustrates the pipeline of three instruction generation frameworks. Conventional approaches directly\nutilize LLMs to generate instruction data from seed tasks or raw documents, which may introduce\nhallucination and bias. In the proposed method, KGs are incorporated as intermediaries to address\nthese limitations. In specific, a KG encompassed with knowledge causal modeling is constructed to\nprovide factual protein knowledge, based on which a debiased sampling strategy is proposed to pick\nKG triples. An LLM (e.g., ChatGPT) then translates the samples into instruction data and enriches\nthem with a wide range of expressions.\n?\nh\n?\nr\nInstruction: I wonder the function.\nInput: MHWGTLC...\nOutput: Hormone activity.\nMRCPGVSLWG\nfunction\nhormone activity\nleptin\nMRCPGVSLWG\nfamily\n\u001f\u001e\u001d\u001c\u001b\u001a\n\u0019\u0018\u0017\nInstruction: MHWGTLC... \nDoes the protein enable hormone activity?  \nInput: None\nOutput: Since it is in the leptin family,\n          the answer is yes. \n?\nr\nt\nr\nt\nh\n\u001f\u0016\u001d\u001b\u0015\u0014\u001e\u001a\u0013\u001d\u0012\u0011\u001d\u0010\u000f\n\u000e\u001a\u0016\u0013\u0015\u0014\u001e\u001a\u0013\u001d\u0012\u0011\u001d\u0010\u000f\nInstruction: I would like a protein.\nInput: It enables hormone activity. \nOutput: MHWGTLC...\n\u001f\u001e\u001d\u001c\u001b\u001a\n\u0018\u001b\u0016\r\r\u001d\f\u0012\u0016\u0011\u001d\u0010\u000f\n\u0019\u000b\u0015\u0018\u0010\n\u001c\u001b\u001a\u0011\u001d\u0010\u000f\nFigure 4: An example of converting a KG\ntriple to instructions.\nGiven a triple with\nKCM, we use an LLM cooperated with KG\ncompletion tasks to generate factual, logical,\nand diverse instructions.\nKG Construction. We use UniProtKB as our data\nsource to construct the protein knowledge graph de-\nnoted as G = {P, R, T }. Here, P, R, and T are sets\nof protein sequences, relations, and textual annota-\ntions. Note that the textual description of proteins\nin UniProtKB is structured, making it easy to trans-\nform them into a knowledge graph. In our pursuit\nof enhancing the quality of the instruction dataset,\nwe augment KG to provide informative relationships.\nBorrowing ideas from chain-of-thoughts (Wei et al.,\n2022b), we recognize that a logical chain also exists\nwithin protein annotations. For example, the biolog-\nical processes in which a protein can participate are\nintricately linked to its molecular function and sub-\ncellular location, with the molecular function itself\nbeing influenced by the protein’s domain. To rep-\nresent this causal chain of protein knowledge, we\nintroduce a novel concept called Knowledge Causal\nModeling (KCM). Specifically, a knowledge causal\nmodel comprises multiple interconnected triples or-\nganized in a directed acyclic graph, where the edge\ndirection signifies causal relationships. This graph\norganizes the triples, moving from the micro-level,\nencompassing characteristics of protein sequences\n(e.g., domains), to the macro-level, encompassing bi-\nological functions. In Figure 4, we show an example\nof KCM retrieved from InterPro (Paysan-Lafosse et al., 2023) based on a given triple.\n4\nPreprint. Under review.\nKG Triple Sampling. To generate instruction data, we need to sample triples from the constructed\nKG. Considering the annotation imbalance problem in the KG, we propose a debiased sampling\nstrategy as an alternative to uniform sampling. In specific, we first group proteins together based on\ntheir sequence and property similarities, and then uniformly pick triples within each cluster.\nTo access sequence similarity, we employ MMseqs2 (Steinegger & Söding, 2017) to calculate the\nediting distance dseq(·, ·) (see Appendix A.2.2). For property similarity, since the protein properties\nare extensive and many of them remain unexplored, we only consider the known annotations in KG\nwhen computing the property similarity. Specifically, given an annotation t and a relation r, we\ndenote Ct = {p : p ∈P ∧(p, r, t) ∈G} and C/t = {p : p ∈P ∧(p, r, t) /∈G} are the protein\nset based on the presence or absence of t. The basic idea is to maximize agreement within Ct and\nminimize agreement between Ct and C/t, via optimizing protein KG embeddings. In practice, we\nminimize a margin-based ranking criterion over the knowledge graph:\nL = −\nX\npt∈Ct,p/t∈C/t\n[log σ(γ −dprop(pt, t + r)) + log σ(dprop(p/t, t + r) −γ)],\n(2)\nwhere p, r, t ∈Rk (k is a hyperparameter) are embeddings of proteins, relations, and annotations,\nσ is the sigmoid function, and γ is the margin. dprop(·, ·) is a dissimilarity measure of properties,\nwhich is implemented as the ℓ1-norm.\nWe define the threshold of sequence and property similarities as δprop and δseq, respectively. We\ndenote two proteins to be similar p1 ≃p2 as dseq(p1, p2) < δseq and dprop(p1, p2) < δprop.\nC = {C1, . . . , Cm} represents the aggregation of proteins with m clusters, and the cluster Ci can be\nformulated as:\nCi = {p : ∃p′ ∈Ci, p ≃p′ ∧∀ρ ∈Cj̸=i, p ̸≃ρ}.\n(3)\nThen, the probability of sampling a triple (p, r, t) is:\nP((p, r, t)) = 1\nm ×\n1\n||Ci|| ×\n1\n||p||,\n(4)\nwhere p ∈Ci, ||Ci|| denotes the size of Ci, and ||p|| are the number of annotations on p.\nKG Triple to Instruction. By employing the debiased sampling strategy, one can sample a large\nnumber of well-balanced KG triples. We then focus on translating these triples into instruction\ndata. While the generation of creative tasks requires domain knowledge, the KG completion tasks\noffer a comprehensive template for proposing domain-specific tasks based on triples. Therefore, we\nsimulate KG completion, and employ general LLMs (e.g., ChatGPT) to transform KG triples with\nretrieved KCM into instruction data, which contains three fields: an instruction describing the task,\nan input argument that instantiates the instruction, and an output result reflecting a correct execution\nof the instruction given the input arguments. Figure 4 shows an example of converting the triple to\ninstructions. The detailed implementation is depicted in Appendix 8.\n3.2.2\nTUNING LLMS WITH INSTRUCTIONS.\nInstruction tuning involves further training LLMs in a supervised manner on an instruction dataset\ncomprising of (instruction, input, output), bridging the gap between the LLMs’ next-word prediction\nobjective and users’ goal of ensuring adherence to human instructions. With the proposed knowledge\ninstruction dataset I, we finetune the pre-trained LLM to align the protein and human languages.\nGiven an instruction Z ∈I and its tokens X = {x1, x2, . . . , xn} ∈Z, the training objective is the\nsame as that defined in Eq.(1).\n4\nEXPERIMENTS\nIn this section, we evaluate the performance of LLMs in terms of protein sequence understanding and\ndesign. To effectively evaluate these two capabilities, we have modified the existing downstream task\ndatasets to facilitate the evaluation of LLMs.\n4.1\nEXPERIMENTAL SETUP\nThe pre-training corpus contains protein sequences from UniRef100 (Suzek et al., 2015) and sentences\nfrom PubMed abstracts. Following the methodology described in Section 3.2.1, we generated an\n5\nPreprint. Under review.\nTable 2: Zero-shot performance on protein sequence understanding.\nModels\nParams.\nLocation\nGO-BP\nGO-MF\nGO-CC\nMIB\nBin\nSub\nACC\nAUPR\nACC\nAUPR\nACC\nAUPR\nOPT\n1.3B\n57.52\n29.06\n51.83\n64.76\n56.10\n74.50\n51.94\n71.90\n49.40\nLLaMA\n7.0B\n57.52\n29.14\n56.96\n61.85\n54.58\n58.06\n51.57\n53.53\n50.00\nAlpaca\n7.0B\n57.52\n18.32\n61.69\n65.13\n59.37\n73.02\n57.98\n61.71\n50.38\nGalactica\n1.3B\n57.52\n18.32\n55.11\n57.08\n61.30\n61.93\n51.17\n54.54\n51.58\nBioMedGPT\n10B\n59.51\n56.39\n50.31\n50.82\n51.02\n50.81\n49.41\n49.39\n54.42\nInstructProtein\n1.3B\n85.19\n70.79\n71.49\n83.16\n85.83\n93.68\n79.79\n86.37\n62.68\ninstruction dataset comprising 2.8 million data. Specifically, the protein knowledge graph was\nconstructed utilizing the annotations provided by UniProt/Swiss-Prot(Consortium, 2019), which\ncontains the superfamily, family, domain, conserved site, active site, binding site, location, function,\nand involved biological process of proteins. Knowledge causal modeling is sourced from the\nInterPro (Paysan-Lafosse et al., 2023) and Gene Ontology (Aleksander et al., 2023) database. Note\nthat proteins appearing in downstream tasks have been excluded from the training data. We leverage\nChatGPT (Ouyang et al., 2022) to convert triples into instruction data. Detailed experimental setups\ncan be found in Appendix A.3.\n4.2\nPROTEIN SEQUENCE UNDERSTANDING\nDatasets and Metrics. We evaluate LLMs on three widely-used protein function classification tasks:\n(1) Protein Localization Prediction, which involves the prediction of the subcellular location of a\ngiven protein. We address two subproblems from DeepLoc (Almagro Armenteros et al., 2017), the\nsubcellular localization prediction (Abbr., Sub) with 10 location categories and the binary localization\nprediction (Abbr., Bin) with 2 location categories; (2) Protein Function Annotation, aiming to predict\nthe correct annotations of proteins. We choose Gene Ontology (GO) dataset Gligorijevi´c et al. (2021),\nwhich has three branches: molecular function (MF), biological process (BP), and cellular component\n(CC). We use the dataset splits under 95% sequence identity cutoff. (3) Metal Ion Binding (MIB)\nPrediction, a binary classification task where the model needs to determine whether there are metal\nion-binding sites in the protein. We use the dataset from Hu et al. (2022).\nSimilar to reading comprehension problems in NLP, we transform all items in the above datasets into\na Question&Answer (QA) format where each item consists of a protein sequence, a question about\nthat protein, and a list of possible answers. LLMs are required to predict which answers are true.\nFollowing Brown et al. (2020), we use a classification approach where, for example, only two outputs\n(\"yes\" and \"no\") are considered and the higher probability one is taken as the model’s prediction. All\nevaluations are carried out in a zero-shot setting, without few-shot demonstrations.\nBaselines. We adopt five state-of-the-art open-sourced LLMs as the baselines. OPT (Zhang et al.,\n2022a) and LLaMA (Touvron et al., 2023) are trained on massive text corpus, and Alpaca (Taori\net al., 2023) is a language model based on instruction-tuned LLaMA. Galactica (Taylor et al., 2022)\nand BioMedGPT (Luo et al., 2023) are domain-specific LLMs, which are trained on a large curated\ncorpus of humanity’s scientific knowledge, such as research papers about proteins and genes. For\na fair comparison, we designed a template for each model through prompt engineering so that the\nmodel could follow our instructions and output the answers.\nResults. We present the evaluation results in Table 2. Compared with all baselines, InstructProtein\nachieves new state-of-the-art performance on all tasks. There are two key observations. First,\nInstructProtein clearly outperforms the LLMs (i.e., OPT, LLaMA, Alpaca) which are stemmed from\nnatural language training corpora. These results demonstrate that training with the corpus where\nproteins and natural language coexist is beneficial to LLMs, enhancing their proficiency in protein\nlanguage understanding. Second, InstructProtein performs consistently better than Galactica and\nBioMedGPT, despite all of them leveraging UniProtKB as a corpus for natural language alignment\nwith proteins. The results verify that our high-quality instruction data can boost zero-shot performance.\nIt is worth noting that in the protein subcellular localization (bin) task, there exists a severe bias in\nLLMs (OPT, LLaMA, Alpaca, and Galactica), leading to the classification of all proteins into a single\ngroup and resulting in the same accuracy of 57.52%.\n6\nPreprint. Under review.\n\u001f\u001e\u001f\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001b\u001a\n\u001f\u001e\u001d\u001d\u001c\u001d\u0016\u0015\u0014\u0013\n\u0012\n\u001f\u0011\u0010\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001b\u001a\n\u001f\u001e\u001d\u001d\u001c\u001d\u0016\u0013\u0014\u000f\n\u000f\u001f\u0010\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001b\u001a\n\u001f\u001e\u001d\u001d\u001c\u001d\u0016\u0013\u0014\u000e\n(a)\n(b)\n(c)\n(125m)\n(350m)\nInstructProtein (1.3b)\n\r\u001b\f\u0017\u001b\u000b\n\u001b\u001d\t\b\u0007\u0007\u0006\n\u001f\u001e\n\u001d\u001e\u001e\n\u001c\u001e\n\u001b\u001e\n\u0005\b\u0007\u0007\u0006\n\u001e\n\u001d\u001e\u001e\n\u0004\u0003\u0003\u001d\u001d\u001d\u001d\u001d\u001d\u001d\u0002\u0001\u001b\u0003\u0019\nα\n\u0004\u0003\u0003\u001d\u001d\u001d\u001d\u001d\u0002\u001a\u0001\u001b\u001b\u001d\nβ\nAll   -helix (  1.3b,  350m,  125m)\nα\nAll   -sheet (  1.3b,  350m,  125m)\nβ\n\u0002\u0001\u001b\u0003\u0019\nα\n\u0002\u001a\u0001\u001b\u001b\u001d\nβ\nFigure 5: Visualization of structure instruction-based protein sequence de novo design. We prompt\nour models with different scales (125m, 350m and 1.3b) to generate three kinds of proteins (all\nα-helix, all β-sheet, and a combination of α-helix and β-sheet), respectively. (a) We visualize the\npLDDT of generated sequences predicted by AlphaFold2 to assess the protein foldability. (b) The\nembeddings of sequences prompted with all α-helix and all β-sheet instructions, which are extracted\nfrom ESM2 and visualized by the MDS algorithm. (c) The structure of generated proteins with the\nhighest confidence in each class.\n4.3\nPROTEIN SEQUENCE DESIGN\nGenerating proteins following human instructions is a highly exciting area of research. With the\nincorporation of protein sequences as part of the language capabilities in LLMs, InstructProtein is\ncapable of generating protein sequences. However, the lack of standardized computational metrics to\nassess the quality of generated proteins poses challenges for advancing protein generation models. In\nthis study, we present our endeavor to build a computational evaluation framework.\n4.3.1\nZERO-SHOT INSTRUCTION-PROTEIN PAIRING\nDatasets and Metrics. We design an instruction-protein pairing task to assess the consistency\nbetween the instruction and the generated protein. Specifically, we employ the dataset proposed\nby Hou et al. (2018) to provide fold-related instructions and proteins. Given a protein p and the\ncorresponding instruction Z0, we randomly sample other n instructions {Z1, Z2, ..., Zn} (n = 9 in\nthis experiment), and the likelihood L of the protein given the various instructions is computed. The\nminimization of L(p|Zi) at i = 0 signifies a correct pairing, and vice versa.\nTable 3: Accuracy of instruction-protein pairing.\nModels\nFold Rank\nFold\nSuperFamily\nFamily\nOPT\n7.79\n6.45\n6.68\nLLaMA\n9.33\n5.90\n10.30\nAlpaca\n5.43\n3.90\n4.71\nGalactica\n11.00\n10.12\n10.37\nBioMedGPT\n-\n-\n-\nInstructProtein\n55.57\n65.07\n79.24\nResults. Table 3 reports the accuracy\nof the instruction-protein pairing task.\nOne can observe that InstructProtein sur-\npasses the baselines by a large margin.\nBioMedGPT focuses solely on convert-\ning proteins to texts and lacks protein\ndesign capabilities. Galactica exhibits\nlimited zero-shot performance in align-\ning instructions with proteins, since it\nis trained with narrative protein corpus.\nThese results confirm the superiority of\nour model in instruction-following for\nprotein generation.\n4.3.2\nPROTEIN SEQUENCE DE NOVO DESIGN\nDesigning proteins with specified structures. We investigate whether InstructProtein could generate\nnew protein sequences that are individually valid and consistent with instructions. SCOPe (Chandonia\net al., 2022) classifies protein structures according to the content and organization of secondary\nstructures, including all α-helix, all β-sheet, and the combination of α-helix and β-sheet. We sample\n100 sequences from each class and assess the foldability of individual sequences by predicting\ntheir corresponding structures using ColabFold (Mirdita et al., 2022; Jumper et al., 2021) and\n7\nPreprint. Under review.\nGoundTruth:\n• Affinity: -6.6 (kcal/mol)\n• PDB id: 2N91-A\nDesign 2:\n• Affinity: -6.3 (kcal/mol)\n• pLDDT: 93.8\nDesign 3:\n• Affinity: -8.9 (kcal/mol)\n• pLDDT: 87.4\nDesign 1:\n• Affinity: -8.7 (kcal/mol)\n• pLDDT: 96.9\n(a)\n(b)\n(c)\n(d)\nFigure 6: Visualization of functional instruction-based protein sequence de novo design. We prompt\nour model with the instruction “I would like a protein that enables heme binding”. (a) is the ground-\ntruth protein that binds with heme. (b), (c) and (d) are generated proteins with decent binding affinity.\nTable 4: Ablation of the proposed sampling strategy and KCM used in knowledge instructions.\nSampling Strategy\nKCM\nLocation (Sub)\nGO (MF)\nFold Rank (Fold)\nUnclustering\nNo\n58.12\n85.58\n51.98\nSeq. Clustering\nNo\n62.77\n83.70\n54.41\nSeq.&Prop. Clustering\nNo\n69.95\n85.92\n53.81\nSeq.&Prop. Clustering\nYes\n70.79\n85.83\n55.57\ncomputing the average predicted local distance difference test (pLDDT) across the whole structure\n(Figure 5 (a)). pLDDT increases with model scale, suggesting that scaling up the parameter size\nleads to generating more foldable sequences. We leverage ESM2 (Lin et al., 2023) as a feature\nextractor to obtain the generated all α-helix and all β-sheet protein representations, which are then\nvisualized using multi-dimensional scaling (MDS) algorithm (Kruskal, 1964) (Figure 5 (b)). We\nobserve that the representations are divided into two groups according to instructions, indicating\nthe instruction-following ability of the proposed model. We visualize the predicted structure of\nthe proteins with the highest confidence in each class (Figure 5 (c)). These results demonstrate\nthat InstructProtein establishes a close correlation between natural language and protein language,\nverifying the effectiveness of protein de novo design based on structure-related instruction.\nDesigning proteins binding with specified ligands. To verify the ability to follow function-related\ninstructions, we employ InstructProtein to design heme binders, which are proteins capable of binding\nto a specific compound, and visualize 3D structures of three generated proteins. In Figure 6, we\npresent the docking result (docked by DiffDock (Corso et al., 2023)), the binding affinity (predicted\nby Smina (Koes et al., 2013; Trott & Olson, 2010), the lower the better), and the pLDDT score\n(predicted by ColabFold; the higher the absolute value, the better). We can observe the resulting\nproteins exhibit notable binding affinity, confirming the efficacy of InstructProtein in heme binder\ndesign. We provide more case studies in Appendix A.5.\n4.4\nABLATION STUDY\nIn this subsection, we conduct ablation studies on the sampling strategy and knowledge causal\nmodeling (KCM) used in our knowledge instruction generation method. From the results in Table 4,\nwe observe that clustering similar proteins in annotation imbalance-related tasks (Location and Fold\nRank) can effectively improve model performance. However, for the GO task without the annotation\nimbalance problem, the clustering method based on sequence similarity alone makes the model\nperformance decrease. This phenomenon arises due to the occurrence of critical mutations at key\nsites, leading to significant functional alterations in proteins. Consequently, their resemblance to\nextensively studied sequences diminishes the likelihood of selection. This leaves the model lacking\nthe ability to distinguish the functionally important sites. Such problems can be avoided by segment\nclusters based on protein properties. We also observe that the causal relationship between annotations\nintroduced by KCM improves the performance.\n8\nPreprint. Under review.\n5\nRELATED WORKS\nLarge Language Models (LLMs) have achieved breakthrough performance in NLP (Brown et al.,\n2020; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022; Zhang et al., 2022a; Chowdhery\net al., 2022; Touvron et al., 2023). These models, trained via self-supervision on extensive, general\ncorpora, exhibit proficiency in a multitude of tasks (Hendrycks et al., 2020; Jin et al., 2021; Pal et al.,\n2022). However, these LLMs are primarily tailored for human language comprehension, limiting\ntheir utility in decoding the intricate language of proteins. To bridge this gap, Protein Language\nModels (PLMs) have garnered significant attention (Alley et al., 2019; Elnaggar et al., 2021; Rives\net al., 2021; Rao et al., 2021; Lin et al., 2023; Rao et al., 2020; Meier et al., 2021; Ferruz et al.,\n2022; Notin et al., 2022). Nonetheless, PLMs confront limitations stemming from their training\ncorpora, lacking factual knowledge of human language.\nTo align protein with human language,\nmultimodal approaches (Abdine et al., 2023; Luo et al., 2023) integrate protein encoders into LLMs\nwithin an encoder-decoder framework. Notwithstanding, these architectures predominantly exhibit a\nunidirectional cross-modal capability, focusing solely on converting protein language to texts. Taylor\net al. (2022) treats protein language and human language as a unified modality. However, the use of\nexisting protein-text corpus hinders the alignment of protein and human languages. The proposed\nInstructProtein represents a pioneering effort with the ability to generate in both human and protein\nlanguages, marking a significant advancement in the field of protein understanding and design.\nInstruction Tuning is a supervised approach to align language models with user intention (Weller\net al., 2020; Mishra et al., 2022; Wang et al., 2022; Wei et al., 2021; Sanh et al., 2021; Ouyang\net al., 2022). It is worth noting that acquiring large-scale instruction data can be a resource-intensive\nand time-consuming endeavor, thereby motivating the exploration of automatic data generation\ntechniques. A prevalent strategy (Anaby-Tavor et al., 2020; Andreas, 2020; Kaushik et al., 2019)\ninvolves augmenting existing datasets. Alternatively, several fully automatic datasets have been\nproposed to eliminate the need for labeled data. Schick & Schütze (2021) and Ye et al. (2022) advocate\nfor leveraging pre-trained language models to generate comprehensive labeled datasets from scratch,\ntailored to predefined tasks. Honovich et al. (2023) and Wang et al. (2023) used pre-trained LLMs to\nautomatically construct instructions by a handful of examples. However, these methodologies may\nintroduce hallucination and bias into the instruction data. To overcome these limitations, our work\nincorporates knowledge graphs as intermediaries, resulting in a protein instruction dataset that is\nfactual, logical, diverse, and well-balanced.\nKnowledge Graph (KG) is often employed to enhance the capabilities of LLMs. A related subfield\nto our work involves integrating KGs into the input of LLMs. Researchers such as Sun et al. (2021);\nLiu et al. (2020); Sun et al. (2020); Zhang et al. (2022b) have pursued this avenue by concatenating a\nKG triplet with corresponding sentences, leveraging language modeling to amalgamate knowledge\nwith textual representations. Our approach also involves utilizing KGs as input, however, a significant\ndifference lies in how LLMs interact with KGs. We focus on generating instruction data using KGs,\nallowing LLMs to capture insights from instructions rather than relying solely on KG triplets.\n6\nDISCUSSION AND CONCLUSION\nInstructProtein explores the feasibility of bidirectional generation between human and protein lan-\nguages within a single large language model. Our approach involved the transformation of a raw\nprotein-text corpus into a structured knowledge graph, from which KG triples were sampled and\nconverted into instructions. This KG-based instruction generation method resulted in a high-quality\ninstruction dataset, facilitating the LLM to align protein language with human language.\nNevertheless, it’s important to acknowledge that there are some limitations inherent in our model.\nOne such limitation, shared with large language models, is that InstructProtein encounters challenges\nwith handling numerical values. This limitation hinders our ability to quantitatively characterize\nproteins, including tasks like thermostability prediction. Besides, the design of a satisfactory protein\nnecessitates meeting a multitude of requirements, such as solubility, stability, and 3D structure.\nHowever, our current model is primarily tailored to support protein design based on qualitative\ndescriptions, such as designing proteins within specific protein families. This limitation arises from\nour instructions exclusively offering qualitative protein descriptions encompassing aspects like family\nand function, while lacking quantitative annotations concerning elements such as 3D structures,\nwhich hold significance in protein design.\n9\nPreprint. Under review.\nIn the future, we will incorporate a broader spectrum of instructions, including quantitative descrip-\ntions. This extension will empower our model to provide quantitative outputs. These developments\nwill open up new avenues for further advancing the integration of protein and human languages, as\nwell as expanding its practical utility in diverse applications.\nREFERENCES\nHadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, and Michalis Vazirgiannis. Prot2Text:\nMultimodal Protein’s Function Generation with GNNs and Transformers.\narXiv preprint\narXiv:2307.14367, 2023.\nSuzi A Aleksander, James Balhoff, Seth Carbon, J Michael Cherry, Harold J Drabkin, Dustin Ebert,\nMarc Feuermann, Pascale Gaudet, Nomi L Harris, et al. The Gene Ontology knowledgebase in\n2023. Genetics, 224(1):iyad031, 2023.\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church.\nUnified rational protein engineering with sequence-based deep representation learning. Nature\nmethods, 16(12):1315–1322, 2019.\nJosé Juan Almagro Armenteros, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik Nielsen,\nand Ole Winther. Deeploc: prediction of protein subcellular localization using deep learning.\nBioinformatics, 33(21):3387–3395, 2017.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov,\nNaama Tepper, and Naama Zwerdling. Do Not Have Enough Data? Deep Learning to the Rescue!\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7383–7390, 2020.\nJacob Andreas. Good-Enough Compositional Data Augmentation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pp. 7556–7566, 2020.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An Open-\nSource Autoregressive Language Model. In Proceedings of BigScience Episode #5 – Workshop on\nChallenges & Perspectives in Creating Large Language Models, pp. 95–136, virtual+Dublin, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, and others Askell. Language Models are Few-\nShot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances\nin Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc.,\n2020.\nJohn-Marc Chandonia, Lindsey Guan, Shiangyi Lin, Changhua Yu, Naomi K Fox, and Steven E\nBrenner. SCOPe: improvements to the structural classification of proteins–extended database to\nfacilitate variant interpretation and machine learning. Nucleic acids research, 50(D1):D553–D559,\n2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Pinto, Jared Kaplan, Harrison Edwards, Yuri\nBurda, Nicholas Joseph, Greg Brockman, et al. Evaluating Large Language Models Trained on\nCode, 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022.\nUniProt Consortium. UniProt: a worldwide hub of protein knowledge. Nucleic acids research, 47\n(D1):D506–D515, 2019.\nGabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock:\nDiffusion Steps, Twists, and Turns for Molecular Docking. International Conference on Learning\nRepresentations (ICLR), 2023.\n10\nPreprint. Under review.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In North American Chapter of the\nAssociation for Computational Linguistics, pp. 4171–4186, Minneapolis, Minnesota, June 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nElie Dolgin. The most popular genes in the human genome. Nature, 551(7681):427–432, 2017.\nPawel Durek and Dirk Walther. The integrated analysis of metabolic and protein interaction networks\nreveals novel molecular organizing principles. BMC systems biology, 2(1):1–20, 2008.\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones,\nTom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. ProtTrans: Toward\nUnderstanding the Language of Life Through Self-Supervised Learning. IEEE transactions on\npattern analysis and machine intelligence, 44(10):7112–7127, 2021.\nNoelia Ferruz, Steffen Schmidt, and Birte Höcker. ProtGPT2 is a deep unsupervised language model\nfor protein design. Nature communications, 13(1):4348, 2022.\nVladimir Gligorijevi´c, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Beren-\nberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-\nbased protein function prediction using graph convolutional networks. Nature communications, 12\n(1):3168, 2021.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. MultiModal-GPT: A Vision and Language Model for\nDialogue with Humans, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring Massive Multitask Language Understanding. In International Conference\non Learning Representations, 2020.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models, 2022.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 14409–14428, Toronto,\nCanada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.\n806.\nJie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping\nprotein sequences to folds. Bioinformatics, 34(8):1295–1303, 2018.\nMingyang Hu, Fajie Yuan, Kevin K Yang, Fusong Ju, Jin Su, Hui Wang, Fei Yang, and Qiuyang\nDing. Exploring evolution-aware & -free protein language models as protein function predictors.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?\nid=U8k0QaBgXS.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Barun Patra, et al. Language Is Not All You Need: Aligning\nPerception with Language Models, 2023.\nArmin Huber. Scaffolding proteins organize multimolecular protein complexes for sensory signal\ntransduction. European Journal of Neuroscience, 14(5):769–776, 2001.\nYining Jiang, Batiste Thienpont, Vinay Sapuru, Richard K Hite, Jeremy S Dittman, James N\nSturgis, and Simon Scheuring. Membrane-mediated protein interactions drive membrane protein\norganization. Nature Communications, 13(1):7373, 2022.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have? a large-scale open domain question answering dataset from medical\nexams. Applied Sciences, 11(14):6421, 2021.\n11\nPreprint. Under review.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021.\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton. Learning the Difference That Makes A\nDifference with Counterfactually-Augmented Data. In International Conference on Learning\nRepresentations, 2019.\nDavid Ryan Koes, Matthew P Baumgartner, and Carlos J Camacho. Lessons learned in empirical\nscoring with smina from the csar 2011 benchmarking exercise. Journal of chemical information\nand modeling, 53(8):1893–1904, 2013.\nJoseph B Kruskal. Nonmetric multidimensional scaling: a numerical method. Psychometrika, 29(2):\n115–129, 1964.\nGeorg Kustatscher, Tom Collins, Anne-Claude Gingras, Tiannan Guo, Henning Hermjakob, Trey\nIdeker, Kathryn S Lilley, Emma Lundberg, Edward M Marcotte, Markus Ralser, et al. An open\ninvitation to the Understudied Proteins Initiative. Nature Biotechnology, 40(6):815–817, 2022.\nMichael J Lee and Michael B Yaffe. Protein regulation in signal transduction. Cold Spring Harbor\nperspectives in biology, 8(6):a005918, 2016.\nZeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,\nRobert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level\nprotein structure with a language model. Science, 379(6637):1123–1130, 2023.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-BERT:\nEnabling Language Representation with Knowledge Graph. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 34, pp. 2901–2908, 2020.\nYizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie.\nBioMedGPT: Open Multimodal Generative Pre-trained Transformer for Biomedicine. arXiv\npreprint arXiv:2308.09442, 2023.\nMarcin Luzarowski, Rubén Vicente, Andrei Kiselev, Mateusz Wagner, Dennis Schlossarek, Alexander\nErban, Leonardo Perez de Souza, Dorothee Childs, Izabela Wojciechowska, Urszula Luzarowska,\net al. Global mapping of protein–metabolite interactions in saccharomyces cerevisiae reveals that\nser-leu dipeptide regulates phosphoglycerate kinase activity. Communications Biology, 4(1):181,\n2021.\nJoshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models\nenable zero-shot prediction of the effects of mutations on protein function. Advances in Neural\nInformation Processing Systems, 34:29287–29303, 2021.\nMilot Mirdita, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin\nSteinegger. ColabFold: Making Protein folding accessible to all. Nature Methods, 2022. doi:\n10.1038/s41592-022-01488-1.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-Task Generalization\nvia Natural Language Crowdsourcing Instructions. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470–3487, 2022.\nPascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez, Debora\nMarks, and Yarin Gal. Tranception: Protein Fitness Prediction with Autoregressive Transformers\nand Inference-time Retrieval. In International Conference on Machine Learning, pp. 16990–17017.\nPMLR, 2022.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730–27744, 2022.\n12\nPreprint. Under review.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. MedMCQA: A Large-scale\nMulti-subject Multi-Choice Dataset for Medical domain Question Answering. In Conference on\nHealth, Inference, and Learning, pp. 248–260. PMLR, 2022.\nTyphaine Paysan-Lafosse, Matthias Blum, Sara Chuguransky, Tiago Grego, Beatriz Lázaro Pinto,\nGustavo A Salazar, Maxwell L Bileschi, Peer Bork, Alan Bridge, Lucy Colwell, et al. InterPro in\n2022. Nucleic Acids Research, 51(D1):D418–D427, 2023.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep Contextualized Word Representations. In North American Chapter of the\nAssociation for Computational Linguistics, pp. 2227–2237, New Orleans, Louisiana, June 2018.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N18-1202.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling Language Models:\nMethods, Analysis & Insights from Training Gopher, 2021.\nRoshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer\nProtein Language Models Are Unsupervised Structure Learners. In International Conference on\nLearning Representations, 2020.\nRoshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu,\nand Alexander Rives. MSA Transformer. In International Conference on Machine Learning, pp.\n8844–8856. PMLR, 2021.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel\nBarth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, et al. A Generalist Agent. Transactions\non Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/\nforum?id=1ikK0kHjvj. Featured Certification, Outstanding Certification.\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo,\nMyle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences. Proceedings of the National\nAcademy of Sciences, 118(15):e2016239118, 2021.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask Prompted Training Enables\nZero-Shot Task Generalization. In International Conference on Learning Representations, 2021.\nTimo Schick and Hinrich Schütze. Generating Datasets with Pretrained Language Models. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.\n6943–6951, 2021.\nMartin Steinegger and Johannes Söding. MMseqs2 enables sensitive protein sequence searching for\nthe analysis of massive data sets. Nature biotechnology, 35(11):1026–1028, 2017.\nThomas C Südhof. The synaptic vesicle cycle: a cascade of protein–protein interactions. Nature, 375\n(6533):645–653, 1995.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuan-Jing Huang, and Zheng\nZhang. CoLAKE: Contextualized Language and Knowledge Embedding. In Proceedings of the\n28th International Conference on Computational Linguistics, pp. 3660–3670, 2020.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi\nChen, Yanbin Zhao, Yuxiang Lu, et al. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training\nfor Language Understanding and Generation. arXiv preprint arXiv:2107.02137, 2021.\nBaris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt\nConsortium. UniRef clusters: a comprehensive and scalable alternative for improving sequence\nsimilarity searches. Bioinformatics, 31(6):926–932, 2015.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n13\nPreprint. Under review.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A Large Language Model for\nScience. arXiv preprint arXiv:2211.09085, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and\nEfficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023.\nOleg Trott and Arthur J Olson. AutoDock Vina: Improving the speed and accuracy of docking\nwith a new scoring function, efficient optimization, and multithreading. Journal of computational\nchemistry, 31(2):455–461, 2010.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.\nSuper-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.\n5085–5109, 2022.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 13484–13508, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.acl-long.754.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In\nInternational Conference on Learning Representations, 2021.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In\nInternational Conference on Learning Representations, 2022a. URL https://openreview.\nnet/forum?id=gEZrGCozdqR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Advances\nin Neural Information Processing Systems, 35:24824–24837, 2022b.\nOrion Weller, Nicholas Lourie, Matt Gardner, and Matthew E Peters. Learning from Task Descriptions.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 1361–1375, 2020.\nMinghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-Modality Learning of Protein\nSequences and Biomedical Texts. arXiv preprint, 2023.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng\nKong. ZeroGen: Efficient Zero-shot Learning via Dataset Generation. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pp. 11653–11669, 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language\nModels, 2022a.\nTaolin Zhang, Chengyu Wang, Nan Hu, Minghui Qiu, Chengguang Tang, Xiaofeng He, and Jun\nHuang. Dkplm: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural\nLanguage Understanding. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 11703–11711, 2022b.\n14\nPreprint. Under review.\nA\nAPPENDIX\nA.1\nDETAILED PROTEIN UNDERSTUDYING PROBLEM ANALYSIS\n0\n100k\n50k\n150k\n(b) Annotation Understudy\n(a) Protein Understudy\n0\n500\n1000\n0\n1K\n2K\n3K\n4K\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\n\u0016\u0015\u0014\u001a\u0013\u0015\u0018\n\u001f\u0013\u001a\u001a\u0012\u0017\u0013\u0017\u0011\u0010\u0019\u000f\u0013\n\u000e\u0013\u0014\u0010\u0013\u001d\u0013\r\n\u001f\u0013\u001a\u001a\u0012\f\u000f\u000f\u0013\u0010\u0012\u0017\u0013\u0017\u0011\u0010\u0019\u000f\u0013\n\u000b\u0013\u0017\u0011\u0010\u0019\u000f\u0013\n\u001f\n\u001a\u001c\u0010\u001c\u001b\u001a\u0019\u0018\u001d\n\t\u000f\r\u001c\u001b\u001a\u0019\u0018\u0017\f\u0014\n\u000b\f\u001d\u001c\u0014\n\u001c\u000f\r\u0010\f\u001c\u000f\n\u001f\n\u001a\u001c\u0010\u001c\u001b\u001a\u0019\u0018\u001d\u0012\u001d\n\u001e\u001a\u0019\b\u001c\f\r\u0012\u0017\u0013\u0017\u0011\u0010\u0019\u000f\u0013\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u001b\u0019\u0016\u001c\u0015\n\u0014\u0013\u0012\u0011\u001b\u001e\u0018\u001d\u0010\u0018\u001f\u001e\u001d\u001c\u001b\u001a\u0019\n\u0014\u0013\u0012\u0011\u001b\u001e\u0018\u001d\u0010\u0018\u001f\u001e\u001d\u001c\u001b\u001a\u0019\nFigure 7: The overview of the problem of understudied proteins. (a) We visualized the protein length\ndistribution for different annotation scores. The annotation score provides a heuristic measure of the\nannotation content (Score 5 is associated with the best-annotated entries, and a score 1 denotes an\nentry with rather basic annotation.). (b) We visualized the ten most used categories in subcellular\nlocation annotations.\nExample 1\nKGC Task: Head Prediction\nTriple: ({protein}, family, Insulin-like receptor)\nAnswer:\nInstruction: I would like a protein in insulin-like receptor.\nOutput: One of the protein that meets the demand is {protein}\nExample 2\nKGC Task: Triple Classification\nTriple: ({protein}, family, insulin-like receptor) -> Yes\nAnswer:\nInstruction: Does {protein} belong to insulin-like receptor?\nOutput: Based on the record, the answer is yes.\nExample 3\nKGC Task: Tail Entity Prediction\nTriple:  ({protein}, function, insulin receptor substrate binding)\nKCM: ({protein}, family, insulin-like receptor) -> \n          ({protein}, function, insulin receptor substrate binding)\nAnswer:\nInstruction: What function does  {protein} have?\nOutput: Since the protein in insulin-like receptor family, \n             the protein enables insulin receptor substrate binding.\nExample 4\nKGC Task: Tail Entity Prediction\nTriple: ({protein}, locate, nucleus)\nKCM: ({protein}, family, retinoid x receptor/hnf4) -> \n          ({protein}, locate, nucleus)\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001a\u0017\u0016\u0015\u0014\u0013\u0012\u0017\u0011\u001a\u0010\u0015\u000f\u001a\u000e\u0014\u001b\u0017\nAnswer:\nInstruction: Where is {protein} located?\nOutput: Since the protein is in the retinoid x receptor/hnf4 family, \n             it is located in the nucleus.\n\r\u0014\u001e\u0018\f\u0014\u0018\u001a\u0017\u000b\u001d\u0019\u001a\u001b\u0017\u0016\u001a\u001e\u001a\u0015\u0014\u0010\u000f\u001d\u001e\nFigure 8: Our data generation prompt. We\nprovide three in-context examples with and\nexternal knowledge needed to generate the\nnext instruction data. Purple: One of the\nmodel’s generations for the given prompt.\nMuch of life science research is dedicated to unravel-\ning the biological functions of proteins. While certain\nproteins, such as the well-studied tumor suppressor\np53 (Dolgin, 2017), have undergone extensive inves-\ntigation, tens of thousands of proteins remain catego-\nrized as understudied. This classification implies that\ntheir biological functions are poorly elucidated, and\nthey lack comprehensive annotation of their molec-\nular properties.\nIn Figure 7, we present an analysis conducted on\nUniProtKB/Swiss-Prot, a highly reputable and man-\nually curated protein knowledge repository. Figure 7\n(a) depicts the relationship between the distribution\nof proteins and their annotation scores. These results\nemphasize the substantial variation in protein distri-\nbution corresponding to different annotations. This\nvariance implies that the annotation of proteins is\nbiased. To illustrate this problem more clearly, we an-\nalyze the subcellular location annotation. Figure 7 (b)\nillustrates the distribution of such annotations. The\ndata reveals a notable concentration of research at-\ntention on proteins residing in the cytoplasm, with\nother subcellular locations significantly lacking in\ncomprehensive labeling and study.\nA.2\nDETAILED METHOD\nA.2.1\nIN-CONTEXT EXAMPLES\nKnowledge Instruction relies on examples to teach\nlanguage models understand how to convert informa-\ntion extracted from the knowledge graph into instruc-\ntion data. Here we provide our example (Figure 8). We notice that when only two examples of\n15\nPreprint. Under review.\nTable 5: Example of instructions in the Knowledge Instruction dataset. We leverage {protein} as a\nplaceholder for the corresponing protein sequence.\nTask\nInstruction\nFamily Generation\nInstruction: Can you provide me with a protein belonging to the secretoglobin family?\nOutput: Sure, here’s a protein from the secretoglobin family: {protein}.\nSuperfamily Prediction\nInstruction: Could you please provide information about the superfamily to which\n{protein} belongs?\nOutput: {protein} is likely a member of the rho GTPase activation protein superfamily.\nDomain Generation\nInstruction: Could you provide me with a protein that contains the\nnad(p)-binding rossmann-fold domain?\nOutput: Certainly, here is a protein that contains the nad(p)-binding rossmann-fold\ndomain: {protein}.\nFamily Prediction\nInstruction: What family does {protein} belong to?\nOutput: The protein belongs to the alpha-1b-glycoprotein/leukocyteimmunoglobulin-\nlike receptor family.\nFunction Prediction\nInstruction: protein Does the protein have the molecular function of zinc ion binding?\nOutput: Yes, based on the information that the protein belongs to the Retinoid X\nreceptor/HNF4 family, it indeed has the molecular function of zinc ion binding.\nProcess Prediction\nInstruction: I’m interested in learning about the biological process associated with\n{protein}. Could you provide information on this?\nOutput: Given its association with the Retinoid X receptor/HNF4 family, {protein} is\nlikely involved in the regulation of DNA-templated transcription.\nBinding Site Generation\nInstruction: Provide me with a protein associated with the IQ motif EF-hand binding site.\nOutput: Sure, the protein that interacts with the IQ motif EF-hand binding site and\nexhibits a protein binding molecular function is {protein}.\nFamily Prediction\nInstruction: Does {protein} belong to nfat activation molecule 1?\nOutput: Based on the record, the answer is yes.\ndifferent expressions are provided for each KGC task, the language capabilities of LLMs are activated,\ngenerating a variety of instruction data as illustrated in Table 5\nA.2.2\nEDIT DISTANCE ALGORITHM\nWe denote A = a1a2 . . . an and B = b1b2 . . . bn as two sequences to be aligned, where n and m are\nthe lengths of A and B, respectively. Before calculating the editing distance, we have to determine\nthe substitution matrix to calculate the replacement score s(·, ·) ∈(0, 1] and the gap penalty scheme\nWk, where k is the gap length. Then the distance matrix H can be formulated as:\nHi,j = min{Hi−1,j−1 + s(ai, bj); Hi−k,j −Wk; Hi,j−1 −W1; 1}\n(5)\nwhere Hk,0 = H0, l = 0 for 0 ≤k ≤n and 0 ≤l ≤m. We leverage Hn,m/ max(n, m) as the\nsequence distance between A and B.\nA.3\nDETAILED EXPERIMENTAL SETUPS\nWe perform incremental training on OPT-1.3b. We wrap the protein sequence with <protein> and\n</protein> and apply character-based tokenization, treating each amino acid as a single token. For\ntext corpus, we tokenize them using the GPT-2 byte level BPE tokenizer. We utilize Pytorch to\nconduct experiments with 8 32G V100 GPUs. We use a batch size of 128 and a context length of\n1,024 tokens. We adopt the Fully Sharded Data Parallel (FSDP) acceleration strategy alongside the\nfp16 data format. We adopt the AdamW optimizer with β = (0.9, 0.98). We set the weight decay to\n0.01 and the dropout rate to 0.1. The learning rate increases to 1e-4 for the first 5000 warming-up\nsteps and decays linearly to 0 for the rest of the training steps. We pre-train InstructProtein for the\nfirst 40,000 steps, and instruction tune it in the next 20,000 steps.\nA.4\nDOWNSTREAM TASK DEFINITION\nWe list the detailed definition of downstream tasks. {protein} and {label} are used as placeholders.\nDataset statistics are summarized in Table 6.\n16\nPreprint. Under review.\nSubcellular Localization Prediction is a sequence-level classification task. Each input sequence x\nis mapped to a label y which represents the subcellular location.\n• Prompt template (InstructProtein, OPT, LLaMA, Alpha, BioMedGPT): {protein} Instruction:\nWhat cellular components is the protein located in?\n• Prompt template (Galactica): {protein} ## Subcellular Location\n• Label words (sub): {0: \"plasma membrane\", 1: \"cytoplasm\", 2: \"endoplasmic reticulum\", 3:\n\"golgi\", 4: \"vacuole\", 5: \"mitochondrion\", 6: \"nucleus\", 7: \"peroxisome\", 8: \"chloroplast\",\n9: \"extracellular\"}\n• Label words (bin): {0: [\"plasma membrane\", \"golgi\", \"vacuole\", \"endoplasmic reticulum\"],\n1: [\"extracellular\", \"peroxisome\", \"nucleus\", \"cytoplasm\", \"mitochondrion\", \"chloroplast\"]}\nProtein Function Annotation is a sequence-level classification task to annotation protein with\nfunctional labels. Each example consists of a protein, a label. They system must predict whether the\nlabel belongs to the protein.\n• Prompt template: {protein} Instruction: Does the protein associate with label?\n• Label words: {0: \"No\", 1: \"Yes\"}\nMetal Ion Binding Prediction is a sequence-level classification task to predict whether a protein can\nbind to ion.\n• Prompt template: {protein} Instruction: Does the protein associate with metal ion binding?\n• Label words: {0: \"No\", 1: \"Yes\"}\nInstruction-Protein Pairing Accuracy probe the insturction-following capabilities in protein gener-\nation. Protein are decoded under 10 different instructions (9 randomly sampled instructions and 1\ntrue corresponding instruction). The system must predict which one is the most relevant instruction.\n• Prompt template: Instruction: I would like a protein that is in {label}. Output: One of the\nprotein that meets the demand is {protein}\"\nTable 6: Dataset Statistics for downstream tasks.\nDataset\n# Test\nSubcellular Localization Prediction - bin\n1,749\nSubcellular Localization Prediction - sub\n2,773\nProtein Function Annotation - Biological Process\n104,794\nProtein Function Annotation - Molecular Function\n22,372\nProtein Function Annotation - Cellular Component\n38,594\nMetal Ion Binding Prediction\n1,332\nInstruction-Protein Pairing Accuracy - Fold\n718\nInstruction-Protein Pairing Accuracy - Family\n1,272\nInstruction-Protein Pairing Accuracy - Superfamily\n7,408\n17\nPreprint. Under review.\nA.5\nMORE EXAMPLES\nWhat superfamily does \nthe protein belong to?\nBioMedGPT\nBelongs to the MqsS/MqsR\nfamily.\nGalactica\nBelongs to the RlmN family.\n### KeywordsCytoplasm, \nGTP-binding, Initiation factor, \nNucleotide-binding, Protein \nbiosynthesis, RNA-binding\nInstructProtein\nThe protein is in signal transduction\nhistidine kinase, dimerisation/phos-\nphoacceptor domain superfamily.\nGround Truth\n1. histidine kinase/hsp90-like \natpase superfamily \n2. signal transduction histidine kinase, \ndimerisation/phosphoacceptor \ndomain superfamily.\nWhat domain does \nthe protein have?\nIn the C-terminal section; belongs\nto the transferase hexapeptide \nrepeat family.\nThe C-terminal region (residues \n470-550) is predicted to be a \ntransmembrane domain, \nsuggesting that it may be located\nin the outer membrane.\nThe protein has glmu, c-terminal \nlbh domain and moba-like ntp \ntransferase domain.\n1. glmu, c-terminal lbh domain \n2. moba-like ntp transferase domain\nWhat is the function\nof this protein?\nCatalyzes the last two sequential \nreactions in the de novo biosynthe-\ntic pathway for UDP-N-acetylgluc-\nosamine.  \nPart of the ABC transporter \ncomplex MetNIQ involved in \nmethionine import. Responsible \nfor energy coupling to the \ntransport system.\nThe protein enables magnesium\nion binding, glucosamine-1-pho-\nsphate n-acetyltransferase activity, \nand udp-n-acetylglucosamine \ndiphosphorylase activity.\n1. magnesium ion binding\n2. glucosamine-1-phosphate n-acetylt-\nransferase activity\n3. udp-n-acetylglucosamine diphosph-\norylase activity\nDoes the protein have\nproteinase, inhibitor i2,\nkunitz, conserved site?\nHas a Kunitz-type B chain.\nAnswer: No\nBased on the record,\nthe answer is yes.\nThe protein has proteinase inhibitor i2, \nkunitz, conserved site.\nFigure 9: More examples of protein understanding.\nInstruction: I would like a protein that has cystatin domain. Output: \nGoundTruth:\n• UniProt ID: B2Z449\nDesign 2:\n• Length: 115 residues\nDesign 3:\n• Length: 107 residues\nDesign 1:\n• Length: 165 residues\nInstruction: I would like a protein that enables GTP binding. Output: \nGoundTruth:\n• Affinity: -6.0 (kcal/mol)\n• PDB id: 5C1S-A\nDesign 2:\n• Affinity: -8.2 (kcal/mol)\n• pLDDT: 44.4\nDesign 3:\n• Affinity: -8.9 (kcal/mol)\n• pLDDT: 40.3\nDesign 1:\n• Affinity: -7.9 (kcal/mol)\n• pLDDT: 96.4\nFigure 10: More examples of function-instruction-based protein de novo design.\n18\nPreprint. Under review.\nInstruction: I would like a protein that is in metallothionein family. Output: \nGoundTruth:\n• UniProt ID: A0A024R6T4\nDesign 2:\n• Length: 61 residues\nDesign 3:\n• Length: 54 residues\nDesign 1:\n• Length: 61 residues\nInstruction: I would like a protein that is in retroviral VpR/VpX protein family. Output: \nGoundTruth:\n• UniProt ID: A0A023HIS7\nDesign 2:\n• Length: 96 residues\nDesign 3:\n• Length: 96 residues\nDesign 1:\n• Length: 126 residues\nInstruction: I would like a protein that is in SsrA-binding protein family. Output: \nGoundTruth:\n• UniProt ID: A0ALD2\nDesign 2:\n• Length: 150 residues\nDesign 3:\n• Length: 151 residues\nDesign 1:\n• Length: 153 residues\nInstruction: Instruction: I would like a protein that is in kappa casein family. Output: \nGoundTruth:\n• UniProt ID: P02668\nDesign 2:\n• Length: 145 residues\nDesign 3:\n• Length: 192 residues\nDesign 1:\n• Length: 98 residues\nFigure 11: More examples of family-instruction-based protein de novo design (colored by pLDDT).\n19\nPreprint. Under review.\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001c\u0015\u0014\u0013\u0012\n\u001f\u0011\u0012\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001c\u001e\u0010\u0013\u0014\n\u0011\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u0015\u001d\u0013\u0011\n\u000f\u0011\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u0015\u0014\u0013\u0012\n\u000f\u001f\u001e\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u001c\u001d\u000e\u0013\u001e\n\u000f\u001e\u0011\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u001c\u0015\u0014\u0013\u000e\n\u001e\u001f\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u0015\u000f\u0013\u0012\n\u001d\u0014\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u0014\u0010\u0013\u0015\n\u0010\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u001d\u0014\u0013\u000e\n\u001d\u001f\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u000e\u001e\u0013\u000f\n\u001f\u000e\u000f\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u001d\u0010\u0013\u001d\n\u000f\u001f\u001f\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001a\u0019\n\u001f\u001e\u001d\u001d\u001c\u001b\u001e\u000e\u0013\u0010\n\u001f\u001e\u001e\u001d\u001d\u001d\u001d\u001d\u001d\u001d\u001c\u001b\u001a\u001e\u0019\u0018\nα\n\u001f\u001e\u001e\u001d\u001d\u001d\u001d\u001d\u001c\u0017\u001b\u001a\u001a\u0016\u001d\nβ\n\r\n\u001c\u001b\u001a\u001e\u0019\u0018\nα\n\u001c\u0017\u001b\u001a\u001a\u0016\u001d\nβ\nFigure 12: More examples of structure-instruction-based protein de novo design (colored by pLDDT).\n20\n",
  "categories": [
    "q-bio.BM",
    "cs.CL"
  ],
  "published": "2023-10-05",
  "updated": "2023-10-05"
}