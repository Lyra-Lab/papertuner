{
  "id": "http://arxiv.org/abs/2310.04496v1",
  "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
  "authors": [
    "Zeyu Yun",
    "Juexiao Zhang",
    "Bruno Olshausen",
    "Yann LeCun",
    "Yubei Chen"
  ],
  "abstract": "Unsupervised representation learning has seen tremendous progress but is\nconstrained by its reliance on data modality-specific stationarity and\ntopology, a limitation not found in biological intelligence systems. For\ninstance, human vision processes visual signals derived from irregular and\nnon-stationary sampling lattices yet accurately perceives the geometry of the\nworld. We introduce a novel framework that learns from high-dimensional data\nlacking stationarity and topology. Our model combines a learnable\nself-organizing layer, density adjusted spectral clustering, and masked\nautoencoders. We evaluate its effectiveness on simulated biological vision\ndata, neural recordings from the primary visual cortex, and gene expression\ndatasets. Compared to state-of-the-art unsupervised learning methods like\nSimCLR and MAE, our model excels at learning meaningful representations across\ndiverse modalities without depending on stationarity or topology. It also\noutperforms other methods not dependent on these factors, setting a new\nbenchmark in the field. This work represents a step toward unsupervised\nlearning methods that can generalize across diverse high-dimensional data\nmodalities.",
  "text": "URLOST: Unsupervised Representation Learning\nwithout Stationarity or Topology\nZeyu Yun3, Juexiao Zhang1, Bruno Olshausen3, Yann LeCun1,4, Yubei Chen1,2\n1 New York University\n2 University of California, Davis\n3 University of California, Berkeley\n4 Meta AI, FAIR\nAbstract\nUnsupervised representation learning has seen tremendous progress but is con-\nstrained by its reliance on data modality-specific stationarity and topology, a lim-\nitation not found in biological intelligence systems. For instance, human vision\nprocesses visual signals derived from irregular and non-stationary sampling lat-\ntices yet accurately perceives the geometry of the world. We introduce a novel\nframework that learns from high-dimensional data lacking stationarity and topol-\nogy. Our model combines a learnable self-organizing layer, density adjusted spec-\ntral clustering, and masked autoencoders. We evaluate its effectiveness on sim-\nulated biological vision data, neural recordings from the primary visual cortex,\nand gene expression datasets. Compared to state-of-the-art unsupervised learning\nmethods like SimCLR and MAE, our model excels at learning meaningful repre-\nsentations across diverse modalities without depending on stationarity or topology.\nIt also outperforms other methods not dependent on these factors, setting a new\nbenchmark in the field. This work represents a step toward unsupervised learning\nmethods that can generalize across diverse high-dimensional data modalities.\n1\nIntroduction\nUnsupervised representation learning, also known as self-supervised representation learning (SSL),\naims to develop models that autonomously detect patterns in data and make these patterns read-\nily apparent through a specific representation. There has been tremendous progress over the past\nfew years in the unsupervised representation learning community. Popular methods like contrastive\nlearning and masked autoencoders [68; 6; 24; 70] work relatively well on typical modalities such\nas images, videos, audio, time series, and point clouds. However, these methods make implicit as-\nsumptions about the data domain’s topology and stationarity. Given an image, topology refers to\nthe neighboring pixels of each pixel, or more generally, the grid structure in images, the temporal\nstructure in time series and sequences, or the 3D structure in molecules and point clouds. Sta-\ntionarity refers to the property that the low-level statistics of the signal remain consistent across\nits domain. For instance, pixels and patches in images exhibit similar low-level statistics (mean,\nvariance, co-variance) regardless of their locations within the domain. The success of state-of-the-\nart self-supervised representation learning relies on knowing the prior topology and stationarity of\nthe modalities. For example, joint-embedding SSL employs random-resized cropping augmentation\n[6], and masked auto-encoding [25] utilizes masked-image-patch augmentation. What if we possess\nhigh-dimensional signals without knowledge of their domain topology or stationarity? Can we still\ncraft a high-quality representation? This is not only the situation that biological vision systems have\nto deal with but also a practical setting for many scientific data analysis problems. In this work, we\nintroduce unsupervised representation learning without stationarity or topology (URLOST) and\ntake a step in this direction.\nPreprint. Under review.\narXiv:2310.04496v1  [cs.CV]  6 Oct 2023\nAs we mentioned earlier, typical modalities possess topology and stationarity prior information\nthat can be utilized by unsupervised representation learning. Taking images as an example, digital\ncameras employ a consistent sensor grid that spans the entire visual field. However, biological visual\nsystems have to deal with signals with less domain regularity. For instance, unlike camera sensors\nwhich have a uniform grid, the cones and rods in the retina distribute unevenly and non-uniformly.\nThis results in a non-stationary raw signal. Retinal ganglion cells connect to more photoreceptors in\nthe fovea than in the periphery. The correlation of the visual signal between two different locations\nin the retina depends not only on the displacement between these locations but also on their absolute\npositions. Yet, biological visual systems can establish precise retinotopy from the retina to neurons\nbased on spontaneous retinal activities and external stimuli [67; 35; 18] and leverage retinotopic\ninput to build unsupervised representation. This implies that we can potentially build unsupervised\nrepresentation without relying on prior stationarity of the raw signal or topology of the input domain.\nEnc\nEnc\nℎ!\nℎ\"\nDec\nEnc\n\"\"\n\"!\nℎ\nBiological Vision System\nData w/ stationarity and topology \nData w/o stationarity and topology\n… many more \nhigh -dimensional \ndata in nature\nℎ\nℎ\n？URLOST\nNew Paradigm\nJoint Embedding\nMAE\nFigure 1: From left to right: the unsupervised representation learning through joint embedding and\nmasked auto-encoding; the biological vision system that perceives via unstructured sensor and un-\nderstands signal without stationarity or topology [48]; and many more such diverse high dimensional\nsignal in natural science that our method supports while most existing unsupervised methods don’t.\nData figures are borrowed from [48; 44; 69].\nIn this work, we aim to build unsupervised representations for general high-dimensional vectors.\nTaking images as an example again, let’s assume we receive a set of images whose pixels are shuffled\nin the same order. How can we build representations in an unsupervised fashion without knowledge\nof the shuffling order? If possible, can we use such a method to build unsupervised representa-\ntions for general high-dimensional data? Inspired by [53], we use low-level statistics and spectral\nclustering to form clusters of the pixels, which recovers a coarse topology of the input domain.\nThese clusters are analogous to image patches except that they are slightly irregularly shaped and\ndifferent in size. We mask a proportion of these “patches” and utilize a Vision Transformer [15]\nto predict the masked “patches” based on the remaining unmasked ones. This “learning to predict\nmasked tokens” approach is proposed in masked autoencoders (MAE) [25] and has demonstrated\neffectiveness on typical modalities. Initially, we test the proposed method on the synthesized biolog-\nical visual dataset, derived from CIFAR-10 [32] using a foveated retinal sampling mechanism [8].\nThen we generalize this method to two high-dimensional vector datasets: a primary visual cortex\nneural response decoding dataset [57] and the TCGA miRNA-based cancer classification dataset\n[62; 66]. Across all these benchmarks, our proposed method outperforms existing SSL techniques,\nestablishing its effectiveness in building unsupervised representations for signals lacking explicit\nstationarity or topology. Given the emergence of new modalities in deep learning from natural sci-\nences [59; 23; 49; 34], such as chemistry, biology, and neuroscience, our method offers a promising\napproach in the effort to build unsupervised representations for high-dimensional data.\n2\nRaw \nSignal\nAligned \nClusters\nSignal \nClustering\nSelf-organizing\nLayer\nEnc\nDec\nMasked\nAutoencoder\nRe-organizing\nLayer\nReconstructed\nSignal\nFigure 2: The overview framework of URLOST. The high-dimensional input signal undergoes\nclustering and self-organization before unsupervised learning using a masked autoencoder for signal\nreconstruction.\n2\nMethod\n2.1\nMotivation and Overall framework\nOur objective is to build robust, unsupervised representations for high-dimensional signals that lack\nexplicit topology and stationarity. These learned representations are intended to enhance perfor-\nmance in downstream tasks, such as classification. To achieve this, we begin by using low-level\nstatistics and clustering to approximate the signal’s topology. The clusters derived from the signal\nserve as input to a masked autoencoder. As depicted in Figure 1, the masked autoencoder randomly\nmasks out patches in an image and trains a Transformer-based autoencoder unsupervisedly to re-\nconstruct the original image. After the unsupervised training, the autoencoder’s latent state yields\nhigh-quality representations. In our approach, signal clusters are input to the masked autoencoder.\nNotably, the clusters differ from image patches in several key aspects due to the differences in the\ninput signal: they are unaligned, exhibit varied sizes and shapes, and their clustering nodes are not\nconfined to fixed 2D locations like pixels in image patches. To cope with these differences, we\nintroduce a self-organizing layer responsible for aligning these clusters through learnable transfor-\nmations. The parameters of this layer are jointly optimized with those of the masked autoencoder.\nOur method is termed URLOST, an acronym for Unsupervised Representation Learning withOut\nStationarity or Topology. Figure 2 provides an overview of the framework. URLOST consists\nof three core components: density adjusted spectral clustering, self-organizing layer, and masked\nautoencoder. The functionalities of these components are detailed in the following subsections.\n2.2\nDensity Adjusted Spectral clustering\nRepresentation learning for high-dimensional signals without explicit topology is challenging. We\npropose to define a metric to measure inter-dimensional relationships. This metric effectively ap-\nproximates a topology for the signal. Similar to [53], where they use the absolute correlation values\nas the metric for pixels, we employ discrete mutual information (refer to Appendix A.1) as the\nmetric. Let affinity matrix Aij denote the mutual information between dimension i and j, which\napproximates the manifold M that the signal lives on. We can define the discretized Laplacian op-\nerator based on A and use the eigenvector of the Laplacian operator to perform spectral clustering,\nwhich segments the manifold. The detailed definition and the algorithm are left in Appendix A.1.\nFinding the eigenvector of the Laplacian operator is a discretized approximation of the following\noptimization problem in function space:\nmin\n||f||L2(M)\nZ\nM\n||p(x)\n1\n2 ∇f(x)||2\n(1)\nwhere f(x) : M →[0, 1] is the normalized signal defined on M and p(x) is the density function.\nThe integral is taken over standard measure on M. Since spectral clustering heavily relies on the\nsolution of equation 1, the definition of the density function p(x) affects the quality of the resulting\nclusters. Standard approaches often assume that nodes are uniformly distributed on the manifold,\nthereby treating p(x) as a constant and excluding it from the optimization process. However, this\n3\nassumption does not hold in our case involving non-stationary signals. To cope with non-stationary\nsignals, our work introduces a variable density function p(x) for each signal, making it a pivotal\ncomponent in building good representations for the signal. This component is referred to as Den-\nsity Adjusted Spectral Clustering. Empirical evidence supporting this design is provided through\nvisualization and ablation studies in the experimental section.\n2.3\nSelf-organizing layer\nTransforming a high-dimensional signal into a sequence of clusters using the above method is not\nenough because it does not capture the internal structure within individual clusters. To effectively\nperform unsupervised learning on these clusters, it is essential to align them in some manner. Di-\nrectly solving the exact alignment problem with low-level statistics of the signal is challenging.\nThus, we propose a self-organizing layer with learnable parameters. Specifically, let vector x(i)\ndenote the ith cluster. Each cluster x(i) is passed through a differentiable function g(·, w(i)) with\nparameter w(i), resulting in a sequence z0:\nz0 = [g(x(1), w(1)), · · · g(x(M), w(M))]\n(2)\nz0 is comprised of projected and aligned representations for all clusters. The weights of the proposed\nself-organizing layer, {w(1), · · · w(M)}, are jointly optimized with the subsequent neural network\nintroduced in the next subsection.\n2.4\nMasked autoencoder\nAfter the self-organizing layer, z0 is passed to a Transformer-based masked autoencoder (MAE)\nwith an unsupervised learning objective. Masked autoencoder (MAE) consists of an encoder and a\ndecoder which both consist of stacked Transformer blocks introduced in [64]. The objective function\nis introduced in [25]: masking random image patches in an image and training an autoencoder to\nreconstruct them, as illustrated in Figure 1. In our case, randomly selected clusters in z0 are masked\nout, and the autoencoder is trained to reconstruct these masked clusters. After training, the encoder’s\noutput is treated as the learned representation of the input signal for downstream tasks. The masked\nprediction loss is computed as the mean square error (MSE) between the values of the masked\nclusters and their corresponding predictions.\n3\nResult\nSince our method is inspired by the biological vision system, we first validate its ability on a syn-\nthetic biological vision dataset created from CIFAR-10. Then we evaluate the generalizability of\nURLOST on two high-dimensional natural datasets collected from diverse domains. Detailed infor-\nmation about each dataset and the corresponding experiments is presented in the following subsec-\ntions. Across all tasks, URLOST consistently outperforms other strong unsupervised representation\nlearning methods.\n3.1\nSynthetic biological vision dataset\nAs discussed in the introduction, the biological visual signal serves as an ideal dataset to validate the\ncapability of URLOST. In contrast to digital images captured by a fixed array of sensors, the biologi-\ncal visual signal is acquired through irregularly positioned ganglion cells, inherently lacking explicit\ntopology and stationarity. However, it is hard to collect real-world biological vision signals with\nhigh precision. Therefore, we employ a retinal sampling technique to modify the classic CIFAR-10\ndataset and simulate imaging from the biological vision signal. The synthetic dataset is referred to\nas Foveated CIFAR-10. To make a comprehensive comparison, we also conduct experiments on the\noriginal CIFAR-10, and a Permuted CIFAR-10 dataset obtained by randomly permuting the image.\nPermuted CIFAR-10. To remove the grid topology inherent in digital imaging, we simply per-\nmute all the pixels within the image, which effectively discards any information related to the grid\nstructure of the original digital image. We applied such permutation to each image in the CIFAR-10\ndataset to generate the Permuted CIFAR-10 dataset. Nevertheless, permuting pixels only removes\nan image’s topology, leaving its stationarity intact. To obtain the synthetic biological vision that has\nneither topology nor stationarity, we introduce the Foveated CIFAR-10.\n4\nFigure 3: Retina sampling (A) An image in CIFAR-10 dataset. (B) Retina sampling lattice. Each\nblue dot represents the center of a Gaussian kernel, which mimics a retinal ganglion cell. (C) Vi-\nsualization of the car image’s signal sampled using the retina lattice. Each kernel’s sampled RGB\nvalue is displayed at its respective lattice location for visualization purposes. (D) density-adjusted\nspectral clustering results are shown. Each unique color represents a cluster, with each kernel col-\nored according to its assigned cluster.\nFoveated CIFAR-10. Much like photosensors installed in a camera, retina ganglion cells within the\nprimate biological visual system sample from visual stimuli and project images. However, unlike\nphotosensors that have uniform receptive fields and adhere to a consistent sampling pattern, retinal\nganglion cells at different locations of the retina vary in their receptive field size: smaller in the cen-\nter (fovea) but larger in the peripheral of the retina. This distinctive retina sampling pattern results\nin foveated imaging [63]. It gives primates the ability to have both a high-resolution vision and a\nbroad overall receptive field while consequently making visual signals sampled by the retina lack\nstationarity. The evidence is that responses of two ganglion cells separated by the same displace-\nment are highly correlated in the retina but less correlated in the peripheral. To mimic the foveated\nimaging with CIFAR-10, we adopt the retina sampling mechanism from [8]. Specifically, each retina\nganglion cell is simplified and modeled using a Gaussian kernel. The response of each cell is de-\ntermined by the dot product between pixel values and the Gaussian kernel. Figure 3 illustrates the\nsampling kernel locations. Applying this sampling grid and permuting the resulting pixels produces\nthe foveated CIFAR-10. In the natural retina, retinal ganglion cell density decreases linearly with\neccentricity, which makes fovea much denser than the peripheral, compared to the simulated lat-\ntice in Figure 3. However, considering the low resolution of the CIFAR-10 dataset, we reduce the\nsimulated fovea’s density to prevent redundant sampling.\nExperiments. We compare URLOST on both of the synthetic vision datasets as well as the original\nCIFAR-10 with popular unsupervised representation learning methods SimCLR [6] and MAE [25].\nAll the models conducted unsupervised learning followed by linear probing for classification accu-\nracy. The evaluations are reported in Table 1. SimCLR excels on CIFAR-10 but struggles badly with\nboth synthetic datasets due to its inability to handle data without stationarity and topology. MAE\ngets close to SimCLR on CIFAR-10 with a 4 × 4 patch size. However, the patch size no longer\nmakes sense when data has no topology. So we additionally tested MAE masking pixels instead of\nimage patches. It maintains the same performance on Permuted CIFAR-10 as on CIFAR-10, though\npoorly, invariant to the removal of topology as it should be. But It still drops greatly to 48.5% on the\nFoveated CIFAR-10 when stationarity is also removed. In contrast, only URLOST is able to main-\ntain consistently strong performances when there is no topology or stationarity, achieving 86.4% on\nPermuted CIFAR-10 and 85.4% on Foveated CIFAR-10 when the baselines completely fail.\n3.2\nV1 neural response to natural image stimulus\nAfter accessing URLOST’s performance on synthetic biological vision data, we take a step further\nto challenge its generalizability with high-dimensional natural datasets. The first task is decoding\nneural response recording in the primary visual area (V1) of mice.\nV1 neural response dataset. The dataset, published by [44], contains responses from over 10,000\nV1 neurons captured via two-photon calcium imaging. These neurons responded to 2,800 unique\nimages from ImageNet [12], with each image presented twice to assess the consistency of the neural\nresponse. In the decoding task, a prediction is considered accurate if the neural response to a given\nstimulus in the first presentation closely matches the response to the same stimulus in the second\n5\nTable 1: Evaluation on computer vision and synthetic biological vision dataset. ViT (Patch)\nstands for the Vision Transformer backbone with image patches as inputs. ViT (Pixel) means pixels\nare treated as input units. ViT (Clusters) means clusters are treated as inputs instead of patches. The\nnumber of clusters is set to 64 for both Permuted CIFAR-10 and Foveated CIFAR-10 dataset.\nDataset\nMethod\nBackbone\nEval Acc\nCIFAR-10\nMAE\nViT (Patch)\n88.3 %\nMAE\nViT (Pixel)\n56.7 %\nSimCLR\nResNet-18\n90.7 %\nPermuted CIFAR-10\nURLOST MAE\nViT (Cluster)\n86.4 %\n(no topology)\nMAE\nViT (Pixel)\n56.7 %\nSimCLR\nResNet-18\n47.9 %\nFoveated CIFAR-10\nURLOST MAE\nViT (Cluster)\n85.4 %\n(no topology or stationarity)\nMAE\nViT (Pixel)\n48.5 %\nSimCLR\nResNet-18\n38.0 %\nTable 2: Evaluation on V1 response decoding and TCGA pan-cancer classification tasks.\n“Raw” indicates preprocessed (standardized and normalized) raw signals. Best β values are used for\nβ-VAE. For URLOST MAE, cluster sizes are 200 (V1) and 32 (TCGA).\nMethod\nV1 Response Decoding Acc\nTCGA Classification Acc\nRaw\n73.9%\n91.1%\nβ-VAE\n75.1%\n94.2%\nMAE\n64.8%\n88.3%\nURLOST MAE\n78.2%\n94.9%\npresentation within the representation space. This task presents greater challenges than the synthetic\nbiological vision described in the prior section. For one, the data comes from real-world neural\nrecordings rather than a curated dataset like CIFAR-10. For another, the geometric structure of the\nV1 area is substantially more intricate than that of the retina. To date, no precise mathematical\nmodel of the V1 neural response has been well established. The inherent topology and stationarity\nof the data still remain difficult to grasp [43; 42]. Nevertheless, evidence of Retinotopy [18; 19]\nand findings from prior research [41; 7; 58] suggest that the neuron population code in V1 are tiling\na low dimensional manifold. This insight led us to treat the population neuron response as high-\ndimensional data and explore whether URLOST can effectively learn its representation.\nExperiments. Following the approach in [44] we apply standardization and normalization to the\nneural firing rate. The processed signals are high-dimensional vectors, and they can be directly\nused for the decoding task, which serves as the “raw” signal baseline in Table 2. For representation\nlearning methods, URLOST is evaluated along with MAE and β-VAE [26]. Note that the baseline\nmethods need to handle high-dimensional vector data without stationarity or topology, so SimCLR is\nno longer applicable. We use β-VAE instead. We first train the neural network with an unsupervised\nlearning task, then use the latent state of the network as the representation for the neural responses\nin the decoding task. The results are presented in the table 2. Our method surpasses the original\nneuron response and other methods, achieving the best performance.\n3.3\nGene expression data\nIn this subsection, we further evaluate URLOST on high-dimensional natural science data from a\ncompletely different domain, the gene expression data.\nGene expression dataset. The dataset comes from The Cancer Genome Atlas (TCGA) [62; 66],\nwhich is a project that catalogs the genetic mutations responsible for cancer using genome sequenc-\ning and bioinformatics. The project molecularly characterized over 20,000 primary cancers and\nmatched normal samples spanning 33 cancer types. We focus on the pan-cancer classification task:\ndiagnose and classify the type of cancer for a given patient based on his gene expression profile.\nThe TCGA project collects the data of 11,000 patients and uses Micro-RNA (miRNA) as their gene\n6\nTable 3: Ablation study on self-organizing layer. Linear probing accuracy with varying param-\neters, keeping others constant. For Locally-Permutated CIFAR-10, we use 4 × 4 patch size. For\nPermutated CIFAR-10 and Foveated CIFAR-10, we set the number of clusters to 64 for the spectral\nclustering algorithm. We kept the hyperparameter of the backbone model the same as in table 1.\nDataset\nProjection\nEval Acc\nLocally-Permuted\nshared\n81.4 %\nCIFAR-10\nnon-shared\n87.6 %\nPermuted\nshared\n80.7 %\nCIFAR-10\nnon-shared\n86.4 %\n(a) Replacing the non-shared projections of the\nself-organizing layer with the shared projection\nlayer entails a significant drop in performance.\nDataset\nCluster\nEval Acc\nFoveated\nSC\n82.7 %\nCIFAR-10\nDSC\n85.4 %\n(b) “SC” denotes spectral clustering with uniform den-\nsity clustering and “DSC” denotes density adjusted\nspectral clustering. For Foveated CIFAR-10 using den-\nsity adjusted spectral clustering to create clusters will\nmake the model perform better than using standard\nspectral clustering with uniform density.\nexpression profiles. Like the V1 response, no explicit topology and stationarity are known and each\ndata point is a high-dimensional vector. Specifically, 1773 miRNA identifiers are used so that each\ndata point is a 1773-dimensional vector. Types of cancer that each patient is diagnosed with serve\nas the classification labels.\nExperiments. Similar to Section 3.2, URLOST is compared with the original signals, MAE, and\nβ-VAE, which is the state-of-the-art unsupervised learning method on TCGA cancer classification\n[71; 72]. We also randomly partition the dataset do five-fold cross-validation and report the average\nperformance in Table 2. Again, our method learns meaningful representation from the original sig-\nnal. The learned representation benefited the classification task and achieved the best performance,\ndemonstrating URLOST’s ability to learn meaningful representation of data from diverse domains.\n4\nAblation study\n4.1\nSelf-organizing Layer vs Shared Projection Layer\nConventional SSL models take a sequential input x = [x(1), · · · x(M)] and embed them into latent\nvectors with a linear transformation:\nz0 = [Ex(1), · · · Ex(M)]\n(3)\nwhich is further processed by a neural network. The sequential inputs can be a list of language\ntokens [14; 50], pixel values [5], image patches [15], or overlapped image patches [6; 24; 70]. E\ncan be considered as a projection layer that is shared among all elements in the input sequence.\nThe self-organizing layer g(·, w(i)) introduced in Section 2.3 can be considered as a non-shared\nprojection layer. We conducted an ablation study comparing the two designs to demonstrate the\neffectiveness of the self-organizing layers both quantitatively and qualitatively. To facilitate the\nablation, we further synthesized another dataset.\nLocally-permuted CIFAR-10. To directly evaluate the performance of the non-shared projection\napproach, we designed an experiment involving intentionally misaligned clusters. In this experi-\nment, we divide each image into patches and locally permute all the patches. The i-th image patch is\ndenoted by x(i), and its permuted version, permutated by the permutation matrix E(i), is expressed\nas E(i)x(i). We refer to this manipulated dataset as the Locally-Permuted CIFAR-10. Our hypoth-\nesis posits that models using shared projections, as defined in Equation 3, will struggle to adapt\nto random permutations, whereas self-organizing layers equipped with non-shared projections can\nautonomously adapt to each patch’s permutation, resulting in robust performance. This hypothesis\nis evaluated quantitatively and through the visualization of learned weights w(i).\nPermuted CIFAR-10. Meanwhile, we also run the ablation study on the Permuted CIFAR-10.\nUnlike locally permuted CIFAR-10, a visualization check is not viable since the permutation is done\nglobally. However, we can still quantitatively measure the performance of the task.\n7\nQuantitative results. Table 3 confirms our hypothesis, demonstrating a significant performance\ndecline in models employing shared projections when exposed to permuted data. In contrast, the\nnon-shared projection model maintains stable performance.\nVisual evidence. Using linear layers to parameterize the self-organizing layers, i.e. let g(x, W (i)) =\nW (i)x, we expect that if the projection layer effectively aligns the input sequence, E(i)T W (i) should\nexhibit visual similarities. That is, after applying the inverse permutation E(i)T , the learned projec-\ntion matrix W (i) at each location should appear consistent or similar. The proof of this statement\nis provided in Appendix A.4. The model trained on Locally-Permuted CIFAR10 provides visual\nevidence supporting this claim. In Figure 4, the weights show similar patterns after reversing the\npermutations.\nFigure 4: Learnt weights of a self-organizing layer. (A) Image is cropped into patches, where\neach patch x(i) first undergoes a different permutation E(i), then the inverse permutation E(i)T . (B)\nThe learned weight of the linear self-organizing layer. The 12th column of W (i) at all positions i\nare reshaped into patches and visualized. When W (i) undergoes the inverse permutation E(i)T , they\nshow similar patterns. (C) Visualization of the 37th column of W (i). Similar to (B).\n4.2\nDensity adjusted clustering vs Uniform Density Clustering\nAs explained in Section 2.2, the shape and size of each cluster depend on how the density function is\ndefined. Let q(i) represent the eccentricity, the distance from ith kernel to the center of the sampling\nlattice, and let n(i) = P\nj Aji where A is the affinity matrix, then the density is defined as:\np(i) = q(i)−αn(i)β1.\n(4)\nBy setting α and β nonzero, the density function is eccentricity-dependent. Setting both α and β to\nzero will make n(i) constant which recovers the uniform density spectral clustering. We vary the\nparameters α and β to generate different sets of clusters for the foveated CIFAR-10 dataset and run\nURLOST using each of these sets of clusters. Results in Table 4 validate that the model performs\nbetter with density adjusted clustering. The intuitive explanation is that by adjusting the values of α\nand β, we can make each cluster carry similar amounts of information (refer to Appendix A.3.). A\nbalanced distribution of information across clusters enhances the model’s ability to learn meaningful\nrepresentations. Without this balance, masking a low-information cluster makes the prediction task\ntrivial, while masking a high-information cluster will make the prediction task too difficult. In either\nscenario, the model’s ability to learn effective representations is compromised.\n1p(i) need to normalized to be the density function but this wouldn’t influence the optimization problem\n8\n5\nAdditional related works\nSeveral interconnected pursuits are linked to this work, and we will briefly address them here:\nTopology in biological visual signal. 2-D topology of natural images is strong prior that requires\nmany bits to encode [13; 2]. Such 2-D topology is encoded in the natural image statistic [55; 28],\nwhich can be recovered [53]. Optic and neural circuits in the retina result in a more irregular 2-\nD topology than the natural image, which can still be simulated [52; 46; 47; 45; 61; 30]. This\ninformation is further processed by the primary visual cortex. Evidence of retinotopy suggests the\nlow-dimensional geometry of visual input from retina is encoded by the neuron in primary visual\ncortex [40; 20; 27; 19; 65; 48]. These evidences suggest we can recover the topology using signal\nfrom retinal ganglion cell and V1 neurons.\nEvidence of self-organizing mechanism in the brain. In computational neuroscience, many works\nuse the self-organizing maps (SOM) as a computational model for V1 functional organization:\n[16; 60; 1; 17; 39; 31]. In other words, this idea of self-organizing is likely a principle govern-\ning how the brain performs computations. Even though V1 functional organizations are present at\nbirth, numerous studies also indicate that the brain’s self-organizing mechanisms continue after full\ndevelopment [22; 54; 29].\nLearning with signal on non-euclidean geometry. In recent years, researchers from the machine\nlearning community have made efforts to consider geometries and special structures beyond classic\nimages, text, and feature vectors. [33] treats an image as a set of points but depends on the 2D co-\nordinates. The geometric deep learning community tries to generalize convolution neural networks\nbeyond the Euclidean domain [3; 37; 11; 21]. Recent research also explores adapting the Trans-\nformer to domains beyond Euclidean spaces [10; 9]. However, none of them has tried to tackle the\nissue when the data has no explicit topology or stationarity, which is the focus of URLOST.\nSelf-supervised learning. Self-supervised learning (SSL) has made substantial progress in recent\nyears. Different SSL method is designed for each modality, for example: predicting the masked/next\ntoken in NLP[14; 50; 4], solving pre-text tasks, predicting masked patches, or building contrastive\nimage pairs in computer vision [36; 25; 68; 6; 24; 70]. These SSL methods have demonstrated\ndescent scalability with a vast amount of unlabeled data and have shown their power by achieving\nperformance on par with or even surpassing supervised methods. They have also exhibited huge\npotential in cross-modal learning, such as the CLIP by [51]. However, we argue that these SSL\nmethods are all built upon specific modalities with explicit topology and stationarity which URLOST\ngoes beyond.\n6\nDiscussion\nThe success of most current state-of-the-art self-supervised representation learning methods relies on\nthe assumption that the data has known stationarity and domain topology, such as the grid-like RGB\nimages and time sequences. However, biological vision systems have evolved to deal with signals\nwith less regularity. In this work, we explore unsupervised representation learning under a more\ngeneral assumption, where the stationarity and topology of the data are unknown to the machine\nlearning model and its designers. We argue that this is a general and realistic assumption for high-\ndimensional data in modalities of natural science. We propose a novel unsupervised representation\nlearning method that works under this assumption and demonstrates our method’s effectiveness and\ngenerality on a synthetic biological vision dataset and two datasets from natural science that have\ndiverse modalities. We also perform a step-by-step ablation study to show the effectiveness of the\nnovel components in our model.\nDuring experiments, we found that density adjusted spectral clustering is crucial for the quality of\nrepresentation learning. How to adjust the density and obtain a balanced clustering for any given\ndata or even learning the clusters end-to-end with the representation via back-propagation is worth\nfuture investigation. Moreover, our current self-organizing layer is still simple though it shows\neffective performance. Extending it to a more sophisticated design and potentially incorporating it\nwith various neural network architectures is also worth future exploration.\nIn summary, our method offers a handy and general unsupervised learning tool when dealing with\nhigh-dimensional data of arbitrary modality with unknown stationarity and topology, particularly\n9\ncommon in the field of natural sciences, where many present strong unsupervised learning baselines\ncannot directly adapt. We hope it can provide inspiration for work in related fields.\nReferences\n[1] Harry G Barrow, Alistair J Bray, and Julian ML Budd. A self-organizing model of “color blob”\nformation. Neural Computation, 8(7):1427–1448, 1996.\n[2] Yoshua Bengio, Yann LeCun, et al. Scaling learning algorithms towards ai. Large-scale kernel\nmachines, 34(5):1–41, 2007.\n[3] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.\nGeometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34\n(4):18–42, 2017.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-\nels are few-shot learners. Advances in neural information processing systems, 33:1877–1901,\n2020.\n[5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In International conference on machine learn-\ning, pp. 1691–1703. PMLR, 2020.\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations.\nIn International conference on machine\nlearning, pp. 1597–1607. PMLR, 2020.\n[7] Yubei Chen, Dylan Paiton, and Bruno Olshausen. The sparse manifold transform. Advances\nin neural information processing systems, 31, 2018.\n[8] Brian Cheung, Eric Weiss, and Bruno A. Olshausen. Emergence of foveal image sampling\nfrom learning to attend in visual scenes. In The Fifth International Conference on Learning\nRepresentations, 2016.\n[9] Sungmin Cho, Raehyuk Jung, and Junseok Kwon.\nSpherical transformer.\narXiv preprint\narXiv:2202.04942, 2022.\n[10] Simon Dahan, Logan ZJ Williams, Abdulah Fawaz, Daniel Rueckert, and Emma C Robinson.\nSurface analysis with vision transformers. arXiv preprint arXiv:2205.15836, 2022.\n[11] Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks\non graphs with fast localized spectral filtering. Advances in neural information processing\nsystems, 29, 2016.\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248–255. Ieee, 2009.\n[13] J.S. Denker and Y. leCun. Natural versus ”universal” probability, complexity, and entropy. In\nWorkshop on Physics and Computation, pp. 122–127, 1992. doi: 10.1109/PHYCMP.1992.\n615508.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In North American Chapter of the\nAssociation for Computational Linguistics, 2019.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learning Representations, 2021.\n[16] Richard Durbin and Graeme Mitchison. A dimension reduction framework for understanding\ncortical maps. Nature, 343(6259):644–647, 1990. doi: 10.1038/343644a0.\n10\n[17] Richard Durbin and Graeme Mitchison. A dimension reduction framework for understanding\ncortical maps. Nature, 343(6259):644–647, 1990.\n[18] Cameron T Ellis, Tristan S Yates, Lena J Skalaban, Vikranth R Bejjanki, Michael J Arcaro, and\nNicholas B Turk-Browne. Retinotopic organization of visual cortex in human infants. Neuron,\n109(16):2616–2626, 2021.\n[19] Stephen A Engel, Gary H Glover, and Brian A Wandell. Retinotopic organization in human\nvisual cortex and the spatial precision of functional mri. Cerebral cortex (New York, NY: 1991),\n7(2):181–192, 1997.\n[20] Daniel J Felleman and David C Van Essen. Distributed hierarchical processing in the primate\ncerebral cortex. Cerebral cortex (New York, NY: 1991), 1(1):1–47, 1991.\n[21] Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich M¨uller. Splinecnn: Fast ge-\nometric deep learning with continuous b-spline kernels.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 869–877, 2018.\n[22] Charles D Gilbert and Wu Li. Adult visual cortical plasticity. Neuron, 75(2):250–264, 2012.\n[23] Marzieh Haghighi, Juan C. Caicedo, Beth A. Cimini, Anne E. Carpenter, and Shantanu\nSingh.\nHigh-dimensional gene expression and morphology profiles of cells across 28,000\ngenetic and chemical perturbations. Nature Methods, 19(12):1550–1557, 2022. doi: 10.1038/\ns41592-022-01667-0.\n[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738, 2020.\n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n[26] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew\nBotvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con-\ncepts with a constrained variational framework. In International conference on learning rep-\nresentations, 2016.\n[27] David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional\narchitecture in the cat’s visual cortex. The Journal of physiology, 160(1):106, 1962.\n[28] Aapo Hyv¨arinen, Jarmo Hurri, and Patrick O Hoyer. Natural image statistics: A probabilistic\napproach to early computational vision., volume 39. Springer Science & Business Media,\n2009.\n[29] Yaseen A Jamal and Daniel D Dilks. Rapid topographic reorganization in adult human primary\nvisual cortex (v1) during noninvasive and reversible deprivation. Proceedings of the National\nAcademy of Sciences, 117(20):11059–11067, 2020.\n[30] Aditya Jonnalagadda, William Yang Wang, BS Manjunath, and Miguel P Eckstein. Foveater:\nFoveated transformer for image classification. arXiv preprint arXiv:2105.14173, 2021.\n[31] Teuvo Kohonen. Self-organized formation of topologically correct feature maps. Biological\ncybernetics, 43(1):59–69, 1982.\n[32] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www.\ncs. toronto. edu/kriz/cifar. html, 55(5), 2014.\n[33] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu, and Yun Fu. Image as set of\npoints. In The Eleventh International Conference on Learning Representations, 2023.\n[34] Rahil Mehrizi, Arash Mehrjou, Maryana Alegro, Yi Zhao, Benedetta Carbone, Carl Fishwick,\nJohanna Vappiani, Jing Bi, Siobhan Sanford, Hakan Keles, et al. Multi-omics prediction from\nhigh-content cellular imaging with deep learning. arXiv preprint arXiv:2306.09391, 2023.\n11\n[35] Ronald L Meyer. Tetrodotoxin inhibits the formation of refined retinotopography in goldfish.\nDevelopmental Brain Research, 6(3):293–298, 1983.\n[36] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant rep-\nresentations. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 6707–6717, 2020.\n[37] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and\nMichael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model\ncnns. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\n2016.\n[38] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algo-\nrithm. Advances in neural information processing systems, 14, 2001.\n[39] Klaus Obermayer, Helge Ritter, and Klaus Schulten. A principle for the formation of the spatial\nstructure of cortical feature maps. Proceedings of the National Academy of Sciences, 87(21):\n8345–8349, 1990.\n[40] Haluk Ogmen and Michael H Herzog. The geometry of visual perception: Retinotopic and\nnonretinotopic representations in the human visual system. Proceedings of the IEEE, 98(3):\n479–492, 2010.\n[41] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by\nlearning a sparse code for natural images. Nature, 381(6583):607–609, 1996.\n[42] Bruno A Olshausen and David J Field.\nHow close are we to understanding v1?\nNeural\ncomputation, 17(8):1665–1699, 2005.\n[43] Bruno A Olshausen and David J Field. What is the other 85 percent of v1 doing. L. van\nHemmen, & T. Sejnowski (Eds.), 23:182–211, 2006.\n[44] Marius Pachitariu, Carsen Stringer, Sylvia Schr¨oder, Mario Dipoppa, L Federico Rossi, Matteo\nCarandini, and Kenneth D Harris. Suite2p: beyond 10,000 neurons with standard two-photon\nmicroscopy. BioRxiv, pp. 061507, 2016.\n[45] Eli Peli, Jian Yang, and Robert B Goldstein. Image invariance with changes in size: The role\nof peripheral contrast thresholds. JOSA A, 8(11):1762–1774, 1991.\n[46] Jeffrey S Perry and Wilson S Geisler. Gaze-contingent real-time simulation of arbitrary visual\nfields. In Human vision and electronic imaging VII, volume 4662, pp. 57–69. SPIE, 2002.\n[47] JS Pointer and RF Hess. The contrast sensitivity gradient across the human visual field: With\nemphasis on the low spatial frequency range. Vision research, 29(9):1133–1151, 1989.\n[48] Jonathan R Polimeni, Bruce Fischl, Douglas N Greve, and Lawrence L Wald. Laminar analysis\nof 7 t bold using an imposed spatial activation pattern in human v1. Neuroimage, 52(4):1334–\n1346, 2010.\n[49] Daniel Probst and Jean-Louis Reymond. Visualization of very large high-dimensional data\nsets as minimum spanning trees. Journal of Cheminformatics, 12(1):12, 2020. doi: 10.1186/\ns13321-020-0416-x.\n[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable\nvisual models from natural language supervision. In International conference on machine\nlearning, pp. 8748–8763. PMLR, 2021.\n[52] Austin Roorda and David R Williams. The arrangement of the three cone classes in the living\nhuman eye. Nature, 397(6719):520–522, 1999.\n12\n[53] Nicolas Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, and Bal´azs K´egl. Learning the\n2-d topology of images. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in\nNeural Information Processing Systems, volume 20. Curran Associates, Inc., 2007.\n[54] Rosanna P Sammons and Tara Keck. Adult plasticity and cortical reorganization after periph-\neral lesions. Current Opinion in Neurobiology, 35:136–141, 2015.\n[55] Eero P Simoncelli and Bruno A Olshausen. Natural image statistics and neural representation.\nAnnual review of neuroscience, 24(1):1193–1216, 2001.\n[56] X Yu Stella and Jianbo Shi. Multiclass spectral clustering. In Computer Vision, IEEE Interna-\ntional Conference on, volume 2, pp. 313–313. IEEE Computer Society, 2003.\n[57] Carsen Stringer, Marius Pachitariu, Matteo Carandini, and Kenneth Harris. Recordings of\n10,000 neurons in visual cortex in response to 2,800 natural images. Figshare Repos, 2018.\n[58] Carsen Stringer, Marius Pachitariu, Nicholas Steinmetz, Matteo Carandini, and Kenneth D\nHarris. High-dimensional geometry of population responses in visual cortex. Nature, 571\n(7765):361–365, July 2019.\n[59] Carsen Stringer, Michalis Michaelos, Dmitri Tsyboulski, Sarah E. Lindo, and Marius Pachi-\ntariu. High-precision coding in visual cortex. Cell, 184(10):2767–2778.e15, 2021. ISSN\n0092-8674. doi: https://doi.org/10.1016/j.cell.2021.03.042.\n[60] Nicholas V. Swindale and Hans-Ulrich Bauer. Application of kohonen’s self-organizing fea-\nture map algorithm to cortical maps of orientation and direction preference. Proceedings:\nBiological Sciences, 265(1398):827–838, 1998. ISSN 09628452.\n[61] Larry N Thibos. Acuity perimetry and the sampling theory of visual resolution. Optometry\nand vision science: official publication of the American Academy of Optometry, 75(6):399–\n406, 1998.\n[62] Katarzyna Tomczak, Patrycja Czerwi´nska, and Maciej Wiznerowicz.\nReview the can-\ncer genome atlas (tcga): an immeasurable source of knowledge.\nContemporary Oncol-\nogy/Wsp´ołczesna Onkologia, 2015(1):68–77, 2015.\n[63] David C Van Essen, Charles H Anderson, and Daniel J Felleman. Information processing in\nthe primate visual system: an integrated systems perspective. Science, 255(5043):419–423,\n1992.\n[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[65] Brian A Wandell, Serge O Dumoulin, and Alyssa A Brewer. Visual field maps in human cortex.\nNeuron, 56(2):366–383, 2007.\n[66] John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger,\nKyle Ellrott, Ilya Shmulevich, Chris Sander, and Joshua M Stuart. The cancer genome atlas\npan-cancer analysis project. Nature genetics, 45(10):1113–1120, 2013.\n[67] Rachel OL Wong. Retinal waves and visual system development. Annual review of neuro-\nscience, 22(1):29–47, 1999.\n[68] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via\nnon-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2018.\n[69] Youzheng Xu, Yixin Xu, Chun Wang, Baoguo Xia, Qingling Mu, Shaohong Luan, and Jun\nFan. Mining tcga database for gene expression in ovarian serous cystadenocarcinoma mi-\ncroenvironment. PeerJ, 9:e11375, 2021.\n[70] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-\nsupervised learning via redundancy reduction. In International Conference on Machine Learn-\ning, pp. 12310–12320. PMLR, 2021.\n13\n[71] Xiaoyu Zhang, Jingqing Zhang, Kai Sun, Xian Yang, Chengliang Dai, and Yike Guo. Inte-\ngrated multi-omics analysis using variational autoencoders: Application to pan-cancer classi-\nfication. In 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),\npp. 765–769, 2019. doi: 10.1109/BIBM47256.2019.8983228.\n[72] Xiaoyu Zhang, Yuting Xing, Kai Sun, and Yike Guo. Omiembed: A unified multi-task deep\nlearning framework for multi-omics data. Cancers, 13(12), 2021. ISSN 2072-6694. doi:\n10.3390/cancers13123047.\nA\nAppendix\nA.1\nSpectral clustering algorithm\nGiven a high dimensional dataset S ∈Rn×m, Let Si be ith column of S, which represents the\nith dimension of the signal. We create probability mass functions P(Si) and P(Sj) and the joint\ndistribution P(Si, Sj) for Si and Sj using histogram. Let the number of bins be K. Then we\nmeasure the mutual information between P(Si) and P(Sj) as:\nI(Si; Sj) =\nK\nX\nl=1\nK\nX\nk=1\nP(Si, Sj)[l, k] log2\n\u0012\nP(Si, Sj)[l, k]\nP(Si)[l]P(Sj)[k])\n\u0013\nLet Aij = I(Xi; Xj) be the affinity matrix, p(i) be the density function defined in 4. We follow the\nsteps from [38] to perform spectral clustering with a modification to adjust the density:\n1. Define D to be the diagonal matrix whose (i,i)-element is the sum of A’s i-th row, P be\nthe identity matrix where Pii = p(i). Construct the matrix L = P\n1\n2 D−1\n2 AD−1\n2 P\n1\n2 .\n2. Find x1, x2, · · · , xk, the k largest eigenvectors of L, and form the matrix X\n=\n[x1, x2, · · · , xk] ∈Rn×k by stacking the eigenvectors in columns.\n3. Form the matrix Y from X by renormalizing each of X’s rows to have unit norms. (i.e.\nYij = Xij/(P\ni X2\nij)\n1\n2 )\n4. Treating each row of Y as a point in Rk, cluster them into k clusters via K-means or other\nalgorithms.\nSome other interpretation of spectral embedding allows one to design a specific clustering algorithm\nin step 4. For example, [56] interprets the eigenvector problem in 1 as a relaxed continuous version\nof K-way normalized cuts problem, where they only allow X to be binary, i.e. X ∈{0, 1}N×K.\nThis is an NP-hard problem. Allowing X to take on real value relaxed this problem but created a\ndegeneracy solution. Given a solution X∗and Z = D−1\n2 X∗, for any orthonormal matrix R, RZ is\nanother solution to the optimization problem 1. Thus, [56] designed an algorithm to find the optimal\northonormal matrix R that converts X∗to discrete value in {0, 1}N×K. From our experiment, [56]\nis more consistent than K-means and other clustering algorithms, so we stick to using it for our\nmodel.\nA.2\nData synthesize process\nWe followed the retina sampling approach described in [8] to achieve foveated imaging. Specifically,\neach retina ganglion cell is represented using a Gaussian kernel. The kernel is parameterized by its\ncenter, denoted as ⃗xi, and its scalar variance, σ′2\ni , i.e. N(⃗xi, σ′2\ni I), which is illustrated in Figure 5.A.\nThe response of each cell, denoted as G[i], is computed by the dot product between the pixel value\nand the corresponding discrete Gaussian kernel. This can be formulated as:\nG[i] =\nN\nX\nn\nW\nX\nm\nK(⃗xi, σ′\ni)[n, m]I[n, m]\nwhere N and W are dimensions of the image, and I represents the image pixels.\n14\nTable 4: Evaluation on foveated CIFAR-10 with varying hyperparameter for density function.\nFor each set of values of α and β, we perform density adjusted spectral clustering and run URLOST\nwith the corresponding cluster. The evaluation of each trial is provided in the table.\nbeta = 0\nbeta = 2\nalpha = 0\n82.74 %\n84.24 %\nalpha = 0.5\n84.52 %\n85.43 %\nalpha = 1.0\n83.83 %\n81.62 %\nFor foveated CIFAR-10, since the image is very low resolution, we first upsample it 3 times from\n32 × 32 to 96 × 96, then use in total of 1038 Gaussian kernels to sample from the upsampled image.\nThe location of each kernel is illustrated in Figure 5.B. The radius of the kernel scales proportionally\nto the eccentricity. Here, we use the distance from the kernel to the center to represent eccentricity.\nThe relationship between the radius of the kernel and eccentricity is shown in Figure 5.C. As men-\ntioned in the main paper, in the natural retina, retinal ganglion cell density decreases linearly with\neccentricity, which makes the fovea much denser than the peripheral, unlike the simulated lattice we\ncreated. The size of the kernel should scale linearly with respect to eccentricity as well. However,\nfor the low-resolution CIFAR-10 dataset, we reduce the simulated fovea’s density to prevent redun-\ndant sampling. In this case, we pick the exponential scale for the relationship between the size of the\nkernel and eccentricity so the kernel visually covers the whole visual field. We also implemented a\nconvolution version of the Gaussian sampling kernel to speed up data loading.\nFigure 5: Foveated retinal sampling (A) Illustration of a Guassian kernel shown in [8]. Diagram\nof single kernel filter parameterized by a mean µ′ and variance σ′. (B) the location of each Gaussian\nkernel is summarized as a point with 2D coordinate µ′. In total, the locations of 1038 Gaussian\nkernels are plotted. (C) The relationship between eccentricity (distance of the kernel to the center)\nand radius of the kernel is shown.\nA.3\nDensity adjusted spectral clustering on foveated CIFAR10 dataset\nWe provide further intuition and visualization on why density adjusted spectral clustering allows the\nmodel to learn a better representation on the foveated CIFAR-10 dataset.\nAs shown in Figure 5, the kernel at the center is much smaller in size than the kernel in the peripheral.\nThis makes the kernel at the center more accurate but smaller, which means it summarizes less\ninformation. Spectral clustering with constant density will make each cluster have a similar number\nof elements in them. Since the kernel in the center is smaller, the cluster in the center will be\nvisually smaller, than the cluster in the peripheral. The effect is shown in Figure 6. Moreover, since\nwe’re upsampling an already low-resolution image (CIFAR-10 image), even though the kernel at the\ncenter is more accurate, we’re not getting more information. There, to make sure each cluster has\nsimilar information, the clusters in the center need to have more elements than the clusters in the\nperipheral. In order to make the clusters at the center have more elements, we need to weight the\nclusters in the center more with the density function. Since the sampling kernels at the center have\nsmall eccentricity and are more correlated to their neighbor, increasing α and β will make sampling\nkernels at the center have higher density, which makes the cluster at the center larger. This is why\n15\nURLOST with density adjusted spectral clustering performs better than URLOST with constant\ndensity spectral clustering, which is shown in Table 4. Meanwhile, setting α and β too large will\nalso hurt the model’s performance because it creates clusters that are too unbalanced.\nFigure 6: Effect of density adjusted clustering. Eccentricity-based sampling lattice. The center of\nthe sampling lattice has more pixels which means higher resolution compared to the peripheral. (A)\nResult of density adjusted spectral clustering (α = 0.5, β = 2). Clusters in the center have more\nelements than clusters in the peripheral. But clusters look more visually similar in size than B. (B)\nResult of uniform density spectral clustering (α = 0, β = 0). Each cluster has a similar number of\nelements in them but the clusters in the center are much smaller than the clusters in the periphery.\nA.4\nself-organizing layer learns inverse permutation\nFor locally-permuted CIFAR-10, we divide each image into patches and locally permute all the\npatches. The i-th image patch is denoted by x(i), and its permuted version, permuted by the permu-\ntation matrix E(i), is expressed as E(i)x(i). We use linear layers to parameterize the self-organizing\nlayers. Let g(x, W (i)) = W (i)x denotes the ith element of the self-organizing layer. We’re provid-\ning the proof for the statement related to the visual evidence shown in Section 4.1\nStatement: If the self-organizing layer effectively aligns the input sequence, then E(i)T w(i) should\nexhibit visual similarities.\nProof: we first need to formally define what it means for the self-organizing layer to effectively\nalign the input sequence. Let ek denote the kth natural basis (one-hot vector at position k), which\nrepresents the pixel basis at location k. Permutation matrix E(i) will send kth pixel to some location\naccordingly. Mathematically, if the projection layer effectively aligns the input sequence, it means\ng(E(j)ek, W (j)) = g(E(i)ek, W (i)) for all i, j, k. We can further expand this property to get the\nfollowing two equations:\ng(E(i)ek, W (i)) = W (i)E(i)ek\ng(E(j)ek, W (j)) = W (j)E(j)ek\nfor all i, j, k. Since the above equation holds for all ek, by linearity and the property of permutation\nmatrix, we have:\nW (i)E(i) = W (j)E(j)\nE(i)T W (i) = E(j)T W (j)\nThis implies E(i)T w(i) should exhibit visual similarities for all i.\n16\nA.5\nVisualizing the weight of self-organizing\nAs explained in the previous section (Appendix A.4) and visualized in Figure 7, we can visualize\nthe weights of the learned self-organizing layer when trained on the locally-permuted CIFAR-10\ndataset. If we apply the corresponding inverse permutation E(i)T to its learned filter W (i) at position\ni, the pattern should show similarity across all position i. This is because the model is trying to\nalign all the input clusters. We have shown this is the case when the model converges to a good\nrepresentation. On the other hand, what if we visualize the weight E(i)T W (i) as training goes\non? If the model learns to align the clusters as it is trained for the mask prediction task, E(i)T W (i)\nshould become more and more consistent as training goes on. We show this visualization in Figure 7,\nwhich confirms our hypothesis. As training goes on, the pattern E(i)T W (i) becomes more and more\nvisually similar, which implies the model learns to gradually learn to align the input clusters.\nFigure 7: Visualize the weight of the self-organizing layer after applying inverse permutation.\nA snapshot of E(i)T W (i) is shown at different training epoch. The number of epochs is shown on\nthe top row. Each figure shows one column of the weight of the self-organizing layer, at different\npositions, i.e. W (1)\n:,k , where k is the column number and i is the position index. In total, 9 columns\nare shown.\n17\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2023-10-06",
  "updated": "2023-10-06"
}