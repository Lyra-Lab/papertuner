{
  "id": "http://arxiv.org/abs/2310.05695v1",
  "title": "Hierarchical Reinforcement Learning for Temporal Pattern Prediction",
  "authors": [
    "Faith Johnson",
    "Kristin Dana"
  ],
  "abstract": "In this work, we explore the use of hierarchical reinforcement learning (HRL)\nfor the task of temporal sequence prediction. Using a combination of deep\nlearning and HRL, we develop a stock agent to predict temporal price sequences\nfrom historical stock price data and a vehicle agent to predict steering angles\nfrom first person, dash cam images. Our results in both domains indicate that a\ntype of HRL, called feudal reinforcement learning, provides significant\nimprovements to training speed and stability and prediction accuracy over\nstandard RL. A key component to this success is the multi-resolution structure\nthat introduces both temporal and spatial abstraction into the network\nhierarchy.",
  "text": "Hierarchical Reinforcement Learning for Temporal Pattern Prediction\nFaith Johnson\nRutgers University\nfaith.johnson@rutgers.edu\nKristin Dana\nRutgers University\nkristin.dana@rutgers.edu\nAbstract\nIn this work, we explore the use of hierarchical reinforce-\nment learning (HRL) for the task of temporal sequence pre-\ndiction. Using a combination of deep learning and HRL, we\ndevelop a stock agent to predict temporal price sequences\nfrom historical stock price data and a vehicle agent to pre-\ndict steering angles from first person, dash cam images.\nOur results in both domains indicate that a type of HRL,\ncalled feudal reinforcement learning, provides significant\nimprovements to training speed and stability and prediction\naccuracy over standard RL. A key component to this success\nis the multi-resolution structure that introduces both tempo-\nral and spatial abstraction into the network hierarchy.\n1. Introduction\nReinforcement learning (RL) has made major strides\nover the past decade, from learning to play Atari games [8]\nto mastering chess and Go [10]. However, RL algorithms\ntend to work in a specific, controlled environment and are\noften difficult to train. In response to this brittleness, hier-\narchical reinforcement learning (HRL) is growing in popu-\nlarity.\nWe combine deep learning and HRL for temporal se-\nquence prediction in two application domains where pub-\nlicly available data is abundant. First, we develop stock\nagents to execute trades in a market environment.\nThe\ntraining data consists of historical stock prices from 1995\nto 2018.\nSecond, we develop a vehicle agent to predict\nsteering angles given visual input.\nWe use the Udacity\ndataset[11] as training data, which consists of five videos\nwith a total duration of 1694 seconds.\nIn HRL, a manager network operates at a lower temporal\nresolution and produces goals that it passes to the worker\nnetwork. The worker network uses this goal to produce a\npolicy over micro-actions at a higher temporal resolution\nthan the manager[12]. Within the stock market, there are\ntwo natural hierarchies. The first hierarchy involves tempo-\nral scales. A trader can consider how a stock’s price fluc-\ntuates over the course of an hour, but also over the course\nof a week, month, or year. The second hierarchy is the sep-\naration of the different market sectors, each containing a\nmultitude of stocks. In order to trade in the market effec-\ntively, stock brokers must consider both the relationships\nbetween sectors and the relationships between the stocks in\neach sector.\nIn the same vein, the task of autonomous navigation is\ncomplicated because, at all times, human drivers have two\nlevels of things they are paying attention to. The first level\nis on a fine grain: don’t immediately crash the vehicle by\nhitting obstacles. The second level is on a coarser grain:\nplan actions a few steps ahead to keep the vehicle going\nin the correct direction as efficiently as possible. In both\ndomains, financial and vehicluar, we implement agents with\nboth RL and HRL. We show that HRL provides improved\ntraining stability and prediction performance.\n2. Methods\n2.1. LSTM Stock Predictions\nFirst, we set up a baseline for future result comparison\nusing a simple LSTM network. Using stock market data\ngathered from Kaggle[7], we predict the closed price for\na single day given the open price of that day. We build\na simple LSTM model in Keras[2] with ten neurons and\na ReLU activation function followed by a fully connected\nlayer. This network is trained for ten epochs using the mean\nsquared error loss function and an adam optimizer.\nNext, we predict a sequence of open prices for a particu-\nlar stock given a sequence of previous open prices. For this,\nwe use a slightly larger LSTM network with three layers of\nLSTMs, each with ten neurons and ReLU activation func-\ntions followed by a fully connected layer. The loss function\nand optimizer for this experiment are also mean squared er-\nror and Adam respectively. However, this network is trained\nfor twenty epochs. We conduct this experiment with several\nsequence pairings: 1 previous price to predict the next price,\n3 previous prices to predict the next 3 prices, and 5 previous\nprices to predict the next 5 prices.\nFinally, we implement a reinforcement learning stock\nagent to predict open stock prices by learning a multiplier to\n1\narXiv:2310.05695v1  [cs.LG]  9 Oct 2023\ntransform the previous open price to the next open price. It\ntakes a sequence of historical open prices for a single stock\nas input and passes them through several layers of LSTMs\nwith ReLU activation functions and a fully connected layer.\nThe new price is computed by multiplying the current open\nprice by the output to produce the predicted open price for\nthe next day.\n2.2. Stock Environment\nAfter the multiplier agent, we rethink our approach to\nstock price prediction by moving away from predicting the\nprice itself, and thus away from computing the answer to a\nregression problem. The next set of experiments involves\nexecuting trades in a stock market environment with the\ngoal of doubling the value in a given portfolio. The ratio-\nnale behind this change is that reinforcement learning ex-\ncels at learning a policy over a given set of actions. In the\nregression problem of learning a multiplier, the action space\nis essentially infinite. In addition, the state space is infinite\nbecause it consists of all possible stock prices. Learning a\npolicy over an infinite set of possibilities is almost impossi-\nble.\nTo create our stock environment for these new reinforce-\nment learning agents, we use Quadnl[9] to collect mar-\nket information for a small subset of stocks from six sec-\ntors. The six stock sectors are technology, energy, finance,\nhealthcare, utilities, and transportation. The goal is to have\na diverse selection of stocks and sectors from the market in\norder for the agent to be able to glean the relationships be-\ntween the sectors along with the relationship between stocks\nwithin each sector.\nWe define the action space to consist of three actions:\nbuy, sell, and hold. All agents buy or sell only one stock\nper action, unless otherwise stated. If an agent does not\nhave enough money to execute the buy action, it is forced\nto hold. The same is true if it does not have enough shares\nto execute the sell action. The environment keeps track of\nan agent’s current balance of cash and portfolio value, as\nwell as giving the agents a reward for their actions, which\nis generally the change in the agent’s total portfolio value.\n2.3. Hard Coded Stock Agent\nThe baseline for performance in the stock environment\ncomes from a hard coded stock agent whose aim is to double\nthe value of a given portfolio. First, it defines two thresh-\nolds, a selling threshold and a buying threshold.\nWhen\ntwo consecutive open prices differ by less than the sell-\ning threshold, the agent decides to sell shares in the stock.\nWhen the two prices differ by more than the buying thresh-\nold, the agent decides to buy a share in the stock. The idea is\nthat the buying threshold is positive and the selling thresh-\nold is negative so that the agent will sell when the price goes\ndown and buy when the price goes up. When the price dif-\nference lies between the two thresholds, the agent takes the\nhold action.\n2.4. Reinforcement Learning Stock Agents\n2.4.1\nQ Learning Agent\nFor the Q learning agent, the state space is limited to the\ncombinations of whether or not the price of a stock has gone\nup or down and whether or not the agent currently possesses\nshares in said stock. In q learning, the agent keeps track of\nthe policy over actions and states using a q table. The q\ntable is indexed by the actions and the states, as in Table 1,\nand is initially filled with zeros. As an agent takes actions in\nthe environment, it receives a reward that it uses to update\nthe table to reflect the utility of each action given a certain\nstate. An agent decides which action to take by referencing\nthe portion of this q table corresponding to its current state\nand choosing the action associated with the highest q value.\nBuy\nHold\nSell\nPrice Increases & Have Shares\nPrice Increases & Have No Shares\nPrice Decreases & Have Shares\nPrice Decreases & Have No Shares\nTable 1. Example q table for the q learning stock agent. It is in-\ndexed by the three actions (buy, hold, sell) and the combination of\nprice fluctuation and share possession.\n2.4.2\nDeep Q Network (DQN) Agent\nThis stock agent builds upon the same ideas as the previous\nq learning agent, but it chooses it’s actions differently. In-\nstead of using the reward from the environment to update\nthe q table, it uses a deep q network, or DQN, to approx-\nimate the q value of a certain action given a state. This\nnetwork is made up of three LSTMs with ReLU activation\nfunctions in sequence, followed by a fully connected layer.\nThe first LSTM has 32 hidden layers, and the last two have\n64 layers.\nIn the case of this network, the state is the previous three\nopen prices for the stocks in each of the sectors.\nHow-\never, the action space remains the same. With the q learn-\ning agent, having an infinite state space would not be ideal\nfor optimal policy convergence, but the DQN is still able to\nconverge on a solution.\nThe reward from the environment plays a role in the loss\nback-propagation of the network. For each action taken by\nthe agent, a tuple containing the initial state, s0, the final\nstate, s, the action taken, a, and the corresponding reward,\nr, is saved in a replay buffer. During training, a random\ntuple is sampled from this buffer, and the loss, which in this\ncase is the reward, is back-propagated through the network\nas in [8].\n2\n2.5. Feudal Reinforcement Learning\nIn feudal reinforcement learning, the manager network\noperates at a lower temporal resolution than the worker net-\nwork. It receives state input from the environment and com-\nmunicates with the worker network through a goal vector.\nThis goal vector encapsulates a temporally extended action\nthat the manager thinks will receive the highest reward from\nthe environment. The worker executes atomic actions in the\nenvironment based on this goal vector and its own state in-\nformation. This process of manager/worker communication\nthrough temporal abstraction helps to break down a problem\ninto more easily digestible pieces.\nTo explain the concept of temporal abstraction further,\ntake the case of an agent attempting to leave a room through\na door. When a person thinks of completing this action, they\ndon’t do it at the low level of straight, straight, left, straight,\nright, etc. In other words, they do not consciously think\nof each atomic action required to exit the room. Instead,\nthey think in terms of temporal abstraction. Find the door.\nApproach it. Pass through it. Each of those actions encap-\nsulates multiple atomic actions that need to be executed in\na specific order for the agent to complete the task.\nFor a feudal network to solve the room example, the\nmanager would create goal vectors for the “find the door”,\n“approach it”, and “pass through it” operations. Then, the\nworker would only have to focus on executing atomic ac-\ntions to complete one of these smaller tasks at a time, which\nis much simpler than the original task of exiting the room\nas a whole. This makes it easier to generate an ideal pol-\nicy. Additionally, the idea of temporal abstraction can be\napplied to space. Incorporating different spatial resolutions\ninto a feudal network can break down problems into spatial\nabstractions which make them easier to solve in the same\nway.\n2.6. Maze Environment\nTo test the performance of feudal reinforcement learn-\ning, we use a maze environment as proposed in Dayan et\nal.[3]. In this environment, there are multiple levels of the\nsame maze, each at a lower spatial resolution than the pre-\nvious level. The agent at the highest spatial resolution is\nthe worker, who receives goal vectors from the agent at the\nnext lowest resolution, who is its manager. This manager\nbecomes the worker for the agent at the next lowest reso-\nlution, and so on, until you reach the level with the lowest\nspatial resolution, where the ultimate manager resides. For\nexample, the worker on the level with the highest spatial\nresolution will operate in a 16x16 grid and have a man-\nager who operates in an 8x8 grid. This manager will be\nthe worker for the agent in the 4x4 grid, etc., until you get\nto the final manager in the 1x1 grid.\nWe create this maze by editing the gym-maze[1] github\nrepository code. In our maze, there are only two levels.\n(a) Manager View\n(b) Worker View\nFigure 1. (a) Manager’s 2x2 view of the maze. (b) Worker’s corre-\nsponding 4x4 view of the same maze.\nThe worker operates in a 4x4 version of the maze, and the\nmanager operates in a 2x2 version, as in Figure 1. Each\nsquare in the manager’s 2x2 rendition of the maze corre-\nsponds to a 2x2 section of the worker’s maze. We omit the\n1x1 manager from the Dayan et al. experiment because it is\ncomputationally irrelevant for this task. The state space of\neach agent is comprised of each square in the grid of their\nrespective maze resolutions. The objective of the agents is\nto reach some goal square in the maze. This goal is mapped\nto the same equivalent location in all maze levels and can\nbe specified at run-time. The action space is comprised of\nmoving north/south/east/west or declaring that the goal is\nthe current space. There is a base reward of\n−0.1\nXdim ∗Ydim\napplied to every movement for all agents, whereXdim and\nYdim are the x and y dimensions of the maze. However, if\nan agent finds the goal, it receives a reward of 1.\n2.7. Maze Agents\nWe test several agents in our maze environment, starting\nwith a reinforcement learning model as a baseline, before\nmoving to a feudal reinforcement learning implementation.\n2.7.1\nQ Learning Agent\nThe first agent we built for the maze environment uses q\nlearning to navigate a single, 4x4 level of the maze in search\nof a goal square. When it reaches this goal, the experiment\nends. The q table of the agent is the same size as the maze\nwith each square in the maze corresponding to one entry in\nthe table. The values in this table are updated based on the\nreward received from the environment.\n2.7.2\nFeudal Q Learning Agent\nThe feudal network solves the maze using q learning as\nwell.\nThe manager network receives it’s location in the\n3\nmaze as it’s current state. It uses this to choose and exe-\ncute the best action from it’s q table. The manager is able to\nmove in any of the four directions, and this direction is the\nbasis for the goal vector that tells the worker what quadrant\nto move to. If the manager declares that the goal is in its\ncurrent space, it is telling the worker that it should look for\nits own goal in a specific quadrant of the maze. When the\nworker receives a goal vector from the manager, it moves\nto the specified quadrant and waits for more instructions. If\nit is indicated that the worker should look for the goal in a\nquadrant, it continues to move until the goal is found.\nBoth the manager and the worker receive a base nega-\ntive reward for every action that doesn’t result in finding the\ngoal. In this way, the spaces furthest away from the goal\nwill have a more negative q value than those closest to the\ngoal. In addition, the manager receives a negative reward\nif the worker finds the goal space without following the in-\nstructions from the goal vector. While the individual reward\nvalues resulting from exploring the maze may be the same,\nthe manager and worker do not receive the same reward sig-\nnals. The worker takes many more steps in the environment\ndue to the difference in spatial resolution, so it will receive a\nreward more often than the manager. In this way, the spatial\nabstraction of the maze results in a temporal abstraction of\nthe reward signals.\n2.8. Feudal Reinforcement Learning Stock Agents\nOnce we discovered the performance improvements of\nfeudal reinforcement learning, we decide to return to the\nstock market portfolio experiments with feudal reinforce-\nment learning agents.\n2.8.1\nFeudal Q Learning Agent\nOur feudal q learning stock agent operates in the same envi-\nronment as the previous reinforcement learning agents and\nhas the same goal of doubling the value of some portfolio.\nIts input is a sequence of open prices for each stock in the\nsix predetermined sectors. The q table structure and state\nspace are also the same. The main difference is the division\nof labor between the manager and worker networks.\nThe manager receives the price input from the environ-\nment and determines whether or not each of the six sec-\ntors should be traded or not. This decision is passed to the\nworker in the goal vector. The worker then decides whether\nto buy or sell the stocks in the sectors specified by the man-\nager. For each goal vector from the manager, the worker\nacts a fixed number of times to introduce temporal abstrac-\ntion to the problem in addition to the spatial abstraction al-\nready present. The reward of the manager is the overall\nportfolio value change, while the worker receives a reward\nfor the portfolio value change of each sector after each ac-\ntion it executes.\n2.8.2\nFeudal Networks with Multiple Workers\nThe feudal reinforcement learning problem can be extended\nto a vertical hierarchy with multiple managers and workers\nin sequence, as we’ve already explored, but this concept can\nalso be extended horizontally to one manager with multiple\nworkers. To this end, we have implemented two different\nexperiments, both using q learning. The first involved a\nmanager network with a set of three different worker net-\nworks, where each worker makes a different number of\ntransactions in the environment. The manager’s action is\nchoosing which of the workers will act in the environment\nat a given time.\nThe second involved a manager network with a set of\nthree workers, where each of these workers have a different\nhard coded behavior. The first buys when a stock’s price\nincreases and sells when it decreases, the second sells when\na stock’s price increases and buys when it decreases, and\nthe third executes a random action. The manager’s action\nset consists of choosing which worker will interact with the\nenvironment.\n2.9. Driving Environment\nWe also test feudal reinforcement learning in the domain\nof autonomous vehicles. For that, we use the Udacity driv-\ning dataset[11]. They provide steering angles, first-person\ndash cam images, braking, and throttle pressure data. We\naugment this dataset to increase its size and influence model\ntraining by performing several transformations on the image\nand angle data. First, we implement a horizontal flip to ef-\nfectively double the size of the dataset. For this change, we\nnegate the angles associated with the flipped images. As\nan additional option, we use the horizontal and vertical op-\ntical flow images. The horizontal optical flow image, ix,\nis obtained by convolving the image with the row vector\n[1, 0, −1], while the vertical optical flow image is obtained\nby convolving the image with the column vector\n\n\n1\n0\n−1\n\n\nFinally, all images are scaled and normalized so that their\npixel values lie in the range [−1, 1].\n2.10. Steering Angle Experiments\n2.10.1\nSteering Angle Prediction\nWe started simple, so our first task was to predict steering\nangles based on visual input. After some initial difficulty\nwith our model, we found a network[4] from the Udacity\nchallenge that accurately predicts steering angles. It has\na convolutional layer with a ReLU activation function fol-\nlowed by a dropout layer. The output of this is saved to\nuse for a skip connection later on in the network. This is\n4\nrepeated four times before the output is fed through some\nfully connected layers, also with ReLU activation functions.\nAt this point, the output and the intermediary representa-\ntions are added together, passed through an ELU layer, and\nnormalized. Then, the previous steering angle and the out-\nput of the ELU layer are passed through an LSTM. Finally,\nthe output of the LSTM is passed through a fully connected\nlayer to produce the steering angle. Note that this network\ntakes in a sequence of images as well as the previous angle\nin order to make its predictions.\n2.10.2\nSubroutine ID Prediction\nBeing able to predict steering angles is useful, but for feudal\nreinforcement learning we also need to classify the steering\nangles into their temporally abstracted categories (such as\ngo right, go left, go straight). This can be done by hand, but\nit would be a lengthy process. Instead, we take inspiration\nfrom Kumar et al.[5] to learn these subroutines, otherwise\ncalled options or macro-actions, using a neural network.\nTo do this, we jointly train two networks. The first takes\nin a sequence of angles and predicts the subroutine ID. The\nsecond takes in the subroutine ID, a sequence of images,\nand the previously predicted angle and predicts the next\nsteering angle in the sequence. A problem we encountered\nwith the steering angle prediction from the previous section\nis that it appears as if the network is simply predicting that\nthe previous angle will be the next steering angle. To cir-\ncumvent this, we give the second network the previously\npredicted angle instead of the ground truth angle. Addition-\nally during training, the sequence of angles fed into the first\nnetwork contains the angle it is trying to predict. However,\nduring testing, we only use a sequence of angles preceding\nthe angle we aim to predict in order to avoid this conflict.\n2.10.3\nt-SNE Prediction\nIdeally, we want an angle prediction network that does not\ntake in the previous steering angle at all. To accomplish this,\nwe explored using t-SNE[6] as an embedding space for our\ndriving data and as the subroutine IDs themselves. To do\nthis, we arranged the steering angle, braking, and throttle\npressure data into vectors of length ten. Then, the vectors\nfrom each category that correspond to the same time steps\nare concatenated together to make vectors of length thirty.\nThe collection of these vectors is passed through the unsu-\npervised t-SNE algorithm to create a coordinate space for\nthe driving data.\nEach vector of length thirty is given one x and y coordi-\nnate pair as illustrated in Figure 2. The greater collection of\nall of the generated points is in Figure 3. The coloring of the\npoints in this figure is hard coded. The points correspond-\ning to vectors with primarily negative steering angles are\nin blue. The points corresponding to vectors with positive\nFigure 2. Steering, braking, and throttle data are concatenated ev-\nery m time steps to make a vector of length 3m. Each 3m vector\ncorresponds to one set of coordinates in the 2D t-SNE space. The\nt-SNE coordinates act like a manager for the steering angle pre-\ndiction and operate at a lower temporal scale. In our experiments,\nm=10. t and τ correspond to the final time step for the driving data\nand t-SNE coordinates respectively.\nFigure 3. Total plot of the t-SNE coordinates for the Udacity data.\nThe colors correspond to the average sign of the angles in each\nlength 3m vector used to generate the points.\nFigure 4. K-Means clustering (k=20) of the TSNE coordinates of\nthe Udacity data with the centroids pictured in red. Not only to\ndistinct clusters form in the data, but each cluster corresponds to a\nunique action of the vehicle.\n5\nFigure 5. Example training images are shown with their corresponding t-SNE centroids. Notice that the bottom right of the figure contains\nsharp right turns. As you move upwards, the right turn gets less sharp until the vehicle begins to go straight. By the top left of the figure,\nthe vehicle is making sharp left turns.\nsteering angles are in green. The orange points correspond\nto vectors with steering angles that are relatively close to\nzero.\nOnce we have the t-SNE embedding of the data, we use\nK-Means clustering on the coordinates and take the cen-\ntroids of the clusters as our new subroutine IDs, as shown\nin Figure 4. We vary k from ten to twenty to determine\nif different numbers of clusters improve prediction perfor-\nmance. Then, we train a network to take in the centroids as\nthe subroutine ID, as well as a sequence of images, in order\nto predict the next steering angle.\nIn order to ensure that no data pertaining to the predicted\nsteering angle is used as input to this network, we use the t-\nSNE centroid corresponding to the data of the previous 3m\nsteering, braking, and throttle data as input to the network.\nTo illustrate, refer back to Figure 2. If we are predicting an\nangle from the range t ∈[2m, 3m], then the t-SNE centroid\nused for the subroutine ID input to the angle prediction net-\nwork will be the centroid at τ = 2, which was made with\nthe steering, braking, and throttle data from t ∈[m, 2m]. In\nthis way, the angle we are attempting to predict will not be\nused to compute the t-SNE centroid used as the subroutine\nID. This shift also incorporates an extra level of temporal\nabstraction into our network.\nAdditionally, we create a tool that displays the visual\ndata corresponding to the different t-SNE coordinates, al-\nlowing the user to visually inspect that neighboring points in\nthe embedding space correspond to similar driving behav-\niors. Figure 5 attempts to replicate this by showing example\ntraining images that correspond to some of the t-SNE cen-\ntroids. Notice that the bottom right of the figure contains\nsharp right turns. Moving diagonally upwards, the right\nturns get less sharp until the vehicle begins to go straight.\nThen, this straight motion gradually begins to become a left\nturn until, by the top left of the figure, the vehicle is making\nsharp left turns.\n6\nFigure 6. LSTM loss comparison for predictions on regular data\nand smoothed data. The more smoothed the data, the more quickly\nthe loss decays.\n3. Results\n3.1. LSTM Experiments\nOur first experiment used LSTMs to predict open prices\nof stocks. We varied the input and output window sizes\nfrom one to fifteen and compared the results. A subset of\nthe prediction graphs are available in Figure 8 for predict-\ning two, four, ten, and twelve prices out. Also included in\neach graph is a line representing the average of the last two,\nfour, ten, or twelve prices as a comparison to the prediction.\nThe prediction with a window of two works extremely well,\nas evidenced by the fact that the three lines (the real, predic-\ntion, and average price) are almost directly on top of each\nother. However, as the window length increases, there is a\nclear divergence of the prediction from the real price. There\nis a trade off between accuracy and the length of the pre-\ndiction, which is expected because data farther out in time\nwill have less of a dependence on the input to the LSTM.\nAlso, the predictions become much noisier, which is to be\nexpected for the same reason.\nWe also tested LSTM prediction on smoothed data. Af-\nter training the LSTM on this smoothed data, we tested the\nmodel on new smoothed and non-smoothed data and com-\npared the loss values, computed through mean squared er-\nror, in Figure 6. The blue line is the loss associated with\nthe smoothed data and the orange line corresponds to the\nregular data. The loss value is on the y-axis, and the x-\naxis shows how much smoothing was applied to the train-\ning data. We smoothed using a moving average filter, so the\nx-axis points represent how many data points were used in\nthe average. The graph shows that feeding smoothed data\ninto an LSTM increases the accuracy of its predictions and\nleads to a quicker decay of the loss.\nThe results for our first stock agent that learns a multi-\nplier to predict the next open price based on the previous\n(a) Real (blue) versus predicted (green) stock price predicted using\nthe multiplier stock agent\n(b) Difference between the real stock price and the predicted price\nfor the multiplier stock agent\nFigure 7. Results for the multiplier stock agent. (a) shows that the\npredictions match very closely, and (b) shows that the areas where\nthe predicted and real price differ the most occur during drastic\nprice changes.\nstock price can be found in Figure 7 (a). The blue line rep-\nresents the real stock open price, and the green line is the\nprediction. The y-axis is the price in dollars, and the x-axis\nis the time step. To get a better idea of exactly how accurate\nthe predictions are, Figure 7 (b) also shows the difference\nbetween the real open prices and the predicted prices for\neach time step. Most of the predictions differ by less than a\ndollar, which is an order of magnitude less than the prices\nthemselves. The largest differences between the real and\npredicted prices occur during drastic changes to the stock\nprice.\nWe then compare these predictions with those from the\nLSTM. In general, the reinforcement agent predictions, as\npictured in Figure 9 (a), are much more accurate than the\nLSTM prediction, as pictured in Figure 9 (b). It seems that\nLSTMs do a decent job at predicting smaller, local changes,\nbut their performance falls short when a large change in\nprice occurs. The reinforcement learning agent is more ro-\n7\n(a) Window of 2\n(b) Window of 4\n(c) Window of 10\n(d) Window of 12\nFigure 8. Prediction results for an LSTM when the size of the input and output prediction windows are from two, four, ten, and twelve re-\nspectively. The LSTM predictions match more closely with smaller windows than larger windows and provide less noisy results. However,\nthe larger windows allow for longer term predictions.\nbust to being able to handle these changes. Additionally, the\nLSTM predictions are much more noisy.\nWe also compare the LSTM and multiplication stock\nagent’s abilities to predict open prices for multiple stocks at\nonce in Figure 10. Once again, we see that the predictions\nof the reinforcement learning stock agent are much better\nthan the LSTM. For this experiment, we used stocks from\nthe same sector in order to increase the likelihood that there\nwould be correlations in the stocks’ behavior. Reinforce-\nment learning is better able to find and exploit these rela-\ntionships to help make predictions than the LSTM. With this\nin mind, we shift our focus towards reinforcement learning\nand other techniques that can be derived from it.\n3.2. Maze Experiments\nMoving to the maze experiments, we compare the rela-\ntive performance of reinforcement learning and feudal rein-\nforcement learning. We had three different agents navigate\na maze until they reached some goal space. For this exper-\niment, we used a fixed maze with a fixed goal space, but\nit is possible to randomize both the maze structure and the\ngoal location at run time. The standard q learning agent’s\nperformance is documented in Figure 11 and Figure 12 in\nblue.\nThe next two agents are different variations of feudal q\nlearning networks. In the first, the goal vector the worker\nreceives from the manager tells it which direction to take in\nthe maze. The results for this agent are in red in Figures\n11 and 12. In the second feudal q learning agent, the goal\nvector received by the worker tells it which quadrant to go\nto in the maze. This is different than the previous agent\nbecause the worker is not explicitly told which direction to\ntake to reach this goal quadrant. This agent’s results are\npictured in Figures 11 and 12 in green.\n8\n(a) Multiplication RL Stock Agent\n(b) LSTM\nFigure 9. Comparison of the multiplication stock agent predictions\nand the LSTM predictions. The multiplication agent matches the\nreal prices closer than the LSTM and provides less noisy predic-\ntions overall.\nIn Figure 11, we compare the amount of time steps per\nepisode for each of the three agents. The q learning agent\ntakes the most amount of time overall, closely followed by\nthe feudal network with a direction as the goal vector. Ad-\nditionally, these two methods solve the maze in approxi-\nmately the same amount of time once they have found the\noptimal path through the maze. However, the feudal net-\nwork with a quadrant as the goal vector is both significantly\nfaster during training and finds a better solution to the maze,\nas evidenced by the fact that its solution takes less time to\nnavigate the maze than the other two agents.\nFigure 12 shows the reward per episode for each of the\nthree maze agents. The reward of the three agents converges\nto the same number, by design, but it is clear from the graph\nthat the feudal agent with a quadrant as the goal vector per-\nforms the best. Its reward reaches the convergence value\nmuch faster than either of the other two agents. However,\n(a) Multiplier Agent Predictions\n(b) LSTM Predictions\nFigure 10. Comparison between the multiplier stock agent and an\nLSTM for predicting prices for multiple stocks at once. The stock\nsymbols are provided in the legend (top left). The LSTM struggles\nto capture the behavior of multiple stocks at once.\nboth of the feudal agents have a large dip in the reward early\non in training that is not present in the q learning agent,\nindicating that the feudal networks do a lot more of their\nexploration in the earlier stages of their training than the q\nlearning network.\n3.3. Portfolio Stock Experiments\nNow that we’ve discovered the power of feudal rein-\nforcement learning, we revisit the problem of predicting\nstock prices.\nHowever, instead of attempting to solve a\nregression problem, we shift our focus to learning a pol-\nicy over actions. We compare the performance of a hard\ncoded agent, a q learning agent, a DQN agent, and a feudal\nQ learning agent at the task of doubling the value of a given\nportfolio in the stock market in Figure 13. Pay attention to\n9\nFigure 11. Comparison of time steps required per episode for three\nagents in the maze environment. Feudal reinforcement learning\n(green) takes less time to solve the maze overall and explores the\nmaze more efficiently.\nFigure 12. Comparison of the reward per episode for three agents\nin the maze environment. The reward for feudal reinforcement\nlearning (green) converges faster than the other two methods.\nthe x-axes in this figure to note the difference between the\npresented methods. The hard coded agent takes the longest\namount of time to accomplish this task, as expected, with\na duration of 1453 time steps. Also unsurprisingly, the q\nlearning agent has the next longest duration with 802 time\nsteps. The DQN agent doubles its portfolio value in 679\ntime steps, and the feudal q learning agent achieves this goal\nin 680 time steps.\nThe main takeaway from this result is that we achieved\ncomparable results with feudal q learning, which is a rela-\ntively simple method, as we did with the DQN, which is a\nrelatively complicated, deep learning method. We wanted\nto verify that this was always the case, so we repeated this\nportfolio doubling experiment multiple times for each agent\nand recorded the duration results in the histograms in Figure\n14. The y-axis is the number of trials in each bin, and the\nx-axis is the duration of each trial. Extra attention should\nbe paid to the x-axes of the graphs in Figure 14. The hard\ncoded and q learning agents have an x-axis from 0 to 2500,\nwhile the other two agent’s x-axes are capped at 1000.\nWe can see that the q learning agent has values skewed\nmore towards zero than the hard coded agent, so we expect\nan overall faster average duration from that agent. The aver-\nage duration for the hard coded agent was 1521 time steps,\nand the q learning agent had an average duration of 1326\ntime steps, so this we prove this claim to be correct. In\nthe same way, the feudal q learning agent has a histogram\nthat is skewed more towards zero than the DQN agent, so\nwe expect this to be the faster method. The DQN agent\ntook an average of 651 time steps, and the feudal q learn-\ning agent took an average of 573 time steps. Therefore, we\nshow that our original result was an understatement, and\nfeudal q learning is, on average, much faster than a DQN at\ndoubling a portfolio’s value in the stock market.\n3.4. Steering Angle Experiments\nIn the stock portfolio experiments, we prove the effec-\ntiveness of feudal reinforcement learning. In this section,\nwe aim to explore the boundary of its abilities in the driving\ndomain. Our first experiment involves predicting steering\nangles based on image input. We create an image cube with\nten sequential frames that we feed into our modified Udac-\nity challenge network[4], along with the previous steering\nangle, to predict the next steering angle. Figure 15 shows a\nsubset of real steering angles from the Udacity[11] dataset,\nin blue, and the corresponding predicted angles, in orange.\nThe predictions follow the real angles very closely except\nwhen there are drastic changes in the steering angles where\nit tends to over/under estimate the steering angle, which is\nthe same issue we encountered with the LSTM stock exper-\niments.\nOur ultimate goal, however, is to use feudal networks to\npredict steering angles. To do this, we first need to label\nsubroutines within the data in order to have data with which\nto train the manager network. Instead of doing this by hand,\nwe jointly train two networks: one that takes in a sequence\nof angles and predicts their subroutine ID, and another that\ntakes in this subroutine ID, an image cube, and the previ-\nously predicted angle and predicts the next angle in the se-\nquence. Figure 16 shows these prediction results. The left\ngraph contains the steering angle predictions. The real an-\ngles are in blue, and the predicted angles are in orange. The\nright graph shows the predicted subroutine IDs. The blue\nline is the raw prediction values, and the orange line shows\nthe binned values. For this, we map the predicted subrou-\n10\nFigure 13. Comparison of hard coded, q learning, DQN, and feudal q learning agent performance, respectively, in the task of doubling the\nvalue of a portfolio in the stock market. The hard coded agent is the slowest, followed by the q learning agent. The DQN agent and the\nfeudal reinforcement learning agent performed comparably. Note the difference in the x-axis scales.\nFigure 14. We run the experiment from Figure 13 multiple times and record the optimal solution durations from each trial in histograms.\nNotice the difference in time scales between the first two and the last two histograms. The ranking from the previous figure still holds.\nHowever, feudal reinforcement learning has cemented itself as the fastest method, as evidenced by the fact that it’s histogram is skewed to\nthe left more than the DQN’s.\nFigure 15. Steering angle predictions on the Udacity dataset from\nthe modified Udacity steering challenge winner network. Note that\nthis network takes the previous angle as input when predicting the\nnext angle which gives it an unfair advantage.\ntine IDs to their closest value in the set {−1, 0, 1}. In this\nway, we have three discrete subroutine IDs corresponding\nto left turns, right turns, and going straight.\nHowever, there are two problems with these solutions.\nThe first is that it stands to reason that there could be more\nthan just three subroutines represented in the driving data.\nDriving is a complex task that involves a lot of minutia. For\ninstance, we could expand left turns to turning a little left,\nturning a moderate amount of left, and turning a lot left. The\nsame could be done for right turns and even going straight.\nTherefore, constraining the subroutine IDs to fit into three\ndiscrete categories, as inspired by [5], may not allow us to\nrepresent an agent’s actions thoroughly enough. The sec-\nond problem is that, ideally, we want a network that pre-\ndicts steering angles without explicitly taking in informa-\ntion about the previous angle because this gives the network\nan unfair advantage.\nTo this end, we shift our focus from handcrafting our\nsubroutine ID definitions to using t-SNE to do it automati-\ncally. We embed the data into 2D space and use those co-\nordinate pairs as the subroutine IDs. However, before we\nattempt to predict the t-SNE coordinates from image data,\nwe run an experiment to determine if the t-SNE coordinates\nwill work as subroutine IDs. We use the ground truth value\nof the t-SNE centroids as the subroutine ID in our angle\nprediction network, along with an image cube of size ten, to\ndetermine whether or not it would be worthwhile to attempt\nto predict the centroids. If using t-SNE as the subroutine ID\nproduces inaccurate results, then we would need to explore\nother avenues. The results of this are in Figure 17. The blue\nlines are the real steering angle, and the orange lines are the\npredicted angle. While the results in this figure are less ac-\ncurate than our other prediction results, the predictions are\n11\nFigure 16. Angle and subroutine ID prediction results on the Udacity dataset. Notice that the subroutine ID’s behavior mimics the real\nangle behavior.\nmore relevant to real world applications because they are\ncomputed using only visual input.\n4. Discussion\nIn this work, we show that feudal reinforcement learning\nis more effective than reinforcement learning at the tasks\nof stock price prediction and steering angle prediction. We\noriginally considered using generative adversarial networks\n(GANs) for sequence prediction, but our early experiments\npointed to the effectiveness of feudal reinforcement learn-\ning instead. With our maze experiments, we find that feudal\nreinforcement learning is faster during training than rein-\nforcement learning. We also find that feudal reinforcement\nlearning achieves the maximum reward more quickly than\nreinforcement learning. Both of these effects are due to feu-\ndal reinforcement learning’s temporal abstraction. Breaking\ndown the problem into more easily digestible pieces nar-\nrows the focus of the worker agent and allows the optimal\npolicy to be found more quickly.\nAdditionally, temporal abstraction also helps alleviate\nthe problems of long term credit assignment and sparse re-\nward signals. The lower temporal resolution of the manager\nshortens the period of time between rewards overall. In ad-\ndition to the original sparse reward, the worker network also\nreceives a reward for obeying the goals from the manager.\nThis feedback can be much more frequent than the sparse\nreward, thus allowing for more consistent network updates.\nIn this way, we were able to achieve better results with feu-\ndal q learning in our stock portfolio experiments than with\na DQN.\nFinally, we find that a t-SNE embedding space can be\nuseful as the goal space for the manager in feudal reinforce-\nFigure 17. Results of steering angle prediction when the t-SNE\ncoordinates of the input data are used as the subroutine IDs. Notice\nthat, for these results, we use a network that does not take the\nprevious angle as input.\n12\nment learning in our steering angle prediction experiment.\nWe use the centroid corresponding to steering angle, brak-\ning, and throttle data from the previous ten time steps as the\nsubroutine ID in our angle prediction network and were able\nto predict future steering angles without the direct use of the\nsteering angle from the previous time step. The temporal\nabstraction inherent in the t-SNE centroid creation mimics\nthe role of the manager network and allows the worker to\nbe able to more accurately predict steering angles than if it\nattempted this task on its own.\nReferences\n[1] Matthew Chan.\ngym-maze.\nhttps://github.com/\nMattChanTK/gym-maze, 2017. 3\n[2] Franc¸ois Chollet et al. Keras. https://keras.io, 2015.\n1\n[3] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement\nlearning. In Advances in neural information processing sys-\ntems, pages 271–278, 1993. 3\n[4] Komanda.\nhttps://github.com/udacity/\nself-driving-car/blob/master/\nsteering-models/community-models/\nkomanda/solution-komanda.ipynb,\n2016.\n4,\n10\n[5] Ashish Kumar, Saurabh Gupta, and Jitendra Malik. Learning\nnavigation subroutines by watching videos. arXiv preprint\narXiv:1905.12612, 2019. 5, 11\n[6] Laurens van der Maaten and Geoffrey Hinton.\nVisualiz-\ning data using t-sne. Journal of machine learning research,\n9(Nov):2579–2605, 2008. 5\n[7] Boris\nMarjanovic.\nHuge\nstock\nmarket\ndataset.\nhttps://www.kaggle.com/borismarjanovic/\nprice-volume-data-for-all-us-stocks-etfs,\nNov 2017. 1\n[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\nRiedmiller. Playing atari with deep reinforcement learning.\narXiv preprint arXiv:1312.5602, 2013. 1, 2\n[9] Quandl. WIKI various end-of-day data. https://www.\nquandl.com/data/WIKI, 2016. 2\n[10] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Lau-\nrent Sifre, Dharshan Kumaran, Thore Graepel, et al. Master-\ning chess and shogi by self-play with a general reinforcement\nlearning algorithm. arXiv preprint arXiv:1712.01815, 2017.\n1\n[11] Udacity.\nUdacity self-driving car driving data 10/3/2016\n(dataset-2-2.bag.tar.gz). 1, 4, 10\n[12] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul,\nNicolas Heess, Max Jaderberg, David Silver, and Koray\nKavukcuoglu. Feudal networks for hierarchical reinforce-\nment learning. In Proceedings of the 34th International Con-\nference on Machine Learning-Volume 70, pages 3540–3549.\nJMLR. org, 2017. 1\n13\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-10-09",
  "updated": "2023-10-09"
}