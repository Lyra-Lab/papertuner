{
  "id": "http://arxiv.org/abs/2310.06228v1",
  "title": "Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI",
  "authors": [
    "Masahiro Yamamoto"
  ],
  "abstract": "Since the invention of computers, communication through natural language\n(actual human language) has been a dream technology. However, natural language\nis extremely difficult to mathematically formulate, making it difficult to\nrealize as an algorithm without considering programming. While there have been\nnumerous technological developments, one cannot say that any results allowing\nfree utilization have been achieved thus far. In the case of language learning\nin humans, for instance when learning one's mother tongue or foreign language,\none must admit that this process is similar to the adage \"practice makes\nperfect\" in principle, even though the learning method is significant up to a\npoint. Deep learning has played a central role in contemporary AI technology in\nrecent years. When applied to natural language processing (NLP), this produced\nunprecedented results. Achievements exceeding the initial predictions have been\nreported from the results of learning vast amounts of textual data using deep\nlearning. For instance, four arithmetic operations could be performed without\nexplicit learning, thereby enabling the explanation of complex images and the\ngeneration of images from corresponding explanatory texts. It is an accurate\nexample of the learner embodying the concept of \"practice makes perfect\" by\nusing vast amounts of textual data. This report provides a technological\nexplanation of how cutting-edge NLP has made it possible to realize the\n\"practice makes perfect\" principle. Additionally, examples of how this can be\napplied to business are provided. We reported in June 2022 in Japanese on the\nNLP movement from late 2021 to early 2022. We would like to summarize this as a\nmemorandum since this is just the initial movement leading to the current large\nlanguage models (LLMs).",
  "text": " \nEvolution of Natural Language Processing Technology: Not Just Language Processing Towards \nGeneral-Purpose AI \n \nEvolution of natural language processing technology: \nFrom “language” processing to general-purpose AI. \nRecent trends from a Japanese point of view. \n \nMasahiro Yamamoto \nINFORMATION-TECHNOLOGY PROMOTION AGENCY,  JAPAN \nGeneral Affairs Planning Department \n \nAbstract \nSince the invention of computers, communication through natural language (actual human \nlanguage) has been a dream technology 1. H owever, n atural l anguage i s e xtremely d ifficult t o \nmathematically formulate, making it difficult to realize as an algorithm without considering \nprogramming. While there have been numerous technological developments, one cannot say that any \nresults allowing free utilization have been achieved thus far. \nIn the case of language learning in humans, for instance when learning one’s mother tongue or \nforeign language, one must admit that this process is similar to the adage “practice makes perfect” in \nprinciple, even though the learning method is significant up to a point. Deep learning has played a \ncentral role in contemporary AI technology in recent years. When applied to natural language \nprocessing (NLP), this produced unprecedented results. Achievements exceeding the initial \npredictions have been reported from the results of learning vast amounts of textual data using deep \nlearning. For instance, four arithmetic operations could be performed without explicit learning, thereby \nenabling the explanation of complex images and the generation of images from corresponding \nexplanatory texts. It is an accurate example of the learner embodying the concept of “practice makes \nperfect” by using vast amounts of textual data. \nThis report provides a technological explanation of how cutting-edge NLP has made it possible to \nrealize the “practice makes perfect” principle. Additionally, examples of how this can be applied to \nbusiness are provided. We reported in June 2022 in Japanese on the NLP movement from late 2021 to \nearly 20222. We would like to summarize this as a memorandum since this is just the initial movement \nleading to the current large language models (LLMs). \nChapter 1 presents an overview of NLP in recent years. What is made possible through vast amount \nof textual data and parameters is explained through the example of GPT-33, which caused a major stir \nwhen it was released. In particular, the outline of Transformer, which is one of the key technologies, \nis being explicated to achieve intuitive understanding. \nIn Chapter 2, the potential applications of GPT-3 are presented to discuss the practical side of large-\nscale language models. Some of these services have already been launched, and while they have \nclearly demonstrated the usage of AI, they have realized unprecedented language-based exchange, \nthus setting them apart from existing services. \nChapter 3 discusses the ongoing studies in Japan in the field of NLP. What is the situation in Japan \nwhen the development of large-scale language model takes place mostly in English? The simple \noverview of this situation is discussed, along with the results of recent dialog competitions. \nChapter 4 discusses the report stating that a large-scale language model will become the \nfundamental base model of a general-purpose AI. As most human activities can be explained using \nlanguage, a general-purpose language model describes most human activities within the model. \nTherefore, the possibilities of large-scale natural language models that can lead to general-purpose AI \nwill be discussed a paper from Stanford University, USA. Moreover, the relationship between large-\nscale language models and robotics is discussed focusing on the perspective of direct interaction with \nthe real world. \nChapter 5 discusses the developments, mostly international, in NLP technology beyond GPT-3. The \nauthor provides a snapshot of the current situation, which sees daily updates, including the \nconstruction of larger models and the method to improve performance while reducing the scale of the \nmodel. \nChapter 6 discusses the important challenges faced in the present NLP and natural language models. \nLanguage has a significant impact on society, and this fact will be re-examined. \nChapter 7 contains the summary and future prospects. \n      \n \n \n \n \n \n \n \n \n \n \n \n \nTable of Contents \n1 \nIntroduction ........................................................................................................................ 4 \n1.1 Significant progress is supported by the rapid increase in the scale of parameters ................... 2 \n1.2 Breakthrough by Transformer that has enabled efficient round-robin ..................................... 4 \n1.3 Features of natural language processing by Transformer ...................................................... 7 \n2. Natural language processing technology application ................................................................ 10 \n2.1 Examples of business applications of GPT-3 ..................................................................... 11 \n2.2 Technologies that apply GPT-3 ......................................................................................... 13 \nDALL･E for flexible image generation through language and CLIP for flexible caption \nwriting. ........................................................................................................................... 13 \nPower Apps ..................................................................................................................... 14 \n3 Trends in Japan ..................................................................................................................... 15 \n3.1 Accommodating the Japanese language ............................................................................ 15 \n3.2 Resources in Japan .......................................................................................................... 16 \n3.3 Discussions within Japan ................................................................................................. 17 \n4 Large-scale natural language model that can potentially lead to general-purpose AI ................. 18 \n4.1 Foundation Models ......................................................................................................... 18 \n4.2 Essence of natural language models .................................................................................. 20 \n4.3 Robotics and large-scale natural language model ............................................................... 21 \n4.4 Correspondence with the real world. ................................................................................. 22 \n5 Introduction of the topics related to natural language models after GPT-3 .................................. 24 \n5.1 MT-NLG, a powerful natural language model by Microsoft and NVIDIA ........................... 24 \n5.2 Microsoft Project Florence-VL......................................................................................... 25 \n5.3 BigScience T0 ............................................................................................................... 26 \n5.4 LaMDA, dialog approach of Google ................................................................................. 27 \n5.5 DeepMind RETRO (Retrieval Enhanced TRansfOrmers) ................................................... 28 \n5.6 South Korean LG EXAONE ............................................................................................ 29 \n5.7 Chinese BAAI Wu Dao 2.0 .............................................................................................. 30 \n5.8 OpenAI GLIDE, DALL･E2 ............................................................................................. 31 \n5.9 MLP Reexamination, beyond Transformer ........................................................................ 32 \n6 Important Challenges ............................................................................................................. 34 \n7 Conclusion ............................................................................................................................ 35 \n \n \n \n \n1 \nIntroduction \nIn 2018, Google launched an AI assistant named Duplex 4 at the company’s development \nconference, and performed a demonstration where it made a reservation at a given restaurant. Upon \nopening the Duplex application and using one’s voice to specify the restaurant, number of people, date, \nand time, it will automatically book a reservation over the phone. As many restaurants do not offer \nonline reservations, reservations can be made at any restaurant using Duplex as it can call the desired \nrestaurant on our behalf. \nExamples of actual interactions with the application sound natural. It also responds sufficiently \nnaturally during phone calls to restaurants to make reservations. It did not cause any confusion in the \nrestaurants where the reservations were made, meaning that they did not notice that Duplex was \nautomatically conducting the whole exchange. In fact, the restaurant staff were surprised when they \nfound out that they were not talking to an actual human. \nWhen Duplex was launched in 2018, it required partial human assistance for 25% of its operations \ninitially; the update report from 2020 revealed that 99% of its reservation operations were conducted \nthrough automatic response by the AI 5. M oreover, D uplex h ad s uccessfully m ade a  m illion \nreservations by that point. \nIn addition, Google reported that it uses Duplex to automatically update business information on \nGoogle Maps and Google search6. Detailed information such as the opening hours of restaurants or \nwhether the restaurant offers takeout service is updated automatically, without the need for manual \nupdates. \nDuplex not only looks up information necessary for making reservations through Google search but \nalso updates the very information it uses by itself. Now, further functions have been added to realize \nadditional applications, such as purchase movie tickets and reserve rental cars. When a person speaks \nto the application using natural language, the AI can perform all the tasks while offering \nunprecedentedly natural responses. \nOne of the key technologies that supports Duplex is the natural language processing (NLP). The \ngeneral term for technologies that enable a system to simulate dialog through everyday language, and \nthat enable the system to automatically perform a task and report the result of the same through natural \nlanguage, is NLP. In fact, the Duplex example shows that NLP has various potential applications, \nranging from interfaces to business logic. \nCurrently, cutting-edge NLP technology is capable of generating a natural sentence from a single \nword irrespective of automated processing and results from language inputs. One can even say that \nthe replacement of even human creative work is becoming a realistic near-future prospect. Fig.1 shows \nan example (diagram) of NLP. \n2 \n \n \n \nFig. 1 Natural Language Processing with use cases \n \n1.1 Significant progress is supported by the rapid increase in the number of parameters \nOne of the factors behind the expansion of Duplex applications is a new natural language model \n(NLM)7 named bidirectional encoder representations from transformers (BERT)8, which has large \nscale model parameters9. The rapid increase in the number of learning parameters is one of the reasons \nfor the explosive development of NLP. Deep learning, which is the central technology for AI in recent \nyears, characteristically improve the capability of AI through numerous learning parameters using \nunprecedentedly vast amounts of data. \nThe performance of NLP in the last few years has improved remarkably owing to the increase in \nthe number of learning parameters. Fig.2 shows seminal NLP technologies at each historical moment \nand their parameter numbers at the time of their release. In fact, the number of parameters used by \nlanguage models has increased by hundred to thousand-fold in last five years. Currently, the number \nof parameters used in Google T5+ is 1.6 trillion. \nHow do neural networks with this many parameters learn and optimize? How does one prepare the \ntraining data for such networks? These questions are important from intuitive and technological \nperspectives. For instance, GPT-3, which gained considerable interest due to its performance in 2020, \nuses textual data comprising four trillion words for its learning, and the required computational cost \nis estimated to be a few billion yen. Furthermore, it requires a vast amount of computational resources, \nestimated at computing on V100-class GPU cluster with 1420 GPUs for one year. \n \n3 \n \n \nFig.2 The parameter numbers in the language model10 \n \nComputational resources continue to evolve considerably with each passing year. For instance, GPU \nA100, introduced in 2021, is expected to have ~20 times more capability than V100 and enable \nlearning with improved realistic times and computational scales. H10011, which NVIDIA released in \n2022, is said to be capable of building NLMs 30 times faster than A100. For instance, it can build a \nvast language model approximately the same size as GPT-3 in a week. Naturally, this makes it easier \nto enlarge the language model itself to ten or several hundred times larger than it is now. The \ncomputational capabilities will continue to increase remarkably as with the evolution of semiconductor \nchips. Thus, computational resources with high cost performance can be realized in the future, which \nmake the facility planning crucial. \nThe NLMs released in late 2021 did not necessarily have numerous learning parameters12. This \nindicates that, through innovation, a model can have performance equal to models with a large number \nof learning parameters as long as it has the required number of learning parameters. This must be \nregarded as a new trend from the perspective of computational resources. \nInteresting results have been obtained by increasing the number of parameters in GPT-3; a NLM \nsuccessfully performed four arithmetic operations without explicitly learning its computing logic13. \nThe aforementioned language model showed signs of performing two- and three-digit additions when \nthe number of model parameters exceeded 6.7 and 13 billion, respectively. When the model parameters \nexceeded ~100 billion, it became capable of almost perfectly performing two- and three-digit addition \nand started to display signs of being able to perform two-digit multiplication and four-digit addition. \nAs the number of parameters increase, the NLM acquires computational capability of addition, \nmultiplication, subtraction, and division. Even though only textual data were used for training, a NLM \nacquired the function to perform calculations when the number of its parameters exceeded 10 billion. \nThe vast amount of text data included the descriptions of calculations and a couple of corresponding \n4 \n \nexamples. However, the capability of a NLM to learn calculations, without explicit training data for \nthe same, is an astonishing result. Furthermore, it demonstrated the possibility that, only through \nlearning via text data a NLM with a vast number of parameters is capable of learning the described \nprocedure itself and integrating it as a function. Naturally, it also learns how to write programs. \n \n1.2 Breakthrough by Transformer that has enabled efficient round-robin \nThe history of research and development of the NLP, which has been expected to be the main \ntechnological area in AI is long. Though there have been many efforts made since deep learning \nbecame the main focus of AI, no major breakthroughs were made immediately. This was partially due \nto the characteristics of natural language. Developments in deep learning include the so-called general-\npurpose computing on graphics processing units (GPGPU), which uses GPU14 for purposes other than \nimage rendering. GPGPU performs calculations using numerous (approximately a few thousand) \nproduct–sum operation units on the GPU in parallel. \nHowever, in the NLP, it is necessary to deal with meanings, directives, pronouns, etc., while going \nback and forth in their context. Understanding texts, in particular, requires such processing. As such \nprocessing requires AI to remember specific parts while processing the whole in time series, it is \ngenerally not very compatible with parallel computing. Therefore recurrent neural network (RNN)15, \nwhich is used to deal with time series, is commonly used in natural language. As the sequence-\ndependent relationship must be considered, it generates (wait for) adjustments between several \ndependent relationships, thereby complicating the parallel operations. Thus, GPGPU cannot be used \neffectively for the NLP involving sequence-dependent relationship. As the current mainstream in \ncalculations using GPGPU prioritizes parallel and mechanical calculations, time-series calculations \nwith dependency is intrinsically unsuitable for it. \nTo address this issue, the Attention 16 mechanism was developed. Using this mechanism, the \nconnection of a word with its counterparts in a sentence is discovered by focusing only on the word. \nNote that the focus is only on these connections and not the grammar and mechanics. Fig.3 shows an \noverview of the attention mechanism. In the sentence “Mount Fuji is high, and the sea is beautiful,” \nthe attention is on “high.” Then, it calculates how much weight should be placed on its connection \nwith other elements in the sentence, namely “Mount Fuji,” “and,” “the sea,” “is,” and “beautiful.” The \nfigure shows this schematically by representing the connections through solid and dotted lines and the \nthickness of these lines. This operation is repeated for every element in the text. As the operation is \nperformed on all the text that can be stored in the memory, in principle, the weight of the connection \nbetween elements that are far apart can be calculated. \n \n \n \n5 \n \n \n \nFig.3 Schematic diagram of attention mechanism \n \nAs the text in the memory can be handled as though it is an image, parallel computation that \ncompares the feature value(s) of one part of the text to the feature value(s) of other part becomes \npossible. In other words, even though NLP is intrinsically unsuitable for parallel computation, it can \nbe processed through parallel computation using the attention mechanism. From a different viewpoint, \none can also say that this reverts to the fully connected network as it calculates the connection between \nevery element on the memory. \nThe effect of parallel processing becoming possible is enormous. Aside from the obvious fact that \nefficient parallel computation using GPGPU has become possible, common supervised learning17 is \nno longer always necessary because, through full connection, the AI is only required to learn the \nweight relationships through text cross-referencing. Natural language learning cannot be easily \nevaluated, unlike human learning, so it is necessary to use existing texts for learning. Furthermore, \nsome amount of learning data should be used as the training data for simulating tests within the context \nto teach questions and answers. The problem was that the cost of producing these pairs was extremely \nhigh. However, if it comes down to what is known as a fill-in-the-blank question, so it can turn, in \nprinciple, any text into training data with almost zero cost. BERT mostly employs two simple learning \nmethods namely masked language modeling (MLM), which performs what is known as fill-in-the-\nblank, and next sentence prediction (NSP), which evaluates the similarity of sentences. \nFig.4 shows the concept of MLM. As attention is based on bidirectional referencing, one can say \nthat the biderectional referencing itself is the learning. If normal text data is present, one can simply \nmask a part of it and turn it into training data with the correct answer. This procedure, where data itself \nis changed/worked on to give the correct label to the original data, is called self-supervised learning. \nAs it does not explicitly supply training data, it is attracting attention as a typical example of self-\nsupervised learning. It has been demonstrated that masked estimation questions, where parts of a \n6 \n \nsentence are masked, which is common in tests, is a valid framework for self-supervised learning, and \nsignificant progress has been made using this approach. \n \n \nFig. 4 Masked Language Model (MLM) \n \nNSP compares two sentences and judges whether the second one is an appropriate sentence to follow \nthe first one (Fig.5). This can be easily determined from cross-referencing information. The knowledge \nregarding training data verified in BERT showed that normal text data can also become high-quality \ntraining data18. In other words, a large amount of training data will be spontaneously generated without \nmodifying the text data. Learning by MLM and NSP, NLMs successfully learned relationships and \ntexts simultaneously. Furthermore, it learned calculation functions. \n \n \nFig. 5 Next Sentence Prediction (NSP) \n \nThe center of this attention-based basic machine learning is the technology called Transformer. \nTransformer enabled usage of normal texts as training data, as well as processing of numerous \nparameters by efficiently using calculation resources in parallel. Transformer, a technology that has \nattracted attention toward BERT, has made it possible to efficiently learn a vast number of learning \nparameters and express the corresponding functions. \n \n \n7 \n \n1.3 Features of natural language processing by Transformer \nLet us introduce additional keywords necessary for understanding Transformer. These keywords, \nParameter scale, Natural language model, Zero(Few)-Shot learning, Language benchmark, and \nMultilingual support, have become more than simply the characteristics of contemporary NLP \ntechnology19. \n \nParameter scale \nFor instance, when there are more parameters, useful results can be expected by expanding the \nreference area during learning and its reflection on the model. Thus, when the parameter scale is \nenlarged, the limit of the existing models will also be pushed up, and their performances are also \nexpected to improve. Indeed, performance assurance through parameter scale enlargement in \nTransformer has been verified20, indicating that the enlargement of parameter scale directly leads to \nperformance improvement. In principle, the larger the parameter number, the better the performance \nof the NLM. Therefore, larger parameter scales are currently being tested. Meanwhile, methods to \ncompress the enlarged model to make it efficient are being investigated \n \nNatural language model \nGenerally, a NLM is the core of machine learning for NLP, and it is the foundation of NLP. One \nelement of NLP is to determine the position of a given sentence in a sentence assess. In Transformer, \nthe model is built based on the probability of appearance of a given word at a specific position in a \nsentence is determined through cross referencing preceding and succeeding words. \nNLMs comprising several parameters and following large-scale prior learning21 through vast text \ndata acquired a surprising amount of knowledge from their training data. This knowledge was mainly \nacquired by learning relationships among words. These models not only acquired knowledge but also \nlearned the logical connections between descriptions, which cannot be acquired only by learning \nrelationships among words. Therefore, they not only raised the quality in text generation tasks, such \nas text summarization or conversational response that is accurate and compositionally correct but also \nbecame capable of performing functions other than language tasks, such as calculation or automatic \nprogramming from specifications, through a single language model. \n \nZero(Few)-Shot learning22 \nThe prior learning model contains all referential relationships described in the text of the training \ndata. Therefore, domain tuning is possible without an example or with only one or two examples, thus \nenabling wide-range output of arbitrary tasks. \nFew-shot learning is when a small number (10 to 100) of examples are offered for the task (during \ninference), and one-shot learning is when only one example is offered. Similarly, no examples are \n8 \n \noffered in zero-shot learning. Zero and Few-Shot learnings are highly dependent on the performance \nof the prior learning model. However, today, almost the best performances according to the \nbenchmarks of successful task completion have been achieved without any task examples or only with \none task example without learning each domain, which was previously necessary. \nGPT-3, in particular, uses prompt learning23. Due to its high performance, it has become an almost \nde facto tuning method for subsequent NLMs. In prompt learning, simple exchanges are tuned on the \ndialog UI screen by presenting examples of simple sentence pairs according to the type of shot learning. \nThis is commonly known as sequential learning. In this case, additional learning methods can be \nunderstood as the process of additional adjustment to determine which relationships among the vast \nnumber of text pairs in the NLM the domain should prioritize. \n \nLanguage benchmark \nThe evaluation of NLM is extremely important when using the model. For instance, a person can \ndetermine the naturalness of a simplesentence. However, quantifying this naturalness in number is \nfundamentally difficult. If quantification of evaluation was at all possible, NLP can be conducted \nsimply by supplying the quantified evaluation to an algorithm. With this situation in the background, \nseveral benchmarks24 have been devised as evaluation indexes for NLP based on training data written \nby humans to enable objective evaluation through tests. For instance, they could include questions and \nanswers, text generation, natural language inference of logical connections across several stages, \nsolutions through referencing, and the resolution of ambiguity in the meaning of a word and its \ntranslation ( common in tests with the Japanese language). \nThe time between the proposal of a benchmark and for an AI to exceed the human scores in it is \nbecoming decreasing every year25. When examining several examples, including the widely used \nMNIST26 and recent GLUE27, it shows that AI took more than 15 years to exceed human scores in \nMNIST, which was proposed before 2000, it only took less than 3 years in GLUE, despite the fact that \nit was proposed recently and the contents of benchmarks became advanced. \nSince deep learning became widespread in the 2010s, the score is improving rapidly, and despite \nthe fact that GLUE was proposed in accordance with this era of deep learning, AI score exceeding \nhuman scores was reported only after two years. Earlier, research and development in NLP system \nbuilt systems only to improve benchmark and achieve higher scores. Today, however, this approach \nhas changed where more general NLP systems are built, which are then examined through various \nbenchmarks. In other words, it has shifted from learning for the sake of tests to conducting general \nand wide-ranging learning before taking tests. This indicates the fact that technologically, the \nperformances of the current general-purpose NLP systems not specialized in any individual tasks and \ndomains are equal to those of the specialized systems. \nCurrently, existing benchmarks are becoming useless owing to the development in NLP technology \n9 \n \nin recent years, and new benchmarks are being proposed28. Additionally, benchmarks are being used \nfor assessing performance and conducting detailed diagnoses of operations. Moreover, most \nbenchmarks focuse on specific task or domain because creation of universal benchmark is \ndifficult. \n However, several problems can arise while realizing applications of LLMs when these applications \nare used in society. Therefore, researchers have been conducted to determine new benchmarks that \nimprove the shortcomings of existing benchmarks and are more suitable for social implementation to \nfurther refine NLP systems. Simultaneously, other aspects are expected of new benchmarks. The \nsituation demands a serious examination of the risks of natural language models through the \nbenchmarks. \nMany people believe that the current benchmarks are insufficient for the evaluation of several \nimportant risks. For instance, let us consider a situation where the output provided by an application \nthat uses a natural language model for a given input is incorrect. Can we be sure that people will not \nbelieve this information to be true? For instance, if this application usually offers information that is \nalmost 100% trustworthy, when as an exception the information it has given is incorrect, people would \nnormally continue to trust the application. This would then become the trust bias and people would \nbelieve the incorrect information. Under such a situation, problems exist even when there is no \nmalicious intent. Thus, usage of natural language models inherently contains significant risks. \nAt this point, the only solution is to avoid evaluating natural language models only through \nbenchmark tools and let people actually test and verify them. Thus, it is important to conduct further \ndiscussions on risk reduction. As discussed earlier, the reproduction of harmful social stereotypes by \nnatural language models are becoming a major problem.29 In fact, one can say that studies on such \nproblems are only at their initial stage. There are many people who are hoping for further \nimprovements in benchmarks for better evaluation of risks. \n \nMultilingual support \nDiversity in a language is also an extremely important aspect of natural language models. The \nstructure of a language is usually of an extremely localized nature, as it is heavily influenced by the \nplace and culture in which it was born. A pretraining model is expected not only to learn cross-\nreferencing in one language but also to learn the universal relationships shared across multiple \nlanguages. Therefore, the learned universality across multiple languages is essentially the distributed \nrepresentation that should be contained within a natural language model. In other words, it is a feature \nrepresentation vector that should acquire. If distributed representation has acquired universality, it \nmeans it has acquired various relationships corresponding with objects and circumstances at a more \nfundamental language level. Thus, one can infer that even when language is different in each country, \nit does not cause problems for language models. For this reason, by using language data that is as \n10 \n \nwide-ranging as possible to build a natural language model, it is possible not only to achieve two-\ndirectional multilingual machine translation but also to make the natural language model itself of \nhigher quality. \nIn principle, bidirectional text data of the translation source and language to be translated into is \nnecessary for translation. However, when a natural language model acquires distributed representation, \nmachine translation of a language is expected to become possible, even when no sufficient \nbidirectional text data for its learning is available. In fact, Google announced during its development \nmeeting on May 2022 that a single language approach, whereby a language model learns translation \nof a new language even when there is an insufficient amount of training data, is possible, and it newly \nadded 24 languages to its translation service30. While these 24 languages are not regarded as major \nlanguages, the total of their users are as large as 300 million people. Thus, there is high expectation \nfor the expansion this next generation translation will offer. \nAs there are more than 7,000 languages in the world31 (and strictly speaking, distinguishing among \nthese languages is considerably difficult), only a small part of these languages can be translated today. \nIt is known that the languages around the world are derived from the top language families31. While \nonly a small number of languages have been modeled thus far, these have still had impacts on the \nimprovement of distributed representation. As the number of modeled languages will increase in the \nfuture, one can expect that LLMs will acquire further universality. Moreover, accommodating more \nlanguages is known to be useful for responding to unknown languages. Machine translation \naccommodating every language in the world is not only an important technological challenge in itself, \nbut also an intellectual challenge to understanding the language itself, considering the current position \nof natural language models. \n \nThese keywords will help us to understand NLP by Transformer. If a natural language model \npossesses all these characteristics, one can say that the expression “practice makes one perfect” \napplies to AI as well. As the results of current NLP can appear to understand the meaning of language, \nthe phrase ‘natural language understanding’ is beginning to be used. This is because the system appears \nto understand the content of a text and processes it considering its meaning. While whether the system \ntruly understands the meaning is still being debated, it is undeniable that extremely advance processing \nhas become possible. \n \n2. Natural language processing technology application \nGPT-3 was released on July 2020 as the third large-scale natural language model by OpenAI as a \nresult of NLP. Its impact grew larger with time32, and now it is highly possible that, in a few years, it \nwill be regarded as the turning point of AI itself. GPT-3 is a natural language model with the \nTransformer that has 175 billion parameters trained with several trillion words. \n11 \n \nGPT-3 has extremely advanced NLP capability, and it processes and generates text as though it is a \nhuman. For instance, when a user inputs text on the prompt window, where text inputs and outputs are \nfed, GPT-3 returns an output in the form of an email, tweet, or trivia quiz with consistent topics \naccording to the purpose. Thus, an environment where, in principle, the entire text of an email, \nexchange with customers, social media entry, and news article, or at least parts of them, can be \nautomatically generated suddenly emerged. \nThe usage policy of GPT-3 by OpenAI supports commercial application, and in response, business \napplication started instantly. Using such a natural language model, a new start-up can launch an \napplication; thus, GPT-3 and its equivalent models can deliver easy access to the enormous power of \nAI to the people who are investigating the application of NLP around the world33. Indeed, when the \nauthor of this paper searched GPT-3 as an author in Amazon34, he found publications where GPT-3 is \nlisted as the co-author. While it appears that human help is still needed to fine-tune the texts from GPT-\n3, the fact that these publications are for sale indicates an extremely important turning point. \n \n2.1 Examples of business applications of GPT-3 \nFor example, OthersideAI35 can generate an entire email from only the keywords of the main text \nor from its opening line. Examples of such text generation from keywords also include Broca36 and \nSnazzy37 used for writing advertisement copies or campaign contents. Both companies offer trials. \nThus, these innovations in automatic text generation can be experienced immediately. These examples \nare thoroughly discussed in the subsequent paragraphs. \nWhile email has occupied a significant position as an important communication tool since the early \ndays of the Internet until today, in recent years it has also been regarded as a representative of \ndecreasing productivity, or indeed its cause. OthersideAI solved this problem by overcoming the \nproductivity loss caused by writing e-mails, and instead is utilizing GPT-3 to increase productivity. In \nprinciple, let us think of a combination of a question and an answer. Not only is this generated each \ntime, but it also enables detailed adjustment for each scenario even if they have similar contents, \nallowing for minor changes and the addition of necessary information for each scenario. Thus, it can \naccommodate several scenarios and situations, such as sales, support, and marketing, that \nconventionally required reorganization each time, without any problem as a common engine. Their \ndemo offers an example of an email where a complete text was generated only after input of keywords. \nIt shows that it only requires entry of necessary and sufficient words, and the remaining email is \ngenerated automatically. \nSeen from another perspective, this automatic text generation function can also be used as typing \nassistance. When writing e-mails, using one’s own expression for every word and clause is difficult to \nbegin with, and for business correspondence, a large portion of one’s email is standard expressions. In \nany case, there is no problem with the text itself if most of it was automatically generated. This means, \n12 \n \nfor instance, when writing a text through eye gaze input, text is generated only through the input of \ninitial words and the rest of the process only requires only checking and minor corrections. The \nbenefits of such a process are in immeasurable. \nSuch text generation through input assistance was not expected at the start of the service. Instead, \nits possibility was recognized during the verification and analysis of automatically generated texts, \nwhich was then chosen as an area of its application. Indeed, one can say that it is one of the easiest \ninput assistance tools developed so far. \nBroca and Snazzy that offers advertisement service fully utilizes text generation through keyword \ninput. The fact that a campaign contents that takes context into account can be automatically generated \nfrom the input of one word, or from extremely short but impactful advertisement copy, appears to be \na highly convenient service. \nWith this automatic generation contents by an AI, Broca is attempting to expand contents marketing. \nIts strength is the fact that contents with high quality that used to take a few hours, or even a few days, \nto prepare can be generated in a few minutes. To quell doubt on its quality and guarantee its good \nperformance, all the contents in the website of Broca is being generated through the service it offers. \nNaturally, this indicates their confidence in the quality of the contents their service offers, which can \nprovide everything from contents for advertisement to social media posts. \nFor instance, its website lists an example of generating campaign contents for a new product. It is a \nthree-step process. In reality, the first two to three sentences entered at the first step are the only input \nrequired from the user. The content is automatically generated only after this input, and one can obtain \npromotion content of any type consistent with its context. \nSeveral examples of BERT as a basic element have been reported in the literature. \nFORETHOUGHT38 provides a question-and-answer search AI agent called Agatha. Agatha uses \nknowledge-based articles and help desk templates to answer general questions through e-mails and \nWeb widgets. It offers immediate answers to users. One feature of Agatha is continuous training \nthrough machine learning and NLP for the purpose of constant improvement with time. It can already \naccommodate 100 languages and is offering language solutions globally. \nGong39 provides a next-generation CRM that performs speech analysis of conversations with \ncustomers. As a result, several thousand sales teams have completed business using the NLP function \nof Gong, shortening their selling cycles. As 99% of the information one shares with the customers \ndoes not reach CRM and the remaining 1% is heavily filtered, they are not useful for sales. For this \nreason, Gong extracts elements with higher order insights that are relevant for information necessary \nfor real business dealings by applying NLP on information from the customers. It copies e-mails, \nphone calls and video calls from customers, then analyzes all the elements that can potentially lead to \ninsights on whether the clients are prepared to propose updates on products or whether there are risks \nof losing business by utilizing machine learning. \n13 \n \nMoveworks40 provides a bot that can autonomously solve various problems of the staff when they \nare using IT. In addition, it is enabled to understand conversation and ambiguous questions through its \nNLP function. Starting in the summer of 2018 where it autonomously solved 20% of IT-related \nproblems for a customer without human support, the bot can currently solve up to 40% of these \nproblems, and under certain circumstances, it can solve 65% of problems. In other words, 65% of \nproblems that arise when using IT can be autonomously solved. See below for examples. \nExample ’Question’ → ‘Automatic answer’ \n‘I want to edit a PDF file.’ → ‘I will issue an Acrobat Pro license.’ \n‘I want a new keyboard because I’m working remotely.’ → ‘Which one of these keyboards do you \nwant? Please select the number.’ \n‘I forgot my password.’ → ‘Please follow this link to create a new password.’ \n \nApplication of this function is clearly not limited to the IT field; this AI function has expanded into \nthe fields of human resources, general affairs, and finance. \nOBSERVE.AI41 analyzes the work of contact centers to optimize the same. Based on the experience \nthat, at a contact center, only 1%, 2%, or even fewer phone calls will reach the other person, \nOBSERVE.AI applied speech analysis to this unexplored field. By using transcription of speech to \ntext and NLP, they can discover the points of interest within a conversation, and they successfully used \nit to improve the customer experience. To better understand the mood of customers, they apply \nsupervised learning to conversations at call centers, which led to better customer experiences. \n \n \n2.2 Technologies that apply GPT-3 \nDALL･E for flexible image generation through language and CLIP for flexible caption writing. \nOpenAI released DALL･E42, which is capable of generating images from text. It learns 12 billion \nparameter versions of GPT-3 through text and image pairs. It reads the contents of the texts entered \nand can generate images that do not exist in its learning data through combination within the model. \nThe released examples43 show that it can generate an image (to be precise, an image group) from \ndescriptions that are seemingly complete when a person reads it, but it becomes clear that detailed \nconditions are not given when one actually tries to draw from them, such as “avocado-like chair” or \n“a vase on the stool by the bed.” \nIn the training of Transformer, elements that are seemingly unrelated are reflected in the training by \nweighting their necessary relationships under every condition. Thus, one can say that it is extracting \nall the necessary conditions for actually generating images, i.e. the detailed conditions necessary for \ndrawing that are obvious conditions and can be obtained by pretraining the already learned \ncommonsense, for instance light direction, shadow, texture, and the direction of the bedside. Thus, the \n14 \n \nmissing conditions are inferred to be necessary and sufficient conditions, enabling it to make a drawing \nthat is a resolved picture without any problem. These conditions include various transformations, such \nas personification and the usage of emoji. It also appears to be capable of considerably flexible \ntransformation, within the range possible from the interpretation of the text, such as simplification or \nthe display of details. As it includes the information complemented during image generation and, in a \nsense, re-links the complemented texts and images, this suggests another usage as an advanced \nautomatic caption generation function, and examples of such usage are available. \nContrastive Language–Image Pretraining (CLIP) 44 with the announcement of the DALL ･E \nperforms the high-precision prediction of image and text snippet (caption) pairs through the new type \nof pretraining that combines an image encoder45 and text encoder. There are still some major problems \nin image and its text description. The goal is to achieve what is known as the zero-shot category, \nmeaning that high-precision text caption can be predicted for image data not used for its training. \nOne of the key ideas is a method that is simple but effective, namely the learning through a large \namount of data, which has been shown in many examples thus far. While ImageNet uses 14 million \nimages for its training, CLIP uses a large-scale dataset that contains 400 million pairs. Moreover, CLIP \ntraining uses pairs of various images and their captions on the Internet to realize the zero-shot function. \nIn the actual training, an image and its corresponding caption are not directly optimized. Instead, it \npredicts the text label that has the closest relationship with the image. \nCLIP selects a text caption from 32,768 randomly sampled captions. This is efficiently performed \nthrough Transformer. Let us look at an easy example. If the candidates for the caption include “a dog \nrunning,” “a cat sleeping on a sofa,” “a playing dog,” and “a dog sleeping in its kennel,” and the image \nshows a “cat,” “a cat sleeping on a sofa” has the highest probability to be the correct caption and is \ntherefore selected. Images and text captions are linked through repetition of this process. Distributed \nrepresentation of images and distributed representation of texts are being learned. Languages and \nimages are strongly linked through the connections at the word and image fragment level. By using \nCLIP, labeling now done by humans can be completely automated. As labeling of images is very \nexpensive, there are high hopes for its application. \n \n \nPower Apps46 \nMicrosoft is in partnership with OpenAI. Afterthe release of GPT-3, it signed an exclusive contract \nwith OpenAI for its usage47. The development platform of OpenAI has moved to Microsoft Azure, its \ncloud environment. During the developer’s meeting, Build2021, it was announced that this powerful \nnatural language model GPT-3 will be used for Power Apps, and several demonstrations were \nconducted. Power Apps, the released product, is popular as a low-code application development \nplatform among wide-ranging users, from so-called Sunday programmers to professional developers \n15 \n \nwhose job is programming. \nWhen GPT-3 was integrated into Power Apps, it became possible to build applications from \ndescriptions in natural language without considering codes and mathematical formulas. It \nautomatically converts explanations in English into Power Fx. As the NLP function of GPT-3 is \nintegrated into Power Fx48, it also became possible to build applications by supplying samples through \nprompts. \nThe new NLP function enabled by GPT-3 will be integrated into famous major products such as \nAzure in the future49. As the exact same result as the description through mathematical formula can be \nachieved through simple words, anyone can create an application that even includes AI. \nThus, Transformer and Attention, which is one of its core technologies, conduct learning with an \nextremely large number of parameters containing vast amounts of data using deep learning, which is \nthe basic technology of AI software today, as well as GPGPU, which enables efficient usage of GPU \nas hardware. This situation was explained through the example of GPT-3, which caused quite a shock \nwhen it was released. The technology improved even further following this, as it generated competition \nto release new technologies. Some of these will be discussed later. \n \n3 Trends in Japan \n3.1 Accommodating the Japanese language \nIt would be difficult for a single Japanese company to create a super-large-scale natural language \nmodel as good as the GPT-3. Thus, we must consider how NLP technology should be approached in \nJapan. For instance, one approach is to have several organizations collaborate to build a super-large-\nscale natural language model. If one is to build a natural language model on the scale of GPT-3, the \nnecessary calculation resources are estimated to be equivalent to using RAIDEN of RIKEN for 24 \nhours every day for a year. (As mentioned earlier, if limited to natural language, the calculation \nresource capability improved by 600 times in the last four years. This speed of improvement is \nprojected to stay the same according to the roadmaps of various companies involved in GPU. \nTherefore, one can also take a position that, as long as hardware can be updated, the calculation \nresources per unit time will eventually stop being a problem.) However, aside from the calculation \nresources, there is the issue of how to collect training data. This is a problem caused by the fact that \nwe use Japanese, which is a language unique to the country of Japan. Even though, as long as there \nare texts, they can be used as training data, a significant amount of text data that is comprehensive and \nwithout bias is necessary to build a practical general-purpose natural language model. This is an \nespecially serious issue. \nAnother approach to address this issue is to improve the algorithm. For example, the same level of \ncapability to that of GPT-3 was observed in PET/iPET50 only with 230 million parameters. Moreover, \nit exceeded the score of GPT-3 in SuperGLUE51, w hich i s a  b enchmark. I t i s a  s emi -supervised \n16 \n \nlearning method where additional training data is generated from few-shot examples using the preleant \nALBERT52. PET functions by first converting entered examples into cloze-style phrases. These are \nused to fine-tune natural language model ensembles. Then they are used to attach annotations to large \nunlabeled datasets to generate datasets with soft labels. The final model is fine-tuned through the soft-\nlabeled data. This soft-labeled training data is used for the main training. In this case, the training cost, \nincluding that of resources, can be reduced to about 1/500, meaning that if the computing environment \nwere the same, what took 500 days before would be ready in less than one day of calculation. It enables \none to develop new strategic methodologies, such as shortening of training time or scaling down of \nthe computing environment. \n \n3.2 Resources in Japan \nFor reference, the representative natural language models (BERT) that are available in Japan are \nlisted below. Most of them take the form of free use open source licenses53, and one can use them at \none’s own company. As it provides a general-purpose model that already completed pretraining, it \nenables trials to easily be run on tasks because one does not need to build pretraining from scratch, \neven though it will require zero (few)-shot learning when used at one’s own company. Currently, there \nare many examples of business applications using BERT, which is relatively agile, instead of using \nsuper-large-scale GPT-3. Moreover, it is also true that there is still a large scope for innovation as the \nunderlying technology, as the example of PET/iPET shows. It is important that today, by using the \nBERT pretraining model, each user actually can apply it to problems to be solved and tasks to be \nperformed. The BERT-type Japanese model (Large) released by the Kawahara Lab at Waseda \nUniversity is one of the most complete models in Japan54.  \n \n \nTable 1 Japanese BERT Models (Sources: Each URL) \nProducer \nSoftware \nframework \nUsed Data \nopen \nsource \nlicense \nURL \nGoogle \nTensorFlow 2 \nJapanese \nWikipedia, etc. \nApache 2.0 \nhttps://github.com/google-\nresearch/bert/blob/master/multilingual.md \nKyoto univ. \nKurohashi-Chu-\nMurakami Lab. \nWasade univ. \nKawahara Lab. \nTensorFlow 2 \nPyTorch \nTransformers \nJapanese \nWikipedia, \nCC-\n100 \nApache 2.0 (Bert \nbase) \nCC BY-SA 4.0 \n(RoBert base) \nhttps://nlp.ist.i.kyoto-\nu.ac.jp/index.php?ku_bert_japanese \nhttps://huggingface.co/nlp-\nwaseda/roberta-base-japanese \nhttps://huggingface.co/nlp-\nwaseda/roberta-large-japanese \n(Newer RoBERTa-base) \n17 \n \nTohoku \nUniv. \nInui-Suzuki Lab. \n \nTensorFlow 2 \nPyTorch \nTransformers \nJapanese \nWikipedia \nApache 2.0 \nhttps://github.com/cl-tohoku/bert-\njapanese \nNICT \nTensorFlow 2 \nPyTorch \nTransformers \nJapanese \nWikipedia \nCC BY 4.0 \nhttps://alaginrc.nict.go.jp/nict-\nbert/index.html \nGPT Model and CLIP \nrinna corp. \n(GPT) \nPyTorch \nTransformers \nJapanese \nC4, \nCC-100, \nWikipedia \nMIT \nhttps://huggingface.co/rinna/japanese-gpt-1b \nrinna corp \n(CLIP) \nPyTorch \nTransformers \nCC-100, \nWikipedia, \nCC12M \nApache 2.0 \nhttps://huggingface.co/rinna/japanese-clip-vit-\nb-16 \nrinna corp \n(CLOOB) \nPyTorch \nTransformers \nCC-100, \nWikipedia, \nCC12M \nApache 2.0 \nhttps://huggingface.co/rinna/japanese-cloob-\nvit-b-16 \n \nOn January 2022, the Japanese GPT language model was launched 55. R inna, w hich b ecame \nindependent from Microsoft, released a natural language model specialized for Japanese with 1.3 \nbillion parameters. It is provided under the MIT license and can be used commercially. The release of \na Japanese GPT model that is more advanced than BERT is attracting attention, as it can be easily used \nin a relatively short amount of time after only few-shot learning, such as prompt learning or fine-\ntuning according to the purpose of each practical application. On May 12, 2022, the same company \nreleased CLIP and its improved version, CLOOB 56. T he l icensing o f b oth o f t he se allow for \ncommercial usage. They will be discussed together.  For more information on recent Japanese LLM, \nplease refer to the summary site57 and the LLM study group58 set up by top Japanese researchers. \n \n3.3 Discussions within Japan \nThe LINE dialog system won two prizes during the dialog system symposium competition of The \nJapanese Society for Artificial Intelligence, and in its evaluation, it was commended for being \nindistinguishable from humans. LINE entered the competition with a dialog system utilizing the large-\nscale Japanese language model59 developed with NEVER from South Korea. LINE won first place at \nthe Open track and Situation track of live competition 4 at the dialog system symposium. “A \nHyperCLOVA60- based Chatbot with Unified Persona-Consistency Consideration and Knowledge \nBase” won top scores in two categories. \nIt is inferred that development took about a year since the release of the general-purpose natural \n18 \n \nlanguage model by LINE in November 2020. At the point of its release, it was the first super-large \nnatural model in Japanese, and it utilized a super computer with performance capability exceeding \n700PFLOPS. It has more than 175 billion parameters, and more than 10 billion pages of Japanese data \nwas prepared. Indeed, looking at the actual development history, up to 39 billion parameter models \ncan be used in its Japanese model, and up to 82 billion in can be used in its multilingual model that \nincludes Japanese. \nDuring the explanation for developers in November 2021 by LINE, a detailed evaluation of the \nfollowing topics was offered: Its natural response, i.e., whether conversations are smoothly conducted; \nif it is following a topic, i.e., whether it can follow the topic in its responses; if it is providing a topic \nor asking a question, i.e., whether it can provide a topic at each response; and its achievement of goals, \ni.e., whether the purpose of the dialogs are being achieved. According to the explanation by LINE, the \nresult of performing these evaluations on the natural language models with different parameters during \nvarious tasks showed that the 39 billion parameters model scored the highest at almost every task. \nHowever, there were also aspects that require further improvement. For instance, the response to users \nwith negative feelings requires a considerable amount of improvement. \nIt is an important result indicating that the preparation and application of large-scale natural \nlanguage models of Japanese have started, and that they can achieve satisfactory results. In 2023 July \n4th, NICT reports that it has created a pre-training LLM with high-quality Japanese text data.61 \n \n4 Large-scale language model that can potentially lead to general-purpose AI \n4.1 Foundation Models \nAt Stanford University, very large natural language models are being proposed as foundation models. \nThis is because the result of the evaluation of multimodal activities using them demonstrated that they \ncan be the basic model not only for language by also for various AI tasks62. At their workshop63, they \ndiscussed not only the technological aspects, but also much wider subjects, taking into account the \nfact that the realization of general-purpose AI is becoming realistic, which they believe will play an \nimportant role in the future society. \n \n \n \n \nFig. 6 Foundation Models is trends of Deep Learning54 \n \n19 \n \nAs a basic concept, Deep Learninghas been developed around features, while it is said that \nfoundation models are further developing in this vein and placing an emphasis on “function.” Fig.6 \nshows the overview of AI trends. \nThe large-scale natural language models that were trained with large-scale and wide-ranging data, \nin other words the foundation models, are models that can accommodate diverse downstream tasks64 \n(BERT, GPT-3, DALL-E, CLIP, etc.), and they clearly have achieved a paradigm shift. Naturally, \nfoundation models are extensions of conventional deep learning, and even though they are based on \nself-supervised learning and transfer learning, they do not appear to be particularly unique at first, \naside from their vast numbers of training data and training parameters. However, their performances \nappear to offer new emergent functions, and their effectiveness has been demonstrated in an extremely \nhigh number of tasks. \nStanford's argument starts at this point. One model can accommodate every task, even multimodal \ninter-task tasks, providing powerful leverage, but it also means all its downstream adaptive models \ninherit the defects in the foundation model.  Thus, one must pay the utmost attention to its influence. \nWith foundation models, which have a far larger number of parameters compared to preceding deep \nlearning, this is even more pronounced. Despite wide-ranging benchmark results, there is still no \nsufficient understanding of how the models might function, how they can potentially fail, and what is \nmost important for emergent characteristics. \n \n \n \nFig. 7 The scope of discussion in Emergence and homogenization54 \nOne of the guiding principles of Stanford today is to take this situation seriously and to \nsystematically address it while following the major flow in the field. They examine the potentials and \nrisks of various subjects, from the functions of foundation models (language, vision, robotics, \nreasoning, interaction with human, etc.) to their technological principles, (model architecture, learning \n20 \n \nprocedure, data, system, security, etc.) and they discuss their possible applications (e.g. law, healthcare, \nand education) as well as their possible social impacts (e.g. equality (inequality), misuse, financial and \nenvironmental impact, and legal/ethical considerations). A diverse number of topics have been/should \nbe discussed. Fig.7 lists the important keywords. In addition to technological aspects, they also \nseriously consider how technology groups are positioned within society, clearly indicating the \npossibility and influence of these technologies. \n \n4.2 Essence of natural language models \nNow, why do natural language models have such multimodal character? This was also debated \nduring the workshop. In principle, a consensus is forming that this is due to an extremely simple reason. \nNamely, as language has the capacity to represent almost all human activities, converting it into objects \nand circumstances is thought to be relatively easy once its representational ability is modeled. \nTherefore, as natural language models learn distributed representation without any distinction, its \ntraining data is diverse and wide-ranging. Thus, it would contain a large amount of objects and \ncircumstances, allowing it to reach the level where it is capable of text description regardless of \nmodality. Lastly, when this distributed representation can be converted back to objects and \ncircumstances, they are recreated as multimodality. \nFig.8 shows a comparison of a common natural language model and foundation models. One can \nunderstand foundation models as something that developed from natural language models, which only \nhandle texts, and expanded into multimodality, while still centered on text. \n \n \nFig. 8 From NLP models to Foundation Models54 \n \nFoundation Models significantly improve the NLP system further, which clearly functions in the \nway similar to humans. Moreover, the language systems they acquire or the learning process they use \n21 \n \nwill likely to be completely different from those of human language learning. Understanding the \nmeaning of the difference between NLP, which is obtained as the result of machine learning, and \nhuman language learning is extremely important for comprehending the limit and potentials of more \npowerful foundation models that will appear in the future, and for effectively utilizing them. For \ninstance, human language acquisition appears to be extremely efficient compared to machine learning. \nFor example, foundation models such as GPT-3 must learn language data that contains three to four \ndigits more words than most people would hear or read in their lifetimes. No children would grow up \nin such an environment. \n \n4.3 Robotics and large-scale natural language model \nWhen building a large-scale language model, the model is trained with an extremely large amount \nof text data. The emergence of four arithmetic operations as a result of this was discussed earlier. \nMoreover, it is natural to consider the possibility of functions other than language emerging from the \ntraining data. In the following, discussions on robotics, which do not appear to be close to NLP at first, \nwill be presented. \nFig.9 shows the representative input data of foundation models and the case where the same is \napplied to robotics. It becomes clear that robotics must function as though it was an integrated system.  \n \n \n \n \nFig. 9 Robotics as an example of using Foundation Models54 \n \nIt is expected that whether foundation models can become literally foundational or not will be made \nclearer by considering their application to robotics. Furthermore, the fact that robots must interact with \n22 \n \nreal environments is also very important. That is because it must clearly solve the symbol grounding \nproblem, which is also often a problem for general-purpose AI. \nRobotics requires large-scale datasets that cover diverse environments and operations. Currently, \nvarious approaches are being taken, including in regard to how data should be collected in real and \nvirtual environments. Even without the application to robotics, simulations, interactions between \nrobots, videos of humans, and explanations of operations/environments through natural language are \nall useful as data for foundation models. The fact that evaluations of success/failure from more \ncomplex data sources are required, and that the tasks involve the same operation under different \nenvironments, normally in zero-shot, significantly increases the difficulty. \nDue to this fact, data with multiple modalities transcends the framework of NLP since they contain \nvideo. On the other hand, it is inferred that by taking into account the characteristics of the foundation \nmodel, a wide array of issues could be resolved. This includes specifications for robotics, \nsupervised/unsupervised learning and reinforcement learning65,66. \n \n4.4 Correspondence with the real world. \nIssuing orders, or inputs, to robots in normal language is one of the dream technologies. To achieve \nthis, robots must understand the common sense and premises that supplement human conversation. \nThus, considering input to robots as an extension of the aforementioned DALL･E and CLIP would be \nextremely useful. If the content of a text can be converted into images, it can potentially be turned into \nrobot operations in a similar way. There is a report stating that the image capture generating capacity \nof CLIP is superior to models using deep imaging or room layout mapping in the Room Rearrangement \nChallenge67. This indicates that the arrangement of objects is accurately grasped. With this in mind, \nlet us think of a general and ideal interaction with a robot. \nLet us think of a situation where, while following an input stating “prepare breakfast,” the robot \ninfers/supplements all other elements to prepare said breakfast. A human would ask about unclear \ninformation on the spot to supplement it. Furthermore, he/she would first check what ingredients are \navailable before preparing the breakfast. In addition, if one does not understand the concept of the \nplace called the kitchen, one cannot prepare breakfast to begin with. To take real action, a robot must \nprocess numerous conditions and adapt to its environment. To solve this issue, it is necessary to \nintegrate all the information and link the same to physical realization in response to text (or speech) \ninput. This is significantly different in its character from the research problems of NLP or image \nprocessing (computer vision). At Stanford University, this possibility is being verified by using \nfoundation models as the foundational models for robotics. This is one of the approaches to realize the \nusage of robots in everyday life. \nIn the high-level context of breakfast preparation, there is a significant amount of ambiguity, and it \ninvolves interdependent and interacting tasks. Furthermore, as it involves interaction with the physical \n23 \n \nworld, every task requires details to be concretely defined. In order for a task to be sufficiently concrete \nand clear and to achieve its objectives, it must be broken into sub-tasks, and sub-objectives must be \ndesignated. Following this, the task to be executed can be specified. While natural language models \nare currently not able to realize every task, one can say that they have more than sufficient potential \nto be foundation models. Needless say, foundation models that can adapt to a body that interacts with \nits environment, like a robot, are more evolved general-purpose AI. \n By understanding the purpose of a task, the robot will become capable of diverse conditioning \naccompanying a hierarchical task structure. As task specifications are normally provided by humans, \nquantifiable metrics for objectively measuring the task itself, to understand how a robot actually \nevaluates and completes the task, are not always available. Foundation Models function to convert the \ntask specifications written in a natural way by humans into the task specifications for robots. This \nconversion includes a self-evaluation function through a reward function and signals necessary for \noperation. Thus, it will also become possible for the robot to evaluate the task by itself. They believe \nthat, as a result, it will become possible to provide optimization of robot operation, diagnosis of \nobstacles and feedback to humans. \nFor instance, it is already known that it is possible to encode the general explanation of a kitchen, \nsuch as the facts that the oven is normally placed by a wall and it must be switched on to produce heat, \ninto the prior information learned through the offline video of human interactions with robots, texts \nand/or simulations. Through such common sense knowledge, physical prior probability and visual \nprior probability, the robot is expected to become capable of adapting to new environments more \nefficiently. Similarly, by using many cooking videos in the training data set, it will become possible to \nlearn how to adapt to the taste of a small number of users, or that of a specific one, from the \nfoundational model of robot task training through adjustment of the data, in addition to learning the \npolicies of common skills such as “frying an egg.” With a trained foundation model, one can simply \nprovide a description saying “I want to eat breakfast,” and then it will prepare a fried egg cooked in \nthe way one likes, as well as bread and coffee. \nFrom another perspective, in the application of foundation models to robotics, there are major issues \naccompanying physical realization, namely safety and robustness. As mentioned earlier, this is the \nperspective that makes it significantly different from NLP or computer vision. Needless to say, what \nis ultimately necessary for a robot is its operation in a real environment, regardless of how satisfactory \nits performance in a virtual space is after training with multimodal data. On the one hand, it is \nextremely important to ensure safety and robustness during this operation. On the other hand, this \nwould make development of a foundational model even more complicated. That is because this \nperspective is not necessary contained in the data. Another problem is the fact that the data collected \nin real environments contains information that does not exist in foundation models, because its \ncollection involves direct interaction with environments in physical worlds. It is not difficult to \n24 \n \nimagine that such information will be closely related to safety. This is normally handled by restricting \nthe system to limit its interaction with the real world. The task is learned under this environment \nwithout making critical mistakes. \nThere is often debate about the chicken or the egg problem, where safety limitations for the system \nmust be assigned before data can be collected. It is imperative  to consider many factors when \ninteracting and training in an environment. Take training in the kitchen as an example. In the kitchen, \nthe system must learn to detect anything that can break. One also has to consider whether items that \nmay break should be replaced when collecting data. Alternatively, the environment should be left as it \nis so that the system will learn that these items can be damaged. When conducting training without \ninterruption, one must choose the former. However, the latter is necessary to generalize various \nstimulations and unpredictable behaviors. The causal analysis of agents68, safety evaluation tools and \nrealistic simulation environments69,70,71 are being debated. \nThus, one can conclude that robotics and foundation models have an extremely close relationship. \nAt the same time, data is even more important in this field. Significant effort is required to collect or \ngenerate data that covers the wide range that includes diverse environments of the physical world and \nvarious states of execution. Moreover, this data is directly connected to the safety and robustness of \nthe system. This offers a very important perspective. \n However, it is not as simple as the adaptation of foundation models by robotics leading to the \nrealization of an ideal robot. For instance, in order for a robot to reproduce the action of closing buttons \nwhile putting on clothes, a foundation model alone is insufficient. That is because, while the \nfoundation model would contain the expression “closing buttons,” the specific procedure would not \nbe outlined. To be exact, the action of closing one button would involve holding a button, holding the \nedge of the front side of one’s top (crest) corresponding to the position of the button, inserting the \nbutton held at right angles into a buttonhole from the back, and then making the button horizontal \nagain when releasing it. No such descriptions would be contained in normal text data. Folding clothes \nis a similar case. There is a possibility that foundation models are not actually suitable for actions \nhumans perform without thinking. One can only hope that this will be addressed by future \ndevelopments. \n \n5 Introduction of the topics related to natural language models after GPT-3 \nIn this section, the technologies released after GPT-3 will be briefly discussed. These technologies \nare directly or indirectly linked to GPT-3. \n \n5.1 MT-NLG72, a powerful natural language model by Microsoft and NVIDIA \nMT-NLG is a natural language model developed by Microsoft and NVIDIA, and it has 530 billion \nparameters, which is 3 times more than those in GPT-3. Consisting of 105 layers, it is currently the \n25 \n \nlargest natural language model. At release, it was announced that its capability exceeded the \nperformance of all existing NLP models. Training with such a large number of parameters required \ncoordination with hardware, and realization of such was only possible through collaboration between \nMicrosoft and NVIDIA. Its heart is a super computer consisting of 560 DCG servers, each one \nequipped with eight A100 GPUs that contains 432 tensor cores for machine learning and 80 GB RAM. \nThe size of its training dataset is 1.5 TB, comprising several hundred billion text data obtained from \n11 databases, including Wikipedia and PubMed. \nIt has been reported that MT-NLG scored high on the benchmarks for prediction of complete text \naccording to its meaning, reading comprehension, generation of logical conclusions, generation of \nconclusions in natural language and distinguishing between words with multiple meanings. \nMoreover, the developers commented that they identified an emergence of the function that \ndisplayed the comprehension of the simplest mathematics, which was also reported in other large-\nscale natural language models73. Nevertheless, the bias problem exists in MT-NLG, and scaling up \nalone will not solve it. Below is an example from MT-NLG. It is interesting that “True” or “False” \njudgments are being made, indicating thatit is capable of evaluating the content of the text. \n \nPrompt: \nThe banker forgot that the judge advised the professors. \nQuestion: The judge advised the professors. True or False? \nAnswer:(MT-NLG)True. The banker forgot that the judge advised the professors. \n \n \nPrompt: \nFamous professors supported the secretary. \nQuestion: Professors supported the secretary. True or False? \nAnswer:(MT-NLG)True. The secretary was supported by famous professors. \n \n5.2 Microsoft Project Florence-VL74 \nOne of the chief goals of artificial intelligence is to develop an algorithm that can effectively learn \nfrom multimodality data. The linking of image and text is equivalent to the linking of vision and \nlanguage we habitually perform to understand the world around us. At the moment, bidirectional \nconversing between text and image, namely searching images that correspond the best to a text query \nand generating a text explanation of an image, is becoming common. One approach to this is Florence-\nVL (Florence-Vision and Language Project) launched by Microsoft. \nWeb and social media are full of image and text pairs, which functions as an annotation of the image. \nThe corresponding can be used as data labels for supervised learning. Moreover, as many videos \n26 \n \ncontain audio channels explaining their contents, it is possible to turn such audio into text, which can \nthen beused as labels. If such data can be used for Vision-Language Pretraining, it will be possible to \ndistill cross-modal knowledge75 in the form of texts, images, and audio files, where knowledge \nlearned in one modality becomes useful for learning in another modality, and manual data labeling is \nno longeer required. \nFor Florence-VL, a large-scale model was prepared through pretraining consisting of the prediction \nof masked elements based on their context. As was the case with existing Transformer-type models, \nself-supervised learning using a large number of image-text pair data from the Web and social media \nwas employed. The cross-modal representation of this model was fine-tuned to accommodate so-called \ndownstream tasks. Microsoft presented the result of this project in a series of papers that were \npublished one after the other. UNITER, OSCAR, VILLA, VinVL, VIVO, and TAP all have their own \nunique features. For instance, VIVO achieved equivalency with humans in the new image caption task \n(nocaps). After strengthening its pretraining by using the scene text detected in images, it achieved \nNo.1 in the TAP TextCaps Challenge2021. \nMoreover, UFO and METER were developed for the end-to-end pretraining necessary for further \nimprovement of Florence-VL. In addition, UNICORN, an the integrated visual image model, LEMON, \nand SimVIM related to scaling and PICa for multimodal few-shot learning have been developed. How \nthese projects are closely linked to each other with the ultimate goal in mind can be demonstrated as \nthus. For instance, the model architecture developed through end-to-end pretraining functions as a \ncomponent of the integrated visual language modeling can be increased and turned into an integrated \nsolution by using another method developed for upscaling. This would naturally enable usage of the \nfew-shot learning function. As a result, and by using several examples in the context, an integrated VL \nfoundational model is born, and it can easily adapt to various tasks. \n \n \n5.3 BigScience T0 76 \nAs mentioned many times in this paper, it has been reported that recent large-scale natural language \nmodels display generalization through zero-shot learning in diverse tasks77. A hypothesis proposing \nthat this is the result of multitask learning in natural language model training has been put forward78. \nHow does generalization in zero-shot learning work? How does multitask learning induce it? To \nconduct a large-scale test for answering these questions, T0, a system that can perform simple mapping \nof common natural language tasks in a prompt format humans can read, was developed and used for \ndetailed analysis, and the result was published. It converted large-scale supervised datasets, and several \nprompts using various natural languages were prepared for each dataset. \nThe prelearnt encoder/decoder model79,80 is fine-tuned using this multitask prompting that covers \ndiverse tasks81. \n27 \n \n The result successfully answered unknown text questions. This means zero-shot learning was \nachieved. The zero-shot performances after the training using several standard datasets displayed \nmany results that were superior to other models, whose sizes were up to 16 times larger than this model. \nMoreover, it achieved performance superior to other models, whose sizes were up to six times larger, \nin the subset of BIG-Bench benchmark tasks. It demonstrated the potential for decreasing the model \nsize through prompt learning. \n \n5.4 LaMDA, dialog approach of Google82 \nThe LaMDA (Language Model for Dialog Applications), which Google developed with a focus on \nconversation, saw further evolution in the field of conversation. Transformer, which was invented by \nGoogle, learns the connections between words. Thus, it predicts the subsequent words in a sentence \nwith high precision. However in conversation, that is insufficient, and it often has to understand the \nhidden nuance in each word before performing its prediction. Therefore, LaMDA employs a method \nto learn from conversations. \nTo appropriately respond during a conversation, it is insufficient to simply return a response that \ncorresponds to the words being used. Instead, it is necessary to understand the context of the topics \nbeing discussed in the conversation and then offer a concrete response that is specific to this context. \nAs Google itself explained, the probability of a response saying “that is good” being appropriate is \nconsiderably high. However, as is the case in human conversation, it is insufficient to continue a \nconversation through such unspecific responses, and evaluation of how vibrantly the ongoing \nconversation is going is also important for that conversation to develop and expand. \nA conversation does not have a landing point, i.e., a final destination, at first, and its topic may \nchange significantly toward the end. Such a situation is sometimes praised as being full of insights, or \nwitty. Regarding this, LaMDA learns the relevancy between topics, which cannot be learned only \nthrough word-level relationships, through conversation learning. Naturally, it is starting to examine \nhow to offer convincing answers that also include fact checking, which has become a problem in recent \nyears. It states that it will continue the effort to integrate conversational ability into more products. \nLaMDA2 was introduced during the developer summit in May 2022. LaMDA2 was refined through \ntrials by a few thousand staff members of the company. One of the function demonstrations of \nLaMDA2, named ‘Imagine It,’ is capable of expansion into clearly relevant topics even without \npreviously defined answers directly related to human input. For instance, it can expand the \nconversation in this way: Deep sea → (Image of) Mariana Trench → Creatures, submarine… through \nquestions that stimulate the dialog. This is the result of the model integrating and responding to the \ntraining data. They declare that backtracking never occurs in the conversation, and it is capable of \ncontinuously expanding dialog. \n \n28 \n \n5.5 DeepMind RETRO (Retrieval Enhanced TRansfOrmers) \nWith conventional large-scale natural language models, the size of the model and data is related to \nthe performance of each, meaning that a larger size will lead to better performance. In response, \nRETRO successfully maintained the performance of the model at a smaller size through direct retrieval \nof training datasets by utilizing search. As a result, it realized significant performance improvement \ncompared to the standard Transformer-based model with the same number of parameters. \nTraining of a natural language model requires vast computational resources. For the training of \nlarge-scale natural language models, such as GPT-3 with 175 billion parameters and Microsoft with \n530 billion parameters, the computing power required is too large. Thus, RETRO uses an external \ndatabase to reduce the training cost without shrinking the training sample size. In other words, it \noutsources some of the parameters, which the model is expected to have within it, to an external \ndatabase. This can be understood through simplification of the Transformer mechanism. The weight \nof the context of the word in focus is normally heavy. Therefore, it is obvious that the best example of \na text with heavy weight in relation to the word in focus is the text that contains that very word. Thus, \nit is logical that the model functions sufficiently when it uses the text data itself for the weight of words \nneighboring the word focus while using the training in Transformer for farther relationships. This \nreduces the number of necessary parameters, and sufficient performance can be achieved through \ntraining with fewer computing resources. The above explanation uses an extreme example. In reality, \nthey devise adjustments because the neighbors cannot be ignored completely. \nRETRO trains its model using a dataset composed of news articles in 10 languages, including \nEnglish, Spanish, German, French, Russian, Chinese, Swahili, and Urdu, Wikipedia texts, books, and \nGitHub texts. The RETRO neural network only has 7 billion parameters. Therefore, it employs a \nmethod whereby they are supplemented by a database that contains approximately two trillion text \npassages. \nWhen RETRO generates text, it performs a nearest neighbor search for each segment (equivalent to \na paragraph of a text), and returns a similar text found from its training database and results that \ncontinue the aforementioned text. These arrays are useful for predicting subsequent texts from the \ninput text. The architecture of RETRO combines the common Self-Attention at the text level and \nCross-Attention with neighbor search at the finer passage level. As a result, it can realize more precise \ncontinuation based on facts. Furthermore, RETRO improve results through the direct control of search \ndatabases to increase the interpretation possibilities of model prediction and improve the accuracy of \ntexts. It uses the database to search for passages that are the same as the written text and compares \nthem. This enables it to perform more precise prediction. The performance of the natural language \nmodel continuously improved as the size of the search database increased. At least up to two trillion \ntokens have been verified, and the two trillion tokens of training data was used as an external database \nas is. \n29 \n \nIn the experiment using Pile, which is a standard natural language model benchmark, the RETRO \nmodel with 7.5 billion parameters displayed performance exceeding that of Jurassic-1, which has 175 \nbillion parameters, in 10/16 datasets, and displayed performance exceeding that of Gopher, which has \n280 billion parameters, in 9/16 datasets. Moreover, satisfactory performance can be achieved at 1/16 \nthe parameter size. \nMoreover, the database is capable of updating the neural network without relearning, thereby \ndemonstrating that it is capable of the speedy addition of new information and deletion of old or \nincorrect information. DeepMind claims that external memory systems such as RETRO are more \ntransparent than black box models such as GPT-3. WebGPT83 by OpenAI employs a method to reuse \nthe Web as the database. \n \n5.6 South Korean LG EXAONE84 \nLG released EXAONE, a giant AI comprising a large-scale natural language model produced in \ncollaboration with Google, is also the largest AI in South Korea. As LG is a conglomerate engaging in \ndiverse businesses, employing various AI applications centered on a natural language model, which is \nthe concept outlined by foundation models, makes sense as its corporate strategy. One can say that \nEXAONE plays a central role in this strategy. \nIt uses a text corpus comprising 600 billion phrases and more than 250 million images as its training \ndata, which combines these two types of data. It uses 300 billion parameters. In the explanation by \nLG, EXAONE is claimed to be an AI that learns big data by itself, and is capable of thinking, learning, \nand making decisions like humans, which can be applied to diverse fields without being limited to \nspecific purposes. Needless to say, EXA is also the prefix “EXA” meaning 10 to the power of 18 (i.e., \na quintillion). If all the words humanity ever used were saved as data, its size is estimated to be \napproximately 5 Exabytes. LG explains that the name EXAONE implies this idea. \nLG AI Research changes the parameter number of the neural network in stages. It has been \nincreasing its size in stages from 1.3 billion to 13 billion, 39 billion, and 175 billion to study its \ncharacteristics. Its result and previous reports suggest that, in theory, training of AI based on deep \nlearning would be more refined when there are more parameters. In multimodality learning, which is \nmutual learning of texts and images, a larger number of parameters is clearly an advantage. EXAONE \nachieved this through large-scale parameters. \nSimilar to other models, EXAONE creates a hat with a pumpkin pattern if you tell it to “make a hat \nwith a pumpkin pattern.” LG AI Research declares that EXAONE reaches the state where such output \nis made through inferences and creations that exceed the level of mere understanding of input and data. \nLG AI Research published a video of EXAONE preparing a Christmas party inside a metaverse space. \nThe video shows the process whereby it understands the intentions outlined by the client, makes \njudgments and creates the decoration. \n30 \n \nAccording to the plan of LG AI Research, EXAONE is expected to become the “specialist AI for \nthe top 1% standard” in virtually every field, including manufacturing, research, education, and \nfinance. First, it opened API to the companies within the LG group to encourage them to use EXAONE. \nThis enabled companies with the LG group to use the giant AI for every business field of LG, \nincluding electronics, chemistry, and communication. It appears that each company is actually using \nEXAONE and achieving results, including the advancement in the performance of chatbots and the \nextraction of new materials/new substances through analysis and learning of approximately 20 million \npublications in the chemistry field from last 100 years. \nLG group is in close collaboration with Google toward the completion of EXAONE. The cutting-\nedge AI chip “TPUv4,” which Google released on May 2021 and is not yet commercially available, is \nused to develop EXAONE. Moreover, LG is collaborating with Google Brain for its software. LG has \nstated that the future goals for EXAONE include extension of its parameters to 600 billion, as well as \nthe formation of global allied forces of national and international AI to create a colossal AI ecosystem \nthrough collective intelligence; indicatingLG belief that AI strategy will play the central role in its \nfuture development. Google Brain is collaborating with them to achieve its goal of breaking nvidia's \nstronghold. In fact, cooperation with LG is significant for Google. In AI learning, NVIDIA has more \nthan 80% of the market share, representing a de facto monopoly. If LG manages to create a B2B \nbusiness model built upon its affiliated companies in the infrastructure field, Google would obtain a \npowerful reference for challenging NVIDIA. \n \n5.7 Chinese BAAI Wu Dao 2.085 \nThe Beijing Academy of Artificial Intelligence (BAAI, Chinese name: 北京智源人工智能研究\n院) released a pretrained deep learning model Wu Dao2.0 with an astonishing 1.75 trillion parameters, \nwhich is described as “the first in China” and “the biggest in the world.” They stated that, in addition \nto the simulation of conversational speech, writing of poetry and comprehension of images, it can even \ngenerate recipes, and everything NLP models could do in English is now possible in Chinese. The \nmodel was trained with 4.9 TB of images and texts, and this includes 1.2 terabyte of texts in two \nlanguages, namely Chinese and English. WuDao 2.0 already has 22 partner companies, including the \nsmartphone maker Xiaomi and the major short video company Kuaishou. \nBAAI has reported that this natural language model has achieved the state-of-the-art (SOTA) level \nin nine benchmarks, and even exceeded it in some of them. When they released Wu Dao2.0, they also \nunveiled Hua Zhibing, the first Chinese virtual student in the world. Hua is said to be capable of \nlearning by herself, drawing pictures and writing poems. During the presentation, it was declared that \nshe will be able to learn how to code in the future. These are unique abilities not found in the original \nversion of GPT-3. However, it has been reported that GPT-3 also becomes capable of generating codes \nafter few-shot learning. While the details of the training data of Wu Dao are unknown, it is possible \n31 \n \nthat the difference in the content of the training data is reflected in its early functions. \n \n5.8 OpenAI GLIDE, DALL･E2 \nCLIP is capable of reading the relative positions of objects in an image because it assigns detailed \ncaptions to images. One of the problems it has is that there is a limit to its capacity when dealing with \nmore abstract expressions or certain types of tasks, such as counting the number of objects in an image \nor measuring which car is the closest one from the camera in an image with several cars. The answers \nit gave to these tasks were far from accurate, only slightly better than random estimation. Another \nproblem was that it could not give satisfactory results, even with more detailed classification (such as \ncar model categories and flower classification). \nWhile CLIP is an unprecedentedly powerful zero-shot classifier, there are occasions where it cannot \ncope with changes in word choice or turns of phrase. When this occurs, trial and error through prompt \nlearning becomes necessary, requiring it to respond to each case individually. This necessitated its \nreexamination, as well as that of GAN86, which is its conventional generative model. \nGAN is a generative model with extremely good ability, and numerous examinations were \nperformed to realize satisfactory generative function. Currently, it requires advanced tuning of its \nhyperparameters and regularizer. This means that it is extremely difficult to apply it to domains other \nthan the one it already learned. Thus, there is always a trade-off between significant quality \nimprovement within its scope of application and considerable quality decrease outside this scope. The \ndiffusion model was proposed as a solution for this problem other than GAN. \nOpenAI has reported that the improved version of this diffusion model achieved better \nperformance than GAN87. While GAN uses noise vectors when generating images, in principle, there \nis no guarantee for these generated images. This is the reason why it requires the aforementioned \ntuning. To improve this situation, the diffusion model gradually adds Gaussian noise the original image \ndata, replacing the data with noise until the original data is completely lost, replaced entirely by noise. \nThis procedure follows the Markov process. In the training of the generative model, the data is restored \nin reverse while removing the noise. This means the control of generated images, which was difficult \nfor GAN, could be learned from the beginning to the end through the relationship between noise and \nimage. \nGLIDE applied this diffusion model to the model intended for text → image. To be specific, it \nemployed a diffusion model with 3.5 billion parameters using a text encoder, whose condition is \nnatural language description, for its training. GLIDE received an 87% more positive evaluation in \nbeing photo realistic, and 69% more positive evaluation in caption similarity, than DALL･E, its \npreceding model. Rendering of these images can be performed in zero-shot. However, the cases where \ngeneration of photorealistic images was difficult when the texts used were complex were identified. \nIn response, the model was fine-tuned to add an editing function, making it capable of painting parts \n32 \n \nof an image, to enable it to generate high-quality images from more complex text. When a part of an \nimage is painted over and then text is entered, the part that was painted over changes according to the \ntext. Moreover, this changed part contains shadows and reflections that match the style and lighting of \nthe surrounding context, enabling synthesis that can blend in with the surrounding. \nDALL･E 288 was released on the 6th of April, 2022. Thanks to unCLIP, which adapted the diffusion \nmodel, its significant improvement from DALL･E is immediately apparent. An example of an image \ngenerated by DALL･E 2 through text input immediately shows that the text content and the image \ndata representing it are more detailed. The evolution of its image generation function from text is so \nadvanced that one cannot help wondering if the model acquired imagination and creativity. \nFurthermore, unCLIP is expected to have a representational ability that is superior to that of CLIP. For \ninstance, it is highly likely that it will be able to generate a perfect digital twin and world model only \nfrom a few images. Its future examination will be drawing attention. \n \n \n5.9 MLP89 Reexamination, beyond Transformer \nGoogle has proposed gMLP90, a network architecture only based on MLP with gating. It is the \nimproved version of Transformer without the Attention mechanism. It demonstrated the same level of \ncapability as that of Transformer in main languages and in the field of image processing. In recent \nyears, detailed analysis of why MLP such as gMLP can achieve high capability has started. For \ninstance, there is a heated discussion on whether it was due to the effect of Attention. For instance, if \nAttention can be regarded as the introduction of dynamic parameter (effective) bias, MLP can be \nregarded as static parameter bias. \nHow the way bias (weight of the attention point) changes performance is currently a very important \nsubject. MLP, which is the focus here, realizes static bias through SGU91. While Attention clarifies the \nreasoning for the parameter weight by following the context, in another word connections, \ncomputation of SGU is based on simple correlation. This is the significant difference, though \nadmittedly, this is not the most precise description. In images, their difference is especially conspicuous, \nand naturally, computation is easier in the latter. The high computation cost is the issue with \nTransformer that is often mentioned. Thus, this is attracting attention as a possible solution for \nimproving the performance while limiting the computation cost. This is one of the reasons why the \nperformance can be scaled through the size of data and number of parameters. Regardless of whether \nit is explicit or not, how to implement it while considering the weight relationship between the target \nvector and other data vectors will be extremely important. \nOther perspectives include the comparison between gMLP and ViT (Vision Transformer)92, which \nis the application of Transformer to images. One of the focuses of its comparison to other methods is \nhow the basis for image recognition is acquired. The same format is derived for the basis for the \n33 \n \nfeatures of human vision regardless of whether biological analysis93, D L o r f rame r ate o f t ensor \nanalysis94 is used. Therefore, it is likely that no problem would be caused by thinking that the general \nbasis for seeing things and distinguishing between them is limited. For instance, there is a report stating \nthat the letters and alphabets we use belong to an even more simple basis group95. Thus, it may not be \nnecessary to acquire them through training with large amounts of data. \nThe network structure of CNN in the imaging field is designed so as to explicitly derive the basis. \nHow is this basis, or the feature filter that conforms to it, extracted and structured? In Transformer, \nwhat came from where is traced by the position embedding layer, while SGU does not perform this \nprocess, and uses correlation instead. This difference is also reflected in the computation cost. \nInterestingly, in addition to the basis observed in the aforementioned visual cortex of the brain (low-\nlevel visual cortex), correlated bases composed of several figures were also observed in the visual \ncortex of the brain (high-level visual cortex)96. As the brain is a super efficient computation system \nrunning at 20W, it is inferred to make effective use of various features. Thus, it is believed to be \nadaptively utilizing various methods including the future development, instead of only using one of \nthe methods that are currently deemed to be effective such as CNN, Transformer, or gMLP. \nRecently, there have been reports on the generation of CNN by HyperTransformer 97 and on \nincreasing the speed of MLP through hash calculation. We are currently in a situation where \nTransformer, which was initially used for NLP, is showing new possibilities in the CV field98, and is \nexpected to spread into other technological fields. \nMetaFormer99 is proposed in the report, which suggests that good results can be obtained by using \nonly Transformer without assigning a token mixer module, typified by Attention. MetaFormer is an \nabstraction of Transformer whose token mixer module has not been defined, whereby the token mixer \nbecomes a kind of model architecture that can be exchanged, for instance, with spatial MLP. \nFor instance, when it is exchanged with a simple spatial pooling operator, it becomes PoolFormer. \nPoolFormer uses a simple pooling operator as the mixer. Naturally, the pooling operator only functions \nto aggregate a token with tokens nearby and average them, and it does not mix information. Moreover, \nin PoolFormer, the pooling operator does not have any parameters it can learn. The fact that it \nnevertheless displayed high performance in computer vision tasks such as ImageNet-1k and DieT-B / \nResMLP-B24 is surprising. \nWhat has been discussed thus far is the analysis of Transformer that was derived from the \napplication of Transformer used for NLP to image processing. In the same vein, there are reports that \nquestion whether Attention is essential, even though it is effective for some tasks in NLP. \nFurthermore, there is a recent discussion on intermodal training methods. In addition, data2vec100, \nwhich vectorizes all data, images, audio, and natural language in the same way, has been released. It \nacquires latent representation, regardless of the data type, through the two modes of Teacher mode, \nwhich conducts feature learning through complete data, and student mode, which predicts complete \n34 \n \ndata from masked data. It analyses which layer of Transformer to be used for masked prediction in \ndetail. The result was SOTA or its equivalent at each conventional modal. What comes after \nTransformer is already being explored, and 2021 can be deemed as the first year of this exploration. \n \n \n6 Important Challenges \nEarlier it was mentioned how the biases contained in natural language models often become a \nproblem. Let us discuss this point further. Equality and discrimination often surface as social issues. \nIt is known that a natural language model is heavily influenced by its training data, and how it can be \ncorrected so as to avoid these problems is frequently debated. As a precondition, an algorithm must be \nbalanced. However, one must be aware that bias is still possible. For instance, if LGBTK+ ID \nterminologies are excluded from the training data, or if their number is very small, they could be \nunderestimated, or completely deleted, from the data. \nIf an algorithm is to be neutral, this is in fact a correct operation. However, this would be a major \nissue for a model. The relationship between training data and the inherent bias in a natural language \nmodel is not accurately known. Therefore, it is currently believed that if a scaling law that can be \napplied to biases is established through small-scale and systematic research, it will also be possible to \napply it to large-scale data practices. The fact that every natural language model must use neutral \ndatasets that nevertheless guarantee equality is a future challenge. \nGoogle conducts especially strict studies on whether it is adhering to its own AI principles. In their \nservices, language plays a central role, and many mechanisms for avoiding the misuse of language \nhave been introduced, based on the premise that language is one of the best tools humanity has. They \nare required to be especially vigilant toward prejudice caused through misuse, malicious expressions \nand the reproduction of misleading information. For instance, with a conversation application, careful \nexamination of its results (i.e. conversation outputs), does not exclude possibilities for misuse of the \nmodel itself. Such misuse must also be prevented. \nFor this reason, Google states that the priority when creating technologies, such as LaMDA, is to \nsufficiently discuss the actions that can minimize the risks and then to execute the same. The research \nand development of these technologies over many years has addressed a number of related problems, \nsuch as how inequality bias affects machine learning models. Furthermore, based on these experiences, \nthey provide an open source resource for analyzing models and training data, and they accept open, \nexternal examinations. For instance, regarding LaMDA, they explain that it was carefully examined \nat each stage of its development. They are attempting to demonstrate their attitude toward equality in \ntheir natural language models and natural language applications. \nEfforts to address content that is incorrect, inappropriate, and/or offensive were also emphasized \nduring the Google developer summit in 2022, where they presented how they are actively utilizing \n35 \n \nfeedback from internal users. Google also reported that they are accepting methods that follow \nrepetitive and regular principles as well as the opinions of external experts, in accordance with their \nAI ethics. \nIn May 2022, Google published a report on Imagen101. It is the Google equivalent of DALL･E2 that \ngenerates images from texts. It employs the diffusion model and has updated SOTA in several \ncategories. Its comparison to DALL･E2 undoubtedly shows more refined image generation results. \nHowever, Google announced that they will not publish its demo or code due to the following reasons. \nIt employed widely used datasets for the image data in its training data (e.g., LAION-400M102). This \nset is known to contain many toxic images. While they, of course, carefully removed toxic data, the \ncuration of Web level text data used for its language model was still insufficient. Therefore, they \nreflected on how the risk of harmful stereotypes and expressions being encoded still remains. On this \noccasion, Google intentionally included toxic expressions in their evaluation. This demonstrates the \nhonesty of this report in both the state of research and development and the bias problem. \nOpenAI also released InstructGPT103, w hich i mproved t he t ext g eneration b y G PT -3 to better \nunderstand the intentions of its users, as well as to process toxic biases. For InstructGPT, the method \ncalled RLHF104, where humans are involved in reinforcement learning, is employed. By using human \npreference as a reward signal, it is possible to fine-tune the model. As the problems in text generation \nare complex and subjective, it plays a significant role in improving situations with which the simple \nand automatic metrics that exist today cannot fully cope. \n7 Conclusion \nThis report summarized recent debates, with a particular focus on developments originating from \nNLP models. It demonstrated that today, it is possible to say that large-scale natural language models \nare already at the center of general-purpose AI. It is one of the representative AI fields, and we must \ncontinue to pay attention to it. At the moment, super computer level computational resources, and \nlarger and larger amounts of data, are required for large-scale models. Thus, powerful players are \nbecoming more powerful, and it is almost impossible for new players to enter this field. At the same \ntime, it is also true that academic exploration of upcoming computation models in small-scale \nenvironments will become more and more important. The fact that Attention (Transformer) was only \na result of small-scale research and development at the time of its initial release proves this point. To \nbe implemented in society, developments must be conducted while considering how to use existing \nmodels, which, fortunately, are most often already available. \nIn a recent DeepMind report105, it was shown that the relationship between the parameter number \nof a model and the scale of its training data is not necessarily linear. In fact, it was shown that a model \nwith 70 billion parameters exceeded the performance of a model with 175 parameters in almost every \nbenchmark. Moreover, it even exceeded human scores in some benchmarks. Furthermore, Flamingo106, \nwith 80 billion parameters, was announced as a visual language model in late April 2022. Its image \n36 \n \ncontents description capability is even more refined. \nGoogle published a report on the Pathways Language Model (PaLM)107 with 540 billion parameters \non April 4th, 2022. According to the report, it updated SOTA in 28 out of 29 NLP benchmarks. What \nwas particularly astonishing was the fact that it could understand jokes and explain the points of those \njokes, something with which previous models struggled. It adapted a solution through multistep \nprompting called Chain-of-Thought Prompting to cope with a context that requires several stages of \nunderstanding. By supplying it with several question-and-answer pairs, it can learn a series of \nhierarchical structures. As a result, it became capable of hierarchical and staged understanding, \nenabling it to, for example, explain jokes, provide answers for a mathematical problem in several steps \nand generate more complex program codes. \nMoreover, when seen from the perspective of the law of scalability16, the achievement of PaLM \nsuggests that efficient learning remains a possibility when the number of parameters is increased. We \nare only starting to understand the number of parameters, scale of training data and performance of \nmodels. Thus, it is a field where future developments and deepening discussions are expected. \nIn late April of 2022, Adept108 was established. It professes Useful General Intelligence to induce \nfurther evolution of AI by building upon the developments in this field thus far. Its members are well-\nknown for NLP and Transformer, typifying this fast-moving field. It appears that one would miss major \nopportunities unless one is able to at least follow these movements. Recent engagements with general-\npurpose intelligence include GATO, released by DeepMind on May 12th, 2022. It is declared to be a \nGeneralist Agent109. Like other large-scale language models, it was trained with Transformer. It is an \nagent that functions with a single model, and performs the role of a multimodal, multitasking, and \nmultibody generalist. In response to various tasks, including Atari, image captioning, chat, and playing \nstack block with a real robot arm, a single model can supply output suitable for each context, including \ntext, movement of joints, and pushing buttons. The term generalist is indeed appropriate. This is a \nlandmark achievement of research and development, and it gives the impression that we are one step \ncloser to the realization of foundation models. \nLastly, let me add that interesting recent engagements show that the application of Transformer is \nstarting to cause major innovations in fields unrelated to language. DeepMind has triggered major \ninnovations in the structural analysis of proteins through its AlphFold110. In addition, AlphaFold2111 \nbrought even further development. While AlphaFold is based on CNN, AlphaFold2 is based on \nTransformer. This was not a coincidence, and it is natural to conclude that it was due to the high \npotential of the Transformer series. Needless to say, it is also true that as the task to convert the names \nof chemical substances into three-dimensional chemical structures involved symbols and structures \ncomposed of them, it had compatibility with Transformer. \nSimilarly, there are high expectations for Materials Informatics (MI), which is a data-driven \ntechnology to optimize/accelerate material development112. Regarding training using data, this field \n37 \n \nhas its own unique problems, namely how to effectively use the data that is much smaller than text \ndata, and how to precisely predict the structures and properties of new materials from extremely small \ndata. In recent years, it has becom clear that pretrained models are also extremely effective for MI. \nThus, it is predicted that pretrained models will continue to spread to various other fields. \nMasahiro Yamamoto, Adviser of General Affairs Planning Department \n[Inquiries] \nInformation-Technology Promotion Agency \nGeneral Affairs Planning Department \nEmail: ga-airesearch@ipa.go.jp \n \n \n \n1 Natural language processing was discussed at the Dartmouth Workshop in 1956, where the first \ndiscussions on artificial intelligence took place. \n2 https://www.ipa.go.jp/digital/chousa/trend/ai.html \n3 GPT-3: An abbreviation of Generative Pre-Training-3, which is the third generation of the general-\npurpose language model developed by Open-AI. https://beta.openai.com/docs/engines/gpt-3, \nhttps://openai.com/blog/openai-api/ \n4 Google Duplex: A.I. Assistant Calls Local Businesses To Make Appointments \nhttps://www.youtube.com/watch?v=D5VN56jQMWM \n5 Duplex is getting smarter and making life a little easier \nhttps://blog.google/technology/ai/duplex-helpful-updates/ \n6 Bringing you the next-generation Google Assistant \nhttps://blog.google/products/assistant/next-genenation-google-assistant-io/ \n7 Language model: A machine learning model for natural language processing. Models based on \nprobability are now mainstream \n8 BERT: Bidirectional Encoder Representations from Transformers. The name of the machine learning \ntechnology for language models that Google introduced to its search engine in 2019. \n9 Parameter(s): Variable(s) to be set in machine learning. \n10 Prepared independently from the following data source; \nhttps://www.youtube.com/watch?v=G5lmya6eKtc, https://ja.stateofaiguides.com/20200914-future-of-nlp/ \n11 H100 is also an NVIDIA GPU, and it uses the latest Hopper architecture released during GTC2022. \nhttps://www.nvidia.com/ja-jp/gtc/keynote/ \n12 Language modelling at scale: Gopher, ethical considerations, and retrieval, \nhttps://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval \n13 Language Models are Few-Shot Learners. Tom B. Brown et al. 2020, https://arxiv.org/abs/2005.14165 \n14 GPU: Graphics Processing Units. When this is used for general-purpose calculation, it is called \n(General-purpose computing on graphics processing units. \n15 RNN: Recurrent Neural Network. It has a network structure where a part of the output is connected to \ninput. Before Transformer, LSTM, which is a type of RNN, was the mainstream in natural language \nprocessing. \n16 Attention Is All You Need., A. Vaswani et al. 2017, https://arxiv.org/abs/1706.03762 \n17 Machine learning can be broadly divided into supervised learning, unsupervised learning, and \nreinforcement learning, and training data is necessary for the first. For details, please refer 2021 DX \nWhite Paper Appendix, Section 1, AI Technology, 3. Learning. https://www.ipa.go.jp/files/000093703.pdf  \n18BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding., J Devlin el al. \n2018, https://arxiv.org/abs/1810.04805 \n19 While Transformer was initially a machine learning method for natural language processing, its \neffectiveness when applied to images has also been demonstrated. This field is known as Vision \nTransformer (ViT). \n38 \n \n \n20Scaling Laws for Neural Language Models., J. Kaplan et al. 2020, https://arxiv.org/abs/2001.08361  \n21 This indicates learning with a large-scale corpus for the purpose of general-purpose language skills \nacquisition. It must be conducted in advance for the training for the intended task. This is expected to \nenable high performance, even when the data for the intended task is small. \n22 Language Models are Few-Shot Learners., T.B. Brown el al.2020,  https://arxiv.org/abs/2005.14165  \n23 The method by which the learner is taught using short exchanges of sentences in dialogue format. \n24 https://paperswithcode.com/area/natural-language-processing \n25 Dynabench: Rethinking Benchmarking in NLP., D. Kiela et al. 2021, \nhttps://aclanthology.org/2021.naacl-main.324.pdf \n26 MNIS: Dataset widely used as the benchmark for handwritten text recognition. https://www.vision-\nsystems.com/home/article/16737424/support-vector-machines-speed-pattern-recognition \n27 GLUE: https://gluebenchmark.com/ \n28https://ruder.io/nlp-benchmarking/、https://github.com/kwchurch/Benchmarking_past_present_future \n29 There are following examples. Example where Microsoft from the United States apologized for a \ncomment by its AI (https://jp.reuters.com/article/tay-idJPKCN0WU056), example of gender \ndiscrimination by the AI tool used by Amazon of the United States. (https://www.reuters.com/article/us-\namazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-\nwomen-idUSKCN1MK08G) \n30 Google I/O keynote lecture, https://io.google/2022/program/8e80903f-955f-4a5b-9118-\nb0ce4acdb0e6/intl/ja/ \n31 https://www.ethnologue.com/ \n32 For example, https://maraoz.com/2020/07/18/openai-gpt3/ \n33 https://gpt3demo.com/ \n34 One can verify this by entering GPT-3 as the name of the author at \nhttps://www.amazon.co.jp/advanced-search/books \n35 https://www.othersideai.com/ \n36 https://www.usebroca.com/ \n37 https://snazzy.ai/ \n38 https://www.forethought.ai/ \n39 https://www.gong.io/ \n40 https://www.moveworks.com/ \n41 https://www.observe.ai/ \n42 DALL･E: AI for generating images from texts developed by OpenAI. Zero-Shot Text-to-Image \nGeneration., A. Ramesh et al. 2021, https://arxiv.org/abs/2102.12092 \n43 https://openai.com/blog/dall-e/ \n44 CLIP: Abbreviation of Contrastive Language-Image Pre-training. https://openai.com/blog/clip/, \nLearning Transferable Visual Models From Natural Language Supervision., A. Radford et al. 2021, \nhttps://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervi\nsion.pdf \n45 An encoder compresses information, and a decoder retrieves the information. \n46 Environment for building customized application that meets business needs provided by Microsoft. It \nallows for quick development of applications. https://powerapps.microsoft.com/ja-jp/ \n47 https://www.infoq.com/jp/news/2021/01/microsoft-license-gpt-3/ \n48 Low-code language used for instances in Power Apps. It includes the general-purpose type used for \ninstances in Excel programming, strict type specification, declarative programming and functional \nprogramming. \n49 https://news.microsoft.com/ja-jp/2021/05/26/210526-microsoft-introduces-its-first-product-features-\npowered-by-gpt-3/ \n50 PET/iPET: Deep learning training method for natural language processing NLP) model. Abbreviation \nof Pattern-Exploiting Training, Iterative Pattern-Enhanced Training. It is a training method to achieve the \nsame performance through smaller models. \n51 One of the natural language processing benchmarks. It consists of tests that include diverse tasks, such \nas question and answer, natural language inference, cross-referencing solution, and resolution of words \nambiguity. \n52 ALBERT: A Lite BERT for Self-supervised Learning of Language Representations., Z. Lan et al. 2019, \nhttps://arxiv.org/abs/1909.11942 \n39 \n \n \n53 For instance, Apache Software License: One of the license agreements that outlines the terms of use, \netc., that are often used when developing/distributing open source software. \n54 https://twitter.com/daisukekawahar1/status/1524288370767134721?cxt=HHwWgoCsrc-MrqcqAAAA \n55 https://huggingface.co/rinna/japanese-gpt-1b \n56 https://rinna.co.jp/ニュース/f/rinna released clip, a language-image model specialized for Japanese. \n57 For example, https://zenn.dev/hellorusk/articles/ddee520a5e4318 \n58 https://llm-jp.nii.ac.jp/ \n59 Announcement of Japanese language AI using the massive language model by LINE \nhttps://linecorp.com/ja/pr/news/ja/2020/3508, the original Korean version, What Changes Can Large-\nScale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative \nPretrained Transformers., B.Kin et al. 2021, https://arxiv.org/abs/2109.04650 \n60 Current state of “HyperCLOVA” equipped with large-scale general-purpose Japanese model -2021 \nJapanese version-https://www.youtube.com/watch?v=V4pZulIWHpY \n61 https://www.nict.go.jp/press/2023/07/04-1.html \n62 https://crfm.stanford.edu/, On the Opportunities and Risks of Foundation Models., R. Bommasani et al. \n2021 https://crfm.stanford.edu/assets/report.pdf \n63 https://crfm.stanford.edu/workshop.html \n64 Concrete natural language processing tasks such as classification, intention recognition and named \nentity extraction. \n65 Deep spatial autoencoders for visuomotor learning. J Chelsea Finn et al., 2016 IEEE International \nConference on Robotics and Automation (ICRA). IEEE, \n66 Habitat 2.0:Training Home Assistants to Rearrange their Habitat., Szot et al. 2021, Neurips 2021  \n67 https://github.com/allenai/ai2thor-rearrangement \n68 Causal Analysis of Agent Behavior for AI Safety., G. Déletang et al. 2021, \nhttps://arxiv.org/abs/2103.03938 \n69 A Survey of Algorithms for Black-Box Safety Validation., A. Corso et al. 2020, \nhttps://arxiv.org/abs/2005.02979 \n70 Compositional Falsification of Cyber-Physical Systems with Machine Learning Components., T. \nDreossi et al. 2017, In NASA Formal Methods, Springer \n71 Guaranteeing Safety for Neural Network-Based Aircraft Collision Avoidance Systems., K. D. Julian et \nal. 2019, IEEE/AIAA 38th Digital Avionics Systems Conference (DASC) \n72 https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-\nturing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/,  \nhttps://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-\nworlds-largest-and-most-powerful-generative-language-model/ \n73 Solving Quantitative Reasoning Problems with Language Models., A. Lewkowycz el al. 2022, \nhttps://arxiv.org/abs/2206.14858 \n74 https://www.microsoft.com/en-us/research/project/project-florence-vl/ \n75 Knowledge distillation: Extraction of only the necessary knowledge elements. In deep learning, it is \nmostly extraction of a small functional network, or its formation through compression. \n76 https://huggingface.co/bigscience/T0pp \nReport: https://github.com/bigscience-workshop/promptsource \nPaper: https://arxiv.org/abs/2110.08207 \n77 Language Models are Few-Shot Learners., T. Brown et al. 2020, Advances in Neural Information \nProcessing Systems \n78 Language Models are Unsupervised Multitask Learners. A. Radford et al., 2019, \nhttps://github.com/openai/gpt-2 \n79 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer., C. Raffel et al. \n2020, Journal of Machine Learning Research \n80 The Power of Scale for Parameter-Efficient Prompt Tuning., B. Lester et al. 2021 \n81 Prompt learning (https://huggingface.co/bigscience/T0pp) \n82 LaMDA: https://blog.google/technology/ai/lamda/ \n83 https://gpt3demo.com/apps/webgpt \n84 LG AI Talk Concert; https://www.youtube.com/watch?v=b6B43VNW1jk \n85 https://wudaoai.cn/home \n86 GAN: Generative Adversarial Network, image generation using deep learning. See 2021 DX White \n40 \n \n \nPaper Appendix, Section 1, AI Technology, Chapter 1 9 Creation, for details. \n87Diffusion Models Beat GANs on Image Synthesis. P. Dhariwal et al. 2021, \nhttps://arxiv.org/abs/2105.05233 \n88 https://openai.com/dall-e-2/, https://cdn.openai.com/papers/dall-e-2.pdf \n89 MLP: Multi layer perceptron, one of basic structures that is multilayer-coupled to allow for forward \ntransmission of neural networks. \n90 Pay Attention to MLPs H. Liu et al. 2021, https://arxiv.org/abs/2105.08050 \n91 SGU: Spatial Gating Unit Gating mechanism that performs sequence direction calculation of input \nelements. \n92 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, A. Dosovitskiy et \nal.,ICLR 2021, https://openreview.net/forum?id=YicbFdNTTy \n93 The complete pattern of ocular dominance stripes in the striate cortex and visual field of the macaque \nmonkey. S. LeVay et al. 1985, J. Neuroscience,5 \n94 Framelet analysis of some geometrical illusions.2010, H. Arai et al.,  \nJapan Journal of Industrial and Applied Mathematics \n95 The Structures of Letters and Symbols throughout Human History Are Selected to Maych Those Found \nin Objects in Natural Scenes., Changjzi et al. 2006, The American Naturalist  \n96 Object Representation in Inferior Temporal Cortex Is Organized Hierarchically in a Mosaic-Like \nStructure. M. Tanifuji et al. 2013, J. Neuroscience \n97 HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning., A. \nZhmoginov et al. 2022, https://arxiv.org/abs/2201.04182 \n98 Computer Vision Saizensen Winter2021, 2021 Kyoritsu Shuppan \n99 MetaFormer is Actually What You Need for Vision., W. Yu et al. 2022, CVPR2022, \nhttps://arxiv.org/abs/2111.11418 \n100 https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-\nspeech-vision-and-language \n101 https://imagen.research.google/ \n102 https://laion.ai/laion-400-open-dataset/ \n103 https://openai.com/blog/instruction-following/ \n104 Deep reinforcement learning from human preferences., P.F Christiano et al. 2017, \nhttps://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf \n105 Training Compute-Optimal Large Language Models., J. Hoffmann et al. 2022, \nhttps://arxiv.org/abs/2203.15556 \n106 Flamingo: a Visual Language Model for Few-Shot Learning., J-B. Alayrac et al. 2022, \nhttps://arxiv.org/abs/2204.14198  \n107 https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html, PaLM: Scaling \nLanguage Modeling with Pathways., A. Chowdhery et al. 2022, https://arxiv.org/abs/2204.02311 \n108 https://www.adept.ai/ \n109 https://www.deepmind.com/publications/a-generalist-agent,  \n110 Improved protein structure prediction using potentials from deep learning., A. W. Senior et al. 2020, \nNature \n111 Highly accurate protein structure prediction with AlphaFold., J. Jumper et al. 2021, Nature \n112 For example, Compositionally restricted attention-based network for \nmaterials property predictions., A. Y. Wang et al. 2021, npj Computational Materials, \nhttps://www.nature.com/articles/s41524-021-00545-1  \n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "I.2.7"
  ],
  "published": "2023-10-10",
  "updated": "2023-10-10"
}