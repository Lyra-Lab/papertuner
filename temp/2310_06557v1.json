{
  "id": "http://arxiv.org/abs/2310.06557v1",
  "title": "Data efficient deep learning for medical image analysis: A survey",
  "authors": [
    "Suruchi Kumari",
    "Pravendra Singh"
  ],
  "abstract": "The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.",
  "text": "Data efficient deep learning for medical image analysis: A survey\nSuruchi Kumaria, Pravendra Singha,∗\naDepartment of Computer Science and Engineering, Indian Institute of Technology Roorkee, India\nAbstract\nThe rapid evolution of deep learning has significantly advanced the field of medical image analysis. However, despite these achieve-\nments, the further enhancement of deep learning models for medical image analysis faces a significant challenge due to the scarcity\nof large, well-annotated datasets. To address this issue, recent years have witnessed a growing emphasis on the development of\ndata-efficient deep learning methods. This paper conducts a thorough review of data-efficient deep learning methods for medical\nimage analysis. To this end, we categorize these methods based on the level of supervision they rely on, encompassing categories\nsuch as no supervision, inexact supervision, incomplete supervision, inaccurate supervision, and only limited supervision. We further\ndivide these categories into finer subcategories. For example, we categorize inexact supervision into multiple instance learning and\nlearning with weak annotations. Similarly, we categorize incomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize commonly used datasets for data efficient deep\nlearning in medical image analysis and investigate future research directions to conclude this survey.\nKeywords: Data efficient deep learning, Medical image analysis, Inexact supervision, Incomplete supervision, Inaccurate\nsupervision, Only limited supervision, No supervision.\n1. Introduction\nDeep learning has significantly influenced various medical\nfields, particularly medical imaging, with its influence expected\nto further expand [1]. In the context of medical image analy-\nsis (MIA), deep learning methods have demonstrated remarkable\nperformance across various tasks, including disease classification\n[2, 3, 4, 5], medical object detection [6, 7], ROI segmentation\n[8, 9, 10, 11], and image registration [12, 13, 14]. Initially, super-\nvised learning was widely adopted in MIA. Despite its success\nin numerous applications, the broader use of supervised models\nfaces a significant challenge due to the typically small size of\nmost medical datasets. Medical image datasets are often consid-\nerably smaller than standard computer vision datasets. The initial\namount of available data is limited, and obtaining additional data\nis hindered by factors such as patient confidentiality and insti-\ntutional policies. Furthermore, in many instances, only a small\nfraction of the images are annotated by domain experts.\nTypically, researchers rely on domain experts, such as radiolo-\ngists or pathologists, to create task-specific annotations for image\ndata. Labeling a sufficiently large dataset can be time-consuming\n[15]. For example, training deep learning systems for radiology,\nespecially when involving 3D data, requires meticulous slice-by-\nslice annotations, which can be particularly time-intensive [12].\nSome research efforts have involved numerous experts in anno-\ntating extensive medical image datasets [16, 17]. However, such\ninitiatives demand substantial financial and logistical resources,\nwhich are often not readily available across various domains.\nOther investigations have resorted to crowd-sourcing approaches\nfor obtaining labels from non-experts [18, 19, 20]. Although this\nmethod may have potential in specific cases, its applicability is\n∗Corresponding author: Pravendra Singh\nEmail addresses: suruchi_k@cs.iitr.ac.in (Suruchi Kumari),\npravendra.singh@cs.iitr.ac.in (Pravendra Singh)\nlimited because non-experts typically cannot provide meaningful\nlabels for most medical applications. To overcome these limi-\ntations, there is a growing trend among researchers to develop\ndata-efficient deep learning approaches for medical image anal-\nysis. We broadly categorize these approaches into the following\ngroups: no supervision, inexact supervision, incomplete supervi-\nsion, inaccurate supervision, and limited supervision, as shown in\nFigure 1.\nThis survey covers more than 250 papers, with the majority\npublished in recent years (2020-2023). These papers span a di-\nverse range of applications of deep learning in medical image\nanalysis and have been presented in conference proceedings for\nMICCAI, EMBC, and ISBI, as well as various journals such as\nTMI, Medical Image Analysis, and Computers in Biology and\nMedicine, among others.\nSeveral related review articles have already been published\nsummarizing a few specific categories of data efficient learning\nin the domain of medical image analysis. Cheplygina et al. [21]\nprovided an overview of semi-supervised learning, multiple in-\nstance learning, and transfer learning within the context of med-\nical imaging, addressing both diagnostic and segmentation tasks.\nMeanwhile, Tajbakhsh et al. [22] explored numerous strategies\nfor handling dataset limitations, such as cases involving scarce or\nweak annotations, with a particular focus on medical image seg-\nmentation. Chen et al. [13] present a summary of the latest de-\nvelopments in deep learning, encompassing supervised, unsuper-\nvised, and semi-supervised methodologies. More recently, Jin et\nal. [23] provide an overview of semi-supervised, self-supervised,\nmulti-instance learning, active learning, and annotation-efficient\ntechniques. However, it’s worth noting that their review does not\ndelve into subjects such as domain-adaptive learning or few-shot\nlearning, among others. Also, in the previously discussed sur-\nveys, their coverage is either restricted concerning data-efficient\nmethods in MIA or not up to date with the current trends. To\ntackle this challenge, we undertake a systematic review of recent\nPreprint submitted on 10 oct 2023\narXiv:2310.06557v1  [eess.IV]  10 Oct 2023\nData efficient deep learning for MIA\nInexact Supervision\nIncomplete Supervision\nInaccurate Supervision\nOnly limited Supervision\nNo Supervision\nPredictive self-supervision\nGenerative self-supervision\nContrastive self-supervision\nMulti-self supervision\nMultiple instance learning\nLearning with weak\nannotations\nSemi-supervised learning\nActive learning\nDomain-adaptive learning\nRobust Loss Design\nData re-weighting\nTraining procedures\nData Augmentation\nFew shot learning\nTransfer learning\nFigure 1: Taxonomy of data efficient deep learning approaches for medical image analysis.\ndata-efficient methodologies, as outlined in Figure 1. Our goal is\nto offer a thorough review of data-efficient learning methods in\nmedical image analysis and outline future challenges. We also\nprovide an overview of several widely used available datasets in\nthe field of medical imaging, as illustrated in Table 1. The major\ncontributions of our work can be summarized as follows:\n• This is the first survey paper that summarizes recent ad-\nvances in data efficient deep learning for medical im-\nage analysis.\nSpecifically, we present a comprehensive\noverview of more than 250 relevant papers to cover the re-\ncent progress.\n• We systematically categorize these methods into five distinct\ngroups: incomplete supervision, no supervision, inaccurate\nsupervision, inexact supervision, and only limited supervi-\nsion.\n• Lastly, we explore several potential future directions for fur-\nther research and development for data-efficient deep learn-\ning methods in MIA.\nThe remainder of this survey is organized as follows. In Sec-\ntion 2, we delve into techniques falling under the category of\nNo Supervision, which we further subdivide into Predictive Self-\nSupervision (Subsection 2.1), Generative Self-Supervision (Sub-\nsection 2.2), Contrastive Self-Supervision (Subsection 2.3), and\nMulti-Self Supervised Learning (Subsection 2.4). In Section 3,\nwe explore Inexact Supervision techniques, further classified into\nMultiple Instance Learning (Subsection 3.1) and Learning with\nWeak Annotations (Subsection 3.2). Section 4 is dedicated to\nIncomplete Supervision methods, which we further categorize\nas Semi-Supervised Learning (Subsection 4.1), Active Learn-\ning (Subsection 4.2), and Domain-Adaptive Learning (Subsec-\ntion 4.3). Similarly, Section 5 deals with Inaccurate Supervision\ntechniques, which we further categorize as Robust Loss Design\n(Subsection 5.1), Data reweighting (Subsection 5.2), and Train-\ning procedures (Subsection 5.3). Moving on to Section 6, we\nfocus on Only Limited Supervision techniques, which are classi-\nfied into Data Augmentation (Subsection 6.1), Few-Shot Learn-\ning (Subsection 6.2), and Transfer Learning (Subsection 6.3).\nAdditionally, we outline potential future research directions in\nSection 7 before concluding this survey in Section 8. The struc-\ntural overview of this survey is presented in Figure 1.\n2. No supervision\nLearning with no supervision, commonly referred to as unsu-\npervised learning, involves the challenge of obtaining supervi-\nsion signals in the absence of explicit guidance. One primary\ntechnique used for this purpose is self-supervised learning (SSL).\nIn SSL, representations are acquired by training on an auxiliary\npretext task and later transferred to a target downstream task of\ninterest. The effectiveness of SSL relies significantly on the de-\nsign of well-crafted pretext tasks. These pretext tasks introduce\nimplicit inductive biases into the model, making it crucial to se-\nlect them thoughtfully to ensure their relevance to the specific do-\nmain of interest. Self-supervised learning can be divided into four\nbroad categories: predictive, generative, contrastive, and multi\nself-supervision [71]. A summary of recent methods for learning\nwith no supervision is provided in Table 2.\n2.1. Predictive self-supervision\nIn this section, we explore predictive self-supervision, where\nthe pretext task is cast as either a classification or regression\ntask. Specifically, each unlabeled image is assigned a pseudo\nlabel, which is generated directly from the data itself.\nThese\npseudo labels can take on categorical or numerical values, de-\npending on the design specifications of the pretext task. Com-\nmon transformation-based predictive tasks involve aspects such\nas assessing relative position [72], solving jigsaw puzzles [73],\nand determining rotation angles [74], among others. These tra-\nditional pretext tasks, and their variations, have been explored\nin MIA and have demonstrated their effectiveness. For instance,\nBai et al. [75] introduced an approach for segmenting cardiac\nMRI scans by proposing a pretext task focused on predicting\nanatomical positions. This pretext task aimed to utilize the var-\nious cardiac views available in the MRI scans, such as short-\naxis, 2CH long-axis, and 4CH long-axis, to represent different\ncardiac anatomical regions, including the left and right atrium\nand ventricle. To accomplish this, the authors defined a series of\nbounding boxes corresponding to specific anatomical positions\nwithin a given view and trained their network to predict these po-\nsitions. Taleb et al. [76] introduced a novel approach inspired\nby Jigsaw puzzle-solving, which makes use of multiple imaging\nmodalities. In this method, an input image is composed of dis-\nordered patches from different modalities, and the model’s task\nis to reconstruct the original image by correctly assembling these\n2\nTable 1: Commonly used datasets for data efficient deep learning in medical image analysis.\nDataset\nOrgan\nTypes\nTask\nDescription\nLink\nJSRT\nDatabase\n(2000)\n[24]\nBrain\nChest radiographs\nClassification\nThe database includes 154 conventional chest ra-\ndiographs with a lung nodule (100 malignant and\n54 benign nodules) and 93 radiographs without a\nnodule.\nhttp://db.jsrt.or.jp/eng.php\nADNI-3 dataset [25, 26]\nBrain\nMRI, PET, fMRI,\netc..\nAlzheimers Disease identifica-\ntion\n697 subjects from ADNI-2 and additional 133 CN,\n151 amnestic MCI and 87 AD subjects were added\n(371 total new subjects)\nhttps://adni.loni.usc.edu/\nadni-3/\nBraTS 2012 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: 30 datasets(pre- and post-therapy im-\nages) Synthetic data: 50 simulated datasets; Test:\n15 clinical and 15 simulated datasets\nhttp://www.imm.dtu.dk/\nprojects/BRATS2012/data.html\nBraTS 2013 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: Clinical dataset from BraTS12 training\ndata; Test: 15 clinical test images from BraTS12\nand 10 new test dataset\nhttps://www.smir.ch/BRATS/\nStart2013#!#download\nBraTS 2014 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: 200 datasets from both BraTS12 and\nBraTS13 and TCIA [16] including longitudinal\ndatasets; Test:\n38 unseen datasets from both\nBraTS12 and BraTS13 test datasets and TCIA\nhttps://www.smir.ch/BRATS/\nStart2014\nBraTS 2015 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: Identical to the BraTS14 training dataset;\nTest: 53 unseen datasets from both BraTS12 and\nBraTS13 test datasets and TCIA\nhttps://www.smir.ch/BRATS/\nStart2015\nTCIA (2015)\nBrain\nMR images\nSegmentation\n20 subjects with primary newly diagnosed glioblas-\ntoma who were treated with surgery and standard\nconcomitant chemo-radiation therapy (CRT) fol-\nlowed by adjuvant chemotherapy.\nhttps://www.\ncancerimagingarchive.net/\nBraTS 2016 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: Identical to the BraTS14 training dataset;\nTest: 191 unseen datasets from both BraTS12 and\nBraTS13 test datasets and TCIA\nhttps://www.smir.ch/BRATS/\nStart2016\nABIDE-II (2016)\nBrain\nfMRI sequences\nAutism\nspectrum\ndisorder\nclassification\n1114 datasets from 521 individuals with ASD and\n593 controls\nhttps://fcon1000.projects.\nnitrc.org/indi/abide/\nBraTS 2017 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: 285 training datasets from BraTS12 and\nBraTS13 + pre-operative MRI scans from 19 insti-\ntution; Validation: 6 unseen datasets from differ-\nent institution; Test: 146 unseen datasets from both\nBraTS13 test datasets and different institutions\nhttps://sites.google.com/site/\nbraintumorsegmentation/\nBraTS 2018 [27]\nBrain\nMR images\nBrain tumor segmentation\nTraining: Identical to the BraTS17 dataset; Valida-\ntion: 6 unseen datasets from different institution;\nTest: 191 unseen datasets from both BraTS13 test\ndatasets and different institutions\nhttps://wiki.\ncancerimagingarchive.net/\npages/viewpage.action?pageId=\n37224922\ndHCP 2018 [28]\nBrain\nMRI\nCortical and sub-cortical vol-\nume\nsegmentation,\ncortical\nsurface extraction, and infla-\ntion\n465 subjects ranging from 28 to 45 weeks post-\nmenstrual age.\nhttp://www.\ndevelopingconnectome.org/\ndata-release/\nCalgary-Campinas-359\n(CC-359) [29]\nBrain\nMR images\nSkull stripping or Brain seg-\nmentation\n359 subjects on scanners from three different ven-\ndors (GE, Philips, and Siemens) and at two mag-\nnetic field strengths (1.5 T and 3 T)\nhttps://www.ccdataset.com/\ndownload\nMICCAI\nWMH\nChal-\nlenge [30]\nBrain\nMR images\nWhite matter hyperintensities\n(WMH) segmentation\nTraining: 60 images; Test: 110 images\nhttps://wmh.isi.uu.nl/#_\nToc122355662\nREST-meta-MDD\nCon-\nsortium [31]\nBrain\nResting-state\nfunctional\nMRI\n(R-fMRI)\nMajor\nDepressive\nDisorder\n(MDD) classification\nNeuroimaging data of 1,300 depressed patients and\n1,128 normal controls from 25 research groups\nhttp://rfmri.org/REST-meta-MDD\nBraTS (2021)\nBrain\nMR images\nSegmentation; Classification\n2,000 cases (8,000 mpMRI scans)\nhttp://braintumorsegmentation.\norg/\nMM-WHS\nchallenge\ndataset (2017) [32, 33]\nHeart\nMR and CT images\nWhole heart segmentation\n20 labeled and 40 unlabeled CT volumes; 20 la-\nbeled and 40 unlabeled MR volumes.\nhttps://zmiclab.github.io/zxh/\n0/mmwhs\nACDC (2018) [34]\nHeart\nCine MR images\nClassification and segmenta-\ntion\nTraining: 100 patients; Test: 50 patients\nhttps://www.creatis.insa-lyon.\nfr/Challenge/acdc/databases.\nhtml\nAtrial LGE-MRI dataset\n(2018) [35]\nHeart\nCardiac (LA) seg-\nmentation\nLate\ngadolinium-enhanced\nmagnetic\nresonance\nimages\n(LGE-MRI)\nTraining: 100 LGE-MRI; Test: 54 LGE-MRI\nhttp://atriaseg2018.\ncardiacatlas.org\nMSCMRseg (2019) [36]\nHeart\nMR images\nCardiac(MYO, RV and LV)\nsegmentation\nData was collected from 45 patients, who under-\nwent cardiomyopathy.\nhttps://zmiclab.github.io/zxh/\n0/mscmrseg19\nM&Ms (2020) [37]\nHeart\nMR images\nCardiac segmentation\nTraining: 175; Validation: 40; Test: 160 MR im-\nages\nhttps://www.ub.edu/mnms/\nSTARE\nEye\nFundus images\nBlood vessel segmentation\n20 equal-sized (700605) color fundus images\nhttps://cecas.clemson.edu/\n~ahoover/stare/\nDRIVE (2004)\nEye\nImages\ncaptured\nwithCanon\nCR5\nnon-mydriatic\n3CCD camera\nVasculature segmentation\nTraining: 20 images; Test: 20 images\nhttps://drive.grand-challenge.\norg/\nDRISHTI-GS\n(2014)\n[38]\nEye\nFundus images\nOptic disc (OD) and (OC) cup\nsegmentation\nTraining: 50 images; Test: 51 images\nhttps://ieeexplore.ieee.org/\ndocument/6867807\ncontinued on the next page\n3\nTable 1: Commonly used datasets for data efficient deep learning in medical image analysis (continued).\nDataset\nOrgan\nTypes\nTask\nDescription\nLink\nReTOUCH (2017) [39]\nEye\nOCT volumes\nFluid detection and fluid seg-\nmentation\nTraining: 70 OCT volumes; Test: 42 OCT volumes\nhttps://retouch.\ngrand-challenge.org\nRetinalOCT (2018) [40]\nEye\nOptical\nCoherence\nTomography (OCT)\nImages\nClassification\n207,130 OCT images\nhttps://www.kaggle.com/\ndatasets/paultimothymooney/\nkermany2018\nLDLOCTCXR\n(2018)\n[40]\nEye\nOCT and Chest X-\nRay images\nClassification\n108,312 images(37,206 with choroidal neovascu-\nlarization, 11,349 with diabetic macular edema,\n8,617 with drusen, and 51,140 normal) from 4,686\npatient\nhttps://data.mendeley.com/\ndatasets/rscbjbr9sj/3\nPALM (2019) [41]\nEye\nImages\ncaptured\nwith Zeiss Visucam\n500\nClassification of normal and\nmyopia fundus;\nlesion seg-\nmentation in pathologic my-\nopia.\nTraining: 400 images, Validation: 400 images;\nTest: 400 images\nhttps://palm.grand-challenge.\norg\nREFUGE\nchallenge\ndataset [42]\nEye\nFundus images\nClassification of clinical Glau-\ncoma; OD and OC segmenta-\ntion; Localization of Fovea\n1200 fundus images with ground truth segmenta-\ntions and clinical glaucoma labels\nhttps://refuge.\ngrand-challenge.org/\nADAM (2020) [43]\nEye\nFundus images cap-\ntured using a Zeiss\nVisucam 500 fundus\ncamera\nClassification;\nOptic\ndisc\ndetection and segmentation;\nFovea localization and Lesion\ndetection and segmentation\n1200 retinal fundus images\nhttps://amd.grand-challenge.\norg/\nRIGA+\ndataset\n(2022)\n[44]\nEye\nFundus images\nSegmentation of Optic Disc\n(OD) and Cup (OC)\n744 labeled samples and 717 Unlabeled samples\nhttps://zenodo.org/record/\n6325549\nISIC (2016)\nSkin\nDermoscopic lesion\nimages\n1.Lesion\nSegmentation;\n2.Dermoscopic Feature Clas-\nsification and segmentation;\n3.Disease Classification\n1.Training:900, Test:379 images; 2.Training:807,\nTest:335 images; 3.Training:900, Test:379 images\nhttps://challenge.\nisic-archive.com/data/#2016\nHAM10000 (2018)\nSkin\nDermatoscopic im-\nages\nLesion classification and seg-\nmentation\n10000 training images\nhttps://dataverse.harvard.edu/\ndataset.xhtml?persistentId=\ndoi:10.7910/DVN/DBW86T\nMITOS12 [45]\nBreast\nHistological Images\nBreast cancer grading\n50 high power fields (HPF) coming from 5 different\nslides scanned at 40 magnification\nhttp://ludo17.free.fr/mitos_\n2012/dataset.html\nMITOS14\nBreast\nHistological Images\nBreast cancer grading\nTraining data set there are 284 frames at X20 mag-\nnification and 1,136 frames at X40 magnification.\nhttps://mitos-atypia-14.\ngrand-challenge.org/Dataset/\nMIAS (2015)\nBreast\nMammograms\nDetection; Classification\n322 images (161 pairs) at 50 micron resolution in\nPortable Gray Map format\nhttps://www.kaggle.\ncom/datasets/kmader/\nmias-mammography\nTUPAC (2016) [46]\nBreast\nWhole-slide\nhistopathology\nimages\nAutomatic prediction of tumor\nproliferation scores of breast\ntumors\nTraining: 500 WSIs; Test: 321 WSIs\nhttps://github.com/CODAIT/\ndeep-histopath\nCAMELYON\n(2016)\n[47]\nBreast\nWhole-slide images\n(WSIs)\nDetection and classification of\nbreast cancer metastases\nTraining: 270 WSI; Test: 130 WSI\nhttps://camelyon16.\ngrand-challenge.org/Data\nCAMELYON\n(2017)\n[47]\nBreast\nWhole-slide images\n(WSIs)\nDetection and classification of\nbreast cancer metastases\nTraining: 500 WSI; Test: 500 WSI\nhttps://camelyon17.\ngrand-challenge.org/Data\nCBIS-DDSM (2017)\nBreast\nMammograms\nSegmentation\nData set contains 753 calcification cases and 891\nmass cases\nhttps://www.kaggle.\ncom/datasets/awsaf49/\ncbis-ddsm-breast-cancer-image-dataset\nBACH (2018) [48]\nBreast\nMicroscopy\nand\nWhole-slide images\nBreast cancer classification\nMicroscopy: 400 images; WSI: 30 images\nhttps://iciar2018-challenge.\ngrand-challenge.org/Dataset/\nTNBC (2018)\nBreast\nHistopathology im-\nages\nstained\nwith\nH&E\nNuclei segmentation\nData Set1: 50 images with a total of 4022 annotated\ncells; Data Set2: 30 images from 7 different organs\nwith a total of 21 623 annotated nuclei\nhttps://ega-archive.org/\ndatasets/EGAD00001000063\nFNAC (2019) [49]\nBreast\nCytology images\nClassification\n212 images in two classes: benign (99) and malig-\nnant (113)\nhttps://1drv.ms/u/s!Al-T6d-_\nENf6axsEbvhbEc2gUFs\nNYUBCS (2019)\nBreast\nMammograms\nSegmentation\n29,426 digital screening mammography exams\n(1,001,093 images) from 141,473 patients\nhttps://cs.nyu.edu/~kgeras/\nreports/datav1.0.pdf\nBreastPathQ (2019) [50]\nBreast\nWhole slide images\nstained with H&E\nEstimation of tumor cellularity\n(TC)\nTraining: 2,579 patches extracted from 69 WSIs;\nTest: 1,121 patches extracted from 25 WSIs\nhttps://breastpathq.\ngrand-challenge.org/Overview/\nCERVIX93 (2018) [51]\nCervix\nCytology images\nClassification; detection\n93 stacks of images (2705 nuclei)\nhttps://github.com/parhamap/\ncytology_dataset\nLBC (2020) [52]\nCervix\nCytology images\nClassification\n963 LBC images in classes of NILM, LSIL, HSIL,\nand SCC\nhttps://data.mendeley.com/\ndatasets/zddtpgzv63/4\nCHAOS (2021) [53]\nAbdomen\nCT and MR images\nLiver and Abdominal segmen-\ntation\nCT: 40 images; MRI: 120 DICOM data sets\nhttps://chaos.grand-challenge.\norg/\nKiTS (2023)\nKidney\nCT scan\nKidney Tumor Segmentation\nTraining: 489 cases; Test: 110 cases\nhttps://kits-challenge.org/\nkits23/\nLiTS (2017)\nLiver\nCT scans\nLiver lesions segmentation\nTraining: 130 CT scans; Test: 70 CT scans\nhttps://competitions.codalab.\norg/competitions/17094\nAsciteps (2020) [54]\nStomach\nClassification;\nde-\ntection\nCytology images\n487 images for classification: malignant(18,558)\nand benign(6089); 176 images for detection (6573\nbounding boxes)\nhttps://pan.baidu.com/s/\n1r0cd0PVm5DiUmaNozMSxgg\ncontinued on the next page\n4\nTable 1: Commonly used datasets for data efficient deep learning in medical image analysis (continued).\nDataset\nOrgan\nTypes\nTask\nDescription\nLink\nMoNuSeg (2017) [55]\nMulti-\norgan\nH&E stained tissue\nimages\nNuclei segmentation\nTraining: 30 images and around 22,000 nuclear\nboundary annotations; Test: 7000 nuclear bound-\nary annotations\nhttps://monuseg.\ngrand-challenge.org/\nBTCV (2017) [56]\nMulti-\norgan\nCT images\nMulti-organ segmentation\n90 abdominal CT images\nnhttps://zenodo.org/record/\n1169361#.Y8Ud-OxBwUE\nDeepLesion (2018) [57]\nMulti-\norgan\nCT slices\nFor different applications\n32,735 lesions in 32,120 CT slices\nhttps://nihcc.app.box.com/v/\nDeepLesion\nDECATHLON (2019)\nMulti-\norgan\nCT and MRI\nSegmentation\nBrain: 750 MRI; Heart: 30 MRI; Liver: 201 CT\nimages; Hippocampus:\n195 MRI; Prostate:\n48\nMRI; Lung: 96 CT scans; Pancreas: 420 CT scans;\nHepaticVessel:\n443 CT scans; Spleen:\n61 CT\nscans; Colon: 190 CT scans\nhttp://medicaldecathlon.com/\nMIDOG [58]\nMulti-\norgan\nWhole Slide Images\nSegmentation\nCanine Lung Cancer: 44 cases; Human Breast\nCancer: 150 cases; Canine Lymphoma: 55 cases;\nHuman neuroendocrine tumor: 55 cases; Canine\nCutaneous Mast Cell Tumor: 50 cases; Human\nmelanoma: 49 cases\nhttps://imig.science/midog/\nthe-dataset/\nCRCHistoPhenotypes\n(2016) [59]\nColon\nHistology images\nCancer classification\n100 H&E stained histology images of colorectal\nadenocarcinomas\nhttps://warwick.ac.uk/\nfac/crossfac/tia/data/\ncrchistolabelednucleihe\nKATHER (2018) [60]\nColon\nHistological images\nCancer classification\n100,000 histological images of human colorectal\ncancer and healthy tissue\nhttps://zenodo.org/record/\n1214456#.Y8fgV-zP1hE\nPROMISE12\nchallenge\ndataset [61]\nProstate\nMR images\nProstate segmentation\nTraining: 50; Test: 30; Live challenge: 20 datasets\npromise12.grand-challenge.org/\nTMA-Zurich (2018) [62]\nProstate\nHistopathology im-\nages\nGleason grading of prostate\ncancer\nTraining: 641 patients; Test: 245 patients\nhttps://www.nature.com/\narticles/s41598-018-30535-1?\nsource=app#data-availability\nThe Cancer Genome At-\nlas (TCGA) dataset\nProstate\nHistopathology\nWSIs\nCancer tumour classification\nbased on gleason scores\n20,000 patient samples spanning 33 cancer types\nhttps://portal.gdc.cancer.gov/\nrepository\nPANDA (2020) [63]\nProstate\nWhole-slide images\nGleason grading of prostate\ncancer\nDevelopment set:\n10,616 biopsies; Tuning set:\n393; Internal validation set: 545; External valida-\ntion: 1071\nhttps://www.kaggle.com/c/\nprostate-cancer-grade-assessment/\ndata\nSCGM dataset [64]\nSpinal\nCord\nMRI images\nSpinal cord gray matter seg-\nmentation\nTraining: 40 images; Test: 40 images\nhttp://niftyweb.cs.ucl.ac.uk/\nprogram.php?p=CHALLENGE\nMontgomery (2014) [65]\nChest\nChest X-rays\nSegmentation\n138 images in two classes: normal (80) and mani-\nfestations of TB (58)\nhttps://www.kaggle.\ncom/datasets/raddar/\ntuberculosis-chest-xrays-montgomery\nShenzhen (2014) [65]\nChest\nChest X-rays\nSegmentation\n662 images in two classes: normal (326) and man-\nifestations of TB (336)\nhttps://www.kaggle.\ncom/datasets/raddar/\ntuberculosis-chest-xrays-shenzhen\nNIH Chest X-ray (2017)\n[66]\nChest\nChest X-rays\nClassification\n112,120 X-ray images with disease labels from\n30,805 unique patients.\nhttps://www.kaggle.com/\ndatasets/nih-chest-xrays/data\nChestX-ray8 (2017) [66]\nChest\nChest x-ray images\nClassification and Localiza-\ntion of Common Thorax Dis-\neases\n108,948 frontal-view X-ray images of 32,717\nunique patients with the text-mined eight disease\nimage labels\nhttps://nihcc.app.box.com/v/\nChestXray-NIHCC/\nMIMIC-CXR\n(2019)\n[67]\nChest\nChest x-ray images\nDetection\nTotal of 377,110 images with semi-structured free-\ntext radiology report that describes the radiological\nfindings of the images\nhttps://physionet.org/content/\nmimic-cxr/2.0.0/\nChestX-ray14 (2019)\nChest\nChest x-ray images\nClassification and Localiza-\ntion of Common Thorax Dis-\neases\n112,120 frontal chest radiographs from 30,805 dis-\ntinct patients with 14 binary labels\nhttps://stanfordmlgroup.\ngithub.io/competitions/\nchexpert/\nCC-COVID (2020) [68]\nChest\nCT images\nLung-lesion segmentation\n532,506 CT images from NCP, common pneumo-\nnia, and normal controls\nhttps://ncov-ai.big.ac.cn/\ndownload?lang=en\nSegTHOR (2020) [69]\nChest\nCT images\nSegmentation of Thoracic Or-\ngans\nTraining: 40 CT scans; Test: 20 CT scans\nhttps://competitions.codalab.\norg/competitions/21145\nVinDr-CXR (2021) [70]\nChest\nChest x-ray images\nClassification; Detection\nTraining: 15000 scans; Test: 3000 scans\nhttps://vindr.ai/datasets/cxr\nChestXR (2021)\nChest\nChest x-ray images\nClassification\n20,000+ images and 3 classes: COVID-19, Pneu-\nmonia and Normal cases\nhttps://cxr-covid19.\ngrand-challenge.org/Dataset/\nMICCAI2018\nIVDM3Seg dataset\nIntervertebral\nDisc\nMRI images\nIntervertebral discs (IVD) lo-\ncalization and segmentation\n24 3D multi-modality MRI data sets each data set\ncontains four aligned high-resolution 3D volumes,\nso total 96 high-resolution 3D MRI volume data\nhttps://ivdm3seg.weebly.com/\ndata.html\n5\n!\"#$%&\n!%'\"'(\n)\n)\n*\n+\n,\n-\n.\n/\n0\n-\n*\n0\n,\n)\n/\n+\n.\n*\n0\n)\n.\n!(\"!!\"#1(\n2(314'564\"!3#1\n73\"&(6(89:'\"$ :%#;%<='3%#\"<5#('2%!>\n?=@3>A65:=@(\nB%!3C%#'\"<5!%'\"'3%#\nD(!'3:\"<5!%'\"'3%#\n9!$(!3#1\n9!3(#'\"'3%#\nFigure 2: Illustration of the Rubik’s Cube pretext task: A Siamese network with\nM (representing the number of cubes) shared-weight branches, referred to as\nSiamese-Octad, is employed to solve the Rubik’s Cube. The backbone network\nfor each branch can be any well-known 3D CNN. The feature maps derived from\nthe final fully-connected or convolutional layer of all branches are concatenated\nand used as input for separate tasks’ fully-connected layers, namely cube order-\ning and orientation. These tasks are supervised by the permutation loss (LP) and\nrotation loss (LR), respectively (image from [77]).\npatches. Their work represents a notable enhancement over the\ntraditional Jigsaw puzzle approach. Zhuang et al. [77] proposed\na self-supervised task called Rubik cube recovery, inspired by the\nearly work on Jigsaw puzzle solving for 2D natural images. The\ntask involves two operations: cube rearrangement and cube rota-\ntion, as shown in Figure 2. The Rubik cube recovery task uses 3D\ninput, where a Rubik cube is divided into a 3D grid of 222 sub-\ncubes. The addition of the cube rotation task ensures learning\nof rotation invariant features, going beyond the original Jigsaw\npuzzle task, which only focuses on learning translation-invariant\nfeatures. Rubik cube+ [78] improves upon the Rubik cube recov-\nery pretext task by using cube masking operation along with both\ncube rearrangement and cube rotation operations. Nguyen et al.\n[79] introduced a spatial awareness pretext task with the aim of\nacquiring semantic and spatial representations from volumetric\nimages. This concept of a spatial pretext task was influenced by\nChen et al.’s [80] context restoration framework; however, it was\nformulated here into a classification problem. Recently, Zhou et\nal. [81] performed multi-scale pixel restoration and siamese fea-\nture comparison within the feature pyramid. This approach effec-\ntively retains semantic, pixel-level, and scale information all at\nonce.\n2.2. Generative self-supervision\nThe generative self-supervised learning approach seeks to learn\nunderlying features in the input data by framing pretext tasks\nas generative problems [71]. The idea behind generative pre-\ntext tasks is that the model can acquire valuable representations\nfrom unlabeled data by either learning to reconstruct the input\ndata itself or by generating new examples that follow the same\ndistribution as the input data. Ross et al. [82] utilized the im-\nage colorization pretext task to address the segmentation of en-\ndoscopic medical instruments in endoscopic video data. How-\never, instead of using the original architecture employed in the\ncolorization task, they opted for a conditional Generative Adver-\nsarial Network (GAN) architecture. This choice aimed to pro-\nmote the generation of more realistic colored images. The au-\nthors evaluated their approach on six datasets from both medi-\ncal and natural domains to assess its effectiveness in downstream\nFigure 3: CNN architecture for self-supervised context restoration learning, where\nblue, green, and orange strides indicate convolutional units, downsampling units,\nand upsampling units, respectively. The specific structure of the CNN in the re-\nconstruction part may vary based on the subsequent task (image from [80]).\ntasks. Chen et al. [80] introduced a new generative pretext task\nthat involves randomly selecting two isolated patches from an\ninput image and swapping their positions. This swapping pro-\ncess is repeated iteratively, resulting in a corrupted version of the\noriginal image while preserving its overall distribution. Subse-\nquently, a generative model is used to restore the corrupted image\nback to its original version (see Figure 3). Building upon ear-\nlier context-restoration-based studies, Zhou et al. [83] incorpo-\nrated four data transformations (non-linear transformation, local-\nshuffling, outer-cutout, and inner-cutout) into a cohesive recon-\nstruction model called Model Genesis. Harvella et al. [10] intro-\nduced a self-supervised multi-modal reconstruction task for reti-\nnal anatomy learning. They assumed that distinct modalities of\nthe same organ could offer complementary knowledge, leading to\nvaluable representations for subsequent tasks.\nIn the medical domain, conventional pretext tasks that heav-\nily rely on the existence of bigger objects in natural images are\ninadequate because disease-related features are usually found in\nsmaller regions of the medical image. To address this, Holmberg\net al. [84] introduced a pretext task, cross-modal self-supervised\nretinal thickness prediction, for ophthalmic disease diagnosis.\nThis task involves the utilization of two distinct modalities: in-\nfrared fundus images and optical coherence tomography scans\n(OCT). Initially, they extracted retinal thickness maps from OCT\nscans by training a segmentation model with the limited anno-\ntated dataset, which served as ground-truth annotations for the\npreliminary task. Then, a model was trained to predict the thick-\nness maps utilizing unlabeled fundus images and the previously\npredicted thickness maps as labels. Other examples of gener-\native self-supervised pretext tasks include the image denoising\nmethod proposed by Prakash et al. [85] and the Rubik cube++\n(introduced by Tao et al. [86]). In the Rubik cube++ approach,\nsignificant modifications were made to the earlier Rubik cube\nmethod [77]. Instead of treating it as a classification task, they\napproached it as a generative problem using a GAN-based frame-\nwork. The generator’s task was to bring back the initial arrange-\nment of the Rubik cube before applying transformations, whereas\nthe discriminator was responsible for distinguishing between cor-\nrect and incorrect arrangements of the generated cubes.\n2.3. Contrastive self-supervision\nContrastive learning is designed to maximize the mutual in-\nformation between positive image pairs and, if needed, mini-\nmize the representation similarity of negative image pairs. Posi-\ntive pairs consist of two augmented views of the same instance,\nwhereas negative pairs come from different instances. This allows\nthe network to learn discriminative representations of instances,\nwhich are beneficial for pattern recognition tasks. In contrastive\n6\nFigure 4: Illustration of the enhanced SimCLR for 3D medical image segmentation: (i) Outline of the global contrastive loss employed for pre-training the encoder e\nusing dense layers g1. (ii) Outline of the local contrastive loss utilized for pre-training the decoder dl with 1 1 convolutional layers g2, with frozen weights of encoder\ne obtained from the previous training stage (image from [87]).\nlearning, the effectiveness of learned representations heavily de-\npends on the choice of positive and negative pairs. However,\nthe conventional pair generation methods used for natural im-\nages might not be suitable for medical images with intricate se-\nmantic concepts, leading to potentially meaningless representa-\ntions. To tackle this challenge, researchers have dedicated con-\nsiderable effort to meticulously devising pair selection strategies\nwithin widely used contrastive learning frameworks [88]. These\nstrategies aim to retain the pathological semantics present in med-\nical images, resulting in significant performance enhancements\nfor medical datasets compared to traditional methods.\nContig [89] employs a contrastive loss to align images and var-\nious genetic modalities within the feature space. The approach is\ndevised to seamlessly incorporate multiple modalities from each\nindividual into a single end-to-end model, even when the modal-\nities available may differ among individuals. Sowrirajan et al.\n[90] asserted that the augmentations used in MOCO [91] are not\nsuitable for gray-scale medical images. Specifically, blurring and\nrandom crop could potentially remove important lesions. To ad-\ndress this issue, they introduced MoCo-CXR, a modified version\nof MOCO, specifically tailored for chest X-ray images by adapt-\ning the augmentations to better suit this medical imaging context.\nVu et al. [92] introduced a SSL technique called MedAug, in-\nspired by MoCo-CXR. In their method, positive pairs are gen-\nerated from diverse images of a single patient based on their\nmetadata. Azizi et al. [5] presented a similar work to MedAug,\nwhich was based on the SimCLR framework [93]. They intro-\nduced a method called Multi-Instance Contrastive Learning to\ncreate more informative positive pairs from various images of a\nsimilar patient. Chaitanya et al. [87] enhanced SimCLR for 3D\nmedical image segmentation (see Figure 4). They introduced a\nnovel contrasting strategy that leveraged the structural similarity\nof volumetric medical images. Additionally, they introduced a\nlocal contrastive loss to facilitate the learning of more detailed\nand fine-grained representations. Ciga et al. [94] introduce a\ncontrastive SSL approach for digital histopathology. They con-\nducted training on 57 unlabeled histopathology datasets. Their\nfindings reveal that enhancing the feature quality is achievable\nby combining multiple multi-organ datasets with diverse staining\nand resolution characteristics. Some techniques leverage anatom-\nical priors within contrastive methods to further enhance perfor-\nmance across various tasks [6, 95]. Specifically, He et al. [95]\nintroduce Geometric Visual Similarity Learning (GVSL). GVSL\nincorporates the concept of topological invariance into the metric,\nensuring a dependable assessment of inter-image similarity. This\napproach aims to learn a consistent representation for equivalent\nsemantic regions across different images.\n2.4. Multi-self supervised learning: combining multiple SSL pre-\ntext tasks into one framework\nMulti-SSL integrates various types of pretext tasks, includ-\ning predictive, generative, and contrastive tasks. By doing so,\nit aims to overcome the limitation of single pretext tasks, which\nmight learn task-specific features. By employing different self-\nsupervision signals during network training, multi-SSL aims to\nextract more robust and generalizable representations. Taleb et\nal.\n[9] proposed that medical images with a 3D nature offer\nthe potential to learn rich representations compared to 2D im-\nages. To accommodate this, they employed five predesigned pre-\ntext tasks, namely contrastive predictive coding (CPC), exemplar\nCNN, rotation prediction, relative position prediction, and Jig-\nsaw puzzle, to adapt to the characteristics of 3D medical im-\nages. Haghighi et al. [96] introduced Semantic Genesis, build-\ning upon the Model Genesis approach [83].\nThis framework\ncomprises three modules: self-classification, self-restoration, and\nself-discovery, aimed at learning semantics-enriched representa-\ntions. In a further extension of Model Genesis, Zhang et al. [97]\nincorporated a scale-aware proxy task for predicting the input’s\nscale. This addition allows for the learning of multi-level repre-\nsentations. Zhou et al. [98] combined generative and contrastive\nSSL into a Preservational Contrastive Representation Learning\n(PCRL) framework, where preservational learning is introduced\nfor the generative SSL to keep more information. Tang et al. [99]\nintroduce a novel 3D transformer-based architecture known as\nSwin UNEt TRansformers (Swin UNETR), with a hierarchical\n7\n𝑧𝐿\n𝑖\n+\nInpainting Contrastive\nRotation\nPatch Partition\nSwin Transformer Encoder\n+\nℒ𝐶𝑜𝑛𝑡𝑟𝑎𝑠𝑡\nℒ𝑅𝑜𝑡\nℒ𝐼𝑛𝑝𝑎𝑖𝑛𝑡\nCutout + Rot\n𝑥𝑖\n𝑥𝑗\n𝑧𝐿\n𝑗\nSelf-Supervised \nHeads\nSub-Volume\nInput CT\nFigure 5: The pre-training framework’s [99] outline begins with the random crop-\nping of input CT images into sub-volumes, followed by the application of random\ninner cutout and rotation augmentations. These processed images are then utilized\nas input for the Swin UNETR encoder. The framework leverages masked volume\ninpainting, contrastive learning, and rotation prediction as proxy tasks aimed at\nacquiring contextual representations from the input images (image from [99]).\nencoder for self-supervised pre-training. In their proposed pre-\ntraining framework, input CT images undergo random cropping\ninto sub-volumes and are augmented with random inner cutout\nand rotation operations. Subsequently, they are inputted into the\nSwin UNETR encoder. The authors employ masked volume in-\npainting, contrastive learning, and rotation prediction as proxy\ntasks to facilitate the learning of contextual representations from\ninput images, as shown in Figure 5.\nCS-CO [100], designed\nspecifically for histopathological images, combines the strengths\nof generative and discriminative approaches. This method com-\nprises two self-supervised learning phases: cross-stain prediction\n(CS) and contrastive learning (CO). Yan et al.\n[101] employ\nMasked Autoencoders (MAE) but demonstrate that directly ap-\nplying MAE is suboptimal for dense downstream prediction tasks\nsuch as multi-organ segmentation. To address this limitation, they\npropose a self-supervised pre-training approach on large-scale\nunlabeled medical datasets, leveraging both contrastive and gen-\nerative modeling techniques.\nYan et al.\n[101] used Masked Autoencoders (MAE) but\ndemonstrated that directly applying MAE is suboptimal for dense\ndownstream prediction tasks, such as multi-organ segmenta-\ntion. To address this limitation, they proposed a self-supervised\npre-training approach on large-scale unlabeled medical datasets,\nleveraging both contrastive and generative modeling techniques.\n3. Inexact supervision\nInexact supervision pertains to situations where some form of\nsupervision information is available but lacks the exactness de-\nsired for the task. In this context, we classify inexact supervision\ninto two categories: Multiple Instance Learning (MIL) and learn-\ning with weak annotations (Figure 6). In the MIL framework\n(Subsection 3.1), each image is treated as a bag, and the patches\nextracted from it are regarded as instances. When a bag is labeled\nas negative, it implies that all instances within it are also consid-\nered negative. Conversely, if a bag is labeled as positive, it indi-\ncates the presence of at least one positive instance within it. This\nlabeling strategy at the bag level significantly reduces the labeling\nFigure 6: Taxonomy of Inexact Supervision methods.\nburden compared to labeling each individual instance separately,\nwhich proves advantageous across various tasks. Learning with\nweak annotations (Subsection 3.2) refers to a scenario in which\nthe available training data is annotated with labels that are less\ndetailed or less precise than what might be ideal for a particular\ntask. In many medical imaging tasks, obtaining precise annota-\ntions at a fine-grained level (such as pixel-level annotations) can\nbe highly valuable but also costly and time-consuming. Weak\nannotations offer an alternative approach where the labels pro-\nvided for the training data are of a coarser or less specific na-\nture, making them easier and more cost-effective to obtain. These\nweak annotations can take various forms, including image-level,\npoint-level, scribble-level, or box-level. In all of these scenarios,\nthe provided annotations are less detailed and precise compared\nto comprehensive pixel-level annotations. A summary of recent\nmethods for learning with inexact supervision is provided in Ta-\nble 3.\n3.1. Multiple instance learning\nMultiple-instance learning (MIL) [103] arises when obtaining\ndetailed annotations for individual pixels or patches in an im-\nage becomes impractical, time-consuming, or infeasible. Instead,\nglobal labels representing the overall image condition are more\nreadily available. However, these global labels do not directly\ncorrespond to every pixel or patch within the image. MIL ex-\ntends supervised learning to train classifiers using weakly labeled\ndata. In MIL, every image is viewed as a bag containing numer-\nous patches, also referred to as instances. If an image, or bag, is\nclassified as disease-positive, it implies that at least one patch, or\ninstance, within that image is disease-positive. Conversely, if an\nimage is labeled as disease-negative, it signifies that all patches,\nor instances, in that image are negative instances. The current ap-\nproaches within deep MIL can be classified into two categories:\ninstance-based methods and bag-based methods.\n3.1.1. Instance-based methods\nThe main concept behind the instance-based method is to\ntrain an effective instance classifier to predict the possible la-\nbels for individual instances (e.g., image patches) within each\nbag.\nSubsequently, the MIL-pooling (the aggregation process\n8\nTable 2: Overview of recent methods in No Supervision category.\nReference\nTask\nPretext task\nDataset\nResult\n[75]\nCardiac segmentation\nAnatomical Position Prediction\nPrivate Dataset: 3825 Subjects\nDSC: 0.93\n[77]\nBrain tumor segmentation Brain\nhemorrhage classification\nRubiks Cube Recovery\nBraTS 2018; Private Dataset: 1,486\nImages\nBraTS 2018: mIoU: 0.773; Private: Acc: 0.838\n[78]\nBrain tumor segmentation Brain\nhemorrhage classification\nRubik cube+ (cube ordering, cube orien-\ntation and masking identification)\nBraTS-2018; Private Dataset: 1,486\nCT volumes\nBraTS 2018: Mean Dice: 81.70; Private: Acc:\n87.84\n[80]\nFetal image classification Abdom-\ninal multi-organ localization Brain\ntumour segmentation\nImage Context Restoration\nPrivate Fetus Dataset:\n2,694 Im-\nages; Private Multi-organ Dataset:\n150 Images; BraTS 2017\nPrivate Fetus Dataset:\nF1:\n0.8942; Private\nMulti-organ Dataset:\nMean Distance:\n2.90;\nBraTS 2017: DSC: 0.8557\n[10]\nOptic disc segmentation\nMulti-modal Reconstruction\nIsfahan MISP\nAUC: 0.818\n[86]\nPancreas and Brain Tissue segmen-\ntation\nRubik cube ++\nNIH PCT; MRBrainS18\nNIH PCT: DSC: 0.8408; MRBrainS18: DSC:\n0.7756\n[5]\nChest X-ray classification Skin le-\nsions classification\nMulti-Instance\nContrastive\nLearning\n(SimCLR)\nPriavte\nDermatology\nDataset;\nCheXpert\nPrivate: Top-1 Acc: 0.7002; CheXpert: AUC:\n0.772\n[102]\nLung\nContrastive Learning\nCheXpert\nAUC: 0.889\n[6]\n2D and 3D landmark detection; 3D\nLesion matching\nGlobal and Local Contrastive Learning\nDeepLesion;\nNIH\nLN;\nPrivate\nDataset: 94 Patients\nMean Radial Error: 4.3; Maximum Radial Er-\nror: 16.4\n[96]\nLung\nSelf-Discovery + Self-Classification +\nSelf-Restoration\nLUNA; LiTS; CAD-PE; BraTS\n2018; ChestX-ray14; LIDC-IDRI;\nSIIM-ACR\nClassification: LUNA: AUC: 0.9847; Segmen-\ntation: IoU: LiTS: 0.8560; BraTS 2018: 0.6882\n[9]\nBrain tumors segmentation pan-\ncreas tumor segmentation\nCPC Jigsaw puzzle Exemplar CNN Ro-\ntation Prediction Relative position pre-\ndiction\nBraTS 2018; DECATHLON; DRD\nBraTS 2018:\nDSC: 0.9080; DECATHLON:\nDSC ≈0.635; DRD DRD: DSC ≈0.80\n[101]\nMulti-organ segmentation\nMasked Autoencoders + contrastive and\ngenerative modeling\nPre-training Dataset:\nAbdomen-\n1K; Fine-tuning Dataset:\nABD-\n110; Thorax-85; HaN\nABD-110: Dice score: 84.67; Thorax-85: Dice\nscore: 90.37; HaN: Dice score: 77.31\nis commonly referred to as MIL-pooling) method is applied to\ncombine the predictions of all instances within each bag, ulti-\nmately generating the bag’s prediction. Given that the actual la-\nbels of individual instances are unknown, these approaches typ-\nically begin by assigning pseudo-labels to each instance based\non their respective bags (i.e., all instances within a positive bag\nare assigned positive labels, and all instances within a negative\nbag are assigned negative labels).\nSubsequently, the instance\nclassifier is trained using pseudo-labels in a supervised man-\nner until it converges [103]. Various MIL pooling techniques\nare employed in this process, including Mean-pooling [104],\nMax-pooling [104], Average-pooling [105] log-sum-exp-pooling\n[106], Noisy-or-pooling [107], Noisy-and-pooling [108], and Dy-\nnamic pooling [109], among others. Couture et al. [110] propose\nan improved MIL aggregation approach that employs a quantile\nfunction as the pooling mechanism. This innovative technique al-\nlows for a comprehensive representation of the variations within\neach sample, leading to improved global classification accuracy.\nIn the recent study by Qu et al. [111], they applied instance-level\ncontrastive learning to aggregate various tumor features for the\npurpose of diagnosing pancreatic cancer.\n3.1.2. Bag-based methods\nBag-based methods rely on shared instance-level feature ex-\ntractors to capture the features of each instance within a bag.\nThese features are then aggregated using MIL-pooling to obtain\nbag-level features, followed by supervised training of the bag\nclassifier until convergence is achieved. In bag-based methods,\nMIL-pooling aggregates instance features rather than instance\npredictions, as is the case in instance-based methods. Bag-based\nmethods excel in bag classification because they have access to\ntrue bag labels, making their training process free from noise and\nmore accurate than instance-based methods. However, they are\nless suitable for localization tasks, and their instance feature ag-\ngregation lacks flexibility in showcasing the contributions of indi-\nvidual instances to bag classification. These methods are suitable\nwhen the target pattern is expected to be visible at the whole-bag\nlevel rather than being localized to specific instances within the\nbag [21].\nBag-based methods primarily vary in three key components:\nthe first being the instance-level feature extraction module, the\nsecond involving instance-level feature selection, and lastly, the\nmethod by which the instance features are aggregated to produce\nbag-level features.\nConcerning the instance-level feature extractor, the majority\nof methods utilize CNNs to automatically extract robust fea-\ntures from patches or employ pre-trained models [3]. Recently,\nthere has been an emergence of methods that utilize unsupervised\nlearning to extract features at the patch level. In this context,\n[112] train the feature extractor using a combination model that\nincludes both a variational autoencoder and a generative adversar-\nial network (VAE-GAN) as shown in Figure 7. Various methods\nemploy a self-supervised contrastive learning approach to obtain\ninstance-level feature representations. For instance, [113] uses\ncontrastive predictive coding (CPC) from [114], while [115] uti-\nlizes SimCLR from [93]. Additionally, Chikontwe et al. [116] in-\ntegrate an unsupervised contrastive loss with their proposed MIL\nmethod to enhance the learning of instance-level features.\nRegarding the feature selection, the high resolution of medical\nimages poses a challenge when applying deep Multiple Instance\nLearning (MIL) methods since only a limited number of patches\ncan be selected from these images for MIL. To address this, some\napproaches use techniques such as random patch selection [117],\nintelligent sampling using weakly supervised discriminator [118]\nand discriminative patch selection [119, 112]. Additionally, patch\nclustering methods [120, 121, 122] have been employed. Patch\nclustering serves the purpose of ensuring the representativeness\n9\nFigure 7: Illustration of the framework from Zhao et al.’s work [112]: VAE-GAN functions as the instance-level feature extractor. The feature selection process\nidentifies and selects discriminative instance-level features. A Graph Convolutional Network (GCN) is employed to synthesize the selected instance-level features,\nresponsible for generating bag representations and performing the final classification (image adapted from [112]).\nof the selected patches to a certain degree, as a few patches cho-\nsen from a cluster can approximately represent the entire cluster.\nUltimately, representative clusters are utilized to make the final\nprediction. Sharma et al. [120] employ clustering and sampling\non the patch features extracted through the feature extractor. Sub-\nsequently, they integrate these features using an adaptive attention\nmechanism to facilitate end-to-end training. To enhance the fea-\nture space learning, Lu et al. [121] select instances with the high-\nest and lowest attention scores within the current bag for cluster-\ning. To advance upon these prior techniques, Yan et al. [122]\nintroduce a patch clustering approach based on unsupervised and\nself-supervised learning methods.\nFor the Bag level representation, pooling methods such as max\npooling, average pooling, and log-sum-exp pooling [106] are typ-\nically adopted in this step. However, these pooling methods are\nnot trainable, which can restrict their usefulness.\nTo address\nthis limitation, Ilse et al. [123] introduced a fully trainable ap-\nproach that uses the attention mechanism to assign weights to\ninstances, thus indicating the contribution of individual instances\nto bag classification. This work has spurred a wave of research\ninto attention-based aggregation methods [113, 124, 2, 125, 115].\nHashimoto et al. [2] utilized the attention mechanism to combine\ninstance features at various resolutions. Li et al. [115] intro-\nduced a dual-stream aggregator that relies on masked non-local\noperations for conducting instance-level classification as well as\nbag-level classification. In contrast to the methods mentioned\nearlier, their model computes attention explicitly using a train-\nable distance measurement. It’s not just important to consider\nthe contribution of various instances to bag classification; the re-\nlationships among these instances should also be fully explored.\nTo address this, several methods proposed to use Transformer to\naggregate instance features [126, 3]. Shao et al. [3] introduced\nVision Transformer (ViT) into MIL for gigapixel Whole Slide Im-\nages (WSIs) because ViT offers significant benefits in capturing\nlong-distance information and correlations among instances in a\nsequence. Wang et al. [126] aimed to improve lymph node metas-\ntasis prediction by incorporating a pruned Transformer model\ninto MIL. To address the issue of limited samples in the original\ndataset and prevent overfitting, they also developed a knowledge\ndistillation mechanism using data from similar datasets. Different\nfrom the approaches mentioned above, [112] work builds the bag\nrepresentation with a Graph Convolutional Network.\n3.2. Learning with weak annotations\nLearning with weak annotations refers to a scenario where the\navailable training data is annotated with labels that are less de-\ntailed or less precise than what might be ideal for a particular\nmedical imaging task. In many MIA applications, obtaining pre-\ncise annotations at a fine-grained level, such as pixel-level an-\nnotations, can be challenging, or expensive. Weak annotations\nprovide a cost-effective alternative with coarser labels.\nThese\nweak annotations can take various forms, including:\n(3.2.1)\nImage-level annotations: Only category labels are provided for\neach training image, lacking precise instance-level information.\n(3.2.2) Point-level annotations: A single specific location or co-\nordinate within an image is marked to highlight a key feature.\n(3.2.3) Scribble-level annotations: A subset of pixels within each\ntraining image is annotated. (3.2.4) Box-level annotations: Ob-\nject bounding boxes are annotated for each training image, offer-\ning coarse localization information but not pixel-level accuracy\n(see Figure 8). In each of these cases, the annotations are less\ndetailed or less precise than full pixel-level annotations, which\npresents challenges but also reduces the labeling effort compared\nto exhaustive pixel-level annotation requirements.\n3.2.1. Learning with image-level supervision\nIn this section, we examine approaches that exclusively rely on\nimage-level supervision for tasks like image detection and seg-\nmentation. It’s worth noting that image-level supervision is com-\nmonly employed to train models for image classification. The\nchallenge here arises from the substantial gap in supervision be-\ntween the high-level information provided by image-level labels\nand the detailed pixel-level predictions required for tasks like de-\ntection and segmentation [128]. In most cases, the Class Acti-\nvation Maps (CAMs) [129] are commonly used as the standard\napproach for producing initial regions of interest using classifica-\ntion models. Essentially, CAMs leverage prior of cross-label con-\nstraints to identify these initial regions within an image based on\nthe information derived from a classification model. Nonetheless,\nthe accuracy of localizing using CAMs is relatively limited. To\ntackle this challenge, researchers have devised multiple strategies\naimed at enhancing CAMs to enable tasks such as segmentation\nwith only image-level supervision. For example, Li et al. [130]\nintroduce an approach named CAM-deep level set (CAM-DLS).\nIn this method, they integrate the DLS loss into the classifica-\ntion loss during the training of the classification network. This\nDLS loss leverages CAMs to emphasize regions within breast\n10\nFigure 8: Illustration of fully supervised mask annotation, weakly supervised box annotation, scribble annotation and point annotation (image from [127]).\ntumors. Similarly, Chen et al. [131] present a causal CAM ap-\nproach for organ segmentation. This method employs the concept\nof causal inference, incorporating a category-causality chain and\nan anatomy-causality chain.\n3.2.2. Learning with point annotation\nPoint annotation involves marking a single specific location or\ncoordinate within an image to indicate a key feature or point of\ninterest. Some works [132, 133, 134] concentrate on employing\nextreme points as annotations for accomplishing pixel-level seg-\nmentation. Specifically, Khan et al. [132] investigate a method\ndesigned to extract information from extreme points and create a\nconfidence map. This map serves as a guide for neural networks\nto comprehend the precise object location within the boundaries\nset by the extreme points. Similarly, Roth et al. [133] utilize\na network that takes two types of input: an image channel and\na point channel representing user-defined extreme points. This\npoint channel is subsequently integrated into the network to pro-\nvide additional guidance during segmentation training. Specif-\nically, it is used as an extra input for attention gates and is in-\ncorporated into the loss function, effectively enhancing the seg-\nmentation process. Nevertheless, these methods demand annota-\ntors to identify the object’s boundary, a task that remains labor-\nintensive in practical applications. In comparison, some meth-\nods [135, 136, 127] employ center point annotation to accomplish\npixel-level segmentation. To achieve this, certain studies employ\nthe Voronoi diagram [137] and clustering algorithms to create ini-\ntial coarse pixel-level labels. Subsequently, various techniques\nare applied to enhance the segmentation outcomes, including it-\nerative optimization [135] and co-training [136, 138]. Zhao et\nal. [127] employ a framework that combines self-training and\nco-training to address cell segmentation. They introduce a diver-\ngence loss to mitigate overfitting and a consistency loss to ensure\nagreement among multiple co-trained networks.\n3.2.3. Learning with scribble-level supervision\nIn this section, we examine techniques related to scribble-\nbased supervision, where annotations are given for a limited num-\nber of pixels, often in the form of manually drawn scribbles.\nThese scribbles essentially act as seed regions. The key challenge\nis to extend semantic information from these sparsely annotated\nscribbles to all other pixels that lack labels. Some approaches ad-\ndress this challenge by aiming to expand the scribbles or recon-\nstruct the complete mask for model training [141, 142, 143]. Nev-\nertheless, the iterative training necessary for the pixel-relabeling\nprocess is time-consuming and susceptible to the introduction of\nnoisy labels. To eliminate the necessity for relabeling, several\napproaches have utilized conditional random fields for refining\nsegmentation results, either in post-processing [144] or as a train-\nable layer [145]. Specifically, Can et al. [144] use region growing\nto create seed areas. They apply a random walk-based segmenta-\ntion method that generates per-pixel probability maps for each la-\nbel, assigning values only when the probability exceeds a specific\nthreshold. However, these methods failed to provide more effec-\ntive guidance for model training. Conversely, alternative tech-\nniques [146, 147] introduced new modules to assess the quality\nof segmentation masks, thereby encouraging the generation of re-\nalistic predictions. For instance, Gabriele et al. [147] proposed\nan adversarial training and an attention gating mechanism to pro-\nduce segmentation masks, leading to enhanced object localization\nacross multiple resolutions, while Zhang et al. [148] leveraged\nthe PatchGAN discriminator to incorporate shape priors. How-\never, these methods required additional data source of complete\nmasks. On the other hand, Zhang et al. [149] utilize mix augmen-\ntation and cycle consistency within the Scribble-Pixel approach.\nThis demonstrates enhancements in both weakly and fully super-\nvised segmentation methodologies. Several studies utilize con-\nsistency learning for scribble-based supervision [150, 151, 152].\nScribble2Label [152] combines guidance signals from scribble\nannotations and pseudo labels using exponential moving averages\nfor cell segmentation. Based on the teacher-student framework,\nGao et al. [150] propose SOUSA, where the student model re-\nceives weak supervision through scribbles and a Geodesic dis-\ntance map created from those scribbles. Simultaneously, a sub-\nstantial volume of unlabeled data containing different forms of\nperturbations is provided to both the student and teacher mod-\nels. The alignment of their output predictions is enforced using a\ncombination of Mean Square Error (MSE) loss and a Multi-angle\nProjection Reconstruction (MPR) loss.\n11\nTable 3: Overview of recent methods in Inexact supervision category.\nReference\nTask\nAlgorithm Design\nDataset\nResult\n[2]\nCancer subtype classification\nDomain Adversarial + Multi-scale MIL\nPrivate Dataset: 196 Images\nAcc: 0.871\n[117]\nColorectal cancer staging,\nGraph Attention MIL\nMCO\nAcc: 0.811; F1: 0.798\n[3]\nWhole slide image classification\nTransformer-based MIL\nCAMELYON\n2016;\nTCGA-\nNSCLC; TCGA-RCC\nAcc:\nCAMELYON: 0.8837;\nTCGA-NSCLC:\n0.8835; TCGA-RCC: 0.9466\n[126]\nLymph node metastasis prediction\nTransformer-based MIL + Knowledge\nDistillation\nPrivate Dataset: 595 Images\nAUC: 0.9835; P: 0.9482; R: 0.9151; F1: 0.9297\n[139]\nHistopathology whole slide image\nclassification\nDouble-Tier Feature Distillation MIL\nCAMELYON 2016; TCGA-Lung\nCAMELYON 2016: AUC: 0.946; TCGA-Lung:\nAUC: 0.961\n[105]\nChest X-rays classification\nJointly Classification and Localization\nRSNA-Lung;\nMIMIC-CXR; Pri-\nvate Dataset: 1,003 Images\nAUC: 0.93\n[110]\nBreast cancer classification\nQuantile Function-based MIL\nCBCS3\nAcc: 0.952\n[123]\nCancer classification\nAttention-based MIL\nTMA-UCSB;\nCRCHistoPheno-\ntypes\nTMA-UCSB: Acc: 0.755; CRCHistoPhenotypes:\nAcc: 0.898\n[124]\nPancreatic ductal adenocarcinoma\nclassification and segmentation\nJointly Global-level Classification and\nLocal-level Segmentation\nPrivate Dataset: 800 Images\nDSC: 0.6029; Sens: 0.9975\n[7]\nDetection of lymph node metas-\ntases\nHybrid MIL\nMSK breast cancer\nAUC: 0.965\n[140]\nBreast Cancer (HER2 scoring: neg-\native, equivocal and positive)\nHybrid MIL\nPrivate dataset: 1105 cases\nAccuracy: 0.8970\n[130]\nBreast tumor segmentation\nCAM + Level-Set\nPrivate dataset: 3062 BUS images\nDSC: fat 0.830\n0.118; mammary gland 0.843\n0.100; muscle 0.807\n0.154; thorax layers 0.910\n0.114\n[131]\nSegmentation\nCausal Inference; CAM\nACDC; ProMRI; CHAOS\nProMRI\nDSC:\n0.8640.004;\nASD:\n3.861.20;\nMSD: 3.851.33 Abdominal Organ ACDC DSC:\n0.8750.008;\nASD: 1.620.41;\nMSD: 1.170.24\nCHAOS DSC: 0.781\n[132]\nMulti-organ segmentation\nConfidence Map Supervision\nSegTHOR\nDSC Aorta: 0.9441\n0.0187; Esophagus 0.8983\n0.0416\n[133]\nMulti-organ segmentation\nRandom Walker + Iterative Training\nBTCV; MSD; CT-ORG\nMO-Liver 0.956\n0.010;\nMO-Pancreas 0.747\n0.082; DSC: MSD-spleen 0.958\n0.007; MO-\nSpleen 0.954 0.027\n[134]\nBrain tumor segmentation\nCNN + CRF\nVestibular-Schwannoma-SEG\nDSC: 0.8190.080; HD95: 3.77.4; P: 0.9290.059\n[136]\nMulti-organ segmentation\nCo-/Self-Training\nMoNuSeg; CPM\nMoNuSeg DSC: 0.7441; AJI: 0.5620; CPM DSC:\n0.7337; AJI: 0.5132\n[127]\nCell segmentation\nSelf-/Co-/Hybrid-Training\nPHC; Phase100\nDSC PHC: 0.871; Phase 100: 0.811\n3.2.4. Learning with box-level supervision\nIn this section, we evaluate approaches for semantic segmen-\ntation guided by box-level supervision. Utilizing box-level su-\npervision proves to be a more robust substitute for image-level\nguidance, as it inherently reduces the exploration area for object\ndetection. For object segmentation, Rajchl et al. [153] recover\npixel-wise annotations given a database of images with corre-\nsponding bounding boxes. To achieve this goal, they devise an it-\nerative energy minimization problem within a densely connected\nconditional random field framework to adjust and refine the pa-\nrameters of a CNN model throughout the iterative process. Wang\net al. [154] utilize MIL and a smooth maximum approximation\nmethod based on the concept of bounding box tightness. In this\ncontext, bounding box tightness implies that an object instance\nshould have contact with all four sides of its bounding box. Con-\nsequently, if there is a vertical or horizontal crossing line within\nthe box, it results in a positive bag classification because it cov-\ners at least one foreground pixel. In the work presented by [155],\nthey introduce a fusion filter sampling (FFS) module designed to\ncreate pixel-level pseudo labels from box annotations while min-\nimizing noise.\n4. Incomplete supervision\nIncomplete supervision refers to a scenario where we have\naccess to a limited quantity of labeled data, which is inade-\nquate for training an effective learner, while there exists a large\npool of unlabeled data. We categorize incomplete supervision\ninto three broad subcategories: Semi-supervised Learning, Ac-\ntive Learning, and Domain-adaptive Learning (Figure 9). Semi-\nsupervised learning aims to enhance learning performance by\nleveraging both labeled and unlabeled data automatically.\nIn\nDomain-adaptive Learning, a domain shift occurs between la-\nbeled and unlabeled data. Conversely, Active learning operates\non the assumption that there is an oracle, like a human expert,\nwho can be consulted to obtain ground-truth labels for specific\nunlabeled instances. A summary of recent methods for learning\nwith incomplete supervision is provided in Table 4.\n4.1. Semi-supervised learning\nIn this section, we will examine techniques used in semi-\nsupervised learning (Semi-SL). In this approach, only a small\nportion of the training images have annotations, while the ma-\njority of training images remain unannotated. The goal of semi-\nsupervised learning is to incorporate the vast number of unla-\nbeled training images into the training process in order to enhance\nmodel performance [156, 157]. Semi-supervised Learning can be\ncategorized into Consistency regularization, Generative, Pseudo-\nlabeling, and Hybrid methods.\n12\nFigure 9: Taxonomy of Incomplete Supervision methods.\n4.1.1. Consistency regularization methods\nConsistency regularization methods rely on the concept of\nsmoothness or manifold assumption, suggesting that perturbing\ndata points should not alter the model’s predictions. Importantly,\nthis approach does not rely on label information, making it an ef-\nfective constraint for learning from unlabeled data. Within this\nframework, various perturbations are available and can be classi-\nfied into two categories: input perturbations and feature map per-\nturbations. These perturbations must be relevant and meaningful\nfor the specific task at hand. Commonly employed input pertur-\nbations encompass random rotation, Gaussian blurring, Gaussian\nnoise, contrast variations, and scaling. Notably, Bortsova et al.\n[158] and Li et al. [159] employ consistency learning by apply-\ning different transformations to input images. Another widely\nadopted form of consistency is mix-up consistency [160, 161],\nwhere the segmentation of interpolation of two inputs is encour-\naged to remain consistent with the interpolation of segmentation\nresults for those inputs. Moreover, recent investigations by [162]\nand [163] delve into perturbations at the feature map level. Zheng\net al. [162] propose a method that introduces random noise into\nthe parameter calculations of the teacher model. Li et al. [163] in-\ntroduce seven distinct feature perturbations, each associated with\nan additional decoder, all conditioned on maintaining consistency\nwith the primary decoder. Furthermore, there are studies that si-\nmultaneously apply perturbations at both the input and feature\nmap levels [164, 165].\nIn contrast to incorporating perturbations, alternative consis-\ntency learning techniques are also available. For instance, the\nπ-model [166] is a straightforward yet powerful approach that\nutilizes a shared encoder to generate various views of the input\nsample through augmentation. It enforces the classifier to provide\nconsistent predictions for different augmentations of the same in-\nput. Simultaneously, the training process incorporates label in-\nformation to enhance the classifier’s overall performance. Li et\nal. [167] developed a semi-supervised algorithm for skin lesion\nsegmentation based on the π-model approach. Temporal ensem-\nbling [168] was created with the aim of enhancing the prediction\nstability of the π-model. This is achieved by incorporating an ex-\nponentially moving average module to update predictions. Sev-\neral researchers have adopted this module to tackle MIA related\nchallenges [169, 170]. To achieve precise breast mass segmen-\ntation, Cao et al. [169] incorporate uncertainty into the temporal\nensembling model. They utilize uncertainty maps as guidance for\nFigure 10: Illustration of the Uncertainty-Aware Mean Teacher (UA-MT) frame-\nwork. The student model is trained by minimizing the supervised loss Ls on\nlabeled data and the consistency loss Lc on both unlabeled and labeled data. The\nteacher model’s estimated uncertainty is used to instruct the student in learning\nfrom the more dependable teacher-provided targets (image courtesy of Yu et al.\n[172]).\nthe neural network to ensure the reliability of the generated pre-\ndictions. Likewise, Luo et al. [170] suggest an uncertainty-aware\ntemporal ensembling method for chest X-ray disease screening.\nIn the training process of temporal ensembling, the activation of\neach training sample is updated only once in one epoch. Mean\nteacher (MT) [171] overcomes this limitation by applying expo-\nnentially moving average on model parameters instead of net-\nwork activations. Several methods enhance the MT framework\nfor its application in MIA contexts [172, 173, 174, 175]. To en-\nhance the performance of the MT, Yu et al. [172] introduced the\nUncertainty-Aware Mean Teacher (UA-MT) framework (see Fig-\nure 10) for 3D left atrium segmentation. In this approach, the\nteacher model, in addition to producing target outputs, also as-\nsesses the uncertainty associated with each target prediction us-\ning Monte Carlo sampling. This allows the removal of unreliable\npredictions, retaining only those with low uncertainty for consis-\ntency loss calculations. This process offers more reliable guid-\nance to the student model, promoting the teacher model to pro-\nduce higher-quality target predictions. Wang et al. [174] incorpo-\nrated multi-task learning into the mean teacher framework includ-\ning segmentation, reconstruction, and SDF prediction tasks to en-\nhance data, model, and task consistency. Additionally, they in-\ntroduced an uncertainty-weighted integration (UWI) approach to\nassess uncertainty across all tasks and created a triple-uncertainty\nmethod to guide the student model to learn reliable information\nfrom the teacher.\nRecently, Xu et al. [176] present a dual uncertainty-guided\nmixing consistency network for precise 3D semi-supervised seg-\nmentation, emphasizing the consideration of context information\nat the volume level.\nTo segment surgical images, Lou et al.\n[177] propose a Min-Max Similarity (MMS) method. This ap-\nproach adopts a dual-view training strategy, utilizing classifiers\nand projectors to construct pairs of all-negative features and posi-\ntive/negative feature pairs. This formulation transforms the learn-\ning process into solving an MMS problem.\n4.1.2. Generative methods\nThe generative adversarial network (GAN) has shown potential\nperformance on semi-supervised learning [178, 179, 180]. GANs\nconsist of two main parts: a generator and a discriminator. The\ngenerator’s goal is to deceive the discriminator by producing fake\ndata that appears real, while the discriminator aims to distinguish\nbetween real and synthetic data (see Figure 11(B)). These two\n13\nnetworks engage in a zero-sum game, where any gain made by\none network comes at the expense of the other. There are differ-\nent ways to use GANs in Semi-SL settings. One such approach\ninvolves employing adversarial techniques to encourage the out-\nputs of unlabeled images to closely resemble those of the labeled\nimages [181, 182]. Peiris et al. [182] incorporate a critic network\ninto their segmentation architecture. This network engages in a\nmin-max game by distinguishing between the predicted masks\nand the actual ground truth masks. The outcomes of their exper-\niments indicate that this approach can enhance the definition of\nboundaries in the prediction masks. Additionally, the discrimi-\nnator can be employed to generate pixel-wise confidence maps,\nfacilitating the selection of reliable pixel predictions for consis-\ntency learning. The study by Wu et al. [179] introduces a pair\nof discriminators to anticipate confidence maps and differentiate\nbetween segmentation outcomes originating from labeled or un-\nlabeled data. Constrained Adversarial Training (CAT) [180] fo-\ncuses on generating anatomically accurate segmentations. This\nmethod incorporates unlabeled samples into an adversarial train-\ning framework, which serves to regularize the network and facil-\nitate constraint learning.\nHou et al. [183] use a GAN-based framework with three en-\nhancements: First, a U-Net style network is employed as the dis-\ncriminator. Second, a polluted discriminator is introduced, incor-\nporating auxiliary leaking links from the generator to encourage\nthe generation of moderate, though unrealistic, samples, thereby\nenhancing semi-supervised learning. Third, the discriminator un-\ndergoes regularization via the mean-teacher mechanism, enhanc-\ning segmentation generalization through input and weight pertur-\nbations. Certain approaches employ GANs as a method for data\naugmentation within the context of Semi-SL. For instance, Chai-\ntanya et al. [184] integrate unlabeled data directly into GAN’s ad-\nversarial training process to enhance the generator’s performance\nfor improving medical data augmentation. They assert that in-\ncorporating unlabeled samples enables greater diversity in terms\nof shape and intensity, thereby enhancing the model’s robustness\nand guiding the optimization process.\nA Variational Autoencoder (VAE) [185] consists of two main\ncomponents: an encoder that transforms input data into a latent\nrepresentation and a decoder that reconstructs the latent represen-\ntation into the original data space. In order to regularize the en-\ncoder of the VAE, a prior over the latent distribution is commonly\nintroduced (see Figure 11(A)). As one of the initial attempts to\napply VAE to semi-supervised segmentation tasks, Sedai et al.\n[178] employed a dual-VAE approach for segmenting the optic\ncup in retinal fundus images. This method involved two VAEs,\nwhere one VAE learned the data distribution from unlabeled data\nand transferred its acquired knowledge to the other VAE respon-\nsible for segmentation using labeled data. Wang et al. [186] ex-\ntended the VAE architecture to 3D medical image segmentation\nby introducing a mean vector and covariance matrix to account\nfor correlations across different slices within an input volume.\n4.1.3. Pseudo-labeling methods\nIn pseudo-labeling, a model is trained on the available labeled\ndata. It then predicts labels for unlabeled samples with high con-\nfidence, effectively creating pseudo-labels. Finally, the model is\nretrained using both the labeled data and these newly generated\npseudo-labeled samples, improving its performance through the\nutilization of additional unlabeled data. Pseudo-labeling methods\ncan be mainly categorized into two sub-categories: Self-training\nmethods and Co-training learning methods.\nSelf-training models: In the self-training framework, an ini-\ntial model is trained using limited labeled data. Then, this initial\nmodel is utilized to generate pseudo labels for the unlabeled data.\nSubsequently, the labeled dataset is combined with the pseudo-\nlabeled dataset to update the initial model. The training process\niteratively alternates between these two steps until a predeter-\nmined number of iterations is reached. Self-training approaches\nprimarily vary in terms of model initialization, pseudo label gen-\neration, and their strategies for addressing pseudo label noise.\nAccording to the study by [189], pseudo labels with higher con-\nfidence tend to be more effective. Consequently, various meth-\nods that take into account confidence or uncertainty in pseudo\nlabels have been introduced to generate more consistent and re-\nliable pseudo labels, such as refining pseudo labels through con-\nditional random fields [190], uncertainty-aware confidence eval-\nuation [191]. Similarly, Ke et al. [192] proposed a three-stage\nself-training framework to refine pseudo labels in a stage-wise\nmanner. It reduces the uncertainty in the predicted probability\nfor the pseudo-masks using a multi-task model. Inf-net [188]\naddresses the shortage of well-annotated data for segmentation\nof COVID-19 lung infections in CT images. Further, a parallel\npartial decoder (PPD), reverse attention (RA), and edge attention\nwere further added to improve the performance of the model, as\nshown in Figure 12. In contrast to conventional pseudo-labeling\ntechniques, which rely on a threshold to pick confidently clas-\nsified samples, Liu et al.\n[193] propose the Anti-Curriculum\nPseudo-labeling (ACPL) method. ACPL utilizes a mechanism\nknown as cross-distribution sample informativeness to identify\nhighly informative unlabeled samples for pseudo-labeling. It also\nemploys an ensemble of classifiers to generate precise pseudo-\nlabels. This approach enables ACPL to effectively handle multi-\nclass and multi-label imbalanced classification issues in the field\nof MIA.\nRecently, Chen et al. [194] introduced a teacher-student frame-\nwork for multi-organ segmentation in CT scans. They proposed\na learning paradigm involving N3 small cubes extracted from\neach CT scan, called magic-cubes. Two data augmentation strate-\ngies were designed. First, labeled and unlabeled data cubes were\nmixed to teach unlabeled data organ semantics in their relative po-\nsitions. Second, for smaller organs, data cubes were shuffled and\nfed into the student network. Finally, the original magic-cubes\nwere reconstructed to align with the ground-truth or teacher’s su-\npervision. Further, the teacher network’s predicted pseudo labels\nare improved by blending them with the learned representations\nof the small cubes. This blending strategy considers local at-\ntributes like texture, luster, and boundary smoothness, addressing\nthe lower performance observed for smaller organs.\nCo-training models: In the Co-training framework [195], a\nmodel is trained on a dataset with two or more views or represen-\ntations of the data. These views are typically different but com-\nplementary. The key idea is that if each view provides unique\ninformation about the data, the model can learn more effectively\nfrom the combined knowledge of all views. In contrast to the\nself-training framework, which expands the labeled dataset based\non a single model’s confidence, co-training iteratively selects in-\nstances on which the model is confident based on different views,\nexpanding the labeled dataset with complementary information.\nThe essence of co-training lies in the process of creating two\nor more deep models that can effectively capture distinct and\nnearly independent perspectives. These approaches typically in-\n14\nFigure 11: Illustration of VAE and GAN architectures: (A) In the VAE architecture, there is an encoder-decoder structure. Here, Zµ represents the mean vector, Zσ\ndenotes the standard deviation vector, and Z is the sampled latent vector. (B) In the GAN architecture, there are both a generator and a discriminator (image courtesy\nof [187]).\nvolve utilizing diverse data sources, implementing various net-\nwork architectures, and applying specialized training techniques\nto acquire a range of diverse deep models [156]. In the context\nof medical images, data can originate from various modalities or\nmedical centers, resulting in distinct distributions. In this regard,\n[196] and [197] make use of different views derived from diverse\nmodalities within the co-training framework. Some approaches\nemploy different network architectures as distinct views. For in-\nstance, Luo et al. [198] propose cross-teaching between CNN\nand Transformer models, which implicitly promotes consistency\nand complementarity between these distinct networks. Peng et\nal. [199] generate adversarial examples as an alternative view.\nSimilarly, for 3D images, Zhao et al.\n[200] utilize coronal,\nsagittal, and axial views of images as diverse input views. Re-\ncently, Wang et al. [201] address the issue of imbalanced class\ndistribution in Semi-SL methods using the Dual-debiased Het-\nerogeneous Co-training (DHC) framework. They introduce two\nloss weighting techniques called Distribution-aware Debiased\nWeighting (DistDW) and Difficulty-aware Debiased Weighting\n(DiffDW). These strategies utilize pseudo labels dynamically to\nhelp the model address data and learning biases effectively.\n4.1.4. Hybrid models\nAn emerging area of research in Semi-SL involves integrat-\ning the previously mentioned methods into a unified framework\nto achieve improved performance. These combined approaches\nare referred to as hybrid methods [202, 203, 204]. Several studies\nhave explored the combination of pseudo-labeling and contrastive\nlearning methods [205, 206, 207, 208] for different tasks. Specifi-\ncally, both Chaitanya et al. [205] and Basak et al. [206] introduce\na self-training method based on local contrastive learning, guided\nby pseudo-labels, and demonstrate its effectiveness across var-\nious medical segmentation datasets. For COVID-19 Screening\nand Lesion Segmentation, Zeng et al. [208] present a double-\nthreshold pseudo-labeling approach and a novel inter-slice con-\nsistency regularization technique designed specifically for CT im-\nages. Wang et al. [202] utilize self-training with consistency reg-\nularization to efficiently extract valuable information from unla-\nbeled data, and they incorporate virtual adversarial training to en-\nhance the model’s generalization capability. ASE-Net [209] com-\nprises segmentation networks and a discriminator network. The\nsegmentation network is constructed using the MT framework,\nwhile the discriminator network employs an adversarial consis-\ntency training strategy (ACTS) with two discriminators focused\non consistency learning. This strategy helps establish prior rela-\ntionships between labeled and unlabeled data.\n4.2. Active learning\nActive learning (AL) [210] operates on the assumption that\nthe ground-truth labels of unlabeled instances can be obtained\nby querying an expert annotators (see Figure 13). Assuming that\nthe labeling cost is solely based on the number of queries, the\nobjective of active learning is to minimize the number of queries\nrequired while still achieving effective model training with mini-\nmized labeling costs. In situations where there is a limited set of\nlabeled data but an abundance of unlabeled data, active learning\naims to identify the most valuable unlabeled instance for query-\ning. There are two commonly used selection criteria: informa-\ntiveness and representativeness. Informativeness assesses how\neffectively an unlabeled instance reduces the uncertainty of a sta-\ntistical model, while representativeness calculates how well an\ninstance represents the structure of input patterns.\n4.2.1. Evaluating informativeness\nThe primary category of informativeness measures revolves\naround calculating uncertainty. The key idea is that including the\nground truth for samples with higher uncertainty in the training\n15\nFigure 12: Illustration of the Inf-net framework: CT images are initially processed through two convolutional layers for the extraction of high-resolution (i.e., low-\nlevel) features. An edge attention module is incorporated to enhance the representation of boundaries within the region of interest. Subsequently, the obtained low-level\nfeatures, denoted as f2, undergo three convolutional layers to extract high-level features. These high-level features serve two primary purposes. Firstly, they are used\nto feed a parallel partial decoder (PPD), which aggregates these features and generates a global map denoted as S g. This global map aids in the coarse localization of\nlung infections. Secondly, these high-level features, along with f2, are directed through multiple cascaded reverse attention (RA) modules under the guidance of S g.\nThe RA module R4 depends on the output of another RA module, R5. Finally, the output of the last RA module, denoted as S 3, is passed through a sigmoid activation\nfunction for the final prediction of lung infection regions (image from [188]).\nUnlabeled\ndata\nLabeled\ndata\nSupervised model training\nActive learning \nNewly-labeled\ndata\nLabeled\ndata\nRe-training\nHuman\nannotation\nInitialization\nSelect most representative and uncertain examples/areas for annotation\nHuman-in-the-Loop\nFigure 13: Overview of the active learning paradigm: In a cycle, a deep learning\nmodel is trained on a labeled medical dataset. Then, active sampling strategies\nare implemented to select the data that is most valuable to the model from an\nunlabeled medical dataset. Finally, oracles are used to annotate the selected data.\nImage courtesy of Peng and Wang [211].\nset can provide more valuable information. In the deep learn-\ning area, uncertainty-based sampling has seen widespread usage\nin recent active learning methods [212, 213, 214]. Specifically,\nWen et al. [213] introduce an active learning approach that em-\nploys uncertainty sampling to facilitate quality control of nucleus\nsegmentation in pathology images. Wu et al. [214] use both the\nnetwork loss and diversity condition as the uncertainty metric for\nsampling from a loss prediction network. They apply this method\nto the COVID-19 classification task. Zhou et al. [215] introduce\nthe concept of active selection policies, where the highest con-\nfidence is determined based on the entropy and diversity of the\nsampled data in the mean prediction outcomes. For classifying ra-\ndiology images, Balram et al. [216] introduce an integrated end-\nto-end solution that merges consistency-driven semi-supervised\nlearning with uncertainty-guided active learning, aiming to allevi-\nate the need for extensive manual annotations. Another prevalent\napproach for estimating informativeness involves assessing the\nagreement among various models executing the same task. The\nreasoning is that greater disagreement observed between predic-\ntions on similar data points indicates a higher degree of uncer-\ntainty. These techniques are commonly employed in situations\nwhere ensembling is utilized to enhance performance. Ensem-\nbling involves training multiple models to execute the same task\nwith slight variations in parameters or settings [217, 218]. Beluch\nBcai et al. [218] showcase the effectiveness of ensembles in ac-\ntive learning and compare them to alternative approaches. Kuo et\nal. [217] employed an ensemble technique to assess uncertainty\nin the context of intracranial hemorrhage segmentation, utilizing\nthe Jensen-Shannon divergence. Additionally, they made an ef-\nfort to predict the time required for manual delineation using a\nlog-linear model. Their approach involved selecting examples for\nmanual segmentation based on maximizing the cumulative uncer-\ntainty within a specified time constraint. Atzeni et al. [219] adopt\nan iterative method, requesting manual delineation for a single\nRegion of Interest (ROI) on a single slice per iteration rather than\nlabeling all structures within a slice or volume.\nThey update\na segmentation CNN that generates dense segmentations for all\nslices using mixed-cross entropy loss, effectively utilizing par-\ntially annotated images. Similar to Kuo et al. [217], they use\ntracing time, based on boundary length, as a practical measure of\neffort. However, in contrast to Kuo et al. [217], they also account\nfor multiple ROIs and their spatial relationships.\nBayesian neural networks have gained significant interest due\nto their capacity to represent and propagate the probability of\ndeep learning models. Gal et al. [220] introduce the concept of\nusing Bayesian CNNs for AL, specifically employing Bayesian\nActive Learning by Disagreement (BALD). Their study demon-\nstrates the superior performance of Bayesian CNNs compared to\ndeterministic CNNs within the context of AL. Mahapatra et al.\n[221] employ a conditional GAN to generate chest X-ray images\nbased on a real image. Additionally, they use a Bayesian neu-\nral network to assess the informativeness of each generated sam-\nple, determining whether it should be utilized as training data.\nIf selected, the sample is used to fine-tune the network. Their\nstudy demonstrates that this method achieves comparable perfor-\nmance to training on fully labeled data, even when working with\na dataset where only 33 % of the pixels in the training set have an-\nnotations. This provides significant time, effort, and cost savings\nfor annotators. Dai et al. [222] proposed a distinctive method for\nbrain tumor segmentation. Instead of traditional approaches, they\n16\nadopted a novel strategy to select the most informative example.\nThis involved moving through the image space along the gradient\ndirection of the Dice loss and identifying the nearest neighbor of\nthis image within a lower-dimensional latent space, which was\nlearned using a variational autoencoder. Certain studies address\nthe challenge of the cold start problem in Active Learning, which\npertains to the initial selection of images for labeling when no la-\nbeled data is available as a starting point [223, 224]. Nath et al.\n[223] address the issue of cold start by introducing a proxy task\nand subsequently leveraging the uncertainty generated from this\nproxy task to prioritize the annotation of unlabeled data.\n4.2.2. Representativeness\nThese approaches go beyond relying solely on uncertainty-\nbased methods and instead focus on evaluating the diversity\nwithin chosen samples to minimize repetitive annotations. By\nintroducing a representativeness measure, these strategies aim to\npromote the selection of samples from various areas of the dis-\ntribution, leading to greater sample diversity and ultimately en-\nhancing the performance of AL. To this end, Yang et al. [225]\nintroduce Suggestive Annotation, a deep active learning frame-\nwork designed for medical image segmentation. This framework\nutilizes a different approach to uncertainty sampling and incorpo-\nrates a form of representativeness density weighting. The method\ninvolves training multiple models, each of which excludes a por-\ntion of the training data. These models are then leveraged to cal-\nculate an ensemble-based uncertainty measure. Ozdemir et al.\n[226] create a Bayesian network and utilize Monte Carlo dropout\nto derive variance information as a measure of model uncertainty.\nIn addition, they employ infoVAE [227] to build a representative-\nness metric, which aids in the selection of samples through max-\nimum likelihood sampling within the latent space. Li et al. [228]\nadopt k-means clustering and curriculum classification (CC) tech-\nniques, leveraging CurriculumNet [229], to estimate uncertainty\nand representativeness in their approach. Li et al. [224] tackle\nthe challenge of the cold start problem by employing represen-\ntativeness sampling that relies on the distance matrix to choose\nan initial dataset that is representative. They also introduce a hy-\nbrid sample selection approach that incorporates pixel entropy,\nregion consistency, and image diversity scores to filter the sam-\nples. These three scores reflect informativeness at different levels:\npixel, region, and image. This strategy, which combines these\nthree levels of scores, proves to be more effective in selecting the\nmost valuable samples compared to using a simple pixel uncer-\ntainty score alone. Wang et al. [230] utilize model ensembles to\nguide user labeling, focusing on cells that optimize a blend of un-\ncertainty, diversity (evaluated using a clustering algorithm), and\nrepresentativeness assessed through cosine similarity of features.\n4.3. Domain-adaptive learning\nDomain adaptive learning, also known as domain adaptation\n[231], is a learning paradigm focused on improving the perfor-\nmance of a model on a target domain by leveraging knowledge\nlearned from a source domain. In this context, a domain refers to\na specific distribution of data, which can vary in terms of charac-\nteristics like data collection settings, sensor types, lighting con-\nditions, or other factors that affect the data’s distribution. The\nmain challenge addressed by domain adaptive learning is the do-\nmain shift problem. This problem arises when there is a mis-\nmatch between the source domain (where the model is trained)\nand the target domain (where the model needs to perform well).\nDue to this mismatch, a model trained on one domain may not\ngeneralize effectively to another domain. Domain adaptive learn-\ning methods aim to bridge this gap by adapting the model to the\ntarget domain. Unsupervised Domain Adaptation (UDA) is a spe-\ncific case of domain adaptation where you only have labeled data\nin the source domain and no labeled data in the target domain.\nThe adaptation process is entirely unsupervised, meaning it relies\nsolely on unlabeled data in the target domain. These methods en-\ncompass various approaches, including feature alignment, image\ntranslation-based methods, learning disentangled representations,\npseudo-labeling approaches, self-supervision, and hybrid meth-\nods.\n4.3.1. Feature alignment\nThe fundamental idea behind feature alignment in UDA is\nto lessen the distinction between the source and target domains\nby learning domain-invariant representations. Various UDA ap-\nproaches map images from both domains onto a common latent\nspace to mitigate disparities. This can be accomplished directly\nby reducing a disparity measure that quantifies domain dissimi-\nlarities. Alternatively, it can be realized implicitly through adver-\nsarial learning techniques. The objective is to align the feature\ndistributions of both the source and target domains, ensuring that\nthe learned representations can be smoothly transferred and ef-\nfectively utilized in diverse domains.\nExplicit discrepancy minimization: Methods focused on ex-\nplicitly minimizing discrepancies usually create a measure or loss\nfunction that calculates how different the source and target distri-\nbutions are from each other. This measure is then reduced during\ntraining to encourage the development of features that work well\nin both domains. Different measures, like Maximum Mean Dis-\ncrepancy (MMD) [232, 233], Kullback-Leibler (KL) divergence\n[234], and Contrastive Loss (CL) [235, 236], can be employed\nfor this purpose. Specifically, Yu et al. [232] use two separate\nfeature encoders for both the target and source domains. They in-\ntegrate an attention technique to focus on particular brain regions\nand employ MMD to acquire features that work well across do-\nmains for the prediction of subjective cognitive decline. Another\nexplicit measurement used in UDA is the Characteristic Function\n(CF) distance [237]. This metric calculates the distinction be-\ntween the distributions of latent features in the frequency domain\ninstead of the spatial domain.\nImplicit discrepancy minimization: Implicit methods for re-\nducing differences in UDA mainly rely on the concepts of ad-\nversarial learning. To ensure that feature distributions are com-\nparable between different domains, a technique called domain-\nadversarial neural network (DANN) [238] is used. This approach\ninvolves incorporating a gradient reversal layer (GRL) into the\nframework of Generative Adversarial Networks (GANs), as il-\nlustrated in Figure 14. The network comprises two classifiers and\nshared feature extraction layers. With the help of GRL, DANN\naims to maximize the loss due to domain confusion while mini-\nmizing the loss associated with label prediction for source sam-\nples and domain confusion loss for all samples. DANN serves\nas a foundational model for different UDA methods that are built\nupon adversarial learning principles.\nDifferent research studies employ implicit techniques for a\nrange of classification issues. For instance, Ren et al. [239] uti-\nlize an adversarial loss along with siamese architecture for whole\nslide images. Zhang et al. [240] leverage adversarial learning and\nintroduce focal loss to tackle the problem of class imbalance in\n17\nFigure 14: The figure demonstrates the Domain Adversarial Neural Network\n(DANN) framework, a classic and effective model designed for learning domain-\ninvariant features using adversarial training (image courtesy of Ganin [238]).\nhistopathology images. More recently, Feng et al. [241] engage\nin binary and multi-class classification tasks related to diagnosing\npneumonia. They make use of a conditional domain adversarial\nnetwork to narrow the domain discrepancy and implement a con-\ntrastive loss to address the challenge of limited data in the tar-\nget domain. Certain investigations have combined self-training\nand adversarial learning for the task of medical image segmen-\ntation [242, 243, 244]. Specifically, Liu et al. [243] proposed\nthe Self-cleansing UDA (S-cuda) technique, which is specifically\ndesigned to address the issue of domain shift and handle noisy\nlabels in the source domain. This method utilizes self-training\nto produce accurate pseudo-labels for both the noisy source and\nunlabeled target domains. Beyond image classification and seg-\nmentation, various other applications also make use of implicit\ndiscrepancy methods. For instance, these methods are applied\nin bronchoscopic depth estimation [245], reconstructing precise\nhigh-resolution (HR) representations from low-resolution (LR)\nOCTA images [246], and automating sleep staging [247].\n4.3.2. Image translation based methods\nImage translation techniques achieve domain alignment by al-\ntering the pixel-level appearance of source data to match the char-\nacteristics of a target domain. Generative Adversarial Networks\n(GANs) are often used for tasks involving direct mapping be-\ntween pixels for image translation. A widely used approach in\nthis category is CycleGAN [248], which operates as an image-\nto-image translation architecture (see Figure 15). It transforms\nfeatures from one image domain into another without relying on\npaired training examples. In the medical field, several approaches\napply CycleGAN for unsupervised domain adaptation (UDA).\nHowever, CycleGAN’s emphasis on pixel-level mapping might\nnot consistently ensure the preservation of semantic information\nin medical images. To overcome this limitation, multiple studies\nhave integrated semantic understanding into the framework. Var-\nious works [249, 250, 251] have incorporated task-specific losses\nwithin the UDA context. These task-specific losses are designed\nto enhance the UDA procedure by introducing extra constraints\naligned with the unique requirements of the task at hand.\nCertain works employ attention mechanisms to capture distant\nrelationships [252, 253]. In the context of cross-modality do-\nmain adaptation, Tomar et al. [252] employ a dual cycle consis-\ntency loss to maintain semantic content while performing image\ntranslation. They propose a self-attentive spatial adaptive normal-\nization technique that comprises two components: the synthesis\nmodule and the attention module. The synthesis module’s inter-\nmediate layers receive semantic layout information from the at-\ntention module, aiding in the learning of the translation process.\nX\nY\nG\nF\nDY\nDX\nG\nF\nˆY\nX\nY\n(\nX\nY\n(\nG\nF\nˆX\n(a)\n(b)\n(c)\ncycle-consistency\nloss\ncycle-consistency\nloss\nDY\nDX\nˆy\nˆx\nx\ny\nFigure 15: Illustration of CycleGAN framework: (a) CycleGAN comprises two\nmapping functions G : X →Y and F : Y →X, accompanied by adversarial\ndiscriminators DY and DX. DY encourages G to translate X into outputs indistin-\nguishable from domain Y, while DX performs the reverse task. (b) The forward\ncycle-consistency loss is represented as: x →G(x) →F(G(x)) ≈x. (c) The back-\nward cycle-consistency loss is represented as: y →F(y) →G(F(y)) ≈y (image\ncourtesy of Zhu et al. [248]).\nCertain studies exploring UDA in image detection also employ\nimage translation techniques. For instance, Xing et al. [254]\ndelve into UDA for cell detection across different data modali-\nties. They leverage the CycleGAN framework to adjust source\nimages to align with the target domain. Their methodology in-\nvolves training a structured regression-based object detector us-\ning these adapted source images. Furthermore, they refine the\ndetector by incorporating pseudo-labels derived from the target\ntraining dataset. Extending their earlier study, Xing et al. [255]\nenhance their method by introducing bidirectional mapping. This\ninvolves translating images both from the source to the target and\nvice versa. They also expand this framework to address the semi-\nsupervised scenario. In a subsequent extension of their research,\nXing and Cornish [256] tackle not just the UDA challenges in\ncell/nucleus detection but also address the challenge of having\nscarce training data in the target domain.\n4.3.3. Learning disentangled representations\nRather than imposing the demanding requirement of making\nthe entire model or features domain-invariant, an alternative ap-\nproach is to ease this constraint by permitting certain components\nto be domain-specific [257]. This essentially involves acquiring\ndisentangled representations. The key idea of disentangled rep-\nresentation is to differentiate between the content and style of\nan image.\nThe underlying premise is that the content, which\nrefers to anatomical information, remains uniform across do-\nmains, whereas the style, encompassing attributes like texture and\nlighting, is specific to each domain. In the process of achieving\ndisentangled representation [258], initial steps involve extracting\nstyle and content codes from both the source and target images us-\ning specialized encoders. Subsequently, generators are employed\nto create images in the opposite domains by combining content\ncodes from one domain with style codes from the other. This in-\nterplay of generators aims to deceive discriminators by generating\nimages that confuse the domains’ distinguishing features, leading\nto the desired disentangled representation. Wang et al. [259] in-\ncorporated the segmentation stage and diverse image translation\nstage into a cohesive end-to-end approach.\nSun et al. [260] employ a combination of the attention mech-\nanism and disentanglement to further mitigate the disparities be-\ntween domains. Specifically, they adopt a preliminary alignment\nphase to address issues like variations in brightness between MRI\nand CT images.\nFollowing this, they introduce an improved\napproach to disentanglement that leverages the Hilbert-Schmidt\nindependence criterion to encourage independence and com-\nplementary characteristics between content and style attributes.\n18\nLastly, they integrate an attention bias mechanism to emphasize\nthe alignment of regions relevant to the task of cardiac segmen-\ntation. Several studies enhance disentanglement learning by em-\nploying various approaches. For example, Xie et al. [261] utilize\na zero loss to ensure that the domain-specific encoder only cap-\ntures information from its corresponding domain. Similarly, Yang\net al. [262] implement a coarse-to-fine prototype alignment pro-\ncess before feature disentanglement to enhance the separation of\nfeatures.\n4.3.4. Pseudo-labeling approach\nPseudo-labeling is a widely used strategy in UDA to make use\nof unlabeled data in the target domain. This method involves\ngenerating pseudo-labels to unlabeled data in the target domain\nusing a model trained on labeled data from the source domain.\nNevertheless, these pseudo-labels can be inaccurate due to the\ndomain gap, leading to noise. Thus, a crucial aspect of pseudo-\nlabeling is how various networks reduce the uncertainty and elim-\ninate noise from the pseudo-labels to enhance their precision. To\nreduce the uncertainty of pseudo labels, Wu et al. [263] introduce\nan uncertainty-aware model that integrates Monte Carlo dropout\nlayers into a U-Net architecture. Likewise, the Strudel approach\n[264] involves incorporating uncertainty details into the training\nprocess through an uncertainty-guided loss function. This aids in\neliminating labels with low level of certainty. Some studies adopt\na curriculum learning strategy, beginning with simpler instances\nto facilitate the model’s learning process and gradually introduc-\ning more complex cases over time [265, 266]. When facing a\nsituation where classes are imbalanced, pseudo-labels frequently\ndemonstrate an uneven distribution because the model tends to\nhave greater confidence in dominant or less complex classes. To\naddress this, Mottaghi et al. [267] introduce a new strategy for\npseudo-label selection. This involves using a subset of pseudo-\nlabels based on the reciprocal of class frequency, favoring less\ncommon or challenging classes. This technique effectively ad-\ndresses label distribution imbalance, boosting the surgical activity\nrecognition model’s reliability and performance.\n4.3.5. Self-supervision\nCertain\nstudies\naddress\nUDA\nby\nemploying\nthe\nself-\nsupervision strategy. In this approach, alignment is achieved by\nconcurrently conducting auxiliary self-supervised tasks in both\ndomains. Each self-supervised task aims to bring the domains\ncloser by focusing on relevant directions.\nSuccessfully train-\ning these self-supervised tasks alongside the primary task in the\nsource domain has proven effective in generalizing to the un-\nlabeled target domain [268]. Various auxiliary self-supervised\ntasks are available, but not all are suitable for UDA. Conse-\nquently, the primary challenge in the self-supervision method is\nto identify an appropriate self-supervised task that enables the\nmodel to learn valuable representations from the data and pro-\nmote alignment between the domains. Koohbanani et al. [269]\npresent a method called Self-Path for histology image classifi-\ncation. In this approach, they propose three innovative domain-\nspecific self-supervision tasks.\nThese tasks involve predicting\nthe magnification level, solving a magnification jigsaw puzzle,\nand predicting the Hematoxylin channel. These tasks are strate-\ngically designed to utilize the contextual, multi-resolution, and\nsemantic features inherent in histopathology images. The Self-\nrule to multi-adapt (SRMA) technique [270] is applied in the de-\ntection of cancer tissue. This method uses a limited set of la-\nbeled images from the source domain and integrates structural\ndetails from both domains by identifying visual similarities using\nself-supervision within each domain and across domains. Ad-\nditional self-supervised tasks include the jigsaw puzzle auxiliary\ntask, where the spatial correlation in an image is learned by recon-\nstructing a CT scan from shuffled patches [271], and an auxiliary\ntask focused on edge generation [272].\n4.3.6. Hybrid methods\nVarious works employ feature alignment and image translation\nmethods together to enhance the performance of UDA. These are\ncalled hybrid methods. Hybrid methods encompass a two-step\nprocedure: initially, image transformation modifies the source\nimages to align them with the target domain’s appearance, and\nsubsequently, feature adaptation is applied to narrow the remain-\ning disparity between the generated target-like images and the\nreal target images [273]. The benefit of employing hybrid tech-\nniques lies in their ability to retain pixel-level, feature-level, and\nsemantic information. The Cycle-Consistent Adversarial Domain\nAdaptation technique (CyCADA), introduced by Hoffman et al.\n[274], is a hybrid learning method developed for natural images.\nIt consists of two stages: image adaptation and feature adaptation,\nboth of which undergo sequential training without direct interac-\ntions. CyCADA has found extensive application as a fundamental\nmodel in different medical imaging scenarios [275, 276].\nIn contrast to CyCADA, Chen et al. [11, 273] introduce an\nalternative technique known as Synergistic Image and Feature\nAlignment (SIFA), which facilitates concurrent image and fea-\nture translation. In particular, the feature encoder is shared, en-\nabling it to simultaneously alter the image’s appearance and ex-\ntract domain-invariant representations for the segmentation task.\nTo enhance domain adaptation accuracy further, certain research\nstudies incorporate attention mechanisms alongside image and\nfeature alignment techniques [277, 278]. Chen et al. [278] em-\nploy the same framework as SIFA. However, in their approach,\nthe alignment of the feature space is directed by the dual adver-\nsarial attention mechanism. This mechanism concentrates on spe-\ncific regions identified by the spatial and class attention mecha-\nnisms rather than treating all semantic feature components uni-\nformly. Label-efficient UDA (LE-UDA) [279] tackles both do-\nmain shift and source label scarcity.\nThe approach utilizes a\nhybrid method to handle domain shift, while for source label\nscarcity, it incorporates two teacher models. These models lever-\nage information within domains as well as across domains from\ndiverse datasets. In a recent study, Li et al. [280] introduce a\nself-training adversarial learning framework for retinal OCT fluid\nsegmentation tasks that utilize a hybrid approach.\n5. Inaccurate supervision\nInaccurate supervision refers to a scenario where the provided\nsupervision information isn’t always entirely accurate, meaning\nthat errors may be present in some of the label information. Such\nnoisy labels [281] can originate from various sources, including\nhuman errors in the labeling process, inter-observer variability\namong medical experts, or reliance on non-experts or automated\nsystems for data labeling (see Figure 16). Since noisy labels can\nsignificantly harm the generalization capabilities of deep neural\nnetworks, it is imperative to develop robust techniques to handle\nand mitigate the impact of noisy labels. This is particularly vi-\ntal in the field of MIA, where precision and accuracy are critical\n19\nTable 4: Overview of recent methods in Incomplete Supervision category.\nReference\nTask\nAlgorithm Design\nDataset\nResult\n[181]\nGland segmentation\nDeep adversarial network\n2015 MICCAI Gland Challenge\ndataset\nF1: 0.916; ObjectDice: 0.903\n[179]\nPolyp segmentation\nAdversarial learning\nKvasir-SEG; CVC-Clinic DB\nKvasir-SEG: Dice: 15% label: 0.7676, 30% la-\nbel: 0.8095; CVC-Clinic DB: Dice: 15% label:\n0.8218, 30% label: 0.8929\n[184]\nHeart; Prostate; Pancreas segmen-\ntation\nSemi-supervised GAN\nACDC; DECATHLON\nACDC: DSC (Dice coefficient):\n0.834; DE-\nCATHLON: DSC: 0.529\n[183]\nFundus segmentation\nLeaking GAN\nDRIVE, STARE, CHASE DB1\nDRIVE: Acc:\n95.74 Sp:\n86.72 Se:\n97.50;\nSTARE: Acc:\n95.65 Sp:\n91.86 Se:\n91.02;\nCHASE DB1: Acc: 96.83 Sp:92.21 Se:94.72\n[178]\nOptic cup segmentation\nTeacher-student VAE\nDRD\nDSC: 0.80\n[186]\nKidney; Heart; Liver\nGenerative Bayesian Deep Learning\nKiTS; ASG; DECATHLON\nDSC:\nKiTS:\n0.898;\nASG:\n0.884;\nDE-\nCATHLON: 0.935\n[167]\nSkin lesion segmentation\nΠ-model\nISIC 2017\nDSC: 0.874; Acc: 0.943\n[158]\nChest X-ray segmentation\nElastic deformations perturbations for\nCL\nJSRT dataset\nMeanIOU: 5 labeled samples: 85.0 ± 2.8; 10\nlabeled samples: 87.9 ± 0.8\n[169]\nBreast\nUncertainty-aware\nTemporal\nEnsem-\nbling\nPrivate Dataset: 170 Volumes; ISIC\n2017\nPrivate Dataset: DSC: 0.7287; ISIC 2017: DSC:\n0.8178\n[176]\nBrain Tumor and Left Atrial Seg-\nmentation\nDual Uncertainty-Guided Mixing Con-\nsistency\nBraTS2020; LA2018\nBraTS: Dice: 85.94 %; LA2018: Dice: 89.28 %\n[190]\nHeart\nCRF-based Self-training\nPrivate Dataset: 8050\nImages DSC: 0.920\n[193]\nThorax Disease and Skin lesion\nclassification\nAnti-Curriculum Pseudo-labeling\nChest X-Ray14; ISIC 2018\nChest X-Ray14: AUC: 81.77; ISIC 2018: AUC:\n94.36 Sensitivity: 72.14 F1: 62.23\n[197]\nMulti-organ abdominal segmenta-\ntion\nCo-training using different modalities\nBTCV; CHAOS\nBTCV: Mean Dice score: 10 % labels: 81.3;\nCHAOS: Mean Dice score: 10 % labels: 82.1\n[198]\nCardiac segmentation\nCo-training using different network ar-\nchitectures\nACDC dataset\nMean DSC: 0.848 (0.085); Mean HD95: 7.6\n(10.8)\n[200]\nCardiac segmentation\nCo-training using different transforma-\ntions\nMM-WHS dataset\n10% labeled data: Dice: 0.743, mIOU: 0.601,\nPixAcc: 0.973; 20% labeled data: Dice: 0.828,\nmIOU: 0.714, PixAcc: 0.979; 40% labeled data:\nDice: 0.849, mIOU: 0.746, PixAcc: 0.985;\n[202]\nBreast; Retina\nSelf-training\n+\nVirtual\nAdversarial\nTraining\nRetinalOCT;\nPrivate\nDataset:\n39,904 Images\nAcc: 0.9513; Macro-R (Macro-Recall): 0.9330\n[203]\nLung detection\nMixMatch + Focal Loss\nLUNA; NLST\nLUNA: CPM: 0.872\n[204]\nMetastatic\nepidural\nspinal\ncord\nclassification\nConsistency Regularization + Pseudo-\nlabeling + Active Learning\nPrivate Dataset: 7,295 Images;\nAcc:\n0.9582;\nMacro-P (Macro-Precision):\n0.8609\n[205]\nCardiac and Prostate segmentation\nSelf-training + Contrastive loss\nACDC; Prostate; MMWHS dataset\nACDC: DSC: 0.881; Prostate:\nDSC: 0.693;\nMMWHS: DSC: 0.803\n[206]\nCardiac, Tumour and histopathol-\nogy images segmentation\nSelf-training + Contrastive loss\nACDC; KiTS19; CRAG\nACDC: DSC: 0.891; KiTS19:\nDSC: 0.919;\nCRAG: 0.882\n[215]\nColon\nTraditional Data Augmentation Entropy\n+ Diversity\nPrivate Dataset:\n6 colonoscopy\nvideos 38 polyp videos + 121\nCTPA datasets\nClassification: 4 % input: AUC: 0.9204; Detec-\ntion: 2.04 % input: AUC: 0.9615\n[214]\nLung Classification\nLoss Prediction Network\nCC-CCII Dataset\n42 % Chest X-Ray input: Acc: 86.6%\n[220]\nSkin disease classification\nBALD + KL-divergence\nISIC 2016\n22 % image input: AUC: 0.75\n[221]\nChest\nBayesian Neural Network + cGAN Data\nAugmentation\nJSRT Database; ChestX-ray8\nClassification: 35 % input: AUC: 0.953; Seg-\nmentation: 35 % input: DSC: 0.910\n[225]\nGland; Lymph\nCosine Similarity + Bootstrapping +\nFCN\nGlaS 2015; Private Dataset: 80 US\nimages\nMICCAI 2015: 50 % input: F1: 0.921; Private\nDataset: 50 % input: F1: 0.871\n[226]\nShoulder\nBNN + MMD Divergence\nPrivate Dataset:\n36 Volume of\nMRIs\n48 % MRI input: DSC ≈0.85\n[233]\nMajor depressive order identifica-\ntion\nFeature (MMD)\nREST-meta-MDD Consortium\nSite/Hospital - 20 Site/Hospital - 1: ACC (%):\n59.73 ± 1.63; AUC (%): 62.50 ± 2.50; SEN\n(%): 69.46 ± 6.43; SPE (%): 50.00 ± 9.63; PRE\n(%): 58.49 ± 2.58\n[234]\n3D Medical Image Synthesis\nKL divergence\nBraTS 2019 dataset (2 subsets are\nused CBICA and TCIA)\nCBICA TCIA: Dice: 0.773; TCIA CBICA:\nDice: 0.874\n[236]\nSegmentation of retinal fluids in 3D\nOCT images\nContrastive and supervised loss\nTwo large OCT datasets (Spectralis\nand Cirrus)\nSpectralis Cirrus: Dice: 62.33; UVD: 10.88\ncontinued on the next page\n20\nTable 4: Overview of recent methods in Incomplete Supervision category (continued).\nReference\nTask\nAlgorithm Design\nDataset\nResult\n[240]\nHistopathology cancer classifica-\ntion\nAdversarial learning + Entropy loss +\nFocal loss\nPrivate cross-modality dataset\nWSI\nMicroscopy images(MSIs):\nAccuracy:\n90.48; Precision: 90.67; Recall: 90.35; F1-\nmeasure: 90.50\n[241]\nAutomated pneumonia diagnosis\nConditional domain adversarial network\n+ Contrastive loss\nRSNA dataset (Stage I); Child X-\nray dataset\nRSNA dataset Child X-ray: AUC score: 90.57;\nRSNA + COVID\nTTSH dataset:\nweighted\nAUC: 88.27\n[245]\nBronchoscopic Depth Estimation\nAdversarial learning\nSynthetic dataset and human pul-\nmonary dataset\nMean abs. rel. diff: 0.379; RMSE: 7.532; Ac-\ncuracy: 0.856\n[250]\nLung Cancer Segmentation\nCycleGan + Tumor-aware loss\nThe\nCancer\nImaging\nArchive\n(TCIA) CT dataset and Private\nMRI dataset\nCT MRI: Validation set (Unsupervised): DSC:\n0.62 ± 0.26 HD95: 7.47 ± 4.66; Test set (Unsu-\npervised): DSC: 0.74 ± 0.15 HD95: 8.88 ± 4.8\n[252]\nBrain tumor and cardiac segmenta-\ntion\nDual CycleGan + Self-attentive spatial\nadaptive normalization\nMM-WHS challenge; BraTS\nMM-WHS: MRI CT: Mean Dice: 0.78 ± 0.10,\nMean ASSD: 4.9 ± 1.5 CT MR: Mean Dice:\n0.70 ± 0.11, Mean ASSD: 9.5 ± 3.2; BraTS:\nMRI-T2 MRI-T1: 0.50 ± 0.06\n[258]\nLiver segmentation\nDisentangled Representation\nLiTS challenge 2017 dataset (CT\nslices)\nand\nmulti-phasic\n(MRI\nslices)\nCT MR: Dice: 0.81\n[259]\nCardiac (LV and MYO) segmenta-\ntion\nDisentangled Representation + Semantic\nconsistency loss\nMS-CMRSeg;\nMM-WHS\nchal-\nlenge\nMS-CMRSeg: bSSFP CMR\nLGE CMR im-\nages: DSC: 79.08, ASSD: 1.68; MM-WHS: CT\nMRI: DSC: 84.51, ASSD: 1.00, MRI CT: DSC:\n84.77 ASSD: 0.98\n[260]\nCardiac segmentation\nDisentangled Representation + HSIC +\nAttention bias\nMMWHS challenge 2017 dataset\nMRI CT: Dice: 80.2, ASD: 5.1; CT MRI: Dice:\n66.3, ASD: 4.9\n[264]\nWhite Matter Hyperintensity Seg-\nmentation\nSelf training + Uncertainty-guided loss\nWMH; ADNI-2\nWMH ADNI-2: DSC: 0.69 ± 0.18, H95: 11.2\n± 14.5\n[265]\nEpithelial-stroma (ES) classi- fica-\ntion\nCurriculum learning\nNetherland Cancer Institutes (NKI)\ndataset, Vancouver General Hospi-\ntals (VGH) and IHC dataset\nAccuracy: VGH NKI: 91.50, IHC NKI: 82.51,\nNKI\nVGH: 92.62, IHC\nVGH: 80.49, VGH\nIHC: 88.15, NKI IHC: 81.90\n[267]\nPseudo labeling\nSurgical\nactivity\nrecognition\nmodels\nacross operating rooms\nDataset\nof\nfull-length\nsurgery\nvideos from two robotic ORs (OR1\nand OR2)\nOR1 OR2: Accuracy: 70.76 mAP: 83.71, OR2\nOR1: Accuracy: 73.53, mAP: 89.96\n[269]\nClassification of Pathology Images\nPrediction of magnification level and\nHematoxylin channel + Solving jigsaw\npuzzle\nWSIs: Camelyon16 and In house\ndataset(LNM-OSCC)\nCamelyon16: AUC-ROC: 93.7 % LNM-OSCC:\n97.4 %\n[270]\nColorectal tissue type classifica-\ntion; Multi-source patch classifica-\ntion\nIntra-domain\nand\ncross-domain\nself-\nsupervision\nKather-16 [242], Kather-19 [243],\nColorectal cancer tissue phenotyp-\ning dataset (CRC-TP)[244] and In-\nhouse dataset\nKather-19\nKather-16:\noverall weighted F1\nscore: 87.7; Kather-19 + Kather-16 CRC-TP:\nweighted F1 score: 83.6\n[272]\nCardiac segmentation\nEdge generation task + Adversarial\nlearning\nMM-WHS challenge dataset (2017)\nMRI →CT: Dice: 76.98, ASD: 4.6\n[275]\nCone-beam computed tomography\n(CBCT) segmentation\nImage and feature alignment\nPrivate Dataset: 90 patients\nCT →CBCT: DSC: 83.6 %\n[11]\nCardiac segmentation\nSynergistic image and feature alignment\n(SIFA)\nMM-WHS challenge dataset\nMRI →CT: Dice: 73.0, ASD: 8.1\n[278]\nSkull segmentation and Cardiac\nsegmentation\nCycleGan + Feature space alignment\nis led by the dual adversarial attention\nmechanism\nCQ500; ADNI; MM-WHS chal-\nlenge\nSkull: CT →MRI: DSC: 84.07 %, ASSD: 1.18;\nCardiac: MRI →CT: DSC: 76.7 %, ASSD: 5.1\n[279]\nMulti-organ and Cardiac segmenta-\ntion\nCycleGan and feature alignment\nMICCAI 2015 Multi-Atlas Ab-\ndomen Labeling(CT images), ISBI\n2019 CHAOS Challenge (MR im-\nages); MM-WHS\nCardiac: MRI →CT: Dice: 70.8, ASD: 9.6;\nCT →MRI: Dice: 66.5, ASD: 4.0; Multi-organ:\nMRI →CT: Dice: 82.8, ASD: 2.3; CT →MRI:\nDice: 87.7, ASD: 1.0\n21\nFigure 16: The major sources of label noise encompass variations among different\nobservers, mistakes made by human annotators, and inaccuracies in computer-\ngenerated labels. The impact of label noise in medical datasets is expected to\ngrow as larger datasets are curated for deep learning purposes (image courtesy of\nKarimi [281]).\nfor medical diagnosis and treatment. A summary of recent ap-\nproaches for learning with inaccurate supervision is provided in\nTable 5. We categorize inaccurate supervision approaches into\nthree broad groups: Robust loss Design, data re-weighting and\ntraining procedures.\n5.1. Robust loss design\nCertain investigations modify the loss function as a strategy\nto mitigate the impact of noisy labels.\nResearchers have in-\ntroduced novel loss functions like Mean Absolute Error (MAE)\n[282] and Generalized Cross Entropy [283] to tackle the issue of\nlabel noise in natural images. In addition, some works choose\nto adapt loss functions specifically for tasks in medical imaging\n[284, 285, 286]. For COVID-19 Pneumonia Lesion segmenta-\ntion, Wang et al.\n[285] proposed an enhanced Dice loss that\naddresses noise-related challenges. This improved loss function\nis an extended version of the traditional Dice loss, tailored for\nsegmentation tasks, and incorporates the Mean Absolute Error\n(MAE) loss to enhance its robustness against noisy data. Chen\net al. [286] introduce a new and versatile loss function called\nAdaptive Cross Entropy (ACE), designed to handle noise in la-\nbels without requiring hyperparameter fine-tuning during train-\ning. They provide both theoretical and practical evaluations of\nthe ACE loss and demonstrate its efficacy across various publicly\navailable datasets. Previous segmentation methods that handle\nnoisy labels have typically focused on preserving semantics in a\npixel-wise manner, which involves actions like pixel-wise label\ncorrection. However, they often overlook the potential benefits of\nconsidering pairwise relationships between pixels [287]. Notably,\nit has been observed that capturing these pairwise affinities can\nsignificantly decrease label noise. Building on this insight, Guo\net al. [287] introduce a joint class-affinity segmentation model\nthat takes into account both pixel-wise label correction and pair-\nwise pixel relationships in order to reduce label noise. To further\nreduce the impact of label noise, they introduce a strategy called\nclass-affinity loss correction (CALC), which includes class-level\nand affinity-level loss correction.\n5.2. Data re-weighting\nBroadly speaking, these methods aim at down-weighting those\ntraining samples that are more likely to have incorrect labels. In\nthis context, Xue et al. [4] introduced an approach for classify-\ning skin lesions with noisy labels. Their method involved a data\nre-weighting technique, effectively excluding data samples with\nsignificant loss values in each training batch. To predict pancre-\natic cancer regions in whole-slide images (WSIs), Le et al. [288]\nutilized a noisy label classification technique. This approach in-\ncorporates a limited set of clean training samples and dynami-\ncally assigns weights to training samples to address sample noise.\nThese weights are assigned in real time to align the network loss\nwith that of the clean samples. For multi-organ segmentation,\nZhu et al. [8] introduced an approach known as pick and learn. In\nthis method, a deep learning model is trained to identify incorrect\nlabels and assign weights to each sample within a training batch.\nThe goal is to reduce the impact of samples with inaccurate la-\nbels. Simultaneously, the primary segmentation model is trained\nin parallel, incorporating these weights into its loss function.\nStrategies involving resampling and reweighting at the pixel\nlevel are intended to focus the segmentation model on learn-\ning from reliable pixels. For example, Mirikharaji et al. [289]\nintroduced a method for skin lesion segmentation that incorpo-\nrates pixel-wise weighting. This approach learns weight maps\nthat adapt spatially and adjusts the influence of each pixel us-\ning a meta-reweighting framework. The Tri-network approach by\nZhang et al. [290] employs three cooperating networks and dy-\nnamically identifies informative samples based on the consensus\namong predictions generated by these distinct networks. Mean-\nwhile, Wang et al. [291] employ meta-learning techniques to au-\ntomatically estimate an importance map, allowing them to extract\nreliable information from crucial pixels.\n5.3. Training procedures\nThe methods in this category are very diverse. Several ap-\nproaches in this category rely on Multi-network Learning, which\nfrequently employs techniques like collaborative learning and co-\ntraining to train multiple networks simultaneously. Meanwhile,\nothers follow the Multi-round Learning paradigm, which itera-\ntively enhances the chosen set of clean examples without the need\nfor maintaining additional DNNs. This improvement is achieved\nby repeating the training process in multiple rounds. Min et al.\n[292] adapted concepts from Malach and Shalev-Shwartz [293]\nto create label-noise-resistant techniques for medical image seg-\nmentation. They simultaneously trained two distinct models and\nexclusively updated these models using data samples where the\npredictions of the two models disagreed. Rather than solely re-\nlying on final layer predictions, Min et al. [292] incorporated\nattention modules at different network depths, allowing them to\nutilize gradient information from various feature maps to iden-\ntify and reduce the influence of samples with incorrect labels.\n22\nThey demonstrated encouraging outcomes in MRI-based cardiac\nand glioma segmentation tasks. For medical image classification,\nXue et al. [294] utilize a self-ensemble model along with a noisy\nlabel filter to effectively identify clean and noisy samples. Sub-\nsequently, they employ a collaborative training approach to train\nthe clean samples, aiming to mitigate the impact of imperfect la-\nbels. Additionally, they introduce an innovative global and local\nrepresentation learning scheme, which serves as an implicit regu-\nlarization method for enabling the networks to make use of noisy\nsamples in a self-supervised fashion. For COVID-19 pneumonia\nlesion segmentation, Yang et al. [295] propose a dual-branch net-\nwork that learns from both accurate and noisy annotations sep-\narately. They introduce the Divergence-Aware Selective Train-\ning (DAST) strategy to distinguish between severely noisy and\nslightly noisy annotations. For severely noisy samples, they ap-\nply regularization through dual-branch consistency between pre-\ndictions from the two branches. Additionally, they refine slightly\nnoisy samples and incorporate them as supplementary data for the\nclean branch to prevent overfitting. Li et al. [296] focus on se-\nlecting training pixels with reliable annotations from pixels with\nuncertain network predictions. They introduce the online proto-\ntypical soft label correction (PSLC) method to estimate pseudo-\nlabels for label-unreliable pixels. They then calibrate the total\nsegmentation loss using the segmentation loss of label-reliable\nand label-unreliable pixels. For hepatic vessel segmentation, Xu\net al. [297, 298] utilized a small set of accurately labeled data\nalongside a larger set of noisily labeled data. They employed con-\nfident learning with the help of a weight-averaged teacher model.\nThis strategy involved progressively refining the noisy labels in\nthe low-quality dataset through pixel-wise soft correction. Shi et\nal. [299] present a framework designed to address noisy labels by\nextracting valuable supervision information from both pixel-level\nand image-level sources. Specifically, they make explicit estima-\ntions of pixel-wise uncertainty, treating it as a measure of noise\nat the pixel level. They then propose a robust learning approach\nat the pixel level, utilizing both the original labels and pseudo-\nlabels. Additionally, they present a complementary image-level\nrobust learning method to incorporate more information along-\nside pixel-level learning.\n6. Only limited supervision\nOnly Limited Supervision refers to a set of methods in which\nthe available supervision or labeling for training data is con-\nstrained or limited in nature. Furthermore, unlabeled data is also\nunavailable. These methods are typically applied in scenarios\nwhere acquiring extensive or detailed annotations is challenging\nor resource-intensive. Instead, they employ alternative strategies\nsuch as few-shot learning, transfer learning, and data augmenta-\ntion to maximize the utility of the limited available supervision\n(Figure 17). These approaches enhance model performance and\nfacilitate tasks like segmentation, classification, or detection with\nminimal labeled data. A summary of recent approaches for learn-\ning with only limited supervision is provided in Table 6.\n6.1. Data Augmentation\nData augmentation offers a means to significantly increase the\nquantity and diversity of training data, all while avoiding the\nneed for additional sample collection. These augmentation tech-\nniques encompass both straightforward yet remarkably impact-\nful transformations like cropping, padding, and flipping, as well\nFigure 17: Taxonomy of Only Limited Supervision methods.\nas more intricate generative models [300]. The efficacy of data\naugmentation strategies varies based on factors like input nature\nand visual tasks. Therefore, the field of medical imaging might\nnecessitate distinct augmentation approaches that yield plausible\ndata instances and effectively enhance the regularization of deep\nneural networks. Moreover, data augmentation can also address\nthe issue of underrepresented classes by generating additional in-\nstances, such as generating synthetic lesions. Following earlier\nsurveys [300, 301], we have categorized data augmentation meth-\nods into three broad groups: transformation of original data, gen-\neration of artificial data and other categories.\n6.1.1. Transformation of original data\nThe first data augmentation category involves applying vari-\nous image manipulation techniques to existing samples. This can\nbe divided into three subcategories: (1) Affine Transformations\nare geometric changes that retain lines and parallelism, though\nnot necessarily distances and angles. This is ensured by transfor-\nmation constraints, typically preserving the image’s aspect ratio\nalong axes of symmetry. The transformations include translation,\nrotation, flipping, scaling, cropping, and shearing [302, 303]. (2)\nElastic transformations involve applying a spatial deformation\nfield to an image. Unlike affine transformations, they don’t en-\nforce the preservation of collinearity or aspect ratio. As a result,\nelastic transformations can introduce shape variations and can be\nemployed to enhance the robustness of segmentation algorithms\n[301]. and (3) Pixel-Level Transformations alter pixel values to\nmodify characteristics like saturation, contrast, noise, and bright-\nness [304]. Given that medical imaging is often grayscale, color-\nbased changes are rare. Pixel-level transformations aid deep neu-\nral networks’ robustness across different scanners and protocols\nthat might affect pixel distribution.\nThe majority of transformations within this category are quite\nstraightforward to apply and are either readily available in deep\nlearning frameworks or can be easily incorporated using versatile\nlibraries [305]. Recently, there has been a surge in frameworks\nand libraries tailored for the medical field, like the Medical Open\nNetwork for AI (MONAI) [306]. However, it’s important to note\nthat since these techniques rely on altering the original samples,\nthey can’t enhance the network’s ability to generalize beyond its\ninitial training data. They often generate samples that are highly\ncorrelated.\n6.1.2. Generation of artificial data\nCreating artificial or synthesized samples can provide a broader\nrange of diverse and complex examples, effectively addressing\n23\nTable 5: Overview of recent methods in Inaccurate Supervision category.\nReference\nTask\nAlgorithm Design\nDataset\nResult\n[8]\nMulti-organ segmentation\nLoss Re-weighting\nJSRT\n25 % noise: Dice: 0.895; 50 % noise: Dice: 0.898;\n75 % noise: Dice: 0.895\n[289]\nSkin Lesions segmentation\nExample reweighting\nISIC 2017\nUnsupervised noise: Dice: 73.55 %\n[4]\nSkin lesion classification\nSample reweighting + Online Uncer-\ntainty Sample Mining\nISIC 2017\n5% noise: Acc: 84.5; 10%: Acc: 83.6; 20%: ;Acc:\n80.7; 40%: Acc: 80.7;\n[290]\nClinical stroke lesion and multi or-\ngan segmentation\nTri-teaching network\nPrivate clinical stroke dataset; JSRT\nClinical stroke: Dice: 68.12 %; JSRT: Dice: 80.43\n[292]\nCardiac and Brain tumour segmen-\ntation\nTwo-Stream Mutual Attention Network\nHVSMR 2016; BRATS 2015\nHVSMR 2016: Myocardium: Dice: 0.820 ADB:\n0.824 HDD: 4.73, Blood Pool: Dice: 0.926 ADB:\n0.957 HDD: 8.81; BRATS 2015:\nMean Dice:\n0.792\n[297]\nHepatic Vessel Segmentation\nMean-Teacher-assisted Confident Learn-\ning\n3DIRCADb;\nMSD8\n(Used\nfor\ntraining)\n3DIRCADb:\nDice:\n0.7245 PRE: 0.7570 ASD:\n1.1718 HD: 7.2111\n[299]\nLeft Atrial(LA) and cervical cancer\nSegmentation\nPixel-wise and Image-level Noise Toler-\nant learning\nLeft Atrial(LA); Private dataset\nLA: 25 % Noise : Dice(%): ASD: 1.60 50 % Noise:\nDice(%): 89.04 ASD: 1.92 75 % Noise: Dice(%):\n76.25 ASD: 4.56; Private dataset: Dice(%): 75.31\nASD: 1.76\n[287]\nSurgical instrument segmentation\nPixel-wise label correction and pairwise\npixel relationships in order to reduce la-\nbel noise.\nEndovis18\nAverage: Ellipse noise: Dice (%): 71.384 Jac (%):\n58.452; Symmetric noise: Dice (%): 74.058 Jac\n(%): 62.667; Asymmetric noise: Dice (%): 74.410\nJac (%): 63.029\nthe limitations of methods based on transformations. The preva-\nlent method for medical image synthesis is through generative\nnetworks, particularly generative adversarial networks (GANs)\n[307]. However, generating synthetic images can also involve\ntechniques like combining features or employing specialized\nmodeling approaches designed for specific medical imaging tasks\nor modalities. While these approaches offer increased diversity,\nthey often require higher computational resources and introduce\ncomplexity.\nAdditionally, artificially generated samples might\nnot fully capture the visual attributes or distribution of genuine\ndata instances.\nDepending on the specific domain, dataset, and task at hand,\ncertain families of GANs may be more suitable, while others may\nbe entirely impractical. Translation-based GANs, which encom-\npass models like CGAN [308], pix2pix [309], CycleGan [248],\nand SPADE [310], specialize in learning how to transform various\ntypes of images. For instance, they can convert a segmentation\nmask into a newly synthesized input or transform a non-contrast\nCT-scan into a contrast CT-scan. In contrast, noise-based gener-\nation models like DC-GAN [311], StyleGAN2 [312], and PGAN\n[313] offer greater flexibility. However, noise-based generation\ntechniques may encounter challenges when dealing with small\ntraining datasets, necessitating mitigation strategies such as patch\nextractions and traditional data augmentation. Although transla-\ntion based models are known for producing images of exception-\nally high quality, they are restricted in terms of the quantity of\nimages they can create because they rely on the use of segmen-\ntation masks or different image modalities as input requirements.\nNoise-based approaches, on the other hand, do not face such lim-\nitations but often yield images with lower visual quality and run\nthe risk of reproducing artifacts (such as vignettes or rulers) that\ncould reinforce biases present in the dataset [314]. These frame-\nworks and other extended versions of GANs have been widely\nemployed for augmenting various types of organ images, includ-\ning liver [315, 316], skin [317], chest [318], eye [319], lung [320],\nbreast [321, 322], brain [323, 324], and more. Readers interested\nin a comprehensive review of GANs for medical image augmen-\ntation can refer to the work of Chen et al. [325].\nOther than GANs, Copy-paste methods have been utilized to\ngenerate artificial data. Copy-paste is a simple yet effective data\naugmentation technique and, it has demonstrated the potential to\namplify the generalization power of deep neural networks. In\nessence, copy-paste involves copying portions of one image and\npasting them onto another. Notably, the mix-up technique by\n[326], and CutMix [327] are well-known approaches for mix-\ning entire images and mixing image crops, respectively. Sev-\neral studies have extended these methods to address specific ob-\njectives in MIA. For example, TumorCP [328] employs lesion\nmasks to extract lesions from scans and paste them onto an-\nother scan at appropriate locations, guided by the lesion masks\nin the target scan. In the context of nuclei segmentation, Ins-\nMix [329] follows a Copy-Smooth-Paste principle and conducts\nmorphology-constrained generative instance augmentation. Self-\nMix [330] leverages both tumor and non-tumor information for\nlesion segmentation. Given a pair of annotated training images,\nCarveMix [331] combines a region of interest (ROI) based on\nlesion location and geometry, replacing the corresponding vox-\nels in other labeled images. TensorMixup [332] is a method that\nmerges two image patches using a tensor and has been utilized\nto enhance the precision of tumor segmentation. Certain alterna-\ntive approaches concentrate on medical datasets that frequently\nexhibit skewness towards negative cases. These methods encom-\npass the creation and incorporation of fabricated lesions into in-\ndividuals who are otherwise healthy. For example, these methods\nhave been applied to simulate lesions resembling multiple scle-\nrosis in brain MR images [333] or to introduce cancer indicators\ninto breast mammography images [334].\n6.1.3. Others\nApart from the aforementioned categories, there exist augmen-\ntation techniques designed for specific purposes. Notably, modal-\nities such as CT and MRI possess a volumetric nature. Lever-\naging 3D convolutions presents the advantage of incorporating\ninformation from neighboring slices, leading to enhanced perfor-\nmance given a large dataset. While several image transforma-\ntions, particularly affine transformations, are well-established for\n2D images, extending them to 3D settings may pose challenges\nin terms of computational efficiency.\nExamples of augmenta-\n24\ntions designed with 3D data augmentation techniques include 3D\nGANs [335], 3D affine transformations [336], and multiplanar\nimage synthesis [337].\nLearnable data augmentation, also known as neural data aug-\nmentation, is an advanced technique in deep learning where the\naugmentation parameters are learned by the neural network dur-\ning the training process. Unlike traditional data augmentation\nmethods that apply fixed transformations to input data, learnable\ndata augmentation allows the model to adaptively determine the\naugmentation parameters based on the data and the task at hand.\nMethods following this strategy involve training two networks si-\nmultaneously: one network learns to solve a specific task, while\nthe second network learns how to augment the data for the first\none. One of the most common approaches for learnable data aug-\nmentation is autoaugment [338]. This method aims to optimize\nnetwork performance by identifying the most effective combina-\ntion of established transformations (like affine transformations,\npixel-level modifications, etc.). The ideal augmentation policy is\ndetermined through a neural network, which can be trained us-\ning adversarial training [339], evolutionary algorithms [340], or\nreinforcement learning [338].\n6.2. Few shot learning\nFew-shot learning (FSL) takes inspiration from human-like ro-\nbust reasoning and analytical abilities. Wang et al. [341] provided\na standard definition for machine learning based on experience\n(E), task (T), and performance (P): A computer program is con-\nsidered to learn from experience E with respect to certain classes\nof task T and performance measure P if its performance can en-\nhance with E on T as measured by P. It’s important to note that\nE, the experience in FSL, is quite limited. Formally, within each\nfew-shot task, we are given three sets: a support set denoted as S,\na query set referred to as Q, and an auxiliary set labeled as A. The\nsupport set S encompasses C distinct categories, with each cate-\ngory comprising K training samples, essentially forming a C-way\nK-shot configuration. The query set Q comprises unlabeled query\ndata. We categorize the current deep FSL methods into enlarging\nthe training data, metric-learning based methods, meta-learning\nbased methods and others.\nEnlarging the training data: These approaches augment the\ntraining data to enlarge the number of samples, enabling the uti-\nlization of standard deep learning models and algorithms on the\naugmented dataset to attain a more accurate model. For multi-\nmodal medical image segmentation, Mondal et al. [342] expand\nthe training set through the use of GANs. On the other hand,\nZhao et al. [343] present a learning-based technique for data aug-\nmentation. Specifically, their approach starts with a single labeled\nimage and a set of unlabeled examples. By employing learning-\nbased registration methods, they model the spatial and appear-\nance transformations between the labeled and unlabeled exam-\nples. These transformations encompass effects such as non-linear\ndeformations and variations in imaging intensity. Subsequently,\nthey generate new labeled examples by sampling these transfor-\nmations and applying them to the labeled example, resulting in a\ndiverse range of realistic images. These synthesized examples are\nthen used to train a supervised segmentation model. For the seg-\nmentation of new WM tracts in a few-shot scenario, [344, 345]\nhave proposed efficient data augmentation techniques. Specifi-\ncally, Lu et al. [344] introduce an efficient data augmentation\ntechnique that creates synthetic annotated images through tract-\naware image mixing. Additionally, they employ a transfer learn-\ning method for few-shot segmentation. [346]\nMetric-learning based approaches: These approaches offer\na straightforward and adaptable framework where they directly\nassess the similarities or dissimilarities between query images in\nthe query set and labeled images in the support set. A classi-\ncal metric-learning technique, known as the Prototypical Network\n(ProtoNet) by Snell et al. [347], illustrates this concept. ProtoNet\ncomputes prototype representations for each base class by averag-\ning the feature vectors and subsequently measures the distances\nbetween these prototype representations and each query image.\nImportantly, metric-learning based methods do not involve data-\nindependent parameters in their classifiers, which means fine-\ntuning is unnecessary during the testing phase. Moreover, some\nresearchers, such as Ali et al. [348], have introduced an innova-\ntive additive angular margin metric to enhance the original Pro-\ntoNet’s ability to classify challenging samples, especially in sce-\nnarios involving multi-center, underrepresented, and difficult-to-\nclassify endoscopy data. To enhance prototype-based few-shot\nsegmentation model for abdominal organs, Wang et al. [349] in-\ntroduce a regularization technique. This enhancement involves\ntwo key elements: self-reference and contrastive learning. Self-\nreference regularization ensures that a class prototype accurately\nrepresents the entire organ within a support image. Contrastive\nlearning aids in the understanding of similarity between fore-\nground and background features.\nIn contrast to natural images, there is a lack of extensive pub-\nlicly available datasets for pre-training medical image segmen-\ntation models. Consequently, some self-supervised learning ap-\nproaches have emerged in the domain of medical image few-shot\nsegmentation, relying on unlabeled data. To this end, Ouyang et\nal. [350, 351] presented SSL-ALPNet, a self-supervised learn-\ning method based on superpixels.\nIn this approach, for ev-\nery unlabeled image, pseudolabels are created at the superpixel\nlevel. During each training iteration, a randomly chosen pseu-\ndolabel, along with the original image, is used as both the sup-\nport and query. Random transformations are introduced between\nthe support and query images.\nThe primary objective of this\nself-supervision task is to segment the pseudolabel on the query\nimage, using the support image as a reference, despite the ap-\nplied transformations. Additionally, they incorporated an adap-\ntive local prototype pooling module into prototypical networks to\naddress the prevalent issue of foreground-background class im-\nbalance in medical image segmentation, as shown in Figure 18.\nHansen et al. [352] expanded on this concept by extending the\nself-supervision task to supervoxels, effectively incorporating 3D\ninformation from image volumes. They introduced ADNet, a pro-\ntotypical segmentation network inspired by anomaly detection,\nwhich avoids modeling the large and diverse background class\nwith prototypes. Building on this work, their recent contribution,\nADNet++ [353], presents a one-step multi-class medical image\nsegmentation framework. The model notably enhances the cur-\nrent 3D FSS model for MRI and CT-based abdominal organ and\ncardiac segmentation.\nIn medical images, a significant imbalance exists between the\nforeground and background. Medical images typically feature\na diverse background comprising numerous tissues and organs,\nwhereas the foreground is typically uniform and occupies a rel-\natively small area.\nApplying the same global operation, like\nmasked average pooling, directly to both foreground and back-\nground, as is commonly done in the processing of natural images,\n25\nFigure 18: Illustration of the SSL-ALPNet framework [350]: (a) The self-supervision task involves segmenting the pseudolabel on the query image with reference\nto the support image, despite the applied transformations (shown in blue boxes). (b) The ALPNet method addresses the challenge of class imbalance by adaptively\nextracting multiple local representations of the large background class (in blue), each of which represents a distinct local background region (image from [350]).\ncan result in the loss of local information. To address this issue,\nrecent studies in the field of prototypical FSS have introduced\nmore adaptive prototype extraction modules to mitigate the im-\npact of complex backgrounds. Specifically, these studies have\nincorporated additional priors, such as spatial location [354], and\nneighborhood correlations [355], into the prototypes to preserve\nspatial and shape information.\nMeta-learning based approaches: Meta-learning methods\ntypically employ a meta-training approach, where they train a\nmodel on a sequence of few-shot tasks derived from the base\nclasses during the training phase. The objective is to equip the\npre-trained model with the capability to quickly adapt to entirely\nnew tasks during the testing phase. One well-known representa-\ntive of meta-learning methods is Model-Agnostic Meta-Learning\n(MAML) [356]. MAML achieves this adaptation by pre-training\nthe initial model parameters using second-order gradients, en-\nabling the model to rapidly adapt to new tasks with only a lim-\nited number of gradient steps. Several studies in medical imaging\nemploy MAML and its extensions for various few-shot learning\ntasks. These tasks include rare disease classification [357], clas-\nsifying whole-genome doubling (WGD) across 17 cancer types\nusing digitized histopathology slide images [358], brain tumor\nsegmentation [359], and more.\nFurther, To address issues re-\nlated to vanishing high-order meta-gradients in MAML, Khadka\net al. [360] utilize the Implicit Model Agnostic Meta-Learning\n(iMAML) optimization strategy [361] for few-shot lesion seg-\nmentation. In this approach, inner optimization focuses on com-\nputing weights using a CNN model, while an analytic solution is\nused to estimate the outer meta-gradients.\nZhao et al.\n[362] suggest that meta-learning can enhance\nthe generator’s ability to learn to hallucinate meaningful im-\nages, leading to improved segmentation models in few-shot un-\nsupervised domain adaptation. In this regard, they introduce a\nmeta-hallucinator to generate valuable samples, enhancing model\nadaptability on the target domain with limited source annotations.\nFor diagnosis of glioblastoma multiforme progression, Song et al.\n[363] propose an interpretable structure-constrained graph neu-\nral network (ISGNN). The ISGNN used a meta-learning strategy\nfor aggregating class-specific graph nodes to enhance classifica-\ntion performance on small-scale datasets while maintaining inter-\npretability. Recently, Gao et al. [364] introduced a discriminative\nensemble meta-learning approach for the diagnosis of rare fundus\ndiseases. They introduced a co-regulation loss during the pre-\ntraining of the meta-learning backbone. Subsequently, ensemble-\nlearning techniques were employed to improve performance, tak-\ning advantage of the hierarchical features within the backbone\nnetwork. They explored three ensemble strategies: uniform av-\neraging, majority voting, and stacking, to identify low-shot rare\nfundus diseases.\nOthers: One approach to accomplishing few-shot learning in-\nvolves utilizing the encoder-decoder framework to explore the\nconnection between a query and a support set. In this regard,\nRoy et al. [365] introduce the Squeeze and Excitep framework,\nwhich was the first implementation of a two-branch architecture\nfor medical image few-shot segmentation. One branch, referred\nto as the conditioner arm, focuses on extracting foreground infor-\nmation from the support set. The other branch, known as the seg-\nmenter arm, engages with the conditioner arm through the spatial\nSE module to rectify the query feature. MprNet [366], as an en-\nhancement of SENet, introduces a fusion module based on cosine\nsimilarity to facilitate information exchange between these two\nbranches. Similarly, Kim et al. [367] introduce a U-Net like net-\nwork tailored for segmentation tasks. This network is designed\nto predict segmentation by capturing the relationship between 2D\nslices from the support data and a query image. It incorporates\na bidirectional gated recurrent unit (GRU) to learn the coherence\nof encoded features among adjacent slices. Recently, Feng et al.\n[368] employ a hybrid method in which they introduce a seg-\nmenter built upon the encoder-decoder architecture. They incor-\nporate spatial and prototypical priors as extra sources of super-\nvisory information. Experimental results in multi-modalities and\nmulti-organs segmentation showcase that the method they pro-\npose significantly surpasses previous state-of-the-art techniques.\nFSL is typically trained using the episode training method. Zhu\net al. [369] introduced a Query-Relative (QR) loss, which is more\neffective when combined with the episode training approach than\nthe Cross-Entropy loss for FSL.\n6.3. Transfer learning\nTo address the challenge of limited training data and enhance\nmodel performance, another common approach known as trans-\nfer learning is frequently employed. In this scenario, the goal is\nto harness knowledge acquired from similar learning tasks. The\nsupervised transfer learning technique [370] has proven valuable\nin addressing various issues in medical image analysis. These\napproaches typically involve initial pre-training of standard ar-\nchitectures like ResNet [371] or VGG [372] on a source domain\ncontaining abundant data, such as natural images from sources\nlike ImageNet [373] or medical images. Subsequently, these pre-\ntrained models are transferred to the target domain and fine-tuned\nusing a significantly smaller set of training examples. Tajbakhsh\net al. [370] demonstrated that pre-trained CNNs, when appropri-\nately fine-tuned, achieved performance levels at least comparable\nto CNNs trained entirely from the beginning. This has established\n26\ntransfer learning as a fundamental technique for image classifica-\ntion tasks across diverse modalities, spanning CT [374], mam-\nmography [375] MRI [376], X-ray [377], and more.\nAside from classification, the use of transfer learning in ad-\ndressing different medical image challenges, such as image seg-\nmentation and localization, has been limited [22, 378, 211]. This\ntrend can be attributed, in part, to the inherent 3D characteristics\nof medical images, which present challenges when adapting 2D\nmodels trained on natural images. Additionally, it is influenced\nby the effective performance of shallower segmentation networks\nin medical imaging, which may not gain significant advantages\nfrom fine-tuning in contrast to deep models. However, certain\nstudies have attempted transfer learning for image segmentation.\nFor instance, Ma et al. [379] performed fine-tuning on an au-\ntoencoder that was originally pre-trained for image segmentation\ntasks in natural images. Similarly, other researchers like Qin et al.\n[380] utilized an encoder pre-trained for the task of image classifi-\ncation in natural images and added a randomly initialized decoder\nto it to address the task of prostate MRI segmentation. Liu et al.\n[381] perform a two-stage transfer learning framework for seg-\nmenting COVID-19 lung infections from CT images. Nguyen et\nal. [382] perform task agnostic transfer learning for skin attribute\ndetection.\nSome studies have attempted to transfer knowledge from 2D\nmodels pre-trained on natural images to models intended for 3D\nmedical applications. For example, Yu et al. [383] adapted mod-\nels trained on natural scene videos, treating the third dimension\nof medical scans as a temporal axis. However, this approach may\nnot effectively capture the 3D context of medical scans. In con-\ntrast, Liu et al. [384] proposed a method to transform a 2D model\ninto a 3D network by expanding 2D convolution filters into 3D\nseparable anisotropic filters. Recently, Messaoudi et al. [385] in-\ntroduced two transfer learning strategies. Firstly, they introduced\nweight transfer learning, an effective method for leveraging the\nweights of a pre-trained 2D classifier network by incorporating it\ninto a network of the same or higher dimension. The second ap-\nproach they proposed is dimensional transfer learning, which re-\nlies on extrapolating 3D weights from a pre-trained 2D network.\nEmpirical evidence demonstrates that their methods outperform\ncurrent state-of-the-art techniques.\n7. Future research scope\n7.1. Continual/lifelong learning\nIn healthcare, most intelligent diagnosis systems are limited\nin their scope, often capable of diagnosing only a few diseases.\nExpanding their capabilities after deployment is challenging, pre-\nventing them from achieving the breadth of diagnoses that med-\nical specialists can. Collecting data for all diseases poses signif-\nicant challenges due to privacy concerns and data sharing con-\nstraints. Consequently, training a single system to diagnose all\ndiseases simultaneously is impractical. One potential solution is\nto make the system with the ability for continual learning. This\nwould allow the system to progressively acquire the capacity to\ndiagnose more diseases over time without needing extensive new\ndata for previously learned diseases.\nContinual learning, also\nknown as lifelong learning or incremental learning, is a learning\nparadigm in which a model learns and adapts to new informa-\ntion and tasks over time without forgetting previously acquired\nknowledge [387]. Unlike traditional deep learning approaches\nthat assume a fixed dataset and task, continual learning addresses\nscenarios where data arrives continuously, and the nature of tasks\ncan evolve over time, including the possibility of introducing new\nclasses.\nDespite its potential, there has been a limited exploration of\ncontinual learning in medical contexts. Current research has pri-\nmarily focused on this paradigm in specific areas such as im-\nage segmentation [388, 389], disease classification [390, 391,\n392, 393], and domain adaptation [394, 395]. Moreover, there\nis currently no unified framework in continual learning capable\nof accommodating diverse types of annotations in medical appli-\ncations. We look forward to the development of an integrated\nframework for continual learning that can encompass the var-\nious settings and challenges highlighted in this paper. Such a\nframework would significantly advance the application of contin-\nual learning in the medical field, providing a more comprehensive\napproach to managing evolving datasets and diverse annotations.\n7.2. Incorporating domain knowledge\nIncorporating additional information beyond the existing med-\nical datasets has emerged as a more promising strategy to tackle\nthe issue of limited-sized medical datasets. Within this context,\ndomain knowledge plays a vital role in guiding the development\nof effective deep learning algorithms for MIA. While many mod-\nels used in medical vision are adapted from those designed for\nnatural images, it’s worth noting that medical images typically\npresent more complex challenges, such as high inter-class simi-\nlarity, a scarcity of labeled data, and label noise. When applied\neffectively, domain knowledge can mitigate these challenges with\nreduced time and computational requirements. Integrating do-\nmain knowledge into deep learning algorithms can be achieved\nby leveraging anatomical details from MRI and CT images [83],\nexploiting multi-instance data from the same patient [5], incor-\nporating patient metadata [92], utilizing radiomic features, and\nconsidering textual reports that accompany the images [396].\nWhile the utilization of medical domain knowledge in deep\nlearning models is a prevalent practice, it is not without its chal-\nlenges. These challenges involve the selection, representation,\nand integration methods for medical domain knowledge [397].\nIdentifying such knowledge is a complex task primarily because\nthe experiences of medical professionals tend to be subjective and\nambiguous. It’s often difficult for medical practitioners to provide\nprecise and objective descriptions of the experiences they draw\nupon to complete specific tasks. Currently, the identification of\nmedical domain knowledge relies on manual processes, and there\nis no existing method for automatically identifying medical do-\nmain knowledge within a given field. Medical professionals typ-\nically draw from various types of domain knowledge simultane-\nously. Furthermore, Most existing approaches, however, incorpo-\nrate only a single type or a few types of medical domain knowl-\nedge, often from the same modality. Consequently, simultane-\nously integrating multiple forms of medical domain knowledge\nhas the potential to provide more robust support for deep learning\nmodels across various medical applications.\n7.3. Label-efficient learning by vision transformers\nCurrent label-efficient segmentation techniques primarily rely\non convolutional neural networks (CNNs). However, there has\nbeen a recent transformation in computer vision, driven by the in-\ntroduction of the transformer module [398]. This innovation has\n27\nTable 6: Overview of recent methods in Only Limited Supervision category.\nReference\nTask\nAlgorithm Design\nDataset\nResult\n[328]\nkidney tumor segmentation\nData Augmentation\nKiTS19\nMean Dice: 77.44\n[343]\nBrain tumour segmentation\nLearning-based technique for data aug-\nmentation\nT1-weighted MRI brain scans\ndescribed in [386]\nDice score: 0.815\n[348]\nClinical endoscopy image classifi-\ncation\nAngular margin metric to ProtoNet\nminiEndoGI\nclassification\ndatase\n5-way: 1-shot: 58.76 ± 1.64, 5-shot: 66.72 ± 1.35; 3-\nway: 1-shot: 75.06 ± 1.87, 5-shot: 81.20 ± 1.72; 2-way:\n1-shot: 85.60 ± 2.21, 5-shot: 90.60 ± 1.70\n[351]\nCardiac and organ segmentation\nPrototype-based\nnetwork\n+\nSelf-\nsupervision\nAbdominal\nCT;\nAbdominal\nT2-SPIR MRI; Cardiac bSSFP\nMRI\nAbdominal CT: 1-shot:\nDice:\n67.62, 5-shot:\nDice:\n75.91; Abdominal MRI: 1-shot: Dice: 76.81, 5-shot:\nDice: 80.16; Cardiac: 1-shot: Dice: 77.94, 5-shot: Dice:\n81.66\n[352]\nAbdomen and cardiac segmentation\nAnomaly detection-inspired FS + Self-\nsupervision\nMS-CMRSeg; CHAOS\nCHAOS: Mean DSC: 72.41; MS-CMRSeg: Mean DSC:\n69.62\n[353]\nAbdomen and cardiac segmentation\nPrototype-based\nnetwork\n+\nSelf-\nsupervision\nMS-CMRSeg;\nCHAOS;\nBTCV\nCHAOS: Mean 95 HD: 12.5; Mean DSC: 80.99; BTCV:\nMean 95 HD: 23.60; Mean DSC: 60.94; MS-CMRSeg:\nMean 95 HD: 6.08; Mean DSC: 69.68\n[355]\nAbdomen segmentation\nContext relation encoder + Recurrent\nmask refine- ment module + Prototypi-\ncal network\nABD-110; BTCV; CHAOS\nABD-110: DSC: 81.91; BTCV: DSC: 72.48; CHAOS:\nDSC: 79.26\n[369]\nSkin Disease Classification\nFSL with Query-Relative loss\nDermatology images\n5-way 1-shot: ACC%: 52.41 Precision%: 53.21 F1%:\n49.52; 5-way 5-shot: ACC%: 71.99 Precision%: 74.23\nF1%: 70.30\n[359]\nBrain tumor segmentation\nMeta-learning\nBraTS2021\n1-way 1-shot: DSC(µ std)% : 0.57 0.19; 1-way 1-shot:\nDSC(µ std)% : 0.63 0.16; 1-way 1-shot: DSC(µ std)%\n: 0.65 0.17\n[358]\nClassification\nof\nwhole-genome\ndoubling across 17 cancer types\nModel-Agnostic Meta-Learning\nTCGA\nAUC (average 1 standard deviation): 0.6944 0.0773\n[363]\nTumour classification\nInterpretable\nstructure-\nconstrained\ngraph neural network\nPrivate dataset: 150 patients\nACC: 83.3, AUC: 81.9, SEN: 67.2 and SPE: 85.7\n[362]\nCardiac segmentation\nGradient-based\nmeta-hallucination\nlearning\nMM-WHS 2017\n4-shots: Average Dice: 75.6, Average ASD: 4.8; 1-\nshots: Average Dice: 51.8, Average ASD: 14.1\n[364]\nRare fundus diseases diagnosis\nMeta learning + Co-regularization loss +\nEnsemble-learning\nFundusData-FS [364]\nAccuracy(%): 2-way: 1-shot: 71.53, 3-shot: 78.20, 5-\nshot: 81.47; Accuracy(%): 3-way: 1-shot: 56.69, 3-\nshot: 62.62, 5-shot: 66.78; Accuracy(%): 4-way: 1-\nshot: 48.17, 3-shot: 56.65, 5-shot: 58.60\n[365]\nMulti-organ segmentation\nSqueeze and excitep framework\nVisceral dataset\nMean Dice score on validation set: 0.567\n[368]\nMulti-organ segmentation\nEncoder-decoder architecture + Spatial\nand prototypical priors\nCHAOS\nLeft atrium (LA): 1-shot: Mean DSC: 86.37; 5-shot:\nMean DSC: 88.02 Left ventricle (LV): 1-shot: Mean\nDSC: 87.06; 5-shot: Mean DSC: 87.87\ngiven rise to vision transformers (ViT) [399] and their adaptations\n[400], leading to significant advancements in numerous medical\napplications, including segmentation, detection, and classifica-\ntion tasks. Transformer-based models can achieve higher perfor-\nmance when trained on extensive datasets, but their effectiveness\ndiminishes when data or annotations are scarce. To overcome\nthis challenge, self-supervised transformers offer a promising so-\nlution. By utilizing unlabeled data and employing proxy tasks\nlike contrastive learning and reconstruction, these transformers\ncan enhance their representation learning capabilities [401]. For\ninstance, the Self-Supervised SwinUNETR [99] and unified pre-\ntraining [402] frameworks in the medical domain demonstrate\nthat training with large-scale unlabeled 2D or 3D images is ad-\nvantageous for fine-tuning models with smaller datasets. How-\never, it’s worth noting that the utilization of pre-training can be\ncomputationally demanding. Future research directions may aim\nto simplify and assess the efficiency of the pre-training frame-\nwork, especially regarding its applicability to smaller datasets.\n7.4. Flexible target model design\nTo develop better architectures for data-efficient deep learn-\ning architectures, one promising avenue is the field of automated\narchitectural engineering. Presently, the architectures predomi-\nnantly in use are crafted by human experts through iterative pro-\ncesses that are susceptible to errors. To circumvent the need for\nmanual design, researchers have put forward the concept of au-\ntomating architectural engineering, with one relevant domain be-\ning neural architecture search (NAS), introduced by Zoph and Le\n[403]. However, it’s essential to note that the majority of NAS in-\nvestigations have been concentrated on image classification tasks\n[404]. Regrettably, this focus has yet to yield truly transformative\nmodels capable of instigating fundamental shifts [405]. Never-\ntheless, the exploration of NAS for data-efficient learning in MIA\nremains a promising avenue.\n7.5. Federated learning\nModern healthcare systems collect significant amounts of med-\nical data, yet the complete utilization of this data by deep learning\nis hindered. This limitation stems from the data being isolated\nwithin silos and privacy concerns that limit data access [20]. To\ntackle this challenge, federated learning (FL) emerges as a learn-\ning paradigm seeking to address the problem of data governance\nand privacy. It achieves this by training algorithms collabora-\ntively without the need to exchange the data itself [406].\nFL\npreserves data privacy while collectively improving model effi-\nciency, making it a valuable tool for data-efficient deep learning\nin MIA. FL has produced valuable outcomes in the MIA domain\n[407, 408, 409]. Nevertheless, existing FL algorithms are pre-\ndominantly trained using supervised methods. When implement-\ning FL in real-world MIA situations, a critical issue arises: label\n28\nscarcity can occur in local healthcare datasets. Different medi-\ncal centers may have varying degrees of missing labels, or the\nlabel granularity may differ. A potential avenue for research is\nthe development of label-efficient federated learning techniques\nto tackle this challenge.\n7.6. Data-efficient learning with text supervision\nText supervision involves using textual descriptions or labels\nas additional sources of information during training. This can\ninclude utilizing clinical reports, medical terminology, or textual\nmetadata associated with images or patient records. By incorpo-\nrating text supervision, MIA models can learn to associate medi-\ncal text with visual patterns in images, facilitating improved gen-\neralization and a deeper understanding of medical data. Some\nstudies have explored this approach [396, 410]. We encourage\nfuture investigations to further explore and expand upon this area\nof study.\n8. Conclusion\nThe challenge of acquiring high-quality labels remains a sig-\nnificant hurdle for supervised learning in Medical Image Anal-\nysis (MIA). This challenge has fueled interest in alternative ap-\nproaches that enhance labeling efficiency to reduce the labeled\ndata requirement.\nIn recent years, extensive research efforts\nhave been dedicated to advancing data-efficient learning within\nthe realm of medical images, resulting in the development of\nnumerous techniques applicable across diverse application do-\nmains. In this paper, we have provided a comprehensive review\nthat explores the recent progress in data-efficient deep learning\nfor MIA. Specifically, we conducted a thorough examination of\ndeep learning-based data-efficient methodologies and categorized\nthem into five distinct groups. These categorizations are rooted\nin the varying degrees of supervision they depend on, covering a\nspectrum from scenarios with no supervision to those involving\ninexact, incomplete, inaccurate, and only limited supervision. Fi-\nnally, we highlight several potential future directions for research\nand development in this area. We hope that this survey serves as a\nvaluable resource, offering insights into the current state of data-\nefficient deep learning in medical imaging and inspiring further\nprogress in this domain.\nReferences\n[1] E. J. Topol, High-performance medicine: the convergence\nof human and artificial intelligence, Nature medicine\n25 (1) (2019) 44–56.\n[2] N. Hashimoto, D. Fukushima, R. Koga, Y. Takagi,\nK. Ko, K. Kohno, M. Nakaguro, S. Nakamura, H. Hon-\ntani, I. Takeuchi, Multi-scale domain-adversarial multiple-\ninstance cnn for cancer subtype classification with unan-\nnotated histopathological images, in: Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 3852–3861.\n[3] Z. Shao, H. Bian, Y. Chen, Y. Wang, J. Zhang, X. Ji, et al.,\nTransmil: Transformer based correlated multiple instance\nlearning for whole slide image classification, Advances in\nneural information processing systems 34 (2021) 2136–\n2147.\n[4] C. Xue, Q. Dou, X. Shi, H. Chen, P.-A. Heng, Robust\nlearning at noisy labeled medical images: Applied to skin\nlesion classification, in: 2019 IEEE 16th International\nSymposium on Biomedical Imaging (ISBI 2019), IEEE,\n2019, pp. 1280–1283.\n[5] S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg,\nJ. Deaton, A. Loh, A. Karthikesalingam, S. Kornblith,\nT. Chen, et al., Big self-supervised models advance medi-\ncal image classification, in: Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2021, pp.\n3478–3488.\n[6] K. Yan, J. Cai, D. Jin, S. Miao, D. Guo, A. P. Harrison,\nY. Tang, J. Xiao, J. Lu, L. Lu, Sam: Self-supervised learn-\ning of pixel-wise anatomical embeddings in radiological\nimages, IEEE Transactions on Medical Imaging 41 (10)\n(2022) 2658–2669. doi:10.1109/TMI.2022.3169003.\n[7] G. Campanella, M. G. Hanna, L. Geneslaw, A. Miraflor,\nV. Werneck Krauss Silva, K. J. Busam, E. Brogi, V. E.\nReuter, D. S. Klimstra, T. J. Fuchs, Clinical-grade com-\nputational pathology using weakly supervised deep learn-\ning on whole slide images, Nature medicine 25 (8) (2019)\n1301–1309.\n[8] H. Zhu, J. Shi, J. Wu, Pick-and-learn:\nAutomatic\nquality evaluation for noisy-labeled image segmentation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2019: 22nd International Confer-\nence, Shenzhen, China, October 13–17, 2019, Proceed-\nings, Part VI 22, Springer, 2019, pp. 576–584.\n[9] A. Taleb, W. Loetzsch, N. Danz, J. Severin, T. Gaertner,\nB. Bergner, C. Lippert, 3d self-supervised methods for\nmedical imaging, Advances in neural information process-\ning systems 33 (2020) 18158–18172.\n[10] ´A. S. Hervella, J. Rouco, J. Novo, M. Ortega, Learning\nthe retinal anatomy from scarce annotated data using self-\nsupervised multimodal reconstruction, Applied Soft Com-\nputing 91 (2020) 106210.\n[11] C. Chen, Q. Dou, H. Chen, J. Qin, P.-A. Heng, Synergistic\nimage and feature adaptation: Towards cross-modality do-\nmain adaptation for medical image segmentation, in: Pro-\nceedings of the AAAI conference on artificial intelligence,\nVol. 33, 2019, pp. 865–872.\n[12] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio,\nF. Ciompi, M. Ghafoorian, J. A. Van Der Laak, B. Van Gin-\nneken, C. I. S´anchez, A survey on deep learning in medical\nimage analysis, Medical image analysis 42 (2017) 60–88.\n[13] X. Chen, X. Wang, K. Zhang, K.-M. Fung, T. C. Thai,\nK. Moore, R. S. Mannel, H. Liu, B. Zheng, Y. Qiu, Recent\nadvances and clinical applications of deep learning in med-\nical image analysis, Medical Image Analysis 79 (2022)\n102444.\n[14] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional\nnetworks for biomedical image segmentation, in: Medical\nImage Computing and Computer-Assisted Intervention–\nMICCAI 2015: 18th International Conference, Munich,\n29\nGermany, October 5-9, 2015, Proceedings, Part III 18,\nSpringer, 2015, pp. 234–241.\n[15] M. J. Willemink, W. A. Koszek, C. Hardell, J. Wu,\nD. Fleischmann, H. Harvey, L. R. Folio, R. M. Summers,\nD. L. Rubin, M. P. Lungren, Preparing medical imaging\ndata for machine learning, Radiology 295 (1) (2020) 4–15.\n[16] V. Gulshan,\nL. Peng,\nM. Coram,\nM. C. Stumpe,\nD. Wu, A. Narayanaswamy, S. Venugopalan, K. Widner,\nT. Madams, J. Cuadros, et al., Development and valida-\ntion of a deep learning algorithm for detection of diabetic\nretinopathy in retinal fundus photographs, jama 316 (22)\n(2016) 2402–2410.\n[17] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter,\nH. M. Blau, S. Thrun, Dermatologist-level classification of\nskin cancer with deep neural networks, nature 542 (7639)\n(2017) 115–118.\n[18] D. Gurari, D. Theriault, M. Sameki, B. Isenberg, T. A.\nPham, A. Purwada, P. Solski, M. Walker, C. Zhang,\nJ. Y. Wong, M. Betke, How to collect segmentations for\nbiomedical images? a benchmark evaluating the perfor-\nmance of experts, crowdsourced non-experts, and algo-\nrithms, in: 2015 IEEE Winter Conference on Applica-\ntions of Computer Vision, 2015, pp. 1169–1176.\ndoi:\n10.1109/WACV.2015.160.\n[19] S. Albarqouni, C. Baur, F. Achilles, V. Belagiannis,\nS. Demirci, N. Navab, Aggnet: deep learning from crowds\nfor mitosis detection in breast cancer histology images,\nIEEE transactions on medical imaging 35 (5) (2016) 1313–\n1321.\n[20] P. Rajpurkar, E. Chen, O. Banerjee, E. J. Topol, Ai in\nhealth and medicine, Nature medicine 28 (1) (2022) 31–\n38.\n[21] V. Cheplygina, M. de Bruijne, J. P. Pluim, Not-so-\nsupervised: a survey of semi-supervised, multi-instance,\nand transfer learning in medical image analysis, Medical\nimage analysis 54 (2019) 280–296.\n[22] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu,\nX. Ding, Embracing imperfect datasets: A review of deep\nlearning solutions for medical image segmentation, Medi-\ncal Image Analysis 63 (2020) 101693.\n[23] C. Jin, Z. Guo, Y. Lin, L. Luo, H. Chen, Label-efficient\ndeep learning in medical image analysis: Challenges and\nfuture directions, arXiv preprint arXiv:2303.12484 (2023).\n[24] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto,\nT. Kobayashi, K.-i. Komatsu, M. Matsui, H. Fujita,\nY. Kodera, K. Doi, Development of a digital image\ndatabase for chest radiographs with and without a lung\nnodule: receiver operating characteristic analysis of radiol-\nogists’ detection of pulmonary nodules, American Journal\nof Roentgenology 174 (1) (2000) 71–74.\n[25] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thomp-\nson, G. Alexander, D. Harvey, B. Borowski, P. J. Britson,\nJ. L. Whitwell, C. Ward, et al., The alzheimer’s disease\nneuroimaging initiative (adni): Mri methods, Journal of\nMagnetic Resonance Imaging: An Official Journal of the\nInternational Society for Magnetic Resonance in Medicine\n27 (4) (2008) 685–691.\n[26] M. W. Weiner, D. P. Veitch, P. S. Aisen, L. A. Beckett, N. J.\nCairns, R. C. Green, D. Harvey, C. R. Jack Jr, W. Jagust,\nJ. C. Morris, et al., The alzheimer’s disease neuroimag-\ning initiative 3: Continued innovation for clinical trial im-\nprovement, Alzheimer’s & Dementia 13 (5) (2017) 561–\n571.\n[27] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer,\nK. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom,\nR. Wiest, et al., The multimodal brain tumor image seg-\nmentation benchmark (brats), IEEE transactions on medi-\ncal imaging 34 (10) (2014) 1993–2024.\n[28] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright,\nS. Fitzgibbon, J. Bozek, S. J. Counsell, J. Steinweg,\nK. Vecchiato, J. Passerat-Palmbach, et al., The develop-\ning human connectome project: A minimal processing\npipeline for neonatal cortical surface reconstruction, Neu-\nroimage 173 (2018) 88–112.\n[29] R. Souza, O. Lucena, J. Garrafa, D. Gobbi, M. Saluzzi,\nS. Appenzeller, L. Rittner, R. Frayne, R. Lotufo, An\nopen, multi-vendor, multi-field-strength brain mr dataset\nand analysis of publicly available skull stripping methods\nagreement, NeuroImage 170 (2018) 482–494.\n[30] H. J. Kuijf, J. M. Biesbroek, J. De Bresser, R. Heinen,\nS. Andermatt, M. Bento, M. Berseth, M. Belyaev, M. J.\nCardoso, A. Casamitjana, et al., Standardized assessment\nof automatic segmentation of white matter hyperintensi-\nties and results of the wmh segmentation challenge, IEEE\ntransactions on medical imaging 38 (11) (2019) 2556–\n2568.\n[31] C.-G. Yan, X. Chen, L. Li, F. X. Castellanos, T.-J. Bai, Q.-\nJ. Bo, J. Cao, G.-M. Chen, N.-X. Chen, W. Chen, et al.,\nReduced default mode network functional connectivity in\npatients with recurrent major depressive disorder, Proceed-\nings of the National Academy of Sciences 116 (18) (2019)\n9078–9083.\n[32] X. Zhuang, J. Shen, Multi-scale patch and multi-modality\natlases for whole heart segmentation of mri, Medical im-\nage analysis 31 (2016) 77–87.\n[33] X. Zhuang, L. Li, C. Payer, D. vStern, M. Urschler, M. P.\nHeinrich, J. Oster, C. Wang, ¨O. Smedby, C. Bian, et al.,\nEvaluation of algorithms for multi-modality whole heart\nsegmentation: an open-access grand challenge, Medical\nimage analysis 58 (2019) 101537.\n[34] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang,\nP.-A. Heng, I. Cetin, K. Lekadir, O. Camara, M. A. G.\nBallester, et al., Deep learning techniques for automatic\nmri cardiac multi-structures segmentation and diagnosis:\nis the problem solved?, IEEE transactions on medical\nimaging 37 (11) (2018) 2514–2525.\n[35] Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng,\nS. Vesal, N. Ravikumar, A. Maier, X. Yang, et al., A\n30\nglobal benchmark of algorithms for segmenting the left\natrium from late gadolinium-enhanced cardiac magnetic\nresonance imaging, Medical image analysis 67 (2021)\n101832.\n[36] X. Zhuang, Multivariate mixture model for cardiac seg-\nmentation from multi-sequence mri, in: International Con-\nference on Medical Image Computing and Computer-\nAssisted Intervention, Springer, 2016, pp. 581–588.\n[37] V. M. Campello, P. Gkontra, C. Izquierdo, C. Martin-Isla,\nA. Sojoudi, P. M. Full, K. Maier-Hein, Y. Zhang, Z. He,\nJ. Ma, et al., Multi-centre, multi-vendor and multi-disease\ncardiac segmentation: the m&ms challenge, IEEE Trans-\nactions on Medical Imaging 40 (12) (2021) 3543–3554.\n[38] J. Sivaswamy, S. Krishnadas, G. D. Joshi, M. Jain, A. U. S.\nTabish, Drishti-gs: Retinal image dataset for optic nerve\nhead (onh) segmentation, in: 2014 IEEE 11th international\nsymposium on biomedical imaging (ISBI), IEEE, 2014,\npp. 53–56.\n[39] H. Bogunovi´c, F. Venhuizen, S. Klimscha, S. Apos-\ntolopoulos, A. Bab-Hadiashar, U. Bagci, M. F. Beg,\nL. Bekalo, Q. Chen, C. Ciller, et al., Retouch: The retinal\noct fluid detection and segmentation benchmark and chal-\nlenge, IEEE transactions on medical imaging 38 (8) (2019)\n1858–1874.\n[40] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim,\nH. Liang, S. L. Baxter, A. McKeown, G. Yang, X. Wu,\nF. Yan, et al., Identifying medical diagnoses and treatable\ndiseases by image-based deep learning, cell 172 (5) (2018)\n1122–1131.\n[41] H. Fu, F. Li, J. I. Orlando, H. Bogunovi, X. Sun, J. Liao,\nY. Xu, S. Zhang, X. Zhang, Palm: Pathologic myopia chal-\nlenge (2019). doi:10.21227/55pk-8z03.\nURL https://dx.doi.org/10.21227/55pk-8z03\n[42] J. I. Orlando, H. Fu, J. B. Breda, K. Van Keer, D. R.\nBathula, A. Diaz-Pinto, R. Fang, P.-A. Heng, J. Kim,\nJ. Lee, et al., Refuge challenge: A unified framework\nfor evaluating automated methods for glaucoma assess-\nment from fundus photographs, Medical image analysis 59\n(2020) 101570.\n[43] H. Fang, F. Li, H. Fu, X. Sun, X. Cao, F. Lin, J. Son,\nS. Kim, G. Quellec, S. Matta, et al., Adam challenge:\nDetecting age-related macular degeneration from fundus\nimages, IEEE Transactions on Medical Imaging 41 (10)\n(2022) 2828–2847.\n[44] S. Hu, Z. Liao, Y. Xia, Domain specific convolution and\nhigh frequency reconstruction based unsupervised domain\nadaptation for medical image segmentation, in:\nInter-\nnational Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2022, pp. 650–\n659.\n[45] R. Ludovic, R. Daniel, L. Nicolas, K. Maria, I. Humayun,\nK. Jacques, C. Fr´ed´erique, G. Catherine, et al., Mitosis de-\ntection in breast cancer histological images an icpr 2012\ncontest, Journal of pathology informatics 4 (1) (2013) 8.\n[46] M. Veta, Y. J. Heng, N. Stathonikos, B. E. Bejnordi,\nF. Beca, T. Wollmann, K. Rohr, M. A. Shah, D. Wang,\nM. Rousson, et al., Predicting breast tumor proliferation\nfrom whole-slide images: the tupac16 challenge, Medical\nimage analysis 54 (2019) 111–121.\n[47] G. Litjens, P. Bandi, B. Ehteshami Bejnordi, O. Geessink,\nM. Balkenhol,\nP. Bult,\nA. Halilovic,\nM. Hermsen,\nR. van de Loo, R. Vogels, et al., 1399 h&e-stained sen-\ntinel lymph node sections of breast cancer patients: the\ncamelyon dataset, GigaScience 7 (6) (2018) giy065.\n[48] G. Aresta, T. Ara´ujo, S. Kwok, S. S. Chennamsetty,\nM. Safwan, V. Alex, B. Marami, M. Prastawa, M. Chan,\nM. Donovan, et al., Bach: Grand challenge on breast can-\ncer histology images, Medical image analysis 56 (2019)\n122–139.\n[49] A. R. Saikia, K. Bora, L. B. Mahanta, A. K. Das, Compar-\native assessment of cnn architectures for classification of\nbreast fnac images, Tissue and Cell 57 (2019) 8–14.\n[50] N. Petrick, S. Akbar, K. H. Cha, S. Nofech-Mozes,\nB. Sahiner, M. A. Gavrielides, J. Kalpathy-Cramer,\nK. Drukker, A. L. Martel, f. t. BreastPathQ Chal-\nlenge Group, Spie-aapm-nci breastpathq challenge: an im-\nage analysis challenge for quantitative tumor cellularity\nassessment in breast cancer histology images following\nneoadjuvant treatment, Journal of Medical Imaging 8 (3)\n(2021) 034501–034501.\n[51] H. A. Phoulady, P. R. Mouton, A new cervical cytol-\nogy dataset for nucleus detection and image classifica-\ntion (cervix93) and methods for cervical nucleus detection,\narXiv preprint arXiv:1811.09651 (2018).\n[52] E. Hussain, L. B. Mahanta, H. Borah, C. R. Das, Liq-\nuid based-cytology pap smear dataset for automated multi-\nclass diagnosis of pre-cancerous and cervical cancer le-\nsions, Data in brief 30 (2020) 105589.\n[53] A. E. Kavur, N. S. Gezer, M. Barıcs, S. Aslan, P.-H. Conze,\nV. Groza, D. D. Pham, S. Chatterjee, P. Ernst, S. ¨Ozkan,\net al., Chaos challenge-combined (ct-mr) healthy ab-\ndominal organ segmentation, Medical Image Analysis 69\n(2021) 101950.\n[54] F. Su, Y. Sun, Y. Hu, P. Yuan, X. Wang, Q. Wang, J. Li, J.-F.\nJi, Development and validation of a deep learning system\nfor ascites cytopathology interpretation, Gastric Cancer 23\n(2020) 1041–1050.\n[55] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Va-\nhadane, A. Sethi, A dataset and a technique for generalized\nnuclear segmentation for computational pathology, IEEE\ntransactions on medical imaging 36 (7) (2017) 1550–1560.\n[56] E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula,\nK. Gurusamy, B. Davidson, S. P. Pereira, M. J. Clarkson,\nD. C. Barratt, Automatic multi-organ segmentation on ab-\ndominal ct with dense v-networks, IEEE transactions on\nmedical imaging 37 (8) (2018) 1822–1834.\n31\n[57] K. Yan, X. Wang, L. Lu, R. M. Summers, Deeplesion: au-\ntomated mining of large-scale lesion annotations and uni-\nversal lesion detection with deep learning, Journal of med-\nical imaging 5 (3) (2018) 036501–036501.\n[58] M.\nAubreville,\nN.\nStathonikos,\nC.\nA.\nBertram,\nR. Klopfleisch, N. Ter Hoeve, F. Ciompi, F. Wilm,\nC. Marzahl, T. A. Donovan, A. Maier, et al., Mitosis\ndomain generalization in histopathology imagesthe midog\nchallenge, Medical Image Analysis 84 (2023) 102699.\n[59] K. Sirinukunwattana, S. E. A. Raza, Y.-W. Tsang, D. R.\nSnead, I. A. Cree, N. M. Rajpoot, Locality sensitive deep\nlearning for detection and classification of nuclei in rou-\ntine colon cancer histology images, IEEE transactions on\nmedical imaging 35 (5) (2016) 1196–1206.\n[60] J. N. Kather, J. Krisam, P. Charoentong, T. Luedde, E. Her-\npel, C.-A. Weis, T. Gaiser, A. Marx, N. A. Valous, D. Fer-\nber, et al., Predicting survival from colorectal cancer his-\ntology slides using deep learning: A retrospective multi-\ncenter study, PLoS medicine 16 (1) (2019) e1002730.\n[61] G. Litjens, R. Toth, W. Van De Ven, C. Hoeks, S. Kerk-\nstra, B. Van Ginneken, G. Vincent, G. Guillard, N. Bir-\nbeck, J. Zhang, et al., Evaluation of prostate segmentation\nalgorithms for mri: the promise12 challenge, Medical im-\nage analysis 18 (2) (2014) 359–373.\n[62] E. Arvaniti, K. S. Fricker, M. Moret, N. Rupp, T. Her-\nmanns, C. Fankhauser, N. Wey, P. J. Wild, J. H. Rueschoff,\nM. Claassen, Automated gleason grading of prostate can-\ncer tissue microarrays via deep learning, Scientific reports\n8 (1) (2018) 12054.\n[63] W. Bulten, K. Kartasalo, P.-H. C. Chen, P. Str¨om, H. Pinck-\naers, K. Nagpal, Y. Cai, D. F. Steiner, H. van Boven,\nR. Vink, et al., Artificial intelligence for diagnosis and\ngleason grading of prostate cancer: the panda challenge,\nNature medicine 28 (1) (2022) 154–163.\n[64] F.\nPrados,\nJ.\nAshburner,\nC.\nBlaiotta,\nT.\nBrosch,\nJ. Carballido-Gamio, M. J. Cardoso, B. N. Conrad,\nE. Datta, G. D´avid, B. De Leener, et al., Spinal cord grey\nmatter segmentation challenge, Neuroimage 152 (2017)\n312–329.\n[65] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. W´ang, P.-X. Lu,\nG. Thoma, Two public chest x-ray datasets for computer-\naided screening of pulmonary diseases, Quantitative imag-\ning in medicine and surgery 4 (6) (2014) 475.\n[66] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Sum-\nmers, Chestx-ray8: Hospital-scale chest x-ray database\nand benchmarks on weakly-supervised classification and\nlocalization of common thorax diseases, in: Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 2097–2106.\n[67] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Green-\nbaum, M. P. Lungren, C.-y. Deng, R. G. Mark, S. Horng,\nMimic-cxr, a de-identified publicly available database of\nchest radiographs with free-text reports, Scientific data\n6 (1) (2019) 317.\n[68] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha,\nW. Liang, C. Wang, K. Wang, et al., Clinically applica-\nble ai system for accurate diagnosis, quantitative measure-\nments, and prognosis of covid-19 pneumonia using com-\nputed tomography, Cell 181 (6) (2020) 1423–1433.\n[69] Z. Lambert, C. Petitjean, B. Dubray, S. Kuan, Segthor:\nSegmentation of thoracic organs at risk in ct images, in:\n2020 Tenth International Conference on Image Processing\nTheory, Tools and Applications (IPTA), IEEE, 2020, pp.\n1–6.\n[70] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran,\nD. B. Nguyen, D. D. Le, C. M. Pham, H. T. Tong, D. H.\nDinh, et al., Vindr-cxr: An open dataset of chest x-rays\nwith radiologists annotations, Scientific Data 9 (1) (2022)\n429.\n[71] S. Shurrab, R. Duwairi, Self-supervised learning methods\nand applications in medical imaging analysis: A survey,\nPeerJ Computer Science 8 (2022) e1045.\n[72] C. Doersch, A. Gupta, A. A. Efros, Unsupervised visual\nrepresentation learning by context prediction, in: Proceed-\nings of the IEEE international conference on computer vi-\nsion, 2015, pp. 1422–1430.\n[73] M. Noroozi, P. Favaro, Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles, in: European\nconference on computer vision, Springer, 2016, pp. 69–84.\n[74] S. Gidaris, P. Singh, N. Komodakis, Unsupervised repre-\nsentation learning by predicting image rotations, in: Inter-\nnational Conference on Learning Representations, 2018.\n[75] W. Bai, C. Chen, G. Tarroni, J. Duan, F. Guitton, S. E.\nPetersen, Y. Guo, P. M. Matthews, D. Rueckert, Self-\nsupervised learning for cardiac mr image segmentation\nby anatomical position prediction, in:\nMedical Image\nComputing and Computer Assisted Intervention–MICCAI\n2019: 22nd International Conference, Shenzhen, China,\nOctober 13–17, 2019, Proceedings, Part II 22, Springer,\n2019, pp. 541–549.\n[76] A. Taleb, C. Lippert, T. Klein, M. Nabi, Multimodal self-\nsupervised learning for medical image analysis, in: Inter-\nnational conference on information processing in medical\nimaging, Springer, 2021, pp. 661–673.\n[77] X. Zhuang, Y. Li, Y. Hu, K. Ma, Y. Yang, Y. Zheng,\nSelf-supervised feature learning for 3d medical images by\nplaying a rubiks cube, in: Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2019: 22nd In-\nternational Conference, Shenzhen, China, October 13–17,\n2019, Proceedings, Part IV 22, Springer, 2019, pp. 420–\n428.\n[78] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, Y. Zheng, Ru-\nbiks cube+: A self-supervised feature learning framework\nfor 3d medical image analysis, Medical image analysis 64\n(2020) 101746.\n[79] X.-B. Nguyen, G. S. Lee, S. H. Kim, H. J. Yang, Self-\nsupervised learning based on spatial awareness for medical\nimage analysis, IEEE Access 8 (2020) 162973–162981.\n32\n[80] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara,\nD. Rueckert, Self-supervised learning for medical image\nanalysis using image context restoration, Medical image\nanalysis 58 (2019) 101539.\n[81] H.-Y. Zhou, C. Lu, C. Chen, S. Yang, Y. Yu, A uni-\nfied visual information preservation framework for self-\nsupervised pre-training in medical image analysis, IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(2023).\n[82] T. Ross, D. Zimmerer, A. Vemuri, F. Isensee, M. Wiesen-\nfarth, S. Bodenstedt, F. Both, P. Kessler, M. Wagner,\nB. M¨uller, et al., Exploiting the potential of unlabeled en-\ndoscopic video data with self-supervised learning, Interna-\ntional journal of computer assisted radiology and surgery\n13 (2018) 925–933.\n[83] Z. Zhou, V. Sodha, M. M. Rahman Siddiquee, R. Feng,\nN. Tajbakhsh, M. B. Gotway, J. Liang, Models genesis:\nGeneric autodidactic models for 3d medical image analy-\nsis, in: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2019: 22nd International Confer-\nence, Shenzhen, China, October 13–17, 2019, Proceed-\nings, Part IV 22, Springer, 2019, pp. 384–393.\n[84] O. G. Holmberg, N. D. K¨ohler, T. Martins, J. Siedlecki,\nT.\nHerold,\nL.\nKeidel,\nB.\nAsani,\nJ.\nSchiefelbein,\nS. Priglinger, K. U. Kortuem, et al., Self-supervised retinal\nthickness prediction enables deep learning from unlabelled\ndata to boost classification of diabetic retinopathy, Nature\nMachine Intelligence 2 (11) (2020) 719–726.\n[85] M. Prakash, T.-O. Buchholz, M. Lalit, P. Tomancak, F. Jug,\nA. Krull, Leveraging self-supervised denoising for image\nsegmentation, in: 2020 IEEE 17th international sympo-\nsium on biomedical imaging (ISBI), IEEE, 2020, pp. 428–\n432.\n[86] X. Tao, Y. Li, W. Zhou, K. Ma, Y. Zheng, Revisit-\ning rubiks cube: self-supervised learning with volume-\nwise transformation for 3d medical image segmentation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2020: 23rd International Confer-\nence, Lima, Peru, October 4–8, 2020, Proceedings, Part\nIV 23, Springer, 2020, pp. 238–248.\n[87] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, Con-\ntrastive learning of global and local features for medical\nimage segmentation with limited annotations, Advances in\nneural information processing systems 33 (2020) 12546–\n12558.\n[88] C. Zhang, H. Zheng, Y. Gu, Dive into the details of self-\nsupervised learning for medical image analysis, Medical\nImage Analysis 89 (2023) 102879.\n[89] A. Taleb, M. Kirchler, R. Monti, C. Lippert, Contig: Self-\nsupervised multimodal contrastive learning for medical\nimaging with genetics, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2022, pp. 20908–20921.\n[90] H. Sowrirajan, J. Yang, A. Y. Ng, P. Rajpurkar, Moco\npretraining improves representation and transferability of\nchest x-ray models, in: Medical Imaging with Deep Learn-\ning, PMLR, 2021, pp. 728–744.\n[91] K. He, H. Fan, Y. Wu, S. Xie, R. Girshick, Momentum\ncontrast for unsupervised visual representation learning,\nin: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2020, pp. 9729–9738.\n[92] Y. N. T. Vu, R. Wang, N. Balachandar, C. Liu, A. Y. Ng,\nP. Rajpurkar, Medaug: Contrastive learning leveraging pa-\ntient metadata improves representations for chest x-ray in-\nterpretation, in: Machine Learning for Healthcare Confer-\nence, PMLR, 2021, pp. 755–769.\n[93] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple\nframework for contrastive learning of visual representa-\ntions, in: International conference on machine learning,\nPMLR, 2020, pp. 1597–1607.\n[94] O. Ciga, T. Xu, A. L. Martel, Self supervised contrastive\nlearning for digital histopathology, Machine Learning with\nApplications 7 (2022) 100198.\n[95] Y. He, G. Yang, R. Ge, Y. Chen, J.-L. Coatrieux, B. Wang,\nS. Li, Geometric visual similarity learning in 3d medical\nimage self-supervised pre-training, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 9538–9547.\n[96] F. Haghighi, M. R. Hosseinzadeh Taher, Z. Zhou, M. B.\nGotway, J. Liang, Learning semantics-enriched repre-\nsentation via self-discovery, self-classification, and self-\nrestoration, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2020: 23rd International\nConference, Lima, Peru, October 4–8, 2020, Proceedings,\nPart I 23, Springer, 2020, pp. 137–147.\n[97] X. Zhang, S. Feng, Y. Zhou, Y. Zhang, Y. Wang, Sar:\nScale-aware restoration learning for 3d tumor segmen-\ntation, in:\nMedical Image Computing and Computer\nAssisted Intervention–MICCAI 2021: 24th International\nConference, Strasbourg, France, September 27–October 1,\n2021, Proceedings, Part II 24, Springer, 2021, pp. 124–\n133.\n[98] H.-Y. Zhou, C. Lu, S. Yang, X. Han, Y. Yu, Preservational\nlearning improves self-supervised medical image models\nby reconstructing diverse contexts, in: Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n2021, pp. 3499–3509.\n[99] Y. Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu,\nV. Nath, A. Hatamizadeh, Self-supervised pre-training of\nswin transformers for 3d medical image analysis, in: Pro-\nceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2022, pp. 20730–20740.\n[100] P. Yang, X. Yin, H. Lu, Z. Hu, X. Zhang, R. Jiang,\nH. Lv, Cs-co: A hybrid self-supervised visual represen-\ntation learning method for h&e-stained histopathological\nimages, Medical Image Analysis 81 (2022) 102539.\n33\n[101] X. Yan, J. Naushad, S. Sun, K. Han, H. Tang, D. Kong,\nH. Ma, C. You, X. Xie, Representation recovering for self-\nsupervised pre-training on medical images, in: Proceed-\nings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, 2023, pp. 2685–2695.\n[102] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, P. Ra-\njpurkar, Expert-level detection of pathologies from unan-\nnotated chest x-ray images via self-supervised learning,\nNature Biomedical Engineering 6 (12) (2022) 1399–1406.\n[103] L. Qu, S. Liu, X. Liu, M. Wang, Z. Song, Towards\nlabel-efficient automatic diagnosis and analysis: a compre-\nhensive survey of advanced deep learning-based weakly-\nsupervised, semi-supervised and self-supervised tech-\nniques in histopathological image analysis, Physics in\nMedicine & Biology (2022).\n[104] X. Wang, Y. Yan, P. Tang, X. Bai, W. Liu, Revisiting\nmultiple instance neural networks, Pattern Recognition 74\n(2018) 15–24.\n[105] E. Schwab, A. Gooßen, H. Deshpande, A. Saalbach, Lo-\ncalization of critical findings in chest x-ray without lo-\ncal annotations using multi-instance learning, in: 2020\nIEEE 17th International Symposium on Biomedical Imag-\ning (ISBI), IEEE, 2020, pp. 1879–1882.\n[106] J. Ramon, L. De Raedt, Multi instance neural networks,\nin: Proceedings of the ICML-2000 workshop on attribute-\nvalue and relational learning, 2000, pp. 53–60.\n[107] O. Maron, T. Lozano-P´erez, A framework for multiple-\ninstance learning, Advances in neural information process-\ning systems 10 (1997).\n[108] O. Z. Kraus, J. L. Ba, B. J. Frey, Classifying and segment-\ning microscopy images with deep multiple instance learn-\ning, Bioinformatics 32 (12) (2016) i52–i59.\n[109] Y. Yan, X. Wang, X. Guo, J. Fang, W. Liu, J. Huang, Deep\nmulti-instance learning with dynamic pooling, in: Asian\nConference on Machine Learning, PMLR, 2018, pp. 662–\n677.\n[110] H. D. Couture, J. S. Marron, C. M. Perou, M. A.\nTroester, M. Niethammer, Multiple instance learning for\nheterogeneous images: Training a cnn for histopathology,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2018: 21st International Confer-\nence, Granada, Spain, September 16-20, 2018, Proceed-\nings, Part II 11, Springer, 2018, pp. 254–262.\n[111] J. Qu, X. Wei, X. Qian, Generalized pancreatic cancer di-\nagnosis via multiple instance learning and anatomically-\nguided shape normalization, Medical Image Analysis 86\n(2023) 102774.\n[112] Y. Zhao, F. Yang, Y. Fang, H. Liu, N. Zhou, J. Zhang,\nJ. Sun, S. Yang, B. Menze, X. Fan, et al., Predicting lymph\nnode metastasis using histopathological images based on\nmultiple instance learning with deep graph convolution, in:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 4837–4846.\n[113] M. Y. Lu, R. J. Chen, J. Wang, D. Dillon, F. Mahmood,\nSemi-supervised histology classification using deep mul-\ntiple instance learning and contrastive predictive coding,\narXiv preprint arXiv:1910.10825 (2019).\n[114] A. v. d. Oord, Y. Li, O. Vinyals, Representation learn-\ning with contrastive predictive coding, arXiv preprint\narXiv:1807.03748 (2018).\n[115] B. Li, Y. Li, K. W. Eliceiri, Dual-stream multiple instance\nlearning network for whole slide image classification with\nself-supervised contrastive learning, in: Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 2021, pp. 14318–14328.\n[116] P. Chikontwe, M. Luna, M. Kang, K. S. Hong, J. H. Ahn,\nS. H. Park, Dual attention multiple instance learning with\nunsupervised complementary loss for covid-19 screening,\nMedical Image Analysis 72 (2021) 102105.\n[117] A. Raju, J. Yao, M. M. Haq, J. Jonnagaddala, J. Huang,\nGraph attention multi-instance learning for accurate col-\norectal cancer staging, in: Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2020: 23rd In-\nternational Conference, Lima, Peru, October 4–8, 2020,\nProceedings, Part V 23, Springer, 2020, pp. 529–539.\n[118] Z. Su, T. E. Tavolara, G. Carreno-Galeano, S. J. Lee, M. N.\nGurcan, M. Niazi, Attention2majority: Weak multiple in-\nstance learning for regenerative kidney grading on whole\nslide images, Medical Image Analysis 79 (2022) 102462.\n[119] M. Adnan, S. Kalra, H. R. Tizhoosh, Representation learn-\ning of histopathology images using graph neural networks,\nin: Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition Workshops, 2020,\npp. 988–989.\n[120] Y. Sharma, A. Shrivastava, L. Ehsan, C. A. Moskaluk,\nS. Syed, D. Brown, Cluster-to-conquer: A framework for\nend-to-end multi-instance learning for whole slide image\nclassification, in: Medical Imaging with Deep Learning,\nPMLR, 2021, pp. 682–698.\n[121] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Bar-\nbieri, F. Mahmood, Data-efficient and weakly supervised\ncomputational pathology on whole-slide images, Nature\nbiomedical engineering 5 (6) (2021) 555–570.\n[122] R. Yan, Y. Shen, X. Zhang, P. Xu, J. Wang, J. Li,\nF. Ren, D. Ye, S. K. Zhou, Histopathological bladder\ncancer gene mutation prediction with hierarchical deep\nmultiple-instance learning, Medical Image Analysis 87\n(2023) 102824.\n[123] M. Ilse, J. Tomczak, M. Welling, Attention-based deep\nmultiple instance learning, in: International conference on\nmachine learning, PMLR, 2018, pp. 2127–2136.\n[124] Y. Wang, P. Tang, Y. Zhou, W. Shen, E. K. Fishman,\nA. L. Yuille, Learning inductive attention guidance for par-\ntially supervised pancreatic ductal adenocarcinoma predic-\ntion, IEEE transactions on medical imaging 40 (10) (2021)\n2723–2735.\n34\n[125] Z. Li, W. Zhao, F. Shi, L. Qi, X. Xie, Y. Wei, Z. Ding,\nY. Gao, S. Wu, J. Liu, et al., A novel multiple instance\nlearning framework for covid-19 severity assessment via\ndata augmentation and self-supervised learning, Medical\nImage Analysis 69 (2021) 101978.\n[126] Z. Wang, L. Yu, X. Ding, X. Liao, L. Wang, Lymph\nnode metastasis prediction from whole slide images with\ntransformer-guided multiinstance learning and knowledge\ntransfer, IEEE Transactions on Medical Imaging 41 (10)\n(2022) 2777–2787.\n[127] T. Zhao, Z. Yin, Weakly supervised cell segmentation by\npoint annotation, IEEE Transactions on Medical Imaging\n40 (10) (2020) 2736–2747.\n[128] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang,\nL. Xie, X. Yang, Q. Tian, A survey on label-efficient deep\nimage segmentation: Bridging the gap between weak su-\npervision and dense prediction, IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence (2023).\n[129] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba,\nLearning deep features for discriminative localization, in:\nProceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 2921–2929.\n[130] Y. Li, Y. Liu, L. Huang, Z. Wang, J. Luo, Deep weakly-\nsupervised breast tumor segmentation in ultrasound im-\nages with explicit anatomical constraints, Medical image\nanalysis 76 (2022) 102315.\n[131] Z. Chen, Z. Tian, J. Zhu, C. Li, S. Du, C-cam: Causal\ncam for weakly supervised semantic segmentation on med-\nical image, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2022, pp.\n11676–11685.\n[132] S. Khan, A. H. Shahin, J. Villafruela, J. Shen, L. Shao,\nExtreme points derived confidence map as a cue for class-\nagnostic interactive segmentation using deep neural net-\nwork, in: Medical Image Computing and Computer As-\nsisted Intervention–MICCAI 2019:\n22nd International\nConference, Shenzhen, China, October 13–17, 2019, Pro-\nceedings, Part II 22, Springer, 2019, pp. 66–73.\n[133] H. R. Roth, D. Yang, Z. Xu, X. Wang, D. Xu, Going to\nextremes: weakly supervised medical image segmentation,\nMachine Learning and Knowledge Extraction 3 (2) (2021)\n507–524.\n[134] R. Dorent, S. Joutard, J. Shapey, A. Kujawa, M. Modat,\nS. Ourselin, T. Vercauteren, Inter extreme points geodesics\nfor end-to-end weakly supervised image segmentation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Confer-\nence, Strasbourg, France, September 27–October 1, 2021,\nProceedings, Part II 24, Springer, 2021, pp. 615–624.\n[135] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G. M.\nRiedlinger, S. De, S. Zhang, D. N. Metaxas, Weakly su-\npervised deep nuclei segmentation using partial points an-\nnotation in histopathology images, IEEE transactions on\nmedical imaging 39 (11) (2020) 3655–3666.\n[136] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma,\nY. Zheng, K.-T. Cheng, Label propagation for annotation-\nefficient nuclei segmentation from pathology images,\narXiv preprint arXiv:2202.08195 (2022).\n[137] K. Kise, A. Sato, M. Iwata, Segmentation of page images\nusing the area voronoi diagram, Computer Vision and Im-\nage Understanding 70 (3) (1998) 370–382.\n[138] Y. Lin, Z. Qu, H. Chen, Z. Gao, Y. Li, L. Xia, K. Ma,\nY. Zheng, K.-T. Cheng, Nuclei segmentation with point\nannotations from pathology images via self-supervised\nlearning and co-training, Medical Image Analysis (2023)\n102933.\n[139] H. Zhang, Y. Meng, Y. Zhao, Y. Qiao, X. Yang, S. E. Cou-\npland, Y. Zheng, Dtfd-mil: Double-tier feature distillation\nmultiple instance learning for histopathology whole slide\nimage classification, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2022, pp. 18802–18812.\n[140] Z. Chen, J. Zhang, S. Che, J. Huang, X. Han, Y. Yuan, Di-\nagnose like a pathologist: Weakly-supervised pathologist-\ntree network for slide-level immunohistochemical scoring,\nin: Proceedings of the AAAI Conference on Artificial In-\ntelligence, Vol. 35, 2021, pp. 47–54.\n[141] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M.\nMatthews, D. Rueckert, Recurrent neural networks for aor-\ntic image sequence segmentation with sparse annotations,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2018: 21st International Confer-\nence, Granada, Spain, September 16-20, 2018, Proceed-\nings, Part IV 11, Springer, 2018, pp. 586–594.\n[142] Z. Ji, Y. Shen, C. Ma, M. Gao, Scribble-based hierar-\nchical weakly supervised learning for brain tumor seg-\nmentation, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2019: 22nd International\nConference, Shenzhen, China, October 13–17, 2019, Pro-\nceedings, Part III 22, Springer, 2019, pp. 175–183.\n[143] Q. Chen, Y. Hong, Scribble2d5: Weakly-supervised volu-\nmetric image segmentation via scribble annotations, in: In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2022, pp. 234–\n243.\n[144] Y. B. Can, K. Chaitanya, B. Mustafa, L. M. Koch,\nE. Konukoglu, C. F. Baumgartner, Learning to segment\nmedical images with scribble-supervision alone, in: Deep\nLearning in Medical Image Analysis and Multimodal\nLearning for Clinical Decision Support: 4th International\nWorkshop, DLMIA 2018, and 8th International Work-\nshop, ML-CDS 2018, Held in Conjunction with MICCAI\n2018, Granada, Spain, September 20, 2018, Proceedings\n4, Springer, 2018, pp. 236–244.\n[145] M. Tang,\nF. Perazzi,\nA. Djelouah,\nI. Ben Ayed,\nC. Schroers, Y. Boykov, On regularized losses for weakly-\nsupervised cnn segmentation, in: Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), 2018, pp.\n507–522.\n35\n[146] X. Luo, M. Hu, W. Liao, S. Zhai, T. Song, G. Wang,\nS. Zhang, Scribble-supervised medical image segmen-\ntation via dual-branch network and dynamically mixed\npseudo labels supervision, in: International Conference on\nMedical Image Computing and Computer-Assisted Inter-\nvention, Springer, 2022, pp. 528–538.\n[147] G. Valvano, A. Leo, S. A. Tsaftaris, Learning to seg-\nment from scribbles using multi-scale adversarial atten-\ntion gates, IEEE Transactions on Medical Imaging 40 (8)\n(2021) 1990–2001.\n[148] P. Zhang, Y. Zhong, X. Li, Accl: Adversarial constrained-\ncnn loss for weakly supervised medical image segmenta-\ntion, arXiv preprint arXiv:2005.00328 (2020).\n[149] K. Zhang, X. Zhuang, Cyclemix: A holistic strategy for\nmedical image segmentation from scribble supervision, in:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 11656–11665.\n[150] F. Gao, M. Hu, M.-E. Zhong, S. Feng, X. Tian, X. Meng,\nZ. Huang, M. Lv, T. Song, X. Zhang, et al., Segmen-\ntation only uses sparse annotations: Unified weakly and\nsemi-supervised learning in medical images, Medical Im-\nage Analysis 80 (2022) 102515.\n[151] K. Zhang, X. Zhuang, Shapepu: A new pu learning frame-\nwork regularized by global consistency for scribble super-\nvised cardiac segmentation, in: International Conference\non Medical Image Computing and Computer-Assisted In-\ntervention, Springer, 2022, pp. 162–172.\n[152] H. Lee, W.-K. Jeong, Scribble2label: Scribble-supervised\ncell segmentation via self-generating pseudo-labels with\nconsistency, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2020: 23rd International\nConference, Lima, Peru, October 4–8, 2020, Proceedings,\nPart I 23, Springer, 2020, pp. 14–23.\n[153] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-\nPalmbach, W. Bai, M. Damodaram, M. A. Rutherford, J. V.\nHajnal, B. Kainz, et al., Deepcut: Object segmentation\nfrom bounding box annotations using convolutional neu-\nral networks, IEEE transactions on medical imaging 36 (2)\n(2016) 674–683.\n[154] J. Wang, B. Xia, Bounding box tightness prior for weakly\nsupervised image segmentation, in: International confer-\nence on medical image computing and computer-assisted\nintervention, Springer, 2021, pp. 526–536.\n[155] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, Z. Li,\nBoxpolyp: Boost generalized polyp segmentation using\nextra coarse bounding box annotations, in: International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, Springer, 2022, pp. 67–77.\n[156] R. Jiao, Y. Zhang, L. Ding, R. Cai, J. Zhang, Learning with\nlimited annotations: a survey on deep semi-supervised\nlearning for medical image segmentation, arXiv preprint\narXiv:2207.14191 (2022).\n[157] X. Yang, Z. Song, I. King, Z. Xu, A survey on deep\nsemi-supervised learning, IEEE Transactions on Knowl-\nedge and Data Engineering 35 (9) (2023) 8934–8954.\ndoi:10.1109/TKDE.2022.3220219.\n[158] G. Bortsova, F. Dubost, L. Hogeweg, I. Katramados,\nM. De Bruijne, Semi-supervised medical image segmen-\ntation via learning consistency under transformations,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2019: 22nd International Confer-\nence, Shenzhen, China, October 13–17, 2019, Proceed-\nings, Part VI 22, Springer, 2019, pp. 810–818.\n[159] X. Li, L. Yu, H. Chen, C.-W. Fu, L. Xing, P.-A.\nHeng, Transformation-consistent self-ensembling model\nfor semisupervised medical image segmentation, IEEE\nTransactions on Neural Networks and Learning Systems\n32 (2) (2020) 523–534.\n[160] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot,\nA. Oliver, C. A. Raffel, Mixmatch: A holistic approach to\nsemi-supervised learning, Advances in neural information\nprocessing systems 32 (2019).\n[161] H. Basak, R. Bhattacharya, R. Hussain, A. Chatterjee, An\nembarrassingly simple consistency regularization method\nfor semi-supervised medical image segmentation, arXiv\npreprint arXiv:2202.00677 (2022).\n[162] K. Zheng, J. Xu, J. Wei, Double noise mean teacher\nself-ensembling model for semi-supervised tumor seg-\nmentation, in:\nICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE, 2022, pp. 1446–1450.\n[163] Y. Li, L. Luo, H. Lin, H. Chen, P.-A. Heng, Dual-\nconsistency semi-supervised learning with uncertainty\nquantification for covid-19 lesion segmentation from ct\nimages, in:\nMedical Image Computing and Computer\nAssisted Intervention–MICCAI 2021: 24th International\nConference, Strasbourg, France, September 27–October 1,\n2021, Proceedings, Part II 24, Springer, 2021, pp. 199–\n209.\n[164] X. Xu, T. Sanford, B. Turkbey, S. Xu, B. J. Wood, P. Yan,\nShadow-consistent semi-supervised learning for prostate\nultrasound segmentation, IEEE Transactions on Medical\nImaging 41 (6) (2021) 1331–1345.\n[165] Y. Shu, H. Li, B. Xiao, X. Bi, W. Li, Cross-mix monitoring\nfor medical image segmentation with limited supervision,\nIEEE Transactions on Multimedia (2022).\n[166] M. Sajjadi, M. Javanmardi, T. Tasdizen, Regularization\nwith stochastic transformations and perturbations for deep\nsemi-supervised learning, Advances in neural information\nprocessing systems 29 (2016).\n[167] X. Li, L. Yu, H. Chen, C.-W. Fu, P.-A. Heng, Semi-\nsupervised skin lesion segmentation via transforma-\ntion consistent self-ensembling model, arXiv preprint\narXiv:1808.03887 (2018).\n[168] S. Laine,\nT. Aila,\nTemporal ensembling for semi-\nsupervised learning, in:\nInternational Conference on\nLearning Representations, 2016.\n36\n[169] X. Cao, H. Chen, Y. Li, Y. Peng, S. Wang, L. Cheng,\nUncertainty aware temporal-ensembling model for semi-\nsupervised abus mass segmentation, IEEE transactions on\nmedical imaging 40 (1) (2020) 431–443.\n[170] L. Luo, L. Yu, H. Chen, Q. Liu, X. Wang, J. Xu, P.-A.\nHeng, Deep mining external imperfect data for chest x-ray\ndisease screening, IEEE transactions on medical imaging\n39 (11) (2020) 3583–3594.\n[171] A. Tarvainen, H. Valpola, Mean teachers are better role\nmodels:\nWeight-averaged consistency targets improve\nsemi-supervised deep learning results, Advances in neural\ninformation processing systems 30 (2017).\n[172] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng, Uncertainty-\naware self-ensembling model for semi-supervised 3d left\natrium segmentation, in: Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2019: 22nd In-\nternational Conference, Shenzhen, China, October 13–17,\n2019, Proceedings, Part II 22, Springer, 2019, pp. 605–\n613.\n[173] Z. Xu, Y. Wang, D. Lu, X. Luo, J. Yan, Y. Zheng, R. K.-y.\nTong, Ambiguity-selective consistency regularization for\nmean-teacher semi-supervised medical image segmenta-\ntion, Medical Image Analysis 88 (2023) 102880.\n[174] K. Wang, B. Zhan, C. Zu, X. Wu, J. Zhou, L. Zhou,\nY.\nWang,\nTripled-uncertainty\nguided\nmean\nteacher\nmodel for semi-supervised medical image segmentation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Confer-\nence, Strasbourg, France, September 27–October 1, 2021,\nProceedings, Part II 24, Springer, 2021, pp. 450–460.\n[175] J. Zhu, B. Bolsterlee, B. V. Chow, Y. Song, E. Mei-\njering, Hybrid dual mean-teacher network with double-\nuncertainty guidance for semi-supervised segmentation of\nmri scans, arXiv preprint arXiv:2303.05126 (2023).\n[176] C. Xu, Y. Yang, Z. Xia, B. Wang, D. Zhang, Y. Zhang,\nS. Zhao, Dual uncertainty-guided mixing consistency for\nsemi-supervised 3d medical image segmentation, IEEE\nTransactions on Big Data (2023).\n[177] A. Lou, K. Tawfik, X. Yao, Z. Liu, J. Noble, Min-max sim-\nilarity: A contrastive semi-supervised deep learning net-\nwork for surgical tools segmentation, IEEE Transactions\non Medical Imaging (2023).\n[178] S.\nSedai,\nD.\nMahapatra,\nS.\nHewavitharanage,\nS. Maetschke, R. Garnavi, Semi-supervised segmen-\ntation of optic cup in retinal fundus images using\nvariational autoencoder, in: Medical Image Computing\nand Computer-Assisted Intervention- MICCAI 2017:\n20th International Conference, Quebec City, QC, Canada,\nSeptember 11-13, 2017, Proceedings, Part II 20, Springer,\n2017, pp. 75–82.\n[179] H. Wu, G. Chen, Z. Wen, J. Qin, Collaborative and adver-\nsarial learning of focused and dispersive representations\nfor semi-supervised polyp segmentation, in: Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 3489–3498.\n[180] P. Wang, J. Peng, M. Pedersoli, Y. Zhou, C. Zhang,\nC. Desrosiers, Cat:\nConstrained adversarial training\nfor anatomically-plausible semi-supervised segmentation,\nIEEE Transactions on Medical Imaging (2023).\n[181] Y. Zhang, L. Yang, J. Chen, M. Fredericksen, D. P.\nHughes, D. Z. Chen, Deep adversarial networks for\nbiomedical image segmentation utilizing unannotated im-\nages, in: Medical Image Computing and Computer As-\nsisted Intervention- MICCAI 2017:\n20th International\nConference, Quebec City, QC, Canada, September 11-13,\n2017, Proceedings, Part III 20, Springer, 2017, pp. 408–\n416.\n[182] H. Peiris, Z. Chen, G. Egan, M. Harandi, Duo-segnet: ad-\nversarial dual-views for semi-supervised medical image\nsegmentation, in: Medical Image Computing and Com-\nputer Assisted Intervention–MICCAI 2021: 24th Inter-\nnational Conference, Strasbourg, France, September 27–\nOctober 1, 2021, Proceedings, Part II 24, Springer, 2021,\npp. 428–438.\n[183] J. Hou, X. Ding, J. D. Deng, Semi-supervised semantic\nsegmentation of vessel images using leaking perturbations,\nin: Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2022, pp. 2625–2634.\n[184] K. Chaitanya, N. Karani, C. F. Baumgartner, E. Erdil,\nA. Becker, O. Donati, E. Konukoglu, Semi-supervised\ntask-driven data augmentation for medical image segmen-\ntation, Medical Image Analysis 68 (2021) 101934.\n[185] D. P. Kingma, M. Welling, Auto-encoding variational\nbayes, arXiv preprint arXiv:1312.6114 (2013).\n[186] J. Wang, T. Lukasiewicz, Rethinking bayesian deep learn-\ning methods for semi-supervised volumetric medical im-\nage segmentation, in: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\n2022, pp. 182–190.\n[187] H. Jiang, Y. Zhou, Y. Lin, R. C. Chan, J. Liu, H. Chen,\nDeep learning for computational cytology: A survey, Med-\nical Image Analysis (2022) 102691.\n[188] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu,\nJ. Shen, L. Shao, Inf-net: Automatic covid-19 lung infec-\ntion segmentation from ct images, IEEE transactions on\nmedical imaging 39 (8) (2020) 2626–2637.\n[189] D.-H. Lee, et al., Pseudo-label:\nThe simple and effi-\ncient semi-supervised learning method for deep neural net-\nworks, in: Workshop on challenges in representation learn-\ning, ICML, Vol. 3, Atlanta, 2013, p. 896.\n[190] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Ra-\njchl, G. Tarroni, B. Glocker, A. King, P. M. Matthews,\nD. Rueckert, Semi-supervised learning for network-based\ncardiac mr image segmentation, in:\nMedical Image\nComputing and Computer-Assisted Intervention- MICCAI\n2017: 20th International Conference, Quebec City, QC,\nCanada, September 11-13, 2017, Proceedings, Part II 20,\nSpringer, 2017, pp. 253–260.\n37\n[191] G. Wang, S. Zhai, G. Lasio, B. Zhang, B. Yi, S. Chen,\nT. J. Macvittie, D. Metaxas, J. Zhou, S. Zhang, Semi-\nsupervised segmentation of radiation-induced pulmonary\nfibrosis from lung ct scans with multi-scale guided dense\nattention, IEEE transactions on medical imaging 41 (3)\n(2021) 531–542.\n[192] R. Ke, A. I. Aviles-Rivero, S. Pandey, S. Reddy, C.-B.\nSch¨onlieb, A three-stage self-training framework for semi-\nsupervised semantic segmentation, IEEE Transactions on\nImage Processing 31 (2022) 1805–1815.\n[193] F. Liu, Y. Tian, Y. Chen, Y. Liu, V. Belagiannis,\nG. Carneiro, Acpl: Anti-curriculum pseudo-labelling for\nsemi-supervised medical image classification, in: Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 20697–20706.\n[194] D. Chen, Y. Bai, W. Shen, Q. Li, L. Yu, Y. Wang,\nMagicnet: Semi-supervised multi-organ segmentation via\nmagic-cube partition and recovery, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 23869–23878.\n[195] A. Blum, T. Mitchell, Combining labeled and unlabeled\ndata with co-training, in: Proceedings of the eleventh an-\nnual conference on Computational learning theory, 1998,\npp. 92–100.\n[196] L. Zhu, K. Yang, M. Zhang, L. L. Chan, T. K. Ng, B. C.\nOoi, Semi-supervised unpaired multi-modal learning for\nlabel-efficient medical image segmentation, in: Medical\nImage Computing and Computer Assisted Intervention–\nMICCAI 2021: 24th International Conference, Strasbourg,\nFrance, September 27–October 1, 2021, Proceedings, Part\nII 24, Springer, 2021, pp. 394–404.\n[197] X. Chen, H.-Y. Zhou, F. Liu, J. Guo, L. Wang, Y. Yu, Mass:\nModality-collaborative semi-supervised segmentation by\nexploiting cross-modal consistency from unpaired ct and\nmri images, Medical Image Analysis 80 (2022) 102506.\n[198] X. Luo, M. Hu, T. Song, G. Wang, S. Zhang, Semi-\nsupervised medical image segmentation via cross teaching\nbetween cnn and transformer, in: International Conference\non Medical Imaging with Deep Learning, PMLR, 2022,\npp. 820–833.\n[199] J. Peng, G. Estrada, M. Pedersoli, C. Desrosiers, Deep co-\ntraining for semi-supervised image segmentation, Pattern\nRecognition 107 (2020) 107269.\n[200] Z. Zhao, J. Hu, Z. Zeng, X. Yang, P. Qian, B. Veeravalli,\nC. Guan, Mmgl: Multi-scale multi-view global-local con-\ntrastive learning for semi-supervised cardiac image seg-\nmentation, in: 2022 IEEE international conference on im-\nage processing (ICIP), IEEE, 2022, pp. 401–405.\n[201] H.\nWang,\nX.\nLi,\nDhc:\nDual-debiased\nheteroge-\nneous co-training framework for class-imbalanced semi-\nsupervised medical image segmentation, arXiv preprint\narXiv:2307.11960 (2023).\n[202] X. Wang, H. Chen, H. Xiang, H. Lin, X. Lin, P.-A. Heng,\nDeep virtual adversarial self-training with consistency reg-\nularization for semi-supervised medical image classifica-\ntion, Medical image analysis 70 (2021) 102010.\n[203] D. Wang, Y. Zhang, K. Zhang, L. Wang, Focalmix: Semi-\nsupervised learning for 3d medical image detection, in:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 3951–3960.\n[204] W. Zhang, L. Zhu, J. Hallinan, S. Zhang, A. Makmur,\nQ. Cai, B. C. Ooi, Boostmis: Boosting medical image\nsemi-supervised learning with adaptive pseudo labeling\nand informative active annotation, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 20666–20676.\n[205] K. Chaitanya, E. Erdil, N. Karani, E. Konukoglu, Lo-\ncal contrastive loss with pseudo-label based self-training\nfor semi-supervised medical image segmentation, Medical\nImage Analysis 87 (2023) 102792.\n[206] H. Basak, Z. Yin, Pseudo-label guided contrastive learn-\ning for semi-supervised medical image segmentation, in:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023, pp. 19786–19797.\n[207] S. Zhang, J. Zhang, B. Tian, T. Lukasiewicz, Z. Xu, Multi-\nmodal contrastive mutual learning and pseudo-label re-\nlearning for semi-supervised medical image segmentation,\nMedical Image Analysis 83 (2023) 102656.\n[208] L.-L. Zeng, K. Gao, D. Hu, Z. Feng, C. Hou, P. Rong,\nW. Wang, Ss-tbn: A semi-supervised tri-branch network\nfor covid-19 screening and lesion segmentation, IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(2023).\n[209] T. Lei, D. Zhang, X. Du, X. Wang, Y. Wan, A. K. Nandi,\nSemi-supervised medical image segmentation using adver-\nsarial consistency learning and dynamic convolution net-\nwork, IEEE Transactions on Medical Imaging (2022).\n[210] S. Budd, E. C. Robinson, B. Kainz, A survey on active\nlearning and human-in-the-loop deep learning for med-\nical image analysis, Medical Image Analysis 71 (2021)\n102062.\n[211] J. Peng, Y. Wang, Medical image segmentation with lim-\nited supervision: A review of deep network models, IEEE\nAccess 9 (2021) 36827–36851. doi:10.1109/ACCESS.\n2021.3062380.\n[212] J. Liu, L. Cao, Y. Tian, Deep active learning for effective\npulmonary nodule detection, in: Medical Image Comput-\ning and Computer Assisted Intervention–MICCAI 2020:\n23rd International Conference, Lima, Peru, October 4–8,\n2020, Proceedings, Part VI 23, Springer, 2020, pp. 609–\n618.\n[213] S. Wen, T. M. Kurc, L. Hou, J. H. Saltz, R. R. Gupta,\nR. Batiste, T. Zhao, V. Nguyen, D. Samaras, W. Zhu,\nComparison of different classifiers with active learning to\nsupport quality control in nucleus segmentation in pathol-\nogy images, AMIA Summits on Translational Science Pro-\nceedings 2018 (2018) 227.\n38\n[214] X. Wu, C. Chen, M. Zhong, J. Wang, J. Shi, Covid-al: The\ndiagnosis of covid-19 with deep active learning, Medical\nImage Analysis 68 (2021) 101913.\n[215] Z. Zhou, J. Y. Shin, S. R. Gurudu, M. B. Gotway, J. Liang,\nActive, continual fine tuning of convolutional neural net-\nworks for reducing annotation efforts, Medical image anal-\nysis 71 (2021) 101997.\n[216] S. Balaram,\nC. M. Nguyen,\nA. Kassim,\nP. Krish-\nnaswamy, Consistency-based semi-supervised evidential\nactive learning for diagnostic radiograph classification, in:\nInternational Conference on Medical Image Computing\nand Computer-Assisted Intervention, Springer, 2022, pp.\n675–685.\n[217] W. Kuo, C. H¨ane, E. Yuh, P. Mukherjee, J. Malik, Cost-\nsensitive active learning for intracranial hemorrhage de-\ntection, in:\nMedical Image Computing and Computer\nAssisted Intervention–MICCAI 2018: 21st International\nConference, Granada, Spain, September 16-20, 2018, Pro-\nceedings, Part III 11, Springer, 2018, pp. 715–723.\n[218] W. H. Beluch, T. Genewein, A. N¨urnberger, J. M. K¨ohler,\nThe power of ensembles for active learning in image clas-\nsification, in: Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2018, pp. 9368–9377.\n[219] A. Atzeni, L. Peter, E. Robinson, E. Blackburn, J. Al-\nthonayan, D. C. Alexander, J. E. Iglesias, Deep active\nlearning for suggestive segmentation of biomedical image\nstacks via optimisation of dice scores and traced boundary\nlength, Medical Image Analysis 81 (2022) 102549.\n[220] Y. Gal, R. Islam, Z. Ghahramani, Deep bayesian active\nlearning with image data, in: International conference on\nmachine learning, PMLR, 2017, pp. 1183–1192.\n[221] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, M. Reyes, Ef-\nficient active learning for image classification and segmen-\ntation using a sample selection and conditional generative\nadversarial network, in: International Conference on Med-\nical Image Computing and Computer-Assisted Interven-\ntion, Springer, 2018, pp. 580–588.\n[222] C. Dai, S. Wang, Y. Mo, K. Zhou, E. Angelini, Y. Guo,\nW. Bai, Suggestive annotation of brain tumour images with\ngradient-guided sampling, in: Medical Image Computing\nand Computer Assisted Intervention–MICCAI 2020: 23rd\nInternational Conference, Lima, Peru, October 4–8, 2020,\nProceedings, Part IV 23, Springer, 2020, pp. 156–165.\n[223] V. Nath, D. Yang, H. R. Roth, D. Xu, Warm start ac-\ntive learning with proxy labels and selection via semi-\nsupervised fine-tuning, in: International Conference on\nMedical Image Computing and Computer-Assisted Inter-\nvention, Springer, 2022, pp. 297–308.\n[224] X. Li, M. Xia, J. Jiao, S. Zhou, C. Chang, Y. Wang, Y. Guo,\nHal-ia: A hybrid active learning framework using interac-\ntive annotation for medical image segmentation, Medical\nImage Analysis (2023) 102862.\n[225] L. Yang, Y. Zhang, J. Chen, S. Zhang, D. Z. Chen, Sug-\ngestive annotation:\nA deep active learning framework\nfor biomedical image segmentation, in: Medical Image\nComputing and Computer Assisted Intervention- MICCAI\n2017: 20th International Conference, Quebec City, QC,\nCanada, September 11-13, 2017, Proceedings, Part III 20,\nSpringer, 2017, pp. 399–407.\n[226] F. Ozdemir, Z. Peng, P. Fuernstahl, C. Tanner, O. Gok-\nsel, Active learning for segmentation based on bayesian\nsample queries, Knowledge-Based Systems 214 (2021)\n106531.\n[227] S. Zhao,\nJ. Song,\nS. Ermon,\nInfovae:\nInforma-\ntion maximizing variational autoencoders, arXiv preprint\narXiv:1706.02262 (2017).\n[228] W. Li, J. Li, Z. Wang, J. Polson, A. E. Sisk, D. P. Sajed,\nW. Speier, C. W. Arnold, Pathal: An active learning frame-\nwork for histopathology image analysis, IEEE Transac-\ntions on Medical Imaging 41 (5) (2021) 1176–1187.\n[229] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R.\nScott, D. Huang, Curriculumnet: Weakly supervised learn-\ning from large-scale web images, in: Proceedings of the\nEuropean conference on computer vision (ECCV), 2018,\npp. 135–150.\n[230] Z. Wang, Z. Yin, Annotation-efficient cell counting,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Confer-\nence, Strasbourg, France, September 27–October 1, 2021,\nProceedings, Part VIII 24, Springer, 2021, pp. 405–414.\n[231] S.\nKumari,\nP.\nSingh,\nDeep\nlearning\nfor\nunsuper-\nvised domain adaptation in medical imaging:\nRecent\nadvancements and future perspectives, arXiv preprint\narXiv:2308.01265 (2023).\n[232] M. Yu, H. Guan, Y. Fang, L. Yue, M. Liu, Domain-prior-\ninduced structural mri adaptation for clinical progression\nprediction of subjective cognitive decline, in: International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, Springer, 2022, pp. 24–33.\n[233] Y. Fang, M. Wang, G. G. Potter, M. Liu, Unsuper-\nvised cross-domain functional mri adaptation for auto-\nmated major depressive disorder identification, Medical\nImage Analysis 84 (2023) 102707.\n[234] Q. Hu, H. Li, J. Zhang, Domain-adaptive 3d medical image\nsynthesis: An efficient unsupervised approach, in: Medical\nImage Computing and Computer Assisted Intervention–\nMICCAI 2022: 25th International Conference, Singapore,\nSeptember 18–22, 2022, Proceedings, Part VI, Springer,\n2022, pp. 495–504.\n[235] M. Sahu, R. Str¨omsd¨orfer, A. Mukhopadhyay, S. Za-\nchow, Endo-sim2real: Consistency learning-based domain\nadaptation for instrument segmentation, in: International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, Springer, 2020, pp. 784–794.\n39\n[236] A. Gomariz, H. Lu, Y. Y. Li, T. Albrecht, A. Maunz,\nF. Benmansour, A. M. Valcarcel, J. Luu, D. Ferrara,\nO. Goksel, Unsupervised domain adaptation with con-\ntrastive learning for oct segmentation, in: International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, Springer, 2022, pp. 351–361.\n[237] F. Wu, X. Zhuang, Cf distance: a new domain discrepancy\nmetric and application to explicit domain adaptation for\ncross-modality cardiac image segmentation, IEEE Trans-\nactions on Medical Imaging 39 (12) (2020) 4274–4285.\n[238] Y.\nGanin,\nE.\nUstinova,\nH.\nAjakan,\nP.\nGermain,\nH. Larochelle, F. Laviolette, M. Marchand, V. Lempitsky,\nDomain-adversarial training of neural networks, The jour-\nnal of machine learning research 17 (1) (2016) 2096–2030.\n[239] J. Ren, I. Hacihaliloglu, E. A. Singer, D. J. Foran,\nX. Qi, Adversarial domain adaptation for classification of\nprostate histopathology whole-slide images, in: Medical\nImage Computing and Computer Assisted Intervention–\nMICCAI 2018: 21st International Conference, Granada,\nSpain, September 16-20, 2018, Proceedings, Part II 11,\nSpringer, 2018, pp. 201–209.\n[240] Y. Zhang, H. Chen, Y. Wei, P. Zhao, J. Cao, X. Fan, X. Lou,\nH. Liu, J. Hou, X. Han, et al., From whole slide imag-\ning to microscopy: Deep microscopy adaptation network\nfor histopathology cancer image classification, in: Inter-\nnational Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2019, pp. 360–\n368.\n[241] Y. Feng, Z. Wang, X. Xu, Y. Wang, H. Fu, S. Li, L. Zhen,\nX. Lei, Y. Cui, J. S. Z. Ting, et al., Contrastive domain\nadaptation with consistency match for automated pneumo-\nnia diagnosis, Medical Image Analysis 83 (2023) 102664.\n[242] C. Bian, C. Yuan, J. Wang, M. Li, X. Yang, S. Yu,\nK. Ma, J. Yuan, Y. Zheng, Uncertainty-aware domain\nalignment for anatomical structure segmentation, Medical\nImage Analysis 64 (2020) 101732.\n[243] L. Liu, Z. Zhang, S. Li, K. Ma, Y. Zheng, S-cuda: Self-\ncleansing unsupervised domain adaptation for medical\nimage segmentation, Medical Image Analysis 74 (2021)\n102214.\n[244] W. Huang, X. Liu, Z. Cheng, Y. Zhang, Z. Xiong, Do-\nmain adaptive mitochondria segmentation via enforcing\ninter-section consistency, in: International Conference on\nMedical Image Computing and Computer-Assisted Inter-\nvention, Springer, 2022, pp. 89–98.\n[245] M. A. Karaoglu, N. Brasch, M. Stollenga, W. Wein,\nN. Navab, F. Tombari, A. Ladikos, Adversarial domain\nfeature adaptation for bronchoscopic depth estimation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Confer-\nence, Strasbourg, France, September 27–October 1, 2021,\nProceedings, Part IV 24, Springer, 2021, pp. 300–310.\n[246] H. Hao, C. Xu, D. Zhang, Q. Yan, J. Zhang, Y. Liu,\nY. Zhao, Sparse-based domain adaptation network for octa\nimage super-resolution reconstruction, IEEE Journal of\nBiomedical and Health Informatics 26 (9) (2022) 4402–\n4413.\n[247] C. Yoo, H. W. Lee, J.-W. Kang, Transferring structured\nknowledge in unsupervised domain adaptation of a sleep\nstaging network, IEEE journal of biomedical and health\ninformatics 26 (3) (2021) 1273–1284.\n[248] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-\nto-image translation using cycle-consistent adversarial net-\nworks, in: Proceedings of the IEEE international confer-\nence on computer vision, 2017, pp. 2223–2232.\n[249] Y. Tang, Y. Tang, V. Sandfort, J. Xiao, R. M. Sum-\nmers, Tuna-net: Task-oriented unsupervised adversarial\nnetwork for disease recognition in cross-domain chest x-\nrays, in: Medical Image Computing and Computer As-\nsisted Intervention–MICCAI 2019:\n22nd International\nConference, Shenzhen, China, October 13–17, 2019, Pro-\nceedings, Part VI 22, Springer, 2019, pp. 431–440.\n[250] J. Jiang, Y.-C. Hu, N. Tyagi, P. Zhang, A. Rimner, G. S.\nMageras, J. O. Deasy, H. Veeraraghavan, Tumor-aware,\nadversarial domain adaptation from ct to mri for lung\ncancer segmentation, in: Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2018: 21st In-\nternational Conference, Granada, Spain, September 16-20,\n2018, Proceedings, Part II 11, Springer, 2018, pp. 777–\n785.\n[251] J. Jiang, Y.-C. Hu, N. Tyagi, A. Rimner, N. Lee, J. O.\nDeasy, S. Berry, H. Veeraraghavan, Psigan: Joint proba-\nbilistic segmentation and image distribution matching for\nunpaired cross-modality adaptation-based mri segmenta-\ntion, IEEE transactions on medical imaging 39 (12) (2020)\n4071–4084.\n[252] D. Tomar, M. Lortkipanidze, G. Vray, B. Bozorgtabar, J.-\nP. Thiran, Self-attentive spatial adaptive normalization for\ncross-modality domain adaptation, IEEE Transactions on\nMedical Imaging 40 (10) (2021) 2926–2938.\n[253] A. Kapil, A. Meier, K. Steele, M. Rebelatto, K. Nekolla,\nA. Haragan, A. Silva, A. Zuraw, C. Barker, M. L. Scott,\net al., Domain adaptation-based deep learning for auto-\nmated tumor cell (tc) scoring and survival analysis on pd-\nl1 stained tissue images, IEEE Transactions on Medical\nImaging 40 (9) (2021) 2513–2523.\n[254] F. Xing, T. Bennett, D. Ghosh, Adversarial domain adap-\ntation and pseudo-labeling for cross-modality microscopy\nimage quantification, in: Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2019: 22nd In-\nternational Conference, Shenzhen, China, October 13–17,\n2019, Proceedings, Part I 22, Springer, 2019, pp. 740–749.\n[255] F. Xing, T. C. Cornish, T. D. Bennett, D. Ghosh, Bidirec-\ntional mapping-based domain adaptation for nucleus de-\ntection in cross-modality microscopy images, IEEE Trans-\nactions on Medical Imaging 40 (10) (2020) 2880–2896.\n[256] F. Xing, T. C. Cornish, Low-resource adversarial domain\nadaptation for cross-modality nucleus detection, in: In-\nternational Conference on Medical Image Computing and\n40\nComputer-Assisted Intervention, Springer, 2022, pp. 639–\n649.\n[257] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, C. C. Loy, Domain\ngeneralization: A survey, IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2022).\n[258] J. Yang, N. C. Dvornek, F. Zhang, J. Chapiro, M. Lin, J. S.\nDuncan, Unsupervised domain adaptation via disentangled\nrepresentations: Application to cross-modality liver seg-\nmentation, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2019: 22nd International\nConference, Shenzhen, China, October 13–17, 2019, Pro-\nceedings, Part II 22, Springer, 2019, pp. 255–263.\n[259] R. Wang, G. Zheng, Cycmis:\nCycle-consistent cross-\ndomain medical image segmentation via diverse image\naugmentation, Medical Image Analysis 76 (2022) 102328.\n[260] X. Sun, Z. Liu, S. Zheng, C. Lin, Z. Zhu, Y. Zhao,\nAttention-enhanced disentangled representation learning\nfor unsupervised domain adaptation in cardiac segmen-\ntation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer,\n2022, pp. 745–754.\n[261] Q. Xie, Y. Li, N. He, M. Ning, K. Ma, G. Wang, Y. Lian,\nY. Zheng, Unsupervised domain adaptation for medical\nimage segmentation by disentanglement learning and self-\ntraining, IEEE Transactions on Medical Imaging (2022).\n[262] C. Yang, X. Guo, M. Zhu, B. Ibragimov, Y. Yuan, Mutual-\nprototype adaptation for cross-domain polyp segmenta-\ntion, IEEE Journal of Biomedical and Health Informatics\n25 (10) (2021) 3886–3897.\n[263] S. Wu, C. Chen, Z. Xiong, X. Chen, X. Sun, Uncertainty-\naware label rectification for domain adaptive mitochondria\nsegmentation, in: Medical Image Computing and Com-\nputer Assisted Intervention–MICCAI 2021: 24th Inter-\nnational Conference, Strasbourg, France, September 27–\nOctober 1, 2021, Proceedings, Part III 24, Springer, 2021,\npp. 191–200.\n[264] F. Gr¨oger, A.-M. Rickmann, C. Wachinger, Strudel:\nSelf-training with uncertainty dependent label refinement\nacross domains, in: Machine Learning in Medical Imag-\ning:\n12th International Workshop, MLMI 2021, Held\nin Conjunction with MICCAI 2021, Strasbourg, France,\nSeptember 27, 2021, Proceedings 12, Springer, 2021, pp.\n306–316.\n[265] Q. Qi, X. Lin, C. Chen, W. Xie, Y. Huang, X. Ding, X. Liu,\nY. Yu, Curriculum feature alignment domain adaptation for\nepithelium-stroma classification in histopathological im-\nages, IEEE Journal of Biomedical and Health Informatics\n25 (4) (2020) 1163–1172.\n[266] H. Cho, K. Nishimura, K. Watanabe, R. Bise, Effective\npseudo-labeling based on heatmap for unsupervised do-\nmain adaptation in cell detection, Medical Image Analysis\n79 (2022) 102436.\n[267] A. Mottaghi, A. Sharghi, S. Yeung, O. Mohareri, Adapta-\ntion of surgical activity recognition models across operat-\ning rooms, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer,\n2022, pp. 530–540.\n[268] Y. Sun, E. Tzeng, T. Darrell, A. A. Efros, Unsupervised\ndomain adaptation through self-supervision, arXiv preprint\narXiv:1909.11825 (2019).\n[269] N. A. Koohbanani, B. Unnikrishnan, S. A. Khurram, P. Kr-\nishnaswamy, N. Rajpoot, Self-path: Self-supervision for\nclassification of pathology images with limited annota-\ntions, IEEE Transactions on Medical Imaging 40 (10)\n(2021) 2845–2856.\n[270] C. Abbet, L. Studer, A. Fischer, H. Dawson, I. Zlobec,\nB. Bozorgtabar, J.-P. Thiran, Self-rule to multi-adapt: Gen-\neralized multi-source feature learning using unsupervised\ndomain adaptation for colorectal cancer tissue detection,\nMedical image analysis 79 (2022) 102473.\n[271] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman,\nA. Yuille, Domain adaptive relational reasoning for 3d\nmulti-organ segmentation, in: Medical Image Computing\nand Computer Assisted Intervention–MICCAI 2020: 23rd\nInternational Conference, Lima, Peru, October 4–8, 2020,\nProceedings, Part I 23, Springer, 2020, pp. 656–666.\n[272] Y. Xue, S. Feng, Y. Zhang, X. Zhang, Y. Wang, Dual-task\nself-supervision for cross-modality domain adaptation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2020: 23rd International Confer-\nence, Lima, Peru, October 4–8, 2020, Proceedings, Part\nI 23, Springer, 2020, pp. 408–417.\n[273] C. Chen, Q. Dou, H. Chen, J. Qin, P. A. Heng, Unsuper-\nvised bidirectional cross-modality adaptation via deeply\nsynergistic image and feature alignment for medical im-\nage segmentation, IEEE transactions on medical imaging\n39 (7) (2020) 2494–2505.\n[274] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola,\nK. Saenko, A. Efros, T. Darrell, Cycada: Cycle-consistent\nadversarial domain adaptation, in: International confer-\nence on machine learning, Pmlr, 2018, pp. 1989–1998.\n[275] X. Jia, S. Wang, X. Liang, A. Balagopal, D. Nguyen,\nM. Yang, Z. Wang, J. X. Ji, X. Qian, S. Jiang, Cone-beam\ncomputed tomography (cbct) segmentation by adversarial\nlearning domain adaptation, in: Medical Image Comput-\ning and Computer Assisted Intervention–MICCAI 2019:\n22nd International Conference, Shenzhen, China, October\n13–17, 2019, Proceedings, Part VI 22, Springer, 2019, pp.\n567–575.\n[276] D. Liu, D. Zhang, Y. Song, F. Zhang, L. ODonnell,\nH. Huang, M. Chen, W. Cai, Pdam: A panoptic-level fea-\nture alignment framework for unsupervised domain adap-\ntive instance segmentation in microscopy images, IEEE\nTransactions on Medical Imaging 40 (1) (2020) 154–165.\n41\n[277] H. Cui, C. Yuwen, L. Jiang, Y. Xia, Y. Zhang, Bidirectional\ncross-modality unsupervised domain adaptation using gen-\nerative adversarial networks for cardiac image segmen-\ntation, Computers in Biology and Medicine 136 (2021)\n104726.\n[278] X. Chen, T. Kuang, H. Deng, S. H. Fung, J. Gateno,\nJ. J. Xia, P.-T. Yap, Dual adversarial attention mechanism\nfor unsupervised domain adaptive medical image segmen-\ntation, IEEE Transactions on Medical Imaging 41 (11)\n(2022) 3445–3453.\n[279] Z. Zhao, F. Zhou, K. Xu, Z. Zeng, C. Guan, S. K. Zhou, Le-\nuda: Label-efficient unsupervised domain adaptation for\nmedical image segmentation, IEEE transactions on medi-\ncal imaging 42 (3) (2023) 633–646.\n[280] X. Li, S. Niu, X. Gao, X. Zhou, J. Dong, H. Zhao, Self-\ntraining adversarial learning for cross-domain retinal oct\nfluid segmentation, Computers in Biology and Medicine\n155 (2023) 106650.\n[281] D. Karimi, H. Dou, S. K. Warfield, A. Gholipour, Deep\nlearning with noisy labels:\nExploring techniques and\nremedies in medical image analysis, Medical image anal-\nysis 65 (2020) 101759.\n[282] A. Ghosh, H. Kumar, P. S. Sastry, Robust loss functions\nunder label noise for deep neural networks, in: Proceed-\nings of the AAAI conference on artificial intelligence,\nVol. 31, 2017.\n[283] Z. Zhang, M. Sabuncu, Generalized cross entropy loss for\ntraining deep neural networks with noisy labels, Advances\nin neural information processing systems 31 (2018).\n[284] D. J. Matuszewski, I.-M. Sintorn, Minimal annotation\ntraining for segmentation of microscopy images, in: 2018\nIEEE 15th International Symposium on Biomedical Imag-\ning (ISBI 2018), IEEE, 2018, pp. 387–390.\n[285] G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng,\nK. Li, N. Huang, S. Zhang, A noise-robust framework\nfor automatic segmentation of covid-19 pneumonia lesions\nfrom ct images, IEEE Transactions on Medical Imaging\n39 (8) (2020) 2653–2663.\n[286] H. Chen, W. Tan, J. Li, P. Guan, L. Wu, B. Yan, J. Li,\nY. Wang, Adaptive cross entropy for ultrasmall object de-\ntection in computed tomography with noisy labels, Com-\nputers in Biology and Medicine 147 (2022) 105763.\n[287] X. Guo, Y. Yuan, Joint class-affinity loss correction for ro-\nbust medical image segmentation with noisy labels, in: In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2022, pp. 588–\n598.\n[288] H. Le, D. Samaras, T. Kurc, R. Gupta, K. Shroyer, J. Saltz,\nPancreatic cancer detection in whole slide images using\nnoisy label annotations, in: Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2019: 22nd In-\nternational Conference, Shenzhen, China, October 13–17,\n2019, Proceedings, Part I 22, Springer, 2019, pp. 541–549.\n[289] Z. Mirikharaji, Y. Yan, G. Hamarneh, Learning to seg-\nment skin lesions from noisy annotations, in: Domain\nAdaptation and Representation Transfer and Medical Im-\nage Learning with Less Labels and Imperfect Data: First\nMICCAI Workshop, DART 2019, and First International\nWorkshop, MIL3ID 2019, Shenzhen, Held in Conjunction\nwith MICCAI 2019, Shenzhen, China, October 13 and 17,\n2019, Proceedings 1, Springer, 2019, pp. 207–215.\n[290] T. Zhang, L. Yu, N. Hu, S. Lv, S. Gu, Robust medical\nimage segmentation from non-expert annotations with tri-\nnetwork, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2020: 23rd International\nConference, Lima, Peru, October 4–8, 2020, Proceedings,\nPart IV 23, Springer, 2020, pp. 249–258.\n[291] J. Wang, S. Zhou, C. Fang, L. Wang, J. Wang, Meta\ncorrupted pixels mining for medical image segmentation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2020: 23rd International Confer-\nence, Lima, Peru, October 4–8, 2020, Proceedings, Part\nI 23, Springer, 2020, pp. 335–345.\n[292] S. Min, X. Chen, Z.-J. Zha, F. Wu, Y. Zhang, A two-stream\nmutual attention network for semi-supervised biomedical\nsegmentation with noisy labels, in: Proceedings of the\nAAAI Conference on Artificial Intelligence, Vol. 33, 2019,\npp. 4578–4585.\n[293] E. Malach, S. Shalev-Shwartz, Decoupling” when to up-\ndate” from” how to update”, Advances in neural informa-\ntion processing systems 30 (2017).\n[294] C. Xue, L. Yu, P. Chen, Q. Dou, P.-A. Heng, Robust medi-\ncal image classification from noisy labeled data with global\nand local representation guided co-training, IEEE Transac-\ntions on Medical Imaging 41 (6) (2022) 1371–1382.\n[295] S. Yang, G. Wang, H. Sun, X. Luo, P. Sun, K. Li, Q. Wang,\nS. Zhang, Learning covid-19 pneumonia lesion segmenta-\ntion from imperfect annotations via divergence-aware se-\nlective training, IEEE Journal of Biomedical and Health\nInformatics 26 (8) (2022) 3673–3684.\n[296] X. Li, Y. Wei, Q. Hu, C. Wang, J. Yang, Learning to seg-\nment subcortical structures from noisy annotations with\na novel uncertainty-reliability aware learning framework,\nComputers in Biology and Medicine 151 (2022) 106326.\n[297] Z. Xu, D. Lu, Y. Wang, J. Luo, J. Jayender, K. Ma,\nY. Zheng, X. Li, Noisy labels are treasure: mean-teacher-\nassisted confident learning for hepatic vessel segmen-\ntation, in:\nMedical Image Computing and Computer\nAssisted Intervention–MICCAI 2021: 24th International\nConference, Strasbourg, France, September 27–October 1,\n2021, Proceedings, Part I 24, Springer, 2021, pp. 3–13.\n[298] Z. Xu, D. Lu, J. Luo, Y. Wang, J. Yan, K. Ma, Y. Zheng,\nR. K.-Y. Tong, Anti-interference from noisy labels: Mean-\nteacher-assisted confident learning for medical image seg-\nmentation, IEEE Transactions on Medical Imaging 41 (11)\n(2022) 3062–3073.\n42\n[299] J. Shi, J. Wu, Distilling effective supervision for robust\nmedical image segmentation with noisy labels, in: Medical\nImage Computing and Computer Assisted Intervention–\nMICCAI 2021: 24th International Conference, Strasbourg,\nFrance, September 27–October 1, 2021, Proceedings, Part\nI 24, Springer, 2021, pp. 668–677.\n[300] F. Garcea, A. Serra, F. Lamberti, L. Morra, Data augmen-\ntation for medical imaging: A systematic literature review,\nComputers in Biology and Medicine (2022) 106391.\n[301] J. Nalepa, M. Marcinkiewicz, M. Kawulok, Data augmen-\ntation for brain-tumor segmentation: a review, Frontiers in\ncomputational neuroscience 13 (2019) 83.\n[302] S. Pereira, A. Pinto, V. Alves, C. A. Silva, Brain tumor seg-\nmentation using convolutional neural networks in mri im-\nages, IEEE transactions on medical imaging 35 (5) (2016)\n1240–1251.\n[303] Y. Liu, S. Stojadinovic, B. Hrycushko, Z. Wardak, S. Lau,\nW. Lu, Y. Yan, S. B. Jiang, X. Zhen, R. Timmerman, et al.,\nA deep convolutional neural network-based automatic de-\nlineation strategy for multiple brain metastases stereotactic\nradiosurgery, PloS one 12 (10) (2017) e0185844.\n[304] A. Galdran, A. Alvarez-Gila, M. I. Meyer, C. L. Saratx-\naga, T. Ara´ujo, E. Garrote, G. Aresta, P. Costa, A. M.\nMendoncca, A. Campilho, Data-driven color augmentation\ntechniques for deep skin image analysis, arXiv preprint\narXiv:1703.03702 (2017).\n[305] A. Buslaev, V. I. Iglovikov, E. Khvedchenya, A. Parinov,\nM. Druzhinin, A. A. Kalinin, Albumentations: fast and\nflexible image augmentations, Information 11 (2) (2020)\n125.\n[306] F. P´erez-Garc´ıa, R. Sparks, S. Ourselin, Torchio: a python\nlibrary for efficient loading, preprocessing, augmentation\nand patch-based sampling of medical images in deep learn-\ning, Computer Methods and Programs in Biomedicine 208\n(2021) 106236.\n[307] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Gener-\native adversarial nets, Advances in neural information pro-\ncessing systems 27 (2014).\n[308] M. Mirza, S. Osindero, Conditional generative adversarial\nnets, arXiv preprint arXiv:1411.1784 (2014).\n[309] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-image\ntranslation with conditional adversarial networks, in: Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 2017, pp. 1125–1134.\n[310] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, Semantic image\nsynthesis with spatially-adaptive normalization, in: Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 2019, pp. 2337–2346.\n[311] A. Radford, L. Metz, S. Chintala, Unsupervised represen-\ntation learning with deep convolutional generative adver-\nsarial networks, arXiv preprint arXiv:1511.06434 (2015).\n[312] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen,\nT. Aila, Analyzing and improving the image quality of\nstylegan, in: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2020, pp. 8110–\n8119.\n[313] T. Karras, T. Aila, S. Laine, J. Lehtinen, Progressive grow-\ning of gans for improved quality, stability, and variation,\nin: International Conference on Learning Representations,\n2018.\n[314] A. Bissoto, E. Valle, S. Avila, Gan-based data augmenta-\ntion and anonymization for skin-lesion analysis: A critical\nreview, in: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2021, pp. 1847–\n1856.\n[315] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger,\nH. Greenspan, Synthetic data augmentation using gan for\nimproved liver lesion classification, in: 2018 IEEE 15th\ninternational symposium on biomedical imaging (ISBI\n2018), IEEE, 2018, pp. 289–293.\n[316] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Gold-\nberger, H. Greenspan, Gan-based synthetic medical image\naugmentation for increased cnn performance in liver lesion\nclassification, Neurocomputing 321 (2018) 321–331.\n[317] H. Rashid, M. A. Tanveer, H. A. Khan, Skin lesion classi-\nfication using gan based data augmentation, in: 2019 41St\nannual international conference of the IEEE engineering\nin medicine and biology society (EMBC), IEEE, 2019, pp.\n916–919.\n[318] A. Waheed, M. Goyal, D. Gupta, A. Khanna, F. Al-\nTurjman, P. R. Pinheiro, Covidgan: data augmentation us-\ning auxiliary classifier gan for improved covid-19 detec-\ntion, Ieee Access 8 (2020) 91916–91923.\n[319] H. Zhao, H. Li, S. Maurer-Stroh, L. Cheng, Synthesiz-\ning retinal and neuronal images with generative adversarial\nnets, Medical image analysis 49 (2018) 14–26.\n[320] M. Nishio, C. Muramatsu, S. Noguchi, H. Nakai, K. Fuji-\nmoto, R. Sakamoto, H. Fujita, Attribute-guided image gen-\neration of three-dimensional computed tomography im-\nages of lung nodules using a generative adversarial net-\nwork, Computers in Biology and Medicine 126 (2020)\n104032.\n[321] K. Chen, Y. Guo, C. Yang, Y. Xu, R. Zhang, C. Li,\nR. Wu, Enhanced breast lesion classification via knowl-\nedge guided cross-modal and semantic data augmentation,\nin: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Confer-\nence, Strasbourg, France, September 27–October 1, 2021,\nProceedings, Part V 24, Springer, 2021, pp. 53–63.\n[322] C. Muramatsu, M. Nishio, T. Goto, M. Oiwa, T. Morita,\nM. Yakami, T. Kubo, K. Togashi, H. Fujita, Improv-\ning breast mass classification by shared data with do-\nmain transformation using a generative adversarial net-\nwork, Computers in biology and medicine 119 (2020)\n103698.\n43\n[323] P. Guo, P. Wang, J. Zhou, V. M. Patel, S. Jiang, Le-\nsion mask-based simultaneous synthesis of anatomic and\nmolecular mr images using a gan, in: Medical Image\nComputing and Computer Assisted Intervention–MICCAI\n2020: 23rd International Conference, Lima, Peru, Octo-\nber 4–8, 2020, Proceedings, Part II 23, Springer, 2020, pp.\n104–113.\n[324] M. Rezaei, K. Harmuth, W. Gierke, T. Kellermeier, M. Fis-\ncher, H. Yang, C. Meinel, A conditional adversarial net-\nwork for semantic segmentation of brain tumor, in: Brain-\nlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\nBrain Injuries: Third International Workshop, BrainLes\n2017, Held in Conjunction with MICCAI 2017, Quebec\nCity, QC, Canada, September 14, 2017, Revised Selected\nPapers 3, Springer, 2018, pp. 241–252.\n[325] Y. Chen, X.-H. Yang, Z. Wei, A. A. Heidari, N. Zheng,\nZ. Li, H. Chen, H. Hu, Q. Zhou, Q. Guan, Generative\nadversarial networks in medical image augmentation: A\nreview, Computers in Biology and Medicine 144 (2022)\n105382.\n[326] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup:\nBeyond empirical risk minimization, in:\nInternational\nConference on Learning Representations, 2018.\n[327] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix:\nRegularization strategy to train strong classifiers with lo-\ncalizable features, in: Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, 2019, pp. 6023–\n6032.\n[328] J. Yang, Y. Zhang, Y. Liang, Y. Zhang, L. He, Z. He,\nTumorcp: A simple but effective object-level data aug-\nmentation for tumor segmentation, in:\nMedical Image\nComputing and Computer Assisted Intervention–MICCAI\n2021: 24th International Conference, Strasbourg, France,\nSeptember 27–October 1, 2021, Proceedings, Part I 24,\nSpringer, 2021, pp. 579–588.\n[329] Y. Lin, Z. Wang, K.-T. Cheng, H. Chen, Insmix: To-\nwards realistic generative data augmentation for nuclei in-\nstance segmentation, in: International Conference on Med-\nical Image Computing and Computer-Assisted Interven-\ntion, Springer, 2022, pp. 140–149.\n[330] Q. Zhu, Y. Wang, L. Yin, J. Yang, F. Liao, S. Li, Selfmix:\na self-adaptive data augmentation method for lesion seg-\nmentation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer,\n2022, pp. 683–692.\n[331] X. Zhang, C. Liu, N. Ou, X. Zeng, Z. Zhuo, Y. Duan,\nX. Xiong, Y. Yu, Z. Liu, Y. Liu, et al., Carvemix: a simple\ndata augmentation method for brain lesion segmentation,\nNeuroImage 271 (2023) 120041.\n[332] Y. Wang, Y. Ji, H. Xiao, A data augmentation method for\nfully automatic brain tumor segmentation, Computers in\nBiology and Medicine 149 (2022) 106039.\n[333] M. Salem, S. Valverde, M. Cabezas, D. Pareto, A. Oliver,\nJ. Salvi, `A. Rovira, X. Llad´o, Multiple sclerosis lesion syn-\nthesis in mri using an encoder-decoder u-net, IEEE Access\n7 (2019) 25171–25184.\n[334] K. H. Cha, N. Petrick, A. Pezeshk, C. G. Graff, D. Sharma,\nA. Badal, B. Sahiner, Evaluation of data augmentation\nvia synthetic images for improved breast mass detection\non mammograms using deep learning, Journal of Medical\nImaging 7 (1) (2020) 012703–012703.\n[335] Y. Sun, P. Yuan, Y. Sun, Mm-gan: 3d mri data augmenta-\ntion for medical image segmentation via generative adver-\nsarial networks, in: 2020 IEEE International conference on\nknowledge graph (ICKG), IEEE, 2020, pp. 227–234.\n[336] Y. Onishi, A. Teramoto, M. Tsujimoto, T. Tsukamoto,\nK. Saito, H. Toyama, K. Imaizumi, H. Fujita, Multiplanar\nanalysis for pulmonary nodule classification in ct images\nusing deep convolutional neural network and generative\nadversarial networks, International journal of computer as-\nsisted radiology and surgery 15 (2020) 173–178.\n[337] Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf,\nW. Yang, D. Li, Z. Fan, Fully automated multiorgan seg-\nmentation in abdominal magnetic resonance imaging with\ndeep neural networks, Medical physics 47 (10) (2020)\n4971–4982.\n[338] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le,\nAutoaugment: Learning augmentation policies from data,\narXiv preprint arXiv:1805.09501 (2018).\n[339] C. Chen, C. Qin, C. Ouyang, Z. Li, S. Wang, H. Qiu,\nL. Chen, G. Tarroni, W. Bai, D. Rueckert, Enhancing mr\nimage segmentation with realistic adversarial data aug-\nmentation, Medical Image Analysis 82 (2022) 102597.\n[340] K. Fujita, M. Kobayashi, T. Nagao, Data augmentation us-\ning evolutionary image processing, in: 2018 Digital Image\nComputing: Techniques and Applications (DICTA), IEEE,\n2018, pp. 1–6.\n[341] Y. Wang, Q. Yao, J. T. Kwok, L. M. Ni, Generalizing from\na few examples: A survey on few-shot learning, ACM\ncomputing surveys (csur) 53 (3) (2020) 1–34.\n[342] A. K. Mondal, J. Dolz, C. Desrosiers, Few-shot 3d\nmulti-modal medical image segmentation using genera-\ntive adversarial learning, arXiv preprint arXiv:1810.12241\n(2018).\n[343] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, A. V.\nDalca, Data augmentation using learned transformations\nfor one-shot medical image segmentation, in: Proceedings\nof the IEEE/CVF conference on computer vision and pat-\ntern recognition, 2019, pp. 8543–8553.\n[344] Q. Lu, W. Liu, Z. Zhuo, Y. Li, Y. Duan, P. Yu, L. Qu,\nC. Ye, Y. Liu, A transfer learning approach to few-shot\nsegmentation of novel white matter tracts, Medical Image\nAnalysis 79 (2022) 102454.\n[345] W. Liu, Q. Lu, Z. Zhuo, Y. Liu, C. Ye, One-shot segmen-\ntation of novel white matter tracts via extensive data aug-\nmentation, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer,\n2022, pp. 133–142.\n44\n[346] M. Fischer, T. Hepp, S. Gatidis, B. Yang, Self-supervised\ncontrastive learning with random walks for medical im-\nage segmentation with limited annotations, Computerized\nMedical Imaging and Graphics 104 (2023) 102174.\n[347] J. Snell, K. Swersky, R. Zemel, Prototypical networks for\nfew-shot learning, Advances in neural information pro-\ncessing systems 30 (2017).\n[348] S. Ali, B. Bhattarai, T.-K. Kim, J. Rittscher, Additive an-\ngular margin for few shot learning to classify clinical en-\ndoscopy images, in: Machine Learning in Medical Imag-\ning: 11th International Workshop, MLMI 2020, Held in\nConjunction with MICCAI 2020, Lima, Peru, October 4,\n2020, Proceedings 11, Springer, 2020, pp. 494–503.\n[349] R. Wang, Q. Zhou, G. Zheng, Few-shot medical image seg-\nmentation regularized with self-reference and contrastive\nlearning, in: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, Springer,\n2022, pp. 514–523.\n[350] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert,\nSelf-supervision with superpixels: Training few-shot med-\nical image segmentation without annotation, in: Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XXIX 16,\nSpringer, 2020, pp. 762–780.\n[351] C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, D. Rueckert,\nSelf-supervised learning for few-shot medical image seg-\nmentation, IEEE Transactions on Medical Imaging 41 (7)\n(2022) 1837–1848.\n[352] S. Hansen, S. Gautam, R. Jenssen, M. Kampffmeyer,\nAnomaly detection-inspired few-shot medical image seg-\nmentation through self-supervision with supervoxels,\nMedical Image Analysis 78 (2022) 102385.\n[353] S. Hansen, S. Gautam, S. A. Salahuddin, M. Kampffmeyer,\nR. Jenssen, Adnet++: A few-shot learning framework\nfor multi-class medical image volume segmentation with\nuncertainty-guided feature refinement, Medical Image\nAnalysis (2023) 102870.\n[354] Q. Yu, K. Dang, N. Tajbakhsh, D. Terzopoulos, X. Ding,\nA location-sensitive local prototype network for few-shot\nmedical image segmentation, in: 2021 IEEE 18th inter-\nnational symposium on biomedical imaging (ISBI), IEEE,\n2021, pp. 262–266.\n[355] H. Tang, X. Liu, S. Sun, X. Yan, X. Xie, Recurrent mask\nrefinement for few-shot medical image segmentation, in:\nProceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 3918–3928.\n[356] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-\nlearning for fast adaptation of deep networks, in: Interna-\ntional conference on machine learning, PMLR, 2017, pp.\n1126–1135.\n[357] R. Singh, V. Bharti, V. Purohit, A. Kumar, A. K. Singh,\nS. K. Singh, Metamed: Few-shot medical image classifi-\ncation using gradient-based meta-learning, Pattern Recog-\nnition 120 (2021) 108111.\n[358] S. Chao, D. Belanger, Generalizing few-shot classification\nof whole-genome doubling across cancer types, in: Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021, pp. 3382–3392.\n[359] A. Achmamad, F. Ghazouani, S. Ruan, Few-shot learning\nfor brain tumor segmentation from mri images, in: 2022\n16th IEEE International Conference on Signal Processing\n(ICSP), Vol. 1, IEEE, 2022, pp. 489–494.\n[360] R. Khadka, D. Jha, S. Hicks, V. Thambawita, M. A.\nRiegler, S. Ali, P. Halvorsen, Meta-learning with implicit\ngradients in a few-shot setting for medical image segmen-\ntation, Computers in Biology and Medicine 143 (2022)\n105227.\n[361] A. Rajeswaran, C. Finn, S. M. Kakade, S. Levine, Meta-\nlearning with implicit gradients, Advances in neural infor-\nmation processing systems 32 (2019).\n[362] Z. Zhao, F. Zhou, Z. Zeng, C. Guan, S. K. Zhou, Meta-\nhallucinator: Towards few-shot cross-modality cardiac im-\nage segmentation, in: International Conference on Med-\nical Image Computing and Computer-Assisted Interven-\ntion, Springer, 2022, pp. 128–139.\n[363] X. Song, J. Li, X. Qian, Diagnosis of glioblastoma multi-\nforme progression via interpretable structure-constrained\ngraph neural networks, IEEE Transactions on Medical\nImaging 42 (2) (2022) 380–390.\n[364] M. Gao, H. Jiang, L. Zhu, Z. Jiang, M. Geng, Q. Ren,\nY. Lu, Discriminative ensemble meta-learning with co-\nregularization for rare fundus diseases diagnosis, Medical\nImage Analysis 89 (2023) 102884.\n[365] A.\nG.\nRoy,\nS.\nSiddiqui,\nS.\nP¨olsterl,\nN.\nNavab,\nC. Wachinger, squeeze & exciteguided few-shot segmen-\ntation of volumetric images, Medical image analysis 59\n(2020) 101587.\n[366] R. Feng, X. Zheng, T. Gao, J. Chen, W. Wang, D. Z. Chen,\nJ. Wu, Interactive few-shot learning: Limited supervision,\nbetter medical image segmentation, IEEE Transactions on\nMedical Imaging 40 (10) (2021) 2575–2588.\n[367] S. Kim, S. An, P. Chikontwe, S. H. Park, Bidirectional rnn-\nbased few shot learning for 3d medical image segmenta-\ntion, in: Proceedings of the AAAI conference on artificial\nintelligence, Vol. 35, 2021, pp. 1808–1816.\n[368] Y. Feng, Y. Wang, H. Li, M. Qu, J. Yang, Learning what\nand where to segment: A new perspective on medical im-\nage few-shot segmentation, Medical Image Analysis 87\n(2023) 102834.\n[369] W. Zhu, H. Liao, W. Li, W. Li, J. Luo, Alleviating the\nincompatibility between cross entropy loss and episode\ntraining for few-shot skin disease classification, in: Medi-\ncal Image Computing and Computer Assisted Intervention\n– MICCAI 2020, Springer International Publishing, Cham,\n2020, pp. 330–339.\n45\n[370] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B.\nKendall, M. B. Gotway, J. Liang, Convolutional neural net-\nworks for medical image analysis: Full training or fine tun-\ning?, IEEE transactions on medical imaging 35 (5) (2016)\n1299–1312.\n[371] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for\nimage recognition, in: Proceedings of the IEEE conference\non computer vision and pattern recognition, 2016, pp. 770–\n778.\n[372] K. Simonyan, A. Zisserman, Very deep convolutional net-\nworks for large-scale image recognition, arXiv preprint\narXiv:1409.1556 (2014).\n[373] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-\nFei, Imagenet: A large-scale hierarchical image database,\nin: 2009 IEEE conference on computer vision and pattern\nrecognition, Ieee, 2009, pp. 248–255.\n[374] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues,\nJ. Yao, D. Mollura, R. M. Summers, Deep convolutional\nneural networks for computer-aided detection: Cnn archi-\ntectures, dataset characteristics and transfer learning, IEEE\ntransactions on medical imaging 35 (5) (2016) 1285–1298.\n[375] B. Q. Huynh, H. Li, M. L. Giger, Digital mammographic\ntumor classification using transfer learning from deep con-\nvolutional neural networks, Journal of Medical Imaging\n3 (3) (2016) 034501–034501.\n[376] Y. Yuan, W. Qin, M. Buyyounouski, B. Ibragimov, S. Han-\ncock, B. Han, L. Xing, Prostate cancer classification\nwith multiparametric mri transfer learning model, Medical\nphysics 46 (2) (2019) 756–765.\n[377] S. Minaee, R. Kafieh, M. Sonka, S. Yazdani, G. J. Soufi,\nDeep-covid: Predicting covid-19 from chest x-ray images\nusing deep transfer learning, Medical image analysis 65\n(2020) 101794.\n[378] P. Kora, C. P. Ooi, O. Faust, U. Raghavendra, A. Gudigar,\nW. Y. Chan, K. Meenakshi, K. Swaraja, P. Plawiak, U. R.\nAcharya, Transfer learning techniques for medical image\nanalysis: A review, Biocybernetics and Biomedical Engi-\nneering 42 (1) (2022) 79–107.\n[379] C. Ma, Z. Ji, M. Gao, Neural style transfer improves\n3d cardiovascular mr image segmentation on inconsis-\ntent data, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2019: 22nd International\nConference, Shenzhen, China, October 13–17, 2019, Pro-\nceedings, Part II 22, Springer, 2019, pp. 128–136.\n[380] X. Qin, Transfer learning with edge attention for prostate\nmri\nsegmentation,\narXiv\npreprint\narXiv:1912.09847\n(2019).\n[381] J. Liu, B. Dong, S. Wang, H. Cui, D.-P. Fan, J. Ma,\nG. Chen, Covid-19 lung infection segmentation with a\nnovel two-stage cross-domain transfer learning frame-\nwork, Medical image analysis 74 (2021) 102205.\n[382] D. M. Nguyen, T. T. Nguyen, H. Vu, Q. Pham, M.-D.\nNguyen, B. T. Nguyen, D. Sonntag, Tatl: Task agnostic\ntransfer learning for skin attributes detection, Medical Im-\nage Analysis 78 (2022) 102359.\n[383] Q. Yu, L. Xie, Y. Wang, Y. Zhou, E. K. Fishman, A. L.\nYuille, Recurrent saliency transformation network: Incor-\nporating multi-stage visual cues for small organ segmenta-\ntion, in: Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2018, pp. 8280–8289.\n[384] S. Liu,\nD. Xu,\nS. K. Zhou,\nO. Pauly,\nS. Grbic,\nT. Mertelmeier, J. Wicklein, A. Jerebko, W. Cai, D. Co-\nmaniciu, 3d anisotropic hybrid network:\nTransferring\nconvolutional features from 2d images to 3d anisotropic\nvolumes, in: Medical Image Computing and Computer\nAssisted Intervention–MICCAI 2018: 21st International\nConference, Granada, Spain, September 16-20, 2018, Pro-\nceedings, Part II 11, Springer, 2018, pp. 851–858.\n[385] H. Messaoudi, A. Belaid, D. B. Salem, P.-H. Conze, Cross-\ndimensional transfer learning in medical image segmenta-\ntion with deep learning, Medical Image Analysis (2023)\n102868.\n[386] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, A. V.\nDalca, An unsupervised learning model for deformable\nmedical image registration, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition,\n2018, pp. 9252–9260.\n[387] C. V. Nguyen, Y. Li, T. D. Bui, R. E. Turner, Variational\ncontinual learning, in: International Conference on Learn-\ning Representations, 2018.\n[388] E. Zheng, Q. Yu, R. Li, P. Shi, A. Haake, A continual\nlearning framework for uncertainty-aware interactive im-\nage segmentation, in: Proceedings of the AAAI Confer-\nence on Artificial Intelligence, Vol. 35, 2021, pp. 6030–\n6038.\n[389] J. Zhang, R. Gu, G. Wang, L. Gu, Comprehensive\nimportance-based selective regularization for continual\nsegmentation across multiple sites, in: Medical Image\nComputing and Computer Assisted Intervention–MICCAI\n2021: 24th International Conference, Strasbourg, France,\nSeptember 27–October 1, 2021, Proceedings, Part I 24,\nSpringer, 2021, pp. 389–399.\n[390] Z. Li, C. Zhong, R. Wang, W.-S. Zheng, Continual learning\nof new diseases with dual distillation and ensemble strat-\negy, in: Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2020: 23rd International Confer-\nence, Lima, Peru, October 4–8, 2020, Proceedings, Part\nI 23, Springer, 2020, pp. 169–178.\n[391] M. M. Derakhshani, I. Najdenkoska, T. van Sonsbeek,\nX. Zhen, D. Mahapatra, M. Worring, C. G. Snoek, Life-\nlonger: A benchmark for continual disease classification,\nin: International Conference on Medical Image Comput-\ning and Computer-Assisted Intervention, Springer, 2022,\npp. 314–324.\n46\n[392] Y. Yang, Z. Cui, J. Xu, C. Zhong, W.-S. Zheng, R. Wang,\nContinual learning with bayesian model based on a fixed\npre-trained feature extractor, Visual Intelligence 1 (1)\n(2023) 5.\n[393] N. Bayasi, G. Hamarneh, R. Garbi, Culprit-prune-net: Ef-\nficient continual sequential multi-domain learning with ap-\nplication to skin lesion classification, in: International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, Springer, 2021, pp. 165–175.\n[394] M. Lenga, H. Schulz, A. Saalbach, Continual learning for\ndomain adaptation in chest x-ray classification, in: Medi-\ncal Imaging with Deep Learning, PMLR, 2020, pp. 413–\n423.\n[395] B. Chen, K. Thandiackal, P. Pati, O. Goksel, Genera-\ntive appearance replay for continual unsupervised domain\nadaptation, arXiv preprint arXiv:2301.01211 (2023).\n[396] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, C. P. Lan-\nglotz, Contrastive learning of medical visual representa-\ntions from paired images and text, in: Machine Learning\nfor Healthcare Conference, PMLR, 2022, pp. 2–25.\n[397] X. Xie, J. Niu, X. Liu, Z. Chen, S. Tang, S. Yu, A sur-\nvey on incorporating domain knowledge into deep learn-\ning for medical image analysis, Medical Image Analysis\n69 (2021) 101985.\n[398] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you\nneed, Advances in neural information processing systems\n30 (2017).\n[399] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al., An image is worth 16x16\nwords: Transformers for image recognition at scale, arXiv\npreprint arXiv:2010.11929 (2020).\n[400] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Ma-\nlik, C. Feichtenhofer, Multiscale vision transformers, in:\nProceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 6824–6835.\n[401] J. Li, J. Chen, Y. Tang, C. Wang, B. A. Landman, S. K.\nZhou, Transforming medical imaging with transformers?\na comparative review of key properties, current progresses,\nand future perspectives, Medical image analysis (2023)\n102762.\n[402] Y. Xie, J. Zhang, Y. Xia, Q. Wu, Unified 2d and 3d pre-\ntraining for medical image classification and segmentation,\narXiv preprint arXiv:2112.09356 1 (2021).\n[403] B. Zoph, Q. Le, Neural architecture search with reinforce-\nment learning, in: International Conference on Learning\nRepresentations, 2016.\n[404] T. Elsken, J. H. Metzen, F. Hutter, Neural architecture\nsearch: A survey, The Journal of Machine Learning Re-\nsearch 20 (1) (2019) 1997–2017.\n[405] A. Yuille, C. Liu, Deep nets: What have they ever done for\nvision?., Int J Comput Vis 129 (2021) 781802.\n[406] D. B. Larson, D. C. Magnus, M. P. Lungren, N. H. Shah,\nC. P. Langlotz, Ethics of using and sharing clinical imag-\ning data for artificial intelligence: a proposed framework,\nRadiology 295 (3) (2020) 675–682.\n[407] N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth,\nS. Albarqouni, S. Bakas, M. N. Galtier, B. A. Landman,\nK. Maier-Hein, et al., The future of digital health with fed-\nerated learning, NPJ digital medicine 3 (1) (2020) 119.\n[408] R. S. Antunes, C. Andr´e da Costa, A. K¨uderle, I. A. Yari,\nB. Eskofier, Federated learning for healthcare: Systematic\nreview and architecture proposal, ACM Transactions on\nIntelligent Systems and Technology (TIST) 13 (4) (2022)\n1–23.\n[409] M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati,\nA. Kotrotsou, M. Milchenko, W. Xu, D. Marcus, R. R.\nColen, et al., Federated learning in medicine: facilitating\nmulti-institutional collaborations without sharing patient\ndata, Scientific reports 10 (1) (2020) 12598.\n[410] Z. Li, Y. Li, Q. Li, P. Wang, D. Guo, L. Lu, D. Jin,\nY. Zhang, Q. Hong, Lvit: language meets vision trans-\nformer in medical image segmentation, IEEE Transactions\non Medical Imaging (2023).\n47\n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2023-10-10",
  "updated": "2023-10-10"
}