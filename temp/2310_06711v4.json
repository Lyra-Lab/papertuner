{
  "id": "http://arxiv.org/abs/2310.06711v4",
  "title": "Reinforcement-learning-based Algorithms for Optimization Problems and Applications to Inverse Problems",
  "authors": [
    "Chen Xu",
    "Yun-Bin Zhao",
    "Zhipeng Lu",
    "Ye Zhang"
  ],
  "abstract": "We design a new iterative algorithm, called REINFORCE-OPT, for solving a\ngeneral type of optimization problems. This algorithm parameterizes the\nsolution search rule and iteratively updates the parameter using a\nreinforcement learning (RL) algorithm resembling REINFORCE. To gain a deeper\nunderstanding of the RL-based methods, we show that REINFORCE-OPT essentially\nsolves a stochastic version of the given optimization problem, and that under\nstandard assumptions, the searching rule parameter almost surely converges to a\nlocally optimal value. Experiments show that REINFORCE-OPT outperforms other\noptimization methods such as gradient descent, the genetic algorithm, and\nparticle swarm optimization, via its ability to escape from locally optimal\nsolutions and its robustness to the choice of initial values. With rigorous\nderivations, we formally introduce the use of reinforcement learning to deal\nwith inverse problems. By choosing specific probability models for the\naction-selection rule, we can also connect our approach to the conventional\nmethods of Tikhonov regularization and iterative regularization. We take\nnon-linear integral equations and parameter-identification problems in partial\ndifferential equations as examples to show how reinforcement learning can be\napplied in solving non-linear inverse problems. The numerical experiments\nhighlight the strong performance of REINFORCE-OPT, as well as its ability to\nquantify uncertainty in error estimates and identify multiple solutions for\nill-posed inverse problems that lack solution stability and uniqueness.",
  "text": "Reinforcement-learning-based Algorithms for\nOptimization Problems and Applications to Inverse\nProblems\nChen Xu1, Yun-bin Zhao2, Zhipeng Lu1, and Ye Zhang∗3,4\n1Guangdong Laboratory of Machine Perception and Intelligent Computing,\nShenzhen MSU-BIT University, China.\n2Shenzhen Research Institute of Big Data, Chinese University of Hong\nKong, Shenzhen, China\n3Faculty of Computational Mathematics and Cybernetics, Shenzhen\nMSU-BIT University, 518172 Shenzhen, China\n4School of Mathematics and Statistics, Beijing Institute of Technology,\n100081 Beijing, China\nEmail: {xuchen, zhipeng.lu, ye.zhang}@smbu.edu.cn,\nyunbinzhao@cuhk.edu.cn\nAbstract\nWe design a new iterative algorithm, called REINFORCE-OPT, for solving a\ngeneral type of optimization problems. This algorithm parameterizes the solution\nsearch rule and iteratively updates the parameter using a reinforcement learning (RL)\nalgorithm resembling REINFORCE. To gain a deeper understanding of the RL-based\nmethods, we show that REINFORCE-OPT essentially solves a stochastic version of\nthe given optimization problem, and that under standard assumptions, the searching\nrule parameter almost surely converges to a locally optimal value. Experiments show\nthat REINFORCE-OPT outperforms other optimization methods such as gradient\ndescent, the genetic algorithm, and particle swarm optimization, via its ability to\nescape from locally optimal solutions and its robustness to the choice of initial values.\nWith rigorous derivations, we formally introduce the use of reinforcement learning to\ndeal with inverse problems. By choosing specific probability models for the action-\nselection rule, we can also connect our approach to the conventional methods of\nTikhonov regularization and iterative regularization.\nWe take non-linear integral\nequations and parameter-identification problems in partial differential equations as\nexamples to show how reinforcement learning can be applied in solving non-linear\ninverse problems. The numerical experiments highlight the strong performance of\nREINFORCE-OPT, as well as its ability to quantify uncertainty in error estimates and\nidentify multiple solutions for ill-posed inverse problems that lack solution stability\nand uniqueness.\n∗Corresponding author\n1\narXiv:2310.06711v4  [math.OC]  24 Jan 2025\n1\nIntroduction\nIn this paper, we study the theory of using a reinforcement-learning (RL) algorithm to solve\nthe continuous optimization problem:\nmax\nx∈H1 L(x),\n(1)\nwhere H1 is a Hilbert space, and L is a functional acting from H1 to R. To facilitate the use\nof RL, we reformulate the problem (1) as a solution-search problem within the framework\nof a Markov decision process (MDP) and solve it using an RL method similar to the\npopular REINFORCE algorithm (e.g. [58, Eq. (13.8)]). Specifically, our algorithm, named\nREINFORCE-OPT, parameterizes the solution search rule as πθ and iteratively updates\nthe parameter θ ∈Rd using the gradient ascent algorithm based on the x values obtained by\nMonte Carlo sampling. In this context, a search rule refers to a non-deterministic function\nthat selects the next points to examine within H1.\nThe application of RL to optimization problems has garnered significant attention re-\ncently. For example, the search for the optimal πθ can be viewed as a search within the\nparameterized space of optimization algorithms. Based on this concept, Li and Malik [41]\nproposed a meta-learning method for continuous optimization algorithm design. Specifi-\ncally, using a set F of randomly generated objective functions, they train the search rule\nπθ through guided policy search [40]. Unlike REINFORCE-OPT, their search rule does not\nrequire retraining when dealing with a new objective function generated from the same ran-\ndom source as F. However, their method relies on the gradients of the objective function as\ninputs to πθ, limiting its application to nondifferentiable objectives. There are also several\nworks that apply RL to solve combinatorial optimization problems, such as the traveling\nsalesman problem and routing problems [2, 48, 39, 45], as well as works that combine RL\nwith popular evolutionary algorithms for global optimization [55, 54, 17]. A good review\nfor the recent development in this direction can be found in [12]. While the basic idea\nbehind these RL-based algorithms is similar to ours, a deeper theoretical understanding of\nthese algorithms is often lacking. For example, the precise relationship between RL’s goal\nof maximizing long-term returns and the original optimization problem (1) is not rigorously\nexplored, nor is there a comprehensive theory on the convergence of algorithms.\nOur first task is to address both of these issues. In Proposition 1, we prove that as the\nlength of the search path used for each update of θ approaches infinity, REINFORCE-OPT\nsolves a stochastic version of the original problem (1) with a regularization term, i.e.\nmax\nθ∈Rd{Ex∼µπθ\n\u0002\nL(x) −β∥θ∥2\nw\n\u0003\n},\n(2)\nwhere µπθ is a distribution over the solution space H1 to be explained in Proposition 1, β > 0\nis the regularization coefficient, and the weighted Euclidean norm ∥·∥w is explained later in\nSections 2. Additionally, we provide an asymptotic convergence analysis of REINFORCE-\nOPT in this work.\nIn the field of optimization, REINFORCE-OPT belongs to the category of meta-heuristic\nmethods, characterized by an iterative process of “evaluating the L-values for the current\nbatch of x samples and generating the next batch based on these evaluations and the search\nrule”. Many popular methods fall into this category, including evolutionary algorithms like\nthe genetic algorithm (GA) [44, Section 3.1.9] and particle swarm optimization (PSO) [44,\nSection 3.1.5], Bayesian optimization (BO) [19], the estimation of distribution algorithm\n(EDA) [28], and the cross-entropy (CE) method [6]. The main difference between these\nmethods and REINFORCE-OPT lies in the latter’s parameterization of the search rule\nusing a flexible function πθ (e.g., a neural network) and iterative updates to maximize the\n2\nexpected L-value of the x values sampled from πθ. In contrast, GA and PSO (except for\nsome variants) do not update key parameters in their search rules, such as the crossover\nand mutation rates in GA or the inertia weight and cognitive coefficient in PSO. BO pa-\nrameterizes L rather than the search rule, as it is designed to handle situations where the\nfunctional form of L is unknown or expensive to evaluate. However, BO is impractical for\nhandling large numbers of sampled x values due to its computational complexity, which\nrequires at least N 2 evaluations of L [19, Eq. (3)], where N denotes the number of sampled\nx-values for one update of the search rule. Additionally, it is recommended that BO be ap-\nplied to cases where the dimension of x does not exceed 20 [19]. EDA and CE update their\nsearch rules differently from REINFORCE-OPT. EDA constructs a probabilistic model to\nfit the distribution of the set X of x-samples with larger L-values and then re-samples\nbased on this model. CE [6, Algorithm 2.1] updates the search rule parameters to increase\nthe probabilities of samples in X.\nOur algorithm offers several advantages over other popular methods:\n1. No requirement for differentiability: Unlike the RL-based algorithm in [41] and lo-\ncal optimization methods (e.g., gradient ascent in H1), REINFORCE-OPT does not\nrequire the objective L to be differentiable1. As shown by the iteration formula in\n(13), the search path {xt} produced by the updating rule πθ∗may escape from local\noptima2 of L(x) (see Figures 1 and 3 for numerical demonstration).\n2. Robustness against initial conditions: Compared to some of the popular global opti-\nmization methods, our experiments in Section 5.1.2 demonstrate that REINFORCE-\nOPT is more robust to the choice of initial values x0 than GA, PSO and CE. Our\nresults in Tables 3 and 4 suggest that it is difficult for these three algorithms to locate\nthe global optimum when it lies outside the region of initial population.\n3. Handling higher dimensions: As shown in Section 5.2.1, REINFORCE-OPT is capa-\nble of handling an x-dimension of 64, which significantly exceeds the upper bound of\n20 suggested for BO [19].\n4. Theoretical convergence: While theoretical results on the convergence of GA for non-\nconcave functional forms of L in infinite solution spaces are lacking, this work provides\nan asymptotic convergence analysis for REINFORCE-OPT.\n5. Limitations of GA theory: Existing theoretical works on GA assume either a finite\nsolution space [50, 3, 24], a concave objective L [57], or focus on general evolu-\ntionary algorithms rather than GA [51, 52, 13]. We believe that the advantage of\nREINFORCE-OPT over GA and PSO in locating the global maximum for problem\n(1) stems from the reliable updating mechanism of gradient ascent in the θ-space.\nThe second task in this work is to apply REINFORCE-OPT to solve a stochastic for-\nmulation of the following general inverse problem:\n(IP) Given an observed value of the random variable yδ, estimate the solution xe of\nthe following model\nf(xe) + δξ = yδ,\n(3)\n1Note that REINFORCE-OPT performs gradient descent in the parameter space, not in the x space.\nTherefore, it does not rely on the derivative of L. Specifically, the algorithm seeks to solve maxθ J(θ) for\nsome differentiable J(θ).\n2While our algorithm with ability to escape local optima has an advantage over local optimization\nmethods, it may not have an advantage over the RL-based algorithm in [41].\n3\nwhere f : H1 ⊃D(f) →H2 is a known map between two Hilbert spaces, δ > 0 denotes\nthe noise level, and ξ represents an unobservable Hilbert space process3 on H2.\nRigorously, problem (3) can be understood as follows: for each z ∈H2, we have access to\nthe value of ⟨yδ, z⟩that satisfies\n⟨f(xe), z⟩+ δ⟨ξ, z⟩= ⟨yδ, z⟩,\nwhere for simplicity we use ⟨·, ·⟩to denote the inner products for both Hilbert spaces. The\nnoise term ξz := ⟨ξ, z⟩, i.e., ξ can be interpreted as a random element in the algebraic\ndual space of H2. Model (3) includes the standardized Gaussian white-noise model where\nξz ∼N(0, ∥z∥2\nH2) and Cov[ξz1, ξz2] = ⟨z1, z2⟩for all z1, z2 ∈H2, where ∥·∥denotes the norm\nin the Hilbert space.\nFor ill-posed problems4, some regularization approaches are required to find stable ap-\nproximations to the solution xe of inverse problem (3). Specific regularization methods for\nsimilar inverse problems can be found in [26, 27, 33]. Within the framework of Tikhonov\nregularization, the approximate solution of (3) can be taken as the solution of the mini-\nmization problem\nmin\nx∈H1 ∥f(x) −yδ∥2 + αΩ(x),\n(4)\nwhere Ω(x) is the regularization term and α > 0 denotes the regularization parameter.\nHowever, the methods commonly used to solve (4) have two drawbacks:\n(a) The solution estimator xδ\nα is deterministic and therefore does not account for the\nrandomness introduced by the noise δξ in the problem (3).\n(b) Difficulties arise when the objective in (4) is nonconvex and/or nondifferentiable; both\nare common in practical applications.\nTo address the two issues, we apply REINFORCE-OPT to solve (4), which by Propo-\nsition 1 is equivalent to solving the following statistical formulation of (4) with double\nregularization penalties:\nθ∗= arg max\nθ\nEx∼µπθ[−∥f(x) −yδ∥2 −αΩ(x) −β∥θ∥2\nw],\n(5)\nwhere the parameterized distribution µπθ is explained in Proposition 1. In this new formu-\nlation, the designed regularization solution x is a random variable following the distribu-\ntion µπθ with parameters θ = θ∗. Also, our experiments in Section 5.1 demonstrate the\nability of REINFORCE-OPT to handle nonconvex cases. In addition to addressing the\nabove-mentioned issues (a) and (b), another advantage of REINFORCE-OPT over existing\nmethods in the field of inverse problems is its ability to handle the case of multiple solutions\nof (3). Commonly used supervised learning frameworks, such as neural networks, cannot\nbe directly applied to model the multi-valued inversion map f −1 (see, e.g., [59, 61]). This\nadvantage will be demonstrated in Section 5.2.4.\nThis paper is organized as follows: Section 2 reformulates the optimization problem\nin (1) within the framework of an MDP and explores the relationship between the goals\nof the reinforcement learning method and the optimization problem.\nIn Section 3, we\npresent the REINFORCE-OPT algorithm and show the convergence theorem. Section 4\n3A Hilbert space process ξ is a bounded linear mapping ξ : H2 →L2(Ω, A, P), where (Ω, A, P) is a\nprobability space.\n4We refer to [30, Def. 1.1] and [29, Def. 3] for a rigorous definition of the (local) ill-posedness of a\nnonlinear operator equation.\n4\nintroduces the application of REINFORCE-OPT to inverse problems and establishes con-\nnections between this algorithm and classical regularization methods. Section 5.1 demon-\nstrates the advantages of REINFORCE-OPT over other commonly used optimization meth-\nods through experimental comparisons, while Section 5.2 showcases the performance of\nREINFORCE-OPT with numerical examples involving auto-convolution equations and in-\nverse chromatography problems. Finally, Section 6 provides concluding remarks, and ad-\nditional technical proofs are included in the Appendices. All codes are available at http:\n//github.com/chen-research/REINFORCE-OPT, with the reinforcement learning-related\ncodes implemented using TensorFlow Agent [25].\n2\nReinforcement Learning for Optimization\nThe following notations will be used throughout the paper.\nNotation\nDescription\nReference\nZ+\nThe set of positive integers.\nyδ\nk\nThe kth observed sample of yδ.\n(3).\nΩ(x)\nThe regularization term.\n(4).\nα, β\nThe non-negative regularization coefficients.\n(4) and (5).\nπθ(·|x)\nA probability density of the action a given the\ncurrent state x.\nNear (6).\nµπθ\nThe invariant distribution of the Markov chain\n{xt}+∞\nt=0 generated by following policy πθ.\nTheorem 1.\nxt\nThe state in step t in a trajectory.\n(7).\nat\nThe action taken in step t in a trajectory.\n(7).\nT\nThe trajectory length {xt}T−1\nt=0 .\n(7).\nJT(θ)\nThe expected sum of rewards obtained in a tra-\njectory of length T generated following the pol-\nicy πθ.\n(7).\nExt,at[·|πθ]\nThe expectation over (xt, at) given that they\nare the step t state-action pair in a trajectory\ngenerated by πθ. This expectation is also de-\ntermined by the initial state x0. However, we\nconsider the initial state’s distribution fixed.\n(7).\nU(0; B1)\n{θ | θ ∈Rd, ∥θ∥≤B1}.\nAssumption 2.\n∇\nIt denotes a partial derivative with respect to\nθ.\n∇JT(θ)\n∂JT (θ)\n∂θ\n.\nˆ∇JT(θ)\nSample estimate of ∇JTθ.\n(13).\ng(·)\nA Lipschitz map from Rd to Rd, with the Lips-\nchitz coefficient of Lg.\nAssumption 1.\nMn+1\nˆ∇JT(θn) −∇JT(θn).\n(14).\nan\nThe updating step size for {θn}.\n(13).\nsn\nsn := Pn−1\ni=0 ai for any n > 0 and s0 := 0.\nLemma 2.\n[t]−\n[t]−:= maxn≥0{sn|sn ≤t}.\n(18).\n[t]+\n[t]+ := minn≥0{sn|sn ≥t}.\n(18).\n{θn}+∞\nn=0\nThe iterate sequence.\n(14) or (13).\n¯θ(·)\n¯θ(t) := λtθn + (1 −λt)θn+1 for t ∈[sn, sn+1],\nwhere λt :=\nsn+1−t\nsn+1−sn ∈[0, 1].\nLemma 2.\n5\nθs(·)\nA solution to ODE (17) with θs(s) = ¯θ(s).\nLemma 2.\nU +\n{x ∈RD : x has non-negative entries}.\nBelow (46).\nLet us first reformulate problem (1) within an MDP framework5, allowing us to apply\nRL algorithms iteratively to solve it. Suppose that there is an agent moving through H1,\nwhere its location represents the state. At each time step t, the agent takes a step (action\nat ∈H1) from the current state xt, transitioning to a new state xt+1 = xt+at and receiving\na reward R(xt, at). In this framework, the approximate solution to problem (1) is the limit\nof the sequence {xt}.\nRL algorithms aim to find the at-selection rule π (time-independent), which is a map\nfrom the x-space S to the set of probability distributions PD(A) over the a-space A, that\nmaximizes the expected average rewards. Specifically, after parameterizing the policy with\nπθ, they aim to solve\nmax\nθ∈Rd{JT(θ) −β∥θ∥2\nw},\n(6)\nwhere β > 0 is the regularization coefficient, ∥·∥w denotes the weighted Euclidean norm,\ni.e., ∥θ∥2\nw = Pd\ni=1 wiθ2\ni with positive numbers wi, and6\nJT(θ) := 1\nT\nT−1\nX\nt=0\nExt,at [R(xt, at)|πθ] ,\n(7)\nwhere\n• πθ(·|x) is a distribution over the action space determined by the current state x.\n• xt ∈S denotes the tth step’s state and S ⊂H1 is called the state space.\n• at ∈A denotes the agent’s tth action and A ⊂H1 is called the action space. The\ndistribution of at is πθ(·|xt).\n• Ext,at [·|πθ] denotes an expectation with respect to (xt, at), where (xt, at) is the tth\nstep state-action pair of a trajectory generated following πθ. Note that the distribution\nof the initial state x0 is independent from πθ.\nIt is determined by the problem\nenvironment or set by the user.\n• The transition rule from a state-action pair to the next state is\nxt+1 = xt + at.\n(8)\nWe note that the transition rule can be set differently for a general problem. Never-\ntheless, we adopt (8) throughout this paper since it is proper for the inverse problem\n(3).\n• The reward function R should be defined so that solving (6) is consistent with finding\na at-selection rule π (∆xt-selection rule) such that the agent will reach the minimum\nof L as soon as possible. There are multiple options for the functional form of R, and\nwe choose the following simplest one:\nR(x, a) := L(x + a).\n(9)\n5An MDP involves an agent that takes a series of actions in a dynamic environment, receiving rewards\nbased on state-action pairs. Each action depends solely on the current state, independent of past states\nand actions, which is known as the Markov property. For more details on MDPs, see [58, Section 3.1].\n6See [58, Eq.(13.15)] for the definition of a long-term performance measure.\nAnother common\nway of defining the performance measure is the expected sum of discounted rewards, i.e., J(π) :=\nEat∼π(xt)\nh\nlimT →∞\nPT\nt=0 γtR(xt, at)\ni\n, where γ ∈[0, 1) is the discount factor.\n6\n• T ∈Z+ is the pre-selected trajectory length.\nTherefore, RL can be viewed as a method for finding an xt-iteration rule to solve (1).\nThroughout the paper, a trajectory generated by the policy πθ refers to {(xt, at)}T−1\nt=0 , where\nx0 is sampled from a distribution that is potentially independent from πθ, at ∼πθ(·|xt),\nand xt+1 = xt + at. We also call {xt}T−1\nt=0 a trajectory generated by πθ. It is clear that\n{xt} is a Markov chain.\nTo better understand the relationship between the RL’s objective (6) and the optimiza-\ntion problem (1), we present the following proposition, which demonstrates that the RL\nalgorithm seeks to solve a stochastic version of (1).\nProposition 1. For any θ ∈Rd, under the conditions that the Markov chain {xt} generated\nfollowing the policy πθ has a unique invariant probability measure7 µπθ, and that L(·) is\nintegrable with respect to the distribution µt of xt for each positive integer t. Then, as\nT →+∞, the RL’s goal (6) becomes\nmax\nθ {Ex∼µπθ [L(x)] −β∥θ∥2\nw}.\n(10)\nProof. By the property of a Markov chain’s invariant probability, the probability measure\nµt of xt converges weakly to µπθ as t →+∞, which further implies that µ′\nT := 1\nT\nPT\nt=1 µt\nconverges weakly to µπθ as T →+∞. This result and the Portemanteau theorem (e.g., [35,\nTheorem 13.16]) imply the second-to-last equality in the following derivation.\nlim\nT→+∞JT(θ) = lim\nT→+∞\n1\nT\nT−1\nX\nt=0\nExt,at [R(xt, at)|πθ] ,\n= lim\nT→+∞\n1\nT\nT−1\nX\nt=0\nExt,at [L(xt+1)|πθ] ,\nby (9),\n= lim\nT→+∞\n1\nT\nT−1\nX\nt=0\nZ\nx∈S\nL(x)dµt+1\n= lim\nT→+∞\nZ\nx∈S\nL(x)dµ′\nT\n=Ex∼µπθ[L(x)],\n(11)\nwhich yields the required results by noting the stationary regularization term β∥θ∥2\nw.\n3\nREINFORCE-OPT and Convergence Analysis\nBefore presenting the REINFORCE-OPT algorithm and the main theorem, we first revisit\nthe policy gradient theorem.\n3.1\nGradient Estimate and Parameter Update Rule\nThe well-known policy gradient theorem provides the expression for the gradient ∇JT(θ).\nIt has several versions; here, we derive the one specific to our problem, which can also be\n7Conditions that ensure the existence of a invariant probability measure of {xt} can be found in Ap-\npendix C. To adapt to the conditions, we can rewrite xt+1 = xt + at = xt + mθ(xt) + zt+1 = f(xt, zt+1),\nwhere mθ(xt) denotes the mean of πθ(·|xt), {zt} is a sequence of i.i.d. random variables, and πθ(·) is a\ndeterministic map.\n7\nfound in [67]. Throughout this paper, we assume that the reward function is bounded.\nStarting from the initial x0 with a given distribution, let hT := {(xt, at)}T−1\nt=0 denote the\ntrajectory generated following πθ, and let HT denote the sample space of hT. Also, define\nR(hT) := 1\nT\nT−1\nX\nt=0\nR(xt, at).\nThen, from the definition of JT(θ) in (7), we derive that\n∇JT(θ) = ∇EhT ∼πθ[R(hT)] = ∇\nZ\nh∈HT\npθ(hT)R(hT)dhT =\nZ\nhT ∈HT\n∇pθ(hT)R(hT)dhT\n=\nZ\nhT ∈HT\npθ(hT)∇ln pθ(hT)R(hT)dhT = EhT ∼πθ [∇ln pθ(hT)R(hT)]\n= EhT ∼πθ\n\"\nR(hT)∇\nT−1\nX\nt=0\nln πθ(at|xt)\n#\n= EhT ∼πθ\n\"\nR(hT)\nT−1\nX\nt=0\n∇ln πθ(at|xt)\n#\n,\nwhere pθ(hT) denotes the probability density of trajectory hT following policy πθ, and\nthe derivative and integration is interchangeable (see the third equality above) due to the\nLebesgue’s dominated convergence theorem (DCT) if8 ∇pθ(hT) exists for all hT ∈HT.\nBased on the above expression, a sample estimate of ∇JT(θ) is straightforward, namely\nˆ∇JT(θ) := 1\nL\nL−1\nX\nl=0\n\"\nR(hl\nT)\nT−1\nX\nt=0\n∇ln πθ(al\nt|xl\nt)\n#\n,\n(12)\nwhere hl\nT denotes the lth sample of hT.\nWe follow a stochastic gradient ascent rule to iteratively improve the objective function\nJT(θ) −β∥θ∥2\nw in (6). Specifically, given the current value θn, we generate L trajectories9\nfollowing πθn and update θn according to\nθn+1 = θn + an( ˆ∇JT(θn) −2βθn),\nn ∈Z+ ∪{0},\n(13)\nwhere θ0 ∈Rd is randomly initialized, ˆ∇JT(θ) is computed according to (12) using the\ngenerated L trajectories, an > 0 is the update step size, and the last term is for regular-\nization purposes. Note that L and T are pre-selected by the user and do not change with\nn.\n3.2\nREINFORCE-OPT\nWe now describe the new algorithm, named REINFORCE-OPT, for solving the potentially\nnonconvex optimization problem (1), as shown in Algorithms 1 and 2.\nA brief explanation of each step of Algorithm 1 is provided as follows:\n• The structure of the distribution πθ of actions can be designed as a multivariate\nnormal distribution, with its mean and covariance equal to the output of a neural\nnetwork Nθ whose input is a state x. πθ is designed in this way except the one in\nSection 5.1.1.\n• The functional form of the reward function R(·, ·) can be constructed by (9).\n8To adapt to DCT, view ∇pθ as a sequence limit, i.e., ∇pθ = limk→+∞\npθk −pθ\nθk−θ , where limk→+∞θk = θ.\n9Note that the initial state x0 in each trajectory is drawn from a user-specified distribution that is\nirrelevant to πθn and remains fixed as n increases. One common choice is to set x0 as a fixed point.\n8\nAlgorithm 1 Algorithm for training the action rule πθ.\n1: Design the structure of πθ.\n2: Construct the reward function R.\n3: Define the distribution of the initial state x0.\n4: Set values of the hyper-parameters {Nθ, T, H0, N, L, β}.\n5: Initialize the weights in Nθ as θ0, and n = 0.\n6: while n < N do\n7:\nFollow πθn to generate L length-T trajectories {hT\nl }L−1\nl=0 .\n8:\nCompute ¯xt = 1\nL\nPL−1\nl=0 xl,t for each t ∈{0, 1, 2, ..., T −1}, where xl,t is the step-t\nstate in trajectory hT\nl .\n9:\nCompute r = PT−1\nt=0 R(¯xt, 0).\n10:\nif r > H0 then\n11:\nbreak;\n12:\nend if\n13:\nCompute ˆ∇JT(θ) according to (12).\n14:\nFollow (13) to update θn to get θn+1.\n15: end while\nAlgorithm 2 REINFORCE-OPT for solving the optimization problem (1).\n1: Input the objective function L(·).\n2: Train the action rule πθ according to Algorithm 1.\n3: Produce samples of approximate solution {x}.\n4: Construct the statistics of the approximate solution, i.e., the expectation and confidence\nintervals.\n• The initial state x0 is used for generating L trajectories {hl\nT}L−1\nl=0 in Eq. (12).\n• The meanings of hyper-parameters are: the architecture of neural network Nθ, i.e.,\nthe number of hidden layers and nodes; the trajectory length T; the threshold H0 for\nthe stop-training rule; the prescribed total number of θ-updates N; the number L of\ntrajectories used for one update of θ; the regularization parameters α and β.\n• Note that the larger R(x, 0) is, the closer x is to the global maximum of L(·). This\nis the reason we use r defined in line 9 of Algorithm 1 as a performance indicator of\nthe policy πθ.\n3.3\nConvergence Analysis\nIn this section, we use the ODE method in stochastic-approximation theory to prove that\nREINFORCE-OPT asymptotically finds a local optimum of θ for any pre-selected positive\nintegers L and T. For this purpose, we rewrite the updating rule (13) as\nθn+1 = θn + an(g(θn) + Mn+1),\nn ∈Z+ ∪{0},\n(14)\nwhere\ng(θn) := ∇JT(θn) −2βθn,\nMn+1 := ˆ∇JT(θn) −∇JT(θn).\n(15)\nIt is worth stressing that the standard convergence analysis for stochastic gradient de-\nscent (e.g. [11, Theorem 2]) does not apply to (14) since the distribution of the error Mn\nmay change with n. In what follows, we first state some necessary assumptions required\n9\nto show our main theoretical result, i.e. Theorem 1. Assumption 1 and 2 are standard\nin the convergence analysis of stochastic approximation algorithms (e.g., [7, Section 8.1]\nand [34, 4, 31]). The boundedness assumption (cf. Assumption 2) is also common for the\nconvergence analysis of algorithms such as stochastic gradient descent (SGD)10.\nAssumption 1.\n1. g : Rd →Rd defined as in (15) is Lipschitz, i.e.\nthere exists a\nconstant Lg > 0 such that for any θ, α ∈Rd it holds that\n∥g(θ) −g(α)∥≤Lg∥θ −α∥.\n2. {Mn} are integrable and satisfy, for any n ≥0,\nE\n\u0002\n∥Mn+1∥2|Fn\n\u0003\n≤K(1 + ∥θn∥2),\nwhere K > 0 is some scalar, and the filtration Fn will be defined in Lemma 1 later.\n3. The step sizes {an} are positive scalars satisfying\nX\nn\nan = +∞,\nX\nn\na2\nn < +∞.\nExamples of {an} satisfying Point 3 above in Assumption 1 are an = 1\nn,\n1\nn log n, · · · .\nAssumption 2. Assume that there exists B1 > 0 such that, almost surely,\nθn ∈U(0; B1) := {θ | θ ∈Rd, ∥θ∥≤B1}, for all n ∈Z+ ∪{0}.\nAssumption 3. Assume that supθ∈U(0;B1){JT(θ)−β∥θ∥2} < +∞, and the set of maximum\npoints,\nH∗:= {θ∗∈U(0; B1) | JT(θ∗) −β∥θ∗∥2 ≥JT(θ) −β∥θ∥2 for any θ ∈U(0; B1)},\nis non-empty and contains all the critical points of (JT(θ) −β∥θ∥2) in U(0; B1).\nThis assumption is on the θ-space rather than the x-space. Hence, it does not rule out\nthe nonconvex optimization problems, such as the ones in Section 5.1.\nAssumption 4. Assume that U(0; B1) as defined in Assumption 2 is an invariant set for\nthe ODE ˙θ(t) = g(θ(t)).\nAssumption 5. Assume that ln πθ(a|x) is differentiable with respect to θ for any θ ∈Rd,\nany a ∈A, and any x ∈S.\nThis assumption is satisfied if πθ is modeled as a multivariate Gaussian distribution,\nwith its mean and standard deviations generated by a neural network Nθ, as is the case in\nmost of our experiments in this work.\nBefore proving our main theorem, we need to establish two useful technical results.\nLemma 1. Given point 2 in Assumption 1, {Mn} defined in (14) is a martingale difference\nsequence with respect to the increasing σ-fields\nFn := σ(θm, Mm, m ≤n), n ≥0,\nwhere σ(θm, Mm, m ≤n) denotes the smallest σ-field that contains {σ(Mm)}n\nm=0 and\n{σ(θm)}n\nm=0.\n10It is stated in [46, Section 4.1] that a large number of researches in the literature on SGD assume that\nthe trajectory is bounded.\n10\nProof. It suffices to prove that Un := Pn\nk=0 Mk is a martingale with respect to Fn. First, it\nis clear that Un is integrable under point 2 of Assumption 1. Second, Un is Fn-measurable\nsince it is the sum of a finite number of Fn-measurable mappings (see [35, Theorem 1.91]).\nFinally,\nE[Un+1|Fn] = E[Mn+1|Fn] + E[Un|Fn]\n= E[Mn+1|Fn] + UnE[1|Fn],\nsince Un is Fn-measurable,\n= E[ ˆ∇JT(θn) −∇JT(θn) |Fn] + Un,\naccording to the definition of Mn+1,\n= E[0|Fn] + Un,\n= Un.\nTherefore, {Un} satisfies the conditions required for being a martingale. Hence, Mn =\nUn −Un−1 is a martingale difference sequence.\nLemma 2. Under Assumptions 1, 2, and 4, for any ∆> 0 we have\nlim\ns→+∞\n \nsup\nt∈[s,s+∆]\n∥¯θ(t) −θs(t)∥\n!\n= 0,\na.s.,\n(16)\nwhere\n1. ¯θ(·) : [0, +∞) →Rl is the continuous-time linear interpolation for {θn}∞\nn=0, defined\nas ¯θ(t) := λtθn + (1 −λt)θn+1 for t ∈[sn, sn+1], where sn := Pn−1\ni=0 ai for any n > 0,\ns0 := 0, and\nλt := sn+1 −t\nsn+1 −sn\n∈[0, 1].\n2. {θn}+∞\nn=0 is the sequence produced by the iteration (14) (the same as (13)).\n3. For any s ≥0, θs(t) denotes a solution to the following ODE, with θs(s) = ¯θ(s):\n˙θ(t) = g(θ(t)),\nt ≥s.\n(17)\nRemark 1. Note that ¯θ(t) and θs(t) start from the same point at time t = s and are close\nto each other only in a finite period [s, s + ∆]. The reason they are not close to each other\nin the entire time interval [s, +∞) is that the noise {Mn}+∞\nn=1 is a martingale difference\nsequence rather than a sequence of i.i.d. mean-zero variables. We define some symbols that\nare frequently used in the proof:\n• For any t ≥0, define\n[t]−:= maxn≥0{sn|sn ≤t} and [t]+ := minn≥0{sn|sn ≥t}.\n(18)\n• Let m∆denote the non-negative integer such that\nsn+m∆:= [sn + ∆]−.\n(19)\n• Bg := supθ∈U(0;B1) ∥g(θ)∥∈[0, +∞).\n11\nProof of Lemma 2. The proof of Lemma 2 consists of 6 steps.\nStep 1. We prove that Eq. (16) can be implied from the fact that for any fixed ∆> 0,\nlim\nn→+∞\n \nsup\nt∈[sn,[sn+∆]−]\n∥¯θ(t) −θsn(t)∥\n!\n= 0,\na.s.\n(20)\nThe proof of this implication is given in Appendix B. Thus it is sufficient to prove Eq. (20).\nTo this goal, we will perform the next few steps (steps 2-5) to show that for any fixed and\nsufficiently large integer n we have\nsup\nt∈[sn,[sn+∆]−]\n∥¯θ(t) −θsn(t)∥≤KneLg∆+ Bg sup\nk≥0\nan+k,\n(21)\nwhere Kn is as defined in Eq. (23). Based on this fact, we eventually prove in Step 6 that\nlimn→+∞Kn = 0, a.s. In addition, point 3 of the assumption 1 implies that limn→∞an = 0,\nwhich further implies limn→+∞Bg supk≥an+k = 0. Therefore, the right-hand side of (21)\napproaches 0 almost surely as n →+∞, which further implies (20).\nNote that ∆> 0 is considered fixed. Since limn→∞an = 0, there exists N∆> 0 such\nthat whenever n > N∆we have an = sn+1−sn < ∆⇒sn+1 < sn+∆, which further implies\nm∆≥1. In step 2-5, n is a fixed positive integer greater than N∆.\nStep 2. For any m ∈Z+, we rewrite ¯θ(sn+m) as\n¯θ(sn+m) = ¯θ(sn) +\nm−1\nX\nk=0\nan+kg(¯θ(sn+k)) + δn,n+m,\nwhere δn,n+m = Pm−1\ni=0 an+iMn+i+1. This equality holds because ¯θ(sk) = θk for any non-\nnegative integer k, and the θ-updating algorithm specified in (14).\nUsing the definition of θs(·) in (17), we rewrite θsn(sn+m) as\nθsn(sn+m) = θsn(sn) +\nZ sn+m\nsn\ng(θsn(t))dt\n= ¯θ(sn) +\nZ sn+m\nsn\ng(θsn(t))dt,\nsince θsn(sn) = ¯θ(sn),\n= ¯θ(sn) +\nm−1\nX\nk=0\nan+kg(θsn(sn+k)) +\nZ sn+m\nsn\n\u0002\ng(θsn(y)) −g(θsn([y]−))\n\u0003\ndy.\nStep 3. We take the difference between the two equations in Step 2, and then prove\nthat for any fixed n ∈Z+,\n∥¯θ(sn+m) −θsn(sn+m)∥≤Lg\nm−1\nX\ni=0\nan+i∥¯θ(sn+i) −θsn(sn+i)∥+ Kn, for all m ∈Z+,\n(22)\nwhere\nKn := BgLg\n∞\nX\nk=0\na2\nn+k + sup\nj∈Z+∥δn,n+j∥.\n(23)\nRecall that Bg := supθ∈U(0;B1) ∥g(θ)∥∈[0, +∞), and note that P∞\nk=0 a2\nn+k < ∞due to\n12\nPoint 3 of Assumption 1. Taking the difference between the two equations in Step 2 gives\n∥¯θ(sn+m) −θsn(sn+m)∥= ∥\nm−1\nX\nk=0\nan+k\n\u0002\ng(¯θ(sn+k)) −g(θsn(sn+k))\n\u0003\n−\nZ sn+m\nsn\n\u0002\ng(θsn(y)) −g(θsn([y]−))\n\u0003\n+ δn,n+m∥\n≤Lg\nm−1\nX\nk=0\nan+k∥¯θ(sn+k) −θsn(sn+k)∥\n+\nZ sn+m\nsn\n∥g(θsn(y)) −g(θsn([y]−))∥dy + sup\nj∈Z+∥δn,n+j∥,\n(24)\nwhere the property11 that the norm of an integral is no greater than the integral of the\nintegrand’s norm is used to derive the inequality. We now bound the second term in the\nabove:\nZ sn+m\nsn\n∥g(θsn(y)) −g(θsn([y]−))∥dy =\nm−1\nX\nk=0\nZ sn+k+1\nsn+k\n∥g(θsn(y)) −g(θsn([y]−))∥dy\n=\nm−1\nX\nk=0\nZ sn+k+1\nsn+k\n∥g(θsn(y)) −g(θsn(sn+k))∥dy\n≤\nm−1\nX\nk=0\nZ sn+k+1\nsn+k\nLg∥θsn(y) −θsn(sn+k)∥dy\n≤\nm−1\nX\nk=0\nZ sn+k+1\nsn+k\nLg∥g(θsn(τy))(y −sn+k)∥dy,\nwhere the inequality in the fourth line is from the mean value theorem (MVT). Since\n∥θsn(τy)∥≤B1 (because U(0; B1) is an invariant set for the ODE ˙θ(t) = g(x(t)) from\nAssumption 4, τy ≥sn, and θsn(sn) = θn ∈U(0; B1)), we have that\nZ sn+m\nsn\n∥g(θsn(y)) −g(θsn([y]−))∥dy ≤LgBg\nm−1\nX\nk=0\nZ sn+k+1\nsn+k\n(y −sn+k)dy\n≤LgBg\nm−1\nX\nk=0\nZ sn+k+1\nsn+k\n(sn+k+1 −sn+k)dy\n≤LgBg\nm−1\nX\nk=0\na2\nn+k,\nThis finishes the proof of Eq. (22).\nStep 4. In accordance with the discrete Gronwall inequality (see Lemma 4 in Appendix\nA), Eq. (22) implies that, for any fixed n ∈Z+,\n∥¯θ(sn+m) −θsn(sn+m)∥≤KneLg\nPm−1\ni=0 an+i,\n11For any integrable function h : R →Rl and any a < b, we have ∥\nR b\na h(t)dt∥≤\nR b\na ∥h(t)∥dt. For the\nproof, define v :=\nR b\na h(t)dt. Then,\n∥v∥2 = vTv =\nZ b\na\nvTh(t)dt ≤\nZ b\na\n∥v∥· ∥h(t)∥dt.\nDividing both the left and the right by ∥v∥gives the desired inequality.\n13\nholds for all m ∈Z+. Therefore, for any m ∈{0, 1, ..., m∆} (where m∆is defined in (19)\nand m∆≥1 because of the content of the last paragraph in Step 1) we have\n∥¯θ(sn+m) −θsn(sn+m)∥≤KneLg∆,\n(25)\nwhich holds because12 for m ∈[1, m∆],\nm−1\nX\ni=0\nan+i ≤\nm∆−1\nX\ni=0\nan+i = sn+m∆−sn = [sn + ∆]−−sn ≤∆.\nStep 5. Now, we use Eq. (25) to bound ∥θsn(t) −¯θ(t)∥for any t ∈[sn+m, sn+m+1],\nwhere m is any integer in [0, m∆−1]. (Recall that ∥θsn(t) −¯θ(t)∥appears in (21).)\n∥θsn(t) −¯θ(t)∥= ∥θsn(t) −(λtθn+m + (1 −λt)(θn+m+1))∥\n= ∥λt(θsn(t) −θn+m) + (1 −λt)(θsn(t) −θn+m+1)∥\n= ∥λt\n\u0000θsn(t) −¯θ(sn+m)\n\u0001\n+ (1 −λt)\n\u0000θsn(t) −¯θ(sn+m+1)\n\u0001\n∥\n= ∥λt\n\u0012\nθsn(sn+m) −¯θ(sn+m) +\nZ t\nsn+m\ng(θsn(s))ds\n\u0013\n+ (1 −λt)\n\u0012\nθsn(sn+m+1) −¯θ(sn+m+1) −\nZ sn+m+1\nt\ng(θsn(s))ds\n\u0013\n∥\n≤KneLg∆+\nZ sn+m+1\nsn+m\n∥g(θsn(s))∥ds,\nby (25) and λt ∈[0, 1],\n≤KneLg∆+ Bgan+m,\nwhere recall that Bg := supθ∈U(0;B1) ∥g(θ)∥∈[0, +∞). Therefore, for any t ∈[sn, sn+m∆] =\n[sn, [sn + ∆]−], we have\n∥θsn(t) −¯θ(t)∥≤KneLg∆+ Bg\nsup\nm∈Z+∪{0}\nan+m,\nwhich further implies\nsup\nt∈[sn,[sn+∆]−]\n∥θsn(t) −¯θ(t)∥≤KneLg∆+ Bg\nsup\nm∈Z+∪{0}\nan+m.\nThis proves (21).\nStep 6. Taking the limit on both sides of the above inequality gives\nlim\nn→+∞\nsup\nt∈[sn,[sn+∆]−]\n∥θsn(t) −¯θ(t)∥≤lim\nn→+∞[KneLg∆+ Bg\nsup\nm∈Z+∪{0}\na(n + m)] = 0,\n(26)\nwhere the last equality follows from limn→+∞supm∈Z+∪{0} a(n + m) = 0 implied from Point\n3 in Assumption 1 and limn→+∞Kn = 0, a.s to which the proof is given below.\nRecall (23),\nKn = BgLg\n∞\nX\nk=0\na2\nn+k + sup\nj∈Z+∥δn,n+j∥.\nNote that (i) Bg < +∞is the upper bound of ∥g(·)∥in the region {x|x ∈Rd, ∥xn∥≤B1}\nand Lg is the Lipschitz constant; (ii) P\nk>0 a2\nn+k →0 as n →∞according to Point 3 of\n12Eq.(25) holds trivially for m = 0.\n14\nAssumption 1; (iii) δn,n+j = Pj−1\ni=0 an+iMn+i+1. Hence, limn→∞Bgλg\nP∞\nk=0 a2\nn+k = 0. To\nshow that limn→+∞Kn = 0, a.s, it remains to prove that\nlim\nn→∞sup\nj∈Z+∥δn,n+j∥= 0, a.s.\nDefine ζn := Pn−1\ni=0 aiMi+1, n ≥0. Then ζn is a martingale with proof similar to that of\nLemma 1. From Point 2 of Assumption 1, for any non-negative integer n we have\nE[∥ζn+1 −ζn∥2|Fn] = E[a2\nn∥Mn+1∥2|Fn]\n≤a2\nnK(1 + ∥θn∥2)\n≤a2\nnK(1 + B2\n1),\naccording to Assumption 2.\nTherefore, according to Lemma 5 (see Appendix A), {ζn} converges almost surely, which\nimplies, according to the Cauchy criterion, that\nlim\nn→+∞( sup\nm∈Z+∥δn,n+m∥) = lim\nn→+∞( sup\nm∈Z+∥ζn+m −ζn∥) = 0, a.s.\nThis finishes the proof of (20) and hence the proof of Lemma 2.\nWe are now ready to present and prove our main result.\nTheorem 1. Under Assumptions 1-5, the parameter-update rule (14) produces θn that\nconverges almost surely to H∗, the set of maximum points of JT(θ) −β∥θ∥2 in U(0; B1) as\ndefined in Assumption 3, with JT(θ) as defined in (7). That is,\nlim\nn→+∞inf\nθ∗∈H∗∥θn −θ∗∥= 0,\na.s.\n(27)\nProof. Define F(θ) := −JT(θ)+β∥θ∥2 and µ := inf∥θ∥≤B1 F(θ). Then, ∇F(θ) = −g(θ).\nFrom Point 1 of Assumption 1, F(θ) is continuously differentiable, which implies that F(θ)\nattains both the maximum and minimum on the closed and bounded set U(0; B1). Hence,\nµ > −∞and ∅̸= {θ ∈U(0; B1) | F(θ) = µ} = H∗(recall Assumption 3 for the definition\nof H∗).\nWe state an inequality whose proof is given later: for any ε > 0 there exists Tε > 0 such\nthat whenever t > Tε we have\nF(¯θ(t)) ≤µ + (Bg + 1)ε.\n(28)\nUsing (28), we prove that\nΘ∗:= lim\nn→+∞\n¯θ(τn) ∈H∗\n(29)\nfor any sequence {τn}+∞\nn=1 such that limn→+∞τn = +∞and {¯θ(τn)}∞\nn=0 converges. Specifi-\ncally, for any ε > 0, suppose whenever n > Nε we have ∥¯θ(τn) −Θ∗∥< ε. Then, whenever\nn > Nε and τn > Tε hold at the same time, one has\nF(Θ∗) = F(Θ∗) −F(¯θ(τn)) + F(¯θ(τn))\n≤Bg∥Θ∗−¯θ(τn)∥+ F(¯θ(τn)),\nby MVT\n≤Bg∥Θ∗−¯θ(τn)∥+ µ + (Bg + 1)ε,\nby (28)\n< µ + (2Bg + 1)ε.\nSince ε > 0 can be arbitrarily small, the above inequality implies that F(Θ∗) ≤µ, which\nfurther implies (according to the definition of µ) that F(Θ∗) = µ. Hence, Θ∗∈H∗.\n15\nNext, we prove Eq. (27) by contradiction. Assume that Eq. (27) is false. Then, there\nexists ε0 > 0 and an increasing sequence {sn}∞\nn=0 ⊂R such that limn→+∞sn = +∞and\ninfθ∈H∗∥¯θ(sn) −θ∥> ε0. This implies that any limit point y∗of the set {¯θ(sn)}∞\nn=0, which\nexists since {¯θ(sn)}∞\nn=0 is bounded13, satisfies infθ∈H∗∥y∗−θ∥> 0. Therefore, y∗/∈H∗,\nwhich contradicts the previously derived result of (29). Hence, Eq. (27) holds and Theorem\n1 is proved.\nNow, it is sufficient to prove Eq. (28). For any ε > 0, define\nTε := 1 + C/∆ε + Sε,\nwhere (i) C := supθ∈U(0;B1)(F(θ) −µ); (ii) ∆ε := infθ∈Oε∥g(θ)∥2, where Oε := {θ ∈\nU(0; B1) | F(θ) ≥µ + ε}; (iii) Sε > 0 denotes the threshold such that whenever s ≥Sε the\nfollowing inequality holds:\nsup\nt∈[s,s+C/∆ε+1]\n∥¯θ(t) −θs(t)∥≤ε,\na.s.\nNote that, almost surely, Sε exists due to (16). We also note that ∆ε > 0 because Oε is\nbounded and closed14, ∥g(θ)∥2 is continuous since g(θ) is Lipschitz, and ∥g(θ)∥2 is positive\non Oε since Oε∩H∗= ∅15 and H∗contains all the critical points of F in U(0; B1)) according\nto Assumption 3.\nWhenever t > Tε, define s := t −C/∆ε −1.\nF(¯θ(t)) = F(¯θ(t)) −F(θs(t)) + F(θs(t)),\n≤|F(¯θ(t)) −F(θs(t))| + F(θs(t))\n≤Bg∥¯θ(t) −θs(t)∥+ F(θs(t)),\nby MVT\n≤Bg∥¯θ(t) −θs(t)∥+ µ + ε,\nIt will be justified later\n(A)\n= Bg∥¯θ(s + C/∆ε + 1) −θs(s + C/∆ε + 1)∥+ µ + ε,\nby definition of s\n≤Bg\nsup\nτ∈[s,s+C/∆ε+1]\n∥¯θ(τ) −θs(τ)∥+ µ + ε,\n≤Bgε + µ + ε,\nsince s = t −C/∆ε −1 > Tε −C/∆ε −1 = Sε,\n= µ + (Bg + 1)ε.\nThis is (28). The final step is to justify the inequality (A) above, which is done below.\nWe prove the inequality F(θs(t)) ≤µ + ε in (A) by contradiction.\nAssume that\nF(θs(t)) > µ + ε. This implies the following inequality for any u ∈[s, t]:\nF(θs(u)) ≥F(θs(t)) ≥µ + ε,\nwhich follows from the fact\ndF(θs(u))\ndu\n= −∥g(θs(u))∥2 ≤0 since by definition ˙θs(u) =\ng(θs)(u) for any u ≥s. The above inequality implies that for any u ∈[s, t],\nθs(u) ∈Oε ⇒∥g(θs(u))∥2 ≥∆ε.\n13From Assumption 2, {¯θ(t)}t≥0 ∈U(0; B1).\n14This set Oε is closed because any limit point Θ∗of this set belongs to the closed set U(0; B1) and\nsatisfies F(Θ∗) ≥µ + ε as a result of the continuity of F, which implies that Θ∗∈Oε.\n15Recall that in Section ?? it was proved that H∗= {θ ∈U(0; B1) | F(θ) = µ}.\n16\nHence,\nF(θs(t)) −F(θs(s)) =\nZ t\ns\ndF(θs(u))\n=\nZ t\ns\n−˙θsT(u)g(θs(u))ds,\nT denotes matrix transpose\n=\nZ t\ns\n−gT(θs(u))g(θs(u))du\n= −\nZ t\ns\n∥g(θs(u))∥2ds\n≤−∆ε(t −s)\n< −C,\nfrom the definition of s.\nhave hence\nF(θs(t)) < F(θs(s)) −C = F(θs(s)) +\ninf\nθ∈U(θ;B1)\n(µ −F(θ)) ≤µ,\nwhich is a contradiction, since F(θ) ≥µ for all θ ∈U(0; B1) and θs(t) ∈U(0; B1) because\nof Assumption 4. This completes the justification for (A), the proof of Eq. (28), and thus\nthe proof of Theorem 1.\n4\nReinforcement Learning for Inverse Problems\nIn this section, we study the theory of applying REINFORCE-OPT to solve the inverse\nproblem (4). To convert (4) to the maximization problem (1), there are multiple options\nfor defining the objective L(·). For example, one can simply set\nL(x) := −∥f(x) −yδ∥2 −αΩ(x).\n(30)\nUnder the above definition of L and the conditions required by Theorem 1, the goal (6)\nof reinforcement learning is equivalent to solving\nθ∗= arg min\nθ∈Rd{Ex∼µπθ\n\u0002\n∥f(x) −yδ∥2 + αΩ(x)\n\u0003\n+ β∥θ∥2},\n(31)\nwhere recall that µπθ denotes the stationary distribution of the Markov chain {xt} produced\nby following16 πθ. In what follows, through an investigation of πθ∗, we reveal the connection\nbetween the RL-based method and the classical regularization methods to solve ill-posed\ninverse problems.\n4.1\nReinforcement-learning Goal (31) Yields Tikhonov Regular-\nization\nWe draw a connection between REINFORCE-OPT and Tikhonov regularization. For sim-\nplicity, we focus on linear inverse problems in the finite-dimensional case, i.e. Ax+δξ = yδ,\nwhere x ∈Rp, yδ ∈Rq, and A : Rp →Rq is a linear operator for some p, q ∈Z+. Specifi-\ncally, the Tikhonov regularization aims to solve\nmin\nx∈Rp ∥Ax −yδ∥2 + α∥x∥2,\n(32)\n16xt+1 = xt + at where at ∼πθ(·|xt), for any non-negative integer t.\n17\nwhere α ≥0 is a regularization coefficient. For a linear operator A, the solution obtained\nusing the Tikhonov regularization method has a closed-form expression: x∗= (ATA +\nαI)−1ATyδ. In this subsection, we demonstrate that if πθ is set to a specific form, the\nmean of the limit distribution of {xt}∞\nt=1 equals x∗, where the sequence {xt}∞\nt=1 is generated\nfollowing πθ∗.\nTheorem 2. Let θ∗be the solution to (31) under the following settings: f(x) := Ax,\nβ := 0, Ω(x) := α∥x∥2, and πθ is modeled as\nπθ(·|x) := N(θ −x, Σ),\nfor any x ∈S := Rq,\n(33)\nwhere N denotes the Gaussian distribution and Σ ∈Rq×q is any symmetric positive definite\nmatrix.\nThen the mean of µπθ∗equals the solution obtained by applying the Tikhonov\nregularization method to (4).\nProof. With the form of πθ in Eq. (33), the invariant distribution µπθ is N(θ, Σ) (the proof\nis postponed to the end of this part). With this information and the conditions assumed\nin the theorem, we rewrite (31) as follows:\nθ∗= arg min\nθ Ex∼µπθ[∥Ax −yδ∥2 + α∥x∥2]\n= arg min\nθ ∥AEx −yδ∥2 + E∥Ax −AEx∥2 + αE∥x −Ex∥2 + α∥Ex∥2\n= arg min\nθ ∥Aθ −yδ∥2 + Trace(AΣAT) + αTrace(Σ) + α∥θ∥2,\nwhere17 the expectation E is taken with respect to x ∼µπθ. Then, the first-order condition\nimplies\nθ∗= (ATA + αI)−1ATyδ.\n(34)\nSince the invariant distribution µπθ∗of the Markov chain {xt} generated by πθ∗is N(θ∗, Σ),\nthe mean of xt’s limiting distribution as t →+∞is θ∗= (ATA+αI)−1ATyδ, which equals\nthe Tikhonov regularization solution to the above linear inverse problem.\nNow, it remains to prove that with πθ defined in (33), the invariant distribution µπθ of\n{xt} generated by πθ is the Gaussian distribution N(θ, Σ). According to the knowledge on\nMarkov chains (e.g., [16, Section 1.4]), the invariant probability measure µπθ satisfies the\nfollowing two equations:\n\u001a R\nx∈Rp µπθ(dx) = 1,\nµπθ(B) =\nR\nx′∈Rp pπθ(B|x)µπθ(dx),\nfor any Borel set B ⊂Rp,\n(35)\nwhere pπθ denotes the transition kernel of the Markov chain {xt}, and pπθ(B|x) is the\nprobability for the next state x′ = x + a to be in B conditional on the current state x.\nBy (33), a ∼N(θ −x, Σ), which implies x′ ∼N(θ, Σ). Hence, pπθ(B|x) = µN(θ,Σ)(B) for\nany x ∈Rp, where µN(θ,Σ) denotes the Gaussian probability measure. Then, for any Borel\nB ⊂Rp, by Eq. (35),\nµπθ(B) =\nZ\nx∈Rp pπθ(B|x)µπθ(dx) = µN(θ,Σ)(B)\nZ\nx∈Rp µπθ(dx) = µN(θ,Σ)(B).\nThis proves that µπθ is the probability measure of the Gaussian distribution N(θ, Σ).\n17We explain the last equality.\nLet z := Ax −AEx ∈Rq.\nThen, E∥Ax −AEx∥2 = E\n\u0002\nzTz\n\u0003\n=\nE\n\u0002\nTrace(zzT)\n\u0003\n= Trace\n\u0000E[zzT]\n\u0001\n= Trace\n\u0000AVar(x)AT \u0001\n= Trace\n\u0000AΣAT \u0001\n.\n18\n4.2\nReinforcement-learning Formulation (31) Yields Iterative Reg-\nularization\nConsider again the same finite-dimensional linear inverse problem, Ax + δξ = yδ, as in the\nprevious subsection. Our goal is to demonstrate that when πθ also follows a Gaussian dis-\ntribution with a carefully designed mean, the updates of {xt} under πθ∗become equivalent\nto certain well-known (stochastic) iterative schemes in the field of inverse problems.\nTheorem 3. Let θ∗denote the solution to (31) under the following settings: f(x) := Ax,\nβ := 0, and πθ is modeled as\nπθ(·|x) := N(θ −Bx, σ2I),\nfor any x ∈S := Rq,\n(36)\nwhere B := ω(ATA + ϵI), ω ∈(0,\n1\n3(∥A∥2+ϵ)), I represents the identity matrix in Rq×q, and\n∥A∥denotes the spectral norm of A. Then the iteration rule by πθ∗is the same as\n• the conventional Landweber method: xt+1 = xt + ωAT(yδ −Axt), if ϵ = α = σ = 0\nand ATA is invertible;\n• the stochastic Landweber method [63, Formulas (5) and (80)] and [64, Formula (6)]:\nxt+1 = xt + ωAT(yδ −Axt) + σzt, if ϵ = α = 0 and σ ̸= 0 and ATA is invertible;\n• the (stochastic) Krasnoselskii-Mann’s acceleration method [65, Formula (6)]: xt+1 =\n(1 −ωϵ)xt + ωAT(yδ −Axt) + σzt, if ϵ = α ̸= 0.\nProof. In the following proof, starting from (36), we derive the expression for µπθ and\nsubstitute it into (31) to obtain the solution θ∗in (40). Next, in (41), we write the transition\nrule using πθ∗and demonstrate that it is equivalent to the (stochastic) Landweber iteration\nunder the conditions specified in the theorem.\nFrom (8) and (36), the transition rule in the trajectory generated by πθ is (recall that\nat ∼πθ(·|xt))\nxt+1 = xt + at = θ + (I −B)xt + σzt,\n(37)\nwhere zt ∼N(0, I) is independent of xt. This implies\nxt =\nt−1\nX\ni=0\n(I −B)iθ +\nt−1\nX\ni=0\n(I −B)iσzt−i−1 + (I −B)tx0.\n(38)\nThis equality and the fact {zt} being independent N(0, I) imply that xt follows N(µt, Σt)\ngiven that x0 is a fixed vector, where\nµt :=\nt−1\nX\ni=0\n(I −B)iθ + (I −B)tx0,\nand\nΣt :=\nt−1\nX\ni=0\n(I −B)iσI\n\u0000(I −B)iσI\n\u0001T = σ2\nt−1\nX\ni=0\n(I −B)2i.\nSince ω ∈(0,\n1\n3(∥A∥2+ϵ)), we have18 ∥I −B∥< 1. Hence,\nlim\nt→∞µt = B−1θ and lim\nt→∞Σt = σ2(2B −B2)−1.\n18∥B∥≤ω(∥A∥2+ϵ) < 1/3. Also, by definition B is symmetric and positive definite. Hence, ∥I −B∥< 1.\n19\nThis implies, according to Levy’s theorem for characteristic functions (e.g., [5, Theorem\n26.3]), that the limiting distribution µπθ of xt as t →∞is Gaussian, with its mean equal\nto B−1θ and its variance equal to σ2(2B −B2)−1, namely\nµπθ = N(B−1θ, σ2(2B −B2)−1).\n(39)\nNow we derive an expression for the solution θ∗to (31) when Ω(x) = α∥x∥2 and β = 0.\nWe rewrite (31) as\nmin\nθ Ex∼dπθ[∥Ax −yδ∥2 + α∥x∥2]\n= min\nθ ∥AEx −yδ∥2 + E∥Ax −AEx∥2 + αE∥x −Ex∥2 + α∥Ex∥2\n= min\nθ ∥AEx −yδ∥2 + Trace\n\u0000A · Var · (x)AT\u0001\n+ αTrace (Var · (x)) + α∥Ex∥2\n= min\nθ ∥AB−1θ −yδ∥2 + σ2Trace\n\u0000A(2B −B2)−1AT\u0001\n+ ασ2Trace\n\u0000(2B −B2)−1\u0001\n+ α∥B−1θ∥2,\nby (39).\nFrom the first-order necessary condition for optimality, the solution θ∗is\nθ∗=\n\u0000B−1ATAB−1 + αB−2\u0001−1 B−1ATyδ.\n(40)\nPlugging this result to (37) gives the iteration rule in the trajectory generated by πθ∗:\nxt+1 = xt + at = xt + θ∗−Bxt + σzt\n= xt +\n\u0000B−1ATAB−1 + αB−2\u0001−1 B−1ATyδ −Bxt + σzt\n= xt +\n\u0000B−1(ATA + αI)B−1\u0001−1 B−1ATyδ −Bxt + σzt\n= xt + B(ATA + αI)−1ATyδ −Bxt + σzt\n= xt + ω(ATA + ϵI)\n\u0002\n(ATA + αI)−1ATyδ −xt\n\u0003\n+ σzt,\n(41)\nwhich leads to the proposed iterative regularization schemes for different choices of param-\neters ϵ, α and σ.\nBefore we close this section, let us now highlight the essential contributions of this work:\n• Theoretical contributions to RL-based methods: This work advances the theoretical\nunderstanding of RL-based methods for solving potentially nonconvex optimization\nproblems with continuous solution spaces.\nTo our knowledge, we are the first to\nestablish the equivalence between the complex objective of RL-based methods and the\nstochastic version (2) of the optimization goal, and the first to provide a theoretical\nconvergence analysis for RL-based optimization methods. In Theorem 1, we rigorously\nprove that, under standard assumptions, REINFORCE-OPT generates a sequence\n{θt}∞\nt=0 that almost surely converges to a locally optimal value. Our proof employs\nthe ordinary-differential-equation method (e.g., [9] and [7, Section 8.1]).\n• Introduction of RL to inverse problems: This work rigorously introduces an RL al-\ngorithm to the field of inverse problems and, for the first time, connects RL with\nclassical regularization methods (both variational regularization and iterative regu-\nlarization) for solving general inverse problems (3), as discussed in Section 4. Existing\nresearch applying RL to inverse problems is limited and primarily focuses on empir-\nical applications. For example, the authors in [56] use deep RL19 to solve a physical\n19Deep RL leverages deep neural networks as function approximators in RL algorithms. See [42] for more\ndetails.\n20\nchemistry problem involving the inversion of nuclear magnetic resonance spectra for\nmolecular structures. The authors in [15] design an inversion algorithm based on Q-\nlearning for geoscience applications, while the authors in [53] show how to transform\nBayesian sequential optimal experimental design, a method for solving inverse prob-\nlems, into a problem solvable by RL. [62] applies RL to jointly learn good samplers\nand reconstructors for MRI inverse problems.\n• Practical applicability: Our numerical results given in Section 5 indicate that REINFORCE-\nOPT is capable of solving inverse problems of the form (3) involving complex functions\n(i.e. non-linear mappings without explicit formula) f, showcasing its practicality and\nsuccess as a method for inverse problems.\n5\nNumerical experiments\n5.1\nPerformance of REINFORCE-OPT for Continuous Optimiza-\ntion\nIn this section, we illustrate the advantages of REINFORCE-OPT over several mainstream\noptimization methods through experiments. For ease of visualization, most of the experi-\nments are conducted in one- or two-dimensional x spaces, without loss of generality.\nFor the purpose of illustrating these advantages, we report quantities obtained in the\ntraining process of the algorithm, rather than the output stated in Steps 3 and 4 in Al-\ngorithm 2. For example, to show that the algorithm may be able to escape from local\noptimums, we report the path {xt}T−1\nt=0 produced by the trained πθ. To show that the al-\ngorithm is more robust than GA and PSO against the choice of initial x0 (GA and PSO\nchoose an initial population, i.e., a group of x0-values), for a proper comparison, we report\nthe best solution candidate found during the training of REINFORCE-OPT in Algorithm\n1.\n5.1.1\nEscape from Local Optimum\nIn this experiment, the problem to solve is maxx∈R L(x) with L(x) = −(x2 −1)2 −0.3(x −\n1)2 + 5, where the starting value is fixed at x0 = −1.5.\nWe follow (13) to solve this\nproblem. For a comparison with the gradient ascent algorithm in a one-dimension case, we\nset the action space as {−1, 0, 1}, and construct the action distribution πθ(·|x) as a neural\nnet whose output consists of the probability of each action. The hyper-parameters of this\nneural net are selected by the commonly used validation-set approach (i.e., from a pre-\nselected candidate set, the set of hyper-parameter values that lead to the best performance\nare selected). For example, the selected hidden layer structure is (10,5). The number of\nθ-updates is set as 3,500.\nAlso, since gradient ascent involves the quantity of a step size γ, for a better comparison,\nwe add this quantity to REINFORCE-OPT by modifying the MDP iteration rule (8) as\nxt+1 = xt + γat, where we set γ = 0.1. With the trained parameter value ˆθ∗, we generate\na path {xt}35\nt=1 using π ˆθ∗.\nThat is, xt+1 = xt + γat, where at equals the mode of the\ndistribution π ˆθ∗(·|xt). The results are plotted in Figure 1, which illustrates the ability of\nREINFORCE-OPT to escape from the local maximum to the global maximum of x = 1.0.\nThe performance of the gradient ascent (performed 35 steps) is also plotted. As the graph\nshows, its search is trapped in the local maximum.\n21\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\nx\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nL(x)\nREINFORCE- OPT\nL(x)\n=\n−\n(x\n2\n−\n1)\n2\n−\n0.3(x\n−\n1)\n2\n+\n5\n(a) REINFORCE Trajectory\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\nx\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nL(x)\nGradient Ascent\nL(x)\n=\n−\n(x\n2\n−\n1)\n2\n−\n0.3(x\n−\n1)\n2\n+\n5\n(b) Gradient-Ascent Trajectory\nFigure 1: Illustration: REINFORCE-OPT escapes from the local minimum. It seems that\nthe gradient ascent agent moves fewer steps than REINFORCE-OPT. But in fact it keeps\nmoving back and forth around the local minimum.\n10\n0\n10\n1\nt\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nL(xt\n)\nREINFORCE- OPT\nGradient Ascent\n(a) The 1D Case\n100\n101\nt\n2\n4\n6\n8\n10\nL(xt)\nREINFORCE-OPT\nGradient Ascent\n(b) The 2D Case\nFigure 2: The fitness trajectory of the two methods in Figure 1, where the fitness at step t\nis L(xt).\nNext, we repeat the experiment with a more complicated objective that resembles Eq.\n(78) in [18], where we set the action space as {\n(a,b)\n√\na2+b2+10−6|a, b ∈{−1, 0, 1}} and γ = 0.1.\nHence, each x-step is normalized to have a size of 0.1. The result is shown in Figure 3,\nwhich again verifies the ability of REINFORCE-OPT to escape from the local optimum.\n22\nx1\n−1.0\n−0.5\n0.0\n0.5\n1.0\nx2\n−1.0\n−0.5\n0.0\n0.5\n1.0\n−2\n0\n2\n4\n6\n8\n(a) Objective: L(x1, x2) = 8 + cos(10x1) +\ncos(10x2) −5(x2\n1 + x2\n2).\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx1\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx2\nGlobal Max\n0.0\n0.0\n0.0\n0.0\n2.0\n2.0\n2.0\n2.0\n4.0\n6.0\n6.0\n6.0\n6.0\n6.0\n8.0\n8.0\n8.0\n8.0\n8.0\nREINFORCE-OPT\nGradient Ascent\n(b) Searc h Trajectory in the contour plot of L(x).\nNote that x-points on the same curve have equal\nL values.\nFigure 3: Illustration: REINFORCE-OPT escapes from the local minimum.\n5.1.2\nRobustness Against Initial Values\nIn this subsection, we compare REINFORCE-OPT (13) with other three popular global\noptimization methods, the genetic algorithm [21], the particle swarm optimization [47],\nand the cross-entropy method for optimization [10, Algorithm 4.1]. All four algorithms\nstart from an initial value x0 (or an initial population), and iteratively decide on the next\ngroup of x-values to be examined according to a search heuristic.\nEach such group is\ncalled a generation. For REINFORCE-OPT, more specifically, we refer a generation to\n{xl\nt : t ∈{0, 1, 2, ..., T −1}, l ∈{0, 1, 2, ..., L −1}}. This generation is used to compute\nˆ∇JT(θ) in (12), the term used for one update of θ. Also, the search heuristic πθ(·|x) for\nREINFORCE-OPT is modeled as a multivariate normal distribution with its mean and\nstandard deviation equal to the output of a neural net Nθ(x).\nThe selections of the initial x values are difficult to make. Therefore, the robustness\nof the method’s performance against the choice of initial values is important. We com-\npare the robustness of three methods through solving a simple but representative problem:\nmaxx∈Rk L(x) with\nL(x) := −ln((x −m1)2 + 0.00001) −ln((x −m2)2 + 0.01),\nwhere all the entries of m1 ∈Rk are set as −0.5, and all the entries of m2 ∈Rk are set as\n0.5. The graph of L when k = 2 is plotted in Figure 4, which shows that L has a global\nmaximum at m1 and a local maximum at m2. For each k ∈{2, 6} and each method,\nwe perform two experiments with varied x0 values (or varied initial populations). Table 2\nreports the hyper-parameters’ values (selected by the validation-set approach) for one20 of\nthe two experiments. The hyper-parameter values for other experiments can be found in\nour code, i.e. http://github.com/chen-research/REINFORCE-OPT.\n20In Table 2, the second column specifies which one of the two experiments the reported hyper-parameters’\nvalues correpond to.\n23\nx1\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx2\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n−1\n0\n1\n2\n3\n4\n5\n6\nFigure 4: The graph of L(x) := −ln((x −m1)2 + 0.00001) −ln((x −m2)2 + 0.01) for the\n2D case.\nFor each pre-selected initial value (or initial population), we perform one of the four\ninvestigated methods, and report the L-value of the best solution candidate found among\nall the generations. Specifically, let {xn} denote the best solution candidate found up to\nthe nth generation. We report x∗= arg maxn L(xn) in Tables 3 and 4. The results indicate\nthat if the range of the initial population does not cover the global maximum, GA will\nbe trapped in the local maximum. PSO seems unaffected by the selection of the initial\npopulation if k = 2. However, this robustness disappears when the dimension increases a\nlittle to k = 6. On the contrary, REINFORCE-OPT is able to find solutions close to the\nglobal maximum in both of the two cases, regardless of the location of the initial value x0,\neven if x0 is much closer to the local maximum m2 that to the global maximum m1. This\nindicates that the robustness of REINFORCE-OPT is stronger than that of GA and PSO.\n5.2\nExperiments on Inverse Problems\nIn this section, we apply REINFORCE-OPT (i.e., Algorithm 2) to solve two typical non-\nlinear inverse problems. In these two examples, we assume that a signal yδ in (3) is observed.\nWe set the objective function as\nL(x) :=\n1\n∥f(x) −yδ∥2 + αΩ(x) + 0.001,\n(42)\nwhere 0.001 is added to prevent the denominator from being a zero. We use this objective\nrather than (30) because it works better in our experiments. As for the functional form\nof the policy, we set πθ(·|x) as a multivariate Gaussian distribution with its mean and\ncovariance equal to the output of a neural network Nθ with input x and weights θ.\n24\nTable 2: Hyper-parameters for one of the two experiments performed for each method when\nk = 6.\nMethod\nThe initial popu-\nlation or x0.\nThe hyper-parameters’ values.\nREINFORCE-\nOPT (13)\nEach\nentry\nof\nx0 is uniformly\nsampled\nfrom\nthe\ninterval\nof\n[0.2, 0.3].\nHidden layers:\n(18,18,18).\nActivation:\ntanh. L = 60. T = 25. an =\n5\n50,000+n.\nGenetic\nAlgorithm\n[21]\nEach\nentry\nof\nany candidate in\nthe initial pop-\nulation\nis\nuni-\nformly\nsampled\nfrom [0,1].\nNumber of mating-parent pairs = 20.\nGene mutation percent= 10%. Crossover\ntype: single point.\nParticle\nSwarm\nOptimiza-\ntion [47]\nEach\nentry\nof\nany candidate in\nthe initial pop-\nulation\nis\nuni-\nformly\nsampled\nfrom [0,1].\nCognitive parameter c1 = 0.2.\nSocial\nparameter c2 = 0.2.\nInertia parameter\nw = 0.5.\nCross-\nentropy\nMethod\n[10,\nAlgo-\nrithm 4.1]\nEach\nentry\nof\nany candidate in\nthe initial pop-\nulation\nis\nuni-\nformly\nsampled\nfrom [0,1].\nN e = 30, where N e denotes the number\nof top solution candidates in a generation\nused to update the Gaussian mean µ and\nstandard deviation σ.\nα = 0.9, which\nis a weight used in the update (µ′, σ′) =\n(µ, σ) + α((µ, σ)).\nTable 3: Experiment Results When k = 2\nInitial Value or Population\nBest Solution\nCandidate x∗\nf(x∗)\nREINF.\nOPT.\nx0 = [0, 0].\n(−0.500, −0.500)10.752\nx0 ∼uniform[0.2, 0.3].\n(−0.500, −0.501)10.761\nGenetic\nAlgo.\nFor any x\n∈\nP0, x\n∼\nuniform[−1, 1].\n(−0.499, −0.502)10.266\nFor any x ∈P0, x ∼uni-\nform [0, 1].\n(0.494, 0.494)\n3.917\nPart.\nSwarm\nFor any x\n∈\nP0, x\n∼\nuniform[−1, 1].\n(−0.500, −0.500)10.815\nFor any x\n∈\nP0, x\n∼\nuniform[0, 1].\n(−0.500, −0.500)10.815\nCross-\nEntropy.\nFor any x\n∈\nP0, x\n∼\nuniform[−1, 1].\n(−0.500, −0.500)10.815\nFor\nany\nx\n∈\nP0,\nx ∼uniform[0, 1].\n(0.495, 0.495)\n3.917\n25\nTable 4: Experiment Results When k = 6\nInitial Value or Popula-\ntion\n∥x∗−m1∥2\n6\n∥x∗−m2∥2\n6\nf(x∗)\nREINF.-\nOPT\nx0 = [0, 0, ..., 0] ∈R6.\n6.776\n×\n10−4\n0.990\n3.720\nx0 ∼uniform[0.2, 0.3].\n8.533\n×\n10−4\n0.978\n3.501\nGenetic\nAlgo.\nx\n∼uniform[−1, 1]\nfor\nany x ∈P0.\n3.963\n×\n10−6\n1.000\n8.503\nx ∼uniform[0, 1] for any\nx ∈P0.\n0.998\n3.747\n×\n10−5\n0.123\nPart.\nSwarm\nx\n∼uniform[−1, 1]\nfor\nany x ∈P0.\n1.638\n×\n10−3\n0.926\n2.905\nx ∼uniform[0, 1] for any\nx ∈P0.\n0.997\n2.796\n×\n10−6\n2.815\nCross-\nEntropy\nx\n∼uniform[−1, 1]\nfor\nany x ∈P0.\n2.769\n×\n10−6\n1.000\n9.720\nx ∼uniform[0, 1] for any\nx ∈P0.\n0.997\n2.796\n×\n10−6\n2.815\n5.2.1\nAuto-convolution Equation\nIn the first group of numerical simulations, we consider the following 1D auto-convolution\nequation,\nZ t\n0\nx(t −s)x(s) ds = y(t).\n(43)\nwhich has many applications in spectroscopy (e.g., the modern methods of ultrashort laser\npulse characterization) [1, 22], the structure of solid surfaces [14], and nano-structures [20].\nTo the best of our knowledge, the results regarding the existence of x(t) given y(t) for the\nnon-linear integral equation (43) are quite limited. Hence, to avoid the possibility that\nthere does not exist any x(t) corresponding to a randomly chosen y(t), for our experiment\nwe pre-define\nx(t) = 10t(1 −t)2,\n(44)\nand let y(t) be the corresponding right-hand side in (43).\nWe test the ability of REINFORCE-IP to numerically solve a discretized version of (43)\ngiven noised y(t). Let f be the map from x = [x(0), x(\n1\nD−1), x(\n2\nD−1), ..., x(1)] ∈RD, the\ndiscretization of {x(t)}t∈[0,1], to y = [y0, y1, ..., yD−1] ∈RD, a discrete-time approximation\nto {y(t)}t∈[0,1], where (x(t), y(t)) satisfy Eq. (43). Here, D ∈Z+ denotes the number of\ndiscrete points, and we set it as D = 64 in the experiment. The approximation y = f(x)\nis computed in the following way21: y0 = 0, and for any j ∈{1, 2, ..., D −1},\nyj =\n1\nD −1\nj\nX\ni=1\n1\n2 (x (tj −ti) x (ti) + x (tj −ti−1) x (ti−1)) ,\n21From Eq. (43), for any j ∈{1, 2, ..., D −1}\ny(tj) =\nj\nX\ni=1\nZ ti\nti−1\nx(tj −s)x(s)ds ≈\n1\nD −1\nj\nX\ni=1\n1\n2 (x(tj −ti)x(ti) + x(tj −ti−1)x(ti−1)) = yj.\n26\nwhere tj := j/(D −1). The inverse problem we will use REINFORCE-IP to solve is\nf(x) + ϵ = yδ,\n(45)\nwhere yδ = f(xe) + ϵ, ϵ is mean-zero Gaussian noise, and\nxe = [x(0), x(\n1\nN −1), x(\n2\nN −1), ..., x(1)] ∈RD\n(46)\nwith x(t) defined in Eq. (44) with D = 64. This inverse problem has a unique solution xe\nin U + := {x ∈RD : x has non-negative entries}, as implied by the following theorem on\n(43), which further implies there are in total two solutions to (45) in RD, xe and −xe.\nTheorem 4. (i) (Weak uniqueness [22]) For y ∈L2[0, 2], the integral equation has at most\ntwo different solutions x1(t), x2(t) ∈L1[0, 1]. Moreover, x1(t) + x2(t) = 0, t ∈[0, 1] holds.\n(ii) (Uniqueness [23]) Define\nS+ : = {x ∈L1[0, 1] : x(t) ≥0},\nS+\nϵ : = {x ∈L1[0, 1] : ϵ = sup{s : x(t) = 0, a.e. in [0, s]}},\nY +\nϵ\n: = {y ∈L2[0, 2] : ϵ = sup{s : y(t) = 0, a.e. in [0, s]}}.\nThen, for y ∈Y +\n0 , the equation (43) has at most one solution in S+. Moreover, the solution\nbelongs to S+\n0 .\nNext, we state some common assumptions and settings for the REINFORCE-OPT\nalgorithm applied in all our later experiments to solve (45). Assume that instead of the\nexact signal f(xe), a set {yδ\nk}100\nk=1 of noised observations of the right-hand side in (45)\nare available, and these observations are generated according to yδ\nk := f(xe) + ϵk, where\nϵk ∼N(0, Σk) is the Gaussian noise whose covariance Σk is a diagonal matrix. The square\nroots of the diagonals in Σk are set as 1%ye.\nWe construct the policy πθ(·|x), a distribution over the action space determined by the\ngiven state x, as a multivariate Gaussian distribution whose mean and standard deviation\nare transformed output22 from a feed-forward neural network Nθ with input x.\nAs a common practice in the literature of solving (45), assume that we are given the\ninformation that the first and last entry in the solution xe are all zero. To use this infor-\nmation, we set the regularizer in (42) as\nΩ(x) = |x0| + |xD−1|.\n5.2.2\nSimulation I\nSuppose we are given the additional information that there is a solution to (45) in U +.\nTo use it, we let the initial state in any trajectory generated by πθ be a fixed point in\nU +, which is randomly chosen as x0 = [0.01, 0.01, ..., 0.01] ∈R64. This setting increases\nthe likelihood of the RL algorithm finding the solution xe in U +, since x0 is closer to the\nsolution in U + (and hence easier to be found by the agent πθ) than any solution not in U +.\nThe experiment result shows that this setting is effective, since all the produced solution\nestimates are approximations of xe ∈U + (see Figure 6).\n22Let [z1, z2] be an output of the neural network Nθ(x). Then, we let the moving average of z1 with\na window length of 3 to serve as the mean of πθ(·|x), and log(1 + ez2) serve as the standard deviation.\nThe moving average is used to ensure the smoothness of the predictions of xe. The function log(1 + ez2) is\ncalled softplus and is a typical method to force outputs from a neural net to be positive.\n27\nWe follow the REINFORCE-IP algorithm, i.e., Algorithm 1, to repeatedly update θ,\nuntil the sum of rewards23 in a trajectory generated by the policy exceeds a threshold H0\nor the total number of θ-updates exceeds 20,000, whichever comes first. Intuitively, this\nprocess keeps decreasing the denominator in Eq. (42).\nThe value of the stop-training-threshold H0, T (the number of total time steps taken in\na trajectory), α, β (recall (6)), and hyper-parameters of the neural net Nθ are all selected\nthrough the validation-set approach. The selected values are reported below Figure 5, which\nplots the performance of πθ during the training process.\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nNumber of Times θ Gets Updated Update\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nSum of Rewards in a Generated T\nrajectory\nPerformance of the Evolved Policy\nFigure 5: Performance of the Evolving Policy for Simulation One. After every 100 updates\nof θ-update, we calculate the performance of πθ.\nFor model hyper-parameters, we set\nT = 10, α = 0.2, β = 0.125, and H0 = 4.0 (the performance threshold for stopping\ntraining). The hidden layers of Nθ are (64, 64, 64) with the activation function ReLU. The\nlearning rate is set as an :=\n0.001\n50,000+n.\nWe now investigate the ability of the trained policy πθ to find the solution to (45).\nWith πθ, we generate L = 10, 000 trajectories of length T = 10 and record the last state\nxT−1 in each trajectory (so that we have {xl,T−1}N\nl=1 and each xl,T−1 can be viewed as an\nestimated solution). The R2 between xe and the average ¯xT−1 of the N estimate is 85.3%,\nindicating that πθ is a successful policy for searching for a solution to the inverse problem.\nTheir graphs, as well as the corresponding forward signals f(xe) and f(ˆxT−1), are plotted\nin Figure 6. The shaded region bounded by the two dashed green curves is the estimated\n99%-confidence interval obtained by the method of bootstrapping24 (e.g., [49]). That is, we\nestimate that for each entry i ∈{1, 2, ..., D} this shaded region contains the ith entry of the\ntrue solution xe with a probability of 99%.\n5.2.3\nSimulation II\nIn our second group of simulations, we aim to find only one solution (any one of two\nsolutions) to the inverse problem (45) without the a priori information of solutions. Hence,\nas a common practice in iterative algorithms, we fix the initial state x0 at the origin 0 ∈RD.\nThe procedures for training and testing πθ are identical to those in Simulation One.\nThe history of πθ’s performance in the θ-updating process is reported in Figure 7, and the\n23Specifically, after every 100 updates of θ, we use πθ to generate L = 1, 000 trajectories {xl,t}T −1\nt=0 for\nl ∈{0, 2, ..., L −1}, take the mean of each step to get {¯xt}T −1\nt=0 , and compute the sum PT −1\nt=0 R(¯xt, 0). This\nsum is considered the performance of πθ.\n24Specifically, we bootstrap from the estimates {xl,T −1}1,000\nl=1\nfor B = 10, 000 times, compute the mean\nof each bootstrap copy, and obtain the 0.5%-percentile and 99.5%-percentile of these 10,000 means as the\nlower and upper bound for the estimated 99%-confidence interval.\n28\nFigure 6: The Estimations and True Values for Simulation I. ¯xT−1 is the mean of 10,000\nsolution estimates, and the region bounded by the two dashed curves of 99%-CI is the\nconfidence interval of the solution estimate obtained by bootstrapping. Since we are looking\nfor a solution in U +, negative entries in the lower bound of the 99%-CI are replaced with\nzeros.\nsolution estimates by the trained model πθ are plotted in Figure 8. The R2 statistics between\n−xe and the estimates average ¯xT−1 is 86.7%, indicating the success of RL algorithm in\nfinding a solution to (45).\nNote that in Figure 8 the agent πθ finds an approximation to −xe. However, as we\nrepeat this experiment more times (each experiment trains πθ independently), we end up\nwith an approximation to xe, at a chance of approximately 50%.\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nNumber of Times θ Gets Updated Update\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nSum of Rewards in a Generated T\nrajectory\nPerformance of the Evolved Policy\nFigure 7: Performance of the Evolving Policy for Simulation II. The procedures and hyper-\nparameter values are identical to those in Figure 5, except that α = 0.1 and the initial state\nx0 for each generated trajectory is fixed at 0 ∈RD.\n29\nFigure 8: The Estimations and True Values for Simulation II. The procedures for producing\nthe plots and the symbols are the same as those in Figure 5.\n5.2.4\nSimulation III\nIn the last group of simulations, we perform another experiment to illustrate that with\na proper setting of the initial state’s distribution, REINFORCE-IP is able to find good\napproximations of both of the two solutions without the need to retrain the model. Suppose\nwe are given the a priori information that one solution is near 3xe/4 and the other is near\n−3xe/4 (but we do not know that the true solutions are equal to 4/3 of the given vectors).\nWith this information, in the θ-updating process we set initial state x0 of each trajectory\nas either 3xe/4 or −3xe/4, each with a probability of 50%.\nFigure 9 shows the performance of the policy πθ that is updated repeatedly according\nto Eq.\n(13) and the performance is computed by the following procedure: (i) Use πθ\nto generated L = 1000 trajectories of length T = 10. (ii) The last-step state xT−1 in\neach trajectory is taken as an estimate of the solution, and hence we have 1000 estimates,\n{xl,T−1}1000\nl=1 .\n(iii) The K-mean algorithm is then applied to divide {xl,T−1}1000\nl=1\nto two\ngroups. Compute the mean of each group, denoted by ¯x1 and ¯x2. (4) The performance of\nπθ is computed as (R(¯x1, 0) + R(¯x2, 0))/2.\n30\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nNumber of Times θ Gets Updated\n0\n5\n10\n15\n20\n25\nSum of Rewards in a Generated T\nrajectory\nPerformance of the Evolved Policy\nFigure 9: Performance of the Evolved Policy for Simulation III. Compared with Simulations\nOne and Two, the main difference lies in the distribution of the initial state x0 and the\nmethod of calculating the performances of πθ. The threshold for stop training is set as\nH0 = 20.\nFigure 10: Upper: Uncertainty quantification of approximate solutions by REINFORCE-\nOPT; Lower: comparison between simulated right-hand side and the exact right-hand side.\nTo test the capacity of the trained policy to find solutions to (45), as in the previous\nexperiment, we generate L = 10, 000 trajectories of length T = 10 with the trained agent\nπθ. However, in this experiment the initial state x0 takes the value of 3xe/4 or −3xe/4,\neach with a probability of 50%. The last-step state xT−1 in each trajectory is taken as an\nestimate of the solution, and hence we have 10,000 estimates, {xl,T−1}10000\nl=1 . The K-mean\nalgorithm is then applied to divide {xl,T−1}10000\nl=1\ninto two groups. Then, for each group we\nuse the bootstrapping method (the same one used in the previous experiment) to estimate\nthe 99% confidence interval of each entry in our xe-estimate. The results are plotted in\nFigure 10, where the region bounded by the two dashed blue curves is the estimated 99%-\nconfidence interval based on estimates in group 2 (whose initial state is fixed at 0.75xe). The\nregion bounded by the two dashed green curves is for the sample means in group 1 (whose\ninitial state is fixed at −0.75xe). Hence, in accordance with the definition of a confidence\ninterval, each entry of the true solution lies in the shaded region with a probability of 99%.\nThe lower half of Figure 10 plots ye, f(¯x1), and f(¯x2), where ¯xi denotes the mean of\ngroup-i estimates, for i = 1 or 2. The R2-statistic between xe and ¯x1 is 97.7% and the R2-\n31\nstatistic between −xe and ¯x2 is 98.4%, both of which are high. This indicates the capability\nof the RL algorithm to produce all the solutions to the inverse problem (45).\n5.3\nParameter Identification in Nonlinear PDE of Liquid Chro-\nmatography\n5.3.1\nExperiment Background\nIn the final group of numerical experiments, we consider an inverse problem in mathemat-\nical physics where the forward mapping has no closed form. Specifically, we examine the\nfollowing mass balance equation for a two-component system in a fixed-bed chromatography\ncolumn with the Danckwerts boundary condition, a common model in column chromatog-\nraphy [66, 43]:\n\n\n\n\n\n\n\n\n\n∂Ci\n∂t + F ∂qi\n∂t + u ∂Ci\n∂x = Da\n∂2Ci\n∂x2 ,\nx ∈X ≡[0, L1], t ∈(0, T1]\nCi(x, 0) = gi(x),\nx ∈X, t = 0\nuCi(0, t) −Da\n∂Ci(0,t)\n∂x\n= uhi(t),\nx = 0, t ∈(0, T1]\nDa\n∂Ci(L1,t)\n∂x\n= 0,\nx = L1, t ∈(0, T1]\n,\n(47)\nwhere x is distance, t is time, and i = 1, 2 refers to the two components. C and q are the\nconcentration in the mobile and stationary phase, respectively, u = 0.1061 is the mobile\nphase velocity, F = 0.6667 is the stationary-to-mobile phase ratio, and Da = Lu/2Nx\n(Nx = 2000 denotes the number of theoretical plates) is the diffusion parameter. L1 = 10 is\nthe length of the chromatographic column, and T1 is an appropriate time point slightly larger\nthan the dead time of chromatographic time T0 = L1/u. In this work, we set T1 = 1.5T0.\nIn addition, g(x) = [0, 0]T is the initial condition and h(t) = [5; 5] · H(10 −t) (H(·) is the\nHeaviside step function) is the boundary condition, which describes the injection profile in\nthe experiment.\nFor the numerical experiments, we focus on the case when the adsorption energy distri-\nbution is bimodal, namely when the bi-Langmuir adsorption isotherm is adopted:\nq1 (C1, C2) =\naI,1C1\n1 + bI,1C1 + bI,2C2\n+\naII,1C1\n1 + bII,1C1 + bII,2C2\n,\n(48)\nq2 (C1, C2) =\naI,2C2\n1 + bI,1C1 + bI,2C2\n+\naII,2C2\n1 + bII,1C1 + bII,2C2\n,\nwhere subscripts I and II refer to two adsorption sites with different adsorption energy.\nFor convenience of notation, the collection of adsorption isotherm parameters is denoted\nby\nx = (aI,1, aII,1, bI,1, bII,1, aI,2, aII,2, bI,2, bII,2)T.\n(49)\nNext, we consider the structure of the measurement data.\nIn most laboratory and\nindustrial settings, the total response R(x, t) is observed at the column outlet x = L1, with\nR(x, t) =\n2\nX\ni=1\nCi(L1, t),\n(50)\nwhere C(x, t) is the solution of problem (47) with the bi-Langmuir adsorption-isotherm\nmodel (48), and Ci(L1, t) represents the concentration of the i-th component at the outlet\nx = L1. The exact collected data at time grid {tj}T1\nj=1(T1 = 800) at the outlet is denoted\nby\nye = {R(x, tj)}T1\nj=1.\n(51)\n32\nThe parameter-to-measurement map f: R8 →RT1\n+ can be expressed as\nf(x) = ye,\n(52)\nwhere the model function f is defined through (50)–(51). To be more precise, for a given\nparameter x, a bi-Langmuir adsorption-isotherm model can be constructed according to\nformula (48). The concentration in the mobile phase, denoted as C, can then be obtained\nby solving the PDE in (47). Finally, the experimental data can be collected using the\ndesigned sensor in accordance with the physical laws described in (50) and (51). The goal\nof this subsection is to estimate the adsorption isotherm parameters x from the time-series\ndata ye using the integrated mathematical model (52) and RL.\n5.3.2\nInverse Problem to be Solved by REINFORCE-OPT\nSince there is no analytical expression for the forward mapping f in (52), we approximate\nit using a neural network ˆf. It is trained using 25 160, 000 samples of (the eight parameters\nx, the injection profile h with two entries26, and the corresponding total response y), where\nthe samples of (x, h) are uniformly and randomly drawn from (0, 100)8 × (0, 30)2, and the\nsamples of y are computed using (x, h)-samples by the forward system described in the\nprevious subsection. The R2 score of ˆf on 40,000 testing samples is above 95%, indicating\nthat it is a good approximation of f.\nThe inverse problem we will use REINFORCE-OPT to solve is\nˆf(x, h) + ϵ = yδ,\n(53)\nwhere yδ = ˆf(xe, h) + ϵ, xe is randomly generated and its value is shown in Table 6, h is\nthe randomly generated projection profile that is considered to be a known parameter (the\nrandomly drawn value is [10, 21]), and ϵ is a mean-zero noise. Later, we write ˆf(x) instead\nof ˆf(x, h) since h is known and fixed.\nAs before, we assume that a set {yδ\nk}100\nk=1 of noised signals is available, where for each\nk, yδ\nk is obtained by first randomly shifting27 the entries in the vector ye = ˆf(xe, h) by one\nunit to obtain yshift\nk\n, after which we add an artificial noise to it:\nyδ\nk = yshift\nk\n+ ϵk,\nwhere ϵk is a vector of independent mean-zero Gaussian random variables with standard\ndeviations equal to 1%yshift\nk\n. According to [59], the shift error is one of the major errors\nin real-world experiments, and hence yshift\nk\nneeds to be taken into account. Moreover, in\nour simulations, all yδ\nk are re-arranged at the same time grids by using the spline technique\nbecause, in general, the time grids of the numerical solution of PDE (52) do not fit the time\ngrids of the observation data.\n5.3.3\nEexperiment and Results\nSince there is no prior information to use, the regularizer Ω(x) is set as 0. Conforming to\n(42), we define the reward function as\nL(x) :=\n1\n(f(x) −yδ)2/800 + 0.001,\n(54)\n25This feed-forward neural network ˆf is trained in the same way as Ffwn in [59, Section 3.1].\n26h is the finite analog of injection function h(t) in PDE (47).\n27Let ye := ˆf(xe) = [y0, y1, ..., y799]. Then, with a probability of 50%, yshift\nk\n= [y1, y2, ..., y799, 0] (shifted\nto the left for one unit). With a probability of 50%, yshift\nk\n= [0, y0, y1, ..., y798] (shifted to the right for one\nunit).\n33\nwhere 800 is the dimension of yδ.\nAs stated in Section 5.2.1, we construct the policy\nπθ(·|x) as a multivariate Gaussian distribution whose mean and standard deviation are\noutputs from a neural network Nθ. Values of the hyper-parameters are selected through\nthe validation-set approach and are reported in Table 5.\nTable 5: Structure of Nθ and Hyper-parameter Values\nT\nα\nHidden\nLayers-\nNodes\nActivationβ\nLearning Rate\nH0\n10 0.1 (128, 64, 16)\nrelu\n0.01\nan :=\n0.001\n100,000+n\n0.45\nWe repeatedly train the neural net until the sum of rewards in a trajectory generated\nby the policy stops improving28 or the total number of θ-updates exceeds 7,500, whichever\ncomes first. Figure 11 shows the performance of the policy πθ that is updated repeatedly\nin the training process.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nNumber of Times θ Gets Updated Update\n0.0\n0.1\n0.2\n0.3\n0.4\nSum of Rewards in a Generated T\nrajectory\nPerformance of the Evolved Policy\nFigure 11: Performance of the Evolved Policy πθ.\nAs with previous experiments, with the trained πθ, we generate L = 1000 trajectories\nof length T = 10, each starting from the fixed initial state [50, 50, ..., 50], and take the last\nstate xl,T−1 in each trajectory as an estimate of xe. The mean ¯xT−1 of these L estimates\nand the estimated 99% confidence interval obtained by bootstrapping from these estimates\nare reported in Table 6. From the table, it can be observed that the entries of ¯xT−1 are\nclose to xe, except entries 2, 4, and 5. These three entries in xe are beyond the estimated\n99% confidence interval. This occurs because this chromatography problem has multiple\nsolutions, and in this experiment REINFORCE-OPT finds one different from xe.\nThe forward signals, f(xe) and f(¯xT−1) (plotted in Figure 12), are close to each other\nwith an R2 between them as high as 91.8%, indicating that REINFORCE-IP is efficient for\nthis experiment.\n6\nConclusion and Outlook\nIn this work, we develop a new method called REINFORCE-OPT to solve general opti-\nmization problems, and we formally introduce this RL-based approach to the field of inverse\nproblems. Our theoretical findings provide a deeper understanding of RL-based optimiza-\ntion methods and their connection to classical regularization techniques for solving general\n28Specifically, we log the sum of rewards in a trajectory every 100 updates of θ. If the logged value does\nnot improve for 20 logs, we stop training.\n34\nTable 6: Estimations by REINFORCE-IP. ¯xT−1 represents the mean of xe-estimates. The\nrow of CI-LB stores the lower bounds of the 99% confidence interval, and the row of CI-UP\nstores the upper bounds of the 99% confidence interval. Column i corresponds to the ith\nentry, for i ∈{1, 2, ..., 8}.\n1\n2\n3\n4\n5\n6\n7\n8\nxe\n50.0\n43.0\n48.0\n40.0\n59.0\n50.0\n51.0\n49.0\n¯xT−1\n48.1\n49.7\n48.6\n50.0\n55.3\n48.9\n54.2\n50.3\n99%CI-\nLB\n42.5\n43.8\n42.9\n43.9\n49.8\n42.9\n49.0\n44.4\n99%CI-\nUB\n53.6\n55.8\n54.7\n56.6\n61.1\n54.7\n59.6\n56.0\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n50\n100\n150\nf(x\ne\n)\nf(\n̄\nx\nT\n−\n1\n)\nFigure 12: The Estimations and True Values\ninverse problems. Numerical experiments demonstrate that REINFORCE-OPT is capable\nof escaping local optima in the solution space (see Figures 1, 2, and 3), is robust to the\nchoice of initial solution candidates (see Tables 3 and 4), and can quantify uncertainty in\nerror estimates while identifying multiple solutions in ill-posed inverse problems (see Figure\n10).\nThe developed REINFORCE-OPT can also be applied readily to discrete optimization\nproblems by selecting appropriate state and action spaces. Given its success with continuous\noptimization, we believe that it is well-suited to handle discrete optimization problems\ninvolving large state spaces, a challenge for traditional methods.\nIt should be noted that REINFORCE is one of the early RL algorithms to use function\napproximators such as πθ, making it powerful when dealing with infinite state spaces.\nHowever, its reliance on a Monte Carlo mechanism leads to high variance in the estimate\nˆ∇Jθ, resulting in a slow learning process, even with the use of a baseline [58, Section\n13.4 and 13.5]. This limitation is addressed by the more advanced actor-critic methods\n[8, 36, 38, 37, 60, 32, 58], where the “actor” refers to πθ and the “critic” refers to a function\napproximator for JT(θ). To our knowledge, the performance of these actor-critic methods\nfor general inverse problems of the type in (3) remains unexplored and is a topic for our\nfuture investigation.\n35\nAcknowledgements\nThis work was supported by the National Key Research and Development Program (2022YF-\nC3310300), the National Natural Science Foundation of China (12171036, 12471295 and\n12426306), the Beijing Foundation for Natural Sciences (Z210001).\nAppendices\nAppendix A Auxiliary Lemmas\nLemma 3. (Gronwall inequality, [7, Lemma B.1]) For continuous u(·), v(·) ≥0 and scalars\nC, K, T ≥0, the condition\nu(t) ≤C + K\nZ t\n0\nu(s)v(s)ds\nfor all t ∈[0, T]\nimplies\nu(t) ≤CeK\nR T\n0 v(s)ds, t ∈[0, T].\nFrom the proof given in [7, Lemma B.1]), Lemma 3 still holds when C is dependent on\nT.\nLemma 4. (Discrete Gronwall Inequality, [7, Lemma B.2]) Let {xn, n ≥0} (resp. {an, n ≥0})\nbe a non-negative (resp. positive) sequence and C, L ≥0 be scalars such that, for all n,\nxn+1 ≤C + L\n \nn\nX\nm=0\namxm\n!\n.\nThen, xn+1 ≤CeLsn, where sn = Pn\nm=0 am for each non-negative integer n.\nLemma 5. [7, Theorem C.3] Suppose {Xn}∞\nn=0 is a martingale with respect to {Fn}, defined\non the probability space (Ω, F, P). Also assume that E [X2\nn] < ∞. Then,\n• P∞\nn=0 E\n\u0002\n(Xn+1 −Xn)2 | Fn\n\u0003\n< +∞implies that, almost surely, {Xn}∞\nn=0 converges.\n• P∞\nn=0 E\n\u0002\n(Xn+1 −Xn)2 | Fn\n\u0003\n= +∞implies that\nXn = o\n n−1\nX\nm=1\nE\n\u0002\n(Xm+1 −Xm)2 | Fm\n\u0003\n!\n, a.s.\nAppendix B Proof of the Implication in Step 1 of the Proof of\nLemma 2\nThe following lemma will be used to prove the implication stated in Step 1 of Lemma 2’s\nproof.\nLemma A.1 Consider the ODE ˙x(t) = f(x(t)), where f : Rd →Rd is Lipschitz with\nthe Lipschitz constant Lf > 0. Assume that x1(·) and x2(·) are two solutions to the ODE.\nThen, for any ∆> 0 and any t ∈[0, ∆], we have\n∥x1(t) −x2(t)∥≤∥x1(0) −x2(0)∥eLf∆.\n36\nProof. Note that\n∥x1(t) −x2(t)∥= ∥\nZ t\n0\n[f(x1(s)) −f(x2(s))]ds + (x1(0) −x2(0))∥\n≤\nZ t\n0\n∥f(x1(s)) −f(x2(s))∥ds + ∥x1(0) −x2(0)∥\n≤Lf\nZ t\n0\n∥x1(s) −x2(s)∥ds + ∥x1(0) −x2(0)∥.\nHence, using the Gronwall inequality (see Lemma 3 in Appendix A), we have for any\nt ∈[0, ∆] that ∥x1(t) −x2(t)∥≤∥x1(0) −x2(0)∥eLf∆.\nProof of the implication in Step 1. Suppose that Eq. (20) holds. Our goal is to\nprove Eq. (16), namely that, for any fixed ∆> 0,\nlim\ns→+∞\n \nsup\nt∈[s,s+∆]\n∥¯θ(t) −θs(t)∥\n!\n= 0,\na.s.\nWe follow the steps (L1) to (L3) below to accomplish this proof.\nStep (L1) We fisrt show that\nlim\nn→+∞\n \nsup\nt∈[sn,sn+∆]\n∥¯θ(t) −θsn(t)∥\n!\n= 0.\nIndeed, from Eq. (20), we have\nlim\nn→+∞\n \nsup\nt∈[sn,[sn+∆+1]−]\n∥¯θ(t) −θsn(t)∥\n!\n= 0,\nfor any fixed ∆> 0.\nTo finish Step (L1), we need to show that, when n is large enough,\nsn + ∆≤[sn + ∆+ 1]−,\n(55)\nwhich further implies\nlim\nn→+∞\n \nsup\nt∈[sn,sn+∆]\n∥¯θ(t) −θsn(t)∥\n!\n≤lim\nn→+∞\n \nsup\nt∈[sn,[sn+∆+1]−]\n∥¯θ(t) −θsn(t)∥\n!\n= 0.\nFrom Point 3 in Assumption 1, limn→+∞an = 0. Therefore, there exists N > 0 such that,\nwhenever n > N, an < 1. Now, suppose n > N and let k denote the positive integer such\nthat sk := [sn + ∆+ 1]−. Then,\nsn + ∆+ 1 −[sn + ∆+ 1]−≤[sn + ∆+ 1]+ −[sn + ∆+ 1]−= sk+1 −sk = ak.\nSubtracting the left and right end of the above expression by 1 gives sn+∆−[sn+∆+1]−≤\nak −1 ≤0, where the last inequality is because k ≥n > N (according to the definition of\nsk), and hence −1+ak < 0. This proves (55) and finishes the proof for the equality in Step\n(L1).\nStep (L2). We prove that, when s is large enough,\nsup\nt∈[s,s+∆]\n∥¯θ(t) −θs(t)∥≤\nsup\nt∈[[s]−,[s]−+∆+1]\n∥¯θ(t) −θ[s]−(t)∥(1 + eLg(∆+1)).\n37\nRecall that limn→+∞an = 0. Let N > 0 denote the threshold such that, whenever n ≥N,\nan < 1. For any s ≥sN, let k denote the non-negative integer such that [s]−= sk. Then,\nk ≥N and\ns −[s]−≤[s]+ −[s]−= sk+1 −sk = ak < 1,\nwhich implies\ns ≤[s]−+ 1 ⇒s + ∆≤[s]−+ ∆+ 1.\n(56)\nThis result is used to derive the second inequality of the following:\nsup\nt∈[s,s+∆]\n∥¯θ(t) −θs(t)∥≤\nsup\nt∈[s,s+∆]\n∥¯θ(t) −θ[s]−(t)∥+\nsup\nt∈[s,s+∆]\n∥θ[s]−(t) −θs(t)∥\n≤\nsup\nt∈[[s]−,[s]−+∆+1]\n∥¯θ(t) −θ[s]−(t)∥+\nsup\nt∈[s,s+∆]\n∥θ[s]−(t) −θs(t)∥.\n(57)\nNow, we derive an upper bound for the second sup-term, supt∈[s,s+∆]∥θ[s]−(t) −θs(t)∥. Let\ny[s]−(t) := θ[s]−(t + s) and ys(t) := θs(t + s). Then, y[s]−(t) and ys(t) are solutions to ODE\n(17) since\n˙y[s]−(t) = ˙θ[s]−(t + s) = g(θ[s]−(t + s)) = g(y[s]−(t));\n˙ys(t) = ˙θs(t + s) = g(θs(t + s)) = g(ys(t)).\nThe definitions of y[s]−(t) and ys(t) imply the second line in the following:\nsup\nt∈[s,s+∆]\n∥θ[s]−(t) −θs(t)∥= sup\nt∈[0,∆]\n∥θ[s]−(t + s) −θs(t + s)∥\n= sup\nt∈[0,∆]\n∥y[s]−(t) −ys(t)∥\n≤∥y[s]−(0) −ys(0)∥eLg∆, by Lemma A.1\n= ∥θ[s]−(s) −θs(s)∥eLg∆\n= ∥θ[s]−(s) −¯θ(s)∥eLg∆, By θs-definition\n≤\nsup\nt∈[[s]−,[s]−+∆+1]\n∥¯θ(t) −θ[s]−(t)∥eLg∆,\nwhere the last inequality follows from the fact when s is sufficiently large (i.e., s ≥sN),\ns ∈[[s]−, [s]−+ ∆+ 1] according to (56). Plugging this result into (57) proves Step (L2).\nStep (L3). Take the limit of both sides of the inequality in Step (L2):\nlim\ns→+∞\nsup\nt∈[s,s+∆]\n∥θ[s]−(t) −θs(t)∥≤lim\ns→+∞\nsup\nt∈[[s]−,[s]−+∆+1]\n∥¯θ(t) −θ[s]−(t)∥(1 + eLg(∆+1)) = 0,\nwhere the equality is in accordance with Step (L1). Thus the whole proof of the implication\nin Step 1 of Lemma 2’s proof.\nAppendix C Existence of A Stationary Distribution of Markov\nChain\nLet {Xk}∞\nk=0 be a Markov chain defined on a measurable space (X, X), and let P denote its\nMarkov kernel. The invariant distribution of {Xk} (also called a stationary or a normalized\ninvariant measure), if it exists, refers to a probability measure µ on (X, X) satisfying\nµP = µ and µ(X) = 1.\n38\nSufficient but possibly unnecessary conditions for the existence of a unique invariant distri-\nbution can be found in [16, Section 2.1.2, H2.1.8] and are copied below, which requires X\nto be a complete and separable metric space:\n(i) There exists a measurable function K : Z →R+ such that, for all (x, y, z) ∈X ×\nX×Z, d(f(x, z), f(y, z)) ≤K(z)d(x, y), E[max{log K(Z1), 0}] < ∞, and E[log K(Z1)] < 0,\nwhere d denotes the metric on X, (Z, Z) is a measurable space, {Zk}∞\nk=0 is a sequence of\ni.i.d. random variables taking values in Z, and f : X × Z →X is a X ⊗Z-X measurable\nmap such that Xk = f(Xk−1, Zk). (Note that its existence is guaranteed by the fact that X\nis finitely generated since X is separable, and by Theorem 1.3.6 in [16]).\n(ii) There exists x0 ∈X such that E[log+ d(x0, f(x0, Z1))] < ∞.\nReferences\n[1] J. Baumeister. Deconvolution of appearance potential spectra. In Methoden Verfahren\nMath. Phys., volume 35, pages 1–13, Frankfurt am Main, 1991. Peter Lang.\n[2] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial opti-\nmization with reinforcement learning. In Workshop Proceedings of the 5th International\nConference on Learning Representations, ICLR., 2017.\n[3] D. Bhandari, C. A. Murthy, and S. K. Pal. Genetic algorithm with elitist model and\nits convergence. Int. J. Pattern Recognit. Artif. Intell., 10(6):731–747, 1996.\n[4] S. Bhatnagar and V. Borkar. A two timescale stochastic approximation scheme for\nsimulation-based parametric optimization. Probab. Eng. Inf. Sci., 12(4):519–531, 1998.\n[5] P. Billingsley. Probability and Measure. John Wiley and Sons, New York, 3rd edition,\n1995.\n[6] P. D. Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. A tutorial on the cross-\nentropy method. Ann. Oper. Res., 134(1):19–67, 2005.\n[7] V. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Hindustan\nBook Agency, Gurgaon, 2022.\n[8] V. Borkar and V. Konda. The actor-critic algorithm as multi-time-scale stochastic\napproximation. Sadhana, 22:525–543, 1997.\n[9] V. Borkar and S. Meyn. The o.d.e. method for convergence of stochastic approximation\nand reinforcement learning. SIAM J. Control Optim., 38(2):447–469, 2000.\n[10] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein, and P. L’Ecuyer. Chapter 3 - the cross-\nentropy method for optimization. In C. Rao and V. Govindaraju, editors, Handbook\nof Statistics, volume 31, pages 35–59. Elsevier, New York, NY, 2013.\n[11] L. Bottou. Stochastic gradient learning in neural networks. Proc. Neuro-Nımes, 1991.\n[12] X. Chen, J. Liu, and W. Yin. Learning to optimize: A tutorial for continuous and\nmixed-integer optimization. Sci. China Math., 67(6), 2024.\n[13] Y. Chen and J. He. Average convergence rate of evolutionary algorithms in continuous\noptimization. Inf. Sci., 562:200–219, 2021.\n[14] Z. Dai. Local regularization methods for inverse Volterra equations applicable to the\nstructure of solid surfaces. J. Integral Equ. Appl., 25(2):223–252, 2013.\n39\n[15] P. Dell’Aversana. Reinforcement learning in optimization problems: Applications to\ngeophysical data inversion. AIMS Geosci., 8(3):488–502, 2022.\n[16] R. Douc, E. Moulines, P. Priouret, and P. Soulier. Markov Chain. Springer, Cham,\n1st edition, 2018.\n[17] M. M. Drugan. Reinforcement learning versus evolutionary computation: A survey on\nhybrid algorithms. Swarm. Evol. Comput., 44:228–246, 2019.\n[18] B. Engquist, K. Ren, and Y. Yang. An algebraically converging stochastic gradient\ndescent algorithm for global optimization, 2023.\n[19] P. I. Frazier. A tutorial on bayesian optimization, 2018.\n[20] Y. Fukuda. Appearance potential spectroscopy (APS): old method, but applicable to\nstudy of nano-structures. Anal. Sci., 26(2):187–197, 2010.\n[21] A. F. Gad. Pygad: An intuitive genetic algorithm python library. Multimed. Tools\nAppl., pages 1–14, 2023.\n[22] D. Gerth, B. Hofmann, S. Birkholz, S. Koke, and G. Steinmeyer. Regularization of an\nautoconvolution problem in ultrashort laser pulse characterization. Inverse Probl. Sci.\nEng., 22(2):245–266, 2014.\n[23] R. Gorenflo and B. Hofmann. On autoconvolution and regularization. Inverse Probl.,\n10(2):353–373, 1994.\n[24] D. Greenhalgh and S. Marshall. Convergence criteria for genetic algorithms. SIAM J.\nComput., 30(1):269–282, 2000.\n[25] S. Guadarrama, A. Korattikara, O. Ramirez, P. Castro, E. Holly, S. Fishman, K. Wang,\nE. Gonina, N. Wu, E., L. Sbaiz, J. Smith, G. Bart´ok, J. Berent, C. Harris, V. Van-\nhoucke, and E. Brevdo. TF-Agents: A library for reinforcement learning in TensorFlow,\n2018.\n[26] R. Harrach, T. Jahn, and R. Potthast. Beyond the Bakushinkii veto: regularising linear\ninverse problems without knowing the noise distribution. Numer. Math., 145:581–603,\n2020.\n[27] R. Harrach, T. Jahn, and R. Potthast. Regularizing linear inverse problems under\nunknown non-gaussian white noise allowing repeated measurements. IMA J. Numer.\nAnal., 43:443–500, 2023.\n[28] M. Hauschild and M. Pelikan. An introduction and survey of estimation of distribution\nalgorithms. Swarm Evol. Comput., 1(3):111–128, 2011.\n[29] B. Hofmann and R. Plato. On ill-posedness concepts, stable solvability and saturation.\nJ. Inverse Ill-posed Probl., 26(2):287–297, 2018.\n[30] B. Hofmann and O. Scherzer. Local ill-posedness and source conditions of operator\nequations in Hilbert spaces. Inverse Probl., 14(5):1189–1206, 1998.\n[31] M. Holzleitner, L. Gruber, J. Arjona-Medina, J. Brandstetter, and S. Hochreiter. Con-\nvergence Proof for Actor-Critic Methods Applied to PPO and RUDDER, pages 105–\n130. Springer, Berlin, Heidelberg, 2021.\n40\n[32] M. Hong, H. Wai, Z. Wang, and Z. Yang. A two-timescale stochastic algorithm frame-\nwork for bilevel optimization: Complexity analysis and application to actor-critic.\nSIAM J. Optim., 33(1):147–180, 2023.\n[33] Q. Jin and W. Wang. Dual gradient method for ill-posed problems using multiple\nrepeated measurement data. Inverse Probl., 39:085005, 2023.\n[34] P. Karmakar and S. Bhatnagar. Two time-scale stochastic approximation with con-\ntrolled markov noise and off-policy temporal-difference learning. Math. Oper. Res.,\n43(1):130–151, 2018.\n[35] A. Klenke. Probability Theory. Springer, London, 2nd edition, 2013.\n[36] V. Konda and V. Borkar. Actor-critic–type learning algorithms for markov decision\nprocesses. SIAM J. Control Optim., 38(1):94–123, 1999.\n[37] V. R. Konda and T. J. N.\nOn actor-critic algorithms.\nSIAM J. Control Optim.,\n42(4):1143–1166, 2003.\n[38] V. R. Konda and J. Tsitsiklis.\nActor-critic algorithms.\nIn S. Solla, T. Leen, and\nK. M¨uller, editors, Advances in Neural Information Processing Systems, volume 12,\nCambridge, MA, 1999. MIT Press.\n[39] W. Kool, H. van Hoof, and M. Welling. Attention, learn to solve routing problems! In\nProceedings of the 7th International Conference on Learning Representations, ICLR.,\n2019.\n[40] S. Levine and V. Koltun. Guided policy search. In Proceedings of the 30th International\nConference on Machine Learning, PMLR, volume 28, pages 1–9, 2013.\n[41] K. Li and J. Malik. Learning to optimize. In International Conference on Learning\nRepresentations, ICLR, 2017.\n[42] Y. Li. Deep reinforcement learning: An overview. http://arxiv.org/abs/1701.07274,\n2018.\n[43] G. Lin, Y. Zhang, X. Cheng, M. Gulliksson, P. Forss´en, and T. Fornstedt. A regu-\nlarizing Kohn-Vogelius formulation for the model-free adsorption isotherm estimation\nproblem in chromatography. Appl. Anal., 97:13–40, 2018.\n[44] M. Locatelli and F. Schoen. Global Optimization: Theory, Algorithms, and Applica-\ntions. SIAM, Philadelphia, PA, 2013.\n[45] N. Mazyavkina, S. Sviridov, S. Ivanov, and E. Burnaev. Reinforcement learning for\ncombinatorial optimization: A survey. Comput. Oper. Res., 134:105400, 2021.\n[46] P. Mertikopoulos, N. Hallak, A. Kavis, and V. Cevher. On the almost sure convergence\nof stochastic gradient descent in non-convex problems. In H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing\nSystems, volume 33, pages 1117–1128, New York, 2020. Curran Associates, Inc.\n[47] L. J. V. Miranda. PySwarms, a research-toolkit for particle swarm optimization in\npython. J. Open Source Softw., 3, 2018.\n41\n[48] M. Nazari, A. Oroojlooy, L. Snyder, and M. Takac. Reinforcement learning for solving\nthe vehicle routing problem. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 31, 2018.\n[49] M. Puth, M. Neuh¨auser, and G. D. Ruxton. On the variety of methods for calculating\nconfidence intervals by bootstrapping. J. Anim. Ecol., 84:892–897, 2015.\n[50] G. Rudolph. Convergence analysis of canonical genetic algorithms. IEEE Trans. Neural\nNetw., 5(1):96–101, 1994.\n[51] G. Rudolph.\nConvergence of evolutionary algorithms in general search spaces.\nIn\nProceedings of IEEE International Conference on Evolutionary Computation, pages\n50–54, 1996.\n[52] G. Rudolph. Convergence rates of evolutionary algorithms for quadratic convex func-\ntions with rank-deficient hessian.\nIn M. Tomassini, A. Antonioni, F. Daolio, and\nP. Buesser, editors, Adaptive and Natural Computing Algorithms, pages 151–160,\nBerlin, Heidelberg, 2013. Springer Berlin Heidelberg.\n[53] F. Santosa and L. Anderson. Bayesian sequential optimal experimental design for linear\nregression with reinforcement learning. In 2022 21st IEEE International Conference\non Machine Learning and Applications (ICMLA), pages 641–646, 2022.\n[54] A. Seyyedabbasi. A reinforcement learning-based metaheuristic algorithm for solving\nglobal optimization problems. Adv. Eng. Softw., 178:103411, 2023.\n[55] A. Seyyedabbasi, R. Aliyev, F. Kiani, M. U. Gulle, H. Basyildiz, and M. A. Shah. Hy-\nbrid algorithms based on combining reinforcement learning and metaheuristic methods\nto solve global optimization problems. Knowl. Based Syst., 223:107044, 2021.\n[56] B. Sridharan, S. Mehta, Y. Pathak, and U. Priyakumar. Deep reinforcement learn-\ning for molecular inverse problem of nuclear magnetic resonance spectra to molecular\nstructure. J. Phys. Chem. Lett., 13(22):4924–4933, 1992.\n[57] L. State. An information theory analysis of the convergence and learning properties\nof a certain class of genetic algorithms in continuous space and infinite population\nassumption. In 1995 IEEE International Conference on Systems, Man and Cybernetics.\nIntelligent Systems for the 21st Century, volume 1, pages 229–234 vol.1, 1995.\n[58] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. The MIT Press,\nCambridge, MA, 2018.\n[59] C. Xu and Y. Zhang. Estimating adsorption isotherm parameters in chromatography\nvia a virtual injection promoting double feed-forward neural network. J. Inverse Ill-\nPosed Probl., 30(5):693–712, 2022.\n[60] T. Xu, Z. Yang, Z. Wang, and Y. Liang. Doubly robust off-policy actor-critic: Conver-\ngence and optimality. In M. Meila and T. Zhang, editors, Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML, volume 139, pages 11581–11591,\nCambridge MA, 2021. PMLR.\n[61] J. Yang, C. Xu, and Y. Zhang. Reconstruction of the s-wave velocity via mixture\ndensity networks with a new rayleigh wave dispersion function. IEEE Trans. Geosci.\nRemote Sens., 60:1–13, 2022.\n42\n[62] P. Yang and B. Dong. L2SR: learning to sample and reconstruct for accelerated mri\nvia reinforcement learning. Inverse Probl., 40:055015, 2024.\n[63] Y. Zhang and C. Chen. Stochastic asymptotical regularization for linear inverse prob-\nlems. Inverse Probl., 39:015007, 2023.\n[64] Y. Zhang and C. Chen. Stochastic linear regularization methods: random discrepancy\nprinciple and applications. Inverse Probl., 40:025007, 2024.\n[65] Y. Zhang and B. Hofmann. Two new non-negativity preserving iterative regularization\nmethods for ill-posed inverse problems. Inverse Probl. Imag., 15(2):229–256, 2021.\n[66] Y. Zhang, G. Lin, P. Forss´en, M. Gulliksson, T. Fornstedt, and X. Cheng. A regulariza-\ntion method for the reconstruction of adsorption isotherms in liquid chromatography.\nInverse Probl., 32(10):105005, 2016.\n[67] T. Zhao, H. Hachiya, G. Niu, and M. Sugiyama. Analysis and improvement of policy\ngradient estimation. Neural Netw., 26:118–129, 2012.\n43\n",
  "categories": [
    "math.OC",
    "90C26, 65J22, 65J20, 90C40, 90C15, 45Q05, 47A52"
  ],
  "published": "2023-10-10",
  "updated": "2025-01-24"
}