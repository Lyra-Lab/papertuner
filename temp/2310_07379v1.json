{
  "id": "http://arxiv.org/abs/2310.07379v1",
  "title": "Causal Unsupervised Semantic Segmentation",
  "authors": [
    "Junho Kim",
    "Byung-Kwan Lee",
    "Yong Man Ro"
  ],
  "abstract": "Unsupervised semantic segmentation aims to achieve high-quality semantic\ngrouping without human-labeled annotations. With the advent of self-supervised\npre-training, various frameworks utilize the pre-trained features to train\nprediction heads for unsupervised dense prediction. However, a significant\nchallenge in this unsupervised setup is determining the appropriate level of\nclustering required for segmenting concepts. To address it, we propose a novel\nframework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages\ninsights from causal inference. Specifically, we bridge intervention-oriented\napproach (i.e., frontdoor adjustment) to define suitable two-step tasks for\nunsupervised prediction. The first step involves constructing a concept\nclusterbook as a mediator, which represents possible concept prototypes at\ndifferent levels of granularity in a discretized form. Then, the mediator\nestablishes an explicit link to the subsequent concept-wise self-supervised\nlearning for pixel-level grouping. Through extensive experiments and analyses\non various datasets, we corroborate the effectiveness of CAUSE and achieve\nstate-of-the-art performance in unsupervised semantic segmentation.",
  "text": "CAUSAL UNSUPERVISED SEMANTIC SEGMENTATION\nJunho Kimâˆ—, Byung-Kwan Leeâˆ—, Yong Man Roâ€ \nSchool of Electrical Engineering\nKorea Advanced Institute of Science and Technology (KAIST)\n{arkimjh, leebk, ymro}@kaist.ac.kr\nABSTRACT\nUnsupervised semantic segmentation aims to achieve high-quality semantic group-\ning without human-labeled annotations. With the advent of self-supervised pre-\ntraining, various frameworks utilize the pre-trained features to train prediction\nheads for unsupervised dense prediction. However, a significant challenge in this\nunsupervised setup is determining the appropriate level of clustering required for\nsegmenting concepts. To address it, we propose a novel framework, CAusal Unsu-\npervised Semantic sEgmentation (CAUSE), which leverages insights from causal\ninference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor\nadjustment) to define suitable two-step tasks for unsupervised prediction. The first\nstep involves constructing a concept clusterbook as a mediator, which represents\npossible concept prototypes at different levels of granularity in a discretized form.\nThen, the mediator establishes an explicit link to the subsequent concept-wise\nself-supervised learning for pixel-level grouping. Through extensive experiments\nand analyses on various datasets, we corroborate the effectiveness of CAUSE and\nachieve state-of-the-art performance in unsupervised semantic segmentation.\n1\nINTRODUCTION\nSemantic segmentation is one of the essential computer vision tasks that has continuously advanced\nin the last decade with the growth of Deep Neural Networks (DNNs) (He et al., 2016; Dosovitskiy\net al., 2020; Carion et al., 2020) and large-scale annotated datasets (Everingham et al., 2010; Cordts\net al., 2016; Caesar et al., 2018). However, obtaining such pixel-level annotations for dense prediction\nrequires an enormous amount of human resources and is more time-consuming compared to other\nimage analysis tasks. Alternatively, weakly-supervised semantic segmentation approaches have been\nproposed to relieve the costs by using of facile forms of supervision such as class labels (Wang et al.,\n2020b; Zhang et al., 2020a), scribbles (Lin et al., 2016), bounding boxes (Dai et al., 2015; Khoreva\net al., 2017), and image-level tags (Xu et al., 2015; Tang et al., 2018).\nWhile relatively few works have been dedicated to explore unsupervised semantic segmentation\n(USS), several methods have presented the way of segmenting feature representations without any\nannotated labels by exploiting visual consistency maximization (Ji et al., 2019; Hwang et al., 2019),\nmulti-view equivalence (Cho et al., 2021), or saliency priors (Van Gansbeke et al., 2021; Ke et al.,\n2022). In parallel with segmentation researches, recent self-supervised learning frameworks (Caron\net al., 2021; Bao et al., 2022) using Vision Transformer have observed that their representations\nexhibit semantic consistency at the pixel-level scale for object targets. Based on such intriguing\nproperties of self-supervised training, recent USS methods (Hamilton et al., 2022; Ziegler & Asano,\n2022; Yin et al., 2022; Zadaianchuk et al., 2023; Li et al., 2023; Seong et al., 2023) have employed\nthe pre-trained features as a powerful source of prior knowledge and introduced contrastive learning\nframeworks by maximizing feature correspondence for the unsupervised segmentation task.\nIn this paper, we begin with a fundamental question for the unsupervised semantic segmentation:\nHow can we define what to cluster and how to do so under an unsupervised setting?, which has\nbeen overlooked in previous works. A major challenge for USS lies in the fact that unsupervised\nsegmentation is more akin to clustering rather than semantics with respect to pixel representation.\nTherefore, even with the support of self-supervised representation, the lack of awareness regarding\nâˆ—Equal contribution. â€  Corresponding author.\n1\narXiv:2310.07379v1  [cs.CV]  11 Oct 2023\nImage\nLabel\nSTEGO\nHP\nCAUSE\nReCo+\nwall\nceiling\nfurniture stuff\nfloor\nstructural\nground\nanimal\nsports\nperson\nbuilding\nplant\nsky\nwater\nvehicle\nelectronic\nfurniture thing\nappliance\noutdoor\nFigure 1: Visual comparison of USS for COCO-stuff (Caesar et al., 2018). Note that, in contrast to\ntrue labels, baseline frameworks (Hamilton et al., 2022; Seong et al., 2023; Shin et al., 2022) fail to\nachieve targeted level of granularity, while CAUSE successfully clusters person, sports, vehicle, etc.\nwhat and how to cluster for each pixel representation makes USS a challenging task, especially\nwhen aiming for the desired level of granularity. For example, elements such as head, torso, hand,\nleg, etc., should ideally be grouped together under the broader-level category person, a task that\nprevious methods (Hamilton et al., 2022; Seong et al., 2023) have had difficulty accomplishing, as in\nFig. 1. To address these difficulties, we, for the first time, treat USS procedure within the context\nof causality and propose suitable two-step tasks for the unsupervised learning. As shown in Fig. 2,\nwe first schematize a causal diagram for a simplified understanding of causal relations for the given\nvariables and the corresponding unsupervised tasks for each step. Note that our main goal is to group\nsemantic concepts Y that meet the targeted level of granularity, utilizing feature representation T\nfrom pre-trained self-supervised methods such as DINO (Caron et al., 2021).\nð‘€ð‘€\nð‘Œð‘Œ\nð‘ˆð‘ˆ\nð‘‡ð‘‡\nConcept\nClusterBook\nPre-trained\nFeature\nSematic\nGroups\nWhat & How to\nCluster?\nStep. 1\nStep. 2\nSelf-supervised \nLearning\nModularity\nMaximization\nFigure 2:\nCausal diagram of\nCAUSE. We split USS into two\nsteps to identify relation between\npre-trained features T and seman-\ntic groups Y using clusterbook M.\nSpecifically, the unsupervised segmentation (T â†’Y ) is a pro-\ncedure for deriving semantically clustered groups Y distilled\nfrom pre-trained features T. However, the indeterminate U of\nunsupervised prediction (i.e., what and how to cluster) can lead\nconfounding effects during pixel-level clustering without su-\npervision. Such effects can be considered as a backdoor path\n(T â†U â†’Y ) that hinders the targeted level of segmenta-\ntion. Accordingly, our primary insight stems from constructing\na subdivided concept representation M, with discretized indices,\nwhich serves as an explicit link between T and Y in alternative\nforms of supervision. Intuitively, the construction of subdivided\nconcept clusterbook M implies the creation of as many inherent\nconcept prototypes as possible in advance, spanning various\nlevels of granularity. Subsequently, for the given pre-trained\nfeatures, we train a segmentation head that can effectively con-\nsolidate the concept prototypes into the targeted broader-level\ncategories using the constructed clusterbook. This strategy in-\nvolves utilizing the discretized indices within M to identify\npositive and negative features for the given anchor, enabling concept-wise self-supervised learning.\nBeyond the intuitive causal procedure of USS, building a mediator M can be viewed as a blocking\nprocedure of the backdoor paths induced from U by assigning possible concepts in discretized\nstates such as in Van Den Oord et al. (2017); Esser et al. (2021). That is, it satisfies a condition for\nfrontdoor adjustment (Pearl, 1993), which is a powerful causal estimator that can establish only causal\nassociation1 (T â†’M â†’Y ). We name our novel framework as CAusal Unsupervised Semantic\n1In Step 1, Y is a collider variable in the path of Tâ†’Y through U, and it blocks backdoor path. Therefore,\ncausal association only flows into M from T. Then, in Step 2, T blocks Mâ†Tâ†Uâ†’Y . By combining two\nsteps, we can distill the pre-trained representation using only causal association path and reflect it on semantic\ngroups, which is our ultimate goal for unsupervised semantic segmentation. Please see preliminary in Section 3.1.\n2\nStep 1: Construct Mediator\nStep 2: Clustering Semantic Groups\nConcept \nClusterbook\nModularity\nClustering\nConcept\nBank\nâ‹®\nðŸŽðŸŽðŸðŸ\nð’Œð’Œ\nðŸðŸ\nâ‹®\nDiscretization\nConcept-wise\nSelf-Sup. Learning\nâ‹®\nPre-trained\nModels\nâ‹®\nSegment \nHead\nSegment \nHead\nvq\nPositive & Negative Concept Selection\nInference Phase\nteacher\nproj.\nproj.\nPrototype Emb\nð’šð’šâˆ’\nð’šð’š+\nEMA\nanchor\nPost\nProcessing\nð‘†ð‘†e\nð‘‡ð‘‡\npos/neg \nq\nk,v\nk,v\nq\nâ‹®\nð‘Œð‘Œ\nð‘Œð‘Œe\nstudent\nsg\nvq: Vector Quantization\nsg: Stop Gradient\n: Positive Feature\n: Negative Feature\nð’šð’šâˆ’\nð’šð’š+\n: Global View Pairs\n: Local View Pairs\n: Anchor Feature\n: Positive Concept\n: Negative Concept\nFigure 3: The overall architecture of CAUSE comprises (i): constructing discretized concept cluster-\nbook as a mediator and (ii): clustering semantic groups using concept-wise self-supervised learning.\nsEgmentation (CAUSE), which integrates the causal approach into the field of USS. As illustrated\nin Fig. 2, in brief, we divide the unsupervised dense prediction into two step tasks: (1) discrete\nsubdivided representation learning with Modularity theory (Newman, 2006) and (2) conducting do-\ncalculus (Pearl, 1995) with self-supervised learning (Oord et al., 2018) in the absence of annotations.\nBy combining the above tasks, we can bridge causal inference into the unsupervised segmentation\nand obtain semantically clustered groups with the support of pre-trained feature representation.\nOur main contributions can be concluded as: (i) We approach unsupervised semantic segmentation\ntask with an intervention-oriented approach (i.e., causal inference) and propose a novel unsupervised\ndense prediction framework called CAusal Unsupervised Semantic sEgmentation (CAUSE), (ii) To\naddress the ambiguity in unsupervised segmentation, we integrate frontdoor adjustment into USS and\nintroduce two-step tasks: deploying a discretized concept clustering and concept-wise self-supervised\nlearning, and (iii) Through extensive experiments, we corroborate the effectiveness of CAUSE on\nvarious datasets and achieve state-of-the-art results in unsupervised semantic segmentation.\n2\nRELATED WORK\nAs an early work for USS, Ji et al. (2019) have proposed IIC to maximize mutual information of\nfeature representations from augmented views. After that, several methods have further improved\nthe segmentation quality by incorporating inductive bias in the form of cross-image correspon-\ndences (Hwang et al., 2019; Cho et al., 2021; Wen et al., 2022) or saliency information in an\nend-to-end manner (Van Gansbeke et al., 2021; Ke et al., 2022). Recently, with the discovery of\nsemantic consistency for pre-trained self-supervised frameworks (Caron et al., 2021), Hamilton et al.\n(2022) have leveraged the pre-trained features for the unsupervised segmentation. Subsequently, vari-\nous works (Wen et al., 2022; Yin et al., 2022; Ziegler & Asano, 2022) have utilized the self-supervised\nrepresentation as a form of pseudo segmentation labels (Zadaianchuk et al., 2023; Li et al., 2023) or a\npre-encoded representation to further incorporate additional prior knowledge (Van Gansbeke et al.,\n2021; Zadaianchuk et al., 2023) into the segmentation frameworks. Our work aligns with previous\nstudies (Hamilton et al., 2022; Seong et al., 2023) in the aspect of refining segmentation features\nusing pre-trained representations without external information. However, we highlight that the lack\nof a well-defined clustering target in the unsupervised setup leads to suboptimal segmentation quality.\nAccordingly, we interpret USS within the context of causality, bridging the construction of discretized\nrepresentation with pixel-level self-supervised learning (see extended explanations in Appendix A.)\n3\nCAUSAL UNSUPERVISED SEMANTIC SEGMENTATION\n3.1\nDATA GENERATING PROCESS FOR UNSUPERVISED SEMANTIC SEGMENTATION\nPreliminary.\nIt is important to define Data Generating Process (DGP) early in the process for\ncausal inference. DGP outlines the causal relationships between treatment T and outcome of our\ninterest Y , and the interrupting factors, so-called confounder U. For example, if we want to identify\nthe causal relationship between smoking (i.e., treatment) and lung cancer (i.e., outcome of our\ninterest), genotype can be deduced as one of potential confounders that provoke confounding effects\nbetween smoking T and lung cancer Y . Once we define the confounder U, and if it is observable,\nbackdoor adjustment (Pearl, 1993) is an appropriate solution to estimate the causal influence between\n3\nAlgorithm 1 (STEP 1) Maximizing Modularity for Constructing Concept Clusterbook M\nRequire: Image Samples X âˆ¼Data, Pre-trained Model f, Concept Fractions M âˆˆRkÃ—c\n1: Initialize M\n2: for X âˆ¼Data do\n3:\nT âˆˆRhwÃ—c â†f(X)\nâ–·Pre-trained Model Representation\n4:\nA â†max(0, cos(T, T)) âˆˆRhwÃ—hw\nâ–·Affinity matrix\n5:\nd, e â†A\nâ–·Degree Matrix and Number of Total edges\n6:\nC â†max(0, cos(T, M)) âˆˆRhwÃ—k\nâ–·Cluster Assignment Matrix\n7:\nH â†\n1\n2eTr\n\u0010\ntanh\n\u0010\nCCT\nÏ„\n\u0011 h\nA âˆ’ddT\n2e\ni\u0011\nâ–·Maximizing Modularity (Ï„ = 0.1)\n8:\nM â†Increase(H)\nâ–·Updating Concept ClusterBook (lr: 0.001)\n9: end for\nT and Y by controlling U. However, not only in the above example but also in many real-world\nscenarios, including high-dimensional complex DNNs, confounder is often unobservable and either\nuncontrollable. In this condition, controlling U may not be a feasible option, and it prevents us from\nprecisely establishing the causal relationship between T and Y .\nFortunately, Pearl (2009) introduces frontdoor adjustment allowing us to elucidate the causal associa-\ntion even in the presence of unobservable confounder U. Here, the key successful points for frontdoor\nadjustment are two factors, as shown in Fig. 2: (a) assigning a mediator M bridging treatment T\ninto outcome of our interest Y while being independent with confounder U and (b) averaging all\npossible treatments between the mediator and outcome. When revisiting the above example, we can\ninstantiate a mediator M as accumulation of tar in lungs, which only affects lung cancer Y from\nsmoking T. We then average the probable effect between tar M and lung cancer Y across all of the\nparticipantsâ€™ population T â€² on smoking. The following formulation represents frontdoor adjustment:\np(Y | do(T)) =\nX\nmâˆˆM\np(m | T)\n|\n{z\n}\nStep 1\nX\ntâ€²âˆˆT â€²\np(Y | tâ€², m)p(tâ€²)\n|\n{z\n}\nStep 2\n,\n(1)\nwhere do(Â·) operator describes do-calculus (Pearl, 1995), which indicates intervention on treatment T\nto block unassociated backdoor path induced from U between the treatments and outcome of interest.\nCausal Perspective on USS.\nBridging the causal view into unsupervised semantic segmentation,\nour objective is clustering semantic groups Y with a support of pre-trained self-supervised features\nT. Here, in unsupervised setups, we define U as indetermination during clustering (i.e., a lack of\nawareness about what and how to cluster), which cannot be observed within the unsupervised context.\nTherefore, in Step 1 of Eq. (1), we first need to build a mediator directly relying on T while being\nindependent with the unobserved confounder U. To do so, we construct concept clusterbook as M,\nwhich is set of concept prototypes that encompass potential concept candidates spanning different\nlevels of granularity only through T. The underlying assumption for the construction of M is based\non the object alignment property observed in recent self-supervised methods (Caron et al., 2021;\nOquab et al., 2023), a characteristic exploited by Hamilton et al. (2022); Seong et al. (2023). Next, in\nStep 2 of Eq. (1), we need to determine whether to consolidate or separate the concept prototypes\ninto the targeted semantic-level groups Y . We utilize the discretized indices from M for discriminate\npositive and negative features for the given anchor and conduct concept-wise self-supervised learning.\nThe following is an approximation of Eq. (1) for the unsupervised dense prediction:\nE\ntâˆˆT [p(Y | do(t))] = E\ntâˆˆT\n\" X\nmâˆˆM\np(m | t)\nX\ntâ€²âˆˆT â€²\np(Y | tâ€², m)p(tâ€²)\n#\n,\n(2)\nwhere, T â€² indicates a population of all feature points, but notably in a pixel-level manner suitable for\ndense prediction. In summary, our focus is enhancing p(Y |do(t)) for feature points t by assigning ap-\npropriate unsupervised two tasks (i) p(m|t): construction of concept clusterbook and (ii) p(Y |tâ€², m):\nconcept-wise self-supervised learning, all of which can be bridged to frontdoor adjustment.\n3.2\nCONSTRUCTING CONCEPT CLUSTERBOOK FOR MEDIATOR\nConcept Prototypes.\nWe initially define a mediator M and maintain it as a link between the pre-\ntrained features T and the semantic groups Y . This mediator necessitates an explicit representation\n4\nAlgorithm 2 (STEP 2): Enhancing Likelihood of Semantic Groups through Self-Supervised Learning\nRequire: Head S;Î¸S, Head-EMA Sema;Î¸Sema, Clusterbook M, Distance DM, Concept Bank Ybank\n1: for X âˆ¼Data do\n2:\nT â†f(X)\nâ–·Pre-trained Model Representation\n3:\nQ â†T\nâ–·Vector Quantization from M\n4:\nY, Yema â†S(Q, T), Sema(Q, T)\n(âˆ—MLP: S(T), Sema(T)) â–·Segmentation Head Output\n5:\ny âˆ¼Y\nâ–·Anchor Selection (Appendix B for Detail)\n6:\ny+, yâˆ’âˆ¼{Yema, Ybank | y} â–·Positive/Negative Selection from DM (Appendix B for Detail)\n7:\np â†Ey\nh\nlog Ey+\nh\nexp(cos(y,y+)/Ï„)\nexp(cos(y,y+)/Ï„)+P\nyâˆ’exp(cos(y,yâˆ’)/Ï„)\nii\nâ–·Self-supervised Learning\n8:\nÎ¸S â†Increase(p)\nâ–·Updating Parameters of Segmentation Head (lr: 0.001)\n9:\nÎ¸Sema â†Î»Î¸Sema + (1 âˆ’Î»)Î¸S\nâ–·Exponential Moving Average (Î» : 0.99)\n10:\nYbank â†R2(Ybank, Yema)\nâ–·R2: Random Cut Ybank and Random Sample Yema\n11: end for\nthat transforms the continuous representation found in pre-trained self-supervised frameworks, into\na discretized form. One of possible approaches is reconstruction-based vector-quantization (Van\nDen Oord et al., 2017; Esser et al., 2021) that is well-suited for generative modeling. However, for\ndense prediction, we require more sophisticated pixel-level clustering methods that consider pixel\nlocality and connectivity. Importantly, they should be capable of constructing such representations in\ndiscretized forms for alternative role of supervisions. Accordingly, we exploit a clustering method\nthat maximizes modularity (Newman, 2006), which is one of the most effective approaches for\nconsidering relations among vertices. The following formulation represents maximizing a measure of\nmodularity H to acquire the discretized concept fractions from pre-trained features T:\nmax\nM H = 1\n2eTr\n\u0012\nC(T, M)T\n\u0014\nA(T) âˆ’ddT\n2e\n\u0015\nC(T, M)\n\u0013\nâˆˆR,\n(3)\nwhere C(T, M) âˆˆRhwÃ—k denotes cluster assignment matrix such that max(0, cos(T, M)) between\nall hw patch feature points in pre-trained features T âˆˆRhwÃ—c and all k concept prototypes in\nM âˆˆRkÃ—c. The cluster assignment matrix implies how each patch feature point is close to concept\nprototypes. In addition, A(T) âˆˆRhwÃ—hw indicates the affinity matrix of T = {t âˆˆRc}hw such\nthat Aij = max(0, cos(ti, tj)) between the two patch feature points ti, tj in T, which represents the\nintensity of connections among vertices. Note that, degree vector d âˆˆRhw describes the number of\nthe connected edges in its affinity A, and e âˆˆR denotes the total number of the edges.\nBy jointly considering cluster assignments C(T, M) and affinity matrix A(T) at once, in brief,\nmaximizing modularity H constructs the discretized concept clusterbook M taking into account the\npatch-wise locality and connectivity in pre-trained representation T. In practical, directly calculating\nEq. (3) can lead to much small value of H due to multiplying tiny elements of C twice. Thus, we\nuse trace property and hyperbolic tangent with temperature term Ï„ to scale up C (see Appendix\nB). Algorithm 1 provides more details on achieving maximizing modularity to generate concept\nclusterbook M, where we train only one epoch with Adam (Kingma & Ba, 2015) optimizer.\n3.3\nENHANCING LIKELIHOOD FOR SEMANTIC GROUPS\nConcept-Matched Segmentation Head.\nAs part of Step 2, to embed segmentation features Y that\nmatch with concept prototypes from pre-trained features T, we train a task-specific prediction head S.\nAs in Fig. 3, the pre-trained model remains frozen, and their features T = {t âˆˆRc}hw are fed into\nthe segmentation head S that performs cross-attention with querying prototype embedding Q = {q âˆˆ\nRc}hw. Here, for the given patch features T, the prototype embedding Q represents a vector-quantized\noutputs, which indicates the most representative concept q = arg maxmâˆˆM cos(t, m) âˆˆRc within\nthe concept clusterbook M. The segmentation head S comprises a single transformer layer followed\nby a MLP projection layer only used for training, and we can derive a concept-matched feature\nY = {y âˆˆRr}hw for concept fractions in M, satisfying Y = S(Q, T) (refer to Appendix B).\nConcept-wise Self-supervised Learning.\nUsing the concept-attended segmentation features, we\nproceed to enhance the likelihood p(Y |tâ€², m) for effectively clustering pixel-level semantics. To\neasily handle it, we first re-formulate it as p(Y |tâ€², m) = Q\nyâˆˆY p(y|tâ€², m)2, recognizing that Y\n2We only utilize the most closest concept at every patch feature point t in T. Hence, p(m|t) of Step 1\ncan be calculated by using sharpening technique: p(m=q|t)=1 if it is q= arg maxmâˆˆM cos(m, t); otherwise,\n5\nconsists of independently learned patch feature points y âˆˆRr. However, we cannot directly compute\nthis likelihood as in standard supervised learning, primarily because there are no available pixel\nannotations. Instead, we substitute the likelihood of unsupervised dense prediction to concept-wise\nself-supervised learning based on Noise-Contrastive Estimation (Gutmann & HyvÃ¤rinen, 2010):\np(y | tâ€², m) = E\ny+\n\"\nexp(cos(y, y+)/Ï„)\nexp(cos(y, y+)/Ï„) + P\nyâˆ’exp(cos(y, yâˆ’)/Ï„)\n#\n,\n(4)\nwhere y, y+, yâˆ’denote anchor, positive, and negative features, and Ï„ indicates temperature term.\nPositive & Negative Concept Selection.\nWhen selecting positive and negative concept features\nfor the proposed self-supervised learning, we use a pre-computed distance matrix DM that reflects\nconcept-wise similarity between all k concept prototypes such that DM = cos(M, M) âˆˆRkÃ—k\nin concept clusterbook M. Specifically, for the given patch feature t âˆˆRc as an anchor, we can\nidentify the most similar concept q âˆˆRc and its index: idq such that q = arg maxmâˆˆM cos(t, m).\nSubsequently, we use the anchor index idq to access all concept-wise distances for k concept\nprototypes within M through DM[idq, :] âˆˆRk as pseudo-code-like manner. By using a selection\ncriterion based on the distance DM, we can access concept indices for whole patch features to\ndistinguish positive and negative concept features for the given anchor. That is, once we find patch\nfeature points in T satisfying DM[idq, :] > Ï•+ for the given anchor t, we designate them as positive\nconcept feature t+. Similarly, if they meet the condition DM[idq, :] < Ï•âˆ’, we categorize them\nas negative concept feature tâˆ’. Here, Ï•+ and Ï•âˆ’represent the hyper-parameters for positive and\nnegative relaxation, which are both set to 0.3 and 0.1, respectively. Note that, we opt for soft\nrelaxation when selecting positive concept features because the main purpose of our unsupervised\nsetup is to group subdivided concept prototypes into the targeted broader-level categories. In this\ncontext, a soft positive bound is advantageous as it facilitates a smoother consolidation process.\nWhile, we set tight negative relaxation for selecting negative concept features, which aligns with\nfindings in various studies (Khosla et al., 2020; Kalantidis et al., 2020; Robinson et al., 2021; Wang\net al., 2021a) emphasizing that hard negative mining is crucial to advance self-supervised learning.\nIn the end, after choosing in-batch positive and negative concept features t+ and tâˆ’for the given\nanchor t, we sample positive segmentation features y+ and negative segmentation features yâˆ’from\nthe concept-matched Y = {y âˆˆRr}hw within the same spatial location as the selected concept\nfeatures. Through the concept-wise self-supervised learning in Eq. (4), we can then guide the\nsegmentation head S to enhance the likelihood of semantic groups Y . We re-emphasize that for the\ngiven anchor feature (head), our goal of USS is the feature consolidation corresponding to positive\nconcept features (torso, hand, leg, etc.), and the separation corresponding to negative concept features\n(sky, water, board, etc.), in order to achieve the targeted broader-level semantic groups (person).\nConcept Bank: Out-batch Accumulation.\nUnlike image-level self-supervised learning, unsu-\npervised dense prediction requires more intricate pixel-wise comparisons, as discussed in Zhang\net al. (2021). To facilitate this, we establish a concept bank, similar to He et al. (2020) but no-\ntably at a pixel-level scale, to accumulate out-batch concept features for additional comparison\npairs. Following the same selection criterion as described above, we dynamically sample in-batch\nfeatures in each training iteration and accumulate them into the concept bank Ybank âˆˆRkÃ—bÃ—r for\ncontinuously utilizing other informative feature from out-batches, where b represents the maximum\nnumber of feature points saved for each concept in M âˆˆRkÃ—c. We incorporate these additional\npositive and negative concept features into the sets of y+ and yâˆ’for the concept-wise self-supervised\nlearning. Here, creating a concept bank can be seen as incorporating global views into the pixel-\nlevel self-supervised learning beyond local views, which also corresponds to considering all feature\nrepresentations T â€² âˆˆRnÃ—hwÃ—c (n: total number of images in dataset) for frontdoor adjustment.\nAs a concept bank update strategy, we implement random removal of 50% of the bankâ€™s patch\nfeatures for each concept prototype, followed by random sampling of 50% new patch features into the\nconcept bank at every training iteration. In addition, to perform stable self-supervised learning, we\nemploy: (i) using log-probability not to converge to near-zero value due to numerous multiplication of\np(m|t)=0. Then, enhancing EtâˆˆT [p(Y |do(t))] for our main purpose to accomplish unsuperivsed dense\nprediction can be simplified with increasing EtâˆˆT [p(Y |tâ€², m=q)p(tâ€²)]. When p(tâ€²) is assumed to be uniform\ndistribution, it satisfies EtâˆˆT [p(Y |do(t))] â†‘âˆEtâˆˆT [p(Y |tâ€², m=q)] â†‘so that enhancing the likelihood of\nsemantic groups Y directly leads to increasing causal effect between T and Y even under the presence of U.\n6\nTable 1: Comparing quantitative results and applicability to other self-supervised methods on CAUSE.\n(a) Experimental results on COCO-Stuff.\nMethod (C = 27)\nBackbone\nmIoU\npAcc\nIIC (Ji et al., 2019)\nResNet18\n6.7\n21.8\nPiCIE (Cho et al., 2021)\nResNet18\n14.4\n50.0\nSegDiscover (Huang et al., 2022)\nResNet50\n14.3\n40.1\nSlotCon (Wen et al., 2022)\nResNet50\n18.3\n42.4\nHSG (Ke et al., 2022)\nResNet50\n23.8\n57.6\nReCo+ (Shin et al., 2022)\nDeiT-B/8\n32.6\n54.1\nDINO (Caron et al., 2021)\nViT-S/16\n8.0\n22.0\n+ STEGO (Hamilton et al., 2022)\nViT-S/16\n23.7\n52.5\n+ HP (Seong et al., 2023)\nViT-S/16\n24.3\n54.5\n+ CAUSE-MLP\nViT-S/16\n25.9\n66.3\n+ CAUSE-TR\nViT-S/16\n33.1\n70.4\nDINO (Caron et al., 2021)\nViT-S/8\n11.3\n28.7\n+ ACSeg (Li et al., 2023)\nViT-S/8\n16.4\n-\n+ TranFGU (Yin et al., 2022)\nViT-S/8\n17.5\n52.7\n+ STEGO (Hamilton et al., 2022)\nViT-S/8\n24.5\n48.3\n+ HP (Seong et al., 2023)\nViT-S/8\n24.6\n57.2\n+ CAUSE-MLP\nViT-S/8\n27.9\n66.8\n+ CAUSE-TR\nViT-S/8\n32.4\n69.6\nDINO (Caron et al., 2021)\nViT-B/8\n13.0\n42.4\n+ DINOSAUR (Seitzer et al., 2023)\nViT-B/8\n24.0\n-\n+ STEGO (Hamilton et al., 2022)\nViT-B/8\n28.2\n56.9\n+ CAUSE-MLP\nViT-B/8\n34.3\n72.8\n+ CAUSE-TR\nViT-B/8\n41.9\n74.9\n(c) Self-supervised methods with CAUSE-TR.\nDataset\nSelf-Supervised Methods\nBackbone mIoU pAcc\nCOCO-Stuff\nDINOv2 (Oquab et al., 2023) ViT-B/14\n45.3\n78.0\nCityscapes\n29.9\n89.8\nPascal VOC\n53.2\n91.5\nCOCO-Stuff\niBOT (Zhou et al., 2022)\nViT-B/16\n39.5\n73.8\nCityscapes\n23.0\n89.1\nPascal VOC\n53.4\n89.6\nCOCO-Stuff\nMSN (Assran et al., 2022)\nViT-S/16\n34.1\n72.1\nCityscapes\n21.2\n89.1\nPascal VOC\n30.2\n84.2\nCOCO-Stuff\nMAE (He et al., 2022)\nViT-B/16\n21.5\n59.1\nCityscapes\n12.5\n82.0\nPascal VOC\n25.8\n83.7\n(b) Experimental results on Cityscapes.\nMethod (C = 27)\nBackbone\nmIoU\npAcc\nIIC (Ji et al., 2019)\nResNet18\n6.4\n47.9\nPiCIE (Cho et al., 2021)\nResNet18\n10.3\n43.0\nSegSort (Hwang et al., 2019)\nResNet101\n12.3\n65.5\nSegDiscover (Huang et al., 2022)\nResNet50\n24.6\n81.9\nHSG (Ke et al., 2022)\nResNet50\n32.5\n86.0\nReCo+ (Shin et al., 2022)\nDeiT-B/8\n24.2\n83.7\nDINO (Caron et al., 2021)\nViT-S/8\n10.9\n34.5\n+ TransFGU (Yin et al., 2022)\nViT-S/8\n16.8\n77.9\n+ HP (Seong et al., 2023)\nViT-S/8\n18.4\n80.1\n+ CAUSE-MLP\nViT-S/8\n21.7\n87.7\n+ CAUSE-TR\nViT-S/8\n24.6\n89.4\nDINO (Caron et al., 2021)\nViT-B/8\n15.2\n52.6\n+ STEGO (Hamilton et al., 2022)\nViT-B/8\n21.0\n73.2\n+ HP (Seong et al., 2023)\nViT-B/8\n18.4\n79.5\n+ CAUSE-MLP\nViT-B/8\n25.7\n90.3\n+ CAUSE-TR\nViT-B/8\n28.0\n90.8\n(d) Experimental results on Pascal VOC 2012.\nMethod (C = 21)\nBackbone\nmIoU\nIIC (Ji et al., 2019)\nResNet18\n9.8\nSegSort (Hwang et al., 2019)\nResNet101\n11.7\nDenseCL (Wang et al., 2021b)\nResNet50\n35.1\nHSG (Ke et al., 2022)\nResNet50\n41.9\nMaskContrast (Van Gansbeke et al., 2021)\nResNet50\n35.0\nMaskDistill (Van Gansbeke et al., 2022)\nResNet50\n48.9\nDINO (Caron et al., 2021)\nViT-S/8\n-\n+TransFGU (Yin et al., 2022)\nViT-S/8\n37.2\n+ACSeg (Li et al., 2023)\nViT-S/8\n47.1\n+CAUSE-MLP\nViT-S/8\n46.0\n+CAUSE-TR\nViT-S/8\n50.0\nDINO (Caron et al., 2021)\nViT-B/8\n-\n+DeepSpectral (Melas-Kyriazi et al., 2022)\nViT-B/8\n37.2\n+DINOSAUR (Seitzer et al., 2023)\nViT-B/8\n37.2\n+Leopart (Ziegler & Asano, 2022)\nViT-B/8\n41.7\n+COMUS (Zadaianchuk et al., 2023)\nViT-B/8\n50.0\n+CAUSE-MLP\nViT-B/8\n47.9\n+CAUSE-TR\nViT-B/8\n53.3\nprobabilities:\n1\n|Y | log p(Y |tâ€², m)= 1\n|Y | log Q\nyâˆˆY p(y|tâ€², m)=EyâˆˆY [log p(y|tâ€², m)], and (ii) utilizing\nexponential moving average (EMA) on teacher-student structure, all of which have been widely used\nby recent self-supervised learning frameworks such as Grill et al. (2020); Chen et al. (2021); Caron\net al. (2021); Zhou et al. (2022); Assran et al. (2022). Please see complete details of Step 2 procedure\nin Algorithm 2 and Appendix B.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL DETAILS\nInference.\nIn inference phase for USS, STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023)\nequally perform the following six steps: (a) learning C cluster centroids (Caron et al., 2018) from the\ntrained segmentation head output where C denotes the number of categories in dataset, (b) upsampling\nsegmentation head output to the image resolution, (c) finding the most closest centroid indices to the\nupsampled output, (d) refining the predicted indices through Fully-connected Conditional Random\nField (CRF) (KrÃ¤henbÃ¼hl & Koltun, 2011) with 10 steps, (e) Hungarian Matching (Kuhn, 1955)\nfor alignment with CRF indices and true labels, and (f) evaluating mean of intersection over union\n(mIoU) and pixel accuracy (pAcc). We follow the equal six steps with Sema of CAUSE.\nImplementation.\nFollowing recent works, we adopt DINO as an encoder baseline and freeze it,\nwhere the feature dimension c of T depends on the size of ViT: small (c = 384) or base (c = 768).\nFor hyper-parameter in the clusterbook, the number of concept k in M is set to 2048 to encompass\nconcept prototypes from pre-trained features as much as possible. During the self-supervised learning,\nthe number of feature accumulation b in concept bank is set to 100. In addition, output dimension\nr of segmentation head is set to 90 based on the dimension analysis (Koenig et al., 2023). For the\nsegmentation head, we use two variations: (i) CAUSE-MLP with simple MLP layers as in Hamilton\net al. (2022) and (ii) CAUSE-TR with a single layer transformer. Please see details in Appendix B.\n7\nTable 2: Comparing linear probing performance.\nCOCO-Stuff\nCityscapes\nMethod\nBaseline mIoU pAcc mIoU pAcc\nDINO (Caron et al., 2021)\nViT-S/8\n33.9\n68.6\n22.8\n84.6\n+HP (Seong et al., 2023)\nViT-S/8\n42.7\n75.6\n30.6\n91.2\n+CAUSE-MLP\nViT-S/8\n46.4\n77.3\n35.2\n92.1\n+CAUSE-TR\nViT-S/8\n47.2\n78.8\n37.2\n93.5\nDINO (Caron et al., 2021)\nViT-B/8\n29.4\n66.8\n23.0\n84.2\n+STEGO (Hamilton et al., 2022) ViT-B/8\n41.0\n76.1\n26.8\n90.3\n+CAUSE-MLP\nViT-B/8\n48.3\n79.8\n38.2\n93.4\n+CAUSE-TR\nViT-B/8\n52.3\n80.1\n40.2\n94.5\nTable 3: Results of CAUSE with larger categories.\nMethod\nBackbone mIoU pAcc\nCOCO-81\nMaskContrast (Van Gansbeke et al., 2021) ResNet50\n3.7\n8.8\nTransFGU (Yin et al., 2022)\nViT-S/8\n12.7\n64.3\nCAUSE-MLP\nViT-S/8\n19.1\n78.8\nCAUSE-TR\nViT-S/8\n21.2\n75.2\nCOCO-171\nIIC (Ji et al., 2019)\nResNet50\n2.2\n15.7\nPiCIE (Cho et al., 2021)\nResNet50\n5.6\n29.8\nTransFGU (Yin et al., 2022)\nViT-S/8\n12.0\n34.3\nCAUSE-MLP\nViT-S/8\n10.6\n44.9\nCAUSE-TR\nViT-S/8\n15.2\n46.6\nImage\nLabel\nSTEGO\nHP\nCAUSE\nReCo+\nsidewalk\nparking\ncar\nbus\nvegetation\nsky\nterrain\nroad\nbuilding\npole\nperson\nbicycle\nrider\ntraffic sign\ntraffic light\ntruck\nwall\nignored\nFigure 4: Qualitative comparison of unsupervised semantic segmentation for Cityscapes dataset.\nDatasets.\nWe mainly benchmark CAUSE with three datasets: COCO-Stuff (Caesar et al., 2018),\nCityscapes (Cordts et al., 2016), and Pascal VOC (Everingham et al., 2010). COCO-Stuff is a\nscene texture segmentation dataset as subset of MS-COCO 2017 (Lin et al., 2014) with full pixel\nannotations of common Stuff and Thing categories. Cityscapes is an urban street scene parsing dataset\nwith annotations. Following Ji et al. (2019); Cho et al. (2021), we use the curated 27 mid-level\ncategories from label hierarchy for COCO-Stuff and Cityscapes. As an object-centric USS, we\nfollow Van Gansbeke et al. (2022) and report the results of total 21 classes for PASCAL VOC.\n4.2\nVALIDATING CAUSE\nQuantitative & Qualitative Results.\nWe validate CAUSE by comparing with recent USS frame-\nworks using mIoU and pAcc on various datasets. Table 1 (a) and (b) show CAUSE generally\noutperforms HSG (Ke et al., 2022), TransFGU (Yin et al., 2022), STEGO (Hamilton et al., 2022),\nHP (Seong et al., 2023), and ReCo+ (Shin et al., 2022), and our method achieves state-of-the-art re-\nsults without any external information. Table 2 shows another superior quantitative results of CAUSE\nfor linear probing than baselines, which indicates competitive dense representation quality learned\nin unsupervised manners. Furthermore, Fig. 1 and Fig. 4 illustrate CAUSE effectively assembles\ndifferent level of granularity (head, torso, hand, leg, etc.), into one semantically-alike group (person).\nPlease see additional qualitative results, analyses, and failure cases in Appendix C.\nApplicability to Object-centric Semantic Segmentation.\nPreceding works, rooted in object-\ncentric semantic segmentation models (Van Gansbeke et al., 2021; Yin et al., 2022; Zadaianchuk\net al., 2023), initially generate pseudo-labels that differentiate between foreground (objects) and\nbackground. This process is typically accomplished by using Mask R-CNN (He et al., 2017) and\nDeepLabv3 (Chen et al., 2017), or saliency maps from DeepUSPS (Nguyen et al., 2019). In contrast,\nSTEGO and HP abstains from relying on any external information beyond self-supervised knowledge.\nTherefore, they inherently lack the capability to segment an image into two broad categories: objects\nand a single background category, making them unsuitable for direct application to object-centric\nsemantic segmentation. However, we highlight that simply adjusting smoother positive relaxation\nin CAUSE enables to discern background from foreground without any external information. The\nresults of Pascal VOC 2012 is shown in Table 1(d), and its figures are illustrated in Appendix C.\n8\n(a) Log scale of IoU results for each categories in COCO-Stuff (Black: Thing / Gray: Stuff)\nmIoU\npAcc\ncoco\ncity\nmIoU\npAcc\ncoco\ncity\nmIoU\npAcc\ncoco\ncity\n(b) Positive Relaxation Ï•+\n(c) Negative Relaxation Ï•âˆ’\n(d) Concept number k in M\nFigure 5: Additional experimental for in-depth analysis and ablation studies of CAUSE-TR.\nTable 4: Quantitative ablation results by controlling the other three factors of CAUSE-TR on ViT-B/8.\n(%)\nCAUSE-MLP\nCAUSE-TR\nCOCO-Stuff\nCityscapes\nCOCO-Stuff\nCityscapes\nMethod of Concept Discretization\nBank\nCRF\nmIoU\npAcc\nmIoU\npAcc\nmIoU\npAcc\nmIoU\npAcc\nMaximizing Modularity (Newman, 2006)\nâœ—\nâœ—\n24.9\n54.1\n15.8\n75.6\n27.8\n57.3\n17.3\n79.2\nâœ“\nâœ—\n31.3\n69.0\n25.3\n89.5\n39.5\n72.5\n28.8\n90.7\nâœ—\nâœ“\n27.5\n57.9\n17.3\n78.8\n30.3\n60.1\n19.6\n82.1\nâœ“\nâœ“\n34.3\n72.8\n25.7\n90.3\n41.9\n74.9\n28.0\n90.8\nK-Means++ (Arthur & Vassilvitskii, 2007)\nâœ“\nâœ“\n27.8\n64.7\n18.9\n81.3\n33.7\n62.7\n20.4\n83.2\nSpectral Clustering (Von Luxburg, 2007)\nâœ“\nâœ“\n30.7\n65.1\n20.8\n83.5\n35.9\n66.7\n22.8\n84.1\nAgglomerative Clustering (MÃ¼llner, 2011)\nâœ“\nâœ“\n31.4\n67.9\n22.2\n84.0\n37.7\n68.1\n24.5\n86.3\nWard-Hierarchical Clustering (Murtagh & Legendre, 2014)\nâœ“\nâœ“\n31.8\n67.5\n22.9\n84.7\n37.5\n68.2\n24.7\n87.0\nGeneralization Capability\nWe first incorporate alternative self-supervised methods as our baseline,\nreplacing DINO (Caron et al., 2021). In Table 1(c), we present an overview of adaptability in CAUSE\nacross DINOv2 (Oquab et al., 2023), iBOT (Zhou et al., 2022), MSN (Assran et al., 2022), and\nMAE (He et al., 2022). Furthermore, we extend the number of clusters in CAUSE by utilizing MS-\nCOCO 2017 (Lin et al., 2014), which comprises 80 object categories and one background category:\n(object-centric) COCO-81, and 171 categories encompassing both Stuff and Thing categories: COCO-\n171. Note that, positive Ï•+ relaxation is set to 0.4 and 0.55 respectively. Table 3 highlights CAUSE\nretains superior performances for USS even with larger categories. Especially, TransFGU (Yin et al.,\n2022) used Grad-CAM (Selvaraju et al., 2017) for generating category-specific pseudo-labels, thereby\nkeeping consistent mIoU performance compared with COCO-81 and COCO-171. Nonetheless,\nCAUSE has a great advantage to pAcc especially in COCO-171 without any external information.\nCategorical Analysis.\nTo demonstrate that CAUSE can effectively address the targeted level of\nsemantic grouping, we closely examine IoU results for each category. By validating the IoU results\non a logarithmic scale in Fig. 5(a), we can observe that STEGO and HP struggle with segmenting\nThing categories in COCO-Stuff, which demands fine-grained discrimination among concepts within\ncomplex scenes. In contrast, CAUSE consistently exhibits superior capability in segmenting concepts\nacross most categories. These results are largely attributed to the causal design aspects, including the\nconstruction of the concept clusterbook and concept-wise self-supervised learning among concept\nprototypes. Beyond the quantitative results, it is important to highlight again that CAUSE exhibits\nsignificantly improved visual quality in achieving targeted level of semantic groupings than baselines\nas in Fig. 1 and Fig. 4. We include further discussions and limitations in Appendix D.\nAblation Studies.\nWe conduct ablation studies on six factors of CAUSE to identify where the\neffectiveness comes from as in Fig. 5 and Table 4: (i) positive Ï•+ and (ii) negative relaxation Ï•âˆ’,\n(iii) the number of concepts k in M, (iv) the effects of concept bank Ybank and (v) fully-connected\nCRF, and (vi) discretizing methods for concept clusterbook M. Through the empirical results, we\nfirst observe the appropriate relaxation parameter plays a crucial role in determining the quality of\nself-supervised learning. Furthermore, unlike semantic representation-level pre-training (Bao et al.,\n2022), we find that the number of discretized concepts saturates after reaching 2048 for clustering.\nWe also highlight the effects of concept bank, CRF, and modularity maximization for effective USS.\n9\n5\nCONCLUSION\nIn this work, we propose a novel framework called CAusal Unsupervised Semantic sEgmentation\n(CAUSE) to address the indeterminate clustering targets that exist in unsupervised semantic segmen-\ntation tasks. By employing frontdoor adjustment, we construct the concept clusterbook as a mediator\nand utilize the concept prototypes for semantic grouping through concept-wise self-supervised learn-\ning. Extensive experiments demonstrate the effectiveness of CAUSE, resulting in state-of-the-art\nperformance in unsupervised semantic segmentation. Our findings bridge causal perspectives into the\nunsupervised prediction, and improve segmentation quality without any pixel annotations.\nREFERENCES\nVedika Agarwal, Rakshith Shetty, and Mario Fritz. Towards causal vqa: Revealing and reducing\nspurious correlations by invariant and covariant semantic editing. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9690â€“9698, 2020.\nDavid Arthur and Sergei Vassilvitskii. K-means++ the advantages of careful seeding. In Proceedings\nof the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 1027â€“1035, 2007.\nIra Assent. Clustering high dimensional data. Wiley Interdisciplinary Reviews: Data Mining and\nKnowledge Discovery, 2(4):340â€“350, 2012.\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent,\nArmand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient\nlearning. In European Conference on Computer Vision, pp. 456â€“473. Springer, 2022.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers.\nIn International Conference on Learning Representations, 2022.\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n1209â€“1218, 2018.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer\nVision, pp. 213â€“229. Springer, 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-\npervised learning of visual features. In European Conference on Computer Vision, pp. 132â€“149,\n2018.\nMathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 9650â€“9660, 2021.\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n9640â€“9649, 2021.\nJang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. Picie: Unsupervised semantic\nsegmentation using invariance and equivariance in clustering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 16794â€“16804, 2021.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban\nscene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 3213â€“3223, 2016.\n10\nJifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploiting bounding boxes to supervise convolutional\nnetworks for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 1635â€“1643, 2015.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In International Conference\non Learning Representations, 2020.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution im-\nage synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12873â€“12883, 2021.\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes (voc) challenge. International journal of computer vision, 88:303â€“338,\n2010.\nJean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural\nInformation Processing Systems, 33:21271â€“21284, 2020.\nMichael Gutmann and Aapo HyvÃ¤rinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings of the thirteenth international conference on\nartificial intelligence and statistics, pp. 297â€“304. JMLR Workshop and Conference Proceedings,\n2010.\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman.\nUnsupervised semantic segmentation by distilling feature correspondences. In International\nConference on Learning Representations, 2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 770â€“778, 2016.\nKaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. Mask r-cnn. In Proceedings of the\nIEEE International Conference on Computer Vision, pp. 2961â€“2969, 2017.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 9729â€“9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked\nautoencoders are scalable vision learners.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16000â€“16009, 2022.\nHaiyang Huang, Zhi Chen, and Cynthia Rudin. Segdiscover: Visual concept discovery via unsuper-\nvised semantic segmentation. arXiv preprint arXiv:2204.10926, 2022.\nJyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang, and\nLiang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 7334â€“7344, 2019.\nXu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised\nimage classification and segmentation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 9865â€“9874, 2019.\nYannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard\nnegative mixing for contrastive learning. Advances in Neural Information Processing Systems, 33:\n21798â€“21809, 2020.\n11\nTsung-Wei Ke, Jyh-Jing Hwang, Yunhui Guo, Xudong Wang, and Stella X Yu. Unsupervised\nhierarchical semantic segmentation with multiview cosegmentation and clustering transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n2571â€“2581, 2022.\nAnna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele. Simple does\nit: Weakly supervised instance and semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 876â€“885, 2017.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural\nInformation Processing Systems, 33:18661â€“18673, 2020.\nDonggyun Kim, Jinwoo Kim, Seongwoong Cho, Chong Luo, and Seunghoon Hong. Universal few-\nshot learning of dense prediction tasks with visual token matching. In International Conference on\nLearning Representations, 2023a.\nJunho Kim, Byung-Kwan Lee, and Yong Man Ro. Demystifying causal features on adversarial\nexamples and causal inoculation for robust network by adversarial instrumental variable regression.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n12302â€“12312, 2023b.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations), San Diega, CA, USA, 2015.\nAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Panoptic feature pyramid networks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n6399â€“6408, 2019.\nAlexander Koenig, Maximilian Schambach, and Johannes Otterbach. Uncovering the inner workings\nof stego for safe unsupervised semantic segmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 3788â€“3797, 2023.\nPhilipp KrÃ¤henbÃ¼hl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian\nedge potentials. Advances in Neural Information Processing Systems, 24, 2011.\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\nquarterly, 2(1-2):83â€“97, 1955.\nByung-Kwan Lee, Junho Kim, and Yong Man Ro. Mitigating adversarial vulnerability through causal\nparameter estimation by adversarial double machine learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 4499â€“4509, October 2023.\nKehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian Zhao, Guoli Song, Chang Liu, Li Yuan,\nand Jie Chen. Acseg: Adaptive conceptualization for unsupervised semantic segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\nDi Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convo-\nlutional networks for semantic segmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 3159â€“3167, 2016.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollÃ¡r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nConference on Computer Vision, pp. 740â€“755. Springer, 2014.\nBing Liu, Dong Wang, Xu Yang, Yong Zhou, Rui Yao, Zhiwen Shao, and Jiaqi Zhao. Show,\ndeconfound and tell: Image captioning with causal inference. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 18041â€“18050, 2022.\nFangrui Lv, Jian Liang, Shuang Li, Bin Zang, Chi Harold Liu, Ziteng Wang, and Di Liu. Causality\ninspired representation learning for domain generalization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 8046â€“8056, 2022.\n12\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Deep spectral methods:\nA surprisingly strong baseline for unsupervised semantic segmentation and localization.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n8364â€“8375, 2022.\nDaniel MÃ¼llner.\nModern hierarchical, agglomerative clustering algorithms.\narXiv preprint\narXiv:1109.2378, 2011.\nFionn Murtagh and Pierre Legendre. Wardâ€™s hierarchical agglomerative clustering method: which\nalgorithms implement wardâ€™s criterion? Journal of classification, 31:274â€“295, 2014.\nMark EJ Newman. Modularity and community structure in networks. Proceedings of the National\nAcademy of Sciencess, 103(23):8577â€“8582, 2006.\nTam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong Nguyen,\nZhongyu Lou, and Thomas Brox. Deepusps: Deep robust unsupervised saliency prediction via\nself-supervision. Advances in Neural Information Processing Systems, 32, 2019.\nYulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counter-\nfactual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 12700â€“12710, 2021.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nMaxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\nJudea Pearl. [bayesian analysis in expert systems]: comment: graphical models, causality and\nintervention. Statistical Science, 8(3):266â€“269, 1993.\nJudea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669â€“688, 1995.\nJudea Pearl. Causality. Cambridge university press, 2009.\nRenÃ© Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12179â€“12188,\n2021.\nJoshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning\nwith hard negative samples. In International Conference on Learning Representations, 2021.\nBernhard SchÃ¶lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,\nAnirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the\nIEEE, 109(5):612â€“634, 2021.\nMaximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann\nSimon-Gabriel, Tong He, Zheng Zhang, Bernhard SchÃ¶lkopf, Thomas Brox, and Francesco\nLocatello. Bridging the gap to real-world object-centric learning. In International Conference on\nLearning Representations, 2023.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-\nization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 618â€“626,\n2017.\nHyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for\nunsupervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023.\nGyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Retrieve and co-segment for zero-shot transfer.\nAdvances in Neural Information Processing Systems, 35:33754â€“33767, 2022.\n13\nKaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the\ngood and removing the bad momentum causal effect. Advances in Neural Information Processing\nSystems, 33:1513â€“1524, 2020a.\nKaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph\ngeneration from biased training. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 3716â€“3725, 2020b.\nMeng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and Yuri\nBoykov. On regularized losses for weakly-supervised cnn segmentation. In European Conference\non Computer Vision, pp. 507â€“522, 2018.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nNeural Information Processing Systems, 30, 2017.\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Unsupervised\nsemantic segmentation by contrasting object mask proposals. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 10052â€“10062, 2021.\nWouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering object masks with\ntransformers for unsupervised semantic segmentation. arXiv preprint arXiv:2206.06363, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing\nSystems, 30, 2017.\nUlrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395â€“416, 2007.\nRuoyu Wang, Mingyang Yi, Zhitang Chen, and Shengyu Zhu. Out-of-distribution generalization\nwith causal invariant transformations. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 375â€“385, 2022.\nTan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual commonsense r-cnn. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n10760â€“10770, 2020a.\nWenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc Van Gool. Ex-\nploring cross-image pixel contrast for semantic segmentation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 7303â€“7313, 2021a.\nXinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning\nfor self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 3024â€“3033, 2021b.\nYude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen. Self-supervised equivariant at-\ntention mechanism for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 12275â€“12284, 2020b.\nXin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and XIAOJUAN QI. Self-supervised visual\nrepresentation learning with semantic grouping. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\nJia Xu, Alexander G Schwing, and Raquel Urtasun. Learning to segment under various forms of\nweak supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 3781â€“3790, 2015.\nXu Yang, Hanwang Zhang, and Jianfei Cai. Deconfounded image captioning: A causal retrospect.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2021a.\nXu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. Causal attention for vision-language tasks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n9847â€“9857, 2021b.\n14\nZhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Hanling Zhang, Hao Li, and Rong Jin. Transfgu:\na top-down approach to fine-grained unsupervised semantic segmentation. In European Conference\non Computer Vision, pp. 73â€“89. Springer, 2022.\nZhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning.\nAdvances in Neural Information Processing Systems, 33:2734â€“2746, 2020.\nZhongqi Yue, Tan Wang, Qianru Sun, Xian-Sheng Hua, and Hanwang Zhang. Counterfactual zero-\nshot and open-set visual recognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 15404â€“15414, 2021.\nAndrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, and Thomas Brox.\nUnsupervised semantic segmentation with self-supervised object-centric representations.\nIn\nInternational Conference on Learning Representations, 2023.\nDong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, and Qianru Sun. Causal intervention for\nweakly-supervised semantic segmentation. Advances in Neural Information Processing Systems,\n33:655â€“666, 2020a.\nFeihu Zhang, Philip Torr, RenÃ© Ranftl, and Stephan Richter. Looking beyond single images for\ncontrastive semantic segmentation learning. Advances in Neural Information Processing Systems,\n34:3285â€“3297, 2021.\nShengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia\nYang, and Fei Wu. Devlbert: Learning deconfounded visio-linguistic representations. In ACM\nInternational Conference on Multimedia, pp. 4373â€“4382, 2020b.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image\nBERT pre-training with online tokenizer. In International Conference on Learning Representations,\n2022.\nAdrian Ziegler and Yuki M Asano. Self-supervised learning of object parts for semantic segmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n14502â€“14511, 2022.\nA\nEXPANSION OF RELATED WORKS\nUnsupervised Semantic Segmentation.\nOne of the key challenges in unsupervised dense prediction\nis the need to learn semantic representations for each pixel without the guidance of labeled data. In\nan early work for unsupervised semantic segmentation (USS), Ji et al. (2019) introduced the IIC\nframework, which aims to maximize mutual information between feature representations obtained\nfrom augmented views. Subsequently, several methods have advanced the quality of segmentation by\nintroducing inductive bias through cross-image correspondences (Hwang et al., 2019; Cho et al., 2021;\nWen et al., 2022) or by incorporating saliency information in an end-to-end manner (Van Gansbeke\net al., 2021; Ke et al., 2022).\nMore recently, the discovery of semantic consistency in pre-trained self-supervised frameworks\nat the feature attention map (Caron et al., 2021) has led to prevalent approaches. Hamilton et al.\n(2022) introduced a method that leverages pre-trained knowledge and distills this information into\nthe unsupervised segmentation task. Following this, various works (Wen et al., 2022; Yin et al.,\n2022; Ziegler & Asano, 2022) have employed self-supervised representations as pseudo segmentation\nlabels (Zadaianchuk et al., 2023; Li et al., 2023) or as pre-encoded representations to incorporate ad-\nditional prior knowledge (Van Gansbeke et al., 2021; Zadaianchuk et al., 2023) into the segmentation\nframeworks.\nOur work aligns with Hamilton et al. (2022); Seong et al. (2023) in terms of enhancing segmen-\ntation features solely with the pre-trained representation. However, we emphasize the presence of\nindeterminate clustering targets inherent in unsupervised segmentation tasks. Our qualitative and\nquantitative results have demonstrated that the absence of evident clustering targets leads to poor\nsegmentation outcomes in unsupervised settings. These negative effects have not been adequately\naddressed in previous works within the existing literature. Accordingly, for the first time, we interpret\n15\nthe unsupervised segmentation task within the context of causality, effectively bridging discretized\nrepresentation learning and contrastive learning within this task.\nCausal Inference.\nIn recent years, numerous studies (Wang et al., 2020a; Zhang et al., 2020b;\nSchÃ¶lkopf et al., 2021; Lv et al., 2022) have applied causal inference techniques in DNNs to estimate\nthe true causal effects between treatments and outcomes of interest. The fundamental approach to\nachieve causal identification involves blocking backdoor paths induced from confounders.\nIn several computer vision methods have employed various causal approaches such as backdoor\nadjustment establishing explicit confounders (Tang et al., 2020a; Zhang et al., 2020a; Yue et al.,\n2020; Liu et al., 2022), mediation analysis (Tang et al., 2020b; Niu et al., 2021), and generating\ncounterfactual augmentations for randomized treatment assignments (Agarwal et al., 2020; Yue et al.,\n2021; Wang et al., 2022) and have been successfully applied at task-specific levels. More recently,\nvarious works (Kim et al., 2023b; Lee et al., 2023) have demonstrated that the causal approaches can\nbe applied into the more specific computer vision areas with more sophisticated theories.\nHowever, one of the challenges of applying causal inference to computer vision tasks is the explicit\ndefinition of confounding variables and the full control of them. Accordingly, we utilize frontdoor\nadjustment (Pearl, 1995) which can identify causal effects without the requirement of observed\nconfounders, but relatively less explored in the context of computer vision tasks (Yang et al., 2021b;a).\nInspired by recent developments in discrete representation learning (Van Den Oord et al., 2017; Esser\net al., 2021), we proactively build a discretized concept representation and serve it as an informative\nmediator, allowing us to establish criteria for identifying positive and negative samples for a given\nquery pixel representation. Consequently, this approach facilitates the integration of discretized\nrepresentation and self-supervised learning into the process of unsupervised semantic segmentation.\nB\nDETAILED IMPLEMENTAION OF CAUSE\nWe present a detailed description of a concrete implementation for CAUSE, expanding upon the\nalgorithms outlined in the method section and providing additional implementation details not covered\nin the experiment section. To validate identifiable and reproducible performance described in the\nexperiment section, one can access the trained parameters of CAUSE-MLP and CAUSE-TR, as well\nas their visual quality, through the code document available in the supplementary material.\nB.1\nMAXIMIZING MODULARITY\nWhen generating a mediator to design the concept cluster book, we need to compute a cluster assign-\nment matrix C âˆˆRhwÃ—k as described in Algorithm 1 in our manuscript. However, a computational\nissue arises when k becomes large, such as the value of 2048 selected for the main experiments,\nthus computing the measure of Modularity H becomes computationally expensive. To address this\nissue, we utilize the trace property of exchanging the inner multiplication terms, thereby reducing the\ncomputational burden, which can be written as follows:\nH = 1\n2eTr\nï£«\nï£¬\nï£¬\nï£¬\nï£­C(T, M)T\n\u0014\nA âˆ’ddT\n2e\n\u0015\nC(T, M)\n|\n{z\n}\nRkÃ—k\nï£¶\nï£·\nï£·\nï£·\nï£¸= 1\n2eTr\nï£«\nï£¬\nï£­C(T, M)C(T, M)T\n|\n{z\n}\nRhwÃ—hw\n\u0014\nA âˆ’ddT\n2e\n\u0015\nï£¶\nï£·\nï£¸. (5)\nHowever, directly calculating the above formulation can lead to very small Modularity due to\nmultiplying very small two values from C(T, M)C(T, M)T, rendering it ineffective optimization. To\novercome this, we use the hyperbolic tangent and temperature term, which can be written as:\nH â‰ˆ1\n2eTr\n\u0012\ntanh\n\u0012C(T, M)C(T, M)T\nÏ„\n\u0013 \u0014\nA âˆ’ddT\n2e\n\u0015\u0013\n,\n(6)\nin order to scale up this value while it can maintain the possible range of the multiplication\nC(T, M)C(T, M)T from the following range:\n0 â‰¤tanh\n\u0012C(T, M)C(T, M)T\nÏ„\n\u0013\n< 1,\n(7)\n16\nsuch that it satisfies 0 â‰¤C(T, M)C(T, M)T â‰¤1 originated from the definition of the clustering\nassignment: 0 â‰¤C(T, M) = max(0, cos(T, M)) â‰¤1.\nB.2\nTRANSFORMER-BASED SEGMENTATION HEAD\nWe use a single layer transformer decoder inspired by Vaswani et al. (2017); Carion et al. (2020) to\nbuild segmentation head with self-attention (SA), cross-attention (CA), and feed forward network\n(FFN) with its 2048 inner-dimension by default hyper-parameter (Vaswani et al., 2017), where a single\nhead attention is used on its enough performance. To explain the detailed propagation procedure for\nCAUSE-TR, we first show vector-quantization mechanism for the pre-trained feature representation\nT = {t âˆˆRc}hw by replacing each patch feature point t with each of the most closest concept\nfeatures in M as follows:\nQ = {q âˆˆRc | q = arg max\nmâˆˆM cos(t, m)}hw.\n(8)\nNext, Q is first propagated in SA, and Q and T are considered as query and key/value, respectively in\nCA, and the output of CA is propagated in FFN. Note that, learnable positional embedding is used\nin both query/key of SA and query of CA as Carion et al. (2020) have carried out. One different\nstructure is to adopt additional two MLP layers in order to reduce the dimension from c (ViT-S:384,\nViT-B:768) to r (90) for segmentation head output Y . This is because Koenig et al. (2023) empirically\ndemonstrate that higher dimension r over 100 brings in gradual collapse of clustering performance\nderived from the curse of dimensionality (Assent, 2012).\nB.3\nANCHOR SELECTION\nIn line 5 of Algorithm 2, we describe that we sample anchor patch feature point y in Y = {y âˆˆRr}hw.\nIn reality, it is extremely inefficient to select all number hw of the patch feature points in Y to perform\nanchor points in self-supervised learning, because of the limitation of the resource-constrained\nhardware. Therefore, we use a high-computation-reduced technique of stochastic sampling only\n6.25% points ( 1\n4\n2 Ã— 100(%)) among the number hw points in Y , where we randomly select one\nfeature point whenever a window having kernel size 4 Ã— 4 and stride 4 is sliding along with Y .\nB.4\nPOSITIVE & NEGATIVE CONCEPT SELECTION\nIn line 6 of Algorithm 2, we either describe that we sample positive and negative concept features\ny+, yâˆ’in the set of Yema and Ybank for the given anchor patch feature point y: it expresses y+, yâˆ’âˆ¼\n{Yema, Ybank | y}. First, we find the patch feature point t corresponding to y and then search the most\nclosest concept q = arg maxmâˆˆM cos(t, m) and its index idq. Next, we filter the positive y+ and\nnegative yâˆ’concept features satisfying each condition on DM[idq, :] > Ï•+ and DM[idq, :] < Ï•âˆ’.\nThen, we sample all of the positive and negative concept features in the set of Yema and Ybank. Note\nthat, there are a few case that the row vector in DM has a minimum value over zero, thus we\ntechnically set hard negative relaxation to 0.1, instead of 0.\nB.5\nCONCEPT BANK\nIn line 10 of Algorithm 2, the concept bank Ybank follows a specific rule: not all of the segmentation\nfeatures Yema are collected, but they are instead 50% re-sampled based on the most closest concept\nindices individually, where the concept bank collects a maximum of 100 features per concept\nprototype. Before re-sampling, 50% of Ybank is randomly discarded. Considering that we have\nthe number 2048 of concept prototypes, the concept bank stores total number 100 Ã— 2048 of the\nsegmentation features. This ensures that the concept bank contains a comprehensive collection of\ntreatment candidates T â€², providing the diversity of selecting positive and negative concept features.\nB.6\nIMAGE RESOLUTION AND AUGMENTATION\nFor COCO-Stuff and Cityscapes, we equally follow data-augmentation method of STEGO (Hamilton\net al., 2022) and HP (Seong et al., 2023) which employ five-crop with crop ratio of 0.5 in full image\nresolution and resizes the cropped images to 224 Ã— 224 for CAUSE-MLP in training phase. For\n17\ninference phase, images are resized to 320Ã—320 along the minor axis followed by center crops of each\nvalidation image. For CAUSE-TR, 320 Ã— 320 image resolution is used to train segmentation head of\na single layer transformer, because the same number of queries and learnable positional embeddings\nis used in training and inference phase. For Pascal VOC 2012, COCO-81, and COCO-171, we\nfollow data-augmentation method of TransFGU (Yin et al., 2022) which employs multiple-crop with\nmultiple ratio. A significant different point is that STEGO, HP, and TransFGU employ additional\ndata-augmentation techniques, including Horizontal Flip, Color-Jittering, Gray-scaling, and Gaussian-\nBlurring as geometric and photometric transforms, but CAUSE utilizes Horizontal Flip only.\nC\nADDITIONAL EXPERIMENTS\nDue to page limitations, we are unable to include a comprehensive set of visual results for unsuper-\nvised semantic segmentation on multiple datasets in the main manuscript. In this additional section,\nwe provide various examples primarily from four datasets and show the comparison results with\nbaseline methods.\nC.1\nADDITIONAL QUALITATIVE RESULTS\nTo provide further evidence of unsupervised semantic segmentation results, we include additional\nqualitative visual results in Fig.6 and Fig.8 for COCO-Stuff and Cityscapes, respectively. The entire\nexperimental setup remains consistent with the main manuscript, and we compare our proposed\nmethod with recent unsupervised semantic segmentation baselines (Hamilton et al., 2022; Seong et al.,\n2023; Shin et al., 2022) that also utilize pre-trained DINO (Caron et al., 2021) feature representations.\nAdditionally, we present qualitative results for object-centric semantic segmentation by providing\nvisualizations for the PASCAL VOC, COCO-81 and COCO-171 in Fig. 9 and Fig. 7, respectively.\nAll of these datasets include an additional background class. While the negative relaxation is set to\nthe same value of 0.1, we have adjusted the positive relaxation to 0.2, 0.4, and 0.55 for PASCAL\nVOC, COCO-81, and COCO-171 datasets, respectively. This modification is primarily due to account\nfor the coarsely merged background class, as it aids in distinguishing the intricate integration of the\nbackground concepts.\nC.2\nFAILURE CASES\nUnsupervised semantic segmentation is considered fundamentally challenging due to the combination\nof the absence of supervision and the need for dense pixel-level predictions. Even if we successfully\nachieve competitive unsupervised segmentation performance, there are some failure cases in which\nCAUSE may produce inadequate segmentation quality.\nIn Fig. 10, we present failed segmentation with other baselines. One of the observations is the\nexistence of noisy segmentation outcomes, especially in complex scenes. A straightforward solution\nis to adjust the larger number k when constructing the concept clusterbook M. However, we have\nobserved a trade-off between handling complex scenes and dealing with relatively easier examples. To\nfundamentally address this issue in future directions, we expect to explore pre-processing techniques\nand incorporate multi-scale feature extraction methods, as demonstrated in previous works such\nas Kirillov et al. (2019); Kim et al. (2023a); Ranftl et al. (2021). These approaches aim to enhance\nthe precision of detailed dense prediction.\nAdditionally, we have observed instances where the cluster assignments were incorrectly predicted\nfor certain object instances. We believe that these failures can primarily be attributed to two factors:\n(i) the inherent limitations of leveraging pre-trained self-supervised frameworks originally designed\nfor image-level prediction tasks, and (ii) the possibility of incompleteness in the employed clustering\nmethods. As part of our future work, we believe it is essential to utilize foundation networks\nspecifically designed for dense prediction tasks and clustering algorithms that can operate in an\nunsupervised manner. This approach will likely lead to more robust performance for unsupervised\nsemantic segmentation.\n18\nC.3\nCONCEPT ANALYSIS IN CLUSTERBOOK\nIn the proposed causal approach in unsupervised semantic segmentation, we define discretized\nrepresentation as a mediator (concept clusterbook) and leverage the advantages of discretization to\nfacilitate concept-wise self-supervised learning through frontdoor adjustment. A natural question\nwould be: What is included in the clusterbook within the representation space? To address the\nquestion, we conduct additional experiments that focused on retrieving concepts using the shared\nindex of clusterbook. Firstly, we select an anchor index from the total 2048 concepts in clusterbook.\nThen, we retrieve image regions that corresponds to the same cluster index as the anchor. Furthermore,\nto merge more wider image regions considering pixel-level clusterbook indices, we employ the\nconcept distance matrix as explained in Section 3.3. Specifically, we merge image regions based on\ntheir discretized index when the concept distance with the anchor index exceeds positive relaxation\nof softly 0.3. The retrieved results can be found in Fig. 11.\nD\nDISCUSSIONS AND LIMITATIONS\nBootstrapping Pre-trained Models.\nIt is significantly challenging to handle fine-grained and\ncomplex scenes when dealing with unsupervised semantic segmentation using pre-trained feature\nrepresentation. Based on the fact that the pre-trained features are designed to capture high-level\nsemantic information, STEGO (Hamilton et al., 2022), HP (Seong et al., 2023), and ReCo+ (Shin\net al., 2022) struggle yet fail to precisely segment intricate details within images, especially in\nscenarios with densely packed objects, complex backgrounds, or small objects as observed in Fig. 6-9.\nThis is because the pre-trained models, originally designed for tasks like image classification or\nobject detection, are not perfectly matched to understand the different level of granularity required for\nfine-grained segmentation. In contrast, our novel framework, CAUSE, bootstraps the knowledge of\nhigh-level pre-trained feature representation to achieve semantic grouping at pixel-level by bridging a\ncausal approach combining the discrete concept representation with concept-wise self-supervised\nlearning.\nHeuristically Static Hyper-Parameter.\nCAUSE carefully assembles the concept clusterbook M,\nin advance, considering which concept features should be amalgamated or distinguished based on\nthe intricate concept-wise distance matrix DM. One of nuisances involves heuristically establishing\nselection criterion for positive Ï•+ and negative Ï•âˆ’relaxation, allowing for the construction of\ndifferent level of granularity within the semantic groups Y . However, tailoring these criterion to\nthe specifics of a dataset can be a challenging endeavor. Grid-search, ranging ambitiously from 0.1\nto 0.7, is employed in the quest to find optimal relaxation values, but such task demands heuristic\nefforts. Moreover, in the realm of image processing, adapting to dynamic environmental contexts\nwithin images, encompassing scenarios such as the presence of small objects or intricate scenes, is\nimperative. In future direction, it requires more dynamical process of selecting criterion, particularly\nfor such specialized and complex contexts.\n19\nImg\nGT\nSTEGO\nHP\nReCo+ Ours\nImg\nGT\nSTEGO\nHP\nReCo+ Ours\nFigure 6: Additional qualitative results of unsupervised semantic segmentation for Coco-Stuff. Please\nlook up the object color maps in the main manuscripts.\nImg\nGT\nCAUSE Img\nGT\nCAUSE\nImg\nGT\nCAUSE Img\nGT\nCAUSE\nFigure 7: Qualitative results of unsupervised semantic segmentation for COCO-171, which is\nspecialized for object-centric semantic segmentation with 171 categories.\n20\nImg\nGT\nSTEGO\nHP\nReCo+ Ours\nImg\nGT\nSTEGO\nHP\nReCo+ Ours\nFigure 8: Additional qualitative results of unsupervised semantic segmentation for Cityscapes. Please\nlook up the object color maps in the main manuscripts.\nImg\nGT\nCAUSE Img\nGT\nCAUSE\nImg\nGT\nCAUSE Img\nGT\nCAUSE\n(a) PASCAL VOC\n(b) COCO-81\nFigure 9: Qualitative results of unsupervised semantic segmentation for PASCAL VOC and COCO-\n81, both of which are specialized for object-centric semantic segmentation.\n21\nImage\nLabel\nSTEGO\nHP\nCAUSE\nReCo+\nFigure 10: Failure cases of CAUSE and comparison results with other baselines.\nPerson\nAnchor ð‘´ð‘´index: 412\nAnchor ð‘´ð‘´index: 1708\nPredicted ð’€ð’€\nPredicted ð’€ð’€\nAnimal\nFigure 11: Retrieval results of the concept with respect to the shared index on clusterBook. We select\nPerson and Animal categories and CAUSE prediction results on COCO-Stuff.\n22\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-10-11",
  "updated": "2023-10-11"
}