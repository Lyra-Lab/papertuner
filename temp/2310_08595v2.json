{
  "id": "http://arxiv.org/abs/2310.08595v2",
  "title": "Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation",
  "authors": [
    "Badr Ben Elallid",
    "Hamza El Alaoui",
    "Nabil Benamar"
  ],
  "abstract": "In this paper, we explore the challenges associated with navigating complex\nT-intersections in dense traffic scenarios for autonomous vehicles (AVs).\nReinforcement learning algorithms have emerged as a promising approach to\naddress these challenges by enabling AVs to make safe and efficient decisions\nin real-time. Here, we address the problem of efficiently and safely navigating\nT-intersections using a lower-cost, single-agent approach based on the Twin\nDelayed Deep Deterministic Policy Gradient (TD3) reinforcement learning\nalgorithm. We show that our TD3-based method, when trained and tested in the\nCARLA simulation platform, demonstrates stable convergence and improved safety\nperformance in various traffic densities. Our results reveal that the proposed\napproach enables the AV to effectively navigate T-intersections, outperforming\nprevious methods in terms of travel delays, collision minimization, and overall\ncost. This study contributes to the growing body of knowledge on reinforcement\nlearning applications in autonomous driving and highlights the potential of\nsingle-agent, cost-effective methods for addressing more complex driving\nscenarios and advancing reinforcement learning algorithms in the future.",
  "text": "Deep Reinforcement Learning for Autonomous\nVehicle Intersection Navigation\nBadr Ben Elallid1, Hamza El Alaoui2, and Nabil Benamar1, 2\n1Moulay Ismail University, Meknes, Morocco. 2Al Akhawayn University in Ifrane, Morocco.\nbadr.benelallid@edu.umi.ac.ma, h.elalaoui@aui.ma, n.benamar@umi.ac.ma\nAbstract—In this paper, we explore the challenges associated\nwith\nnavigating\ncomplex\nT-intersections\nin\ndense\ntraffic\nscenarios for autonomous vehicles (AVs). Reinforcement learning\nalgorithms have emerged as a promising approach to address\nthese challenges by enabling AVs to make safe and efficient\ndecisions in real-time. Here, we address the problem of efficiently\nand safely navigating T-intersections using a lower-cost, single-\nagent approach based on the Twin Delayed Deep Deterministic\nPolicy Gradient (TD3) reinforcement learning algorithm. We\nshow that our TD3-based method, when trained and tested in the\nCARLA simulation platform, demonstrates stable convergence\nand improved safety performance in various traffic densities.\nOur results reveal that the proposed approach enables the AV\nto effectively navigate T-intersections, outperforming previous\nmethods in terms of travel delays, collision minimization, and\noverall cost. This study contributes to the growing body of\nknowledge on reinforcement learning applications in autonomous\ndriving and highlights the potential of single-agent, cost-effective\nmethods for addressing more complex driving scenarios and\nadvancing reinforcement learning algorithms in the future.\nIndex Terms—Autonomous vehicles, reinforcement learning,\ntwin\ndelayed\ndeep\ndeterministic\npolicy\ngradient\n(TD3),\nintersection navigation, CARLA simulator\nI. INTRODUCTION\nIntersections present a considerable challenge to road safety\ndue to their intricate traffic conditions, accounting for 36%\nof road collisions [1]. Conventional methods of controlling\nvehicle flow, such as traffic lights and stop signs, often\nhinder traffic progression and restrict intersection capacity.\nAutonomous driving strategies hold the potential to enhance\nintersection navigation by minimizing collisions caused by\nhuman error and optimizing driving behavior to reduce travel\ntimes [2], [3].\nExisting intersection navigation solutions in autonomous\ndriving predominantly depend on vehicle-to-vehicle (V2V)\ncommunication and centralized systems. In this paper, we\ninvestigate the prospects of a single-agent approach employing\nReinforcement Learning (RL) techniques to develop advanced,\nadaptive, and cost-efficient solutions for traversing complex\nintersections [4], [5].\nIn this study, we apply the Twin-Delayed DDPG algorithm\n(TD3) to facilitate autonomous vehicles’ (AVs) intersection\nnavigation\nwithout\nrelying\non\nV2V\ncommunication\nor\ncentralized systems. Our model trains AVs to arrive at their\ndestinations without collisions by processing features extracted\nfrom images produced by the vehicle’s front camera sensor and\nemploying TD3 to predict the optimal action for each state.\nWe simulate and train the proposed method using the CARLA\nsimulator. Simulations results demonstrate the capacity of our\nmodel to learn over episodes in terms of reducing travel delay\nand collision rate.\nThe remainder of the paper is structured as follows:\nSection II reviews related works; Section III presents the\nproblem formulation, including the state space, actions, and\nreward functions; Section IV details the simulation setup and\nexperimental design; Section V presents our simulation results.\nFinally, Section VI concludes the paper and highlights future\nwork and potential research directions.\nII. RELATED WORKS\nIn\nrecent\nyears,\nautonomous\ndriving\nresearch\nhas\nadvanced significantly, particularly in the area of intersection\nmanagement. Two primary solutions have emerged: vehicle-\nto-vehicle (V2V) communication and centralized solutions.\nConcurrently, reinforcement learning (RL) algorithms have\ngained traction within autonomous driving, especially for\nnavigation tasks. However, their application in intersection\nnavigation remains under investigated. Thus, while V2V\nand centralized strategies dominate the literature, RL-based\napproaches present a promising yet unexplored research\ndirection [6].\nA. Vehicle-to-Vehicle Solutions\nV2V\ncommunication\nhas\nbeen\nproposed\nas\na\nway\nto improve traffic throughput and safety at intersections\nfor autonomous vehicles. In [7], the authors investigate\nV2V communication for cooperative driving, particularly in\nintersection management. They propose V2V intersection\nprotocols that not only enhance traffic throughput but also\nprevent deadlock situations. Simulation results demonstrate\nconsiderable improvements in both performance and safety.\nAnother study [8] explores cooperative driving at blind\nintersections, which lack traffic lights, utilizing intervehicle\ncommunication. The authors introduce safety driving patterns\nrepresenting collision-free movements and develop trajectory\nplanning algorithms aiming to minimize execution time.\nSimulated results underscore the potential and utility of the\nproposed algorithms.\nB. Centralized Solutions\nCentralized\napproaches\nto\nintersection\nmanagement\ntypically rely on a Roadside Unit (RSU) to coordinate the\narXiv:2310.08595v2  [cs.RO]  16 Oct 2023\ncrossing sequence. However, this can be costly, as each\nintersection would require an RSU. For example, in [9], the\nauthors present a centralized Model Predictive Control (MPC)\napproach for the optimal control of autonomous vehicles\nwithin an intersection control area. The problem is formulated\nas a convex quadratic program, enabling efficient solutions.\nIn [10], researchers propose an autonomous T-intersection\nstrategy that combines motion-planning and path-following\ncontrol, considering oncoming vehicles. Through CarSim\nsimulations [11] and scaled car experiments, the effectiveness\nof the motion planner in generating collision-free trajectories\nand the path-following controller in ensuring safe and swift\nintersection navigation is demonstrated.\nIn\n[12],\na\nCooperative\nIntersection\nControl\n(CIC)\nmethodology is developed to improve T-intersection navigation\nfor autonomous vehicles. By employing virtual platoons and\nsimulating six vehicles crossing the intersection, the authors\ndemonstrate increased traffic efficiency without stopping.\nAlthough\nconsiderable\nprogress\nhas\nbeen\nmade\nin\nintersection management through V2V communication and\ncentralized solutions, exploring alternative approaches remains\ncrucial. Employing reinforcement learning (RL) techniques\nwithin a single-agent framework for intersection navigation\nhas the potential to produce adaptive and advanced solutions.\nThese solutions could not only improve the efficiency and\nsafety of autonomous vehicles in intricate intersections but\nalso potentially offer cost advantages over multi-agent and\ncentralized methods.\nC. Reinforcement Learning Algorithms\nReinforcement learning (RL) is a branch of machine\nlearning focused on training agents to make decisions by\ninteracting with their environment. In RL, an agent learns an\noptimal policy, which maps states to actions, by maximizing\nthe cumulative reward it receives from the environment. This\nlearning process typically involves exploring the environment\nto gather information and exploiting the knowledge acquired\nto optimize actions [13]\nPopular RL algorithms, such as Deep Q-Network [14],\n[15], Deep Deterministic Policy Gradient [16], Proximal\nPolicy Optimization [17], and Soft Actor-Critic [18], have\nbeen employed to address various challenges in autonomous\ndriving, including intersection navigation. However, their\nperformance and suitability for different driving scenarios can\nvary significantly.\nTwin Delayed DDPG [19] has emerged as a relevant\nand promising algorithm for autonomous vehicles due to\nits stability, reduced overestimation bias, and improved\nexploration capabilities. TD3 is an off-policy actor-critic\nalgorithm that extends DDPG by incorporating three key\nenhancements:\n1) Twin\nQ-networks:\nwhich\nare\nused\nto\nmitigate\noverestimation bias by maintaining two separate Q-function\napproximators and taking the minimum value of the two.\n2) Delayed policy updates: wherein the actor and target\nnetworks are updated less frequently than the Q-networks to\nimprove stability.\n3) Target policy smoothing: which adds noise to the target\nactions during the learning process to encourage exploration\nand prevent overfitting to deterministic policies.\nThese improvements have enabled TD3 to achieve superior\nperformance in a variety of tasks, including intersection\nmanagement,\ncompared\nto\nits\npredecessor\nDDPG\n[19].\nAdditionally, TD3 offers an efficient approach for learning\ncomplex decision-making policies required for navigating\nintersections safely, making it well-suited for autonomous\ndriving applications.\nD. Simulation\nEnvironments\nfor\nAutonomous\nDriving\nResearch\nVarious simulation platforms have been developed to\nfacilitate the evaluation of reinforcement learning (RL)\nalgorithms\nfor\nautonomous\ndriving\napplications.\nWidely\nused platforms include CARLA, SUMO, Gazebo, CarSim,\nand LGSVL. Each platform offers unique advantages and\ncapabilities tailored to different aspects of autonomous driving\nresearch.\nCARLA (Car Learning to Act) is an open-source simulator\nspecifically\ndesigned\nfor\nautonomous\ndriving\nresearch,\nproviding high-fidelity urban environments, diverse traffic\nscenarios, and a range of weather conditions. It enables\ncomprehensive testing and validation of autonomous driving\nalgorithms, including RL approaches [20].\nSUMO (Simulation of Urban Mobility) is a microscopic\ntraffic simulator that models individual vehicles and their\nbehavior\nin\nvarious\ntraffic\nsituations.\nIt\nis\nparticularly\nbeneficial for large-scale simulations and can be integrated\nwith\nother\nplatforms\nor\ntools,\nallowing\nresearchers\nto\ninvestigate the impact of RL algorithms on overall traffic flow\nand intersection management [21].\nGazebo is a versatile and extensible 3D robotics simulator\nthat supports multiple physics engines, sensor models, and\ncontrol interfaces. It facilitates the simulation of intricate\nautonomous driving scenarios, making it well-suited for the\nevaluation of RL algorithms in dynamic and challenging\nenvironments [22].\nCarSim is a commercial vehicle dynamics simulation\nsoftware that offers high-fidelity vehicle models and realistic\ndriving environments. It allows for the integration of control\nsystems and sensors, enabling researchers to evaluate the\nperformance of RL algorithms in terms of vehicle control and\ndynamics [11].\nLGSVL is a multi-robot AV simulator developed by LG\nElectronics America R&D Center, providing an out-of-the-\nbox solution for testing autonomous vehicle algorithms. It\noffers photo-realistic virtual environments, sensor simulation,\nand vehicle dynamics, and supports integration with popular\nAD stacks such as Autoware and Baidu Apollo [23].\nIn our research, we opted for the CARLA simulator\nplatform,\ngiven\nits\nfocus\non\nautonomous\ndriving\nand\ncomprehensive feature set. CARLA offers high-fidelity urban\nenvironments, diverse traffic scenarios, and a range of weather\nconditions, enabling rigorous testing and validation of our\nreinforcement learning (RL) algorithms [20]. Additionally,\nCARLA’s open-source nature and active community support\nmake it a highly accessible and extensible tool for our research\npurposes. The platform’s compatibility with Python allows\nseamless integration with machine learning frameworks, such\nas PyTorch and TensorFlow, streamlining the development\nprocess. Consequently, the combination of CARLA and\nPython provides a powerful and accessible foundation for our\nautonomous driving research.\nIII. PROBLEM FORMULATION\n• State space: In the real world, human drivers rely on\nmore than just their visual perception to comprehend their\nsurroundings; they also take into account the motion of\nother road users. Analogously, an autonomous vehicle (AV)\nmust utilize a sequence of images to grasp the movement of\nobjects within its environment. In this particular instance,\nour model processes a series of four consecutive RGB\nimages acquired by the AV’s front camera. These images\nhave dimensions of 800 × 600 × 3 × 4 pixels, which we\nsubsequently resize to 84 × 84 × 3 × 4 pixels and convert\ninto grayscale. The resulting state St possesses dimensions\nof 84 × 84 × 4, which are then input into our Actor and\nCritic architecture.\n• Action space: The AV in the CARLA simulator receives\nthree control commands from the environment: acceleration,\nsteering, and braking. These commands are represented\nby float values ranging [0, 1] for acceleration, [−1, 1] for\nsteering, and [0, 1] for braking. Since TD3 is a continuous\nDRL algorithm, the agent must select actions continuously.\nTherefore, at each step t, the agent needs to choose an action\nrepresented by (acceleration, steering, brake) while ensuring\nthat each command falls within its respective range.\n• Reward function : In our scenario, we formulate a reward\nfunction that takes into account potential situations in urban\ntraffic. The AV must not collide with other participants\non the road, such as vehicles, cyclists, motorcycles, and\npedestrians, while also successfully reaching its intended\ndestination. For this purpose, the reward function includes\nfour components: a reward for maintaining speed, a penalty\nfor collisions, driving in other or off the road, and the\ndistance to the goal. We can represent the current distance\nto the goal as Dcu, the previous distance to the goal as\nDpre, the velocity of the AV as Vspeed, and the measure\nof the AV being off or in another lane as Meoffroad and\nMotherlane, respectively.\nreward =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRt1 = −Ccollison\nRt2 = Dpre −Dcu\nRt3 = max(0, min(Vspeed, Vlimit))\nRt4 = −Moffroad −Motherlane\nRt5 = 100\nRt = Rt1 + Rt2 + Rt4 + Rt5\nWhere Vlimit speed limit and Ccollison is the penalty for\ncolliding with road users such as vehicles, pedestrians,\ncyclists, and motorcycles.\n• Training: Our TD3-based deep reinforcement learning\narchitecture comprises of two networks: actor and critic. The\nactor network takes the current state as input and generates\nthe next action for the agent. On the other hand, the critic\nnetwork predicts the action value based on both the state and\nthe value obtained from the actor. To encourage exploration\nduring training, the actor network adds noise to the predicted\naction. We have set the number of hidden layers to two,\nwith each layer containing 256 neurons. Other parameters\nare presented in Table I\nIV. SIMULATION SETUP\nWe employed the CARLA simulator (version 0.9.10) and\nPyTorch for our autonomous driving experiments, focusing\non a T-intersection scenario within the CARLA environment.\nThis scenario presents a complex and challenging situation for\nautonomous driving systems due to its unique traffic dynamics.\nVehicles in the T-intersection scenario must navigate\nmultiple lanes while interacting with other road users, such\nas managing oncoming traffic, merging with traffic flow, and\nresponding to pedestrians at designated crossing locations. The\nnumerous real-time decisions and actions required for safe and\nefficient navigation contribute to the scenario’s complexity.\nTo create a comprehensive and realistic simulation, we\nincluded various traffic participants like pedestrians, cars,\nbicycles, and motorcycles. This diverse set of road users\nenables a thorough assessment of the autonomous driving\nalgorithm’s performance across a wide range of traffic\nsituations and challenges.\nWe adjusted parameters such as vehicle speeds, distances\nbetween vehicles, and the frequency of traffic participants\nentering the simulation to emulate dense traffic conditions.\nThis allowed us to subject our algorithms to demanding\ncircumstances\nthat\nclosely\nresemble\nreal-world\ndriving\nconditions, essential for evaluating the effectiveness of our\nproposed solutions in complex driving environments.\nFigure 2, illustrates a dense traffic environment where\nin an autonomous vehicle navigates safely through a T-\nintersection. The vehicle starts at an initial position, follows\na designated path to its destination, and avoids collisions\nwith pedestrians, cyclists, motorcycles, and other vehicles. The\nscenario involves 300 randomly moving vehicles, including\nmotorcycles, cyclists, and four-wheeled vehicles, managed\nby the CARLA traffic manager. All vehicles are set to\nautopilot mode with a safe distance of 2.5 meters between\nthem and speed limit equal 30 m/s. We also randomly\nplace 100 pedestrians, with 80% of them crossing the road\nusing a crosswalk, adding to the challenge. Training episodes\nterminate under the following conditions: 1) collision between\nthe AV and other road users; 2) the AV successfully reaches\nits destination; 3) the episode exceeds the maximum number\nof training steps, which is set to 500.\nFig. 1: TD3-based Deep Reinforcement Learning architecture for controlling AV in a T-intersection scenario with dense traffic.\nTABLE I: Parameters used in the simulation\nParameter\nValue\nLearning rate (actor & critic)\n0.0003\nEpisodes\n2000\nBatch size\n64\nγ\n0.99\nExploration noise\n0.1\nExploration step\n10000\nPolicy update frequency\n2\nReplay Memory Size\n5000\nV. RESULTS & DISCUSSION\nIn this section, we present the results obtained using our\nproposed method. Figure 3 illustrates that the average reward\nprogressively increases over episodes, ultimately attaining a\nhigh value at 2000 episodes. Furthermore, around the 2000th\nepisode, the model stabilizes and converges, showcasing the\nefficacy of our approach.\nDuring\nthe\ntesting\nphase,\nthe\nT-intersection\nremains\nconsistent with the training phase, though the traffic density\nvaries. The density is determined by the random spawning\nof pedestrians (Ped) and other vehicles, such as cyclists and\nmotorcycles (Veh), within the environment. We select five\nscenarios: 1) Ped = 100 and V eh = 100; 2) Ped = 200 and\nV eh = 200; 3) Ped = 300 and V eh = 300; 4) Ped = 400\nand V eh = 400; 5) Ped = 450 and V eh = 450.\nThe policy network trained by the model governs the vehicle\nas it navigates its environment and reaches its destination\nduring the testing phase. To evaluate the model, we execute ten\nepisodes and measure the travel delay and number of collisions\nin each episode. Since the model’s objective is to minimize\ntravel delay and accidents in dense traffic, we compute the\naverage of both metrics. We repeat this process ten times and\nestablish the confidence interval. The test results, depicted in\nFigures 4 and 5, demonstrate that the vehicle quickly arrives\nat its destination and avoids collisions. Our model exhibits a\ndistinct advantage in evading collisions with road participants,\nas the average number of collisions remains low.\nVI. CONCLUSION\nIn this paper, we presented single-agent approach for\nnavigating\ncomplex\nT-intersections\nusing\nTwin\nDelayed\nDeep\nDeterministic\nPolicy\nGradient\n(TD3)\nin\na\ndense\ntraffic scenario. Our proposed method employs reinforcement\nlearning to train an autonomous vehicle (AV) to make safe\nand efficient decisions in real-time. We leveraged the CARLA\nsimulation platform to create a realistic urban environment,\nfeaturing diverse traffic participants and challenging driving\nconditions.\nOur results indicate that the TD3 algorithm demonstrates\nstable convergence and improved exploration capabilities,\nenabling the AV to navigate T-intersections safely and\neffectively. Furthermore, the proposed method exhibits a low\nnumber of collisions and reduced travel delays in various\ntraffic density scenarios, highlighting its potential for real-\nworld autonomous driving applications.\nIn future work, we aim to explore the integration of\nadditional sensors, such as LIDAR and RADAR, to enhance\nthe AV’s perception capabilities. Moreover, we plan to\nextend our research to more complex driving scenarios and\ninvestigate other advanced reinforcement learning algorithms\nfor improved performance and robustness. Additionally, we\nintend to explore the impact of different network architectures\n(a)\n(b)\nFig. 2: The scenario we considered : (a) The ego vehicle navigating through an T-intersection with heavy traffic; (b) The path\nthat the autonomous vehicle (AV) is supposed to follow to reach its desired destination.\nFig. 3: Average reward during episodes\nand hyperparameters on the performance of the proposed\nmethod.\nFig. 4: The average of travel delay each test\nFig. 5: The average of collisions in each test\nREFERENCES\n[1] V. Milanés, J. Pérez, E. Onieva, and C. González, “Controller for\nurban intersections based on wireless communications and fuzzy logic,”\nIEEE Transactions on Intelligent Transportation Systems, vol. 11, no. 1,\npp. 243–248, 2009.\n[2] J. Huang and H.-S. Tan, “A low-order dgps-based vehicle positioning\nsystem\nunder\nurban\nenvironment,”\nIEEE/ASME\nTransactions\non\nmechatronics, vol. 11, no. 5, pp. 567–575, 2006.\n[3] B. B. Elallid, N. Benamar, A. S. Hafid, T. Rachidi, and N. Mrani, “A\ncomprehensive survey on the application of deep and reinforcement\nlearning approaches in autonomous driving,” Journal of King Saud\nUniversity-Computer and Information Sciences, 2022.\n[4] B. B. Elallid, S. E. Hamdani, N. Benamar, and N. Mrani, “Deep learning-\nbased modeling of pedestrian perception and decision-making in refuge\nisland for autonomous driving,” in Computational Intelligence in Recent\nCommunication Networks, pp. 135–146, Springer, 2022.\n[5] B. B. Elallid, N. Benamar, N. Mrani, and T. Rachidi, “Dqn-\nbased reinforcement learning for vehicle control of autonomous\nvehicles interacting with pedestrians,” in 2022 International Conference\non Innovation and Intelligence for Informatics, Computing, and\nTechnologies (3ICT), pp. 489–493, IEEE, 2022.\n[6] B. B. Elallid, A. Abouaomar, N. Benamar, and A. Kobbane, “Vehicles\ncontrol:\nCollision\navoidance\nusing\nfederated\ndeep\nreinforcement\nlearning,” arXiv preprint arXiv:2308.02614, 2023.\n[7] S. Azimi, G. Bhatia, R. Rajkumar, and P. Mudalige, “Reliable\nintersection protocols using vehicular networks,” in Proceedings of the\nACM/IEEE 4th International Conference on Cyber-Physical Systems,\npp. 1–10, 2013.\n[8] L. Li and F.-Y. Wang, “Cooperative driving at blind crossings\nusing intervehicle communication,” IEEE Transactions on Vehicular\ntechnology, vol. 55, no. 6, pp. 1712–1724, 2006.\n[9] L. Riegger, M. Carlander, N. Lidander, N. Murgovski, and J. Sjöberg,\n“Centralized mpc for autonomous intersection crossing,” in 2016 IEEE\n19th international conference on intelligent transportation systems\n(ITSC), pp. 1372–1377, IEEE, 2016.\n[10] Y. Chen, J. Zha, and J. Wang, “An autonomous t-intersection driving\nstrategy considering oncoming vehicles based on connected vehicle\ntechnology,” IEEE/ASME Transactions on Mechatronics, vol. 24, no. 6,\npp. 2779–2790, 2019.\n[11] M. S. Corporation, “Carsim.” https://www.carsim.com, 2023. Accessed:\n[March 15, 2023].\n[12] A. I. M. Medina, N. Van de Wouw, and H. Nijmeijer, “Automation of a t-\nintersection using virtual platoons of cooperative autonomous vehicles,”\nin 2015 IEEE 18th international conference on intelligent transportation\nsystems, pp. 1696–1701, IEEE, 2015.\n[13] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nnature, vol. 518, no. 7540, pp. 529–533, 2015.\n[15] B. B. Elallid, M. Bagaa, N. Benamar, and N. Mrani, “A reinforcement\nlearning based approach for controlling autonomous vehicles in complex\nscenarios,” in 2023 International Wireless Communications and Mobile\nComputing (IWCMC), pp. 1358–1364, IEEE, 2023.\n[16] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971, 2015.\n[17] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning, pp. 1861–1870,\nPMLR, 2018.\n[19] S.\nFujimoto,\nH.\nHoof,\nand\nD.\nMeger,\n“Addressing\nfunction\napproximation error in actor-critic methods,” in International conference\non machine learning, pp. 1587–1596, PMLR, 2018.\n[20] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,\n“CARLA: An open urban driving simulator,” in Proceedings of the 1st\nAnnual Conference on Robot Learning, pp. 1–16, 2017.\n[21] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent\ndevelopment and applications of sumo-simulation of urban mobility,”\nInternational journal on advances in systems and measurements, vol. 5,\nno. 3&4, 2012.\n[22] N. Koenig and A. Howard, “Design and use paradigms for gazebo,\nan open-source multi-robot simulator,” in 2004 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS)(IEEE Cat. No.\n04CH37566), vol. 3, pp. 2149–2154, IEEE, 2004.\n[23] G. Rong, B. H. Shin, H. Tabatabaee, Q. Lu, S. Lemke, M. Možeiko,\nE. Boise, G. Uhm, M. Gerow, S. Mehta, E. Agafonov, T. H. Kim,\nE. Sterner, K. Ushiroda, M. Reyes, D. Zelenkovsky, and S. Kim, “SVL\nSimulator: A High Fidelity Simulator for Autonomous Driving,” arXiv\ne-prints, p. arXiv:2005.03778, May 2020.\n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-09-30",
  "updated": "2023-10-16"
}