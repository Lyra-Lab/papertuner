{
  "id": "http://arxiv.org/abs/2310.08887v2",
  "title": "METRA: Scalable Unsupervised RL with Metric-Aware Abstraction",
  "authors": [
    "Seohong Park",
    "Oleh Rybkin",
    "Sergey Levine"
  ],
  "abstract": "Unsupervised pre-training strategies have proven to be highly effective in\nnatural language processing and computer vision. Likewise, unsupervised\nreinforcement learning (RL) holds the promise of discovering a variety of\npotentially useful behaviors that can accelerate the learning of a wide array\nof downstream tasks. Previous unsupervised RL approaches have mainly focused on\npure exploration and mutual information skill learning. However, despite the\nprevious attempts, making unsupervised RL truly scalable still remains a major\nopen challenge: pure exploration approaches might struggle in complex\nenvironments with large state spaces, where covering every possible transition\nis infeasible, and mutual information skill learning approaches might\ncompletely fail to explore the environment due to the lack of incentives. To\nmake unsupervised RL scalable to complex, high-dimensional environments, we\npropose a novel unsupervised RL objective, which we call Metric-Aware\nAbstraction (METRA). Our main idea is, instead of directly covering the entire\nstate space, to only cover a compact latent space $Z$ that is metrically\nconnected to the state space $S$ by temporal distances. By learning to move in\nevery direction in the latent space, METRA obtains a tractable set of diverse\nbehaviors that approximately cover the state space, being scalable to\nhigh-dimensional environments. Through our experiments in five locomotion and\nmanipulation environments, we demonstrate that METRA can discover a variety of\nuseful behaviors even in complex, pixel-based environments, being the first\nunsupervised RL method that discovers diverse locomotion behaviors in\npixel-based Quadruped and Humanoid. Our code and videos are available at\nhttps://seohong.me/projects/metra/",
  "text": "Published as a conference paper at ICLR 2024\nMETRA: SCALABLE UNSUPERVISED RL\nWITH METRIC-AWARE ABSTRACTION\nSeohong Park1\nOleh Rybkin1\nSergey Levine1\n1University of California, Berkeley\nseohong@berkeley.edu\nABSTRACT\nUnsupervised pre-training strategies have proven to be highly effective in natural\nlanguage processing and computer vision. Likewise, unsupervised reinforcement\nlearning (RL) holds the promise of discovering a variety of potentially useful be-\nhaviors that can accelerate the learning of a wide array of downstream tasks. Pre-\nvious unsupervised RL approaches have mainly focused on pure exploration and\nmutual information skill learning. However, despite the previous attempts, mak-\ning unsupervised RL truly scalable still remains a major open challenge: pure\nexploration approaches might struggle in complex environments with large state\nspaces, where covering every possible transition is infeasible, and mutual infor-\nmation skill learning approaches might completely fail to explore the environ-\nment due to the lack of incentives. To make unsupervised RL scalable to com-\nplex, high-dimensional environments, we propose a novel unsupervised RL ob-\njective, which we call Metric-Aware Abstraction (METRA). Our main idea is,\ninstead of directly covering the entire state space, to only cover a compact latent\nspace Z that is metrically connected to the state space S by temporal distances.\nBy learning to move in every direction in the latent space, METRA obtains a\ntractable set of diverse behaviors that approximately cover the state space, being\nscalable to high-dimensional environments. Through our experiments in five lo-\ncomotion and manipulation environments, we demonstrate that METRA can dis-\ncover a variety of useful behaviors even in complex, pixel-based environments,\nbeing the first unsupervised RL method that discovers diverse locomotion behav-\niors in pixel-based Quadruped and Humanoid. Our code and videos are available\nat https://seohong.me/projects/metra/\n1\nINTRODUCTION\nUnsupervised pre-training has proven transformative in domains from natural language processing\nto computer vision: contrastive representation learning (Chen et al., 2020) can acquire effective\nfeatures from unlabeled images, and generative autoregressive pre-training (Brown et al., 2020)\ncan enable language models that can be adapted to a plethora of downstream applications. If we\ncould derive an equally scalable framework for unsupervised reinforcement learning (RL) that au-\ntonomously explores the space of possible behaviors, then we could enable general-purpose unsu-\npervised pre-trained agents to serve as an effective foundation for efficiently learning a broad range\nof downstream tasks. Hence, our goal in this work is to propose a scalable unsupervised RL ob-\njective that encourages an agent to explore its environment and learn a breadth of potentially useful\nbehaviors without any supervision.\nWhile this formulation of unsupervised RL has been explored in a number of prior works, making\nfully unsupervised RL truly scalable still remains a major open challenge. Prior approaches to un-\nsupervised RL can be categorized into two main groups: pure exploration methods (Burda et al.,\n2019; Pathak et al., 2019; Liu & Abbeel, 2021b; Mendonca et al., 2021; Rajeswar et al., 2023)\nand unsupervised skill discovery methods (Eysenbach et al., 2019a; Sharma et al., 2020; Laskin\net al., 2022; Park et al., 2022). While these approaches have been shown to be effective in several\nunsupervised RL benchmarks (Mendonca et al., 2021; Laskin et al., 2021), it is not entirely clear\nwhether such methods can indeed be scalable to complex environments with high intrinsic dimen-\nsionality. Pure exploration-based unsupervised RL approaches aim to either completely cover the\nentire state space (Burda et al., 2019; Liu & Abbeel, 2021b) or fully capture the transition dynamics\nof the Markov decision process (MDP) (Pathak et al., 2019; Sekar et al., 2020; Mendonca et al.,\n2021; Rajeswar et al., 2023). However, in complex environments with a large state space, it may be\n1\narXiv:2310.08887v2  [cs.LG]  10 Mar 2024\nPublished as a conference paper at ICLR 2024\nS = R64×64×3 (Pixels)\nTemporally close\nZ = R2\nMETRA covers the most \n“temporally spread-out” manifold\nTemporally distant\nFigure 1: Illustration of METRA. Our main idea for scalable unsupervised RL is to cover only the most\n“important” low-dimensional subset of the state space, analogously to PCA. Specifically, METRA covers the\nmost “temporally spread-out” (non-linear) manifold, which would lead to approximate coverage of the state\nspace S. In the example above, the two-dimensional Z space captures behaviors running in all directions, not\nnecessarily covering every possible leg pose.\ninfeasible to attain either of these aims. In fact, we will show that these methods fail to cover the\nstate space even in the state-based 29-dimensional MuJoCo Ant environment. On the other hand,\nunsupervised skill discovery methods aim to discover diverse, distinguishable behaviors, e.g., by\nmaximizing the mutual information between skills and states (Gregor et al., 2016; Eysenbach et al.,\n2019a). While these methods do learn behaviors that are mutually different, they either do not nec-\nessarily encourage exploration and thus often have limited state coverage in the complete absence of\nsupervision (Eysenbach et al., 2019a; Sharma et al., 2020), or are not directly scalable to pixel-based\ncontrol environments (Park et al., 2022; 2023b).\nIn this work, we aim to address these challenges and develop an unsupervised RL objective, which\nwe call Metric-Aware Abstraction (METRA), that scales to complex, image-based environments\nwith high intrinsic dimensionality. Our first main idea is to learn diverse behaviors that maximally\ncover not the original state space but a compact latent metric space defined by a mapping function\nϕ : S →Z with a metric d. Here, the latent state is connected by the state space by the metric\nd, which ensures that covering latent space leads to coverage of the state space. Now, the question\nbecomes which metric to use. Previous metric-based skill learning methods mostly used the Eu-\nclidean distance (or its scaled variant) between two states (He et al., 2022; Park et al., 2022; 2023b).\nHowever, such state-based metrics are not directly applicable to complex, high-dimensional state\nspace (e.g., images). Our second main idea is therefore to use temporal distances (i.e., the num-\nber of minimum environment steps between two states) as a metric for the latent space. Temporal\ndistances are invariant to state representations and thus applicable to pixel-based environments as\nwell. As a result, by maximizing coverage in the compact latent space, we can acquire diverse be-\nhaviors that approximately cover the entire state space, being scalable to high-dimensional, complex\nenvironments (Figure 1).\nThrough our experiments on five state-based and pixel-based continuous control environments, we\ndemonstrate that our method learns diverse, useful behaviors, as well as a compact latent space\nthat can be used to solve various downstream tasks in a zero-shot manner, outperforming previous\nunsupervised RL methods. To the best of our knowledge, METRA is the first unsupervised RL\nmethod that demonstrates the discovery of diverse locomotion behaviors in pixel-based Quadruped\nand Humanoid environments.\n2\nWHY MIGHT PREVIOUS UNSUPERVISED RL METHODS FAIL TO SCALE?\nThe goal of unsupervised RL is to acquire useful knowledge, such as policies, world models, or\nexploratory data, by interacting with the environment in an unsupervised manner (i.e., without tasks\nor reward functions). Typically, this knowledge is then leveraged to solve downstream tasks more\nefficiently. Prior work in unsupervised RL can be categorized into two main groups: pure explo-\nration methods and unsupervised skill discovery methods. Pure exploration methods aim to cover\nthe entire state space or fully capture the environment dynamics. They encourage exploration by\nmaximizing uncertainty (Pathak et al., 2017; Shyam et al., 2019; Burda et al., 2019; Pathak et al.,\n2019; Sekar et al., 2020; Mazzaglia et al., 2022) or state entropy (Lee et al., 2019; Pong et al.,\n2020; Liu & Abbeel, 2021b; Yarats et al., 2021). Based on the data collected by the exploration\n2\nPublished as a conference paper at ICLR 2024\nPure Exploration\nRND, Disag., APT, …\nDIAYN, DADS, …\nMETRA (ours)\nI(S; Z)\nIW(S; Z)\nMI vs. WDM\nI(S; Z)\nIW(S; Z)\n=\n<\nFigure 2: Sketch comparing different unsupervised RL objectives. Pure exploration approaches try to\ncover every possible state, which is infeasible in complex environments (e.g., such methods might be “stuck”\nat forever finding novel joint angle configurations of a robot, without fully exploring the environment; see\nFigure 3). The mutual information I(S; Z) has no underlying distance metrics, and thus does not prioritize\ncoverage enough, only focusing on skills that are discriminable. In contrast, our proposed Wasserstein de-\npendency measure IW(S; Z) maximizes the distance metric d, which we choose to be the temporal distance,\nforcing the learned skills to span the “longest” subspaces of the state space, analogously to (temporal, non-\nlinear) PCA.\npolicy, these methods learn a world model (Rajeswar et al., 2023), train a goal-conditioned pol-\nicy (Pong et al., 2020; Pitis et al., 2020; Mendonca et al., 2021; Hu et al., 2023), learn skills via\ntrajectory autoencoders (Campos Cam´u˜nez et al., 2020; Mazzaglia et al., 2023), or directly fine-\ntune the learned exploration policy (Laskin et al., 2021) to accelerate downstream task learning.\nWhile these pure exploration-based approaches are currently the leading methods in unsupervised\nRL benchmarks (Mendonca et al., 2021; Laskin et al., 2021; Mazzaglia et al., 2023; Rajeswar et al.,\n2023), their scalability may be limited in complex environments with large state spaces because it\nis often computationally infeasible to completely cover every possible state or fully capture the dy-\nnamics. In Section 5, we empirically demonstrate that these approaches even fail to cover the state\nspace of the state-based 29-dimensional Ant environment.\nAnother line of research in unsupervised RL aims to learn diverse behaviors (or skills) that are\ndistinguishable from one another, and our method also falls into this category. The most common\napproach to unsupervised skill discovery is to maximize the mutual information (MI) between states\nand skills (Gregor et al., 2016; Eysenbach et al., 2019a; Sharma et al., 2020; Hansen et al., 2020):\nI(S; Z) = DKL(p(s, z)∥p(s)p(z)).\n(1)\nBy associating different skill latent vectors z with different states s, these methods learn diverse\nskills that are mutually distinct. However, they share the limitation that they often end up discover-\ning simple, static behaviors with limited state coverage (Campos Cam´u˜nez et al., 2020; Park et al.,\n2022). This is because MI is defined by a KL divergence (Equation (1)), which is a metric-agnostic\nquantity (e.g., MI is invariant to scaling; see Figure 2). As a result, the MI objective only focuses\non the distinguishability of behaviors, regardless of “how different” they are, resulting in limited\nstate coverage (Campos Cam´u˜nez et al., 2020; Park et al., 2022). To address this limitation, prior\nworks combine the MI objective with exploration bonuses (Campos Cam´u˜nez et al., 2020; Strouse\net al., 2022; Park & Levine, 2023) or propose different objectives that encourage maximizing dis-\ntances in the state space (He et al., 2022; Park et al., 2022; 2023b). Yet, it remains unclear whether\nthese methods can scale to complex, high-dimensional environments, because they either attempt\nto completely capture the entire MDP (Campos Cam´u˜nez et al., 2020; Strouse et al., 2022; Park &\nLevine, 2023) or assume a compact, structured state space (He et al., 2022; Park et al., 2022; 2023b).\nIndeed, to the best of our knowledge, no previous unsupervised skill discovery methods have suc-\nceeded in discovering locomotion behaviors on pixel-based locomotion environments. Unlike these\napproaches, our method learns a compact set of diverse behaviors that are maximally different in\nterms of the temporal distance. As a result, they can approximately cover the state space, even in a\ncomplex, high-dimensional environment. We discuss further related work in Appendix A.\n3\nPRELIMINARIES AND PROBLEM SETTING\nWe consider a controlled Markov process, an MDP without a reward function, defined as M =\n(S, A, µ, p). S denotes the state space, A denotes the action space, µ : ∆(S) denotes the initial\nstate distribution, and p : S × A →∆(A) denotes the transition dynamics kernel. We consider a set\nof latent vectors z ∈Z, which can be either discrete or continuous, and a latent-conditioned policy\nπ(a|s, z). Following the terminology in unsupervised skill discovery, we refer to latent vectors z\n(and their corresponding policies π(a|s, z)) as skills. When sampling a trajectory, we first sample\na skill from the prior distribution, z ∼p(z), and then roll out a trajectory with π(a|s, z), where\n3\nPublished as a conference paper at ICLR 2024\nz is fixed for the entire episode. Hence, the joint skill-trajectory distribution is given as p(τ, z) =\np(z)p(s0) QT −1\nt=0 π(at|st, z)p(st+1|st, at), where τ denotes (s0, a0, s1, a1, . . . , sT ). Our goal in this\nwork is to learn a set of diverse, useful behaviors π(a|s, z), without using any supervision, data, or\nprior knowledge.\n4\nA SCALABLE OBJECTIVE FOR UNSUPERVISED RL\nDesiderata. We first state our two desiderata for a scalable unsupervised RL objective. First, instead\nof covering every possible state in a given MDP, which is infeasible in complex environments, we\nwant to have a compact latent space Z of a tractable size and a latent-conditioned policy π(a|s, z)\nthat translates latent vectors into actual behaviors. Second, we want the behaviors from different\nlatent vectors to be different, collectively covering as much of the state space as possible. In other\nwords, we want to maximize state coverage under the given capacity of Z. An algorithm that\nsatisfies these two desiderata would be scalable to complex environments, because we only need to\nlearn a compact set of behaviors that approximately cover the MDP.\nObjective. Based on the above, we propose the following novel objective for unsupervised RL:\nIW(S; Z) = W(p(s, z), p(s)p(z)),\n(2)\nwhere IW(S; Z) is the Wasserstein dependency measure (WDM) (Ozair et al., 2019) between states\nand skills, and W is the 1-Wasserstein distance on the metric space (S × Z, d) with a distance\nmetric d. Intuitively, the WDM objective in Equation (2) can be viewed as a “Wasserstein variant”\nof the previous MI objective (Equation (1)), where the KL divergence in MI is replaced with the\nWasserstein distance. However, despite the apparent similarity, there exists a significant difference\nbetween the two objectives: MI is completely agnostic to the underlying distance metric, while\nWDM is a metric-aware quantity. As a result, the WDM objective (Equation (2)) not only discovers\ndiverse skills that are different from one another, as in the MI objective, but also actively maximizes\ndistances d between different skill trajectories (Figure 2). This makes them collectively cover the\nstate space as much as possible (in terms of the given metric d). The choice of metric for d is\ncritical for effective skill discovery, and simple choices like Euclidean metrics on the state space\nwould generally not be effective for non-metric state representations, such as images. Therefore,\ninstantiating this approach with the right metric is an important part of our contribution, as we will\ndiscuss in Section 4.2. Until then, we assume that we have a given metric d.\n4.1\nTRACTABLE OPTIMIZATION\nWhile our objective IW(S; Z) has several desirable properties, it is not immediately straightforward\nto maximize this quantity in practice. In this section, we describe a simple, tractable objective\nthat can be used to maximize IW(S; Z) in practice. We begin with the Kantorovich-Rubenstein\nduality (Villani et al., 2009; Ozair et al., 2019), which provides a tractable way to maximize the\nWasserstein dependency measure:\nIW(S; Z) =\nsup\n∥f∥L≤1\nEp(s,z)[f(s, z)] −Ep(s)p(z)[f(s, z)],\n(3)\nwhere ∥f∥L denotes the Lipschitz constant for the function f : S ×Z →R under the given distance\nmetric d, i.e., ∥f∥L = sup(s1,z1)̸=(s2,z2) |f(s1, z1) −f(s2, z2)|/d((s1, z1), (s2, z2)). Intuitively, f\nis a score function that assigns larger values to (s, z) tuples sampled from the joint distribution and\nsmaller values to (s, z) tuples sampled independently from their marginal distributions. We note\nthat Equation (3) is already a tractable objective, as we can jointly train a 1-Lipschitz-constrained\nscore function f(s, z) using gradient descent and a skill policy π(a|s, z) using RL, with the reward\nfunction being an empirical estimate of Equation (3), r(s, z) = f(s, z)−N −1 PN\ni=1 f(s, zi), where\nz1, z2, . . . , zN are N independent random samples from the prior distribution p(z).\nHowever, since sampling N additional zs for each data point is computationally demanding, we will\nfurther simplify the objective to enable more efficient learning. First, we consider the parameteri-\nzation f(s, z) = ϕ(s)⊤ψ(z) with ϕ : S →RD and ψ : Z →RD with independent 1-Lipschitz\n4\nPublished as a conference paper at ICLR 2024\nconstraints1, which yields the following objective:\nIW(S; Z) ≈\nsup\n∥ϕ∥L≤1,∥ψ∥L≤1\nEp(s,z)[ϕ(s)⊤ψ(z)] −Ep(s)[ϕ(s)]⊤Ep(z)[ψ(z)].\n(4)\nHere, we note that the decomposition f(s, z) = ϕ(s)⊤ψ(z) is universal; i.e., the expressiveness of\nf(s, z) is equivalent to that of ϕ(s)⊤ψ(z) when D →∞. The proof can be found in Appendix B.\nNext, we consider a variant of the Wasserstein dependency measure that only depends on the last\nstate: IW(ST ; Z), similarly to VIC (Gregor et al., 2016). This allows us to further decompose the\nobjective with a telescoping sum as follows:\nIW(ST ; Z) ≈\nsup\n∥ϕ∥L≤1,∥ψ∥L≤1\nEp(τ,z)[ϕ(sT )⊤ψ(z)] −Ep(τ)[ϕ(sT )]⊤Ep(z)[ψ(z)]\n(5)\n= sup\nϕ,ψ\nT −1\nX\nt=0\n\u0000Ep(τ,z)[(ϕ(st+1) −ϕ(st))⊤ψ(z)] −Ep(τ)[ϕ(st+1) −ϕ(st)]⊤Ep(z)[ψ(z)]\n\u0001\n,\n(6)\nwhere we also use the fact that p(s0) and p(z) are independent. Finally, we set ψ(z) to z. While this\nmakes ψ less expressive, it allows us to derive the following concise objective:\nIW(ST ; Z) ≈\nsup\n∥ϕ∥L≤1\nEp(τ,z)\n\"T −1\nX\nt=0\n(ϕ(st+1) −ϕ(st))⊤(z −¯z)\n#\n,\n(7)\nwhere ¯z = Ep(z)[z]. Here, since we can always shift the prior distribution p(z) to have a zero\nmean, we can assume ¯z = 0 without loss of generality. This objective can now be easily maximized\nby jointly training ϕ(s) and π(a|s, z) with r(s, z, s′) = (ϕ(s′) −ϕ(s))⊤z under the constraint\n∥ϕ∥L ≤1. Note that we do not need any additional random samples of z, unlike Equation (3).\n4.2\nFULL OBJECTIVE: METRIC-AWARE ABSTRACTION (METRA)\nSo far, we have not specified the distance metric d for the Wasserstein distance in WDM (or equiv-\nalently for the Lipschitz constraint ∥ϕ∥L ≤1). Choosing an appropriate distance metric is crucial\nfor learning a compact set of useful behaviors, because it determines the priority by which the be-\nhaviors are learned within the capacity of Z. Previous metric-based skill discovery methods mostly\nemployed the Euclidean distance (or its scaled variant) as a metric (He et al., 2022; Park et al., 2022;\n2023b). However, they are not directly scalable to high-dimensional environments with pixel-based\nobservations, in which the Euclidean distance is not necessarily meaningful.\nIn this work, we propose to use the temporal distance (Kaelbling, 1993; Hartikainen et al., 2020; Du-\nrugkar et al., 2021) between two states as a distance metric dtemp(s1, s2), the minimum number of\nenvironment steps to reach s2 from s1. This provides a natural way to measure the distance between\ntwo states, as it only depends on the inherent transition dynamics of the MDP, being invariant to\nthe state representation and thus scalable to pixel-based environments. Using the temporal distance\nmetric, we can rewrite Equation (7) as follows:\nsup\nπ,ϕ\nEp(τ,z)\n\"T −1\nX\nt=0\n(ϕ(st+1) −ϕ(st))⊤z\n#\ns.t. ∥ϕ(s) −ϕ(s′)∥2 ≤1, ∀(s, s′) ∈Sadj,\n(8)\nwhere Sadj denotes the set of adjacent state pairs in the MDP. Note that ∥ϕ∥L ≤1 is equivalently\nconverted into ∥ϕ(s) −ϕ(s′)∥2 ≤1 under the temporal distance metric (see Theorem B.3).\nIntuition and interpretation. We next describe how the constrained objective in Equation (8) may\nbe interpreted. Intuitively, a policy π(a|s, z) that maximizes our objective should learn to move as\nfar as possible along various directions in the latent space (specified by z). Since distances in the\nlatent space, ∥ϕ(s1) −ϕ(s2)∥2, are always upper-bounded by the corresponding temporal distances\nin the MDP, given by dtemp(s1, s2), the learned latent space should assign its (limited) dimensions\nto the manifolds in the original state space that are maximally “spread out”, in the sense that shortest\npaths within the set of represented states should be as long as possible. This conceptually resembles\n1While ∥ϕ∥L ≤1, ∥ψ∥L ≤1 is not technically equivalent to ∥f∥L ≤1, we use the former as it is\nmore tractable. Also, we note that ∥f∥L can be upper-bounded in terms of ∥ϕ∥L, ∥ψ∥L, sups ∥ϕ(s)∥2, and\nsupz ∥ψ(z)∥2 under d((s1, z1), (s2, z2)) = (sups ∥ϕ(s)∥2)∥ψ∥Ld(z1, z2) + (supz ∥ψ(z)∥2)∥ϕ∥Ld(s1, s2).\n5\nPublished as a conference paper at ICLR 2024\nAlgorithm 1 Metric-Aware Abstraction (METRA)\n1: Initialize skill policy π(a|s, z), representation function ϕ(s), Lagrange multiplier λ, replay buffer D\n2: for i ←1 to (# epochs) do\n3:\nfor j ←1 to (# episodes per epoch) do\n4:\nSample skill z ∼p(z)\n5:\nSample trajectory τ with π(a|s, z) and add to replay buffer D\n6:\nend for\n7:\nUpdate ϕ(s) to maximize E(s,z,s′)∼D[(ϕ(s′) −ϕ(s))⊤z + λ · min(ε, 1 −∥ϕ(s) −ϕ(s′)∥2\n2)]\n8:\nUpdate λ to minimize E(s,z,s′)∼D[λ · min(ε, 1 −∥ϕ(s) −ϕ(s′)∥2\n2)]\n9:\nUpdate π(a|s, z) using SAC (Haarnoja et al., 2018a) with reward r(s, z, s′) = (ϕ(s′) −ϕ(s))⊤z\n10: end for\n“principal components” of the state space, but with respect to shortest paths rather than Euclidean\ndistances, and with non-linear ϕ rather than linear ϕ. Thus, we would expect ϕ to learn to abstract\nthe state space in a lossy manner, preserving temporal distances (Figure 10), and emphasizing those\ndegrees of freedom of the state that span the largest possible “temporal” (non-linear) manifolds\n(Figure 1). Based on this intuition, we call our method Metric-Aware Abstraction (METRA). In\nAppendix C, we derive a formal connection between METRA and principal component analysis\n(PCA) under the temporal distance metric under several simplifying assumptions.\nTheorem 4.1 (Informal statement of Theorem C.2). Under some simplifying assumptions, linear\nsquared METRA is equivalent to PCA under the temporal distance metric.\nConnections to previous skill discovery methods.\nThere exist several intriguing connections\nbetween our WDM objective (Equation (2)) and previous skill discovery methods, including DI-\nAYN (Eysenbach et al., 2019a), DADS (Sharma et al., 2020), CIC (Laskin et al., 2022), LSD (Park\net al., 2022), and CSD (Park et al., 2023b). Perhaps the most apparent connections are with LSD and\nCSD, which also use similar constrained objectives to Equation (7). In fact, although not shown by\nthe original authors, the constrained inner product objectives of LSD and CSD are also equivalent\nto IW(ST ; Z), but with the Euclidean distance (or its normalized variant), instead of the temporal\ndistance. Also, the connection between WDM and Equation (7) provides further theoretical insight\ninto the rather “ad-hoc” choice of zero-centered one-hot vectors used in discrete LSD (Park et al.,\n2022); we must use a zero-mean prior distribution due to the z −¯z term in Equation (7). There\nexist several connections between our WDM objective and previous MI-based skill discovery meth-\nods as well. Specifically, by simplifying WDM (Equation (2)) in three different ways, we can obtain\n“Wasserstein variants” of DIAYN, DADS, and CIC. We refer to Appendix D for detailed derivations.\nZero-shot goal-reaching with METRA. Thanks to the state abstraction function ϕ(s), METRA\nprovides a simple way to command the skill policy to reach a goal state in a zero-shot manner, as\nin LSD (Park et al., 2022). Since ϕ abstracts the state space preserving temporal distances, the\ndifference vector ϕ(g) −ϕ(s) tells us the skill we need to select to reach the goal state g from the\ncurrent state s. As such, by simply setting z = (ϕ(g) −ϕ(s))/∥ϕ(g) −ϕ(s)∥2 (for continuous\nskills) or z = arg maxdim (ϕ(g) −ϕ(s)) (for discrete skills), we can find the skill that leads to the\ngoal. With this technique, METRA can solve goal-conditioned tasks without learning a separate\ngoal-conditioned policy, as we will show in Section 5.3.\nImplementation. We optimize the constrained objective in Equation (8) using dual gradient descent\nwith a Lagrange multiplier λ and a small relaxation constant ε > 0, similarly to Park et al. (2023b);\nWang et al. (2023). We provide a pseudocode for METRA in Algorithm 1.\nLimitations. One potential issue with Equation (8) is that we embed the temporal distance into the\nsymmetric Euclidean distance in the latent space, where the temporal distance can be asymmetric.\nThis makes our temporal distance abstraction more “conservative” in the sense that it considers the\nminimum of both temporal distances, i.e., ∥ϕ(s1) −ϕ(s2)∥2 ≤min(dtemp(s1, s2), dtemp(s2, s1)).\nWhile this conservatism is less problematic in our benchmark environments, in which transitions are\nmostly “symmetric”, it might be overly restrictive in highly asymmetric environments. To resolve\nthis, we can replace the Euclidean distance ∥ϕ(s1) −ϕ(s2)∥2 in Equation (8) with an asymmetric\nquasimetric, as in Wang et al. (2023). We leave this extension for future work. Another limitation\nis that the simplified WDM objective in Equation (7) only considers behaviors that move linearly\nin the latent space. While this does not necessarily imply that the behaviors are also linear in the\n6\nPublished as a conference paper at ICLR 2024\nMETRA\nLSD\nDIAYN\nDADS2\nICM\nRND\nAPT\nAPS\nP2E/Disag\nAnt\n(States)\nLBS3\nHalfCheetah\n(States)\nQuadruped\n(Pixels)\nHumanoid\n(Pixels)\nKitchen\n(Pixels)\nUnsupervised skill discovery\nUnsupervised exploration\nCIC\nFigure 3: Examples of behaviors learned by 11 unsupervised RL methods. For locomotion environments,\nwe plot the x-y (or x) trajectories sampled from learned policies. For Kitchen, we measure the coincidental\nsuccess rates for six predefined tasks. Different colors represent different skills z. METRA is the only method\nthat discovers diverse locomotion skills in pixel-based Quadruped and Humanoid. We refer to Figure 12 for the\ncomplete qualitative results (8 seeds) of METRA and our project page for videos.\noriginal state space (because ϕ : S →Z is a nonlinear mapping), this simplification, which stems\nfrom the fact that we set ψ(z) = z, might restrict the diversity of behaviors to some degree. We\nbelieve this can be addressed by using the full WDM objective in Equation (4). Notably, the full\nobjective (Equation (4)) resembles contrastive learning, and we believe combining it with scalable\ncontrastive learning techniques is an exciting future research direction (see Appendix D.3).\n5\nEXPERIMENTS\nThrough our experiments in benchmark environments, we aim to answer the following questions:\n(1) Can METRA scale to complex, high-dimensional environments, including domains with image\nobservations? (2) Does METRA discover meaningful behaviors in complex environments with no\nsupervision? (3) Are the behaviors discovered by METRA useful for downstream tasks?\n5.1\nEXPERIMENTAL SETUP\nQuadruped\n(Pixels)\nHumanoid\n(Pixels)\nKitchen\n(Pixels)\nHalfCheetah\n(States)\nAnt\n(States)\nFigure 4: Benchmark environments.\nWe evaluate our method on five robotic lo-\ncomotion and manipulation environments (Fig-\nure 4): state-based Ant and HalfCheetah from\nGym (Todorov et al., 2012; Brockman et al.,\n2016), pixel-based Quadruped and Humanoid\nfrom the DeepMind Control (DMC) Suite (Tassa\net al., 2018), and a pixel-based version of Kitchen\nfrom Gupta et al. (2019); Mendonca et al. (2021). For pixel-based DMC locomotion environments,\nwe use colored floors to allow the agent to infer its location from pixels, similarly to Hafner et al.\n(2022); Park et al. (2023a) (Figure 11). Throughout the experiments, we do not use any prior knowl-\nedge, data, or supervision (e.g., observation restriction, early termination, etc.). As such, in pixel-\nbased environments, the agent must learn diverse behaviors solely from 64 × 64 × 3 camera images.\nWe compare METRA against 11 previous methods in three groups: (1) unsupervised skill discov-\nery, (2) unsupervised exploration, and (3) unsupervised goal-reaching methods. For unsupervised\nskill discovery methods, we compare against two MI-based approaches, DIAYN (Eysenbach et al.,\n2019a) and DADS (Sharma et al., 2020), one hybrid method that combines MI and an exploration\nbonus, CIC (Laskin et al., 2022), and one metric-based approach that maximizes Euclidean dis-\ntances, LSD (Park et al., 2022). For unsupervised exploration methods, we consider five pure explo-\nration approaches, ICM (Pathak et al., 2017), RND (Burda et al., 2019), Plan2Explore (Sekar et al.,\n2020) (or Disagreement (Pathak et al., 2019)), APT (Liu & Abbeel, 2021b), and LBS (Mazzaglia\net al., 2022), and one hybrid approach that combines exploration and successor features, APS (Liu &\nAbbeel, 2021a). We note that the Dreamer (Hafner et al., 2020) variants of these methods (especially\nLBS (Mazzaglia et al., 2022)) are currently the state-of-the-art methods in the pixel-based unsuper-\nvised RL benchmark (Laskin et al., 2021; Rajeswar et al., 2023). For unsupervised goal-reaching\nmethods, we mainly compare with a state-of-the-art unsupervised RL approach, LEXA (Mendonca\n7\nPublished as a conference paper at ICLR 2024\nMETRA\nLSD\nCIC\nDIAYN\nDADS\n0.0\n0.5\n1.0\n1.5\nSteps\n£107\n0\n50\n100\n150\n200\nPolicy State Coverage\nHalfCheetah (States)\n0.0\n0.5\n1.0\n1.5\nSteps\n£107\n0\n1000\n2000\nPolicy State Coverage\nAnt (States)\n0\n2\n4\n6\nSteps\n£106\n0\n100\n200\n300\nPolicy State Coverage\nQuadruped (Pixels)\n0.0\n0.5\n1.0\nSteps\n£107\n0\n25\n50\n75\n100\nPolicy State Coverage\nHumanoid (Pixels)\n0.0\n0.5\n1.0\nSteps\n£106\n0\n1\n2\n3\n4\nPolicy Task Coverage\nKitchen (Pixels)\nFigure 5: Quantitative comparison with unsupervised skill discovery methods (8 seeds). We measure the\nstate/task coverage of the policies learned by five skill discovery methods. METRA exhibits the best coverage\nacross all environments, while previous methods completely fail to explore the state spaces of pixel-based\nlocomotion environments. Notably, METRA is the only method that discovers locomotion skills in pixel-based\nQuadruped and Humanoid.\n0.0\n0.5\n1.0\nEpisodes\n£104\n2\n4\nReturn\nAntMultiGoals\n0.0\n0.5\n1.0\nEpisodes\n£105\n0.0\n2.5\n5.0\n7.5\nReturn\nHalfCheetahGoal\n0.0\n0.5\n1.0\nEpisodes\n£105\n0\n2\n4\nReturn\nHalfCheetahHurdle\n0.0\n0.5\n1.0\nEpisodes\n£104\n0\n2\n4\n6\nReturn\nQuadrupedGoal\n0.0\n0.5\n1.0\nEpisodes\n£104\n1\n2\n3\n4\nReturn\nHumanoidGoal\nMETRA\nLSD\nCIC\nDIAYN\nDADS\nFigure 6: Downstream task performance comparison of unsupervised skill discovery methods (4 seeds).\nTo verify whether learned skills are useful for downstream tasks, we train a hierarchical high-level controller\non top of the frozen skill policy to maximize task rewards. METRA exhibits the best or near-best performance\nacross the five tasks, which suggests that the behaviors learned by METRA are indeed useful for the tasks.\net al., 2021), as well as two previous skill discovery methods that enable zero-shot goal-reaching,\nDIAYN and LSD. We use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, 16 discrete\nskills for HalfCheetah, and 24 discrete skills for Kitchen. For CIC, we use 64-D skill latent vectors\nfor all environments, following the original suggestion (Laskin et al., 2022).\n5.2\nQUALITATIVE COMPARISON\nFigure 7: Example skills.\nMETRA\nlearns diverse locomotion behaviors on\npixel-based Quadruped (videos).\nWe first demonstrate examples of behaviors (or skills) learned\nby our method and the 10 prior unsupervised RL methods on\neach of the five benchmark environments in Figure 3. The\nfigure illustrates that METRA discovers diverse behaviors in\nboth state-based and pixel-based domains. Notably, METRA\nis the only method that successfully discovers locomotion\nskills in pixel-based Quadruped and Humanoid (Figure 7),\nand shows qualitatively very different behaviors from previ-\nous unsupervised RL methods across the environments. Pure\nexploration methods mostly exhibit chaotic, random behav-\niors (videos), and fail to fully explore the state space (in terms\nof x-y coordinates) even in state-based Ant and HalfCheetah.\nThis is because it is practically infeasible to completely cover\nthe infinitely many combinations of joint angles and positions\nin these domains. MI-based skill discovery methods also fail\nto explore large portions of the state space due to the metric-\nagnosticity of the KL divergence (Section 2), even when com-\nbined with an exploration bonus (i.e., CIC). LSD, a previous metric-based skill discovery method\nthat maximizes Euclidean distances, does discover locomotion skills in state-based environments,\nbut fails to scale to the pixel-based environments, where the Euclidean distance on image pixels does\nnot necessarily provide a meaningful metric. In contrast to these methods, METRA learns various\ntask-related behaviors by maximizing temporal distances in diverse ways. On our project page, we\nshow additional qualitative results of METRA with different skill spaces. We note that, when com-\nbined with a discrete latent space, METRA discovers even more diverse behaviors, such as doing\na backflip and taking a static posture, in addition to locomotion skills. We refer to Appendix E for\nvisualization of learned latent spaces of METRA.\n5.3\nQUANTITATIVE COMPARISON\nNext, we quantitatively compare METRA against three groups of 11 previous unsupervised RL\napproaches, using different metrics that are tailored to each group’s primary focus. For quantitative\nresults, we use 8 seeds and report 95% confidence intervals, unless otherwise stated.\nComparison with unsupervised skill discovery methods. We first compare METRA with other\nmethods that also aim to solve the skill discovery problem (i.e., learning a latent-conditioned policy\nπ(a|s, z) that performs different skills for different values of z). These include LSD, CIC, DIAYN,\n8\nPublished as a conference paper at ICLR 2024\n0\n2\nTime (s)\n£104\n0\n50\n100\n150\n200\nTotal State Coverage\nHalfCheetah (States)\n0\n1\n2\n3\nTime (s)\n£104\n0\n5000\n10000\nTotal State Coverage\nAnt (States)\n0\n2\n4\nTime (s)\n£104\n0\n200\n400\n600\nTotal State Coverage\nQuadruped (Pixels)\n0.0\n2.5\n5.0\n7.5\nTime (s)\n£104\n0\n50\n100\n150\nTotal State Coverage\nHumanoid (Pixels)\n0\n1\n2\n3\nTime (s)\n£104\n0\n2\n4\n6\nQueue Task Coverage\nKitchen (Pixels)\n0\n1\n2\n3\nTime (s)\n£104\n0\n1\n2\n3\n4\nPolicy Task Coverage\nKitchen (Pixels)\nMETRA\nLSD\nICM\nLBS\nRND\nAPT\nAPS\nP2E/Disag\nFigure 8: Quantitative comparison with pure exploration methods (8 seeds). We compare METRA with\nsix unsupervised exploration methods in terms of state coverage. Since it is practically infeasible to completely\ncover every possible state or transition, pure exploration methods struggle to explore the state space of complex\nenvironments, such as pixel-based Humanoid or state-based Ant.\nMETRA\nLSD\nLEXA\nDIAYN\n0\n1\n2\n3\nTime (s)\n£104\n°50\n°40\n°30\n°20\n°10\nNegative Goal Distance\nHalfCheetah (States)\n0\n2\nTime (s)\n£104\n°40\n°30\n°20\n°10\nNegative Goal Distance\nAnt (States)\n0\n2\n4\nTime (s)\n£104\n°12\n°10\n°8\n°6\nNegative Goal Distance\nQuadruped (Pixels)\n0.0\n2.5\n5.0\n7.5\nTime (s)\n£104\n°8\n°7\n°6\n°5\nNegative Goal Distance\nHumanoid (Pixels)\n0\n2\n4\nTime (s)\n£104\n0\n1\n2\n3\n4\n# Achieved Tasks\nKitchen (Pixels)\nFigure 9: Downstream task performance comparison with LEXA (8 seeds). We compare METRA against\nLEXA, a state-of-the-art unsupervised goal-reaching method, on five goal-conditioned tasks. The skills learned\nby METRA can be employed to solve these tasks in a zero-shot manner, achieving the best performance.\nand DADS2. We implement these methods on the same codebase as METRA. For comparison,\nwe employ two metrics: policy coverage and downstream task performance. Figure 5 presents the\npolicy coverage results, where we evaluate the skill policy’s x coverage (HalfCheetah), x-y coverage\n(Ant, Quadruped, and Humanoid), or task (Kitchen) coverage at each evaluation epoch. The results\nshow that METRA achieves the best performance in most of the domains, and is the only method\nthat successfully learns meaningful skills in the pixel-based settings, where previous skill discovery\nmethods generally fail. In Figure 6, we evaluate the applicability of the skills discovered by each\nmethod to downstream tasks, where the downstream task is learned by a hierarchical controller\nπh(z|s) that selects (frozen) learned skills to maximize the task reward (see Appendix F for details).\nMETRA again achieves the best performance on most of these tasks, suggesting that the behaviors\nlearned by METRA not only provide greater coverage, but also are more suitable for downstream\ntasks in these domains.\nComparison with pure exploration methods. Next, we quantitatively compare METRA to five\nunsupervised exploration methods, which do not aim to learn skills but only attempt to cover the\nstate space, ICM, LBS3, RND, APT, and Plan2Explore (or Disagreement), and one hybrid method\nthat combines exploration and successor features, APS. We use the original implementations by\nLaskin et al. (2021) for state-based environments and the Dreamer versions by Rajeswar et al. (2023)\nfor pixel-based environments. As the underlying RL backbones are very different (e.g., Dreamer is\nmodel-based, while METRA uses model-free SAC), we compare the methods based on wall clock\ntime. For the metric, instead of policy coverage (as in Figure 5), we measure total state coverage\n(i.e., the number of bins covered by any training trajectories up to each evaluation epoch). This\nmetric is more generous toward the exploration methods, since such methods might not cover the\nentire space on any single iteration, but rather visit different parts of the space on different iterations\n(in contrast to our method, which aims to produce diverse skills). In Kitchen, we found that most\nmethods max out the total task coverage metric, and we instead use both the queue coverage and\npolicy coverage metrics (see Appendix F for details). Figure 8 presents the results, showing that\nMETRA achieves the best coverage in most of the environments. While pure exploration methods\nalso work decently in the pixel-based Kitchen, they fail to fully explore the state spaces of state-\nbased Ant and pixel-based Humanoid, which have complex dynamics with nearly infinite possible\ncombinations of positions, joint angles, and velocities.\nComparison with unsupervised goal-reaching methods.\nFinally, we compare METRA with\nLEXA, a state-of-the-art unsupervised goal-reaching method. LEXA trains an exploration policy\nwith Plan2Explore (Sekar et al., 2020), which maximizes epistemic uncertainty in the transition\ndynamics model, in parallel with a goal-conditioned policy π(a|s, g) on the data collected by the\nexploration policy. We compare the performances of METRA, LEXA, and two previous skill dis-\ncovery methods (DIAYN and LSD) on five goal-reaching downstream tasks. We use the procedure\ndescribed in Section 4.2 to solve goal-conditioned tasks in a zero-shot manner with METRA. Fig-\n2We do not compare against DADS in pixel-based environments due to the computational cost of its skill\ndynamics model p(s′|s, z), which requires predicting the full next image.\n3Since LBS requires a world model, we only evaluate it on pixel-based environments, where we use the\nDreamer variants of pure exploration methods (Rajeswar et al., 2023).\n9\nPublished as a conference paper at ICLR 2024\nure 9 presents the comparison results, where METRA achieves the best performance on all of the\nfive downstream tasks. While LEXA also achieves non-trivial performances in three tasks, it strug-\ngles with state-based Ant and pixel-based Humanoid, likely because it is practically challenging to\ncompletely capture the transition dynamics of these complex environments.\n6\nCONCLUSION\nIn this work, we presented METRA, a scalable unsupervised RL method based on the idea of cover-\ning a compact latent skill space that is connected to the state space by a temporal distance metric. We\nshowed that METRA learns diverse useful behaviors in various locomotion and manipulation envi-\nronments, being the first unsupervised RL method that learns locomotion behaviors in pixel-based\nQuadruped and Humanoid.\nLimitations. Despite its state-of-the-art performance in several benchmark environments, METRA,\nin its current form, has limitations. We refer to Section 4.2 for the limitations and future research\ndirections regarding the METRA objective. In terms of practical implementation, METRA, like\nother similar unsupervised skill discovery methods (Sharma et al., 2020; Park et al., 2022; 2023b),\nuses a relatively small update-to-data (UTD) ratio (i.e., the average number of gradient steps per\nenvironment step); e.g., we use 1/4 for Kitchen and 1/16 for Quadruped and Humanoid. Although\nwe demonstrate that METRA learns efficiently in terms of wall clock time, we believe there is room\nfor improvement in terms of sample efficiency. This is mainly because we use vanilla SAC (Haarnoja\net al., 2018a) as its RL backbone for simplicity, and we believe increasing the sample efficiency of\nMETRA by combining it with recent techniques in model-free RL (Kostrikov et al., 2021; Chen\net al., 2021; Hiraoka et al., 2022) or model-based RL (Hafner et al., 2020; Hansen et al., 2022) is an\ninteresting direction for future work.\nAnother limitation of this work is that, while we evaluate METRA on various locomotion and ma-\nnipulation environments, following prior work in unsupervised RL and unsuperivsed skill discov-\nery (Eysenbach et al., 2019a; Sharma et al., 2020; Mendonca et al., 2021; Laskin et al., 2021; He\net al., 2022; Park et al., 2022; Zhao et al., 2022; Shafiullah & Pinto, 2022; Laskin et al., 2022; Park\net al., 2023b; Yang et al., 2023), we have not evaluated METRA on other different types of environ-\nments, such as Atari games. Also, since we assume a fixed MDP (i.e., stationary, fully observable\ndynamics, Section 3), METRA in its current form does not particularly deal with non-stationary or\nnon-Markovian dynamics. We leave applying METRA to more diverse environments or extending\nthe idea behind METRA to non-stationary or non-Markovian environments for future work.\nFinal remarks. In unsupervised RL, many excellent prior works have explored pure exploration\nor mutual information skill learning. However, given that these methods are not necessarily readily\nscalable to complex environments with high intrinsic state dimensionality, as discussed in Section 2,\nwe may need a completely different approach to enable truly scalable unsupervised RL. We hope\nthat this work serves as a step toward broadly applicable unsupervised RL that enables large-scale\npre-training with minimal supervision.\nACKNOWLEDGMENTS\nWe would like to thank Youngwoon Lee for an informative discussion, and RAIL members and\nanonymous reviewers for their helpful comments. This work was partly supported by the Korea\nFoundation for Advanced Studies (KFAS), ARO W911NF-21-1-0097, and the Office of Naval Re-\nsearch. This research used the Savio computational cluster resource provided by the Berkeley Re-\nsearch Computing program at UC Berkeley.\nREPRODUCIBILITY STATEMENT\nWe provide our code at the following repository: https://github.com/seohongpark/\nMETRA. We provide the full experimental details in Appendix F.\n10\nPublished as a conference paper at ICLR 2024\nREFERENCES\nJoshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery\nalgorithms. ArXiv, abs/1807.10299, 2018.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Mart´ın Arjovsky, Alexander Pritzel, Andew Bolt, and Charles\nBlundell. Never give up: Learning directed exploration strategies. In International Conference\non Learning Representations (ICLR), 2020.\nRichard F Bass. Real analysis for graduate students. Createspace Ind Pub, 2013.\nKate Baumli, David Warde-Farley, Steven Stenberg Hansen, and Volodymyr Mnih. Relative varia-\ntional intrinsic control. In AAAI Conference on Artificial Intelligence (AAAI), 2021.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R´emi\nMunos. Unifying count-based exploration and intrinsic motivation. In Neural Information Pro-\ncessing Systems (NeurIPS), 2016.\nGlen Berseth, Daniel Geng, Coline Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman,\nand Sergey Levine. Smirl: Surprise minimizing reinforcement learning in unstable environments.\nIn International Conference on Learning Representations (ICLR), 2021.\nRajendra Bhatia. Matrix analysis. Springer Science & Business Media, 2013.\nG. Brockman, Vicki Cheung, Ludwig Pettersson, J. Schneider, John Schulman, Jie Tang, and\nW. Zaremba. OpenAI Gym. ArXiv, abs/1606.01540, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In Neural Informa-\ntion Processing Systems (NeurIPS), 2020.\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network\ndistillation. In International Conference on Learning Representations (ICLR), 2019.\nV´ıctor Campos Cam´u˜nez, Alex Trott, Caiming Xiong, Richard Socher, Xavier Gir´o Nieto, and Jordi\nTorres Vi˜nals. Explore, discover and learn: unsupervised discovery of state-covering skills. In\nInternational Conference on Machine Learning (ICML), 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. In International Conference on Machine Learning\n(ICML), 2020.\nXinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross.\nRandomized ensembled double q-\nlearning: Learning fast without a model. In International Conference on Learning Representa-\ntions (ICLR), 2021.\nJongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Gu. Variational empow-\nerment as representation learning for goal-conditioned reinforcement learning. In International\nConference on Machine Learning (ICML), 2021.\nJohn D. Co-Reyes, Yuxuan Liu, Abhishek Gupta, Benjamin Eysenbach, P. Abbeel, and Sergey\nLevine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec-\ntory embeddings. In International Conference on Machine Learning (ICML), 2018.\nIshan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for\nreinforcement learning. In Neural Information Processing Systems (NeurIPS), 2021.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. First return, then\nexplore. Nature, 590:580–586, 2020.\n11\nPublished as a conference paper at ICLR 2024\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\nLearning skills without a reward function. In International Conference on Learning Representa-\ntions (ICLR), 2019a.\nBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridg-\ning planning and reinforcement learning. In Neural Information Processing Systems (NeurIPS),\n2019b.\nCarlos Florensa, Yan Duan, and P. Abbeel. Stochastic neural networks for hierarchical reinforcement\nlearning. In International Conference on Learning Representations (ICLR), 2017.\nCarlos Florensa, Jonas Degrave, Nicolas Manfred Otto Heess, Jost Tobias Springenberg, and Mar-\ntin A. Riedmiller. Self-supervised learning of image embedding for continuous control. ArXiv,\nabs/1901.00943, 2019.\nJustin Fu, John D. Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep\nreinforcement learning. In Neural Information Processing Systems (NeurIPS), 2017.\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. ArXiv,\nabs/1611.07507, 2016.\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy\nlearning: Solving long-horizon tasks via imitation and reinforcement learning. In Conference on\nRobot Learning (CoRL), 2019.\nMichael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In International Conference on Artificial Intelligence and\nStatistics (AISTATS), 2010.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International Confer-\nence on Machine Learning (ICML), 2018a.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker, Sehoon Ha, Jie Tan, Vikash Ku-\nmar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms\nand applications. ArXiv, abs/1812.05905, 2018b.\nDanijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learn-\ning behaviors by latent imagination. In International Conference on Learning Representations\n(ICLR), 2020.\nDanijar Hafner, Kuang-Huei Lee, Ian S. Fischer, and P. Abbeel. Deep hierarchical planning from\npixels. In Neural Information Processing Systems (NeurIPS), 2022.\nNicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive\ncontrol. In International Conference on Machine Learning (ICML), 2022.\nS. Hansen, Will Dabney, Andr´e Barreto, T. Wiele, David Warde-Farley, and V. Mnih. Fast task\ninference with variational intrinsic successor features. In International Conference on Learning\nRepresentations (ICLR), 2020.\nKristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance\nlearning for semi-supervised and unsupervised skill discovery. In International Conference on\nLearning Representations (ICLR), 2020.\nElad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum\nentropy exploration. In International Conference on Machine Learning (ICML), 2019.\nShuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, and Xiangyang Ji. Wasserstein\nunsupervised reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), 2022.\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka.\nDropout q-functions for doubly efficient reinforcement learning. In International Conference on\nLearning Representations (ICLR), 2022.\n12\nPublished as a conference paper at ICLR 2024\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and P. Abbeel. Vime: Varia-\ntional information maximizing exploration. In Neural Information Processing Systems (NeurIPS),\n2016.\nEdward S. Hu, Richard Chang, Oleh Rybkin, and Dinesh Jayaraman. Planning goals for exploration.\nIn International Conference on Learning Representations (ICLR), 2023.\nZheyuan Jiang, Jingyue Gao, and Jianyu Chen. Unsupervised skill discovery via recurrent skill\ntraining. In Neural Information Processing Systems (NeurIPS), 2022.\nLeslie Pack Kaelbling. Learning to achieve goals. In International Joint Conference on Artificial\nIntelligence (IJCAI), 1993.\nPierre-Alexandre Kamienny, Jean Tarbouriech, Alessandro Lazaric, and Ludovic Denoyer. Direct\nthen diffuse: Incremental unsupervised skill discovery for state covering and goal reaching. In\nInternational Conference on Learning Representations (ICLR), 2022.\nJaekyeom Kim, Seohong Park, and Gunhee Kim. Unsupervised skill discovery with bottleneck\noption learning. In International Conference on Machine Learning (ICML), 2021.\nSeongun Kim, Kyowoon Lee, and Jaesik Choi. Variational curriculum reinforcement learning for\nunsupervised discovery of skills. In International Conference on Machine Learning (ICML),\n2023.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations (ICLR), 2015.\nMartin Klissarov and Marlos C. Machado. Deep laplacian-based options for temporally-extended\nexploration. In International Conference on Machine Learning (ICML), 2023.\nIlya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In International Conference on Learning Representa-\ntions (ICLR), 2021.\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel\nPinto, and P. Abbeel. Urlb: Unsupervised reinforcement learning benchmark. In Neural Informa-\ntion Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2021.\nMichael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and P. Abbeel. Un-\nsupervised reinforcement learning with contrastive intrinsic control. In Neural Information Pro-\ncessing Systems (NeurIPS), 2022.\nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E.\nHubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition.\nNeural Computation, 1:541–551, 1989.\nLisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-\ndinov. Efficient exploration via state marginal matching. ArXiv, abs/1906.05274, 2019.\nMengdi Li, Xufeng Zhao, Jae Hee Lee, Cornelius Weber, and Stefan Wermter. Internally rewarded\nreinforcement learning. In International Conference on Machine Learning (ICML), 2023.\nHao Liu and Pieter Abbeel.\nAPS: Active pretraining with successor features.\nIn International\nConference on Machine Learning (ICML), 2021a.\nHao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In Neural\nInformation Processing Systems (NeurIPS), 2021b.\nMarlos C. Machado, Marc G. Bellemare, and Michael Bowling. A laplacian framework for option\ndiscovery in reinforcement learning. In International Conference on Machine Learning (ICML),\n2017.\nMarlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray\nCampbell. Eigenoption discovery through the deep successor representation. In International\nConference on Learning Representations (ICLR), 2018.\n13\nPublished as a conference paper at ICLR 2024\nPietro Mazzaglia, Ozan C¸ atal, Tim Verbelen, and B. Dhoedt. Curiosity-driven exploration via latent\nbayesian surprise. In AAAI Conference on Artificial Intelligence (AAAI), 2022.\nPietro Mazzaglia, Tim Verbelen, B. Dhoedt, Alexandre Lacoste, and Sai Rajeswar. Choreographer:\nLearning and adapting skills in imagination. In International Conference on Learning Represen-\ntations (ICLR), 2023.\nRussell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discover-\ning and achieving goals via world models. In Neural Information Processing Systems (NeurIPS),\n2021.\nShakir Mohamed and Danilo J. Rezende. Variational information maximisation for intrinsically\nmotivated reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2015.\nMirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gra-\ndient of a non-parametric state entropy estimate. In AAAI Conference on Artificial Intelligence\n(AAAI), 2021.\nOpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter\nWelinder, Ruben D’Sa, Arthur Petron, Henrique Pond´e de Oliveira Pinto, Alex Paino, Hyeonwoo\nNoh, Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba. Asymmetric self-play for\nautomatic goal discovery in robotic manipulation. ArXiv, abs/2101.04882, 2021.\nGeorg Ostrovski, Marc G. Bellemare, A¨aron van den Oord, and R´emi Munos. Count-based explo-\nration with neural density models. In International Conference on Machine Learning (ICML),\n2017.\nSherjil Ozair, Corey Lynch, Yoshua Bengio, A¨aron van den Oord, Sergey Levine, and Pierre Ser-\nmanet.\nWasserstein dependency measure for representation learning.\nIn Neural Information\nProcessing Systems (NeurIPS), 2019.\nSeohong Park and Sergey Levine. Predictable mdp abstraction for unsupervised model-based rl. In\nInternational Conference on Machine Learning (ICML), 2023.\nSeohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim.\nLipschitz-\nconstrained unsupervised skill discovery. In International Conference on Learning Represen-\ntations (ICLR), 2022.\nSeohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine.\nHiql: Offline goal-\nconditioned rl with latent states as actions. In Neural Information Processing Systems (NeurIPS),\n2023a.\nSeohong Park, Kimin Lee, Youngwoon Lee, and P. Abbeel. Controllability-aware unsupervised skill\ndiscovery. In International Conference on Machine Learning (ICML), 2023b.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International Conference on Machine Learning (ICML), 2017.\nDeepak Pathak, Dhiraj Gandhi, and Abhinav Kumar Gupta. Self-supervised exploration via dis-\nagreement. In International Conference on Machine Learning (ICML), 2019.\nSilviu Pitis, Harris Chan, S. Zhao, Bradly C. Stadie, and Jimmy Ba. Maximum entropy gain ex-\nploration for long horizon multi-goal reinforcement learning. In International Conference on\nMachine Learning (ICML), 2020.\nVitchyr H. Pong, Murtaza Dalal, S. Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-Fit:\nState-covering self-supervised reinforcement learning. In International Conference on Machine\nLearning (ICML), 2020.\nBen Poole, Sherjil Ozair, A¨aron van den Oord, Alexander A. Alemi, and G. Tucker. On variational\nbounds of mutual information. In International Conference on Machine Learning (ICML), 2019.\nA. H. Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, and Michael C. Yip.\nComposing task-agnostic policies with deep reinforcement learning. In International Conference\non Learning Representations (ICLR), 2020.\n14\nPublished as a conference paper at ICLR 2024\nSai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich’e, B. Dhoedt, Aaron C. Courville,\nand Alexandre Lacoste.\nMastering the unsupervised reinforcement learning benchmark from\npixels. In International Conference on Machine Learning (ICML), 2023.\nNick Rhinehart, Jenny Wang, Glen Berseth, John D. Co-Reyes, Danijar Hafner, Chelsea Finn, and\nSergey Levine. Information is power: Intrinsic control via information capture. In Neural Infor-\nmation Processing Systems (NeurIPS), 2021.\nNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory\nfor navigation. In International Conference on Learning Representations (ICLR), 2018.\nTom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators.\nIn International Conference on Machine Learning (ICML), 2015.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and P. Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation.\nIn International Conference on\nLearning Representations (ICLR), 2016.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. ArXiv, abs/1707.06347, 2017.\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, P. Abbeel, Danijar Hafner, and Deepak Pathak.\nPlanning to explore via self-supervised world models. In International Conference on Machine\nLearning (ICML), 2020.\nYounggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, P. Abbeel, and Kimin Lee. State entropy\nmaximization with random encoders for efficient exploration. In International Conference on\nMachine Learning (ICML), 2021.\nNur Muhammad (Mahi) Shafiullah and Lerrel Pinto. One after another: Learning incremental skills\nfor a changing world. In International Conference on Learning Representations (ICLR), 2022.\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman.\nDynamics-\naware unsupervised discovery of skills. In International Conference on Learning Representations\n(ICLR), 2020.\nPranav Shyam, Wojciech Ja´skowski, and Faustino J. Gomez. Model-based active exploration. In\nInternational Conference on Machine Learning (ICML), 2019.\nDJ Strouse, Kate Baumli, David Warde-Farley, Vlad Mnih, and Steven Stenberg Hansen. Learning\nmore skills through optimistic exploration. In International Conference on Learning Representa-\ntions (ICLR), 2022.\nSainbayar Sukhbaatar, Ilya Kostrikov, Arthur D. Szlam, and Rob Fergus. Intrinsic motivation and\nautomatic curricula via asymmetric self-play. In International Conference on Learning Represen-\ntations (ICLR), 2018.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and P. Abbeel. #exploration: A study of count-based exploration for deep rein-\nforcement learning. In Neural Information Processing Systems (NeurIPS), 2017.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-\nden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A.\nRiedmiller. Deepmind control suite. ArXiv, abs/1801.00690, 2018.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.\nAhmed Touati and Yann Ollivier. Learning one representation to optimize all rewards. In Neural\nInformation Processing Systems (NeurIPS), 2021.\nAhmed Touati, J´er´emy Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? In\nInternational Conference on Learning Representations (ICLR), 2023.\n15\nPublished as a conference paper at ICLR 2024\nA¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. ArXiv, abs/1807.03748, 2018.\nC´edric Villani et al. Optimal transport: old and new. Springer, 2009.\nTongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching rein-\nforcement learning via quasimetric learning. In International Conference on Machine Learning\n(ICML), 2023.\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and\nVolodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In In-\nternational Conference on Learning Representations (ICLR), 2019.\nRushuai Yang, Chenjia Bai, Hongyi Guo, Siyuan Li, Bin Zhao, Zhen Wang, Peng Liu, and Xuelong\nLi. Behavior contrastive learning for unsupervised skill discovery. In International Conference\non Machine Learning (ICML), 2023.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with pro-\ntotypical representations. In International Conference on Machine Learning (ICML), 2021.\nJesse Zhang, Haonan Yu, and Wei Xu. Hierarchical reinforcement learning by discovering intrinsic\noptions. In International Conference on Learning Representations (ICLR), 2021.\nAndrew Zhao, Matthieu Lin, Yangguang Li, Y. Liu, and Gao Huang. A mixture of surprises for\nunsupervised reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2022.\n16\nPublished as a conference paper at ICLR 2024\npreserves temporal distances\n(        denotes adjacent states)\nφ\nZ = R2\nS = R64×64×3\nφ\nLearn to move in every direction in   ,\nwhich approximately covers    \nZ\nZ = R2\nS = R64×64×3\nS\n∀(s, s′) ∥φ(s) −φ(s′)∥≤1\nmaxπ(a|s,z) (φ(s′) −φ(s))⊤z\nFigure 10: Intuitive interpretation of METRA. Our main idea is to only cover a compact latent space Z\nthat is metrically connected to the state space S. Specifically, METRA learns a lossy, compact representation\nfunction ϕ : S →Z, which preserves temporal distances (left), and learns to move in every direction in Z,\nwhich leads to approximate coverage of the state space S (right).\nObservation\nGlobal view\n(a) Quadruped\nObservation\nGlobal view\n(b) Humanoid\nFigure 11: Visualization of pixel-based DMC Quadruped and Humanoid. We use gradient-colored floors\nto allow the agent to infer its location from pixel observations, similarly to Hafner et al. (2022); Park et al.\n(2023a).\nA\nEXTENDED RELATED WORK\nIn addition to unsupervised skill discovery (Mohamed & Rezende, 2015; Gregor et al., 2016; Flo-\nrensa et al., 2017; Co-Reyes et al., 2018; Achiam et al., 2018; Eysenbach et al., 2019a; Warde-Farley\net al., 2019; Shyam et al., 2019; Lee et al., 2019; Sharma et al., 2020; Campos Cam´u˜nez et al., 2020;\nHansen et al., 2020; Pong et al., 2020; Baumli et al., 2021; Choi et al., 2021; Yarats et al., 2021;\nKim et al., 2021; Zhang et al., 2021; He et al., 2022; Strouse et al., 2022; Laskin et al., 2022; Park\net al., 2022; Shafiullah & Pinto, 2022; Jiang et al., 2022; Zhao et al., 2022; Kamienny et al., 2022;\nPark & Levine, 2023; Park et al., 2023b; Li et al., 2023; Kim et al., 2023) and pure exploration (or\nunsupervised goal-conditioned RL) methods (Houthooft et al., 2016; Bellemare et al., 2016; Tang\net al., 2017; Ostrovski et al., 2017; Fu et al., 2017; Pathak et al., 2017; Hazan et al., 2019; Shyam\net al., 2019; Burda et al., 2019; Pathak et al., 2019; Lee et al., 2019; Ecoffet et al., 2020; Pitis et al.,\n2020; Badia et al., 2020; Mutti et al., 2021; Liu & Abbeel, 2021b; Mendonca et al., 2021; Yarats\net al., 2021; Seo et al., 2021; Mazzaglia et al., 2022; 2023; Hu et al., 2023; Rajeswar et al., 2023),\nthere have also been proposed other types of unsupervised RL approaches, such as ones based on\nasymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), surprise minimization (Berseth\net al., 2021; Rhinehart et al., 2021), and forward-backward representations (Touati & Ollivier, 2021;\nTouati et al., 2023). One potentially closely related line of work is graph Laplacian-based option\ndiscovery methods (Machado et al., 2017; 2018; Klissarov & Machado, 2023). These methods learn\na set of diverse behaviors based on the eigenvectors of the graph Laplacian of the MDP’s adjacency\nmatrix. Although we have not found a formal connection to these methods, we suspect there might\nexist a deep, intriguing connection between METRA and graph Laplacian-based methods, given that\n17\nPublished as a conference paper at ICLR 2024\nthey both discover behaviors based on the temporal dynamics of the MDP. METRA is also related\nto several works in goal-conditioned RL that consider temporal distances (Kaelbling, 1993; Schaul\net al., 2015; Savinov et al., 2018; Eysenbach et al., 2019b; Florensa et al., 2019; Hartikainen et al.,\n2020; Durugkar et al., 2021; Wang et al., 2023). In particular, Durugkar et al. (2021); Wang et al.\n(2023) use similar temporal distance constraints to ours for goal-conditioned RL.\nB\nTHEORETICAL RESULTS\nB.1\nUNIVERSALITY OF INNER PRODUCT DECOMPOSITION\nLemma B.1. Let X and Y be compact Hausdorff spaces (e.g., compact subsets in RN) and C(A)\nbe the set of real-valued continuous functions on A. For any function f(x, y) ∈C(X × Y) and\nϵ > 0, there exist continuous functions ϕ(x) : X →RD and ϕ(y) : Y →RD with D ≥1 such that\nsupx∈X,y∈Y |f(x, y) −ϕ(x)⊤ψ(y)| < ε.\nProof. We invoke the Stone-Weierstrass theorem (Bass (2013), Theorem 20.44), which implies\nthat the set of functions T := {PD\ni=1 ϕi(x)ψi(y) : D ∈N, ∀1 ≤i ≤D, ϕi ∈C(X), ψi ∈\nC(Y)} is dense in C(X × Y) if T is an algebra that separates points and vanishes at no\npoint.\nThe only non-trivial part is to show that T is closed under multiplication.\nConsider\ng(1)(x, y) = PD1\ni=1 ϕ(1)\ni (x)ψ(1)\ni\n(y) ∈T and g(2)(x, y) = PD2\ni=1 ϕ(2)\ni (x)ψ(2)\ni\n(y) ∈T . We have\ng(1)(x, y)g(2)(x, y) = PD1\ni=1\nPD2\nj=1 ϕ(1)\ni (x)ϕ(2)\nj (x)ψ(1)\ni\n(y)ψ(2)\nj (y), where ϕ(1)\ni (x)ϕ(2)\nj (x) ∈C(X)\nand ψ(1)\ni\n(y)ψ(2)\nj (y) ∈C(Y) for all i, j. Hence, g(1)(x, y)g(2)(x, y) ∈T .\nTheorem B.2 (ϕ(x)⊤ψ(y) is a universal approximator of f(x, y)). Let X and Y be compact Haus-\ndorff spaces and Φ ⊂C(X) and Ψ ⊂C(Y) be dense sets in C(X) and C(Y), respectively. Then,\nT := {PD\ni=1 ϕi(x)ψi(y) : D ∈N, ∀1 ≤i ≤D, ϕi ∈Φ, ψi ∈Ψ} is also dense in C(X × Y).\nIn other words, ϕ(x)⊤ψ(y) can approximate f(x, y) to arbitrary accuracy if ϕ and ψ are modeled\nwith universal approximators (e.g., neural networks) and D →∞.\nProof. By Lemma B.1, for any f ∈C(X × Y) and ε > 0, there exist D ∈N, ϕi ∈C(X), and\nψi ∈C(Y) for 1 ≤i ≤D such that supx∈X,y∈Y |f(x, y) −PD\ni=1 ϕi(x)ψi(y)| < ε/3. Define\nMy := sup1≤i≤D,y∈Y |ψi(y)|. Since Φ is dense, for each 1 ≤i ≤D, there exists ˜ϕi ∈Φ such that\nsupx∈X |ϕi(x) −˜ϕi(x)| < ε/(3DMy). Define Mx := sup1≤i≤D,x∈X |˜ϕi(x)|. Similarly, for each\n1 ≤i ≤D, there exists ˜ψi ∈Ψ such that supy∈Y |ψi(y) −˜ψi(y)| < ε/(3DMx). Now, we have\n\f\f\f\f\ff(x, y) −\nD\nX\ni=1\n˜ϕi(x) ˜ψi(y)\n\f\f\f\f\f ≤\n\f\f\f\f\ff(x, y) −\nD\nX\ni=1\nϕi(x)ψi(y)\n\f\f\f\f\f +\nD\nX\ni=1\n\f\f\f˜ϕi(x) ˜ψi(y) −ϕi(x)ψi(y)\n\f\f\f (9)\n< ε\n3 +\nD\nX\ni=1\n|˜ϕi(x)( ˜ψi(y) −ψi(y))| +\nD\nX\ni=1\n|(˜ϕi(x) −ϕi(x))ψi(y)|\n(10)\n< ε\n3 + ε\n3 + ε\n3\n(11)\n= ε,\n(12)\nfor any x ∈X and y ∈Y. Hence, T is dense in C(X × Y).\nB.2\nLIPSCHITZ CONSTRAINT UNDER THE TEMPORAL DISTANCE METRIC\nTheorem B.3. The following two conditions are equivalent:\n(a) ∥ϕ(u) −ϕ(v)∥2 ≤dtemp(u, v) for all u, v ∈S.\n(b) ∥ϕ(s) −ϕ(s′)∥2 ≤1 for all (s, s′) ∈Sadj.\n18\nPublished as a conference paper at ICLR 2024\nProof. We first show (a) implies (b). Assume (a) holds. Consider (s, s′) ∈Sadj. If s ̸= s′, by (a),\nwe have ∥ϕ(s) −ϕ(s′)∥2 ≤dtemp(s, s′) = 1. Otherwise, i.e., s = s′, ∥ϕ(s) −ϕ(s′)∥2 = 0 ≤1.\nHence, (a) implies (b).\nNext, we show (b) implies (a). Assume (b) holds. Consider u, v ∈S. If dtemp(u, v) = ∞(i.e., v is\nnot reachable from u), (a) holds trivially. Otherwise, let k be dtemp(u, v). By definition, there exists\n(s0 = u, s1, . . . , sk−1, sk = v) such that (si, si+1) ∈Sadj for all 0 ≤i ≤k −1. Due to the triangle\ninequality and (b), we have ∥ϕ(u) −ϕ(v)∥2 ≤Pk−1\ni=0 ∥ϕ(si) −ϕ(si+1)∥2 ≤k = dtemp(u, v).\nHence, (b) implies (a).\nC\nA CONNECTION BETWEEN METRA AND PCA\nIn this section, we derive a theoretical connection between METRA and principal component anal-\nysis (PCA). Recall that the METRA objective can be written as follows:\nsup\nπ,ϕ\nEp(τ,z)\n\"T −1\nX\nt=0\n(ϕ(st+1) −ϕ(st))⊤z\n#\n= Ep(τ,z)\n\u0002\nϕ(sT )⊤z\n\u0003\n(13)\ns.t. ∥ϕ(u) −ϕ(v)∥2 ≤dtemp(u, v), ∀u, v ∈S,\n(14)\nwhere dtemp denotes the temporal distance between two states. To make a formal connection be-\ntween METRA and PCA, we consider the following squared variant of the METRA objective in this\nsection.\nsup\nπ,ϕ\nEp(τ,z)\n\u0002\n(ϕ(sT )⊤z)2\u0003\ns.t. ∥ϕ(u) −ϕ(v)∥2 ≤dtemp(u, v), ∀u, v ∈S,\n(15)\nwhich is almost the same as Equation (13) but the objective is now squared. The reason we consider\nthis variant is simply for mathematical convenience.\nNext, we introduce the notion of a temporally consistent embedding.\nDefinition C.1 (Temporally consistent embedding). We call that an MDP M admits a temporally\nconsistent embedding if there exists ψ(s) : S →Rm such that\ndtemp(u, v) = ∥ψ(u) −ψ(v)∥2, ∀u, v ∈S.\n(16)\nIntuitively, this states that the temporal distance metric can be embedded into a (potentially very\nhigh-dimensional) Euclidean space. We note that ψ is different from ϕ in Equation (13), and Rm\ncan be much higher-dimensional than Z. An example of an MDP that admits a temporally consistent\nembedding is the PointMass environment: if an agent in Rn can move in any direction up to a unit\nspeed, ψ(x) = x satisfies dtemp(u, v) = ∥u −v∥2 for all u, v ∈Rn (with a slightly generalized\nnotion of temporal distances in continuous time) and thus the MDP admits the temporally consistent\nembedding of ψ. A pixel-based PointMass environment is another example of such an MDP.\nNow, we formally derive a connection between squared METRA and PCA. For simplicity, we as-\nsume Z = Rd and p(z) = N(0, Id), where N(0, Id) denotes the d-dimensional isotropic Gaussian\ndistribution. We also assume that M has a deterministic initial distribution and transition dynamics\nfunction, and every state is reachable from the initial state within T steps. We denote the set of n×n\npositive definite matrices as Sn\n++, the operator norm of a matrix A as ∥A∥op, and the m-dimensional\nunit ℓ2 ball as Bm.\nTheorem C.2 (Linear squared METRA is PCA in the temporal embedding space). Let M be an\nMDP that admits a temporally consistent embedding ψ : S →Rm. If ϕ : S →Z is a linear\nmapping from the embedding space, i.e., ϕ(s) = W ⊤ψ(s) with W ∈Rm×d, and the embedding\nspace Ψ = {ψ(s) : s ∈S} forms an ellipse, i.e., ∃A ∈Sm\n++ s.t. Ψ = {x ∈Rm : x⊤A−1x ≤1},\nthen W = [a1 a2 · · · ad] maximizes the squared METRA objective in Equation (15), where a1, ...,\nad are the top-d eigenvectors of A.\n19\nPublished as a conference paper at ICLR 2024\nProof. Since M admits a temporally consistent embedding, we have\n∥ϕ(u) −ϕ(v)∥2 ≤dtemp(u, v) ∀u, v ∈S\n(17)\n⇐⇒∥W ⊤(ψ(u) −ψ(v))∥2 ≤∥ψ(u) −ψ(v)∥2 ∀u, v ∈S\n(18)\n⇐⇒∥W ⊤(u −v)∥2 ≤∥u −v∥2 ∀u, v ∈Ψ\n(19)\n⇐⇒∥W∥op ≤1,\n(20)\nwhere we use the fact that ψ is a surjection from S to Ψ and that A is positive definite. Now, we\nhave\n=\nsup\nπ,∥W ∥op≤1\nEp(τ,z)[(ϕ(sT )⊤z)2]\n(21)\n=\nsup\nπ,∥W ∥op≤1\nEp(τ,z)[(ψ(sT )⊤Wz)2]\n(22)\n=\nsup\nf:Rd→Ψ,∥W ∥op≤1\nEp(z)[(f(z)⊤Wz)2]\n(∵Every state is reachable within T steps)\n(23)\n=\nsup\ng:Rd→Bm,∥W ∥op≤1\nEp(z)[(g(z)⊤√\nAWz)2]\n(g(z) =\n√\nA−1f(z))\n(24)\n=\nsup\n∥W ∥op≤1\nEp(z)[\nsup\ng:Rd→Bm(g(z)⊤√\nAWz)2]\n(25)\n=\nsup\n∥W ∥op≤1\nEp(z)[ sup\n∥u∥2≤1\n(u⊤√\nAWz)2]\n(26)\n=\nsup\n∥W ∥op≤1\nEp(z)[∥\n√\nAWz∥2\n2]\n(∵Dual norm)\n(27)\n=\nsup\n∥W ∥op≤1\nEp(z)[z⊤W ⊤AWz]\n(28)\n=\nsup\n∥W ∥op≤1\nEp(z)[tr(zz⊤W ⊤AW)]\n(29)\n=\nsup\n∥W ∥op≤1\ntr(Ep(z)[zz⊤]W ⊤AW)\n(30)\n=\nsup\n∥W ∥op≤1\ntr(WW ⊤A).\n(31)\nSince WW ⊤is a positive semidefinite matrix with rank at most d and ∥W∥op ≤1, there exists d\neigenvalues 0 ≤λ1, . . . , λd ≤1 and the corresponding orthonormal eigenvectors v1, . . . , vd such\nthat WW ⊤= Pd\nk=1 λkvkv⊤\nk . Hence, tr(WW ⊤A) = Pd\nk=1 λkv⊤\nk Avk, and to maximize this, we\nmust set λ1 = · · · = λd = 1 as A is positive definite. The remaining problem is to find d orthonor-\nmal vectors v1, . . . , vd that maximize Pd\nk=1 v⊤\nk Avk. By the Ky Fan’s maximum principle (Bhatia,\n2013), its solution is given as the d eigenvectors corresponding to the d largest eigenvalues of A.\nTherefore, W = [a1 a2 · · · ad], where a1, . . . , ad are the top-d principal components of A, maxi-\nmizes the squared METRA objective in Equation (15).\nTheorem C.2 states that linear squared METRA is equivalent to PCA in the temporal embedding\nspace. In practice, however, ϕ can be nonlinear, the shape of Ψ can be arbitrary, and the MDP may\nnot admit any temporally consistent embeddings. Nonetheless, this theoretical connection hints at\nthe intuition that the METRA objective encourages the agent to span the largest “temporal” mani-\nfolds in the state space, given the limited capacity of Z.\nD\nCONNECTIONS BETWEEN WDM AND DIAYN, DADS, AND CIC\nIn this section, we describe connections between our WDM objectives (either IW(S; Z) or\nIW(ST ; Z)) and previous mutual information skill learning methods, DIAYN (Eysenbach et al.,\n2019a), DADS (Sharma et al., 2020), and CIC (Laskin et al., 2022). Recall that the IW(S; Z)\n20\nPublished as a conference paper at ICLR 2024\nobjective (Equation (4)) maximizes\nT −1\nX\nt=0\n\u0000Ep(τ,z)[ϕL(st)⊤ψL(z)] −Ep(τ)[ϕL(st)]⊤Ep(z)[ψL(z)]\n\u0001\n,\n(32)\nand the IW(ST ; Z) objective (Equation (6)) maximizes\nT −1\nX\nt=0\n\u0000Ep(τ,z)[(ϕL(st+1) −ϕL(st))⊤ψL(z)] −Ep(τ)[ϕL(st+1) −ϕL(st)]⊤Ep(z)[ψL(z)]\n\u0001\n,\n(33)\nwhere we use the notations ϕL and ψL to denote that they are Lipschitz constrained. By simplifying\nEquation (32) or Equation (33) in three different ways, we will show that we can obtain “Wasser-\nstein counterparts” of DIAYN, DADS, and CIC. For simplicity, we assume p(z) = N(0, I), where\nN(0, I) denotes the standard Gaussian distribution.\nD.1\nDIAYN\nIf we set ψL(z) = z in Equation (32), we get\nrt = ϕL(st)⊤z.\n(34)\nThis is analogous to DIAYN (Eysenbach et al., 2019a), which maximizes\nI(S; Z) = −H(Z|S) + H(Z)\n(35)\n≳Ep(τ,z)\n\"T −1\nX\nt=0\nlog q(z|st)\n#\n(36)\n≃Ep(τ,z)\n\"T −1\nX\nt=0\n−∥z −ϕ(st)∥2\n2\n#\n,\n(37)\nrDIAYN\nt\n= −∥ϕ(st) −z∥2\n2,\n(38)\nwhere ‘≳’ and ‘≃’ respectively denote ‘>’ and ‘=’ up to constant scaling or shifting, and we assume\nthat the variational distribution q(z|s) is modeled as N(ϕ(s), I). By comparing Equation (34) and\nEquation (38), we can see that Equation (34) can be viewed as a Lipschitz, inner-product variant of\nDIAYN. This analogy will become clearer later.\nD.2\nDADS\nIf we set ϕL(s) = s in Equation (33), we get\nrt = (st+1 −st)⊤ψL(z) −(st+1 −st)⊤Ep(z)[ψL(z)]\n(39)\n≈(st+1 −st)⊤ψL(z) −1\nL\nL\nX\ni=1\n(st+1 −st)⊤ψL(zi),\n(40)\nwhere we use L independent samples from N(0, I), z1, z2, . . . , zL, to approximate the expectation.\nThis is analogous to DADS (Sharma et al., 2020), which maximizes:\nI(S′; Z|S) = −H(S′|S, Z) + H(S′|S)\n(41)\n≳Ep(τ,z)\n\"T −1\nX\nt=0\nlog q(st+1|st, z) −log p(st+1|st)\n#\n(42)\n≈Ep(τ,z)\n\"T −1\nX\nt=0\n \nlog q(st+1|st, z) −1\nL\nL\nX\ni=1\nlog q(st+1|st, zi)\n!#\n,\n(43)\nrDADS\nt\n= −∥(st+1 −st) −ψ(st, z)∥2\n2 + 1\nL\nL\nX\ni=1\n∥(st+1 −st) −ψ(st, z)∥2\n2,\n(44)\nwhere we assume that the variational distribution q(s′|s, z) is modeled as q(s′ −s|s, z) =\nN(ψ(s, z), I), as in the original implementation (Sharma et al., 2020). We also use the same sample-\nbased approximation as Equation (40). Note that the same analogy also holds between Equation (40)\nand Equation (44) (i.e., Equation (40) is a Lipschitz, inner-product variant of DADS).\n21\nPublished as a conference paper at ICLR 2024\nAnt\n(States)\nHalfCheetah\n(States)\nQuadruped\n(Pixels)\nHumanoid\n(Pixels)\nFigure 12: Full qualitative results of METRA (8 seeds). METRA learns diverse locomotion behaviors\nregardless of the random seed.\nD.3\nCIC\nIf we do not simplify ϕL or ψL in Equation (32), we get\nrt = ϕL(st)⊤ψL(z) −ϕL(st)⊤Ep(z)[ψL(z)]\n(45)\n≈ϕL(st)⊤ψL(z) −1\nL\nL\nX\ni=1\nϕL(st)⊤ψL(zi),\n(46)\nwhere we use the same sample-based approximation as Equation (40). By Jensen’s inequality, Equa-\ntion (46) can be lower-bounded by\nϕL(st)⊤ψL(z) −log 1\nL\nL\nX\ni=1\nexp\n\u0000ϕL(st)⊤ψL(zi)\n\u0001\n,\n(47)\nas in WPC (Ozair et al., 2019). This is analogous to CIC (Laskin et al., 2022), which estimates the\nMI via noise contrastive estimation (Gutmann & Hyv¨arinen, 2010; van den Oord et al., 2018; Poole\net al., 2019):\nI(S; Z) ≳Ep(τ,z)\n\"T −1\nX\nt=0\n \nϕ(st)⊤ψ(z) −log 1\nL\nL\nX\ni=1\nexp\n\u0000ϕ(st)⊤ψ(zi)\n\u0001\n!#\n,\n(48)\nrCIC\nt\n= ϕ(st)⊤ψ(z) −log 1\nL\nL\nX\ni=1\nexp\n\u0000ϕ(st)⊤ψ(zi)\n\u0001\n.\n(49)\nNote that Equation (47) can be viewed as a Lipschitz variant of CIC (Equation (49)).\nIn this work, we use the ψL(z) = z simplification with Equation (33) (i.e., Equation (7)), as we\nfound this variant to work well while being simple, but we believe exploring these other variants is\nan interesting future research direction. In particular, given that Equation (47) resembles the standard\ncontrastive learning formulation, combining this (more general) objective with existing contrastive\nlearning techniques may lead to another highly scalable unsupervised RL method, which we leave\nfor future work.\nE\nADDITIONAL RESULTS\nE.1\nFULL QUALITATIVE RESULTS\nFigure 12 shows the complete qualitative results of behaviors discovered by METRA on state-based\nAnt and HalfCheetah, and pixel-based Quadruped and Humanoid (8 seeds for each environment).\n22\nPublished as a conference paper at ICLR 2024\nAnt\n(States)\nHumanoid\n(Pixels)\ntrajectories\ntrajectories\nx-y\nφ(s)\nZ = R2\ntrajectories\ntrajectories\nx-y\nφ(s)\nS = R64×64×3\nZ = R2\nS = R29\nFigure 13: Latent space visualization. METRA learns to capture x-y coordinates in two-dimensional latent\nspaces in both state-based Ant and pixel-based Humanoid, as they are the most temporally spread-out dimen-\nsions in the state space. We note that, with a higher-dimensional latent space (especially when Z is discrete),\nMETRA not only learns locomotion skills but also captures more diverse behaviors, as shown in the Cheetah\nand Kitchen videos on our project page.\nAnt\n(States)\nHalfCheetah\n(States)\nZ = R1\nZ = R2\nZ = R4\nZ = {1, 2}\nZ = {1, 2, 3, 4} Z = {1, . . . , 8} Z = {1, . . . , 16} Z = {1, . . . , 24}\nFigure 14: Skills learned with different latent space sizes. Since METRA maximizes state coverage under\nthe capacity of the latent space Z, skills become more diverse as the capacity of Z grows.\nWe use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, and 16 discrete skills for\nHalfCheetah. The full qualitative results suggest that METRA discovers diverse locomotion be-\nhaviors regardless of the random seed.\nE.2\nLATENT SPACE VISUALIZATION\nMETRA simultaneously learns both the skill policy π(a|s, z) and the representation function ϕ(s),\nto find the most “temporally spread-out” manifold in the state space. We train METRA on state-\nbased Ant and pixel-based Humanoid with 2-D continuous latent spaces Z, and visualize the learned\nlatent space by plotting ϕ(s) trajectories in Figure 13. Since the x-y plane corresponds to the most\ntemporally “important” manifold in both environments, METRA learns to capture the x-y coordi-\nnates in two-dimensional ϕ, regardless of the input representations (note that Humanoid is pixel-\nbased). We also note that, with a higher-dimensional latent space (especially when Z is discrete),\nMETRA not only learns locomotion skills but also captures more diverse, non-linear behaviors, as\nshown in the Cheetah and Kitchen videos on our project page.\nE.3\nABLATION STUDY OF LATENT SPACE SIZES\nTo demonstrate how the size of the latent space Z affects skill learning, we train METRA with 1-\nD, 2-D, and 4-D continuous skills and 2, 4, 8, 16, and 24 discrete skills on Ant and HalfCheetah.\nFigure 14 compares skills learned with different latent space sizes, which suggests that the diversity\nof skill generally increases as the capacity of Z grows.\nF\nEXPERIMENTAL DETAILS\nWe implement METRA on top of the publicly available LSD codebase (Park et al., 2022). Our\nimplementation is available at https://github.com/seohongpark/METRA. For unsuper-\nvised skill discovery methods, we implement LSD (Park et al., 2022), CIC (Laskin et al., 2022), DI-\nAYN (Eysenbach et al., 2019a), and DADS (Sharma et al., 2020) on the same codebase as METRA.\nFor six exploration methods, ICM (Pathak et al., 2017), LBS (Mazzaglia et al., 2022), RND (Burda\net al., 2019), APT (Liu & Abbeel, 2021b), APS (Liu & Abbeel, 2021a), and Plan2Explore (Sekar\net al., 2020) (or Disagremeent (Pathak et al., 2019)), we use the original implementations by\nLaskin et al. (2021) for state-based environments and the Dreamer (Hafner et al., 2020) variants\n23\nPublished as a conference paper at ICLR 2024\nby Rajeswar et al. (2023) for pixel-based environments. For LEXA (Mendonca et al., 2021) in Sec-\ntion 5.3, we use the original implementation by Mendonca et al. (2021). We run our experiments on\nan internal cluster consisting of A5000 GPUs. Each run in Section 5.3 takes no more than 24 hours.\nF.1\nENVIRONMENTS\nBenchmark environments. For state-based environments, we use the same MuJoCo HalfCheetah\nand Ant environments (Todorov et al., 2012; Brockman et al., 2016) as previous work (Sharma\net al., 2020; Park et al., 2022; 2023b). HalfCheetah has an 18-dimensional state space and Ant\nhas a 29-dimensional state space. For pixel-based environments, we use pixel-based Quadruped\nand Humanoid from the DeepMind Control Suite (Tassa et al., 2018) and a pixel-based version of\nKitchen by Gupta et al. (2019); Mendonca et al. (2021). In DMC locomotion environments, we\nuse gradient-colored floors to allow the agent to infer its location from pixels, similarly to Hafner\net al. (2022); Park et al. (2023a). In Kitchen, we use the same camera setting as LEXA (Mendonca\net al., 2021). Pixel-based environments have an observation space of 64 × 64 × 3, and we do not\nuse any proprioceptive state information. The episode length is 200 for Ant and HalfCheetah, 400\nfor Quadruped and Humanoid, and 50 for Kitchen. We use an action repeat of 2 for pixel-based\nQuadruped and Humanoid, following Mendonca et al. (2021). In our experiments, we do not use\nany prior knowledge or supervision, such as the x-y prior (Eysenbach et al., 2019a; Sharma et al.,\n2020), or early termination (Park et al., 2022).\nMetrics. For the state coverage metric in locomotion environments, we count the number of 1 × 1-\nsized x-y bins (Ant, Quadruped, and Humanoid) or 1-sized x bins (HalfCheetah) that are occupied\nby any of the target trajectories. In Kitchen, we count the number of pre-defined tasks achieved by\nany of the target trajectories, where we use the same six pre-defined tasks as Mendonca et al. (2021):\nKettle (K), Microwave (M), Light Switch (LS), Hinge Cabinet (HC), Slide Cabinet (SC), and Bottom\nBurner (BB). Each of the three types of coverage metrics, policy state coverage (Figures 5 and 8),\nqueue state coverage (Figure 8), and total state coverage (Figure 8), uses different target trajectories.\nPolicy state coverage, which is mainly for skill discovery methods, is computed by 48 deterministic\ntrajectories with 48 randomly sample skills at the current epoch. Queue state coverage is computed\nby the most recent 100000 training trajectories up to the current epoch. Total state coverage is\ncomputed by the entire training trajectories up to the current epoch.\nDownstream tasks. For quantitative comparison of skill discovery methods (Figure 6), we use\nfive downstream tasks, AntMultiGoals, HalfCheetahGoal, HalfCheetahHurdle, QuadrupedGoal,\nand HumanoidGoal, mostly following the prior work (Park et al., 2022). In HalfCheetahGoal,\nQuadrupedGoal, and HumanoidGoal, the task is to reach a target goal (within a radius of 3) ran-\ndomly sampled from [−100, 100], [−7.5, 7.5]2, and [−5, 5]2, respectively. The agent receives a re-\nward of 10 when it reaches the goal. In AntMultiGoals, the task is to reach four target goals (within\na radius of 3), where each goal is randomly sampled from [sx −7.5, sx + 7.5] × [sy −7.5, sy + 7.5],\nwhere (sx, sy) is the agent’s current x-y position. The agent receives a reward of 2.5 whenever it\nreaches the goal. A new goal is sampled when the agent either reaches the previous goal or fails\nto reach it within 50 steps. In HalfCheetahHurdle (Qureshi et al., 2020), the task is to jump over\nmultiple hurdles. The agent receives a reward of 1 whenever it jumps over a hurdle. The episode\nlength is 200 for state-based environments and 400 for pixel-based environments.\nFor quantitative comparison with LEXA (Figure 9), we use five goal-conditioned tasks. In locomo-\ntion environments, goals are randomly sampled from [−100, 100] (HalfCheetah), [−50, 50]2 (Ant),\n[−15, 15]2 (Quadruped), or [−10, 10]2 (Humanoid). We provide the full state as a goal g, whose\ndimensionality is 18 for HalfCheetah, 29 for Ant, and 64 × 64 × 3 for pixel-based Quadruped and\nHumanoid. In Kitchen, we use the same six (single-task) goal images and tasks as Mendonca et al.\n(2021). We measure the distance between the goal and the final state in locomotion environments\nand the number of successful tasks in Kitchen.\nF.2\nIMPLEMENTATION DETAILS\nUnsupervised skill discovery methods. For skill discovery methods, we use 2-D continuous skills\nfor Ant and Humanoid, 4-D continuous skills for Quadruped, 16 discrete skills for HalfCheetah,\nand 24 discrete skills for Kitchen, where continuous skills are sampled from the standard Gaussian\ndistribution, and discrete skills are uniformly sampled from the set of zero-centered one-hot vec-\n24\nPublished as a conference paper at ICLR 2024\nTable 1: Hyperparameters for unsupervised skill discovery methods.\nHyperparameter\nValue\nLearning rate\n0.0001\nOptimizer\nAdam (Kingma & Ba, 2015)\n# episodes per epoch\n8\n# gradient steps per epoch\n200 (Quadruped, Humanoid),\n100 (Kitchen), 50 (Ant, HalfCheetah)\nMinibatch size\n256\nDiscount factor γ\n0.99\nReplay buffer size\n106 (Ant, HalfCheetah), 105 (Kitchen),\n3 × 105 (Quadruped, Humanoid)\nEncoder\nCNN (LeCun et al., 1989)\n# hidden layers\n2\n# hidden units per layer\n1024\nTarget network smoothing coefficient\n0.995\nEntropy coefficient\n0.01 (Kitchen),\nauto-adjust (Haarnoja et al., 2018b) (others)\nMETRA ε\n10−3\nMETRA initial λ\n30\nTable 2: Hyperparameters for PPO high-level controllers.\nHyperparameter\nValue\n# episodes per epoch\n64\n# gradient steps per episode\n10\nMinibatch size\n256\nEntropy coefficient\n0.01\nGAE λ (Schulman et al., 2016)\n0.95\nPPO clip threshold ϵ\n0.2\ntors (Park et al., 2022). METRA and LSD use normalized vectors (i.e., z/∥z∥2) for continuous\nskills, as their objectives are invariant to the magnitude of z. For CIC, we use 64-D continuous skills\nfor all environments, following the original suggestion (Laskin et al., 2022), and we found that using\n64-D skills for CIC leads to better state coverage than using 2-D or 4-D skills. We present the full\nlist of hyperparameters used for skill discovery methods in Table 1.\nUnsupervised exploration methods. For unsupervised exploration methods and LEXA, we use the\noriginal implementations and hyperparameters (Laskin et al., 2021; Mendonca et al., 2021; Rajeswar\net al., 2023). For LEXA’s goal-conditioned policy (achiever), we test both the temporal distance and\ncosine distance variants and use the former as it leads to better performance.\nHigh-level controllers for downstream tasks. In Figure 6, we evaluate learned skills on down-\nstream tasks by training a high-level controller πh(z|s, stask) that selects a skill every K = 25 (Ant\nand HalfCheetah) or K = 50 (Quadruped and Humanoid) environment steps, where stask denotes\nthe task-specific information: the goal position (‘-Goal’ or ‘-MultiGoals’ tasks) or the next hurdle\nposition and distance (HalfCheetahHurdle). At every K steps, the high-level policy selects a skill\nz, and then the pre-trained low-level skill policy π(a|s, z) executes the same z for K steps. We\ntrain high-level controllers with PPO (Schulman et al., 2017) for discrete skills and SAC (Haarnoja\net al., 2018a) for continuous skills. For SAC, we use the same hyperparameters as unsupervised\nskill discovery methods (Table 1), and we present the full list of PPO-specific hyperparameters in\nTable 2.\nZero-shot goal-conditioned RL. In Figure 9, we evaluate the zero-shot performances of METRA,\nLSD, DIAYN, and LEXA on goal-conditioned downstream tasks. METRA and LSD use the proce-\n25\nPublished as a conference paper at ICLR 2024\ndure described in Section 4.2 to select skills. We re-compute z every step for locomotion environ-\nments, but in Kitchen, we use the same z selected at the first step throughout the episode, as we find\nthat this leads to better performance. DIAYN chooses z based on the output of the skill discriminator\nat the goal state (i.e., q(z|g), where q denotes the skill discriminator of DIAYN). LEXA uses the\ngoal-conditioned policy (achiever), π(a|s, g).\n26\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2023-10-13",
  "updated": "2024-03-10"
}