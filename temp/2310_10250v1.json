{
  "id": "http://arxiv.org/abs/2310.10250v1",
  "title": "Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation",
  "authors": [
    "Simon Hakenes",
    "Tobias Glasmachers"
  ],
  "abstract": "This work addresses the challenge of navigating expansive spaces with sparse\nrewards through Reinforcement Learning (RL). Using topological maps, we elevate\nelementary actions to object-oriented macro actions, enabling a simple Deep\nQ-Network (DQN) agent to solve otherwise practically impossible environments.",
  "text": "Leveraging Topological Maps in Deep Reinforcement Learning for\nMulti-Object Navigation\nSimon Hakenes∗and Tobias Glasmachers\nInstitute for Neural Computation, Ruhr University Bochum, Germany\nAbstract\nThis work addresses the challenge of navigating\nexpansive spaces with sparse rewards through Rein-\nforcement Learning (RL). Using topological maps,\nwe elevate elementary actions to object-oriented\nmacro actions, enabling a simple Deep Q-Network\n(DQN) agent to solve otherwise practically impossi-\nble environments.\n1\nIntroduction\nNavigating large spaces is practically impossible\nfor standard RL algorithms relying on elementary\nactions. While random exploration is theoretically\npossible, it proves practically infeasible in large state\nspaces. Solutions incorporating scene understanding\nand prior knowledge about space to design macro\nactions are a promising direction. Instead of leav-\ning it up to the RL algorithm to learn these macro\nactions solely based on the information from the\nMarkov Decision Process [11], incorporating exter-\nnal world knowledge about space is a more efficient\nstrategy for designing macro actions.\nWe propose using objects to design macro ac-\ntions. Translation between macro and elementary\nactions is done with a topological map where object\npositions and their connections are stored. In con-\ntrast to metric maps, topological maps are a more\neffective planning tool and are even employed by\nmammals for spatial navigation [7].\nAll the ingredients for such a system are already\ndeveloped: For instance, Simultaneous Localization\nand Mapping (SLAM) [5] can build a map of 3D\nspace from monocular RGB(D) data while localiz-\ning the agent on the map. Storing traversed paths\n∗Corresponding Author: simon.hakenes@ini.rub.de\nFigure 1: Screenshots of the environment.\nThe\ncylinders mark the goals.\nas a graph is straightforward, and with it building\na topological map. Finding the shortest path in\na graph is solved for a long time. Object detec-\ntion and recognition algorithms grant us semantic\nunderstanding of our visual environment and RL\nalgorithms leverage a single sparse and delayed re-\nward signal to select actions. The challenge lies in\ncohesively integrating these components.\nIn this work, we integrate the above mentioned\ncomponents and connect it to a novel network ar-\nchitecture and training scheme for the DQN [3]\ninspired by [2]. An object oriented topological map\nis created and connected to the DQN to enable effi-\ncient exploration and navigation. Recognizing the\ninefficiencies of RL, we aim to streamline as much\nof the process as possible, minimizing the workload\nfor the core RL algorithm.\nExisting literature on navigation can be broadly\ncategorized into differentiable metric memory [6, 14],\nnon-differential metric maps [13] and unstructured\nand implicit memory (map less memory) [4]. Some\nauthors use topological maps [9], although their task\nis simpler and not trained by RL.\nTo the best of our knowledge, this is the first work\nabout navigation where the agent remains oblivious\nabout what the goals are (e.g., target images or\ndistance-based rewards), and their sequential order.\n1\narXiv:2310.10250v1  [cs.LG]  16 Oct 2023\nWe leverage biologically plausible topological maps\nand and rely exclusively on reinforcement learning\nfor training.\n2\nMethod\nWhenever an object is detected in an RGBD frame,\nits position is estimated from the agent’s position,\nviewing angle and its depth. A node on the map\nis created, which stores the corresponding pixels\nof the object inside the bounding box, as well as\nthe position and a flag whether or not the node is\nalready explored, greatly aiding exploration. Nodes\nare marked as explored when the agent was close to\nthe node. An edge is added if the agent moved from\none node to the other, creating a navigable graph.\nAs the map grows, so does the action space. Nec-\nessarily, we changed the DQN architecture and train-\ning procedure, inspired by [2]: The neural network,\nimplemented as a convolutional neural network,\ntakes the outer product of one action feature (in\nthe form of pixels) and a one-hot encoded state vec-\ntor indicating task progress as input. In each step,\nQ-Values—representing expected future rewards—\nare computed iteratively for all map-stored objects.\nThe agent then selects the action with the high-\nest Q-Value. Unexplored actions receive a Q-Value\nbonus to encourage exploration.\n3\nExperiments and Results\nWe use the Habitat 2.0 environment [10, 12] com-\nbined with the photo realistic Habitat-Matterport\n3D dataset [8] (Fig. 1). Based on [13], the task\nrequires to find up to 3 target objects in a scene\nsequentially. While each episode retains the object\norder, their positions vary. The target objects are\ncolor coded cylinders. In a more challenging variant\nthe objects are real world objects that blend into\nthe scene and are not recognizable just by their\ncolor.\nEach scene includes multiple rooms and hundreds\nof different objects. The only cue is the reward\nsignal, which makes it a challenging environment.\nTwo reward systems were tested: one provided a +1\nreward for each identified subgoal, while the more\nchallenging variant granted +1 only when all goals\nwere achieved.\n0\n50\n100\n200\nSteps per Episode\n1 Target\n0\n50\nEpisode\n2 Targets\n0\n50\n3 Targets\nFigure 2: Plots showing steps per episode for 1, 2,\nand 3 targets. Blue lines depict intermediate reward\nexperiments, and red lines represent runs with only\na single reward at the episode’s end. There is a\nlimit of 250 macro actions per episode.\nThe experiments are conducted in 100 different\nscenes with different goals positions for each scene.\nFor now, a working SLAM and object detection\nare assumed.\nIn the Habitat environment, they\nare not needed since corresponding ground truth\ninformation is available.\nAs shown in Fig. 2 there is a decrease in episode\nsteps length during training, which shows that the\nagent is capable to (re-)recognize the objects and\nto learn the correct order reliably.\n4\nConclusion and Future Work\nThe results clearly show the great potential of using\nmacro actions based on topological maps in RL.\nCurrent limitations include long training, reliance\non handcrafted map heuristics and the need for\neffective object detection and SLAM algorithms.\nFor future work, we aim to use photo-realistic\ntargets that blend into the scene, incorporate an\nautoencoder to assist the training of the convolu-\ntional layer as in [1], integrate an actual SLAM\nalgorithm, introduce more targets and investigate\ndifferent network architectures.\nReferences\n[1] S. Hakenes and T. Glasmachers. Boosting Rein-\nforcement Learning with Unsupervised Feature\nExtraction. In Artificial Neural Networks and\nMachine Learning – ICANN 2019: Theoreti-\ncal Neural Computation, volume 11727 LNCS,\n2\npages 555–566. Springer International Publish-\ning, 2019. doi: 10.1007/978-3-030-30487-4 43.\n[2] J. He, J. Chen, X. He, J. Gao, L. Li, L. Deng,\nand M. Ostendorf. Deep Reinforcement Learn-\ning with a Natural Language Action Space. In\nProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1621–1630, Berlin,\nGermany, Aug. 2016. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/P16-1153.\n[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A.\nRusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostro-\nvski,\nS. Petersen,\nC. Beattie,\nA. Sadik,\nI. Antonoglou, H. King, D. Kumaran, D. Wier-\nstra, S. Legg, and D. Hassabis.\nHuman-\nlevel control through deep reinforcement learn-\ning.\nNature, 518(7540):529–533, Feb. 2015.\nISSN 0028-0836, 1476-4687.\ndoi:\n10.1038/\nnature14236.\n[4] V. Mnih, A. P. Badia, M. Mirza, A. Graves,\nT.\nLillicrap,\nT.\nHarley,\nD.\nSilver,\nand\nK. Kavukcuoglu. Asynchronous Methods for\nDeep Reinforcement Learning. In M. F. Balcan\nand K. Q. Weinberger, editors, Proceedings of\nThe 33rd International Conference on Machine\nLearning, volume 48 of Proceedings of Machine\nLearning Research, pages 1928–1937, New York,\nNew York, USA, 2016. PMLR.\n[5] R. Mur-Artal, J. M. M. Montiel, and J. D.\nTard´os. ORB-SLAM: A Versatile and Accurate\nMonocular SLAM System. IEEE Trans. Rob.,\n31(5):1147–1163, Oct. 2015. ISSN 1941-0468.\ndoi: 10.1109/TRO.2015.2463671.\n[6] E. Parisotto and R. Salakhutdinov.\nNeural\nMap: Structured Memory for Deep Reinforce-\nment Learning. In International Conference on\nLearning Representations, 2018.\n[7] E.\nParra-Barrero,\nS.\nVijayabaskaran,\nE. Seabrook,\nL. Wiskott,\nand S. Cheng.\nA Map of Spatial Navigation for Neuroscience.\nNeurosci. Biobehav. Rev., page 105200, May\n2023.\nISSN 0149-7634, 1873-7528.\ndoi:\n10.1016/j.neubiorev.2023.105200.\n[8] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans,\nO. Maksymets, A. Clegg, J. Turner, E. Under-\nsander, W. Galuba, A. Westbury, A. X. Chang,\nM. Savva, Y. Zhao, and D. Batra. Habitat-\nMatterport 3D Dataset (HM3D): 1000 Large-\nscale 3D Environments for Embodied AI. Sept.\n2021. doi: 10.48550/arXiv.2109.08238.\n[9] N. Savinov, A. Dosovitskiy, and V. Koltun.\nSemi-parametric Topological Memory for Navi-\ngation. 6th International Conference on Learn-\ning Representations, ICLR 2018 - Conference\nTrack Proceedings, Mar. 2018.\n[10] M. Savva, J. Malik, D. Parikh, D. Batra, A. Ka-\ndian, O. Maksymets, Y. Zhao, E. Wijmans,\nB. Jain, J. Straub, J. Liu, and V. Koltun. Habi-\ntat: A platform for embodied AI research. In\n2019 IEEE/CVF International Conference on\nComputer Vision (ICCV). IEEE, Oct. 2019.\nISBN 9781728148038. doi: 10.1109/iccv.2019.\n00943.\n[11] R. S. Sutton, D. Precup, and S. Singh. Be-\ntween MDPs and semi-MDPs: A framework\nfor temporal abstraction in reinforcement learn-\ning. Artif. Intell., 112(1):181–211, Aug. 1999.\nISSN 0004-3702. doi: 10.1016/S0004-3702(99)\n00052-1.\n[12] A. Szot, A. Clegg, E. Undersander, E. Wijmans,\nY. Zhao, J. Turner, N. Maestre, M. Mukadam,\nD. Chaplot, O. Maksymets, A. Gokaslan,\nV. Vondrus, S. Dharur, F. Meier, W. Galuba,\nA. Chang, Z. Kira, V. Koltun, J. Malik,\nM. Savva, and D. Batra. Habitat 2.0: Training\nHome Assistants to Rearrange their Habitat.\nIn Advances in Neural Information Processing\nSystems (NeurIPS), 2021.\n[13] S. Wani, S. Patel, U. Jain, A. Chang, and\nM. Savva. MultiON: Benchmarking Seman-\ntic Map Memory using Multi-Object Naviga-\ntion. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, vol-\nume 33, pages 9700–9712. Curran Associates,\nInc., 2020.\n[14] J. Zhang, L. Tai, M. Liu, J. Boedecker, and\nW. Burgard. Neural SLAM: Learning to Ex-\nplore with External Memory. June 2017.\n3\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-10-16",
  "updated": "2023-10-16"
}