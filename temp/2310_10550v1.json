{
  "id": "http://arxiv.org/abs/2310.10550v1",
  "title": "Deep learning applied to EEG data with different montages using spatial attention",
  "authors": [
    "Dung Truong",
    "Muhammad Abdullah Khalid",
    "Arnaud Delorme"
  ],
  "abstract": "The ability of Deep Learning to process and extract relevant information in\ncomplex brain dynamics from raw EEG data has been demonstrated in various\nrecent works. Deep learning models, however, have also been shown to perform\nbest on large corpora of data. When processing EEG, a natural approach is to\ncombine EEG datasets from different experiments to train large deep-learning\nmodels. However, most EEG experiments use custom channel montages, requiring\nthe data to be transformed into a common space. Previous methods have used the\nraw EEG signal to extract features of interest and focused on using a common\nfeature space across EEG datasets. While this is a sensible approach, it\nunderexploits the potential richness of EEG raw data. Here, we explore using\nspatial attention applied to EEG electrode coordinates to perform channel\nharmonization of raw EEG data, allowing us to train deep learning on EEG data\nusing different montages. We test this model on a gender classification task.\nWe first show that spatial attention increases model performance. Then, we show\nthat a deep learning model trained on data using different channel montages\nperforms significantly better than deep learning models trained on fixed 23-\nand 128-channel data montages.",
  "text": " \n \n \nDung Truong* \nSCCN, INC, UCSD, La Jolla CA, USA \ndutruong@ucsd.edu \nhttps://orcid.org/0000-0003-4540-3551 \n \n \n \n \nMuhammad Abdullah Khalid* \nSCCN, INC, UCSD, La Jolla CA, USA \nmkhalid.bee18seecs@seecs.edu.pk \n \n \n \n \n \nArnaud Delorme \nSCCN, INC, UCSD, La Jolla CA, USA \nCerCo CNRS, Paul Sabatier University, \nToulouse, France \narnodelorme@gmail.com \nhttps://orcid.org/0000-0002-0799-3557 \n \nAbstract— The ability of Deep Learning to process and extract \nrelevant information in complex brain dynamics from raw EEG \ndata has been demonstrated in various recent works. Deep \nlearning models, however, have also been shown to perform best \non large corpora of data. When processing EEG, a natural \napproach is to combine EEG datasets from different \nexperiments to train large deep-learning models. However, most \nEEG experiments use custom channel montages, requiring the \ndata to be transformed into a common space. Previous methods \nhave used the raw EEG signal to extract features of interest and \nfocused on using a common feature space across EEG datasets. \nWhile this is a sensible approach, it underexploits the potential \nrichness of EEG raw data. Here, we explore using spatial \nattention applied to EEG electrode coordinates to perform \nchannel harmonization of raw EEG data, allowing us to train \ndeep learning on EEG data using different montages. We test \nthis model on a gender classification task. We first show that \nspatial attention increases model performance. Then, we show \nthat a deep learning model trained on data using different \nchannel montages performs significantly better than deep \nlearning models trained on fixed 23- and 128-channel data \nmontages. \n \nI. \nINTRODUCTION \nDeep learning is commonly used for the end-to-end \nclassification of electrophysiology (EEG) data or as a feature \ntransformer for EEG data [1, 4]. However, training deep \nlearning models requires a large amount of data, and this is \noften a limitation with most EEG datasets where only a \nhandful of subjects are available. A solution to the scarcity of \nlarge EEG datasets is to combine datasets from different \nsources. However, a challenge to this approach is the \nharmonization of data with different channel montages. \nDifferent EEG caps are being used in different EEG \nexperiments, \nso \nthe \nnumber \nand \nlocations \nof \nchannels/electrodes are rarely the same. \nA straightforward solution would be to subsample the \nchannel space, only selecting common channels across \nexperiments when it is even possible. This approach, \nhowever, potentially underutilized available data. Previous \nworks have been solving this problem by transforming raw \nEEG signals into a common feature space, such as the \ntopography distribution of spectral power [4] or the \nspectrogram of single-channel EEG [16], before using deep \nneural networks. These solutions are nevertheless limiting \nsince only spectral information of specific frequency bands is \nbeing considered.  \nRecent works are now showing that deep models trained \non raw EEG data might learn statistical properties of the data \nwell beyond the commonly used EEG frequency bands [14] \nand that deep learning models trained on raw EEG outperform \nmodels trained on spectral features [12]. Spectral approaches \nlimit the data-driven capacity of deep learning models, which \ncould potentially extract more information from the raw EEG \ndata. Thus, researchers should explore channel harmonization \nmethods for the training of deep learning models on raw EEG \ndata from datasets with different montages. \nIn recent works, a mechanism called spatial attention was \nimplemented to train deep models to correlate EEG/MEG \nwith speech data [10]. In this method, deep learning (DL) \nmodels take into account the relative position of EEG \nelectrodes on the scalp – information that is not available \nwhen DL models use as input 2-D channel x time EEG \nsegment time series. This procedure consists of mapping \nchannel locations from the coordinate space into a 2-D \nFourier space where spatial dimensions are defined by \nchannel 2-D locations proximity, then applying the attention \nmechanism to the frequency-transformed channels to map the \ninput channels to a fixed number of output channels. While \nDéfossez and collaborators [10] applied this mechanism to \nleverage \ninformation \npertaining \nto \nchannel \nspatial \ndistribution, this could also be used to map different channel \nmontages into a common output channel space.  \nIn this report, we explore the application of the \naforementioned spatial attention mechanism to the problem of \ntraining a single deep-learning model on data with different \ninput channel montages. We used a large EEG resting state \ndataset from more than a thousand juvenile (5-22 years) \nparticipants, collected and made publicly available by the \nChild Mind Institute Healthy Brain Network project. We \nperformed gender classification using a simple convolution \ndeep neural model [12], to which we added spatial attention. \nTo instigate the usefulness of using the spatial attention \nmechanism for channel harmonization, we performed data \nsubsampling to obtain a subset of the data with two different \nDeep learning applied to EEG data with different montages using \nspatial attention \n________________________ \n* authors contributed equally \n \n \nchannel montages, one with 128 channels and one with 23 \nchannels.  \nII. \nMETHODS \nEEG recordings.  High-density EEG data were recorded in a \nsound-shielded room at a sampling rate of 500 Hz with a \nbandpass of 0.1 to 100 Hz, using a 128-channel EEG geodesic \nhydrogel system by Electrical Geodesics Inc. (EGI) [13]. The \ndata \nare \npublicly \navailable \nfor \ndownload \nat \nhttp://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_n\network. We only considered the resting data files. These were \n6 minutes in length and were composed of successive 20-s to \n40-s periods of eyes open, and eyes closed rest, respectively.  \nRaw data preprocessing. Although deep learning may be \napplied to raw EEG data without any preprocessing [1], we \nminimally preprocessed the data [2] using the EEGLAB \nv2023 software package [5] running on MATLAB 2022b. We \nused only eye-closed data segments (~170s per subject), \nignoring the first and last 3 seconds of each eye-closed period \n(resulting in five periods of 34 seconds). We removed the \nmean for each data epoch from each channel, down-sampled \nthe data to 128 Hz, and subsequently band-pass filtered the \ndata between 0.25–25 Hz (FIR filter of order 6601; 0.125 Hz \nand 25.125 Hz cutoff frequencies (-6 dB); zero phase; non \ncausal). Data were re-referenced to the averaged mastoids and \ncleaned using Artifact Subspace Reconstruction EEGLAB \nplug-in clean_rawdata (v2.3) [6], an automated method that \nremoves artifact-dominated channels (parameters used were \n5 for FlatLineCriterion, 0.7 for ChannelCriterion, and 4 for \nLineNoiseCriterion). \nRemoved \nchannels \nwere \nthen \ninterpolated using 3-D spline interpolation (EEGLAB \ninterp.m function). No bad portions of data were removed. \nFinally, we segmented eye-closed data periods into non-\noverlapping 2-s windows: each preprocessed 2-s epoch was \nused as a sample for our final dataset. Each subject provided \nabout 81 2-s samples (mean 80.8 ± 3.32). The 128-Hz down-\nsampling procedure and 2-s window length extraction were \nidentical to those used in Van Putten et al. [2]. No bad epochs \nwere removed, and no further preprocessing was performed. \nDeep learning model. VGG-16 was originally designed to \ncategorize 15 million images of dimension 256x256 with 3 \ncolor channels [15]. In a previous publication, we \nsuccessfully applied VGG-16 to EEG spectral and raw data \nclassification after removing some of the layers to account for \nthe reduced dimensionality of EEG data [4, 12]. We decreased \nthe model complexity by reducing the number of \nconvolutional layers, omitting layers 19-32 of VGG-16. We \nalso divided the number of filters and hidden units in the \nconvolutional and FC layers by 4 to reflect our lower number \nof training samples. To allow the model to take raw EEG data \nas input, we decreased the number of input channels in the \nfirst convolutional layer from 3 to 1. The rest of the network \nremained unchanged. In total, this model, called R-VGG, \ncontained 7,452,850 trainable parameters. We chose this \nmodel to benchmark attention since it is simple and leads to \ngood performance on the task of gender classification of EEG \ndata [12]. \n \n \nLayer \nFilter size \n# of filters/hidden units \nConvolutional \n3x3 \n16 \nConvolutional \n3x3 \n16 \nMaxPooling \nConvolutional \n3x3 \n32 \nConvolutional \n3x3 \n32 \nMaxPooling \nConvolutional \n3x3 \n64 \nConvolutional \n3x3 \n64 \nConvolutional \n3x3 \n64 \nMaxPooling \nFully connected \n  \n1024 \nDropout (50%) \nFully connected \n  \n1024 \nDropout (50%) \nFully connected \n  \n2 \nSoftmax \nTable 1. R-VGG configurations. The ReLU activation function is \nnot shown for brevity. All convolutional layers have stride 1 and \npadding 1. All pooling layers have window size 2x2, stride 2, and no \npadding. \nAttention mechanism. Attention is a sequence-to-sequence \nmechanism in which each element of the output sequence is a \nweighted sum of the elements of the input sequence. The goal \nof the attention mechanism can be interpreted as finding the \nrelevance of the input elements to the output element. The \ngeneral attention mechanism works by having the query, key, \nand value matrices. The computation steps start with \ncombining the query and key, which can be implemented as a \ndot product or a linear transform:  \n𝑎(𝑄, 𝐾) =  𝑄. 𝐾𝑇                                   (1) \nWe then score the linear transformation of the input using a \nscoring function, most often softmax: \n𝛼(𝑄, 𝐾) =  𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑎(𝑄, 𝐾))                   (2) \nThen finally, the attention output will be the weighted sum of \nthe elements of value matrix V given the attention score \n 𝛼(𝑄, 𝐾): \n𝑎𝑡𝑡𝑛(𝑄, 𝐾, 𝑉)  =  𝛼(𝑄, 𝐾) 𝑉                      (3) \nThis algorithm mostly finds applications in Natural Language \nProcessing. However, its use has been expanded to Computer \nVision and EEG Signal Processing. Su and collaborators [9] \nproposed a spatio-temporal attention network (STANet) in \nwhich they calculate spatial and temporal EEG feature \nrepresentation. Zhang et al. [8] use a \"Learnable Spatial \nMapping Module\" for projecting and averaging the EEG \nchannels to a new domain. \nSpatial attention. Défossez and collaborators [10] applied \nthe \nattention \nmechanism \nto \nmulti-channel \nelectrophysiological data by first parameterizing the electrode \n \n \ncoordinates by a 2D frequency space. The 3D channel \npositions are converted to a 2D plane using the MNE-Python \nfunction find_layout. These 2D positions are then normalized \nto a range of 0 to 1 (Fig. 1).  \n \nFigure 1. Encoding of electrode location in normalized  2-D space. \nThe (x,y) coordinate of the channel mark in black is shown. Only a \nsubset of the 128 channels are shown for clarity. \nFor an output channel j, the equation below computes the \nscore for each input channel location (x, y) where 𝑅𝑒 and 𝐼𝑚 \nare the real and the imaginary part of the elements of zj \nrespectively: \n \n𝑎𝑗(𝑥, 𝑦) = ∑∑𝑅𝑒(𝑧𝑗\n(𝑘,𝑙))𝑐𝑜𝑠(2𝜋(𝑘𝑥+ 𝑙𝑦)) +\n𝐾\n𝑙 = 1\n𝐾\n𝑘 = 1\n \n        𝐼𝑚(𝑧𝑗\n(𝑘,𝑙))𝑠𝑖𝑛(2𝜋(𝑘𝑥+ 𝑙𝑦))       (4) \nOne interpretation of this equation is that it is letting the deep \nlearning model learn an operation similar to the inverse 2D \nFourier transformation parameterized by the complex zj \nmatrix [11]. The function value aj(x, y) is computed by \naggregating the powers of KxK (K = 32) frequencies, with \neach frequency’s power being pondered by the complex zj \nmatrix, which is defined for each output channel – and \nidentical for all input channel locations. Thus, input channels \nclose to each other are weighted in a similar manner by the zj \nmatrix (especially for low values of k and l at low spatial \nfrequencies). The attention scores are then obtained by \napplying the softmax function on  aj(x, y) of all possible input \nchannel locations. The value of each output channel j can then \nbe computed by \n∀𝑗∈[𝐷1], 𝑆𝐴(𝑋)(𝑗) =\n1\n∑\n𝑒𝑎𝑗(𝑥𝑖, 𝑦𝑖)\n𝐶\n𝑖= 1\n(∑\n𝑒𝑎𝑗(𝑥𝑖, 𝑦𝑖) 𝑋(𝑖)\n𝐶\n𝑖 = 1\n) (5) \na weighted sum over all input channel locations. C is the \nnumber of channels, and X(i) is the raw time series data for \nchannel i. Note that since zj is learned with the classification \ntask, the attention scores may only be relevant to the task at \nhand. In practical applications of spatial attention (SA), we \nnormalize the coordinates (x, y) by reducing the values to \ninclude a 0.1 margin on each side, as the variable aj is \nperiodic. Additionally, for each output channel, we introduce \nspatial dropout by randomly selecting an input channel \nlocation  xdrop, ydrop and eliminating any sensor within a \ndistance of 0.1 from that location in the softmax. This means \nthat for each output channel, a random region of the 2-D \nchannel space will be ignored. Following previous work [10], \nwe applied a 1x1 convolution (filter size of 1) with no \nactivation on the output of the spatial attention, using the same \nnumber of filters as the number of output channels. \nChannel harmonization using spatial attention. Défossez \nand collaborators [10] used the spatial attention mechanism to \ntake into account channel locations and potentially increase \nperformance for a DL model trained on M/EEG data. We \napplied the spatial attention mechanism to perform channel \nharmonization by first noticing that the Fourier transform \nmaps channel coordinates into a 2D frequency space \nsupported by KxK frequencies. Thus, the matrix zj learns this \nmapping in a way that can handle all possible channel \nlocations. This mapping is thus independent of the number of \ninput channels and the location of these channels. The number \nof input channels then only affects the number of terms used \nby the softmax in equation (5). Thus, for each input EEG \nsample, we accordingly compute the aj values for each of its \ninput channels given its location in equation (4), then the \ncorresponding output channel values can be computed \naccordingly using equation (5).  \nIII. \nEXPERIMENTS \nTraining, validation, and test sets. While the dataset we \nused included 2,224 participants, there were only 787 females \n(35%). We decided to use 1574 participants (50% female) to \nensure class balance by selecting the first 787 males from a \nlist of participants ordered by their IDs.We checked the age \ndistribution of the two subjects in the two classes matched \n[14]. Following [2], we then split the balanced data into \ntraining, validation, and test sets in a size ratio of 60:30:10. \nEach segment received a binary label, indicating a male (0) or \na female (1). This led to 71,300 samples (885 participants; \n49.94% female) for training, 39,868 samples (492 subjects; \n50% female) for validation, and 16,006 samples (197 \nsubjects; 50.3% female) for testing. There was no leakage of \nparticipants from the training set into the testing set.   \nExperimental setup. All models were trained on a single \nNVIDIA V100 SMX2 GPU (32 GB) with Python 3.7.10 and \nPyTorch 1.3.1 on the Expanse supercomputer. Generating the \nresults in Table 2 and Table 3 required about 100 hours of \nGPU time. We trained all four models using an Adamax \noptimizer with default hyperparameters (learning rate = \n0.002, β1 = 0.9, β2 = 0.999, ϵ = 1e-08) except for decay set to \n0.001. The batch size was set at 70 following [2]; training was \nperformed \nfor \n15 \nepochs \n(see \nbelow). No \nother \nhyperparameter tuning nor batch normalization was \nperformed. \nData sampling and channel sub-sampling. To generate a \ndataset with a different channel count, we  sub-selected 23 \nchannels from our original 128-channel data using the same \nchannels as with previous works [2, 12]. Each sample in our \n \n \ndataset thus had dimensions of either 128x256 or 23x256 \n(number of channels and 2(s) x 128(Hz) time points). To \ngenerate the mix-channel data, we first performed data sub-\nsampling for each training, validation, and test set by splitting \neach set in half, ensuring balance between the two classes for \neach set and no overlapping of subjects across sets. For one \nhalf of each dataset, we performed channel sub-selection to \nretrieve a set of 23-channel data. The combination of 128-\nchannel and 23-channel data for each training, validation, and \ntest set constitutes the mixed-channel channel data for each of \nthose sets. There was no leakage of participants between the \n23-channel samples and the 128-channel samples. \nValidating spatial attention on individual datasets. We set \nthe baseline for our experiments by first training and \nevaluating the deep learning models without spatial attention \non both 128-channel and 23-channel data individually (see \nTable 2). We then applied spatial attention and retrained the \nindividual models to ensure that adding spatial attention \nwould still allow the model to learn the gender classification \ntask while not reducing the models’ learning capacity on the \ndata.  \nEvaluating spatial attention performance. The spatial \nattention mechanism allowed data trained on one set of \nchannels (e.g., 128 channels) to be evaluated on data of the \nother set of channels (e.g., 23 channels) and hence also a \ncombination of them (mixed dataset of both 128 and 23 \nchannels). We also trained models on mixed-channel data \n(dataset with both 128-channel and 23-channel samples) and \nevaluated them on 128-channel, 23-channel, and mixed-\nchannel data. \nStopping and evaluation criteria. Overfitting is a common \nissue in deep learning. One common DL practice to avoid \noverfitting is early stopping, in which training is stopped (and \nmodel performance evaluated) when validation accuracy \n(here, per-sample classification accuracy) starts to plateau or \ndecrease as training accuracy continues to grow [7]. While \ntraining 128-channel data and 23-channel data models, we \nobserved that the models converged and started to overfit the \ntraining data after about 15 epochs. Hence, for computational \nuniformity, we stopped the training of all subsequent models \nwith all experiments after 15 epochs. \nStatistics. To assess the robustness of our classification \nresults, we trained each model 10 times using different \nrandom seeds, giving 10 different weight initializations, \nallowing us to calculate statistics using either unpaired 2-way \nANOVA with variables model and attention (for Tables 2 and \n3) or unpaired parametric t-test assuming unequal variances \n(for Table 3). \nIV. \nRESULTS \n128-channel model vs. 23-channel model with or without \nattention. Table 2 shows that increasing the number of \nchannels increases performance (average of 80.4% for the \n128-channel model vs 78.0% for the 23-channel model; \nF=40.7; \np<3.10-7). \nAdding \nattention \nalso \nincreases \nperformance (average of 83.7% with attention vs 80.4% \nwithout attention; F=37.5; p<5.10-7). We did not notice an \ninteraction between the two factors (F=1.2; ns). \n \n Model→ \n \nAttention↓ \n128-channel \nmodel \n23-channel \nmodel \nNo spatial attention \n80.4 (0.8) \n78.0 (1.8) \nSpatial attention \n83.7 (1.5)✝ \n80.3 (1.4)✝ \nTable 2. Mean classification accuracy (and standard deviation in \nparenthesis) for the 23-channel and 128-channel models with or \nwithout using spatial attention. ✝ values are repeated in Table 3. \n \nMixed-channel model overall performance. Table 3 shows \nthat the mided-channel model performed overall the best \nwhen considering all the different test data sets (mixed-\nchannel model average performance of 80.3% vs. 70.3 for the \n128-channel model (F=142.6; p<10-12) and vs. 78.6% for the \n23-channel model (F=2.5; p=0.09)). The difference with the \n23-channel model is a trend at p= 0.09 but would likely \nbecome significant with more repetitions given the significant \ndifference observed when the two models are tested on 128-\nchannel data – mean performance of 76.9% for the 23-channel \nmodel vs 81.4% for the mixed-channel model (t=6;  \np=0.0001). \n \n Model→ \nTest data↓ \n128 \nchannel  \n23 \nchannel  \nMixed \nchannel  \n128 channel \n83.7 (1.5) \n76.9 (1.2) \n81.4 (2.2) \n23 channel \n57.2 (3.0) \n80.3 (1.4) \n79.8 (2.5) \nMixed \nchannel \n70.1 (4.2) \n78.6 (2.0) \n78.9 (1.6) \nTable 3. Classification accuracy (standard deviation in parenthesis) \nof models trained on different channel counts experiments, all while \napplying spatial attention. \n \nMixed-channel model performance on homogenous test \ndata. We observed that models trained and tested on \nhomogeneous 128-channel data give the best performance out \nof all models (83.7% vs. 81.4% on the same test data for the \nmixed model; t=2.7; p=0.02). However, the 128-channel and \nthe mixed-channel models are the same models trained with \ndifferent data – so this difference in performance does not \nreflect an issue with the architecture of the mixed-channel \nmodel. The model trained and tested on homogeneous 23-\nchannel data did not differ significantly from the mix-channel \nmodel, though (80.3% vs 79.8% on the same 23-channel test \ndata; t=0.56; ns). \n \n \nModels’ performances on mixed-channel test data (bottom \nrow of Table 3). Considering all models’ performances on the \nmixed-channel test data, the models trained on 128-channel \ndata performed significantly worse than the 23-channel and \nmixed model (t>5.8; p<0.0001 in both cases).  The 23-channel \nand mixed models performed on par (78.6% vs 78.9% \nperformance, respectively; t=0.36; ns). This could be an \nindication of data regularization by the 23-channel model, as \nthe 23-channel data was subsampled from the 128-channel \noriginal dataset.  \n \nMixed-channel model vs 128-channel model performance \non 23-channel test data. The 128-channel model performed \npoorly on the 23-channel test data, with a mean performance \nof 57.2% vs 79.8% for the mixed-channel model (Table 3) ( \nt=18.5; p<10-12). We interpret this poor result in the \ndiscussion. \n \nMixed-channel model vs 23-channel model performance \non 128-channel test data. The 23-channel model performed \nworse than the mixed-channel model when evaluated on 128-\nchannel test data, with a mean performance of 76.9% vs \n81.4% (Table 3) (t=6;  p=0.0001). However, we noticed that \nsuch performance is significantly better than the model \ntrained on 128-channel data and evaluated on 23-channel data \n(76.9% vs. 57.2%, Table 3; t=19.3; p<10-9). Since our 128-\nchannel and 23-channel data originated from the same dataset \noriginally, this  indicates that the subsampling of 23-channel \ndata acted as a form of regularization on the model.  \n \nV. \nDISCUSSION \nWe have shown both spatial attention and the number of \ndata channels improve the classification performance of DL \nmodels applied to EEG data. \nWe have also shown a mix-channel deep learning model \nusing spatial attention outperformed models trained on a fixed \n23 or 128-channel montage. This type of result is important \nbecause mixed-channel models may be trained with data from \nexperiments using different channel montages.   \nThus, the spatial attention mechanism can flexibly be used \nas a method  to combine data with different channel counts \nfor training deep learning models while taking into account \nthe spatial information of the channels. Models trained on \nheterogeneous \ndata \ncan \nbe \neffectively \napplied \nto \nhomogeneous data of different channel counts without \nsignificant (if any) performance loss. Moreover, when \napplying such models on collections of data using different \nchannel counts, models trained on mixed-channel data could \npotentially be highly performant. Channel harmonization \nusing spatial attention thus gives a promising path forward for \naggregating EEG data across datasets for the effective large-\nscale training of deep neural networks.  \nHomogeneity of input channel locations. For this work, \nwhile the number of input channels varies across training and \ntesting sets, all data uses the same montage. This makes \ncomparison easier, but it also has its drawbacks. The data \nregularization effect we observed when evaluating models \ntrained on the subsampled 23-channel data on 128-channel \nand mixed-channel data might only hold for this experiment. \nWe expect that in the situation where our data comes from \nheterogeneous sources with different channel montages, \nmodels trained on datasets of specific recording setup will \nonly perform well when applied to data of the same recording \nparameters, further emphasizing the need for training deep \nmodels on non-uniform data samples. Thus, future work can  \napply the spatial attention mechanism to harmonize data from \ndifferent datasets to test the efficacy of mixed-channel \nmodels. \nGeneralization power of the 23 vs 128-channel model. The \n128-channel model failed to generalize to testing on 23-\nchannel data. We think this could also be because the \nminimum distance between channels in the 23-channel data is \nlarge. This means that high frequencies (k or l equals 32) lead \nto more than 1 cycle with no smooth transition between \nneighbor channels. This pseudo-random information, \nmultiplied by weights learned by the 128-channel model \nwhere higher spatial information was available, could lead to \npoor performance. However, this hypothesis would need to be \ntested with different channel montages. \nIn conclusion, spatial attention is useful to increase deep \nlearning \nmodel \nperformance \nand \nperform \nchannel \nharmonization across datasets using different montages. \nCODE AND DATA AVAILABILITY \nAll data processing  and model training code is made \npublicly accessible via Github: https://github.com/sccn/deep-\nchannel-harmonization. We also made the raw data publicly \navailable on Amazon Web Storage with full access \ninstructions outlined in a previous publication [17]. \nACKNOWLEDGMENTS \nExpanse supercomputer time was provided via XSEDE \nallocations and NSG (the Neuroscience Gateway). We thank \nAmitava Majumdar, Subhashini Sivagnanam, and Kenneth \nYoshimoto for providing computational resources. \nREFERENCES \n[1] Roy, Y., Banville, H., Albuquerque, I., Gramfort, A., \nFalk, T. H., & Faubert, J. (2019). Deep learning-based \nelectroencephalography analysis: a systematic review. \nJournal of neural engineering, 16(5), 051001. \n[2] Van Putten, M. J., Olbrich, S., & Arns, M. (2018). \nPredicting sex from brain rhythms with deep learning. \nScientific reports, 8(1), 3069. \n[3] Alexander, L. M., Escalera, J., Ai, L., Andreotti, C., \nFebre, K., Mangone, A., ... & Milham, M. P. (2017). An \nopen resource for transdiagnostic research in pediatric \nmental health and learning disorders. Scientific data, \n4(1), 1-26. \n \n \n[4] Jung, T. P., & Sejnowski, T. J. (2019). Utilizing deep \nlearning towards multi-modal bio-sensing and vision-\nbased affective computing. IEEE Transactions on \nAffective Computing, 13(1), 96-107. \n[5] Delorme, A., & Makeig, S. (2004). EEGLAB: an open \nsource toolbox for analysis of single-trial EEG dynamics \nincluding independent component analysis. Journal of \nneuroscience methods, 134(1), 9-21. \n[6] Mullen, T. R., Kothe, C. A., Chi, Y. M., Ojeda, A., Kerth, \nT., Makeig, S., ... & Cauwenberghs, G. (2015). Real-time \nneuroimaging and cognitive monitoring using wearable \ndry EEG. IEEE transactions on biomedical engineering, \n62(11), 2553-2567..  \n[7] Prechelt, L. (2002). Early stopping-but when?. In Neural \nNetworks: Tricks of the trade (pp. 55-69). Berlin, \nHeidelberg: Springer Berlin Heidelberg. \n[8] Zhang, Y., Ruan, H., Yuan, Z., Du, H., Gao, X., & \nLu, J. (2023, June). A Learnable Spatial Mapping for \nDecoding the Directional Focus of Auditory Attention \nUsing EEG. In ICASSP 2023-2023 IEEE International \nConference on Acoustics, Speech and Signal Processing \n(ICASSP) (pp. 1-5). IEEE. \n[9] Su, E., Cai, S., Xie, L., Li, H., & Schultz, T. (2022). \nSTAnet: A spatiotemporal attention network for \ndecoding auditory spatial attention from EEG. IEEE \nTransactions on Biomedical Engineering, 69(7), 2233-\n2242. \n[10] Défossez, A., Caucheteux, C., Rapin, J., Kabeli, O., & \nKing, J. R. (2023). Decoding speech perception from \nnon-invasive \nbrain \nrecordings. \nNature \nMachine \nIntelligence, 1-11. \n[11]Gonzalez, R. C. (2009). Digital image processing. \nPearson education india. \n[12]Truong, D., Milham, M., Makeig, S., & Delorme, A. \n(2021, November). Deep convolutional neural network \napplied to electroencephalography: Raw Data vs spectral \nfeatures. In 2021 43rd Annual International Conference \nof the IEEE Engineering in Medicine & Biology Society \n(EMBC) (pp. 1039-1042). IEEE. \n[13] Alexander, L. M., Escalera, J., Ai, L., Andreotti, C., \nFebre, K., Mangone, A., ... & Milham, M. P. (2017). An \nopen resource for transdiagnostic research in pediatric \nmental health and learning disorders. Scientific data, \n4(1), 1-26. \n[14]Truong, D., Makeig, S., & Delorme, A. (2021, \nDecember). Assessing learned features of Deep Learning \napplied to EEG. In 2021 IEEE International Conference \non Bioinformatics and Biomedicine (BIBM) (pp. 3667-\n3674). IEEE. \n[15]Simonyan, K., & Zisserman, A. (2014). Very deep \nconvolutional \nnetworks \nfor \nlarge-scale \nimage \nrecognition. arXiv preprint arXiv:1409.1556. \n[16] Li, C., Qi, Y., Ding, X., Zhao, J., Sang, T., & Lee, \nM. (2022). A deep learning method approach for sleep \nstage classification with eeg spectrogram. International \nJournal of Environmental Research and Public Health, \n19(10), 6322. \n[17]Truong, D., Sinha, M., Venkataraju, K. U., Milham, M., \n& Delorme, A. (2022, July). A streamable large-scale \nclinical EEG dataset for Deep Learning. In 2022 44th \nAnnual \nInternational \nConference \nof \nthe \nIEEE \nEngineering in Medicine & Biology Society (EMBC) \n(pp. 1058-1061). IEEE. \n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-10-16",
  "updated": "2023-10-16"
}