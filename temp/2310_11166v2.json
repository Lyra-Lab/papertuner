{
  "id": "http://arxiv.org/abs/2310.11166v2",
  "title": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing",
  "authors": [
    "Quoc-Nam Nguyen",
    "Thang Chau Phan",
    "Duc-Vu Nguyen",
    "Kiet Van Nguyen"
  ],
  "abstract": "English and Chinese, known as resource-rich languages, have witnessed the\nstrong development of transformer-based language models for natural language\nprocessing tasks. Although Vietnam has approximately 100M people speaking\nVietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,\nperformed well on general Vietnamese NLP tasks, including POS tagging and named\nentity recognition. These pre-trained language models are still limited to\nVietnamese social media tasks. In this paper, we present the first monolingual\npre-trained language model for Vietnamese social media texts, ViSoBERT, which\nis pre-trained on a large-scale corpus of high-quality and diverse Vietnamese\nsocial media texts using XLM-R architecture. Moreover, we explored our\npre-trained model on five important natural language downstream tasks on\nVietnamese social media texts: emotion recognition, hate speech detection,\nsentiment analysis, spam reviews detection, and hate speech spans detection.\nOur experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses\nthe previous state-of-the-art models on multiple Vietnamese social media tasks.\nOur ViSoBERT model is available only for research purposes.",
  "text": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media\nText Processing\nQuoc-Nam Nguyen1, 3, *, Thang Chau Phan1, 3, *, Duc-Vu Nguyen2, 3, Kiet Van Nguyen1, 3\n1Faculty of Information Science and Engineering, University of Information Technology,\nHo Chi Minh City, Vietnam\n2Multimedia Communications Laboratory, University of Information Technology,\nHo Chi Minh City, Vietnam\n3Vietnam National University, Ho Chi Minh City, Vietnam\n{20520644, 20520929}@gm.uit.edu.vn\n{vund, kietnv}@uit.edu.vn\nAbstract\nEnglish and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1\nIntroduction\nLanguage models based on transformer architec-\nture (Vaswani et al., 2017) pre-trained on large-\nscale datasets have brought about a paradigm shift\nin natural language processing (NLP), reshaping\nhow we analyze, understand, and generate text. In\n*Equal contribution.\n4https://huggingface.co/uitnlp/visobert\nparticular, BERT (Devlin et al., 2019) and its vari-\nants (Liu et al., 2019; Conneau et al., 2020) have\nachieved state-of-the-art performance on a wide\nrange of downstream NLP tasks, including but not\nlimited to text classification, sentiment analysis,\nquestion answering, and machine translation. En-\nglish is moving for the rapid development of lan-\nguage models across specific domains such as med-\nical (Lee et al., 2019; Rasmy et al., 2021), scientific\n(Beltagy et al., 2019), legal (Chalkidis et al., 2020),\npolitical conflict and violence (Hu et al., 2022),\nand especially social media (Nguyen et al., 2020;\nDeLucia et al., 2022; Pérez et al., 2022; Zhang\net al., 2022).\nVietnamese is the eighth largest language used\nover the internet, with around 85 million users\nacross the world5. Despite a large amount of Viet-\nnamese data available over the Internet, the ad-\nvancement of NLP research in Vietnamese is still\nslow-moving. This can be attributed to several fac-\ntors, to name a few: the scattered nature of avail-\nable datasets, limited documentation, and minimal\ncommunity engagement. Moreover, most existing\npre-trained models for Vietnamese were primarily\ntrained on large-scale corpora sourced from general\ntexts (Tran et al., 2020; Nguyen and Tuan Nguyen,\n2020; Tran et al., 2023). While these sources pro-\nvide broad language coverage, they may not fully\nrepresent the sociolinguistic phenomena in Viet-\nnamese social media texts. Social media texts often\nexhibit different linguistic patterns, informal lan-\nguage usage, non-standard vocabulary, lacking dia-\ncritics and emoticons that are not prevalent in for-\nmal written texts. The limitations of using language\nmodels pre-trained on general corpora become ap-\nparent when processing Vietnamese social media\ntexts. The models can struggle to accurately un-\n5https://www.internetworldstats.com/stats3.htm\narXiv:2310.11166v2  [cs.CL]  28 Oct 2023\nderstand and interpret the informal language, using\nemoji, teencode, and diacritics used in social media\ndiscussions. This can lead to suboptimal perfor-\nmance in Vietnamese social media tasks, including\nemotion recognition, hate speech detection, sen-\ntiment analysis, spam reviews detection, and hate\nspeech spans detection.\nWe present ViSoBERT, a pre-trained language\nmodel designed explicitly for Vietnamese social\nmedia texts to address these challenges. ViSoBERT\nis based on the transformer architecture and trained\non a large-scale dataset of Vietnamese posts and\ncomments extracted from well-known social me-\ndia networks, including Facebook, Tiktok, and\nYoutube. Our model outperforms existing pre-\ntrained models on various downstream tasks, in-\ncluding emotion recognition, hate speech detec-\ntion, sentiment analysis, spam reviews detection,\nand hate speech spans detection, demonstrating its\neffectiveness in capturing the unique characteristics\nof Vietnamese social media texts. Our contribu-\ntions are summarized as follows.\n• We presented ViSoBERT, the first PLM based\non the XLM-R architecture and pre-training\nprocedure for Vietnamese social media text\nprocessing. ViSoBERT is available publicly\nfor research purposes in Vietnamese social\nmedia mining. ViSoBERT can be a strong\nbaseline for Vietnamese social media text pro-\ncessing tasks and their applications.\n• ViSoBERT produces SOTA performances on\nmultiple Vietnamese downstream social me-\ndia tasks, thus illustrating the effectiveness of\nour PLM on Vietnamese social media texts.\n• To understand our pre-trained language model\ndeeply, we analyze experimental results on the\nmasking rate, examining social media char-\nacteristics, including emojis, teencode, and\ndiacritics, and implementing feature-based ex-\ntraction for task-specific models.\n2\nFundamental of Pre-trained Language\nModels for Social Media Texts\nPre-trained Language Models (PLMs) based on\ntransformers (Vaswani et al., 2017) have become a\ncrucial element in cutting-edge NLP tasks, includ-\ning text classification and natural language genera-\ntion. Since then, language models based on trans-\nformers related to our study have been reviewed,\nincluding PLMs for Vietnamese social media texts.\n2.1\nPre-trained Language Models for\nVietnamese\nSeveral PLMs have recently been developed for\nprocessing Vietnamese texts. These models have\nvaried in their architectures, training data, and eval-\nuation metrics. PhoBERT, developed by Nguyen\nand Tuan Nguyen (2020), is the first general pre-\ntrained language model (PLM) created for the Viet-\nnamese language. The model employs the same\narchitecture as BERT (Devlin et al., 2019) and\nthe same pre-training technique as RoBERTa (Liu\net al., 2019) to ensure robust and reliable perfor-\nmance. PhoBERT was trained on a 20GB word-\nlevel Vietnamese Wikipedia corpus, which pro-\nduces SOTA performances on a range of down-\nstream tasks of POS tagging, dependency parsing,\nNER, and NLI.\nFollowing the success of PhoBERT, viBERT\n(Tran et al., 2020) and vELECTRA (Tran et al.,\n2020), both monolingual pre-trained language\nmodels based on the BERT and ELECTRA ar-\nchitectures, were introduced. They were trained on\nsubstantial datasets, with ViBERT using a 10GB\ncorpus and vELECTRA utilizing an even larger\n60GB collection of uncompressed Vietnamese text.\nviBERT4news6 was published by NlpHUST, a\nVietnamese version of BERT trained on more than\n20 GB of news datasets. For Vietnamese text sum-\nmarization, BARTpho (Tran et al., 2022) is pre-\nsented as the first large-scale monolingual seq2seq\nmodels pre-trained for Vietnamese, based on the\nseq2seq denoising autoencoder BART. Moreover,\nViT5 (Phan et al., 2022) follows the encoder-\ndecoder architecture proposed by Vaswani et al.\n(2017) and the T5 framework proposed by Raffel\net al. (2020). Many language models are designed\nfor general use, while the availability of strong\nbaseline models for domain-specific applications\nremains limited. Since then, Minh et al. (2022) in-\ntroduced ViHealthBERT, the first domain-specific\nPLM for Vietnamese healthcare.\n2.2\nPre-trained Language Models for Social\nMedia Texts\nMultiple PLMs were introduced for social me-\ndia for multilingual and monolinguals. BERTweet\n(Nguyen et al., 2020) was presented as the first pub-\nlic large-scale PLM for English Tweets. BERTweet\nhas the same architecture as BERTBase (Devlin\net al., 2019) and is trained using the RoBERTa pre-\n6https://github.com/bino282/bert4news\ntraining procedure (Liu et al., 2019). Koto et al.\n(2021) proposed IndoBERTweet, the first large-\nscale pre-trained model for Indonesian Twitter. In-\ndoBERTweet is trained by extending a monolin-\ngually trained Indonesian BERT model with an\nadditive domain-specific vocabulary. RoBERTu-\nito, presented in Pérez et al. (2022), is a robust\ntransformer model trained on 500 million Span-\nish tweets.\nRoBERTuito excels in various lan-\nguage contexts, including multilingual and code-\nswitching scenarios, such as Spanish and English.\nTWilBert (Ángel González et al., 2021) is proposed\nas a specialization of BERT architecture both for\nthe Spanish language and the Twitter domain to\naddress text classification tasks in Spanish Twitter.\nBernice, introduced by DeLucia et al. (2022), is\nthe first multilingual pre-trained encoder designed\nexclusively for Twitter data. This model uses a cus-\ntomized tokenizer trained solely on Twitter data\nand incorporates a larger volume of Twitter data\n(2.5B tweets) than most BERT-style models. Zhang\net al. (2022) introduced TwHIN-BERT, a multilin-\ngual language model trained on 7 billion Twitter\ntweets in more than 100 different languages. It is\ndesigned to handle short, noisy, user-generated text\neffectively. Previously, (Barbieri et al., 2022) ex-\ntended the training of the XLM-R (Conneau et al.,\n2020) checkpoint using a data set comprising 198\nmillion multilingual tweets. As a result, XLM-T is\nadapted to the Twitter domain and was not exclu-\nsively trained on data from within that domain.\n3\nViSoBERT\nThis section presents the architecture, pre-training\ndata, and our custom tokenizer on Vietnamese so-\ncial media texts for ViSoBERT.\n3.1\nPre-training Data\nWe crawled textual data from Vietnamese public\nsocial networks such as Facebook7, Tiktok8, and\nYouTube9 which are the three most well known\nsocial networks in Vietnam, with 52.65, 49.86, and\n63.00 million users10, respectively, in early 2023.\nTo effectively gather data from these platforms,\nwe harnessed the capabilities of specialized tools\nprovided by each platform.\n7https://www.facebook.com/\n8https://www.tiktok.com/\n9https://www.youtube.com/\n10https://datareportal.com/reports/\ndigital-2023-vietnam\n1. Facebook:\nWe crawled comments from\nVietnamese-verified pages by Facebook posts\nvia the Facebook Graph API11 between Jan-\nuary 2016 and December 2022.\n2. TikTok:\nWe collected comments from\nVietnamese-verified channels by TikTok\nthrough TikTok Research API12 between Jan-\nuary 2020 and December 2022.\n3. YouTube:\nWe scrapped comments from\nVietnam-verified\nchannels’\nvideos\nby\nYouTube via YouTube Data API13 between\nJanuary 2016 and December 2022.\nPre-processing Data: Pre-processing is vital for\nmodels consuming social media data, which is mas-\nsively noisy, and has user handles (@username),\nhashtags, emojis, misspellings, hyperlinks, and\nother noncanonical texts. We perform the follow-\ning steps to clean the dataset: removing noncanon-\nical texts, removing comments including links, re-\nmoving excessively repeated spam and meaning-\nless comments, removing comments including only\nuser handles (@username), and keeping emojis in\ntraining data.\nAs a result, our pretraining data after crawling\nand preprocessing contains 1GB of uncompressed\ntext. Our pretraining data is available only for re-\nsearch purposes.\n3.2\nModel Architecture\nTransformers (Vaswani et al., 2017) have signifi-\ncantly advanced NLP research using trained mod-\nels in recent years. Although language models\n(Nguyen and Tuan Nguyen, 2020; Nguyen and\nNguyen, 2021) have also proven effective on a\nrange of Vietnamese NLP tasks, their results on\nVietnamese social media tasks (Nguyen et al.,\n2022) need to be significantly improved. To ad-\ndress this issue, taking into account successful\nhyperparameters from XLM-R (Conneau et al.,\n2020), we proposed ViSoBERT, a transformer-\nbased model in the style of XLM-R architecture\nwith 768 hidden units, 12 self-attention layers, and\n12 attention heads, and used a masked language\nobjective (the same as Conneau et al. (2020)).\n11https://developers.facebook.com/\n12https://developers.tiktok.com/products/\nresearch-api/\n13https://developers.google.com/youtube/v3\n3.3\nThe Vietnamese Social Media Tokenizer\nTo the best of our knowledge, ViSoBERT is the first\nPLM with a custom tokenizer for Vietnamese social\nmedia texts. Bernice (DeLucia et al., 2022) was\nthe first multilingual model trained from scratch on\nTwitter14 data with a custom tokenizer; however,\nBernice’s tokenizer doesn’t handle Vietnamese so-\ncial media text effectively. Moreover, existing Viet-\nnamese pre-trained models’ tokenizers perform\npoorly on social media text because of different\ndomain data training. Therefore, we developed the\nfirst custom tokenizer for Vietnamese social media\ntexts.\nOwing to the ability to handle raw texts of Sen-\ntencePiece (Kudo and Richardson, 2018) without\nany loss compared to Byte-Pair Encoding (Con-\nneau et al., 2020), we built a custom tokenizer on\nVietnamese social media by SentencePiece on the\nwhole training dataset. A model has better cov-\nerage of data than another when fewer subwords\nare needed to represent the text, and the subwords\nare longer (DeLucia et al., 2022). Figure 2 (in Ap-\npendix A) displays the mean token length for each\nconsidered model and group of tasks. ViSoBERT\nachieves the shortest representations for all Viet-\nnamese social media downstream tasks compared\nto other PLMs.\nEmojis and teencode are essential to the “lan-\nguage” on Vietnamese social media platforms. Our\ncustom tokenizer’s capability to decode emojis and\nteencode ensure that their semantic meaning and\ncontextual significance are accurately captured and\nincorporated into the language representation, thus\nenhancing the overall quality and comprehensive-\nness of text analysis and understanding.\nTo assess the tokenized ability of Vietnamese so-\ncial media textual data, we conducted an analysis of\nseveral data samples. Table 1 shows several actual\nsocial comments and their tokenizations with the\ntokenizers of the two pre-trained language models,\nViSoBERT and PhoBERT, the best strong base-\nline. The results show that our custom tokenizer\nperformed better compared to others.\n4\nExperiments and Results\n4.1\nExperimental Settings\nWe accumulate gradients over one step to simu-\nlate a batch size of 128. When pretraining from\nscratch, we train the model for 1.2M steps in 12\n14https://twitter.com/\nepochs.\nWe trained our model for about three\ndays on 2×RTX4090 GPUs (24GB). Each sen-\ntence is tokenized and masked dynamically with\na probability equal to 30% (which is extensively\nexperimented on Section 5.1 to explore the opti-\nmal value). Further details on hyperparameters and\ntraining can be found in Table 6 of Appendix B.\nDownstream tasks. To evaluate ViSoBERT, we\nused five Vietnamese social media datasets avail-\nable for research purposes, as summarized in Table\n2. The downstream tasks include emotion recogni-\ntion (UIT-VSMEC) (Ho et al., 2020), hate speech\ndetection (UIT-ViHSD) (Luu et al., 2021), sen-\ntiment analysis (SA-VLSP2016) (Nguyen et al.,\n2018), spam reviews detection (ViSpamReviews)\n(Dinh et al., 2022), and hate speech spans detection\n(UIT-ViHOS) (Hoang et al., 2023).\nFine-tuning.\nWe conducted empirical fine-\ntuning for all pre-trained language models using\nthe simpletransformers15. Our fine-tuning process\nfollowed standard procedures, most of which are\noutlined in (Devlin et al., 2019). For all tasks men-\ntioned above, we use a batch size of 40, a maxi-\nmum token length of 128, a learning rate of 2e-5,\nand AdamW optimizer (Loshchilov and Hutter,\n2019) with an epsilon of 1e-8. We executed a 10-\nepoch training process and evaluated downstream\ntasks using the best-performing model from those\nepochs. Furthermore, none of the pre-processing\ntechniques is applied in all datasets to evaluate our\nPLM’s ability to handle raw texts.\nBaseline models. To establish the main baseline\nmodels, we utilized several well-known PLMs, in-\ncluding monolingual and multilingual, to support\nVietnamese NLP social media tasks. The details of\neach model are shown in Table 3.\n• Monolingual language models:\nviBERT\n(Tran et al., 2020) and vELECTRA (Tran\net al., 2020) are PLMs for Vietnamese based\non BERT and ELECTRA architecture, respec-\ntively. PhoBERT, which is based on BERT\narchitecture and RoBERTa pre-training proce-\ndure, (Nguyen and Tuan Nguyen, 2020) is the\nfirst large-scale monolingual language model\npre-trained for Vietnamese; PhoBERT obtains\nstate-of-the-art performances on a range of\nVietnamese NLP tasks.\n• Multilingual language models: Addition-\nally, we incorporated two multilingual PLMs,\n15https://simpletransformers.ai/ (ver 0.63.11)\nComments\nViSoBERT\nPhoBERT\nconcặc cáilồn gìđây\nEnglish: Wut is dis fuckingd1ck\n<s>, \"conc\", \"ặc\", \"cái\", \"l\", \"ồn\", \"gì\", \"đây\",\n\"\n\", </s>\n<s>, \"c o n @ @\", \"c @ @\", \"ặc\", \"c á @ @\",\n\"i l @ @\", \"ồn\", \"g @ @\",\"ì @ @\", \"đ â y\",\n<unk>, <unk>, <unk>, </s>\ne cảmơn anh\nEnglish: Thankyou\n<s>, \"e\", \"cảm\", \"ơn\", \"anh\", \"\n\", \"\n\", </s>\n<s>, \"e\", \"c ả@ @\", \"m @ @\", \"ơ n\", \"a n h\",\n<unk>, <unk>, </s>\nd4y l4 vj du cko mot cau teencode\nEnglish: Th1s 1s 4 teencode s3nt3nc3\n<s>, \"d\", \"4\", \"y\", \"l\", \"4\", \"vj\", \"du\", \"cko\", \"mot\",\n\"cau\", \"teen\", \"code\", </s>\n<s>, \"d @ @\", \"4 @ @\", \"y\", \"l @ @\", \"4\",\n\"v @ @\", \"j\", \"d u\", \"c k @ @\", \"o\", \"m o @ @\",\n\"t\", \"c a u\"; \"t e @ @\", \"e n @ @\", \"c o d e\", </s>\nTable 1: Actual social comments and their tokenizations with the tokenizers of the two pre-trained language models,\nViSoBERT and PhoBERT.\nDataset\nTrain\nDev\nTest\nTask\nEvaluation Metrics\nClasses\nUIT-VSMEC\n5,548\n686\n693\nEmotion Recognition (ER)\nAcc, WF1, MF1 (%)\n7\nUIT-HSD\n24,048\n2,672\n6,680\nHate Speech Detection (HSD)\n3\nSA-VLSP2016\n5,100\n-\n1,050\nSentiment Analysis (SA)\n3\nViSpamReviews\n14,306\n1,590\n3,974\nSpam Reviews Detection (SRD)\n4\nViHOS\n8,844\n1,106\n1,106\nHate Speech Spans Detection (HSSD)\n3\nTable 2: Statistics and descriptions of Vietnamese social media processing tasks. Acc, WF1, and MF1 denoted\nAccuracy, weighted F1-score, and macro F1-score metrics, respectively.\nmBERT (Devlin et al., 2019) and XLM-R\n(Conneau et al., 2020), which were previously\nshown to have competitive performances to\nmonolingual Vietnamese models. XLM-R,\na cross-lingual PLM introduced by Conneau\net al. (2020), has been trained in 100 lan-\nguages, among them Vietnamese, utilizing\na vast 2.5TB Clean CommonCrawl dataset.\nXLM-R presents notable improvements in\nvarious downstream tasks, surpassing the per-\nformance of previously released multilingual\nmodels such as mBERT (Devlin et al., 2019)\nand XLM (Lample and Conneau, 2019).\n• Multilingual social media language models:\nTo ensure a fair comparison with our PLM,\nwe integrated multiple multilingual social me-\ndia PLMs, including XLM-T (Barbieri et al.,\n2022), TwHIN-BERT (Zhang et al., 2022),\nand Bernice (DeLucia et al., 2022).\n4.2\nMain Results\nTable 4 shows ViSoBERT’s scores with the previ-\nous highest reported results on other PLMs using\nthe same experimental setup. It is clear that our Vi-\nSoBERT produces new SOTA performance results\nfor multiple Vietnamese downstream social media\ntasks without any pre-processing technique.\nEmotion Recognition Task: PhoBERT and\nTwHIN-BERT archive the previous SOTA perfor-\nmances on monolingual and multilingual models,\nrespectively. ViSoBERT obtains 68.10%, 68.37%,\nand 65.88% of Acc, WF1, and MF1, respec-\ntively, significantly higher than these PhoBERT\nand TwHIN-BERT models.\nHate Speech Detection Task:\nViSoBERT\nachieves significant improvements over previous\nstate-of-the-art models, PhoBERT and TwHIN-\nBERT, with scores of 88.51%, 88.31%, and 68.77%\nin Acc, WF1, and MF1, respectively. Notably, these\nachievements are made despite the presence of bias\nwithin the dataset16.\nSentiment Analysis Task: XLM-R archived\nSOTA performance on three evaluation metrics.\nHowever, there is no significant increase in per-\nformance on this downstream task, for 0.45%,\n0.46%, and 0.46% higher on Acc, WF1, and\nMF1 compared to our pre-trained language model,\nPhoBERTLarge.\nThe SA-VLSP2016 dataset do-\nmain is technical article reviews, including Tin-\nhTe17 and VnExpress18, which are often used as\nVietnamese standard data. The reviews or com-\nments in these newspapers are in proper form.\nWhile most of the dataset is sourced from ar-\nticles, it also includes data from Facebook19, a\nVietnamese social media platform that accounts\nfor only 12.21% of the dataset. Therefore, the\ndataset does not fully capture Vietnamese social\nmedia platforms’ diverse characteristics and infor-\n16UIT-HSD is massively imbalanced, included 19,886;\n1,606; and 2,556 of CLEAN, OFFENSIVE, and HATE class.\n17https://tinhte.vn/\n18https://vnexpress.net/\n19https://www.facebook.com/\nModel\n#Layers\n#Heads\n#Steps\n#Batch\nDomain Data\n#Params\n#Vocab\n#MSL\nCSMT\nviBERT (Tran et al., 2020)\n12\n12\n-\n16\nVietnamese News\n-\n30K\n256\nNo\nvELECTRA (Tran et al., 2020)\n12\n3\n-\n16\nNewsCorpus + OscarCorpus\n-\n30K\n256\nNo\nPhoBERTBase (Nguyen and Tuan Nguyen, 2020)\n12\n12\n540K\n1024\nViWiki + ViNews\n135M\n64K\n256\nNo\nPhoBERTLarge (Nguyen and Tuan Nguyen, 2020)\n24\n16\n1.08M\n512\nViWiki + ViNews\n370M\n64K\n256\nNo\nmBERT (Devlin et al., 2019)\n12\n12\n1M\n256\nBookCorpus + EnWiki\n110M\n30K\n512\nNo\nXLM-RBase (Conneau et al., 2020)\n12\n12\n1.5M\n8192\nCommonCrawl + Wiki\n270M\n250K\n512\nNo\nXLM-RLarge (Conneau et al., 2020)\n24\n16\n1.5M\n8192\nCommonCrawl + Wiki\n550M\n250K\n512\nNo\nXLM-T (Barbieri et al., 2022)\n12\n12\n-\n8192\nMultilingual Tweets\n-\n250k\n512\nNo\nTwHIN-BERTBase (Zhang et al., 2022)\n12\n12\n500K\n6K\nMultilingual Tweets\n135M to 278M\n250K\n128\nNo\nTwHIN-BERTLarge (Zhang et al., 2022)\n24\n16\n500K\n8K\nMultilingual Tweets\n550M\n250K\n128\nNo\nBernice (DeLucia et al., 2022)\n12\n12\n405K+\n8192\nMultilingual Tweets\n270M\n250K\n128\nYes\nViSoBERT (Ours)\n12\n12\n1.2M\n128\nVietnamese social media\n97M\n15K\n512\nYes\nTable 3: Detailed information about baselines and our PLM. #Layers, #Heads, #Batch, #Params, #Vocab, #MSL,\nand CSMT indicate the number of hidden layers, number of attention heads, batch size value, domain training data,\nnumber of total parameters, vocabulary size, max sequence length, and custom social media tokenizer, respectively.\nModel\nAvg\nEmotion Regconition\nHate Speech Detection\nSentiment Analysis\nSpam Reviews Detection\nHate Speech Spans Detection\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nviBERT\n71.57\n61.91\n61.98\n59.70\n85.34\n85.01\n62.07\n74.85\n74.73\n74.73\n89.93\n89.79\n76.80\n90.42\n90.45\n84.55\nvELECTRA\n72.43\n64.79\n64.71\n61.95\n86.96\n86.37\n63.95\n74.95\n74.88\n74.88\n89.83\n89.68\n76.23\n90.59\n90.58\n85.12\nPhoBERTBase\n72.81\n63.49\n63.36\n61.41\n87.12\n86.81\n65.01\n75.72\n75.52\n75.52\n89.83\n89.75\n76.18\n91.32\n91.38\n85.92\nPhoBERTLarge\n73.47\n64.71\n64.66\n62.55\n87.32\n86.98\n65.14\n76.52\n76.36\n76.22\n90.12\n90.03\n76.88\n91.44\n91.46\n86.56\nmBERT (cased)\n68.07\n56.27\n56.17\n53.48\n83.55\n83.99\n60.62\n67.14\n67.16\n67.16\n89.05\n88.89\n74.52\n89.88\n89.87\n84.57\nmBERT (uncased)\n67.66\n56.23\n56.11\n53.32\n83.38\n81.27\n58.92\n67.25\n67.22\n67.22\n88.92\n88.72\n74.32\n89.84\n89.82\n84.51\nXLM-RBase\n72.08\n60.92\n61.02\n58.67\n86.36\n86.08\n63.39\n76.38\n76.38\n76.38\n90.16\n89.96\n76.55\n90.74\n90.72\n85.42\nXLM-RLarge\n73.40\n62.44\n61.37\n60.25\n87.15\n86.86\n65.13\n78.28\n78.21\n78.21\n90.36\n90.31\n76.75\n91.52\n91.50\n86.66\nXLM-T\n72.23\n64.64\n64.37\n59.86\n86.22\n86.12\n63.48\n75.66\n75.60\n75.60\n90.07\n90.11\n76.66\n90.88\n90.88\n85.53\nTwHIN-BERTBase\n71.60\n61.49\n60.88\n57.97\n86.63\n86.23\n63.67\n73.76\n73.72\n73.72\n90.25\n90.35\n76.98\n90.99\n90.90\n85.67\nTwHIN-BERTLarge\n73.42\n64.21\n64.29\n61.12\n87.23\n86.78\n65.23\n76.92\n76.83\n76.83\n90.47\n90.42\n77.28\n91.45\n91.47\n86.65\nBernice\n72.49\n64.21\n64.27\n60.68\n86.12\n86.48\n64.32\n74.57\n74.90\n74.90\n90.22\n90.21\n76.89\n90.48\n90.06\n85.67\nViSoBERT\n75.65\n68.10\n68.37\n65.88‡\n88.51\n88.31\n68.77‡\n77.83\n77.75\n77.75\n90.99\n90.92\n79.06‡\n91.62\n91.57\n86.80\nTable 4: Performances on downstream Vietnamese social media tasks of previous state-of-the-art monolingual and\nmultilingual PLMs without pre-processing techniques. Avg denoted the average MF1 score of each PLM. ‡ denotes\nthat the highest result is statistically significant at p < 0.01 compared to the second best, using a paired t-test.\nmal language. However, ViSoBERT still surpassed\nother baselines by obtaining 1.31%/0.91% Acc,\n1.39%/0.92% WF1, and 1.53%/0.92% MF1 com-\npared to PhoBERT/TwHIN-BERT.\nSpam Reviews Detection Task: ViSoBERT\nperformed better than the top two baseline mod-\nels, PhoBERT and TwHIN-BERT. Specifically, it\nachieved 0.8%, 0.9%, and 2.18% higher scores in\naccuracy (Acc), weighted F1 (WF1), and micro F1\n(MF1) compared to PhoBERT. When compared to\nTwHIN-BERT, ViSoBERT outperformed it with\n0.52%, 0.50%, and 1.78% higher scores in Acc,\nWF1, and MF1, respectively.\nHate Speech Spans Detection Task20: Our\npre-trained ViSoBERT boosted the results up to\n91.62%, 91.57%, and 86.80% on Acc, WF1, and\nMF1, respectively. While the difference is insignif-\nicant, ViSoBERT indicates an outstanding ability\nto capture Vietnamese social media information\ncompared to other PLMs (see Section 5.3).\nMultilingual social media PLMs: The results\nshow that ViSoBERT consistently outperforms\n20For the Hate Speech Spans Detection task, we evaluate the\ntotal of spans on each comment rather than spans of each word\nin Hoang et al. (2023) to retain the context of each comment.\nXLM-T and Bernice in five Vietnamese social me-\ndia tasks. It’s worth noting that XLM-T, TwHIN-\nBERT, and Bernice were all exclusively trained\non data from the Twitter platform. However, this\napproach has limitations when applied to the Viet-\nnamese context. The training data from this source\nmay not capture the intricate linguistic and contex-\ntual nuances prevalent in Vietnamese social media\nbecause Twitter is not widely used in Vietnam.\n5\nResult Analysis and Discussion\nIn this section, we consider the improvement of\nour PLM more compared to powerful others, in-\ncluding PhoBERT and TwHIN-BERT, in terms of\ndifferent aspects. Firstly, we investigate the effects\nof masking rate on our pre-trained model perfor-\nmance (see Section 5.1). Additionally, we examine\nthe influence of social media characteristics on the\nmodel’s ability to process and understand the lan-\nguage used in these social contexts (see Section\n5.2). Lastly, we employed feature-based extraction\ntechniques on task-specific models to verify the\npotential of leveraging social media textual data to\nenhance word representations (see Section 5.3).\n5.1\nImpact of Masking Rate on Vietnamese\nSocial Media PLM\nFor the first time presenting the Masked Language\nModel, Devlin et al. (2019) consciously utilized a\nrandom masking rate of 15%. The authors believed\nmasking too many tokens could lead to losing cru-\ncial contextual information required to decode them\naccurately. Additionally, the authors felt that mask-\ning too few tokens would harm the training process\nand make it less effective. However, according to\nWettig et al. (2023), 15% is not universally optimal\nfor model and training data.\nWe experiment with masking rates ranging from\n10% to 50% and evaluate the model’s performance\non five downstream Vietnamese social media tasks.\nFigure 1 illustrates the results obtained from our\nexperiments with six different masking rates. Inter-\nestingly, our pre-trained ViSoBERT achieved the\nhighest performance when using a masking rate of\n30%. This suggests a delicate balance between the\namount of contextual information retained and the\nefficiency of the training process, and an optimal\nmasking rate can be found within this range.\nHowever, the optimal masking rate also depends\non the specific task. For instance, in the hate speech\ndetection task, we found that a masking rate of 50%\nyielded the best results, surpassing other masking\nrate values. This implies that the optimal masking\nrate may vary depending on the nature and require-\nments of different tasks.\nConsidering the overall performance across mul-\ntiple tasks, we determined that a masking rate\nof 30% produced the optimal balance for our\npre-trained ViSoBERT model. Consequently, we\nadopted this masking rate for ViSoBERT, ensur-\ning efficient and effective utilization of contextual\ninformation during training.\nVSMEC\nViSpam\nViHSD\nSA\nViHOS\nTasks\n0\n20\n40\n60\n80\nF1-macro (%)\nF1-macro Scores by Task and Masking Rate\nMasking Rate (%)\n10\n15\n20\n30\n40\n50\nFigure 1: Impact of masking rate on our pre-trained\nViSoBERT in terms of MF1.\n5.2\nImpact of Vietnamese Social Media\nCharacteristics\nEmojis, teencode, and diacritics are essential fea-\ntures of social media, especially Vietnamese so-\ncial media. The ability of the tokenizer to de-\ncode emojis and the ability of the model to un-\nderstand the context of teencode and diacritics are\ncrucial. Hence, to evaluate the performance of Vi-\nSoBERT on social media characteristics, compre-\nhensive experiments were conducted among sev-\neral strong PLMs: PM4ViSMT, PhoBERT, and\nTwHIN-BERT.\nImpact of Emoji on PLMs: We conducted two\nexperimental procedures to comprehensively inves-\ntigate the importance of emojis, including convert-\ning emojis to general text and removing emojis.\nTable 5 shows our detailed setting and experi-\nmental results on downstream tasks and pre-trained\nmodels. The results indicate a moderate reduc-\ntion in performance across all downstream tasks\nwhen emojis are removed or converted to text in our\npre-trained ViSoBERT model. Our pre-trained de-\ncreases 0.62% Acc, 0.55% WF1, and 0.78% MF1\non Average for downstream tasks while converting\nemojis to text. In addition, an average reduction\nof 1.33% Acc, 1.32% WF1, and 1.42% MF1 can\nbe seen in our pre-trained model while removing\nall emojis in each comment. This is because when\nemojis are converted to text, the context of the\ncomment is preserved, while removing all emojis\nresults in the loss of that context.\nThis trend is also observed in the TwHIN-BERT\nmodel, specifically designed for social media pro-\ncessing. However, TwHIN-BERT slightly improves\nemotion recognition and spam reviews detection\ntasks compared to its competitors when operat-\ning on raw texts. Nevertheless, this improvement\nis marginal and insignificant, as indicated by the\nsmall increments of 0.61%, 0.13%, and 0.21% in\nAcc, WF1, and MF1 on the emotion recognition\ntask, respectively, and 0.08% Acc, 0.05% WF1,\nand 0.04% MF1 on spam reviews detection task.\nOne potential reason for this phenomenon is that\nTwHIN-BERT and ViSoBERT are PLMs trained\non emojis datasets. Consequently, these models can\ncomprehend the contextual meaning conveyed by\nemojis. This finding underscores the importance\nof emojis in social media texts.\nIn contrast, there is a general trend of improved\nperformance across a range of downstream tasks\nwhen removing or converting emojis to text on\nModel\nEmotion Regconition\nHate Speech Detection\nSentiment Analysis\nSpam Reviews Detection\nHate Speech Spans Detection\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nConverting emojis to text\nPhoBERTLarge\n66.08\n66.15\n63.35\n87.43\n87.22\n65.32\n76.73\n76.48\n76.48\n90.35\n90.11\n77.02\n92.16\n91.98\n86.72\n∆\n↑1.37\n↑1.49\n↑0.80\n↑0.11\n↑0.24\n↑0.18\n↓0.21\n↓0.12\n↓0.12\n↑0.23\n↑0.08\n↑0.14\n↑0.72\n↑0.52\n↑0.16\nTwHIN-BERTLarge\n64.82\n64.42\n61.33\n86.03\n85.52\n63.52\n75.42\n75.95\n75.95\n90.55\n90.47\n77.32\n92.21\n92.01\n86.84\n∆\n↑0.61\n↑0.13\n↑0.21\n↓1.20\n↓1.26\n↓1.71\n↓1.50\n↓0.88\n↓0.88\n↑0.08\n↑0.05\n↑0.04\n↑0.76\n↑0.54\n↑0.19\nViSoBERT [♣]\n67.53\n67.93\n65.42\n87.82\n87.88\n67.25\n76.95\n76.85\n76.85\n90.22\n90.18\n78.25\n92.42\n92.11\n87.01\n∆\n↓0.57\n↓0.44\n↓0.46\n↓0.69\n↓0.41\n↓1.49\n↓0.88\n↓0.90\n↓0.90\n↓0.77\n↓0.74\n↓0.81\n↑0.80\n↑0.54\n↑0.21\nRemoving emojis\nPhoBERTLarge\n65.21\n65.14\n62.81\n87.25\n86.72\n64.85\n76.72\n76.48\n76.48\n90.21\n90.09\n77.02\n91.53\n91.51\n86.62\n∆\n↑0.50\n↑0.48\n↑0.26\n↓0.07\n↓0.26\n↓0.29\n↑0.20\n↑0.12\n↑0.12\n↑0.09\n↑0.06\n↑0.10\n↑0.09\n↑0.05\n↑0.09\nTwHIN-BERTLarge\n62.03\n62.14\n59.25\n86.98\n86.32\n64.22\n75.00\n75.11\n75.11\n89.83\n89.75\n76.85\n91.32\n91.33\n86.42\n∆\n↓2.18\n↓1.15\n↓1.87\n↓0.25\n↓0.46\n↓1.01\n↓1.92\n↓1.72\n↓1.72\n↓0.64\n↓0.67\n↓0.43\n↓0.13\n↓0.14\n↓0.23\nViSoBERT [♦]\n66.52\n67.02\n64.55\n87.32\n87.12\n66.98\n76.25\n75.98\n75.98\n89.72\n89.69\n77.95\n91.58\n91.53\n86.72\n∆\n↓1.58\n↓1.35\n↓1.33\n↓1.19\n↓1.19\n↓1.79\n↓1.58\n↓1.77\n↓1.77\n↓1.27\n↓1.23\n↓1.11\n↓0.04\n↓0.04\n↓0.08\nViSoBERT [♠]\n68.10\n68.37\n65.88\n88.51\n88.31\n68.77\n77.83\n77.75\n77.75\n90.99\n90.92\n79.06\n91.62\n91.57\n86.80\nTable 5: Performances of pre-trained models on downstream Vietnamese social media tasks by applying two emojis\npre-processing techniques. [♣], [♦], and [♠] denoted our pre-trained language model ViSoBERT converting\nemoji to text, removing emojis and without applying any pre-processing techniques, respectively. ∆denoted the\nincrease (↑) and the decrease (↓) in performances of the PLMs compared to their competitors without applying any\npre-processing techniques.\nPhoBERT, the Vietnamese SOTA pre-trained lan-\nguage model. PhoBERT is a PLM on a general\ntext (Vietnamese Wikipedia) dataset containing no\nemojis; therefore, when PhoBERT encounters an\nemoji, it treats it as an unknown token (see Table 1\nAppendix B). Therefore, while applying emoji pre-\nprocessing techniques, including converting emoijs\nto text and removing emojis, PhoBERT produces\nbetter performances compared to raw text.\nOur pre-trained model ViSoBERT on raw texts\noutperformed PhoBERT and TwHIN-BERT even\nwhen applying two pre-processing emojis tech-\nniques. This claims our pre-trained model’s ability\nto handle Vietnamese social media raw texts.\nImpact of Teencode on PLMs: Due to infor-\nmal and casual communication, social media texts\noften lead to common linguistic errors, such as mis-\nspellings and teencode. For example, the phrase\n“ăng kơmmmmm” should be “ăn cơm” (“Eat rice”\nin English), and “ko” should be “không” (“No”\nin English). To address this challenge, Nguyen\nand Van Nguyen (2020) presented several rules to\nstandardize social media texts. Building upon the\nprevious work, Quoc Tran et al. (2023) proposed\na strict and efficient pre-processing technique to\nclean comments on Vietnamese social media.\nTable 7 (in Appendix C) shows the results\nwith and without standardizing teencode on so-\ncial media texts.\nThere is an uptrend across\nPhoBERT, TwHIN-BERT, and ViSoBERT while\napplying standardized pre-processing techniques.\nViSoBERT, with standardized pre-processing tech-\nniques, outperforms almost downstream tasks but\nspam reviews detection. The possible reason is that\nthe ViSpamReviews dataset contains samples in\nwhich users use the word with duplicated characters\nto improve the comment length while standardizing\nteencodes leads to misunderstanding.\nExperimental results strongly suggest that the\nimprovement achieved by applying complex pre-\nprocessing techniques to pre-trained models in the\ncontext of Vietnamese social media text is rela-\ntively insignificant. Despite the considerable time\nand effort invested in designing and implementing\nthese techniques, the actual gains in PLMs perfor-\nmance are not substantial and unstable.\nImpact of Vietnamese Diacritics on PLMs:\nVietnamese words are created from 29 letters, in-\ncluding seven letters using four diacritics (ă, â-ê-ô,\nơ-ư, and đ) and five diacritics used to designate\ntone (as in à, á, ả, ã, and ạ) (Ngo, 2020). These\ndiacritics create meaningful words by combining\nsyllables (Le-Hong, 2021). For instance, the syl-\nlable “ngu” can be combined with five different\ndiacritic marks, resulting in five distinct syllables:\n“ngú”, “ngù”, “ngụ”, “ngủ”, and “ngũ”. Each of\nthese syllables functions as a standalone word.\nHowever, social media text does not always ad-\nhere to proper writing conventions. Due to var-\nious reasons, many users write text without dia-\ncritic marks when commenting on social media\nplatforms. Consequently, effectively handling dia-\ncritics in Vietnamese social media becomes a crit-\nical challenge. To evaluate the PLMs’ capability\nto address this challenge, we experimented by re-\nmoving all diacritic marks from the datasets of five\ndownstream tasks. This experiment aimed to assess\nthe model’s performance in processing text without\ndiacritics and determine its ability to understand\nVietnamese social media content in such cases.\nTable 8 (in Appendix C) presents the results of\nthe two best baselines compared to our pre-trained\ndiacritics experiments. The experimental results\nreveal that the performance of all pre-trained mod-\nels, including ours, exhibited a significant decrease\nwhen dealing with social media comments lack-\ning diacritics. This decline in performance can\nbe attributed to the loss of contextual information\ncaused by the removal of diacritics. The lower the\npercentage of diacritic removal in each comment,\nthe more significant the performance improvement\nin all PLMs. However, our ViSoBERT demon-\nstrated a relatively minor reduction in performance\nacross all downstream tasks. This suggests that\nour model possesses a certain level of robustness\nand adaptability in comprehending and analyzing\nVietnamese social media content without diacritics.\nWe attribute this to the efficiency of the in-domain\npre-training data of ViSoBERT.\nIn contrast, PhoBERT and TwHIN-BERT expe-\nrienced a substantial drop in performance across\nthe benchmark datasets. These PLMs struggled to\ncope with the absence of diacritics in Vietnamese\nsocial media comments. The main reason is that\nthe tokenizer of PhoBERT can not encode non-\ndiacritics comments due to not including those in\npre-training data. Several tokenized examples of\nthe three best PLMs are presented in Table 10 (in\nAppendix F). Thus, the significant decrease in its\nperformance highlights the challenge of handling\ndiacritics on Vietnamese social media. While han-\ndling diacritics remains challenging, ViSoBERT\ndemonstrates promising performance, suggesting\nthe potential for specialized language models tai-\nlored for Vietnamese social media analysis.\n5.3\nImpact of Feature-based Extraction to\nTask-Specific Models\nIn task-specific models, the contextualized word\nembeddings from PLMs are typically employed\nas input features. We aim to assess the quality\nof contextualized word embeddings generated by\nPhoBERT, TwHIN-BERT, and ViSoBERT to verify\nwhether social media data can enhance word repre-\nsentation. These contextualized word embeddings\nare applied as embedding features to BiLSTM, and\nBiGRU is randomly initialized before the classifi-\ncation layer. We append a linear prediction layer\nto the last transformer layer of each PLM regard-\ning the first subword of each word token, which is\nsimilar to Devlin et al. (2019).\nOur experiment results (see Table 9 in Ap-\npendix C) demonstrate that the word embeddings\ngenerated by our pre-trained language model Vi-\nSoBERT outperform other pre-trained embeddings\nwhen utilized with BiLSTM and BiGRU for all\ndownstream tasks. The experimental results in-\ndicate the significant impact of leveraging social\nmedia text data for enriching word embeddings.\nFurthermore, this finding underscores the effec-\ntiveness of our model in capturing the linguistic\ncharacteristics prevalent in Vietnamese social me-\ndia texts.\nFigure 3 (in Appendix D) presents the perfor-\nmances of the PLMs as input features to BiLSTM\nand BiGRU on the dev set per epoch in terms of\nMF1. The results demonstrate that ViSoBERT\nreaches its peak MF1 score in only 1 to 3 epochs,\nwhereas other PLMs typically require an average of\n8 to 10 epochs to achieve on-par performance. This\nsuggests that ViSoBERT has a superior capabil-\nity to extract Vietnamese social media information\ncompared to other models.\n6\nConclusion and Future Work\nWe presented ViSoBERT, a novel large-scale\nmonolingual pre-trained language model on Viet-\nnamese social media texts. We illustrated that Vi-\nSoBERT with fewer parameters outperforms re-\ncent strong pre-trained language models such as\nviBERT, vELECTRA, PhoBERT, XLM-R, XLM-\nT, TwHIN-BERT, and Bernice and achieves state-\nof-the-art performances for multiple downstream\nVietnamese social media tasks, including emotion\nrecognition, hate speech detection, spam reviews\ndetection, and hate speech spans detection. We\nconducted extensive analyses to demonstrate the ef-\nficiency of ViSoBERT on various Vietnamese so-\ncial media characteristics, including emojis, teen-\ncodes, and diacritics. Furthermore, our pre-trained\nlanguage model ViSoBERT also shows the poten-\ntial of leveraging Vietnamese social media text to\nenhance word representations compared to other\nPLMs. We hope the widespread use of our open-\nsource ViSoBERT pre-trained language model will\nadvance current NLP social media tasks and ap-\nplications for Vietnamese. Other low-resource lan-\nguages can adopt how to create PLMs for enhanc-\ning their current NLP social media tasks and rele-\nvant applications.\nLimitations\nWhile we have demonstrated that ViSoBERT can\nperform state-of-the-art on a range of NLP social\nmedia tasks for Vietnamese, we think additional\nanalyses and experiments are necessary to fully\ncomprehend what aspects of ViSoBERT were re-\nsponsible for its success and what understanding\nof Vietnamese social media texts ViSoBERT cap-\ntures. We leave these additional investigations to\nfuture research. Moreover, future work aims to ex-\nplore a broader range of Vietnamese social media\ndownstream tasks that this paper may not cover.\nAlso, we chose to train the base-size transformer\nmodel instead of the Large variant because base\nmodels are more accessible due to their lower com-\nputational requirements. For PhoBERT, XLM-R,\nand TwHIN-BERT, we implemented two versions\nBase and Large for all Vietnamese social media\ndownstream tasks. However, it is not a fair compar-\nison due to their significantly larger model configu-\nrations. Moreover, regular updates and expansions\nof the pre-training data are essential to keep up with\nthe rapid evolution of social media. This allows\nthe pre-trained model to adapt effectively to the dy-\nnamic linguistic patterns and trends in Vietnamese\nsocial media.\nEthics Statement\nThe authors introduce ViSoBERT, a pre-trained\nlanguage model for investigating social language\nphenomena in social media in Vietnamese. Vi-\nSoBERT is based on pre-training an existing pre-\ntrained language model (i.e., XLM-R), which\nlessens the influence of its construction on the en-\nvironment. ViSoBERT makes use of a large-scale\ncorpus of posts and comments from social commu-\nnities that have been found to express harassment,\nbullying, incitement of violence, hate, offense, and\nabuse, as defined by the content policies of social\nmedia platforms, including Facebook, YouTube,\nand TikTok.\nAcknowledgement\nThis research was supported by The VNUHCM-\nUniversity of Information Technology’s Scientific\nResearch Support Fund. We thank the anonymous\nEMNLP reviewers for their time and helpful sug-\ngestions that improved the quality of the paper.\nReferences\nFrancesco Barbieri, Luis Espinosa Anke, and Jose\nCamacho-Collados. 2022.\nXLM-T: Multilingual\nlanguage models in Twitter for sentiment analysis\nand beyond. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n258–266, Marseille, France. European Language Re-\nsources Association.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientific\nText.\nIn Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3615–3620.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The Muppets straight out of\nLaw School. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexandra DeLucia, Shijie Wu, Aaron Mueller, Carlos\nAguirre, Philip Resnik, and Mark Dredze. 2022. Ber-\nnice: A Multilingual Pre-trained Encoder for Twitter.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6191–6205, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nCo Van Dinh, Son T. Luu, and Anh Gia-Tuan Nguyen.\n2022. Detecting Spam Reviews on Vietnamese E-\nCommerce Websites. In Intelligent Information and\nDatabase Systems, pages 595–607. Springer Interna-\ntional Publishing.\nVong\nAnh\nHo,\nDuong\nHuynh-Cong\nNguyen,\nDanh Hoang Nguyen, Linh Thi-Van Pham, Duc-Vu\nNguyen, Kiet Van Nguyen, and Ngan Luu-Thuy\nNguyen. 2020. Emotion Recognition for Vietnamese\nSocial Media Text. In Computational Linguistics:\n16th International Conference of the Pacific Associa-\ntion for Computational Linguistics, PACLING 2019,\nHanoi, Vietnam, October 11–13, 2019, Revised\nSelected Papers 16, pages 319–333. Springer.\nPhu Gia Hoang, Canh Luu, Khanh Tran, Kiet Nguyen,\nand Ngan Nguyen. 2023. ViHOS: Hate Speech Spans\nDetection for Vietnamese. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 652–669.\nYibo Hu, MohammadSaleh Hosseini, Erick Sko-\nrupa Parolin, Javier Osorio, Latifur Khan, Patrick\nBrandt, and Vito D’Orazio. 2022. ConfliBERT: A\nPre-trained Language Model for Political Conflict\nand Violence. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5469–5482, Seattle, United States.\nAssociation for Computational Linguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021.\nIndoBERTweet: A pretrained language model for\nIndonesian Twitter with effective domain-specific\nvocabulary initialization. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10660–10668, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual Language Model Pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\nPhuong Le-Hong. 2021.\nDiacritics generation and\napplication in hate speech detection on Viet-\nnamese social networks. Knowledge-Based Systems,\n233:107504.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining\nApproach.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nSon T. Luu, Kiet Van Nguyen, and Ngan Luu-Thuy\nNguyen. 2021.\nA Large-Scale Dataset for Hate\nSpeech Detection on Vietnamese Social Media Texts.\nIn Advances and Trends in Artificial Intelligence.\nArtificial Intelligence Practices, pages 415–426.\nSpringer International Publishing.\nNguyen Minh, Vu Hoang Tran, Vu Hoang, Huy Duc\nTa, Trung Huu Bui, and Steven Quoc Hung Truong.\n2022. ViHealthBERT: Pre-trained language models\nfor Vietnamese in health text mining. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 328–337, Marseille, France.\nEuropean Language Resources Association.\nB. Ngo. 2020. Vietnamese: An Essential Grammar. Es-\nsential grammar. Routledge, Taylor & Francis Group.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhoBERT: Pre-trained language models for Viet-\nnamese. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1037–1042,\nOnline. Association for Computational Linguistics.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model for\nEnglish tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 9–14, On-\nline. Association for Computational Linguistics.\nHuyen TM Nguyen, Hung V Nguyen, Quyen T Ngo, Lu-\nong X Vu, Vu Mai Tran, Bach X Ngo, and Cuong A\nLe. 2018. VLSP Shared Task: Sentiment Analy-\nsis. Journal of Computer Science and Cybernetics,\n34(4):295–310.\nKhang Phuoc-Quy Nguyen and Kiet Van Nguyen. 2020.\nExploiting Vietnamese social media characteristics\nfor textual emotion recognition in Vietnamese. In\n2020 International Conference on Asian Language\nProcessing (IALP), pages 276–281. IEEE.\nLinh The Nguyen and Dat Quoc Nguyen. 2021.\nPhoNLP: A joint multi-task learning model for Viet-\nnamese part-of-speech tagging, named entity recog-\nnition and dependency parsing. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nDemonstrations, pages 1–7.\nLuan Nguyen, Kiet Nguyen, and Ngan Nguyen. 2022.\nSMTCE: A social media text classification evaluation\nbenchmark and BERTology models for Vietnamese.\nIn Proceedings of the 36th Pacific Asia Conference on\nLanguage, Information and Computation, pages 282–\n291, Manila, Philippines. De La Salle University.\nJuan Manuel Pérez, Damián Ariel Furman, Laura\nAlonso Alemany, and Franco M. Luque. 2022.\nRoBERTuito: a pre-trained language model for social\nmedia text in Spanish. In Proceedings of the Thir-\nteenth Language Resources and Evaluation Confer-\nence, pages 7235–7243, Marseille, France. European\nLanguage Resources Association.\nLong Phan, Hieu Tran, Hieu Nguyen, and Trieu H.\nTrinh. 2022.\nViT5: Pretrained text-to-text trans-\nformer for Vietnamese language generation. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies: Student\nResearch Workshop, pages 136–142. Association for\nComputational Linguistics.\nKhanh Quoc Tran, An Trong Nguyen, Phu Gia\nHoang, Canh Duc Luu, Trong-Hop Do, and Kiet\nVan Nguyen. 2023. Vietnamese hate and offensive\ndetection using PhoBERT-CNN and social media\nstreaming data. Neural Computing and Applications,\n35(1):573–594.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and\nDegui Zhi. 2021. Med-BERT: pretrained contextual-\nized embeddings on large-scale structured electronic\nhealth records for disease prediction. NPJ digital\nmedicine, 4(1):86.\nCong Dao Tran, Nhut Huy Pham, Anh-Tuan Nguyen,\nTruong Son Hy, and Tu Vu. 2023. ViDeBERTa: A\npowerful pre-trained language model for Vietnamese.\nIn Findings of the Association for Computational\nLinguistics: EACL 2023, pages 1041–1048.\nNguyen Luong Tran, Duong Minh Le, and Dat Quoc\nNguyen. 2022. BARTpho: Pre-trained Sequence-to-\nSequence Models for Vietnamese. In Proceedings\nof the 23rd Annual Conference of the International\nSpeech Communication Association.\nThi Oanh Tran, Phuong Le Hong, et al. 2020. Im-\nproving sequence tagging for Vietnamese text using\ntransformer-based neural models. In Proceedings of\nthe 34th Pacific Asia conference on language, infor-\nmation and computation, pages 13–20.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and\nDanqi Chen. 2023. Should You Mask 15% in Masked\nLanguage Modeling?\nIn Proceedings of the 17th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, pages 2985–\n3000, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nXinyang Zhang, Yury Malkov, Omar Florez, Serim\nPark, Brian McWilliams, Jiawei Han, and Ahmed El-\nKishky. 2022. TwHIN-BERT: A Socially-Enriched\nPre-trained Language Model for Multilingual Tweet\nRepresentations. arXiv preprint arXiv:2209.07562.\nJosé Ángel González, Lluís-F. Hurtado, and Ferran\nPla. 2021. TWilBert: Pre-trained deep bidirectional\ntransformers for Spanish Twitter. Neurocomputing,\n426:58–69.\nA\nTokenizations of the PLMs on Social Comments\nWe conducted an analysis of average token length by tasks of Pre-trained Language Models to provide\ninsights into how different PLMs perform regarding token length across various Vietnamese social media\ndownstream tasks. Figure 2 shows the average token length by Vietnamese social media downstream tasks\nof baseline PLMs and ours.\nEmotion Regconition\nHate Speech\n Detection\nSentiment Analysis\nSpam Reviews\n Detection\nHate Speech Spans\n Detection\nTasks\n0\n10\n20\n30\n40\nAverage length\nAverage token length by Task\nPLMs\nviBERT\nvELECTRA\nPhoBERT\nmBERT (cased)\nmBERT (uncased)\nXLM-R\nXLM-T\nTwHIN-BERT\nBernice\nViSoBERT (ours)\nFigure 2: Average token length by tasks of PLMs.\nB\nExperimental Settings\nFollowing the hyperparameters in Table 6, we train our pre-trained language model ViSoBERT for\nVietnamese social media texts.\nOptimizer\nAlgorithm\nAdam\nLearning rate\n5e-5\nEpsilon\n1e-8\nLR scheduler\nlinear decay and warmup\nWarmup steps\n1000\nBetas\n0.9 and 0.99\nWeight decay\n0.01\nBatch\nSequence length\n128\nBatch size\n128\nVocab size\n15002\nMisc\nDropout\n0.1\nAttention dropout\n0.1\nTable 6: All hyperparameters established for training ViSoBERT.\nC\nPLMs with Pre-processing Techniques\nFor an in-depth understanding of the impact of social media texts on PLMs, we conducted an analysis of\nthe test results on various processing aspects. Table 7 presents performances of the pre-trained language\nmodels on downstream Vietnamese social media tasks by applying word standardizing pre-processing\ntechniques, while Table 8 presents performances of the pre-trained language models on downstream\nVietnamese social media tasks by removing diacritics in all datasets.\nModel\nEmotion Recognition\nHate Speech Detection\nSentiment Analysis\nSpam Reviews Detection\nHate Speech Spans Detection\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nPhoBERTLarge\n64.94\n64.85\n62.71\n87.68\n87.25\n65.41\n76.80\n76.61\n76.61\n89.47\n89.41\n76.12\n91.73\n91.62\n86.59\n∆\n↑0.23\n↑0.19\n↑0.16\n↑0.36\n↑0.27\n↑0.27\n↑0.28\n↑0.25\n↑0.25\n↓0.65\n↓0.62\n↓0.76\n↑0.29\n↑0.16\n↑0.03\nTwHIN-BERTLarge\n64.42\n64.46\n61.28\n87.82\n87.28\n65.68\n77.17\n76.94\n76.94\n89.49\n89.43\n76.35\n91.74\n91.64\n86.67\n∆\n↑0.21\n↑0.17\n↑0.16\n↑0.59\n↑0.50\n↑0.45\n↑0.25\n↑0.11\n↑0.11\n↓0.98\n↓0.99\n↓0.93\n↑0.29\n↑0.17\n↑0.02\nViSoBERT [♣]\n68.25\n68.52\n65.94\n88.53\n88.33\n68.82\n78.01\n77.88\n77.88\n90.83\n90.75\n78.77\n91.89\n91.82\n86.93\n∆\n↑0.15\n↑0.15\n↑0.06\n↑0.02\n↑0.02\n↑0.08\n↑0.18\n↑0.13\n↑0.13\n↓0.16\n↓0.17\n↓0.29\n↑0.27\n↑0.25\n↑0.13\nViSoBERT [♦]\n68.10\n68.37\n65.88\n88.51\n88.31\n68.74\n77.83\n77.75\n77.75\n90.99\n90.92\n79.06\n91.62\n91.57\n86.80\nTable 7: Performances of the pre-trained language models on downstream Vietnamese social media tasks by\napplying word standardizing pre-processing techniques. [♣] and [♦] denoted with and without standardizing word\ntechnique, respectively. ∆denoted the increase (↑) and the decrease (↓) in performances of the pre-trained language\nmodels compared to its competitors without normalizing teencodes.\nTo emphasize the essentials of diacritics, we conducted an analysis on several data samples by removing\n100%, 75%, 50%, and 25% diacritics of total words that included diacritics in each comment of five\ndownstream tasks. Table 8 presents performances of the pre-trained language models on downstream\nVietnamese social media tasks by removing diacritics in all datasets.\nModel\nEmotion Regconition\nHate Speech Detection\nSentiment Analysis\nSpam Reviews Detection\nHate Speech Spans Detection\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nRemoving 100% diacritics in each comment\nPhoBERTLarge\n49.35\n49.18\n43.95\n81.25\n81.42\n55.43\n62.38\n62.36\n62.36\n87.68\n87.56\n71.89\n91.32\n91.37\n86.43\n∆\n↓15.36\n↓15.48\n↓18.60\n↓6.07\n↓5.56\n↓9.71\n↓14.14\n↓14.00\n↓13.86\n↓2.44\n↓2.47\n↓4.99\n↓0.12\n↓0.09\n↓0.13\nTwHIN-BERTLarge\n49.32\n49.15\n43.52\n84.25\n78.48\n51.32\n66.66\n66.68\n66.68\n89.45\n89.26\n74.59\n91.12\n91.22\n86.33\n∆\n↓14.89\n↓15.14\n↓17.60\n↓2.98\n↓8.30\n↓12.99\n↓10.26\n↓10.15\n↓10.15\n↓1.02\n↓1.16\n↓2.69\n↓0.33\n↓0.25\n↓0.32\nViSoBERT [♣]\n61.96\n62.05\n58.48\n87.29\n86.76\n64.87\n72.95\n72.91\n72.91\n89.75\n89.72\n76.12\n91.48\n91.42\n86.69\n∆\n↓6.14\n↓6.32\n↓7.40\n↓1.22\n↓1.55\n↓3.90\n↓4.88\n↓4.84\n↓4.84\n↓1.24\n↓1.20\n↓2.94\n↓0.14\n↓0.15\n↓0.11\nRemoving 75% diacritics in each comment\nPhoBERTLarge\n51.94\n51.79\n47.79\n84.74\n84.03\n58.37\n66.00\n65.98\n65.98\n88.23\n88.12\n72.42\n90.38\n90.23\n85.42\n∆\n↓12.77\n↓12.87\n↓14.76\n↓2.58\n↓2.95\n↓6.77\n↓10.52\n↓10.38\n↓10.24\n↓1.89\n↓1.91\n↓4.46\n↓1.06\n↓1.23\n↓1.14\nTwHIN-BERTLarge\n51.32\n51.17\n44.63\n83.22\n81.42\n52.24\n67.23\n67.32\n67.32\n89.12\n88.95\n75.20\n90.62\n89.93\n85.81\n∆\n↓12.89\n↓13.12\n↓16.49\n↓4.01\n↓5.36\n↓12.99\n↓9.69\n↓9.51\n↓9.51\n↓1.35\n↓1.47\n↓2.08\n↓0.83\n↓1.54\n↓0.84\nViSoBERT [♠]\n62.34\n62.26\n58.13\n87.35\n86.88\n65.12\n73.90\n73.97\n73.97\n90.41\n90.31\n76.17\n91.02\n91.17\n86.02\n∆\n↓5.76\n↓6.11\n↓7.75\n↓1.16\n↓1.43\n↓3.65\n↓3.93\n↓3.78\n↓3.78\n↓0.58\n↓0.61\n↓2.89\n↓0.60\n↓0.40\n↓0.78\nRemoving 50% diacritics in each comment\nPhoBERTLarge\n57.28\n57.36\n54.02\n85.29\n84.71\n59.40\n66.57\n66.46\n66.46\n89.02\n88.81\n73.10\n90.42\n90.47\n85.62\n∆\n↓7.43\n↓7.30\n↓8.53\n↓2.03\n↓2.27\n↓5.74\n↓9.95\n↓9.90\n↓9.76\n↓1.10\n↓1.22\n↓3.78\n↓1.02\n↓0.99\n↓0.94\nTwHIN-BERTLarge\n53.70\n53.39\n49.55\n83.41\n83.31\n55.22\n70.42\n70.53\n70.53\n89.33\n89.05\n75.32\n90.73\n90.12\n85.92\n∆\n↓10.51\n↓10.90\n↓11.57\n↓3.82\n↓3.47\n↓10.01\n↓6.50\n↓6.30\n↓6.30\n↓1.14\n↓1.37\n↓1.96\n↓0.72\n↓1.35\n↓0.73\nViSoBERT [♥]\n62.96\n62.87\n60.55\n87.44\n87.10\n65.25\n74.76\n74.72\n74.72\n90.41\n90.35\n77.31\n91.12\n91.24\n86.22\n∆\n↓5.14\n↓5.50\n↓5.33\n↓1.07\n↓1.21\n↓3.52\n↓3.07\n↓3.03\n↓3.03\n↓0.58\n↓0.57\n↓1.75\n↓0.50\n↓0.33\n↓0.58\nRemoving 25% diacritics in each comment\nPhoBERTLarge\n61.03\n60.80\n57.87\n85.97\n85.51\n61.96\n73.42\n73.28\n73.28\n89.80\n89.59\n75.53\n90.63\n90.69\n85.76\n∆\n↓3.68\n↓3.86\n↓4.68\n↓1.35\n↓1.47\n↓3.18\n↓3.10\n↓3.08\n↓2.94\n↓0.32\n↓0.44\n↓1.35\n↓0.81\n↓0.77\n↓0.80\nTwHIN-BERTLarge\n61.18\n60.98\n57.42\n86.85\n86.13\n63.14\n73.21\n73.11\n73.11\n89.91\n89.43\n76.32\n91.09\n90.72\n86.02\n∆\n↓3.03\n↓3.31\n↓3.70\n↓0.38\n↓0.65\n↓2.09\n↓3.71\n↓3.72\n↓3.72\n↓0.56\n↓0.99\n↓0.96\n↓0.36\n↓0.75\n↓0.63\nViSoBERT [✠]\n64.64\n64.53\n61.29\n87.85\n87.56\n66.54\n75.42\n75.44\n75.44\n90.76\n90.64\n78.15\n91.22\n91.24\n86.47\n∆\n↓3.43\n↓3.84\n↓4.59\n↓0.66\n↓0.75\n↓2.23\n↓2.41\n↓2.31\n↓2.31\n↓0.23\n↓0.28\n↓0.91\n↓0.40\n↓0.33\n↓0.33\nViSoBERT [♦]\n68.10\n68.37\n65.88\n88.51\n88.31\n68.77\n77.83\n77.75\n77.75\n90.99\n90.92\n79.06\n91.62\n91.57\n86.80\nTable 8: Performances of the pre-trained language models on downstream Vietnamese social media tasks by\nremoving diacritics in all datasets. [♣], [♠], [♥], [✠] and [♦] denoted the performances of our pre-trained\non removing 100%, 75%, 50%, 25% in each comment, respectively, and not removing diacritics marks dataset,\nrespectively. ∆denoted the increase (↑) and the decrease (↓) in performances of the pre-trained language models\ncompared to its competitors without removing diacritics marks.\nD\nPLM-based Features for BiLSTM and BiGRU\nWe conduct experiments with various models, including BiLSTM and BiGRU, to understand better the\nword embedding feature extracted from the pre-trained language models. Table 9 shows performances\nof the pre-trained language model as input features to BiLSTM and BiGRU on downstream Vietnamese\nsocial media tasks.\nModel\nEmotion Regconition\nHate Speech Detection\nSentiment Analysis\nSpam Reviews Detection\nHate Speech Spans Detection\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nAcc\nWF1\nMF1\nBiLSTM\nPhoBERTLarge\n57.58\n56.65\n50.55\n86.11\n84.04\n56.03\n69.71\n69.70\n69.70\n87.80\n87.10\n68.95\n84.01\n80.70\n74.35\nTwHIN-BERTLarge\n61.47\n61.31\n56.73\n83.14\n82.72\n55.84\n64.76\n64.82\n64.82\n88.73\n88.23\n72.18\n85.92\n84.43\n78.28\nViSoBERT\n63.06\n62.36\n59.16\n87.62\n86.81\n64.82\n73.52\n73.50\n73.50\n90.11\n89.79\n75.71\n88.37\n87.87\n82.18\nBiGRU\nPhoBERTLarge\n55.12\n54.53\n49.59\n85.21\n83.23\n54.59\n70.01\n70.01\n70.01\n86.06\n84.89\n62.54\n84.23\n81.01\n74.57\nTwHIN-BERTLarge\n60.46\n60.30\n55.23\n85.73\n83.45\n54.74\n63.11\n61.39\n61.39\n87.67\n86.38\n66.83\n86.10\n84.52\n78.49\nViSoBERT\n63.20\n63.25\n60.73\n87.02\n86.25\n63.36\n70.48\n70.53\n70.53\n89.33\n88.98\n76.57\n88.88\n88.19\n82.63\nTable 9: Performances of the pre-trained language models as input features to BiLSTM and BiGRU on downstream\nVietnamese social media tasks.\nWe implemented various PLMs when used as input features in combination with BiLSTM and BiGRU\nmodels to verify the ability to extract Vietnamese social media texts. The evaluation is conducted on the\ndev set, and the performance is measured per epoch for downstream tasks. Table 9 shows performances of\nthe PLMs as input features to BiLSTM and BiGRU on the dev set per epoch.\n2\n4\n6\n8\n10\nEpoch\n10\n20\n30\n40\n50\nMF1\n(a) Emotion Regconition\n2\n4\n6\n8\n10\nEpoch\n30\n40\n50\n60\nMF1\n(b) Hate Speech Detection\n2\n4\n6\n8\n10\nEpoch\n30\n40\n50\n60\n70\nMF1\n(c) Sentiment Analysis\n2\n4\n6\n8\n10\nEpoch\n40\n50\n60\n70\nMF1\n(d) Spam Reviews Detection\n2\n4\n6\n8\n10\nEpoch\n70\n75\n80\n85\nMF1\n(e) Hate Speech Spans Detection\nViSoBERT-BiLSTM\nPhoBERT-BiLSTM\nTWHIN-BiLSTM\nViSoBERT-BiGRU\nPhoBERT-BiGRU\nTWHIN-BiGRU\nFigure 3: Performances of the PLMs as input features to BiLSTM and BiGRU on the dev set per epoch on\nVietnamese social media downstream tasks. Large versions of PhoBERT and TwHIN-BERT are implemented for\nthese experiments.\nE\nUpdating New Spans of Hate Speech Span Detection Samples with Pre-processing\nTechniques\nDue to performing pre-processing techniques, the span positions on the data samples can be changed.\nTherefore, we present Algorithm 1, which shows how to update new span positions of samples applied\nwith pre-processing techniques in the Hate Speech Spans Detection task (UIT-ViHOS dataset). This\nalgorithm takes as input a comment and its spans and returns the pre-processed comment and its span\nalong with pre-processing techniques.\nAlgorithm 1 Updating new spans of samples applied with pre-processing techniques in Hate Speech\nSpans Detection task (UIT-ViHOS dataset).\n1: procedure ALGORITHM(comment, label, delete)\n2:\nassert len(comment) == len(label)\n3:\nnew_comment ←[], new_label ←[]\n4:\nfor i ←0 to len(comment) do\n5:\ncheck ←0\n6:\nif comment[i] in emoji_to_word.keys() then\n7:\nif delete then\n8:\ncontinue\n9:\nfor j ←0 to len(emoji_to_word[comment[i]].split(‘ ’)) do\n10:\nif label[i] == ‘B-T’ then\n11:\nif check == 0 then\n12:\ncheck ←check + 1, new_label.append(label[i])\n13:\nelse\n14:\nnew_label.append(‘I-T’)\n15:\nelse\n16:\nnew_label.append(label[i])\n17:\nnew_comment.append(emoji_to_word[comment[i]].split(‘ ’)[j])\n18:\nelse\n19:\nnew_sentence.append(comment[i])\n20:\nnew_label.append(label[i])\n21:\nassert len(new_comment) == len(new_label)\n22:\nreturn new_comment, new_label\nF\nTokenizations of the PLMs on Removing Diacritics Social Comments\nWe analyze several data samples to see the tokenized ability of Vietnamese social media textual data while\nremoving diacritics on comments. Table 10 shows several non-diacritics Vietnamese social comments and\ntheir tokenizations with the tokenizers of the three best pre-trained language models, ViSoBERT (ours),\nPhoBERT, and TwHIN-BERT.\nModel\nExample 1\nExample 2\nRaw comment\ncái con đồchơi đó mua ởđâu nhỉ. cười đéo nhặt được\nmồm\nEnglish: where did you buy that toy . LMAO\nÔi bốcái lũ thanh niên hãm lol. Đẹp mặt quá\nEnglish: Oh my god damn teenagers, lol. So deserved\nRemoving 100% diacritics in each comment\nComment\ncai con do choi do mua o dau nhi . cuoi deo nhat duoc\nmom .\nOi bo cai lu thanh nien ham lol. Dep mat qua\nPhoBERT\n<s>, \"c a i\", \"c o n\", \"d o\", \"c h o @ @\", \"i\", \"d o\", \"m u a\",\n\"o\", \"d @ @\", \"a u\", \"n h i\", \".\", \"c u @ @\", \"o i\",\n\"d @ @\", \"e o\", \"n h @ @\", \"a t\", \"d u @ @\", \"o c\",\n\"m o m\", \".\", <unk>, <unk>, <unk>, </s>\n<s>, \"O @ @\", \"i\", \"b o\", \"c a i\", \"l u\", \"t h a n h\", \"n i @ @\",\n\"e n\", \"h a m\", \"l o @ @\", \"l\", \".\", \"D e @ @\", \"p\", \"m a t\",\n\"q u a\", <unk>, <unk>, </s>\nTwHIN-BERT\n<s>, \"cai\", \"con\", \"do\", \"cho\", \"i\", \"do\", \"mua\", \"o\", \"dau\",\n\"nhi\", \"\", \".\", \"cu\", \"oi\", \"de\", \"o\", \"nha\", \"t\", \"du\", \"oc\",\n\"mom\", \"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>\n<s>, \"Oi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"nie\", \"n\", \"ham\", \"lol\",\n\".\", \"De\", \"p\", \"mat\", \"qua\", \"\", \"\n\", \"\n\", </s>\nViSoBERT\n<s>, \"cai\", \"con\", \"do\", \"choi\", \"do\", \"mua\", \"o\", \"dau\", \"nhi\",\n\".\",\"cu\", \"oi\", \"d\", \"eo\", \"nhat\", \"duoc\", \"m\", \"om\", \".\",\n\"\n\", </s>\n<s>, \"O\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"ni\", \"en\", \"h\", \"am\",\n\"lol\", \".\", \"D\", \"ep\", \"mat\", \"qua\", \"\n\", </s>\nRemoving 75% diacritics in each comment\nComment\ncai con do chơi do mua o đâu nhi . cười deo nhat duoc\nmom .\nÔi bo cai lu thanh niên hãm lol. Dep mat qua\nPhoBERT\n<s>, \"c a i\", \"c o n\", \"d o\", \"c h ơ i\", \"d o\", \"m u a\", \"o\",\n\"đ â u\", \"n h i\", \".\", \"c ư ời\", \"d @ @\", \"e o\", \"n h @ @\",\n\"a t\", \"d u @ @\", \"o c\", \"m o m\", \".\",<unk>, <unk>,\n<unk>, </s>\n<s>, \"Ô i\", \"b o\", \"c a i\", \"l u\", \"t h a n h _ n i ê n\", \"h ã m\",\n\"l o @ @\", \"l\", \".\", \"D e @ @\", \"p\", \"m a t\", \"q u a\", <unk>,\n<unk>, </s>\nTwHIN-BERT\n<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhi\", \"\", \".\", \"cười\", \"de\", \"o\", \"nha\", \"t\", \"du\", \"oc\",\n\"mom\", \"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>\n<s>, \"Ô\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"niên\", \"\", \"hã\", \"m\",\n\"lol\", \".\", \"De\", \"p\", \"mat\", \"qua\", \"\", \"\n\", \"\n\", </s>\nViSoBERT\n<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhi\", \".\", \"cười\", \"d\", \"eo\", \"nhat\", \"duoc\", \"m\", \"om\",\n\".\", \"\n\", </s>\n<s>, \"Ôi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"n\", \"iên\", \"hã\", \"m\",\n\"lol\", \".\", \"D\", \"ep\", \"mat\", \"qua\", \"\n\", </s>\nRemoving 50% diacritics in each comment\nComment\ncai con do chơi do mua o đâu nhỉ. cười đéo nhặt duoc\nmom .\nÔi bo cai lu thanh niên hãm lol. Dep mặt quá\nPhoBERT\n<s>, \"c a i\", \"c o n\", \"d o\", \"c h ơ i\", \"d o\", \"m u a\", \"o\",\n\"đ â u\", \"n h ỉ\", \".\", \"c ư ời\", \"đ @ @\", \"é o\", \"n h ặt\",\n\"d u @ @\", \"o c\", \"m o m\", \".\", <unk>, <unk>, <unk>, </s>\n<s>, \"Ô i\", \"b o\", \"c a i\", \"l u\", \"t h a n h _ n i ê n\", \"h ã m\",\n\"l o @ @\", \"l\", \".\", \"D e @ @\", \"p\", \"m ặt\", \"q u á\", <unk>,\n<unk>, </s>\nTwHIN-BERT\n<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\", \"nhỉ\",\n\"\", \".\", \"cười\", \"đ\", \"é\", \"o\", \"nh\", \"ặt\", \"du\", \"oc\", \"mom\",\n\"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>\n<s>, \"Ô\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"niên\", \"\", \"hã\", \"m\",\n\"lol\", \".\", \"De\", \"p\", \"mặt\", \"quá\", \"\", \"\n\", \"\n\", </s>\nViSoBERT\n<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhỉ\", \".\", \"cười\", \"đéo\", \"nh\", \"ặt\", \"duoc\", \"m\", \"om\",\n\".\", \"\n\", </s>\n<s>, \"Ôi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"n\", \"iên\", \"hã\", \"m\",\n\"lol\", \".\", \"D\", \"ep\", \"mặt\", \"quá\", \"\n\", </s>\nRemoving 25% diacritics in each comment\nComment\ncai con do chơi đó mua ởđâu nhỉ. cười đéo nhặt duoc\nmồm .\nÔi bo cai lu thanh niên hãm lol. Đep mặt quá\nPhoBERT\n<s>, \"c a i\", \"c o n\", \"d o\", \"c h ơ i\", \"đ ó\", \"m u a\", \"ở\",\n\"đ â u\", \"n h ỉ\", \".\", \"c ư ời\", \"đ @ @\", \"é o\", \"n h ặt\",\n\"d u @ @\", \"o c\", \"m ồm\", \".\", <unk>, <unk>, <unk>, </s>\n<s>, \"Ô i\", \"b o\", \"c a i\", \"l u\", \"t h a n h _ n i ê n\", \"h ã m\",\n\"l o @ @\", \"l\", \".\", \"Đ e p _ @ @\", \"m ặt\", \"q u á\", <unk>,\n<unk>, </s>\nTwHIN-BERT\n<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhỉ\", \"\", \".\", \"cười\", \"đ\", \"é\", \"o\", \"nh\", \"ặt\", \"du\", \"oc\",\n\"mom\", \"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>\n<s>, \"Ô\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"niên\", \"\", \"hã\", \"m\",\n\"lol\", \".\", \"Đep\", \"mặt\", \"quá\", \"\", \"\n\", \"\n\", </s>\nViSoBERT\n<s>, \"cai\", \"con\", \"do\", \"chơi\", \"đó\", \"mua\", \"ở\", \"đâu\",\n\"nhỉ\", \".\", \"cười\", \"đéo\", \"nh\", \"ặt\", \"duoc\", \"mồm\", \".\",\n\"\n\", </s>\n<s>, \"Ôi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"n\", \"iên\", \"hã\", \"m\",\n\"lol\", \".\", \"Đep\", \"mặt\", \"quá\", \"\n\", </s>\nTable 10: Actual social comments and their tokenizations with the tokenizers of the three pre-trained language\nmodels, including PhoBERT, TwHIN-BERT, and ViSoBERT, on removing diacritics of social comments.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-10-17",
  "updated": "2023-10-28"
}