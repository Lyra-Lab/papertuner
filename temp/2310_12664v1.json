{
  "id": "http://arxiv.org/abs/2310.12664v1",
  "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing",
  "authors": [
    "Yue Guo",
    "Zian Xu",
    "Yi Yang"
  ],
  "abstract": "The emergence of Large Language Models (LLMs), such as ChatGPT, has\nrevolutionized general natural language preprocessing (NLP) tasks. However,\ntheir expertise in the financial domain lacks a comprehensive evaluation. To\nassess the ability of LLMs to solve financial NLP tasks, we present FinLMEval,\na framework for Financial Language Model Evaluation, comprising nine datasets\ndesigned to evaluate the performance of language models. This study compares\nthe performance of encoder-only language models and the decoder-only language\nmodels. Our findings reveal that while some decoder-only LLMs demonstrate\nnotable performance across most financial tasks via zero-shot prompting, they\ngenerally lag behind the fine-tuned expert models, especially when dealing with\nproprietary datasets. We hope this study provides foundation evaluations for\ncontinuing efforts to build more advanced LLMs in the financial domain.",
  "text": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial\nNatural Language Processing\nYue Guo\nZian Xu\nYi Yang\nThe Hong Kong University of Science and Technology\nyguoar@connect.ust.hk\nzxubz@connect.ust.hk\nimyiyang@ust.hk\nAbstract\nThe emergence of Large Language Models\n(LLMs), such as ChatGPT, has revolutionized\ngeneral natural language preprocessing (NLP)\ntasks. However, their expertise in the finan-\ncial domain lacks a comprehensive evalua-\ntion. To assess the ability of LLMs to solve\nfinancial NLP tasks, we present FinLMEval,\na framework for Financial Language Model\nEvaluation, comprising nine datasets designed\nto evaluate the performance of language mod-\nels. This study compares the performance of\nencoder-only language models and the decoder-\nonly language models. Our findings reveal that\nwhile some decoder-only LLMs demonstrate\nnotable performance across most financial tasks\nvia zero-shot prompting, they generally lag be-\nhind the fine-tuned expert models, especially\nwhen dealing with proprietary datasets. We\nhope this study provides foundation evaluations\nfor continuing efforts to build more advanced\nLLMs in the financial domain.\n1\nIntroduction\nRecent progress in natural language processing\n(NLP) demonstrates that large language models\n(LLMs), like ChatGPT, achieve impressive results\non various general domain NLP tasks.\nThose\nLLMs are generally trained by first conducting\nself-supervised training on the unlabeled text (Rad-\nford et al., 2019; Brown et al., 2020; Touvron\net al., 2023a) and then conducting instruction tun-\ning (Wang et al., 2023; Taori et al., 2023) or rein-\nforcement learning from human feedback (RLHF)\n(Ouyang et al., 2022) to let them perform tasks\nfollowing human instructions.\nFinancial NLP, in contrast, demands specialized\nknowledge and specific reasoning skills to tackle\ntasks within the financial domain. However, for\ngeneral language models like ChatGPT, their self-\nsupervised training is performed on the text from\nvarious domains, and the reinforcement learning\nfeedback they receive is generated by non-expert\nworkers. Therefore, how much essential knowledge\nand skills are acquired during the learning process\nremains uncertain. As a result, a comprehensive\ninvestigation is necessary to assess its performance\non financial NLP tasks.\nTo fill this research gap, we are motivated to\nevaluate language models on financial tasks com-\nprehensively. For doing so, we propose a frame-\nwork for Financial Language Model Evaluation\n(FinLMEval). We collected nine datasets on fi-\nnancial tasks, five from public datasets evaluated\nbefore. However, for those public datasets, it is\npossible that their test sets are leaked during the\ntraining process or provided by the model users\nas online feedback. To eliminate this issue, We\nused four proprietary datasets on different financial\ntasks: financial sentiment classification (FinSent),\nenvironmental, social, and corporate governance\nclassification (ESG), forward-looking statements\nclassification (FLS), and question-answering clas-\nsification (QA) for evaluation.\nIn the evaluation benchmark, we evaluate the\nencoder-only language models with supervised\nfine-tuning, with representatives of BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), FinBERT\n(Yang et al., 2020) and FLANG (Shah et al., 2022).\nWe then compare the encoder-only models with the\ndecoder-only models, with representatives of Chat-\nGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023),\nPIXIU (Xie et al., 2023), LLAMA2-7B (Touvron\net al., 2023b) and Bloomberg-GPT (Wu et al., 2023)\nby zero-shot prompting. Besides, we evaluate the\nefficacy of in-context learning of ChatGPT with\ndifferent in-context sample selection strategies.\nExperiment results show that (1) the fine-tuned\ntask-specific encoder-only model generally per-\nforms better than decoder-only models on the fi-\nnancial tasks, even if decoder-only models have\nmuch larger model size and have gone through\nmore pre-training and instruction tuning or RLHF;\n(2) when the supervised data is insufficient, the\narXiv:2310.12664v1  [cs.CL]  19 Oct 2023\nGet response with Prompt:\nPerform financial sentiment classification. \nClassify the following sentence into \n'neutral',' positive', or 'negative' class. \nOnly provide the label in the output. \n{placeholder}\nThe sentence: Energy-\nRelated Investments \nreported a 277% \nincrease in quarterly \nearnings. The label: \nHere are some examples: \nThe sentence: {example 1}\nThe label: {label 1} …\nThe sentence: {example n}\nThe label: {label n}\nThe sentence: Energy-\nRelated Investments \nreported a 277% increase in \nquarterly earnings.\nThe label: \nEvaluating Encoder-only Models\nExpected output: positive\nEnergy-Related Investments reported a 277% \nincrease in quarterly earnings. \npositive\nFine-tune the parameters of the \nencoder-only models with the training \nsets to output the correct label.\nVS\nEvaluating Decoder-only Models\nZero-shot Learning\nIn-context Learning\nFinancial Tasks\nSentiment\nForward-looking \nstatements\nEnvironmental, social,\nand governance\nMonetary \npolicy\nNews\nHeadlines\nNER\nQA\nFinLMEval Framework\nFigure 1: The framework of financial language model evaluation (FinLMEval).\nzero-shot decoder-only models have more advan-\ntages than fine-tuned encoder-only models; (3) the\nperformance gap between fine-tuned encoder-only\nmodels and zero-shot decoder-only models is more\nsignificant on private datasets than the publicly\navailable datasets; (4) in-context learning is only\neffective under certain circumstances.\nTo summarize, we propose an evaluation frame-\nwork for financial language models. Compared to\nprevious benchmarks in the financial domain like\nFLUE (Shah et al., 2022), our evaluation includes\nfour new datasets and involves more advanced\nLLMs like ChatGPT. We show that even the most\nadvanced LLMs still fall behind the fine-tuned ex-\npert models. We hope this study contributes to the\ncontinuing efforts to build more advanced LLMs\nin the financial domain.\n2\nRelated Works\nThe utilization of language models in financial NLP\nis a thriving research area. While some general do-\nmain language models, like BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), GPT (Brown\net al., 2020; OpenAI, 2023) and LLAMA (Tou-\nvron et al., 2023a,b) have been applied to finan-\ncial NLP tasks, financial domain models like Fin-\nBERT (Araci, 2019; Yang et al., 2020; Huang et al.,\n2023), FLANG (Shah et al., 2022), PIXIU (Xie\net al., 2023), InvestLM (Yang et al., 2023) and\nBloombergGPT (Wu et al., 2023) are specifically\ndesigned to contain domain expertise and generally\nperform better in financial tasks. Recent work such\nas FLUE (Shah et al., 2022) has been introduced to\nbenchmark those language models in the finance\ndomain. However, the capability of more advanced\nLLMs, like ChatGPT and GPT-4, has not been\nbenchmarked, especially on proprietary datasets.\nIn this work, in addition to the public tasks used\nin FLUE, we newly include four proprietary tasks\nin FinLMEval and conduct comprehensive evalua-\ntions for those financial language models.\n3\nMethods\nWe compare two types of models in FinLMEval:\nthe Transformers encoder-only models that require\nfine-tuning on the labeled dataset, and decoder-only\nmodels that are prompted with zero-shot or few-\nshot in-context instructions. Figure 1 provides an\noutline of evaluation methods of FinLMEval.\n3.1\nEncoder-only Models\nOur experiments explore the performance of vari-\nous notable encoder-only models: BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), FinBERT\n(Yang et al., 2020) and FLANG (Shah et al., 2022).\nBERT and RoBERTa are pre-trained on general\ndomain corpora, while FinBERT and FLANG are\npre-trained on a substantial financial domain cor-\npus. We fine-tune the language models on specific\ntasks. Following the fine-tuning process, inference\ncan be performed on the fine-tuned models for spe-\ncific applications.\n# train\n# test\nsource\ndescription\nFinSent\n8996\n1000\n-\nFinancial sentiment classification dataset from analyst reports.\nFPB\n2453\n1000\n(Malo et al., 2014)\nSentiment classification dataset from financial news.\nFiQA SA\n973\n200\n(FiQA)\nAspect-based financial sentiment analysis.\nESG\n3000\n1000\n-\nEnvironmental, social, and corporate governance classification dataset.\nFLS\n2600\n1000\n-\nForward-looking statements classification dataset from corporate reports.\nQA\n868\n200\n-\nClassification on the validity of question-answering pairs.\nHeadlines\n9570\n1000\n(Sinha and Khandait, 2020)\nMulitple tasks classification dataset from news headlines.\nNER\n14041\n1000\n(Alvarado et al., 2015)\nNamed entity recognition on financial agreements.\nFOMC\n1831\n450\n(Shah et al., 2023)\nHawkish-dovish monetary policy classification from FOMC documents.\nTable 1: The summarization of nine datasets in FinLMEval. FPB, FiQA SA, Headlines, NER and FOMC are from\npublic datasets, and FinSent, ESG, FLS and QA are newly collected and not released before.\n3.2\nDecoder-only Models\nWe also evaluate the performance of various pop-\nular decoder-only language models:\nChatGPT\n(Ouyang et al., 2022), GPT-4 (OpenAI, 2023),\nPIXIU (Xie et al., 2023), LLAMA2-7B (Touvron\net al., 2023b) and Bloomberg-GPT (Wu et al.,\n2023). ChatGPT and GPT-4, developed by Ope-\nnAI, are two advanced LLMs that showcase ex-\nceptional language understanding and generation\nabilities. The models are pre-trained on a wide\narray of textual data and reinforced by human feed-\nback. PIXIU is a financial LLM based on fine-\ntuning LLAMA (Touvron et al., 2023a) with in-\nstruction data. LLAMA2 is a popular open-sourced\nLLM pre-trained on extensive online data, and\nBloombergGPT is an LLM for finance trained on\na wide range of financial data. As the model size\nof the evaluated decoder-only models is extremely\nlarge, they usually do not require fine-tuning the\nwhole model on downstream tasks. Instead, the\ndecoder-only models provide answers via zero-shot\nand few-shot in-context prompting.\nWe conduct zero-shot prompting for all decoder-\nonly models. We manually write the prompts for\nevery task. An example of prompts for the senti-\nment classification task is provided in Figure 1, and\nthe manual prompts for other tasks are provided\nin Appendix A. Furthermore, to evaluate whether\nfew-shot in-context learning can improve the model\nperformance, we also conduct in-context learning\nexperiments on ChatGPT. We use two strategies\nto select the in-context examples for few-shot in-\ncontext learning: random and similar. The former\nstrategy refers to random selection, and the lat-\nter selects the most similar sentence regarding the\nquery sentence. All in-context examples are se-\nlected from the training set, and one example is\nprovided from each label class.\n4\nDatasets\nOur evaluation relies on nine datasets designed\nto evaluate the financial expertise of the models\nfrom diverse perspectives. Table 1 overviews the\nnumber of training and testing samples and the\nsource information for each dataset. Below, we\nprovide an introduction to each of the nine datasets.\nFinSent is a newly collected sentiment classifica-\ntion dataset containing 10,000 manually annotated\nsentences from analyst reports of S&P 500 firms.\nFPB Sentiment Classification (Malo et al.,\n2014) is a classic sentiment dataset of sentences\nfrom financial news. The dataset consists of 4840\nsentences divided by the agreement rate of 5-8 an-\nnotators. We use the subset of 75% agreement.\nFiQA SA (FiQA) is a aspect-based financial sen-\ntiment analysis dataset. Following the \"Sentences\nfor QA-M\" method in (Sun et al., 2019), for each\n(sentence, target, aspect) pair, we transform the\nsentence into the form \"what do you think of the\n{aspect} of {target}? {sentence}\" for classification.\nESG evaluates an organization’s considerations\non environmental, social, and corporate gover-\nnance. We collected 2,000 manually annotated\nsentences from firms’ ESG reports and annual re-\nports.\nFLS, the forward-looking statements, are beliefs\nand opinions about a firm’s future events or re-\nsults. FLS dataset, aiming to classify whether a\nsentence contains forward-looking statements, con-\ntains 3,500 manually annotated sentences from the\nManagement Discussion and Analysis section of\nannual reports of Russell 3000 firms.\nQA contains question-answering pairs extracted\nfrom earnings conference call transcripts. The goal\nof the dataset is to identify whether the answer is\nvalid to the question.\nHeadlines (Sinha and Khandait, 2020) is a\ndataset for the commodity market that analyzes\nDatasets\nEncoder-only Models\nDecoder-only Models\nBERT\nRoBERTa\nFinBERT\nFLANG-\nBERT\nChatGPT\nGPT-4\nPIXIU\nLLAMA2-\n7B\nBloomberg-\nGPT\nFinSent\n0.841\n0.871\n0.851\n0.849\n0.782\n0.809\n0.800\n0.243\n-\nFPB\n0.914\n0.934\n0.912\n0.881\n0.869\n0.905\n0.965\n0.339\n0.511\nFiQA SA\n0.750\n0.875\n0.805\n0.695\n0.898\n0.920\n0.930\n0.480\n0.751\nESG\n0.931\n0.956\n0.958\n0.925\n0.477\n0.626\n0.509\n0.209\n-\nFLS\n0.875\n0.862\n0.882\n0.861\n0.652\n0.565\n0.275\n0.365\n-\nQA\n0.865\n0.825\n0.825\n0.785\n0.695\n0.775\n0.680\n0.625\n-\nHeadlines-PDU\n0.937\n0.947\n0.956\n0.940\n0.889\n0.878\n0.842\n0.411\n-\nHeadlines-PDC\n0.978\n0.979\n0.981\n0.978\n0.936\n0.947\n0.702\n0.053\n-\nHeadlines-PDD\n0.954\n0.961\n0.960\n0.956\n0.896\n0.900\n0.763\n0.382\n-\nHeadlines-PI\n0.974\n0.964\n0.976\n0.977\n0.225\n0.105\n0.753\n0.966\n-\nHeadlines-AC\n0.996\n0.993\n0.997\n0.995\n0.806\n0.838\n0.902\n0.346\n-\nHeadlines-FI\n0.976\n0.964\n0.976\n0.974\n0.711\n0.780\n0.981\n0.048\n-\nHeadlines-PS\n0.905\n0.918\n0.924\n0.906\n0.630\n0.811\n0.776\n0.546\n-\nNER\n0.980\n0.981\n0.964\n0.978\n0.748\n0.707\n0.749\n0.714\n0.608\nFOMC\n0.587\n0.611\n0.602\n0.602\n0.633\n0.729\n0.522\n0.349\n-\nAverage\n0.897\n0.909\n0.905\n0.907\n0.723\n0.753\n0.739\n0.405\n-\nTable 2: The results of fine-tuned encoder-only models and zero-shot decoder-only models in 9 financial datasets.\nThe results, except the NER dataset, are measured in micro-F1 score. NER is measured in accuracy. Although some\nzero-shot decoder-only models can achieve considerate results in most cases, the fine-tuned encoder-only models\nusually perform better than decoder-only models.\nnews headlines across multiple dimensions. The\ntasks include the classifications of Price Direction\nUp (PDU), Price Direction Constant (PDC), Price\nDirection Down (PDD), Asset Comparison(AC),\nPast Information (PI), Future Information (FI), and\nPrice Sentiment (PS).\nNER (Alvarado et al., 2015) is a named entity\nrecognition dataset of financial agreements.\nFOMC (Shah et al., 2023) aims to classify the\nstance for the FOMC documents into the tightening\nor the easing of the monetary policy.\nAmong the datasets, FinSent, ESG, FLS, and\nQA are newly collected proprietary datasets.\n5\nExperiments\nThis section introduces the experiment setups and\nreports the evaluation results.\n5.1\nModel Setups\nEncoder-only models setups. We use the BERT\n(base,uncased), RoBERTa (base), FinBERT (pre-\ntrain), and FLANG-BERT from Huggingface1, and\nthe model fine-tuning is implemented via Trainer 2.\nFor all tasks, we fix the learning rate as 2 × 10−5,\nweight decay as 0.01, and the batch size as 48. We\nrandomly select 10% examples from the training\nset as the validation set for model selection and\n1https://huggingface.co/\n2https://huggingface.co/docs/transformers/\nmain_classes/trainer\nfine-tune the model for three epochs. Other hyper-\nparameters remain the default in Trainer.\nDecoder-only models setups. In the zero-shot\nsetting, for ChatGPT and GPT-4, We use the \"gpt-\n3.5-turbo\" and \"gpt-4\" model API from OpenAI,\nrespectively. We set the temperature and top_p\nas 1, and other hyperparameters default by Ope-\nnAI API. The ChatGPT results are retrieved from\nthe May 2023 version, and the GPT-4 results\nare retrieved in August 2023.\nFor PIXIU and\nLLAMA2, we use the \"ChanceFocus/finma-7b-\nnlp\" and \"meta-llama/Llama-2-7b\" models from\nHuggingface. The model responses are generated\ngreedily. All prompts we used in the zero-shot\nsetting are shown in Appendix A. Besides, as the\nBloombergGPT (Wu et al., 2023) is not publicly\navailable, we directly adopt the results from the\noriginal paper.\nFor in-context learning, we conduct two strate-\ngies for in-context sample selection: random and\nsimilar. We select one example from each label\nwith equal probability weighting for random sam-\nple selection. For similar sample selection, we get\nthe sentence embeddings by SentenceTransformer\n(Reimers and Gurevych, 2019) \"all-MiniLM-L6-\nv2\" model3 and use cosine similarity as the measure\nof similarity. Then, we select the sentences with\nthe highest similarity with the query sentence as the\n3https://www.sbert.net/\nDatasets\nChatGPT\nzero\nic-ran\nic-sim\nFinSent\n0.782\n0.761\n0.761\nFPB\n0.869\n0.832\n0.844\nFiQA SA\n0.898\n0.891\n0.891\nESG\n0.477\n0.726\n0.800\nFLS\n0.652\n0.673\n0.636\nQA\n0.695\n0.660\n0.675\nHeadlines-PDU\n0.889\n0.839\n0.765\nHeadlines-PDC\n0.936\n0.323\n0.413\nHeadlines-PDD\n0.896\n0.816\n0.788\nHeadlines-PI\n0.225\n0.768\n0.844\nHeadlines-AC\n0.806\n0.576\n0.597\nHeadlines-FI\n0.711\n0.606\n0.592\nHeadlines-PS\n0.630\n0.690\n0.729\nNER\n0.748\n0.784\n0.793\nFOMC\n0.633\n0.672\n0.650\nAverage\n0.723\n0.708\n0.719\nTable 3: The results of ChatGPT in zero-shot and in-\ncontext few-shot learning. Zero, ic-ran, and ic-sim repre-\nsent zero-shot learning, in-context learning with random\nsample selection, and in-context learning with similar\nsample selection. The zero-shot and few-shot perfor-\nmances are comparable in most cases.\nin-context examples. The prompts for in-context\nlearning are directly extended from the correspond-\ning zero-shot prompts, with the template shown in\nFigure 1.\n5.2\nMain Results\nTable 2 compares the results of the fine-tuned\nencoder-only models and zero-shot decoder-only\nmodels in 9 financial datasets. We have the follow-\ning findings:\nIn 6 out of 9 datasets, fine-tuned encoder-\nonly models can perform better than decoder-\nonly models. The decoder-only models, especially\nthose that have experienced RLHF or instruction-\ntuning, demonstrate considerable performance on\nzero-shot settings on the financial NLP tasks. How-\never, their performance generally falls behind the\nfine-tuned language models, implying that these\nlarge language models still have the potential to im-\nprove their financial expertise. On the other hand,\nfine-tuned models are less effective when the\ntraining examples are insufficient (FiQA SA) or\nimbalanced (FOMC).\nThe performance gaps between fine-tuned\nmodels and zero-shot LLMs are larger on pro-\nprietary datasets than publicly available ones.\nFor example, the FinSent, FPB, and FiQA SA\ndatasets are comparable and all about financial sen-\ntiment classification. However, zero-shot LLMs\nperform the worst on the proprietary dataset Fin-\nSent. The performance gaps between fine-tuned\nmodels and zero-shot LLMs are also more signifi-\ncant on other proprietary datasets (ESG, FLS, and\nQA) than the public dataset.\nTable 3 compares the zero-shot and in-context\nfew-shot learning of ChatGPT. In ChatGPT, the\nzero-shot and few-shot performances are com-\nparable in most cases. When zero-shot prompting\nis ineffective, adding demonstrations can improve\nChatGPT’s performance by clarifying the task, as\nthe results of ESG and Headlines-PI tasks show.\nDemonstrations are ineffective for easy and well-\ndefined tasks, such as sentiment classifications and\nHeadlines (PDU, PDC, PDD, AC, and FI), as the\nzero-shot prompts clearly instruct ChatGPT.\n6\nConclusions\nWe present FinLMEval, an evaluation framework\nfor financial language models. FinLMEval com-\nprises nine datasets from the financial domain, and\nwe conduct the evaluations on various popular lan-\nguage models. Our results show that fine-tuning\nexpert encoder-only models generally perform bet-\nter than the decoder-only LLMs on the financial\nNLP tasks, and adding in-context demonstrations\nbarely improves the results.\nOur findings sug-\ngest that there remains room for enhancement for\nmore advanced LLMs in the financial NLP field.\nOur study provides foundation evaluations for con-\ntinued progress in developing more sophisticated\nLLMs within the financial sector.\n7\nLimitations\nThis paper has several limitations to improve in\nfuture research. First, our evaluation is limited\nto some notable language models, while other ad-\nvanced LLMs may exhibit different performances\nfrom our reported models. Also, as the LLMs keep\nevolving and improving over time, the future ver-\nsions of the evaluated models can have different\nperformance from the reported results. Second,\nFinLMEval only focuses on financial classification\ntasks, and the analysis of the generation ability of\nthe LLMs still needs to be included. Future work\ncan be done toward developing evaluation bench-\nmarks on generation tasks in the financial domain.\nReferences\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timo-\nthy Baldwin. 2015. Domain adaption of named entity\nrecognition to support credit risk assessment. In Pro-\nceedings of the Australasian Language Technology\nAssociation Workshop, ALTA 2015, Parramatta, Aus-\ntralia, December 8 - 9, 2015, pages 84–90. ACL.\nDogu Araci. 2019.\nFinbert:\nFinancial sentiment\nanalysis with pre-trained language models. CoRR,\nabs/1908.10063.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nFiQA. Financial question answering. https://sites.\ngoogle.com/view/fiqa.\nAllen H Huang, Hui Wang, and Yi Yang. 2023. Finbert:\nA large language model for extracting information\nfrom financial text. Contemporary Accounting Re-\nsearch, 40(2):806–841.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nPekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki\nWallenius, and Pyry Takala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. J. Assoc. Inf. Sci. Technol., 65(4):782–796.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nAgam Shah, Suvan Paturi, and Sudheer Chava. 2023.\nTrillion dollar words: A new financial dataset, task &\nmarket analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 6664–6679. Associa-\ntion for Computational Linguistics.\nRaj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah,\nWendi Du, Sudheer Chava, Natraj Raman, Charese\nSmiley, Jiaao Chen, and Diyi Yang. 2022. When\nFLUE meets FLANG: Benchmarks and large pre-\ntrained language model for financial domain. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2322–\n2335, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nAnkur Sinha and Tanmay Khandait. 2020. Impact of\nnews on the commodity market: Dataset and results.\nCoRR, abs/2009.04202.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-\nlizing BERT for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 380–385. Associa-\ntion for Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2023, Toronto, Canada, July 9-14, 2023,\npages 13484–13508. Association for Computational\nLinguistics.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravol-\nski, Mark Dredze, Sebastian Gehrmann, Prabhanjan\nKambadur, David S. Rosenberg, and Gideon Mann.\n2023. Bloomberggpt: A large language model for\nfinance. CoRR, abs/2303.17564.\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao\nLai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023. PIXIU: A large language model, in-\nstruction data and evaluation benchmark for finance.\nCoRR, abs/2306.05443.\nYi Yang, Yixuan Tang, and Kar Yan Tam. 2023. In-\nvestlm: A large language model for investment using\nfinancial domain instruction tuning. arXiv preprint\narXiv:2309.13064.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020.\nFinbert: A pretrained language model for\nfinancial communications. CoRR, abs/2006.08097.\nA\nChatGPT Prompts\nPrompts used for zero-shot learning on decoder-\nonly LLMs are shown in Table 4. The prompts used\nfor in-context learning are similar, except \" The\nsentence: {}\" is replaced by the few-shot examples\nas shown in Figure 1.\nFinSent\nPerform financial sentiment classification. Classify the following sentence into ’neutral’,’ positive’, or\n’negative’ class. Only provide the label in the output. The sentence: {} Answer:\nFPB\nPerform financial sentiment classification. Classify the following sentence into one of the ’negative’,\n’neutral’, ’positive’ classes. Only provide the label in the output. The sentence: {} Answer:\nFiQA SA\nPerform aspect-based financial sentiment classification. Only output ’negative’ or ’positive’.\nThe sentence: {} Answer:\nESG\nPerform sentence classificaion under the environmental, social, and corporate governance (ESG)\nframework. Classify the following sentence into one of the ’ENVIRONMENTAL’,’SOCIAL’,\n’GOVERNANCE’, ’NON-ESG’ classes. Only provide the label in the output. The sentence: {} Answer:\nFLS\nForward-looking statements (FLS) are beliefs and opinions about a firm’s future events or results.\nPerform text classification to detect whether the sentence is a forward-looking statement. Classify\nthe sentence into ’Not-FLS’, ’Specific FLS’, or ’Non-specific FLS’ class. Only provide the label\nin the output. The sentence: {} Answer:\nQA\nGiven two sentences, one is the question, and the other is the answer. Classify whether the answer is valid\nto the question. Output ’YES’ if the answer is valid; otherwise, output ’NO’. Only provide the label in the\noutput. The question: {} The answer: {} Answer:\nHeadlines\nPerform text classification to detect whether the sentence implies price direction up (PDU). Classify the\nsentence into the ’PDU’ class if the sentence implies price direction up, or ’Not-PDU’ otherwise.\nOnly provide the label in the output. The sentence: {} Answer:\nNER\nPerform named entity recognition for each token in the following sentence. the sentece consists of\nall the tokens. Classify each token into one of the ’O’,’B-PER’,’I-PER’, ’B-ORG’, ’I-ORG’, ’B-LOC’,\n’I-LOC’, ’B-MISC’, ’I-MISC’ classes. ’O’ represents tokens that are not part of any named entity.\n’B-PER’ represents the beginning of a person’s name. ’I-PER’ represents a token inside a person’s\nname. ’B-ORG’ represents the beginning of an organization’s name. ’I-ORG’ represents a token inside\nan organization’s name. ’B-LOC’ represents the beginning of a location name. ’I-LOC’ represents a\ntoken inside a location name. ’B-MISC’ represents the beginning of a miscellaneous named entity\n(e.g., events, products). ’I-MISC’ represents a tokeninside a miscellaneous named entity. Only\nprovide the label in the output in the form of a list. The sentence: {} Answer:\nFOMC\nClassify the following sentence from FOMC into ‘HAWKISH’, ‘DOVISH’, or ‘NEUTRAL’ class. Label\n‘HAWKISH’ if it is corresponding to tightening of the monetary policy, ‘DOVISH’ if it is corresponding\nto easing of the monetary policy, or ‘NEUTRAL’ if the stance is neutral. Only provide the label in the\noutput. The sentence: {} Answer:\nTable 4: Prompts used for zero-shot learning on decoder-only models.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-10-19",
  "updated": "2023-10-19"
}