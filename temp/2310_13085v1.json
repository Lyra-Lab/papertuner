{
  "id": "http://arxiv.org/abs/2310.13085v1",
  "title": "Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning",
  "authors": [
    "Atik Faysal",
    "Mohammad Rostami",
    "Huaxia Wang",
    "Avimanyu Sahoo",
    "Ryan Antle"
  ],
  "abstract": "Few-shot learning or meta-learning leverages the data scarcity problem in\nmachine learning. Traditionally, training data requires a multitude of samples\nand labeling for supervised learning. To address this issue, we propose a\none-shot unsupervised meta-learning to learn the latent representation of the\ntraining samples. We use augmented samples as the query set during the training\nphase of the unsupervised meta-learning. A temperature-scaled cross-entropy\nloss is used in the inner loop of meta-learning to prevent overfitting during\nunsupervised learning. The learned parameters from this step are applied to the\ntargeted supervised meta-learning in a transfer-learning fashion for\ninitialization and fast adaptation with improved accuracy. The proposed method\nis model agnostic and can aid any meta-learning model to improve accuracy. We\nuse model agnostic meta-learning (MAML) and relation network (RN) on Omniglot\nand mini-Imagenet datasets to demonstrate the performance of the proposed\nmethod. Furthermore, a meta-learning model with the proposed initialization can\nachieve satisfactory accuracy with significantly fewer training samples.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n1\nUnsupervised Representation Learning to Aid\nSemi-Supervised Meta Learning\nAtik Faysal, Mohammad Rostami, Huaxia Wang, Avimanyu Sahoo, and Ryan Antle\nAbstract—Few-shot learning or meta-learning leverages the\ndata scarcity problem in machine learning. Traditionally, training\ndata requires a multitude of samples and labeling for supervised\nlearning. To address this issue, we propose a one-shot unsupervised\nmeta-learning to learn the latent representation of the training\nsamples. We use augmented samples as the query set during the\ntraining phase of the unsupervised meta-learning. A temperature-\nscaled cross-entropy loss is used in the inner loop of meta-learning\nto prevent overfitting during unsupervised learning. The learned\nparameters from this step are applied to the targeted supervised\nmeta-learning in a transfer-learning fashion for initialization\nand fast adaptation with improved accuracy. The proposed\nmethod is model agnostic and can aid any meta-learning model\nto improve accuracy. We use model agnostic meta-learning\n(MAML) and relation network (RN) on Omniglot and mini-\nImagenet datasets to demonstrate the performance of the proposed\nmethod. Furthermore, a meta-learning model with the proposed\ninitialization can achieve satisfactory accuracy with significantly\nfewer training samples.\nIndex Terms—meta-learning, image classification, representa-\ntion learning, unsupervised learning, semi-supervised learning.\nI. INTRODUCTION\nM\nETA learning is a relatively new branch of machine\nlearning that deals with learning to learn problems [1]\nwith only a few samples. Traditional machine learning algo-\nrithms require massive datasets to reach their peak performance.\nNevertheless, these algorithms suffer if the test domain slightly\ndeviates from the training domain. Furthermore, if a new class\nis introduced, it requires training from scratch again. On the\nother hand, human learning is far more advanced as they can\nlearn from only a few samples and distinguish a new class\nwithout seeing many samples. This is because humans use their\nprevious memory when learning a new task. Meta-learning\nmimics the process of human learning and tries to bridge the\ngap between machine learning and human learning [2].\nAlmost all meta-learning algorithms [3], [4], [5], [6] deal\nwith a task or episode generation during the training phase to\nlearn to use this knowledge during the testing phase for being\nable to distinguish from a few samples. This phenomenon is\ndefined as learning to learn, and both the training and testing\nphases have samples that are called support and query sets\n[7], respectively. The support set is used for learning the class\nAtik Faysal and Mohammad Rostami are PhD candidates in the Department\nof Electrical and Computer Engineering at Rowan University.\nHuaxia Wang is a faculty member in the Department of Electrical and\nComputer Engineering at Rowan University.\nAvimanyu Sahoo is a faculty member in the Department of Electrical and\nComputer Engineering at the University of Alabama, Huntsville.\nRyan Antle is the R&D head at Baker Hughes.\nrepresentation, and the query set is applied for inference. All\nmeta-learning algorithms are built on this fundamental strategy.\nSupport and query sets are generated in batches (also known as\nepisodes in meta-learning lingo) by drawing samples from the\ntraining data. ’One hot encoded’ pseudo labels are added to\nthe classes in the episodes. Exact class labeling is not essential\nat this stage because, during the training time, meta-learning\nalgorithms only try to learn to perform testing on some new\nclasses never seen before. This motivates our study to use\nrandom training samples for support sets from the pool of\nthe training data and generate query sets using the augmented\ntraining samples. This pseudo-labeling helps the classifier learn\nsome feature representations from the dataset without going\nthrough the time-consuming manual labeling process.\nOur proposed method uses specific image augmentation\ntechniques to generate the training episodes. First, we lose\nall the labels and class information from our data pool. Then\nwe randomly draw samples from the pool to generate our\nsupport sets and do image augmentation on the support sets\nto generate our query sets. Technically, it works for datasets\nlike Omniglot [8] and mini-Imagenet [9] or larger datasets\nbecause they contain a multitude of samples and classes, and\nthe probability of drawing from the same class, is much lower.\nOur method contains two steps of training. First, the fully\nunsupervised training to learn the latent representations of the\ndataset. We use the labeled test sets to observe the performance\nduring this time. Although not as good as supervised learning,\nthe meta-learning algorithm achieves some accuracy during\nunsupervised representation learning. Later, these learned\nparameters are used to initialize the final supervised meta-\nlearning and to boost the performance. Therefore, in the\nsecond step of meta-learning, we initialize with the learned\nparameters from the unsupervised learning model instead of\nrandom initialization. Thus, the whole process becomes a semi-\nsupervised meta-learning [10].\nFor an effective augmentation technique, we followed the\nsuggestion from the SimCLR [11] with a few additional\naugmentations to increase the effectiveness. Our proposed\nmethod is model-agnostic and can be applied to any meta-\nlearning model. We used two prominent meta-learning ar-\nchitectures, model agnostic meta-learning (MAML) [12] and\nrelation network (RN) [13], to test our hypothesis. We also\nmodified a part of the MAML network architecture by adding\ntemperature [14] to the SoftMax activation function in the inner\nloop of MAML to reduce overfitting during the unsupervised\ntraining. We did not modify the RN architecture but used our\nhyperparameters and architecture to obtain higher accuracy\nthan reported in the original paper. Our proposed method can\narXiv:2310.13085v1  [cs.LG]  19 Oct 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n2\nenhance the accuracy of any state-of-the-art meta-learning\nmodel, as proved in the experiments of this study.\nOur contributions to this work are listed below:\n• We proposed a more effective data augmentation technique\nto generate query sets by combining techniques from\nSimCLR and our additional steps.\n• We used a temperature-scaled SoftMax in the inner steps\nof MAML to reduce overfitting during meta-training.\nOur implementation of RN surpasses the accuracy of\nthe original RN.\n• We replaced random initialization of meta-learning with\nunsupervised representation learning for inherent feature\nlearning that does not require extensive data labeling. After\ntransferring the parameters from unsupervised learning,\nwe applied supervised meta-learning to achieve improved\naccuracy.\n• We showed that our two-step meta-learning is model\nagnostic and improves the accuracy of any existing meta-\nlearning model. We also experimented with partially\nlabeled data and found that the classifier loses insignificant\naccuracy when trained with our method.\nThe codes for our implementation can be found at https:\n//github.com/atik666/representationTransfer\nII. RELATED WORK\nMeta-learning [15] has many practical applications, such\nas self-driving cars, face recognition, and computer vision.\nAlthough the core motivation of meta-learning is to classify\nwith a few samples, training the model still requires a lot of\nlabeled samples. This popularized the use of data augmentation\nin meta-learning. Yao et al. [16] proposed two task augmenta-\ntion methods, called MetaMix and channel shuffle. MetaMix\nlinearly combines features and labels of samples from both the\nsupport and query sets. Channel shuffle randomly replaces a\nsubset of their channels with the corresponding ones from a\ndifferent class. Experimental analysis showed that their method\neffectively reduces overfitting in meta-learning. Rajendran et al.\n[17] introduced an information-theoretic framework of meta-\naugmentation for better generalization by adding randomness,\nwhich discourages the base learner and model from learning\nunimportant features. Nevertheless, all these methods are\nsupervised learning and still need the labeling of a large number\nof samples.\nHsu et al. proposed one of the earliest unsupervised meta-\nlearning algorithm called CACTUs [18] which assigns pseudo\nlevel to the remaining unlabelled datasets using a nearest\nneighbor approach. It is an iterative process where the pseudo-\nlabels are incorporated into the clustering and adaptation steps\nleading to an improved accuracy. Nevertheless, the proposed\nmethod requires additional steps such as embedding learning\nalgorithm and k-means clustering [19] for the purpose of\npseudo label generation. These extra steps make the algorithm\ncomputationally expensive. Moreover, the authors did not\nextend the idea to semi-supervised learning. Therefore, the\nmethod cannot match the accuracy of a supervised learning.\nKhodadadeh et al. [20] proposed UMTRA, an algorithm\nthat performs unsupervised, model-agnostic meta-learning for\nclassification tasks. They used augmented query samples for the\nunsupervised classification of MAML. However, their proposed\nmethod is fully unsupervised and ultimately achieves much\nlower accuracy than supervised meta-learning. Chen et al. [11]\nproposed SimCLR that investigates the most effective data\naugmentation for semi-supervised learning. They used a nor-\nmalized temperature-scaled cross-entropy loss to achieve better\ngeneralization during the unsupervised representation learning.\nThe two aforementioned pieces of research heavily influenced\nour proposed work to develop a semi-supervised meta-learning\nthat utilizes the power of unsupervised representation learning\nand meta-transfer learning.\nThere are several state-of-the-art meta-learning models\npopular in the research community. MAML [12] is one of\nthe pioneers of deep meta-learning models. MAML tries\nto find the optimal parameters over the task embeddings\nfor fast adaptation. The family of MAML contains several\npopular and almost similar classifiers, namely, Reptile [5],\nMeta-SGD [4], LEO [21]. Another popular model is called\nPrototypical network [22], which learns a metric space in\nwhich classification can be performed by computing distances\nto prototype representations of each class. This network obtains\nhigher accuracy than many of its predecessors. RN [13] came\nout right after the Prototypical network, which surpassed\nthe accuracy of the Prototypical network in most cases. Our\nstudy obtained promising outputs using a modified MAML for\nunsupervised learning and additionally uses RN to show its\nmodel-agnostic ability.\nIII. PROPOSED METHOD\nA. Step 1: Unsupervised Learning\nData Preparation: To incorporate representation learning\nwith meta-learning, we first take the entire or partial dataset\nwithout any label information. An effective way to learn\nthe representation is to use both the labeled and unlabelled\ndata. This ensures that the classifiers learn all the inherent\nrepresentations in a semi-supervised way.\nFirst, we draw the samples xi,j from the data pool of XN\nwhere i, j are the number of shots and the number of ways,\nrespectively, considered in the unsupervised learning and N\nis the total number of unlabelled samples. We only design\nn-way (n is the number of ways or classes), 1-shot support\nsets because each sample in the support set is drawn randomly,\nand we cannot randomly add more same-class support samples\nto that set. However, we can apply data augmentation for the\nquery set to generate multiple query samples of the same class.\nBut is generating more query samples more effective? We\nanswer that question in the later part of this research.\nThe exact labeling in meta-training episodes is not crucial.\nTherefore, after generating the training episodes, we randomly\nassign labeled values yi,j to each class of the support sets,\nwhere j is the number of ways generated as {c0, c1, ..., cj−1}\nand one-hot encoded later. We initialize the random initializa-\ntion parameter for the unsupervised classifier, θ. We randomly\ndraw the support sets for each task episode and generate\nthe randomly generated support labels. To generate the query\nset, we pass each sample of the support set through a data\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n3\naugmentation function f(A) and similarly generate the pseudo\nlabels. Ultimately, we use the regular supervised meta-learning\nlearning test setup to examine the classifier’s performance.\nDeep Dive into Support-Query Set Generation: We\nintuitively know that when we draw a few samples from a\nlarge pool of data, more than one sample belonging to the\nsame class is low. Therefore, we must ensure that n << c\nwhere n is the number of ways (or the drawn samples since\nwe only apply 1-shot learning) and c is the total number of\nclasses. Nevertheless, we need to mathematically compute the\nprobability of getting unique samples in each class for the\ndatasets used in this study.\nWe use two different datasets, Omniglot and mini-Imagenet.\nThe prior one has less number of samples in each class than\nthe total number of classes. Therefore, it is most likely that all\ndrawn samples will originate from different classes. The latter\nhas more samples (600) in each class than the total number of\nclasses. Therefore, the probability of originating from different\nclasses would be slightly lower. Nevertheless, we have an equal\nnumber of samples in both datasets, m for each class. Now,\nwe can calculate the probability of the samples belonging to\ndifferent classes as follows:\nP = c! · mn(c · m −n)\n(c −n)! · (c · m)!\n(1)\nUsing the aforementioned formula, the probabilities of 5-way\n1-shot classification for the Omniglot (1200 classes) and mini-\nImagenet (64 classes) are 99.21% and 85.23%, respectively.\nEffective data augmentation is important in this research to\ngenerate the query sample. We follow the suggestion from the\nSimCLR [11] and combine it with other methods to make\nit more effective for the RGB image classification (mini-\nImagenet). SimCLR paper elaborates on the effectiveness\nof data augmentation and choosing the proper augmentation\nfunction, which motivates us to follow their method. They\nsuggested the most effective combination of Gaussian blur,\nrandom crop, and random color distortion. We added horizontal\nflip and random color invert (50% probability) with these three\nmethods as we found that it reduces overfitting and improves\naccuracy. On the other hand, for the grayscale Omniglot dataset,\nwe only use random affine transform because we found that\nboth the support and query samples are very similar, and a\nhard augmentation hurts the performance.\nClassifiers: Our proposed method is model agnostic and can\nbe applied to any model. In this paper, we use two meta-learning\nmodels, MAML and RN, to demonstrate the performance on\ndifferent architectures. We find that for MAML, the classifier\ntrained on RGB samples (mini-Imagenet in our case) has a\nsevere overfitting issue using the regular classifier. This is\nbecause the augmented query samples are similar to the original\nsupport samples. Therefore, the classifier learns very little\nduring the training phase. We solve this problem by using\na temperature-scaled SoftMax activation function only in the\ninner loop of MAML. The temperature term makes the classifier\nless confident of the support set samples, and thus the classifier\ncan learn more information from the subtle differences. The\nmathematical expression for temperature-scaled SoftMax is as\nfollows:\nexp(zi/T)\nPj−1\nk=0 exp(zk/T)\n(2)\nwhere the scaling is accomplished by dividing the logits of\nSoftMax by a value T, known as temperature. j is the number\nof ways, and zi, zk represent the ith, kth input to the SoftMax,\nrespectively.\nWe found RN performing counter effectively when using a\ntemperature-scaled SoftMax. We instead used our own set of\nhyperparameters that led to more improved accuracy than the\nRN in the original paper.\nAfter training the unsupervised learning algorithm, we save\nthe weights and biases to perform semi-supervised meta-\nlearning. Therefore, in the classifier of step two, instead of\nrandomly initialized parameters, θ, we used the transferred\nparameters, θ∗. Then, we perform the regular meta-learning\nfor fine-tuning and improved accuracy.\nB. Step 2: Semi-Supervised Meta Learning (SSML)\nIn this step, we apply SSML on the regular meta-learning\nsettings but initialize the weights and biases from the first\nclassifier. First, let us talk briefly about the two classifiers,\nMAML and RN.\nMAML: MAML tries to find the optimal parameters θ\nderived from a few parametric models fθ. In MAML, we\ngenerate the episodes from the data distribution such as\nτi = (Dtr, Dval). We use the gradient update to update the\ninitialize parameter θ to θ\n′\ni across tasks sampled from p(τ)\nand is obtained as follows:\nθ\n′\ni = θ −α∇θ£τi(fθ)\n(3)\nwhere α is the learning rate of the meta-inner loop, and £\nis the loss function. In the outer loop of meta-learning, the\noptimization is performed across tasks via stochastic gradient\ndescent (SGD) to update the θ. It is obtained as follows:\nθ ←θ −β∇θ\nX\nτi∼p(τ)\n£τi(fθi)\n(4)\nwhere β is the learning rate of the meta-outer loop.\nRN: The main two components of RN are a feature extractor\nand a relation module. The feature extractor concatenates the\nfeatures from the support sets, and the query sets as fφ(xi)\nand fφ(xj) through a function C(fφ(xi), fφ(xj)).\nThe combined features are passed through the relation\nmodule to obtain their relation score. It is passed through\na Sigmoid activation function to obtain the score in a range\nbetween 0 to 1. The equation for that is provided below:\nri,j = gϕ(C(fφ(xi), fφ(xj)))\n(5)\nTo create the final output, the relation network’s output can\nalso be subjected to extra processing by layers, such as a\nfully connected neural network. Because of this, the relation\nnetwork is an adaptable architecture that may be used for\nvarious applications. A mean-square-error (MSE) loss function\nis used to update the network using gradient descent.\nφ, ϕ ←arg min\nm\nX\ni=1\nn\nX\nj=1\n(ri,j −1(yi == yj))2\n(6)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n4\nOverall Summary: The overall method is summarized in\nthis sector with a diagram for better understanding. Figure 1\ndepicts the steps of the proposed method. We generate the\ntraining episodes from the unlabeled samples. Here, the NT-\nXent loss [14] (temperature-scaled SoftMax) is only applied\non the MAML for the mini-Imagenet dataset. After training\nthe initial model, we save the parameters and transfer them\nto the final model for improved performance. Moreover, the\npseudo-code for our proposed method is provided in Algorithm\n1.\nAlgorithm 1 Unsupervised representation learning for semi-\nsupervised meta learning\nrequire: unlabeled dataset, U{xi}\nrequire: α, β: learning rate hyperparameters\nrequire: f(A): augmentation function\nInitialize random parameter, θ\nwhile not done do\ngenerate episodes, {xi} and create pseudo labels, {yi}\nfor all {xi, yi} do\nupdate inner loop of meta-learning with custom loss\nfunction or hyperparameters\nend for\nupdate outer loop of meta-learning with the regular loss\nfunction\nend while\nsave weights and biases, θ∗\nrequire: labeled dataset, {xj, yj} ⊑{xi, yi} Initialize θ∗\ndo regular meta-learning steps\nIV. EXPERIMENTS\nA. Data Augmentation for Representation Learning\nWe validate our proposed method using two different\nbenchmark datasets in computer vision, Omniglot and mini-\nImagenet. Omniglot contains images of handwritten letters\nfrom 50 different languages. This dataset is suitable for few-\nshot learning because it has 1623 characters or classes but\nonly 20 instances or samples per class. We used 1200 classes\nfor training, 100 classes for validation, and the remaining for\ntesting. In input image dimension to the classifier is 1×28×28\npixels, as all are grayscale samples. On the other hand, the\nmini-Imagenet dataset contains 3×84×84 pixels color images.\nIt has a total of 100 classes, each with 600 samples. Here,\nwe use 64 for training, 16 for validation, and 20 classes for\ntesting.\nSelecting the most effective data augmentation is an essential\npart of our research for unsupervised learning. We experimented\nwith different augmentation methods on a trial-and-error basis\nand found the SimCLR augmentation with an additional\naugmentation gave the best output for the mini-Imagenet dataset.\nThis section lists the results from different augmentation\nmethods in this research. We focus on the mini-Imagenet\ndataset for the augmentation part because the Omniglot dataset\ndoes not require heavy data augmentation. We also try to\nexplain why our chosen augmentation works the best for our\ndataset. Table I lists the outputs from different augmentation\nmethods using unsupervised learning. Note that all the outputs\nare obtained by re-implementing different methods using our\nown hyperparameters, which may provide different results than\nother literature.\nFrom Table I, we observe the outputs from unsupervised\nlearning for various augmentation functions. Let us discuss the\naccuracy of MAML first. First of all, we use the traditional\nmeta-learning where the temperature parameter in the meta-\ninner loop for the SoftMax activation function is 1. A\ntemperature of 1 means basically no temperature parameter.\nFor MAML, we discovered that using the optimal temperature\nin the inner loop increased the accuracy of all the augmentation\nfunctions. It is because, when the temperature is 1, the training\nclassifier overfits a lot due to the query set not being very\nchallenging for the support set. When we apply the temperature,\nthe classifier becomes less confident of the classes and can learn\nmore features because of the introduced uncertainty. First, we\napply the auto-augment function for query sample generation,\nwhich achieved slightly higher accuracy than the SimCLR\naugmentation function in all cases. Our augmentation function\nachieved 33.8% and 13.65% accuracy, which is the highest of\nall. We introduce temperature parameters as 100 and 10 for\n5-way and 20-way, respectively. All the classifier exhibits im-\nproved accuracy for the optimal temperature, and our proposed\nmethod obtained the highest accuracy. The temperature is a\nhyperparameter that shows different performances for different\nvalues. In Figure 2 we illustrated the output accuracy from\ndifferent temperatures to select the optimal ones. As observed,\ntemperature 100 and 10 provides the highest accuracy for 5-way\nand 20-way, respectively.\nFor the RN, we do not modify anything in the classifier\narchitecture; rather, our motivation is to show that the proposed\nmethod is model agnostic. Nevertheless, our combination\nof hyperparameters with a ResNet-18 [23] achieved higher\naccuracy than MAML for 5-way classification but obtained\nlower accuracy for 20-way classification. The auto-augment\nmethod obtained slightly higher accuracy than the SimCLR\naugmentation for both 5-way and 20-way classifications. How-\never, SimCLR performs poorly for the 20-way classification\nand achieves only 7% accuracy. On the other hand, when\nwe apply our proposed augmentation method, we obtain the\nhighest accuracy for both 5-way and 20-way classifications.\nNevertheless, using our proposed method, the RN module\nachieved higher accuracy than the MAML module for 5-way\nclassification and lower accuracy for 20-way. Therefore, it is\nevident that the RN unsupervised meta-learning fails to achieve\nsatisfactory accuracy for a higher number of classifications.\nWe also present the outputs from the Omniglot dataset in\nTable II to show the domain adaptability of our proposed\nmethod. In Omniglot, the support samples are quite similar\nto the query samples. As a result, our experiment found\nthat doing any hard augmentation on the samples hurts the\nperformance. Therefore, we perform a minimum augmentation\nto keep the features intact and yet introduce some information\nin the augmented samples. Moreover, since the samples are\ngrayscale, we could not follow the color distortion function\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n5\nFig. 1. Steps of the proposed method, top-left: unlabeled train samples, bottom-left: labeled test samples, middle: episodes for training and testing, middle-left\ntwo columns: support set, middle-right two columns: query set, right-top: performing unsupervised meta-learning with the episodes, right-middle: transferring\nparameters from unsupervised learning to supervised learning, right-bottom: supervised learning with improved accuracy.\nTABLE I\nTHE TEST ACCURACY (%) OF UNSUPERVISED META-LEARNING FOR 5-WAY 1-SHOT (5W1S) AND 20-WAY 1-SHOT (20W1S) CLASSIFICATION USING\nMINI-IMAGENET DATASET. FOR MAML, DIFFERENT TEMPERATURES (DENOTED BY T ) ARE APPLIED IN THE META-INNER LOOP.\nAugmentation method\nMAML\nRN\n5W1S\n20W1S\n5W1S\n20W1S\nAuto augment (UMTRA∗)\n30.1 (T =1)\n9.25 (T =1)\n35\n9\n35.2 (T =100)\n11.65 (T =10)\nResized crop + Gaussian blur + color distortions (SimCLR)\n28.4 (T =1)\n7.6 (T =1)\n32\n7\n34.4 (T =100)\n11.1 (T =10)\nHorizontal flip(p=0.5) + color invert (p=0.5) + resized crop +\nGaussian blur + color distortions (Ours)\n33.8 (T =1)\n13.65 (T =1)\n39\n11.5\n38.2 (T =100)\n13.95 (T =10)\n∗re-implementation.\nTABLE II\nTHE TEST ACCURACY (%) OF UNSUPERVISED META-LEARNING FOR 5W1S AND 20W1S CLASSIFICATION USING OMNIGLOT DATASET.\nAugmentation method\nMAML\nRN\n5W1S\n20W1S\n5W1S\n20W1S\nRandom transformation + zero pixels (UMTRA∗)\n48.80\n24.94\n61.25\n35.78\nResized crop + Gaussian blur (SimCLR)\n48.93\n27.47\n66.25\n43.13\nRandom affine transform (30◦) (Ours)\n52.83\n27.95\n69.12\n44.37\n∗re-implementation.\nfrom SimCLR. So, we only applied affine transformation within\n30◦to obtain the best transformation. This transformation\ndistorts the samples slightly but keeps the meaning intact. In the\ncase of SimCLR, we only apply resized crop and Gaussian blur\nas the color distortions do not apply to this dataset. Proof of the\neffectiveness of our method can be found in the experimental\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n6\n(a) 5W1S accuracy for different temperatures.\n(b) 20W1S accuracy for different temperatures.\nFig. 2. Effect of different temperatures on test accuracy.\noutputs. In this case, the SimCLR augmentation achieved higher\naccuracy for both MAML and RN in all 5-way 1-shot and\n5-way 20-shot classifications. Our proposed method achieves\nthe highest accuracy in all comparisons.\nOur proposed method can be technically extended to an n-\nway 1-shot multi-query classification. Because we can generate\ndifferent augmented samples in each run for multiple query\ngeneration. However, we found an accuracy drop in our\nproposed model when applied to multiple queries. We suspect it\nhappens because the classifier gets overfit from multiple queries\nas they are not very visibly distinguishable. Table III represents\noutputs from MAML for multiple queries. The accuracy drop\nwas significant in all 5-way 5-query and 20-way 5-query shot\nclassifications. Therefore, we do not suggest using our method\nto n-query shot. One should rather apply 1-shot unsupervised\nlearning and then transfer the learned parameters to supervised\nlearning. We only applied a 5-way 5-query shot to RN and\nopted out 20-way 5-query shot because it requires substantial\ncomputational resources for a backbone of ResNet-18. We used\na 12GB Nvidia 3080Ti GPU to train our MAML module. For\nthe RN module, we used parallel computing on two 12GB\nNvidia 3090Ti GPUs and Google Cloud Platform GPUs. In\nthe Omniglot dataset, the accuracy drop for 5-way 5-shot and\n20-way 5-shot were 3.68% and 0.73% for MAML and 3.87%\nand 5.71% for RN. For mini-Imagenet, the drops are 3.04%\nand 1.52% for MAML and 11% for RN (N.B. no experiments\nconducted for 20W1S5Q RN).\nB. Semi-Supervised Meta-Learning (SSML)\nThe second stage is just like regular meta-learning but\ninitialized with the parameters from our previous method. In\nthis section, we report our accuracy for the whole process\nand compare it with the traditional method. We conduct the\nexperiments on n-way 1-shot 1-query and 5-shot 5-query\nfor different classifiers. Additionally, we show how well the\nclassifier can perform with partially labeled data instead of the\nwhole labeled dataset.\nTable IV presents the accuracy for the original method and\nour proposed method for the Omniglot and mini-Imagenet\ndatasets. We also present the outputs from the Baseline model to\nemphasize the effectiveness of MAML and RN. For Omniglot,\nour method achieves improved accuracy than the original\nmethod and proves its model-agnostic ability. In MAML, we\nobserve significantly higher accuracy for 1-shot learning and\nslightly improved accuracy for 5-shot learning in both 5-way\nand 20-way setups. Our SSML MAML improves the accuracy\nof MAML further. In RN, the performance improvement is not\nas significant as in MAML, as the original RN already achieved\nvery high accuracy. Nevertheless, we achieve 100% accuracy\non 5W1S1Q, which improves from 99.38%. However, in both\n5W5S5Q, both RN and SSML RN achieved 100% accuracy.\nWe observe a tiny improvement for 20-way SSML MAML. In\nSSML RN, we observe a 4% improvement in accuracy in both\n5W1S1Q and 5W5S5Q. All the outputs from MAML and RN\nare re-implemented in our code.\nC. Transferability of SSML\nIn this section, we test the transferability of the proposed\nmethod on different datasets. We use CIFAR-FS [24] and\ntieredImageNet [25] datasets for this experiment where we\ntransfer the learned representations from miniImageNet dataset.\nThe CIFAR-FS dataset has 100 classes and 600 images per\nclass. Train, test validation sets are split into 64, 16 and 20,\nrespectively. The tieredImageNet consists of 608 classes and\n779,165 total images. We use 351 classes for training, 97 for\nvalidation and 160 for testing.\nWe initialize SSML MAML with miniImageNet represen-\ntation and fine-tune on both datasets. The outputs are listed\nin Table V. In all cases, SSML MAML improves accuracy\nover MAML. The most significant improvement is for CIFAR-\nFS 5W1S1Q, which is 3.6%. This proves that the proposed\nmethod can also transfer the learned representations to different\ndomains for improved accuracy.\nV. WHY OUR METHOD IS EFFECTIVE\nIn Figure 3, we explained why our proposed method\nworks effectively by visualizing the pixel intensities from the\nhistograms of augmented images. The histogram analysis shows\nhow much uncertainty is introduced in different augmented\nsamples compared to the original samples. In both auto\naugment and SimCLR augmentation, we find the histograms\nof augmented samples are very similar to each other. That\nmeans all the augmented samples fail to introduce enough new\nuncertainties in each augmentation. It is essential because a\nsample can appear often in meta-learning in different episodes.\nSo, we must supply query samples with distinguishable features\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n7\nTABLE III\nTHE TEST ACCURACY (%) AND DROP OF THE PROPOSED UNSUPERVISED META-LEARNING FOR N-WAY, 1-SHOT MULTI-QUERY.\nDataset\nMAML\nRN\n5W1S5Q\n20W1S5Q\n5W1S5Q\n20W1S5Q\nOmniglot\n49.15 (drop 3.68)\n27.22 (drop 0.73)\n65.25 (drop 3.87)\n38.66 (drop 5.71)\nmini-Imagenet\n32.96 (drop 3.04)\n12.43 (drop 1.52)\n28 (drop 11)\nN/A\nTABLE IV\nTHE TEST ACCURACY (%) OF THE SUPERVISED META-LEARNING FOR THE OMNIGLOT DATASET.\nMethod\nOmniglot\nmini-Imagenet\n5-way accuracy\n20-way accuracy\n5-way accuracy\n20-way accuracy\n1S1Q\n5S5Q\n1S1Q\n5S5Q\n1S1Q\n5S5Q\n1S1Q\n5S5Q\nBaseline\n86\n97.6\n72.9\n92.3\n38.4\n51.2\nN/A\nN/A\nMAML∗\n93.8\n98.3\n82.5\n92.3\n46.8\n61.6\n18.75\n30.4\nRN∗\n99.38\n100\n97.19\n99.59\n53\n64\n24.25\nN/A\nSSML MAML (Ours)\n96.44\n98.34\n83.35\n92.72\n47.6\n61.8\n18.88\n30.71\nSSML RN (Ours)\n100\n100\n97.34\n99.69\n57\n67\n25\nN/A\n∗re-implementation.\nTABLE V\nTRANSFERABLITY OF SSML MAML FOR DIFFERENT DATASETS.\nData\nMethod\n5W1S1Q\n5W5S5Q\n20W1S1Q\n20W5S5Q\nCIFAR-FS\nMAML\n49.6\n71.2\n25.76\n42.16\nSSML MAML\n53.2\n71.73\n26.34\n42.8\ntieredImageNet\nMAML\n48\n61.47\n19.56\n32.35\nSSML MAML\n48.2\n62.04\n20.1\n33.23\nin each run. On the other hand, for the proposed augmentation\nmethod, we find the histograms have whole new pixel intensities\nfor each run. Therefore, the features have new information in\neach query sample. It can also be explained by the uncertainty\nwe introduce in our augmentation function by doing a horizontal\nflip and color invert with a 50% probability for each one.\nTherefore, our proposed method achieves the highest accuracy\nfor unsupervised meta-learning learning.\nVI. ABLATION STUDY\nAdditionally, we highlight that our method can obtain\nhigh accuracy or less accuracy loss for partially labeled\ndatasets (Table VI). We test our hypothesis on mini-Imagenet\nonly because it contains 600 samples per class, whereas\nOmniglot only has 20 samples per class. We randomly\nselect 50% (300) and 25% (150) training samples from the\nmini-Imagenet data and train our classifier to compare the\nproposed and original methods. This time we report the\npercentage accuracy drop from the main output (trained on\n100% samples) to have a fair comparison between the original\nmethod and SSML. It is obtained as ((all labeled accuracy −\npartially labeled accuracy)/all labeled accuracy) × 100%.\nIn most outputs, our proposed method has less drop except\nfor SSML RN with 50% and 25% labeled data for 5W5S5Q\nand 5W1S1Q, respectively. In MAML and SSML MAML, for\n5W1S1Q, we have negative accuracy drop percentages. This is\nbecause the accuracy, in fact, increases when we train MAML\nwith 50% data in this setup. We hypothesize this improvement\nis due to the episode generation with fewer samples in each\nclass. Some research points out that having a large number\nof meta-training data can counter-intuitively hurt performance.\nBecause of multiple possibilities for generating each episode,\nthe probability of all the samples appearing in the episodes will\nbe lower. For example, Triantafillou et al. [26] found that having\na large meta-dataset hurts the accuracy of the mini-Imagenet\ndataset. Setlur et al. [6] showed that having a fixed support\nset and having less diversity can improve accuracy. This new\nresearch direction deals with the optimal number of samples\nin meta-training and a more effective way of generating the\nepisodes. We aim to focus on this area in our future research.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n8\nFig. 3. Histogram of pixel intensities for different augmentation methods.\nTABLE VI\nACCURACY DROP (%) OF SUPERVISED META-LEARNING FOR THE PARTIALLY LABELED MINI-IMAGENET TRAINING SET.\nMethod\n5-way accuracy\n20-way accuracy\n1S1Q\n% Drop\n5S5Q\n% Drop\n1S1Q\n% Drop\n5S5Q\n% Drop\nMAML (50% labeled data)∗\n48.2\n-2.99\n61.45\n1.87\n18.75\n5.07\n28.42\n6.51\nSSML MAML (50% labeled data)\n49.4\n-3.78\n61.04\n1.23\n17.94\n4.98\n28.83\n6.12\nMAML (25% labeled data)∗\n46\n1.71\n57.4\n6.82\n16.25\n13.33\n26.54\n12.70\nSSML MAML (25% labeled data)\n47.2\n0.84\n58.25\n5.74\n17.5\n7.31\n27.05\n11.92\nRN (50% labeled data)∗\n39\n26.42\n58.2\n9.06\n19.75\n18.56\nN/A\nN/A\nSSML RN (50% labeled data)\n43\n24.56\n59.2\n11.64\n22.25\n11\nN/A\nN/A\nRN (25% labeled data)∗\n38\n28.3\n51.8\n19.06\n18.75\n22.68\nN/A\nN/A\nSSML RN (25% labeled data)\n40\n29.82\n54.4\n18.81\n20.25\n19\nN/A\nN/A\n∗re-implementation.\nVII. CONCLUSION\nIn this research, we propose a meta-learning strategy that\nlearns the latent representation from the dataset using unsu-\npervised meta-learning and then performs SSML using the\nlearned parameters. Unsupervised learning gives a performance\nboost to supervised learning. Therefore, our method is fast\nadaptive and obtains improved accuracy. Our unsupervised\nmethod depends on effective data augmentation for query\nsample generation. Additionally, we visually represent why\nour proposed combination of augmentations is more effective\nthan other augmentations. The temperature-scaled SoftMax\nalso plays a vital role in unsupervised classification accuracy.\nWe tested our proposed model with two different datasets and\nmodels. Our method achieves better test accuracy in all cases\nthan the original methods. We also show that our method can\nretain good accuracy and lower loss when trained on partially\nlabeled training samples.\nREFERENCES\n[1] M. Wortsman, K. Ehsani, M. Rastegari, A. Farhadi, and R. Mottaghi,\n“Learning to learn how to learn: Self-adaptive visual navigation using\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2023\n9\nmeta-learning,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2019, pp. 6750–6759.\n[2] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune,\n“Deep neuroevolution: Genetic algorithms are a competitive alternative\nfor training deep neural networks for reinforcement learning,” arXiv\npreprint arXiv:1712.06567, 2017.\n[3] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., “Matching\nnetworks for one shot learning,” Advances in neural information\nprocessing systems, vol. 29, 2016.\n[4] Z. Li, F. Zhou, F. Chen, and H. Li, “Meta-sgd: Learning to learn quickly\nfor few-shot learning,” arXiv preprint arXiv:1707.09835, 2017.\n[5] A. Nichol and J. Schulman, “Reptile: a scalable metalearning algorithm,”\narXiv preprint arXiv:1803.02999, vol. 2, no. 3, p. 4, 2018.\n[6] A. Setlur, O. Li, and V. Smith, “Is support set diversity necessary for\nmeta-learning?” arXiv preprint arXiv:2011.14048, 2020.\n[7] E. Bennequin, V. Bouvier, M. Tami, A. Toubhans, and C. Hudelot, “Bridg-\ning few-shot learning and adaptation: new challenges of support-query\nshift,” in Machine Learning and Knowledge Discovery in Databases.\nResearch Track: European Conference, ECML PKDD 2021, Bilbao,\nSpain, September 13–17, 2021, Proceedings, Part I 21.\nSpringer, 2021,\npp. 554–569.\n[8] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-level\nconcept learning through probabilistic program induction,” Science, vol.\n350, no. 6266, pp. 1332–1338, 2015.\n[9] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei,\n“ImageNet Large Scale Visual Recognition Challenge,” International\nJournal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.\n[10] Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele, “Meta-transfer learning\nfor few-shot learning,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 403–412.\n[11] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” in International\nconference on machine learning.\nPMLR, 2020, pp. 1597–1607.\n[12] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast\nadaptation of deep networks,” in International conference on machine\nlearning.\nPMLR, 2017, pp. 1126–1135.\n[13] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,\n“Learning to compare: Relation network for few-shot learning,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 1199–1208.\n[14] K. Sohn, “Improved deep metric learning with multi-class n-pair loss\nobjective,” Advances in neural information processing systems, vol. 29,\n2016.\n[15] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning\nin neural networks: A survey,” IEEE transactions on pattern analysis\nand machine intelligence, vol. 44, no. 9, pp. 5149–5169, 2021.\n[16] H. Yao, L.-K. Huang, L. Zhang, Y. Wei, L. Tian, J. Zou, J. Huang\net al., “Improving generalization in meta-learning via task augmentation,”\nin International conference on machine learning.\nPMLR, 2021, pp.\n11 887–11 897.\n[17] J. Rajendran, A. Irpan, and E. Jang, “Meta-learning requires meta-\naugmentation,” Advances in Neural Information Processing Systems,\nvol. 33, pp. 5705–5715, 2020.\n[18] K. Hsu, S. Levine, and C. Finn, “Unsupervised learning via meta-learning,”\narXiv preprint arXiv:1810.02334, 2018.\n[19] J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means clustering\nalgorithm,” Journal of the royal statistical society. series c (applied\nstatistics), vol. 28, no. 1, pp. 100–108, 1979.\n[20] S. Khodadadeh, L. Boloni, and M. Shah, “Unsupervised meta-learning for\nfew-shot image classification,” Advances in neural information processing\nsystems, vol. 32, 2019.\n[21] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero,\nand R. Hadsell, “Meta-learning with latent embedding optimization,”\narXiv preprint arXiv:1807.05960, 2018.\n[22] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot\nlearning,” Advances in neural information processing systems, vol. 30,\n2017.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[24] L. Bertinetto, J. F. Henriques, P. H. S. Torr, and A. Vedaldi,\n“Meta-learning with differentiable closed-form solvers,” CoRR, vol.\nabs/1805.08136, 2018. [Online]. Available: http://arxiv.org/abs/1805.\n08136\n[25] M. Ren, E. Triantafillou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,\nH. Larochelle, and R. S. Zemel, “Meta-learning for semi-supervised\nfew-shot classification,” CoRR, vol. abs/1803.00676, 2018. [Online].\nAvailable: http://arxiv.org/abs/1803.00676\n[26] E. Triantafillou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu,\nR. Goroshin, C. Gelada, K. Swersky, P.-A. Manzagol et al., “Meta-\ndataset: A dataset of datasets for learning to learn from few examples,”\narXiv preprint arXiv:1903.03096, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-10-19",
  "updated": "2023-10-19"
}