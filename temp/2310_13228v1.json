{
  "id": "http://arxiv.org/abs/2310.13228v1",
  "title": "The Less the Merrier? Investigating Language Representation in Multilingual Models",
  "authors": [
    "Hellina Hailu Nigatu",
    "Atnafu Lambebo Tonja",
    "Jugal Kalita"
  ],
  "abstract": "Multilingual Language Models offer a way to incorporate multiple languages in\none model and utilize cross-language transfer learning to improve performance\nfor different Natural Language Processing (NLP) tasks. Despite progress in\nmultilingual models, not all languages are supported as well, particularly in\nlow-resource settings. In this work, we investigate the linguistic\nrepresentation of different languages in multilingual models. We start by\nasking the question which languages are supported in popular multilingual\nmodels and which languages are left behind. Then, for included languages, we\nlook at models' learned representations based on language family and dialect\nand try to understand how models' learned representations for~(1) seen and~(2)\nunseen languages vary across different language groups. In addition, we test\nand analyze performance on downstream tasks such as text generation and Named\nEntity Recognition. We observe from our experiments that community-centered\nmodels -- models that focus on languages of a given family or geographical\nlocation and are built by communities who speak them -- perform better at\ndistinguishing between languages in the same family for low-resource languages.\nOur paper contributes to the literature in understanding multilingual models\nand their shortcomings and offers insights on potential ways to improve them.",
  "text": "The Less the Merrier? Investigating Language Representation in\nMultilingual Models\nHellina Hailu Nigatu1∗, Atnafu Lambebo Tonja2,3,4∗, Jugal Kalita2\n1 University of California Berkeley, USA, 2 University of Colorado, Colorado Springs, USA,\n3 Instituto Polit´ecnico Nacional, Mexico, 4 Lelapa AI\nCorrespondence: hellina nigatu@berkeley.edu, atonja@uccs.edu\nAbstract\nMultilingual Language Models offer a way to\nincorporate multiple languages in one model\nand utilize cross-language transfer learning\nto improve performance for different Natural\nLanguage Processing (NLP) tasks. Despite\nprogress in multilingual models, not all lan-\nguages are supported as well, particularly in\nlow-resource settings.\nIn this work, we in-\nvestigate the linguistic representation of dif-\nferent languages in multilingual models. We\nstart by asking the question which languages\nare supported in popular multilingual models\nand which languages are left behind. Then,\nfor included languages, we look at models’\nlearned representations based on language fam-\nily and dialect and try to understand how mod-\nels’ learned representations for (1) seen and (2)\nunseen languages vary across different lan-\nguage groups. In addition, we test and ana-\nlyze performance on downstream tasks such\nas text generation and Named Entity Recogni-\ntion. We observe from our experiments that\ncommunity-centered models—models that fo-\ncus on languages of a given family or geo-\ngraphical location and are built by communities\nwho speak them—perform better at distinguish-\ning between languages in the same family for\nlow-resource languages. Our paper contributes\nto the literature in understanding multilingual\nmodels and their shortcomings and offers in-\nsights on potential ways to improve them.\n1\nIntroduction\nWhile we have seen improvements in state-of-the-\nart performance in various NLP tasks by multilin-\ngual models (Conneau et al., 2019; Doddapaneni\nand Kumar., 2021), there is a disparity in which\nlanguages are actively studied. The field of NLP\nhas largely been Anglocentric, with a large portion\nof the world’s languages, particularly low-resource\nlanguages, not being covered in the literature (Joshi,\n2020; Bender, 2019; Talat et al., 2022).\n∗Equal contribution.\nFor languages that have been included in pop-\nular multilingual models, performance is not the\nsame for all languages (Joshi, 2020). Low-resource\nlanguages suffer in performance even when they\nare included in multilingual models. Several works\n(Hangya and Fraser., 2022; Wang and Roth, 2020;\nPfeiffer and Ruder., 2021; Schuster and Lewis,\n2019) have proposed methods for improving per-\nformance for low-resource languages. Previous\nwork (Doddapaneni and Kumar., 2021) presents\nan argument that languages might benefit from be-\ning included in multilingual models as the models\nlearn language-independent feature representations,\nwhile it concludes that the question of the benefit of\nmultilingual training for a given language remains\nopen. Previous works also show that multilingual\nmodels might suffer from “negative interference”\n(Zirui Wang, 2019) in both high-resource (Con-\nneau et al., 2019; Xu Tan and Liu., 2019) and low-\nresource (Zirui Wang, 2020) settings.\nIn this work, we first look at the linguistic diver-\nsity in multilingual models. We then investigate the\nembeddings for different languages in multilingual\nmodels and show how the representations affect\ndownstream performance in language identifica-\ntion. For our analysis, we used three autoregressive\nand five autoencoder models. First, we looked at\n2D visualizations of learned representations for all\nmodels. Then, we evaluated autoregressive mod-\nels’ performance on text generation and the autoen-\ncoder models by language classification based on\nlearned representations and performance on Named\nEntity Recognition (NER). We base our analysis\non (1) language family, (2) dialects, and (3) writ-\ning scripts, with a focus on low-resource language\nsettings.\n2\nRelated Works\nIn efforts to include more of the world’s languages,\nprevious works have built multilingual language\nmodels over the years. While models with larger\narXiv:2310.13228v1  [cs.CL]  20 Oct 2023\nnumbers and more diverse sets of languages have\nshown commendable performance in several NLP\ntasks (Conneau et al., 2019; Zhang et al., 2021),\nwe have also seen community-centered models im-\nprove upon task performance (Dossou et al., 2022;\nDabre et al., 2022). Previous work (Conneau et al.,\n2019) hypothesizes that models might suffer from\n“curse of multilinguality”, which describes how for\na given model size, increasing the number of lan-\nguages does not improve performance for individ-\nual languages after a certain point. With this in\nmind, we ask the question: Do Multilingual Lan-\nguage Models with fewer, community-centered lan-\nguages learn better representations of different lan-\nguages depending on language families, dialects,\nand writing scripts?\nPrevious works have analyzed how pre-trained\nlanguage models learn representations for differ-\nent languages using probing tasks (Choenni and\nShutova, 2020; Eva Schlinger, 2019; Jindrich Li-\nbovicky, 2019) as well as investigating the geome-\ntry of sub-spaces (Chang and Bergen, 2022; Rajaee\nand Pilehvar, 2022). One previous work (Chang\nand Bergen, 2022) focuses on understanding multi-\nlingual language models’ overall embedding space\ngeometry and identifies axes for language-sensitive\nand language-neutral features. In our work, we\nare interested in how different languages are rep-\nresented in multilingual language models with a\ncloser look at how different language families, di-\nalects, and writing scripts are represented.\n3\nModels and Data\nWe chose models from two categories for our ex-\nperiments: autoencoder and autoregressive models.\nIn this section, we give descriptions of the models\nwe chose and their training data. Table 1 gives a\nsummary of the models we used with information\non their training data, model size, and languages\ncovered.\n3.1\nAutoencoder Models\nAutoencoder models are trained to recreate their\ninputs from an internal latent representation of the\ninput (Dor Bank, 2021). Many such models use\npartial input masking during training, known as\nmasked language modeling (MLM). In this paper,\nwe focus on transformer-based autoencoders. In\nparticular, we look at BERT (Devlin et al., 2018)\nand RoBERTA (Liu et al., 2019) models, which\nuse MLM.\nXLM-R (Conneau et al., 2019) is a multilingual\nTransformer-based MLM trained on the Common\nCrawl data for 100 languages. To balance data\nbetween English and other languages, the XLM-R\nuses one dump for English and 12 dumps for all\nother languages.\nBERT multilingual (Devlin et al., 2018) is a\npre-trained model on the top 104 languages with\nthe largest Wikipedias using an MLM objective. It\nis designed to pre-train deep bidirectional represen-\ntations from unlabeled text by jointly conditioning\non both the left and right context in all layers.\nAfroLM (Dossou et al., 2022) is a multilin-\ngual language model pre-trained from scratch on\n23 African languages using a self-active learning\nframework with an MLM objective.\nIndicBERT (Doddapaneni et al., 2022) is a mul-\ntilingual ALBERT (Lan et al., 2019) model pre-\ntrained exclusively on 12 major Indian languages.\nIt is pre-trained on a novel monolingual corpus of\naround 9 billion tokens and subsequently evaluated\non a set of diverse tasks.\nAraBERT (Antoun et al., 2020) is an Arabic pre-\ntrained language model based on BERT (Devlin\net al., 2018) and trained on 70 million sentences\nfollowing the original BERT pre-training objective.\n3.2\nAutoregressive Models\nAutoregressive models are sequential models that\nuse the previous tokens to predict the next token.\nTransformer-based autoregressive models use a\ntransformer decoder and causal masked attention\nto learn sequential representation regressively.\nGPT-3 (Brown et al., 2020) is a generative model\nwith 175 billion parameters. It has been used in\nseveral downstream tasks and to demonstrate in-\ncontext learning.\nLLaMa is an autoregressive model that is\ntrained on “publicly available datasets exclusively”\n(Touvron et al., 2023).\nBLOOM (BigScience, 2022a) is an autoregres-\nsive model that is trained on the ROOTS corpus\n(BigScience, 2022b).\n4\nLanguage Diversity\nBefore starting our experiments to understand how\nmultilingual models learn representations for differ-\nent languages, we first looked at which languages\nare included and which languages are excluded\nfrom mainstream NLP research. From the mod-\nels discussed in Section 3, we selected XLM-R\nModel Type\nModel Name\nSize\nLanguages\nData\nAutoencoders\nXLM-R\n270M\n100\nFiltered CommonCrawl\nAfroLM\n270M\n23\nJW300, Bible, News\nBERT-multilingual\n110M\n104\nBooksCorpus and English Wikipedia\nAraBERT\n110M\n1\nArabic news, Arabic Corpus, OSIAN: the Open\nSource International Arabic News Corpus\nIndicBERT\n12M\n12\nAI4Bharat’s monolingual corpus\nAutoregressive\nGPT-3\n175B\n119\nFiltered CommonCrawl, WebText (Liu and Curran,\n2006), e-books, English Wikipedia\nLLaMA\n65B\n20\nEnglish CommonCrawl, C4, Github, Wikipedia,\nGutenberg and Books3, ArXiv, Stack Exchange\nBLOOM\n560M\n59\nROOTS (BigScience, 2022b)\nTable 1: Models and their parameter size, number of languages included, and their training data sources (Conneau\net al., 2019; Dossou et al., 2022; Devlin et al., 2018; Antoun et al., 2020; Doddapaneni et al., 2022; Brown et al.,\n2020; Touvron et al., 2023; BigScience, 2022b). In our experiments, we only used base or small models due to\ncomputational resource limitations. For GPT-3, we obtained the language count from github.\nand LLaMA from generic multilingual models and\nAfroLM from community-centered models to eval-\nuate their linguistic diversity across countries. First,\nwe look at the linguistic diversity of the community-\ncentered model, AfroLM. In Fig 1 (a), we show a\nmap representing the number of languages in each\nAfrican country represented by AfroLM. We con-\ntrast these results with Fig 1 (b), where we rep-\nresent the same data by dividing the number of\nlanguages represented in the model by the number\nof languages spoken in the country. We see in Fig-\nure 1 that a large number of languages per country\nare still left behind, even in cases where the models\nare focused on a single continent.\nNext, we look at XLM-R which has been a pop-\nular choice in multilingual studies (Choudhury and\nDeshpande, 2021). We see that 55 out of the 100\nlanguages included in XLM-R are Indo-European\nin comparison with 2 each from Niger-Congo and\nKra-Dai languages included in the model. In Fig.\n2, we use the Indo-European language family tree\nfrom (Young, 2015) to show the languages that\nare represented from that family by XLM-R. For\nLLaMA, Table 2, shows the languages included\nin the training data, which are exclusively in the\nEuropean branch of the Indo-European family with\nthe exception of Hungarian which is a member of\nthe Uralic language family.\n5\nMethods\n5.1\nEvaluation dataset\nWe used publicly available datasets to evaluate\nmultilingual models. For embedding space rep-\nresentation and language classification, we used\nFlores (NLLB Team, 2022) dataset except for the\nTigrinya dialect evaluation. To evaluate embedding\nDataset\nSampling prop\nLanguages\nCommonCrawl\n67%\nEnglish\nC4\n15%\nEnglish\nGithub\n4.5%\nEnglish\nWikipedia\n4.5%\nEnglish, Bulgarian\nCatalan, Czech\nDanish, German\nSpanish, French\nCroatian, Hungarian\nItalian, Dutch\nPolish, Portuguese\nRomanian, Russian\nSlovenian, Serbian\nSwedish, Ukrainian\nBooks\n4.5%\nEnglish\nArXiv\n2.5%\nEnglish\nStackExchange\n2%\nEnglish\nTable 2: Pre-training data of LLaMA (Touvron et al.,\n2023). A large portion of the data is English, with other\nEuropean languages included collectively making up\nless than 4.5% of the total data.\nspace representation and language classification\nfor the Tigrinya dialect, we used a dataset from\n(Haileslasie et al., 2023). To evaluate NER, we\nused MasakhaNER (Adelani et al., 2021) dataset\nfor Bantu languages and WikiANN (Pan et al.,\n2017) dataset for Romance languages.\nFor the text generation task, we designed our\nprompts in English in five categories: Bible, World\nKnowledge, News, Generic, and Bias. We chose\nthese categories (1) to explore what pre-trained\nLLMs represent (in World Knowledge, Bias, and\nNews), (2) to get closer to training data for low-\nresource languages (in Bible), and (3) to observe\nthe behaviors of pre-trained LLMs in generic situa-\ntions in different languages (in Generic). We trans-\nlated the English prompts into Amharic, Tigrinya,\nSpanish, Italian, and French. For Spanish, Ital-\nian, and French, we used Google Translate. For\n(a) AfroLM - languages per\ncountry\n(b) AfroLM - languages out of\ntotal languages spoken in the\ncountry\nFigure 1: Visualization of languages included in AfroLM. In Fig. 1 (a), we show the languages that are included\nin the model per country. We observe some geographic diversity in countries included: East, West, and Southern\nAfrican countries included in the model. However, looking at Fig. 1 (b), dividing the number of languages included\nin AfroLM by the number of languages that are spoken in the country gives us the contrast that even in cases where\nmodels are concentrated on a single continent, many of the languages are left unrepresented.\nFigure 2: Language family tree for Indo-European lan-\nguages showing how many of the languages in this fam-\nily are included in XLM-R. We used the size of the\noranges to give an intuition of how big a language is\nbased on the number of speakers. (Image is not drawn\nto scale.) We see more concentration on the European\nside of the tree than on the Indo-Iranian side.\nTigrinya, we used Lesan.ai (Hadgu et al., 2022)\nwith manual corrections after translation and for\nAmharic, we translated manually. The prompts we\nused are in Appendix A.\n5.2\nEmbedding Space Extraction and\nVisualization\nDistinguishing between Languages through Vi-\nsualization:\nWe wanted to understand how differ-\nent models represent different languages in multi-\nlingual settings. We extracted the hidden states\nfrom the models in our experiments and used\nUMAP (Leland McInnes, 2020) to visualize the\nrepresentations. Following previous work (Devlin\net al., 2018), we used the hidden state vectors of\nthe first token in each sentence as the sentence em-\nbedding for all layers for autoencoder models. We\nalso experimented with averaging different hidden\nstates of all tokens and found that while it reduced\nwithin cluster distances (distances between individ-\nual points in already formed clusters), it retained\nthe overall cluster formations. For LLaMA and\nBLOOM, we used the hidden state vector of the\nlast token in each sentence as the sentence embed-\nding for all layers, as was done in previous work\n(Neelakantan et al., 2022). For GPT-3, we used\nthe embedding models from OpenAI’s API end-\npoints; we used the text-embedding-ada-002 and\ntext-similarity-davinci-001 models. From Afro-\nAsiatic language family, we choose Semitic and\nCushtic languages. From Niger-Congo language\nfamily, we chose Bantu languages. From Indo-\nEuropean language family, we choose Indo-Iranian\nand Romance languages.\nDistinguishing between Languages through\nClassification:\nTo corroborate the results we ob-\nserved in the visualization of the embedding spaces,\nwe used K-Means clustering on the learned repre-\nsentations we extracted from the pre-trained mod-\nels to test to what extent different models can dif-\nferentiate among languages. In Section 6.2, we\ndiscuss the result of language classification for dif-\nferent language groups and models.\n5.3\nDownstream Tasks\nScholars have previously evaluated the perfor-\nmance of Large Language Models (LLMs) in differ-\nent downstream tasks (Adelani et al., 2021, 2023;\nDione et al., 2023). Our interest is in understanding\nhow multilingual models learn and represent lan-\nguages from different language families, dialects,\nand writing scripts. Hence, in addition to investi-\ngating the models’ learned representations through\nvisualization and classification, we evaluated how\nthese models perform in downstream tasks across\nlanguages, focusing on two tasks: NER and text\ngeneration.\nNamed Entity Recognition (NER):\nNER is an\nInformation Extraction task which serves as one\nof the “fundamental building blocks of complex\nNLP tasks” (Singh, 2018). We selected NER task\nto understand how selected autoencoder models\nperform tasks that require deeper language under-\nstanding capabilities for models. In this work, we\nevaluated Bantu and Romance family languages,\nand we discuss the results in Section 6.3.\nText Generation:\nFor autoregressive models, we\nevaluated downstream applications by prompting\nthe models in 6 languages from our experiment\nand using GEEZSwitch (Gaim and Park, 2022) to\nclassify the generated text. For this experiment,\nwe chose GPT-3 and BLOOM. We prepared three\nprompts in 5 categories for a total of 15 prompts\nper language. More details about the prompts are in\nAppendix A. We then generated text eight times for\neach prompt for a total of 120 text generations per\nlanguage per model and 1440 generations overall.\n6\nResults\n6.1\nVisualizing Models’ Embedding Spaces\n6.1.1\nLanguage family\nAs detailed in Section 4, the language families rep-\nresented in the multilingual models are predomi-\nnantly Indo-European. We wanted to investigate\nhow multilingual models learned representations\nvary for different language families both for seen\nand unseen languages.\nFor\nRomance\nlanguages,\nXLM-R\nshows\nlanguage-neutral semantic clusters in the middle\nlayers, with language-specific clusters in the last\nlayer (in Appendix D.3 Fig. 22 (b)). In XLM-R,\nBantu language representations were mixed in pairs\nfor all languages except Swahili which formed\nsomewhat of an independent cluster (in Appendix\nD.3, Fig. 22 (a)). AfroLM showed independent\nclusters for 3 of the Bantu languages with Xohsa\nand isZulu mixing (in Appendix D.3 Fig. 21). For\nSemitic languages, we observed, except for XLM-\nR, AfroLM and GPT models, all other language\nmodels had mixed representations for Amharic and\nTigrinya in both autoregressive (in Fig. 3) and au-\ntoencoder (in Fig. 5) models. Looking more in\ndetail at the representations that are close together\nfor Amharic and Tigrinya in AfroLM, we observe\nthat the sentences are translations of each other (in\nFig. 4).\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 3: Autoregressive models’ learned representa-\ntions for Semitic languages. GPT-3 embeddings show\nindependent clusters for all languages with some mix\nbetween Arabic (orange) and Amharic(blue). LLaMa\nand BLOOM on the other hand, separate Arabic as a\nseparate cluster but mix Tigrinya(green) and Amharic.\n6.1.2\nDialect\nIn addition to language families, we wanted to in-\nvestigate how multilingual language models learn\nrepresentations for languages with different di-\nalects. We selected Tigrinya and Arabic languages\nfrom the Semitic language group for this experi-\nment. For Arabic, we chose four dialects: Morocco,\nLevantine, Egypt, and Standard Modern Arabic,\nbased on the numbers of speakers. For Tigrinya di-\nalects, we choose three dialects used in (Haileslasie\net al., 2023). Our result shows that for the Arabic\ndialect, except AraBERT, all the models mixed\nthe representations for all dialects. The AraBERT\nmodel clustered some sentences from the Egyp-\ntian dialect independently but mixed the rest of the\ndialects (in Appendix D Fig. 19). Similarly, all\nthe models mix the representations for Tigrinya\ndialects, as shown in Figure 6.\n6.1.3\nWriting Script\nWe also evaluated how multilingual models rep-\nresent languages that use different writing scripts.\nFor this experiment, we selected Amharic, Ara-\nFigure 4: Taking a closer look at AfroLM representa-\ntions for Semitic languages, we observe that in cases\nwhere Tigrinya representations are mixed with the\nAmharic cluster, the sentences are closer to their trans-\nlation pairs from the dataset. In the instance that an\nAmharic sentence is mixed with the Tigrinya embed-\nding, there is a 6-letter English acronym in the sentence.\nbic, Spanish, Chinese, and Hindi, which use Geez\n(Ethiopic), Arabic, Latin, Chinese (Han), and Hindi\n(Devanagari) scripts, respectively. Our result in Fig-\nures 7 and 8 show that all models except AraBERT\nform somewhat distinct language-specific clusters.\nWhile XLM-R and mBERT clustered Amharic\nseparately, other languages were clustered near\neach other with some cross-language mixing. The\nAraBERT model independently clustered Spanish\nand Arabic while mixing the other languages. As\nshown in Figure 8, GPT-3 models showed some\nlanguage-specific clusters with cross-language mix-\ning, while LLaMA and BLOOM had separate clus-\nters for each language.\n6.2\nLanguage Classification\nTable 4 shows the language classification results\nfor autoencoder models across different language\ngroups. As discussed in Section 6.1, we are inter-\nested in evaluating how multilingual models clas-\nsify languages regardless of their writing script,\nlanguage families, and dialects. We used F1 score\nto evaluate the clustering performance. As shown\nin Table 4, we observed similar differences be-\ntween the models in language classification tasks as\nseen in the visualization experiments (Section 6.1).\nModels that showed separate clusters in the embed-\nding space across language families and writing\nscripts showed the highest F1 score than the rest\nof the models. For Semitic and Cushtic languages,\nAfroLM classified all the languages correctly with\nan F1-score of 100%, while for Indo-Iranian lan-\nguage classification, IndicBERT classified all the\nlanguages correctly with F1-score of 100%. For\nBantu language classification, AfroLM showed the\nhighest performance with F1-score of 79% while\nIndicBERT showed the lowest F1-score of 13%.\nFor both Arabic and Tigrinya dialects, all the mod-\nels show an F1-score below 40%, this shows all the\nmodels are struggling to classify different dialects\nwithin the languages. For different writing scripts,\nAraBERT showed F1-score of 62% while XLM-R\nshowed the lowest F1-score of 37%.\nModels\nLanguage\nCorrect Language\nGenerated (%)\nGPT-3\nSpanish\n95.83\nFrench\n91.67\nItalian\n91.67\nEnglish\n90.83\nAmharic\n76.66\nTigrinya\n45.00\nBLOOM\nSpanish\n93.33\nFrench\n100\nItalian\n51\nEnglish\n100\nAmharic\n80.00\nTigrinya\n43.33\nTable 3: Accuracy of BLOOM and GPT-3 in generating\ntext in the same language it was prompted with. High-\nresource languages have over 90% accuracy except for\nItalian in BLOOM\n6.3\nNamed Entity Recognition\nTable 5 shows NER results for Bantu and Romance\nlanguage families. In both NER tasks, we observed\nidentical distinctions between the models as seen\nin the visualization experiments (Section 6.1). For\nthe Bantu languages, AfroLM outperformed other\nmodels except in Kinyarwanda, while generic mod-\nels outperformed community-centered models for\nthe Romance languages except for French.\n6.4\nText Generation\nIn Table 3, we show the results of language classi-\nfication for the text generation task. GPT-3 genera-\ntions are in the same language of the prompt above\n90% of the time for high-resourced languages. A\ndeeper look at the generations that were in a dif-\nferent language for English reveals that for the\nprompt of a verse from the Bible, GPT-3 generates\nthe exact name of the verse “(John 3:16)” which\nGEEZSwitch detects as German language, account-\ning for 8 out of 12 of the wrong language identifi-\ncations in English. Two of the remaining wrongful\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 5: Autoencoder models learned representations for three Semitic languages. We observe that the representa-\ntions for Tigrinya and Amharic (which use the same writing script) are mixed in all models except XLM-R and\nAfroLM.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 6: Autoencoder representations for Tigrinya dialects. All models mix the representations for all three dialects.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 7: Autoencoder learned representations for languages with different writing scripts. Except for AraBERT,\nother models form somewhat distinct language-specific clusters, while AraBERT mixes Amharic, Chinese, and\nHindi and has separate clusters for Arabic and Spanish.\nLanguage groups\nModels (F1-score)\nGeneral\nCommunity-Centric\nXLM-R\nmBERT\nAfroLM\nAraBERT\nIndicBERT\nSemitic\n0.68\n0.52\n1.0\n0.61\n0.62\nCushetic\n0.52\n0.41\n1.0\n0.33\n0.37\nBantu\n0.37\n0.23\n0.79\n0.18\n0.13\nRomance\n0.42\n0.24\n0.38\n0.34\n0.21\nIndo-Iranian\n0.33\n0.30\n0.71\n0.2\n1.0\nDialects\nArabic dialects\n0.25\n0.23\n0.25\n0.32\n0.26\nTigrinya\n0.23\n0.30\n0.39\n0.31\n0.26\nWriting script\nDifferent writing script\n0.37\n0.49\n0.55\n0.62\n0.53\nTable 4: Language classification F1-scores for K-Means clustering of the embedding space for autoencoder models.\nHere, we see that community-centered models perform better at distinguishing between languages they focus on,\nwhile none of the models perform well in dialectic and writing script categories.\ndetection results were due to GEEZSwitch detect-\ning a list of African countries in English as Swahili.\nThe remaining wrongful detection was a mix of\nAmharic and English for the prompt “I am a tourist.\nTell me common phrases in Amharic.”\nWhile GPT-3 does decently on Amharic, closer\nanalysis reveals that a 14% of the generated text,\nwhich was classified as Amharic, also includes boil-\nerplate code and a mix of English. For Tigrinya,\n51.51% of the mis-generated text is in Amharic,\nLanguage Family\nLanguage\nModels(F1-score)\nGeneral\nCommunity-Centric\nXLM-R\nmBERT\nAfroLM\nAraBERT\nIndicBERT\nBantu\nzul\n84.6\n81.7\n86.3\n76.9\n67.2\nxho\n87\n85\n87.5\n75.8\n75.3\nsna\n93.6\n92.4\n94.4\n73\n83.4\nswa\n87.5\n86.3\n87.6\n78.9\n79.9\nkin\n73.9\n70.9\n72.8\n69.3\n71.1\nRomance\nfra\n88.95\n90.66\n87.89\n91.15\n91.97\nspa\n94.58\n92.03\n80.12\n81.71\n83.73\ncat\n91.72\n95.67\n82.76\n85.24\n84.48\nita\n90.43\n91.91\n80.28\n83.82\n83.25\nTable 5: NER Performances: F1-scores on languages test sets, these results cover all four tags (PER, ORG, LOC,\nDATE) in the MasakhaNER dataset for Bantu languages and WikiANN dataset for Romance languages.\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 8: Autoregressive learned representations for\nlanguages with different writing scripts. Here, LLaMA\nand BLOOM have more distinct language specific clus-\nters while GPT models show some mix across language\nclusters.\nand 45.45% is in English. The remaining 3% are\nattempts at romanized Tigrinya, which were mis-\nclassified as German and Swahili. In Appendix\nB, Figure 9, we show examples of text that were\ndetected as generated in a language other than the\nprompt language. We want to emphasize that these\nresults are conditioned on the fact that we are look-\ning ONLY at the language of the generated text;\nNONE of the Amharic and Tigrinya generations,\neven when they are in the same language as the\nprompt, are coherent sentences or phrases.\nFor BLOOM text generation performance, we\nsee in Table 3 that generated text is in the same\nlanguage as the prompt for 100 % of the time for\nEnglish and French and 80% for Amharic. In con-\ntrast to GPT-3, the generated text is in the same\nlanguage of the prompt 51% of the time for Italian,\nwith Spanish accounting for 40.67% of the mis-\ngenerations, Catalan and French each accounting\nfor 27.11% of the mis-generations, and English\naccounting for 5.08% of the mis-generation. In\nAppendix B, Figure 10, we go into further depth\non some examples of generations in Tigrinya and\nAmharic, highlighting the existence of artifacts that\nseem to be inherited from web interfaces.\n7\nDiscussion\n7.1\nOn Inclusion of Diverse Languages and\nLanguage Families\nOur analysis of linguistic diversity in popular mul-\ntilingual models aligns with criticism of previous\nworks that a large proportion of the world’s lan-\nguages remain understudied. We observe a skew\ntowards Indo-European languages, with a heav-\nier skew towards the European side of the fam-\nily tree (Section 4). This indicates there is still a\nhuge room for improvement in the NLP commu-\nnity, particularly in encouraging community-driven\nresearch for the inclusion of a more diverse set of\nlanguages, dialects, and writing scripts. Looking\nat the community-centered models, we see there\nis greater diversity and more inclusion for low-\nresource languages, though there is still a long way\nto go (Section 4). Encouraging research driven by\ncommunities of such languages could allow the\nNLP community to benefit from more diversity\nper community, which collectively could result in\ngreater diversity overall.\n7.2\nAre Community-Centered Language\nModels Better at Distinguishing between\nLanguages?\nFrom the visualizations of the learned represen-\ntations for different languages (Section 6.1) and\nthe language classification for autoencoder models\n(Section 6.2), we observe that community-centered\nmodels’ representations are more distinct for the\nlanguages they focus on. Looking at Semitic lan-\nguages, only AfroLM and XLM-R had separate\nclusters for each language (Amharic, Arabic, and\nTigrinya), while all other models put Tigrinya and\nAmharic in the same cluster (Fig. 5). For AfroLM,\nwe see two Tigrinya sentences that were placed\ncloser to the Amharic cluster are closer to their\ntranslation pairs from the dataset (Fig. 4) while\nXLM-R mixed representations were not explain-\nable as such. We see this behavior in visualizations\nof representations for other language families in\nAppendix D.\nFor autoregressive models, we did not have\ncommunity-centered models, as defined in our ab-\nstract, to compare with. However, we looked at\nthe output of the text generated from GPT-3 and\nBLOOM models. We looked at 6 languages and\nobserved that Amharic and Tigrinya are mixed in\nthe generated text, while the generated text was not\ncoherent for either of the languages. There is still\na long way to go in terms of text generation for\nlow-resource languages, starting with models that\nrespond in the same language as the prompt.\n7.3\nSame Language, Different Dialects\nWe also looked at learned representations for lan-\nguages with different dialects. We observe from\nour visualizations and language classification ex-\nperiments that, for both Tigrinya and Arabic, the\nlearned representations form no particular cluster\ndepending on dialect for any of the models for\nTigrinya with a small cluster observed for Egyptian\nArabic in AraBERT representations (Section 6.1.2).\nThis shows there is huge room for improvement in\nthe NLP space for understanding and accommodat-\ning dialectical differences.\n7.4\nLearned Representations for Unseen\nLanguages\nIn our experiments, we also included languages\nthat are not seen by the models in training. We\ndid this because (1) including languages that all\nmodels have in common would leave a lot of low-\nresource languages behind, and (2) we wanted\nto observe how models deal with out-of-domain\nlanguages. From our visualizations, we observe\nthat some models cluster unseen languages based\non writing scripts. For example, for Semitic lan-\nguages, LLaMa, BLOOM, mBERT, AraBERT, and\nIndicBERT clustered Tigrinya and Amharic, lan-\nguages which both use the Ge’ez script, together\nand formed a separate cluster for Arabic (in Fig. 3\nand Fig. 5). AfroLM and XLM-R formed language-\nspecific clusters for all three languages even though\nTigrinya is unseen for both models while Amharic\nis seen.\n8\nConclusion\nIn this work, we investigated linguistic diversity,\nthe learned representation of different languages,\nand the downstream task performance of multilin-\ngual models. We observe from our experiments that\ncommunity-centered language models perform bet-\nter at distinguishing among languages in the same\nfamily for low-resource languages. We also ob-\nserve that there is still a long way to go in accommo-\ndating dialectical differences in NLP. Our work con-\ntributes to understanding multilingual models, and\nwe hope to see future work with more community-\ncentered models and more diverse languages.\nLimitations\nOur study is limited by the models included in the\nexperiments and the languages available in the data\nwe used. For instance, after we submitted our pa-\nper, models like GPT-SW3 (Ariel Ekgren, 2023)\nand Jais (MBZUAI, 2023) presented community-\ncentered autorgressive models. Future work can\ndo a more comprehensive study with more lan-\nguage models and more diverse data included. Fu-\nture work can also dig deeper into particular points\nraised in our work, such as dialectical diversity or\nthe effects of writing systems in language model\nrepresentations.\nAdditionally, we note that using dimension re-\nduction techniques to understand representation\nmight lead to loss of some separate spaces which\nare collapsed down. In our work, we used clus-\ntering techniques on top of learned representations\nand downstream task performance to further corrob-\norate our observations. Future work could also in-\nvestigate difference between community-centered\nand generic models through an investigation of the\ngeometric space of the models. Additionally, while\nwe showed some correlation between separate clus-\nter formation in visualizations and downstream per-\nformance (Section 7.2), it is still unclear if there\nis a causal relationship between models that have\nseparate language clusters and their downstream\nperformance. Future work could build upon our\nresults to investigate whether or not a causal rela-\ntionship exists.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig,\nDaniel D’souza, Julia Kreutzer, Constantine Lignos,\nChester Palen-Michel, Happy Buzaaba, Shruti Rijh-\nwani, Sebastian Ruder, et al. 2021.\nMasakhaner:\nNamed entity recognition for african languages.\nTransactions of the Association for Computational\nLinguistics, 9:1116–1131.\nDavid Ifeoluwa Adelani, Marek Masiak, Israel Abebe\nAzime, Jesujoba Oluwadara Alabi, Atnafu Lam-\nbebo Tonja, Christine Mwase, Odunayo Ogundepo,\nBonaventure FP Dossou, Akintunde Oladipo, Doreen\nNixdorf, et al. 2023. Masakhanews: News topic\nclassification for african languages. arXiv preprint\narXiv:2304.09972.\nWissam Antoun,\nFady Baly,\nand Hazem Hajj.\n2020.\nArabert:\nTransformer-based model for\narabic language understanding.\narXiv preprint\narXiv:2003.00104.\nFelix Stollenwerk Joey ¨Ohman Tim Isbister Evan-\ngelia Gogoulou Fredrik Carlsson Alice Heiman Judit\nCasademont Magnus Sahlgren Ariel Ekgren, Amaru\nCuba Gyllensten. 2023. Gpt-sw3: An autoregres-\nsive language model for the nordic languages. arxiv\npreprint arxiv:2305.12987.\nEmily M Bender. 2019. The# benderrule: On nam-\ning the languages we study and why it matters. the\ngradient (2019).\nBigScience. 2022a. Bloom: A 176b-parameter open-\naccess multilingual language model. arxiv preprint\narxiv:2211.05100.\nBigScience. 2022b. Bloom: A 176b-parameter open-\naccess multilingual language model. arxiv preprint\narxiv:2211.05100.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nZhuowen Tu Chang, Tyler A and Benjamin K Bergen.\n2022. The geometry of multilingual language model\nrepresentations. Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning.\nRochelle Choenni and Ekaterina Shutova. 2020. What\ndoes it mean to be language-agnostic? probing multi-\nlingual sentence encoders for typological properties.\narxiv preprint arxiv.:2009.12862.\nMonojit Choudhury and Amit Deshpande. 2021. How\nlinguistically fair are multilingual pre-trained lan-\nguage models?\nIn Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, pages\n12710–12718.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nRaj Dabre, Himani Shrotriya, Anoop Kunchukuttan,\nRatish Puduppully, Mitesh Khapra, and Pratyush Ku-\nmar. 2022. IndicBART: A pre-trained model for indic\nnatural language generation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 1849–1863, Dublin, Ireland. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nCheikh M Bamba Dione, David Adelani, Peter Nabende,\nJesujoba Alabi, Thapelo Sindane, Happy Buzaaba,\nShamsuddeen Hassan Muhammad, Chris Chinenye\nEmezue, Perez Ogayo, Anuoluwapo Aremu, et al.\n2023. Masakhapos: Part-of-speech tagging for typo-\nlogically diverse african languages. arXiv preprint\narXiv:2305.13989.\nGowtham\nRamesh\nMitesh\nM.\nKhapra\nAnoop\nKunchukuttan\nDoddapaneni,\nSumanth\nand Pratyush Kumar. 2021. A primer on pretrained\nmultilingual language models.\nSumanth Doddapaneni, Rahul Aralikatte, Gowtham\nRamesh, Shreyansh Goyal, Mitesh M. Khapra,\nAnoop Kunchukuttan, and Pratyush Kumar. 2022.\nTowards leaving no indic language behind: Build-\ning monolingual corpora, benchmark and models for\nindic languages. ArXiv, abs/2212.05409.\nRaja Giryes Dor Bank, Noam Koenigstein. 2021. Au-\ntoencoders. arxiv preprint arxiv:2003.05991.\nBonaventure FP Dossou, Atnafu Lambebo Tonja,\nOreen Yousuf, Salomey Osei, Abigail Oppong, Iyan-\nuoluwa Shode, Oluwabusayo Olufunke Awoyomi,\nand Chris Chinenye Emezue. 2022.\nAfrolm: A\nself-active learning-based multilingual pretrained lan-\nguage model for 23 african languages. arXiv preprint\narXiv:2211.03263.\nDan Garrette Eva Schlinger. 2019. How multilingual\nis multilingual bert?\nProceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics.\nWonsuk Yang Gaim, Fitsum and Jong C Park. 2022.\nGeezswitch: Language identification in typologically\nrelated low-resourced east african languages. Pro-\nceedings of the 13th Conference on Language Re-\nsources and Evaluation (LREC 2022).\nAsmelash Teka Hadgu, Abel Aregawi, and Adam Beau-\ndoin. 2022. Lesan–machine translation for low re-\nsource languages. In NeurIPS 2021 Competitions\nand Demonstrations Track, pages 297–301. PMLR.\nAsfaw Gedamu Haileslasie, Asmelash Teka Hadgu, and\nSolomon Teferra Abate. 2023. Tigrinya dialect iden-\ntification. In 4th Workshop on African Natural Lan-\nguage Processing.\nHossain Shaikh Saadi Hangya, Viktor and Alexander\nFraser. 2022. Improving low-resource languages in\npre-trained multilingual language models. Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing.\nAlexander Fraser Jindrich Libovicky, Rudolf Rosa.\n2019. How language-neutral is multilingual bert?\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nSebastin Santy Amar Budhiraja Kalika Bali Mono-\njit Choudhury. Joshi, Pratik. 2020. The state and fate\nof linguistic diversity and inclusion in the nlp world.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 6282–93.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations.\narXiv preprint\narXiv:1909.11942.\nJames Melville Leland McInnes, John Healy. 2020.\nUmap: Uniform manifold approximation and pro-\njection for dimension reduction.\narxiv preprint\narxiv:1802.03426.\nVinci Liu and James R Curran. 2006. Web text corpus\nfor natural language processing. In 11th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 233–240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMBZUAI. 2023. Jais: a new pinnacle in open arabic\nnlp.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005.\nJames Cross Onur C¸ elebi Maha Elbayad Kenneth\nHeafield Kevin Heffernan Elahe Kalbassi Janice\nLam Daniel Licht Jean Maillard Anna Sun Skyler\nWang Guillaume Wenzek Al Youngblood Bapi Akula\nLoic Barrault Gabriel Mejia Gonzalez Prangthip\nHansanti John Hoffman Semarley Jarrett Kaushik\nRam Sadagopan Dirk Rowe Shannon Spruit Chau\nTran Pierre Andrews Necip Fazil Ayan Shruti Bhos-\nale Sergey Edunov Angela Fan Cynthia Gao Vedanuj\nGoswami Francisco Guzm´an Philipp Koehn Alexan-\ndre Mourachko Christophe Ropers Safiyyah Saleem\nHolger Schwenk Jeff Wang NLLB Team, Marta R.\nCosta-juss`a. 2022. No language left behind: Scaling\nhuman-centered machine translation.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nIvan Vuli´c Iryna Gurevych Pfeiffer, Jonas and Sebastian\nRuder. 2021. Unks everywhere: Adapting multilin-\ngual language models to new scripts. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing.\nSara Rajaee and Mohammad Taher Pilehvar. 2022. An\nisotropy analysis in the multilingual bert embedding\nspace. In Findings of the Association for Computa-\ntional Linguistics: ACL.\nSonal Gupta Rushin Shah Schuster, Sebastian and Mike\nLewis. 2019. Cross-lingual transfer learning for mul-\ntilingual task oriented dialog. In Proceedings of the\n2019 Conference of the North America Association\nfor Computational Linguistics.\nSonit Singh. 2018.\nNatural language process-\ning for information extraction.\narXiv preprint\narXiv:1807.02383.\nZeerak Talat, Aur´elie N´ev´eol, Stella Biderman, Miruna\nClinciu, Manan Dey, Shayne Longpre, Sasha Luc-\ncioni, Maraim Masoud, Margaret Mitchell, Dragomir\nRadev, et al. 2022. You reap what you sow: On\nthe challenges of bias evaluation under multilingual\nsettings. In Proceedings of BigScience Episode# 5–\nWorkshop on Challenges & Perspectives in Creating\nLarge Language Models, pages 26–41.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nKarthikeyan K Stephen Mayhew Wang, Zihan and Dan\nRoth. 2020.\nExtending multilingual bert to low-\nresource languages. In Findings of the Association\nfor Computational Linguistics: EMNLP.\nDi He Tao Qin Zhou Zhao Xu Tan, Yi Ren and TieYan\nLiu. 2019.\nMultilingual neural machine transla-\ntion with knowledge distillation.\narXiv preprint\narXiv:1902.10461.\nHolly Young. 2015. A language family tree - in pictures.\nThe Guardian.\nXinsong Zhang, Pengshuai Li, and Hang Li. 2021. AM-\nBERT: A pre-trained language model with multi-\ngrained tokenization. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 421–435, Online. Association for Computa-\ntional Linguistics.\nBarnab´as P´oczos Jaime Carbonell Zirui Wang, Zi-\nhang Dai. 2019. Characterizing and avoiding nega-\ntive transfer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition.\nYulia Tsvetkov Zirui Wang, Zachary C. Lipton. 2020.\nOn negative interference in multilingual models:\nFindings and a meta-learning treatment.\narxiv\npreprint arxiv:2010.03017.\nA\nPrompts\nBible\n• In the beginning, God created the heavens and\nthe earth\n• For God so loved the world, that He gave His\nonly begotten Son, that whoever believes in\nHim shall not perish, but have eternal life\n• What is the verse Matthew 6:33?\nWorld Knowledge\n• I am a history student. Can you tell me about\nthe Battle of Adwa?\n• List 10 African countries.\n• Which African country was the first to gain\nindependence?\nNews\n• What is COVID-19?\n• How do I protect myself from COVID-19?\n• Who is more likely to get COVID-19?\nGeneric\n• Write an article about studying abroad.\n• Tell me about the different seasons in a year.\n• I am a tourist. Tell me common phrases in\nAmharic.\nBias\n• What characteristics do African people have?\n• What characteristics do European people\nhave?\n• Habiba was born and raised in Kombolcha,\nEthiopia. Habiba’s profession is\nB\nText Generation Samples\nC\nLanguage detail\nTable 6 shows the details of languages included in\nthe experiment.\nD\nEmbedding space\nIn this section, we present the UMAP plots for\nthe Romance, Cushtic, Bantu, and Indo-Iranian\nlanguage families and the Arabic dialects. We also\npresent the visualization for all layers in XLM-R\nand AfroLM for Bantu and Romance languages.\nD.1\nLanguage family\nIn this subsection, we present the visualizations\nfor Romance (Fig. 11 and Fig. 12), Cushtic (Fig.\n13 and Fig. 14), Bantu (Fig. 15 and Fig. 16) and\nIndo-Iranian (Fig. 17 and Fig. 18) families.\nD.2\nDialect\nIn this section, we present the visualizations for the\nembeddings for Arabic dialects (Fig. 19 and Fig.\n20).\nD.3\nEmbedding space for Bantu and\nRomance -All layers\nFigure 9: Examples of cases where the generated text was detected as in a language other than the prompt language.\nWe show the case where the English prompt has text generated that is detected as German. We also showcase where\nAmharic and Tigrinya prompts result in English text generations.\nFigure 10: Generation examples for Amharic and Tigrinya using BLOOM model. We see that the generations for\nTigrinya and Amharic have repeated characters that make for long strings; such generations are mostly misclassified\nas Bangla(byn). On the other hand, even in cases where the generated text is predicted to be in the same language as\nthe prompt language, there are artifacts like ‘emailuser’ that appear in the generated text for these languages.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 11: Last layer autoencoder embeddings for Romance languages. We see that XLM-R and AraBERT form\nsome language-specific clusters while the rest have mixed representations for the languages.\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 12: Learned representations for autoregressive models for Romance languages. All models form some\nlanguage-specific clusters while BLOOM forms the most distinct clusters for all lanaguegs.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 13: Last layer autoencoder representations for Cushtic languages. We observe clearer separation in XLM-R\nand AfroLM, while all other models have mixed representations.\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 14: Learned representations of autoregressive models for Cushtic languages. Except for GPT-Ada, all other\nmodels have mixed representations.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 15: Last layer autoencoder representations for Bantu languages. We observe language-specific clusters\nfor AfroLM with isZulu and Xhosa mixing, while XLM-R forms a clear cluster for Swahili and somewhat mixed\nclusters for the other languages. The other models mix all the languages.\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 16: Learned representations from autoregressive models for Bantu languages. We observe that LLaMA and\nBLOOM have mixed representations for the Bantu languages, while there is a small cluster for some of the Swahili\nsentences. While GPT-3 Ada separates out all languages, GT3-Davinci has some mix for Shona and Kinyarwanda\nand mix for isZulu and Xhosa.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 17: Last layer representations for Indo-Iranian languages. All models have mixed representations for all the\nIndo-Iranian languages.\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 18: Learned representations from autoregressive models for Indo-Iranian languages. Here, we see LLaMA\nand BLOOM have some language specific clusters with LLaMA mixing Bengali and Assamese. GPT-3 Davinci\nmixed Persian, Hindi, and Assamese, while GPT-3 Ada mixed Persian and Hindi and also mixed Gujarati and\nAssamese.\n(a) XLM-R\n(b) mBERT\n(c) AfroLM\n(d) AraBERT\n(e) IndicBERT\nFigure 19: Last layer representations for the Arabic dialects. All models have mixed representations for all dialects,\nwhile we observe a small cluster for Egyptian Arabic in AraBERT.\nLanguages\nFamily\nNo. of\nspeaker\nArabic (ara)\nAfro-Asiatic/ Semitic\n630M\nAmharic (amh)\nAfro-Asiatic/ Semitic\n57M\nTigrinya (tir)\nAfro-Asiatic/ Semitic\n9M\nAfaan Oromo (orm)\nAfro-Asiatic/ Cushitic\n37M\nSomali (som)\nAfro-Asiatic/ Cushitic\n22M\nFrench (fra)\nIndo-European/ Romance\n350M\nCatalan (cat)\nIndo-European/ Romance\n9.2M\nSpanish (spa)\nIndo-European/ Romance\n595M\nItalian (ita)\nIndo-European/ Romance\n68M\nHindi (hin)\nIndo-European/ Indo-Iranian\n592M\nIranian Persian (per)\nIndo-European/ Indo-Iranian\n81M\nAssamese (asm)\nIndo-European/ Indo-Iranian\n15M\nBengali (ben)\nIndo-European/ Indo-Iranian\n234M\nGujarati (guj)\nIndo-European/ Indo-Iranian\n56M\nisZulu (zul)\nNiger-Congo/Bantu\n26M\nSwahili (swa)\nNiger-Congo/Bantu\n88M\nisiXhosa (xho)\nNiger-Congo/Bantu\n19M\nKinyarwanda (kin)\nNiger-Congo/Bantu\n15M\nShona(sna)\nNiger-Congo/Bantu\n17.8M\nChinese (zho)\nIndo-Chinese (Sino-Tibetan)\n1.35 B\nTable 6: Languages included in the experiment.\n(a) GPT-3 Ada\n(b) GPT-3 Davinci\n(c) LLaMA\n(d) BLOOM\nFigure 20: Learned representations from autoregressive models for Arabic Dialects. Both BLOOM and LLaMA\nmodels have mixed representations for all dialects, while GPT models form dialect-specific clusters.\n(a) Bantu\n(b) Romance\nFigure 21: AfroLM representations on all layers for Bantu and Romance Languages. For Bantu languages, we\nobserve that Xhosa and isZulu occupy the same cluster while all other languages form independent clusters. We\nobserve that in the middle layers, the individual clusters are further away from each other and start spreading closer\nin the last layers. For Romance languages, we observe that all languages are mixed with no clear cluster formed for\nany of the languages.\n(a) Bantu\n(b) Romance\nFigure 22: XLM-R representations on all layers for Bantu and Romance Languages. We observe for Romance\nlanguages, the representations in the middle layers, particularly layers 8, 9, and 10, exhibit semantic clusters, and\nthe last layers show language-specific clusters. For Bantu languages, we observe some language-specific clusters in\nthe middle layers, with Kinrwanda and Shona representations mixing as well as Xhosa and isXulu forming a pair.\nOnly Swahili forms an independent cluster for Bantu languages.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-10-20",
  "updated": "2023-10-20"
}