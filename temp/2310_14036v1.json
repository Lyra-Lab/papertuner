{
  "id": "http://arxiv.org/abs/2310.14036v1",
  "title": "On discretisation drift and smoothness regularisation in neural network training",
  "authors": [
    "Mihaela Claudia Rosca"
  ],
  "abstract": "The deep learning recipe of casting real-world problems as mathematical\noptimisation and tackling the optimisation by training deep neural networks\nusing gradient-based optimisation has undoubtedly proven to be a fruitful one.\nThe understanding behind why deep learning works, however, has lagged behind\nits practical significance. We aim to make steps towards an improved\nunderstanding of deep learning with a focus on optimisation and model\nregularisation. We start by investigating gradient descent (GD), a\ndiscrete-time algorithm at the basis of most popular deep learning optimisation\nalgorithms. Understanding the dynamics of GD has been hindered by the presence\nof discretisation drift, the numerical integration error between GD and its\noften studied continuous-time counterpart, the negative gradient flow (NGF). To\nadd to the toolkit available to study GD, we derive novel continuous-time flows\nthat account for discretisation drift. Unlike the NGF, these new flows can be\nused to describe learning rate specific behaviours of GD, such as training\ninstabilities observed in supervised learning and two-player games. We then\ntranslate insights from continuous time into mitigation strategies for unstable\nGD dynamics, by constructing novel learning rate schedules and regularisers\nthat do not require additional hyperparameters. Like optimisation, smoothness\nregularisation is another pillar of deep learning's success with wide use in\nsupervised learning and generative modelling. Despite their individual\nsignificance, the interactions between smoothness regularisation and\noptimisation have yet to be explored. We find that smoothness regularisation\naffects optimisation across multiple deep learning domains, and that\nincorporating smoothness regularisation in reinforcement learning leads to a\nperformance boost that can be recovered using adaptions to optimisation\nmethods.",
  "text": "On discretisation drift and\nsmoothness regularisation in neural\nnetwork training\nMihaela Claudia Rosca\nA dissertation submitted in partial fulfillment\nof the requirements for the degree of\nDoctor of Philosophy\nof\nUniversity College London.\nDepartment of Computer Science\nUniversity College London\nJune 14, 2023\narXiv:2310.14036v1  [stat.ML]  21 Oct 2023\nii\nI, Mihaela Claudia Rosca, confirm that the work presented in this thesis is my\nown. Where information has been derived from other sources, I confirm that this\nhas been indicated in the work.\nAbstract\nThe deep learning recipe of casting real-world problems as mathematical optimisation\nand tackling the optimisation by training deep neural networks using gradient-based\noptimisation has undoubtedly proven to be a fruitful one. The understanding behind\nwhy deep learning works, however, has lagged behind its practical significance. We\naim to make steps towards an improved understanding of deep learning with a\nfocus on optimisation and model regularisation. We start by investigating gradient\ndescent (GD), a discrete-time algorithm at the basis of most popular deep learning\noptimisation algorithms. Understanding the dynamics of GD has been hindered by\nthe presence of discretisation drift, the numerical integration error between GD and\nits often studied continuous-time counterpart, the negative gradient flow (NGF).\nTo add to the toolkit available to study GD, we derive novel continuous-time flows\nthat account for discretisation drift. Unlike the NGF, these new flows can be used\nto describe learning rate specific behaviours of GD, such as training instabilities\nobserved in supervised learning and two-player games. We then translate insights\nfrom continuous time into mitigation strategies for unstable GD dynamics, by con-\nstructing novel learning rate schedules and regularisers that do not require additional\nhyperparameters. Like optimisation, smoothness regularisation is another pillar of\ndeep learning’s success with wide use in supervised learning and generative modelling.\nDespite their individual significance, the interactions between smoothness regularisa-\ntion and optimisation have yet to be explored. We find that smoothness regularisation\naffects optimisation across multiple deep learning domains, and that incorporating\nsmoothness regularisation in reinforcement learning leads to a performance boost\nthat can be recovered using adaptions to optimisation methods. We end by showing\nthat optimisation can also affect smoothness, as discretisation drift can act as an\nimplicit smoothness regulariser in neural network training.\nImpact Statement\nOur work focuses on understanding and improving components of deep learning\nsystems. As such, it provides two avenues for impact: first, by providing insights that\ncan be useful to other researchers in the field and second, by helping downstream\napplications that use such systems.\nInside deep learning academic research, this work provides a set of novel tools\nand insights concerning two main building blocks of deep learning systems, namely\noptimisation and models. Our insights include novel continuous-time flows to analyse\noptimisation dynamics, as well as a rethinking of the effect of smoothness constraints\nimposed on deep learning models. We have accompanied theoretical results with\nexperimental frameworks for analysis and validation, including approaches that can\nstabilise training and reduce hyperparameter sensitivity and thus computational\ncosts.\nFuture theoretical research directions based on our line of work include\nspecialising our theoretical results to specific classes of neural networks, further\nexpanding the family of available continuous-time flows capturing optimisation\ndynamics, and finding new beneficial or detrimental implicit regularisation forces\nin deep learning optimisation. Promising avenues for practical impact include new\nmethods for training deep learning models, and finding new approaches for model\nselection. Throughout this work, we provide insights across multiple deep learning\ndomains, including supervised learning, two-player games, and reinforcement learning.\nThe work included here has been presented at multiple conferences and published in\njournals.\nWhile we do not tackle specific applications directly, this work can have impact\noutside the research domain by improving the stability of deep learning models used\nin applications across multiple domains; examples include image classification and\ngeneration, as well as game play using reinforcement learning.\nAcknowledgements\nOne of the biggest opportunities I have been provided with throughout my PhD\nstudies was to have not one, but two incredible supervisors. My primary supervisor,\nMarc Deisenroth, has been an wonderful source of guidance and strength. Marc,\nyou have raised my bar for rigour in thinking and writing, and have shown me that\ntechnical excellence does not come in spite of kindness, but due to kindness; for that,\nI will be eternally grateful. My second supervisor, Shakir Mohamed, has been my\nchampion and supporter for many years. Shakir, working with you has been the\nprivilege of a life-time; your breadth of technical knowledge is outstanding, and your\ncareful and deliberate application of it through a meticulous choice of impactful\nresearch directions should serve as a guide to us all.\nI have also been lucky to have an amazing set of collaborators throughout\nmy PhD: Andriy Mnih, Arthur Gretton, Benoit Dherin, Chongli Qin, Claudia\nClopath, David G.T. Barrett, Florin Gogianu, Lucian Busoniu, Michael Figurnov,\nMichael Munn, Razvan Pascanu, Theophane Weber, Tudor Berariu, and Yan Wu.\nSpecial thanks to Benoit Dherin, for many evening chats on everything to do with\ndeep learning optimisation and regularisation; to Yan Wu, for a long term fruitful\ncollaboration; to Theophane Weber for many chats on reinforcement learning; to\nTudor Berariu and Florin Gogianu for being so much fun while doing research; and\nto Arthur Gretton for guidance early in my PhD.\nThis work was done as part of the DeepMind-UCL PhD program, and I am\ngrateful to those organising it and championing this program. Thanks also Frederic\nBesse, and to Sarah Hodkinson and Claudia Pope for their support.\nDuring my PhD I was lucky to write a book chapter on Implicit Generative\nmodels with Balaji Lakshminarayanan and Shakir Mohamed, for Kevin P. Murphy’s\nsecond edition of ‘Machine Learning: a Probabilistic Perspective’. Beyond allowing\nAcknowledgements\nvi\nme to share my enthusiasm for this area of machine learning, a great clarity of thinking\nfollowed the writing process, which helped fledge some of the new ideas presented in\nthis work. I thus must thank Kevin, Balaji, and Shakir for this opportunity.\nThanks also go to my examination committee, Ferenc Husz´ar and Patrick\nRebeschini, for their time and an incredible and thoughtful discussion on my work.\nI also have many reasons to be grateful to those who have supported me personally\nduring this time and beyond. I am here because my parents have prioritised my\neducation and encouraged me throughout the way. I also had the most loving of\ngrandmothers, and I am sure she would have loved to see me pursue this degree.\nI have been fortunate to have a great set of friends, especially in the though\ntimes of coronavirus lockdowns. Niklas, you have been there for me since the times\nof the Imperial labs, and have always had faith in me; there are no words that can\nexpress how grateful I am. Fabio, you are a consistent, reliable, and wise friend; you\nare also raising my favourite humans, who are a great source of happiness for me.\nMany thanks also go to Ada, Dhruva, David, George, James, Ira, Sascha, Siqi, Slemi,\nVictor, and many other friends, for the time spent together and the great memories.\nContents\n1\nIntroduction\n2\n1.1\nOptimisation in deep learning . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nSmoothness w.r.t. inputs . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.3\nInteractions between optimisation and smoothness regularisation . . .\n9\n2\nOptimisation in deep learning\n14\n2.1\nThe direction of steepest descent\n. . . . . . . . . . . . . . . . . . . .\n14\n2.1.1\nComputing the gradient\n. . . . . . . . . . . . . . . . . . . . .\n15\n2.2\nAlgorithms: from gradient descent to Adam\n. . . . . . . . . . . . . .\n16\n2.3\nContinuous-time methods\n. . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.3.1\nA brief overview of stability analysis\n. . . . . . . . . . . . . .\n24\n2.3.2\nBackward error analysis\n. . . . . . . . . . . . . . . . . . . . .\n26\n2.3.3\nBEA proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n2.4\nMultiple objective optimisation\n. . . . . . . . . . . . . . . . . . . . .\n29\n2.5\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3\nA new continuous-time model of gradient descent\n33\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.2\nContinuous time models of gradient descent\n. . . . . . . . . . . . . .\n35\n3.2.1\nLimitations of existing continuous-time flows . . . . . . . . . .\n36\n3.3\nThe principal flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.3.1\nThe PF and the Hessian eigen-decomposition\n. . . . . . . . .\n44\n3.3.2\nThe stability analysis of the PF . . . . . . . . . . . . . . . . .\n47\n3.4\nPredicting neural network gradient descent dynamics with the PF . .\n50\nContents\nviii\n3.4.1\nPredicting (∇θE)T u0 using the PF . . . . . . . . . . . . . . .\n51\n3.4.2\nAround critical points: escaping sharp local minima and saddles 53\n3.5\nThe PF, stability coefficients and edge of stability results . . . . . . .\n55\n3.6\nNon-principal terms can stabilise training . . . . . . . . . . . . . . . .\n63\n3.7\nStabilising training by accounting for discretisation drift\n. . . . . . .\n64\n3.7.1\n∇2\nθE∇θE determines discretisation drift . . . . . . . . . . . .\n65\n3.7.2\nDrift adjusted learning (DAL) . . . . . . . . . . . . . . . . . .\n67\n3.7.3\nThe trade-off between stability and performance . . . . . . . .\n69\n3.8\nFuture work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n3.9\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n3.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n4\nContinuous time models of optimisation in two-player games\n80\n4.1\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n4.1.1\nGenerative adversarial networks . . . . . . . . . . . . . . . . .\n82\n4.2\nDiscretisation drift in two-player games . . . . . . . . . . . . . . . . .\n84\n4.2.1\nDD for simultaneous Euler updates . . . . . . . . . . . . . . .\n85\n4.2.2\nDD for alternating Euler updates . . . . . . . . . . . . . . . .\n86\n4.2.3\nSketch of the proofs . . . . . . . . . . . . . . . . . . . . . . . .\n87\n4.2.4\nVisualising trajectories . . . . . . . . . . . . . . . . . . . . . .\n88\n4.3\nThe stability of DD . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n4.4\nCommon-payoff games . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n4.5\nAnalysis of zero-sum games\n. . . . . . . . . . . . . . . . . . . . . . .\n93\n4.5.1\nDirac-GAN: an illustrative example . . . . . . . . . . . . . . .\n94\n4.6\nExperimental analysis of GANs . . . . . . . . . . . . . . . . . . . . .\n97\n4.6.1\nDoes DD affect training? . . . . . . . . . . . . . . . . . . . . .\n98\n4.6.2\nThe importance of learning rates in DD . . . . . . . . . . . . .\n99\n4.6.3\nImproving performance by explicit regularisation\n. . . . . . . 100\n4.6.4\nExtension to non-zero-sum GANs . . . . . . . . . . . . . . . . 103\n4.7\nA comment on different learning rates . . . . . . . . . . . . . . . . . . 104\n4.8\nExtending the PF for two-player games . . . . . . . . . . . . . . . . . 109\nContents\nix\n4.8.1\nStability analysis . . . . . . . . . . . . . . . . . . . . . . . . . 112\n4.8.2\nAn empirical example and different learning rates . . . . . . . 114\n4.9\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n4.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n5\nFinding new implicit regularisers by revisiting backward error anal-\nysis\n118\n5.1\nRevisiting BEA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n5.2\nImplicit regularisation in multiple stochastic gradient descent steps\n. 121\n5.3\nImplicit regularisation in generally differentiable two-player games . . 126\n5.3.1\nThe effect of the interaction terms: a GAN example . . . . . . 129\n5.4\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n6\nThe importance of model smoothness in deep learning\n132\n6.1\nWhy model smoothness? . . . . . . . . . . . . . . . . . . . . . . . . . 132\n6.2\nMeasuring smoothness . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n6.3\nSmoothness regularisation in deep learning . . . . . . . . . . . . . . . 134\n6.4\nThe benefits of learning smooth models . . . . . . . . . . . . . . . . . 136\n6.5\nChallenges with smoothness regularisation . . . . . . . . . . . . . . . 142\n6.5.1\nToo much smoothness hurts performance . . . . . . . . . . . . 142\n6.5.2\nInteractions between smoothness regularisation and optimisation145\n6.5.3\nSensitivity to data scaling . . . . . . . . . . . . . . . . . . . . 147\n6.6\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7\nSmoothness and optimisation effects of Spectral Normalisation\n149\n7.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.2\nReinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.2.1\nDeep reinforcement learning . . . . . . . . . . . . . . . . . . . 152\n7.2.2\nChallenges with optimisation in DRL . . . . . . . . . . . . . . 153\n7.2.3\nReinforcement learning environments . . . . . . . . . . . . . . 154\n7.3\nSpectral Normalisation in RL\n. . . . . . . . . . . . . . . . . . . . . . 155\n7.3.1\nApplying Spectral Normalisation to a subset of layers . . . . . 155\nContents\nx\n7.3.2\nExperimental results on the Atari benchmark\n. . . . . . . . . 156\n7.3.3\nAn optimisation effect\n. . . . . . . . . . . . . . . . . . . . . . 158\n7.4\nRevisiting Spectral Normalisation in GANs . . . . . . . . . . . . . . . 166\n7.4.1\nSpectral Normalisation and the discriminator output\n. . . . . 167\n7.4.2\nInteractions with loss functions\n. . . . . . . . . . . . . . . . . 170\n7.4.3\nHypothesis: Adam in the low-curvature regime . . . . . . . . . 174\n7.5\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n7.6\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n8\nGeometric Complexity: a smoothness complexity measure implicitly\nregularised in deep learning\n180\n8.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n8.2\nGeometric Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n8.2.1\nGC for deep linear models . . . . . . . . . . . . . . . . . . . . 186\n8.2.2\nGC for rectifier feed-forward networks . . . . . . . . . . . . . . 186\n8.2.3\nGC for convolutional and residual layers\n. . . . . . . . . . . . 187\n8.3\nGC increases as training progresses . . . . . . . . . . . . . . . . . . . 188\n8.4\nGC and regularisation\n. . . . . . . . . . . . . . . . . . . . . . . . . . 190\n8.4.1\nExplicit regularisation\n. . . . . . . . . . . . . . . . . . . . . . 191\n8.4.2\nImplicit regularisation via discretisation drift . . . . . . . . . . 193\n8.5\nDouble descent and GC\n. . . . . . . . . . . . . . . . . . . . . . . . . 196\n8.6\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n8.7\nLimitations and future work . . . . . . . . . . . . . . . . . . . . . . . 199\n8.8\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n9\nConclusion\n201\nA On a new continuous-time model of gradient descent\n243\nA.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\nA.1.1\nBEA proof structure . . . . . . . . . . . . . . . . . . . . . . . 243\nA.1.2\nThird-order flow . . . . . . . . . . . . . . . . . . . . . . . . . . 244\nA.1.3\nHigher order terms of the form ( df\ndθ)\nnf or (∇2\nθE)n ∇θE\n. . . . 246\nContents\nxi\nA.1.4\nLinearisation results around critical points . . . . . . . . . . . 255\nA.1.5\nThe solution of the PF for quadratic losses . . . . . . . . . . . 256\nA.1.6\nThe Jacobian of the PF at critical points . . . . . . . . . . . . 257\nA.1.7\nJacobians of the IGR flow and NGF at critical points . . . . . 259\nA.1.8\nMultiple gradient descent steps\n. . . . . . . . . . . . . . . . . 259\nA.1.9\nApproximations to per-iteration drift for gradient descent and\nmomentum\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\nA.2 Comparison with a discrete-time approach . . . . . . . . . . . . . . . 262\nA.2.1\nChanges in loss function . . . . . . . . . . . . . . . . . . . . . 262\nA.2.2\nThe dynamics of ∇θETui\n. . . . . . . . . . . . . . . . . . . . 263\nA.2.3\nThe connection between DAL and Taylor expansion optimal\nlearning rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\nA.3 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\nA.4 Additional experimental results\n. . . . . . . . . . . . . . . . . . . . . 267\nA.5 Figure reproduction details . . . . . . . . . . . . . . . . . . . . . . . . 281\nB Continuous time models of gradient descent in two-player games 286\nB.1 Proof of the main theorems\n. . . . . . . . . . . . . . . . . . . . . . . 286\nB.1.1\nSimultaneous updates (Theorem 4.2.1) . . . . . . . . . . . . . 287\nB.1.2\nAlternating updates (Theorem 4.2.2) . . . . . . . . . . . . . . 290\nB.2 Proof of the main corollaries . . . . . . . . . . . . . . . . . . . . . . . 296\nB.2.1\nCommon-payoff alternating two-player games\n. . . . . . . . . 296\nB.2.2\nZero-sum simultaneous two-player games . . . . . . . . . . . . 298\nB.2.3\nZero-sum alternating two-player games . . . . . . . . . . . . . 299\nB.2.4\nSelf and interaction terms in zero-sum games . . . . . . . . . . 299\nB.3 Discretisation drift in Runge–Kutta 4 . . . . . . . . . . . . . . . . . . 300\nB.3.1\nRunge–Kutta 4 for one player . . . . . . . . . . . . . . . . . . 300\nB.3.2\nRunge–Kutta 4 for two players\n. . . . . . . . . . . . . . . . . 300\nB.4 Stability analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\nB.4.1\nSimultaneous Euler updates . . . . . . . . . . . . . . . . . . . 304\nB.4.2\nAlternating Euler updates . . . . . . . . . . . . . . . . . . . . 306\nContents\nxii\nB.5 SGA in zero-sum games\n. . . . . . . . . . . . . . . . . . . . . . . . . 308\nB.6 DiracGAN - an illustrative example . . . . . . . . . . . . . . . . . . . 310\nB.6.1\nReconciling discrete and continuous updates in Dirac-GAN . . 310\nB.6.2\nDD changes the convergence behaviour of Dirac-GAN . . . . . 312\nB.6.3\nExplicit regularisation stabilises Dirac-GAN . . . . . . . . . . 313\nB.7 The PF for games: linearisation results around critical points . . . . . 314\nB.8 GAN Experimental details . . . . . . . . . . . . . . . . . . . . . . . . 316\nB.8.1\nImplementing explicit regularisation . . . . . . . . . . . . . . . 316\nB.9 Additional experimental results\n. . . . . . . . . . . . . . . . . . . . . 317\nB.9.1\nAdditional results using zero-sum GANs\n. . . . . . . . . . . . 317\nB.9.2\nGANs using the non-saturating loss . . . . . . . . . . . . . . . 317\nB.9.3\nExplicit regularisation in zero-sum games trained using simul-\ntaneous gradient descent . . . . . . . . . . . . . . . . . . . . . 318\nB.9.4\nExplicit regularisation in zero-sum games trained using alter-\nnating gradient descent . . . . . . . . . . . . . . . . . . . . . . 321\nB.10 Individual figure reproduction details . . . . . . . . . . . . . . . . . . 329\nC Finding new implicit regularisers by revisiting backward error anal-\nysis\n331\nC.1 Two consecutive steps of SGD . . . . . . . . . . . . . . . . . . . . . . 332\nC.2 Multiple steps of SGD\n. . . . . . . . . . . . . . . . . . . . . . . . . . 334\nC.3 Multiple steps of full-batch gradient descent . . . . . . . . . . . . . . 338\nC.3.1\nExpectation over all shufflings . . . . . . . . . . . . . . . . . . 339\nC.4 Two-player games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\nC.4.1\nEffects on the non-saturating GAN . . . . . . . . . . . . . . . 341\nD The importance of model smoothness in deep learning\n343\nD.1 Individual figure reproduction details . . . . . . . . . . . . . . . . . . 343\nE Spectral Normalisation in Reinforcement learning\n345\nE.1 Additional experimental results\n. . . . . . . . . . . . . . . . . . . . . 345\nE.1.1\nThe weak correlation between smoothness and performance . . 345\nContents\nxiii\nE.1.2\nOther regularisation methods\n. . . . . . . . . . . . . . . . . . 345\nE.1.3\ndivOut, divGrad and mulEps . . . . . . . . . . . . . . . . 347\nE.2\nExperimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\nE.2.1\nApplication of Spectral Normalisation . . . . . . . . . . . . . . 350\nE.2.2\nMeasuring smoothness: the norm of the Jacobians . . . . . . . 350\nE.2.3\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nE.2.4\nMinatar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nE.2.5\nAtari experiments . . . . . . . . . . . . . . . . . . . . . . . . . 352\nE.3 Additional hyperparameter sweeps . . . . . . . . . . . . . . . . . . . . 353\nE.4\nIndividual figure reproduction details . . . . . . . . . . . . . . . . . . 356\nF Geometric Complexity: a smoothness complexity measure implicitly\nregularised in deep learning\n358\nF.1\nFigure and experiments reproduction details . . . . . . . . . . . . . . 358\nF.2\nAdditional experimental results\n. . . . . . . . . . . . . . . . . . . . . 359\nList of Figures\n1.1\nOverview of optimisation in deep learning. . . . . . . . . . . . . . . .\n6\n1.2\nThe importance of smoothness on the model fit. . . . . . . . . . . . .\n8\n2.1\nThe importance of the learning rate in gradient descent. . . . . . . . .\n18\n2.2\nThe importance of the learning rate and decay rate in gradient descent\nwith momentum.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.3\nThe effect of hyperparameters in Adam.\n. . . . . . . . . . . . . . . .\n24\n3.1\nDefining discretisation drift. . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.2\nMotivation for the Principal Flow (PF): existing flows fail to capture\nthe oscillatory or unstable behaviour of gradient descent. . . . . . . .\n38\n3.3\nComplex flows can capture oscillations and divergence of gradient\ndescent around local minima.\n. . . . . . . . . . . . . . . . . . . . . .\n38\n3.4\nPer-iteration error between the NGF, IGR and gradient descent. . . .\n39\n3.5\nComparing the coefficients αNGF and αPF based on the value of αh.\nWhile αNGF is always −1, the value of αPF depends on αh and can\nbe complex, both with positive and negative real parts. . . . . . . . .\n45\n3.6\nThe behaviour of the PF on E(z) = 1\n2z2. . . . . . . . . . . . . . . . .\n46\n3.7\nThe PF captures the behaviour of gradient descent exactly for\nquadratic losses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n3.8\nThe behaviour of the PF when the loss is the banana function. . . . .\n48\n3.9\nSmall neural networks: comparing continuous flows for multiple itera-\ntions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n3.10 Predictions of ∇θETu0 according to continuous-time flows for a VGG\nmodel trained on CIFAR-10. . . . . . . . . . . . . . . . . . . . . . . .\n53\nList of Figures\nxv\n3.11 Sharp minima are not attractive to gradient descent, a neural network\nexperiment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n3.12 The edge of stability behaviour in neural networks.\n. . . . . . . . . .\n55\n3.13 Comparison of how continuous-time models capture the behaviour of\ngradient descent at edge of stability.\n. . . . . . . . . . . . . . . . . .\n56\n3.14 Using the PF to understand stability and instability in a 4 layer MLP\ntrained on MNIST. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n3.15 Using the PF and stability coefficient to understand instabilities in\ndeep learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n3.16 Understanding the behaviour of the loss through the PF and the\nbehaviour of λ0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n3.17 Assessing whether 1 dimension is enough to cause instability in a\ncontinuous-time flow. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n3.18 Decreasing the learning rate in the edge of stability area leads to\nincreased stability.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.19 The unstable dynamics of ∇θETu in the edge of stability area. . . . .\n62\n3.20 The value of non-principal terms; a Resnet-18 trained on CIFAR-10\nsweep. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3.21 ||∇2\nθE∇θE|| and the per-iteration drift measured during training. . .\n66\n3.22 Correlation between ||∇2\nθE∇θE|| and the per-iteration drift. . . . . .\n67\n3.23 Models trained using a learning rate sweep or DAL on CIFAR-10 and\nImagenet. Models are VGG and Resnet-50. . . . . . . . . . . . . . . .\n68\n3.24 Learning rate and update norms in DAL compared to fixed learning\nrate training; a Resnet-18 model trained on CIFAR-10. . . . . . . . .\n69\n3.25 DAL-0.5 results on a CIFAR-10 and Imagenet using VGG, and Resnet-\n50 model respectively.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n3.26 DAL-p sweep on full-batch training on CIFAR-10 with VGG and\nResnet-18 models and a Resnet-50 model trained on Imagenet. . . . .\n70\n3.27 VGG model trained on CIFAR-10 using full batch gradient descent,\neither with a fixed learning rate in a sweep or a DAL-p sweep. . . . .\n71\nList of Figures\nxvi\n3.28 DAL and gradient descent learned landscapes; batch size 64 on CIFAR-\n10. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n3.29 DAL-p and momentum Resnet-50; model trained on Imagenet. . . . .\n74\n3.30 Per-parameter DAL-p; batch size 8192 on Imagenet. . . . . . . . . . .\n75\n4.1\nVisualisation of our approach using backward error analysis (BEA) to\nfind new continuous-time flows describing the behaviour of gradient\ndescent in two-player games. . . . . . . . . . . . . . . . . . . . . . . .\n85\n4.2\nModified flows given by BEA better capture discrete dynamics on a\ntwo dimensional example.\n. . . . . . . . . . . . . . . . . . . . . . . .\n89\n4.3\nDiscretisation drift can change the stability of a game.\n. . . . . . . .\n91\n4.4\nIn common-payoff games alternating updates lead to higher gradient\nnorms and unstable training when equal learning rates are used. . . .\n93\n4.5\nDiscretisation drift can cause instability in DiracGAN; cancelling the\nsources of instabilities leads to convergence.\n. . . . . . . . . . . . . .\n96\n4.6\nThe effect of discretisation drift on zero-sum games. . . . . . . . . . .\n98\n4.7\nAlternating gradient descent performs better for learning rate ratios\nwhich reduce the adversarial nature of discretisation drift.\n. . . . . .\n99\n4.8\nExplicit regularisation cancelling the interaction terms of discretisation\ndrift in simultaneous gradient descent improves performance in zero-\ngames. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n4.9\nComparison with Symplectic Gradient Adjustment and Consensus\nOptimisation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n4.10 Explicit regularisation informed by discretisation drift for alternating\nupdates in zero-sum games.\n. . . . . . . . . . . . . . . . . . . . . . . 103\n4.11 The effect of discretisation drift depends on the game: its effect is less\nstrong for the non-saturating loss. . . . . . . . . . . . . . . . . . . . . 104\n4.12 An alternative interpretation to different learning rates leads to a new\nset of flows for simultaneous gradient descent updates.\n. . . . . . . . 109\n4.13 An alternative interpretation to different learning rates leads to a new\nset of flows for alternating gradient descent updates. . . . . . . . . . . 109\nList of Figures\nxvii\n4.14 A visualisation of a simple example of the extension of the PF to\ntwo-games. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n5.1\nVisualising the standard approach to backward error analysis alongside\nan approach which constructs a different flow per gradient descent\niteration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n6.1\nDouble descent in deep learning compared to the traditional U-shaped\ncomplexity curve. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n6.2\nThe importance of critic smoothness when estimating divergences and\ndistances.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n6.3\nDecision surfaces on two moons under different regularisation methods.143\n6.4\nLipschitz constant of each layer of an MLP trained on the two moons\ndataset.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n6.5\nThe effect of regularisation on local smoothness. . . . . . . . . . . . . 144\n6.6\nSmoothness constraints interact with learning rates. . . . . . . . . . . 146\n6.7\nSmoothness constraints interact with momentum decay rates.\n. . . . 147\n7.1\nAdding Spectral Normalisation to C51 improves the agent’s perfor-\nmance on Atari. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n7.2\nSelectively applying Spectral Normalisation to one layer leads to\nsignificant improvements over a DQN agent trained with Adam. . . . 158\n7.3\nSpectral Normalisation shows gains for all model sizes when applied\nto a DQN agent on MinAtar.\n. . . . . . . . . . . . . . . . . . . . . . 159\n7.4\nSpectral Normalisation can significantly decrease hyperparameter\nsensitivity of DQN when the Adam optimiser is used. . . . . . . . . . 161\n7.5\nThe effect of divOut, divGrad and mulEps on RL performance on\nMinAtar. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n7.6\nOptimisation changes based on Spectral Normalisation do not recover\nthe performance of Spectral Normalisation in GANs.\n. . . . . . . . . 167\n7.7\nThe effect of Spectral Normalisation on discriminator predictions. . . 168\nList of Figures\nxviii\n7.8\nSpectral Normalisation increases the entropy of the discriminator’s\npredictions on data and samples throughout training. . . . . . . . . . 169\n7.9\nSpectral Normalisation increases the entropy of the discriminator’s\npredictions on data and samples in the early stages of training. . . . . 169\n7.10 Spectral Normalisation increases the entropy of the discriminator’s\npredictions on data and samples in the late stages of training.\n. . . . 170\n7.11 Understanding the landscape of GAN losses and gradients: comparing\nthe saturating and non-saturating generator loss.\n. . . . . . . . . . . 171\n7.12 Only applying the Jacobian change to the discriminator only leads to\nextremely poor performance when the saturating loss is used. . . . . . 172\n7.13 DivOut obtains similar performance to Spectral Normalisation for\nGANs on CIFAR-10 when the non-saturating loss is used.\n. . . . . . 173\n7.14 The effect of the learning rate and ϵ on the magnitude of the update\nin the low-curvature regime in Adam. . . . . . . . . . . . . . . . . . . 175\n7.15 The effect of ϵ on Adam performance in GAN training. . . . . . . . . 176\n8.1\nNeural networks initialise close to the 0 constant function.\n. . . . . . 189\n8.2\nGeometric Complexity (GC) at initialisation decreases as the number\nof layers in the network increases. . . . . . . . . . . . . . . . . . . . . 190\n8.3\nGeometric Complexity (GC) increases as training progresses. . . . . . 191\n8.4\nCommon explicit regularisation methods lead to decreased GC when\nusing vanilla stochastic gradient descent. . . . . . . . . . . . . . . . . 192\n8.5\nExplicit minimising GC leads to increased generalisation. . . . . . . . 193\n8.6\nThe implicit regularisation effect of large learning rates on GC. . . . . 194\n8.7\nThe implicit regularisation effect of small batch sizes on GC. . . . . . 195\n8.8\nDouble descent and GC. . . . . . . . . . . . . . . . . . . . . . . . . . 197\nA.1 Backward error analysis proof structure.\n. . . . . . . . . . . . . . . . 243\nA.2 The effects of the approximation of ∇2\nθE∇θE on DAL; results on\nfull-batch training on CIFAR-10.\n. . . . . . . . . . . . . . . . . . . . 266\nList of Figures\nxix\nA.3 The effects of the approximation of ∇2\nθE∇θE on DAL; results with\nbatch size 512 on CIFAR-10. . . . . . . . . . . . . . . . . . . . . . . . 266\nA.4 The effects of the approximation of ∇2\nθE∇θE on DAL; Imagenet\nresults.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\nA.5 The effects of the approximation of ∇2\nθE∇θE on DAL-0.5; Imagenet\nresults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\nA.6 Continuous time flows approximating gradient descent for a 1-D non\nquadratic function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\nA.7 Global parameter error between gradient descent and continuous-time\nflows on an MLP trained on the UCI breast cancer dataset. . . . . . . 269\nA.8 The eigenspectrum obtained along the gradient descent path of a 5\nlayer small MLP trained on the UCI Iris dataset.\n. . . . . . . . . . . 269\nA.9 One eigendirection is sufficient to cause instability in continuous-time;\nMNIST results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\nA.10 Peak areas of loss increase and the stability coefficient for the largest\neigendirection of the PF. . . . . . . . . . . . . . . . . . . . . . . . . . 271\nA.11 Understanding changes to λ0 using the PF. . . . . . . . . . . . . . . . 271\nA.12 The connection between the PF, λ0 and loss instabilities with Resnet-\n18 trained on CIFAR-10. . . . . . . . . . . . . . . . . . . . . . . . . . 272\nA.13 Assessing whether the value of the DAL learning rate for a fixed\nlearning rate sweep has magnitudes which can be used as a learning\nrate in training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\nA.14 DAL-p sweep with a VGG model trained on CIFAR-10 for small and\nlarge batch sizes.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\nA.15 Models trained using a learning rate sweep or DAL on CIFAR-10\nusing a Resnet-18 model. . . . . . . . . . . . . . . . . . . . . . . . . . 274\nA.16 DAL on Imagenet, a learning rate sweep across batch sizes. . . . . . . 274\nA.17 DAL−0.5: on Imagenet, a learning rate sweep across batch sizes. . . . 275\nA.18 DAL-p sweep on VGG, Resnet-18 and Imagenet. . . . . . . . . . . . . 276\nA.19 DAL results with a least square loss.\n. . . . . . . . . . . . . . . . . . 276\nList of Figures\nxx\nA.20 DAL and gradient descent learned landscapes; full batch on CIFAR-10.277\nA.21 λ0 in DAL and gradient descent; full batch on CIFAR-10. . . . . . . . 278\nA.22 DAL and gradient descent learned landscapes; batch size 8192 on\nImageNet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\nA.23 VGG trained on CIFAR-10 with batch size 1024: connection between\ndrift, test error and λ0. . . . . . . . . . . . . . . . . . . . . . . . . . . 279\nA.24 DAL-p with momentum 0.9 on Imagenet. The model is Resnet-50\ntrained with batch size 1024. . . . . . . . . . . . . . . . . . . . . . . . 280\nA.25 The global error in parameter space between the NGF and gradient\ndescent; MNIST results.\n. . . . . . . . . . . . . . . . . . . . . . . . . 280\nB.1 The effect of discretisation drift on zero-sum games; using the saturat-\ning generator loss. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\nB.2 The effect of discretisation drift on zero-sum games; least square losses.319\nB.3 The effect of discretisation drift depends on the game. . . . . . . . . . 320\nB.4\nUsing explicit regularisation to cancel the interaction terms of discreti-\nsation drift leads to a substantial performance improvement. . . . . . 321\nB.5\nUsing explicit regularisation to cancel the interaction terms of discreti-\nsation drift obtains the same peak performance as Adam. . . . . . . . 322\nB.6 The form of discretisation drift allows us to construct efficient regu-\nlarisers without the need of a hyperparameter sweep.\n. . . . . . . . . 323\nB.7 Cancelling interaction terms increases performance across multiple\npercentages used to select top models.\n. . . . . . . . . . . . . . . . . 323\nB.8 Cancelling interaction terms increases performance across batch sizes. 324\nB.9 Comparing cancelling interactions terms with Symplectic Gradient\nAdjustment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\nB.10 Comparing hyperparameter sensitivity of cancelling interactions terms\nand Symplectic Gradient Adjustment. . . . . . . . . . . . . . . . . . . 325\nB.11 Comparing cancelling interactions terms with Consensus Optimisation.325\nB.12 Comparing hyperparameter sensitivity of cancelling interactions terms\nand Consensus Optimisation.\n. . . . . . . . . . . . . . . . . . . . . . 326\nList of Figures\nxxi\nB.13 Variability across seeds when cancelling interaction terms.\n. . . . . . 326\nB.14 Gradient clipping can reduce variability across seeds when cancelling\ninteraction terms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\nB.15 Cancelling interaction terms of discretisation drift in alternating gra-\ndient descent updates.\n. . . . . . . . . . . . . . . . . . . . . . . . . . 327\nB.16 Variability when cancelling interaction terms of discretisation drift in\nalternating gradient descent updates. . . . . . . . . . . . . . . . . . . 328\nB.17 Cancelling interaction terms in alternating updates can increase per-\nformance across multiple percentages used to select top models.\n. . . 328\nE.1 Applying Spectral Normalisation on a subset of layers does not always\nresult in smoother networks. . . . . . . . . . . . . . . . . . . . . . . . 346\nE.2\nOther normalisation and regularisation methods do not recover Spec-\ntral Normalisation performance when applied to the DQN critic. . . . 347\nE.3 Assessing the effect of the Lipschitz constant K when applying Spectral\nNormalisation to a DQN agent. . . . . . . . . . . . . . . . . . . . . . 347\nE.4\nSpectral radii in a long training run on MinAtar using a 4-layer critic. 348\nE.5\nMinAtar Normalised Scores for the four architectures in Table E.6. . . 349\nF.1\nThe implicit regularisation effect of increased learning rates and de-\ncreased batch sizes leads to decreased Geometric Complexity (GC)\nwhen momentum is used. . . . . . . . . . . . . . . . . . . . . . . . . . 360\nF.2\nGradient norm regularisation leads to decreased GC when using\nstochastic gradient descent.\n. . . . . . . . . . . . . . . . . . . . . . . 361\nF.3\nSpectral regularisation leads to decreased Geometric Complexity when\nusing stochastic gradient descent. . . . . . . . . . . . . . . . . . . . . 362\nF.4\nL2 regularisation leads to decreased GC when using stochastic gradient\ndescent.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\nList of Tables\n3.1\nWriting the continuous-time flows discussed in terms of the eigen-\ndecomposition of the Hessian. . . . . . . . . . . . . . . . . . . . . . .\n44\n4.1\nContrasting the implicit regularisation effects of the continuous-time\nflows proposed in zero-sum games. . . . . . . . . . . . . . . . . . . . . 108\n4.2\nComparing cancelling discretisation drift terms with existing explicit\nregularisation methods in zero-sum games. . . . . . . . . . . . . . . . 116\n7.1\nThe percentage of training settings for which applying Spectral Nor-\nmalisation to more layers leads to smoother networks, as measured by\nthe model’s Jacobian at peak performance. . . . . . . . . . . . . . . . 160\n7.2\nSpearman Correlation coefficient between the negative norm of the\nJacobian and peak performance for each game. . . . . . . . . . . . . . 160\nE.1\nMinAtar default hyperparameter settings, used for all MinAtar exper-\niments unless otherwise specified. . . . . . . . . . . . . . . . . . . . . 352\nE.2\nMinAtar maximum and random scores used for computing the MinAtar\nNormalised Score.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nE.3\nNetwork used for MinAtar experiments. The last fully connected layer\nhas an output size given by the number of actions.\n. . . . . . . . . . 353\nE.4\nDQN-Adam hyperparameters.\n. . . . . . . . . . . . . . . . . . . . . 354\nE.5\nNetwork used for Atari experiments.\n. . . . . . . . . . . . . . . . . . 354\nE.6\nDetails of neural architecture sweeps. . . . . . . . . . . . . . . . . . . 355\nChapter 1\nIntroduction\nWhy has deep learning been so successful in recent decades at solving a myriad of\nproblems, such as scientific discovery and applications [1–3], language and image\ngeneration [4–7], and super level human game play in intricate games [8–10]? A\ncommon answer to the question of why deep learning has been so successful is that\nit scales well with increases in amount of data and computational resources, both\nof which have drastically increased in recent years. But this answer raises further\nquestions, since many of the reasons why deep learning scales well have yet to be fully\nuncovered. Particularly, we still do not know why the recipe behind deep learning—\nusing simple gradient-based algorithms to optimise deep neural networks—is so\neffective on large datasets. One valid response to this observation is to state that it\nmight not matter why deep learning works, but that it does and to continue exploring\nits benefits on a wide range of domains, as well as continue the trend of scaling\nup data, architectures, and compute. This approach works, can lead to progress,\nand has been widely used in other sciences. This is not the approach we take here,\nhowever. Throughout this thesis, we take the point of view that understanding\nwhy a system works is not only desirable, but necessary in order to ensure targeted\nresearch progress, by exploring the areas most likely to lead to an acceleration of new\nmethods, increased resource efficiency, and safe deployment. In taking this point of\nview, we follow the footsteps of a growing body of work aiming to uncover, examine,\nand understand deep learning systems [11–21].\nThere are many avenues worthy of study inside the deep learning domain,\nranging from investigating why certain model architectures work well [22–25], to\n3\nstudying why certain types of generative models or learning principles outperform\nothers [26–28]. Here, we choose to focus on the study of optimisation and model\nregularisation in the form of smoothness regularisation. We are motivated by their\ngenerality and applicability across deep learning domains from supervised learning,\nprobabilistic modelling, and reinforcement learning; their significant impact on\ntraining performance and test set and out of distribution generalisation [29–31];\nand the opportunity for analysis due to the vast gap between their aforementioned\npractical performance and the understanding of their mechanisms. Unanswered\nquestions include: What causes instability in deep learning? How can instability be\nmitigated? Why is there not more instability? What are the implicit regularisation\neffects of popular optimisers? What are the effects of smoothness regularisation?\nWhat are the interactions between model regularisation and optimisation in deep\nlearning? Which optimisation and regularisation effects are domain specific and\nwhich transfer across deep learning domains? These are some of the questions we\nwill tackle in this thesis.\nA motivating example. Generative Adversarial Networks (GANs) [32] are\na type of generative model that has led to many image generation breakthroughs\n[7, 33, 34]. Many GAN variants have been proposed, some based on minimising dif-\nferent distributional divergences or distances [27, 35–38] while others use convergence\nanalysis around a Nash equilibrium [15, 17]; each of these variants are theoretically\nappealing and have different properties worth studying. Yet, the biggest successes\nin GANs have consistently come from changes to optimisation and regularisation:\nthe choice of optimiser [31], large batch sizes [33], and incorporating smoothness\nregularisation [30] are the key ingredients of all successful GANs, while the loss func-\ntions used are often those closest to the original formulation [30, 33, 39, 40]. Similar\noptimisation and regularisation techniques have also allowed the expansion of GANs\nfrom continuous input domains, such as images, to discrete input domains, such as\ntext [41]. The GAN example showcases what opportunities are available by under-\nstanding and improving optimisation and smoothness regularisation, opportunities\nwe aim to explore here.\n1.1. Optimisation in deep learning\n4\n1.1\nOptimisation in deep learning\nMachine learning is the science of casting real world problems such as prediction,\ngeneration, and action into optimisation problems. Thus, many machine learning\nproblems can be formulated as\nmin\nθ E(θ),\n(1.1)\nwhere E is a loss function suitably chosen for the underlying task and θ ∈RD. For\nmany machine learning tasks, E is an expected value of an unknown data distribution\np∗(x); E gets estimated given a training dataset D of unbiased samples from p∗(x):\nE(θ) = Ep∗(x)E(θ; x) ≈\n1\n|D|\nP\nxi∈D E(θ; xi). A distinction is made between the model\nand the objective function components of E(θ; x), with the model f(θ; x) depending\non the parameters θ, and the objective function depending only on the model’s\noutput. In deep learning, f is a neural network, θ are its parameters, and E is non-\nconvex. Due to the high dimensionality of θ or large dataset size, many optimisation\nalgorithms are too costly or inefficient to be used in deep learning, as they scale\nunfavourably with parameter or dataset size. Luckily, however, we can use the\ncompositional structure of neural networks to compute the gradient ∇θE ∈RD\nusing the backpropagation algorithm [42–44]. From there, we can use first-order\ngradient-based iterative algorithms, such as gradient descent and its variants [45–48].\nStudying the behaviour of optimisation algorithms can be challenging, even\nfor algorithms as seemingly simple as gradient descent. Typical questions of study\ninclude: Will the algorithm converge to a local minima? Does the algorithm get\nstuck in saddle points? When does the algorithm fail to converge and when it\ndoes converge, how quickly does it converge? The complexity of these issues gets\ncompounded when studying the optimisation of deep neural networks with millions\nto billions of parameters that do not satisfy commonly used assumptions such as\nconvexity; this often creates a discrepancy between what theory analyses and what\nis used in practice. Fundamentally, the deep learning optimisation community is\ninterested in answering the questions: Why do first-order optimisers work so well in\ntraining neural networks and How can we improve optimisation in deep learning?\n1.1. Optimisation in deep learning\n5\nProgress in this area has been made recently both through theoretical and empirical\nmeans of studying neural network optimisation dynamics [18, 49–53]. Novel insights\nchallenge commonly held assumptions, such as the belief that local minima and\nsaddle points are a major challenge for neural network optimisation due to high\nparameter dimensionality [49–51, 54]. There has also been a growing body of work\nshowing the importance of the Hessian ∇2\nθE in supervised learning optimisation,\nand specifically on the connection between the learning rate and the largest Hessian\neigenvalue and observed instabilities in training [18, 52, 53]; we will examine the\nimportance of the Hessian in a new light in Chapter 3.\nMany analyses of discrete-time methods, such as gradient descent, take a\ncontinuous-time approach [11, 17, 54–61] as continuous-time proofs tend to be easier\nto construct [54, 62]. The downside of taking a continuous-time and not discrete-time\napproach to optimisation analysis is that one does not directly analyse the update\nof interest; the discrepancy between the discrete-time trajectory and its continuous\ncounterpart, which we will call discretisation drift, can lead to conclusions that\ndo not transfer from continuous-time analysis to discrete-time algorithms [63, 64].\nDiscretisation drift is still not greatly understood and has only recently come into\nattention in deep learning [12, 13, 65]. The incorporation of techniques from the\nnumerical integration community that construct continuous-time flows accounting\nfor discretisation drift has shed light on the implicit regularisation effects of gradient\ndescent and highlighted the importance of the learning rate in generalisation [12, 13].\nWe will use the same numerical integration techniques in Chapter 3, where we find a\nnew continuous-time flow that, unlike existing flows, can capture instabilities induced\nby gradient descent. We then use insights from our continuous-time analysis to\ndevise an automatic learning rate schedule that can trade-off training stability and\ngeneralisation performance.\nWhile many studies focus on optimisation dynamics in the single-objective case,\nsuch as supervised learning, strides have also been made towards understanding\noptimisation in two-player games [15, 17, 61, 66, 67]. Unlike in the single-objective\nsetting, in two-player games not all parameters minimise the same objective, as each\n1.1. Optimisation in deep learning\n6\nθ1\n−1\n0\n1\nθ2\n−1\n0\n1\nE(θ)\n0.0\n2.5\n5.0\n7.5\n10.0\nequilibruim\noptimisation trajectory\n2\n4\n6\n8\n10\n(a) Single objective.\nϕ\n−1\n0\n1\nθ\n−1\n0\n1\nE(ϕ, θ)\n−3.0\n−2.5\n−2.0\n−1.5\n−1.0\nequilibruim\noptimisation trajectory\n−2.5\n−2.0\n−1.5\n−1.0\n(b) Zero-sum games.\nFigure 1.1: Overview of optimisation in deep learning. Many optimisation problems\nin deep learning can be seen as either single-objective problems, where all\nparameters are updated to minimise the same loss, or multi objective problems\nwhere where different sets of parameters are updated to minimise different\nobjectives; when the number of objectives is two, these are called two-player\ngames. A special case of two-player games are adversarial games such as\nzero-sum games, where the objective of one player is to minimise a function E,\nwhile the objective of the other player is to maximise the same function. This\nadversarial structure can lead to challenges in optimisation, with approaches\nthat lead to converge to local minima in single objective optimisation (a)\nresulting in cyclic trajectories in zero sum games (b).\nplayer has its own set of parameters, here denoted by ϕ and θ, and its own objective:\nmin\nϕ Eϕ(ϕ, θ)\n(1.2)\nmin\nθ Eθ(ϕ, θ).\n(1.3)\nTwo-player games can be adversarial, where the two players have opposing objectives,\nand even zero sum, where one player’s gain is another’s loss: Eϕ(θ, ϕ) = −Eθ(θ, ϕ).\nWe visualise a simple two-player zero-sum game in Figure 1.1b, showing that adver-\nsarial dynamics can make it challenging to reach a local equilibrium compared to a\nsingle-objective counterpart, shown in Figure 1.1a.\nInterest in the analysis of two-player game optimisation has been fuelled by\ninterest in GANs [32], a generative model trained via an adversarial two player game.\nWhile GANs have been a successful generative model, they have a reputation of\nbeing notoriously hard to train, and this has served as a motivation for finding\napproaches to stabilise and improve the dynamics of adversarial two-player games.\nAnalysing the behaviour and convergence of two-player games has led to progress\n1.2. Smoothness w.r.t. inputs\n7\nin understanding the sources of instability and divergence in games and has led to\nmany a regulariser that can stabilise training [17, 61, 67–70]. We add to this body of\nwork by further investigating sources of instability in two-player games, coming from\ndiscretisation drift rather than the original continuous-time system, in Chapter 4. By\nmodelling discretisation drift, we devise explicit regularisers that stabilise training\nwithout requiring any additional hyperparameter sweep.\n1.2\nSmoothness w.r.t. inputs\nOptimisation provides an approach to approximate an unknown decision surface\nrequired for prediction, generation, or action from an available dataset. That is, given\nan optimal predictor f ∗, optimisation provides a recipe to find a parametrised function\nf(·; θ) close to f ∗on the training dataset. But there are often many such optimisation\nsolutions, each with different properties outside the training data. How useful this\napproximation is in the underlying application depends on how well it performs on\nnew data. This is an essential desideratum of machine learning algorithms, and it is\nrelevant for generalisation, continual learning [71], and transfer learning [72]. Since\nwe know from the no-free-lunch theorem [73] that no machine learning algorithm will\nperform better than all other algorithms for any data distribution and prediction\nproblem, to provide guarantees beyond the training data requires making assumptions\nabout the data distribution, the structure of f ∗, or both. One approach to encode\nbeliefs about f ∗into f(·; θ) is through what is often referred to in machine learning\nas an inductive bias [74]1.\nIn accordance with Occam’s razor [75], we often want to encode a simplicity\ninductive bias: we are searching for the least complex functions that can explain\nthe data. One approach to formalising simplicity is by measuring the effect changes\nin function input have on the function’s output. If small changes in function input\ndo not result in large changes in function output, we call that function smooth.\nA smoothness inductive bias encodes a preference for learning smooth functions:\n1When we talk about smoothness with respect to inputs, we often write the predictor as a\nfunction of data, given parameters. When discussing optimisation and the focus is on parameters,\nwe write the reverse for clarity.\n1.2. Smoothness w.r.t. inputs\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n3\n2\n1\n0\n1\n2\n3\nmodel\ndata\n(a) Very smooth fit,\nhurts training data fit.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n3\n2\n1\n0\n1\n2\n3\nmodel\ndata\n(b) Desired level of smoothness.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n3\n2\n1\n0\n1\n2\n3\nmodel\ndata\n(c) Too smooth fit,\nhurts generalisation.\nFigure 1.2: The importance of smoothness with respect to inputs on the model fit: too\nmuch smoothness can hurt capacity (a), while not enough smoothness can\nhurt generalisation (c).\nbetween two functions that fit the data equally well, the smoothest one should be\npreferred. We show an intuitive example highlighting the importance of smoothness\nin Figure 1.2. Figure 1.2a exemplifies how too much smoothness can hurt training\nperformance by reducing model capacity, while Figure 1.2c shows how the lack of\nsmoothness can lead to overfitting.\nThe formal definition of smoothness is often taken to be Lipschitz smoothness\nor related measures. A function f(·; θ) : X →Y is K-Lipschitz if\n∥f(x1; θ) −f(x2; θ)∥Y ≤K ∥x1 −x2∥X\n∀x1, x2 ∈X,\n(1.4)\nwhere K is known as the Lipschitz constant of function f(·; θ). Throughout this\nmanuscript, when we refer to the smoothness of a neural network, we refer to the\nsmoothness w.r.t. data x, and not w.r.t. parameters θ.\nFor many a machine learning task smoothness with respect to inputs constitutes\na reasonable prior; this is can be easily illustrated in the image domain, where the\nprediction or action that ought to be taken does not change when a few pixels in\nthe inputs change: an image of a dog still encodes a dog if a few pixels are modified.\nThus, it does not come as a surprise that despite having vastly different motivations\nand implementations, many existing deep learning regularisation methods target a\nsmoothness inductive bias, including L2 regularisation, dropout [76], early stopping.\nWe will discuss in detail how these regularisation approaches are connected to\n1.3. Interactions between optimisation and smoothness regularisation\n9\nsmoothness in Chapter 6, formalise these connections in Chapter 8, as well as link\nsmoothness to important phenomena in deep learning, such as double descent [77–79].\nAs the importance of smoothness is beginning to be recognised by the deep\nlearning community, methods that directly target smoothness regularisation have\nbeen developed [29, 30, 80]. These regularisation methods have been incorporated\nin supervised learning to help generalisation [29, 81] and adversarial robustness [82–\n84], as well as into generative models leading to improvements in performance and\nstability [30, 39, 85]. We investigate the importance of smoothness regularisation\nand its effects in Chapter 6 as well as incorporate it into a new problem domain,\nreinforcement learning, in Chapter 7.\n1.3\nInteractions between optimisation and\nsmoothness regularisation\nWe have thus far discussed optimisation and smoothness regularisation as separate\npillars of a deep learning system. Optimisation—concerned with changes in param-\neters—affects training performance, speed, and stability. Smoothness—concerned\nwith changes in inputs—can be a powerful inductive bias avoiding learning overly\ncomplex models. What is often overlooked is the implicit bias optimisation has on\nthe model class being learned. While neural architectures and model regularisers\ndefine the class of functions we can represent with our models, optimisation methods\ndetermine which functions in the function class we can learn [12]. Likewise, different\nmodel architectures are easier to learn than others, with ResNets [86] providing a\nprime example of an architecture that aids optimisation [22], followed by model\nnormalisation techniques such as batch normalisation [87], which have been shown\nto primarily benefit optimisation [23].\nWhile not previously studied, we show that smoothness regularisation and\noptimisation heavily interact in deep learning. We start by showing examples of\nthis interaction in Chapter 6, where we observe that smoothness regularisation\nmethods can interact with hyperparameter choices such as learning rates. We then\nhighlight how in some problem domains smoothness regularisation leads to increased\n1.3. Interactions between optimisation and smoothness regularisation\n10\nperformance by improving optimisation dynamics in Chapter 7.\nIn Chapter 8,\nwe show how the implicit regularisation induced by the discretisation drift of an\noptimisation method can lead to a smoothness inductive bias, and that the strengh\nof this inductive bias is dependent on optimsation hyperparameters such as learning\nrate and batch size.\nThroughout this thesis, we will see examples of how optimisation can affect the\nlearned model class and how the model smoothness can affect optimisation. While it\nis tempting to think of each component of a deep learning system individually, it\ncan hinder progress and misguide our intuitions. We will use these observations to\nargue for a cohesive view between optimisation and regularisation in deep learning.\n1.3. Interactions between optimisation and smoothness regularisation\n11\nRelationship to published papers\nThe presented thesis is based on the following published works:\n• On a continuous-time model of gradient descent dynamics and instability in deep\nlearning. Mihaela Rosca, Yan Wu, Chongli Qin, Benoit Dherin. Transactions\nof Machine Learning Research, 2023.\n• Discretization Drift in Two-Player Games. Mihaela Rosca, Yan Wu, Benoit\nDherin and David G. T. Barrett. International Conference for Machine Learn-\ning, 2021.\n• A case for new neural network smoothness constraints.\nMihaela Rosca,\nTheophane Weber, Arthur Gretton, Shakir Mohamed. 1st I Can’t Believe It’s\nNot Better Workshop (ICBINB@NeurIPS 2020), Vancouver, Canada.\n• Spectral Normalisation for Deep Reinforcement Learning: an Optimisation Per-\nspective. Florin Gogianu, Tudor Berariu, Mihaela Rosca, Claudia Clopath,\nLucian Busoniu, Razvan Pascanu. International Conference for Machine Learn-\ning, 2021.\n• Why neural networks find simple solutions: The many regularizers of geometric\ncomplexity.\nBenoit Dherin, Michael Munn, Mihaela Rosca, David G.T.\nBarrett. Neural Information Processing Systems, 2022.\nOther work performed as part of the PhD program that is not discussed here:\n• Chapter on implicit generative models in the second edition of ‘Machine\nLearning: a Probabilistic Perspective’ by Kevin P. Murphy. Mihaela Rosca,\nBalaji Lakshminarayanan and Shakir Mohamed.\n• Monte Carlo gradient estimation in machine learning. Shakir Mohamed, Mi-\nhaela Rosca, Michael Figurnov, Andriy Mnih. Equal contributions. Journal\nof Machine Learning Research, 21(132), pp.1–62. 2020.\n• Proposed and advised a project for a master student at UCL. The topic of\nstudy was the optimisation landscape in offline reinforcement learning.\n• Co-organised the ICML 2021 workshop on ‘Continuous Time Perspectives in\nMachine Learning’ https://icml.cc/virtual/2022/workshop/13452.\n1.3. Interactions between optimisation and smoothness regularisation\n12\nOverview of thesis\n• Chapter 2 is an overview of the optimisation concepts required for the rest of\nthis thesis.\n• Chapter 3 consists of the work published in On a continuous-time model of gra-\ndient descent dynamics and instability in deep learning [88]. My contributions\nto this paper were: I started the project and investigated the idea of finding a\nnew continuous-time flow for gradient descent; did all the proofs (except the\njustification of Thm 3.3.2); I wrote all the code, ran all the experiments, and\ndid all the visualisations; I wrote the paper.\n• Chapter 4 consists of the work published in Discretization Drift in Two-Player\nGames [89], together with novel insights presented in this thesis regarding\nthe use of different learning rates for the two-players (Section 4.7) as well\nas expanding the flow derived in Chapter 3 to games (Section 4.8).\nMy\ncontributions to the paper were: I initiated this project by suggesting to\nexpand the work of Barrett and Dherin [12] to two-player games; derived the\nmain results; did the relevant literature survey; I wrote all the code, ran all\nthe experiments and did all the visualisations; and I wrote most of the paper.\n• Chapter 5 consists of unpublished work aiming to develop a new framework\nfor constructing modified flows and implicit regularisers in deep learning.\n• Chapter 6 consists of the work published in the workshop paper A case for\nnew neural network smoothness constraints [90]. This work is the result of the\nearly reading I have done as part of the PhD program. I wrote the code and\nperformed all the experiments for this work, and wrote the paper.\n• Chapter 7 is based on the paper Spectral Normalisation for Deep Reinforcement\nLearning: an Optimisation Perspective [91] together with the application of the\ninsights from the paper applied to GANs (Section 7.4), which is unpublished\nnovel work I did while writing this thesis. The paper contents were rewritten\nand the text is mine. My contributions to the paper were: I added to the view\nthat Spectral Normalisation helps RL through optimisation, helped derive the\n1.3. Interactions between optimisation and smoothness regularisation\n13\noptimisers that allow us to recover the performance of Spectral Normalisation,\nand suggested experiments and analysis such as plotting the change in Jacobian\nof the critic against performance as well as provide intuition and advice on\nthe application on Spectral Normalisation and other smoothness regularisation\nmethods; I did not contribute with any coding or experiments to the paper.\n• Chapter 8 is based on Why neural networks find simple solutions: The many\nregularizers of Geometric Complexity [92]. The paper contents were rewritten\nand the text is mine. My contributions to the paper were: I provided a broader\ncontext and related work on smoothness regularisation in deep learning; I\nsuggested, analysed, and visualised the double descent experiments that led to\nshowing that Geometric Complexity (GC) follows the traditional U-shape curve\nassociated with complexity measures; provided coding and debugging help\nthroughout the project, which enabled the use of GC explicit regularisation,\nimproved evaluation and training. I also contributed to writing and the rebuttal.\nI did not run experiments for this project, but provided guidance for what\nexperiments to run. I did not derive the main results in the paper, but was\ninvolved in related discussions throughout the project.\nChapter 2\nOptimisation in deep learning\nIn this chapter we describe the optimisation tools and methods we will require in the\nrest of this thesis. When describing the commonly used algorithms in deep learning\nwe often take a continuous-time approach. Compared to a discrete-time approach,\na continuous-time approach tends to be more amenable to proof construction [54],\nhaving many tools available, including stability analysis, and can be used to construct\nconserved quantities [65]. Indeed, the ease of analysis of continuous-time approaches\nhas lead to a general uptake into continuous-time methods in deep learning recently,\nwith use in generative models, architectures, and optimisation [67, 93–97].\n2.1\nThe direction of steepest descent\nOnce we have formulated our machine learning problem into an optimisation problem\nmin\nθ∈RD E(θ),\n(2.1)\nwe have to choose an optimisation algorithm with which to solve the above problem.\nSolving the problem in Eq (2.1) in closed form is intractable for the problems we\nare interested in. Thus, the optimisation algorithms used are iterative, and aim to\nanswer the question: given a current set of parameters θ, what is a direction of\ndescent of function E? The simplest choice of the direction is given by the negative\ngradient −∇θE(θ), known as the direction of steepest descent. To see why, consider\nthe continuous-time flow, often referred to as the gradient flow or negative gradient\n2.1. The direction of steepest descent\n15\nflow (NGF)\n˙θ = −∇θE.\n(2.2)\nIf parameters θ follow the NGF in continuous-time, E(θ) will decrease until a\nstationary point ∇θE = 0 is reached, since\ndE\ndt = dθ\ndt\nT\n∇θE = −(∇θE)T ∇θE = −||∇θE||2 ≤0.\n(2.3)\nIf E is convex, following the NGF will find the global minimum. If E is not convex,\nfollowing the NGF in continuous-time will reach a local minimum (a proof will\nbe provided in Corollary 2.1). A point θ∗∈RD is a local minimum if there is a\nneighbourhood V around θ∗such that ∀θ′ ∈V\nE(θ′) ≤E(θ∗); a more amenable\ncharacterisation is that local minima are stationary points ∇θE(θ∗) = 0 with a\npositive semi-definite Hessian, i.e. ∇2\nθE(θ∗) has only non-negative eigenvalues. It is\noften convenient to also define strict local minima, i.e. local minima where eigenvalues\nare strictly positive.\nWe have already found two insights that would follow us throughout the rest\nof this thesis: the ease of analysis in continuous-time, and the importance of the\nnegative gradient. Before exploring the optimisation algorithms using these insights,\nwe briefly turn to the question of computing the gradient.\n2.1.1\nComputing the gradient\nTo compute the gradient ∇θE we have to consider the specification of the loss\nfunction E. In machine learning, E often takes the form of an expectation or sum\nof expectations; if the expectation is under the data distribution p∗(x) and we can\nwrite E(θ) = Ep∗(x)l(θ; x), we have\n∇θE(θ) = ∇θEp∗(x)l(θ; x) = Ep∗(x)∇θl(θ; x).\n(2.4)\nWe can now compute an unbiased estimate of the gradient via Monte Carlo estimation:\n∇θE(θ) = Ep∗(x)∇θl(θ; x) ≈1\nN\nN\nX\ni=1\n∇θl(θ; xi),\nxi ∼p∗(x).\n(2.5)\n2.2. Algorithms: from gradient descent to Adam\n16\nIn deep learning, the gradient ∇θl(θ; xi) is often computed via backpropagation,\ni.e. using the compositional structure of neural networks to compute gradients in a\nrecursive fashion via the chain rule. If the entire dataset is used to approximate the\ngradient in Eq (2.5), this corresponds to full-batch training; if an unbiased sample\nfrom the dataset is used, the training procedure is referred to as mini-batch training.\nAdditional challenges arise for objectives of the form E(θ) = Ep(x;θ)l(x), as is\nthe case for some reinforcement learning and generative modelling objectives. In such\nsituations, the change of expectation and gradient in Eq (2.4) is no longer possible.\nFor certain family of distributions p(x; θ), however, gradient estimators are available.\nIn later chapters we will use the pathwise estimator, where the random variable X\nwith distribution p(x; θ) can be written as X = g(Z; θ), in which case we can use\nthe change of variable formula for probability distributions:\nEp(x;θ)l(x) = Ep(z)l(g(Z; θ)).\n(2.6)\nFrom here we can write\n∇θEp(x;θ)l(x) = ∇θEp(z)l(g(Z; θ)) = Ep(z)∇θl(g(Z; θ)),\n(2.7)\nwhich is again amenable to Monte Carlo estimation. We refer to our general overview\nfor more details on the pathwise estimator as well as other estimators [98].\n2.2\nAlgorithms: from gradient descent to Adam\nWe have seen how following the negative gradient in continuous-time leads to\nconvergence to a local minimum. Implementing infinitely small updates in continuous\ntime on our discrete computers is not feasible, however, and thus we need a discrete-\ntime algorithm for optimisation. Discretising continuous-time flows is a heavily\nstudied topic with its own area of research in applied mathematics, numerical\nintegration [99].\nNumerical integrators provide approaches to approximate the\nsolution of the flow at time h and initial conditions θ(0), which we denote as\nθ(h; θ(0)).\n2.2. Algorithms: from gradient descent to Adam\n17\nA simple discretisation method is Euler discretisation, which when applied to\nthe NGF leads to\nθ(h; θ(0)) = θ(0) +\nZ h\n0\n˙θ(t)dt = θ(0) −\nZ h\n0\n∇θE(θ(t))dt\n(2.8)\n≈θ(0) −\nZ h\n0\n∇θE(θ(0))dt = θ(0) −h∇θE(θ(0)).\n(2.9)\nSetting θ(0) = θt−1 in the above equation leads to the familiar gradient descent\nupdate:\nθt = θt−1 −h∇θE(θt−1).\n(2.10)\nBy assuming that the vector field of the flow—the gradient ∇θE—does not change\nover the time interval h we obtained a fast discrete-time algorithm. However, due to\nthe approximation in Eq (2.9), following gradient descent is not the same as following\nthe NGF: the numerical integration error leads to a difference between the gradient\ndescent and NGF trajectories. Throughout this thesis we will refer to this difference\nin trajectories as discretisation drift (often also known as discretisation error). Due\nto discretisation drift, there are no guarantees that gradient descent will reach a\nlocal minimum. Unlike the NGF, gradient descent can diverge or converge to saddle\npoints (stationary points that have at least one negative eigenvalue of the Hessian).\nWe visualise the effects of discretisation drift in Figure 2.1. Discretisation drift\ncan speed up training (Figure 2.1a), destabilise it (Figure 2.1b) or lead to divergence\n(Figure 2.1c). Importantly, when using gradient descent, the discrete counterpart of\nthe NGF, for large h the function E can increase, leading to training instabilities.\nUnderstanding when gradient descent leads to instabilities can be challenging, due\nto intricate dependencies between the learning rate and the shape of E; this gets\ncompounded in the case of deep learning due to the very high dimensional nature of\nthe parameter space. We will tackle this question later in this thesis.\nRecent work has made great strides in understanding the behaviour of gradi-\nent descent in deep learning. Questions of study include whether gradient descent\nconverges to local or global minima [49, 100–102], the prevalence and effect of\nsaddle points [51, 103], the interaction between neural architectures and optimisa-\n2.2. Algorithms: from gradient descent to Adam\n18\ngradient descent\nNGF\nglobal min\ninit point\n(a) Stability.\n(b) Instability.\n(c) Divergence.\nFigure 2.1: The importance of the learning rate in gradient descent: small learning rates\nlead to convergence to local minima but slow to converge (a), while larger\nlearning rates can lead to instability (b) and divergence (c).\ntion [22, 23, 104], the connection between learning rates and Hessian eigenvalues and\ninstabilities [52], the effect of the learning rate on generalisation [12, 105].\nRprop and RMSProp. We have derived gradient descent as a discretisation\nof the NGF; we used the NGF since it decreases the value of the objective function E.\nWe note, however, that only the sign of the gradient for each parameter is relevant\nin order to achieve a descent direction locally. To see why, consider constants ci > 0\nand the system:\n˙θi = −ci∇θiE.\n(2.11)\nThis flow, like the NGF, also decreases E, since as ci > 0\ndE\ndt =\nX\ni\ndE\ndθi\ndθi\ndt =\nX\ni\ndE\ndθi\n(−ci∇θiE) = −\nX\ni\nci(∇θiE)2 ≤0.\n(2.12)\nThis observation is the motivation for Rprop [106], which instead of using the gradient\nas the parameter update, uses only its sign: sign(∇θiE) =\n∇θiE\nq\n(∇θiE)\n2. We can think\nof Rprop as the discretisation of the following flow at each iteration:\n˙θi = −\n1\n|∇θiE(θ0)|∇θiE.\n(2.13)\nWhile this can solve issues with exploding gradients and an imbalance between the\nmagnitude of the updates of different parameters, it can be crude: Rprop cannot\n2.2. Algorithms: from gradient descent to Adam\n19\ndistinguish between areas of space where gradient are positive, but very small or\nareas of space with very large gradients; the update in both of these cases would be\nthe learning rate h. The solution proposed to this issue was RMSprop [48], which\ninstead of normalising the gradient of each parameter by (∇θiE)2, it normalises the\ngradient by a moving average of (∇θiE)2 obtained from previous iterations. By using\nelement-wise operations, we can write the RMSprop update as\nvt = βvt−1 + (1 −β)∇θE(θt−1)2;\nθt = θt−1 −h∇θE(θt−1)\n√vt\n,\n(2.14)\nwhere β is a hyperparameter often set to values in [0.9, 0.999]. Since the denominator\nis positive, the RMSprop update has the same sign as the gradient, but a different\nmagnitude. The use of moving averages can dampen the effect of a large gradient,\nwhich would lead to instabilities in gradient descent, but still accounts for the the\nmagnitude of the gradient, not only its sign, unlike Rprop. RMSprop and its variants\nare often used in reinforcement learning [107–109].\nMomentum. Moving averages are an important staple of optimisation algo-\nrithms, beyond adjusting the magnitude of the local gradient, as we have seen with\nRMSprop. The idea behind momentum algorithms is to speed up or stabilise training\nby using previous iteration gradients.\nTo provide intuition regarding momentum, we note that along a trajectory\ntowards a local minimum the sign of the gradient will be the same throughout that\ntrajectory. Let θ∗be a local minimum of E(θ) and θt+1 and θt iterates along a\ntrajectory towards θ∗and consider parameter index i such that sign(θt+1,i −θ∗\ni ) =\nsign(θt,i −θ∗\ni ); this encodes the assumption that the trajectory goes towards a θ∗\nwithout escaping it in dimension i. But sign(θt+1,i−θ∗\ni ) = sign(∇θE(θt)i) as they are\nboth the direction which minimises E. Thus, sign(∇θE(θt)i) = sign(∇θE(θt+1)i).\nThis tells us that as long as a trajectory is not jumping over a local minimum in\na direction, the sign of the gradient in that direction does not change. Hence, one\ncan speed up training by taking bigger steps in that direction by accounting for\nthe previous updates; alternatively this is often framed as an approach to speed up\ntraining in areas of low curvature (where by definition the gradient does not change\n2.2. Algorithms: from gradient descent to Adam\n20\nsubstantially) [110]. This intuition suggests the following algorithm [111]:\nθt = θt−1 −h\nt\nX\ni=1\nβi−1∇θE(θt−i),\n(2.15)\nwhere the update is given by a geometrical average of the gradients at previous\niterations, and β is the decay rate, a hyperparameter with β < 1. We can rewrite\nthe momentum algorithm in its commonly known form:\nvt = βvt−1 −h∇θE(θt−1);\nθt = θt−1 + vt.\n(2.16)\nTraditionally momentum has been seen as a discretisation of the second order flow\n¨θ + cv ˙θ = −∇θE(θ),\n(2.17)\nfrom where the vector v can be introduced:\n˙v = −cvv −∇θE(θ);\n˙θ = v\n(2.18)\nwhich, when setting cv = 1−β\nh\nand Euler discretised with learning rates h and 1,\nleads to the discrete updates above. Momentum can also be seen as a discretisation\nof the first-order continuous-time flow, by combining implicit and explicit Euler\ndiscretisation [65]:\n˙θ = −\n1\n1 −β ∇θE.\n(2.19)\nSince β < 1 =⇒\n1\n1−β > 1, this is in line with our previous intuition of speeding\nup training. As with gradient descent, however, this intuition does not account for\nthe effect of large learning rates leading to discretisation errors; while following the\nflow in Eq (2.19) always decreases E, following the trajectory of the momentum\noptimiser instead loses that guarantee.\nWe show the effect of β and h on the\noptimisation trajectory in Figure 2.2. While momentum can speed up training and\nreach a neighbourhood of a local minimum quicker than vanilla gradient descent,\nfor large decay rates β it can lead to large updates that move further away from\n2.2. Algorithms: from gradient descent to Adam\n21\nmomentum\nglobal min\ninit point\n(a) β = 0.9 and small h.\n(b) β = 0.9 and large h.\n(c) β = 0.5 and large h.\nFigure 2.2: The importance of the learning rate h and decay rate β in gradient descent\nwith momentum. (a): momentum can speed up training but can also introduce\ninstabilities when the gradient changes direction; the learning rate used is the\nsame as in Figure 2.1a, where gradient descent converged to the minimum\nwithout instabilities. (b) and (c): for learning rates where gradient descent\ndiverges, as we observed in Figure 2.1c, momentum can stabilise training by\ndampening the effect of local large gradient magnitudes and large learning\nrates; from this simple example one can already observe the intricacies of the\ninteractions of decay rates and learning rates when momentum is used.\nthe equilibrium. We show this effect in Figure 2.2a, where we observe that despite\nusing a small learning rate, with a high decay rate β momentum can oscillate around\nthe minimum significantly before reaching it, while gradient descent with the same\nlearning rate does not oscillate around the minimum (Figure 2.1a). This effect can\nbe mitigated by using a smaller decay rate, as exemplified in Figure 2.2c. Dampening\nthe strength of the current gradient update through momentum can also stabilise\ntraining, as we show in Figure 2.2b. While for the same learning rate gradient descent\ndiverges (Figure 2.1c), momentum does not, since the large local gradients moving\naway from the equilibrium are dampened by the moving averages obtained from\npast gradients. For quadratic functions, this difference between gradient descent\nand momentum can be seen in the smallest learning rate which leads to divergence:\nwhile gradient descent diverges if h > 2/λ0 where λ0 is the largest eigenvalue of the\nHessian ∇2\nθE, for momentum divergence occurs when h > 2/λ0 +2β/λ0 ≥2/λ0 since\nβ ≥0 [52]. We will come back to the importance of the learning rate relative to the\nHessian eigenvalues in later chapters.\nGradient descent with momentum is still very much used for deep learning opti-\n2.2. Algorithms: from gradient descent to Adam\n22\nmisation and has been credited with increased generalisation performance compared\nto other optimisers [112, 113], though recent work has shown that other well tuned\nmethods can outperform momentum if well tuned [47].\nThe Adam optimiser [47], perhaps the most popular optimisation algorithm in\ndeep learning, combines some of the previously discussed insights: it uses momentum\nand an RMSprop-like approach of dividing by a moving average of the square gradient\n(all operations below are element-wise):\nmt = β1mt−1 + (1 −β1)∇θE(θt−1)\n(2.20)\nvt = β2vt−1 + (1 −β2)∇θE(θt−1)2\n(2.21)\nθt = θt−1 −h\n1\n1−βt\n1mt\nq\n1\n1−βt\n2vt + ϵ\n,\n(2.22)\nwhere β1, β2 and ϵ are hyperparameters usually set to β2 = 0.999 and β1 ∈[0, 0.9]\nand ϵ ∈[10−1, 10−8] with the default at 10−8. The division in Eq (2.22) is obtained\nusing a bias correction argument for the moving averages. To see this, we write:\nmt = (1 −β1)\nt\nX\ni=1\nβi−1\n1\n∇θE(θt−i).\n(2.23)\nThe authors make the argument that if the gradients at iteration i ∇θE(θi) are\ndrawn from the distribution pg,i\nEpm[mt] = (1 −β1)\nt\nX\ni=1\nβi−1\n1\nEpg,i [∇θE(θt−i)]\n(2.24)\n= (1 −βt\n1)Epg [∇θE(θt−1)] + ξ,\n(2.25)\nwhere ξ is a bias term accounting for the changes in the expected value of the gradients\nat different iterations; it will be 0 if the gradient mean does not change between\niterations otherwise it is expected to be small due to the decaying contribution of\nolder gradients. Thus, to ensure the expected value of the moving average stays\nthe same as the gradient and does not decrease steadily as training progresses, the\n2.2. Algorithms: from gradient descent to Adam\n23\ndivision in Eq (2.22) is used; a similar argument can be made for the second moment.\nThis ensures that in expectation the update (if a small ϵ is used), is approximately\nof magnitude h for each parameter. We note that while the assumption that the\nbias ξ is small does not hold for certain areas of training, such as when the gradient\nchanges direction due to instability around an equilibrium, the above argument holds\nin areas of low curvature (i.e. areas of space where the gradient does not change\nsubstantially); this is something we will use later.\nBy investigating the commonly used Adam hyperparameters, β1 ∈[0, 0.9] and\nβ2 = 0.999, we get intuition about its behaviour. Since β1 is smaller than β2, Adam\nis more sensitive to the sign of the gradient (controlled by β1) than its magnitude\n(controlled by β2). Figures 2.3a and 2.3b show that the exponential moving averages\nin the denominator of the Adam update can dampen large updates compared to\nmomentum (the corresponding momentum figures are shown in Figure 2.2a and 2.2b),\nbut oscillations around the minimum still occur. When a low decay rate for the\nnominator is used, such as β1 = 0.5, in conjunction with a large h, instabilities can\noccur if small gradients are followed by a large gradient, since mt will be large but vt\nwill remain small due to β2 = 0.999. We see this in Figure 2.3c, which we can contrast\nwith momentum for β = β1 and the same learning rate h in Figure 2.2c: momentum\nexhibits stable training, and does not exit the area around the equilibrium. Relatedly,\nthe use of moving averages in the denominator vt has been linked to the lack of\nconvergence in Adam for simple online convex optimisation problems [114].\nThe hyperparameter ϵ does not change the sign of the update, but it can change\nits magnitude. Specifically, a high ϵ decreases the magnitude of the parameter update.\nWe show the effect of ϵ in Figure 2.3, where we show that using ϵ = 10−2 (bottom\nrow) can lead to more stable trajectories compared to ϵ = 10−8 (top row).\nAdam is a very popular the optimiser in deep learning, and has been attributed\nwith progress in various fields including GANs [31], and training modern models\nsuch as transformers [115, 116]. The ϵ hyperparameter in Adam has been noted to\nbe important for certain machine learning applications [116], including reinforcement\nlearning, where the ϵ used is large compared to supervised learning [109, 117].\n2.3. Continuous-time methods\n24\nAdam\nglobal min\ninit point\n(a) β1 = 0.9, small h, ϵ = 10−8. (b) β1 = 0.9, large h, ϵ = 10−8.\n(c) β1 = 0.5, large h, ϵ = 10−8.\n(d) β1 = 0.9, small h, ϵ = 10−2. (e) β1 = 0.9, small h, ϵ = 10−2.\n(f) β1 = 0.5, large h, ϵ = 10−2.\nFigure 2.3: The importance of the Adam hyperparameters: learning rate h and decay\nrate β1 and ϵ.\nWe use the default β2 = 0.999.\nTop row: the update\nnormalisation in Adam can stabilise training but can also lead to instability\ncompared to momentum; this row uses ϵ = 10−8, the default value and\nthe other hyperparameters are the same as in Figure 2.2 which shows the\ncorresponding momentum trajectories. Bottom row: while keeping other\nhyperparameters fixed, changing ϵ to 10−2 leads to vastly different behaviours\nand can stabilise training by avoiding dividing by small values.\n2.3\nContinuous-time methods\nWe now describe the continuous-time approaches we will use in the rest of this thesis.\n2.3.1\nA brief overview of stability analysis\nStability analysis is a tool for continuous-time systems that helps decide whether\nθ∗∈RD is a stable fixed point for a continuous-time flow\n˙θ = f(θ),\n(2.26)\n2.3. Continuous-time methods\n25\nwhere f : RD →RD is the vector field of the flow. The most used definitions of\nstability are Lyapunov stability and asymptotic stability. Lyapunov stability requires\n∀ϵ > 0, ∃δ > 0\n∥θ(0) −θ∗∥< δ =⇒\n∀t ≥0 ∥θ(t) −θ∗∥< ϵ.\n(2.27)\nAsymptotic stability requires Lyapunov stability and additionally\n∃δ > 0\n∥θ(0) −θ∗∥< δ =⇒lim\nt→∞∥θ(t) −θ∗∥= 0.\n(2.28)\nThe rate of convergence of asymptotic stability is exponential if\n∃δ, C, α > 0\n∥θ(0) −θ∗∥< δ =⇒∀t ≥0 ∥θ(t) −θ∗∥≤C ∥θ(0) −θ∗∥e−αt.\n(2.29)\nExponential asymptotic stability can be established using the following criterion:\nRemark 2.3.1 (From [118]) A fixed point θ∗with f(θ∗) = 0 where the Jacobian of\nthe vector field df\ndθ(θ∗) has eigenvalues with strictly negative real part is a stable fixed\npoint of the flow ˙θ = f(θ), with asymptotic exponential convergence. In contrast, if\na fixed point has a Jacobian with an eigenvalue with strictly positive real part, that is\nan unstable equilibrium and the flow will not converge to it.\nRemark 2.3.1 provides us with a useful test to establish convergence based on a fixed\npoint’s Jacobian eigenvalues. The test is inconclusive for equilibria with non-strictly\nnegative real part Jacobian eigenvalues, i.e. Jacobians where a subset of eigenvalues\nare have strictly negative real part while some have 0 real part. Throughout this\nthesis, when performing stability analysis of a continuous-time system we will be\ntesting exponential asymptotic stability using the above criterion.\nCorollary 2.1 (The NGF and local minima.) The NGF ˙θ = −∇θE is only\nlocally attracted to local minima of E, and all strict local minima are exponentially\nattractive under the NGF.\nThis follows from the application of Remark 2.3.1. If a stationary point ∇θE(θ∗) = 0\nis attractive under the NGF, the eigenvalues of the Jacobian\nd(−∇θE)\ndθ\n(θ∗) have\n2.3. Continuous-time methods\n26\nstrictly negative or 0 real part. Since d(−∇θE)\ndθ\n(θ∗) = −∇2\nθE is a symmetric matrix,\nthis condition is equivalent to the eigenvalues of ∇2\nθE being non-negative, the local\nminimum condition. Conversely, since strict local minima have strictly positive\neigenvalues, the Jacobian has strictly negative eigenvalues at strict local minima,\nleading to exponential local stability under the NGF.\n2.3.2\nBackward error analysis\nBackward error analysis (BEA) is a tool in numerical analysis developed to understand\nthe discretisation error of numerical integrators. We now present an overview of how\nto use BEA in the context of the gradient descent update θt = θt−1 −h∇θE(θt−1)\nwith θ ∈RD; for a general overview see Hairer et al. [99]. BEA provides a modified\nvector field\n˜fn(θ) = −∇θE + hf1(θ) + · · · + hnfn(θ)\n(2.30)\nby finding functions f1 : RD →RD, ..., fn : RD →RD, such that the solution of the\nmodified flow at order n, that is,\n˙θ = −∇θE + hf1(θ) + · · · + hnfn(θ)\n(2.31)\nfollows the discrete dynamics of the gradient descent update with an error ∥θt−θ(h)∥\nof order O(hn+2), where θ(h) is the solution of the modified equation truncated at\norder n at time h and θ(0) = θt−1. The full modified vector field with all orders\n(n →∞)\n˜f(θ) = −∇θE + hf1(θ) + · · · + hnfn(θ) + · · ·\n(2.32)\nis usually divergent and only forms an asymptotic expansion. What BEA provides is\nthe Taylor expansion in h of an unknown h-dependent vector field fh(θ) developed\nat h = 0:\n˜f(θ) = Taylorh=0fh(θ).\n(2.33)\nThus, a strategy for finding fh is to find a series of the form in Eq (2.32) via BEA\nand then find the function fh such that its Taylor expansion in h at 0 results in the\nfound series. Using this approach we can find the flow ˙θ = fh(θ) which describes\n2.3. Continuous-time methods\n27\nthe gradient descent step θt = θt−1 −h∇θE(θt−1) exactly.\nWhile flows obtained using BEA are constructed to approximate one gradient\ndescent step, the same flows can be used over multiple gradient descent steps as\nshown in Section A.1.8 in the Appendix.\nBarrett and Dherin [12] used this technique to find the O(h2) correction term of\ngradient descent in the single-objective setting. They showed that for a model with\nparameters θ and loss E(θ), optimised with gradient descent θt = θt−1 −h∇θE(θ),\nthe first-order modified equation is\n˙θ = −∇θE −h\n2∇2\nθE∇θE,\n(2.34)\nwhich can be written as\n˙θ = −∇θ ˜E(θ) = −∇θ\n\u0012\nE(θ) + h\n4 ∥∇θE(θ)∥2\n\u0013\n.\n(2.35)\nThus, gradient descent can be seen as implicitly minimising the modified loss\n˜E(θ) = E(θ) + h\n4 ∥∇θE(θ)∥2 .\n(2.36)\nThis shows that when training models with gradient descent, there is an implicit\nregularisation effect, dependent on learning rate h, which biases learning towards\npaths with low gradient norms. The authors refer to this phenomenon as ‘implicit\ngradient regularisation’; we will thus refer to the flow in Eq (2.34) as the IGR flow.\n2.3.3\nBEA proofs\nThe general structure of BEA proofs is as follows: start with a Taylor expansion in\nh of the modified flow in Eq (2.31); write each term in the Taylor expansion as a\nfunction of ∇θE and the desired fi using the chain rule; group together terms of the\nsame order in h in the expansion; and identify fi such that all terms of O(hp) are 0\nfor p ≥2, as is the case in the gradient descent update. A formal overview of BEA\nproofs can be found in Section A.1.1 in the Appendix.\nWe now exemplify how to use BEA to find the IGR flow we showed in\n2.3. Continuous-time methods\n28\nEq (2.34) [12]. Since we are only looking for the first correction term, we only\nneed to find f1. We perform a Taylor expansion to find the value of θ(h) up to order\nO(h3) and then identify f1 from that expression such that the error ∥θt −θ(h)∥is\nof order O(h3). We have\nθ(h) = θt−1 + hθ(1)(θt−1) + h2\n2 θ(2)(θt−1) + O(h3).\n(2.37)\nWe know by the definition of the modified vector field in Eq (2.31) that\nθ(1) = −∇θE + hf1(θ).\n(2.38)\nWe can then use the chain rule to obtain\nθ(2) = d (−∇θE + hf1(θ))\ndt\n= −d∇θE\ndt\n+ O(h) = ∇2\nθE∇θE + O(h).\n(2.39)\nThus,\nθ(h) = θt−1 −h∇θE(θt−1) + h2f1(θt−1) + h2\n2 ∇2\nθE(θt−1)∇θE(θt−1) + O(h3).\n(2.40)\nWe can then write\nθt −θ(h) = θt−1 −h∇θE(θt−1) −θ(h)\n(2.41)\n= h2f1(θt−1) + h2\n2 ∇2\nθE(θt−1)∇θE(θt−1) + O(h3).\n(2.42)\nFor the error to be of order O(h3) the terms of order O(h2) have to be 0. This entails\nf1(θt−1) = −1\n2∇2\nθE(θt−1)∇θE(θt−1),\n(2.43)\nfrom which we conclude to f1 = −1\n2∇2\nθE∇θE leading to Eq (2.34).\n2.4. Multiple objective optimisation\n29\n2.4\nMultiple objective optimisation\nWe have thus far discussed optimisers for single-objective problems, which encom-\npasses many machine learning use cases, including supervised learning and certain\nformulations of reinforcement learning and generative modelling. There are, however,\nuse cases in machine learning beyond single-objective problems, including GANs and\ntypes of reinforcement learning, such as actor-critic frameworks. These settings are\noften formulated via nested minimisation problems:\nmin\nθ Eθ(arg min\nϕ Eϕ(ϕ, θ), θ).\n(2.44)\nSolving nested optimisation problems is computationally expensive, as it would\nrequire solving the inner optimisation problem for each parameter update of the\nouter optimisation problem. Thus, in deep learning practice nested optimisation\nproblems are often translated into problems of the form\nmin\nϕ Eϕ(ϕ, θ)\n(2.45)\nmin\nθ Eθ(ϕ, θ).\n(2.46)\nMulti-objective optimisation problems of this kind are often cast as a game. Each\nplayer has a set of actions to take, in the form of an update for their parameters,\nϕ for the first player, and θ for the second. The loss functions are the negative of\nthe player’s respective payoff. As it is common in the literature, we will often adapt\nthe game theory nomenclature in this setting; this connection allows for a bridge\nbetween the machine learning and the game theory communities.\nWhen translating problems of the form in Eq (2.44) into optimisation proce-\ndures that preserve the nested structure of a game it is common to use one of\nthe single-objective algorithms discussed Section 2.2 to update the first player’s\nparameters, followed by the use of the same algorithm to update the second player’s\nparameters. This approach is often referred to as alternating updates; we summarise\nit in Algorithm 1. Optimisation approaches that further account for the nested game\n2.4. Multiple objective optimisation\n30\nAlgorithm 1 Alternating updates\nk ≥0; the number of first player updates for each second player update\nChoice of optimiser routine(E, current value); e.g: gd(E, θt) = θt −h∇θE(θt)\nInitialise ϕ0, θ0\nt ←0\nwhile training do\nϕ ←ϕt\nfor i in {1, ..., k} do\nϕ ←optimiser routine(Eϕ(·, θt), ϕ)\nend for\nϕt+1 ←ϕ\nθt+1 ←optimiser routine(Eθ(ϕt+1, ·), θt)\nt ←t + 1\nend while\nAlgorithm 2 Simultaneous updates\nInitialise ϕ0, θ0\nChoice of optimiser routine(E, current value); e.g: gd(E, θt) = θt −h∇θE(θt)\nwhile training do\nϕt+1 ←optimiser routine(Eϕ(·, θt), ϕt)\nθt+1 ←optimiser routine(Eθ(ϕt, ·), θt)\nt ←t + 1\nend while\nstructure do exist [119], but they tend to be more computationally prohibitive and\nthus less used in practice. In some settings, there is no nested structure or the nested\nstructure is further discarded and both players are updated simultaneously, as we\nshow in Algorithm 2.\nSimultaneous gradient descent updates derived for solving the problem in\nEqs (2.45) and (2.46)—using gradient descent as the optimiser in Algorithm 2—\ncan be seen as the Euler discretisation of the continuous-time flow\n˙ϕ = −∇ϕEϕ\n(2.47)\n˙θ = −∇θEθ.\n(2.48)\nAnalysis of these game dynamics is more challenging than in the single objective\ncase (shown in Eq (2.3)), as we can no longer ascertain that following such flows\n2.4. Multiple objective optimisation\n31\nminimises the corresponding loss functions since\ndEϕ\ndt\n= dϕ\ndt\nT\n∇ϕEϕ + dθ\ndt\nT\n∇θEϕ = −∥∇ϕEϕ∥2 −∇θET\nθ ∇θEϕ,\n(2.49)\nwhich need not be negative. Furthermore, it is unclear how to transfer the alternating\nstructure that preserves player order from Algorithm 1 into a continuous-time flow; we\nwill address this in a later chapter. Nonetheless, the flows in Eqs (2.47) and (2.48) are\noften studied to understand the behaviour of gradient descent in nested optimisation\nand games [17, 61, 67, 120].\nThe challenges pertaining to analysing optimisation in games get further com-\npounded when considering the notions of equilibria. The natural translation of local\nminimum from single-objective optimisation to games is a local Nash equilibrium. A\nlocal Nash equilibrium is a point in parameter space (ϕ∗, θ∗) such that no player has\nan incentive to deviate from (ϕ∗, θ∗) to another point in its local neighbourhood;\nthat is ϕ∗is a local minimum for Eϕ(·, θ∗) and θ∗is a local minimum for Eθ(ϕ∗, ·).\nEquivalently, ∇ϕEϕ(ϕ∗, θ∗) = 0 and ∇θEθ(ϕ∗, θ∗) = 0 and ∇2\nϕEϕ(ϕ∗, θ∗) and\n∇2\nθEθ(ϕ∗, θ∗) are positive semi-definite, i.e. do not have negative eigenvalues.\nWhile in the single-objective case, local minima are the only exponentially\nattractive equilibria of the NGF (see Corollary 2.1), this correspondence between\ndesired equilibrium and attractive dynamics does not generally occur in games. By\napplying stability analysis (Remark 2.3.1), we know that the system in Eqs (2.47)\nand (2.48) is attracted to equilibria for which the Jacobian\nJ =\n\n−dEϕ\ndϕdϕ\n−dEϕ\ndθdϕ\n−dEθ\ndϕdθ\n−dEθ\ndθdθ\n\n\n(2.50)\nevaluated at (ϕ∗, θ∗) has only eigenvalues with strictly negative real part. In contrast,\na Nash equilibrium requires that the block diagonals of J, namely −∇2\nϕEϕ(ϕ∗, θ∗)\nand −∇2\nθEθ(ϕ∗, θ∗), have negative eigenvalues. Since for an unconstrained matrix\nthere is no relationship between its eigenvalues and the eigenvalues of its block\ndiagonals, there is no inclusion relationship between Nash equilibria and locally\n2.5. Conclusion\n32\nattractive equilibria of the underlying continuous game dynamics that applies to all\ntwo-player games.\nFor zero-sum games, i.e. games where Eϕ = −Eθ = E, a strict Nash equilibrium\nwill be locally attractive when following the flow in Eqs (2.47) and (2.48). To see\nwhy, we have to examine the eigenvalues of the flow’s Jacobian\nJ =\n\n−dE\ndϕdϕ\n−dE\ndθdϕ\ndE\ndϕdθ\ndE\ndθdθ\n\n\n(2.51)\nevaluated at a strict Nash equilibrium of such game. Due to the zero-sum formulation,\nthe off-diagonal blocks of J satisfy −dE\ndθdϕ = −( dE\ndϕdθ)T, and we have\nJ + JT =\n\n−∇2\nϕE\n0\n0\n∇2\nθE\n\n.\n(2.52)\nThus, J + JT at the Nash equilibrium (ϕ∗, θ∗) is a symmetric block diagonal matrix\nwith negative definite blocks, −∇2\nϕE(ϕ∗, θ∗) and ∇2\nθE(ϕ∗, θ∗), and thus has negative\neigenvalues. From here it follows that J has eigenvalues with strictly negative real\npart, and thus satisfies the condition of Remark 2.3.1. We note that the reverse does\nnot hold, as being locally attractive in this case is a weaker condition than that of a\nNash equilibrium. This observation together with the realisation that in deep learning\ncertain games might not have Nash equilibria [121], has lead the search for other\nequilibrium measures including local stability and Stackelberg equilibria [69, 122, 123].\n2.5\nConclusion\nIn this section we have introduced the main optimisation algorithms used in deep\nlearning, together with their main challenges and the analytical tools we will employ\nin the rest of this thesis.\nChapter 3\nA new continuous-time model of\ngradient descent\nThe recipe behind the success of deep learning has been training neural networks\nwith gradient-based optimisation on large datasets. Understanding the behaviour\nof gradient descent, however, and particularly its instability, has lagged behind its\nempirical success. To add to the theoretical tools available to study gradient descent\nwe propose the principal flow (PF), a continuous-time flow that approximates gradient\ndescent dynamics. To the best of our knowledge, the PF is the only continuous\nflow that captures the divergent and oscillatory behaviours of gradient descent,\nincluding escaping local minima and saddle points. Through its dependence on\nthe eigen-decomposition of the Hessian the PF sheds light on the recently observed\nedge of stability phenomena in deep learning [52]. Using our new understanding of\ninstability we propose a learning rate adaptation method that enables us to control\nthe trade-off between training stability and test set evaluation performance.\n3.1\nIntroduction\nOur goal is to use continuous-time models to understand the behaviour of gradient\ndescent.\nUsing continuous-time dynamics to understand discrete-time systems\nopens up tools from dynamical systems, such as stability analysis, and has a long\nhistory in optimisation and machine learning [11–13, 17, 54–60]. Most theoretical\nanalysis of gradient descent using continuous-time systems uses the negative gradient\nflow, but this has well-known limitations, such as not being able to explain any\n3.1. Introduction\n34\nbehaviour contingent on the learning rate. To mitigate these limitations we find a\nnew continuous-time flow that reveals important new roles of the Hessian in gradient\ndescent training. To do so, we use backward error analysis (BEA), a method with a\nlong history in the numerical integration community [99] that has only recently been\nused in the deep learning context [12, 13].\nWe find that the proposed flow sheds new light on gradient descent stability,\nincluding, but not limited to, divergent and oscillatory behaviour around a fixed\npoint.\nInstability—areas of training where the loss consistently increases—and\nedge of stability behaviours [52]—areas of training where the loss does not behave\nmonotonically but decreases over long time periods—are pervasive in deep learning\nand occur for all learning rates and architectures [52, 53, 124, 125]. We use our novel\ninsights to understand and mitigate these instabilities.\nThe structure of this chapter is as follows:\n• We discuss the advantages of a continuous-time approach in Section 3.2, where\nwe also highlight the limitations of existing continuous-time flows.\n• We introduce the principal flow (PF), a flow in complex space defined by the\neigen-decomposition of the Hessian (Section 3.3). To the best of our knowledge\nthe PF is the first continuous-time flow that captures that gradient descent can\ndiverge around local minima and saddle points. We show that using a complex\nflow is crucial in understanding instabilities in gradient descent.\n• We show the PF is better than existing flows at modelling neural network\ntraining dynamics in Section 3.4. In Section 3.5, we use the PF to shed new\nlight on edge of stability behaviours in deep learning. We do so by connecting\nchanges in the loss and Hessian eigenvalues with core quantities exposed by the\nPF and neural network landscapes explored through the behaviour of gradient\nflows.\n• Through a continuous-time perspective we demonstrate empirically how to\ncontrol the trade-off between stability and performance in deep learning in\nSection 3.7. We do so using DAL (Drift Adjusted Learning), an approach to\n3.2. Continuous time models of gradient descent\n35\nsetting the learning rate dynamically based on insights on instability derived\nfrom the PF.\n• We end by showcasing the potential of integrating our continuous-time approach\nwith other optimisation schemes and highlighting how the PF can be used as a\ntool for existing continuous-time analyses in Section 3.8.\nNotation: We denote as E the loss function, θ the parameter vector of dimension\nD, ∇2\nθE ∈RD×D the loss Hessian and λi the Hessian’s ith largest eigenvalue with ui\nthe corresponding eigenvector. Since if ui is an eigenvector of ∇2\nθE so is −ui, we\nalways use ui such that ℜ[∇θETui] ≥0; this has no effect on our results and is only\nused for convenience and we used the notation ℜ(z) to be the real part of z. For a\ncontinuous-time flow θ(h) refers to the solution of the flow at time h.\n3.2\nContinuous time models of gradient descent\nThe aim of this work is to understand the dynamics of gradient descent updates with\nlearning rate h\nθt = θt−1 −h∇θE(θt−1)\n(3.1)\nfrom the perspective of continuous dynamics. When using continuous-time dynamics\nto understand gradient descent it is most common to use the negative gradient flow\n(NGF)\n˙θ = −∇θE.\n(3.2)\nGradient descent can be obtained from the NGF through Euler numerical integration,\nwith an error of O(h2) after one gradient descent step, see Section 2.2. Studying\ngradient descent and its behaviour around equilibria and beyond has thus taken\ntwo main approaches: directly studying the discrete updates of Eq (3.1) [15, 49,\n64, 102, 126–130], or the continuous-time NGF of Eq (3.2) [11, 17, 54–61]. The\nappeal of continuous-time systems lies in their connection with dynamical systems\nand the plethora of tools that thus become available, such as stability analysis; the\n3.2. Continuous time models of gradient descent\n36\nsimplicity by which conserved quantities can be obtained [60, 131]; and analogies\nthat can be constructed through similarities with physical systems [60]. Because\nof the availability of tools for the analysis of continuous-time systems, it has been\npreviously noted that discrete-time approaches are often more challenging and\ndiscrete-time proofs are often inspired from continuous-time ones [54, 62]. We use\nan example to showcase the ease of continuous-time analyses: when following the\nNGF the loss E decreases since dE\ndt = −||∇θE||2, as we derived in Eq 2.3. Showing\nthat and when following the discrete-time gradient descent update in Eq (3.1)\nis more challenging and requires adapting the analysis on the form of the loss\nfunction E. Classical convergence guarantees associated with other optimisation\napproaches, such as natural gradient, are also derived in continuous-time [132–134].\nBy analysing the properties of continuous-time systems one can also determine\nwhether optimisers should more closely follow the underlying continuous-time flow\n[67, 135], what regularisers should be constructed to ensure convergence or stability\n[17, 61, 136], construct converge guarantees in functional space for infinitely wide\nnetworks [100, 137].\n3.2.1\nLimitations of existing continuous-time flows\nThe well-known discrepancy between Euler integration and the NGF, often called\ndiscretisation error or discretisation drift (Figure 3.1) leads to certain limitations\nwhen using the NGF to describe gradient descent: the NGF cannot explain divergence\naround a local minimum for high learning rates or convergence to flat minima as\noften seen in the training of neural networks. Critically, since the NGF does not\ndepend on the learning rate, it cannot explain any learning rate dependent behaviour.\nThe appeal of continuous-time methods together with the limitations of the NGF\nhave inspired the machine learning community to look for other continuous-time\nsystems that may better approximate the gradient descent trajectory. One approach\nto constructing continuous-time flows approximating gradient descent that takes\ninto account the learning rate is backward error analysis (BEA); for an overview of\nBEA, see Section 2.3.2. Using this approach, Barrett and Dherin [12] introduce the\n3.2. Continuous time models of gradient descent\n37\nθt−1\nθt = θt−1 −h∇θE(θt−1)\n˙θ = −∇θE\nθ(h)\ndiscretisation\ndrift\nFigure 3.1: Discretisation drift. Using continuous-time flows to understand gradient\ndescent is limited by the gap between the discrete and continuous dynamics.\nIn the case of the negative gradient flow ˙θ = −∇θE, we call this gap\ndiscretisation drift. Since the negative gradient flow always minimises E,\ninstabilities in gradient descent—areas where the loss E increases—are due\nto discretisation drift.\nImplicit Gradient Regularisation flow (IGR flow):\n˙θ = −∇θE −h\n2∇2\nθE∇θE,\n(3.3)\nwhich tracks the dynamics of the gradient descent step θt = θt−1 −h∇θE(θt−1)\nwith an error of O(h3), thus reducing the order of the error compared to the NGF.\nUnlike the NGF flow, the IGR flow depends on the learning rate h. This dependence\nexplains certain properties of gradient descent, such as avoiding trajectories with\nhigh gradient norm; the authors connect this behaviour to convergence to flat local\nminima.\nLike the NGF flow, however, the IGR flow does not explain the instabilities\nof gradient descent, as we illustrate in Figure 3.2. Indeed, Barrett and Dherin [12]\n(their Remark 3.4) show that performing stability analysis around local minima\nusing the IGR flow does not lead to qualitatively different conclusions from those\nusing the NGF: both NGF and the IGR flow predict gradient descent to be always\nlocally attractive around a local minimum (proofs in Section A.1.7), contradicting\nthe empirically observed behaviour of gradient descent.\nTo understand why both the NFG and the IGR flow cannot capture oscillations\nand divergence around a local minimum, we note that stationary points ∇θE = 0\nare fixed points for both flows. We visualise an example in Figure 3.3a: since to\n3.2. Continuous time models of gradient descent\n38\n0.4\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n1\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n2\ngradient descent\nNGF\nIGR flow\nglobal min\ninit point\n(a) 2D convex case.\n2\n1\n0\n1\n2\n1\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2\ngradient descent\nNGF\nIGR flow\ninit point\nglobal min\n0\n0\n1\n10\n100\n1000\n10000\n(b) Banana function.\nFigure 3.2: Motivation. Existing continuous-time flows fail to capture the oscillatory\nor unstable behaviour of gradient descent.\n−1.0 −0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nθ\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nRe[L(θ)]\nloss\nNGF\nIGR flow\ngradient descent\nstarting point\nglobal minimum\n(a) Real flows.\n−1.0 −0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nRe[θ]\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nRe[L(θ)]\nloss\nprincipal flow (ours)\ngradient descent\nstarting point\nglobal minimum\n(b) Complex flow.\nFigure 3.3: Complex flows capture oscillations and divergence around local\nminima. In the real space, the trajectory going from the starting point to\nthe second gradient descent iterate goes through the global minima, and real\nflows stop there. In complex space however, that need not be the case.\ngo from the initial point to the gradient descent iterates requires passing through\nthe local minimum, both flows would stop at the local minimum and never reach\nthe following gradient descent iterates. In the case of neural networks we show in\nFigure 3.4 that while the IGR flow is better than the NGF at describing gradient\ndescent, a substantial gap remains.\nThe lack of ability of existing continuous-time flows to model instabilities\nempirically observed in gradient descent such as those shown in Figure 3.2 has been\nused as a motivation to use discrete-time methods instead [63, 64]. The goal of\nour work is to overcome this issue by introducing a novel continuous-time flow that\n3.2. Continuous time models of gradient descent\n39\n0.0\n0.1\n0.2\n0.3\n|| (h;\nt\n1)\nt||\nIGR flow\nNGF\n0\n100\n200\n300\n400\n500\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nIGR errror / NGF error\nMNIST 4 layer MLP\n(a) MNIST.\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n|| (h;\nt\n1)\nt||\nIGR flow\nNGF\n0\n200\n400\n600\n800\n1000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nIGR errror / NGF error\nCIFAR-10, VGG\n(b) CIFAR-10.\nFigure 3.4: Motivation.\nWhile the IGR flow captures aspects of the per-iteration\ndiscretisation drift for neural networks trained with gradient descent, a\nsignificant gap remains. To measure how well each flow captures gradient\ndescent dynamics at one iteration, we train a model using gradient descent\nθt = θt−1−h∇θE(θt−1) and at each iteration t we approximate the respective\nflows for time h. We then compute the difference in norms between the\nresulting flow parameters and the gradient descent parameters at the next\niteration: ∥θt −θ(h)∥with θ(0) = θt−1, which we report in the top row. The\nbottom row reports the ratio between these error norms obtained with the IGR\nand NGF flow, in order to provide a relative error scale. We observe that in\nthe two experiments we perform here, the IGR flow captures approximatively\n10% of discretisation drift at the majority of training iterations.\ncaptures instabilities observed in gradient descent. To do so, we follow the footsteps\nof Barrett and Dherin [12] and use BEA. By using a continuous-time flow we can\nleverage the tools and advantages of continuous-time methods discussed earlier in\nthis section; by incorporating discretisation drift into our model of gradient descent\nwe can increase their applicability to explain unstable training behaviour. Indeed,\nwe show in Figure 3.3b that the flow we propose captures the training instabilities;\na key reason why is that, unlike existing flows, it operates in complex space. In\n3.3. The principal flow\n40\nSection 3.3, we show the importance of operating in complex space in order to\nunderstand oscillatory and instability behaviours of gradient descent.\n3.3\nThe principal flow\nIn the previous section, we have seen how BEA can be used to define continuous-time\nflows that capture the dynamics of gradient descent up to a certain order in learning\nrate. We have also explored the limitations of these flows, including the lack of\nability to explain oscillations observed empirically when using gradient descent. To\nfurther expand our understanding of gradient descent via continuous-time methods,\nwe would like to get an intuition for the structure of higher order modified vector\nfields provided by BEA. We start with the following modified vector field, which we\nwill call the third-order flow (proof in Section A.1.2):\n˙θ = −∇θE −h\n2∇2\nθE∇θE −h2\n\u00121\n3(∇2\nθE)2∇θE + 1\n12∇θET(∇3\nθE)∇θE\n\u0013\n,\n(3.4)\nwith ∇3\nθE ∈RD×D×D and (∇θET(∇3\nθE)∇θE)k = P\ni,j (∇θE)i(∇3\nθE)i,k,j(∇θE)j.\nThe\nthird-order\nflow\ntracks\nthe\ndynamics\nof\nthe\ngradient\ndescent\nstep\nθt = θt−1 −h∇θE(θt−1) with an error of O(h4), thus further reducing the or-\nder of the error compared to the IGR flow. Like the IGR flow and the NGF, the\nthird-order flow has the property that ˙θ = 0 if ∇θE = 0 and thus will exhibit the\nsame limitations observed in Figure 3.3. The third-order flow allows us to spot a\npattern: the correction term of order O(hn) in the BEA modified flow describing\ngradient descent contains the term (∇2\nθE)n∇θE and terms that contain higher order\nderivatives with respect to parameters, terms which we will denote as C(∇3\nθE).\nOur approach. We will use the terms of the form (∇2\nθE)n∇θE to construct a\nnew continuous-time flow. We will take a three-step approach. First, for an arbitrary\norder O(hn) we will find the terms containing only first and second order derivatives\nin the modified vector field given by BEA and show they are of the form (∇2\nθE)n∇θE\n(Theorem 3.3.1). Second, we will use all orders to create a series (Corollary 3.1).\nThird, we will use the series to find the modified flow given by BEA (Theorem 3.3.2).\nAll proofs are provided in Section A.1 of the Appendix.\n3.3. The principal flow\n41\nTheorem 3.3.1 The modified vector field with an error of order O(hn+2) to the\ngradient descent update θt = θt−1 −h∇θE(θt−1) has the form:\n˙θ =\nn\nX\np=0\n−1\np + 1hp(∇2\nθE)p∇θE + C(∇3\nθE),\n(3.5)\nwhere C(∇3\nθE) denotes the family of functions that can be written as a sum of terms,\neach term containing a derivative of higher order than 3 with respect to parameters.\nThe result is proven by induction, with the full proof provided in Section A.1.3\nof the Appendix. The base cases for n = 1, 2, and 3 follow from the NGF, IGR,\nand third-order flows. For higher order terms, the proof uses induction to find the\nterm in fi depending on ∇2\nθE and ∇θE only and follows the BEA proof structure\nhighlighted in Section 2.3.2, but Step 3 is modified to not account for terms in\nC(∇3\nθE). From the above, we can obtain the following corollary by using all orders\nn and the eigen-decomposition of ∇2\nθE:\nCorollary 3.1 The full order modified flow obtained by performing BEA on gradient\ndescent updates is of the form:\n˙θ =\n∞\nX\np=0\n−1\np + 1hp(∇2\nθE)p∇θE + C(∇3\nθE)\n(3.6)\n=\n∞\nX\np=0\n−1\np + 1hp\n D−1\nX\ni=0\nλp\ni uiuT\ni\n!\n∇θE + C(∇3\nθE)\n(3.7)\n=\nD−1\nX\ni=0\n ∞\nX\np=0\n−1\np + 1hpλp\ni\n!\n(∇θETui)ui + C(∇3\nθE),\n(3.8)\nwhere λi and ui are the respective eigenvalues and eigenvectors of the Hessian ∇2\nθE.\nIf λ0 > 1/h the BEA series above diverges.\nGenerally BEA series are\nnot convergent and approximate the discrete scheme only by truncation [99].\nWhen the series in Eq (3.8) diverges, truncating it up to any order n, how-\never, will result in a flow that will not be able to capture instabilities, even in\nthe quadratic case.\nSuch flows (including the IGR flow) will always predict\nthe loss function will decrease for a quadratic loss where a minimum exists, since:\n3.3. The principal flow\n42\ndE\ndt = ∇θET \u0010Pn\np=0\n−1\np+1hp(∇2\nθE)p∇θE\n\u0011\n= −Pn\np=0\n1\np+1hp PD−1\ni=0 (λp\ni )(∇θETui)2 which\nis never positive for any quadratic loss where a minimum exists (i.e. when λi ≥0, ∀i).\nThe above also entails that the flows always predict convergence around a local\nminimum, which is not the case for gradient descent which can diverge for large\nlearning rates. To further track instabilities we can use the BEA series to find the\nfollowing flow:\nDefinition 3.3.1 We define the principal flow (PF) as\n˙θ =\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\n(∇θETui)ui.\n(3.9)\nWe note that limλ→0\nlog(1−hλ)\nhλ\n= −1 and thus the PF is well defined when the Hessian\n∇2\nθE is not invertible. Unlike the NGF and the IGR flow, the modified vector field\nof the PF cannot be always written as the gradient of a loss function in R, and can\nbe complex valued.\nTheorem 3.3.2 The Taylor expansion in h at h = 0 of the PF vector field coincides\nwith the series coming from the BEA of gradient descent (Eq (3.8)).\nProof: Using the Taylor expansion at z = 0, Taylorz=0\nlog(1−z)\nz\n= P∞\np=0\n−1\np+1zp we\nobtain:\nTaylorh=0\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\n(∇θETui)ui =\nD−1\nX\ni=0\n ∞\nX\np=0\n−1\np + 1hpλp\ni\n!\n(∇θETui)ui.\n(3.10)\n□\nWe have used BEA to find the flow that when Taylor expanded at h = 0 leads to\nthe series in Eq (3.8). When the BEA series in Eq (3.8) converges, namely λ0 < 1/h,\nthe PF and the flow given by the BEA series are the same. When λ0 > 1/h, however,\nthe PF is complex and the BEA series diverges. While in this case any BEA truncated\nflow will not be able to track gradient descent closely, we show that for quadratic\nlosses the PF will track gradient descent exactly, and that it is a good model of\n3.3. The principal flow\n43\ngradient descent around fixed points. We show examples of the PF tracking gradient\ndescent exactly in the quadratic case in Figures 3.3b and 3.7.\nRemark 3.3.1 For quadratic losses of the form E = 1\n2θTAθ+bTθ, the PF captures\ngradient descent exactly. This case has been proven in Hairer et al. [99]. The\nsolution of the PF can also be computed exactly in terms of the eigenvalues of ∇2\nθE:\nθ(t) = PD−1\ni=0 e\nlog(1−hλi)\nh\ntθT\n0 uiui + t PD−1\ni=0\nlog(1−hλi)\nhλi\nbTui.\nRemark 3.3.2 In a small enough neighbourhood around a critical point (where\nhigher-order derivatives can be ignored) the PF can be used to describe gradient\ndescent dynamics closely. We show this also using a linearisation argument in\nSection A.1.4 in the Appendix.\nDefinition 3.3.2 The terms C(∇3\nθE) are called non-principal terms. The term\n1\n12∇θET(∇3\nθE)∇θE in Eq (3.4) is a non-principal term (we will call this term\nnon-principal third-order term).\nDefinition 3.3.3 We define the principal flow with third-order non- princi-\npal term as\n˙θ =\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\n(∇θETui)ui −h2\n12∇θET(∇3\nθE)∇θE\n|\n{z\n}\nthird-order non-principal term\n.\n(3.11)\nGeneral theoretical bounds on the error between continuous time flows and\ngradient descent are challenging to construct in the case of a general parametrised\nE(θ) as the error will be determined by the shape of E. We know the conditions\nwhich determine when certain flows follow gradient descent exactly. The NGF and\ngradient descent will follow the same trajectory in areas where ∇2\nθE∇θE = 0 (see\nTheorem 3.7.1) and thus E has a constant gradient in time, since d∇θE\ndt\n= ∇2\nθE∇θE.\nThe PF generalises the NGF, in that it follows the same trajectory as gradient\ndescent not only for trajectories where ∇2\nθE∇θE = 0, but also when E is quadratic.\nInformally, we can state that the closer we are to these exact conditions, the more\nlikely the flows are to capture the dynamics of gradient descent. Formally, bounds\non the error between GD and NGF can be provided by the Fundamental Theorem\n3.3. The principal flow\n44\nNGF\n˙θ = PD−1\ni=0\n−\n|{z}\nαNGF (hλi)=−1\n(∇θETui)ui\nIGR Flow\n˙θ = PD−1\ni=0\n−(1 + h\n2λi)\n|\n{z\n}\nαIGR(hλi)=−(1+ h\n2 λi)\n(∇θETui)ui\nPF\n˙θ = PD−1\ni=0\nlog(1 −hλi)\nhλi\n|\n{z\n}\nαP F (hλi)= log(1−hλi)\nhλi\n(∇θETui)ui\nTable 3.1: Understanding the differences between the flows discussed in terms of\nthe eigen-decomposition of the Hessian.\nAll flows have the form\n˙θ =\nPD−1\ni=0 α(hλi)(∇θET ui)ui with different α summarised here.\n(Theorem 10.6 in Wanner and Hairer [138]) which has recently been adapted to a\nneural network parametrisation by Elkabetz and Cohen [54]; this bound depends\non the magnitude of the smallest Hessian eigenvalue along the NGF trajectory. We\nhope that future work can expand the Fundamental Theorem such that error bounds\nbetween the PF and gradient descent can be constructed for deep neural networks.\nHere we take an empirical approach and show that although not exact outside the\nquadratic case the PF captures key features of the gradient descent dynamics in\nstable or unstable regions of training, around and outside critical points, for small\nexamples or large neural networks.\n3.3.1\nThe PF and the Hessian eigen-decomposition\nAll flows considered here have the form form ˙θ = PD−1\ni=0 α(hλi)(∇θETui)ui, where\nα is a function computing the corresponding coefficient; we will denote the one\nassociated with each flow as αNGF, αIGR and αPF respectively. For a side-by-side\ncomparison between the NGF, IGR flow, and the PF as functions of the Hessian\neigen-decomposition see Table 3.1. Since ∇θETui ≥0, the α function determines\nthe sign of a modified vector field in the direction ui. For brevity it will be useful to\ndefine the coefficient of ui in the vector field of the PF:\nDefinition 3.3.4 We call sci = log(1−hλi)\nhλi\n(∇θETui) = αPF(hλi)∇θETui the stabil-\nity coefficient for eigendirection i. sign(sci) = sign(αPF(hλi)).\nIn order to understand the PF and how it is different from the NGF we explore\n3.3. The principal flow\n45\n10\n8\n6\n4\n2\n0\n2\n4\n6\n8\n10\ni h\n5\n4\n3\n2\n1\n0\n1\nRe[ PF(h\ni)]\nRe[ NGF(h\ni)]\n0\n(a) Real part.\n10\n8\n6\n4\n2\n0\n2\n4\n6\n8\n10\ni h\n1\n0\n1\n2\n3\n4\nImag[ PF(h\ni)]\nImag[ NGF(h\ni)]\n(b) Imaginary part.\nFigure 3.5: Comparing the coefficients αNGF and αPF based on the value of αh. While\nαNGF is always −1, the value of αPF depends on αh and can be complex,\nboth with positive and negative real parts.\nthe change in each eigendirection ui and we perform case analysis on the relative\nvalue of the eigenvalues λi and the learning rate h. To do so, we will compare\nαNGF(hλi) and αPF(hλi) since the sign of αNGF(hλi) determines the direction that\nminimises E given by ui. Since our goal is to understand the behaviour of gradient\ndescent, we perform the case by case analysis of what happens at the start of a\ngradient descent iteration and thus use real values for λi and ui even when the PF is\ncomplex valued. We visualise αNGF and αPF in Figure 3.5 and we use Figure 3.6 to\nshow examples of each case using a simple function.\nReal stable case: λi < 1/h.\n• sign(αNGF(hλi)) = sign(αPF(hλi)) = −1.\n• αNGF(hλi) = −1 and αPF(hλi) = log(1−hλi)\nhλi\n< 0.\n• The coefficients of the NGF and PF in eigendirection ui are negative and real.\n• This case is visualised in Figure 3.6a.\nComplex stable case: 1/h < λi < 2/h.\n• sign(αNGF(hλi)) = sign(ℜ[αPF(hλi)]) = −1. αPF(hλi) ∈C.\n• αNGF(hλi) = −1 and αPF(hλi) =\nlog(1−hλi)\nhλi\n=\nlog(−1+hλi)+iπ\nhλi\n∈C and\nℜ[αPF(hλi)] = log(−1+hλi)\nhλi\n< 0.\n• The real part of the coefficient of the NGF and PF in eigendirection ui are both\nnegative. The imaginary part of αPF can introduce instability and oscillations.\n• This case is visualised in Figure 3.6b.\n3.3. The principal flow\n46\n0.2\n0.0\n0.2\nRe(z)\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nImag(z)\nsolution\nstarting point\ntrajectory\n(a)\nλ0 < 1/h,\nαP F (λ0h) < 0\n0.2\n0.0\n0.2\nRe(z)\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nImag(z)\n(b)\n1/h < λ0 < 2/h,\nℜ(αP F (λ0h)) < 0\n0.5\n0.0\n0.5\nRe(z)\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nImag(z)\n(c)\n2/h < λ0,\nℜ(αP F (λ0h)) > 0\nFigure 3.6: The behaviour of PF on the quadratic function E(z) =\n1\n2z2 with so-\nlution z(t) = elog(1−h)/hz(0).\nWhen λ0 < 1/h, z(t) = (1 −h)t/hz(0)\nwhich is in real space and converges to the equilibrium. When λ0 > 1/h,\nz(t) = (h −1)t/h (cos(πt/h) + i sin(πt/h)) z(0). This exhibits oscillatory be-\nhaviour, and when λ0 > 2/h, diverges.\nUnstable complex case: 2/h < λi.\n• sign(αNGF(hλi)) ̸= sign(ℜ[αPF(hλi)]). αPF(hλi) ∈C.\n• αNGF(hλi) = −1 and αPF(hλi) =\nlog(1−hλi)\nhλi\n=\nlog(−1+hλi)+iπ\nhλi\n∈C and\nℜ[αPF(hλi)] = log(−1+hλi)\nhλi\n> 0.\n• The real part of the coefficient of the NGF in eigendirection ui is negative,\nwhile the real part of the coefficient of the PF is positive. The PF goes in\nthe opposite direction of the NGF which minimises E; this change in sign can\ncause instabilities. The imaginary component can still introduce oscillations,\nhowever, the larger λih, the smaller the imaginary part of αPF.\n• This case is visualised in Figure 3.6c.\nThe importance of the largest eigenvalue λ0. The largest eigenvalue λ0\nplays an important part in the PF. Since hλ0 ≥hλi\n∀i, λ0 determines where in the\nabove cases the PF is situated and thus whether there are oscillations and unstable\nbehaviour in training. For all flows of the form we consider we can write:\ndE(θ)\ndt\n= ∇θET dθ\ndt = ∇θET\nD−1\nX\ni=0\nα(hλi)∇θETuiui =\nD−1\nX\ni=0\nα(hλi)(∇θETui)2 (3.12)\nand thus if α(hλi) ∈R and α(hλi) < 0 ∀i then dE(θ)\ndt\n≤0 and following the correspond-\n3.3. The principal flow\n47\ning flow minimises E. In the case of the PF this gets determined by λ0. If λ0 < 1\nh then\nαPF(hλi) < 0 ∀i (real stable case above) and the PF minimises E. If 1/h < λ0 < 2\nh\nthen ℜ[αPF(hλi)] < 0 ∀i (complex stable case above) close to a gradient descent\niteration λi, ui ∈R we can write that dℜ[E(θ)]\ndt\n= PD−1\ni=0 ℜ[αPF(hλi)](∇θETui)2 and\nthus the real part of the loss function decreases. If λ0 > 2\nh then ℜ[αPF(hλ0)] > 0\n(unstable complex case above) and if (∇θETu0)2 is sufficiently large we can no\nlonger ascertain the behaviour of E. We present a discrete-time argument for this\nobservation in Section A.2.1.\nBuilding intuition. For a quadratic objective E(θ) = 1\n2θTAθ the PF describes\ngradient descent exactly. We show examples Figures 3.3 and 3.7. Unlike the NGF or\nthe IGR flow, the PF captures the oscillatory and divergent behaviour of gradient\ndescent. Importantly, to capture the unstable behaviour that occurs when λ0 > 1/h\nthe imaginary part of the PF is needed. To expand intuition outside the quadratic\ncase, we show the PF for the banana function [139] in Figure 3.8 and an additional\nexample in 1D with a non-quadratic function in Figure A.6 in the Appendix. While\nin this case, the PF no longer follows the gradient descent trajectory exactly, we\nstill observe the importance of the PF in capturing instabilities of gradient descent.\nIn Figure 3.8c, we also observe that adding non-principal terms—described in\nDefinition 3.3.3—can restabilise the trajectory when λ0 >> 2/h.\nRemark 3.3.3 For the banana function, the principal terms have a destabilising\neffect when h > 2/λ0 while the non-principal terms can have a stabilising effect.\n3.3.2\nThe stability analysis of the PF\nWe now perform stability analysis on the PF, to understand how it can be used\nto predict certain behaviours of gradient descent around critical points of the loss\nfunction E; for an overview of stability analysis, see Section 2.3.1. Consider θ∗\nsuch a critical point, i.e. ∇θE(θ∗) = 0. For a critical point θ∗to be exponentially\nasymptotically attractive, all eigenvalues of the Jacobian evaluated at θ∗need to\nhave strictly negative real part.\nThe PF has the following Jacobian at critical points (proof in Section A.1.6 in\n3.3. The principal flow\n48\nPF\nIGR flow\ngradient descent\nglobal min\ninit point\n(a)\nλ0 < 1/h\n(stability)\n(b)\n1/h < λ0 < 2/h\n(oscillations)\n(c)\nλ0 > 2/h\n(divergence)\nFigure 3.7: Quadratic losses in 2 dimensions. The PF captures the behaviour of\ngradient descent exactly for quadratic losses, including oscillatory behaviour\n(b) and divergence (c).\ngradient descent\nPF\nIGR flow\ninit point\nglobal min\n(a) λ0 < 1/h\n(b) λ0 < 2/h(λ0 ≈1.9/h)\nPF with third order \nnon-principal term\n0.0\n0.1\n1.0\n10.0\n100.0\n1000.0\n(c) λ0 >> 2/h(λ0 ≈5/h)\nFigure 3.8: Banana function. The PF can capture instability and the gradient descent\ntrajectory over many iterations when λ0 is close to 2/h. When λ0 >> 2/h\n(c) the PF does not track the GD trajectory over many gradient descent\nsteps, but when including a non-principal term the flow is able to capture\nthe general trajectory of gradient descent and unstable behaviour of gradient\ndescent.\nthe Appendix):\nJPF(θ∗) =\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nh\nu∗\ni u∗\ni\nT,\n(3.13)\nwhere λ∗\ni , u∗\ni are the eigenvalues and eigenvectors of the Hessian ∇2\nθE(θ∗). We\nthus have that the eigenvalues of the Jacobian JPF(θ∗) at the critical point θ∗are\n1\nh log(1 −hλ∗\ni ) for i = 1, . . . , D.\nLocal minima.\nSuppose that θ∗is a local minimum.\nThen all Hessian\neigenvalues are non-negative λ∗\ni ≥0. We perform the stability analysis in cases given\n3.3. The principal flow\n49\nby the value of λ∗\ni , corresponding to the cases in Section 3.3.1:\n• h < 1/λ∗\ni . The corresponding eigenvalue of the Jacobian\n1\nh log(1 −hλ∗\ni ) is\nnegative, since 0 < 1 −hλ∗\ni < 1. The PF is attractive in the corresponding\neigenvector direction.\n• h ∈[1/λ∗\ni , 2/λ∗\ni ). The corresponding eigenvalue of the Jacobian 1\nh log(1−hλ∗\ni ) =\n1\nh log(hλ∗\ni −1) + i π\nh is complex, with negative real part since since hλ∗\ni −1 < 1.\nThe PF is attractive in the corresponding eigenvector direction.\n• h ≥2/λ∗\ni . The corresponding eigenvalue of the Jacobian 1\nh log(1 −hλ∗\ni ) =\n1\nh log(hλ∗\ni −1)+i π\nh is complex, with non-negative real part, since since hλ∗\ni −1 ≥\n1. If h > 2/λ∗\ni , the PF is repelled in the corresponding eigenvector direction.\nThe last case tells us that the PF is not always attracted to local minima, as it is\nrepelled in eigen-directions where h > 2/λ∗\ni . Thus like gradient descent, the PF\ncan be repelled around local minima for large learning rates. This is in\ncontrast to the NGF and the IGR flow, which always predict convergence around a\nstrict local minimum: the eigenvalues of the NGF Jacobian are −λ∗\ni , and for the IGR\nflow the eigenvalues are −λ∗\ni −h2\n2 λ∗\ni\n2, both are strictly negative when λ∗\ni is strictly\npositive. For derivations see Section A.1.7 in the Appendix.\nRemark 3.3.4 For quadratic losses, where the PF is exact, the results above recover\nthe classical gradient descent result for quadratic losses namely that gradient descent\nconvergences if λ0 < 2/h, otherwise diverges.\nSaddle points. Suppose that θ∗is a strict saddle point. In this case there\nexists λ∗\ns, such that λ∗\ns < 0. We want to analyse the behaviour of the PF in the\ndirection of the corresponding eigenvector u∗\ns. In that case, log(1 −hλ∗\ns) > 0 which\nentails that the PF is repelled in the eigendirections of strict saddle points. Note\nthat this is also the case for the NGF since the corresponding eigenvalues of the\nJacobian of the NGF would be −λ∗\ns, also positive. Unlike the NGF, however, the\nsubspace of eigendirections that the PF is repelled by can be larger since it includes\nalso eigendirections where λ∗\ni > 2/h > 0.\n3.4. Predicting neural network gradient descent dynamics with the PF\n50\n0\n2\n4\n6\n8\nIteration n\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n||\nn\n(nh)||\nNGF\nIGR flow\nThird order flow\nPF\n(a) λ0 < 1/h\n0\n2\n4\n6\n8\nIteration n\n0.000\n0.005\n0.010\n0.015\n0.020\n||\nn\n(nh)||\n(b) 1/h < λ0 < 2/h\n0\n2\n4\n6\n8\nIteration n\n0.0\n0.2\n0.4\n0.6\n0.8\n||\nn\n(nh)||\n(c) λ0 > 2/h\nFigure 3.9: Error between gradient descent parameters and parameters obtained following\ncontinuous-time flows for multiple iterations: ∥θn −θ(nh)∥with θ(0) = θ0.\nFor small n, the PF is better at capturing the behaviour of gradient descent\nacross all cases.\n3.4\nPredicting neural network gradient descent\ndynamics with the PF\nComputing the PF on large neural networks during training is computationally\nprohibitive, as it requires finding all eigenvalues of the Hessian matrix once for each\nstep of the flow simulation, corresponding to many eigen-decompositions per gradient\ndescent step. To build intuition about the PF for neural networks, we start with\na small MLP for a two-dimensional input regression problem, with random inputs\nand labels. Here, we can compute the PF’s vector field exactly and compare it\nwith the behaviour of gradient descent. We show results in Figure 3.9, where we\nvisualise the norm of the difference between gradient descent parameters at each\niteration and the parameters produced by the continuous-time flows we compare\nwith. We observe that short term the PF is better than all other flows at tracking the\nbehaviour of gradient descent. As the number of iterations increases, however, the\nPF accumulates error in the case of λ0 > 2/h; this is likely due to the fact that while\ngradient descent parameters are real, this is not the case for the PF, as discussed in\nRemark 3.4.1. Since we are primarily concerned with using the PF to understand\ngradient descent for a small number of iterations this will be less of a concern in\nour experimental settings. Additional results that confirm the PF is better than the\nother flows at tracking gradient descent on a bigger network trained the UCI breast\ncancer dataset [140] are shown in Figure A.7 in the Appendix.\n3.4. Predicting neural network gradient descent dynamics with the PF\n51\nRemark 3.4.1 Multiple iteration behaviour of the PF. While gradient descent\nparameters are real for any iteration θt, θt+1, ... , θt+n when we approximate the\nbehaviour of gradient descent by initialising θ(0) = θt and running the PF for time\nnh, there is nothing enforcing that θ(h), ... θ(nh) will be real when the PF is complex\nvalued (λ0 > 1/h). Outside the quadratic case the Hessian is not Hermitian, the\neigenvalues and eigenvectors of the Hessian will not be real, and the eigenvectors are\nno longer guaranteed to form a basis1. For long-term trajectories (larger n), this can\nhave an effect on the long-term error between gradient descent and PF trajectories,\nthrough an accumulating effect of the imaginary part in the PF. This can be mitigated\nby using the PF to understand the short-term behaviour of gradient descent (small\nn).\n3.4.1\nPredicting (∇θE)T u0 using the PF\nFor large neural networks, instead of simulating the PF describing how the entire\nparameter vector changes in time we can use the PF to approximate changes in a\nscalar quantity only. This will allow us to compare the predictions of the PF against\nthe predictions of the NGF and IGR flow on realistic settings, where the models\nhave millions of parameters. To do so, we first have to compute how the gradient\n∇θE changes in time:\nCorollary 3.2 If θ follows the PF, then:\n˙\n(∇θE) =\nD−1\nX\ni=0\nlog(1 −hλi)\nh\n(∇θETui)ui.\n(3.14)\nThis follows from applying the chain rule and using the definition of the PF. We now\ncontrast this result with how the gradient evolves if the parameters follow the NGF\nor the IGR flow.\n1To avoid the concern around the eigenvectors of the Hessian no longer forming a basis, one can\nuse the Jordan normal form instead, as we will do later, in Section 4.8, where we expand the PF\nto games. We don’t take this approach here as most of our following analysis is not affected, and\nis concerned with the behaviour of the PF around one gradient descent iteration. Furthermore,\nsupport of the Jordan normal form in code libraries is limited (especially for complex matrices),\nand we did not find this to be a significant issue in the experiments where we simulate the PF\noutside the quadratic case for a few iterations. We note, however, that mathematical analysis of\nlong-term PF trajectories for general functions should use the Jordan normal form.\n3.4. Predicting neural network gradient descent dynamics with the PF\n52\nCorollary 3.3 If θ follows the NGF, then:\n˙\n(∇θE) =\nD−1\nX\ni=0\n−λi(∇θETui)ui.\n(3.15)\nCorollary 3.4 If θ follows the IGR flow, then:\n˙\n(∇θE) =\nD−1\nX\ni=0\n−\n\u0012\nλi + h\n2λ2\ni\n\u0013\n(∇θETui)ui.\n(3.16)\nWe would like to use the above to assess how ∇θETui changes in time under\nthe above flows and check their predictions empirically against results obtained when\ntraining neural networks with gradient descent. Since ui is an eigenvector of the\nHessian it also changes in time according to the changes given by the corresponding\nflow, making\n˙\n(∇θETui) =\nd(∇θET ui)\ndt\ndifficult to calculate. Even when if we wrote an\nexact flow for\nd(∇θET ui)\ndt\n, it would be computationally challenging to simulate it since\nfinding the new values of ui would depend on the full Hessian and would lead to the\nsame computational issues we are trying to avoid in the case of large neural networks.\nIn order to mitigate these concerns, we will make the additional approximation that\nλi and ui do not change inside an iteration, which will allow us to approximate\nchanges to ∇θETui and compare them against empirical observations. We note that\nwe will not use this approximation for any other results.\nRemark 3.4.2 If we assume that λi, ui do not change between iterations, if θ\nfollows the PF then\nd(∇θET ui)\ndt\n= log(1−hλi)\nh\n∇θETui.\nRemark 3.4.3 If we assume that λi, ui do not change between iterations, if θ\nfollows the NGF we can write\nd(∇θET ui)\ndt\n= −λi∇θETui.\nRemark 3.4.4 If we assume that λi, ui do not change between iterations, if θ\nfollows the IGR flow we can write\nd(∇θET ui)\ndt\n= −\n\u0000λi + h\n2λ2\ni\n\u0001\n∇θETui.\nThe above flows have the form ˙x = cx, with solution x(t) = x(0)ect. We can thus\ntest these solutions empirically by training neural networks with gradient descent\nwith learning rate h and at each step compute ∇θE(θt)T(ui)t−1 and compare it\n3.4. Predicting neural network gradient descent dynamics with the PF\n53\n4\n2\n0\n2\n4\nETu0\n4\n3\n2\n1\n0\n1\n2\n3\n4\nETu0 prediction\nLearning rate 0.01\nperfect (oracle)\nPF\nNGF\nIGR flow\n(a) h = 0.01\n4\n2\n0\n2\n4\nETu0\n4\n3\n2\n1\n0\n1\n2\n3\n4\nETu0 prediction\nLearning rate 0.03\n(b) h = 0.03\n4\n2\n0\n2\n4\nETu0\n4\n3\n2\n1\n0\n1\n2\n3\n4\nETu0 prediction\nLearning rate 0.05\n(c) h = 0.05\nFigure 3.10: Predictions of ∇θET u0 according to the NGF, IGR flow and the PF on\nfull-batch training of a VGG model on CIFAR-10. On the x axis we plot\nthe value of ∇θET u0 as measured empirically in training, and on the y\naxis we plot the corresponding prediction according to the flows from the\nvalue of the dot product at the previous iteration. The ‘exact match’ line\nindicates a perfect prediction, the upper bound of performance. The PF\nperforms best from all the compared flows, however, for higher learning\nrates its performance degrades when ∇θET u0 is large; this is due to the\nfact that the higher the learning rate and the higher the gradient norm, the\nmore likely it is that the additional assumption we used that λi, ui do not\nchange does not hold.\nwith the prediction x(h) obtained from the solution from each flow initialised at\nthe previous iteration, i.e. x(0) = ∇θE(θt−1)T(ui)t−1. We show results with a\nVGG model trained on CIFAR-10 in Figure 3.10. The results show that the PF\nis substantially better than the NGF and IGR flow at predicting the behaviour of\n∇θETu0. Since the NGF and the IGR flow solutions scale the initial value by the\ninverse of an exponential of magnitude given by λ0, for large λ0 this leads to a small\nprediction, which is not aligned with what is observed empirically. We also note that\nthe higher the value of ∇θETu0, the worse the prediction of the PF; these are the\nareas where the approximations made in the above remarks are likely not to hold\ndue to large gradient norms.\n3.4.2\nAround critical points: escaping sharp local minima\nand saddles\nThe stability analysis we performed in Section 3.3.2 showed the PF is repelled by local\nminima where λ∗\n0 > 2/h: that is, even if the model is close to a sharp local minima\n3.4. Predicting neural network gradient descent dynamics with the PF\n54\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nIteration\n1e4\n10\n20\n30\n40\n0\nLr change\nVGG CIFAR-10 Full batch (10K subset)\nLearning rate\n0.1\n0.13\n(a) λ0.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nIteration\n1e4\n10\n3\n10\n1\nLoss\nLr change\nVGG CIFAR-10 Full batch (10K subset)\nLearning rate\n0.1\n0.13\n(b) E(θ).\nFigure 3.11: Local minima are not attractive to gradient descent if λ∗\n0 > 2/h. Since we\ncannot easily find local minima in neural network landscapes, we perform\nan experiment where we train a neural network with learning rate h′ < h\nclose to convergence, and then change the learning rate to h, close to but\nabove 2/λ∗\n0. Increasing the learning rate slightly leads to instabilities and\nexiting the stable training area, despite being in a late stage of training.\nWe note that while this is not the same as assessing local attraction in\na stability sense, since we do not know if the point of the learning rate\nchange is in the neighbourhood provided by the existence conditions of local\nstability, it is the closest practical approximation that we can provide given\nthe complexity of neural network landscapes.\n(with λ∗\n0 > 2/h), that local minima will not be attractive and training will continue\nuntil a shallow minima is reached. We provide experimental evidence to support\nthat hypothesis in neural network training in Figure 3.11. We train a neural network\nclose to convergence, and then increase the learning rate slightly to a learning rate\nthat exceeds the sharpness threshold h > 2/λ∗\n0, and observe that the model promptly\nexits the area, despite being in a late stage of training. These results are consistent\nwith observations in the deep learning literature [52, 141]. Furthermore, while saddle\npoints have long been considered a challenge with high dimensional optimisation [103]\nin practice gradient descent has not been observed to converge to saddles [142]. Our\nanalysis suggests that saddles will be repelled not only in the direction of strictly\nnegative eigenvalues, but also in the eigendirections with large positive eigenvalues\nwhen large learning rates are used; this can explain why neural networks do not\nconverge to non-strict saddles which exist in deep neural landscapes [143] but need\nnot be repelling for the NGF and IGR flow (existing analyses of escaping saddle\npoints by gradient descent apply only to strict saddles [51, 142]).\n3.5. The PF, stability coefficients and edge of stability results\n55\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nIteration\n1e3\n0\n1\n2\n3\n4\nTraining loss\nVGG CIFAR-10\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.5\n(a) Loss E(θ).\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nIteration\n1e3\n0\n100\n200\n300\n400\n500\n0\nVGG CIFAR-10\n(b) λ0.\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nLearning rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion\n0 at loss increase\n0 > 2/h\n0 < 2/h\n(c) Increases in loss function over-\nwhelmingly occur when λ0 >\n2/h.\nFigure 3.12: Edge of stability in neural networks. Cohen et al. [52] observed that when\ntraining neural networks with full-batch gradient descent, training is stable\nand λ0 increases until λ0 > 2/h, when local loss instabilities occur and λ0\nstarts oscillating around 2/h.\n3.5\nThe PF, stability coefficients and edge of\nstability results\nEdge of stability results. Cohen et al. [52] did a thorough empirical study to\nshow that when training deep neural networks with full-batch gradient descent the\nlargest eigenvalue of the Hessian, λ0, keeps growing until reaching approximately 2/h\n(a phase of training they call progressive sharpening), after which it remains in that\narea; for mean-squared losses this continues indefinitely while for cross entropy losses\nthey show it further decreases later in training. They also show that instabilities\nin training occur when λ0 > 2/h. Their empirical study spans neural architectures,\ndata modalities, and loss functions. We visualise the edge of stability behaviour they\nobserve in Figure 3.12; since we use a cross entropy loss λ0 decreases later in training.\nWe also visualise that iterations where the loss increases compared to the previous\niteration overwhelmingly occur when λ0 > 2/h. Cohen et al. [52] also empirically\nobserve that θTu0 has oscillatory behaviour in the edge of stability area but is 0 or\nsmall outside it.\nContinuous-time models of gradient descent at edge of stability. To\ninvestigate if existing continuous-time flows and the PF capture gradient descent\nbehaviour at the edge of stability we train a 5 layer MLP on the toy UCI Iris\n3.5. The PF, stability coefficients and edge of stability results\n56\n0.0225\n0.0250\n0.0275\n0.0300\n0.0325\n0.0350\nGradient descent E(\nn)\n1.90\n1.95\n2.00\n2.05\n2.10\n2.15\n2.20\nGradient descent \n0\n200 205 210 215 220 225 230 235 240 245\nIteration n\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\n0.0025\n0.0030\n|| (h;\nn\n1)\nn||\nNGF\nIGR flow\nPF\n(a) Changes in parameters.\n0.0225\n0.0250\n0.0275\n0.0300\n0.0325\n0.0350\nGradient descent E(\nn)\n1.90\n1.95\n2.00\n2.05\n2.10\n2.15\n2.20\nGradient descent \n0\n200 205 210 215 220 225 230 235 240 245\nIteration n\n0.0125\n0.0100\n0.0075\n0.0050\n0.0025\n0.0000\nE( (h;\nn\n1))\nE(\nn)\n(b) Changes in loss.\nFigure 3.13: Comparing different continuous-time models of gradient descent at the edge\nof stability area on a small 5 layer MLP, with 10 units per layer. We show\nthe local parameter prediction error ∥θn −θ(h; θn−1)∥for the NGF, IGR\nand PF flows (a), as well as the loss prediction error E(θ(h; θn−1)) −E(θn)\n(b); when plotting the loss, for the PF we take the real part of the loss\nvalue.\ndataset [140]; this simple setting allows for the computation of the full eigenspectrum\nof the Hessian. We show results in Figure 3.13: the NGF and IGR flow have a\nlarger error compared to the PF when predicting the parameters at the next gradient\ndescent iteration in the edge of stability regime; the NGF and IGR flow predict the\nloss will decrease, while the PF captures the loss increase observed when following\ngradient descent. As we remarked in Section 3.2, the NGF and the IGR flow do not\ncapture instabilities when the eigenvalues of the Hessian are positive, which has been\nremarked to be largely the case for neural network training through empirical studies\n[144–146] and we observe here (Figure A.8 in the Appendix). We spend the rest of\nthe section using the PF to understand and model edge of stability phenomena using\na continuous-time approach.\n3.5. The PF, stability coefficients and edge of stability results\n57\nConnection with the principal flow: stability coefficients. The PF\ncaptures the key quantities observed in the edge of stability phenomenon: the\neigenvalues of the Hessian λi and the threshold 2/h. These quantities appear in\nthe PF via the stability coefficient sci = log(1−hλi)\nhλi\n∇θETui = αPF(λih)∇θETui of\neigendirection ui. Through the PF, by connecting the case analysis in Section 3.3.1\nwith existing and new empirical observations, we can shed light on the edge of\nstability behaviour in deep learning.\nFirst phase of training (progressive sharpening): λ0 < 2/h.\nThis entails\nℜ[sci] = ℜ[αPF(hλi)] ≤0, ∀i (Real stable and complex stable cases of the anal-\nysis in Section 3.3.1). sign(αNGF) = sign(αPF) = −1 and following the PF minimises\nE or its real part (Eq (3.12)). To understand the behaviour of λ0, we now have to\nmake use of empirical observations about the behaviour of the NGF early in the\ntraining of neural networks. It has been empirically observed that in early areas\nof training, λ0 increases here when following the NGF [52]; we further show this in\nFigure A.25 in the Appendix. Since in this part of training gradient descent follows\nclosely the NGF, it exhibits similar behaviour and λ0 increases. We show this case\nin Figure 3.14a.\nSecond phase of training (edge of stability) λ0 ≥2/h. This entails ℜ[sc0(θ)] =\nℜ[αPF(hλi)] ≥0.\n(Unstable complex case of the analysis in Section 3.3.1).\nWe can no longer say that following the PF minimises E.\nsign(αNGF(hλ0)) ̸=\nsign(ℜ[(αPF(hλ0)]), since αNGF(hλ0) = −1 and sign(ℜ[(αPF(hλ0)]) > 0 mean-\ning that in that direction gradient descent resembles the positive gradient flow\n˙θ = ∇θE rather than the NGF. The positive gradient flow component can cause\ninstabilities, and the strength of the instabilities depends on the stability coefficient\nsc0 = αPF(hλ0)∇θETu0. We show in Figures 3.14b and 3.16 how the behaviour\nof the loss and λ0 are affected by the behaviour of the positive gradient flow when\nλ0 > 2/h.\nMore than λ0: the importance of stability coefficients. While the sign\nof the real part of the stability coefficient sc0 is determined by λ0, its magnitude\nis modulated by the dot product ∇θETu0, since sc0 = αPF(hλ0)∇θETu0. The\n3.5. The PF, stability coefficients and edge of stability results\n58\n25\n30\n35\n40\n45\n0\nE\nE\nGD\n2/h\n1.0\n1.5\n2.0\nLoss\n0\n5\n10\n15\n20\n25\nIteration\n0.3\n0.2\n0.1\n0.0\nRe[sc0]\n(a) Early training.\n50\n100\n150\n200\n0\n0\n1\n2\n3\n4\n5\nLoss\n60\n65\n70\n75\nIteration\n0.00\n0.05\n0.10\nRe[sc0]\n(b) Edge of stability.\nFigure 3.14: Understanding the edge of stability results using the PF on a 4 layer MLP:\nwe plot the behaviour of the NGF ˙θ = −∇θE and the positive gradient flow\n˙θ = ∇θE initialised at each gradient descent iteration parameters, and see\nthat the behaviour of gradient descent is connected to the behaviour of the\nrespective flow through the stability coefficient sc0(θ) = αPF (hλ0)∇θET u0.\n(a) shows that even when λ0 > 2/h, if the real part of the stability coefficient\nsc0 is negative or close to 0, there are no instabilities in the loss and the\neigenvalue λ0 keeps increasing, as it does when following the NGF in that\nregion.\nmagnitude of ∇θETu0 plays an important role, since if λ0 is the only eigenvalue\ngreater than 2/h training is stable if ∇θETu0 = 0, as we observe in Figure 3.14. To\nunderstand instabilities, we have to look at stability coefficients, not only eigenvalues.\nWe show in Figure 3.15 how the instabilities in training can be related with the\nstability coefficient sc0: the increases in loss occur when the corresponding ℜ[sc0]\nis positive and large. In Figure 3.16, we show results with the behaviour of λ0: λ0\nincreases or decreases based on the behaviour of the corresponding flow and the\nstrength of the stability coefficient and that gets reflected in instabilities in the\nloss function; specifically when λ0 > 2/h, we use the positive gradient flow and see\nhow the strength of its fluctuations affect the changes both in the loss value and\nλ0 of gradient descent. We show additional results in Figures A.11 and A.12 in the\nAppendix.\nIs one eigendirection enough to cause instability? One question that\n3.5. The PF, stability coefficients and edge of stability results\n59\n0\n1\n2\nLoss\nloss increasing\n0\n50\n100\n150\n200\n250\nIteration\n0\n1\nRe[sc0]\nMNIST 4 layers MLP\n(a) MNIST.\n1.0\n1.2\nLoss\nloss increasing\n200\n250\n300\n350\n400\nIteration\n0.0\n0.2\n0.4\nRe[sc0]\nCIFAR-10 VGG\n(b) CIFAR-10.\nFigure 3.15: The value of the loss function and stability coefficients: areas where the\nloss increases corresponds to areas where sc0 is large. The highlighted areas\ncorrespond to regions where the loss increases.\n100\n200\n0\nE\nE\nGD\n2/h\n0\n1\n2\nLoss\n100\n200\n300\n400\n500\nIteration\n0.5\n0.0\nRe[sc0]\nCIFAR-10, Resnet-18\nFigure 3.16: Loss instabilities, λ0 and stability coefficients for CIFAR-10. Together with\nthe behaviour of gradient descent, we plot the behaviour of the NGF and\npositive gradient flow initialised at θt and simulated for time h for each\niteration t. The analysis we performed based on the PF suggests that\nwhen ℜ[sc0] > 0 and large we should expected gradient descent to exhibit\nbehaviours close to those of the positive gradient flow. What we observe\nempirically is that increases in loss value of gradient descent are proportional\nto the increase of the positive gradient flow in that area (can be seen best\nbetween iterations 200 and 350); the same behaviour can be seen in relation\nto the eigenvalue λ0.\narises from the PF is whether the leading eigendirection u0 can be sufficient to\ncause instabilities, especially in the context of deep networks with millions of pa-\nrameters. To assess this we train a model with gradient descent until it reaches\n3.5. The PF, stability coefficients and edge of stability results\n60\n0\n2\n4\n6\n8\n10\n12\n14\nPhysical Time\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTraining loss\nVGG CIFAR-10 Full batch\nGD (LR 0.05)\nNGF\nNGF with \nETu0 sign swap\n(a) Loss E(θ).\n0\n2\n4\n6\n8\n10\n12\n14\nPhysical Time\n102\n103\n104\n0\nVGG CIFAR-10 Full batch\nGD (LR 0.05)\nNGF\nNGF with \nETu0 sign swap\n(b) λ0.\nFigure 3.17: One eigendirection is sufficient to lead to instabilities. To create a situa-\ntion similar to that of the PF in neural network training, we construct a\nflow given by the NGF in all eigendirections but u0; in the direction of\nu0, we change the sign of the flow’s vector field. This leads to the flow\n˙θ =\n\u0000∇θET u0\n\u0001\nu0 + PD−1\ni=1 −\n\u0000∇θET ui\n\u0001\nui. We show this flow can be very\nunstable when initialised in an edge of stability area, with increases in loss\n(a) and λ0 (b).\nthe edge of stability (λ0 ≈2/h), after which we simulate the continuous flow\n˙θ =\n\u0000∇θETu0\n\u0001\nu0 + PD−1\ni=1 −\n\u0000∇θETui\n\u0001\nui. The coefficients of the modified vector\nfield of this flow are negative for all eigendirections except from u0, which is positive;\nthis is also the case for the PF when λ0 is the only eigenvalue greater than 2/h. In\nFigure 3.17 we empirically show that a positive coefficient for u0 can be responsible\nfor an increase in loss value and a significant change in λ0 in neural network training.\nDecreasing the learning rate. Cohen et al. [52] show that if the edge of\nstability behaviour is reached and the learning rate is decreased, the training stabilises\nand λ0 keeps increasing; we visualise this phenomenon in Figure 3.18. The PF tells\nus that decreasing the learning rate entails going from ℜ[sc0] ≥0 to ℜ[sc0] ≤0\nsince λ0 < 2/h after the learning rate change. Since all stability coefficients are now\nnegative, this reduces instability. The increase in λ0 is likely due to the behaviour of\nthe NGF in that area (as can be seen in Figure 3.17 when changing from gradient\ndescent training to the NGF in an edge of stability area leads to an increase of λ0).\nThe behaviour of ∇θETu0.\nThe PF also allows us to explain the\nunstable behaviour of ∇θETu0 around edge of stability areas.\nAs done in\nSection 3.4.1, we assume that λi, ui do not change substantially between it-\nerations and write\nd(∇θET ui)\ndt\n= log(1−hλi)\nh\n∇θETui under the PF, with solution\n3.5. The PF, stability coefficients and edge of stability results\n61\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n0\n1\n2\nTraining loss\nVGG CIFAR-10 Full batch\nLearning rate\n0.01\n0.05\n(a) Loss E(θ).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n50\n100\n150\n200\n0\nVGG CIFAR-10 Full batch\nLearning rate\n0.01\n0.05\n(b) λ0.\nFigure 3.18: Decreasing the learning rate in the edge of stability area leads to increased\nstability, as discovered by Cohen et al. [52]. The lack of loss instability with\na smaller learning rate (a) is explained by the PF now being in a situation\nwhere all stability coefficients are negative, and the increase in eigenvalue\nλ0 (b) is consistent with what occurs when the PF more closely follows the\nNGF, since the NGF tends to increase eigenvalues in that part of training,\nas observed in Figure 3.17.\n(∇θETui)(t) = (∇θETui)(0)e\nlog(1−hλi)\nh\nt.\nThis solution has different behaviour de-\npending on the value of λ0 relative to 2/h: decreasing below 2/h and increasing\nabove 2/h. We show this theoretically predicted behaviour in Figure 3.19, alongside\nempirical behaviour showcasing the fluctuation of ∇θETu0 in the edge of stability\narea, which confirms the theoretical prediction. We also compute the prediction error\nof the proposed flow and show it can capture the dynamics of ∇θETu0 closely in this\nsetting. We present a discrete-time argument for this observation in Section A.2.2.\nWe note that the stable behaviour early in training together with the oscillatory\nbehaviour of ∇θETu0 in the edge of stability area, which we predict and observe can\nexplain the results of Cohen et al. [52] on the behaviour of θTu0, since θ accumulates\nchanges given by gradient updates.\nWhy not more instability? To determine why there isn’t more instability in\nthe edge of stability area we have to consider that neural networks are not quadratic,\nwhich has two effects. Firstly, when following the PF the landscape changes slightly\nlocally; this leads to changes in stability coefficients and thus the behaviour of gradient\ndescent as we have consistently seen in the experiments in this section. Secondly,\nnon-principal terms can have an effect; while we do not know all non-principal terms\nin Section 3.6 we provide a justification for why the non-principal term we do know\n3.5. The PF, stability coefficients and edge of stability results\n62\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nRe[\nETu]\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nImag[\nETu]\n> 2/h\ninit\n< 2/h\n(a) ∇θET u predicted behaviour\nunder the PF based on hλ0.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nIteration\n1e3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nETu0\nLearning rate 0.01\n0 > 2/h\n(b) ∇θET u0 observed behaviour\nin gradient descent training.\n0.02\n0.04\n0.06\n0.08\n0.10\nLearning rate\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\nMean integration error (PF)\n1.8h <\n0 < 2.2h\n(c) Integration error in edge of sta-\nbility area.\nFigure 3.19: Predicting the unstable dynamics of ∇θET u in the edge of stability area\n(λ ≈2/h) using the PF. (a): the predicted behaviour of ∇θET u under\nd(∇θET ui)\ndt\n= log(1−hλi)\nh\n∇θET ui, with an inflection point at λ = 2/h. (b):\nempirical behaviour of ∇θET u for a model shows instabilities in the edge of\nstability area (highlighted). (c): the approximation made to derive the flow\nis suitable around λ ≈2/h. (b) and (c) plots are obtained from training\nVGG models on CIFAR-10 with full-batch training.\n(Eq (3.11)) can have a stabilising effect by inducing a regularisation pressure to\nminimise λi(∇θETui)2 in certain parts of the training landscape.\nIn this section we have shown the PF closely predicts the behaviour of gradient\ndescent in neural network training. This has led to additional insights, including the\nimportance of stability coefficients in determining instabilities in gradient descent\n(Figures 3.14, 3.15, 3.16), causally showing one eigendirection is sufficient to cause\ninstability (Figure 3.17) and being able to closely predict the behaviour of the dot\nproduct between the gradient and the largest eigenvector (Figure 3.19). This evidence\nsuggests that the PF captures significant aspects of the behaviour of gradient descent\nin deep learning; this is likely due to the specific structure of neural network models.\nWhile we take a continuous-time approach, a discrete-time approach can be used\nto motivate some of our observations (Section A.2); this is complementary to our\napproach but nonetheless related, since it also does not account for higher order\nderivatives of the loss and further suggests the strength of a quadratic approximation\nof the loss in the case of neural networks, as observed by Cohen et al. [52].\n3.6. Non-principal terms can stabilise training\n63\n3.6\nNon-principal terms can stabilise training\nThis chapter focuses on understanding the effects of the PF on the behaviour of\ngradient descent. The principal terms, however, are not the only terms in the dis-\ncretisation drift: we have found one non-principal term of the form ∇θET(∇3\nθE)∇θE\n(Eq (3.4)) and have seen that it can have a stabilising effect (Figure 3.8). To provide\nsome intuition, while ∇θET(∇3\nθE)∇θE cannot be written as a gradient operator we\ncan write:\n∇θET∇3\nθE∇θE =\nD−1\nX\ni=0\n\u0000uT\ni ∇3\nθEui\n\u0001\n(∇θETui)2 ≈\nD−1\nX\ni=0\n∇θλi(∇θETui)2,\n(3.17)\nwhere we used that ui does not change substantially around a GD iteration to write\nthe derivative of λi, namely ∇θλi = uT\ni ∇3\nθEui, since ∇2\nθE is real and symmetric\naround a gradient descent iteration [147]. We can now write, again assuming ui is\nlocally constant:\nD−1\nX\ni=0\n∇θλi(∇θETui)2 =\nD−1\nX\ni=0\n∇θ\n\u0000λi(∇θETui)2\u0001\n−\nD−1\nX\ni=0\n2λ2\ni (∇θETui)ui.\n(3.18)\nFrom here we can see that if λi or ∇θETui are close to 0, the non-principal term leads\nto a pressure to minimise λi(∇θETui)2, since the non-gradient term λ2\ni (∇θETui)ui\nin Eq (3.18) diminishes. This creates an incentive to keep the value of λi(∇θETui)2\naround 0. We note that the work of Damian et al. [148] is what inspired us to write\nthe third-order non-principal term in the form of Eq (3.18), after we had previously\nnoted its stabilising properties.\nThe incentive to keep λi(∇θETui)2 close to 0 can have a stabilising effect, since\nλi = 0 or ∇θETui = 0 results in αPF(hλi) = αNGF(hλi) = −1. Thus, minimising\nλi(∇θETui)2 can reduce instability from the PF, since in those directions the PF has\nthe same behaviour as the NGF. We think this can shed light on observations about\nneural network training behaviour: the NGF tends to increase the eigenvalues, but\nas they reach close to 0 there is a pressure to minimise the above, which leads to λi\nstaying around 0; this is consistent with observations in the literature [144–146]. This\n3.7. Stabilising training by accounting for discretisation drift\n64\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\n100\n102\nNon principal term norm \n(with coefficient)\nVGG CIFAR-10\n(a) h2\n12∇θET ∇3\nθE∇θE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\n100\n102\nGradient norm\nVGG CIFAR-10\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.5\n(b) ∥∇θE∥\nFigure 3.20: The value of the non-principal third-order term in training: outside the\nedge of stability areas (best seen for learning rates 0.005 and 0.01), the non-\nprincipal third-order term is very small compared to the gradient. Inside\nthe edge of stability areas, the non-principal term has a higher magnitude.\nCorresponding loss and λ0 plots are provided in Figure 3.12.\nobservation also explains the pressure for the dot product ∇θETu0 to stay small if it\nis initialised around 0, as we see is the case in the early in neural network training\n(see Figures 3.14a and 3.15). Furthermore, as we show in Figure 3.19 and previously\nargued, the dot product ∇θETu0 fluctuates around 0 in the edge of stability areas,\nmaking it likely for the minimisation effect induced by Eq (3.18) to kick in.\nWe plot the value of this non-principal term in neural network training in\nFigure 3.20. These results show that the non-principal third-order term is very\nsmall outside the edge of stability area, but has a larger magnitude around the\nedge of stability, where the largest eigenvalues stop increasing but the magnitude\nof (∇θE(θ0)Tui)2 fluctuates and can be large (Figure 3.19). This observation is\nconsistent with the above interpretation, but more theoretical and empirical work\nis needed to understand the effects of non-principal terms on training stability and\ngeneralisation.\n3.7\nStabilising training by accounting for\ndiscretisation drift\nThe PF allows us to understand not only how gradient descent differs from the trajec-\ntory given by the NGF, but also when they follow each other closely. Understanding\nwhen gradient descent behaves like the NGF flow reveals when the existing analyses\n3.7. Stabilising training by accounting for discretisation drift\n65\nof gradient descent using the NGF discussed in Section 3.2 are valid. It also has\npractical implications, since, in areas where gradient descent follows the NGF closely,\ntraining can be sped up by increasing the learning rate. Prior works have empirically\nobserved that gradient descent follows the NGF early in neural network training [52],\nand this observation can be used to explain why decaying learning rates [149] or\nlearning rate warm up [150] are successful when training neural networks: having a\nhigh learning rate in areas where the drift is small will not cause instabilities and\ncan speed up training while decaying the learning rate avoids instabilities later in\ntraining when the drift is larger.\n3.7.1\n∇2\nθE∇θE determines discretisation drift\nIn previous sections we have seen that the Hessian plays an important role in defining\nthe PF and in training instabilities. We now want to quantify the difference between\nthe NGF and the PF in order to understand when the NGF can be used as a model\nof gradient descent. We find that:\nRemark 3.7.1 In a region of the space where ∇2\nθE∇θE = 0 the PF is the same as\nthe NGF.\nTo see why, we can expand\n∇2\nθE∇θE =\nD−1\nX\ni=0\nλi∇θETuiui.\n(3.19)\nIf ∇2\nθE∇θE = 0 we have that λj∇θETuj = 0, ∀j ∈{1, . . . , D}, thus either\nλj = 0 leading to αNGF(hλj) = αPF(hλj) = −1 or ∇θETuj = 0.\nThen\n˙θ = PD−1\ni=0 αPF(hλi)(∇θETui)ui = PD−1\ni=0 αNGF(hλi)(∇θETui)ui.\nThus comparing the PF with the NGF reveals an important quantity: ∇2\nθE∇θE.\nFurther investigating this quantity reveals it has a connection with the total drift,\nsince:\nTheorem 3.7.1 The discretisation drift (error between gradient descent and the\nNGF) after one iteration θt = θt −h∇θE(θt−1) is h2\n2 ∇2\nθE(θ′)∇θE(θ′) for a set of\nparameters θ′ in the neighborhood of θt−1.\n3.7. Stabilising training by accounting for discretisation drift\n66\n0\n200\n400\n600\n800\n1000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMLP Full batch MNIST\ndrift\n||h2\n2\n2E\nE||\n(a) MNIST MLP.\n0\n500\n1000\n1500\n2000\nIteration\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nVGG Full batch CIFAR-10\n(b) CIFAR-10 VGG.\n0\n500\n1000\n1500\n2000\nIteration\n0.00\n0.05\n0.10\n0.15\nVGG Batch size 1024 CIFAR-10\n(c) CIFAR-10 VGG,\nBatch Size 1024.\nFigure 3.21: ||∇2\nθE∇θE|| and the per-iteration drift have similar behaviour during train-\ning. As suggested by Theorem 3.7.1, the norm of the per-iteration drift\n∥θ(h; θn−1) −θn∥can be approximated by ||∇2\nθE(θn−1)∇θE(θn−1)||. The\nerror in the approximation is due to the theorem providing an existence\nargument for a set of parameters θ′ in the neighbourhood of θn−1 such that\n||∇2\nθE(θ′)∇θE(θ′)|| is equal to the per-iteration drift; since we do not know\nthose parameters we use the iteration parameters θn−1 instead.\nThis follows from the Taylor reminder theorem in mean value form (proof in\nSection A.1.9). This leads to:\nCorollary 3.5 In a region of space where ∇2\nθE∇θE = 0 gradient descent follows\nthe NGF.\nThus the PF revealed ∇2\nθE∇θE as a core quantity in the discretisation drift of\ngradient descent. To further see the connection between with the PF consider that\n∥∇2\nθE∇θE∥2 =\n\r\r\rPD−1\ni=0 λi∇θETuiui\n\r\r\r\n2\n= PD−1\ni=0\n\r\rλi∇θETui\n\r\r2; the higher each term\nin the sum, the higher the difference between the NGF and the PF. To measure the\nconnection between per-iteration drift and ∥∇2\nθE∇θE∥in neural network training we\napproximate it via\n\r\r\rθt −^\nNGF(θt−1, h)\n\r\r\r where ^\nNGF is the numerical approximation\nto the NGF initialised at θt−1. Results in Figures 3.21 and 3.22 show the strong\ncorrelation between per-iteration drift and ∥∇2\nθE∇θE∥throughout training and\nacross learning rates. Since Theorem 3.7.1 tells us the form of the drift but not the\nexact value of θ′, we have used θt−1 instead to evaluate ∥∇2\nθE∇θE∥and thus some\nerror exists.\nUnderstanding this connection is advantageous since computing discretisation\ndrift is computationally expensive as it requires simulating the continuous-time NGF\n3.7. Stabilising training by accounting for discretisation drift\n67\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nLearning rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpearman Correlation\nMLP Full batch MNIST\nCC(drift, ||\n2E\nE|||)\nCC(drift, ||\nE||)\nCC(drift,  ||\n2E\nE|||/||\nE||\n(a) MNIST MLP.\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nLearning rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpearman Correlation\nVGG Full batch CIFAR-10\n(b) CIFAR-10 VGG.\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nLearning rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpearman Correlation\nVGG Batch size 1024 CIFAR-10\n(c) CIFAR-10 VGG,\nBatch Size 1024.\nFigure 3.22: Correlation between ||∇2\nθE∇θE|| and the per-iteration drift.\nSince\n||∇2\nθE∇θE|| =\n\u0000||∇2\nθE∇θE||\n\u0001\n/||∇θE||||∇θE||, we measure the correlation\nbetween per-iteration drift and\n\u0000||∇2\nθE∇θE||\n\u0001\n/||∇θE|| as well as ||∇θE||.\nAs Theorem 3.7.1 provides an existence argument for an unknown set of\nparameters θ′ in the neighbourhood of θn−1 such that ||∇2\nθE(θ′)∇θE(θ′)|| is\nequal to the per-iteration drift ∥θ(h; θn−1) −θn∥, we evaluate ||∇2\nθE∇θE||\nat parameters θn−1.\nbut computing ∥∇2\nθE∇θE∥via Hessian-vector products is cheaper and approxima-\ntions are available, such as ∇2\nθE∇θE ≈∇θE(θ+ϵ∇θE)−∇θE(θ)\nϵ\nwhich only requires an\nadditional backward pass [151].\n3.7.2\nDrift adjusted learning (DAL)\nA natural question to ask is how to use the correlation between ∥∇2\nθE∇θE∥and\nthe per-iteration drift to improve training stability; ∥∇2\nθE∇θE∥captures all the\nquantities we have shown to be relevant to instability highlighted by the PF: λi and\n∇θETui (Eq. (3.19)). One way to use this information is to adapt the learning rate\nof the gradient descent update, such as using\n2\n∥∇2\nθE∇θE∥as the learning rate. This\nlearning rate slows down training when the drift is large—areas where instabilities\nare likely to occur—and it speeds up training in regions of low drift—areas where\ninstabilities are unlikely to occur. Computing the norm of the update provided by\nthis learning rate shows a challenge, however, since\n2∥∇θE∥\n∥∇2\nθE∇θE∥≥\n2∥∇θE∥\nλ0∥∇θE∥=\n2\nλ0; this\nimplies that when using this learning rate the norm of the gradient descent update\nwill never be 0 and thus training will not result in convergence. Furthermore, the\nmagnitude of the parameter update will be independent of the gradient norm. To\n3.7. Stabilising training by accounting for discretisation drift\n68\n0\n250\n500\n750\n1000 1250 1500 1750 2000\nIteration\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTrain loss\nVGG Full batch CIFAR-10\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.5\nDAL\n(a) VGG, CIFAR-10.\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n2\n4\n6\nTrain loss\nResnet-50, Batch size 2048, Imagenet\nLearning rate\n0.05\n0.1\n0.5\nDAL\n(b) Resnet-50, Imagenet.\nFigure 3.23: DAL: using the learning rate\n2\n∥∇2\nθEˆg(θ)∥results in improved stability with-\nout requiring a hyperparameter sweep; Resnet-18 results on CIFAR-10 in\nFigure A.15 in the Appendix.\nreinstate the gradient norm, we propose using the learning rate\nh(θ) =\n2\n∥∇2\nθE∇θE∥/ ∥∇θE∥=\n2\n∥∇2\nθEˆg(θ)∥,\n(3.20)\nwhere ˆg(θ) is the unit normalised gradient ∇θE/ ∥∇θE∥. We will call this learning\nrate DAL (Drift Adjusted Learning). As shown in Figure 3.21, ∥∇2\nθEˆg(θ)∥has a\nstrong correlation with the per-iteration drift. Another interpretation of DAL can\nbe provided through a signal-to-noise perspective: the size of the learning signal\nobtained by minimising E is that of the update h ∥∇θE∥, while the norm of the noise\ncoming from the drift can be approximated as h2\n2 ∥∇2\nθE∇θE∥. Thus, the ‘signal-to-\nnoise ratio’ can be approximated as h ∥∇θE∥/(h2\n2 ∥∇2\nθE∇θE∥) = 2/(h ∥∇2\nθEˆg(θ)∥),\nwhich when using DAL (Eq (3.20)) is 1; DAL can be seen as balancing the gradient\nsignal and the regularising drift noise in gradient descent training.\nWe use DAL to set the learning rate and show results across architectures,\nmodels, and datasets in Figure 3.23 (with additional results in Figure A.16 in the\nAppendix). Despite not requiring a learning rate sweep, DAL is stable compared to\nusing fixed learning rates. To provide intuition about DAL, we show the learning rate\nand the update norm in Figure 3.24: for DAL the learning rate decreases in training\nafter which it slowly increases when reaching areas with low drift. Compared to\nlarger learning static learning rates where the update norm can increase in the edge\n3.7. Stabilising training by accounting for discretisation drift\n69\n0\n200\n400\n600\n800\n1000\nIteration\n10\n2\n10\n1\n100\nLearning rate\nResnet-18 Full batch CIFAR-10\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.5\nDAL\n(a) Learning rate.\n0\n200\n400\n600\n800\n1000\nIteration\n10\n4\n10\n2\n100\nUpdate norm\nResnet-18 Full batch CIFAR-10\n(b) Update norm.\nFigure 3.24: Key quantities in DAL versus fixed learning rate training: (a) shows the\nlearning rate h(θ) which is either constant or\n2\n∥∇2\nθEˆg(θ)∥for DAL, and (b)\nshows update norms h(θ) ∥∇θE∥. Unlike using a fixed learning rate, DAL\nadjusts to the training landscape and even though later in training the\nlearning rate increases, this does not lead to an increase in the update norm.\nof stability area with DAL the update norm steadily decreases in training.\n3.7.3\nThe trade-off between stability and performance\nSince we are interested in understanding the optimisation dynamics of gradient\ndescent, we have so far focused on training performance. We now try to move our\nattention to test performance and generalisation. Previous works [12, 105, 152] have\nshown that higher learning rates lead to better generalisation performance. We now\ntry to further connect this information with the per-iteration drift and DAL. To\ndo so, we use learning rates with various degrees of sensitivity to per-iteration drift\nusing DAL-p:\nhp(θ) =\n2\n(∥∇2\nθEˆg(θ)∥)p.\n(3.21)\nThe higher p, the slower the training and less drift there is; the lower p, there is\nmore drift. We start with extensive experiments with p = 0.5, which we show in\nFigure 3.25, and show more results in Figure A.17. Compared to p = 1 (DAL), there\nis faster training but at times also more instability. Performance on the test set\nshows that DAL-0.5 performs as well or better than when using fixed learning rates.\nRemark 3.7.2 We find that across datasets and batch sizes, DAL-0.5 performs best\n3.7. Stabilising training by accounting for discretisation drift\n70\n0\n1\n2\n3\nTrain loss\nLearning rate\n0.01\n0.05\n0.1\n0.5\nDAL-0.5\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nVGG Full Batch CIFAR-10\n(a) VGG, CIFAR-10.\n0\n2\n4\n6\nTrain loss\nLearning rate\n0.05\n0.1\n0.5\nDAL-0.5\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 2048\n(b) Resnet-50, Imagenet.\nFigure 3.25: DAL-0.5 leads to increased training speed and generalisation compared to a\nsweep of fixed learning rates. Resnet-18 results on CIFAR-10 in Figure A.15\nin the Appendix.\n0\n250\n500\n750\n1000 1250 1500 1750 2000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nVGG Full batch CIFAR-10\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n(a) VGG, CIFAR-10.\n0\n10000\n20000\n30000\n40000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest (Top 1) accuracy\nResnet-50 Batch size 8192 Imagenet\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n(b) Resnet-50, Imagenet.\nFigure 3.26: DAL-p sweep: discretisation drift helps test performance at the cost of\nstability. Corresponding training curves and loss functions are present in\nthe Figure A.18 in the Appendix; results showing the same trends across\nvarious batch sizes are shown in Figure A.14.\nin terms of the stability generalisation trade-off and in these settings it can be used\nas a drop-in replacement for a learning rate sweep.\nTo further investigate the connection between drift and test set performance,\nwe perform a set of sweeps over the power p and show results in Figure 3.26. These\nresults show that the higher the drift (the smaller p), the more generalisation;\nadditional results across batch sizes showing the same trend are shown in Figure A.14\nin the Appendix. We also show in Figure 3.27 the correlation between mean per-\n3.7. Stabilising training by accounting for discretisation drift\n71\n0.05\n0.10\n0.15\nMean per iteration drift (approx)\n0.60\n0.62\n0.64\n0.66\n0.68\n0.70\nTest accuracy\nLearning rate\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.05\n0.10\n0.15\nMean per iteration drift (approx)\n20\n40\n60\n80\n100\n120\n140\nMean \n0 before convergence\nLearning rate\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n(a) Fixed learning rate sweep.\n0\n1\n2\n3\n4\nMean per iteration drift (approx)\n0.60\n0.65\n0.70\n0.75\n0.80\nTest accuracy\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0\n1\n2\n3\n4\nMean per iteration drift (approx)\n0\n100\n200\n300\n400\n500\nMean \n0 before convergence\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n(b) DAL-p sweep.\nFigure 3.27: The correlation between per-iteration drift, test set performance and λ0\nin full batch training on CIFAR-10. With fixed learning rates (a), the\ndrift increases as the learning rate increases which leads to improved test\nperformance and lower λ0 on average in training. When using DAL (b),\nthe drift is controlled through the dynamic learning rate which adjusts to\nthe magnitude of the drift; here too we observe that with high drift leads\nto increased performance and lower λ0 on average in training. The same\npattern can be seen in results with mini-batch training in Figure A.23.\niteration drift and test accuracy both for learning rate and DAL-p sweeps. The\nresults consistently show that the higher the mean per-iteration drift, the higher the\ntest accuracy. We also show that the mean per-iteration drift has a connection to\nthe largest eigenvalue λ0: the higher the drift, the smaller λ0. These results add\nfurther evidence to the idea that discretisation drift is beneficial for generalisation\n3.7. Stabilising training by accounting for discretisation drift\n72\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n0\n10\n20\n30\n40\n50\nDAL-0.125\n0\n10\n20\n30\n40\n50\n(a) DAL\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n0\n10\n20\n30\n40\n50\nSGD with LR 0.5\n0\n10\n20\n30\n40\n50\n(b) SGD\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n20\n40\n60\n80\n100\n0\nVGG 64 batch size\np in DAL-p\n0.125\n(c) DAL\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n20\n40\n60\n80\n100\n0\nVGG, batch size 64\nLearning rate\n0.5\n(d) SGD\nFigure 3.28: CIFAR-10, batch size 64. The 2D projection of the DAL-p (a) and SGD\n(b) learned landscapes on CIFAR-10 using a VGG model. The visualisation\nis made using the method of Li et al. [22]. Though both models achieve\na 86% accuracy on the test dataset, DAL-p has learned a flatter model.\nBottom row: We also show the trajectory of λ0 for both models, which\nshows that throughout the learning trajectory, DAL (c) travels through\nflatter landscapes compared to SGD (d), as measured by λ0.\nperformance in deep learning. We also notice that DAL-p with smaller values of p\nleads to a small λ0 compared to vanilla gradient descent even when large learning\nrates are used for the latter; this could explain its generalisation capabilities as\nlower sharpness has been connected to generalisation in previous works [18, 141, 153].\nTo consolidate these results, we use the method of Li et al. [22] to visualise the\nloss landscape learned by DAL-p compared to that learned using gradient descent\nin Figure 3.28, and observe that even when reporting similar accuracies, DAL-p\nconverges to a flatter landscape and traverses a flatter area of space throughout\n3.8. Future work\n73\ntraining. These results are consistent across small and large batch sizes; we show\nadditional results in Figures A.20 and A.22 in the Appendix.\nInspired by understanding when the PF is close to the NGF, in this section\nwe investigated the total discretisation drift of gradient descent. This led us to\nDAL-p, a method to automatically set the learning rate based on approximation to\nthe per-iteration drift of gradient descent; we have seen that DAL produces stable\ntraining and further connected discretisation drift, generalisation, and flat landscapes\nas measured by leading Hessian eigenvalues.\n3.8\nFuture work\nBeyond gradient descent. In this chapter, we focused on understanding vanilla\ngradient descent. Understanding discretisation drift via the PF can be beneficial\nfor improving other gradient based optimisation algorithms as well, as we briefly\nillustrate for momentum updates with decay β and learning rate h (for a discussion\non momentum, see Section 2.2):\nvt = βvt−1 −h∇θE(θt−1);\nθt = θt−1 + vt.\n(3.22)\nWe can scale ∇θE(θt−1) in the above not by a fixed learning rate h, but by adjusting\nthe learning rate according to the approximation to the drift. This has two advantages:\nit removes the need for a learning rate sweep and it uses local landscape information\nin adapting the moving average, such that in areas of large drift the contribution\nis decreased, while it is increased in areas where the drift is small (a more formal\njustification is provided in Section A.1.9). This leads to the following updates:\nvt = βvt−1 −\n1\n2||∇2\nθE(θt−1)ˆg(θ)(θt−1)||∇θE(θt−1);\nθt = θt−1 + vt.\n(3.23)\nAs with DAL-p, we can use powers to control the stability performance trade-off: the\nlower p, the more the current update contribution is reduced in high drift (instability)\nareas. We tested this approach on Imagenet and show results in Figure 3.29. The\n3.8. Future work\n74\n0\n2000\n4000\n6000\n8000\n10000 12000 14000\nIteration\n0\n1\n2\n3\n4\n5\n6\n7\nTraining loss\nImagenet Batch size 8192 with momentum 0.9\nLearning rate 0.05\nLearning rate 0.1\nLearning rate 0.5\nDrift scaling power 0.5\nDrift scaling power 1.0\n(a) Training loss.\n0\n2000\n4000\n6000\n8000\n10000 12000 14000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest (Top 1) accuracy\nImagenet Batch size 8192 with momentum 0.9\n(b) Test accuracy.\nFigure 3.29: DAL with momentum: integrating drift information results in faster and\nmore stable training compared to a fixed learning rate sweep. Compared\nto vanilla gradient descent there is also a significant performance and\nconvergence speed boost.\nresults show that integrating drift information improves the speed of convergence\ncompared to standard gradient descent (Figure 3.26), and leads to more stable\ntraining compared to using a fixed learning rate. We present additional experimental\nresults in the Appendix.\nJust as momentum is a common staple of optimisation algorithms, so are\nadaptive schemes such as Adam [47] and Adagrad [154], which adjust the step taken\nfor each parameter independently. We can also use the knowledge from the PF\nto set a per-parameter learning rate: instead of using ∥∇2\nθE(θt−1)∇θE(θt−1)∥to\nset a global learning rate, we can use the per-parameter information provided by\n∇2\nθE(θt−1)∇θE(θt−1) to adapt the learning rate of each parameter. We present\npreliminary results in the Figure 3.30. The above two approaches (momentum and\nper-parameter learning rate adaptation) can be combined, bringing us closer to the\nmost commonly used deep learning optimisation algorithms, such as Adam (see\nSection 2.2). While we do not explore this avenue here, we are hopeful that this\nunderstanding of discretisation drift can be leveraged further to stabilise and improve\ndeep learning optimisation.\nNon-principal terms. This work focuses on understanding the effects of the\nPF on the behaviour of gradient descent. Beyond the PF, we found one non-principal\nterm (Eq (3.4)) and have seen that it can have a stabilising effect (Figure 3.8). We\n3.8. Future work\n75\n0\n2000\n4000\n6000\n8000 10000 12000 14000\nIteration\n1\n2\n3\n4\n5\n6\n7\nTrain loss\nResnet-50 Batch size 2048 Imagenet\n(a) Training loss.\n0\n2000\n4000\n6000\n8000 10000 12000 14000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nResnet-50 Batch size 2048 Imagenet\nDAL-p (per parameter)\n0.25\n0.5\n1.0\n1.5\n(b) Test accuracy.\nFigure 3.30: Imagenet results when using a DAL like method of scaling per-parameter.\nInstead of using the global learning rate 2/||∇θE2bg(θ)||, using the per-\nparameter θi learning rate h(θi) =\n2\n(∇θE2bg(θ)/\n√\nD)\np\ni\n.\nThese preliminary\nresults show that using the per parameter information from discretisation\ndrift can be used as an information to guide training.\nprovided a preliminary explanation for the stabilising effect of this non-principal\nterm in Section 3.6. One promising avenue of non-principal terms is theoretically\nmodelling the change of the eigenvalues λi in time; another promising direction is\nthat of implicit regularisation: while existing work that uses BEA in deep learning\nhas found important implicit regularisation effects [12, 13, 136], we have shown here\nthat effects of O(h3) are not sufficient to capture the intricacies of gradient descent,\nsuggesting that other implicit regularisation effects could be uncovered using the\nnon-principal terms.\nNeural network theory. Many theoretical works studying at gradient descent\nin the neural network context use the NGF [54, 65, 100, 131]. We posit that replacing\nNGF in these theoretical contexts with PF may yield interesting results. In contrast\nto the NGF, the PF allows for the incorporation of the learning rate into the analysis,\nand unlike existing continuous-time models of gradient descent, it can model unstable\nbehaviours observed in the discrete case. An example can be seen using the Neural\nTangent Kernel: Jacot et al. [100] model gradient descent using the NGF to show\nthat in the infinite-width limit gradient descent for neural networks follows kernel\ngradient descent. The PF can be incorporated in this analysis either by replacing\nthe NGF with the PF as a model of gradient descent or by studying the difference in\n3.9. Related work\n76\nthe PF for infinitely wide and finite width networks, since discretisation drift could\nbe responsible for the observed gap between finite and infinite networks in the large\nlearning rate case [155].\n3.9\nRelated work\nModified flows for deep learning optimisation.\nBarrett and Dherin [12]\nfound the first order correction modified flow for gradient descent using BEA and\nuncovered its regularisation effects; they were the first to show the power of BEA in\nthe deep learning context. Smith et al. [13] find the first order error correction term in\nexpectation during one epoch of stochastic gradient descent. Modified flows have also\nbeen used for other optimisers than vanilla gradient descent: Fran¸ca et al. [60], Shi\net al. [156] compare momentum and Nesterov accelerated momentum; Kunin et al.\n[65] study the symmetries of deep neural networks and use modified flows to show\ncommonly used discrete updates break conservation laws present when using the\nNGF (for gradient descent they use the IGR flow while for momentum and weight\ndecay they introduce different flows); Kovachki and Stuart [157] use modified flows\nto understand the behaviour of momentum by approximating Hamiltonian systems;\nFran¸ca et al. [158] construct optimisers controlling their stability and convergence\nrates while Li et al. [159] construct optimisers with adaptive learning rates in the\ncontext of stochastic differential equations.\nIn concurrent work Miyagawa [160] use BEA to find a modified flow coined\n‘Equations of Motion’ (EOM) to describe gradient descent and find higher order\nterms, including non-principal terms; their focus is, however, on EOM(1), which is the\nIGR flow, which they use to understand scale and translation invariant layers. Their\napproach does not expand to complex space and does not capture the instabilities\nstudied here (see also the discussion on the difference between the full modified flow\nprovided by BEA and the PF in Section 3.3).\nEdge of stability and the importance of the Hessian. There have been a\nnumber of empirical studies on the Hessian in gradient descent. Cohen et al. [52]\nobserved the edge of stability behaviour and performed an extensive study which led\n3.9. Related work\n77\nto many empirical observations used in this work. Jastrzebski et al. [141] performed a\nsimilar study in the context of stochastic gradient descent. Sagun et al. [144], Ghorbani\net al. [145], Papyan [146] approximate the entire spectrum of the Hessian, and show\nthat there are only a few negative eigenvalues, plenty of eigenvalues centered around\n0, and a few positive eigenvalues with large magnitude. Similarly, Gur-Ari et al. [124]\ndiscuss how gradient descent operates in a small subspace. Lewkowycz et al. [53]\ndiscuss the large learning rate catapult in deep learning when the largest eigenvalue\nexceeds 2/h. Gilmer et al. [125] assess the effects of the largest Hessian eigenvalue in\na large number of empirical settings.\nThere have been a series of concurrent works aimed at theoretically explain-\ning the empirical results above.\nAhn et al. [161] connect the edge of stability\nbehaviour with what they coin as the ‘relative progress ratio’:\nE(θ−h∇θE)−E(θ)\nh∥∇θE∥2\n,\nwhich they empirically show is 0 in stable areas of training and 1 in the edge\nof stability areas. To see the connection between the relative progress ratio and\nthe quantities discussed in this chapter, one can perform a Taylor expansion on\nE(θ−h∇θE)−E(θ)\nh∥∇θE∥2\n≈\n−h∇θET ∇θE+h2/2∇θET ∇2\nθE∇θE\nh∥∇θE∥2\n= −1 + h\n2\n∇θET ∇2\nθE∇θE\n∥∇θE∥2\n. While this ra-\ntio is related to the quantities we discuss, we also note significant differences: it is\na scalar, and not a parameter-length vector and thus does not capture per eigendi-\nrection behaviour as we see with the stability coefficients (Section 3.5). Arora et al.\n[162] prove the edge of stability result occurs under certain conditions either on\nthe learning rate or on the loss function. Ma et al. [163] empirically observe the\nmulti-scale structure of the loss landscape in neural networks and use it to theoreti-\ncally explain the edge of stability behaviour of gradient descent. Chen and Bruna\n[164] use low dimensional theoretical insights around a local minima to understand\nthe edge of stability behaviour. Damian et al. [148] use a cubic Taylor expansion\nto show that gradient descent follows the trajectory of a projected method which\nensures that λ0 < 2/h and ∇θETu = 0; their work is what inspired us to write the\nthird-order non-principal term in the form of Eq (3.18), after we had previously\nnoted its stabilising properties. These important works are complementary to our\nown work; they do not use continuous-time approaches and tackle primarily the\n3.9. Related work\n78\nedge of stability problem or its subcases, while we focus on understanding gradient\ndescent and applying that understanding broadly, including but not limited to the\nedge of stability phenomenon.\nDiscrete models of gradient descent. The desire to understand learning\nrate specific behaviour in gradient descent has been a motivation in the construc-\ntion of discrete-time analyses. These analyses have provided great insights, from\nstudying noise in the stochastic gradient descent setting [64, 165], the study of over-\nparametrised neural models and their convergence [49, 127, 128], providing examples\nof when gradient descent can converge to local maxima [130], the importance of\nwidth for proving convergence in deep linear networks [129]. We differ from these\nstudies both in motivation and execution: we are looking for a continuous-time flow\nthat will increase the applicability of continuous-time analysis of gradient descent.\nWe do so by incorporating discretisation drift using BEA and showing that the\nresulting flow is a useful model of gradient descent, which captures instabilities and\nescape of local minima and saddle points.\nUnderstanding the difference between the negative gradient flow and\ngradient descent. Elkabetz and Cohen [54] recently examined the differences\nbetween gradient descent and the NGF in the deep learning context; their work\nexamines the importance of the Hessian in determining when gradient descent follows\nthe NGF. Their theoretical results show that neural networks are roughly convex\nand thus for reasonably sized learning rates one can expect that gradient descent\nfollows the NGF flow closely. Their results complement ours and their approach\nmight be extended to help us understand why the PF is sufficient to shed light on\nmany instability observations in the neural network training.\nSecond-order optimisation. By using second order information (or approxi-\nmations thereof) to set the learning rate, DAL is related to second-order approaches\nused in deep learning. Many second-order methods can be seen as approximates of\nNewton’s method θt = θt−1 −∇2\nθE−1(θt−1)∇θE(θt−1). Since computing the Hessian\ninverse, or storing a matrix of the size of the Hessian as required by Quasi-Netwon\nmethods [166], can be prohibitively expensive for large models, many practical\n3.10. Conclusion\n79\nmethods approximate it with tractable alternatives [167]. Foret et al. [153] propose\nan optimisation scheme directly aimed at minimising sharpness and show this can\nimprove generalisation.\nConnection between drift and generalisation. We have made the con-\nnection between discretisation drift and generalisation. This connection was first\nmade by Barrett and Dherin [12] through the IGR flow. Generalisation has also\nbeen connected to the largest eigenvalue λ0[18, 53, 141, 168]; recently Kaur et al.\n[169] however showed a more complex picture, primarily in the context of stochastic\ngradient descent. The largest eigenvalue could be a confounder to the drift as we have\nobserved in Section 3.7.3; we hope that future work can deepen these connections.\n3.10\nConclusion\nWe have expanded on previous works that used backward error analysis in deep\nlearning to find a new continuous-time flow, called the Principal Flow (PF), to\nanalyse the behaviour of gradient descent. Unlike existing flows, the PF operates in\ncomplex space, which enables it to better capture the behaviour of gradient descent\ncompared to existing flows, including but not limited to instability and oscillatory\nbehaviour. We use the form of the PF to find new quantities relevant to the stability\nof gradient descent, and shed light on newly observed empirical phenomena, such as\nthe edge of stability results. After understanding the core quantities connected to\ninstabilities in deep learning we devised an automatic learning rate schedule, DAL,\nwhich exhibits stable training. We concluded by cementing the connection between\nlarge discretisation drift and increased generalisation performance. We ended by\nhighlighting future work avenues including incorporating the PF in existing theoretical\nanalyses of gradient descent which use the negative gradient flow, incorporating our\nunderstanding of the drift of gradient descent in other optimisation approaches and\nspecialising the PF for neural network function approximators.\nChapter 4\nContinuous time models of\noptimisation in two-player games\nThe fusion of deep learning with two-player games has produced a wealth of break-\nthroughs in recent years, from Generative Adversarial Networks (GANs) [32] through\nto model-based reinforcement learning [170, 171]. Gradient descent methods are\nwidely used across these settings, partly because these algorithms scale well to high-\ndimensional models and large datasets. However, much of the recent progress in our\ntheoretical understanding of two-player differentiable games builds upon the analysis\nof continuous differential equations that model the dynamics of training [17, 172, 173],\nleading to a gap between theory and practice. Our aim is to take a step forward in\nour understanding of two player games by finding continuous systems which better\nmatch the gradient descent updates used in practice.\nOur work builds upon Barrett and Dherin [12], who use backward error analysis\n(BEA) to quantify the discretisation drift induced by using gradient descent in\nsupervised learning. We extend their work and use BEA to understand the impact\nof discretisation in the training dynamics of two-player games. More specifically,\nwe quantify Discretisation Drift (DD), the difference between the solutions of the\noriginal flows defining the game and the discrete steps of the numerical scheme used\nto approximate them. To do so, we construct modified continuous systems that\nclosely follow the discrete updates. While in supervised learning DD has a beneficial\nregularisation effect [12] (see also Section 2.3.2), we find that the interaction between\nplayers in DD can have a destabilising effect in adversarial games.\n4.1. Background\n81\nContributions: Our primary contribution, Theorems 4.2.1 and 4.2.2, provides\nthe continuous modified systems that quantify the discretisation drift in simultaneous\nand alternating gradient descent for general two-player differentiable games. Both\ntheorems are novel in their scope and generality, as well as their application toward\nunderstanding the effect of the discretisation drift in two-player games parametrised\nwith neural networks. Theorems 4.2.1 and 4.2.2 allow us to use dynamical system\nanalysis to describe GD without ignoring discretisation drift, which we then use to:\n• Provide new stability analysis tools (Section 4.3).\n• Motivate explicit regularisation methods that drastically improve the perfor-\nmance of simultaneous gradient descent in GAN training (Section 4.6.3).\n• Pinpoint optimal regularisation coefficients and shed new light on existing\nexplicit regularisers (Table 4.2).\n• Pinpoint the best performing learning rate ratios for alternating updates\n(Sections 4.4 and 4.6.2).\n• Explain previously observed but unexplained phenomena such as the difference\nin performance and stability between alternating and simultaneous updates in\nGAN training (Section 4.5).\n4.1\nBackground\nA well-developed strategy for understanding two-player games in gradient-based\nlearning is to analyse the continuous dynamics of the game [172, 173]. Tools from\ndynamical systems theory have been used to explain convergence behaviours and to\nimprove training using modifications of learning objectives or learning rules [17, 61, 70].\nWhile many insights from continuous analysis carry over to two-player games trained\nwith discrete updates, discrepancies are often observed between the continuous\nanalysis conclusions and discrete game training [68]. In this chapter, we extend BEA\nfrom the single-objective case to the more general two-player game setting, thereby\nbridging the gap between the continuous systems that are often analysed in theory\n4.1. Background\n82\nand the discrete numerical methods used in practice. For an overview of optimisation\nin two-player games, we refer the reader to Section 2.4. We provided an overview of\nBEA in Section 2.3.2, and used it in the previous chapter in the single-objective case.\n4.1.1\nGenerative adversarial networks\nGenerative adversarial networks(GANs) [32] learn an implicit generative model\nthrough a two-player game. An implicit generative model does not have an explicit\nlikelihood associated with it, as it is often the case with other machine learning\nmodels. Instead, an implicit model can be thought of as a simulator: it provides a\nsampling path that can be used to generate samples from its distribution. In the\ncase of GANs the sampling path is defined via a latent variable model:\nG(z; θ),\nz ∼p(z)\n(4.1)\nwhere p(z) is a prior distribution and G(z; θ) is the model with parameters θ, often\na deep neural network. This sampling process induces the implicit distribution\np(x; θ) =\nZ\nG(z;θ)=x\np(x|z; θ)p(z)dz =\nZ\nI(G(z; θ), x)p(z)dz,\n(4.2)\nwhere I(G(z; θ), x) is 1 if its two arguments are equal and 0 otherwise. The above\nintegral is intractable to compute or expensive to approximate via approaches such\nas annealed importance sampling [174] for many cases of interest, such as when\nG(·, θ) is given by a deep neural network. This makes standard methods, such as\nmaximum likelihood, unavailable to learn implicit models. To learn the parameters θ,\nGANs introduce another model, the discriminator D(·, ϕ), which aims to distinguish\nbetween real data—samples from the dataset—and generated data—samples from the\nmodel G(·, θ). The generative model G(·, θ), called the generator, aims to generate\nsamples that cannot be distinguished from real data according to the discriminator.\nIn the original GAN paper [32] the discriminator D(·, ϕ) is a binary classifier\ntrained using the binary cross entropy loss; the generator’s aim is to ensure that the\ndiscriminator does not classify generated data as fake. This can be formalised as the\n4.1. Background\n83\nzero-sum game:\nmin\nθ max\nϕ\nE = 1\n2Ep∗(x) log D(x; ϕ) + 1\n2Ep(x;θ) log(1 −D(x; ϕ))\n(4.3)\n= 1\n2Ep∗(x) log D(x; ϕ) + 1\n2Ep(z) log(1 −D(G(z; θ); ϕ).\n(4.4)\nwhere Eq (4.4) uses the change-of-variable in the pathwise estimator discussed in\nSection 2.1.1. To connect this zero-sum game to generative modelling objectives used\nto learn probability distributions, Goodfellow et al. [32] set the functional derivative\nin Eq (4.4) to 0 to find the optimal discriminator D∗:\n1\n2p∗(x)\n1\nD∗(x) −1\n2p(x; θ)\n1\n1 −D∗(x) = 0 =⇒D∗(x) =\n2p∗(x)\np∗(x) + p(x; θ).\n(4.5)\nReplacing the above in Eq (4.4) we have:\nE = 1\n2Ep∗(x) log\n2p∗(x)\n(p∗(x) + p(x; θ)) + 1\n2Ep(x;θ)\n2p(x; θ)\n(p∗(x) + p(x; θ))\n(4.6)\n= KL(p∗||\n1\np∗+p(·;θ)\n2\n) + KL(p(·; θ)||\n1\np∗+p(·;θ)\n2\n)\n(4.7)\n= JSD(p∗||p(·; θ)),\n(4.8)\nwhere KL and JSD denote the KL and Jensen–Shannon divergences respectively.\nThis entails that if the discriminator is optimal, the GAN generator is minimising\nthe Jensen–Shannon divergence between the model and data distribution.\nThe connection between distributional divergences and two-player games has\nfuelled a line of research motivated by finding the best objective to train GANs.\nThis lead to new GANs motivated by Integral Probability Metrics, such as the\nWasserstein distance or the Maximum Mean Discrepancy [36, 37]; f-divergences [35];\nand others [175]. Practitioners have observed, however, that what is most relevant\nin increasing GAN performance often comes from changes architectures, optimisa-\ntion, and regularisation [31, 33, 34], rather than underlying divergence used. An\nexplanation for this observation is that the GAN discriminator is not optimal, both\nbecause it is modelled using a parametric model and because the discriminator is not\n4.2. Discretisation drift in two-player games\n84\noptimised to convergence for each generator update, as required to obtain the best\nestimate of the underlying divergence [28]. Instead, as discussed in Section 2.4, due\nto computational considerations the nested optimisation problem in Eq (4.4) gets\ntranslated into the alternating updates described in Algorithm 1.\nSince GANs are implicit generative models, evaluating them via likelihoods\nis computationally intractable; approaches to approximate the likelihood do exist,\nbut they have strong limitations [176] and are rarely used in practice. Instead,\nthe main avenue for GAN evaluation is to measure aspects of sample quality and\ndiversity via a pretrained classifier, as done by the Inception Score [177] and Fr´echet\nInception Distance [173]. The Inception Score uses label information from the dataset,\ntogether with a pre-trained classifier, to assess a generative model. It does so by\nusing the pre-trained classifier to obtain class posterior probabilities from samples\np(y|x), and from there the marginal distribution induced over class labels by the\nmodel distribution p(y; θ) =\nR\np(y|x)p(x; θ)dx, which is estimated using Monte-\nCarlo estimation. The Inception Score then measures average KL divergence between\np(y; θ) and the distribution p(y|x) obtained from model samples: eEp(x;θ)KL(p(y|x)||pθ(y)).\nWhen a model produces high quality samples from all classes in the dataset p(y; θ)\nwill be a broad distribution capturing the marginal distribution over classes in the\ndataset, while p(y|x) will be a sharp distribution pre-training to the class associated\nwith x; this leads to a high KL(p(y|x)||p(y; θ) and thus a high Inception Score score.\n4.2\nDiscretisation drift in two-player games\nWe denote by ϕ ∈Rm and θ ∈Rn the parameters of the first and second player,\nrespectively.\nThe players update functions will be denoted correspondingly by\nf(ϕ, θ) : Rm × Rn →Rm and by g(ϕ, θ) : Rm × Rn →Rn. The Jacobian df\ndθ(ϕ, θ) is\nthe m × n matrix df\ndθ(ϕ, θ)i,j =\n\u0000∂θjfi\n\u0001\nwith i = 1, . . . , m and j = 1, . . . , n.\nWe aim to understand the impact of discretising the flows\n˙ϕ = f(ϕ, θ)\n(4.9)\n˙θ = g(ϕ, θ),\n(4.10)\n4.2. Discretisation drift in two-player games\n85\nθt−1\nθt\nθt+1\nt −1 →t\nt →t + 1\n˙θ\n˙θ\n˙˜θ\n˙˜θ\nModified flow error\nO(h3)\nOriginal flow error\nO(h2)\nFigure 4.1: We use BEA to find the modified flow ˙˜θ that captures the change in parameters\nintroduced by the discrete updates with an error of O(h3). The modified\nflow follows the discrete update more closely than the flow ˙θ, which has an\nerror of O(h2). Here, we show only θ for simplicity.\nusing either simultaneous or alternating Euler updates. To do so, we derive a modified\ncontinuous system of the form\n˙ϕ = f(ϕ, θ) + hf1(ϕ, θ)\n(4.11)\n˙θ = g(ϕ, θ) + hg1(ϕ, θ),\n(4.12)\nwhich closely follows the discrete Euler update steps; the local error between a\ndiscrete update and the modified system is of order O(h3) instead of O(h2) as is\nthe case for the continuous system given by Eqs (4.9) and (4.10). We visualise our\napproach in Figure 4.1.\nWe can specialise these modified equations using f = −∇ϕEϕ and g = −∇θEθ,\nwhere Eϕ and Eθ are the loss functions for the two players. We will use this setting\nlater to investigate the modified dynamics of simultaneous or alternating gradient\ndescent. We can further specialize the form of these updates for common-payoff\ngames (Eϕ = Eθ = E) and zero-sum games (Eϕ = −E, Eθ = E).\n4.2.1\nDD for simultaneous Euler updates\nThe simultaneous Euler updates with learning rates υϕh and υθh respectively are\ngiven by\nϕt = ϕt−1 + υϕhf(θt−1, ϕt−1)\n(4.13)\nθt = θt−1 + υθhg(θt−1, ϕt−1).\n(4.14)\n4.2. Discretisation drift in two-player games\n86\nBy applying BEA to these discrete updates, we obtain:\nTheorem 4.2.1 The discrete simultaneous Euler updates in (4.13) and (4.14) follow\nthe continuous system\n˙ϕ = f −υϕh\n2\n\u0012 df\ndϕf + df\ndθg\n\u0013\n(4.15)\n˙θ = g −υθh\n2\n\u0012 dg\ndϕf + dg\ndθg\n\u0013\n(4.16)\nwith an error of size O(h3) after one update step.\nRemark 4.2.1 A special case of Theorem 4.2.1 for zero-sum games with equal\nlearning rates can be found in Lu [178].\n4.2.2\nDD for alternating Euler updates\nFor alternating Euler updates, the players take turns to update their parameters, and\ncan perform multiple updates each. We denote the number of alternating updates by\nm and k for the first player and second player, respectively. We scale the learning\nrates by the number of updates, leading to the following updates ϕt := ϕm,t and\nθt := θk,t where\nϕi,t = ϕi−1,t + υϕh\nm f(ϕi−1,t, θt−1),\ni = 1, . . . , m,\n(4.17)\nθj,t = θj−1,t + υθh\nk g(ϕm,t, θj−1,t),\nj = 1, . . . , k.\n(4.18)\nBy applying BEA to these discrete updates, we obtain:\nTheorem 4.2.2 The discrete alternating Euler updates in (4.17) and (4.18) follow\nthe continuous system\n˙ϕ = f −υϕh\n2\n\u0012 1\nm\ndf\ndϕf + df\ndθg\n\u0013\n(4.19)\n˙θ = g −υθh\n2\n\u0012\n(1 −2υϕ\nυθ\n) dg\ndϕf + 1\nk\ndg\ndθg\n\u0013\n(4.20)\nwith an error of size O(h3) after one update step.\n4.2. Discretisation drift in two-player games\n87\nRemark 4.2.2 Equilibria of the original continuous systems (i.e., points where\nf = 0 and g = 0) remain equilibria of the modified continuous systems.\nDefinition 4.2.1 The discretisation drift components we identify for each player\nhave two terms: one term containing a player’s own update function only—terms\nwe will call self terms—and a term that also contains the other player’s update\nfunction—which we will call interaction terms.\n4.2.3\nSketch of the proofs\nFollowing BEA, the idea is to modify the original continuous system by\nadding corrections in powers of the learning rate:\n˜f = f + hf1 + h2f2 + · · · and\n˜g = g + hg1 + h2g2 + · · ·, where for simplicity in this proof sketch, we use the same\nlearning rate for both players (detailed derivations can be found in the Appendix\nSection B.1; the general structure of BEA proofs can be found in Section A.1.1).\nWe want to find corrections fi, gi such that the modified system ˙ϕ = ˜f and ˙θ = ˜g\nfollows the discrete update steps exactly. To do that we proceed in three steps:\nStep 1: We expand the numerical scheme to find a relationship between ϕt\nand ϕt−1 and θt and θt−1 up to order O(h2).\nIn the case of the simultaneous\nEuler updates this does not require any change to Eqs (4.13) and (4.14), while for\nalternating updates we have to expand the intermediate steps of the integrator using\nTaylor series.\nStep 2: We compute the Taylor series of the modified equations solution:\nϕ(h) = ϕt−1 + hf + h2\n\u0012\nf1 + 1\n2( df\ndϕf + df\ndθg)\n\u0013\n+ O(h3)\n(4.21)\nθ(h) = θt−1 + hg + h2\n\u0012\ng1 + 1\n2( dg\ndϕf + dg\ndθg)\n\u0013\n+ O(h3),\n(4.22)\nwhere all the fs, gs, and their derivatives are evaluated at (ϕt−1, θt−1).\nStep 3: We match the terms of equal power in h so that the solution of modified\nequations coincides with the discrete update after one step. This amounts to finding\nthe corrections, fis and gis, so that all the terms of order higher than O(h) in the\nmodified equation solution above will vanish; this yields the first order drift terms f1\n4.3. The stability of DD\n88\nand g1 in terms of f, g, and their derivatives. For simultaneous updates we obtain\nf1 = −1\n2( df\ndϕf + df\ndθg) and g1 = −1\n2( dg\ndϕf + dg\ndθg), and by construction, we have obtained\nthe modified truncated system which follows the discrete updates exactly up to order\nO(h3), leading to Theorem 4.2.1.\nRemark 4.2.3 The modified equations in Theorems 4.2.1 and 4.2.2 closely follow\nthe discrete updates only for learning rates where errors of size O(h3) can be neglected.\nBeyond this, higher order corrections are likely to contribute to the DD.\n4.2.4\nVisualising trajectories\nTo illustrate the effect of DD in two-player games, we use a simple example adapted\nfrom Balduzzi et al. [61]:\n˙ϕ = f(ϕ, θ) = −ϵ1ϕ + θ;\n˙θ = g(ϕ, θ) = ϵ2θ −ϕ.\n(4.23)\nIn Figure 4.2 we validate our theory empirically by visualising the trajectories of the\ndiscrete Euler steps for simultaneous and alternating updates, and show that they\nclosely match the trajectories of the corresponding modified continuous systems that\nwe have derived. To visualise the trajectories of the original unmodified continuous\nsystem, we use Runge–Kutta 4 (a fourth-order numerical integrator that has no DD\nup to O(h5) in the case where the same learning rates are used for the two players—\nsee Hairer and Lubich [179] and Appendix Section B.3). We will use Runge–Kutta 4\nas a low drift method for the rest of this chapter, and compare it with Euler updates\nin order to understand the effect of DD.\n4.3\nThe stability of DD\nThe long-term behaviour of gradient-based training can be characterised by the\nstability of its equilibria. Using stability analysis of a continuous system to understand\ndiscrete dynamics in two-player games has a fruitful history; however, prior work\nignored discretisation drift, since they analyse the stability of the unmodified game\nflows in Eq (4.9) and 4.10. A general overview of stability analysis is provided in\nSection 2.3.1. Using the modified flows given by BEA provides two benefits here:\n4.3. The stability of DD\n89\n−7.5\n−2.5\n2.5\n7.5\n1e−2\n−7.5\n−2.5\n2.5\n7.5\n1e−2\nSimultaneous Euler updates\nOriginal flow\nEuler\nModified flow\n(a) Simultaneous updates.\n−7.5\n−2.5\n2.5\n7.5\n1e−2\n−7.5\n−2.5\n2.5\n7.5\n1e−2\nAlternating Euler updates\nOriginal flow\nEuler\nModified flow\n(b) Alternating updates.\nFigure 4.2: By capturing the first order DD, the modified continuous flows we derive\nbetter capture the dynamics of their corresponding discrete update than the\noriginal game continuous dynamics. By doing so, the flows also capture the\ndifference between simultaneous (a) and alternating Euler updates (b).\nfirst, they account for the discretisation drift, and second, they provide different\nflows for simultaneous and for alternating updates, capturing the specificities of the\ntwo optimisers.\nThe modified flow approach gives us a method to analyse the stability of the\ndiscrete updates: 1) Choose the system and the update type—simultaneous or\nalternating—to be analysed; 2) write the modified flows for the chosen system as\ngiven by Theorems 4.2.1 and 4.2.2; 3) write the corresponding modified Jacobian,\nand evaluate it at an equilibrium; 4) determine the stability of the equilibrium by\ncomputing the eigenvalues of the modified Jacobian, using Remark 2.3.1.\nSteps 1 and 2 are easy, and step 4 is required in the stability analysis of any\ncontinuous system. For step 3, we provide a general form of the modified Jacobian\nat the equilibrium point of the original system, where f = 0 and g = 0\neJ =\n\n\nd ef\ndϕ\nd ef\ndθ\ndeg\ndϕ\ndeg\ndθ\n\n= J −h\n2K,\n(4.24)\nwhere J is the Jacobian of the un-modified system in Eqs (4.9)-(4.10) and K depends\n4.3. The stability of DD\n90\non the update type; proofs are provided in Section B.4. For simultaneous updates\nKsim =\n\nυϕ( df\ndϕ)2 + υϕ\ndf\ndθ\ndg\ndϕ\nυϕ\ndf\ndϕ\ndf\ndθ + υϕ\ndf\ndθ\ndg\ndθ\nυθ\ndg\ndθ\ndg\ndϕ + υθ\ndg\ndϕ\ndf\ndϕ\nυθ( dg\ndθ)2 + υθ\ndg\ndϕ\ndf\ndθ\n\n.\n(4.25)\nFor alternating updates, we have:\nKalt =\n\n\nυϕ\nm ( df\ndϕ)2 + υϕ\ndf\ndθ\ndg\ndϕ\nυϕ\nm\ndf\ndϕ\ndf\ndθ + υϕ\ndf\ndθ\ndg\ndθ\nυθ\nk\ndg\ndθ\ndg\ndϕ + υθ(1 −2υϕ\nυθ ) dg\ndϕ\ndf\ndϕ\nυθ\nk ( dg\ndθ)2 + υθ(1 −2υϕ\nυθ ) dg\ndϕ\ndf\ndθ\n\n.\n(4.26)\nUsing the method above we show that, in two-player games, the drift term of order\nO(h2) can change a stable equilibrium into an unstable one. This contrasts games and\nsupervised learning, as the stability analysis of the IGR flow shows it is attracted to\nstrict local minima (see Barrett and Dherin [12] and Section 3.3.2). For simultaneous\nupdates with equal learning rates, this recovers a result of Daskalakis and Panageas\n[180] derived for zero-sum games. We show this by example: consider the game given\nby the system of flows in Eq (4.23). By replacing the values of f and g into the results\nfor the Jacobian of the modified flows obtain above, we obtain the corresponding\nJacobians. For simultaneous updates\n˜Jsim =\n\n−ϵ1 −h/2ϵ2\n1 + h/2\n1 + h/2ϵ1 −h/2ϵ2\n−1 −h/2ϵ1 + h/2ϵ2\nϵ2 + h/2 −h/2ϵ2\n2\n\n,\n(4.27)\nwhile for alternating updates\n˜Jalt =\n\n−ϵ1 −h/2ϵ2\n1 + h/2\n1 + h/2ϵ1 −h/2ϵ2\n−1+h/2ϵ1 + h/2ϵ2\nϵ2−h/2 −h/2ϵ2\n2\n\n.\n(4.28)\nIf we consider the system with ϵ1 = ϵ2 = 0.09, the stability analysis of its modified\nflows for the simultaneous Euler updates shows they diverge when υϕh = υθh = 0.2,\nsince Tr(˜Jsim) = 0.19 > 0, and thus at least one eigenvalue has positive real part.\nFor alternating updates, we obtain Tr(˜Jalt) = −0.0016 < 0 and |˜Jalt| = 0.981 > 0,\nleading to a stable system. In both cases, the results obtained using the stability\n4.3. The stability of DD\n91\n−15\n−5\n5\n15\n1e−2\n−15\n−5\n5\n15\n1e−2\nSimultaneous Euler updates\nOriginal flow\nEuler\nModified flow\n(a) Simultaneous updates.\n−15\n−5\n5\n15\n1e−2\n−15\n−5\n5\n15\n1e−2\nAlternating Euler updates\nOriginal flow\nEuler\nModified flow\n(b) Alternating updates.\nFigure 4.3: Discretisation drift can change the stability of a game. We show here an\nexample where the flow given by the original game dynamics converges, but\nsimultaneous Euler updates do not (a); for the same system, alternating\nupdates converge (b). In both cases, the modified flows capture the convergent\nor divergent behaviour of Euler updates. ϵ1 = ϵ2 = 0.09 and a learning rate\nυϕh = υθh = 0.2.\nanalysis of the modified flows is consistent with empirical outcomes obtained by\nfollowing the corresponding discrete updates, as shown in Figure 4.3; this would not\nhave been the case had we used the original system to do stability analysis, which\nwould have always predicted convergence to an equilibrium and would not have been\nable to distinguish between simultaneous and alternating updates.\nBenefits and caveats of using the modified flows for stability analysis. The\nmodified flows help us bridge the gap between theory and practice: they allow us to\nextend the reach of stability analysis to a wider range of techniques used for training,\nsuch as alternating gradient descent. We hope the method we provide will be used\nin the context of GANs, to expand prior work, such as that of Nagarajan and Kolter\n[17], to alternating updates. However, the modified flows we propose here are not\nwithout limitations: they ignore discretisation errors smaller than O(h3), and thus\nthey are not equivalent to the discrete updates. We have seen in Chapter 3 how in\nsupervised learning higher order terms can be crucial in determining instabilities of\ngradient descent; we will briefly investigate the effect of these higher order terms\n4.4. Common-payoff games\n92\nin games in Section 4.8. To fully account for DD, methods that directly assess the\nconvergence of discrete updates (e.g. Mescheder et al. [15]) remain an indispensable\ntool for understanding discrete systems.\n4.4\nCommon-payoff games\nWhen the players share a common loss, as in common-payoff games, we recover\nsupervised learning with a single loss E, but with the extra-freedom of training\nthe weights corresponding to different parts of the model with possibly different\nlearning rates and update strategies (see for instance You et al. [181] where a per-layer\nlearning rate is used to obtain extreme training speed-ups at equal levels of test\naccuracy). A special case occurs when two players with equal learning rates (υϕ = υθ)\nperform simultaneous gradient descent. In this case, the modified losses recover the\nImplicit Gradient Regularisation flow in Eq (2.36). Barrett and Dherin [12] argue\nthat minimising the loss-gradient norm, in this case, has a beneficial effect.\nIn this section, we instead focus on alternating gradient descent. We partition a\nneural network into two sets of parameters, ϕ for the parameters closer to the input\nand θ for the parameters closer to the output. This procedure freezes one part of the\nnetwork while training the other part and alternating between the two parts. This\nscenario may arise in a distributed training setting, as a form of block coordinate\ndescent. In the case of common-payoff games, we have the following as a special case\nof Theorem 4.2.2 by substituting f = −∇ϕE and g = −∇θE:\nCorollary 4.4.1 In a two-player common-payoff game with common loss E, alter-\nnating gradient descent—as described in Eqs (4.17) and (4.18)—with one update per\nplayer follows a gradient flow given by the modified losses\n˜Eϕ = E + υϕh\n4\n\u0000∥∇ϕE∥2 + ∥∇θE∥2\u0001\n(4.29)\n˜Eθ = E + υθh\n4\n\u0012\n(1 −2υϕ\nυθ\n) ∥∇ϕE∥2 + ∥∇θE∥2\n\u0013\n(4.30)\nwith an error of size O(h3) after one update step.\nThe term (1 −2υϕ\nυθ ) ∥∇ϕE∥2 in Eq. (4.30) is negative when the learning rates are\n4.5. Analysis of zero-sum games\n93\n0\n200\n400\n600\n800\n1000\nIteration\n10−2\n10−1\n100\n101\n102\n||∇ϕE||2\nSimultaneous\nAlternating\n(a) Gradient norm of the first player.\n0\n200\n400\n600\n800\n1000\nIteration\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nSimultaneous\nAlternating\n(b) Accuracy.\nFigure 4.4: In common-payoff games alternating updates lead to higher gradient norms\nand unstable training when equal learning rates υϕh = υθh are used. As\npredicted by Corollary 4.4.1, the maximisation of the first player’s gradient\nnorm ∥∇ϕE∥by the second player’s modified loss in alternating updates leads\nto an increased gradient norm for the first player compared to simultaneous\nupdates (a), which also results to loss instability and decrease in performance\n(b). The common-payoff objective here is classification of MNIST digits.\nequal, seeking to maximise the gradient norm of player ϕ. According to Barrett and\nDherin [12], we expect less stable training and worse performance in this case. This\nprediction is confirmed in Figure 4.4, where we compare simultaneous and alternating\ngradient descent for a MLP trained on MNIST with a common learning rate.\n4.5\nAnalysis of zero-sum games\nWe now study zero-sum games, where the one player’s gain is another player’s loss:\n−Eϕ = Eθ = E. We substitute the updates f = ∇ϕE and g = −∇θE in the\nTheorems in Section 4.2 and obtain:\nCorollary 4.5.1 In a zero-sum two-player differentiable game, simultaneous gradi-\nent descent updates—as described in Eqs (4.13) and (4.14)—follows a gradient flow\ngiven by the modified losses\n˜Eϕ = −E + υϕh\n4 ∥∇ϕE∥2 −υϕh\n4 ∥∇θE∥2\n(4.31)\n˜Eθ = E −υθh\n4 ∥∇ϕE∥2 + υθh\n4 ∥∇θE∥2 ,\n(4.32)\nwith an error of size O(h3) after one update step.\n4.5. Analysis of zero-sum games\n94\nCorollary 4.5.1 shows that the modified losses for simultaneous gradient descent\npreserve the adversarial structure of the game, with the interaction terms maximising\nthe gradient norm of the opposite player. We note, however, that the modified losses\nof zero-sum games trained with simultaneous gradient descent with different learning\nrates are no longer zero-sum, due to the different implicit regularisation coefficients\nintroduced by DD.\nCorollary 4.5.2 In a zero-sum two-player differentiable game, alternating gradient\ndescent—as described in Eqs (4.17) and (4.18)—follows a gradient flow given by the\nmodified losses\n˜Eϕ = −E + υϕh\n4m ∥∇ϕE∥2 −υϕh\n4 ∥∇θE∥2\n(4.33)\n˜Eθ = E −υθh\n4 (1 −2υϕ\nυθ\n) ∥∇ϕE∥2 + υθh\n4k ∥∇θE∥2\n(4.34)\nwith an error of size O(h3) after one update step.\nCorollary 4.5.2 shows that the modified losses of zero-sum games trained with\nalternating gradient descent are not zero-sum.\nRemark 4.5.1 Since (1−2υϕ\nυθ ) < 1 in the alternating case there is always less weight\non the term encouraging maximizing ∥∇ϕE∥2 compared to the simultaneous case\nunder the same learning rates. For υϕ\nυθ > 1\n2 both players minimise ∥∇ϕE∥2.\nCorollaries 4.32 and 4.34 show that in zero-sum games interaction terms can\ninduce a pressure to maximise the gradient norm of the other player. We postulate\nthat this maximisation effect can be a source of instability, something we will study\nthroughout this chapter.\n4.5.1\nDirac-GAN: an illustrative example\nMescheder et al. [68] introduce the Dirac-GAN as an example to illustrate the often\ncomplex training dynamics in zero-sum games. We follow this example to provide an\nintuitive understanding of DD. Dirac-GAN aims to learn a Dirac delta distribution\nwith mass at zero; the generator is modelling a Dirac with parameter θ: G(z; θ) = θ\n4.5. Analysis of zero-sum games\n95\nand the discriminator is a linear model on the input with parameter ϕ: D(x; ϕ) = ϕx.\nThis results in the zero-sum game given by\nE = l(θϕ) + l(0),\n(4.35)\nwhere l depends on the GAN formulation used (l(z) = −log(1 + e−z) for instance).\nAs in Mescheder et al. [68], we assume l is continuously differentiable with l′(x) ̸= 0\nfor all x ∈R. The partial derivatives of the loss function\n∂E\n∂ϕ = l′(θϕ)θ,\n∂E\n∂θ = l′(θϕ)ϕ,\n(4.36)\nlead to the underlying continuous dynamics\n˙ϕ = f(θ, ϕ) = l′(θϕ)θ,\n˙θ = g(θ, ϕ) = −l′(θϕ)ϕ.\n(4.37)\nThe unique equilibrium point is θ = ϕ = 0.\nReconciling discrete and continuous updates in Dirac-GAN. The original\ncontinuous dynamics in Eq (4.37) preserve θ2 + ϕ2, since\nd (θ2 + ϕ2)\ndt\n= 2θdθ\ndt + 2ϕdϕ\ndt = −2θl′(θϕ)ϕ + 2ϕl′(θϕ)θ = 0.\n(4.38)\nMescheder et al. [68] show, however, that for simultaneous gradient descent θ2 + ϕ2\nincreases with each update: different conclusions are reached when analysing the\ndynamics of the original continuous system versus the discrete updates. We show\nthat, by using the modified flows given by Eqs (4.31) and (4.32) instead of the original\ngame dynamics, we can resolve this discrepancy. When following the modified flow\ninduced by simultaneous gradient descent in DiracGAN, we have that (proof in\nSection B.6.1 in the Appendix), unless we start at the equilibrium θ = ϕ = 0,\nd (θ2 + ϕ2)\ndt\n= hθ2l′(ϕ, θ)2 + hϕ2l′(ϕ, θ2) > 0.\n(4.39)\nWe thus obtain consistent conclusions from the modified continuous flows and the\n4.5. Analysis of zero-sum games\n96\nDirac GAN\nModified flow\nSimultaneous gradient descent\n(a) Reconciling discrete and continuous dynamics.\nDirac GAN\nSimultaneous GD + Explicit regularization\nSimultaneous gradient descent\n(b) Explicit regularisation leads to convergence.\nFigure 4.5: DiracGAN. (a): The dynamics of simultaneous gradient descent updates\nmatch the continuous dynamics given by BEA for DiracGAN. (b): Explicit\nregularisation cancelling the interaction terms of DD stabilises the DiracGAN\ntrained with simultaneous gradient descent.\ndiscrete dynamics used in practice; we visualise this in Figure 4.5a.\nExplicit regularisation stabilises Dirac-GAN. We show that the instability\nof simultaneous updates in DiracGAN can be seen as a result of the effect of interaction\nterms in zero-sum games, as we postulated as a result of Corollary 4.32. We counteract\nthe norm maximisation effect of the interaction terms in simultaneous updates by\ncancelling the interaction terms using explicit regularisation: Eϕ = −E + γ∥∇θE∥2\nand Eθ = E + ζ∥∇ϕE∥2 where γ, ζ are of O(h). We find (proofs are provided in\nSection B.6.3 of the Appendix) that the modified Jacobian of simultaneous updates\ninduced by these new losses is negative definite if γ > hυϕ/4 and ζ > hυθ/4—\nwhich are exactly the coefficients required to cancel the interaction terms shown in\nCorollary 4.32—so the system is asymptotically stable and converges to the optimum.\nUnlike the modified flow of the original zero-sum dynamics, which diverge as we have\nseen in Eq 4.39, the flow of the system induced by cancelling the interaction terms\nconverges. We visualise this behaviour in Figure 4.5b. Notably, by quantifying DD,\nwe are able to find the regularisation coefficients that guarantee convergence and\nshow that they depend on learning rates.\n4.6. Experimental analysis of GANs\n97\n4.6\nExperimental analysis of GANs\nTo understand the effect of DD on more complex adversarial games, we analyse GANs\ntrained for image generation on the CIFAR-10 dataset. We follow the model architec-\ntures from Spectral-Normalised GAN [30]. Both players have millions of parameters.\nUnlike Spectral-Normalised GAN, we employ the original GAN formulation, as\ndescribed in Eq (4.4).\nWhen it comes to gradient descent, GAN practitioners often use alternating, not\nsimultaneous updates: the discriminator is updated first, followed by the generator;\nwe described alternating updates and simultaneous updates in Algorithms 1 and 2,\nrespectively. Recent work, however, shows higher-order numerical integrators can\nwork well with simultaneous updates [67]. We will show that DD can be seen as the\nculprit behind some of the challenges in simultaneous gradient descent in zero-sum\nGANs, indicating ways to improve training performance in this setting. For a clear\npresentation of the effects of DD, we employ a minimalist training setup. Instead of\nusing popular adaptive optimisers, such as Adam [47], we train all models with vanilla\nstochastic gradient descent, without momentum or variance reduction methods.\nWe use the Inception Score (IS) [177] for evaluation, and we report Fr´echet\nInception Distance (FID) results [173] in the Appendix; we described the computation\nof the Inception Score in Section 4.1. Our training curves contain a horizontal line at\nthe Inception Score of 7.5, obtained with the same architectures we use, but with the\nAdam optimiser (the score reported by Miyato et al. [30] is 7.42). We report box plots\nshowing the performance quantiles across all hyperparameter and seeds, together with\nlearning curves corresponding to the best 10% models from a sweep over learning rates\nand 5 seeds. This is due to the large variability observed across hyperparameters and\nseeds, but we observe the same relative results with the top 20% and 30% of models,\nas we show in the Appendix Section B.9, which also has additional discussions on\nrobustness. For SGD we use learning rates {5×10−2, 1×10−2, 5×10−3, 1×10−3} for\neach player; for Adam, we use learning rates {1 × 10−4, 2 × 10−4, 3 × 10−4, 4 × 10−4},\nwhich have been widely used in the literature [30]. When comparing to Runge–\nKutta (RK4) to assess the effect of DD, we always use the same learning rates\n4.6. Experimental analysis of GANs\n98\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nSimultaneous\nAlternating\n(a) A sweep over a range learning rates.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nZero sum\nEuler (sim)\nEuler (alt)\nRK4\n(b) Equal learning rates.\nFigure 4.6: The effect of DD on zero-sum games. Over a sweep of learning rates, al-\nternating updates perform better than simultaneous updates (a), and show\nlower variance. When using equal learning rates RK4 has O(h5) drift, and\nthus we use it as a baseline to understand the effect of the drift in (b); we\nobserve that reducing drift leads to improved performance. These results\nshow that DD has a strong effect on these games, with a substantial differ-\nence in performance between simultaneous and alternating updates, and an\nincreased speed early in training when using RK4. While the modified flows\nwe study only capture DD up to O(h3), these results are consistent with\nwhat we expect from Corollaries 4.5.1 and 4.5.2, which show that the drift of\nthe first player leads to an incentive to maximise the second player’s gradient\nnorm; this can lead to instability compared to methods with reduced drift\nsuch as RK4 as we have seen in (b). Similarity when comparing alternating\nand simultaneous updates, we expect less instability for alternating updates\nas the second player does not always have the incentive to maximise the first\nplayer’s gradient norm, which is always the case for simultaneous updates;\nthis is consistent with what we observe in (a).\nfor both players. We present results using additional losses, via LS-GAN [38], and\nexperimental details in the Appendix. The code associated with this work can be\nfound at https://github.com/deepmind/discretisation_drift.\n4.6.1\nDoes DD affect training?\nWe start our experimental analysis by showing the effect of DD on zero-sum games.\nWe compare simultaneous gradient descent, alternating gradient descent, and Runge–\nKutta 4 updates, since they follow different continuous dynamics given by the\nmodified equations we have derived up to O(h3) error. Figure 4.6 shows simultaneous\ngradient descent performs substantially worse than alternating gradient descent.\nWhen compared to Runge–Kutta 4, which has a DD error of O(h5) when the two\n4.6. Experimental analysis of GANs\n99\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nLearning rate ratio\n0.1\n0.2\n0.5\n1.0\n2.0\n5.0\n(a) Simultaneous updates.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nLearning rate ratio\n0.1\n0.2\n0.5\n1.0\n2.0\n5.0\n(b) Alternating updates.\nFigure 4.7: Alternating gradient descent (b) performs better for learning rate ratios which\nreduce the adversarial nature of DD, namely learning rates where υϕ/υθ ≥0.5\nand thus the second player’s (the generator) modified loss minimises, instead\nof maximises the discriminator’s gradient norm. The same learning rate\nratios show no advantage in the simultaneous case (a).\nplayers have equal learning rates, we see that Runge–Kutta performs better, and that\nremoving the drift improves training. Multiple updates also affect training, either\npositively or negatively, depending on learning rates—see Appendix Section B.3.\n4.6.2\nThe importance of learning rates in DD\nWhile in simultaneous updates (Eqs (4.31) and (4.32)) the interaction terms of\nboth players maximise the gradient norm of the other player, alternating gradient\ndescent (Eqs (4.33) and (4.34)) exhibits less pressure on the second player (generator)\nto maximise the norm of the first player (discriminator). In alternating updates,\nwhen the ratio between the discriminator and generator learning rates exceeds 0.5,\nboth players are encouraged to minimise the discriminator’s gradient norm. To\nunderstand the effect of learning rate ratios in training, we performed a sweep where\nthe discriminator learning rates υϕ are sampled uniformly between [0.001, 0.01], and\nthe learning rate ratios υϕ/υθ are in {0.1, 0.2, 0.5, 1, 2, 5}, with results shown in\nFigure 4.7. Learning rate ratios greater than 0.5 perform best for alternating updates\nand enjoy a substantial increase in performance compared to simultaneous updates,\nwhich is consistent with our expectations based on the sign of the interaction terms.\n4.6. Experimental analysis of GANs\n100\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms\nAdam\nSGD\nCancel interaction terms\nAdam\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nFigure 4.8: Simultaneous updates in zero-sum games: Explicit regularisation cancelling\nthe interaction terms of DD improves performance and stability, both for the\nbest performing models (a) and across a learning rate sweep (b).\n4.6.3\nImproving performance by explicit regularisation\nWe have postulated that the interaction terms can hinder performance and stability\nin zero-sum games, as they can lead to an incentive to maximise the gradient norm\nof one, or both players. Now, we investigate whether cancelling the interaction terms\nbetween the two players can improve training stability and performance in zero-sum\ngames trained using simultaneous gradient descent. We train models using the losses\nEϕ = −E + c1 ∥∇θE∥2\n(4.40)\nEθ = E + c2 ∥∇ϕE∥2 .\n(4.41)\nIf c1, c2 are O(h) we can ignore the DD from these regularisation terms, since\ntheir effect on DD will be of order O(h3). We can set coefficients to be the negative\nof the coefficients present in DD, namely c1 = υϕh/4 and c2 = υθh/4 to cancel the\ninteraction terms, which maximise the gradient norm of the other player, while\nkeeping the self terms, which minimise the player’s own gradient norm. We show\nresults in Figure 4.8: cancelling the interaction terms leads to substantial improvement\ncompared to SGD, obtains the same peak performance as Adam (though requires\nmore training iterations) and recovers the performance of Runge–Kutta 4 (Figure 4.6).\nUnlike Adam, we do not observe a decrease in performance later in training but\nreport higher variance in performance across seeds—see Figure 4.8b and additional\n4.6. Experimental analysis of GANs\n101\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nCancel interaction terms\nSGD\nSGA Coeffs 0.0001\nSGA Coeffs 0.001\nSGA Coeffs 0.01\n(a) SGA comparison.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nCancel interaction terms\nSGD\nCO Coeffs 0.01\nCO Coeffs 0.001\nCO Coeffs 0.0001\nDD coeffs\n(b) CO comparison.\nFigure 4.9: Comparison with Symplectic Gradient Adjustment (SGA) and Consensus\nOptimisation (CO) in simultaneous gradient descent for zero-sum games:\nDD motivates the form of explicit regularisation and provides does not\nrequire a larger hyperparameter sweep for the regularisation coefficient, as the\nexplicit regularisation coefficients are determined by the player’s learning rates.\nCancelling the interaction terms leads to improved performance compared to\nboth SGA and CO.\nexperiments in Appendix Section B.9.\nConnection with Symplectic Gradient Adjustment (SGA): Balduzzi et al. [61]\nproposed SGA to improve the dynamics of gradient-based method for games, by\ncounter-acting the rotational force of the vector field (see Section B.5 for a discussion).\nAdjusting the gradient field can be viewed as modifying the losses as in Eq (4.41);\nthe modification from SGA cancels the interaction terms we identified. However, it is\nunclear whether the fixed coefficients of c1 = c2 = 1\n2 used by SGA are always optimal,\nwhile our analysis shows the strength of DD changes with the exact discretisation\nscheme, such as the step size. Indeed, as our experimental results in Figure 4.9a\nshow, adjusting the coefficient in SGA strongly affects training, and that cancelling\nthe interaction terms outperforms SGA.\nConnection with Consensus Optimisation (CO): Mescheder et al. [15] analyse\nthe discrete dynamics of gradient descent in zero-sum games and prove that, under\ncertain assumptions, adding explicit regularisation that encourages the players to\nminimise the gradient norm of both players guarantees convergence to a local Nash\nequilibrium. Their approach includes cancelling the interaction terms, but also\n4.6. Experimental analysis of GANs\n102\nrequires strengthening the self terms, using losses:\nEϕ = −E + c1 ∥∇θE∥2 + s1 ∥∇ϕE∥2\n(4.42)\nEθ = E + s2 ∥∇θE∥2 + c2 ∥∇ϕE∥2 ,\n(4.43)\nwhere they use s1 = s2 = c1 = c2 = γ where γ is a hyperparameter. In order to\nunderstand the effect of the self and interaction terms, we compare to CO, as well\nas a similar approach where we use coefficients proportional to the drift, namely\ns1 = υϕh/4 and s2 = υθh/4; this effectively doubles the strength of the self terms\nin DD. We show results in Figure 4.9b.\nWe first notice that CO can improve\nresults over vanilla SGD. However, similarly to what we observed with SGA, the\nregularisation coefficient is important and thus requires a hyperparameter sweep,\nunlike our approach, which uses the coefficients provided by the DD. We further\nnotice that strengthening the norms using the DD coefficients can improve training,\nbut performs worse compared to only cancelling the interaction terms. This shows\nthe importance of finding the right training regime, and that strengthening the self\nterms does not always improve performance.\nAlternating updates: We perform the same exercise for alternating updates, where\nc1 = υϕh/4 and c2 = υθh/4(1−2υϕ\nυθ ). We also study the performance obtained by only\ncancelling the discriminator interaction term, since when υϕ/υθ > 0.5 the generator\ninteraction term minimises, rather than maximises, the discriminator gradient norm\nand thus the generator interaction term might not have a strong destabilising force.\nWe observe that adding explicit regularisation only brings the benefit of reduced\nvariance when cancelling the discriminator interaction term (Figure 4.10). As for\nsimultaneous updates, we find that knowing the form of DD guides us to a choice\nof explicit regularisation: for alternating updates cancelling both interaction terms\ncan hurt training, but the form of the modified losses suggests that we should only\ncancel the discriminator interaction term, with which we can obtain some gains.\nDoes cancelling the interaction terms help for every choice of learning rates?\nThe substantial performance and stability improvement we observe applies to the\nperformance obtained across a learning rate sweep. For individual learning rate\n4.6. Experimental analysis of GANs\n103\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nSGD\nCancel interaction terms\nCancel disc interaction term\nSGD\nCancel interaction \nterms\nCancel disc \nInteraction term\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nFigure 4.10: Alternating updates in zero-sum games: the form of DD guides us in finding\nexplicit regularisation terms. Since for alternating updates the generator’s\ninteraction term can be beneficial by minimising the discriminator’s gradient\nnorm, cancelling only the discriminator interaction term improves sensitivity\nacross hyperparameters (b).\nchoices, cancelling the interaction terms is not guaranteed to improve learning.\n4.6.4\nExtension to non-zero-sum GANs\nFinally, we extend our analysis to GANs with the non-saturating loss for the generator\nEθ = −log D(G(z; θ); ϕ)\n(4.44)\nintroduced by Goodfellow et al. [32], while keeping the discriminator loss unchanged\nas that from the zero-sum formulation in Eq (4.4). The non-saturating loss has\nbeen observed to perform better than its zero-sum counterpart, and has often been\nused in practice. In contrast with the dynamics from zero-sum GANs we analysed\nearlier, changing from simultaneous to alternating updates results in little change\nin performance, as can be seen in Figure 4.11. Despite having the same adversarial\nstructure and the same discriminator loss, changing the generator loss changes the\nrelative performance of the different discrete update schemes.\nSince the effect of DD strongly depends on the game, we recommend analysing\nthe performance of discrete numerical schemes on a case by case basis. Indeed,\nwhile for general two-player games we cannot always write modified losses as for\nthe zero-sum case—see the Section 5.3 for a discussion—we can use Theorems 4.2.1\n4.7. A comment on different learning rates\n104\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nNon saturating loss\nSimultaneous\nAlternating\n(a) A sweep over a range learning rates.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nNon saturating loss\nEuler (sim)\nEuler (alt)\nRK4\n(b) Equal learning rates.\nFigure 4.11: The effect of DD depends on the game: its effect is less strong for the\nnon-saturating loss. Since when using the non-saturating loss GAN losses\nare no longer zero-sum, estimating the impact of the drift becomes more\nchallenging, since we cannot easily write modified losses (though we provide\nnovel insights into this problem in the next chapter, Chapter 5). When\nusing equal learning rates RK4 has O(h5) drift, and thus we use it as a\nbaseline to understand the effect of the drift in (b).\nand 4.2.2 to understand the effect of the drift for specific choices of loss functions.\nWe will tackle the question of modified losses for generally differentiable two-player\ngames in the Chapter 5, and further contrast the non-saturating and the saturating\ngenerator losses in GANs with Spectral Normalisation in Chapter 7.\n4.7\nA comment on different learning rates\nWhen considering different learning rates for the two players, we have thus far kept\nthe consistency between physical time and learning rates. That is, we assumed the\ngradient descent updates are obtained by Euler discretisation with learning rates\nυϕh and υθh of the flow\n˙ϕ = f(ϕ, θ)\n(4.45)\n˙θ = g(ϕ, θ).\n(4.46)\nWe then used BEA to fine modified flow such that after time υϕh and υθh the error\nbetween the modified flow and the discrete updates is O(h3).\nAlternatively, we can consider the gradient descent updates as the result of\n4.7. A comment on different learning rates\n105\nEuler discretisation with learning rate h of the flow\n˙ϕ = υϕf(ϕ, θ)\n(4.47)\n˙θ = υθg(ϕ, θ).\n(4.48)\nWhile in this approach the learning rate loses the connection with physical time for\neach player, it keeps the consistency of time between two players: after one iteration\nthe same amount of time, h, has passed for both players. With this in mind, we will\ncall this ‘the same-physical time between players’ approach.\nConsidering gradient descent as a discretisation of Eqs (4.47) and (4.48) instead\nof Eqs (4.45) and (4.46) has implications for the analysis of the behaviour of gradient\ndescent using continuous-time tools such as stability analysis, as the two systems can\nlead to different results based on the two learning rates. Using Eqs (4.47) and (4.48)\ncan also lead to a different perspective from the loss minimisation view. For the\ngame with f = −∇ϕE and g = ∇θE, the second system leads to the interpretation\nthat the first player is minimising function υϕE(ϕ, θ) while the second player is\nmaximising function υθE(ϕ, θ). This is different than the zero-sum formulation of\nthe first player minimising E(ϕ, θ) while the second player is maximising it, resulting\nfrom our previous interpretation. The same-physical time between players approach\nalso presents the following challenge:\nRemark 4.7.1 (The identifiability problem.) Given learning rates υϕh and υθh\nfor the two-players, h cannot be identified exactly and has to be chosen by the user.\nThe choice of h leads to different flows describing the dynamics of the discrete updates.\nThis is in contrast with the approach taken previously, which only depends on υϕ and\nυθ through the learning rates υϕh and υθh.\nFollowing the same-physical time between players approach also has implications\nfor the use of BEA to find flows with error O(h3) after one Euler update. Luckily, to\nfind the modified flows using BEA under the same-physical time approach, we simply\nhave to apply our results from Section 4.2 for the system in Eqs (4.47) and (4.48) by\nusing their corresponding vector fields and learning rate h. We do so, and obtain:\n4.7. A comment on different learning rates\n106\nTheorem 4.7.1 The discrete simultaneous Euler updates in Eqs (4.13) and (4.14)\nfollow the continuous system\n˙ϕ = υϕf −h\n2\n\u0012\nυϕ\n2 df\ndϕf + υϕυθ\ndf\ndθg\n\u0013\n(4.49)\n˙θ = υθg −h\n2\n\u0012\nυϕυθ\ndg\ndϕf + υθ\n2 dg\ndθg\n\u0013\n(4.50)\nwith an error of size O(h3) after one update step.\nTheorem 4.7.2 The discrete alternating Euler updates in (4.17) and (4.18) follow\nthe continuous system\n˙ϕ = υϕf −h\n2\n\u0012υϕ2\nm\ndf\ndϕf + υϕυθ\ndf\ndθg\n\u0013\n(4.51)\n˙θ = υθg −h\n2\n\u0012\n−υθυϕ\ndg\ndϕf + υθ2\nk\ndg\ndθg\n\u0013\n(4.52)\nwith an error of size O(h3) after one update step.\nWe observe that both modified flows each player’s vector field depends on the\nlearning rate of the other player, unlike in our previous results where for simultaneous\nupdates, the vector field for each player depended only on its learning rate. When\ninterpreting these results to obtain implicit regularisation effects as we have done\npreviously in zero-sum and common-payoff games, the resulting coefficients of the\nimplicit regularisers are different than those obtained in Section 4.5. For zero-sum\ngames, we obtain the following corollaries:\nCorollary 4.7.1 In a zero-sum two-player differentiable game, simultaneous gradi-\nent descent updates—as described in Eqs (4.13) and (4.14)—follows a gradient flow\ngiven by the modified losses\n˜Eϕ = −υϕE + υϕ2h\n4\n∥∇ϕE∥2 −υϕυθh\n4\n∥∇θE∥2\n(4.53)\n˜Eθ = υθE −υϕυθh\n4\n∥∇ϕE∥2 + υθ2h\n4\n∥∇θE∥2\n(4.54)\nwith an error of size O(h3) after one update step.\n4.7. A comment on different learning rates\n107\nCorollary 4.7.2 In a zero-sum two-player differentiable game, alternating gradient\ndescent—as described in Eqs (4.17) and (4.18)—follows a gradient flow given by the\nmodified losses\n˜Eϕ = −υϕE + υϕ2h\n4m ∥∇ϕE∥2 −υϕυθh\n4\n∥∇θE∥2\n(4.55)\n˜Eθ = υθE + υϕυθh\n4\n∥∇ϕE∥2 + υθ2h\n4k ∥∇θE∥2\n(4.56)\nwith an error of size O(h3) after one update step.\nFrom Corollary 4.7.2 one concludes for alternating updates that the second\nplayer’s interaction always minimises the gradient norm of the first player, which\nagain helps explain the empirically observed stability of alternating updates over\nsimultaneous updates; this is in contrast with our previous interpretation which\nsuggested that this occurs only for certain learning rate ratios (Corollary 4.5.2).\nIf we would like to implement explicit regularisation methods to cancel the\nimplicit regularisers obtained above using the same-physical time between players\napproach, we have to discretise the modified losses with learning rate h; this is\nopposed to using learning rates υϕh and υθh to discretise the modified flows obtained\nin Section 4.5. For the self terms, no difference is obtained compared to the previous\ninterpretation; under both approaches of tackling different learning rates, the strength\nof the self terms in the parameters update is proportional to υϕ2h2 and υθ2h2,\nrespectively. For interaction terms, however, the coefficients required in the parameter\nupdate are changed. We highlighted these changes in Table 4.1; these coefficients\nalso show that while in order to find the modified flow in the same-physical time\nbetween player approach we require to choose υϕ, υθ and h, for explicit regularisation\nwe only need the learning rates of the two players.\nWe thus can investigate the effect of explicit regularisation to cancel the interac-\ntion terms under the same-physical time approach in zero-sum games. As before,\nwe do so by using the generator saturating loss in GAN training. We show results\nin Figures 4.12 and 4.13 and compare with our previous results from Section 4.6.3.\nWhile for the simultaneous updates using the same physical time approach leads\n4.7. A comment on different learning rates\n108\nSimultaneous updates\nFirst player (ϕ)\nSecond player (θ)\nCorollary 4.5.1\nυϕ2h2\n4\n∇ϕ ∥∇θE∥2\nυθ2h2\n4\n∇θ ∥∇ϕE∥2\nCorollary 4.7.1\nυϕυθh2\n4\n∇ϕ ∥∇θE∥2\nυϕυθh2\n4\n∇θ ∥∇ϕE∥2\nAlternating updates\nFirst player (ϕ)\nSecond player (θ)\nCorollary 4.5.2\nυϕ2h2\n4\n∇ϕ ∥∇θE∥2\n\u0010\nυθ2h\n4\n−2υϕυθh2\n4\n\u0011\n∇θ ∥∇ϕE∥2\nCorollary 4.7.2\nυϕυθh2\n4\n∇ϕ ∥∇θE∥2\n−υϕυθh2\n4\n∇θ ∥∇ϕE∥2\nTable 4.1: The strength of the interaction terms in DD under the different modified\ncontinuous-time flows we find for simultaneous and alternating gradient descent\nin zero-sum games. We obtained the different flows via different approaches of\nhandling different learning rates for the two players, namely υϕh ̸= υθh. We do\nnot write the self terms here as they are the same for both approaches. We write\nthe implicit regularisation coefficients as observed in the continuous-time flow\ndisplacement for one gradient descent update, rather than the modified loss, in\norder to account for the different learning rates under the two interpretations\n(which are υϕh and h for the first player, respectively). For simultaneous\nupdates, the insight that the interaction terms for each player leads to a\nmaximisation of the other player’s gradient norm remains true regardless of\ninterpretation, though the strength of the regularisation changes in the two\ninterpretations. For alternating updates, the different interpretations suggest\nnot only different regularisation strengths but also signs, with the result from\nCorollary 4.7.2 showing that for alternating updates, the interaction term\nof the second player always minimises the gradient norm of the first player,\nas opposed to the result in Corollary 4.5.2, for which this effect depends on\nthe learning rate ratio υϕ/υθ. Both results are consistent in explaining why\nalternating updates are more stable than simultaneous updates, since in both\ncases for alternating updates the second player’s interaction term has a smaller\ncoefficient when maximising the first player’s gradient norm compared to that\nof simultaneous updates, or is minimising the first player’s gradient norm.\nto more hyperparameters with increased performance, no significant difference is\nobtained for alternating updates.\nWe conclude by noting that when studying two-player games trained with\ndifferent learning rates, one can choose between two approaches of constructing\ncorresponding continuous dynamics and modified losses. Both are valid, in that\nboth lead to modified flows with the same error in learning rate, and can be used to\nunderstand implicit regularisation effects and the discrepancies between simultaneous\nand alternating Euler updates. Additional examination is needed to understand how\nthese approaches explain continuous-time behaviour over a larger number of steps;\nwe leave that as future work.\n4.8. Extending the PF for two-player games\n109\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nCancel interaction terms\nSGD\nAdam\nCancel interaction \nterms (same physical time)\nCancel interaction terms\nSGD\nAdam\nCancel interaction \nterms (same physical time)\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nFigure 4.12: Simultaneous updates in zero-sum games: cancelling the interaction terms\nunder the different interpretations of tackling different learning rates (Corol-\nlaries 4.5.1 and 4.7.1) does not result in a significant empirical difference.\nWe denote the approach of Corollary 4.7.1 as ‘same physical time’, as per\nits motivation of using the same physical time between the two players.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nSGD\nCancel disc Interaction term\nCancel disc Interaction \nterm (same physical time)\nSGD\nCancel disc \nInteraction term\nCancel disc Interaction \nterm (same phys time)\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nFigure 4.13: Alternating updates in zero-sum games: cancelling the interaction terms\nunder the different interpretations of tackling different learning rates (Corol-\nlaries 4.5.2 and 4.7.2) does not result in a significant empirical difference.\nWe denote the approach of Corollary 4.7.2 as ‘same physical time’, as per\nits motivation of using the same physical time between the two players.\n4.8\nExtending the PF for two-player games\nIn Chapter 3, we introduced the principal flow (PF) as a model of single-objective\ngradient descent. We now briefly show how the same approach can be generalised\nto simultaneous Euler updates in two-player games, which will allow us to capture\ndiscretisation drift effects beyond those of order O(h3) we explored in the rest of\nthis chapter. For simplicity, we use assume equal learning rates for the two-players,\nthough one can use the ‘same-physical time approach’ from the previous section to\n4.8. Extending the PF for two-player games\n110\nadapt the results to different learning rates.\nThe approach we took in the single-objective setting had two steps: first, we\ndeveloped the principal series with BEA (Theorem 3.3.1) and second, we used the\nprincipal series given by BEA to find the PF based on the eigen-decomposition of the\nHessian (Corollary 3.1 and Theorem 3.3.2). The first step can be readily translated\nto games as it applies to any vector field, using Theorem A.1.2 in the Appendix,\nwhich Theorem 3.3.1 is a corollary of. For completeness, we reproduce Theorem A.1.2\nhere:\nTheorem 4.8.1 The modified flow given by BEA with an error of order O(hp+1) to\nthe Euler update ψt = ψt−1 + hu(ψt−1) has the form:\n˙ψ =\np\nX\nn=0\n(−1)n\nn + 1 ( du\ndψ)\nn\nu + C( du\nd2ψ)\n(4.57)\nwhere C( du\nd2ψ) denotes a class of functions defined as a sum of individual terms each\ncontaining higher than first order derivatives applied to u.\nTo use the above result in games, consider the two-player game with the original\nthe dynamics in Eqs (4.9)-(4.10):\n˙\n\nϕ\nθ\n\n=\n\nf\ng\n\n.\n(4.58)\nIf we define\nψ =\n\nϕ\nθ\n\n\nu(ψ) =\n\nf(ϕ, θ)\ng(ϕ, θ)\n\n\n(4.59)\nand the negative of the Jacobian as\nH(ϕ, θ) = −\n\n\ndf\ndϕ\ndf\ndθ\ndg\ndϕ\ndg\ndθ\n\n,\n(4.60)\nwe can replace this choice vector field and H in Theorem 4.8.1, and obtain:\n4.8. Extending the PF for two-player games\n111\nCorollary 4.1 The full series of BEA constructed from simultaneous Euler updates\n(Eqs 4.13 and 4.14) with learning rate h exactly is of the form\n˙\n\nϕ\nθ\n\n=\n∞\nX\np=0\n1\np + 1hpH(ϕ, θ)p\n\nf\ng\n\n+ C( df\nd2θ).\n(4.61)\nWe can no longer use the eigen-decomposition of H, as unlike in the single-objective\ncase (f = −∇ϕE and g = −∇θE) we examined in Chapter 3, H(ϕ, θ) need not be\nsymmetric and thus we can no longer conclude Corollary 3.1. We instead consider\nthe Jordan normal form of H\nH(ϕ, θ) = P−1JP,\n(4.62)\nwhere J is a block diagonal matrix. We then have\nH(ϕ, θ)p = P−1JpP,\n(4.63)\nleading to\n˙\n\nϕ\nθ\n\n=\n∞\nX\np=0\n−1\np + 1P−1(hJ)pP\n\n−f\n−g\n\n+ C( df\nd2θ),\n(4.64)\nwhere we use the negative of the vector field in order to obtain consistency with\nthe PF results in Chapter 3, and recover single-objective results if f = −∇ϕE and\ng = −∇θE. We are interested in finding a form for the series\n∞\nX\np=0\n−1\np + 1(hJ)p,\n(4.65)\nwhich if converges can be written as another block diagonal matrix,\n1\nh log(I −hJ)J−1.\n(4.66)\nThis result leads to the game equivalent of the PF:\n4.8. Extending the PF for two-player games\n112\nDefinition 4.8.1 When expanding the PF to simultaneous Euler updates (Eqs 4.13\nand 4.14) with learning rate h in two-player games, we obtain:\n˙\n\nϕ\nθ\n\n= 1\nhP−1 log(I −hJ)J−1P\n\n−f\n−g\n\n+ C( df\nd2θ),\n(4.67)\nwhere H(ϕ, θ) = P−1JP is the Jordan normal form of H.\nWhile the Jordan normal form of H is not unique due to permutations of the order of\nthe blocks in J, P−1 log(I −hJ)J−1P will be the same regardless of the block order.\n4.8.1\nStability analysis\nWe now perform stability analysis on the modified flow obtained in Eq (4.67), and\nhope it sheds light on the behaviour of Euler updates—and thus gradient descent—in\ngames. We start by estimating the eigenvalues of the flow’s Jacobian:\n1\nhP−1 log(I −hJ)P.\n(4.68)\nAs P is invertible, the Jacobian’s eigenvalues are the same as the eigenvalues of\n1\nh log(I −hJ).\n(4.69)\nSince log(I −hJ) is block diagonal, its eigenvalues are the eigenvalues of its blocks\n1\nh log(Ink −hJk), where Ink is the identity matrix of dimension nk and Jk ∈Cnk,nk.\nEach block Jk corresponds to a unique eigenvalue λk of H with duplicity nk. Given\nZnk =\n\n\n0\n1\n. . .\n0\n0\n...\n...\n...\n...\n...\n0\n0\n0\n. . .\n1\n0\n0\n0\n0\n0\n\n\n,\n(4.70)\na matrix with all 0s apart from the one off diagonal, Jk can be written as\nJk = λkInk + Znk.\n(4.71)\n4.8. Extending the PF for two-player games\n113\nFrom here\nlog(I −hJk) = log(I −hλkInk −hZnk),\n(4.72)\nand since I −hJk is upper triangular, its eigenvalues are its diagonal entries, which\nare 1 −hλk and the eigenvalues of log(I −hJk) are log(1 −hλk). Thus, to perform\nstability analysis we have to assess whether ℜ[ 1\nh log(1 −hλk)] < 0 for all λk. Since\nh > 0, this is equivalent to ℜ[log(1 −hλk)] < 0 for all λk. Unlike the single-objective\ncase, λk might be complex. If we write λk = xk + iyk we have:\nℜ[log(1 −h(xk + iyk))] = log\nq\n(1 −hxk)2 + h2y2\nk,\n(4.73)\nwhich is negative if (1 −hxk)2 + (hyk)2 < 1.\nWe now contrast this condition to the exponential stability convergence condition\nof the original continuous-time game dynamics. Under the original continuous-time\ndynamics, for a fixed point to be attractive, the real part of the H’s eigenvalues\nwould need to be strictly positive since H is the negative of the flow’s Jacobian, i.e.\nxk > 0. When accounting for DD, however, the importance of the imaginary part yk\nbecomes apparent. Moreover, if we rewrite the condition (1 −hxk)2 + (hyk)2 < 1 as\na condition on the maximum learning rate to be convergent, we obtain\n(1 −hxk)2 + (hyk)2 < 1 =⇒h < 1\nxk\n2\n1 +\n\u0010\nyk\nxk\n\u00112,\n(4.74)\nwhich is the condition Mescheder et al. [15] (their Lemma 4) obtained from a\ndiscrete-time perspective. Thus in the game setting too, we recover existing fun-\ndamental results using a continuous-time perspective. We note that using this\nintuition Mescheder et al. [15] derive the algorithm Consensus Optimisation (CO),\nwhich we empirically assessed in Section 4.6.\nIf f = −∇ϕE and g = −∇θE we have a single objective, with yk = 0 ∀k. In\nthis case we recover the convergence conditions we are familiar with from the PF,\nwhich we derived in Section 3.3.2.\n4.8. Extending the PF for two-player games\n114\nEuler\nPF (games)\nDD flow\n(a) Same learning rate.\nNGF\nEuler\nPF (games)\n(b) Different learning rates.\nFigure 4.14: A visualisation of a simple example of the extension of the PF to two-games.\nHere we observe that the PF better tracks the gradient descent update\ncompared to the flow obtained from using the O(h3) error, and the NGF.\nIn the zero-sum case, where f = −∇ϕE and g = ∇θE, we have seen in Section\n2.4 that strict local Nash equilibria are local stable fixed points under the original\ngame dynamics. That is, at a strict local Nash equilibrium we have that xk > 0, ∀k.\nUnder the continuous-time dynamics we derived in this section, which account for DD,\nthis is no longer true, since xk being positive does not lead to (1−hxk)2 +(hyk)2 < 1.\nWe thus obtain an analogous result in zero-sum games to that of the single-objective\ncase in Chapter 3, where we saw that unlike the NGF, whether or not the PF is\nattracted to strict local minima depends on the learning rate h.\n4.8.2\nAn empirical example and different learning rates\nThe game PF describes the Euler discretisation of the system in Eq (4.23) exactly\n(see Section B.7 for a full proof). We visualise this example in Figure 4.14a, where\nwe observe that the PF tracks the behaviour of the discrete updates better than the\nmodified third-order correction previously derived. We note that the Jordan normal\nform (here implemented using ‘sympy’ [182]) is not always numerically stable and\nthe observed (small) difference between the discrete updates and the PF is due to\nnumerical issues.\nIf different learning rates are used for the two-players, we can use the approach\n4.9. Related work\n115\noutlined in Section 4.7 to construct vector fields that account for the each player’s\nlearning rate, and then apply the same analysis as above to the corresponding\nJacobian to construct a flow and stability analysis conditions. We use this approach\nto visualise the same example from Eq (4.23) in Figure 4.14b, and observe that here\ntoo the PF tracks the Euler updates closely.\n4.9\nRelated work\nBackward error analysis: There have been a number of recent works applying BEA\nto machine learning in the one-player case, which can potentially be extended to the\ntwo-players settings. In particular, Smith et al. [13] extended the analysis of Barrett\nand Dherin [12] to stochastic gradient descent. Kunin et al. [65] used modified\ngradient flow equations to show that discrete updates break certain conservation\nlaws present in the gradient flows of deep learning models. Fran¸ca et al. [60] compare\nmomentum and Nesterov acceleration using their modified equations. Fran¸ca et al.\n[158] used BEA to help devise optimisers with a control on their stability and\nconvergence rates. Li et al. [159] used modified equation techniques in the context of\nstochastic differential equations to devise optimisers with adaptive learning rates.\nOther recent works such as Unlu and Aitchison [183] and Jastrzebski et al. [184]\nnoticed the strong impact of implicit gradient regularisation in the training of over-\nparametrised models with SGD using different approaches. These works provide\nevidence that DD has a significant impact in deep learning, and that BEA is a\npowerful tool to quantify it. At last, note that a special case of Theorem 4.2.1 for\nzero-sum games with equal learning rates can be found in Lu [178].\nTwo-player games: As one of the best-known examples at the interface of game\ntheory and deep learning, GANs have been powered by gradient-based optimisers\nas other deep neural networks. The idea of an implicit regularisation effect induced\nby simultaneous gradient descent in GAN training was first discussed in Sch¨afer\net al. [185]; the authors show empirically that the implicit regularisation induced\nby gradient descent can have a positive effect on GAN training, and take a game\ntheoretic approach to strengthening it using competitive gradient descent [66]. By\n4.9. Related work\n116\nFirst player (ϕ)\nSecond player (θ)\nCancel DD\ninteraction terms (S)\nυϕh\n4 ∥∇θE∥2\nυθh\n4 ∥∇ϕE∥2\nCancel DD\ninteraction terms (A)\nυϕh\n4 ∥∇θE∥2\n−(2υϕ−υθ)h\n4\n∥∇ϕE∥2\nSGA (S)\n1\n2 ∥∇θE∥2\n1\n2 ∥∇ϕE∥2\nCO (S)\nζ ∥∇ϕE∥2 + ζ ∥∇θE∥2\nζ ∥∇ϕE∥2 + ζ ∥∇θE∥2\nLocally stable GAN (S)\nX\nζ ∥∇ϕE∥2\nODE-GAN (S)\nζ ∥∇θE∥2\nX\nTable 4.2: Comparing cancelling discretisation drift terms with explicit regularisation\nmethods in zero-sum games. SGA (without alignment, see the Appendix\nSection B.5 and Balduzzi et al. 61), Consensus Optimisation (CO) [15], Locally\nStable GAN [17], ODE-GAN [67]. We assume a minθ minϕ game with learning\nrates υϕh and υθh and number of updates m and k, respectively. ζ denotes the\nregualrisation coefficient hyperparameter in methods which require one. S and\nA denote simultaneous and alternating updates. Unlike other approaches, the\ndrift provides us with the coefficients to use to cancel the interaction terms and\nimprove performance, and tackles both alternating and simultaneous updates.\nquantifying the DD induced by gradient descent using BEA, we shed additional light\non the regularisation effect that discrete updates have on GAN training, and show it\nhas both beneficial and detrimental components (as also shown in Daskalakis and\nPanageas [180] with different methods in the case of simultaneous GD). Moreover, we\nshow that the explicit regularisation inspired by DD results in drastically improved\nperformance. The form of the modified losses we have derived are related to explicit\nregularisation for GANs, which has been one of the most efficient methods for\nstabilising GANs as well as other games. Some of the regularisers constrain the\ncomplexity of the players [30, 33, 36], while others modify the dynamics for better\nconvergence properties [17, 61, 67–70]. Our approach is orthogonal in that, we\nstart from understanding the most basic gradient steps, underpinning any further\nmodifications of the losses or gradient fields. Importantly, we discovered a relationship\nbetween learning rates and the underlying regularisation. Since merely cancelling the\neffect of DD is insufficient in practice (as also observed by Qin et al. [67]), our approach\ncomplements regularisers that are explicitly designed to improve convergence.\nWe leave to future research to further study other regularisers in combination\nwith our analysis. We summarise these regularisers in Table 4.2, and contrast them to\n4.10. Conclusion\n117\nour results which cancel the interaction terms of DD, as suggested by Corollaries 4.5.1\nand 4.5.2 respectively.\nTo our knowledge, this is the first work towards finding continuous systems\nthat better match the gradient descent updates used in two-player games. Studying\ndiscrete algorithms using continuous systems has a rich history in optimisation [186,\n187]. Recently, models that directly parametrise differential equations demonstrated\nadditional potential from bridging the discrete and continuous perspectives [94, 96].\n4.10\nConclusion\nWe have shown that using modified continuous systems to quantify the discretisation\ndrift (DD) induced by gradient descent updates can help bridge the gap between the\ndiscrete optimisation dynamics and those of continuous methods. This allowed us to\ncast a new light on the stability and performance of games trained using gradient\ndescent, and guided us towards explicit regularisation strategies inspired by these\nmodified systems. We note however that DD merely modifies a game’s original\ndynamics, and that the DD terms alone can not fully characterise a discrete scheme.\nIn this sense, our approach complements works analysing the underlying game [188].\nWe have focused our empirical analysis on a few classes of two-player games, but\nthe effect of DD will be relevant for all games trained using gradient descent. Our\nmethod can be expanded beyond gradient descent to other optimisation methods such\nas Adam [47], as well as to the stochastic setting as shown in Smith et al. [13]. We\nhope that our analysis provides a useful building block to further the understanding\nand performance of two-player games.\nChapter 5\nFinding new implicit regularisers\nby revisiting backward error\nanalysis\nIn previous chapters, we used Backward Error Analysis (BEA) to construct flows that\napproximate a discrete optimiser update and shed light on optimisation dynamics,\nboth for single-objectives and two-player games trained with gradient descent. The\ncurrent usage of BEA is not without limitations, however, since not all the vector fields\nof continuous-time flows obtained using BEA can be written as a gradient, hindering\nthe construction of modified losses revealing implicit regularisers. Implicit regularisers\nuncover quantities minimised by following discrete updates and thus can be used to\nimprove performance and stability across problem domains, from supervised learning\nto two-player games such as Generative Adversarial Networks [12, 13, 151, 185];\nwe have seen an example in common-payoff and zero-sum games in the previous\nchapter. In this chapter, we provide a novel approach to use BEA, and show how\nour approach can be used to construct continuous-time flows with vector fields that\ncan be written as gradients. We then use this to find previously unknown implicit\nregularisation effects, such as those induced by multiple stochastic gradient descent\nsteps while accounting for the exact data batches used in the updates, and in generally\ndifferentiable two-player games.\n5.1. Revisiting BEA\n119\n5.1\nRevisiting BEA\nIn Section 2.3.2, we described BEA and showed how Barrett and Dherin [12] use\nBEA to find the IGR flow\n˙θ = −∇θE −h\n2∇2\nθE∇θE = −∇θ\n\u0012\nE(θ) + h\n4 ∥∇θE(θ)∥2\n\u0013\n,\n(5.1)\nwhich has an error of O(h3) after one gradient descent update θt = θt−1 −h∇θE(θ).\nGradient descent can thus be seen as implicitly minimising the modified loss\nE(θ) + h\n4 ∥∇θE(θ)∥2. This showcases an implicit regularisation effect induced by the\ndiscretisation drift of gradient descent, dependent on learning rate h, which biases\nlearning towards paths with low gradient norms.\nWe would like to go beyond full-batch gradient descent and model the implicit\nregularisation induced by the discretisation of one stochastic gradient descent (SGD).\nGiven the SGD update θt = θt−1−h∇θE(θt−1; Xt)—where we denoted E(θt−1; X) =\n1\nB\nPB\ni=1 E(θt−1; xi) as the average loss over batch X and B is the batch size— we\ncan use the IGR flow induced at this time step:\n˙θ = −∇θE(θ; Xt) −h\n4∇θ ∥∇θE(θ; Xt)∥2 .\n(5.2)\nSince the vector field in Eq (5.2) is the negative gradient of the modified loss\nE(θ; Xt) + h\n4 ∥∇θE(θ; Xt)∥2, this reveals a local implicit regularisation which min-\nimises the gradient norm ∥∇θE(θ; Xt)∥2. It is not immediately clear, however, how\nto combine the IGR flows obtained for each SGD update in order to model the\ncombined effects of multiple SGD updates, each using a different batch. What are, if\nany, the implicit regularisation effects induced by two SGD steps\nθt = θt−1 −h∇θE(θt−1; Xt)\n(5.3)\nθt+1 = θt −h∇θE(θt; Xt+1)?\n(5.4)\nSmith et al. [13] find a modified flow in expectation over the shuffling of batches\nin an epoch, and use it to find implicit regularisation effects specific to SGD and\n5.1. Revisiting BEA\n120\nstudy the effect of batch sizes in SGD. Since their approach works in expectation\nover an epoch, however, it does not account for the implicit regularisation effects of\na smaller number of SGD steps, or account for the exact data batches used in the\nupdates; we return to their results in the next section. We take a different approach,\nand introduce a novel way to find implicit regularisers in SGD by revisiting the BEA\nproof structure and the assumptions made thus far when using BEA. Our approach\ncan be summarised as follows:\nRemark 5.1.1 Given the discrete update θt = θt−1 −h∇θE(θt−1), BEA constructs\n˙θ = ∇θE + hf1(θ) + · · · + hpfp(θ), such that ∥θ(h; θt−1) −θt∥∈O(hp+2) for a\nchoice of p ∈N, p ≥1. This translates into a constraint on the value of θ(h; θt−1).\nThus, BEA asserts only what the value of the correction terms fi in the vector field\nof the modified flow is at θt−1. Given the constraints on fi(θt−1), we can choose\nfi : RD →RD in the vector field of the modified flow ˙θ to depend on the initial\ncondition θt−1.\nFor example, in the proof by construction of the IGR flow (Section 2.3.3), we ob-\ntained that if θt = θt−1−h∇θE(θt−1) and we want to find ˙θ = −∇θE(θ)+hf1(θ) such\nthat ∥θ(h; θt−1) −θt∥is of order O(h3), then f1(θt−1) = −1\n2∇2\nθE(θt−1)∇θE(θt−1)\n(Eq (2.43)).\nFrom there, following Barrett and Dherin [12], we concluded that\nf1(θ) = −1\n2∇2\nθE(θ)∇θE(θ).\nBut notice how BEA only sets a constraint on\nthe value of the vector field at the initial point θt−1.\nIf we allow the modi-\nfied vector field to depend on the initial condition, an equally valid choice for\nf1(θ) = −1\n2∇2\nθE(θ)∇θE(θt−1) or f1(θ) = −1\n2∇2\nθE(θt−1)∇θE(θ). By construction,\nthe above flows will also have an error of O(h3) after one gradient descent step of\nlearning rate h with initial parameters θt−1. The latter vector fields only describe the\ngradient descent update with initial parameters θt−1 and thus they only apply to this\nspecific gradient descent step, though as previously noted that is also the case with\nthe IGR flow due to the dependence on the data batch—see Eq (5.2). The advantage\nof constructing modified vector fields that depend on initial parameters lies in the\nability to write modified losses when a modified vector field depending only on θ\ncannot be written as a gradient operator, as we shall see in the next sections.\n5.2. Implicit regularisation in multiple stochastic gradient descent steps\n121\nθt−1\nθt\nθt+1\nt −1 →t\nt →t + 1\n˙θ\n˙θ\n˙˜θ\n˙˜θ\nO(h3)\nO(h2)\n(a) Standard interpretation.\nθt−1\nθt\nθt+1\nt −1 →t\nt →t + 1\n˙θ\n˙θ\n˙˜θt\n˙˜θt+1\nO(h3)\nO(h2)\n(b) Novel interpretation.\nFigure 5.1: Visualising the standard approach to BEA alongside an approach which\nconstructs a different flow per gradient descent iteration. In our previous use\nof BEA (a), we constructed modified flows ˙˜θ to capture the discretisation\nerror of gradient descent; these flows did not depend on the initial iteration\nparameters. In this chapter, we take the second approach (b), allowing us\nto construct additional flows ˙˜θt, which depend on initial parameters, and\nshowcase additional implicit regularisation effects.\nThis observation about BEA leads us to the following remarks:\nRemark 5.1.2 There are multiple flows that lead to the same order in learning rate\nerror after one discrete update. Many of these flows depend on the initial conditions\nof the system, i.e. the initial parameters of the discrete update. We visualise this\napproach in Figure 5.1 and contrast it with the existing BEA interpretation.\nRemark 5.1.3 Implicit in the choice of the IGR flow [12], as well as the flows we\nhave introduced thus far in this work using BEA, namely the PF (Chapter 3) and\nthe modified flows of two-player games (Chapter 4), make an implicit assumption:\nthat we are looking for the modified flows that hold for every training iteration and\ndo not depend on the initial parameters. This is however challenged already in the\ncase of stochastic gradient descent, where each implementation of the flows requires\ndependence on data batches, as shown in Eq (5.2).\n5.2\nImplicit regularisation in multiple stochastic\ngradient descent steps\nWe now use the above observations to build modified flows that capture multiple\ngradient descent updates with an error of O(h3) after n SGD steps. We are interested\nin modified flows that can be used to construct modified losses by writing the vector\n5.2. Implicit regularisation in multiple stochastic gradient descent steps\n122\nfield of the flow as the negative gradient of a function; this enables us to capture\nthe implicit regularisation effects of taking multiple SGD steps. We analyse n SGD\nsteps, starting at iteration t\nθt+µ = θt+µ−1 −∇θE(θt+µ−1; Xt+µ),\nµ ∈{0, . . . , n −1}\n(5.5)\nwhere for simplicity we denoted E(θt−1; X) = 1\nB\nPB\ni=1 E(θt−1; xi), with elements xi\nforming the batch X. We start with the following remark:\nLemma 5.2.1 Denote E(θ; {Xt, . . . , Xt+n−1}) = 1\nn\nPn−1\nµ=0 E(θ; Xt+µ), i.e. the aver-\nage loss obtained from the n data batches. Then the trajectory obtained by taking n\nsteps of stochastic gradient descent follows the trajectory of minimising the loss in\ncontinuous-time with O(h2) error:\n˜E(θ) = E(θ; {Xt, . . . , Xt+n−1}).\n(5.6)\nPerhaps surprisingly, the above shows that effects from mini-batches appear only\nat higher-order terms in learning rate h. Thus, to capture implicit regularisation\neffects that account for the mini-batches used in the SGD updates, we use BEA. We\nfind a modified flow that describes the SGD update with an error of O(h3), with\na vector field that can be written as a negative gradient. We provide proofs and\nthe flows that construct the regularisers in the Appendix C. We use the same steps\nused for our other BEA proofs: 1) we expand the n discrete updates in Eq (5.5) to\nwrite a relation between the parameters at time t −1 and t + n −1 up to second\norder in learning rate; 2) expand the changes in continuous-time up to O(h2); and 3)\nmatch the O(h2) terms to find the terms which quantify the drift. The significant\ndifference with our previous approaches is that we allow the vector field to depend\non the initial parameters, and choose a vector field that can be written as a gradient\nin order to construct a modified loss.\nTheorem 5.2.2 Denote E(θ; {Xt, . . . , Xt+n−1}) =\n1\nn\nPn−1\nµ=0 E(θ; Xt+µ), i.e.\nthe\naverage loss obtained from the n data batches. Then the trajectory obtained by taking\nn steps of stochastic gradient descent follows the trajectory of minimising the loss in\n5.2. Implicit regularisation in multiple stochastic gradient descent steps\n123\ncontinuous-time with O(h3) error\n˜E(θ) = E(θ; {Xt, . . . , Xt+n−1}) + nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n|\n{z\n}\nfull batch norm regularisation\n(5.7)\n−h\nn\nn−1\nX\nµ=1\n\"\n∇θE(θ; Xt+µ)T\n µ−1\nX\nτ=0\n∇θE(θt−1; Xt+τ)\n!#\n|\n{z\n}\nmini-batch gradient alignment\n.\n(5.8)\nWe thus find the implicit regularisation effects induced by n steps of SGD, capturing\nthe importance of exact batches used and their order in Eq (5.8). We note that\nwithout making use the observations regarding BEA in the previous section, and\nthus without the parameters θt−1, a modified loss could not have been constructed\noutside of the full-batch case where one can recover the IGR flow. The following\nremark immediately follows by setting n = 2 in Eq (5.8):\nRemark 5.2.1 When taking a second stochastic gradient descent step, there is an\nimplicit regularisation term maximising the dot product between the gradient at the\ncurrent step and the gradient at the previous step: ∇θET∇θE(θt−1). This can be\nachieved by aligning the direction of the gradients between the two iterations or\nincreasing the gradient norm, but we note that increasing gradient norm is counter\nother implicit regularisation effect in Eq (5.7).\nWe now compare this novel modified loss with the modified loss we obtained by\nignoring stochasticity and assuming all n updates have been done with a full-batch;\nthis entails using the IGR loss (proof for multiple steps in Section C.3)\n˜E = E(θ; {Xt, . . . , Xt+n−1}) + h\n4∥∇θE(θ; {Xt, . . . , Xt+n−1})∥2.\n(5.9)\nThe above modified losses show that both full-batch gradient descent and SGD have\na pressure to minimise the gradient norm ∥∇θE(θ; {Xt, . . . , Xt+n−1})∥. SGD leads\nto an additional regularisation effect capturing the importance of the order in which\nmini-batches are presented in training: maximising the dot product between gradients\ncomputed at the current parameters given a batch and the gradients computed at the\n5.2. Implicit regularisation in multiple stochastic gradient descent steps\n124\ninitial parameters for all batches presented before the given batch. While this can be\nachieved both by increasing the norm of the gradients or by aligning gradients with\nthose at the initial iteration, we note that increasing the gradient norm is counter to\nthe other regulariser induced by SGD, the gradient norm minimisation effect shown\nin Eq (5.7).\nThe important role of learning rates in BEA appears here too: for our approxi-\nmations to hold nh has to be sufficiently small. If we adjust the learning rate by the\nnumber of updates, i.e. if we set the learning rate for SGD equal to h/n, where h is\nthe learning rate used by full-batch gradient descent, we obtain the same implicit\nregularisation coefficient for the gradient norm ∥∇θE(θ; {Xt, . . . , Xt+n−1})∥minimi-\nsation as the IGR flow in Eq 5.9, and the main difference between the two modified\nlosses is given by the mini-batch gradient alignment terms present in Eq (5.8).\nThe number of updates, n, also plays an important role.\nWhile the mini-\nbatch gradient regularisation term in Eq (5.8) has a coefficient of\nh\nn, there are\nn(n−1)\n2\nterms composing the term. Thus the magnitude of the mini-batch gradient\nregularisation term can grow with n, but its effects strongly depend on the distri-\nbution of the gradients computed at different batches. For example, if gradients\n∇θE(θt−1; Xt+i) are normally distributed with the mean at the full-batch gradient,\ni.e. ∇θE(θt−1; Xt+i) ∼N(∇θE(θt−1), σ2), as the number of updates grows the\nregularisation effect in Eq 5.8 will result in a pressure to align mini-batch gradients\nwith the full-batch gradient at the initial parameters, ∇θE(θt−1). Since our results\nhold for multiple values of n, empirical assessments need to be made to understand\nthe interplay between the number of updates and the strength of mini-batch gradient\nregularisation on training.\nThe approach provided here is complementary to that of that of Smith et al.\n[13], who obtain a relationship similar to Eq (5.8) but in expectation—they describe\nexpected value of the modified loss Eσ\n\u0002\nEsgd(θ; {Xσ(t), . . . , Xσ(t+n−1)})\n\u0003\nwhere the\nexpectation is taken over all possible data batch shufflings σ, but not the elements\nin the batch. As before we denote E(θ; Xk) as the loss given by mini-batch k (the\nequivalent to ˆCk in their notation, see their Eqs (1) and (2)), and write the modified\n5.2. Implicit regularisation in multiple stochastic gradient descent steps\n125\nloss they obtain in expectation as\nEσ [Esgd(θ)] = E(θ; {X0, . . . , Xn−1}) + h\n4n\nn−1\nX\nk=0\n\r\r∇θE(θ; Xk)\n\r\r2\n(5.10)\n= E(θ; {X0, . . . , Xn−1}) + h\n4∥∇θE(θ; {X0, . . . , Xn−1})∥2\n(5.11)\n+ h\n4n\nn−1\nX\nk=0\n\r\r∇θE(θ; Xk) −∇θE(θ; {X0, . . . , Xn−1}\n\r\r2 .\n(5.12)\nOur proposed approach does not require working in expectations and accounts\nfor the exact batches sampled from the dataset; thus it directly describes SGD as\nused in practice. If we make the same assumptions as Smith et al. [13] and take\nexpectation over all possible batch shufflings in an epoch in Eq (5.8) we obtain (proof\nin Section C.3.1):\nEσ [Esgd(θ)] = E(θ; {Xt, . . . , Xt+n−1})\n(5.13)\n+ nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n(5.14)\n−h\n2n∇θE(θ; {Xt, . . . , Xt+n−1})T∇θE(θt−1; {Xt, . . . , Xt+n−1}) (5.15)\n−h\n2n\n\"n−1\nX\nk=0\n∇θE(θ; Xt+k)T∇θE(θt−1; Xt+k)\n#\n.\n(5.16)\nAs expected, we obtain a different result than that of Smith et al. [13]: while the\ngradient norm minimisation is still present in our results, the per-batch regularisation\nin their formulation gets translated into a dot product term, where both the full-batch\nand per-batch gradients are regularised to be aligned with those at the beginning of\nthe epoch. Both approaches share the limitation that nh needs to be suitably small\nfor approximations to be relevant; we note that since we do not require n to be the\nnumber of updates in an epoch—and we obtain interestingly regularisation effects for\nn = 2, see Remark 5.2.1—this is less of an issue for our approach. Their approach\nhas the advantage of finding an implicit regularisation effect that does not depend on\nthe initial parameters. By depending on initial parameters, however, our approach\ndoes not require working in expectations and accounts for the exact batches used in\n5.3. Implicit regularisation in generally differentiable two-player games\n126\nthe SGD updates. We hope this can be used to stabilise SGD over multiple steps, as\nwe have done with a single GD step in Section 3.7, that it can be used in continual\nand transfer learning [71, 72, 189], as well as understanding the effects of the order\nof examples on model optimisation in online learning.\n5.3\nImplicit regularisation in generally\ndifferentiable two-player games\nIn Chapter 4, we presented a framework for the quantification of discretisation drift\nin two-player games. We found distinct modified flows that describe simultaneous\nEuler updates and alternating Euler updates. In both cases, we found corrections f1\nand g1 to the original system\n˙ϕ = f(ϕ, θ)\n(5.17)\n˙θ = g(ϕ, θ),\n(5.18)\nsuch that the modified continuous system\n˙ϕ = f(ϕ, θ) + hf1(ϕ, θ)\n(5.19)\n˙θ = g(ϕ, θ) + hg1(ϕ, θ),\n(5.20)\nfollows the discrete steps of the method with a local error of order O(h3). More\nprecisely, if (ϕt, θt) denotes the discrete step of the method at time t and (ϕ(h), θ(h))\ncorresponds to the continuous solution of the modified system above starting at\n(ϕt−1, θt−1), ∥ϕt −ϕ(h)∥and ∥θt −θ(h)∥are of order O(h3). In this section, we\nassume for simplicity that both players use the same learning rate h and simultaneous\nupdates, but the same arguments can be made when they use different learning rates\nor alternating updates.\nUsing this framework, we constructed modified loss functions in the case of\nzero-sum (Section 4.5) and common-payoff games (Section 4.4). However, using the\naforementioned modified flows, we cannot always write the vector fields of the modified\n5.3. Implicit regularisation in generally differentiable two-player games\n127\nflows as a gradient for differentiable two-player games and thus we cannot construct\nmodified losses. To see why, consider the case where we have two loss functions for the\ntwo players respectively Eϕ(ϕ, θ) : Rm ×Rn →R and Eθ(ϕ, θ) : Rm ×Rn →R. This\nleads to the update functions f = −∇ϕEϕ and g = −∇θEθ. By using f = −∇ϕEϕ\nand g = −∇θEθ in Theorem 4.2.1, we have that for simultaneous gradient descent\nf1 = −1\n2\ndf\ndϕf −1\n2\ndf\ndθg\n(5.21)\n= −1\n4∇ϕ ∥∇ϕEϕ∥2\n|\n{z\n}\nself term\n−1\n2\nd∇ϕEϕ\ndθ\n∇θEθ\n|\n{z\n}\ninteraction term\n(5.22)\nand similarly\ng1 = −1\n4∇θ ∥∇θEθ∥2\n|\n{z\n}\nself term\n−1\n2\nd∇θEθ\ndϕ\n∇ϕEϕ\n|\n{z\n}\ninteraction term\n.\n(5.23)\nThus, it is not always possible to write f1 and g1 as gradient functions, since the\ninteraction terms—see Definition 4.2.1 for the definition of self and interaction terms—\ncannot always be written as a gradient. Thus, the modified flows cannot be used to\nconstruct modified losses leading to implicit regularisers in generally differentiable\ntwo-player games, as we have done for zero-sum games in Chapter 4.\nWe now use the interpretation of BEA we provided in this chapter, to choose\ntwo other functions f1 and g1 and we will use them to construct another set of\nmodified flows. These flows will still satisfy the value constraints required by the\nBEA proofs and by construction after one gradient descent update the error between\nthe modified flows and the discrete updates is O(h3). Indeed, what the BEA proofs\nprovide for simultaneous gradient descent is (see Eq (B.34) in the Appendix)\nf1(ϕt−1, θt−1) = −1\n2\ndf\ndϕ(ϕt−1, θt−1)f(ϕt−1, θt−1) −1\n2\ndf\ndθ(ϕt−1, θt−1)g(ϕt−1, θt−1)\n(5.24)\ng1(ϕt−1, θt−1) = −1\n2\ndg\ndϕ(ϕt−1, θt−1)f(ϕt−1, θt−1) −1\n2\ndg\ndθ(ϕt−1, θt−1)g(ϕt−1, θt−1).\n(5.25)\n5.3. Implicit regularisation in generally differentiable two-player games\n128\nFrom here, we can choose f1 and g1, now depending on the iteration number t:\nf1,t(ϕ, θ) = −1\n2\ndf\ndϕ(ϕ, θ)f(ϕ, θ) −1\n2\ndf\ndθ(ϕ, θ)g(ϕt−1, θt−1)\n(5.26)\ng1,t(ϕ, θ) = −1\n2\ndg\ndϕ(ϕ, θ)f(ϕt−1, θt−1) −1\n2\ndg\ndθ(ϕ, θ)g(ϕ, θ).\n(5.27)\nHere, we treat g(ϕt−1, θt−1) and f(ϕt−1, θt−1) as constants. We replace f = −∇ϕEϕ\nand g = −∇θEθ in f1,t and g1,t and write the drift terms as gradient functions:\nf1,t(ϕ, θ) = −1\n2\nd∇ϕEϕ\ndϕ\n∇ϕEϕ −1\n2\nd∇ϕEϕ\ndθ\n∇θEθ(ϕt−1, θt−1)\n(5.28)\n= −1\n2∇ϕ ∥∇ϕEϕ∥2 −1\n2∇ϕ\n\u0000∇θET\nϕ∇θEθ(ϕt−1, θt−1)\n\u0001\n(5.29)\n= −∇ϕ\n\u00121\n2 ∥∇ϕEϕ∥2 + 1\n2∇θET\nϕ∇θEθ(ϕt−1, θt−1)\n\u0013\n(5.30)\ng1,t(ϕ, θ) = −∇θ\n\u00121\n2 ∥∇θEθ∥2 + 1\n2∇ϕET\nθ ∇ϕEϕ(ϕt−1, θt−1)\n\u0013\n.\n(5.31)\nReplacing the above in the modified flows in Eqs (5.19) and (5.20)), we obtain\n˙ϕ = f(ϕ, θ) + hf1(ϕ, θ)\n(5.32)\n= −∇ϕ\n\u0012\nEϕ + h\n\u00121\n2 ∥∇ϕEϕ∥2 + 1\n2∇θET\nϕ∇θEθ(ϕt−1, θt−1)\n\u0013\u0013\n(5.33)\n˙θ = g(ϕ, θ) + hg1(ϕ, θ)\n(5.34)\n= −∇θ\n\u0012\nEθ + h\n\u00121\n2 ∥∇θEθ∥2 + 1\n2∇ϕET\nθ ∇ϕEϕ(ϕt−1, θt−1)\n\u0013\u0013\n.\n(5.35)\nWe can now write modified losses for each gradient descent iteration of a general\ntwo-player differentiable game (which depend on the iteration t), which describe the\nlocal trajectory of simultaneous gradient descent up to O(h3):\n˜Eϕ,t = Eϕ + h\n\n\n1\n2 ∥∇ϕEϕ∥2\n|\n{z\n}\nself term\n+ 1\n2∇θET\nϕ∇θEθ(ϕt−1, θt−1)\n|\n{z\n}\ninteraction term\n\n\n\n(5.36)\n˜Eθ,t = Eθ + h\n\n\n1\n2 ∥∇θEθ∥2\n|\n{z\n}\nself term\n+ 1\n2∇ϕET\nθ ∇ϕEϕ(ϕt−1, θt−1\n|\n{z\n}\ninteraction term\n)\n\n\n.\n(5.37)\n5.3. Implicit regularisation in generally differentiable two-player games\n129\nThe self terms result in the implicit gradient regularisation found in supervised\nlearning [12] and zero-sum games (Chapter 4): each player has an incentive to\nminimise its own gradient norm. The interaction term for each player encourages\nminimising the dot product between the gradients of its loss with respect to the other\nplayer’s parameters and the previous gradient update of the other player. Consider\nthe first player’s interaction term, equal to (−∇θEϕ(ϕt, ·))T(−∇θEθ(ϕt−1, θt−1)),\nwhere −Eθ(ϕt−1, θt−1) is the previous update direction of θ aimed at minimising\nEθ. Its implicit regularisation effect depends on the functional form of Eϕ and\nEθ: if ∇θEθ and ∇θEϕ have aligned directions by construction, then the implicit\nregularisation effect nudges the first player’s update towards a point in space where\nthe second player’s update changes direction; the opposite is true if ∇θEθ and ∇θEϕ\nare misaligned by construction.\n5.3.1\nThe effect of the interaction terms: a GAN example\nWe work through the regularisation effect of interaction terms for a pair of commonly\nused GAN losses that do not form a zero-sum game (using the generator non-\nsaturating loss [190]) and contrast it with the zero-sum case (the saturating loss);\nwe have previously investigated these losses in Chapter 4. As before, we denote\nthe first player, the discriminator, as D, parametetrised by ϕ, and the generator\nas G, parametrised by θ. We denote the data distribution as p∗(x) and the latent\ndistribution p(z). Given the non-saturating GAN loss Goodfellow et al. [32]:\nEϕ(ϕ, θ) = Ep∗(x) log D(x; ϕ) + Ep(z) log(1 −D(G(z; θ); ϕ))\n(5.38)\nEθ(ϕ, θ) = Ep(z) −log D(G(z; θ); ϕ),\n(5.39)\nwe can then write the interaction term ∇θET\nϕ∇θ∇θEθ(ϕt−1, θt−1) in Eq (5.36) at\niteration t as (derivation in Appendix C.4.1)\n1\nB2\nB\nX\ni,j=1\ncnon−sat\ni,j\n∇θD(G(zi\nt; θ); ϕ)T∇θD(G(zj\nt−1; θt−1); ϕt−1)),\nwith\n(5.40)\ncnon−sat\ni,j\n=\n1\n1 −D(G(zi\nt; θ); ϕ)\n1\nD(G(zj\nt−1; θt−1); ϕt−1)\n,\n(5.41)\n5.3. Implicit regularisation in generally differentiable two-player games\n130\nwhere zi\nt is the latent variable with index i in the batch at time t with batch size B.\nThus, the strength of the regularisation—cnon−sat\ni,j\n—depends on how confident the dis-\ncriminator is. In particular, this implicit regularisation encourages the discriminator\nupdate into a new set of parameters where the gradient ∇θD(G(zi\nt; θ); ϕ) points away\nfrom the direction of ∇θD(G(zj\nt−1; θt−1); ϕt−1) when ci,j is large. This occurs for zi\nt\nwhere the discriminator is fooled by the generator—i.e. 1 −D(G(zi; θ); ϕ) is close to\n0 and\n1\n1−D(G(zi\nt;θ);ϕ) is large—and samples zj\nt−1 where the discriminator was correct\nat the previous iteration—D(G(zj\nt−1; θt−1); ϕt−1) low and thus\n1\nD(G(zj\nt−1;θt−1);ϕt−1) is\nlarge. This can be seen as beneficial regularisation for the generator, by ensuring the\nupdate direction −∇θEθ = Ep(z)\n1\nD(G(z;θ);ϕ)∇θD(G(z; θ); ϕ) is adjusted accordingly\nto the discriminator’s output. We note, however, that regularisation might not have a\nstrong effect, as gradients ∇θD(G(z; θ); ϕ) that have a high weight in the generator’s\nupdate are those where the discriminator is correct and classifies generated data\nas fake—i.e.\n1\nD(G(z;θ);ϕ) is large—but there the factor\n1\n1−D(G(z;θ);ϕ) in the interaction\nterm coefficient in Eq (5.41) will be low. This is inline with our empirical results in\nSection 4.6.4, where we saw that for the non-saturating loss discretisation drift does\nnot have a strong effect on performance.\nWe can contrast this with what regularisation we obtain when using the saturat-\ning loss [32], where Eθ(ϕ, θ) = −Ep(z) log(1−D(G(z; θ); ϕ)). We used the saturating\nloss extensively in experiments in Section 4.6, and we have shown it performs poorly\nin comparison to the non-saturating loss, in-line with the literature [32]. If we\nperform the same analysis as above for the saturating loss we obtain the following\nimplicit regulariser\n1\nB2\nB\nX\ni,j=1\ncsat\ni,j ∇θD(G(zi\nt; θ); ϕ)T∇θD(G(zj\nt−1; θt−1); ϕt−1)),\nwith\n(5.42)\ncsat\ni,j =\n1\n1 −D(G(zi\nt; θ); ϕ)\n1\n1 −D(G(zj\nt−1; θt−1); ϕt−1))\n.\n(5.43)\nHere, csat\ni,j is high for zi\nt and zj\nt−1 where the generator was fooling the discriminator—\ni.e.\nlow 1 −D(G(zj\nt−1; θt−1); ϕt−1) and 1 −D(G(zi\nt; θ); ϕ).\nThus, instead of\nmoving ∇θD(G(zi\nt; θ); ϕ) away from directions where the generator was doing\n5.4. Conclusion\n131\npoorly previously as is the case for the non-saturating loss, it is moving it away\nfrom directions where the generator was performing well, which could lead to in-\nstabilities or loss of performance. Moreover, unlike for the non-saturating loss,\nthe implicit regularisation will have a strong effect on the generator’s update\n−∇θEθ = Ep(z)\n1\n1−D(G(z;θ);ϕ)∇θD(G(z; θ); ϕ), since gradients ∇θD(G(z; θ); ϕ) that\nhave a high weight in the generator’s update are those where the interaction term\ncoefficient in Eq (5.43) is high. This is inline with our results in Section 4.6, showing\nthat in zero-sum games (such as the one induced by the saturating loss), the regular-\nisation induced by the discretisation error of gradient descent is strong, and can hurt\nperformance and stability.\n5.4\nConclusion\nWe provided a novel approach of interpreting backward error analysis and used it to\nfind implicit regularisers induced by gradient descent in the single objective setting\nand in two-player games. In the single objective case, we found implicit regularisation\nterms revealing importance of the alignment of gradients at the exact data batches\nused in multiple steps of stochastic gradient descent, while in two-player games we\nhighlighted the need to examine the game structure in order to determine the effects\nof implicit regularisation. We hope future work can empirically verify the effects of\nthese implicit regularisers in deep learning. We believe that our observations in this\nchapter might unlock future research, including verifying whether the dot product\nalignment regularisation we found explains the benefits of using stochastic gradient\ndescent in deep learning, and what the empirical effect of the interaction terms is on\ngenerally differentiable two-player games.\nChapter 6\nThe importance of model\nsmoothness in deep learning\nIn previous chapters, we used continuous-time approaches to model gradient descent\noptimisation dynamics and empirically assessed the effect of these dynamics on neural\nnetwork training. We have seen how improvements to optimisation lead to increased\nstability and better performance across problem domains, from supervised learning to\ntwo-player games such as GANs. Optimisation also interacts with the choice of model\nthrough neural architectures and model regularisation: some neural architectures\nare easier to optimise than others [115, 191], and model regularisation techniques\ncan improve training stability [23, 30, 85]. For the rest of this thesis, we explore\ninteractions between optimisation and models, and in particular model smoothness\nand smoothness regularisers. While smoothness quantifies sensitivity to changes\nin model inputs and optimisation is concerned with changes to model parameters,\nwe show that in neural network training smoothness regularisation can improve\noptimisation and that implicit regularisation effects induced by optimisation—such\nas those discussed in previous chapters—can increase model smoothness.\n6.1\nWhy model smoothness?\nHow certain should a classifier be when it is presented with out of distribution\ndata? How much mass should a generative model assign around a datapoint? How\nmuch should an agent’s behaviour change when its environment changes slightly?\nAnswering these questions shows the need to quantify the manner in which the\n6.2. Measuring smoothness\n133\noutput of a model varies with changes in its input, a quantity we will intuitively\ncall the smoothness of the model. In deep learning, neural network smoothness has\nbeen shown to boost generalisation and robustness of classifiers and increase the\nstability and performance of generative models, as well as provide better priors for\nrepresentation learning and reinforcement learning agents [30, 81–85, 192–194]. After\nproviding an overview of the benefits of smoothness in deep learning and motivated\nby its impact across problem domains, later in the chapter we investigate some of\nthe previously unexplored effects of smoothness regularisation: strong interactions\nwith optimisation, reduced model capacity, and interactions with data scaling.\n6.2\nMeasuring smoothness\nNeural network “smoothness” is a broad, vague, catch all term.\nWe use it to\nconvey formal definitions, such as differentiable, bounded, Lipschitz, as well as\nintuitive concepts such as invariant to data dimensions or projections, robust to\ninput perturbations, and others. One definition states that a function f : X →Y\nis n smooth if is n times differentiable with the n-th derivative being continuous.\nThe differentiability of a function is not a very useful inductive bias for a model,\nas it is both very local and constructed according to the metric of the space where\nlimits are taken. What we are looking for is the ability to choose both the distance\nmetric and how local or global our smoothness inductive biases are. With this in\nmind, Lipschitz continuity is appealing as it defines a global property and provides\nthe choice of distances in the domain and co-domain of f. It is defined as:\n∥f(x1) −f(x2)∥Y ≤K ∥x1 −x2∥X\n∀x1, x2 ∈X,\n(6.1)\nwhere K is denoted as the Lipschitz constant of function f. To avoid learning trivially\nsmooth functions and maintain useful variability, it is often beneficial to constrain the\nfunction variation both from above and below. This leads to bi-Lipschitz continuity:\nK1 ∥x1 −x2∥X ≤∥f(x1) −f(x2)∥Y ≤K2 ∥x1 −x2∥X .\n(6.2)\n6.3. Smoothness regularisation in deep learning\n134\nEnforcing Eq (6.1) can be difficult, but according to Rademacher’s theorem [195]\nif X ⊂RI is an open set and Y = RO and f is K-Lipschitz then\n\r\r\rdf(x)\ndx\n\r\r\r\nop ≤K\nwherever the total derivative Dxf(t) = df(x)\ndx t exists, which is almost everywhere.\nConversely, a function f that is differentiable everywhere with\n\r\r\rdf(x)\ndx\n\r\r\r\nop ≤K is\nK-Lipschitz. Since the operator norm is not always easily computable, at times the\nFrobenius norm of the Jacobian df(x)\ndx\nis constrained instead, since the Frobenius\nnorm bounds the operator norm from above:\n\r\r\rdf(x)\ndx\n\r\r\r\nop ≤\n\r\r\rdf(x)\ndx\n\r\r\r\n2; for O = 1,\n∥∇xf(x)∥op = ∥∇xf(x)∥2. Thus, a convenient strategy to make a differentiable\nfunction K-Lipschitz is to ensure\n\r\r\rdf(x)\ndx\n\r\r\r\n2 ≤K, ∀x ∈X.\nIf f and g are Lipschitz with constants Kf and Kg, f ◦g is Lipschitz with\nconstant KfKg. Since commonly used activation functions are 1-Lipschitz, a neural\nnetwork can be made Lipschitz by constraining each learneable layer to be Lipschitz.\nMany neural networks layers are linear operators (linear and convolutional layers,\nBatchNormalisation [87]), and to compute their Lipschitz constant we can use that\nthe Lipschitz constant of a linear operator A under common norms such as l1, l2, l∞\nis supx̸=0\n∥Ax∥\n∥x∥[147]. For the l2 norm, most commonly used in practice, this leads to\nfinding the spectral norm of A.\n6.3\nSmoothness regularisation in deep learning\nSmoothness regularisers have long been part of the toolkit of the deep learning\npractitioner: early stopping encourages smoothness by stopping optimisation before\nthe model overfits the training data; dropout [76] makes the network more robust to\nsmall changes in the input by randomly masking hidden activations; max pooling\nencourages smoothness with respect to local changes; L2 weight regularisation and\nweight decay [196] discourage large changes in output by not allowing individual\nweight norms to grow; data augmentation allows us to specify what changes in\nthe input should not result in large changes in the model prediction and thus is\nalso closely related to smoothness and invariance to input transformations. These\nsmoothness regularisation techniques are often introduced as methods that directly\ntarget generalisation and other beneficial effects of smoothness discussed in Section 6.4,\n6.3. Smoothness regularisation in deep learning\n135\ninstead of being seen through the lens of smoothness regularisation.\nMethods that explicitly target smoothness on the entire input space focus on\nrestricting the learned model family. A common approach is to ensure Lipschitz\nsmoothness with respect to the l2 metric by individually restricting the Lipschitz\nconstant of each layer; for layers that are linear operators, this entails restricting their\nspectral norm. Spectral regularisation [80] uses the sum of the spectral norms—the\nlargest singular value—of each layer as a regularisation loss to encourage Lipschitz\nsmoothness. Spectral Normalisation [30] ensures the learned models are 1-Lipschitz\nby adding a node in the computational graph of the model layers by replacing the\nweights with their normalised version:\nW →W/||W||op,\n(6.3)\nwhere ||W||op is the spectral norm of W. Both Spectral Normalisation and Spectral\nRegularisation use power iteration [197] to compute the spectral norm of weight\nmatrices ||W||op for layers which are linear operators, such as convolutional or\nlinear layers. Gouk et al. [198] use a projection method by dividing the weights by\nthe spectral norm after a gradient update. This is unlike Spectral Normalisation,\nwhich backpropgates through the normalisation operation. The majority of this\nline of work has focused on constraints for linear and convolutional layers, and\nonly recently attempts to expand to other layers, such as self attention, have been\nmade [199]. Efficiency is always a concern and heuristics are often used even for\npopular layers such as convolutional layers [30] despite more accurate algorithms\nbeing available [198, 200]. Parseval networks [84] ensure weight matrices of linear and\nconvolutional layers are 1-Lipschitz by enforcing a stronger constraint, an extension of\northogonality to non-square matrices. Bartlett et al. [126] show that any bi-Lipschitz\nfunction can be written as a composition of residual layers [191].\nInstead of restricting the learned function on the entire space, another approach\nof targeting smoothness constraints is to regularise the norm of the gradients of a\npredictor f : RI →R with respect to inputs of the network ∥∇xf(x; θ)∥, at different\nregions of the space [28, 36, 82, 201, 202]. This is often enforced by adding a gradient\n6.4. The benefits of learning smooth models\n136\npenalty to the loss function L(θ):\nL(θ) + ζEpreg(x)\n\u0000∥∇xf(x; θ)∥2 −K2\u00012 ,\n(6.4)\nwhere ζ is a regularisation coefficient, preg(x) is the distribution at which the regu-\nlarisation is applied, which can either be the data distribution [82, 201] or around\nit [28, 202], or, in the case of generative models, at linear interpolations between data\nand model samples [36]. Gradient penalties encourage the function to be smooth\naround the support of preg(x) either by encouraging Lipschitz continuity (K ̸= 0) or\nby discouraging drastic changes of the function as the input changes (K = 0).\nSmoothness for classification tasks is defined by Lassance et al. [192] as preserving\nfeatures similarities within the same class as we advance through the layers of the\nnetwork. The penalty used is PL\nl=1\nPC\nc=1 |σl(sc) −σl+1(sc)|, where σl(sc) is the\nsignal of features belonging to class c computed using the Laplacian of layer l. The\nLaplacian of a layer is defined by constructing a weighted symmetric adjacency matrix\nof the graph induced by the pairwise most similar layer features in the dataset.\n6.4\nThe benefits of learning smooth models\nGeneralisation. Learning models that generalise beyond training data is the goal of\nmachine learning. The study of generalisation has long been connected with the study\nof model complexity, as models with small complexity generalise better [203]. Despite\nthis, we have seen that deep, over-parametrised neural networks tend to generalise\nbetter than their shallow counterparts [83] and that for Bayesian methods, Occam’s\nrazor does not apply to the number of parameters used, but to the complexity of\nthe function [204]. A way to reconcile these claims is to incorporate smoothness\ninto definitions of model complexity and to show that smooth, over-parametrised\nneural networks generalise better than their less smooth counterparts. Methods\nthat encourage smoothness—such as weight decay, dropout, and early stopping\n—have been long shown to aid generalisation [76, 79, 205–207]. Data augmentation\nhas been shown to increase robustness to random noise or to modality specific\ntransformations, such as image cropping and rotations [208–210]. Sokoli´c et al. [82]\n6.4. The benefits of learning smooth models\n137\n0\n5\n10\n15\nCapacity (polynomial degree)\n0\n1\n2\n3\n4\nTest error\n(a) Traditional U-shaped curve.\n0\n25\n50\n75\n100\n125\nNetwork Width\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nTest Loss\n(b) Double descent in neural networks.\nFigure 6.1: Double descent in deep learning compared to the traditional U-shaped com-\nplexity curve. (a): traditional U-shaped curve associated with increased\ncapacity; as we increase capacity, the test error first decreases after which\nit increases due to overfitting—the example we show here is that from Fig-\nure 1.2. Figure 6.1b: when the complexity of neural networks is measured\nusing the number of model parameters a double descent is observed, where a\nfirst descent is followed by a second descent.\nshow that the generalisation error of a network with linear, softmax, and pooling\nlayers is bounded by the classification margin in input space. Since classifiers are\ntrained to increase classification margins in output space, smoothing by bounding\nthe spectral norm of the model’s Jacobian increases generalisation performance; this\nleads to empirical gains on standard image classification tasks. Bartlett et al. [81]\nprovide a generalisation bound depending on classification margins and the product\nof spectral norms of the model’s weights and show how empirically the product of\nspectral norms correlates with excess risk.\nGeneralisation in deep learning has been recently re-examined under the light of\ndouble descent [79, 211], a phenomenon named after the shape of the generalisation\nerror plotted against the size of a deep neural network: as the size of the network\nincreases the generalisation error decreases (first descent), then increases, after which\nit decreases again (second descent). This is unlike the traditional U-shaped curve\nassociated with non-neural models, where the second descent does not occur. We\nplot the traditional U-shaped curve in Figure 6.1a and contrast it with the double\ndescent phenomenon shown in Figure 6.1b. We postulate there is a strong connection\nbetween double descent and smoothness: in the first descent, the generalisation\n6.4. The benefits of learning smooth models\n138\nerror is decreasing as the model is given extra capacity to capture the decision\nsurface; the increase happens when the model has enough capacity to fit the training\ndata, but it cannot do so and retain smoothness; the second descent occurs as the\ncapacity increases and smoothness can be retained. This view of double descent is\nsupported by empirical evidence that shows that its effect is most pronounced on\nclean label datasets and when early stopping and other regularisation techniques are\nnot used [79]. Later in this chapter, we show that smoothness constraints heavily\ninteract with optimisation. This further suggests that empirical investigations into the\nimpact of smoothness on the double descent phenomenon are needed, since implicit\nregularisation effects induced by optimisation might act as smoothness regularisers\naffecting model complexity. We will come back to this question in Chapter 8.\nReliable uncertainty estimates. Neural networks trained to minimise classifi-\ncation losses provide notoriously unreliable uncertainty estimates; an issue which gets\ncompounded when the networks are faced with out of distribution data. However,\none can still leverage the power of neural networks to obtain reliable uncertainty\nestimates by combining smooth neural feature learners with non-softmax decision\nsurfaces [212, 213]. The choice of smoothness regularisation or classifier can vary,\nfrom using gradient penalties on the neural features with a Radial Basis Function clas-\nsifier [212], to using Spectral Normalisation on neural features and a Gaussian Process\nclassifier [213]. These methods are competitive with standard techniques used for\nout-of-distribution detection [214] on both vision and language understanding tasks.\nThe importance of smoothness regularising neural features indicates that having a\nsmooth decision surface, such as a Gaussian Process, is not sufficient to compensate\nfor sharp feature functions when learning models for uncertainty estimation [214].\nRobustness to adversarial attacks. Adversarial robustness has become an\nactive area of research in recent years [190, 215–217]. Early works have observed that\nthe existence of adversarial examples is related to the magnitude of the gradient of\nthe hidden network activation with respect to its input, and suggested that constrain-\ning the Lipschitz constant of individual layers can make networks more robust to\nattacks [215]. Initial approaches to combating adversarial attacks, however, focused\n6.4. The benefits of learning smooth models\n139\non data augmentation methods [190, 218–220], and only more recently smoothness\nhas come into focus [82–84, 192]. We can see the connection between smoothness\nand robustness by looking at the desired robustness properties of classifiers, which\naim to ensure that inputs in the same ϵ-ball result in the same function output:\n∥x −x′∥≤ϵ =⇒arg max f(x) = arg max f(x′).\n(6.5)\nThe aim of adversarial defences and robustness techniques is to have ϵ be as large\nas possible without affecting classification accuracy. Robustness against adversarial\nexamples has been shown to correlate with generalisation [221], and with the sensi-\ntivity of the network output with respect to the input as measured by the Frobenius\nnorm of the Jacobian of the learned function [82, 83].\nLassance et al. [192] show\nthat robustness to adversarial examples is enhanced when the function approximator\nis smooth as defined by the Laplacian smoothness signal discussed in Section 6.3.\nTsuzuku et al. [200] show that Eq (6.5) holds when the l2 norm is used if ϵ is smaller\nthan the ratio of the classification margin and the Lipschitz constant of the network\ntimes a constant, and thus they increase robustness by ensuring the margin is larger\nthan the Lipschitz constant.\nImproved generative modelling performance. Smoothness constraints\nthrough gradient penalties or Spectral Normalisation have become a recipe for\nobtaining state of the art generative models. In Generative Adversarial Networks\n(GANs) [32], smoothness constraints on the discriminator and the generator have\nplayed a big part in scaling up training on large, diverse image datasets at high\nresolution [33] and a combination of smoothness constraints has been shown to be\na requirement to get GANs to work on discrete data such as text [41]. The latest\nvariational autoencoders [222, 223] incorporate Spectral Regularisation to boost\nperformance and stability [85]. Explicit likelihood tractable models like normalizing\nflows [224] benefit from smoothness constraints through powerful invertible layers\nbuilt using residual connections g(x) = x + f(x) where f is Lipschitz [225].\nMore informative critics.\nCritics, learned approximators to intractable\ndecision functions, have become a fruitful endeavour in generative modelling, repre-\n6.4. The benefits of learning smooth models\n140\nsentation learning, and reinforcement learning.\nCritics are used in generative modelling to approximate divergences and distances\nbetween the learned model and the true unknown data distribution, and have\nbeen mainly popularised by GANs. A critic in a function class F can be used to\napproximate the KL divergence by minimizing the bound [226, 227]:\nKL(p||q) = Ep(x) log p(x)\nq(x) = sup\nf\nEp(x)f(x) −Eq(x)ef(x)−1\n(6.6)\n≥sup\nf∈F\nEp(x)f(x) −Eq(x)ef(x)−1.\n(6.7)\nDue to the density ratio p(x)/q(x) in its definition, the KL divergence provides\nno learning signal when the model and data distributions do not have overlapping\nsupport. Choosing F to be a family of smooth functions in Eq (6.7), however, results\nin a bound on the KL that provides useful gradients and can be used to train a\nmodel, even when the two distributions do not have overlapping support [28, 228].\nWe show an illustrative example in Figure 6.2a: the true decision surface jumps\nfrom zero to infinity, while the approximation provided by the MLP is smooth.\nSimilarly, training the critic more and making it better at estimating the true\ndecision surface but less smooth can hurt training [185]. It’s not surprising that\nimposing smoothness constraints on critics has become part of many flavours of\nGANs [27, 28, 33, 36, 80, 201, 229].\nThe same conclusions have been reached in unsupervised representation learning,\nwhere parametric critics are trained to approximate another intractable quantity, the\nmutual information, using the Donsker–Varadhan or similar bounds [230, 231]. An\nextensive study on representation learning techniques based on mutual information\nshowed that tighter bounds do not lead to better representations [194]. Instead, the\nsuccess of these methods is attributed to the inductive biases of the critics employed\nto approximate the mutual information. In reinforcement learning, neural function\napproximators or neural “critics” approximate state-value functions or action-state\nvalue functions and are then used to train a policy to maximise the expected reward.\nDirectly learning a neural network parametric estimator of the action value gradients\n6.4. The benefits of learning smooth models\n141\np\nq\nf *\nMLP approx to f *\n(a) KL divergence optimal critic f ∗= p\nq\nand smooth critic estimate.\np\nq\nf\n(b) Optimal Wasserstein critic is smooth.\nFigure 6.2: The importance of critic smoothness when estimating divergences and dis-\ntances. (a): When the two distributions do not have overlapping support, the\nKL divergence provides no learning signal, while a smooth approximation via\na learned critic does. (b): The optimal Wasserstein critic has a smoothness\nLipschitz constraint in its definition.\n—the gradients of the action value with respect to the action—results in more accurate\ngradients (Figure 3 in [193]), but also makes gradients smoother. This provides an\nessential exploration prior in continuous control, where similar actions likely result\nin the same reward and observing the same action twice is unlikely due to size of\nthe action space; encouraging the policy network to extrapolate from the closest\nseen action improves performance over both model free and model based continuous\ncontrol approaches [193].\nDistributional distances. Including smoothness constraints in the definition\nof distributional distances by using optimal transport has seen a uptake in machine\nlearning applications in recent years, from generative modelling [27, 36, 232, 233] to\nreinforcement learning [117, 234], neural flows [235] and fairness [236, 237]. Optimal\ntransport is connected to Lipschitz smoothness as the Wasserstein distance can be\ncomputed via the Kantorovich-Rubinstein duality [238]:\nW1(p(x), q(x)) =\nsup\nf:∥f∥Lip≤1\nEp(x)f(x) −Eq(x)f(x).\n(6.8)\nThe Wasserstein distance finds the critic that can separate the two distributions\nin expectation, but constraints that critic to be Lipschitz in order to avoid pathological\nsolutions. The importance of the Lipschitz constraint on the critic can be seen in\nFigure 6.2b: unlike the KL divergence, the optimal Wasserstein critic is well defined\n6.5. Challenges with smoothness regularisation\n142\nwhen the two distributions do not have overlapping support, and does not require\nan approximation to provide useful learning signal for a generative model.\n6.5\nChallenges with smoothness regularisation\nDespite the many benefits of learning smooth models we outlined in the previous\nsection, many of the effects of smoothness regularisation have not been extensively\nstudied. In this section, we lay the ground work for the next chapters by looking\nat some of the unexplored effects of smoothness regularisation, from reduced model\ncapacity to interactions with optimisation. Figure reproduction details for this\nchapter are available in Appendix Chapter D.1.\n6.5.1\nToo much smoothness hurts performance\nNeedlessly limiting the capacity of our models by enforcing smoothness constraints is\na significant danger: a constant function is very smooth, but not very useful. Beyond\ntrivial examples, Jacobsen et al. [239] show that one of the reasons neural networks\nare vulnerable to adversarial perturbations is invariance to task relevant changes—\ntoo much smoothness with respect to the wrong metric. A neural network can be\n“too Lipschitz”: methods aimed at increasing robustness to adversarial examples do\nindeed decrease the Lipschitz constant of a classifier, but once the Lipschitz constant\nbecomes too low, accuracy drops significantly [240].\nThere are two main avenues for being too restrictive in the specification of\nsmoothness constraints, depending on where and how smoothness is encouraged.\nSmoothness constraints can be imposed on the entire input space or only in certain\npockets, often around the data distribution. Methods that impose constraints on\nthe entire space throw away useful information about the input distribution and\nrestrict the learned function needlessly by forcing it to be smooth in areas of the\nspace where there is no data. This is especially problematic when the input lies on a\nsmall manifold in a large dimensional space, such as in the case of natural images,\nwhich are a tiny fraction of the space of all possible images. Model capacity can also\nbe needlessly restrained by imposing strong constraints on the individual components\nof the model, often the network layers, instead of allowing the network to allocate\n6.5. Challenges with smoothness regularisation\n143\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\nlabel 0 data\nlabel 1 data\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) 2 layer MLP.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) 4 layer MLP.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Gradient penalty at data;\nK = 1.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) Spectral norm;\nK = 1.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(e) Spectral norm;\nK = 10.\n20\n10\n0\n10\n20\n20\n10\n0\n10\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(f) Spectral norm;\nscaling the data by 10.\nFigure 6.3: Decision surfaces on two moons under different regularisation methods.\nUnless otherwise specified the model architecture is a 4 layer MLP. We\nobserve that network depth can decrease smoothness (b); that data dependent\nsmoothness penalties can increase smoothness (c); that hard smoothness\nconstraints can lead to smooth surfaces but hurt performance (d), but that\nthis effect can be mitigated by increasing the Lipschitz constant of the\nconstraint (e) or scaling the range of the input data (f).\ncapacity as needed.\nWe can exemplify the importance of where and how constraints are imposed\nwith an example, by contrasting gradient penalties—end to end regularisation applied\naround the training data—and Spectral Normalisation—layer-wise regularisation\napplied to the entire space. Figure 6.3d shows that using Spectral Normalisation to\nrestrict the Lipschitz constant of an MLP to be 1 decreases the capacity of the network\nand severely affects accuracy compared to the baseline MLP—Figure 6.3b—or the\nMLP regularised using gradient penalties—Figure 6.3c. Further insight comes from\nFigure 6.4, which shows that the gradient penalty only enforces a weak constraint on\nthe model and does not heavily restrict the spectral norms of individual layers; this is\nin stark contrast with Spectral Normalisation, which, by construction, ensures each\nnetwork layer has spectral norm equal to 1. To show the effect of data dependent\n6.5. Challenges with smoothness regularisation\n144\n0\n25\n50\n75\n100\nIteration\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nWeight spectral norm\n(a) Unregularised.\n0\n25\n50\n75\n100\nIteration\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nWeight spectral norm\n(b) Gradient penalty.\nK = 1.\n0\n25\n50\n75\n100\nIteration\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nWeight spectral norm\nlayer_0\nlayer_1\nlayer_2\nlayer_3\n(c) Spectral Normalisation.\nK = 1.\nFigure 6.4: Lipschitz constant of each layer of an MLP trained on the two moons dataset.\nThe decision surfaces for the same models can be seen in Figure 6.3. Smaller\nmeans smoother. Soft regularisation via the gradient penalty leads to a\ndecrease in spectral norm of the model’s weight matrices, with each layer\nadjusting differently to the constraint. In contrast, for Spectral Normalisation\nthe spectral norm of the weight matrix for every layer is equal to 1.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\nlabel 0 data\nlabel 1 data\n0\n20\n40\n60\n80\n100\n(a) Unregularised.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\nlabel 0 data\nlabel 1 data\n0\n1\n2\n3\n4\n5\n(b) Gradient penalty.\nK = 1.\n2\n1\n0\n1\n2\n2\n1\n0\n1\n2\nlabel 0 data\nlabel 1 data\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Spectral Normalisation.\nK = 1.\nFigure 6.5: The effect of regularisation on local smoothness. We plot the local Lipschitz\nconstants obtained using an exhaustive grid search in local neighbourhoods,\ninstead of loose bounds. We use different colours to emphasise the different\nscale of the constants for the different methods. While gradient penalties\nincrease smoothness, they do so primarily around the data, where the con-\nstraint is active, while Spectral Normalisation affects a large part of the input\nspace.\nregularisation on local smoothness we plot the Lipschitz constants of the model at\nneighbourhoods spanning the entire space in Figure 6.5. Each Lipschitz constant is\ncomputed using an exhaustive grid search inside each local neighbourhood rather\nthan a bound—details are provided in Appendix D.1. As expected, gradient penalties\nimpose stronger constraints around the training data, while Spectral Normalisation\nhas a strong effect on the smoothness around points in the entire space. This simple\nexample suggests that the search for better smoothness priors needs to investigate\n6.5. Challenges with smoothness regularisation\n145\nwhere we want functions to be smooth and re-examine how smoothness constraints\nshould account for the compositional aspect of neural networks, otherwise we run\nthe risk of learning trivially smooth functions. In the next chapter, we will first\nhand encouter the need to avoid enforcing too strong smoothness constraints when\napplying Spectral Normalisation to reinforcement learning.\n6.5.2\nInteractions between smoothness regularisation and\noptimisation\nWe show that viewing smoothness only through the lens of the model is misleading, as\nsmoothness constraints have a strong effect on optimisation. The interaction between\nsmoothness and optimisation has been mainly observed when training generative\nmodels; encouraging the smoothness of the encoder through Spectral Regularisation\nincreased the stability of hierarchical VAEs and led variational inference models\nto the state of the art of explicit likelihood non autoregressive models [85], while\nsmoothness regularisation of the critic (or discriminator) has been established as\nan indispensable stabiliser of GAN training, independently of the training criteria\nused [28, 33, 80, 201, 241].\nSome smoothness regularisation techniques affect optimisation by changing the\nloss function (gradient penalties, Spectral Regularisation) or the optimisation regime\ndirectly (early stopping, projection methods). Even if they don’t explicitly change\nthe loss function or optimisation regime, smoothness constraints affect the path the\nmodel takes to reach convergence. We use a simple example to show why smoothness\nregularisation interacts with optimisation in Figure 6.6. We use different learning\nrates to train two unregularised MLP classifiers on MNIST [242] and observe that\nthe learning rate used affects its smoothness throughout training, without changing\ntesting accuracy. Here, we measured the model’s smoothness as the upper bound on\nits Lipschitz constant given by product of spectral norms of learnable layers. These\nresults show that imposing similar smoothness constraints on two models that share\nthe same architecture but are trained with different learning rates would lead to\nvery different strengths of regularisation and drastically change the trajectory of\noptimisation. This is particularly true for methods such as Spectral Normalisation,\n6.5. Challenges with smoothness regularisation\n146\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest Accuracy\n0\n10\n20\n30\n40\n50\n60\n70\n80\nLipschitz constant upper bound\nlearning rate = 0.001\nlearning rate = 0.0005\nFigure 6.6: Smoothness constraints interact with learning rates. MNIST classifiers trained\nwith different learning rates exhibit different smoothness behaviour through-\nout training, even at the same test accuracy. We measured the model’s\nsmoothness as the upper bound on its Lipschitz constant given by product\nof spectral norms of learnable layers. This example shows that imposing the\nsame smoothness constraint on the same model architecture trained with\ndifferent learning rates will have different effects on optimisation dynamics.\nThis can be particularly the case for methods that impact the spectral norms\nof individual layers we measured here.\nwhich directly impact the spectral norms of individual layers we measured here.\nBeyond learning rates, smoothness constraints also interact decay rate hyper-\nparameters in optimisers that use momentum. When the Adam optimiser is used,\nthis is the β1 hyperparameter; for an overview of Adam and momentum we refer\nthe reader to Section 2.2. When training GANs, Gulrajani et al. [36] observed\nthat weight clipping in the Wasserstein GAN critic requires low to no momentum.\nWeight clipping has since been abandoned in the favour of other methods, but as we\nshow in Figure 6.7, current methods like Spectral Normalisation applied to GAN\ncritics trained with low momentum decrease sensitivity to learning rates but perform\npoorly in conjunction with high momentum, leading to slower convergence and higher\nhyperparameter sensitivity.\nWe have shown that smoothness constraints interact with optimisation parame-\nters, such as learning rates and momentum, and argue that we need to reassess our\nunderstanding of smoothness constraints, not only as constraints on the final model,\nbut as methods that influence the optimisation path. We will study this in detail in\nthe next chapter.\n6.6. Conclusion\n147\n0.0\n0.5\n1.0\n1.5\n2.0\nIteration\n1e5\n0\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\n7.5\n1 = 0.5\nSN discriminator\nVanilla discriminator\n(a) Momentum decay rate β1 = 0.5.\n0.0\n0.5\n1.0\n1.5\n2.0\nIteration\n1e5\n0\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\n7.5\n1 = 0.9\n(b) Momentum decay rate β1 = 0.9.\nFigure 6.7: Smoothness constraints interact with momentum decay rates: Spectral Nor-\nmalisation requires low momentum in GAN training. The effects of mo-\nmentum decay rate β1 in the Adam optimiser on Spectral Normalisation\napplied to the GAN discriminator on CIFAR-10, over a sweep of learning\nrates. Higher is better. Individual learning curves are obtained from a sweep\nover seeds and learning rates.\n6.5.3\nSensitivity to data scaling\nSensitivity to data scaling of smoothness constraints can make training neural network\nmodels sensitive to additional hyperparameters. Let f ∗be the optimal decision\nfunction for a task obtained from using i.i.d. samples from random variable X, and\nf ∗\nc obtained similarly from i.i.d. samples obtained from cX. Since f ∗and f ∗\nc can be\nhighly non linear, the relationship between the smoothness of the two functions is\nunclear. This gets further complicated when we consider their closest approximators\nunder a neural family. The effect of data scaling on the smooth constraints required\nto fit a model can be exemplified using the two moons dataset: with a Lipschitz\nconstraint of 1 on the model the data is poorly fit—Figure 6.3d—but a much better\nfit can be obtained by changing the Lipschitz constant to 10—Figure 6.3e—or scaling\nthe data—Figure 6.3f.\n6.6\nConclusion\nIn this chapter, we outlined the many benefits of neural network smoothness across\nmultiple problem domains. Motivated by these benefits and the wide use of smooth-\n6.6. Conclusion\n148\nness regularisation in deep learning, we highlighted areas where the effects of smooth-\nness have not been fully understood, from connections to the double descent phe-\nnomenon to interactions between smoothness regularisation and optimisation. We\ncontinue this line of inquiry in the following chapters.\nChapter 7\nSmoothness and optimisation\neffects of Spectral Normalisation\nDespite the many benefits of smoothness in supervised learning and generative\nmodelling outlined in the previous chapter, smoothness regularisation has only been\nbriefly studied in deep reinforcement learning (DRL). In this chapter, we study\nthe effects of Spectral Normalisation, a common smoothness regularisation method\ndescribed in Section 6.3, on DRL. We use Spectral Normalisation as it has shown\ntremendous success in GANs, and DRL and GANs share common characteristics, such\nas operating in non-stationary settings and challenges with optimisation [15, 30, 243,\n244]. We find that Spectral Normalisation can lead to improved performance in DRL,\nas long as it is only applied selectively to a few layers in order to avoid the reduced\ncapacity effects discussed in Section 6.5.1. Building upon results in Section 6.5.2,\nwe find interactions between Spectral Normalisation and optimisation dynamics,\nand show that in DRL the improvements brought by Spectral Normalisation can\nbe obtained by optimisation only changes.\nEquipped with this knowledge, we\nreinvestigate the effect of Spectral Normalisation on GANs, and observe that there\nSpectral Normalisation plays a compounded role, with model and optimisation\nchanges interacting with the choice of loss function in intricate ways.\n7.1\nIntroduction\nReinforcement Learning (RL) has made astounding progress in recent years, show-\ncasing the ability to play games where alternatives, such as handcrafted search or\n7.2. Reinforcement learning\n150\nbrute force approaches, are intractable [8, 10, 245]. While this progress could not\nbe achieved without the feature learning aspect of DRL due to a prohibitively large\nstate space or number of actions, research focus has been on RL-specific algorith-\nmic improvements, such as objective functions, exploration strategies, and replay\nprioritisation [117, 246–251]. We take a different approach, and focus on the deep\naspect of DRL, by investigating the interplay between training neural networks and\nRL objectives. We do so by exploring the use Spectral Normalisation in DRL; as\nwe have done in previous chapters, we hone in on interactions between smoothness\nregularisation and optimisation. To this effect, we make the following contributions:\n• We show that Spectral Normalisation, when applied to a subset of layers, can im-\nprove performance of DRL agents and match the performance of Rainbow [109],\na competitive agent combining multiple RL algorithmic advances.\n• We show that the effect of selectively applying of Spectral Normalisation to a\nsubset of layers is not solely a smoothness one, but an optimisation one, and\ncan decrease sensitivity to optimisation hyperparameters.\n• We introduce a set of adaptations to optimisation methods based on Spectral\nNormalisation, which without changing the model, can recover and sometimes\nimprove the results of Spectral Normalisation in DRL.\n• We use our new insights to re-examine the effects of Spectral Normalisation in\nGANs, where we observe that normalising the discriminator avoids loss areas\nof low or high gradient magnitudes, and that Spectral Normalisation interacts\nwith optimisation hyperparameters in important ways.\n7.2\nReinforcement learning\nRL translates the problem of acting sequentially in an environment in order to\nmaximise reward into a formal optimisation problem, usually defined via Markov\nDecision Processes (MDP). An MDP is a tuple (S, A, P, r, γ) with S a set of available\nstates, A a set of available actions (which throughout this thesis we will assume to\nbe discrete), P a conditional transition probability or density P : S × A × S →R+,\n7.2. Reinforcement learning\n151\nsuch that P(s′|s, a) defines the density to transition to state s′ from state s if action\na is taken by the agent, as well as the unconditional P(s0) defining the density over\ninitial states. For our use cases, we assume that P is not available in closed form\nbut can be sampled from by obtaining s′ ∼P(·|s, a) from an environment simulator.\nThe reward function r : S × A →R defines the reward r(s, a) obtained by the agent\nif it takes action a from state s and the discount factor γ ∈R, with γ ∈(0, 1). The\ngoal of RL is to find a policy π defined as the conditional probability distribution\nπ : S × A →R+, where π(a|s) encodes the probability of taking action a if in state s\nunder policy π, such that the time discounted reward over an episode is maximised:\nmax\nπ\nEP(s0)Eπ(a0|s0)\n\"\nr(s0, a0) +\n∞\nX\nt=1\nEP(st|st−1,at−1)Eπ(at|st)γtr(st, at)\n#\n.\n(7.1)\nDirectly optimising Eq (7.1) via Monte Carlo estimation can be challenging as it can\nlead to high variance estimates of the objectives, as well as slow learning in the case\nof long episodes [170]. Instead, it is common to define state-action value functions as\nQπ(s, a) = r(s, a) +\n∞\nX\nt=1,\na0=a,s0=s\nEP(st|st−1,at−1)Eπ(at|st)γtr(st, at).\n(7.2)\nState-action value functions satisfy the recurrence relation\nQπ(s, a) = r(s, a) + γEP(s′|s,a)Eπ(a′|s′)Qπ(s′, a′),\n∀s ∈S, a ∈A,\n(7.3)\nknown as the Bellman equation. The objective in Eq (7.1) can now be written as\nmax\nπ\nEP(s0)Eπ(a0|s0)Qπ(s0, a0).\n(7.4)\nIf we denote the optimal policy as π∗, another Bellman recurrence equation can also\nbe used to find the Q∗\nπ, namely the Bellman optimality equation\nQπ∗(s, a) = r(s, a) + γEP(s′|s,a) max\na′\nQπ∗(s′, a′),\n∀s ∈S, a ∈A.\n(7.5)\n7.2. Reinforcement learning\n152\nIterative updates ensuring Bellman optimality consistency of the current estimate Q\nof Qπ∗form the basis for Q-learning algorithms [252]. Since P(·|s, a) is unknown,\nthe expectation in Eq (7.5) is replaced with a Monte Carlo estimate obtained by\nperforming action a in state s and obtaining s′ from the environment; actions are\noften obtained using the ϵ-greedy policy\nπ(a|s) =\n\n\n\n\n\n1 −ϵ, if a = arg maxa′ Q(s, a′)\nϵ/(|A| −1), otherwise\n(7.6)\nand thus the Bellman optimality updates are evaluated at tuples\n(s, a ∼π(·|s), s′ ∼P(·|s, a), r = r(s, a)).\nWe note that while here we focus on value based, model-free, offline algorithms,\nother popular formulations of RL exist, and investigating the effect of smoothness\nfor other algorithm families, including actor-critic and model-based RL, is a worthy\narea of study; for an overview of RL algorithms, see Sutton and Barto [170].\n7.2.1\nDeep reinforcement learning\nWhile in traditional RL, often called tabular RL, when estimating a state-action\nvalue function individual values are approximated for each s ∈S and a ∈A, in\nDRL Q(s, a) is modelled using a neural network. The advantage of this approach is\ntwo fold: first, it makes large state and action spaces—which otherwise would be\nprohibitive due to memory requirements of storing a value for each state and each\naction—tractable; and second, it benefits from the generalisation provided by neural\nnetwork features by learning a similarity measure between states.\nOne approach to DRL is DQN [107], which adapts the Q-learning algorithm to\nthe neural setting, with the objective\nmin\nθ E(θ) = E(s,a,s′,r)∼R\n\u0010\nQ(s, a; θ) −\n\u0010\nr + γ max\na′\nQ(s′, a′; θ)\n\u0011\u00112\n,\n(7.7)\nwhere R is a replay buffer that stores experiences obtained by acting according\nto an ϵ greedy policy induced by Q.\nSince computing the maximum operator\nrequires computing Q(s, a)\n∀a ∈A, the neural critic implementation takes as input\n7.2. Reinforcement learning\n153\na state s and predicts using a forward pass all state-action value functions, thus\nimplementing Q(·; θ) : S →R|A|. Due to instabilities in training, the target value\nr+γ maxa′ Q(s′, a′; θ) is often replaced with a target network r+γ maxa′ Q(s′, a′; θold),\nwhere θold are a set of parameters obtained previously in training. The target network\nparameters θold get updated at regular intervals in training.\nMany improvements in DRL have focused on changing RL specific aspects of\ntraining, such as finding better approaches to sample from the replay buffer R, by\nweighing experiences either according to recency or estimated importance [246] [233,\n247]. Significant progress has also been achieved by changes in loss functions and\nparametrisations, particularly through a distributional approach to value based\nlearning [117, 253], which updates the Bellman operators to act on a distributional\nspace rather than in expectation as shown in Eq (7.3). The resulting DQN style agent\nobtained from the distributional perspective is called CategoricalDQN. Rainbow [251]\nis an agent collating many of the above improvements, by combining categorical losses,\nimproved prioritised replay strategies [246], exploration bonuses [247], and other RL\nspecific advances such as double Q-learning [254] and disambiguation of state-action\nvalue estimates [255]. We will show how similar performance improvements can\nbe obtained by focusing on the neural training aspects via changes to the critic\nmodelling the state-action value, instead of taking an RL centric view.\n7.2.2\nChallenges with optimisation in DRL\nDRL presents its own unique set of optimisation challenges within the deep learning\nfamily. One challenge stems from non-stationarity and the lack of a unique objective\nthroughout training: due to the expectation under the policy in loss functions, the\nloss function itself changes as training progresses; in this aspect, RL is similar to\nGANs, as the GAN loss also changes as the distribution induced by the generator\nchanges. In DRL the i.i.d. assumption required to obtain an unbiased estimate of the\ngradient (see Section 2.1.1) is also often violated, as data instances might come from\nthe same episode; furthermore it can be shown that in RL the parameter updates\nmight not form a gradient vector field [243, 244]. Both of these aforementioned\nissues are mitigated, but not fully resolved, through the use of replay buffers, which\n7.2. Reinforcement learning\n154\naccumulate transition pairs required to perform the Q-learning update (Eq 7.7)\nacross a long horizon of agent experience. The loss formulation in RL can present\nan additional challenge: consistency losses where the target value of the function\napproximator depends on itself, as can be seen in Eq (7.7), are harder to optimise\nthan standard regression losses where the regression target is provided by the dataset,\nas is the case in supervised learning.\nDespite these specificities of DRL, the optimisation algorithms used are those\nin standard deep learning and described in Section 2.2, which assume i.i.d. data\nand the existence of a unique function to optimise. Many a practical implication\nresult as a consequence, including sensitivity to hyperparameterameters [256], the\nlack of performance of regularisation methods that have been shown to improve\noptimisation in supervised learning [23], the heavy reliance on adaptive optimisers\nsuch as Adam and RMSprop compared to supervised learning where similar results\ncan be obtained with gradient descent with momentum [109, 257].\nInspired by the certain similarities between DRL and GANs, such as non-\nstationarity, together with the observed common challenges such as sensitivity to\nhyperparameters, we investigate the use of Spectral Normalisation to alleviate some\nof the optimisation challenges in DRL. In GANs, Spectral Normalisation has been\nshown to improve sensitivity to hyperparameters such as learning rate ranges [30],\nand has been integrated in many state of the art algorithms; for a wider overview of\nthe effects of Spectral Normalisation outside RL, we refer the reader to Chapter 6.\n7.2.3\nReinforcement learning environments\nWe evaluate agents on the Atari ALE benchmark [258], containing 51 Atari games.\nThe Atari benchmark is often used in DRL, and thus provides useful baselines to\nserve as a comparison against our methods. Here we use as baselines C51—the Cate-\ngoricalDQN agent introduced by Bellemare et al. [117] trained on Atari ALE—and\nthe Rainbow agent [109]—which improves C51 with additional enhancements, pro-\nviding a strong baseline on this environment. Unless otherwise specified, experiments\non Atari use the standard DQN critic architecture and the experimental setting\nprovided by the C51 baseline, shown in the Appendix in Table E.5 and Section E.2.5,\n7.3. Spectral Normalisation in RL\n155\nrespectively.\nWhen performing ablation studies using a large setting such as ALE is com-\nputationally prohibitive, and we thus use the MinAtar environment [259] instead.\nMinAtar reproduces five games from ALE without the visual complexity and has\nbeen found to be a good testing ground for reproducing ablations made using ALE\n[260]. Unless otherwise specified, MinAtar experiments use the critic architecture\ndescribed in Table E.3 in the Appendix.\n7.3\nSpectral Normalisation in RL\nOur aim is to investigate whether the application of Spectral Normalisation in DRL\ncan improve performance, as has been observed in other deep learning domains.\nTo do so, we use Spectral Normalisation applied to neural critics in Q learning\nalgorithms; for a description of Spectral Normalisation, see Section 6.3. The Spectral\nNormalisation formulation we use is largely based on the one proposed in the original\npaper Miyato et al. [30], with a few alterations: we do not backpropagate through\nthe power iteration operation; we only apply the normalisation if the spectral\nnorm of a layer is larger than 1; for convolutional layers we do not reshape the 4\ndimensional filter tensors to a two-dimensional matrix and compute the spectral norm\nof the newly obtained linear operator but estimate the of the original convolutional\noperator instead [198]1. Code available to reproduce our results is available at\nhttps://github.com/floringogianu/snrl.\n7.3.1\nApplying Spectral Normalisation to a subset of layers\nWhen we started experimenting with applying Spectral Normalisation to the DQN\ncritic, we applied Spectral Normalisation to all layers, as is standard in GANs [30]. We\nquickly noticed, however, that imposing the hard smoothness constraint on the state-\naction value function to be 1-Lipschitz can substantially decrease performance. As we\nhave observed in Section 6.5.1, enforcing a 1-Lipschitz constraint can reduce a model’s\n1This only applies to the RL experiments in this section, and not to any other Spectral\nNormalisation experiments in the thesis. We also note that the majority of the results in this\nsection apply normalisation to linear layers, yielding them comparable with the other presented\nresults.\n7.3. Spectral Normalisation in RL\n156\ncapacity to capture the decision surface and decrease performance (Figure 6.3d);\nrelaxing the constraint by forcing the model to be 10-Lipschitz can mitigate this\nloss of performance (Figure 6.3e). Similar observations have been made by Gouk\net al. [261], who propose introducing a hyperparameter that determines the strength\nof the Lipschitz constraint. We take a different approach here, where instead of\nchanging the Lipschitz constant as a hyperparameter, we choose to only normalise\ncertain layers of the network, most often just one. In particular we observe that\nnormalising the last layer of the network is detrimental. This suggests that imposing a\nsmoothness constraint on the Q function is prohibiting the model from approximating\nthe required decision surface. In the case of Atari games we study here this is not\nentirely unexpected, since similar input states can lead to a drastically different\ngame outcome. The expected reward of a particular action a can be determined\nby a few pixels, such as the location of the ball and paddle in Pong, and that of\nthe ghost in MS-PacMan. Thus, even for states s, s′ such that ||s −s′|| is small,\n||Q(s, a; θ) −Q(s′, a; θ)|| can be large, making the optimal Q function non-smooth.\nInstead of normalising the entire network, we instead normalise only a few\nlayers. Experimentally we found that Spectral Normalisation applied to a DRL critic\nperforms best on the layer with the largest number of parameters; for the standard\nDQN architecture this is the penultimate layer [107, 109]. We thus denote the index\nof the Spectral Normalised layers from the bottom of the network, i.e. SN[-1] will\nrefer to a critic where Spectral Normalisation is applied to the output layer and\nSN[-2] to the penultimate layer, while SN[-2,-3] refers to a network where Spectral\nNormalisation is applied both the penultimate layer and the one preceding it.\n7.3.2\nExperimental results on the Atari benchmark\nWe assess the effect of applying Spectral Normalisation to the critic both for the\nstandard DQN as well as its distributional counterpart (C51) when trained on the ALE\nbenchmark containing 54 Atari games and show results in Figure 7.1. We consistently\nobserve that SN[-2] improves over the baseline significantly. Furthermore, SN[-2]\nprovides a significant improvement over Rainbow [251], a competitive RL agent based\non C51 combining multiple competitive RL specific techniques. We further show\n7.3. Spectral Normalisation in RL\n157\n(a) Average HNS in training.\nAgent\nMean\nMedian\nDQN [255]\n216.84\n78.37\nDQN-Adam∗\n358.45\n119.45\nDQN-Adam SN[-2]\n719.95\n178.18\nC51 [251]\n523.06\n146.73\nC51 [117]\n633.49\n174.84\nC51∗\n778.68\n182.26\nRainbow [251]\n855.11\n227.05\nC51 SN[-2]\n1073.18\n248.45\n(b) Mean and Median HNS.\nFigure 7.1: Selectivly adding Spectral Normalisation to RL agents improves their per-\nformance. (a): Average Human Normalised Score (HNS) per time-step for\nC51 with Spectral Normalisation. (b): Mean and median Human Normalised\nScore on 54 Atari games with random starts evaluation. We observe that\napplying Spectral Normalisation to the penultimate layer of the critic leads to\na significant performance boost both for DQN and C51, and rivals algorithm\nimprovements obtained by Rainbow. References indicate the sources for the\nscores for each algorithm. We mark our own implementations of the baseline\nwith ∗. Agents are evaluated with the protocol in [251]; all results use an\naverage over 54 ALE games.\nin Figure 7.2a that SN[-2] leads to better results when trained longer, as opposed\nto the un-normalised DQN which plateaus earlier in training, and towards the end\nof training has a decrease in performance. Figures 7.2a and 7.2b show that, while\nnormalising the layer with the largest number of parameters—SN[-2]—performs best,\nnormalising other layers also significantly improves performance over the baseline\nDQN agent. For all experiments in Figure 7.1, when applying Spectral Normalisation\nwe use the same hyperparameters as the baseline, without retuning hyperparameters.\nThese results suggest that Spectral Normalisation can be added to existing performant\nDRL training configurations without requiring a new hyperparameter search.\nIn order to assess the performance of Spectral Normalisation beyond the standard\nDQN network, we perform a sweep over architectures and consistently observe that\nSpectral Normalisation improves performance when applied to a range of architectures,\nas shown in Figure 7.3; here we use 12 different model sizes of three different model\ndepths and four different layer widths.\n7.3. Spectral Normalisation in RL\n158\n(a) Atari environment.\n(b) MinAtar environment.\nFigure 7.2: Selectively applying Spectral Normalisation to one layer leads to significant\nimprovements over a DQN trained with Adam. While the baseline plateaus\nearly in training, when applying Spectral Normalisation the agents continue\nto improve throughout training. (a): Human Normalised Score for a DQN-\nAdam baseline with Spectral Normalisation applied on three different layers.\nAverage over 54 Atari games. (b): On a 15M steps training on MinAtar;\neach line is an average of 10 seeds over four MinAtar games and four different\nmodel sizes.\n7.3.3\nAn optimisation effect\nWhile selectively applying Spectral Normalisation to a subset of layers of a Q-learning\ncritic is a simple intervention, we have seen in the previous section that it has a strong\neffect on performance. We now aim to understand why Spectral Normalisation leads\nto these improvements. We first start with the smoothness hypothesis, considering\nwhether the performance improvement is due to an increased smoothness of the critic.\nWe note, however, that unlike other applications of Spectral Normalisation which\nnormalise the entire network, selectively applying Spectral Normalisation to a subset\nof layers as we do here does not lead to any guarantee regarding the smoothness\nof the entire critic. Indeed, since there are no constraints on the other layers, the\nsmoothness of those layers could increase throughout training to compensate for the\nconstraint on the normalised layers. When aggregating over the games in MinAtar\nwe observe 75% of applications of SN[-2] decrease the smoothness of the model—as\napproximated by the maximum norm of the critic’s Jacobian with respect to inputs\nover a large set of state-action pairs, for a connection between input Jacobians\n7.3. Spectral Normalisation in RL\n159\nFigure 7.3: Spectral Normalisation shows gains for all model sizes. Looking at\nthe baseline (\nDQN), we observe two performance regimes on MinAtar: for\nshallow, depth 3 models, performance increases with the width of the model;\nfor deeper models performance generally stagnates with increasing depth and\nwidth. In both regimes applying SN on individual (\n,\n) or multiple (\n)\nlayers improves upon the baseline suggesting a regularisation effect we could\nnot reproduce with other regularisation methods. Notice that the strong\nregularisation resulted from applying Spectral Normalisation to input layers\n(\n) or too many layers (\n) can however degrade performance. Each line is\nan average over normalised scores of 4 games × 10 seeds.\nand Lipschitz smoothness, see Section 6.2—when controlling over hyperparameters;\nfor certain games it is as high as 90%, while for others as low as 58%. We also\nobserve that applying Spectral Normalisation to more layers do not always increase\nsmoothness in Table 7.1. Furthermore, we notice that while applying Spectral\nNormalisation increased performance for all games in MinAtar, there is not a strong\neffect between smoothness and performance as measured by the Spearman-rank\ncorrelation between the smoothness of the learned model and the best performance\nobtained by the agent in all games, as we show in Table 7.2. To further assess the\neffect of the critic smoothness on performance, we used gradient penalties as a soft\nconstraint to encourage smoothness (see Section 6.3 for more details on gradient\npenalties), which did not result in any significant benefit compared to the baseline,\nas we show in Figure E.2 in the Appendix.\nThe above evidence suggests that the increased performance from the application\n7.3. Spectral Normalisation in RL\n160\nLess or no normalisation\nNo normalisation\nSN[-2]\nSN[-3]\nMore SN\nSN[-2]\n74.6%\n✗\n✗\nSN[-3]\n69.2%\n✗\n✗\nSN[-2, -3]\n75%\n57%\n60%\nTable 7.1: The percentage of training settings (architectures and seeds) for which applying\nSpectral Normalisation to more layers leads to smoother networks, as measured\nby the model’s Jacobian at peak performance. Selectively applying Spectral\nNormalisation to only a subset of layers does not consistently lead to an\nincrease in smoothness, and neither does applying Spectral Normalisation to\ntwo layers compared to only one layer. Results are obtained from a sweep over\narchitectures detailed in Table E.6, with 10 seeds each.\nGame\nSpearman Rank\nAsterix\n0.129\nBreakout\n0.199\nSeaquest\n0.151\nSpace Invaders\n0.453\nTable 7.2: Spearman Correlation coefficient between the negative norm of the Jaco-\nbian (our measure of decreased smoothness) and peak performance for each\ngame. Higher means more correlation. We don’t consistently observe a strong\ncorrelation between performance and increased smoothness across games.\nof Spectral Normalisation to certain critic layers in DRL might not be fully explain-\nable due to an increased smoothness effect. To better understand why Spectral\nNormalisation helps DRL, we turn our attention to the possibility of an increase in\nperformance due to an improvement in optimisation; we have already seen in Sec-\ntion 6.5.2 that Spectral Normalisation can interact with learning rate and momentum\ndecay rates in supervised learning and in GANs. In RL, we show in Figure 7.2 that\nusing Spectral Normalisation allows for longer training without early plateaus, and\nthat applying Spectral Normalisation to existing choice of hyperparameters leads to\nan increase of performance without requiring an additional hyperparameter sweep,\nas shown in the results in Figure 7.1. To investigate whether Spectral Normalisation\nhas an effect on hyperparameter sensitivity of DQN agents, we perform a large sweep\nover Adam hyperparameters learning rate h and ϵ (for an overview of Adam see\n7.3. Spectral Normalisation in RL\n161\nh\nh\nh\nh\nh\nh\nh\nh\nFigure 7.4: SN[-2] applied to DQN, Spectral Normalisation applied to the largest layer\nin the DQN critic, can significantly decrease sensitivity to the learning rate\nand ϵ hyperparameter of Adam. The cross product of learning rates in\nh ∈{0.00001, ..., 0.00215} and in ϵ ∈{0.00001, ..., 0.01} is explored.\nSection 2.2) and show that indeed, applying Spectral Normalisation increases the\nrange of performant hyperparameters significantly; results are shown in Figure 7.4.\nIn order to further investigate the effect of Spectral Normalisation in DRL\ntraining, we develop a set of methods which independently assess its effects. To this\nend, consider the neural network to which Spectral Normalisation is applied, which\nwe assume has N layers, with weights Wi ∈Rdi+1×di, bi ∈Rdi+1:\no = WNhN−1 + bN\n(7.8)\nhN−1 = a(zN−1) = a(WN−1hN−2 + bN−1)\n(7.9)\n· · ·\n(7.10)\nh1 = a(z1) = a(W1h0 + b1),\n(7.11)\nwith h0 = x and a is an activation function applied element-wise to the input vector.\nThe activation a has to be a rectifier, such as ReLU [262] or Leaky ReLU [263], since\nwe rely on a(σlx) = σla(x), with σl > 0. We can write our optimisation problem as:\nmin\nθ E(θ) = E(o; θ = {W1≤i≤N, b1≤i≤N}).\n(7.12)\nWe assess the effect of applying Spectral Normalisation to layer l, but note the same\n7.3. Spectral Normalisation in RL\n162\nderivation applies when multiple layers are normalised. We note that for simplicity\nwe assumed above that the model is an MLP, though the same computation carries\nfor convolutional layers. The analysis that follows relies on the following observation:\nRemark 7.3.1 In a neural network with recitifer activation functions, if the bias\nterm of a layer where Spectral Normalisation is applied gets adjusted to account for\nthe normalisation, Spectral Normalisation can be seen as scaling the output of the\nlayer by the inverse of the spectral norm σl.\nTo see why, consider hl+1 = a(Wlhl+1 + bl) and its normalised counterpart:\nh\n′\nl+1 = a(Wl/σlhl+1 + bl).\nSince we chose the activations function such that\na(h/σl) = a(h)/σl, we observe that if we also scale the bias bl by 1/σl, we can\nobtain h\n′\nl+1 = hl+1/σl. Furthermore, if we apply the same bias scaling to subsequent\nlayers bi →bi/σl, ∀i ≥l, we obtain that h′\ni = hi/σl, including o′ = o/σl. Thus if\nwe assume that the bias vectors can be scaled by a constant in training, the effect\nof Spectral Normalisation is to scale each activation following the normalisation\nby the inverse of the spectral norm of the normalised layer; if multiple layers are\nnormalised, activations are scaled by inverse of the product of the spectral norm of\nthe normalised layers. We note that the bias scaling assumption does not change how\nexpressive the model can be, as the bias term could learn such as scaling through\ntraining. Indeed, we test this assumption empirically and use a procedure we term\ndivOut, in which instead of applying Spectral Normalisation, we divide the model\noutput by the product of the spectral norms of the normalised layers ρ:\ndivOut[ρ] :\nmin\nθ E(θ) = E(\no\nQ\ns∈ρ σs\n; θ).\n(7.13)\nWe investigate where divOut can perform similarly to its Spectral Normalisation\ncounterparts and show in Figure 7.5 that in offline reinforcement learning divOut[ρ]\nand SN[ρ] have similar behaviours.\nWe now turn our attention towards the effects that Spectral Normalisation\nhas on the model gradients. To do so, we would like to compute\ndE\ndWi and\ndE\ndbi for\nnormalised and un-normalised networks. As before, for simplicity we consider the\n7.3. Spectral Normalisation in RL\n163\n(a) One normalised layer.\n(b) Multiple normalised layers.\nFigure 7.5: The effect of divOut, divGrad and mulEps on RL performance on MinAtar.\nDespite not changing the model, but only changing the optimisation procedure\nbased on the effect Spectral Normalisation has on parameter updates, divGrad\nand mulEps perform comparable to, or better than, Spectral Normalisation.\n(a) shows results obtained from simulating optimisation effects of Spectral\nNormalisation when applied to one layer, where divGrad and mulEps and\ndivOut obtain similar performance performance to Spectral Normalisation,\nand significantly improve over the baseline. (b) shows results obtained when\nlooking at the optimisation effects of applying Spectral Normalisation to\nmultiple layers; here Spectral Normalisation hurts performance, and so does\ndivOut, but divGrad and mulEps still significantly improve performance\ncompared to the baseline. This suggests that large changes model output\nobtained by Spectral Normalisation and divOut can be detrimental, while\noptimisation only changes are beneficial in that setting.\nsituation of one normalised layer l and we assume bias scaling. We then have\no′ = o/σl;\nh′\ni = hi/σl, ∀i ≥l;\nh′\ni = hi, ∀i < l.\n(7.14)\nWhile the computational graph only changes at layer l where the hl = a(Wlhl−1 + bl)\nis now replaced with\nh′\nl = a(z′\nl) = a(Wl/σlhl−1 + bl/σl) = a(Wlhl−1 + bl)/σl,\n(7.15)\nthe evaluation of the higher level derivatives will also change due to the scaling\neffect in Eq (7.14). To make the changes in the computational graph due to the\nnormalisation explicit, we will call the loss with the normalisation E′. We are\ninterested in computing\ndE′\ndWi and dE′\ndbi , and comparing it with the backpropagation in\n7.3. Spectral Normalisation in RL\n164\nan un-normalised model\ndE\ndWN\n= dE\ndo hT\nN−1\n(7.16)\ndE\ndhN−1\n=\ndo\ndhN−1\nT dE\ndo = WT\nN\ndE\ndo\n(7.17)\ndE\ndWi\n= dE\ndzi\nhT\ni−1 =\n\u0012 dE\ndhi\n⊙a′(zi)\n\u0013\nhT\ni−1\ni < N\n(7.18)\ndE\ndhi−1\n=\ndzi\ndhi−1\nT dE\ndzi\n= WT\ni\n\u0012 dE\ndhi\n⊙a′(zi)\n\u0013\ni < N.\n(7.19)\nWhen normalisation is applied, the computational graph does not change for\nthe layers following the normalisation compared to the un-normalised case, but the\nevaluation of the derivatives changes due to the scaling in Eq (7.14)\ndE′\ndo′ (o′) = dE\ndo (o/σl)\n(7.20)\ndE′\ndWN\n= dE′\ndo′ h′\nN−1\nT = dE′\ndo′\n\n\nhN−1/σl\n|\n{z\n}\n(7.14)\n\n\n\nT\n= 1\nσl\ndE′\ndo′ hT\nN−1\n(7.21)\ndE′\ndh′\nN−1\n= WT\nN\ndE\ndo (o/σl)\n(7.22)\ndE′\ndWi\n=\n\u0012dE′\ndh′\ni\n⊙a′(z′\ni)\n\u0013\nh′\ni−1\nT = 1\nσl\n\u0012dE′\ndh′\ni\n⊙a′(zi)\n\u0013\nhi−1\nT,\ni > l.\n(7.23)\nFor the normalised layer l, we have that due to Eq (7.15)\ndE′\ndWl\n= dE′\ndz′\nl\nh′\nl−1\nT/σl\n|\n{z\n}\nnormalisation,(7.15)\n= 1\nσl\n\u0012dE′\ndh′\nl\n⊙a′(zl)\n\u0013\nhT\nl−1\n(7.24)\ndE′\ndh′\nl−1\n=\n1\nσl\nWT\nl\n| {z }\nnormalisation,(7.15)\n\u0012dE′\ndh′\nl\n⊙a′(z′\nl)\n\u0013\n.\n(7.25)\nFor the layers before the normalisation, we have\ndE′\ndWi\n=\n\u0012dE′\ndh′\ni\n⊙a′(zi)\n\u0013\nhT\ni−1\ni < l\n(7.26)\ndE\ndh′\ni−1\n= WT\ni\n\u0012dE′\ndh′\ni\n⊙a′(zi)\n\u0013\ni < l.\n(7.27)\n7.3. Spectral Normalisation in RL\n165\nThus, when comparing the gradients obtained with the normalised network with\nthose in an un-normalised network, we observe two changes: first, a change in the\noutput Jacobian (Eq (7.20)), and second, a scaling effect which applies to all layers\nin the network, either through the scaling of the activations for layers following\nthe normalisation (Eq (7.23)), through the changes in the computational graph for\nthe normalised layer (Eq (7.24)), or through the effect of backpropagation of the\nscaling of\ndE′\ndh′\nl−1, for the layers preceding the normalisation (Eq (7.26)). A similar\nargument can be made for biases. While the change in Jacobian is difficult to obtain\nwithout affecting the model as we have done with divOut, to capture the gradient\nscaling effect we propose divGrad, a normalisation scheme based on the effects of\napplying Spectral Normalisation which does not change the model, but only changes\nthe optimisation procedure, by scaling the gradients of the parameters:\ndivGrad[ρ] :\ndE\ndθi\n→\n1\nQ\ni∈ρ σi\ndE\ndθi\n.\n(7.28)\nResults in Figure 7.5 show that divGrad performs well and that it can outperform\nSpectral Normalisation or divOut on MinAtar; further results supporting this\nhypothesis are shown in Figure E.5 in the Appendix.\nThe above observation regarding gradient scaling also allows us to intuit the\neffect of Spectral Normalisation in conjunction with optimisers like Adam (for a\ndetailed description of the Adam optimiser, see Section 2.2). If we consider that\nthe spectral norm of a weight matrices changes slowly in training (which we verify\nempirically in Figure E.4), under the bias scaling assumption, the effect of Spectral\nNormalisation on the Adam update can be seen as\nm′\nt = β1m′\nt−1 + (1 −β1)∇θE(θt−1)/σ ≈m′\nt/σ\n(7.29)\nv′\nt = β2vt−1 + (1 −β2)∇θE(θt−1)2 ≈v′\nt/σ2\n(7.30)\n∆θ′\nt = h\n1\n1−βt\n1m′\nt\nq\n1\n1−βt\n2v′\nt + ϵ\n= h\n1\n1−βt\n1mt\nq\n1\n1−βt\n2vt + σϵ\n,\n(7.31)\nwhere σ = Q\ni∈ρ σi, the product of the spectral norms of the normalised layers\n7.4. Revisiting Spectral Normalisation in GANs\n166\nand we used element-wise operations. Thus, the effect on the Adam update is\nreflected in a scaling of the hyperparameter ϵ by σ. To investigate this effect on\nthe Adam optimiser, we introduce mulEps, which changes the Adam update by\nmultiplying ϵ by the product of the spectral norms of the normalised layers. We\nshow in Figure 7.5 that mulEps performs similarly to selectively applying Spectral\nNormalisation in DRL, and can improve upon it. A strong effect is particularly\nobserved when mulEps and Spectral Normalisation are applied to multiple layers\n(Figure 7.5b), where Spectral Normalisation decreases performance compared to the\nbaseline, while mulEps increases it. Thus, mulEps, provides another example of\nhow we can use Spectral Normalisation to derive optimisation only changes that lead\nto improvements in performance in DRL.\n7.4\nRevisiting Spectral Normalisation in GANs\nBy analysing the effect of Spectral Normalisation in the previous section, we have\nuncovered multiple effects: model effects such as output scaling (Eq (7.20)), and\noptimisation effects such as gradient scaling (Eq (7.28)) and interactions with opti-\nmisation hyperparameters such as ϵ in Adam (Eq (7.31)). We saw that in DRL the\neffects of Spectral Normalisation can be recovered through the use of optimisation\nschemes, such as divGrad and mulEps, derived based on the optimisation only\neffects of Spectral Normalisation. Since Spectral Normalisation was first introduced\nfor GANs, and indeed is a key component of many successful GANs [33, 39], it is only\nnatural to ask whether the same optimisation methods perform equally well when\ntraining GANs. We studied GANs previously and described them in Section 4.1,\nand the interactions between smoothness normalisation and optimisation in GANs\nhave been discussed in Chapter 6. The application of Spectral Normalisation is\ndifferent in the GAN setting compared to RL, in that in GANs it is applied to\nevery layer of the model2. This difference between the DRL and GANs is significant,\nsince applying Spectral Normalisation on every layer ensures the model is Lipschitz\n2When Spectral Normalisation was introduced, it was only applied to the discriminator, but\nsince it has also been applied to the generator; here, for consistency with the Spectral Normalisation\nbaseline in Miyato et al. [30], we apply it only to the discriminator.\n7.4. Revisiting Spectral Normalisation in GANs\n167\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception Score\nNon-saturating loss\n(a) Spectral Normalisation.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception Score\nNon-saturating loss\n(b) No normalisation.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nNon-saturating loss\n(c) MulEps.\nFigure 7.6: Optimisation changes based on Spectral Normalisation do not recover the\nperformance of Spectral Normalisation in GANs. Different units indicate\ndifferent learning rates for the discriminator and generator.\n(at least when Spectral Normalisation is applied on the linear operator given by\nconvolutional layers and not on the reshaped filters), while applying it to only one\nlayer leads to no such assurances, as we have seen in the previous section. This\ndifference in application of Spectral Normalisation suggests perhaps that the change\nin the model is important in GANs. To test this intuition, we experimented with\noptimisation only methods divGrad and mulEps in GAN training. We note that as\nthe generator’s backpropagation requires a backward pass through the discriminator,\nany normalisation applied to the discriminator in mulEps and divGrad will also\naffect the generator, as was the case with un-normalised early layers in the RL critic\n(see Eqs (7.31) and (7.28)). Results in Figure 7.6 confirm that indeed, methods that\nonly make changes to optimisation dynamics and do not account for model changes\ninduced by Spectral Normalisation do not recover its performance in GAN training.\n7.4.1\nSpectral Normalisation and the discriminator output\nSince optimisation only changes do not recover the performance of Spectral Nor-\nmalisation in GAN training, we now investigate whether Spectral Normalisation\nalters the loss prediction landscape in a substantial way. We start by considering\nthe effect of Spectral Normalisation on the range of discriminator outputs. Since\nSpectral Normalisation applies a global constraint on the entire space, this applies\nbetween pairs of data instances, pairs of samples, and pairs of data and samples.\nIf we write D(·; ϕ) = ς(DL(·; ϕ)), with ς being the sigmoid function, Spectral Nor-\n7.4. Revisiting Spectral Normalisation in GANs\n168\n10\n5\n0\n5\n10\nDL(x)\n0.00\n0.25\n0.50\n0.75\n1.00\nD(x) = (DL(x))\nNo normalisation\nsamples\ndata\n(a) No normalisation.\n10\n5\n0\n5\n10\nDL(x)\n0.00\n0.25\n0.50\n0.75\n1.00\nD(x) = (DL(x))\nSpectral Normalisation\nsamples\ndata\n(b) Spectral Normalisation.\nFigure 7.7: Visualisation of the effect of Spectral Normalisation on discriminator predic-\ntions: Spectral Normalisation is making the discriminator output less likely\nto reach the saturating areas of the sigmoid function, denoted here by ς.\nmalisation ensures that DL(·; ϕ) is 1-Lipschitz by normalising each layer in the\ndiscriminator’s network. Given a generator sample G(z; θ) and a data instance\nx, ∥DL(G(z; θ); ϕ) −DL(x; ϕ)∥2 is bound by ∥G(z; θ) −x∥2. Thus, while with no\nnormalisation the discriminator has an easier task of separating the data and samples\nby using the saturating areas of the sigmoid function, this is harder to do when\nSpectral Normalisation is used; we visualise this effect in Figure 7.7.\nWe now turn to empirically investigating this effect in GAN training. We\nmeasure the output of the discriminator throughout training both on samples and\ndata batches and create a binned-histogram of the outputs obtained from a batch.\nThat is, for each xi in a data batch with D(xi; ϕ) ∈[0, 1] we assign it the interval\n[0.1p, 0.1(p + 1)] with p ∈{0, ..., 9} if D(xi; ϕ) ∈[0.1p, 0.1(p + 1)]; we do the same\nfor samples D(G(zi; θ); ϕ) where zi are the latent vectors used to obtain the sample\nbatch. This allows us to measure the variability of the discriminator prediction\ninside data and sample batches; we measure the entropy of the discriminator output\nacross a batch with and without Spectral Normalisation by measuring the entropy of\nthe distribution induced by the histogram. We show aggregate results throughout\ntraining over a hyperparameter sweep in Figure 7.8, and individual results with\na fixed set of hyperparameters in Figures 7.9 and 7.10. As we intuited, Spectral\nNormalisation increases the entropy of the discriminator output throughout training,\nboth on data (Figure 7.8b) and samples (Figure 7.8c). Early in training (Figure 7.9),\n7.4. Revisiting Spectral Normalisation in GANs\n169\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception Score\nNo Spectral Nornalisation\nSpectral Normalisation\nNon-saturating baseline\n(a) Performance.\n0\n1\n2\n3\nIteration\n1e5\n0.0\n0.5\n1.0\n1.5\n2.0\nEntropy of D(x)\nNo Spectral Nornalisation\nSpectral Normalisation\nMaximum entropy value\n(b) Entropy on data.\n0\n1\n2\n3\nIteration\n1e5\n0.0\n0.5\n1.0\n1.5\n2.0\nEntropy of D(G(z)\nNo Spectral Nornalisation\nSpectral Normalisation\nMaximum entropy value\n(c) Entropy on samples.\nFigure 7.8: Spectral Normalisation increases the entropy of the discriminator’s predictions\non data (b) and samples (c). The results are obtained over a sweep of learning\nrates.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nD(x) probability bucket in [0, 1]\n0\n20\n40\n60\n80\n100\nNumber of examples in batch\nEarly in training\nNo Spectral Normalisation\nSpectral Normalisation\n(a) Data.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nD(G(z)) probability bucket in [0, 1]\n0\n20\n40\n60\n80\n100\nNumber of examples in batch\nEarly in training\nNo Spectral Normalisation\nSpectral Normalisation\n(b) Samples.\nFigure 7.9: Early in training. Spectral Normalisation increases the entropy of the\ndiscriminator’s predictions on data (a) and samples (b) in the early stages of\ntraining. Without Spectral Normalisation the discriminator is confident that\nthe generator samples are fake, with most samples being assigned less than\n0.3 probability of being real.\ndiscriminators trained with Spectral Normalisation are less likely to assign high\nprobability to real data being real, or to samples being fake.\nLate in training\n(Figure 7.10), GANs trained with Spectral Normalisation also exhibit higher entropy\nof discriminator outputs; this end of training behaviour is more consistent to that of\na Nash equilibrium (see Section 2.4 for a definition and discussion on Nash equilibria),\nwhere the discriminator should not be able to distinguish between samples and data.\nGiven the effect of Spectral Normalisation on the discriminator’s predictions,\nit’s natural to ask how this impacts training. We turn to this question next.\n7.4. Revisiting Spectral Normalisation in GANs\n170\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nD(x) probability bucket in [0, 1]\n0\n20\n40\n60\n80\n100\nNumber of examples in batch\nEnd of training\nNo Spectral Normalisation\nSpectral Normalisation\n(a) Data.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nD(G(z)) probability bucket in [0, 1]\n0\n20\n40\n60\n80\n100\nNumber of examples in batch\nEnd of training\nNo Spectral Normalisation\nSpectral Normalisation\n(b) Samples.\nFigure 7.10: Late in training. Spectral Normalisation increases the entropy of the\ndiscriminator’s predictions on data (a) and samples (b) in the late stages\nof training. Without Spectral Normalisation the discriminator is confident\nthat real data is real and generator samples are fake.\nWhen Spectral\nNormalisation is used, the discriminator assigns both data and samples a\nwide range of probability of being real.\n7.4.2\nInteractions with loss functions\nTo examine the effect of the increased entropy on the discriminator’s output, we use\ntwo formulations of GAN losses: the non-saturating loss (Eq (4.44)), more commonly\nused in practice and in the above experiments; and the original zero-sum formulation\nof GANs (Eq (4.4)), which has been known to be harder to train than other losses.\nThe low performance of this original loss has been attributed to the lack of learning\nsignal the generator early in training, since when the generator performs poorly and\nthe discriminator easily classifies its data as fake, the discriminator does not provide\nstrong gradients for the generator to improve, as we visualise in Figure 7.11; for this\nreason, the generator loss in this formulation is sometimes called the ‘saturating loss’.\nMoreover, strong gradients are provided when the generator is doing well, which can\nlead to unstable behaviour and exiting areas of good performance. Figure 7.11b also\nhighlights why the increased entropy effect in the discriminator’s output induced\nby Spectral Normalisation—and specifically the decreased probability of assigning\noutputs close to 0 or 1 observed in Figures 7.9 and 7.10—can be beneficial, as for\nboth losses, it avoids areas with large or small gradient magnitudes.\nBy analysing the interaction between the loss function and the behaviour of\nthe discriminator, we can now make predictions about the empirical behaviours\n7.4. Revisiting Spectral Normalisation in GANs\n171\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nD(G(z))\n4\n2\n0\n2\n4\nGenerator loss Lg\nsaturating (zero-sum): log (1 - D(G(z)))\nnon saturating: - log  D(G(z))\n(a) Loss.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nD(G(z))\n100\n101\n102\n|\nd LG\nd D(G(z))|\nsaturating (zero-sum): log (1 - D(G(z))) \nnon saturating: - log  D(G(z))\n(b) Gradients.\nFigure 7.11: Understanding the landscape of GAN losses and gradients. (a): comparing\nthe saturating and non-saturating generator loss. (b): unlike the satu-\nrating loss, the non-saturating loss provides stronger gradients when the\ndiscriminator can easily classify generated data as fake.\nof the two losses.\nFor the saturating loss, without Spectral Normalisation the\nchallenge is that early in training there is no learning signal for the generator; we\nthus would expect that without Spectral Normalisation the performance is low, and\nthat Spectral Normalisation improves is significantly by avoiding the saturated areas\nearly in training. We observe this in Figure 7.12, which shows that without Spectral\nNormalisation the saturating loss exhibits extremely poor performance, but that\nadding Spectral Normalisation significantly decreases the gap between the saturating\nand non-saturating losses. For the non-saturating loss, we expect that without\nSpectral Normalisation, the discriminator can be too confident in its prediction that\ngenerated data is fake late in the game, as we observed in Figure 7.10b. This leads\nto large gradients—see Figure 7.11b—which can destabilise training and exit areas of\ngood performance. This expectation is consistent with what we observe empirically\nin Figure 7.13: adding Spectral Normalisation improves stability later in training.\nWe are now ready to return to the optimisation methods we derived in Sec-\ntion 7.3.3 based on Spectral Normalisation. Since divOut captures the changes\nin the model output induced by Spectral Normalisation, we would expect divOut\nto perform well for GANs trained on CIFAR-10. In particular, by dividing the\ndiscriminator output by the product of the spectral norms of the discriminator’s\n7.4. Revisiting Spectral Normalisation in GANs\n172\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSaturating loss\n(a) Spectral Normalisation.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception Score\nSaturating loss\n(b) No normalisation.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSaturating  loss\n(c) DivOut, discriminator update only.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSaturating loss\n(d) DivOut.\nFigure 7.12: Saturating loss. Spectral Normalisation improves performance of GANs\ntrained with the saturating loss. divOut obtains similar performance as\nSpectral Normalisation, but only when applied to both the discriminator\nand the generator. Only applying the output scaling from divOut to the\ndiscriminator only leads to extremely poor performance, as it does not\naffect the generator update, and thus does not avoid the areas with poorly\nconditioned generator gradients. Different units indicate different learning\nrates for the discriminator and generator. The training optimiser is Adam,\nused with simultaneous updates.\nweights, divOut captures the intuition present in Figure 7.7: it ensures that data\nand generator samples are less likely to be separated and reach the saturating areas\nof the sigmoid function. Indeed, as we show in Figures 7.12 and 7.13, divOut\nobtains a similar performance to Spectral Normalisation both for the saturating\nand non-saturating loss. Importantly, however, when applying divOut only to the\ndiscriminator update, divOut obtains extremely poor performance in the case of the\nsaturating loss as we show in Figure 7.12c, but still performs on par with Spectral\nNormalisation when the non-saturating loss is used, as seen in Figure 7.13c. This\n7.4. Revisiting Spectral Normalisation in GANs\n173\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception Score\nNon-saturating loss\n(a) Spectral Normalisation.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception Score\nNon-saturating loss\n(b) No normalisation.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nNon-saturating loss\n(c) DivOut, discriminator update only.\n0\n1\n2\n3\n4\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nNon-saturating loss\n(d) DivOut.\nFigure 7.13: Non-saturating loss. DivOut obtains similar performance to Spectral\nNormalisation for GANs on CIFAR-10 when the non-saturating loss is\nused. Different units indicate different learning rates for the discriminator\nand generator. Only applying the output scaling from divOut to the\ndiscriminator leads to similar performance as when applying it to both the\ndiscriminator and the generator, since the non-saturating loss makes the\ngenerator less susceptible to the scale of the discriminator output, unlike\nfor the saturating loss as we saw in Figure 7.12. The training optimiser is\nAdam, used with simultaneous updates.\nfurther confirms that in the case of the saturating loss, one of the effects of Spectral\nNormalisation is to change the conditioning of the generator by avoiding the areas\nwith low magnitude gradients early in training. We are also ready to revisit the\nmulEps results with the non-saturating loss presented in Figure 7.6; since mulEps\ndoes not capture the changes in discriminator output, it will not be able to avoid\nthe decrease in performance observed when no Spectral Normalisation is used.\nWe thus see that in the GAN case, there is a strong effect in model training\nbrought by Spectral Normalisation, which does not come from an only optimisation\n7.4. Revisiting Spectral Normalisation in GANs\n174\nchange. We now turn our attention to the following question: even if in GANs Spectral\nNormalisation does not lead to a primarily optimisation effect, are GANs potentially\nbenefiting from the optimisation changes brought by Spectral Normalisation?\n7.4.3\nHypothesis: Adam in the low-curvature regime\nWe have seen in previous sections that Spectral Normalisation can be seen as acting\nto multiply the hyperparameter ϵ of Adam (Eq (7.31)). Since the spectral norms\nof weight matrices grow in training, this can be seen as an anneal of ϵ from small\nto large. In the previous section we have seen that in the GAN case the scheduler\nderived from this observation, MulEps, does not recover the performance of Spectral\nNormalisation in GANs. Now we ask whether the ϵ annealing behaviour of Spectral\nNormalisation can still have a positive effect on GAN training, despite not being the\nonly effect.\nWe start with a few observations. First, when a small ϵ is used, for dimensions\nof low-curvature Adam leads to a constant update given by the learning rate. Second,\nGANs trained with Adam exit the area of high performance, even when Spectral\nNormalisation and the non-saturating losses are used, as we show in Figure 7.6.\nThird, this behaviour is not observed when gradient descent or Runge–Kutta4 are\nused (see Figure 4.8 and Qin et al. [67]), so this behaviour seems specific to the use\nof the Adam optimiser. These observations lead to us to postulate the following\nhypothesis: Is Adam exiting the areas of high performance in GAN training because\nmany parameters are in a low-curvature regime, and taking a step of size given by\nthe learning rate is then leading to an exit from the high performance area?\nTo first understand the effect of ϵ in Adam in the low-curvature regime, we use\nthe arguments from Section 2.2. If we are in a low-curvature regime for parameter\nθ[l] ∈R, then by definition its gradient ∇θ[l]E does not change between iterations.\nWe use this to expand mt in the Adam update, defined in Eq (2.23), at iteration t:\n(mt)[l] = (1 −β1)\nt\nX\ni=1\nβi−1\n1\n∇θ[l]E(θt−i) ≈(1 −β1)\nt\nX\ni=1\nβt−i\n1 ∇θ[l]E\n(7.32)\n= (1 −βt\n1)∇θ[l]E.\n(7.33)\n7.4. Revisiting Spectral Normalisation in GANs\n175\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGradient magnitude\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nUpdate magnitude\nh\n0.0001\n0.01\n0.1\n(a) ϵ = 10−8.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGradient magnitude\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nUpdate magnitude\n1\n0.1\n0.01\n0.0001\n1e-06\n1e-08\n(b) Before applying fixed h.\nFigure 7.14: The effect of the learning rate (a) and ϵ (b) on the magnitude of the update\nin the low-curvature regime in Adam: while the learning rate scales the\ngradient by a constant, the effect of ϵ strongly depends on the magnitude\nof the gradient, particularly for large ϵ.\nWith the same arguments we have\n(vt)[l] ≈(1 −βt\n2)(∇θ[l]E)2.\n(7.34)\nWe thus have that the Adam update for parameter θ[l] is\n∆θ[l] = h\n1\n1−βt\n1(mt)[l]\nq\n1\n1−βt\n2(vt)[l] + ϵ\n≈h\n∇θ[l]E\nq\n(∇θ[l]E)2 + ϵ\n.\n(7.35)\nThis result shows the importance of ϵ in Adam in the low-curvature regime: if ϵ\nis small then the update is given by the learning rate h; this makes the update\nindependent of the gradient magnitude. We plot this in Figure 7.14b, and show how\nincreasing ϵ can reinstate the dependence on gradient magnitudes. Importantly, even\nif the model is in the neighbourhood of a local equilibrium and the gradient is close\nto 0, the size of the Adam update with default ϵ = 10−8 is given by the learning rate.\nThus, if there are many parameters in the low-curvature regime around an area of\nhigh performance, Adam can exit that area of high performance by taking large steps\nwhen gradients are small; we have seen examples of this behaviour in a simple setting\nin Figure 2.3. Since SGD and Runge–Kutta4 only use local information about the\ngradient, they would not exit the high performance area if gradients are small there.\n7.4. Revisiting Spectral Normalisation in GANs\n176\n2\n4\n6\nInception Score\nLearning rates\n0.0004,0.0004\n0\n50\n100\n% of params\nDiscriminator\nsmall curvature\nlarge curvature\n0\n1\n2\n3\n4\n5\nIteration\n1e5\n0\n50\n100\n% of params\nGenerator\nFixed = 10\n8 (default)\n(a) ϵ = 10−8.\nAt peak: ϵ = 10−8.\n2\n4\n6\nInception Score\nLearning rates\n0.0004,0.0004\n0\n50\n100\n% of params\nDiscriminator\nsmall curvature\nlarge curvature\n0\n1\n2\n3\n4\n5\nIteration\n1e5\n0\n50\n100\n% of params\nGenerator\n annealed from 10\n8 to 10\n4.\n(b) ϵ annealed from 10−8 to 10−4.\nAt peak: ϵ ≈10−5.\n2\n4\n6\nInception Score\nLearning rates\n0.0004,0.0004\n0\n50\n100\n% of params\nDiscriminator\nsmall curvature\nlarge curvature\n0\n1\n2\n3\n4\n5\nIteration\n1e5\n0\n50\n100\n% of params\nGenerator\n annealed from 10\n8 to 10\n2.\n(c) ϵ annealed from 10−8 to 10−2.\nAt peak: ϵ ≈10−3.\nFigure 7.15: The effect of ϵ on Adam performance. Increasing ϵ in training ensures that\nAdam does not exit areas of good performance, even with many parameters\nin the low-curvature regime, particularly for the discriminator. We note that\nusing a large ϵ from early in training leads to extremely low performance\n(likely due to small gradients training does not pick up) and we do not show\nit here.\nIn order to understand whether this effect occurs when training GANs in practice,\nwe need to measure whether there are many parameters in a low-curvature regime,\nas well as investigate the effect of ϵ on GAN training. To answer the first question,\nwe measure\n\f\f\f\f∇θ[l]E(θt) −\n1\n1 −βt\n1\n(mt)[l]\n\f\f\f\f\nand\n\f\f\f\f\n\u0010\n∇θ[l]E(θt)\n\u00112\n−\n1\n1 −βt\n2\n(vt)[l]\n\f\f\f\f\n(7.36)\nfor all discriminator and generator parameters. These quantities are readily avail-\nable when using Adam, we know they are 0 for parameters in areas of low cur-\nvature (Eq 7.33 and Eq 7.34), and we know how they affect the Adam update\n(Eq 7.35). We define a parameter as being in an area of low curvature at iteration t if\n\f\f\f∇θ[l]E(θt) −\n1\n1−βt\n1(mt)[l]\n\f\f\f < 10−5 and\n\f\f\f\f\n\u0010\n∇θ[l]E(θt)\n\u00112\n−\n1\n1−βt\n2(vt)[l]\n\f\f\f\f < 10−5. Results\nin Figure 7.15 show that when training GANs with Adam on CIFAR-10 many pa-\nrameters are in the low-curvature regime when the high performance area is reached,\nparticularly for the discriminator. When using ϵ = 10−8, Figure 7.15a shows that\n7.5. Related work\n177\nafter the area of high performance is reached, the model exits it and performance\ndegrades; this is consistent with our hypothesis that, due to large updates in areas\nof low curvature, Adam using a small ϵ can exit areas of high performance. By\nannealing ϵ throughout training we can ensure that a larger ϵ is used areas of high\nperformance, and thus the Adam update is more sensitive to the gradient magnitude,\nespecially for parameters with small gradients, as we show in Figure 7.14b. We show\nresults in GAN training by annealing the Adam ϵ in Figures 7.15b and 7.15c: the\nrate at which the high performance area is left decreases, and for ϵ annealed to as\nhigh as 10−2, the high performance area is no longer exited.\nThese results suggest that indeed, even though Spectral Normalisation does\nnot have a purely optimisation effect, its annealing of ϵ when using the Adam\noptimiser has a beneficial effect, by avoiding large steps in areas of high performance.\nFurthermore, these results are consistent with what we observed in the DRL case,\nwhere Spectral Normalisation ensures that performance increases for longer in training\n(Figure 7.2). While here we focused on the effect of ϵ in the low-curvature regime, we\nwant to emphasise that this is not the only effect of ϵ, but simply one easy to analyse.\nAs ϵ dampens the size of parameter updates regardless of curvature, increasing ϵ has\nan effect on all parameters. From that perspective, increasing ϵ in training is akin\nto decreasing to learning rate. We do note significant differences, however: as we\nshow in Figure 7.14a, a small ϵ in the low-curvature regime always leads to a lack of\nsensitivity to the gradient magnitude, regardless of learning rate, while changing ϵ\ncan lead to increased sensitivity to gradient magnitudes. We believe more studies in\nthe role of ϵ in Adam are needed, specifically in the context of DRL, where it has\nempirically been shown (here and elsewhere) to play an important role.\n7.5\nRelated work\nFor an overview of the use of smoothness regularisation and normalisation in deep\nlearning, together with benefits and downsides, see Chapter 6. The relaxation of\nthe 1-Lipschitz constraint in the application of Spectral Normalisation has been\npreviously done by Gouk et al. [261], who relax the Lipschitz constant but in contrast\n7.5. Related work\n178\nto our approach in DRL, still regularise the entire network.\nOther normalisation methods, such as Weight Normalisation and Batch Normal-\nisation have been studied from an optimisation perspective [23, 264]. Unlike Spectral\nNormalisation however, these normalisation techniques are not hard normalisations,\nas their parametrisation allows recovering the un-normalised approaches. Weight\nNormalisation [264] decouples the learning of the Frobenius norm of the matrix\nfrom its direction; in DRL, the authors show that applying Weight Normalisation\nimproves the performance of DQN on a subset of Atari games. When introduc-\ning Spectral Normalisation, Miyato et al. [30] compare Spectral Normalisation and\nWeight Normalisation empirically and in the GAN case obtain significant gains using\nSpectral Normalisation compared to Weight Normalisation (their Table 2). The\nconnection between optimisation hyperparameters in Adam and normalisation—as\nwe have done with mulEps—has been previously made for Batch Normalisation and\nWeight Normalisation [265–267]; in contrast to our approach, these works require\nL2 regularisation and obtain the ϵ scaling effect jointly with a learning rate scaling\neffect, while we obtained no change on the learning rate (Eq (7.31)). The connection\nbetween ϵ and low-curvature is connected to existing works which track curvature\nduring training [167].\nWhile optimisation specific issues in DRL have long been overlooked, recent\nworks have studied the negative interplay between temporal difference learning\nmethods such as Q-learning and adaptive optimisation methods including RMSprop\nand Adam [244], while others propose optimisers for DRL, but they are either\nrestricted to linear estimators [268, 269], or they show no empirical advantages [270].\nInspired by the insights obtained when investigating the effects of Spectral\nNormalisation in DRL, we also analysed its effects on GANs. A study of the effects\nof Spectral Normalisation was similarly performed recently, with Lin et al. [271]\nexploring the effects of Spectral Normalisation theoretically and uncovering its\nconnections with exploding and vanishing gradients. Their work is related to our\napproach, which similarly uncovered the effects of Spectral Normalisation on training\ndynamics, as well as explored the effect of optimisation methods based on Spectral\n7.6. Conclusion\n179\nNormalisation, such as divOut, and the interaction with loss function choices and\noptimisers such as Adam.\n7.6\nConclusion\nWe investigated the use of Spectral Normalisation in DRL and showed that normal-\nising one layer of the learned neural critic in Q-learning agents can substantially\nimprove performance over methods which collate multiple RL specific improvements.\nWe then investigated why Spectral Normalisation helps DRL. By using the com-\npositional structure of neural networks we uncovered strong optimisation effects,\nincluding interactions with optimiser hyperparameters. Inspired by these optimisation\neffects, we developed novel optimisation updates that recover, or even outperform,\nthe effects of Spectral Normalisation in DRL. Equipped with this knowledge we\nre-examined the effects of Spectral Normalisation in GANs, another domain where\nSpectral Normalisation has been applied with success, but where the normalisation\nprocedure is applied to the entire network. We observed that for GANs optimisation\nonly changes do not recover Spectral Normalisation performance, and that a strong\neffect of Spectral Normalisation is avoiding poor-conditioned areas of loss functions.\nDespite not recovering its performance however, optimisation changes due to Spectral\nNormalisation can help training when the Adam optimiser is used, particularly for\nparameters which exhibit low-curvature during training.\nChapter 8\nGeometric Complexity: a\nsmoothness complexity measure\nimplicitly regularised in deep\nlearning\nMachine learning methods have long faced the challenge of finding the right balance\nbetween fitting the training data and generalising beyond said training data [272].\nThe need to avoid overly complex models that overfit by memorising training datasets\nhas led to the need to measure model complexity, closely followed by the desire to\nconstruct regularisation methods that control said model complexity. The aim of\ncomplexity measures is thus to capture generalisation (often measured via test set\nerror, where lower is better) via a U-shaped curve: when complexity is low, the\nmodel is underfitting the training data and underperforming on the test data leading\nto a high test error (high bias), but as complexity increases the test error decreases,\nfollowed by an increase in test error once the model is overly complex and overfitting\n(high variance); we have observed this behaviour in Figure 6.1a of Chapter 6. While\nthis traditional U-shaped curve has been long associated with complexity measures\nin machine learning, deep learning has led to a few puzzling observations. For deep\nor over-parametrised networks, overfitting is less of an issue than we would expect\nunder the traditional U-shape curve, and the models both fit and the data and\n181\ngeneralise [79, 273, 274]. Deep neural networks present an additional challenge for\ncomplexity measures, as many existing measures are either too simplistic, such as\nnumber of learnable parameters or weight norms, or intractable to compute for the\nneural network class of models, such as classical measures like Rademacher [275, 276]\ncomplexity and VC dimensions [203, 277].\nRecent work has also uncovered connections between optimisation and gener-\nalisation in deep learning, while previously optimisation was viewed purely from\na training objective minimisation perspective; these works broadly fall under the\numbrella of implicit regularisation [12, 13, 65, 278, 279]. Since many existing com-\nplexity measures measure the capacity of a family of functions [203, 275, 277], they\ncannot capture implicit regularisation effects induced by optimisation inside a model\nclass, such as the implicit regularisation induced by discretisation drift we studied in\nprevious chapters.\nMotivated by the need to reconcile complexity measures and deep learning, and\nemboldened by our previous study of implicit regularisation and interactions between\nsmoothness and optimisation, we make the following contributions:\n• We introduce a new measure of model complexity for deep neural networks,\nwhich we call Geometric Complexity (GC). GC captures the complexity of a\nmodel rather than function class, can be computed exactly during training,\nand has connections to Lipschitz smoothness.\n• We show that GC is connected with or implicitly regularised by many aspects\nof deep learning, including initialisation, explicit regularisation, and implicit\nregularisers induced by the discretisation drift of optimisation methods.\n• We show that through implicit regularisation induced by discretisation drift,\nthe implicit minimisation of GC in neural network training can be strengthened\nthrough hyperparameter choices such as batch size or learning rate.\n• We show that GC captures the double descent phenomenon, and restores the\nclassical complexity versus performance U-shaped curve.\n8.1. Introduction\n182\n8.1\nIntroduction\nExisting complexity measures. Many existing complexity measures target the\ncomplexity of a class of functions. These types of complexity measures range from\nthe very simple, such as number of learned parameters, to more sophisticated, such\nas Rademacher complexity and VC dimensions [203, 275–277, 280]. While convenient\nand inexpensive, using the number of learned parameters as a complexity measure\ndoes not account for architectures and expressivity; when using the number of param-\neters to measure the capacity of a neural network, the now famous double descent\nphenomenon [77–79] occurs in deep learning: as the number of parameters exceeds a\ncritical threshold, the test error of the model decreases again, as opposed to increasing\nas expected under the U-shaped complexity curve. Rademacher complexity [275, 276]\nand VC dimension do account for model class [203, 277], but are geared towards\nmeasuring capacity of fitting random labels. They thus do not account for the specific\ntask and cannot leverage the inherent structure in the data present in many deep\nlearning problems, and are also challenging to compute for specific deep learning\narchitectures, though bounds do exist [281, 282]. Additional challenges get presented\nby the fact that neural networks have been shown to consistently be able to learn\nmappings to random labels [274], making it hard to distinguish between classes of\nneural networks.\nMetrics for measuring the complexity of a single model rather than model\nfamily are available. They range from simple approaches, such as the norm of the\nlearned parameters [278], to the more complex such as classification margins [283] or\nflatness [18, 20], Kolmogorov complexity [284] and relatedly minimum description\nlength [285, 286], the number of linear pieces for ReLU networks [14, 287], but\nthey are not without limitations.\nMetrics based on parameter norms, such as\nweight norms, are independent of model architectures and for large models it is not\ninherently clear how changing the parameter norm changes the model predictions.\nNorm-based metrics have also been shown to correlate poorly with generalisation,\nespecially when stochastic optimisation is used [20]; classification margins can be\nchallenging to compute for data with a high number of dimensions or classes;\n8.1. Introduction\n183\nwhile flatness (or equivalently low sharpness) has been shown to be connected\nwith generalisation [12, 18, 20, 141, 168], limitations have been noted [19, 169].\nWe previously discussed flatness—defined there as the leading eigenvalue of the\nHessian—as a measure of generalisation in Chapter 3. Complexity measures for\ndistributions over models, such as PAC-Bayes [288], have also been adapted to neural\nnetworks [289], but they are outside of the scope of this thesis.\nNeyshabur et al. [281] analysed many existing measures of complexity and\nshowed that they increase alongside the number of the hidden units, and thus cannot\nexplain behaviours observed in over-parametrised neural networks, where increasing\nthe number of hidden units does not lead to increased overfitting (their Figure\n5). Jiang et al. [20] perform an extensive study of existing complexity measures and\ntheir correlation with generalisation as well as aim to find causal relationships and\nfind that flatness has the strongest positive connection with generalisation.\nExplicit and implicit regularisation. Explicit regularisation in deep learning\ntakes many forms, either through adding terms to loss functions or regularising\nthe model architecture [15, 17, 61, 76, 80, 151, 290]. Explicit regularisation can\naffect model capacity in intricate ways and its effect can strongly depend on the\nmagnitude of regularisation coefficients, with no straightforward method of translating\nregularisation effects into restrictions on a model class. Thus, despite their practical\nsignificance in obtaining increased generalisation performance, the effect of common\nexplicit regularisers has not been unified by existing complexity measures.\nRecent years have seen an expansion in the discovery of implicit regularisa-\ntion effects in deep learning, from the implicit regularisation effects of learning\nrates [12, 13, 21, 105, 152, 278, 291], mini-batch optimisation noise [165, 292, 293]\nor initialisation [16, 50, 294–298]. We previously explored implicit regularisation\ninduced by the discretisation drift of optimisers in Chapters 4 and 5. Implicit regu-\nlarisation effects have been largely unexplained by existing complexity measures; by\nconstruction complexity measures of function classes rather than individual models\ncannot capture the effects of optimisation inside a function class. Novel connec-\ntions between optimisation hyperparameters such as learning rates and per-model\n8.2. Geometric Complexity\n184\ncomplexity measures such as sharpness have been recently found (Cohen et al. [52]\nand Chapter 3), but more investigation into its connection with stochasticity is\nneeded [169]. The role of network depth and overparametrisation as an implicit\nregulariser has also been studied [101, 281, 297], and has been shown to play an\nimportant role in the generalisation properties of neural networks as well as the\ndouble descent phenomenon; in Chapter 6 we postulated a connection between depth,\noverparametrisation, and smoothness, which we will make concrete here.\nSmoothness: We highlighted the benefits of smoothness for neural networks\nextensively in Section 6.4 and intuited its deep connection with generalisation and\ndouble descent.\nExisting works have shown that generalisation bounds can be\nobtained from smoothness constraints [82]. The connection between smoothness\nand many implicit and explicit regularisers has been highlighted in Chapter 6; here\nwe will define a smoothness based complexity measure and make this association\nconcrete through theoretical arguments and experimentation.\n8.2\nGeometric Complexity\nWe are now ready to define a novel metric of complexity for neural models, with\nthe aim of finding a measure that can capture some of the implicit and explicit\nregularisation effects previously discussed.\nDefinition 8.2.1 (Geometric Complexity) For a neural network g(·; θ) : RI →\nRO with parameters θ ∈RD, we write g(·; θ) = τ(f(·; θ)) where τ is the last layer\nactivation function (such as the softmax for multi-label classification). Given a dataset\nD with elements x i.i.d. realisations of random variable X, we define Geometric\nComplexity (GC) as\nGC(f; θ, D) =\n1\n|D|\nX\nx∈D\n\r\r\r\r\ndf(x; θ)\ndx\n\r\r\r\r\n2\n2\n,\n(8.1)\nwhere ||v||2 denotes the Frobenius norm.\nGC measures the complexity of a single model f(·; θ), not a model class, can be\ncomputed efficiently, and depends on the task at hand through the training dataset D.\n8.2. Geometric Complexity\n185\nWhile GC is defined based on the training data, it is an estimate of an expectation:\nRemark 8.2.1 GC is the unbiased estimate of the underlying expectation under\np(x) the density of X:\nEp(x)\n\"\r\r\r\r\ndf(x; θ)\ndx\n\r\r\r\r\n2\n2\n#\n=\nZ \r\r\r\r\ndf(x; θ)\ndx\n\r\r\r\r\n2\n2\np(x)dx.\n(8.2)\nGC can be connected to Lipschitz smoothness—which we discussed at length in\nChapter 6—using the following bound:\nRemark 8.2.2 (Connection to Lipschitz smoothness) If K is the Lipschitz\nconstant of f(·; θ) : RI →RO w.r.t. the Frobenius norm then we have that:\nGC(f; θ, D) =\n1\n|D|\nX\nx∈D\n\r\r\r\r\ndf(x; θ)\ndx\n\r\r\r\r\n2\n2\n(8.3)\n≤min(I, O) 1\n|D|\nX\nx∈D\n\r\r\r\r\ndf(x; θ)\ndx\n\r\r\r\r\n2\nop\n≤min(I, O)K2,\n(8.4)\nwhere ∥A∥2 and ∥A∥op denote the Frobenius and operator norms of A, respectively.\nThis result follows from ∥A∥2\n2 ≤rank(A) ∥A∥2\nop ≤min(I, O) ∥A∥2\nop ,\n∀A ∈RI × RO\nand\n\r\r\rdf(x;θ)\ndx\n\r\r\r\n2\nop ≤K2. For many problems of concern we have that min(I, O) = O\nand thus Remark 8.2.2 exposes that the tightness of the bound between GC and\nLipschitz smoothness depends on the number of outputs of the network.\nWhile the Lipschitz smoothness of a function provides an upper bound on GC,\nthere are a few fundamental differences between the two quantities. Firstly, GC is\ndata dependent: a model can have low GC while having high Lipschitz constant due\nto the model not being smooth in parts of the space where there is no training data.\nMeasures on the entire space, such as Lipschitz constants, are unlikely to explain\nimplicit and explicit regularisation effects when many of the regularisers are applied\naround data instances. Secondly, GC can be computed exactly given a model and\ndataset, while for when estimating the Lipschitz constant of neural networks loose\nupper and lower bounds that rely on function composition are often used [299, 300].\nFazlyab et al. [240] provide an algorithm with tighter bounds by leveraging that\n8.2. Geometric Complexity\n186\nactivation functions are derivatives of convex functions and cast finding the Lipschitz\nconstant as the result of a convex optimisation problem; however, their most accurate\napproach scales quadratically with the number of neurons and only applies to feed\nforward networks.\nSokoli´c et al. [82] provide an upper bound for the Lipschitz\nconstant of a neural network with linear, softmax and pooling layers restricted to\ninput space X via ∥f(x) −f(y)∥2 ≤supz∈convex hull(X)\n\r\r df\ndz\n\r\r\n2 ∥x −y∥2 ,\n∀x, y ∈X,\nbut empirically resort to layer-wise bounds. In contrast, GC can be computed exactly\nregardless of network architecture and can be easily implemented using modern\nmachine learning software frameworks.\n8.2.1\nGC for deep linear models\nConsider a deep linear network lin(x, θ) = Wn . . . W1x+b, with θ = {b}∪{Wi, i ∈\n{1, . . . , n}} We then have that\nGC(lin; θ, D) = ∥Wn . . . W1∥2\n2.\n(8.5)\nThis shows that the GC of an affine transformation does not depend on the data\nand recovers the standard Frobenius norm, which is often used as a regulariser,\nparticularly in ridge regression.\n8.2.2\nGC for rectifier feed-forward networks\nNetworks with rectifier activations, such as ReLu [301] and Leaky Relu [302], are\npiecewise affine models, that is for each xi we have that f(xi; θ) = Aixi + bi, where\nAi and bi depend on where in the partition of the input space input xi falls in. We\ncan then write\nGC(f; θ, D) =\n1\n|D|\nX\nx∈D\n\r\r\r\r\ndf(x; θ)\ndx\n\r\r\r\r\n2\n2\n=\nP\nX\np=1\nnp\n|D|∥Ap∥2\n2,\n(8.6)\nwhere np is the number of times an input is in the partitioning corresponding to\nthe linear piece Apx + bp in the piecewise model. Eq (8.6) shows that for models\nwith rectifier activations, GC can be approximated by using batches, instead of the\nentire dataset, as long as the batch size is large enough to include representation\n8.2. Geometric Complexity\n187\nfrom many piecewise branches of the network.\nTo further understand the form of GC for rectifier models, let’s consider a\nsimple two-layer network, where we have f(x; θ) = W2a(W1x + b1) + b2 with\nθ = {W1, W2, b1, b2}. Denote h = a(z) = a(W1x + b1), where a is the rectifier\nactivation function. We then have\ndf\ndx = W2diag(a′(z))W1,\n(8.7)\nwhere diag(v) is the diagonal matrix with diagonal entries given by v.\nSince\n(diag(a′(z))W1)i,j = a′(zi)Wi,j and since in the case of rectifiers a′(zi) is a piecewise\nconstant function (such as a′(x) = 0, if x < 0 or 1 otherwise for ReLu activations), we\nsee the connection between L2 regularisation and weight decay in rectifier networks,\nand how decreasing the norm of the weight matrices will decrease GC. We observe\nthat unlike weight norms, GC is sensitive to the impact the weight has on the output\nof the network. This can be most clearly seen for the ReLU activation, where the\nactivation derivative a′ can be 0 and thus for units that do not contribute to the\noutput of a data instance, no norm penalty is imposed. Consider the case of unit\nhi = 0, where zi = (W1)T\ni x + (b1)i < 0. Then the entries of row (W1)i, which don’t\ncontribute to the model output, will not be penalised by\n\r\r df\ndx\n\r\r in GC, since they are\nmodulated by a′(zi) = 0 in Eq 8.7. The same can be said about the column i of\nW2, which does not contribute to the model output as o = W2z + b2. Thus, GC is\nrobust to weights that do not contribute to model output, such as when a weight\nmatrix has an entry with negative but large absolute value, which leads to a negative\npre-activation and a zero hidden unit for all inputs in the dataset. In contrast, the\nweight norm will be heavily skewed by such a weight due to its large absolute value;\nthis insight might help explain why parameter norm measures are not indicative of\ngeneralisation performance [20].\n8.2.3\nGC for convolutional and residual layers\nSince convolutional layers are linear operators, the form of GC when convolutional\nlayers are used is that of Eq (8.6), where the linear operator Ap will be determined\n8.3. GC increases as training progresses\n188\nby the linear operators of the convolutional layers. A significant difference between\nconvolutional and linear layers will be that each parameter of a convolutional filter\nwill appear repeatedly in the linear operator present in GC, which further highlights\nthe difference between GC and weight norms discussed above.\nResidual layers [86] have the form of a(Mx + x), where M is a composition of\nconvolutional layers followed by the activation function a, usually a rectifier [262, 302].\nAlternatively, they can use an additional projection R leading to a(Mx + Rx). Thus,\nnetworks rectifier activations with residual layers will also be piecewise linear models\nand Eq (8.6) holds.\n8.3\nGC increases as training progresses\nWe now focus on the value of GC at network initialisation, and we show that for\ndeep networks with rectifier activations and commonly used initialisers, the GC of\nthe model is close to 0 at initialisation. To see why, we again leverage the piecewise\nnature of networks with rectifier activations, together with the fact that network\nbiases are initialised to be 0: at initialisation rectifier MLPs can be written as\nf(x; θ) = WnPn−1Wn−1 . . . P1W1x where Pi = diag(a′(zi)) = diag(a′(Wizi−1)).\nFor ReLu networks, since the diagonal entries of Pi will be 0 if zi−1 is negative, as\nthe depth of the network increases, the chance that the output is not 0 diminishes.\nConsider the 1-dimensional case where f(x; θ) = wna(wn−1 . . . a(w1x)) and a is the\nReLu activation function. If we assume standard Gaussian initialisations of the\nform wi ∼N(0, σ2\ni ) we have that P(w1x > 0) = 1\n2, thus P(a(w1x) ̸= 0) = 1\n2. With\nsimilar arguments, P(a(w2z1) ̸= 0|z1 ̸= 0) = 1\n2 leading to P(a(w2z1) ̸= 0) = 1\n2\n2. By\ninduction we can then show that for n layers P(f(x; θ) ̸= 0) =\n1\n2n; this shows that\nas we add one layer, the probability of the output of the network not being 0 is\nexponentially decreasing. When we consider higher dimensional weight matrices,\nthe chance of hitting exactly 0 decreases, but the obtained function values will\nbe small due to the decreased chance of consistently obtaining positive numbers\nunder the random initialisation.\nThus, we expect GC, as the average norm of\ndf(x;θ)\ndx\n= WnPn−1Wn−1 . . . P1W1, to be low at initialisation and decrease as the\n8.3. GC increases as training progresses\n189\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.020\n0.015\n0.010\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\nmean(f(C1 +\n(C2\nC1;\n0)\n(a) Mean model output.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nmax(f(C1 +\n(C2\nC1;\n0)\nnumber of layers\n1\n2\n4\n8 \n16\n32\n64\n(b) Max model output.\nFigure 8.1: Initialisation of MLPs. Outputs given by f(C1 + α(C2 −C1); θ) where\nC1 ∈RI with every entry equal to 1 and C2 = −C1 where f is an MLP\nwith 500 neurons per layer; the input dimension I = 150528 = 224 × 224 × 3.\nThe mean model outputs (a) and max model outputs (b) are closer to 0 at\ninitialisation as depth increases. Uncertainty is obtained by using multiple\nseeds to initialise the network.\nnumber of layers in the network increases. In Figure 8.1 we confirm this by looking\nat the effect of depth on the output of an MLP with input dimensions given by those\nof input dimension I = 150528 = 224 × 224 × 3 and O = 1000 (as from the Imagenet\ndataset), and show how both the mean and the maximum output obtained from 100\nsamples are decreasing as the number of layers increases. To empirically assess the\neffect of the number of layers on GC at initialisation, we use MLPs initialised either\nwith the default initialisation in many deep learning packages, which we will call\n‘standard initialisation’ (with weights sampled from a truncated normal distribution\nbounded between -2 and 2 and scaled by the square root of number of incoming\nunits), and Glorot initialisation [303]. We show in Figure 8.2 that the value of GC\nat initialisation decreases as the number of layers increases. These results show deep\nMLPs are smoother at initialisation, in that they are less sensitive to the value of\nthe input and are close to the constant function 0; this is a useful initialisation to\nstart with as otherwise the training procedure might have to undo the effects of a\nbad initialisation (where for example inputs from the same class might be initialised\nto have very different output logits), and furthermore increases reliability as training\n8.4. GC and regularisation\n190\n1\n2\n4\n8\n16\n32\n64\nNumber of Layers\n0\n5\n10\n15\n20\n25\nGeometric Complexity\nnumber of layers\nrelu-standard\nrelu-glorot\nsigmoid-standard\nsigmoid-glorot\nFigure 8.2: Geometric Complexity (GC) at initialisation decreases as the number of layers\nin an MLP increases; this reflects the initialisation phenomenon showcased\nin Figure 8.1. This effect shows that the deeper the MLP, the smoother it is\nat initialisation.\nschemes become less dependent on initialisations. We also observe that GC captures\nthis effect at initialisation, and helps explain why deeper models can better model\nthe training data.\nWe have seen how deep MLPs initialised with commonly used initialisation\nschemes start training at low (close to 0) GC. We also observe that as training\nprogresses and the model captures the training data, its GC increases; we see this in\nFigure 8.3 and we will consistently observe this when training larger models as well\n(see Figures 8.6 and 8.7). During training however, deep networks do not learn overly\ncomplex functions, even as GC increases; as shown in Figure 8.3, even when using a\nlarge MLP to train 10 data points, the model fits the data without overfitting. We\nnow turn to explore the potential mechanisms for this observation.\n8.4\nGC and regularisation\nWe now highlight how many implicit and explicit regularisation methods regularise\nGC, despite not targetting it directly. To disentangle the effects studied here from\noptimisation choices, we use stochastic gradient descent without momentum to train\nall models unless otherwise specified. We also study the impact of a given training\nheuristic on GC in isolation of other techniques to avoid masking effects. For this\nreason, we do not use data augmentation or learning rate schedules.\n8.4. GC and regularisation\n191\n1\n0\n1\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGC=0.01\ndata\nmodel prediction\n(a) 0 epochs.\n1\n0\n1\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGC=0.63\n(b) 10 epochs.\n1\n0\n1\nx\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGC=1.17\n(c) 1000 epochs.\n1\n0\n1\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGC=1.33\n(d) 10000 epochs.\nFigure 8.3: For a large MLP fitting 10 points in 1 dimension, the GC of the function\nbeing learned gradually grows in training. Despite having capacity to overfit,\nthe model learns a simple function while fitting the data.\n8.4.1\nExplicit regularisation\nL2 norm regularisation. Section 8.2.2 has shown the connection between GC and\nweight norms in the case of rectifier neural networks. L2 norm regularisation adds the\nregulariser ζ PN\nl=1 ∥Wl∥2\n2 with ζ > 0 to the loss function, minimising the magnitude\nof each weight parameter. This results in a decreased GC in the case of rectifier\nneural networks where the GC can be written using the form in Eq (8.6), where the\nvalue of each piecewise linear function will be determined by the magnitude of the\nnetwork’s weights. While this is not a formal argument, we confirm in Figure 8.4a\nthat as the L2 regularisation strength increases, GC decreases.\nGradient norm regularisation. Another popular form of regularisation is to\nadd an explicit regulariser with the norm of the gradient ∥∇θE(θ)∥2\n2 to the loss\nfunction [13, 15, 17, 61, 67, 151].\nWhile GC is concerned with gradients with\nrespect to inputs, and the gradient norm regulariser penalises the gradient norm with\nrespect to parameters, the two are connected via backpropagation; we previously\nused neural network structure and backpropagation to connect the effect of an\ninput smoothness regularisation technique—Spectral Normalisation—to scaling of\ngradients with respect to parameters in Chapter 7. As we see in backpropagation\nEqs (7.16)—(7.19), decreasing the gradient norm ∥∇θE(θ)∥2\n2 can be achieved by\ndecreasing the magnitude of the weight matrix entries or that of the activation\nderivative a′(z), which for ReLu networks can be achieved with increased sparsity;\nboth of these will lead to a decrease in GC. We show in Figure 8.4b that indeed,\n8.4. GC and regularisation\n192\n0.0\n0.1\n0.2\nGC at max accuracy\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nRegularisation Rate\n0.65\n0.70\n0.75\n0.80\nMax accuracy\nL2 regularisation\n(a) L2.\n0.10\n0.15\n0.20\n0.25\nGC at max accuracy\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nRegularisation Rate\n0.65\n0.70\nMax Accuracy\nGradient norm regularisation\n(b) Gradient norm.\n0.175\n0.200\n0.225\n0.250\nGC at max accuracy\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nRegularisation Rate\n0.66\n0.68\n0.70\nMax accuracy\nSpectral regularisation\n(c) Spectral.\nFigure 8.4: Common explicit regularisation methods lead to decreased GC when using\nvanilla stochastic gradient descent to train a Resnet-18 model on CIFAR-10.\nGC decreases with increased regularisation strength. Similar trends are\nobserved when momentum is used, as shown in Section F.2 of the Appendix,\nwhere we also show loss curves.\nincreased gradient norm regularisation consistently leads to a decrease in GC.\nSpectral Normalisation/regularisation. GC is deeply connected to Lipschitz\nsmoothness; as we have shown in Section 8.2, the Lipschitz constant of the model is\nan upper bound on its GC. Hard Lipschitz normalisation methods, such as Spectral\nNormalisation, will thus impose a fixed upper bound on the GC of the model,\nwhile regularisation methods such as Spectral Regularisation induce a regularisation\npressure to decrease that bound; for an overview of these methods as well as a\ncontrast between soft and hard constraints, see Chapter 6. We show in Figure 8.4c\nthat minimising the spectral norm of each layer, as done by Spectral Regularisation,\nleads to a decrease in GC. Furthermore, as the strength of Spectral Regularisation\nincreases—by increasing the regularisation coefficient—GC decreases.\nJacobian regularisation.\nJacobian regularisation is an existing form of ex-\nplicit regularisation which directly minimises GC, i.e.\nit adds ζGC(f; θ, D) =\nζ\n|D|\nP\nx∈D ∥df(x;θ)\ndx\n∥2\n2 to the loss function. Jacobian regularisation has been corre-\nlated with increased generalisation but also robustness to input distributional shift\n[29, 80, 82, 304]. Sokoli´c et al. [82] show that adding Jacobian regularisation to the\nloss function can lead to an increase in test set accuracy (their Tables III, IV, and\n8.4. GC and regularisation\n193\n0\n1\n2\n3\n4\nRegularisation Rate\n1e\n6\n5\n10\n15\n20\n25\n30\n35\n40\nGC at Maximum Accuracy\nJacobian (GC) Regularisation\n(a) GC decreases with increased regulari-\nsation strength.\n0\n1\n2\n3\n4\nRegularisation Rate\n1e\n6\n0.720\n0.725\n0.730\n0.735\n0.740\n0.745\n0.750\nMaximum Accuracy\nJacobian (GC) Regularisation\n(b) Accuracy increases with GC regularisa-\ntion strength, unless too much regulari-\nsation is used.\nFigure 8.5: Jacobian (GC) regularisation on CIFAR-10. Explicit regularisation min-\nimising GC (already existing in literature as Jacobian regularisation) leads\nto increased generalisation, unless the regularisation coefficient is too large.\nThis effect reflects what we have observed in other settings, where too much\nsmoothness can hurt performance, from an illustrative example in Figure 1.2,\nto the Spectral Normalisation example in Figure 6.3d and the reinforcement\nlearning examples in Figure 7.5. Results obtained using 1 seed.\nV). We confirm their findings, namely an increase in test accuracy and a decrease in\nGC with Jacobian/GC regularisation in Figure 8.5. As we have noted previously,\ntoo much smoothness can hurt model capacity and decrease performance, which is\nsomething that we observe here too for large regularisation coefficients.\n8.4.2\nImplicit regularisation via discretisation drift\nWe now show how the implicit regularisation induced by discretisation drift in\ngradient descent optimisation, previously discussed in Chapters 2, 4 and 5, can lead\nto a decrease in GC due the connection between gradient norm minimisation and\nGC in neural networks. Since the strength of implicit regularisation is affected by\noptimisation hyperparameters such as learning rate and batch size, we empirically\nobserve how changing these hyperparameters leads to direct effect on GC.\nLearning rate. Large learning rates have been consistently shown to have an\nimplicit regularisation effect and lead to increased generalisation [12, 13, 105, 305].\nAttempts at formalising this include the Implicit Gradient Regularisation (IGR) flow\n8.4. GC and regularisation\n194\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nIteration\n1e4\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nTrain Loss\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.2\n(a) Train loss.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nIteration\n1e4\n0.1\n0.2\n0.3\n0.4\nGeometric Complexity\n(b) GC.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nIteration\n1e4\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTest Accuracy\n(c) Test accuracy.\nFigure 8.6: The implicit regularisation effect of large learning rates on GC: increasing\nthe learning rate results in decreased GC and increased performance. The\nresults here use vanilla gradient descent; a similar trend is observed when\nmomentum is used, as shown in Section F.2 of the Appendix.\ndiscussed in Chapter 2, which suggests that gradient descent implicitly minimises the\nloss function ˜E(θ) = E(θ) + h\n4 ∥∇θE∥2, where h is the learning rate. In the previous\nsection, we described how backpropagation reveals shared components between\ngradients w.r.t. parameters—∇θE present in IGR—and gradients w.r.t. inputs—\n∇xf present in GC. We then showed that explicitly regularising the gradient norm\n∥∇θE∥2 leads to a decrease in GC. Based on these results, we expect that increasing\nthe learning rate h leads to a decrease in GC due to increased implicit gradient\nregularisation pressure h\n4 ∥∇θE∥2, which we empirically observe in Figure 8.6.\nBatch size. The increased generalisation performance of lower batch sizes has been\nlong studied in deep learning, often under the name ‘the generalisation gap’ [18,\n292, 305–307].\nWhile traditionally this increased generalisation effect has been\nattributed to gradient noise due to the sampling process, recent work has revisited\nthis assumption: the generalisation gap was recently bridged with Geiping et al.\n[151] showing that full-batch training can achieve the same test set performance as\nstochastic gradient descent. Geiping et al. [151] use an explicit regulariser inspired\nby IGR, with the intuition that the strength of the implicit gradient norm regulariser\ninduced by discretisation drift ∥∇θE∥2 is stronger for stochastic gradient descent\ncompared to full-batch gradient descent, as described by Smith et al. [13]; we\npreviously discussed the implicit regularisation induced by discretisation drift in\nstochastic gradient descent and the results of Smith et al. [13] in Chapter 5. Thus,\n8.4. GC and regularisation\n195\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e5\n0.0\n0.2\n0.4\n0.6\nTrain Loss\nBatch size\n8\n16\n32\n64\n128\n256\n512\n1024\n(a) Train loss.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e5\n0.0\n0.1\n0.2\n0.3\n0.4\nGeometric Complexity\n(b) GC.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e5\n0.4\n0.5\n0.6\n0.7\nTest Accuracy\n(c) Test accuracy.\nFigure 8.7: The implicit regularisation effect of small batch sizes on GC: decreasing the\nbatch size results in decreased GC and increased performance. The results\nhere use vanilla gradient descent; a similar trend is observed when momentum\nis used, as shown in Section F.2 of the Appendix.\nGeiping et al. [151] use\nh\n4|B|\nP\nB∈B\n\r\r 1\nB\nP\nX∈B ∇θE(X; θ)\n\r\r2 instead, where B is the\nset of batches used for training, in contrast to h\n4\n\r\r\r 1\n|D|\nP\nX∈D ∇θE(X; θ)\n\r\r\r\n2\n, the IGR\nregulariser in the full-batch case. Since the gradient norm is computed over a batch\nrather than the entire dataset, outliers, such as examples with a large gradient, are less\nlikely to be averaged out and this leads to a stronger regularisation pressure. Their\nresult further strengthens the connection between small batch sizes and increased\ngradient norm regularisation pressure, which we have consistently seen to lead to\nsmaller GC. Coupling these results, suggesting that for smaller batch sizes there is\na stronger effect due to IGR, with what we have previously observed, namely that\nminimising ∥∇θE∥2 leads to a decrease in GC, we should expect that decreasing\nbatch sizes leads to a decrease in GC. We empirically assess this question and show\nresults in Figure 8.7, observing a consistent decrease in GC with smaller batch\nsizes. These results show the implicit regularisation effect of batch sizes on model\nsmoothness.\nResults and justifications in this section use vanilla gradient descent, but we\nnote that the gradient norm minimisation effect induced by the discretisation drift\nof momentum has recently been mathematically formalised by Ghosh et al. [308],\nand thus we expect our arguments to hold for gradient descent with momentum too.\nWe confirm this with empirical results in Appendix F.\n8.5. Double descent and GC\n196\n8.5\nDouble descent and GC\nWhen neural network complexity is measured using a simple parameter count, a\ndouble descent phenomenon has been consistently observed: as the number of\nparameters increases, the test loss decreases at first, before increasing again, followed\nby a second descent toward a low test error value [77–79]. An excellent overview of\nthe double descent phenomena in deep learning can be found in Belkin [77], together\nwith connections to smoothness as an inductive bias of deep networks and the role of\noptimisation. We previously discussed the double descent phenomenon in Section 6.4,\nwhere we contrasted it with the U-shaped curve associated with standard machine\nlearning methods and complexity measures in Figure 6.1. We discussed the double\ndescent phenomenon from the smoothness perspective and intuited that smoothness\nlikely plays an important role in the second descent: we postulated that in the first\ndescent, the generalisation error is decreasing as the model is given extra capacity\nto capture the decision surface; the increase happens when the model has enough\ncapacity to fit the training data, but it cannot do so and retain smoothness, and\nthus overfits; the second descent occurs as the capacity increases and smoothness\ncan be retained.\nTo explore whether GC as a measure of model complexity based on input smooth-\nness can capture the double descent phenomena we follow the set up introduced\nin Nakkiran et al. [79]: we train multiple ResNet-18 networks on CIFAR-10 with\nincreasing layer width, and show results in Figure 8.8. We make two observations:\nfirst, like the test loss, GC follows a double descent curve as width increases (Fig-\nure 8.8a); second, when plotting GC against the test loss, we observe a U-shape curve,\nrecovering the traditional expected behaviour of complexity measures (Figure 8.8b).\nImportantly, we observe the connection between the generalisation properties of\nover-parametrised models and GC: after the critical region, increase in model size\nleads to a decrease in GC. These results provide further evidence that GC is able\nto capture model capacity in a meaningful way, suggestive of a reconciliation of\ntraditional complexity theory and deep learning, as well as provide an empirical basis\nfor our previously highlighted connection between smoothness and double descent.\n8.6. Related work\n197\n0\n2\n4\nTrain/Test Loss\nTest loss\nTrain loss\ncritical region\n0\n25\n50\n75\n100\n125\nNetwork Width\n0.0\n0.2\n0.4\nGC\n(a) GC and train/test losses as the network\nwidth increases.\n0\n2\n4\nLoss\nAll models\nTrain Loss\nTest Loss\n1\n2\n3\n4\n5\nGeometric Complexity\n1e\n1\n0\n2\n4\nLoss\nSeed average\n(b) Train and test losses as GC increases.\nFigure 8.8: Double descent and GC. (a): GC captures the double descent phenomenon.\n(b): GC captures the traditional U-shape curve, albeit with some noise,\nshowing (top) GC compared to train and test losses for all models and\n(bottom) GC compared to train and test losses averaged across different seeds.\nWe fit a 6 degree polynomial to the curves to showcase the trend.\n8.6\nRelated work\nComplexity measures for deep neural networks. There has been a recent effort\nto create complexity measures that capture both the generalisation phenomenon of\ndeep learning and are computationally tractable. Lee et al. [309] introduce ‘Neural\nComplexity Measures’ in which they use a neural network to approximate the\nexcess risk—the difference between empirical and true risk, which the author call\n‘the generalisation gap’—from validation data and use it as a regulariser for the\nmodel; Nagarajan and Kolter [16] argue for an initialisation dependent measure\nof complexity; Jiang et al. [283] approximate classification decision margins for\nneural networks and show their predictive generalisation gap correlates to the true\ngeneralisation gap. Maddox et al. [310] use the effective dimensionality [75] of the\nHessian matrix ∇2\nθE and show that it can capture the double descent phenomenon;\nsince effective dimensionality depends on the eigenspectrum of the Hessian which\nis computationally expensive to compute for large networks, the authors select the\nleading eigenvalues to estimate the metric.\nMost recently, Grant and Wu [311] modernised the Generalised Degrees of\nFreedom (GDof) [312, 313] complexity measure by providing a tractable approach\n8.6. Related work\n198\nto estimate it online during training. GDof is a similar measure to GC in that\nit computes the model sensitivity with respect to the dataset labels (GC instead\ncomputes the sensitivity to inputs). Grant and Wu [311] showed that GDof can\nresolve the double descent phenomenon; we hope that future work will assess the\nconnection between GC and GDof both empirically and theoretically.\nHuh et al. [314] argue that deep neural networks exhibit a low-rank bias, mea-\nsured as the rank of the Gram matrix of feature embeddings. By measuring a\nrelationship between data points, a low-rank measure is less local than GC, which\nassess how sensitive the model is to individual input changes. Huh et al. [314] connect\nlow-rank bias with depth and initialisation, but contend that depth plays a more\nsignificant role than initialisation and optimisation. Instead, we have associated the\nimplicit bias induced by GC with optimisation and optimisation hyperparameters.\nJacobian regularisation. Using GC as an explicit regulariser has existed previously\nas Jacobian regularisation, used to induce model robustness with respect to inputs\nand increase generalisation [29, 80, 82, 304]. Our goal here was not to introduce\na new regulariser, but to show that many existing implicit or explicit regularisers\nminimise GC, and that GC can be used as a neural network complexity measure.\nGradient penalties. Gradient penalties are used to regularise f : RI →R using a\npenalty of the form P\nx(||∇xf(x; θ)|| −K)2 with K a hyperparameter; instead of\nminimising the Jacobian norm, these methods ensure it is close K. For additional\ndiscussion on gradient penalties, see Section 6.3. Gradient penalties have been\nprimarily used for GAN training [28, 36, 202]; we leave the investigation of the\nimportance of GC outside supervised learning for future work.\nSmoothness and generalisation. Novak et al. [83] is perhaps the closest work to\nours: they show the correlation across multiple datasets between generalisation and\n1\nDtest\nP\nxi∈Dtest\n\r\r\r\r\r\nd(τ◦f(·;θ))\ndx\n\f\f\f\f\nxi\n\r\r\r\r\r\n2\nwhere τ is the last layer activation function (such as\nthe softmax). They also briefly show that their Jacobian norm metric captures the\nregularisation effect of stochastic gradient descent compared to full-batch training\nusing L-BFGS [315]. Since our goal is to find a complexity measure we do not use\nthe test set but the training set instead; our main focus is to show the connections\n8.7. Limitations and future work\n199\nbetween GC implicit and explicit regularisation, as well as initialisation in deep\nlearning.\nBartlett et al. [81] provide a generalisation bound depending on classification\nmargins and the product of spectral norms of the model’s weights and show empirically\nthat the product of spectral norms correlates with excess risk. Bubeck and Sellke [316]\nconnect Lipschitz smoothness with over-parametrisation, by providing a probabilistic\nupper bound on the model’s Lipschitz constant based on its training performance\nand the inverse of its number of parameters.\nSmoothness and double descent. In concurrent work, Gamba et al. [317] discuss\nthe connection between the Jacobian and the Hessian of the loss with respect to\ninputs, namely the empirical average of ||∇xE||2 and ||∇2\nxE||2 over the training set\nand show that they follow a double descent curve. Like our work with GC, Gamba\net al. [317] mention how their work suggests our postulate in Chapter 3: ‘Finally,\nRosca et al. (2020) investigate how to encourage smoothness, either on the entire\ninput space or around training data points. Interestingly, they postulate a connection\nbetween model-wise double descent and smoothness: during the first ascent the model\nsize is large enough to fit the training data at the expense of smoothness, while the\nsecond descent happens as the model size becomes large enough for smoothness to\nincrease. [...] Our work provides empirical evidence supporting the postulate of Rosca\net al. (2020).’\n8.7\nLimitations and future work\nOur experiments use the Resnet-18 and MLP architectures, and the ReLu activation\nfunction. We hope that future work will further validate these results in a variety of\nsettings, including for other architectures, activation functions, and input domains.\nOur results suggest that GC is able to capture a model’s complexity and its\ngeneralisation abilities on a test set obtained using i.i.d. sampling from the same\nunderlying distribution as the training set. We hope future work will investigate the\nproperties of GC when the test time distribution differs from the training distribution,\nas is the case in transfer learning [72, 189], or when the training set gradually changes\n8.8. Conclusion\n200\nin time, as in continual learning [71]. We intuit that the results will depend on the\ndistance between the training and test distribution in the case of transfer learning,\nor the rate of change of the input in the case of continual learning.\nWe believe incorporating GC for model selection has the potential to construct\ncriteria analogous to the Bayesian information criterion [318], adapted to the deep\nlearning domain. For architecture search GC could prove to be useful by excluding\narchitectures that have high GC at initialisation on the training data. While we did\nnot investigate this here, we hope this is a fruitful avenue for future work.\n8.8\nConclusion\nWe introduced Geometric Complexity (GC), a measure of model complexity defined\nas the average Jacobian norm of a network’s logits with respect to inputs from the\ntraining dataset. We showed that GC is closely connected to Lipschitz smoothness,\nbut unlike the Lipschitz constant of a deep network, GC is tractable and can\nbe measured during training. We have empirically shown how common explicit\nregularisers that target other quantities, such as weight norm regularisation and\ngradient norm regularisation, implicitly regularise GC. We then connected GC with\nthe implicit regularisation induced by the discretisation drift of gradient descent. We\nsaw how mechanisms that induce higher implicit regularisation, such as decreased\nbatch size and increased learning rate, lead to a decrease in GC. Last but not least, we\nshowed how GC can resolve the double descent challenge associated with traditional\ncomplexity measures and recovers the expected U-shaped curve.\nChapter 9\nConclusion\nThroughout the thesis we have seen that despite being fundamental ingredients in the\nmelting pot behind deep learning’s recipe for success, the effects of gradient-based\noptimisation and smoothness regularisation are far from being understood. Our\naim was to take a few steps towards a practical theory of deep learning, inspired\nby great recent works making strides in that direction. To this end, we started\nwith investigating gradient descent, a perhaps deceptively simple algorithm, which\nforms the basis of all modern deep learning optimisation. We took a continuous-time\nperspective to understanding gradient descent, either by quantifying discretisation\ndrift between existing continuous-time flows and gradient descent or by finding new\nflows that better describe the gradient descent trajectory. We then constructed novel\ncontinuous-time analyses to determine sources of instability in supervised learning and\ntwo-player games such as Generative Adversarial Networks, which led to mitigation\nstrategies either in the form of adaptive learning rates or explicit regularisers. We\ncontinued our investigation by studying interactions between optimisation and models,\nin particular model smoothness with respect to inputs and related regularisers. We\nexpanded the use of smoothness regularisation to a new domain, reinforcement\nlearning, where we observed that Spectral Normalisation leads to a consistent gain\nin agent performance that can be recovered through changes to optimisation. When\nrevisiting the use of Spectral Normalisation in Generative Adversarial Networks,\nhowever, we encountered a mixture of model regularisation and optimisation effects.\nWe concluded by introducing a measure of model complexity based on smoothness\nand explored its interactions with other ingredients of the deep learning melting pot,\n202\nsuch as implicit regularisation effects induced by discretisation drift, initialisation,\nand explicit regularisation.\nMotivated by our desire to dig deeper into why deep learning works so well, we\nemployed theoretical and empirical tools to perform an investigation into individual\ndeep learning components and their many interactions. Using our novel intuitions and\nunderstanding of deep learning, we constructed methods that led to increased training\nstability or improved evaluation performance across multiple problem domains. We\nhope both the approach and the results presented here can form the fruitful basis of\nfurther lines of inquiry, for the author as well as the wider community.\nBibliography\n[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,\nOlaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek,\nAnna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J.\nBallard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub\nJain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy,\nMichal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer,\nSebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray\nKavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein\nstructure prediction with alphafold. In Nature, 2021.\n[2] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam,\nPiotr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem,\nSam Madge, Rachel Prudden, Amol Mandhane, Aidan Clark, Andrew Brock,\nKaren Simonyan, Raia Hadsell, Niall Robinson, Ellen Clancy, Alberto Arribas,\nand Shakir Mohamed. Skilful precipitation nowcasting using deep generative\nmodels of radar. In Nature, 2021.\n[3] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-\nParedes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz,\nJulian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and\nPushmeet Kohli. Discovering faster matrix multiplication algorithms with\nreinforcement learning. In Nature, 2022.\n[4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125, 2022.\nBIBLIOGRAPHY\n204\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners. In Advances in Neural Information Processing Systems, 2020.\n[6] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican,\nGeorge van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\nKaren Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\nTraining compute-optimal large language models.\nIn Advances in Neural\nInformation Processing Systems, 2022.\n[7] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila.\nStylegan-t: Unlocking the power of gans for fast large-scale text-to-image\nsynthesis. arXiv preprint arXiv:2301.09515, 2023.\n[8] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\nHuang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian\nBolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den\nDriessche, Thore Graepel, and Demis Hassabis. Mastering the game of go\nwithout human knowledge. In Nature, 2017.\n[9] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,\nThore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.\nMastering chess and shogi by self-play with a general reinforcement learning\nalgorithm. arXiv preprint arXiv:1712.01815, 2017.\nBIBLIOGRAPHY\n205\n[10] OpenAI: Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,\nPrzemys law Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq\nHashme, Chris Hesse, Rafal J´ozefowicz, Scott Gray, Catherine Olsson, Jakub\nPachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim\nSalimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie\nTang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680, 2019.\n[11] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep\nnetworks: Implicit acceleration by overparameterization. In International\nConference on Machine Learning, 2018.\n[12] David GT Barrett and Benoit Dherin. Implicit gradient regularization. In\nInternational Conference on Learning Representations, 2021.\n[13] Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the\norigin of implicit regularization in stochastic gradient descent. In International\nConference on Learning Representations, 2021.\n[14] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Un-\nderstanding deep neural networks with rectified linear units. In International\nConference on Learning Representations, 2018.\n[15] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of\ngans. In Advances in Neural Information Processing Systems, 2017.\n[16] Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The\nrole of distance from initialization. 2019.\n[17] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is\nlocally stable. In Advances in Neural Information Processing Systems, 2017.\n[18] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyan-\nskiy, and Ping Tak Peter Tang. On large-batch training for deep learning:\nBIBLIOGRAPHY\n206\nGeneralization gap and sharp minima. International Conference for Learing\nRepresentations, 2017.\n[19] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp\nminima can generalize for deep nets. In International Conference on Machine\nLearning, 2017.\n[20] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and\nSamy Bengio. Fantastic generalization measures and where to find them. In\nInternational Conference on Learning Representations, 2019.\n[21] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the\nlearning rate, increase the batch size. In International Conference on Learning\nRepresentations, 2018.\n[22] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.\nVisualizing the loss landscape of neural nets. In Advances in Neural Information\nProcessing Systems, 2018.\n[23] Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Under-\nstanding batch normalization. In Advances in Neural Information Processing\nSystems, 2018.\n[24] Andrew Brock, Soham De, and Samuel L Smith. Characterizing signal propa-\ngation to close the performance gap in unnormalized resnets. In International\nConference on Learning Representations, 2021.\n[25] Zhiyan Ding, Shi Chen, Qin Li, and Stephen J Wright. Overparameterization of\ndeep resnet: zero loss and mean-field analysis. In Journal of machine learning\nresearch, 2022.\n[26] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard\nSch¨olkopf, and Gert RG Lanckriet.\nOn integral probability metrics,\\phi-\ndivergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.\nBIBLIOGRAPHY\n207\n[27] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative\nadversarial networks. In International Conference on Machine Learning, 2017.\n[28] William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai,\nShakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: Gans do\nnot need to decrease a divergence at every step. In International Conference\non Learning Representations, 2018.\n[29] Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with jacobian\nregularization. arXiv preprint arXiv:1908.02729, 2019.\n[30] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.\nSpectral normalization for generative adversarial networks. In International\nConference on Learning Representations, 2018.\n[31] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised represen-\ntation learning with deep convolutional generative adversarial networks. In\nInternational Conference on Learning Representations, 2015.\n[32] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adver-\nsarial nets. In Advances in Neural Information Processing Systems, 2014.\n[33] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for\nhigh fidelity natural image synthesis. In International Conference on Learning\nRepresentations, 2018.\n[34] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture\nfor generative adversarial networks. In Conference on Computer Vision and\nPattern Recognition, 2019.\n[35] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.\nf-gan: Training\ngenerative neural samplers using variational divergence minimization.\nIn\nAdvances in Neural Information Processing Systems, 2016.\nBIBLIOGRAPHY\n208\n[36] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron C Courville. Improved training of wasserstein gans. In Advances in\nNeural Information Processing Systems, 2017.\n[37] Miko laj Bi´nkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton.\nDemystifying mmd gans. In International Conference on Learning Representa-\ntions, 2018.\n[38] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and\nStephen Paul Smolley. Least squares generative adversarial networks. In\nInternational Conference on Computer Vision, 2017.\n[39] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-\nattention generative adversarial networks. In International Conference on\nMachine Learning, 2019.\n[40] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks.\nIn Advances in Neural Information Processing Systems, 2021.\n[41] Cyprien de Masson d’Autume, Shakir Mohamed, Mihaela Rosca, and Jack\nRae. Training language gans from scratch. In Advances in Neural Information\nProcessing Systems, 2019.\n[42] Paul J Werbos. Applications of advances in nonlinear sensitivity analysis. In\nSystem modeling and optimization, 1982.\n[43] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning\nrepresentations by back-propagating errors. In Nature, 1986.\n[44] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E\nHoward, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied\nto handwritten zip code recognition. In Neural Computation, 1989.\n[45] Augustin Cauchy et al. M´ethode g´en´erale pour la r´esolution des systemes\nd’´equations simultan´ees. In Comp. Rend. Sci. Paris, number 1847.\nBIBLIOGRAPHY\n209\n[46] Herbert Robbins and Sutton Monro. A stochastic approximation method. In\nThe annals of mathematical statistics, 1951.\n[47] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-\ntion. In International Conference on Learning Representations, 2015.\n[48] Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a\nrunning average of its recent magnitude. coursera: Neural networks for machine\nlearning. Coursera Neural Networks Machine Learning course, 2012.\n[49] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient de-\nscent finds global minima of deep neural networks. In International Conference\non Machine Learning, 2019.\n[50] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent op-\ntimizes over-parameterized deep relu networks. In Machine Learning. Springer,\n2020.\n[51] Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas\nPoczos. Gradient descent can take exponential time to escape saddle points.\nIn Advances in Neural Information Processing Systems, 2017.\n[52] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Tal-\nwalkar. Gradient descent on neural networks typically occurs at the edge of\nstability. In International Conference on Learning Representations, 2021.\n[53] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and\nGuy Gur-Ari. The large learning rate phase of deep learning: the catapult\nmechanism. arXiv preprint arXiv:2003.02218, 2020.\n[54] Omer Elkabetz and Nadav Cohen. Continuous vs. discrete optimization of\ndeep neural networks. In Advances in Neural Information Processing Systems,\n2021.\n[55] Paul Glendinning. Stability, instability and chaos: an introduction to the theory\nof nonlinear differential equations. Cambridge university press, 1994.\nBIBLIOGRAPHY\n210\n[56] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions\nto the nonlinear dynamics of learning in deep linear neural networks. In\nInternational Conference on Learning Represenatations, 2014.\n[57] Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization\ndynamics and transfer learning in deep linear networks.\nIn International\nConference on Learning Represenatations, 2019.\n[58] Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional\ndynamics of generalization error in neural networks. In Neural Networks, 2020.\n[59] Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the\nsquare loss. In Conference on Learning Theory, 2021.\n[60] Guilherme Fran¸ca, Jeremias Sulam, Daniel Robinson, and Ren´e Vidal. Confor-\nmal symplectic and relativistic optimization. In Advances in Neural Information\nProcessing Systems, 2020.\n[61] David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl\nTuyls, and Thore Graepel. The mechanics of n-player differentiable games. In\nInternational Conference on Machine Learning, 2018.\n[62] Robert M May. Simple mathematical models with very complicated dynamics.\nIn Nature, 1976.\n[63] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. In\nInternational Conference on Learning Representations, 2018.\n[64] Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite\nlearning rate stochastic gradient descent.\nIn International Conference on\nMachine Learning, 2021.\n[65] Daniel Kunin, Javier Sagastuy-Brena, and Hidenori Tanaka Ganguli, Surya\nDaniel L.K. Yamins. Symmetry, conservation laws, and learning dynamics in\nneural networks. In International Conference on Learning Representations,\n2021.\nBIBLIOGRAPHY\n211\n[66] Florian Sch¨afer and Anima Anandkumar. Competitive gradient descent. In\nAdvances in Neural Information Processing Systems, 2019.\n[67] Chongli Qin, Yan Wu, Jost Tobias Springenberg, Andy Brock, Jeff Don-\nahue, Timothy Lillicrap, and Pushmeet Kohli. Training generative adversarial\nnetworks by solving ordinary differential equations. In Advances in Neural\nInformation Processing Systems, 2020.\n[68] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training\nmethods for gans do actually converge?\nIn International Conference on\nMachine Learning, 2018.\n[69] Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax opti-\nmization locally: A follow-the-ridge approach. In International Conference on\nLearning Representations, 2019.\n[70] Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On finding local\nnash equilibria (and only local nash equilibria) in zero-sum games. arXiv\npreprint arXiv:1901.00838, 2019.\n[71] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan\nWermter. Continual lifelong learning with neural networks: A review. In Neural\nNetworks, 2019.\n[72] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning.\nIn Proceedings of the Institute of Electrical and Electronics Engineers, 2020.\n[73] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning:\nFrom theory to algorithms. Cambridge university press, 2014.\n[74] Tom M Mitchell. The need for biases in learning generalizations. Citeseer,\n1980.\n[75] David MacKay. Bayesian model comparison and backprop nets. In Advances\nin Neural Information Processing Systems, 1991.\nBIBLIOGRAPHY\n212\n[76] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and\nRuslan Salakhutdinov. Dropout: a simple way to prevent neural networks from\noverfitting. In The Journal of Machine Learning Research, 2014.\n[77] Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep\nlearning through the prism of interpolation. In Acta Numerica. Cambridge\nUniversity Press, 2021.\n[78] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling\nmodern machine-learning practice and the classical bias–variance trade-off. In\nNational Academy of Sciences, 2019.\n[79] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak,\nand Ilya Sutskever. Deep double descent: Where bigger models and more data\nhurt. In International Conference on Learning Representations, 2019.\n[80] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving\nthe generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.\n[81] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized\nmargin bounds for neural networks. In Advances in Neural Information Pro-\ncessing Systems, 2017.\n[82] Jure Sokoli´c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust\nlarge margin deep neural networks. In Transactions on Signal Processing, 2017.\n[83] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and\nJascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an\nempirical study. In International Conference on Learning Representations,\n2018.\n[84] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and\nNicolas Usunier.\nParseval networks: improving robustness to adversarial\nexamples. In International Conference on Machine Learning, 2017.\nBIBLIOGRAPHY\n213\n[85] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoen-\ncoder. In Advances in Neural Information Processing Systems, 2020.\n[86] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\nlearning for image recognition. In Conference on Computer Vision and Pattern\nRecognition, 2016.\n[87] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep net-\nwork training by reducing internal covariate shift. In International Conference\non Machine Learning, 2015.\n[88] Mihaela Rosca, Yan Wu, Chongli Qin, and Benoit Dherin. On a continuous-\ntime model of gradient descent dynamics and instability in deep learning. In\nTransactions on Machine Learning Research, 2022.\n[89] Mihaela Rosca, Yan Wu, Benoit Dherin, and David GT Barrett. Discretization\ndrift in two-player games. In International Conference on Machine Learning,\n2021.\n[90] Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A\ncase for new neural network smoothness constraints. 2020.\n[91] Florin Gogianu, Tudor Berariu, Mihaela Rosca, Claudia Clopath, Lucian\nBusoniu, and Razvan Pascanu. Spectral normalisation for deep reinforcement\nlearning: an optimisation perspective. In International Conference on Machine\nLearning, 2021.\n[92] Benoit Dherin, Michael Munn, Mihaela Rosca, and David GT Barrett. Why\nneural networks find simple solutions: The many regularizers of geometric\ncomplexity. In Advances in Neural Information Processing Systems, 2022.\n[93] Changyou Chen, Chunyuan Li, Liqun Chen, Wenlin Wang, Yunchen Pu, and\nLawrence Carin Duke. Continuous-time flows for efficient inference and density\nestimation. In International Conference on Machine Learning, 2018.\nBIBLIOGRAPHY\n214\n[94] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud.\nNeural ordinary differential equations. In Advances in Neural Information\nProcessing Systems, 2018.\n[95] Patrick Kidger.\nOn neural differential equations.\narXiv preprint\narXiv:2202.02435, 2022.\n[96] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David\nDuvenaud. Ffjord: Free-form continuous dynamics for scalable reversible\ngenerative models. In International Conference on Learning Representations,\n2019.\n[97] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.\nMaximum\nlikelihood training of score-based diffusion models. In Advances in Neural\nInformation Processing Systems, 2021.\n[98] Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte\ncarlo gradient estimation in machine learning. In Journal of Machine Learning\nResearch, 2020.\n[99] Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric\nnumerical integration. Oberwolfach Reports, 2006.\n[100] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel:\nConvergence and generalization in neural networks. In Advances in Neural\nInformation Processing Systems, 2018.\n[101] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient de-\nscent provably optimizes over-parameterized neural networks. In International\nConference on Learning Representations, 2018.\n[102] Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity\ninitialization efficiently learns positive definite linear transformations by deep\nresidual networks. In International Conference on Machine Learning, 2018.\nBIBLIOGRAPHY\n215\n[103] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya\nGanguli, and Yoshua Bengio.\nIdentifying and attacking the saddle point\nproblem in high-dimensional non-convex optimization. In Advances in Neural\nInformation Processing Systems, 2014.\n[104] Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more\nsample-efficient than fully-connected nets? In International Conference on\nLearning Representations, 2021.\n[105] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization\neffect of initial large learning rate in training neural networks. In Advances in\nNeural Information Processing Systems, 2019.\n[106] Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster\nbackpropagation learning: The rprop algorithm. In International Conference\non Neural Networks, 1993.\n[107] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel\nVeness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K.\nFidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis\nAntonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg,\nand Demis Hassabis. Human-level control through deep reinforcement learning.\nIn Nature, 2015.\n[108] Alex Kearney, Vivek Veeriah, Jaden B Travnik, Richard S Sutton, and\nPatrick M Pilarski. Tidbd: Adapting temporal-difference step-sizes through\nstochastic meta-descent. arXiv preprint arXiv:1804.03334, 2018.\n[109] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Os-\ntrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David\nSilver. Rainbow: Combining improvements in deep reinforcement learning. In\nAssociation for the Advancement of Artificial Intelligence, 2018.\n[110] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the\nBIBLIOGRAPHY\n216\nimportance of initialization and momentum in deep learning. In International\nConference on Machine Learning, 2013.\n[111] Boris T Polyak. Some methods of speeding up the convergence of iteration\nmethods. Ussr computational mathematics and mathematical physics, 1964.\n[112] Aman Gupta, Rohan Ramanath, Jun Shi, and S Sathiya Keerthi. Adam vs.\nsgd: Closing the generalization gap on image classification. In Workshop on\nOptimization for Machine Learning, Advances in Neural Information Processing\nSystems, 2021.\n[113] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, and Weinan E.\nTowards theoretically understanding why sgd generalizes better than adam in\ndeep learning. In Advances in Neural Information Processing Systems, 2020.\n[114] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of\nadam and beyond. In International Conference on Learning Representations,\n2019.\n[115] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han.\nUnderstanding the difficulty of training transformers.\narXiv preprint\narXiv:2004.08249, 2020.\n[116] Dami Choi, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J\nMaddison, and George E Dahl. On empirical comparisons of optimizers for\ndeep learning. arXiv preprint arXiv:1910.05446, 2019.\n[117] Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective\non reinforcement learning. In International Conference on Machine Learning,\n2017.\n[118] Philip Hartman. A lemma in the theory of structural stability of differential\nequations. In Proceedings of the American Mathematical Society, 1960.\nBIBLIOGRAPHY\n217\n[119] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled\ngenerative adversarial networks. In International Conference on Learning\nRepresentations, 2017.\n[120] Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in\nnonconvex-nonconcave minimax optimization? In International Conference on\nMachine Learning, 2020.\n[121] Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria?\nIn International Conference on Machine Learning, 2020.\n[122] Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning\ndynamics in stackelberg games.\nIn International Conference on Machine\nLearning, 2020.\n[123] Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon\nLacoste-Julien. A closer look at the optimization landscapes of generative\nadversarial networks. In International Conference on Learning Representations,\n2020.\n[124] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens\nin a tiny subspace. arXiv preprint arXiv:1812.04754, 2018.\n[125] Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam\nNeyshabur, David Cardoze, George Dahl, Zachary Nado, and Orhan Firat. A\nloss curvature perspective on training instability in deep learning. In Interna-\ntional Conference on Learning Representations, 2022.\n[126] Peter L Bartlett, Steven N Evans, and Philip M Long. Representing smooth\nfunctions as compositions of near-identity functions with implications for deep\nnetwork optimization. arXiv preprint arXiv:1804.05012, 2018.\n[127] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias\nof gradient descent on linear convolutional networks. In Advances in Neural\nInformation Processing Systems, 2018.\nBIBLIOGRAPHY\n218\n[128] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep\nlearning via over-parameterization. In International Conference on Machine\nLearning, 2019.\n[129] Simon Du and Wei Hu. Width provably matters in optimization for deep linear\nneural networks. In International Conference on Machine Learning, 2019.\n[130] Liu Ziyin, Botao Li, James B Simon, and Masahito Ueda. Sgd can converge to\nlocal maxima. In International Conference on Learning Representations, 2021.\n[131] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning\ndeep homogeneous models: Layers are automatically balanced. In Advances in\nNeural Information Processing Systems, 2018.\n[132] Shun-Ichi Amari. Natural gradient works efficiently in learning. In Neural\ncomputation, 1998.\n[133] Yann Ollivier. Riemannian metrics for neural networks: feedforward networks.\nIn Information and Inference: A Journal of the Institute of Mathematics, 2015.\n[134] Yann Ollivier. Riemannian metrics for neural networks ii: recurrent networks\nand learning symbolic data sequences. Information and Inference: A Journal\nof the IMA, 2015.\n[135] Yang Song, Jiaming Song, and Stefano Ermon. Accelerating natural gradient\nwith higher-order invariance. In International Conference on Machine Learning,\n2018.\n[136] Mihaela C Rosca, Yan Wu, Benoit Dherin, and David GT Barrett.\nDis-\ncretization drift in two-player games. In International Conference on Machine\nLearning, 2021.\n[137] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak,\nJascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any\ndepth evolve as linear models under gradient descent. In Advances in Neural\nInformation Processing Systems, 2019.\nBIBLIOGRAPHY\n219\n[138] Gerhard Wanner and Ernst Hairer. Solving ordinary differential equations II.\nSpringer Berlin Heidelberg New York, 1996.\n[139] HoHo Rosenbrock. An automatic method for finding the greatest or least value\nof a function. The computer journal, 1960.\n[140] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.\n[141] Stanis law Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua\nBengio, and Amos Storkey. On the relation between the sharpest directions\nof dnn loss and the sgd step length. In International Conference on Learning\nRepresentations, 2019.\n[142] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient\ndescent only converges to minimizers. In Conference on learning theory, 2016.\n[143] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in\nNeural Information Processing Systems, 2016.\n[144] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou.\nEmpirical analysis of the hessian of over-parametrized neural networks. In\nWorkshop track - International Conference on Learning Representations 2018,\n2017.\n[145] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into\nneural net optimization via hessian eigenvalue density. In International Con-\nference on Machine Learning, 2019.\n[146] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics\nwith sgd training and sample size. arXiv preprint arXiv:1811.07062, 2018.\n[147] Kaare Brandt Petersen and Michael Syskind Pedersen. The matrix cookbook.\n2008.\n[148] Alex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit\nbias of gradient descent at the edge of stability. In Workshop on Optimization\nBIBLIOGRAPHY\n220\nfor Machine Learning, Advances in Neural Information Processing Systems,\n2022.\n[149] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with\nwarm restarts. In International Conference on Learning Representations, 2017.\n[150] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li.\nBag of tricks for image classification with convolutional neural networks. In\nConference on Computer Vision and Pattern Recognition, 2019.\n[151] Jonas Geiping, Micah Goldblum, Phillip E Pope, Michael Moeller, and Tom\nGoldstein. Stochastic training is not necessary for generalization. In Interna-\ntional Conference on Learning Representations, 2022.\n[152] Stanis law Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek\nTabor, Kyunghyun Cho, and Krzysztof Geras.\nThe break-even point on\noptimization trajectories of deep neural networks. In International Conference\non Learning Representations, 2019.\n[153] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.\nSharpness-aware minimization for efficiently improving generalization.\nIn\nInternational Conference on Learning Representations, 2020.\n[154] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods\nfor online learning and stochastic optimization. In Journal of machine learning\nresearch, 2011.\n[155] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao\nXiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural\nnetworks: an empirical study. In Advances in Neural Information Processing\nSystems, 2020.\n[156] Bin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding\nthe acceleration phenomenon via high-resolution differential equations. In\nMathematical Programming, 2021.\nBIBLIOGRAPHY\n221\n[157] Nikola B Kovachki and Andrew M Stuart. Continuous time analysis of mo-\nmentum methods. In Journal of Machine Learning Research, 2021.\n[158] Guilherme Fran¸ca, Michael I Jordan, and Ren´e Vidal. On dissipative symplec-\ntic integration with applications to gradient-based optimization. Journal of\nStatistical Mechanics: Theory and Experiment, 2021.\n[159] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adap-\ntive stochastic gradient algorithms. In International Conference on Machine\nLearning, 2017.\n[160] Taiki Miyagawa.\nToward equation of motion for deep neural networks:\nContinuous-time gradient descent and discretization error analysis. In Advances\nin Neural Information Processing Systems.\n[161] Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra. Understanding the unstable\nconvergence of gradient descent. In International Conference on Machine\nLearning, 2022.\n[162] Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient\ndescent on edge of stability in deep learning. In International Conference on\nMachine Learning,, 2022.\n[163] Chao Ma, Lei Wu, and Lexing Ying.\nThe multiscale structure of neural\nnetwork loss functions: The effect on optimization and origin. arXiv preprint\narXiv:2204.11326, 2022.\n[164] Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge\nof stability. arXiv preprint arXiv:2206.04172, 2022.\n[165] Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of mini-\nbatch noise in sgd. In International Conference on Learning Representations,\n2022.\n[166] Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 2013.\nBIBLIOGRAPHY\n222\n[167] James Martens and Roger Grosse. Optimizing neural networks with kronecker-\nfactored approximate curvature. In International Conference on Machine\nLearning, 2015.\n[168] Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. In Neural computation,\n1997.\n[169] Simran Kaur, Jeremy Cohen, and Zachary C Lipton. On the maximum hessian\neigenvalue and generalization. 2022.\n[170] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduc-\ntion. MIT press, 2018.\n[171] Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic\nframework for model based reinforcement learning. In International Conference\non Machine Learning, 2020.\n[172] Satinder P Singh, Michael J Kearns, and Yishay Mansour. Nash convergence\nof gradient dynamics in general-sum games.\nIn Uncertainty in Artificial\nIntelligence, 2000.\n[173] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and\nSepp Hochreiter. Gans trained by a two time-scale update rule converge to a\nlocal nash equilibrium. In Advances in Neural Information Processing Systems,\n2017.\n[174] Radford M Neal. Annealed importance sampling. Statistics and computing,\n2001.\n[175] Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element\nmissing from standard gan. In International Conference on Learning Repre-\nsentations, 2019.\n[176] Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Combining maxi-\nmum likelihood and adversarial learning in generative models. In Proceedings\nof the AAAI conference on artificial intelligence, 2018.\nBIBLIOGRAPHY\n223\n[177] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,\nand Xi Chen. Improved techniques for training gans. In Advances in Neural\nInformation Processing Systems, 2016.\n[178] Haihao Lu. An o(sr)-resolution ode framework for understanding discrete-time\nalgorithms and applications to the linear convergence of minimax problems. In\nMathematical Programming, 2021.\n[179] Ernst Hairer and Christian Lubich. The life-span of backward error analysis\nfor numerical integrators. In Numerische Mathematik, 1997.\n[180] Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic)\ngradient descent in min-max optimization. In Advances in Neural Information\nProcessing Systems, 2018.\n[181] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Im-\nagenet training in minutes. In International Conference on Parallel Processing,\n2018.\n[182] Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondˇrej ˇCert´ık,\nSergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Ja-\nson K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger,\nRichard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik\nJohansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, ˇStˇep´an\nRouˇcka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman,\nand Anthony Scopatz. Sympy: symbolic computing in python. PeerJ Computer\nScience, 2017.\n[183] Ali Unlu and Laurence Aitchison. Gradient regularisation as approximate\nvariational inference. In Symposium on Advances in Approximate Bayesian\nInference, 2020.\n[184] Stanis law Kamil Jastrzebski, Devansh Arpit, Oliver ˚Astrand, Giancarlo Kerg,\nHuan Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J.\nBIBLIOGRAPHY\n224\nGeras.\nCatastrophic fisher explosion: Early phase fisher matrix impacts\ngeneralization. In International Conference on Machine Learning, 2021.\n[185] Florian Sch¨afer, Hongkai Zheng, and Animashree Anandkumar.\nImplicit\ncompetitive regularization in gans. In International Conference on Machine\nLearning, 2020.\n[186] Weijie Su, Stephen Boyd, and Emmanuel J Candes. A differential equation\nfor modeling nesterov’s accelerated gradient method: theory and insights. The\nJournal of Machine Learning Research, 2016.\n[187] Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational per-\nspective on accelerated methods in optimization. proceedings of the National\nAcademy of Sciences, 2016.\n[188] Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic,\ngame-theoretic, and logical foundations. Cambridge University Press, 2008.\n[189] Mohammadreza Iman, Khaled Rasheed, and Hamid R Arabnia.\nA re-\nview of deep transfer learning and recent advancements.\narXiv preprint\narXiv:2201.09679, 2022.\n[190] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and\nharnessing adversarial examples. In International Conference on Learning\nRepresentations, 2015.\n[191] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\nlearning for image recognition. In Conference on Computer Vision and Pattern\nRecognition, 2016.\n[192] Carlos Eduardo Rosar Kos Lassance, Vincent Gripon, and Antonio Ortega.\nLaplacian networks: Bounding indicator function smoothness for neural network\nrobustness. In Transactions on Signal and Information Processing, 2018.\nBIBLIOGRAPHY\n225\n[193] Pierluca D’Oro and Wojciech Ja´skowski. How to learn a useful critic? model-\nbased action-gradient-estimator policy optimization. In Advances in Neural\nInformation Processing Systems, 2020.\n[194] Michael Tobias Tschannen, Josip Djolonga, Paul Kishan Rubenstein, Sylvain\nGelly, and Mario Luˇci´c. On mutual information maximization for representation\nlearning. In International Conference on Learning Representations, 2020.\n[195] Herbert Federer. Geometric measure theory. Springer, 1996.\n[196] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations, 2018.\n[197] RV Mises and Hilda Pollaczek-Geiringer.\nPraktische verfahren der gle-\nichungsaufl¨osung. ZAMM-Journal of Applied Mathematics and Mechanic-\ns/Zeitschrift f¨ur Angewandte Mathematik und Mechanik, 9(1):58–77, 1929.\n[198] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regu-\nlarisation of neural networks by enforcing lipschitz continuity. In Machine\nLearning, 2020.\n[199] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant\nof self-attention. In International Conference on Machine Learning, 2021.\n[200] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training:\nScalable certification of perturbation invariance for deep neural networks. In\nAdvances in Neural Information Processing Systems, 2018.\n[201] Michael Arbel, Dougal Sutherland, Miko laj Bi´nkowski, and Arthur Gretton.\nOn gradient regularizers for mmd gans. In Advances in Neural Information\nProcessing Systems, 2018.\n[202] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence\nand stability of gans. arXiv preprint arXiv:1705.07215, 2017.\nBIBLIOGRAPHY\n226\n[203] Vladimir Vapnik. The nature of statistical learning theory. Springer science &\nbusiness media, 2013.\n[204] Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. In Advances\nin Neural Information Processing Systems, 2001.\n[205] Peter L Bartlett. For valid generalization the size of the weights is more\nimportant than the size of the network. In Advances in Neural Information\nProcessing Systems, 1997.\n[206] Noah Golowich, Alexander Rakhlin, and Ohad Shamir.\nSize-independent\nsample complexity of neural networks. In Conference On Learning Theory,\n2018.\n[207] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better:\nStability of stochastic gradient descent. In International Conference on Machine\nLearning, 2016.\n[208] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification\nwith deep convolutional neural networks. In Advances in Neural Information\nProcessing Systems, 2012.\n[209] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In International Conference on Learning\nRepresentations, 2015.\n[210] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh,\nAlec Radford, and Ilya Sutskever. Distribution augmentation for generative\nmodeling. In International Conference on Machine Learning, 2020.\n[211] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling\nmodern machine-learning practice and the classical bias–variance trade-off.\nIn Proceedings of the National Academy of Sciences. National Acad Sciences,\n2019.\nBIBLIOGRAPHY\n227\n[212] Joost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Simple\nand scalable epistemic uncertainty estimation using a single deep deterministic\nneural network. In International Conference on Machine Learning, 2020.\n[213] Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss,\nand Balaji Lakshminarayanan. Simple and principled uncertainty estimation\nwith deterministic deep learning via distance awareness. In Advances in Neural\nInformation Processing Systems, 2020.\n[214] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and\nscalable predictive uncertainty estimation using deep ensembles. In Advances\nin Neural Information Processing Systems, 2017.\n[215] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru\nErhan, Ian Goodfellow, and Rob Fergus.\nIntriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199, 2013.\n[216] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay\nCelik, and Ananthram Swami. The limitations of deep learning in adversarial\nsettings. In European symposium on Security and Privacy, 2016.\n[217] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay,\nand Debdeep Mukhopadhyay. Adversarial attacks and defences: A survey. In\nTransactions on Intelligence Technology, 2018.\n[218] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine\nlearning at scale. In International Conference on Learning Representations,\n2016.\n[219] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal\nFrossard. Universal adversarial perturbations. In Conference on Computer\nVision and Pattern Recognition, 2017.\n[220] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,\nBIBLIOGRAPHY\n228\nand Adrian Vladu. Towards deep learning models resistant to adversarial\nattacks. In International Conference on Learning Representations, 2018.\n[221] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra\nRaghu, Martin Wattenberg, and Ian Goodfellow. Adversarial spheres. In\nWorkshop Track, International Conference on Learning Representations, 2018.\n[222] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In\nInternational Conference on Learning Representations, 2014.\n[223] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic\nbackpropagation and approximate inference in deep generative models. In\nInternational Conference on Machine Learning, 2014.\n[224] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing\nflows. In International Conference on Machine Learning, 2015.\n[225] Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J¨orn-\nHenrik Jacobsen. Invertible residual networks. In International Conference on\nMachine Learning, 2019.\n[226] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating\ndivergence functionals and the likelihood ratio by convex risk minimization. In\nTransactions on Information Theory, 2010.\n[227] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.\nf-gan: Training\ngenerative neural samplers using variational divergence minimization.\nIn\nAdvances in Neural Information Processing Systems, 2016.\n[228] Michael Arbel, Liang Zhou, and Arthur Gretton. Generalized energy based\nmodels. In International Conference on Learning Representations, 2021.\n[229] Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang,\nWeinan Zhang, Yong Yu, and Zhihua Zhang. Lipschitz generative adversarial\nnets. In International Conference on Machine Learning, 2019.\nBIBLIOGRAPHY\n229\n[230] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair,\nYoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual infor-\nmation neural estimation. In International Conference on Machine Learning,\n2018.\n[231] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[232] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf.\nWasserstein auto-encoders. In International Conference on Learning Represen-\ntations, 2018.\n[233] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile\nnetworks for generative modeling. In International Conference on Machine\nLearning, 2018.\n[234] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile\nnetworks for distributional reinforcement learning. In International Conference\non Machine Learning, 2018.\n[235] Chris Finlay, J¨orn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman.\nHow to train your neural ode. In International Conference on Machine Learning,\n2020.\n[236] Chiappa Silvia, Jiang Ray, Stepleton Tom, Pacchiano Aldo, Jiang Heinrich,\nand Aslanides John. A general approach to fairness with optimal transport. In\nAssociation for the Advancement of Artificial Intelligence, 2020.\n[237] Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa.\nWasserstein fair classification. In Uncertainty in Artificial Intelligence, 2020.\n[238] C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen\nWissenschaften. Springer Berlin Heidelberg, 2008.\nBIBLIOGRAPHY\n230\n[239] Joern-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge.\nExcessive invariance causes adversarial vulnerability. In International Confer-\nence on Learning Representations, 2018.\n[240] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and\nGeorge Pappas. Efficient and accurate estimation of lipschitz constants for\ndeep neural networks. In Advances in Neural Information Processing Systems,\n2019.\n[241] Karol Kurach, Mario Luˇci´c, Xiaohua Zhai, Marcin Michalski, and Sylvain\nGelly. A large-scale study on regularization and normalization in gans. In\nInternational Conference on Machine Learning, 2019.\n[242] Yann LeCun, Patrick Haffner, L´eon Bottou, and Yoshua Bengio.\nObject\nrecognition with gradient-based learning. In Shape, contour and grouping in\ncomputer vision. Springer, 1999.\n[243] Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar,\nGrzegorz Swirszcz, and Max Jaderberg.\nDistilling policy distillation.\nIn\nInternational Conference on Artificial Intelligence and Statistics, 2019.\n[244] Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and gen-\neralization in temporal difference learning. In International Conference on\nMachine Learning, 2020.\n[245] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha¨el Mathieu,\nAndrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo\nEwalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Dani-\nhelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg,\nAlexander S. Vezhnevets, R´emi Leblond, Tobias Pohlen, Valentin Dalibard,\nDavid Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre,\nZiyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario\nW¨unsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Ko-\nBIBLIOGRAPHY\n231\nray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. In Nature, 2019.\n[246] T. Schaul, John Quan, Ioannis Antonoglou, and D. Silver. Prioritized experience\nreplay. In International Conference for Learning Representations, 2016.\n[247] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick,\nIan Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier\nPietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration.\nIn International Conference on Learning Representations.\n[248] Ian Osband, Charles Blundell, A. Pritzel, and Benjamin Van Roy. Deep explo-\nration via bootstrapped dqn. In Advances in Neural Information Processing\nSystems, 2016.\n[249] Georg Ostrovski, Marc G Bellemare, A¨aron van den Oord, and R´emi Munos.\nCount-based exploration with neural density models. In International Confer-\nence on Machine Learning, 2017.\n[250] Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos. Distribu-\ntional reinforcement learning with quantile regression. In Association for the\nAdvancement of Artificial Intelligence, 2018.\n[251] Matteo Hessel, Joseph Modayil, H. V. Hasselt, T. Schaul, Georg Ostrovski,\nW. Dabney, Dan Horgan, B. Piot, Mohammad Gheshlaghi Azar, and D. Sil-\nver. Rainbow: Combining improvements in deep reinforcement learning. In\nAssociation for the Advancement of Artificial Intelligence, 2018.\n[252] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning,\n1992.\n[253] Will Dabney, Andr´e Barreto, Mark Rowland, Robert Dadashi, John Quan,\nMarc G Bellemare, and David Silver. The value-improvement path: Towards\nbetter representations for reinforcement learning. In Association for the Ad-\nvancement of Artificial Intelligence, 2021.\nBIBLIOGRAPHY\n232\n[254] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning\nwith double q-learning.\nIn Association for the Advancement of Artificial\nIntelligence, 2016.\n[255] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and\nNando Freitas. Dueling network architectures for deep reinforcement learning.\nIn International Conference on Machine Learning, 2016.\n[256] Peter Henderson, Joshua Romoff, and Joelle Pineau. Where did my optimum\ngo?: An empirical analysis of gradient descent optimization in policy gradient\nmethods. 2018.\n[257] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon\nSchmitt, and Hado van Hasselt. Multi-task deep reinforcement learning with\npopart. In Association for the Advancement of Artificial Intelligence, 2019.\n[258] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The\narcade learning environment: An evaluation platform for general agents. In\nJournal of Artificial Intelligence Research, 2013.\n[259] Kenny Young and Tian Tian. Minatar: An atari-inspired testbed for thor-\nough and reproducible reinforcement learning experiments. arXiv preprint\narXiv:1903.03176, 2019.\n[260] Johan Samir Obando Ceron and Pablo Samuel Castro. Revisiting rainbow:\nPromoting more insightful and inclusive deep reinforcement learning research.\nIn International Conference on Machine Learning, 2021.\n[261] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regu-\nlarisation of neural networks by enforcing lipschitz continuity. In Machine\nLearning, 2020.\n[262] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted\nboltzmann machines. In International Conference on Machine Learning, 2010.\nBIBLIOGRAPHY\n233\n[263] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of\nrectified activations in convolutional network. arXiv preprint arXiv:1505.00853,\n2015.\n[264] Tim Salimans and Diederik P Kingma.\nWeight normalization: A simple\nreparameterization to accelerate training of deep neural networks. In Advances\nin Neural Information Processing Systems, 2016.\n[265] Twan Van Laarhoven. L2 regularization versus batch and weight normalization.\narXiv preprint arXiv:1706.05350, 2017.\n[266] Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters:\nefficient and accurate normalization schemes in deep networks. In Advances in\nNeural Information Processing Systems, 2018.\n[267] Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto\nrate-tuning by batch normalization. In International Conference on Learning\nRepresentations, 2019.\n[268] A. Givchi and M. Palhang. Quasi newton temporal difference learning. In\nAsian Conference on Machine Learning, 2014.\n[269] Tao Sun, Han Shen, Tianyi Chen, and Dongsheng Li. Adaptive temporal\ndifference learning with linear function approximation. In Transactions on\nPattern Analysis and Machine Intelligence, 2020.\n[270] Joshua Romoff, Peter Henderson, David Kanaa, Emmanuel Bengio, Ahmed\nTouati, Pierre-Luc Bacon, and Joelle Pineau. Tdprop: Does jacobi precondi-\ntioning help temporal difference learning? arXiv preprint arXiv:2007.02786,\n2020.\n[271] Zinan Lin, Vyas Sekar, and Giulia Fanti. Why spectral normalization stabi-\nlizes gans: Analysis and improvements. In Advances in Neural Information\nProcessing Systems, 2021.\nBIBLIOGRAPHY\n234\n[272] Scott Fortmann-Roe.\nUnderstanding the bias-variance tradeoff.\nURL:\nhttp://scott. fortmann-roe. com/docs/BiasVariance. html (h¨amtad 2019-03-27),\n2012.\n[273] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the\nreal inductive bias: On the role of implicit regularization in deep learning. In\nWorkshop Track, International Conference on Learning Representations, 2014.\n[274] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol\nVinyals. Understanding deep learning (still) requires rethinking generalization.\nIn Communications of the Association for Computing Machinery, 2021.\n[275] V. Koltchinskii and D. Panchenko. Rademacher processes and bounding the\nrisk of function learning. In High Dimensional Probability II. Birkh¨auser, 1999.\n[276] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities:\nRisk bounds and structural results. In Journal of Machine Learning Research,\n2002.\n[277] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative\nfrequencies of events to their probabilities. In Theory of Probability and its\nApplications, 1971.\n[278] Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint\narXiv:1709.01953, 2017.\n[279] Chao Ma and Lexing Ying. The sobolev regularization effect of stochastic\ngradient descent. arXiv preprint arXiv:2105.13462, 2021.\n[280] Peter L Bartlett, St´ephane Boucheron, and G´abor Lugosi. Model selection and\nerror estimation. In Machine Learning, 2002.\n[281] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and\nNathan Srebro. The role of over-parametrization in generalization of neural\nnetworks. In International Conference on Learning Representations, 2018.\nBIBLIOGRAPHY\n235\n[282] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical\nfoundations. Cambridge university press, 1999.\n[283] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the\ngeneralization gap in deep networks with margin distributions. In International\nConference on Learning Representations, 2018.\n[284] J. Schmidhuber. Discovering neural nets with low kolmogorov complexity and\nhigh generalization capability. In Neural networks : the official journal of the\nInternational Neural Network Society, 1997.\n[285] Geoffrey E Hinton and Richard Zemel. Autoencoders, minimum description\nlength and helmholtz free energy. In J. Cowan, G. Tesauro, and J. Alspector, ed-\nitors, Advances in Neural Information Processing Systems. Morgan-Kaufmann,\n1994.\n[286] L´eonard Blier and Yann Ollivier. The description length of deep learning\nmodels. Advances in Neural Information Processing Systems, 2018.\n[287] Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding\nand counting linear regions of deep neural networks. In International Conference\non Machine Learning, 2018.\n[288] Pierre Alquier. User-friendly introduction to pac-bayes bounds. arXiv preprint\narXiv:2110.11216, 2021.\n[289] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous gen-\neralization bounds for deep (stochastic) neural networks with many more\nparameters than training data. Uncertainty in Artificial Intelligence, 2017.\n[290] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT\npress, 2016.\n[291] Sihyeon Seong, Yegang Lee, Youngwook Kee, Dongyoon Han, and Junmo Kim.\nTowards flatter loss surface via nonmonotonic learning rate scheduling. In\nUncertainty in Artificial Intelligence, 2018.\nBIBLIOGRAPHY\n236\n[292] Samuel Smith, Erich Elsen, and Soham De. On the generalization benefit of\nnoise in stochastic gradient descent. In International Conference on Machine\nLearning, 2020.\n[293] Daniel A. Roberts. Sgd implicitly regularizes generalization error. In Advances\nin Neural Information Processing Systems 2018 Workshop. 2018.\n[294] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training\ndeep feedforward neural networks. In International Conference on Artificial\nIntelligence and Statistics, 2010.\n[295] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characteriz-\ning implicit bias in terms of optimization geometry. In International Conference\non Machine Learning, 2018.\n[296] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into\nrectifiers: Surpassing human-level performance on imagenet classification. In\nInternational Conference on Computer Vision, 2015.\n[297] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks\nvia stochastic gradient descent on structured data. In Advances in Neural\nInformation Processing Systems, 2018.\n[298] Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generaliza-\ntion error induced by initialization in deep neural networks. In Mathematical\nand Scientific Machine Learning, 2020.\n[299] Aladin Virmaux and Kevin Scaman.\nLipschitz regularity of deep neural\nnetworks: analysis and efficient estimation. In Advances in Neural Information\nProcessing Systems, 2018.\n[300] Patrick L Combettes and Jean-Christophe Pesquet. Lipschitz certificates for\nneural network structures driven by averaged activation operators. arXiv\npreprint arXiv:1903.01014, 2019.\nBIBLIOGRAPHY\n237\n[301] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv\npreprint arXiv:1803.08375, 2018.\n[302] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities\nimprove neural network acoustic models.\nIn International Conference on\nMachine Learning, 2013.\n[303] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training\ndeep feedforward neural networks. In International Conference on Artificial\nIntelligence and Statistics, 2010.\n[304] D´aniel Varga, Adri´an Csisz´arik, and Zsolt Zombori. Gradient regularization\nimproves accuracy of discriminative models. arXiv preprint arXiv:1712.09936,\n2017.\n[305] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better:\nclosing the generalization gap in large batch training of neural networks. In\nAdvances in Neural Information Processing Systems, 2017.\n[306] Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller.\nEfficient backprop. In Neural networks: Tricks of the trade. Springer, 2012.\n[307] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep\nneural networks. arXiv preprint arXiv:1804.07612, 2018.\n[308] Avrajit Ghosh, He Lyu, Xitong Zhang, and Rongrong Wang. Implicit regular-\nization in heavy-ball momentum accelerated stochastic gradient descent. In\nInternational Conference on Learning Representations, 2023.\n[309] Yoonho Lee, Juho Lee, Sung Ju Hwang, Eunho Yang, and Seungjin Choi.\nNeural complexity measures. In Advances in Neural Information Processing\nSystems, 2020.\n[310] Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking\nparameter counting in deep models: Effective dimensionality revisited. arXiv\npreprint arXiv:2003.02139, 2020.\nBIBLIOGRAPHY\n238\n[311] Erin Grant and Yan Wu. Predicting generalization with degrees of freedom in\nneural networks. In International Conference on Machine Learning 2022 2nd\nAI for Science Workshop, 2022.\n[312] Bradley Efron. Estimating the error rate of a prediction rule: improvement on\ncross-validation. In Journal of the American Statistical Association. Taylor &\nFrancis, 1983.\n[313] Jianming Ye. On measuring and correcting the effects of data mining and\nmodel selection. Journal of the American Statistical Association, 1998.\n[314] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal,\nand Phillip Isola. The low-rank simplicity bias in deep networks. Transactions\non Machine Learning Research, 2023.\n[315] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large\nscale optimization. In Mathematical programming, 1989.\n[316] S´ebastien Bubeck and Mark Sellke. A universal law of robustness via isoperime-\ntry. In Advances in Neural Information Processing Systems, 2021.\n[317] Matteo Gamba, Erik Englesson, M˚arten Bj¨orkman, and Hossein Azizpour. Deep\ndouble descent via smooth interpolation. arXiv preprint arXiv:2209.10080,\n2022.\n[318] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and\nmachine learning. Springer, 2006.\n[319] David M Grobman. Homeomorphism of systems of differential equations.\nDoklady Akademii Nauk SSSR, 1959.\n[320] Yan LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database\nof handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n[321] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\nBIBLIOGRAPHY\n239\n[322] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In Conference on Computer Vision\nand Pattern Recognition, 2009.\n[323] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accu-\nrate deep network learning by exponential linear units (elus). In International\nConference on Learning Representations, 2015.\n[324] Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric numerical\nintegration. Springer-Verlag, Berlin, second edition, 2006.\n[325] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris\nLeary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas,\nSkye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations\nof Python+NumPy programs. 2018. URL http://github.com/google/jax.\n[326] Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku:\nSonnet for JAX. 2020. URL http://github.com/deepmind/dm-haiku.\n[327] Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and\nTom Hennigan. Optax: composable gradient transformation and optimisation,\nin jax! 2020. URL http://github.com/deepmind/optax.\n[328] Marco Cuturi and David Avis. Ground metric learning. In Journal of Machine\nLearning Research, 2014.\n[329] Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew\nHausknecht, and Michael Bowling. Revisiting the arcade learning environment:\nEvaluation protocols and open problems for general agents. In Journal of\nArtificial Intelligence Research, 2018.\n[330] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. Is deep reinforcement\nlearning really superhuman on atari? leveling the playing field. arXiv preprint\narXiv:1908.04683, 2019.\nAppendices\n240\n241\nA On a new continuous-time model of gradient descent\n243\nA.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\nA.2 Comparison with a discrete-time approach . . . . . . . . . . . . . . . 262\nA.3 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\nA.4 Additional experimental results\n. . . . . . . . . . . . . . . . . . . . . 267\nA.5 Figure reproduction details . . . . . . . . . . . . . . . . . . . . . . . . 281\nB Continuous time models of gradient descent in two-player games 286\nB.1 Proof of the main theorems\n. . . . . . . . . . . . . . . . . . . . . . . 286\nB.2 Proof of the main corollaries . . . . . . . . . . . . . . . . . . . . . . . 296\nB.3 Discretisation drift in Runge–Kutta 4 . . . . . . . . . . . . . . . . . . 300\nB.4 Stability analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\nB.5 SGA in zero-sum games\n. . . . . . . . . . . . . . . . . . . . . . . . . 308\nB.6 DiracGAN - an illustrative example . . . . . . . . . . . . . . . . . . . 310\nB.7 The PF for games: linearisation results around critical points . . . . . 314\nB.8 GAN Experimental details . . . . . . . . . . . . . . . . . . . . . . . . 316\nB.9 Additional experimental results\n. . . . . . . . . . . . . . . . . . . . . 317\nB.10 Individual figure reproduction details . . . . . . . . . . . . . . . . . . 329\nC Finding new implicit regularisers by revisiting backward error anal-\nysis\n331\nC.1 Two consecutive steps of SGD . . . . . . . . . . . . . . . . . . . . . . 332\nC.2 Multiple steps of SGD\n. . . . . . . . . . . . . . . . . . . . . . . . . . 334\nC.3 Multiple steps of full-batch gradient descent . . . . . . . . . . . . . . 338\nC.4 Two-player games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\nD The importance of model smoothness in deep learning\n343\nD.1 Individual figure reproduction details . . . . . . . . . . . . . . . . . . 343\nE Spectral Normalisation in Reinforcement learning\n345\nE.1 Additional experimental results\n. . . . . . . . . . . . . . . . . . . . . 345\nE.2\nExperimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\n242\nE.3 Additional hyperparameter sweeps . . . . . . . . . . . . . . . . . . . . 353\nE.4\nIndividual figure reproduction details . . . . . . . . . . . . . . . . . . 356\nF Geometric Complexity: a smoothness complexity measure implicitly\nregularised in deep learning\n358\nF.1\nFigure and experiments reproduction details . . . . . . . . . . . . . . 358\nF.2\nAdditional experimental results\n. . . . . . . . . . . . . . . . . . . . . 359\nAppendix A\nOn a new continuous-time model of\ngradient descent\nA.1\nProofs\nA.1.1\nBEA proof structure\nθt−1\nθt\n= θt−1−h∇θE(θt−1)\nθ(h) = P∞\np=0\nhp\np! θ(p)\n= θt−1−h∇θE(θt−1) + Pn+1\ni=2 hi li(θt−1)\n| {z }\n0\n+O(hn+2)\n˙θ\nO(hn+2)\nFigure A.1: BEA finds continuous modified flows that describe the gradient descent\nupdate with learning rate h with an error of O(hn+2). We identify f1, · · · fn\nso that terms of order O(hp), 2 ≤p ≤n + 1 in θ(h) are 0.\nGeneral structure. The goal of BEA is to find the functions f1, . . . fn such\nthat the flow\n˙θ = −∇θE + hf1(θ) + · · · + hnfn(θ)\n(A.1)\nhas an error ∥θt −θ(h)∥of order O(hn+2) after 1 gradient descent step of learning\nrate h. To do so requires multiple steps (visualized in Figure A.1):\n1. Expand θ(h) via a Taylor expansion in h: θ(h) = P∞\np=0\nhp\np! θ(p);\n2. Expand each θ(p) up to order O(hn+2−p) as a function of f1, . . . fn via the chain\nA.1. Proofs\n244\nrule;\n3. Group together terms of the same order in h in the expansion, up to order\nn + 2.\nθ(h) =\nn+1\nX\ni=0\nhili(θt−1) + O(hn+2)\n(A.2)\n= θt−1 −h∇θE(θt−1) +\nn+1\nX\ni=2\nhili(θt−1) + O(hn+2)\n(A.3)\n4. Compare the above update with the gradient descent update θt = θt−1 −\nh∇θE(θt−1) and conclude that li = 0, ∀i ∈{2, n + 1}. Use this to identify f1,\n. . . fn.\nNotation and context: all proofs below apply to general Euler updates not\nonly gradient descent. We thus assume an update function f with the Euler step\nθt = θt−1 +hf(θt−1). We can then use BEA to find the higher order correction terms\ndescribing the Euler discrete update up to a certain order, and replace f = −∇θE\nto obtain the corresponding results for gradient descent. When we perform a Taylor\nexpansion in h we often drop in notation the evaluation at h = 0 and we make that\nimplicit.\nA.1.2\nThird-order flow\nTheorem A.1.1 The modified flow\n˙θ = f −h1\n2\ndf\ndθf + h2\n \n1\n3\n\u0012 df\ndθ\n\u00132\nf + 1\n12f T df\nd2θf\n!\n(A.4)\nwith θ(0) = θt−1 follows an Euler update θt = θt−1 +hf(θt−1) with an error of O(h4)\nafter 1 gradient descent step.\nProof: We are looking for functions f1 and f2 such that the modified flow\n˙θ = f + hf1 + h2f2\n(A.5)\nA.1. Proofs\n245\nfollows the steps of GD with an error up to O(h4). We perform a Taylor expansion\nof step size h of the above modified flow to be able to see the displacement in that\ntime up to order O(h4). We obtain (all function evaluations of f and fi are at θt−1\nwhich we omit for simplicity, and annotate proof steps, CR denotes Chain Rule):\nθ(h) = θ(0) + h ˙θ(0) + 1\n2h2 ˙˙θ(0) + 1\n6h3 ˙˙˙θ(0) + O(h4)\n(A.6)\n= θt−1 + h\n\u0000f + hf1 + h2f2\n\u0001\n+ 1\n2h2 d\ndt(f + hf1 + h2f2)\n(A.7)\n+ 1\n6h3 ˙˙˙θ + O(h4)\n(using Eq (A.5), A.8)\n= θt−1 + hf + h2f1 + h3f2\n(A.9)\n+ 1\n2h2 d\ndθ\n\u0000f + hf1 + h2f2\n\u0001\n(f + hf1 + h2f2)\n(A.10)\n+ 1\n6h3 ˙˙˙θ + O(h4)\n(CR, A.11)\n= θt−1 + hf + h2f1 + h3f2 + 1\n2h2( df\ndθf + h df\ndθf1 + hdf1\ndθ f)\n(A.12)\n+ 1\n6h3 ˙˙˙θ + O(h4)\n(A.13)\n= θt−1 + hf + h2f1 + h3f2 + 1\n2h2( df\ndθf + h df\ndθf1 + hdf1\ndθ f)\n(A.14)\n+ h3\n6\nd\ndt\n\u0014 df\ndθf + h df\ndθf1 + hdf1\ndθ f\n\u0015\n+ O(h4)\n(A.15)\n= θt−1 + hf + h2f1 + h3f2 + 1\n2h2( df\ndθf + h df\ndθf1 + hdf1\ndθ f)\n(A.16)\n+ 1\n6h3 d\ndt\n\u0014 df\ndθf\n\u0015\n+ O(h4)\n(A.17)\n= θt−1 + hf + h2f1 + h3f2 + 1\n2h2( df\ndθf + h df\ndθf1 + hdf1\ndθ f)\n(A.18)\n+ 1\n6h3 d\ndθ\n\u0012 df\ndθf\n\u0013\n(f + hf1 + h2f2) + O(h4)\n(CR, A.19)\n= θt−1 + hf + h2f1 + h3f2 + 1\n2h2( df\ndθf + h df\ndθf1 + hdf1\ndθ f)\n(A.20)\n+ 1\n6h3 d\ndθ\n\u0012 df\ndθf\n\u0013\nf + O(h4)\n(A.21)\nIf we match the terms with the Euler update, we obtain that:\nf1 = −1\n2\ndf\ndθf\n(A.22)\nA.1. Proofs\n246\nand\nf2 = −1\n2\n\u0014 df\ndθf1 + df1\ndθ f\n\u0015\n−1\n6\nd\ndθ\n\u0012 df\ndθf\n\u0013\nf\n(A.23)\nf2 = −1\n2\n\u0014 df\ndθ(−1\n2\ndf\ndθf) + d\ndθ\n\u0012\n−1\n2\ndf\ndθf\n\u0013\nf\n\u0015\n−1\n6\nd\ndθ\n\u0012 df\ndθf\n\u0013\nf\n(A.24)\nf2 = 1\n4( df\ndθ)\n2\nf + 1\n4\nd\ndθ\n\u0012 df\ndθf\n\u0013\nf −1\n6\nd\ndθ\n\u0012 df\ndθf\n\u0013\nf\n(A.25)\nf2 = 1\n4( df\ndθ)\n2\nf + 1\n12\nd\ndθ\n\u0012 df\ndθf\n\u0013\nf\n(A.26)\nf2 = 1\n4( df\ndθ)\n2\nf + 1\n12\n\u0012\n( df\ndθ)\n2\n+ f T df\nd2θ\n\u0013\nf\n(A.27)\nf2 = 1\n3( df\ndθ)\n2\nf + 1\n12f T df\nd2θf\n(A.28)\nwhere f T df\nd2θ is a matrix in RD×D, with\n\u0000f T df\nd2θ\n\u0001\ni,j =\n\u0012\nd dfi\ndθj\ndθ\n\u0013T\nf and f T df\nd2θf is a\nvector in RD with\n\u0000f T df\nd2θf\n\u0001\nk = P\ni,j\ndfk\ndθidθj fifj.\nReplacing f1 and f2 in Eq (A.5) leads to the desired Eq (A.4).\n□\nIn the case of gradient descent, f = −∇θE, and thus df\ndθ = ∇2\nθE and we write\ndf\nd2θ = ∇3\nθE ∈RD×D×D and (∇θET(∇3\nθE)∇θE)k = P\ni,j (∇θE)i(∇3\nθE)i,k,j(∇θE)j.\nSubstituting in f1 and f2, we obtain (Eq (3.4)):\n˙θ = −∇θE −h\n2∇2\nθE∇θE −h2\n\u00121\n3(∇2\nθE)2∇θE + 1\n12∇θET(∇3\nθE)∇θE\n\u0013\n.\n(A.29)\nA.1.3\nHigher order terms of the form ( df\ndθ)\nnf or\n\u0000∇2\nθE\n\u0001n ∇θE\nTheorem A.1.2 The modified flow with an error of order O(hp+1) to the Euler\nupdate θt = θt−1 + hf(θt−1) has the form:\n˙θ =\np\nX\nn=0\n(−1)n\nn + 1 ( df\ndθ)\nn\nf + C( df\nd2θ)\n(A.30)\nwhere C( df\nd2θ) denotes a class of functions defined as a sum of individual terms each\ncontaining higher than first order derivatives applied to f.\nA.1. Proofs\n247\nProof: Consider the modified flow given by BAE:\n˙θ = ˜f = f + hf1 + h2f2 + · · · + hnfn + . . .\n(A.31)\nFor the proof we need to show that the only terms not involving higher order\nderivatives are of the form ( df\ndθ)\nnf and we need to find the coefficients of ( df\ndθ)\nnf in\nfn, which we will call cn. We already know that c0 = 1, c1 = −1\n2 (Eq. (A.22)) and\nc3 = 1\n3 (Eq. (A.28)). We use the general structure provided in Section A.1.1.\nWe start by expanding θ(h) via Taylor expansion (Step 1 in the proof structure\nin Section A.1.1):\nθ(h) = θ(0) + h ˙θ +\n∞\nX\nk=2\n1\nk!θ(k) = θt−1 + hf(θt−1) +\n∞\nX\nn=1\nhn+1fn +\n∞\nX\nk=2\nhk\nk! θ(k)\n(A.32)\nWe now express in θ(k) in terms of the wanted quantities fn via Lemma A.1.3:\nθ(k) =\nX\nm≥0\nhm dk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ fi\nk ≥2\n(A.33)\nBy replacing Eq (A.33) in Eq (A.32) we obtain (Step 2 in the proof structure in\nSection A.1.1):\nθ(h) = θt−1 + hf(θt−1) +\n∞\nX\nn=1\nhn+1fn(θt−1)\n(A.34)\n+\n∞\nX\nk=2\nhk\nk!\nX\nm≥0\nhm dk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ\n\f\f\f\f\nθt−1\nfi(θt−1)\n(A.35)\n= θt−1 + hf(θt−1) +\n∞\nX\nn=1\nhn+1fn(θt−1)\n(A.36)\n+\nX\nm≥0\n∞\nX\nk=2\nhk+m\nk!\ndk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ\n\f\f\f\f\nθt−1\nfi(θt−1)\n(A.37)\nWe note that due to the derivative w.r.t. t and a dependence of h in the modified\nvector field, there will be a dependence of h in\ndk−2\ndtk−2\nP\ni+j=m\ndfj\ndθ fi, since the chain\nA.1. Proofs\n248\nrule will expose dθ\ndt which depends on h (Eq (A.31)). To highlight this dependence\non h we introduce the notation\ndk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ fi =\nX\np\nhpAp\nk,m\n(A.38)\nwhere Ap\nk,m : RD →RD is defined to be the term of order hp in\ndk−2\ndtk−2\nP\ni+j=m\ndfj\ndθ fi;\nthus by definition Ap\nk,m does not depend on h. Replacing the above in Eq (A.37), we\nconclude Step 3 outlined in the proof structure in Section A.1.1:\nθ(h) = θt−1 + hf(θt−1) +\n∞\nX\nn=1\nhn+1fn(θt−1) +\nX\nm≥0\n∞\nX\nk=2\nhk+m\nk!\nX\np\nhpAp\nk,m(θt−1)\n(A.39)\nSince the Euler update θt = θt−1 + hf(θt−1) does not involve terms of order higher\nthan h the fn terms are obtained by cancelling the terms of order O(hn+1) in\nθ(h) (Step 4 outlined in the proof structure in Section A.1.1), we have that for\nk + m + p = n + 1:\nfn = −\nX\nm≥0\n∞\nX\nk=2\n1\nk!An+1−(k+m)\nk,m\n(A.40)\nSince An+1−(k+m)\nk,m\ndepends on the functions fn, we have found a recursive relation\nfor fn. We now use induction to show that\nfn = cn( df\ndθ)\nn\nf + C( df\nd2θ)\n(A.41)\nwhere C( df\nd2θ) denotes a class of functions defined as a sum of individual terms each\ncontaining higher than first order derivatives applied to f (Lemma A.1.4). The same\nproof shows the recurrence relation for cn:\ncn = −\nn+1\nX\nk=2\n1\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n(A.42)\nA.1. Proofs\n249\nwith solution cn = (−1)n\nn+1 (Lemma A.1.5).\nWe have found that each fn can be written as (−1)n\nn+1 ( df\ndθ)\nnf + C( df\nd2θ). Replacing\nthis in Eq.(A.31) concludes the proof.\n□\nIn the case of gradient descent, where f = −∇θE and df\ndθ = ∇2\nθE, we have that\nthe modified flow has the form\n˙θ =\np\nX\nn=0\n(−1)n\nn + 1 (−df\ndθ)n(−∇θE) + C( df\nd2θ)\n(A.43)\n=\np\nX\nn=0\n(−1)n\nn + 1 (−∇2\nθE)n(−∇θE) + C( df\nd2θ)\n(A.44)\n=\np\nX\nn=0\n−1\nn + 1(∇2\nθE)n(∇θE) + C( df\nd2θ),\n(A.45)\nwhich is the statement of Theorem 3.3.1 in the main thesis.\nLemma A.1.3\nθ(k) =\nX\nm≥0\nhm dk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ fi\nk ≥2\n(A.46)\nProof:\n˙θ = ˜f\n(A.47)\n˙˙θ = d ˜f\ndθ\n˜f\n(A.48)\nθ(k) = dk−2\ndtk−2\n˙˙θ = dk−2\ndtk−2\nd ˜f\ndθ\n˜f\n(A.49)\nWe have that\nd ˜f\ndθ\n˜f =\n X\ni\nhidfi\ndθ\n!  X\nj\nhjfj\n!\n=\nX\nm≥0\nhm X\ni+j=m\ndfi\ndθfj\n(A.50)\nwhich concludes the proof. We note that this is an adaptation of Lemma A.2 from [12].\nWe need this adaptation to avoid assuming that f is a symmetric vector field—i.e.\nA.1. Proofs\n250\nthat d ˜f\ndθ ˜f = 1\n2∇θ\n\r\r\r ˜f\n\r\r\r\n2\n.\n□\nLemma A.1.4 By induction we have that:\nfn = cn( df\ndθ)\nn\nf + C( df\nd2θ)\n(A.51)\nwith\ncn = −\nn+1\nX\nk=2\n1\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n(A.52)\nProof: Base step: This is true for n = 1, 2 and 3, for which we computed all terms\nin the BEA expansion.\nInduction step: We know that (Eq (A.38)):\nfn = −\nX\nm≥0\n∞\nX\nk=2\n1\nk!An+1−(k+m)\nk,m\n(A.53)\nwhere Ap\nk,m is defined as the coefficient of hp in:\ndk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ fi =\nX\np\nhpAp\nk,m\n(A.54)\nUnder the induction hypothesis we can use that\nfi = ci( df\ndθ)\ni\nf + C( df\nd2θ)\n∀i < n\n(A.55)\nAnd thus that the flow is of the form:\n˙θ = f +\nn−1\nX\ni=1\nci( df\ndθ)\ni\nf + C( df\nd2θ) + O(hn)\n(A.56)\nTo find fn (Eq (A.53)) we need to find An+1−(k+m)\nk,m\nwith k ≥2 and m ≥0. We thus\nneed to find Ap\nk,m with p ≤n −1 (since p ≤(n + 1) −(k + m) and k ≥2, m ≥\n0 =⇒p ≤n −1). We do so expanding\ndk−2\ndtk−2\nP\ni+j=m\ndfj\ndθ fi under the induction step.\nA.1. Proofs\n251\nSince i + j = m and k + m ≤n + 1 and k ≥2, we have that m ≤n −1 and thus\ni, j ≤n −1 and thus we can apply the induction hypothesis for fi and fj.\nWe annotate the steps with IH (Induction Hypothesis, often by using Eq. (A.56))\nand CR (Chain Rule) and S (Simplifying or grouping terms of C( df\nd2θ)) by using that\nproducts and sum of terms in this function class also belong in this function class).\ndk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ fi\n(A.57)\n= dk−2\ndtk−2\nX\ni+j=m\nd\ndθ\n\u0012\ncj( df\ndθ)\nj\nf + C( df\nd2θ)\n\u0013\n(ci( df\ndθ)\ni\nf + C( df\nd2θ))\n(IH, A.58)\n= dk−2\ndtk−2\nX\ni+j=m\n\u0012\ncj( df\ndθ)\nj+1\n+ cjf T d\ndθ\n\u0012\n( df\ndθ)\nj\u0013\n+ C( df\nd2θ)\n\u0013\n(ci( df\ndθ)\ni\nf + C( df\nd2θ))\n(CR, A.59)\n= dk−2\ndtk−2\nX\ni+j=m\n\u0012\ncj( df\ndθ)\nj+1\n+ C( df\nd2θ)\n\u0013\n(ci( df\ndθ)\ni\nf + C( df\nd2θ))\n(S, A.60)\n= dk−2\ndtk−2\nX\ni+j=m\ncicj( df\ndθ)\nm+1\nf + C( df\nd2θ)\n(S, A.61)\n= dk−3\ndtk−3\nX\ni+j=m\nd\ndtcicj( df\ndθ)\nm+1\nf + C( df\nd2θ)\n(A.62)\n= dk−3\ndtk−3\nX\ni+j=m\nd\ndθ\n\u0012\ncicj( df\ndθ)\nm+1\nf + C( df\nd2θ)\n\u0013 dθ\ndt\n(CR, A.63)\n= dk−3\ndtk−3\nX\ni+j=m\nd\ndθ\n\u0012\ncicj( df\ndθ)\nm+1\nf + C( df\nd2θ)\n\u0013  n−1\nX\nl1=0\nhl1fl1 + O(hn)\n!\n(Eq. (A.56), A.64)\n= dk−3\ndtk−3\nX\ni+j=m\nd\ndθ\n\u0012\ncicj( df\ndθ)\nm+1\nf + C( df\nd2θ)\n\u0013\n(\nn−1\nX\nl1=0\nhl1fl1) + O(hn)\n(A.65)\n= dk−3\ndtk−3\nX\ni+j=m\n\u0012\ncicj( df\ndθ)\nm+2\n+ C( df\nd2θ)\n\u0013\n(\nn−1\nX\nl1=0\nhl1fl1) + O(hn)\n(CR, S, A.66)\n= dk−3\ndtk−3\nX\ni+j=m\n\u0012\ncicj( df\ndθ)\nm+2\n+ C( df\nd2θ)\n\u0013  n−1\nX\nl1=0\nhl1cl1( df\ndθ)l1f + C( df\nd2θ)\n!\n+ O(hn)\n(IH, A.67)\nA.1. Proofs\n252\nContinuing by applying the induction hypothesis and the chain rule we obtain:\ndk−2\ndtk−2\nX\ni+j=m\ndfj\ndθ fi\n(A.68)\n= dk−3\ndtk−3\nX\ni+j=m\nn−1\nX\nl1=0\nhl1cicjcl1( df\ndθ)\nl1+m+2\nf + C( df\nd2θ) + O(hn)\n(S, A.69)\n= dk−4\ndtk−4\nX\ni+j=m\n n−1\nX\nl1=0\nhl1cicjcl1\nd\ndθ\n\u0012\n( df\ndθ)\nl1+m+2\nf + C( df\nd2θ)\n\u0013!\n(\nn−1\nX\nl2=0\nhl2fl2) + O(hn)\n(CR, Eq. (A.56), A.70)\n= dk−4\ndtk−4\nX\ni+j=m\n n−1\nX\nl1=0\nhl1cicjcl1\n\u0012\n( df\ndθ)\nl1+m+3\n+ C( df\nd2θ)\n\u0013!\n(A.71)\n n−1\nX\nl2=0\nhl2cl2( df\ndθ)\nl2\nf + C( df\nd2θ)\n!\n+ O(hn)\n(CR, IH, A.72)\n= dk−4\ndtk−4\nX\ni+j=m\nn−1\nX\nl1=0\nn−1\nX\nl2=0\nhl1+l2cicjcl1cl2( df\ndθ)\nl1+l2+m+3\nf + C( df\nd2θ) + O(hn)\n(S, A.73)\n=\nX\ni+j=m\nn−1\nX\nl1=0\nn−1\nX\nl2=0\n· · ·\nn−1\nX\nlk−2=0\nhl1+l2+···+lk−2cicjcl1cl2 . . . clk−2( df\ndθ)\nl1+l2+···+lk−2+m+k−1\nf\n+ C( df\nd2θ) + O(hn)\n(A.74)\n=\nX\ni+j=m\nX\n0≤l1,...,lk−2≤n−1,\nl1+l2+...lk−2=p\nhpcicjcl1cl2 . . . clk−2( df\ndθ)\np+m+k−1\nf + C( df\nd2θ) + O(hn) (A.75)\nBy extracting the terms of O(hp) with p ≤n−1 from the above we can now conclude\nthat (note that since we are now only concerned with p ≤n−1 we drop the li ≤n−1\nsince it is implied from l1 + l2 + . . . lk−2 = p):\nAp\nk,m =\nX\ni+j=m\nX\nl1,...,lk−2≥0,\nl1+l2+...lk−2=p\ncicjcl1cl2 . . . clk−2( df\ndθ)\np+m+k−1\nf + C( df\nd2θ)\n(A.76)\n∀p ≤n −1\n(A.77)\nA.1. Proofs\n253\nReplacing this in fn:\nfn = −\nX\nm≥0\n∞\nX\nk=2\n1\nk!An+1−(k+m)\nk,m\n(A.78)\n= −\nX\nm≥0\n∞\nX\nk=2\n1\nk!\nX\ni+j=m\nX\nl1,...,lk−2≥0,\nl1+l2+...lk−2=n+1−(k+m)\ncicjcl1cl2 . . . clk−2( df\ndθ)\nn+1−(k+m)+k+m−1\nf\n+ C( df\nd2θ)\n(A.79)\n= −\nX\nm≥0\n∞\nX\nk=2\n1\nk!\nX\ni+j=m\nX\nl1,...,lk−2≥0,\nl1+l2+...lk−2=n+1−(k+m)\ncicjcl1cl2 . . . clk−2( df\ndθ)\nn\nf + C( df\nd2θ)\n(A.80)\n= −\n∞\nX\nk=2\n1\nk!\nX\nm≥0\nX\nl1,...,lk≥0,\nl1+l2+...lk=n+1−k\ncl1cl2 . . . clk( df\ndθ)\nn\nf + C( df\nd2θ)\n(A.81)\nWe can now say that fn = cn( df\ndθ)\nnf + C( df\nd2θ). Not only that, we have now found the\nrecurrence relation we were seeking:\ncn = −\nn+1\nX\nk=2\n1\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n(A.82)\n□\nLemma A.1.5 The solution to the recurrence relation\ncn = −\nn+1\nX\nk=2\n1\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n(A.83)\nwith c0 = 1, is cn = (−1)n\nn+1 .\nProof: We will use generating functions to solve the recurrence. Let c(x) be the\ngenerating function of cn:\nc(x) = c0 +\n∞\nX\nn=1\ncnxn\n(A.84)\nA.1. Proofs\n254\nWe have that:\nc(x) = 1 −\n∞\nX\nn=1\nxn\n n+1\nX\nk=2\n1\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n!\n(A.85)\n= 1 −1\nx\n∞\nX\nn=1\nxn+1\n n+1\nX\nk=2\n1\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n!\n(A.86)\n= 1 −1\nx\n∞\nX\nn=1\n n+1\nX\nk=2\nxk\nk! xn+1−k\nX\nl1+l2+···+lk=n−k+1\ncl1cl2 . . . clk\n!\n(A.87)\n= 1 −1\nx\n∞\nX\nn=1\n n+1\nX\nk=2\nxk\nk!\nX\nl1+l2+···+lk=n−k+1\ncl1xl1cl2xl2 . . . clkxlk\n!\n(A.88)\n= 1 −1\nx\n∞\nX\nk=2\nxk\nk!\n ∞\nX\nl1=0\n∞\nX\nl2=0\n∞\nX\nlk=0\ncl1xl1cl2xl2 . . . clkxlk\n!\n(A.89)\n= 1 −1\nx\n∞\nX\nk=2\nxk\nk!\n ∞\nX\nl1=0\ncl1xl1\n!  ∞\nX\nl2=0\ncl2xl2\n!\n. . .\n ∞\nX\nlk=0\nclkxlk\n!\n(A.90)\n= 1 −1\nx\n∞\nX\nk=2\nxk\nk! c(x)k\n(A.91)\n= 1 −1\nx(exc(x) −1 −xc(x))\n(A.92)\nWhere Eq (A.89) can be obtained via the following reasoning:\n∞\nX\nk=2\nxk\nk!\n ∞\nX\nl1=0\n∞\nX\nl2=0\n∞\nX\nlk=0\ncl1xl1cl2xl2 . . . clkxlk\n!\n(A.93)\n=\n∞\nX\nk=2\nxk\nk!\n ∞\nX\nm=0\nX\nl1+l2+...lk=m\ncl1xl1cl2xl2 . . . clkxlk\n!\n(A.94)\n=\n∞\nX\nk=2\nxk\nk!\n \n∞\nX\nn=k−1\nX\nl1+l2+...lk=n+1−k\ncl1xl1cl2xl2 . . . clkxlk\n!\n(A.95)\n=\n∞\nX\nk=2\n∞\nX\nn=k−1\nxk\nk!\nX\nl1+l2+...lk=n+1−k\ncl1xl1cl2xl2 . . . clkxlk\n(A.96)\n=\n∞\nX\nn=1\nn+1\nX\nk=2\nxk\nk!\nX\nl1+l2+...lk=n+1−k\ncl1xl1cl2xl2 . . . clkxlk\n(A.97)\nA.1. Proofs\n255\nSolving for c(x) in Eq (A.92) we obtain:\nc(x) = 1 −1\nx(exc(x) −1 −xc(x))\n(A.98)\nc(x) = 1 −1\nxexc(x) + 1\nx + c(x)\n(A.99)\n0 = 1 −1\nxexc(x) + 1\nx\n(A.100)\nexc(x) = x + 1\n(A.101)\nxc(x) = log(x + 1)\n(A.102)\nc(x) = log(x + 1)\nx\n(A.103)\nWe now perform the series expansion for c(x):\nc(x) = log(x + 1)\nx\n=\nP∞\nn=1\n(−1)n−1\nn\nxn\nx\n=\n∞\nX\nn=1\n(−1)n−1\nn\nxn−1 =\n∞\nX\nn=0\n(−1)n\nn + 1 xn\n(A.104)\nWe thus have that cn = (−1)n\nn+1 which finishes the proof.\n□\nA.1.4\nLinearisation results around critical points\nAssume we are around a critical point θ∗, i.e.\n∇θE(θ∗) = 0.\nVia the Hart-\nman–Grobman theorem [118, 319] we can write:\n∇θE ≈∇θE(θ∗) + ∇2\nθE(θ∗)(θ −θ∗) = ∇2\nθE(θ∗)(θ −θ∗)\n(A.105)\nReplacing this in the gradient descent updates:\nθt = θt−1 −h∇θE(θt−1) =⇒θt −θ∗≈(I −h∇2\nθE(θ∗))(θt−1 −θ∗)\n(A.106)\n=⇒θt −θ∗≈(I −h∇2\nθE(θ∗))t(θ0 −θ∗)\n(A.107)\nSince (I −h∇2\nθE(θ∗))t (θ0 −θ∗) = PD−1\ni=0 (1 −hλ∗\ni )tu∗\ni u∗\ni\nT(θ0 −θ∗), gradient descent\nconverges in the limit of t →∞when |1 −hλ∗\ni | < 1, i.e. λi < 2/h. Thus, we obtain\nthe same conclusion as obtained with the PF around a local minima.\nA.1. Proofs\n256\nWe now also show how we can use these results to derived a specialised version\nof the PF, one that is only valid around critical points and requires the additional\nassumption that the Hessian of the critical point is invertible. We start with\nθt −θ∗≈(I −h∇2\nθE(θ∗))t(θ0 −θ∗) =\nD−1\nX\ni=0\n(1 −hλ∗\ni )tu∗\ni u∗\ni\nT(θ0 −θ∗).\n(A.108)\nUnder the above approximation we have:\nθt = θ∗+\nD−1\nX\ni=0\n(1 −hλ∗\ni )tu∗\ni u∗\ni\nT(θ0 −θ∗)\n(A.109)\nwhich we can write in continuous form as (we use that iteration n is at time t = nh):\nθ(t) = θ∗+\nD−1\nX\ni=0\n(1 −hλ∗\ni )t/hu∗\ni u∗\ni\nT(θ0 −θ∗)\n(A.110)\n= θ∗+\nD−1\nX\ni=0\nelog(1−hλ∗\ni )t/hu∗\ni u∗\ni\nT(θ0 −θ∗).\n(A.111)\nTaking the derivative wrt to t:\n˙θ =\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nh\nelog (1−hλ∗\ni )t/hu∗\ni u∗\ni\nT(θ0 −θ∗)\n|\n{z\n}\nEq.(A.109)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nh\n(θ −θ∗)\n(A.112)\nFrom here, if we assume the Hessian at the critical point is invertible we can\nwrite from the approximation above (Eq. (A.105)) that θ −θ∗= (∇2\nθE)−1∇θE =\nPD−1\ni=0\n1\nλ∗\ni ∇θETu∗\ni u∗\ni\nT and replacing this in the above we the PF:\n˙θ =\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\n∇θETu∗\ni u∗\ni\nT.\n(A.113)\nA.1.5\nThe solution of the PF for quadratic losses\nWe derive the solution of the PF in the case of quadratic losses - Remark 3.3.1 in\nthe main thesis. We have E(θ) = 1\n2θtAθ + bTθ with A symmetric. ∇θE = Aθ + b\nA.1. Proofs\n257\nand ∇2\nθE = A. Thus λi and ui are the eigenvalues and eigenvectors of A and do not\nchange based on θ. Replacing this in the PF leads to:\n˙θ =\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\n(Aθ + b)T uiui\n(A.114)\n=\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\n(Aθ)Tuiui +\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\nbTuiui\n(A.115)\n=\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\nθTAuiui +\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\nbTuiui\n(A.116)\n=\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\nλiθTuiui +\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\nbTuiui\n(A.117)\n=\nD−1\nX\ni=0\nlog(1 −hλi)\nh\nθTuiui +\nD−1\nX\ni=0\nlog(1 −hλi)\nhλi\nbTuiui\n(A.118)\nFrom here\n˙\n(θTui) = log(1 −hλi)\nh\nθTui + log(1 −hλi)\nhλi\nbTui.\n(A.119)\nThe solution of this flow is (θTui)(t) = e\nlog(1−hλi)\nh\ntθT\n0 ui + tlog(1−hλi)\nhλi\nbTui.\nSince ui form a basis of RD, we can now write\nθ(t) =\nD−1\nX\ni=0\nθ(t)Tuiui\n(A.120)\n=\nD−1\nX\ni=0\n\u0012\ne\nlog(1−hλi)\nh\ntθT\n0 ui + tlog(1 −hλi)\nhλi\nbTui\n\u0013\nui.\n(A.121)\nWe thus have obtained the solution described in Remark 3.3.1 in the main text\nof the thesis.\nA.1.6\nThe Jacobian of the PF at critical points\nWe compute Jacobian of the PF at a critical point θ∗with the eigenvalues and\neigenvectors of ∇2\nθE(θ∗) denoted as λ∗\ni and u∗\ni . We will use that ∇θE(θ∗) = 0.\nA.1. Proofs\n258\nJPF(θ∗) =\nD−1\nX\ni=0\nd(∇θETui)log(1−hλi)\nhλi\nui\ndθ\n(θ∗)\n(A.122)\n=\nD−1\nX\ni=0\nd(∇θETui)log(1−hλi)\nhλi\ndθ\n(θ∗)u∗\ni\nT\n(A.123)\n+\nD−1\nX\ni=0\n∇θE(θ∗)T\n|\n{z\n}\n0\nu∗\ni\nlog(1 −hλ∗\ni )\nhλ∗\ni\ndui\ndθ\n(A.124)\n=\nD−1\nX\ni=0\nd(∇θETui)log(1−hλi)\nhλi\ndθ\n(θ∗)u∗\ni\nT\n(A.125)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\nd(∇θETui)\ndθ\n(θ∗)u∗\ni\nT\n(A.126)\n+\nD−1\nX\ni=0\n(∇θE(θ∗)T\n|\n{z\n}\n0\nu∗\ni )\ndlog(1−hλi)\nhλi\ndθ\nu∗\ni\nT\n(A.127)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\nd(∇θETui)\ndθ\n(θ∗)u∗\ni\nT\n(A.128)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\n\u0012d∇θE\ndθ\n(θ∗)\n\u0013T\nu∗\ni u∗\ni\nT\n(A.129)\n+\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\n\u0012dui\ndθ (θ∗)\n\u0013T\n∇θE(θ∗)\n|\n{z\n}\n0\nu∗\ni\nT\n(A.130)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\n∇2\nθE(θ∗)u∗\ni u∗\ni\nT\n(A.131)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nhλ∗\ni\nλ∗\ni u∗\ni u∗\ni\nT\n(A.132)\n=\nD−1\nX\ni=0\nlog(1 −hλ∗\ni )\nh\nu∗\ni u∗\ni\nT\n(A.133)\nWe thus arrived at the result used in Equation 3.13 of the thesis, where we\nused to perform stability analysis on the PF. This results shows that the eigenvalues\nof the Jacobian for the PF are log(1−hλ∗\ni )\nh\n, thus local minima where λ∗\ni ≥0 are only\nattractive if hλ∗\ni < 2.\nA.1. Proofs\n259\nA.1.7\nJacobians of the IGR flow and NGF at critical points\nFor the NGF, we have\nJNGF(θ∗) = −∇2\nθE(θ∗) = −\nD−1\nX\ni=0\nλ∗\ni u∗\ni u∗\ni\nT\n(A.134)\nand thus the eigenvalues of the Jacobian for the NGF are −λ∗\ni . Thus local minima\nwhere λ∗\ni > 0 =⇒−λ∗\ni < 0 are attractive for the NGF.\nFor the IGR flow, we have\nJIGR(θ∗) = −\nd\n\u0010\n∇θE + h2\n2 ∇2\nθE∇θE\n\u0011\ndθ\n(θ∗)\n(A.135)\n= −d∇θE\ndθ\n(θ∗) −h2\n2\nd∇2\nθE∇θE\ndθ\n(θ∗)\n(A.136)\n= −\nD−1\nX\ni=0\nλ∗\ni u∗\ni u∗\ni\nT −h2\n2\n\u0012\n∇θET∇3\nθE + ∇2\nθE d∇θE\ndθ\n\u0013\n(θ∗)\n(A.137)\n= −\nD−1\nX\ni=0\nλ∗\ni u∗\ni u∗\ni\nT −h2\n2 ∇θE(θ∗)T\n|\n{z\n}\n0\n∇3\nθE(θ∗) −h2\n2 ∇2\nθE (θ∗) ∇2\nθE (θ∗)\n(A.138)\n= −\nD−1\nX\ni=0\nλ∗\ni u∗\ni u∗\ni\nT −h2\n2 ∇2\nθE (θ∗) ∇2\nθE (θ∗)\n(A.139)\n= −\nD−1\nX\ni=0\nλ∗\ni u∗\ni u∗\ni\nT −\nD−1\nX\ni=0\nh2\n2 λ∗\ni\n2u∗\ni u∗\ni\nT\n(A.140)\n= −\nD−1\nX\ni=0\n\u0012\nλ∗\ni + h2\n2 λ∗\ni\n2\n\u0013\nu∗\ni u∗\ni\nT\n(A.141)\nand thus the eigenvalues of the Jacobian for the IGR flow are −\n\u0010\nλ∗\ni + h2\n2 λ∗\ni\n2\u0011\n. Thus\nlocal minima where λ∗\ni > 0 =⇒−\n\u0010\nλi + h2\n2 λ∗\ni\n2\u0011\n< 0 are attractive for the IGR flow.\nA.1.8\nMultiple gradient descent steps\nWe now show that if we use BEA to construct a flow that has an error of order\nO(hp) after 1 gradient descent step, then the error will be of the same order after 2\nsteps. The same argument can be applied to multiple steps, though the error scales\nproportionally to the number of steps and the bound becomes vacuous as the number\nA.1. Proofs\n260\nof steps increases. We have ˙θ with θ(0) = θt−1 tracks θt = θt−1 −h∇θE(θt−1) with\nan error O(hp) (i.e ∥θ(h) −θt∥is O(hp)), then ˙θ tracks two steps of gradient descent\nθt = θt−1 −h∇θE(θt−1) and θt+1 = θt −h∇θE(θt) with an error of the same order\nO(hp) (i.e. ∥θ(2h) −θt+1∥is O(hp)). We prove this below. For the purpose of this\nproof we use the following notation θ(h; θ′) is the value of the flow at time h with\ninitial condition θ′. Making the initial condition explicit is necessary for the proof.\n∥θ(2h; θt−1) −θt+1∥= ∥θ(2h; θt−1) −θ(h; θt) + θ(h; θt) −θt+1∥\n(A.142)\n≤∥θ(2h; θt−1) −θ(h; θt)∥+ ∥θ(h; θt) −θt+1∥\n(A.143)\n≤∥θ(2h; θt−1) −θ(h; θt)∥+ O(hp)\n(A.144)\n= ∥θ(h; θ(h; θt−1)) −θ(h; θt)∥+ O(hp)\n(A.145)\nWe thus have to bound how the flow changes after time h when starting with two\ndifferent initial conditions θ(h; θt−1) and θt). We also know that ∥θ(h; θt−1) −θt∥is\nO(hp). Thus by expanding the Taylor series and the mean value theorem we obtain:\n∥θ(h; θ(h; θt−1)) −θ(h; θt)∥\n(A.146)\n=\n\r\r\r\r\r\n∞\nX\ni=0\nhi\ni! θ(i) (θ(h; θt−1)) −\n∞\nX\ni=0\nhi\ni! θ(i)(θt))\n\r\r\r\r\r\n(A.147)\n=\n\r\r\r\r\r\n∞\nX\ni=0\nhi\ni!\n\u0000θ(i) (θ(h; θt−1)) −θ(i)(θt))\n\u0001\n\r\r\r\r\r\n(A.148)\n=\n\r\r\r\r\r\n∞\nX\ni=0\nhi\ni!\nd\ndθθ(i)(θ′) (θ(h; θt−1) −θt)\n\r\r\r\r\r\n(A.149)\n≤\n∞\nX\ni=0\nhi\ni!\n\f\f\f\f\nd\ndθθ(i)(θ′)\n\f\f\f\f ∥(θ(h; θt−1)) −θt∥\n|\n{z\n}\nO(hp)\n= O(hp)\n(A.150)\nThis tells us that we can construct a bound:\n∥θ(2h; θt−1) −θt+1∥≤O(hp) + O(hp) = O(hp)\n(A.151)\nDependence on the number of steps. While the order in learning rate\nis the same, we note that as the number of steps increases the errors are likely to\nA.1. Proofs\n261\naccumulate with the number of steps (for n discrete steps we will sum n terms of\norder O(hp) in the above bound). For example when taking the number of steps\nn →∞the above no longer provides a bound.\nA.1.9\nApproximations to per-iteration drift for gradient\ndescent and momentum\nWe now prove Thm 3.7.1, on the per-iteration drift of gradient descent. We apply\nthe Taylor reminder theorem on the NGF initialised at gradient descent iteration\nparameters θt−1. We have that there exists h′ such that\nθ(h) =θ(0) + h ˙θ(0) + h2\n2\n˙˙θ(h′)\n(A.152)\n=θt−1 −h∇θE(θt−1) + h2\n2 ∇2\nθE(θ′)∇θE(θ′)\n(A.153)\nwith h′ ∈(0, h). The above proof measures the drift of gradient descent, which we\nthen used to construct DAL.\nIn Section 3.8, we also used DAL for momentum, and used an intuitive jus-\ntification. We now show a more theoretical justification, by measuring the total\nper-iteration drift of momentum. To do so, we use the following flow to describe\nmomentum, provided by Kunin et al. [65]\n˙θ = −\n1\n1 −β ∇θE,\n(A.154)\nand we measure the discretisation drift of a momentum update compared to this\nflow. If we apply the same strategy as above, we have that\nθ(h) = θ(0) + h ˙θ(0) + h2\n2\n˙˙θ(h′)\n(A.155)\n= θt−1 −\nh\n1 −β ∇θE(θt−1) +\nh2\n2(1 −β)2∇2\nθE(θ′)∇θE(θ′)\n(A.156)\nWhen we compare this with the discrete update\nθt = θt−1 + βvt−1 −h∇θE(θt−1)\n(A.157)\nA.2. Comparison with a discrete-time approach\n262\nand obtain that the per-iteration drift is\n\u0012\nθt−1 −\nh\n1 −β ∇θE(θt−1) +\nh2\n2(1 −β)2∇2\nθE(θ′)∇θE(θ′)\n\u0013\n(A.158)\n−(θt−1 + βvt−1 −h∇θE(θt−1))\n(A.159)\n=\nh2\n2(1 −β)2∇2\nθE(θ′)∇θE(θ′) + βvt−1 + h∇θE(θt−1) −\nh\n1 −β ∇θE(θt−1) (A.160)\n=\nh2\n2(1 −β)2∇2\nθE(θ′)∇θE(θ′) + βvt−1 −\nhβ\n1 −β ∇θE(θt−1)\n(A.161)\nThus there is one part of the drift that comes from the same source as gradient\ndescent and another part of the drift which comes from the alignment of the current\ngradient and the moving average obtained using momentum. Thus when using\nDAL-p with momentum, we only focus on one of the sources of drift (the first term).\nA.2\nComparison with a discrete-time approach\nA.2.1\nChanges in loss function\nWe aim to obtain similar intuition to what we have obtained from the PF by\ndiscretising the NGF using Euler steps. We have:\nE(θt+1) ≈E(θt) −h∇θE(θt)T∇θE(θt) + h2\n2 ∇θE(θt)T∇2\nθE(θt)∇θE(θt) (A.162)\n≈E(θt) −h∇θE(θt)T∇θE(θt) + h2\n2\nX\ni\nλi\n\u0000∇θE(θt)Tui\n\u00012\n(A.163)\n≈E(θt) −h\nX\ni\n\u0000∇θE(θt)Tui\n\u00012 + h2\n2\nX\ni\nλi\n\u0000∇θE(θt)Tui\n\u00012\n(A.164)\n≈E(θt) +\nX\ni\n(1 −h/(2λi))\n\u0000∇θE(θt)Tui\n\u00012\n(A.165)\nthus under this approximation loss to decrease between iterations one requires\n1 −h/(2λi) ≤0. This is consistent with the observations of the loss obtained using\nthe PF in Eq (3.12).\nA.2. Comparison with a discrete-time approach\n263\nA.2.2\nThe dynamics of ∇θETui\nWe now show to obtain the approximated dynamics of ∇θETui obtained from the\nPF in Section 3.5 using a discrete-time approach.\nWe have that:\n∇θE(θt+1) ≈∇θE (θt −h∇θE(θt))\n(A.166)\n≈∇θE(θt) −h∇2\nθE(θt)∇θE(θt)\n(A.167)\nIf we assume that the Hessian eigenvectors do not change between iterations, we\nhave:\n∇θE(θt+1)T(ui)t+1 ≈∇θE(θt)T(ui)t+1 −h∇2\nθE(θt)∇θE(θt)(ui)t+1\n(A.168)\n= (1 −hλi)∇θE(θt)T(ui)t+1\n(A.169)\nand obtain the same behaviour predicted in Section 3.5.\nA.2.3\nThe connection between DAL and Taylor expansion\noptimal learning rate\nIn Section 3.7.2 introduced DAL as a way to control the drift of gradient descent;\nto do so, we set the learning rate of gradient descent to be inverse proportional to\n∥∇2\nθE∇θE∥/ ∥∇θE∥= ∥∇2\nθEˆg∥.\nThe learning rate set by DAL is similar, but distinct to that obtained by using\na Taylor expansion of the loss E, as follows:\nE(θ −h∇θE) = E(θ) −h∇θET∇θE + h2\n2 ∇θET∇2\nθE∇θE + O(h3)\n(A.170)\nsolving for the optimal h in the above leads to:\nh =\n∇θET∇θE\n∇θET∇2\nθE∇θE =\n1\nˆgT∇2\nθEˆg\n(A.171)\nWhile this learning rate contains similarities to DAL, since ˆgT∇2\nθEˆg = P\ni λi(ˆgTui)2\nand ∥∇2\nθEˆg∥= |P\ni λiˆgTui|, we also note a few significant differences:\nA.3. Experimental details\n264\n• Since DAL is derived from discretisation drift, we can use it in a variety of\nsettings. We have shown in Section 3.8 how DAL can be adapted to be used\nwith momentum, where the main idea is to increase the contribution of local\ngradients to the momentum moving average when the drift is large, and decrease\nit otherwise.\n• While we focused on using one learning rate for all parameters in the main\nthesis, and thus use the norm operator in DAL, this is not necessary. Instead\nwe can use the per-parameter drift to construct per-parameter adaptive learning\nrates. We show preliminary results in Figure 3.30.\n• The aim of DAL is to control the discretisation drift of gradient descent and\nshow that this can affect both the stability and performance of gradient descent,\nthrough DAL-p. Beyond providing a training tool, DAL-p is another tool to\nuse to understand gradient descent. Importantly, the ability to control the\ndrift also enhances the generalisation capabilities of the method.\nA.3\nExperimental details\nEstimating continuous-time trajectories of flows. To estimate the continuous-\ntime trajectories we use Euler integration with a small learning rate 5 × 10−5. We\nreached this learning rate through experimentation: further decreasing it did not\nchange the obtained results. We note that this approach can be computationally\nexpensive: to estimate the trajectory of the NGF of time corresponding to one\ngradient descent step with learning rate 10−2, we need to do 5000 gradient steps. It\nis common in the literature to use Runge–Kutta4 to approximate continuous-time\nflows, but we noticed that approximating a flow for time h using Runge–Kutta4\nwith learning rate h still introduced significant drift: if the learning rate was further\nreduced and multiple steps were taken the results were significantly different. Thus\nRunge–Kutta4 also needs multiple steps to estimate continuous trajectories. Given\nthat one Runge–Kutta4 update requires computing 4 gradients, we found that using\nEuler integration with small learning rates is both sufficient and more efficient in\npractice.\nA.3. Experimental details\n265\nDatasets. When training neural networks with primarily used three standard\ndatasets: MNIST [320], CIFAR-10 [321] and Imagenet [322]. On the small NN\nexample in Section 3.3, we used a dataset of 5 examples, where the input is 2D\nsampled from a Gaussian distribution and the regression targets are also sample\nrandomly. We used the UCI breast cancer dataset [140] in Figure A.7.\nArchitectures. We use standard architectures: MLPs for MNIST, VGG [209] or\nResnet-18 (Version 1) [86] for CIFAR-10, Resnet-50 for Imagenet (Version 1). We\ndo not use any form of regularisation or early stopping. We use the Elu activation\nfunction [323] to ensure that the theoretical setting we discussed applies directly (we\nthus avoid discontinuities caused by ReLus [301]). We note that in the CIFAR-10\nexperiments, we did not adapt the Resnet-18 architecture to the dataset; doing so\nwill likely increase performance.\nLosses. Unless otherwise specified we used a cross entropy loss.\nComputing eigenvalues and eigenvectors. We use the Lanczos algorithm to\ncompute λi and ui.\nSeeds. All test accuracies are shown averaged from 3 seeds. For training curves, we\ncompare models across individual training runs to be able to observe the behaviour\nof gradient descent. We did not observe variability across seeds with any of the\nbehaviours reported in this thesis.\nDAL. When using DAL we set a maximum learning rate of 5 to avoid any potential\ninstabilities. We did not experiment with other values.\nComputing ∥∇2\nθE∇θE∥. We use Hessian vector products to compute ∥∇2\nθE∇θE∥.\nWe experimented with using the approximation ∇2\nθE∇θE =\n1\n2∇θ ∥∇θE∥2 ≈\n∇θE(θ+ϵ∇θE)−∇θE(θ)\nϵ\nwith ϵ = 0.01/ ∥∇θE∥as suggested by Geiping et al. [151]\nand saw no decrease in performance in the full-batch setting when using it in DAL,\nbut a slight decrease in performance when using it in stochastic gradient descent. We\nshow experimental results in Figures A.2, A.3, A.4 and A.5. More experimentation is\nneeded to see how to best leverage this approximation, either by trying other values\nof ϵ or using larger batches to approximate ∥∇2\nθE∇θE∥. We note however that all\nthe conclusions we observed in the thesis regarding the relative performance of p\nA.3. Experimental details\n266\nvalues of DAL still hold, with and without the approximation and that indeed we\nsee less of a difference when lower values of p are used.\n0\n1\n2\nTrain loss\nDAL-1.0\nExact\nApproximation\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nCIFAR-10 VGG Full Batch\n(a) DAL-1\n0\n2\n4\nTrain loss\nDAL-0.5\nExact\nApproximation\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nCIFAR-10 VGG Full Batch\n(b) DAL-0.5\n0\n1\n2\n3\n4\nTrain loss\nDAL-0.25\nExact\nApproximation\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nCIFAR-10 VGG Full Batch\n(c) DAL-0.25\nFigure A.2: CIFAR-10 DAL results with a Hessian vector product computation of\n∇2\nθE∇θE compared to an approximation.\nIn this case, we observe no\ndifference in performance between the two approaches.\n0\n1\n2\nTrain loss\nDAL-1.0\nExact\nApproximation\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nCIFAR-10 VGG Batch size 512\n(a) DAL-1\n0\n2\n4\nTrain loss\nDAL-0.5\nExact\nApproximation\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nCIFAR-10 VGG Batch size 512\n(b) DAL-0.5\n0\n2\n4\nTrain loss\nDAL-0.25\nExact\nApproximation\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nCIFAR-10 VGG Batch size 512\n(c) DAL-0.25\nFigure A.3: CIFAR-10 DAL results with a Hessian vector product computation of\n∇2\nθE∇θE compared to an approximation. For DAL-1, using the approxima-\ntion leads to a decrease in test accuracy, but the same is not observed for\nDAL-0.5 and DAL-0.25.\nA.4. Additional experimental results\n267\n0\n2\n4\n6\n8\nTrain loss\nDAL\nExact\nApproximation\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 1024\n(a) Batch size 1024.\n0\n2\n4\n6\nTrain loss\nDAL\nExact\nApproximation\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 2048\n(b) Batch size 2048.\n0\n2\n4\n6\nTrain loss\nDAL\nExact\nApproximation\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 4096\n(c) Batch size 4096.\nFigure A.4: Imagenet DAL results with a Hessian vector product computation of\n∇2\nθE∇θE compared to an approximation.\nUsing the approximation re-\nsults into a significant decrease in performance; we suspect that this can be\nresolved by using another value for ϵ.\n0\n2\n4\n6\n8\nTrain loss\nDAL-0.5\nExact\nApproximation\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 1024\n(a) Batch size 1024.\n0\n2\n4\n6\nTrain loss\nDAL-0.5\nExact\nApproximation\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 2048\n(b) Batch size 2048.\n0\n2\n4\n6\nTrain loss\nDAL-0.5\nExact\nApproximation\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 4096\n(c) Batch size 4096.\nFigure A.5: Imagenet DAL−0.5 results with a Hessian vector product computation of\n∇2\nθE∇θE compared to an approximation. Using the approximation results\ninto a significant decrease in performance; we suspect that this can be\nresolved by using another value for ϵ.\nA.4\nAdditional experimental results\nOscillations for non linear functions. We show that in the case of non-linear\nfunctions in Figure A.6, using cos. Here too, the PF is better at describing the\nbehaviour of GD than the NGF and IGR flow, which stop at local minima.\nGlobal error for the UCI breast cancer dataset. We show in Figure A.7 that\nthe PF is better at tracking the behaviour of GD in the case of NNs both in the\nstability and edge of stability areas.\nAdditional edge of stability experiments. We show that one dimension is\nsufficient to cause instability in training on MNIST in Figure A.9 (as we have done in\nA.4. Additional experimental results\n268\n−1.0 −0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nθ\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nE(θ)\nMedium range learning rate\nloss\nIGR flow\nprincipal flow\ngradient descent\nstarting point\n(a) Medium sized learning rate h = 0.5.\n−1.0 −0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nθ\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nE(θ)\nLarge range learning rate\nloss\nIGR flow\nprincipal flow\ngradient descent\nstarting point\n(b) Large learning rate h = 0.85.\nFigure A.6: Results with a non quadratic function, including cos and sin. The function\nused is: E(θ) = cos(θ) + θ, if θ < 0; (θ/3)22 + 1 + θ/3) otherwise. The\nprincipal flow and the IGR flow stay on the cosine branch, but the PF is\nable to exit the local minimum. The main reason for using this function\nwas to show the escape of gradient descent into a flatter valley and to assess\nwhat the corresponding flows do.\nFigure 3.17 for CIFAR-10). We show the connection between the stability coefficient\nsc0 and areas where the loss increases in Figure A.10: this shows that when the\nloss increases, it is in an area where sc0 is also increasing. We note that to get\na full picture of the behaviour of the PF we would need to use continuous-time\ncomputation, but since that is computationally prohibitive we use the incomplete\nbut easily available discrete data. Figure A.11 zooms in the behaviour of λ0 as\ndependent of the behaviour of the corresponding flows on MNIST and CIFAR-10\nand Figure A.12 shows additional Resnet-18 results.\nDAL. We use gradient descent training with a fixed learning rate to measure the\nquantities we would like to use as a learning rate in DAL, to see if they have a\nreasonable range and show results in Figure A.13. We show sweeps across batch\nsizes Figures A.14, A.16, A.17. We show DAL-p sweeps with effective learning rates,\ntraining losses and test accuracies in Figure A.18. We show results with DAL on a\nmean square loss in Figure A.19.\nDAL learned landscapes. To investigate the landscapes learned by DAL-p, we\nuse the method of Li et al. [22] and compare against the landscapes learned using\nA.4. Additional experimental results\n269\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nGlobal trajectory norm diff with GD\n1\n0 < h < 2\n0\nIGR flow\nPrincipal flow\n(a) Stability.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n0\n50\n100\n150\n200\n250\nGlobal trajectory norm diff with GD\nh > 2\n0\nIGR flow\nPrincipal flow\n(b) Edge of stability.\nFigure A.7: Global parameter error ∥θ(nh) −θn∥for IGR ad PF flows on an MLP trained\non the UCI breast cancer dataset. We initialise both gradient descent and\nthe IGR and PF flows at a set of initial parameters, and run them for time\nstep 2h and compare the error after each time step h. The PF is better at\ntracking the trajectory of GD than the IGR flow by a significant margin.\n0\n100\n200\n300\n400\n500\ni\n0\n2\n4\n6\n8\n10\n12\ni\nIteration 200\ni >\n0.01\ni <\n0.01\n(a) Iteration 200.\n0\n100\n200\n300\n400\n500\ni\n0\n2\n4\n6\n8\n10\n12\ni\nIteration 225\ni >\n0.01\ni <\n0.01\n(b) Iteration 225.\n0\n100\n200\n300\n400\n500\ni\n0\n2\n4\n6\n8\n10\n12\ni\nIteration 250\ni >\n0.01\ni <\n0.01\n(c) Iteration 250.\nFigure A.8: The eigenspectrum obtained along the gradient descent path of a 5 layer\nsmall MLP trained on the UCI Iris dataset. The Hessian eigenvalues are\nlargely positive. The result accompanies Figure 3.13 in the main thesis.\nA.4. Additional experimental results\n270\n0\n5\n10\n15\n20\n25\nPhysical time\n0\n2\n4\n6\n8\n10\nTraining loss\nMNIST\n(a) Training loss.\n0\n5\n10\n15\n20\n25\nPhysical time\n102\n104\n106\n108\n1010\n0\nMNIST\nGD (LR 0.05)\nNGF with \nETu0 sign swap\n(b) λ0.\nFigure A.9: One eigendirection is sufficient to lead to instabilities. To create a situa-\ntion similar to that of the PF in neural network training, we construct a\nflow given by the NGF in all eigendirections but u0; in the direction of\nu0, we change the sign of the flow’s vector field. This leads to the flow\n˙θ =\n\u0000∇θET u0\n\u0001\nu0 + PD−1\ni=1 −\n\u0000∇θET ui\n\u0001\nui. We show this flow can be very\nunstable when initialised in an edge of stability area, with increases in loss (a)\nand λ0 (b). MNIST results when changing one direction in continuous-time:\none eigendirection is sufficient to cause instability. The model was trained\nwith learning rate 0.05 until the edge of stability area is reached after which\nthe flow ˙θ =\n\u0000∇θET u0\n\u0001\nu0 + PD−1\ni=1 −\n\u0000∇θET ui\n\u0001\nui is used.\nSGD. We have shown results with batch size 64 on CIFAR-10 in Figure 3.28 in the\nmain text. Here present additional results in Figures A.20, A.22. Across datasets\nand batch sizes, we consistently observe that DAL-p learns flatter landscapes. We\nalso consistently observe that during training, λ0 is smaller with DAL-p than with\nSGD, as shown in Figure A.21.\nMomentum. We show additional results with learning rate adaptation and momen-\ntum in Figure A.24.\nGlobal discretisation error. We show the global error in trajectory between the\nNGF and gradient descent in Figure A.25. As previously observed [52], gradient\ndescent follows the NGF early in training, and the eigenvalues of the trajectory\nfollowing the NGF keeps growing.\nA.4. Additional experimental results\n271\n0.02\n0.04\n0.06\n0.08\n0.10\nLearning rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCommon peak areas\n(a) Proportion of loss function local peaks\nthat are also stability coefficient local peaks.\n20\n10\n0\n10\n20\nPeak delta (in iterations)\n0\n10\n20\n30\n40\n50\nLearning rate\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n(b) Delta in peak areas of the loss and the stability\ncoefficient counted in iterations.\nFigure A.10: Peak areas of loss increase and the stability coefficient for the largest\neigendirection of the PF. We find peaks in our training signals via\nscipy. signal .find peaks. Here, we normalise the coefficient by the gra-\ndient norm to remove the gradient norm as a potential confounder. Results\non CIFAR-10. (a) shows that most loss function peaks are 5 iterations away\nfrom a local stability coefficient peak. (b) shows the delta in iterations\nbetween a loss function peak and the closes stability coefficient peak.\n0\n50\n100\n150\n0\nE\nE\nGD\n0\n25\n50\n75\n100\n125\n150\n175\n200\nIteration\n0.2\n0.1\n0.0\n0.1\nRe[sc0]\n(a) MNIST MLP.\n0\n50\n100\n150\n200\n250\n0\nE\nE\nGD\n600\n625\n650\n675\n700\n725\n750\n775\n800\nIteration\n0.05\n0.00\n0.05\n0.10\n0.15\nRe[sc0]\n(b) CIFAR-10 VGG.\nFigure A.11: Understanding changes to λ0 using the PF: increases in λ0 above 2/h\ncorrespond to increases in the flow ˙θ = ∇θE. The stability coefficient\nreflects sc0 the changes in λ0.\nA.4. Additional experimental results\n272\n200\n400\n0\nE\nE\nGD\n2/h\n0\n1\n2\nLoss\n200\n400\n600\n800\n1000\nIteration\n4\n2\n0\nRe[sc0]\nCIFAR-10, Resnet-18\nFigure A.12: Learning rate: 0.01. Edge of stability results: the connection between the\nPF, stability coefficients and loss instabilities with Resnet-18. Together\nwith the behaviour of gradient descent, we plot the behaviour of the NGF\nand positive gradient flow initialized at θt and simulated for time h for\neach iteration t. The analysis we performed based on the PF suggests\nthat when ℜ[sc0] > 0 and large we should expected gradient descent to\nexhibit behaviours close to those of the positive gradient flow. What we\nobserve empirically is that increases in loss value of gradient descent are\nproportional to the increase of the positive gradient flow in that area (can\nbe seen best between iterations 600 and 800); the same behaviour can be\nseen in relation to the eigenvalue λ0. Results for a larger learning rate are\nshown in Figure 3.16.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n10\n2\n10\n1\n100\n101\n102\n103\n2/||\n2Eg( )||\nVGG Full batch\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.5\n(a) VGG full batch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n10\n2\n10\n1\n100\n101\n102\n2/||\n2Eg( )||\nResnet-18 Full batch\n(b) Resnet-18 full batch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIteration\n1e3\n10\n2\n10\n1\n100\n101\n102\n2/||\n2Eg( )||\nVGG Batch Size 2048\n(c) VGG batch size 64.\nFigure A.13: Evaluating the value of the DAL learning rate for a fixed learning rate\nsweep to assess whether it would have reasonable magnitudes. We observe\nthat the quantity is mainly the ranges 10−2 to 10, which can be used for\nmodel training. We note that this is a preliminary check only, as using\nDAL in training changes the optimisation trajectory and thus can affect\nthese ranges. As we have seen however, DAL can be suitably used for\nmodel training.\nA.4. Additional experimental results\n273\n0\n2\n4\n6\n8\n10\n12\nTrain loss\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n0\n2000\n4000\n6000\n8000\n10000\nIteration\n10\n4\n10\n2\n100\nLearning rate\nVGG Batch size 64\n(a) Batch size 64.\n0\n2\n4\n6\n8\nTrain loss\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\nVGG Batch size 256\n(b) Batch size 256.\n0\n1\n2\n3\n4\n5\n6\n7\nTrain loss\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\nVGG Batch size 1024\n(c) Batch size 1024.\n0\n1\n2\n3\n4\n5\nTrain loss\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\nVGG Full batch CIFAR-10\n(d) Full batch.\nFigure A.14: DAL-p training a VGG model on CIFAR-10. Sweep across batch sizes:\ndiscretisation drift helps test performance, but at the cost of stability. We\nalso show the effective learning rate and train losses and test accuracies.\nA.4. Additional experimental results\n274\n0\n200\n400\n600\n800\n1000\nIteration\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTrain loss\nResnet-18 Full batch CIFAR-10\nLearning rate\n0.005\n0.01\n0.05\n0.1\n0.5\nDAL\n(a) Training loss.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nResnet-18 Full batch CIFAR-10\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n(b) Test accuracy.\nFigure A.15: DAL and DAL−p results with Resnet-18 on CIFAR-10: DAL results in\nimproved stability without requiring a hyperparameter sweep, while DAL-p\nachieves high generalisation.\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n2\n4\n6\n8\nTrain loss\nResnet-50, Batch size 1024, Imagenet\nLearning rate\n0.05\n0.1\n0.5\nDAL\n(a) Batch size 1024.\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n2\n4\n6\nTrain loss\nResnet-50, Batch size 2048, Imagenet\nLearning rate\n0.05\n0.1\n0.5\nDAL\n(b) Batch size 2048.\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n2\n4\n6\nTrain loss\nResnet-50, Batch size 4096, Imagenet\nLearning rate\n0.05\n0.1\n0.5\nDAL\n(c) Batch size 4096.\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0\n2\n4\n6\nTrain loss\nResnet-50, Batch size 8192, Imagenet\nLearning rate\n0.05\n0.1\n0.5\nDAL\n(d) Batch size 8192.\nFigure A.16: DAL: Imagenet results across batch sizes. We observe quick convergence\nand stable training.\nA.4. Additional experimental results\n275\n0\n2\n4\n6\n8\nTrain loss\nLearning rate\n0.05\n0.1\n0.5\nDAL-0.5\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 1024\n(a) Batch size 1024.\n0\n2\n4\n6\nTrain loss\nLearning rate\n0.05\n0.1\n0.5\nDAL-0.5\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 2048\n(b) Batch size 2048.\n0\n2\n4\n6\nTrain loss\nLearning rate\n0.05\n0.1\n0.5\nDAL-0.5\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 4096\n(c) Batch size 4096.\n0\n2\n4\n6\nTrain loss\nLearning rate\n0.05\n0.1\n0.5\nDAL-0.5\n0\n10000\n20000\n30000\n40000\n50000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\nImagenet Resnet-50 Batch Size 8192\n(d) Batch size 8192.\nFigure A.17: DAL−0.5: Imagenet results across batch sizes. We observe quick training\nand good generalisation performance.\nA.4. Additional experimental results\n276\n0\n2\n4\n6\n8\nTrain loss\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\nResnet-18 Full batch CIFAR-10\n(a) Resnet-18 on CIFAR-10.\n0\n2\n4\n6\nTrain loss\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n0\n5000 10000 15000 20000 25000 30000 35000 40000\nIteration\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\nResnet-50 Batch size 8196 Imagenet\n(b) Resnet-50 on Imagenet.\nFigure A.18: DAL-p sweep: discretisation drift helps test performance, but at the cost\nof stability. We also show the effective learning rate and train losses and\ntest accuracies.\n0.0\n0.5\n1.0\n1.5\nIteration\n1e4\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nTraining loss\nVGG, CIFAR-10\nLearning rate\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nDAL-0.5\nDAL-1\n(a) Training loss.\n0.0\n0.5\n1.0\n1.5\nIteration\n1e4\n0\n10\n20\n30\n40\n50\n60\n70\n0\nVGG, CIFAR-10\n(b) λ0.\nFigure A.19: Results with a least square loss. DAL and DAL-0.5 lead to quicker training\nand lower λ0 in this setting as well.\nA.4. Additional experimental results\n277\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n0\n50\n100\n150\n200\nDAL-0.125\n0\n50\n100\n150\n200\n(a) DAL\n1.0\n0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n0\n50\n100\n150\n200\nSGD with LR 0.5\n0\n50\n100\n150\n200\n(b) SGD\n0.4\n0.2\n0.0\n0.2\n0.4\n0.4\n0.2\n0.0\n0.2\n0.4\n0\n5\n10\n15\n20\nDAL-0.125\n0\n5\n10\n15\n20\n(c) DAL\n0.4\n0.2\n0.0\n0.2\n0.4\n0.4\n0.2\n0.0\n0.2\n0.4\n0\n5\n10\n15\n20\nSGD with LR 0.5\n0\n5\n10\n15\n20\n(d) SGD\nFigure A.20: CIFAR-10, full batch, at various levels of zoom into around the\nconvergence points. The 2D projection of the DAL-p and SGD learned\nlandscapes on CIFAR-10, using a VGG model. The visualisation is made\nusing the method of Li et al. [22]. The DAL-p model achieves an accuracy\nof 82% which the SGD model achieves 77% accuracy. We also show the\ntrajectory of λ0 for both models in Figure A.21.\nA.4. Additional experimental results\n278\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n0\nVGG full batch\np in DAL-p\n0.125\n(a) DAL\n0\n1000\n2000\n3000\n4000\n5000\nIteration\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n0\nLearning rate\n0.5\n(b) SGD\nFigure A.21: λ0 for learned models using full batch gradient descent and DAL-p on\nCIFAR-10: DAL traverses areas of space with smaller λ0.\n0.04\n0.02\n0.00\n0.02\n0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nDAL-0.125\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n(a) DAL\n0.04\n0.02\n0.00\n0.02\n0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nSGD with LR 0.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n(b) SGD\nFigure A.22: Imagenet, batch size 8192. The 2D projection of the DAL-p and SGD\nlearned landscapes, using the method of Li et al. [22]. The DAL-p model\nachieves an accuracy of 50% which the SGD model achieves 42% accuracy.\nA.4. Additional experimental results\n279\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nMean per iteration drift (approx)\n0.64\n0.66\n0.68\n0.70\n0.72\nTest accuracy\nLearning rate\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nMean per iteration drift (approx)\n40\n60\n80\n100\n120\n140\nMean \n0 before convergence\nLearning rate\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n(a) Fixed learning rate sweep.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMean per iteration drift (approx)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nTest accuracy\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMean per iteration drift (approx)\n0\n100\n200\n300\n400\nMean \n0 before convergence\np in DAL-p\n0.125\n0.25\n0.5\n0.75\n1.0\n1.5\n2.0\n(b) DAL-p sweep.\nFigure A.23: VGG CIFAR-10 with batch size 1024: connection between drift, test error\nand λ0. We observe the same patterns for other batch sizes, namely that\nthe higher the drift the more generalisation and the lower λ0, both with\nfixed learning rates and DAL.\nA.4. Additional experimental results\n280\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nIteration\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nTraining loss\nImagenet Batch size 1024 with momentum 0.9\nLearning rate 0.05\nLearning rate 0.1\nLearning rate 0.5\nDrift scaling power 0.5\nDrift scaling power 1.0\n(a) Training loss.\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest (Top 1) accuracy\nImagenet Batch size 1024 with momentum 0.9\n(b) Test accuracy.\nFigure A.24: Integrating information on drift with momentum: with smaller batch sizes,\nwe observe a smaller empirical gain. Since these are preliminary results,\nmore investigation is needed into why that is. These results are consistent\nwith the observation obtained with vanilla gradient descent, where we say\nlarger gains with larger batch sizes on Imagenet.\n0\n100\n200\n300\n400\n500\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\nGD-Grad Flow global diff\n(a) ∥θ(nh) −θn∥\n0\n100\n200\n300\n400\n500\nIteration\n0\n20\n40\n60\n80\nEigen at flow params\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n(b) Leading eigenvalues λi of the Hessian.\nFigure A.25: The global error in parameter space ∥θ(nh) −θn∥between the NGF and\ngradient descent (a), as well as the behaviour of λ0 of the NGF (b). We\nobserve that early in training gradient descent follows the NGF, and then\nwhen following the NGF the leading eigenvalues grow.\nA.5. Figure reproduction details\n281\nA.5\nFigure reproduction details\n• Figure 3.2: 2D convex plot.\nThe loss function is E = θTAθ with A =\n((1, 0.0), (0.0, 0.01)). The learning rate is 0.9.\n• Figure 3.3: 1D example. E = 1\n2(θ −0.6)2 and h = 1.2\n• Figure 3.6: E(z) = 1\n2z2. Learning rates are 0.8, 1.5 and 2.1.\n• Figure 3.7:\nQuadratic losses in 2 dimensions.\nE = θTAθ with A =\n((1, 0.0), (0.0, 0.01)), with learning rates 0.5, 0.9 and 1.05.\n• Figure 3.8: Banana function. Learning rates 0.0006, 0.0017 0.005.\n• Figure 3.9: Error between gradient descent parameters and parameters ob-\ntained following continuous-time flows for multiple iterations on a small MLP:\n∥θt+n −θ(nh)∥. Input size 2, MLP of with output units 10, 1. Learning rates\n0.1, 0.2 and 0.25. We use a dataset with 5 examples where the input is gen-\nerated from a Gaussian distribution with 2 dimensions and the targets are\nsamples from a Gaussian distribution with 1 dimension. A mean square error\nloss is used.\n• Figure 3.10: Predictions of ∇θETu0 using the PF on neural networks. Full-\nbatch training with a VGG network on CIFAR-10. Learning rates 0.01, 0.03\nand 0.05.\n• Figure 3.12: Edge of stability on VGG networks on CIFAR-10. Full-batch\ntraining.\n• Figure 3.13: Edge of stability on a small 5 layer MLP with 10 units per layer\non the UCI Iris dataset. Full-batch training. The learning rate used is 0.18.\n• Figure 3.14: Edge of stability results. MNIST MLP with 4 layers. Learning\nrate 0.05. For each model we approximate the NGF and the positive gradient\nflow initialised at the current gradient descent parameters and measure the\nvalue of the loss function and λ0.\nA.5. Figure reproduction details\n282\n• Figure 3.15: Using the PF and stability coefficient to understand instabilities\nin deep learning. MNIST results use a 4 layer MLP and a 0.05 learning rate.\nCIFAR-10 MLP use a VGG network and a 0.02 learning rate.\n• Figure 3.16: Understanding the behaviour of the loss through the PF and the\nbehaviour of λ0. Using a Resnet-18 trained on CIFAR-10 using a learning rate\nof 0.03. For each model we approximate the NGF and the positive gradient\nflow initialised at the current gradient descent parameters and measure the\nvalue of the loss function and λ0.\n• Figure 3.17: Assessing whether 1 dimension is enough to cause instability\nin a continuous-time flow. We train a model with gradient descent until it\nreaches the edge of stability (λ0 ≈2/h), after which we use a approximate\nthe continuous flow ˙θ = ∇θETu0u0 + PD−1\ni=1 −∇θETuiui. The model is a\nVGG model trained on CIFAR-10 with learning rate 0.05 and after the edge of\nstability is reached the flow is approximated using Euler steps of size 5 × 10−5.\n• Figure 3.19: The unstable dynamics of ∇θETu in the edge of stability area.\nThe model results are obtained from a VGG model trained on CIFAR-10 with\na learning rate of 0.01.\n• Figure 3.20: Learning rate sweep showing the value of the non-principal third-\norder term in training, on a Resnet-18 model trained on CIFAR-10. The\ncoefficients used for the non-principal term are those in Eq (3.4).\n• Figure 3.21: ||∇2\nθE∇θE|| and the per-iteration drift as measured during train-\ning. The plots are obtained on an MNIST MLP with 4 layers and 100 units\nper layer, and a VGG model trained on CIFAR-10. The learning rates used\nare 0.05, 0.01 and 0.01 respectively.\n• Figure 3.22: Correlation between ||∇2\nθE∇θE|| and the iteration drift. The\nplots are obtained on an MNIST MLP with 4 layers and 100 units per layer,\nand a VGG model trained on CIFAR-10. Results are obtained over a learning\nrate sweep of 10 learning rates 0.01, 0.02, . . . , 0.1.\nA.5. Figure reproduction details\n283\n• Figure 3.23: Models trained using a learning rate sweep or DAL on CIFAR-10\nand Imagenet. Models are VGG, Resnet-18 and Resnet-50 respectively.\n• Figure 3.24: Key quantities in DAL versus fixed learning rate training: learning\nrate, and update norms. Results obtained on a Resnet-18 model trained on\nCIFAR-10.\n• Figure 3.25: DAL-0.5 results on a CIFAR-10 and Imagenet using VGG, Resnet-\n18 and Resnet-50 model respectively.\n• Figure 3.26: DAL-p sweep on full-batch training on CIFAR-10 with VGG and\nResnet-18 models and a Resnet-50 model trained on Imagenet.\n• Figure 3.27: VGG model trained on CIFAR-10 using full-batch gradient descent,\neither with a fixed learning rate in a sweep or a DAL-p sweep. The drift\nper-iteration is approximated using h2/2∇θE2∇θE or h(θ)2/2∇θE2∇θE (the\nadaptive learning rate) in case of DAL-p.\n• Figure 3.29: Resnet-50 results with 0.9 momentum and a learning rate sweep\nor DAL. Models trained on Imagenet.\n• Figure A.2: CIFAR-10 DAL results with a Hessian vector product computa-\ntion of ∇2\nθE∇θE compared to an approximation. We used ϵ = 0.01 in the\napproximation. Full-batch training.\n• Figure A.3: CIFAR-10 DAL results with a Hessian vector product computa-\ntion of ∇2\nθE∇θE compared to an approximation. We used ϵ = 0.01 in the\napproximation. Batch size 512.\n• Figure A.4: Imagenet DAL results with a Hessian vector product computa-\ntion of ∇2\nθE∇θE compared to an approximation. We used ϵ = 0.01 in the\napproximation.\n• Figure A.5: Imagenet DAL-0.5 results with a Hessian vector product compu-\ntation of ∇2\nθE∇θE compared to an approximation. We used ϵ = 0.01 in the\napproximation.\nA.5. Figure reproduction details\n284\n• Figure A.6: Results with a non quadratic function, including cos and sin. The\nfunction used is: E(θ) = cos(θ) + θ, if θ < 0; (θ/3)22 + 1 + θ/3) otherwise.\n• Figure 3.4: Per-iteration error between the NGF, IGR and PF and gradient\ndescent. MNIST learning rate 0.05. CIFAR learning rate 0.02. We obtain the\nresults by training a model with gradient descent θt = θt−1−h∇θE(θt−1) and at\neach iteration t of gradient descent approximating the respective flows for time\nh and computing the difference in norms between the resulting flow parameters\nand the gradient descent parameters at the next iteration: ∥θt −θ(h)∥with\nθ(h) = θt−1. Results obtained on full-batch training.\n• Figure A.7: Global parameter error for IGR ad PF flows compared on an MLP\ntrained on the UCI breast cancer dataset. MLP with 10 hidden units and 1\noutput unit. Learning rates 1.5 and 5. respectively.\n• Figure 3.11: We train a VGG model on CIFAR-10 with learning rate 0.1, and\nas it reaches converges increase the learning rate slightly so that λ0 > 2/h. We\nuse a square loss here to avoid training the decrease in eigenvalues later in\ntraining associated with cross entropy losses.\n• Figure A.8: The eigenspectrum obtained along the gradient descent path of a\n5 layer small MLP trained on the UCI Iris dataset. The Hessian eigenvalues\nare largely positive. The result accompanies Figure 3.13 in the main thesis.\n• Figure 3.18: Training a VGG model on CIFAR-10 and training with learning\nrate 0.05 after which switching to 0.01.\n• Figure A.9: MNIST results when changing one direction in continuous-time.\nThe model is a 4 layer MLP with 100 hidden units per layer. The model was\ntrained with learning rate 0.05 until the edge of stability area is reached after\nwhich the flow ˙θ = ∇θETu0u0 + PD−1\ni=1 −∇θETuiui is used.\n• Figure A.10: Peak areas of loss increase and the stability coefficient for the\nlargest eigendirection of the PF. Results using a VGG model trained with\nfull-batch gradient descent on CIFAR-10 across a learning rate sweep.\nA.5. Figure reproduction details\n285\n• Figure A.11: Understanding changes to λ0 using the PF. MLP with 4 layers\nwith 100 units each trained on MNIST, and VGG model trained on CIFAR-10.\nLearning rate 0.05 and 0.01 respectively.\n• Figure A.12: The connection between the PF, λ0 and loss instabilities with\nResnet-18 trained on CIFAR-10 with a learning rate 0.01.\n• Figure A.13: Evaluating the value of the DAL learning rate for a vanilla (fixed)\nlearning rate sweep to assess whether it would have reasonable magnitudes.\nModels trained on CIFAR-10 using a learning rate sweep.\n• Figure A.14: DAL-p sweep with a VGG model trained on CIFAR-10 for small\nand large batch sizes.\n• Figure A.16: DAL on Imagenet, a learning rate sweep across batch sizes.\n• Figure A.17: DAL−0.5: on Imagenet, a learning rate sweep across batch sizes.\n• Figure A.18: DAL-p sweep on VGG, Resnet-18 and Imagenet.\n• Figure A.19: DAL results with a least square loss. Full-batch training of a\nVGG model on CIFAR-10.\n• Figure 3.30: Resnet-50 Imagenet training with a learning rate per parameter.\nBatch size 8192.\n• Figure A.23: VGG trained on CIFAR-10 with batch size 1024: connection\nbetween drift, test error and λ0.\n• Figure A.24: DAL-p with momentum 0.9 on Imagenet. The model is Resnet-50\ntrained with batch size 1024.\n• Figure A.25: MNIST results with a 4 layer MLP with 100 hidden units. The\nlearning rate used is 0.05.\nAppendix B\nContinuous time models of gradient\ndescent in two-player games\nB.1\nProof of the main theorems\nNotation: We use ϕ ∈Rm to denote the parameters of the first player and θ ∈Rn for\nthe second player. We assume that the players parameters are updated simultaneously\nwith learning rates υϕh and υθh respectively. We consider the vector fields f(ϕ, θ) :\nRm ×Rn →Rm and g(ϕ, θ) : Rm ×Rn →Rn. The Jacobian df\ndθ(ϕ, θ) ∈Rn,m denotes\nthe matrix with entries\n\u0002 df\ndθ(ϕ, θ)\n\u0003\ni,j = dfj\ndθi with i ∈{1, ..., n}, j ∈{1, ..., m}.\nWe now prove Theorem 4.2.1 and Theorem 4.2.2 in the main thesis, corresponding\nto the simultaneous Euler updates and to the alternating Euler updates, respectively.\nIn both cases, our goal is to find corrections f1 and g1 to the original system\n˙ϕ = f(ϕ, θ),\n(B.1)\n˙θ = g(ϕ, θ),\n(B.2)\nsuch that the modified continuous system\n˙ϕ = f(ϕ, θ) + hf1(ϕ, θ),\n(B.3)\n˙θ = g(ϕ, θ) + hg1(ϕ, θ),\n(B.4)\nfollows the discrete steps of the method with a local error of order O(h3). More\nB.1. Proof of the main theorems\n287\nprecisely, if (ϕt, θt) denotes the discrete step of the method at time t, and (ϕ(h), θ(h))\ncorresponds to the continuous solution of the modified system above starting at\n(ϕt−1, θt−1), we want that the local errors for both players, i.e.,\n∥ϕt −ϕ(υϕh)∥\nand\n∥θt −θ(υθh)∥\nto be of order O(h3). In the expression above, υϕh and υθh are the effective learning\nrates (or step-sizes) for the first and the second player respectively for both the\nsimultaneous and alternating Euler updates.\nOur proofs from BEA follow the same steps (see Section 5.1.1 for an overview):\n1. Expand the discrete updates to find a relationship between ϕt and ϕt−1 and\nθt and θt−1 up to order O(h2).\n2. Expand the changes in continuous-time of the modified flow given by BEA.\n3. Find the first order Discretisation Drift (DD) by matching the discrete and\ncontinuous updates up to second order in learning rates.\nNotation: To avoid cluttered notations, we use f(t) to denote the f(ϕt, θt) and\ng(t) to denote g(ϕt, θt) for all t. If no index is specified, we use f to denote f(ϕ, θ),\nwhere ϕ and θ are variables in a continuous system.\nB.1.1\nSimultaneous updates (Theorem 4.2.1)\nHere we prove Theorem 4.2.1 in the main thesis, which we reproduce here:\nThe simultaneous Euler updates with learning rates υϕh and υθh respectively\nare given by:\nϕt = ϕt−1 + υϕhf(ϕt−1, θt−1)\n(B.5)\nθt = θt−1 + υθhg(ϕt−1, θt−1)\n(B.6)\nTheorem (DD for simultaneous Euler updates) (Theorem 4.2.1) The dis-\ncrete simultaneous Euler updates in (B.5) and (B.6) follow the continuous system\nB.1. Proof of the main theorems\n288\n˙ϕ = f −υϕh\n2\n\u0012 df\ndϕf + df\ndθg\n\u0013\n(B.7)\n˙θ = g −υθh\n2\n\u0012 dg\ndϕf + dg\ndθg\n\u0013\n(B.8)\nwith an error of size O(h3) after one update step.\nStep 1: Expand the updates per player via a Taylor expansion.\nWe expand the numerical scheme to find a relationship between ϕt and ϕt−1 and\nθt and θt−1 up to order O(h2). In the case of the simultaneous Euler updates, this\ndoes not require any change to Eqs (B.5) and (B.6).\nStep 2: Expand the continuous-time changes for the modified flow\ngiven by BEA.\nLemma B.1.1 If:\n˙ϕ = ˜f(ϕ, θ)\n(B.9)\n˙θ = ˜g(ϕ, θ)\n(B.10)\nwhere\n˜f(ϕ, θ) = f(ϕ, θ) +\nX\ni=1\nτϕ\nifi(ϕ, θ)\n(B.11)\n˜g(ϕ, θ) = g(ϕ, θ) +\nX\ni=1\nτθ\nigi(ϕ, θ)\n(B.12)\nand τθ and τϕ are scalars. Then—for ease of notation, we drop the argument τ on\nthe evaluations of ϕ and θ on the RHS:\nϕ(τ + τϕ) = ϕ(τ) + τϕf + τ 2\nϕf1 + τ 2\nϕ\n1\n2\ndf\ndϕf + τ 2\nϕ\n1\n2\ndf\ndθg + O(τ 3\nϕ)\n(B.13)\nB.1. Proof of the main theorems\n289\nProof: We perform a Taylor expansion:\nϕ(τ + τϕ)\n(B.14)\n= ϕ + τϕ ˙ϕ + τ 2\nϕ\n1\n2\n¨ϕ + O(τ 3\nϕ)\n(B.15)\n= ϕ + τϕ ˜f + τ 2\nϕ\n1\n2\n˙˜f + O(τ 3\nϕ)\n(B.16)\n= ϕ + τϕ ˜f + τ 2\nϕ\n1\n2\n \nd ˜f\ndϕ\n˜f + d ˜f\ndθ ˜g\n!\n+ O(τ 3\nϕ)\n(B.17)\n= ϕ + τϕ(f + τϕf1 + O(τ 2\nϕ)) + τ 2\nϕ\n1\n2\n \nd ˜f\ndϕ\n˜f + d ˜f\ndθ ˜g\n!\n+ O(τ 3\nϕ)\n(B.18)\n= ϕ + τϕf + τ 2\nϕf1 + τ 2\nϕ\n1\n2\n \nd ˜f\ndϕ\n˜f + d ˜f\ndθ ˜g\n!\n+ O(τ 3\nϕ)\n(B.19)\n= ϕ + τϕf + τ 2\nϕf1\n(B.20)\n+ τ 2\nϕ\n1\n2\n \nd ˜f\ndϕ\n\u0000f + τϕf1 + O(τ 2\nϕ)\n\u0001\n+ d ˜f\ndθ\n\u0000g + τθg1 + O(τ 2\nθ )\n\u0001\n!\n+ O(τ 3\nϕ)\n(B.21)\n= ϕ + τϕf + τ 2\nϕf1 + τ 2\nϕ\n1\n2\nd ˜f\ndϕf + τ 2\nϕ\n1\n2\nd ˜f\ndθg + O(τ 3\nϕ)\n(B.22)\n= ϕ + τϕf + τ 2\nϕf1 + τ 2\nϕ\n1\n2\ndf\ndϕf + τ 2\nϕ\n1\n2\ndf\ndθg + O(τ 3\nϕ)\n(B.23)\nwhere we assumed that τϕ and τθ are in the same order of magnitude.\n□\nStep 3: Matching the discrete and modified continuous updates.\nWe substitute the values given by the discrete updates, namely ϕt−1 and θt−1, in\nLemma B.1.1 in order to calculate the displacement according to the continuous\nupdates:\nϕ(τ + τϕ) = ϕt−1 + τϕf(t−1) + τ 2\nϕf1(ϕt−1, θt−1)\n(B.24)\n+ 1\n2τ 2\nϕ\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2τ 2\nϕ\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + O(τ 3\nϕ)\n(B.25)\nθ(τ + τθ) = θt−1 + τθg(t−1) + τ 2\nθ g1(ϕt−1, θt−1)\n(B.26)\n+ 1\n2τ 2\nθ\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2τ 2\nθ\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + O(τ 3\nθ )\n(B.27)\nIn order to find f1 and g1 such that the continuous dynamics of the modified updates\nB.1. Proof of the main theorems\n290\nf + υϕhf1 and g + υθhg1 match the discrete updates in Eqs (B.5) and (B.6), we look\nfor the corresponding continuous increments of the discrete updates in the modified\ncontinuous system, such that ∥ϕ(τ + τϕ) −ϕt∥and ∥θ(τ + τθ) −θt∥are O(h3). The\nfirst order terms in Eqs (B.5) and (B.6) and Eqs (B.26) and (B.27) suggest that:\nυϕh = τϕ\n(B.28)\nυθh = τθ\n(B.29)\nWe can now proceed to find f1 and g1 from the second order terms\n0 = υϕ\n2h2f1(ϕt−1, θt−1) + 1\n2υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2υϕ\n2h2 df\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.30)\nf1(ϕt−1, θt−1) = −1\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1).\n(B.31)\nSimilarly, for g1 we obtain\ng1(ϕt−1, θt−1) = −1\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1).\n(B.32)\nLeading to the first order corrections\nf1(ϕt−1, θt−1) = −1\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.33)\ng1(ϕt−1, θt−1) = −1\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1).\n(B.34)\nWe have found the functions f1 and g1 such that after one discrete optimisation step\nthe flows ˙ϕ = f + υϕhf1 and ˙θ = g + υθhg1 follow the discrete updates up to order\nO(h3), finishing the proof.\nB.1.2\nAlternating updates (Theorem 4.2.2)\nHere we prove Theorem 4.2.2 in the main thesis, which we reproduce here. For the\nalternating Euler updates, the players take turns to update their parameters, and can\nperform multiple updates each. We denote the number of alternating updates of the\nfirst player (resp. second player) by m (resp. k). We scale the learning rates by the\nB.1. Proof of the main theorems\n291\nnumber of updates, leading to the following updates ϕt := ϕm,t and θt := θk,t where\nϕi,t = ϕi−1,t + υϕh\nm f(ϕi−1,t, θt−1),\ni = 1 . . . m,\n(B.35)\nθj,t = θj−1,t + υθh\nk g(ϕm,t, θj−1,t),\nj = 1 . . . k.\n(B.36)\nTheorem DD for alternating Euler updates (Theorem 4.2.2) The discrete al-\nternating Euler updates in (B.35) and (B.36) follow the continuous system\n˙ϕ = f −υϕh\n2\n\u0012 1\nm\ndf\ndϕf + df\ndθg\n\u0013\n(B.37)\n˙θ = g −υθh\n2\n\u0012\n(1 −2υϕ\nυθ\n) dg\ndϕf + 1\nk\ndg\ndθg\n\u0013\n(B.38)\nwith an error of size O(h3) after one update step.\nStep 1: Discrete updates\nIn the case of alternating Euler discrete updates, we have\nϕ1,t = ϕt−1 + υϕ\nm hf(ϕt−1, θt−1)\n(B.39)\n. . .\n(B.40)\nϕm,t = ϕm−1,t + υϕ\nm hf(ϕm−1,t, θt−1)\n(B.41)\nθ1,t = θt−1 + υθ\nk hg(ϕm,t, θt−1)\n(B.42)\n. . .\n(B.43)\nθk,t = θk−1,t−1 + υθ\nk hg(ϕm,t, θk−1,t)\n(B.44)\nϕt = ϕm,t\n(B.45)\nθt = θk,t.\n(B.46)\nLemma B.1.2 Given updates ϕm,t = ϕm−1,t + hf(ϕm−1,t, θt−1), we can write:\nϕm,t = ϕt−1 + mhf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.47)\nB.1. Proof of the main theorems\n292\nProof: Proof by induction. Base step.\nϕ2,t = ϕ1,t + hf(ϕ1,t, θt−1)\n(B.48)\n= ϕt−1 + hf(ϕt−1, θt−1) + hf(ϕ1,t, θt−1)\n(B.49)\n= ϕt−1 + hf(t−1) + hf\n\u0000ϕt−1 + hf(t−1), θt−1\n\u0001\n(B.50)\n= ϕt−1 + hf(t−1) + h\n \nf(t−1) + h df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n!\n(B.51)\n= ϕt−1 + 2hf(t−1) + h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.52)\nInductive step:\nϕm+1,t = ϕm,t + hf(ϕm,t, θt−1)\n(B.53)\n= ϕt−1 + mhf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.54)\n+ hf\n\u0010\nϕt−1 + mhf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3), θt−1\n\u0011\n+ O(h3)\n(B.55)\n= ϕt−1 + mhf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.56)\n+ h\n \nf(t−1) + df\ndϕ\n\f\f\f\f\n(t−1)\n \nmhf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n!!\n+ O(h3)\n(B.57)\n= ϕt−1 + (m + 1)hf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.58)\n+ h df\ndϕ\n\f\f\f\f\n(t−1)\n \nmhf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1))\n!\n+ O(h3)\n(B.59)\n= ϕt−1 + (m + 1)hf(t−1) + m(m −1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.60)\n+ h2m df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.61)\n= ϕt−1 + (m + 1)hf(t−1) + m(m + 1)\n2\nh2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.62)\n□\nB.1. Proof of the main theorems\n293\nFrom Lemma B.1.2 with h = υϕh/m we have that\nϕm,t = ϕt−1 + υϕhf(t−1) + m −1\n2m υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3).\n(B.63)\nWe now turn our attention to the second player. We define gt′ = g(ϕm,t, θt−1). This\nis captures the difference between simultaneous and alternating updates. From\nLemma B.1.2 with h = υθh/k we have that\nθk,t = θt−1 + υθhgt′ + (k −1)\n2k\nυθ\n2h2 dg\ndθ\n\f\f\f\f\nt′\ngt′ + O(h3).\n(B.64)\nWe now expand gt′ by Taylor expansion:\ngt′ = g(ϕm,t, θt−1)\n(B.65)\n= g(ϕt−1 + υϕhf(t−1) + υϕ\n2m −1\n2m h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3), θt−1)\n(B.66)\n= g(t−1) + dg\ndϕ\n\f\f\f\f\n(t−1)\n \nυϕhf(t−1) + (m −1)\n2m\nυϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n!\n(B.67)\nIf we replace the above in Eq (B.64):\nθk,t = θt−1 + υθhgt′ + k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\nt′\ngt′ + O(h3)\n(B.68)\n= θt−1 + υθh\n \ng(t−1) + dg\ndϕ\n\f\f\f\f\n(t−1)\n \nυϕhf(t−1) + m −1\n2m υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n!!\n(B.69)\n+ k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\nt′\ngt′ + O(h3)\n(B.70)\n= θt−1 + υθhg(t−1) + υθh dg\ndϕ\n\f\f\f\f\n(t−1)\n \nυϕhf(t−1) + m −1\n2m υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n!\n(B.71)\n+ k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\nt′\ngt′ + O(h3)\n(B.72)\n= θt−1 + υθhg(t−1) + υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\nt′\ngt′ + O(h3)\n(B.73)\nB.1. Proof of the main theorems\n294\nθk,t = θt−1 + υθhg(t−1) + υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.74)\n+ k −1\n2k h2 dg\ndθ\n\f\f\f\f\nt′\n \ngt−1 + dg\ndϕ\n\f\f\f\f\n(t−1)\n \nυϕhf(t−1) + (m −1)\n2m\nυϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n!!\n(B.75)\n+ O(h3)\n(B.76)\n= θt−1 + υθhg(t−1) + υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\nt′\ng(t−1) + O(h3) (B.77)\n= θt−1 + υθhg(t−1) + υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.78)\n+ k −1\n2k υθ\n2h2d\n\u0010\ng + dg\ndϕ\n\u0010\nυϕhf + (m−1)\n2m υϕ2h2 df\ndϕf\n\u0011\u0011\ndθ\n\f\f\f\f\nt−1\ng(t−1) + O(h3)\n(using Eq (B.67), B.79)\n= θt−1 + υθhg(t−1) + υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + O(h3)\n(B.80)\nWe then have:\nϕm,t = ϕt−1 + υϕhf(t−1) + m −1\n2m υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.81)\nθk,t = θt−1 + υθhg(t−1) + υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + O(h3)\n(B.82)\nStep 2: Expand the continuous-time changes for the modified flow\ngiven by BEA (Identical to the simultaneous update case.)\nStep 3: Matching the discrete and modified continuous updates.\nThe linear terms are identical to those in the simultaneous updates:\nτϕ = υϕh\n(B.83)\nτθ = υθh\n(B.84)\nWe then obtain f1 from matching the quadratic terms in Eqs (B.26) and Eqs (B.81)—\nB.1. Proof of the main theorems\n295\nbelow we denote f1(ϕt−1, θt−1) by f1 and g1(ϕt−1, θt−1) by g1, for brevity:\nυϕ\n2h2f1 + 1\n2υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2υϕ\n2h2 df\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n= m −1\n2m υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.85)\nf1 + 1\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) = m −1\n2m\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.86)\nf1 =\n\u0012m −1\n2m\n−1\n2\n\u0013 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.87)\nf1 = −1\n2m\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.88)\nFor g1, from Eqs (B.27) and (B.82):\nυθ\n2h2\u0010\ng1 + 1\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n\u0011\n= υθυϕh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k υθ\n2h2 dg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.89)\ng1 + 1\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n= υϕ\nυθ\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.90)\ng1 + 1\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + 1\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n= υϕ\nυθ\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + k −1\n2k\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.91)\ng1 =\n\u0012υϕ\nυθ\n−1\n2\n\u0013 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2k\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.92)\nWe thus have that\nf1(ϕt−1, θt−1) = −1\n2m\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.93)\ng1(ϕt−1, θt−1) =\n\u0012υϕ\nυθ\n−1\n2\n\u0013 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) −1\n2k\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1).\n(B.94)\nWe found f1 and g1 such that after one optimisation step the flows ˙ϕ = f + υϕhf1\nB.2. Proof of the main corollaries\n296\nand ˙θ = g + υθhg1 follow the discrete updates up to order O(h3), finishing the proof.\nB.2\nProof of the main corollaries\nIn this section, we will write the modified equations in the case of using gradient\ndescent common-payoff games and zero-sum games. To do so, we will replace f\nand g with the values given by gradient descent in the first order corrections we\nhave derived in the previous sections. In the common-payoff case f = −∇ϕE and\ng = −∇θE, while in the zero-sum case f = ∇ϕE and g = −∇θE, where E(ϕ, θ) is\na function of the player parameters. We will use the following identities, which are\nobtained from the chain rule:\nd∇ϕE\ndϕ\n∇ϕE = ∇ϕ\n\u0010∥∇ϕE∥2\n2\n\u0011\n(B.95)\nd∇θE\ndθ\n∇θE = ∇θ\n\u0010∥∇θE∥2\n2\n\u0011\n(B.96)\nd∇ϕE\ndθ\n∇θE =\n\u0012d∇θE\ndϕ\n\u0013T\n∇θE = ∇ϕ\n\u0010∥∇θE∥2\n2\n\u0011\n(B.97)\nd∇θE\ndϕ ∇ϕE =\n\u0012d∇ϕE\ndθ\n\u0013T\n∇ϕE = ∇θ\n\u0010∥∇ϕE∥2\n2\n\u0011\n.\n(B.98)\nB.2.1\nCommon-payoff alternating two-player games\nCorollary common-payoff alternating two-player games (Corollary\n4.4.1)\nIn a two-player common-payoff game with common loss E, alternating gradient\ndescent – as described in Eqs (B.35) and (B.36)—with one update per player follows\na gradient flow given by the modified losses\n˜Eϕ = E + υϕh\n4\n\u0000∥∇ϕE∥2 + ∥∇θE∥2\u0001\n(B.99)\n˜Eθ = E + υθh\n4\n\u0012\n(1 −2υϕ\nυθ\n) ∥∇ϕE∥2 + ∥∇θE∥2\n\u0013\n(B.100)\nwith an error of size O(h3) after one update step.\nIn the common-payoff case, both players minimise the loss function E. Substi-\ntuting f = −∇ϕE and g = −∇θE into the corrections f1 and g1 for the alternating\nB.2. Proof of the main corollaries\n297\nEuler updates in Theorem 4.2.2 and using the gradient identities above yields\nf1 = −1\n2\n\u0012 df\ndϕf + df\ndθg\n\u0013\n(B.101)\n= −1\n2\n\u0012 1\nm\nd∇ϕE\ndϕ\n∇ϕE + d∇ϕE\ndθ\n∇θE\n\u0013\n(def f, g, B.102)\n= −1\n2\n\u0012 1\nm∇ϕ\n∥∇ϕE∥2\n2\n+ ∇ϕ\n∥∇θE∥2\n2\n\u0013\n(via (B.95),(B.97), B.103)\n= −∇ϕ\n\u0012 1\n4m∥∇ϕE∥2 + 1\n4∥∇θE∥2\n\u0013\n(B.104)\ng1 = −1\n2\n\u0012\n(1 −2υϕ\nυθ\n) dg\ndϕf + 1\nk\ndg\ndθg\n\u0013\n(B.105)\n= −1\n2\n\u0012\n(1 −2υϕ\nυθ\n)d∇θE\ndϕ ∇ϕE + 1\nk\nd∇θE\ndθ\n∇θE\n\u0013\n(def f, g, B.106)\n= −1\n2\n\u0012\n(1 −2υϕ\nυθ\n)∇θ\n∥∇ϕE∥2\n2\n+ 1\nk∇θ\n∥∇θE∥2\n2\n\u0013\n(via (B.96),(B.98), B.107)\n= −∇θ\n\u00121\n4(1 −2υϕ\nυθ\n)∥∇ϕE∥2 + 1\nk∥∇θE∥2\n\u0013\n(B.108)\nNow, replacing the gradient expressions for f1 and g1 calculated above into the\nmodified equations ˙ϕ = −∇ϕE + υϕhf1 and ˙θ = −∇θE + υθhg1 and factoring out\nthe gradients, we obtain the modified equations in the form of the flows:\n˙ϕ = −∇ϕ eEϕ\n(B.109)\n˙θ = −∇θ eEθ,\n(B.110)\nwith the following modified losses for each players:\neEϕ = E + υϕh\n4\n\u0012 1\nm∥∇ϕE∥2 + ∥∇θE∥2\n\u0013\n(B.111)\neEθ = E + υθh\n4\n\u0012\n(1 −2υϕ\nυθ\n)∥∇ϕE∥2 + 1\nk∥∇θE∥2\n\u0013\n.\n(B.112)\nWe obtain Corollary 5.1 by setting the number of player updates to one: m = k = 1.\nB.2. Proof of the main corollaries\n298\nB.2.2\nZero-sum simultaneous two-player games\nCorollary Zero-sum simultaneous two-player games (Corollary 4.5.1) In a\nzero-sum two-player differentiable game, simultaneous gradient descent updates - as\ndescribed in Eqs (B.5) and (B.6) - follows a gradient flow given by the modified losses\n˜Eϕ = −E + υϕh\n4 ∥∇ϕE∥2 −υϕh\n4 ∥∇θE∥2\n(B.113)\n˜Eθ = E −υθh\n4 ∥∇ϕE∥2 + υθh\n4 ∥∇θE∥2\n(B.114)\nwith an error of size O(h3) after one update step.\nSubstituting f = ∇ϕE and g = −∇θE into the corrections f1 and g1 for the\nsimultaneous Euler updates and using the gradient identities above yields\nf1 = −1\n2\n\u0012 df\ndϕf + df\ndθg\n\u0013\n(B.115)\n= −1\n2\n\u0012d∇ϕE\ndϕ\n∇ϕE −d∇ϕE\ndθ\n∇θE\n\u0013\n(def f, g, B.116)\n= −1\n2\n\u0012\n∇ϕ\n∥∇ϕE∥2\n2\n−∇ϕ\n∥∇θE∥2\n2\n\u0013\n(via (B.95),(B.97), B.117)\n= −∇ϕ\n\u00121\n4∥∇ϕE∥2 −1\n4∥∇θE∥2\n\u0013\n(B.118)\ng1 = −1\n2\n\u0012 dg\ndϕf + dg\ndθg\n\u0013\n(B.119)\n= −1\n2\n\u0012\n−d∇θE\ndϕ ∇ϕE + d∇θE\ndθ\n∇θE\n\u0013\n(def f, g, B.120)\n= −1\n2\n\u0012\n−∇θ\n∥∇ϕE∥2\n2\n+ ∇θ\n∥∇θE∥2\n2\n\u0013\n(via (B.96),(B.98), B.121)\n= −∇θ\n\u0012\n−1\n4∥∇ϕE∥2 + 1\n4∥∇θE∥2\n\u0013\n(B.122)\nNow, replacing the gradient expressions for f1 and g1 calculated above into the\nmodified equations ˙ϕ = −∇ϕ(−E) + υϕhf1 and ˙θ = −∇θE + υθhg1 and factoring\nout the gradients, we obtain the modified equations in the form of the flows:\n˙ϕ = −∇ϕ eEϕ\n(B.123)\n˙θ = −∇θ eEθ,\n(B.124)\nB.2. Proof of the main corollaries\n299\nwith the following modified losses for each players:\neEϕ = −E + υϕh\n4 ∥∇ϕE∥2 −υϕh\n4 ∥∇θE∥2\n(B.125)\neEθ = E −υθh\n4 ∥∇ϕE∥2 + υθh\n4 ∥∇θE∥2.\n(B.126)\nB.2.3\nZero-sum alternating two-player games\nCorollary Zero-sum alternating two-player games (Corollary 4.5.2) In a\nzero-sum two-player differentiable game, alternating gradient descent - as described\nin Eqs (B.35) and (B.36) - follows a gradient flow given by the modified losses\n˜Eϕ = −E + υϕh\n4m ∥∇ϕE∥2 −υϕh\n4 ∥∇θE∥2\n(B.127)\n˜Eθ = E −υθh\n4 (1 −2υϕ\nυθ\n) ∥∇ϕE∥2 + υθh\n4k ∥∇θE∥2\n(B.128)\nwith an error of size O(h3) after one update step.\nIn this last case, substituting f = ∇ϕE and g = −∇θE into the corrections\nf1 and g1 for the alternating Euler updates and using the gradient identities above\nyields the modified system as well as the modified losses exactly in the same way as\nfor the two previous cases above. (This amounts to a single sign change in the proof\nof Corollary 5.1)\nB.2.4\nSelf and interaction terms in zero-sum games\nRemark B.2.1 Throughout the Appendix, we will refer to self terms and interaction\nterms, as originally defined in our thesis (Definition 4.2.1), and we will also use this\nterminology to refer to terms in our derivations that originate from the self terms and\ninteraction terms. While a slight abuse of language, we find it useful to emphasise\nthe provenance of these terms in our discussion.\nFor the case of zero-sum games trained with simultaneous gradient descent, the\nself terms encourage the minimisation of the player’s own gradient norm, while the\nB.3. Discretisation drift in Runge–Kutta 4\n300\ninteraction terms encourage the maximisation of the other player’s gradient norm:\neEϕ = −E + υϕh\n4 ∥∇ϕE∥2\n|\n{z\n}\nself\n−υϕh\n4 ∥∇θE∥2\n|\n{z\n}\ninteraction\n,\n(B.129)\neEθ = E −υθh\n4 ∥∇ϕE∥2\n|\n{z\n}\ninteraction\n+υθh\n4 ∥∇θE∥2\n|\n{z\n}\nself\n.\n(B.130)\nSimilar terms are obtained for alternating gradient descent, with the only\ndifference that the sign of the second player’s interaction term can be positive,\ndepending on the learning rate ratio of the two players.\nB.3\nDiscretisation drift in Runge–Kutta 4\nB.3.1\nRunge–Kutta 4 for one player\nRunge–Kutta 4 (RK4) is a Runge–Kutta a method of order 4. That is, the discrete\nsteps of RK4 coincide with the dyanamics of the flow they discretise up to O(h5)\n(i.e., the local error after one step is of order O(h5)). The modified equation for a\nmethod of order n starts with corrections at order hn+1 (i.e., all the lower corrections\nvanish; see [179] and [324] for further details). This means that RK4 has no DD up\nto order O(h5), and why for small learning rates RK4 can be used as a proxy for the\nexact flow.\nB.3.2\nRunge–Kutta 4 for two players\nWhen we use equal learning rates and simultaneous updates, the two-player game is\nalways equivalent to the one player case, so Runge–Kutta 4 will have a local error of\nO(h5). However, in the case of two-players games, we have the additional freedom\nof having different learning rates for each of the players. We now show that when\nthe learning rates of the two players are different, RK4 also also has a drift effect of\norder 2 and the DD term comes exclusively from the interaction terms. To do so, we\nuse BEA and apply the same steps we performed for simultaneous and alternating\nEuler updates.\nB.3. Discretisation drift in Runge–Kutta 4\n301\nStep 1: Expand the updates per player via a Taylor expansion.\nThe simultaneous Runge–Kutta 4 updates for two players require defining:\nk1,ϕ = f(ϕt−1, θt−1)\n(B.131)\nk1,θ = g(ϕt−1, θt−1)\n(B.132)\nk2,ϕ = f(ϕt−1 + υϕh\n2 k1,ϕ, θt−1 + υθh\n2 k1,θ)\n(B.133)\nk2,θ = g(ϕt−1 + υϕh\n2 k1,ϕ, θt−1 + υθh\n2 k1,θ)\n(B.134)\nk3,ϕ = f(ϕt−1 + υϕh\n2 k2,ϕ, θt−1 + υθh\n2 k2,θ)\n(B.135)\nk3,θ = g(ϕt−1 + υϕh\n2 k2,ϕ, θt−1 + υθh\n2 k2,θ)\n(B.136)\nk4,ϕ = f(ϕt−1 + υϕh\n2 k3,ϕ, θt−1 + υθh\n2 k3,θ)\n(B.137)\nk4,θ = g(ϕt−1 + υϕh\n2 k3,ϕ, θt−1 + υθh\n2 k3,θ)\n(B.138)\nFrom here we can define the update vectors:\nkϕ = 1\n6 (k1,ϕ + 2k2,ϕ + 2k3,ϕ + k4,ϕ)\n(B.139)\nkθ = 1\n6 (k1,θ + 2k2,θ + 2k3,θ + k4,θ)\n(B.140)\nAnd the parameter updates\nϕt = ϕt−1 + υϕhkϕ\n(B.141)\nθt = θt−1 + υθhkθ\n(B.142)\nB.3. Discretisation drift in Runge–Kutta 4\n302\nTo find the discretisation drift we expand each intermediate step:\nk1,ϕ = f(t−1)\n(B.143)\nk1,θ = g(t−1)\n(B.144)\nk2,ϕ = f\n\u0012\nϕt−1 + υϕh\n2 k1,ϕ, θt−1 + υθh\n2 k1,θ\n\u0013\n(B.145)\n= f(t−1) + υθh\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\nk1,θ + υϕh\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nk1,ϕ + O(h2)\n(B.146)\n= f(t−1) + υθh\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.147)\nk2,θ = g\n\u0012\nϕt−1 + υϕh\n2 k1,ϕ, θ + υθh\n2 k1,θ\n\u0013\n(B.148)\n= g(t−1) + υθh\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\nk1,θ + υϕh\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nk1,ϕ + O(h2)\n(B.149)\n= g(t−1) + υθh\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.150)\nk3,ϕ = f(t−1) + υθh\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\nk2,θ + υϕh\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nk2,ϕ + O(h2)\n(B.151)\n= f(t−1) + υθh\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.152)\nk3,θ = g(t−1) + υθh\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.153)\nk4,ϕ = f(t−1) + υθh\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.154)\nk4,θ = g(t−1) + υθh\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.155)\nwith the update direction:\nkϕ = 1\n6 (k1,ϕ + 2k2,ϕ + 2k3,ϕ + k4,ϕ)\n(B.156)\n= f(t−1) + υθh\n2\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.157)\nkθ = 1\n6 (k1,θ + 2k2,θ + 2k3,θ + k4,θ)\n(B.158)\n= g(t−1) + υθh\n2\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + υϕh\n2\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h2)\n(B.159)\nB.3. Discretisation drift in Runge–Kutta 4\n303\nand thus the discrete update of the Runge–Kutta 4 for two players are:\nϕt = ϕt−1 + υϕhf(t−1) + 1\n2υϕυθh2 df\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + 1\n2υϕ\n2h2 df\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.160)\nθt = θt−1 + υθhg(t−1) + 1\n2υθ\n2h2 dg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + 1\n2υϕυθh2 dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1) + O(h3)\n(B.161)\nStep 2: Expand the continuous-time changes for the modified flow\n(Identical to the simultaneous Euler updates.)\nStep 3: Matching the discrete and modified continuous updates.\nAs in the always in Step 3, we substitute ϕt−1 and θt−1 in Lemma B.1.1:\nϕ(τ + τϕ) = ϕt−1 + τϕf(t−1) + τ 2\nϕf1(ϕt−1, θt−1) + 1\n2τ 2\nϕ\ndf\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n+ 1\n2τ 2\nϕ\ndf\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + O(τ 3\nϕ)\n(B.162)\nθ(τ + τθ) = θt−1 + τθg(t−1) + τ 2\nθ g1(ϕt−1, θt−1) + 1\n2τ 2\nθ\ndg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n+ 1\n2τ 2\nθ\ndg\ndθ\n\f\f\f\f\n(t−1)\ng(t−1) + O(τ 3\nθ )\n(B.163)\nFor the first order terms we obtain τϕ = υϕh and τθ = υθh. We match the O(h2)\nterms in the equations above with the discrete Runge–Kutta 4 updates shown in\nEq (B.160) and (B.161) and notice that:\nf1(ϕt−1, θt−1) = 1\n2(υθ\nυϕ\n−1) df\ndθ\n\f\f\f\f\n(t−1)\ng(t−1)\n(B.164)\ng1(ϕt−1, θt−1) = 1\n2(υϕ\nυθ\n−1) dg\ndϕ\n\f\f\f\f\n(t−1)\nf(t−1)\n(B.165)\nThus, if υϕ ̸= υθ RK4 has second order drift. This is why, in all our experiments\ncomparing with RK4, we use the same learning rates for the two players υϕ = υθ, to\nensure that we use a method which has no DD up to order O(h5).\nB.4. Stability analysis\n304\nB.4\nStability analysis\nIn this section, we give all the details of the stability analysis results, to showcase\nhow the modified flows we have derived can be used as a tool for stability analysis.\nWe provide the full computation for the Jacobian of the modified vector fields for\nsimultaneous and alternating Euler updates, as well as the calculation of their trace,\nand show how this can be used to determine the stability of the modified vector fields.\nWhile analysing the modified vector fields is not equivalent to analysing the discrete\ndynamics due to the higher order errors of the drift which we ignore, it provides a\nbetter approximation than what is often used in practice, namely the original flows,\nwhich completely ignore the drift.\nB.4.1\nSimultaneous Euler updates\nConsider a two-player game with dynamics given by ϕt = ϕt−1 + υϕhf(t−1) and\nθt = θt−1 + υθhg(t−1) (Eqs (B.5) and (B.6)). The modified dynamics for this game\nare given by ˙ϕ = ef, ˙θ = eg, where ef = f −υϕh\n2 ( df\ndϕf + df\ndθg) and eg = g −υθh\n2 ( dg\ndϕf + dg\ndθg)\n(Theorem (DD for simultaneous Euler updates)).\nThe stability of this system can be characterised by the modified Jacobian\nmatrix evaluated at the equilibria of the two-player game. The equilibria that we are\ninterested in for our stability analysis are the steady-state solutions of Eqs (B.5) and\n(B.6), given by f = 0, g = 0. These are also equilibrium solutions for the steady-state\nmodified equations1 given by ˜f = 0, ˜g = 0.\nThe modified Jacobian can be written, using block matrix notation as:\neJ =\n\n\nd ef\ndϕ\nd ef\ndθ\ndeg\ndϕ\ndeg\ndθ\n\n\n(B.166)\nNext, we calculate each term in this block matrix. (In the following analysis, each\n1There are additional steady-state solutions for the modified equations. However, we can ignore\nthese since they are spurious solutions that do not correspond to steady states of the two-player\ngame, arising instead as an artifact of the O(h3) error in BEA.\nB.4. Stability analysis\n305\nterm is evaluated at an equilibrium solution given by f = 0, g = 0). We find:\nd ˜f\ndϕ = df\ndϕ −υϕh\n2\n\u0012\n( df\ndϕ)2 +\ndf\ndϕdϕf + df\ndθ\ndg\ndϕ +\ndf\ndϕdθg\n\u0013\n(B.167)\n= df\ndϕ −υϕh\n2\n\u0012\n( df\ndϕ)2 + df\ndθ\ndg\ndϕ\n\u0013\n(B.168)\nd ˜f\ndθ = df\ndθ −υϕh\n2\n\u0012 df\ndϕ\ndf\ndθ +\ndf\ndθdϕf + df\ndθ\ndg\ndθ +\ndf\ndθdθg\n\u0013\n(B.169)\n= df\ndθ −υϕh\n2\n\u0012 df\ndϕ\ndf\ndθ + df\ndθ\ndg\ndθ\n\u0013\n(B.170)\nd˜g\ndϕ = dg\ndϕ −υθh\n2 (dg\ndθ\ndg\ndϕ +\ndg\ndϕdθg + dg\ndϕ\ndf\ndϕ +\ndg\ndϕdϕ)f\n(B.171)\n= dg\ndϕ −υθh\n2 (dg\ndθ\ndg\ndϕ + dg\ndϕ\ndf\ndϕ)\n(B.172)\nd˜g\ndθ = dg\ndθ −υθh\n2\n\u0012\n(dg\ndθ)2 +\ndg\ndθdθg + dg\ndϕ\ndf\ndθ +\ndg\ndθdϕf\n\u0013\n(B.173)\n= dg\ndθ −υθh\n2\n\u0012\n(dg\ndθ)2 + dg\ndϕ\ndf\ndθ\n\u0013\n(B.174)\nwhere\ndf\ndϕdθg is the matrix in Rm,m with\n\u0010\ndf\ndϕdθg\n\u0011\ni,j =\n\u0012\nd ∂fi\n∂ϕj\ndθ\n\u0013T\ng.\nGiven these calculations, we can now write\neJ =\n\n\nd ef\ndϕ\nd ef\ndθ\ndeg\ndϕ\ndeg\ndθ\n\n= J −h\n2Ksim\n(B.175)\nwhere J is the Jacobian of the unmodified flow\nJ =\n\n\ndf\ndϕ\ndf\ndθ\ndg\ndϕ\ndg\ndθ\n\n\n(B.176)\nand\nKsim =\n\nυϕ( df\ndϕ)2 + υϕ\ndf\ndθ\ndg\ndϕ\nυϕ\ndf\ndϕ\ndf\ndθ + υϕ\ndf\ndθ\ndg\ndθ\nυθ\ndg\ndθ\ndg\ndϕ + υθ\ndg\ndϕ\ndf\ndϕ\nυθ( dg\ndθ)2 + υθ\ndg\ndϕ\ndf\ndθ\n\n.\n(B.177)\nUsing the modified Jacobian, we can use Remark 2.3.1 to determine the stability\nof fixed points. A necessary condition for stability is that the trace of the modified\nB.4. Stability analysis\n306\nJacobian is less than or equal to zero (i.e. Tr(eJ) ≤0), since the trace is the sum of\neigenvalues. Using the property of trace additivity and the trace cyclic property we\nsee that:\nTr(eJ) = Tr(J)−h\n2\n\u0012\nυϕ Tr(( df\ndϕ)2) + υθ Tr((dg\ndθ)2)\n\u0013\n−h\n2(υϕ+υθ) Tr( df\ndθ\ndg\ndϕ). (B.178)\nInstability caused by discretisation drift. We now use the above anal-\nysis to show that the equilibria of a two-player game following the modified flow\nobtained simultaneous Euler updates as defined by Eqs (B.5) and (B.6) can become\nasymptotically unstable for some games.\nThere are choices of f and g that have stable equilibrium without DD, but are\nunstable under DD. For example, consider the zero-sum two-player game with f =\n∇ϕE and g = −∇θE. In this case df\ndϕ = ∇2\nϕE, dg\ndθ = −∇2\nθE and df\ndθ = dg\ndϕ\nT = ∇ϕ,θE.\nWe can thus use Tr(ATA) = ∥A∥2 where ∥.∥2 denotes the Frobenius norm, we have\nTr(eJ) = Tr(J) −h\n2\n\u0010\nυϕ∥∇2\nϕE∥\n2\n2 + υθ∥∇2\nθE∥\n2\n2\n\u0011\n+ h\n2(υϕ + υθ)∥∇ϕ,θE∥2\n2.\n(B.179)\nThe Dirac-GAN is an example of a zero-sum two-player game that is stable without\nDD, but becomes unstable under DD with Tr(eJ) = h(υϕ + υθ)∥∇ϕ,θE∥2\n2/2 > 0 (see\nSection B.6.2).\nB.4.2\nAlternating Euler updates\nConsider a two-player game with dynamics given by Eqs (B.35) and (B.36). The\nmodified dynamics for this game are given by ˙ϕ = f −υϕh\n2\n\u0010\n1\nm\ndf\ndϕf + df\ndθg\n\u0011\n, ˙θ =\ng −υθh\n2\n\u0010\n(1 −2υϕ\nυθ ) dg\ndϕf + 1\nk\ndg\ndθg\n\u0011\n(Theorem DD for alternating Euler updates).\nThe stability of this system can be characterised by the modified Jacobian\nmatrix evaluated at the equilibria of the two-player game. The equilibria that we are\ninterested in for our stability analysis are the steady-state solutions of Eqs (B.5) and\n(B.6), given by f = 0, g = 0. These are also equilibrium solutions for the steady-state\nmodified equations2 given by ˜f = 0, ˜g = 0.\n2There are additional steady-state solutions for the modified equations. However, we can ignore\nthese since they are spurious solutions that do not correspond to steady states of the two-player\nB.4. Stability analysis\n307\nThe modified Jacobian can be written, using block matrix notation as:\neJ =\n\n\nd ef\ndϕ\nd ef\ndθ\ndeg\ndϕ\ndeg\ndθ\n\n\n(B.180)\nNext, we calculate each term in this block matrix. (In the following analysis, each\nterm is evaluated at an equilibrium solution given by f = 0, g = 0). We find:\nd ˜f\ndϕ = df\ndϕ −υϕh\n2\n\u0012 1\nm( df\ndϕ)2 + 1\nm\ndf\ndϕdϕf + df\ndθ\ndg\ndϕ +\ndf\ndϕdθg\n\u0013\n(B.181)\n= df\ndϕ −υϕh\n2\n\u0012 1\nm( df\ndϕ)2 + df\ndθ\ndg\ndϕ\n\u0013\n(B.182)\nd ˜f\ndθ = df\ndθ −υϕh\n2\n\u0012 1\nm\ndf\ndθ\ndf\ndϕ + 1\nm\ndf\ndθdϕf + df\ndθ\ndg\ndθ +\ndf\ndθdθg\n\u0013\n(B.183)\n= df\ndθ −υϕh\n2\n\u0012 1\nm\ndf\ndϕ\ndf\ndθ + df\ndθ\ndg\ndθ\n\u0013\n(B.184)\nd˜g\ndϕ = dg\ndϕ −υθh\n2 (1\nk\ndg\ndθ\ndg\ndϕ + 1\nk\ndg\ndϕdθg + (1 −2υϕ\nυθ\n) dg\ndϕ\ndf\ndϕ + (1 −2υϕ\nυθ\n)\ndg\ndϕdϕf)\n(B.185)\n= dg\ndϕ −υθh\n2 (1\nk\ndg\ndθ\ndg\ndϕ + (1 −2υϕ\nυθ\n) dg\ndϕ\ndf\ndϕ)\n(B.186)\nd˜g\ndθ = dg\ndθ −υθh\n2\n\u00121\nk(dg\ndθ)2 + 1\nk\ndg\ndθdθg + (1 −2υϕ\nυθ\n) dg\ndϕ\ndf\ndθ + (1 −2υϕ\nυθ\n) dg\ndθdϕf\n\u0013\n(B.187)\n= dg\ndθ −υθh\n2\n\u00121\nk(dg\ndθ)2 + (1 −2υϕ\nυθ\n) dg\ndϕ\ndf\ndθ\n\u0013\n(B.188)\nwhere\ndf\ndϕdθg is the matrix in Rm,m with\n\u0010\ndf\ndϕdθg\n\u0011\ni,j =\n\u0012\nd ∂fi\n∂ϕj\ndθ\n\u0013T\ng.\nGiven these calculations, we can now write:\neJ =\n\n\nd ef\ndϕ\nd ef\ndθ\ndeg\ndϕ\ndeg\ndθ\n\n= J −h\n2Kalt\n(B.189)\ngame, arising instead as an artifact of the O(h3) error in BEA.\nB.5. SGA in zero-sum games\n308\nwhere J is the Jacobian of the unmodified flow:\nJ =\n\n\ndf\ndϕ\ndf\ndθ\ndg\ndϕ\ndg\ndθ\n\n\n(B.190)\nand\nKalt =\n\n\nυϕ\nm ( df\ndϕ)2 + υϕ\ndf\ndθ\ndg\ndϕ\nυϕ\nm\ndf\ndϕ\ndf\ndθ + υϕ\ndf\ndθ\ndg\ndθ\nυθ\nk\ndg\ndθ\ndg\ndϕ + υθ(1 −2υϕ\nυθ ) dg\ndϕ\ndf\ndϕ\nυθ\nk ( dg\ndθ)2 + υθ(1 −2υϕ\nυθ ) dg\ndϕ\ndf\ndθ\n\n\n(B.191)\nUsing the modified Jacobian, we can use Remark 2.3.1 to determine the stability\nof fixed points. A necessary condition for stability is that the trace of the modified\nJacobian is less than or equal to zero (i.e. Tr(eJ) ≤0), since the trace is the sum of\neigenvalues. Using the property of trace additivity and the trace cyclic property we\nsee that:\nTr(eJ) = Tr(J)−h\n2\n\u0012υϕ\nm Tr(( df\ndϕ)2) + υθ\nk Tr((dg\ndθ)2)\n\u0013\n−h\n2(υθ−υϕ) Tr( df\ndθ\ndg\ndϕ). (B.192)\nWe note that unlike for simultaneous updates, even if Tr( df\ndθ\ndg\ndϕ) is negative, if υθ < υϕ,\nthe trace of the modified system will stay negative, so the necessary condition for\nthe system to remain stable is still satisfied. However, since this is not a sufficient\ncondition, the modified system could still be unstable.\nB.5\nSGA in zero-sum games\nFor clarity, this section reproduces Symplectic Gradient Adjustment (SGA) from Bal-\nduzzi et al. [61] for zero-sum games, to showcase the appearance of interaction terms.\nWe have two players ϕ and θ minimizing loss functions E and −E, respectivelly,\nthen SGA defines the vector\nξ =\n\n∇ϕE\n−∇θE\n\n.\n(B.193)\nB.5. SGA in zero-sum games\n309\nThey then define Hξ the game Hessian (their Section 2.2)\nHξ =\n\n\ndE\ndϕdϕ\ndE\ndϕdθ\n−dE\ndθdϕ\n−dE\ndθdθ\n\n.\n(B.194)\nThus Hξ has the anti-symmetric component\nA = 1\n2(Hξ −HT\nξ ) = 1\n2\n\n\n0\ndE\ndϕdθ +\ndE\ndθdϕ\nT\n−dE\ndθdϕ −\ndE\ndϕdθ\nT\n0\n\n=\n\n\n0\ndE\ndϕdθ\n−dE\ndθdϕ\n0\n\n.\n(B.195)\nThe vector field ξ is modified according to SGA as\nˆξ = ξ + ζAT ξ =\n\n∇ϕE\n−∇θE\n\n+ ζ\n\n\n0\ndE\ndϕdθ\n−dE\ndθdϕ\n0\n\n\n\n∇ϕE\n−∇θE\n\n\n(B.196)\n=\n\n∇ϕE\n−∇θE\n\n+ ζ\n\n−dE\ndϕdθ∇θE\n−dE\ndθdϕ∇ϕE\n\n,\n(B.197)\nwhere ζ is either a hyperparameter or adjusted dynamically in training [61]. Thus,\nvia Eqs (B.98) and (B.97) the modified vector field can be simplified to\nˆξ =\n\n∇ϕ(E −ζ 1\n2 ∥∇θE∥2)\n∇θ(−E −ζ 1\n2 ∥∇ϕE∥2)\n\n.\n(B.198)\nTherefore, since ˆξ defines the negative of the vector field followed by the system, the\nmodified losses for the two players can be written respectively as\neL1 = E −ζ\n2 ∥∇θE∥2\n(B.199)\neL2 = −E −ζ\n2 ∥∇ϕE∥2 .\n(B.200)\nThus if ζ < 0, which is what we use in our experiments, the functional form of the\nmodified losses given by SGA is the same used to cancel the interaction terms of\nDD in the case of simultaneous gradient descent updates in zero sum games. We do\nB.6. DiracGAN - an illustrative example\n310\nhowever highlight a few differences in our approach compared to SGA: our approach\nextends to alternating updates and provides the optimal regularisation coefficients;\ncancelling the interaction terms of the drift is different compared to SGA for general\ngames.\nB.6\nDiracGAN - an illustrative example\nIn their work assessing the convergence of GAN training Mescheder et al. [68]\nintroduce the example of the DiracGAN, where the GAN is trying to learn a delta\ndistribution with mass at zero. More specifically, the generator G(z; θ) = θ with\nparameter θ parametrises constant functions whose images {θ} correspond to the\nsupport of the delta distribution δθ. The discriminator is a linear model D(x; ϕ) = ϕ·x\nwith parameter ϕ. The loss function is given by\nE(θ, ϕ) = l(θϕ) + l(0),\n(B.201)\nwhere l depends on the GAN used; for the standard GAN it is l = −log(1 + e−t).\nAs in Mescheder et al. [68], we assume l is continuously differentiable with l′(x) ̸= 0\nfor all x ∈R. The partial derivatives of the loss function\n∂E\n∂ϕ = l′(θϕ)θ,\n∂E\n∂θ = l′(θϕ)ϕ,\n(B.202)\nlead to the underlying continuous dynamics\n˙ϕ = f(θ, ϕ) = l′(θϕ)θ,\n˙θ = g(θ, ϕ) = −l′(θϕ)ϕ.\n(B.203)\nThus the only equilibrium of the game is θ = 0 and ϕ = 0.\nB.6.1\nReconciling discrete and continuous updates in\nDirac-GAN\nMescheder et al. [68] observed a discrepancy between the continuous and discrete\ndynamics of DiracGAN. They show that, for the problem in Eq (B.201), the contin-\nuous dynamics preserve θ2 + ϕ2, and thus cannot converge (Lemma 2.3 in Mescheder\nB.6. DiracGAN - an illustrative example\n311\net al. [68]), since\nd (θ2 + ϕ2)\ndt\n= 2θdθ\ndt + 2ϕdϕ\ndt = −2θl′(θϕ)ϕ + 2ϕl′(θϕ)θ = 0.\n(B.204)\nThey also observe that when following the discrete dynamics of simultaneous gradient\ndescent that θ2+ϕ2 increases in time (Lemma 2.4 in Mescheder et al. [68]). We resolve\nthis discrepancy between conclusions reached with continuous-time and discrete-time\nanalysis in DiracGAN, by showing that the modified continuous dynamics given by\nDD result in behaviour consistent with that of the discrete updates.\nProposition B.6.1 Following the continuous-time modified flow we obtained using\nBEA describing simultaneous gradient descent increases θ2 + ϕ2 in DiracGAN.\nProof: We assume both the generator and the discriminator use learning rates h, as\nin Mescheder et al. [68]. We first compute terms used by the zero-sum Colloraries 4.5.1\nand 4.5.2 in the main thesis.\n∥∇θE∥2 = l′(θϕ)2ϕ2\n(B.205)\n∥∇ϕE∥2 = l′(θϕ)2θ2\n(B.206)\nand\n∇θ ∥∇θE∥2 = 2ϕ3l′(θϕ)l′′(θϕ)\n(B.207)\n∇θ ∥∇ϕE∥2 = 2θl′(θϕ)2 + 2θ2ϕl′(θϕ)l′′(θϕ)\n(B.208)\n∇ϕ ∥∇θE∥2 = 2ϕl′(θϕ)2 + 2ϕ2θl′(θϕ)l′′(θϕ)\n(B.209)\n∇ϕ ∥∇ϕE∥2 = 2θ3l′(θϕ)l′′(θϕ).\n(B.210)\nThus, the modified flows are given by:\n˙ϕ = l′(θϕ)θ + h\n2\n\u0002\n−θ3l′(θϕ)l′′(θϕ) + ϕl′(θϕ)2 + ϕ2θl′(θϕ)l′′(θϕ)\n\u0003\n(B.211)\n˙θ = −l′(θϕ)ϕ −h\n2\n\u0002\n−θl′(θϕ)2 −θ2ϕl′(θϕ)l′′(θϕ) + ϕ3l′(θϕ)l′′(θϕ)\n\u0003\n.\n(B.212)\nB.6. DiracGAN - an illustrative example\n312\nBy denoting l′(θϕ) by l′ and l′′(θϕ) by l′′, then we have:\nd (θ2 + ϕ2)\ndt\n= 2θdθ\ndt + 2ϕdϕ\ndt\n(B.213)\n= 2θ\n\u0012\n−l′ϕ −h\n2\n\u0002\n−θl′2 −θ2ϕl′l′′ + ϕ3l′l′′\u0003\u0013\n+ 2ϕ\n\u0012\nl′θ + h\n2\n\u0002\n−θ3l′l′′ + ϕl′2 + ϕ2θl′l′′\u0003\u0013\n(B.214)\n= 2θ\n\u0012\n−h\n2\n\u0002\n−θl′2 −θ2ϕl′l′′ + ϕ3l′l′′\u0003\u0013\n+ 2ϕ\n\u0012h\n2\n\u0002\n−θ3l′l′′ + ϕl′2 + ϕ2θl′l′′\u0003\u0013\n(B.215)\n= hθ2l′2 + hθ3ϕl′l′′ −hϕ3θl′l′′ −hϕθ3l′l′′ + hϕ2l′2 + hϕ3θl′l′′\n(B.216)\n= hθ2l′2 + hϕ2l′2 > 0\n(B.217)\nfor all ϕ, θ ̸= 0, which shows that θ2 + ϕ2 is not preserved and it will strictly\nincrease for all values away from the equilibrium (we have used the assumption that\nl′(x) ̸= 0, ∀x ∈R).\n□\nWe have thus identified a continuous system which exhibits the same behaviour\nas described by Lemma 2.4 in Mescheder et al. [68], where a discrete system is\nanalysed.\nB.6.2\nDD changes the convergence behaviour of Dirac-GAN\nThe Jacobian of the unmodified Dirac-GAN evaluated at the equilibrium solution\ngiven by ϕ = 0, θ = 0. Since each parameter is one dimensional, we can write\nJ =\n\n∇ϕ,ϕE\n∇θ,ϕE\n−∇ϕ,θE\n−∇θ,θE\n\n=\n\n\n0\nl′(0)\n−l′(0)\n0\n\n\n(B.218)\nWe see that Tr(J) = 0 and the determinant |J| = l′(0)2. Therefore, the eigenvalues\nof this Jacobian are υθ± = Tr(J)/2 ±\np\nTr(J)2 −4|J|/2 = ±il′(0) (Reproduced from\nMescheder et al. [68]). This is an example of a stable center equilibrium.\nNext we calculate the Jacobian of the modified flows given by DiracGAN,\nB.6. DiracGAN - an illustrative example\n313\nevaluated at an equilibrium solution and find eJ = J −h∆/2, where\n∆=\n\nυϕ(∇ϕ,ϕE)2 −υϕ∇ϕ,θE∇θ,ϕE\nυϕ∇θ,ϕE∇ϕ,ϕE −υϕ∇θ,θE∇θ,ϕE\nυθ∇ϕ,θE∇θ,θE −υθ∇ϕ,ϕE∇ϕ,θE\nυθ(∇θ,θE)2 −υθ∇θ,ϕE∇ϕ,θE\n\n(B.219)\n=\n\n−υϕ∇ϕ,θE∇θ,ϕE\n0\n0\n−υθ∇θ,ϕE∇ϕ,θE\n\n\n(B.220)\n=\n\n−υϕl′(0)2\n0\n0\n−υθl′(0)2\n\n\n(B.221)\nso\neJ =\n\nhυϕl′(0)2/2\nl′(0)\n−l′(0)\nhυθl′(0)2/2\n\n.\n(B.222)\nNow, we see that the trace of the modified Jacobian for the Dirac-GAN is Tr(eJ) =\n(h/2)(υϕ + υθ)l′(0)2 > 0, so the modified flows induced by gradient descent in\nDiracGAN are unstable.\nB.6.3\nExplicit regularisation stabilises Dirac-GAN\nHere, we show that we can use our stability analysis to identify forms of explicit\nregularisation that can counteract the destabilizing impact of DD. Consider the\nDirac-GAN with explicit regularisation of the following form: Eϕ = −E + γ∥∇θE∥2\nand Eθ = E + ζ∥∇ϕE∥2 where ϕt = ϕt−1 −υϕh∇ϕEϕ and θt = θt−1 −υθh∇θEθ and\nwith γ, ζ ∼O(h). The modified Jacobian for this system is given by\neJ =\n\nhυϕ/2∇ϕ,θE∇θ,ϕE −γ∇ϕ,ϕ ∥∇θE∥2\n∇θ,ϕE\n−∇ϕ,θE\nhυθ/2∇θ,ϕE∇ϕ,θE −ζ∇θ,θ ∥∇ϕE∥2\n\n\n(B.223)\n=\n\n(hυϕ/2 −2γ)l′(0)2\nl′(0)\n−l′(0)\n(hυθ/2 −2ζ)l′(0)2\n\n\n(B.224)\nB.7. The PF for games: linearisation results around critical points\n314\nThe determinant of the modified Jacobian is | eJ| = (hυϕ/2 −2γ)(hυθ/2 −2ζ)l′(0)4 +\nl′(0)2 and the trace is Tr(eJ) = (hυϕ/2 −2γ)l′(0)2 + (hυθ/2 −2ζ)l′(0)2. A necessary\nand sufficient condition for asymptotic stability is |eJ| > 0 and Tr(eJ) < 0 (since this\nguarantees that the eigenvalues of the modified Jacobian have negative real part).\nTherefore, if γ > hυϕ/4 and ζ > hυθ/4, the system is asymptotically stable. We\nnote however that in practice, when using discrete updates, the exact threshold for\nstability will have a O(h3) correction, arising from the O(h3) error in our BEA. Also,\nwe see that when γ = hυϕ/4 and ζ = hυθ/4, the contribution of the cross-terms is\ncancelled out (up to an O(h3) correction). In Figure 4.5b, we see an example where\nthis explicit regularisation stabilises the DiracGAN, so that it converges toward the\nequilibrium solution.\nB.7\nThe PF for games: linearisation results\naround critical points\nAssume we are in the general setting described in Section 4.8. To simplify our\nnotation we denote [ϕ, θ] = ψ and the vector [∇ϕEϕ, ∇θEθ] as u(ψ). Thus our\ndiscrete simultaneous gradient descent updates become\nψt = ψt−1 −hu(ψt−1).\n(B.225)\nWe also note that from here on we no longer use the structure of the vector field, so\nthe below proof applies to any Euler update.\nAssume we are around a critical point ψ∗, i.e. u(ψ∗) = 0. We can write\nu(ψ) ≈u(ψ∗) + du\ndψ(ψ)(ψ −ψ∗) = du\ndψ(ψ∗)(ψ −ψ∗).\n(B.226)\nReplacing this in the discrete updates in Eq (B.225)\nψt −ψ∗≈(I −h du\ndψ(ψ∗))(ψt−1 −ψ∗) ≈(I −h du\ndψ(ψ∗))t(ψ0 −ψ∗).\n(B.227)\nB.7. The PF for games: linearisation results around critical points\n315\nIf we now take the Jordan normal form of du\ndψ(ψ∗) = P−1JP, we can write\nψt −ψ∗≈(I −h du\ndψ(ψ∗))t(ψ0 −ψ∗)\n(B.228)\n= (I −hP−1JP)t(ψ0 −ψ∗)\n(B.229)\n= P−1(I −hJ)tP(ψ0 −ψ∗).\n(B.230)\nUnder the above approximation\nψt = ψ∗+ P−1(I −hJ)tP(ψ0 −ψ∗),\n(B.231)\nwhich we can write in continuous form as (we use that iteration n is at time t = nh):\nψ(t) = ψ∗+ P−1(I −hJ)t/hP(ψ0 −ψ∗)\n(B.232)\n= ψ∗+ P−1elog(I−hJ)t/hP(ψ0 −ψ∗).\n(B.233)\nTaking the derivative w.r.t. t\n˙ψ = 1\nhP−1 log(I −hJ)elog(I−hJ)t/hP(ψ0 −ψ∗)\n(B.234)\n= 1\nhP−1 log(I −hJ)elog(I−hJ)t/hPP−1(elog(I−hJ)t/h)−1P(ψ −ψ∗)\n(B.235)\n= 1\nhP−1 log(I −hJ)P(ψ −ψ∗).\n(B.236)\nFrom here, if we assume u(ψ∗) is invertible we can write from the approximation\nabove (Eq. (B.226)) that ψ −ψ∗= du\ndψ(ψ∗)−1u(ψ) = P−1J−1Pu(ψ) and replacing\nthis in the above we obtain\n˙ψ = 1\nhP−1 log(I −hJ)PP−1J−1Pu(ψ)\n(B.237)\n= 1\nhP−1 log(I −hJ)J−1Pu(ψ),\n(B.238)\nwhich is the generalisation of the PF presented in Eq (4.67).\nB.8. GAN Experimental details\n316\nB.8\nGAN Experimental details\nUnless otherwise specified, all GAN experiments use the CIFAR-10 dataset and the\nCIFAR-10 convolutional SN-GAN architectures—see Table 3 in Section B.4 in Miyato\net al. [30].\nLibraries: We use JAX [325] to implement our models, with Haiku [326] as the\nneural network library, and Optax [327] for optimisation.\nComputer Architectures: All models are trained on NVIDIA V100 GPUs. Each\nmodel is trained on 4 devices.\nB.8.1\nImplementing explicit regularisation\nThe loss functions we are interested are of the form\nL = Ep(x)fθ(x)\n(B.239)\nWe then have\n∇θiL = Ep(x)∇θifθ(x)\n(B.240)\n∥∇θL∥2 =\ni=|θ|\nX\ni=1\n(∇θiL)2 =\ni=|θ|\nX\ni=1\n\u0000Ep(x)∇θifθ(x)\n\u00012\n(B.241)\nto obtain an unbiased estimate of the above using samples, we have that:\n∥∇θL∥2 =\ni=|θ|\nX\ni=1\n\u0000Ep(x)∇θifθ(x)\n\u00012\n(B.242)\n=\ni=|θ|\nX\ni=1\n(Ep(x)∇θifθ(x))(Ep(x)∇θifθ(x))\n(B.243)\n≈\ni=|θ|\nX\ni=1\n \n1\nN\nN\nX\nk=1\n∇θifθ(d\nx1,k)\n!  \n1\nN\nN\nX\nj=1\n∇θifθ(d\nx2,j)\n!\n(B.244)\nso we have to use two sets of samples x1,k ∼p(x) and x2,j ∼p(x) from the true\ndistribution (by splitting the batch into two or using a separate batch) to obtain\nthe correct norm. To compute an estimator for ∇ϕ ∥∇θL∥2, we can compute the\ngradient of the above unbiased estimator of ∥∇θL∥2. However, to avoid computing\nB.9. Additional experimental results\n317\ngradients for two sets of samples, we derive another unbiased gradient estimator,\nwhich we use in all our experiments:\n2\nN\ni=|θ|\nX\ni=1\nN\nX\nk=1\n∇ϕ∇θifθ(d\nx1,j)∇θifθ(d\nx2,j)\n(B.245)\nB.9\nAdditional experimental results\nB.9.1\nAdditional results using zero-sum GANs\nWe show additional results showcasing the effect of DD on zero-sum games in\nFigure B.1. We see that not only do simultaneous updates perform worse than\nalternating updates when using the best hyperparameters, but that simultaneous\nupdating is much more sensitive to hyperparameter choice. We also see that multiple\nupdates can improve the stability of a GAN trained using zero-sum losses, but this\nstrongly depends on the choice of learning rate.\nLeast squares GANs. In order to assess the robustness of our results inde-\npendent of the GAN loss used, we perform additional experimental results using\nGANs trained with a least square loss (LS-GAN [38]). We show results in Figure B.2,\nwhere we see that for the least square loss too, the learning rate ratios for which the\ngenerator drift does not maximise the discriminator norm (learning rate ratios above\nor equal to 0.5) perform best and exhibit less variance.\nB.9.2\nGANs using the non-saturating loss\nNext, we explore how the strength of the DD depends on the game dynamics. We\ndo this by comparing the relative effect that numerical integration schemes have\nacross different games. To this end, we consider the non-saturating loss introduced\nin the original GAN paper (−log D(G(z; θ); ϕ)). This loss has been extensively\nused since it helps to avoid problematic gradients early in training. When using\nthis loss, we see that there is little difference between simultaneous and alternating\nupdates (Figure B.3), unlike the saturating loss case. These results demonstrate\nthat since DD depends on the underlying dynamics, it is difficult to make general\ngame-independent predictions about which numerical integrator will perform best.\nB.9. Additional experimental results\n318\nSimultaneous\nAlternating\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\n(a) Hyperparameter sensitivity.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nSimultaneous\nAlternating\n(b) Learning curves.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating, zero sum\nNum updates\n1.0\n4.0\nLearning rate\n0.001\n0.005\n(c) The effect of multiple alternating updates.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nZero sum\nEuler (sim)\nEuler (alt)\nRK4\n(d) Equal learning rates.\nFigure B.1: The effect of discretisation drift on zero-sum games; using the saturating\ngenerator loss.\nB.9.3\nExplicit regularisation in zero-sum games trained\nusing simultaneous gradient descent\nWe now show additional experimental results and visualisations obtained using\nexplicit regularisation obtained using the original zero sum GAN objective, as\npresented in Goodfellow et al. [32].\nWe show the improvement that can be obtained compared to gradient descent\nwith simultaneous updates by cancelling the interaction terms in Figure B.4. We\nadditionally show results obtained from strengthening the self terms in Figure B.6.\nIn Figure B.5, we show that by cancelling interaction terms, SGD becomes a\ncompetitive optimisation algorithm when training the original GAN. We also note\nthat in the case of Adam, while convergence is substantially faster than with SGD,\nwe notice a degrade in performance later in training. This is something that has\nB.9. Additional experimental results\n319\nSimultaneous\nAlternating\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum, LS-GAN\n(a) Hyperparameter sensitivity.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nLearning rate ratio\n0.1\n0.2\n0.5\n1.0\n2.0\n5.0\n(b) The effect of learning rate ratios for alternating\nupdates.\nFigure B.2: The effect of discretisation drift on zero-sum games; least square losses.\nbeen observed in other works as well (e.g. [67]).\nMore percentiles. Throughout the main thesis, we displayed the best 10%\nperforming models for each optimisation algorithm used. We now expand that to\nshow performance results across the best 20% and 30% of models in Figure B.7. We\nobserve a consistent increase in performance obtained by cancelling the interaction\nterms.\nBatch size comparison. We now show that the results which show the efficacy\nof cancelling the interaction terms are resilient to changes in batch size in Figure B.8.\nComparison with Symplectic Gradient Adjustment. We show results\ncomparing with Symplectic Gradient Adjustment (SGA) [61] in Figure B.9 (best\nperforming models) and Figure B.10 (quantiles showing performance across all\nhyperparameters and seeds). We observe that despite having the same functional\nform as SGA, cancelling the interaction terms of DD performs better; this is due to\nthe choice of regularisation coefficients, which in the case of cancelling the interaction\nterms is provided by Corollary 4.5.1.\nComparison with Consensus Optimisation. We show results comparing\nwith Consensus Optimisation (CO) [15] in Figure B.11 (best performing models) and\nFigure B.12 (quantiles showing performance across all hyperparameters and seeds).\nWe observe that cancelling the interaction terms performs best, and that additionally\nB.9. Additional experimental results\n320\nSimultaneous\nAlternating\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nNon saturating loss\n(a) Hyperparameter sensitivity.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nNon saturating loss\nSimultaneous\nAlternating\n(b) Learning curves.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\nInception score\nNon saturating loss - alternating updates\nNum updates\n1.0\n4.0\nLearning rate\n0.001\n0.005\n(c) The effect of multiple alternating updates.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nNon saturating loss\nEuler (sim)\nEuler (alt)\nRK4\n(d) Equal learning rates.\nFigure B.3: The effect of discretisation drift depends on the game: with the non saturating\nloss, the relative performance of different numerical estimators is different\ncompared to the saturating loss, and the effect of explicit regularisation is\nalso vastly different.\nstrengthening the self terms does not provide a performance benefit.\nVariance across seeds. We have mentioned in the main text the challenge\nwith variance across seeds observed when training GANs with SGD, especially in the\ncase of simultaneous updates in zero sum games. We first notice that performance\nof simultaneous updates depends strongly on learning rates, with most models not\nlearning. We also notice variance across seeds, both in vanilla SGD and when using\nexplicit regularisation to cancel interaction terms. In order to investigate this effect,\nwe ran a sweep of 50 seeds for the best learning rates we obtain when cancelling\ninteraction terms in simultaneous updates, namely a discriminator learning rate\nof 0.01 and a generator learning rate of 0.005, we obtain Inception Score results\nwith mean 5.61, but a very large standard deviation of 2.34. Indeed, as shown\nB.9. Additional experimental results\n321\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms\n(a) Inception Score (↑).\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n0\n100\n200\n300\n400\n500\nFrechet Inception Distance\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms\n(b) Fr´echet Inception Distance (↓).\nFigure B.4: Using explicit regularisation to cancel the effect of the interaction components\nof drift leads to a substantial improvement compared to SGD without explicit\nregularisation.\nin Figure B.13, more than 50% of the seeds converge to an IS grater than 7. To\ninvestigate the reason for the variability, we repeat the same experiment, but clip the\ngradient value for each parameter to be in [−0.1, 0.1], and show results in Figure B.14.\nWe notice that another 10% of jobs converge to an IS score grater than 7, and 10%\ndrop in the number of jobs that do not manage to learn. This makes us postulate\nthat the reason for the variability is due to large gradients, perhaps early in training.\nWe contrast this variability across seeds with the consistent performance we obtain by\nlooking at the best performing models across learning rates, where as we have shown\nin the main thesis and throughout the Appendix, we obtain consistent performance\nwhich consistently leads to a substantial improvement compared to SGD without\nexplicit regularisation, and obtains performance comparable with Adam.\nB.9.4\nExplicit regularisation in zero-sum games trained\nusing alternating gradient descent\nWe perform the same experiments as done for simultaneous updates also with\nalternating updates. To do so, we cancel the effect of DD using the same explicit\nregularisation functional form, but updating the coefficients to be those of alternating\nB.9. Additional experimental results\n322\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nAdam\nCancel interaction terms (SGD)\n(a) Inception Score (↑).\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n0\n100\n200\n300\n400\n500\nFrechet Inception Distance\nSimultaneous updates, zero sum\nAdam\nCancel interaction terms (SGD)\n(b) Fr´echet Inception Distance (↓).\nFigure B.5: Using explicit regularisation to cancel the effect of the interaction components\nof drift allows us to obtain the same peak performance as Adam using SGD\nwithout momentum.\nupdates. We show results in Figure B.15, where we see very little difference in the\nresults compared to vanilla SGD, perhaps apart from less instability early on in\ntraining. We postulate that this could be due the effect of DD in alternating updates\ncan can be beneficial, especially for learning rate ratios for which the second player\nalso minimises the gradient norm of the first player. We additionally show results\nobtained from strengthening the self terms in Figure B.16.\nMore percentiles. Throughout the main thesis, we displayed the best 10%\nperforming models for each optimisation algorithm used. We now expand that to\nshow performance results across the 20% and 30% jobs in Figure B.17. We observe\na consistent increase in performance obtained by cancelling the interaction terms.\nB.9. Additional experimental results\n323\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms (A)\nStrengthen self terms (B)\nA + B\n(a) Inception Score (↑).\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n0\n100\n200\n300\n400\n500\nFrechet Inception Distance\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms (A)\nStrengthen self terms (B)\nA + B\n(b) Fr´echet Inception Distance (↓).\nFigure B.6: Using explicit regularisation to cancel the effect of the drift leads to a\nsubstantial improvement compared to SGD without explicit regularisation.\nStrengthening the self terms—the terms which minimise the player’s own\nnorm—does not lead to a substantial improvement; this is somewhat expected\nsince while the modified flows give us the exact coefficients required to cancel\nthe drift, they do not tell us how to strengthen it, and our choice of exact\ncoefficients from the drift might not be optimal.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms\n(a) Top 10% models.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\n(b) Top 20% models.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\n(c) Top 30% models.\nFigure B.7: Performance across the top performing models for vanilla SGD and with\ncancelling the interaction terms.\nAcross all percentages, cancelling the\ninteraction terms improves performance.\nB.9. Additional experimental results\n324\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nSGD\nCancel interaction terms\n(a) Batch size 64.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\n(b) Batch size 128.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\n(c) Batch size 256.\nFigure B.8: Performance when changing the batch size. We consistently see that can-\ncelling te interaction terms improves performance.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\nCancel interaction terms\nSGD\nSGA Coeffs 0.0001\nSGA Coeffs 0.001\nSGA Coeffs 0.01\n(a) Inception Score (↑).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n0\n100\n200\n300\n400\n500\nFrechet Inception Distance\nSimultaneous updates, zero sum\nCancel interaction terms\nSGD\nSGA Coeffs 0.0001\nSGA Coeffs 0.001\nSGA Coeffs 0.01\n(b) Fr´echet Inception Distance (↓).\nFigure B.9: Comparison with Symplectic Gradient Adjustment (SGA). Cancelling the\ninteraction terms results in similar performance, but with less variance.\nThe performance of SGA heavily depends on the strength of regularisation,\nadding another hyperparmeter to the hyperparameter sweep, while cancelling\nthe interaction terms of the drift requires no other hyperparameters, since\nthe explicit regularisation coefficients strictly depend on learning rates.\nB.9. Additional experimental results\n325\nCancel interaction terms\nSGD\nSGA Coeffs 0.0001\nSGA Coeffs 0.001\nSGA Coeffs 0.01\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nFigure B.10: Comparison with Symplectic Gradient Adjustment (SGA), obtained from\nall models in the sweep. Without requiring an additional sweep over the\nregularisation coefficient, cancelling the interaction terms results in better\nperformance across the learning rate sweep and less sensitivity to hyperpa-\nrameters.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nCancel interaction terms\nSGD\nCO Coeffs 0.01\nCO Coeffs 0.001\nCO Coeffs 0.0001\nDD coeffs\n(a) Inception Score (↑).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n0\n100\n200\n300\n400\nFrechet Inception Distance\nSimultaneous updates, zero sum\nCancel interaction terms\nSGD\nCO Coeffs 0.01\nCO Coeffs 0.001\nCO Coeffs 0.0001\nDD coeffs\n(b) Fr´echet Inception Distance (↓).\nFigure B.11: Comparison with Consensus Optimisation (CO). Despite not requiring addi-\ntional hyperparameters compared to the standard SGD learning rate sweep,\ncancelling the interaction terms of the drift performs better than consensus\noptimisation. Using consensus optimisation with a fixed coefficient can\nperform better than using the drift coefficients when we use them to the\nstrengthen the norms – this is somewhat expected since while the modified\nflows give us the exact coefficients required to cancel the drift, they do not\ntell us how to strengthen it, and our choice of exact coefficients from the\ndrift might not be optimal.\nB.9. Additional experimental results\n326\nCancel interaction terms\nSGD\nCO Coeffs 0.01\nCO Coeffs 0.001\nCO Coeffs 0.0001\nDD coeffs\n1\n2\n3\n4\n5\n6\n7\n8\nInception score\nZero sum\nFigure B.12: Comparison with Consensus Optimisation (CO), obtained from all models\nin the sweep. Without requiring an additional sweep over the regularisation\ncoefficient, cancelling the interaction terms results in better performance\nacross the learning rate sweep and less sensitivity to hyperparameters.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\n Cancel interaction terms, 50 seeds\n(a) Learning curves.\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPerformance distribution across seeds\n(b) Performance Histogram.\nFigure B.13: Variability across seeds for the best performing hyperparameters, when\ncancelling interaction terms in simultaneous updates for the original GAN,\nwith a zero sum loss.\nB.9. Additional experimental results\n327\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nSimultaneous updates, zero sum\n Cancel interaction terms, 50 seeds\n(a) Learning curves.\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Performance distribution across seeds with clip 0.1\n(b) Performance Histogram.\nFigure B.14: With gradient clipping. Gradient clipping can reduce variability. This\nsuggests that the instabilities observed in gradient descent are caused by\nlarge gradient updates.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nSGD\nCancel interaction terms\n(a) Inception Score (↑).\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n0\n100\n200\n300\n400\n500\nFrechet Inception Distance\nAlternating updates, zero sum\nSGD\nCancel interaction terms\n(b) Fr´echet Inception Distance (↓).\nFigure B.15: In alternating updates, using explicit regularisation to cancel the effect of the\ninteraction components of drift does not substantially improve performance\ncompared to SGD, but can reduce variance. This is expected, given that the\ninteraction terms for the second player in the case of alternating updates\ncan have a beneficial regularisation effect.\nB.9. Additional experimental results\n328\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nSGD\nCancel interaction terms (A)\nStrengthen self terms (B)\nA + B\n(a) Inception Score (↑).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nIteration\n1e5\n0\n100\n200\n300\n400\n500\nFrechet Inception Distance\nAlternating updates, zero sum\nSGD\nCancel interaction terms (A)\nStrengthen self terms (B)\nA + B\n(b) Fr´echet Inception Distance (↓).\nFigure B.16: In alternating updates, using explicit regularisation to cancel the effect of the\ninteraction components of drift does not substantially improve performance\ncompared to SGD, but can reduce variance. This is expected, given that the\ninteraction terms for the second player in the case of alternating updates\ncan have a beneficial regularisation effect. Strengthening the self terms—the\nterms which minimise the player’s own norm—does not lead to a substantial\nimprovement; this is somewhat expected since while the modified flows\ngive us the exact coefficients required to cancel the drift, they do not tell\nus how to strengthen it, and our choice of exact coefficients from the drift\nmight not be optimal.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\nSGD\nCancel interaction terms\nCancel disc interaction term\n(a) Top 10% models.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\n(b) Top 20% models.\n0\n1\n2\n3\n4\n5\n6\nIteration\n1e5\n1\n2\n3\n4\n5\n6\n7\n8\nInception Score\nAlternating updates, zero sum\n(c) Top 30% models.\nFigure B.17: Performance across the top performing models for vanilla SGD and with\ncancelling the interaction terms and only cancelling the discriminator inter-\naction terms. We notice that cancelling only the discriminator interaction\nterms can result in higher performance across more models, and that the\ninteraction term of the generator can play a positive role, likely due to the\nsmaller strength of the generator interaction term compared to simultaneous\nupdates.\nB.10. Individual figure reproduction details\n329\nB.10\nIndividual figure reproduction details\n• Figure 4.4 performs MNIST classification results with alternating updates. We\nuse an MLP with layers of size [100, 100, 100, 10] and a learning rate of 0.08.\nThe batch size used is 50 and models are trained for 1000 iterations. Error\nbars are obtained from 5 different seeds.\n• Figure 4.5 uses a learning rate of 0.01 for both the discriminator and the\ngenerator. The continuous-time simulation is obtained using Runge–Kutta4.\n• Figure 4.6 is obtained using sweep over {0.01, 0.005, 0.001, 0.0005} for the\ndiscriminator and for the generator learning rates.\nWe restrict the ratio\nbetween the two learning rates to be in the interval [0.1, 10] to ensure the\nvalidity of our approximations. The architecture is Spectral Normalised GAN.\nBatch size of 128. When comparing with Runge–Kutta 4, we no longer perform\na cross product over discriminator and generator learning rates, but instead\nrestrict ourselves to equal learning rates.\n• Figure 4.7 controls for the number of experiments which have the same learning\nrate ratio. To do so, we obtain 5 learning rates uniformly sampled from the\ninterval [0.001, 0.01] which we use for the discriminator, we fix the learning\nrate ratios to be in {0.1, 0.2, 0.5, 1., 2., 5.} and we obtain the generator learning\nrate from the discriminator learning rate and the learning rate ratio. Batch\nsize of 128.\n• Figure 4.8, B.4, B.5, B.6, B.7, B.13, and B.14 use the unbiased estimator\ndescribed in Section B.8.1 for explicit regularisation. SGD results are obtained\nfrom a sweep of {0.01, 0.005, 0.001, 0.0005} for the discriminator and for the\ngenerator learning rates. We restrict the ratio between the two learning rates\nto be in the interval [0.1, 10] to ensure the validity of our approximations. For\nAdam, we use the learning rate sweep {10−4, 2 × 10−4, 3 × 10−4, 4 × 10−4} for\nthe discriminator and the same for the generator, with β1 = 0.5 and β2 = 0.99.\nAll models use simultaneous updates and batch size of 128.\nB.10. Individual figure reproduction details\n330\n• Figure 4.9, B.11, B.12, B.9, B.10 use the unbiased estimator described in\nSection B.8.1 for explicit regularisation. Results are obtained from a sweep of\n{0.01, 0.005, 0.001, 0.0005} for the discriminator and for the generator learning\nrates. We restrict the ratio between the two learning rates to be in the interval\n[0.1, 10] to ensure the validity of our approximations. The SGA and CO explicit\nregularisation coefficients are taken from a sweep over {0.01, 0.001, 0.0001}. All\nmodels use simultaneous updates and batch size of 128.\n• Figures 4.10, B.15 B.16, B.17 use the same experimental setup as Figure 4.8,\nwith the exception of the update type: here we use alternating updates.\n• Figure 4.11 uses the same experimental setting as Figure 4.6, but the non-\nsaturating GAN loss is used.\n• Figure B.1 uses the same experimental setting as Figure 4.6.\n• Figure B.2 uses a least square loss. Figure B.2a uses the same experimental\nsetting as Figure 4.6, while Figure B.2b uses the same experimental setting as\nFigure 4.7.\n• Figure B.3 uses the same experimental setting as Figure B.1, but uses the\nnon-saturating generator loss.\n• Figure B.7 shows results from the same experiment Figure 4.8.\n• Figure B.8 uses the same experimental setting as Figure 4.8, but instead of\nusing a fixed batch size of 128, performs a sweep over batch sizes.\nAppendix C\nFinding new implicit regularisers\nby revisiting backward error\nanalysis\nWe now provide the proofs required to write modified losses for stochastic gradient\ndescent in supervised learning discussed in Chapter 5. We denote by θ the parameters\nof the model and by f the update function. Since we are now interested in the\nstochastic setting, we write f as an argument both of data and parameters: f(θ; x)\ndenotes the application of f at input x using parameters θ. We often consider\nf = −∇θE(θ; x), where E is the loss function. We will denote as\nE(θ; Xt) = 1\nB\nB\nX\ni=1\nE(θ; xt\ni)\n(C.1)\nand\nf(θ; Xt) = −1\nB\nB\nX\ni=1\n∇θE(θ; xt\ni)\n(C.2)\nf(θ; {Xt, ...Xt+n−1}) = 1\nn\nn−1\nX\ni=0\nf(θ; {Xt+i})\n(C.3)\nas averages over batches for convenience and clarity.\nOur goal is to find ˙θ such that the distance between n steps of stochastic gradient\ndescent (SGD) with learning rate h and ˙θ is of order O(h3). We take the same\nC.1. Two consecutive steps of SGD\n332\napproach as in the other BEA proofs:\n• We expand the n discrete SGD updates up to O(h3).\n• We expand change of the modified continuous updates of the form ˙θ = f + hf1\nin time nh, where f is the gradient function obtained using the concatenation\nof all batches used in the first step.\n• We match the terms between the discrete and continuous updates of O(h2) to\nfind f1.\nC.1\nTwo consecutive steps of SGD\nStep 1: Expand the discrete updates.\nFrom the definition of SGD:\nθt = θt−1 + hf(θt−1; Xt)\n(C.4)\nθt+1 = θt + hf(θt; Xt+1)\n(C.5)\nWe expand the gradient descent steps and obtain:\nθt+1 = θt + hf(θt; Xt+1)\n(C.6)\n= θt−1 + hf(θt−1; Xt) + hf(θt; Xt+1)\n(From (C.4), C.7)\n= θt−1 + hf(θt−1; Xt) + hf(θt−1 + hf(θt−1; Xt); Xt+1)\n(From (C.5), C.8)\n= θt−1 + hf(θt−1; Xt) + hf(θt−1; Xt+1)\n(C.9)\n+ h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt) + O(h3)\n(Taylor expansion, C.10)\n= θt−1 + 2h\n\u00121\n2f(θt−1; Xt) + 1\n2f(θt−1; Xt+1)\n\u0013\n(Grouping f terms, C.11)\n+ h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt) + O(h3)\n(C.12)\n= θt−1 + 2h f(θt−1; {Xt, Xt+1})\n|\n{z\n}\nupdate with a batch obtained by\nconcatenating the two batches\n(Eq (C.3), C.13)\n+ h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt) + O(h3)\n(C.14)\nC.1. Two consecutive steps of SGD\n333\nStep 2: Taylor expansion of modified flow\nWe now expand what happens in a time of 2h in continuous-time, by using the form\nof ˙θ given by BEA\n˙θ = f(θ; {Xt, Xt+1}) + 2hf1(θ; {Xt, Xt+1})\n(C.15)\nthus for any τ\nθ(τ + 2h) = θ(τ) + 2h ˙θ(τ) + 4h2 ¨θ(τ) + O(h3)\n(C.16)\n= θ + 2hf(θ; {Xt, Xt+1})\n(C.17)\n+ 4h2\n\u0014\nf1 + 1\n2\ndf(θ; {Xt, Xt+1})\ndθ\nf(θ; {Xt, Xt+1})\n\u0015\n+ O(h3). (C.18)\nStep 3: Matching terms of the second order\nFrom the above expansion of the discrete updates (Step 1) and of the continuous\nupdates (Step 2) we can find the value of f1 at the last initial parameters, θt−1:\n4h2\n\u0014\nf1 + 1\n2\ndf(θ; {Xt, Xt+1})\ndθ\nf(θ; {Xt, Xt+1})\n\u0015\n= h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(C.19)\nThis leads to:\nf1(θt−1) = −1\n2\ndf(·; {Xt, Xt+1})\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; {Xt, Xt+1})\n|\n{z\n}\nthe IGR full-batch drift term\n(From flow: (C.18), C.20)\n+ 1\n4\ndf(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(From SGD: (C.14), C.21)\nFrom here we choose a function f1 which satisfies the above constraint\nf1(θ) = −1\n2\ndf(θ; {Xt, Xt+1})\ndθ\nf(θ; {Xt, Xt+1})\n|\n{z\n}\nthe IGR full-batch drift term\n(C.22)\n+ 1\n4\ndf(θ; Xt+1)\ndθ\nf(θt−1; Xt).\n(C.23)\nC.2. Multiple steps of SGD\n334\nWe have now found the modified flow ˙θ, which by construction follows two Euler\nupdates with an error of O(h3). Note the use of the initial parameters (highlighted in\nred) in the flow’s vector field; this choice is required to ensure we can write a modified\nloss function in the single objective optimisation setting by using f = −∇θE:\nf1(θ) = −1\n2\nd∇θE(θ; {Xt, Xt+1})\ndθ\n∇θE(θ; {Xt, Xt+1})\n(C.24)\n+ 1\n4\nd∇θE(θ; Xt+1)\ndθ\n∇θE(θt−1; Xt)\n(C.25)\n= −∇θ\n\u00121\n4∥∇θE(θ; {Xt, Xt+1})∥2 −1\n4∇θE(θ; Xt+1)T∇θE(θt−1; Xt)\n\u0013\n,\n(using (B.96), C.26)\nwhere E(θ; {Xt, Xt+1}) = 1\n2 (E(θ; Xt) + E(θ; Xt+1)). We can now write\n˙θ = −∇θ\n \nE(θ; {Xt, Xt+1})\n(C.27)\n+ 2h\n\u00121\n4∥∇θE(θ; {Xt, Xt+1})∥2 −1\n4∇θE(θ; Xt+1)T∇θE(θt−1; Xt)\n\u0013 !\n,\n(C.28)\nand since the vector field is a negative gradient, this leads to the modified loss\n˜E =E(θ; {Xt, Xt+1})\n(C.29)\n+ 2h\n\u00121\n4∥∇θE(θ; {Xt, Xt+1})∥2 −1\n4∇θE(θ; Xt+1)T∇θE(θt−1; Xt)\n\u0013\n. (C.30)\nC.2\nMultiple steps of SGD\nWe will now derive a similar result, for n SGD steps.\nStep 1: Expand discrete updates\nθt = θt−1 + hf(θt−1; Xt)\n(C.31)\n. . .\nθt+n−1 = θt+n−2 + hf(θt+n−2; Xt+n−1)\n(C.32)\nC.2. Multiple steps of SGD\n335\nFrom Eq (C.14), we know that if we expand two steps we obtain\nθt+1 = θt−1 + 2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt) + O(h3).\n(C.33)\nThen, by expanding the third step\nθt+2 = θt+1 + hf(θt+1; Xt+2)\n(C.34)\n= θt−1 + 2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(Eq (C.33), C.35)\n+ hf(θt+1; Xt+2) + O(h3)\n(C.36)\n= θt−1 + 2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(C.37)\n+ hf\n \nθt−1 + 2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt); Xt+2\n!\n(Eq (C.33), C.38)\n+ O(h3)\n(C.39)\n= θt−1 + 2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(C.40)\n+ hf(θt−1; Xt+2)\n(C.41)\n+ hdf(·; Xt+2)\ndθ\n\f\f\f\f\nθt−1\n \n2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n!\n(Taylor expansion, C.42)\n+ O(h3)\n(C.43)\n= θt−1 + 2hf(θt−1; {Xt, Xt+1}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(C.44)\n+ hf(θt−1; Xt+2)\n(C.45)\n+ 2h2df(·; Xt+2)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; {Xt, Xt+1})\n(Simplifying h3 term, C.46)\n+ O(h3)\n(C.47)\nC.2. Multiple steps of SGD\n336\nθt+2 = θt−1 + 3hf(θt−1; {Xt, Xt+1, Xt+2})\n(Grouping f terms, C.48)\n+ h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(C.49)\n+ 2h2df(·; Xt+2)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; {Xt, Xt+1}) + O(h3)\n(C.50)\n= θt−1 + 3hf(θt−1; {Xt, Xt+1, Xt+2}) + h2df(·; Xt+1)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt)\n(C.51)\n+ h2df(·; Xt+2)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt) + h2df(·; Xt+2)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt+1)\n(Eq (C.3), C.52)\n+ O(h3).\n(C.53)\nThus by induction we get\nθt+n−1 = θt−1 + nhf(θt−1; {Xt, . . . , Xt+n−1})\n(C.54)\n+ h2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\ndf(·; Xt+µ)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt+τ) + O(h3).\n(C.55)\nStep 2: Taylor expansion of modified flow\nθ(τ + nh) = θ + nhf(θ; {Xt, . . . , Xt+n−1})\n(C.56)\n+ n2h2\n\u0014\nf1 + 1\n2\ndf(θ; {Xt, . . . , Xt+n−1})\ndθ\nf(θ; {Xt, . . . , Xt+n−1})\n\u0015\n(C.57)\n+ O(h3)\n(C.58)\nStep 3: Matching terms\nFrom the above expansion of the discrete updates (Step 1) and of the continuous\nupdates (Step 2) we can find the value of f1 at θt−1 by matching the terms of order\nC.2. Multiple steps of SGD\n337\nO(h2):\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\ndf(·; Xt+µ)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt+τ)\n(SGD: Eq (C.55), C.59)\n= n2\n\u0014\nf1 + 1\n2\ndf(θ; {Xt, . . . , Xt+n−1})\ndθ\nf(θ; {Xt, . . . , Xt+n−1})\n\u0015\n(Flow: Eq (C.58), C.60)\nLeading to\nf1(θt−1) = −1\n2\ndf(·; {Xt, . . . , Xt+n−1})\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; {Xt, . . . , Xt+n−1})\n|\n{z\n}\nthe usual drift term\n(C.61)\n+ 1\nn2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\ndf(·; Xt+µ)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt+τ).\n(C.62)\nFrom here we choose a function f1 which satisfies the above constraint.\nf1(θ) = −1\n2\ndf(θ; {Xt, . . . , Xt+n−1})\ndθ\nf(θ; {Xt, . . . , Xt+n−1})\n|\n{z\n}\nthe usual drift term\n(C.63)\n+ 1\nn2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\ndf(θ; Xt+µ)\ndθ\nf(θt−1; Xt+τ)\n(C.64)\nWe now replace f = −∇θE, using that in this case df\ndθ = ∇2\nθE,\nf1(θt−1) = −1\n2\nd∇θE(θ; {Xt, . . . , Xt+n−1})\ndθ\n∇θE(θ; {Xt, . . . , Xt+n−1})\n(C.65)\n+ 1\nn2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\nd∇θE(θ; Xt+µ)\ndθ\n∇θE(θt−1; Xt+τ)\n(C.66)\n= −∇θ\n\u00101\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n(using (B.96), C.67)\n−1\nn2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\n∇θE(θ; Xt+µ)T∇θE(θt−1; Xt+τ)\n\u0011\n.\n(C.68)\nC.3. Multiple steps of full-batch gradient descent\n338\nThis leads to the modified loss\n˜E(θ) = E(θ; {Xt, . . . , Xt+n−1})\n(C.69)\n+ nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n(C.70)\n−h\nn\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\n∇θE(θ; Xt+µ)T∇θE(θt−1; Xt+τ).\n(C.71)\nThrough algebraic manipulation we can write the above in the form\n˜E(θ) = E(θ; {Xt, . . . , Xt+n−1})\n(C.72)\n+ nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n(C.73)\n−h\nn\nn−1\nX\nµ=1\n\"\n∇θE(θ; Xt+µ)T\n µ−1\nX\nτ=0\n∇θE(θt−1; Xt+τ)\n!#\n.\n(C.74)\nWe can now compare this with the modified loss we obtained by ignoring\nstochasticity and assuming both updates have been done with a full batch; this\nentails using the IGR loss:\n˜E = E(θ; {Xt, . . . , Xt+n−1}) + h\n4∥∇θE(θ; {Xt, . . . , Xt+n−1})∥2\n(C.75)\nThus, when using multiple batches, there is the additional pressure to maximise\nthe dot product of the gradients obtained using the last k batches that came before\nthe current batch, evaluated at the parameters at which we started the n iterations.\nC.3\nMultiple steps of full-batch gradient descent\nTo contrast our results with multiple steps of gradient descent with the same batch\n(full-batch case), we show that the IGR flow follows gradient descent with error of\norder O(h3) after n gradient descent steps. The proof steps follow the same approach\nas above, with two main differences: first, we assume all batches are the same, and\nsecond, we no longer fix the starting parameters when choosing f1. With the same\nC.3. Multiple steps of full-batch gradient descent\n339\nsteps as before we obtain Eq (C.62):\nf1(θt−1) = −1\n2\ndf(·; {Xt, . . . , Xt+n−1})\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; {Xt, . . . , Xt+n−1})\n(C.76)\n+ 1\nn2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\ndf(·; Xt+µ)\ndθ\n\f\f\f\f\nθt−1\nf(θt−1; Xt+τ).\n(C.77)\nAssuming identical batches and using that f is an empirical average, we obtain\nf(·; {Xt, . . . , Xt+n−1}) = f(·; Xt) = f(·; Xt+i), for any choice of i. We then have\nf1(θ) = −1\n2\ndf(θ, Xt)\ndθ\nf(θ; Xt) + 1\nn2\nn−1\nX\nτ=0\nn−1\nX\nµ=τ+1\ndf(θ, Xt)\ndθ\nf(θ; Xt) =\n(C.78)\n= (−1\n2 + n(n −1)\n2n2\n)df(θ, Xt)\ndθ\nf(θ; Xt)\n(C.79)\n= −1\n2n\ndf(θ, Xt)\ndθ\nf(θ; Xt)\n(C.80)\nReplacing f = −∇θE(·; Xt) into the form of the modified flow in Eq C.58, we obtain:\n˙θ = −∇θE(θ; Xt) −nh\n2n∇2\nθE(θ, Xt)∇θE(θ; Xt)\n(C.81)\n= −∇θE(θ; Xt) −h\n2∇2\nθE(θ, Xt)∇θE(θ; Xt),\n(C.82)\nwhich recovers the IGR flow. Importantly, the flow in the full-batch case does not\ndepend on the number of iterations n.\nC.3.1\nExpectation over all shufflings\nWe contrast our results with those of Smith et al. [13], who construct a modified\nloss over an epoch of stochastic gradient descent in expectation over all possible\nshufflings of the batches in the epoch. That is, Smith et al. [13] describe expected\nvalue of the modified loss Eσ\nh\n˜E(θ; {Xσ(t), . . . , Xσ(t+n−1)})\ni\n, where n is the number\nof batches in an epoch, and t is the iteration at which the new epoch start, while σ\ndenotes the set of all possible permutations of batches {1, ...n}. While our results\ndo not require an expectation and account for the exact batches used, to constrast\nour results with Smith et al. [13], we also take an expectation over all possible batch\nC.3. Multiple steps of full-batch gradient descent\n340\nshufflings (but not the elements in the batch) in an epoch in our results shown in\nEq (5.8), and obtain\nEσ [Esgd(θ)] = E(θ; {Xt, . . . , Xt+n−1}) + nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2 (C.83)\n−h\nnEσ\n\"n−1\nX\nk=0\n∇θE(θ; Xt+k)T\n k−1\nX\ni=0\n∇θE(θt−1; Xt+i)\n!#\n(C.84)\n= E(θ; {Xt, . . . , Xt+n−1}) + nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2 (C.85)\n−h\nn\n1\n2Eσ\n\"n−1\nX\nk=0\nn−1\nX\ni=0,i̸=k\n∇θE(θ; Xt+k)T∇θE(θt−1; Xt+i)\n#\n,\n(C.86)\nWe used the symmetry of the permutation structure since for each permutation\nwhere σ(i) < σ(j) there is also a permutation where σ(i) > σ(j) by swapping the\nvalues of σ(i) and σ(j). We expand the last term\nEσ [Esgd] = E(θ; {Xt, . . . , Xt+n−1}) + nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n(C.87)\n−h\nn\n1\n2Eσ\n\"n−1\nX\nk=0\nn−1\nX\ni=0\n∇θE(θ; Xt+k)T∇θE(θt−1; Xt+i)\n#\n(C.88)\n−h\nn\n1\n2Eσ\n\"n−1\nX\nk=0\n∇θE(θ; Xt+k)T∇θE(θt−1; Xt+k)\n#\n(C.89)\nFrom here\nEσ [Esgd] = E(θ; {Xt, . . . , Xt+n−1}) + nh\n4\n\r\r∇θE(θ; {Xt, . . . , Xt+n−1})\n\r\r2\n(C.90)\n−h\n2n∇θE(θ; {Xt, . . . , Xt+n−1})T∇θE(θt−1; {Xt, . . . , Xt+n−1}\n(C.91)\n−h\n2n\n\"n−1\nX\nk=0\n∇θE(θ; Xt+k)T∇θE(θt−1; Xt+k)\n#\n.\n(C.92)\nWe obtain that the pressure to minimise individual batch gradient norms is translated\ninto a pressure to maximise the dot product between the gradients at the end of\nthe epoch with those at the beginning of the epoch, both for each batch and for the\nentire dataset.\nC.4. Two-player games\n341\nC.4\nTwo-player games\nC.4.1\nEffects on the non-saturating GAN\nWe now expand of implicit regularisation terms found in Section 5.3 on GANs,\nspecifically, the effect of the interaction terms, which takes the form of a dot product.\nWe will denote the first player, the discriminator, as D, parametetrised by ϕ, and\nthe generator as G, parametrised by θ. We denote the data distribution as p∗(x)\nand the latent distribution p(z). Consider the non-saturating GAN loss described by\nGoodfellow et al. [32]:\nEϕ(ϕ, θ) = Ep∗(x) log D(x; ϕ) + Ep(z) log(1 −D(G(z; θ); ϕ))\n(C.93)\nEθ(ϕ, θ) = Ep(z) −log D(G(z; θ); ϕ)\n(C.94)\nConsider the interaction term for the discriminator (see Eq (5.33)), and replace\nthe definitions of the above loss functions:\n∇θET\nϕ∇θEθ(ϕt−1, θt−1)\n(C.95)\n= ∇θ\n\u0000Ep∗(x) log D(x; ϕ) + Ep(z) log(1 −D(G(z; θ); ϕ)\n\u0001T ∇θEθ(ϕt−1, θt−1)\n(C.96)\n= ∇θ\n\u0000Ep(z) log(1 −D(G(z; θ); ϕ)\n\u0001T ∇θEθ(ϕt−1, θt−1)\n(C.97)\n=\n\u0000Ep(z)∇θ log(1 −D(G(z; θ); ϕ)\n\u0001T ∇θEθ(ϕt−1, θt−1)\n(C.98)\n=\n\u0012\n−Ep(z)\n1\n1 −D(G(z; θ); ϕ)∇θD(G(z; θ); ϕ)\n\u0013T\n∇θEθ(ϕt−1, θt−1)\n(C.99)\n=\n\u0012\n−Ep(z)\n1\n1 −D(G(z; θ); ϕ)∇θD(G(z; θ); ϕ)\n\u0013T\n(C.100)\n\u0012\n−Ep(z)\n1\nD(G(z; θt−1); ϕt−1)∇θD(G(z; θt−1); ϕt−1)\n\u0013\n(C.101)\n=\n\u0012\nEp(z)\n1\n1 −D(G(z; θ); ϕ)∇θD(G(z; θ); ϕ)\n\u0013T\n(C.102)\n\u0012\nEp(z)\n1\nD(G(z; θt−1); ϕt−1)∇θD(G(z; θt−1); ϕt−1)\n\u0013\n(C.103)\nwhere the expectations can be evaluated at the respective mini-batches used in the\nC.4. Two-player games\n342\nupdates for iterations t and t −1, respectively. Consider zi\nt the latent variable with\nindex i in the batch at time t. Then the above is approximated as\n1\nB2\nB\nX\ni,j=1\ncnon−sat\ni,j\n∇θD(G(zi\nt; θ); ϕ)T∇θD(G(zj\nt−1; θt−1); ϕt−1),\nwith\n(C.104)\ncnon−sat\ni,j\n=\n1\n1 −D(G(zi\nt; θ); ϕ)\n1\nD(G(zj\nt−1; θt−1); ϕt−1)\n.\n(C.105)\nAppendix D\nThe importance of model\nsmoothness in deep learning\nD.1\nIndividual figure reproduction details\n• Figure 6.1: the U-shape curved is obtained by fitting polynomials of increased\ndegree on a simple one dimensional regression problem, with the true underlying\nfunction f(x) = sin(5x) −sin(10x). The double descent curve is obtained by\ntraining Resnet-18 models on CIFAR-10.\n• Figure 6.2 uses samples from two Beta distributions, one with parameters 3\nand 5, and one with parameters 2 and 3. The MLP approximation to the\ndensity ratio is obtained with an MLP with units [10, 10, 40, 1]. The Wasserstein\noptimal critic is approximated using a linear programming approach [328].\n• Figures 6.3, 6.4, 6.5 use the an MLP with 4 layers and 100, 100, 100 and 1\noutput unit, respectively. The shallow MLP has 2 layers of 100 and 1 unit.\nAll methods were trained for 100 iterations on 50 data points. We use the\nAdam optimiser, with learning rate 0.01, β1 = 0.9, β2 = 0.999. To compute\nthe local Lipschitz function of the decision surface learned on two moons in\nFigure 6.5, we split the space into small neighbourhoods (2500 equally sized\ngrids); for each grid, we sample 2500 random pairs of points in the grid and\nreport max ∥f(x) −f(y)∥/ ∥x −y∥.\n• Figure 6.6 is obtained using an MLP with 4 layers of 1000, 1000, 1000 and\nD.1. Individual figure reproduction details\n344\n10 units each and are trained for 500 iterations at batch size 100, reaching an\naccuracy of 95% on the entire test set. The models are trained with the Adam\noptimiser, with β1 = 0.9 β2 = 0.999, and learning rates 0.001 and 0.0005.\n• Figure 6.7 For the GAN CIFAR-10 experiments, we use the architectures spec-\nified in the Spectral normalization paper [30]. We use the Adam optimiser [47]\nwith default β2, β1 as specified in the respective subfigure and a learning rate\nsweep {10−4, 2 × 10−4, 3 × 10−4, 4 × 10−4} for the discriminator and the same\nfor the generator.\nAppendix E\nSpectral Normalisation in\nReinforcement learning\nE.1\nAdditional experimental results\nE.1.1\nThe weak correlation between smoothness and\nperformance\nIn Section 7.3.3 we have made the argument that smoothness and performance\nare not strongly correlated. We visulise the individual results used to obtain the\ncorrelation values shown in the main thesis in Figure E.1.\nE.1.2\nOther regularisation methods\nWe investigated whether other regularisation methods imposing smoothness con-\nstraints can have similar effects on the agent’s performance. To this end we ran\nexperiments with both gradient penalties and Batch Normalisation on several archi-\ntectures (Table E.6). We consistently observe that these approaches do not recover\nthe performance of Spectral Normalisation.\nBatch Normalisation. For each of the architectures we employed Batch Normali-\nsation after the ReLU activation of every convolutional or linear layer except the\noutput.\nGradient Penalty. We did extensive experimentation with gradient penalty regu-\nlairsation. We tried multiple forms of regularisation, including penalising the norm of\nthe sum of the gradients of all actions\n\f\f\f\n\f\f\f\nP|A|\ni=1\ndQi\nds\n\f\f\f\n\f\f\f\n2, regularising the expected norm\nE.1. Additional experimental results\n346\nmaxs,i || d Qi(s)/d s||\n(a) Average over seeds.\nmaxs,i ||d Qi(s)/d s||\n(b) Individual seeds.\nFigure E.1: Applying normalisation does not always result in smoother networks. Even\nif normalising a subset of the network’s layers makes the network less smooth\nthan the baseline while performance still improves. Results averaged over\n10 seeds in (a); individual seeds shown in (b). Results are obtained from a\nsweep over architectures detailed in Table E.6.\nof each Q-value with respect to the state P|A|\ni=1\n\f\f\f\f dQi\nds\n\f\f\f\f\n2, and also regularising the\nnorm of the gradient of the Q-value associated with the optimal action\n\f\f\f\f d max Qi\nds\n\f\f\f\f\n2.\nIn all cases we swept through a wide range of penalty coefficients ζ. In Figure E.2\nwe report the results of the best setting we could identify.\nRelaxations to 1-Lipschitz normalisation. We now investigate whether relaxing\nthe 1-Lipschitz constraint imposed by Spectral Normalisation to a K-Lipschitz\nconstraint allows us to normalise more layers without a decrese in performance.\nWe have seen in Chapter 6 that this can occur on simple classification tasks; the\nneed for relaxing the Lipschitz constraint has also been observed by Gouk et al.\n[261]. Figure E.3 shows that K > 1 values Spectral Normalisation can be applied to\nmultiple layers, and it further leads to an increase in performance. As expected, we\nalso observe that for rather large K (such as K = 10), Spectral Normalisation no\nlonger provides a benefit. The increased computation required when approximating\nthe spectral norm for all the layers and the addition of one hyperparameter per layer\ndetermined us to not pursue this setup further.\nE.1. Additional experimental results\n347\nFigure E.2: Regularisation does not recover Spectral Normalisation perfor-\nmance. Performance on MinAtar games of Spectral Normalisation, gradient\npenalties and Batch Normalisation. Each line is an average over normalized\nscores of each game. Ten seeds for each configuration.\nFigure E.3: A sweep over Lipschitz constants K when normalising all but the last layer\nof the critic. We see that while for K = 1, normalising 3 layers drastically\nreduces performance, this is not the case for K > 1. For large K, however,\nwe also do not observe any performance improvement. Each line is an average\nover normalised scores of 4 games × 10 seeds.\nE.1.3\ndivOut, divGrad and mulEps\nWe present results comparing divOut, mulEps and divGrad across multiple\nnormalised layers and architectures in Figure E.5. divOut has a close behaviour to\nthat of Spectral Normalisation. In contrast, the mulEps and divGrad optimisers\noutperform Spectral Normalisation when all hidden layers are normalised.\nE.1. Additional experimental results\n348\nFigure E.4: All spectral radii for the 15M experiment on MinAtar using a 4-layer ar-\nchitecture (conv=24-24,fc=128). Colors code the subsets of layers that are\nnormalised (consistent with the rest of the document), while line styles code\nthe four layers. Note how the penultimate layer has the largest spectral norm\nacross all normalisation variants. 10 seeds.\nE.1. Additional experimental results\n349\nFigure E.5: MinAtar Normalised Scores for the four architectures in Table E.6 and various\nsubsets of layers whose spectral radii are used for Spectral Normalisation or\noptimisation methods we derived based on Spectral Normalisation. Notice\nthat divOut behaves similarly to Spectral Normalisation (even when they\nfail to train), while mulEps and divGrad converge even when all hidden\nlayers are normalised.\nE.2. Experimental details\n350\nE.2\nExperimental details\nE.2.1\nApplication of Spectral Normalisation\nWe use power iteration to approximate the spectral norm of weight matrices, as\nsuggested by Miyato et al. [30]. For convolutional layers we adapt the procedure\nin Gouk et al. [261] and the two matrix-vector multiplications are replaced by\nconvolutional and transposed convolutional operations.\nBackpropagating through the norm. Since the parameters tuned during opti-\nmisation are the unnormalised weights Wi, we investigated whether there are any\nadvantages in backpropagating through the power iteration step. In a set of experi-\nments performed on a subset of games of Atari we noticed no loss in performance\nwhen treating the spectral norm as a constant from the backpropagation perspective,\nand thus for computational efficiency we adopt this approach.\nE.2.2\nMeasuring smoothness: the norm of the Jacobians\nExperiments in Section E.1.1 used the maximum norm of the Jacobians w.r.t. the\ninputs as an indirect metric for network function’s smoothness. This is to avoid\nusing potential loose bounds discussed in Chapter 6, since the exact computation of\nthe Lipschitz constant of a network is NP-hard [299]. For each network we collected\nthousands of states (on-policy), computing the maximum Euclidean norm of the\nJacobian w.r.t. the inputs: maxi,x\n\f\f\f\f dQi\nds\n\f\f\f\f\n2, where ||s||2 denotes the Frobenius norm\nand Qi denotes the Q value corresponding to action i.\nWe note that what we compute is a lower bound of the Lipschitz constant. To\nsee why consider Q(·; θ) : R|S| →R|A| with Lipschitz constant w.r.t. the Frobenius\nnorm K. We then we have that:\nmax\ni\n\f\f\f\f\n\f\f\f\f\ndQi\nds\n\f\f\f\f\n\f\f\f\f\n2\n2\n≤\n\f\f\f\f\n\f\f\f\f\ndQ\nds\n\f\f\f\f\n\f\f\f\f\n2\n2\n≤min(|S|, |A|)\n\f\f\f\f\n\f\f\f\f\ndQ\nds\n\f\f\f\f\n\f\f\f\f\n2\nop\n≤min(|S|, |A|)K2.\n(E.1)\nSince we chose the value of s with maximum norm, this ensures that the bound\nis as tight as possible with the available data (though this can be made tighter by\ncomputing the max\n\f\f\f\f dQ\nds\n\f\f\f\f2\n2).\nE.2. Experimental details\n351\nE.2.3\nEvaluation\nThe mean and median Human Normalised Score have been critiqued in the past\n[329, 330] because they can be dominated by some large normalised scores. However\nwe notice that with very few exceptions (3/54 for DQN and 11/54 games for C51)\nnormalising at most one of the network’s layers will not degrade the performance\ncompared to the baseline but instead it will improve upon it, often substantially.\nE.2.4\nMinatar\nGame selection. MinAtar [259] benchmark is a collection of five games that repro-\nduce the dynamics of ALE counterparts, albeit in a smaller observational space. Out\nof Asterix, Breakout, Seaquest, Space Invaders and Freeway we excluded the latter\nfrom all our experiments since all agents performed similarly on this game.\nNetwork architecture. All experiments on MinAtar are using a convolutional\nnetwork with LC convolutional layers with the same number of channels, a hidden\nlinear layer, and the output layer. The number of input channels is game-dependent\nin MinAtar. All convolutional layers have a kernel size of 3 and a stride of 1. All\nhidden layers have rectified linear units. Whenever we vary depth we change the\nnumber of convolutional layers LC, keeping the two linear layers. When the width\nis varied we change the width of both convolutional layers and the penultimate\nlinear layer. All convolutional layers are always identically scaled. We list all the\narchitectures used in various experiments described in this section in Table E.6.\nGeneral hyperparameter settings. In all our MinAtar experiments we used the\nsame set of hyperparameters returned by a small grid search around the initial\nvalues published by Young and Tian [259]. We list the values we settled on in\nTable E.1. For the rest of this section we only mention how we deviate from this set\nof hyperparameters and settings for each of the experiments that follow.\nMinAtar Normalised Score. In our work we present many MinAtar experiments\nas averages over the four games we tested on. Since in MinAtar the range of the\nexpected returns is game dependent we normalise the score. Inspired by the Human\nNormalised Score in Mnih et al. [107] we take the largest score ever recorded by a\nbaseline agent in our experiments and use it to compute MNS = 100 × (scoreagent −\nE.2. Experimental details\n352\nHyperparameter\nValue\ndiscount γ\n0.99\nupdate frequency\n4\ntarget update frequency\n4,000\nstarting ϵ\n1.0\nfinal ϵ\n0.01\nϵ steps\n250,000\nϵ schedule\nlinear\nwarmup steps\n5,000\nreplay size\n100,000\nhistory length\n1\ncost function\nMSE Loss\noptimiser\nAdam\nlearning rate h\n0.00025\ndamping term ϵ\n0.0003125\nβ1, β2\n(0.9, 0.999)\nvalidation steps\n125000\nvalidation ϵ\n0.001\nTable E.1: MinAtar default hyperparameter settings, used for all MinAtar experiments\nunless otherwise specified.\nscorerandom)/(scoremax−scorerandom). We use the resulting MinAtar Normalised Score\nwhenever we report performance aggregates over the games.\nNetwok architecture. The default network architecture used for MinAtar experi-\nments is shown in Table E.5.\nE.2.5\nAtari experiments\nEvaluation protocols on Atari. Since we mostly compare our ALE results with\nthe Rainbow agent, we adopt the evaluation protocol from Hessel et al. [251]. Every\n250K training steps in the environment we suspend the learning and evaluate the\nagent on 125K steps (or 500K frames). All the agents we train on ALE follow this\nvalidation protocol, the only difference being the validation epsilon value: ϵ = 0.001\nfor C51 and DQN-Adam that we directly compare to Rainbow, which uses the same\nvalue we use.\nDQN-Adam. We list the full details in Table E.4 especially since these hyperparam-\nE.3. Additional hyperparameter sweeps\n353\nGame\nMax\nRandom\nAsterix\n78.90\n0.49\nBreakout\n122.88\n0.52\nSeaquest\n93.91\n0.09\nSpace Invaders\n360.92\n2.86\nTable E.2: MinAtar maximum and random scores used for computing the MinAtar\nNormalised Score.\nNetwork hyperparameter\nValue\nNumber of convolutional channels\n16\nFilter size for convolutional channels\n3 × 3\nStride for convolutional channels\n1\nHidden units for fully connected layers\n128\nNumber of fully connected layers\n2\nNumber of convolutional layers\n2\nNumber of outputs\nNumber of actions\nTable E.3: Network used for MinAtar experiments. The last fully connected layer has an\noutput size given by the number of actions.\neters differ considerably from the the original DQN agent; since we also wanted to\nbe able to compare our results with those of Rainbowwe use similar hyperparameters\nto those in Hessel et al. [251].\nNetwok architecture. The network architecture used for Atari experiments is\nshown in Table E.5.\nE.3\nAdditional hyperparameter sweeps\nNetwork architecture results from other sweeps are shown in Table E.6.\nE.3. Additional hyperparameter sweeps\n354\nHyper-parameter\nValue\ndiscount γ\n0.99\nupdate frequency\n4\ntarget update frequency\n8000\nstarting ϵ\n1.0\nfinal ϵ\n0.01\nϵ steps\n250000\nϵ schedule\nlinear\nwarmup steps\n20000\nreplay size\n1M\nbatch size\n32\nhistory length\n4\ncost function\nSmoothL1Loss\noptimiser\nAdam\nlearning rate h\n0.00025\ndamping term ϵ\n0.0003125\nβ1, β2\n(0.9, 0.999)\nvalidation steps\n125000\nvalidation ϵ\n0.001\nTable E.4: DQN-Adam hyperparameters.\nNetwork hyperparameter\nValue\nNumber of convolutional channels\n32, 64, 64\nFilter size for convolutional channels\n8 × 8, 4 × 4, 3 × 3\nStride for convolutional channels\n4, 2, 1\nHidden units for fully connected layers\n512\nNumber of fully connected layers\n2\nNumber of outputs\nNumber of actions\nTable E.5: Network used for Atari experiments, taken from Hessel et al. [109]. The last\nfully connected layer has an output size given by the number of actions.\nE.3. Additional hyperparameter sweeps\n355\nExperiment\nNr. of\nconv layers\nConv\nwidth\nFC\nwidth\nSweep over optimisation hyperparameters (Figure 7.4.)\n1\n2\n3\n4\n2\n3\n24\n24\n24\n24\n32\n32\n128\n128\n128\n128\n256\n256\nRegularisation (Figure E.2)\n1\n3\n1\n3\n1\n3\n16\n16\n24\n24\n32\n32\n64\n64\n128\n128\n256\n256\nOptimisation methods (Figure E.5)\n1\n2\n1\n2\n24\n24\n32\n32\n128\n128\n256\n256\nSmoothness analysis (Figures 7.3 E.1 and E.1b,Tables 7.2 and 7.1)\n1\n2\n1\n3\n2\n3\n3\n2\n2\n1\n3\n1\n8\n8\n32\n8\n32\n32\n24\n16\n24\n24\n16\n16\n32\n32\n256\n32\n256\n256\n128\n64\n128\n128\n64\n64\nTable E.6: Details of neural architecture sweeps. The total number of critic layers is the\nnumber of convolutional layers specified here, followed by two linear layers.\nE.4. Individual figure reproduction details\n356\nE.4\nIndividual figure reproduction details\n• Figure 7.1 uses the default Atari network shown in Table E.5. The DQN-Adam\nhyperparameters as shown in Table E.4. The evaluation protocol is described\nin Section E.2.5. The C51 hyperparameters are the same as those in Hessel\net al. [251].\n• Figure 7.3 performs a sweep over depth and width of networks on MinAtar.\nThe optimisation and reinforcement learning hyperparameters as shown in\nTable E.1. The model sweep is described in Table E.6.\n• Figure 7.2 presents both Atari and MinAtar results. The Atari results in\nFigure 7.2a use the default Atari network shown in Table E.5. The DQN-\nAdam hyperparameters as shown in Table E.4. The Figure 7.2b uses the\nhyperparameters as shown in Table E.1 and the network in Table E.3.\n• Tables 7.1 and 7.2 use data from the same experiment as Figure 7.3.\n• Figure 7.4 uses a MinAtar sweep over architectures detailed in Table E.6, with a\nsweep over learning rates h ∈{0.00001, ..., 0.00215} and ϵ ∈{0.00001, ..., 0.01}.\nThe remaining hyperparameters are the ones in Table E.1.\n• Figure 7.5 used the default MinAtar optimisation and reinforcement learning\nhyperparameters as shown in Table E.1, and the network in Table E.3.\n• Figures 7.6, 7.12, 7.13 use the architectures in Spectral Normalised GAN [30],\nwith the Adam default hyperparameters with the exception of β1 = 0.5, and\nlearning rates sweeps as a cross product of {0.0001, 0.0002, 0.0003, 0.0004} for\nthe discriminator and generator. Simultaneous updates are used.\n• Figure 7.8 uses the architectures in Spectral Normalised GAN [30], with the\nAdam default hyperparameters with the exception of β1 = 0.5, and learning\nrates sweeps as a cross product of {0.0001, 0.0002, 0.0003, 0.0004} for the\ndiscriminator and generator.\nThe generator uses the non-saturating loss.\nAlternating updates are used.\nE.4. Individual figure reproduction details\n357\n• Figures 7.9 and 7.10 use the architectures in Spectral Normalised GAN [30],\nwith the Adam default hyperparameters with the exception of β1 = 0.5, and\nlearning rates 0.0001 for both the discriminator and generator. The generator\nis trained using the non-saturating loss. Alternating updates are used.\n• Figure 7.15 uses the architectures in Spectral Normalised GAN [30], with the\nAdam default hyperparameters with the exception of β1 = 0.5, and learning\nrates 0.0004 for both the discriminator and generator.\nThe ϵ values are\nprovided in each subfigure. The generator is trained using the non-saturating\nloss. Simultaneous updates are used.\n• Figures E.1 and E.1b use a sweep over architectures detailed in Table E.6.\nThe optimisation and reinforcement learning hyperparameters as shown in\nTable E.1.\n• Figure E.2 used a sweep over architectures detailed in Table E.6. The optimi-\nsation and reinforcement learning hyperparameters as shown in Table E.1.\n• Figure E.3 uses the hyperparameters as shown in Table E.1 and the network\nin Table E.3.\n• Figure E.5 is a different visualisation of the same experiment shown in Figure 7.5.\nThe architectures are those in Table E.6.\nAppendix F\nGeometric Complexity: a\nsmoothness complexity measure\nimplicitly regularised in deep\nlearning\nF.1\nFigure and experiments reproduction details\n• Figure 8.2: the MLP shown have 500 units per layer. GC is computed using\n100 data points.\n• Figure 8.3: the MLP shown have 300 units per layer and 3 layers. The learning\nrate used is 0.02.\n• Figure 8.4: The network used is a Resnet-18, with a batch size of 512 and a\nlearning rate of 0.02. Each experiment uses 3 seeds. The activation function is\nReLu.\n• Figure 8.5: The network used is a Resnet-18, with a batch size of 128 and a\nlearning rate of 0.02. Each experiment uses 3 seeds. The activation function is\nReLu.\n• Figure 8.6: The network used is a Resnet-18, trained with batch size 512. Each\nexperiment uses 3 seeds. The activation function is ReLu.\nF.2. Additional experimental results\n359\n• Figure 8.7: The network used is a Resnet-18, trained with learning rate 0.02.\nEach experiment uses 3 seeds. The activation function is ReLu.\n• Figure 8.8: The network used is a Resnet-18, trained with learning rate 0.02\nand batch size 512. The activation function is ReLu.\nF.2\nAdditional experimental results\nWe present additional the implicit and explicit regularisation experiments using\ngradient descent with momentum, which is widely used in practice. We observe\nthe same phenomena as with gradient descent without momentum, namely that\nmore implicit regularisation through decreased batch size and increased learning\nrate (Figure F.1) and explicit regularisation using gradient norm, spectral and L2\nregularisation (shown in Figures F.2, F.3 and F.4 respectively) produces solutions\nwith higher test accuracy and lower Geometric Complexity (GC).\nF.2. Additional experimental results\n360\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nLearning Rate\n0.0001\n0.0005\n0.001\n0.005\n0.01\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n0\n5000 10000 15000 20000 25000 30000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\nGC\nCIFAR-10\n(a) Learning rate.\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nBatch Size\n8.0\n16.0\n32.0\n64.0\n128.0\n256.0\n512.0\n1024.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest Accuracy\n0\n20000\n40000\n60000\n80000 100000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nGC\nCIFAR-10\n(b) Batch size.\nFigure F.1: The implicit regularisation effect of increased learning rates (a) and\ndecreased batch sizes (b) leads to decreased GC when momentum\nis used. The network used is a Resnet-18, the momentum rate is 0.9. For\nthe learning rate experiments, the batch size used is 512. For the batch size\nexperiments, the learning rate used is 0.005. Each experiment uses 3 seeds.\nF.2. Additional experimental results\n361\n0\n5\n10\n15\nTrain Loss\nRegularization Rate\n0.0\n0.005\n0.0075\n0.01\n0.025\n0.05\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n0\n2000\n4000\n6000\n8000\n10000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nGC\nCIFAR-10\n(a) Vanilla SGD.\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nRegularization Rate\n0.0\n0.0001\n0.0005\n0.001\n0.005\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTest Accuracy\n0\n2000\n4000\n6000\n8000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\nGC\nCIFAR-10\n(b) Momentum 0.9.\nFigure F.2: Gradient norm regularisation leads to decreased Geometric Com-\nplexity when using stochastic gradient descent, with and without\nmomentum.. The network used is a Resnet-18, with a batch size of 512\nand a learning rate of 0.005 for momentum, and 0.02 for vanilla gradient\ndescent. Each experiment uses 3 seeds.\nF.2. Additional experimental results\n362\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nRegularization Rate\n0.0\n0.01\n0.025\n0.05\n0.075\n0.1\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nTest Accuracy\n0\n2000\n4000\n6000\n8000\n10000\nIteration\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGC\nCIFAR-10\n(a) Vanilla SGD.\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nRegularization Rate\n0.0\n0.01\n0.025\n0.05\n0.075\n0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n0\n2000\n4000\n6000\n8000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\nGC\nCIFAR-10\n(b) Momentum 0.9.\nFigure F.3: Spectral regularisation leads to decreased GC when using stochastic\ngradient descent, with (b) and without momentum (a). The network\nused is a Resnet-18, with a batch size of 512 and a learning rate of 0.005 for\nmomentum, and 0.02 for vanilla gradient descent. Each experiment uses 3\nseeds.\nF.2. Additional experimental results\n363\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nRegularization Rate\n0.0\n0.001\n0.002\n0.005\n0.007\n0.01\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n0\n2000\n4000\n6000\n8000\n10000\nIteration\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGC\nCIFAR-10\n(a) Vanilla SGD.\n0.0\n0.5\n1.0\n1.5\n2.0\nTrain Loss\nRegularization Rate\n0.0\n0.01\n0.025\n0.05\n0.075\n0.1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy\n0\n2000\n4000\n6000\n8000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\nGC\nCIFAR-10\n(b) Momentum 0.9.\nFigure F.4: L2 regularisation leads to decreased Geometric Complexity when\nusing stochastic gradient descent, with (b) and without momentum\n(a). The network used is a Resnet-18, with a batch size of 512 and a learning\nrate of 0.005 for momentum, and 0.02 for vanilla gradient descent. Each\nexperiment uses 3 seeds.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2023-10-21",
  "updated": "2023-10-21"
}