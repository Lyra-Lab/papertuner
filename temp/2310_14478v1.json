{
  "id": "http://arxiv.org/abs/2310.14478v1",
  "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
  "authors": [
    "Zekun Li",
    "Wenxuan Zhou",
    "Yao-Yi Chiang",
    "Muhao Chen"
  ],
  "abstract": "Humans subconsciously engage in geospatial reasoning when reading articles.\nWe recognize place names and their spatial relations in text and mentally\nassociate them with their physical locations on Earth. Although pretrained\nlanguage models can mimic this cognitive process using linguistic context, they\ndo not utilize valuable geospatial information in large, widely available\ngeographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a\ngeospatially grounded language model that enhances the understanding of\ngeo-entities in natural language. GeoLM leverages geo-entity mentions as\nanchors to connect linguistic information in text corpora with geospatial\ninformation extracted from geographical databases. GeoLM connects the two types\nof context through contrastive learning and masked language modeling. It also\nincorporates a spatial coordinate embedding mechanism to encode distance and\ndirection relations to capture geospatial context. In the experiment, we\ndemonstrate that GeoLM exhibits promising capabilities in supporting toponym\nrecognition, toponym linking, relation extraction, and geo-entity typing, which\nbridge the gap between natural language processing and geospatial sciences. The\ncode is publicly available at https://github.com/knowledge-computing/geolm.",
  "text": "GeoLM: Empowering Language Models for Geospatially\nGrounded Language Understanding\nZekun Li\nWenxuan Zhou\nYao-Yi Chiang\nMuhao Chen\nDepartment of Computer Science and Engineering, University of Minnesota, Twin Cities\nDepartment of Computer Science, University of Southern California\nDepartment of Computer Science, University of California, Davis\n{li002666,yaoyi}@umn.edu; zhouwenx@usc.edu; muhchen@ucdavis.edu\nAbstract\nHumans subconsciously engage in geospatial\nreasoning when reading articles. We recog-\nnize place names and their spatial relations in\ntext and mentally associate them with their\nphysical locations on Earth.\nAlthough pre-\ntrained language models can mimic this cog-\nnitive process using linguistic context, they do\nnot utilize valuable geospatial information in\nlarge, widely available geographical databases,\ne.g., OpenStreetMap. This paper introduces\nGEOLM (\n), a geospatially grounded lan-\nguage model that enhances the understanding\nof geo-entities in natural language. GEOLM\nleverages geo-entity mentions as anchors to\nconnect linguistic information in text corpora\nwith geospatial information extracted from ge-\nographical databases. GEOLM connects the\ntwo types of context through contrastive learn-\ning and masked language modeling. It also\nincorporates a spatial coordinate embedding\nmechanism to encode distance and direction\nrelations to capture geospatial context.\nIn\nthe experiment, we demonstrate that GEOLM\nexhibits promising capabilities in supporting\ntoponym recognition, toponym linking, rela-\ntion extraction, and geo-entity typing, which\nbridge the gap between natural language pro-\ncessing and geospatial sciences.\nThe code\nis publicly available at https://github.com/\nknowledge-computing/geolm.\n1\nIntroduction\nSpatial reasoning and semantic understanding of\nnatural language text arise from human communi-\ncations. For example, “I visited Paris in Arkansas\nto see the smaller Eiffel Tower”, a human can eas-\nily recognize the toponyms “Eiffel Tower”, “Paris”,\n“Arkansas”, and the spatial relation “in”. Implic-\nitly, a human can also infer that this “Eiffel Tower”\nmight be a replica1 of the original one in Paris,\n1 The small Eiffel Tower in Paris, Arkansas, United States:\nhttps://www.arkansas.com/paris/accommodations/\neiffel-tower-park\nFrance. Concretely, the process of geospatially\ngrounded language understanding involves core\ntasks such as recognizing geospatial concepts be-\ning described, inferring the identities of those con-\ncepts, and reasoning about their spatially qualified\nrelations. These tasks are essential for applications\nthat involve the use of place names and their spa-\ntial relations, such as social media message analy-\nsis (Hu et al., 2022), emergency response (Gritta\net al., 2018b), natural disaster analysis (Wang et al.,\n2020), and geographic information retrieval (Wall-\ngrün et al., 2018).\nPretrained language models (PLMs; Devlin et al.\n2019; Liu et al. 2019; Raffel et al. 2020) have seen\nbroad adaptation across various domains such as\nbiology (Lee et al., 2020), healthcare (Alsentzer\net al., 2019), law (Chalkidis et al., 2020; Douka\net al., 2021), software engineering (Tabassum et al.,\n2020), and social media (Röttger and Pierrehum-\nbert, 2021; Guo et al., 2021). These models ben-\nefit from in-domain corpora (e.g., PubMed for\nthe biomedical domain) to learn domain-specific\nterms and concepts.\nSimilarly, a geospatially\ngrounded language model requires training with\ngeo-corpora. The geo-corpora should cover world-\nwide geo-entity names and their variations, geo-\ngraphical locations, and spatial relations to other\ngeo-entities. Although some geo-corpora are avail-\nable, e.g., LGL (Lieberman et al., 2010) and Spa-\ntialML (Mani et al., 2010), the sizes of the datasets\nare relatively small.\nIn contrast, Wikipedia stores many articles de-\nscribing places worldwide and can serve as com-\nprehensive geo-corpora for geospatially grounded\nlanguage understanding. However, training with\nWikipedia only solves partial challenges in geospa-\ntial grounding, as it only provides the linguistic\ncontext of a geo-entity with sentences describing\nhistory, demographics, climate, etc. The infor-\nmation about the geospatial neighbors of a geo-\nentity is still missing. On the other hand, large-\narXiv:2310.14478v1  [cs.CL]  23 Oct 2023\nFigure 1: Outline of GEOLM. Wikipedia and Wikidata form the NL corpora, and OpenStreetMap (OSM) form\nthe pseudo-sentence corpora (Details in §2.2). GEOLM takes the NL and pseudo-sentence corpora as input, then\npretrain with MLM and contrastive loss using geo-entities as anchors. (See §2.3)\nscale geographical databases (e.g., OpenStreetMap)\nand knowledge bases (e.g., Wikidata) can pro-\nvide extensive amounts of geo-entity locations\nand geospatial relations, enriching the information\nsourced from Wikipedia. Additionally, the geo-\nlocations from OpenStreetMap can help connecting\nthe learned geo-entity representations to physical\nlocations on the Earth.\nTo address these challenges, we propose GE-\nOLM, a language model specifically designed to\nsupport geospatially grounded natural language un-\nderstanding. Our model aims to enhance the com-\nprehension of places, types, and spatial relations\nin natural language. We use Wikipedia, Wikidata,\nand OSM together to create geospatially grounded\ntraining samples using geo-entity names as anchors.\nGEOLM verbalizes the Wikidata relations into nat-\nural language and aligns the linguistic context in\nWikipedia and Wikidata with the geospatial con-\ntext in OSM. Since existing language models do\nnot take geocoordinates as input, GEOLM further\nincorporates a spatial coordinate embedding mod-\nule to learn the distance and directional relations\nbetween geo-entities. During inference, our model\ncan take either natural language or geographical\nsubregion (i.e., a set of nearby geo-entities) as in-\nput and make inferences relying on the aligned\nlinguistic-geospatial information. We employ two\ntraining strategies: contrastive learning (Oord et al.,\n2018) between natural language corpus and lin-\nearized geographic data, and masked language\nmodeling (Devlin et al., 2019) with a concatena-\ntion of these two modalities. We treat GEOLM as\na versatile model capable of addressing various ge-\nographically related language understanding tasks,\nsuch as toponym recognition, toponym linking, and\ngeospatial relation extraction.\n2\nGEOLM\nThis section introduces GEOLM’s mechanism for\nrepresenting both the linguistic and geospatial con-\ntext (§2.1), followed by the detailed development\nprocess of pretraining tasks (§2.3) and corpora\n(§2.2), and how GEOLM is further adapted to vari-\nous downstream geospatial NLU tasks (§2.4).\n2.1\nRepresenting Linguistic and Geospatial\nContext\nThe training process of GEOLM aims to simul-\ntaneously learn the linguistic and geospatial con-\ntext, aligning them in the same embedding space to\nobtain geospatially grounded language representa-\ntions. The linguistic context refers to the sentential\ncontext of the geographical entity (i.e., geo-entity).\nThe linguistic context contains essential informa-\ntion to specify a geo-entity, including sentences\ndescribing geography, environment, culture, and\nhistory. Additionally, geo-entities exhibit strong\ncorrelations with neighboring geo-entities (Li et al.,\n2022b). We refer to these neighboring geo-entities\nas the geospatial context. The geospatial context\nencompasses locations and implicit geo-relations\nof the neighbors to the center geo-entity. 2\nTo capture the linguistic context, GEOLM takes\nnatural sentences as input and generates entity-level\nrepresentations by averaging the token representa-\ntions within the geo-entity name span. For the\ngeospatial context, GEOLM follows the geospatial\ncontext linearization method and the spatial em-\nbedding module in our previous work SPABERT\n(Li et al., 2022b), a PLM that generates geospa-\ntially contextualized entity representations using\n2Here, we assume that all geo-entities in the geographical\ndataset are represented as points.\nFigure 2: Sample inputs to GEOLM. Note that segment IDs for the NL tokens are zeros, and for the pseudo-sentence\ntokens are ones.\npoint geographic data. Given a center geo-entity\nand its spatial neighbors, GEOLM linearizes the\ngeospatial context by sorting the neighbors in as-\ncending order based on their geospatial distances\nfrom the center geo-entity. GEOLM then concate-\nnates the name of the center geo-entity and the\nsorted neighbors to form a pseudo-sentence. To\npreserve directional relations and relative distances,\nGEOLM employs the geocoordinates embedding\nmodule, which takes the geocoordinates as input\nand encodes them with a sinusoidal position em-\nbedding layer.\nTo enable GEOLM to process both natural lan-\nguage text and geographical data, we use the fol-\nlowing types of position embedding mechanisms\nfor each token and the token embedding (See\nFig. 2). By incorporating these position embed-\nding mechanisms, GEOLM can effectively process\nboth natural language text and geographical data,\nallowing the model to capture and leverage spatial\ninformation.\nPosition ID describes the index position of the\ntoken in the sentence. Note that the position ID for\nboth the NL and geospatial input starts from zero.\nSegment ID indicates the source of the input\ntokens. If the tokens belong to the natural language\ninput, then the segment ID is zero; otherwise one.\nX-coord and Y-coord are inputs for the spatial\ncoordinate embedding. Tokens within the same\ngeo-entity name span share the same X-coord and\nY-coord values. Since NL tokens do not have as-\nsociated geocoordinate information, we set their\nX-coord and Y-coord to be DSEP, which is a con-\nstant value as distance filler.\nIn addition, GEOLM projects the geocoordinates\n(lat, lng) into a 2-dimensional World Equidistant\nCylindrical coordinate system EPSG:4087.3 This\nis because (lat, lng) represent angle-based values\nthat model the 3D sphere, whereas coordinates in\nEPSG:4087 are expressed in Cartesian form. Ad-\nditionally, when generating the pseudo-sentence,\nwe sort neighbors of the center geo-entity based on\nthe Haversine distance which reflects the geodesic\ndistance on Earth instead of the Euclidean distance.\n2.2\nPretraining Corpora\nWe divide the training corpora into two parts:\n1) pseudo-sentence corpora from a geographical\ndataset, OpenStreetMap (OSM), to provide the\ngeospatial context; 2) natural language corpora\nfrom Wikipedia and verbalized Wikidata to pro-\nvide the linguistic context.\nGeographical Dataset. OpenStreetMap (OSM) is\na crowd-sourced geographical database containing\na massive amount of point geo-entities worldwide.\nIn addition, OSM stores the correspondence from\nOSM geo-entity to Wikipedia and Wikidata links.\nWe preprocess worldwide OSM data and gather\nthe geo-entities with Wikidata and Wikipedia links\nto prepare paired training data used in contrastive\npretraining. To linearize geospatial context and\nprepare the geospatial input in Fig. 2, For each\ngeo-entity, we retrieve its geospatial neighbors and\nconstruct pseudo-sentences (See Fig. 1) by con-\ncatenating the neighboring geo-entity names after\nsorting the neighbors by distance. In the end, we\ngenerate 693,308 geo-entities with the same num-\nber of pseudo-sentences in total.\n3 EPSG:4087: https://epsg.io/4087\nNatural Language Text Corpora. We prepare\nthe text corpora from Wikipedia and Wikidata.\nWikipedia provides a large corpus of encyclope-\ndic articles, with a subset describing geo-entities.\nWe first find the articles describing geo-entities by\nscraping all the Wikipedia links pointed from the\nOSM annotations, then break the articles into sen-\ntences and adopt Trie-based phrase matching (Hsu\nand Ottaviano, 2013) to find the sentences contain-\ning the name of the corresponding OSM geo-entity.\nThe training samples are paragraphs containing\nat least one corresponding OSM geo-entity name.\nFor Wikidata, the procedure is similar to Wikipedia.\nWe collect the Wikidata geo-entities using the QID\nidentifier pointed from the OSM geo-entities. Since\nWikidata stores the relations as triples, we convert\nthe relation triples to natural sentences with a set\nof pre-defined templates (See example in Fig. 1).\nAfter merging the Wikipedia and Wikidata samples,\nwe gather 1,458,150 sentences/paragraphs describ-\ning 472,067 geo-entities.\n2.3\nPretraining Tasks\nWe employ two pretraining tasks to establish con-\nnections between text and geospatial data, enabling\nGEOLM to learn geospatially grounded represen-\ntations of natural language text.\nThe first is a contrastive learning task using an\nInfoNCE loss (Oord et al., 2018), which contrasts\nbetween the geo-entity features extracted from the\ntwo modalities. This loss encourages GEOLM to\ngenerate similar representations for the same geo-\nentity, regardless of whether the representation is\ncontextualized based on the linguistic or geospatial\ncontext. Simultaneously, GEOLM learns to dis-\ntinguish between geo-entities that share the same\nname by maximizing the distances of their repre-\nsentations in the embedding space.\nFormally, let the training data D consist of pairs\nof samples (snl\ni , sgeo\ni\n), where snl\ni\nis a linguistic\nsample (a natural sentence or a verbalized rela-\ntion from a knowledge base), and sgeo\ni\nis a pseudo-\nsentence created from the geographic data. Both\nsamples mention the same geo-entity. Let f(·) be\nGEOLM that takes both snl\ni and sgeo\ni\nas input and\nproduces entity-level representation hnl\ni = f(snl\ni )\nand hgeo\ni\n= f(sgeo\ni\n). Then the loss function is:\nLcontrast\ni\n= −log\nesim(hnl\ni ,hgeo\ni\n)/τ\nP2N\nj=1 1[j̸=i]esim(hnl\ni ,hgeo\nj\n)/τ ,\nFigure 3: Downstream tasks to evaluate geospatially\ngrounded language understanding.\nwhere τ is a temperature, and sim(·) denotes the\ncosine similarity.\nTo improve GEOLM’s ability to disambiguate\ngeo-entities, we include in-batch hard negatives\ncomprising geo-entities with identical names. As\na result, each batch is composed of 50% random\nnegatives and 50% hard negatives.\nAdditionally, we employ a masked language\nmodeling task (Devlin et al., 2019) on a concate-\nnation of the paired natural language sentence and\ngeographical pseudo-sentence (Fig. 2). This task\nencourages GEOLM to recover masked tokens by\nleveraging both linguistic and geographical data.\n2.4\nDownstream Tasks\nOur study further adapts GEOLM to several down-\nstream tasks to demonstrate its ability for geospa-\ntially grounded language understanding, including\ntoponym recognition, toponym linking, geo-entity\ntyping, and geospatial relation extraction (Fig. 3).\nToponym recognition or geo-tagging (Gritta et al.,\n2020) is to extract toponyms (i.e., place names)\nfrom unstructured text. This is a crucial step in\nrecognizing geo-entities mentioned in the text be-\nfore inferring their identities and relationships in\nsubsequent tasks. We frame this task as a multi-\nclass sequence tagging problem, where tokens are\nclassified into one of three classes: B-topo (begin-\nning of a toponym), I-topo (inside a toponym),\nor O (non-toponym). To accomplish this, we ap-\npend a fully connected layer to GEOLM and train\nthe model end-to-end on a downstream dataset to\nclassify each token from the text input.\nToponym linking, also referred to as toponym res-\nolution and geoparsing (Gritta et al., 2020, 2018a;\nHu et al., 2022), aims to infer the identity or geoco-\nordinates of a mentioned geo-entity in the text by\ngrounding geo-entity mention to the correct record\nin a geographical database, which might contain\nmany candidate entities with the same name as the\nextracted toponym from the text. During inference,\nwe process the candidate geo-entities from the ge-\nographical databases the same way as during pre-\ntraining, where nearby neighbors are concatenated\ntogether to form pseudo-sentences. For this task,\nwe perform zero-shot linking by directly applying\nGEOLM without fine-tuning. GEOLM extracts\nrepresentations of geo-entities from linguistic data\nand calculates representations for all candidate en-\ntities. After obtaining the representations for both\nquery and candidate geo-entities, the linking results\nare formed as a ranked list of candidates sorted by\ncosine similarity in the feature space.\nGeo-entity typing is the task of categorizing the\ntypes of locations in a geographical database (e.g.,\nOpenStreetMap) (Li et al., 2022b). Geo-entity typ-\ning helps us understand the characteristics of a\nregion and can be useful for location-based recom-\nmendations. We treat this task as a classification\nproblem and append a one-layer classification head\nafter GEOLM. We train GEOLM to predict the type\nof the central geo-entity given a subregion of a geo-\ngraphical area. To accomplish this, we construct a\npseudo-sentence following Fig. 1, then compute the\nrepresentation of the geo-entity using GEOLM and\nfeed the representation to the classification head\nfor training.\nGeospatial relation extraction is the task of clas-\nsifying topological relations between a pair of lo-\ncations (Mani et al., 2010). We treat this task as\nan entity pair classification problem. We compute\nthe average embedding of tokens to form the entity\nembedding and then concatenate the embeddings\nof the subject and object entities to use as input\nfor the classifier, then predict the relationship type\nwith a softmax classifier.\n3\nExperiments\nWe hereby evaluate GEOLM on the aforemen-\ntioned four downstream tasks. All compared mod-\nels (except GPT3.5) are finetuned on task-specific\ndatasets for toponym recognition, geo-entity typ-\ning, and geospatial relation extraction.4\n4Toponym linking is an unsupervised task.\n3.1\nToponym Recognition\nTask Setup. We adopt GEOLM for toponym recog-\nnition on the GeoWebNews (Gritta et al., 2020)\ndataset. This dataset contains 200 news articles\nwith 2,601 toponyms.5 The annotation includes the\nstart and end character positions (i.e., the spans)\nof the toponyms in paragraphs. We use 80% for\ntraining and 20% for testing.\nEvaluation Metrics. We report precision, recall,\nand F1, and include both token-level scores and\nentity-level scores. For entity-level scores, a pre-\ndiction is considered correct only when it matches\nexactly with the ground-truth mention (i.e., no miss-\ning or partial overlap).\nModels in Comparison. We compare GEOLM\nwith fine-tuned PLMs including BERT, SimCSE-\nBERT6 (Gao et al., 2021) , SpanBERT (Joshi et al.,\n2020) and SapBERT (Liu et al., 2021). SpanBERT\nis a BERT-based model with span prediction pre-\ntraining. Instead of masking out random tokens,\nSpanBERT masks out a continuous span and tries\nto recover the missing parts using the tokens right\nnext to the masking boundary. SapBERT learns to\nalign biomedical entities through self-supervised\ncontrastive learning.7 We compare all models in\nbase versions.\nResults and Discussion. Tab. 1 shows that our\nGEOLM yields the highest entity-level F1 score,\nwith BERT being the close second. Since GE-\nOLM’s weights are initialized from BERT, the\nimprovement over BERT shows the effectiveness\nof in-domain training with geospatial grounding.\nFor token-level results, SpanBERT has the best\nF1 score for I-topo, showing that span prediction\nduring pretraining is beneficial for predicting the\ncontinuation of toponyms. However, GEOLM is\nbetter at predicting the start token of the toponym,\nwhich SpanBERT does not perform as well.\n3.2\nToponym Linking\nTask Setup. We run unsupervised toponym linking\non two benchmark datasets: Local Global Corpus\n(LGL; Lieberman et al. 2010) and Wikipedia To-\n5We only consider the place names associated with valid\ngeocoordinates as toponyms and do not count the literal ex-\npression (e.g., the word “street”, “blocks” and “intersection”)\nas toponyms.\n6We use the unsupervised pretrained weights.\n7Although SapBERT is trained with biomedical corpus, it\ngeneralizes well on geo-entity recognition and linking since\nthe study of diseases often relates to places and regions.\nGeoWebNews\nToken(B-topo)\nToken (I-topo)\nmicro-\nEntity\nPrec\nRecall\nF1\nPrec\nRecall\nF1\nF1\nPrec\nRecall\nF1\nBERT\n90.00\n89.28\n89.64\n78.55\n79.44\n78.99\n84.46\n77.03\n83.42\n80.10\nSimCSE-BERT\n83.86\n90.26\n86.95\n74.61\n82.07\n78.16\n82.67\n72.76\n83.68\n77.84\nSpanBERT\n85.98\n88.37\n87.16\n86.13\n89.19\n87.63\n87.38\n75.32\n81.16\n78.13\nSapBERT\n83.12\n88.32\n85.64\n76.26\n81.11\n78.61\n82.22\n72.48\n80.16\n76.12\nGEOLM\n91.15\n90.43\n90.79\n79.16\n84.27\n81.63\n86.33\n82.18\n85.67\n83.89\nTable 1: Toponym recognition results on GeoWebNews dataset. Bolded and underlined numbers are for best and\nsecond best scores respectively.\nFigure 4: Visualization of BERT and GEOLM predictions on two “Paris” samples from the WikToR dataset. The\ntwo left figures are BERT results, and the two right figures are GEOLM results. The colored circles in the figure\ndenote all the geo-entities named “Paris” in the GeoNames database. The circle’s color represents the cosine\nsimilarity (or ranking score) between the candidate geo-entitity from GeoNames and the query toponym Paris. It is\nworth noting that the BERT predictions are almost the same given the two sentences because BERT does not align\nthe linguistic context with the geospatial context.\nponym Retrieval (WikToR; Gritta et al. 2018b).\nLGL contains 588 news articles with 4,462 to-\nponyms that have been linked to the GeoNames\ndatabase. This dataset has ground-truth geoco-\nordinates and GeoNames ID annotations. This\ndataset has only one sample with a unique name\n(among 4,462 samples in total). On average, each\ntoponym corresponds to 27.52 geo-entities with\nthe same in GeoNames. WikToR is derived from\n5,000 Wikipedia pages, where each page describes\na unique place. This dataset also contains many\ntoponyms that share the same lexical form but refer\nto distinct geo-entities. For example, Santa Maria,\nLima, and Paris are the top ambiguous place names.\nFor WikToR, there is no sample with a unique\nname. The least ambiguous one has four candidate\ngeo-entities with the same name in GeoNames. On\naverage, each toponym corresponds to 70.45 geo-\nentities with the same name in GeoNames. This\ndataset is not linked with the GeoNames database,\nbut the ground-truth geocoordinates of the toponym\nare provided instead.\nEvaluation Metrics. For LGL, we evaluate model\nperformance using two metrics. 1) R@k is a stan-\ndard retrieval metric that computes the recall of\nthe top-k prediction based on the ground-truth\nGeoNames ID. 2) P@D, following previous studies\n(Gritta et al., 2018b), computes the distance-based\napproximation rate of the top-ranked prediction. If\nthe prediction falls within the distance threshold\nD from the ground truth geocoordinates, we con-\nsider the prediction as a correct approximation.8\n8Here,\nP@D161 is the same as Acc@161km (or\nAcc@100miles ) reported by Gritta et al. (2018b).\nLGL\nR@1\nR@5\nR@10\nP@D161\nBERT\n34.6\n67.5\n78.1\n41.2\nRoBERTa\n24.2\n48.7\n60.6\n27.9\nSpanBERT\n25.2\n48.8\n61.0\n28.8\nSapBERT\n30.8\n58.8\n72.2\n35.1\nGEOLM\n38.2\n65.3\n72.6\n44.1\nWikToR\nP@D20\nP@D50\nP@D100\nP@D161\nBERT\n16.1\n16.3\n16.9\n17.6\nRoBERTa\n11.7\n11.9\n12.4\n13.0\nSpanBERT\n5.5\n5.7\n5.9\n6.3\nSapBERT\n25.9\n26.3\n27.0\n28.3\nGEOLM\n32.5\n33.4\n34.3\n35.8\nTable 2: Toponym linking results on LGL and Wik-\nTOR datasets. Bolded numbers are the best scores\nand underlined numbers are the second-best scores.\nR@k measures whether ground-truth GeoNames ID\npresents among the top k retrieval results. P@D mea-\nsures whether the top retrieval is within the distance D\nfrom the ground-truth. D20, D50 , D100 and D161 indi-\ncate the distance thresholds of {20, 50, 100, 161} km.\nFor WikToR, the ground-truth geocoordinates are\ngiven, and we follow Gritta et al. (2020, 2018b) to\nreport P@D metrics with various D values.\nModels in Comparison. We compare GEOLM\nwith multiple PLMs, including BERT, RoBETRa,\nSpanBERT (Joshi et al., 2020) and SapBERT (Liu\net al., 2021). In the experiments, we use all base\nversions of the above models. For a fair compar-\nison, to calculate the representation of the candi-\ndate entity, we concatenate the center entity’s name\nand its neighbors’ names then input the concate-\nnated sequence (i.e., pseudo-sentence) to all base-\nline PLMs to provide the linguistic context, follow-\ning the same unsupervised procedure as in §2.4.\nResults and Discussion. One challenge of this\ntask is that the input sentences may not have any\ninformation about their neighbor entities; thus, in-\nstead of only considering the linguistic context or\nrelying on neighboring entities to appear in the\nsentence, GEOLM’s novel approach aligns the lin-\nguistic and geospatial context so that GEOLM can\neffectively map the geo-entity embeddings learned\nfrom the linguistic context in articles to the embed-\ndings learned from the geospatial context in geo-\ngraphic data and perform linking. The contrastive\nlearning process during pretraining is designed to\nhelp the context alignment. From Tab. 2, GEOLM\noffers more reliable performance than baselines.\nOn WikTor and LGL datasets, GEOLM obtains the\nbest or second-best scores in all metrics. On LGL,\nGEOLM demonstrates more precise top-1 retrieval\n(R@1) than other models, and also the highest\nscores on P@D161. Since baselines are able to har-\nness only the linguistic context, while GEOLM can\nuse the linguistic and geospatial context when tak-\ning sentences as input. The improvement of R@1\nand P@D161 from BERT shows the effectiveness\nof the geospatial context. On WikToR, GEOLM\nperforms the best on all metrics. Since WikToR\nhas many geo-entities with the same names, the\nscores of GEOLM indicate strong disambiguation\ncapability obtained from aligning the linguistic and\ngeospatial context.\nFig. 4 shows the visualization of the toponym\nlinking results given “Paris” mentioned in two sen-\ntences. The input sentences are provided in the\nfigures. Apparently, the first Paris should be linked\nto Paris, France, and the second Paris goes to\nParis, AK, US. However, the BERT model fails\nto ground these two mentions into the correct loca-\ntions. BERT ranks the candidate geo-entities only\nslightly differently for the two input sentences, in-\ndicating that BERT relies more on the geo-entity\nname rather than the linguistic context when per-\nforming prediction. On the other hand, our model\ncould predict the correct location of geo-entities.\nThis is because even though the lexical forms of\nthe geo-entity names (i.e., Paris) are the same, the\nlinguistic context describing the geo-entity are dis-\ntinct. Fig. 4 demonstrate that the contrastive learn-\ning helps GEOLM to map the linguistic context to\nthe geospatial context in the embedding space.\n3.3\nGeo-entity Typing\nTask Setup.\nWe apply GEOLM on the super-\nvised geo-entity typing dataset released by Li et al.\n(2022b). The goal is to classify the type of the cen-\nter geo-entity providing the geospatial neighbors\nas context. We linearize the set of geo-entities to a\npseudo-sentence that represent the geospatial con-\ntext for the center geo-entity then feed the pseudo-\nsentence into GEOLM. There are 33,598 pseudo-\nsentences for nine amenity classes, with 80% for\ntraining and 20% for testing. We train multiple\nlanguage models to perform amenity-type classifi-\ncation for the center geo-entity.\nEvaluation Metric. Following (Li et al., 2022b),\nwe report the F1 score for each class and the micro\nF1 for all the samples.\nModels in Comparison. In addition to BERT,\nSpanBERT and SimCSE-BERT, this task also takes\nLUKE (Yamada et al., 2020) and SpaBERT (Li\nClasses →\nEdu.\nEnt.\nFac.\nFin.\nHea.\nPub.\nSus.\nTra.\nWas.\nMicro F1\nBERT\n67.4\n63.4\n76.3\n92.9\n85.6\n87.2\n85.6\n86.2\n67.8\n83.5\nSpanBERT\n63.3\n58.9\n60.8\n91.6\n85.9\n88.2\n82.4\n86.7\n73.5\n81.9\nSimCSE-BERT\n62.3\n59.0\n50.4\n92.5\n86.7\n85.2\n85.7\n81.0\n47.0\n81.0\nLUKE\n64.8\n60.8\n59.8\n94.5\n85.7\n86.7\n85.4\n85.1\n51.7\n82.5\nSpaBERT\n67.4\n65.3\n68.0\n95.9\n86.5\n90.0\n88.3\n88.8\n70.3\n85.2\nGEOLM\n72.5\n70.9\n73.0\n97.8\n91.5\n83.6\n90.5\n90.8\n62.2\n87.8\nTable 3: Comparing GEOLM with the state-of-the-art LMs on geo-entity typing. Column names are the OSM\nclasses (education, entertainment, facility, financial, healthcare, public service, sustenance, transportation and waste\nmanagement). Bolded and underlined numbers are for best and second best scores respectively.\nSettings\nPrec\nRecall\nF1\nGEOLM\n82.18\n85.67\n83.89\nGEOLM (No Contrastive)\n70.67\n77.98\n74.14\nGEOLM (No Spatial Embed.) 75.05\n87.00\n80.59\nTable 4:\nAblation study on toponym recognition to\nshow the GEOLM performance after removing various\ncomponents.\net al., 2022b) into comparison. LUKE is designed\nto solve entity-related tasks with a specially de-\nsigned entity tokenizer. SpaBERT generates geo-\nentity representations given small geographical re-\ngions as input. In addition, BERT, SpanBERT,\nSimCSE-BERT, and LUKE rely only on linguistic\ninformation, and SpaBERT relies only on geospa-\ntial context to make predictions.\nResults and Discussion. Tab. 3 shows that the\nperformance of GEOLM surpasses the baseline\nmodels using only linguistic or geospatial informa-\ntion. The experiment demonstrates that combining\nthe information from both modalities and aligning\nthe context is useful for geo-entity type inference.\nCompared to the second best model, SpaBERT,\nGEOLM has robust improvement on seven types.\nThe result indicates that the contrastive learning\nduring pretraining helps GEOLM to align the lin-\nguistic context and geospatial context, and although\nonly geospatial context is provided as input during\ninference, GEOLM can still employ the aligned\nlinguistic context to facilitate the type prediction.\n3.4\nGeospatial Relation Extraction\nTask Setup. We apply GEOLM to the SpatialML\ntopological relation extraction dataset released\nby Mani et al. (2010). Given a sentence containing\nmultiple entities, the model classifies the relations\nbetween each pair of entities into six types9 (includ-\n9The types come from the RCC8 relations, which contain\neight types of geospatial relations. The SpatialML dataset\nmerges four of the relations (TPP, TPPi, NTTP, NTTPi) into\nthe “IN” relation thus resulting in five geo-relations in the\ndataset.\nFigure 5: Relation extraction results on SpatialML com-\nparing with other baseline models.\ning an NA class indicating that there is no relation).\nThe dataset consists of 1,528 sentences, with 80%\nfor training and 20% for testing. Within the dataset,\nthere are a total of 10,592 entity pairs, of which\n10,232 pairs do not exhibit any relation.\nEvaluation Metric. We report the micro F1 score\non the test dataset. We exclude the NA instances\nwhen calculating the F1 score.\nModels in Comparison. We compare GEOLM\nwith other pretrained language models, including\nBERT, RoBERTa, and SpanBERT, as well as a\nlarge language model (LLM), GPT-3.5.\nResults and Discussion. Fig. 5 shows that GE-\nOLM achieves the best F1. The results suggest that\nGEOLM effectively captures the topological rela-\ntionships between entities. Furthermore, GEOLM\ndemonstrates a better ability to learn geospatial\ngrounding between the mentioned entities.\n3.5\nAblation Study\nWe conduct two ablation experiments to validate\nthe model design using toponym recognition and\ntoponym linking: 1) removing the spatial coordi-\nnate embedding layer that takes the geo coordinates\nas input during the training; 2) removing the con-\ntrastive loss that encourages the model to learn\nsimilar geo-entity embeddings from two types of\ncontext (i.e., linguistic context and geospatial con-\ntext) and only applying MLM on the NL corpora.\nFor the toponym recognition task, we compare\nthe entity-level precision, recall, and F1 scores on\nthe GeoWebNews dataset. Tab. 4 shows that re-\nmoving either component could cause performance\ndegradation.\nFor the toponym linking task, in\nthe first ablation experiment, the linking accuracy\n(P@D161, i.e., Acc@161km or Acc@100miles)\ndrops from 0.358 to 0.321 after removing the spa-\ntial coordinate embedding, indicating that the geo-\ncoordinate information is beneficial and our em-\nbedding layer design is effective. In the second ab-\nlation experiment, the linking accuracy (P@D161)\ndrops from 0.358 to 0.146, showing that contrastive\nlearning is essential.\n4\nRelated Work\nGeospatial NLU. Understanding geospatial con-\ncepts in natural language has long been a topic of\ninterest. Previous studies (Liu et al., 2022; Hu,\n2018; Wang and Hu, 2019) have used general-\npurpose NER tools such as Stanford NER (Finkel\net al., 2005) and NeuroNER (Dernoncourt et al.,\n2017) to identify toponyms in text. In the geo-\ngraphic information system (GIS) domain, tools\nsuch as the Edinburgh geoparser (Grover et al.,\n2010; Tobin et al., 2010a), Yahoo! Placemaker10\nand Mordecai (Halterman, 2017, 2023) have been\ndeveloped to detect toponyms and link them to\ngeographical databases. Also, several heuristics-\nbased approaches have been proposed (Woodruff\nand Plaunt, 1994; Amitay et al., 2004; Tobin et al.,\n2010b) to limit the spatial range of gazetteers\nand associate the toponym with the most likely\ngeo-entity (e.g., most populous ones). More re-\ncently, deep learning models have been adopted\nto establish the connection between extracted to-\nponyms and geographical databases (Gritta et al.,\n2017, 2018a; DeLozier et al., 2015). For instance,\nTopoCluster (DeLozier et al., 2015) learns the as-\nsociation between words and geographic locations,\nderiving a geographic likelihood for each word in\nthe vocabulary. Similarly, CamCoder (Gritta et al.,\n2018a) introduces an algorithm that encodes to-\nponym mentions in a geodesic vector space, predict-\ning the final location based on geodesic and lexical\nfeatures. These models utilize supervised training,\nwhich assumes that the testing and training data\ncover the same region. However, this assumption\nmay limit their applicability in scenarios where the\ntesting and training regions differ. Furthermore, Yu\n10Yahoo!\nplacemaker: https://simonwillison.net/\n2009/May/20/placemaker/\nand Lu (2015) use keyword extraction approach\nand Yang et al. (2022) use language model based\n(e.g., BERT) classification to solve the geospatial\nrelation extraction problem. However, these mod-\nels often struggle to incorporate the geospatial con-\ntext of the text during inference. Our previous work\nSpaBERT (Li et al., 2022b) is related to GEOLM\nin terms of representing the geospatial context. It is\na language model trained on geographical datasets.\nAlthough SpaBERT can learn geospatial informa-\ntion, it does not fully employ linguistic information\nduring inference.\nGEOLM is specifically designed to align the lin-\nguistic and geospatial context within a joint embed-\nding space through MLM and contrastive learning.\nLanguage Grounding. Language grounding in-\nvolves mapping the NL component to other modal-\nities, and it encompasses several areas of research.\nIn particular, vision-language grounding has gained\nsignificant attention, with popular approaches in-\ncluding contrastive learning (Radford et al., 2021;\nJia et al., 2021; You et al., 2022; Li et al., 2022a)\nand (masked) vision-language model on distantly\nparallel multi-modal corpora (Chen et al., 2020,\n2021; Su et al., 2020; Li et al., 2020). Addition-\nally, knowledge graph grounding has been explored\nwith similar strategies (He et al., 2021; Wang et al.,\n2021). GEOLM leverages both contrastive learn-\ning and MLM on distantly parallel geospatial and\nlinguistic data, and it represents a pilot study on the\ngrounded understanding of these two modalities.\n5\nConclusion\nIn this paper, we propose GEOLM, a PLM for\ngeospatially grounded language understanding.\nThis model can handle both natural language in-\nputs and geospatial data inputs, and provide con-\nnected representation for both modalities. Tech-\nnically, GEOLM conducts contrastive and MLM\npretraining on a massive collection of distantly par-\nallel language and geospatial corpora, and incorpo-\nrates a geocoordinate embedding mechanism for\nimproved spatial characterization. Through evalua-\ntions on four important downstream tasks, toponym\nrecognition, toponym linking, geo-entity typing\nand geospatial relation extraction, GEOLM has\ndemonstrated competent and comprehensive abili-\nties for geospatially grounded NLU. In the future,\nwe plan to extend the work to question answering\nand autoregressive tasks.\nLimitations\nThe current version of our model only uses point ge-\nometry to represent the geospatial context, ignoring\npolygons and polylines. Future work can expand\nthe model’s capabilities to handle those complex\ngeometries. Also, it is important to note that the\nOpenStreetMap data were collected through crowd-\nsourcing, which introduces possible labeling noise\nand bias. Lastly, model pre-training was conducted\non a GPU with at least 24GB of memory. Attempt-\ning to train the model on GPUs with smaller mem-\nory may lead to memory constraints and degraded\nperformance.\nEthics Statement\nThe model weights in our research are initialized\nfrom a pretrained BERT model for English. In ad-\ndition, the training data used in our research are\nprimarily extracted from crowd-sourced databases,\nincluding OpenStreetMap (OSM), Wikipedia, and\nWikidata. Although these sources provide exten-\nsive geographical coverage, the geographical distri-\nbution of training data exhibits significant dispar-\nities, with Europe having the most abundant data.\nAt the same time, Central America and Antarctica\nare severely underrepresented, with less than 1% of\nthe number of samples compared to Europe. This\nuneven training data distribution may introduce bi-\nases in the model’s performance, particularly in\nregions with limited annotated samples.\nAcknowledgements\nWe appreciate the reviewers for their insightful\ncomments and suggestions. We thank the Min-\nnesota Supercomputing Institute (MSI) for provid-\ning resources that contributed to the research re-\nsults reported in this article. Zekun Li and Yao-Yi\nChiang were supported by the University of Min-\nnesota Computer Science & Engineering Faculty\nstartup funds. Wenxuan Zhou and Muhao Chen\nwere supported by the NSF Grant IIS 2105329, the\nNSF Grant ITE 2333736, and the DARPA MCS\nprogram under Contract No. N660011924033 with\nthe United States Office of Naval Research, a Cisco\nResearch Award, two Amazon Research Awards,\nand a Keston Research Award.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nEinat Amitay, Nadav Har’El, Ron Sivan, and Aya Soffer.\n2004. Web-a-where: geotagging web content. In\nProceedings of the 27th annual international ACM\nSIGIR conference on Research and development in\ninformation retrieval, pages 273–280.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nJiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang,\nand Changhu Wang. 2021. Learning the best pooling\nstrategy for visual semantic embedding. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 15789–15798.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part XXX, pages 104–\n120. Springer.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2020. Autoregressive entity retrieval.\narXiv preprint arXiv:2010.00904.\nGrant DeLozier, Jason Baldridge, and Loretta London.\n2015.\nGazetteer-independent toponym resolution\nusing geographic word profiles. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 29.\nFranck Dernoncourt, Ji Young Lee, and Peter Szolovits.\n2017. Neuroner: an easy-to-use program for named-\nentity recognition based on neural networks. arXiv\npreprint arXiv:1705.05487.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nStella Douka, Hadi Abdine, Michalis Vazirgiannis, Ra-\njaa El Hamdani, and David Restrepo Amariles. 2021.\nJuriBERT: A masked-language model adaptation for\nFrench legal text. In Proceedings of the Natural Le-\ngal Language Processing Workshop 2021, pages 95–\n101, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJenny Rose Finkel, Trond Grenager, and Christopher D\nManning. 2005. Incorporating non-local informa-\ntion into information extraction systems by gibbs\nsampling. In Proceedings of the 43rd annual meet-\ning of the association for computational linguistics\n(ACL’05), pages 363–370.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nMilan Gritta, Mohammad Taher Pilehvar, and Nigel Col-\nlier. 2018a. Which Melbourne? augmenting geocod-\ning with maps. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1285–1296,\nMelbourne, Australia. Association for Computational\nLinguistics.\nMilan Gritta, Mohammad Taher Pilehvar, and Nigel Col-\nlier. 2020. A pragmatic guide to geoparsing evalua-\ntion: Toponyms, named entity recognition and prag-\nmatics. Language resources and evaluation, 54:683–\n712.\nMilan Gritta, Mohammad Taher Pilehvar, Nut Lim-\nsopatham, and Nigel Collier. 2017. Vancouver wel-\ncomes you! minimalist location metonymy resolu-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1248–1259, Vancouver,\nCanada. Association for Computational Linguistics.\nMilan Gritta, Mohammad Taher Pilehvar, Nut Lim-\nsopatham, and Nigel Collier. 2018b. What’s missing\nin geographical parsing? Language Resources and\nEvaluation, 52:603–623.\nClaire Grover, Richard Tobin, Kate Byrne, Matthew\nWoollard, James Reid, Stuart Dunn, and Julian Ball.\n2010. Use of the edinburgh geoparser for georefer-\nencing digitized historical collections. Philosophical\nTransactions of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences, 368(1925):3875–\n3889.\nYanzhu Guo, Virgile Rennard, Christos Xypolopoulos,\nand Michalis Vazirgiannis. 2021. BERTweetFR : Do-\nmain adaptation of pre-trained language models for\nFrench tweets. In Proceedings of the Seventh Work-\nshop on Noisy User-generated Text (W-NUT 2021),\npages 445–450, Online. Association for Computa-\ntional Linguistics.\nAndrew Halterman. 2017. Mordecai: Full text geop-\narsing and event geocoding. The Journal of Open\nSource Software, 2(9).\nAndrew Halterman. 2023.\nMordecai 3:\nA neu-\nral geoparser and event geocoder. arXiv preprint\narXiv:2303.13675.\nLei He, Suncong Zheng, Tao Yang, and Feng Zhang.\n2021. KLMo: Knowledge graph enhanced pretrained\nlanguage model with fine-grained relationships. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 4536–4542, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nBo-June Hsu and Giuseppe Ottaviano. 2013. Space-\nefficient data structures for top-k completion.\nIn\nProceedings of the 22nd international conference on\nWorld Wide Web, pages 583–594.\nXuke Hu, Yeran Sun, Jens Kersten, Zhiyong Zhou,\nFriederike Klan, and Hongchao Fan. 2023. How\ncan voting mechanisms improve the robustness and\ngeneralizability of toponym disambiguation? Inter-\nnational Journal of Applied Earth Observation and\nGeoinformation, 117:103191.\nXuke Hu, Zhiyong Zhou, Hao Li, Yingjie Hu, Fuqiang\nGu, Jens Kersten, Hongchao Fan, and Friederike\nKlan. 2022. Location reference recognition from\ntexts: A survey and comparison.\narXiv preprint\narXiv:2207.01683.\nYingjie Hu. 2018. Eupeg: Towards an extensible and\nunified platform for evaluating geoparsers. In Pro-\nceedings of the 12th Workshop on Geographic Infor-\nmation Retrieval, pages 1–2.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision.\nIn International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020.\nSpan-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020. What does BERT\nwith vision look at? In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5265–5275, Online. Association\nfor Computational Linguistics.\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang,\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.\n2022a. Grounded language-image pre-training. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10965–\n10975.\nZekun Li, Jina Kim, Yao-Yi Chiang, and Muhao Chen.\n2022b.\nSpaBERT: A pretrained language model\nfrom geographic data for geo-entity representation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2757–2769, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMichael D Lieberman, Hanan Samet, and Jagan\nSankaranarayanan. 2010. Geotagging with local lexi-\ncons to build indexes for textually-specified spatial\ndata. In 2010 IEEE 26th international conference\non data engineering (ICDE 2010), pages 201–212.\nIEEE.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZilong Liu, Krzysztof Janowicz, Ling Cai, Rui Zhu,\nGengchen Mai, and Meilin Shi. 2022. Geoparsing:\nSolved or biased? an evaluation of geographic biases\nin geoparsing. AGILE: GIScience Series, 3:9.\nInderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-\nman, Rob Quimby, Justin Richer, Ben Wellner, Scott\nMardis, and Seamus Clancy. 2010. Spatialml: anno-\ntation scheme, resources, and evaluation. Language\nResources and Evaluation, 44:263–280.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nPaul Röttger and Janet Pierrehumbert. 2021. Temporal\nadaptation of BERT and performance on downstream\ndocument classification: Insights from social media.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2400–2412, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-training\nof generic visual-linguistic representations. In Inter-\nnational Conference on Learning Representations.\nJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan\nRitter. 2020. Code and named entity recognition in\nStackOverflow. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4913–4926, Online. Association for\nComputational Linguistics.\nRichard Tobin, Claire Grover, Kate Byrne, James Reid,\nand Jo Walsh. 2010a. Evaluation of georeferencing.\nIn Proceedings of the 6th Workshop on Geographic\nInformation Retrieval, GIR ’10, New York, NY, USA.\nAssociation for Computing Machinery.\nRichard Tobin, Claire Grover, Kate Byrne, James Reid,\nand Jo Walsh. 2010b. Evaluation of georeferencing.\nIn proceedings of the 6th workshop on geographic\ninformation retrieval, pages 1–8.\nJan Oliver Wallgrün, Morteza Karimzadeh, Alan M\nMacEachren, and Scott Pezanowski. 2018. Geocor-\npora: building a corpus to test and train microblog\ngeoparsers. International Journal of Geographical\nInformation Science, 32(1):1–29.\nJimin Wang and Yingjie Hu. 2019. Enhancing spatial\nand textual analysis with eupeg: An extensible and\nunified platform for evaluating geoparsers. Transac-\ntions in GIS, 23(6):1393–1419.\nJimin Wang, Yingjie Hu, and Kenneth Joseph. 2020.\nNeurotpr: A neuro-net toponym recognition model\nfor extracting locations from social media messages.\nTransactions in GIS, 24(3):719–735.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176–194.\nAllison Gyle Woodruff and Christian Plaunt. 1994.\nGipsy: Automated geographic indexing of text docu-\nments. Journal of the American Society for Informa-\ntion Science, 45(9):645–655.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442–6454, On-\nline. Association for Computational Linguistics.\nJiannan Yang, Hong Jia, and Hanbing Liu. 2022. Spatial\nrelationship extraction of geographic entities based\non bert model. In Journal of Physics: Conference\nSeries, volume 2363, page 012031. IOP Publishing.\nHaoxuan You, Luowei Zhou, Bin Xiao, Noel Codella,\nYu Cheng, Ruochen Xu, Shih-Fu Chang, and\nLu Yuan. 2022.\nLearning visual representation\nfrom modality-shared contrastive language-image\npre-training. In Computer Vision–ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23–\n27, 2022, Proceedings, Part XXVII, pages 69–87.\nSpringer.\nLi Yu and Feng Lu. 2015. A bootstrapping algorithm\nfor geo-entity relation extraction from online ency-\nclopedia. In 2015 23rd International Conference on\nGeoinformatics, pages 1–5. IEEE.\nA\nDistribution of Geo-entities\nWe analyze the distribution of geo-entities in the\npretraining corpora and the downstream datasets.\nWhen considering only the name (without geoco-\nordinates), the overlapping percentages of the pre-\ntraining data and downstream datasets are shown in\nTab. 5. Ablation experiments show that for the geo-\nentities already included during the pretraining, the\naverage P@D161 score is 0.362. For the ones that\nare not included during the pretraining, the average\nP@D161 score is 0.341, which is not significantly\ndifferent from the prior one. This indicates that the\nimproved performance of GEOLM comparing with\nother models benefits from enhancing the geospa-\ntial representations.\nB\nComparison with Other Methods\nFor the toponym recognition task, we compare\nthe results of GEOLM with other existing mod-\nels designed specifically for this problem.\nAc-\ncording to (Gritta et al., 2020), the token-level F1\nachieved by Yahoo! Placemaker, Edinburgh Geop-\narser, Spacy NLP, Google Cloud Natural Language,\nand NCRF++ are 63.2%, 63.6%, 74.9%, 83.2%\nand 88.6% respectively. GEOLM has a token-level\nF1 of 86.3%, which is better than all existing ones\nexcept NCRF++. The reason is that NCRF++ uses\nfine-grained toponym taxonomy to boost the to-\nponym recognition performance. However, the fine-\ngrained labels can sometimes be difficult to collect\nfor large-scale datasets. In addition, NCRF++ is a\nspecific toponym recognition model that does not\nsupport other geospatial-inference tasks. With GE-\nOLM, we can generate representations useful for\nvarious tasks.\nFor the toponym linking task, we compare our\nmodel with the other existing geoparser models\nmentioned in EUPEG (Hu, 2018) and Voting (Hu\net al., 2023), including CLAVIN11, TopoClus-\nter (DeLozier et al., 2015), CamCoder (Gritta\net al., 2018a), Modecai (Halterman, 2017), GENRE\n(De Cao et al., 2020), and Voting (Hu et al., 2023).\nSince EUPEG evaluates the toponym resolution\nand toponym linking together and does not provide\n11CLAVIN:https://github.com/Novetta/CLAVIN\nFigure 6: Example of the constructed Trie if Open-\nStreetMap only contains “Los Angeles”, “Los Angeles\nCounty”, “Los Angeles Elementary School”, “Los Ca-\nbos” and “Las Vegas”. Each node is a word in a place\nname. The gray color indicates that this node is the last\nword of a place name.\nthe scores for linking only, we use the scores re-\nported in Voting, which assumes gold toponyms as\ninputs for toponym linking (same as ours).\nThe scores in P@D161 (or Accuracy@161km)\nare shown in Tab. 6. The models that perform better\nthan GEOLM are all supervised learning models\n(i.e. CamCoder, GENRE and Voting) while GE-\nOLM is unsupervised. Within the unsupervised\ngroup, GEOLM performs the best. The benefit\nof the unsupervised nature of GEOLM is that GE-\nOLM can handle new samples without the need for\nextra training data, which is often difficult to gather.\nAlso, new geo-entity names can appear in docu-\nments and the way people call the same geo-entity\ncan change over time, the unsupervised approach\nhas the advantage of handling ever-changing doc-\numents, e.g., online text, while the supervised ap-\nproach focuses on existing names of geo-entities.\nWe acknowledge that there is a gap between GE-\nOLM and the domain-specific geoparsers, and we\nwill aim to narrow this gap in the future.\nC\nTrie\nTrie supports a tree structure for efficient search\nfor geo-entity names, and it helps distinguish be-\ntween two geo-entities with shared substrings in\ntheir names. For example, Trie helps extract the\ngeo-entity “Los Angeles High School” from the\nsentence “I work at the Los Angeles High School,”\ninstead of extracting the geo-entity “Los Angeles”.\nFig. 6 shows an example Trie constructed from a\nset of geo-entity names.\nWe use all geo-entity names in the worldwide\nOpenStreetMap database to construct a worldwide\nTrie, where each node is a single word in the name.\nWhen using Trie to preprocess the Wikipedia doc-\numents, we apply the Trie searching to find all\nTasks\nDataset\n# Total Records # Intersection Percentage\nToponym Recognition GeoWebNews\n912\n325\n35.6%\nToponym Linking\nWikToR\n1886\n1280\n67.9%\nGeo-entity Typing\nOpenStreetMap Subset\n23195\n544\n2.3%\nRelation Extraction\nSpatialML\n309\n91\n29.4%\nTable 5: The number of geo-entities in the downstream datasets, and the overlapping portion of geo-entities with\nthe pretraining corpora.\nUnsupervised\nSupervised\nModecai\n0.15 CamCoder 0.63\nCLAVIN\n0.22\nGENRE\n0.81\nTopoCluster\n0.24\nVoting\n0.85\nGEOLM (ours) 0.35\nTable 6: Comparison with other toponym linking mod-\nels in both unsupervised and supervised category.\nTypes\nKeywords\nOthers\nOther; No relation\nEqual (EQ)\nEQ\nDisconnected (DC)\nDC\nExternally connected (EC)\nEC\nWithin (IN)\nIn; Within\nPartially overlapping (PO)\nPO\nTable 7: Relation types and the corresponding accept-\nable keywords in GPT outputs\nthe mentioned geo-entity names. To mitigate the\ndisambiguation error, we use the Wikipedia page\ntitle to filter out the mentions that do not describe\nthe “entity-of-interest”. After this step, the dis-\nambiguation error only occurs when two distinct\ngeo-entities with the same name occur on the same\nWikipedia page, which is pretty rare. This does\nnot fully resolve the disambiguation error issue,\nand there may still be noises in the training data.\nHowever, as long as most of the data is clean,\nthe model can still learn meaningful information\nfrom the data. Thus we have compiled a quite\nlarge pretraining NL corpora with 1,458,150 sen-\ntences/paragraphs describing 472,067 geo-entities.\nD\nGPT-3.5 Prompt\nWe use GPT-3.5-turbo to help predict the rela-\ntions between two geo-entities. With the system\nrole, we prompt the model with the general task\ndescription and ask the model to choose from one\nof the possible relationships. With the user role, we\nprovide the input sentence and the two geo-entity\nnames. The example prompt is shown below.\n• System:Given a sentence, and two entities\nwithin the sentence, classify the relationship\nbetween the two entities based on the pro-\nvided sentence. All possible Relationships are\nlisted below: [ disconnected (DC): Entity A\nand B have no spatial intersection, both in\nterms of interiors and boundaries; externally\nconnected (EC): Entity A and B touch each\nother only at their boundaries; equal (EQ):\nEntity A and B are identical; partially overlap-\nping (PO): Interiors of entity A and B overlap\nbut neither is completely contained within the\nother; within (IN): One entity is part of the\nother entity; Others: No relation between en-\ntities]\n• User: Sentence: {input-sentence} Entity1:\n{entity1-name} Entity2: {entity2-name} Re-\nlationship:\nTo address the randomness in the GPT outputs\nand robustly evaluate the performance, we only\nlook for some particular keywords from the GPT\noutputs. If the output contains the desired keyword,\nwe consider the prediction as correct. Tab. 7 lists\nthe keywords for each relation type.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-10-23",
  "updated": "2023-10-23"
}