{
  "id": "http://arxiv.org/abs/2310.15269v1",
  "title": "GradSim: Gradient-Based Language Grouping for Effective Multilingual Training",
  "authors": [
    "Mingyang Wang",
    "Heike Adel",
    "Lukas Lange",
    "Jannik Strötgen",
    "Hinrich Schütze"
  ],
  "abstract": "Most languages of the world pose low-resource challenges to natural language\nprocessing models. With multilingual training, knowledge can be shared among\nlanguages. However, not all languages positively influence each other and it is\nan open research question how to select the most suitable set of languages for\nmultilingual training and avoid negative interference among languages whose\ncharacteristics or data distributions are not compatible. In this paper, we\npropose GradSim, a language grouping method based on gradient similarity. Our\nexperiments on three diverse multilingual benchmark datasets show that it leads\nto the largest performance gains compared to other similarity measures and it\nis better correlated with cross-lingual model performance. As a result, we set\nthe new state of the art on AfriSenti, a benchmark dataset for sentiment\nanalysis on low-resource African languages. In our extensive analysis, we\nfurther reveal that besides linguistic features, the topics of the datasets\nplay an important role for language grouping and that lower layers of\ntransformer models encode language-specific features while higher layers\ncapture task-specific information.",
  "text": "GradSim: Gradient-Based Language Grouping\nfor Effective Multilingual Training\nMingyang Wang1,2\nHeike Adel3\nLukas Lange1\nJannik Strötgen4\nHinrich Schütze2\n1Bosch Center for Artificial Intelligence, Renningen, Germany\n2LMU Munich, Germany\n3Hochschule der Medien, Stuttgart, Germany\n4Karlsruhe University of Applied Sciences, Germany\nmingyang.wang2@de.bosch.com\nAbstract\nMost languages of the world pose low-resource\nchallenges to natural language processing mod-\nels. With multilingual training, knowledge can\nbe shared among languages. However, not all\nlanguages positively influence each other and\nit is an open research question how to select\nthe most suitable set of languages for multilin-\ngual training and avoid negative interference\namong languages whose characteristics or data\ndistributions are not compatible. In this pa-\nper, we propose GradSim, a language grouping\nmethod based on gradient similarity. Our ex-\nperiments on three diverse multilingual bench-\nmark datasets show that it leads to the largest\nperformance gains compared to other similar-\nity measures and it is better correlated with\ncross-lingual model performance. As a result,\nwe set the new state of the art on AfriSenti, a\nbenchmark dataset for sentiment analysis on\nlow-resource African languages. In our exten-\nsive analysis, we further reveal that besides lin-\nguistic features, the topics of the datasets play\nan important role for language grouping and\nthat lower layers of transformer models encode\nlanguage-specific features while higher layers\ncapture task-specific information.\n1\nIntroduction\nMost natural language processing (NLP) research\ntoday still focuses on a small number of languages.\nExtending NLP models to further languages poses\ndifferent challenges, i.a., little (annotated) data\n(Hedderich et al., 2021). Multilingual training can\nhelp in those cases by sharing knowledge across\nlanguages. However, adding new languages to the\nmultilingual training set may not necessarily lead to\nperformance gains. In fact, certain languages might\nactually hurt the performance on downstream tasks\nin a specific target language (Adelani et al., 2022;\nSnæbjarnarson et al., 2023), for instance, due to\nunrelatedness to the target language.\nFigure 1:\nExemplary transfer learning setup with\nAfrican languages: The motivation for this work is\nthat neither language family (indicated by node col-\nors) nor typological distance (indicated by distance of\nlanguages in the plot) are consistent predictors of good\nperformance when choosing a source language for cross-\nlingual transfer. Red cross: source language affecting\nperformance negatively. Green tick: source language\naffecting performance positively.\nAs a solution, previous work investigates dif-\nferent measures for language similarity and selects\nonly languages similar to the target language for the\nmultilingual training set (i.a. Tan et al., 2019; Lin\net al., 2019; Pires et al., 2019; Oncevay et al., 2020;\nShaffer, 2021; Snæbjarnarson et al., 2023). How-\never, it is an open research question whether lan-\nguage similarity translates into performance gains\nof multilingual models. For multilingual training,\nother characteristics might play a role, such as top-\nical shifts of the training data. As a result, it is still\nunclear how to select the set of languages that leads\nto the most effective multilingual training setup.\nIn this paper, we study multilingual fine-tuning\nof language models with a diverse set of training\nlanguages.1 In particular, we show that linguistics-\nbased language similarities are only weakly corre-\nlated with cross-lingual transfer performance. Fig-\nure 1 illustrates a sample case in which neither\n1Note that our proposed method is generic and could also\nbe applied for multilingual pre-training.\narXiv:2310.15269v1  [cs.LG]  23 Oct 2023\nlanguage family information (indicated by node\ncolors) nor similarity of language embeddings (in-\ndicated by proximity in the vector space) is helpful\nfor finding languages that have a positive cross-\nlingual transfer score with the target language.\nThus, prior information about the languages, such\nas their language families or typological features,\nalone is not enough for an effective multilingual\ntraining. Instead, similarity measures that capture\nadditional information about the data and task be-\nyond linguistics similarity may achieve better per-\nformance. Wang et al. (2020), for instance, show\nthat gradient similarity across languages measured\nalong the optimization trajectory correlates with\nlanguage proximity and cross-lingual performance.\nWe draw inspiration from this observation. How-\never, instead of projecting conflicting gradients\nthroughout the training process, we propose to\nleverage the gradient similarity to group languages\nwith a branch-and-bound-like algorithm that opti-\nmizes the overall similarity score of all languages.\nThis approach has the following advantages: (i) It\ncan be applied without any prior knowledge of the\nlanguages or topics of the given datasets, (ii) it is\nwell correlated with downstream task performance\nof the multilingual model, (iii) it finds the best\nlanguage groups from a global perspective, i.e., in-\nstead of selecting source languages independently\nof each other (which may create groups of mutually\ninterfering languages), we form each group based\non a criterion that evaluates the group as a whole.\nIn our experiments, we show the superior perfor-\nmance of our grouping method compared to various\nbaseline approaches on three multilingual datasets\nwith different tasks and set the new state of the\nart on AfriSenti, a sentiment analysis dataset in 12\nlow-resource African languages.\nFurthermore, we extensively analyze our models\nwith a topic analysis, a correlation-based analy-\nsis and an ablation study, revealing important in-\nsights, for instance that the topic distribution of\nthe training data heavily affects multilingual train-\ning and that lower layers of transformer models\nencode language-specific features while higher lay-\ners capture task-specific information. This con-\nfirms results from prior work (i.a., Raganato and\nTiedemann, 2018; Jawahar et al., 2019; Tenney\net al., 2019; Kovaleva et al., 2019) from another\n(correlation-based) perspective.\nThe code base for GradSim is available online.2\n2https://github.com/boschresearch/gradsim\n2\nRelated Work\nMultilingual and multi-task training.\nA grow-\ning number of research projects investigates mul-\ntilingual training to cover a variety of languages,\nincluding low-resource languages (Hu et al., 2020;\nLange et al., 2020; Hedderich et al., 2021; FitzGer-\nald et al., 2022). In the context of low-resource\nsentiment analysis, Wang et al. (2023) recently use\nthe cross-lingual transfer score between pairs of lan-\nguages to select source languages for multilingual\ntraining. Our approach differs from these works\nin that we investigate language interactions from a\nglobal optimization perspective.\nConsidering each language as a separate task,\nmultilingual training can be treated as a multi-task\nlearning (MTL) problem (Ruder, 2017). A line of\nexisting work utilizes gradient-based techniques\nto improve multi-task learning (Chen et al., 2018;\nSener and Koltun, 2018; Yu et al., 2020; Wang\net al., 2020). They show that negative cosine sim-\nilarity between gradients leads to negative inter-\nference for MTL optimization, and projecting out\nthe conflicting gradients can improve the optimiza-\ntion dynamics. Our work follows this insightful\nobservation. However, in contrast to their work, we\npropose to leverage multilingual gradients for lan-\nguage grouping to ensure that gradients are aligned\nin each language group.\nLanguage similarity measures.\nIn order to\ngroup languages for multilingual training or trans-\nfer learning, related work has proposed different\nways to estimate the similarity between languages,\ne.g., leveraging the language family taxonomy (Tan\net al., 2019; Shaffer, 2021; Chronopoulou et al.,\n2023; Snæbjarnarson et al., 2023) or represent-\ning languages as information-rich vectors based\non their typological or conceptual features (Littell\net al., 2017; Lin et al., 2019; Oncevay et al., 2020;\nLiu et al., 2023).\nAnother line of works measures language simi-\nlarity based on embeddings from multilingual pre-\ntrained language models (mPLMs) (Raganato and\nTiedemann, 2018; Lange et al., 2021b; Chang et al.,\n2022; Lin et al., 2023). Tan et al. (2019) and Shaf-\nfer (2021), for instance, perform language grouping\nfor multilingual named entity recognition and neu-\nral machine translation based on embeddings. In\ncontrast to these studies, we propose to use the gra-\ndient cosine similarity between languages as the\nsimilarity measure for language grouping. This\nmodel-based similarity measure reflects how each\nlanguage interacts in the optimization process, with\nno need of any prior knowledge of the languages.\n3\nMethod\nIn this section, we describe our proposed language\ngrouping approach and the general multilingual\ntraining in which we apply its results. Note that\nthe gradient-based similarity estimation is purely\nmodel-based, thus, can be applied to other settings,\ne.g., multi-domain or multi-task problems, as well.\n3.1\nStep I: Gradient Similarities\nDue to the high discrepancy among languages,\nmultilingual optimization often suffers from the\nconflicting gradient issue (Wang et al., 2020; Xu\nand Murray, 2022), i.e., gradients of different lan-\nguages point into different directions. Previous\nworks show that gradient similarity is correlated\nwith model performance (Chen et al., 2018; Sener\nand Koltun, 2018; Wang et al., 2020; Yu et al.,\n2020). Inspired by this observation, we propose to\nuse gradient similarity for grouping languages.\nGiven a set of languages L = {l1, l2, . . . , lN},\nwe study the gradient similarities across languages\nby training a multilingual model jointly on all\nlanguages and measure the language gradients\nG = {g1, g2, . . . , gN} along the optimization pro-\ncess. To reduce computational costs, we average\nlanguage gradients first at the epoch level and cal-\nculate the per-epoch gradient cosine similarity be-\ntween languages. Then we average the gradient\nsimilarity over all epochs. Finally, we get a gradient\nsimilarity matrix S ∈RN×N across N languages,\nwith si,j = cos(gi, gj) =\ngi·gj\n|gi||gj|.\nSince it is very expensive to calculate the gra-\ndient similarity based on the gradients w.r.t. all\nparameters, we choose to only use the gradients\nbased on the classification layer of the model. An\nanalysis of gradients of different layers and abla-\ntions studies can be found in Sections 5.2 and 5.3.\n3.2\nStep II: Language Grouping\nBased on the pairwise similarity matrix S from\nStep I, we next determine the best grouping into a\npre-defined number of K groups.\nIn particular, our goal is to find the K language\ngroups which (i) cover all languages of the given\nlanguage set L, and (ii) maximize the overall simi-\nlarity score of all languages, which is a reduction\nfrom the Set-Cover problem. We solve it using the\nbranch-and-bound-like algorithm as in Standley\net al. (2020) and Fifty et al. (2021).3 The algorithm\nevaluates different combinations of K language\ngroups under the constraint that each language is\nincluded in at least one, but potentially multiple\ngroups. We finally select the language grouping\nthat leads to the highest overall similarity score.\nGiven Γ = {γ1, . . . , γK} as a potential grouping\nresult, we define the overall similarity score for Γ as\nPN\ni=1 Sim(li|Γ) where Sim(li|Γ) is the collective\nsimilarity score of language li in its language group\nγj ∈Γ. The collective similarity score of li ∈γj is\ndefined as the average of all pair-wise similarities\nbetween li and the other languages in γj.\n3.3\nSteps III : Training and Inference\nGiven the language groups Γ from Step II, we train\none multilingual model per group γj ∈Γ, using\nthe training data from the respective languages. For\ninference, we select the appropriate multilingual\nmodel for each target language and apply it to the\ntest data. If a target language li appears in more\nthan one group, we select the group with the highest\ncollective similarity score of li for inference.\n4\nExperiments\nIn this section, we describe our experimental set-\ntings as well as our results for three tasks.\n4.1\nTasks and Datasets\nWe experiment with the following three datasets\ncovering different languages as well as text classifi-\ncation and sequence tagging tasks. (Dataset statis-\ntics are given in Table 8 and 9 in Appendix A.)\nAfriSenti (Muhammad et al., 2023a,b): This\nshared task dataset provides a challenging testbed\nfor sentiment analysis: Both the languages (12\nAfrican languages) and the text genre (Twitter)\npose challenges to NLP models. To investigate\nmultilingual training results, we focus on the mul-\ntilingual subtask of the shared task (Subtask B),\nand report macro-weighted F1 scores following\nMuhammad et al. (2023b).\nWikiAnn (Pan et al., 2017): This dataset of-\nfers automatically extracted labels for named entity\nrecognition (NER). Following Shaffer (2021), we\nselect 15 languages for our experiments and use\nmicro-averaged F1 as the evaluation metric.\n3Alternatively, the binary integer program (BIP) solver\ncould be used as in Zamir et al. (2018).\nⅠ - Measure Gradient Similarities \nacross Languages \nⅡ – Language Grouping\nⅢ – Train LMs for Each \nLanguage Group\ndz\npcm\nts\ntwi\nam\nha\nkr\nma\npt\nsw\n…\nam\nha\nkr\nma\npt\nsw\nam\ndz\nha\nig\nyo\n…\nSimilarity Matrix\nLanguage Groups\nig\nyo\ndz\npcm\nts\ntwi\nGroup 1\nGroup 2\nBranch-and-bound \nalgorithm\nFigure 2: Overview of our proposed method GradSim for language grouping. (Step I) We train all languages in\none multilingual model to measure the gradient similarities across languages. (Step II) We determine the best K\nlanguage groups based on the similarity measure from the first step. (Step III) We train one language model on each\nlanguage group and deploy it for inference.\nUniversal Dependency (UD) treebank v1.2\n(Nivre et al., 2016): We experiment with part-of-\nspeech (POS) tagging using the 17 Universal POS\nlabels. Following prior work (Yasunaga et al., 2018;\nLange et al., 2021a), we use 27 languages from the\ndataset with 21 high-resource and 6 low-resource\nlanguages and report accuracy for evaluation.\n4.2\nTraining Details\nFor our experiments on AfriSenti, we use the pre-\ntrained AfroXLM-R large transformer (Alabi et al.,\n2022), an XLM-R model adapted to African lan-\nguages, as our base model. To measure language\ngradients in Step I, we use 25% of the training data\nin AfriSenti and set the batch size to 8 for compu-\ntational efficiency. For multilingual training and\ninference (Step III ), we use all training data and\na batch size of 32. In both stages, we finetune the\nmodel with a learning rate of 1e-5 and set the maxi-\nmum sequence length to 128. We set the number of\nlanguage groups to K = 4 which equals the num-\nber of language families in the dataset. We further\nprovide a comparison of different K in Section 5.3.\nFor the NER task, we follow the training setup\nused in Shaffer (2021) for a fair comparison.\nSpecifically, we use XLM-R as our base model\nand finetune it for 3 epochs. We set the batch size\nto 20 with a learning rate of 2e-5 and a max se-\nquence length of 300. Following Shaffer (2021),\nwe set the number of language groups to K = 4.\nFor the POS tagging task, we use the XLM-R\nmodel as well and set the training epoch to 20.\nWe use a batch size of 8, a learning rate of 2e-5\nand a maximum sequence length of 128. Here,\nwe specify K = 6 for language grouping, as 6\nlanguage families are covered by the 27 languages\nwe study.\nOn all three datasets, we use the AdamW opti-\nmizer (Loshchilov and Hutter, 2017). The training\nwas performed on Nvidia A100 GPUs.4 All re-\nported results are averaged over 5 random seeds.\n4.3\nBaselines\nBesides a monolingual model (trained only on the\ntarget language) and a purely multilingual model\n(trained on all available languages), we consider\nthe following baselines for language grouping that\nhave been presented by prior work:\nLanguage family. We group languages based\non their language family information and train one\nmultilingual model per language family. Language\nfamily-based grouping is also studied by, i.a., Tan\net al. (2019); Shaffer (2021); Chronopoulou et al.\n(2023); Snæbjarnarson et al. (2023).\nTypological similarity. Languages can be rep-\nresented by typological features, e.g., the syn-\ntax, phonology or inventory features. Using the\nlang2vec tool and the URIEL knowledge base (Lit-\ntell et al., 2017), we retrieve language vectors and\nuse the pairwise distances among them as the simi-\nlarity measure of our algorithm. This is similar to\nLin et al. (2019) and Oncevay et al. (2020).\nEmbedding distance. Multilingual pretrained\nlanguage models (mPLMs) also encode language-\nspecific information (Raganato and Tiedemann,\n2018; Chang et al., 2022). Tan et al. (2019) and\nShaffer (2021) use mPLM-based language embed-\ndings to determine language similarities for lan-\nguage grouping. Following this idea, we compute\n4All experiments ran on a carbon-neutral GPU cluster.\nMethod\navg∗\nam∗\ndz\nha\nig∗\nkr∗\nma∗\npcm\npt∗\nsw∗\nts\ntwi\nyo∗\nOracle upper bound\n71.46\n69.87\n69.45\n80.15\n78.48\n70.72\n52.41\n68.67\n71.85\n61.59\n55.42\n64.79\n75.52\nMultilingual\n59.97\n51.99\n56.62\n66.10\n67.64\n61.03\n43.07\n55.70\n67.37\n58.18\n42.86\n49.94\n62.87\nMonolingual\n68.29\n49.00\n57.16\n80.36\n79.55\n70.72\n48.73\n68.24\n66.12\n62.50\n44.66\n54.56\n75.09\nLanguage family\n66.19\n67.84\n68.54\n78.88\n68.16\n61.10\n51.10\n67.32\n64.88\n58.38\n47.04\n54.60\n64.36\nTypological similarity\n68.93\n66.10\n69.27\n79.38\n78.00\n70.09\n49.17\n63.81\n66.12\n63.23\n46.79\n61.25\n73.95\nEmbedding dis. (PLM)\n68.81\n72.09\n67.20\n71.57\n74.49\n71.89\n51.49\n69.09\n67.32\n62.50\n52.01\n60.88\n75.09\nEmbedding dis. (FT)\n69.62\n59.69\n67.36\n79.78\n78.71\n69.99\n50.01\n68.46\n66.43\n62.62\n49.20\n64.01\n75.07\nGradSim (ours)\n71.34\n66.11\n67.90\n79.97\n79.55\n72.12\n53.68\n68.40\n72.30\n63.05\n50.36\n63.46\n75.09\nTable 1: Results on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. Bold\nshows best results, underline highlights second-best results per language and on average. * indicates the settings\nwith statistically significant improvements (p-value < 0.05) using GradSim compared to Embedding dis. (FT), the\noverall second-best system.\nMethod\nAverage F1\nSOTA Single model\n74.08\nGradSim+TAPT Single model (ours)\n75.29\nSOTA Ensemble\n75.06\nGradSim+TAPT Ensemble (ours)\n75.34\nTable 2: Results on AfriSenti in comparison to the state\nof the art (Wang et al., 2023). We apply task-adaptive\npretraining (TAPT) and ensemble methods on top of\nGradSim for a fair comparison to the state of the art.\nsentence embeddings using the pretrained encoder\nfrom our base model and average sentence embed-\ndings of the same language. Then, we use the em-\nbedding distance across languages as the similarity\nmeasure in Step I (denoted by Embedding distance\n(PLM)). As an alternative, we also consider embed-\ndings from the language model fine-tuned on the\ntask (denoted by Embedding distance (FT)).\nOracle upper bound.\nAs an upper bound,\nwe group languages based on the post-hoc cross-\nlingual transfer performance. The cross-lingual\ntransfer performance is often used for source lan-\nguage selection as in Adelani et al. (2022) and\nWang et al. (2023). We consider this an oracle\nupper bound as it is a direct indication of how\nknowledge learned from one language affects the\nperformance on another language. Note that this ap-\nproach is computationally expensive as it requires\nN×N transfer experiments for N languages, while\nour gradient-based approach only needs a single\ntraining run for collecting gradient information.\n4.4\nResults\nText classification.\nTable 1 shows our experimen-\ntal results on the AfriSenti dataset (per language\nand on average). While for a few languages, a\ngrouping based on our baseline approaches per-\nforms best (e.g., embedding distance for am and\npcm, or typological similarity for sw), GradSim\nperforms best or second best for most languages\nand, as a result, best on average. Its average result\ncomes very close to the oracle upper bound, which,\nin contrast to our approach, requires prior knowl-\nedge about cross-lingual transfer performance.\nWe also compare GradSim with the state-of-the-\nart method on AfriSenti (Wang et al., 2023), which\nuses AfroXLM-R with task-adaptive pretraining\n(TAPT) (Gururangan et al., 2020) and performs\ntransfer learning after selecting the best source lan-\nguages based on their cross-lingual transfer score.\nFor a direct comparison, we also apply TAPT and\nuse GradSim to group languages for multilingual\ntraining. As shown in Table 2, GradSim sets the\nnew state of the art on AfriSenti. It is superior to the\nprevious approach of Wang et al. (2023) that only\nconsiders the pairwise transfer scores, neglecting\npossible interactions of different source languages.\nInstead, GradSim maximizes the overall gradient\nsimilarities from a global perspective.\nSequence tagging.\nTable 3 provides our results\nfor multilingual named entity recognition. We re-\nport the state-of-the-art results from Shaffer (2021)\nas baseline results. Our approach GradSim out-\nperforms the prior state of the art on most high-\nresource languages and all low-resource languages,\nagain leading to the best overall results.\nOur results for POS tagging are provided in\nTable 4. GradSim outperforms multilingual and\nmonolingual training without language grouping as\nwell as language grouping based on other metrics.\nIt performs best on average over the low-resource\nlanguages as well as on average over all languages.\nGiven the results on sequence tagging tasks, we\nfind that low-resource languages benefit more from\nlanguage grouping. For high-resource languages,\nadditional training sources from other languages\nNER\nMulti.\nMono.\nFamily\nEmbed.\n(prior)\nGradSim\n(ours)\nhigh-resource\nar∗\n86.65\n85.25\n84.92\n85.25\n88.02\nhe\n84.21\n84.51\n82.47\n84.83\n84.06\nda∗\n90.00\n87.57\n89.64\n90.49\n91.65\nde\n84.42\n82.42\n84.18\n85.73\n87.27\nen\n81.97\n77.91\n81.28\n83.37\n83.31\nes∗\n89.59\n82.01\n88.87\n89.90\n90.85\nfr\n88.22\n82.83\n87.78\n89.79\n89.54\nhi∗\n87.30\n84.04\n85.51\n87.17\n88.25\nit\n92.27\n86.03\n88.60\n90.52\n90.72\nru∗\n88.32\n88.18\n87.77\n88.55\n88.70\nko\n85.97\n86.54\n84.66\n86.91\n85.92\nja∗\n71.08\n66.83\n66.83\n71.40\n75.56\nzh\n79.36\n73.66\n73.66\n79.12\n75.49\navg∗\n85.34\n82.14\n83.55\n85.62\n86.10\nlow-resource\nsw∗\n88.22\n63.30\n55.11\n90.13\n90.22\nyo∗\n77.24\n7.74\n21.81\n85.33\n86.22\navg∗\n82.73\n35.52\n38.46\n87.73\n88.22\navg (all)∗\n84.99\n75.92\n77.54\n85.90\n86.39\nTable 3: Results on WikiAnn, a NER benchmark, in\nmicro F1. The numbers of the four baseline / previous\nstate-of-the-art methods are taken from Shaffer (2021)\nand micro-averaged over the different classes. * indi-\ncates the settings with statistically significant improve-\nments using GradSim compared to Embed. (prior), the\nsecond-best system. Further baseline results are given\nin Table 10 in Appendix B for space reasons.\nhave a less prominent impact when enough in-\nlanguage training data is available. It highlights the\nvalue of multilingual learning with well-suited lan-\nguages to enhance the performance of low-resource\nlanguages, providing a key strategy for advancing\nfuture low-resource NLP research.\nSignificance tests.\nWe run permutation-based\nsignificance tests following Dror et al. (2018) with\na significance level of 0.05 between GradSim and\nthe respective second-best system on all three\ndatasets.\nIn Tables 1, 3 and 4, settings with\nstatistically significant improvements when using\nGradSim are marked with *. The results show that\nGradSim is significantly better than the second-best\nsystem in 32 out of 37 single language settings\nwhere GradSim outperforms the second-best sys-\ntem across three datasets. In addition, its average\nperformance across all languages is significantly\nbetter than the other systems on all three datasets.\n5\nAnalysis\nTo analyze the behavior of the model, we perform\nthe following analyses on the AfriSenti dataset: A\nqualitative analysis of the data in order to better un-\nMulti.\nMono.\nFamily\nEmbed.\n(FT)\nGradSim\n(ours)\nhigh-resource\nbg∗\n99.42\n99.34\n99.21\n99.38\n99.40\ncs∗\n99.00\n99.00\n98.91\n98.99\n99.01\nda∗\n98.22\n98.66\n98.12\n98.06\n98.55\nde\n94.47\n94.72\n94.43\n94.59\n94.43\nen\n97.06\n97.34\n96.88\n97.25\n97.18\nes\n97.35\n97.29\n97.18\n97.23\n97.21\neu\n95.95\n96.01\n96.01\n96.04\n96.09\nfa∗\n97.30\n97.31\n97.20\n97.35\n97.41\nfi∗\n97.51\n97.64\n97.61\n97.51\n97.70\nfr∗\n96.58\n96.48\n96.21\n96.40\n96.67\nhe∗\n97.39\n97.25\n97.25\n97.31\n97.43\nhi\n97.56\n97.62\n97.49\n97.64\n97.55\nhr\n97.66\n97.67\n97.56\n97.65\n97.57\nid\n91.16\n91.78\n91.78\n91.56\n91.20\nit∗\n98.60\n98.68\n98.45\n98.51\n98.58\nnl∗\n93.76\n93.94\n93.67\n93.80\n93.88\nno∗\n98.88\n99.03\n98.91\n98.95\n99.01\npl∗\n98.58\n98.56\n98.42\n98.47\n98.52\npt∗\n98.49\n98.51\n98.43\n98.44\n98.54\nsl∗\n98.94\n99.03\n98.90\n98.96\n99.02\nsv\n98.86\n98.71\n98.77\n98.81\n98.81\navg∗\n97.27\n97.36\n97.21\n97.28\n97.32\nlow-resource\nel∗\n98.56\n98.41\n98.20\n98.57\n98.59\net∗\n95.34\n94.76\n95.46\n95.27\n95.78\nga∗\n93.34\n92.49\n93.32\n93.10\n93.52\nhu\n96.82\n96.87\n97.01\n97.14\n96.94\nro\n95.74\n94.65\n95.32\n95.74\n95.14\nta∗\n85.63\n84.55\n84.55\n87.16\n88.32\navg∗\n94.24\n93.62\n93.98\n94.50\n94.72\navg (all)∗\n96.60\n96.53\n96.49\n96.66\n96.74\nTable 4: Results on the UD POS tagging dataset in ac-\ncuracy. * indicates the settings with statistically signifi-\ncant improvements using GradSim compared to Embed.\n(FT), the second-best system. Further baseline results\nare provided in Table 11 in Appendix B.\nderstand differences coming from data peculiarities\n(Section 5.1), a correlation analysis to explain why\nsome grouping methods work better than others\n(Section 5.2), and an ablation study to investigate\nthe impact of our design choices (Section 5.3).\n5.1\nTopic Analysis\nAlthough data analysis is valuable for research\nprogress, it is challenging for foreign languages.\nTherefore, we choose a semi-automatic approach\ninvolving machine translation and manual inspec-\ntion for better understanding the input data of our\nmodels: For each language, we first extract the\n50 most relevant keywords via a term frequency-\ninverse document frequency (TF-IDF) method.\nThen, we use the pygoogletranslate API5 to trans-\nlate the keywords into English and remove dupli-\ncate words and stop words. Table 6 provides exem-\n5https://github.com/Saravananslb/\npy-googletranslation\nGrouping methods\nPearson correlation coefficient\n↔transfer score\n↔typological similarity\n↔topic similarity\nBaseline\nCross-Transfer Score (oracle)\n1\n0.353\n0.5079\nLanguage Family\n0.0869\n0.5278\n0.2680\nTypological similarity\n0.3530\n1\n0.0353\nEmbedding distance (PLM)\n0.4029\n0.7696\n-0.2383\nEmbedding distance (FT)\n0.4667\n0.7240\n-0.0252\nGradient similarity wrt. different layers (from deep to shallow)\nClassification layer\n0.6963\n0.3944\n0.4749\nEncoder layer 23\n0.6485\n0.6377\n0.1486\nEncoder layer 21\n0.5526\n0.7811\n0.1134\nEncoder layer 18\n0.4462\n0.8181\n-0.0601\nEncoder layer 15\n0.4602\n0.8329\n0.1083\nEncoder layer 12\n0.4532\n0.8566\n-0.0731\nEncoder layer 6\n0.4542\n0.8586\n-0.0342\nEncoder layer 0\n0.4526\n0.8526\n0.0721\nTable 5: Results of our correlation analysis on the AfriSenti dataset.\nLanguage (family)\nKeywords\nSwahili\ngod, thank you, major, national,\n(Niger-Congo)\nminister, better, package, service,\ncontinue, dr, education, citizens,\nnews, world, construction, people,\nregion, police, state, president, fa-\nther, army\nAmharic\nflower, city, season, a matter,\n(Afro-Asiatic)\ngovernment, discussion, decem-\nber, press release, district, informa-\ntion, administration, public, gov-\nernment, the racist, man, poison\nXitsonga\nmozambique, listen, wake up,\n(Niger-Congo)\nawake, live, conform, sugar, home,\nlake, leave, speed, connect, come\nTable 6: Exemplary keywords from AfriSenti tweets\nof different languages (reduced to a subset for space\nreasons, full set is provided in Appendix D (Table 15)).\nplary results for three languages of the AfriSenti\ndataset. The complete set of keywords for all lan-\nguages is provided in Appendix D (see Table 15).\nWhile the keywords extracted for Swahili (sw)\nand Amharic (am) are mainly centered around po-\nlitical and administrative topics, e.g., national, min-\nister, education, government etc, the keywords for\nXitsonge (ts) are more related to every-day life as-\npects. The multilingual model performance reveals\nthat indeed Swahili and Amharic can effectively be\ntrained together while Swahili and Xitsonga rather\nharm each other, even though Swahili and Xitsonga\nbelong to the same language family and Swahili\nand Amharic do not. When looking at the language\ngrouping results, GradSim indeed groups sw and\nam together (see Table 12 in Appendix D), thus,\nis able to capture their topical similarity, while a\nlanguage family-based grouping would cluster sw\nand ts into the same group.\n5.2\nCorrelation Analysis\nTable 5 provides the results of our correlation anal-\nysis that we perform on the AfriSenti dataset. In\nparticular, we compute the Pearson correlation co-\nefficient between the different grouping methods\n(similarity measures) that we study and different\ncharacteristics, such as model-specific character-\nistics (measured by cross-lingual transfer score),\nlanguage-specific characteristics (measured by ty-\npological similarity) and topic-specific characteris-\ntics (measured by keyword embedding distance).\nFrom the results, we can draw a number of inter-\nesting conclusions, namely:\n(i) The transfer score is not correlated with lan-\nguage family information and only weakly corre-\nlated with embedding-based similarity measures\noften used in related work. For gradient similar-\nity, we see considerably higher correlation values,\nsupporting our proposal of using this similarity\nmeasure for language grouping.\n(ii) There is a relatively weak correlation be-\ntween the cross-lingual transfer score and the ty-\npological similarity, while language family and\nembedding-based similarity measures show a high\ncorrelation with typological language similariy.\nThis indicates that these similarity measures cap-\nture the linguistics-based language information\nwell, which, however, does not translate into better\ntransfer performance. Similar to the oracle mea-\nsure (transfer score), gradient similarity based on\nthe classifier parameters is only weakly correlated\nwith typological language similarity.\n(iii) Based on the keywords extracted for our\nanalysis in Section 5.1, we retrieve keyword em-\nbeddings from the pretrained model encoder and\naverage them for each language. We then compare\nthe similarities of the keyword-based language em-\nbeddings with our different similarity measures\nusing Pearson correlation. We find that they are\nonly weakly correlated with language family infor-\nmation and even weakly negatively correlated with\nembedding distances. However, the correlation\nwith the cross-transfer score and our proposed gra-\ndient similarity is larger, indicating that the gradient\nsimilarity can indeed pick up the topic information\nof the data.\n(iv) While higher layers are higher correlated\nwith task performance, lower layers show a higher\ncorrelation with typological distance.\nThis in-\ndicates that lower layers encode rather general\nlanguage-specific information while higher layers\ncapture task-related information.\n5.3\nAblation Study\nGradients from different layers.\nTable 7 shows\nan ablation study of our model. The main design\nchoice of our approach is the position in the model\nwhere to take the gradients. In our analysis, we\ncompare model performance when using gradients\nfrom different layers. We see a clear trend that\nhigher layers are better suited than lower layers. In\nparticular, taking the gradients directly from the\nclassification layers leads to the best results.\n0\n2\n4\n6\n8\nNumber of language groups\n60\n65\n70\nF1 score\nFigure 3: Ablation study of different number of groups\non AfriSenti: average F1 w.r.t. number of groups.\nNumber of language groups.\nFor our experi-\nments, we choose the number of language groups\nK to be the same as the number of language fam-\nilies covered in the datasets.6 However, K is a\nhyperparameter of our method. Therefore, we in-\n6Except for WikiAnn, where we set K to the same number\nas prior work (Shaffer, 2021) for a fair comparison.\nvestigate the performance for K ∈{1 . . . 8} in\nFigure 3. Choosing K = 2 can already improve\nthe performance compared to purely multilingual\ntraining (K = 1). Until K = 4, the performance\nfurther improves and then converges for larger K.\nGradient from layer\nTask performance\nClassification layer\n71.34\nEncoder layer 23\n69.95\nEncoder layer 21\n69.43\nEncoder layer 18\n69.21\nEncoder layer 15\n68.91\nEncoder layer 12\n69.19\nEncoder layer 6\n69.19\nEncoder layer 0\n68.22\nTable 7: Ablation study of gradients from different lay-\ners on the AfriSenti dataset.\n6\nDiscussion\nIn this section, we summarize our main findings.\nLanguage similarity is not enough to determine\ntransfer suitability.\nWhen sharing knowledge\nacross languages, the information about linguistics-\nbased language similarity (e.g., whether the lan-\nguages come from the same language family or\nhow languages are typologically similar to each\nother) is not enough for optimal performance. This\nobservation is in line with the findings by Tan et al.\n(2019), Shaffer (2021) and Malkin et al. (2022)\nthat languages from the same family may still ex-\nhibit distinct linguistic features and, thus, language-\nfamily based grouping can enhance model perfor-\nmance only to a certain extent. In addition, we find\nthat there are other aspects that will affect multilin-\ngual model performance and, therefore, need to be\ntaken into account, such as the topical distribution\nof the data.\nGradient-based method does not require any\nprior knowledge.\nOur proposed gradient-based\napproach for grouping languages is a pure model-\nbased approach, thus, does not require any prior\nknowledge about the language, task or data. As a\nresult, it can be successfully applied, even when\nthe data distribution (e.g., topical distribution) is\nunknown (e.g., because we are dealing with foreign\nlanguages). While our current work only presents\nresults for language grouping for multilingual mod-\nels, the method itself is more general and can be\napplied to other settings as well, such as multi-task\nlearning or multi-domain setups.\nLower layers capture language, upper layers\ntask information.\nAdding to previous work on\nanalyzing transformer-based pretrained language\nmodels (Raganato and Tiedemann, 2018; Jawahar\net al., 2019; Tenney et al., 2019), our correlation\nanalysis shows that gradient similarity between lan-\nguages from lower layers are more correlated to\nlanguage-specific distances, i.e., low layers seem to\nencode language-specific information, while gradi-\nent similarity from upper layers are more correlated\nto task-specific performance, i.e., upper layers tend\nto capture task-specific information.\n7\nConclusion\nIn this paper, we addressed the challenging prob-\nlem of grouping languages for effective multi-\nlingual training. We proposed a gradient-based\ngrouping approach and showed in our experiments\nthat it is better correlated to cross-lingual trans-\nfer performance than language family or language\nembedding-based grouping. In our analysis, we\nidentified topical distribution differences as one\npotential challenge that can be addressed effec-\ntively by our approach. Furthermore, our corre-\nlation analysis confirmed results from prior work\nthat lower layers of transformer-based pretrained\nmodels seem to encode language-specific features,\nwhile upper layers capture task-specific informa-\ntion. Our method shows superior performance com-\npared to a variety of baseline methods for language\ngrouping on three diverse datasets and, in particu-\nlar, sets the new state of the art on a multilingual\nsentiment analysis benchmark dataset consisting of\nlow-resource African languages.\nLimitations\nOne limitation of our work is the scope of evalu-\nation. While we performed experiments on three\ndiverse text classification and sequence tagging\ntasks, GradSim is generally applicable to a wide\nrange of tasks and could thus be evaluated on even\nfurther tasks.\nBesides, our experiments currently focus on mul-\ntilingual settings and datasets. Experiments for\nmulti-domain and multi-task settings are outside\nthe scope of this work, however, an interesting di-\nrection for future work.\nFinally, compared to the large number of lan-\nguages in the world, the set of languages in our\nwork is still limited and, thus, our results might not\nbe representative for all languages of the world.\nHowever, we chose the datasets for our experi-\nments with the aim of covering a broad variety\nof languages, including African languages which\nare typically under-explored in NLP research.\nEthics Statement\nOur work focuses on multilingual and low-resource\nsettings. For instance, we investigate our models\non African languages which are typically under-\nrepresented and under-explored in NLP research.\nIncluding them into NLP research is important\nfrom an ethical point of view.\nReferences\nDavid Adelani, Graham Neubig, Sebastian Ruder,\nShruti Rijhwani, Michael Beukman, Chester Palen-\nMichel, Constantine Lignos, Jesujoba Alabi, Sham-\nsuddeen Muhammad,\nPeter Nabende,\nCheikh\nM. Bamba Dione, Andiswa Bukula, Rooweither\nMabuya, Bonaventure F. P. Dossou, Blessing Sibanda,\nHappy Buzaaba, Jonathan Mukiibi, Godson Kalipe,\nDerguene Mbaye, Amelia Taylor, Fatoumata Ka-\nbore, Chris Chinenye Emezue, Anuoluwapo Aremu,\nPerez Ogayo, Catherine Gitau, Edwin Munkoh-\nBuabeng, Victoire Memdjokam Koagne, Allah-\nsera Auguste Tapo, Tebogo Macucwa, Vukosi Mari-\nvate, Mboning Tchiaze Elvis, Tajuddeen Gwad-\nabe, Tosin Adewumi, Orevaoghene Ahia, Joyce\nNakatumba-Nabende, Neo Lerato Mokono, Ig-\nnatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa\nOluwaseun Adeyemi, Gilles Quentin Hacheme,\nIdris Abdulmumin, Odunayo Ogundepo, Oreen\nYousuf, Tatiana Moteu, and Dietrich Klakow. 2022.\nMasakhaNER 2.0: Africa-centric transfer learning\nfor named entity recognition.\nIn Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4488–4508, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nTyler Chang, Zhuowen Tu, and Benjamin Bergen. 2022.\nThe geometry of multilingual language model repre-\nsentations. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 119–136, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and\nAndrew Rabinovich. 2018.\nGradnorm: Gradient\nnormalization for adaptive loss balancing in deep\nmultitask networks. In International conference on\nmachine learning, pages 794–803. PMLR.\nAlexandra Chronopoulou, Dario Stojanovski, and\nAlexander Fraser. 2023. Language-family adapters\nfor low-resource multilingual neural machine trans-\nlation. In Proceedings of the The Sixth Workshop\non Technologies for Machine Translation of Low-\nResource Languages (LoResMT 2023), pages 59–72,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi\nReichart. 2018. The hitchhiker’s guide to testing sta-\ntistical significance in natural language processing.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1383–1392, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nChris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan\nAnil, and Chelsea Finn. 2021. Efficiently identifying\ntask groupings for multi-task learning. Advances in\nNeural Information Processing Systems, 34:27503–\n27516.\nJack FitzGerald, Christopher Hench, Charith Peris,\nScott Mackie, Kay Rottmann, Ana Sanchez, Aaron\nNash, Liam Urbach, Vishesh Kakarala, Richa Singh,\nSwetha Ranganath, Laurie Crist, Misha Britan,\nWouter Leeuwis, Gokhan Tur, and Prem Natara-\njan. 2022.\nMassive:\nA 1m-example multilin-\ngual natural language understanding dataset with 51\ntypologically-diverse languages.\nSuchin\nGururangan,\nAna\nMarasovi´c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nMichael A. Hedderich, Lukas Lange, Heike Adel, Jan-\nnik Strötgen, and Dietrich Klakow. 2021. A survey\non recent approaches for natural language process-\ning in low-resource scenarios. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2545–2568,\nOnline. Association for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalization.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for Com-\nputational Linguistics.\nLukas Lange, Heike Adel, Jannik Strötgen, and Dietrich\nKlakow. 2021a. FAME: Feature-based adversarial\nmeta-embeddings for robust input representations.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8382–8395, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nLukas Lange, Anastasiia Iurshina, Heike Adel, and Jan-\nnik Strötgen. 2020. Adversarial alignment of multi-\nlingual models for extracting temporal expressions\nfrom text. In Proceedings of the 5th Workshop on\nRepresentation Learning for NLP, pages 103–109,\nOnline. Association for Computational Linguistics.\nLukas Lange, Jannik Strötgen, Heike Adel, and Diet-\nrich Klakow. 2021b. To share or not to share: Pre-\ndicting sets of sources for model transfer learning.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8744–8753, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nPeiqin Lin, Chengzhi Hu, Zheyu Zhang, André F. T.\nMartins, and Hinrich Schütze. 2023. mplm-sim: Un-\nveiling better cross-lingual similarity and transfer in\nmultilingual pretrained language models.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junx-\nian He, Zhisong Zhang, Xuezhe Ma, Antonios Anas-\ntasopoulos, Patrick Littell, and Graham Neubig. 2019.\nChoosing transfer languages for cross-lingual learn-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3125–3135, Florence, Italy. Association for Compu-\ntational Linguistics.\nPatrick Littell, David R. Mortensen, Ke Lin, Katherine\nKairis, Carlisle Turner, and Lori Levin. 2017. URIEL\nand lang2vec: Representing languages as typological,\ngeographical, and phylogenetic vectors. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 8–14, Valencia, Spain.\nAssociation for Computational Linguistics.\nYihong Liu, Haotian Ye, Leonie Weissweiler, Philipp\nWicke, Renhao Pei, Robert Zangenfeind, and Hin-\nrich Schütze. 2023. A crosslingual investigation of\nconceptualization in 1335 languages.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nDan Malkin,\nTomasz Limisiewicz,\nand Gabriel\nStanovsky. 2022. A balanced data approach for eval-\nuating cross-lingual transfer: Mapping the linguistic\nblood bank. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4903–4915, Seattle, United States.\nAssociation for Computational Linguistics.\nShamsuddeen Hassan Muhammad, Idris Abdulmumin,\nAbinew Ali Ayele, Nedjma Ousidhoum, David Ife-\noluwa Adelani, Seid Muhie Yimam, Ibrahim Sa’id\nAhmad, Meriem Beloucif, Saif Mohammad, Se-\nbastian Ruder, Oumaima Hourrane, Pavel Brazdil,\nFelermino Dário Mário António Ali, Davis Davis,\nSalomey Osei, Bello Shehu Bello, Falalu Ibrahim,\nTajuddeen Gwadabe, Samuel Rutunda, Tadesse Be-\nlay, Wendimu Baye Messelle, Hailu Beshada Balcha,\nSisay Adugna Chala, Hagos Tesfahun Gebremichael,\nBernard Opoku, and Steven Arthur. 2023a. AfriSenti:\nA Twitter Sentiment Analysis Benchmark for African\nLanguages.\nShamsuddeen Hassan Muhammad, Idris Abdulmu-\nmin, Seid Muhie Yimam, David Ifeoluwa Ade-\nlani, Ibrahim Sa’id Ahmad, Nedjma Ousidhoum,\nAbinew Ali Ayele, Saif M. Mohammad, Meriem\nBeloucif, and Sebastian Ruder. 2023b. SemEval-\n2023 Task 12: Sentiment Analysis for African Lan-\nguages (AfriSenti-SemEval). In Proceedings of the\n17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajiˇc, Christopher D. Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman.\n2016. Universal Dependencies v1: A multilingual\ntreebank collection. In Proceedings of the Tenth In-\nternational Conference on Language Resources and\nEvaluation (LREC’16), pages 1659–1666, Portorož,\nSlovenia. European Language Resources Association\n(ELRA).\nArturo Oncevay, Barry Haddow, and Alexandra Birch.\n2020. Bridging linguistic typology and multilingual\nmachine translation with multi-view language repre-\nsentations. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2391–2406, Online. Association for\nComputational Linguistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nSebastian Ruder. 2017.\nAn overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098.\nOzan Sener and Vladlen Koltun. 2018. Multi-task learn-\ning as multi-objective optimization. Advances in\nneural information processing systems, 31.\nKyle Shaffer. 2021. Language clustering for multilin-\ngual named entity recognition. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 40–45, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nVésteinn Snæbjarnarson, Annika Simonsen, Goran\nGlavaš, and Ivan Vuli´c. 2023. Transfer to a low-\nresource language via close relatives: The case study\non Faroese. In Proceedings of the 24th Nordic Con-\nference on Computational Linguistics (NoDaLiDa),\npages 728–737, Tórshavn, Faroe Islands. University\nof Tartu Library.\nTrevor Standley, Amir Zamir, Dawn Chen, Leonidas\nGuibas, Jitendra Malik, and Silvio Savarese. 2020.\nWhich tasks should be learned together in multi-task\nlearning? In International Conference on Machine\nLearning, pages 9120–9132. PMLR.\nXu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, and\nTie-Yan Liu. 2019.\nMultilingual neural machine\ntranslation with language clustering. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 963–973, Hong\nKong, China. Association for Computational Lin-\nguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nMingyang Wang, Heike Adel, Lukas Lange, Jan-\nnik Strötgen, and Hinrich Schütze. 2023.\nNl-\nnde at semeval-2023 task 12: Adaptive pretrain-\ning and source language selection for low-resource\nmultilingual sentiment analysis.\narXiv preprint\narXiv:2305.00090.\nZirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.\n2020. Gradient vaccine: Investigating and improv-\ning multi-task optimization in massively multilingual\nmodels. arXiv preprint arXiv:2010.05874.\nHaoran Xu and Kenton Murray. 2022. Por qué não\nutiliser alla språk? mixed training with gradient opti-\nmization in few-shot cross-lingual transfer. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 2043–2059, Seattle, United\nStates. Association for Computational Linguistics.\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev.\n2018. Robust multilingual part-of-speech tagging\nvia adversarial training. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 976–986, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey\nLevine, Karol Hausman, and Chelsea Finn. 2020.\nGradient surgery for multi-task learning. Advances\nin Neural Information Processing Systems, 33:5824–\n5836.\nAmir R Zamir,\nAlexander Sax,\nWilliam Shen,\nLeonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. 2018.\nTaskonomy: Disentangling task\ntransfer learning. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 3712–3722.\nA\nDataset Information\nTable 8 provides statistics of AfriSenti, the bench-\nmark dataset we chose for our text classification ex-\nperiments. Table 9 gives language information for\nthe WikiAnn and Universal Dependencies datasets.\nWe selected 15 languages from WikiAnn for NER\nfollowing prior work (Shaffer, 2021) and 27 lan-\nguages for POS tagging following Yasunaga et al.\n(2018); Lange et al. (2021a).\nFamily\nLanguage\nDataset\nTrain\nDev\nTest\nAfro-Asiatic\nAmharic (am)\n5,985 1,498 2,000\nAlgerian Arabic (dz)\n1,652\n415\n959\nHausa (ha)\n14,173 2,678 5,304\nMoroccan Arabic (ma)\n5,584 1,216 2,962\nEnglish-Creole Nigerian Pidgin (pcm)\n5,122 1,282 4,155\nIndo-European Mozambican\nPortuguese (pt)\n3,064\n768 3,663\nNiger-Congo\nIgbo (ig)\n10,193 1,842 3,683\nKinyarwanda (kr)\n3,303\n828 1,027\nSwahili (sw)\n1,198\n454\n749\nXitsonga (ts)\n805\n204\n255\nTwi (twi)\n3,482\n389\n950\nYoruba (yo)\n8,523 2,091 4,516\nTable 8: Language information and dataset statistics\nof AfriSenti. 12 African languages from 4 language\nfamilies are included in our study.\nB\nAdditional Baseline Experimental\nResults\nHere we provide the experimental results on the\nWikiAnn and UD POS tagging dataset with group-\ning methods based on cross-lingual transfer score,\ntypological language similarity and language em-\nbeddings distance.\nComparing GradSim to other baseline methods\nin Table 10 and 11 , we can draw similar conclu-\nsions as in Section 4.4: First, GradSim achieves the\nbest average performance and performs especially\nwell in low-resource languages, revealing the im-\nportance of multilingual learning with suitable sets\nof source languages for low-resource languages.\nAdditionally, GradSim comes very close to the ora-\ncle upper bound without any prior knowledge about\nthe cross-lingual transfer performance.\nC\nLanguage Grouping Reults\nIn this section, we provide further details in Table\n12, 13 and 14 of our language grouping results\nwith the best language groups based on different\nsimilarity measures on the three datasets.\nWikiAnn NER dataset\nFamily\nLanguage\nAfro-Asiastic\nArabic (ar)\nHebrew (he)\nIndo-European\nDanish (da)\nGerman (de)\nEnglish (en)\nSpanish (es)\nFrench (fr)\nHindi (hi)\nItalian (it)\nRussian (ru)\nNiger-Congo\nSwahili (sw)\nYoruba (yo)\nKoreanic\nKorean (ko)\nJaponic\nJapanese (ja)\nSino-Tibetan\nMandarin (zh)\nUD POS tagging\nFamily\nLanguages\nIndo-European\nBulgarian (bg)\nCzech (cd)\nDanish (da)\nGerman (de)\nGreek (el)\nEnglish (en)\nSpanish (es)\nPersian (fa)\nFrench (fr)\nIrish (ga)\nHindi (hi)\nCroatian (hr)\nItalian (it)\nDutch (nl)\nNorwegian (no)\nPolish (pl)\nPortuguese (pt)\nRomanian (ro)\nSlovenian (sl)\nSwedish (sv)\nBasque\nBasque (eu)\nUralic\nFinnish (fi)\nEstonian (et)\nHungarian (hu)\nAfro-Asiastic\nHebrew (he)\nAustronesian\nIdonesian (id)\nDravidian\nTamil (ta)\nTable 9: Language information of WikiAnn and Uni-\nversal Dependencies datsets for NER and POS tagging\ntasks, respectively.\nD\nKeyword extraction results\nIn Table 15, we give the full set of keywords ex-\ntracted from the AfriSenti dataset of 12 African\nlanguages.\nOracle upper bound\nTypological similarity\nEmbedding dis (PLM & FT)\nGradSim (ours)\nhigh-resource\nar\n88.43\n87.40\n88.38\n88.02\nhe\n85.22\n84.12\n84.88\n84.06\nda\n92.36\n92.22\n92.17\n91.65\nde\n88.24\n88.01\n87.89\n87.27\nen\n83.98\n83.75\n83.42\n83.31\nes\n91.58\n91.92\n91.38\n90.85\nfr\n89.72\n90.13\n89.83\n89.54\nhi\n88.99\n86.20\n89.00\n88.25\nit\n91.13\n91.16\n90.84\n90.72\nru\n88.70\n88.71\n88.39\n88.70\nko\n87.03\n84.61\n86.99\n85.92\nja\n67.74\n67.80\n67.33\n75.56\nzh\n76.43\n76.72\n75.93\n75.49\navg\n86.12\n85.60\n85.88\n86.10\nlow-resource\nsw\n90.37\n78.79\n89.70\n90.22\nyo\n86.22\n61.91\n6.20\n86.22\navg\n88.30\n70.35\n47.95\n88.22\navg (all)\n86.41\n83.56\n80.82\n86.39\nTable 10: Additional experimental results on the WikiAnn dataset. We compare the grouping performance based on\ndifferent language simialrity measures. We have the same language grouping results for Embedding distance (PLM)\nand Embedding distance (FT) (See Table 13) and merge them in one column.\nOracle\nupper bound\nTypological\nsimilarity\nEmbedding\ndistance (PLM)\nEmbedding\ndistance (FT)\nGradSim\n(ours)\nhigh-resource\nbg\n99.42\n99.40\n99.35\n99.37\n99.40\ncs\n99.01\n98.99\n99.01\n98.99\n99.01\nda\n98.56\n98.04\n98.09\n98.07\n98.55\nde\n94.68\n94.61\n94.36\n94.55\n94.43\nen\n97.36\n96.92\n97.16\n97.26\n97.18\nes\n97.36\n97.35\n97.23\n97.25\n97.21\neu\n96.06\n96.01\n95.98\n96.07\n96.09\nfa\n97.34\n97.24\n97.35\n97.33\n97.41\nfi\n97.66\n97.50\n97.63\n97.53\n97.70\nfr\n96.59\n96.40\n96.51\n96.45\n96.67\nhe\n97.44\n97.24\n97.29\n97.30\n97.43\nhi\n97.60\n97.60\n97.61\n97.63\n97.55\nhr\n97.68\n97.40\n97.74\n97.65\n97.57\nid\n91.36\n91.75\n91.36\n91.59\n91.20\nit\n98.60\n98.56\n98.58\n98.51\n98.58\nnl\n94.01\n93.53\n93.72\n93.79\n93.88\nno\n99.04\n98.91\n98.92\n98.96\n99.01\npl\n98.59\n98.54\n98.49\n98.49\n98.52\npt\n98.56\n98.56\n98.49\n98.42\n98.54\nsl\n99.03\n98.87\n99.02\n98.96\n99.02\nsv\n98.74\n98.88\n98.80\n98.83\n98.81\navg\n97.37\n97.25\n97.27\n97.29\n97.32\nlow-resource\nel\n98.62\n98.42\n98.50\n98.56\n98.59\net\n95.58\n95.65\n95.04\n95.26\n95.78\nga\n93.50\n93.46\n93.28\n93.08\n93.52\nhu\n96.99\n96.90\n97.39\n97.14\n96.94\nro\n95.20\n95.92\n94.76\n95.79\n95.14\nta\n88.33\n86.16\n88.04\n87.02\n88.32\navg\n94.70\n94.42\n94.50\n94.48\n94.72\navg (all)\n96.77\n96.62\n96.66\n96.66\n96.74\nTable 11: Additional experimental results on the UD POS tagging dataset.\nMethod\nLanguage groups\nOracle upper bound\ngroup 0\nam, ha, kr, ma, pt, sw\ngroup 1\ndz, pcm\ngroup 2\nig\ngroup 3\npcm, ts, twi, yo\nLanguage Family\ngroup 0\nam, dz, ha, ma\ngroup 1\nig, kr, sw, ts, twi, yo\ngroup 2\npcm\ngroup 3\npt\nTypological dis.\ngroup 0\nam, dz, ha, ig, ma, sw,\ntwi, yo\ngroup 1\nkr\ngroup 2\npcm, ts\ngroup 3\npt\nEmbedding dis. (PLM)\ngroup 0\nam, dz, ma, pt, twi\ngroup 1\nha, ig, kr, pcm, ts, twi\ngroup 2\nsw\ngroup 3\nyo\nEmbedding dis. (FT)\ngroup 0\nam, kr, pt, sw\ngroup 1\ndz, ma, pcm\ngroup 2\nha, ig, ma, ts, yo\ngroup 3\nma, ts, twi\nGradSim (ours)\ngroup 0\nam, ha, kr, ma, pt, sw\ngroup 1\ndz, pcm, ts, twi\ngroup 2\nig\ngroup 3\nyo\nTable 12: Language grouping results on AfriSenti\ndataset. Unbolded languages are the source-only lan-\nguages in the group, i.e., they only participate in the\nmultilingual training but are evaluated in another lan-\nguage group during inference (details see Section 3.2).\nMethod\nLanguage groups\nOracle upper bound\ngroup 0\nar, fr, sw, yo\ngroup 1\nhe, da, de, en, es, hi, it, ko\ngroup 2\nja\ngroup 3\nru, zh\nLanguage Family\ngroup 0\nar, he\ngroup 1\nda, de, en, es, fr, hi, it, ru\ngroup 2\nsw, yo\ngroup 3\nko\ngroup 4\nja\ngroup 5\nzh\nTypological dis.\ngroup 0\nar, he\ngroup 1\nda, de, en, es, fr, it, ru\ngroup 2\nsw, yo\ngroup 3\nhi, ko, ja, zh\nEmbedding dis. (PLM)\ngroup 0\nar, en, es, fr, hi, it, sw\ngroup 1\nhe, da, de, ru, ko\ngroup 2\nja, zh\ngroup 3\nyo\nEmbedding dis. (FT)\ngroup 0\nar, en, es, fr, hi, it, sw\ngroup 1\nhe, da, de, ru, ko\ngroup 2\nja, zh\ngroup 3\nyo\nGradSim (ours)\ngroup 0\nar, en, es, sw, yo\ngroup 1\nhe, da, de, ko, ja\ngroup 2\nfr, hi, it, ru\ngroup 3\nzh\nTable 13: Language grouping results on the WikiAnn\ndataset. The 15 languages we study covers 6 language\nfamilies, while we use K = 4 numbers of groups for\nour experiment following prior work Shaffer (2021) for\na fair comparison.\nMethod\nLanguage groups\nOracle upper bound\ngroup 0\nbg, de, fa, pl, sl, sv, el\ngroup 1\ncs, da, en, es, fr, hr, it,\nno, pt, ga, ro, eu, fi, et,\nhu, he\ngroup 2\nhi\ngroup 3\nid\ngroup 4\nnl\ngroup 5\nta\nLanguage Family\ngroup 0\nbg, cs, da, de, en, es, fa,\nfr, hi, hr, it, nl, no, pl,\npt, sl, sv, el, ga, ro\ngroup 1\neu\ngroup 2\nfi, et, hu\ngroup 3\nhe\ngroup 4\nid\ngroup 5\nta\nTypological dis.\ngroup 0\nbg, cs, da, de, en, es, fi,\nfr, hr, it, nl, no, pl, pt,\nsl, sv,el, et, ga, hu, ro\ngroup 1\neu\ngroup 2\nfa\ngroup 3\nhe\ngroup 4\nid\ngroup 5\nhi, ta\nEmbedding dis. (PLM)\ngroup 0\nbg,nl\ngroup 1\ncs, da, de, en, it, no, pl,\npt, sv, fi, id\ngroup 2\nes, fa, hi, hr, el\ngroup 3\nfr, he, ta\ngroup 4\nga, eu, et\ngroup 5\nsl, ro, hu\nEmbedding dis. (FT)\ngroup 0\nbg, de, fa, nl, pl, sl, sv,\nel\ngroup 1\ncs, da, hr, it, no, ro, hu\ngroup 2\nen, es, fr, pt, he\ngroup 3\nhi\ngroup 4\nga, eu, fi, et\ngroup 5\nid, ta\nGradSim (ours)\ngroup 0\nbg, cs, pt, ro, eu, hu, he\ngroup 1\nda, de, sv, fi, et\ngroup 2\nen, es, fr, hi, it, id\ngroup 3\nfa, pl, el\ngroup 4\nhr, nl, ta\ngroup 5\nno, sl, ga\nTable 14: Language grouping results on the Universal\nDependency POS tagging dataset.\nLanguages\nKeywords\nam\nflower, city, season, a matter, government, discussion, december, press release, district, information,\nadministration, public, government, the racist, man, poison\ndz\nadvantage, god, on my mind, welcome, my lord, dramatically, from, i swear, the people, , sugar,\ncomplain, we manage, overwhelming, i was overwhelmed, wicked, the incident, need, cash, poor\nha\nameen, safe, bring, amen, lord, bless you, fight, money, now, ai, whether, month, nonsense\nig\nlove, good, bless, husband, god, happy, birthday, leader, glory, king, thank you, sustain, glory, jesus,\nmoney, people, crazy, power, mouth, people, problem, dug, pieces, down, stupid, shut up, devil, fear,\nkill, call, poison, anger\nkr\nvery, good, Rwanda, god, good, peace, day, best, thank you, lord, together, comfort, under, discus-\nsion, news, plan, president, problem, person’, bad, child, now, woman\nma\ngod, justice, for you, thanks, upon you, amazing, regards, good luck, congrats, blessed, development,\none thousand, cave, between, facing, good, awake, president, facing, good, awake, president, special,\ncauses, minister, film, desert, causes, minister, film, desert\npcm\nyou, love, fine, god, sweet, my, baby, fit, bless, thank, today, hustle, rush, happy, enjoy, like, even,\nlife, no, say, like, person, people, pain, go, even\npt\nno, say, like, them, again, person, want, people, you all, use, pain, why, god, lord, eternal, gospel, sin,\nChrist, church, earth, repentance, Jesus, name, message, June, worse, nothing, people, unfortunately,\nproblem\nsw\ngod, thank you, major, national, minister, better, package, service, continue, dr, education, citizens,\nnews, world, construction, people, region, police, state, president, father, army\nts\nmozambique, listen, wake up, awake, live, conform, sugar, home, lake, leave, speed, connect, come\ntwi\nmother, god, thousand, money kill, sleep, yes, a little, even, why, who, stop, team, good, father,\nword, say, that\nyo\nall, god, give, good, day, lord, come, amen, how, that, know, put, son, word, may, want, who, her,\nbecome, go\nTable 15: Keyword extraction results.\n",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "published": "2023-10-23",
  "updated": "2023-10-23"
}