{
  "id": "http://arxiv.org/abs/2310.15829v1",
  "title": "Unnatural language processing: How do language models handle machine-generated prompts?",
  "authors": [
    "Corentin Kervadec",
    "Francesca Franzon",
    "Marco Baroni"
  ],
  "abstract": "Language model prompt optimization research has shown that semantically and\ngrammatically well-formed manually crafted prompts are routinely outperformed\nby automatically generated token sequences with no apparent meaning or\nsyntactic structure, including sequences of vectors from a model's embedding\nspace. We use machine-generated prompts to probe how models respond to input\nthat is not composed of natural language expressions. We study the behavior of\nmodels of different sizes in multiple semantic tasks in response to both\ncontinuous and discrete machine-generated prompts, and compare it to the\nbehavior in response to human-generated natural-language prompts. Even when\nproducing a similar output, machine-generated and human prompts trigger\ndifferent response patterns through the network processing pathways, including\ndifferent perplexities, different attention and output entropy distributions,\nand different unit activation profiles. We provide preliminary insight into the\nnature of the units activated by different prompt types, suggesting that only\nnatural language prompts recruit a genuinely linguistic circuit.",
  "text": "Unnatural language processing:\nHow do language models handle machine-generated prompts?\nCorentin Kervadec and Francesca Franzon\nUniversitat Pompeu Fabra (UPF) / Barcelona\n{name.lastname}@upf.edu\nMarco Baroni\nUPF and ICREA / Barcelona\nmarco.baroni@upf.edu\nAbstract\nLanguage model prompt optimization research\nhas shown that semantically and grammatically\nwell-formed manually crafted prompts are rou-\ntinely outperformed by automatically generated\ntoken sequences with no apparent meaning or\nsyntactic structure, including sequences of vec-\ntors from a model’s embedding space. We use\nmachine-generated prompts to probe how mod-\nels respond to input that is not composed of nat-\nural language expressions. We study the behav-\nior of models of different sizes in multiple se-\nmantic tasks in response to both continuous and\ndiscrete machine-generated prompts, and com-\npare it to the behavior in response to human-\ngenerated natural-language prompts.\nEven\nwhen producing a similar output, machine-\ngenerated and human prompts trigger different\nresponse patterns through the network process-\ning pathways, including different perplexities,\ndifferent attention and output entropy distribu-\ntions, and different unit activation profiles. We\nprovide preliminary insight into the nature of\nthe units activated by different prompt types,\nsuggesting that only natural language prompts\nrecruit a genuinely linguistic circuit.\n1\nIntroduction\nNeural language models (LMs) are parameterized\nprobabilistic models that can assign a probability to\nany sequence of language tokens. Given that they\nare trained on huge amounts of natural language,\nwe expect their statistics to mimic those of the\nlatter. In this paper, we study what happens when\na LM trained on English must process “unnatural\nlanguage”, that is, sequences that are extremely\nunlikely in English, as they are syntactically and\nsemantically ill-formed.\nWe tackle this topic through the lens of machine-\ngenerated prompts, that is, automatically discov-\nered input token sequences that optimize the\nmodel’s performance in a target zero-shot task\n(Shin et al., 2020; Deng et al., 2022). It has indeed\nbeen widely observed that such prompts, while em-\npirically effective, consist of nonsensical sequences\nof jumbled tokens. For example, using the pop-\nular AutoPrompt algorithm of Shin et al. (2020)\nand the OPT-1.3b language model (Zhang et al.,\n2022), we found that the prompt “[X] Antarctica\n= sequelsStationrough [Y]” outperforms reason-\nable human-crafted prompts such as “[X] belongs\nto the continent of [Y]” on the task of retrieving\nthe continent a geographic body belongs to. Even\nmore extremely, recent prompt generation methods\nfind sequences of embedding vectors that do not\ncorrespond to items in the model vocabulary, but\nstill outperform both human-crafted and machine-\nderived discrete prompts (Lester et al., 2021; Liu\net al., 2023; Zhong et al., 2021). This state of af-\nfairs is paradoxical: why does a LM that has been\ntrained to reproduce the statistics of natural lan-\nguage respond better to input sequences that are\ncompletely outside this distribution?\nWe present a detailed comparative study of\nhow LMs internally process manually-crafted\nprompts and both discrete and continuous machine-\ngenerated prompts. While we do not solve the\npuzzle of why linguistically ill-formed machine-\ngenerated prompts are better than human prompts,\nwe discover that there are fairly deep differences\ncharacterizing the various prompt types through all\nthe processing stages of a LM, suggesting that the\nlatter has fortuitously developed a distinct pathway\nto process unnatural language.\n2\nRelated work\nUnderstanding prompts.\nThe advent of zero-\nshot prompting stimulated interest in the linguis-\ntic and semantic properties of prompts.1\nFor\nexample, Webson and Pavlick (2022) showed\n1There is also related work on the effect of ablations such\nas word order permutations in the context of models fine-tuned\nfor a specific task, such as natural language inference (e.g.,\nGupta et al., 2021; Pham et al., 2021; Sinha et al., 2021a,b).\narXiv:2310.15829v1  [cs.CL]  24 Oct 2023\nthat, with minimal fine-tuning, highly semanti-\ncally irrelevant prompts can be as effective as\nprompts with pertinent semantic content. Start-\ning with Wallace et al. (2019) and Shin et al.\n(2020), the fact that inscrutable machine-generated\ndiscrete prompts outperform natural language se-\nquences has also attracted attention. For exam-\nple, Deng et al. (2022) showed that constraining\nmachine-generated prompts to be more “language-\nlike” harms performance. Ishibashi et al. (2023)\nand Rakotonirina et al. (2023) studied how vari-\nous ablations affect the performance of machine-\ngenerated prompts. The second study also demon-\nstrated that it is possible to find discrete machine-\ngenerated prompts that are effective across a range\nof LMs. Khashabi et al. (2022) found that continu-\nous prompts can be optimized to be near any arbi-\ntrary text in embedding space, while being equally\neffective. These studies focus on properties of the\nprompts themselves. We complement them with\nan analysis of how LMs respond when exposed to\nthese prompts.\nUnderstanding LMs\nMore generally, under-\nstanding how LMs process unnatural linguistic in-\nput contributes to our understanding of their inner\nworkings. Therefore, our study is also related to\nwork on interpretability (Lipton, 2018), defined as\nthe analysis of a trained model’s decision policy. In\nparticular, one can approach neural network inter-\npretability by adopting a mechanistic paradigm,\nconsisting in directly studying the weights and\ntheir activation in order to reverse-engineer the neu-\nral network. Successful mechanistic insights have\nbeen obtained in computer vision (Voss et al., 2021;\nOlah et al., 2020). Cammarata et al. (2020) is an\nexample of mechanistic interpretability applied to\nTranformer LMs. In this context, the Transformer\nfeed-forward layers have been shown to behave\nlike key-value memories (Geva et al., 2021). No-\ntably, as shown in Dai et al. (2022), these memory\nslots, also called knowledge neurons, encode spe-\ncific concepts acquired during pre-training. Even\nmore interesting, manually editing these memo-\nries allows to causally control the prediction output\n(Meng et al., 2022), suggesting that they play a\ncentral role in language processing (see also Geva\net al., 2022). In the present paper, we show that\nunnatural language processing is achieved by re-\ncruiting different knowledge neurons than the ones\nused for natural language processing.\n3\nSetup\n3.1\nLanguage model and tasks\nOPT family LMs\nWe conduct our analyses on\nOPT-350m and OPT-1.3b (Zhang et al., 2022),\ntwo pre-trained auto-regressive Transformer-based\nmodels trained on The Pile corpus (Gao et al.,\n2020), whose pre-trained weights are publicly\navailable from HuggingFace.\nWe choose auto-\nregressive models since LM development has in-\ncreasingly shifted to this class, and OPT models\nsince, in informal experiments, we found them\nto perform better on our tasks than comparable\nauto-regressive models available from Hugging-\nFace (e.g., the GTP2 family). OPT models use a\nvocabulary set composed of 50,265 items.\nKnowledge-retrieval tasks\nWe base our experi-\nment on the LAMA dataset (Petroni et al., 2019).\nInitially designed to probe factual knowledge and\ncommonsense in LM, this dataset is a collection\nof ⟨r, s, o⟩triplets describing a relation r between\na subject s and an object o, e.g.: ⟨continent of,\nLavoisier Island, Antarctica⟩. In particular, we\nuse the TREx (ElSahar et al., 2018) subset, whose\ntest set contains 41 relations, each with up to\n1,000 tuples. All the machine-generated prompts\nare trained using the data collected by Shin et al.\n(2020), also containing 1,000 tuples per relation.\nEach relation defines a different knowledge re-\ntrieval task. We focused on these tasks because\nthey require semantically contentful prompts (e.g.,\nfor the relation above, the prompt must carry some\ngeographic information), as opposed to other se-\ntups where a prompt might simply have to describe\nthe task at a meta-linguistic level (“translate the\nfollowing sentence into Chinese”; “does paragraph\nX entail paragraph Y?”, etc.).\n3.2\nPrompts\nTerminology\nWe refer to different methods to\nderive prompts as prompt types. We refer to the\nactual token sequences generated by a method for\na certain task as templates.\nHuman prompts\nHuman prompts (human) come\nfrom an augmented version of PARAREL (Elazar\net al., 2021). PARAREL provides a set of near-\nparaphrase templates capturing each LAMA rela-\ntion, e.g. “[X] belongs to the continent of [Y]”.\nParaRel enlarged the initial templates provided\nby LAMA using paraphrases from LPAQA (Jiang\net al., 2020) and additional patterns mined from\nWikipedia. Each prompt was then evaluated by a\nset of human experts. We further manually aug-\nmented the set with more paraphrases, and we\ncleaned the prompt set, e.g., by removing templates\nnot adapted to auto-regressive LMs.\nMachine-generated prompts\nWe compare hu-\nman prompts with both discrete (M-disc) and con-\ntinuous (M-cont) machine-generated prompts. The\ndiscrete ones are obtained using the popular Auto-\nprompt (Shin et al., 2020) algorithm. For a given\ntask, this algorithm generates a sequence of N to-\nkens relying on a gradient-guided search in the\ndiscrete LM’s vocabulary space. We set template\nlength to N = 5, as it is the average human prompt\nlength. The continuous machine generated prompts\nare obtained using Optiprompt (Zhong et al., 2021).\nFor each task, Optiprompt generates a sequence of\nN continuous vectors through optimization in the\nLM’s embedding space. Similarly to Autoprompt,\nwe set N = 5. Machine-generated prompts are\nextracted using the LAMA-TREx training set (see\nabove). 10 templates are obtained for each task by\ninitializing training with different random seeds.\nTemplate filtering\nWe only use tasks for which\nwe have, for each prompt type, at least one template\nreaching > 10% accuracy. We end up with 5.9 hu-\nman, 8.3 M-disc, and 9.0 M-cont templates on av-\nerage per task (across 21 tasks) for OPT-350m, and\n6.3 human, 8.9 M-disc, and 10 M-cont templates on\naverage per task (across 24 tasks) for OPT-1.3b.2\n3.3\nDiagnostic metrics\nAccuracy\nWe measure the effectiveness of a\nprompt type (human, M-disc or M-cont) by com-\nputing its micro-accuracy (following Zhong et al.\n(2021)), defined as the proportion of cases where\nthe prompted LM succesfully assigned maximum\ncompletion probability to the ground-truth object.\nWe average across templates and LAMA tasks. It\nis worth noting that, contrary to other works, we\ndid not perform any filtering on the LM’s output.\nInput perplexity and output entropy\nWe mea-\nsure the average perplexity for each prompt type,\ndefined as the exponentiated average negative log-\nlikelihood of a ‘‘[subject] [template]’’ se-\nquence, averaged across subjects, relations and tem-\nplates. To characterize the LM probability distribu-\ntion output, we also measure the average Shannon\n2We attach the filtered template list as a supplementary\narchive.\nentropy of the output probability vector computed\nacross all samples of the evaluation set.\nAttention distribution\nWe quantify how atten-\ntion is distributed over input tokens following Ram-\nsauer et al. (2021). For each attention head of each\nlayer, we compute the average minimal number of\nattention values required to get a cumulative soft-\nmax probability mass of 0.90. This value ranges\nfrom 0% to 100%. Intuitively, given a row of an\nattention map of a transformer layer, it corresponds\nto the number (in %) of attention values you have\nto sum to reach 90% of the total attention. Because\nattention values are normalized, if the attention\nis flat then the score will be 90%. In contrast, if\nall the attention is focused on one token, then the\nscore will be close to 1. This score decreases as the\nattention distribution becomes more peaky.\nKnowledge neuron activation overlap\nMoti-\nvated by Geva et al. (2021) and Dai et al. (2022),\nwho empirically demonstrated that Transformer\nfeed-forward (FF) layers act as key-value memo-\nries, or knowledge neurons, we measure the acti-\nvation overlap of the intermediate FF units (cor-\nresponding to memory keys) between different\nprompt types. More formally, for a given Trans-\nformer layer, let x ∈Rd be the token-wise hidden\nrepresentation contextualised by the self-attention\noperation. The FF layer can be expressed as:\nu = f(x · KT + bK)\n(1)\nFF(x) = u · V + bV\n(2)\nwhere V, K ∈Rd×dm are the FF parameters,\nbK, bV their respective biases, and f(·) the non-\nlinearity. K can be seen as a set of dm keys, giving\naccess to dm “memory slots” stored in V . In order\nto quantify which knowledge neurons are being\naccessed during prompt processing, we look at the\nunits u ∈Rdm corresponding to the weights associ-\nated to each key-value pair (Eq. 1). A high overlap\nmeans that the prompts activate the same knowl-\nedge neurons, indicating similar processing by the\nLM. On the opposite, a low overlap suggests that\nthe prompts trigger different activation pathways\nin the LM. The measure is described in more detail\nin Appendix A.\nInput similarity\nFor each pair of same-task tem-\nplates, we measure the cosine similarity of their\nembedded representations. We then compute the\naverage to get similarities at the prompt-type level.\nhuman\nM-disc\nM-cont\n100\n80\n60\n40\n20\n0\nhuman\nM-disc\nM-cont\nhuman\nM-disc\nM-cont\nOPT-350m\nOPT-1.3b\nFigure 1: Unit activation overlap (0 to 100) between\nhuman, M-disc and M-cont prompt types for OPT-350m\n(left) and OPT-1.3b (right). Higher values (more intense\ncolor, larger squares) represent larger overlap. Con-\nfidence intervals (CIs) are shown as square outlines:\nthicker lines indicate wider CIs (CIs are generally small).\nWithin-prompt overlap is higher than betweem-types\noverlap, suggesting a difference in processing.\nOutput agreement\nFor each pair of same-task\ntemplates, we measure the proportion of test cases\nwhere the templates lead to the same prediction.\nWe then compute prompt-type-level averages.\nUncertainty quantification\nWe provide the un-\ncertainty estimation of our measurements by com-\nputing the 95% Confidence Interval (CI) of each\nmeasure. In Table 1, the CI associated to each\nmetric (accuracy, perplexity, attention distribution\nscore and output entropy) is obtained by comput-\ning the 0.025 and 0.975 quantiles given the list\nof scores obtained with each templates of a given\nprompt type (note that each template’s score is\naveraged at the level of the relation). 95%CI in\nFigure 1,2,3, and Table 2 are obtained using boot-\nstrapping by randomly sampling with replacement\nfrom the list of templates (the number of resamples\nis found by incrementally increasing it until the\nuncertainty estimation converge).\n4\nProcessing machine-generated prompts\nWe experimentally demonstrate that differences be-\ntween human and machine-generated prompts exist\nat three different levels: (1) at the input level, when\ncomparing prompt types in the embedding space,\n(2) at the output level, when analyzing predictions\nand output probabilities, and (3) at the level of in-\ntermediate activation, indicating a difference in pro-\ncessing at work in the LM. We conclude this quan-\ntitative analysis by showing that, although these\nmetrics are correlated when compared within the\nsame prompt type, the correlation is weak between\nprompts of different types, leading to a number of\nhuman\nM-disc\nM-cont\n100\n80\n60\n40\n20\n0\nhuman\nM-disc\nM-cont\nhuman\nM-disc\nM-cont\nOPT-350m\nOPT-1.3b\nFigure 2: Percentage input similarity between human,\nM-disc and M-cont prompt types for OPT-350m (left)\nand OPT-1.3b (right). Higher values (more intense color,\nlarger squares) cue high similarity. Within-prompt-type\nsimilarities (the scores on the diagonal) are generally\nhigher than similarity between types. Note that the\nabsolute values of the input similarity obtained with\nboth model sizes are not directly comparable due to a\ndifference in the input dimension (2048 vs. 1024).\ncounterintuitive patterns in LM prompt processing.\n4.1\nHuman and machine-generated prompts\nare processed differently.\nHigh accuracy and high perplexity\nAs con-\nfirmed in Table 1, the main motivation to use\nmachine-generated prompts is their good perfor-\nmance, M-cont prompts outperforming human ones\nby +25pts. This higher accuracy comes along with\nlower output entropy, suggesting better LM calibra-\ntion, where a larger mass of the output probability\ndistribution is concentrated on the correct token.3\nHowever, prompt perplexity – quantifying the de-\ngree of predictability of a token sequence given an\nLM – is two order of magnitude higher for M-disc\nthan for human templates.4 We discuss this further\nin Section 4.2 below.\nLow activation overlap of knowledge neurons\nActivation overlap statistics are provided in Fig-\nure 1.\nFor both OPT-350m and OPT-1.3b, we\nobserve that, while within-prompt-type overlap is\nmild or high, ranging from 33 to 66 (on a 0-to-\n100 scale), between-prompt-type overlap is always\n3Calibration in LM analysis (e.g., Liang et al., 2023) refers\nto the confidence that a model has in its predictions when\nthe latter are correct. Our output entropy measure does not\ndirectly correlate confidence and accuracy. However, as ma-\nchine prompts are in general more likely to trigger the correct\noutput and, at the same time, they have lower output entropy,\nthe global trends do suggest that they tend to produce correct\nanswers with more confidence. We informally use the term\n“calibration” to refer to this property.\n4Due to their continuous nature, there is not trivial way to\nestimate perplexity for M-cont prompts.\nOPT-350m\nOPT-1.3b\nhuman\nM-disc\nM-cont\nhuman\nM-disc\nM-cont\nAccuracy\n29.5\n43.4\n54.9\n28.8\n46.1\n58.0\n[95% CI]\n[11.5, 65.0]\n[17.0, 79.5]\n[20.7, 86.0]\n[10.4, 78.2]\n[15.1, 83.4]\n[23.8, 89.6]\nPerplexity (103)\n0.60\n40.9\n-\n0.40\n30.3\n-\n[95% CI]\n[0.1, 1.9]\n[16.0, 95.0]\n[0.04, 1.48]\n[2.0, 911.3]\nAttention distribution (%)\n34.4\n30.0\n23.2\n30.8\n28.7\n29.4\n[95% CI]\n[29.2, 39.7]\n[27.3, 32.5]\n[21.1, 25.5]\n[17.0, 85.8]\n[16.6, 84.4]\n[14.3, 74.7]\nOutput entropy\n5.00\n4.30\n2.10\n4.70\n3.90\n2.10\n[95% CI]\n[3.2, 6.0]\n[1.9, 5.7]\n[0.5, 4.3]\n[1.7, 6.0]\n[1.3, 6.4]\n[0.4, 5.9]\nTable 1: Human and machine-generated prompts (both M-disc and M-cont) significantly differ in at least four\naspects: (1) machine-generated prompts outperform the human ones in terms of accuracy; (2) they are also better\ncalibrated on average, given their lower output entropy; while at the same time (3) machine-generated prompts are\nless predictable by the LMs, reaching significantly higher perplexity; (4) in machine-generated prompts, attention is\nconcentrated on a smaller amount of tokens. For technical convenience, perplexity is not computed for M-cont.\nlow, ranging from 13 to 26. This pattern is more\npronounced when comparing human and M-cont.\nBetween-prompt overlap tends to be higher with\nOPT-1.3b, suggesting that larger LMs could show\na convergence of human and machine-generated\nprompts (this remains to be further explored). The\nlow-overlap result is confirmed by the diagnostic\nclassifier analysis presented in App. B, that shows\nthat a simple linear classifier can distinguish be-\ntween any prompt type pair based on activation\npatterns on any layer of either LM.\nAttention is focused on fewer tokens\nAs trans-\nformer behaviour is a by-product of both FF and\nattention layers, we also look at the difference in at-\ntention distributions, shown in Table 1. Here again,\nwe observe a clear distinction between human and\nmachine-generated prompts, the latter leading to\nattention being focused on a smaller amount of to-\nkens. Recall that prompt length is a hyperparameter\nof automated prompt induction algorithms, fixed\nat 5 tokens without tuning. This result might sug-\ngest that the algorithms only associated meaningful\ninformation to a subset of these tokens.\nMachine prompts are drifting away from Hu-\nman prompts in the input space.\nFigure 2\nshows that, for both OPT-350m and OPT-1.3b, in-\nput similarity within prompt types is higher than\nsimilarity between different prompt types. In par-\nticular, the input similarity between human and Ma-\nchine prompts dramatically decreases when mov-\ning from discrete to continuous prompts.\n4.2\nSurprising aspects of LM processing\nThe significant differences that emerged between\nhuman and machine prompt processing suggest\nthat these prompt types trigger different decision\npathways. Furthermore, they provide interesting\ninsights concerning the nature of LM processing,\nand, in particular, how it can occasionally be quite\ncounter-intuitive. We explore this by considering\nsome unexpected correlation patterns.\nPerplexity does not predict accuracy\nGonen\net al. (2022) reported a negative correlation be-\ntween perplexity and effectiveness of handcrafted\nprompts. However, we observe that, when using\nmachine-generated prompt, it is possible to reach a\nhigher prediction accuracy while having a higher\nperplexity. Thus, counter-intuitively, perplexity\ndoes not necessarily predict effectiveness.\nInput similarity does not predict output agree-\nment\nThe Input predicts Output? column of Ta-\nble 2 measures the correlation between embedding-\nspace similarities of same-task templates (e.g., a\nhuman and a M-disc template for the continent\nof relation) and the rate of output agreement (de-\nfined as the portion of times different templates\nlead to the same prediction) for the corresponding\ntemplates. There is a significantly lower correla-\ntion when templates belonging to different prompt\ntypes are compared, especially when comparing\nhuman vs. machine-generated templates (e.g., hu-\nman vs. M-disc templates), than for within-type\ncomparisons (e.g., different M-disc templates for\nthe same task). When comparing different prompt\ntypes, counter-intuitively, the degree of similarity\nInput predicts Output?\nInput predicts Activation?\nActivation predicts Output?\nOPT-350m\nhuman vs. M-disc\n0.11 [-0.06, 0.27]\n0.21 [0.16, 0.26]\n0.01 [-0.05, 0.07]\nhuman vs. M-cont\n0.14 [0.01, 0.26]\n0.06 [0.02, 0.09]\n-0.04 [-0.08, -0.00]\nM-disc vs. M-cont\n0.30 [0.18, 0.42]\n0.06 [0.03, 0.08]\n0.02 [-0.01, 0.05]\nhuman vs. human\n0.53 [0.43, 0.62]\n0.73 [0.69, 0.77]\n0.66 [0.59, 0.72]\nM-disc vs. M-disc\n0.54 [0.45, 0.63]\n0.85 [0.83, 0.88]\n0.55 [0.49, 0.63]\nM-cont vs. M-cont\n0.63 [0.55, 0.72]\n0.74 [0.69, 0.78]\n0.54 [0.47, 0.61]\nOPT-1.3b\nhuman vs. M-disc\n0.18 [0.07, 0.28]\n0.24 [0.18, 0.30]\n0.13 [0.05, 0.22]\nhuman vs. M-cont\n-0.06 [-0.21, 0.08]\n0.08 [0.03, 0.14]\n-0.03 [-0.08, 0.03]\nM-disc vs. M-cont\n0.12 [0.01, 0.22]\n0.03 [-0.01, 0.07]\n-0.00 [-0.05, 0.04]\nhuman vs. human\n0.58 [0.52, 0.66]\n0.65 [0.61, 0.68]\n0.60 [0.55, 0.65]\nM-disc vs. M-disc\n0.64 [0.58, 0.70]\n0.89 [0.87, 0.90]\n0.61 [0.55, 0.67]\nM-cont vs. M-cont\n0.78 [0.73, 0.83]\n0.74 [0.71, 0.77]\n0.53 [0.49, 0.57]\nNotation: average [95% confidence interval]\nTable 2: Pearson correlations between input similarity, output agreement and activation overlap. First, we compute\na single comparative statistic (input similarity, output agreement or activation overlap) for each pair of prompts in\nsome comparison set (e.g., human vs. M-disc or human vs. human); then, for each comparison set, we look at the\ncorrelation across prompt pairs between two statistics (e.g., input similarity vs. activation overlap). Within-type\ncorrelations range from mild to high. Between-type correlations are significantly lower. These low correlations\nhighlight counteractive aspects of LM language processing. Results in bold are significant (p < 0.01). We provide\nthe average and [95%CI interval] correlations obtained using bootstrapped uncertainty estimation.\nis not a good predictor of whether the templates\nwill trigger the same output or not.\nActivation overlap is only weakly correlated\nwith output agreement and input similarity\nTa-\nble 2 also provides correlations between activation\noverlap and input similarity (Input predicts Activa-\ntion?) or output agreement (Activation predicts\nOutput?) across different pairwise prompt type\ncombinations. Within a prompt type, these correla-\ntions are mild or high, with the higher correlations\npertaining to input similarity. On the contrary, ac-\ntivation overlap ceases to be correlated to either\ninput similarity or output agreement as soon as we\ncompare different prompt types. This drop in cor-\nrelation highlights the complexity of LM internal\nprocessing. Without any prior on input type, it is\ndifficult to predict the decision pathway that will\nbe used by the model, even in the presence of high\ninput or output similarities.\n5\nA closer look at the typical units of each\nprompt type\n5.1\nUnit distribution across layers\nThe low activation overlap between prompt types\nreported in Section 4 taught us that machine-\ngenerated prompts trigger units which are distinct\nfrom the ones triggered by human prompts. The\nunits that are most often activated by the various\nprompt types also appear, to some degree, to be\ndistributed differently across layers (cf. Figure 3).\nIn particular, machine prompts display a tendency\nto activate more units on the last layer and, espe-\ncially, on the first one (it is worth recalling that this\nis the first proper Transformer layer, and not the\nembedding layer). The M-disc profile lies some-\nwhere between human and M-cont, confirming the\ntrends already observed in Section 4.\n5.2\nProfiling prompt-type-typical units\nthrough associated vocabulary items\nHaving shown that the three prompt types activate\ndifferent pathways through the network, we seek\nnow some insights into the nature of the units char-\nacterizing these different pathways.\nMethodology\nWe identify those units that are\nboth typical of a single prompt type across tasks,\nand significantly impacting the network output dis-\ntribution, in the sense that their gradients w.r.t to\nthe max output probability are in the top quartile\nof all network units (recall that, as usual, we fo-\ncus on those units we identified as knowledge neu-\nFigure 3: For each prompt type, we plot the number of units belonging to the top 20% most activated units (overall\nacross prompt types). M-cont and M-disc have significantly more highly activated units than human on the first\nlayer, with the effect particularly strong for M-cont. There is also a weaker tendency for the machine prompts to\nactivate more units on the last layer compared to the human ones. Data from OPT-1.3b.\nrons). We define the typical units for prompt A as\nthose that are among the top 20% most frequently\nactivated by this prompt type, while at the same\ntime being among the bottom 20% least frequently\nactivated by prompt types B and C.5 This filter-\ning procedure leaves 14 human (resp. 6), 4 M-disc\n(resp. 4) and 58 M-cont (resp. 238) units for OPT-\n350m (resp. OPT-1.3b). As a sanity check, we also\nrepeated the analysis with laxer thresholds involv-\ning more units, and the results were similar to the\nones we report here.\nNext, we associate each selected unit to a set of\nitems from the LM vocabulary that strongly trig-\nger its activation. Using the Wikipedia corpus,6\nfor each item in the vocabulary we save the aver-\nage unit activation in a forward pass. We sort the\nresulting matrix to get, for each unit, the top 500\nvocabulary items leading to the strongest activation.\nWe extract both input items, recording unit acti-\nvation when an item is in the input sequence, and\noutput items, recording activation when an item is\npredicted by the LM. We apply lower-casing and\ninitial-space stripping on the resulting vocabulary\nset. More details are provided in Appendix C. This\nmethod has been chosen for its simplicity. How-\never, it is also noisy and sensitive to rare but “ex-\nciting” tokens (e.g., magikarp, see Appendix C Ta-\n5A unit is activated when its value is greater than 0.\n6Subset “20220301.en” from HuggingFace\nble 4). Improving unit-item association extraction\nis left to future work.\nHaving obtained the list of vocabulary items as-\nsociated to each unit, we count how many times\neach vocabulary item occurrs in association with\nany typical unit of each prompt type (as defined\nat the beginning of this Methodology paragraph),\nobtaining 3 frequency lists, one for each prompt\ntype. We compare the relative frequencies of each\nvocabulary item in each list to determine which vo-\ncabulary items are most distinctively associated to\n(the set of typical units of) each type. In particular,\nusing a standard method from corpus linguistics,\nwe compute the local Mutual Information score\n(Evert, 2005) between each vocabulary item v and\neach prompt type t:\nLMI = |v, t| log\nP(v, t)\nP(v)P(t)\nwhere |v, t| counts the occurrences of v in the t\nlist, the joint probability P(v, t) is estimated based\non |v, t|; P(v) is estimated using the cumulative\noccurrence count of v in all lists, and P(t) is the\ntotal number of occurrences of any item in the t list.\nTable 3 reports the top-30 input vocabulary items\nranked by LMI for each prompt type and both LMs.\nMachine prompts recruit “non-linguistic” units\nLooking at the OPT-350m results first, nearly all\ncharacteristic human items are well-formed words,\nhuman\nM-disc\nM-cont\nOPT-350m\nwhats\ngazed\nful\nhandler\n(&\n361\nˆOøΩ\nstat\n////\nname\nnifty\ndarn\nexpr\navascript\nancel\nˆOøΩˆOøΩ\npage\npwr\nwhy\ndevs\nfreaking\niterator\ncpp\nyout\n});\n{\\\nsts\nfuck\nmuch\nthese\nterness\naddons\nrisome\n());\n0000\n]}\nnoticed\nlike\nhave\nhillary\n\\-\nframeworks\n...........\n+=\ntable\nreally\ndaddy\nwanna\nfilename\nlication\nithub\ncrossref\n};\ninterstit.\nthats\nlikes\nwhat\neasy\n702\nˆOøΩ\nprintln\nstats\n/**\ndoes\nhonestly\ncrappy\ndisabled\n502\nerrors\nwarn\n));\n({\nthing\nworkaround\nrelent\nrc\n601\npoons\n})\ncrip\n/////////\ngoddamn\nbothers\nbeen\njson\nsacrific\ninline\n———–\n–\ndebug\nOPT-1.3b\nundermines\nsevere\nconducive\nˆa©\n¨aˆh®\nattle\nˆak¸ `g\nleilan\nlooph\ncurls\ndictates\nfrowned\nappalling\nextreme\ncram\nˆał`gˆał`gˆał\nˆałˆałˆałˆał\nˆˆˆˆ\nmakes\ntempt\noptimized\nmal\neff\ndop\nendif\neveral\nˆaKˆj\nwill\nunfold\nremain\nearly\nmonitor\negregious\ncanaver\narchdemon\nxff\ndoes\nbounces\nreap\ncomplex\nschizophren\nrep\ncitiz\nmarketable\n../\nmanifests\npersist\nstroll\nhou\nrece\npass\n%%%%\n//\n0000\nprevail\noutweigh\nshines\ngres\nfail\ninsanely\n´o\nˆaℏ¢:\naeper\nhaunt\nhaun\nhangs\ncrazy\nkinda\n˜aˆh¨\nˆak¸\ndilig\nnanto\nmeshes\nsmokes\ngoverns\ndelay\nshitty\nprototype\n%%\n................\ncryst\nwipes\npoised\nfills\ncapital\n///\ndevices\n#####\nˆa·ˆa·\nleban\nTable 3: Top 30 vocabulary items associated to each prompt type ranked by LMI. Machine-generated prompts\nrespond to less language-like items than those triggered by human prompts. Nearly all human items are well-formed\nwords. Many M-disc items, on the other hand, are non-English diacritics, special symbols or code-related terms.\nM-cont items are entirely “non-linguistic”. Some strings have been abbreviated to fit column width.\nand include a high number of forms cuing syntac-\ntic processing, such as function elements (whats,\nwhy, does...), inflected verbs (noticed, gazed,\nliked. . . ) and modifiers (really, much, honestly. . . ).\nA remarkable amount of M-disc items are coding-\nrelated terms (handler, expr, iterator. . . ). Numbers\nand punctuation sequences that could be coding-\nrelated or web-page boilerplate ((&, \\-) also appear,\nas well as a few regular words or word fragments\n(Hillary, easy, sacrific. . . ). Finally, for M-cont, the\nitems are entirely “non-linguistic”, being composed\nof sequences of non-Latin characters or punctua-\ntion marks, as well as code fragments.\nConcerning OPT-1.3b, we observe pretty much\nthe same patterns for human and M-cont. For M-\ndisc, on the other hand, together with a number of\nnon-English diacritics and special symbols, there\nis a strong increase in regular words and word frag-\nments, although the latter still clearly differ from\nthose associated to human prompts, in that syntax-\nrelated items, such as function words or inflected\nverb forms, are largely missing. In line with what\nwe observed in Figure 1, we thus observe a cline\non which, at least for M-disc, the difference in pro-\ncessing human and machine-generating prompts\ndecreases with model size.\nWe tentatively conclude that machine prompts\nare not only triggering different activation path-\nways, but that the units involved in these pathways\ntend to respond to less language-like items than\nthose triggered by human prompts. Note that these\nunits are spread across the layers of the network,\nso that we are not only recording low-level differ-\nences in processing the input strings or vectors.7\nMoreover, the results are largely mirrored by those\nobtained when associating units with output instead\nof input vocabulary items (Table 5 in App. D).\nRecall that our analysis is based on units that are\nnot only highly typical of a prompt type across rela-\ntions, but also in the top gradient quartile, suggest-\ning that they significantly contribute to the model’s\noutput distribution. It is puzzling that units that\nmostly respond to coding fragments or unusual\ncharacters could lead the network to produce the\ncorrect next token in the semantic tasks we are\nstudying. We conjecture that distributed activation\nfrom such units might nudge the network towards\nthe right output semantic fields through connectiv-\n7The selected typical human units occur in layers 4th to\nlast of OPT-350m and layers 2 to 14 of OPT-1.3b (counting\nfrom 0). The M-disc units range from layers 3 to 10 of OPT-\n350m and 2 to 13 of OPT-1.3b. The M-cont units are the only\nones where, as expected given the distribution illustrated in\nFigure 3, a significant proportion occurs on the first (0-th)\nlayer (about one third for OPT-350m and one fourth for OPT-\n1.3b), but the remaining two thirds/three fourths range from\nlayers 2 to 21 and 1 to 10, respectively.\nity pathways that fortuitously arose during network\ntraining. This is an important topic for future work.\n6\nDiscussion\nWe have studied the phenomenon of linguisti-\ncally and semantically opaque machine-generated\nprompts from the perspective of how LMs process\nthem, compared to human-crafted ones. Our study\nhas important Limitations, that are discussed in\nthe relevant section below. However, at least for\nthe prompt generation methods, LMs and tasks we\nexplored, we can draw some general conclusions.\nMore than a “happy accident”\nOur evidence\nsuggests that the differences between human and\nmachine-generated prompts are not just superficial,\nbut affect all levels of network processing, and\nresult in the activations of qualitatively different\nunits. Some of these units are stable across seman-\ntic tasks, suggesting that they are more generally\nrecruited to process any “unnatural” input. More-\nover, contrary to what one could reasonably predict,\nthere is some evidence that machine prompts are\nmore robust than human ones, in the sense that they\nachieve better output calibration.\nIt’s unlikely that the LM has been exposed to\nanything like M-disc prompts during its initial train-\ning, and definitely it could not have seen out-of-\nvocabulary M-cont prompts, so we can only as-\nsume that the special pathways triggered by these\nprompts arose through unforeseen side effects of\npre-training.8 However, they seem to be more than\njust lucky connectivity accidents exploited by spe-\ncific prompts to solve specific tasks, or else it would\nbe difficult to explain the overall low entropy of\nmachine prompt predictions and the commonali-\nties in the units they activate. Moreover, there is\nevidence that M-disc prompts can transfer across\nTransformer-based LMs (Rakotonirina et al., 2023;\nZou et al., 2023), suggesting that unnatural lan-\nguage pathways might arise from the interaction\nof general characteristics of the Transformer ar-\nchitecture with Web-derived training data that are\npartially shared across many current pre-trained\nLMs. We must defer a better understanding of the\nnature of these unnatural pathways to future work.\n8We experimentally verified that model training is neces-\nsary for effective unnatural prompts to arise. We ran both\nAutoprompt and Optiprompt on 3 distinct random initializa-\ntions of OPT-1.3b with the same hyperparameters as in our\nmain experiments, and found that the resulting M-disc prompts\nachieved 0% accuracy in nearly all cases, whereas M-cont\nwhere at best able to retrieve the majority output of a task.\nIn particular, we plan to zoom further in into the\nprocessing of specific templates, tracking their pro-\ncessing throughout the network with methods such\nas the vocabulary-based unit analysis of Section 5.\nOn investigating unnatural language\nWe be-\nlieve that investigating “unnatural language” as we\ndid here (see also Khashabi et al., 2022; Ishibashi\net al., 2023; Rakotonirina et al., 2023) should be a\ncentral concern to NLP for at least three reasons.\nFirst, understanding why LMs work as well as\nthey do, and what are their failure modes, is one\nof the questions with the broadest scientific and\nsocietal implications we can ask today. It would\nhowever be dangerously limiting to narrow our in-\nvestigation to how LMs process natural language\nonly, ignoring their behaviour when presented in-\nputs outside their training distribution.\nSecond, unnatural language can be exploited\nfor negative purposes, as shown by Wallace et al.\n(2019) and Zou et al. (2023), who derived appar-\nently nonsensical prompts that could steer multiple\nLMs’ responses towards harmful behaviour, such\nas generating racist language.\nFinally, there is recent interest in letting LMs di-\nrectly communicate with each other to jointly solve\ntasks or to build a community (Park et al., 2023;\nZeng et al., 2023). Based on our evidence, it might\nbe pointless to insist that LM-to-LM communica-\ntion takes place in natural language, given that LMs\nmight share information more efficiently through\nunnatural prompts. Conversely, if being able to\ndecode the communication flow is important (e.g.,\nfor safety reasons), care must be taken to stop LMs\nfrom drifting into unnatural language.\nFor all these reasons, we hope that our prelimi-\nnary contribution will encourage our community to\npay more attention to the phenomenon of unnatural\nlanguage processing.\nLimitations\n• Main limitation: We presented an extended\nstudy of how two pre-trained language models\nprocess human and machine-generated inputs,\nbut we did not provide an account of why\nwe are observing the processing differences\nwe are seeing. We noticed, for example, that\nM-cont prompts activate units associated to\npunctuation marks and special characters. We\ndo not know, however, in which way these\nunits contribute to retrieving the correct an-\nswer in the target semantic tasks, nor how the\noptimization procedure chances upon them.\nThis is our priority for future work.\n• Our work is limited to the OPT family of mod-\nels trained on the English language, to the\nLAMA semantic tasks and to the AutoPrompt\nand OptiPrompt prompt extraction methods.\nA straightforward direction for future work is\nto extend our analysis to more models (includ-\ning instruction-tuned models, as instruction\ntuning might have a significant impact on how\nmodels respond to unnatural input), languages,\ndata-sets and prompt extraction algorithms.\nEthics Statement\nThe advent of publicly accessible LM interfaces\nsuch as ChatGPT has heated up the debate around\nthe broader impact of LMs. While there is a variety\nof possible societal issues to consider (Weidinger\net al., 2022), we believe that a better understanding\nof how LMs process information is a crucial part\nof bias and harm containment. If we do not under-\nstand the models, we cannot control their behaviour,\nand we are exposed to intentional adversarial at-\ntacks and other forms of unintentional model mis-\nuse. The very existence of completely opaque but\nempirically effective machine-generated prompts\nis proof of how counterintuitive the behaviour of\nLMs can be, and of how little we understand them.\nWe thus believe that our investigations of “unnatu-\nral language processing” fit well into the broader\nprogram of improving our scientific understanding\nof LMs, in order to make them more predictable,\ncontrollable and, ultimately, safer.\nAcknowledgements\nWe thank Emmanuel Chemla, Emily Cheng,\nNathana¨el Carraz Rakotonirina, Xavier Suau, the\nmembers of the UPF COLT lab, the members of\nthe Barcelona Apple Machine Learning Research\ngroup and the participants in the EviL seminar for\nhelpful feedback and suggestions. Our work was\nfunded by the European Research Council (ERC)\nunder the European Union’s Horizon 2020 research\nand innovation programme (grant agreement No.\n101019291). This paper reflects the authors’ view\nonly, and the ERC is not responsible for any use\nthat may be made of the information it contains.\nReferences\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah,\nMichael Petrov, Ludwig Schubert, Chelsea Voss, Ben\nEgan, and Swee Kiat Lim. 2020. Thread: Circuits.\nDistill. Https://distill.pub/2020/circuits.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric\nXing, and Zhiting Hu. 2022. RLPrompt: Optimizing\ndiscrete text prompts with reinforcement learning.\nIn Proceedings of EMNLP, pages 3369–3391, Abu\nDhabi, United Arab Emirates.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Sch¨utze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nHady ElSahar, Pavlos Vougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Fr´ed´erique\nLaforest, and Elena Paslaru Bontas Simperl. 2018. T-\nrex: A large scale alignment of natural language with\nknowledge base triples. In International Conference\non Language Resources and Evaluation.\nStephanie Evert. 2005. The Statistics of Word Cooccur-\nrences. Ph.D dissertation, Stuttgart University.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020.\nThe Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 30–45.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of EMNLP, pages\n5484–5495, Online and Punta Cana, Dominican Re-\npublic.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Under\nthe hood: Using diagnostic classifiers to investigate\nand improve how language models track agreement\ninformation. In Proceedings of the EMNLP Black-\nboxNLP Workshop, pages 240–248, Brussels, Bel-\ngium.\nHila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and\nLuke Zettlemoyer. 2022. Demystifying prompts in\nlanguage models via perplexity estimation. https:\n//arxiv.org/abs/2212.04037.\nAshim Gupta, Giorgi Kvernadze, and Vivek Srikumar.\n2021. BERT and family eat word salad: experiments\nwith text understanding. In Proceedings of AAAI,\npages 12946–12954, Online.\nYoichi Ishibashi, Danushka Bollegala, Katsuhito Sudoh,\nand Satoshi Nakamura. 2023. Evaluating the robust-\nness of discrete prompts. In Proceedings of EACL,\npages 2373–2384, Dubrovnik, Croatia.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDaniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui\nQin, Kyle Richardson, Sean Welleck, Hannaneh Ha-\njishirzi, Tushar Khot, Ashish Sabharwal, Sameer\nSingh, and Yejin Choi. 2022. Prompt waywardness:\nThe curious case of discretized interpretation of con-\ntinuous prompts. In Proceedings of NAACL, pages\n3631–3643, Seattle, WA.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of EMNLP, pages 3045–3059,\nPunta Cana, Dominican Republic.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher Man-\nning, Christopher R´e, Diana Acosta-Navas, Drew\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel\nGuha, Niladri Chatterji, Omar Khattab, Peter Hender-\nson, Qian Huang, Ryan Chi, Sang Xie, Shibani San-\nturkar, Surya Ganguli, Tatsunori Hashimoto, Thomas\nIcard, Tianyi Zhang, Vishrav Chaudhary, William\nWang, Xuechen Li, Yifan Mai, Yuhui Zhang, and\nYuta Koreeda. 2023. Holistic evaluation of language\nmodels. https://arxiv.org/abs/2211.09110.\nZachary C Lipton. 2018. The mythos of model inter-\npretability. Communications of the ACM, 61(10):36–\n43.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 35:17359–17372.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel\nGoh, Michael Petrov, and Shan Carter. 2020.\nZoom in:\nAn introduction to circuits.\nDistill.\nHttps://distill.pub/2020/circuits/zoom-in.\nJoon-Sung Park, Joseph O’Brien, Carrie Cai, Meredith\nRingel Morris, Percy Liang, and Michael Bernstein.\n2023. Generative agents: Interactive simulacra of\nhuman behavior. https://arxiv.org/abs/2304.\n03442.\nFabian Pedregosa, Ga¨el Varoquaux, , Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, Jake Vanderplas, Alexandre Passos,\nDavid Cournapeau, Matthieu Brucher, Matthieu Per-\nrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Ma-\nchine learning in Python. Journal of Machine Learn-\ning Research, 12:2825–2830.\nFabio Petroni, Tim Rockt¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings EMNLP, pages 2463–\n2473, Hong Kong, China.\nThang Pham, Trung Bui, Long Mai, and Anh Nguyen.\n2021. Out of Order: How important is the sequen-\ntial order of words in a sentence in natural language\nunderstanding tasks?\nIn Findings of ACL, pages\n1145–1160, Online.\nNathana¨el Rakotonirina, Roberto Dess`ı, Fabio Petroni,\nSebastian Riedel, and Marco Baroni. 2023.\nCan\ndiscrete information extraction prompts general-\nize across language models?\nIn Proceed-\nings of ICLR, Kigali, Rwanda.\nPublished on-\nline:\nhttps://openreview.net/group?id=ICLR.\ncc/2023/Conference.\nHubert Ramsauer, Bernhard Sch¨afl, Johannes Lehner,\nPhilipp Seidl, Michael Widrich, Lukas Gruber,\nMarkus Holzleitner, Thomas Adler, David Kreil,\nMichael K Kopp, et al. 2021. Hopfield networks is all\nyou need. In International Conference on Learning\nRepresentations 2021.\nTaylor Shin, Yasaman Razeghi, Robert Logan IV, Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting knowledge from language models with automat-\nically generated prompts. In Proceedings of EMNLP,\npages 4222–4235, Online.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021a.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of EMNLP, pages 2888–2913, Punta\nCana, Dominican Republic.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,\nand Adina Williams. 2021b. UnNatural Language\nInference. In Proceedings of ACL, pages 7329–7346,\nOnline.\nChelsea Voss, Nick Cammarata, Gabriel Goh, Michael\nPetrov, Ludwig Schubert, Ben Egan, Swee Kiat\nLim, and Chris Olah. 2021. Visualizing weights.\nDistill. Https://distill.pub/2020/circuits/visualizing-\nweights.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal adversarial\ntriggers for attacking and analyzing NLP. In Pro-\nceedings of EMNLP, pages 2153–2162, Hong Kong,\nChina.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of NAACL, pages 2300–\n2344, Seattle, WA.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\nCourtney Biles, Sasha Brown, Zac Kenton, Will\nHawkins, Tom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia Haas,\nSean Legassick, Geoffrey Irving, and Iason Gabriel.\n2022. Taxonomy of risks posed by language mod-\nels. In Proceedings of FAccT, pages 214–229, Seoul,\nKorea.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof\nChoromanski, Adrian Wong, Stefan Welker, Fed-\nerico Tombari, Aveek Purohit, Michael Ryoo, Vikas\nSindhwani, Johnny Lee, Vincent Vanhoucke, and\nPete Florence. 2023.\nSocratic models: Compos-\ning zero-shot multimodal reasoning with language.\nIn Proceedings of ICLR, Kigali, Rwanda.\nPub-\nlished online: https://openreview.net/group?\nid=ICLR.cc/2023/Conference.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Lin, Todor Mihaylov,\nMyle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig,\nPunit Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. 2022. OPT: Open pre-trained trans-\nformer language models.\nhttps://arxiv.org/\nabs/2205.01068.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning to\nrecall. In Proceedings of NAACL, pages 5017–5033,\nOnline.\nAndy Zou, Zifan Wang, Zico Kolter, and Matt Fredrik-\nson. 2023. Universal and transferable adversarial\nattacks on aligned language models. http://arxiv.\norg/abs/2307.15043.\nA\nMeasuring the overlap of activated\nknowledge neurons\nIn Section 4, we measure and compare which\nknowledge neurons, that is, units found in the in-\ntermediate layers of the feed-forward Transformer\nblocks (Dai et al., 2022; Geva et al., 2022), are ac-\ntivated by different prompt types. In particular, we\nmeasure the knowledge neuron activation overlap\n(abbreviated as activation overlap). The following\nalgorithms in pseudo-code detail how this measure\nis obtained.\nFirst, we construct a Boolean matrix recording\nwhich units are activated by a template. As shown\nin the pseudo-code below, a unit is said to be ac-\ntivated if its value is greater than 0 on more than\nk = 20% of cases when instantiated with each\nof the subjects associated to the template relation.\nIn the pseudo-code, Relations is the set of re-\nlations available in our task set; given a relation,\nSubjects provides the list of relevant subjects and\ntemplate(s) instantiates the template with subject\ns. Model is the LM being used.\ndef get_act(template, relation):\ncpt = 0\nk = 0.2\n# iterate across subject\nfor s in Subjects(relation):\ninpt = template(s)\nfor u in Model(inpt).units:\ncpt[u] += (u>0)\n# u is activated if >0 for\n# more than k% of inputs\nact = cpt > k*len(Subjects(relation))\nreturn act\nThen, for each pair of templates (in Templates),\nwe compute the intersection over union of their\nrespective activation matrices:\noverlap = {}\nfor r in Relations:\nfor t_A in Templates(r):\nfor t_B in Templates(r):\nact_A = get_act(t_A,r)\nact_B = get_act(t_B,r)\ni = act_A & act_B\nu = act_A | act_B\noverlap[(t_A, t_B)] = i/u\nFinally, we average these pairwise overlaps\nwhile filtering by prompt type (e.g., only averag-\nLayer 0, unit 248\n“(\n¿¿¿¿¿¿¿¿\n..............\n[\n..........\n=”/\n::::::::\n{:\n,,,,\nLayer 10, unit 674\nantidepress\ndebian\nfrieza\nmagikarp\nminecraft\nxperia\noneplus\nawakens\nbitcoin\nLayer 16, unit 2126\nfilename\nwindows\ndrm\nsshd\nmisunder\nrm\nfolder\npkg\nvm ‘\nLayer 22, unit 3617\neveral\nhuge\nevery\nrisome\nsome\nany\nthese\ncrappy\nnifty\nTable 4: For each intermediate unit in the OPT’s feed-\nforward layers we extract the set of items leading to\nthe strongest activation on the Wikipedia corpus. As an\nillustration, we selected four different units with distinct\nprofiles and displayed them in this table, along with their\ntop 9 most associated input items, for OPT-350m. We\nobserve varying degrees of consistency and naturalness\nacross units.\ning the activation overlap for pairs containing one\nhuman and one M-cont).\nB\nDiagnostic classifiers\nTo complement the activation-overlap-based analy-\nsis presented in Section 4.1 of the main paper, we\nrun a set of shallow linear “diagnostic” classifiers\n(Giulianelli et al., 2018) of the activations gener-\nated by the models on each layer in response to\ninputs from each prompt type. As usual, we focus\non the activation of knowledge neurons.\nData\nAs in the main paper, we use all templates\nwith LAMA accuracy >= 10%, filtering out a ran-\ndom subset of the LAMA P176 relation human tem-\nplates, as this relation is greatly over-represented.\nWe are left with 21 and 24 tasks for OPT-350m\nand OPT-1.3b, respectively. As we are interested in\nunits inherently distinguishing prompt types inde-\npendently of lexico-semantic aspects associated to\nspecific templates or tasks, we partition the data so\nthat the training and test data contain disjoint tasks\n(and, a fortiori, disjoint templates). We consider\n4 such partitions, each time using data from 16\n(OPT-350m)/18 (OPT-1.3b) tasks for training and\n5 (OPT-350m) /6 (OPT-1.3b) for testing, such that\nthere is no test task overlap across the partitions.9\nWe instantiate each template with 10 randomly se-\nlected subjects from the corresponding LAMA lists.\nFor each pairwise classification task, we balance\nthe test instances by downsampling the larger class,\nso that chance/majority/minority accuracies (“base-\nlines” in Figure 4) are at 50%.\nClassifier\nWe use a logistic regression classifier\nwith L1 regularization (to encourage sparseness),\nwith the L1 term coefficient fixed at α = 0.01. We\nfit the classifier with stochastic gradient descent, us-\ning the Scikit-learn toolkit (Pedregosa et al., 2011).\nFor each of the 4 training partitions, we repeat the\nexperiment with 5 different seeds, resulting in a\ntotal of 20 runs for each layer.\nResults. Figure 4 reports average per-layer clas-\nsification accuracies for each pairwise prompt type\ncomparison, with standard deviations across the 20\nruns. In all cases, accuracy values are well above\nbaseline level, and typically very high. We con-\nclude that each single layer contains units whose\nactivation is sufficiently discriminative for each\nprompt type to successfully train the classifiers, de-\nspite the challenging setup in which training and\ntest tasks (and consequently templates) are com-\npletely disjoint. Interestingly, few units on each\nlayer suffice to discriminate prompt types (the aver-\nage classifier weight sparsity across all experiments\nis at 99.5% with 0.2% standard deviation for OPT-\n350m and 99.6% with 0.4% s.d. for OPT-1.3b). The\neasiest distinctions involve M-cont as one of the\nclasses, confirming that out-of-vocabulary embed-\ndings make continuous prompts particularly differ-\nent from natural language. Indeed, it’s remarkable\nthat distinguishing M-disc from M-cont is gener-\nally easier than distinguishing between M-disc and\nhuman prompts.\nTo conclude, the classification experiments bring\nstrong convergent evidence that genuinely differ-\nent pathways characterize different prompt types\nacross all layers of the network.\nC\nUnit/vocabulary item association\nImplementation\ndetails\nWhen\nextracting\nunit/vocabulary-item association, we empirically\nset the window size to 15. This value is reason-\nable close to prompt size (5 in average), while\ncontaining a sufficient amount of tokens to get a\n9As we have 21 total tasks meeting our conditions for OPT-\n350m, one of the 4 partitions used for this model consists of\n15 training and 6 test tasks\nmeaningful context. In addition, we set the window\nstride to 15 to save computation time. Furthermore,\ndue to our limited computation resources, and\ngiven the size of the Wikipedia corpus (6B), we\nonly used 66% of the data for OPT-1.3b.\nSamples\nAs illustrated in Table 4, we came up\nwith a large diversity of unit profiles, some of them\nbeing associated to more or less linguistically valid\nitems, and with varying degree of semantic consis-\ntency.\nD\nProfiling typical units by output\nvocabulary analysis\nTable 3 in the main paper reports the 30 input vo-\ncabulary items with the largest LMI with respect to\neach prompt type. Table 5 here reports the top-\n30 output items. We largely confirm the same\ntrends, although we do notice an overall tendency\nfor the units triggered by machine prompts to be\nassociated to more “language-like” output material,\nwhich makes sense as ultimately these prompts do\nproduce well-formed task-relevant outputs.\nFigure 4: Average accuracies and standard deviations for 20 runs of the pairwise prompt-type classification\nexperiments across the 24 layers of the networks.\nhuman\nM-disc\nM-cont\nOPT-350m\nnobody\nundone\ntonight\nincompet\nfanc\n{\nˆOøΩ\n,˜n`a\ncompare\nyesterday\nbefore\nscrambled\nalluded\nridic\n\\’\nˆOøΩˆOøΩ\n”,\npref\nokay\nanybody\nreasonably\n\\\\\n782\n¨%\ntracker\n});\nalso\nreally\nplugged\nmessed\n):\nxcom\nblat\n;;\nmoreover\ncomple\nawoken\ncaptcha\npinpoint\njuxtap\n”:[”\nforge\nchecking\n[’\nattempt\nbother\ninterchangeable\nright\n”\\\n-—\nphysic\nsupported\ntherefore\n’\\);\ncorrobor\nglanced\nauthenticated\nrevert\n698\ntyr\nmeanwhile\ntext\n0000\nparsed\nglean\nsnowball\ninsin\n({\ndst\navg\n—–\n((\nbothering\ntweaked\nearlier\nfaintly\ninvoke\n772\ninsert\n¿¿¿\ncurrently\nnailed\nfigured\ntasted\nirresist\nmysql\nceremon\nheader\n},”\nprev\nOPT-1.3b\naccumulating\nsnug\npeeled\nreconc\noddly\nlocalhost\nˆak´g\nˆa¯\nˆał˙gˆał˙gˆał\nelic\nattributable\ndistinctly\nmonstrous\nvaugh\nfilib\nˆałˆał\nˆa·ˆa\n˘oł\noverpower\nsolely\nooz\ntrembling\nprett\nthough\nˆa´lı\n˜a´h ˘ı\ninst\nsubstantially\nunmatched\nwaging\ncheck\noutlandish\nrecip\n`ı¶\nˆał˙gˆał\n}{\nfundamentally\noverloaded\nincred\nadolesc\nmpg\ndisag\nkinnikuman\nsay\nˆa˜ıij\nreinvest\nobligatory\nimpecc\nˆa¯ı¼\npause\nacquies\n%%%%\n+=\nˆa´l˜ı\nunparalleled\ninfused\ndeprecated\nunderstanding\nindepend\nmurky\n===\nany\n*****\nanonym\nconstrained\ninherently\ncollaps\nbicy\nbudgetary\nservices\nsample\n¨ı¸ı\noverseen\nsandwic\nsurg\ndisbel\nscram\ngame\nprovider\nˆa¶\n˜aˆh˜i\nideally\nachievable\ntempted\nunpop\nfoundational\nbillboards\nˆałˆałˆałˆałˆałˆałˆał\nurl\nˆˆˆˆ\nTable 5: Top 30 output items associated to each prompt type ranked by LMI. Some strings have been abbreviated to\nfit column width.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-10-24",
  "updated": "2023-10-24"
}