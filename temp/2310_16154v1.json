{
  "id": "http://arxiv.org/abs/2310.16154v1",
  "title": "Breaking the Curse of Dimensionality in Deep Neural Networks by Learning Invariant Representations",
  "authors": [
    "Leonardo Petrini"
  ],
  "abstract": "Artificial intelligence, particularly the subfield of machine learning, has\nseen a paradigm shift towards data-driven models that learn from and adapt to\ndata. This has resulted in unprecedented advancements in various domains such\nas natural language processing and computer vision, largely attributed to deep\nlearning, a special class of machine learning models. Deep learning arguably\nsurpasses traditional approaches by learning the relevant features from raw\ndata through a series of computational layers.\n  This thesis explores the theoretical foundations of deep learning by studying\nthe relationship between the architecture of these models and the inherent\nstructures found within the data they process. In particular, we ask What\ndrives the efficacy of deep learning algorithms and allows them to beat the\nso-called curse of dimensionality-i.e. the difficulty of generally learning\nfunctions in high dimensions due to the exponentially increasing need for data\npoints with increased dimensionality? Is it their ability to learn relevant\nrepresentations of the data by exploiting their structure? How do different\narchitectures exploit different data structures? In order to address these\nquestions, we push forward the idea that the structure of the data can be\neffectively characterized by its invariances-i.e. aspects that are irrelevant\nfor the task at hand.\n  Our methodology takes an empirical approach to deep learning, combining\nexperimental studies with physics-inspired toy models. These simplified models\nallow us to investigate and interpret the complex behaviors we observe in deep\nlearning systems, offering insights into their inner workings, with the\nfar-reaching goal of bridging the gap between theory and practice.",
  "text": "Breaking the Curse of Dimensionality in Deep Neural Networks\nby Learning Invariant Representations\nTHIS IS A TEMPORARY TITLE PAGE\nIt will be replaced for the final print by a version\nprovided by the registrar’s office.\nThèse présentée le 12 septembre 2023\nà la Faculté des sciences de base\nLaboratoire de physique des systèmes complexes\nProgramme doctoral en physique\nÉcole polytechnique fédérale de Lausanne\npour l’obtention du grade de Docteur ès Sciences\npar\nLeonardo Petrini\nacceptée sur proposition du jury :\nProf Hugo Dil, président du jury\nProf Matthieu Wyart, directeur de thèse\nProf Florent Krzakala, rapporteur\nProf Andrew Saxe, rapporteur\nProf Nathan Srebro, rapporteur\nLausanne, EPFL, 2023\nTutto se pò fa’.\n— Babbo\nEverything can be done.\n(regarding practical problem-solving)\n— Dad\nAi miei genitori\nTo my parents\nAcknowledgements\nI extend my deepest gratitude to Matthieu, my thesis advisor, for his invaluable guidance,\nmentorship, and thorough and honest feedback throughout the journey of this PhD.\nI also thank the members of the jury—Prof. Florent Krzakala, Prof. Andrew Saxe, and Prof.\nNathan Srebro—for their critical reading of my thesis and their insightful questions. Special\nacknowledgment goes to Prof. Hugo Dil, the president of the jury, for presiding over the\nevaluation process.\nFor my years at PCSL, I thank Mario for his teachings on problem-solving and coding and\nFrancesco for being a mentor and collaborator on multiple projects. Alessandro, Antonio, and\nUmberto for the time spent together at Cubotron and outside. And Tom, I appreciate your\nunderstanding when we let our Italian conversations take over. A special mention goes to\nCorinne, our irreplaceable secretary, for her readiness to help and her untiring efforts. I thank\nall past group members of PCSL for the good times spent together.\nAfter six fulfilling years in Lausanne, including four spent working towards this PhD, I owe a\nmassive debt of gratitude to the friends I made along the way.1 To Andre and Sté, we literally\nwalked through countless adventures and thousands of kilometers side by side. To Giorgia,\nl’amica geniale, I owe you lots of laughs and advice. Even far apart, we always find ways to\nmake wonderful memories together. To Diego, so deep in startup life he might as well be\nthru-hiking in New Zealand. You’re missed, caro, but never forgotten. Ondine for the perpetual\nsmile on your face; Ali, for having a WhatsApp group ready for every occasion; Niko, for all the\ngourmet meals you prepared us; and Eli, for sharing hikes, ski slopes, and office desks alike.\nBeing far from home never felt too distant, thanks to the friends I always find upon my return.\nAle and Manu, despite our divergent paths in academics, our end-of-year reunions have been\nrevitalizing exchanges. To my lifetime friends, Ale, Diego, Paco, Giulia, Lucrezia, Cecilia, Poch,\nTeo, Fil. Thank you for your enduring friendships and the precious moments we continue to\nshare, whether near or far.\nTo my nonni, Angelo and Sara for constantly providing food and love. To my mum for teaching\nme how to organize work and get it done, to my dad for teaching me how to think out of the\nbox and solve problems. Thank you for this, and for everything else. To Alle, for making life\nfull of color.\nLausanne, September 13, 2023\nLeo\n1Ordering in this paragraph is based on the number of shared WhatsApp groups (Andre and Sté 58, Giorgia 47,\nDiego 39, Ondine 33, Ali 23, Niko 17, Eli 12).\ni\nA Neural Network\nDrawing courtesy of A.P.\nAbstract\nArtificial intelligence, particularly the subfield of machine learning, has seen a paradigm\nshift towards data-driven models that learn from and adapt to data. This has resulted in\nunprecedented advancements in various domains such as natural language processing and\ncomputer vision, largely attributed to deep learning, a special class of machine learning models.\nDeep learning arguably surpasses traditional approaches by learning the relevant features\nfrom raw data through a series of computational layers.\nThis thesis explores the theoretical foundations of deep learning by studying the relationship\nbetween the architecture of these models and the inherent structures found within the data\nthey process. In particular, we ask: What drives the efficacy of deep learning algorithms\nand allows them to beat the so-called curse of dimensionality—i.e. the difficulty of generally\nlearning functions in high dimensions due to the exponentially increasing need for data points\nwith increased dimensionality? Is it their ability to learn relevant representations of the data by\nexploiting their structure? How do different architectures exploit different data structures? In\norder to address these questions, we push forward the idea that the structure of the data can be\neffectively characterized by its invariances–—i.e. aspects that are irrelevant for the task at hand.\nOur methodology takes an empirical approach to deep learning, combining experimental stud-\nies with physics-inspired toy models. These simplified models allow us to investigate and in-\nterpret the complex behaviors we observe in deep learning systems, offering insights into their\ninner workings, with the far-reaching goal of bridging the gap between theory and practice.\nSpecifically, we compute tight generalization error rates of shallow fully connected networks\ndemonstrating that they are capable of performing well by learning linear invariances, i.e.\nbecoming insensitive to irrelevant linear directions in input space. However, we show that\nthese network architectures can perform poorly in learning non-linear invariances such as\nrotation invariance or the invariance with respect to smooth deformations of the input. This\nresult illustrates that, if a chosen architecture is not suitable for a task, it might overfit, making\na kernel method, for which representations are not learned, potentially a better choice.\nModern architectures like convolutional neural networks, however, are particularly well-fitted\nto learn the non-linear invariances that are present in real data. In image classification, for\nexample, the exact position of an object or feature might not be crucial for recognizing it. This\nproperty gives rise to an invariance with respect to small deformations. Our findings show that\nthe neural networks that are more invariant to deformations tend to have higher performance,\nunderlying the importance of exploiting such invariance.\nAnother key property that gives structure to real data is the fact that high-level features are\nv\nAbstract\na hierarchical composition of lower-level features—a dog is made of a head and limbs, the\nhead is made of eyes, nose, and mouth, which are then made of simple textures and edges.\nThese features can be realized in multiple synonymous ways, giving rise to an invariance. To\ninvestigate the synonymic invariance that arises from the hierarchical structure of data, we\nintroduce a toy data model that allows us to examine how features are extracted and combined\nto form increasingly higher-level representations. We show that deep neural networks, unlike\ntheir shallow counterparts, can learn this invariance layer by layer, and this allows beating the\ncurse. Our analysis within this setting provides an estimate of the number of data samples\nneeded for learning a task, given its hierarchical structure.\nOverall, our research shows that deep learning is able to mitigate the curse of dimensionality\nby learning representations that are invariant to aspects of the data irrelevant for the task,\neffectively reducing the problem’s dimensionality. Our quantitative characterizations of the\ngeneralization error as a function of the number of training points bring us closer to knowing\na priori how many data points are needed to learn a task, given its structure.\nKeywords: artificial neural networks, deep learning, curse of dimensionality, feature learning,\nrepresentation learning, data structure.\nvi\nRésumé\nL’intelligence artificielle, notamment le sous-domaine de l’apprentissage automatique, a\nconnu un changement de paradigme vers des modèles basés sur les données qui apprennent\net s’adaptent à ces données. Ceci a abouti à des avancées sans précédent dans divers domaines\ncomme le traitement du langage naturel et la vision par ordinateur, principalement grâce à\nl’apprentissage profond, une classe spéciale de modèles d’apprentissage automatique. L’ap-\nprentissage profond surpasse apparemment les approches traditionnelles en apprenant les\ncaractéristiques pertinentes directement à partir des données à travers une série de couches\nde calcul.\nCette thèse explore les fondements théoriques de l’apprentissage profond en étudiant la rela-\ntion entre l’architecture de ces modèles et les structures inhérentes trouvées dans les données\nqu’ils traitent. En particulier, nous nous demandons : Quel est le facteur clé qui rend les\nalgorithmes d’apprentissage profond efficaces et les aide à surmonter la difficulté d’apprendre\ndes fonctions dans des espaces à haute dimension, connue sous le nom de malédiction de la\ndimensionalité ? Est-ce leur capacité à apprendre des représentations pertinentes des données\nen exploitant leur structure? Comment différentes architectures exploitent-elles différentes\nstructures de données? Pour aborder ces questions, nous avançons l’idée que la structure des\ndonnées peut être efficacement caractérisée par ses invariances—c’est-à-dire les aspects qui\nsont non pertinents pour la tâche en question.\nNotre méthodologie adopte une approche empirique de l’apprentissage profond, combinant\ndes études expérimentales avec des modèles jouets inspirés de la physique. Ces modèles\nsimplifiés nous permettent d’investiguer et d’interpréter les comportements complexes que\nnous observons dans les systèmes d’apprentissage profond, offrant des perspectives sur leur\nfonctionnement interne, avec l’objectif à long terme de combler l’écart entre la théorie et la\npratique.\nPlus précisément, nous calculons les taux d’erreur de généralisation des réseaux a deux\ncouches totalement connectés, démontrant qu’ils peuvent apprendre des invariances linéaires,\nc’est-à-dire devenir insensibles à des directions linéaires dans l’espace d’entrée qui sont non\npertinents pour la tâche. Cependant, nous montrons que ces architectures peuvent mal fonc-\ntionner dans l’apprentissage d’invariances non linéaires comme l’invariance rotationnelle\nou l’invariance par rapport aux petites déformations de l’entrée. Ce résultat illustre que si\nune architecture choisie n’est pas adaptée à une tâche donnée, elle pourrait être en overfit-\nting, rendant une méthode de kernel, pour laquelle les représentations ne sont pas apprises,\npotentiellement un meilleur choix.\nvii\nRésumé\nLes architectures modernes telles que les réseaux de neurones convolutionnels, en revanche,\nsont particulièrement bien adaptées à l’apprentissage des invariances non linéaires présentes\ndans les données réelles. Dans la classification d’images, par exemple, la position exacte\nd’un objet ou d’une caractéristique peut ne pas être cruciale pour sa reconnaissance. Cette\npropriété donne lieu à une invariance par rapport aux petites déformations. Nos résultats\nmontrent que les réseaux neuronaux les plus invariants aux déformations ont tendance à\navoir des performances supérieures, soulignant l’importance d’exploiter une telle invariance.\nUne autre propriété clé qui donne de la structure aux données réelles est le fait que les\ncaractéristiques de haut niveau sont une composition hiérarchique de caractéristiques de\nniveau inférieur—un chien est composé d’une tête et de membres, la tête est composée\nd’yeux, de nez et de bouche, qui sont ensuite composés de textures et de contours simples. Ces\ncaractéristiques peuvent être réalisées de plusieurs manières synonymes, donnant lieu à une\ninvariance. Pour étudier l’invariance synonymique qui découle de la structure hiérarchique\ndes données, nous introduisons un modèle de données jouet qui nous permet d’examiner\ncomment les caractéristiques sont extraites et combinées pour former des représentations de\nplus en plus élevées. Nous montrons que les réseaux de neurones profonds sont capable de\napprendre avec succès ces structures hiérarchiques.\nFinalement, nos travaux fournissent un aperçu des principaux mécanismes qui rendent les\nalgorithmes d’apprentissage profond si efficaces. Par le biais d’études empiriques et de l’utili-\nsation de modèles jouets, nous mettons en évidence le rôle de la structure des données dans\nles performances des algorithmes d’apprentissage profond, nous démontrons que des inva-\nriances spécifiques permettent d’obtenir des performances supérieures, et nous fournissons\ndes orientations pour le choix de l’architecture de réseau la plus appropriée pour une tâche\ndonnée.\nMots-clés : réseaux de neurones artificiels, apprentissage profond, malédiction de la dimen-\nsionalité, apprentissage de caractéristiques, apprentissage de représentations, structure de\ndonnées.\nviii\nSommario\nL’intelligenza artificiale, e in particolare la branca dell’apprendimento automatico hanno\nsubito una profonda trasformazione negli ultimi anni, convergendo verso modelli guidati dai\ndati, capaci di adattarsi e imparare da essi. Questa evoluzione ha generato progressi notevoli\nin diverse aree applicative, come l’elaborazione del linguaggio naturale e la visione artificiale.\nTali avanzamenti sono in larga parte dovuti al deep learning (o apprendimento profondo), una\ncategoria speciale di modelli di apprendimento.\nQuesta tesi esplora le basi teoriche del deep learning, con una particolare attenzione alla\nrelazione tra l’architettura dei modelli e la struttura intrinseca dei dati su cui operano. In\nparticolare, ci chiediamo: che ruolo svolge la struttura dei dati nel successo degli algoritmi\ndi Deep Learning, specialmente nel superare la problematica nota come maledizione della\ndimensionalità? È nell’abilità di adattare le rappresentazioni interne ai dati il segreto del loro\nsuccesso? Come diverse architetture sfruttano differenti tipologie di struttura nei dati? Per\nrispondere a queste domande, proponiamo di caratterizzare la struttura dei dati attraverso le\nloro invarianze, ovvero aspetti degli input che sono irrilevanti per il task in questione.\nLa nostra metodologia si basa su un approccio empirico al deep learning, integrando studi\nsperimentali con modelli teorici semplificati, ispirati alla fisica. Questi toy models (o modelli\ngiocattolo) ci consentono di indagare ed interpretare i fenomeni complessi che si manifestano\nnei sistemi di deep learning. Con eventualmente l’obbiettivo di avvicinare teoria e pratica.\nNel dettaglio, quantifichiamo l’errore di generalizzazione in reti neurali non profonde, mo-\nstrando la loro capacità di imparare invarianze lineari. Al contrario, evidenziamo come queste\narchitetture possano non essere ottimali nell’apprendere invarianze non lineari, come quelle\nper rotazioni o deformazioni dell’input. Ciò suggerisce che in certi scenari, metodi kernel,\nle cui rappresentazioni interne non si adattano alla struttura dei dati, potrebbero essere più\nefficaci.\nEsaminiamo poi le architetture moderne, come le reti neurali convoluzionali, dimostrando la\nloro predisposizione a cogliere le invarianze non lineari presenti nei dati reali. Ad esempio,\nnella classificazione di immagini, una leggera deformazione dell’oggetto in questione non\nne compromette il riconoscimento. Mostrando che reti più invarianti hanno prestazioni\nsuperiori, sottolineiamo l’importanza di questa proprietà.\nInfine, discutiamo l’importanza della struttura gerarchica nei dati, dove le caratteristiche di\nalto livello emergono come combinazioni di quelle a livelli inferiori—un cane è composto\nda una testa e da arti, la testa è composta da occhi, naso e bocca, che sono poi composti\nda semplici texture e linee. Queste caratteristiche possono essere realizzate in più modi\nix\nSommario\nequivalenti, o sinonimi, dando luogo a un’invarianza. Proponiamo un modello di dati che\npermette di studiare questa forma di invarianza e dimostriamo come solo le reti neurali\nprofonde possano apprenderla, superando la maledizione della dimensionalità. In questo\ncontesto, forniamo una stima quantitativa del numero di dati necessario per l’apprendimento\ndi un dato compito, considerata la sua struttura gerarchica.\nIn conclusione, il nostro studio evidenzia come il deep learning sia capace di attenuare la\nmaledizione della dimensionalità attraverso l’apprendimento di rappresentazioni invarianti.\nLe nostre analisi quantitative forniscono un’indicazione sul numero dati di addestramento\nnecessari per il successo in un determinato task, in relazione alla sua struttura intrinseca.\nParole Chiave: reti neurali artificiali, apprendimento profondo, maledizione della dimensio-\nnalità, feature learning, apprendimento delle rappresentationi, struttura dei dati\nx\nContents\nAcknowledgements\ni\nAbstract (English/French/Italian)\nv\nList of Figures\nxiii\nList of Tables\nxvii\nIntroduction\n1\n1.1\nMachine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.1\nSupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.2\nThe Curse of Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.1.3\nKernel methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.1.4\nNeural Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n1.2\nFeature Learning in Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n1.2.1\nEmpirical Studies of Feature Learning . . . . . . . . . . . . . . . . . . . . .\n14\n1.2.2\nTraining Regimes: Feature vs. Lazy . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.3\nData Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n1.3.1\nLinear Invariance\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n1.3.2\nDeformation Invariance in Images . . . . . . . . . . . . . . . . . . . . . . .\n24\n1.3.3\nHierarchical Tasks and Synonymic Invariance\n. . . . . . . . . . . . . . .\n25\n1.4\nOverview of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n1.4.1\nStructure of the Thesis and Main Results . . . . . . . . . . . . . . . . . . .\n27\n1.4.2\nMethodology: Empirical Science Approach to Deep Learning . . . . . . .\n30\nI\nLinear Invariance\n33\n2\nGeneralization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n35\nII\nInvariance toward Smooth Deformations\n63\n3\nDeformation Invariance Strongly Correlates to Performance in Image Tasks\n65\nxi\nContents\n4\nWhen Feature Learning Fails: Deformation Invariance deteriorates with training in\nFully-Connected Networks\n93\n5\nWhen Feature Learning Succeeds: How Deformation Invariance is learned in Convo-\nlutional Neural Networks\n121\nIII\nSynonymic Invariance\n141\n6\nA Toy Model for the Hierarchical Compositionality of Real Data\n143\nIV\nConclusions and Perspectives\n173\n7\nConclusion\n175\n7.1\nTable of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n7.2\nEmpirical Characterization of Invariants Learning\n. . . . . . . . . . . . . . . . . 178\n7.3\nLessons Learned\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n7.4\nThe Big Picture\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n7.5\nSome Open Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\nBibliography\n202\nCurriculum Vitae\n203\nxii\nList of Figures\n1.1\nTest error vs. number of training points: different scenarios and characteriza-\ntions. (a) When the curve follows a power law, ϵ(P) ∼P−β, it can be represented\nas a straight line in a log-log plot, where the exponent −β defines the slope\nof the line. (b) Alternative scenario where ϵ(P) transitions from a large error,\ncorresponding to random guessing, to nearly zero error. Both scenarios can\nbe described using the sample complexity, P∗, which quantifies the number of\ntraining points required to achieve a specific finite error, ϵ∗. . . . . . . . . . . . .\n4\n1.2\nIllustration of the bias-variance tradeoff in polynomial regression. Red crosses\nrepresent data points (xi, yi), with yi = f ∗(xi) + εi. The red curve represents\nf ∗, the true function. Three different polynomials of degrees N = 1,5,14 are\nconsidered. The linear polynomial (underfitting) exhibits high bias and low\nvariance, failing to capture the complexity of the true function. The N = 5\npolynomial achieves a good balance between bias and variance, fitting the true\nfunction. The N = 14 polynomial (overfitting) shows low bias but high variance,\nmodeling the noise in the data and deviating significantly from the true function.\n6\n1.3\nGraphical representation of a 2-layer neural network with an input dimension\nd = 5 and h = 9 hidden neurons. The network function, denoted by f , takes as\ninput the vector x. The activation function is denoted by σ(·) and applied to\neach hidden neuron independently. The weights for the two layers are denoted\nby w2, w1, respectively. The different strength of the lines depicts weights of\ndifferent magnitude. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n1.4\nComparison between (a) a Fully Connected Neuron that receives input from\nevery element of the preceding layer, and (b) a Local Neuron, where each neuron\nprocesses data only from a specific part of the input (in this case, the patch\ncomposed by first two pixels x1 and x2). . . . . . . . . . . . . . . . . . . . . . . . .\n12\n1.5\nGraphical representation of a convolutional neural network (CNN) with 4\nconvolutional layers with many channels each and 2 fully-connected layers. The\nCNN acts on 2D images with filters of size 3×3 at each convolutional layer.\n. .\n13\nxiii\nList of Figures\n1.6\nFeature visualization in deep networks. This figure, adapted from Olah et al.\n(2017), uses feature visualization to showcase how GoogLeNet Szegedy et al.\n(2015), trained on the ImageNet dataset Deng et al. (2009), builds its understand-\ning of images progressively with depth. These images are generated via gradient\ndescent in input space such as to maximize the neurons’ response at each layer.\nWe can observe the images inducing maximal activation at increasingly deeper\nlayers (left to right), growing in complexity and abstraction with depth. . . . . .\n15\n1.7\nIllustration of the bias-variance trade-off vs. double descent phenomenon.\nTest error is reported on the y-axis, number of parameters of the model on the\nx-axis. The standard bias-variance trade-off that is present when e.g. fitting a\nfunction with a polynomial as in Figure 1.2, would predict the U-shaped behavior\nshown in blue (N, in this case, would correspond to the degree of the polynomial).\nDeep neural networks, instead, show a behavior like the one reported in green.\n18\n1.8\nStylized representation of a dog. This figure illustrates the idea of data invari-\nances within image data, where recognizable features can be represented with a\nfew lines, and small deformations often do not alter the overall semantic content.\n22\n1.9\nAn image of a dog x and its deformed version τx. The deformation τ is sampled\nfrom the distribution we introduce in chapter 3. . . . . . . . . . . . . . . . . . . .\n24\n1.10 Illustrating the hierarchical structure of real data. (a) An example of the hierar-\nchical structure of images: the class (dog) consists of high-level features (head,\npaws), that in turn can be represented as sets of lower-level features (eyes, nose,\nmouth, and ear for the head). (b) A similar hierarchical structure can be found\nin natural language: a sentence is made of clauses, each having different parts\nsuch as subject and predicate, which in turn may consist of several words. . . .\n25\n7.1\nGraphical Representation of the Hierarchically Compositional and Local Func-\ntion of Equation 7.5. The tree represents the structure of the hierarchical func-\ntion with L = 3 layers, whose constituent functions (g’s) are local on patches of\nsize s = 2. The leaves correspond to input variables. . . . . . . . . . . . . . . . . . 178\n7.2\nRotation sensitivity of the predictor when learning the constant function on the\nsphere (cf. chapter 4), in the feature and lazy regime, for a varying number of\ntraining points. Results for different input-space dimensions are reported in\neach column. To see the correlation between sensitivity and performance, these\ncurves are to be compared with Figure 4 in chapter 4. . . . . . . . . . . . . . . . . 179\n7.3\nImage of a swan deformed with the ensemble of max-entropy diffeomorphisms\nwe introduced in chapter 3. Starting from the original image on the left, de-\nformations of increasingly larger magnitudes are applied while going to the\nright.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\nxiv\nList of Figures\n7.4\nTest error rates of Fully-Connected Networks (FCN), LeNet LeCun et al. (1989),\nand ResNet18 He et al. (2016) with increasing number of training points, for\ndifferent image datasets. As depicted, a widening performance gap emerges\nbetween FCN and the two convolutional neural networks (LeNet and ResNet18)\nwith the increase in dataset complexity (left: black and white digits, center: black\nand white clothes, right: colored objects). . . . . . . . . . . . . . . . . . . . . . . . 183\n7.5\nThe task of recognizing an object in a visual scene. In both drawings we are able\nto recognize a dog even if some features of the dog come in different positions\nand shapes (e.g. the legs), or in different realizations (e.g. the ears). In this thesis,\nwe argue that the invariance of the task to such changes is crucial for neural\nnetworks to learn it, as it allows for reducing the dimensionality of the problem\nand breaking the curse of dimensionality. Illustration courtesy of A.P. . . . . . . 184\n7.6\nDeep FCNs Learn Hierarchical Tasks. 3-hidden layers fully-connected network\ntrained on the Random Hierarchy Model with three hierarchy levels (L = 3)\nand patches of size s = 2, resulting in input dimension d = 8. The number of\nclasses equals number of features and multiplicity nc = v = m = 8, resulting in\na predicted sample complexity for deep CNNs of P∗\ndCNN = ncmL = 4′096 and\na total data set size Pmax = ncmd−1 ≈108. (a) On the rows, there are different\nneurons, on the columns different input locations x1,...,xd. The color indicates\nthe weights norm across the v input channels. Left and right correspond to\nbefore and after training with P∗\ndCNN ≪P ≪Pmax. FCN learns locality in the\nsense that different neurons specialize to different input patches (x1,x2), (x3,x4),\netc. with training. A similar behavior can be observed for the following layers\nfocalizing on the next level of the hierarchy, i.e. patches of patches. (b) Test error\nvs. number of training points for Deep (blue) vs Shallow (orange) FCNs. Deep\nnetworks are able to reach zero test error with a number of training points P that\nsatisfies P∗\ndCNN < P ≪Pmax, while shallow do not. . . . . . . . . . . . . . . . . . . 188\nxv\nList of Tables\n7.1\nSummary of Results. The rows represent the various data structures considered,\ncategorized by data invariances, while the columns detail the types of neural\nnetworks and training regimes studied, including 2-layer neural networks in\nboth the lazy and feature regimes, deep fully-connected networks, and deep con-\nvolutional networks. Gray cells indicate configurations that were not expressly\naddressed in our research. For each invariance, symbols provide a ranking of\nthe performance of the architecture and training regime we investigated (green:\nbest, orange: intermediate, red: worst).\nThe performance is more precisely reported by the predicted scaling exponent β,\nwhen the test error follows a power law, or by the sample complexity P∗. When\npredictions of test error are not available (i.e. for deformation invariance), we\nindicate the qualitative value of relative sensitivity R f , which correlates closely\nwith test error.\nThe algorithms used in each setting are also indicated, namely Support Vector\nMachine SVM, (Stochastic) Gradient Descent (S)GD, and Gradient Flow GF, that\nis GD with adaptive learning rate as defined in Geiger et al. (2020b). . . . . . . . 176\nxvii\nIntroduction\nMachine learning, a cornerstone of artificial intelligence, is driven by algorithms that learn\npatterns in data to perform tasks without explicit instructions. This field has brought about\nsignificant advancements across various domains and given rise to specialized branches.\nAmong these branches, deep learning is arguably the most successful. The goal of this thesis is\nto investigate the reasons behind this success.\nTo set the stage, we begin this introductory chapter with an overview of supervised learning\nfundamentals and the challenges inherent to it—particularly the bias-variance tradeoff and,\nmore crucially for this thesis, the phenomenon known as the curse of dimensionality. It seems\nparadoxical that modern supervised learning algorithms perform well in high-dimensional\ntasks despite the curse of dimensionality typically impeding such learning. This suggests\nthat high-dimensional data might be rich in structure in the form of invariances and symme-\ntries. Hence, this thesis aims to unravel the nature of this structure and how deep learning\nalgorithms, using suitable architectures, can exploit it.\nTo investigate this problem, we will review modern supervised learning algorithms, namely\nkernel methods and neural networks—readers familiar with these topics may wish to skip this\ndiscussion, as well as the preliminary one on supervised learning. One significant aspect we\nwill emphasize is the ability of neural networks to adapt to the features of the data and possibly\nlearn relevant representations, a capability absent in kernel methods. This leads us to the\nfollowing key questions: (i) What features do neural networks learn, are they indeed relevant\nfor the task at hand?, (ii) Does this feature learning contribute to some kind of dimensionality\nreduction, and therefore could it be a factor in overcoming the curse? and (iii) If so, can we\nquantify the impact of feature learning on performance in terms of how many data points are\nneeded for learning a given task?\nTo address these questions, we first review empirical works, specifically (i) techniques that\nallow us to visualize the representations learned by deep networks and (ii) observables to\ncharacterize the learning of task relevant representations in deep network.\nFor answering the third question, we will go into the theoretical study of the infinite-width\nlimit of neural networks, which allows establishing a strong link between neural networks and\nkernel methods. In fact, the same neural network architecture can be trained in two different\n1\nIntroduction\nregimes named feature and lazy, depending on the scale of parameters at initialization. The\nfeature regime corresponds to standard neural network training where features can be learned,\nwhile the lazy regime to kernel methods, where features are dictated by the architecture and\nremain static during training. The joint study of these two training regimes allows us to dissect\nthe impact of feature adaptation versus the influence of architecture choice, and thus to study\nwhether feature adaptation truly underpins the success of deep learning.\nHaving formulated a framework for gauging the benefits of feature learning, the next step\nis to identify which features of the data are indeed learned by neural networks. This leads\nus to a more in-depth discussion on data structure, which we will examine in terms of data\ninvariances. We will discuss three types of invariances. The first, linear invariance, is associ-\nated with the observation that some input coordinates may not be relevant for a given task.\nThe second, deformation invariance, reflects the notion that the precise location of relevant\nfeatures within the input might not affect the task. Finally, we will delve into synonymic in-\nvariance. This invariance builds on the notion that some tasks can be viewed as a hierarchical\ncomposition of features at varying levels, with the property that substituting these features\nwith their synonyms does not alter the label. We conclude this introduction by providing an\noverview of the thesis structure and main results, pointing out chapters that delve into the\ntopics introduced here. We finish with a note on our methodology.\n1.1\nMachine Learning\n1.1.1\nSupervised Learning\nSupervised learning is a machine learning paradigm where a model learns to make predictions\nbased on a set of labeled examples. More formally, given a set of P pairs of data (xi, yi),\nwhere xi ∈X represents an input and yi ∈Y represents the corresponding label or target, the\nobjective of supervised learning is to approximate the true, yet unknown, function f ∗: X →Y\nthat maps the inputs to the outputs.\nRegression and Classification Tasks\nSupervised learning can be broadly divided into two\ncategories: regression and classification. In regression, Y = R, and the task is to predict a\ncontinuous target variable. In contrast, in classification, Y = {1,2,...,C} represents a finite set\nof C discrete classes, and the goal is to assign each input to one of these classes.\nTo give a concrete example of a regression setting, consider a scenario where we try to predict\na person’s height (y) based on their age (x). While there exists a general trend or a true function\nf ∗(children grow as they age), the exact height of a person at a certain age is often influenced\nby noise factors such as genetics, diet, and lifestyle. We could model this situation as follows:\nyi = f ∗(xi)+ϵi,\n(1.1)\n2\n1.1 Machine Learning\nwhere f ∗represents the true underlying function, and ϵi ∼N (0,σ2) is Gaussian noise with\nzero mean and σ2 variance. This noise reflects the uncertainty or the individual differences\nthat cannot be captured by age alone.\nA classification task can be modeled similarly, except that the labels are categorical, e.g.\nY = {0,1} and they are given by\nyi = sign(f ∗(xi)+εi)\n(1.2)\nwhere sign(z) is a function that returns 1 if z ≥0, and 0 otherwise.\nBuilding a Model, Training, and Testing\nIn supervised learning, the objective is to find an\naccurate approximation to the true function f ∗typically through a parameterized model,\ndenoted as fθ. The parameters θ could be coefficients in linear regression, weights in a neural\nnetwork, etc., depending on the specific model used. These parameters are tuned using a set\nof training examples to produce an optimal function fθ that can accurately predict the labels\nof unseen, test examples. This ability of a model to effectively predict unseen data is known\nas generalization and, for classification tasks, it is measured as the percentage of errors the\nmodel makes on the test data set, the test error.\nTest Error vs. Number of Training Points\nIn this thesis, to evaluate a model’s performance,\nwe will typically examine its test error as a function of the number of training points, expressed\nas ϵ(P). The characterization of this function is practically valuable as it can provide insights\ninto the necessary sample size to meet a specified accuracy goal for a given task. In many\nreal-world settings Hestness et al. (2017); Spigler et al. (2020); Kaplan et al. (2020), this function\nexhibits a power-law behavior, that can be characterized by a scaling exponent β with ϵ(P) ∼\nP−β, as illustrated in Figure 1.1(a). Alternatively, one can characterize the test error by a\nsample complexity, defined as the number of training points P∗needed to achieve a given\nfinite test error, e.g. ϵ∗= 1%. The use of sample complexity as a characterization is especially\nuseful when the test error shows a transition in P from random guessing to nearly zero error\n(Figure 1.1(b)), as it will be the case in the artificial task introduced in chapter 6, for example.\nIn such cases, P∗pinpoints the location of this transition as P∗(ϵ∗) is characterized by a mild\ndependence on ϵ∗.\nOptimization\nThe process of training involves adjusting the parameters θ in order to mini-\nmize the difference between the predictions of fθ and the true output values y on the training\nset. This is usually achieved through an optimization procedure that finds the θ’s that mini-\nmize a training loss\nL = 1\nP\nX\ni\nl(fθ(xi), yi),\nwhere the loss function l quantifies the dissimilarity between predictions fθ(xi) and true\noutputs yi.\n3\nIntroduction\nnumber of training points, log P\ntest error, log ε\nnumber of training points, P\ntest error, ε\nP*\n-β\n(a)\n(b)\nε*\nP*\nε*\nFigure 1.1: Test error vs. number of training points: different scenarios and characteriza-\ntions. (a) When the curve follows a power law, ϵ(P) ∼P−β, it can be represented as a straight\nline in a log-log plot, where the exponent −β defines the slope of the line. (b) Alternative\nscenario where ϵ(P) transitions from a large error, corresponding to random guessing, to\nnearly zero error. Both scenarios can be described using the sample complexity, P∗, which\nquantifies the number of training points required to achieve a specific finite error, ϵ∗.\nAt the core of this training process is the definition of a suitable loss function. For instance, in\nregression tasks, the mean square error (MSE) is a popular choice. Given the predicted output\nˆy and the true output y, the MSE loss function is defined as:\nlMSE = ( ˆy −y)2.\nFor binary classification tasks, the hinge loss is frequently used\nlhinge = max(0,1−y ˆy),\nwhere y = ±1 and ˆy ∈R.\nIn the case of multi-class classification, a common choice of loss function is the cross-entropy\nloss. For a predicted vector of outputs ˆy and a label y, the cross-entropy loss is defined as:\nlCE = −\nCX\nc=1\nyc log( ˆyc),\nwhere C is the number of classes, y are the true labels encoded as a one-hot vector2 and ˆy\nrepresents the network output, normalized through a Softmax operation,\nσsoftmax(fθ(x)) =\ne(fθ(x))c\nPC\nc′=1 e(fθ(x))c′ ,\n2One-hot encoding of a class refers to a representation method in which the class of interest is assigned a value\nof one, while all other class categories are assigned a value of zero, thus creating a binary vector that uniquely\nidentifies the particular class.\n4\n1.1 Machine Learning\nwith (fθ(x))c denoting the c-th component of the output vector.\nThe optimization process that reduces the loss, and thus improves the model’s fit, commonly\nemploys gradient descent, which iteratively adjusts the model parameters in the direction that\nminimizes the loss:\nθt+1 = θt −η∇L (θt),\nwhere η is the learning rate controlling the size of optimization steps, t represents the step\nnumber, and ∇L (θt) is the gradient of the loss function with respect to the parameters, at the\ncurrent parameter values.\nIn this manner, we adjust the function fθ to align as closely as possible with the true function\nf ∗using the available training data. However, we stress again that the aim is not to perfectly\nfit the training data, but to generalize well to new, unseen data. This is where the challenges of\nsupervised learning start to emerge.\nBias-Variance Tradeoff\nThe bias-variance tradeoff Luxburg and Schölkopf (2011) is a key\nconcept that helps us understand the generalization error of a predictive model. The gener-\nalization error can be decomposed into bias, variance, and an irreducible error term due to\nnoise in the labels.\nThe bias of a model reflects the error introduced by approximating the true function (which\nmay be highly complex) by a simpler model. High-bias models oversimplify the problem,\nleading to underfitting. The variance, on the other hand, quantifies the sensitivity of our model\nto fluctuations in the training data. Models with high variance are likely to be over-complex,\nand therefore susceptible to overfitting. As an example, let’s consider the task of fitting a\npolynomial function\nfθ(x) =\nN\nX\ni=0\nθi xi\n(1.3)\nto some data as in Figure 1.2. If we try to fit this data using a high-degree polynomial (a\ncomplex model with many parameters), we may obtain a model that passes exactly through\nevery point of the training set but fluctuates wildly in between (high variance, overfitting).\nHowever, if we fit a simple linear model (a low-degree polynomial) to the same data, we may\nfind that our model does not capture the oscillating nature of the true underlying function\nat all (high bias, underfitting). In this scenario, a polynomial of intermediate degree may\noffer the best bias-variance tradeoff, capturing the broad trends in the data without being\noverly sensitive to noise. We see here that the number of parameters of the model affects its\ncomplexity.\nRegularization\nRegularization techniques offer a pragmatic approach to managing this\nbias-variance tradeoff by controlling the complexity of the model. Adding a regularization\nterm to the loss function, for example, places a constraint on the size or sparsity of the model\n5\nIntroduction\nx\ny\nf ∗\ndata points\nN = 1 (underﬁt)\nN = 5 (good ﬁt)\nN = 14 (overﬁt)\nFigure 1.2: Illustration of the bias-variance tradeoff in polynomial regression. Red crosses\nrepresent data points (xi, yi), with yi = f ∗(xi) + εi. The red curve represents f ∗, the true\nfunction. Three different polynomials of degrees N = 1,5,14 are considered. The linear\npolynomial (underfitting) exhibits high bias and low variance, failing to capture the complexity\nof the true function. The N = 5 polynomial achieves a good balance between bias and variance,\nfitting the true function. The N = 14 polynomial (overfitting) shows low bias but high variance,\nmodeling the noise in the data and deviating significantly from the true function.\nparameters. Regularization may allow us to find a balance between bias and variance.\nCommon regularization methods include L1 and L2 regularization, also known as Lasso\nTibshirani (1996) and Ridge regularization (or weight decay), respectively. These techniques\nintroduce a term to the loss function that penalizes the size of the model, with L1 regularization\nalso favoring sparse solutions in which many of the parameters are zeroed. The regularization\nterm is either the L1 or L2 norm of the parameters:\nλ\nX\nj\n|θj|,\nor\nλ\nX\nj\nθ2\nj ,\nwhere λ controls the regularization strength.\nDepending on the model at hand, other regularization techniques may be used—e.g. dropout\nSrivastava et al. (2014), early stopping, etc.\nInterestingly, the bias-variance tradeoff does not pose the same challenge for deep learning\nalgorithms. As we will explore in greater detail in subsection 1.2.2, deep learning models are\nintrinsically biased towards finding simple, small-norm solutions, even when their number of\nparameters diverges.\n6\n1.1 Machine Learning\n1.1.2\nThe Curse of Dimensionality\nWhile the bias-variance tradeoff manifests itself even in one-dimensional learning scenarios, a\ndistinct and fundamental challenge arises when we consider high-dimensional data: the curse\nof dimensionality Bach (2017); Wainwright (2019). This term refers to the various difficulties\nand counterintuitive phenomena that arise when dealing with high-dimensional data.\nFor instance, consider a dataset containing measurements of d-dimensional vectors. Suppose\nwe want to cover a fraction of the d-dimensional space with a grid of small hypercubes of side\nlength ϵ, one for each data point. The number of data points required to cover the space scales\nas (1/ϵ)d, which grows exponentially as the number of dimensions d increases. This implies\nthat even for a moderate value of d, we need a huge number of data points to populate the\nspace.\nThe issue is that as the number of dimensions grows, the space itself expands so fast that\nour data points start to seem sparse, or spread out. This sparsity can be problematic for any\nmethod trying to estimate a function in this space. Any new data point we use to test our\nmethod will likely be quite far from any other sample. To properly cover the space as the\ndimensionality increases we would need an exponentially growing amount of data. More\nconcretely, for regression tasks where the target is only assumed to be a Lipschitz continuous\nfunction, the test error decays only at a rate of −1/d with respect to the number of training\npoints Luxburg and Bousquet (2004), making learning impossible when d ≫1.\nThe Surprising Effectiveness of Deep Learning\nStrikingly, modern supervised learning\nalgorithms—more specifically, deep learning—are able to beat the curse of dimensionality\nand learn tasks even in very high dimensions. Examples of deep learning success range from\ncomputer vision Voulodimos et al. (2018) and natural language processing OpenAI (2023)\nto computational biology Jumper et al. (2021) and game playing Silver et al. (2017). Despite\nthese practical successes, a comprehensive theoretical understanding of how these algorithms\novercome the curse of dimensionality remains elusive.\nThe Intrinsic Dimensionality of Real Data\nOne possible explanation for this puzzle could\nlie in the fact that although our machine learning tasks typically involve high-dimensional\ndatasets, the actual data often exist in a lower-dimensional subspace.\nThe term intrinsic dimensionality (ID) is employed to describe the dimensionality of this\nsubspace and can be thought of as the minimum number of variables needed to characterize\nit. For instance, consider a dataset of points lying on a line within a three-dimensional space.\nAlthough they exist in a 3D space, their intrinsic dimensionality is just one, as only one\ncoordinate along the line accurately describes all points.\nThe intrinsic dimensionality of a dataset can be estimated by randomly drawing P data points,\nand analyzing the typical distance δ between nearest neighbors, typically scaling as δ ∼P−1/dID.\n7\nIntroduction\nIf these data points are embedded in a d-dimensional space but exist on a dID-dimensional\nmanifold, the ratio logδ/logP provides an estimate of the intrinsic dimensionality. Advanced\nintrinsic dimension estimation methods further use information regarding the neighborhood\nof datapoints, other than the nearest neighbor distance alone Levina and Bickel (2004); Granata\nand Carnevale (2016); Facco et al. (2017). However, it’s vital to approach these estimations\nwith caution. Although they can offer insights into the intrinsic dimensionality of a dataset,\nthey hinge on the assumption that real data reside on a smooth, continuous manifold of fixed\nintrinsic dimensionality. This assumption lacks empirical support, as the estimates regarding\nreal-world datasets can only be based on discrete sets of data points. We will further discuss\nintrinsic dimension estimation in subsection 1.2.1, in the context of neural networks’ internal\nrepresentations.\nWith this understanding, we can discuss the intrinsic dimensionality of real datasets, and the\nimplication on the curse of dimensionality. For a benchmark image classification dataset as\nImageNet Deng et al. (2009), intrinsic dimensionality is estimated to be around fifty Pope et al.\n(2021), while it contains about 107 data points. This number is orders of magnitude less than\nthe expected e50 ∼1020 data points needed to adequately sample such a high-dimensional\nspace and beat the curse. This fact suggests that lower intrinsic dimensionality alone isn’t the\nkey to deep learning’s success, implying additional structural elements within the data.\nInvariances give Structure to Real Data\nA popular idea is that this additional structure is\ndue to the presence of invariances, namely transformations of the input that leave the label\nunchanged Goodfellow et al. (2009); Bengio et al. (2013); Bruna and Mallat (2013); Mallat (2016).\nBy developing representations that are invariant to these transformations the dimensionality\nof the problem could be effectively lowered and the curse beaten. This thesis centers around\nthis concept of learning invariances—a phrase we use to describe the process of developing\nrepresentations that are invariant to aspects of the data irrelevant to the task at hand. More\nspecifically, our aim is to address the following set of questions:\n• Which invariances are present in real data?\n• Are deep learning algorithms able to learn these data invariances?\n• If so, how many training examples do they need to achieve that?\n• Is learning invariances indeed responsible for breaking the curse of dimensionality?\nBefore attacking these questions we need to introduce our main objects of study. We will\nstart with kernel methods that we will use as a proxy for investigating the performance of\nalgorithms in which features are fixed and hence learning invariances is not possible, and we\nwill then introduce neural networks, in which feature adaptation allows the learning of data\ninvariances.\n8\n1.1 Machine Learning\n1.1.3\nKernel methods\nKernel methods Scholkopf and Smola (2001) comprise a family of machine learning techniques\nthat utilize a fixed feature representation, and determine the optimal weights for these features.\nThe use of kernel functions facilitates the implicit mapping of data into a potentially higher-\ndimensional feature space, sometimes even infinite-dimensional, without having to compute\nthe coordinates of the data in that space explicitly.\nTo illustrate this, let’s consider the task of approximating a non-linear function with linear\nregression. A common approach to this problem involves creating a feature vector φ(x) that\nprojects each data point x into a high-dimensional feature space, and fitting a linear model to\nthese feature vectors:\nfθ(x) = θ⊤φ(x).\n(1.4)\nHowever, the direct computation of these feature vectors can be computationally challenging\nwhen the feature space is large or infinite-dimensional.\nThis is where the kernel trick comes in. This key technique in kernel methods bypasses\nthe explicit computation of these high-dimensional feature vectors. It introduces a kernel\nfunction k : X × X →R, which calculates the inner product between feature vectors in the\nhigh-dimensional feature space, k(x,x′) = 〈φ(x),φ(x′)〉. The representer theorem Schölkopf\net al. (2001) tells us that, when solving an optimization problem with L2 regularization of the\nform\nmin\nθ\n1\nP\nX\ni\nl(fθ(xi), yi)+λ∥θ∥2,\nthe predictor fθ(x) of Equation 1.4 can equivalently be written as:\nfα(x) =\nPX\ni=1\nαik(xi,x),\nwhere xi are the training inputs, and αi are the new (dual) parameters that can be learned\nfrom data. This formulation allows performing linear regression in infinite feature spaces and\nhence the application of linear techniques to non-linear problems, making kernel methods a\npowerful tool in machine learning.\nSmoothness and Curse of Dimensionality\nThe understanding of the generalization capa-\nbilities of kernel methods has been a significant subject of investigation. This understanding\ninvolves examining the smoothness of the target function, that is related to its differentiability.\nThe most favorable situation occurs when the target function resides in the Reproducing Kernel\nHilbert Space (RKHS) of the chosen kernel Scholkopf and Smola (2001). In such a case, the\nerror decays as O(1/\np\nP) Smola et al. (1998); Rudi and Rosasco (2017). However, in the context\nof isotropic kernels of the form k(x,x′) = k(|x −x′|), that are commonly used in practice, for\n9\nIntroduction\nthe target function to belong to the RKHS in dimension d, it must be s-times differentiable,\nwith s proportional to d. In high-dimensional settings, requiring the smoothness to grow with\nd is unreasonable, and this can be seen as a manifestation of the curse of dimensionality. If the\ntarget falls outside of the RKHS, the error rate degrades to O(P−s/d) Bach (2022), recovering\nthe scaling of O(P−1/d) when dealing with Lipschitz continuous functions (s = 1) Luxburg and\nBousquet (2004).\nExact Generalization Error Rates\nThe exact asymptotics of the test error in between the\nRKHS and Lipschitz continuity extrema have been computed in Spigler et al. (2020); Bordelon\net al. (2020); Canatar et al. (2021) in the case of isotropic kernel and noiseless target functions—\ni.e. function of the form Equation 1.1, without the noise term. More specifically, these works\nshow the presence of a spectral bias in kernel regression: the projections of the target function\non the eigenfunctions of the kernel with the largest eigenvalues are learned first. For isotropic\nkernels in dimension d, if the inputs x are distributed uniformly on the d-sphere, these\neigenfunctions correspond to Fourier modes in d = 1, and to spherical harmonics in larger\ndimensions. In particular, P training points allow for learning the first P modes. Extensions of\nthese results to the noisy case are discussed in Loureiro et al. (2022); Cui et al. (2022); Mei et al.\n(2022). Notice that these results rely on statistical assumptions on the predictor function and\nkernel features that are rarely satisfied in practice, and can cause the spectral bias prediction\nto fail in low-dimensional settings Tomasini et al. (2022b), while it appears to be accurate in\nhigh-dimensions.\nIn chapter 4, we will use the presence of a spectral bias to compute tight generalization error\nrates both for kernel methods and neural networks.\n1.1.4\nNeural Networks\nIn kernel methods, the selection of the kernel predetermines the feature vectors. We now\ntransition to exploring neural networks, which offer the flexibility to learn features directly\nfrom the data.\nNeural networks are at the heart of modern machine learning. They are built from intercon-\nnected nodes or neurons organized in layers and can learn to perform complex tasks. In this\nsection, we introduce some fundamental types of neural networks.\nPerceptron\nThe perceptron Rosenblatt (1958) is one of the simplest forms of a neural network\nand is the basic building block for more complex structures. A single perceptron takes a vector\nof inputs x, applies a set of weights w, adds a bias term b, and passes the result through a\nnon-linear activation function σ(·) to produce an output. Mathematically, this can be written\nas\nf (x) = σ\nµw⊤x\np\nd\n+b\n¶\n,\n10\n1.1 Machine Learning\nwhere parameters are initialized with a standard Gaussian distribution N (0,1) and d is the\ninput dimension. The\np\nd factor serves as a normalization to keep the argument of the\nactivation function O(1) in the limit of large d. Classically, activation functions such as the\nstep or sigmoid σ(z) = (1−e−z)−1 were employed as they reflect the active-inactive states of\nbiological neurons. However, in contemporary practice, the Rectified Linear Unit (ReLU),\nσReLU(z) = max(0,z),\nis most commonly utilized.\nTwo-layers Neural Networks\nA 2-layers (or one-hidden-layer) neural network extends the\nperceptron by introducing an additional layer of neurons. Each neuron in the hidden layer\nperforms a similar operation to the perceptron, and their outputs are then linearly combined\nto produce the final output. Formally, the operation of a 2-layers neural network can be\ndescribed by:\nf (x) = 1\np\nh\nw⊤\n2 σ\nÃ\nw⊤\n1 x\np\nd\n+b1\n!\n,\n(1.5)\nwhere w2, w1, and b1, are the weights and biases of the network, σ(·) is the activation function\nand h is the number of hidden neurons or network width. See also a graphical representation\nin Figure 1.3. This setup allows the model to learn more complex representations with respect\nFigure 1.3: Graphical representation of a 2-layer neural network with an input dimension\nd = 5 and h = 9 hidden neurons. The network function, denoted by f , takes as input the vector\nx. The activation function is denoted by σ(·) and applied to each hidden neuron independently.\nThe weights for the two layers are denoted by w2, w1, respectively. The different strength of\nthe lines depicts weights of different magnitude.\nto the perception. For example, a 2-layers can solve classification tasks that are not linearly\nseparable. More generally, the Universal Approximation Theorem states that 2-layer neural\nnetworks can approximate any function to arbitrary precision, provided they have enough\nneurons Hornik et al. (1989); Barron (1993).\n11\nIntroduction\nRandom Features\nNotice that usually all the network parameters of a 2-layers architecture\nare trained. A model in which only the second layer weights w2 are trained while the others are\nkept fixed is called random features model Rahimi and Recht (2007). This is a kernel method\nwhere the features are given by the first layer activations: φ(x) = σ\n³ w⊤\n1 x\np\nd +b1\n´\n.\nDeep Fully Connected Networks (FCNs)\nDeep fully connected networks (also known as\nmultilayer perceptrons) consist of multiple layers of neurons, with each layer fully connected\nto the next. A L-hidden-layers FCN can be described recursively as\nal = σ\nÃ\nw⊤\nl al−1\np\nhl\n+bl\n!\nfor\nl = 1,...,L,\n(1.6)\nwhere l is the layer index, wl and bl are the weights and biases for the l-th layer, al−1 is the\noutput of the previous layer with a0 being the input x, and hl the number of neurons at layer l,\nwith h0 being the input dimension d. The output of the network is then a linear combination\nof the last hidden layer activations,\nf (x) =\n1\np\nhL+1\nw⊤\nL+1aL.\n(1.7)\nDeep learning generically refers to solving machine learning tasks with deep neural networks.\nConvolutional Neural Networks (CNNs)\nConvolutional Neural Networks are a type of neural\nnetwork designed to process data in which locality is important in the sense that the relevance\nof each input coordinate or pixel is closely tied to its neighbors Lecun et al. (1998).\nx5\nx4\nx3\nx2\nx1\n(a) Fully Connected Neuron\nx5\nx4\nx3\nx2\nx1\n(b) Local Neuron\nFigure 1.4: Comparison between (a) a Fully Connected Neuron that receives input from every\nelement of the preceding layer, and (b) a Local Neuron, where each neuron processes data\nonly from a specific part of the input (in this case, the patch composed by first two pixels x1\nand x2).\n12\n1.1 Machine Learning\nFigure 1.5: Graphical representation of a convolutional neural network (CNN) with 4 convo-\nlutional layers with many channels each and 2 fully-connected layers. The CNN acts on 2D\nimages with filters of size 3×3 at each convolutional layer.\nUnlike FCNs where neurons respond to all input locations, CNNs have local neurons that\nfocus on specific patches of the input, see illustration in Figure 1.4. In addition, standard\nconvolutional layers also use weight sharing, where the same weights are used by neurons\nexamining different parts of the input. Weight sharing not only reduces the model’s number\nof parameters but also introduces translation equivariance into the network architecture.\nTranslation equivariance means that if the input is translated, the output will change in the\nsame way. If f is the activation of a particular layer of a CNN, and T is a translation operator,\nthen the layer is translation equivariant if:\nf (T (x)) = T (f (x))\n(1.8)\nwhere x is the input data. This property ensures that if a pattern is learned in one part of\nthe image, it will be recognized in any other part. Translation invariance can be obtained\nby pooling together equivariant representations, i.e. by summing or averaging equivariant\nactivations over all possible translations:\nX\nτ\nf (Tτ(x)).\n(1.9)\nHere, τ represents a translation, and Tτ is the corresponding translation operator. By sum-\nming over all translations, the response becomes independent of the specific location of the\nrecognized feature in the input, leading to translation invariance.\nEarly implementations of CNNs often relied on pooling layers, such as max-pooling or average-\npooling, to achieve approximate translation invariance by aggregating the responses of neu-\nrons within a local region. However, the use of pooling layers has diminished in modern\npractices and we will demonstrate that contemporary CNNs possess the capability to learn\npooling directly from data, as also noted in Ruderman et al. (2018).\nConvolutional layers have proven particularly effective in solving computer vision tasks,\nwhere locality is clearly important as neighboring pixels make sense together, and translation\ninvariance as well, as the same object or feature can appear in different parts of the image\nframe. In these cases, 2D local filters are used, leading to CNNs as the one illustrated in\nFigure 1.5. In particular, with the publication of the AlexNet paper by Krizhevsky et al. (2012),\nCNNs started the modern deep learning revolution. AlexNet won the ImageNet Large Scale\n13\nIntroduction\nVisual Recognition Challenge Deng et al. (2009), a prestigious image recognition competition,\nby a significant margin, outperforming the second-best entry by over 10%. This breakthrough\ndemonstrated the potential of deep CNNs to handle high-dimensional data, and it set the\nstage for the widespread adoption of deep learning techniques in a variety of domains.\n1.2\nFeature Learning in Neural Networks\nOur discussion so far has reviewed kernel methods and neural networks, touching upon the\nsuccesses of the latter. While kernel methods rely on fixed feature representations, neural net-\nworks are characterized by their ability to adapt features to the data at hand. This adaptability\nis often seen as a fundamental component of their success.\nIn this section, we discuss the adaptability of neural networks more in depth. First, we focus\non empirical studies to understand the nature of the features that these networks learn, and if\nthese are relevant for the task at hand. Following that, we survey empirical tools in existing\nliterature that elucidate how this feature learning process shapes representations in such a\nway of discarding irrelevant information. We then discuss how this representation learning\ncan lead to dimensionality reduction, and to eventually beat the curse. To determine whether\nrepresentation learning is indeed a key factor in overcoming the curse of dimensionality, and\nthereby contributing to the superior performance of neural networks, we require a framework\nthat quantifies the impact of feature learning on performance.\nAs hinted at in the previous section, modern neural networks’ successes also came with\nimportant architectural advancements. This raises the question: is it the adaptability that\nis primarily driving the success of deep learning, or is it more about the selection of the\nright architecture, or perhaps a combination of both? To answer these questions, we will\nexplore the deeper ties that link neural networks and kernel methods beyond random feature\nmodels. We will focus on the two distinct training regimes that emerge when we consider the\ninfinite-width limit of neural network architectures based on the scale of initialization: the\nNeural Tangent Kernel (NTK) or lazy regime, akin to kernel methods, and the mean-field or\nfeature learning regime, mirroring the training of neural networks with feature adaptation.\nThis discussion will provide a framework to disentangle the respective roles of architectural\nchoice—–which dictates the features for kernel methods—–and adaptability in driving neural\nnetworks performance.\n1.2.1\nEmpirical Studies of Feature Learning\nVisualizing the Learned Features\nIn this paragraph, we discuss the questions: What kind of\nfeatures of the data do neural networks learn thanks to their adaptability? Are they relevant\nfor the task at hand? Empirical research offers some fascinating insights into this matter.\nIn particular, it has been found that deep networks, especially CNNs, tend to learn relevant\nfeatures in a hierarchical manner Zeiler and Fergus (2014); Yosinski et al. (2015); Olah et al.\n14\n1.2 Feature Learning in Neural Networks\nFigure 1.6: Feature visualization in deep networks. This figure, adapted from Olah et al.\n(2017), uses feature visualization to showcase how GoogLeNet Szegedy et al. (2015), trained on\nthe ImageNet dataset Deng et al. (2009), builds its understanding of images progressively with\ndepth. These images are generated via gradient descent in input space such as to maximize\nthe neurons’ response at each layer. We can observe the images inducing maximal activation\nat increasingly deeper layers (left to right), growing in complexity and abstraction with depth.\n(2017). Early layers tend to focus on simple, local features, such as edges in an image. As we\nmove further into the depths of the network, these simple features are progressively combined\nto create more complex and abstract representations, including shapes or even entire objects,\nsee illustration in Figure 1.6.\nSeveral works tried to characterize this process of building more and more abstract represen-\ntations empirically. In this context, we review here the information bottleneck framework of\ndeep learning, and measurements of the intrinsic dimensionality of internal representations.\nThe Information Bottleneck\nThe information bottleneck provides a framework to analyze\nhow to compress a random variable X into an intermediate representation T , while keeping\nessential information about another related variable Y , given the joint probability distribution\nP(X ;Y ) Tishby et al. (2000). This leads to the following optimization problem:\nmin\nP(T |X )I(T ;X )−βI(T ;Y )\n(1.10)\nwhere I(T ;X ) is the mutual information3 between the representation and the original data,\nI(T ;Y ) is the mutual information between the representation and the target, and β is a\nparameter that controls the trade-off between compression and preservation of relevant\ninformation.\nIn the context of deep learning, this framework has been used to explore the functioning\n3Mutual Information is a measure that quantifies the amount of information gained about one random variable\nby observing another. It essentially describes how much knowing one variable reduces uncertainty about the other.\nIf the variables are independent, the mutual information is zero. If they are identical, the mutual information is\nthe same as the individual entropy of either variable.\n15\nIntroduction\nof neural networks. Shwartz-Ziv and Tishby (2017) argue that the information bottleneck\nframework can be used to understand how deep networks progressively compress input\ndata across successive layers, while maintaining task-relevant information, in order to build\nincreasingly abstract internal representations. The authors suggest that deep neural networks\ninternal representations (denoted by T in this context) tend to solve a problem of the form in\nEquation 1.10 when trained with stochastic gradient descent. In particular, they argue that the\ntraining process consists in two distinct phases—a fitting phase where it captures the relevant\ninformation about the output and both I(T ;X ) and I(T ;Y ) increase, and a compression phase\nwhere it sheds irrelevant input details, characterized by a decrease of I(T ;X ).\nHowever, Saxe et al. (2019) presents a critical examination of these claims. It posits that the\nconclusions drawn by Shwartz-Ziv and Tishby (2017) heavily depend on the activation function\nused in their experiments, making their findings not universally applicable. In particular, the\ncompression phase could not be observed if instead of the sigmoidal activation function,\nthe more common ReLU is deployed. Additionally, the critique highlights that the concept\nof mutual information, a cornerstone of the information bottleneck framework, isn’t well-\ndefined in deterministic settings. And the output or internal activations of a neural network is\ndeterministically defined for a given input.\nThe Intrinsic Dimension of Internal Representations\nAnother way to characterize the\nprocess of learning features with depth is to idealize the data representation at each network\nlayer as existing on a manifold in the activations space of that layer. By assessing the intrinsic\ndimensionality of these manifolds (cf. definition in subsection 1.1.2), we can gain insights into\nhow the network reduces the dimensionality of the problem at each layer. Ansuini et al. (2019);\nRecanatesi et al. (2019) show that the intrinsic dimensionality of internal representations\ntypically increases with training, in the first part of the network, while decreasing in the\nsecond part. Consequently, after training, the intrinsic dimensionality displays a concave or\nhunchback shape, as a function of depth. The reduction of dimensionality with training and\ndepth in the second part of the networks fits with the idea that learning progressively filters\nout irrelevant details to construct increasingly abstract representations. The initial expansion,\nhowever, is harder to parse. The authors interpret it as the neural networks generating a broad\nset of features at early stages, and later keeping only those essential for the task.\nAlthough this approach offers insightful observations, it comes with inherent difficulties. One\nkey issue is that it assumes that real data exist on a smooth manifold, while in practice, the\nmeasurements are based on a discrete set of points. This leads to counter-intuitive results\nsuch as the increase in the intrinsic dimensionality with depth near the input, an effect that is\nimpossible for continuous smooth manifolds. We resort to an example to illustrate how this\nincrease with depth can result from spurious effects. Consider a manifold of a given intrinsic\ndimension that undergoes a transformation where one of the coordinates is multiplied by\na large factor. This transformation would result in an elongated manifold that appears one-\ndimensional. The measured intrinsic dimensionality would consequently be one, despite\n16\n1.2 Feature Learning in Neural Networks\nthe higher dimensionality of the manifold. In the context of neural networks, a network\nthat operates on such an elongated manifold could effectively reduce this extra, spurious\ndimension. This operation could result in an increase in the observed intrinsic dimensionality\nas a function of network depth, even though the actual dimensionality of the manifold did\nnot change. This phenomenon appears as a plausible explanation for what is observed in\npractice. Also, if this interpretation holds, the measures of intrinsic dimensionality of real data\nwe discussed in subsection 1.1.2 might underestimate the true values.\nWhile these tools provide valuable insights, they also underscore the need for new methodolo-\ngies to probe the complex mechanisms responsible for building abstract and task-relevant\nrepresentations in deep neural networks. In this thesis, we propose additional tools to this\ngoal. These include the study of the neural tangent kernel after training (chapter 2), and,\ncentral to our work, the introduction of relative sensitivity measures for network activations\nin response to input transformations that leave the label unchanged (chapter 3, 4, 5 and 6).\nThese sensitivity measures are centered on the concept of data invariances that we discuss\nmore in depth in section 1.3.\nTaking a broader view, the methods we discussed in this section seem to suggest that deep\nneural networks are able to learn increasingly abstract representations that are also lower\nand lower dimensional with depth, as they disregard irrelevant variability in the input. A key\nquestion arises from this observation:\n• Does this dimensionality reduction via learning relevant features enable beating the\ncurse of dimensionality?\nTo address this question we need to understand how feature adaptation affects performance—\na matter that the empirical methods reviewed so far do not clarify. To investigate this point,\nwe will introduce the two different training regimes of neural networks as they have been\ncharacterized in the literature. As already hinted at, this will allow us to isolate the contribution\nof feature learning to neural networks’ performance, and to characterize it.\n1.2.2\nTraining Regimes: Feature vs. Lazy\nIn this section, we will consider highly overparametrized neural networks, in particular, their\ninfinite-width limit. The significance of this limit lies in the fact that it often corresponds in\npractice to the point of optimal performance. We discuss how to easily access this limit in\npractice via ensemble averaging, and its potential to elucidate various training regimes of\nneural networks, notably the feature and lazy regimes.\nOverparametrization and Double Descent\nOverparametrization in the context of neural\nnetworks refers to the scenario where the number of parameters in the model significantly\nexceeds the number of training data points. While we have seen that in traditional machine\n17\nIntroduction\ndouble descent\nbias-variance\ntradeoff\nFigure 1.7: Illustration of the bias-variance trade-off vs. double descent phenomenon.\nTest error is reported on the y-axis, number of parameters of the model on the x-axis. The\nstandard bias-variance trade-off that is present when e.g. fitting a function with a polynomial\nas in Figure 1.2, would predict the U-shaped behavior shown in blue (N, in this case, would\ncorrespond to the degree of the polynomial). Deep neural networks, instead, show a behavior\nlike the one reported in green.\nlearning settings, such scenario can lead to overfitting due to the bias-variance tradeoff\nintroduced in section 1.1 (see also illustration in Figure 1.2)—deep learning models have\nrepeatedly shown that they can generalize well even when highly overparametrized Neyshabur\net al. (2015); Zhang et al. (2017); Advani et al. (2020). Neyshabur et al. (2015) highlight that\nthis may be the case thanks to an inductive bias or implicit regularization toward small norm\nsolutions. In this context, a larger number of hidden neurons would allow for solutions of\nlower complexity to exist, and training would find them thanks to the inductive bias.\nOne of the most striking demonstrations of this seemingly counter-intuitive behavior is the\nphenomenon of double descent, as described by Spigler et al. (2019); Belkin et al. (2019); Advani\net al. (2020); Nakkiran et al. (2021); Deng et al. (2020); Gerace et al. (2020); Hastie et al. (2020);\nBelkin et al. (2020); Mei and Montanari (2020); d’Ascoli et al. (2020a,b). The double descent\ncurve refines our understanding of the bias-variance trade-off showing that the test error of a\nmodel does not always monotonically increase with overparametrization, as the U-shaped\nbias-variance curve would suggest. Instead, the curve exhibits a second descent, starting\nfrom the point where the model becomes overparametrized, see illustration in Figure 1.7. The\ndouble descent is a robust phenomenon that can be observed across a variety or architectures\ndatasets and optimization procedures Nakkiran et al. (2021).\nThe second descent starts at the interpolation threshold, this is the minimal number of\nparameters for which the network can fit all training data, and is due to the noise coming from\nthe random initialization of the network parameters Neal et al. (2019); Geiger et al. (2020a).\nIndeed, if one uses an ensemble of differently initialized networks as the predictor, the second\npeak disappears, and, when it can fit the training data, such a predictor often achieves the\nasymptotic test error a single network would display when the number of parameters tends to\ninfinity Geiger et al. (2020a).\n18\n1.2 Feature Learning in Neural Networks\nHaving illustrated the significance of the infinite-width limit, and how to access it in practice,\nin the next paragraphs we will see how this limit serves as a foundation for the theoretical\nunderstanding of neural networks behavior, both at initialization and in the training process.\nPropagation at initialization\nIn the infinite-width limit, the signal propagation at initializa-\ntion has been extensively explored Neal (1996); Williams (1997); Lee et al. (2018); de G. Matthews\net al. (2018); Novak et al. (2019). Early foundational work by Neal (1996) and Williams (1997)\ndemonstrated that a single-layer network with infinite hidden nodes behaves like a Gaus-\nsian process Görtler et al. (2019). This was later extended to deep networks Lee et al. (2018),\nshowing that in the limit of infinite width, a deep neural network with random weights also\nbehaves like a Gaussian process. These findings have paved the way for more recent studies\ninvestigating the infinite-width dynamics of neural networks.\nNeural Tangent Kernel and the Lazy Regime\nJacot et al. (2018) showed that in the limit of\ninfinite width, the dynamics of deep neural networks during training can be exactly described\nby a kernel coined the Neural Tangent Kernel (NTK). They observed that the evolution under\ngradient descent of the network parameters in parameters space\n∂tθ = −∇θL ,\n(1.11)\n= −1\nP\nPX\ni=1\n∇θ f (xi)∇f l(f , yi)\n(1.12)\ncould be rewritten in function space via the chain rule as\n∂t f (x) = ∂θ f (x)∂tθ\n(1.13)\n= −1\nP\nPX\ni=1\n∇θ f ⊤(x)∇θ f (xi)∇f l(f , yi)\n(1.14)\n= −1\nP\nPX\ni=1\nΘ(x,xi)∇f l(f , yi),\n(1.15)\nwhere we introduced the NTK:\nΘ(x,x′) = ∇θ f (x)⊤∇θ f (x′).\n(1.16)\nImportantly, they discovered that, at large widths, the network’s parameters move little with\nrespect to initialization. For this reason, this regime has been also coined lazy Chizat and Bach\n(2018). As a consequence, the NTK remains constant during training in the infinite width limit,\neffectively evolving the network function in a linearized subspace around initialization Lee\net al. (2019).\nThis work showed that, in this limit, neural networks effectively behave as kernel methods,\n19\nIntroduction\ntwo machine learning approaches previously considered very different. Furthermore, this\nconnection allowed to prove the convergence of gradient descent to zero loss solutions Li and\nLiang (2018); Du et al. (2019a,b); Allen-Zhu et al. (2019); Chizat and Bach (2018); Soltanolkotabi\net al. (2019); Arora et al. (2019a); Zou et al. (2020). Arora et al. (2019b) extended the analysis to\nconvolutional networks by computing the NTK of these architectures and the diagonalization\nCagnetta et al. (2022) of such kernels allowed for understanding which functions they are able\nto efficiently learn.\nFeature or Active Regime\nWhile the NTK framework sheds light on the lazy training regime of\ndeep learning, several works have highlighted a distinct, complementary aspect known as the\nfeature, active, rich or mean-field regime Mei et al. (2018, 2019); Rotskoff and Vanden-Eijnden\n(2018); Chizat and Bach (2018); Sirignano and Spiliopoulos (2020); Nguyen (2019).\nIn this regime, we consider 2-layer neural networks whose width is taken to infinity but with\na crucial modification to the output definition. Specifically, an additional factor 1/\np\nh is\nintroduced in the output of the model as defined in Equation 1.5, leading to a rescaling of the\nform:\nfθ(x) →1\np\nh\nfθ(x),\n(1.17)\nThis rescaling makes the output ≪1 at initialization, hence, to fit a target function of order\none, the weights need to change significantly from their initialization. This limit gives rise to\nthe so-called feature learning regime as neurons learn how to respond to different aspects of\nthe input data in a substantial, rather than infinitesimal, manner.\nIn the feature learning regime and for h →∞, the neural network can be described entirely\nin terms of the density ρ(w2,w1,b1) of parameters characterizing each neuron. An integral\neffectively replaces the sum in the network’s output, which now takes the form:\nfθ(x) =\nZ\ndw2dw1db1ρ(w2,w1,b1)w⊤\n2 σ\nÃ\nw⊤\n1 x\np\nd\n+b1\n!\n.\n(1.18)\nGradient descent dynamics under this limit leads to a dynamical equation on ρ that is the\ntypical one in physics for conserved quantities: i.e. the divergence of a flux\n∂tρ = −∇· J,\n(1.19)\nwhere J = ρΨ(w2,w1,b1;ρt) and Ψ is a function that can be expressed in terms of the loss\nfunction Mei et al. (2018).\nThe term hydrodynamic comes into play due to the analogy between the evolution of ρ and\nthe hydrodynamics of interacting particles, each representing a neuron, in some external\npotential defined by the loss function. This formulation elegantly describes the collective\nbehavior of the neurons as if they were fluid particles moving in a potential.\n20\n1.2 Feature Learning in Neural Networks\nDespite their value, these studies have limitations. It’s apparent from Figure 1.6 that depth\nplays a crucial role in feature learning. However, even though the mean-field limit has been\ninvestigated for deep networks Araújo et al. (2019); Sirignano and Spiliopoulos (2021); Nguyen\nand Pham (2023), incorporating depth into the mean-field framework fails to provide a com-\npact description of the training dynamics, making the task of tracking it infeasible.\nIn sum, the feature learning regime provides a deeper understanding of why and when neural\nnetworks move beyond linear models, and it emphasizes the importance of understanding the\ninterplay between these two regimes to fully grasp the behavior of deep learning. Specifically,\nexamining these two regimes could help disentangle the influence of feature adaptation and\narchitectural choice on the performance of neural networks. This perspective gives rise to a\ncrucial question for this thesis: Is the future a return to kernel methods, as seen in the lazy\nregime, or does the magic of deep learning mainly stem from its ability to learn the relevant\nfeatures in the data?\nFeature vs Lazy Regimes: Performance\nMany studies have focused on the transition be-\ntween feature and lazy training regimes in deep learning Chizat and Bach (2018); Geiger et al.\n(2020b); Woodworth et al. (2020)—see also our review paper Geiger et al. (2021), omitted\nfrom this thesis. These works have shown that, even at finite widths, the scale of initialization\ncontrols the transition between the two regimes. This understanding has facilitated empirical\ninvestigations of performance in different settings as a function of the initialization scale.\nChizat et al. (2019); Geiger et al. (2020b, 2021); Lee et al. (2020); Woodworth et al. (2020) has\nshown interesting differences in performance between the two regimes. When learning image\ntasks, fully-connected networks trained with gradient descent tend to do better in the lazy\ntraining regime. In contrast, for convolutional neural networks, feature learning generally per-\nforms better, see also Arora et al. (2019b); Shankar et al. (2020). This difference in performance\nleads to several important questions that we are going to address in the following chapters.\n• What causes this performance gap between the two regimes?\n• More specifically, can we design simple data models to make sense of the generalization\nerror difference between feature learning and lazy training (chapter 2)?\n• Why do FCNs struggle with image data in the feature learning regime when trained via\ngradient descent (chapter 4)?\n• Why CNNs succeed in such setting (chapter 3 and 6)?\n• Which mechanisms are responsible for successful feature learning in CNNs (chapter 5)?\nTo provide answers to these questions we need to discuss how to model and characterize the\nstructure of the data that the feature learning regime can adapt to.\n21\nIntroduction\n1.3\nData Structure\nData in the real world comes in various forms, each with its own unique properties and\nstructures. The ability of a neural network to profitably learn features from data depends on its\ncapacity to adapt to these particular structures. Therefore, understanding and characterizing\nthe structure of data becomes a necessary step for assessing the benefits of feature learning\nquantitatively.\nIn particular, in this thesis we push forward the idea that this structure can be better char-\nacterized in terms of data invariances, that is aspects of the input data that leave the label\nunchanged. For instance, the stylized dog sketch in Figure 1.8 shows how a few lines can\ncapture the essence of an object. This suggests that pixels at the corner of the frame, or the\nexact position of the relevant features, do not matter for recognizing the dog. Likewise, in\nhierarchical tasks like text, synonyms can be exchanged without altering the overall content\nof a sentence. Understanding which invariances rise from these properties of data, and their\nFigure 1.8: Stylized representation of a dog. This figure illustrates the idea of data invariances\nwithin image data, where recognizable features can be represented with a few lines, and small\ndeformations often do not alter the overall semantic content.\nroles across different tasks, is a key step toward comprehending how deep learning models\nlearn. In the sections that follow, we better define and investigate these different types of\ninvariances, discussing how neural network architectures are able to exploit them. We will\npoint to the chapters of this thesis that address these concepts in depth.\n1.3.1\nLinear Invariance\nArguably the simplest invariance that gives structure to data is linear invariance. This man-\nifests when the target function is highly anisotropic, in the sense that it depends only on a\nlinear subspace of the input space—i.e. the target function is of the form\nf ∗(x) = g(Ax)\nwhere\nA : Rd →Rd′\nand\nd′ ≪d.\n(1.20)\n22\n1.3 Data Structure\nThis functional form for the target is often referred to as single-index model in the literature.\nAn example of such linear invariance in real data could be pixels at the corner of an image,\nwhose content may be irrelevant for the task.\nShallow FCNs can learn Linear Invariance\nSeveral works have established that 2-layers fully\nconnected networks can profitably exploit linear invariance in the feature learning regime\nBarron (1993); Bach (2017); Chizat and Bach (2020); Schmidt-Hieber (2020); Yehudai and\nShamir (2019); Ghorbani et al. (2019, 2020); Wei et al. (2019). In particular, Barron (1993);\nMei et al. (2016); Bach (2017) characterize the approximation properties of 2-layer networks,\nwhile Bach (2022); Schmidt-Hieber (2020); Chizat and Bach (2020); Ghorbani et al. (2020)\ndiscuss generalization, showing that these networks in the feature regime can beat the curse\nof dimensionality by adapting to the low-dimensional subspace, while this is not the case\nfor kernel methods. Several results followed showing an advantage of the feature over the\nlazy regime, in various specific classification and regression settings with anisotropic target\nfunctions Ghorbani et al. (2019, 2020); Refinetti et al. (2021). Damian et al. (2022); Abbe et al.\n(2021, 2023); Bietti et al. (2022) have tackled the problem of determining the required number\nof samples to learn anisotropic tasks. They provide upper bounds for the sample complexity\nof the feature regime and highlight that it performs better than the lazy regime for similar\nproblems Ghorbani et al. (2019). It remains unclear whether these bounds are tight in practice.\nThe work presented in chapter 2 fits in this line of research. Specifically, we revisit the binary\nclassification task introduced in Paccolat et al. (2021b) and show that, if the target function\nonly depends on a linear subspace of input space, the weights associated with the relevant\ninput subspace grow by O(\np\nP) compared to the irrelevant weights, thereby building the right\nfeatures for the task. This allows us to estimate the rates of generalization error with P for both\nfeature and lazy regimes. Notably, we demonstrate that these estimates are tight, in the sense\nthat they accurately predict generalization error in practice. Importantly, our examination of a\nproblem where test error versus P follows a power law further enables us to draw qualitative\ncomparisons with observations from real-world datasets, where power laws behaviors are\nubiquitous Hestness et al. (2017); Spigler et al. (2020); Kaplan et al. (2020).\nLimitations of the study of Shallow Networks\nWhile this growing body of theoretical work\nallows for a better and better understanding of the advantages of learning features in 2-layers\nneural networks, we have seen that, for these architectures, learning features is sometimes in\npractice a drawback Geiger et al. (2020b); Lee et al. (2020). This implies that linear invariances,\nsuch as those concerning boundary pixels, may not be the most crucial in practical applica-\ntions, and other forms of invariances might be at play for beating the curse of dimensionality.\n23\nIntroduction\nFigure 1.9: An image of a dog x and its deformed version τx. The deformation τ is sampled\nfrom the distribution we introduce in chapter 3.\n1.3.2\nDeformation Invariance in Images\nIt is fascinating to note that we can often identify objects even from simple sketches containing\nonly a few lines, as shown in Figure 1.8. This ability suggests that the important features in\nimages are sparse in space, in the sense that they only occupy small portions of the image\nframe. As a consequence, the image frame can be slightly deformed, and hence the relevant\nfeatures be moved, without altering the overall content of the image. Examples of such small\ndeformations are shown in Figure 1.9. This hypothesis—–that effective image classifiers should\nbe stable to deformations–—was advanced in Bruna and Mallat (2013); Mallat (2016). For a\nfunction f , being stable to deformations means that, given an image x and an operator τ that\napplies a smooth deformation, then ∥f (x)−f (τx)∥is small, if the norm of τ is small. Bruna\nand Mallat (2013); Mallat (2016) further propose an architecture, the Scattering Transform, that\nis specifically designed to be stable to deformation by the use of localized filters at different\nscales and frequencies, providing insights into which kind of filters CNNs may learn to achieve\nsuch stability.\nThis picture naturally leads to an important question:\n• Is this intuitive hypothesis verified in practice? In other words, do the high-performing\nmodern neural networks actually succeed because they effectively leverage deformation\ninvariance?\nEmpirical studies have shown that even slight shifts, rotations, or scale changes in images\ncan significantly alter the network’s output Azulay and Weiss (2018); Zhang (2019). This\nobservation seems to contradict the hypothesis that CNNs are robust to minor deformations.\nHowever, the image transformations applied in these works often do not qualify as small\ndeformations as they led to images with statistical properties dramatically different from the\ntraining set and involved procedures such as image cropping. A class of smooth deformations\nis introduced in Ruderman et al. (2018), suggesting that some level of deformation stability\ncan be learn by deep neural networks. However, similarly to the previous studies, this work\nalso did not investigate the impact of the learned stability on performance.\n24\n1.3 Data Structure\n(b) Hierarchy in Text\n(a) Hierarchy in Images\nsentence\nmain clause\nsub-clause\nsubject  predicate subject  predicate\nverb   object\nverb        object\nadj. noun\ndog\nhead\npaws\neyes\nnose\nmouth\near\nedges\n...\nFigure 1.10: Illustrating the hierarchical structure of real data. (a) An example of the hierar-\nchical structure of images: the class (dog) consists of high-level features (head, paws), that\nin turn can be represented as sets of lower-level features (eyes, nose, mouth, and ear for the\nhead). (b) A similar hierarchical structure can be found in natural language: a sentence is\nmade of clauses, each having different parts such as subject and predicate, which in turn may\nconsist of several words.\nMoving forward, we need a more comprehensive empirical examination of deformation\nstability in neural networks. This is the aim of chapter 3. In this chapter, we build an empirical\nframework to study deformation stability in neural networks. This framework includes both an\nensemble of diffeomorphisms of controlled norm to be applied to images, and an observable\nto characterize deformation invariance. We will investigate whether deformation invariance is\nlearned in neural network, or present from the start, and how different neural networks achieve\ndifferent levels of invariance on benchmark image classification tasks. Finally, we establish a\nrelationship between the learned deformation stability and performance of neural networks.\n1.3.3\nHierarchical Tasks and Synonymic Invariance\nThe invariance discussed in the previous section is related to the fact that relevant features only\noccupy a small portion of the whole input. However, this does not address how these features\nneed to be combined to accomplish a task, nor what is the role of depth in neural networks\narchitectures. This leads us to another property that is likely relevant for the learnability of\nreal data: hierarchical compositionality. This property prescribes how different low-level\nfeatures are combined in order to produce higher-level features and eventually the label. This\nis a concept highlighted in multiple studies Lee et al. (2009); Bruna and Mallat (2013); Patel\net al. (2015); Mossel (2018); Poggio et al. (2017); Mhaskar and Poggio (2016, 2019); Malach\nand Shalev-Shwartz (2018); Schmidt-Hieber (2020); Cagnetta et al. (2022), among others. To\ngive a concrete example, consider an image of a dog—see illustration in Figure 1.10(a). At\nthe highest level, we recognize the overall form of the dog. Breaking it down, we notice more\n25\nIntroduction\nspecific features: the head, the body, the tail, and the legs. Each of these elements can be\nfurther divided into even lower-level features, such as the eyes, nose, and mouth on the head,\nor the fur patterns on the body. This hierarchical structure, from general to specific, mirrors\nhow we recognize and interpret complex images. In fact, such a hierarchy can be found both\nin the visual cortex Gazzaniga et al. (2006) and in the layers of artificial networks trained on\nimage classification (Figure 1.6), suggesting a fundamental relevance of these structures in\nthe processing and understanding of natural visual data. Finally, this hierarchical structure\nis not exclusive to vision as it can be also found, for example, in natural language where a\npiece of text can be decomposed into paragraphs, sentences, sub-clauses, words, and syllables\n(Figure 1.10(b)).\n• Can deep neural networks trained by gradient descent efficiently solve hierarchical\ntasks?\n• How many data points do they need to achieve small generalization error?\n• Furthermore, is there an associated invariance with the hierarchical structure, and can\nits understanding aid in answering the aforementioned questions?\nPrevious Works\nPoggio et al. (2017); Mhaskar and Poggio (2016, 2019) study the approxima-\ntion properties of shallow and deep networks of hierarchically local compositional functions\nas for example\nf (x) = g4(g1(x1,x2,x3),g2(x1,x4),g3(x5)).\n(1.21)\nEven if both shallow and deep networks can approximate any function with enough parame-\nters, deep networks only need a number of parameters linear in d, while shallow networks\nneed exponentially many. Moreover, deep networks allow for solutions whose generalization\nerror is controlled by the maximum dimensionality of the constituent functions instead of\nthe input dimension d Schmidt-Hieber (2020). Gradient descent, though, can efficiently\nlearn these kinds of tasks only if correlations between input features and the target label exist\nShalev-Shwartz et al. (2017); Mossel (2018). To understand how the magnitude of correla-\ntions influence the sample complexity of gradient descent Malach and Shalev-Shwartz (2018,\n2020) introduce a class tasks where inputs are a hierarchical composition of multiple levels of\nfeatures, starting from a class down to low-level features. They further propose a sequential,\nlayer-wise algorithm that alternates clustering with gradient descent steps, and are able to\ncompute the sample complexity of such algorithm as a function of input-label correlations.\nThese seminal works start to address the questions we highlighted in this section. It still\nremains unclear though what is the sample complexity of the standard deep learning algo-\nrithms, i.e. vanilla or stochastic gradient descent on modern deep networks like convolutional\nneural networks, and if hierarchical compositionality gives rise to some kind of invariance. In\nchapter 6 we answer these questions in the context of a generative model of hierarchical data\nthat falls in the class of Malach and Shalev-Shwartz (2018), but for which we can explicitly\n26\n1.4 Overview of the Thesis\ncompute input-output correlations in terms of the model parameters. We show what is the\nrole of these correlations in determining the sample complexity of standard algorithms, we\nintroduce the invariance with respect to the exchange of synonymous features as a way to\ncharacterize dimensionality reduction and show that deep networks are able to profitably\nlearn it.\n1.4\nOverview of the Thesis\nThe core focus of this thesis is an exploration of deep neural networks’ ability to adapt to\ndata structures, specifically focusing on data invariances, and the consequential impacts of\nsuch adaptability on network performance. The various questions raised in this introduction\nserve as the central motivations guiding our research. In the first part of this section, we\nprovide an overview of the content of the thesis, which is divided into four parts. We conclude\nthis introductory chapter by describing the methodological framework behind our work\n(subsection 1.4.2).\n1.4.1\nStructure of the Thesis and Main Results\nIn Part I we start our investigation of the role of data invariances in the success of deep\nlearning, focusing on linear invariance.\n• In particular, in chapter 2 we develop a quantitative framework to study how learning\ninvariant representations can benefit performance, and then use this framework to\ngain qualitative understanding of real-world scenarios. We consider a simple model of\nlinear invariance, where the data points lie in d dimensions while the labels vary within\na linear manifold of lower dimension d′ < d. We provide evidence that in the feature\nlearning regime, shallow neural networks can effectively adapt to the data structure,\nbecoming invariant to the uninformative directions in input space by aligning weights\ntowards the relevant directions. We quantify the magnitude of this alignment and show\nthat it depends on the square root of the training set size. Contrarily, we show that\nthis adaptation is absent in neural networks trained in the lazy regime, resulting in\npoorer performance. We quantify this gap with a prediction on the scaling exponent of\ngeneralization error vs. the number of training points, and we empirically verify that\nthese exponents are tight in practice.\nTo better understand the benefits of feature learning, we study the evolution of the\nNeural Tangent Kernel (NTK) over training time. We find that, as the NTK evolves, only\na few of its eigenvalues become non-negligible, and the corresponding eigenvectors\nbecome more informative and align closely with the labels. This process leads to a\nkernel better suited for the task, with a performance matching the one of the feature\nregime. This finding applies to both shallow networks trained on the simple model of\nlinear invariance and to deep CNNs trained on benchmark image datasets, highlighting\n27\nIntroduction\nthe importance of invariants learning in both settings.\nMotivated by the need to study more complex forms of invariances to understand the perfor-\nmance of deep neural networks on real data, Part II is devoted to the study of deformation\ninvariance.\n• In chapter 3, we aim to provide empirical evidence to the hypothesis that deep net-\nworks learn deformation invariance with training, hence reducing the dimensionality\nof the problem and leading to improved performance. To do so we introduced a maxi-\nmum entropy distribution over diffeomorphisms. This allowed for generating typical\ndiffeomorphisms with controlled norm.\nWe then define the relative stability to diffeomorphisms, denoted as R f , which is an\nempirical observable that characterizes the stability of the network function f when\ntransforming the input along a diffeomorphism, relative to the stability to additive noise\nof the same amplitude. The need for defining stability in relative terms arises from the\nobservation that different network architectures exhibit varying levels of stability to\nadditive noise, with better architectures typically being less stable.\nBefore training, R f is found to be close to one for various datasets and architectures,\nsuggesting that the initial network output is as sensitive to smooth deformations as it\nis to random perturbations of the image. However, after training, we observe a strong\ncorrelation between R f and the test error, with R f reducing significantly in modern\narchitectures and benchmark image datasets with training. Contrarily, for FCNs R f\neven increases with training, highlighting their inability to learn deformation invariance.\nThese results support the hypothesis that learning diffeomorphism invariance is key to\nachieving good generalization performance in image tasks.\nThis naturally raises the question of why fully-connected neural networks perform so\npoorly, with an R f that even increases with training, and how this invariance is mecha-\nnistically achieved by deep neural networks that perform well. These two questions are\naddressed in chapter 4 and chapter 5, respectively.\n• In chapter 4, we explore the limitations of feature learning in 2-layers neural networks\nfor settings where non-linear invariances are present. In particular, we argue that if a\ngiven task presents a non-linear invariance, then it is better solved by a predictor that\nhas little variations (i.e. is smooth) along directions of input space that correspond\nto the invariance. A fully-connected network would require a continuous distribution\nof neurons to represent such a task. However, in the feature regime and in the limit\nof small initialization or with regularization of the weights, neurons become sparse,\ni.e. orient themselves along a finite number of input directions Bach (2017); de Dios\nand Bruna (2020). In the case where linear invariances are not present, this sparsity\ncan lead to a predictor that overfits some spurious directions that are not relevant for\nthe task. We argued that this picture applies to: (i) regression of random functions of\n28\n1.4 Overview of the Thesis\ncontrolled smoothness on the sphere—where the target is stable to small rotations—\nand (ii) classification of benchmark image datasets—where the target is stable to small\ndeformations.\nIn particular, we characterize the generalization error of neural networks trained in\nthe feature and lazy regime in the first setting (i), and find that lazy training leads to\nsmoother predictors than feature learning. As a result, lazy training outperforms feature\nlearning when the target function is also sufficiently smooth. We derive generalization\nerror rates through asymptotic arguments that we systematically back up with numerical\nstudies.\nIn the second setting (ii), we build on our observation that good performance in image\nclassification is associated with the stability or smoothness of the predictor function\nalong diffeomorphisms. In fact, we show that lazy training, by maintaining a con-\ntinuous distribution of neurons, results in predictors with smaller variations along\ndiffeomorphisms compared to the feature regime, which leads to sparse solutions. This\nobservation offers an explanation as to why lazy training outperforms feature learning\nin the context of image datasets.\nThis chapter focuses on the drawbacks of feature learning. Why this training regime is\nso effective when training modern architectures is still an open question.\n• Building on the results of chapter 3, the goal of chapter 5 is to elucidate some of the\nmechanisms by which CNNs learn the deformation invariance when classifying images,\nand how this surprisingly leads to instability to noise. We discuss two kinds of pooling\nmechanisms that can grant deformation invariance, and disentangle their role. The\nfirst, spatial pooling, integrates local patches within the image, thus losing the exact\nlocation of its features. The second, channel pooling, requires the interaction of different\nchannels, which allows the network to become invariant to any local transformation by\nproperly learning filters that are transformed versions of one another. Our experiments\non benchmark image datasets reveal that both kinds of pooling are learned by deep\nnetworks and are responsible for improving deformation invariance. In particular, we\nshow that spatial pooling is learned by making filters low-frequency.\nWe then focus on understanding how deep networks learn spatial pooling. To do so,\nwe introduce idealized scale-detection tasks that can be solved by performing spatial\npooling alone. Our findings suggest that the neural networks performing best on real\ndata tend to excel in these tasks, highlighting their relevance in mirroring real-world\nscenarios. These tasks also open the way for theoretical analysis. In particular, they\nallow us to understand how deformation stability is achieved layerwise, by developing\nlow-frequency filters, and how this naturally results in instability to additive noise.\nAfter discussing how neural networks are able to handle the spatial sparsity of features, we\ngo into the study of how these features are composed in order to solve hierarchically and\ncompositionally local tasks (Part III).\n29\nIntroduction\n• To this end, in chapter 6 we introduce a model for hierarchical data that we name the\nRandom Hierarchy Model, which is characterized by classes composed of combinations\nof high-level features, iteratively constructed from sub-features. Moreover, multiple\ncombinations of sub-features can construct the same high-level feature. We refer to\nthese different combinations as synonyms.\nOne of our key findings is that the number of training data, referred to as sample com-\nplexity and denoted as P∗, required by deep convolutional neural networks trained by\ngradient descent to learn this task, contrary to what one might expect from dealing with\nhigh-dimensional data, grows only polynomially with the input dimensionality. This\nsuggests that the curse of dimensionality can indeed be overcome for such hierarchical\ntasks. Our study further reveals that the sample complexity of deep CNNs P∗is closely\nlinked with the learning of invariant representations. Specifically, P∗corresponds to the\ntraining set size at which the network’s representations become invariant to exchanges\nof synonyms–—in other words, it no longer matters for the internal representations\nwhich specific features are used to represent a class, as long as they are semantically\nequivalent. This is a critical aspect of learning hierarchically compositional tasks, as it\nshows that deep CNNs are not merely learning to recognize specific features, but are\nlearning the underlying structure and relationships among the features. Notice that the\npicture is very different for shallow neural networks as they can only solve the task with\na number of data points that is exponential in the dimension, hence incurring in the\ncurse of dimensionality.\nFurthermore, P∗coincides with the number of data at which the correlations between\nlow-level features and classes become detectable. This finding suggests that deep CNNs\nleverage the correlations in the data in order to solve the task, enabling us to rationalize\nhow the sample complexity depends on the hierarchical and compositional structure of\nthe task.\nThis introductory chapter has attempted to highlight the primary outcomes of our research.\nThe chapters that follow delve deeper into these findings, each representing an individual\nresearch paper that took shape during my doctoral journey. These chapters document my\nresearch progression and contributions to the field, with the aim of providing a basis to inspire\nand inform future investigations in this domain. The latter themes are further developed and\nexpanded upon in the final concluding chapter 7.\n1.4.2\nMethodology: Empirical Science Approach to Deep Learning\nIn our research, we have adopted a methodology that combines the experimental approach\nused in the natural sciences with theoretical principles akin to those found in physics Zde-\nborová (2020). This method is well presented and advocated for in Nakkiran and Belkin (2022).\nWe summarize their viewpoint in the following paragraph.\nMachine learning research falls into two main areas. The first, technological research, works\n30\n1.4 Overview of the Thesis\non improving learning systems, while the second, scientific research, aims to understand\nhow learning works. Most current research is technological, focusing on improving practical\nperformance. Scientific research, although essential, often receives less attention and is judged\nusing similar standards as technological research, which may not be ideal. Moreover, while\nrigorous mathematical theories have gained significant attention in the community, they are\njust one part of the picture. The necessity for an empirical approach in machine learning stems\nfrom these shortcomings. Machine learning, especially in its application to deep learning,\noften deals with high levels of complexity that are beyond our current analytical capabilities.\nIn these situations, it becomes essential to build systems and observe them, employing the\nprinciples of empirical inquiry commonly used in natural sciences.\nIn our research, we have aimed to embrace this empirical approach to machine learning,\nfocusing on conducting experiments that reveal key insights into the behavior of deep learn-\ning algorithms, despite the scarcity of rigorous mathematical theorems regarding practical\nsettings. Our goal has been to understand and interpret the patterns that emerge from these\nexperiments, and subsequently develop theoretical frameworks sometimes in the form of\nsimplified toy models, that elucidate these patterns. Toy models, borrowed from the practice\nof physics, are simplified theoretical constructs that, while not capturing all aspects of the\ncomplex systems they represent, isolate and illustrate critical features or behaviors. In the\ncontext of machine learning, such toy models can offer valuable qualitative insights into the\nworkings of more complex deep learning algorithms.\nThis approach echoes the tradition of experimental physics, where empirical data and the-\noretical models, including toy models, engage in a continuous dialogue, each refining and\ninforming the other. Such a feedback loop between empirical studies and theoretical modeling\ncan greatly enhance our understanding of deep learning systems, providing a path to bridge\nthe gap between theory and practice.\nWe conclude by discussing more practical considerations and challenges regarding our method-\nological approach to the study of deep learning systems.\nHyperparameters Tuning\nOur methodology crucially involves the efficient handling of\nhyperparameters in deep learning models due to their significant impact on performance.\nWe employ strategies to limit the number of hyperparameters needing tuning. One example\nof that is choosing the width of 2-layer networks. We set the width it sufficiently large to\nensure results remain stable when further increased, the goal being to operate in the infinite-\nwidth limit. This choice is driven by observations that the infinite-width limit usually delivers\nsuperior performance in all training regimes—as we highlight in Geiger et al. (2021). Other\nexamples involve the choice of loss function and training dynamics. For binary classification,\nwe often use the hinge loss which provides a clear stopping criterion–—zero training loss—that\nallows us to avoid tuning training time. Additionally, to avoid tuning the learning rate, we use\na dynamics that mimics gradient flow, corresponding to the zero learning rate limit of gradient\ndescent.\n31\nIntroduction\nSensitivity to Hyperparameters\nWhile we have aimed to reduce the dependence of our\nresults on hyperparameter tuning by adopting specific experimental settings, we appreciate\nthe importance of understanding the sensitivity of our models to these choices and we aim\nto measure such sensitivity when computational resources allow for it. This is especially\nneeded in cases where the choice of some of the parameters is not completely justified, or\nto check the generality of our findings. For instance, in chapter 3, to thoroughly test the\nrelationship we find between stability to diffeomorphisms and test errors we examine it across\nmultiple benchmark image datasets, across different values of the parameters that control\nthe magnitude and spatial frequencies of the diffeomorphisms, and for varying number of\ntraining points.\nStatistical Significance\nEnsuring statistical significance is also an essential aspect of our\nresearch approach. We maintain full control over all sources of randomness, including the\nsampling of the training set, the initialization of network parameters, and the noise of SGD. To\nensure that the variance of the results is under control, we perform multiple experiments in\nwhich we vary all sources of randomness simultaneously and average the results.\nReproducibility\nReproducibility is vital in scientific research, including empirical deep\nlearning studies. Therefore, all the code used in our experiments is publicly available.4 We\nalso maintain consistency in our experiments by standardizing preprocessing steps of our\ndatasets and computational constraints during model training. This uniformity ensures fair\ncomparison across models and setups, minimizing biases from varied conditions. By doing\nso, we aim to enhance the transparency and reproducibility of our work, offering a stable\nframework within which our results can be replicated.\nLimitations of Empirical Research\nAlthough empirical research can yield relevant insights\ninto deep learning systems, it’s worth noting some inherent limitations. A comprehensive\nand rigorous validation of empirical claims, as seen in theoretical work, is not possible as no\nnumber of experiments can ever prove a scientific theory. Still, a reproducible experiment or\nobservation can refute one, and this falsifiability is what gives validity to scientific theories\nPopper (1968). In this light, our aim is to support our hypotheses by exposing them to as\ndiverse and challenging scenarios as possible in order to test their validity. However, this does\nnot negate the existence of situations, contexts, or datasets where our findings may not apply,\nor the possibility of unaccounted confounding factors. As we aim to conduct responsible and\nrobust research, we want to be transparent about these potential limitations. Moreover, we\nvalue the role of the peer review community in highlighting potential weaknesses and offering\ninsightful critiques, which are essential to empirical research.\n4The code to reproduce the experiments of this thesis is available online at github.com/leonardopetrini and\ngithub.com/pcsl-epfl.\n32\nPart I\nLinear Invariance\n33\n2 Generalization Error Rates of Fully-\nConnected Networks when Learning\nLinear Invariances\nThe following paper is the preprint version of Paccolat et al. (2021a) published in Journal of\nStatistical Mechanics: Theory and Experiment.\nCandidate contributions\nThe candidate contributed to discussions and was responsible for\nthe second part of the paper (Sections 4 and 5).\n35\nGeometric compression of invariant manifolds in neural nets\nJonas Paccolata, Leonardo Petrinia, Mario Geigera, Kevin Tylooa, and Matthieu Wyarta\naInstitute of Physics, ´Ecole Polytechnique F´ed´erale de Lausanne, 1015 Lausanne,\nSwitzerland\nMarch 12, 2021\nAbstract\nWe study how neural networks compress uninformative input space in models where data lie in d\ndimensions, but whose label only vary within a linear manifold of dimension d∥< d. We show that for a\none-hidden layer network initialized with inﬁnitesimal weights (i.e. in the feature learning regime) trained\nwith gradient descent, the ﬁrst layer of weights evolve to become nearly insensitive to the d⊥= d −d∥\nuninformative directions. These are eﬀectively compressed by a factor λ ∼√p, where p is the size of the\ntraining set. We quantify the beneﬁt of such a compression on the test error ϵ. For large initialization\nof the weights (the lazy training regime), no compression occurs and for regular boundaries separating\nlabels we ﬁnd that ϵ ∼p−β, with βLazy = d/(3d −2). Compression improves the learning curves so\nthat βFeature = (2d −1)/(3d −2) if d∥= 1 and βFeature = (d + d⊥/2)/(3d −2) if d∥> 1. We test these\npredictions for a stripe model where boundaries are parallel interfaces (d∥= 1) as well as for a cylindrical\nboundary (d∥= 2). Next we show that compression shapes the Neural Tangent Kernel (NTK) evolution\nin time, so that its top eigenvectors become more informative and display a larger projection on the\nlabels. Consequently, kernel learning with the frozen NTK at the end of training outperforms the initial\nNTK. We conﬁrm these predictions both for a one-hidden layer FC network trained on the stripe model\nand for a 16-layers CNN trained on MNIST, for which we also ﬁnd βFeature > βLazy. The great similarities\nfound in these two cases support that compression is central to the training of MNIST, and puts forward\nkernel-PCA on the evolving NTK as a useful diagnostic of compression in deep nets.\n1\nIntroduction and related works\nDeep neural networks are successful at a variety of tasks, yet understanding why they work remains a\nchallenge. Speciﬁcally, the data from which a rule or classes are learnt often lie in high dimension d where\nthe curse of dimensionality is expected. Quantitatively, this curse can be expressed on how the test error\nϵ(p) depends on the training set size p. If mild assumptions are made on the task (for example regressing\na Lipschitz continuous function), then ϵ(p) cannot be guaranteed to decay faster than ϵ ∝p−β with an\nexponent β = O (1/d) [1]: learning is essentially impossible. In practice, β is found to be much larger and\nto depend on the task, on the dataset and on the learning algorithm [2, 3], implying that learnable data are\nhighly structured.\nAccordingly, success of neural networks is often attributed to their ability to adapt to the structure of\nthe data, which present many invariances [4]. For example in the context of classiﬁcation, some pixels at\nthe edge of the image may be unrelated to the class label. Likewise, smooth deformations of the image may\nleave the class unchanged. In that view, neural networks correspond to a succession of non-linear and linear\noperations where directions of neural representation for which the label does not vary are compressed. It\nis supported by the observations that kernels designed to perform such compression perform well [4]. Yet,\nthere is no quantitative general framework to describe this compression and its eﬀect on the exponent β.\nThe information bottleneck framework for deep learning [5] proposes that information is compressed as it\npropagates deeper in the network. However, information in such a deterministic setting is ill-deﬁned and\nconclusions can depend qualitatively on details of the architecture or on the estimation of information [6].\n1\narXiv:2007.11471v4  [cs.LG]  11 Mar 2021\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n36\nStill, more robust measures, such as the eﬀective dimension of the neural representation of the data, support\nthat compression occurs in deeper layers [7, 8].\nSuch a framework should include in which learning regime nets operate. Diﬀerent regimes have recently\nbeen delineated by focusing on the inﬁnite-width limits of neural networks, shown to converge to well-deﬁned\nlearning algorithms [9, 10, 11, 12]. These are practically useful limits to consider as performance generally\nimproves with width [13, 14, 15, 16, 17], which simply comes from the fact that convergence to these\nasymptotic algorithms removes noise stemming from the random initialization of the weights [18, 19, 20].\nTwo limits are found, depending on how weights scale with width. In one limit [9], deep learning becomes\nequivalent to a kernel method coined Neural Tangent Kernel or NTK. Weights and neuron activities barely\nchange and dimension reduction cannot occur. In the feature learning regime [10, 11], weights and neuron\nactivities signiﬁcantly change, the NTK evolves in time [10, 21] and compression can in principle occur.\nYet understanding this dynamic and its eﬀect on performance remains a challenge. For CNNs the feature\nlearning regime tends to perform better [22, 19, 23] but it is not so for fully connected nets using vanilla\ngradient descent on various benchmarks of images [19]. This state of aﬀairs calls for simple models of data\nin which the kernel evolution and its associated compression of invariants can be quantiﬁed, together with\nits eﬀect on performance.\n1.1\nOur contribution\nHere we consider binary classiﬁcation and assume that the label does not vary along d⊥directions of\ninput space. We will ﬁrst focus on the stripe model, arguably the simplest model of invariant yet non linearly-\nseparable data for which d⊥= d −1, and later show that our results holds for smaller d⊥. Data consists of\nGaussian random points x in d dimensions, whose label is a function of a single coordinate y(x) = y(x1),\ncorresponding to parallel planes separating labels. In Section 3, we show for the stripe model that: (i) in the\nNTK limit, βLazy = d/(3d−2) as we found earlier for isotropic kernels [24]. (ii) In the feature learning regime,\nif the weights are initialized inﬁnitesimally a geometric compression along invariant directions of magnitude\nλ ∼√p occurs at intermediate times. This weight compression is equivalent to a spatial compression of\nthe data points as illustrated in Fig. 1. (iii) In the NTK limit if data are compressed by λ before learning,\nperformance closely matches that of the feature learning regime. This observation supports that the main\ngain of the latter regime is to perform this compression. Assuming that it is the case leads to the prediction\nβFeature = (2d −1)/(3d −2). In Section 4 we generalize this result to the case d⊥< d −1, and argue that\nfor suﬃciently regular boundaries separating labels βFeature = (d + d⊥/2)/(3d −2). We test this prediction\nwhen the boundaries separating labels is a cylinder with d⊥= 1 and d = 3.\nIn Section 5, we argue that the evolution of the NTK is such that at the end of learning: (iv) The top\neigenvectors of the associated Gram matrix become much more informative on the labels than at initialization.\n(v) The projection of the labels on these eigenvectors becomes large for the top eigenvectors and small\notherwise, supporting that the performance of kernel methods using the NTK improves as it evolves during\nlearning. We conﬁrm these predictions empirically in the stripe model. Finally, we show that these points\nhold true in a multi-layer CNN applied to MNIST data, for which various observables are found to behave\nvery similarly to the stripe model, including the fact that βFeature > βLazy. These observations support that\ncompression along invariant directions is indeed key to the success of this architecture, and underlines kernel\nPCA applied to the evolving NTK as a tool to characterize it.\nThe code used for this article is available online at https://github.com/mariogeiger/feature_lazy/\ntree/compressing_invariant_manifolds.\n2\n37\nFigure 1: Illustration of data compression when the labels do not depend of d⊥directions in input space, as\nexempliﬁed in the left panels. During training, the ﬁrst layer weights inﬂate much more in the informative\nd∥directions.\nIn relative terms, the network thus becomes much less sensitive to the d⊥uninformative\ndirections. This eﬀect is equivalent to a compression of uninformative directions in data space, as illustrated\non the right panels.\n1.2\nRelated works\nIn the physics literature, β has been computed in regression or classiﬁcation tasks for ﬁxed kernels [25,\n16, 26, 27, 28]. These results for classiﬁcation generally consider linearly separable data and apply in the\nlimit d →∞and p →∞with α = p/d ﬁxed. In that limit for speciﬁc data it was shown for a regression task\nthat feature learning can outperform the NTK regime [26]. Here we consider classiﬁcation of non-linearly\nseparable data, and take the limit of large training set size p at ﬁxed dimension d which appears appropriate\nfor common benchmarks 1.\nOur work also connects to previous studies on how the anisotropy of the data distribution aﬀects perfor-\nmance [3, 29, 24, 30]. For a large anisotropy, the eﬀective dimension of the data is reduced, improving kernel\nmethods [3]. The eﬀect of a moderate anisotropy was investigated for kernel classiﬁcation [24] and regression\nin neural nets [30]. Here we argue that in the presence of invariant, neural nets in the feature learning regime\nperform a compression equivalent to making the data anisotropic, and to our knowledge produce the ﬁrst\nestimates of the training curves rate β for both the lazy training and feature learning regime in the limit of\nlarge training set size p at ﬁxed dimension d.\nGuarantees of performance for a one-hidden layer in the feature learning regime exist if some norm\n(characterizing the magnitude of the weight representing the function to be learnt) is ﬁnite, and if the\ndynamics penalizes this norm [10, 31]. In our model that norm is inﬁnite (because there is no margin between\nlabels of diﬀerent classes). Instead we focus on vanilla gradient descent without special regularization (such\nregularizations are usually not used in practice). For gradient descent, with the logistic loss for a one-hidden\nlayer can be shown to correspond to a max-margin classiﬁer in a certain non-Hilbertian space of functions\n[32]. Dimension-independent guarantees on performance can be obtained if the data can be separated after\nprojection in a low dimensional space, as occurs in our model.\nThe analysis requires however to go to\nextremely long times. Here instead we focus on the hinge loss for which the dynamic stops after a reasonable\ntime and we estimate the error and β in speciﬁc cases instead of providing an upper bound to it.\nOn the empirical side, the alignment occurring during learning between the function being learnt and the\ntop eigenvectors of the Gram matrix was noticed in [33] and observed more systematically in [34]. Our work\noﬀers an explanation for these ﬁndings in terms of the compression of invariant directions in data space.\n2\nGeneral considerations on data and dynamics\n2.1\nLinear invariant data\nWe consider a binary classiﬁcation task on data points lying in a d-dimensional space whose labels only\ndepend on a linear subspace of dimension d∥< d. Without loss of generality, we write the data points as\n1MNIST or CIFAR present an eﬀective dimension dM ∈[15, 35] [3]and p ≈6 · 104.\n3\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n38\nx = (x∥, x⊥) ∈Rd with x∥= (x1, . . . , xd∥) and x⊥= x −x∥, so that the label function only depends on the\nd∥ﬁrst components: y(x) = y(x∥). In this work, we consider data points drawn from the standard normal\ndistribution. In particular, we refer to the points of a training set of size p as xµ ∼N(0, Id), for µ = 1, . . . , p.\nIn Section 3, we shall focus on the simplest case where d∥= 1, that we call the stripe model. In Section 4,\nwe then generalize our ﬁndings to higher dimensional tasks and we conﬁrm our results on a “cylindrical”\nmodel with d∥= 2.\n2.2\nLearning algorithm\nWe consider the following fully-connected one-hidden layer neural network of ReLU activation,\nf(x) = 1\nh\nh\nX\nn=1\nβn σ\n\u0012ωn · x\n√\nd\n+ bn\n\u0013\n,\n(1)\nwhere σ(x) =\n√\n2 max(0, x). In our simulations h = 10000. The trained parameters of the network are\nβn, ωn and bn. We use a vanilla gradient descent algorithm with the hinge loss on the predictor function\nF(x) = α (f(x) −f0(x)), where f0 is the network function at initialisation and is not aﬀected by gradient\ndescent.\nWith this trick, the amplitude of the network output is controlled by the scale α.\nVarying it\ndrives the network dynamics from the feature regime (small α) to the lazy regime (large α) [22].\nThe\ndynamical evolution of a generic weight W ∈{βn, bn, ωn}n=1,...,h belonging to the network (1) thus follows\nthe diﬀerential equation\n˙W = 1\np\np\nX\nµ=1\n∂W f(xµ) y(xµ\n∥) l′ h\ny(xµ\n∥)F(xµ)\ni\n,\n(2)\nwhere l′(x) = Θ(1−x) is the derivative of the hinge loss. All weights of the network are initialized according to\nthe standard normal distribution. We show in Appendix A that the network output is statistically invariant\nunder a rotation of the informative directions. Without loss of generality, we can thus choose the same\nbasis for the data points as for the ﬁrst layer weights. In particular, we introduce the following notation:\nωn = (ωn,∥, ωn,⊥).\n2.3\nAmpliﬁcation factor\nThe eﬀect of learning is quantiﬁed by the compression of the uninformative weights ωn,⊥with regard to\nthe informative weights ωn,∥. Mathematically, the neuron ampliﬁcation factor λ and the global ampliﬁcation\nfactor Λ are deﬁned as\nλn =\n\f\f\f\n\f\f\fωn,∥\n\f\f\f\n\f\f\f\nd∥\n\f\f\f\fωn,⊥\n\f\f\f\f\nd⊥\nand\nΛ =\nv\nu\nu\nu\nu\nt\nPh\nn=1\n\f\f\f\n\f\f\fωn,∥\n\f\f\f\n\f\f\f\n2\nd∥\nPh\nn=1\n\f\f\f\fωn,⊥\n\f\f\f\f2\nd⊥\n,\n(3)\nwhere d⊥= d −d∥and the d-dimensional norm of a vector v = (v1, . . . , vd) is deﬁned as ||v||2\nd = Pd\ni=1 v2\ni /d,\nin order to remove the dimensional bias from the ratio.\n2.4\nFeature regime vs lazy regime\nThroughout this work, it is assumed that the network width h is suﬃciently large for the algorithm to\noperate in the overparametrized regime [17, 18]. We deﬁne the Neural Tangent Kernel (NTK) Θ(x1, x2) =\n∂W f(x1) · ∂W f(x2), where the scalar product runs over all weights of the network. The gradient descent\nevolution (Eq. (2)) on the functional space then reads\n˙f(x) = 1\np\np\nX\nµ=1\nΘ(x, xµ) y(xµ\n∥) l′ h\ny(xµ\n∥)F(xµ)\ni\n,\n(4)\nwhere the NTK can in principle evolve over time.\n4\n39\nAt initialization, the predictor function is zero. It then grows to ﬁt the training set and doesn’t stop until\nit is at least equal to one on all training points. The smaller the network scale α the more the weights need\nto evolve.\nIf α ≫1, the condition F(x) ∼1 can be fulﬁlled with inﬁnitesimal weight increments δW, so that the\npredictor function is linear in δW. The dynamics thus reduces to a kernel method [9], meaning that the\nNTK is frozen to its initial state Θ0. For an isotropic distribution of the weights, the kernel Θ0 is isotropic\nand thus blind to the existence of many invariants in the data to be learned. This regime is coined the lazy\nregime for ﬁnite h or the NTK regime if h →∞.\nIf α ≪1, the weights of the network need to evolve signiﬁcantly in order to satisfy the condition F(x) ∼1\n[19]. In that case, the NTK adapts to the data and we shall show that it becomes more and more sensitive\nto the informative directions. In particular, the ﬁrst layer weights ωn aligns toward the informative linear\nsubspace, as shown in Fig. 2 for the stripe model and in Fig. 7 for the cylinder model. This regime is coined\nthe feature regime (or sometimes the rich regime) and we study it in the limit h →∞.\nThe transition between the two regimes is illustrated in Appendix B by learning the stripe model with\ndiﬀerent values of α.\n2.5\nLearning timescales\nWe now give a general overview of the network evolution in time. We deﬁne the characteristic time t⋆\nas the time when the predictor function ﬁrst becomes of order one. Also, we introduce the neuron vector\nz = −\n√\ndbω/ ||ω||2, which localizes the closest point of the ReLU hyperplane to the origin. We drop the\nneuron index for simplicity of notation. In the feature regime, we identify three temporal regimes:\n◦Compressing regime: Before t⋆, all neuron vectors z converge toward a ﬁnite number of ﬁxed points\nthat we generically call z⋆[35]. We shall see that the individual weights all diverge exponentially\nwith a time constant τ ⋆∼τ = h\np\nd/2, which depends on the ﬁxed point. As a consequence, at t⋆,\nthe predictor function scales as et⋆/τα ∼1. In the mean ﬁeld limit (α →0), the characteristic time\nt⋆∼τ log(1/α) thus diverges and all neurons eﬀectively reach their ﬁxed point. The logarithmic scaling\nof t⋆is veriﬁed numerically in Appendix B.\nIn the limit of inﬁnite training set size (p →∞), all ﬁxed points are located on the informative subspace,\nnamely z⋆= (z⋆\n∥, 0). We quantify this compression with the ampliﬁcation factor λ = ||ω∥||/||ω⊥|| which\nis divergent in this limit. For ﬁnite p, the compression is saturated by ﬁnite size eﬀects: the data\ndistribution is subject to ﬂuctuations of the order of 1/√p compared to its population expectation.\nThe ﬁxed points are thus located at a distance of the order O (1/√p) perpendicular to the informative\nsubspace. In other words, as we show below the ampliﬁcation factor saturates at λ ∼√p.\n◦Fitting regime: After t⋆, a ﬁnite fraction of the training points satisfy the condition yµF(xµ) > 1.\nBecause we consider the hinge loss these training points no longer contribute to the network evolution.\nIn particular, they drop out of the sum in Eq. (2) [36]. The ﬁrst points to be excluded are the furthest\nfrom the interfaces separating distinct labels. During this process, the ﬁxed points move within the\ninformative manifold such as to better ﬁt the data. Relative ﬂuctuations are still of order O (1/√p), thus\none expects the ampliﬁcation factor to remain of the same order λ ∼√p, as we conﬁrm empirically.\n◦Over-ﬁtting regime: When the number of points still remaining in the sum of Eq. (2) is of the order\nof one, the sum is dominated by ﬂuctuations and the network overﬁts the remaining constraints. We\ncheck numerically that the previous predictions are not signiﬁcantly altered during this ﬁnal regime,\nwhich we don’t study theoretically.\nThe neuron compression mechanism scales up to the whole network so that the global ampliﬁcation factor\nalso saturates with the ﬂuctuations, namely Λ ∼√p. We expect this scaling to be a general property of\nlinear invariant problems. In the next section, we describe this process in more details for the stripe model.\n5\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n40\n3\nStripe model\nWe consider the simplest model of linear invariant data, where the label function only depends on d∥= 1\ninformative direction, namely y(x) = y(x1). Layers of y = +1 and y = −1 regions alternate along the\ndirection e1, separated by parallel planes. In particular, we deﬁne the single-stripe model, where the labels\nare negative if xmin < x1 < xmax and positive otherwise. In our numerical simulations, we use this model\nwith the parameters xmin = −0.3 and xmax = 1.185492.\nβ⃗ω\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nx∥\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nx⊥\nloss = 1.00\nϵt = 0.16\nβ⃗ω\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nx∥\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nx⊥\nloss = 1.00\nϵt = 0.16\nβ⃗ω\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nx∥\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nx⊥\nloss = 0.01\nϵt = 0.00\nFigure 2: Representation of the weights alignment in the single-stripe model. An instance of the labelled\ntraining set is shown in the background. The arrows represent the quantity βω – properly rescaled to ﬁt the\nﬁgure – for a random subset of neurons. Left: At initialization, the weights are distributed isotropically. In\nthe lazy regime, the same distribution persists during the training. Center / Right: During learning, in the\nfeature regime, the weights tend to align with the direction e1. An animation of the current ﬁgure can be\nfound at git.io/JJTS9.\n3.1\nLearning curves\nWe compare the lazy regime and the feature regime, by computing their respective learning curves,\nnamely the test error vs the training set size p. Fig. 3 illustrates how the feature regime outperforms the\nlazy regime, when applied on the single-stripe model.\nIn the lazy regime, the algorithm reduces to a kernel method and one can rely on [24] to predict the\nlearning curve exponent β. In that work, it is shown that for an isotropic kernel of bandwidth larger than the\ndistance between nearest neighbours of the training set, the learning curve of the Support Vector Classiﬁer\n(SVC) algorithm applied to the stripe model in dimension d scales as ϵ ∼p−β, with β = (d−1+ξ)/(3d−3+ξ),\nwhere ξ ∈(0, 2) is an exponent characterizing the kernel cusp at the origin. The NTK is isotropic on data\nlying on the sphere, has a bandwidth of order O(1) and its cusp is similar to the one of a Laplace kernel,\nnamely ξ = 1. Hence, as the SVC algorithm minimizes the hinge loss, the learning curve of the lazy regime\nis expected to have an exponent βLazy = d/(3d −2). This prediction is tested on Fig. 3.\nIn the same work, it is shown that if the uninformative directions of the data are compressed by a factor\nΛ, namely x⊥→x⊥/Λ, the test error is improved by a factor Λ−2(d−1)\n3d−2\nfor ξ = 1. In the next section,\nwe shall argue that, in the feature regime, the perpendicular weights ωn,⊥are suppressed by a factor √p\ncompared to the informative weights ωn,1 as their growth is governed by ﬂuctuations of the data. Such a\nweight compression acts similarly as a data compression with Λ ∼√p as depicted on Fig. 1. Assuming that\nthe main eﬀect of feature learning is this compression, we expect the learning curve exponent of the feature\nregime to be βFeature = (2d−1)/(3d−2). This scaling is again consistent with the numerical results of Fig. 3.\n2The value xmax =\n√\n2 erf−1(1 + erf(xmin)) ≈1.18549 is chosen so that the two labels are equiprobable.\n6\n41\n102\n103\n104\np\n10−3\n10−2\n10−1\nTest Error\nd\n3d−2\n2d−1\n3d−2\nd = 5\nFeature\nLazy: Θ0\nLazy: ΘΛ\n0\nKernel: Θ∞\n102\n103\n104\np\n10−3\n10−2\n10−1\nTest Error\nd\n3d−2\n2d−1\n3d−2\nd = 10\nFeature\nLazy: Θ0\nLazy: ΘΛ\n0\nKernel: Θ∞\nFigure 3: Test error vs the training set size p for the single-stripe model in dimension d = 5 and d = 10.\nTwo datasets are considered: points drawn from the standard normal distribution in dimension d and its\ncompression, where x⊥→x⊥/Λ⋆, where Λ⋆∼√p is the global ampliﬁcation factor at t⋆(see Section 3.2\nfor the deﬁnitions).\nThe labels are deﬁned according to the single-stripe model with xmin = −0.3 and\nxmax = 1.18549. The task is learned following the dynamics of Eq. (2). In the feature regime (solid blue\nlines), the network scale is set to α = 10−6. In the lazy regime, learning is performed with a frozen Gram\nmatrix (α →∞), computed at initialization for both the original (solid red lines) and compressed (dashed red\nlines) datasets. The performance of the frozen Gram matrix at the end of feature training is also computed\n(solid orange lines). All results correspond to the median over 20 realizations of both the data distribution\nand the network initialization. The benchmark triangles represent the expected power laws.\n3.2\nAmpliﬁcation eﬀect\nIn this section we show that when learning the stripe model in the feature regime, the ﬁrst layer weights\nalign along the informative direction e1.\nIn particular, we show that the ratio between the informative\n(or parallel) weights ωn,1 and the uninformative (or perpendicular) weights ωn,⊥scales as Λ ∼√p. This\nsection being more technical can be skipped at ﬁrst reading. For the interested reader, details are given in\nAppendix C.\n3.2.1\nNeuronal dynamics\nWe ﬁrst consider the dynamics of a single generic neuron, whose dynamics is obtained from Eq. (2):\n˙ω = 1\nhp\np\nX\nµ=1\nσ′\n\u0014ω · xµ\n√\nd\n+ b\n\u0015\nl′ [yµF(xµ)] β xµ\n√\nd\nyµ\n˙b = 1\nhp\np\nX\nµ=1\nσ′\n\u0014ω · xµ\n√\nd\n+ b\n\u0015\nl′ [yµF(xµ)] β yµ ,\n(5)\n˙β = 1\nhp\np\nX\nµ=1\nσ\n\u0014ω · xµ\n√\nd\n+ b\n\u0015\nl′ [yµF(xµ)] yµ\nwhere the neuron index n is dropped. Because the ReLU activation is homogeneous, σ(x) = xσ′(x), the\nequality β ˙β −ω · ˙ω −b˙b = 0 holds during the whole evolution. Following the discussion of Section 2.5, we\n7\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n42\nnow solve the above system in the limit α →0, so that t⋆∼τ log(1/α) →∞. In the numerical experiments,\nwe choose the network scale α = 10−6 and deﬁne t⋆as the time when 10% of the training set satisﬁes the\ncondition yµF(xµ) > 1.\nCompressing regime\nAs long as t ≪t⋆, the quantity l′ [yµF(xµ)] = 1, ∀µ, so that the system (5) only\ndepends on the weights associated to the considered neuron. Each neuron thus evolves independently and\nonly diﬀers from the other neurons by its initial conditions.\nWe ﬁrst consider the limit p →∞and neglect the ﬁnite size eﬀects. Applying the central-limit theorem,\nwe carry out the integration over the perpendicular space in Appendix C.1. Deﬁning the neuron ampliﬁcation\nfactor λ = ω1/ω⊥, where ω⊥= ||ω⊥||, the neuronal dynamics (5) becomes\n˙ω1 = β\nτ ⟨y(x1) x1 gλ(x1 −ζ1)⟩x1 + O\n\u0010\np−1/2\u0011\n˙ω⊥= β\nτ\ne−db2/2ω2\n√\n2π\nω⊥\nω\n\u001c\ny\n\u0012ω⊥\nω x1 + ω2\n1\nω2 ζ1\n\u0013\u001d\nx1\n+ O\n\u0010\np−1/2\u0011\n,\n(6)\n˙b =\n√\ndβ\nτ\n⟨y(x1) gλ(x1 −ζ1)⟩x1 + O\n\u0010\np−1/2\u0011\nwhere τ = h\np\nd/2, ω = ||ω|| and ζ1 = −\n√\ndb/ω1 is the intercept of the ReLU hyperplane with the e1 axis,\nwhile gλ(x) = 1\n2\n\u00001 + erf(λx/\n√\n2)\n\u0001\n. The notation ⟨·⟩x1 refers to the expectation over the Gaussian variable\nx1.\nWe recall the deﬁnition of the neuron vector z = −\n√\ndbω/ω2. In [35], the authors show that the ﬁrst layer\nweights of a one-hidden layer network of ReLU activation tend to align along a ﬁnite number of directions\ndepending only on the dataset. Relying on the symmetries of the model, we seek solutions on the informative\naxis. We thus make the hypothesis that the ﬁxed points are of the form z⋆= (z⋆, 0), where z⋆= z⋆\n1 = ζ⋆\n1,\nwhich is equivalent to assuming that the ampliﬁcation factor associated to such ﬁxed points is diverging. In\nthis limit, the system (6) simpliﬁes: the expectation values only depend on the parameter ζ1 and the sign of\nλ. We respectively call them C±\n1 (ζ1), C±\n⊥(ζ1) and C±\nb (ζ1). As a consequence, the dynamics of ζ1,\n˙ζ1\nλ→±∞\n−−−−−→−1\nτ\nβ\nω1\n[d C±\nb (ζ1) + ζ1 C±\n1 (ζ1)],\n(7)\nyields the location of the ﬁxed points as they lie where the above bracket vanishes. For the ﬁxed points\nto be stable along the e1 axis, the second derivative of ζ1 needs to be negative. On a given ﬁxed point z⋆\nthe expectation values C±\n1 (z⋆), C±\n⊥(z⋆) and C±\nb (z⋆) are constant and it is straight-forward to see that ω1,\nb and β all diverge exponentially with a time constant τ ⋆∼τ given in Appendix C.2. Finally, we verify in\nAppendix C.2 that the perpendicular weights do not diverge as fast as ω1 as long as λC±\n1 (z⋆)y(z⋆) < 0 or\n√\n2π|C±\n1 (z⋆)| −e−z⋆2/2 > 0. Under these conditions, the ampliﬁcation factor λ thus diverges exponentially\nin time which justiﬁes our initial hypothesis. We checked numerically that these conditions indeed hold for\nthe considered models. The panel b of Fig. 4 illustrates ˙ζ for the single-stripe model in d = 2.\nWe now consider the ﬁnite p corrections to a given ﬁxed point z⋆and show that the ampliﬁcation factor\nsaturates at λ⋆∼√p. The ﬁnite p eﬀects lead to an additional ﬂuctuation term in each equation of the\nsystem (5). This correction is negligible for the dynamics of ω1, b and β, however for the perpendicular\nweights it yields\n˙ω⊥= β\nτ\n\"\ne−z⋆2/2\n√\n2π\nω⊥\nω C±\n⊥(z⋆) + N(z⋆)\n√p D±\n⊥(z⋆)\n#\n,\n(8)\nwhere D±\n⊥(z⋆) = ⟨Θ(±(x1 −z⋆))⟩x1 and N(z⋆) is a vector of random variables of variance one (see Ap-\npendix C.3). The ﬁrst term in the above bracket is proportional to 1/λ and thus vanishes exponentially with\ntime until it is of the order of the second term, namely O (1/√p). We call τ⊥the time when this crossover\noccurs. After τ⊥, ˙ω⊥is merely proportional to β/√p. Therefore, the perpendicular weights follow the same\nexponential growth as the other weights up to a O (1/√p) prefactor and the ampliﬁcation factor converges to\na ﬁnite value λ⋆that scales as\nλ⋆∼√p.\n(9)\n8\n43\nWe test numerically that all neurons converge to one of the above described ﬁxed points by considering\nthe single-stripe model. The panel c of Fig. 4 illustrates the trajectories of a random selection of neurons\nwhile training the network Eq. (1) until t⋆. Note that some neurons may not have yet reached a ﬁxed point\nfor two reasons. First, because p is ﬁnite, a neuron initial position may lie too far from the training set\ndomain. If no training point lies within the positive side of its associated ReLU hyperplane, it won’t feel any\ngradient and will thus remain static. Second, the simulation is run with a ﬁnite network scale (α = 10−6),\nimplying that the time t⋆∼τ log(1/α) is also ﬁnite. Hence, some neurons may not have reached their\nasymptotic regime at t⋆3.\nb\nc\na\nFigure 4:\nNumerical analysis of the single-stripe model with xmin = −0.3 and xmax = 1.18549 in\ndimension d = 2 with a training set of size p = 10000. The location of the two interfaces is illustrated by\nthe vertical dashed lines. (a) Temporal evolution of the weights of a randomly chosen neuron. The solid\nlines illustrate the considered neuron dynamics in the neural network, while the dashed lines correspond\nto the numerical solutions of the ODE Eq. (6) and Eq. (8) obtained for the same initial conditions. The\nrandom variables N(z⋆) is computed numerically. The curves are truncated at the time t⋆. (b) Function\ndeﬁning the location of the ﬁxed points along ζ1 in the limit λ →∞. The two scenarios λ > 0 and λ < 0\nare shown. The unstable regions, where the limit λ →∞is inconsistent are represented with dashed lines.\n(c) Selection of neuronal trajectories in the z-plane for t < t⋆. The small black dots mark the location of\nthe initial conditions, while the large black dots lie on the predicted location of the three attractors of the\ncompressing regime.\nFitting regime\nAfter t⋆, the loss derivative is zero on a ﬁnite fraction of the training set. As discussed in\nSection 2.5, these training points no longer contribute to the network dynamics. This long time evolution\nis beyond the scope of this work, but could be solved numerically in the limit p →∞following the work of\n[36]. It requires to compute the network function at each step in order to decide which training points still\ncontribute to the dynamics.\nIn this regime, the neurons are still sparsely distributed on the same number of ﬁxed points [35] as in\nthe previous regime. The location of the ﬁxed points is however changing to ﬁt the stripe. This process is\nshown on Fig. 5 for the stripe model in d = 2. Concerning the ampliﬁcation factor, the √p suppression of\n˙ω⊥compared to ˙ω1 remains true until the eﬀective number of training points contributing to the dynamics\nbecomes of order O(1), as shown on the top panel of Fig. 6.\n3Because λ initially grows exponentially, the deﬁnition of the perpendicular timescale yields τ⊥∼τ ⋆log p. For the ampliﬁ-\ncation factor to reach its plateau λ⋆∼√p during the compressing regime, it is essential that τ⊥< t⋆∼τ log(1/α). Hence the\nlarger the training set size, the smaller α needs to be.\n9\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n44\nFigure 5:\nEvolution of the network while ﬁtting the single-stripe model in d = 2 with a training set of\nsize p = 10000. Left: Neural network decision function along the informative direction e1 at three diﬀerent\ntimes. The ζ1 variable of each neuron is represented on the x-axis by colored dots. The darker the region,\nthe larger the point density. On the top plot, the location of the predicted ﬁxed points is marked by black\ncrosses. The location of the two interfaces is illustrated by the vertical dashed lines. Right: Train loss and\ntest error vs time. The three times considered on the left plot are indicated with the same color code. The\ncharacteristic time t⋆is represented by the vertical red line.\n3.2.2\nGlobal ampliﬁcation factor\nIn the previous discussion, we deﬁned an ampliﬁcation factor λ = ω1/ω⊥for each neuron of the network.\nFollowing the deﬁnition Eq. (2.3) we now consider the global ampliﬁcation factor Λ averaged over all neurons,\nnamely\nΛ2 = (d −1)\nPh\nn=1 ω2\nn,1\nPh\nn=1 ω2\nn,⊥\n.\n(10)\nThis deﬁnition compares the largest parallel weights to the largest perpendicular weights. The prefactor\nguarantees that Λ(t = 0) = 1. The top panel of Fig. 6 shows the exponential growth of Λ toward the plateau\nat Λ⋆= Λ(t⋆). The longer time evolution is subject to ﬂuctuations but doesn’t alter signiﬁcantly the picture.\nOn the bottom panel, we conﬁrm the predicted scaling Λ⋆∼√p. We also show that the same scaling applies\nto the maximum of the global ampliﬁcation factor, Λmax = maxt Λ(t), which occurs during the ﬁtting regime.\nIn this section we illustrated with a particular example how the neurons of the network converge to a\nﬁnite set of ﬁxed points. The associated ampliﬁcation factors are shown to diverge with the dataset size:\nλ ∼√p. At the network scale this eﬀect is equivalent to a data compression of the same amplitude. In the\nnext section we extend this discussion to other linear invariant datasets.\n10\n45\nFigure 6:\nTop: Temporal evolution of the global ampliﬁcation factor while learning the single-stripe\nmodel. Two dimensions d = 5 and d = 10 and two training set sizes p = 411 and p = 4640 are illustrated.\nThe curves are averaged over 20 realizations of both the data distribution and the network initialization.\nThe red vertical ticks mark the averaged critical time of the associated setup. Bottom: Global ampliﬁcation\nfactor vs the size of the training set p for the single-stripe model in dimensions d = 5 and d = 10. Both the\nampliﬁcation factor Λ⋆computed at t⋆and the maximal ampliﬁcation factor Λmax are displayed. The curves\ncorrespond to the median over 20 realizations of both the data distribution and the network initialization.\nThe benchmark triangle of slope 1/2 conﬁrms our scaling predictions for Λ⋆and Λmax.\n4\nGeneralization and cylinder model\nCompression mechanism\nThe compression mechanism illustrated in the stripe model is expected to\noccur generically in linear invariant models. If the label function were to depend on d∥directions, all neuron\nvectors z would converge toward ﬁxed points located in the informative subspace of dimension d∥. Similar\nﬁnite p eﬀects as in the stripe model would saturate the resolution of the informative subspace, so that\nthe informative weights ω∥would be larger than the perpendicular weights ω⊥by an ampliﬁcation factor\nλ ∼√p.\nAdvantage of feature regime\nAs the NTK is blind to the existence of invariants in the data, the\nperformance of the lazy regime should not depend on d∥. Indeed following the results of [24], the lazy regime\nlearning curve follows an exponent βLazy = d/(3d−2) for simple boundaries separating labels (such as plane,\nspheres or cylinders), a result conjectured to hold more generally for suﬃciently smooth boundaries. The\ncorrespondence between the lazy training and the SVC considered in [24] is discussed in Section 3.1.\nIn [24], it is also shown that for linear invariant models with d∥> 1, a compression of the perpendicular\nspace by a factor Λ, x⊥→x⊥/Λ, improves the performance of the SVC by a factor Λ−d⊥/(3d−2), for a kernel\nof exponent ξ = 1. As discussed in Section 3.1, because in the feature regime such a compression occurs with\nΛ ∼√p, we expect the learning curve exponent of the feature regime to be βFeature = (d + d⊥/2)/(3d −2).\nCylinder model\nWe test our predictions by considering a cylinder model in d = 3. The data points\nare drawn from the standard normal distribution: x ∼N(0, Id), while the label function is a circle in the\n11\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n46\ninformative subspace of dimension d∥= 2, namely y(x) = y(\n\f\f\f\n\f\f\fx∥\n\f\f\f\n\f\f\f) = +1 if\n\f\f\f\n\f\f\fx∥\n\f\f\f\n\f\f\f > R and negative otherwise.\nFor the numerical simulations we use R = 1.17744. We learn this model following the gradient descent\nalgorithm described in Section 2.2.\nβ⃗ω\n−2\n−1\n0\n1\n2\nx∥1\n−2\n−1\n0\n1\n2\nx∥2\nloss = 1.00\nϵt = 1.00\nβ⃗ω\n−2\n−1\n0\n1\n2\nx∥1\n−2\n−1\n0\n1\n2\nx⊥\nloss = 1.00\nϵt = 1.00\nβ⃗ω\n−2\n−1\n0\n1\n2\nx∥1\n−2\n−1\n0\n1\n2\nx∥2\nloss = 0.01\nϵt = 0.01\nβ⃗ω\n−2\n−1\n0\n1\n2\nx∥1\n−2\n−1\n0\n1\n2\nx⊥\nloss = 0.01\nϵt = 0.01\nFigure 7:\nRepresentation of the ampliﬁcation eﬀect in the cylinder model with d = 3 and d∥= 2. An\ninstance of the labelled training set is shown in the background. The arrows represent the quantity βω –\nproperly rescaled to ﬁt the ﬁgure – for a random subset of neurons. We show the x⊥= 0 section (ﬁrst row)\nand the x∥2 = 0 section (second row) of data-space. The ﬁrst column reports the weights distribution at\ninitialization, the second column at the end of training. An animated version of the current ﬁgure can be\nfound at git.io/JJTS9.\nThe compression of the weight vectors ω into the informative subspace displayed on Fig. 7 supports the\nprevious general discussion. Also, we verify both the scaling of the ampliﬁcation factor and the scaling of\nthe learning curves on Fig. 8. As in the stripe model the time t⋆is numerically deﬁned as the time when the\nequality yµF(xµ) > 1 ﬁrst holds for 10% of the training set. On the top panel, both the global ampliﬁcation\nfactor at t⋆and the maximal global ampliﬁcation factor are shown to scale as Λ⋆∼Λmax ∼√p.\nThe\nadvantage of the feature regime over the lazy regime is displayed on the bottom panel. In particular, the\npredicted learning curve exponents βLazy = d/(3d −2) = 3/7 and βFeature = (d + d⊥/2)/(3d −2) = 1/2 are\nshown to be consistent with the numerical results.\n4This value is √2 log 2 ≃1.1774. It is chosen so that the positive and negative labels are equiprobable.\n12\n47\n102\n103\n104\np\n101\nΛ\n1/2\nt = t∗\nt = tmax\n102\n103\n104\np\n10−2\n10−1\nTest Error\n3/7\n1/2\nFeature\nLazy\nFigure 8:\nTop: Global ampliﬁcation factor vs the size of the training set p for the cylinder model in\ndimension d = 3. Both the ampliﬁcation factor Λ⋆computed at t⋆and the maximal ampliﬁcation factor\nΛmax are displayed. The curves correspond to the median over 49 realizations of both the data distribution\nand the network initialization. The benchmark triangle illustrates the expected power law. Bottom: Test\nerror vs the training set size p for the cylinder model in d = 3. In the feature regime (blue line), the\nnetwork scale is set to α = 10−6. In the lazy regime (red line), learning is performed with the frozen Gram\nmatrix computed at initialization (α →∞). The curves correspond to the median over 9 realizations of\nboth the data distribution and the network initialization. The benchmark triangles illustrate the power law\npredictions.\n5\nSignatures of compression in the temporal evolution of the NTK\nPrevious empirical studies of compression of uninformative directions in data space in neural nets have\nfocused on the neural representations of the data layer by layer [5, 8]. Here instead we study how compression\naﬀects the evolution of the NTK as learning takes place, and show how this kernel becomes better suited for\nthe considered task. We start from the stripe model and extend our analysis to a CNN trained on MNIST,\nand ﬁnd striking similarities between the two cases.\n5.1\nNeural Tangent Kernel Principal Components\nGeneral facts\nThe neural tangent kernel reads Θ(x, z) = ψ(x) · ψ(z) where ψ(x) is a vector of N\ncomponents ψW (x) := ∂W f(x) and W is one of the N parameters of the model. The kernel can be ex-\npressed in terms of its eigenvalues and eigenfunctions (Mercer’s Theorem) Θ(x, z) = P\nλ λ φλ(x)φλ(z). The\nfunctions φλ(·) form an orthogonal basis on the space of functions, and satisfy the integral equation [37]\nR\nΘ(x, z)φλ(z)ρ(z)dz = λφλ(x) where ρ(·) is the distribution of the data. In general, a kernel is expected to\nperform well if the RKHS norm ∥y∥θ of the function y(x) being learnt is small [38]. It writes ∥y∥2\nθ = P\nλ ω2\nλ/λ\nwhere ωλ :=\nR\ny(x)φλ(x)ρ(x)dx. Thus, a kernel performs better if the large coeﬃcients ωλ in the eigenbasis\nof the kernel correspond to large λ. We will argue below that such a trend is enforced when the NTK evolves\nby compressing uninformative directions.\nIn practice, for a ﬁnite training set {xµ}p\nµ=1 of size p, the Gram matrix K is accessible empirically. It\nis deﬁned as the p × p matrix of scalar products Kµν = ψ(xµ) · ψ(xν). Diagonalizing it corresponds to\nperforming Kernel PCA [38], which identiﬁes the principal components in the feature representation ψ(xµ)\nof the data: Kµν = P\n˜λ ˜λ ˜φλ(xµ)˜φλ(xν). One has ˜λ →λ and ˜φλ(xν) →φλ(xν) as p →∞for a ﬁxed λ. Thus\n13\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n48\nthe coeﬃcients ωλ can be estimated as ˜ωλ := 1\np\nP\nµ=1...p ˜φλ(xµ)y(xµ). In the following sections, we drop the\ntilde for ease of notation.\nEﬀect of compression on the evolution of the NTK\nAt initialization, for fully connected nets the NTK\nis isotropic, and its eigenvectors are spherical harmonics [9]. For a ﬁxed dimension d∥of the informative space,\nas the overall dimension d grows, the value of a given spherical harmonics leads to vanishing information\non the speciﬁc components x∥. As a consequence, we expect that even for large λ, φλ(x) contains little\ninformation on the label y(x). It follows that the magnitude of the projected signal ωλ is small in that limit.\nBy contrast, after learning in the limit Λ ∼√p →∞, the output function looses its dependence on the\northogonal space x⊥. The NTK can then generically be rewritten as:\nΘ(x, z) = Θ1(x∥, z∥) + Θ2(x∥, z∥)x⊥· z⊥\n(11)\nwhere the second term comes from the derivative with respect to the ﬁrst layer of weights (see Appendix D).\nFor a Gaussian data density ρ considered in this paper, eigenvectors with non-vanishing eigenvalues are then\nof two kind: φ1\nλ(x∥) – the eigenvectors of Θ1 – and φ2\nλ(x∥)u · x⊥where φ2\nλ(x∥) is an eigenvector of Θ2 and\nu any non-zero vectors. The null-space of the kernel then corresponds to all functions of the orthogonal\nspace that are orthogonal to constant or linear functions. However for a ﬁnite Λ, we expect the associated\neigenvalues to be small but diﬀerent from zero.\nTwo qualitative predictions follow:\n◦The eigenvectors φ1\nλ only depend on x∥and are thus generically more informative on the label y(x∥)\nthan spherical harmonics. It is also true, but to a lesser extent, for the eigenvectors φ2\nλ(x∥)u · x⊥.\nIndeed for Gaussian data, they can be considered as a function of x∥times a random Gaussian noise.\nOverall, we thus expect that for large eigenvalues the mutual information between φλ(x) and y(x) to\nincrease during learning.\n◦As a consequence, the magnitude of ωλ associated to the top eigenvalues also tends to increase. We\nthus expect that the performance of kernel learning using the NTK at the end of training to be superior\nto that using the NTK at initialization.\n14\n49\n102\n103\np\n10−1\nTest Error\n-1/2\n-1/3\nNetwork\nFull Kernel\nLast Layer Kernel\nAfter training\nAt initialization\nFigure 9: Performance of CNN trained to classify the parity of MNIST digits (binary class problem) as a\nfunction of the trainset size. The network is trained in the feature learning regime (blue line) using vanilla\ngradient descent with momentum, leading to βFeature ≈0.5. Before and after training the full kernel (i.e. with\nrespect to all the parameters) and the kernel of the last layer are computed. These four frozen kernels are\nthen used in a gradient descent algorithm using an independent trainset of the same size. All the measures\nare done 5 times with diﬀerent initialization seeds and averaged. For the full kernel at initialization (dashed\norange) we ﬁnd βLazy ≈0.3 and consequently βLazy < βFeature.\n5.2\nEmpirical tests\nPerformance of kernel methods based on the NTK\nIn Fig.3 we test our prediction that kernel\nmethods based on the NTK obtained at the end of training outperforms the NTK at initialization. We\nperform kernel learning using diﬀerent data for the training set than those used to generate the NTK. We\nﬁnd that it is indeed the case: in fact, performance is found to be very similar to that of the neural net in the\nfeature learning regime, except for the largest training set size where it even outperforms it. Note that this\nsimilarity is natural, since the features associated to the NTK contain the the last hidden layer of neurons,\nwhich can represent the network output with the last layer of weights.\nWe test the generality of this result in Fig.9 using a more modern CNN architecture on the MNIST\ndata set. This architecture is inspired from MnasNet [39] with 16 convolutional layers. It distinguishes from\nMnasNet by the absence of batch-normalization. We again ﬁnd that kernel methods based on the NTK at\ninﬁnite time perform as well as the network in the feature learning regime, and even once again slightly\nbetter for the largest p.\nFinally, it is interesting to compare this analysis with the kernel whose features correspond to the last\nlayer of hidden neurons at the end of training. Training such a kernel simply corresponds to retraining the\nlast layer of weights while ﬁxing the activity of the last hidden neurons. Interestingly, this kernel performs\nwell but generally less so than the network itself, as illustrated in Fig.9.\nKernel PCA v.s. labels (Information and projection)\nWe now conﬁrm that such improved perfor-\nmance of the NTK corresponds to the top kernel principal components becoming more informative on the\ntask. As we argued in Section 5.1, we expected this to be the case, in the presence of compression. Specif-\nically, we consider the r largest eigenvalues λmax, . . . λr of the NTK Gram Matrix and their corresponding\n15\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n50\neigenvectors. We ﬁrst compute the mutual information between a given eigenvector magnitude and the label\nI(φλr; y) – for details on the estimator see Appendix E. This mutual information is small and essentially\nindependent of r in the range studied for the NTK at initialization; both for the stripe model (Fig.10.a) and\nMNIST (Fig.10.c). However, at the end of learning, mutual information has greatly improved in both cases,\na fact that holds true for the NTK and for the kernel obtained from the last layer of hidden neurons.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n2\n4\nω2\nλ\n×10−4\nStripe\nb\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.0\n0.5\n1.0\nI(φλr, y)\nStripe\na\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nr\n0.0\n0.5\n1.0\nω2\nλ\n×10−4\nMNIST\nd\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nr\n0.0\n0.5\n1.0\nI(φλr, y)\nMNIST\nc\nInitial Kernel PCA\nFinal Kernel PCA\nLast Layer Post-activations PCA\nFigure 10:\nMutual Information I(φλr; y) between each of the ﬁrst ten NTK eigenvectors and the output\nlabel for the stripe model (a) and MNIST (c), respectively. The eigenvectors projection on the output\nlabels ω2\nλ = ⟨φλr |y⟩2\np2\nis shown in panels (b) – stripe model – and (d) – MNIST. We show in blue the\nresults for the NTK at initialization, in orange for the NTK after training in the feature regime and in green\nfor the principal components of last layer post-activations.\nAs expected, the magnitude of the projection of each of the ﬁrst r eigenvectors onto the output labels\nω2\nλ = ⟨φλr|y⟩2/p2 also greatly improves during learning. This eﬀect is striking both for the stripe model\n(Fig.10.b) and for MNIST (Fig.10.d). At initialization, that projection does not show a signiﬁcant trend\nwith rank within the ﬁrst 10 eigenvectors. Yet after learning, most of the projection occurs along the ﬁrst\nmode of the NTK alone, with the second mode also showing a sizable projection for MNIST.\nOverall, the similarities of these plots between MNIST and the stripe model support that compression\nis indeed a key eﬀect characterizing learning for MNIST as well. To study further these similarities, we\nfocus on the ﬁrst two eigenvectors and plot data points (diﬀerent labels appear as diﬀerent colors) in the\n(φλ1(x), φλ2(x)) plane as shown in Fig. 11. As expected, these eigenvectors at initialization have essentially\nno information on the output label – the scatter plot looks like Gaussian noise both for the stripe model and\nMNIST (left column). By contrast, after learning data of diﬀerent classes appear as well separated clouds of\npoints in that plane (central column). Strikingly, performing the same analysis for the kernel obtained from\nthe last layer of hidden neurons shows that data organize into a smaller manifold, which is approximately one-\ndimensional (right column). It is expected in the stripe model, since for Λ →∞the hidden neurons activity\ncan only depend on a single variable x1. It is interesting that a similar dimension-reduction appears so clearly\nin MNIST as well, suggesting the importance of a nearly-one dimensional manifold in the representation of\nthe last hidden layer. We have checked that such a one-dimensional structure is not apparent in the eﬀective\ndimension of this representation 5.\n5Computing the eﬀective dimension (based on the scaling of the distance between points in terms of the number of points\n[40]) of that representation leads to deﬀ≈6, possibly coming from the ﬁnite width of the nearly one-dimensional manifold\napparent in Fig. 11, bottom right.\n16\n51\n−0.1\n0.0\n0.1\n−0.1\n0.0\n0.1\nφλ2\nStripe\nInitialization Kernel\n0.00\n0.05\n−0.1\n0.0\n0.1\nStripe\nFinal Kernel\n−0.1\n0.0\n−0.05\n0.00\n0.05\n0.10 Stripe\nLast Layer Post-activations\n0.0\n0.1\nφλ1\n−0.05\n0.00\n0.05\n0.10\nφλ2\nMNIST\n0.00\n0.05\nφλ1\n−0.05\n0.00\n0.05\nMNIST\n0.000\n0.025\nφλ1\n−0.025\n0.000\n0.025\n0.050 MNIST\nFigure 11: Scatter plot of the ﬁrst two NTK eigenvectors – φλ1 and φλ2 – for the stripe model (ﬁrst row)\nand MNIST (second row). Colors map class labels. Eigenvectors are computed for the NTK at initialization\n(ﬁrst column) and after training (second column). The last column refers to the last layer post-activation\nprincipal components. These results are consistent with Fig. 10: (1) Before learning, the eigenvectors are not\ncorrelated to the labels, while they are after learning. (2) For the stripe model, only the ﬁrst eigenvector of\nthe ﬁnal kernel contains information on the labels, as expected from panel b) of Fig. 10. (3) Two informative\neigenvectors are necessary to linearly separate the stripe data as illustrated on the top-right panel as well\nas on panel b) of Fig. 10. The associated unidimensional representation is expected from the eﬀective data\ncompression for the stripe model where d∥= 1. (4) For MNIST, the ﬁrst two eigenvectors of the ﬁnal kernel\nare not suﬃcient to classify completely the data as expected from panel d) of Fig. 10, but still suggest\na compression along the uninformative directions. This last point is also motivated by the approximate\nunidimensional collapse observed in the bottom-right panel.\n6\nConclusion\nWe have shown that in the presence of d⊥uninformative dimensions of the input, the weights of a one-\nhidden layer neural network become orthogonal to them. For a vanishingly small initialization of the weights\nand vanilla gradient descent, this eﬀect is limited by the sample noise of the training set, and its magnitude\nis of order Λ ∼√p. For simple geometries of the boundaries separating labels, this eﬀect increases the\nexponent β characterizing learning curves with respect to the lazy training regime (in which the neuron\norientation is nearly frozen). This increase depends on both d⊥and d. Both for the stripe and cylindrical\nmodel, the observed exponents β are consistent with this prediction, supporting that for these models at\nleast the main advantage of the feature learning regime is to compress invariant directions.\nNext we have argued that such a compression shapes the evolution of the neural tangent kernel during\nlearning, so that its principal components become more informative and display a larger projection on the\nlabel, eﬀectively reducing the RKHS norm of the function being learnt. As a consequence, using gradient\ndescent with the frozen NTK at the end of training leads to much better performance than at initialization,\nand we observe that it even outperforms the neural net in the feature learning regime. The analysis underlines\n17\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n52\nthat kernel PCA on the NTK is a valuable tool to characterize the compression of invariants. Overall we ﬁnd\nstriking similarities between a one-hidden layer FC network trained on the stripe model and a deep CNN\ntrained on MNIST, supporting that compression is central to the performance of the latter as well.\nOne challenge for the future is to classify which conditions on the data can guarantee such an improvement\nof the NTK during learning – a question directly connected to the relative performance of lazy training v.s.\nfeature learning, which appears to depend on the architecture for real data [19].\nA second challenge is the development of quantitative models for the compression of other symmetries in\nthe data, including the invariance of the label toward smooth deformations that characterize images. Is this\ncompression ultimately responsible for the success of deep learning in beating the curse of dimensionality\n? Answering this question presumably requires to focus on more modern architectures, in particular deep\nCNNs.\nAcknowledgments\nWe acknowledge G. Biroli, M. Gabrie, D. Kopitkov, S. Spigler, Y. Rouzaire and all members of the\nPCSL group for discussions. This work was partially supported by the grant from the Simons Foundation\n(#454953 Matthieu Wyart). M.W. thanks the Swiss National Science Foundation for support under Grant\nNo. 200021-165509.\n18\n53\nReferences\n[1]\nUlrike von Luxburg and Olivier Bousquet. “Distance-based classiﬁcation with Lipschitz functions”. In:\nJournal of Machine Learning Research 5.Jun (2004), pp. 669–695.\n[2]\nJoel Hestness et al. “Deep Learning Scaling is Predictable, Empirically”. In: CoRR abs/1712.00409\n(2017).\n[3]\nStefano Spigler, Mario Geiger, and Matthieu Wyart. “Asymptotic learning curves of kernel methods:\nempirical data vs Teacher-Student paradigm”. In: arXiv preprint arXiv:1905.10843 (2019).\n[4]\nSt´ephane Mallat. “Understanding deep convolutional networks”. In: Philosophical Transactions of the\nRoyal Society A: Mathematical, Physical and Engineering Sciences 374.2065 (Apr. 2016), p. 20150203.\nissn: 1471-2962. doi: 10.1098/rsta.2015.0203. url: http://dx.doi.org/10.1098/rsta.2015.\n0203.\n[5]\nRavid Shwartz-Ziv and Naftali Tishby. “Opening the Black Box of Deep Neural Networks via Infor-\nmation”. In: arXiv preprint arXiv:1703.00810 (2017).\n[6]\nAndrew M Saxe et al. “On the information bottleneck theory of deep learning”. In: Journal of Statistical\nMechanics: Theory and Experiment 2019.12 (2019), p. 124020.\n[7]\nAlessio Ansuini et al. “Intrinsic dimension of data representations in deep neural networks”. In: Ad-\nvances in Neural Information Processing Systems. 2019, pp. 6111–6122.\n[8]\nStefano Recanatesi et al. “Dimensionality compression and expansion in Deep Neural Networks”. In:\narXiv preprint arXiv:1906.00443 (2019).\n[9]\nArthur Jacot, Franck Gabriel, and Clement Hongler. “Neural Tangent Kernel: Convergence and Gen-\neralization in Neural Networks”. In: Advances in Neural Information Processing Systems 31. 2018,\npp. 8580–8589.\n[10]\nGrant M Rotskoﬀand Eric Vanden-Eijnden. “Neural networks as Interacting Particle Systems: Asymp-\ntotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error”. In: arXiv\npreprint arXiv:1805.00915 (2018).\n[11]\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. “A mean ﬁeld view of the landscape of two-layer\nneural networks”. In: Proceedings of the National Academy of Sciences 115.33 (2018), E7665–E7671.\nissn: 0027-8424. doi: 10.1073/pnas.1806579115.\n[12]\nSimon S. Du et al. “Gradient Descent Provably Optimizes Over-parameterized Neural Networks”. In:\nInternational Conference on Learning Representations. 2019. url: https://openreview.net/forum?\nid=S1eK3i09YQ.\n[13]\nBehnam Neyshabur et al. “Geometry of optimization and implicit regularization in deep learning”. In:\narXiv preprint arXiv:1705.03071 (2017).\n[14]\nBehnam Neyshabur et al. “Towards Understanding the Role of Over-Parametrization in Generalization\nof Neural Networks”. In: arXiv preprint arXiv:1805.12076 (2018).\n[15]\nYamini Bansal et al. “Minnorm training: an algorithm for training over-parameterized deep neural\nnetworks.” In: CoRR (2018).\n[16]\nMadhu S Advani and Andrew M Saxe. “High-dimensional dynamics of generalization error in neural\nnetworks”. In: arXiv preprint arXiv:1710.03667 (2017).\n[17]\nStefano Spigler et al. “A jamming transition from under-to over-parametrization aﬀects generalization\nin deep learning”. In: Journal of Physics A: Mathematical and Theoretical 52.47 (2019), p. 474001.\n[18]\nMario Geiger et al. “Scaling description of generalization with number of parameters in deep learning”.\nIn: Journal of Statistical Mechanics: Theory and Experiment 2020.2 (2020), p. 023401.\n[19]\nMario Geiger et al. Disentangling feature and lazy training in deep neural networks. 2019. arXiv:\n1906.08034 [cs.LG].\n[20]\nArthur Jacot et al. “Implicit regularization of random feature models”. In: arXiv preprint arXiv:2002.08404\n(2020).\n19\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n54\n[21]\nXialiang Dou and Tengyuan Liang. “Training neural networks as learning data-adaptive kernels: Prov-\nable representation and approximation beneﬁts”. In: Journal of the American Statistical Association\n(2020), pp. 1–14.\n[22]\nLenaic Chizat, Edouard Oyallon, and Francis Bach. “On Lazy Training in Diﬀerentiable Programming”.\nIn: NeurIPS 2019 - 33rd Conference on Neural Information Processing Systems. Vancouver, Canada,\nDec. 2019. url: https://hal.inria.fr/hal-01945578.\n[23]\nBlake Woodworth et al. “Kernel and rich regimes in overparametrized models”. In: arXiv preprint\narXiv:2002.09277 (2020).\n[24]\nJonas Paccolat, Stefano Spigler, and Matthieu Wyart. “How isotropic kernels perform on simple invari-\nants”. en. In: Machine Learning: Science and Technology 2.2 (Mar. 2021). Publisher: IOP Publishing,\np. 025020. issn: 2632-2153. doi: 10.1088/2632-2153/abd485. url: https://doi.org/10.1088/\n2632-2153/abd485 (visited on 03/11/2021).\n[25]\nElizabeth Gardner. “The space of interactions in neural network models”. In: Journal of physics A:\nMathematical and general 21.1 (1988), p. 257.\n[26]\nAndrea Montanari et al. “The generalization error of max-margin linear classiﬁers: High-dimensional\nasymptotics in the overparametrized regime”. In: arXiv preprint arXiv:1911.01544 (2019).\n[27]\nSt´ephane d’Ascoli et al. “Double Trouble in Double Descent: Bias and Variance (s) in the Lazy Regime”.\nIn: arXiv preprint arXiv:2003.01054 (2020).\n[28]\nRainer Dietrich, Manfred Opper, and Haim Sompolinsky. “Statistical mechanics of support vector\nnetworks”. In: Physical review letters 82.14 (1999), p. 2975.\n[29]\nSebastian Goldt et al. “Modelling the inﬂuence of data structure on learning in neural networks”. In:\narXiv preprint arXiv:1909.11500 (2019).\n[30]\nBehrooz Ghorbani et al. “When Do Neural Networks Outperform Kernel Methods?” In: arXiv preprint\narXiv:2006.13409 (2020).\n[31]\nGreg Ongie et al. “A Function Space View of Bounded Norm Inﬁnite Width ReLU Nets: The Multi-\nvariate Case”. In: arXiv preprint arXiv:1910.01635 (2019).\n[32]\nLenaic Chizat and Francis Bach. “Implicit bias of gradient descent for wide two-layer neural networks\ntrained with the logistic loss”. In: arXiv preprint arXiv:2002.04486 (2020).\n[33]\nSamet Oymak et al. “Generalization guarantees for neural networks via harnessing the low-rank struc-\nture of the jacobian”. In: arXiv preprint arXiv:1906.05392 (2019).\n[34]\nDmitry Kopitkov and Vadim Indelman. “Neural Spectrum Alignment”. In: arXiv preprint arXiv:1910.08720\n(2019).\n[35]\nHartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient Descent Quantizes ReLU Network\nFeatures. 2018. arXiv: 1803.08367 [stat.ML].\n[36]\nFranco Pellegrini and Giulio Biroli. An analytic theory of shallow networks dynamics for hinge loss\nclassiﬁcation. 2020. arXiv: 2006.11209 [stat.ML].\n[37]\nCarl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.\nInglese. Cambridge, Mass: Mit Pr, Nov. 2005. isbn: 978-0-262-18253-9.\n[38]\nBernhard Scholkopf, Alexander Smola, and Klaus-Robert M¨uller. “Kernel principal component analy-\nsis”. In: Advances in Kernel Methods - Support Vector Learning. MIT Press, 1999, pp. 327–352.\n[39]\nMingxing Tan et al. “MnasNet: Platform-Aware Neural Architecture Search for Mobile”. In: 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019). doi: 10.\n1109/cvpr.2019.00293. url: http://dx.doi.org/10.1109/CVPR.2019.00293.\n[40]\nElena Facco et al. “Estimating the intrinsic dimension of datasets by a minimal neighborhood infor-\nmation”. In: Scientiﬁc Reports 7.1 (Sept. 2017). issn: 2045-2322. doi: 10.1038/s41598-017-11873-y.\nurl: http://dx.doi.org/10.1038/s41598-017-11873-y.\n20\n55\nA\nRotation invariance\nIn this appendix we prove that if we rotate the input of the network it doesn’t aﬀect its performance.\nLemma: For a group G and a G-invariant function f, the gradient of f is G-equivariant:\n∇f(D(g)x) = D(g)−T ∇f(x)\n∀g ∈G and ∀x,\nwhere D is the representation of G acting on the space of inputs x and A−T denotes the inverse transpose\nof the matrix A.\nProof The derivative of f in the direction u evaluated in D(g)x is given by\nu · ∇f(D(g)x) = lim\nh→0\nf(D(g)x + hu) −f(D(g)x)\nh\n(12)\n= lim\nh→0\nf(x + hD(g)−1u) −f(x)\nh\n(13)\n= (D(g)−1u) · ∇f(x) = u · (D(g)−T ∇f(x)).\n(14)\nSince this formula holds for any direction u, it proves the lemma.\nIn the context of a neural network, if the loss function of a neural network satisﬁes L(Dw(g)w, Dx(g)x) =\nL(w, x) with Dw orthogonal, it is easy to see that the lemma applied to the loss reads ∇wL(Dw(g)w, Dx(g)x) =\nDw(g)∇wL(w, x).\nHere w refers to the weights of the network, whose dynamics is given by\n˙w(t) =\n−P\nµ ∇wL(w, xµ), where µ is the training set index.\nIf we act with G on w and on the training set,\nthe derivative ˙w is transformed in the same way as w. A network initialised to Dw(g)w0 instead of w0\nand trained on {Dx(g)xµ}µ instead of {xµ}µ during a time t will thus have its weights equal to Dw(g)w(t)\ninstead of w(t).\nIn particular, this discussion holds for a network starting with a fully-connected layer: in this case G is\nthe orthogonal group, Dx is the orthogonal matrix and Dw is acting on the ﬁrst weights with an orthogonal\nmatrix and leaves the rest of the weights invariant.\nIn case of an initialisation distribution of the weights that satisﬁes ρ(Dw(g)w) = ρ(w), the expected\nperformance (averaged over the initialisations) will be independent of the global orientation of the inputs.\nB\nα scan in the stripe model\nWe illustrate the transition from the feature regime to the lazy regime by considering the single-stripe\nmodel in dimension d = 10 with a training set of size p = 1000. We vary the network scale from α = 10−16 to\nα = 108 (see Fig. 12). In the limit α →∞, the test error converges to the one obtained by running the kernel\ndynamics with the NTK frozen at initialization, the characteristic time scales as t⋆∼1/α as expected from\n[19] and the global ampliﬁcation factor equals one. In the opposite limit, α →0, the test error converges to a\nplateau better than the lazy regime performance, the characteristic time grows logarithmically as discussed\nin Section 2.5 and the global ampliﬁcation factor reaches a plateau.\nC\nStripe model dynamics\nIn this section, we give additional details to the computation carried in Section 3.2.1. We consider the\nlarge p limit of the system (5), where it is well approximated by the central-limit theorem. For t ≪t⋆, the\ndynamics of each neuron is governed by the system\n˙ω1 = β\nτ\n\u0010\nµ1 + σ1\n√pN1\n\u0011\n˙ω⊥= β\nτ\n\u0010\nµ⊥+ σ⊥\n√pN ⊥\n\u0011\n˙b = β\nτ\n\u0010\nµb + σb\n√pNb\n\u0011\n21\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n56\nFigure 12: On each plot, the dots are obtained by averaging the gradient descent results over 5 diﬀerent data\nrealizations and network initialization. Left: Test error vs the network scale α. The horizontal dashed line\ncorrespond to the test error of the frozen initial kernel dynamics, also averaged over 5 realizations. Center:\nCharacteristic time t⋆vs the network scale α. Right: Global ampliﬁcation factor vs the network scale α.\nBoth the ampliﬁcation factor at t⋆and the maximal ampliﬁcation factor are represented.\nup to O(p−1) corrections. The last layer weight is obtained from the constant of motion β2−||ω||2−b2 = const.\nWe compute the averages µ1, µ⊥and µb in Appendix C.1 and discuss the asymptotic solution in the limit\np →∞in Appendix C.2. The ﬁnite p corrections and the associated standard deviations σ1, σ⊥and σb are\nconsidered in Appendix C.3.\nC.1\nComputation of the averages\nWe compute the averages µ1, µ⊥and µb for data distributed according to the standard normal distribu-\ntion: ρ(x) = ρ(||x||) = (2π)−d/2 exp\n\u0010\n−||x||2 /2\n\u0011\n. For the bias and the informative weights, we get\nµ1 =\nZ\ndxρ(x)x1y(x1)Θ\n\u0014ω · x\n√\nd\n+ b\n\u0015\n=\nZ\ndx1ρ(x1)x1y(x1)1\n2\n\"\n1 + erf\n \nb\n√\nd + ω1x1\n√\n2ω⊥\n!#\nµb =\n√\nd\nZ\ndxρ(x)y(x1)Θ\n\u0014ω · x\n√\nd\n+ b\n\u0015\n=\n√\nd\nZ\ndx1ρ(x1)y(x1)1\n2\n\"\n1 + erf\n \nb\n√\nd + ω1x1\n√\n2ω⊥\n!#\n.\nFor the perpendicular weights, we treat each components independently, so that for i > 1:\nµi =\nZ\ndxρ(x)xiy(x1)Θ\n\u0014ω · x\n√\nd\n+ b\n\u0015\n= sgn(ωi)\n√\n2π\nZ\ndx1ρ(x1)y(x1)\nZ\ndx⊥ρ(x⊥)e\n−\n(b\n√\nd+ω1x1+˜\nω⊥x⊥)2\n2ω2\ni\n=\n1\n√\n2π\nωi\nω e−db2\n2ω2\nZ\ndx1ρ(x1)y\n \nω⊥\nω x1 −\n√\ndbω1\nω2\n!\nwhere we used the notation ˜ω⊥=\np\nω2\n⊥−ω2\ni . Using the deﬁnition of λ and ζ1, one thus recovers the system\n(6).\n22\n57\nC.2\nInﬁnite p\nExpectation values in the limit |λ| →∞\nIn the limit λ →±∞, the function gλ becomes a Heaviside\nfunction whose direction depends on the sign of λ: gλ(x)\nλ→±∞\n−−−−−→Θ(±x). Consequently, the remaining\nintegrals over the x1 distribution in Appendix C.1 simpliﬁes:\n⟨y(x1) x1 gλ(x1 −ζ1)⟩x1\nλ→±∞\n−−−−−→⟨y(x1) x1 Θ (±(x1 −ζ1))⟩x1 = C±\n1 (ζ1)\n⟨y(x1) gλ(x1 −ζ1)⟩x1\nλ→±∞\n−−−−−→⟨y(x1) Θ (±(x1 −ζ1))⟩x1 = C±\nb (ζ1)\n\u001c\ny\n\u0012ω⊥\nω x1 + ω2\n1\nω2 ζ1\n\u0013\u001d\nx1\nλ→±∞\n−−−−−→y(ζ1) = C⊥(ζ1)\nAsymptotic solutions\nWe assume that the neuron vector z is set constant and equal to z⋆= (z⋆, 0). The\ndynamics of ω1, β and b thus no longer depend on the perpendicular weights. In the asymptotic regime, the\nsign of ω1β is given by the sign of the constant C±\n1 (z⋆). In particular, using the constant of motion and the\ndeﬁnition b = −ω1z⋆/\n√\nd, we get β = sign\n\u0000C±\n1 (z⋆)\n\u0001 p\n1 + z⋆2/d ω1, where we neglected the order one value\nof the constant of motion. Finally, one ﬁnds that the informative weights diverge as\nω1 ∼et/τ ⋆, with τ ⋆=\nτ\n|C±\n1 (z⋆)|\np\n1 + z⋆2/d\n.\n(15)\nInserting the above relations into the perpendicular weights dynamics yields\n˙ω⊥= sign\n\u0002\nC±\n1 (z⋆)ω1y(z⋆)\n\u0003 p\n1 + z⋆2/d\n2πτ\ne−z⋆2\n2 ω⊥.\n(16)\nHence, if sign\n\u0002\nC±\n1 (z⋆)ω1y(z⋆)\n\u0003\n= −1, the perpendicular weights all vanish exponentially.\nHowever, if\nsign\n\u0002\nC±\n1 (z⋆)ω1y(z⋆)\n\u0003\n= +1, they all diverge exponentially with a time constant\nτ ⋆\n⊥=\n2πτ\np\n1 + z⋆2/d\ne\nz⋆2\n2\nwhich still leads to a diverging ampliﬁcation factor if τ ⋆\n⊥> τ ⋆.\nC.3\nFinite p\nWe assess the ﬁnite p corrections of the asymptotic solutions given in Appendix C.2. Since the bias\nand the informative weights are divergent, they are not sensitive to ﬁnite p corrections. However, for the\nperpendicular weights, it is essential to compute the standard deviations. Since the expectations have been\ncomputed previously, it is suﬃcient to look at the second non-central moments. For simplicity, we directly\nconsider the limit |λ| →∞, so that for i > 1:\nµ2\ni + σ2\ni =\nZ\ndxρ(x)x2\ni Θ\n\u0014ω · x\n√\nd\n+ b\n\u0015\nλ→±∞\n−−−−−→\nZ\ndx1ρ(x1)Θ [±(x1 −z⋆)] = D±\n⊥(ζ1).\nFor each perpendicular direction, a random variable of variance one quantiﬁes the discrepancy between the\naverage µi and the exact sum over the dataset. Its value depends on the location of the ReLU hyperplane. In\nparticular, once the considered neuron has reached its ﬁxed point z⋆, all random variables can be arranged\ninto the constant perpendicular vector N ⊥(z⋆).\nD\nNTK decomposition and eigenfunctions\nIn section 5.1 we argued that, for the setting considered in this paper, the NTK can be decomposed as\nΘ(x, z) = Θ1(x∥, z∥) + Θ2(x∥, z∥)x⊥· z⊥.\n(17)\nIn this appendix, we look at this decomposition more in details and derive the eigenfunctions functional\nform.\n23\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n58\nNTK decomposition\nRecall the architecture considered in this paper,\nf(x) = 1\nh\nh\nX\nn=1\nβn σ\n\u0012ωn · x\n√\nd\n+ bn\n\u0013\n.\nFor this architecture, the NTK reads\nΘ(x, z) = 1\nh2\nh\nX\nn=1\n\"\nσ\n\u0012ωn · x\n√\nd\n+ bn\n\u0013\nσ\n\u0012ωn · z\n√\nd\n+ bn\n\u0013\n+ β2\nnσ′\n\u0012ωn · x\n√\nd\n+ bn\n\u0013\nσ′\n\u0012ωn · z\n√\nd\n+ bn\n\u0013 \u0010\n1 + x · z\nd\n\u0011 #\n.\nIf the input space has only d∥informative directions, after feature learning (Λ →∞), the output function\nwill only depend on x∥. This is because ωn · x →ωn,∥· x∥and the NTK can be rewritten as\nΘ(x, z) = 1\nh2\nh\nX\nn=1\n\"\nσ\n\u0012ωn,∥· x∥\n√\nd\n+ bn\n\u0013\nσ\n\u0012ωn,∥· z∥\n√\nd\n+ bn\n\u0013\n+ β2\nnσ′\n\u0012ωn,∥· x∥\n√\nd\n+ bn\n\u0013\nσ′\n\u0012ωn,∥· z∥\n√\nd\n+ bn\n\u0013 \u0012\n1 +\nx∥· z∥\nd\n+ x⊥· z⊥\nd\n\u0013 #\n,\nwhere one can readily identify Θ1(x∥, z∥) and Θ2(x∥, z∥).\nNTK eigenfunctions\nEigenfunctions satisfy the integral equation\nZ\nΘ(x, z)φλ(z)ρ(z)dz = λφλ(x),\nwhere ρ(·) is the distribution of the data. We assume here that ρ(x) = ρ∥(x∥)ρ⊥(x⊥) = ρ∥(x∥)Πiρ⊥(x⊥,i)\nwith zero mean and the same variance in all directions. If we plug in the decomposition (17), we notice that\neigenvectors are of two kinds, they are either eigenvectors of Θ1(x∥, z∥) or of Θ2(x∥, z∥)x⊥· z⊥– i.e. they\ngive zero when the other operator acts on them. The ones coming from Θ1 are solutions of\nZ\nΘ1(x∥, z∥)φ1\nλ(z)ρ(z)dz = λφ1\nλ(x).\nGiven that the l.h.s. only depends on x∥, we have φ1\nλ(x) = φ1\nλ(x∥). Integrating out z⊥we get\nZ\nΘ1(x∥, z∥)φ1\nλ(z∥)ρ(z∥)dz∥= λφ1\nλ(x∥).\nThe second kind of eigenvectors satisfy\nZ\nΘ2(x∥, z∥)x⊥· z⊥φ2\nλ(z)ρ(z)dz = λφ2\nλ(x).\nNotice that x⊥can be moved out of the integral. Consequently, eigenfunctions can only linearly depend on\nthe perpendicular component – i.e. φ2\nλ(x) = φ2\nλ(x∥) u · x⊥. The integral equation reads\n\u0012Z\nΘ2(x∥, z∥)φ2\nλ(z∥)ρ(z∥)dz∥\n\u0013\n(u · x⊥)\nZ\nz2\n⊥ρ(z⊥)dz⊥= λφ2\nλ(x∥) u · x⊥,\nwhere u can be any non-zero vector. To back what we stated previously – i.e. that eigenvectors are either of\nthe two kinds – we show that no other eigenvector, diﬀerent from linear combinations of φ1\nλ and φ2\nλ, exists.\nAssume there exists φ∗\nλ(x) ̸= aφ1\nλ1(x) + bφ2\nλ2(x), this would solve\nZ h\nΘ1(x∥, z∥) + Θ2(x∥, z∥)x⊥· z⊥\ni\nφ∗\nλ(z)ρ(z)dz = λφ∗\nλ(x)\nZ\nΘ1(x∥, z∥)φ∗\nλ(z)ρ(z)dz +\nZ\nΘ2(x∥, z∥)x⊥· z⊥φ∗\nλ(z)ρ(z)dz = λφ∗\nλ(x)\nλ1φ1\nλ1(x) + λ2φ2\nλ2(x) = λφ∗\nλ(x),\nresulting in a contradiction.\n24\n59\nE\nMutual Information Estimator\nWe propose a mutual information estimator bI(x; y) that exploits the information we know about the\nbinary labels distribution P(y):\nP(y = +) = P(y = −) = 1\n2.\nThe variable x is continuous and can live in high dimension. We deﬁne\nq+ = P(x|y = +)\n2P(x)\n= P(y = +|x),\nq−= P(x|y = −)\n2P(x)\n= P(y = −|x).\nWe recall the deﬁnition of diﬀerential entropy for continuous variables,\nH(x) = −\nZ\nddxP(x) log P(x)\nGiven that the mutual information can be expressed I(x; y) = H(x)−H(x|y), we compute the conditional\nentropy knowing P(y) as6\nH(x|y) = −1\n2\nZ\nddx P(x|y = +) log P(x|y = +) −1\n2\nZ\nddx P(x|y = −) log P(x|y = −)\n= −\nZ\nddx P(x)q+ log(2P(x)q+) −\nZ\nddx P(x)q−log(2P(x)q−)\n= −\nZ\nddx P(x)(q+ + q−) log(2P(x)) −\nZ\nddx P(x)\n\u0002\nq+ log(q+) + q−log(q−)\n\u0003\n= H(x) −1 −Ex\n\u0002\nq+ log(q+) + q−log(q−)\n\u0003\n.\nFinally, the mutual information is given by\nI(x; y) = H(x) −H(x|y)\n= 1 + Ex\n\u0002\nq+ log(q+) + q−log(q−)\n\u0003\n.\nWe ﬁnd the following estimator\nbI(x; y) = 1 + 1\np\np\nX\ni=1\nc\nq+(xi) log c\nq+(xi) + c\nq−(xi) log c\nq−(xi)\n= 1 −1\np\np\nX\ni=1\nh2(c\nq+(xi)),\nwhere h2(·) is the binary entropy function7.\nWe notice that we can rewrite\nP(y|x) =\nP(x|y)\n2 P\ny P(x|y)P(y)\n=\nP(x|y)\nP(x|y = +) + P(x|y = −),\nhence the MI estimation reduces to estimating P(x|y).\nAt this stage we propose the following approximation: suppose that P(x|y) is uniform in the ball con-\ntaining the k nearest neighbors of x which are labelled y, i.e.8\nP(x|y) ∼r−d\ny (xi).\n6All the logarithms of this section are computed in base 2.\n7h2(x) = −x log x −(1 −x) log(1 −x) .\n8The estimation depends on the value of k which is omitted to simplify the notation. For the estimations in this paper we\nuse k = 5.\n25\nChapter 2. Generalization Error Rates of Fully-Connected Networks when Learning Linear\nInvariances\n60\nThe estimation of c\nq+ ﬁnally reduces to\nc\nq+(xi) =\nr−d\n+ (xi)\nr−d\n+ (xi) + r−d\n−(xi)\n=\n1\n1 +\n\u0010\nr+(xi)\nr−(xi)\n\u0011d\n(18)\nc\nq−(xi) = 1 −c\nq+(xi).\nWe tested the estimator on diﬀerent datasets and identiﬁed two main ﬂaws:\n◦For large d, the estimator gets aﬀected by the curse of dimensionality, distances between data-points\nbecome all similar to each other. As a result, the c\nq+ estimator gets biased towards 1/2.\n◦If x lives on a manifold of dimension lower than the one of the embedding space, the use of d in Eq. (18)\n– instead of the eﬀective local dimension around xi – biases the estimator towards its extrema.\nConsidering we employ the estimator only in d = 1, we skip the discussion on the possible ways to correct\nthese ﬂaws.\n26\n61\nPart II\nInvariance toward Smooth\nDeformations\n63\n3 Deformation Invariance Strongly Cor-\nrelates to Performance in Image Tasks\nThe following paper is the preprint version of Petrini et al. (2021) published in Advances in\nNeural Information Processing Systems.\nCandidate contributions\nThe candidate contributed to all discussions. The candidate\nperformed the experiments, in particular, discovered the correlation between test error and\nrelative stability. The candidate drafted the first version of the paper.\n65\nRelative stability toward diffeomorphisms\nindicates performance in deep nets\nLeonardo Petrini,\nAlessandro Favero,\nMario Geiger,\nMatthieu Wyart\nInstitute of Physics\nÉcole Polytechnique Fédérale de Lausanne\n1015 Lausanne, Switzerland\n{name.surname}@epfl.ch\nAbstract\nUnderstanding why deep nets can classify data in large dimensions remains a chal-\nlenge. It has been proposed that they do so by becoming stable to diffeomorphisms,\nyet existing empirical measurements support that it is often not the case. We revisit\nthis question by deﬁning a maximum-entropy distribution on diffeomorphisms, that\nallows to study typical diffeomorphisms of a given norm. We conﬁrm that stability\ntoward diffeomorphisms does not strongly correlate to performance on benchmark\ndata sets of images. By contrast, we ﬁnd that the stability toward diffeomorphisms\nrelative to that of generic transformations Rf correlates remarkably with the test\nerror ϵt. It is of order unity at initialization but decreases by several decades during\ntraining for state-of-the-art architectures. For CIFAR10 and 15 known architectures\nwe ﬁnd ϵt ≈0.2\np\nRf, suggesting that obtaining a small Rf is important to achieve\ngood performance. We study how Rf depends on the size of the training set and\ncompare it to a simple model of invariant learning.\n1\nIntroduction\nDeep learning algorithms LeCun et al. (2015) are now remarkably successful at a wide range of\ntasks Amodei et al. (2016); Huval et al. (2015); Mnih et al. (2013); Shi et al. (2016); Silver et al.\n(2017). Yet, understanding how they can classify data in large dimensions remains a challenge. In\nparticular, the curse of dimensionality associated with the geometry of space in large dimension\nprohibits learning in a generic setting Luxburg and Bousquet (2004). If high-dimensional data can be\nlearnt, then they must be highly structured.\nA popular idea is that during training, hidden layers of neurons learn a representation Le (2013) that\nis insensitive to aspects of the data unrelated to the task, effectively reducing the input dimension and\nmaking the problem tractable Ansuini et al. (2019); Recanatesi et al. (2019); Shwartz-Ziv and Tishby\n(2017). Several quantities have been introduced to study this effect empirically. It includes (i) the\nmutual information between the hidden and visible layers of neurons Saxe et al. (2019); Shwartz-Ziv\nand Tishby (2017), (ii) the intrinsic dimension of the neural representation of the data Ansuini et al.\n(2019); Recanatesi et al. (2019) and (iii) the projection of the label of the data on the main features of\nthe network Kopitkov and Indelman (2020); Oymak et al. (2019); Paccolat et al. (2021a), the latter\nbeing deﬁned from the top eigenvectors of the Gram matrix of the neural tangent kernel (NTK) Jacot\net al. (2018). All these measures support that the neuronal representation of the data indeed becomes\nwell-suited to the task. Yet, they are agnostic to the nature of what varies in the data that need not\nbeing represented by hidden neurons, and thus do not specify what it is.\nRecently, there has been a considerable effort to understand the beneﬁts of learning features for one-\nhidden-layer fully connected nets. Learning features can occur and improve performance when the\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2105.02468v3  [cs.LG]  4 Nov 2021\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n66\ntrue function is highly anisotropic, in the sense that it depends only on a linear subspace of the input\nspace Bach (2017); Chizat and Bach (2020); Ghorbani et al. (2019, 2020); Paccolat et al. (2021a);\nReﬁnetti et al. (2021); Yehudai and Shamir (2019). For image classiﬁcation, such an anisotropy would\noccur for example if pixels on the edge of the image are unrelated to the task. Yet, fully-connected\nnets (unlike CNNs) acting on images tend to perform best in training regimes where features are not\nlearnt Geiger et al. (2021, 2020); Lee et al. (2020), suggesting that such a linear invariance in the data\nis not central to the success of deep nets.\nInstead, it has been proposed that images can be classiﬁed in high dimensions because classes\nare invariant to smooth deformations or diffeomorphisms of small magnitude Bruna and Mallat\n(2013); Mallat (2016). Speciﬁcally, Mallat and Bruna could handcraft convolution networks, the\nscattering transforms, that perform well and are stable to smooth transformations, in the sense that\n∥f(x) −f(τx)∥is small if the norm of the diffeomorphism τ is small too. They hypothesized that\nduring training deep nets learn to become stable and thus less sensitive to these deformations, thus\nimproving performance. More recent works generalize this approach to more common CNNs and\ndiscuss stability at initialization Bietti and Mairal (2019a,b). Interestingly, enforcing such a stability\ncan improve performance Kayhan and Gemert (2020).\nAnswering if deep nets become more stable to smooth deformations when trained and quantifying\nhow it affects performance remains a challenge. Recent empirical results revealed that small shifts of\nimages can change the output a lot Azulay and Weiss (2018); Dieleman et al. (2016); Zhang (2019),\nin apparent contradiction with that hypothesis. Yet in these works, image transformations (i) led\nto images whose statistics were very different from that of the training set or (ii) were cropping\nthe image, thus are not diffeophormisms. In Ruderman et al. (2018), a class of diffeomorphisms\n(low-pass ﬁlter in spatial frequencies) was introduced to show that stability toward them can improve\nduring training, especially in architectures where pooling layers are absent. Yet, these studies do\nnot address how stability affects performance, and how it depends on the size of the training set. To\nquantify these properties and to ﬁnd robust empirical behaviors across architectures, we will argue\nthat the evolution of stability toward smooth deformations needs to be compared relatively to that of\nany deformation, which turns out to vary signiﬁcantly during training.\nNote that in the context of adversarial robustness, attacks that are geometric transformations of small\nnorm that change the label have been studied Alaifari et al. (2018); Alcorn et al. (2019); Athalye et al.\n(2018); Engstrom et al. (2019); Fawzi and Frossard (2015); Kanbak et al. (2018); Xiao et al. (2018).\nThese works differ for the literature above and from out study below in the sense that they consider\nworst-case perturbations instead of typical ones.\n1.1\nOur Contributions\n◦We introduce a maximum entropy distribution of diffeomorphisms, that allow us to generate\ntypical diffeomorphisms of controlled norm. Their amplitude is governed by a \"temperature\"\nparameter T.\n◦We deﬁne the relative stability to diffeomorphisms index Rf that characterizes the square\nmagnitude of the variation of the output function f with respect to the input when it is\ntransformed along a diffeomorphism, relatively to that of a random transformation of the\nsame amplitude. It is averaged on the test set as well as on the ensemble of diffeomorphisms\nconsidered.\n◦We ﬁnd that at initialization, Rf is close to unity for various data sets and architectures,\nindicating that initially the output is as sensitive to smooth deformations as it is to random\nperturbations of the image.\n◦Our central result is that after training, Rf correlates very strongly with the test error ϵt:\nduring training, Rf is reduced by several decades in current State Of The Art (SOTA) archi-\ntectures on four benchmark datasets including MNIST Lecun et al. (1998), FashionMNIST\nXiao et al. (2017), CIFAR-10 Krizhevsky (2009) and ImageNet Deng et al. (2009). For more\nprimitive architectures (whose test error is higher) such as fully connected nets or simple\nCNNs, Rf remains of order unity. For CIFAR10 we study 15 known architectures and ﬁnd\nempirically that ϵt ≈0.2\np\nRf.\n◦Rf decreases with the size of the training set P. We compare it to an inverse power 1/P\nexpected in simple models of invariant learning Paccolat et al. (2021a).\n2\n67\nThe library implementing diffeomorphisms on images is available online at github.com/pcsl-\nepﬂ/diffeomorphism.\nThe code for training neural nets can be found at github.com/leonardopetrini/diffeo-sota and the\ncorresponding pre-trained models at doi.org/10.5281/zenodo.5589870.\n2\nMaximum-entropy model of diffeomorphisms\n2.1\nDeﬁnition of maximum entropy model\nWe consider the case where the input vector x is an image. It can be thought as a function x(s)\ndescribing intensity in position s = (u, v) ∈[0, 1]2, where u and v are the horizontal and vertical\ncoordinates. To simplify notations we consider a single channel, in which case x(s) is a scalar\n(but our analysis holds for colored images as well). We denote by τx the image deformed by τ,\ni.e. [τx](s) = x(s −τ(s)). τ(s) is a vector ﬁeld of components (τu(s), τv(s)). The deformation\namplitude is measured by the norm\n∥∇τ∥2 =\nZ\n[0,1]2((∇τu)2 + (∇τv)2)dudv.\n(1)\nTo test the stability of deep nets toward diffeomorphisms, we seek to build typical diffeomorphisms\nof controlled norm ∥∇τ∥. We thus consider the distribution over diffeomorphisms that maximizes\nthe entropy with a norm constraint. It can be solved by introducing a Lagrange multiplier T and by\ndecomposing these ﬁelds on their Fourier components, see e.g. Kardar (2007) or Appendix A. In this\ncanonical ensemble, one ﬁnds that τu and τv are independent with identical statistics. For the picture\nframe not to be deformed, we impose ﬁxed boundary conditions: τ = 0 if u = 0, 1 or v = 0, 1. One\nthen obtains:\nτu =\nX\ni,j∈N+\nCij sin(iπu) sin(jπv)\n(2)\nwhere the Cij are Gaussian variables of zero mean and variance ⟨C2\nij⟩= T/(i2 + j2). If the picture\nis made of n × n pixels, the result is identical except that the sum runs on 0 < i, j ≤n. For large n,\nthe norm then reads ∥∇τ∥2 = (π2/2) n2T, and is dominated by high spatial frequency modes. It\nis useful to add another parameter c to cut-off the effect of high spatial frequencies, which can be\nsimply done by constraining the sum in Eq.2 to i2 + j2 ≤c2, one then has ∥∇τ∥2 = (π3/8) c2T.\nOnce τ is generated, pixels are displaced to random positions. A new pixelated image can then be\nobtained using standard interpolation methods. We use two interpolations, Gaussian and bi-linear1,\nas described in Appendix C. As we shall see below, this choice does not affect our result as long as\nthe diffeomorphism induced a displacement of order of the pixel size, or larger. Examples are shown\nin Fig.1 as a function of T and c.\n2.2\nPhase diagram of acceptable diffeomorphisms\nDiffeomorphisms are bijective, which is not the case for our transformations if T is too large. When\nthis condition breaks down, a single domain of the picture can break into several pieces, as apparent\nin Fig.1. It can be expressed as a condition on ∇τ that must be satisﬁed in every point in space\nLowe (2004), as recalled in Appendix B. This is satisﬁed locally with high probability if ∥τ∥2 ≪1,\ncorresponding to T ≪(8/π3)/c2. In Appendix, we extract empirically a curve of similar form in the\n(T, c) plane at which a diffeomorphism is obtained with probability at least 1/2 . For much smaller T,\ndiffeomorphisms are obtained almost surely.\nFinally, for diffeomorphisms to have noticeable consequences, their associated displacement must\nbe of the order of magnitude of the pixel size. Deﬁning δ2 as the average square norm of the pixel\ndisplacement at the center of the image in the unit of pixel size, it is straightforward to obtain from\nEq.2 that asymptotically for large c (cf. Appendix B for the derivation),\nδ2 = π\n4 n2T ln(c).\n(3)\nThe line δ = 1/2 is indicated in Fig.1, using empirical measurements that add pre-asymptotic terms\nto Eq.3. Overall, the green region corresponds to transformations that (i) are diffeomorphisms with\nhigh probability and (ii) produce signiﬁcant displacements at least of the order of the pixel size.\n1Throughout the paper, if not speciﬁed otherwise, bi-linear interpolation is employed.\n3\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n68\n100\n101\n102\ncut-oﬀc\n10−6\n10−5\n10−4\n10−3\n10−2\n10−1\nT\nAverage pixel displacement δ = 1\nT = 1e −05, c = 3\nT = 7e −06, c = 10\nT = 6e −06, c = 30\nrelevant diﬀeo region\nT = 4e −04, c = 3\nT = 1e −05, c = 3\nT = 1e −02, c = 3\nT = 8e −06, c = 15\nT = 4e −04, c = 15\nT = 6e −06, c = 100\nT = 6e −04, c = 100\nFigure 1: Samples of max-\nentropy diffeomorphisms\nfor different temperatures\nT and high-frequency cut-\noffs c for an ImageNet data-\npoint of resolution 320 ×\n320. The green region cor-\nresponds to\nwell behav-\ning diffeomorphisms (see\nSection 2.2). The dashed\nline corresponds to δ = 1.\nThe colored points on the\nline are those we focus our\nstudy in Section 3.\n3\nMeasuring the relative stability to diffeomorphisms\nRelative stability to diffeomorphisms\nTo quantify how a deep net f learns to become less sen-\nsitive to diffeomorphisms than to generic data transformations, we deﬁne the relative stability to\ndiffeomorphisms Rf as:\nRf =\n⟨∥f(τx) −f(x)∥2⟩x,τ\n⟨∥f(x + η) −f(x)∥2⟩x,η\n.\n(4)\nwhere the notation ⟨⟩y can indicate alternatively the mean or the median with respect to the distribution\nof y. In the numerator, this operation is made over the test set and over the ensemble of diffeomor-\nphisms of parameters (T, c) (on which Rf implicitly depends). In the denominator, the average is on\nthe test set and on the vectors η sampled uniformly on the sphere of radius ∥η∥= ⟨∥τx −x∥⟩x,τ. An\nillustration of what Rf captures is shown in Fig.2. In the main text, we consider median quantities,\nas they reﬂect better the typical values of distribution. In Appendix E.3 we show that our results for\nmean quantities, for which our conclusions also apply.\nDependence of Rf on the diffeomorphism magnitude\nIdeally, Rf could be deﬁned for inﬁnites-\nimal transformations, as it would then characterize the magnitude of the gradient of f along smooth\ndeformations of the images, normalized by the magnitude of the gradient in random directions. How-\never, inﬁnitesimal diffeomorphisms move the image much less than the pixel size, and their deﬁnition\nthus depends signiﬁcantly on the interpolation method used. It is illustrated in the left panels of Fig.3,\nshowing the dependence of Rf in terms of the diffeomorphism magnitude (here characterised by the\nmean displacement magnitude at the center of the image δ) for several interpolation methods. We\ndo see that Rf becomes independent of the interpolation when δ becomes of order unity. In what\nfollows we thus focus on Rf(δ = 1), which we denote Rf.\nSOTA architectures become relatively stable to diffeomorphisms during training, but are not\nat initialization\nThe central panels of Fig.3 show Rf at initialization (shaded), and after training\n(full) for two SOTA architectures on four benchmark data sets. The ﬁrst key result is that, at initial-\nization, these architectures are as sensitive to diffeomorphisms as they are to random transformations.\nRelative stability to diffeomorphisms at initialization (guaranteed theoretically in some cases Bietti\nand Mairal (2019a,b)) thus does not appear to be indicative of successful architectures.\n4\n69\ndata-space\nr\nx+η\nx\nx\nτx\nFigure 2: Illustrative drawing of the data-space Rn×n around a data-point x (black point). We\nfocus here on perturbations of ﬁxed magnitude – i.e. on the sphere of radius r centered in x. The\nintersection between the images of x transformed via typical diffeomorphisms and the sphere is\nrepresented in dashed green. By contrast, the red point is an example of random transformation. For\nlarge n, it is equivalent to adding an i.i.d. Gaussian noise to all the pixel values of x. Figures on the\nright illustrate these transformations, the color of the dot labelling them corresponds to that of the\nleft illustration. The relative stability to diffeomorphisms Rf characterizes how a net f varies in the\ngreen directions, normalized by random ones.\nBy contrast, for these SOTA architectures, relative stability toward diffeomorphisms builds up during\ntraining on all the data sets probed. It is a signiﬁcant effect, with values of Rf after training generally\nfound in the range Rf ∈[10−2, 10−1].\nStandard data augmentation techniques (translations, crops, and horizontal ﬂips) are employed for\ntraining. However, the results we ﬁnd only mildly depend on using such techniques, see Fig.12 in\nAppendix.\nLearning relative stability to diffeos requires large training sets\nHow many data are needed to\nlearn relative stability toward diffeomorphisms? To answer this question, newly initialized networks\nare trained on different training-sets of size P. Rf is then measured for CIFAR10, as indicated in\nthe right panels of Fig.3. Neural nets need a certain number of training points (P ∼103) in order to\nbecome relatively stable toward smooth deformations. Past that point, Rf monotonically decreases\nwith P. In a range of P, this decrease is approximately compatible with the an inverse behavior\nRf ∼1/P found in the simple model of Section 6. Additional results for MNIST and FashionMNIST\ncan be found in Fig.13, Appendix E.3.\nSimple architectures do not become relatively stable to diffeomorphisms\nTo test the universal-\nity of these results, we focus on two simple architectures: (i) a 4-hidden-layer fully connected (FC)\nnetwork (FullConn-L4) where each hidden layer has 64 neurons and (ii) LeNet LeCun et al. (1989)\nthat consists of two convolutional layers followed by local max-pooling and three fully-connected\nlayers.\nMeasurements of Rf for these networks are shown in Fig.4. For the FC net, Rf ≈1 at initialization\n(as observed for SOTA nets) but grows after training on the full data set, showing that FC nets do not\nlearn to become relatively stable to smooth deformations. It is consistent with the modest evolution of\nRf(P) with P, suggesting that huge training sets would be required to obtain Rf < 1. The situation\nis similar for the primitive CNN LeNet, which only becomes slightly insensitive (Rf ≈0.6) in a\nsingle data set (CIFAR10), and otherwise remains larger than unity.\nLayers’ relative stability monotonically increases with depth\nUp to this point, we measured the\nrelative stability of the output function for any given architecture. We now study how relative stability\nbuilds up as the input data propagate through the hidden layers. In Fig.14 of Appendix E.3, we report\nRf as a function of depth for both simple and deep nets. What we observe is Rf0 ≈1 independently\n2With the only exception of the ImageNet results (central panel) in which only one trained network is\nconsidered.\n5\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n70\nreduced\nsensitivity\nto diffeo\nFigure 3: Relative stability to diffeomorphisms Rf for SOTA architectures. Left panels: Rf vs.\ndiffeomorphism displacement magnitude δ at initialization (dashed lines) and after training (full lines)\non the full data set of CIFAR10 (P = 50k) for several cut-off parameters c and two interpolations\nmethods, as indicated in legend. ResNet is shown on the top and EfﬁcientNet on the bottom. Central\npanels: Rf(δ = 1) for four different data-sets (x−axis) and two different architectures at initialization\n(shaded histograms) and after training (full histograms). The values of c (in different colors) are\n(3, 5, 15) and (3, 10, 30) for the ﬁrst three data-sets and ImageNet, respectively. ResNet18 and\nEfﬁcientNetB0 are employed for MNIST, F-MNIST and CIFAR10, ResNet101 and EfﬁcientNetB2\nfor ImageNet. Right panels: Rf(δ = 1) vs. training set size P at c = 3 for ResNet18 (top) and\nEfﬁcientNetB0 (bottom) trained on CIFAR10. The value of Rf0 at initialization is indicated with\ndashed lines. The triangles indicate the predicted slope Rf ∼P −1 in a simple model of invariant\nlearning, see Section 6. Statistics: Each point in the graphs2 is obtained by training 16 differently\ninitialized networks on 16 different subsets of the data-sets; each network is then probed with 500\ntest samples in order to measure stability to diffeomorphisms and Gaussian noise. The resulting Rf\nis obtained by log-averaging the results from single realizations.\nFigure 4: Relative stability to diffeomorphisms Rf in primitive architectures. Top panels: Rf at\ninitialization (shaded) or for trained nets (full) for a fully connected net (left) or a primitive CNN\n(right) at P = 50k. Bottom panels: Rf(P) for c = 3 and different data sets as indicated in legend.\nStatistics: see caption in the previous ﬁgure.\nof depth at initialization, and monotonically decreases with depth after training. Overall, the gain\nin relative stability appears to be well-spread through the net, as is also found for stability alone\nRuderman et al. (2018).\n6\n71\n4\nRelative stability to diffeomorphisms indicates performance\nThus, SOTA architectures appear to become relatively stable to diffeomorphisms after training,\nunlike primitive architectures. This observation suggests that high performance requires such a\nrelative stability to build up. To test further this hypothesis, we select a set of architectures that\nhave been relevant in the state of the art progress over the past decade; we systematically train them\nin order to compare Rf to their test error ϵt. Apart from fully connected nets, we consider the\nalready cited LeNet (5 layers and ≈60k parameters); then AlexNet Krizhevsky et al. (2012) and\nVGG Simonyan and Zisserman (2015), deeper (8-19 layers) and highly over-parametrized (10-20M\n(million) params.) versions of the latter. We introduce batch-normalization in VGGs and skip\nconnections with ResNets. Finally, we go to EfﬁcientNets, that have all the advancements introduced\nin previous models and achieve SOTA performance with a relatively small number of parameters\n(<10M); this is accomplished by designing an efﬁcient small network and properly scaling it up.\nFurther details about these architectures can be found in Table 1, Appendix E.2.\nThe results are shown in Fig.5. The correlation between Rf and ϵt is remarkably high (corr. coeff.3 :\n0.97), suggesting that generating low relative sensitivity to diffeomorphisms Rf is important to obtain\ngood performance. In Appendix E.3 we also report how changing the train set size P affects the\nposition of a network in the (ϵt, Rf) plane, for the four architectures considered in the previous\nsection (Fig.18). We also show that our results are robust to changes of δ, c (Fig.21) and data sets\n(Fig.20).\nWhat architectures enable a low Rf value? The latter can be obtained with skip connections or not,\nand for quite different depths as indicated in Fig.5. Also, the same architecture (EfﬁcientNetB0)\ntrained by transfer learning from ImageNet – instead of directly on CIFAR10 – shows a large\nimprovement both in performance and in diffeomorphisms invariance. Clearly, Rf is much better\npredicted by ϵt than by the speciﬁc features of the architecture indicated in Fig.5.\nnets performance vs relative diffeo stability\nCIFAR10\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nVGG11bn\nLeNet\nEfficientNetB0\nVGG19bn\nVGG16bn\nResNet18\nResNet34\nResNet50\nEfficientNetB0\nEfficientNetB2\nnumber of layers\n2          50\nbatch-norm\nskip connections\ntransfer learning from ImageNet\n10%\n3%\n30%\nFigure 5: Test error ϵt vs. relative stability to diffeomorphisms Rf computed at δ = 1 and c = 3\nfor common architectures when trained on the full 10-classes CIFAR10 dataset (P = 50k) with SGD\nand the cross-entropy loss; the EfﬁcientNets achieving the best performance are trained by transfer\nlearning from ImageNet (⋆) – more details on the training procedures can be found in Appendix E.1.\nThe color scale indicates depth, and the symbols the presence of batch-norm (⋄) and skip connections\n(†). Dashed grey line: power low ﬁt ϵt ≈0.2\np\nRf. Rf strongly correlates to ϵt, much less so to\ndepth or the presence of skip connections. Statistics: Each point is obtained by training 5 differently\ninitialized networks; each network is then probed with 500 test samples in order to measure Rf. The\nresults are obtained by log-averaging over single realizations. Error bars – omitted here – are shown\nin Fig.19, Appendix E.3.\n7\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n72\n5\nStability toward diffeomorphisms vs. noise\nThe relative stability to diffeomorphisms Rf can be written as Rf = Df/Gf where Gf characterizes\nthe stability with respect to additive noise and Df the stability toward diffeomorphisms:\nGf = ⟨∥f(x + η) −f(x)∥2⟩x,η\n⟨∥f(x) −f(z)∥2⟩x,z\n,\nDf = ⟨∥f(τx) −f(x)∥2⟩x,τ\n⟨∥f(x) −f(z)∥2⟩x,z\n.\n(5)\nHere, we chose to normalize these stabilities with the variation of f over the test set (to which both x\nand z belong), and η is a random noise whose magnitude is prescribed as above. Stability toward\nadditive noise has been studied previously in fully connected architectures Novak et al. (2018) and\nfor CNNs as a function of spatial frequency in Tsuzuku and Sato (2019); Yin et al. (2019).\nThe decrease of Rf with growing training set size P could thus be due to an increase in the stability\ntoward diffeomorphisms (i.e. Df decreasing with P) or a decrease of stability toward noise (Gf\nincreasing with P). To test these possibilities, we show in Fig.6 Gf(P), Df(P) and Rf(P) for\nMNIST, Fashion MNIST and CIFAR10 for two SOTA architectures. The central results are that (i)\nstability toward noise is always reduced for larger training sets. This observation is natural: when\nmore data needs to be ﬁtted, the function becomes rougher. (ii) Stability toward diffeomorphisms\ndoes not behave universally: it can increase with P or decrease depending on the architecture and the\ntraining set. Additionally, Gf and Df alone show a much smaller correlation with performance than\nRf– see Figs.15,16,17 in Appendix E.3.\nFigure 6: Stability toward Gaussian noise (Gf) and diffeomorphisms (Df) alone, and the rela-\ntive stability Rf. Columns correspond to different data-sets (MNIST, FashionMNIST and CIFAR10)\nand rows to architectures (ResNet18 and EfﬁcientNetB0). Each panel reports Gf (blue), Df (orange)\nand Rf (green) as a function of P and for different cut-off values c, as indicated in the legend.\nStatistics: cf. caption in Fig.3. Error bars – omitted here – are shown in Fig.22, Appendix E.3.\n6\nA minimal model for learning invariants\nIn this section, we discuss the simplest model of invariance in data where stability to transformation\nbuilds up, that can be compared with our observations of Rf above. Speciﬁcally, we consider the\n\"stripe\" model Paccolat et al. (2021b), corresponding to a binary classiﬁcation task for Gaussian-\ndistributed data points x = (x∥, x⊥) where the label function depends only on one direction in data\nspace, namely y(x) = y(x∥). Layers of y = +1 and y = −1 regions alternate along the direction\nx∥, separated by parallel planes. Hence, the data present d −1 invariant directions in input-space\ndenoted by x⊥as illustrated in Fig.7-left.\nWhen this model is learnt by a one-hidden-layer fully connected net, the ﬁrst layer of weights can be\nshown to align with the informative direction Paccolat et al. (2021a). The projection of these weights\n3Correlation coefﬁcient:\nCov(log ϵt,log Rf )\n√\nVar(log ϵt)Var(log Rf ).\n8\n73\n−2\n−1\n0\n1\n2\nx∥\n−2\n−1\n0\n1\n2\nx⊥\nStripe Model\n103\nP\n10−2\n10−1\nRf\n-1\nFigure 7: Left: example of the\nstripe model. Dots are data-\npoints, the vertical lines rep-\nresent the decision boundary\nand the color the class label.\nRight: Relative stability Rf\nfor the stripe model in d = 30.\nThe slope of the curve is −1,\nas predicted.\non the orthogonal space vanishes with the training set size P as 1/\n√\nP, an effect induced by the\nsampling noise associated to ﬁnite training sets.\nIn this model, Rf can be deﬁned as:\nRf = ⟨∥f(x∥, x⊥+ ν) −f(x∥, x⊥)∥2⟩x,ν\n⟨∥f(x + η) −f(x)∥2⟩x,η\n,\n(6)\nwhere we made explicit the dependence of f on the two linear subspaces. Here, the isotropic noise\nν is added only in the invariant directions. Again, we impose ∥η∥= ∥ν∥. Rf(P) is shown in Fig.\n7-right. We observe that Rf(P) ∼P −1, as expected from the weight alignment mentioned above.\nInterestingly, Fig.3 for CIFAR10 and SOTA architectures support that the 1/P behavior is compatible\nwith the observations for some range of P. In Appendix E.3, Fig.13, we show analogous results for\nMNIST and Fashion-MNIST. We observe the 1/P power-law scaling for ResNets. It suggests that\nfor these architectures, learning to become invariant to diffeomorphisms may be limited by a naive\nmeasure of sampling noise as well. By contrast for EfﬁcientNets, in which the decrease in Rf is\nmore limited, a 1/P behavior cannot be identiﬁed.\n7\nDiscussion\nA common belief is that stability to random noise (small Gf) and to diffeomorphisms (small Df)\nare desirable properties of neural nets. Its underlying assumption is that the true data label mildly\ndepends on such transformations when they are small. Our observations suggest an alternative view:\n1. Figs.6,16: better predictors are more sensitive to small perturbations in input space.\n2. As a consequence, the notion that predictors are especially insensitive to diffeomorphisms is\nnot captured by stability alone, but rather by the relative stability Rf = Df/Gf.\n3. We propose the following interpretation of Fig.5: to perform well, the predictor must build\nlarge gradients in input space near the decision boundary – leading to a large Gf overall.\nNetworks that are relatively insensitive to diffeomorphisms (small Rf) can discover with\nless data that strong gradients must be there and generalize them to larger regions of input\nspace, improving performance and increasing Gf.\nThis last point can be illustrated in the simple model of Section 6, see Fig.7-left panel. Imagine\ntwo data points of different labels falling close to the – e.g. – left true decision boundary. These\ntwo points can be far from each other if their orthogonal coordinates differ. Yet, if Rf = 0 (now\ndeﬁned in Eq.6), then the output does not depend on the orthogonal coordinates, and it will need to\nbuild a strong gradient – in input space – along the parallel coordinate to ﬁt these two data. This\nstrong gradient will exist throughout that entire decision boundary, improving performance but also\nincreasing Gf. Instead, if Rf = 1, ﬁtting these two data will not lead to a strong gradient, since they\ncan be far from each other in input space. Beyond this intuition, in this model decreasing Rf can\nquantitatively be shown to increase performance, see Paccolat et al. (2021b).\n9\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n74\n8\nConclusion\nWe have introduced a novel empirical framework to characterize how deep nets become invariant to\ndiffeomorphisms. It is jointly based on a maximum-entropy distribution for diffeomorphisms, and on\nthe realization that stability of these transformations relative to generic ones Rf strongly correlates to\nperformance, instead of just the diffeomorphisms stability considered in the past.\nThe ensemble of smooth deformations we introduced may have interesting applications. It could\nserve as a complement to traditional data-augmentation techniques (whose effect on relative stability\nis discussed in Fig.12 of the Appendix). A similar idea is present in Hauberg et al. (2016); Shen\net al. (2020) but our deformations have the advantage of being easier to sample and data agnostic.\nMoreover, the ensemble could be used to build adversarial attacks along smooth transformations, in\nthe spirit of Alaifari et al. (2018); Engstrom et al. (2019); Kanbak et al. (2018). It would be interesting\nto test if networks robust to such attacks are more stable in relative terms, and how such robustness\naffects their performance.\nFinally, the tight correlation between relative stability Rf and test error ϵt suggests that if a predictor\ndisplays a given Rf, its performance may be bounded from below. The relationships we observe\nϵt(Rf) may then be indicative of this bound, which would be a fundamental property of a given data\nset. Can it be predicted in terms of simpler properties of the data? Introducing simpliﬁed models of\ndata with controlled stability to diffeomorphisms beyond the toy model of Section 6 would be useful\nto investigate this key question.\nAcknowledgements\nWe thank Alberto Bietti, Joan Bruna, Francesco Cagnetta, Pascal Frossard, Jonas Paccolat, Antonio\nSclocchi and Umberto M. Tomasini for helpful discussions. This work was supported by a grant from\nthe Simons Foundation (#454953 Matthieu Wyart).\nReferences\nAlaifari, R., Alberti, G. S., and Gauksson, T. (2018). ADef: an Iterative Algorithm to Construct\nAdversarial Deformations.\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and Nguyen, A. (2019). Strike\n(With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects. In 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4840–4849,\nLong Beach, CA, USA. IEEE.\nAmodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J.,\nCatanzaro, B., Cheng, Q., Chen, G., et al. (2016). Deep speech 2: End-to-end speech recognition\nin english and mandarin. In International conference on machine learning, pages 173–182.\nAnsuini, A., Laio, A., Macke, J. H., and Zoccolan, D. (2019). Intrinsic dimension of data represen-\ntations in deep neural networks. In Advances in Neural Information Processing Systems, pages\n6111–6122.\nAthalye, A., Engstrom, L., Ilyas, A., and Kwok, K. (2018). Synthesizing Robust Adversarial\nExamples. In International Conference on Machine Learning, pages 284–293. PMLR. ISSN:\n2640-3498.\nAzulay, A. and Weiss, Y. (2018). Why do deep convolutional networks generalize so poorly to small\nimage transformations? arXiv preprint arXiv:1805.12177.\nBach, F. (2017). Breaking the curse of dimensionality with convex neural networks. The Journal of\nMachine Learning Research, 18(1):629–681.\nBeale, P. (1996). Statistical Mechanics. Elsevier Science.\nBietti, A. and Mairal, J. (2019a). Group invariance, stability to deformations, and complexity of deep\nconvolutional representations. The Journal of Machine Learning Research, 20(1):876–924.\n10\n75\nBietti, A. and Mairal, J. (2019b). On the inductive bias of neural tangent kernels. arXiv preprint\narXiv:1905.12173.\nBruna, J. and Mallat, S. (2013). Invariant scattering convolution networks. IEEE transactions on\npattern analysis and machine intelligence, 35(8):1872–1886.\nChizat, L. and Bach, F. (2020). Implicit Bias of Gradient Descent for Wide Two-layer Neural\nNetworks Trained with the Logistic Loss. In Conference on Learning Theory, pages 1305–1338.\nPMLR. ISSN: 2640-3498.\nDeng, J., Dong, W., Socher, R., Li, L., Kai Li, and Li Fei-Fei (2009). ImageNet: A large-scale hierar-\nchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npages 248–255. ISSN: 1063-6919.\nDieleman, S., De Fauw, J., and Kavukcuoglu, K. (2016). Exploiting cyclic symmetry in convolutional\nneural networks. arXiv preprint arXiv:1602.02660.\nEngstrom, L., Tran, B., Tsipras, D., Schmidt, L., and Madry, A. (2019). Exploring the Landscape of\nSpatial Robustness. In International Conference on Machine Learning, pages 1802–1811. PMLR.\nISSN: 2640-3498.\nFawzi, A. and Frossard, P. (2015). Manitest: Are classiﬁers really invariant? In Procedings of the\nBritish Machine Vision Conference 2015, pages 106.1–106.13, Swansea. British Machine Vision\nAssociation.\nGeiger, M., Petrini, L., and Wyart, M. (2021). Landscape and training regimes in deep learning.\nPhysics Reports.\nGeiger, M., Spigler, S., Jacot, A., and Wyart, M. (2020). Disentangling feature and lazy training in\ndeep neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2020(11):113301.\nPublisher: IOP Publishing.\nGhorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2019). Limitations of lazy training\nof two-layers neural network. In Advances in Neural Information Processing Systems, pages\n9111–9121.\nGhorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2020). When Do Neural Networks\nOutperform Kernel Methods? Advances in Neural Information Processing Systems, 33.\nHauberg, S., Freifeld, O., Larsen, A. B. L., Fisher III, J. W., and Hansen, L. K. (2016). Dreaming\nMore Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation.\narXiv:1510.02795 [cs]. arXiv: 1510.02795.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep Residual Learning for Image Recognition.\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778.\nISSN: 1063-6919.\nHuval, B., Wang, T., Tandon, S., Kiske, J., Song, W., Pazhayampallil, J., Andriluka, M., Rajpurkar, P.,\nMigimatsu, T., Cheng-Yue, R., et al. (2015). An empirical evaluation of deep learning on highway\ndriving. arXiv preprint arXiv:1504.01716.\nJacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization\nin neural networks. In Proceedings of the 32Nd International Conference on Neural Information\nProcessing Systems, NIPS’18, pages 8580–8589, USA. Curran Associates Inc.\nKanbak, C., Moosavi-Dezfooli, S.-M., and Frossard, P. (2018). Geometric Robustness of Deep\nNetworks: Analysis and Improvement. In 2018 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 4441–4449, Salt Lake City, UT. IEEE.\nKardar, M. (2007). Statistical physics of ﬁelds. Cambridge University Press.\nKayhan, O. S. and Gemert, J. C. v. (2020). On translation invariance in cnns: Convolutional layers\ncan exploit absolute spatial location. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14274–14285.\n11\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n76\nKopitkov, D. and Indelman, V. (2020). Neural Spectrum Alignment: Empirical Study. Artiﬁcial\nNeural Networks and Machine Learning – ICANN 2020.\nKrizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q., editors,\nAdvances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc.\nLe, Q. V. (2013). Building high-level features using large scale unsupervised learning. In 2013 IEEE\ninternational conference on acoustics, speech and signal processing, pages 8595–8598. IEEE.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.\n(1989). Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation,\n1(4):541–551.\nLecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324. Conference Name: Proceedings of the\nIEEE.\nLee, J., Schoenholz, S. S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and Sohl-Dickstein, J.\n(2020). Finite versus inﬁnite neural networks: an empirical study. arXiv preprint arXiv:2007.15801.\nLoshchilov, I. and Hutter, F. (2016). SGDR: Stochastic Gradient Descent with Warm Restarts.\nLowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal\nof computer vision, 60(2):91–110.\nLuxburg, U. v. and Bousquet, O. (2004). Distance-based classiﬁcation with lipschitz functions.\nJournal of Machine Learning Research, 5(Jun):669–695.\nMallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of the\nRoyal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150203.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.\n(2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\nNovak, R., Bahri, Y., Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J. (2018). Sensitivity and\ngeneralization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760.\nOymak, S., Fabian, Z., Li, M., and Soltanolkotabi, M. (2019). Generalization guarantees for neural\nnetworks via harnessing the low-rank structure of the jacobian. arXiv preprint arXiv:1906.05392.\nPaccolat, J., Petrini, L., Geiger, M., Tyloo, K., and Wyart, M. (2021a). Geometric compression of\ninvariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment,\n2021(4):044001. Publisher: IOP Publishing.\nPaccolat, J., Spigler, S., and Wyart, M. (2021b). How isotropic kernels perform on simple invariants.\nMachine Learning: Science and Technology, 2(2):025020. Publisher: IOP Publishing.\nRecanatesi, S., Farrell, M., Advani, M., Moore, T., Lajoie, G., and Shea-Brown, E. (2019). Dimen-\nsionality compression and expansion in deep neural networks. arXiv preprint arXiv:1906.00443.\nReﬁnetti, M., Goldt, S., Krzakala, F., and Zdeborová, L. (2021). Classifying high-dimensional\ngaussian mixtures: Where kernel methods fail and neural networks succeed. arXiv preprint\narXiv:2102.11742.\nRuderman, A., Rabinowitz, N. C., Morcos, A. S., and Zoran, D. (2018). Pooling is neither necessary\nnor sufﬁcient for appropriate deformation stability in CNNs. arXiv:1804.04438 [cs, stat]. arXiv:\n1804.04438.\nSaxe, A. M., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey, B. D., and Cox, D. D.\n(2019). On the information bottleneck theory of deep learning. Journal of Statistical Mechanics:\nTheory and Experiment, 2019(12):124020.\n12\n77\nShen, Z., Xu, Z., Olut, S., and Niethammer, M. (2020). Anatomical Data Augmentation via Fluid-\nBased Image Registration. In Martel, A. L., Abolmaesumi, P., Stoyanov, D., Mateus, D., Zuluaga,\nM. A., Zhou, S. K., Racoceanu, D., and Joskowicz, L., editors, Medical Image Computing and\nComputer Assisted Intervention – MICCAI 2020, Lecture Notes in Computer Science, pages\n318–328, Cham. Springer International Publishing.\nShi, B., Bai, X., and Yao, C. (2016). An end-to-end trainable neural network for image-based\nsequence recognition and its application to scene text recognition. IEEE transactions on pattern\nanalysis and machine intelligence, 39(11):2298–2304.\nShwartz-Ziv, R. and Tishby, N. (2017). Opening the black box of deep neural networks via information.\narXiv preprint arXiv:1703.00810.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,\nL., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. nature,\n550(7676):354–359.\nSimonyan, K. and Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image\nRecognition. ICLR.\nTan, M. and Le, Q. (2019). EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural\nNetworks. In International Conference on Machine Learning, pages 6105–6114. PMLR. ISSN:\n2640-3498.\nTsuzuku, Y. and Sato, I. (2019). On the structural sensitivity of deep convolutional networks to the\ndirections of fourier basis functions. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 51–60.\nXiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D. (2018). Spatially Transformed Adversarial\nExamples.\nXiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-MNIST: a Novel Image Dataset for Bench-\nmarking Machine Learning Algorithms. arXiv:1708.07747 [cs, stat]. arXiv: 1708.07747.\nYehudai, G. and Shamir, O. (2019). On the power and limitations of random features for understanding\nneural networks. In Advances in Neural Information Processing Systems, pages 6598–6608.\nYin, D., Lopes, R. G., Shlens, J., Cubuk, E. D., and Gilmer, J. (2019). A fourier perspective on model\nrobustness in computer vision. arXiv preprint arXiv:1906.08988.\nZhang, R. (2019).\nMaking convolutional networks shift-invariant again.\narXiv preprint\narXiv:1904.11486.\n13\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n78\nA\nMaximum entropy calculation\nUnder the constraint on the borders, τu and τv can be expressed in a real Fourier basis as in Eq.2. By\ninjecting this form into ∥∇τ∥2 we obtain:\n∥∇τ∥2 = π2\n4\nX\ni,j∈N+\n(C2\nij + D2\nij)(i2 + j2)\n(7)\nwhere Dij are the Fourier coefﬁcients of τv. We aim at computing the probability distributions that\nmaximize their entropy while keeping the expectation value of ∥∇τ∥2 ﬁxed. Since we have a sum\nof quadratic random variables, the equipartition theorem Beale (1996) applies: the distributions are\nnormal and every quadratic term contributes in average equally to ∥∇τ∥2. Thus, the variance of the\ncoefﬁcients follows\nT\ni2+j2 where the parameter T determines the magnitude of the diffeomorphism.\nB\nBoundaries of studied diffeomorphisms\nAverage pixel displacement magnitude δ\nWe derive here the large-c asymptotic behavior of δ\n(Eq.3). This is deﬁned as the average square norm of the displacement ﬁeld, in pixel units:\nδ2 = n2\nZ\n[0,1]2 ∥τ(u, v)∥2dudv\n= 2Tn2\nX\ni2+j2≤c2\n1\ni2 + j2\nZ\n[0,1]2 sin2(iπu) sin2(jπv)dudv\n= Tn2\n2\nX\ni2+j2≤c2\n1\ni2 + j2\n≈Tn2\n2\nZ\n1≤x2+y2≤c2\n1\nx2 + y2 dxdy\n= πTn2\n4\nZ c\n1\n1\nr dr\n= π\n4 n2T log c,\nwhere we approximated the sum with an integral, in the third step. The asymptotic relations for ∥∇τ∥\nthat are reported in the main text are computed in a similar fashion. In Fig.8, we check the agreement\nbetween asymptotic prediction and empirical measurements. If δ ≪1, our results strongly depend\non the choice of interpolation method. To avoid it, we only consider conditions for which δ ≥1/2,\nleading to\nT >\n1\nπn2 log c.\n(8)\n100\n101\n102\nc\n2\n4\nδ2/(Tn2)\nprediction:\nπ\n4 log(c)\nmeasure\n100\n101\n102\nc\n100\n101\n102\n103\nmaxs Ξ/\n√\nT\nprediction:\nc\n2\np\nπ3 log c\nmeasure\nFigure 8: Left: The characteristic displacement δ(c, T) is observed to follow δ2 ≃π\n4 n2T log c. Right:\nmeasurement of maxs Ξ supporting Eq.13.\n14\n79\n(a)\n(b)\n(c)\n(d)\nFigure 9: (a) Idealized image at T = 0. (b) Diffeomorphism of the image. (c) Deformation of the\nimage at large T: colors get mixed-up together, shapes are not preserved anymore. (d) Allowed\nregion for vector transformations under τ. For any point in the image s and any direction u, only\ndisplacement ﬁelds for which all the deformed direction u′ is non-zero generate diffeomorphisms.\nThe bound in Eq.12 (u′ · u > 0) correspond to the green region. The gray disc corresponds to the\nbound ∥∇τ∥∞< 1.\nCondition for diffeomorphism in the (T, c) plane\nFor a given value of c, there exists a tempera-\nture scale beyond which the transformation is not injective anymore, affecting the topology of the\nimage and creating spurious boundaries, see Fig.9a-c for an illustration. Speciﬁcally, consider a curve\npassing by the point s in the deformed image. Its tangent direction is u at the point s. When going\nback to the original image (s′ = s −τ(s)) the curve gets deformed and its tangent becomes\nu′ = u −(u · ∇)τ(s).\n(9)\nA smooth deformation is bijective iff all deformed curves remain curves which is equivalent to have\nnon-zero tangents everywhere\n∀s, u ̸= 0\n∥u′∦= 0.\n(10)\nImposing ∥u′∦= 0 does not give us any constraint on τ. Therefore, we constraint τ a bit more and\nallow only displacement ﬁelds such that u · u′ > 0, which is a sufﬁcient condition for Eq.10 to be\nsatisﬁed – cf. Fig. 9d. By extremizing over u, this condition translates into\n1\n2\n\u0012q\n(∂xτx −∂yτy)2 + (∂xτy + ∂yτx)2 −∂xτx −∂yτy\n\u0013\n< 1\n(11)\nor, equivalently,\nΞ = 1\n2\n\u0010p\n||∇τ||2 −2 det(∇τ) −Tr(∇τ)\n\u0011\n< 1,\n(12)\nwere we identiﬁed by Ξ the l.h.s. of the inequality. We ﬁnd that the median of the maximum of Ξ\nover all the image (∥Ξ(s)∥∞) can be approximated by (see Fig.8b):\nmax\ns\nΞ ≃c\n2\np\nπ3T log c.\n(13)\nThe resulting constraint on T reads\nT <\n4\nπ3c2 log c.\n(14)\nC\nInterpolation methods\nWhen a deformation is applied to an image x, each of its pixels gets mapped, from the original pixels\ngrid, to new positions generally outside of the grid itself – cf. Fig. 9a-b. A procedure (interpolation\nmethod) needs to be deﬁned to project the deformed image back into the original grid.\nFor simplicity of notation, we describe interpolation methods considering the square [0, 1]2 as the\nregion in between four pixels – see an illustration in Fig. 10a. We propose here two different ways\nto interpolate between pixels and then check that our measurements do not depend on the speciﬁc\nmethod considered.\nBi-linear Interpolation\nThe bi-linear interpolation consists, as the name suggests, of two steps of\nlinear interpolation, one on the horizontal, and one on the vertical direction – Fig. 10b. If we look at\nthe square [0, 1]2 and we apply a deformation τ such that (0, 0) 7→(u, v), we have\nx(u, v) = x(0, 0)(1 −u)(1 −v) + x(1, 0)u(1 −v) + x(0, 1)(1 −u)v + x(1, 1)uv.\n(15)\n15\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n80\nτ\n(0,0)\n(0,1)\n(1,1)\n(1,0)\n(u,v)\n(0,0)\n(0,1)\n(1,1)\n(1,0)\nx\nx\n(a)\n(b)\n(c)\ns3\ns4\ns2\ns1\ns\n(u,v)\nFigure 10: (a) We consider the region between four pixels as the square [0, 1]2 where, after the\napplication of a deformation τ, the pixel (0, 0) is mapped into (u, v). (b) Bi-linear interpolation:\nthe value of x in (u, v) is computed by two steps of linear interpolation. First, we compute x in the\nred crosses, by averaging values on the vertical axis. Then, a line interpolates horizontally the values\nin the red crosses to give the result. (c) Gaussian interpolation: we denote by si the pixel positions\nin the original grid. The interpolated value of s in any point of the image is given by a weighted sum\nof n × n Gaussian centered in each si – in red.\nGaussian Interpolation\nIn this case, a Gaussian function4 is placed on top of each point in the\ngrid – cf. Fig.10. The pixel intensity x can be evaluated at any point outside the grid by computing\nx(s) =\nP\ni x(si)G(s −si)\nP\ni G(s −si)\n.\n(16)\nIn order to ﬁx the standard deviation σ of G, we introduce the participation ratio n. Given Ψi =\nG(s, si)|s=(0.5,0.5), we deﬁne\nn =\n\u0000P\ni Ψ2\ni\n\u00012\nP\ni Ψ4\ni\n.\n(17)\nThe participation ratio is a measure of how many pixels contribute to the value of a new pixel,\nwhich results from interpolation. We ﬁx σ in such a way that the participation ratio for the Gaussian\ninterpolation matches the one for the bi-linear (n = 4), when the new pixel is equidistant from the\nfour pixels around. This gives σ = 0.4715.\nNotice that this interpolation method is such that it applies a Gaussian smoothing of the image even if\nτ is the identity. Consequently, when computing observables for f with the Gaussian interpolation,\nwe always compare f(τx) to f(˜x), where ˜x is the smoothed version of x, in such a way that\nf(τ [T =0]x) = f(˜x).\nEmpirical results dependence on interpolation\nFinally, we checked to which extent our results\nare affected by the speciﬁc choice of interpolation method. In particular, blue and red colors in Figs3,\n13 correspond to bi-linear and Gaussian interpolation, respectively. The interpolation method only\naffects the results in the small displacement limit (δ →0).\nNote: throughout the paper, if not speciﬁed otherwise, bi-linear interpolation is employed.\n4G(s) = (2πσ2)−1/2e−s2/2σ2.\n16\n81\nD\nStability to additive noise vs. noise magnitude\n10−1\n100\n101\n∥η∥\n10−7\n10−5\n10−3\n10−1\nGf\n2\nCIFAR10\nResNet18\nEﬃcientNetB0\ninit\ntrained\n10−1\n100\n101\n∥η∥\n10−7\n10−4\n10−1\n2\nImageNet\nResNet101\nEﬃcientNetB2\ninit\ntrained\nFigure 11: Stability to isotropic noise Gf as a function of the noise magnitude ∥η∥for CIFAR10\n(left) and ImageNet (right). The color corresponds to two different classes of SOTA architecture:\nResNet and EfﬁcientNet. The slope 2 at small ∥η∥identiﬁes the linear regime. For larger noise\nmagnitudes, non-linearities appear.\nWe introduced in Section 5 the stability toward additive noise:\nGf = ⟨∥f(x + η) −f(x)∥2⟩x,η\n⟨∥f(x) −f(z)∥2⟩x,z\n.\n(18)\nWe study here the dependence of Gf on the noise magnitude ∥η∥. In the η →0 limit, we expect the\nnetwork function to behave as its ﬁrst-order Taylor expansion, leading to Gf ∝∥η∥2. Hence, for\nsmall noise, Gf gives an estimate of the average magnitude of the gradient of f in a random direction\nη.\nEmpirical results\nMeasurements of Gf on SOTA nets trained on benchmark data-sets are shown\nin Figure 11. We observe that the effect of non-linearities start to be signiﬁcant around ∥η∥= 1. For\nlarge values of the noise – i.e. far away from data-points – the average gradient of f does not change\nwith training.\nE\nNumerical experiments\nIn this Appendix, we provide details on the training procedure, on the different architectures employed\nand some additional experimental results.\nE.1\nImage classiﬁcation training set-up:\n◦Trainings\nare\nperformed\nin\nPyTorch,\nthe\ncode\ncan\nbe\nfound\nhere\ngithub.com/leonardopetrini/diffeo-sota.\n◦Loss function: cross-entropy.\n◦Batch size: 128.\n◦Dynamics:\n– Fully connected nets: ADAM with learning rate = 0.1 and no scheduling.\n– Transfer learning: SGD with learning rate = 10−2 for the last layer and 10−3 for\nthe rest of the network, momentum = 0.9 and weight decay = 10−3. Both learning\nrates decay exponentially during training with a factor γ = 0.975.\n– All the other networks are trained with SGD with learning rate = 0.1, momentum\n= 0.9 and weight decay = 5 × 10−4. The learning rate follows a cosine annealing\nscheduling Loshchilov and Hutter (2016).\n17\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n82\n◦Early-stopping is performed – i.e. results shown are computed with the network obtaining\nthe best validation accuracy out of 250 training epochs.\n◦For the experiments involving a training on a subset of the training date of size P < Pmax,\nthe total number of epochs is accordingly re-scaled in order to keep constant the total number\nof optimizer steps.\n◦Standard data augmentation is employed: different random translations and horizontal ﬂips\nof the input images are generated at each epoch. As a safety check, we verify that the\ninvariance learnt by the nets is not purely due to such augmentation (Fig.12).\n◦Experiments are run on 16 GPUs NVIDIA V100. Individual trainings run in ∼1 hour of\nwall time. We estimate a total of a few thousands hours of computing time for running the\npreliminary and actual experiments present in this work.\nThe stripe model is trained with an approximation of gradient ﬂow introduced in Geiger et al. (2020),\nsee Paccolat et al. (2021a) for details.\nnone\ntranslation\ndiﬀeo\nData Augmentation\n10−2\n10−1\n100\nRf|δ=1\nResNet on MNIST\nc = 3\nc = 5\nc = 15\nnone\ntranslation\ndiﬀeo\nData Augmentation\n10−2\n10−1\n100\nResNet on F-MNIST\nc = 3\nc = 5\nc = 15\nnone\ntranslation\ndiﬀeo\nData Augmentation\n10−1\n100\nResNet on CIFAR10\nc = 3\nc = 5\nc = 15\nFigure 12: Effect of data augmentation on Rf. Relative stability to diffeomorphisms Rf after\ntraining with different data augmentations: \"none\" (1st group of bars in each plot) for no data\naugmentation, \"translation\" (2nd bars) corresponds to training on randomly translated (by 4 pixels)\nand cropped inputs, and \"diffeo\" (3rd bars) to training on randomly deformed images with max-\nentropy diffeomorphisms (T = 10−2, c = 1). Results are averaged over 5 trainings of ResNet18 on\nMNIST (left), FashionMNIST (center), CIFAR10 (right). Colors indicate different cut-off values\nwhen probing the trained networks. Different augmentations have a small quantitative, and no\nqualitative effect on the results. As expected, augmenting the input images with smooth deformations\nmakes the net more invariant to such transformations.\nA note on computing stabilities at init. in presence of batch-norm\nWe recall that batch-norm\n(BN) can work in either of two modes: training and evaluation. During training, BN computes the\nmean and variance on the current batch and uses them to normalize the output of a given layer. At\nthe same time, it keeps memory of the running statistics on such batches, and this is used for the\nnormalization steps at inference time (evaluation mode). When probing a network at initialization for\ncomputing stabilities, we put the network in evaluation mode, except for batch-norm (BN), which\noperates in train mode. This is because BN running mean and variance are initialized to 0 and 1,\nin such a way that its evaluation mode at initialization would correspond to not having BN at all,\ncompromising the input signal propagation in deep architectures.\n18\n83\nE.2\nNetworks architectures\nAll\nnetworks\nimplementations\ncan\nbe\nfound\nat\ngithub.com/leonardopetrini/diffeo-\nsota/tree/main/models.\nIn Table 1, we report salient features of the network architectures\nconsidered.\nTable 1: Network architectures, main characteristics. We list here (columns) the classes of net\narchitectures used throughout the paper specifying some salient features (depth, number of parameters,\netc...) for each of them.\nfeatures\nFullConn\nLeNet\nAlexNet\nLeCun et al. (1989)\nKrizhevsky et al. (2012)\ndepth\n2, 4, 6\n5\n8\nnum. parameters\n200k\n62k\n23 M\nFC layers\n2, 4, 6\n3\n3\nactivation\nReLU\nReLU\nReLU\npooling\n/\nmax\nmax\ndropout\n/\n/\nyes\nbatch norm\n/\n/\n/\nskip connections\n/\n/\n/\nfeatures\nVGG\nResNet\nEfﬁcientNetB0-2\nSimonyan and Zisserman (2015)\nHe et al. (2016)\nTan and Le (2019)\ndepth\n11, 16, 19\n18, 34, 50\n18, 25\nnum. parameters\n9-20 M\n11-24 M\n5, 9 M\nFC layers\n1\n1\n1\nactivation\nReLU\nReLU\nswish\npooling\nmax\navg. (last layer only)\navg. (last layer only)\ndropout\n/\n/\nyes + dropconnect\nbatch norm\nif ’bn’ in name\nyes\nyes\nskip connections\n/\nyes\nyes (inv. residuals)\n19\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n84\nE.3\nAdditional ﬁgures\nWe present here:\n◦Fig.13: Rf as a function of P for MNIST and FashionMNIST with the corresponding\npredicted slope, omitted in the main text.\n◦Fig.14: Relative diffeomorphisms stability Rf as a function of depth for simple and deep\nnets.\n◦Figs15,16: diffeomorphisms and inverse of the Gaussian stability Df and 1/Gf vs. test\nerror for CIFAR10 and the set of architectures considered in Section 4.\n◦Fig.17: Df, 1/Gf and Rf when using the mean in place of the median for computing\naverages ⟨·⟩.\n◦Fig.18: curves in the (ϵt, Rf) plane when varying the training set size P for FullyConnL4,\nLeNet, ResNet18 and EfﬁcientNetB0.\n◦Figs19, 22: error estimates for the main quantities of interest – often omitted in the main\ntext for the sake of ﬁgures’ clarity.\n102\n103\n104\nP\n10−2\n100\nRf\n-1\n(a) — ResNet18 on MNIST\n102\n103\n104\nP\n10−2\n100\n-1\n(b) — EﬃcientNetB0 on MNIST\n102\n103\n104\nP\n10−2\n100\n-1\n(c) — ResNet18 on F-MNIST\n102\n103\n104\nP\n10−2\n100\n-1\n(d) — EﬃcientNetB0 on F-MNIST\nLinear interp.\nGauss. interp.\ninit\ntrained\nFigure 13: Relative stability to diffeomorphisms Rf(P) at δ = 1. Analogous to Figure 3-right but\nhere we have MNIST (a-b) and FashionMNIST (c-d) in place of CIFAR10. Stability monotonically\ndecreases with P. The triangles give a reference for the predicted slope in the stripe model – i.e.\nRf ∼P −1 – see Section 6. The slopes in case of ResNets are compatible with the prediction. For\nEfﬁcientNets, the second panel of Fig.3 suggests that stability to diffeomorphisms is less important.\nHere, we also see that it builds up more slowly when increasing the training set size. Finally, blue\nand red colors indicate different interpolation methods used for generating image deformations, as\ndiscussed in Appendix C. Results are not affected by this choice.\n20\n85\n0.0\n0.25\n0.5\n0.75\n1.0\n5\n10\nRf\nFullConnL2\nFullConnL4\nFullConnL6\ninit.\ntrained\n0.0\n0.25\n0.5\n0.75\n1.0\n10−1\n100\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\ninit.\ntrained\n0.0\n0.25\n0.5\n0.75\n1.0\nrelative depth\n1\n2\n1\nRf\nLeNet\ninit.\ntrained\n0.0\n0.25\n0.5\n0.75\n1.0\nrelative depth\n10−1\n100\nResNet18\nResNet34\nResNet50\ninit.\ntrained\nFigure 14: Relative stability to diffeomorphisms as a function of depth. Rf as a function of the\nlayers relative depth (i.e. current layer depth\ntotal depth\n) where \"0\" identiﬁes the output of the 1st layer and \"1\" the\nlast. The relative stability is measured for the output of layers (or blocks of layers) inside the nets\nfor simple architectures (1st column) and deep ones (2nd column) at initialization (dashed) and after\ntraining (full lines). All nets are trained on the full CIFAR10 dataset. Rf0 ≈1 independently of\ndepth at initialization while it decreases monotonically as a function of depth after training. Statistics:\nEach point is obtained by training 5 differently initialized networks; each network is then probed\nwith 500 test samples in order to measure Rf. The results are obtained by log-averaging over single\nrealizations.\nCIFAR10\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nLeNet\nVGG16bn\nResNet18\nVGG19bn\nEfficientNetB2\nResNet50\nResNet34\nEfficientNetB0\nVGG11bn\nEfficientNetB0\nnumber of layers\n2          50\nbatch-norm\nskip connections\ntransfer learning from ImageNet\n10%\n3%\n30%\nFigure 15: Test error ϵt vs. stability to diffeomorphisms Df for common architectures when\ntrained on the full 10-classes CIFAR10 dataset (P = 50k) with SGD and the cross-entropy loss; the\nEfﬁcientNets achieving the best performance are trained by transfer learning from ImageNet (⋆) –\nmore details on the training procedures can be found in Appendix E.1. The color scale indicates\ndepth, and the symbols the presence of batch-norm (⋄) and skip connections (†). Df correlation\nwith ϵt (corr. coeff.: 0.62) is much smaller than the one measured for Rf – see Fig.3. Statistics:\nEach point is obtained by training 5 differently initialized networks; each network is then probed\nwith 500 test samples in order to measure Df. The results are obtained by log-averaging over single\nrealizations.\n21\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n86\n10%\n3%\n30%\nnets performance vs inverse noise stability\nCIFAR10\nnumber of layers\n2          50\nbatch-norm\nskip connections\ntransfer learning from ImageNet\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nVGG11bn\nLeNet\nVGG16bn\nResNet34\nResNet18\nVGG19bn\nEfficientNetB2\nResNet50\nEfficientNetB0\nEfficientNetB0\nFigure 16: Test error ϵt vs. inverse of stability to noise 1/Gf for common architectures when\ntrained on the full 10-classes CIFAR10 dataset (P = 50k) with SGD and the cross-entropy loss; the\nEfﬁcientNets achieving the best performance are trained by transfer learning from ImageNet (⋆) –\nmore details on the training procedures can be found in Appendix E.1. The color scale indicates\ndepth, and the symbols the presence of batch-norm (⋄) and skip connections (†). Gf correlation\nwith ϵt (corr. coeff.: 0.85) is less important than the one measured for Rf – see Fig.3. Statistics:\nEach point is obtained by training 5 differently initialized networks; each network is then probed\nwith 500 test samples in order to measure Gf. The results are obtained by log-averaging over single\nrealizations.\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nVGG11bn\nLeNet\nEfficientNetB0\nVGG19bn\nVGG16bn\nResNet18 ResNet34\nResNet50\nEfficientNetB0\nEfficientNetB2\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nVGG11bn\nLeNet\nEfficientNetB0\nVGG19bn\nVGG16bn\nResNet18\nResNet34\nResNet50\nEfficientNetB0\nEfficientNetB2\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nVGG11bn\nLeNet\nEfficientNetB0\nVGG19bn\nVGG16bn\nResNet18\nResNet34\nResNet50\nEfficientNetB0\nEfficientNetB2\nFigure 17: Test error ϵt vs. Df, 1/Gf and Rf where ⟨·⟩is the mean. Analogous to Figs15-19, we\nuse here the mean instead of the median to compute averages over samples and transformations.\n22\n87\n512\n5e4 train set size P\nLeNet\nFullConnL4\nEfficientNetB0\nResNet18\nother nets\nnets performance vs relative diffeo stability\ntrain set size P dependence\nCIFAR10\nFigure 18: Test error ϵt vs. relative stability to diffeomorphisms Rf for different training set\nsizes P. Same data as Fig.5, we report here curves corresponding to training on different set sizes\nfor 4 architectures. The other architectures considered together with the power-law ﬁt are left in\nbackground. For a small training set, CNNs behave similarly. Statistics: Each point is obtained by\ntraining 5 differently initialized networks; each network is then probed with 500 test samples in order\nto measure Rf. The results are obtained by log-averaging over single realizations.\nnets performance vs diffeo invariance\nCIFAR10\nFullConnL2\nFullConnL4\nFullConnL6\nVGG11\nAlexNet\nVGG11bn\nLeNet\nEfficientNetB0\nVGG19bn\nVGG16bn\nResNet18\nResNet34\nResNet50\nEfficientNetB0\nEfficientNetB2\nFigure 19: Test error ϵt vs. relative stability to diffeomorphisms Rf with error estimates. Same\ndata as Fig.5, we report error bars here. Statistics: Each point is obtained by training 5 differently\ninitialized networks; each network is then probed with 500 test samples in order to measure Rf. The\nresults are obtained by log-averaging over single realizations.\n23\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n88\n10−4\n10−2\n10−2\n4 × 10−3\n6 × 10−3\n2 × 10−2\nϵt\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nmnist\n10−1\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\n10−3\n10−1\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\n10−3\n10−2\n10−1\n3 × 10−2\n4 × 10−2\n6 × 10−2\n2 × 10−1\nϵt\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34 ResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nsvhn\n2 × 10−1\n3 × 10−1\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34 ResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\n10−3\n10−2\n10−1\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34ResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\n10−4\n10−3\n10−2\n10−1\n7 × 10−2\n8 × 10−2\n9 × 10−2\nϵt\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nfashionmnist\n10−2\n10−1\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\n10−2\n10−1\n100\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\n10−1\n8 × 10−2 9 × 10−2\nDf\n4 × 10−1\n5 × 10−1\n6 × 10−1\nϵt\nResNet18\nEﬃcientNetB0\nResNet34\nResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\ntiny-imagenet32\n100\n5 × 10−1\n6 × 10−1\n7 × 10−1\n8 × 10−1\n9 × 10−1\nGf\nResNet18\nEﬃcientNetB0\nResNet34\nResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\n10−1\n2 × 10−1\nRf\nResNet18\nEﬃcientNetB0\nResNet34\nResNet50\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nFigure 20: Test error ϵt vs. Df, Gf and Rf (on the columns) for different data sets (on the\nrows). The corresponding correlation coefﬁcients are shown in Table 2. Lines 1-2: MNIST and\nSVHN both contain images of digits and show a similar ϵt(Rf). Line 3: FashionMNIST results are\ncomparable to the CIFAR10 ones shown in the main text. Line 4: Tiny ImageNet32 is a re-scaled\n(32x32 pixels) version of ImageNet with 200 classes and 100’000 training points. The task is harder\nthan the other data sets and is such that we could not train simple networks (FC, LeNet) on it – i.e.\nthe loss stays O(1) throughout training – so these are not reported here.\nTable 2: Test error vs. stability: correlation coefﬁcients for different data sets.\ndata-set\nDf\nGf\nRf\nMNIST\n0.71\n-0.43\n0.75\nSVHN\n0.87\n-0.28\n0.81\nFashionMNIST\n0.72\n-0.68\n0.94\nTiny ImageNet\n0.69\n-0.66\n0.74\n24\n89\n10−1\nϵt\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 0.5, c = 3, r = 0.96\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 0.5, c = 5, r = 0.96\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 0.5, c = 15, r = 0.96\nGaussian interp.\nLinear interp.\n10−1\nϵt\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 1.0, c = 3, r = 0.97\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 1.0, c = 5, r = 0.97\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 1.0, c = 15, r = 0.96\nGaussian interp.\nLinear interp.\n10−1\n100\n101\nRf\n10−1\nϵt\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 2.0, c = 3, r = 0.96\n10−1\n100\n101\nRf\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 2.0, c = 5, r = 0.93\n10−1\n100\n101\nRf\nDenseNetL4\nLeNet\nResNet18\nEﬃcientNetB0\nDenseNetL2\nDenseNetL6\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nδ = 2.0, c = 15, r = 0.95\nGaussian interp.\nLinear interp.\nFigure 21: Test error ϵt vs. Df, Gf and Rf for CIFAR10 and varying δ and cut-off c. Titles\nreport the values of the varying parameters together with corr. coeffs. Parameters corresponding to\nallowed diffeo are indicated by the green background. Red and blue colors correspond to different\ninterpolation methods. Overall, results are robust when varying these parameters.\n25\nChapter 3. Deformation Invariance Strongly Correlates to Performance in Image Tasks\n90\n102\n103\n104\nP\n10−3\n10−1\n101\nResNet18\nMNIST\nGf\nDf\nRf\n102\n103\n104\nP\n10−3\n10−2\n10−1\n100\nF-MNIST\n102\n103\n104\nP\n10−3\n10−2\n10−1\n100\nCIFAR10\n102\n103\n104\nP\n10−2\n10−1\n100\nEﬃcientNetB0\n102\n103\n104\nP\n10−2\n10−1\n100\n102\n103\n104\nP\n10−2\n10−1\n100\n101\nFigure 22: Stability toward Gaussian noise (Gf) and diffeomorphisms (Df) alone, and the\nrelative stability Rf with the relative errors. Analogous to Fig.6 in which error estimates are\nomitted to favour clarity. Here we ﬁx the cut-off to c = 3 and show error estimates instead. Columns\ncorrespond to different data-sets (MNIST, FashionMNIST and CIFAR10) and rows to architectures\n(ResNet18 and EfﬁcientNetB0). Each panel reports Gf (blue), Df (orange) and Rf (green) as a\nfunction of P and for different cut-off values c, as indicated in the legend. Statistics: Each point\nin the graphs is obtained by training 16 differently initialized networks on 16 different subsets of\nthe data-sets; each network is then probed with 500 test samples in order to measure stability to\ndiffeomorphisms and Gaussian noise. The resulting Rf is obtained by log-averaging the results from\nsingle realizations. As we are plotting quantities in log scale, we report the relative error (shaded).\n26\n91\n4 When Feature Learning Fails: Defor-\nmation Invariance deteriorates with\ntraining in Fully-Connected Networks\nThe following paper is the preprint version of Petrini et al. (2022) published in Advances in\nNeural Information Processing Systems.\nCandidate contributions\nThe candidate contributed to all discussions and led the experi-\nmental part of the paper. F Cagnetta led the theoretical part.\n93\nLearning sparse features can lead to overﬁtting in\nneural networks\nLeonardo Petrini ∗\nInstitute of Physics\nÉcole Polytechnique Fédérale de Lausanne\nleonardo.petrini@epfl.ch\nFrancesco Cagnetta ∗\nInstitute of Physics\nÉcole Polytechnique Fédérale de Lausanne\nfrancesco.cagnetta@epfl.ch\nEric Vanden-Eijnden\nCourant Institute of Mathematical Sciences\nNew York University\neve2@cims.nyu.edu\nMatthieu Wyart\nInstitute of Physics\nÉcole Polytechnique Fédérale de Lausanne\nmatthieu.wyart@epfl.ch\nAbstract\nIt is widely believed that the success of deep networks lies in their ability to learn a\nmeaningful representation of the features of the data. Yet, understanding when and\nhow this feature learning improves performance remains a challenge: for example,\nit is beneﬁcial for modern architectures trained to classify images, whereas it is\ndetrimental for fully-connected networks trained on the same data. Here we propose\nan explanation for this puzzle, by showing that feature learning can perform worse\nthan lazy training (via random feature kernel or the NTK) as the former can lead\nto a sparser neural representation. Although sparsity is known to be essential for\nlearning anisotropic data, it is detrimental when the target function is constant or\nsmooth along certain directions of input space. We illustrate this phenomenon in\ntwo settings: (i) regression of Gaussian random functions on the d-dimensional unit\nsphere and (ii) classiﬁcation of benchmark datasets of images. For (i), we compute\nthe scaling of the generalization error with the number of training points and show\nthat methods that do not learn features generalize better, even when the dimension\nof the input space is large. For (ii), we show empirically that learning features\ncan indeed lead to sparse and thereby less smooth representations of the image\npredictors. This fact is plausibly responsible for deteriorating the performance,\nwhich is known to be correlated with smoothness along diffeomorphisms.\n1\nIntroduction\nNeural networks are responsible for a technological revolution in a variety of machine learning tasks.\nMany such tasks require learning functions of high-dimensional inputs from a ﬁnite set of examples,\nthus should be generically hard due to the curse of dimensionality [1, 2]: the exponent that controls\nthe scaling of the generalization error with the number of training examples is inversely proportional\nto the input dimension d. For instance, for standard image classiﬁcation tasks with d ranging in\n103 ÷ 105, such exponent should be practically vanishing, contrary to what is observed in practice [3].\nIn this respect, understanding the success of neural networks is still an open question. A popular\nexplanation is that, during training, neurons adapt to features in the data that are relevant for the task\n[4], effectively reducing the input dimension and making the problem tractable [5, 6, 7]. However,\nunderstanding quantitatively if this intuition is true and how it depends on the structure of the task\nremains a challenge.\n∗Equal contribution (a coin was ﬂipped).\nPreprint. Under review.\narXiv:2206.12314v2  [stat.ML]  12 Oct 2022\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n94\nOverﬁtting in Feature Learning\n102\n103\n104\ntrainset size, n\n10−1\ntest error, ϵ\nMNIST\nMNIST\nFeature\nLazy\n102\n103\n104\ntrainset size, n\n10−1\n3 × 10−2\n4 × 10−2\n6 × 10−2\nF-MNIST\nF-MNIST\nFeature\nLazy\n102\n103\n104\ntrainset size, n\n3 × 10−1\n4 × 10−1\nCIFAR10\nCIFAR10\nFeature\nLazy\nFigure 1: Feature vs. Lazy in image classiﬁcation. Generalization error as a function of the training-\nset size n for inﬁnite-width fully-connected networks (FCNs) trained in the feature (blue) and lazy\nregime (orange). In the latter case the limit is taken exactly by training an SVC algorithm with the\nanalytical NTK [23]. In the former case, the inﬁnite-width limit can be accurately approximated for\nthese datasets by considering very wide nets (H = 103), and performing ensemble averaging on\ndifferent initial conditions of the parameters as shown in [24, 25]. Panels correspond to different\nbenchmark image datasets [26, 27, 28]. Results are averaged over 10 different initializations of the\nnetworks and datasets.\nRecently much progress was made in characterizing the conditions which lead to features learning, in\nthe overparameterized setting where networks generally perform best. When the initialization scale\nof the network parameters is large [8] one encounters the lazy training regime, where neural networks\nbehave as kernel methods [9, 10] (coined Neural Tangent Kernel or NTK) and features are not learned.\nBy contrast, when the initialization scale is small, a feature learning regime is found [11, 12, 13]\nwhere the network parameters evolve signiﬁcantly during training. This limit is much less understood\napart from very simple architectures, where it can be shown to lead to sparse representations where\na limited number of neurons are active after training [14]. Such sparse representations can also be\nobtained by regularizing the weights during training [2, 15].\nIn terms of performance, most theoretical works have focused on fully-connected networks. For these\narchitectures, feature learning was shown to signiﬁcantly outperform lazy training [16, 17, 18, 19, 11]\nfor certain tasks, including approximating a function which depends only on a subset or a linear\ncombination of the input variables. However, when such primitive networks are trained on image\ndatasets, learning features is detrimental [20, 21], as illustrated in Fig. 1 (see [19, Fig. 3] for the\nanalogous plot in the case of a target function depending on just one of the input variables, where\nlearning features is beneﬁcial). A similar result was observed in simple models of data [22]. These\nfacts are unexplained, yet central to understanding the implicit bias of the feature learning regime.\n1.1\nOur contribution\nOur main contribution is to provide an account of the drawbacks of learning sparse representations\nbased on the following set of ideas. Consider, for concreteness, an image classiﬁcation problem: (i)\nimages class varies little along smooth deformations of the image; (ii) due to that, tasks like image\nclassiﬁcation require a continuous distribution of neurons to be represented; (iii) thus, requiring\nsparsity can be detrimental for performance. We build our argument as follows.\n• In order to ﬁnd a quantitative description of the phenomenon, we start from the problem of\nregression of a random target function of controlled smoothness on the d-dimensional unit\nsphere, and study the property of the minimizers of the empirical loss with n observations,\nboth in the lazy and the feature learning regimes. More speciﬁcally, we consider two extreme\nlimits—the NTK limit and mean-ﬁeld limit—as representatives of lazy and feature regimes,\nrespectively (section 2). Both these limits admit a simple formulation that allows us to\npredict generalization performances. In particular, our results on feature learning rely on\nsolutions having an atomic support. This property can be justiﬁed for one-hidden-layer\nneural networks with ReLU activations and weight decay. Yet, we also ﬁnd such a sparsity\nempirically using gradient descent in the absence of regularization, if weights are initialized\nto be small enough.\n• We ﬁnd that lazy training leads to smoother predictors than feature learning. As a result, lazy\ntraining outperforms feature learning when the target function is also sufﬁciently smooth.\nOtherwise, the performances of the two methods are comparable, in the sense that they\ndisplay the same asymptotic decay of generalization error with the number of training\n2\n95\nOverﬁtting in Feature Learning\nexamples. Our predictions are obtained from asymptotic arguments that we systematically\nback up with numerical studies.\n• For image datasets, it is believed that diffeomorphisms of images are key transformations\nalong which the predictor function should only mildly vary to obtain good performance [29].\nFrom the results above, a natural explanation as to why lazy beats feature for fully connected\nnetworks is that it leads to predictors with smaller variations along diffeomorphisms. We\nconﬁrm that this is indeed the case empirically on benchmark datasets.\nNumerical experiments are performed in PyTorch [30], and the code for reproducing experiments is\navailable online at github.com/pcsl-epﬂ/regressionsphere.\n1.2\nRelated Work\nThe property that training ReLU networks in the feature regime leads to a sparse representation was\nobserved empirically [31]. This property can be justiﬁed for one-hidden-layer networks by casting\ntraining as a L1 minimization problem [32, 2], then using a representer theorem [33, 15, 34]. This is\nanalogous to what is commonly done in predictive sparse coding [35, 36, 37, 38].\nMany works have investigated the beneﬁts of learning sparse representations in neural networks.\n[2, 16, 17, 18, 19, 39, 40] study cases in which the true function only depends on a linear subspace of\ninput space, and show that feature learning proﬁtably capture such property. Even for more general\nproblems, sparse representations of the data might emerge naturally during deep network training—a\nphenomenon coined neural collapse [41]. Similar sparsiﬁcation phenomena, for instance, have been\nfound to allow for learning convolutional layers from scratch [42, 43]. Our work builds on this body\nof literature by pointing out that learning sparse features can be detrimental, if the task does not allow\nfor it.\nThere is currently no general framework to predict rigorously the learning curve exponent β deﬁned\nas ϵ(n) = O(n−β) for kernels. Some of our asymptotic arguments can be obtained by other\napproximations, such as assuming that data points lie on a lattice in Rd [44], or by using the non-\nrigorous replica method of statistical physics [45, 46, 47]. In the case d = 2, we provide a more\nexplicit mathematical formulation of our results, which leads to analytical results for certain kernels.\nWe systematically back up our predictions with numerical tests as d varies.\nFinally, in the context of image classiﬁcation, the connection between performance and ‘stability’ or\nsmoothness toward small diffeomorphisms of the inputs has been conjectured by [29, 48]. Empirically,\na strong correlation between these two quantities was shown to hold across various architectures for\nreal datasets [49]. In that reference, it was found that fully connected networks lose their stability\nover training: here we show that this effect is much less pronounced in the lazy regime.\n2\nProblem and notation\nTask\nWe consider a supervised learning scenario with n training points {xi}n\ni=1 uniformly drawn\non the d-dimensional unit sphere Sd−1. We assume that the target function f ∗is an isotropic Gaussian\nrandom process on Sd−1 and control its statistics via the spectrum: by introducing the decomposition\nof f ∗into spherical harmonics (see App. A for deﬁnitions),\nf ∗(x) =\nX\nk≥0\nNk,d\nX\nℓ=1\nf ∗\nk,ℓYk,ℓ(x)\nwith\nE\n\u0002\nf ∗\nk,ℓ\n\u0003\n= 0,\nE\n\u0002\nf ∗\nk,ℓf ∗\nk′,ℓ′\n\u0003\n= ckδk,k′δℓ,ℓ′.\n(2.1)\nWe assume that all the ck with k odd vanish apart from c1: this is required to guarantee that f ∗can be\napproximated as well as desired with a one-hidden-layer ReLU network with no biases, as discussed\nin App. A. We also assume that the non-zero ck decay as a power of k for k ≫1, ck ∼k−2νt−(d−1).\nThe exponent νt > 0 controls the (weak) differentiability of f ∗on the sphere (see App. A) and also\nthe statistics of f ∗in real space:\nE\n\u0002\n|f ∗(x) −f ∗(y)|2\u0003\n= O\n\u0000|x −y|2νt\u0001\n= O ((1 −x · y)νt)\nas x →y.\n(2.2)\nExamples of such a target function for d = 3 and different values of νt are reported in Fig. 2.\n3\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n96\nOverﬁtting in Feature Learning\nd = 3, νt = 1/2\nd = 3, νt = 4\n(a):\n(b):\nf ∗(x)\nx 1\nx 2\nx 3\n0\n+1\n-1\nFigure 2: Gaussian random process on the sphere. We show here two samples of the task intro-\nduced in section 2 when the target function f ∗(x) is deﬁned on the 3−dimensional unit sphere. (a)\nand (b) show samples of large and small smoothness coefﬁcient νt, respectively.\nNeural network representation in the feature regime\nIn this regime we aim to approximate the\ntarget function f ∗(x) via a one-hidden-layer neural network of width H,\nfH(x) = 1\nH\nH\nX\nh=1\nwhσ(θh · x),\n(2.3)\nwhere {θh}H\nh=1 (the features) and {wh}H\nh=1 (the weights) are the network parameters to be optimized,\nand σ(x) denotes the ReLU function, σ(x) = max {0, x}. If we assume that {θh, wh}H\nh=1 are\nindependently drawn from a probability measure µ on Sd−1 × R such that the Radon measure\nγ =\nR\nR wµ(·, dw) exists, then as H →∞,\nlim\nH→∞fH(x) =\nZ\nSd−1 σ(θ · x)dγ(θ)\na.e. on Sd−1.\n(2.4)\nThis is the so-called mean-ﬁeld limit [11, 12], and it is then natural to determine the optimal γ via\nγ∗= arg min\nγ\nZ\nSd−1|dγ(θ)|\nsubject to:\nZ\nSd−1 σ(θ · xi)dγ(θ)=f ∗(xi)\n∀i = 1, . . . , n. (2.5)\nIn practice, we can approximate this minimization problem by using a network with large but ﬁnite\nwidth, constraining the feature to be on the sphere |θh| = 1, and minimizing the following empirical\nloss with L1 regularization on the weights,\nmin\n{wh,θh}H\nh=1\n|θh|=1\n1\n2n\nn\nX\ni=1\n \nf ∗(xi) −1\nH\nH\nX\nh=1\nwhσ(θh · xi)\n!2\n+ λ\nH\nH\nX\nh=1\n|wh|.\n(2.6)\nThis minimization problem leads to (2.5) when H →∞and λ →0. Note that, by homogeneity\nof ReLU, (2.6) can be shown to be equivalent to imposing a regularization on the L2 norm of all\nparameters [32, Thm. 10], i.e. the usual weight decay.\nTo proceed we will make the following assumption about the minimizer γ∗:\nAssumption 1. The minimizer γ∗of (2.5) is unique and atomic, with nA ≤n atoms, i.e. there exists\n{w∗\ni , θ∗\ni }nA\ni=1 such that\nγ∗=\nnA\nX\ni=1\nw∗\ni δθ∗\ni .\n(2.7)\nThe main component of the assumption is the uniqueness of γ∗; if it holds the sparsity of γ∗follows\nfrom the representer theorem, see e.g. [33]. Both the uniqueness and sparsity of the minimizer can be\njustiﬁed as holding generically using asymptotic arguments involving recasting the L1 minimization\nproblem 2.5 as a linear programming one: these arguments are standard (see e.g. [50]) and are\npresented in App. B for the reader convenience. In our arguments below to deduce the scaling of\nthe generalization error we will mainly use that nA = O(n)—we shall conﬁrm this fact numerically\neven in the absence of regularization, if the weights are initialized to be small enough. Notice that\nfrom Assumption 1 it follows that the predictor in the feature regime corresponding to the minimizer\nγ∗takes the following form\nf FEATURE(x) =\nnA\nX\ni=1\nw∗\ni σ(θ∗\ni · x).\n(2.8)\n4\n97\nOverﬁtting in Feature Learning\nNeural network representation in the lazy regime.\nIn this regime we approximate the target\nfunction f ∗(x) via\nf NTK(x) =\nn\nX\ni=1\ngiKNTK(xi · x),\n(2.9)\nwhere the weights {gi}n\ni=1 solve\nf ∗(xj) =\nn\nX\ni=1\ngiKNTK(xi · xj),\nj = 1, . . . , n.\n(2.10)\nand KNTK(x · y) is the Neural Tangent Kernel (NTK) [9]\nKNTK(x · y) =\nZ\nSd−1×R\n\u0000σ(θ · x)σ(θ · y) + w2 x · y σ′(θ · x)σ′(θ · y)\n\u0001\ndµ0(θ, w).\n(2.11)\nHere µ0 is a ﬁxed probability distribution which, in the NTK training regime [9], is the distribution\nof the features and weights at initialization. It is well-known [51] that the solution to kernel ridge\nregression problem can also be expressed via the kernel trick as\nf NTK(x) =\nZ\nSd−1×R\n(gw(θ, w)σ(θ · x) + wx · gθ(θ, w)σ′(θ · x)) dµ0(θ, w)\n(2.12)\nwhere gθ and gw are the solutions of\nmin\ngw,gθ\nZ\nSd−1×R\n\u0000g2\nw(w, θ) + |gθ(w, θ)|2\u0001\ndµ0(θ, w)\nsubject to:\nZ\nSd−1×R\n(gw(w, θ)σ(θ · xi) + wxi · gθ(w, θ)σ′(θ · xi)) dµ0(θ, w) = f ∗(xi)\n∀i = 1, . . . , n.\n(2.13)\nAnother lazy limit can be obtained equivalently by training only the weights while keeping the\nfeatures to their initialization value. This is equivalent to forcing gθ(θ, w) to vanish in Eq. 2.13,\nresulting again in a kernel method. The kernel, in this case, is called Random Feature Kernel (KRFK),\nand can be obtained from Eq. 2.11 by setting dµ0(θ, w) = δw=0d˜µ0(θ). The minimizer can then be\nwritten as in Eq. 2.9 with KNTK replaced by KRFK.\n3\nAsymptotic analysis of generalization\nIn this section, we characterize the asymptotic decay of the generalization error ϵ(n) averaged over\nseveral realizations of the target function f ∗. Denoting with dτ d−1(x) the uniform measure on Sd−1,\nϵ(n) = Ef ∗\n\u0014Z\ndτ d−1(x) (f n(x) −f ∗(x))2\n\u0015\n= Adn−β + o(n−β),\n(3.1)\nfor some constant Ad which might depend on d but not on n. Both for the lazy (see Eq. 2.9) and\nfeature regimes (see Eq. 2.8) the predictor can be written as a sum of O(n) terms:\nf n(x) =\nO(n)\nX\nj=1\ngjϕ(x · yj) :=\nZ\nSd−1 gn(y)ϕ(x · y)dτ(y).\n(3.2)\nIn the feature regime, the gj’s (yj) coincide with the optimal weights w∗\nj (features θ∗\nj ), ϕ with\nthe activation function σ. In the lazy regime, the yj are the training points xj, ϕ is the neural\ntangent or random feature kernel the gj’s are the weights solving Eq. 2. We have deﬁned the density\ngn(x) = P\nj |Sd−1|gjδ(x −yj) so as to cast the predictor as a convolution on the sphere. Therefore,\nthe projections of f n onto spherical harmonics Yk,ℓread f n\nk,ℓ= gn\nk,ℓϕk, where gn\nk,ℓis the projection\nof gn(x) and ϕk that of ϕ(x · y). For ReLU neurons one has (as shown in App. A)\nϕLAZY\nk\n∼k−(d−1)−2ν\nwith ν = 1/2 (NTK), 3/2 (RFK),\nϕFEATURE\nk\n∼k−d−1\n2\n−3/2.\n(3.3)\n5\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n98\nOverﬁtting in Feature Learning\n0\nπ/2\nπ\nangle\n0.7\n0.8\n0.9\n1.0\n1.1\npredictor\nlazy\npredictor\ntraining points\n0\nπ/2\nπ\nangle\nfeature\npredictor\ntraining points\nFigure 3: Feature vs. Lazy Predictor. Predictor of the lazy (left) and feature (right) regime when\nlearning the constant function on the ring with 8 uniformly-sampled training points.\nMain Result\nConsider a target function f ∗with smoothness exponent νt as deﬁned above, with\ndata lying on Sd−1. If f ∗is learnt with a one-hidden-layer network with ReLU neurons in the regimes\nspeciﬁed above, then the generalization error follows ϵ(n) ∼n−β with:\nβLAZY = min {2(d −1) + 4ν, 2νt}\nd −1\nwith ν =\n\u001a1/2 for NTK,\n3/2 for RFK, ,\n(3.4a)\nβFEATURE = min {(d −1) + 3, 2νt}\nd −1\n.\n(3.4b)\nThis is our central result. It implies that if the target function is a smooth isotropic Gaussian ﬁeld\n(realized for large νt), then lazy beats feature, in the sense that training the network in the lazy regime\nleads to a better scaling of the generalization performance with the number of training points.\nStrategy\nThere is no general framework for a rigorous derivation of the generalization error in the\nridgeless limit λ →0: predictions such as that of Eq. 3.4 can be obtained by either assuming that\ntraining points (for Eq. 3.4a) and neurons (for Eq. 3.4b) lie on a periodic lattice [44], or (for Eq. 3.4a)\nusing the replica method from physics [45] as shown in App. F. Here we follow a different route, by\nﬁrst characterizing the form of the predictor for d = 2 (proof in App. C). This property alone allows\nus to determine the asymptotic scaling of the generalization error. We use it to analytically obtain the\ngeneralization error in the NTK case with a slightly simpliﬁed function ϕ (details in App. D). This\ncalculation motivates a simple ansatz for the form of gn(x) entering Eq. 3.2 and its projections onto\nspherical harmonics, which extends naturally to arbitrary dimension. We conﬁrm the predictions\nresulting from this ansatz systematically in numerical experiments.\nProperties of the predictor in d = 2\nOn the unit circle S1 all points are identiﬁed by a polar angle\nx ∈[0, 2π). Hence both target function and estimated predictor are functions of the angle, and all\nfunctions of a scalar product are in fact functions of the difference in angle. In particular, introducing\n˜ϕ(x) = ϕ(cos(x)),\nf n(x) =\nX\nj\ngj ˜ϕ(x −xj) ≡\nZ 2π\n0\ndy\n2π gn(y) ˜ϕ(x −y),\n(3.5)\nwhere we deﬁned\ngn(x) =\nn\nX\nj=1\n(2πgj)δ(y −xj).\n(3.6)\nBoth for feature regime and NTK limit, the ﬁrst derivative of ˜ϕ(x) is continuous except for two\nvalues of x (0 and π for lazy, −π/2 and π/2 for feature), so that ˜ϕ(x)′′ has a singular part consisting\nof two Dirac delta functions.\nAs a result, the second derivative of the predictor (f n)′′ has a singular part consisting of many Dirac\ndeltas. If we denote with (f n)′′\nr the regular part, obtained by subtracting all the delta functions, we\ncan show that (see App. C):\nProposition 1. (informal) As n →∞, (f n)′′\nr converges to a function having ﬁnite second moment,\ni.e.\nlim\nn→∞Ef ∗[(f n)′′\nr(x)]2 = const. < ∞.\n(3.7)\n6\n99\nOverﬁtting in Feature Learning\nIn the large n limit, the predictor displays a singular second derivative at O(n) points. Proposition 1\nimplies that outside of these singular points the second derivative is well deﬁned. Thus, as n gets\nlarge and the singular points approach each other, the predictor can be approximated by a chain of\nparabolas, as highlighted in Fig. 3 and noticed in [47] for a Laplace kernel. This property alone\nallows to determine the asymptotic scaling of the error in d = 2. In simple terms, Prop. 1 follows from\nthe convergence of gn to the function satisfying f ∗(x) =\nR dy\n2πg(y) ˜ϕr(x −y), which is guaranteed\nunder our assumptions on the target function—a detailed proof is given in App. C.\nDecay of the error in d = 2 (sketch)\nThe full calculation is in App. D. Consider a slightly\nsimpliﬁed problem where ˜ϕ has a single discontinuity in its derivative, located at x = 0. In this case,\nf n(x) is singular if and only if x is a data point. Consider then the interval x ∈[xi, xi+1] and set\nδi = xi+1 −xi, xi+1/2 = (xi+1 + xi)/2. If the target function is smooth enough (νt > 2), then a\nTaylor expansion implies |f ∗(xi+1/2) −f n(xi+1/2)| ∼δ2\ni . Since the distances δi between adjacent\nsingular points are random variables with mean of order 1/n and ﬁnite moments, it is straightforward\nto obtain that ϵ(n) ∼P\ni(f ∗(xi+1/2) −f n(xi+1/2))2 ∼P\ni δ4\ni ∼n−4. By contrast if f ∗is not\nsufﬁciently smooth (νt ≤2), then |f ∗(xi+1/2) −f n(xi+1/2)| ∼δ2νt\ni\n, leading to ϵ(n) ∼n−2νt. Note\nthat for this asymptotic argument to apply to the feature learning regime, one must ensure that the\ndistribution of the rescaled distance between adjacent singularities nδi has a ﬁnite fourth moment.\nThis is obvious in the lazy regime, where the δi’s are controlled by the position of the training points,\nbut not in the feature regime, where the distribution of singular points is determined by that of the\nneuron’s features. Nevertheless, we show that it must be the case in our setup in App. D.\nInterpretation in terms of spectral bias\nFrom the discussion above it is evident that there is a\nlength scale δ of order 1/n such that f n(x) is a good approximation of f ∗(x) over scales larger\nthan δ. In terms of Fourier modes2, one has: i) c\nf n(k) matches c\nf n(k) at long wavelengths, i.e. for\nk ≪kc ∼1/n. ii) In addition, since the phases exp(ikxj) become effectively random phases for\nk ≫kc, c\ngn(k) = P\nj gj exp(ikxj) becomes a Gaussian random variable with zero mean and ﬁxed\nvariance and thus iii) c\nf n(k) = c\ngn(k)b˜ϕ(k) decorrelates from f ∗for k ≫kc. Therefore\nϵ(n) ∼\nX\n|k|>kc\nEf ∗\n\u0014\u0010\nc\ngn(k)b˜ϕ(k) −c\nf n(k)\n\u00112\u0015\n∼\nX\n|k|≥kc\nEf ∗\u0002\n(c\ngn(k))2\u0003 b˜ϕ(k)2 + Ef ∗\nh\n(c\nf n(k))2i\n.\n(3.8)\nFor νt > 2, one has P\nj g2\nj ∼n−1 limn→∞\nR\ngn(x)2dx ∼n−1. It follows (see App. E for details)\nthat the sum is dominated by the ﬁrst term, hence entirely controlled by the Fourier coefﬁcients of\nc\nf n(k) at large k. A smoother predictor corresponds to a faster decay of c\nf n(k) with k, thus a faster\ndecay of the error with n. Plugging the relevant decays yields ϵ ∼n−4 for feature regime and lazy\nregime with the NTK, and n−6 for lazy regime with the RFK (which is smoother than the NTK). For\nνt ≤2, the two terms have comparable magnitude (see App. E), thus ϵ ∼n−2νt.\nGeneralization to higher dimensions\nThe argument above can be generalized for any d by replac-\ning Fourier modes with projections onto spherical harmonics. The characteristic distance between\ntraining points scales as n−1/(d−1), thus kc ∼n−1/(d−1). Our ansatz is that, as in d = 2: i) for\nk ≪kc, the predictor modes coincide with those of the target function, f n\nk,l ≈f ∗\nk,l (this corresponds\nto the spectral bias result of kernel methods, stating that the predictor reproduces the ﬁrst O(n)\nprojections of the target in the kernel eigenbasis [45]); ii) For k ≫kc, gn\nk,l is a sum of uncorrelated\nterms, thus a Gaussian variable with zero mean and ﬁxed variance; iii) f n\nk,ℓ= gn\nk,ℓ˜ϕk decorrelates\nfrom f ∗\nk,ℓfor k ≫kc. i), ii) and iii) imply that:\nϵ(n) ∼\nX\nk≥kc\nNk,d\nX\nl=1\nEf ∗\nh\u0000f n\nk,l −f ∗\nk,l\n\u00012i\n∼\nX\nk≥kc\nNk,d\nX\nl=1\nEf ∗\u0002\n(gn\nk,l)2\u0003\nϕ2\nk + k−2νt−(d−1).\n(3.9)\nAs shown in App. E, from this expression it is straightforward to obtain Eq. 3.4. Notice again that\nwhen the target is sufﬁciently smooth so that the predictor-dependent term dominates, the error is\ndetermined by the smoothness of the predictor. In particular, as d > 2, the predictor of feature learning\nis less smooth than both the NTK and RFK ones, due to the slower decay of the corresponding ϕk.\n2The Fourier transform of a function f(x) is indicated by the hat, bf(k).\n7\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n100\nOverﬁtting in Feature Learning\n4\nNumerical tests of the theory\nWe test successfully our predictions by computing the learning curves of both lazy and feature\nregimes when (i) the target function is constant on the sphere for varying d, see Fig. 4, and (ii)\nthe target is a Gaussian random ﬁeld with varying smoothness νt, as shown in Fig. G.1 of App. G.\nFor the lazy regime, we perform kernel regression using the analytical expression of the NTK [52]\n(see also Eq. A.19). For the feature regime, we ﬁnd that our predictions hold when having a small\nregularization, although it takes unreachable times for gradient descent to exactly recover the minimal-\nnorm solution—a more in-depth discussion can be found in App. G. An example of the atomic\ndistribution of neurons found after training, which contrasts with the initial distribution, is displayed\nin Fig. 5a, left panel.\nAnother way to obtain sparse features is to initialize the network with very small weights [14], as\nproposed in [8]. As in the presence of an inﬁnitesimal weights decay, this scheme also leads to\nsparse solutions with nA = O(n) – an asymptotic dependence conﬁrmed in Fig. G.3 of App. G. This\nobservation implies that our predictions must apply in that case too, as we conﬁrm in Fig. G.3.\n101\n102\n103\ntrainset size, n\n10−10\n10−8\n10−6\n10−4\n10−2\ntest error, ϵ ∝n−β\nβF = 4.0\nβL = 4.0\nd = 2\nFeat., t = 106\nFeat., t = 3 · 106\nFeat., t = 107\nLazy\n101\n102\n103\ntrainset size, n\n10−7\n10−5\n10−3\n10−1\nβF = 2.5\nβL = 3.0\nd = 3\nFeature\nLazy\nExperiments\nTheory\n101\n102\n103\ntrainset size, n\n10−6\n10−4\n10−2\nβF = 1.75\nβL = 2.5\nd = 5\nFeature\nLazy\nExperiments\nTheory\nFigure 4: Generalization error for a constant function f ∗(x) = 1. Generalization error as a\nfunction of the training set size n for a network trained in the feature regime with L1 regularization\n(blue) and kernel regression corresponding to the inﬁnite-width lazy regime (orange). Numerical\nresults (full lines) and the exponents predicted by the theory (dashed) are plotted. Panels correspond\nto different input-space dimensions (d = 2, 3, 5). Results are averaged over 10 different initializations\nof the networks and datasets. For d = 2 and large n, the gap between experiments and prediction for\nthe feature regime is due to the ﬁnite training time t. Indeed our predictions become more accurate as\nt increases, as illustrated in the left.\n5\nEvidence for overﬁtting along diffeomorphisms in image datasets\nFor fully-connected networks, the feature regime is well-adapted to learn anisotropic tasks [16]: if\nthe target function does not depend on a certain linear subspace of input space, e.g. the pixels at\nthe corner of an image, then neurons align perpendicularly to these directions [19]. By contrast, our\nresults highlight a drawback of this regime when the target function is constant or smooth along\ndirections in input space that require a continuous distribution of neurons to be represented. In\nsuch a case, the adaptation of the weights to the training points leads to a predictor with a sparse\nrepresentation. Such a predictor would be less smooth than in the lazy regime and thus underperform.\nDoes this view hold for images, and explain why learning their features is detrimental for fully-\nconnected networks? The ﬁrst positive empirical evidence is that the neurons’ distribution of networks\ntrained on image data becomes indeed sparse in the feature regime, as illustrated in Fig. 5a, right,\nfor CIFAR10 [28]. This observation raises the question of which are the directions in input space\ni) along which the target should vary smoothly, and ii) that are not easily represented by a discrete\nset of neurons. An example of such directions are global translations, which conserve the norm of\nthe input and do not change the image class: the lazy regime predictor is indeed smoother than the\nfeature one with respect to translations of the input (see App. H). Yet, these transformations live in a\nspace of dimension 2, which is small in comparison with the full dimensionality d of the data and\nthus may play a negligible role.\nA much larger class of transformations believed to have little effect on the target are small diffeomor-\nphisms [29]. A diffeomorphism τ acting on an image is illustrated in Fig. 5b, which highlights that\n8\n101\nOverﬁtting in Feature Learning\n−1.0\n−0.5\n0.0\n0.5\n1.0\nθ1\n−1.0\n−0.5\n0.0\n0.5\n1.0\nθ2\nSpherical Task\ndata-points\nFeature\nLazy\n−4\n−2\n0\n2\n4\nθPC1\n−4\n−2\n0\n2\n4\nθPC2\nCIFAR10\nFeature\nLazy\n(a) Features sparsiﬁcation. 1stPanel: Distribution of neuron’s feature for the task\nof learning a constant function on the sphere in 2D. Arrows represent a subset of the\nnetwork features {θh}H\nh=1 after training in the lazy and feature regimes. Training is\nperformed on n = 8 data-points (black dots). 2ndPanel: FCN trained on CIFAR10.\nOn the axes the ﬁrst two principal components of the features {θh}H\nh=1 after training\non n = 32 points in the feature (blue) and lazy (orange) regimes. Similarly to what\nis observed when learning a constant function, the θh angular distribution becomes\nsparse with training in the feature regime.\nτ\n(b) Example of diffeo-\nmorphism. Sample of\na max-entropy deforma-\ntion τ [49] when applied\nto a natural image, illus-\ntrating that it does not\nchange the image class\nfor the human brain.\nFigure 5: Features sparsiﬁcation and example of a diffeomorphism.\nour brain still perceives the content of the transformed image as in the original one. Near-invariance\nof the task to these transformations is believed to play a key role in the success of deep learning,\nand in explaining how neural networks beat the curse of dimensionality [48]. Indeed, if modern\narchitectures can become insensitive to these transformations, then the dimensionality of the problem\nis considerably reduced. In fact, it was found that the architectures displaying the best performance\nare precisely those which learn to vary smoothly along such transformations [49].\nSmall diffeomorphisms are likely the directions we are looking for. To test this hypothesis, follow-\ning [49], we characterize the smoothness of a function along such diffeomorphisms, relative to that\nof random directions in input space. Speciﬁcally, we use the relative sensitivity:\nRf =\nEx,τ∥f(τx) −f(x)∥2\nEx,η∥f(x + η) −f(x)∥2 .\n(5.1)\nIn the numerator, the average is made over the test set and over an ensemble of diffeomorphisms,\nreviewed in App. I. The magnitude of the diffeomorphisms is chosen so that each pixel is shifted\nby one on average. In the denominator, the average runs over the test set and the vectors η sampled\nuniformly on the sphere of radius ∥η∥= Ex,τ∥τx −x∥, and this ﬁxes the transformations magnitude.\nWe measure Rf as a function of n for three benchmark datasets of images, as shown in Fig. 6.\nWe indeed ﬁnd that Rf is consistently smaller in the lazy training regime, where features are not\nlearned. Overall, this observation supports the view that learning sparse features is detrimental\nwhen data present (near) invariance to transformations that cannot be represented sparsely by the\narchitecture considered. Fig. 1 supports the idea that—for benchmark image datasets—this negative\neffect overcomes well-known positive effects of learning features, e.g. becoming insensitive to pixels\non the edge of images (see App. H for evidence of this effect).\n6\nConclusion\nOur central result is that learning sparse features can be detrimental if the task presents invariance\nor smooth variations along transformations that are not adequately captured by the neural network\narchitecture. For fully-connected networks, these transformations can be rotations of the input, but\nalso continuous translations and diffeomorphisms.\n9\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n102\nOverﬁtting in Feature Learning\n102\n103\n104\ntrainset size, n\n101\ndeformation rel. stability, Rf\nMNIST\nMNIST\nFeature\nLazy\n102\n103\n104\ntrainset size, n\n101\n2 × 100\n3 × 100\n4 × 100\n6 × 100\nF-MNIST\nF-MNIST\nFeature\nLazy\n102\n103\n104\ntrainset size, n\n101\nCIFAR10\nCIFAR10\nFeature\nLazy\nFigure 6: Sensitivity to diffeomorphisms vs number of training points. Relative sensitivity of the\npredictor to small diffeomorphisms of the input images, in the two regimes, for varying number of\ntraining points n and different image datasets. Smaller values correspond to a smoother predictor, on\naverage. Results are computed using the same predictors as in Fig. 1.\nOur analysis relies on the sparsity of the features learned by a shallow fully-connected architecture:\neven in the inﬁnite width limit, when trained in the feature learning regime such networks behave as\nO(n) neurons. The asymptotic analysis we perform for random Gaussian ﬁelds on the sphere leads\nto predictions for the learning curve exponent β in different training regimes, which we verify. Such\nkind of results is scarce in the literature.\nNote that our analysis focuses on ReLU neurons because (i) these are very often used in practice\nand (ii) in that case, β will depend on the training regime, allowing for stringent numerical tests.\nIf smooth activations (e.g. softplus) are considered, we expect that learning features will still be\ndetrimental for generalization. Yet, the difference will not appear in the exponent β, but in other\naspects of the learning curves (including numerical coefﬁcients and pre-asymptotic effects) that are\nharder to predict.\nMost fundamentally, our results underline that the success of feature learning for modern architectures\nstill lacks a sufﬁcient explanation. Indeed, most of the theoretical studies that previously emphasized\nthe beneﬁts of learning features have been considering fully-connected networks, for which learning\nfeatures can be in practice a drawback. It is tempting to argue that in modern architectures, learning\nfeatures is not at a disadvantage because smoothness along diffeomorphisms can be enforced from\nthe start—due to the locally connected, convolutional, and pooling layers [53, 29]. Yet the best\narchitectures often do not perform pooling and are not stable toward diffeomorphisms at initialization.\nDuring training, learning features leads to more stable and smoother solutions along diffeomorphisms\n[54, 49]. Understanding why building sparse features enhances stability in these architectures may\nultimately explain the magical feat of deep CNNs: learning tasks in high dimensions.\nAcknowledgements\nWe thank Lénaïc Chizat, Antonio Sclocchi, and Umberto M. Tomasini for helpful discussions. The\nwork of MW is supported by a grant from the Simons Foundation (#454953) and from the NSF under\nGrant No. 200021-165509. The work of EVE is supported by the National Science Foundation under\nawards DMR-1420073, DMS-2012510, and DMS-2134216, by the Simons Collaboration on Wave\nTurbulence, Grant No. 617006, and by a Vannevar Bush Faculty Fellowship.\nReferences\n[1] Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.\nJournal of Machine Learning Research, 5(Jun):669–695, 2004.\n[2] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal\nof Machine Learning Research, 18(1):629–681, 2017.\n[3] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\nKianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is\npredictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n[4] Quoc V Le. Building high-level features using large scale unsupervised learning. In 2013 IEEE\ninternational conference on acoustics, speech and signal processing, pages 8595–8598. IEEE,\n2013.\n10\n103\nOverﬁtting in Feature Learning\n[5] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via\ninformation. arXiv preprint arXiv:1703.00810, 2017.\n[6] Alessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan. Intrinsic dimension\nof data representations in deep neural networks. In Advances in Neural Information Processing\nSystems, pages 6111–6122, 2019.\n[7] Stefano Recanatesi, Matthew Farrell, Madhu Advani, Timothy Moore, Guillaume Lajoie, and\nEric Shea-Brown. Dimensionality compression and expansion in deep neural networks. arXiv\npreprint arXiv:1906.00443, 2019.\n[8] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-\nming. In Advances in Neural Information Processing Systems, pages 2937–2947, 2019.\n[9] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence\nand generalization in neural networks. In Proceedings of the 32Nd International Conference\non Neural Information Processing Systems, NIPS’18, pages 8580–8589, USA, 2018. Curran\nAssociates Inc.\n[10] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably\noptimizes over-parameterized neural networks.\nIn International Conference on Learning\nRepresentations, 2019.\n[11] Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:\nAsymptotic convexity of the loss landscape and universal scaling of the approximation error.\narXiv preprint arXiv:1805.00915, 2018.\n[12] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of\ntwo-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–\nE7671, August 2018. Publisher: National Academy of Sciences Section: PNAS Plus.\n[13] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Neural Networks: A\nLaw of Large Numbers. SIAM Journal on Applied Mathematics, 80(2):725–752, January 2020.\nPublisher: Society for Industrial and Applied Mathematics.\n[14] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay\nGolan, Daniel Soudry, and Nathan Srebro. Kernel and Rich Regimes in Overparametrized\nModels. In Conference on Learning Theory, pages 3635–3673. PMLR, July 2020. ISSN:\n2640-3498.\n[15] Jaume de Dios and Joan Bruna. On sparsity in overparametrised shallow relu networks. arXiv\npreprint arXiv:2006.10225, 2020.\n[16] Lénaïc Chizat and Francis Bach. Implicit Bias of Gradient Descent for Wide Two-layer Neural\nNetworks Trained with the Logistic Loss. In Conference on Learning Theory, pages 1305–1338.\nPMLR, July 2020. ISSN: 2640-3498.\n[17] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When Do Neural\nNetworks Outperform Kernel Methods? Advances in Neural Information Processing Systems,\n33, 2020.\n[18] Maria Reﬁnetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborová. Classifying high-\ndimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. arXiv\npreprint arXiv:2102.11742, 2021.\n[19] Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric\ncompression of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory\nand Experiment, 2021(4):044001, April 2021. Publisher: IOP Publishing.\n[20] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and\nlazy training in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment,\n2020(11):113301, November 2020. Publisher: IOP Publishing.\n[21] Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman\nNovak, and Jascha Sohl-Dickstein. Finite versus inﬁnite neural networks: an empirical study.\narXiv preprint arXiv:2007.15801, 2020.\n[22] Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can\nlinearized neural networks actually say about generalization? Advances in Neural Information\nProcessing Systems, 34, 2021.\n11\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n104\nOverﬁtting in Feature Learning\n[23] Yilan Chen, Wei Huang, Lam M. Nguyen, and Tsui-Wei Weng. On the Equivalence between\nNeural Network and Support Vector Machine. arXiv:2111.06063 [cs, math, stat], November\n2021. arXiv: 2111.06063.\n[24] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,\nGiulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization\nwith number of parameters in deep learning. Journal of Statistical Mechanics: Theory and\nExperiment, 2020(2):023401, February 2020. Publisher: IOP Publishing.\n[25] Mario Geiger, Leonardo Petrini, and Matthieu Wyart. Landscape and training regimes in deep\nlearning. Physics Reports, 924:1–18, August 2021.\n[26] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[27] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for\nBenchmarking Machine Learning Algorithms. arXiv:1708.07747 [cs, stat], September 2017.\narXiv: 1708.07747.\n[28] Alex Krizhevsky. Learning multiple layers of features from tiny images. ., 2009.\n[29] Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions\non pattern analysis and machine intelligence, 35(8):1872–1886, 2013.\n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc., 2019.\n[31] Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network\nfeatures. arXiv preprint arXiv:1803.08367, 2018.\n[32] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural\nnetworks. In Conference on Learning Theory, pages 1376–1401. PMLR, 2015.\n[33] Claire Boyer, Antonin Chambolle, Yohann De Castro, Vincent Duval, Frédéric De Gournay,\nand Pierre Weiss. On Representer Theorems and Convex Regularization. SIAM Journal on\nOptimization, 29(2):1260–1281, May 2019.\n[34] Lenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent.\nMathematical Programming, pages 1–46, 2021.\n[35] Bruno A. Olshausen and David J. Field. Emergence of simple-cell receptive ﬁeld properties by\nlearning a sparse code for natural images. Nature, 381(6583):607–609, June 1996. Number:\n6583 Publisher: Nature Publishing Group.\n[36] Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, and Francis Bach. Supervised\nDictionary Learning. In Advances in Neural Information Processing Systems, volume 21. Curran\nAssociates, Inc., 2008.\n[37] Nishant A Mehta and Alexander G Gray. Sparsity-Based Generalization Bounds for Predictive\nSparse Coding. page 9.\n[38] Jeremias Sulam, Ramchandran Muthukumar, and Raman Arora. Adversarial Robustness of\nSupervised Sparse Coding, January 2021. arXiv:2010.12088 [cs, stat].\n[39] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for un-\nderstanding neural networks. In Advances in Neural Information Processing Systems, pages\n6598–6608, 2019.\n[40] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of\nlazy training of two-layers neural network. In Advances in Neural Information Processing\nSystems, pages 9111–9121, 2019.\n[41] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the\nterminal phase of deep learning training. Proceedings of the National Academy of Sciences,\n117(40):24652–24663, 2020.\n[42] Behnam Neyshabur. Towards Learning Convolutions from Scratch. arXiv:2007.13657 [cs, stat],\nJuly 2020. arXiv: 2007.13657.\n12\n105\nOverﬁtting in Feature Learning\n[43] Alessandro Ingrosso and Sebastian Goldt. Data-driven emergence of convolutional structure\nin neural networks. Proceedings of the National Academy of Sciences, 119(40):e2201854119,\n2022.\n[44] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel\nmethods: empirical data versus teacher–student paradigm. Journal of Statistical Mechanics:\nTheory and Experiment, 2020(12):124001, December 2020. Publisher: IOP Publishing.\n[45] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum Dependent Learning\nCurves in Kernel Regression and Wide Neural Networks. In International Conference on\nMachine Learning, pages 1024–1034. PMLR, November 2020. ISSN: 2640-3498.\n[46] Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová. Generalization error rates\nin kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural\nInformation Processing Systems, 34, 2021.\n[47] Umberto M Tomasini, Antonio Sclocchi, and Matthieu Wyart. Failure and success of the\nspectral bias prediction for kernel ridge regression: the case of low-dimensional data. arXiv\npreprint arXiv:2202.03348, 2022.\n[48] Stéphane Mallat. Understanding deep convolutional networks. Philosophical Transactions of\nthe Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150203,\n2016.\n[49] Leonardo Petrini, Alessandro Favero, Mario Geiger, and Matthieu Wyart. Relative stability\ntoward diffeomorphisms indicates performance in deep nets. In Advances in Neural Information\nProcessing Systems, volume 34, pages 8727–8739. Curran Associates, Inc., 2021.\n[50] Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by\nbasis pursuit. SIAM Journal on Scientiﬁc Computing, 20(1):33–61, 1998.\n[51] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,\nregularization, optimization, and beyond. MIT press, 2001.\n[52] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in\nNeural Information Processing Systems 22, pages 342–350. Curran Associates, Inc., 2009.\n[53] Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of\ndeep convolutional representations. The Journal of Machine Learning Research, 20(1):876–924,\n2019.\n[54] Avraham Ruderman, Neil C. Rabinowitz, Ari S. Morcos, and Daniel Zoran. Pooling is neither\nnecessary nor sufﬁcient for appropriate deformation stability in CNNs. arXiv:1804.04438 [cs,\nstat], May 2018. arXiv: 1804.04438.\n[55] Alex Smola, Zoltán Ovári, and Robert C. Williamson. Regularization with dot-product kernels.\nAdvances in neural information processing systems, 13, 2000.\n[56] Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit\nsphere: an introduction, volume 2044. Springer Science & Business Media, 2012.\n[57] Costas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World Scientiﬁc,\n2014.\n[58] Alberto Bietti and Francis Bach. Deep Equals Shallow for ReLU Networks in Kernel Regimes.\narXiv:2009.14397 [cs, stat], August 2021. arXiv: 2009.14397.\n[59] Francis Bach. Learning Theory from First Principles. In preparation, 2022.\n13\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n106\nOverﬁtting in Feature Learning\nA\nQuick recap of spherical harmonics\nSpherical harmonics\nThis appendix collects some introductory background on spherical harmonics\nand dot-product kernels on the sphere [55]. See [56, 57] for an expanded treatment. Spherical\nharmonics are homogeneous polynomials on the sphere Sd−1 = {x ∈Rd | ∥x∥= 1}, with ∥.∥\ndenoting the L2 norm. Given the polynomial degree k ∈N, there are Nk,s linearly independent\nspherical harmonics of degree k on Ss−1, with\nNk,d = 2k + d −2\nk\n\u0012d + k −3\nk −1\n\u0013\n,\n(\nN0,d = 1\n∀d,\nNk,d ≍Adkd−2\nfor k ≫1,\n(A.1)\nwhere ≍means logarithmic equivalence for k →∞and Ad =\np\n2/π(d −2)\n3\n2 −ded−2. Thus, we\ncan introduce a set of Nk,d spherical harmonics Yk,ℓfor each k, with ℓranging in 1, . . . , Nk,d, which\nare orthonormal with respect to the uniform measure on the sphere dτ(x),\n{Yk,ℓ}k≥0,ℓ=1,...,Nk,d ,\n⟨Yk,ℓ, Yk,ℓ′⟩Sd−1 :=\nZ\nSd−1 Yk,ℓ(x)Yk,ℓ′(x) dτ(x) = δℓ,ℓ′.\n(A.2)\nBecause of the orthogonality of homogeneous polynomials with different degree, the set is a com-\nplete orthonormal basis for the space of square-integrable functions on Sd−1. For any function\nf : Sd−1 →R, then\nf(x) =\nX\nk≥0\nNk,d\nX\nℓ=1\nfk,ℓYk,ℓ(x),\nfk,ℓ=\nZ\nSd−1 f(x)Yk,ℓ(x)dτ(x).\n(A.3)\nFurthermore, spherical harmonics are eigenfunctions of the Laplace-Beltrami operator ∆, which is\nnothing but the restriction of the standard Laplace operator to Sd−1,\n∆Yk,ℓ= −k(k + d −2)Yk,ℓ.\n(A.4)\nLegendre polynomials\nBy ﬁxing a direction y in Sd−1 one can select, for each k, the only spherical\nharmonic of degree k which is invariant for rotations that leave y unchanged. This particular spherical\nharmonic is, in fact, a function of x · y and is called the Legendre polynomial of degree k, Pk,d(x · y)\n(also referred to as Gegenbauer polynomial). Legendre polynomials can be written as a combination\nof the orthonormal spherical harmonics Yk,ℓvia the addition theorem [56, Thm. 2.9],\nPk,d(x · y) =\n1\nNk,d\nNk,d\nX\nℓ=1\nYk,ℓ(x)Yk,ℓ(y).\n(A.5)\nAlternatively, Pk,d is given explicitly as a function of t = x · y ∈[−1, 1] via the Rodrigues’ for-\nmula [56, Thm. 2.23],\nPk,d(t) =\n\u0012\n−1\n2\n\u0013k\nΓ\n\u0000 d−1\n2\n\u0001\nΓ\n\u0000k + d−1\n2\n\u0001 \u00001 −t2\u0001 3−d\n2\ndk\ndtk\n\u00001 −t2\u0001k+ d−3\n2 .\n(A.6)\nHere Γ denotes the Gamma function, Γ(z) =\nR ∞\n0\nxz−1e−x dx. Legendre polynomials are orthogonal\non [−1, 1] with respect to the measure with density (1 −t2)(d−3)/2, which is the probability density\nfunction of the scalar product between to points on Sd−1.\nZ +1\n−1\nPk,d(t)Pk′,d(t)\n\u00001 −t2\u0001 d−3\n2 dt = |Sd−1|\n|Sd−2|\nδk,k′\nNk,s\n.\n(A.7)\nHere |Sd−1| = 2π\nd\n2 /Γ( d\n2) denotes the surface area of the d-dimensional unit sphere (|S0| = 2 by\ndeﬁnition).\nTo sum up, given x, y ∈Sd−1, functions of x or y can be expressed as a sum of projections on the\northonormal spherical harmonics, whereas functions of x · y can be expressed as a sum of projections\non the Legendre polynomials. The relationship between the two expansions is elucidated in the\nFunk-Hecke formula [56, Thm. 2.22],\nZ\nSd−1 f(x · y)Yk,ℓ(y) dτ(y) = Yk,ℓ(x)|Sd−2|\n|Sd−1|\nZ +1\n−1\nf(t)Pk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt := fkYk,ℓ(x).\n(A.8)\n14\n107\nOverﬁtting in Feature Learning\nA.1\nExpansion of ReLU and combinations thereof\nWe can apply Eq. A.8 to have an expansion of neurons σ (θ · x) in terms of spherical harmonics [2,\nAppendix D]. After deﬁning\nϕk := |Sd−2|\n|Sd−1|\nZ +1\n−1\nσ(t)Pk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt,\n(A.9)\none has\nσ (θ · x) =\nX\nk≥0\nNk,dϕkPk,d (θ · x) =\nX\nk≥0\nϕk\nNk,d\nX\nℓ=1\nYk,ℓ(θ)Yk,ℓ(x).\n(A.10)\nFor ReLU activations, in particular, σ(t) = max(0, t), thus\nϕReLU\nk\n= |Sd−2|\n|Sd−1|\nZ +1\n0\ntPk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt.\n(A.11)\nNotice that when k is odd Pk,d is an odd function of t, thus the integrand tPk,d(t)(1 −t2)\nd−3\n2\nis an\neven function of t. As a result the integral on the right-hand side of Eq. A.11 coincides with half the\nintegral over the full domain [−1, 1],\nZ +1\n0\ntPk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt = 1\n2\nZ +1\n−1\ntPk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt = 0 for k > 1,\n(A.12)\nbecause, due to Eq. A.7, Pk,d is orthogonal to all polynomials with degree strictly lower than k. For\neven k we can use Eq. A.6 and get [2] (see Eq. 3.3, main text)\nZ +1\n0\ntPk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt =\n\u0012\n−1\n2\n\u0013k\nΓ\n\u0000 d−1\n2\n\u0001\nΓ\n\u0000k + d−1\n2\n\u0001\nZ 1\n0\nt dk\ndtk\n\u00001 −t2\u0001k+ d−3\n2\ndt\n= −\n\u0012\n−1\n2\n\u0013k\nΓ\n\u0000 d−1\n2\n\u0001\nΓ\n\u0000k + d−1\n2\n\u0001 dk−2\ndtk−2\n\u00001 −t2\u0001k+ d−3\n2\n\f\f\f\f\nt=1\nt=0\n⇒ϕReLU\nk\n∼k−d−1\n2\n−3\n2 for k ≫1 and even.\n(A.13)\nBecause all ϕReLU\nk\nwith k > 1 and odd vanish, even summing an inﬁnite amount of neurons σ(θ · x)\nwith varying θ does not allow to approximate any function on Sd−1, but only those which have\nvanishing projections on all the spherical harmonics Yk,ℓwith k > 1 and odd. This is why we set the\nodd coefﬁcients of the target function spectrum to zero in Eq. 2.1.\nA.2\nDot-product kernels on the sphere\nAlso general dot-product kernels on the sphere admit an expansion such as Eq. A.10,\nC (x · y) =\nX\nk≥0\nNk,dckPk,d (θ · x) =\nX\nk≥0\nck\nNk,d\nX\nℓ=1\nYk,ℓ(θ)Yk,ℓ(x),\n(A.14)\nwith\nck = |Sd−2|\n|Sd|\nZ 1\n−1\nC(t)Pk,d(t)\n\u00001 −t2\u0001 d−3\n2 dt.\n(A.15)\nThe asymptotic decay of ck for large k is controlled by the behaviour of C(t) near t = ±1, [58]. More\nprecisely [58, Thm. 1], if C is inﬁnitely differentiable in (−1, 1) and has the following expansion\naround ±1,\n\u001aC(t) = p1(1 −t) + c1(1 −t)ν + o ((1 −t)ν) near t = +1;\nC(t) = p−1(−1 + t) + c−1(−1 + t)ν + o ((−1 + t)ν) near t = −1,\n(A.16)\nwhere p±1 are polynomials and ν is not an integer, then\nk even: ck ∼(c1 + c−1)k−2ν−(d−1);\nk odd: ck ∼(c1 −c−1)k−2ν−(d−1),\n(A.17)\nThe result above implies that that if c1 = c−1 (c1 = −c−1), then the eigenvalues with k odd (even)\ndecay faster than k−2ν−(d−2). Moreover, if C is inﬁnitely differentiable in [−1, 1] then ck decays\nfaster than any polynomial.\n15\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n108\nOverﬁtting in Feature Learning\nNTK and RFK of one-hidden-layer ReLU networks\nLet Eθ denote expectation over a multivari-\nate normal distribution with zero mean and unitary covariance matrix. For any x, y ∈Sd−1, the RFK\nof a one-hidden-layer ReLU network Eq. 2.3 with all parameters initialised as independent Gaussian\nrandom numbers with zero mean and unit variance reads\nKRFK(x · y) = Eθ [σ(θ · x)σ(θ · y)]\n= (π −arccos (t))t +\n√\n1 −t2\n2π\n, with t = x · y.\n(A.18)\nThe NTK of the same network reads, with σ′ denoting the derivative of ReLU or Heaviside function,\nKNTK(x · y) = Eθ [σ(θ · x)σ(θ · y)] + (x · y)Eθ [σ′(θ · x)σ′(θ · y)]\n= 2(π −arccos (t))t +\n√\n1 −t2\n2π\n, with t = x · y.\n(A.19)\nAs functions of a dot-product on the sphere, both NTK and RFK admit a decomposition in terms of\nspherical harmonics as Eq. A.15. For dot-product kernels, this expansion coincides with the Mercer’s\ndecomposition of the kernel [55], that is the coefﬁcients of the expansion are the eigenvalues of the\nkernel. The asymptotic decay of the eigenvalues of such kernels ϕNTK\nk\nand ϕRFK\nk\ncan be obtained\nby applying Eq. A.16 [58, Thm. 1]. Equivalently, one can notice that KRFK is proportional to the\nconvolution on the sphere of ReLU with itself, therefore ϕRFK\nk\n= (ϕReLU\nk\n)2. Similarly, the asymptotic\ndecay of ϕNTK\nk\ncan be related to that of the coefﬁcients of σ′, derivative of ReLU: ϕk(σ′) ∼kϕ(σ),\nthus ϕNTK\nk\n∼k2(ϕReLU\nk\n)2. Both methods lead to Eq. 3.3 of the main text.\nGaussian random ﬁelds and Eq. 2.2\nConsider a Gaussian random ﬁeld f ∗on the sphere with\ncovariance kernel C(x · y),\nE [f ∗(x)] = 0,\nE [f ∗(x)f ∗(y)] = C(x · y),\n∀x, y ∈Sd−1.\n(A.20)\nf ∗can be equivalently speciﬁed via the statistics of the coefﬁcients f ∗\nk,ℓ,\nE\n\u0002\nf ∗\nk,ℓ\n\u0003\n= 0,\nE\n\u0002\nf ∗\nk,ℓf ∗\nk′,ℓ′\n\u0003\n= ckδk,k′δℓ,ℓ′,\n(A.21)\nwith ck denoting the eigenvalues of C in Eq. A.15. Notice that the eigenvalues are degenerate with\nrespect to ℓbecause the covariance kernel is a function x · y: as a result, the random function f ∗is\nisotropic in law.\nIf ck decays as a power of k, then such power controls the weak differentiability (in the mean-squared\nsense) of the random ﬁeld f ∗. In fact, from Eq. A.4,\n\r\r\r∆m/2f ∗\r\r\r =\nX\nk≥0\nX\nℓ\n(−k(k + d −2))m \u0000f ∗\nk,ℓ\n\u00012 .\n(A.22)\nUpon averaging over f ∗one gets\nE\nh\r\r\r∆m/2f ∗\r\r\r\ni\n=\nX\nk≥0\n(−k(k + d −2))m X\nℓ\nE\nh\u0000f ∗\nk,ℓ\n\u00012i\n=\nX\nk≥0\n(−k(k + d −2))m Nk,dck.\n(A.23)\nFrom Eq. A.16 [58, Thm. 1], if C(t) ∼(1−t)νt for t →1 and/or C(t) ∼(−1+t)νt for t →−1, then\nck ∼k−2νt−(d−1) for k ≫1. In addition, for ﬁnite but arbitrary d, (−k(k + d −2))m ∼k2m and\nNk,s ∼kd−2 (see Eq. A.1). Hence the summand in the right-hand side of Eq. A.23 is ∼k2(m−νt)−1,\nthus\nE\nh\r\r\r∆m/2f ∗\r\r\r\ni\n< ∞\n∀m < νt.\n(A.24)\nAlternatively, one can think of νt as controlling the scaling of the difference δf ∗over inputs separated\nby a distance δ. From Eq. A.20,\nE\n\u0002\n|f ∗(x) −f ∗(y)|2\u0003\n= 2C(1) −2C(x · y) = 2C(1) + O((1 −x · y)νt)\n= 2C(1) + O(|x −y|2νt)\n(A.25)\n16\n109\nOverﬁtting in Feature Learning\nB\nUniqueness and Sparsity of the L1 minimizer\nRecall that we want to ﬁnd the γ∗that solves\nγ∗= arg min\nγ\nZ\nSd−1|dγ(θ)|\nsubject to\nZ\nSd−1 σ(θ · xi)dγ(θ)=f ∗(xi)\n∀i = 1, . . . , n. (B.1)\nIn this appendix, we argue that the uniqueness of γ∗which implies that it is atomic with at most\nn atoms is a natural assumption. We start by discretizing the measure γ into H atoms, with H\narbitrarily large. Then the problem Eq. B.1 can be rewritten as\nw∗= arg min\nw\n∥w∥1,\nsubject to\nΦw = y,\n(B.2)\nwith Φ ∈RH×n, Φh,i = σ(θh · xi) and yi = f ∗(xi).\nGiven w ∈RH, let u = max(w, 0) ≥0 and v = −max(−w, 0) ≥0 so that w = u −v. It is\nwell-known (see e.g. [50]) that the minimization problem in (B.2) can be recast in terms of u and v\ninto a linear programming problem. That is, w∗= u∗−v∗with\n(u∗, v∗) = arg min\nu,v\neT (u + v),\nsubject to Φu −Φv = y,\nu ≥0,\nv ≥0\n(B.3)\nwhere e = [1, 1, . . . , 1]T . Assuming that this problem is feasible (i.e. there is at least one solution to\nΦu −Φv = y such that u ≥0, v ≥0), it is known that it admits extremal solution, i.e. solutions\nsuch that at most n entries of (u∗, v∗) (and hence w∗) are non-zero. The issue is whether such\nan extremal solution is unique. Assume that there are two, say (u∗\n1, v∗\n1) and (u∗\n2, v∗\n2). Then, by\nconvexity,\n(u∗\nt , v∗\nt ) = (u∗\n1, v∗\n1)t + (u∗\n2, v∗\n2)(1 −t)\n(B.4)\nis also a minimizer of (B.3) for all t ∈[0, 1], with the same minimum value u∗\nt + v∗\nt = u∗\n1 + v∗\n1 =\nu∗\n2 + v∗\n2. Generalizing this argument to the case of more than two extremal solutions, we conclude\nthat all minimizers are global, with the same minimum value, and they live on the simplex where\neT (u + v) = eT (u1 + v1). Therefore, nonuniqueness requires that that this simplex has a nontrivial\nintersection with the feasible set where Φu−Φv = y with u ≥0, v ≥0. We argue that, generically,\nthis will not be the case, i.e. the intersection will be trivial, and the extremal solution unique. In\nparticular, since in our case we are in fact interested in the problem (B.1), we can always perturb\nslightly the discretization into H atoms of γ to guarantee that the extremal solution is unique. Since\nthis is true no matter how large H is, and any Radon measure can be approached to arbitrary precision\nusing such discretization, we conclude that the minimizer of (B.1) should be unique as well, with at\nmost n atoms.\nC\nProof of Proposition 1\nIn this section, we provide the formal statement and proof of Proposition 1. Let us recall the general\nform of the predictor for both lazy and feature regimes in d = 2. From Eq. 3.6,\nf n(x) =\nn\nX\nj=1\ngj ˜ϕ(x −xj) =\nZ dy\n2π gn(y) ˜ϕ(x −y).\n(C.1)\nwhere n is the number of training points for the lazy regime and the number of atoms for the feature\nregime and, for x ∈(−π, π],\n˜ϕ(x) =\n\n\n\n\n\n\n\n\n\n\n\nmax {0, cos (x)}\n(feature regime),\n2(π −|x|) cos(x) + sin(|x|)\n2π\n(lazy regime, NTK),\n(π −|x|) cos(x) + sin(|x|)\n2π\n(lazy regime, RFK).\n(C.2)\nAll these functions ˜ϕ have jump discontinuities on some derivative: the ﬁrst for feature and NTK, the\nthird for RFK. If the l-th derivative has jump discontinuities, the l + 1-th only exists in a distributional\nsense and it can be generically written as a sum of a regular function and a sequence of Dirac masses\n17\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n110\nOverﬁtting in Feature Learning\nlocated at the discontinuities. With m denoting the number of such discontinuities and {xj}j their\nlocations, f (l) denoting the l-th derivative of f, for some cj ∈R,\nf (l+1)(x) = f (l+1)\nr\n(x) +\nm\nX\nj=1\ncjδ(x −xj),\n(C.3)\nwhere fr denotes the regular part of f.\nProposition 2. Consider a random target function f ∗satisfying Eq. 2.1 and the predictor f n obtained\nby training a one-hidden-layer ReLU network on n samples (xi, f ∗(xi)) in the feature or in the lazy\nregime (Eq. C.1). Then, with bf(k) denoting the Fourier transform of f(x), one has\nlim\n|k|→∞lim\nn→∞\n\\\n(f n)′′r(k)\nc\nf ∗(k)\n= c,\n(C.4)\nwhere c is a constant (different for every regime). This result implies that as n →∞, (f n)′′(x)\nconverges to a function having ﬁnite second moment, i.e.\nlim\nn→∞Ef ∗[(f n)′′\nr(x)]2 = lim\nn→∞Ef ∗\n\u0014Z\ndx ((f n)′′\nr)2 (x)\n\u0015\n= lim\nn→∞Ef ∗\n\"X\nk\n\\\n(f n)′′r\n2\n(k)\n#\n= const. < ∞,\n(C.5)\nusing the fact that Ef ∗[(f n)′′\nr(x)]2 does not depend on x and Ef ∗[P\nk d\n(f ∗)\n2\n(k)] = const.\nProof: Because our target functions are random ﬁelds that are in L2 with probability one, and the\nRKHS of our kernels are dense in that space, we know that the test error vanishes as n →∞[59].\nAs a result\nf ∗(x) = lim\nn→∞f n(x) = lim\nn→∞\nZ dy\n2π gn(y) ˜ϕ(x −y).\n(C.6)\nConsider ﬁrst the feature regime and the NTK lazy regime. In both cases ˜ϕ has two jump discontinu-\nities in the ﬁrst derivative, located at x = 0, π for the NTK and at x = ± π/2, therefore we can write\nthe second derivative as the sum of a regular function and two Dirac masses,\n( ˜ϕFEATURE)′′ = −max {0, cos (x)} + δ(x −π/2) + δ(x + π/2),\n( ˜ϕNTK)′′ = −2(π −|x|) cos(x) + 3 sin(|x|)\n2π\n−1\n2π δ(x) + 1\n2π δ(x −π).\n(C.7)\nAs a result, the second derivative of the predictor can be written as the sum of a regular part (f n)′′\nr\nand a sequence of 2n Dirac masses. After subtracting the Dirac masses, both sides of Eq. C.1 can be\ndifferentiated twice and yield\n(f n)′′\nr(x) =\nZ dy\n2π gn(y) ˜ϕ′′\nr(x −y).\n(C.8)\nHence in the Fourier representation we have\n\\\n(f n)′′r(k) = c\ngn(k)(−k2b˜ϕr(k))\n(C.9)\nwhere we deﬁned\nb˜ϕ(k) =\nZ π\n−π\ndx\n√\n2π eikx ˜ϕ(x),\nc\n˜ϕr(k) =\nZ π\n−π\ndx\n√\n2π eikx ˜ϕr(x).\n(C.10)\nand used c\n˜ϕ′′r(k) = −k2c\n˜ϕr(k). By universal approximation we have\nc\nf ∗(k) =\nZ π\n−π\ndx\n√\n2π eikxf ∗(x) = lim\nn→∞c\ngn(k)b˜ϕ(k)\n⇒\nlim\nn→∞c\ngn(k) =\nc\nf ∗(k)\nb˜ϕ(k)\n.\n(C.11)\nAs a result by combining Eq. C.9 and Eq. C.11 we deduce\nlim\nn→∞\n\\\n(f n)′′r(k) = −k2b˜ϕr(k)\nb˜ϕ(k)\nc\nf ∗(k).\n(C.12)\n18\n111\nOverﬁtting in Feature Learning\nTo complete the proof using this result it remains to estimate the scaling of b˜ϕr(k) and b˜ϕ(k) in the\nlarge |k| limit.\nFor the feature regime, a direct calculation shows that ˜ϕ′′\nr = −˜ϕ, implying that b˜ϕr(k) = −b˜ϕ(k).\nThis proves that Eq. C.4 is satisﬁed with c = −1.\nFor the NTK lazy regime ˜ϕ′′\nr and −˜ϕ are different but they have similar singular expansions near x = 0\nand π. Therefore their Fourier coefﬁcients display the same asymptotic decay. More speciﬁcally,\nwith t = cos(x) (or x = arccos(t)), so that ˜ϕ(x) = ϕ(t), one has\n\n\n\n\n\n\n\nϕNTK(t) = t −\n1\n√\n2π (1 −t)1/2 + O\n\u0010\n(1 −t)3/2\u0011\nnear t = +1;\nϕNTK(t) = −\n1\n√\n2π (−1 + t)1/2 + O\n\u0010\n(−1 + t)3/2\u0011\nnear t = −1,\n(C.13)\nand\n\n\n\n\n\n\n\n(ϕNTK)′′\nr(t) = −t +\n5\n√\n2π (1 −t)1/2 + O\n\u0010\n(1 −t)3/2\u0011\nnear t = +1;\n(ϕNTK)′′\nr(t) = +\n5\n√\n2π (−1 + t)1/2 + O\n\u0010\n(−1 + t)3/2\u0011\nnear t = −1.\n(C.14)\nTherefore, due to Eq. A.17, Eq. C.4 is satisﬁed with c = −5. The same procedure can be applied to\nthe RFK lazy regime, with the exception that it is the fourth derivative of ˜ϕRFK which can be written\nas a regular part plus Dirac masses, but one can still obtain the Fourier coefﬁcients of the second\nderivative’s regular part by dividing those of the fourth derivative’s regular part by k2.\nD\nAsymptotics of generalization in d = 2\nIn this section we compute the decay of generalization error ϵ with the number of samples n in the\nfollowing 2-dimensional setting:\nf n(x) =\nn\nX\nj=1\ngj ˜ϕ(x −xj),\n(D.1)\nwhere the xj’s are the training points (like in the NTK case) and ϕ has a single discontinuity on the\nﬁrst derivative in 0.\nLet us order the training points clockwise on the ring, such that x1 = 0 and xi+1 > xi for all\ni = 1, . . . , n, with xn+1 := 2π. On each of the xi the predictor coincides with the target,\nf n(xi) = f ∗(xi)\n∀i = 1, . . . , n.\n(D.2)\nFor large enough n, the difference xi+1 −xi is small enough such that, within (xi, xi+1), f n(x) can\nbe replaced with its Taylor series expansion up to the second order. In practice, the predictors appear\nlike the cable of a suspension bridge with the pillars located on the training points. In particular, we\ncan consider an expansion around x+\ni := xi + ϵ for any ϵ > 0 and then let ϵ →0 from above:\nf n(x) = f n(x+\ni ) + (x −x+\ni )f n′(x+\ni ) + (x −x+\ni )2\n2\n(f n)′′(x+\ni ) + O\n\u0000(x −x+\ni )3\u0001\n.\n(D.3)\nBy differentiability of f n in (xi, xi+1) the second derivative can be computed at any point inside\n(xi, xi+1) without changing the order of approximation in Eq. D.3, in particular we can replace\n(f n)′′(x+\ni ) with ci, the mean curvature of f n in (xi, xi+1). Moreover, as ϵ →0, f n(x+\ni ) →f ∗(xi)\nand f n(x−\ni+1) →f ∗(xi+1). By introducing the limiting slope m+\ni := limx→0+ f n′(xi + x), we can\nwrite\nf n(x) = f ∗(xi) + (x −xi)m+\ni + (x −xi)2\n2\nci + O\n\u0000(x −x+\ni )3\u0001\n(D.4)\nComputing Eq. D.4 at x = xi+1 yields a closed form for the limiting slope m+\ni as a function of the\nmean curvature ci, the interval length δi := (xi+1 −xi) and ∆fi := f ∗(xi+1) −f ∗(xi). Speciﬁcally,\nm+\ni = ∆fi\nδi\n−δi\n2 ci.\n(D.5)\n19\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n112\nOverﬁtting in Feature Learning\nThe generalization error can then be split into contributions from all the intervals. If νt > 2, A Taylor\nexpansion leads to:\nϵ(n) =\nZ 2π\n0\ndx\n2π (f n(x) −f ∗(x))2\n=\nn\nX\ni=1\nZ xi+1\nxi\ndx\n2π\n\u0014\n(x −xi)\n\u0000m+\ni −(f ∗)′(xi)\n\u0001\n+ (x −xi)2\n2\n(ci −(f ∗)′′(xi)) + o\n\u0000(x −x+\ni )2\u0001\u00152\n=\nn\nX\ni=1\nZ δi\n0\ndδ\n2π\n\u0014\nδ\n\u0000m+\ni −(f ∗)′(xi)\n\u0001\n+ δ2\n2 (ci −(f ∗)′′(xi)) + o\n\u0000δ2\u0001\u00152\n=\nn\nX\ni=1\n1\n2π\n\u0014δ3\ni\n3\n\u0000m+\ni −(f ∗)′(xi)\n\u00012 + δ5\ni\n20 (ci −(f ∗)′′(xi))2\n+δ4\ni\n4\n\u0000m+\ni −(f ∗)′(xi)\n\u0001\n(ci −(f ∗)′′(xi)) + o(δ5\ni )\n\u0015\n.\n(D.6)\nIn addition, as ∆fi = (f ∗)′(xi)δi + (f ∗)′′(xi)δ2\ni /2 + O(δ3\ni ),\nm+\ni −(f ∗)′(xi) = δi\n2 ((f ∗)′′(xi) −ci) + o(δi)2,\n(D.7)\nthus\nϵ(n) = 1\n2π\nn\nX\ni=1\n\u0014 δ5\ni\n120 (ci −(f ∗)′′(xi))2 + o(δ5\ni )\n\u0015\n.\n(D.8)\nimplying:\nϵ(n) = n−4\u0000n−1 Pn\ni=1(nδi)5\u0001\n240π\nlim\nn→∞\nZ\nEf ∗\nh\n((f n)′′(x) −(f ∗)′′(x))2i\ndx + o(n−4) ∼1\nn4\n(D.9)\nwhere we used that (i) the integral converges to some ﬁnite value, due to proposition 2. From App. C,\nthis integral can be estimated as P\nk Ef ∗\nh\u0000cf ∗(k) −k2f ∗(k)\n\u00012i\n, that indeed converges for νt > 2.\n(ii)\n\u0000n−1 Pn\ni=1(nδi)5\u0001\nhas a deterministic limit for large n. It is clear for the lazy regime since the\ndistance between adjacent singularities δi follows an exponential distribution of mean ∼1\nn. We\nexpect this result to be also true for the feature regime in our set-up. Indeed, in the limit n →∞,\nthe predictor approaches a parabola between singular points, which generically cannot ﬁt more than\nthree random points. There must thus be a singularity at least every two data-points with a probability\napproaching unity as n →∞, which implies that\n\u0000n−1 Pn\ni=1(nδi)5\u0001\nconverges to a constant for\nlarge n.\nFinally, for νt < 2, the same decomposition in intervals applies, but a Taylor expansion to second\norder does not hold. The error is then dominated by the ﬂuctuations of f ∗on the scale of the intervals,\nas indicated in the main text.\nE\nAsymptotic of generalization via the spectral bias ansatz\nAccording to the spectral bias ansatz, the ﬁrst n modes of the predictor f n\nk,ℓcoincide with the modes\nof the target function f ∗\nk,ℓ. Therefore, the asymptotic scaling of the error with n is entirely controlled\nby the remaining modes,\nϵ(n) ∼\nX\nk≥kc\nNk,d\nX\nℓ=1\n\u0000f n\nk,ℓ−f ∗\nk,ℓ\n\u00012 with\nX\nk≤kc\nNk,d ∼n.\n(E.1)\nSince Nk,d ∼kd−2 for k ≫1, one has that, for large n, kc ∼n\n1\nd−1 . After averaging the error over\ntarget functions we get\nϵ(n) ∼\nX\nk≥kc\nNk,d\nX\nℓ=1\nn\nEf ∗\nh\u0000f n\nk,ℓ\n\u00012i\n+ Ef ∗\nh\u0000f ∗\nk,ℓ\n\u00012i\n−2Ef ∗\u0002\u0000f n\nk,ℓf ∗\nk,ℓ\n\u0001\u0003o\n.\n(E.2)\n20\n113\nOverﬁtting in Feature Learning\nLet us recall that, with the predictor having the general form in Eq. 3.2, then\nf n\nk,ℓ= gn\nk,ℓϕk\nwith\ngn\nk,ℓ=\nn\nX\nj=1\ngjYk,ℓ(yj),\n(E.3)\nwhere the yj’s denote the training points for the lazy regime and the neuron features for the feature\nregime. For k ≪kc, where f n\nk,ℓ= f ∗\nk,ℓ, gn\nk,ℓ= f ∗\nk,ℓ/ϕk. For k ≫kc, due to the highly oscillating\nnature of Yk,ℓ, the factors Yk,ℓ(yj) are essentially decorrelated random numbers with zero mean and\nﬁnite variance, since the values of (Yk,ℓ(yj))2 are limited by the addition theorem Eq. A.5. Let us\ndenote the variance with σY . By the central limit theorem, gn\nk,ℓconverges to a Gaussian random\nvariable with zero mean and ﬁnite variance σ2\nY\nPn\nj=1 g2\nj . As a result,\nϵ(n) ∼\nX\nk≥kc\nNk,d\nX\nℓ=1\n\n\n\n\n\nn\nX\nj=1\ng2\nj\n\nϕ2\nk + Ef ∗\nh\u0000f ∗\nk,ℓ\n\u00012i\n\n\n\n=\n\n\nn\nX\nj=1\ng2\nj\n\nX\nk≥kc\nNk,dϕ2\nk +\nX\nk≥kc\nNk,dck,\n(E.4)\nwhere we have used the deﬁnition of f ∗(Eq. 2.1) to set the expectation of (f ∗\nk,ℓ)2 to ck.\nLarge νt case\nWhen f ∗is smooth enough the error is controlled by the predictor term proportional\nto Pn\nj=1 g2\nj . More speciﬁcally, if\nX\nk≥0\nNk,d\nX\nℓ=1\nck\nϕ2\nk\n< +∞,\n(E.5)\nthen the function gn(x) converges to the square-summable function g∗(x) such that\nf ∗(x) =\nR\ng∗(y)ϕ(x · y) dτ(y). With ck ∼k−2νt−(d−1) and Nk,d ∼kd−2, in the lazy regime\nϕk ∼k−(d−1)−2ν Eq. E.5 is satisﬁed when 2νt > 2(d −1) + 4ν (ν = 1/2 for the NTK and 3/2 for\nthe RFK). In the feature regime ϕk ∼k−(d−1)/2−3/2, Eq. E.5 is satisﬁed when 2νt > (d −1) + 3. If\ngn(x) converges to a square-summable function, then\nn\nX\nj=1\ng2\nj = 1\nn\nZ\ngn(x)2 dτ(x) + o(n−1) = 1\nn\nX\nk≥0\nNk,d\nck\nϕ2\nk\n+ o(n−1),\n(E.6)\nwhich is proportional to n−1. In addition, since Nk,d ∼kd−2 and kc ∼n\n1\nd−1 , one has\nn−1 X\nk≥kc\nNk,dϕk ∼\n\n\n\n\n\nn−1kd−1k−2(d−1)−4ν\f\f\f\nk=n\n1\nd−1 ∼n−2−4ν\nd−1 (Lazy),\nn−1kd−1k−(d−1)−3\f\f\f\nk=n\n1\nd−1 ∼n−1−\n3\nd−1 (Feature),\n(E.7)\nand\nX\nk≥kc\nNk,dck ∼kd−1k−2νt−(d−1)\f\f\f\nk=n\n1\nd−1 ∼n−2νt\nd−1 .\n(E.8)\nHence, if νt is large enough so that Eq. E.5 is satisﬁed, the asymptotic decay of the error is given\nby Eq. E.7.\nSmall νt case\nIf Eq. E.7 does not hold then gn(x) is not square-summable in the limit n →∞.\nHowever, for large but ﬁnite n only the modes up to the kc-th are correctly reconstructed, therefore\nn\nX\nj=1\ng2\nj ∼1\nn\nX\nk≤kc\nNk,d\nck\nϕ2\nk\n∼\n\n\n\n\n\nn−1k−2νtk2(d−1)+4ν\f\f\f\nk=n\n1\nd−1 ∼n−2νt\nd−1 n1+ 4ν\nd−1 (Lazy),\nn−1k−2νtk(d−1)+3\f\f\f\nk=n\n1\nd−1 ∼n−2νt\nd−1 n\n3\nd−1 (Feature),\n(E.9)\nBoth for feature and lazy, multiplying the term above by P\nk≥kc Nk,dϕk from Eq. E.7 yields\n∼n−2νt/(d−1). This is also the scaling of the target function term Eq. E.8, implying that for small νt\none has\nϵ(n) ∼n−2νt\nd−1\n(E.10)\nboth in the feature and in the lazy regimes.\n21\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n114\nOverﬁtting in Feature Learning\nF\nSpectral bias via the replica calculation\nDue to the equivalence with kernel methods, the asymptotic decay of the test error in the lazy regime\ncan be computed with the formalism of [45], which also provides a non-rigorous justiﬁcation for\nthe spectral bias ansatz. By ranking the eigenvalues from the biggest to the smallest, such that ϕρ\ndenotes the ρ-th eigenvalue and denoting with cρ the variance of the projections of the target onto the\nρ-th eigenfunction, one has\nϵ(n) =\nX\nρ\nϵρ(n),\nϵρ(n) =\nκ(n)2\n(ϕρ + κ(n))2 cρ,\nκ(n) = 1\nn\nX\nρ\nϕρκ(n)\nϕρ + κ(n).\n(F.1)\nIt is convenient to introduce the eigenvalue density,\nD(ϕ) :=\nX\nk≥0\nNk,d\nX\nl=1\nδ(ϕ −ϕk) =\nX\nk≥0\nNk,dδ(ϕ −ϕk) ∼\nZ ∞\n0\nkd−2δ(ϕ −k−(d−1)−2ν) for k ≫1.\n(F.2)\nAfter changing variables in the delta function, one ﬁnds\nD(ϕ) ∼ϕ−2(d−1)+2ν\n(d−1)+2ν for ϕ ≪1.\n(F.3)\nThis can be used for inferring the asymptotics of κ(n),\nκ(n) = 1\nn\nX\nρ\nϕρκ(n)\nϕρ + κ(n) ∼1\nn\nZ\ndϕ D(ϕ) ϕκ(n)\nϕ + κ(n)\n∼1\nn\nZ κ(n)\n0\ndϕ D(ϕ)ϕ + κ(n)\nn\nZ ϕ0\nκ(n)\ndϕ D(ϕ)\n∼1\nnκ(n)1−\n(d−1)\n(d−1)+2ν ⇒κ(n) ∼n−1−2ν\nd−1 .\n(F.4)\nOnce the scaling of κ(n) has been determined, the modal contributions to the error can be split\naccording to whether ϕρ ≪κ(n) or ϕρ ≫κ(n). The scaling of ϕρ with the rank ρ is determined\nself-consistently,\nρ ∼\nZ ϕ1\nϕρ\ndϕ D(ϕ) ∼ϕ\n−\nd−1\n(d−1)+2ν\nρ\n⇒ϕρ ∼ρ−1−2ν\nd−1 ⇒ϕρ ≫(≪)κ(n) ⇔ρ ≪(≫)n.\n(F.5)\nTherefore\nϵ(n) ∼κ(n)2 X\nρ≪n\ncρ\nϕ2ρ\n+\nX\nρ≫n\ncρ.\n(F.6)\nNotice that κ(n)2 scales as n−1 P\nk≥kc Nk,sϕk in Eq. E.7, whereas P\nρ≪n cρ/ϕ2\nρ corresponds to\nn P\nj g2\nj in Eq. E.9, so that the ﬁrst term on the right-hand side of Eq. F.6 matches that of Eq. E.4.\nThe same matching is found for the second term on the right-hand side of Eq. F.6, so that the replica\ncalculation justiﬁes the spectral bias ansatz.\nG\nTraining wide neural networks: does gradient descent (GD) ﬁnd the\nminimal-norm solution?\nIn the main text we provided predictions for the asymptotics of the test error of the minimal norm\nsolution that ﬁts all the training data. Does the prediction hold when solution of Eq. 2.5 and Eq. 2.13\nis approximately found by GD? More speciﬁcally, is the solution found by GD the minimal-norm\none?\nFeature Learning\nWe answer these questions by performing full-batch gradient descent in two\nsettings (further details about the trainings are provided in the code repository, experiments.md\nﬁle),\n22\n115\nOverﬁtting in Feature Learning\n1. Min-L1. Here we update weights and features of Eq. 2.3, with ξ = 0, by following the\nnegative gradient of\nLMin-L1 = 1\n2n\nn\nX\ni=1\n(f ∗(xi) −f(xi))2 + λ\nH\nH\nX\nh=1\n|wh|,\n(G.1)\nwith λ →0+. The weights wh are initialized to zero and the features are initialized\nuniformly and constrained to be on the unit sphere.\n2. α-trick. Following [8], here we minimize\nLα-trick =\n1\n2nα\nn\nX\ni=1\n(f ∗(xi) −αf(xi))2 ,\n(G.2)\nwith α →0. This trick allows to be far from the lazy regime by forcing the weights to\nevolve to O(1/α), when ﬁtting a target of order 1.\nIn both cases, the solution found by GD is sparse, in the sense that is supported on a ﬁnite number of\nneurons – in other words, the measure γ(θ) becomes atomic, satisfying Assumption 1. Furthermore,\nwe ﬁnd that\n1. For Min-L1, the generalization error prediction holds (Fig. 4 and Fig. G.1) as the the\nminimal norm solution if effectively recovered, see Fig. G.2. Such clean results in terms of\nfeatures position are difﬁcult to achieve for large n because the training dynamics becomes\nvery slow and reaching convergence becomes computationally infeasible. Still, we observe\nthe test error to plateau and reach its inﬁnite-time limit much earlier than the parameters,\nwhich allows for the scaling predictions to hold.\n2. α-trick, however, does not recover the minimal-norm solution, Fig. G.2. Still, the solution\nfound is of the type (2.7) as it is sparse and supported on a number of atoms that scales lin-\nearly with n, Fig. G.3, left. For this reason, we ﬁnd that our predictions for the generalization\nerror hold also in this case, see Fig. G.3, right.\nLazy Learning\nIn this case, the correspondence between the solution found by gradient descent\nand the minimal-norm one is well established [9]. Therefore, numerical experiments are performed\nhere via kernel regression and the analytical NTK Eq. A.19: given a dataset {xi, yi = f ∗(xi)}n\ni=1,\nwe deﬁne the gram matrix K ∈Rn×n with elements Kij = K(xi, xj) and the vector of target labels\ny = [y1, y2, . . . , yn]. The qi’s in Eq. 2.9 can be easily recovered by solving the linear system\ny = 1\nnKq.\n(G.3)\nExperiments\nNumerical experiments are run with PyTorch on GPUs NVIDIA V100 (univer-\nsity internal cluster). Details for reproducing experiments are provided in the code repository,\nexperiments.md ﬁle. Individual trainings are run in 1 minute to 1 hour of wall time. We estimate\na total of a thousand hours of computing time for running the preliminary and actual experiments\npresent in this work.\n23\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n116\nOverﬁtting in Feature Learning\n1\n2\n3\n4\n5\n6\nνt\n1\n2\n3\nβ\nfeature\nlazy\ntarget\n101\n102\n103\n10−3\n10−1\ntest error\nfeature, νt = 1.5\nexperiments\nf n : n−1.75\nf ∗: n−0.75\n101\n102\n103\n10−4\n10−1\nfeature, νt = 4.5\nexperiments\nf n : n−1.75\nf ∗: n−2.25\n101\n102\n103\n104\ntrainset size, n\n10−4\n10−1\ntest error\nlazy, νt = 1.5\nexperiments\nf n : n−2.50\nf ∗: n−0.75\n101\n102\n103\n104\ntrainset size, n\n10−6\n10−3\n100\nlazy, νt = 6.5\nexperiments\nf n : n−2.50\nf ∗: n−3.25\nFigure G.1: Gen. error decay vs. target smoothness and training regime. Here, data-points are\nsampled uniformly from the spherical surface in d = 5 and the target function is an inﬁnite-width\nFCN with activation function σ(·) = | · |νt−1/2, corresponding to a Gaussian random process of\nsmoothness νt. 1strow: gen. error decay exponent as a function of the target smoothness νt. The\nthree curves correspond to the target contribution to the generalization error (black) and the predictor\ncontribution in either feature (blue) or lazy (orange) regime. Full lines highlight the dominating\ncontributions to the gen. error. 2ndrow: agreement between predictions and experiments in the feature\nregime for a non-smooth (left) and smooth (right) target. In the ﬁrst case, the error is dominated by\nthe target f ∗, in the second by the predictor f n – predicted exponents β are indicated in the legends.\n3rdrow: analogous of the previous row for the lazy regime.\n0\nπ/2\nπ\n3π/2\n2π\nneurons angle\n0\n5\n10\n15\nneurons norm\nn = 4\nBasis Pursuit, norm=1.881\nGD: Min-L1, norm=1.881\nGD: α-trick, norm=2.012\n0\nπ/2\nπ\n3π/2\n2π\nneurons angle\nn = 8\nBasis Pursuit, norm=2.937\nGD: Min-L1, norm=2.937\nGD: α-trick, norm=3.270\nFigure G.2: Comparing solutions. Solutions to the spherically symmetric task in d = 2 for n = 4\n(left) and n = 8 (right) training points. In red the minimal norm solution (Eq. 2.5) as found by\nBasis Pursuit [50]. Solutions found by GD in the Min-L1 and α-trick setting are respectively shown\nin blue and orange. Dots correspond to single neurons in the network. The x-axis reports their\nangular position while the y-axis reports their norm: |wh|∥θh∥2. The total norm of the solutions,\nα\nH\nPH\nh=1 |wh|∥θh∥2, is indicated in the legend.\n24\n117\nOverﬁtting in Feature Learning\n101\n102\n103\ntrainset size, n\n101\n102\n103\nnumber of atoms\nd = 2\nd = 3\nd = 5\nn\n101\n102\n103\ntrainset size, n\n10−10\n10−8\n10−6\n10−4\n10−2\n100\ntest error\nd = 2\nd = 3\nd = 5\nGD w/ α →0\nprediction β = 1 +\n3\nd−1\nFigure G.3: Solution found by the α-trick. We consider here the case of approximating the constant\ntarget function on Sd−1 with an FCN. Training is performed starting from small initialization through\nthe α-trick. Left: Number of atoms nA as a function of the number of training points n. Neurons that\nare active on the same subset of the training set are grouped together and we consider each group a\ndistinct atom for the counting. Right: Generalization error in the same setting (full), together with\nthe theoretical predictions (dashed). Different colors correspond to different input dimensions. The\ncase of d = 2 and large n suffers from the same ﬁnite time effects discussed in Fig. 4. Results are\naveraged over 10 different initializations of the networks and datasets.\nH\nSensitivity of the predictor to transformations other than\ndiffeomorphisms\nThis section reports experiments to integrate the discussion of section 5. In particular, we: (i) show\nthat the lazy regime predictor is less sensitive to image translations than the feature regime one (as\nis the case for deformations, from Fig. 6); (ii) provide evidence of the positive effects of learning\nfeatures in image classiﬁcations, namely becoming invariant to pixels at the border of images which\nare unrelated to the task.\nTo prove the above points we consider, as in Fig. 6, the relative sensitivity of the predictors of lazy\nand feature regime with respect to global translations for point (i) and corruption of the boundary\npixels for point (ii). The relative sensitivity to translations is obtained from Eq. 5.1 after replacing\nthe transformation τ with a one-pixel translation of the image in a random direction. For the relative\nsensitivity to boundary corruption, the transformation consists in adding zero-mean and unit-variance\nGaussian numbers to the boundary pixels. Both relative sensitivities are plotted in Fig. H.1, with\ntranslations on the left and boundary pixels corruption on the right.\nIn section 5 we then argue that differences in performance between the two training regimes can\nbe explained by gaps in sensitivities with respect to input transformations that do not change the\nlabel. For (i), the gap is similar to the one observed for diffeomorphisms (Fig. 6). Still, the space of\ntranslations has negligible size with respect to input space, hence we expect the diffeomorphisms\nto have a more prominent effect. In case (ii), the feature regime is less sensitive with respect to\nirrelevant pixels corruption and this would give it an advantage over the lazy regime. The fact that the\nperformance difference is in favor of the lazy regime instead, means that these transformations only\nplay a minor role.\n25\nChapter 4. When Feature Learning Fails: Deformation Invariance deteriorates with\ntraining in Fully-Connected Networks\n118\nOverﬁtting in Feature Learning\n102\n103\n104\ntrain set size, n\n100\n101\nRf to small translations\nFeature\nLazy\n102\n103\n104\ntrain set size, n\n100\n2 × 10−1\n3 × 10−1\n4 × 10−1\n6 × 10−1\nRf to border pixels noise\nFeature\nLazy\nFigure H.1: Sensitivity to input transformations vs number of training points. Relative sensitiv-\nity of the predictor to (left) random 1-pixel translations and (right) white noise added at the boundary\nof the input images, in the two regimes, for varying number of training points n and when training on\nFashionMNIST. Smaller values correspond to a smoother predictor, on average. Results are computed\nusing the same predictors as in Fig. 1. Left: For small translations, the behavior is the same compared\nto applying diffeomorphisms. Right: The lazy regime does not distinguish between noise added at\nthe boundary or on the whole image (Rf = 1), while the feature regime gets more insensitive to the\nformer.\nI\nMaximum-entropy model of diffeomorphisms\nWe brieﬂy review here the maximum-entropy model of diffeomorphisms as introduced in [49].\nAn image can be thought of as a function x(s) describing intensity in position s = (u, v) ∈[0, 1]2,\nwhere u and v are the horizontal and vertical (pixel) coordinates. Denote τx the image deformed by\nτ, i.e. [τx](s) = x(s −τ(s)). [49] propose an ensemble of diffeomorphisms τ(s) = (τu, τv) with\ni.i.d. τu and τv deﬁned as\nτu =\nX\ni,j∈N+\nCij sin(iπu) sin(jπv)\n(I.1)\nwhere the Cij’s are Gaussian variables of zero mean and variance T/(i2 + j2) and T is a parameter\ncontrolling the deformation magnitude. Once τ is generated, pixels are displaced to random positions.\nSee Fig. 5b for an example of such transformation.\n26\n119\n5 When Feature Learning Succeeds:\nHow Deformation Invariance is\nlearned in Convolutional Neural\nNetworks\nThe following paper is the preprint version of Tomasini et al. (2022a) appeared at the ICLR\n2023 Workshop on Physics for Machine Learning and is currently under review.\nCandidate contributions\nThe candidate contributed to all discussions and led research\nregarding the first part of the paper (Sections 2 and 3).\n121\nHOW\nDEEP\nCONVOLUTIONAL\nNEURAL\nNETWORKS\nLOSE SPATIAL INFORMATION WITH TRAINING\nUmberto M. Tomasini ∗, Leonardo Petrini ∗, Francesco Cagnetta, Matthieu Wyart\nInstitute of Physics\n´Ecole Polytechnique F´ed´erale de Lausanne\nname.surname@epfl.ch\nABSTRACT\nA central question of machine learning is how deep nets manage to learn tasks\nin high dimensions. An appealing hypothesis is that they achieve this feat by\nbuilding a representation of the data where information irrelevant to the task is\nlost. For image datasets, this view is supported by the observation that after (and\nnot before) training, the neural representation becomes less and less sensitive\nto diffeomorphisms acting on images as the signal propagates through the net.\nThis loss of sensitivity correlates with performance, and surprisingly correlates\nwith a gain of sensitivity to white noise acquired during training. These facts are\nunexplained, and as we demonstrate still hold when white noise is added to the\nimages of the training set. Here, we (i) show empirically for various architectures\nthat stability to image diffeomorphisms is achieved by both spatial and channel\npooling, (ii) introduce a model scale-detection task which reproduces our empirical\nobservations on spatial pooling and (iii) compute analitically how the sensitivity to\ndiffeomorphisms and noise scales with depth due to spatial pooling. The scalings\nare found to depend on the presence of strides in the net architecture. We ﬁnd that\nthe increased sensitivity to noise is due to the perturbing noise piling up during\npooling, after being rectiﬁed by ReLU units.\n1\nINTRODUCTION\nDeep learning algorithms can be successfully trained to solve a large variety of tasks (Amodei et al.,\n2016; Huval et al., 2015; Mnih et al., 2013; Shi et al., 2016; Silver et al., 2017), often revolving\naround classifying data in high-dimensional spaces. If there was little structure in the data, the\nlearning procedure would be cursed by the dimension of these spaces: achieving good performances\nwould require an astronomical number of training data (Luxburg & Bousquet, 2004). Consequently,\nreal datasets must have a speciﬁc internal structure that can be learned with fewer examples. It has\nbeen then hypothesized that the effectiveness of deep learning lies in its ability of building ‘good’\nrepresentations of this internal structure, which are insensitive to aspects of the data not related to\nthe task (Ansuini et al., 2019; Shwartz-Ziv & Tishby, 2017; Recanatesi et al., 2019), thus effectively\nreducing the dimensionality of the problem.\nIn the context of image classiﬁcation, Bruna & Mallat (2013); Mallat (2016) proposed that neural\nnetworks lose irrelevant information by learning representations that are insensitive to small defor-\nmations of the input, also called diffeomorphisms. This idea was tested in modern deep networks\nby Petrini et al. (2021), who introduced the following measures\nDf = Ex,τ∥f(τ(x)) −f(x)∥2\nEx1,x2∥f(x1) −f(x2)∥2 ,\nGf = Ex,η∥f(x + η) −f(x)∥2\nEx1,x2∥f(x1) −f(x2)∥2 ,\nRf = Df\nGf\n,\n(1)\nto probe the sensitivity of a function f—either the output or an internal representation of a trained\nnetwork—to random diffeomorphisms τ of x (see example in Fig. 1, left), to large white noise\nperturbations η of magnitude ∥τ(x) −x∥, and in relative terms, respectively. Here the input images\nx, x1 and x2 are sampled uniformly from the test set. In particular, the test error of trained networks\n∗Equal contribution.\n1\narXiv:2210.01506v2  [cs.LG]  23 Nov 2022\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n122\nτ\n*\nFigure 1: Left: example of a random diffeomorphism τ applied to an image. Center: test er-\nror vs relative sensitivity to diffeomorphisms of the predictor for a set of networks trained on\nCIFAR10, adapted from Petrini et al. (2021).\nRight: Correlation coefﬁcient between test er-\nror ϵ and Df, Gf and Rf when training different architectures on noisy CIFAR10, ρ(ϵ, X) =\nCov(log ϵ, log X)/\np\nVar(log ϵ)Var(log X). Increasing noise magnitudes are shown on the x-axis\nand η∗= Eτ,x∥τ(x) −x∥2 is the one used for the computation of Gf. Samples of a noisy CIFAR10\ndatum are shown on top. Notice that Df and particularly Rf are positively correlated with ϵ, whilst\nGf is negatively correlated with ϵ. The corresponding scatter plots are in Fig. 10 (appendix).\nch 1\nch 2\nfilters w\ninput x\nw · x\n= 1.1\n= 0.2\n1.3\nch 1\nch 2\n(a) Spatial pooling\n(b) Channel pooling\navg. pooling\nsize = 2x2\nstride = 1\nrotated\ninput x\n= 0.2\n= 1.1\n1.3\n7  6.5\n6  6.5   \n8  7   5\n4  9   5\n2  9   3\n+\n+\n·\n·\nFigure 2: Spatial vs. channel pooling. (a) Spatial average pooling (size 2x2, stride 1) computed on\na representation of size 3x3. One can notice that nearby pixel variations are smaller after pooling.\n(b) If the ﬁlters of different channels are identical up to e.g. a rotation of angle θ, then, averaging\nthe output of the application of such ﬁlters makes the result invariant to input rotations of θ. This\naveraging is an example of channel pooling.\nis correlated with Df when f is the network output. Less intuitively, the test error is anti-correlated\nwith the sensitivity to white noise Gf. Overall, it is the relative sensitivity Rf which correlates\nbest with the error (Fig. 1, middle). This correlation is learned over training—as it is not seen at\ninitialization—and built up layer by layer (Petrini et al., 2021). These phenomena are not simply due\nto benchmark data being noiseless, as they persist when input images are corrupted by some small\nnoise (Fig. 1, right).\nOperations that grant insensitivity to diffeomorphisms in a deep network have been identiﬁed\npreviously (e.g. Goodfellow et al. (2016), section 9.3, sketched in Fig. 2). The ﬁrst, spatial pooling,\nintegrates local patches within the image, thus losing the exact location of its features. The second,\nchannel pooling, requires the interaction of different channels, which allows the network to become\ninvariant to any local transformation by properly learning ﬁlters that are transformed versions of one\nanother. However, it is not clear whether these operations are actually learned by deep networks\nand how they conspire in building good representations. Here we tackle this question by unveiling\nempirically the emergence of spatial and channel pooling, and disentangling their role. Below is a\ndetailed list of our contributions.\n2\n123\n1.1\nOUR CONTRIBUTIONS\n• We disentangle the role of spatial and channel pooling within deep networks trained on\nCIFAR10 (Section 2). More speciﬁcally, our experiments reveal the signiﬁcant contribution\nof spatial pooling in decreasing the sensitivity to diffeomorphisms.\n• In order to isolate the contribution of spatial pooling and quantify its relation with the\nsensitivities to diffeomorphism and noise, we introduce idealized scale-detection tasks\n(Section 3). In these tasks, data are made of two active pixels and classiﬁed according to\ntheir distance. We ﬁnd the same correlations between test error and sensitivities of trained\nnetworks as found in Petrini et al. (2021). In addition, the neural networks which perform\nthe best on real data tend to be the best on these tasks.\n• We theoretically analyze how simple CNNs, made by stacking convolutional layers with\nﬁlter size F and stride s, learn these tasks (Section 4). We ﬁnd that the trained networks\nperform spatial pooling for most of its layers. We show and verify empirically that the\nsensitivities Dk and Gk of the k-th hidden layer follow Gk ∼Ak and Dk ∼A−αs\nk\n, where\nAk is the effective receptive ﬁeld size and αs = 2 if there is no stride, αs = 1 otherwise.\nThe\ncode\nand\ndetails\nfor\nreproducing\nexperiments\nare\navailable\nonline\nat\ngithub.com/leonardopetrini/relativestability/experiments ICLR23.md.\n1.2\nRELATED WORK\nIn the neuroscience literature, the understanding of the relevance of pooling in building invariant\nrepresentations dates back to the pioneering work of Hubel & Wiesel (1962). By studying the cat\nvisual cortex, they identiﬁed two different kinds of neurons: simple cells responding to e.g. edges at\nspeciﬁc angles and complex cells that pool the response of simple cells and detect edges regardless of\ntheir position or orientation in the receptive ﬁeld. More recent accounts of the importance of learning\ninvariant representations in the visual cortex can be found in Niyogi et al. (1998); Anselmi et al.\n(2016); Poggio & Anselmi (2016).\nIn the context of artiﬁcial neural networks, layers jointly performing spatial pooling and strides\nhave been introduced with the early CNNs of Lecun et al. (1998), following the intuition that local\naveraging and subsampling would reduce the sensitivity to small input shifts. Ruderman et al. (2018)\ninvestigated the role of spatial pooling and showed empirically that networks with and without pooling\nlayers converge to similar deformation stability, suggesting that spatial pooling can be learned in\ndeep networks. In our work, we further expand in this direction by jointly studying diffeomorphisms\nand noise stability and proposing a theory of spatial pooling for a simple task.\nThe depth-wise loss of irrelevant information in deep networks has been investigated by means of the\ninformation bottleneck framework (Shwartz-Ziv & Tishby, 2017; Saxe et al., 2019) and the intrinsic\ndimension of the networks internal representations (Ansuini et al., 2019; Recanatesi et al., 2019).\nHowever, these works do not specify what is the irrelevant information to be disregarded, nor the\nmechanisms involved in such a process.\nThe stability of trained networks to noise is extensively studied in the context of adversarial robust-\nness (Fawzi & Frossard, 2015; Kanbak et al., 2018; Alcorn et al., 2019; Alaifari et al., 2018; Athalye\net al., 2018; Xiao et al., 2018a; Engstrom et al., 2019). Notice that our work differs from this literature\nby the fact that we consider typical perturbations instead of worst-case ones.\n2\nEMPIRICAL OBSERVATIONS ON REAL DATA\nIn this section we analyze the parameters of deep CNNs trained on CIFAR10 and ImageNet, so as to\nunderstand how they build representations insensitive to diffeomorphisms (details of the experiments\nin App. B). The analysis builds on two premises, the ﬁrst being the assumption that insensitivity is\nbuilt layer by layer in the network, as shown in Fig. 3. Hence, we focus on how each of the layers\nin a deep network contribute towards creating an insensitive representation. More speciﬁcally, let\nus denote with fk(x) the internal representation of an input x at the k-th layer of the network. The\nentries of fk have three indices, one for the channel c and two for the spatial location (i, j). The\n3\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n124\nrelation between fk and fk−1 is the following,\n[fk(x)]c;i,j = φ\n\nbk\nc +\nHk−1\nX\nc′=1\nwk\nc,c′ · pi,j ([fk−1(x)]c′)\n\n\n∀c = 1, . . . , Hk,\n(2)\nwhere: Hk denotes the number of channels at the k-th layer; bk\nc and wk\nc,c′ the biases and ﬁlters of\nthe k-th layer; each ﬁlter wk\nc,c′ is a F × F matrix with F the ﬁlter size; pi,j ([fk−1(x)]c′) denotes\na F × F-dimensional patch of [fk−1(x)]c′ centered at (i, j); φ the activation function. The second\npremise is that a general diffeomorphism can be represented as a displacement ﬁeld over the image,\nwhich indicates how each pixel moves in the transformation. Locally, this displacement ﬁeld can be\ndecomposed into a constant term and a linear part: the former corresponds to local translations, the\nlatter to stretchings, rotations and shears.1\nInvariance to translations via spatial pooling.\nDue to weight sharing, i.e. the fact that the same\nﬁlter wk\nc,c′ is applied to all the local patches (i, j) of the representation, the output of a convolutional\nlayer is equivariant to translations by construction: a shift of the input is equivalent to a shift of the\noutput. To achieve an invariant representation it sufﬁces to sum up the spatial entries of fk—an\noperation called pooling in CNNs, we refer to it as spatial pooling to stress that the sum runs over\nthe spatial indices of the representation. Even if there are no pooling layers at initialization, they\ncan be realized by having homogeneous ﬁlters, i.e. all the F × F entries of wk+1\nc,c′ are the same.\nTherefore, the closer the ﬁlters are to the homogeneous ﬁlter, the more they decrease the sensitivity\nof the representation to local translations.\nInvariance to other transformations via channel pooling.\nThe example of translations shows\nthat building invariance can be performed by constructing an equivariant representation, and then\npooling it. Invariance can also be built by pooling across channels. A two-channel example is shown\nFig. 2, panel (b), where the ﬁlter of the second channel is built so as to produce the same output as\nthe ﬁrst channel when applied to a rotated input. The same idea can be applied more generally, e.g.\nto the other components of diffeomorphisms—such as local stretchings and shears. Below, we refer\ngenerically to any operation that build invariance to diffeomorphisms by assembling distinct channels\nas channel pooling.\nDisentangling spatial and channel pooling.\nThe relative sensitivity to diffeomorphisms Rk of the\nk-th layer representation fk decreases after each layer, as shown in Fig. 3. This implies that spatial\nor channel pooling are carried out along the whole network. To disentangle their contribution we\nperform the following experiment: shufﬂe at random the connections between channels of successive\nconvolutional layers, while keeping the weights unaltered. Channel shufﬂing amounts to randomly\npermuting the values of c, c′ in Eq. 2, therefore it breaks any channel pooling while not affecting\nsingle ﬁlters. The values of Rk for deep networks after channel shufﬂing are reported in Fig. 3 as\ndashed lines and compared with the original values of Rk in full lines. If only spatial pooling was\npresent in the network, then the two curves would overlap. Conversely, if the decrease in Rk was\nall due to the interactions between channels, then the shufﬂed curves should be constant. Given that\nneither of these scenarios arises, we conclude that both kinds of pooling are being performed.\nEmergence of spatial pooling after training.\nTo bolster the evidence for the presence of spatial\npooling, we analyze the ﬁlters of trained networks. Since spatial pooling can be built by having\nhomogeneous ﬁlters, we test for its presence by looking at the frequency content of learned ﬁlters\nwk\ni,j. In particular, we consider the average squared projection of ﬁlters onto “Fourier modes”\n{Ψl}l=1,...,F 2, taken as the eigenvectors of the discrete Laplace operator on the F × F ﬁlter grid.\nThe square projections averaged over channels read\nγk,l =\n1\nHk−1Hk\nHk\nX\nc=1\nHk−1\nX\nc′=1\n\u0002\nΨl · wk\nc,c′\n\u00032,\n(3)\n1The displacement ﬁeld around a pixel (u0, v0) is approximated as τ(u, v) ≃τ(u0, v0) + J(u0, v0)[u −\nu0, v −v0]T , where τ(u0, v0) corresponds to translations and J is the Jacobian matrix of τ whose trace,\nantisymmetric and symmetric traceless parts correspond to stretchings, rotations and shears, respectively.\n4\n125\nand are shown in Fig. 4, 1stand 2nd row. When training a deep network such as VGG11 (with and\nwithout batch-norm) (Simonyan & Zisserman, 2015) on CIFAR10, ﬁlters of layers 2 to 6 become\nlow-frequency with training, while layers 1, 7, 8 do not. Accordingly, larger gaps between dashed\nand full lines in Fig. 3 (right) open at layer 1, 7, 8: reduction in sensitivity is not due to spatial pooling\nin these layers. Moreover, the fact that the two dashed curves overlap is consistent with the frequency\ncontent of ﬁlters being the same for the two architectures after training. In the case of ImageNet,\nﬁlters at all layers become low-frequency, except for k = 1.\n1\n2\n3\n4\n5\n6\n7\n8\nconvolutional layer index k\n100\nRk\nCIFAR10\nVGG11\nVGG11bn\noriginal\nshuffle ch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrelative depth k\nK\n10\n1\n100\nImageNet\nvgg11\nvgg11bn\nvgg13\nvgg16\noriginal\nshuffle ch.\nFigure 3: Relative sensitivity Rk as a function of depth for VGG architectures trained on CIFAR10\n(left) and ImageNet (right). Full lines refer to the original networks, dashed lines to the ones with\nshufﬂed channels. K is the total depth of the networks. Experiments with different architectures are\nreported in App. C.\n10\n1\nk = 1\nk = 2\nk = 3\nk = 4\nk = 5\nk = 6\nk = 7\nCIFAR10\nk = 8\n10\n1\nImageNet\n0\n2\n4\n6\n8\n10\n1\n2 × 10\n1\n3 × 10\n1\n0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8\nscale-detection\nVGG11\nVGG11bn\ninit.\ntrained\neigenvalue index, l\nk, l\nFigure 4: Projections of the network ﬁlters for VGG11 and VGG11bn onto the 9 eigenvectors\nof the (3 × 3)-grid Laplacian when training on CIFAR10 (1st row), ImageNet, (2nd row) and the\nscale-detection task (3rd row): dotted and full lines correspond to initialization and trained networks,\nrespectively. The x-axis reports low to high frequencies from left to right. Deeper layers are reported\nin rightmost panels. Low-frequency modes are the dominant components in layers 2-6 when training\non CIFAR10, in layers 2-8 for ImageNet. The ﬁrst (constant) mode has most of the power throughout\nthe network for scale-detection task 1. An aggregate measure of the spatial frequency content of\nﬁlters is reported in App. C, Fig. 12.\n3\nSIMPLE SCALE-DETECTION TASKS CAPTURE REAL-DATA OBSERVATIONS\nTo sum up, the empirical evidence presented in Section 2 indicates that (i) the generalization\nperformance of deep CNNs correlates with their insensitivity to diffeomorphisms and sensitivity to\nGaussian noise (Fig. 1); (ii) deep CNNs build their sensitivities layer by layer via spatial and channel\npooling. We introduce now two idealized scale-detection tasks where the phenomena (i) and (ii)\nemerge again, and we can isolate the contribution of spatial pooling. Given the simpler structure of\nthese tasks with respect to real data, we can understand quantitatively how spatial pooling builds up\ninsensitivity to diffeomorphisms and sensitivity to Gaussian noise, as we show in Section 4.\n5\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n126\ny = sign(ξ −d) = −1 y = sign(ξ −d) = +1\nd\nd\ny = +1\ny = –1\nDifferent patches, \nSame patch, \n(b)\n(a)\nFigure 5: Example inputs for the scale-detection tasks. Task 1 (a): the label depends on whether\nthe euclidean distance d is larger (left) or smaller (right) than the characteristic scale ξ. Task 2\n(b): the label depends on whether the active pixels belong to the same patch of size ξ (right) or not\n(left)—patches are shown in different colors.\nDeﬁnition of scale-detection tasks.\nConsider input images x consisting of two active pixels on an\nempty background.\nTask 1: Inputs are classiﬁed by comparing the euclidean distance d between the two active pixels\nand some characteristic scale ξ, as in Fig. 5, left. Namely, the label is y = sign (ξ −d).\nNotice that a small diffeomorphism of such images corresponds to a small displacement of the active\npixels. Speciﬁcally, each of the active pixels is moved to either of its neighboring pixels or left in\nits original position with equal probability.2 By introducing a gap g such that d ∈[ξ −g/2, ξ + g/2],\ntask 1 becomes invariant to displacements of size smaller than g. Therefore, we expect that a neural\nnetwork trained on task 1 will lose any information on the exact location of the active pixels within\nthe image, thus becoming insensitive to diffeomorphisms. Intuitively, spatial pooling up to the scale ξ\nis the most direct mean to achieve such insensitivity. The result of the integration depends on whether\nnone, one or both the active pixels lie within the pooling window, thus it is still informative of the\ntask. We will show empirically that this is indeed the solution reached by trained CNNs.\nTask 2: Inputs are partitioned into nonoverlapping patches of size ξ, as in Fig. 5, right. The label y\nis +1 if the active pixels fall within the same patch, −1 otherwise.\nIn task 2, the irrelevant information is the location of the pixels within each of the non-overlapping\npatches. The simplest means to lose such information requires to couple spatial pooling with a stride\nof the size of the pooling window itself.\nSame phenomenology as in real image datasets.\nAlthough these scale-detection tasks are much\nsimpler than standard benchmark datasets, deep networks trained on task 1 display the same\nphenomenology highlighted in Section 2 for networks trained on CIFAR10 and ImageNet. First,\nthe test error is positively correlated with the sensitivity to diffeomorphisms of the network predictor\n(Fig. 8, left panel, in App. C) and negatively correlated with its sensitivity to Gaussian noise (middle\npanel) for a whole range of architectures. As a result, the error correlates well with the relative\nsensitivity Rf (right panel). Secondly, the internal representations of trained networks fk become\nprogressively insensitive to diffeomorphisms and sensitive to Gaussian noise through the layers, as\nshown in Fig. 9 of App. C. Importantly, the curves relating sensitivities to the relative depth remain\nessentially unaltered if the channels of the networks are shufﬂed (shown as dashed lines in Fig. 9).\nWe conclude that, on the one hand channel pooling is negligible, and, on the other hand, all channels\nare approximately equal to the mean channel. Finally, direct inspection of the ﬁlters (Fig. 4, bottom\nrow) shows that the 0-frequency component grows much larger than the others over training for\nlayers 1-7, which are the layers where Rk decreases the most in Fig. 9. Filters are thus becoming\nnearly homogeneous, which means that the convolutional layers become effectively pooling layers.\n2We ﬁx the length of these displacements to 1 pixel because (i) is the smallest value that prevents the use of\npixel interpolation, which would make one active pixel an extended object (ii) allows for the analysis of Section 4.\n6\n127\n2layer\n√layer\nFigure 6: Hidden layers representations of simple CNNs for a scale-detection input for stride s = 1\nand ﬁlter size F = 5 (left) and s = F = 2 (right) when having homogeneous ﬁlters at every layer. The\neffective receptive ﬁeld size of the last layer in the two different cases is shown in red. (Left) every\nactive pixel in the input becomes a Gaussian proﬁle whose width increases throughout the network.\n(Right) every neuron in layer k has activity equal to the number of active pixels which are present in\nits receptive ﬁeld of width 2k. The dark blue in the last layer indicates that there are 2 active pixels in\nits receptive ﬁeld, while the lighter blue of the precedent layers indicates that there is just 1.\n4\nTHEORETICAL ANALYSIS OF SENSITIVITIES IN SCALE-DETECTION TASKS\nWe now provide a scaling analysis of the sensitivities to diffeomorphisms and noise in the internal\nrepresentations of simple CNNs trained on the scale-detection tasks of Section 3. It allows to\nquantitatively understand how spatial pooling makes the internal representations of the network\nprogressively more insensitive to diffeomorphisms and sensitive to Gaussian noise.\nSetup.\nWe consider simple CNNs made by stacking ˜K identical convolutional layers with generic\nﬁlter size F, stride s = 1 or F and ReLU activation function φ(x) = max(0, x). In particular, we\ntrain CNNs with stride 1 on task 1 and CNNs with stride F on task 2. For the sake of simplicity, we\nconsider the one-dimensional version of the scale-detection tasks, but our analysis carries unaltered\nto the two-dimensional case. Thus, input images are sequences x = (xi)i=1,...,L of L pixels, where\nxi = 0 for all pixels except two. For the active pixels xi =\np\nL/2, so that all input images have\n∥x∥2 = L. We will also consider single-pixel data δj = (δj,i)i=1,...,L. If the active pixels in x are the\ni-th and the j-th, then x =\np\nL/2 (δi + δj). For each layer k, the internal representation fk(x) of the\ntrained network is deﬁned as in Eq. 2. The receptive ﬁeld of the k-th layer is the number of input\npixels contributing to each component of fk(x). We deﬁne the effective receptive ﬁeld Ak as the\ntypical size of the representation of a single-pixel input, fk(δi), as illustrated in red in Fig. 6. We\ndenote the sensitivities of the k-th layer representation with a subscript k (Dk for diffeomorphisms,\nGk for noise, Rk for relative).\nAssumptions.\nAll our results are based on the assumption that the ﬁrst few layers of the trained\nnetwork behave effectively as a single channel with a homogeneous positive ﬁlter and no bias. The\nequivalence of all the channels with their mean is supported by Fig. 9, which shows how shufﬂing\nchannels does not affect the internal representations of VGGs. In addition, Fig. 4 (bottom row) shows\nthat the mean ﬁlters of the ﬁrst few layers are nearly homogeneous. We set the homogeneous value\nof each ﬁlter so as to keep the norm of representations constant over layers. Moreover, we implement\na deformation of the input x of our scale-detection tasks as a random displacement of each active\npixel at either left or wight with probability 1/2.\n4.1\nTASK 1, STRIDE 1\nFor a CNN with stride 1, under the homogeneous ﬁlter assumption, the size of the effective receptive\nﬁeld Ak grows as\n√\nk. A detailed proof is presented in App. A and Fig. 6, left panel, shows an\nillustration of the process. Intuitively, applying a homogeneous ﬁlter to a representation is equivalent\nto making each pixel diffuse, i.e. distributing its intensity uniformly over a neighborhood of size\nF. With a single-pixel input δi, the effective receptive ﬁeld of the k-th layer fk(δi) is equivalent\nto a k-step diffusion of the pixel, thus it approaches a Gaussian distribution of standard deviation\n√\nk centered at i. The size Ak is the standard deviation, thus Ak ∼\n√\nk. The proof we present\n7\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n128\nin App. A requires large depth ˜K ≫1 and large image width L ≫F ˜K1/2 and the empirical studies\nof Section 3 satisfy these contraints (F ∼3, L ∼32 and ˜K ∼10).\nWe remark that at initialization, fk(x) behave, in the limit of large number of channels and width (and\nsmall bias), as Gaussian random ﬁelds with correlation matrix E [fk(x)fk(y)] ≈δ(x −y), with δ the\nDirac delta (Schoenholz et al., 2017; Xiao et al., 2018b). This spiky correlation matrix implies that\nfor any perturbation y = x + ε, the representation fk(y) changes with respect to fk(x) independently\non ε. This behavior is remarkably different to the smooth case achieved by the diffusion, after training.\nConsequently, both Dk and Gk are constant with respect to k at initialization . This is consistent with\nthe observations reported in Fig. 7.\nSensitivity to diffeomorphisms.\nLet i and j denote the active pixels locations, so that x ∝δi + δj.\nSince both the elements of the inputs and those of the ﬁlters are non-negative, the presence of ReLU\nnonlinearities is irrelevant and the ﬁrst few hidden layers are effectively linear layers. Hence the\nrepresentations are linear in the input, so that fk(x) = fk(δi + δj) = fk(δi) + fk(δj). In addition,\nsince the effect of a diffeomorphism is just a 1-pixel translation of the representation irrespective of\nthe original positions of the pixels, the normalized sensitivity Dk can be approximated as follows\nDk ∼∥fk(δi+1) −fk(δi)∥2\n2\n∥fk(δi)∥2\n2\n.\n(4)\nThe denominator in Eq. 4 is the squared norm of a Gaussian distribution of width\n√\nk, ∥fk(vi)∥2\n2 ∼\nk−1/2. The numerator compares fk with a small translation of itself, thus it can be approximated\nby the squared norm of the derivative of the Gaussian distribution, ∥fk(δi+1) −fk(δi)∥2\n2 ∼k−3/2.\nConsequently, we have\nDk ∼k−1 ∼A−2\nk .\n(5)\nSensitivity to Gaussian noise.\nTo analyze Gk one must take into account the rectifying action of\nReLU, which sets all the negative elements of its input to zero. The ﬁrst ReLU is applied after the\nﬁrst homogeneous ﬁlters, thus the zero-mean noise is superimposed on a patch of F active pixels.\nOutside such a patch, only positive noise terms survive. Within the patch, being summed to a positive\nbackground, also negative terms can survive the rectiﬁcation of ReLU. Nevertheless, if the size of the\nimage is much larger than the ﬁlter size, the contribution from active pixels to Gk is negligible and we\ncan approximate the difference between noisy and original representations f1(x + η) −f1(x) with\nthe rectiﬁed noise φ(η). After the ﬁrst layer, the representations consist of non-negative numbers,\nthus we can forget again the ReLU and write\nGk ∼Eη∥fk(φ(η))∥2\n2\n∥fk(δi)∥2\n2\n.\n(6)\nRepeated applications of homogeneous ﬁlters to the rectiﬁed noise φ(η) result again in a diffusion\nof the signal. Since φ(η) has different independent and identically distributed non-zero entries for\ndifferent realizations of η, averaging over η is equivalent to considering a homogeneous proﬁle for\nfk(φ(η)). As a result, the numerator in Eq. 6 is a constant independent of k. The denominator is the\nsame as in Eq. 4, ∥fk(δi)∥2\n2 ∼k−1/2, hence\nGk ∼k1/2 ∼Ak,\n(7)\ni.e. the sensitivity to Gaussian noise grows as the size of the effective receptive ﬁelds. From the ratio\nof Eq. 5 and Eq. 7, we get Rk ∼A−3\nk .\n4.2\nTASK 2, STRIDE EQUAL FILTER SIZE\nWhen the stride s equals to the ﬁlter size F the number of pixels of the internal representations is\nreduced by a factor F at each layer, thus fk consists of L/F k pixels. Meanwhile, the effective size of\nthe receptive ﬁelds grows exponentially at the same rate: Ak = F k (see Fig. 6, left for an illustration).\nSensitivity to diffeomorphisms.\nFor a given layer k, consider a partition of the input image into\nL/F k patches. Each pixel of fk only looks at one such patch and its intensity coincides with the\nnumber of active pixels within the patch. As a result, the only diffeomorphisms that change fk are\n8\n129\n100\n2 × 100\n3 × 100\n10−1\nTASK 1, s = 1\nDk\n100\n2 × 100\n3 × 100\n100\n6 × 10−1\n2 × 100\n3 × 100\nGk\n100\n2 × 100\n3 × 100\n10−1\n100\nRk\n101\n102\nAk\n10−3\n10−2\n10−1\n100\nTASK 2, F = s\nDk\nInit\nPrediction\nTrain\nMean channel\n101\n102\nAk\n100\n101\n102\nGk\n101\n102\nAk\n10−3\n10−2\n10−1\n100\nRk\nFigure 7: Sensitivities of internal representations fk of simple CNNs against the k-th layer receptive\nﬁeld size Ak for trained networks (solid blue) and at initialization (solid gray). The top row refers\nto task 1 with s = 1 and F = 3; the bottom row to task 2 with F = s = 2. For a ﬁrst large part\nof the network, the sensitivities obtained by replacing each layer with the mean channel (blue dotted)\noverlap with the original sensitivities. Predictions Eq. 5, Eq. 7 for task 1 and Eq. 8, Eq. 9 for task\n2 are shown as black dashed lines.\nthose which move one of the active pixels from one patch to another. Since active pixels move by 1,\nthis can only occur if one of the active pixels was originally located at the border of a patch, which in\nturn occurs with probability ∼1/F k. In addition, the norm ∥fk(δi)∥2 at the denominator does not\nscale with k, so that\nDk ∼F −k ∼A−1\nk .\n(8)\nSensitivity to Gaussian noise.\nEach pixel of fk looks at a patch of the input of size F k, thus fk\nis affected by the sum of all the noises acting on such patch. Since these noises have been rectiﬁed\nby ReLU, by the Central Limit Theorem the sum scales as the number of summands Fk. Thus, the\ncontribution of each pixel of fk to the numerator of Gk scales as (F k)2. As there are L/F k pixels\nin fk, one has\nGk ∼(F k)2 \u0000L/F k\u0001\n∼F k ∼Ak.\n(9)\nWithout rectiﬁcation, the sum of F k independent noises would scale as the square root of the number\nof summands F k, yielding a constant Gk. We conclude that the rectifying action of ReLU is crucial\nin building up sensitivity to noise. Rk ∼A−2\nk\nfollows from the ratio of Eq. 8 and Eq. 9.\n4.3\nCOMPARING PREDICTIONS WITH EXPERIMENTS\nWe test our scaling predictions (Eq. 5 to Eq. 9) in Fig. 7, for stride 1 CNNs trained on task 1 and\nstride F CNNs trained on task 2 in the top and bottom panels, respectively. Notice that if all the ﬁlters\nat a given layer are replaced with their average, the behavior of the sensitivities as a function of depth\ndoes not change (compare solid and dotted blue curves in the ﬁgure). This conﬁrms our assumption\nthat all channels behave like the mean channel. In addition, Tables 1 and 2 show that the mean ﬁlters\nare approximately homogeneous. Further details on the experiments are provided in App. B.\n5\nCONCLUSION\nThe meaning of an image often depends on sparse regions of the data, as evidenced by the fact\nthat artists only need a small number of strokes to represent a visual scene. The exact locations\nof the features determining the image class are ﬂexible, and indeed diffeomorphisms of limited\nmagnitude leave the class unchanged. Here, we have shown that such an invariance is learned in\ndeep networks by performing spatial pooling and channel pooling. Modern architectures learn these\npooling operations—as they are not imposed by the architecture—suggesting that it is best to let the\npooling adapt to the speciﬁc task considered. Interestingly, spatial pooling comes together with an\nincreased sensitivity to random noise in the image, as captured in simple artiﬁcial models of data.\n9\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n130\nIt is commonly believed that the best architectures are those that extract the features of the data most\nrelevant for the task. The pooling operations studied here, which allow the network to forget the exact\nlocations of these features, are probably more effective when features are better extracted. This point\nmay be responsible for the observed strong correlations between the network performance and its\nstability to diffeomorphisms. Designing synthetic models of data whose features are combinatorial\nand stable to smooth transformations is very much needed to clarify this relationship, and ultimately\nunderstand how deep networks learn high-dimensional tasks with limited data.\nREFERENCES\nRima Alaifari, Giovanni S. Alberti, and Tandri Gauksson. ADef: an Iterative Algorithm to Construct\nAdversarial Deformations. September 2018. URL https://openreview.net/forum?\nid=Hk4dFjR5K7.\nMichael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen.\nStrike (With) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects. In\n2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4840–4849,\nLong Beach, CA, USA, June 2019. IEEE. ISBN 978-1-72813-293-8. doi: 10.1109/CVPR.2019.\n00498. URL https://ieeexplore.ieee.org/document/8954212/.\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl\nCase, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-\nend speech recognition in english and mandarin. In International conference on machine learning,\npp. 173–182, 2016.\nFabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio.\nUnsupervised learning of invariant representations. Theoretical Computer Science, 633:112–\n121, June 2016.\nISSN 0304-3975.\ndoi: 10.1016/j.tcs.2015.06.048.\nURL https://www.\nsciencedirect.com/science/article/pii/S0304397515005587.\nAlessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan. Intrinsic dimension of data\nrepresentations in deep neural networks. In Advances in Neural Information Processing Systems,\npp. 6111–6122, 2019.\nAnish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing Robust Adversarial\nExamples. In International Conference on Machine Learning, pp. 284–293. PMLR, July 2018.\nURL http://proceedings.mlr.press/v80/athalye18b.html. ISSN: 2640-3498.\nJoan Bruna and St´ephane Mallat. Invariant scattering convolution networks. IEEE transactions on\npattern analysis and machine intelligence, 35(8):1872–1886, 2013.\nLogan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Exploring\nthe Landscape of Spatial Robustness. In International Conference on Machine Learning, pp. 1802–\n1811. PMLR, May 2019. URL http://proceedings.mlr.press/v97/engstrom19a.\nhtml. ISSN: 2640-3498.\nAlhussein Fawzi and Pascal Frossard. Manitest: Are classiﬁers really invariant? In Procedings of\nthe British Machine Vision Conference 2015, pp. 106.1–106.13, Swansea, 2015. British Machine\nVision Association. ISBN 978-1-901725-53-7. doi: 10.5244/C.29.106. URL http://www.\nbmva.org/bmvc/2015/papers/paper106/index.html.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. The MIT Press, Cambridge,\nMassachusetts, November 2016. ISBN 978-0-262-03561-3.\nD. H. Hubel and T. N. Wiesel. Receptive ﬁelds, binocular interaction and functional architecture in the\ncat’s visual cortex. The Journal of Physiology, 160(1):106–154.2, January 1962. ISSN 0022-3751.\nURL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/.\nBrody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo\nAndriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, et al. An empirical evaluation\nof deep learning on highway driving. arXiv preprint arXiv:1504.01716, 2015.\n10\n131\nCan Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Geometric Robustness of\nDeep Networks: Analysis and Improvement. In 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 4441–4449, Salt Lake City, UT, June 2018. IEEE. ISBN 978-1-\n5386-6420-9. doi: 10.1109/CVPR.2018.00467. URL https://ieeexplore.ieee.org/\ndocument/8578565/.\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998. ISSN 1558-2256. doi:\n10.1109/5.726791. Conference Name: Proceedings of the IEEE.\nUlrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.\nJournal of Machine Learning Research, 5(Jun):669–695, 2004.\nSt´ephane Mallat. Understanding deep convolutional networks. Philosophical Transactions of the\nRoyal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150203, 2016.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nP. Niyogi, F. Girosi, and T. Poggio. Incorporating prior information in machine learning by creating\nvirtual examples. Proceedings of the IEEE, 86(11):2196–2209, November 1998. ISSN 1558-2256.\ndoi: 10.1109/5.726787. Conference Name: Proceedings of the IEEE.\nLeonardo Petrini,\nAlessandro Favero,\nMario Geiger,\nand Matthieu Wyart.\nRelative\nstability toward diffeomorphisms indicates performance in deep nets.\nIn Advances\nin Neural Information Processing Systems, volume 34, pp. 8727–8739. Curran Asso-\nciates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n497476fe61816251905e8baafdf54c23-Abstract.html.\nTomaso\nA.\nPoggio\nand\nFabio\nAnselmi.\nVisual\nCortex\nand\nDeep\nNetworks:\nLearning\nInvariant\nRepresentations.\nSeptember\n2016.\ndoi:\n10.7551/mitpress/\n10177.001.0001.\nURL\nhttps://direct.mit.edu/books/book/4088/\nVisual-Cortex-and-Deep-NetworksLearning-Invariant.\nStefano Recanatesi, Matthew Farrell, Madhu Advani, Timothy Moore, Guillaume Lajoie, and Eric\nShea-Brown. Dimensionality compression and expansion in deep neural networks. arXiv preprint\narXiv:1906.00443, 2019.\nHannes Risken. The Fokker-Planck Equation Springer Series in Synergetics. 1996.\nAvraham Ruderman, Neil C. Rabinowitz, Ari S. Morcos, and Daniel Zoran. Pooling is neither\nnecessary nor sufﬁcient for appropriate deformation stability in CNNs. arXiv:1804.04438 [cs,\nstat], May 2018. URL http://arxiv.org/abs/1804.04438. arXiv: 1804.04438.\nLaurent Saloff-Coste and Pierre Bremaud. Markov chains: Gibbs ﬁelds, monte carlo simulation,\nand queues. Journal of the American Statistical Association, 95, 2000. ISSN 01621459. doi:\n10.2307/2669802.\nAndrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D\nTracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of\nStatistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.\nSamuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information\npropagation. 2017.\nBaoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based\nsequence recognition and its application to scene text recognition. IEEE transactions on pattern\nanalysis and machine intelligence, 39(11):2298–2304, 2016.\nRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.\narXiv preprint arXiv:1703.00810, 2017.\n11\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n132\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\nhuman knowledge. nature, 550(7676):354–359, 2017.\nK. Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image\nRecognition. ICLR, 2015.\nChaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially Transformed\nAdversarial Examples. February 2018a. URL https://openreview.net/forum?id=\nHyydRMZC-.\nLechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-\nton. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla\nconvolutional neural networks. volume 12, 2018b.\n12\n133\nAPPENDIX\nA\nTASK 1, STRIDE 1: PROOFS\nIn Section 4.1 we consider a simple CNN with stride s = 1 and ﬁlter size F trained on scale-detection\ntask 1. We ﬁx the total depth of these networks to be ˜K. We postulated in Sec. 4 that this\nnetwork displays a one-channel solution with homogeneous ﬁlter [1/F, ..., 1/F] and no bias. We can\nunderstand the representation fk(x) at layer k of an input datum x by using single-pixel inputs δi.\nLet us recall that these inputs have all components to 0 except the i-th, set to 1. Then, we have that a\ngeneral datum x is given by x ∝(δi + δj), where i and j are the locations of the active pixel in x.\nWe have argued in the main text that the representation fk(δi) is a Gaussian distribution with width\n√\nk. In this Appendix we prove this statement.\nFirst, we observe that in this solution, since both the elements of the ﬁlters and those of the inputs\nare non-negative, the networks behaves effectively as a linear operator. In particular, each layer\ncorresponds to the application of a L × L circulant matrix M, which is obtained by stacking all the L\nshifts of the following row vector,\n[1, 1, ..., 1\n|\n{z\n}\nF\n0, 0, 0, ..., 0\n|\n{z\n}\nL−F\n].\n(10)\nwith periodic boundary conditions. The ﬁrst row of such a matrix is ﬁxed as follows. If F is odd the\npatch of size F is centered on the ﬁrst entry of the ﬁrst row, while if F is even we choose to have\n(F/2) ones at left of the ﬁrst entry and (F/2) −1 at its right. The output fk of the layer k is then the\nfollowing: fk(δi) = M kδi.\nProposition A.1 Let’s consider the L × L matrix M and a given L vector δi, as deﬁned above. For\nodd F ≥3, in the limit of large depth ˜K ≫1 and large width ˜L ≫F\np\n˜K, we have that\n(M k)abδi =\n1\n2√π\n√\nD(1)√\nk\ne−(a−i)2\n4D(1)k ,\nD(1) =\n1\n12F (F −1)3,\n(11)\nwhile for even F:\n(M k)abδi =\n1\n2√π\n√\nD(2)√\nk\ne\n−(v(2)k+a−i)2\n4D(2)\nF\nk\n,\nD(2) =\n1\n12F\n\u0000F 3 −3F 2 + 6F −4\n\u0001\n,\n(12)\nwith v(2) = (1 −F)/(2F).\nProof:\nThe matrix M can be seen as the stochastic matrix of a Markov process, where at each step\nthe random walker has uniform probability 1/F to move in a patch of width F around itself. We\nwrite the following recursion relation for odd F,\np(k+1)\na,i\n= 1\nF\n\u0010\np(k)\na−(F −1)/2,i + ... + p(k)\na,i + ... + p(k)\na+(F −1)/2,i\n\u0011\n,\n(13)\nand even F,\np(k+1)\na,i\n= 1\nF\n\u0010\np(k)\na−F/2,i + ... + p(k)\na,i + ... + p(k)\na+(F/2−1),i\n\u0011\n.\n(14)\nIn any of these two cases, this is the so-called master equation of the random walk (Risken, 1996).\nIn the limit of large image width L and large depth ˜K, we can write the related equation for\nthe continuous process pi(a, k), which is called Fokker-Planck equation in physics and chemistry\n(Risken, 1996) or forward Kolmogorov equation in mathematics (Saloff-Coste & Bremaud, 2000),\n∂kp(k)\na,i = v∂ap(k)\na,i + D∂2\nap(k)\na,i .\n(15)\nwhere the drift coefﬁcient v and the diffusion coefﬁcient D are deﬁned in terms of the probability\ndistribution Wi(x) of having a jump x starting from the location i\nv =\nZ\ndxWi(x)x,\nD =\nZ\ndxWi(x)x2.\n(16)\n13\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n134\nIn our case we have Wi(x) = 1/F for x ∈[i −(F −1)/2, i + (F −1)/2] for odd F and\nx ∈[i −F/2, i + F/2 −1] for even F, yielding the solutions for the Fokker-Planck equations for\neven and odd F reported in Eq. 11 and Eq. 12.\nWe can better characterize the limits of large image width L and large network depth ˜K as follows.\nThe proof relies on the fact that a random walk, after a large number of steps, converges to a diffusion\nprocess. Here the number of steps is given by the depth ˜K of the network. Consequently, we need\n˜K ≫1. Moreover, we want that the diffusion process is not inﬂuenced by the boundaries of the\nimage, of width L. The average path walked by the random walker after ˜K steps is given by F\n√\nK.\nThen, we require F\n√\nK ≪L.\n□\nB\nEXPERIMENTAL SETUP\nAll experiments are performed in PyTorch. The code with the instructions on how to reproduce\nexperiments are found here: github.com/leonardopetrini/relativestability/experiments ICLR23.md.\nB.1\nDEEP NETWORKS TRAINING\nIn this section, we describe the experimental setup for the training of the deep networks deployed in\nSections 1, 2 and 3.\nFor CIFAR10, fully connected networks are trained with the ADAM optimizer and learning rate = 0.1\nwhile for CNNs SGD, learning rate = 0.1 and momentum = 0.9. In the latter case, the learning rate\nfollows a cosine annealing scheduling. In all cases, the networks are trained on the cross-entropy loss,\nwith a batch size of 128 and for 250 epochs. Early stopping at the best validation error is performed\nfor selecting the networks to study. During training, we employ standard data augmentation consisting\nof random translations and horizontal ﬂips of the input images. On the scale-detection task, we\nperform SGD on the hinge loss and halve the learning rate to 0.05. All results are averaged when\ntraining on 5 or more different networks initializations.\nFor ImageNet, we used pretrained models from Pytorch, torchvision.models.\nB.2\nSIMPLE CNNS TRAINING\nIn this section we present the experimental setup for the training of simple CNNs introduced in\nSection 4, whose sensitivities to diffeomorphisms and Gaussian noise are shown in Fig. 7.\nTo learn task 1 we use CNNs with stride s = 1 and ﬁlter size F = 3. The width of the CNN is ﬁxed\nto 1000 channels, while the depth to 12 layers. We use the Scale-Detection task in the version of\nFig. 5 (b), with ξ = 11 and gap g = 4 and image size L = 32. For the training, we use P = 48\ntraining points and Stochastic Gradient Descent (SGD) with learning rate 0.01 and batch size 8. We\nuse weight decay for the L2 norm of the ﬁlters weights with ridge 0.01. We stop the training after 500\ntimes the interpolation time, which is the time required by the network to reach zero interpolation\nerror of the training set. The goal of this procedure is to reach the solution with minimal norm. The\ngeneralization error of the trained CNNs is exactly zero: they learn spatial pooling perfectly. We\nshow the sensitivities of the trained CNNs, averaged over 4 seeds, in the top panels of Fig. 7, where\nwe also successfully test the predictions (Eq. 5, Eq. 7). We remark that to compute Gk we inserted\nGaussian noise with already the ReLU applied on, since we observe that without it we would see a\npre-asymptotic behaviour for Gk with respect to Ak.\nTask 2 is learned using CNNs with stride equal to ﬁlter size s = F = 2. For the dataset, we use the\nblock-wise version of the Scale-Detection task shown in Fig. 5 (c), ﬁxing ξ = 25 and L = 27. We\nuse 7 layers and 1000 channels for the CNNs. The training is performed using SGD and weight decay\nwith the same parameters as in task 1, with P = 210 training points. In the bottom panels of Fig. 7\nwe show that the predictions (Eq. 8, Eq. 9) capture the experimental results, averaged over 10 seeds.\n14\n135\nTo support the assumption done in Section 4 that the trained CNNs are effectively behaving as one\nchannel with homogeneous positive ﬁlters, we report the numerical values of the average ﬁlter over\nchannels per layer in Table 1 for Task 1 and Table 2 for Task 2. They are positive in the ﬁrst 9 hidden\nlayers, where channel pooling is most pronounced.\nInit.\nAfter training\nk = 1\n[0.0132, 0.0023, −0.0068]\n[0.2928, 0.2605, 0.2928]\nk = 2\n[0.0014, −0.0007, −0.0009]\n[0.0039, 0.0035, 0.0039]\nk = 3\n[−0.0006, −0.0001, 0.0010]\n[0.0043, 0.0038, 0.0043]\nk = 4\n[3.4610e −05, 6.5687e −04, −9.1634e −04]\n[0.0039, 0.0033, 0.0038]\nk = 5\n[−0.0006, 0.0002, −0.0009]\n[0.0038, 0.0032, 0.0038]\nk = 6\n[0.0012, −0.0011, −0.0003]\n[0.0038, 0.0031, 0.0038]\nk = 7\n[−0.0006, 0.0004, 0.0003]\n[0.0041, 0.0032, 0.0040]\nk = 8\n[0.0005, −0.0012, 0.0010]\n[0.0036, 0.0024, 0.0035]\nk = 9\n[0.0005, −0.0012, 0.0010]\n[0.0021, 0.0016, 0.0017]\nk = 10\n[−0.0025, 0.0015, −0.0006]\n[−0.0013, −0.0008, −0.0010]\nk = 11\n[−0.0006, 0.0005, 0.0009]\n0.0002, 0.0002, 0.0002]\nk = 12\n[3.3418e −04, 3.3521e −05, 1.3936e −03]\n[0.0009, 0.0008, 0.0009]\nTable 1: Average over channels of ﬁlters in layer k, before and after training, for simple CNNs with\ns = 1 and F = 3 trained on task 1. The network learns ﬁlters which are much more homogeneous\nthan initialization.\nInit.\nAfter training\nk = 1\n[−0.0559, −0.0291]\n[0.3828, 0.3737]\nk = 2\n[−0.0022, 0.0010]\n[0.0060, 0.0059]\nk = 3\n[0.0006, −0.0010]\n[0.0064, 0.0065]\nk = 4\n[−0.0020, 0.0009]\n[0.0059, 0.0060]\nk = 5\n[0.0002, 0.0008]\n[9.9935e −05, 2.1380e −04]\nk = 6\n[−0.0003, −0.0010]\n[−0.0028, −0.0029]\nk = 7\n[−7.4610e −04, 8.4595e −05]\n[−0.0009, −0.0009]\nTable 2: Average over channels of ﬁlters in layer k, before and after training, for simple CNNs with\ns = F = 2 trained on task 2. The network learns ﬁlters which are much more homogeneous than\ninitialization.\n15\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n136\nC\nADDITIONAL FIGURES AND TABLES\n10−1\nDf\n10−3\n10−2\n10−1\nϵ\nFCN\nLeNet\nMinCNN\nResNet34\nResNet50\nVGG11\nVGG16bn\ncorr. coef.: 0.70\n100\n101\n102\nGf\nFCN\nLeNet\nMinCNN\nResNet34\nResNet50\nVGG11\nVGG16bn\ncorr. coef.: -0.72\n10−2\n100\nRf\nFCN\nLeNet\nMinCNN\nResNet34\nResNet50\nVGG11\nVGG16bn\ncorr. coef.: 0.84\nFigure 8: Generalization error ϵ versus sensitivity to diffeomorphisms Df (left), noise Gf (center)\nand relative sensitivity Rf (right) for a wide range of architectures trained on scale-detection task 1\n(train set size: 1024, image size: 32, ξ = 14, g = 2). As in real data, ϵ is positively correlated with\nDf and negatively correlated with Gf. The correlation is the strongest for the relative measure Rf.\n10\n1\n100\nDk\nAlexNet\noriginal\nshuffle ch.\n100\n101\nGk\nAlexNet\noriginal\nshuffle ch.\n10\n1\n100\nRk\nAlexNet\noriginal\nshuffle ch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrelative depth k\nK\n10\n1\n100\nDk\nVGG11\noriginal\nshuffle ch.\nVGG11bn\nVGG13bn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrelative depth k\nK\n10\n1\n100\n101\n102\nGk\nVGG11\noriginal\nshuffle ch.\nVGG11bn\nVGG13bn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrelative depth k\nK\n10\n3\n10\n2\n10\n1\n100\nRk\nVGG11\noriginal\nshuffle ch.\nVGG11bn\nVGG13bn\nFigure 9: Sensitivities (Dk left, Gk middle and Rk right) of the internal representations vs relative\ndepth for AlexNet (1st row) and VGG networks (2nd row) trained on scale-detection task 1. Dot-dashed\nlines show the sensitivities of networks with shufﬂed channels.\n16\n137\n10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 0\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 10−2\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34ResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34ResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−1\ntest error, ϵ\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 100\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11VGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 101\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 30\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34ResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−2\nDf\n10−1\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\nη = 102\n10−3\n10−2\n10−1\nGf\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50\nVGG11\nVGG11bn\nVGG16bn\nVGG19bn\n10−1\n100\nRf\nAlexNet\nDenseNetL2\nDenseNetL4\nDenseNetL6\nEﬃcientNetB0\nLeNet\nResNet18\nResNet34\nResNet50VGG11\nVGG11bn\nVGG16bn\nVGG19bn\nFigure 10: Test error vs. sensitivities (columns) when training on noisy CIFAR10. The different rows\ncorrespond to increasing noise magnitude η. Different points correspond to networks architectures,\nsee gray labels. The content of this ﬁgure is also represented in compact form in Fig. 1, right.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrelative depth k\nK\n100\n2 × 10\n1\n3 × 10\n1\n4 × 10\n1\n6 × 10\n1\nRk\nCIFAR10\nAlexNet\nLeNet\noriginal\nshuffle ch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrelative depth k\nK\n100\nImageNet\nalexnet\noriginal\nshuffle ch.\nFigure 11: Analogous of Fig. 3 for different network architectures: relative sensitivity Rk as a\nfunction of depth for LeNet and AlexNet architectures trained on CIFAR10 (left) and ImageNet\n(right). Full lines indicate experiments done on the original networks, dashed lines the ones after\nshufﬂing channels. K indicates the networks total depth.\n17\nChapter 5. When Feature Learning Succeeds: How Deformation Invariance is learned in\nConvolutional Neural Networks\n138\n1\n2\n3\n4\n5\n6\n7\n8\nlayer\n0.6\n0.8\n1.0\nspatial frequency content\nCIFAR10\nVGG11\nVGG11bn\ninit.\ntrained\n1\n2\n3\n4\n5\n6\n7\n8\nlayer\n0.4\n0.6\n0.8\n1.0\nImageNet\n1\n2\n3\n4\n5\n6\n7\n8\nlayer\n0.7\n0.8\n0.9\n1.0\nscale-detection\nVGG11\nVGG11bn\ninit.\ntrained\nFigure 12: Spatial frequency content of ﬁlters for CIFAR10 (left), ImageNet (center) and the\nscale-detection task (right). The y-axis reports an aggregate measure among spatial frequencies:\nN(PN\ni=1 λl)−1⟨∥wk\nc ∥2⟩−1\nc\nPF 2\nl=1 λl⟨(Ψl · wk\nc )2⟩c, where Ψl are the 3 × 3 Laplacian eigenvectors\nand λl the corresponding eigenvalues, wk\nc the c-th ﬁlter of layer k and ⟨·⟩c denotes the average over\nc. This is an aggregate measure over frequencies, the frequencies distribution is reported in the main\ntext, Fig. 4.\n0.0\n0.5\nlayer 1\nlayer 2\nvgg13\nvgg16\nvgg19\nlayer 3\nlayer 4\nlayer 5\nlayer 6\nlayer 7\nvgg13\nvgg16\nvgg19\nlayer 8\n0\n2\n4\n6\n8\n0.0\n0.5\nlayer 9\n0\n2\n4\n6\n8\nlayer 10\n0\n2\n4\n6\n8\nlayer 11\n0\n2\n4\n6\n8\nlayer 12\n0\n2\n4\n6\n8\nlayer 13\n0\n2\n4\n6\n8\nlayer 14\n0\n2\n4\n6\n8\nlayer 15\n0\n2\n4\n6\n8\nlayer 16\neigenvalue index, l\nk, l\nFigure 13: Analogous of Fig. 4 for deep VGGs trained on ImageNet. Dotted and full lines respectively\ncorrespond to initialization and trained networks. The x-axis reports low to high frequencies from\nleft to right. Deeper layers are reported in rightmost panels.\n0\n2\n4\n6\n8\n0.1\n0.2\nk = 1\n0\n2\n4\n6\n8\nk = 2\n0\n2\n4\n6\n8\nk = 3\n0\n2\n4\n6\n8\nk = 4\n0\n2\n4\n6\n8\nCIFAR10\nk = 5\n0\n100\n0.0\n0.5\n0\n20\nalexnet\ninit.\ntrained\n0\n5\n0\n5\n0\n5\nImageNet\n0\n2\n4\n6\n8\n0.10\n0.15\n0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8 0\n2\n4\n6\n8\nscale-detection\nAlexNet\ninit.\ntrained\neigenvalue index, l\nk, l\nFigure 14: Analogous of Fig. 4 for AlexNet trained on CIFAR10 (1st row), ImageNet (2ndrow) and\nthe scale detection task (3rd row). Dotted and full lines respectively correspond to initialization and\ntrained networks. The x-axis reports low to high frequencies from left to right. Deeper layers are\nreported in rightmost panels.\n18\n139\nPart III\nSynonymic Invariance\n141\n6 A Toy Model for the Hierarchical Com-\npositionality of Real Data\nThe following paper is the preprint version of Petrini et al. (2023) that is currently under review.\nCandidate contributions\nThe candidate contributed to all discussions, implemented the\nRandom Hierarchy Model, designed and performed the experiments.\n143\nHow Deep Neural Networks Learn Compositional Data:\nThe Random Hierarchy Model\nLeonardo Petrinia*, Francesco Cagnettaa†*, Umberto M. Tomasinia, Alessandro Faveroa,b, and\nMatthieu Wyarta\naInstitute of Physics, EPFL, Lausanne, Switzerland\nbInstitute of Electrical Engineering, EPFL, Lausanne, Switzerland\nJune 28, 2023\nAbstract\nLearning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in\nthe dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this\nchallenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to\nbuild a low-dimensional representation of the data. However, little is known about how much training data they require,\nand how this number depends on the data structure. This paper answers this question for a simple classification task\nthat seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the nc classes\ncorresponds to m synonymic compositions of high-level features, which are in turn composed of sub-features through\nan iterative process repeated L times. We find that the number of training data P ∗required by deep CNNs to learn\nthis task (i) grows asymptotically as ncmL, which is only polynomial in the input dimensionality; (ii) coincides with\nthe training set size such that the representation of a trained network becomes invariant to exchanges of synonyms;\n(iii) corresponds to the number of data at which the correlations between low-level features and classes become\ndetectable. Overall, our results indicate how deep CNNs can overcome the curse of dimensionality by building invariant\nrepresentations, and provide an estimate of the number of data required to learn a task based on its hierarchically\ncompositional structure.\nThe achievements of deep learning algorithms [1] are\noutstanding. These methods exhibit superhuman perfor-\nmances in areas ranging from image recognition [2] to Go-\nplaying [3], and large language models such as GPT4 [4]\ncan generate unexpectedly sophisticated levels of reasoning.\nHowever, despite these accomplishments, we still lack a\nfundamental understanding of the underlying factors. In-\ndeed, Go configurations, images, and patches of text lie\nin high-dimensional spaces, which are hard to sample due\nto the curse of dimensionality [5]: the distance δ between\nneighboring data points decreases very slowly with their\nnumber P, as δ = O(P −1/d) where d is the space dimen-\nsion. A generic task such as regression of a continuous\nfunction [6] requires a small δ for high performance, imply-\ning that P must be exponential in the dimension d. Such\na number of data is unrealistically large: for example, the\nbenchmark dataset ImageNet [7], whose effective dimen-\nsion is estimated to be ≈50 [8], consists of only ≈107\ndata, significantly smaller than e50 ≈1020. This immense\n*Equal contribution.\n†Correspondence to francesco.cagnetta@epfl.ch.\ndifference implies that learnable tasks are not generic, but\nhighly structured. What is then the nature of this struc-\nture, and why are deep learning methods able to exploit it?\nWithout a quantitative answer, it is impossible to predict\neven the order of magnitude of the order of magnitude of\nthe number of data necessary to learn a specific task.\nA popular idea attributes the efficacy of deep learning\nmethods to their ability to build a useful representation of\nthe data, which becomes increasingly complex across the\nlayers. In simple terms, neurons closer to the input learn\nto detect simple features like edges in a picture, whereas\nthose deeper in the network learn to recognize more abstract\nfeatures, such as faces [9,10]. Intuitively, if these represen-\ntations are also invariant to aspects of the data unrelated to\nthe task, such as the exact position of an object in a frame\nfor image classification [11], they may effectively reduce\nthe dimensionality of the problem and make it tractable.\nThis view is supported by several empirical studies of the\nhidden representations of trained networks. In particular,\nmeasures such as (i) the mutual information between such\nrepresentations and the input [12,13], (ii) their intrinsic\n1\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n144\ndimensionality [14, 15], and (iii) their sensitivity toward\ntransformations that do not affect the task (e.g. smooth\ndeformations for image classification [16,17]), all eventually\ndecay with the layer depth. In some cases, the magnitude\nof this decay correlates with performance [16]. However,\nthese studies do not indicate how much data is required to\nlearn such representations, and thus the task.\nHere we study this question for tasks which are hierar-\nchically compositional—arguably a key property for the\nlearnability of real data [18–25]. To provide a concrete\nexample, consider the picture of a dog (see Fig. 1). The\nimage consists of several high-level features like head, body,\nand limbs, each composed of sub-features like ears, mouth,\neyes, and nose for the head. These sub-features can be\nfurther thought of as combinations of low-level features\nsuch as edges. Recent studies have revealed that: (i) deep\nnetworks represent hierarchically compositional tasks more\nefficiently than shallow networks [21]; (ii) the minimal\nnumber of data that contains enough information to recon-\nstruct such tasks is polynomial in the input dimension [24],\nalthough extracting this information remains impractical\nwith standard optimization algorithms; (iii) correlations\nbetween the input data and the task are critical for learn-\ning [19,26] and can be exploited by algorithms based on\nthe iteration of clustering methods [22, 27]. While these\nseminal works offer important insights, they do not directly\naddress practical settings, specifically deep convolutional\nneural networks (CNNs) trained using gradient descent.\nConsequently, we currently don’t know how the hierarchi-\ncally compositional structure of the task influences the\nsample complexity, i.e., the number of data necessary to\nlearn the task.\nIn this work, we adopt the physicist’s approach [28–31]\nof introducing a simplified model of data, which we then\ninvestigate quantitatively via a combination of theoreti-\ncal arguments and numerical experiments. The task we\nconsider, introduced in Section 1, is a multinomial classifica-\ntion where the class label is determined by the hierarchical\ncomposition of input features into progressively higher-\nlevel features (see Fig. 1). This model belongs to the class\nof generative models introduced in [22,27], corresponding\nto the specific choice of random composition rules. More\nspecifically, we consider a classification problem with nc\nclasses, where the class label is expressed as a hierarchy\nof L randomly-chosen composition rules. In each rule, m\ndistinct tuples of s adjacent low-level features are grouped\ntogether and assigned the same high-level feature taken\nfrom a finite vocabulary of size v (see Fig. 1). Then, in Sec-\ntion 3, we show empirically that the sample complexity\nP ∗of deep CNNs trained with gradient descent scales as\nncmL. Furthermore, we find that P ∗coincides with both\na) the number of data that allows for learning a represen-\ntation that is invariant to exchanging the m semantically\nequivalent low-level features (subsection 3.1) and b) the\nsize of the training set for which the correlations between\nlow-level features and class label become detectable (Sec-\ntion 4). Via b), P ∗can be derived under our assumption\non the randomness of the composition rules.\n1\nThe Random Hierarchy Model\nIn this section, we introduce our model task, which is a\nmultinomial classification problem with nc classes, where\nthe input-output relation is compositional, hierarchical,\nand local. To build the dataset, we let each class label\nα = 1, . . . , nc generate the set of input data with label α as\nfollows.\ni) Each label generates m distinct representations con-\nsisting of s-tuples of high-level features (see Fig. 2 for\nan example with s = 2 and m = nc = 3). Each of these\nfeatures belongs to a finite vocabulary of size v (v = 3\nin the figure), so that there are vs possible represen-\ntations and ncm ≤vs. We call the assignment of m\ndistinct s-tuples to each label a composition rule;1\nii) Each of the v high-level feature (level-L) generates m\ndistinct representations of s sub-features (level-(L−1)),\nout of the vs possible ones. Thus, m ≤vs−1. After\ntwo generations, labels are represented as s2-tuples\nand there are m × ms data per class;\niii) The input data are obtained after L generations (level-\n1 representation) so that each datum x consists of\nd = sL input features xj. We apply one-hot encod-\ning to the input features: each of the xj’s is a v-\ndimensional sequence with one element set to 1 and\nthe others to 0, the index of the non-zero component\nrepresenting the encoded feature. The number of data\nper class is\nm × ms × · · · × msL−1 = m\nPL−1\ni=0 si = m\nsL−1\ns−1 ,\n(1)\nhence the total number of data Pmax reads\nPmax ≡ncm\nsL−1\ns−1 = ncm\nd−1\ns−1 .\n(2)\nA generic classification task is thus specified by L com-\nposition rules and can be represented as a s-ary tree—an\nexample with s = 2 and L = 3 is shown in Fig. 1(c) as a bi-\nnary tree. The tree representation highlights that the class\nlabel α(x) of a datum x can be written as a hierarchical\ncomposition of L local functions of s variables [20,21]. For\ninstance, with s = L = 2 (x = (x1, x2, x3, x4)),\nα(x1, . . . , x4) = g2 (g1(x1, x2), g1(x3, x4)) ,\n(3)\nwhere g1 and g2 represent the 2 composition rules.\n1Composition rules are called production rules in formal language\ntheory [32].\n2\n145\nlabel\nhigh-level features\n(level 3)\nsub-features\n(level 2)\ninput features\n(level 1)\n(c) Random Hierarchy Model\n(b) Hierarchy in Text\n(a) Hierarchy in Images\nsentence\nmain clause\nsub-clause\nsubject  predicate subject  predicate\nverb   object\nverb        object\nadj. noun\ndog\nhead\npaws\neyes\nnose\nmouth\near\nedges\n...\nFigure 1: Illustrating the hierarchical structure of real-world and artificial data. (a) An example of the hierarchical\nstructure of images: the class (dog) consists of high-level features (head, paws), that in turn can be represented as\nsets of lower-level features (eyes, nose, mouth, and ear for the head). Notice that, at each level, there can be multiple\ncombinations of low-level features giving rise to the same high-level feature. (b) A similar hierarchical structure can be\nfound in natural language: a sentence is made of clauses, each having different parts such as subject and predicate,\nwhich in turn may consist of several words. (c) An illustration of the artificial data structure we propose. The samples\nreported here were drawn from an instance of the Random Hierarchy Model for depth L = 3 and tuple length s = 2.\nDifferent features are shown in different colors.\nIn the Random Hierarchy Model (RHM) the L compo-\nsition rules are chosen uniformly at random over all the\npossible assignments of m low-level representations to each\nhigh-level feature. As sketched in Fig. 2, the random choice\ninduces correlations between low- and high-level features.\nIn simple terms, each of the high-level features—1, 2 or\n3 in the figure—is more likely to be represented with a\ncertain low-level feature in a given position—blue on the\nleft for 1, yellow for 2 and green for 3. These correlations\nare crucial for our predictions and are analyzed in detail\nin Appendix B.\nLet us remark that the L composition rules can be cho-\nsen such that the low-level features are homogeneously\ndistributed across high-level features for all positions, as\nsketched in Fig. 3. We refer to this choice as the Homoge-\nneous Features Model. In this model, none of the low-level\nfeatures is predictive of the high-level feature. With s = 2\nand Boolean features v = m = 2, the Homogeneous Fea-\ntures Model reduces to the problem of learning a parity\nfunction [33].\nFinally, note that we only consider the case where the\nparameters s, m and v are constant through the hierarchy\nlevels for clarity of exposition. It is straightforward to\nextend the model, together with the ensuing conclusions, to\nthe case where all the levels of the hierarchy have different\nparameters.\nFigure 2: Label to lower-level features mapping in the Ran-\ndom Hierarchy Model (RHM). Each of the nc = 3 classes\n(numbered boxes at the top) corresponds to m = 3 distinct\ncouples (unnumbered boxes at the bottom) of features.\nThese features belong to a finite vocabulary (blue, orange\nand green, with size v = 3). Iterating this mapping L times\nwith the lower-level features as high-level features of the\nnext step yields the full dataset. Notice that some features\nappear more often in the representation of a certain class\nthan in those of the others, e.g. blue on the left appears\ntwice in class 1, once in class 2 and never in class 3. As a\nresult, low-level features are generally correlated with the\nlabel.\n3\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n146\nFigure 3: Label to lower-level features mapping in the\nHomogeneous Feature Model with v = m = nc = 3 and s = 2.\nIn contrast with the case illustrated in Fig. 2, this mapping\nis such that each of the 3 possible low-level features appears\nexactly once in each of the 2 elements of the representation\nof each class. In maths, denoting with Ni(µ; α) the number\nof times that the low-level feature µ appears in the i-th\nposition of the representation of class α, one has that\nNi(µ; α) = 1 for all i = 1, 2, for all µ = green, blue, orange\nand for all α = 1, 2, 3.\n2\nCharacteristic Sample Sizes\nThe main focus of our work is the answer to the following\nquestion:\nQ: How much data is required to learn a typical instance\nof the Random Hierarchy Model?\nIn this section, we first discuss two characteristic scales of\nthe number of training data for an RHM with nc classes,\nvocabulary size v, multiplicity m, depth L, and tuple size\ns. The first, related to the curse of dimensionality, rep-\nresents the sample complexity of methods that are not\nable to learn the hierarchical structure of the data. The\nsecond, which comes from information-theoretic consider-\nations, represents the minimal number of data necessary\nto reconstruct an instance of the RHM. These two sample\nsizes can be thought of as an upper and lower bound to\nthe sample complexity of deep CNNs, which indeed lies\nbetween the two bounds (cf. Section 3).\n2.1\nCurse of Dimensionality (Pmax)\nLet us recall that the curse of dimensionality predicts\nan exponential growth of the sample complexity with the\ninput dimension d = sL.\nFig. 4 shows the test error of\na one-hidden-layer fully-connected network trained on in-\nstances of the RHM while varying the number of training\ndata P (see methods for details of the training procedure)\nin the maximal m case, m = vs−1.\nThe bottom panel\ndemonstrates that the sample complexity is proportional\nto the total dataset size Pmax. Since, from Eq. 2, Pmax\ngrows exponentially with d, we conclude that shallow fully-\nconnected networks suffer from the curse of dimensionality.\nBy contrast, we will see that using CNNs results in a much\ngentler growth (i.e. polynomial) of the sample complexity\nwith d.\n101\n102\n103\n104\n105\n106\nP\n0.4\n0.6\n0.8\n1.0\ntest error, ¯ϵ\n101\n102\n103\n104\n105\n106\nPmax = v2L\n102\n104\n106\nP ∗s.t. ¯ϵ = 70%\nv = 4\nv = 8\nv = 16\nv = 24\nv = 32\ny = x\nFigure 4: Sample complexity for one-hidden-layer\nfully-connected networks, v = nc = m and s = 2. Top:\nTest error vs.\nthe number of training data.\nDifferent\ncolors correspond to different vocabulary sizes v. Bottom:\nnumber of data corresponding to test error ϵ = 0.7 as a\nfunction of Pmax. The black dashed line indicates a linear\nrelationship: one-hidden-layer fully-connected networks\nachieve a small test error only when trained on a finite\nfraction of the whole dataset, thus their sample complexity\ngrows exponentially with the input dimension.\n2.2\nInformation-Theoretic Limit (Pmin)\nAn algorithm with full prior knowledge of the genera-\ntive model can reconstruct an instance of the RHM with\na number of points Pmin ≪Pmax. For instance, we can\nconsider an extensive search within the hypothesis class of\nall possible hierarchical models with fixed nc, m, v, and L.\nThen, if nc = v and m = vs−1, so that the model generates\nall possible input data, we can use a classical result of\nthe PAC (Probably Approximately Correct) framework\nof statistical learning theory [34] to relate Pmin with the\nlogarithm of the cardinality of the hypothesis class, that is\nthe number of possible instances of the hierarchical model.\nThe number of possible composition rules equals the num-\nber of ways of allocating vs−1 of vs possible tuples to each\n4\n147\nof the v classes/features, i.e., a multinomial coefficient,\n# {rules} =\n(vs)!\n((vs−1)!)v\n(4)\nSince an instance consists of L independently chosen com-\nposition rules, we have\n# {instances} = (# {rules})L\n\u0012 1\nv!\n\u0013L−1\n(5)\nwhere the additional multiplicative factor (v!)1−L takes\ninto account that the input-label mapping is invariant for\nrelabeling of the features of the L −1 internal representa-\ntions. Upon taking the logarithm and approximating the\nfactorials for large v via Stirling’s formula,\nPmin = log (# {instances})\nv≫1\n−−−→Lvs,\n(6)\nIntuitively, the problem boils down to understanding the\nL composition rules, each needing m × v examples (vs for\nm = vs−1). Pmin grows only linearly with the depth L—\nhence logarithmically in d—whereas Pmax is exponential\nin d. Having used full knowledge of the generative model,\nPmin can be thought of as a lower bound for the sample\ncomplexity of a generic supervised learning algorithm which\nis agnostic of the data structure.\n3\nSample\nComplexity\nof\nDeep\nCNNs\nIn this section, we focus on deep learning methods. In\nparticular, we ask\nQ: How much data is required to learn a typical instance\nof the Random Hierarchy Model with a deep CNN?\nThus, after generating an instance of the RHM with fixed\nparameters nc, s, m, v, and L, we train a deep CNN with\nL hidden layers, filter size and stride equal to s (see Fig. 5\nfor an illustration) with stochastic gradient descent (SGD)\non P training points selected at random among the RHM\ndata. Further details of the training are in Materials and\nMethods.\nBy looking at the test error of trained networks as a func-\ntion of the training set size (top panels of Fig. 6 and Fig. 7,\nsee also Fig. 15 in Appendix G for a study with varying nc),\nwe notice the existence of a characteristic value of P where\nthe error decreases dramatically, thus the task is learned.\nIn order to study the behavior of this threshold with the\nparameters of the RHM, we define the sample complexity\nas the smallest P such that the test error ϵ(P) is smaller\nthan ϵrand/10, with ϵrand = 1 −n−1\nc\ndenoting the average\nerror when choosing the label uniformly at random. The\nbottom panels of Fig. 6 (for the case nc = m = v) and Fig. 7\n: multiple channels \n+ non-linearity\ninput\n}\nhidden layers\noutput\nFigure 5: Neural network architecture that matches the\nRHM hierarchy. This is a deep CNN with L hidden layers,\nand stride and filter size equal to the tuple length s. Filters\nthat act on different input patches are the same (weight\nsharing). The number of input channels equals v and the\noutput is a vector of size nc.\n(with m < v, see Appendix G for varying nc) show that the\nsample complexity scales as\nP ∗= ncmL ⇔P ∗\nnc\n= d ln(m)/ ln(s),\n(7)\nindependently of the vocabulary size v. Eq. 7 shows that\ndeep CNNs only require a number of samples that scales as\na power of the input dimension d = sL to learn the RHM:\nthe curse of dimensionality is beaten. This evidences the\nability of CNNs to harness the hierarchical compositionality\ninherent to the task. The question then becomes: what\nmechanisms do these networks employ to achieve this feat?\n3.1\nEmergence of Synonymic Invariance\nin Deep CNNs\nA natural approach to learning the RHM would be to\nidentifying the sets of s-tuples of input features that corre-\nspond to the same higher-level feature. Examples include\nthe pairs of low-level features in Fig. 2 and Fig. 3 which\nbelong to the same column. In general, we refer to s-tuples\nthat share the same higher-level representation as syn-\nonyms. Identifying synonyms at the first level would allow\nus to replace each s-dimensional patch of the input with a\nsingle symbol, reducing the dimensionality of the problem\nfrom sL to sL−1. Repeating this procedure L times would\nlead to the class labels and, consequently, to the solution\nof the task.\nIn order to test if deep CNNs trained on the RHM re-\nsort to a similar solution, we introduce the synonymic\nsensitivity, which is a measure of the invariance of any\ngiven function of the input with respect to the exchange\nof synonymic s-tuples. We define Sk,l as the sensitivity of\nthe k-th layer representation of a trained network with re-\nspect to exchanges of synonymous tuples of level-l features.\n5\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n148\n101\n102\n103\n104\n105\nP\n0.00\n0.25\n0.50\n0.75\n1.00\ntest error, ϵ\n101\n102\n103\n104\n105\nvL+1\n102\n104\nP ∗s.t. ¯ϵ = 10%\nv = 3\nv = 4\nv = 5\nv = 6\nv = 7\nv = 8\nv = 10\nv = 12\nv = 16\nv = 18\nv = 20\nv = 24\nv = 32\nL = 2\nL = 3\nL = 4\ny = x\nFigure 6: Sample complexity for deep CNNs, m =\nnc = v and s = 2. Top: Test error vs number of training\npoints. Different colors correspond to different vocabulary\nsizes v.\nMarkers to hierarchy depths L.\nDeep CNNs\nare able to achieve zero generalization error when enough\ntraining points are provided. Bottom: sample complexity\nP ∗corresponding to a test error ϵ∗= 0.1. Remarkably, the\nneural networks can generalize with a number of samples\nP ∗= vL+1 ≪Pmax.\nNamely,\nSk,l = ⟨∥fk(x) −fk(Plx)∥2⟩x,Pl\n⟨∥fk(x) −fk(z)∥2⟩x,z\n,\n(8)\nwhere: fk is the vector of the activations of the k-th layer\nin the network; Pl is an operator that replaces all the level-l\ntuples with synonyms selected uniformly at random; ⟨·⟩\nwith subscripts x, z denote an average over all the inputs in\nan instance of the RHM; the subscript Pl denotes average\nover all the exchanges of synonyms.\nIn particular, Sk,1 quantifies the invariance of the hidden\nrepresentations learned by the network at layer k with\nrespect to exchanges of synonymic tuples of input fea-\ntures. Fig. 8 reports S2,1 as a function of the training\nset size P for different combinations of the model param-\neters. We focused on S2,1—the sensitivity of the second\nlayer of the deep CNN to permutations at the first level of\nthe hierarchy—since synonymic invariance can generally\nbe achieved at all layers k starting from k = l + 1, and\nnot before 2 Notice that all curves display a sigmoidal\nshape, signaling the existence of a characteristic sample\n2To illustrate this, consider a hierarchy of depth L = 2, s = 2, and\na two-hidden-layers CNN. In the general case, synonymic invariance\nto permutations at level one, cannot be achieved at the first layer of\nthe network. This is because, say a level-1 feature can be represented\n101\n102\n103\n104\nP\n0.00\n0.25\n0.50\n0.75\n1.00\ntest error, ϵ\n101\n102\n103\n104\nvmL\n101\n102\n103\n104\nP ∗s.t. ¯ϵ = 10%\nnc = 4\nnc = 8\nnc = 16\nL = 2\nL = 3\nL = 4\ny = x\nFigure 7: Sample complexity for deep CNNs, m < v,\nnc = v and s = 2. Top: Test error vs number of training\npoints. Different colors correspond to different vocabulary\nsizes v. Markers to hierarchy depths L. Bottom: sample\ncomplexity P ∗corresponding to a test error ϵ∗= 0.1.\nSimilarly to the previous plot, this confirms that the sample\ncomplexity of deep CNNs scales as P ∗= ncmL.\nsize which marks the emergence of synonymic sensitivity in\nthe learned representations. Remarkably, by rescaling the\nx-axis by the sample complexity of Eq. 7 (bottom panel),\ncurves corresponding to different parameters collapse. We\nconclude that the generalization ability of a network relies\non the synonymic invariance of its hidden representations.\nMeasures of the synonymic sensitivity Sk,1 for different\nlayers k are reported in Fig. 9 (blue lines), showing indeed\nthat small values of Sk,1 are achieved for k ≥2. Fig. 9\nalso shows the sensitivities to exchanges of synonyms in\nthe higher-level representations of the RHM: all levels are\nlearned together as P increases, and invariance to level-l\nexchanges is achieved at layer k = l + 1, as expected. The\nfigure displays the test error too (gray dashed), to further\nemphasize its correlation with synonymic invariance.\nat the input as (α, β), (α, α) and (β, α), but not as (β, β). Then, it is\nimpossible to build a neuron that would have the same response to the\nfirst three pairs but not the fourth. Instead, a simple solution exists\nfor layer 2 to become invariant to exchanges at level 1. This consists\nin building v2 neurons at the first layer k = 1, each responding to one\ninput pair. Clearly, the representation at k = 1 is not invariant to\nthe substitution of synonyms. The second layer, though, can assign\nidentical weights to all the v neurons that encode for the same feature,\nhence becoming invariant to permutations at l = 1.\n6\n149\n101\n102\n103\n104\n105\n106\ntrain set size, P\n0.25\n0.50\n0.75\nsensitivity, S2,1\nv = 4\nv = 8\nv = 16\nv = 24\nv = 32\nL = 2\nL = 3\n10−1\n100\n101\nP/(ncmL)\n0.25\n0.50\n0.75\nsensitivity, S2,1\nv = 4\nv = 8\nv = 16\nv = 24\nv = 32\nL = 2\nL = 3\nFigure 8: Sensitivity S2,1 of the second layer of a deep\nCNN to permutations in the first level of the RHM with\nL = 2, 3, s = 2, nc = m = v, as a function of the training\nset size (top) and after rescaling by P ∗= ncmL (bottom).\nSensitivity decreases from 1 to approximately zero, i.e. deep\nCNNs are able to learn synonymic invariance with enough\ntraining points. The collapse after rescaling highlights that\nthis can be done with P ∗training points.\n4\nCorrelations Govern Synonymic\nInvariance\nWe now provide a theoretical argument for understanding\nthe scaling of P ∗of Eq. 7 with the parameters of the RHM.\nFirst, we compute a third characteristic sample size Pc,\ndefined as the size of the training set for which the local\ncorrelation between any of the input patches and the label\nbecomes detectable. Remarkably, Pc coincides with P ∗\nof Eq. 7. Secondly, we demonstrate how a one-hidden-\nlayer neural network acting on a single patch can use such\ncorrelations to build a synonymic invariant representation\nin a single step of gradient descent, so that Pc and P ∗also\ncorrespond to the emergence of an invariant representation.\n4.1\nIdentify Synonyms by Counting\nThe invariance of the RHM labels with respect to ex-\nchanges of synonymous input patches can be inferred by\ncounting the occurrences of such patches in all the data\nbelonging to a given class α. Intuitively, tuples of features\nthat appear with identical frequencies are likely synonyms.\n0.0\n0.5\n1.0\nsensitivity, Sk,l\nk = 1\nl = 1\nl = 2\nl = 3\ntest error\nk = 2\n104\ntrain set size, P\n0.0\n0.5\n1.0\nsensitivity, Sk,l\nk = 3\n104\ntrain set size, P\noutput\nFigure 9: Permutation sensitivity Sk,l of the layers of a\ndeep CNN trained on the RHM with L = 3, s = 2, nc =\nm = v = 8, as a function of the training set size P. The\npermutation of synonyms is performed at different levels,\nas indicated in colors. The different panels correspond to\nthe sensitivity of different layers’ activations, indicated by\nthe gray box. Synonymic invariance is learned at the same\ntime for all layers, and most of the invariance to level l is\nobtained at layer k = l + 1.\nMore specifically, let us denote an s-dimensional input\npatch with xj for j in 1, . . . , sL−1, a s-tuple of input fea-\ntures with µ = (µ1, . . . , µs), and the number of data in\nclass α which display µ in the j-th patch with Nj(µ; α).\nNormalizing this number by Nj(µ) = P\nα Nj(µ; α) yields\nthe conditional probability fj(α|µ) for a datum to belong\nto class α conditioned on displaying the s-tuple µ in the\nj-th input patch,\nfj(α|µ) := Pr {x ∈α|xj = µ} = Nj(µ; α)\nNj(µ) .3\n(9)\nIf the low-level features are homogeneously spread across\nclasses, as in the Homogeneous Features Model of Fig. 3,\nthen f = n−1\nc , independently of and α, µ and j. In contrast,\ndue to the aforementioned correlations, the probabilities\nof the RHM are all different from n−1\nc\n(see Fig. 2). We\nrefer to this difference as signal.4 Distinct level-1 tuples\nµ and ν yield a different f (and thus a different signal)\nwith high probability unless they share the same level-\n2 representation. Therefore, this signal can be used to\nidentify synonymous level-1 tuples.\n4.2\nSignal vs. Sampling Noise\nWhen measuring the conditional class probabilities with\nonly P training data, the occurrences in the right-hand side\n3The notation xj = µ means that the elements of the patch xj\nencode the tuple of features µ\n4Cases in which all features are homogeneously spread across\nclasses can also appear in the RHM, but with vanishing probability\nin the limit of large nc and m, see Appendix E.\n7\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n150\nsignal\nnoise\nFigure 10: Signal vs. noise illustration. The dashed\nfunction represents the distribution of f(α|µ) resulting\nfrom the random sampling of the RHM rules. The solid dots\nillustrate the true frequencies f(α|µ) sampled from this\ndistribution, with different colors corresponding to different\ngroups of synonyms. The typical spacing between the solid\ndots, given by the width of the distribution, represents the\nsignal. Transparent dots represent the empirical frequencies\nˆfj(α|µ), with dots of the same color corresponding to\nsynonymous features. The spread of transparent dots of\nthe same color, which is due to the finiteness of the training\nset, represents the noise.\nof Eq. 24 are replaced with empirical occurrences, which\ninduce a sampling noise on the f’s. For the identification\nof synonyms to be possible, this noise must be smaller\nin magnitude than the aforementioned signal—a visual\nrepresentation of the comparison between signal and noise\nis depicted in Fig. 10.\nThe magnitude of the signal can be computed as the ratio\nbetween the standard deviation and mean of fj(α|µ) over\nrealizations of the RHM. The full calculation is presented\nin Appendix B: here we present a simplified argument\nbased on an additional independence assumption. Given\na class α, the tuple µ appearing in the j-th input patch\nis determined by a sequence of L choices—one choice per\nlevel of the hierarchy—of one among m possible lower-level\nrepresentations. These mL possibilities lead to all the mv\ndistinct s-tuples. Nj(µ; α) is proportional to how often\nthe tuple µ is chosen—mL/(mv) times on average. Under\nthe assumption of independence of the mL choices, the\nfluctuations of Nj(µ; α) relative to its mean are given by\nthe central limit theorem and read (mL/(mv))−1/2 in the\nlimit of large m. If nc is sufficiently large, the fluctuations of\nNj(µ) are negligible in comparison. Therefore, the relative\nfluctuations of fj are the same as those of Nj(µ; α): the\nsize of the signal is (mL/(mv))−1/2.\nThe magnitude of the noise is given by the ratio between\nthe standard deviation and mean, over independent sam-\nplings of a training set of fixed size P, of the empirical\nconditional probabilities ˆfj(α|µ). Only P/(ncmv) of the\ntraining points will, on average, belong to class α while\ndisplaying feature µ in the j-th patch. Therefore, by the\nconvergence of the empirical measure to the true proba-\nbility, the sampling fluctuations of ˆf relative to the mean\nare of order [P/(ncmv)]−1/2—see Appendix B for details.\nBalancing signal and noise yields the characteristic Pc for\nthe emergence of correlations. For large m, nc and P,\nPc = ncmL,\n(10)\nwhich coincides with the empirical sample complexity of\ndeep CNNs discussed in Section 3.\n4.3\nLearning Synonymic Invariance by the\nGradients\nTo complete the argument, we consider a simplified one-\nstep gradient descent setting [35,36], where Pc marks the\nnumber of training examples required to learn a synonymic\ninvariant representation. In this setting (details presented\nin Appendix C), we train a one-hidden layer fully-connected\nnetwork on the first s-dimensional patches of the data. This\nnetwork cannot fit data which have the same features on\nthe first patch while belonging to different classes. Nev-\nertheless, the hidden representation of the network can\nbecome invariant to exchanges of synonymous patches.\nMore specifically, as we show in Appendix C, with identi-\ncal initialization of the hidden weights and orthogonalized\ninputs, the update of the hidden representation fh(µ) of\nthe s-tuple of low-level features µ after one step of gradient\ndescent follows\n∆fh(µ) = 1\nP\nnc\nX\nα=1\nah,α\n\nˆN1(µ; α) −1\nnc\nnc\nX\nβ=1\nˆN1(µ; β)\n\n,\n(11)\nwhere fh(µ) coincides the pre-activation of the h-th neuron\nand ah = (ah,1, . . . , ah,nc) denotes the associated nc dimen-\nsional readout weight. ˆN1 is used to denote the empirical\nestimate of the occurrences in the first input patch. Hence,\nby the result of the previous section, the hidden represen-\ntation becomes insensitive to the exchange of synonymic\nfeatures for P ≫Pc.\nThis prediction is confirmed empirically in Fig. 11, which\nshows the sensitivity S1,1 of the hidden representation of\nshallow fully-connected networks trained in the setting of\nthis section, as a function of the number P of training data\nfor different combinations of the model parameters. The\nbottom panel, in particular, highlights that the sensitivity\nis close to 1 for P ≪Pc and close to 0 for P ≫Pc. In\naddition, notice that the collapse of the pre-activations\nof synonymic tuples onto the same, synonymic invariant\nvalue, implies that the rank of the hidden weights matrix\ntends to v—the vocabulary size of higher-level features.\nThis low-rank structure is typical in the weights of deep\nnetworks trained on image classification [37–40].\nUsing all patches via weight sharing.\nNotice that\nusing a one-hidden-layer CNN which looks at all patches via\n8\n151\n102\n103\n104\n105\n106\ntrain set size P\n0.00\n0.25\n0.50\n0.75\n1.00\nsensitivity S1,1\n10−1\n100\n101\nP/ncmL\n0.00\n0.25\n0.50\n0.75\n1.00\nsensitivity S1,1\nm = v = 24\nm = v = 16\nm = v = 11\nm = v = 8\nL = 2\nL = 3\nL = 4\nFigure 11: Synonymic sensitivity of the hidden representa-\ntion vs P for a one-hidden-layer fully-connected network\ntrained on the first patch of the inputs of an RHM with\ns = 2 and m = v, for several values of L, v, and nc ≤v. The\ntop panel shows the bare curves whereas, in the bottom\npanel, the x-axis is rescaled by Pc = ncmL. The collapse\nof the rescaled curves highlights that Pc coincides with the\nthreshold number of training data for building a synonymic\ninvariant representation.\nweight sharing and global average pooling would yield the\nsame result since the average over patches reduces both the\nsignal and the noise by the same factor–see subsection C.1\nfor details.\nImproved Performance via Clustering.\nNote that\nour signal-vs-noise argument is based on a single class\nα, as it considers the scalar quantity ˆf(α|µ). However,\nan observer seeking to identify synonyms could in prin-\nciple use the information from all classes, represented\nby the nc-dimensional vector of empirical frequencies\n( ˆf(α|µ))α=1,...,nc. Following this idea, one can devise a\nlayer-wise algorithm where the representations of each layer\nare first updated with a single step of gradient descent (as\nin Eq. 82), then clustered into synonymic groups [22,27].\nSuch an algorithm can solve the RHM with less than\nncmL training points—√ncmL in the maximal dataset\ncase nc = v and m = vs−1, as we show empirically and jus-\ntify theoretically in Appendix D. Notably, the dependence\non the dimensionality mL is unaffected by the change of\nalgorithm, although the prefactor reveals the advantage of\nthe dedicated clustering algorithm over standard CNNs.\n5\nConclusions\nWe have introduced a hierarchical model of classification\ntask, where each class is identified by a number of equivalent\nhigh-level features (synonyms), themselves consisting of a\nnumber of equivalent sub-features according to a hierarchy\nof random composition rules. First, we established via\na combination of extensive esperiments and theoretical\narguments that the sample complexity of deep CNNs is a\nsimple function of the number of classes nc, the number\nof synonymic features m and the depth of the hierarchy L.\nThis result provides a rule of thumb for estimating the order\nof magnitude of the sample complexity of real datasets. In\nthe case of CIFAR10 [41], for instance, having 10 classes,\ntaking reasonable values for the RHM parameters such as\nm ∈[5, 15] and L = 3, yields P ∗∈[103, 3×104],comparable\nwith the sample complexity of modern architectures (see\nFig. 16 in Appendix G).\nSecondly, our results indicate a separation between shal-\nlow networks, which are cursed by the input dimensionality,\nand sufficiently deep CNNs, which are not. We thus com-\nplement previous analyses based on expressivity [21] or\ninformation-theoretical considerations [24] with a general-\nization result.\nLast but not least, we proposed to characterize the qual-\nity of internal representations with their sensitivity toward\ntransformations of the data which leave the task invari-\nant. This analysis bypasses the issues of previous char-\nacterizations. For example, approaches based on mutual\ninformation [12] that is ill-defined when the network rep-\nresentations are deterministic functions of the input [13].\nApproaches based on intrinsic dimension [14,15] can dis-\nplay counter-intuitive results, refer to Appendix F for a\nmore in-depth discussion on the intrinsic dimension, and\non how this quantity behaves in our setup. Interestingly,\nour approach supports that performance should strongly\ncorrelate with the invariance toward synonyms of the inter-\nnal representation. This prediction could in principle be\ntested in natural language processing models, but also for\nimage data sets by performing discrete changes to images\nthat leave the class unchanged.\nLooking forward, the Random Hierarchy Model is a\nrich but minimal model where open questions in the the-\nory of deep learning could be clarified. For instance, a\nformidable challenge such as the description of the gradient\ndescent dynamics of deep networks, becomes significantly\nsimpler for the RHM, owing to the simple structure of\nthe target representations. Other important questions, in-\ncluding the ability of fully-connected networks to learn\n9\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n152\nlocal connections [30,42,43], the benefits of residual con-\nnections [44] or the advantages of deep learning over kernel\nmethods [25,45–47] can be studied quantitatively within\nthis model, as functions of the multiple parameters that\ndefine the hierarchical structure of the task.\nMaterials and Methods\nExperimental Setup\nThe experiments are performed using the PyTorch deep\nlearning framework [48]. The code used for the experiments\nis available online at https://github.com/pcsl-epfl/\nhierarchy-learning.\nRHM implementation\nThe\ncode\nimplementing\nthe\nRHM\nis\navail-\nable\nonline\nat\nhttps://github.com/pcsl-epfl/\nhierarchy-learning/blob/master/datasets/\nhierarchical.py.\nThe\ninputs\nsampled\nfrom\nthe\nRHM are represented as a one-hot encoding of low-level\nfeatures. This makes each input of size sL × v. The inputs\nare whitened so that the average pixel value over channels\nis equal to zero.\nModel Architecture\nOne-hidden-layer fully-connected networks have input\ndimension equal to sL×v, H = 104 hidden neurons, and nc\noutputs. The deep convolutional neural networks (CNNs)\nhave weight sharing, stride equal to filter size equal to s\nand L hidden layers. In this case, we set the width H to\nbe larger than the number of possible s-tuples that can\nexist at a given layer, H ≫vs.\nTraining Procedure\nNeural networks are trained using stochastic gradient\ndescent (SGD) on the cross-entropy loss, with a batch size\nof 128 and a learning rate equal to 0.3. Training is stopped\nwhen the training loss decreases below a certain threshold\nfixed to 10−3.\nMeasurements\nThe performance of the models is measured as the per-\ncentage error on a test set. The test set size is chosen to be\nmin(Pmax−P, 20′000). Synonymic sensitivity, as defined in\nEq. 8, is measured on a test set of size min(Pmax−P, 1′000).\nReported results for a given value of RHM parameters are\naveraged over 10 jointly different instances of the RHM\nand network initializations.\nReferences\n[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning. Nature, 521(7553):436, 2015.\n[2] Athanasios Voulodimos, Nikolaos Doulamis, et al.\nDeep learning for computer vision: A brief review.\nComputational intelligence and neuroscience, 2018.\n[3] David Silver, Julian Schrittwieser, Karen Simonyan,\nIoannis Antonoglou,\nAja Huang,\nArthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian\nBolton, et al. Mastering the game of go without human\nknowledge. Nature, 550(7676):354–359, 2017.\n[4] OpenAI.\nGPT-4 Technical Report, March 2023.\narXiv:2303.08774 [cs].\n[5] Francis Bach. Breaking the curse of dimensionality\nwith convex neural networks. The Journal of Machine\nLearning Research, 18(1):629–681, 2017.\n[6] Ulrike von Luxburg and Olivier Bousquet. Distance-\nbased classification with lipschitz functions. The Jour-\nnal of Machine Learning Research, 5(Jun):669–695,\n2004.\n[7] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. IEEE Conference on Computer Vision and\nPattern Recognition, pages 248–255, 2009.\n[8] Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Gold-\nblum, and Tom Goldstein. The Intrinsic Dimension of\nImages and Its Impact on Learning. International Con-\nference on Learning Representations, January 2021.\n[9] Matthew D. Zeiler and Rob Fergus. Visualizing and\nUnderstanding Convolutional Networks. In Computer\nVision – ECCV 2014, Lecture Notes in Computer\nScience, pages 818–833, Cham, 2014.\n[10] Diego Doimo, Aldo Glielmo, Alessio Ansuini, and\nAlessandro Laio. Hierarchical nucleation in deep neural\nnetworks. Advances in Neural Information Processing\nSystems, 33:7526–7536, 2020.\n[11] Joan Bruna and St´ephane Mallat. Invariant scattering\nconvolution networks. IEEE transactions on pattern\nanalysis and machine intelligence, 35(8):1872–1886,\n2013.\n[12] Ravid Shwartz-Ziv and Naftali Tishby. Opening the\nBlack Box of Deep Neural Networks via Information,\nApril 2017. arXiv:1703.00810 [cs].\n[13] Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu\nAdvani, Artemy Kolchinsky, Brendan D Tracey, and\n10\n153\nDavid D Cox. On the information bottleneck theory\nof deep learning. Journal of Statistical Mechanics:\nTheory and Experiment, 2019(12):124020, 2019.\n[14] Alessio Ansuini, Alessandro Laio, Jakob H Macke,\nand Davide Zoccolan.\nIntrinsic dimension of data\nrepresentations in deep neural networks. Advances in\nNeural Information Processing Systems, 32:6111–6122,\n2019.\n[15] Stefano Recanatesi, Matthew Farrell, Madhu Ad-\nvani, Timothy Moore, Guillaume Lajoie, and Eric\nShea-Brown.\nDimensionality compression and ex-\npansion in Deep Neural Networks, October 2019.\narXiv:1906.00443 [cs, stat].\n[16] Leonardo Petrini, Alessandro Favero, Mario Geiger,\nand Matthieu Wyart. Relative stability toward dif-\nfeomorphisms indicates performance in deep nets.\nAdvances in Neural Information Processing Systems,\n34:8727–8739, 2021.\n[17] Umberto M. Tomasini, Leonardo Petrini, Francesco\nCagnetta, and Matthieu Wyart. How deep convolu-\ntional neural networks lose spatial information with\ntraining, November 2022. arXiv:2210.01506 [cs].\n[18] Ankit B. Patel, Tan Nguyen, and Richard G. Baraniuk.\nA Probabilistic Theory of Deep Learning, April 2015.\narXiv:1504.00641 [cs, stat].\n[19] Elchanan Mossel. Deep Learning and Hierarchal Gen-\nerative Models, September 2018. arXiv:1612.09057\n[cs].\n[20] Hrushikesh Mhaskar, Qianli Liao, and Tomaso Pog-\ngio. When and Why Are Deep Networks Better Than\nShallow Ones? Proceedings of the AAAI Conference\non Artificial Intelligence, 31(1), February 2017.\n[21] Tomaso\nPoggio,\nHrushikesh\nMhaskar,\nLorenzo\nRosasco, Brando Miranda, and Qianli Liao.\nWhy\nand when can deep-but not shallow-networks avoid\nthe curse of dimensionality: a review. International\nJournal of Automation and Computing, 14(5):503–519,\n2017.\n[22] Eran Malach and Shai Shalev-Shwartz. A Provably\nCorrect Algorithm for Deep Learning that Actually\nWorks, June 2018. arXiv:1803.09522 [cs, stat].\n[23] Javier Zazo, Bahareh Tolooshams, Demba Ba, and\nHarvard John A. Paulson. Convolutional dictionary\nlearning in hierarchical networks. IEEE 8th Interna-\ntional Workshop on Computational Advances in Multi-\nSensor Adaptive Processing, pages 131–135, 2019.\n[24] Johannes Schmidt-Hieber. Nonparametric regression\nusing deep neural networks with relu activation func-\ntion. The Annals of Statistics, 48(4):1875–1897, 2020.\n[25] Francesco Cagnetta, Alessandro Favero, and Matthieu\nWyart. What can be learnt with wide convolutional\nneural networks? International Conference on Ma-\nchine Learning, 2022.\n[26] Shai Shalev-Shwartz, Ohad Shamir, and Shaked\nShammah. Failures of gradient-based deep learning. In\nInternational Conference on Machine Learning, pages\n3067–3075. PMLR, 2017.\n[27] Eran Malach and Shai Shalev-Shwartz. The impli-\ncations of local correlation on learning some deep\nfunctions. Advances in Neural Information Processing\nSystems, 33:1322–1332, 2020.\n[28] Andrew M Saxe, James L McClelland, and Surya\nGanguli. Exact solutions to the nonlinear dynamics of\nlearning in deep linear neural networks. International\nConference on Learning Representations, 2014.\n[29] Marc M´ezard. Mean-field message-passing equations\nin the hopfield model and its generalizations. Physical\nReview E, 95(2):022117, 2017.\n[30] Alessandro Ingrosso and Sebastian Goldt. Data-driven\nemergence of convolutional structure in neural net-\nworks. Proceedings of the National Academy of Sci-\nences, 119(40), 2022.\n[31] Yu Feng and Yuhai Tu. The inverse variance–flatness\nrelation in stochastic gradient descent is critical for\nfinding flat minima.\nProceedings of the National\nAcademy of Sciences, 118(9), 2021.\n[32] Grzegorz Rozenberg and Arto Salomaa. Handbook of\nFormal Languages. Springer, January 1997.\n[33] Michael Kearns. Efficient noise-tolerant learning from\nstatistical queries. Journal of the ACM, 45(6):983–\n1006, November 1998.\n[34] Shai Shalev-Shwartz and Shai Ben-David.\nUnder-\nstanding machine learning: From theory to algorithms.\nCambridge university press, 2014.\n[35] Alexandru\nDamian,\nJason\nLee,\nand\nMahdi\nSoltanolkotabi.\nNeural networks can learn rep-\nresentations with gradient descent.\nProceedings\nof Thirty-Fifth Conference on Learning Theory,\n178:5413–5452, 02–05 Jul 2022.\n[36] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao\nWang, Denny Wu, and Greg Yang. High-dimensional\nasymptotics of feature learning: How one gradient\n11\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n154\nstep improves the representation. Advances in Neu-\nral Information Processing Systems, 35:37932–37946,\n2022.\n[37] Misha Denil, Babak Shakibi, Laurent Dinh, Marc' Au-\nrelio Ranzato, and Nando de Freitas. Predicting pa-\nrameters in deep learning. In C.J. Burges, L. Bottou,\nM. Welling, Z. Ghahramani, and K.Q. Weinberger,\neditors, Advances in Neural Information Processing\nSystems, volume 26. Curran Associates, Inc., 2013.\n[38] Emily L Denton, Wojciech Zaremba, Joan Bruna,\nYann LeCun, and Rob Fergus. Exploiting linear struc-\nture within convolutional networks for efficient eval-\nuation. In Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K.Q. Weinberger, editors, Advances\nin Neural Information Processing Systems, volume 27.\nCurran Associates, Inc., 2014.\n[39] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng\nTao. On compressing deep models by low rank and\nsparse decomposition. 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n67–76, 2017.\n[40] Florentin Guth, Brice M´enard, Gaspar Rochette, and\nSt´ephane Mallat. A Rainbow in Deep Network Black\nBoxes, May 2023. arXiv:2305.18512 [cs, eess].\n[41] Alex Krizhevsky. Learning multiple layers of features\nfrom tiny images. Technical report, 2009.\n[42] Behnam Neyshabur. Towards Learning Convolutions\nfrom Scratch. Advances in Neural Information Pro-\ncessing Systems, 33:8078–8088, 2020.\n[43] Franco Pellegrini and Giulio Biroli. Sifting out the\nfeatures by pruning: Are convolutional networks the\nwinning lottery ticket of fully connected ones?, May\n2021. arXiv:2104.13343 [cs, stat].\n[44] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual\nLearning for Image Recognition. IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 770–778, June 2016.\n[45] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler.\nNeural tangent kernel: Convergence and generalization\nin neural networks. Advances in Neural Information\nProcessing Systems, 31:8580–8589, 2018.\n[46] Mario Geiger, Leonardo Petrini, and Matthieu Wyart.\nLandscape and training regimes in deep learning.\nPhysics Reports, 924, April 2021.\n[47] Alessandro Favero, Francesco Cagnetta, and Matthieu\nWyart. Locality defeats the curse of dimensionality in\nconvolutional teacher-student scenarios. Advances in\nNeural Information Processing Systems, 34:9456–9467,\n2021.\n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. PyTorch: An Imperative\nStyle, High-Performance Deep Learning Library. Ad-\nvances in Neural Information Processing Systems, 32,\n2019.\n12\n155\nAppendix\nA\nStatistics of The Composition Rules\nIn this section, we consider a single composition rule, that is the assignment of m s-tuples of low-level features to\neach of the v high-level features. In the RHM these rules are chosen uniformly at random over all the possible rules,\nthus their statistics are crucial in determining the correlations between the input features and the class label.\nA.1\nStatistics of a single rule\nFor each rule, we call Ni(µ1; µ2) the number of occurrences of the low-level feature µ1 in position i of the s-tuples\ngenerated by the higher-level feature µ2. The probability of Ni(µ1; µ2) is that of the number of successes when drawing\nm (number of s-tuples associated with the high-level feature µ2) times without replacement from a pool of vs (total\nnumber of s-tuples with vocabulary size v) objects where only vs−1 satisfy a certain condition (number of s-tuples\ndisplaying feature µ1 in position i):\nPr {Ni(µ0; µ1) = k} =\n\u0012vs−1\nk\n\u0013\u0012vs −vs−1\nm −k\n\u0013 \u001e\u0012vs\nm\n\u0013\n,\n(12)\nwhich is a hypergeometric distribution Hgvs,vs−1,m, with mean\n⟨N⟩= mvs−1\nvs\n= m\nv ,\n(13)\nand variance\nσ2\nN :=\nD\n(N −⟨N⟩)2E\n= mvs−1\nvs\nvs −vs−1\nvs\nvs −m\nvs −1 = m\nv\nv −1\nv\nvs −m\nvs −1\nm ≫1\n−−−−→m\nv ,\n(14)\nindependently of the position i and the specific low- and high-level features. Notice that, since m ≤vs−1 with s fixed,\nlarge m implies also large v.\nA.2\nJoint statistics of a single rule\nShared high-level feature.\nFor a fixed high-level feature µ2, the joint probability of the occurrences of two different\nlow-level features µ1 and ν1 is a multivariate Hypergeometric distribution,\nPr {Ni(µ1; µ2) = k; Ni(ν1; µ2) = l} =\n\u0012vs−1\nk\n\u0013\u0012vs−1\nl\n\u0013\u0012vs −2vs−1\nm −k −l\n\u0013 \u001e\u0012vs\nm\n\u0013\n,\n(15)\ngiving the following covariance,\ncN := ⟨(Ni(µ1; µ2) −⟨N⟩) (Ni(ν1; µ2) −⟨N⟩)⟩= −m\nv2\nvs −m\nvs −1\nm ≫1\n−−−−→−\n\u0010m\nv\n\u00112 1\nm.\n(16)\nThe covariance can also be obtained via the constraint P\nµ1 Ni(µ1; µ2) = m. For any finite sequence of identically\ndistributed random variables Xµ with a constraint on the sum P\nµ Xµ = m,\nv\nX\nµ=1\nXµ = m ⇒\nv\nX\nµ=1\n(Xµ −⟨Xµ⟩) = 0 ⇒\n(Xν −⟨Xν⟩)\nv\nX\nµ=1\n(Xµ −⟨Xµ⟩) = 0 ⇒\nv\nX\nµ=1\n⟨(Xν −⟨Xν⟩)(Xµ −⟨Xµ⟩)⟩= 0 ⇒\nVar [Xµ] + (v −1)Cov [Xµ, Xν] = 0.\n(17)\nIn the last line, we used the identically distributed variables hypothesis to replace the sum over µ ̸= ν with the factor\n(v −1). Therefore,\ncN = Cov [Ni(µ1; µ2), Ni(ν1; µ2)] = −Var [Ni(µ1; µ2)]\nv −1\n= −σ2\nN\nv −1.\n(18)\n13\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n156\nShared low-level feature.\nThe joint probability of the occurrences of the same low-level feature µ1 starting from\ndifferent high-level features µ2 ̸= ν2 can be written as follows,\nPr {N(µ1; µ2) = k; N(µ1; ν2) = l} = Pr {N(µ1; µ2) = k|N(µ1; ν2) = l} × Pr {N(µ1; ν2) = l}\n(19)\n= Hgvs−m,vs−1−l,m(k) × Hgvs,vs−1,m(l),\n(20)\nresulting in the following ‘inter-feature’ covariance,\ncif := Cov [Ni(µ1; µ2), Ni(µ1; ν2)] = −\n\u0010m\nv\n\u00112 v −1\nvs −1.\n(21)\nNo shared features.\nFinally, by multiplying both sides of P\nµ1 N(µ1; µ2) = m with N(ν1; ν2) and averaging, we get\ncg := Cov [Ni(µ1; µ2), Ni(ν1; ν2)] = −Cov [Ni(µ1; µ2), Ni(µ1; ν2)]\nv −1\n=\n\u0010m\nv\n\u00112\n1\nvs −1.\n(22)\nB\nEmergence of input-output correlations (Pc)\nAs discussed in the main text, the Random Hierarchy Model presents a characteristic sample size Pc corresponding\nto the emergence of the input-output correlations. This sample size predicts the sample complexity of deep CNNs, as\nwe also discuss in the main text. In this appendix, we prove that\nPc\nnc,m→∞\n−−−−−−→ncmL.\n(23)\nB.1\nEstimating the Signal\nThe correlations between input features and the class label can be quantified via the conditional probability (over\nrealizations of the RHM) of a data point belonging to class α conditioned on displaying the s-tuple µ in the j-th input\npatch,\nfj(α|µ) := Pr {x ∈α|xj = µ} ,\n(24)\nwhere the notation xj = µ means that the elements of the patch xj encode the tuple of features µ. We say that the\nlow-level features are correlated with the output if\nfj(α|µ) ̸= 1\nnc\n,\n(25)\nand define a ‘signal’ as the difference fj(α|µ) −n−1\nc . In the following, we compute the statistics of the signal over\nrealizations of the RHM.\nOccurrence of low-level features.\nLet us begin by defining the joint occurrences of a class label α and a low-level\nfeature µ1 in a given position of the input. Using the tree representation of the model, we will identify an input position\nwith a set of L indices iℓ= 1, . . . , s, each indicating which branch to follow when descending from the root (class label)\nto a given leaf (low-level feature). These joint occurrences can be computed by combining the occurrences of the single\nrules introduced in Appendix A of this Appendix. With L = 2, for instance,\nN (1→2)\ni1i2\n(µ1; α) =\nv\nX\nµ2=1\n\u0010\nms−1N (1)\ni1 (µ1; µ2)\n\u0011\n× N (2)\ni2 (µ2; α),\n(26)\nwhere:\ni) N (2)\ni2 (µ2; α)5 counts the occurrences of µ2 in position i2 of the level-2 representations of α, i.e. the s-tuples\ngenerated from α according to the second-layer composition rule;\n5We are using the superscript (ℓ) to differentiate the occurrences of the different composition rules.\n14\n157\nii) N (1)\ni1 (µ1; µ2) counts the occurrences of µ1 in position i1 of the level-1 representations of µ2, i.e. s-tuples generated\nby µ2 according to the composition rule of the first layer;\niii) the factor ms−1 counts the descendants of the remaining s−1 elements of the level-2 representation (m descendants\nper element);\niv) the sum over µ2 counts all the possible paths of features that lead to µ1 from α across 2 generations.\nThe generalization of Eq. 26 is immediate once one takes into account that the multiplicity factor accounting for the\ndescendants of the remaining positions at the ℓ-th generation is equal to msℓ−1/m (sℓ−1 is the size of the representation\nat the previous level). Hence, the overall multiplicity factor after L generations is\n1 × ms\nm × ms2\nm × · · · × msL−1\nm\n= m\nsL−1\ns−1 −L,\n(27)\nso that the number of occurrences of feature µ1 in position i1 . . . iL of the inputs belonging to class α is\nN (1→L)\ni1→L (µ1; α) = m\nsL−1\ns−1 −L\nv\nX\nµ2,...,µL=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; α),\n(28)\nwhere we used i1→L as a shorthand notation for the tuple of indices i1, i2, . . . , iL.\nThe same construction allows us to compute the number of occurrences of up to s−1 features within the s-dimensional\npatch of the input corresponding to the path i2→L. The number of occurrences of a whole s-tuple, instead, follows a\nslightly different rule, since there is only one level-2 feature µ2 which generates the whole s-tuple of level-1 features\nµ1 = (µ1,1, . . . , µ1,s)—we call this feature g1(µ1), with g1 denoting the first-layer composition rule. As a result, the\nsum over µ2 in the right-hand side of Eq. 28 disappears and we are left with\nN (1→L)\ni2→L (µ1; α) = m\nsL−1\ns−1 −L\nv\nX\nµ3,...,µL=1\nN (2)\ni2 (g1(µ1); µ3) × · · · × N (L)\niL (µL; α).\n(29)\nCoincidentally, Eq. 29 shows that the joint occurrences of a s-tuple of low-level features µ1 depend on the level-2\nfeature corresponding to µ1. Hence, N (1→L)\ni2→L (µ1; α) is invariant for the exchange of µ1 with one of its synonyms, i.e.\nlevel-1 tuples ν1 corresponding to the same level-2 feature.\nClass probability conditioned on low-level observations.\nWe can turn these numbers into probabilities by\nnormalizing them appropriately. Upon dividing by the total occurrences of a low-level feature µ1 independently of the\nclass, for instance, we obtain the conditional probability of the class of a given input, conditioned on the feature in\nposition i1 . . . iL being µ1.\nf (1→L)\ni1→L (α|µ1) :=\nN (1→L)\ni1→L (µ1; α)\nnc\nX\nα′=1\nN (1→L)\ni1→L (µ1; α′)\n=\nv\nX\nµ2,...,µL=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; α)\nv\nX\nµ2,...,µL=1\nnc\nX\nµL+1=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; µL+1)\n.\n(30)\nLet us also introduce, for convenience, the numerator and denominator of the right-hand side of Eq. 30.\nU (1→L)\ni1→L (µ1α) =\nv\nX\nµ2,...,µL=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; α);\nD(1→L)\ni1→L (µ1) =\nnc\nX\nα=1\nU (1→L)\ni1→L (µ1; α).\n(31)\nB.1.1\nStatistics of the numerator U\nWe now determine the first and second moments of the numerator of f (1→L)\ni1→L (µ1; α). Let us first recall the definition\nfor clarity,\nU (1→L)\ni1→L (µ1; α) =\nv\nX\nµ2,...,µL=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; α)\n(32)\n15\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n158\nLevel 1 L = 1.\nFor L = 1, U is simply the occurrence of a single production rule Ni(µ1; α),\nD\nU (1)E\n= m\nv ;\n(33)\nσ2\nU (1) := Var\nh\nU (1)i\n= m\nv\nv −1\nv\nvs −m\nvs −1\nv≫1\n−−−→m\nv ;\n(34)\ncU (1) := Cov\nh\nU (1)(µ1; α), U (1)(ν1; α)\ni\n= −Var\n\u0002\nU (1)\u0003\n(v −1)\n= −\n\u0010m\nv\n\u00112 vs −m\nvs −1\n1\nm\nv≫1\n−−−→\n\u0010m\nv\n\u00112 1\nm;\n(35)\nwhere the relationship between variance and covariance is due to the constraint on the sum of U (1) over µ1, see Eq. 17.\nLevel 2 L = 2.\nFor L = 2,\nU (1→2)\ni1→2 (µ1; α) =\nv\nX\nµ2=1\nN (1)\ni1 (µ1; µ2) × N (2)\ni2 (µ3; α) =\nv\nX\nµ2=1\nN (1)\ni1 (µ1; µ2)U (2)\ni2 (µ2; α).\n(36)\nTherefore,\nD\nU (1→2)E\n= v\n\u0010m\nv\n\u0011\n×\nD\nU (1)E\n= v\n\u0010m\nv\n\u00112\n;\n(37)\nσ2\nU (2) := Var\nh\nU (1→2)i\n=\nv\nX\nµ2,ν1=1\n\u0012D\nN (1)(µ1; µ2)N (1)(µ1; ν2)\nE D\nU (2)(µ2; α)U (2)(ν2; α)\nE\n−⟨N⟩2 D\nU (1)E2\u0013\n=\nX\nµ2,ν2=µ2\n· · · +\nX\nµ2\nX\nν2̸=µ2\n. . .\n= v\n\u0012\nσ2\nNσ2\nU (1) + σ2\nN\nD\nU (1)E2\n+ σ2\nU (1) ⟨N⟩2\n\u0013\n+ v(v −1)\n\u0012\ncifcU (1) + cif\nD\nU (1)E2\n+ cU (1) ⟨N⟩2\n\u0013\n= v\n\u0000σ2\nNσ2\nU (1) + (v −1)cifcU (1)\n\u0001\n+ v\nD\nU (1)E2 \u0000σ2\nN + (v −1)cif\n\u0001\n+ v ⟨N⟩2 \u0000σ2\nU (1) + (v −1)cU (1)\n\u0001\n= vσ2\nU (1)\n\u0000σ2\nN −cif\n\u0001\n+ v\nD\nU (1)E2 \u0000σ2\nN + (v −1)cif\n\u0001\n,\n(38)\ncU (2) = −σ2\nU (2)\n(v −1)\n(39)\nLevel L.\nIn general,\nU (1→L)\ni1→L (µ1; α) =\nv\nX\nµ2=1\nN (1)\ni1 (µ1; µ2)U (2→L)\ni2→L (µ2; α).\n(40)\nTherefore,\nD\nU (L)E\n= v\n\u0010m\nv\n\u0011\n×\nD\nU (L−1)E\n= vL−1 \u0010m\nv\n\u0011L\n;\n(41)\nσ2\nU (L) =\nv\nX\nµ2,ν1=1\n\u0012D\nN (1)(µ1; µ2)N (1)(µ1; ν2)\nE D\nU (2→L)(µ2; α)U (2→L)(ν1; α)\nE\n−⟨N⟩2 D\nU (1→(L−1))E2\u0013\n=\nX\nµ2,ν2=µ2\n· · · +\nX\nµ2\nX\nν2̸=µ2\n. . .\n= v\n\u0012\nσ2\nNσ2\nU (L−1) + σ2\nN\nD\nU (L−1)E2\n+ σ2\nU (L−1) ⟨N⟩2\n\u0013\n+ v(v −1)\n\u0012\nσ2\nifcU (L−1) + cif\nD\nU (L−1)E2\n+ cU (L−1) ⟨N⟩2\n\u0013\n= vσ2\nU (L−1)\n\u0000σ2\nN −cif\n\u0001\n+ v\nD\nU (L−1)E2 \u0000σ2\nN + (v −1)cif\n\u0001\n,\n(42)\ncU (L) = −σ2\nU (L)\n(v −1)\n(43)\n16\n159\nConcentration for large m.\nIn the large multiplicity limit m ≫1, the U’s concentrate around their mean value.\nDue to m ≤vs−1, large m implies large v, thus we can proceed by setting m = qvs−1, with q ∈(0, 1] 6 and studying the\nv ≫1 limit. From Eq. 41,\nD\nU (L)E\n= qLvL(s−1)−1.\n(44)\nIn addition,\nσ2\nN\nv≫1\n−−−→m\nv = qv(s−1)−1,\ncif\nv≫1\n−−−→−\n\u0010m\nv\n\u00112\n1\nvs−1 = −q2v(s−1)−2,\n(45)\nso that\nσ2\nU (L) = vσ2\nU (L−1)\n\u0000σ2\nN −σ2\nif\n\u0001\n+ v\nD\nU (L−1)E2 \u0000σ2\nN + (v −1)σ2\nif\n\u0001\nv≫1\n−−−→σ2\nU (L−1)qv(s−1) + σ2\nU (L−1)q2v(s−1)−1 + q2L−1(1 −q)v(2L−1)(s−1)−2\n(46)\nThe second of the three terms is always subleading with respect to the first, so we can discard it for now. It remains to\ncompare the first and the third terms. For L = 2, since σ2\nU (1) = σ2\nN, the first term depends on v as v2(s−1)−1, whereas\nthe third is proportional to v3(s−1)−2. For L ≥3 the dominant scaling is that of the third term only: for L = 3 it can be\nshown by simply plugging the L = 2 result into the recursion, and for larger L it follows from the fact that replacing\nσ2\nU (L−1) in the first term with the third term of the precious step always yields a subdominant contribution. Therefore,\nσ2\nU (L)\nv≫1\n−−−→\n(\nq2v2(s−1)−1 + q3(1 −q)v3(s−1)−2,\nfor L = 2,\nq2L−1(1 −q)v(2L−1)(s−1)−2,\nfor L ≥3.\n(47)\nUpon dividing the variance by the squared mean we get\nσ2\nU (L)\n\nU (L)\u000b2\nv≫1\n−−−→\n\n\n\n\n\n\n\n1\nq2\n1\nv2(s−1)−1 + 1 −q\nq\n1\nv(s−1) ,\nfor L = 2,\n1 −q\nq\n1\nv(s−1) ,\nfor L ≥3,\n(48)\nwhose convergence to 0 guarantees the concentration of the U’s around the average over all instances of the RHM.\nB.1.2\nStatistics of the denominator D\nHere we compute the first and second moments of the denominator of f (1→L)\ni1→L (µ1; α),\nD(1→L)\ni1→L (µ1) =\nv\nX\nµ2,...,µL=1\nnc\nX\nµL+1=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; µL+1)\n(49)\nLevel 1 L = 1.\nFor L = 1, D is simply the sum over classes of the occurrences of a single production rule,\nD(1) = P\nα Ni(µ1; α),\nD\nD(1)E\n= nc\nm\nv ;\n(50)\nσ2\nD(1) := Var\nh\nD(1)i\n= ncσ2\nN + nc(nc −1)cif = nc\n\u0010m\nv\n\u00112 v −1\nvs −1\n\u0012vs\nm −nc\n\u0013\nv≫1\n−−−→nc\n\u0010m\nv\n\u00112 \u0010 v\nm −\nnc\nvs−1\n\u0011\n;\n(51)\ncD(1) := Cov\nh\nD(1)(µ1), D(1)(ν0)\ni\n= −Var\n\u0002\nD(1)\u0003\n(v −1)\n= nccN + nc(nc −1)cg,\n(52)\nwhere, in the last line, we used the identities σ2\nN + (v −1)cN = 0 from Eq. 16 and cif + (v −1)cg = 0 from Eq. 22.\n6The minimum m is 1, which corresponds to q = v1−s, but actually there is no stochasticity in the U’s and D’s in that case. Thus the\nminimal q is actually 2v1−s.\n17\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n160\nLevel 2 L = 2.\nFor L = 2,\nD(1→2)\ni1→2 (µ1) =\nv\nX\nµ2\nnc\nX\nµ3=1\nN (1)\ni1 (µ1; µ2) × N (2)\ni2 (µ2; µ3) =\nv\nX\nµ2=1\nN (1)\ni1 (µ1; µ2)D(2)\ni2 (µ2).\n(53)\nTherefore,\nD\nD(1→2)E\n= v\n\u0010m\nv\n\u0011\n×\nD\nD(1)E\n= nc\nv m2;\n(54)\nσ2\nD(2) := Var\nh\nD(1→2)i\n=\nv\nX\nµ2,ν1=1\n\u0012D\nN (1)(µ1; µ2)N (1)(µ1; ν1)\nE D\nD(2)(µ2)D(2)(ν1)\nE\n−⟨N⟩2 D\nD(1)E2\u0013\n=\nX\nµ2,ν1=µ2\n· · · +\nX\nµ2\nX\nν1̸=µ2\n. . .\n= v\n\u0012\nσ2\nNσ2\nD(1) + σ2\nN\nD\nD(1)E2\n+ σ2\nD(1) ⟨N⟩2\n\u0013\n+ v(v −1)\n\u0012\ncifcD(1) + cif\nD\nD(1)E2\n+ cD(1) ⟨N⟩2\n\u0013\n= v\n\u0000σ2\nNσ2\nD(1) + (v −1)cifcD(1)\n\u0001\n+ v\nD\nD(1)E2 \u0000σ2\nN + (v −1)cif\n\u0001\n+ v ⟨N⟩2 \u0000σ2\nD(1) + (v −1)cD(1)\n\u0001\n= vσ2\nD(1)\n\u0000σ2\nN −cif\n\u0001\n+ v\nD\nD(1)E2 \u0000σ2\nN + (v −1)cif\n\u0001\n,\n(55)\ncD(2) = −σ2\nD(2)\n(v −1).\n(56)\nLevel L.\nIn general,\nD(1→L)\ni1→L (µ1) =\nv\nX\nµ2=1\nN (1)\ni1 (µ1; µ2)D(2→L)\ni2→L (µ2).\n(57)\nTherefore,\nD\nD(L)E\n= v\n\u0010m\nv\n\u0011\n×\nD\nD(L−1)E\n= nc\nv mL;\n(58)\nσ2\nD(L) =\nv\nX\nµ2,ν1=1\n\u0012D\nN (1)(µ1; µ2)N (1)(µ1; ν1)\nE D\nD(2→L)(µ2; α)D(2→L)(ν1; α)\nE\n−⟨N⟩2 D\nD(1→(L−1))E2\u0013\n=\nX\nµ2,ν1=µ2\n· · · +\nX\nµ2\nX\nν1̸=µ2\n. . .\n= v\n\u0012\nσ2\nNσ2\nD(L−1) + σ2\nN\nD\nD(L−1)E2\n+ σ2\nD(L−1) ⟨N⟩2\n\u0013\n+ v(v −1)\n\u0012\ncifcD(L−1) + cif\nD\nD(L−1)E2\n+ cD(L−1) ⟨N⟩2\n\u0013\n= vσ2\nD(L−1)\n\u0000σ2\nN −cif\n\u0001\n+ v\nD\nD(L−1)E2 \u0000σ2\nN + (v −1)cif\n\u0001\n,\n(59)\ncD(L) = −σ2\nD(L)\n(v −1).\n(60)\nConcentration for large m.\nSince the D’s can be expressed as a sum of different U’s, their concentration for m ≫1\nfollows directly from that of the U’s.\nB.1.3\nEstimate of the conditional class probability\nWe can now turn back to the original problem of estimating\nf (1→L)\ni1→L (α|µ1) =\nv\nX\nµ2,...,µL=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; α)\nv\nX\nµ2,...,µL=1\nnc\nX\nµL+1=1\nN (1)\ni1 (µ1; µ2) × · · · × N (L)\niL (µL; µL+1)\n= U (1→L)\ni1→L (µ1; α)\nD(1→L)\ni1→L (µ1)\n.\n(61)\n18\n161\nHaving shown that both numerator and denominator converge to their average for large m, we can expand for small\nfluctuations around these averages and write\nf (1→L)\ni1→L (α|µ1) =\nv−1mL\n\u0012\n1 +\nU (1→L)\ni1→L (µ1;α)−mL/v\nmL/v\n\u0013\nncv−1mL\n\u0012\n1 +\nD(1→L)\ni1→L (µ1)−ncmL/v\nmL\n\u0013\n(62)\n= 1\nnc\n+ 1\nnc\nU (1→L)\ni1→L (µ1; α) −mL/v\nmL/v\n−1\nnc\nD(1→L)\ni1→L (µ1) −ncmL/v\nmL/v\n= 1\nnc\n+\nv\nncmL\n\u0012\nU (1→L)\ni1→L (µ1; α) −1\nnc\nD(1→L)\ni1→L (µ1)\n\u0013\n.\n(63)\nSince the conditional frequencies average to n−1\nc , the term in brackets averages to zero. We can then estimate the size\nof the fluctuations of the conditional frequencies (i.e. the ‘signal’) with the standard deviation of the term in brackets.\nIt is important to notice that, for each L and position i1→L, D is the sum over α of U, and the U with different α\nat fixed low-level feature µ1 are identically distributed. In general, for a sequence of identically distributed variables\n(Xα)α=1,...,nc,\n*\n1\nnc\nv\nX\nβ=1\nXβ\n\n\n2+\n= 1\nn2c\nnc\nX\nβ=1\n\n⟨Xβ⟩2 +\nX\nβ′̸=β\n⟨XβXβ′⟩\n\n= 1\nnc\n\n⟨Xβ⟩2 +\nX\nβ′̸=β\n⟨XβXβ′⟩\n\n.\n(64)\nHence,\n*\nXα −1\nnc\nnc\nX\nβ=1\nXβ\n\n\n2+\n=\n\nX2\nα\n\u000b\n+ n−2\nc\nnc\nX\nβ,γ=1\n⟨XβXγ⟩−2n−1\nc\nnc\nX\nβ=1\n⟨XαXβ⟩\n=\n\nX2\nα\n\u000b\n−n−1\nc\n\n⟨Xα⟩2 +\nX\nβ̸=α\n⟨XαXβ⟩\n\n\n=\n\nX2\nα\n\u000b\n−n−2\nc\n*\n\nnc\nX\nβ=1\nXβ\n\n\n2+\n.\n(65)\nIn our case\n*\u0012\nU (1→L)\ni1→L (µ1; α) −1\nnc\nD(1→L)\ni1→L (µ1)\n\u00132+\n=\n\u001c\u0010\nU (1→L)\ni1→L (µ1; α)\n\u00112\u001d\n−nc\n−2\n\u001c\u0010\nD(1→L)\ni1→L (µ1)\n\u00112\u001d\n= σ2\nU (L) −n−2\nc σ2\nD(L),\n(66)\nwhere, in the second line, we have used that\n\nU (L)\u000b\n=\n\nD(L)\u000b\n/nc to convert the difference of second moments into a\ndifference of variances. By Eq. 41 and Eq. 58,\nσ2\nU (L) −n−2\nc σ2\nD(L) = vσ2\nU (L−1)\n\u0000σ2\nN −σ2\nif\n\u0001\n+ v\nD\nU (L−1)E2 \u0000σ2\nN + (v −1)σ2\nif\n\u0001\n−v\nn2c\nσ2\nD(L−1)\n\u0000σ2\nN −σ2\nif\n\u0001\n−v\nn2c\nD\nD(L−1)E2 \u0000σ2\nN + (v −1)σ2\nif\n\u0001\n= v\n\u0000σ2\nN −σ2\nif\n\u0001 \u0000σ2\nU (L−1) −n−2\nc σ2\nD(L−1)\n\u0001\n,\n(67)\nhaving used again that\n\nU (L)\u000b\n=\n\nD(L)\u000b\n/nc. Iterating,\nσ2\nU (L) −n−2\nc σ2\nD(L) =\n\u0002\nv\n\u0000σ2\nN −σ2\nif\n\u0001\u0003L−1 \u0000\u0000σ2\nU (1) −n−2\nc σ2\nD(1)\n\u0001\u0001\n.\n(68)\n19\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n162\nSince\nσ2\nU (1) = m\nv\nv −1\nv\nvs −m\nvs −1\nv≫1\n−−−→m\nv ,\nn−2\nc σ2\nD(1) = n−1\nc σ2\nN + n−1\nc (nc −1)σ2\nif\nv≫1\n−−−→n−1\nc\n\u0010m\nv\n\u00112 \u0010 v\nm −\nnc\nvs−1\n\u0011\n= 1\nnc\nm\nv\n\u0010\n1 −mnc\nvs\n\u0011\n,\n(69)\nOne has\nσ2\nU (L) −n−2\nc σ2\nD(L)\nv≫1\n−−−→mL\nv\n\u0012\n1 −1 −ncm/vs\nnc\n\u0013\n,\n(70)\nso that\nVar\nh\nf (1→L)\ni1→L (α|µ1)\ni\n= v2\n\u001c\u0010\nU (1→L)\ni1→L (µ1; α) −\n1\nnc D(1→L)\ni1→L (µ1)\n\u00112\u001d\nn2cm2L\nv,nc≫1\n−−−−−→v\nnc\n1\nncmL .\n(71)\nB.2\nIntroducing sampling noise due to the finite training set\nIn a supervised learning setting where only P of the total data are available, the occurrences N are replaced with\ntheir empirical counterparts ˆN. In particular, the empirical joint occurrence ˆN(µ; α)7 coincides with the number of\nsuccesses when sampling P points without replacement from a population of Pmax where only N(µ; α) belong to class\nα and display feature µ in position j. Thus, ˆN(µ; α) obeys a hypergeometric distribution where P plays the role of the\nnumber of trials, Pmax the population size, and the true occurrence N(µ; α) the number of favorable cases. If P is large\nand Pmax, N(µ; α) are both larger than P, then\nˆN(µ; α) →N\n\u0012\nP N(µ; α)\nPmax\n, P N(µ; α)\nPmax\n\u0012\n1 −N(µ; α)\nPmax\n\u0013\u0013\n,\n(72)\nwhere the convergence is meant as a convergence in probability and N(a, b) denotes a Gaussian distribution with mean\na and variance b. The statement above holds when the ratio N(µ; α)/Pmax is away from 0 and 1, which is true with\nprobability 1 for large v due to the concentration of f(α|µ). In complete analogy, the empirical occurrence ˆN(µ) obeys\nˆN(µ) →N\n\u0012\nP N(µ)\nPmax\n, P N(µ)\nPmax\n\u0012\n1 −N(µ)\nPmax\n\u0013\u0013\n.\n(73)\nWe obtain the empirical conditional frequency by the ratio of Eq. 72 and Eq. 73.\nSince N(µ) = Pmax/v and\nf(α|µ) = N(µ; α)/N(µ), we have\nˆf(α|µ) =\nf(α|µ)\nv\n+ ξP\nr\n1\nP\nf(α|µ)\nv\n\u0010\n1 −f(α|µ)\nv\n\u0011\n1\nv + ζP\nq\n1\nP\n1\nv\n\u00001 −1\nv\n\u0001\n,\n(74)\nwhere ξP and ζP are correlated zero-mean and unit-variance Gaussian random variables over independent drawings of\nthe P training points. By expanding the denominator of the right-hand side for large P we get, after some algebra,\nˆf(α|µ) ≃f(α|µ) + ξP\ns\nvf(α|µ)\nP\n\u0012\n1 −f(α|µ)\nv\n\u0013\n−ζP f(α|µ)\ns\nv\nP\n\u0012\n1 −1\nv\n\u0013\n.\n(75)\nRecall that, in the limit of large nc and m, f(α|µ) = n−1\nc (1 + σfξRHM) where ξRHM is a zero-mean and unit-variance\nGaussian variable over the realizations of the RHM, while σf is the ‘signal’, σ2\nf = v/mL by Eq. 71. As a result,\nˆf(α|µ)\nnc,m,P ≫1\n−−−−−−−→1\nnc\n\u0012\n1 +\nr v\nmL ξRHM +\nrvnc\nP ξP\n\u0013\n.\n(76)\n7For ease of notation, we drop level and positional indices in this subsection.\n20\n163\nB.3\nSample complexity\nFrom Eq. 76 it is clear that for the ‘signal’ ˆf, the fluctuations due to noise must be smaller than those due to the\nrandom choice of the composition rules. Therefore, the crossover takes place when the two nose terms have the same\nsize, occurring at P = Pc such that\nr v\nmL =\nrvnc\nPc\n⇒Pc = ncmL.\n(77)\nC\nOne-Step Gradient Descent (GD)\nWe will consider a simplified but tractable setting, where we generate an instance of the RHM and then train a\none-hidden-layer fully-connected network only on the first s-dimensional patch of the input. Since there are many data\nhaving the same first s-dimensional patch but a different label, this network does not have the capacity to fit the data.\nNevertheless, in the case where the s-dimensional patches are orthogonalized, neural networks can learn the synonymic\ninvariance of the RHM if trained on at least Pc data.\nGD on Cross-Entropy Loss.\nMore specifically, let us first sample an instance of the RHM, then P input-label\npairs (xk, αk) with αk := α(xk) for all k = 1, . . . , P. For any datum x, we denote with µ1(x) the s-tuple of features in\nthe first patch and with δµ the one-hot encoding of the s-tuple µ (with dimension vs). The fully-connected network\nacts on the one-hot encoding of the s-tuples with ReLU activations σ(x) = max (0, x),\nFNN(δµ) = 1\nH\nH\nX\nh=1\nahσ(wh · δµ),\n(78)\nwhere the inner-layer weights wh’s have the same dimension as δµ and the top-layer weights ah’s are nc-dimensional.\nThe top-layer weights are initialized as i.i.d. Gaussian with zero mean and unit variance and fixed. The wh’s are\ntrained by Gradient Descent (GD) on the cross-entropy loss,\nL = ˆEδ\n\n−\nnc\nX\nβ=1\nδβ,α(x) log\n \ne(FNN(δ))β\nPnc\nβ′=1 e(FNN(δ))β′\n!\n,\n(79)\nwhere δβ,α(x) stems from the one-hot encoding of the class label α(x) and ˆE denotes expectation over the training set.\nFor simplicity, we consider the mean-field limit H →∞, so that F(0)\nNN = 0 identically, and initialize all the inner-layer\nweights to 1 (the vector with all elements set to 1)8.\nUpdate of the Hidden Representation.\nIn this setting, with enough training points, one step of gradient descent\nis sufficient to build a representation invariant to the exchange of synonyms. Due to the one-hot encoding, (wh · δµ),\nnamely the h-th component of the hidden representation of the s-tuple µ, coincides with the µ-th component of the\nweight wh. This component, which is set to 1 at initialization, is updated by (minus) the corresponding component\nof the gradient of the loss in Eq. 79. Recalling that at initialization the predictor is 0 and all the components of the\ninner-layer weights are 1, we get\n−∇(wh)µL = 1\nP\nnc\nX\nα=1\nah,α\n\u0012\nˆN1(µ; α) −1\nnc\nˆN1(µ)\n\u0013\n,\n(80)\nwhere ˆN1(µ) is the empirical occurrence of the s-tuple µ in the first patch of the P training points and ˆN1(µ; α) is\nthe (empirical) joint occurrence of the s-tuple µ and the class label α. As P increases, the empirical occurrences ˆN\nconverge to the true occurrences N, which are invariant for the exchange of synonym s-tuples µ. Hence, the hidden\nrepresentation is also invariant for the exchange of synonym s-tuples in this limit.\n8These two assumptions can be relaxed by extending the tools developed in [22].\n21\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n164\nC.1\nExtension to a one-hidden-layer CNN\nThe same argument can be carried out by considering a one-hidden-layer CNN with weight sharing and global average\npooling:\nFCNN(x) =\n1\nHsL−1\nH\nX\nh=1\nsL−1\nX\nj=1\nahσ(wh · xj),\n(81)\nwhere we added an average over all input patches j. The gradient updates now read,\n−∇(wh)µL = 1\nP\nnc\nX\nα=1\nah,α\n1\nsL−1\nsL−1\nX\nj=1\n\u0012\nˆNj(µ; α) −1\nnc\nˆNj(µ)\n\u0013\n,\n(82)\nhence, synonymic invariance can now be inferred from the average occurrences over patches. This average results\nin a reduction of both the signal and noise term by the same factor\n√\nsL−1. Consequently, analogously to the case\nwithout weight sharing, the hidden representation becomes insensitive to the exchange of synonymic features for\nP ≫Pc = ncmL.\nD\nImproved Sample Complexity via Clustering\nIn Section 4.C of the main text and Appendix 4, we showed that the hidden representation of a one-hidden layer\nfully-connected network trained on the first patch of the RHM inputs becomes insensitive to exchanges of synonyms at\nP = P ∗= Pc = ncmL. Here we consider the maximal dataset case nc = v and m = vs−1, and show that a distance-based\nclustering method acting on these hidden representations would identify synonyms at P ≃√ncmL, much smaller than\nPc in the large-nc limit.\nLet us then imagine feeding the representations updates ∆fh(µ) of Eq. 82 to a clustering algorithm aimed at\nidentifying synonyms. This algorithm is based on the distance between the representations of different tuples of input\nfeatures µ and ν,\n∥∆f(µ) −∆f(ν)∥2 := 1\nH\nH\nX\nh=1\n(∆fh(µ) −∆fh(ν))2 ,\n(83)\nwhere H is the number of hidden neurons. By defining\nˆgα(µ) :=\nˆN1(µ; α)\nP\n−1\nnc\nˆN1(µ)\nP\n,\n(84)\nand denoting with ˆg(µ) the nc-dimensional sequence having the ˆgα’s as components, we have\n∥∆f(µ) −∆f(ν)∥2 =\nnc\nX\nα,β=1\n \n1\nH\nH\nX\nh\nah,αah,β\n!\n(ˆgα(µ) −ˆgα(ν)) (ˆgβ(µ) −ˆgβ(ν))\nH→∞\n−−−−→\nnc\nX\nα=1\n(ˆgα(µ) −ˆgα(ν))2 = ∥ˆg(µ) −ˆg(ν)∥2,\n(85)\nwhere we used the i.i.d. Gaussian initialization of the readout weights to replace the sum over neurons with δα,β.\nDue to the sampling noise, from Eq. 72 and Eq. 73, when 1 ≪P ≪Pmax,\nˆgα(µ) = gα(µ) +\nr\n1\nncmvP ηα(µ),\n(86)\nwhere ηα(µ) is a zero-mean and unit-variance Gaussian noise and g without hat denotes the P →Pmax limit of ˆg. In\nthe limit 1 ≪P ≪Pmax, the noises with different α and µ are independent of each other. Thus,\n∥ˆg(µ) −ˆg(ν)∥2 = ∥g(µ) −g(ν)∥2 +\n1\nncmvP ∥η(µ) −η(ν)∥2 +\n2\n√ncmvP (g(µ) −g(ν)) · (η(µ) −η(ν)) .\n(87)\n22\n165\nIf µ and ν are synonyms, then g(µ) = g(ν) and only the noise term contributes to the right-hand side of Eq. 87. If this\nnoise is sufficiently small, then the distance above can be used to cluster tuples into synonymic groups.\nBy the independence of the noises and the Central Limit Theorem, for nc ≫1,\n∥η(µ) −η(ν)∥2 ∼N(2nc, O(√nc)),\n(88)\nover independent samplings of the P training points. The g’s are also random variables over independent realizations of\nthe RHM with zero mean and variance proportional to the variance of the conditional probabilities f(α|µ) (see Eq. 62\nand Eq. 71),\nVar [gα(µ)] =\n1\nncmvncmL =\n1\nncmvPc\n.\n(89)\nTo estimate the size of ∥g(µ) −g(ν)∥2 we must take into account the correlations (over RHM realizations) between\ng’s with different class label and tuples. However, in the maximal dataset case nc = v and m = vs−1, both the sum\nover classes and the sum over tuples of input features of the joint occurrences N(µ; α) are fixed deterministically. The\nconstraints on the sums allow us to control the correlations between occurrences of the same tuple within different\nclasses and of different tuples within the same class, so that the size of the term ∥g(µ) −g(ν)∥2 for nc = v ≫1 can be\nestimated via the Central Limit Theorem:\n∥g(µ) −g(ν)∥2 ∼N\n\u0012\n2nc\nncmvPc\n, O(√nc)\nncmvPc\n\u0013\n.\n(90)\nThe mixed term (g(µ) −g(ν)) · (η(µ) −η(ν)) has zero average (both with respect to training set sampling and RHM\nrealizations) and can also be shown to lead to relative fluctuations of order O(√nc) in the maximal dataset case.\nTu sum up, we have that, for synonyms,\n∥ˆg(µ) −ˆg(ν)∥2 = ∥η(µ) −η(ν)∥2 ∼\n1\nmvP\n\u0012\n1 +\n1\n√nc\nξP\n\u0013\n,\n(91)\nwhere ξP is some O(1) noise dependent on the training set sampling. If µ and ν are not synonyms, instead,\n∥ˆg(µ) −ˆg(ν)∥2 ∼\n1\nmvP\n\u0012\n1 +\n1\n√nc\nξP\n\u0013\n+\n1\nmvPc\n\u0012\n1 +\n1\n√nc\nξRHM\n\u0013\n,\n(92)\nwhere ξRHM is some O(1) noise dependent on the RHM realization. In this setting, the signal is the deterministic\npart of the difference between representations of non-synonymic tuples. Due to the sum over class labels, the signal\nis scaled up by a factor nc, whereas the fluctuations (stemming from both sampling and model) are only increased\nby O\n\u0000√nc\n\u0001\n. Therefore, the signal required for clustering emerges from the sampling noise at P = Pc/√nc = √ncmL,\nequal to v1/2+L(s−1) in the maximal dataset case. This prediction is tested for s = 2 in Fig. 12, which shows the error\nachieved by a layerwise algorithm which alternates single GD steps to clustering of the resulting representations [22,27].\nMore specifically, the weights of the first hidden layer are updated with a single GD step while keeping all the other\nweights frozen. The resulting representations are then clustered, so as to identify groups of synonymic level-1 tuples.\nThe centroids of the ensuing clusters, which correspond to level-2 features, are orthogonalized and used as inputs of\nanother one-step GD protocol, which aims at identifying synonymic tuples of level-2 features. The procedure is iterated\nL times.\n23\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n166\n103\n104\n105\n106\nP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntest error, ¯ϵ = ϵ/ϵrnd\n100\n101\n102\nP/vL+1/2\nv = 6\nv = 8\nv = 10\nv = 12\nv = 16\nv = 20\nv = 24\n100\n101\nP/vL+1\nFigure 12: Sample complexity for layerwise training, m = nc = v, L = 3, s = 2. Training of a L-layers network is\nperformed layerwise by alternating one-step GD as described in Section 4.C and clustering of the hidden representations.\nClustering of the mv = v2 representations for the different one-hot-encoded input patches is performed with the\nk-means algorithms. Clustered representations are then orthogonalized and the result is given to the next one-step GD\nprocedure. Left: Test error vs. number of training points. Different colors correspond to different values of v. Center:\ncollapse of the test error curves when rescaling the x-axis by vL+1/2. Right: analogous, when rescaling the x-axis by\nvL+1. The curves show a better collapse when rescaling by vL+1/2, suggesting that these layerwise algorithms as an\nadvantage of a factor √v over end-to-end training with deep CNNs, for which P ∗= vL+1.\nE\nInstances of the Homogeneous Feature Model (HFM) in the Random\nHierarchy Model (RHM)\nGiven that the rules in the RHM are chosen uniformly at random, a non-zero probability exists that an HFM, where\nno input-output correlations exist, is sampled as an instance of the RHM. In these instances, semantic invariance, and\ngood generalization, would be impossible to learn from correlations, as illustrated in the main text. In this appendix\nwe show that such specific instances of the RHM are sampled with vanishing probability, for increasing values of the\nRHM parameters.\nE.1\nm = vs−1 case\nThe number of rules for the RHM made by L layers for generic values of m and v is given by\n\u0014\n(vs)!\n(m!)v(vs −vm)!\n\u0015L \u0012 1\nv!\n\u0013L−1\n.\n(93)\nFor m = vs−1 it becomes\n\u0000vs\nvs−1...vs−1\n\u0001L \u0000 1\nv!\n\u0001L−1.\nThe number of HFM rules, defined by having Ni(µ; α) = vs−2\nindependently of µ in each of the single-layer rules, can be computed as follows. Let’s look at a given feature of the\nprevious layer, for example the symbol 1. We want to assign to it m = vs−1 s−tuples\n(1, α1,1\n1 , ..., α1,s−1\n1\n), ..., (1, α1,1\nvs−2, ..., α1,s−1\nvs−2 ), ..., (v, α1,1\nvs−1−(vs−2−1), , ..., α1,s−1\nvs−1−(vs−2−1)), ..., (v, α1,1\nvs−1, , ..., α1,s−1\nvs−1 ),\n(94)\nwhere the first vs−2 tuples has as first symbol 1, the second vs−2 the symbol 2 and we continue up to the last vs−2\ntuples with first symbol v. The numbers {α1,t\ni }i=1,...,vs−1 are permutations of the set {1, ..., v}vs−2 for feature 1 and\nlocation t. In these rules, there is no symbol that occurs more than the others at a given location. Consequently, the\nnetwork cannot exploit any correlation between the presence of a symbol at a given location and the label to solve\nthe task. With regard to the other features j of the previous layer, to any of these we will assign the vs−1 tuples\n(1, αj,1\n1 , ..., αj,s−1\n1\n), ..., (v, αj,1\nvs−1, , ..., αj,s−1\nvs−1 ), with the numbers {αj,t\ni } being the same of α1,t\ni\nfor any t but shifted forward\nof (j −1)v positions. For example, the numbers related to the ”block” of tuples with first element 1 for feature 1\nwill be the same related to the ”block” of tuples with first element 2 for feature. In formulae: αj,t\ni\n= α1,t\ni−(j−1)v, with\ni −(j −1)v being equivalent to (vs−1 −(v −i) + 1) (periodic boundary conditions). The number of such uncorrelated\n24\n167\nrules is just the number (vs−1)! of permutations of the numbers {α1,t\ni }i=1,...,vs−1 for a fixed tuple location t, elevated to\nthe number of positions s −1. Consequently, the fraction of uncorrelated rules for L layers is:\nfHFM = 1\nv!\n \n(vs−1!)(s−1)\n\u0000vs\nvs−1...vs−1\n\u0001 1\nv!\n!L\n.\n(95)\nWe now want to show that fHFM vanishes for large v. We implement the Stirling approximation in (95) getting\nfHFM ≈1\nv!\n \nv(s−1)(vs−1+ 1\n2 )e−vs−1(s−1)\nvs(vs+ 1\n2 )e−vs\nvv(s−1)(vs−1+ 1\n2 )e−vs\n!L\n,\n(96)\nyielding the following limit behavior for large v:\nfHFM ≈1\nv!\n \ne−vs−1(s−1)\nvvs\n!L\n,\n(97)\nwhich is vanishing for large v and large L.\nE.2\nGeneric m case\nLet’s characterize the uncorrelated rules in the case of generic m. For each single-layer rule, we assign m s−tuples to\neach symbol of the previous layer. Let’s consider a given symbol j. To this symbol, we assign m s−tuples of the type\n(αj,1\n1 , ..., αj,s\n1 ), (αj,1\n2 , ..., αj,s\n2 ), ..., (αj,1\nm , ..., αj,s\nm ), with the m numbers (αj,t\ni )i at fixed location t being a permutation of a\nsubset of {1, ..., v}vs−2 such that, if we call mq the number of items in the subset equal to q ∈{1, ..., v}, each mq is\neither 0 or we have that mq1 = mq2 for q1 and q1 such that mq1 = mq2 > 0 and Pv\nq=1 mq = m. Moreover, since each\nsymbol q can appear at most vs−2 times, we have that mq ≤vs−2. We take the m numbers (αj,1\ni )i=1,...m at the first\nlocation t = 1 ordered in increasing order. Note that these tuples can be picked just once across different symbols j,\nimposing then constraints on the numbers αj,t\ni\nfor different features j.\nAs in the case of m = vs−1 in Sec. E.1, we want to show that the probability of occurrence fHFM of such uncorrelated\nrules, given by the number of these rules divided by the number of total rules, is vanishing for large v and/or large L.\nTo count the number of uncorrelated rules, that we call #HFM, we first count the number #j,t of possible series of\nnumbers {αf,t\ni }i=1,...,m for a fixed feature j and position t > 1, and for a single-layer rule. In other words, we have to\ncount the number of possible subsets made by m elements of {1, ..., v}vs−2 such that each symbol q ∈{1, ..., v} appears\nmq times, with the mq satisfying the constraints above. We introduce the quantity v0 which is the number of symbols\nq which appears 0 times in a given subset. Once we fix v0, from the constraint Pv\nq=1 mq = m we get that the features\nwith mq > 0 appear ¯m =\nm\nv−v0 if\nm\nv−v0 is a positive integer, otherwise, there are no subsets with that v0. Consequently,\nthe number #j,t is given by:\n#j,t =\nv−1\nX\nv0=0\n\u0012 v\nv0\n\u0013\nI\n\u0014\nm\nv −v0\n∈N>0\n\u0015\nI\n\u0014\nm\nv −v0\n≤vs−2\n\u0015\nm!,\n(98)\nwhere (i)\n\u0000 v\nv0\n\u0001\ncounts the number of choices of the features with 0 appearances and (ii) m! counts the number of\npermutations of the m numbers {αj,t\ni }i. Since we are interested in proving that fHFM is vanishing for large v and L,\nwe upper bound it relaxing the constraint that\nm\nv−v0 ∈N>0 and using that\n\u0000 v\nv0\n\u0001\n≤\n\u0000v\n⌈v/2⌉\n\u0001\n:\n#j,t ≤\n\u0010\nv −\nm\nvs−2\n\u0011 \u0012\nv\n⌈v/2⌉\n\u0013\nm!\n(99)\nConsidering all the s locations, we get\n#j ≤\n\u0010\nv −\nm\nvs−2\n\u0011s \u0012\nv\n⌈v/2⌉\n\u0013s\n(m!)s−1,\n(100)\n25\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n168\nwhere #j is defined similarly as #f,t. Notice that for the first location t = 1 there is not a factor m! since we are\nordering the numbers αj,1\ni\nin ascending order there.\nIf we want to count #HFM, we have to take into account that we are sampling without replacement from the space of\ns−tuples, and hence two different symbols j cannot share the same s−tuples, hence increasing the number of possible\nrules. To upper bound #HFM, we relax this constraint, hence sampling the tuples with replacement. Consequently, we\nhave:\n#HFM ≤\n\u0014\u0010\nv −\nm\nvs−2\n\u0011s \u0012\nv\n⌈v/2⌉\n\u0013s\n(m!)s−1\n\u0015v\n,\n(101)\nsince the choice of the αj,t\ni\nis independent between different features j. Consequently for a L−layer rule:\nfHFM ≤\n\u0014\u0010\nv −\nm\nvs−2\n\u0011s \u0012\nv\n⌈v/2⌉\n\u0013s\n(m!)s−1\n\u0015vL ,\"\u0012\nvs\nm . . . m\n\u0013L \u0012 1\nv!\n\u0013L−1#\n(102)\nWe now assume m ∼vα, with 0 < α < (s −1) and for large n we implement the Stirling approximation9, getting:\nfHFM ≤1\nv!\n\u0010\nv −vα−(s−2)\u0011svL\n2(v+1)vLvα(vα+1/2)(s−1)vL+vL/2e−vα(s−1)vL−vL\n.\nvL(vs+v/2−sv/2+s/2)e−\nL\n2vs−1 (v(vα−vs−1)+(vs−v1+α−vs−1))\n(103)\nwhere we approximated ⌈v/2⌉with v/2. At the leading order for large v we get, using the fact that (α + 1) < s\nfHFM ≤1\nv!\ne−(s−1)Lvα+1\nv(vs)L\n2(v+1)vL,\n(104)\nhence the probability of occurrence of a parity-like rule is vanishing for large n and L.\n0.5\n0.6\n0.7\n0.8\n0.9\nP/Pmax\n0.2\n0.4\n0.6\ntest error\nL = 2\nv = 2\nv = 3\nv = 4\nv = 2\nv = 3\nv = 4\n0.5\n0.6\n0.7\n0.8\n0.9\nP/Pmax\n0.2\n0.4\n0.6\nL = 3\nFigure 13: Test error of deep CNNs on the Homogeneous Feature Model for different v and L. Horizontal dashed lines\nstand for the test error (v −1)/v given by guessing labels uniformly at random. For v = 2 the networks can generalize\nto a number of training points P which scales with the total size of the dataset Pmax. Increasing v, performance is very\nclose to chance already at v = 4.\nF\nIntrinsic Dimensionality of Data Representations\nIn deep learning, the representation of data at each layer of a network can be thought of as lying on a manifold\nin the layer’s activation space. Measures of the intrinsic dimensionality of these manifolds can provide insights into\nhow the networks lower the dimensionality of the problem layer by layer [14,15]. However, such measurements have\nchallenges. One key challenge is that it assumes that real data exist on a smooth manifold, while in practice, the\n9The Stirling approximation for the multinomial\n\u0000n\na1...ak\n\u0001 for n →∞and integers ai such that Pk\ni=1 = n is given by\n\u0000n\na1...ak\n\u0001\n∼\n(2πn)(1/2−k/2)kn+k/2exp{−k\n2n\nPk\ni=1(ai −n/k)2}. In our case n = vs, k = (v + 1) and ai = m for i ∈1, ..., v and ak+1 = (vs −vm).\n26\n169\ndimensionality is estimated based on a discrete set of points. This leads to counter-intuitive results such as an increase\nin the intrinsic dimensionality with depth, especially near the input. An effect that is impossible for continuous smooth\nmanifolds. We resort to an example to illustrate how this increase with depth can result from spurious effects. Consider\na manifold of a given intrinsic dimension that undergoes a transformation where one of the coordinates is multiplied by\na large factor. This operation would result in an elongated manifold that appears one-dimensional. The measured\nintrinsic dimensionality would consequently be one, despite the higher dimensionality of the manifold. In the context of\nneural networks, a network that operates on such an elongated manifold could effectively ’reduce’ this extra, spurious\ndimension. This could result in an increase in the observed intrinsic dimensionality as a function of network depth,\neven though the actual dimensionality of the manifold did not change.\nIn the specific case of our data, the intrinsic dimensionality of the internal representations of deep CNNs monotonically\ndecreases with depth, see Fig. 14, consistently with the idea proposed in the main text that the CNNs solve the problem\nby reducing the effective dimensionality of data layer by layer. We attribute this monotonicity to the absence of\nspurious or noisy directions that might lead to the counter-intuitive effect described above.\n101\n102\n103\ndataset size, P\n10−1\n100\nnear neigh. distance, δ/δ0\ninput, deff = 7.06\nlayer 1, deff = 6.89\nlayer 2, deff = 3.86\nlayer 3, deff = 3.66\noutput, deff = 2.25\ninput\nlayer 1\nlayer 2\nlayer 3\noutput\n3\n4\n5\n6\n7\neﬀective dimension, deff\nFigure 14: Effective dimension of the internal representation of a CNN trained on one instance of the RHM with\nm = nc = v, L = 3 resulting in Pmax = 6′232. Left: average nearest neighbor distance of input or network activations\nwhen probing them with a dataset of size P. The value reported on the y-axis is normalized by δ0 = δ(P = 10). The\nslope of δ(P) is used as an estimate of the effective dimension. Right: effective dimension as a function of depth. We\nobserve a monotonic decrease, consistent with the idea that the dimensionality of the problem is reduced by DNNs\nwith depth.\nG\nAdditional Figures\n103\n104\n105\nP\n10−3\n10−1\ntest error, ¯ϵ = ϵ/ϵrnd\nnc = 2\nnc = 4\nnc = 8\nnc = 16\nnc = 24\n100\n101\nP/(ncmL)\n10−3\n10−1\n103\n104\nncmL\n104\n105\nP ∗s.t. ¯ϵ = 10%\nv = 2\nv = 4\nv = 8\nv = 16\nv = 24\ny = 3x\nFigure 15: Sample complexity for deep CNNs, m = v = 24, L = s = 2 and different values of nc. Left: Test\nerror vs. number of training points. Different colors correspond to different numbers of classes nc. Center: collapse of\nthe test error curves when rescaling the x-axis by ncmL. Right: sample complexity P ∗corresponding to a test error\nϵ∗= 0.1.\n27\nChapter 6. A Toy Model for the Hierarchical Compositionality of Real Data\n170\n102\n103\n104\ntraining set size, P\n0.2\n0.4\n0.6\n0.8\ntest error\nFigure 16: Test error vs. number of training points for a ResNet18 [44] trained on subsamples of the CIFAR10 dataset.\nResults are the average of 10 jointly different initializations of the networks and dataset sampling.\n28\n171\nPart IV\nConclusions and Perspectives\n173\n7 Conclusion\nThe central takeaway from this thesis is that the power of deep learning indeed resides in its\nability to adapt to the structure of the data. In particular, we put forward the hypothesis that\nthe curse of dimensionality–—a major challenge in processing high-dimensional data—–can\nbe substantially mitigated by learning representations that are invariant to aspects of the data\nthat are irrelevant for the task, extending the ideas of Bruna and Mallat (2013); Mallat (2016) to\nother invariances beyond smooth deformations and image tasks. Our findings demonstrate\nthat neural networks are able to learn such representations, provided they have the right\narchitecture and are trained in the feature learning regime. Crucially, we have shown that this\nlearning of invariances is fundamental for achieving good performance.\nThe first section (7.1) of this concluding chapter presents a summarizing table that encapsu-\nlates our primary results, systematically categorizing the invariances, neural network architec-\ntures, and training regimes that were examined. One of the central aspects of our research is\nthe introduction of empirical tools to characterize invariance learning in neural networks in\norder to connect it to performance. The second section (7.2) of this conclusion elaborates on\nthese tools, providing a unified formulation for all the invariances we studied, and placing\nthese tools in context with existing literature. In section 7.3 we discuss more in detail the\nresults encapsulated in the table and the key lessons drawn from them; section 7.4 provides\nthe big picture that emerges from our work. We close with the limitations of our work and the\nongoing and future investigations that our findings open up to (section 7.5).\n7.1\nTable of Results\nWe provide a compact summary of our results in Table 7.1. On the rows, one finds the\ninvariances we considered:\n1. Linear Invariance. This invariance could model the pixels at the corner of an image,\n175\nChapter 7. Conclusion\n1. Linear Inv.\n2. Rotation Inv.\n4. Deformation Inv.\n(scale detection)\n3. Deformation Inv.\n(image classification)\n5. Hierarchy\n(synonymic inv.)\nshallow FCN\n(lazy)\nshallow FCN\n(feature)\ndeep FCN\n(feature)\ndeep CNN\n(feature)\nArch. + regime\nData Structure\nAlgo: GF\nAlgo: GF\nAlgo: GD\nAlgo: GD\nAlgo: SVM\nAlgo: GF\nAlgo: GD\nAlgo: SGD\nAlgo: SVM\nAlgo: SGD\nAlgo: SGD\nAlgo: SGD\nAlgo: SGD\n(preliminary results)\nTable 7.1: Summary of Results. The rows represent the various data structures considered,\ncategorized by data invariances, while the columns detail the types of neural networks and\ntraining regimes studied, including 2-layer neural networks in both the lazy and feature\nregimes, deep fully-connected networks, and deep convolutional networks. Gray cells indicate\nconfigurations that were not expressly addressed in our research. For each invariance, symbols\nprovide a ranking of the performance of the architecture and training regime we investigated\n(green: best, orange: intermediate, red: worst).\nThe performance is more precisely reported by the predicted scaling exponent β, when the\ntest error follows a power law, or by the sample complexity P∗. When predictions of test error\nare not available (i.e. for deformation invariance), we indicate the qualitative value of relative\nsensitivity R f , which correlates closely with test error.\nThe algorithms used in each setting are also indicated, namely Support Vector Machine SVM,\n(Stochastic) Gradient Descent (S)GD, and Gradient Flow GF, that is GD with adaptive learning\nrate as defined in Geiger et al. (2020b).\n176\n7.1 Table of Results\nlikely uncorrelated to the task. It generically includes target functions of the form:\nf ∗(x) = g(Ax)\nwhere\nA : Rd →Rd′\nand\nd′ ≪d.\n(7.1)\n2. Rotation Invariance. This invariance is introduced as a simple model of non-linear\ninvariance. In this setting, data points xi are sampled uniformly at random from the\nd-dimensional unit sphere Sd−1. The target f ∗is a Gaussian random function of\ncontrolled smoothness:\nE∥f ∗(x)−f ∗(z)∥2 = O\n¡\n∥x −z∥2νt ¢\n,\nas\nx →z,\n(7.2)\nwhere the exponent νt controls the smoothness of f ∗and hence its stability with respect\nto rotations of the input. For νt →∞, f ∗is the constant function that is invariant to\nrotations. The results reported in the second row of Table 7.1 are valid for large νt (cf.\nchapter 4, Equation 3.4).\n3. Deformation Invariance in Images. Given τ an operator that applies a small deforma-\ntion to 2D images, we have that the target function defining the class of an image x\nsatisfies\nf ∗(x) ≈f ∗(τx),\n(7.3)\nin the sense that ∥f ∗(x)−f ∗(τx)∥is small, if the norm of τ is small.\n4. Scale-detection tasks. We’ve designed artificial tasks as a model for scenarios where\ndeformation invariance plays a role to better characterize it. These include models of\n2D images where only two pixels i and j are active, and the target is given by\nf ∗(x) = sign(∥i −j∥−ξ).\n(7.4)\nThe task hence consists of classifying whether the two pixels are closer or not than\na given distance or scale ξ and moving one active pixel to a neighboring position is\nunlikely to affect the class.\n5. Synonymic Invariance of Hierarchical Tasks. This invariance is related to hierarchically\ncompositional and local tasks. Consider, for instance:\nf ∗(x) = g3(g2(g1(x1,x2),g1(x3,x4)),g2(g1(x5,x6),g1(x7,x8))),\n(7.5)\nwith x = (x1,...,x8)—see also graphical representation in Figure 7.1. Here, we have\na function f ∗of a d-dimensional input (d = 8), written as a hierarchy of L = 3 levels\nof constituent functions. Each of the constituent functions is local in the sense that\nit only depends on a number of variables s ≪d, with s = 2 in this case. Synonymic\ninvariance stems from the fact that, for each constituent function, different tuples e.g.\n(x1,x2),(x′\n1,x′\n2) with identical meaning may exist, i.e. g11(x1,x2) = g11(x′\n1,x′\n2). We call\nthese tuples synonyms.\n177\nChapter 7. Conclusion\ng3\ng2\ng1\nx1\nx2\ng1\nx3\nx4\ng2\ng1\nx5\nx6\ng1\nx7\nx8\nFigure 7.1: Graphical Representation of the Hierarchically Compositional and Local Func-\ntion of Equation 7.5. The tree represents the structure of the hierarchical function with L = 3\nlayers, whose constituent functions (g’s) are local on patches of size s = 2. The leaves corre-\nspond to input variables.\nIn the model of the hierarchical data we introduced in chapter 6, each input takes values\nfrom a finite vocabulary V , with v = |V |, and the constituents gl : V s →V are surjective\nfunctions that map sets of m inputs of the domain to each one of the v outputs. The\nfunctions gl are chosen by drawing the sets of m synonyms uniformly at random from\nall the possible s-tuples of inputs.\nNotice that the results reported in the fifth row of Table 7.1, are valid when the input\ndistribution has full support, in the sense that all possible vd input data are generated—\nthis is the case when m is large. If only a fraction of input data was generated (m = O(1)),\nthen even kernel methods could beat the curse of dimensionality, as they are known to\nbe adaptive to a low-dimensional input support Bach (2022).\n7.2\nEmpirical Characterization of Invariants Learning\nObservables to measure the invariance of neural networks to input transformations that\nleave the label unchanged are crucial for the findings of this thesis, as they are needed to\nquantitatively link invariance learning and performance.\nOur main contribution in this context consists in proposing to characterize the learning of\ninvariances through the relative measure of the sensitivity of the network activations with\nrespect to input transformations that the target function is invariant to. Relative sensitivities\ngenerally take the form,\nS f = Ex,T ∥f (x)−f (T x)∥2\nEx,G∥f (x)−f (Gx)∥2 ,\n(7.6)\nwhere f is the neural network output or internal activations, the x’s are test samples, T is an\noperator applying the invariant transformation to the inputs, and G is an operator applying a\ngeneric transformation of the same magnitude, i.e. Ex,T ∥T x −x∥= Ex,G∥Gx −x∥.\nThis definition is such that S f ≈1 if the function f does not distinguish between the invariant\ntransformation T and a generic one G, otherwise S f ≪1. Such measures of sensitivity have\n178\n7.2 Empirical Characterization of Invariants Learning\n101\n102\n103\ntrainset size, n\n10−10\n10−8\n10−6\n10−4\nrotation stability\nd = 2\nFeature\nLazy\n101\n102\n103\ntrainset size, n\n10−9\n10−7\n10−5\nd = 3\nFeature\nLazy\n101\n102\n103\ntrainset size, n\n10−9\n10−7\n10−5\nd = 5\nFeature\nLazy\nFigure 7.2: Rotation sensitivity of the predictor when learning the constant function on\nthe sphere (cf. chapter 4), in the feature and lazy regime, for a varying number of training\npoints. Results for different input-space dimensions are reported in each column. To see the\ncorrelation between sensitivity and performance, these curves are to be compared with Figure\n4 in chapter 4.\nproven to be successful to characterize the learning of invariance in different contexts, and are\nshown to have a remarkable correlation with performance.\nOur sensitivity measurements have drawn inspiration from the definition of deformation\nstability Bruna and Mallat (2013); Mallat (2016). Yet, as found in chapter 3, measures of relative\nstability more effectively correlate with performance in practical settings. Furthermore, we\nadapt these observables to encompass other invariances beyond just diffeomorphisms. While\nGoodfellow et al. (2009) has also proposed methods to assess the invariance of deep networks\nto various input changes, their approach is based on the assumption that individual neurons\nare specialized for specific invariances. This assumption is challenged by the empirical\nevidence in Szegedy et al. (2014), which indicates that the entire activation space, rather than\nindividual neurons/coordinates, carries semantic information about the task.\nWe summarize here the invariant and generic transformations specific to each invariance and\ntask we considered in this thesis, and our main results on relative sensitivity measurements.\n1. For linear invariance, T corresponds to Gaussian noise added to the d −d′ irrelevant\ndirections, and G to Gaussian noise added to all directions. We have shown that this\ninvariance can be learned by 2-layers neural networks in the feature regime both in\nanisotropic targets (chapter 3, Figure 7) and image classification tasks (chapter 4, Figure\nH.1), where linear invariance is associated with pixels at the boundary of images.\n2. For rotation invariance, T corresponds to a small rotation of x over the input coordi-\nnates and G to Gaussian noise. We report results for the sensitivity of feature and lazy\npredictor when regressing the constant function on the d-sphere in Figure 7.2. The\nseparation in sensitivity to rotations between the two regimes is consistent with the one\nof test error as reported in chapter 4.\n4./5. For deformation invariance the transformation T correspond to a small deformation.\nTo generate deformations of controlled norm we introduced an ensemble of maximum-\n179\nChapter 7. Conclusion\noriginal\ndeformed\nlarger deformation norm\nFigure 7.3: Image of a swan deformed with the ensemble of max-entropy diffeomorphisms\nwe introduced in chapter 3. Starting from the original image on the left, deformations of\nincreasingly larger magnitudes are applied while going to the right.\nentropy diffeomorphisms that allows us to uniformly sample from the distribution of all\ndiffeomorphisms having the same norm. Samples from this distribution as a function\nthe deformation norm are reported in Figure 7.3. In this case, G corresponds to isotropic\nadditive noise. This invariance can be profitably learned by deep CNNs (chapter 3\nand chapter 5), but even deteriorates when learning features in shallow architectures\n(chapter 4).\n6. For synonymic invariance, T corresponds to the exchange of synonymous features at a\ngiven level of the hierarchy, G to the substitution of a feature at the same level with a\ndifferent one taken at random. This invariance can be learned by deep but not shallow\nnetworks (chapter 6 and Figure 7.6).\nOverall, our findings suggest that, if a certain invariance is relevant for the task, then S f of a\ngiven network predictor correlates with its test error. In deep networks, we observe that these\ninvariances are gradually learned, layer by layer, an observation in line with previous findings\nwithin the information bottleneck framework reviewed in subsection 1.2.1. As for the measures\nof intrinsic dimension that we discussed in subsection 1.2.1, our results are consistent with the\ndecrease in intrinsic dimension in later layers, but hint that the increase in early layers might\nactually be a side effect due to the inherent difficulties of intrinsic dimension estimation, as\ndescribed in subsection 1.2.1. This is because such an increase is not observed for any of\nthe invariances considered, and measuring the intrinsic dimension of representation in deep\nCNNs trained on the Random Hierarchy Model does not reveal this effect either (cf. chapter 6).\nIn summary, our sensitivity measures advance existing tools for assessing dimensionality\nreduction and allow establishing quantitative relationships to performance.\n180\n7.3 Lessons Learned\n7.3\nLessons Learned\nTo understand which elements in deep neural networks are responsible for learning which\nkind of invariances, we broke down such architectures and studied their different aspects\nseparately. Which benefits come from feature learning and/or architectural advancements\nlike depth and convolutional filters?\nShallow Neural Networks\nIn our study of shallow neural networks we established that (i)\nthey are able to learn linear invariances in the feature regime, (ii) they are bad for certain\nnon-linear invariances with a performance deterioration in the feature regime with respect to\nlazy and (iii) they are not able to learn hierarchical tasks in all regimes. In particular,\n(i) In the case of 2-layer neural networks trained on classification tasks that exhibit linear\ninvariance (as in Equation 7.1), we established that in the feature learning regime, fully-\nconnected neural networks can adapt effectively to data structure by orienting their\nweights and develop an invariance to irrelevant input directions. However, we show\nthis is not the case when the networks are trained in the lazy regime, which leads to a\nperformance difference that we quantified via scaling exponents of generalization error\nversus the number of training points, and have shown these exponents to be practically\ntight—–a finding scarce in existing literature. We highlight that here the target is still\ngiven by the sign of a smooth function, and for this reason kernels are not cursed by\ndimensionality. Indeed, bounds to the scaling exponent β for a problem of this kind\nwould predict β ≥1/4 Bartlett and Mendelson (2001). In our setting, such bound is not\ntight as, for large d, we find and βlazy = 1/3, as also highlighted in Paccolat et al. (2021b).\nFinally, to draw qualitative parallels between the learning of linear invariances by neural\nnetworks and that of more complex invariances of deep CNNs on real-world tasks, we\nstudied the neural tangent kernel and the one computed from the last layer activations,\nafter training. We found these kernels to retain the properties of the neural network,\ndisplaying only a few non-negligible eigenvalues, whose corresponding eigenvectors\nhave a large projection on the target function. This finding showed that dimensionality\nreduction takes place in the feature regime both in the presence of linear invariance for\nshallow FCNs, and more complex invariances for deep CNNs. Determining the exact\nnature of these complex invariances remained an open question at this stage. The low-\ndimensionality of the space of the final layer activations has been further characterized\nin the context of the neural collapse phenomenon Papyan et al. (2020). Moreover, more\nrecent works Guth et al. (2023) report that low-dimensional structures can be found not\nonly at the last, but at all layers of deep networks.\n(ii) In which cases learning features in shallow networks is instead a disadvantage? Building\non the results of Geiger et al. (2020b); Lee et al. (2020), we empirically characterized\nthe performance vs. number of training points of shallow networks trained on image\nclassification tasks and showed a systematic gap between feature and lazy regime,\n181\nChapter 7. Conclusion\nemphasizing the presence of drawbacks of learning features in shallow fully-connected\nnetworks.\nWe proposed to rationalize these drawbacks by arguing that shallow FCNs, when trained\nin the feature regime, tend to overfit irrelevant input directions in the absence of linear\ninvariances. We quantitatively characterized this overfitting in the case of rotationally\ninvariant tasks as defined in Equation 7.2. In this setting, we computed tight gener-\nalization error rates with the number of training points for 2-layer FCNs and show a\nperformance gap in favor of the lazy regime. Notice that, consistently with our discus-\nsion on generalization in kernel methods of subsection 1.1.3, the curse of dimensionality\ncan only be beaten if the target smoothness νt is proportional to the input dimension (cf.\nchapter 4, Equation 3.4). If that is the case, for d ≫1 we find βfeature = 1 and βlazy = 2,\nhighlighting that the optimal generalization bound for kernel methods is not tight in\nthis setting, as it would predict βlazy = 1/2.\nIn practical applications, these results suggest that when the selected architecture is not\nideally suited to the task, gradient descent could potentially lead to overfitting solutions.\nIn such cases, resorting to a kernel method might be a more prudent choice.\n(iii) In the case of hierarchically compositional tasks exhibiting synonymic invariance, we\nshowed that 2-layers networks are not able to learn the invariance and hence are cursed\nby dimensionality, with a sample complexity scaling exponentially with the input di-\nmension.\nThese results underscore a limitation of studying the benefits of feature learning by consid-\nering 2-layer neural networks. Indeed, in real-world settings, 2-layer fully-connected neural\nnetworks do not perform well, especially for more complex datasets (Figure 7.4). Surprisingly,\nthe lazy regime often yields better performance in these settings. These observations suggest\nthat linear invariance might not be the most relevant aspect of data structure that enables\nthe learnability of real data. Thus, while building a robust understanding of 2-layer networks\nis a necessary first step to making further progress, the study of more complex architectures\nappears necessary to explain the success of deep learning. In particular, what is the role of\ndepth? What is the role of local filters implemented through convolutional layers?\nThe Role of Depth and Locality\nDeep learning has demonstrated an ability to capture a\nhierarchy of increasingly abstract representations with depth Zeiler and Fergus (2014); Yosinski\net al. (2015); Olah et al. (2017). Our work demonstrated that depth is indispensable for learning\nhierarchical tasks in the feature learning regime. In particular, we have shown that (i) for\na toy model of hierarchically compositional tasks, deep networks are able to perform well\nby learning the associated synonymic invariance and (ii) for real-world tasks, specifically\nimage classification, deeper networks generally perform better as they are able to better learn\ndeformation invariance. Alongside depth, the utilization of local filters is crucial for image\nclassification tasks. However, the advantage of using local filters for tasks that are purely\n182\n7.3 Lessons Learned\n102\n103\n104\ntraining set size\n10−2\n10−1\ntest error\nMNIST\nFCN\nLeNet\nResNet18\n102\n103\n104\ntraining set size\n10−1\nFashionMNIST\nFCN\nLeNet\nResNet18\n102\n103\n104\ntraining set size\n2 × 10−1\n3 × 10−1\n4 × 10−1\n6 × 10−1\nCIFAR10\nFCN\nLeNet\nResNet18\nFigure 7.4: Test error rates of Fully-Connected Networks (FCN), LeNet LeCun et al. (1989),\nand ResNet18 He et al. (2016) with increasing number of training points, for different image\ndatasets. As depicted, a widening performance gap emerges between FCN and the two\nconvolutional neural networks (LeNet and ResNet18) with the increase in dataset complexity\n(left: black and white digits, center: black and white clothes, right: colored objects).\nhierarchical, such as the ones we introduced, remains a subject for further exploration and is\ndiscussed in section 7.5 regarding open questions.\nMore specifically,\n(i) We developed the Random Hierarchy Model to mimic the hierarchical structure of\nreal data and study synonymic invariance. We find that deep, but not shallow, neural\nnetworks are able to profitably learn this invariance, and beat the curse of dimensionality.\nInterestingly, we have shown that the possibility to detect input-output correlations\nis crucial for deep neural networks trained by gradient descent to learn, as previously\nnoted in related but different settings Arora et al. (2014); Malach and Shalev-Shwartz\n(2018, 2020). While our quantitative characterization of the sample complexity in this\nsetting offers an estimate of the data required to learn a task based on its hierarchical\nstructure, how to fit the parameters of the model (m,v,s,L) to real data remains an open\nquestion.\n(ii) In real data, these features, other than being hierarchically composed, also need to be\nidentified in space. In particular, their exact position may not matter, in the sense that\nthey can be slightly displaced without changing the label. This property gives rise to\ndeformation invariance. For image classification tasks we found that neural networks\nthat are more invariant to input deformations are also the ones that perform best, as\nhypothesized in Bruna and Mallat (2013); Mallat (2016). Notably, neural networks em-\nploying local filters are better than fully connected ones in learning the invariance, and\ndeeper CNNs are better than shallow ones. By examining the mechanisms responsi-\nble for granting deformation invariance, we found that deep CNNs often achieve it by\nutilizing low-frequency filters.\nComplementary to our findings, other works have explored the benefits of depth and locality\n183\nChapter 7. Conclusion\nwhen features are not learned, i.e. in the lazy regime, and regression tasks Bietti and Bach\n(2021); Favero et al. (2021); Bietti (2021); Cagnetta et al. (2022); Xiao (2022); Misiakiewicz and\nMei (2022). In particular, for fully connected networks, the NTK of a deep network has no\nadvantages over its shallow counterpart Bietti and Bach (2021). Instead, when filters are local,\nas for a deep CNN, then the NTK can learn targets that are local on patches of different sizes,\nwithout suffering from the curse of dimensionality Cagnetta et al. (2022).\nFor classification tasks, preliminary results obtained from the Random Hierarchy Model (not\nshown) suggest that the NTK of a deep CNN cannot learn tasks that are hierarchical if the target\nfunction has full support, exhibiting a sample complexity exponential in d. These preliminary\nfindings further emphasize the fundamental importance of learning features for the success\nof deep neural networks.\n7.4\nThe Big Picture\nFigure 7.5: The task of recognizing an object in a visual scene. In both drawings we are able\nto recognize a dog even if some features of the dog come in different positions and shapes (e.g.\nthe legs), or in different realizations (e.g. the ears). In this thesis, we argue that the invariance\nof the task to such changes is crucial for neural networks to learn it, as it allows for reducing the\ndimensionality of the problem and breaking the curse of dimensionality. Illustration courtesy\nof A.P.\nThe task of recognizing objects within a visual scene, such as identifying a dog as depicted in\nFigure 7.5, involves several steps.\nFirstly, (i) we must discern which parts of the scene are irrelevant to our task, such as pixels in\nthe upper corners. Secondly, (ii) it becomes apparent that solving our task does not require\nan exact match to a specific, idealized image of a dog. Instead, the features that constitute\nthe dog can manifest in a multitude of positions, and the precise relative location of these\nfeatures is not vital to our task. Lastly, (iii) upon recognizing that the exact placement of these\n184\n7.5 Some Open Questions\nfeatures is unimportant, we begin to compose them to form a coherent picture of the object.\nWe discern edges and textures, realizing that they form components of the dog such as legs,\neyes, and mouth. Gradually, these elements combine, allowing us to recognize the dog as a\nwhole. It’s noteworthy that these elements can appear, not only in various positions or shapes,\nbut also in various synonymous realizations, that are not just deformations or rearrangings\nof the constituent features in space; for example, the ears might be pictured from their back\nand appear droopy and brown when relaxed Figure 7.5(left), or show their pink interior when\nperked up if the dog is alert (right).\nThis thesis posits that these three aspects of real data structure are relevant in solving practical\ntasks like image classification, with particular emphasis on the significance of (ii) and (iii). If\nthis is the case, then unraveling how neural networks tackle these properties is fundamental\nto understanding their remarkable success. Specifically, we argue that these properties can\nbe more precisely defined in terms of data invariances: linear invariance for (i), deformation\ninvariance for (ii), and synonymic invariance for (iii). We propose that in order to effectively\nleverage these invariances, neural networks must learn them from data since they are not\ntypically built into the network’s architecture from the start. As such, feature learning and\nadaptation are essential for the optimal performance of neural networks. Yet, we also demon-\nstrate that the architectural selection of the network is equally important for proper feature\nlearning. A suboptimal architectural choice could result in learning the wrong features, leading\nto poor performance. Thus, implementing proven architectural advancements, such as the\nuse of local filters through convolutional layers and the incorporation of multiple layers, plays\na critical role in successful feature learning.\n7.5\nSome Open Questions\nThis final section outlines the limitations of our study and suggests potential avenues for\nfuture exploration. We first discuss how to unify the two main lines of inquiry in this thesis:\nthe roles of deformation and synonymic invariance both (i) in the Random Hierarchy Model\nand (ii) in real-world tasks. Then (iii) we show novel results aimed at disentangling the role of\ndepth and local filters in learning hierarchical tasks. (iv) We propose extensions to the Random\nHierarchy Model to get the power-law behavior of test error that is often observed in practice.\nWe conclude (v) by discussing practical applications of our ensemble of diffeomorphisms,\nspecifically for data augmentation and confidence estimation.\n(i) Our study on deformation invariance currently does not provide a theoretical framework\nfor understanding the observed correlation with test error. Such an understanding\nwould allow for making quantitative predictions on how many training points would be\nneeded to learn the invariance, hence the task. While some progress in this direction\nhas been made in the study of kernel methods Bietti et al. (2021), the sample complexity\nof algorithms that are allowed to learn features is still unknown.\nOne way to make progress consists in unifying the two main lines of inquiry of this\n185\nChapter 7. Conclusion\nthesis by creating a data model that can be invariant to both deformations and to the\nexchange of synonymous features. This can be done by adding a notion of sparsity in\nspace to the Random Hierarchy Model (RHM) in such a way that, at every hierarchy level,\nthe relevant features can equivalently appear at different locations of a given patch,\nwith the rest of the patch being empty or noisy, irrelevant pixels. This construction is\nsuch that exchanging the position of a relevant feature with an irrelevant pixel within\na patch does not change the task, hence modeling invariance to small deformations.\nSome preliminary results in this direction (to appear in Tomasini et al. 2023) show\nthat synonymic and deformation invariance are learned together in deep CNNs. From\nthese results, we can derive the following interpretation. The RHM can be efficiently\nlearned only by understanding the structure of the problem at all levels, which involves\nachieving synonymic invariance: a model that did not learn synonymic invariance could\nonly memorize the input samples, and would need to see all of them to solve the task.\nHence a correlation between synonymic invariance and performance comes naturally\nin this setting. The preliminary findings we report here—specifically the correlation\nbetween deformation and synonymic invariance—suggest that deformation invariance\nis indicative of the goodness of a network representation on various aspects, hence\nrationalizing its strong correlation to performance. Estimating the sample complexity\nas a function of both invariances is still an ongoing work.\n(ii) A related and complementary direction would include studying synonymic invariance\nin real-world tasks: when learning to predict e.g. age from face pictures, is the invariance\nto exchange of synonymous features (e.g. eyes with or without sunglasses) correlated to\nthe performance of a given neural network, as predicted from our findings in chapter 6?\nAre synonymic and deformation or 2D/3D rotation Goodfellow et al. (2009) invariance\nlearned together in real-world tasks as well? Moreover, the transformer architecture\nVaswani et al. (2017) has been the most recent technological breakthrough in the context\nof natural language processing. The study of synonymic invariance in real tasks may\ninclude understanding how transformers learn that the exchanges of two synonyms do\nnot change e.g. the sentiment of a sentence, and whether measures of the invariance of\na transformer to such exchanges are also correlated to its performance.\n(iii) In our study of hierarchical tasks we mainly focus on shallow FCNs and deep CNNs. One\nimportant question though is how locality is learned, when not imposed from the start,\nor \"What is the performance of deep FCNs?\". In the case of a target function just depend-\ning on one of the input patches, locality corresponds to the linear invariance studied in\nchapter 2. However, when the function depends on different patches independently, as\nin\nf ∗(x) = g3(g1(x1),g2(x2)),\n(7.7)\nwhere x1 and x2 are two input patches, the picture is different. In this case, the con-\nstituent functions g1 and g2 are invariant to all the input except x1 and x2, respectively,\nbut the target function is not. As a consequence, to solve the task, different neurons need\nto specialize to different parts of the input. Understanding when and how non-local\n186\n7.5 Some Open Questions\nneurons can achieve such specialization in practice to learn compositionally local func-\ntions is still an open question Neyshabur (2020); Pellegrini and Biroli (2021); Ingrosso\nand Goldt (2022).\nWe believe that the Random Hierarchy Model (RHM) provides a practical setting to start\naddressing this question. To this end, we conducted some preliminary experiments on\ndeep fully-connected networks trained on the RHM, see results in Figure 7.6. Neurons\nare indeed able to achieve specialization: they clearly separate into distinct groups, each\nresponding to one of the different input patches. Interestingly, (not shown) we observe\nthat this specialization occurs at all hierarchy levels, with neurons in successive layers\nspecializing to patches of patches, and so on. These experiments led to the following\ninsight: Deep FCNs can learn the Random Hierarchy Model with a sample complexity\nthat is orders of magnitude smaller than the one of shallow FCNs, corresponding to the\ntotal number of data points. These preliminary results suggest that depth is crucial for\nlearning tasks that are solely hierarchical, but imposing locality from the start is not.\nHowever, additional experiments with varying RHM parameters (L in particular) are\nneeded to validate this finding and characterize the scaling of the sample complexity of\ndeep FCNs. Our current hypothesis is that P∗\ndFCN = C(d)P∗\ndCNN, with C(d) a prefactor\npolynomial in d, that could be large for large d. Hence, even if both deep FCNs and\nCNNs beat the curse of dimensionality, deep CNNs may still have a relevant edge on\nperformance. Ultimately, while we observe neuron specialization in practice, a com-\nprehensive theoretical understanding of this phenomenon remains an open challenge.\n(iv) The Random Hierarchy Model serves as a toy model, and while it may provide useful\ninsights, it clearly does not perfectly reflect real-world data. Future investigations could\nprofitably extend to more sophisticated and realistic models. We already discussed how\nto include the sparsity of features in space in the model. One other possible extension\ndrawn from the context of natural languages is the incorporation of Zipf’s law Zipf\n(1935), an empirical principle in linguistics, stating that the frequency of a word in a\nnatural language is a power law of its rank in the frequency table.\nCurrently, in the Random Hierarchy Model, the features are uniformly distributed and\nhence all appear with the same probability. As a consequence, there exists a well-defined\nscale P∗= ncmL at which all correlations between input features and labels emerge\nfrom sampling noise. If, instead of being uniform, the features were distributed as\na power law, then correlations of rarer features would need an increasing number of\ntraining points to emerge from sampling noise. As a consequence, the sharp transitions\nwe observe in our setting around P∗for the test error versus number of training points\ncould manifest as power-law behaviors when Zipf’s law is incorporated into the model.\nIndeed, power-law behaviors have been observed in large language models Kaplan et al.\n(2020).\n(v) We conclude by discussing the practical application of the ensemble of diffeomorphisms\nwe introduced to data augmentation and confidence estimation.\n187\nChapter 7. Conclusion\nx1x2x3x4x5x6x7x8\nneuron 1\nneuron 2\nneuron 3\n. . .\nx1x2x3x4x5x6x7x8\n0\n1\n2\n3\n4\n5\n0\n10\n20\n30\n40\n50\n60\ninput norm over channels, ∥xi∥\n(a)\n(b)\nBefore training\nAfter training\n103\n104\n105\n106\ntraining set size, P\n0.0\n0.2\n0.4\n0.6\n0.8\ntest error, ϵ(P)\ndeep FCN\nshallow FCN\nFigure 7.6: Deep FCNs Learn Hierarchical Tasks. 3-hidden layers fully-connected network\ntrained on the Random Hierarchy Model with three hierarchy levels (L = 3) and patches of size\ns = 2, resulting in input dimension d = 8. The number of classes equals number of features\nand multiplicity nc = v = m = 8, resulting in a predicted sample complexity for deep CNNs of\nP∗\ndCNN = ncmL = 4′096 and a total data set size Pmax = ncmd−1 ≈108. (a) On the rows, there\nare different neurons, on the columns different input locations x1,...,xd. The color indicates\nthe weights norm across the v input channels. Left and right correspond to before and after\ntraining with P∗\ndCNN ≪P ≪Pmax. FCN learns locality in the sense that different neurons\nspecialize to different input patches (x1,x2), (x3,x4), etc. with training. A similar behavior can\nbe observed for the following layers focalizing on the next level of the hierarchy, i.e. patches\nof patches. (b) Test error vs. number of training points for Deep (blue) vs Shallow (orange)\nFCNs. Deep networks are able to reach zero test error with a number of training points P that\nsatisfies P∗\ndCNN < P ≪Pmax, while shallow do not.\n188\n7.5 Some Open Questions\nData augmentation consists in creating additional training samples by applying transfor-\nmations to existing data that do not change the label, thereby enhancing the networks’\nability to generalize or robustness to adversarial attacks Ortiz-Jiménez et al. (2021).\nAlthough we found that diffeomorphisms didn’t outperform standard data augmen-\ntation methods when trying to improve generalization, successful applications of our\nmaximum-entropy transformations in this domain of adversarial robustness have ap-\npeared in the literature Modas et al. (2022), suggesting this may be a promising direction\nfor further future work.\nConfidence estimation is the process of quantifying the certainty or confidence level\nof a model’s predictions Guo et al. (2017). The goal is to assess how likely the model’s\nprediction is to be correct, providing a measure of trustworthiness for the output that is\ncrucial in high-stakes applications like healthcare or finance. Our ensemble of diffeo-\nmorphisms can be employed to design better confidence estimators: by exploring the\nvalue of the predictor in the neighborhood of data points, other than on the data points\nalone, this could provide more accurate confidence estimations. Current work, that\nI’m co-supervising, involves this approach. Preliminary results indicate that averaging\nstandard confidence estimators over max-entropy diffeomorphisms of a specific input\nyields improved confidence estimators, that are more informative than standard ones,\nand better calibrated. These findings will appear in Hasler et al. (2023).\n189\nBibliography\nAbbe, E., Boix-Adsera, E., Brennan, M. S., Bresler, G., and Nagaraj, D. (2021). The staircase\nproperty: How hierarchical structure can guide deep learning. In Ranzato, M., Beygelzimer,\nA., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information\nProcessing Systems, volume 34, pages 26989–27002. Curran Associates, Inc.\nAbbe, E., Boix-Adsera, E., and Misiakiewicz, T. (2023). SGD learning on neural networks: leap\ncomplexity and saddle-to-saddle dynamics. arXiv:2302.11055 [cs, stat].\nAdvani, M. S., Saxe, A. M., and Sompolinsky, H. (2020). High-dimensional dynamics of\ngeneralization error in neural networks. Neural Networks.\nAllen-Zhu, Z., Li, Y., and Song, Z. (2019). A Convergence Theory for Deep Learning via Over-\nParameterization. In International Conference on Machine Learning, pages 242–252. PMLR.\nISSN: 2640-3498.\nAnsuini, A., Laio, A., Macke, J. H., and Zoccolan, D. (2019). Intrinsic dimension of data\nrepresentations in deep neural networks. In Advances in Neural Information Processing\nSystems, pages 6111–6122.\nAraújo, D., Oliveira, R. I., and Yukimura, D. (2019). A mean-field limit for certain deep neural\nnetworks. arXiv:1906.00193 [cond-mat, stat].\nArora, S., Bhaskara, A., Ge, R., and Ma, T. (2014). Provable Bounds for Learning Some Deep\nRepresentations. In Proceedings of the 31st International Conference on Machine Learning,\npages 584–592. PMLR. ISSN: 1938-7228.\nArora, S., Du, S., Hu, W., Li, Z., and Wang, R. (2019a). Fine-Grained Analysis of Optimization\nand Generalization for Overparameterized Two-Layer Neural Networks. In Proceedings\nof the 36th International Conference on Machine Learning, pages 322–332. PMLR. ISSN:\n2640-3498.\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. (2019b). On Exact Compu-\ntation with an Infinitely Wide Neural Net. In Wallach, H., Larochelle, H., Beygelzimer, A.,\nAlché-Buc, F. d., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing\nSystems 32, pages 8141–8150. Curran Associates, Inc.\n191\nBibliography\nAzulay, A. and Weiss, Y. (2018). Why do deep convolutional networks generalize so poorly to\nsmall image transformations? arXiv preprint arXiv:1805.12177.\nBach, F. (2017). Breaking the curse of dimensionality with convex neural networks. The Journal\nof Machine Learning Research, 18(1):629–681.\nBach, F. (2022). Learning Theory from First Principles. In preparation.\nBarron, A. (1993). Universal approximation bounds for superpositions of a sigmoidal func-\ntion. IEEE Transactions on Information Theory, 39(3):930–945. Conference Name: IEEE\nTransactions on Information Theory.\nBartlett, P. L. and Mendelson, S. (2001). Rademacher and Gaussian Complexities: Risk Bounds\nand Structural Results. In Helmbold, D. and Williamson, B., editors, Computational Learning\nTheory, Lecture Notes in Computer Science, pages 224–240, Berlin, Heidelberg. Springer.\nBelkin, M., Hsu, D., Ma, S., and Mandal, S. (2019). Reconciling modern machine-learning\npractice and the classical bias–variance trade-off. Proceedings of the National Academy of\nSciences, 116(32):15849–15854.\nBelkin, M., Hsu, D., and Xu, J. (2020). Two Models of Double Descent for Weak Features. SIAM\nJournal on Mathematics of Data Science, 2(4):1167–1180. Publisher: Society for Industrial\nand Applied Mathematics.\nBengio, Y., Courville, A., and Vincent, P. (2013). Representation Learning: A Review and New\nPerspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–\n1828. Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.\nBietti, A. (2021). Approximation and Learning with Deep Convolutional Models: a Kernel\nPerspective.\nBietti, A. and Bach, F. (2021). Deep Equals Shallow for ReLU Networks in Kernel Regimes.\narXiv:2009.14397 [cs, stat].\nBietti, A., Bruna, J., Sanford, C., and Song, M. J. (2022). Learning single-index models with\nshallow neural networks.\nBietti, A., Venturi, L., and Bruna, J. (2021). On the Sample Complexity of Learning under\nGeometric Stability. In Advances in Neural Information Processing Systems, volume 34,\npages 18673–18684. Curran Associates, Inc.\nBordelon, B., Canatar, A., and Pehlevan, C. (2020). Spectrum Dependent Learning Curves\nin Kernel Regression and Wide Neural Networks. In International Conference on Machine\nLearning, pages 1024–1034. PMLR. ISSN: 2640-3498.\nBruna, J. and Mallat, S. (2013). Invariant scattering convolution networks. IEEE transactions\non pattern analysis and machine intelligence, 35(8):1872–1886.\n192\nBibliography\nCagnetta, F., Favero, A., and Wyart, M. (2022). What can be learnt with wide convolutional\nneural networks? arXiv preprint arXiv:2208.01003.\nCanatar, A., Bordelon, B., and Pehlevan, C. (2021). Spectral bias and task-model alignment\nexplain generalization in kernel regression and infinitely wide neural networks. Nature\ncommunications, 12(1):2914.\nChizat, L. and Bach, F. (2018). On the Global Convergence of Gradient Descent for Over-\nparameterized Models using Optimal Transport. In Advances in Neural Information Process-\ning Systems 31, pages 3040–3050. Curran Associates, Inc.\nChizat, L. and Bach, F. (2020). Implicit Bias of Gradient Descent for Wide Two-layer Neural\nNetworks Trained with the Logistic Loss. In Conference on Learning Theory, pages 1305–1338.\nPMLR. ISSN: 2640-3498.\nChizat, L., Oyallon, E., and Bach, F. (2019). On lazy training in differentiable programming. In\nAdvances in Neural Information Processing Systems, pages 2937–2947.\nCui, H., Loureiro, B., Krzakala, F., and Zdeborová, L. (2022). Generalization error rates in\nkernel regression: the crossover from the noiseless to noisy regime*. Journal of Statistical\nMechanics: Theory and Experiment, 2022(11):114004. Publisher: IOP Publishing and SISSA.\nDamian, A., Lee, J., and Soltanolkotabi, M. (2022). Neural Networks can Learn Representations\nwith Gradient Descent. In Proceedings of Thirty Fifth Conference on Learning Theory, pages\n5413–5452. PMLR. ISSN: 2640-3498.\nd’Ascoli, S., Refinetti, M., Biroli, G., and Krzakala, F. (2020a). Double Trouble in Double Descent:\nBias and Variance(s) in the Lazy Regime. In Proceedings of the 37th International Conference\non Machine Learning, pages 2280–2290. PMLR. ISSN: 2640-3498.\nd’Ascoli, S., Sagun, L., and Biroli, G. (2020b). Triple descent and the two kinds of overfitting:\nwhere and why do they appear? In Advances in Neural Information Processing Systems,\nvolume 33, pages 3058–3069. Curran Associates, Inc.\nde Dios, J. and Bruna, J. (2020). On sparsity in overparametrised shallow relu networks. arXiv\npreprint arXiv:2006.10225.\nde G. Matthews, A. G., Hron, J., Rowland, M., Turner, R. E., and Ghahramani, Z. (2018). Gaussian\nprocess behaviour in wide deep neural networks. In International Conference on Learning\nRepresentations.\nDeng, J., Dong, W., Socher, R., Li, L., Kai Li, and Li Fei-Fei (2009). ImageNet: A large-scale\nhierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248–255. ISSN: 1063-6919.\nDeng, Z., Kammoun, A., and Thrampoulidis, C. (2020). A Model of Double Descent for High-\ndimensional Binary Linear Classification. arXiv:1911.05822 [cs, eess, stat].\n193\nBibliography\nDu, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019a). Gradient Descent Finds Global Minima\nof Deep Neural Networks. In Proceedings of the 36th International Conference on Machine\nLearning, pages 1675–1685. PMLR. ISSN: 2640-3498.\nDu, S. S., Zhai, X., Poczos, B., and Singh, A. (2019b). Gradient descent provably optimizes over-\nparameterized neural networks. In International Conference on Learning Representations.\nFacco, E., d’Errico, M., Rodriguez, A., and Laio, A. (2017). Estimating the intrinsic dimension of\ndatasets by a minimal neighborhood information. Scientific Reports, 7(1):12140. Number: 1\nPublisher: Nature Publishing Group.\nFavero, A., Cagnetta, F., and Wyart, M. (2021). Locality defeats the curse of dimensionality\nin convolutional teacher-student scenarios. Advances in Neural Information Processing\nSystems, 34:9456–9467.\nGazzaniga, M., Ivry, R. B., and Mangun, G. R. (2006). Cognitive Neuroscience: The Biology of the\nMind. W. W. Norton & Company, New York, 2nd edition edition.\nGeiger, M., Jacot, A., Spigler, S., Gabriel, F., Sagun, L., d’Ascoli, S., Biroli, G., Hongler, C., and\nWyart, M. (2020a). Scaling description of generalization with number of parameters in\ndeep learning. Journal of Statistical Mechanics: Theory and Experiment, 2020(2):023401.\nPublisher: IOP Publishing.\nGeiger, M., Petrini, L., and Wyart, M. (2021). Landscape and training regimes in deep learning.\nPhysics Reports, 924:1–18.\nGeiger, M., Spigler, S., Jacot, A., and Wyart, M. (2020b). Disentangling feature and lazy train-\ning in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment,\n2020(11):113301. Publisher: IOP Publishing.\nGerace, F., Loureiro, B., Krzakala, F., Mezard, M., and Zdeborova, L. (2020). Generalisation error\nin learning with random features and the hidden manifold model. In Proceedings of the 37th\nInternational Conference on Machine Learning, pages 3452–3462. PMLR. ISSN: 2640-3498.\nGhorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2019). Limitations of lazy training of\ntwo-layers neural network. In Advances in Neural Information Processing Systems, pages\n9111–9121.\nGhorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2020). When Do Neural Networks\nOutperform Kernel Methods? Advances in Neural Information Processing Systems, 33.\nGoodfellow, I., Lee, H., Le, Q., Saxe, A., and Ng, A. (2009). Measuring Invariances in Deep Net-\nworks. In Advances in Neural Information Processing Systems, volume 22. Curran Associates,\nInc.\nGranata, D. and Carnevale, V. (2016). Accurate Estimation of the Intrinsic Dimension Using\nGraph Distances: Unraveling the Geometric Complexity of Datasets. Scientific Reports,\n6(1):31377. Number: 1 Publisher: Nature Publishing Group.\n194\nBibliography\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On Calibration of Modern Neural\nNetworks. In Proceedings of the 34th International Conference on Machine Learning, pages\n1321–1330. PMLR. ISSN: 2640-3498.\nGuth, F., Ménard, B., Rochette, G., and Mallat, S. (2023). A Rainbow in Deep Network Black\nBoxes. arXiv:2305.18512 [cs, eess].\nGörtler, J., Kehlbeck, R., and Deussen, O. (2019). A Visual Exploration of Gaussian Processes.\nDistill, 4(4):e17.\nHasler, T., Favero, A., Petrini, L., and Wyart, M. (2023). Work in preparation.\nHastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2020). Surprises in High-Dimensional\nRidgeless Least Squares Interpolation. arXiv:1903.08560 [cs, math, stat].\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep Residual Learning for Image Recognition. In\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778.\nISSN: 1063-6919.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M., Ali,\nM., Yang, Y., and Zhou, Y. (2017). Deep learning scaling is predictable, empirically. arXiv\npreprint arXiv:1712.00409.\nHornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are\nuniversal approximators. Neural Networks, 2(5):359–366.\nIngrosso, A. and Goldt, S. (2022). Data-driven emergence of convolutional structure in neural\nnetworks. Proceedings of the National Academy of Sciences, 119(40):e2201854119.\nJacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and general-\nization in neural networks. In Proceedings of the 32Nd International Conference on Neural\nInformation Processing Systems, NIPS’18, pages 8580–8589, USA. Curran Associates Inc.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool,\nK., Bates, R., Žídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J.,\nCowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman,\nD., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein,\nS., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D. (2021).\nHighly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583–589.\nNumber: 7873 Publisher: Nature Publishing Group.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A.,\nWu, J., and Amodei, D. (2020). Scaling Laws for Neural Language Models. arXiv:2001.08361\n[cs, stat].\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep\nconvolutional neural networks. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger,\n195\nBibliography\nK. Q., editors, Advances in Neural Information Processing Systems 25, pages 1097–1105.\nCurran Associates, Inc.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel,\nL. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. Neural\nComputation, 1(4):541–551.\nLecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11):2278–2324. Conference Name:\nProceedings of the IEEE.\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief networks for\nscalable unsupervised learning of hierarchical representations. In Proceedings of the 26th\nAnnual International Conference on Machine Learning, ICML ’09, pages 609–616, New York,\nNY, USA. Association for Computing Machinery.\nLee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., and Sohl-Dickstein, J. (2018).\nDeep Neural Networks as Gaussian Processes. arXiv:1711.00165 [cs, stat].\nLee, J., Schoenholz, S. S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and Sohl-Dickstein,\nJ. (2020).\nFinite versus infinite neural networks: an empirical study.\narXiv preprint\narXiv:2007.15801.\nLee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. (2019).\nWide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent. In\nWallach, H., Larochelle, H., Beygelzimer, A., Alché-Buc, F. d., Fox, E., and Garnett, R., editors,\nAdvances in Neural Information Processing Systems 32, pages 8572–8583. Curran Associates,\nInc.\nLevina, E. and Bickel, P. (2004). Maximum Likelihood Estimation of Intrinsic Dimension. In\nAdvances in Neural Information Processing Systems, volume 17. MIT Press.\nLi, Y. and Liang, Y. (2018). Learning Overparameterized Neural Networks via Stochastic\nGradient Descent on Structured Data. In Advances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc.\nLoureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., Mézard, M., and Zdeborová, L. (2022).\nLearning curves of generic features maps for realistic datasets with a teacher-student model*.\nJournal of Statistical Mechanics: Theory and Experiment, 2022(11):114001. Publisher: IOP\nPublishing and SISSA.\nLuxburg, U. v. and Bousquet, O. (2004). Distance-based classification with lipschitz functions.\nJournal of Machine Learning Research, 5(Jun):669–695.\nLuxburg, U. v. and Schölkopf, B. (2011). Statistical Learning Theory: Models, Concepts, and\nResults. In Gabbay, D. M., Hartmann, S., and Woods, J., editors, Handbook of the History of\nLogic, volume 10 of Inductive Logic, pages 651–706. North-Holland.\n196\nBibliography\nMalach, E. and Shalev-Shwartz, S. (2018). A provably correct algorithm for deep learning that\nactually works. arXiv preprint arXiv:1803.09522.\nMalach, E. and Shalev-Shwartz, S. (2020). The implications of local correlation on learning\nsome deep functions. Advances in Neural Information Processing Systems, 33:1322–1332.\nMallat, S. (2016). Understanding deep convolutional networks. Philosophical Transactions of\nthe Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150203.\nMei, S., Bai, Y., and Montanari, A. (2016). The landscape of empirical risk for non-convex\nlosses. arXiv preprint arXiv:1607.06534.\nMei, S., Misiakiewicz, T., and Montanari, A. (2019). Mean-field theory of two-layers neural\nnetworks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages\n2388–2464. PMLR. ISSN: 2640-3498.\nMei, S., Misiakiewicz, T., and Montanari, A. (2022). Generalization error of random feature\nand kernel methods: hypercontractivity and kernel matrix concentration. Applied and\nComputational Harmonic Analysis, 59:3–84.\nMei, S. and Montanari, A. (2020). The generalization error of random features regression:\nPrecise asymptotics and double descent curve. arXiv:1908.05355 [math, stat].\nMei, S., Montanari, A., and Nguyen, P.-M. (2018). A mean field view of the landscape of\ntwo-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–\nE7671. Publisher: National Academy of Sciences Section: PNAS Plus.\nMhaskar, H. N. and Poggio, T. (2016). Deep vs. shallow networks: An approximation theory per-\nspective. Analysis and Applications, 14(06):829–848. Publisher: World Scientific Publishing\nCo.\nMhaskar, H. N. and Poggio, T. (2019).\nFunction approximation by deep networks.\narXiv:1905.12882 [cs, stat].\nMisiakiewicz, T. and Mei, S. (2022). Learning with convolution and pooling operations in\nkernel methods. Advances in Neural Information Processing Systems, 35:29014–29025.\nModas, A., Rade, R., Ortiz-Jiménez, G., Moosavi-Dezfooli, S.-M., and Frossard, P. (2022). PRIME:\nA Few Primitives Can Boost Robustness to Common Corruptions. In Avidan, S., Brostow, G.,\nCissé, M., Farinella, G. M., and Hassner, T., editors, Computer Vision – ECCV 2022, Lecture\nNotes in Computer Science, pages 623–640, Cham. Springer Nature Switzerland.\nMossel, E. (2018). Deep Learning and Hierarchal Generative Models. arXiv:1612.09057 [cs].\nNakkiran, P. and Belkin, M. (2022). Incentivizing Empirical Science in Machine Learning:\nProblems and Proposals. ML Evaluation Standards Workshop at ICLR 2022.\n197\nBibliography\nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. (2021). Deep double\ndescent: where bigger models and more data hurt*. Journal of Statistical Mechanics: Theory\nand Experiment, 2021(12):124003. Publisher: IOP Publishing and SISSA.\nNeal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., and Mitliagkas, I.\n(2019). A Modern Take on the Bias-Variance Tradeoff in Neural Networks. arXiv:1810.08591\n[cs, stat].\nNeal, R. M. (1996). Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc.,\nSecaucus, NJ, USA.\nNeyshabur, B. (2020). Towards Learning Convolutions from Scratch. In Advances in Neural\nInformation Processing Systems, volume 33, pages 8078–8088. Curran Associates, Inc.\nNeyshabur, B., Tomioka, R., and Srebro, N. (2015). In Search of the Real Inductive Bias: On the\nRole of Implicit Regularization in Deep Learning. arXiv:1412.6614 [cs, stat].\nNguyen, P.-M. (2019). Mean field limit of the learning dynamics of multilayer neural networks.\narXiv preprint arXiv:1902.02880.\nNguyen, P.-M. and Pham, H. T. (2023). A Rigorous Framework for the Mean Field Limit of\nMultilayer Neural Networks. arXiv:2001.11443 [cond-mat, stat].\nNovak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Abolafia, D. A., Pennington, J., and Sohl-dickstein,\nJ. (2019). Bayesian deep convolutional networks with many channels are gaussian processes.\nIn International Conference on Learning Representations.\nOlah, C., Mordvintsev, A., and Schubert, L. (2017). Feature Visualization. Distill, 2(11):e7.\nOpenAI (2023). Gpt-4 technical report.\nOrtiz-Jiménez, G., Modas, A., Moosavi-Dezfooli, S.-M., and Frossard, P. (2021). Optimism in\nthe Face of Adversity: Understanding and Improving Deep Learning Through Adversarial\nRobustness. Proceedings of the IEEE, 109(5):635–659. Conference Name: Proceedings of the\nIEEE.\nPaccolat, J., Petrini, L., Geiger, M., Tyloo, K., and Wyart, M. (2021a). Geometric compression\nof invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and\nExperiment, 2021(4):044001. Publisher: IOP Publishing.\nPaccolat, J., Spigler, S., and Wyart, M. (2021b). How isotropic kernels perform on simple invari-\nants. Machine Learning: Science and Technology, 2(2):025020. Publisher: IOP Publishing.\nPapyan, V., Han, X. Y., and Donoho, D. L. (2020). Prevalence of neural collapse during the\nterminal phase of deep learning training. Proceedings of the National Academy of Sciences,\n117(40):24652–24663. Publisher: Proceedings of the National Academy of Sciences.\n198\nBibliography\nPatel, A. B., Nguyen, T., and Baraniuk, R. G. (2015). A Probabilistic Theory of Deep Learning.\narXiv:1504.00641 [cs, stat].\nPellegrini, F. and Biroli, G. (2021). Sifting out the features by pruning: Are convolutional\nnetworks the winning lottery ticket of fully connected ones? arXiv:2104.13343 [cs, stat].\nPetrini, L., Cagnetta, F., Tomasini, U. M., Favero, A., and Wyart, M. (2023). How Deep Neural\nNetworks Learn Compositional Data: The Random Hierarchy Model. arXiv:2307.02129 [cs,\nstat].\nPetrini, L., Cagnetta, F., Vanden-Eijnden, E., and Wyart, M. (2022). Learning sparse features can\nlead to overfitting in neural networks. Advances in Neural Information Processing Systems,\n35:9403–9416.\nPetrini, L., Favero, A., Geiger, M., and Wyart, M. (2021). Relative stability toward diffeomor-\nphisms indicates performance in deep nets. In Advances in Neural Information Processing\nSystems, volume 34, pages 8727–8739. Curran Associates, Inc.\nPoggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. (2017). Why and when can\ndeep-but not shallow-networks avoid the curse of dimensionality: a review. International\nJournal of Automation and Computing, 14(5):503–519.\nPope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2021). The Intrinsic\nDimension of Images and Its Impact on Learning.\nPopper, K. R. (1968). The Logic of Scientific Discovery. Hutchinson, London, 3rd edition edition.\nRahimi, A. and Recht, B. (2007). Random Features for Large-Scale Kernel Machines. In\nAdvances in Neural Information Processing Systems, volume 20. Curran Associates, Inc.\nRecanatesi, S., Farrell, M., Advani, M., Moore, T., Lajoie, G., and Shea-Brown, E. (2019).\nDimensionality compression and expansion in deep neural networks.\narXiv preprint\narXiv:1906.00443.\nRefinetti, M., Goldt, S., Krzakala, F., and Zdeborová, L. (2021). Classifying high-dimensional\ngaussian mixtures: Where kernel methods fail and neural networks succeed. arXiv preprint\narXiv:2102.11742.\nRosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and orga-\nnization in the brain. Psychological Review, 65(6):386–408. Place: US Publisher: American\nPsychological Association.\nRotskoff, G. M. and Vanden-Eijnden, E. (2018). Neural networks as interacting particle systems:\nAsymptotic convexity of the loss landscape and universal scaling of the approximation error.\narXiv preprint arXiv:1805.00915.\n199\nBibliography\nRuderman, A., Rabinowitz, N. C., Morcos, A. S., and Zoran, D. (2018). Pooling is neither\nnecessary nor sufficient for appropriate deformation stability in CNNs. arXiv:1804.04438\n[cs, stat]. arXiv: 1804.04438.\nRudi, A. and Rosasco, L. (2017). Generalization properties of learning with random features.\nIn Advances in Neural Information Processing Systems, pages 3215–3225.\nSaxe, A. M., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey, B. D., and Cox, D. D. (2019).\nOn the information bottleneck theory of deep learning. Journal of Statistical Mechanics:\nTheory and Experiment, 2019(12):124020.\nSchmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with relu\nactivation function. The Annals of Statistics, 48(4):1875–1897.\nScholkopf, B. and Smola, A. J. (2001). Learning with kernels: support vector machines, regular-\nization, optimization, and beyond. MIT press.\nSchölkopf, B., Herbrich, R., and Smola, A. J. (2001). A Generalized Representer Theorem. In\nHelmbold, D. and Williamson, B., editors, Computational Learning Theory, Lecture Notes in\nComputer Science, pages 416–426, Berlin, Heidelberg. Springer.\nShalev-Shwartz, S., Shamir, O., and Shammah, S. (2017). Failures of gradient-based deep\nlearning. volume 6.\nShankar, V., Fang, A., Guo, W., Fridovich-Keil, S., Ragan-Kelley, J., Schmidt, L., and Recht, B.\n(2020). Neural Kernels Without Tangents. In International Conference on Machine Learning,\npages 8614–8623. PMLR. ISSN: 2640-3498.\nShwartz-Ziv, R. and Tishby, N. (2017). Opening the black box of deep neural networks via\ninformation. arXiv preprint arXiv:1703.00810.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,\nL., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge.\nnature, 550(7676):354–359.\nSirignano, J. and Spiliopoulos, K. (2020). Mean Field Analysis of Neural Networks: A Law of\nLarge Numbers. SIAM Journal on Applied Mathematics, 80(2):725–752. Publisher: Society\nfor Industrial and Applied Mathematics.\nSirignano, J. and Spiliopoulos, K. (2021). Mean Field Analysis of Deep Neural Networks.\narXiv:1903.04440 [math, stat].\nSmola, A. J., Schölkopf, B., and Müller, K.-R. (1998). The connection between regularization\noperators and support vector kernels. Neural networks, 11(4):637–649.\nSoltanolkotabi, M., Javanmard, A., and Lee, J. D. (2019). Theoretical Insights Into the Opti-\nmization Landscape of Over-Parameterized Shallow Neural Networks. IEEE Transactions on\nInformation Theory, 65(2):742–769. Conference Name: IEEE Transactions on Information\nTheory.\n200\nBibliography\nSpigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli, G., and Wyart, M. (2019). A jamming transi-\ntion from under- to over-parametrization affects generalization in deep learning. Journal of\nPhysics A: Mathematical and Theoretical, 52(47):474001. Publisher: IOP Publishing.\nSpigler, S., Geiger, M., and Wyart, M. (2020). Asymptotic learning curves of kernel methods:\nempirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory\nand Experiment, 2020(12):124001. Publisher: IOP Publishing.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout:\nA Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning\nResearch, 15(56):1929–1958.\nSzegedy, C., Wei Liu, Yangqing Jia, Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,\nV., and Rabinovich, A. (2015). Going deeper with convolutions. In 2015 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 1–9, Boston, MA, USA. IEEE.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R.\n(2014). Intriguing properties of neural networks. arXiv:1312.6199 [cs].\nTibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal\nStatistical Society. Series B (Methodological), 58(1):267–288. Publisher: [Royal Statistical\nSociety, Wiley].\nTishby, N., Pereira, F. C., and Bialek, W. (2000).\nThe information bottleneck method.\narXiv:physics/0004057.\nTomasini, U. M., Petrini, L., Cagnetta, F., and Wyart, M. (2022a). How deep convolutional\nneural networks lose spatial information with training. arXiv:2210.01506 [cs].\nTomasini, U. M., Sclocchi, A., and Wyart, M. (2022b). Failure and success of the spectral\nbias prediction for Laplace Kernel Ridge Regression: the case of low-dimensional data. In\nProceedings of the 39th International Conference on Machine Learning, pages 21548–21583.\nPMLR. ISSN: 2640-3498.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and\nPolosukhin, I. (2017). Attention is All you Need. In Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc.\nVoulodimos, A., Doulamis, N., Doulamis, A., Protopapadakis, E., et al. (2018). Deep learning\nfor computer vision: A brief review. Computational intelligence and neuroscience, 2018.\nWainwright, M. J. (2019). High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cam-\nbridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,\nCambridge.\nWei, C., Lee, J. D., Liu, Q., and Ma, T. (2019). Regularization Matters: Generalization and\nOptimization of Neural Nets v.s. their Induced Kernel. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc.\n201\nBibliography\nWilliams, C. K. (1997). Computing with infinite networks. In Advances in neural information\nprocessing systems, pages 295–301.\nWoodworth, B., Gunasekar, S., Lee, J. D., Moroshko, E., Savarese, P., Golan, I., Soudry, D., and\nSrebro, N. (2020). Kernel and Rich Regimes in Overparametrized Models. In Conference on\nLearning Theory, pages 3635–3673. PMLR. ISSN: 2640-3498.\nXiao, L. (2022). Eigenspace Restructuring: A Principle of Space and Frequency in Neural\nNetworks. In Proceedings of Thirty Fifth Conference on Learning Theory, pages 4888–4944.\nPMLR. ISSN: 2640-3498.\nYehudai, G. and Shamir, O. (2019). On the power and limitations of random features for\nunderstanding neural networks. In Advances in Neural Information Processing Systems,\npages 6598–6608.\nYosinski, J., Clune, J., Nguyen, A., Fuchs, T., and Lipson, H. (2015). Understanding Neural\nNetworks Through Deep Visualization. arXiv:1506.06579 [cs].\nZdeborová, L. (2020). Understanding deep learning is also a job for physicists. Nature Physics,\n16(6):602–604. Number: 6 Publisher: Nature Publishing Group.\nZeiler, M. D. and Fergus, R. (2014). Visualizing and Understanding Convolutional Networks.\nIn Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T., editors, Computer Vision – ECCV\n2014, Lecture Notes in Computer Science, pages 818–833, Cham. Springer International\nPublishing.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep\nlearning requires rethinking generalization. 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nZhang, R. (2019).\nMaking convolutional networks shift-invariant again.\narXiv preprint\narXiv:1904.11486.\nZipf, G. K. (1935). The psycho-biology of language;: An introduction to dynamic Philology.\nHoughton Mifflin, first edition edition.\nZou, D., Cao, Y., Zhou, D., and Gu, Q. (2020). Gradient descent optimizes over-parameterized\ndeep ReLU networks. Machine Learning, 109(3):467–492.\n202\nLeonardoPetrini\nPhD Student, Physics and Machine Learning @ EPFL\nhigher education\n2019-present PhD Student @ Physics of Complex Systems Lab\nEPFL, Lausanne CH\nDeep Learning Theory\nsummer ’18\nCERN Summer Student Program - ATLAS Experiment\nCERN, Meyrin CH\nProject: Classification and Regression Studies for Flavour Tagging\n2017 - 2019\nMaster in Physics @ EPFL (GPA: 5.7/6)\nEPFL, Lausanne CH\nMinor in Computational Science and Engineering\nMaster Thesis: Replicated Affinity Propagation Algorithm.\nSupervisor: Prof. Riccardo Zecchina, Artificial Intelligence Lab–Bocconi University\n2016 - 2017\nETH Exchange program\nETHZ, Zurich CH\nVisiting student\n2014 - 2017\nBachelor cum laude (110L/110)\nPolitecnico di Torino, Turin IT\nPhysical Engineering and Young Talents Program (Progetto Giovani Talenti)\npublications\n2023\nHow Deep Neural Networks Learn Compositional Data:\nThe Random Hierarchy Model\nPaper under review\nLP, F. Cagnetta, U.M. Tomasini, A. Favero, M. Wyart (arXiv link)\n2022\nHow deep convolutional neural networks lose spatial information with training\nPaper @ ICLR 2023 Workshop, Physics for ML\nU.M. Tomasini, LP, F. Cagnetta, M. Wyart (arXiv link)\n2022\nLearning sparse features can lead to overfitting in neural networks\nPaper @ NeurIPS 2022\nLP, F. Cagnetta, E. Vanden-Eijnden, M. Wyart (OpenReview link)\n2021\nRelative stability toward diffeomorphisms indicates performance in deep nets\nPaper @ NeurIPS 2021\nLP, A. Favero, M. Geiger, M. Wyart (OpenReview link)\n2020\nLandscape and training regimes in deep learning\nPaper @ Physics Reports\nM. Geiger, LP, M. Wyart\n2020\nGeometric compression of invariant manifolds in neural networks\nPaper @ Journal of Statistical Mechanics: Theory and Experiment\nJ. Paccolat, LP, M. Geiger, K. Tyloo, M. Wyart\nteaching and reviewing\n• Teaching assistant for Statistical Physics II and III, 2019 to 2022.\n• Teacher and supervisor of semester and master projects, 2019 to 2023.\n• Reviewer for the Journal of Machine Learning Research (JMLR), 2022.\n• Workshop on the Theory of Overparameterized Machine Learning (TOPML), 2022\n• Reviewer for the Conference on Neural Information Processing Systems (NeurIPS),\n2023.\nabout\nCurrently in Lausanne, CH\nwebsite: leopetrini.me\nleo.petrini@outlook.com\ngithub/leonardopetrini\nslides.com/leopetrini\ntwitter.com/leopetrini_\nlanguages\nitalian native\nenglish full proficiency\nfrench fluent\nprogramming\nPython advanced (6yrs)\n(Numpy, sklearn, Pandas)\nPyTorch advanced (4yrs)\nJulia beginner (1yr)\nsoft skills\ncuriosity\nteam work\ncommunication\ninterests\nfood\ndeep learning\n(personal) finance\nmountains\nphotography\n203\nconferences and schools\nFebruary ’23 NeuroStatPhys Workshop [poster]\nEcole de Physique des Houches, FR\nAugust ’22\nIAIFI PhD Summer School and Workshop [poster]\nInstitute for Artificial Intelligence and Fundamental Interactions, Boston, US\nApr. ’22\nWorkshop on the Theory of Overparameterized Machine Learning [talk]\nhttps://topml.rice.edu/\nSept. ’21\nOn Future Synergies for Stochastic and Learning Algorithms [poster]\nCIRM Marseille, FR\nJune ’21\nStatistical Mechanics and Emergent Phenomena in Biology [poster]\nThe Beg Rohu Summer School, FR\nJune ’21\nYouth in High Dimensions Conference [poster]\nICTP, Trieste, IT\nMarch ’21\nHow neural nets compress invariant manifolds [talk]\nAmerical Physical Society, March Meeting, US\nAugust ’20\nStatistical Physics and Machine Learning Workshop [talk]\nEcole de Physique des Houches, FR\nsecondary education\n2018\nNational High School Model United Nations NHSMUN\nNew York, NY\nFaculty Advisor for Liceo Scientifico G. Galilei, Macerata\n2014\nNational Physics Olympiad, Italy\nSenigallia, IT\nWinner of 1st and 2nd level competitions, admitted to the national stage\n2014\nNational Mathematical Olimpiad, Italy\nCesenatico, IT\nTeam competition\n2013\nNational Physics Olympiad, Italy\nSenigallia, IT\nWinner of 1st and 2nd level competitions, admitted to the national stage\n2013\nNational High School Model United Nations NHSMUN\nNew York, NY\nNigeria delegation, Legal Committee\n2013-2014\nScientific degrees program, University of Camerino\nCamerino, IT\nProgetto lauree scientifiche, physics division\n2012\nNational Mathematical Olimpiad, Italy\nCesenatico, IT\nTeam competition\n2009-2014\nLiceo Scientifico G.Galilei Macerata\nMacerata, IT\nPNI, Piano Nazionale Informatica (National program for informatics)\n204\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-10-24",
  "updated": "2023-10-24"
}