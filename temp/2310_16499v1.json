{
  "id": "http://arxiv.org/abs/2310.16499v1",
  "title": "Data Optimization in Deep Learning: A Survey",
  "authors": [
    "Ou Wu",
    "Rujing Yao"
  ],
  "abstract": "Large-scale, high-quality data are considered an essential factor for the\nsuccessful application of many deep learning techniques. Meanwhile, numerous\nreal-world deep learning tasks still have to contend with the lack of\nsufficient amounts of high-quality data. Additionally, issues such as model\nrobustness, fairness, and trustworthiness are also closely related to training\ndata. Consequently, a huge number of studies in the existing literature have\nfocused on the data aspect in deep learning tasks. Some typical data\noptimization techniques include data augmentation, logit perturbation, sample\nweighting, and data condensation. These techniques usually come from different\ndeep learning divisions and their theoretical inspirations or heuristic\nmotivations may seem unrelated to each other. This study aims to organize a\nwide range of existing data optimization methodologies for deep learning from\nthe previous literature, and makes the effort to construct a comprehensive\ntaxonomy for them. The constructed taxonomy considers the diversity of split\ndimensions, and deep sub-taxonomies are constructed for each dimension. On the\nbasis of the taxonomy, connections among the extensive data optimization\nmethods for deep learning are built in terms of four aspects. We probe into\nrendering several promising and interesting future directions. The constructed\ntaxonomy and the revealed connections will enlighten the better understanding\nof existing methods and the design of novel data optimization techniques.\nFurthermore, our aspiration for this survey is to promote data optimization as\nan independent subdivision of deep learning. A curated, up-to-date list of\nresources related to data optimization in deep learning is available at\n\\url{https://github.com/YaoRujing/Data-Optimization}.",
  "text": "1\nData Optimization in Deep Learning: A Survey\nOu Wu, Rujing Yao\nAbstract—Large-scale, high-quality data are considered an es-\nsential factor for the successful application of many deep learning\ntechniques. Meanwhile, numerous real-world deep learning tasks\nstill have to contend with the lack of sufficient amounts of\nhigh-quality data. Additionally, issues such as model robustness,\nfairness, and trustworthiness are also closely related to training\ndata. Consequently, a huge number of studies in the existing\nliterature have focused on the data aspect in deep learning\ntasks. Some typical data optimization techniques include data\naugmentation, logit perturbation, sample weighting, and data\ncondensation. These techniques usually come from different deep\nlearning divisions and their theoretical inspirations or heuristic\nmotivations may seem unrelated to each other. This study aims to\norganize a wide range of existing data optimization methodologies\nfor deep learning from the previous literature, and makes the\neffort to construct a comprehensive taxonomy for them. The\nconstructed taxonomy considers the diversity of split dimensions,\nand deep sub-taxonomies are constructed for each dimension.\nOn the basis of the taxonomy, connections among the extensive\ndata optimization methods for deep learning are built in terms\nof four aspects. We probe into rendering several promising\nand interesting future directions. The constructed taxonomy and\nthe revealed connections will enlighten the better understanding\nof existing methods and the design of novel data optimization\ntechniques. Furthermore, our aspiration for this survey is to\npromote data optimization as an independent subdivision of\ndeep learning. A curated, up-to-date list of resources related\nto data optimization in deep learning is available at https:\n//github.com/YaoRujing/Data-Optimization.\nIndex Terms—Deep learning, data optimization, data augmen-\ntation, sample weighting, data perturbation.\nI. INTRODUCTION\nD\nEEP learning has received increasing attention in both\nthe AI community and many application domains due to\nits superior performance in various machine-learning tasks in\nrecent years. A successful application of deep learning cannot\nleave the main factors, which include a properly designed deep\nneural network (DNN), a set of high-quality training data, and\na well-suited learning strategy (e.g., initialization schemes for\nhyper-parameters). Among the main factors, training data is\nof great importance and it usually plays a decisive role in\nthe entire training process [1]. The concept of data-centric\nAI is rising, which breaks away from the widespread model-\ncentric perspective [2]. Large models like GPT-4 show signif-\nicant potential in the direction of achieving general artificial\nintelligence (AGI). It is widely accepted that the training for\nlarge models requires a huge size of high-quality training data.\nHowever, most real applications lack ideal training data.\nReal training data usually encounters one or several of the\nOu Wu is with National Center for Applied Mathematics, Tianjin University,\nTianjin, China, 300072. Rujing Yao is with Department of Information\nResources Management, Nankai University, Tianjin, China, 300071. E-mail:\nwuou@tju.edu.cn, rjyao@mail.nankai.edu.cn.\nFig. 1. Nine issues around real training data.\nnine common issues as shown in Fig. 1. The following six\nissues are directly related to training data:\n(1) Biased distribution: This issue denotes that the distribu-\ntion of the training data does not conform to the true\ndistribution of the data in a learning task. One typical\nbias is class imbalance, in which the proportions of\ndifferent categories in the training data are not identical\ndue to reasons such as data collection difficulties, whereas\nthe proportions of different categories in test data are\nidentical.\n(2) Low quality: This issue corresponds to at least two\nscenarios. The first refers to data noise that either partial\ntraining samples or partial training labels contain noises.\nAs for sample noises, partial samples themselves are\ncorrupted by noises. Taking optical character recogni-\ntion (OCR) for example, some scanned images may\ncontain serious background noises. The second typical\ncase occurs in multi-model/multi-view learning scenarios.\nInconsistency and information missing may exist [3], [4].\nFor instance, the text title for an image may be mistakenly\nprovided, or it may contain limited words with little\ninformation.\n(3) Small size: The training size surely impacts the train-\ning performance [5]. The larger the training data, the\nbetter the training performance usually being attained.\nDue to insufficient data collection budget or technique\nlimitation, the training data will be relatively small for\nreal use. Therefore, learning under small-size training\ndata is a serious concern in deep learning. This study\ndoes not discuss the extreme cases of small size, such as\nfew/one/zero-shot learning.\n(4) Sample redundancy: Even though large training data is\nexpected, it does not mean that every datum is useful.\nThere are still learning tasks that the training set contains\nredundant data [6]. Two typical cases exist. First, the\ntraining size is relatively large and exceeds the processing\ncapacity of the available computing hardware. Second,\narXiv:2310.16499v1  [cs.LG]  25 Oct 2023\n2\nsome regions of training samples may be sampled exces-\nsively, and the deletion of such excessive training samples\ndoes not affect the training performance. In this case,\nsample redundancy may occur in certain subsets of some\ncategories.\n(5) Lack of diversity: This issue refers to the fact that some\nattributes for certain categories concentrate excessively in\nthe training corpus. Data diversity is also crucial for DNN\ntraining [7]. Taking object classification as an example,\nthe backgrounds in images of the dog category may usu-\nally be green grass. However, the “dog” category is not\nnecessarily related to green grass. The lack of diversity\nin some non-essential attributes can lead to a spurious\ncorrelation between some non-essential attributes and the\ncategory. This issue is similar to the second case of\nsample redundancy. Nevertheless, lack of diversity does\nnot necessarily imply the presence of redundant samples.\n(6) Distribution drift: This issue denotes that the distribution\nof the involved data varies over time. Indeed, distribution\ndrift may occur in most real learning applications, as\neither the concept or the form (e.g., object appearances,\ntext styles) of samples varies fast or slow. Concept\ndrift [8] is the research focus in distribution drift.\nThe above summary of data issues is not mutually exclusive, as\nthere are overlaps among different issues. For example, small\nsize may only occur in several categories, which can also be\nattributed to a type of biased distribution. Besides these data\nissues, there are also some other (not exhaustive) issues closely\nrelated to the training data:\n(7) Model robustness: This issue concerns the resistance\nability of a DNN model to adversarial attacks [9]. Model\nrobustness is highly important for applications related to\nhealth, finance, and human life. If DNN models for these\napplications are compromised by adversarial attacks, se-\nrious consequences may ensue.\n(8) Fairness: This issue concerns the performance differences\namong different categories or different attributes in a\nlearning task [10]. For example, the recognition accuracy\nof faces in different color groups should be at the same\nlevel.\n(9) Trustworthiness. This issue has emerged in recent years\nas deep learning has been gradually applied in many\nsafety-critical applications such as autonomous driving\nand medical assistance [11]. This issue is closely related\nto robustness and fairness. It mainly refers to the explain-\nability and calibration of DNN models.\nTo address the above-mentioned issues, numerous theoret-\nical explorations have been conducted and tremendous new\nmethodologies have been proposed in previous literature. Most\nof these existing methods directly optimize the involved data\nin learning rather than explore new DNN structures, which\nis referred to as data optimization for deep learning in this\npaper. As the listed issues belong to different machine learn-\ning divisions, the inspirations and focuses of these methods\nare usually distinct and seem unrelated to each other. For\ninstance, the primary learning strategy for imbalanced learning\n(belonging to the biased distribution issue) is sample weighting\nwhich assigns different weights to training samples in deep\nlearning training epochs. The primary manipulation for the\nsmall-size issue is to employ the data augmentation technique\nsuch as image resize and mixup [12] for image classification.\nWhen dealing with label noise in deep learning, one strategy\nis to identify noisy labels and then remove them during\ntraining. In cases where training data for certain categories lack\nsufficient diversity, causal learning is employed to break down\nthe spurious correlations among labels and some irrelevant\nattributes such as certain backgrounds. Due to the apparent\nlack of connection, these studies typically do not mutually\ncite or discuss each other.\nOur previous study [13] partially reveals that one technique,\nnamely, data perturbation, has been leveraged to deal with\nmost aforementioned issues. This observation illuminates us to\nexplore the data optimization methodologies for those issues in\na more broad view. In this study, a comprehensive review for a\nwide range of data optimization methods is conducted. First, a\nsystematic data optimization taxonomy is established in terms\nof eight dimensions, including pipeline, object, technical path,\nand so on. Second, the intrinsic connections among some clas-\nsical methods are explored according to four aspects, including\ndata perception, application scenario, similarity/opposite, and\ntheory. Third, theoretical studies are summarized for the\nexisting data optimization techniques. Lastly, several future\ndirections are presented according to our analysis.\nThe differences between our survey and existing surveys\nin relevant areas, including imbalanced learning, noisy-label\nlearning, data augmentation, adversarial training, and distilla-\ntion, lie in two aspects. First, this survey takes a data-centric\nview for studies from a wide range of distinct deep learning\nrealms. Therefore, our focus is merely on the data optimization\nstudies for the listed issues. Methods that do not belong to\ndata optimization for the listed issues are not referred to in\nthis study. Second, the split dimensions (e.g., data perception\nand theory) which facilitate the establishment of connections\namong seemingly unrelated methods are considered in our\ntaxonomy. These dimensions are usually not referred to in\nthe existing surveys.\nThe contributions of this study are summarized as follows.\n• Methodologies related to data enhancement for dealing\nwith distinct deep learning issues are reviewed with a new\ntaxonomy. To our knowledge, this is the first work that\naims to construct a data-centric taxonomy focusing on\ndata optimization across multiple deep learning divisions\nand applications.\n• The connections among many seemingly unrelated meth-\nods are built according to our constructed taxonomy.\nThe connections can inspire researchers to design more\npotential new techniques.\n• Theoretical studies for data optimization are summarized\nand interesting future directions are discussed.\nThis paper is organized as follows. Section II introduces\nmain survey studies related to data optimization techniques.\nSection III describes the main framework of our constructed\ndata optimization taxonomy. Sections IV, V, VI, and VII\nintroduce the details of our taxonomy. Section VIII explores\nthe connections among different data optimization techniques.\n3\nSection IX presents several future directions, and conclusions\nare presented in Section X.\nII. RELATED STUDIES\nThe issues listed in the previous section gradually spawn\nnumerous independent research realms of deep learning. Sub-\nsequently, there have been many survey studies conducted\nfor these issues. The following introduces related surveys in\nseveral typical research topics.\nImbalanced learning. It is a hot research area in deep learn-\ning [14]. He and Garcia [15] conducted the first comprehensive\nyet deep survey study on imbalanced learning. They explored\nthe intrinsic characteristics of learning tasks incurred by imbal-\nanced data. It is noteworthy that He and Garcia pointed out that\nan imbalanced dataset is “a high-complexity dataset with both\nbetween-class and within-class imbalances, multiple concepts,\noverlapping, noise, and a lack of representative data”. This\nstatement refers to most data issues listed in Section I. For\ninstance, the lack of diversity and sample redundancy can be\nconsidered as a lack of representativeness. Recent studies have\nfocused on the extreme case of imbalanced learning, namely,\nlong-tailed classification. Zhang et al. [16] summarized the\nrecent developments in deep long-tailed classification. In their\nconstructed taxonomy, module improvement such as a new\nclassifier is listed as one of the three main techniques. In this\nstudy, module improvement is not considered, as it does not\nfall under data optimization.\nNoisy-label learning. This is another research area gaining\ntremendous attention in recent years as label noise is nearly\nunavoidable in real learning tasks. Algan and Ulisory [17]\nsummarized the methods in noisy-label learning for image\nclassification. Song et al. [18] elaborately designed taxon-\nomy for noisy-label learning along with three categories,\nincluding “data”, “objective”, and “optimization”. Their tax-\nonomy facilitates the understanding of a huge number of\nexisting techniques. Nevertheless, overlap exists between the\nthree dimensions. For example, reweighting locates in the\n“objective” category, whereas learning to reweight locates in\nthe “optimization” category. The taxonomy introduces in this\nstudy may aid the construction a more appropriate taxonomy\nfor noisy-label learning.\nLearning with small data. Big data has achieved great\nsuccess in deep learning tasks. Meanwhile, many real learning\ntasks still confront with the challenge of small-size training\ndata. Cao et al. [19] performed rigorous theoretical analysis\nfor the generalization error and label complexity of learning on\nsmall data. They categorized the small-data learning methods\ninto those with the Euclidean or non-Euclidean mean repre-\nsentation. Wang et al. [20] constructed a few-shot learning\ntaxonomy with three folds, including “data”, “model”, and\n“algorithm”. Data-centric learning methods are also among the\nprimary choices for few-shot learning.\nConcept drift. Lu et al. [8] investigated the learning\nfor concept drift under three components, including concept\ndrift detection, concept drift understanding, and concept drift\nadaptation. Yuan et al. [21] divided existing studies into\ntwo categories, namely, model parameter updating and model\nstructure updating in concept drift adaptation. This division\nis from the viewpoint of the model. Indeed, pure data-based\nstrategy is also employed in learning under concept drift. For\nexample, Diez-Olivan et al. [22] leveraged data augmentation\nto fine-tune the last layer of DNNs for quickly concept drift\nadaptation. This study may motivate researchers on distribu-\ntion drift to focus more on data optimization manners. Some\nearly surveys can be found in [23], [24].\nAdversarial robustness. In many studies, model robustness\nis limited to adversarial robustness. Silva and Najafirad [25]\nexplored challenges and future directions for model robustness\nin deep learning. They divided existing adversarial robust\nlearning methods into three categories, including adversarial\ntraining, regularization, and certified defenses. Xu et al. [26]\nsummarized the studies for model robustness on graphs. Goyal\net al. [27] reviewed the adversarial defense and robustness\nin the NLP community. Their constructed taxonomy contains\nfour categories, including adversarial training, perturbation\ncontrol, certification, and miscellaneous. In this study, adver-\nsarial training is taken as a data optimization strategy, as it\nenhances the training set by adding or virtually adding new\ndata.\nFairness-aware learning. It receives increasingly attention\nin recent years. Mehrabi et al. [28] explored different sources\nof biases that can affect the fairness of learning models. They\nrevealed that each of the three factors, namely, data, learning\nalgorithms, and involved users may result in bias. Petrovi´c et\nal. [29] pointed out that sample reweighting and adversarial\ntraining are two common strategies for fair machine learning.\nTrustworthy learning. It is the key of trustworthy AI,\nwhich aims to ensure that an AI system is worthy of being\ntrusted. Trust is a complex phenomenon [30] highly related\nto fairness, explainability, reliability, etc. Kaur et al. [31]\nsummarized studies on trustworthy artificial intelligence in a\nquite broad view. Wu et al. [32] provided an in-depth review\nfor studies about trustworthy learning on graphs.\nThere are also studies that focus on learning tasks with more\nthan one of the listed data issues. For example, Fang et al. [33]\naddressed noisy-label learning under the long-tailed distribu-\ntions of training data. Singh et al. [34] conducted an empirical\nstudy concerning fairness, adversarial robustness, and concept\ndrift, simultaneously. To our knowledge, no survey study pays\nattention to the intersection of the research areas related to the\nlisted issues. The unified taxonomy constructed in this survey\nwould enlighten the study on the intersection of multiple\nresearch areas.\nThe most similar study to this work is the survey presented\nby Wan et al. [35], which focuses on data optimization in\ncomputer vision. There are significant differences between our\nand Wan et al.’s study. First, the covered technical scopes\nof ours are much broader than those of Wan et al.’s study.\nTheir study limits the scope merely in data selection, including\nresampling, subset selection, and active learning-based selec-\ntion. Nevertheless, perturbation, weighting, dataset distillation,\nand augmentation which attempt to optimize the training data\nwithout modifying the backbone network are considered as the\ndata optimization in this study. Second, the split dimensions\nof ours are quite different from those in Wan et al.’s study\n4\nFig. 2. The six split dimensions of our constructed taxonomy for data optimization.\nFig. 3. The sub-taxonomy for optimization goals.\nfor the overlapped methods. For example, curriculum learning\nis divided into the resampling category, whereas it is divided\ninto the weighting category in our study. Dataset distillation\nis merged into one division, namely, data pruning. In contrast,\ndataset distillation is not included in Wan et al.’s study. Lastly,\nadditional important parts including data perception, connec-\ntions among different paths, and theoretical investigation are\nintroduced and discussed in this study. Zheng et al. [36] holds\na data-centric perspective to review studies on graph machine\nlearning, which is also similar in spirit with our study. Most\ndata issues summarized in this study are also discussed in\ntheir study. They divided existing studies into graph data\ncollection, enhancement, exploration, maintenance (for privacy\nand security), and graph operations, which are not applicable\nfor our taxonomy.\nIt is noteworthy that classical data pre-processing method-\nologies such as data cleaning (e.g., missing data imputation),\nstandardization (e.g., z-score), and transformation (e.g., data\ndiscretization) also aim to make data better for learning.\nConsidering that these methodologies are mature and mainly\nutilized in shallow learning, they are not introduced in this\nsurvey.\nIII. OVERALL OF THE PROPOSED TAXONOMY\nTo ensure our constructed taxonomy well organized and\ncomprehensive coverage on previous data optimization tech-\nniques about the issues listed in Section I as much as possible,\nthe following principles for the design of split dimensions are\nconsidered.\n(1) The first layer of the taxonomy should consider multiple\nviews, with each view corresponding to a sub-taxonomy.\nMost existing taxonomies for specific research realms\nadopt only a single view. In this study, only a single\nview is inadequate for systematically arranging studies\nfrom various deep learning realms.\n(2) The dividing dimension should be general so as to em-\nbrace existing studies as much as possible. Therefore, the\ndimensions designed in existing taxonomies for specific\nresearch areas should not be directly followed. A new\ncomprehensive taxonomy is required.\n(3) The new taxonomy should be compatible with exist-\ning taxonomies. That is, inconsistency between our and\nexisting taxonomies is allowed. However, contradiction\nbetween them should be avoided.\nOn the basis of these principles, the first layer1 of our\ntaxonomy is designed as shown in Fig. fig2. This layer consists\nof six dimensions for data optimization as follows:\n• Ultimate goals. This dimension refers to the final goal of\na data optimization method used in a deep learning task.\nWe divide the optimization goals into five main aspects2,\nincluding generalization, robustness, fairness, trustworthy,\nand efficiency.\n• Application scenarios. This dimension refers to the deep\nlearning applications that utilize data optimization. Nine\napplications are involved, including learning under biased\ndistribution, noisy-label learning, learning with redundant\ntraining data, learning with limited training data, model\nsafety, fairness-aware learning, learning under distribution\ndrift, trustworthy learning, and learning for large models.\n• Data objects. This dimension refers to the objects to\noptimize in the employed data optimization method. Most\nstudies focus on the raw training data. There are also\nmethods concentrating in other data objects such as\nhyper-parameters and meta data.\n• Optimization pipeline. This dimension refers to the com-\nmon steps for a concrete data optimization method for\na deep learning procedure. We divide the pipeline into\nthree common steps, namely, data perception, analysis,\nand optimizing.\n• Optimization techniques. This dimension refers to the em-\nployed technique paths in data optimization. This study\nsummarizes five main paths, namely, data resampling,\ndata augmentation, data perturbation, data weighting, and\ndataset pruning. Each path also contains sub-divisions.\nThe introduction for this part is the focus of this survey.\n• Optimization theories. This dimension refers to the the-\noretical analysis and exploration for data optimization\nin deep learning. We divided this dimension into two\naspects: formulation and explanation.\nSection IV introduces the ultimate goals, application sce-\nnarios, and data objects. Sections V, VI, and VII introduces\nthe optimization pipeline, technique, and theory, respectively.\nIV. GOALS, SCENARIOS, AND DATA OBJECTS\nThis section introduces ultimate goals, targeted applications,\nand data objects.\nA. Optimization goals\nFig. 14 describes the sub-taxonomy for the dimension of\noptimization goals, including generalization, fairness, robust-\nness, trustworthiness, and efficiency.\n1The fine-granularity layers are detailed in the succeeding sections.\n2It should be noted that these five aspects are not exhaustive and there are\noverlaps among them as revealed by the previous literature.\n5\nGeneralization is the primary optimization goal in most\ndata optimization techniques, as it is almost the sole goal\nin most deep learning tasks. According to the generalization\ntheory studied in shallow learning, generalization of a category\nis highly related to class margin, inter-class distance, and\nclass compactness [37]. A larger margin/larger inter-class\ndistance/higher class compactness of a category indicates a\nbetter generalization performance on the learned model on the\ncategory. The data augmentation strategy that injects noise to\ntraining samples is proven to increase the generalization [38].\nThe implicit data augmentation method ISDA [39] actually\naims to improve the class compactness3 of each category.\nAdaptive margin loss [40] also aims to improve the class\ncompactness by perturbing the logits. Fujii et al. [41] modified\nthe classical data augmentation method mixup [12] by con-\nsidering the “between-class distance”, which finally increases\nthe inter-class distance. In addition, some studies explore the\ncompiling of an optimal batch in the training process of deep\nlearning [42]. The ultimate goal is also the generalization.\nNevertheless, the direct goal of the batch compiling may\nconsist of balance, diversity, and others.\nAs previously stated, fairness is also an important learning\ngoal in many deep learning tasks. To combat unfairness on\nsamples with certain attributes, techniques such as data aug-\nmentation [43], perturbation [44], and sample weighting [45]\nhave been used in previous literature. Indeed, imbalanced\nlearning also pursues fairness among different categories. A\ncategory with a small prior probability, denoted as a minor\ncategory, will receive more attention in the employed data\noptimization. For example, larger weights [46], larger degrees\nof data perturbation [47], or more augmented quantities [48]\nare exerted on minor categories than others.\nAdversarial robustness is an essential goal in deep learning\ntasks that are quite sensitive to model safety. Adversarial train-\ning is usually leveraged to improve the adversarial robustness\nof a model. It can be attributed to a special type of data\naugmentation. Thus, adversarial training is actually a data\noptimization technique, which aims to improve the quality of\ntraining data such that the models trained on the optimized\ntraining data have better adversarial robustness.\nTrustworthiness is a goal that has recently been highly\nvalued. Explainability and calibration are two crucial re-\nquirements for the trustworthiness of a deep learning model.\nExplainability mainly relies on methodologies such as feature\nattribution and causal reasoning rather than pure data opti-\nmization technique. Nevertheless, data optimization is widely\nused in model calibration. Calibration mainly concerns the\ntrustworthiness of the predicted probability of a probabilistic\nmodel [49]. Liu et al. [50] introduced margin-aware label\nsmoothing to improve the calibration of trained models.\nMukhoti et al. [51] leveraged sample weighting to achieve\nbetter calibration.\nEfficiency is crucial for real applications as many learning\ntasks are sensitive to both time complexity and storage.\nTherefore, how to optimally reduce the redundant training\n3Some methods such as center loss also aim to increase the class compact-\nness. These methods are considered not data optimization.\nFig. 4. The sub-taxonomy for targeted application scenarios.\ndata and remain the diverse important training data deserves\nfurther investigation. The time complexity can be significantly\nreduced after data pruning.\nB. Application scenarios\nFig. fig4 describes the sub-taxonomy for the dimension of\ntargeted application scenarios. The first eight scenarios have\nbeen referred to in previous sections, so they are not further\nintroduced in this subsection.\nLearning under insufficient data contains the case that the\ntraining data are not as diverse as possible. Data diversity\naffects the model generalization [7]. Dunlap et al. [52] utilized\nlarge vision and language models to automatically generate\nvisually consistent yet significantly diversified training data.\nSome studies [53], [54] consider that data augmentation is\nactually a widely-used technique to increase data diversity.\nThese studies develop new data augmentation methods for\ndeep learning tasks.\nLarge models, such as large language models (LLMs),\nhave made remarkable advancements in nearly each AI field.\nThe data quality is crucial for the training or fine-tuning\nof a large model. Therefore, data optimization techniques\nalso prevail in learning for large models. Yang et al. [55]\nutilized flip operation on the training corpus to balance the\ntwo-way translation in language pairs in their building of a\nlarge multilingual translation model. Liu et al. [56] applied\nadversarial training in both the pre-training and fine-tuning\nstages. Results show that the error rate of the trained model\nis also reduced.\nFig. 5. The sub-taxonomy for data objects.\nC. Data objects\n1) Primary objects: The primary objects of data optimiza-\ntion for deep learning are the training data. Some studies\noptimize raw samples, while some others optimize labels.\nThere are also studies focusing on the data transformed by\nDNNs, e.g., features and logits. In Sections VI-B and VI-C,\nmore details will be presented.\n2) Other objects: There are also numerous studies con-\ncerning other data objects in deep learning. Fig. 5 lists three\nother data objects, namely, hyper-parameters, initial values,\nand meta data.\nHyper-parameters highly affect the final performance of\ntrained models. They are determined either by grid searching\n6\nin a pre-defined scope or directly being set as fixed values.\nConsequently, setting a proper searching scope or fixed initial\nvalues is a crucial step in DNN training.\nNetwork initialization is also important for DNN training.\nGaussian distribution-based initialization is the primary choice\nin most learning tasks. Other effective strategies are also inves-\ntigated and applied. Glorot and Bengio [57] adopted a scaled\nuniform distribution for initialization which is called “Xavier”\ninitialization. He et al. [58] proposed a robust initialization\nmethod for rectifier nonlinearities, which is called “Kaiming”\ninitialization.\nMeta learning offers a powerful manner to optimize hyper-\nparameters of independent modules in deep learning. It relies\non an unbiased meta dataset. Nevertheless, in most learning\ntasks there are no independent high-quality meta data and\nconstructing a high-quality unbiased meta dataset is challeng-\ning. Su et al. [59] conducted a theoretical analysis for the\ncompiling of a high-quality meta dataset from the training set.\nFour criteria, namely, balance, uncertainty, clean, and diversity,\nare selected in their proposed compiling method.\nThere are numerous classical studies for the optimization\nof the three types of data which are not mentioned in this\nstudy. The placing of these studies into data optimization for\ndeep learning may facilitate the further development of the\noptimization for the three types of data. The focus of this\nsurvey is the training data. Therefore, the following parts will\nbe limited in the scope of training data optimization.\nFig. 6. Three main steps in data optimization pipeline.\nV. OPTIMIZATION PIPELINE\nThe pipeline mainly consists of three steps, namely, data\nperception, analysis, and optimizing, as shown in Fig. 6.\nSome notations and symbols are defined as follows. Let\nD = {xi, yi}N\ni=1 be a set of N training samples, where xi\nis the feature and yi is the label. Let C be the number of\ncategories and Nc be the number of the samples in the cth\ncategory in D. πc = Nc/N is the proportion of the cth\ncategory. Let pc and p(x|y = c) be the prior and the class\nconditional probability density for the cth class, respectively.\nWhen there is no ambiguity, xi represents the feature output\nby the last feature encoding layer and ui represents the logit\nvector output by the Softmax layer for xi. Let l be the loss\nfor x. In this study, the cross-entropy loss is assumed and Θ\nrepresents the network parameters.\nA. Data perception\nIn this study, data perception refers to all possible methods\naimed at sensing and diagnosing the training data to capture\nFig. 7. The sub-taxonomy for data perception.\nthe intrinsic data characteristics and patterns that affect learn-\ning performance. It serves as the first step in the pipeline,\nand an effective data optimization method cannot work well\nwithout accurate perception of the training corpus\nGenerally, data perception for training data quantifies the\nfactors related to the true distribution, training data distribu-\ntion, cleanliness, diversity, etc. We construct a sub-taxonomy\nfor data perception in three dimensions as shown in Fig. 7.\nFirst, in terms of quantifying granularity, there are three\nlevels, namely, sample-wise, category-wise, and corpus-wise.\nSecondly, in terms of perception types, there are eight di-\nvisions, namely, distribution, cleanliness, difficulty, diversity,\nbalance, consistency, neighborhood, and valuation. Thirdly, in\nterms of quantifying variation, there are two divisions, namely,\nstatic and dynamics. Each of the above divisions and partial\nrepresentative studies are introduced as follows.\n1) Perception on different granularity levels: There are\nthree granularity levels, including sample-wise, category-wise,\nand corpus-wise.\nSample-wise data perception. It denotes that the perceived\nquantities reflect or influence a sample’s positive/negative or\ntrivial/important role in training. For example, most noisy-\nlabel learning methods employ sample-wise data perception,\ne.g., training loss [60] and gradient norm [61], to infer the\nnoisy degree of a training sample.\nCategory-wise data perception. It denotes that the per-\nceived quantities reflect or influence a category’s posi-\ntive/negative or trivial/important role in training. In category-\nwise perception, the learning performance of each cate-\ngory is usually monitored to return feedback for the entire\nscheme [62]. Therefore, the average outcome (e.g., average\n7\nloss or precision) is also used to infer a reasonable category-\nwise weight in the next training epoch [63], [64]. Another\npopular quantity is the category proportion (πc) for imbalanced\nlearning. Some studies [65] measure the compactness of a\ncategory as it reflects the generalization of the features for a\ncategory. Studies on/leveraging category-wise perception are\nfewer than sample-wise studies.\nCorpus-wise data perception. It denotes that the per-\nceived quantities reflect or influence a training corpus’ pos-\nitive/negative or trivial/important role in training. Limited\nstudies fall into this division. Lin et al. [66] used the query\nscore to measure the utility of a training dataset.\nThese three levels can be used together to more compre-\nhensively perceive the training data [67].\n2) Perception on different types: The eight quantifying\ntypes are introduced as follows:\n• Distribution. This type aims to quantify the true data\ndistribution for a learning task and the training data distri-\nbution. An effective quantification of these two distribu-\ntions is significantly beneficial for training. Nevertheless,\nit is nearly impossible to obtain a clear picture of them.\nTherefore, the true distribution is usually assumed to con-\nform to several some basic assumptions, such as Gaussian\ndistribution for each category [39]. For the training data\ndistribution, some studies [68], [69] apply clustering to\ndeduce the intrinsic structure of the training data. These\nstudies concern the global distribution of a category.\nRecently, researchers have investigated local distributions\nof training samples. One typical characteristic is about the\nneighborhood of each training sample. In deep learning\non graphs, the distribution of neighborhood samples with\nheterogeneous labels negatively impacts the training or\nprediction for the sample. Wang et al. [70] defined a label\ndifference index to quantify the difference between a node\nand its neighborhood in a graph as follows:\nLDI(xi) =\n1\n√\n2||pxi −pNi||2,\n(1)\nwhere pxi and pNi are the category distributions of xi\nand its neighborhood Ni.\n• Cleanliness. This type aims to identify the degree of noise\nin each training sample. This study primarily focuses on\nlabel noise, as it garners more attention than sample noise.\nThere are numerous metrics for noise measurement.\nAs illustrated in Fig. 7, typical measures include loss-\nbased, gradient-based, uncertainty-based, margin-based,\nand multi-training-based techniques. Samples with large\nlosses, gradient norms, or uncertainties are more likely\nto be noisy. In margin-based measures, a small margin\nindicates a high probability of being noise. Huang et\nal. [60] conducted multiple training procedures to identify\nnoisy labels.\n• Difficulty. This type aims to infer the degree of learning\ndifficulty for a training sample or a category. The ac-\ncurate measurement of learning difficulty for each train-\ning sample is of great importance because several deep\nlearning paradigms employ adaptive learning strategies\nbased on the level of learning difficulty. For instance,\ncurriculum learning [71] holds the perspective that easy\nsamples should receive more focus in the early train-\ning stages, while hard samples should be given more\nattention in the later stages of training. Some other stud-\nies [72] hold the opposite perspective that hard samples\nshould be prioritized throughout the training procedure.\nAs shown in Fig. 7, there are five major manners to\nmeasure learning difficulty of samples, namely, loss-\nbased, gradient-based, uncertainty-based, multi-training-\nbased, and distance-based. Obviously, the measures for\nlearning difficulty are quite similar to those for cleanli-\nness. In fact, some studies consider that noisy samples\nare those quite difficult to learn and divide samples into\neasy/medium/hard/noisy. Paul et al. [73] proposed the\nerror l2-norm score to measure difficulty. Zhu et al. [74]\nestablished a formal definition for learning difficulty of\nsamples inspired by the bias-variance trade-off theorem\nand proposed a new learning difficulty measures. Sorscher\net al. [75] defined the cosine distance of a sample to its\nnearest cluster center as the sample’s difficulty measure\nand applied it in sample selection.\n• Uncertainty. This type contains two sub-types, namely,\naleatory uncertainty and epistemic uncertainty [76]. The\nformer is also called data uncertainty and occurs when\ntraining samples are imperfect, e.g., noisy. Therefore,\nthe cleanliness degree can be used as a measure of\ndata uncertainty [77]. Epistemic uncertainty is also called\nmodel uncertainty. It appears when the learning strategy\nis imperfect. Model uncertainty can be calculated based\non information entropy of the DNN prediction or the\nvariance of multiple predictions output by a DNN with\nthe dropout trick [78].\n• Diversity. This type aims to identify the diversity of\na subset of training samples. The subset is usually a\ncategory. The measurement for subset diversity is useful\nin the design of data augmentation strategy for the sub-\nset [79] and data selection [59]. Friedman and Dieng [80]\nleveraged the exponential of the Shannon entropy of\nthe eigenvalues of a similarity matrix, namely, vendi\nscore to measure diversity. Salimans et al. [81] utilized\na pre-trained Inception model to measure diversity called\ninception score.\n• Balance. This type aims to measure the balance be-\ntween/within categories. The balance between categories\nbelongs to global balance, while that within a category\nbelongs to local balance. Global balance can be simply\nmeasured by the proportion of the training sample of a\ncategory. Nevertheless, our previous study [82] reveals\nthat other factors such as variance and distance may also\nresult in serious imbalance. Local balance is relatively\ndifficult to measure. Some studies define local balance as\nattribute balance [68].\n• Consistency. This type aims to identify the consistency\nof the training dynamics of a training sample along the\ntemporal or spatial dimensions. In the temporal dimen-\nsion, the variations of the training dynamics between\nthe previous and the current epochs are recorded [83].\nIn the spatial dimension, the differences in the training\n8\ndynamics between a sample and other samples such as\nneighbors [84] or samples within the same category are\nrecorded. A classical measure called “forgetting” [85]\nquantifies the number of variations in the prediction\nbetween adjacent epochs. Singh et al. [86] investigated\nclass-wise forgetting. Maini et al. [87] utilized forgetting\nto distinguish among examples that are hard for distinct\nreasons, such as membership in a rare subpopulation, be-\ning mislabeled, or belonging to a complex subpopulation.\nWang et al. [88] provided a comprehensive summary for\nsample forgetting in learning. Kim et al. [89] focused on\nthe dynamics of each sample’s latent representation and\nmeasured the alignment between the latent distributions.\n• Valuation. This value is usually measured by the Shapley\nvalue, which is a concept from the game theory [90].\nGhorbani and Zou firstly introduced Shapley value for\ndata valuation [91] as follows:\nϕ(xi) =\nX\nS∈D−xi\n1\nC|S|\n|D|−1\n[V (S ∪{xi}) −V (S)]\n(2)\nwhere V (·) is the utility function of a dataset and S\nis a subset of the training corpus D. Their values are\ndifferent when modeling the clean and the noisy sam-\nples. Nevertheless, the calculation for the Shapley value\nas shown in Eq. (2) is NP-hard, thereby hindering its\nuse in real applications. Yoon et al. [92] proposed a\nreinforcement learning-based method for data valuation.\nTheir inferred weights reflect the importance of a sample\nin learning, which is not equal to the Shapley value.\nSome other studies [66], [93] proposed more practical\nmethods to approximate the Shapley value. Jiang et\nal. [94] established an easy-to-use and unified framework\nthat facilitates researchers and practitioners to apply and\ncompare existing data valuation algorithms. Compared\nwith the aforementioned perception quantities such as\ncleanliness and difficulty, the Shapley value has a more\nsolid theoretical basis. Therefore, establishing a direct\nconnection among the Shapley value and the quantities\nlisted above deserves further study.\nThis study only lists commonly used measures for data\nperception. Additionally, there are some other important mea-\nsures which will be explored in our future work. For example,\nthe neighborhoods for each training sample may vary as the\nfeature encoding network is updated at each epoch. In shallow\nlearning, neighborhood is an important information and many\nclassical methods are based on the utilization of neighbor-\nhood. However, previous deep learning methodologies rarely\nleverage neighborhood information, as the computational com-\nplexity for neighborhood identification is high. Some studies\nadopt a simplified manner to construct the neighborhood. For\nexample, Bahri and Heinrich Jiang [95] employed the logit\nvectors rather than the features to construct neighborhood. The\ndimension of logit vector is usually much smaller than the\nfeature dimension, so the computational complexity is signif-\nicantly reduced. It is believable that with the advancement of\nrelated computational techniques, neighborhood information\nwill receive increasingly attention in deep learning. Some other\nFig. 8. The statistics for the forgetting numbers for training samples [85].\nimportant quantities such as problematic score [96] and data\ninfluence [97] in learning, which have large overlaps with the\naforementioned quantities, also deserve further exploration.\nIf the perceived quantities are required to fed into a model,\nhidden representation for the raw quantities is favored. In\nmeta learning-based sample weighting or perturbation [98], the\nweights or perturbation vectors for each sample are derived\nbased on the hidden representations of the the perceived\nquantities. For example, Shu et al. [99] extracted the training\nloss for each sample as the input and fed it into an MLP\nnetwork containing 100 hidden nodes. In other words, the\nraw training loss is represented by a 100-dimensional hidden\nvector. Zhou et al. [67] utilized six quantities to perceive the\ncharacter of a training sample, including loss, margin, gradient\nnorm, entropy of the Sofxmax prediction, class proportion, and\naverage categorical loss. Likewise, these six quantities are also\ntransformed into a 100-dimensional feature vector through an\nMLP network.\n3) Static and dynamic perception: Static perception de-\nnotes that the perceived quantities remain unchanged during\noptimization, whereas dynamic perception denotes that the\nquantities vary.\nIn imbalanced learning, category proportion is widely used\nto quantify a category. It belongs to static perception because\nthis quantity remains unchanged. In noisy-label learning, many\nstudies adopt a two-stage strategy in which the noisy degree\nof each training sample is measured and the degrees are used\nin the second training stage [60]. In this two-stage strategy,\nthe perception for label noise is static.\nThe impact of a training sample usually varies during\ntraining. Therefore, compared with static perception, dynamic\nperception is more prevailing in deep learning tasks. Many\nstudies utilize training dynamics of training samples for the\nsuccessive sample weighting or perturbation. Such training\ndynamics also belong to the dynamic perception. The training\ndynamics including loss, prediction, uncertainty, margin, and\nneighborhood vary at each epoch. For example, self-paced\nlearning [100] determines the weights of each training sample\naccording to their losses in the previous epoch and a varied\nthreshold. Therefore, the weight may also vary in each epoch.\nB. Analysis on perceived quantities\nAnalysis on perceived quantities contains two manners,\nnamely, statistics and modeling, as shown in Fig. 6.\nStatistical analysis. Most studies employ this manner for\nthe perceived data quantities. These studies considered only\none or two quantities. For example, Toneva et al. [85] made\na statistics for the forgetting numbers of training samples\n9\nFig. 9. The statistics for loss along the training cycle rounds [60].\nFig. 10. The statistics for the mean and the variance of the loss under the\ncross validation-based training. [74].\nas shown in Fig. 8. The left figure shows the distributions\nof forgetting numbers for clean and noisy samples, while\nthe right one shows the distributions of forgetting numbers\nbefore and after the noise is added. Distinct difference exists\nbetween the distributions of clean and noisy samples. Huang\net al. [60] proposed a cycle training strategy that the model is\ntrained from overfitting to underfitting cyclically. The epoch-\nwise loss for each training sample is recorded. Fig. 9 shows the\ndifferences between the average losses for the noisy and clean\nsamples. Noisy samples have larger training losses. Therefore,\nthey leveraged the average loss as an indicator for noisy labels.\nZhu et al. [74] proposed a cross validation-based training\nstrategy. Multiple training losses are also recorded for each\ntraining sample. They revealed that the variance of the multiple\nlosses for each sample is also useful in identifying noisy labels\nas shown in Fig. 10.\nModeling. This manner refers to the statistical modeling on\nthe perceived quantities for training data. Arazo et al. [101]\nassumed that the traning loss conforms to the following\ndistribution:\np(l|α, β) = Γ(α + β)\nΓ(α)Γ(β)lα−1(1 −l)β−1.\n(3)\nwhere Γ(·) is the Gamma function; α and β are parameters to\ninfer. Their values are different when modeling the clean and\nthe noisy samples. Hu et al. [102] leveraged the Weibull mix-\nture distribution to model the memorization-forgetting value\nof each training sample. The mixture distribution contains\ntwo components which suit for the clean and noisy samples,\nrespectively.\nBoth manners are transparent and thus the entire data\noptimization approach is explainable. Nevertheless, these two\ndivisions usually rely on appropriate prior distributions about\nthe involved quantities. If the prior distributions are incorrect,\nthe successive optimizing will negatively influence the model\ntraining.\nC. Optimizing\nThe data perception and analysis act as the pre-processing\nfor data operation. This step is the key processing of the\nentire data optimization pipeline. The successive section will\nintroduce current optimization techniques in detail.\nVI. DATA OPTIMIZATION TECHNIQUES\nThis section describes the most important dimension for the\npresented taxonomy, namely, data optimization techniques for\ndeep learning. Fig. 11 presents the sub-taxonomy along this\ndimension. We summarized six sub-divisions for existing data\noptimization techniques, including resampling, augmentation,\nperturbation, weighting, pruning, and others. It is noteworthy\nthat this survey covers numerous technique/methodology divi-\nsions and leaves a through comparison for them as our future\nwork. The reason lies in two folds. First, each division has\nits own merits and defects and their effectiveness have been\nverified in previous literature, so it is difficulty to judge which\none is absolutely the best in universal learning tasks. Second,\na thorough theoretical or empirical comparison is not a trivial\ntask.\nA. Data resampling\nData resampling compiles a new training set in which\nsamples are randomly sampled from the raw training set. It is\nwidely used in tasks encountering the issues, including biased\ndistribution [103] and redundancy. This study summarizes two\nsplit dimensions for this division. The first dimension concerns\nthe size of the sampled datasets, while the second dimension\nconcerns the sampling rates.\nIn the first dimension, resampling is divided into under-\nsampling and oversampling. undersampling compiles a new\ntraining set whose size is smaller than that of the raw training\nset. Contrarily, oversampling compiles a new training set\nwhose size is larger than that of the raw training set. Both\nmanners are widely used in previous machine learning tasks,\nincluding imbalanced learning, bagging, and cost-sensitive\nlearning. Meanwhile, tremendous theoretical studies have been\nconducted to explain the effectiveness of these two manners\nin both the statistics and the machine learning communities.\nNevertheless, there is currently no consensus on which manner\nis more effective. Some studies concluded that undersampling\nshould be the primary choice when dealing with imbalanced\ndatasets [104]. However, some other studies hold the opposite\nview [105].\nIn the second dimension, resampling is divided into uni-\nform, proportion-based, importance-based, learning difficulty-\nbased, and uncertainty-based. They are detailed as follows:\n• Uniform sampling. This manner is quite intuitive. It treats\nsamples definitely equal regardless of their distributions,\n10\nFig. 11. The sub-taxonomy of data optimization techniques.\n11\nlocation, categories, and training performances. In fact, in\nnearly all existing deep learning tasks, the batch is con-\nstructed by uniformly sampling from the training corpus.\nKirsch et al. [106] claimed that the independent selection\nof a batch of samples leads to data inefficiency due to\nthe correlations between samples. Some studies explore\nalternative sampling strategies. For example, Loshchilov\nand Hutter [42] proposed a rank-based batch selection\nstrategy according to the training loss in previous epochs\nand samples with large losses have high probabilities to\nbe sampled. Experiments reveal that this new strategy\naccelerates the training speed by a factor of five.\n• Proportion-based sampling. This manner simply assigns\nthe total sampling rate for each category with its propor-\ntion (πc) in the corpus. It is mainly used in imbalanced\nlearning in which the minor categories are assigned with\nlarge sampling rates [15].\n• Importance-based sampling. This manner assigns sam-\npling probabilities according to samples’ importance. In\nthis study, the definition for importance sampling fol-\nlows several classical studies [107] [108]. Given a target\ndistribution q(x, y) and a source distribution on training\ndata p(x, y), the importance (sampling rate) for a training\nsample {x, y} in importance sampling is defined as\nw(x) = q(x, y)\np(x, y).\n(4)\nAs the target distribution is unknown, some studies [109]\nutilize the kernel trick to generate sampling rates. In some\nimportance sampling studies, the sampling rates are not\nbased on the probability density ration as presented in\nEq. (4). For example, Atharopoulos and Fleuret [110]\ntook the gradient norms of each training sample as their\nimportance. These methods actually belong to learning\ndifficulty-based sampling.\n• Learning difficulty-based sampling. This manner assigns\nsampling rates according to samples’ learning difficulties.\nAs summarized in Section V-A2, learning difficulty is\nusually measured by loss and gradient norm. For instance,\nLi et al. [61] applied the gradient norm of logit vectors as\nthe difficulty measurement. Similar with the study con-\nducted by Atharopoulos and Fleuret [110], Johnson and\nGuestrin [111] proposed the O-SGD sampling method\nwith the following sampling rate:\nw(x, y) =\n||∇l(x, y)||\nP\nx ||∇l(x, y)||.\n(5)\nThey claimed that this “importance sampling” can reduce\nthe stochastic gradient’s variance and thus accelerate the\ntraining speed. Jiang et al. [112] introduced a selective\nback propagation strategy, in which samples with large\nlosses have relative large probabilities to be selected in\nback propagation. Gui et al. [113] utilized sampling strat-\negy for noisy-label learning. They calculated the sampling\nweights based on the mean loss of each example along the\ntraining process. The training samples with large mean\nlosses are assigned low weights. Liu et al. [114] proposed\nadaptive data sampling, in which the sampling rates of\nthe samples correctly classified in previous epochs are\nreduced. Xu et al. [115] conducted a theoretical analysis\nand concluded that inverse margin-based sampling may\naccelerate gradient descent in finite-step optimization by\nmatching weights with the inverse margin.\n• Uncertainty-based sampling. This manner assigns sam-\npling rates according to samples’ uncertainties. It is\nwidely used in active learning, in which a subset of data\nis sampled for human labeling [116], [117]. Aljuhani\net al. [118] presented an uncertainty-aware sampling\nframework for robust histopathology image analysis. The\nuncertainty is calculated by predictive entropy.\nThere are also some other sampling manners. For instance,\nTing and Brochu [119] calculated the sample influence for\noptimal data sampling. Li and Vasconcelos [120] proposed the\nadversarial sampling to improve OOD detection performance\nof an image classifier. In their adversarial sampling, the sam-\npling weights are pursued by maximizing the OOD loss. Wang\nand Wang [121] sampled sentences according to their semantic\ncharacteristics. Zhang et al. [122] sampled training data of the\nmajority categories by considering the samples’ sensitivities.\nSamples with low sensitivities may be noisy or safe ones,\nwhile those with high sensitivities are borderline ones. Sun\net al. [123] explored an automatic scheme for effective data\nresampling.\nB. Data augmentation\nData augmentation compiles a new training set in which\nsamples (or features) are generated based on the raw training\nset or sometimes other relevant sets. It is a powerful tool\nto improve the generalization capability [124], [125] and\neven adversarial robustness [126], [127] of DNNs. Illuminated\nby related surveys on data augmentation [128]–[131], two\nsplit dimensions are considered, namely, sample/feature and\nexplicit/implicit, as shown in Fig. 11.\n1) Sample/feature augmentation: In sample augmentation,\nthe new training set consists of generated new samples, while\nin feature augmentation, the new training set consists of\ngenerated new features.\nSample augmentation. This division is subject to data\ntypes (e.g., image, text, or others). For image corpus, aug-\nmentation methods adopt noise adding, color transformation,\ngeometric transformation, or other basic operations such as\ncropping to augment new images [129]. For texts, new samples\ncan be generated by noise adding, paraphrasing, or other basic\noperations such as word swapping [132].\nFeature augmentation. This division is performed on the\nfeature space, so learning tasks for different data types may\nutilize the same or similar augmentation strategies. Some\nintuitive feature augmentation methods include adding noise,\ninterpolating, or extrapolating [133], which are applicable for\ngeneral data types, including both image and text data. Li et\nal. [134] revealed that the simply perturbing the feature em-\nbedding with Gaussian noise in training leads to comparable\ndomain-generalization performance compared with the SOTA\nmethods. Ye et al. [135] proposed novel domain-agnostic\naugmentation strategies on feature space. Cui et al. [136]\n12\ndecomposed features into the class-generic and the class-\nspecific components. They generated samples by combining\nthese two components for minor categories. A classical robust\nlearning paradigm, namely, adversarial training, is actually\na feature-wise augmentation strategy when it is run on the\nfeature space [137]–[139].\nSome studies augment other data targets such as label and\ngradient. For example, Lee et al. [140] rotated a training image\nand the rotation angle is also used as supervised information.\nElezi et al. [141] proposed a transductive label augmentation\nmethod to generate labels for unlabeled large set using graph\ntransduction techniques. Some other studies [142] investigated\ngradient augmentation. Compared with sample/feature aug-\nmentation, label/gradient augmentation receives quite limited\nattention.\n2) Explicit/implicit augmentation: Explicit augmentation\ndirectly generates new samples/features. Meanwhile, implicit\naugmentation conducts data augmentation only theoretically\nyet do not generate any new samples/features actually.\nExplicit augmentation. According to the employed tech-\nniques, existing explicit data augmentation can be divided into\nbasic operation, model-based (GAN, diffusion model), loss-\noptimization-based, and automatic augmentation, as described\nin Fig. 11. They are introduced as follows:\n• Basic operations. This technique is widely used in practi-\ncal learning tasks as basic operations conform to human\nintuitions. The popular deep learning platforms such as\npyTorch provide several common basic operations such as\ncropping, rotation, replacement, masking, cutout, etc. One\nof the most popular data augmentation method used for\nshallow learning tasks, namely, SMOTE [143] has been\nutilized in deep learning tasks [144]. Dablain et al. [145]\ndesigned more sophisticated improvement for SMOTE for\ndeep learning tasks. Among the basic operations, mixup\nis a simple yet quite effective augmentation manner [12],\n[146]. It generates a new sample with a new label that\ndoes not belong to the raw label space.\n• Model-based augmentation. This technique generates new\nsamples by leveraging independent models. There are\nthree main schemes:\n①GAN-based\nscheme.\nGenerative\nadversarial\nnet-\nwork (GAN) trains a generative model and a discrim-\ninative model simultaneously in a well designed two-\nplayer min-max game [147]. The trained generative\nmodel can be used to generate new samples conforming\nto the distribution of the involved training data. A\nlarge number of variations have been designed in the\nprevious literature [148]. Mariani et al. [149] proposed\nbalancing GAN for imbalanced learning tasks. Huang\net al. [150] developed AugGAN for the data augmen-\ntation in cross domain adaptation. Yang et al. [151]\ninvestigated the GAN-based augmentation for time\nseries.\n②Diffusion model-based scheme. Diffusion models are a\nnew class of generative models and achieve SOTA per-\nformance in many applications [152]. Xiao et al. [153]\nleveraged a text-to-image stable diffusion model to\nexpand the training set. Dunlap et al. [52] utilized large\nvision and language models to automatically generate\nnatural language descriptions of a dataset’s domains\nand augment the training data via language-guided\nimage editing.\n③Bi-transformation-based scheme. This scheme usually\nrelies on two transformation models. The first model\ntransforms a training sample into a new type of data.\nThe second model transforms the new type of data into\na new sample. In natural language processing (NLP),\nback-translation is a popular data augmentation tech-\nnique [154], which translates the raw text sample into\nnew texts in another language and back translates the\nnew texts into a new sample in the same language with\nthe raw sample. Dong et al. [155] proposed a new aug-\nmentation technique called Image-Text-Image (I2T2I)\nwhich integrates text-to-image and image-to-text (im-\nage captioning) models. There are also augmenta-\ntion attempts about Text-Image-Text (T2I2T) [156]\nand Text-Text-Image (T2T2I) [157]. Theoretically, tri-\ntransformation-based augmentation may be also appli-\ncable. We leave it our future work.\n• Loss-optimization-based augmentation. This manner gen-\nerates new sample/features by minimizing or maximizing\na defined loss with heuristic or theoretical inspirations.\nAdversarial training is a typical loss-optimization-based\nmanner. It generates a new sample for x by solving the\nfollowing optimization problem:\nxadv = x + arg max\n∥δ∥≤ϵ ℓ(f(x + δ), y),\n(6)\nwhere δ and ϵ are the perturbation term and bound,\nrespectively. Zhou et al. [67] proposed anti-adversaries\nby solving the following optimization problem:\nxanti-adv = x + arg min\n∥δ∥≤ϵ ℓ(f(x + δ), y).\n(7)\nPagliardini et al. [158] obtained new samples by maxi-\nmizing an uncertainty-based loss.\n• Automatic augmentation. This manner investigates auto-\nmated data augmentation techniques [159] based on meta\nlearning [160] or reinforcement learning [161]. Nishi\net al. [162] proposed new automated data augmenta-\ntion method and validated its usefulness in noisy-label\nlearning. Some studies focus on differentiable automatic\ndata augmentation which can dramatically reduces the\ncomputational complexity of existing methods [163].\nImplicit augmentation. Wang et al. [39] proposed the\nfirst implicit augmentation method called ISDA. It establishes\na Gaussian distribution N(µy, Σy) for each category. New\nsamples can be generated (i.e., sampled) from its correspond-\ning Gaussian distribution. An upper bound of the loss with\naugmented samples can then be derived when the number\nof generated samples for each training sample approaches to\ninfinity. Finally, the upper bound of the loss is used for the final\ntraining loss. There are several variations for ISDA, such as\nIRDA [164] and ICDA [165]. Li et al. [166] also proposed an\nimplicit data augmentation approach mainly based on heuristic\ninspirations.\n13\nExplicit augmentation is the primary choice in data aug-\nmentation tasks. Nevertheless, implicit augmentation is more\nefficient than explicit augmentation as it does not actually\ngenerate new samples or features. There are also theoretical\nstudies about data augmentation. A mainstream perspective\nis that data augmentation performs regularization in train-\ning [167]–[170]. Chen et al. [171] conducted a probabilistic\nanalysis and concluded that data augmentation can result in\nvariance reduction and thus prevent overfitting according to\nthe bias-variance theory. In fact, the regularizer in machine\nlearning also reduces model variance.\nC. Data perturbation\nGiven a datum x (x can be the raw sample, feature, logit,\nlabel, or others), data perturbation will generate a perturbation\n△x such that x′ = x + △x can replace x or be used as a new\ndatum. Therefore, some data augmentation methods, such as\nadversarial perturbation, cropping, and masking, can also be\nviewed as data perturbation. In our previous work [13], we\nconstructed a taxonomy for compensation learning, which is\nactually learning with perturbation. This study follows our\nprevious taxonomy in [13] with slight improvements. The\nsub-taxonomy for data perturbation is presented in Fig. 11.\nFour split dimensions are considered, namely, target, direction,\ngranularity, and assignment manner.\n1) Perturbation target: The perturbation targets can be raw\nsample, feature, logit vector, label, and gradient.\n• Sample perturbation. This division adds the perturbation\ndirectly to the raw samples. The basic operations in\ndata augmentation can be placed into this division. For\ninstance, noise addition and masking used in image\nclassification actually exert a small perturbation on the\nraw image.\n• Feature perturbation. This division adds the perturbation\non the hidden features. Jeddi et al. [172] perturbed the\nfeature space at each layer to increase uncertainty in the\nnetwork. Their perturbation conforms to the Gaussian\ndistribution. Shu et al. [173] designed a single network\nlayer that can generate worst-case feature perturbations\nduring training to improve the robustness of DNNs.\n• Logit perturbation. This division adds the perturbation\non the logit vectors in the involved DNNs. Li et al. [47]\nanalyzed several classical learning methods such as logit\nadjustment [174], LDAM [175], and ISDA [39] in a\nunified logit perturbation viewpoint. They proposed a new\nlogit perturbation method and extended it to the multi-\nlabel learning tasks [176].\n• Label perturbation. This division adds the perturbation on\neither the ground-truth label of the predicted label. One\nclassical learning skill, namely, label smoothing [177],\nis a kind of label perturbation method. Let C be the\nnumber of categories and λ be a hyper-parameter. Label\nsmoothing perturbs the label y (one-hot type) with the\nfollowing perturbation △y = λ( I\nC −y), where I is\na C-dimensional vector and its each element is 1. A\nlarge number of variations have been proposed for label\nsmoothing [62], [178], [179].\n• Gradient perturbation. This division adds the perturbation\ndirectly on gradient. Studies on gradient perturbation are\nfew. Orvieto et al. [180] proposed a gradient perturbation\nmethod and verified its effectiveness both theoretically\nand empirically.\nThere are also studies [181] which perturb other data such\nas network weights in training, which is not the focus of\nthis study. Wang et al. [182] proposed a reward perturbation\nmethod for noisy reinforcement learning.\n2) Perturbation direction: Data perturbation will either\nincrease or decrease the loss values of training samples in\nthe learning process. Based on whether the loss increases or\ndecreases, existing methods can be categorized as positive or\nnegative augmentations.\nPositive perturbation. It increases the training losses of\nperturbed training samples. Obviously, adversarial perturbation\nbelongs to positive perturbation, as it maximizes the training\nloss with the adversarial perturbations. ISDA [39] also belongs\nto positive perturbation as it adds positive real numbers to the\ndenominator of the Softmax function.\nNegative perturbation. It reduces the training losses. Anti-\nadversarial perturbation [67] belongs to negative perturbation,\nas it minimizes the training loss with the adversarial pertur-\nbations. Bootstrapping [183] is a typical robust loss based on\nlabel perturbation. It also belongs to negative perturbation as\nits perturbation is △y = λ(p −y), where p is the prediction\nof the current trained model.\nSome methods increase the losses of some samples and\ndecrease those of others simultaneously. For instance, the\nlosses of noisy-label training samples may be reduced, while\nthose of clean samples may be increased in label smoothing. Li\net al. [47] proposed a conjecture for the relationship between\nloss increment/decrement and data augmentation.\n3) Perturbation granularity:\nAccording to perturbation\ngranularity, existing methods can be divided into sample-wise,\nclass-wise, and corpus-wise.\n• Sample-wise perturbation. In this division, each training\nsample has its own perturbation and different samples\nusually have distinct perturbations. The aforementioned\nBootstrapping and adversarial perturbation all belong to\nthis division. The random cropping and masking also\nbelong to this division.\n• Class-wise perturbation. In this division, all the training\nsamples in a category share the same perturbation, and\ndifferent categories usually have distinct perturbations.\nBenz et al. [184] proposed a class-wise adversarial per-\nturbation method. Wang et al. [185] introduced class-wise\nlogit perturbation for the training in semantic segmenta-\ntion. Label smoothing also belongs to this division.\n• Corpus-wise perturbation. In this division, all the training\nsamples in the training corpus share only one perturba-\ntion. Shafahi et al. [186] pursued the universal adversar-\nial perturbation for all the training samples, which has\nproven to be effective in various applications [187]. Wu\net al. [188] proposed a corpus-wise logit perturbation\nmethod for multi-label learning tasks.\n4) Assignment manner: The perturbation variables should\nbe assigned before or during training. As presented in Fig. 11,\n14\nthere are four typical assignment manners to determine the\nperturbations.\nRule-based assignment. In this manner, the perturbation is\nassigned according to pre-fixed rules. These rules are usually\nbased on prior knowledge or statistical inspirations. In both\nlabel smoothing and Booststrapping loss, the label perturbation\nis determined according to manually defined formulas. In text\nclassification, word replacement and random masking also\nobey rules.\nRegularization-based assignment. In this manner, a regu-\nlarizer for the perturbation is usually added in the total loss.\nTake the logit perturbation as an example. A loss function with\nregularization for logit perturbation can be defined as follows:\nL =\nX\ni\nl(S(vi + △vi), yi) + λReg(△vi).\n(8)\nwhere S is the Softmax function, vi is the logit vector for\nxi, △vi is the perturbation vector for vi, and Reg(·) is the\nregularizer. Zhou et al. [189] introduced a novel perturbation\nway for adversarial examples by leveraging smoothing regular-\nization on adversarial perturbations. Wei et al. [190] took the\nnotation that adversarial perturbations are temporally sparse\nfor videos and then proposed a sparse-regularized adversarial\nperturbation method. Zhu et al. [191] proposed a Bayesian\nneural network with non-zero mean of Gaussian noise. The\nmean is actually a feature perturbation and inferred with l2\nregularization.\nLoss-optimization-based assignment. This division is sim-\nilar to the loss-optimization-based augmentation introduced\nin Section VI-B2. A new loss containing the perturbations\nis defined and the perturbation is pursued by optimizing the\nloss. In the optimization procedure, only the perturbations are\nthe variables to be optimized, while the model parameters are\nfixed.\nLearning-based assignment. In this manner, the perturba-\ntion is assigned by leveraging a learning method. Three learn-\ning paradigms are usually applied, including self-supervised\nlearning, meta learning, and reinforcement learning.\n• Self-supervised learning. This paradigm leverages self-\nsupervised learning methodologies such as contrastive\nlearning [192] to pursue the perturbations. Naseer et\nal. [193] constructed a self-supervised perturbation frame-\nwork to optimize the feature distortion for a training im-\nage. Zhang et al. [194] proposed a generative adversarial\nnetwork-based self-supervised method to generate EEG\nsignals.\n• Meta learning. This paradigm leverages meta-learning\nmethodologies to pursue the perturbations using an ad-\nditional meta dataset. It assumes that the perturbation\n△x (or △y) for a training sample x (or its label y) is\ndetermined by the representation of x or factors such as\ntraining dynamics for x, which is described as follows:\n△x = g(x, η(x)),\n(9)\nwhere g(·) can be a black-box neural network such as\nMLP; η(x) represents the training dynamics for x. Li et\nal. [195] applied meta learning to directly optimize the\ncovariant matrix used in ISDA, which is used to calculate\nthe logit perturbation. Qiao and Peng [196] utilized the\nmeta learning to learn an independent DNN for both\nfeatures and label perturbation.\n• Reinforcement learning. This paradigm leverages rein-\nforcement learning to pursue the perturbations without\nrelying on additional data. Many data augmentation meth-\nods [197], [198], which also belong to data perturbation,\nare based on reinforcement learning. Giovanni et al. [199]\nleveraged deep reinforcement learning to automatically\ngenerate realistic attack samples that can evade detection\nand train producing hardened models. Lin et al. [200]\nformulated the perturbation generation as a Markov deci-\nsion process and optimized it by reinforcement learning\nto generate perturbed instructions sequentially.\nGiven a learning task, it is difficulty to directly judge which\nassignment manner is the most appropriate without a thorough\nand comprehensive understanding for the task. Each assign-\nment manner has its own merits and defects.\nD. Data weighting\nData weighting assigns a weight for each training sample\nin loss calculation. It is among the most popular data op-\ntimization techniques in many learning scenarios, including\nfraud detection [201], portfolio selection [202], medical di-\nagnosis [203], and fairness-aware learning [45], [204]. Three\ndividing dimensions are considered, namely, granularity, de-\npendent factor, and assignment manner for weights.\n1) Weighting granularity: According to the granularity of\nweights, existing weighting methods can be divided into\nsample-wise and category-wise. Noisy-label learning usually\nadopts sample-wise weighting methods [205], [206], while\nimbalanced learning usually adopts category-wise ones [46],\n[207], [208]. Data weighting is also widely used in standard\nlearning [209], [210], which are usually sample-wise.\n2) Dependent factor: Dependent factor in this study de-\nnotes the factors that are leveraged to calculate the sample\nweights. Similar with the resampling introduced in Section\nVI-A, three factor types are usually considered, namely, cate-\ngory proportion, importance, and learning difficulty. As these\nconcepts are introduced in Section VI-A and quite similar pro-\ncedures are adopted, these factors are not detailed in this part.\nThere are an increasing number of studies employing learning\ndifficulty-based weighting. They can be further summarized\naccording to which samples are learned first.\nAs samples with larger weights than others can be consid-\nered as having priority in training, learning difficulty-based\nweighting contains three basic folds, namely, easy-first, hard-\nfirst, and complicated.\n• Easy-first. Easy samples are given higher weights than\nhard ones in this fold. There are a huge number of easy-\nfirst weighting methods, which mainly belong to two\nparadigms: curriculum learning [71] and self-paced learn-\ning [100]. These two paradigms assign larger weights to\neasy samples during the early training stage and gradually\nincrease the weights of hard samples. Numerous studies\nhave been conducted on the design of the weighting\nformulas [211]–[213]. Easy-first weighting is usually\n15\nused in noisy-label learning. Extensive experiments on\ncurriculum learning indicate that it mainly takes effects\non noisy-label learning tasks [214].\n• Hard-first. Hard samples have higher weights than easy\nones in this fold. Focal loss is a typical hard-first strat-\negy [72]. Zhang et al. [210] also assigned large weights\non hard samples. Santiagoa et al. [215] utilized the\ngradient norm to measure learning difficulty and exerted\nlarge weights on samples with large gradient norms.\n• Complicated. In some weighting methods, the easy-first\nor the hard-first is combined with other weighting inspi-\nrations. In Balanced CL [216](should be replaced), on\nthe basis of the easy-first mode, the selection of samples\nhas to be balanced under certain constraints to ensure\ndiversity across image regions or categories. Therefore,\nBalanced CL adopts the complicated mode.\nBesides the three general ways, Zhou et al. [217] revealed\nsome other priority types including both-ends-first and varied\nmanners during training. There also other dependent factors\nsuch as misclassified cost and those reflecting other concerns\nsuch as fairness and confidence [218], [219].\n3) Assignment manner: Generally, there are four manners\nto assign weights for training samples as shown in Fig. 11.\nRule-based assignment. This manner determines the sam-\nple weights according to theoretical or heuristic rules. For\nexample, many methods assume that the category proportion is\nthe prior probability. Consequently, the inverse of the category\nproportion is used as the weight based on the Bayesian rule.\nCui et al. [46] established a theoretical framework for weight\ncalculation based on the effective number theory in computa-\ntion geometry. The classical Focal loss [72] heuristic defines\nthe weight using w = (1 −p)γ, where p is the prediction\non the ground-truth label and γ is a hyper-parameter. Han et\nal. [220] defined an uncertainty-based weighting manner for\nthe two random samples in mixup. Importance weighting [221]\nis also placed in this division.\nRegularization-based assignment. This method defines a\nnew loss function which contains a weighted loss and a\nregularizer (Reg(W)) on the weights as follows:\nL = 1\nN\nN\nX\ni=1\nwil(f(xi), yi) + λReg(W),\n(10)\nwhere W = {w1, · · · , wN}T is the vector of sample weights.\nThe classical self-paced learning, which mimics the mecha-\nnism of human learning from easy to hard gradually, is actually\nthe regularization method defined as Reg(W) = −|W|1 (wi ∈\n{0, 1}) [100]. Fan et al. [222] presented a new group of self-\npaced regularizers deduced from robust loss functions and\nfurther analyzed the relation between the presented regularizer-\nbased optimization and half-quadratic optimization.\nAdversarial optimization-based assignment. This manner\npursues the sample weights by optimizing a defined objective\nfunction, which is similar to the pursing of the adversarial\nperturbation. For instance, Gu et al. [223] adversarially learned\nthe weights of source domain samples to align the source\nand target domain distributions by maximizing the Wasserstein\ndistance. Yi et al. [224] defined a maximal expected loss and\nobtained a simple and interpretable closed-form solution for\nsamples’ weights: larger weights should be given to augmented\nsamples with large loss values.\nLearning-based assignment. Similar with that in data\nperturbation, learning-based assignment also usually applies\nmeta learning or reinforcement learning to infer the sample\nweights. Ren et al. [225] firstly introduced meta learning\nfor sample weighting in imbalanced learning and noisy-label\nlearning. Shu et al. [99] utilized an MLP network to model the\nrelationship between samples’ characters and their weights,\nand then trained the network using meta learning. Zhao et\nal. [226] further proposed a probabilistic formulation for meta\nlearning-based weighting. Trung et al. [227] leveraged meta\nlearning to train a neural network-based self-paced learning for\nunsupervised domain adaption. Wei et al. [228] also combined\nmeta learning and self-paced network to automatically gener-\nate a weighting scheme from data for cross-modal matching.\nLi et al. [229] proposed meta learning-based weighting for\npseudo-labeled target samples in unsupervised domain adap-\ntation. Meta learning requires additional meta data, whereas\nreinforcement learning does not require additional data. Zhou\net al. [230] leveraged an augmentation policy network which\ntakes a transformation and the corresponding augmented im-\nage as inputs to generate the loss weight of an augmented. Ge\net al. [231] used a delicately designed controller network to\ngenerate sample weights and combined the weights with the\nloss of each input data to train a recommendation system. The\ncontroller network is optimized by reinforcement learning.\nWeights assignment can also be divided into static and dy-\nnamic. There are a few methods adopting static weighting [46],\nwhereas most methods adopting dynamic. Fang et al. [232]\nproposed dynamic importance weighting to train the models.\nE. Data pruning\nData pruning is contrary to data augmentation. In this study,\nit is divided into dataset distillation and subset selection.\n1) Dataset distillation: Dataset distillation is firstly pro-\nposed by Wang et al. [233] and it aims to synthesize a\nsmall typical training set from substantial data [234]. The\nsynthesized dataset replaces the given dataset for efficient\nand accurate data-usage for the learning task. Following the\ndivision established by Sachdeva and McAuley [235], existing\ndata distillation methods can be placed in four folds.\nMeta-model matching-based strategy. This strategy is\nfirstly proposed by Wang et al. [233]. It performs an inner-\nloop optimization for a temporal optimal model based on the\nsynthesized set and an outer-loop optimization for a temporal\nsubset (i.e., the synthesized set) by turns. Some recent studies\ndiscussed its drawbacks such as the ineffectiveness of the\nTBPTT-based optimization [236] and proposed new solutions\nsuch as momentum-based optimizers [237]. Loo et al. [238]\nutilized the light-weight empirical neural network Gaussian\nprocess kernel for the inner-loop optimization and a new loss\nfor outer-loop optimization. Zhou et al. [239] combined feature\nextractor in the distillation procedure.\nGradient matching-based strategy. This strategy [236],\n[240] does not require to perform the inner-loop optimization\n16\nas used in the meta-model matching-based strategy. There-\nfore, it is more efficient than the meta-model matching-based\nstrategy. Numerous approaches have been proposed along this\ndivision. Kim et al. [241] further utilized spatial redundancy\nremoving to accelerate the optimization process and gradients\nmatching on the original dataset.\nTrajectory matching-based strategy. This strategy per-\nforms distillation by matching the training trajectories of\nmodels trained on the original and the pursued datasets [242].\nCui et al. [243] proposed a memory-efficient method which is\navaliable for large datasets.\nDistribution matching-based strategy. This strategy per-\nforms the distillation by directly matching the distribution of\nthe original dataset and the pursued dataset [244]. Wang et\nal. [245] constructed a bilevel optimization strategy to jointly\noptimize a single encoder and summarize data.\nThere are some solutions [237], [246]–[250] which take\nalternative technical strategies. Zhou et al. [246] introduced\nreinforcement learning to solve the bi-level optimization in\ndata distillation. Zhao and Bilen [247] learned a series of\nlow-dimensional codes to generate highly informative images\nthrough the GAN generator.\n2) Subset selection: Different from dataset distillation that\ngenerates a new training set, subset selection aims to select\nthe most useful samples from the original training set [35].\nIt does not generate new samples and can be used in any\ntraining stages that require to select samples from the original\ntraining set. In Fig. 11, there are two divisions, including\ngreedy search-based and mathematics-based.\nIn the greedy search-based strategy, the utility of each\ntraining sample is measured, and the subset is searched based\non the utility rankings. According to the employed measures,\nexisting methods can be divided into four categories, including\ndifficulty-based, influence-based, value-based, and confidence-\nbased. Meding et al. [251] utilized the misclassified rate by\nmultiple classifiers as the learning difficulty of a training\nsample to select samples. Feldman and Zhang [252] defined\nan influence score and a memorization score to measure the\nusefulness of a training sample. Samples with low influence\nand memorization scores are redundant and can be deleted.\nBirodkar et al. [6] employed clustering to select most valuable\nsamples which are close to the cluster centers and delete\nthe rest redundant ones. Northcutt et al. [253] leveraged the\nconfidence score to prune training samples. There are also\nmany studies combining the measures and active learning to\nselect samples [254].\nDifferent from the greedy search strategy, some other meth-\nods seek a global optimal subset according to a mathematical\napproach. Yang et al. [255] proposed a scalable framework\nto iteratively extract multiple mini-batch coresets from larger\nrandom subsets of training data by solving a submodular\ncover problem. Mirzasoleiman et al. [256] defined a monotonic\nfunction for coreset selection and proposed a generic algorithm\nwith approximately linear complexity.\nF. Other typical techniques\nThis study lists two representative technical paths, including\npure mathematical optimization and the combination of more\nthan one aforementioned methods described in Sections VI-A\nto VI-E.\n1) Pure mathematical optimization: This division refers\nto the manners that perform data optimization via a pure\nmathematical optimization procedure in the above-mentioned\ndivisions.\nThe first typical scenario for pure mathematical optimization\nis the construction of a small-size yet high-quality dataset\nfrom the original training set. The tasks involving batch\nconstruction, meta data compiling in meta learning, or dataset\ndistillation usually adopt mathematical optimization. Liu et\nal. [257] constructed a set variance diversity-based objective\nfunction for data augmentation and pursued the selection for\na set of augmented samples via the maximization of the ob-\njective function in batch construction. Joseph et al. [258] pro-\nposed a submodular optimization-based method to construct\na mini-batch in DNN training. Significant improvements in\nconvergence and accuracy with their constructed mini-batches\nhave been observed. Su et al. [59] established an objective\nfunction for meta data compiling. The objective consists of\nfour criterion, including cleanliness, diversity, balance, and\ninformative. As introduced in Section VI-E, data pruning is\nusually performed based on pure mathematical optimization.\nThe second typical scenario is the regularized sample\nweighting or perturbation. The details are described in Sections\nVI-C4 and VI-D3. For instance, Li et al. [259] devised a new\nobjective function for the label perturbation strength, which\ncan also reduce the Bayes error rate during training. Meister\net al. [260] constructed a general form of regularization that\ncan derive a series of label perturbation methods.\nThe third typical scenario is the constrained optimization,\nwhich embeds prior knowledge or conditions in data weight-\ning, perturbation, or pruning into the constraints. For instance,\nChai et al. [261] defined an optimization objective function\nwith the constraints that each demographic group should have\nequal total weights in fairness-aware learning. The adversarial\nperturbation of multi-label learning is usually attained by\nsolving constrained optimization problems [262]–[264]. Hu\net al. [263] developed a novel loss for multi-label top-k\nattack with the constraints that considers top-k ranking relation\namong labels.\n2) Technique combination: Indeed, many learning algo-\nrithms do not employ a single data optimization technique.\nInstead, they combine different data optimization techniques.\nThe following lists a few combination examples.\nIn data augmentation, many methods choose to generate\nsamples in the first step and resample or reweight the samples\nin the second step. For instance, Cao et al. [265] dealt with\ngrammatical error correction by using a data augmentation\nmethod during training and a data weighting method to auto-\nmatically balance the importance of each kind of augmented\nsamples. Liu et al. [266] generated new source phrases from\na masked language model then sampled an aligned counter-\nfactual target phrase for neural machine translation. Zang et\nal. [267] combined data augmentation and resampling for a\nlong-tailed learning task.\nIn data perturbation, different directions/granularity levels\nare usually combined in the same method. For example,\n17\nadversarial perturbation belongs to the positive direction, while\nanti-adversarial perturbation belongs to the negative one. Zhao\net al. [268] considered both category-wise and sample-wise\nfactors to define the logit perturbation for imbalanced learning.\nZhou et al. [67] combined both adversarial and anti-adversarial\nperturbations and theoretically revealed the superiority of the\ncombination than the adversarial perturbation only.\nIn data weighting, numerous methods combine it with data\naugmentation. Han et al. [220] combined uncertainty-based\nweighting and the classical augmentation method mixup. Chen\net al. [164] combined effective number-based weighting and\nlogit perturbation for long-tail learning tasks. In addition,\nsome methods combine different granularity levels or different\npriority models. For example, Focal loss [72] employs both\ncategory-wise and sample-wise weight coefficients for each\nsample.\nVII. DATA OPTIMIZATION THEORIES\nThere are a large amount of studies focusing on the the-\noretical aspects of data optimization. It is quite challenging\nto arrange existing theoretical studies into a clear roadmap.\nThis study summarizes existing studies in the following two\ndimensions, including formalization and explanation.\nA. Formalization\nIn order to theoretically analyze and understand the data\noptimization methods, it is essential to establish mathematical\nformulations. Statistical modeling is the primary tool for\ntheir formalization [269]–[271]. Basic assumptions are usually\nrelied on. The most widely used assumptions for the statistical\nmodeling include the following.\n• Gaussian distribution assumption. Many studies [29],\n[272]–[274] assume that data in each category conforms\nto a Gaussian distribution, which simplifies computation\nand inference compared to other complicated distribu-\ntions [275].\n• Equal class CPD assumption. In many learning stud-\nies [276], [277] excepting those for distribution drift,\nthe class-conditional probability densities (CPD) of the\ntraining and testing sets are assumed to be identical.\n• Uniform distribution assumption. In many studies [175],\n[278], the distribution over categories in the testing set is\nassumed to be uniform. Some studies implicitly use this\nassumption by using modified losses such as the balanced\naccuracy or balanced test error [174], [279], even if the\ncategory proportions in the test corpus are not identical.\n• Linear boundary assumption. In many studies [115],\n[280], the decision boundary of the involved classifier\nis assumed to be linear. The decision boundary between\ntwo categories under the cross-entropy loss is linear.\nBased on these assumptions, the data optimization problems\nare usually formalized into probabilistic, constrained opti-\nmization, or regularization-based problems. For example, Xu\net al. [281] investigated importance weighting for covariate-\nshift generalization based on probabilistic analysis. Chen et\nal. [282] defined the classification accuracy based on poste-\nrior probability for zero-shot learning. Qraitem et al. [283]\nformalized a constrained linear program problem to investigate\nthe effect of data resampling. Roh et al. [284] formulated\na combinatorial optimization problem for the unbiased se-\nlection of samples in the presence of data corruption. In\nclassical weighting paradigm such as SPL, data weighting is\ndirectly formalized in the optimization object consisting of the\nweighted loss and a regularizer. Zhang et al. [285] defined a\nre-weighted score function consisting of weighted loss and a\nsparsity regularization for causal discovery.\nJiang et al. [286] proposed a new adversarial perturbation\ngeneration method by adding a diversity-based regularization\nwhich measures the diversity of candidates. Hounie et al. [287]\nproposed a constrained learning problem for automatic data\naugmentation by combining conventional training loss and\nthe constraints for invariance risk. Blum and Stangl [288]\ninvestigated the utility of fairness constraints in fair machine\nlearning.\nB. Explanation\nMost theoretical studies on data optimization aim to explain\nwhy the existing methods are effective or ineffective.\nIn data perception, researchers usually conducted theoretical\nanalysis on the role of one typical data measure or leveraged\nthe measure to understand the training process of DNNs. Doan\net al. [289] conducted a theoretical analysis of catastrophic\nforgetting in continuous learning with neural-tangent-kernel\noverlap matrix. Chatterjee et al. [290] utilized the perception\non gradients to explain the generalization of deep learning.\nIn data resampling, existing theoretical studies focus on\nimportance sampling for deep learning. Katharopoulos and\nFleuret [110] derived an estimator of the variance reduc-\ntion achieved with importance sampling in deep learning.\nKatharopoulos and Fleuret [291] theoretically revealed that\nthe loss value can be used as an alternative importance metric,\nand propose an efficient way to perform importance sampling\nfor a deep model. Wang et al. [292] proposed an unweighted\ndata sub-sampling method, and proved that the subset-model\nacquired through the method outperforms the full-set-model.\nIn data augmentation, more and more theoretical studies\nare performed. Dao et al. [293] established a theoretical\nframework for understanding data augmentation. According\nto their framework, data augmentation is approximated by two\ncomponents, namely, first-order feature averaging and second-\norder variance regularization. Zhao et al. [125] defined an\neffective regularization term for adversarial data augmentation\nand theoretically derived it from the information bottleneck\nprinciple. Wu and He [294] investigated the theoretical is-\nsues for adversarial perturbations for multi-source domain\nadaptation. Gilmer et al. [295] also attempted to explain the\nadversarial samples.\nIn data perturbation, most theoretical studies focus on the\nadversarial perturbation. Yi et al. [296] investigated the models\ntrained by adversarial training on OOD data and justified that\nthe input perturbation robust model in pre-training provides\nan initialization that generalizes well on downstream OOD\ndata. Peck et al. [297] formally characterized adversarial\nperturbations by deriving lower bounds on the magnitudes of\n18\nTABLE I\nSOME DATA OPTIMIZATION METHODS IN NOISY-LABEL LEARNING.\nDatasets\nResampling\nAugmentation\nPerturbation\nWeighting\nPruning\nCIFAR10\n[113], [92], [292]\n[162], [305], [306], [307]\n[177], [183]\n[100], [308], [309], [310]\n[311], [312], [313]\nCIFAR100\n[113], [92], [292]\n[162], [305], [306], [307]\n[177], [183]\n[100], [309], [310]\n[312], [313]\nClothing1M\n[292]\n[162], [307], [305]\n[177], [183]\n[100], [310]\n[313]\nSVHN\n[120]\n[314]\n[67], [177], [183]\n[100]\n[245]\nWebVision\n[315]\n[307], [305]\n[84]\n[308], [309], [310]\n[312]\nFig. 12. High-level connections for existing data optimization studies.\nperturbations required to change the classification of neural\nnetworks. Some studies delved into the theoretical justification\nfor label and logit perturbation. Xu et al. [298] analyzed the\nconvergence SGD with label smoothing regularization and\nrevealed that an appropriate LSR can help to speed up the\nconvergence of SGD. Li et al. [176] theoretically analyzed the\nusefulness of logit adjustment in dealing with class imbalanced\nissues.\nIn data weighting, Byrd and Lipton [108] investigated the\nrole of importance in deep learning. Fang et al. [232] discussed\nthe limitations of importance weighting and found that it\nsuffers from a circular dependency. Meng et al. [299] analyzed\nthe capability of the self-paced learning and provided an\ninsightful interpretation of the effectiveness of several classical\nSPL variations. Weinshall et al. [300] proved that the rate\nof convergence of an ideal curriculum learning method is\nmonotonically increasing with the learning difficulty of the\ntraining samples.\nIn data pruning, theoretical studies are relatively limited.\nZhu et al. [301] revealed that distilled data lead to networks\nthat are not calibratable. The reason lies in two folds, including\na more concentrated distribution of the maximum logits and\nthe loss of information that is semantically meaningful but\nunrelated to classification tasks. Dong et al. [302] emerged\ndataset distillation into the privacy community and theoreti-\ncally revealed the connection between dataset distillation and\ndifferential privacy.\nThere are also studies which aim to reveal the intrinsic\nconnections between two different technical paths. For in-\nstance, regularization is a widely used technique in deep learn-\ning [169], and several typical data optimization techniques\nare revealed to be a regularization method [39], [303], [304].\nTherefore, intrinsic connections among these techniques can\nbe established, which enlightens a better understanding of the\ninvolved technical paths and can envision novel inspirations\nor methods.\nVIII. CONNECTIONS AMONG DIFFERENT TECHNIQUES\nThe connections among different data optimizations tech-\nniques can be described by Fig. 12. Four aspects, namely,\nperception, application scenarios, similarity/opposition, and\ntheories, connect different methods within a technical path or\nacross different paths.\nA. Connections via data perception\nData perception is the first (explicit or implicit) step in the\ndata optimization pipeline. Methods along different technical\npaths introduced in Section VI may choose the same or\nsimilar quantities in perception. Therefore, quantities for data\nperception connect different methods. For example, many data\noptimization methods are on the basis of training loss in re-\nsampling [320], augmentation [321], perturbation [47], weight-\ning [299], and subset selection [322]. Gradient is widely used\nin resampling [110], augmentation [323], perturbation [67],\nweighting [61], and dataset distillation [236]. Other quantities\nincluding margin and uncertainty are also used in different\noptimization techniques.\nThe utilization of the same or similar perception quantities\ndemonstrates that these methods have the same or similar\nheuristic observations or theoretical inspirations.\nB. Connections via application scenarios\nMost data optimization methods can be leveraged for the\napplication scenarios discussed in Section IV-B.\nOne of the most focused scenarios of data optimization\nmethods is noisy-label learning. Many classical methods are\nfrom resampling [324], augmentation, weighting, or perturba-\ntion. These are also dataset distillation studies for noisy-label\ndatasets [325]. Table I shows some representative data opti-\nmization methods for noisy-label learning on five benchmark\ndatasets CIFAR10 [326], CIFAR100 [326], Clothing1M [327],\nSVHN [328], and WebVision [329].\nImbalanced learning is also among the most focused scenar-\nios. Nearly all the listed data optimization technical paths have\nbeen used in imbalanced learning. Table II shows some rep-\nresentative data optimization methods for imbalanced learning\non four benchmark datasets CIFAR10-LT [46], CIFAR100-\nLT [46], iNaturalist [330], and ImageNet-LT [331]. There\nare some studies employing more than one type of data\noptimization techniques such as ReMix [316], which combines\nresampling and augmentation, in Table II.\nRobust learning for adversarial attacks is another typical\nscenario. Karimireddy and Jaggi [332] employed resampling\n19\nTABLE II\nSOME DATA OPTIMIZATION METHODS IN IMBALANCED LEARNING.\nDatasets\nResampling\nAugmentation\nPerturbation\nWeighting\nDataset pruning\nCIFAR10-LT\n[316]\n[316], [317]\n[174], [195]\n[72], [46]\n[59], [318]\nCIFAR100-LT\n[175]\n[164], [317]\n[174], [195]\n[72], [46]\n[59], [318]\niNaturallist\n[175]\n[164], [317]\n[174], [195]\n[72], [46]\n[59]\nImageNet-LT\n[175]\n[164], [317]\n[174], [195]\n[72], [46]\n[59], [319]\nto design robust model in distributed learning. Data weight-\ning [210] and dataset distillation [333] are also used in robust\nlearning.\nC. Connections via similarity/opposition\nThe similar and opposite relationships existing among the\nfive technical paths are introduced in Section VI.\nData resampling and weighting are closely related tech-\nniques, as their key steps are nearly the same. Therefore, in\nmany studies on noisy-label learning and imbalanced learning,\nthese two techniques are often considered as a single strategy.\nAlthough data pruning and augmentation are opposite to\neach other, they have consistent ultimate goals in learning\ntasks. They are overlapped in terms of employed methodolo-\ngies as shown in Fig. 13. It is believable that more intrinsic\nconnections can be explored for them.\nFig. 13. Connection between augmentation and pruning.\nIn the data resampling, weighting, and perturbation, the\nassignment manners for the sampling rate, weighting score,\nand perturbation variable are quite similar. In addition to\nthe classical importance score, both meta learning [334] and\nadversarial strategy [120] have also been used in data re-\nsampling. Regularization-based manner is used in nearly all\nthe data optimization paths except resampling. Due to space\nconstraints, methods with different assignment manners are\nnot summarized in a table as those in Section VIII-B.\nThere are other opposite relationships, such as undersam-\npling vs. oversampling, easy-first weighting vs. hard-first\nweighting, positive perturbation vs. negative perturbation, and\nexplicit augmentation vs. implicit augmentation. Both method-\nologies in these opposite relationships have been demonstrated\nto be effective, aligning with the proverb “All roads lead to\nRome”.\nD. Connections via theory\nThere are some common theoretical issues, analyses, and\nconclusions heavily influencing most data optimization tech-\nniques. They are the natural connections among different\ntechniques. Several examples are listed as follows:\n• Theoretical issues in data perception. A solid theoretical\nbasis for data perception in data optimization is lacking,\neven though most data optimization methods implicitly\nor explicitly rely on the perception for the training data.\nFor instance, many methods from resampling, weighting,\nand perturbation are based on dividing samples into easy\nand hard. Nevertheless, there is not yet a widely accepted\nlearning difficulty measure with a rigorous theoretical\nbasis in the literature. More than ten types of learning\ndifficulty measures are utilized to distinguish easy from\nhard samples in previous literature. A theoretical formu-\nlation for data perception is of great importance.\n• Probabilistic density (ratio) estimation. Many data opti-\nmization methods, especially data resampling and weight-\ning, heavily rely on the probabilistic density (ratio) es-\ntimation. The most representative method is the impor-\ntance sampling. In learning difficulty-based weighting,\nthe probabilistic density ratio, in terms of learning dif-\nficulty, is revealed to determine the priority mode [217],\nnamely, easy/medium/hard-first.\n• Regularization-based explanation. Many data optimiza-\ntion methods are considered as a type of regularization,\nincluding data augmentation and perturbation. In these\nmethods, data optimization performs implicit model reg-\nularization other than explicit regularization that directly\nworks on model parameters. Regularization is not al-\nways beneficial as over-regularization may occur. Li et\nal. [335] pointed out that large amount of augmented\nnoisy data could lead to over-regularization and proposed\nan adaptive augmentation method. Adversarial training\nmay result in robust overfitting [336].\n• Generalization bound for data optimization. Many studies\nchoose to deduce a mathematical bound for the general-\nization risk in terms of the empirical risk and variables\nrelated to the data optimization. This manner can theoret-\nically explain the utility of the involved data optimization.\nXiao et al. [337] derived stability-based generalization\nbounds for stochastic gradient descent (SGD) on the loss\nwith adversarial perturbations. Xu et al. [115] established\na new generalization bound that reflects how importance\nweighting leads to the interplay between the empirical\nrisk and the deviation between the source and target\ndistributions.\nThe progress in each of the above theoretical aspects will\npromote the advancement of many data optimization methods\nin different technical paths. Hopefully, this survey will pro-\nmote the mutual understanding of the referred technical paths.\nIX. FUTURE DIRECTIONS\nThis section summarizes some research directions deserving\nfurther exploring.\n20\nA. Principles of data optimization\nUp till now, there has been no consensus theoretical\nframework that is suitable for all or most technical paths.\nThere are some studies aiming to establish the connection\nbetween two different technical paths, such as resampling\nvs. weighting [338]. Many open problems or controversies\nremain unsolved. For example, there is no ideal answer for\nwhich resampling strategy should be employed first: over-\nsampling or undersampling? Megahed et al. [104] suggested\nthat undersampling should be used firstly, whereas Xie et\nal. [275] demonstrated that oversampling is effective. Like-\nwise, although Zhou et al. [217] provided an initial answer\nfor the choice of easy-first and hard-first weighting strategies,\na solid theoretical framework is still lacking in their study.\nMoreover, even for a single data optimization method,\nmultiple explanations from different views may exist. The\nexplanation for label smoothing is a typical example. At least\nfour studies provide empirical or theoretical explanations for\nit [260], [298], [339], [340]. Regarding the effectiveness of\nadversarial samples, some researchers have pointed out that\nadversarial samples are useful features [270], while some other\nresearchers investigated it in terms of gradient regulariza-\ntion [341].\nConsequently, the construction of the data optimization prin-\nciples is of great importance, as it can promote the establishing\nof a unified and solid theoretical framework which can be\nused to analyze and understand of each data optimization\ntechnical path. There have been studies on the first principle\nfor the design of DNNs [342]. To explore the principles for\ndata optimization, a unified mathematical formalization tool is\nrequired and large-scale empirical studies (e.g., [343], [344])\nwill also be helpful.\nB. Interpretable data optimization\nInterpretable data optimization refers to the explanation for\nthe involved data optimization techniques in terms of how\nand which aspects they affect the training process of DNNs.\nAlthough interpretable deep learning receives much attention\nin recent years [345], it focuses on DNN models other than\nthe training processing in which data optimization techniques\nare involved. Interpretable data optimization is an under-\nexplored research topic and there are limited studies on this\ntopic [346]. The well explanation of how and which aspects of\na data optimization method affects a specific training process\nis significant beneficial for the design or selecting of more\neffective optimization methods.\nThe aforementioned theoretical studies on data optimiza-\ntion provide partial explanations for the corresponding data\noptimization method. However, the partial explanations are\nconcentrated in common aspects across different learning\ntasks. How and which aspects for the involved method on\na specific learning task remain unexplored.\nThe interpretable deep learning area has raised many ef-\nfective methodologies. Recently, researchers have attempted\nto introduce interpretable methodologies to explore the data\noptimization methods. Zelaya and Vladimiro [347] explored\nmetrics to quantify the effect of some data-processing steps\nsuch as undersampling and data augmentation on the model\nperformance. Hopefully, more and more studies on explainable\ndata optimization appear in the future.\nC. Human-in-the-loop data optimization\nRecently, human-in-the-loop (HITL) deep learning receives\nincreasing attention in the AI community [348]. With out\nhuman’s participants, high-quality training samples are not\nintractable to obtain. Naturally, HITL data optimization can\nalso be beneficial for deep learning. Collins et al. [349]\ninvestigated HITL mixup and indicated that collating humans’\nperceptions on augmented samples could impact model per-\nformance. Wallace et al. [350] proposed HITL adversarial\ngeneration, where human authors are guided to break models.\nAgarwal et al. [351] proposed Variance of Gradients (VoG) to\nmeasure samples’ learning difficulty and ranked samples by\nVoG. Then, a tractable subset of the most difficult samples is\nselected for HITL auditing. Overall, research on HITL data\noptimization is in the early stage.\nD. Data optimization for new challenges\nNew challenges are constantly emerging in deep learning\napplications. We take the following three recent challenges as\nexamples to illustrate the future direction of data optimization:\n• Open-world learning. This learning scenario confronts the\nchallenge of out-of-distribution (OOD) samples. Wu et\nal. [352] investigated noisy-label learning under the open-\nworld setting, in which both OOD and noisy samples\nexist. Some other studies investigate cases when ODD\nmeets imbalanced learning [353] and adversarial robust-\nness [354].\n• Large-model training. Large models especially the large\nlanguage models [355] have achieved great success in\nrecent years. Data optimization can also take effect in\nthe training of large models. Wei et al. [356] investigated\nthe condensation of prompts and promising results are\nobtained. Contrarily, Jiang et al. [357] leveraged prompt\naugmentation to calibrate large language models. Many\nissues investigated in conventional deep learning tasks\nmay also exist for large-model training, e.g., prompt\nvaluation.\n• Multi-modal learning. With the development of data sens-\ning and collection technology, multi-modal data are avali-\nable in more and more real tasks [358]. Consequently,\nmany learning tasks are actually multi-modal learning.\nAs each sample consists of raw data/features from dif-\nferent modalities, the data perception for multi-modal\nsamples should be different from that for conventional\nsingle-modal samples. The data optimization methods are\nlikewise different from conventional methods [359].\nE. Data optimization agent\nGiven a concrete learning task, a selection dilemma oc-\ncurs for the tremendous data optimization techniques. There\nhave been studies on the automatic data optimization such\nas automatic data augmentation [287]. Nevertheless, existing\n21\nFig. 14. The training of a data optimization agent.\nautomatic data optimization methods still focus on a particular\ntype of technical path rather than the types across different\ntechnical paths [360], [361]. A more general data optimization\nagent can be trained by iteratively training on a large number\nof deep learning tasks via reinforcement learning.\nFig. 14 shows a possible mean to construct a data optimiza-\ntion agent. New learning tasks are compiled based on existing\nclassical tasks via operations such as noise adding, and class\nproportion re-distributing. The candidate of data optimization\noperators are from arbitrary optimization techniques or a single\ntechnique introduced in Section VI. A powerful data optimiza-\ntion agent can then be trained via reinforcement learning based\non compiled learning tasks and their rewards.\nX. CONCLUSIONS\nThis paper aims to summarize a wide range of learning\nmethods within an independent deep learning realm, namely,\ndata optimization. A taxonomy for data optimization, as well\nas fine-granularity sub-taxonomies, is established for existing\nstudies on data optimization. Connections among different\nmethods are discussed, and potential future directions are\npresented. It is noteworthy that many classical methods, such\nas dropout, are essentially data optimization methods. In our\nfuture work, we will explore a more fundamental and unified\nviewpoint on data optimization, and develop a more com-\nprehensive taxonomy to incorporate more classical methods.\nWe hope that this study can inspire more researchers to gain\ninsight into data-centric AI.\nREFERENCES\n[1] S. E. Whang, Y. Roh, H. Song, and J.-G. Lee, “Data collection and\nquality challenges in deep learning: A data-centric ai perspective,” The\nVLDB Journal, vol. 32, no. 4, pp. 791–813, 2023.\n[2] M. H. Jarrahi, A. Memariani, and S. Guha, “The principles of data-\ncentric ai,” Communications of the ACM, vol. 66, no. 8, pp. 84–92,\n2023.\n[3] Y. Liang, D. Huang, C.-D. Wang, and P. S. Yu, “Multi-view graph\nlearning by joint modeling of consistency and inconsistency,” IEEE\nTNNLS, pp. 1–15, 2022.\n[4] P. Zhu, X. Yao, Y. Wang, M. Cao, B. Hui, S. Zhao, and Q. Hu, “Latent\nheterogeneous graph network for incomplete multi-view learning,”\nIEEE TMM, vol. 25, pp. 3033–3045, 2023.\n[5] L. Brigato and L. Iocchi, “A close look at deep learning with small\ndata,” in ICPR, 2021, pp. 2490–2497.\n[6] V. Birodkar, H. Mobahi, and S. Bengio, “Semantic redundan-\ncies in image-classification datasets: The 10% you don’t need,”\narXiv:1901.11409, 2019.\n[7] Y. Yu, S. Khadivi, and J. Xu, “Can data diversity enhance learning\ngeneralization?” in COLING, 2022, pp. 4933–4945.\n[8] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang, “Learning under\nconcept drift: A review,” IEEE TKDE, vol. 31, no. 12, pp. 2346–2363,\n2018.\n[9] W. Wang, R. Wang, L. Wang, Z. Wang, and A. Ye, “Towards a robust\ndeep neural network against adversarial texts: A survey,” IEEE TKDE,\nvol. 35, no. 3, pp. 3159–3179, 2023.\n[10] Y. Wu, L. Zhang, and X. Wu, “On convexity and bounds of fairness-\naware classification,” in WWW, 2019, pp. 3356–3362.\n[11] P. Xiong, S. Buffett, S. Iqbal, P. Lamontagne, M. Mamun, and\nH. Molyneaux, “Towards a robust and trustworthy machine learning\nsystem development: An engineering perspective,” JISA, vol. 65, p.\n103121, 2022.\n[12] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\nempirical risk minimization,” ICLR, 2018.\n[13] R. Yao and O. Wu, “Compensation learning,” arXiv:2107.11921, 2022.\n[14] X. Wang, L. Jing, Y. Lyu, M. Guo, J. Wang, H. Liu, J. Yu, and T. Zeng,\n“Deep generative mixture model for robust imbalance classification,”\nIEEE TPAMI, vol. 45, no. 3, pp. 2897–2912, 2023.\n[15] H. He and E. A. Garcia, “Learning from imbalanced data,” IEEE\nTKDE, vol. 21, no. 9, pp. 1263–1284, 2009.\n[16] Y. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, “Deep long-tailed\nlearning: A survey,” IEEE TPAMI, vol. 45, no. 9, pp. 10 795–10 816,\n2023.\n[17] G. Algan and I. Ulusoy, “Image classification with deep learning in\nthe presence of noisy labels: A survey,” Knowledge-Based Systems,\nvol. 215, no. 5, p. 106771, 2021.\n[18] H. Song, M. Kim, D. Park, Y. Shin, and J.-G. Lee, “Learning from\nnoisy labels with deep neural networks: A survey,” IEEE TNNLS, pp.\n1–19, 2022.\n[19] X. Cao, W. Bu, S. Huang, M. Zhang, I. W. Tsang, Y. S. Ong, and J. T.\nKwok, “A survey of learning on small data,” arXiv:2207.14443, 2022.\n[20] Y. Wang, Q. Yao, J. Kwok, and L. M. Ni, “Generalizing from a few\nexamples: A survey on few-shot learning,” ACM computing surveys,\nvol. 53, no. 3, pp. 1–34, 2020.\n[21] L. Yuan, H. Li, B. Xia, C. Gao, M. Liu, W. Yuan, and X. You, “Recent\nadvances in concept drift adaptation methods for deep learning,” in\nIJCAI, 2022, pp. 5654–5661.\n[22] A. Diez-Olivan, P. Ortego, J. D. Ser, I. Landa-Torres, D. Galar,\nD. Camacho, and B. Sierra, “Adaptive dendritic cell-deep learning\napproach for industrial prognosis under changing conditions,” IEEE\nTII, vol. 17, no. 11, pp. 7760–7770, 2021.\n[23] J. Gama, I. ˇZliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia, “A\nsurvey on concept drift adaptation,” ACM Computing Surveys, vol. 46,\nno. 4, pp. 1–37, 2014.\n[24] A. S. Iwashita and J. P. Papa, “An overview on concept drift learning,”\nIEEE Access, vol. 7, pp. 1532–1547, 2019.\n[25] S. H. Silva and P. Najafirad, “Opportunities and challenges in deep\nlearning adversarial robustness: A survey,” arXiv:2007.00753, 2020.\n[26] J. Xu, J. Chen, S. You, Z. Xiao, Y. Yang, and J. Lu, “Robustness of\ndeep learning models on graphs: A survey,” AI Open, vol. 2, pp. 69–78,\n2021.\n[27] S. Goyal, S. Doddapaneni, M. M. Khapra, and B. Ravindran, “A\nsurvey of adversarial defenses and robustness in nlp,” ACM Computing\nSurveys, vol. 55, no. 14s, pp. 1–39, 2023.\n[28] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\n“A survey on bias and fairness in machine learning,” ACM Computing\nSurveys, vol. 54, no. 6, pp. 1–35, 2021.\n[29] A. Petrovi´c, M. Nikoli´c, S. Radovanovi´c, B. Delibaˇsi´c, and M. Jo-\nvanovi´c, “Fair: Fair adversarial instance re-weighting,” Neurocomput-\ning, vol. 476, pp. 14–37, 2022.\n[30] S. K. Devitt, “Trustworthiness of autonomous systems,” Foundations\nof trusted autonomy (Studies in Systems, Decision and Control, Volume\n117), pp. 161–184, 2018.\n[31] D. Kaur, S. Uslu, K. J. Rittichier, and A. Durresi, “Trustworthy artificial\nintelligence: A review,” ACM Computing Surveys, vol. 55, no. 2, pp.\n1–38, 2022.\n[32] B. Wu, Y. Bian, H. Zhang, J. Li, J. Yu, L. Chen, C. Chen, and J. Huang,\n“Trustworthy graph learning: Reliability, explainability, and privacy\nprotection,” ACM KDD, pp. 4838–4839, 2022.\n[33] C. Fang, L. Cheng, H. Qi, and D. Zhang, “Combating noisy labels in\nlong-tailed image classification,” arXiv:2209.00273, 2022.\n[34] M. Singh, G. Ghalachyan, K. R. Varshney, and R. E. Bryant, “An\nempirical study of accuracy, fairness, explainability, distributional ro-\nbustness, and adversarial robustness,” in KDD Workshop, 2021.\n[35] Z. Wan, Z. Wang, C. Chung, and Z. Wang, “A survey of data opti-\nmization for problems in computer vision datasets,” arXiv:2210.11717,\n2022.\n[36] X. Zheng, Y. Liu, Z. Bao, M. Fang, X. Hu, A. W.-C. Liew, and S. Pan,\n“Towards data-centric graph machine learning: Review and outlook,”\narXiv:2309.10979, 2023.\n22\n[37] Y. Luo, Y. Wong, M. Kankanhalli, and Q. Zhao, “G-softmax: Improving\nintraclass compactness and interclass separability of features,” IEEE\nTNNLS, vol. 31, no. 2, pp. 685–699, 2020.\n[38] A. Damian, T. Ma, and J. D. Lee, “Label noise sgd provably prefers\nflat global minimizers,” in NeurIPS, 2021, pp. 27 449–27 461.\n[39] Y. Wang, X. Pan, S. Song, H. Zhang, C. Wu, and G. Huang, “Implicit\nsemantic data augmentation for deep networks,” in NeurIPS, 2019, pp.\n12 635–12 644.\n[40] M. Wang, Y. Zhang, and W. Deng, “Meta balanced network for fair\nface recognition,” IEEE TPAMI, 2021.\n[41] S. Fujii, Y. Ishii, K. Kozuka, T. Hirakawa, T. Yamashita, and H. Fu-\njiyoshi, “Data augmentation by selecting mixed classes considering\ndistance between classes,” arXiv:2209.05122, 2022.\n[42] I. L. . F. Hutter, “Online batch selection for faster training of neural\nnetworks,” ICLR Workshop, 2016.\n[43] C.-Y. Chuang and Y. Mroueh, “Fair mixup: Fairness via interpolation,”\nin ICLR, 2021.\n[44] L. E. Celis, A. Mehrotra, and N. Vishnoi, “Fair classification with\nadversarial perturbations,” in NeurIPS, 2021, pp. 8158–8171.\n[45] B. Yan, S. Seto, and N. Apostoloff, “Forml: Learning to reweight data\nfor fairness,” arXiv:2202.01719, 2022.\n[46] Y. Cui, M. Jia, T. Lin, Y. Song, and S. Belongie, “Class-balanced loss\nbased on effective number of samples,” in CVPR, 2019, pp. 9260–9269.\n[47] M. Li, F. Su, O. Wu, and J. Zhang, “Logit perturbation,” in AAAI,\n2022, pp. 10 388–10 396.\n[48] Y. Chen, P. Zhang, T. Kong, Y. Li, X. Zhang, L. Qi, J. Sun, and\nJ. Jia, “Scale-aware automatic augmentations for object detection with\ndynamic training,” IEEE TPAMI, vol. 45, no. 2, pp. 2367–2383, 2023.\n[49] M. P. Naeini, G. F. Cooper, and M. Hauskrecht, “Obtaining well\ncalibrated probabilities using bayesian binning,” in AAAI, 2015.\n[50] B. Liu, I. Ben Ayed, A. Galdran, and J. Dolz, “The devil is in the\nmargin: Margin-based label smoothing for network calibration,” in\nCVPR, 2022, pp. 80–88.\n[51] J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and P. Dokania,\n“Calibrating deep neural networks using focal loss,” in NeurIPS, 2020,\npp. 15 288–15 299.\n[52] L. Dunlap, A. Umino, H. Zhang, J. Yang, J. E. Gonzalez, and\nT. Darrell, “Diversify your vision datasets with automatic diffusion-\nbased augmentation,” arXiv:2305.16289, 2023.\n[53] Z. Ye, Y. Dai, C. Hong, Z. Cao, and H. Lu, “Infusing definiteness into\nrandomness: Rethinking composition styles for deep image matting,”\nAAAI, 2023.\n[54] S. Yang, W. Xiao, M. Zhang, S. Guo, J. Zhao, and F. Shen, “Image data\naugmentation for deep learning: A survey,” arXiv:2204.08610, 2022.\n[55] W. Yang, C. Li, J. Zhang, and C. Zong, “Bigtranslate: Augmenting\nlarge language models with multilingual translation capability over 100\nlanguages,” arXiv:2305.18098, 2023.\n[56] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, and J. Gao, “Ad-\nversarial training for large neural language models,” arXiv:2004.08994,\n2020.\n[57] X. Glorot and Y. Bengio, “Understanding the difficulty of training deep\nfeedforward neural networks,” AISTATS, pp. 249–256, 2010.\n[58] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Sur-\npassing human-level performance on imagenet classification,” ICCV,\npp. 1026–1034, 2015.\n[59] F. Su, Y. Zhu, O. Wu, and Y. Deng, “Submodular meta data compiling\nfor meta optimization,” ECML/PKDD, 2022.\n[60] J. Huang, L. Qu, R. Jia, and B. Zhao, “O2u-net: A simple noisy\nlabel detection approach for deep neural networks,” in ICCV, 2019,\npp. 3326–3334.\n[61] B. Li, Y. Liu, and X. Wang, “Gradient harmonized single-stage\ndetector,” in AAAI, 2019, pp. 8577–8584.\n[62] C.-B. Zhang, P.-T. Jiang, Q. Hou, Y. Wei, Q. Han, Z. Li, and M.-M.\nCheng, “Delving deep into label smoothing,” IEEE TIP, vol. 30, pp.\n5984–5996, 2021.\n[63] S. Sinha, H. Ohashi, and K. Nakamura, “Class-wise difficulty-balanced\nloss for solving class-imbalance,” in ACCV, 2020.\n[64] M. Escudero-Vi˜nolo and A. L´opez-Cifuentes, “Ccl: Class-wise cur-\nriculum learning for class imbalance problems,” in ICIP, 2022, pp.\n1476–1480.\n[65] X. Ning, W. Tian, F. He, X. Bai, L. Sun, and W. Li, “Hyper-sausage\ncoverage function neuron model and learning algorithm for image\nclassification,” Pattern Recognition, vol. 136, p. 109216, 2023.\n[66] J. Lin, A. Zhang, M. L´ecuyer, J. Li, A. Panda, and S. Sen, “Measuring\nthe effect of training data on deep learning predictions via randomized\nexperiments,” in ICML, 2022, pp. 13 468–13 504.\n[67] X. Zhou, N. Yang, and O. Wu, “Combining adversaries with anti-\nadversaries in training,” in AAAI, 2023.\n[68] K. Tang, M. Tao, J. Qi, Z. Liu, and H. Zhang, “Invariant feature learning\nfor generalized long-tailed classification,” in ECCV, 2022, pp. 709–726.\n[69] S. Shrivastava, X. Zhang, S. Nagesh, and A. Parchami, “Datasetequity:\nAre all samples created equal? in the quest for equity within datasets,”\nin ICCV, 2023, pp. 4417–4426.\n[70] R. Wang, W. Xiong, Q. Hou, and O. Wu, “Tackling the imbalance for\ngnns,” in IJCNN, 2022.\n[71] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum\nlearning,” in ICML, 2009, pp. 41–48.\n[72] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss for\ndense object detection,” in CVPR, 2017, pp. 2999–3007.\n[73] M. Paul, S. Ganguli, and G. K. Dziugaite, “Deep learning on a data\ndiet: Finding important examples early in training,” in NeurIPS, 2021,\npp. 20 596–20 607.\n[74] W. Zhu, O. Wu, F. Su, and Y. Deng, “Exploring the learning difficulty\nof data: Theory and measure,” arXiv:2205.07427, 2022.\n[75] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos,\n“Beyond neural scaling laws: beating power law scaling via data\npruning,” in NeurIPS, 2022.\n[76] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu,\nM. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya,\nV. Makarenkov, and S. Nahavandi, “A review of uncertainty quan-\ntification in deep learning: Techniques, applications and challenges,”\nInformation fusion, vol. 76, pp. 243–297, 2021.\n[77] D. D’souza, Z. Nussbaum, C. Agarwal, and S. Hooker, “A tale of two\nlong tails,” arXiv:2107.13098, 2021.\n[78] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian\ndeep learning for computer vision?” in NeurIPS, 2017, pp. 5574–5584.\n[79] A. Kumar, S. Bhattamishra, M. Bhandari, and P. Talukdar, “Submodular\noptimization-based diverse paraphrasing and its effectiveness in data\naugmentation,” in NAACL, 2019, pp. 3609–3619.\n[80] D. Friedman and A. B. Dieng, “The vendi score: A diversity evaluation\nmetric for machine learning,” arXiv:2210.02410, 2023.\n[81] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and\nX. Chen, “Improved techniques for training gans,” NeurIPS, pp. 2234–\n2242, 2016.\n[82] O.\nWu,\n“Rethinking\nclass\nimbalance\nin\nmachine\nlearning,”\narXiv:2305.03900, 2023.\n[83] S. Swayamdipta, R. Schwartz, N. Lourie, Y. Wang, H. Hajishirzi, N. A.\nSmith, and Y. Choi, “Dataset cartography: Mapping and diagnosing\ndatasets with training dynamics,” in EMNLP, 2020.\n[84] A. Iscen, J. Valmadre, A. Arnab, and C. Schmid, “Learning with\nneighbor consistency for noisy labels,” in CVPR, 2022, pp. 4672–4681.\n[85] M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio,\nand G. J. Gordon, “An empirical study of example forgetting during\ndeep neural network learning,” ICLR, 2019.\n[86] P. Singh, P. Mazumder, and M. A. Karim, “Attaining class-level\nforgetting in pretrained model using few samples,” in ECCV, 2022,\npp. 433–448.\n[87] P. Maini, S. Garg, Z. Lipton, and J. Z. Kolter, “Characterizing dat-\napoints via second-split forgetting,” in NeurIPS, 2022, pp. 30 044–\n30 057.\n[88] Z. Wang, E. Yang, L. Shen, and H. Huang, “A comprehensive\nsurvey of forgetting in deep learning beyond continual learning,”\narXiv:2307.09218, 2023.\n[89] T. Kim, J. Ko, s. Cho, J. Choi, and S.-Y. Yun, “Fine samples for learning\nwith noisy labels,” in NeurIPS, 2021, pp. 24 137–24 149.\n[90] L. S. Shapley, “A value for n-person games,” in In Contributions to\nthe Theory of Games, 1953, pp. 307–317.\n[91] A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for\nmachine learning,” in ICML, 2019, pp. 2242–2251.\n[92] J. Yoon, S. Arik, and T. Pfister, “Data valuation using reinforcement\nlearning,” in ICML, 2020, pp. 10 842–10 851.\n[93] Y. Bian, Y. Rong, T. Xu, J. Wu, A. Krause, and J. Huang, “Energy-\nbased learning for cooperative games, with applications to valuation\nproblems in machine learning,” in ICLR, 2022.\n[94] K. F. Jiang, W. Liang, J. Zou, and Y. Kwon, “Opendataval: a unified\nbenchmark for data valuation,” in NeurIPS, 2023.\n[95] D. Bahri and H. Jiang, “Locally adaptive label smoothing improves\npredictive churn,” in ICML, 2021, pp. 532–542.\n[96] C. Dong, L. Liu, and J. Shang, “Data profiling for adversarial training:\nOn the ruin of problematic data,” arXiv:2102.07437v1, 2021.\n[97] Z. Hammoudeh and D. Lowd, “Training data influence analysisand\nestimation: A survey,” arXiv:2212.04612, 2023.\n23\n[98] Y. Wu, J. Shu, Q. Xie, Q. Zhao, and D. Meng, “Learning to purify noisy\nlabels via meta soft label corrector,” in AAAI, 2021, pp. 10 388–10 396.\n[99] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng, “Meta-\nWeight-Net: Learning an explicit mapping for sample weighting,” in\nNeurIPS, 2019, pp. 1917–1928.\n[100] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning for latent\nvariable models,” NeurIPS, pp. 1–9, 2010.\n[101] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness,\n“Unsupervised label noise modeling and loss correction,” in ICML,\n2019.\n[102] C. Hu, S. Yan, Z. Gao, and X. He, “Mild: Modeling the instance\nlearning dynamics for learning with noisy labels,” arXiv:2306.11560,\n2023.\n[103] Y. Li and N. Vasconcelos, “Repair: Removing representation bias by\ndataset resampling,” CVPR, 2019.\n[104] F. M. Megahed, Y.-J. Chen, A. Megahed, Y. Ong, N. Altman, and\nM. Krzywinski, “The class imbalance problem,” Nature Methods,\nvol. 18, pp. 1270–1272, 2021.\n[105] J. Cui, S. Liu, Z. Tian, Z. Zhong, and J. Jia, “Reslt: Residual learning\nfor long-tailed recognition,” IEEE TPAMI, vol. 45, no. 3, pp. 3695–\n3706, 2023.\n[106] A. Kirsch, J. van Amersfoort, and Y. Gal, “Batchbald: Efficient\nand diverse batch acquisition for deep bayesian active learning,” in\nNeurIPS, 2019.\n[107] H. Shimodaira, “Improving predictive inference under covariate shift by\nweighting the log-likelihood function,” Journal of statistical planning\nand inference, 2000.\n[108] J. Byrd and Z. Lipton, “What is the effect of importance weighting in\ndeep learning?” in ICML, 2019, pp. 872–881.\n[109] Q. Liu and J. Lee, “Black-box Importance Sampling,” in AISTATS,\n2017, pp. 952–961.\n[110] A. Katharopoulos and F. Fleuret, “Not all samples are created equal:\nDeep learning with importance sampling,” in ICML, 2018, pp. 2525–\n2534.\n[111] T. B. Johnson and C. Guestrin, “Training deep models faster with\nrobust, approximate importance sampling,” in NeurIPS, 2018.\n[112] A. H. Jiang, D. L.-K. Wong, G. Zhou, D. G. Andersen, J. Dean,\nG. R. Ganger, G. Joshi, M. Kaminksy, M. Kozuch, Z. C. Lipton, and\nP. Pillai, “Accelerating deep learning by focusing on the biggest losers,”\narXiv:1910.00762, 2019.\n[113] X. J. Gui, W. Wang, and Z. H. Tian, “Towards understanding deep\nlearning from noisy labels with small-loss criterion,” IJCAI, 2021.\n[114] H. Liu, X. Zhu, Z. Lei, and S. Z. Li, “Adaptiveface: Adaptive margin\nand sampling for face recognition,” in CVPR, 2019, pp. 11 947–11 956.\n[115] D. Xu, Y. Ye, and C. Ruan, “Understanding the role of importance\nweighting for deep learning,” arXiv:2103.15209, 2021.\n[116] V. Nguyen, M. Shaker, and E. H¨ullermeier, “How to measure uncer-\ntainty in uncertainty sampling for active learning,” Machine Learning,\nvol. 111, pp. 89–122, 2022.\n[117] J. Mena, O. Pujol, and J. Vitri`a, “A survey on uncertainty estimation\nin deep learning classification systems from a bayesian perspective,”\nACM Computing Surveys, vol. 54, no. 9, pp. 1–35, 2021.\n[118] A. Aljuhani, I. Casukhela, J. Chan, D. Liebner, and R. Machiraju,\n“Uncertainty aware sampling framework of weak-label learning for\nhistology image classification,” in MICCAI, 2022, pp. 366–376.\n[119] D. Ting and E. Brochu, “Optimal subsampling with influence func-\ntions,” in NeurIPS, 2018, pp. 3650–3659.\n[120] Y. Li and N. Vasconcelos, “Background data resampling for outlier-\naware classification,” in CVPR, 2020, pp. 13 218–13 227.\n[121] X. Wang and Y. Wang, “Sentence-level resampling for named entity\nrecognition,” in NAACL, 2022, pp. 2151–2165.\n[122] J. Zhang, T. Wang, W. W. Y. Ng, S. Zhang, and C. D. Nugent,\n“Undersampling near decision boundary for imbalance problems,” in\nICMLC, 2019, pp. 1–8.\n[123] M. Sun, H. Dou, B. Li, J. Yan, W. Ouyang, and L. Cui, “Autosampling:\nSearch for effective data sampling schedules,” in ICML, 2017, p.\n9923–9933.\n[124] G. Li, L. Liu, G. Huang, C. Zhu, and T. Zhao, “Understanding data\naugmentation in neural machine translation: Two perspectives towards\ngeneralization,” in EMNLP-IJCNLP, 2019, pp. 5689–56 958.\n[125] L. Zhao, T. Liu, X. Peng, and D. Metaxas, “Maximum-entropy adver-\nsarial data augmentation for improved generalization and robustness,”\nin NeurIPS, vol. 33, 2020, pp. 14 435–14 447.\n[126] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. A.\nMann, “Data augmentation can improve robustness,” in NeurIPS, 2021,\npp. 29 935–29 948.\n[127] L. Li and M. Spratling, “Data augmentation alone can improve adver-\nsarial training,” in ICLR, 2023.\n[128] M. Bayer, M.-A. Kaufhold, and C. Reuter, “A survey on data augmen-\ntation for text classification,” ACM Computing Surveys, vol. 55, no. 7,\npp. 1–39, 2022.\n[129] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmen-\ntation for deep learning,” Journal of Big Data, vol. 6, no. 60, 2019.\n[130] K. Ding, Z. Xu, H. Tong, and H. Liu, “Data augmentation for deep\ngraph learning: A survey,” ACM SIGKDD Explorations Newsletter,\nvol. 24, no. 2, 2022.\n[131] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang, and H. Xu, “Time\nseries data augmentation for deep learning: A survey,” in IJCAI, 2021,\npp. 1–8.\n[132] B. Li, Y. Hou, and W. Che, “Data augmentation approaches in natural\nlanguage processing: A survey,” AI Open, vol. 3, pp. 71–90, 2022.\n[133] T. DeVries and G. W. Taylor, “Dataset augmentation in feature space,”\nICLR Workshop, 2017.\n[134] P. Li, D. Li, W. Li, S. Gong, Y. Fu, and T. M. Hospedales, “A simple\nfeature augmentation for domain generalization,” in ICCV, 2021, pp.\n8886–8895.\n[135] M. Ye, J. Shen, X. Zhang, P. C. Yuen, and S.-F. Chang, “Augmentation\ninvariant and instance spreading feature for softmax embedding,” IEEE\nTPAMI, vol. 44, no. 2, pp. 924–939, 2022.\n[136] P. Chu, X. Bian, S. Liu, and H. Ling, “Feature space augmentation for\nlong-tailed data,” in ECCV, 2020, pp. 694–710.\n[137] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards\ndeep learning models resistant to adversarial attacks,” in ICLR, 2018.\n[138] T. Bai and J. Luo, “Recent advances in adversarial training for\nadversarial robustness,” in IJCAI, 2021, pp. 4312–4321.\n[139] S. Lee, H. Kim, and J. Lee, “Graddiv: Adversarial robustness of\nrandomized neural networks via gradient diversity regularization,”\nTPAMI, vol. 45, no. 2, pp. 2645–2651, 2023.\n[140] H. Lee, S. J. Hwang, and J. Shin, “Self-supervised label augmentation\nvia input transformations,” in ICML, 2020, pp. 5714–5724.\n[141] I. Elezi, A. Torcinovich, S. Vascon, and M. Pelillo, “Transductive label\naugmentation for improved deep network learning,” in ICPR, 2018, pp.\n1432–1437.\n[142] F. Huang, L. Zhang, Y. Zhou, and X. Gao, “Adversarial and isotropic\ngradient augmentation for image retrieval with text feedback,” IEEE\nTMM, pp. 1–12, 2022.\n[143] N. Chawla, K. Bowyer, L. Hall, and W. Kegelmeyer, “Smote: synthetic\nminority over-sampling technique,” Journal of Artificial Intelligence\nResearch, vol. 16, no. 1, pp. 321–357, 2002.\n[144] A. Telikani, A. H. Gandomi, K.-K. R. Choo, and J. Shen, “A cost-\nsensitive deep learning-based approach for network traffic classifica-\ntion,” IEEE TNSE, vol. 19, no. 1, pp. 661–670, 2022.\n[145] D. Dablain, B. Krawczyk, and N. V. Chawla, “Deepsmote: Fusing deep\nlearning and smote for imbalanced data,” IEEE TNNLS, vol. 34, no. 9,\npp. 6390–6404, 2023.\n[146] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-\nPaz, and Y. Bengio, “Manifold mixup: Better representations by\ninterpolating hidden states,” in ICML, 2019, pp. 6438–6447.\n[147] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nin NeurIPS, 2014.\n[148] J. Gui, Z. Sun, Y. Wen, D. Tao, and J. Ye, “A review on generative\nadversarial networks: Algorithms, theory, and applications,” IEEE\nTKDE, vol. 35, no. 4, pp. 3313–3332, 2023.\n[149] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi,\n“Bagan: Data augmentation with balancing gan,” arXiv:1803.09655,\n2018.\n[150] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, and S.-H. Lai,\n“Auggan: Cross domain adaptation with gan-based data augmentation,”\nin ECCV, 2018.\n[151] Z. Yang, Y. Li, and G. Zhou, “Ts-gan: Time-series gan for sensor-based\nhealth data augmentation,” ACM TOCH, vol. 4, no. 2, pp. 1–21, 2022.\n[152] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, “Diffusion models: A comprehensive survey\nof methods and applications,” ACM Computing Surveys, 2023.\n[153] C. Xiao, S. X. Xu, and K. Zhang, “Multimodal data augmentation for\nimage captioning using diffusion models,” arXiv:2305.01855, 2023.\n[154] P. McNamee and K. Duh, “An extensive exploration of back-translation\nin 60 languages,” in Findings of ACL, 2023, pp. 8166–8183.\n[155] H. Dong, J. Zhang, D. McIlwraith, and Y. Guo, “I2t2i: Learning text\nto image synthesis with textual data augmentation,” in ICIP, 2017, pp.\n2015–2019.\n24\n[156] D. Lu, Z. Wang, T. Wang, W. Guan, H. Gao, and F. Zheng, “Set-level\nguidance attack: Boosting adversarial transferability of vision-language\npre-training models,” in ICCV, 2023.\n[157] Y. Yin, J. Kaddour, X. Zhang, Y. Nie, Z. Liu, L. Kong, and Q. Liu,\n“Ttida: Controllable generative data augmentation via text-to-text and\ntext-to-image models,” arXiv:2304.08821, 2023.\n[158] M. Pagliardini, G. Manunza, M. Jaggi, M. I. Jordan, and T. Chav-\ndarova, “Improving generalization via uncertainty driven perturba-\ntions,” arXiv:2202.05737, 2022.\n[159] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment:\nPractical automated data augmentation with a reduced search space,”\narXiv:1909.13719, 2019.\n[160] Z. Mai, G. Hu, D. Chen, F. Shen, and H. T. Shen, “Metamixup:\nLearning adaptive interpolation policy of mixup with metalearning,”\nIEEE TNNLS, vol. 33, no. 7, pp. 3050–3064, 2021.\n[161] T. Qin, Z. Wang, K. He, Y. Shi, Y. Gao, and D. Shen, “Automatic\ndata augmentation via deep reinforcement learning for effective kidney\ntumor segmentation,” in ICASSP, 2020, pp. 1419–1423.\n[162] K. Nishi, Y. Ding, A. Rich, and T. Hollerer, “Augmentation strategies\nfor learning with noisy labels,” in CVPR, 2021, pp. 8022–8031.\n[163] Y. Li, G. Hu, Y. Wang, T. Hospedales, N. M. Robertson, and Y. Yang,\n“Differentiable automatic data augmentation,” in ECCV, A. Vedaldi,\nH. Bischof, T. Brox, and J.-M. Frahm, Eds., 2020, pp. 580–595.\n[164] X. Chen, Y. Zhou, D. Wu, W. Zhang, Y. Zhou, B. Li, and W. Wang,\n“Imagine by reasoning: A reasoning-based implicit semantic data\naugmentation for long-tailed classification,” in AAAI, Online, February\n2022, pp. 356–364.\n[165] X. Zhou and O. Wu, “Implicit counterfactual data augmentation for\ndeep neural networks,” arXiv:2304.13431, 2023.\n[166] B. Li, F. Wu, S.-N. Lim, S. Belongie, and K. Q. Weinberger, “On\nfeature normalization and data augmentation,” in CVPR, 2021, pp.\n12 383–12 392.\n[167] D. LeJeune, R. Balestriero, H. Javadi, and R. G. Baraniuk, “Implicit ru-\ngosity regularization via data augmentation,” arXiv:1905.11639, 2019.\n[168] H. Guo, Y. Mao, and R. Zhang, “Mixup as locally linear out-of-\nmanifold regularization,” AAAI, pp. 3714–3722, 2019.\n[169] C. F. G. D. Santos and J. P. Papa, “Avoiding overfitting: A survey\non regularization methods for convolutional neural networks,” ACM\nComputing Surveys, vol. 54, no. 10, pp. 1–25, 2022.\n[170] C.-H. Lin, C. Kaushik, E. L. Dyer, and V. Muthukumar, “The good,\nthe bad and the ugly sides of data augmentation: An implicit spectral\nregularization perspective,” arXiv:2210.05021, 2022.\n[171] S. Chen, E. Dobriban, and J. Lee, “A group-theoretic framework for\ndata augmentation,” Journal of Machine Learning Research, vol. 21,\nno. 1, pp. 9885–9955, 2020.\n[172] A. Jeddi, M. J. Shafiee, M. Karg, C. Scharfenberger, and A. Wong,\n“Learn2perturb: An end-to-end feature perturbation learning to improve\nadversarial robustness,” in CVPR, 2020.\n[173] M. Shu, Z. Wu, M. Goldblum, and T. Goldstein, “Encoding robustness\nto image style via adversarial feature perturbations,” in NeurIPS, 2021,\npp. 28 042–28 053.\n[174] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and\nS. Kumar, “Long-tail learning via logit adjustment,” in ICLR, 2021.\n[175] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning imbal-\nanced datasets with label-distribution-aware margin loss,” in NeurIPS,\n2019, pp. 1567–1578.\n[176] M. Li, F. Su, O. Wu, and J. Zhang, “Class-level logit perturbation,”\nIEEE TNNLS, 2023.\n[177] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\ning the inception architecture for computer vision,” in CVPR, 2016, pp.\n2818–2826.\n[178] M. Goibert and E. Dohmatob, “Adversarial robustness via label-\nsmoothing,” arXiv:1906.11567, 2019.\n[179] J. Lienen and E. H¨ullermeier, “From label smoothing to label relax-\nation,” in AAAI, 2021, pp. 8583–8591.\n[180] A. Orvieto, H. Kersting, F. Proske, F. Bach, and A. Lucchi, “Anticor-\nrelated noise injection for improved generalization,” in ICML, 2022,\npp. 17 094–17 116.\n[181] D. Wu, S.-T. Xia, and Y. Wang, “Adversarial weight perturbation helps\nrobust generalization,” in NeurIPS, 2020, pp. 2958–2969.\n[182] J. Wang, Y. Liu, and B. Li, “Reinforcement learning with perturbed\nrewards,” in AAAI, 2020, pp. 6202–6209.\n[183] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabi-\nnovich, “Training deep neural networks on noisy labels with bootstrap-\nping,” in ICLR Workshop, 2015.\n[184] P. Benz, C. Zhang, A. Karjauv, and I. S. Kweon, “Universal adversarial\ntraining with class-wise perturbations,” in ICME, 2021, pp. 1–6.\n[185] Y. Wang, J. Fei, H. Wang, W. Li, T. Bao, L. Wu, R. Zhao, and Y. Shen,\n“Balancing logit variation for long-tailed semantic segmentation,” in\nCVPR, 2023, pp. 19 561–19 573.\n[186] A. Shafahi, M. Najibi, Z. Xu, J. Dickerson, L. S. Davis, and T. Gold-\nstein, “Universal adversarial training,” in CVPR, 2017, pp. 5636–5643.\n[187] A. Chaubey, N. Agrawal, K. Barnwal, K. K. Guliani, and P. Mehta,\n“Universal adversarial perturbations: A survey,” arXiv:2005.08087,\n2020.\n[188] T. Wu, Q. Huang, Z. Liu, Y. Wang, and D. Lin, “Distribution-balanced\nloss for multi-label classification in long-tailed datasets,” in ECCV,\n2020, pp. 162–178.\n[189] W. Zhou, X. Hou, Y. Chen, M. Tang, X. Huang, X. Gan, and Y. Yang,\n“Transferable adversarial perturbations,” in ECCV, 2018, pp. 452–467.\n[190] X. Wei, J. Zhu, S. Yuan, and H. Su, “Sparse adversarial perturbations\nfor videos,” in AAAI, 2019, pp. 8973–8980.\n[191] Y. Zhu, Y. Ye, M. Li, J. Zhang, and O. Wu, “Investigating annotation\nnoise for named entity recognition,” Neural Comput. Appl., vol. 35,\nno. 1, pp. 993–1007, 2023.\n[192] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple frame-\nwork for contrastive learning of visual representations,” in ICML, 2020,\npp. 1597–1607.\n[193] M. Naseer, S. Khan, M. Hayat, F. S. Khan, and F. Porikli, “A self-\nsupervised approach for adversarial robustness,” in CVPR, 2020, pp.\n262–271.\n[194] Z. Zhang, S.-h. Zhong, and Y. Liu, “Ganser: A self-supervised data\naugmentation framework for eeg-based emotion recognition,” IEEE\nTAC, 2022.\n[195] S. Li, K. Gong, C. H. Liu, Y. Wang, F. Qiao, and X. Cheng, “Metasaug:\nMeta semantic augmentation for long-tailed visual recognition,” in\nCVPR, 2021, pp. 5212–5221.\n[196] F. Qiao and X. Peng, “Uncertainty-guided model generalization to\nunseen domains,” in CVPR, 2021, pp. 6790–6800.\n[197] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le,\n“Autoaugment: Learning augmentation policies from data,” in CVPR,\n2019.\n[198] T. Niu and M. Bansal, “Automatically learning data augmentation\npolicies for dialogue tasks,” in EMNLP, 2019.\n[199] G. Apruzzese, M. Andreolini, M. Marchetti, A. Venturi, and M. Cola-\njanni, “Deep reinforcement adversarial learning against botnet evasion\nattacks,” IEEE TNSE, vol. 17, no. 4, pp. 1975–1987, 2020.\n[200] B. Lin, Y. Zhu, Y. Long, X. Liang, Q. Ye, and L. Lin, “Adversarial\nreinforced instruction attacker for robust vision-language navigation,”\nIEEE TPAMI, vol. 44, no. 10, pp. 7175–7189, 2022.\n[201] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, and G. Bontempi,\n“Credit card fraud detection: A realistic modeling and a novel learning\nstrategy,” IEEE TNNLS, vol. 29, no. 8, pp. 3784–3797, 2017.\n[202] Y. Zhang, P. Zhao, Q. Wu, B. Li, J. Huang, and M. Tan, “Cost-\nsensitive portfolio selection via deep reinforcement learning,” IEEE\nTKDE, vol. 34, no. 1, pp. 236–248, 2022.\n[203] D. Gan, J. Shen, B. An, M. Xu, and N. Liu, “Integrating tanbn with\ncost sensitive classification algorithm for imbalanced data in medical\ndiagnosis,” Computers & Industrial Engineering, vol. 140, p. 106266,\n2020.\n[204] Y. Dong, J. Ma, S. Wang, C. Chen, and J. Li, “Fairness in graph mining:\nA survey,” IEEE TKDE, pp. 1–22, 2023.\n[205] W. Wang, F. Feng, X. He, L. Nie, and T. Chua, “Denoising implicit\nfeedback for recommendation,” in WSDM, 2021, pp. 373–381.\n[206] T. Castells, P. Weinzaepfel, and J. Revaud, “Superloss: A generic loss\nfor robust curriculum learning,” in NeurIPS, 2020, pp. 1–12.\n[207] S. Zhang, Z. Li, S. Yan, X. He, , and J. Sun, “Distribution alignment:\nA unified framework for long-tail visual recognition,” in CVPR, 2021,\npp. 2361–2370.\n[208] K. R. M. Fernando and C. P. Tsokos, “Dynamically weighted balanced\nloss: Class imbalanced learning and confidence calibration of deep\nneural networks,” IEEE TNNLS, vol. 33, no. 7, pp. 2940–2951, 2022.\n[209] E. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh,\nS. Sagawa, P. Liang, and C. Finn, “Just train twice: Improving group\nrobustness without training group information,” in ICML, 2021, pp.\n6781–6792.\n[210] J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, and M. Kankanhalli,\n“Geometry-aware instance-reweighted adversarial training,” in ICLR,\n2021.\n[211] L. Jiang, D. Meng, T. Mitamural, and A. G. Hauptmann, “Easy samples\nfirst: Self-paced reranking for zero-example multimedia search,” in\nACM MM, 2014, pp. 547–556.\n[212] L. Jiang, D. Meng, S. Yu, Z. Lan, S. Shan, and A.-G. Hauptmann,\n“Self-paced learning with diversity,” in NeurIPS, 2014, pp. 2078–2086.\n25\n[213] D. Zhang, D. Meng, C. Li, L. Jiang, Q. Zhao, and J. Han, “A self-\npaced multiple-instance learning framework for co-saliency detection,”\nin ICCV, 2015, pp. 594–602.\n[214] P. Soviany, R. T. Ionescu, P. Rota, and N. Sebe, “Curriculum learning:\nA survey,” IJCV, vol. 130, no. 6, pp. 1526–1565, 2022.\n[215] C.\nSantiagoa,\nC.\nBarataa,\nM.\nSasdellib,\nG.\nCarneirob,\nand\nJ. C.Nasciment, “Low: Training deep neural networks by learning\noptimal sample weights,” Pattern Recognition, vol. 110, no. 1, pp. 1–\n12, 2021.\n[216] P. Soviany, “Curriculum learning with diversity for supervised com-\nputer vision tasks,” in ICML Workshop, 2020.\n[217] X. Zhou and O. Wu, “Which samples should be learned first: Easy or\nhard?” IEEE TNNLS, pp. 1–15, 2023.\n[218] W. Zhang, Y. Wang, and Y. Qiao, “Metacleaner: Learning to hallucinate\nclean representations for noisy-labeled visual recognition,” in CVPR,\nJune 2019, pp. 7373–7382.\n[219] C. Northcutt, L. Jiang, and I. Chuang, “Confident learning: Estimating\nuncertainty in dataset labels,” JAIR, vol. 70, pp. 1373–1411, 2021.\n[220] Z. Han, Z. Liang, F. Yang, L. Liu, L. Li, Y. Bian, P. Zhao, B. Wu,\nC. Zhang, and J. Yao, “Umix: Improving importance weighting for\nsubpopulation shift via uncertainty-aware mixup,” in NeurIPS, 2022,\npp. 37 704–37 718.\n[221] T. Liu and D. Tao, “Classification with noisy labels by importance\nreweighting,” IEEE TPAMI, vol. 38, no. 3, p. 447–461, 2023.\n[222] Y. Fan, R. He, J. Liang, and B. Hu, “Self-paced learning: An implicit\nregularization perspective,” in AAAI, 2017.\n[223] X. Gu, X. Yu, Y. Yang, J. Sun, and Z. Xu, “Adversarial reweighting\nfor partial domain adaptation,” in NeurIPS, 2021, pp. 14 860–14 872.\n[224] M. Yi, L. Hou, L. Shang, X. Jiang, Q. Liu, and Z.-M. Ma, “Reweighting\naugmented samples by minimizing the maximal expected loss,” in\nICLR, 2021.\n[225] M. Ren, W. Zeng, B. Yang, and R. Urtasun, “Learning to reweight\nexamples for robust deep learning,” in ICML, 2018, pp. 4334–4343.\n[226] Q. Zhao, J. Shu, X. Yuan, Z. Liu, and D. Meng, “A probabilistic\nformulation for meta-weight-net,” IEEE TNNLS, vol. 34, no. 3, pp.\n1194–1208, 2023.\n[227] N. N. Trung, L. N. Van, and T. H. Nguyen, “Unsupervised domain\nadaptation for text classification via meta self-paced learning,” in\nCOLING, 2022, pp. 4741–4752.\n[228] J. Wei, X. Xu, Z. Wang, and G. Wang, “Meta self-paced learning for\ncross-modal matching,” in ACM MM, 2021, pp. 3835–3843.\n[229] S. Li, W. Ma, J. Zhang, C. H. Liu, J. Liang, and G. Wang, “Meta-\nreweighted regularization for unsupervised domain adaptation,” IEEE\nTKDE, vol. 35, no. 3, pp. 2781–2795, 2023.\n[230] F. Zhou, J. Li, C. Xie, F. Chen, L. Hong, R. Sun, and Z. Li,\n“Metaaugment: Sample-aware data augmentation policy learning,” in\nAAAI, 2021, pp. 11 097–11 105.\n[231] Y. Ge, M. Rahmani, A. Irissappane, J. Sepulveda, J. Caverlee,\nand F. Wang, “Automated data denoising for recommendation,”\narXiv:2305.07070, 2023.\n[232] T. Fang, N. Lu, G. Niu, and M. Sugiyama, “Rethinking importance\nweighting for deep learning under distribution shift,” in NeurIPS, 2020,\npp. 11 996–12 007.\n[233] T. Wang, J. Y. Zhu, A. Torralba, and A. A. Efros, “Dataset distillation,”\narXiv:1811.10959, 2018.\n[234] S. Lei and D. Tao, “A comprehensive survey of dataset distillation,”\narXiv:2301.05603, 2023.\n[235] N.\nSachdeva\nand\nJ.\nMcAuley,\n“Data\ndistillation:\nA\nsurvey,”\narXiv:2301.04272v1, 2023.\n[236] B. Zhao, K. R. Mopuri, and H. Bilen, “Dataset condensation with\ngradient matching,” in ICLR, 2021.\n[237] Z. Deng and O. Russakovsky, “Remember the past: Distilling datasets\ninto addressable memories for neural networks,” in NeurIPS, 2022.\n[238] N. Loo, R. Hasani, A. Amini, and D. Rus, “Efficient dataset distillation\nusing random feature approximation,” in NeurIPS, 2022.\n[239] Y. Zhou, E. Nezhadarya, and J. Ba, “Dataset distillation using neural\nfeature regression,” in NeurIPS, 2022.\n[240] B. Zhao and H. Bilen, “Dataset condensation with differentiable\nsiamese augmentation,” in ICML, 2021, pp. 12 674–12 685.\n[241] J.-H. Kim, J. Kim, S. J. Oh, S. Yun, H. Song, J. Jeong, J.-W. Ha,\nand H. O. Song, “Dataset condensation via efficient synthetic-data\nparameterization,” in ICML, 2022.\n[242] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu,\n“Dataset distillation by matching training trajectories,” in CVPR, 2022.\n[243] J. Cui, R. Wang, S. Si, and C.-J. Hsieh, “Scaling up dataset distillation\nto imagenet-1k with constant memory,” arXiv:2211.10586, 2022.\n[244] B. Zhao and H. Bilen, “Dataset condensation with distribution match-\ning,” in WACV, 2023.\n[245] K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang,\nH. Bilen, X. Wang, and Y. You, “Cafe: Learning to condense dataset\nby aligning features,” in CVPR, 2022, pp. 12 196–12 205.\n[246] X. Zhou, R. Pi, W. Zhang, Y. Lin, Z. Chen, and T. Zhang, “Probabilistic\nbilevel coreset selection,” in ICML, 2022, pp. 27 287–27 302.\n[247] B. Zhao and H. Bilen, “Synthesizing informative training samples with\ngan,” in NeurIPS Workshop, 2022.\n[248] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos.,\n“Beyond neural scaling laws: beating power law scaling via data\npruning,” in NeurIPS, 2022.\n[249] Z. Qin, K. Wang, Z. Zheng, J. Gu, X. Peng, D. Zhou, and Y. You,\n“Infobatch: Lossless training speed up by unbiased dynamic data\npruning,” arXiv:2303.04947, 2023.\n[250] S. Liu, K. Wang, X. Yang, J. Ye, and X. Wang, “Dataset distillation\nvia factorization,” in NeurIPS, 2022.\n[251] K. Meding, L. M. S. Buschoff, R. Geirhos, and F. A. Wichmann,\n“Trivial or impossible – dichotomous data difficulty masks model\ndifferences (on imagenet and beyond),” in ICLR, 2022.\n[252] V. Feldman and C. Zhang, “What neural networks memorize and why:\nDiscovering the long tail via influence estimation,” in NeurIPS, 2020,\npp. 2881–2891.\n[253] C. G. Northcutt, T. Wu, and I. L. Chuang, “Learning with confident\nexamples: Rank pruning for robust classification with noisy labels,”\narXiv:1705.01936, 2017.\n[254] V. Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and\nG. Ramakrishnan, “Learning from less data: A unified data subset\nselection and active learning framework for computer vision,” in IEEE\nWACV, 2019, pp. 1289–1299.\n[255] Y. Yang, H. Kang, and B. Mirzasoleiman, “Towards sustainable learn-\ning: Coresets for data-efficient deep learning,” in ICML, 2023.\n[256] B. Mirzasoleiman, J. Bilmes, and J. Leskovec, “Coresets for data-\nefficient training of machine learning models,” in ICML, 2020, pp.\n6950–6960.\n[257] Z. Liu, H. Jin, T.-H. Wang, K. Zhou, and X. Hu, “Divaug: Plug-in\nautomated data augmentation with explicit diversity maximization,” in\nICCV, 2021, pp. 4762–4770.\n[258] K. J. Joseph, K. Singh, and V. N. Balasubramanian, “Submodular batch\nselection for training deep neural networks,” in IJCAI, 2019, pp. 2677–\n2683.\n[259] W. Li, G. Dasarathy, and V. Berisha, “Regularization via structural\nlabel smoothing,” in AISTATS, 2020, pp. 1453–1463.\n[260] C. Meister, E. Salesky, and R. Cotterell, “Generalized entropy regu-\nlarization or: There’s nothing special about label smoothing,” in ACL,\n2020.\n[261] J. Chai and X. Wang, “Fairness with adaptive weights,” in ICML, 2022,\npp. 2853–2866.\n[262] Q. Song, H. Jin, X. Huang, and X. Hu, “Multi-label adversarial\nperturbations,” in ICDM.\nIEEE, 2018, pp. 1242–1247.\n[263] S. Hu, L. Ke, X. Wang, and S. Lyu, “Tkml-ap: Adversarial attacks to\ntop-k multi-label learning,” in ICCV, 2021, pp. 7649–7657.\n[264] L. Kong, W. Luo, H. Zhang, Y. Liu, and Y. Shi, “Evolutionary\nmultilabel adversarial examples: An effective black-box attack,” IEEE\nTAI, vol. 4, no. 3, pp. 562–572, 2023.\n[265] H. Cao, W. Yang, and H. T. Ng, “Mitigating exposure bias in gram-\nmatical error correction with data augmentation and reweighting,” in\nEACL, 2023, pp. 2123–2135.\n[266] Q. Liu, M. Kusner, and P. Blunsom, “Counterfactual data augmentation\nfor neural machine translation,” in NAACL, 2021, pp. 187–197.\n[267] Y. Zang, C. Huang, and C. C. Loy, “Fasa: Feature augmentation and\nsampling adaptation for long-tailed instance segmentation,” in ICCV,\n2021.\n[268] Y. Zhao, W. Chen, X. Tan, K. Huang, and J. Zhu, “Adaptive logit\nadjustment loss for long-tailed visual recognition,” in AAAI, 2022, pp.\n3472–3480.\n[269] J.-H. Xue and P. Hall, “Why does rebalancing class-unbalanced data\nimprove auc for linear discriminant analysis?” IEEE TPAMI, vol. 37,\nno. 5, pp. 1109–1112, 2015.\n[270] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,\n“Adversarial examples are not bugs, they are features,” in NeurIPS,\n2019.\n[271] D. Elreedy, A. F. Atiya, and F. Kamalov, “A theoretical distribution\nanalysis of synthetic minority oversampling technique (smote) for\nimbalanced learning,” Machine Learning, 2023.\n[272] Y. Yang, K. Zha, Y. Chen, H. Wang, and D. Katabi, “Delving into deep\nimbalanced regression,” in ICML), 2021, pp. 11 842–11 851.\n26\n[273] M. Li, Y.-m. Cheung, and Y. Lu, “Long-tailed visual recognition via\ngaussian clouded logit adjustment,” in CVPR, 2022, pp. 6929–6938.\n[274] J. Ren, M. Zhang, C. Yu, and Z. Liu, “Balanced mse for imbalanced\nvisual regression,” in CVPR, 2022, pp. 7926–7935.\n[275] Y. Xie, M. Qiu, H. Zhang, L. Peng, and Z. Chen, “Gaussian distribution\nbased oversampling for imbalanced data classification,” IEEE TKDE,\nvol. 32, no. 2, pp. 667–679, 2022.\n[276] Y. Yang and Z. Xu, “Rethinking the value of labels for improving\nclass-imbalanced learning,” in NeurIPS, 2020, pp. 19 290–19 301.\n[277] H. Liu, J. Z. HaoChen, A. Gaidon, and T. Ma, “Self-supervised learning\nis more robust to dataset imbalance,” arXiv:2110.05025, 2022.\n[278] L. Jin, D. Lang, and N. Lei, “An optimal transport view of class-\nimbalanced visual recognition,” International Journal of Computer\nVision, 2023.\n[279] Q. Dong, S. Gong, and X. Zhu, “Imbalanced deep learning by minority\nclass incremental rectification,” IEEE TPAMI, vol. 41, no. 6, pp. 1367–\n1381, 2019.\n[280] K. A. Wang, N. S. Chatterji, S. Haque, and T. Hashimoto, “Is\nimportance weighting incompatible with interpolating classifiers?” in\nICLR, 2022.\n[281] R. Xu, X. Zhang, Z. Shen, T. Zhang, and P. Cui, “A theoretical\nanalysis on independence-driven importance weighting for covariate-\nshift generalization,” in ICML, 2022, pp. 24 803–24 829.\n[282] D. Chen, Y. Shen, H. Zhang, and P. H. Torr, “Zero-shot logit adjust-\nment,” in IJCAI, 2022.\n[283] M. Qraitem, K. Saenko, and B. A. Plummer, “Bias mimicking: A\nsimple sampling approach for bias mitigation,” in CVPR, 2023, pp.\n20 311–20 320.\n[284] Y. Roh, K. Lee, S. Whang, and C. Suh, “Sample selection for fair and\nrobust training,” in NeurIPS, 2021, pp. 815–827.\n[285] A. Zhang, F. Liu, W. Ma, Z. Cai, X. Wang, and T.-S. Chua, “Boosting\ncausal discovery via adaptive sample reweighting,” in ICLR, 2023.\n[286] Y. Jang, T. Zhao, S. Hong, and H. Lee, “Adversarial defense via\nlearning to generate diverse attacks,” in ICCV, 2019.\n[287] I. Hounie, L. F. O. Chamon, and A. Ribeiro, “Automatic data augmen-\ntation via invariance-constrained learning,” in ICML, 2023, pp. 13 410–\n13 433.\n[288] A. Blum and K. Stangl, “Recovering from biased data: Can fairness\nconstraints improve accuracy?” arXiv:1912.01094, 2019.\n[289] T. Doan, M. Abbana Bennani, B. Mazoure, G. Rabusseau, and\nP. Alquier, “A theoretical analysis of catastrophic forgetting through\nthe ntk overlap matrix,” in AISTATS, 2021, pp. 1072–1080.\n[290] S. Chatterjee and P. Zielinski, “On the generalization mystery in deep\nlearning,” arXiv:2203.10036, 2022.\n[291] A. Katharopoulos and F. Fleuret, “Biased importance sampling for deep\nneural network training,” arXiv:1706.00043, 2017.\n[292] Z. Wang, H. Zhu, Z. Dong, X. He, and S.-L. Huang, “Less is better:\nUnweighted data subsampling via influence function,” in AAAI, 2020,\npp. 6340–6347.\n[293] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, and C. Re, “A kernel\ntheory of modern data augmentation,” in ICML, 2019, pp. 1528–1537.\n[294] J. Wu and J. He, “A unified framework for adversarial attacks on multi-\nsource domain adaptation,” IEEE TKDE, pp. 1–12, 2022.\n[295] J. Gilmer, N. Ford, N. Carlini, and E. Cubuk, “Adversarial examples\nare a natural consequence of test error in noise,” in ICML, 2019, p.\n2280–2289.\n[296] M. Yi, L. Hou, J. Sun, L. Shang, X. Jiang, Q. Liu, and Z. Ma,\n“Improved ood generalization via adversarial training and pretraing,”\nin ICML, 2021, pp. 11 987–11 997.\n[297] J. Peck, J. Roels, B. Goossens, and Y. Saeys, “Lower bounds on the\nrobustness to adversarial perturbations,” in NeurIPS, 2017.\n[298] Y. Xu, Y. Xu, Q. Qian, H. Li, and R. Jin, “Towards understanding label\nsmoothing,” arXiv:2006.11653, 2017.\n[299] D. Meng, Q. Zhao, and L. Jiang, “A theoretical understanding of self-\npaced learning,” Information Sciences, vol. 414, pp. 319–328, 2017.\n[300] D. Weinshall, G. Cohen, and D. Amir, “Curriculum learning by transfer\nlearning: Theory and experiments with deep networks,” in ICML, 2018,\npp. 5238–5246.\n[301] D. Zhu, B. Lei, J. Zhang, Y. Fang, R. Zhang, Y. Xie, and D. Xu,\n“Rethinking data distillation: Do not overlook calibration,” in ICCV,\n2023.\n[302] T. Dong, B. Zhao, and L. Lyu, “Privacy for free: How does dataset\ncondensation help privacy?” arXiv:2206.00240, 2022.\n[303] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, “Revisiting knowledge\ndistillation via label smoothing regularization,” in CVPR, 2020.\n[304] A. D. Assis, L. C. B. Torres, L. R. G. Ara´ujo, V. M. Hanriot, and\nA. P. Braga, “Neural networks regularization with graph-based local\nresampling,” IEEE Access, vol. 9, pp. 50 727–50 737, 2021.\n[305] F. R. Cordeiro, V. Belagiannis, I. Reid, and G. Carneiro, “Propmix:\nHard sample filtering and proportional mixup for learning with noisy\nlabels,” in BMVC, 2021.\n[306] K. Yang, Y. Sun, J. Su, F. He, X. Tian, F. Huang, T. Zhou, and D. Tao,\n“Adversarial auto-augment with label preservation: A representation\nlearning principle guided approach,” in NeurIPS, 2022, pp. 22 035–\n22 048.\n[307] J. Li, R. Socher, and S. C. Hoi, “Dividemix: Learning with noisy labels\nas semi-supervised learning,” in ICLR, 2020.\n[308] J. Shu, X. Yuan, D. Meng, and Z. Xu, “Cmw-net: Learning a class-\naware sample weighting mapping for robust deep learning,” IEEE\nTPAMI, vol. 45, no. 10, 2023.\n[309] Z. Zhang and T. Pfister, “Learning fast sample re-weighting without\nreward data,” in ICCV, 2021, pp. 725–734.\n[310] X. Wang, E. Kodirov, Y. Hua, and N. M. Robertson, “Derivative\nmanipulation for general example weighting,” arXiv:1905.11233, 2020.\n[311] E. Yang, T. Liu, C. Deng, W. Liu, and D. Tao, “Distillhash: Unsu-\npervised deep hashing by distilling data pairs,” in CVPR, 2019, pp.\n2946–2955.\n[312] B. Mirzasoleiman, K. Cao, and J. Leskovec, “Coresets for robust\ntraining of deep neural networks against noisy labels,” in NeurIPS,\n2020, pp. 11 465–11 477.\n[313] S. Mindermann, J. M. Brauner, M. T. Razzak, M. Sharma, A. Kirsch,\nW. Xu, B. H¨oltgen, A. N. Gomez, A. Morisot, S. Farquhar, and Y. Gal,\n“Prioritized training on points that are learnable, worth learning, and\nnot yet learnt,” in ICML, 2022, pp. 15 630–15 649.\n[314] X. Hu, Y. Zeng, X. Xu, S. Zhou, and L. Liu, “Robust semi-supervised\nclassification based on data augmented online elms with deep features,”\nKnowledge-Based Systems, vol. 229, p. 107307, 2021.\n[315] S. Kim, S. Bae, and S.-Y. Yun, “Coreset sampling from open-set for\nfine-grained self-supervised learning,” in CVPR, 2023, pp. 7537–7547.\n[316] C. Bellinger, R. Corizzo, and N. Japkowicz, “Remix: Calibrated\nresampling for class imbalance in deep learning,” arXiv:2012.02312,\n2020.\n[317] F. Du, P. Yang, Q. Jia, F. Nan, X. Chen, and Y. Yang, “Global and\nlocal mixture consistency cumulative learning for long-tailed visual\nrecognitions,” in CVPR, June 2023, pp. 15 814–15 823.\n[318] O. Pooladzandi, D. Davini, and B. Mirzasoleiman, “Adaptive second\norder coresets for data-efficient machine learning,” in ICML, 2022, pp.\n17 848–17 869.\n[319] G. Zhao, G. Li, Y. Qin, and Y. Yu, “Improved distribution matching\nfor dataset condensation,” in CVPR, 2023, pp. 7856–7865.\n[320] K. Kim and H. S. Lee, “Probabilistic anchor assignment with iou\nprediction for object detection,” in ECCV, 2020, pp. 355–371.\n[321] T. Takase, R. Karakida, and H. Asoh, “Self-paced data augmentation\nfor training neural networks,” Neurocomputing, vol. 442, pp. 296–306,\n2021.\n[322] S. A. Siddiqui, N. Rajkumar, T. Maharaj, D. Krueger, and S. Hooker,\n“Metadata archaeology: Unearthing data subsets by leveraging training\ndynamics,” in ICLR, 2023.\n[323] X. Peng, F.-Y. Wang, and L. Li, “Mixgradient: A gradient-based re-\nweighting scheme with mixup for imbalanced data streams,” Neural\nNetworks, vol. 161, pp. 525–534, 2023.\n[324] W. Xu, L. Jiang, and C. Li, “Resampling-based noise correction\nfor crowdsourcing,” Journal of Experimental & Theoretical Artificial\nIntelligence, vol. 33, no. 6, pp. 985–999, 2020.\n[325] C. Huang and S. Zhang, “Generative dataset distillation,” in BigCom,\n2021, pp. 212–218.\n[326] A. Krizhevsky, “Learning multiple layers of features from tiny images.”\nMIT, 2009.\n[327] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang, “Learning from\nmassive noisy labeled data for image classification,” in CVPR, 2015,\np. 2691–2699.\n[328] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,\n“Reading digits in natural images with unsupervised feature learning,”\nin NeurIPSW, 2011.\n[329] W. Li, L. Wang, W. Li, E. Agustsson, and L. Van Gool, “Webvi-\nsion database: Visual learning and understanding from web data,”\narXiv:1708.02862, 2017.\n[330] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard,\nH. Adam, P. Perona, and S. Belongie, “The inaturalist species classifi-\ncation and detection dataset,” in CVPR, 2018, pp. 8769–8778.\n27\n[331] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu,\n“Largescale long-tailed recognition in an open world,” in CVPR, 2019,\np. 2537–2546.\n[332] S. P. Karimireddy, L. He, and M. Jaggi, “Byzantine-robust learning on\nheterogeneous datasets via bucketing,” arXiv:2006.09365, 2022.\n[333] N. Tsilivis, J. Su, and J. Kempe, “Can we achieve robustness from data\nalone?” arXiv:2006.09365, 2022.\n[334] Z. Liu, P. Wei, J. Jiang, W. Cao, J. Bian, and Y. Chang, “Mesa: Boost\nensemble imbalanced learning with meta-sampler,” in NeurIPS, 2020,\npp. 14 463–14 474.\n[335] Y. Li, X. Liu, and F. Liu, “Adaptive noisy data augmentation for reg-\nularization of undirected graphical models,” arXiv:1810.04851, 2019.\n[336] C. Yu, B. Han, L. Shen, J. Yu, C. Gong, M. Gong, and T. Liu,\n“Understanding robust overfitting of adversarial training and beyond,”\nin ICML, 2022, pp. 25 595–25 610.\n[337] J. Xiao, Y. Fan, R. Sun, J. Wang, and Z.-Q. Luo, “Stability analysis\nand generalization bounds of adversarial training,” in NeurIPS, 2022,\npp. 15 446–15 459.\n[338] J.\nAn,\nL.\nYing,\nand\nY.\nZhu,\n“Why\nresampling\noutperforms\nreweighting for correcting sampling bias with stochastic gradients,”\narXiv:2009.13447, 2021.\n[339] R. M¨uller, S. Kornblith, and G. E. Hinton, “When does label smoothing\nhelp?” in NeurIPS, 2019.\n[340] B. Chen, L. Ziyin, Z. Wang, and P. P. Liang, “An investigation of how\nlabel smoothing affects generalization,” arXiv:2010.12648, 2020.\n[341] Z. Qian, K. Huang, Q.-F. Wang, and X.-Y. Zhang, “A survey of robust\nadversarial training in pattern recognition: Fundamental, theory, and\nmethodologies,” Pattern Recognition, vol. 131, p. 108889, 2022.\n[342] K. H. R. Chan, Y. Yu, C. You, H. Qi, J. Wright, and Y. Ma, “Redunet:\na white-box deep network from the principle of maximizing rate\nreduction,” Journal of Machine Learning Research, vol. 23, no. 1, pp.\n4907–5009, 2022.\n[343] Y. Wen, G. Jerfel, R. Muller, M. W. Dusenberry, J. Snoek, B. Lakshmi-\nnarayanan, and D. Tran, “Combining ensembles and data augmentation\ncan harm your calibration,” in ICLR, 2021.\n[344] M. Lukasik, S. Bhojanapalli, A. K. Menon, and S. Kumar, “Does label\nsmoothing mitigate label noise?” in ICML, 2020.\n[345] M. Du, N. Liu, and X. Hu, “Techniques for interpretable machine\nlearning,” Communications of the ACM, vol. 63, pp. 68–77, 2019.\n[346] C. R. Pochimireddy, A. T. Siripuram, and S. S. Channappayya, “Can\nperceptual guidance lead to semantically explainable adversarial per-\nturbations?” IEEE J-STSP, pp. 1–11, 2023.\n[347] G. Zelaya and C. Vladimiro, “Towards explaining the effects of data\npreprocessing on machine learning,” in ICDE, 2019, pp. 2086–2090.\n[348] E. Mosqueira-Rey, E. Hern´andez-Pereira, D. Alonso-R´ıos, J. Bobes-\nBascar´an, and ´Angel Fern´andez-Leal, “Human-in-the-loop machine\nlearning: a state of the art,” Journal of Machine Learning Research,\nvol. 56, pp. 3005–3054, 2023.\n[349] K. M. Collins, U. Bhatt, W. Liu, V. Piratla, I. Sucholutsky, B. Love, and\nA. Weller, “Human-in-the-loop mixup,” in UAI, 2023, pp. 454–464.\n[350] E. Wallace, P. Rodriguez, S. Feng, I. Yamada, and J. Boyd-Graber,\n“Trick me if you can: Human-in-the-loop generation of adversarial\nexamples for question answering,” TACL, vol. 7, pp. 387–401, 2019.\n[351] C. Agarwal, D. D’souza, and S. Hooker, “Estimating example difficulty\nusing variance of gradients,” in CVPR, 2022, pp. 10 368–10 378.\n[352] Z.-F. Wu, T. Wei, J. Jiang, C. Mao, M. Tang, and Y.-F. Li, “Ngc: a\nunified framework for learning with open-world noisy data,” in ICCV,\n2021, p. 62–71.\n[353] Z. Jiang, T. Chen, T. Chen, and Z. Wang, “Improving contrastive\nlearning on imbalanced data via open-world sampling,” in NeurIPS,\n2021, pp. 5997–6009.\n[354] T. Gokhale, S. Mishra, M. Luo, B. S. Sachdeva, and C. Baral,\n“Generalized but not robust? comparing the effects of data modification\nmethods on out-of-domain generalization and adversarial robustness,”\nin ACL Findings, 2022.\n[355] W. X. Zhao, K. Zhou, and et al., “A survey of large language models,”\narXiv:2303.18223, 2023.\n[356] L. Wei, Z. Jiang, W. Huang, and L. Sun, “Instructiongpt-4: A 200-\ninstruction paradigm for fine-tuning minigpt-4,” arXiv:2308.12067,\n2023.\n[357] M. Jiang, Y. Ruan, S. Huang, S. Liao, S. Pitis, R. Grosse, and J. Ba,\n“Calibrating language models via augmented prompt ensembles,” in\nICML, 2023.\n[358] T. Baltruˇsaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine\nlearning: A survey and taxonomy,” IEEE TPAMI, vol. 41, no. 2, pp.\n423–443, 2019.\n[359] S. Ge, Z. Jiang, Z. Cheng, C. Wang, Y. Yin, and Q. Gu, “Learning\nrobust multi-modal representation for multi-label emotion recognition\nvia adversarial masking and perturbation,” in The Web Conference,\n2023, pp. 1510–1518.\n[360] V. A. Trinh, H. Salami Kavaki, and M. I. Mandel, “Importantaug: A\ndata augmentation agent for speech,” in ICASSP, 2022, pp. 8592–8596.\n[361] M. Li, X. Zhang, C. Thrampoulidis, J. Chen, and S. Oymak, “Auto-\nbalance: Optimized loss functions for imbalanced data,” in NeurIPS,\n2021, pp. 3163–3177.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-10-25",
  "updated": "2023-10-25"
}