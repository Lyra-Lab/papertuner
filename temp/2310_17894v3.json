{
  "id": "http://arxiv.org/abs/2310.17894v3",
  "title": "Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey",
  "authors": [
    "Weixu Zhang",
    "Yifei Wang",
    "Yuanfeng Song",
    "Victor Junqiu Wei",
    "Yuxing Tian",
    "Yiyan Qi",
    "Jonathan H. Chan",
    "Raymond Chi-Wing Wong",
    "Haiqin Yang"
  ],
  "abstract": "The emergence of natural language processing has revolutionized the way users\ninteract with tabular data, enabling a shift from traditional query languages\nand manual plotting to more intuitive, language-based interfaces. The rise of\nlarge language models (LLMs) such as ChatGPT and its successors has further\nadvanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language\ninterfaces for tabular data querying and visualization, which allow users to\ninteract with data using natural language queries. We introduce the fundamental\nconcepts and techniques underlying these interfaces with a particular emphasis\non semantic parsing, the key technology facilitating the translation from\nnatural language to SQL queries or data visualization commands. We then delve\ninto the recent advancements in Text-to-SQL and Text-to-Vis problems from the\nperspectives of datasets, methodologies, metrics, and system designs. This\nincludes a deep dive into the influence of LLMs, highlighting their strengths,\nlimitations, and potential for future improvements. Through this survey, we aim\nto provide a roadmap for researchers and practitioners interested in developing\nand applying natural language interfaces for data interaction in the era of\nlarge language models.",
  "text": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n1\nNatural Language Interfaces for Tabular Data\nQuerying and Visualization: A Survey\nWeixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan,\nRaymond Chi-Wing Wong, and Haiqin Yang, Senior Member, IEEE\nAbstract—The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift\nfrom traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models\n(LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization,\nwhich allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques\nunderlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural\nlanguage to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis\nproblems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence\nof LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a\nroadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the\nera of large language models.\nIndex Terms—Natural Language Interface, Text-to-SQL, Text-to-Visualization, Semantic Parsing, Large Language Models\n✦\n1\nINTRODUCTION\nT\nABULAR, or structured, data form the backbone of many\nfields in today’s digital age, including business, health-\ncare, and scientific research [56], [80]. However, the ability\nto interact effectively and efficiently with vast amounts\nof structured data to extract valuable insights remains a\ncrucial challenge. Traditional methods of interaction, such\nas querying with structured query languages or manual\nplotting a visualization, often require a significant degree\nof technical expertise, thereby limiting their accessibility to\na wider user base [2].\nWith the emergence of natural language processing tech-\nnologies, the way we interact with structured data is begin-\nning to shift. These technologies enable the development\nof Natural Language Interfaces (NLIs), making tabular data\nquerying and visualization more intuitive and accessible.\nThrough these interfaces, users can extract information from\ndatabases or generate visual representations of data using\nnatural language queries and commands [46], [92]. This shift\ntowards language-based interfaces marks a significant stride\nW. Zhang (Xi’an Jiaotong University, email: weixu zhang@stu.xjtu.edu.cn),\nY. Wang (University of Toronto, email: yifeii.wang@mail.utoronto.ca), and\nY. Tian (Xidian University, email: tianyxxx@stu.xidian.edu.cn) are interns at\nInternational Digital Economy Academy (IDEA), Shenzhen, China\nY.\nSong\nis\nwith\nWeBank\nCo.,\nLtd.,\nShenzhen,\nChina.\nEmail:\nyf-\nsong@webank.com\nV. J. Wei and R. C. Wong are with Department of Computer Science and\nEngineering, Hong Kong University of Science and Technology (HKUST),\nHong Kong. Email: {victorwei,raywong}@cse.ust.hk\nY. Qi is with IDEA, Shenzhen, China. Email: qiyiyan@idea.edu.cn\nJ. H. Chan is with Innovative Cognitive Computing (IC2) Research Center at\nSchool of Information Technology, King Mongkut’s University of Technology\nThonburi. Email: jonathan@sit.kmutt.ac.th\nH.\nYang\n(corresponding\nauthor)\nis\naffiliated\nwith\nIDEA.\nEmail:\nhqyang@ieee.org\ntowards simplifying data interaction, making it more user-\nfriendly and accessible to non-technical users.\nThe foundational technologies powering these language-\nbased interfaces are rooted in semantic parsing tasks, which\ntransform natural language queries into formal represen-\ntations tailored for execution on structured databases [49].\nWhile various formal languages and functional representa-\ntions have been introduced for this purpose, such as Prolog,\nDatalog, and FunQL, two are particularly dominant in tabu-\nlar data interaction: SQL for data querying and visualization\nspecifications for data visualization. SQL has been the de\nfacto standard for querying relational databases for decades,\noffering comprehensive operations to retrieve and manipu-\nlate data. Visualization specifications provide a structured\nway to represent complex visualizations, making them an\nintegral part of the data visualization process. Notably, data\nquerying and visualization are two of the most critical\ntechnical directions in tabular data interaction, and they are\noften intertwined in practical applications. Querying is fre-\nquently a sub-step in the visualization process, as users need\nto first retrieve the relevant data before visualizing it. In\nreal-world scenarios, such as generating data reports, these\ntwo tasks are commonly used together to extract insights\nand present them in a visually appealing and informative\nmanner. For example, a business analyst might use natural\nlanguage to query a sales database for ”total revenue by\nproduct category in the last quarter”, and then request a\n”bar chart showing the revenue breakdown” to include in\ntheir quarterly report. Given their importance, widespread\nuse, and interconnected nature, this survey will focus on\nthese two types of representations, delving deep into the\nchallenges and advancements in the tasks of translating\nnatural language into SQL (Text-to-SQL) and visualization\nspecifications (Text-to-Vis).\narXiv:2310.17894v3  [cs.CL]  20 May 2024\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n2\nThe development of these two semantic parsing tasks\nhas evolved significantly over the years, driven by advance-\nments in machine learning and natural language process-\ning techniques. Early approaches often rely on rule-based\nor template-based systems [1], [49] and shallow parsing\ntechniques. However, these methods struggle with complex\nqueries and visualizations and are sensitive to the specific\nphrasing of the user’s input. Introducing neural networks\nand deep learning methods brings about a significant leap\nin performance. These methods, often based on sequence-\nto-sequence models [52], can capture more complex patterns\nin the data and are more robust to variations in the input.\nHowever, they still require substantial amounts of training\ndata and struggle with out-of-domain queries [14]. The rise\nof Pretrained Language Models (PLMs), such as BERT [16],\nT5 [84], GPT [78], marks a turning point in the field. With\ntheir ability to leverage pre-training on vast amounts of text\ndata, PLMs have shown remarkable success in a wide range\nof natural language processing tasks, including Text-to-SQL\nand Text-to-Vis. Recently, the advent of Large Language\nModels (LLMs) such as ChatGPT and the exploration of\nprompt engineering techniques have opened new avenues\nfor developing more effective and user-friendly NLIs for\ndata interaction.\nThe interdisciplinary research on NLIs for tabular data\nquerying and visualization incorporates multiple research\naspects, such as natural language processing and data\nmining, with advancements often following diverse and\ndistinct trajectories. Despite its increasing importance, no\nsingle study has comprehensively reviewed the problem of\nsemantic parsing for both querying and visualization tasks\nin a systematic and unified manner. Prior surveys have\nprimarily focused on early approaches and subsequent deep\nlearning developments in querying [14], [46], [52] and visu-\nalization [1], [92] separately, without offering a consolidated\nview of these intertwined domains. Furthermore, to the best\nof our knowledge, no existing surveys cover the recent ad-\nvancements by LLMs in these areas. The profound influence\nof LLMs on NLIs for data querying and visualization is\na rapidly growing area that requires more attention and\nexploration.\nThis survey aims to fill these gaps by offering a com-\nprehensive overview of NLIs for tabular data querying and\nvisualization, emphasizing their interconnected nature and\nthe impact of LLMs. By providing a holistic perspective, we\nwill thoroughly analyze the relationship between these two\ncritical tasks, exploring how they can be unified from the\nperspective of semantic parsing and how advances in one\ntask can inform the other. We will also examine the practical\napplications where these tasks are used in conjunction and\nhow NLIs can streamline data-driven workflows. Further-\nmore, we will provide an in-depth review of the latest LLM-\nbased approaches, discussing how they have pushed the\nstate-of-the-art and opened up new possibilities for more\npowerful and user-friendly NLIs. We source references from\nkey journals and conferences over the past two decades,\nspanning Natural Language Processing, Human-Computer\nInteraction, Data Mining, and Visualization. Our search is\nguided by terms such as ”Natural Language Interface”,\n”Visualization”, and ”Text-to-SQL”, and we also explore\ncited publications to capture foundational contributions.\nInterface\nTabular \nDatabase\nSQL\nVis Specification\nFeedback & \nRefinement\nNatural \nLanguage \nQuestion\nInput Reception \n& Pre-processing\nQuery \nTranslation\nQuery \nExecution\nOutput \nPresentation\nQuerying \nData\nVisualization\nCharts\nOutput\nInput\nFig. 1. Schematic representation of natural language interfaces for tab-\nular data querying and visualization\nThrough this survey, we aim to address a set of critical\nresearch questions:\n• How have NLIs for tabular data querying and visual-\nization evolved over time?\n• What is the relationship between these two tasks, and\nhow can they be unified from the perspective of semantic\nparsing?\n• How have recent advancements, especially LLMs, in-\nfluenced the field?\n• What are the inherent strengths and weaknesses of\nexisting methods?\nBy drawing upon an extensive literature review and\nanalysis, we aim to provide informed and insightful an-\nswers to these questions. We will delve into functional\nrepresentations, datasets, evaluation metrics, and system ar-\nchitectures, particularly emphasizing the influence of LLMs.\nOur goal is to present a clear and succinct overview of the\ncurrent state of the art, emphasizing existing approaches’\nstrengths and limitations while exploring potential avenues\nfor future enhancements.\n2\nBACKGROUND AND FRAMEWORK\n2.1\nContext\nThe need for Natural Language Interfaces to process tab-\nular data arises from the growing importance of data-\ndriven decision-making across various industries, making\nit a crucial ability to interact with data efficiently and\nintuitively. Natural language interfaces simplify access to\nvaluable insights by enabling a wider user base, including\nthose without technical expertise, to query and visualize\nstructured data [2].\nFigure 1 shows the workflow of natural language in-\nterfaces for tabular data querying and visualization, where\nthe user provides input in the form of a natural language\nquestion targeting a specific structured database. The inter-\nface pre-processes this input, translating it into functional\nrepresentations, such as SQL queries for data extraction or\nvisualization specifications for chart generation. Executing\nthe SQL queries retrieves relevant data from the database,\nand the visualization specifications produce corresponding\ncharts. The resulting output, whether raw data or visuals,\nis then presented to the user, who can provide feedback or\nfurther refine their query. This streamlined process enables\nusers to extract data insights and generate visuals without\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n3\nWhat were the total sales in the \nlast quarter?\nSELECT SUM(sales) FROM D \nWHERE quarter = ‘Q4'\nQuarter\nProduct Category\nSales\nQ1\nElectronics\n3732\nQ1\nClothing\n3607\nQ2\nElectronics\n1763\nQ2\nClothing\n2731\nQ3\nElectronics\n1277\nQ3\nClothing\n2778\nQ4\nElectronics\n4544\nQ4\nClothing\n3648\nTable: sales data for a company\nTabular Database\nExecution Engine\nParsing Model\nResult：\nNLQ：\nQuery:\nSQL\nShow me a bar chart of sales of \nelectronics by quarter.\n{\"mark\": \"bar\", \"encoding\": {\"x\": \n\"quarter\", \"y\": \"sales\" }}\nVis\n8192\nFig. 2. Example of the process of translating natural language queries to SQL and visualization specifications on sales data. A textual query about\nquarterly sales is parsed into an SQL command to fetch numerical data, and a request for a sales visualization is transformed into the corresponding\nbar chart specification.\ndiving into the complexities of databases or visualization\ntools merely by posing their questions.\nThe practical application of natural language interfaces\nfor tabular data querying and visualization is exemplified in\nseveral existing tools. Microsoft’s Power BI [83], for instance,\nincludes a feature called Q&A which allows users to ask\nnatural language questions about their data and receive an-\nswers in the form of charts or tables. This feature leverages\nadvanced natural language processing to understand the\nquestion and generate appropriate visualizations, thereby\nsimplifying the process of data exploration for users. Sim-\nilarly, Tableau [101], a popular data visualization tool, in-\ncludes a feature named Ask Data. Users can type a ques-\ntion, and the system generates an answer through a data\nvisualization. These applications underscore the potential\nand impact of natural language interfaces in enhancing the\naccessibility and usability of data interaction.\n2.2\nProblem Definition\nIn the context of natural language interfaces for tabular data,\nthe central problem is to parse a natural language query\ninto a functional representation that can be executed on a\nstructured database.\nFormally, given the input x = {q, s} with a natural\nlanguage query q and database schema s containing tables\nT = {ti}|T |\ni=1 and columns C = {qi}|C|\ni=1 for each table ti ∈T,\nthe task of the semantic parser P is to translate q into a\nfunctional expression e. Once the functional expression e is\ngenerated, it can be executed on the structured database D\nby an execution engine E to produce a result r, represented\nas:\nE(e, D) →r\nThe functional expression e and result r vary depending\non the specific task:\n• Text-to-SQL. The functional expression e is an SQL\nquery that manages and queries data held in relational\ndatabases [132]. The result r obtained through the execution\nof the SQL query is a piece or set of precise data.\n• Text-to-Vis. The functional expression e is a visual-\nization specification (e.g., Vega-Lite, D3.js) that determines\nhow data should be presented visually, often in the form\nof charts, graphs, or other graphical elements [92]. The\nresult r obtained through the execution of the visualization\nspecification is a graphical representation such as a pie chart,\nbar graph, or scatter plot.\n• Prolog and Datalog. The functional expression e is a\nset of rules and facts that define relationships and logic for\noperations on the data. The result r can be in any structure\nrepresenting the outcome of the query.\n• FunQL. The functional expression e is an intermediate\nquery language that maps natural language constructs into\nstructured queries, emphasizing the relationships between\nentities. The result r is a database query language.\nThe overall process can be seen as a translation from\na natural language query q to a result r, facilitated by a\nsemantic parser P and an execution engine E.\n2.3\nFramework\nThe natural language interfaces for tabular data querying\nand visualization encompass a variety of components, each\nplaying a crucial role in the technology framework, as\nshown in Fig 3.\n• Datasets. Datasets play a vital role in training and\nevaluating the performance of these interfaces. Datasets can\nbe single-turn, where a single query is posed without any\nprior context, or multi-turn, where a series of queries are\nposed in a conversational manner. There are also various\ntypes of datasets designed to evaluate different aspects of\nthe systems, such as their ability to handle complex queries,\nout-of-domain queries, and more. Additionally, datasets can\nvary in terms of the domain they cover, such as business,\nhealthcare, or scientific data, each posing unique challenges\nfor natural language interfaces.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n4\n• Approaches. The approaches to building natural lan-\nguage interfaces have evolved over time. Early approaches\nwere rule-based, using pre-defined rules to translate natural\nlanguage queries into functional representations. With the\nadvent of neural networks, sequence-to-sequence models\nbecame popular, providing more flexibility in handling\ndiverse queries. The rise of pre-trained language models,\nsuch as BERT [16] and GPT [78], marked a significant\nadvancement in this field. Recently, the advent of LLMs\nlike ChatGPT, and the exploration of prompt engineering\ntechniques, have opened new avenues for the development\nof more effective natural language interfaces for data in-\nteraction. These advancements have not only improved the\naccuracy of these systems but have also made them more\naccessible to a wider range of users.\n• Evaluation Metrics. Evaluation metrics are used to mea-\nsure the performance of these interfaces. These can be string-\nbased, comparing the generated functional representation to\na ground truth, or execution-based, comparing the result of\nexecuting the generated representation on the database to\nthe expected result. Manual evaluation is also sometimes\nused to assess aspects like the system’s usability. User sat-\nisfaction and usability are important evaluation criteria, as\nthey directly impact the adoption and effectiveness of these\ninterfaces in real-world scenarios.\n• System Design. System architecture is a crucial com-\nponent of natural language interfaces which involves the\nunderlying mechanisms that translate user queries into ac-\ntionable outputs. The architectural paradigms, ranging from\nrule-based to end-to-end designs, provide varied solutions\nand trade-offs in terms of flexibility, interpretability, and\naccuracy. We also provide user-centric analysis to provide\nsuggestions on system-selection for individual and pro-\nfessional users, taking into account their specific needs,\ntechnical proficiency, and the characteristics of their data\nenvironment.\nEach of these components contributes to the effective-\nness and usability of natural language interfaces for tabular\ndata querying and visualization. The subsequent sections of\nthis survey will delve into these components in more detail,\ndiscussing their role, the various methods and technologies\nused, and the recent advancements in each area.\n3\nDATASETS\n3.1\nText-to-SQL Datasets\n3.1.1\nExisting Benchmarks\nText-to-SQL datasets have evolved significantly over time,\nadapting to the growing complexity of the field. Early\ndatasets are single-domain, focusing on simple, context-\nspecific queries. As the field progressed, cross-domain\ndatasets emerged, featuring diverse schemas and queries\nacross multiple domains. The introduction of multi-turn\nconversational datasets added another layer of complexity,\nrequiring the understanding of inter-query dependencies\nwithin a conversation. The most recent advancement is\nthe emergence of multilingual datasets, which extend\nthe challenge to handling queries in multiple languages.\nResearchers are also exploring complex scenarios such as\nambiguous queries, queries requiring external knowledge,\nand queries involving temporal and spatial reasoning.\nThis evolution reflects the progress and the expanding\nchallenges in the Text-to-SQL domain. Table 1 presents\na comprehensive overview of various Text-to-SQL and\nText-to-Vis datasets.\nSingle Domain. The early phase of Text-to-SQL research\nis marked by single-domain datasets, which focus on\nhandling queries within a specific context. Academic [56]\nand Advising [25] are examples of early single-domain\ndatasets. The ATIS dataset [13], [82] and GeoQuery [128]\nare notable for their focus on flight information and U.S.\ngeography respectively. Datasets like Yelp and IMDB [117],\nScholar [47], and Restaurants [107] are also developed\naround this time, each catering to queries pertaining to their\nrespective domains. In recent years, the development of\nsingle-domain datasets has continued with the introduction\nof\nSEDE\n[39]\nand\nMIMICSQL\n[111].\nThese\ndatasets\nrepresent the ongoing efforts to explore and address more\ncomplex and diverse queries within specific domains.\nCross Domain. Following the single-domain datasets, the\nfocus shifts to cross-domain datasets, which widen the\nscope of the Text-to-SQL task by including queries from\nmultiple domains. A pivotal dataset marking this shift is\nWikiSQL [132]. It offers a rich collection of 80,654 natural\nlanguage inquiries paired with SQL queries. These pairs\ncorrespond to SQL tables extracted from a vast set of 26,521\nWikipedia tables. The dataset’s uniqueness lies in its exten-\nsive coverage of tables and its capacity to challenge models\nto adapt to novel queries and table schemas. Another monu-\nmental contribution to this arena is the Spider dataset [126].\nThis dataset encompasses 10,181 natural language questions\nfrom 138 varied domains. Its diversity and inclusion of\nintricate queries make it a tougher challenge compared to\nits predecessors.\nThe Spider dataset has inspired the creation of several\nvariants, each designed to test specific capabilities of\nText-to-SQL\nmodels.\nFor\ninstance,\nSpider-SYN\n[27]\ntweaks\nthe\noriginal\nSpider\nquestions\nby\nsubstituting\nschema-related\nterms\nwith\ntheir\nsynonyms,\nelevating\nthe schema linking challenge. Spider-DK\n[28] infuses\ndomain-specific\nknowledge\ninto\nquestions,\nprobing\nmodels’ domain knowledge comprehension. Variants like\nSpider-CG [26] and Spider-SSP [91] focus on models’\ngeneralization\nabilities\nthrough\ndiverse\nstrategies,\nsuch\nas\nsub-sentence\nsubstitutions\nand\ncompositional\ngeneralization, respectively. Dr. Spider [8] serves as a\ndiagnostic tool, introducing variations in the original\nSpider dataset across multiple dimensions. Lastly, Spider-\nrealistic [15] enhances task complexity by removing direct\ncolumn name mentions from questions, demanding an\nimproved robustness from models.\nMulti-turn. As the field of Text-to-SQL expanded to\nencompass\nmore\ncomplex\ninteractions,\nthe\nneed\nfor\ndatasets\nthat\ncould\nsimulate\nmulti-turn\nconversations\nbecame\napparent.\nTo\ncater\nto\nthis,\nvarious\ndatasets\nemphasizing\ncontext-driven\nText-to-SQL\ninteractions\nhave been developed. SParC [127] is a prominent cross-\ndomain dataset boasting approximately 4.3k sequences\nof questions, which cumulatively constitute over 12k\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n5\nNatural Language Interfaces for Tabular Data Querying and Visualization\nFormal Language\nSQL Query\nVis Specification\nOthers\nDataset\nSingle Domain\nCross Domain\nOthers\nApproach\nTraditional\nNeural Network\nFoundation Model\nEvaluation Metric\nString-based\nExecution-based\nManual Evaluation\nSystem Design\nRule-based\nParsing-based\nMulti-stage\nEnd-to-end\nFig. 3. Framework for Natural Language Interfaces in Tabular Data Querying and Visualization. The technology framework comprises various pivotal\ncomponents: functional representation, dataset, approach, evaluation metric, and system design.\nquestion-SQL pairings. What’s unique about SParC is that\neach of its question sequences evolves from an original\nquestion in Spider, with subsequent questions intricately\nwoven in. Similarly, the CoSQL dataset [125], established\nunder the Wizard-of-Oz framework, stands out as the\nfirst large-scale, cross-domain conversational Text-to-SQL\ncollection. It houses nearly 3k dialogues, translating to\nover 30k dialogue turns and 10k associated SQL queries.\nThrough these dialogues, the dataset replicates a scenario\nwhere\nannotators,\nposing\nas\ndatabase\nusers,\nutilize\nnatural language to extract database responses. Another\nnoteworthy contribution is the CHASE dataset [37]. This\ndataset introduces a large-scale, context-sensitive Chinese\nText-to-SQL\ncollection,\nfeaturing\n5,459\ninterconnected\nquestion sequences and 17,940 individual questions paired\nwith SQL queries. Collectively, these datasets push the\nboundaries in the Text-to-SQL domain, emphasizing more\nfluid, dialogue-centric database interactions and offering\ndiverse challenges for research exploration.\nMultilingual. As the Text-to-SQL field expands globally,\nthe need for multilingual datasets has become increasingly\napparent. Several datasets have been developed to address\nthis need, offering benchmarks in different languages and\nthereby broadening the scope of Text-to-SQL research.\nCSpider [73], TableQA [103] and DuSQL [110] extend the\nText-to-SQL task to Chinese, introducing a new linguistic\nchallenge. ViText2SQL [77] broadens the field further with a\nVietnamese Text-to-SQL dataset, pushing models to handle\nthe complexities of the Vietnamese language. Similarly,\nPortugueseSpider [48] extends the task to Portuguese,\nrequiring models to translate Portuguese queries into SQL.\nThese multilingual datasets represent a significant stride\ntowards developing Text-to-SQL systems that can cater to\na global, multilingual user base, thereby democratizing\naccess to data across linguistic boundaries.\nKnowledge Grounding. Recent advancements in Text-to-\nSQL research have seen a growing emphasis on knowledge-\nintensive benchmarks, reflecting the need for models that\ncan handle real-world analysis scenarios. Such benchmarks,\nlike Spider-dk [28], extends the Spider dataset to focus more\non domain knowledge, reflecting the need for models to\nunderstand and incorporate domain-specific knowledge in\ntheir translations. Another datset, knowSQL [23], prioritize\nknowledge grounding or commonsense reasoning, helping\nexperts make informed decisions. A most recent benchmark\nis BIRD [59], specifically tailored for expansive database-\nanchored Text-to-SQL tasks. What sets BIRD apart is its\nemphasis on the values within databases. It underscores\nnovel hurdles, such as inconsistencies in database content,\nthe imperative of bridging external knowledge with nat-\nural language queries and database content, as well as\nthe efficiency of SQL, particularly when dealing with vast\ndatabases. These knowledge-intensive datasets represent a\nsignificant stride towards developing Text-to-SQL systems\nthat can handle complex, real-world scenarios, bridging the\ngap between academic study and practical application.\n3.2\nText-to-Vis Datasets\n3.2.1\nExisting Benchmarks\nText-to-Vis datasets generally follow the same format as\nText-to-SQL datasets with a set of tabular data and (NLQ,\nVis) pairs for each database. The progression was similarly\na transition from single to cross-domain datasets though\nseveral\nText-to-Vis\ndatasets\npiggybacked\nText-to-SQL\nbenchmarks.\nSingle\nDomain. During early development stages of\nText-to-Vis interfaces, datasets are generally used as a proof\nof concept. These datasets are each concentrated on one\ndomain that only involves queries within a set range of\ncontext. The dataset by Gao et al [54] was developed by\nasking test subjects to pose natural utterances by looking\nat several human-generated visualizations with the goal to\ngain certain information. The dataset by Kumar et al. [54]\nfocused on crime data and the queries are to gain insight to\npolice force allocations.\nCross Domain. Srinivasan et al. [99] collected queries\nacross 3 datasets. They provided thorough analyses on\nthe classification of Text-to-Vis natural language queries.\nnvBench [68] is the largest and most used Text-to-Vis\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n6\nTABLE 1\nStatistics for Text-to-SQL and Text-to-Vis datasets sorted by ascending years and grouped by common main features. The details of each dataset\ninclude the number of NLQs (#Query), databases (#Database), domains (#Domain), tables per database (#T/DB) and the language of the queries.\nDatasets\n#Query\n#Database\n#Domain\n#T/DB\nLanguage\nMain Features\nText-to-SQL Datasets\nATIS (Hemphill et al., 1990; Dahl et al., 1994)\n5,280\n1\n1\n32\nEnglish\nSingle Domain\nGeoQuery (Zelle and Mooney, 1996)\n877\n1\n1\n6\nEnglish\nRestaurants (Tang and Mooney, 2000)\n378\n1\n1\n3\nEnglish\nAcademic (Li and Jagadish, 2014)\n196\n1\n1\n15\nEnglish\nScholar (Iyer et al., 2017)\n817\n1\n1\n7\nEnglish\nIMDB (Yaghmazadeh et al., 2017)\n131\n1\n1\n16\nEnglish\nYelp (Yaghmazadeh et al., 2017)\n128\n1\n1\n7\nEnglish\nAdvising (Finegan-Dollak et al., 2018)\n3,898\n1\n1\n10\nEnglish\nMIMICSQL (Wang et al., 2020)\n10,000\n1\n1\n5\nEnglish\nSEDE (Hazoom et al., 2021)\n12,023\n1\n1\n29\nEnglish\nWikiSQL (Zhong et al., 2017)\n80,654\n26,521\n-\n1\nEnglish\nCross Domain\nSpider (Yu et al., 2018)\n10,181\n200\n138\n5\nEnglish\nSquall (Shi et al., 2020)\n11,468\n1,679\n-\n1\nEnglish\nKaggleDBQA (Lee et al., 2021)\n272\n8\n8\n2\nEnglish\nSParC (Yu et al., 2019)\n12,726\n200\n138\n5.1\nEnglish\nMulti-turn\nCoSQL (Yu et al., 2019)\n15,598\n200\n138\n5.1\nEnglish\nCHASE (Guo et al., 2021)\n17,940\n280\n-\n4.6\nChinese\nSpider-SYN (Gan et al., 2021)\n7,990\n166\n-\n5\nEnglish\nRobustness\nSpider-SSP (Shaw et al., 2021)\n3,282\n-\n-\n5\nEnglish\nSpider-realistic (Deng et al., 2021)\n508\n-\n-\n5\nEnglish\nSpider-CG (Gan et al., 2022)\n45,599\n-\n-\n5\nEnglish\nDr. Spider (Chang et al., 2023)\n-\n166\n-\n5\nEnglish\nCSpider (Min et al., 2019)\n10,181\n200\n138\n5\nChinese\nMultilingual\nDuSQL (Wang et al., 2020)\n23,797\n200\n-\n4.1\nChinese\nTableQA (Sun et al., 2020)\n64,891\n6,029\n-\n1\nChinese\nViText2SQL (Nguyen et al., 2020)\n9,691\n166\n-\n5\nVietnamese\nPortugueseSpider (Archanjo Jose et al., 2021)\n9,691\n166\n-\n5\nPortuguese\nPAUQ (Bakshandaeva et al., 2022)\n9,691\n166\n-\n5\nRussian\nSpider-DK (Gan et al., 2021)\n535\n10\n-\n5\nEnglish\nKnowledge Grounding\nknowSQL (Dou et al., 2022)\n25,888\n200\n3\n-\nChinese\nBIRD (Li et al., 2023)\n12,751\n95\n-\n7\nEnglish\nText-to-Vis Datasets\nGao et al., 2015\n10\n3\n-\n1\nEnglish\nSingle Domain\nKumar et al., 2016\n490\n1\n-\n-\nEnglish\nSrinivasan et al., 2021\n893\n3\n-\n1\nEnglish\nnvBench (Luo et al., 2021)\n25750\n153\n105\n5\nEnglish\nCross Domain\nChartDialogs (Shao et al., 2020)\n3284\n-\n-\n-\nEnglish\nMulti-turn\nDial-NVBench (Song et al., 2023)\n4495\n-\n-\n-\nEnglish\nCNvBench (Ge et al., 2023)\n25750\n153\n105\n5\nChinese\nMultilingual\nbenchmark,\ncontaining\n25,750\nnatural\nlanguage\nand\nvisualization pairs from 750 tables over 105 domains. It is\nsynthesized from Text-to-SQL benchmark Spider [126] to\nsupport cross-domain Text-to-Vis task.\nMulti-turn. Due to the large amount of information\nneeded to produce accurate visualizations, it is apparent\nthat not all information may be provided in just one\nround of natural language query. To tackle this issue,\nmulti-turn datasets have been introduced to make several\nrounds\nof\nmodifications\non\nthe\noutput\nvisualization.\nChartDialogs [90] contains 3,284 dialogues and is curated\nfor plotting using matplotlib. Building on the cross-domain\ndataset nvBench, Dial-NVBench [96] was created to target\ndialogue\ninputs.\nThe\ndataset\ncontains\n4,495\ndialogue\nsessions and each is aimed to contain enough information\nso the system can output a suitable visualization.\nMultilingual. While the majority of Text-to-Vis datasets\nare in English, there is a growing need for multilingual\ndatasets to support the development of natural language\ninterfaces for diverse language communities. CNvBench is a\nrecently introduced Chinese Text-to-Vis dataset, which aims\nto bridge the gap between English and Chinese resources\nin this domain. The dataset is constructed by translating\nand localizing the English nvBench dataset, ensuring that\nthe natural language queries are fluent and idiomatic in\nChinese.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n7\nTakeaways for Datasets\n• Text-to-SQL dataset evolution: Single-domain,\nCross-domain\n(Spider),\nMulti-turn\n(SParC),\nMultilingual (CSpider), Knowledge-grounded\n(BIRD).\n• Text-to-Vis dataset evolution: Single-domain,\nCross-domain (nvBench), Multi-turn (ChartDi-\nalogs), Multilingual (CNvBench)\n• Guidelines: Choose based on specific challenges\nand capabilities. Use Spider, nvBench for cross-\ndomain tasks; SParC, CoSQL, ChartDialogs,\nDial-NVBench for multi-turn scenarios.\n4\nAPPROACHES\n4.1\nText-to-SQL Parsing\nThe approaches to the Text-to-SQL task have evolved sig-\nnificantly over time, mirroring the broader developments in\nnatural language processing, as illustrated in the timeline in\nFig 4. Early efforts have focused on rule-based approaches,\nwhere queries are translated into SQL based on a predefined\nset of rules and patterns. The emergence of neural networks\nand the sequence-to-sequence paradigm mark a turning\npoint in Text-to-SQL research. Neural network approaches,\nwhich translate a source sequence (the natural language\nquery) into a target sequence (the SQL query), show a\ngreater capacity to handle the intricacies of natural language\nand the diversity of SQL queries. In recent years, the advent\nof foundation language models like BERT [16] and GPT [78]\nhas opened up new possibilities for the Text-to-SQL task.\nThis evolution in approaches reflects the ongoing efforts to\ndevelop models that can accurately and efficiently translate\nnatural language queries into SQL, handling the challenges\npresented by the variability of natural language and the\ncomplexity of SQL. Table 2 provides a comparative analysis\nof notable approaches in the Text-to-SQL and Text-to-Vis\ndomains.\n4.1.1\nTraditional Stage\nText-to-SQL research began with rule-based approaches,\nwhich were the primary method of handling this task for\nseveral decades. Surveys like [53], [80] have presented the\nwork of this stage in more detailed ways. Early rule-based\nmethods like TEAM [32] and CHAT-80 [112] used interme-\ndiate logical representations, translating natural language\nqueries into logical queries that were independent of the\ndatabase schema, and then converting these logical queries\ninto SQL. However, these methods relied heavily on hand-\ncrafted mapping rules.\nIn the early 2000s, more advanced rule-based meth-\nods were developed. PRECISE [79] utilized an off-the-\nshelf natural language parser to translate queries, but\nits coverage was limited due to the assumption of a\none-to-one correspondence between words in the query\nand database elements. To address this, methods like\nNaLIR [56], ATHENA [87], and SQLizer [117] adopted a\nranking-based approach, finding multiple candidate map-\npings and ranking them based on a score. NaLIR further\nimproved performance by involving user interaction, while\nATHENA leveraged a domain-specific ontology for richer\nsemantic information. SQLizer used an iterative process to\nrefine the logical form of the query. Templar [3] offered an\noptimization technique for mapping and joint path gener-\nation using a query log. Despite their significant improve-\nments, these methods still relied on manually-defined rules,\nwhich limited their ability to handle many variations in\nnatural language.\n4.1.2\nNeural Network Stage\nThe advent of neural networks and the sequence-to-\nsequence\n(Seq2Seq)\nparadigm\nhas\nmarked\na\nturning\npoint in the field of Text-to-SQL. Originally for machine\ntranslation,\nSeq2Seq\nmodels\ncan\nlearn\nintricate\ndata\nmappings, accommodating diverse queries and complex\nSQL structures. Such a model typically uses an encoder\nto process the natural language query and a decoder\nto generate the corresponding SQL query. For a deeper\ndive into neural network-based approaches, readers are\nencouraged to consult prior surveys like [14], [51], [52].\nEncoder. Encoders in the Text-to-SQL context determine\nhow the natural language query and the database schema\nare jointly transformed into a continuous representation that\nthe model can work with. They can broadly be classified into\ntwo categories: sequence-based encoders and graph-based\nencoders.\n• Sequence-based Encoder. Sequence-based encoders form\nthe foundation of many Text-to-SQL systems. They are often\nbased on Recurrent Neural Networks (RNNs), Long Short-\nTerm Memory (LSTM) networks, Gated Recurrent Units\n(GRUs), or Transformer architectures.\nBi-directional\nLong\nShort-Term\nMemory\n(bi-LSTM)\nbased models have been widely used in early Text-to-SQL\nsystems due to their capability to capture dependencies\nin both directions of a sequence. Notable work includes\nTypeSQL [122], which assigns a type to each word in the\nquestion, with a word being an entity from the knowledge\ngraph, a column, or a number. The model then concatenates\nword embeddings and the corresponding type embeddings\nas input to the bi-LSTM, which helps it better encode\nkeywords in questions. Seq2SQL [132], SQLNet [116], and\nIncSQL [94] employ a bi-LSTM to produce a hidden state\nrepresentation for each word in the natural language query.\nFor column headers, a bi-LSTM is also used for each col-\numn name, with the final hidden state used as the initial\nrepresentation for the column. For example, EditSQL [130]\nalso utilizes two separate Bi-LSTMs for encoding the natural\nlanguage questions and the table schema, and then applies\na dot-product attention layer to integrate the two encodings.\nWith the advent of the Transformer architecture, self-\nattention models have gained popularity in the Text-to-SQL\ntask. The original self-attention mechanism is the building\nblock of the Transformer structure, and models like those\ndeveloped by He et al. [40], Hwang et al. [45], and Xie et\nal. [115], have incorporated this mechanism. These models\nleverage the Transformer’s ability to capture dependencies\nregardless of their distance in the sequence, which is espe-\ncially useful for handling complex, non-local dependencies\noften present in the Text-to-SQL task.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n8\n2016\n1980\n2019\nTraditional Stage\nNeural Network Stage\nFoundation Model Stage\nPLM-based\nLLM-based\nRule-based \nText-to-SQL \nparsing \nText-to-Vis \nparsing \nLLM-based\nTemplate-based \nEncoder \nDecoder \n•\nSequence-based\n•\nGraph-based \n•\nMonolithic\n•\nSkeleton-based\n•\nGrammar-based\n•\nExecution-based\n•\nSequence-based\n•\nGraph-based \nEncoder \n•\nMonolithic\n•\nGrammar-based\nDecoder \nFig. 4. Evolution of Text-to-SQL (upper timeline) and Text-to-Vis (lower timeline) approaches over time. The colored rectangles represent different\nstages of the approaches: traditional (blue), neural network (green), and foundation language model (orange). Note that the development of Text-\nto-Vis approaches generally occurred later than similar Text-to-SQL approaches, hence the upper and lower timelines are misaligned.\n• Graph-based Encoder. Graphs are an effective way to\ncapture complex structures, making them particularly suit-\nable for encoding database (DB) schemas, which are rich\nin structural information. Bogin et al. [4] were pioneers\nin using graph representations for DB schemas. They em-\nployed nodes for tables and columns and edges to depict\ntable-column relationships, such as table compositions, and\nprimary and foreign key constraints. These graph structures\nwere then encoded using graph neural networks (GNNs). In\na follow-up study, Bogin et al. introduced Global-GNN [5],\nemphasizing global reasoning to encode the schema, in-\ntegrating question token representations between question\nterms and schema entities. RAT-SQL [108] combined global\nreasoning, structured reasoning, and relation-aware self-\nattention for schema entities and question terms.\nGraphs\nhave\nalso\nbeen\nemployed\nto\nconcurrently\nencode\nquestions\nand\nDB\nschemas.\nCao\net\nal.\nput\nforward\nthe\nLGESQL\n[7]\nmodel\nto\nunearth\nmulti-\nhop\nrelational\nattributes\nand\nsignificant\nmeta-paths.\nS2SQL [44] explored question token syntax’s role in Text-\nto-SQL encoders and introduced a versatile and resilient\ninjection technique. To strengthen the graph method’s\ngeneralization for unfamiliar domains, SADGA [6] crafted\nboth question and schema graphs based on the dependency\nstructure of natural language queries and schema layout,\nrespectively.\nShawdowGNN\n[11]\ncountered\ndomain\ninformation influenced by disregarding table or column\nnames and employing abstract schemas for delexicalized\nrepresentations. Lastly, Hui et al. 2021 [43] designed a\ndynamic graph framework to capture interactions among\nutterances, tokens, and database schemas, leveraging both\ncontext-independent and dependent parsing.\nDecoder. The decoder is a crucial component of the\nsequence-to-sequence paradigm, responsible for generating\nthe SQL query from the encoded representation of the natu-\nral language query and database schema. Broadly, these de-\ncoders can be classified into four categories: monolithic de-\ncoders, skeleton-based decoders, grammar-based decoders,\nand execution-guided decoders.\n• Monolithic Decoder. The Monolithic decoder, influenced\nby advancements in machine translation, primarily utilizes\nRNNs for the sequential generation of SQL commands.\nEarly implementations of this method relied on RNNs to\ncompute the probability of each SQL token, considering\nboth the prior context and previously generated tokens [47].\nThe context from the input is encoded, often using mecha-\nnisms like soft-attention, which emphasizes the most per-\ntinent input components for each token generation. As\nfor representing previously generated tokens, a common\nmethod is to use hidden states from the prior decoder step.\n• Skeleton-based Decoder. Skeleton-based decoders tackle\nthe Text-to-SQL problem by first generating a template\nor skeleton of the SQL query, which is then populated\nwith specific details from the input. This approach can\nhelp manage the complexity of SQL queries by breaking\ndown the generation process into more manageable steps.\nFor example, SQLNet [116] introduced the approach that\nfocuses on filling in the slots in a SQL sketch, aligning\nwith the SQL grammar, rather than predicting both the\noutput grammar and the content. This approach captures\nthe dependency of the predictions, where the prediction\nof one slot is conditioned only on the slots it depends on.\nHydraNet [71] uses a multi-headed selection network for\nsimultaneous generation of different parts of the SQL query.\nIE-SQL and TypeSQL [122] also use a slot-filling approach,\nwhere a pre-defined SQL template is filled in based on\nthe input. COARSE2FINE [20] adopts a two-step coarse-\nto-fine generation process, where an initial rough sketch is\ngenerated and subsequently refined with low-level details\nconditioned on the question and the sketch. RYANSQL [12]\ntakes a recursive approach to yield SELECT statements and\nemploys a sketch-based slot filling for each of the SELECT\nstatements. This approach effectively handles complex SQL\nqueries with nested structures.\n• Grammar-based Decoder. Grammar-based decoders gen-\nerate the SQL query directly from the encoded represen-\ntation of the input, often utilizing SQL grammar rules,\nintermediate representations, or incorporating constraints in\nthe decoding process to ensure the generation of valid SQL\nqueries.\nDecoders utilizing rules aim to reduce the chances of\ngenerating out-of-place tokens or syntactically incorrect\nqueries. By generating a sequence of grammar rules in-\nstead of simple tokens, these models ensure the syntactical\ncorrectness of the generated SQL queries. For example,\nSeq2Tree [19] employs a top-down decoding strategy, gener-\nating logical forms that respect the hierarchical structure of\nSQL syntax. Seq2AST [118] takes this idea further by using\nan abstract syntax tree (AST) for decoding. SyntaxSQL-\nNet [124] adapts this approach to SQL-specific syntax. It\nemploys a tree-based decoder that recursively calls modules\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n9\nTABLE 2\nApproaches for natural language interfaces for database querying and visualization. The approaches are categorized into three stages: Traditional,\nNeural Network, and Foundation Language Model. The ”WikiSQL” and ”Spider” columns report the execution accuracy (EX) and exact match (EM)\non the dev sets of the respective datasets, which are widely used benchmarks for Text-to-SQL tasks. WikiSQL focuses on single-table queries,\nwhile Spider covers more complex multi-table queries. The ”NVBench” column reports the overall accuracy (exact match) on the NVBench\ndataset, a benchmark for Text-to-Vis tasks. ”-” indicates that the metric is not reported or the approach is not evaluated on that dataset.\nApproach\nTask\nWikiSQL\nSpider\nNVBench\nDescription\nKey Features\nEX(%)\nEM(%)\nAcc.(%)\nTraditional Stage\nCHAT-80 (Warren and Pereira, 1982)\nQuery\n-\n-\n-\nEnglish to logic using extraposition grammars\nRule-based\nTEAM (Grosz, 1983)\nQuery\n-\n-\n-\nAdapts to DBs using intermediate logic\nPRECISE (Popescu et al., 2004)\nQuery\n-\n-\n-\nSemantic-enhanced statistical parser\nNaLIR (Li and Jagadish, 2014)\nQuery\n-\n-\n-\nGuided user interaction for complex queries\nATHENA (Saha et al., 2016)\nQuery\n-\n-\n-\nDomain ontology for query translation\nTemplar (Baik et al., 2019)\nQuery\n-\n-\n-\nOptimized keyword mapping and join path\nSQLizer (Yaghmazadeh et al., 2017)\nQuery\n-\n-\n-\nProgram synthesis for logical form refinement\nTemplate-based\nDataTone (Gao et al., 2015)\nVisual\n-\n-\n-\nInteractive widgets for NL ambiguity\nADVISor (Liu et al., 2021)\nVisual\n-\n-\n-\nNL-based annotated visualizations\nNL4DV (Narechania et al., 2021)\nVisual\n-\n-\n-\nNL to analytic specs toolkit\nNeural Network Stage\nSQLNet (Xu et al., 2017)\nQuery\n69.8\n-\n-\nSeq-to-set, column attention\nSeq-based Enc.\nTypeSQL (Yu et al., 2018)\nQuery\n85.5\n-\n-\nType-aware entity and value understanding\nIncSQL (Shi et al., 2018)\nQuery\n87.2\n-\n-\nIncremental seq-to-action SQL construction\nEditSQL (Zhang et al., 2019)\nQuery\n-\n57.6\n-\nQuery generation via editing\nData2Vis (Dibia and Demiralp, 2019)\nVisual\n-\n-\n-\nEnd-to-end neural vis generation\nSeq2Vis (Luo et al., 2021)\nVisual\n-\n-\n1.95\nPre-trained word embeddings for input\nncNet (Luo et al., 2022)\nVisual\n-\n-\n25.78\nVis-aware Transformer optimizations\nMMCoVisNet (Song et al., 2023)\nVisual\n-\n-\n-\nMulti-modal dialogue context understanding\nGNN (Bogin et al., 2019)\nQuery\n-\n40.7\n-\nGraph neural network for DB schema encoding\nGraph-based Enc.\nGlobal-GNN (Bogin et al., 2019)\nQuery\n-\n52.1\n-\nGlobal reasoning for query structure\nRAT-SQL (Wang et al., 2020)\nQuery\n-\n69.7\n-\nRelation-aware schema encoding and linking\nLGESQL (Cao et al., 2021)\nQuery\n-\n75.1\n-\nLine graph encoding for message passing\nSADGA (Cai et al., 2021)\nQuery\n-\n73.1\n-\nDual graph aggregation for question-schema\nShadowGNN (Chen et al., 2021)\nQuery\n-\n72.3\n-\nGraph projection for delexicalized representations\nHui et al., 2021\nQuery\n-\n-\n-\nDynamic graph for contextual relations\nS2SQL (Hui et al., 2022)\nQuery\n-\n76.4\n-\nSyntactic info for question-schema encoding\nRGVisNet (Song et al., 2023)\nVisual\n-\n-\n44.9\nRetrieval-generation for data vis\nCOARSE2FINE (Dong and Lapata, 2018)\nQuery\n79.6\n-\n-\nCoarse-to-fine semantic parsing\nSkeleton-based Dec.\nIE-SQL (Ma et al., 2020)\nQuery\n92.6\n-\n-\nExtraction-linking for slot filling\nHydraNet (Lyu et al., 2020)\nQuery\n92.4\n-\n-\nSeparate column ranking and decoding\nRYANSQL (Choi et al., 2021)\nQuery\n-\n66.6\n-\nSketch-based slot filling with SPCs\nSeq2Tree (Dong et al., 2016)\nQuery\n-\n-\n-\nEncoded repr. for seq-to-tree generation\nGrammar-based Dec.\nSeq2AST (Yin and Neubig, 2017)\nQuery\n-\n-\n-\nTarget syntax as prior knowledge\nSyntaxSQLNet (Yu et al., 2018)\nQuery\n-\n24.8\n-\nSQL-specific syntax tree decoder\nIRNet (Guo et al., 2019)\nQuery\n-\n61.9\n-\nIntermediate repr. bridging NL and SQL\nSmBoP (Rubin and Berant, 2021)\nQuery\n-\n69.5\n-\nSemi-autoregressive bottom-up parsing\nNatSQL (Gan et al., 2021)\nQuery\n-\n73.7\n-\nSimplified IR for complex SQL\nPICARD (Scholak et al., 2021)\nQuery\n-\n75.5\n-\nConstrained autoregressive decoding\nUniSAr (Dou et al., 2022)\nQuery\n91.7\n70.0\n-\nUnified structure-aware autoregressive\nSeq2SQL (Zhong et al., 2017)\nQuery\n60.8\n-\n-\nExecution-guided SQL generation\nExecution-based Dec.\nWang et al., 2018\nQuery\n78.5\n-\n-\nRuntime error correction, ensemble\nSuhr et al., 2020\nQuery\n-\n65.8\n-\nCross-domain semantic parsing evaluation\nFoundation Language Model Stage\nSQLOVA (Hwang et al., 2019)\nQuery\n90.2\n-\n-\nBERT with schema-aware contextualization\nPLM-based\nX-SQL (He et al., 2019)\nQuery\n92.3\n-\n-\nPLM-augmented schema representation\nTaBERT (Yin et al., 2020)\nQuery\n-\n65.2\n-\nJoint text-table pre-training\nBridge (Lin et al., 2020)\nQuery\n-\n71.1\n-\nBERT-contextualized question and schema\nGraPPa (Yu et al., 2021)\nQuery\n82.2\n73.4\n-\nGrammar-guided pre-training\nGAP (Shi et al., 2021)\nQuery\n-\n71.8\n-\nGeneration-augmented pre-training\nUnifiedSKG (Xie et al., 2022)\nQuery\n85.96\n71.76\n-\nT5 benchmarking for SKG tasks\nGraphix-T5 (Li et al., 2023)\nQuery\n-\n77.1\n-\nGraph-aware T5 for complex reasoning\nRESDSQL (Li et al., 2023)\nQuery\n-\n80.5\n-\nRanking-enhanced schema linking\nC3 (Dong et al., 2023)\nQuery\n-\n-\n-\nZero-shot ChatGPT with calibration\nLLM-based\nZERoNL2SQL (Gu et al., 2023)\nQuery\n-\n-\n-\nPLM sketch, LLM reasoning\nDIN-SQL (Li et al., 2023)\nQuery\n-\n60.1\n-\nTask decomposition for few-shot LLM\nLiu and Tan, 2023\nQuery\n-\n-\n-\nChain-of-Thought LLM prompting\nSC-Prompt (Gu et al., 2023)\nQuery\n-\n76.9\n-\nStructure-content decoupling, hybrid prompting\nNan et al., 2023\nQuery\n-\n-\n-\nIn-context learning with demo selection\nTai et al., 2023\nQuery\n-\n-\n-\nSimplified CoT for Text-to-SQL\nSQL-PaLM (Sun et al., 2023)\nQuery\n-\n-\n-\nSelf-consistency prompting with PaLM-2\nGuo et al., 2023\nQuery\n-\n-\n-\nRetrieval-augmented iterative refinement\nDAIL-SQL (Gao et al., 2023 )\nQuery\n-\n-\n-\nSystematic LLM prompt engineering\nPET-SQL (Li et al., 2024 )\nQuery\n-\n-\n-\nSchema-aware cross-model consistency\nNL2INTERFACE (Chen et al., 2022)\nVisual\n-\n-\n-\nNL-based interactive vis interface\nChat2VIS (Maddigan and Susnjak, 2023)\nVisual\n-\n-\n-\nPrompt-engineered LLMs for vis code\nPrompt4Vis (Li et al., 2024)\nVisual\n-\n-\n52.69\nIn-context learning for text-to-vis\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n10\nto predict different SQL components, providing a structured\napproach to SQL query generation. SmBoP [86] stands out\nfor its bottom-up decoding mechanism. Given a set of trees,\nit scores and selects trees based on SQL grammar, ensuring\nthat the generated queries are both syntactically valid and\nsemantically aligned with the input. Bridge [60] uses an\nLSTM-based pointer-generator with multi-head attention\nand a copy mechanism as the decoder. This model is capable\nof generating a token from the vocabulary, copying a token\nfrom the question, or copying a schema component from the\ndatabase schema at each decoding step, providing a flexible\napproach to SQL query generation.\nSome other decoders generate an intermediate repre-\nsentation (IR) of the SQL query first, simplifying the SQL\ngeneration task by breaking it down into more manageable\nsteps. Typical models include IncSQL [94] which defines\ndistinct actions for different SQL components and lets the\ndecoder predict these actions instead of directly generating\nSQL queries, effectively simplifying the generation task. IR-\nNet [38] introduces SemQL, an intermediate representation\nfor SQL queries. SemQL can cover a wide range of SQL\nqueries, making it a versatile tool for SQL query generation.\nNatSQL [29] builds on the idea of SemQL by removing\nset operators, streamlining the IR and making it easier to\nhandle.\nThere are also constrained decoding-based decoders,\nwhich incorporate constraints into the decoding process to\nguide the SQL query generation. Models like PICARD [88]\nand UniSAr [24] use reinforcement learning and rule-based\nsystems, respectively, to incorporate constraints into the de-\ncoding process. These constraints guide the model towards\ngenerating valid SQL queries, contributing to the accuracy\nand reliability of these models.\n• Execution-based Decoder. Execution-based decoders offer\na unique approach to the Text-to-SQL task, utilizing an off-\nthe-shelf SQL executor such as SQLite to verify the validity\nand correctness of the generated SQL queries during the\ndecoding process. This methodology ensures both syntac-\ntic and semantic accuracy of the produced SQL queries.\nWang et al. [109] leverages a SQL executor to check the\npartially generated SQL queries during the decoding pro-\ncess. Queries that raise errors are discarded, and the model\ncontinues to refine the generation until a valid SQL query\nis produced. Suhr et al. [102] follow a similar approach, but\nthey avoid altering the decoder’s structure. Instead, they\nexamine the executability of each candidate SQL query.\nOnly the queries that can be successfully executed are\nconsidered valid, which helps in maintaining the grammat-\nical correctness of the generated SQL queries. In another\napproach, SQLova [45] incorporate an execution-guided\ndecoding mechanism that filters out non-executable partial\nSQL queries from the output candidates. This methodology\nensures the generation of SQL queries that are not only\nsyntactically correct but also executable.\n4.1.3\nFoundation Language Model Stage\nThe recent upsurge in the performance of NLP tasks is\nsignificantly attributed to the advancement of foundation\nlanguage models (FMs) such as BERT, T5, and GPT. These\nmodels, trained on large corpora, capture rich semantic and\nsyntactic features of languages and have been successful\nacross a variety of tasks.\nWe categorize the FM-based approaches in Text-to-SQL\ninto two categories based on the language models they\nincorporate: Pretrained Language Models(PLMs) and Large\nLanguage Models(LLMs). PLMs, representing the earlier\nevolution like BERT and initial GPT versions, capture\ndetailed linguistic nuances through extensive training.\nThey are often refined for specific tasks via methods like\nfine-tuning. LLMs represent an advancement, characterized\nby their vast scale. By amplifying model parameters or\ntraining data, these models exhibit enhanced ”emergent\nabilities” [113]. A prime example is ChatGPT, an adaptation\nof the GPT architecture that excels in dialogue interactions.\nLLM-based\nText-to-SQL\nmethods\nleverage\nprompts,\nutilizing in-context learning [21] and chain-of-thought [114]\nreasoning to produce apt SQL queries.\nPLM-based. Early PLM-based approaches directly utilize\nand fine-tune pre-trained language models, refining them\nspecifically for the Text-to-SQL task. These models can be\nbroadly categorized into encoder-only language models and\nencoder-decoder language models.\n• Encoder-only Language Models. Models like BERT and\nRoBERTa [65] serve as foundational encoder-only PLMs in\nvarious Text-to-SQL models, transforming input sequences\ninto context-sensitive numerical representations. IRNet [38],\nfor example, harnesses BERT to craft a specialized input\nsequence. BRIDGE [60] fuses BERT’s prowess with schema-\nconsistency guided decoding in a seq-to-seq architecture,\nenhancing the schema linking ability. HydraNet [71] and\nSQLova [45] process questions and columns separately,\npredicting for each column individually with BERT, notably\nexcelling on the WikiSQL benchmark. X-SQL [40] makes\na novel modification to BERT by replacing segment em-\nbeddings with column type embeddings. This model also\nencodes additional feature vectors for matching question\ntokens with table cells and column names, and concatenates\nthem with BERT embeddings of questions and database\nschemas.\n• Encoder-decoder Language Models. Unlike encoder-only\nmodels, encoder-decoder models like T5 [84] and BART [55]\nare end-to-end models designed for seq-to-seq tasks. These\nmodels take a sequence of textual input and generate a\nsequence of textual output. They have been adapted and\nfine-tuned for the Text-to-SQL task, resulting in innovative\nand effective models. UnifiedSKG [115], for example, fine-\ntunes T5 on Text-to-SQL task with PICARD [88] decoding.\nBy combining the advantages of T5’s powerful language\nunderstanding capabilities with the benefits of a sketch-\nbased approach, it captures both the structural aspects of\nSQL and the semantic nuances of natural language ques-\ntions. Graphix-T5 [58] leverages the robust contextual en-\ncoding intrinsic of T5 to enhance domain generalization by\nmodeling relational structures. Its GRAPHIX layer encodes\nboth semantic and structural insights, marking a pioneering\nstep in infusing graphs into Text-to-SQL translation. RESD-\nSQL [57] also taps into the T5 model to craft the SQL query,\nutilizing a fusion of the question and schema sequences,\nwhere various T5 variants are adapted to generate skeletons\nderived from questions.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n11\n• Additional Pretraining. Other than finetuning from gen-\neral pretrained language models, there are some approaches\ninvolving additional pretraining of language models with\nText-to-SQL data. Rather than directly employing off-the-\nshelf PLMs, these methods construct a new model using\narchitectures like BERT or BART, and train these models\nusing Text-to-SQL data (tabular data and text-to-SQL pairs)\nwith specially designed objectives that are related to SQL\ngeneration.\nFor instance, TaBERT [119] enhances BERT by training\non tabular data, focusing on predicting concealed column\nnames and restoring cell values. This equips the model with\ninsights into database tables’ structure and content, which\nis crucial for accurate SQL query generation. Grappa [123]\nfinetunes BERT by generating question-SQL pairs over\ntables. The training targets objectives like masked language\nmodeling (MLM), column prediction, and SQL operation\nprediction, honing the model’s ability to produce SQL\nqueries aligned with the natural language intent. GAP [93]\nfollows a parallel strategy, pretraining BART on combined\nText-to-SQL and tabular datasets. The training focuses\non objectives like MLM, predicting columns, restoring\ncolumns, and crafting SQL. Integrating these goals, GAP\nensures that the model comprehends subtle differences in\nthe database tables and the posed questions, improving the\nprecision of generated SQL queries.\nLLM-based. LLM-based methods mark the latest trend in\nText-to-SQL, combining the power of large language models\nwith the art of prompt engineering. These approaches use\ncarefully designed prompts to steer the models toward\ngenerating accurate SQL queries, with two main categories:\nzero-shot prompting and few-shot prompting.\n• Zero-shot Prompting. In zero-shot prompting, the LLM\nreceives a specific prompt without any additional training\nexamples, banking on the extensive knowledge it has gained\nduring the pre-training phase. Rajkumar et al. [85] first em-\nbarked on an empirical exploration of zero-shot Text-to-SQL\ncapabilities on Codex [9]. After ChatGPT came out, Liu et\nal. [62] conducted an extensive evaluation of zero-shot Text-\nto-SQL ability on it across an array of benchmark datasets.\nBuilding on this, the method C3\n[22] based on ChatGPT\nemerged as a leading zero-shot Text-to-SQL solution on the\nSpider Challenge. The essence of C3 lies in its three founda-\ntional prompting components: Clear Prompting, Calibration\nwith Hints, and Consistent Output. ZERoNL2SQL [34] has\nmerged the strengths of both PLMs and LLMs to foster\nzero-shot Text-to-SQL capabilities. The approach leverages\nPLMs for the generation of an SQL sketch through schema\nalignment and subsequently employs LLMs to infuse the\nmissing details via intricate reasoning. A distinctive fea-\nture of their method is the predicate calibration, designed\nto align the generated SQL queries closely with specific\ndatabase instances.\n• Few-shot Prompting. Few-shot prompting in Text-to-\nSQL presents a fascinating landscape where models are\nguided to achieve complex tasks with minimal examples.\nThe strategies of in-context learning (ICL) and chain-of-\nthought (CoT) reasoning play pivotal roles in these ap-\nproaches, enabling models to extract knowledge from a\nhandful of demonstrations and reason through intricate SQL\ngeneration processes.\nA notable work in this area is DIN-SQL [81], which\nshowcases how breaking down SQL query generation into\nconstituent problems can significantly improve the perfor-\nmance of LLMs. This is achieved through a four-module\nstrategy: schema linking, query classification and decompo-\nsition, SQL generation, and a novel self-correction mecha-\nnism. Similarly, Liu et al. [64] brought forth the Divide-and-\nPrompt paradigm which decomposes the primary task into\nsimpler sub-tasks, tackling each through a CoT approach,\nthereby enhancing the reasoning abilities of LLMs for the\nText-to-SQL task. Gu et al. [33] presented a unique Divide-\nand-Conquer framework which steers LLMs to generate\nstructured SQL queries and subsequently populates them\nwith concrete values, ensuring both validity and accuracy.\nIn a comprehensive study, Nan et al. [75] explored vari-\nous prompt design strategies to enhance Text-to-SQL mod-\nels. The research probes into different demonstration selec-\ntion methods and optimal instruction formats, revealing that\na balance between diversity and similarity in demonstration\nselection combined with database-related knowledge aug-\nmentations can lead to superior outcomes. Tai et al. [105]\nproposed a systematic investigation of enhancing LLM’s\nreasoning abilities for text-to-SQL parsing through various\nchain-of-thought style promptings. The research found that\navoiding excessive detail in reasoning steps and improving\nmulti-step reasoning can lead to superior results.\nMore recently, SQL-PaLM [104], an LLM-based ap-\nproach grounded in PaLM-2, is proposed employing an\nexecution-based self-consistency prompting approach tai-\nlored for Text-to-SQL. Guo et al. [35] propose a retrieval-\naugmented prompting method that integrates sample-\naware demonstrations and a dynamic revision chain. This\napproach aims to generate executable and accurate SQLs\nby iteratively adapting feedback from previously generated\nSQL, ensuring accuracy without human intervention.\n4.2\nText-to-Vis Parsing\nCurrently, there are several models specifically handling\nthe Text-to-Vis problem. They typically accept a natural\nlanguage query and tabular data, producing a self-defined\nvisual language query (VQL), a SQL-like pseudo syntax\nfor combining database querying with visualization direc-\ntives, which is then hard-coded to visual specification code.\nSimilar to Text-to-SQL, Text-to-Vis parsing approaches have\ntransitioned through three evolutionary stages: traditional,\nneural network, and foundation language model, as illus-\ntrated in Fig. 4.\n4.2.1\nTraditional Stage\nDuring this stage, the main focus was to improve accu-\nracy by using different parsing methods, keywords, and\ngrammar rules. Between 2015 and 2020, the works mostly\nexplored the effects of different semantic parsing tech-\nniques. Notable works include DataTone [30], Eviza [89],\nEvizeon [41], VisFlow [120], FlowSense [121], Orko [100],\nValletto [50], and InChorus [98]. The survey by Shen et\nal. [92] gave a thorough walk-through of the different meth-\nods. Stemming from the method in DataTone, several works\nin 2020 and 2021 have deployed a more structured VQL\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n12\ntemplate. The VQLs for each system are defined slightly\ndifferently but they generally follow the SQL style and in-\nclude additional visualization attributes. ADVISor [63] has\ndeveloped an automatic pipeline to generate visualization\nwith annotations. The input is a set of table headers and an\nNLQ and the output is a set of aggregations in a SQL-like\nformat. NL4DV\n[76] has provided a python package that\ntakes an NLQ and the associated tabular dataset as input\nand outputs visualization recommendations in the form of\na JSON object that can help users generate visualizations.\n4.2.2\nNeural Network Stage\nThe emergence of deep neural networks, especially attention\nmechanisms, brings a shift towards encoder-decoder-based\nmodels. As discussed earlier, the template approach can\nbe easily converted to a neural network model. In some\nmodels, visualization specifications are directly produced,\nbypassing the intermediate VQL sequence step. This section\ndelves into various models leveraging the encoder-decoder\narchitecture.\nEncoder. Sequence-based encoders like LSTMs and trans-\nformers excel at managing sequential data’s long-term de-\npendencies, while graph-based encoders grasp non-linear\nrelationships, comprehensively depicting the input. Their\ncapability to represent complex data structures establishes\ntheir significance in crafting efficient Text-to-Vis systems.\n• Sequence-based Encoder. Sequence-based encoders like\nLSTM, attention mechanisms, and transformers have be-\ncome essential to Text-to-Vis. While LSTMs are great at man-\naging sequential long-term dependencies, they are restricted\nin modeling complex interactions between distant words.\nThis limitation is addressed by the attention mechanism and\nis further enhanced by the Transformer architecture.\nSeq2Vis [69], evolving from Data2Vis [17], employs a\nseq2seq model, enhancing it with pretrained global word\nembeddings for richer input understanding. Combined with\nLSTM encoders, attention, and LSTM decoders, Seq2Vis\nadeptly translates natural language queries into visualiza-\ntions. Similarly, MMCoVisNet [96] leverages an LSTM-based\nencoder for text-to-Vis dialogues. Conversely, ncNet [70]\ntransitions to a Transformer-based model. Its multi-self-\nattention design eliminates recurrent computations, height-\nening efficiency. In ncNet, tokenized inputs from three\nsources are sequenced and merged. Each word is tokenized,\nmasked tokens are populated, and boundary-indicating to-\nkens are added. These tokens undergo vectorization using\nvarious embeddings, establishing ncNet as a state-of-the-art\nin Text-to-Vis, proficiently converting queries into visualiza-\ntion codes.\n• Graph-based Encoder. As the field of Text-to-Vis pro-\ngresses, there is a notable shift toward leveraging more com-\nplex and efficient encoding methods for input data. Unlike\nsequence-based methods that linearly process input data,\ngraph-based encoders can capture non-linear relationships\nwithin the data, thus offering a richer and more contextually\naccurate representation of the input.\nA notable work in this direction is RGVisNet [97]. It\nmerges sequence and graph-based encoding in a novel\nretrieval-generation approach. The input natural language\nquery(NLQ) is parsed to extract relevant VQL from its\ncodebase, achieved by retrieving schemas in the NLQ,\nperforming schema linking, and locating similar VQLs\nfrom the codebase. The NLQ is embedded through an\nLSTM encoder, while the candidate VQLs are processed\nthrough a Graph Neural Network (GNN) encoder using\nan abstract syntax tree (AST) representation. The relevance\nbetween NLQ and VQL embeddings is assessed using\ncosine similarity, with the embeddings then funneled into\na Transformer encoder to ascertain relationships and yield\nthe final output.\nDecoder.\nDecoders in Text-to-Vis systems translate en-\ncoded textual input into coherent visualizations. Exist-\ning approaches have incorporated LSTM, transformer, and\ngrammar-based decoders.\n• Monolithic Decoder. In the context of Text-to-Vis tasks,\nmonolithic decoders utilize a single, end-to-end model, of-\nten based on RNNs, LSTMs, or Transformer architectures,\nto transform a natural language description into a com-\nplete and coherent visual representation by sequentially\ngenerating components of a visualization, conditioned on\nan encoded representation of the input text.\nSeq2Vis [69] uses an LSTM decoder within its architec-\nture to generate visual queries. The attention mechanism\nit incorporates enables dynamic consideration of the input\nsequence’s segments during output generation. Conversely,\nncNet [70] employs a Transformer-based encoder-decoder\napproach. Both its encoder and decoder are built using self-\nattention blocks, optimizing inter-token relationship pro-\ncessing. This design provides flexibility in sequence trans-\nlation, with the auto-regressive decoder ensuring coherent\nand logically sequenced outputs.\n• Grammar-based Decoder. RGVisNet [97] introduces a\ngrammar-aware decoder tailored for VQL revision. Given\nVQL’s strict and defined grammar, similar to program-\nming languages, leveraging this structure becomes essential.\nThis approach mirrors text-to-SQL tasks, where integrating\ngrammar as inherent knowledge effectively guides code\ngeneration. RGVisNet adapts the SemSQL grammar to sup-\nport DV queries. The core decoder in RGVisNet adapts an\nLSTM-based structure underpinned by the formation of a\ncontext-free grammar tree. As the model traverses this tree,\nit leverages an LSTM model at every step to opt for the most\nlikely branch, based on prior routes.\n4.2.3\nFoundation Language Model Stage\nFoundation language models (FMs), especially large lan-\nguage models such as CodeX [9] and GPT-3, have revo-\nlutionized natural language processing with their ability\nto generate contextually accurate text. This is leveraged\nto advance the field of Text-to-Vis towards a new set of\napproaches.\n• Zero-Shot Prompting. Zero-shot prompts refer to the\nuse of untrained prompts to guide LLMs in generating\nvisualization codes straight from textual or spoken queries.\nLeveraging LLMs’ natural language understanding capabil-\nities, zero-shot prompting in text-to-visualization systems\nemploys carefully crafted prompts as guiding instructions,\nsteering the models to generate specific and contextually\nappropriate visualizations based on user input. Mitra et\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n13\nal. [74] developed a prototype web application by prompt-\ning CodeX. Chat2VIS [72] also chose the model CodeX and\nspecifically included a code prompt component to guide\nthe LLM. These two methods both output visualization\nspecification code directly.\n• Few-Shot Prompting. Few-shot methods employ lim-\nited examples to guide LLMs toward desired outputs.\nNL2INTERFACE [10] utilizes CodeX by first preparing ex-\namples that translate natural language queries into a specific\nVQL format named SPS. This step forms a suitable prompt\nfor in-context learning by CodeX. Subsequently, given the\nnatural language queries and a database catalog, Codex\npredicts the corresponding VQL. Finally, NL2INTERFACE\nmaps these SPS representations to generate interactive in-\nterfaces, following a procedure similar to PI2 based on a\npredefined and extensible cost model.\nTakeaways for Approaches\n• Traditional Stage:\n– Rule-based: Interpretable, limited adaptabil-\nity. Use for interpretability.\n– Template-based: Fast, struggles with novel\nqueries. Use for well-defined scenarios.\n• Neural Network Stage:\n– Encoders: Sequence (flexible, data-hungry),\nGraph (captures relations, complex). Choose\nbased on data complexity and structure.\n– Decoders: Monolithic (unified, may miss nu-\nances), Skeleton (captures structure, limited\ngeneralization), Grammar (aligns with for-\nmal languages, limited complexity), Execu-\ntion (validates output, might be slower).\nChoose based on generalization needs.\n• Foundation Language Model Stage:\n– PLM-based: Strong performance, hard to in-\nterpret. Prefer for performance.\n– LLM-based: Strong performance and general-\nization, requires prompt engineering. Prefer\nfor generalization (with prompt engineering).\n5\nEVALUATION METRICS\nEvaluation metrics play a pivotal role in assessing the\nperformance of semantic parsers for both Text-to-SQL and\nText-to-Vis tasks. Table 3 provides a comparative analysis of\ncommonly used metrics.\n5.1\nText-to-SQL Metrics\n5.1.1\nString-based Matching\nString-based matching metrics evaluate the textual match\nbetween the generated SQL query and the ground truth.\nExact String Match. Exact String Match [25] is the strictest\nform of string-based evaluation, requiring the generated\nquery to be identical to the target query. While efficient and\nbroadly applicable, it can overlook semantically equivalent\nqueries with slight syntactic differences.\nFuzzy Match. Fuzzy Match allows for approximate match-\ning, quantifying similarity by assigning scores based on\nstring closeness, such as BLEU [18]. It offers flexibility for\nminor discrepancies but may be overly lenient, potentially\noverlooking significant errors [61].\nComponent Match. Component matching, such as Exact Set\nMatch [126], focuses on individual components or segments\nof the predicted SQL query. It ensures the correctness of each\ncomponent independently.\n5.1.2\nExecution-based Matching\nExecution-based matching evaluates the correctness of a\nSQL query based on its execution results.\nExecution Match. Execution match considers the generated\nquery correct if its results match the reference query, regard-\nless of syntactic differences. It is beneficial when distinct\nqueries can lead to the same output, avoiding false negatives\nfrom string-based metrics.\nTest Suite Match. Test Suite Match, proposed by Zhong\net al. [131], creates multiple knowledge base variants to\ndifferentiate between predicted and reference queries. A\nquery is considered correct only if its execution results align\nwith the reference across all variants, ensuring consistency.\n5.1.3\nManual Evaluation\nHuman Evaluation of SQL. In the Text-to-SQL task, human\nevaluation discerns semantic equivalence when execution\nresults differ but are valid in real-world scenarios. Dahl et\nal. [13] introduced an approach where a result is correct\nif it falls within a predefined interval which often requires\nhuman judgment.\n5.2\nText-to-Vis Metrics\n5.2.1\nString-based Matching\nExact String Match. Exact String Match, often referred as\nOverall Accuracy in the context of Text-to-Vis [97], directly\nmeasures the matches between the predicted visualization\nquery and the ground truth query. This metric reflects the\ncomprehensive performance of the models.\nComponent\nMatch. Component matching, such as in\nRGVisNet [97] and Seq2Vis, focuses on individual compo-\nnents of the predicted visualization specification. It ensures\nthe correctness of each component independently.\n5.2.2\nManual Evaluation\nUser Study of Vis. In Text-to-Vis, user studies evaluate mod-\nels’ practical effectiveness, user-friendliness, and efficiency.\nThey capture user feedback on system speed, ease of use,\npreferences, and suggestions for improvements, offering\ninsights into real-world applicability.\nTakeaways for Evaluation Metrics\n• Text-to-SQL:\nString-based\n(Exact,\nFuzzy,\nComponent), Execution-based (Execution, Test\nSuite), Manual (Human Evaluation)\n• Text-to-Vis: String-based (Exact, Component),\nManual (User Study)\n• Guidelines: Use string-based metrics for syntac-\ntic correctness. Use execution-based metrics for\nsemantic equivalence. Manual evaluation can\nbe considered to capture nuances and subtleties\nthat automated metrics might miss.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n14\nTABLE 3\nComparative Analysis of Evaluation Metrics\nType\nMethod\nAdvantages\nDisadvantages\nString-based Matching\nExact String Match\nHigh efficiency, wide applicability\nCannot handle alias expressions\nFuzzy Match\nSuitable for complex queries\nInsufficient precision\nComponent Match\nCan handle simple alias expressions\nNeeds to be customized\nExecution-based Matching\nNaive Execution Match\nConvenient, robust to alias expressions\nProne to false positives\nTest Suite Match\nCan handle semantically close expressions\nNeeds to be customized\nManual Evaluation\nManual Evaluation\nPrecise, flexible\nHigh cost, low efficiency\n5.3\nSystem Architectures\nSystem architecture is crucial in shaping the capabilities\nof natural Language interfaces for tabular data querying\nand visualization. Various architectural paradigms have\nemerged as the field has evolved, each tailored to specific\nchallenges and needs. While in-depth analyses and compar-\nisons of earlier systems can be found in surveys like [1], [31],\n[53], this section will categorize these systems into four main\narchitectural types: rule-based systems, parsing-based sys-\ntems, multi-stage systems, and end-to-end systems. Table 4\npresents a comprehensive overview of various Text-to-SQL\nand Text-to-Vis systems.\n5.3.1\nRule-based System\nRule-based systems stand as foundational architectures for\nnatural language interfaces to databases. These systems\nleverage a set of predefined rules, mapping natural lan-\nguage inputs directly to database queries or visualizations.\nFor Text-to-SQL, systems like PRECISE [79] and NaLIR [56]\nemploy rule-based strategies, translating linguistic patterns\ninto SQL queries. In the Text-to-Vis context, DataTone [30]\nrepresents this approach, converting user language into\nvisualization specifications via established patterns. While\nprecise, rule-based systems can face challenges in scalability\nand adaptability to diverse linguistic constructs.\n5.3.2\nParsing-based System\nParsing-based systems primarily focus on understanding\nthe inherent grammatical structure of the input question.\nDrawing inspiration from traditional linguistic parsing,\nthese systems convert natural language questions into syn-\ntactic structures or logical forms. In the field of Text-to-\nSQL, systems such as SQLova [45] and Seq2Tree [19] uti-\nlize semantic parsers to bridge the gap between natural\nlanguage and structured database queries. For Text-to-Vis,\nsystems ncNet [70] process user queries through semantic\nparsing, transforming them into Visualization Query Lan-\nguages (VQL). Parsing-based systems prioritize linguistic\nstructure and semantics, offering depth in understanding,\nbut might struggle with the variability and ambiguity in-\nherent to natural language.\n5.3.3\nMulti-stage System\nMulti-stage systems in natural language interfaces for tab-\nular data operate through sequenced processing pipelines.\nThese systems dissect the overarching task into distinct\nstages, each addressing a particular sub-task. This layered\napproach allows for focused improvements at every junc-\nture. Within the Text-to-SQL domain, the DIN-SQL sys-\ntem [81] exemplifies this architecture, segmenting SQL gen-\neration into stages for schema linking, query classification\nand decomposition, SQL generation, and self-correction. In\nthe Text-to-Vis sphere, DeepEye [67] emerges as a notable\nmulti-stage system to discern the quality of visualizations,\nrank them, and optimally select the top-k visualizations\nfrom a dataset. By segmenting the process, multi-stage\nsystems can apply tailored techniques to each segment,\nenhancing accuracy. However, the modular approach de-\nmands careful orchestration between stages to ensure co-\nherency in the final output and can potentially bring higher\ncomputational cost.\n5.3.4\nEnd-to-end System\nEnd-to-end systems represent a holistic approach to natural\nlanguage interfaces for tabular data. Rather than relying\non intermediate representations or multi-phase processing,\nthese systems process input questions and directly generate\nthe desired output in one cohesive step. For example, Pho-\nton [129] offers a modular framework tailored for indus-\ntrial applications of Text-to-SQL systems. It takes a user’s\nquestion and a database schema, directly generating SQL\nand executing it to produce the desired result, with its core\nstrength lying in its SQL parser and a unique confusion\ndetection mechanism. Another exemplar is VoiceQuerySys-\ntem [95], which elevates the user experience by converting\nvoice-based queries directly into SQL, bypassing the need\nfor text as an intermediary. Similarly, in the Text-to-Vis\ndomain, Sevi [106] stands out as an end-to-end visualization\nassistant. It empowers novices to craft visualizations using\neither natural language or voice commands. Furthermore,\nDeepTrack [66] integrates data preparation, visualization\nselection, and intuitive interactions within a singular frame-\nwork, exemplifying the comprehensive capabilities of end-\nto-end systems.\n5.4\nUser-centric Analysis\nWhen designing natural language interfaces for tabular data\nquerying and visualization, it is essential to consider the\nneeds of different user groups.\n5.4.1\nBasic Users\nFor basic users with limited technical backgrounds, rule-\nbased systems are most suitable due to their simplicity and\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n15\nTABLE 4\nComparison of Different Systems of Text-to-SQL and Text-to-Vis\nType\nSQL system example\nVis system example\nAdvantages\nDisadvantages\nRule-based\nNaLIR, PRECISE\nDataTone\nRobustness and consistency for familiar queries\nLimited adaptability\nParsing-based\nSQLova, Seq2Tree\nncNet\nGrasps deeper language structures\nStruggles with ambiguity\nMulti-stage\nDIN-SQL\nDeepEye\nEnhanced accuracy and flexibility\nSynchronization challenges\nEnd-to-end\nPhoton, VoiceQuerySystem\nSevi, DeepTrack\nHigh adaptability, unified training process\nDifficult to interpret and debug\naccuracy in well-defined domains. End-to-end systems are\nrecommended for those needing flexibility to handle diverse\nqueries effortlessly. Users with stronger technical skills or\nthose working with complex data structures may prefer\nparsing-based systems, which excel in handling intricate\nlinguistic structures.\n5.4.2\nProfessional Users\nProfessional users in corporations and academic institutions\noften deal with large volumes of data and have higher\nquerying frequencies. In stable and standardized data envi-\nronments, rule-based systems ensure reliable performance\nfor repetitive queries. For complex data environments re-\nquiring integration and analysis of different data types,\nmulti-stage systems offer enhanced adaptability and accu-\nracy. In fast-paced environments where speed and efficiency\nare crucial, end-to-end systems are the most suitable choice,\nminimizing latency and allowing for rapid adaptation to\nchanging data landscapes.\nTakeaways for System Design\n• Rule-based: Precise but limited adaptability.\nSuitable\nfor\nwell-defined\ndomains.\nRecom-\nmended for basic users seeking simplicity and\nprofessional users in stable environments.\n• Parsing-based: Focuses on grammar but strug-\ngles with ambiguity. Ideal for complex linguis-\ntics. Recommended for basic users with techni-\ncal expertise.\n• Multi-stage: Sequential processing for enhanced\naccuracy. Best for complex data. Recommended\nfor professional users dealing with complex\ndata.\n• End-to-end: Holistic and highly adaptable. Per-\nfect for fast-paced environments.Recommended\nfor basic users requiring flexibility and profes-\nsional users prioritizing efficiency.\n6\nFUTURE RESEARCH DIRECTIONS\nAs natural language interfaces for tabular data querying\nand visualization evolve, new challenges and opportuni-\nties emerge. This section highlights six pivotal areas that\npromise to shape the domain’s future, emphasizing the\nongoing research evolution and its potential. Table 5 com-\npares Text-to-SQL and Text-to-Vis tasks across these research\ndirections.\n6.1\nAdvancing Neural Models and Approaches\nThe landscape of Natural Language Interfaces for Tabular\nData has seen impressive strides, especially with the ad-\nvent of neural models in the text-to-SQL domain. How-\never, there remains substantial room for improvement and\ninnovation. While plenty of models have been proposed\nfor text-to-SQL tasks, continual refinement is essential to\nhandle more complex queries, multi-turn interactions, and\ndomain-specific problems [14]. Concurrently, the text-to-\nvisualization domain hasn’t witnessed the same influx of\nneural network-based models. The challenges here are mul-\ntifold: from generating diverse visualizations based on user\nintent to ensuring those visualizations maintain both accu-\nracy and aesthetic appeal [92]. For both domains, it is vital\nto push the boundaries of current neural architectures. This\ncould involve exploring deeper networks, advanced atten-\ntion mechanisms, or hybrid models combining rule-based\nlogic with neural insights. Leveraging external knowledge\nbases, transfer learning, and multi-modal strategies could\nfurther optimize the interpretation and translation of user\nintent into SQL queries or visual representations.\n6.2\nHarnessing Potential of Large Language Models\nLarge Language Models (LLMs) like ChatGPT have revo-\nlutionized various Natural Language Processing domains\nwith their profound text understanding and generation\ncapabilities. Despite this, exploring LLMs in the context\nof natural language interfaces for databases remains rel-\natively nascent. While preliminary efforts have begun in-\ntegrating LLMs into text-to-SQL and text-to-visualization\nsystems [10], [81], the vast potential of LLMs has not been\nfully harnessed. Their ability to capture context, understand\nnuances, and generalize from limited examples could be\ninvaluable in understanding and translating complex user\nqueries. However, merely deploying LLMs without cus-\ntomization might not be optimal. Future research should\nfocus on tailoring these models to the specific challenges\nof querying and visualization. This might involve adapt-\ning LLMs on domain-specific datasets, integrating them\nwith existing architectures, or developing novel prompting\nstrategies to better align them with the tasks at hand.\n6.3\nExploring Advanced Learning Methods\nThe heavy reliance on traditional supervised learning on\nlarge labeled datasets poses challenges for evolving natural\nlanguage interfaces for tabular data. This underscores the\nneed for alternative learning approaches. Semi-supervised\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n16\nTABLE 5\nComparison between Text-to-SQL and Text-to-Vis Research\nAspects\nText-to-SQL\nText-to-Vis\nNeural Models and Approaches\nNotable advancements with a variety of models\nStill in nascent stages with limited models\nIntegration of LLMs\nPreliminary efforts with room for more exploration\nLimited work, significant potential\nLearning Methods\nMainly supervised; early exploration of semi-supervised methods\nPredominantly supervised\nDatasets\nSeveral large-scale datasets available\nFewer datasets; need for more diversity\nRobustness and Generalizability\nIncreased focus, especially for complex queries\nEmerging focus; essential for diverse visualizations\nAdvanced Applications\nIntegration with chatbots, recommendation systems\nPotential for multimodal systems, dynamic visualizations\nand weakly supervised methods, which capitalize on un-\nlabeled data or weak supervision signals, present viable\nsolutions [36]. For example, implicit user interactions might\noffer weak guidance for model refinement. Additionally,\nparameter-efficient training methods like LoRA [42] have\ndemonstrated superior data efficiency, especially in low-\nresource settings, compared to traditional fine-tuning meth-\nods. Fusing large pre-trained models with these parameter-\nefficient techniques hints at a promising future for data-\nefficient semantic parsing.\n6.4\nConstructing Large-Scale and Diverse Datasets\nThe potency of natural language interfaces for databases\ndepends on high-quality, diverse datasets. While several\ndatasets are tailored for text-to-SQL and text-to-vis tasks,\nthere’s a pressing need for even larger-scale, more varied\ndatasets. Such datasets foster better generalization and ro-\nbustness to a broad spectrum of user queries, spanning\nvarious domains and complexities. Moreover, the current\ndataset landscape is predominantly English-centric, over-\nlooking the global spectrum of data user [37]. Embracing\nmultilingual or under-represented language datasets can\namplify the reach and inclusivity of these interfaces.\n6.5\nAdvancing Robustness and Generalizability\nAs natural language interfaces for tabular data become\nmore integral in various applications, the robustness and\ngeneralizability of the underlying models and systems are\ncentral. It’s not just about achieving high performance on\nbenchmark datasets; real-world scenarios demand models\nthat can reliably handle diverse, unexpected, and sometimes\nadversarial inputs.\n• Robustness Against Adversarial and Out-of-Distribution\nPerturbations. As with many machine learning models, ad-\nversarial attacks or unexpected inputs can pose significant\nchallenges. There’s a need for models that can gracefully\nhandle and respond to such inputs without compromising\non accuracy or reliability. This involves developing mod-\nels inherently resistant to such perturbations and creating\ndatasets that can effectively test such robustness [8].\n• Compositional Generalization. The ability for models to\nunderstand and combine known concepts in novel ways is\nvital. For instance, if a model understands two separate\nqueries, it should ideally be able to handle a composite\nquery that combines elements of both. This capability en-\nsures that models can effectively tackle unseen queries by\nleveraging their understanding of underlying concepts.\n• Domain Generalization. As these interfaces permeate\nvarious sectors, models should adapt across domains and\nincorporate domain-specific knowledge. This ensures that,\nwhile retaining versatility, models are attuned to the nu-\nances of diverse queries, from finance to healthcare and\nbeyond [28].\n6.6\nPioneering Advanced Applications in the LLM Era\nThe LLM era presents opportunities for revolutionizing\napplications and systems of natural language interfaces for\ndatabases.\n• Multimodal Systems. Combining the power of LLMs\nwith other modalities, such as visual or auditory inputs, can\nlead to the creation of truly multi-modal systems. Imagine\nquerying a database not just with text, but with images,\nvoice commands, or even gestures. Such systems can cater\nto a broader audience and offer more dynamic and natural\ninteractions.\n• Integrated Systems. As LLMs continue to excel in vari-\nous tasks, there’s potential for integrating natural language\ninterfaces with other functionalities, like document summa-\nrization, recommendation systems, or even chatbots. This\ncan result in comprehensive systems where users can query\ndata, get summaries, seek recommendations, and more, all\nwithin a unified, language-centric interface.\n• User-Centric Design. The LLM era emphasizes user\ninteraction. There’s a need for applications prioritizing user\nexperience, offering intuitive interfaces, interactive feed-\nback, and personalized responses. By harnessing the capa-\nbilities of these models and focusing on creating holistic,\nuser-centric applications, we can set the stage for a future\nwhere data interaction is both efficient and delightful.\n7\nCONCLUSION\nIn this survey, we explore Natural Language Interfaces\nfor Tabular Data Querying and Visualization in-depth,\ndelving into the intricacies of the field, its evolution,\nand the challenges it addresses. We trace its evolution\nfrom\nfoundational\nproblem\ndefinitions\nto\nstate-of-the-\nart approaches. We highlight the significance of diverse\ndatasets fueling these interfaces and discuss the metrics that\ngauge their efficacy. By exploring system architectures, we\nexamine the differences of distinct system designs. Lastly,\nour gaze turns toward the horizon, pointing to promising\nresearch avenues in the era of Large Language Models. As\nthis dynamic field evolves, our exploration offers a concise\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n17\nsnapshot of its current state, challenges, and potential.\nAcknowledgements. We are grateful to the anonymous\nreviewers for their constructive comments on this paper.\nThe research of Victor Junqiu Wei was supported in part\nby the HKUST-WeBank Joint Laboratory Project (Ref. Code:\nHWJL-2023.003, Project No.: WEB24EG01-A).\nREFERENCES\n[1]\nK. Affolter, K. Stockinger, and A. Bernstein.\nA comparative\nsurvey of recent natural language interfaces for databases. VLDB\nJ., 28(5):793–819, 2019.\n[2]\nI. Androutsopoulos, G. D. Ritchie, and P. Thanisch.\nNatural\nlanguage interfaces to databases - an introduction.\nNat. Lang.\nEng., 1(1):29–81, 1995.\n[3]\nC. Baik, H. V. Jagadish, and Y. Li. Bridging the semantic gap with\nSQL query logs in natural language interfaces to databases. In\nICDE, pages 374–385. IEEE, 2019.\n[4]\nB. Bogin, J. Berant, and M. Gardner.\nRepresenting schema\nstructure with graph neural networks for text-to-sql parsing. In\nACL, pages 4560–4565, 2019.\n[5]\nB. Bogin, M. Gardner, and J. Berant.\nGlobal reasoning over\ndatabase structures for text-to-sql parsing. In EMNLP-IJCNLP,\npages 3657–3662, 2019.\n[6]\nR. Cai, J. Yuan, B. Xu, and Z. Hao.\nSADGA: structure-aware\ndual graph aggregation network for text-to-sql. In NeurIPS, pages\n7664–7676, 2021.\n[7]\nR. Cao, L. Chen, Z. Chen, Y. Zhao, S. Zhu, and K. Yu. LGESQL:\nline graph enhanced text-to-sql model with mixed local and non-\nlocal relations. In ACL/IJCNLP, pages 2541–2555, 2021.\n[8]\nS. Chang, J. Wang, M. Dong, L. Pan, H. Zhu, A. H. Li, W. Lan,\nS. Zhang, J. Jiang, J. Lilien, S. Ash, W. Y. Wang, Z. Wang,\nV. Castelli, P. Ng, and B. Xiang. Dr.spider: A diagnostic evaluation\nbenchmark towards text-to-sql robustness. In ICLR, 2023.\n[9]\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto,\nJ. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray,\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,\nM. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,\nM. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji,\nS. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,\nV. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei,\nS. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large\nlanguage models trained on code. CoRR, abs/2107.03374, 2021.\n[10]\nY. Chen, R. Li, A. Mac, T. Xie, T. Yu, and E. Wu. NL2INTERFACE:\ninteractive visualization interface generation from natural lan-\nguage queries. CoRR, abs/2209.08834, 2022.\n[11]\nZ. Chen, L. Chen, Y. Zhao, R. Cao, Z. Xu, S. Zhu, and K. Yu. Shad-\nowgnn: Graph projection neural network for text-to-sql parser. In\nNAACL-HLT, pages 5567–5577, 2021.\n[12]\nD. Choi, M. Shin, E. Kim, and D. R. Shin. RYANSQL: recursively\napplying sketch-based slot fillings for complex text-to-sql in\ncross-domain databases. Comput. Linguistics, 47(2):309–332, 2021.\n[13]\nD. A. Dahl, M. Bates, M. Brown, W. M. Fisher, K. Hunicke-Smith,\nD. S. Pallett, C. Pao, A. I. Rudnicky, and E. Shriberg. Expanding\nthe scope of the ATIS task: The ATIS-3 corpus. In Human Language\nTechnology, Proceedings of a Workshop held at Plainsboro, New Jerey,\nUSA, March 8-11, 1994, 1994.\n[14]\nN. Deng, Y. Chen, and Y. Zhang. Recent advances in text-to-sql:\nA survey of what we have and what we expect. In COLING,\npages 2166–2187, 2022.\n[15]\nX. Deng, A. H. Awadallah, C. Meek, O. Polozov, H. Sun, and\nM. Richardson.\nStructure-grounded pretraining for text-to-sql.\nIn NAACL-HLT, pages 1337–1350, 2021.\n[16]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training\nof deep bidirectional transformers for language understanding.\nIn NAACL-HLT, pages 4171–4186, 2019.\n[17]\nV. Dibia and C¸ . Demiralp.\nData2vis: Automatic generation of\ndata visualizations using sequence-to-sequence recurrent neural\nnetworks. IEEE Computer Graphics and Applications, 39(5):33–46,\n2019.\n[18]\nG. Doddington.\nAutomatic evaluation of machine translation\nquality using n-gram co-occurrence statistics. In Proceedings of\nthe Second International Conference on Human Language Technology\nResearch, HLT ’02, page 138–145, 2002.\n[19]\nL. Dong and M. Lapata. Language to logical form with neural\nattention. In ACL, 2016.\n[20]\nL. Dong and M. Lapata.\nCoarse-to-fine decoding for neural\nsemantic parsing. In ACL, pages 731–742, 2018.\n[21]\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu,\nL. Li, and Z. Sui. A survey on in-context learning, 2023.\n[22]\nX. Dong, C. Zhang, Y. Ge, Y. Mao, Y. Gao, L. Chen, J. Lin,\nand D. Lou.\nC3: zero-shot text-to-sql with chatgpt.\nCoRR,\nabs/2307.07306, 2023.\n[23]\nL. Dou, Y. Gao, X. Liu, M. Pan, D. Wang, W. Che, D. Zhan, M. Kan,\nand J. Lou.\nTowards knowledge-intensive text-to-sql semantic\nparsing with formulaic knowledge. In EMNLP, pages 5240–5253,\n2022.\n[24]\nL. Dou, Y. Gao, M. Pan, D. Wang, J. Lou, W. Che, and D. Zhan.\nUnisar: A unified structure-aware autoregressive language model\nfor text-to-sql. CoRR, abs/2203.07781, 2022.\n[25]\nC. Finegan-Dollak, J. K. Kummerfeld, L. Zhang, K. Ramanathan,\nS. Sadasivam, R. Zhang, and D. R. Radev. Improving text-to-sql\nevaluation methodology. In ACL, pages 351–360, 2018.\n[26]\nY. Gan, X. Chen, Q. Huang, and M. Purver.\nMeasuring and\nimproving compositional generalization in text-to-sql via com-\nponent alignment. In Findings of NAACL, pages 831–843, 2022.\n[27]\nY. Gan, X. Chen, Q. Huang, M. Purver, J. R. Woodward, J. Xie,\nand P. Huang. Towards robustness of text-to-sql models against\nsynonym substitution. In ACL/IJCNLP, pages 2505–2515, 2021.\n[28]\nY. Gan, X. Chen, and M. Purver.\nExploring underexplored\nlimitations of cross-domain text-to-sql generalization. In EMNLP,\npages 8926–8931, 2021.\n[29]\nY. Gan, X. Chen, J. Xie, M. Purver, J. R. Woodward, J. H. Drake,\nand Q. Zhang. Natural SQL: making SQL easier to infer from\nnatural language specifications.\nIn Findings of EMNLP, pages\n2030–2042, 2021.\n[30]\nT. Gao, M. Dontcheva, E. Adar, Z. Liu, and K. G. Karahalios.\nDatatone: Managing ambiguity in natural language interfaces\nfor data visualization.\nIn Proceedings of the 28th Annual ACM\nSymposium on User Interface Software & Technology, UIST, pages\n489–500. ACM, 2015.\n[31]\nO. Gkini, T. Belmpas, G. Koutrika, and Y. E. Ioannidis. An in-\ndepth benchmarking of text-to-sql systems. In SIGMOD, pages\n632–644. ACM, 2021.\n[32]\nB. J. Grosz. TEAM: A transportable natural-language interface\nsystem.\nIn 1st Applied Natural Language Processing Conference,\nANLP, pages 39–45, 1983.\n[33]\nZ. Gu, J. Fan, N. Tang, L. Cao, B. Jia, S. Madden, and X. Du. Few-\nshot text-to-sql translation using structure and content prompt\nlearning. Proc. ACM Manag. Data, 1(2):147:1–147:28, 2023.\n[34]\nZ. Gu, J. Fan, N. Tang, S. Zhang, Y. Zhang, Z. Chen, L. Cao, G. Li,\nS. Madden, and X. Du. Interleaving pre-trained language models\nand large language models for zero-shot NL2SQL generation.\nCoRR, abs/2306.08891, 2023.\n[35]\nC. Guo, Z. Tian, J. Tang, S. Li, Z. Wen, K. Wang, and T. Wang.\nRetrieval-augmented gpt-3.5-based text-to-sql framework with\nsample-aware prompting and dynamic revision chain.\nCoRR,\nabs/2307.05074, 2023.\n[36]\nJ. Guo, J. Lou, T. Liu, and D. Zhang. Weakly supervised semantic\nparsing by learning from mistakes. In Findings of EMNLP, pages\n2603–2617, 2021.\n[37]\nJ. Guo, Z. Si, Y. Wang, Q. Liu, M. Fan, J. Lou, Z. Yang, and T. Liu.\nChase: A large-scale and pragmatic chinese dataset for cross-\ndatabase context-dependent text-to-sql.\nIn ACL/IJCNLP, pages\n2316–2331, 2021.\n[38]\nJ. Guo, Z. Zhan, Y. Gao, Y. Xiao, J. Lou, T. Liu, and D. Zhang.\nTowards complex text-to-sql in cross-domain database with in-\ntermediate representation. In ACL, pages 4524–4535, 2019.\n[39]\nM. Hazoom, V. Malik, and B. Bogin. Text-to-sql in the wild: A\nnaturally-occurring dataset based on stack exchange data. CoRR,\nabs/2106.05006, 2021.\n[40]\nP. He, Y. Mao, K. Chakrabarti, and W. Chen. X-SQL: reinforce\nschema representation with context. CoRR, abs/1908.08113, 2019.\n[41]\nE. Hoque, V. Setlur, M. Tory, and I. Dykeman. Applying pragmat-\nics principles for interaction with visual analytics. IEEE Trans. Vis.\nComput. Graph., 24(1):309–318, 2018.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n18\n[42]\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen.\nLora: Low-rank adaptation of large language\nmodels. In ICLR, 2022.\n[43]\nB. Hui, R. Geng, Q. Ren, B. Li, Y. Li, J. Sun, F. Huang, L. Si,\nP. Zhu, and X. Zhu. Dynamic hybrid relation exploration network\nfor cross-domain context-dependent semantic parsing. In AAAI,\npages 13116–13124, 2021.\n[44]\nB. Hui, R. Geng, L. Wang, B. Qin, Y. Li, B. Li, J. Sun, and\nY. Li. S2sql: Injecting syntax to question-schema interaction graph\nencoder for text-to-sql parsers. In Findings of ACL, pages 1254–\n1262, 2022.\n[45]\nW. Hwang, J. Yim, S. Park, and M. Seo.\nA comprehensive\nexploration on wikisql with table-aware word contextualization.\nCoRR, abs/1902.01069, 2019.\n[46]\nR. C. A. Iacob, F. Brad, E. S. Apostol, C. Truica, I. Hosu, and\nT. Rebedea. Neural approaches for natural language interfaces to\ndatabases: A survey. In COLING, pages 381–395, 2020.\n[47]\nS. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettle-\nmoyer. Learning a neural semantic parser from user feedback. In\nACL, pages 963–973, 2017.\n[48]\nM. A. Jos´e and F. G. Cozman. mrat-sql+gap: A portuguese text-\nto-sql transformer. In BRACIS, volume 13074 of Lecture Notes in\nComputer Science, pages 511–525, 2021.\n[49]\nA. Kamath and R. Das. A survey on semantic parsing. In AKBC,\n2019.\n[50]\nJ. Kassel and M. Rohs.\nValletto: A multimodal interface for\nubiquitous visual analytics. In CHI. ACM, 2018.\n[51]\nG. Katsogiannis-Meimarakis and G. Koutrika. A deep dive into\ndeep learning approaches for text-to-sql systems. In SIGMOD,\npages 2846–2851, 2021.\n[52]\nG. Katsogiannis-Meimarakis and G. Koutrika. A survey on deep\nlearning approaches for text-to-sql. VLDB J., 32(4):905–936, 2023.\n[53]\nH. Kim, B. So, W. Han, and H. Lee. Natural language to SQL:\nwhere are we today? Proc. VLDB Endow., 13(10):1737–1750, 2020.\n[54]\nA. Kumar, J. Aurisano, B. D. Eugenio, A. E. Johnson, A. Gonzalez,\nand J. Leigh.\nTowards a dialogue system that supports rich\nvisualizations of data. In SIGDIAL, pages 304–309, 2016.\n[55]\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V. Stoyanov, and L. Zettlemoyer.\nBART: denoising\nsequence-to-sequence pre-training for natural language genera-\ntion, translation, and comprehension. In ACL, pages 7871–7880,\n2020.\n[56]\nF. Li and H. V. Jagadish.\nConstructing an interactive natural\nlanguage interface for relational databases. Proc. VLDB Endow.,\n8(1):73–84, 2014.\n[57]\nH. Li, J. Zhang, C. Li, and H. Chen.\nRESDSQL: decoupling\nschema linking and skeleton parsing for text-to-sql.\nIn AAAI,\npages 13067–13075, 2023.\n[58]\nJ. Li, B. Hui, R. Cheng, B. Qin, C. Ma, N. Huo, F. Huang, W. Du,\nL. Si, and Y. Li. Graphix-t5: Mixing pre-trained transformers with\ngraph-aware layers for text-to-sql parsing. In AAAI, pages 13076–\n13084, 2023.\n[59]\nJ. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\nR. Geng, N. Huo, X. Zhou, C. Ma, G. Li, K. C. Chang, F. Huang,\nR. Cheng, and Y. Li.\nCan LLM already serve as A database\ninterface? A big bench for large-scale database grounded text-\nto-sqls. CoRR, abs/2305.03111, 2023.\n[60]\nX. V. Lin, R. Socher, and C. Xiong.\nBridging textual and tab-\nular data for cross-domain text-to-sql semantic parsing. CoRR,\nabs/2012.12627, 2020.\n[61]\nX. V. Lin, C. Wang, L. Zettlemoyer, and M. D. Ernst. Nl2bash: A\ncorpus and semantic parser for natural language interface to the\nlinux operating system. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation, LREC, 2018.\n[62]\nA. Liu, X. Hu, L. Wen, and P. S. Yu.\nA comprehensive\nevaluation of chatgpt’s zero-shot text-to-sql capability.\nCoRR,\nabs/2303.13547, 2023.\n[63]\nC. Liu, Y. Han, R. Jiang, and X. Yuan.\nAdvisor: Automatic\nvisualization answer for natural-language question on tabular\ndata. In 14th IEEE Pacific Visualization Symposium, pages 11–20.\nIEEE, 2021.\n[64]\nX. Liu and Z. Tan.\nDivide and prompt: Chain of thought\nprompting for text-to-sql. CoRR, abs/2304.11556, 2023.\n[65]\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly\noptimized BERT pretraining approach.\nCoRR, abs/1907.11692,\n2019.\n[66]\nY. Luo, W. Li, T. Zhao, X. Yu, L. Zhang, G. Li, and N. Tang.\nDeeptrack: Monitoring and exploring spatio-temporal data - A\ncase of tracking COVID-19 -. Proc. VLDB Endow., 13(12):2841–\n2844, 2020.\n[67]\nY. Luo, X. Qin, N. Tang, G. Li, and X. Wang. Deepeye: Creating\ngood data visualizations by keyword search. In SIGMOD. ACM,\n2018.\n[68]\nY. Luo, J. Tang, and G. Li. nvbench: A large-scale synthesized\ndataset for cross-domain natural language to visualization task.\nCoRR, abs/2112.12926, 2021.\n[69]\nY. Luo, N. Tang, G. Li, C. Chai, W. Li, and X. Qin. Synthesizing\nnatural language to visualization (NL2VIS) benchmarks from\nNL2SQL benchmarks. In SIGMOD, pages 1235–1247. ACM, 2021.\n[70]\nY. Luo, N. Tang, G. Li, J. Tang, C. Chai, and X. Qin.\nNatural\nlanguage to visualization by neural machine translation. IEEE\nTrans. Vis. Comput. Graph., 28(1):217–226, 2022.\n[71]\nQ. Lyu, K. Chakrabarti, S. Hathi, S. Kundu, J. Zhang, and Z. Chen.\nHybrid ranking network for text-to-sql. CoRR, abs/2008.04759,\n2020.\n[72]\nP. Maddigan and T. Susnjak. Chat2vis: Generating data visualisa-\ntions via natural language using chatgpt, codex and GPT-3 large\nlanguage models. CoRR, abs/2302.02094, 2023.\n[73]\nQ. Min, Y. Shi, and Y. Zhang.\nA pilot study for chinese SQL\nsemantic parsing. In EMNLP-IJCNLP, pages 3650–3656, 2019.\n[74]\nR. Mitra, A. Narechania, A. Endert, and J. T. Stasko. Facilitat-\ning conversational interaction in natural language interfaces for\nvisualization. CoRR, abs/2207.00189, 2022.\n[75]\nL. Nan, Y. Zhao, W. Zou, N. Ri, J. Tae, E. Zhang, A. Cohan, and\nD. Radev.\nEnhancing few-shot text-to-sql capabilities of large\nlanguage models: A study on prompt design strategies. CoRR,\nabs/2305.12586, 2023.\n[76]\nA. Narechania, A. Srinivasan, and J. T. Stasko.\nNL4DV: A\ntoolkit for generating analytic specifications for data visualization\nfrom natural language queries. IEEE Trans. Vis. Comput. Graph.,\n27(2):369–379, 2021.\n[77]\nA. T. Nguyen, M. H. Dao, and D. Q. Nguyen.\nA pilot study\nof text-to-sql semantic parsing for vietnamese.\nIn Findings of\nEMNLP, pages 4079–4085, 2020.\n[78]\nA. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel, and\nC. Callison-Burch. Bidirectional language models are also few-\nshot learners. In ICLR, 2023.\n[79]\nA. Popescu, A. Armanasu, O. Etzioni, D. Ko, and A. Yates.\nModern natural language interfaces to databases: Composing\nstatistical parsing with semantic tractability. In COLING, 2004.\n[80]\nA. Popescu, O. Etzioni, and H. A. Kautz. Towards a theory of\nnatural language interfaces to databases. In IUI, pages 149–157,\n2003.\n[81]\nM. Pourreza and D. Rafiei.\nDIN-SQL: decomposed in-context\nlearning of text-to-sql with self-correction. In NeurIPS, 2023.\n[82]\nP. J. Price.\nEvaluation of spoken language systems: the ATIS\ndomain. In Speech and Natural Language: Proceedings of a Workshop\nHeld at Hidden Valley, 1990.\n[83]\nX. Qin, Y. Luo, N. Tang, and G. Li. Making data visualization\nmore efficient and effective: a survey. VLDB J., 29(1):93–117, 2020.\n[84]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer\nlearning with a unified text-to-text transformer. J. Mach. Learn.\nRes., 21:140:1–140:67, 2020.\n[85]\nN. Rajkumar, R. Li, and D. Bahdanau. Evaluating the text-to-\nsql capabilities of large language models. CoRR, abs/2204.00498,\n2022.\n[86]\nO. Rubin and J. Berant. Smbop: Semi-autoregressive bottom-up\nsemantic parsing. In ACL-IJCNLP, pages 12–21, 2021.\n[87]\nD. Saha, A. Floratou, K. Sankaranarayanan, U. F. Minhas, A. R.\nMittal, and F. ¨Ozcan. ATHENA: an ontology-driven system for\nnatural language querying over relational data stores. Proc. VLDB\nEndow., 9(12):1209–1220, 2016.\n[88]\nT. Scholak, N. Schucher, and D. Bahdanau.\nPICARD: parsing\nincrementally for constrained auto-regressive decoding from lan-\nguage models. In EMNLP, pages 9895–9901, 2021.\n[89]\nV. Setlur, S. E. Battersby, M. Tory, R. Gossweiler, and A. X.\nChang. Eviza: A natural language interface for visual analysis. In\nProceedings of the 29th Annual Symposium on User Interface Software\nand Technology, UIST, pages 365–377. ACM, 2016.\n[90]\nY. Shao and N. Nakashole. Chartdialogs: Plotting from natural\nlanguage instructions. In ACL, pages 3559–3574, 2020.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n19\n[91]\nP. Shaw, M. Chang, P. Pasupat, and K. Toutanova. Compositional\ngeneralization and natural language variation: Can a semantic\nparsing approach handle both? In ACL/IJCNLP, pages 922–938,\n2021.\n[92]\nL. Shen, E. Shen, Y. Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, and\nJ. Wang. Towards natural language interfaces for data visualiza-\ntion: A survey. IEEE Trans. Vis. Comput. Graph., 29(6):3121–3144,\n2023.\n[93]\nP. Shi, P. Ng, Z. Wang, H. Zhu, A. H. Li, J. Wang, C. N. dos Santos,\nand B. Xiang. Learning contextual representations for semantic\nparsing with generation-augmented pre-training. In AAAI, pages\n13806–13814, 2021.\n[94]\nT. Shi, K. Tatwawadi, K. Chakrabarti, Y. Mao, O. Polozov, and\nW. Chen.\nIncsql: Training incremental text-to-sql parsers with\nnon-deterministic oracles. CoRR, abs/1809.05054, 2018.\n[95]\nY. Song, R. C. Wong, X. Zhao, and D. Jiang. Voicequerysystem:\nA voice-driven database querying system using natural language\nquestions. In SIGMOD, pages 2385–2388. ACM, 2022.\n[96]\nY. Song, X. Zhao, and R. C. Wong. Marrying dialogue systems\nwith data visualization: Interactive data visualization generation\nfrom natural language conversations.\nCoRR, abs/2307.16013,\n2023.\n[97]\nY. Song, X. Zhao, R. C. Wong, and D. Jiang. Rgvisnet: A hybrid\nretrieval-generation neural framework towards automatic data\nvisualization generation. In KDD, pages 1646–1655. ACM, 2022.\n[98]\nA. Srinivasan, B. Lee, N. H. Riche, S. M. Drucker, and K. Hinckley.\nInchorus: Designing consistent multimodal interactions for data\nvisualization on tablet devices. In CHI, pages 1–13. ACM, 2020.\n[99]\nA. Srinivasan, N. Nyapathy, B. Lee, S. M. Drucker, and J. T.\nStasko.\nCollecting and characterizing natural language utter-\nances for specifying data visualizations.\nIn CHI, pages 464:1–\n464:10, 2021.\n[100] A. Srinivasan and J. T. Stasko.\nOrko: Facilitating multimodal\ninteraction for visual exploration and analysis of networks. IEEE\nTrans. Vis. Comput. Graph., 24(1):511–521, 2018.\n[101] A. St¨ockl.\nNatural language interface for data visualization\nwith deep learning based language models. In 26th International\nConference Information Visualisation, IV, pages 142–148. IEEE, 2022.\n[102] A. Suhr, M. Chang, P. Shaw, and K. Lee. Exploring unexplored\ngeneralization challenges for cross-database semantic parsing. In\nACL, pages 8372–8388, 2020.\n[103] N. Sun, X. Yang, and Y. Liu.\nTableqa: a large-scale chi-\nnese text-to-sql dataset for table-aware SQL generation. CoRR,\nabs/2006.06434, 2020.\n[104] R. Sun, S. ¨O. Arik, H. Nakhost, H. Dai, R. Sinha, P. Yin, and\nT. Pfister. Sql-palm: Improved large language model adaptation\nfor text-to-sql. CoRR, abs/2306.00739, 2023.\n[105] C. Tai, Z. Chen, T. Zhang, X. Deng, and H. Sun. Exploring chain-\nof-thought style prompting for text-to-sql. CoRR, abs/2305.14215,\n2023.\n[106] J. Tang, Y. Luo, M. Ouzzani, G. Li, and H. Chen. Sevi: Speech-to-\nvisualization through neural machine translation. In SIGMOD,\npages 2353–2356. ACM, 2022.\n[107] L. R. Tang and R. J. Mooney. Automated construction of database\ninterfaces: Intergrating statistical and relational learning for se-\nmantic parsing. In EMNLP, pages 133–141, 2000.\n[108] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson. RAT-\nSQL: relation-aware schema encoding and linking for text-to-sql\nparsers. In ACL, pages 7567–7578, 2020.\n[109] C.\nWang,\nP.\nHuang,\nA.\nPolozov,\nM.\nBrockschmidt,\nand\nR. Singh.\nExecution-guided neural program decoding.\nCoRR,\nabs/1807.03100, 2018.\n[110] L. Wang, A. Zhang, K. Wu, K. Sun, Z. Li, H. Wu, M. Zhang, and\nH. Wang. Dusql: A large-scale and pragmatic chinese text-to-sql\ndataset. In EMNLP, pages 6923–6935, 2020.\n[111] P. Wang, T. Shi, and C. K. Reddy.\nText-to-sql generation for\nquestion answering on electronic medical records.\nIn WWW,\npages 350–361, 2020.\n[112] D. H. D. Warren and F. C. N. Pereira. An efficient easily adaptable\nsystem for interpreting natural language queries. Am. J. Comput.\nLinguistics, 8(3-4):110–122, 1982.\n[113] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi,\nT. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus.\nEmergent abilities of large language models. Trans. Mach. Learn.\nRes., 2022, 2022.\n[114] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H.\nChi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits\nreasoning in large language models. In NeurIPS, 2022.\n[115] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga,\nC. Wu, M. Zhong, P. Yin, S. I. Wang, V. Zhong, B. Wang, C. Li,\nC. Boyle, A. Ni, Z. Yao, D. Radev, C. Xiong, L. Kong, R. Zhang,\nN. A. Smith, L. Zettlemoyer, and T. Yu. Unifiedskg: Unifying and\nmulti-tasking structured knowledge grounding with text-to-text\nlanguage models. In EMNLP, pages 602–631, 2022.\n[116] X. Xu, C. Liu, and D. Song. Sqlnet: Generating structured queries\nfrom natural language without reinforcement learning.\nCoRR,\nabs/1711.04436, 2017.\n[117] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. Sqlizer: query\nsynthesis from natural language.\nProc. ACM Program. Lang.,\n1(OOPSLA):63:1–63:26, 2017.\n[118] P. Yin and G. Neubig.\nA syntactic neural model for general-\npurpose code generation. In ACL, pages 440–450, 2017.\n[119] P. Yin, G. Neubig, W. Yih, and S. Riedel. Tabert: Pretraining for\njoint understanding of textual and tabular data. In ACL, pages\n8413–8426, 2020.\n[120] B. Yu and C. T. Silva. Visflow - web-based visualization frame-\nwork for tabular data with a subset flow model. IEEE Trans. Vis.\nComput. Graph., 23(1):251–260, 2017.\n[121] B. Yu and C. T. Silva. Flowsense: A natural language interface\nfor visual data exploration within a dataflow system. IEEE Trans.\nVis. Comput. Graph., 26(1):1–11, 2020.\n[122] T. Yu, Z. Li, Z. Zhang, R. Zhang, and D. R. Radev.\nTypesql:\nKnowledge-based type-aware neural text-to-sql generation.\nIn\nNAACL-HLT, pages 588–594, 2018.\n[123] T. Yu, C. Wu, X. V. Lin, B. Wang, Y. C. Tan, X. Yang, D. R.\nRadev, R. Socher, and C. Xiong. Grappa: Grammar-augmented\npre-training for table semantic parsing. In ICLR, 2021.\n[124] T. Yu, M. Yasunaga, K. Yang, R. Zhang, D. Wang, Z. Li, and D. R.\nRadev. Syntaxsqlnet: Syntax tree networks for complex and cross-\ndomain text-to-sql task. In EMNLP, pages 1653–1663, 2018.\n[125] T. Yu, R. Zhang, H. Er, S. Li, E. Xue, B. Pang, X. V. Lin, Y. C. Tan,\nT. Shi, Z. Li, Y. Jiang, M. Yasunaga, S. Shim, T. Chen, A. R. Fabbri,\nZ. Li, L. Chen, Y. Zhang, S. Dixit, V. Zhang, C. Xiong, R. Socher,\nW. S. Lasecki, and D. R. Radev. Cosql: A conversational text-to-\nsql challenge towards cross-domain natural language interfaces\nto databases. In EMNLP-IJCNLP, pages 1962–1979, 2019.\n[126] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma,\nI. Li, Q. Yao, S. Roman, Z. Zhang, and D. R. Radev. Spider: A\nlarge-scale human-labeled dataset for complex and cross-domain\nsemantic parsing and text-to-sql task. In EMNLP, pages 3911–\n3921, 2018.\n[127] T. Yu, R. Zhang, M. Yasunaga, Y. C. Tan, X. V. Lin, S. Li, H. Er,\nI. Li, B. Pang, T. Chen, E. Ji, S. Dixit, D. Proctor, S. Shim, J. Kraft,\nV. Zhang, C. Xiong, R. Socher, and D. R. Radev. Sparc: Cross-\ndomain semantic parsing in context. In ACL, pages 4511–4523,\n2019.\n[128] J. M. Zelle and R. J. Mooney. Learning to parse database queries\nusing inductive logic programming. In AAAI, pages 1050–1055,\n1996.\n[129] J. Zeng, X. V. Lin, S. C. H. Hoi, R. Socher, C. Xiong, M. R. Lyu,\nand I. King. Photon: A robust cross-domain text-to-sql system.\nIn ACL, pages 204–214, 2020.\n[130] R. Zhang, T. Yu, H. Er, S. Shim, E. Xue, X. V. Lin, T. Shi,\nC. Xiong, R. Socher, and D. R. Radev. Editing-based SQL query\ngeneration for cross-domain context-dependent questions.\nIn\nEMNLP-IJCNLP, pages 5337–5348, 2019.\n[131] R. Zhong, T. Yu, and D. Klein. Semantic evaluation for text-to-sql\nwith distilled test suites. In EMNLP, pages 396–411, 2020.\n[132] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured\nqueries from natural language using reinforcement learning.\nCoRR, abs/1709.00103, 2017.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, AUGUST 20XX\n20\nWeixu Zhang received the B.Eng. degree from\nXi’an Jiaotong University, China and Engineer’s\ndegree from Paris-Saclay University, France.\nShe is currently a master candidate in Xi’an Jiao-\ntong University. Her research interests include\nsemantic parsing, data mining, and natural lan-\nguage processing.\nYifei Wang is currently pursuing a Bachelor of\nApplied Science in Machine Intelligence at the\nUniversity of Toronto. Her research interests in-\nclude natural language processing, data visual-\nization, and data querying.\nYuanfeng Song received the M.Phil degree in\ncomputer science from the Hong Kong Univer-\nsity of Science and Technology in 2012. He is\ncurrently a senior researcher at WeBank AI. His\nresearch interests include information retrieval,\nnatural language processing, and speech recog-\nnition.\nVictor Junqiu Wei is currently working as a re-\nsearch assistant professor in the Department of\nComputer Science and Engineering (CSE), the\nHong Kong University of Science and Technol-\nogy (HKUST). He obtained his bachelor degree\nfrom Nanjing University and PhD degree from\nDepartment of Computer Science and Engineer-\ning, the Hong Kong University of Science and\nTechnology. He also has several years’ working\nexperience in the world-famous research labs in\nthe AI industry including Baidu natural language\nprocessing (NLP) group, the AI group of WeBank, and Noah’s Ark Lab\nof Huawei.\nYuxing Tian is currently pursuing the B.Eng. de-\ngree at Xidian University. His research interests\ninclude graph neural networks, large language\nmodels, and federated learning.\nYiyan Qi received the B.S in automation engi-\nneering and the Ph.D. degree in automatic con-\ntrol from Xi’an Jiaotong University, Xi’an, China,\nin 2014 and 2021 respectively. He is currently a\nResearcher at IDEA. Prior to joining IDEA, he\nwas working in Tencent. His current research in-\nterests include abnormal detection, graph mining\nand embedding, and recommender systems.\nJonathan H. Chan is an Associate Professor of\nComputer Science at the School of Information\nTechnology, King Mongkut’s University of Tech-\nnology Thonburi (KMUTT), Thailand. Jonathan\nholds a B.A.Sc., M.A.Sc., and Ph.D. degree from\nthe University of Toronto and was a visiting pro-\nfessor back there on several occasions. He also\nholds a status as a visiting scientist at The Cen-\ntre for Applied Genomics at Sick Kids Hospital in\nToronto. Jonathan is the Director of the Innova-\ntive Cognitive Computing (IC2) Research Center\nat KMUTT. In addition, he is a founding member and a past Chair of the\nIEEE-CIS Thailand Chapter, the current President (2023-2024) of the\nAsia Pacific Neural Network Society (APNNS), and a senior member of\nIEEE, ACM, INNS, and APNNS. His research interests include intelligent\nsystems, cognitive computing, biomedical informatics, data science, and\nmachine learning in general.\nRaymond Chi-Wing Wong is a Professor in\nComputer Science and Engineering (CSE) of\nThe Hong Kong University of Science and Tech-\nnology (HKUST). He is currently the associate\nhead of Department of Computer Science and\nEngineering (CSE). He was the director of the\nRisk Management and Business Intelligence\n(RMBI) program (from 2017 to 2019) and the\nComputer Engineering (CPEG) program (from\n2014 to 2016). He received the BSc, MPhil and\nPhD degrees in Computer Science and Engi-\nneering in the Chinese University of Hong Kong (CUHK) in 2002, 2004\nand 2008, respectively.\nHaiqin Yang (M’11, SM’18) received the BSc\ndegree in computer science from Nanjing Uni-\nversity, Nanjing, China, and the MPhil and PhD\ndegrees from Department of Computer Science\nand Engineering, The Chinese University of\nHong Kong, Hong Kong. He is currently a prin-\ncipal researcher at IDEA. His research inter-\nests include machine learning, natural language\nprocessing, and large language models. He re-\nceived the Young Researcher Award of the Asia\nPacific Neural Network Society in 2018 and was\nrecognized by AMiner’s Most Influential Scholar Award Honorable Men-\ntion to the field of AAAI/IJCAI three times.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2023-10-27",
  "updated": "2024-05-20"
}