{
  "id": "http://arxiv.org/abs/2310.18116v2",
  "title": "Direct Unsupervised Denoising",
  "authors": [
    "Benjamin Salmon",
    "Alexander Krull"
  ],
  "abstract": "Traditional supervised denoisers are trained using pairs of noisy input and\nclean target images. They learn to predict a central tendency of the posterior\ndistribution over possible clean images. When, e.g., trained with the popular\nquadratic loss function, the network's output will correspond to the minimum\nmean square error (MMSE) estimate. Unsupervised denoisers based on Variational\nAutoEncoders (VAEs) have succeeded in achieving state-of-the-art results while\nrequiring only unpaired noisy data as training input. In contrast to the\ntraditional supervised approach, unsupervised denoisers do not directly produce\na single prediction, such as the MMSE estimate, but allow us to draw samples\nfrom the posterior distribution of clean solutions corresponding to the noisy\ninput. To approximate the MMSE estimate during inference, unsupervised methods\nhave to create and draw a large number of samples - a computationally expensive\nprocess - rendering the approach inapplicable in many situations. Here, we\npresent an alternative approach that trains a deterministic network alongside\nthe VAE to directly predict a central tendency. Our method achieves results\nthat surpass the results achieved by the unsupervised method at a fraction of\nthe computational cost.",
  "text": "Direct Unsupervised Denoising\nBenjamin Salmon\nand Alexander Krull\nUniversity of Birmingham\nBirmingham B15 2TT, UK\nbrs209@student.bham.ac.uk, a.f.f.krull@bham.ac.uk\nAbstract\nTraditional supervised denoisers are trained using pairs\nof noisy input and clean target images. They learn to pre-\ndict a central tendency of the posterior distribution over\npossible clean images. When, e.g., trained with the popu-\nlar quadratic loss function, the network’s output will cor-\nrespond to the minimum mean square error (MMSE) es-\ntimate. Unsupervised denoisers based on Variational Au-\ntoEncoders (VAEs) have succeeded in achieving state-of-\nthe-art results while requiring only unpaired noisy data as\ntraining input. In contrast to the traditional supervised ap-\nproach, unsupervised denoisers do not directly produce a\nsingle prediction, such as the MMSE estimate, but allow\nus to draw samples from the posterior distribution of clean\nsolutions corresponding to the noisy input. To approximate\nthe MMSE estimate during inference, unsupervised methods\nhave to create and draw a large number of samples – a com-\nputationally expensive process – rendering the approach in-\napplicable in many situations. Here, we present an alterna-\ntive approach that trains a deterministic network alongside\nthe VAE to directly predict a central tendency. Our method\nachieves results that surpass the results achieved by the un-\nsupervised method at a fraction of the computational cost.\n1. Introduction\nThe prevalence of noise in biomedical imaging makes\ndenoising a necessary step for many applications [14]. Deep\nlearning has proven itself to be the most powerful tool\nfor this task, as is evidenced by a growing body of re-\nsearch [27]. Although deep learning-based approaches typi-\ncally require large amounts of training data, recent advances\nin unsupervised deep learning [20, 19, 25] have shown that\nthis requirement need not be a barrier to their use. Unlike\nwith supervised deep learning-based denoisers, which are\ntrained with pairs of corresponding noisy and noise-free im-\nages, users of unsupervised methods can train their models\nwith the very data they want to denoise.\n101\n102\n103\n104\nInference time (s)\n34.0\n34.5\n35.0\n35.5\n36.0\n36.5\n37.0\n37.5\nPSNR (dB)\nDirect solution\n1 sample\n10 samples\n100 samples 1000 samples\nFigure 1.\nOur Direct Denoiser outperforms unsupervised\nVAE-based denoising (HDN) [19], while requiring only a frac-\ntion of the computational cost: In red, the time to draw 1, 10,\n100 and 1000 samples from HDN’s learned denoising distribution\nplotted against the PSNR (higher is better) of the per-pixel mean of\nthese samples. Additionally, in blue, the time to take a single so-\nlution from our Direct Denoiser is plotted against its PSNR. These\nresults are from denoising the Convallaria dataset.\nThe performance of unsupervised deep learning-based\ndenoisers is now approaching and even sometimes match-\ning the performance of their supervised counterparts [20,\n19, 25], however, these two methods are fundamentally dif-\nferent in the way they do inference. By training a Varia-\ntional AutoEncoder (VAE) [11], unsupervised methods ap-\nproximate a posterior distribution over the clean images that\ncould underlie a noisy input image. This distribution will\nbe referred to as the denoising distribution. Random sam-\nples from the denoising distribution then constitute the infi-\nnite possible solutions to a denoising problem. Supervised\nand self-supervised learning methods, on the other hand,\noffer a single prediction that compromises between all pos-\nsible solutions. This is usually a central tendency of the de-\nnoising distribution and the specific central tendency that is\npredicted depends on the loss function used. For example,\narXiv:2310.18116v2  [cs.CV]  4 Dec 2023\na supervised method trained with the mean squared error\n(MSE) loss function will predict the mean, which is also\nknown as the minimum mean squared error (MMSE) esti-\nmate. A model trained with the mean absolute error (MAE)\nloss function will predict the pixel-wise median, which is\nknown as the minimum mean absolute error (MMAE) esti-\nmate.\nWhile the ability of unsupervised methods to produce di-\nverse solutions can in some circumstances be beneficial for\ndownstream processing [20], users oftentimes require only\na single solution such as the MMSE estimate. If they are to\napproximate this from an unsupervised learning-based de-\nnoiser, they must process their image many times and av-\nerage many possible sampled solutions, leading to a sig-\nnificant computational overhead. For example, the authors\nof [20, 19, 25] average 100 or 1000 samples per image to\nobtain their MMSE estimate. Such an approach requires\nsubstantial computational effort, and is not likely to be eco-\nnomically and ecologically reasonable for labs regularly an-\nalyzing terabytes of data.\nThis paper presents an alternative route to estimating the\ncentral tendencies from an unsupervised denoiser; one that\nrequires noisy images to be processed only once. We do so\nby training an additional deterministic convolutional neu-\nral network (CNN), termed Direct Denoiser, that directly\npredicts MMSE or MMAE solutions and is trained along-\nside the VAE. It uses noisy training images as input and\nthe sampled predictions from the VAE as training targets.\nLacking a probabilistic nature, this network will minimize\nits MSE or MAE loss function by predicting the mean or\npixel-wise median of the denoising distribution. The result\nis a denoising network with the evaluation times of a su-\npervised approach and the training data requirements of an\nunsupervised approach.\nIn summary, we propose an extension to unsupervised\ndeep learning-based denoisers that dramatically reduces in-\nference time by estimating a central tendency of the learned\ndenoising distribution in a single evaluation step. Moreover,\nwe show these estimates to be more accurate than those ob-\ntained by averaging even up to 1000 samples from the de-\nnoising distribution. Figure 1 shows how much shorter in-\nference time is with our proposed approach, and how much\nhigher the quality of results are.\nThe remainder of the paper is structured as follows. In\nSection 2, we give a brief overview of related work, concen-\ntrating on different approaches to denoising. In Section 3,\nwe provide a formal introduction to the unsupervised VAE-\nbased denoising approach, which is the foundation of our\nmethod. In Section 4, we describe the training of the Direct\nDenoiser. We evaluate our approach in Section 5, showing\nthat we consistently outperform our baseline at a fraction of\nthe computational cost. Finally, in Sections 6 and 7 we dis-\ncuss our results and give an outlook on the expected impact\nof our work and future perspectives.\n2. Related Work\n2.1. Supervised denoising\nTraditional supervised deep learning-based methods\n(e.g. [30, 28]) rely on paired training data consisting of\ncorresponding noisy and clean images.\nThese methods\nview denoising as a regression problem, and usually train\na UNet [23] or variants of the architecture to learn a map-\nping from noisy to clean. The most commonly used loss\nfunction for this purpose is the sum of pixel-wise quadratic\nerrors (L2 or MSE), which directs the network to predict the\nMMSE estimate for the noisy input.\nThe approach’s requirement for clean training images\ngreatly limits its applicability, particularly for scientific\nimaging applications, where often no clean data can be ob-\ntained. In 2018, Lehtinen et al. [16] had the insight that\ntraining of equivalent quality can be achieved by replacing\nthe clean training image with a second noisy image of the\nsame content; a training method termed Noise2Noise. In\npractice, such image pairs can often be acquired by record-\ning two images in quick succession. By using the L2 loss\nand assuming that the imaging noise is zero-centered, the\nnetwork is expected to minimize the loss to its noisy train-\ning target by converging to the same MMSE estimate as in\nsupervised training.\nWhile Noise2Noise and traditional supervised methods\nare state-of-the-art with respect to the quality of their re-\nsults, their requirement for paired training data makes them\ninapplicable in many situations. In contrast, our method re-\nquires only unpaired noisy data, which is available for any\ndenoising task, making it directly applicable in situations\nwhere supervised methods are not.\n2.2. Self-supervised denoising\nSelf-supervised methods have been introduced to enable\ndenoising with unpaired noisy data.\nHere we focus on\nblind-spot approaches (e.g. [12, 2, 17, 22]), which mask in-\ndividual pixels in the input image and use them as training\ntargets. These methods rely on the assumption that imag-\ning noise is pixel-wise independent given an underlying\nsignal. By effectively forcing the network to predict each\npixel value from its surroundings, blind-spot approaches\ncan learn to denoise images without the need for paired\nnoisy-clean data. Like supervised methods, self-supervised\ndenoisers (when used with L2 loss) predict an MMSE esti-\nmate for each pixel, albeit based on less information, since\nthe corresponding input pixel cannot be used during predic-\ntion. As a result, the quality of the output can be worse than\nsupervised methods. The blind-spot approach has been im-\nproved to reintroduce the lost pixel information during in-\nference [21, 15], achieving improved quality in some situa-\ntions. In [4], Broaddus et al. extended the method to allow\nfor the removal of structured noise.\nOur method also does not require paired data, but we do\nnot follow the self-supervised blind-spot paradigm. As a\nconsequence, we do not have to address the loss of pixel\ninformation.\n2.3. Unsupervised VAE-based denoising\nUnsupervised VAE-based denoising methods [20] form\nthe backbone of our method. Like in self-supervised meth-\nods, training requires only noisy images. However, their\ntraining and inference procedures differ greatly from self-\nsupervised approaches. We discuss this class of methods in\ndetail in Section 3.\n2.4. Knowledge distillation\nKnowledge distillation [9] is the process of training a\nsmaller student network using a large teacher network or\nan ensemble [5] of teachers. The goal of this approach is\nto reduce the computational effort required during inference\nand enable more efficient employment of a powerful model.\nSurprisingly, the student model can achieve better results\ncompared to being trained on the data directly. A survey of\nthe topic can be found in [7].\nThe approach of training our Direct Denoiser with the\noutput of another network can be seen as knowledge distil-\nlation. However, in our case the Direct Denoiser is not in-\ntended as a smaller replacement of the VAE, but as a model\nwith a faster inference procedure.\n3. Background\n3.1. The denoising task\nA noisy observation, x, of a signal, s, can be thought of\nas sampled from an observation likelihood, or noise model,\npNM(x|s). A noise model describes the random, unwanted\nvariation that is added to a signal when it is recorded. The\ngoal of denoising is to estimate the s that parameterized the\nnoise model from which a known x was sampled.\n3.2. Unsupervised denoising\nIt was Prakash et al. [20] who proposed doing so via\nvariational inference, using a VAE [11] to approximate the\nposterior distribution p(s|x). They improved their approach\nwith a more powerful architecture that could also handle\nmild forms of structured noise in [19]. Salmon and Krull\nthen presented an alternative approach to tackling structured\nnoise in [25], but it unfortunately cannot yet be applied in\nrealistic settings.\nTo understand how unsupervised denoising works, we\nmust give a brief explanation of the VAE [11]. For a full\nintroduction, see [6].\nGiven a tractable prior distribution pθ(z) and a like-\nlihood pθ(x|z), the marginal distribution pθ(x) could be\nlearnt by minimizing the objective\n−log pθ(x) = −log\nZ\nz\npθ(x|z)pθ(z)dz.\n(1)\nHowever, this integral is often intractable for high dimen-\nsional x. VAEs instead approximate pθ(x) by minimizing\nthe following upper bound,\n−log pθ(x) + DKL[qϕ(z|x) ∥pθ(z|x)]\n= Eqϕ(z|x)[−log pθ(x|z))] + DKL[qϕ(z|x)) ∥pθ(z)],\n(2)\nwhere θ and ϕ are learnable parameters and DKL is the\nalways positive Kullback-Leibler (KL) divergence [13].\nHere, an approximate posterior qϕ(z|x) is introduced and\noptimized to diverge as little as possible from the true pos-\nterior pθ(z|x).\nThe authors of DivNoising [20], Hierarchical DivNois-\ning (HDN) [19] and AutoNoise [25] adapt the VAE for de-\nnoising by incorporating a known explicit noise model into\nthis objective, directing the decoder of the VAE to map the\nlatent variable z to estimates of the signal s,\n−log pθ(x) + DKL[qϕ(z|x) ∥pθ(z|x)]\n= Eqϕ(z|x)[−log pNM(x|s))] + DKL[qϕ(z|x)) ∥pθ(z)],\n(3)\nwhere s = gθ(z).\n3.3. Inference in unsupervised denoising\nAfter minimizing this new denoising objective, the sig-\nnal underlying a given x is estimated by first encoding x\nwith qϕ(z|x), sampling a z and mapping that sample to an\nestimate of the signal with gθ(z). These solutions are sam-\nples from an approximation of the posterior p(s|x), which\nwe refer to as the denoising distribution.\nEach sample from the denoising distribution is unique,\nallowing users to examine the uncertainty involved in their\ndenoising problem. However, a single consensus solution\nis often preferred.\nThe authors of [20, 19, 25] chose to\ncalculate the per pixel mean of 100 or 1000 samples, de-\nriving the minimum mean square error (MMSE) estimate\nof the denoising distribution, to get a consensus solution for\nmeasuring denoising performance. Taking so many samples\nrequires many forward passes of the denoiser and incurs\na potentially prohibitive computational overhead for large\ndatasets.\nOur method extends the high quality denoising perfor-\nmance and minimal training requirements of VAE-based de-\nnoisers by allowing them to directly and efficiently produce\nMMAE and MMSE results without repeated sampling.\nDirect Denoiser\nnetwork\nEncoder\nnetwork\nDecoder\nnetwork\nNoise model\nL1/L2\nloss\nSampled\nclean\nimage\nNoisy\ntraining\nimage\nLatent distribution\nSample\nMMAE/\nMMSE\nestimate\nTraining gradient\nFigure 2.\nTraining scheme: We train our novel Direct Denoiser (blue) along side a Variational AutoEncoder (VAE) [20, 19]. The\nprocessing of data is shown with solid arrows and the backward propagation of gradients required for training is shown with dashed\narrows. The VAE encoder takes a noisy image as input and predicts the parameters of a distribution in latent space, a sample is drawn from\nhere and mapped to a possible clean image by the decoder network. The reconstruction loss is computed using a pre-trained noise model.\nOur Direct Denoiser is trained using noisy images as input and the clean image samples (predicted by the VAE) as target. Since individual\nsamples differ for the same input, there is no unique correct solution for this task. As a consequence, by using an L2 loss, the Direct\nDenoiser will learn to predict the expected value, i.e., the MMSE solution. Using an L1 loss leads to predicting the pixel-wise median. We\nblock gradients from passing through the sampled clean image to prevent the VAE changing its outputs.\n4. Method\nWhen given samples from a probability distribution, we\nare often interested in what a representative value of those\nsamples is. In the case of unsupervised denoising, we are\ninterested in a representative image from the denoising dis-\ntribution. A common value to choose for this is the central\ntendency of the distribution [29], a point which minimizes\nsome measure of deviation from all of the samples.\nFor samples from a learned denoising distribution,\np(ˆs|x), over possible solutions ˆs for a noisy input image\nx, this would be\nˆs∗= arg min\ny Eˆs|x[L(y,ˆs)],\n(4)\nwhere L is some per-pixel loss function. If L is the L1 loss,\nL(y,ˆs) = 1/n\nn\nX\ni\n|yi −ˆsi|,\n(5)\nthen ˆs∗corresponds to the pixel-wise median of the distribu-\ntion, i.e., the MMAE estimate. Here, n denotes the number\nof pixels and yi and ˆsi denote ith pixel values. For the L2\nloss,\nL(y,ˆs) = 1/n\nn\nX\ni\n(yi −ˆsi)2,\n(6)\nˆs∗will be the arithmetic mean, i.e., the MMSE.\nThe authors of [20, 19, 25] estimated ˆs∗using a large\nnumber of samples from their denoising distribution. We\npropose instead training a CNN to directly predict a central\ntendency.\nLet hη be our Direct Denoiser with parameters η and\np(ˆs|x) be a denoising distribution. The following objective,\narg min\nη\nEx[Eˆs|x[L(hη(x),ˆs)]],\n(7)\nwhere L is either the L1 or L2 loss, would train hη to pre-\ndict either the pixel-wise median or mean of p(ˆs|x), respec-\ntively. After training an unsupervised denoiser according to\n[20, 19, 25], we could train our Direct Denoiser with Eq. 7\nby sampling noisy images x from a training set and then\nrunning them through the unsupervised denoiser to obtain\npossible clean solutions ˆs from the denoising distribution.\nWe however find that it is possible to train both mod-\nels simultaneously. Let fθ,ϕ represent a VAE with the loss\nfunction in Equation 3, where ˆs ∼fθ,ϕ(x) is a sample from\nthe denoising distribution.\nA single training step for simultaneously optimizing an\nunsupervised denoiser and an accompanying Direct De-\nnoiser is as follows:\n1. Pass a noisy training image x to the unsupervised de-\nnoiser and sample a possible solution ˆs.\n2. Update the parameters (θ, ϕ) towards minimizing the\nloss function in Equation 3.\n3. Pass the same x to the Direct Denoiser, calculating\nhη(x).\n4. Update the parameters η to minimize L(hη(x),ˆs),\nwhere L is the L1 or L2 loss function.\n5. Repeat until convergence.\nA visual representation of this training scheme can be\nfound in Figure 2.\n5. Experiments\nOur Direct Denoiser was trained alongside HDN [19],\nusing six datasets of intrinsically noisy microscopy images\nthat come with known ground truth signal. Each dataset\ncan be found in [19], as can details of their size, spatial\nresolution and train, validation and test splits. Note that\nfor the Struct. Convallaria dataset, we adapted HDN into\nHDN3-6, making it capable of handling structured noise.\nDenoising Performance To evaluate denoising perfor-\nmance, we compare the Peak Signal-to-Noise Ratio (PSNR)\nof our Direct Denoiser’s direct solutions to the PSNR of\nHDN’s consensus solutions. The consensus solutions were\nproduced by averaging samples of size 1, 10, 100 and 1000,\nreporting both their per-pixel median and mean. The Direct\nDenoiser’s solutions were reported from a network trained\nwith an L1 loss and a network trained with an L2 loss. Re-\nsults are in Table 1. Visual results from the same experiment\ncan be seen in Figure 3.\nInference Times We also compared inference time to de-\nnoising performance. In Figure 1, the total time for HDN to\ngenerate 1, 10, 100 and 1000 samples for all 100 images in\nthe Convallaria test set was measured, then plotted against\nthe PSNR of the mean of those samples, averaged over all\n100 images. On the same plot, the total time for our Direct\nDenoiser to produce single solutions for each image is plot-\nted against their average PSNR. Each test image consisted\nof 512×512 pixels.\nUsing our GPU (an NVIDIA GeForce RTX 3090 Ti),\ngenerating a single 512×512 solution from HDN’s denois-\ning distribution takes 0.076 seconds, using 2207MB of the\nGPU’s memory. Our Direct Denoiser takes 0.029 seconds\nat 1909MB to do the same. Processing one image with ei-\nther model uses the full capacity of the GPU’s parallelism,\nso we saw no speed improvements by processing more than\none image at a time.\nIf a consensus solution from HDN with PSNR approach-\ning that of the the Direct Denoiser requires sampling 1000\nsolutions, inference with the proposed method is 2621×\nfaster.\nTraining Times and Memory Usage Finally, the additional\ntraining time incurred by co-training HDN with the Direct\nDenoiser was examined. The authors of HDN [19] train\ntheir network for 200,000 steps for all datasets, using a\nbatch size of 64 and image patch size of 64×64. Using our\nGPU, training HDN alone takes 0.27 seconds per step for\n15 hours total, using 13GB of GPU memory. Training both\nHDN and the Direct Denoiser takes 0.34 seconds per step\nfor 18.9 hours total, using 15GB of GPU memory. Note\nthat smaller virtual batches can be used as in [19] to reduce\nmemory consumption. For the proposed method to be a net\ntime saving, inference would have to take 3.9 hours less.\nUsing our hardware and inference image resolution, time\nis saved when the inference test set consists of 185 images\nwith 512 × 512 resolution.\nNetwork Architecture and Training The Direct Denoiser\nused in these experiments was a UNet [23] with approxi-\nmately 12 million parameters, while the unsupervised de-\nnoiser was the same Hierarchical VAE [26] used in [19]\nwith approximately 7 million parameters. We chose to give\nour UNet more parameters than the Hierarchical VAE to\nensure the former had the capacity to learn the full relation-\nship between noisy images and solutions generated by the\nlatter. This may not have been necessary, and training a Di-\nrect Denoiser with a lower computational demand would be\nan interesting topic for future research.\nOur UNet had a depth of four, with a residual block [8]\nconsisting of two convolutions followed by a ReLU ac-\ntivation function [1] at each level.\nDownsampling was\nperformed by convolutions with a stride of two, and up-\nsampling by nearest neighbor interpolation [24] followed\nby a single convolution with stride one. All convolutions\nhad a kernel size of 3. The number of filters was 32 at\nthe first level and that number doubled at each subsequent\nlevel. Skip connections were merged by concatenating the\nskipped features with the features from the previous level\nand passing the two through a residual block.\nTraining followed the same procedure described in [19],\nwith the only difference being that our Direct Denoiser had\nits own Adamax optimizer [10] with an initial learning rate\nof 3e-4 that reduced by a factor of 0.5 when validation loss\nhad plateaued for 10 epochs.\n6. Discussion\nSolutions from our Direct Denoiser consistently scored\na higher PSNR than consensus solutions of 1000 samples\nfrom HDN. Table 1 shows HDN’s PSNRs converging to-\nwards our direct prediction result with increased sample\nsize. It seems that solutions from our Direct Denoiser are\nsometimes equivalent to averaging sample sizes orders of\nmagnitude larger than the largest samples size we used in\nour experiment. Moreover, by looking at the inference times\nreported in Figure 1, the time required to take such a sample\nsize would be impractical for large datasets.\n7. Conclusions\nWe have demonstrated that an extension of the unsu-\npervised denoising approach–the Direct Denoiser–can be\nused to dramatically speed up inference time, while at the\nTable 1. Average PSNR of consensus solutions from HDN [19] and direct solutions from our novel Direct Denoiser. HDN’s consensus\nsolutions were obtained by taking samples of varying sizes from its denoising distribution and calculating both their per-pixel median and\ntheir per-pixel mean. The Direct Denoiser’s solutions were obtained from a single pass of a network trained under an L1 loss and a single\npass of a network trained under an L2 loss. PSNRs are presented as the median/mean consensus for HDN and as the solution from the\nL1/L2 network for the Direct Denoiser. Best results are printed in bold.\nNumber of samples (HDN)\nDataset\n1\n10\n100\n1000\nDirect\nConvallaria\n33.69 / 33.69\n36.59 / 36.76\n37.17 / 37.23\n37.19 / 37.27\n37.50 / 37.45\nConfocal Mice\n35.43 / 35.43\n37.30 / 37.42\n37.58 / 37.68\n37.62 / 37.69\n37.77 / 37.75\n2 Photon Mice\n31.21 / 31.21\n32.63 / 32.68\n32.86 / 32.87\n32.89 / 32.89\n33.55 / 33.54\nMouse Actin\n31.62 / 31.62\n33.52 / 33.66\n33.87 / 33.92\n33.91 / 33.95\n34.22 / 34.28\nMouse Nuclei\n33.48 / 33.48\n36.24 / 36.44\n36.79 / 36.89\n36.81 / 36.90\n36.87 / 36.93\nStruct. Convallaria\n29.02 / 29.02\n30.88 / 31.00\n31.22 / 31.27\n31.27 / 31.29\n31.58 / 31.64\nsame time improving performance when compared the stan-\ndard inference procedure with up to 1000 sampled images.\nWe believe our approach will become the default way of\nproducing central tendencies from unsupervised denoising\nmodels with the increase in speed potentially allowing an\neasy adaptation by the community.\nWhile we have evaluated our method only for MSE and\nMAE loss functions, we believe the approach could also\nbe used with other loss functions such as Tukey’s biweight\nloss [3], which might allow us to find regions of high prob-\nability density or even the maximum a posteriori estimate.\nRecent work in image restoration has suggested the\nuse of more sophisticated perceptual loss functions (see\ne.g. [18]). These types of loss functions would likely only\nbe usable in a supervised setting with clean training data\nand would be unlikely to work with Noise2Noise or self-\nsupervised methods.\nHowever, since the training targets\nsampled by our VAE are essentially clean images, they\nshould be compatible with different types of complex loss\nfunctions, opening the door to using perceptual loss with\nnoisy unpaired data.\nReferences\n[1] Abien Fred Agarap. Deep learning using rectified linear units\n(relu). arXiv preprint arXiv:1803.08375, 2018.\n[2] Joshua Batson and Loic Royer. Noise2self: Blind denoising\nby self-supervision, 2019.\n[3] Vasileios\nBelagiannis,\nChristian\nRupprecht,\nGustavo\nCarneiro, and Nassir Navab.\nRobust optimization for\ndeep regression. In Proceedings of the IEEE international\nconference on computer vision, pages 2830–2838, 2015.\n[4] C. Broaddus, A. Krull, M. Weigert, U. Schmidt, and G. My-\ners. Removing structured noise with self-supervised blind-\nspot networks. In 2020 IEEE 17th International Symposium\non Biomedical Imaging (ISBI), pages 159–163, 2020.\n[5] Thomas G Dietterich. Ensemble methods in machine learn-\ning. In International workshop on multiple classifier systems,\npages 1–15. Springer, 2000.\n[6] Carl Doersch. Tutorial on variational autoencoders. arXiv\npreprint arXiv:1606.05908, 2016.\n[7] Jianping Gou, Baosheng Yu, Stephen J Maybank, and\nDacheng Tao. Knowledge distillation: A survey. Interna-\ntional Journal of Computer Vision, 129:1789–1819, 2021.\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[9] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the\nknowledge in a neural network. stat, 1050:9, 2015.\n[10] Diederik P Kingma and Jimmy Ba.\nAdam: A method\nfor stochastic optimization. 3rd international conference\non learning representations, iclr 2015.\narXiv preprint\narXiv:1412.6980, 9, 2015.\n[11] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. stat, 1050:1, 2014.\n[12] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.\nNoise2void-learning denoising from single noisy images. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2129–2137, 2019.\n[13] Solomon Kullback and Richard A Leibler.\nOn informa-\ntion and sufficiency. The annals of mathematical statistics,\n22(1):79–86, 1951.\n[14] Romain F Laine, Guillaume Jacquemet, and Alexander\nKrull. Imaging in focus: an introduction to denoising bioim-\nages in the era of deep learning. The International Journal\nof Biochemistry & Cell Biology, 140:106077, 2021.\n[15] Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila.\nHigh-quality self-supervised deep image denoising. In Ad-\nvances in Neural Information Processing Systems, pages\n6968–6978, 2019.\n[16] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli\nLaine,\nTero Karras,\nMiika Aittala,\nand Timo Aila.\nNoise2noise: Learning image restoration without clean data.\nIn International Conference on Machine Learning, pages\n2965–2974, 2018.\n[17] Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady.\nNoisier2noise: Learning to denoise from unpaired noisy\ndata. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 12064–12072,\n2020.\n[18] Aamir Mustafa, Aliaksei Mikhailiuk, Dan Andrei Iliescu,\nVarun Babbar, and Rafał K Mantiuk. Training a task-specific\nimage reconstruction loss. In Proceedings of the IEEE/CVF\nConvallaria\nNoisy\n1 Sample\n10 Samples\n100 Samples\n1000 Samples\nDirect\nGround truth\nConfocal Mice\n2 Photon Mice\nMouse Actin\nMouse Nuclei\nStruct. Convallaria\nFigure 3. Visual results: Cropped images from each dataset showing consensus solutions of varying sample sizes from HDN’s denoising\ndistribution with direct solutions from our Direct Denoiser. For each dataset, the top row shows the median of HDN samples and a solution\nfrom our L1 trained Direct Denoiser, while the bottom row shows the mean of HDN samples and a solution from our L2 trained Direct\nDenoiser.\nwinter conference on applications of computer vision, pages\n2319–2328, 2022.\n[19] Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, and\nFlorian Jug. Interpretable unsupervised diversity denoising\nand artefact removal. In International Conference on Learn-\ning Representations, 2022.\n[20] Mangal Prakash, Alexander Krull, and Florian Jug. Fully un-\nsupervised diversity denoising with convolutional variational\nautoencoders. In International Conference on Learning Rep-\nresentations, 2020.\n[21] Mangal Prakash, Manan Lalit, Pavel Tomancak, Alexander\nKrull, and Florian Jug.\nFully unsupervised probabilistic\nnoise2void. arXiv preprint arXiv:1911.12291, 2019.\n[22] Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji.\nSelf2self with dropout: Learning self-supervised denoising\nfrom single image. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n1890–1898, 2020.\n[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, pages 234–241.\nSpringer, 2015.\n[24] Olivier Rukundo and Hanqiang Cao. Nearest neighbor value\ninterpolation. arXiv preprint arXiv:1211.1768, 2012.\n[25] Benjamin Salmon and Alexander Krull. Towards structured\nnoise models for unsupervised denoising. In European Con-\nference on Computer Vision, pages 379–394. Springer, 2022.\n[26] Casper Kaae Sønderby,\nTapani Raiko,\nLars Maaløe,\nSøren Kaae Sønderby, and Ole Winther. Ladder variational\nautoencoders.\nAdvances in neural information processing\nsystems, 29, 2016.\n[27] Jingwen Su, Boyan Xu, and Hujun Yin. A survey of deep\nlearning approaches to image restoration. Neurocomputing,\n487:46–65, 2022.\n[28] Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas\nM¨uller, Alexandr Dibrov, Akanksha Jain, Benjamin Wil-\nhelm, Deborah Schmidt, Coleman Broaddus, Siˆan Culley,\net al. Content-aware image restoration: pushing the limits\nof fluorescence microscopy. Nature methods, 15(12):1090–\n1097, 2018.\n[29] Herbert Weisberg. Central tendency and variability. Num-\nber 83. Sage, 1992.\n[30] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang. Be-\nyond a gaussian denoiser: Residual learning of deep cnn for\nimage denoising. IEEE Transactions on Image Processing,\n26(7):3142–3155, 2017.\n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2023-10-27",
  "updated": "2023-12-04"
}