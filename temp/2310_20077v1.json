{
  "id": "http://arxiv.org/abs/2310.20077v1",
  "title": "Partial Tensorized Transformers for Natural Language Processing",
  "authors": [
    "Subhadra Vadlamannati",
    "Ryan Solgi"
  ],
  "abstract": "The transformer architecture has revolutionized Natural Language Processing\n(NLP) and other machine-learning tasks, due to its unprecedented accuracy.\nHowever, their extensive memory and parameter requirements often hinder their\npractical applications. In this work, we study the effect of tensor-train\ndecomposition to improve the accuracy and compress transformer vision-language\nneural networks, namely BERT and ViT. We focus both on embedding-layer\ncompression and partial tensorization of neural networks (PTNN) through an\nalgorithmic approach. Our novel PTNN approach significantly improves the\naccuracy of existing models by up to 5%, all without the need for post-training\nadjustments, breaking new ground in the field of tensor decomposition.",
  "text": "Partial Tensorized Transformers for Natural Language Processing\nSubhadra Vadlamannati1, Ryan Solgi1\n1Mercer Island High School, 9100 SE 42nd St, Mercer Island, USA\n2Department of Electrical and Computer Engineering, University of California Santa Barbara, Santa Barbara, USA\n{subhadra, solgi}@ucsb.edu\nKeywords:\nNeural Networks, Machine Learning, Natural Language Processing, ALIGN, Tensor-Train Decomposition,\nVision-Language Modelling\nAbstract:\nThe transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-\nlearning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter require-\nments often hinder their practical applications. In this work, we study the effect of tensor-train decomposition\nto improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT.\nWe focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through\nan algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models\nby up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor\ndecomposition.\n1\nINTRODUCTION\nThe transformer architecture has been extremely in-\nfluential in the field of Natural Language Process-\ning (NLP) by achieving state-of-the-art performance\nin multiple downstream tasks (Brown et al., 2020).\nSpecifically, transformer-based models have achieved\nstate-of-the-art performance in machine translation,\nsentence representation, and language generation\n(Brown et al., 2020). However, due to the multi-head\nself-attention layers present in such architectures and\nthe sheer number of parameters present in each layer\n(upwards of a million parameters during each forward\npass of the model), transformers are computationally\ndemanding in terms of both memory and computation\ntime (Gu et al., 2022).\nEffective network compression for convolutional\nneural networks with fully-connected networks is\nstill an open research question.\nTherefore, com-\npressing Transformer networks is essential for their\nuse on edge devices and in low-resource environ-\nments. Tensor Decomposition works by represent-\ning higher-order tensors as a series of low-rank fac-\ntors(Oseledets, 2011).\nTensor Decomposition (TD) works by represent-\ning higher-order tensors as a series of simpler, lower-\norder tensors via elementary operations (Oseledets,\n2011). There are a variety of methods proposed to\nexpress tensors in a low-rank format factored form,\nincluding CANDECOMP PARAFAC (CP), tucker,\nand tensor-train decomposition (Bacciu and Mandic,\n2020).\nSeveral studies have explored the application of\ntensor decomposition methods to reduce the size of\nneural networks and improve their efficiency. For ex-\nample, CP decomposition has been utilized to factor-\nize weight matrices as the sum of rank-1 tensors, re-\nducing the model’s memory requirements and speed-\ning up inference time 8x (Maldonado-Chan et al.,\n2021).\nTechniques like tensorized training can be applied\nto neural networks to achieve both faster model in-\nference and also a large reduction in the number of\nparameters.\nMore recently, tensorized training ap-\nproaches have explored the possibility of updating the\nfactored tensors of the weights of deep neural net-\nworks during the forward pass instead of the model\nweights directly (Bacciu and Mandic, 2020).\nIn the field of natural language processing, while\ntensor decomposition methods have been extensively\nstudied for text-based neural networks, their appli-\ncation to multimodal neural networks has received\nlimited attention. Multimodal neural networks such\nas ALIGN (Jia et al., 2021b) and VisualBERT (Li\net al., 2019), have gained significant attention in re-\ncent years due to their remarkable ability to process\nboth text and image data. These dual-encoder archi-\ntectures require a larger number of parameters com-\narXiv:2310.20077v1  [cs.CL]  30 Oct 2023\npared to traditional text-based models due to their\nneed to capture information from many modalities.\nWhile there has been existing research on apply-\ning traditional tensor decomposition methods to re-\nduce the size of neural networks, their application on\nmultimodal neural networks has been severely lim-\nited (Maldonado-Chan et al., 2021). Existing methods\nhave demonstrated that tensor decomposition meth-\nods can effectively reduce memory time and increase\nefficiency on neural networks while maintaining min-\nimal accuracy loss (Zhong 2019; Bacciu and Mandic\n2020).\nIn this paper,\nwe focus on vision-language\ntransformer-based models as prime candidates for\nmodels that require compression, as these models em-\nploy a dual-encoder architecture that requires double\nthe amount of parameters traditional text-based mod-\nels use. These models must encode both text and im-\nage data and then generate a latent space that can be\nemployed on a variety of downstream tasks includ-\ning visual question-answering, visual common-sense\nreasoning, natural language for visual reasoning, and\nregion-to-phrase grounding.\nVision-language tasks\nhave a wide range of practical applications, including\nclosed-captioning for the hard-of-hearing and screen\nreading capabilities (Morris et al., 2018).\nThis paper aims to advance the field of multi-\nmodal language processing by applying state-of-the-\nart tensor decomposition techniques to compress pop-\nular networks such as ViT and BERT while maintain-\ning model accuracy. We additionally study the effect\nof tensor size and hyperparameter values during ten-\nsorized training.\n2\nMETHODS\n2.1\nData\nWe use the ImageNet dataset to train ViT. ImageNet is\na collection of 80 million tiny images for image clas-\nsification. Instead of using the entire dataset, we use\nthe standard 1000 image test set of ImageNet known\nas CIFAR10 (Krizhevsky, 2009) to evaluate the com-\npressed model.\n2.2\nTensor-Train Decomposition\nWe use tensor-train (TT) decomposition throughout\nthis paper to compress weight tensors. In the tensor\ntrain (TT) format (Oseledets, 2011), a d-way tensor\nW ∈Rn1×....×nd is approximated with a set of d cores\n¯G = {G1,G2,...,Gd} where G j ∈Rr j−1×nj×rj , rj’s\nfor j = 1,...,d −1 are the ranks, r0 = rd = 1, and each\nInput: d-way tensor Y , error bound ε.\nOutput: ¯G = {G1,G2,...,Gd}\nσ =\nε\nd−1∥Y ∥F\nr0 = 1, rd = 1, W = reshape(Y ,(n1, |Y |\nn1 ))\nfor j = 1 to j = d −1 do\nW = reshape(W,(rj−1n j,\n|W|\nr j−1n j ))\nCompute σ-truncated SVD:\nW = USVT +E, where ∥E∥F ≤σ\nr j = the rank of matrix W based on\nσ-truncated SVD\nG j = reshape(U,(rj−1,nj,rj))\nW = SVT\nend\nGd = reshape(W,(rd−1,nd,rd))\nReturn ¯G = {G1,G2,...,Gd}\nAlgorithm 1: TT-SVD\nelement of Y is approximated by the following equa-\ntion:\nˆW [i1,...,id] = ∑\nl0,...,ld\nG1[l0,i1,l1]...Gd[ld−1,id,ld].\nWith a prescribed error tolerance (ε), the fun-\ndamental components, denoted as Gj, are obtained\nthrough a sequence of (d −1) consecutive Singular\nValue Decompositions (SVDs) performed on auxil-\niary matrices derived from the unfolding of the tensor\nY along various axes. This decomposition method,\nreferred to as TT-SVD, is detailed in Algorithm 1.\n2.3\nModels and Pretraining\n2.3.1\nModels\nWe perform an analysis on the effects of various mod-\nels to encode both visual and textual data. For the text\nencoder, we focus on BERT (Liu et al., 2019) which\napplies the bi-directional training of the Transformer\nto create in-context word embeddings. To test BERT\non image classification, we pair it with EfficientNet\nfor the visual encoding, which is consistent with the\nimplementation in the dual-encoder network ALIGN\n(Jia et al., 2021a). BERT has 12 layers, a hidden layer\nsize of 768, and around 120 million parameters.\nFor the visual encoder we focus on ViT (Dosovit-\nskiy et al., 2021). ViT is an image processor based on\nthe Transformer architecture.\n2.3.2\nPre-Compression Training\nWe fine-tune the image encoder ViT as it’s trained on\na much larger, more general dataset ImageNet. Ad-\nditionally, this fine-tuning improves baseline model\nFigure 1: Bimodal Space-saving distribution for the two models.\naccuracy which ensures that our post-training adjust-\nments truly improve model accuracy. Specifically, we\nfine-tune ViT on the CIFAR10 train set with 50,000\nimages and hyperparameters as outlined in Section\n4.1.\nPost fine-tuning, we achieve an accuracy of\n97.89%, a validation loss of 0.2564 and a training loss\nof 0.4291.\n2.4\nEmbedding Layer Tensorization\nDue to the relatively large size of the embedding\nlayer, comprising around 28% of BERT’s total param-\neters, and around 40% of ViT’s total parameters, we\nfirst explore the effects of embedding layer tensoriza-\ntion. Following the aforementioned approach in 2.2,\nwe compress the embedding layers of the model and\nstudy the effects on model accuracy.\n2.5\nPartially Tensorized Neural\nNetwork (PTNN)\nAs the compression of the embedding layer caused\nthe accuracy of the network to increase, as men-\ntioned in 3.0.1, we continue exploring this accuracy\nimprovement for the rest of the model using our\nmethod in 2.4. In this case, we do not perform post-\ntraining adjustments (like retraining). Initially, we ex-\ntract model weights from all relevant encoders. We\ncompress every layer by initially transforming the\nweight’s 2D matrix into a tensor of higher dimension\nwith the same volume as the original matrix. Tensor-\ntrain decomposition is then used to decompose model\nweights with a given error bound.\nLastly, we re-\nshape the tensor back into the original dimensions of\nthe 2D weight matrix. To perform this iteratively as\ndescribed in Algorithm 2, we re-initialize the model\nwith the decomposed weights and perform the same\nprocess to compress the remaining model weights.\nwhile not at the end of the model do\ncompress layers 0 to n using TT decomp;\nif model accuracy ≥5% of the original\naccuracy then\nproceed to layer n+1;\nelse\nleave layer n uncompressed;\nmove to layer n+1;\nend\nend\nAlgorithm 2: Iterative model tensorization.\n2.6\nDecomposition Evaluation\nWe evaluate the effectiveness of the decomposition\nusing commonly known metrics like compression ra-\ntio and space saving, as described in Equation (1).\nWe additionally evaluate the memory saved and\ntime complexity of our decomposed models.\nFor\nmemory reduction, we simply multiply the percent-\nage space saving with the total number of parameters\nin the non-decomposed matrix. Then, we divide this\nover the total number of parameters in the model.\nFor time complexity, we use a standard approach\nto find the O(n) based on the rank of the Tensor-\nTrain decomposed matrices as outlined in (Oseledets,\n2011).\nFigure 2: BERT and ViT model iterative compression with exclusion of low-accuracy layers. Lighter color corresponding\ndashed lines indicate baseline accuracy.\nModel\nTrain\nTest\nSpace-saving\nDecomposed ViT\n0.986\n0.966\n0.267\nBase ViT\n0.960\n0.960\nNA\nTensorized BERT (ALIGN)\n0.767\n0.833\n0.269\nBase BERT (ALIGN)\n0.750\n0.746\nNA\nTable 1: Embedding Layer Decomposition Results\n3\nRESULTS\nWe evaluate both models on both the train (50,000)\nand test (10,000) image split of the CIFAR10 dataset.\nThe ViT model has 98 weight tensors while BERT\ncontains 99 weight tensors. All layers are compressed\nwith an epsilon of 0.5 and 3D projections are con-\nstructed with lowest norms.\n3.0.1\nEmbedding Layer Decomposition\nThe results of embedding layer decomposition are\npresented in Table 1. Embedding layer decomposi-\ntion results in an accuracy increase of around 2% and\nstill provides us with a noticeable parameter reduc-\ntion. Even though we are not able to achieve high\nspace-saving on these layers (i.e. only around 26%),\nthe success of this approach in maintaining or even\nimproving accuracy leads us to apply this technique\nto compress more layers of the model.\nAs seen in 3.0.1, decomposing the embedding\nlayer of BERT provides an accuracy gain over base-\nline comparisons. Following these promising results,\nwe follow the methodology in Algorithm 2 to deter-\nmine the amount of BERT that can be compressed\nwhilst still maintaining overall accuracy.\n3.0.2\nIterative and Individual Decomposition\nWe first individually compressed ViT’s weight ten-\nsors, but did not saw decreases in model accuracy af-\nter the embedding layer (results found in 4). There-\nfore, individual decomposition was not tested exten-\nsively on BERT due to computational limitations. In-\nstead, we rather focused on the synergistic effects be-\ntween layers of the model as mentioned in 2.5.\nAfter compressing both models utilizing the pro-\ncess described in Algorithm 2, the results are seen in\n2. In general, we see accuracy improvements even\nwhile compressing around half of the model’s pa-\nrameters (53% in BERT and around 49% in ViT).\nThis is achieved all without post-training adjustments.\nMost notably, more layers in BERT tend to have an\nimproved accuracy, however in ViT, compression of\nmany layers does not change the baseline accuracy,\nwhile still resulting in a parameter reduction.\nTT-decomposition can be an effective way to\nimprove the accuracy of transformer-based models.\nWhile some layers vastly decrease the accuracy of the\nmodel, we can still achieve a high compression even\nwhen not including such layers in the overall decom-\nposition.\n4\nCONCLUSIONS\nThis paper introduces a novel approach to compress-\ning transformer-based vision-language neural net-\nworks using TensorTrain decomposition. By applying\nthis method to popular models like ViT and BERT, we\nachieved significant improvements in accuracy, up to\n5%, without the need for post-training adjustments.\nThe iterative compression of model layers, coupled\nwith retraining, enables us to preserve model accu-\nracy while reducing up to 53% of the model’s pa-\nrameters.\nIn the future, we would like to general-\nize our approach to other dual encoder models and\ntest our approach on other multimodal tasks like vi-\nsual question answering and caption generation. This\nwork represents a valuable advancement in the field\nof multimodal language processing and contributes to\nour broader goal of making transformer-based mod-\nels more efficient and practical for real-world appli-\ncations.\nACKNOWLEDGEMENTS\nThe first author would like to extend her gratitude to\nDr. Lina Kim and the Research Mentorship Program\nCohort for their support throughout the research pro-\ncess.\nREFERENCES\nBacciu, D. and Mandic, D. P. (2020). Tensor decomposi-\ntions in deep learning. Computational Intelligence.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger,\nG., Henighan, T., Child, R., Ramesh, A., Ziegler,\nD. M., Wu, J., Winter, C., and Amodei, D. (2020).\nLanguage models are few-shot learners. arXiv.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,\nN. (2021). An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv.\nGu, J., Keller, B., Kossaifi, J., Anandkumar, A., Khailany,\nB., and Pan, D. Z. (2022). Heat: Hardware-efficient\nautomatic tensor decomposition for transformer com-\npression. arXiv.\nJia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham,\nH., Le, Q. V., Sung, Y., Li, Z., and Duerig, T.\n(2021a). Scaling up visual and vision-language repre-\nsentation learning with noisy text supervision. CoRR,\nabs/2102.05918.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham,\nH., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. (2021b).\nScaling up visual and vision-language representation\nlearning with noisy text supervision. arXiv.\nKrizhevsky, A. (2009). Learning multiple layers of features\nfrom tiny images.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. (2019). Visualbert: A simple and performant\nbaseline for vision and language. arXiv.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,\nV. (2019). Roberta: A robustly optimized bert pre-\ntraining approach. arXiv.\nMaldonado-Chan,\nM.,\nMendez-Vazquez,\nA.,\nand\nGuardado-Medina,\nR. O. (2021).\nMultimodal\ntucker\ndecomposition\nfor\ngated\nrbm\ninference.\nApplied Sciences, 11(16):16.\nMorris, M. R., Johnson, J., Bennett, C. L., and Cutrell,\nE. (2018). Rich representations of visual content for\nscreen reader users. In Proceedings of the 2018 CHI\nConference on Human Factors in Computing Systems,\npages 1–11.\nOseledets, I. V. (2011). Tensor-train decomposition. SIAM\nJournal on Scientific Computing, 33(5):2295–2317.\nAPPENDIX\n4.1\nViT Fine-tuning Hyperparameters\nLearning Rate: 5\nTrain/Evaluation Batch Size: 32\nOptimizer: Adam, linear learning rate\nEpoch(s): 1\n4.2\nIndividual Compression\nFigure 3: ViT Individual Compression\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2023-10-30",
  "updated": "2023-10-30"
}