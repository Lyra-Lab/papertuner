{
  "id": "http://arxiv.org/abs/2311.00587v2",
  "title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
  "authors": [
    "Xiaoqian Li",
    "Ercong Nie",
    "Sheng Liang"
  ],
  "abstract": "The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.",
  "text": "Crosslingual Retrieval Augmented In-context Learning for Bangla\nXiaoqian Li1\nErcong Nie1,2\nSheng Liang†1,2\n1Center for Information and Language Processing (CIS), LMU Munich, Germany\n2 Munich Center for Machine Learning (MCML), Germany\nXiaoqian.Li@campus.lmu.de\n{nie, shengliang}@cis.lmu.de\nAbstract\nThe promise of Large Language Models\n(LLMs) in Natural Language Processing has\noften been overshadowed by their limited per-\nformance in low-resource languages such as\nBangla. To address this, our paper presents\na pioneering approach that utilizes cross-\nlingual retrieval augmented in-context learn-\ning. By strategically sourcing semantically sim-\nilar prompts from high-resource language, we\nenable multilingual pretrained language mod-\nels (MPLMs), especially the generative model\nBLOOMZ, to successfully boost performance\non Bangla tasks. Our extensive evaluation high-\nlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs\nover the zero-shot performance.\n1\nIntroduction\nIn recent years, the field of Natural Language Pro-\ncessing (NLP) has witnessed transformative ad-\nvancements, especially with the advent of deep\ntransformer techniques (Vaswani et al., 2017; De-\nvlin et al., 2019; Radford et al., 2019). The in-\ntroduction of Large Language Models (LLMs),\nsuch as GPT-3 (Brown et al., 2020b) and GPT-\n4 (OpenAI, 2023), has further revolutionized the\nlandscape. These models showcase unparalleled\nprowess in tasks like text classification and genera-\ntion, unified under the umbrella of in-context learn-\ning, and cater to a plethora of applications across di-\nverse languages (Conneau et al., 2020; Raffel et al.,\n2020; Radford et al., 2019). While comprehen-\nsive benchmarks like XTREME (Hu et al., 2020)\nand BUFFET (Asai et al., 2023) underscore their\ncapabilities, languages such as English remain the\nprimary beneficiaries. In stark contrast, several low-\nresource languages, Bangla being a prime example,\ngrapple with challenges, notably the scarcity of\npretraining corpora (Artetxe and Schwenk, 2019;\nHangya et al., 2022; Sazzed, 2020).\n† Corresponding author.\nFigure 1: PARC pipeline using decoder-only Multilin-\ngual Pretrained Language Models.\nDespite having a significant number of native\nspeakers, Bangla remains underrepresented in the\nNLP arena due to linguistic intricacies, limited la-\nbeled datasets, and prevalent issues like data dupli-\ncation (Das and Bandyopadhyay, 2010; Das and\nGambäck, 2014). Although there have been com-\nmendable strides using conventional machine learn-\ning techniques in Bangla NLP tasks, the untapped\npotential of the latest LLMs is evident (Bhowmick\nand Jana, 2021; Wahid et al., 2019; Hoq et al.,\n2021).\nIn the evolving landscape of in-context learn-\ning with LLMs, the concept of retrieval augmen-\ntation, which emphasizes sourcing semantically\nrich prompts, has gained traction (Shi et al., 2023).\nHowever, when it comes to multilingual in-context\nlearning, previous works like MEGA (Ahuja et al.,\n2023) often limit their scope to task instructions\nand lack deeper semantic insights due to their ap-\nproach of random prompt selection. In contrast,\nstrategies like PARC (Nie et al., 2023) pave the way\narXiv:2311.00587v2  [cs.CL]  2 Dec 2023\nfor a more comprehensive methodology, fetching\nsemantically aligned prompts from high-resource\nlanguages.\nOur work draws inspiration from these method-\nologies but introduces novel perspectives. While\nMEGA offers task-level instructions, we infuse\nsemantic understanding into our approach. Sim-\nilar to PARC, our approach is cross-lingual, en-\nsuring a broader application spectrum. Diverg-\ning from PARC’s focus on masked language mod-\nels like mBERT and XLMR, as shown in Fig-\nure 1, we venture into uncharted territories by\nemploying larger, decoder-only multilingual pre-\ntrained language models (MPLMs) — BLOOM\nand BLOOMZ — to tackle Bangla NLP tasks in\na generative style (Muennighoff et al., 2023; Scao\net al., 2022).\nIn this paper, we explore the application of cross-\nlingual retrieval augmented in-context learning to\nBangla text classification and summarization tasks.\nOur main contributions encompass:\n• An extensive evaluation of cross-language re-\ntrieval augmented in-context learning meth-\nods in Bangla, achieving steady improvements\nover the zero-shot performance of MPLMs.\n• A pioneering exploration to extend PARC\nto the generative models, BLOOM and\nBLOOMZ, providing insights for a unified\npipeline of cross-lingual retrieval augmented\nin-context learning.\n2\nRelated Work\nBangla Natural Language Processing\nBangla\nis a morphologically rich language with various di-\nalects that belongs to the Indo-Aryan branch of the\nIndo-European language family. With roughly 270\nmillion speakers concentrating in Bangladesh and\nsome regions of India, Bangla is ranked as the 7th\nmost widely spoken language in the world1. How-\never, Bangla is still considered as a low-resource\nlanguage in the NLP research due to the scarcity of\ndigital text resources and annotated corpora.\nResearch on Bangla NLP has covered a variety\nof common NLP subfields since 1990s, such as\nPOS tagging (Dandapat et al., 2004; Ekbal and\nBandyopadhyay, 2008b), stemming and lemmati-\nzation (Islam et al., 2007; Paik and Parui, 2008),\n1https://www.ethnologue.com/insights/\nethnologue200/\nnamed entity recognition (Ekbal and Bandyopad-\nhyay, 2007, 2008a), sentiment analysis (Das and\nBandyopadhyay, 2010; Wahid et al., 2019), news\ncategorization (Mansur, 2006; Mandal and Sen,\n2014), etc. However, the research in different ar-\neas of Bangla NLP still remains sparse. In the era\nof deep learning, further progress has been made\nin Bangla NLP, particularly in terms of the devel-\nopment datasets (Rahman and Kumar Dey, 2018;\nIslam et al., 2021, 2023) and models (Tripto and\nAli, 2018; Ashik et al., 2019; Karim et al., 2020).\nPretrained language models have achieved decent\nperformance in a large variety of NLP downstream\ntasks through the finetuning.\nUnder this back-\nground, Bhattacharjee et al. (2022) pretrained the\nBanglaBERT model, a BERT-based language un-\nderstanding model pretrained on Bangla language\ncorpora. With the advent of the large language mod-\nels (LLMs), zero- and few-shot prompting methods\nhave gradually gained prominence. Hasan et al.\n(2023) compared the zero- and few-shot prompting\nperformance of LLMs with the finetuned models\nfor the Bangla sentiment analysis task. Our work\nexplores the application of the retrieval-augmented\nprompting method in Bangla violence detection\nand sentiment analysis tasks.\nMultilingual In-context Learning\nBrown et al.\n(2020a) demonstrated that LLMs like GPT-3 can ac-\nquire task-solving abilities by incorporating input-\noutput pairs as context. The in-context learning\napproach involves concatenating input with ran-\ndomly selected examples from the training dataset,\nwhich is also called the prompting method. Recent\nresearch (Gao et al., 2021; Liu et al., 2022, 2023;\nShi et al., 2023) has expanded on this idea by en-\nhancing prompts for pretrained models through\nthe inclusion of semantically similar examples.\nThe effectiveness of prompting methods for En-\nglish models extends to multilingual models in\ncross-lingual transfer learning as well. Zhao and\nSchütze (2021) and Huang et al. (2022) investi-\ngated the prompt-based learning with multilingual\nPLMs. Nie et al. (2023) incorporated augmented\nthe prompt with cross-lingual retrieval samples in\nthe multilingual understanding and proposed the\nPARC pipeline. Tanwar et al. (2023) augmented\nthe prompt with not only cross-lingual semantic\ninformation but also additional task information.\nHowever, previous studies mainly concentrated on\nthe multilingual encoder or encode-decoder mod-\nels, while our work extend the PARC pipeline to\nFigure 2: Detailed overview of the PARC pipeline for LRLs using cross-lingual retrieval: (a) An LRL input is used\nas a query for the cross-lingual retriever, which then retrieves the most semantically similar HRL sample from\nthe HRL corpus. The associated label is either taken directly from the corpus (labeled setting) or determined by\nself-prediction (unlabeled setting). (b) Next, this HRL sample, its label, and the original input are combined to\ncreate a retrieval-enhanced prompt for MPLM prediction.\nthe decoder-only multilingual LLMs.\nMultilingual LLMs\nIn the era of LLMs,\nBLOOMZ and mT0 (Muennighoff et al., 2023)\nare two representative newly emerging multilin-\ngual models. These two multilingual LLMs are\nfinetuned on xP3, a multilingual multitask fine-\ntuning dataset, and based on the pretrained mod-\nels BLOOM (Scao et al., 2022) and mT5 (Xue\net al., 2021), respectively. Six different sizes of\nBLOOMZ models are released from 560M to 176B\nand 5 different sizes of mT0 models are released\nfrom 300M to 13B. These multilingual LLMs open\nup the possibility for conducting few- and zero-\nshot cross-lingual in-context learning, as demon-\nstrated by recent benchmarking efforts, for exam-\nple MEGA (Ahuja et al., 2023) and BUFFET (Asai\net al., 2023).\n3\nMethodology\nOur research extends the work of Nie et al. (2023)\nby focusing on improving multilingual pre-trained\nlanguage models (MPLMs) for low-resource lan-\nguages in a zero-shot setting, specifically using re-\ntrieved content from high-resource languages such\nas English.\nThe backbone of our research approach is a two-\nstage pipeline consisting of a cross-lingual retriever\nand a prompt engineering process as shown in Fig-\nure 2. This pipeline aims to build on the strengths\nof MPLMs while mitigating their limitations, espe-\ncially when dealing with low-resource languages.\nThe first stage of the pipeline uses a cross-lingual\nretriever that maps the input Bangla text q to a vec-\ntor qembed in a shared embedding space and uses it\nas a query. Using semantic similarities with qembed,\nthe retriever returns the most similar k examples\nfrom high-resource languages either with or with-\nout their labels:\nR = arg\nk\nmax\ni∈{1,...,|d|} cos(qembed, di)\nwhere di means each document in the high-\nresource language corpus and |d| is the number\nof documents. If there’s no label, it suggests a\nself-prediction step.\nThe second stage of the pipeline is the prompt\nengineering. The input Bangla text and the re-\ntrieved pattern are subjected to this process. A\nprefix prompt template P is used to reformulate\nthe input to facilitate the model’s prediction y:\ny = MPLM(P(q, R))\nDepending on the architecture of the chosen\nMPLM, for decoder-only models, the answer is\ngenerated by the model directly. For encoder mod-\nels, the answer is obtained by first mapping each\nlabel to its predefined word using the verbalizer\nand then deducing the label word using mask token\nprediction.\nBy integrating cross-lingual content retrieval\nwith prompt-guided prediction, we aim to improve\nthe ability of MPLMs to handle low-resource lan-\nguages. This synergy not only extracts rich linguis-\ntic insights from high-resource languages, but also\nuses them to improve performance on low-resource\nlanguage tasks.\n4\nExperiments\nIn this study, we focused on the tasks of classifica-\ntion and summarization. We refer to our research\napproach, which uses k retrieved samples for cross-\nlingual augmented in-context learning methods, as\nthe main method in the following sections.\n4.1\nBaselines\nZero-shot\nThe template, when populated with\nthe input sample, is fed directly into the MPLM\nfor prediction. This process bypasses the use of\ncross-lingual context.\nLead64\nThe first 64 tokens of the input text are\ntaken as a summary of the text (For summarization\ntasks only).\n4.2\nTasks\n4.2.1\nClassification\nVio-Lens\nThe Vio-Lens dataset (Saha et al.,\n2023) contains YouTube comments related to vio-\nlent incidents in the Bengal region, with the goal\nof highlighting potential threats that could incite\nfurther violence. The prompt templates for both\nmain method and zero-shot baseline are defined as\nfollows:\n• BLOOMZ-3b and BLOOM-3b:\nReflecting on the statement \"{text}\",\nwhich\naggressive\nlevel\ndoes\nit\nresonate\nwith:\nnon-aggressive,\nslightly\naggressive,\nor\nhighly\naggressive?\n• mBERT: The underlying theme in {text}\nis [MASK].\nwith the verbalizer:\nv(Direct Violence) = assaultive,\nv(Passive Violence) = indirect,\nv(Non-Violence) = peaceful\nWe use the ETHOS (onlinE haTe speecH de-\ntectiON dataSet) (Mollas et al., 2020) as sentence\npool in our experiments. This repository provides\na dataset designed to identify hate speech on social\nmedia. We use the binary variants of the dataset,\nwhich contains 998 comments, each labeled for\nthe presence or absence of hate speech. Since the\nlabels are inconsistent, we use the self-prediction\nmethod to predict the labels.\nSentNoB\nDesigned to capture the sentiment\nwithin text, SentNoB classifies content as positive,\nnegative or neutral (Islam et al., 2021). The prompt\ntemplates for both main method and zero-shot base-\nline are defined as follows:\n• BLOOMZ-3b and BLOOM-3b:\nText:\n{text}\nWhat\nis\na\npossible\nsentiment\nfor\nthe\ntext\ngiven\nthe\nfollowing options?\n• mBERT: {text} Sentiment: [MASK]\nwith the verbalizer:\nv(0) = positive, v(1) = neural,\nv(2) = negative\nThe English Sentiment Analysis dataset (Rosen-\nthal et al., 2017), which consists of tweets anno-\ntated for sentiment on 2-, 3-, and 5-point scales\nwith labels positive, negative, and neutral, serves as\nthe HRL corpora in our study. We use the labeled\ntraining set for our experimental sentence pool.\n4.2.2\nSummarization\nXL-Sum\nis a large and varied dataset consisting\nof 1.35 million pairs of articles and their corre-\nsponding summaries (Hasan et al., 2021). These\npairs have been expertly annotated by the BBC and\nmeticulously extracted through a series of carefully\ndesigned heuristic methods. The dataset includes\n45 languages, from low to high resource, many\nof which do not currently have publicly available\ndatasets. The prompt template is defined for all\nmodels as follows:\n• Main method:\n{text} Generate a concise summary of\nthe above text using the same language\nas the original text ({target_lang}):\n• Zero-shot baseline:\n{text} Generate a concise summary of\nthe given text:\n4.3\nModels\nBLOOM\nis an autoregressive Large Language\nModel trained on a diverse corpus to generate text\nbased on prompts (Scao et al., 2022). It is capable\nof generating coherent text in 46 languages.\nBLOOMZ\ntakes a novel approach in the MPLM\nlandscape by applying Bloom filters in the con-\ntext of language models (Muennighoff et al., 2023).\nThis allows the model to use high-resource lan-\nguages to improve embeddings for low-resource\nlanguages, effectively bridging the gap between lan-\nguages with different levels of available resources.\nmBERT\nis an early MPLM that extends the orig-\ninal BERT model (Devlin et al., 2018). It is pre-\ntrained on a corpus of 104 languages, using shared\nWordPiece vocabularies and a unified architecture\nfor all languages.\nmT5\nor Multilingual T5 (Xue et al., 2021), is an\nextension of the T5 (Text-to-Text Transfer Trans-\nformer) model (Raffel et al., 2020) designed specif-\nically for multilingual capabilities. Pre-trained on\nmC4, a large multilingual dataset, mT5 demon-\nstrates multilingual capabilities by transforming\ninput text sequences into output sequences.\nCross-Lingual Retriever\nWe followed Nie\net al. (2023) to use the multilingual sentence\ntransformer “paraphrase-multilingual-mpnet-base-\nv2” (Reimers and Gurevych, 2019). This trans-\nformer maps sentences and paragraphs into a 768-\ndimensional dense vector space.\nSuch a high-\ndimensional embedding facilitates tasks such as\nclustering and semantic search. In our experiments,\nthe number of retrieval samples k is 1 and 3 for\nclassification task and 1 for summarization task.\n5\nResults\n5.1\nResults of classification tasks\nTable 1 provides an overview of the results of clas-\nsification. With the instructions of k = 3 retrieval\naugmented English prompts, we enhance the F1-\nscores of Bloomz-3b on the two tasks by 5% and\n10% respectively. While Bloom-3b, without in-\nstruction tuning compared to Bloomz-3b, cannot\ngenerate any meaningful result, suggesting that in-\nstruction tuning has a strong impact on retrieval\naugmented in-context learning. The traditional\nmasked MLM, mBERT, also gained improvement\nby 8% and 7%.\nVio-Lens\nzero shot\nk=1\nk=3\nbloomz-3b\n0.19\n0.2\n0.24\nbloom-3b\n0.00\n0.00\n0.00\nmbert\n0.21\n0.28\n0.29\nSentNoB\nzero shot\nk=1\nk=3\nbloomz-3b\n0.34\n0.44\n0.44\nbloom-3b\n0.00\n0.00\n0.00\nmbert\n0.30\n0.36\n0.37\nTable 1:\nF1-scores of the two classification tasks:\nBangla zero-shot baseline and with k retrieval aug-\nmented prompts.\nTo facilitate a comprehensive understanding of\nthe performance and discrepancies associated with\neach task, we present confusion matrices for anal-\nysis as follows. Given the confusion matrix in\nTable 2 , we find that:\n1) With a general assessment across micro,\nmacro, and weighted F1 scores, Bloomz-3b and\nmBERT gained improvement from the retrieval\nprompts. 2) Compare the two models, Bloomz-\n3b’s zero-shot setting tends to misclassify “non-\nviolence” and “Neutral”, and has a reduced macro\nF1 compared to its weighted F1, while mBERT\nhas a more balanced distribution of confusion be-\ntween “non-violence” (“Neutral”) and the other\nclasses. This may indicate that for classification\ntasks, the text generation struggles more with mi-\nnority classes compared to masked prediction.\n5.2\nResults of summarisation task\nThe Table 3 compares several models and methods\nfor summarization task.\nLEAD-64\nAs an extractive method, it performs\nwell across all metrics. This indicates that in many\ncases the first few sentences or tokens of an article\nor document provide a fairly informative summary.\nAs expected, LEAD-64 outperforms the mt5 base\nmodel in the zero-shot setting, but is outperformed\nby the Bloomz models in the same scenario.\nZero-Shot Models\nmt5-base produces the low-\nest scores across all metrics, suggesting that it\nstruggles to produce satisfactory summaries with-\nout domain-specific fine-tuning or data augmenta-\ntion. Both bloomz-1b1 and bloomz-3b show signif-\nicantly better performance, with bloomz-3b having\na slight edge over bloomz-1b1, especially in bigram\ncapture (R-2).\nRetrieval augmentation with k=1\nRetrieval aug-\nmentation seems to drastically affect the perfor-\nzero shot\nk=1\nk=3\nbloomz-3b\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\naccuracy\n0.33\n0.35\n0.36\nmacro avg\n0.15\n0.33\n0.20\n0.18\n0.34\n0.20\n0.26\n0.26\n0.17\nweighted avg\n0.14\n0.33\n0.19\n0.15\n0.35\n0.20\n0.42\n0.36\n0.24\nmbert\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\naccuracy\n0.22\n0.32\n0.33\nmacro avg\n0.31\n0.30\n0.18\n0.52\n0.29\n0.21\n0.18\n0.28\n0.21\nweighted avg\n0.40\n0.22\n0.21\n0.62\n0.32\n0.28\n0.26\n0.33\n0.29\nzero shot\nk=1\nk=3\nbloomz-3b\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\naccuracy\n0.61\n0.60\n0.61\nmacro avg\n0.31\n0.37\n0.34\n0.48\n0.48\n0.44\n0.47\n0.48\n0.44\nweighted avg\n0.51\n0.61\n0.55\n0.53\n0.60\n0.54\n0.53\n0.61\n0.54\nmbert\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\naccuracy\n0.35\n0.37\n0.39\nmacro avg\n0.38\n0.34\n0.30\n0.40\n0.38\n0.36\n0.42\n0.39\n0.37\nweighted avg\n0.43\n0.35\n0.34\n0.47\n0.37\n0.39\n0.48\n0.39\n0.41\nTable 2: Confusion matrix of main method in Vio-Lens (top) and SentNoB (bottom) test set of BLOOMZ-3b and\nmBERT.\nR-1\nR-2\nR-L\nR-LSum\nLEAD-64\n18.17\n5.23\n12.73\n12.74\nzero shot\nmt5-base\n5.01\n0.84\n4.83\n4.84\nbloomz-1b1\n22.08\n7.11\n18.43\n18.44\nbloomz-3b\n22.36\n7.88\n18.60\n18.58\nk=1\nmt5-base\n0.97\n0.13\n0.91\n0.92\nbloomz-1b1\n10.84\n2.80\n9.11\n9.12\nbloomz-3b\n6.61\n1.52\n5.56\n5.55\nTable 3: Rouge scores of Bangla summarization.\nmance of mt5-base, reducing its score considerably.\nThis could be due to noise introduced by the re-\ntrieved sample or ineffective use of the additional\ninformation. For the Bloomz models, bloomz-1b1\nstill retains decent performance, although there’s a\ndrop when compared to its zero-shot performance.\nSurprisingly, blommz-3b shows a sharper drop,\nsuggesting that the additional retrieval data may\nbe more of a distraction than an advantage for this\nmodel configuration in the summarization task.\n5.3\nAnalysis and Discussion\nWhen examining the performance of different mod-\nels on different tasks, several key observations\nemerge that are related to linguistic nuances, the\nunderlying language models, and resource alloca-\ntion.\nFor classification tasks, it’s clear that models\nwith a strong grasp of complex sentence structure\nand deeper semantics, such as the Bloomz-3b, are\nmore adept at distinguishing nuanced categories\nlike “passive violence” or the more ambiguous\n“neutral” sentiment. This aptitude likely stems from\ntheir ability to understand context better than their\nsimpler counterparts. In parallel, the critical role of\nzero-shot learning becomes apparent. The ability\nof a model to generalize a task without specific\nfine-tuning speaks volumes about its robustness.\nFor example, in our studies, models such as the\nBloomz-3b showed commendable performance in\na zero-shot setting. Furthermore, as we played\naround with the variable k (representing the num-\nber of samples retrieved), it was instructive to see\nthat a larger value didn’t always translate into bet-\nter performance. This underscores the nuanced\nability of a model to sift through information and\npotentially eliminate noise.\nTurning to the summarization task, coherence\nand relevance seem to be the pillars of excellence.\nAdvanced models are more adept at weaving sen-\ntences that are not only structurally coherent, but\nalso rich in information. This finesse is evident\nin the superior Rouge scores of the models. The\ndichotomy between generative and extractive ap-\nproaches is also evident. While generative mod-\nels, including mt5-base and Bloomz-1b1, outper-\nformed the extractive model (LEAD-64) in a zero-\nshot framework, they seemed a bit sensitive when\nFigure 3: Model performance over differences between zero-shot (represented as ‘0’ on the y-axis) and main method\nwith k=1 and k=3 demonstrations for Vio-Lens test set using bloomz-3b (left) and mbert (right).The y-axis shows\nthe deviations of the main method from the zero-shot values. The statistics are based on 8 and 6 templates, shown in\nAppendix Table 5 and Table 6, respectively.\nretrieval augmentation came into play.\nFinally, when it comes to resource distribution,\nthere’s an undeniable correlation between perfor-\nmance and computational resources.\nThe stel-\nlar performance of models like Bloomz-3b likely\ncomes at the cost of intense computational de-\nmands.\nHowever, one must consider the cost-\nbenefit ratio. In addition, the drop in performance\nof these models with retrieval augmentation at k=1\nsuggests a potential sensitivity to the balance or\ndiversity of the dataset.\nFor the summarization task, an interesting ob-\nservation is that more extensive models don’t al-\nways outperform on all metrics, suggesting that\nwe need to be more discriminating in our resource\nallocation. The significant performance drop with\nretrieval augmentation further supports this argu-\nment.\nTo conclude this analysis, while modern lan-\nguage models are capable of handling complex\ntasks, they require careful configuration and\nthoughtful resource distribution. Unraveling the\ncomplexity of these models can pave the way for\noptimized solutions in both classification and sum-\nmarization.\n6\nAblation Study\n6.1\nThe Stability across Templates\nIn our experiment for Vio-Lens, we compared the\nperformance of Bloomz-3b and mbert, in terms\nof their ability to classify text samples into cate-\ngories. In order to assess the effectiveness of the\nretrieval augmented prompting method compared\nto the zero-shot baseline, we conduct a statistic\nacross different templates.\nFor Bloomz-3b and mBERT, we test different\nprompt templates , and created a boxplot (Figure 3)\nto visualize the difference of F1 scores from our\nmain method to the zero-shot baseline across tem-\nplates. It’s shown that with the retrieval augmented\nEnglish prompts under different templates, both\nmodels achieved a stable improvement compared\nto the Bangla zeroshot baseline. Aso it’s clear that\nmBERT, on average, shows greater improvements\nin F1 scores when transitioning from the zero-shot\nbaseline to retrieval augmented prompting, com-\npared to Bloomz-3b.\n6.2\nImpact of Bangla and Hindi Prompt\nTemplate\nInstead of English, we further explore applying\nBangla itself and its linguistically similar high-\nresource language Hindi as the language of the\nprompt template , as shown in Table 4.\nMain method with English prompt: This config-\nuration yields the highest macro average F1 score\nof all three prompt templates.\nHindi Prompt Template: While the Hindi prompt\ntemplate leads to significant improvements in pre-\ncision and recall for individual categories such as\n“Neutral”, the macro average F1 score is still lower\nthan that of the main method with the English\nprompt.\nBangla prompt template: The Bangla prompt\ntemplate, while showing some improvements in\nprecision for specific categories such as “positive”,\nexperiences a decrease in recall and overall accu-\nk=1\nk=3\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nbangla prompt\npaFY: {text} in\u001ailixt ibk°pguil edOJa paeFYr jnY séabY Anuvit k?\naccuracy\n0.14\n0.45\nmacro avg\n0.34\n0.09\n0.13\n0.32\n0.28\n0.29\nweighted avg\n0.51\n0.14\n0.21\n0.49\n0.45\n0.46\nhindi prompt\npAW: {text} EnmnElEKt EvkSpo\\ ko d\u0003Kt\u0003 h\u0000 e pAW k\u0003 Ele s\\BAEvt BAvnA kyA h{ ?\naccuracy\n0.39\n0.54\nmacro avg\n0.34\n0.28\n0.29\n0.34\n0.34\n0.34\nweighted avg\n0.51\n0.39\n0.43\n0.52\n0.54\n0.53\nTable 4: Results of prompt template in bangla and hindi of main method in SentNoB test of bloomz-3b.\nracy. As a result, the macro average F1 score is the\nlowest of the three templates.\nThis means that while the Bangla prompt tem-\nplate may improve performance for specific cat-\negories, it has an overall negative impact on the\nmodel’s ability to generalize across all categories\nin the SentNoB test. Conversely, the Hindi prompt\ntemplate’s improvements in precision and recall for\nindividual categories don’t translate into a higher\nmacro average F1 score compared to the main\nmethod with the English prompt.\nIn summary, the macro average F1-score results\nshow that the main method with the English prompt\ntemplate remains the most effective overall. How-\never, the choice of prompt template can signifi-\ncantly affect performance for specific categories,\nas demonstrated by the Hindi and Bangla tem-\nplates. This nuanced understanding underscores\nthe need to balance category-specific and overall\nperformance when selecting prompt templates in\ncross-lingual retrieval augmentation.\n6.3\nImpact of Hindi sentence pool\nComparing the results in Table 7 with the previous\nexperiments, we observe that the Hindi retrieval\ndataset generally improves the model’s ability to\nretrieve “Neutral” content in the mBERT model.\nHowever, the model continues to struggle with the\n“Neutral” category, with low recall and F1 scores,\nregardless of the sentence pool used. This sug-\ngests that further refinements may be needed to\nimprove retrieval accuracy for neutral sentiment\nsentences. The studies with Hindi retrieval data\nshow that both bloomz-3b and mbert don’t show\nany improvements compared to the main method\nwith the English prompt template. This suggests\nthat while using alternative retrieval datasets can\nimprove performance for specific sentiment cate-\ngories, the choice of retrieval data may need to be\ncarefully considered to maximize overall perfor-\nmance across categories in cross-lingual sentiment\nanalysis tasks.\n7\nConclusion\nIn this paper, we have introduced a novel approach\nto address the challenges of applying Large Lan-\nguage Models to low-resource languages, with\na focus on Bangla. Our methodology employs\ncross-lingual retrieval-augmented in-context learn-\ning, thereby enriching the capabilities of MPLMs,\nspecifically BLOOM and BLOOMZ. We have ex-\ntensively tested our approach on two classification\ntasks and one summarization task.\nOur experimental results demonstrate the effec-\ntiveness of our approach in achieve superior F1\nscores for classification tasks.\nUpon further analysis, the cross-lingual retrieval\nmechanism contributes significantly to the model’s\nperformance.\nThis work lays the foundation for further stud-\nies on the application of cross-lingual retrieval and\nin-context learning methods in low-resource lan-\nguages. Future work could extend this approach to\neven more underrepresented languages and poten-\ntially adapt it to more complex NLP tasks such as\nquestion answering or machine translation.\nLimitations\nWhile our study has yielded promising results, it\nis not without limitations. The effectiveness of\nretrieval augmentation is also tied to the model\narchitecture, and its impact on different models\nremains largely unexplored. In addition, the avail-\nability of specific language datasets for sentence\nretrieval and resource constraints remain practical\nchallenges. Further exploration of prompt design\nand consideration of external factors could improve\nour methodology. Acknowledging these limitations\nis essential for a full interpretation of our results\nand the direction of future research.\nAcknowledgements\nThis work was supported by Leibniz Supercom-\nputing Centre (LRZ), Munich Center for Machine\nLearning (MCML) and China Scholarship Council\n(CSC).\nReferences\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi\nJain, Harshita Diddee, Samuel Maina, Tanuja Ganu,\nSameer Segal, Maxamed Axmed, Kalika Bali, et al.\n2023. Mega: Multilingual evaluation of generative\nai. arXiv preprint arXiv:2303.12528.\nMikel Artetxe and Holger Schwenk. 2019.\nMas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions\nof the Association for Computational Linguistics,\n7:597–610.\nAkari Asai, Sneha Kudugunta, Xinyan Velocity Yu,\nTerra Blevins, Hila Gonen, Machel Reid, Yulia\nTsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.\n2023. Buffet: Benchmarking large language models\nfor few-shot cross-lingual transfer. arXiv preprint\narXiv:2305.14857.\nMd Akhter-Uz-Zaman Ashik, Shahriar Shovon, and\nSummit Haque. 2019. Data set for sentiment analysis\non bengali news comments and its baseline evalua-\ntion. In 2019 International Conference on Bangla\nSpeech and Language Processing (ICBSLP), pages\n1–5. IEEE.\nAbhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad,\nKazi Samin Mubasshir, Md Saiful Islam, Anindya\nIqbal, M. Sohel Rahman, and Rifat Shahriyar.\n2022. BanglaBERT: Language model pretraining\nand benchmarks for low-resource language under-\nstanding evaluation in Bangla. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022, pages 1318–1327, Seattle, United States. Asso-\nciation for Computational Linguistics.\nAnirban Bhowmick and Abhik Jana. 2021. Sentiment\nanalysis for Bengali using transformer based models.\nIn Proceedings of the 18th International Conference\non Natural Language Processing (ICON), pages 481–\n486, National Institute of Technology Silchar, Silchar,\nIndia. NLP Association of India (NLPAI).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020a. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020b.\nLanguage models are few-shot learners.\nArXiv,\nabs/2005.14165.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nSandipan Dandapat, Sudeshna Sarkar, and Anupam\nBasu. 2004. A hybrid model for part-of-speech tag-\nging and its application to bengali. In International\nconference on computational intelligence, pages 169–\n172. Citeseer.\nAmitava Das and Sivaji Bandyopadhyay. 2010. Phrase-\nlevel polarity identification for bangla. Int. J. Comput.\nLinguistics Appl., 1(1-2):169–182.\nAmitava Das and Björn Gambäck. 2014. Identifying\nlanguages at the word level in code-mixed Indian\nsocial media text. In Proceedings of the 11th Interna-\ntional Conference on Natural Language Processing,\npages 378–387, Goa, India. NLP Association of In-\ndia.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAsif Ekbal and Sivaji Bandyopadhyay. 2007. A hid-\nden markov model based named entity recognition\nsystem: Bengali and hindi as case studies. In Pat-\ntern Recognition and Machine Intelligence: Second\nInternational Conference, PReMI 2007, Kolkata, In-\ndia, December 18-22, 2007. Proceedings 2, pages\n545–552. Springer.\nAsif Ekbal and Sivaji Bandyopadhyay. 2008a. Devel-\nopment of bengali named entity tagged corpus and\nits use in ner systems. In Proceedings of the 6th\nWorkshop on Asian Language Resources.\nAsif Ekbal and Sivaji Bandyopadhyay. 2008b. A web-\nbased bengali news corpus for named entity recogni-\ntion. Language Resources and Evaluation, 42:173–\n182.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nViktor Hangya, Hossain Shaikh Saadi, and Alexander\nFraser. 2022. Improving low-resource languages in\npre-trained multilingual language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 11993–\n12006, Abu Dhabi, United Arab Emirates. Associa-\ntion for Computational Linguistics.\nMd. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj\nAlam, Anika Anjum, Avijit Sarker, and Sheak\nRashed Haider Noori. 2023.\nZero- and few-shot\nprompting with llms: A comparative study with fine-\ntuned models for bangla sentiment analysis.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nMuntasir Hoq, Promila Haque, and Mohammed Nazim\nUddin. 2021. Sentiment analysis of bangla language\nusing deep learning approaches. In COMS2.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. ArXiv, abs/2003.11080.\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu\nWei, and Houfeng Wang. 2022.\nZero-shot cross-\nlingual transfer of prompt-based tuning with a unified\nmultilingual prompt. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11488–11497, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nKhondoker Ittehadul Islam, Sudipta Kar, Md Saiful Is-\nlam, and Mohammad Ruhul Amin. 2021. SentNoB:\nA dataset for analysing sentiment on noisy Bangla\ntexts. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3265–3271,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMd Ekramul Islam, Labib Chowdhury, Faisal Ahamed\nKhan, Shazzad Hossain, Md Sourave Hossain, Mo-\nhammad Mamun Or Rashid, Nabeel Mohammed, and\nMohammad Ruhul Amin. 2023. Sentigold: A large\nbangla gold standard multi-domain sentiment anal-\nysis dataset and its evaluation. In Proceedings of\nthe 29th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, pages 4207–4218.\nMd Zahurul Islam, Md Nizam Uddin, and Mumit Khan.\n2007. A light weight stemmer for bengali and its use\nin spelling checker.\nMd Rezaul Karim, Bharathi Raja Chakravarthi, John P\nMcCrae, and Michael Cochez. 2020. Classification\nbenchmarks for under-resourced bengali language\nbased on multichannel convolutional-lstm network.\nIn 2020 IEEE 7th International Conference on Data\nScience and Advanced Analytics (DSAA), pages 390–\n399. IEEE.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022.\nWhat\nmakes good in-context examples for GPT-3?\nIn\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYanchen Liu, Timo Schick, and Hinrich Schtze. 2023.\nSemantic-oriented unlabeled priming for large-scale\nlanguage models.\nIn Proceedings of The Fourth\nWorkshop on Simple and Efficient Natural Language\nProcessing (SustaiNLP), pages 32–38, Toronto,\nCanada (Hybrid). Association for Computational Lin-\nguistics.\nAshis Kumar Mandal and Rikta Sen. 2014. Supervised\nlearning methods for bangla web document catego-\nrization. arXiv preprint arXiv:1410.2045.\nMunirul Mansur. 2006. Analysis of n-gram based text\ncategorization for bangla in a newspaper corpus.\nPh.D. thesis, BRAC University.\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\nand Grigorios Tsoumakas. 2020. Ethos: an online\nhate speech detection dataset.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991–16111, Toronto, Canada. Association\nfor Computational Linguistics.\nErcong Nie, Sheng Liang, Helmut Schmid, and Hinrich\nSchütze. 2023. Cross-lingual retrieval augmented\nprompt for low-resource languages. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 8320–8340, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nJiaul H Paik and Swapan K Parui. 2008. A simple\nstemmer for inflectional languages. In Forum for\nInformation Retrieval Evaluation. Citeseer.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nMd Atikur Rahman and Emon Kumar Dey. 2018.\nDatasets for aspect-based sentiment analysis in\nbangla and its baseline evaluation. Data, 3(2):15.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nIn Proceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017), pages 502–\n518, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nSourav Saha, Jahedul Alam Junaed, Maryam Saleki,\nArnab Sen Sharma, Mohammad Rashidujjaman Rifat,\nMohamed Rahout, Syed Ishtiaque Ahmed, Nabeel\nMohammad, and Mohammad Ruhul Amin. 2023.\nVio-lens: A novel dataset of annotated social net-\nwork posts leading to different forms of commu-\nnal violence and its evaluation. In Proceedings of\nthe 1st International Workshop on Bangla Language\nProcessing (BLP-2023), Singapore. Association for\nComputational Linguistics.\nSalim Sazzed. 2020. Cross-lingual sentiment classifica-\ntion in low-resource Bengali language. In Proceed-\nings of the Sixth Workshop on Noisy User-generated\nText (W-NUT 2020), pages 50–60, Online. Associa-\ntion for Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki,\nElizabeth-Jane Pavlick, Suzana Ili’c, Daniel Hesslow,\nRoman Castagn’e, Alexandra Sasha Luccioni, Franc-\ncois Yvon, Matthias Gallé, Jonathan Tow, Alexan-\nder M. Rush, Stella Rose Biderman, Albert Web-\nson, Pawan Sasanka Ammanamanchi, Thomas Wang,\nBenoît Sagot, Niklas Muennighoff, Albert Villanova\ndel Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Beltagy, Huu\nNguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz\nSuarez, Victor Sanh, Hugo Laurenccon, Yacine Jer-\nnite, Julien Launay, Margaret Mitchell, Colin Raf-\nfel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etx-\nabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris C. Emezue, Christopher Klamm, Colin Leong,\nDaniel Alexander van Strien, David Ifeoluwa Ade-\nlani, Dragomir R. Radev, Eduardo Gonz’alez Pon-\nferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar\nNatan, Francesco De Toni, Gérard Dupont, Germán\nKruszewski, Giada Pistilli, Hady ElSahar, Hamza\nBenyamina, Hieu Trung Tran, Ian Yu, Idris Abdul-\nmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier\nde la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, Jorg Frohberg, Josephine L. To-\nbing, Joydeep Bhattacharjee, Khalid Almubarak,\nKimbo Chen, Kyle Lo, Leandro von Werra, Leon\nWeber, Long Phan, Loubna Ben Allal, Ludovic Tan-\nguy, Manan Dey, Manuel Romero Muñoz, Maraim\nMasoud, Mar’ia Grandury, Mario vSavsko, Max\nHuang, Maximin Coavoux, Mayank Singh, Mike\nTian-Jian Jiang, Minh Chien Vu, Mohammad Ali\nJauhar, Mustafa Ghaleb, Nishant Subramani, Nora\nKassner, Nurulaqilla Khamis, Olivier Nguyen, Omar\nEspejel, Ona de Gibert, Paulo Villegas, Peter Hender-\nson, Pierre Colombo, Priscilla A. Amuok, Quentin\nLhoest, Rheza Harliman, Rishi Bommasani, Roberto\nL’opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,\nSebastian Nagel, Shamik Bose, Shamsuddeen Has-\nsan Muhammad, Shanya Sharma, S. Longpre, So-\nmaieh Nikpoor, S. Silberberg, Suhas Pai, Sydney\nZink, Tiago Timponi Torrent, Timo Schick, Tris-\ntan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Elizabeth Salesky, Sab-\nrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, An-\ndrea Santilli, Antoine Chaffin, Arnaud Stiegler, Deba-\njyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-Shaibani, Matteo Manica, Ni-\nhal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Févry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin\nYong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar\nTojarieh, Adam Roberts, Hyung Won Chung, Jae-\nsung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Franc-\ncois Lavall’ee, Rémi Lacroix, Samyam Rajbhan-\ndari, Sanchit Gandhi, Shaden Smith, Stéphane Re-\nquena, Suraj Patil, Tim Dettmers, Ahmed Baruwa,\nAmanpreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur’elie N’ev’eol,\nCharles Lovering, Daniel H Garrette, Deepak R.\nTunuguntla, Ehud Reiter, Ekaterina Taktasheva, Eka-\nterina Voloshina, Eli Bogdanov, Genta Indra Winata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jekate-\nrina Novikova, Jessica Zosa Forde, Xiangru Tang,\nJungo Kasai, Ken Kawamura, Liam Hazan, Ma-\nrine Carpuat, Miruna Clinciu, Najoung Kim, New-\nton Cheng, Oleg Serikov, Omer Antverg, Oskar\nvan der Wal, Rui Zhang, Ruochen Zhang, Sebastian\nGehrmann, Shachar Mirkin, S. Osher Pais, Tatiana\nShavrina, Thomas Scialom, Tian Yun, Tomasz Lim-\nisiewicz, Verena Rieser, Vitaly Protasov, Vladislav\nMikhailov, Yada Pruksachatkun, Yonatan Belinkov,\nZachary Bamberger, Zdenvek Kasner, Zdenˇek Kas-\nner, Amanda Pestana, Amir Feizpour, Ammar Khan,\nAmy Faranak, Ananda Santa Rosa Santos, Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Are-\nzoo Abdollahi, Aycha Tammour, Azadeh HajiHos-\nseini, Bahareh Behroozi, Benjamin Olusola Ajibade,\nBharat Kumar Saxena, Carlos Muñoz Ferrandis,\nDanish Contractor, David M. Lansky, Davis David,\nDouwe Kiela, Duong Anh Nguyen, Edward Tan,\nEmily Baylor, Ezinwanne Ozoani, Fatim Tahirah\nMirza, Frankline Ononiwu, Habib Rezanejad, H.A.\nJones, Indrani Bhattacharya, Irene Solaiman, Irina\nSedenko, Isar Nejadgholi, Jan Passmore, Joshua\nSeltzer, Julio Bonis Sanz, Karen Fort, Lívia Macedo\nDutra, Mairon Samagaio, Maraim Elbadri, Mar-\ngot Mieskes, Marissa Gerchick, Martha Akinlolu,\nMichael McKenna, Mike Qiu, M. K. K. Ghauri,\nMykola Burynok, Nafis Abrar, Nazneen Rajani, Nour\nElkott, Nourhan Fahmy, Olanrewaju Samuel, Ran\nAn, R. P. Kromann, Ryan Hao, Samira Alizadeh,\nSarmad Shubber, Silas L. Wang, Sourav Roy, Syl-\nvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu\nNguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,\nAbhinav Ramesh Kashyap, A. Palasciano, Alison\nCallahan, Anima Shukla, Antonio Miranda-Escalada,\nAyush Kumar Singh, Benjamin Beilharz, Bo Wang,\nCaio Matheus Fonseca de Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on\nPerin’an, Daniel Molano, Dian Yu, Enrique Man-\njavacas, Fabio Barth, Florian Fuhrimann, Gabriel\nAltay, Giyaseddin Bayrak, Gully Burns, Helena U.\nVrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang,\nJohn Giorgi, Jonas Golde, Jose David Posada, Karthi\nSivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, María Andrea Castillo, Mar-\nianna Nezhurina, Mario Sanger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, M Wolf, Mina\nMihaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pas-\ncale Fung, Patricia Haller, R. Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo L. Canalli, Ros-\naline Su, Ruisi Su, Samuel Cahyawijaya, Samuele\nGarda, Shlok S Deshmukh, Shubhanshu Mishra, Sid\nKiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti\nKumar, Stefan Schweter, Sushil Pratap Bharati, T. A.\nLaud, Th’eo Gigant, Tomoya Kainuma, Wojciech\nKusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman,\nYifan Xu, Ying Xu, Yu Xu, Zhee Xao Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2022.\nBloom: A 176b-parameter\nopen-access multilingual language model. ArXiv,\nabs/2211.05100.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models.\narXiv\npreprint arXiv:2301.12652.\nEshaan Tanwar, Subhabrata Dutta, Manish Borthakur,\nand Tanmoy Chakraborty. 2023. Multilingual LLMs\nare better cross-lingual in-context learners with align-\nment. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 6292–6307, Toronto,\nCanada. Association for Computational Linguistics.\nNafis Irtiza Tripto and Mohammed Eunus Ali. 2018.\nDetecting multilabel sentiment and emotions from\nbangla youtube comments. In 2018 International\nConference on Bangla Speech and Language Pro-\ncessing (ICBSLP), pages 1–6. IEEE.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nMd Ferdous Wahid, Md Jahid Hasan, and Md Shahin\nAlom. 2019. Cricket sentiment analysis from bangla\ntext using recurrent neural network with long short\nterm memory model. In 2019 International Confer-\nence on Bangla Speech and Language Processing\n(ICBSLP), pages 1–4. IEEE.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nMengjie Zhao and Hinrich Schütze. 2021. Discrete and\nsoft prompting for multilingual models. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8547–8555,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nA\nAppendix\nzero-shot\nk=1\nk=3\nprompt\n{text} Direct Aggression, Indirect Aggression, or No Aggression?\naccuracy\n0.53\n0.54\n0.54\nmacro avg\n0.17\n0.18\n0.18\nweighted avg\n0.38\n0.38\n0.38\nprompt\nEvaluate the text: ’{text}’. Would you categorize it as absence of aggression, mild aggression,\nor strong aggression?\naccuracy\n0.18\n0.23\n0.23\nmacro avg\n0.17\n0.15\n0.15\nweighted avg\n0.13\n0.16\n0.16\nprompt\nIn the context of ’{text}’, which category best captures its aggression level: absence of aggression,\nmild aggression, or strong aggression?\naccuracy\n0.12\n0.15\n0.16\nmacro avg\n0.1\n0.15\n0.16\nweighted avg\n0.06\n0.11\n0.12\nprompt\nFor the text: ’{text}’, ascertain its aggression scale: absence of aggression, mild aggression, or\nstrong aggression?\naccuracy\n0.19\n0.21\n0.21\nmacro avg\n0.13\n0.14\n0.14\nweighted avg\n0.14\n0.16\n0.15\nprompt\nFrom the following choices, which resonates with the theme of ’{text}’? Options: No Intensity,\nLow Intensity, High Intensity\naccuracy\n0.13\n0.24\n0.19\nmacro avg\n0.1\n0.17\n0.15\nweighted avg\n0.12\n0.26\n0.2\nprompt\nFrom the following choices, which resonates with the theme of ’{text}’? Options: no intensity,\nlow intensity, high intensity\naccuracy\n0.23\n0.28\n0.27\nmacro avg\n0.18\n0.22\n0.2\nweighted avg\n0.22\n0.31\n0.26\nprompt\nIn the context of the text ’{text}’, which of the following best describes its tone? Options: No\nIntensity, Low Intensity, High Intensity\naccuracy\n0.14\n0.2\n0.15\nmacro avg\n0.11\n0.15\n0.12\nweighted avg\n0.1\n0.18\n0.13\nprompt\nReflecting on the statement ’{text}’, which aggressive level does it resonate with: non-aggressive,\nslightly aggressive, or highly aggressive?\naccuracy\n0.33\n0.35\n0.36\nmacro avg\n0.2\n0.2\n0.17\nweighted avg\n0.19\n0.2\n0.24\nTable 5: F1-score results with 8 prompt templates of Vio-Lens test using bloomz-3b model\nzero-shot\nk=1\nk=3\nprompt\nThe text displays [MASK] aggression: {text}\nverbalizer\ndirect, indirect, none\naccuracy\n0.36\n0.35\n0.36\nmacro avg\n0.22\n0.23\n0.23\nweighted avg\n0.31\n0.31\n0.31\nprompt\nConsidering aggressive tendencies, this is [MASK]: {text}\nverbalizer\novert, covert, absent\naccuracy\n0.1\n0.2\n0.17\nmacro avg\n0.07\n0.17\n0.14\nweighted avg\n0.03\n0.19\n0.15\nprompt\nFrom an aggression perspective, the text is [MASK]: {text}\nverbalizer\novert, covert, absent\naccuracy\n0.12\n0.22\n0.2\nmacro avg\n0.09\n0.18\n0.16\nweighted avg\n0.06\n0.21\n0.18\nprompt\nThe described behavior in {text} is [MASK] aggression.\nverbalizer\nexplicit, implicit, neutral\naccuracy\n0.24\n0.36\n0.35\nmacro avg\n0.19\n0.24\n0.23\nweighted avg\n0.23\n0.31\n0.3\nprompt\nThe underlying theme in {text} is [MASK] aggression.\nverbalizer\nassaultive, indirect, peaceful\naccuracy\n0.22\n0.32\n0.33\nmacro avg\n0.18\n0.21\n0.21\nweighted avg\n0.21\n0.28\n0.29\nprompt\n{text} is interpreted as [MASK] aggression.\nverbalizer\nassaultive, indirect, peaceful\naccuracy\n0.51\n0.49\n0.51\nmacro avg\n0.23\n0.27\n0.25\nweighted avg\n0.37\n0.37\n0.37\nTable 6: F1-score results with 6 prompt templates of Vio-Lens test using mBert model\nk=1\nk=3\nbloomz-3b\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nNegative\n0.58\n0.84\n0.69\n0.59\n0.88\n0.70\nNeutral\n0.09\n0.00\n0.00\n0.08\n0.00\n0.00\nPositive\n0.55\n0.49\n0.52\n0.58\n0.47\n0.52\naccuracy\n0.57\n0.58\nmacro avg\n0.41\n0.44\n0.40\n0.42\n0.45\n0.41\nweighted avg\n0.48\n0.57\n0.51\n0.49\n0.58\n0.51\nmbert\nprecision\nrecall\nf1-score\nprecision\nrecall\nf1-score\nNegative\n0.48\n0.24\n0.32\n0.48\n0.33\n0.39\nNeutral\n0.21\n0.34\n0.26\n0.21\n0.28\n0.24\nPositive\n0.27\n0.37\n0.31\n0.25\n0.33\n0.28\naccuracy\n0.30\n0.32\nmacro avg\n0.32\n0.32\n0.30\n0.31\n0.31\n0.31\nweighted avg\n0.36\n0.30\n0.30\n0.36\n0.32\n0.33\nTable 7: Results in SentNoB test of BLOOMZ-3b and mBERT with hindi retrieval corpus.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-11-01",
  "updated": "2023-12-02"
}