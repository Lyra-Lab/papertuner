{
  "id": "http://arxiv.org/abs/2311.00756v2",
  "title": "The Quantum Cartpole: A benchmark environment for non-linear reinforcement learning",
  "authors": [
    "Kai Meinerz",
    "Simon Trebst",
    "Mark Rudner",
    "Evert van Nieuwenburg"
  ],
  "abstract": "Feedback-based control is the de-facto standard when it comes to controlling\nclassical stochastic systems and processes. However, standard feedback-based\ncontrol methods are challenged by quantum systems due to measurement induced\nbackaction and partial observability. Here we remedy this by using weak quantum\nmeasurements and model-free reinforcement learning agents to perform quantum\ncontrol. By comparing control algorithms with and without state estimators to\nstabilize a quantum particle in an unstable state near a local potential energy\nmaximum, we show how a trade-off between state estimation and controllability\narises. For the scenario where the classical analogue is highly nonlinear, the\nreinforcement learned controller has an advantage over the standard controller.\nAdditionally, we demonstrate the feasibility of using transfer learning to\ndevelop a quantum control agent trained via reinforcement learning on a\nclassical surrogate of the quantum control problem. Finally, we present results\nshowing how the reinforcement learning control strategy differs from the\nclassical controller in the non-linear scenarios.",
  "text": "SciPost Physics\nSubmission\nThe Quantum Cartpole:\nA benchmark environment for non-linear reinforcement learning\nK. Meinerz1⋆, S. Trebst1, M. Rudner2, and E. van Nieuwenburg3\n1 Institute for Theoretical Physics, University of Cologne, 50937 Cologne, Germany\n2 Department of Physics, University of Washington, Seattle, Washington 98195-1560, USA\n3 Lorentz Institute and Leiden Institute of Advanced Computer Science, Leiden University,\nP.O. Box 9506, 2300 RA Leiden, The Netherlands\n⋆kmeinerz@thp.uni-koeln.de\nMarch 12, 2024\nAbstract\nFeedback-based control is the de-facto standard when it comes to controlling classical\nstochastic systems and processes. However, standard feedback-based control methods\nare challenged by quantum systems due to measurement induced backaction and partial\nobservability. Here we remedy this by using weak quantum measurements and model-\nfree reinforcement learning agents to perform quantum control. By comparing control\nalgorithms with and without state estimators to stabilize a quantum particle in an un-\nstable state near a local potential energy maximum, we show how a trade-off between\nstate estimation and controllability arises. For the scenario where the classical analogue\nis highly nonlinear, the reinforcement learned controller has an advantage over the stan-\ndard controller. Additionally, we demonstrate the feasibility of using transfer learning\nto develop a quantum control agent trained via reinforcement learning on a classical\nsurrogate of the quantum control problem. Finally, we present results showing how the\nreinforcement learning control strategy differs from the classical controller in the non-\nlinear scenarios.\nContents\n1\nIntroduction\n2\n2\nThe Quantum Cartpole\n3\n2.1\nClassical surrogate of the quantum cartpole\n5\n3\nControl algorithms\n5\n3.1\nLinear Quadratic Gaussian Control\n5\n3.2\nReinforcement Learning Control\n6\n4\nControlling the quantum cartpole\n7\n4.1\nPotential variations\n7\n4.2\nTransfer learning\n10\n4.3\nController characteristics\n10\n5\nConclusions and Outlook\n11\nA\nAppendix\n12\n1\narXiv:2311.00756v2  [quant-ph]  10 Mar 2024\nSciPost Physics\nSubmission\nA.1\nSystem parameters\n12\nA.2\nWeak Measurement\n12\nA.3\nNoise determination of classical system\n14\nA.4\nLinear Quadratic Gaussian Controller LQGC\n14\nA.4.1\nKalman Filter\n14\nA.4.2\nKalman Filter with same time step correlated noise\n15\nA.4.3\nExtended Kalman Filter\n16\nA.4.4\nLinear quadratic regulator\n16\nA.5\nReinforcement learning training\n17\nA.6\nExtended classical benchmarks\n19\nReferences\n20\n1\nIntroduction\nFeedback-based control is essential in many different industries and domains.\nStabilizing\ntemperatures, chemical reactions, robotics and even biomedical devices are all possible by\ncontinuously adjusting the system inputs based on real-time feedback. Applications of this\ntype of control to quantum systems has not yet reached this level of maturity, though sev-\neral other quantum optimal control methods, such as GRAPE and CRAB [1–3] have gained\nmore widespread adoption. A key issue with feedback-based control for quantum systems is\nthat measurements of the quantum system cause measurement back-action [4–6] , limiting\nthe amount of information that can be obtained. In many cases that renders standard opti-\nmal control techniques inapplicable or infeasible, either because they require a model of the\nquantum system or because they need gradients that require many measurements to estimate.\nIn this work we discuss a simple quantum problem for benchmarking feedback-based con-\ntrol, building on the quantum cartpole [7]. This control problem is based on the classical cart-\npole problem, which has become the de-facto standard benchmark for reinforcement learning\ncontrollers. We adapt the quantum cartpole problem to include explicit weak measurements\nof position and momentum, and use a continuous control parameter for feedback. In addition,\nwe introduce a classical surrogate for this quantum problem by mimicking the measurement\nfeedback on the system and the uncertainty in the measurements via noise. For this classical\nmodel, we investigate a standard optimal control algorithm – linear quadratic Gaussian con-\ntrol (LQGC), and show that this same controller can also control the quantum system based\non weak measurement inputs. In regimes where the standard optimal controller struggles,\ne.g. for more non-linear systems or in scenarios were a noise characterization is infeasible, we\ndemonstrate that deep reinforcement learning [8] remains a valid option for achieving control.\nDeep reinforcement learning (RL) has been demonstrated to be a useful tool for control in\nseveral previous works [9–14], starting with [15]. It provides a general approach to devising\ncontrol strategies in cases where a model of the system’s dynamics is incomplete, or where\nother properties such as the noise model are unknown.\nThis work hence fits into the more general context of machine learning applications to\nquantum systems [16]. Some of those have used RL for feedback-based control, such as [7,\n17,18]. An interesting recent work has also explored the use of weak nonlinear measurements\nas a way to compensate for purely linear controllers [19].\n2\nSciPost Physics\nSubmission\n2\nThe Quantum Cartpole\nThe quantum system we consider in this work is derived from the well-known classical cartpole\nproblem [20]. In it, a cart rolls on a flat one-dimensional track, and a force must be applied\nin either direction in order to keep a pole, hinged to the middle of the cart, upright. At every\ntimestep, the system is described by a vector st = (xt, ˙xt,θt, ˙θt) containing the instantaneous\nposition xt and pole angle θt and their time derivatives. When the angle θ exceeds a threshold\nangle θth, the failure condition is met and the controller failed to stabilize the system. The time\nstep at which this failure condition is met is labelled the termination time ttermination.\nA related but slightly simpler control problem is that of a particle sliding off of a hill needing\nto be pushed back up, for which the state vector is simply st = (xt, ˙xt) and for which the failure\ncondition is when the particle slides beyond a certain distance xth, e.g., |x| > |xth|. Although\nthis system is an inverted pendulum, the quantum version of this problem has been dubbed the\nquantum cartpole [4,7], in which a free particle [initialized as a Gaussian wavepacket ψ(x)] is\ncentered on an inverted potential V(ˆx) (for which we will discuss several choices later). The\nsystem undergoes unitary dynamics governed by the Hamiltonian\nˆH = ˆp2\n2m + V(ˆx),\n(1)\nand the failure condition is now set by at least 50% of the wavepacket’s probability density\nextending beyond xth. The state vector of this system is the wavefunction ψ(x), though a\ncontroller will not have access to it. Instead, the controller has access to the results of weak\nmeasurements on the system, based on which the controller must decide to apply a particular\nunitary ‘kick’ uF to the system. This control force is realized through a momentum shift opera-\ntor, i.e. uF = e−iF ˆx. Differently from [7], we will allow the controllers to choose a continuous\nF (with bounded strength |F| < |Fmax|) and we will provide the controller access to data from\nrepeated discrete weak measurements of position and momentum (rather than continuous\nmeasurements of position only).\nX\nX\nControl Algorithm\nApply Control\nWeak Measurements\nFigure 1: The quantum cartpole setup. A wavepacket ψ(x) is placed on top of an\ninverted potential V(ˆx), and a controller must ensure that at least 50% of |ψ(x)|2\nstays within the interval [−xth, xth]. As input, the control algorithm receives weak\nmeasurement outcomes, and as output it sets the strength of a unitary control force\nthat is applied to the wavepacket.\n3\nSciPost Physics\nSubmission\nThe full dynamics of the problem are hence as follows. First, a Gaussian wavepacket is\ninitialized at the top of the inverted potential. The wavepacket has a width set by σ = 1.0,\nand has an initial momentum chosen randomly from a zero mean uniform distribution of width\nσp =\nq\n〈p2\ninit〉= 0.1, where all units are set to 1. A more detailed description of the parameters\nand some comments regarding the units can be found in App. A.1. From then on, in every time\nstep ∆t:\n(i) The system evolves unitarily under Hamiltonian (1) for a time ∆t;\n(ii) weak position and momentum measurements are performed, sequentially, and are re-\nported to the controller (more details below);\n(iii) the unitary operator uF is applied to the system, with F chosen by the controller.\nThis loop is the core of the dynamics, and runs until the failure condition is met.\nTo investigate a trade-off between different timescales of the dynamics and control, we\nintroduce a variable Nmeas representing the number of times the dynamics loop runs until the\nforce F can be changed. This is shown schematically in Fig. 2. In each repetition we perform a\nweak measurement of the system’s position and momentum [21], and average them into xest\nand pest, which are both passed to the control algorithm. Notice that this is not identical to\nperforming an Nmeas times stronger weak measurement, as between every single measurement\nthe wave function evolves in time over the duration ∆t, which is kept constant independent\nfrom Nmeas.\nThe pre-processing step of averaging the weak measurement results (rather than providing\nthe measurements directly and using, e.g., a controller with memory), is inspired by the frame-\nstacking technique used in reinforcement learning [22] and has the goal of getting better\nestimates of the position and momentum, and thereby preventing the controllers from applying\ntoo strong or too weak forces. The weak measurements performed at each step are essential for\nTime Evolve\nWeak Measurements\nApply Control\nRepeat             times\nduration \nFigure 2: Scheme of the dynamics of the quantum cartpole problem. In every time\nstep ∆t a force F is applied, the wavefunction is evolved in time and a weak position\nand momentum measurement are performed. These steps are repeated Nmeas times.\nAfterwards, the mean values of the weak measurements xest and pest are passed to a\ncontrol algorithm, which will decide on a value for the stabilizing force F.\ncontrol. As a specific implementation, one can consider these measurements to be performed\nby coupling the quantum cartpole system with an ancilla system for a short period that is then\nprojectively measured (see App. A.2 for a detailed derivation). Without the weak position\n4\nSciPost Physics\nSubmission\nmeasurement, the wavepacket would continue delocalizing irrespective of the unitary force uF.\nFinally, we mention that we have implemented this control problem as an OpenAI Gymnasium\nenvironment [23], making it suitable for reinforcement learning control.\n2.1\nClassical surrogate of the quantum cartpole\nTo be able to compare the aspects of controlling this quantum system versus a classical system,\nwe introduce a noisy (stochastic) classical version whose noise properties are tuned to mimic\nthe uncertainty of weak measurements. For the rest of this work, the noisy classical system\nrefers to the following implementation rather than the original classical cartpole.\nThis classical system is a linear stochastic system, describing a classical particle on an\ninverted potential, given by\nst+1 = Ast + But + wt\n(2)\nyt = Cst + vt ,\n(3)\nwhere st = (xt, ˙xt) is the state vector introduced above, yt describes the results of measure-\nments (and hence the inputs to the controller), and ut is the control vector set by the control\nalgorithm, containing the force applied to the system (playing a role analogous to uF in the\nquantum problem). The matrices A, B, and C describe the dynamics and measurements (see\nApp. A.1), and the vectors wt ∼N (0,σdyn) and vt ∼N (0,σmeas) describe noise on the dy-\nnamics and on the measurements, respectively. That is, wt and vt are normally-distributed\nrandom variables with zero mean and standard deviations σdyn and σmeas, respectively. The\nuncertainty coming from the weak measurements is reflected in vt, and the measurement\nbackaction is reflected through noise on the state, wt. Because the backaction depends on the\nmeasurement outcome, the noise models for vt and wt are correlated (see App. A.3). Also in\nthis case, the particle (now a point-mass) is initialized on top of the inverted potential with a\nrandom momentum chosen from a uniform distribution of width 0.1.\n3\nControl algorithms\nNow that we have a quantum problem and a classical problem that mimics it, we turn our\nattention towards two possible control strategies. In particular, we ask how well an optimal\ncontroller for the classical variant performs on the quantum version, and whether a reinforce-\nment learning controller can go beyond such optimal control in scenarios where the latter\nstruggles.\n3.1\nLinear Quadratic Gaussian Control\nThe well-known linear quadratic Gaussian control (LQGC) algorithm is a classical control algo-\nrithm that is known to optimally control a linear system subjected to Gaussian noise [24,25].\nThe algorithm itself consists of two parts: the Kalman filter [26, 27] (the estimator) and the\nlinear quadratic regulator (LQR) (the controller) [28]. The latter assumes that we can apply\nlinear control, ut = −KLQRxt in Eq. (2), and provides KLQR by minimizing a quadratic cost\nfunction J (see App. A.4.4). The performance of this controller (without the Kalman filter) on\nthe classical problem with a quadratic inverted potential is shown in Fig. 3a for various num-\nbers of measurements and for several values of noise, where the performance is measured in\nterms of ttermination, which is the average number of time steps ∆t until the termination condi-\ntion is met1. There is an intuitive trade-off where more measurements allow for better control\n1In the absence of noise, this system can be stabilized indefinitely.\n5\nSciPost Physics\nSubmission\n(averaging out the noise), but where too many measurements is detrimental since they take\ntoo much time. In that time, the system either reaches the failure condition or goes beyond the\npoint where control is possible (e.g., because a control force |F| > |Fmax| would be required).\nBetter performance can be achieved by incorporating an estimator such as the Kalman\nfilter into the feedback protocol. The Kalman filter provides an estimate of the system’s state\nˆst, by using a model of the underlying dynamics to calculate the most probable state of the\nsystem based on the measurement yt−1, the previous state estimation ˆst−1. Appendix A.4\ndescribes how this is done for a linear system in more detail. Figure 3b shows that when we\nuse the Kalman filter, the trade-off disappears entirely and a single measurement provides the\nbest result. This is true for Markovian systems, for which the current state of the system only\ndepends on the previous state (and not further history), so that knowing the previous state\nprovides all information to provide a good estimate of the current state.\n3.2\nReinforcement Learning Control\nBecause we will move away from the scenario where LQGC is designed to work, we turn our\nattention to reinforcement learned control. A possible advantage of such controllers is that\nthey can learn control without having access to a model and without access to the noise model\n(i.e., without explicitly making use of Eqs. (3)). Hence, being model free and relying only\non measurement results as input, the agents we study can be applied either to the control of\nquantum or classical systems (though performance and optimal parameters may be different\nfor the two cases). A thorough introduction to reinforcement learning control can be found\nin [8], and we mention specifically here that reinforcement learning agents are capable of\nlearning the LQR algorithm [29].\nInspired by LQGC, rather than training a single agent to stabilize the quantum cartpole,\nwe train two distinct agents: one for determining the control force (the reinforcement learned\ncontroller, RLC) and another responsible for state estimation (the reinforcement learned es-\ntimator, RLE). Both agents are trained using a stochastic on-policy training algorithm called\nthe proximal policy optimization (PPO) algorithm [30], and both use continuous input and\noutput spaces. Detailed of the training process, a short description of the PPO algorithm, and\nthe parameters used are listed in Appendix A.5.\nThe first reinforcement learning agent – the reinforcement learning controller (RLC) – is\ntrained with the goal of providing the control input based on the raw measurement inputs.\nThe input to the agent is hence directly xest and pest. As output the agent returns a controlling\nforce uF from the range [−Fmax, Fmax] for the next time step. The reward is −1 if the control\nfails (i.e., the wavefunction moves outside the boundaries), and 0 every time step otherwise.\nTesting the RLC on a classical system with inverse quadratic potential in Fig. 3a, we see that\nit has the same trade-off as the LQR controller, but performs slightly better due to difference\nin the objective in both algorithms. The LQR minimizes the quadratic cost during the run,\nwhereas the RLC aims to avoid the worst case of the wavefunction being pushed out of the\nthreshold.\nThe second agent – reinforcement learned estimator (RLE) – the is trained with the goal of\nreplacing the Kalman filter. To do so, we provide it with the previous state estimate ˆst−1, the\nmean of the Nmeas measurements taken at timestep t and the last control value ut. Compared\nto the Kalman filter, however, the agent has no knowledge about the noise covariance nor of\nthe system’s equations of motion. As output the agent returns the predicted change of the state\n∆ˆst, so that ˆst+1 = st + ∆ˆst. The goal of the training is to minimize the squared prediction\nerror e2\nt , where et = yt −CAst−1 (see App. A.4), by providing a reward rt = −e2\nt in each time\nstep. For the purpose of training, the controller is replaced by a simple random controller\n(choosing a random force every time step), since the estimation task does not depend on the\nactual control strategy.\n6\nSciPost Physics\nSubmission\n0\n10\n20\n30\n40\n50\n# measurements\n400\n600\n800\n1000\n1200\n1400\n1600\nttermination [dt]\na)\nmeas = 0.7\nmeas = 1.0\nRLC\nLQR\n0\n10\n20\n30\n40\n50\n# measurements\n500\n1000\n1500\n2000\n2500\n3000\nttermination [dt]\nb)\nRLC + RLE\nRLC\nLQR + Kalman\nLQR\nFigure 3: Controller performance for noisy classical cartpole (with inverted\nquadratic potential). Plot a) compares the RLC against the LQR for different levels of\nmeasurement noise σmeas (see Eqs 3), showing a comparable increase and decrease\nin performance with increasing number of measurements. The RLC reaches a higher\nmaximum compared to the LQR. In b) the noise level is fixed at σmeas = 0.8 and\nstate estimators in form of the Kalman Filter and RLE are added to the comparison,\nshifting the peak performance to a single measurement.\nPutting the estimator to the test in combination with the RLC on the classical surrogate\nsystem with inverted quadratic potential is shown in Fig. 3b, where, like the LQGC, the per-\nformance always reaches the maximum for Nmeas = 1, independent of the noise level. Overall,\nthe performance of the reinforcement learned estimator is below that of the Kalman filter, in-\ndicating that learning the state estimator is more challenging than learning the controller2 and\nmaking LQGC the better choice if the system is linear and a noise model is available.\n4\nControlling the quantum cartpole\nWe now turn our attention to the quantum version of the problem. In Fig. 4 we show how a\nreinforcement learning controller, tranined on the quantum system, performs in this scenario\n(still with a quadratic inverted potential), once without the estimator (panel a) and then with\n(panel b). Here, too, the trade-off between more measurements for more information versus\nlatency is apparent if only the controller is used. Panel a also shows that control quickly turns\ninfeasible in the weak measurement regime (corresponding to the top of the panel), where the\ntrade-off fades out, but still indicates a finite number of measurements remains optimal. Like\nin the classical case, using the estimator in addition makes it such that the optimum in Nmeas\nshifts to a single measurement as seen in Fig. 3b.\n4.1\nPotential variations\nTo comprehensively explore the capabilities of our controllers, we consider four possible com-\nbinations of controllers (LQR and the RLC) and estimators (Kalman filter and the RLE). We\nexplore how these combinations perform as a function of different numbers of measurements,\n2The training of the estimator agent converged, so the difference in performance is not due to training. More\nlikely is that a more complex agent would be able to learn the system’s dynamics and perform better estimation,\ncompared to the standard PPO agent we chose.\n7\nSciPost Physics\nSubmission\n1\n10\n20\n30\n40\nNmeas\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nancilla\na)\nRLC\nweak\nstrong\nmeasurement\n1\n10\n20\n30\n40\nNmeas\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nancilla\nb)\nRLC + RLE\n103\n104\nttermination\nFigure 4: Controller performance for quantum cartpole with an inverted quadratic\npotential. Shown is the performance of various controllers (measured in the termi-\nnation time ttermination) indicating a trade-off between the number of measurements\nNmeas and the strength of the measurement, indicated by the width σancilla of the mea-\nsurement wavefunction (see Appendix A.2 for details). Larger σancilla corresponds to\nweaker measurements. Panels a) and b) show RLC and RLC + RLE as controllers,\nrespectively, with the heat maps showing the average termination time ttermination on\na logarithmic scale and the white line indicating ttermination = 103.\nevaluating them based on the average time ttermination (calculated over 105 runs). To check\nthe validity and usefulness of the LQGC versus the reinforcement learning controller, we in-\nvestigate different potentials that make the system non-linear. The potentials that we study\nare depicted in Fig. 5, which next to the quadratic inverted potential shows two more:\n(i) A cosine potential V(ˆx) = k1 (cos(πˆx/k2) −1), and\n(ii) An inverted quartic potential V(ˆx) = −kˆx4 .\nThe values for k1, k2 and k are listed in Appendix A.1, which also shows the performance of\nthe controllers on the classical system with these potentials. For the RL based controllers, we\ntrained multiple agents and averaged the results of the 10 best performing ones and presenting\ntheir performance relative to the LQGC performance, to highlight the improvement of the\nperformance, independent from the concrete performance, which depends on the underlying\npotential.\nFor the quadratic inverted potential the combination of reinforcement learning controller\n(RLC) and the Kalman filter performs as well as the LQGC algorithm (see panel a of Fig. 6).\nThis is a notable result, because the LQGC is not a guaranteed optimal controller in the quan-\ntum environment. However, our findings are consistent with those presented for discrete con-\ntrol of the quantum cartpole [7]. At the same time, we note that the reinforcement learned es-\ntimator (RLE) struggles to match the performance of the Kalman filter up to about Nmeas ∼40,\nand that does not remove the trade-off behavior discussed previously. For larger Nmeas the RLE\nseems to be able to match the Kalman performance.\nNow going to a nonlinear system, starting with the cosine potential Fig. 6b, the overall\nperformances are similar to the linear system. It is notable to see that the combination RLC +\nKalman is now able to gain a notable advantage over the LQGC, increasing the performance\nby ∼10% for a single measurement Nmeas = 1. Similarly, the controllers involving RLE were\nable to close the performance gap to the LQGC by small margins, but remain far behind.\n8\nSciPost Physics\nSubmission\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0x\n140\n120\n100\n80\n60\n40\n20\n0\n20\nV(x)\nxth\nxth\nquadratic\nquartic\ncosine\nFigure 5: The different potentials used for the quantum cartpole problem. The\ncosine and quartic (red and green) potentials are used to demonstrate behavior on\nnonlinear systems, while the quadratic (blue) potential is used for a linear system.\nIt is for the quartic potential that the advantage of using RL becomes really evident. Here\nthe RLC + Kalman controller is able to achieve an increase in performance of ∼60% compared\nto the LQGC. At the same, we also see that both the RLC + RLE and the LQR + RLE controllers\nare both able to also achieve a performance advantage over the LQGC and narrow the gap\nwith the RLC + Kalman controller, indicating that the LQR algorithm is the main bottleneck.\nOn a broader level, we expect that for even more non-linear problems reinforcement learning\ncontrol indeed becomes the go-to choice instead of LQGC.\n0\n20\n40\n# weak measurements\n0.5\n1.0\n1.5\nttermination/tLQR + Kalman\nquadratic  a)\nRLC + Kalman\nRLC + RLE\nRLC + Kalman transfer\nRLC + RLE transfer\nLQR + RLE\n0\n20\n40\n# weak measurements\ncosine  b)\n0\n20\n40\n# weak measurements\nquartic  c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nttermination/tLQR + Kalman\nFigure 6: Benchmarks of the various controllers, on the quantum system depend-\ning on the number of measurements for the input. We showcase the performance of\nthe Kalman Filter + LQR (blue) and the pure reinforcement learning controller (light\nred), as well as the mix of those controllers with Kalman Filter + RLC (red) and LQR\n+ RLE (light blue). Additionally, transferred RLC + Kalman Filter and RLC + RLE\n(black) are presented, which were trained on the classical system and then applied\non the quantum system. The performance is showcased as ratio of the average ter-\nmination time between a selected controller and the Kalman + LQR controller. Each\nplot represents a different potential, the first being the quadratic potential, followed\nby the cosine potential and quartic potential.\n9\nSciPost Physics\nSubmission\n4.2\nTransfer learning\nFinally, we consider a transfer learning scenario in which we train a reinforcement learning\nagent on the classical surrogate, and then apply it to control the quantum system. The results\nof these comparisons for the different combinations of controllers and estimators are shown\nin Fig. 6. Interestingly, RL agents trained on the classical system perform almost identically to\nthose trained on the quantum system (comparing the ‘transfer’ controllers with their counter-\nparts). This suggests that training on a classical surrogate model for controlling the quantum\ncartpole is indeed a viable strategy.\n4.3\nController characteristics\nTo further elucidate the control strategies used by the control algorithms, we investigate the\nresulting distributions of position 〈x〉and momentum 〈p〉expectation values, shown in Fig. 7.\nFor this we compare the LQGC and the RLC + RLE controllers on the three different potentials.\nThe distributions were taken by collecting the position and momentum of the wavefunction\nover 106 time steps. In order to avoid measurement artifacts from the initilization of the\nwavefuntions, only data from t = 300 and onwards were taken.\nLooking first at the LQGC, it can be observed that the distributions for all potentials are\nsymmetrical and centered around 0, showing that the controller aims to stabilize the wave-\nfunction at the centers of the potentials. When comparing the quadratic and cosine potentials,\nit is notable that the cosine potential has a wider distribution due to the fact that it starts to flat-\nten out near the threshold. It appears that this allows the controller to stabilize it closer to the\nthreshold for longer durations. In contrast, the quartic potential has the sharpest distribution,\nsuggesting that it is unable to recover the wavefunction when it is close to the thresholds.\nLooking at the distributions of the full reinforcement learning control algorithm (RLE +\nRLC) one notable disparity is observed. The distributions are neither centered around 0 nor\nare they symmetric. This is attributed to the training process, where the agent can develop\na bias for stabilizing the wavefunction at a particular point. This is particularly evident in\nthe quadratic and quartic potentials, where the wavefunction is stabilized left and right of the\ncenter.\n5\n0\n5\nx\np( x )\nI)\nLQR + Kalman\nRLC + RLE\n5\n0\n5\nx\nII)\n5\n0\n5\nx\nIII)\np\np( p )\np\np( p )\np\np( p )\nFigure 7:\nDistributions of the position and momentum of the stabilized\nwavepacket on the quantum system for the three different potentials: (I) quadratic,\n(II) cosine, (III) quartic. The position 〈x〉of the wavefunction was tracked over 106\ntimesteps for the LQR + Kalman controller (blue) as well as the RLC + RLE (red)\ncontroller and converted to a histogram of the position probability p (〈x〉). This is\nalso showcased for the momentum 〈p〉in the top right insets.\nFor the cosine potential, the distribution of the average position with RLC + RLE signif-\nicantly differs from that of the LQGC. Instead of a clear peak in the distribution a broader\nplateau appears, indicating that the controller has learned to balance the wavefunction on the\nside of the potential rather than the center.\n10\nSciPost Physics\nSubmission\n5\nConclusions and Outlook\nStandard classical feedback-based control algorithms are challenged by quantum systems, due\nto their intrinsic measurement induced backaction and partial observability. However, for\nquantum systems with observables that can approximately be described by linear stochastic\nequations, we found that the classical LQGC algorithm performs well if full knowledge of the\nmodel as well as its noise characteristics are available. If such knowledge is not available,\nor if the system is non-linear, the classical controller is found to struggle. In scenarios where\nknowledge of the model and the noise are unavailable (as is often of the case for complex\nexperiments), and/or where the system is non-linear, we showed that a model-free reinforce-\nment learned controller outperforms the LQGC algorithm.\nTo fairly demonstrate the advantage of our reinforcement learning controller, we con-\nstructed a surrogate model in the form of a classical stochastic system, whose properties are\ndesigned to closely mimic the measurement-induced backaction of the quantum cartpole. Us-\ning this classical surrogate, we demonstrate that transfer learning is feasible by training the\nRL agent on this classical model and applying it to controlling the true quantum system. This\nopens up the possibility of more efficient training methods.\nBoth the LQGC algorithm as well as our reinforcement learned controller are composed\nof a separate estimator and a controller, and we show that without the estimator a trade-off\nbetween the number of measurements and the controllabilty exists. Including the estimators\nremoves this trade-off, allowing for optimized control with just a single (weak) measurement.\nFor the LQGC algorithm, estimating the state (with the Kalman filter) again requires knowledge\nof the model and the noise characteristics. The reinforcement learned estimator struggles\nto match the performance of the Kalman filter, but still ensures controllability with single\nmeasurements. Frame-stacking techniques improve upon this further.\nFinally, we found that when analyzing the control strategies for the linear system, the rein-\nforcement learning controller learns a strategy similar to the optimal strategy of the LQGC. For\nthe non-linear case, the reinforcement learning controller manages to outperform the LQGC\nalgorithm by stabilizing the system with a different control strategy that allows for a broader\ndistribution of the system’s momentum.\nIt would be an interesting future direction to focus on making the RL agent more au-\ntonomous, by having it choose when and how strong to perform the weak measurements.\nThis would result in an adaptive algorithm that may learn to only measure when necessary.\nSimilarly, instead of frame-stacking the agent could learn to use the raw weak measurement\noutputs instead, instead of only their average. Finally, future work could focus on extending\nthe system in interesting directions such as time-varying potentials, non-Markovian noise, or\ninteracting systems.\nAcknowledgements\nFunding information\nK. M. thanks the Niels Bohr Institute for hospitality during the initial\nstages of this work. S.T. thanks A. Sengupta for inspiring discussions. We acknowledge partial\nfunding through the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)\n– Projektnummer 277101999 – TRR 183 (projects A01, B01). The numerical simulations have\nbeen performed on the JUWELS Booster at FZ Julich. This work also recieved funding from\nthe Dutch National Growth Fund (NGF) as part of the Quantum Delta NL programme. M.R.\nacknowledges the Brown Investigator Award, a program of the Brown Science Foundation, the\nUniversity of Washington College of Arts and Sciences, and the Kenneth K. Young Memorial\nProfessorship for support.\n11\nSciPost Physics\nSubmission\nData availability\nThe reinforcement learning environment [31] (in the form of an OpenAI\nGymnasium) as well as the code and all configuration data used for obtaining the benchmarks\nin this paper [32], are made available.\nA\nAppendix\nA.1\nSystem parameters\nFor the simulation in order to make fair comparison between the different potential, we have\nused the same initialization parameters on the on all three potentials, with the exception of\nthe potential constant k.\nTable 1: Parametrization of the environment used for the benchmarks. Including\nthe parameters of the potentials, the time evolution, and the weak measurements.\nAll units are set to 1.\nSystem Parameters\nparameter\nquadratic\ncosine\nquartic\nk\nπ\n67\nπ\n100\n∆t\n0.01\nπ\n0.01\nπ\n0.01\nπ\nm\n1\nπ\n1\nπ\n1\nπ\nλ\n0.05\n0.05\n0.05\nσsystem\n1.0\n1.0\n1.0\nσancilla\n0.7\n0.7\n0.7\n〈p2〉init\n0.1\n0.1\n0.1\nxth\n8\n8\n8\nFmax\n8π\n8π\n8π\nA.2\nWeak Measurement\nWe want to perform the weak measurement on the quantum state |Ψ〉= |ψ〉⊗|φ〉, where\n|ψ〉is the system state and |φ〉the ancilla state. The two systems interact via the Hamilto-\nnian Hint = A ⊗p. We assume that the interaction time δt is small enough, thus the time\nevolution is dominated by the weak measurement.\nThe time evolution can be written as\n|Ψ′〉= U(t)|ψ〉⊗|φ〉with :\nU(t) = exp[−iλHint],\n(4)\nwith λ = sδt and s being the interaction strength and ∆t the interaction time. Here A is a\nHermitian operator with the eigenstates α, acting on the system quantum state ψ, and p is the\nmomentum operator acting on the ancilla state φ.\nChoosing the form |φ(q)〉=\n1\n(2πσ2)(1/4)\nR\ndq′exp[−q′2/(4σ2)]|q′〉for the ancilla state, per-\nform a projective measurement in the q state using\nQ\nq = I ⊗|q〉〈q|. This leaves the state in\nthe form |Ψqm〉=\nMqm|Ψ′〉\nN\n, and returns the measured quantity qm and the Kraus operator\nMqm =\n1\n(2πσ2)(1/4)\nZ ∞\n−∞\ndα exp[−(qm −λα)2/(4σ2)]|α〉〈α|,\n(5)\nwhich is a Gaussian weighted sum of projectors onto the eigenstates of A.\n12\nSciPost Physics\nSubmission\nFrom the Kraus operator the probability density of the measurement follows as\nP (q) = Tr\n\u0002\nM (q)† M (q)|ψ〉〈ψ|\n\u0003\n(6)\n=\n1\np\n2πσ\nZ ∞\n−∞\ndα|ψ(α)|2e−(λα−q)2/(2σ2) .\n(7)\nNext we want to calculate the uncertainty of the measurement σq = 〈q2〉−〈q〉2 using the\nprobability density. Starting with the expectation value of q,\n〈q〉=\nZ ∞\n−∞\ndqqP (q)\n(8)\n=\n1\np\n2πσ\nZ ∞\n−∞\nZ ∞\n−∞\ndqdαq|ψ(α)|2e−(λα−q)2/(2σ2)\n(9)\n=\nZ ∞\n−∞\ndα|ψ(α)|2λα\n(10)\n= λ〈A〉,\n(11)\none can see that the expectation value of the measurement is given by the expectation value\nof the operator A. If we assume that ancilla wavefunction is much broader than the system\nwavefunction, we can approximate |ψ(α)|2 with a delta function.\nP(q) ≈\n1\n(2πσ2)(1/2)\nZ ∞\n−∞\ndαδ(α′ −〈A〉)exp[−(q −λα)2/(2σ2)]\n=\n1\n(2πσ2)(1/2) exp[−(q −λ〈A〉)2/(2σ2)].\nThis allows us to write the measurement result q as a stochastic quantity\nq = λ〈A〉+ ∆W ,\n(12)\nwhere ∆W is a zero-mean Gaussian random variable with a variance σ2.\nThe same derivation can be done for 〈q2〉.\n〈q2〉=\nZ ∞\n−∞\ndqq2P (q)\n(13)\n=\n1\np\n2πσ\nZ ∞\n−∞\nZ ∞\n−∞\ndqdαq2|ψ(α)|2e−(λα−q)2/(2σ2)\n(14)\n=\nZ ∞\n−∞\ndα|ψ(α)|2 \u0002\n2σ2 + λ2α2\u0003\n(15)\n= 2σ2 + λ2〈A2〉.\n(16)\nPutting both together yields the uncertainty of the measurement\nσ2\nq = λ2〈α2〉+ 2σ2 −λ2〈α〉2\n(17)\n= λ2σ2\nα + 2σ2 .\n(18)\nFrom this we can see that in the case λ = 1 and σ →0, the uncertainty of the measurement\nreduces to the uncertainty of the system wavefunction, recovering strong measurement. This\n13\nSciPost Physics\nSubmission\nalso translates to the uncertainty relation assuming the additional measurement using the\noperator B with the eigenstates β and that A & B do not commute, giving us the relation\nσ2\nqσ2\np = σ2\nασ2\nβ ≥1\n4 .\n(19)\nLooking also in the other case with λ = 0, we have no interaction between the system and\nancilla wavefunction and the uncertainty of a measurement reduces to σ2\nq = 2σ2 the variance\nof the ancilla wavefunction.\nA.3\nNoise determination of classical system\nThe measurement and system noise of the classical system are defined by\nE\n\u0014\u0012\nwt\nvt\n\u0013\u0000wT\nt′\nvT\nt′\n\u0001\u0015\n=\n\u0012\nQ\nS\nST\nR\n\u0013\nδtt′ ,\n(20)\nwhere Q,R are the covariance matrices of the system and measurement noise, respectively, and\nthe S is the cross-covariance matrix between those two. Those matrices are chosen to mimic\nthe quantum system as closely as possible.\nBased on Eq. 12, the measurement noise covariance matrix follows directly as:\nQ =\n\u0014\nσ2\nancilla\n0\n0\nσ2\nancilla\n\u0015\n.\n(21)\nThe covariance matrix R of the system noise and the cross-covariance matrix S depend on the\nwidth σsys of the system wavefunction, which is varying around a fixed value during a run.\nThe fixed value also depends on the underlying potential, so that the noise matrices change\ndepending on the potential. Because of that we numerically determined the values to be as\nclose as possible to the weak measurement back actions. This is done by running the quantum\nsystem for a large number of steps and extracting the measurement and system noise added\nby the weak measurements over 106 timesteps. The matrices are then given by:\nR =\n\u0014\ncov(x, x)\ncov(x, p)\ncov(p, x)\ncov(p, p)\n\u0015\n,\nS =\n\u0014\ncov(x, xmeas)\ncov(x, pmeas)\ncov(p, xmeas)\ncov(p, pmeas)\n\u0015\n.\n(22)\nA.4\nLinear Quadratic Gaussian Controller LQGC\nThe LQGC is an optimal control algorithm for linear systems subject to Gauss noise. It is com-\nposed of the Kalman filter (or linear quadratic estimator LQE), which is a recursive estimator\nusing a time series of measurements to approximate the unknown variables, and the linear\nquadratic regulator quadratic regulator (LQR), which converts the estimated values into an\napplicable force.\nA.4.1\nKalman Filter\nAssume we have a linear system, which can be described by the equations\nst+1 = Ast + But + wt\nyt = Cst + vt ,\nwhere ut is the known input at the timestep t and wt.vt are the process and measurement\nnoise respectively. It is assumed that the noises can be described as zero meas Gaussian noises\nwith the covariances Q,R.\n14\nSciPost Physics\nSubmission\nIn order to predict the next step, only the estimation from the previous timestep and the\nmeasurement from the current timestep are needed. The state of the filter can be described\nusing the a posteriori state estimate mean at the time t, including measurement up to the time\nt′, ˆst|t′, and the a posteriori estimate covariance matrix, Pt|t′, which is used as a measure for\nthe accuracy of the state estimation.\nFirstly the Kalman filter predicts the next state of the estimation, by updating the last state as\nif process noise is applied\nˆst+1|t = Aˆst|t + But\nPt+1|t = APt|tAT + Q .\nNext the state has to be corrected using the latest measurement yt+1 by calculating the differ-\nence between the measurement and the optimal forecast of the estimated state:\nˆzt+1 = yt+1 −Cˆst+1|t\n(23)\nSt+1 = CPt+1|tC T + R,\n(24)\nwhere St+1 is the covariance of yt+1. From this the Kalman gain K follows as\nKk+1 = Pt+1|tC TS−1\nk+1\n(25)\nand the state estimation and covariance can be updated as\nˆst+1|t+1 = ˆxt+1|t + Kk+1ˆzk+1 ,\n(26)\nPt+1|t+1 = (I −Kt+1C)Pt+1|t .\n(27)\nBased on the estimation we can now also define the prediction error\net+1 = yt+1 −Cˆst+1|t+1 .\n(28)\nA.4.2\nKalman Filter with same time step correlated noise\nNormally it is assumed that the linear system has uncorrelated process and measurement noise,\nbut in the case of weak measurement, we have correlation between both noise types. To\naccommodate this correlation, we rewrite the system equation, to be of uncorrelated noise,\nfollowing the derivation from [33]\nUsing an arbitrary matrix T, we transform the system to\nst+1 = Ast + But + wt + T[yt −Cst −vt]\n= (A−TC)st + But + wt + T yt −T vt\n= A∗st + u∗\nt −w∗\nt ,\nwith the new transition matrix A∗= (A−TC), new known input u∗\nt = But + T yt and the new\nnoise w∗\nt = wt + T vt.\nNow we choose the matrix T by setting the correlation between the new system noise and the\nmeasurement noise to zero\nE[w∗\nt v′\nt] = E[[wt + T vt]v′\nt] = S −TR = 0,\n(29)\nwhich yields\nT = SR−1 .\n(30)\nFrom this follows the covariance of the new process noise as:\nQ∗= Q −SR−1S′ .\n(31)\nUsing the new state equation and covariance, the Kalman gain can be calculated normally.\n15\nSciPost Physics\nSubmission\nA.4.3\nExtended Kalman Filter\nIn the case that the system is described by the non linear functions f and h\nst+1 = f (st,ut) + wt\nyt = h(st) + vt ,\nit is necessary for the Kalman filter to linearize the current estimate and covariance. This\nmodel is called the extended Kalman Filter [33]. While the overall strategy of the filter stays\nthe same, there are some differences. We start by linearizing the system, with\nA(t) = ∂f\n∂s |ˆst,ut ,\n(32)\nC(t) = ∂h\n∂s |ˆst ,\n(33)\n(34)\nwhere A, C are now the Jacobians of their respective functions. From the extended Kalman\nfunctions follows from the same process as the standard Kalman:\nˆst+1|t = f (st,ut)\nPt+1|t = APt|tAT + Q\nˆzt+1 = yt+1 −c(ˆst+1|t)\nSt+1 = CPt+1|tC T + R\nKk+1 = Pt+1|tC TS−1\nk+1\nˆst+1|t+1 = ˆst+1|t + Kk+1ˆzk+1\nPt+1|t+1 = (I −Kt+1C)Pt+1|t .\nA.4.4\nLinear quadratic regulator\nThe second part of the LQGC consists of the linear quadratic regulator LQR, which is defined\nfor a linear system by\nst+1 = Ast + But .\n(35)\nIn the case of inverted quadratic potential, we can use the A, B from the state equations,\nwhereas for the inverse quartic potential, we instead use the Jacboian matrices of the state\nequations.\nThe LQR should return a controller\nut = −Kst\n(36)\nthat minimizes the quadratic cost\nJ =\nX\nt\n\u0000sT\nt W1st + uT\nt W2ut\n\u0001\n,\n(37)\nwhere Q, R are the weight matrices of the cost functions. Here we assume that applying\ncontrols does not generate any cost and set W2 = 0. The W1 weight matrix is modeled so that\nsT\nt W1st = H and therefore the quadratic cost represents the energy of the system.\nUsing this approach we get the following weight matrix W1 for the inverted quadratic potential,\nWquadratic =\n\u0014 k\n2\n0\n0\n1\n2m\n\u0015\n.\n(38)\n16\nSciPost Physics\nSubmission\n10\n20\n30\n40\n# measurements\n2000\n4000\n6000\n8000\n10000\n12000\nttermination\na)\n0\n50\n100\n150\n200\n250\n300\ntraining epoch \n4500\n4000\n3500\n3000\n2500\n2000\n1500\n1000\nreward\nb)\n1\n6\n12\n18\n24\n30\n36\n42\n48\nperformance order\nFigure 8: Visualization of the training results for training agents on the classical\nsystem with inverse quartic potential. In a) the average termination time of all agents\nafter the training is shown against the number of measurements performed. For every\nmeasurement number 48 different agent are shown, and are color coded depending\nto rank their performance. In b) the process of training a RLE agent is showcased,\nwhere the reward are plotted against the first 300 training epochs.\nIn case that we have a nonlinear system, we approximate the dynamics according to Eq. 32\nand set the weights to be sT\nt W1st = H|st:\nWcosine =\n\u0014 k1(cos(πˆx/k2)−1)\ns2\n0\n0\n1\n2m\n\u0015\n,\nWquartic =\n\u0014\nks2\n0\n0\n1\n2m\n\u0015\n,\n(39)\nwhere we evaluate the Wcosine and Wquart every timestep, according to the latest state estima-\ntion ˆst.\nA.5\nReinforcement learning training\nThe PPO algorithm trains a policy πθ from which the actions a are sampled allowing for\nexploration in the training. The policy is updated by maximizing the objective function L\nθk+1 = argmaxθ Es,a≈πθk [L(s, a,θ,θk)] ,\n(40)\nwith L:\nL(s, a,θ,θk) = min\n\u0012\nπθ(a,s)\nπθk(a,s)Aπθk (s, a), g(ε,Aπθk (s, a))\n\u0013\n.\n(41)\nSince reinforcement learning is known to be vulnerable to performance collapse [34, 35],\ncaused by a few unfortunate episodes in the training, the PPO limits how far any new pol-\nicy is allowed to differ from the previous one, by clipping the probability ratio πθ (a,s)\nπθk(a,s) in the\nobjective function\ng(ε,A) =\n¨\n(1 + ε)A\nA ≥0\n(1 −ε)A\nA < 0 ,\n(42)\nwith ε controlling the clipping range. The new policy does not benefit by going far away from\nthe old policy.\nIn the training of the reinforcement learning models, we used 2 different sets of hyperpa-\nrameters. One for the training of the RLC models and one for the RLE.\n17\nSciPost Physics\nSubmission\nTable 2: Hyperparameters of the reinforcement learning approach, specifying the\ninitialisation of the RLC and RLE agents and their respective training process.\nReinforcement Learning Parameters\nParameter\nRLE\nRLC\nlearning rate\n2e-05 ·Nmeas\n3e-05\nclipping rate\n0.7\n0.5\nepochs\n1000\n10000\nsteps per epoch\n100000\n200000\nbatchsize\n2048\n1024\nnepochs\n20\n10\naction network\n[32, 32]\n[32, 32]\nvalue network\n[32, 32]\n[32, 32]\nactivation function\ntanh\ntanh\nThe training of the RLE and RLC agents is done by utilizing the same underlying methods.\nThere is a variance in the used hyperparameters, which were determined using grid search\nand are listed in the Tab. 2. During the training we tracked the returned reward after each\nepoch and saved the models, which returned the highest reward.\nThe training of the RLC agent turned out be easily affected by getting stuck in local min-\nima, completely halting the learning process. Since this more often happened at small number\nof weak measurements, we have utilized transfer learning [36] to circumvent this problem.\nIn transfer learning we trained 48 agents on Nmeas = 48 weak measurements. These agents\nwere then used as the starting point for training with Nmeas = 47 weak measurements. This\nwas repeated for each number of weak measurements until Nmeas = 1 is reached. The trained\nagents have then been evaluated on the potential as shown in 8 a) for the Kalman FIlter +\nRLC controller on a classical system with an inverted quartic potential. At the starting point\nof the training at Nmeas = 48, the agents all show a performance close to on another, but as\n0\n10\n20\n30\n40\n50\n# measurements\n0\n50000\n100000\n150000\n200000\nttermination [dt]\nRLC + RLE\nRLC\nLQR + Kalman\nLQR\nFigure 9: Comparison of RL vs LQR & Kalman Filter on the classical cartpole (with\ninverse quadratic system). The noise level is fixed at σmeas = 0.5 and state estimators\nin form of the Kalman Filter and RLE are added towards the comparison.\n18\nSciPost Physics\nSubmission\n0\n20\n40\n# measurements\n1\n2\n3\nttermination/tLQR + Kalman\nquadratic  a)\nRLC + Kalman\nRLC + RLE\nLQR + RLE\n0\n20\n40\n# measurements\ncosine  b)\n0\n20\n40\n# measurements\nquartic  c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nttermination/tLQR + Kalman\nFigure 10: Benchmarks of the various controllers, on the classical system depend-\ning on the number of measurements for the input. We showcase the performance\nof classical controller using Kalman Filter + LQR (blue) and the pure reinforcement\nlearning controller (light red), as well as the mix of classical and reinforcement learn-\ning controllers with Kalman Filter + RLC (red) and RLE + LQR (light blue). The\nperformance is showcased as ratio of the average termination time between a se-\nlected controller and the Kalman + LQR controller. Each plot represents a different\npotential, the first being the quadratic potential, followed by the cosine potential\nand quartic potential.\nthe transfer learning continues, at around Nmeas = 20, the performance of the agents starts\nto diverge. The majority of the agents exhibits increasing performance with the number of\nmeasurements, while a small number of agents get stuck during the training and only show a\nfraction of the performance of other agents. THis shows the difficulty of the training process.\nBecause the training of the RLE isn’t directly depending on the average termination time of\nthe wavefunction, we don’t need to make use of transfer learning and can train the 48 agents\ndirectly, independent from each other, one agent for every number of measurements we per-\nform. In Fig. 8 b) the training process of the RLE agent for Nmeas = 1 in the classical system\nwith inverse quartic potential is shown, demonstrating a fast converging in the training.\nA.6\nExtended classical benchmarks\nIn Fig. 9 the performance of adding an estimator to the RLC and LQR is showcased. Compared\nto Fig. 3 the noise level is fixed at σmeas = 0.7, resulting in higher overall performance for all\ncontroller. Furthermore the performance gap has widened while using an estimator, with RLC\n+ RLE achieving a higher peak performance by a factor of ≈5 compared to the RLC controller.\nPerforming the extended benchmarks on the classical surrogate model yields results similar\nto the corresponding quantum test. Fig. 10 illustrates that, once again, the Kalman + RLC\nmodel matches the performance of the LQGC for the quadratic and cosine potential. Only in\nthe case of the quartic potential is a performance advantage observed, with the performance\nincreasing by a factor ≥3. Those results showcase a greater performance increase than that\non the quantum environment.\nThe controller combinations involving the RLE also only show a performance advantage\nin the quartic potential. Compared to the quanutm version, here the pure RL controller (RLC\n+ RLE) is able the showcase almost the same performance as RLC + Kalman.\nLooking closer at the controller behaviour in the classical case, one can see in Fig. 11,\nthat the general observations from Fig. 4 remain valid. The differences are that the position\ndistribution of the RL controller in the cosine potential now displays a distinct peak around\n19\nSciPost Physics\nSubmission\n5\n0\n5\nI)\nLQR + Kalman\nRLC + RLE\n5\n0\n5\nII)\n5\n0\n5\nx\nIII)\np\np( p )\np\np( p )\np\np( p )\nFigure 11:\nDistributions of the position and momentum of the stabilized\nwavepacket on the classical system.\nThe position 〈x〉of the wavefunction was\ntracked over 106 timesteps for the LQR + Kalman controller (blue) as well as the RLC\n+ RLE (red) controller and converted to histogram of the position probability p (〈x〉).\nThis is also showcased for the momentum 〈p〉in the top right inserts. The three plots\nrepresent measurement from different potentials, those being the quadratic (I), co-\nsine (II) and quartic (III) potential.\nthe center of the potential, instead of having a plateau. Moreover, the position distribution\nof the LQGC controller in the quartic case actually loses its single peak and is replaced by a\nsymmetrical double peak located around the center of the potential.\nReferences\n[1] A. P. Peirce, M. A. Dahleh and H. Rabitz, Optimal control of quantum-mechanical systems:\nExistence, numerical approximation, and applications,\nPhys. Rev. A 37, 4950 (1988),\ndoi:10.1103/PhysRevA.37.4950.\n[2] P. Doria, T. Calarco and S. Montangero, Optimal control technique for many-body quantum\ndynamics, Phys. Rev. Lett. 106, 190501 (2011), doi:10.1103/PhysRevLett.106.190501.\n[3] J. Werschnik and E. K. U. Gross,\nQuantum optimal control theory,\nJournal of\nPhysics B: Atomic, Molecular and Optical Physics 40, R175 (2007), doi:10.1088/0953-\n4075/40/18/R01.\n[4] A. C. Doherty and K. Jacobs, Feedback control of quantum systems using continuous state\nestimation, Phys. Rev. A 60, 2700 (1999), doi:10.1103/PhysRevA.60.2700.\n[5] I. M. Georgescu, S. Ashhab and F. Nori, Quantum simulation, Rev. Mod. Phys. 86, 153\n(2014), doi:10.1103/RevModPhys.86.153.\n[6] H. Mabuchi and A. C. Doherty, Cavity Quantum Electrodynamics: Coherence in Context,\nScience 298, 1372 (2002), doi:10.1126/science.1078446.\n[7] Z. T. Wang, Y. Ashida and M. Ueda, Deep Reinforcement Learning Control of Quantum\nCartpoles, Phys. Rev. Lett. 125, 100401 (2020), doi:10.1103/PhysRevLett.125.100401.\n[8] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, The MIT Press,\nsecond edn., URL http://incompleteideas.net/book/the-book-2nd.html (2018).\n[9] M. Y. Niu, S. Boixo, V. N. Smelyanskiy and H. Neven,\nUniversal quantum control\nthrough deep reinforcement learning,\nnpj Quantum Information 5(1), 33 (2019),\ndoi:10.1038/s41534-019-0141-3.\n20\nSciPost Physics\nSubmission\n[10] M. Bukov, A. G. R. Day, D. Sels, P. Weinberg, A. Polkovnikov and P. Mehta, Reinforce-\nment Learning in Different Phases of Quantum Control, Phys. Rev. X 8, 031086 (2018),\ndoi:10.1103/PhysRevX.8.031086.\n[11] E. Zahedinejad, S. Schirmer and B. C. Sanders, Evolutionary algorithms for hard quantum\ncontrol, Phys. Rev. A 90, 032310 (2014), doi:10.1103/PhysRevA.90.032310.\n[12] M. Dalgaard, F. Motzoi, J. J. Sørensen and J. Sherson, Global optimization of quantum\ndynamics with alphazero deep exploration, npj Quantum Information 6(1), 6 (2020),\ndoi:10.1038/s41534-019-0241-0.\n[13] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel and N. Friis, Optimizing Quantum Error\nCorrection Codes with Reinforcement Learning, Quantum 3, 215 (2019), doi:10.22331/q-\n2019-12-16-215.\n[14] R. Sweke, M. S. Kesselring, E. P. L. van Nieuwenburg and J. Eisert, Reinforcement learn-\ning decoders for fault-tolerant quantum computation,\nMachine Learning: Science and\nTechnology 2(2), 025005 (2020), doi:10.1088/2632-2153/abc609.\n[15] T. Fösel,\nP. Tighineanu,\nT. Weiss and F. Marquardt,\nReinforcement learning\nwith neural networks for quantum feedback,\nPhys. Rev. X 8, 031084 (2018),\ndoi:10.1103/PhysRevX.8.031084.\n[16] G. Carleo, K. Choo, D. Hofmann, J. E. Smith, T. Westerhout, F. Alet, E. J. Davis,\nS. Efthymiou, I. Glasser, S.-H. Lin, M. Mauri, G. Mazzola et al., SoftwareX 10, 100311\n(2019), doi:10.1016/j.softx.2019.100311.\n[17] C. Sommer, M. Asjad and C. Genes, Prospects of reinforcement learning for the simul-\ntaneous damping of many mechanical modes,\nScientific Reports 10(1), 2623 (2020),\ndoi:10.1038/s41598-020-59435-z.\n[18] S. Borah, B. Sarma, M. Kewming, G. J. Milburn and J. Twamley, Measurement-based\nfeedback quantum control with deep reinforcement learning for a double-well nonlinear\npotential, Phys. Rev. Lett. 127, 190403 (2021), doi:10.1103/PhysRevLett.127.190403.\n[19] R. Porotti, A. Essig, B. Huard and F. Marquardt, Deep Reinforcement Learning for Quan-\ntum State Preparation with Weak Nonlinear Measurements,\nQuantum 6, 747 (2022),\ndoi:10.22331/q-2022-06-28-747.\n[20] D. Michie and R. A. Chambers, Boxes: An experiment in adaptive control, URL https:\n//api.semanticscholar.org/CorpusID:18229198 (2013).\n[21] Y. Aharonov and L. Vaidman, Properties of a quantum system during the time interval\nbetween two measurements, Phys. Rev. A 41, 11 (1990), doi:10.1103/PhysRevA.41.11.\n[22] V. Mnih,\nK. Kavukcuoglu,\nD. Silver,\nA. A. Rusu,\nJ. Veness,\nM. G. Bellemare,\nA. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie et al.,\nHuman-level control through deep reinforcement learning,\nNature 518, 529 (2015),\ndoi:10.1038/nature14236.\n[23] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang and\nW. Zaremba, Openai gym, doi:10.48550/arXiv.1606.01540 (2016), arXiv:1606.01540.\n[24] A. Lindquist, On feedback control of linear stochastic systems, SIAM Journal on Control\n11(2), 323 (1973), doi:10.1137/0311025.\n21\nSciPost Physics\nSubmission\n[25] T. T. Georgiou and A. Lindquist, The separation principle in stochastic control, redux, IEEE\nTransactions on Automatic Control 58, 2481 (2013), doi:10.1109/TAC.2013.2259207.\n[26] R. E. Kalman, A New Approach to Linear Filtering and Prediction Problems, Journal of\nBasic Engineering 82, 35 (1960), doi:10.1115/1.3662552.\n[27] J. Humpherys, P. Redd and J. West, A fresh look at the kalman filter, SIAM Review 54,\n801 (2012), doi:10.1137/100799666.\n[28] R. E. Kálmán,\nContributions to the theory of optimal control,\nURL https://api.\nsemanticscholar.org/CorpusID:845554 (1960).\n[29] S. Bradtke, Reinforcement learning applied to linear quadratic regulation, In S. Hanson,\nJ. Cowan and C. Giles, eds., Advances in Neural Information Processing Systems, vol. 5.\nMorgan-Kaufmann, URL https://proceedings.neurips.cc/paper_files/paper/1992/file/\n19bc916108fc6938f52cb96f7e087941-Paper.pdf (1992).\n[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov, Proximal policy optimiza-\ntion algorithms (2017), doi:10.48550/arXiv.1707.06347, 1707.06347.\n[31] K. Meinerz, Quantum cartpole environment, doi:10.5281/zenodo.10060570, URL https:\n//doi.org/10.5281/zenodo.10060570 (2023).\n[32] K. Meinerz, S. Trebst, M. Rudner and E. van Nieuwenburg, Dataset underpinning: \"The\nQuantum Cartpole\", doi:https://doi.org/10.5281/zenodo.10036492 (2023).\n[33] Y. Bar-Shalom, X. Li and T. Kirubarajan, Estimation with applications to tracking and\nnavigation: Theory, algorithms and software,\nURL https://api.semanticscholar.org/\nCorpusID:108666793 (2001).\n[34] V. Dasagi, J. Bruce, T. Peynot and J. Leitner, Ctrl-z: Recovering from instability in rein-\nforcement learning, doi:10.48550/arXiv.1910.03732 (2019), 1910.03732.\n[35] S. Thrun and A. Schwartz, Issues in using function approximation for reinforcement learn-\ning, URL https://api.semanticscholar.org/CorpusID:1115058 (1999).\n[36] T. G. Karimpanal and R. Bouffanais,\nSelf-organizing maps for storage and trans-\nfer of knowledge in reinforcement learning,\nAdaptive Behavior 27(2), 111 (2019),\ndoi:10.1177/1059712318818568.\n22\n",
  "categories": [
    "quant-ph"
  ],
  "published": "2023-11-01",
  "updated": "2024-03-10"
}