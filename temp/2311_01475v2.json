{
  "id": "http://arxiv.org/abs/2311.01475v2",
  "title": "Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts",
  "authors": [
    "Isaac Wasserman",
    "Jeova Farias Sales Rocha Neto"
  ],
  "abstract": "Unsupervised image segmentation aims at grouping different semantic patterns\nin an image without the use of human annotation. Similarly, image clustering\nsearches for groupings of images based on their semantic content without\nsupervision. Classically, both problems have captivated researchers as they\ndrew from sound mathematical concepts to produce concrete applications. With\nthe emergence of deep learning, the scientific community turned its attention\nto complex neural network-based solvers that achieved impressive results in\nthose domains but rarely leveraged the advances made by classical methods. In\nthis work, we propose a patch-based unsupervised image segmentation strategy\nthat bridges advances in unsupervised feature extraction from deep clustering\nmethods with the algorithmic help of classical graph-based methods. We show\nthat a simple convolutional neural network, trained to classify image patches\nand iteratively regularized using graph cuts, naturally leads to a\nstate-of-the-art fully-convolutional unsupervised pixel-level segmenter.\nFurthermore, we demonstrate that this is the ideal setting for leveraging the\npatch-level pairwise features generated by vision transformer models. Our\nresults on real image data demonstrate the effectiveness of our proposed\nmethodology.",
  "text": "Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts\nIsaac Wasserman\nUniversity of Pennsylvania\nPhiladelphia, Pennsylvania\nisaacrw@seas.upenn.edu\nJeov´a Farias Sales Rocha Neto\nBowdoin College\nBrunswick, Maine\nj.farias@bowdoin.edu\nAbstract\nUnsupervised image segmentation aims at grouping dif-\nferent semantic patterns in an image without the use of hu-\nman annotation. Similarly, image clustering searches for\ngroupings of images based on their semantic content with-\nout supervision. Classically, both problems have captivated\nresearchers as they drew from sound mathematical concepts\nto produce concrete applications. With the emergence of\ndeep learning, the scientific community turned its attention\nto complex neural network-based solvers that achieved im-\npressive results in those domains but rarely leveraged the\nadvances made by classical methods. In this work, we pro-\npose a patch-based unsupervised image segmentation strat-\negy that bridges advances in unsupervised feature extrac-\ntion from deep clustering methods with the algorithmic help\nof classical graph-based methods.\nWe show that a sim-\nple convolutional neural network, trained to classify image\npatches and iteratively regularized using graph cuts, natu-\nrally leads to a state-of-the-art fully-convolutional unsuper-\nvised pixel-level segmenter. Furthermore, we demonstrate\nthat this is the ideal setting for leveraging the patch-level\npairwise features generated by vision transformer models.\nOur results on real image data demonstrate the effective-\nness of our proposed methodology.\n1. Introduction\nImage segmentation has long been one of the main tasks\nin computer vision and it has been widely applied in general\nimage understanding or as a preprocessing step for other\ntasks such as object detection. It aims at corresponding each\npixel in an image to a semantically relevant class, in such\na way that similar pixels are assigned to the same class.\nThis problem finds various industrial applications such as\nautonomous driving, medical image analysis, video surveil-\nlance and virtual reality to name a few [32].\nOn the supervised front, deep learning approaches using\nConvolutional (CNN) and Fully Convolutional Neural Net-\nworks (FCN) achieved unprecedented results in image seg-\nmentation, as illustrated by the UNet [37] and DeepLab [13]\nmodels.\nRecently, however, methods using transformer-\nbased models, such as Segformer [46], DETR [9] and DINO\n[11], are slowly outperforming established CNN solutions.\nThis has prompted the recent interest in deep models that\nutilize image patches instead of their full-sized counterparts\n[22,41], leading some to postulate that patch representations\nare the main source of the transformer’s success [42].\nThese accomplishments come, however, at the cost of\nlong training schemes and the need for larger amounts of\nannotated data, which hinder their application in many do-\nmains where data can be expensive or scarce, such as in bi-\nology, or astrophysics [47]. These issues can be resolved via\nthe application of unsupervised techniques instead. In this\nsetting, one aims at creating a model that automatically dis-\ncovers semantically important visual features or groups that\ncharacterize the various objects in a scene. Classically, this\ncould be approached via variational, statistical, and graph-\nical methods, exemplified in active contours [12], condi-\ntional random fields [27], and graph cuts [7,39]. Within the\ndeep learning literature, prominent advances were made in\nthe field of unsupervised deep image clustering [36], which\neventually led to developments in deep image segmentation\ntechniques [20,21,25,30,44].\nIn this work, we introduce GraPL, an unsupervised im-\nage segmentation technique that draws inspiration from the\nsuccess of CNNs for imaging tasks, the learning strategies\nof deep clustering methods, and the regularization power\nof graph cut algorithms. Here, we alternate the training\nof a CNN classifier on image patches and the minimiza-\ntion of a clustering energy via graph cuts. To the best of\nour knowledge, this is the first attempt in both the deep\nclustering and image segmentation literature to make use\nof graphs cuts to solve a deep learning-based unsupervised\ntask. We show that our zero-shot approach detects visual\nsegments in an image without onerous unsupervised train-\ning on an entire image dataset, automatically finds a satis-\nfactory number of image segments, and easily translates the\npatch-level training to efficient pixel-level inference. Fur-\nthermore, because of its structure, it also naturally incorpo-\narXiv:2311.01475v2  [cs.CV]  15 Jan 2024\nrates pretrained patch embeddings [11,33], without relying\non them for the final product. Finally, we show that this sim-\nple approach achieves state-of-the-art results in deep unsu-\npervised image segmentation, demonstrating the potential\nof graph cuts to improve other patch-based deep segmenta-\ntion algorithms. Specifically, we make the following main\ncontributions with our work:\n• We introduce GraPL, an unsupervised segmentation\nmethod that learns a fully convolutional segmenter di-\nrectly from the image’s patches, using an iterative al-\ngorithm regularized by graph cuts.\n• We show that this framework naturally employs patch\nembeddings for pixel-level segmentation without the\nneed for postprocessing schemes such as CRF refining.\n• We demonstrate that GraPL is able to outperform the\nstate-of-the-art in deep unsupervised segmentation by\niteratively training a small, low complexity CNN.\n2. Related Work\n2.1. Deep Clustering\nWith the advancements in deep supervised image clas-\nsification techniques, interest in deep architectures to solve\nunsupervised problems followed naturally. This pursuit led\nto the task of partitioning image datasets into clusters us-\ning deep representations without human supervision, inau-\ngurating the body of work which is now referred to as “deep\nclustering.” The interested reader is referred to [36] for a\ncomprehensive review on the available approaches to deep\nclustering.\nIn GraPL, we treat image patches as individual images\nto be clustered as a pretext task to train our segmenter\nand efficiently use graph cuts to impose constraints on the\npatch clusters. To the best of our knowledge, our method is\nthe first to use MRF-based algorithms for clustering CNN-\ngenerated visual features.\n2.2. Deep Unsupervised Image Segmentation\nAs deep clustering aims to learn visual features and\ngroupings without human annotation via deep neural mod-\nels, deep unsupervised image segmentation hopes to use the\nsame models to learn coherent and meaningful image re-\ngions without the use of ground-truth labels. To do so, many\nmethods explore strategies that resemble those from deep\nclustering. Cho et al. [15] iteratively employs k-means to\ncluster pixel-level features extracted from a network trained\non different photometrically and geometrically varying im-\nage views. The work in [24] efficiently extends a mutual\ninformation-based deep clustering algorithm to the pixel-\nlevel by recognizing that such a process can be achieved via\nconvolution operations. [23] computes pixel embeddings\nfrom a metric learning network and segments each image\nusing a spherical k-means clustering algorithm.\nIn [25], the authors train an FCN with pseudo-labels gen-\nerated by the same network in a prior step. They attain re-\nliable segmentations by proposing a complex loss function\nthat ensures the similarity among pixels in shared segments,\nwhile encouraging their spacial continuity and limiting their\ntotal count. Our method, while similarly training an FCN,\nworks on patches and is able to reinforce spacial continuity\nand low segment count via our graph cut approach. Fur-\nthermore, due to its use of graph cuts, GraPL is able to\nnaturally incorporate pairwise patch relationships. Finally,\nwhile other patch-based unsupervised solutions require a\nsegmentation refinement stage after a patch feature clus-\ntering step [21, 43, 44], we both discover and instill patch\nknowledge interactively, without the need for postprocess-\ning our result.\n2.3. Graph Cuts for Image Segmentation\nModeling image generation as a Markov random field\n(MRF) has a long history in Computer Vision, dating its ini-\ntial theoretical and algorithmic achievements to the works\nof Abel et al [1] and Besag [5]. Soon enough, MRFs found\napplications in various image processing tasks, such as edge\ndetection, image denoising, segmentation, and stereo [28].\nIn particular, the works conducted by Boykov and Jolly [7]\nand Boykov et al. [6] demonstrated that one can apply ef-\nficient min st-cut based algorithms to solve image segmen-\ntation by modeling it as a Maximum a Posteriori (MAP)\nestimator of an MRF. Their groundbreaking results made\npossible the emergence of classical graph-based segmenta-\ntion methodologies such as GrabCut [38] and was, more\nrecently, used to improve the training of CNN-based seg-\nmenters [31]. CRF modeling, closely related to MRF, has\nalso played an important role in refining coarse network pre-\ndictions in recent segmentation methods [13,14,48].\nIn some ways, our proposed method draws inspirations\nfrom the methodologies proposed by Rother et al. [38], and\nMarin et al. [31]. In [38], the authors propose GrabCut,\nan algorithm that iteratively bound-optimizes a segmenta-\ntion energy, requiring the solution of a min st-cut problem\nat each iteration in order to perform unsupervised regular-\nized fitting of the image’s appearance, which is modeled as\na Gaussian mixture model. Our algorithm also uses min st-\ncut solvers iteratively, but here we (1) work on patch data,\ninstead of individual pixels, and (2) fit the image appearance\nusing a CNN classifier. Due to the nature of CNNs, our net-\nwork can seamlessly translate the patch-level classifier into\na pixel-level image segmenter. In [31], the authors show\nhow to perform weakly-supervised CNN segmentation via\nan optimizer that alternates between solving an MAP-MRF\nproblem and gradient computation. In contrast, our method\nsolves a fully unsupervised segmentation problem and does\nnot use our MAP solution to adjust gradient directions.\n3. Methodology\nGraPL (Graph Cuts at the Patch Level) is a fully unsu-\npervised segmentation method which operates in a single-\nimage paradigm. Using patch clustering as a pretext task\nfor segmentation, during training it is able to learn distinc-\ntive segment features that enable it to effectively segment\nthe image at the pixel level. Although other techniques have\npreviously shown patch clustering to be an effective surro-\ngate task [24,34,43,44], our method demonstrates that clus-\ntering the patches of a single image provides sufficient fea-\nture learning to accurately segment it. GraPL’s training is\nguided by patch-level graph cuts; this intervention imposes\nspatial coherence priors which are helpful for learning clus-\nters that are conducive to segmentation. At inference, the\ncomplexities of the pipeline disappear, leaving only the net-\nwork. Leveraging a generalization of CNNs, the trained\nmodel is “convolved” over the entire image to produce a\npixel-level segmentation.\n3.1. Algorithm\nLet I : Ω7→Rc be an image of c channels with pixel set\nΩ= {1, . . . , n} × {1, . . . , m}, and S : Ω7→{1, . . . , K}\nbe a segmentation map of I in K regions. Let P be a set of\npatches from I, such that all patches are of the same size,\ni.e., for each p in P, p : Ωp 7→Rc, Ωp ⊂Ω, |Ωp| = const.\nIn practice, we populate P by selecting all patches on a non-\noverlapping d × d grid of I, resulting in patches of shape\n(w/d, h/d). We make this choice of P based on two fac-\ntors: (1) efficiency, as this operation can be efficiently per-\nformed by most deep learning libraries via their unfolding\nmethods, and (2) simplicity, as it’s one of the simplest ways\nto generate equal sized patches that span Ω.\nLet Fθ : Ω7→[0, 1]K×|Ω| be an FCN, and F ′\nθ : Ωp 7→\n[0, 1]K be a CNN patch classifier. In GraPL, both networks\nare parametrized by the same parameters θ. F ′\nθ is used in\nour training stage and is applied to the patches in P, while\nFθ is employed in our inference phase and is our final seg-\nmenter. The full algorithm is shown in Figure 1.\nTraining Stage\nOur goal is to learn θ exclusively from the\ndata in P and transfer it to Fθ. To do so, our method trains\nF ′\nθ by minimizing an energy formulated at the patch level of\nI. Let S′ : P 7→{1, . . . , K} be a labeling for the patches\nin P. Following the literature on MRF modeling [7], we\ndefine the energy of S′ for an unknown θ as:\nE(S′, θ|P) = U(S′, θ|P) + λV (S′|P),\n(1)\nwith λ ≥0. The unary term U(·) is traditionally defined as:\nU(S′, θ|X) = −\nX\np∈P\nK\nX\nk=1\n1(S′(p) = k)[ln F ′\nθ(p)]k,\n(2)\nwhere 1(·) is the indicator function and [·]k is the k-th posi-\ntion of a vector. Let α and β be probability distributions in\nRK, and let H(α, β) = −PK\nk=1[α]k ln[β]k be their cross\nentropy. This means that Eq. 2 can be seen as the sum of\ncross entropies H(S′(p), F ′\nθ(p)) between S′(p), taken as a\none-hot probability distribution, and F ′\nθ(p) over all p ∈P.\nThe pairwise energy term V (·) is given by:\nV (S′|P) =\nX\n(p,q)∈P×P\n1(S′(p) ̸= S′(q))ϕ(p, q),\n(3)\nwith the patch similarity function ϕ(·) defined as:\nϕ(p, q) =\n1\ndist(p, q) exp\n\u0012\n−aff(p, q)2\n2σ\n\u0013\n,\n(4)\nwhere the data affinity function aff(p, q) evaluates the data\nsimilarity between p and q, and dist(p, q) considers the Eu-\nclidean distance between the centers of p and q. We select\nσ as the standard deviation of affinities for all p, q ∈P. To\ncompute patch affinities we make use of a patch encoder,\nwhich extracts an embedding from each patch in P.\nInspired by GrabCut [38], GraPL minimizes E using a\nblock-coordinate descent iterative strategy, where we alter-\nnate between optimizing for θ and S′, keeping the other\nconstant. The current labeling S′\nt is updated using the cur-\nrent network parameters θt−1, now taken as fixed in Eq. 1:\nS′\nt = arg min\nS′\nE(S′|θt−1, P).\n(5)\nThe above problem can be approximately solved by a series\nof minimum st-cut in the form of α-expansion or αβ-swap\nmoves [6]. This step is can be quickly accomplished due to\nthe efficiency of such graph cut algorithms and the compar-\natively small size of P, which presents a further advantage\nto our patch-based framework. We then compute the up-\ndated parameters θt via:\nθt = arg min\nθ\nL(F ′\nθ(P), S′\nt),\n(6)\nwhere F ′\nθ(P) = {F ′\nθ(p)}p∈P. We employ traditional gradi-\nent descent-based backpropagation to solve the above prob-\nlem. The loss L(·), is designed to predict the outputs of F ′\nθ\non each patch using the labels from S′\nt. Keeping S′\nt fixed,\nEq. 2 conveniently formulates that process as a sum of cross\nentropy losses, just as one would naturally devise in a super-\nvised segmentation learning scheme. We then follow Kim\net al. [25] and include a patch-level continuity loss C(θ):\nC(θ) =\nX\np∈P\nX\nq∈Np\n|F ′\nθ(p) −F ′\nθ(q)|,\n(7)\nwhere |·| is the L1 norm and Np is the set of patches imme-\ndiately neighboring p in Ωspace. In the general case, one\ncan employ a k-nearest neighbors graph of the elements in\nP, considering the Euclidean distance between patch cen-\n1\n4\n2\n3\n3\n2\n2\n4\n1\n1\n4\n2\n2\n2\n4\n4\n11\n11\n4\n4\n2\n4\n7\n7\n1\n9\n9\n4\n4\n7\n8\n8\n9\n9\n9\n9\n11\n8\n8\n12\n1\n13\n9\n11\n10\n8\n8\n12\n13\n15\n5\n5\n13\n5\n12\n12\n13\n15\n14\n14\n10\n13\n10\n4\nFigure 1. The proposed algorithm. GraPL trains a convolutional neural network to cluster patches of a single image without supervision\nunder the guidance of graph cuts, spatial continuity loss, and a patch affinity encoder. At inference, this patch clustering knowledge is\napplied to pixel-level segmentation of the image. F ′\nθ and Fθ share the same parameters.\nters. For the d × d grid from Figure 1, we choose Np to\nbe given by the patches immediately above and to the left\nof p, resembling what is done in [25]. This continuity loss\nbrings spatial coherence outside the graph step and encour-\nages smooth boundaries on the network outputs. In practice,\nwe found it to be beneficial to have both the graph step and\nC(θ) in our method. Our final loss is then defined as:\nL(F ′\nθ(P), S′\nt−1) =\nX\np∈P\nH(S′\nt−1(p), F ′\nθ(p)) + µC(θ), (8)\nwhere µ ≥0. As a consequence of the use of both graph\ncuts and the continuity loss described above, GraPL nat-\nurally suppresses extraneous labels arising from irrelevant\npatterns or textures, automatically promoting model selec-\ntion. As the alternation continues, F ′\nθ improves to the point\nwhere it no longer requires the guidance of the graph cuts to\nproduce spatially and semantically coherent patch clusters.\nAt that point, we end our training phase.\nInference Stage\nOnce F ′\nθ is trained, our next goal is to\nclassify all possible patches in I of shape equal to the\npatches in P. To that end, we first assume that, as a CNN,\nF ′\nθ is composed of an initial series of convolutional layers\nand a final stage of say Q dense layers, along with a soft-\nmax function at the end. Assume that the inputs of all layers\nare unpadded, and that each dense layer has sq units lead-\ning to a final output of size K. Now, one can replace each\ndense layer in F ′\nθ with a convolutional one of kernel size\n√sq and retain its exact functionality. Our resulting FCN,\nFθ, is now capable of efficiently being applied to I, by ef-\nfectively “convolving” it with patch classifier F ′\nθ.\n3.2. Advantages of using Graph Cut Iterations\nIn the absence of labels, GraPL learns to cluster patches\nvia an iterative procedure. This general formulation allows\nus to inject knowledge about the domain by designing an apt\nmethod for selecting pseudo-labels. While similar methods\nuse k-means [10], mixture models [23], or simply argmax\n[25] to transform network outputs into new pseudo-labels,\nGraPL uses these response vectors to define the unary en-\nergy of a patch-level MRF graph of the image.\nThis approach for patch clustering introduces some ad-\nvantages to our method. First, while the MRF modeling step\nis done primarily to impose a spatial coherence prior, due to\nthe known shrinking bias of graph cuts [26], the resultant\npartition also smooths segment boundaries and reduces the\nnumber of distinct segments, leading to natural model selec-\ntion. The spatial regularization introduced by the proposed\ngraph can also be generalized to accommodate other classi-\ncal graph formulations that consider segmentation seeds [6],\nappearance disparity [40], curvature [16], or target distri-\nbutions [4]. Finally, in contrast to methods that discover\nobjects by clustering patch embeddings arising from pre-\ntrained transformers and applying a segmentation head [21]\nor CRF refinement [43,44], GraPL considers patch embed-\ndings only as way to guide its training stage, yielding a final\npixel-level segmentation map without postprocessing. We\nconsider our graph cut-based approach to handle rich patch\nfeatures beneficial, as we do not overly depend on their clus-\ntering power, and simply reference them as guidance when\nregularizing our training.\n4. Experiments\n4.1. Implementation and Experimental Setup\nSegmentation Task\nTo evaluate the performance and be-\nhaviors of GraPL, the algorithm was tasked with segment-\ning the 200 image test set of BSDS500 [3] using a vari-\nety of hyperparameters. Segmentation performance is mea-\nsured in terms of mean intersection over union (mIoU) [19],\nwith predicted segments matched one-to-one with target\nsegments using a version of the Hungarian algorithm mod-\nified to accommodate ˆK ̸= K∗, where ˆK is the number of\ndistinct segments in the final segmentation, and K∗is the\nnumber of segments in the ground truth. Results are aver-\naged across 10 different random seeds for initialization.\nHyperparameters\nUnless otherwise specified, the fol-\nlowing configuration was used during testing.\nPseudo-\nlabels were initialized according to the SLIC [2] based al-\ngorithm described in Section 4.2. GraPL was run for four\ntraining iterations, and the number of gradient steps in the\nloss minimization at each iteration was 40, 32, 22, and 12\nrespectively. K0, the number of initial segments was set\nat 14, and d was set to 32. Graph cuts were implemented\nusing pyGCO [29], and the pairwise energy coefficient, λ,\nwas set to 64. The continuity loss was assigned a weight\nof µ = 3. The L2 norm between DINOv2 [33] (ViT-L/14\ndistilled) patch embeddings was used as an affinity metric\nto determine pairwise weights.\nNetwork Architecture\nAn intentionally minimal CNN\narchitecture was used, consisting of 2 3×3 unpadded convo-\nlutional layers with 32 and 8 filters, respectively. In F ′\nθ, this\nis followed by a dense classification head with K0 units,\nand in Fθ it is followed by a ( h\nd −4) × ( w\nd −4) convo-\nlutional segmentation head with K0 filters. The network\nlayers are each separated by batch normalization, tanh ac-\ntivations, and dropout with rate 0.2. Without padding, our\nnetwork is subject to certain regularization implications. In\nCNNs, zero padding an image has the effect of dropping out\nsome of the weights of the subsequent convolutional layer.\nAs our method requires the training phase to be executed on\nunpadded images, it is effectively deprived of this regular-\nization feature. We found that applying dropout before the\nfirst convolutional layer all but resolved issues arising from\nthe network’s lack of padding. Despite its simplicity, this\nnetwork is complex enough to achieve reliable segmenta-\ntions, and more complex networks did not lead to better per-\nformance. In our work, we also abstain from using padding\non our inference phase, which results in ˆS = Fθ(I) being\nof a size smaller than Ω, due to the convolution operations\nin Fθ. GraPL handles this discrepancy by interpolating ˆS to\nthe original dimensions via nearest neighbor interpolation.\nNetworks were implemented using PyTorch 2.0.1 [35]1.\nEarly Stopping\nIf a cross entropy loss of less than 1.0\nwas reached during the first iteration, it was stopped early,\nand new pseudo-labels were assigned. During the first iter-\nation, we are fitting the initial pseudo-labels, which are ei-\nther arbitrary or assigned by SLIC. By imposing this early\nstopping condition, we are avoiding the local minima where\nGraPL may be overfitting to a less performant (or worse, ar-\nbitrary) segmentation.\n4.2. Ablation Studies\nInitialization\nAs an iterative algorithm, proper initializa-\ntion is an important factor in training GraPL. Although sim-\nilar deep clustering algorithms have used randomly initial-\nized pseudo-labels [10,25], we were unsure whether ignor-\ning more principled approaches was leaving performance\non the table. To answer this question, we compared four ini-\ntialization strategies: “patchwise random,” “seedwise ran-\ndom,” “spatial clustering,” and an approach based on SLIC\n[2]. The “patchwise random” approach individually assigns\neach patch p in P a random label. In the “seedwise random”\nstrategy, we select K0 random patches and assign them each\none of the K0 labels; the remaining patches are assigned\nthe label of the patch closest to them. For “spatial cluster-\ning,” patches are clustered using k-means according to their\n(x, y) spatial coordinates to form K0 clusters of roughly\nequal size. In the SLIC-based approach, we unfold a K0\ncluster SLIC segmentation with low compactness into the\nsame patches as the input image. The onehot labels of these\npatches are averaged and normalized to produce soft labels.\nThese soft initializations are an attempt to regularize and\nretain all salient features of the patch during training.\nTests demonstrated that patchwise random initialization\nis not an ideal choice for GraPL (Table 1). This is likely\nbecause it encourages a disregard for spatial coherence dur-\ning the first and most important iteration. While SLIC was\n1A demo implementation of GraPL is available at https://\ngithub.com/isaacwasserman/GraPL\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 2. Example of undersegmentation from non-SLIC initial-\nization. (a) Input image. (b) Patchwise Random ( ˆK = 2). (c)\nSeedwise Random ( ˆK = 4). (d) Spatial Clustering ( ˆK = 4). (e)\nSLIC ( ˆK = 6).\nshown to be the best choice out of the methods tested, seed-\nwise random and spatial clustering initialization performed\nonly 1.0% and 0.6% worse, respectively, and the algorithm\ncould likely be tuned such that they meet or exceed the per-\nformance of SLIC. However, in the current configuration,\nwe notice a tendency for both of these methods to result in\nundersegmentation, in which ∆K = K0 −ˆK is consider-\nably higher than the SLIC version (Figure 2).\nPairwise Edge Weights\nThe pairwise energy function\n(Eq.\n4) used by GraPL includes an affinity function\naff(p, q). Designed with vision transformers in mind, this\nfunction is defined by the Euclidean distance between some\npatch metric or embedding m(p) for p ∈P.\nThough DINOv2 [33] has been shown to produce excel-\nlent, fully unsupervised features on the patch level, requir-\ning minimal supervised fine-tuning to produce an effective\nsegmentation model [33]. However, it’s unclear whether the\nfeatures are easily separable using unsupervised methods.\nWe examined the applicability of three definitions for\nm(p): DINOv2 embedding, mean RGB color, and patch\nposition. To produce the final DINO embeddings, images\nwere resized to 14d × 14d, such that each GraPL patch\ncorresponds to a 14 × 14 DINO patch. These embeddings\nwere reduced to K0 dimensions using 2nd degree polyno-\nmial PCA. As a baseline, we also tested a version where the\nfully connected graph was replaced with a 4-neighborhood\nlattice of uniformly weighted edges.\nIn our tests, DINOv2 embeddings were a significantly\nbetter metric than distance alone (Table 2). However, they\nwere outperformed by simple RGB color vectors. Acknowl-\nedging DINOv2’s ability to act as a feature extractor for su-\npervised segmentation, further research is needed to deter-\nmine what types of transformations are necessary for con-\nverting the embeddings into a better affinity metric.\nInitializer\nmIoU\nPatchwise Rand.\n0.496\nSeedwise Rand.\n0.507\nSpatial\n0.509\nSLIC\n0.512\nTable 1. Comparison of pseudo-\nlabel initialization methods\nMetric\nmIoU\nUniform\n0.459\nPosition\n0.476\nColor\n0.527\nDINOv2\n0.512\nTable 2. Comparison of pair-\nwise weighting metrics\nWarm Start\nGraPL is designed to train the same network\ncontinuously throughout all iterations. This is in contrast\nto similar iterative methods which prefer a “cold start,” re-\ninitializing the parameters of the surrogate function prior to\nsubsequent iterations. Preliminary tests showed that in our\ncase, a “warm start” approach is preferred to re-initializing\nthe network each time. These two approaches in fact pro-\n0\n40\n72\n94\n0\n1\n2\n3\nGradient Step\nCross Entopy\nWarm\nCold\nFigure 3. Comparison of loss curves using warm and cold starting\nmethods, averaged over all test images in BSDS500 [3]. Here we\nconsider the loss value at the end of each gradient step. On the\nx-axis, we depict the instants where a new training iteration starts.\nduce very different loss curves (Figure 3). Cold starts pro-\nduce large spikes in loss at the beginning of each training\niteration, whereas warm starts require only minor adjust-\nments at these points. We expect that the first iterations of\ntraining provide important feature learning to the first lay-\ners of the network. By starting cold at each iteration, subse-\nquent iterations are unable to benefit from the learned low-\nlevel feature detectors and therefore present a more unstable\ntraining phase.\nPairwise Energy Coefficient (λ)\nGraPL uses graph cuts\nto generate each new set of pseudo-labels, working on the\ntheory that this graphical representation of the image pro-\nvides an important spatial coherence prior, which is perhaps\nmissing from similar unsupervised methods, and accounts\nfor its success. Furthermore, GraPL relies on pairwise costs\nas well as the continuity loss to gradually decrease ˆK. To\ntest these ideas, we evaluated the segmentation performance\nof the algorithm as well as ∆K across different values of λ,\nthe hyperparameter which defines the scale of the pairwise\nenergy as defined in Eq. 4.\nWhen λ = 0, cutting any non-terminal edge incurs no\ncost.\nIn this case, the function of the cut is effectively\nthe same as the argmax clustering step found in [25], as\npseudo-labels are entirely dependent on the current network\nresponse vectors. As λ increases, network response vec-\ntors are made less influential in the pseudo-label assignment\nprocess, as expected.\nThe results in Figure 4 demonstrate a logarithmic in-\ncrease in segmentation performance as λ is raised from 0\nthrough 64. However, increasing λ to values higher than 64\ntends to result in comparatively poor performance. Because\nincreasing λ strengthens pairwise connections, we would\nexpect it to be closely correlated with ∆K. When λ ≤64,\nwe observe this behavior; however, higher values actually\nresult in a plateau or slight decrease in ∆K.\nIn a configuration where the pairwise edges were uni-\n(a)\n0\n2\n8\n32 128\n0.46\n0.48\n0.5\n0.52\nλ\nmIoU\n(b)\n0\n2\n8\n32 128\n11.4\n11.6\n11.8\n12\n12.2\nλ\n∆K\nFigure 4. Effects of pairwise energy coefficient. (a) Effect of λ on\nmIoU. (b) Effect of λ on ∆K.\nformly weighted (or weighted according to spatial dis-\ntance), we would expect higher than optimal values of λ\nto push ∆K too high and produce oversimplified segmen-\ntations, where multiple target segments are collapsed into a\nsingle predicted segment. However, when pairwise edges\nare weighted by patch encoder embedding affinity, push-\ning λ too high can instead result in an overly detailed seg-\nmentation, in which the graph cut considers the pairwise\nenergy (dictated by the patch embeddings) more than the\nunary weights learned by GraPL.\nContinuity Loss\nSpatial continuity loss, first introduced\nin [25], provides GraPL a spatial coherence prior which pe-\nnalizes the network directly at each gradient step, rather\nthan through the graph cut produced pseudo-labels at the\nend of each iteration. Though shown effective in [25], we\ninstinctively believed that it would be redundant in a graph-\nically guided pipeline like GraPL. However, we observed\nthat the combination of the two different spatial coherence\npriors produced more accurate segmentations than either\none alone (Figure 5).\nIn practice, we noted that this loss has a different mech-\nanism of action than the graphical coherence prior. In the\nabsence of this spatial loss, GraPL employs a level of trust\nin the patch encoder that may be unfounded, as the pairwise\nenergy only penalizes the separation of patches with a great\naffinity; but when using a patch encoder like DINO, which\nis defined by a large neural network, the edge weights may\nbe high variance. In this case, increasing λ only serves to\nemphasize the patch encoder’s bias for certain edges. How-\never, increasing the weight of the spatial continuity loss ap-\nplies a higher penalty for all edges. In effect, it could be\ncompared to an additive bias term in the pairwise energy\n0\n0.5\n1\n2\n4\n8\n0.35\n0.4\n0.45\n0.5\n0.55\nµ\nmIoU\nFigure 5. Effect of spatial continuity loss weight on mIoU\nfunction that raises the cost, no matter the patch affinity.\nPatch Size\nPatch-based approaches are faced with a\nchoice between granularity (with smaller patches) and the\ninformation richness of input (with larger patches).\nIn\nGraPL’s case, smaller patches also entail more complex\ngraphs that take longer to solve, and larger patches entail\nhigher memory usage. We found that setting d equal to\n32 offered both optimal performance and near optimal ef-\nficiency (Table 3).\nd\nmIoU\nSeconds per Image\n8\n0.248\n3.49\n16\n0.372\n1.72\n32\n0.512\n1.75\n64\n0.496\n6.98\nTable 3. mIoU and segmentation time as a function of patch size.\nTime measurements are based on segmentation of BSDS500 test\nset on an Nvidia A100 GPU and AMD Epyc 7343 CPU.\n4.3. Comparison to Other Methods\nWe compared the segmentation performance of GraPL\nto six other unsupervised deep-learning methods: Differen-\ntiable Feature Clustering (DFC) [25], Invariant Information\nClustering (IIC) [24], Pixel-level feature Clustering using\nInvariance and Equivariance (PiCIE) [15], Segment Sorting\n(SegSort) [23] and W-Net [45]. We also tested two base-\nlines which use SLIC to segment images based on RGB and\nDINOv2 [33] patch embeddings (interpolated to the pixel\nlevel). These baselines were selected to demonstrate that\nthe success of our method does not simply originate from its\ninitialization or its pretrained guidance. PiCIE and SegSort\nwere trained on their preferred datasets, COCO-Stuff [8]\nand PASCAL VOC 2012 [17], as BSDS500 is too small,\nwhile the others used only BSDS500.\nTable 4 summarizes the quantitative comparative results\nof the above methods, where segmentation performance\nwas measured in terms of both mIoU and pixel accuracy\n[19]. As with mIoU, pixel accuracy was computed using a\none-to-one label matching strategy. Figure 6 displays some\nsegmentation results from the above methods for qualitative\ncomparison.\nCompared to other unsupervised methods, GraPL is able\nto decompose complex foregrounds into detailed yet seman-\ntically salient components. Notice how GraPL is able to\npick up on small details like sunglasses in the distance while\nignoring less relevant features of the image, such as creases\nin clothing. In many cases, it is able to handle color gradi-\nent variation, usually present in sky backgrounds or shadow\nregions. On occasion, GraPL detects segments that are not\npresent in the ground-truth, such as the bird heads on the last\n(a) Input\n(b) GT1\n(c) GT2\n(d) GraPL\n(e) DFC\n(f) Double DIP\n(g) PiCIE\n(h) SegSort\n(i) W-Net\nFigure 6. Qualitative comparison of GraPL to other deep learning-based unsupervised segmentation methods. We selected two of the\navailable ground truth segmentations (GT1 and GT2) for comparison, one more detailed and one less detailed.\nMethod\nmIoU\nAccuracy\nSLIC (RGB features)\n0.137\n0.416\nSLIC (DINOv2 features)\n0.258\n0.280\nDFC [25]\n0.398\n0.505\nDoubleDIP [18]\n0.356\n0.423\nIIC [24]†\n0.172\nPiCIE [15]\n0.325\n0.405\nSegSort [23]\n0.480\n0.505\nW-Net [45]\n0.428\n0.531\nGraPL (proposed)\n0.527\n0.569\nTable 4. BSDS500 performance comparison of GraPL with other\nunsupervised deep-learning methods and baselines.\n†The value listed for IIC is sourced from [25].\nqualitative example, which, despite being reasonable, de-\ncreases its quantitative performance. Finally, it also strug-\ngles to detect fine structures, such as castle tops, small holes\nand bird beaks. Despite that, our proposed method is able\nto outperform all of the compared methods by a margin of\nat least 6.9% in accuracy and 9.3% mIoU. It is also worth\nnoting the low performance of our baselines, when com-\npared to GraPL. This demonstrates that our method does\nnot merely rest on the success of our our initializer, SLIC.\nInstead, GraPL’s success is a product of its novel training\nand inference methodology.\n5. Conclusion\nIn this paper, we introduce GraPL, a deep learning-based\nunsupervised segmentation framework that approaches the\nproblem by solving a patch clustering surrogate task to\nlearn network parameters which are then used for pixel-\nlevel classification. Additionally, GraPL is the first deep\nlearning method to employ a graph cut regularizer during\ntraining, which encourages spacial coherence and leverages\nthe discriminative power of patch embeddings.\nFurther-\nmore, it seamlessly translates patch-level learning to the\npixel-level without the need for postprocessing. Our experi-\nments demonstrate our algorithm’s promising capacities, as\nit is able to outperform many state-of-the-art unsupervised\nsegmentation methods. Our work can be seen as further ev-\nidence for the benefit of using graph cuts in deep learning,\nespecially in the context of unsupervised segmentation.\nReferences\n[1] Kenneth Abend, Tl Harley, and L Kanal. Classification of\nbinary random patterns. IEEE Transactions on Information\nTheory, 11(4):538–544, 1965. 2\n[2] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien\nLucchi, Pascal Fua, and Sabine S¨usstrunk. Slic superpix-\nels compared to state-of-the-art superpixel methods. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n34(11):2274–2282, 2012. 5\n[3] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik.\nContour detection and hierarchical image\nsegmentation.\nIEEE Trans. Pattern Anal. Mach. Intell.,\n33(5):898–916, May 2011. 5, 6\n[4] Ismail Ben Ayed, Hua-mei Chen, Kumaradevan Punithaku-\nmar, Ian Ross, and Shuo Li. Graph cut segmentation with a\nglobal constraint: Recovering region distribution via a bound\nof the bhattacharyya measure. In 2010 IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recognition,\npages 3288–3295. IEEE, 2010. 4\n[5] Julian Besag. On the statistical analysis of dirty pictures.\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology, 48(3):259–279, 1986. 2\n[6] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approxi-\nmate energy minimization via graph cuts. IEEE Transactions\non pattern analysis and machine intelligence, 23(11):1222–\n1239, 2001. 2, 3, 4\n[7] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for op-\ntimal boundary & region segmentation of objects in nd im-\nages. In Proceedings eighth IEEE international conference\non computer vision. ICCV 2001, volume 1, pages 105–112.\nIEEE, 2001. 1, 2, 3\n[8] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Computer vision\nand pattern recognition (CVPR), 2018 IEEE conference on.\nIEEE, 2018. 7\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213–229. Springer, 2020. 1\n[10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In Proceedings of the European confer-\nence on computer vision (ECCV), pages 132–149, 2018. 4,\n5\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650–9660, 2021. 1, 2\n[12] Tony F Chan and Luminita A Vese. Active contours without\nedges. IEEE Transactions on image processing, 10(2):266–\n277, 2001. 1\n[13] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834–848, 2017. 1,\n2\n[14] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for seman-\ntic image segmentation. arXiv preprint arXiv:1706.05587,\n2017. 2\n[15] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath\nHariharan. Picie: Unsupervised semantic segmentation us-\ning invariance and equivariance in clustering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16794–16804, 2021. 2, 7, 8\n[16] Noha Youssry El-Zehiry and Leo Grady. Fast global opti-\nmization of curvature. In 2010 IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition, pages\n3257–3264. IEEE, 2010. 4\n[17] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\nand A. Zisserman.\nThe PASCAL Visual Object Classes\nChallenge 2012 (VOC2012) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\n7\n[18] Yosef Gandelsman, Assaf Shocher, and Michal Irani.\n”\ndouble-dip”: unsupervised image decomposition via coupled\ndeep-image-priors. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11026–11035, 2019. 8\n[19] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea,\nVictor Villena-Martinez, and Jose Garcia-Rodriguez. A re-\nview on deep learning techniques applied to semantic seg-\nmentation, 2017. 5, 7\n[20] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences. In In-\nternational Conference on Learning Representations, 2021.\n1\n[21] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences. arXiv\npreprint arXiv:2203.08414, 2022. 1, 2, 4\n[22] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and En-\nhua Wu. Vision gnn: An image is worth graph of nodes. Ad-\nvances in Neural Information Processing Systems, 35:8291–\n8303, 2022. 1\n[23] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D\nCollins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen.\nSegsort:\nSegmentation by discriminative sorting of seg-\nments. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 7334–7344, 2019. 2, 4,\n7, 8\n[24] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-\nformation clustering for unsupervised image classification\nand segmentation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9865–9874,\n2019. 2, 3, 7, 8\n[25] Wonjik Kim, Asako Kanezaki, and Masayuki Tanaka. Unsu-\npervised learning of image segmentation based on differen-\ntiable feature clustering. IEEE Transactions on Image Pro-\ncessing, 29:8055–8068, 2020. 1, 2, 3, 4, 5, 6, 7, 8\n[26] Vladimir Kolmogorov and Yuri Boykov.\nWhat metrics\ncan be approximated by geo-cuts, or global optimization of\nlength/area and flux. In Tenth IEEE International Confer-\nence on Computer Vision (ICCV’05) Volume 1, volume 1,\npages 564–571. IEEE, 2005. 4\n[27] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efficient inference\nin fully connected crfs with gaussian edge potentials. Ad-\nvances in neural information processing systems, 24, 2011.\n1\n[28] Stan Z Li. Markov random field modeling in image analysis.\nSpringer Science & Business Media, 2009. 2\n[29] Yujia Li and Jirka Borovec. pygco. https://github.\ncom/Borda/pyGCO, 2023. 5\n[30] Qinghong Lin, Weichan Zhong, and Jianglin Lu. Deep super-\npixel cut for unsupervised image segmentation. In 2020 25th\nInternational Conference on Pattern Recognition (ICPR),\npages 8870–8876. IEEE, 2021. 1\n[31] Dmitrii Marin, Meng Tang, Ismail Ben Ayed, and Yuri\nBoykov. Beyond gradient descent for regularized segmen-\ntation losses. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10187–\n10196, 2019. 2\n[32] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza,\nNasser Kehtarnavaz, and Demetri Terzopoulos. Image seg-\nmentation using deep learning: A survey. IEEE transactions\non pattern analysis and machine intelligence, 44(7):3523–\n3542, 2021. 1\n[33] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 2, 5, 6, 7\n[34] Yassine Ouali, C´eline Hudelot, and Myriam Tami. Autore-\ngressive unsupervised image segmentation.\nIn Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part VII 16, pages\n142–158. Springer, 2020. 3\n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An\nImperative Style, High-Performance Deep Learning Library.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems 32, pages 8024–8035. Curran\nAssociates, Inc., 2019. 5\n[36] Yazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li,\nXiaorong Pu, Philip S Yu, and Lifang He. Deep clustering:\nA comprehensive survey. arXiv preprint arXiv:2210.04142,\n2022. 1, 2\n[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234–241. Springer, 2015. 1\n[38] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\n” grabcut” interactive foreground extraction using iter-\nated graph cuts.\nACM transactions on graphics (TOG),\n23(3):309–314, 2004. 2, 3\n[39] Jianbo Shi and Jitendra Malik. Normalized cuts and image\nsegmentation. IEEE Transactions on pattern analysis and\nmachine intelligence, 22(8):888–905, 2000. 1\n[40] Meng Tang, Lena Gorelick, Olga Veksler, and Yuri Boykov.\nGrabcut in one cut. In Proceedings of the IEEE international\nconference on computer vision, pages 1769–1776, 2013. 4\n[41] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.\nMlp-mixer: An all-mlp architecture for vision.\nAdvances\nin neural information processing systems, 34:24261–24272,\n2021. 1\n[42] Asher Trockman and J Zico Kolter. Patches are all you need?\narXiv preprint arXiv:2201.09792, 2022. 1\n[43] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra.\nCut and learn for unsupervised object detection and instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3124–\n3134, 2023. 2, 3, 4\n[44] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao\nLi, Shell Xu Hu, James L Crowley, and Dominique Vaufrey-\ndaz. Tokencut: Segmenting objects in images and videos\nwith self-supervised transformer and normalized cut. arXiv\npreprint arXiv:2209.00383, 2022. 1, 2, 3, 4\n[45] Xide Xia and Brian Kulis.\nW-net:\nA deep model for\nfully unsupervised image segmentation.\narXiv preprint\narXiv:1711.08506, 2017. 7, 8\n[46] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo.\nSegformer: Simple and\nefficient design for semantic segmentation with transform-\ners.\nAdvances in Neural Information Processing Systems,\n34:12077–12090, 2021. 1\n[47] Hongshan Yu, Zhengeng Yang, Lei Tan, Yaonan Wang, Wei\nSun, Mingui Sun, and Yandong Tang. Methods and datasets\non semantic segmentation:\nA review.\nNeurocomputing,\n304:82–103, 2018. 1\n[48] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-\nParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang\nHuang, and Philip HS Torr. Conditional random fields as\nrecurrent neural networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1529–1537,\n2015. 2\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2023-11-01",
  "updated": "2024-01-15"
}