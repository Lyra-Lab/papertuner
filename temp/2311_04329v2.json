{
  "id": "http://arxiv.org/abs/2311.04329v2",
  "title": "Formal Aspects of Language Modeling",
  "authors": [
    "Ryan Cotterell",
    "Anej Svete",
    "Clara Meister",
    "Tianyu Liu",
    "Li Du"
  ],
  "abstract": "Large language models have become one of the most commonly deployed NLP\ninventions. In the past half-decade, their integration into core natural\nlanguage processing tools has dramatically increased the performance of such\ntools, and they have entered the public discourse surrounding artificial\nintelligence. Consequently, it is important for both developers and researchers\nalike to understand the mathematical foundations of large language models, as\nwell as how to implement them. These notes are the accompaniment to the\ntheoretical portion of the ETH Z\\\"urich course on large language models,\ncovering what constitutes a language model from a formal, theoretical\nperspective.",
  "text": "arXiv:2311.04329v2  [cs.CL]  17 Apr 2024\nFormal Aspects\nof\nLanguage Modeling\nRyan Cotterell, Anej Svete, Clara Meister,\nTianyu Liu, and Li Du\nThursday 18th April, 2024\nContents\n1\nIntroduction\n5\n1.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2\nProbabilistic Foundations\n7\n2.1\nAn Invitation to Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nA Measure-theoretic Foundation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3\nLanguage Models: Distributions over Strings\n. . . . . . . . . . . . . . . . . . . . . .\n14\n2.3.1\nSets of Strings\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3.2\nDefining a Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.4\nGlobal and Local Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4.1\nGlobally Normalized Language Models . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4.2\nLocally Normalized Language Models\n. . . . . . . . . . . . . . . . . . . . . .\n20\n2.5\nTight Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2.5.1\nTightness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2.5.2\nDefining the probability measure of an LNM\n. . . . . . . . . . . . . . . . . .\n28\n2.5.3\nInterpreting the Constructed Probability Space . . . . . . . . . . . . . . . . .\n35\n2.5.4\nCharacterizing Tightness\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3\nModeling Foundations\n45\n3.1\nRepresentation-based Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . .\n46\n3.1.1\nVector Space Representations . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n3.1.2\nCompatibility of Symbol and Context\n. . . . . . . . . . . . . . . . . . . . . .\n52\n3.1.3\nProjecting onto the Simplex . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n3.1.4\nRepresentation-based Locally Normalized Models . . . . . . . . . . . . . . . .\n58\n3.1.5\nTightness of Softmax Representation-based Models . . . . . . . . . . . . . . .\n58\n3.2\nEstimating a Language Model from Data\n. . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.2.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.2.2\nLanguage Modeling Objectives . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n3.2.3\nParameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n3.2.4\nRegularization Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n4\nClassical Language Models\n75\n4.1\nFinite-state Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n4.1.1\nWeighted Finite-state Automata\n. . . . . . . . . . . . . . . . . . . . . . . . .\n76\n4.1.2\nFinite-state Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n3\n4\nCONTENTS\n4.1.3\nNormalizing Finite-state Language Models . . . . . . . . . . . . . . . . . . . .\n87\n4.1.4\nTightness of Finite-state Models\n. . . . . . . . . . . . . . . . . . . . . . . . .\n93\n4.1.5\nThe n-gram Assumption and Subregularity . . . . . . . . . . . . . . . . . . .\n97\n4.1.6\nRepresentation-based n-gram Models . . . . . . . . . . . . . . . . . . . . . . . 101\n4.2\nPushdown Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n4.2.1\nHuman Language Is not Finite-state . . . . . . . . . . . . . . . . . . . . . . . 107\n4.2.2\nContext-free Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n4.2.3\nWeighted Context-free Grammars . . . . . . . . . . . . . . . . . . . . . . . . . 115\n4.2.4\nContext-free Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . 118\n4.2.5\nTightness of Context-free Language Models . . . . . . . . . . . . . . . . . . . 120\n4.2.6\nNormalizing Weighted Context-free Grammars\n. . . . . . . . . . . . . . . . . 124\n4.2.7\nPushdown Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n4.2.8\nPushdown Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n4.2.9\nMulti-stack Pushdown Automata . . . . . . . . . . . . . . . . . . . . . . . . . 133\n4.3\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n5\nNeural Network Language Models\n137\n5.1\nRecurrent Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n5.1.1\nHuman Language is Not Context-free\n. . . . . . . . . . . . . . . . . . . . . . 138\n5.1.2\nRecurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n5.1.3\nGeneral Results on Tightness . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n5.1.4\nElman and Jordan Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n5.1.5\nVariations on Recurrent Networks\n. . . . . . . . . . . . . . . . . . . . . . . . 152\n5.2\nRepresentational Capacity of Recurrent Neural Networks\n. . . . . . . . . . . . . . . 157\n5.2.1\nRNNs and Weighted Regular Languages\n. . . . . . . . . . . . . . . . . . . . 158\n5.2.2\nAddendum to Minsky’s Construction: Lower Bounds on the Space Complexity\nof Simulating PFSAs with RNNs . . . . . . . . . . . . . . . . . . . . . . . . . 172\n5.2.3\nLower Bound in the Probabilistic Setting\n. . . . . . . . . . . . . . . . . . . . 187\n5.2.4\nTuring Completeness of Recurrent Neural Networks\n. . . . . . . . . . . . . . 192\n5.2.5\nThe Computational Power of RNN Variants . . . . . . . . . . . . . . . . . . . 204\n5.2.6\nConsequences of the Turing completeness of recurrent neural networks . . . . 205\n5.3\nTransformer-based Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n5.3.1\nInformal Motivation of the Transformer Architecture . . . . . . . . . . . . . . 208\n5.3.2\nA Formal Definition of Transformers . . . . . . . . . . . . . . . . . . . . . . . 210\n5.3.3\nTightness of Transformer-based Language Models . . . . . . . . . . . . . . . . 222\n5.4\nRepresentational Capacity of Transformer Language Models . . . . . . . . . . . . . . 225\nChapter 1\nIntroduction\n1.1\nIntroduction\nWelcome to the class notes for the first third of Large Language Models (263-5354-00L). The course\ncomprises an omnibus introduction to language modeling. The first third of the lectures focuses on\na formal treatment of the subject. The second part focuses on the practical aspects of implementing\na language model and its applications. Many universities are offering similar courses at the moment,\ne.g., CS324 at Stanford University (https://stanford-cs324.github.io/winter2022/) and CS\n600.471 (https://self-supervised.cs.jhu.edu/sp2023/) at Johns Hopkins University. Their\nsyllabi may serve as useful references.\nDisclaimer.\nThis is the third time the course is being taught and we are improving the notes\nas we go. We will try to be as careful as possible to make them typo- and error-free. However,\nthere will undoubtedly be mistakes scattered throughout. We will be very grateful if you report\nany mistakes you spot, or anything you find unclear and confusing in general—this will benefit the\nstudents as well as the teaching staff by helping us organize a better course!\n5\n6\nCHAPTER 1. INTRODUCTION\nChapter 2\nProbabilistic Foundations\n2.1\nAn Invitation to Language Modeling\nThe first module of the course focuses on defining a language model mathematically. To see why\nsuch a definition is nuanced, we are going to give an informal definition of a language model and\ndemonstrate two ways in which that definition breaks and fails to meet our desired criteria.\nDefinition 2.1.1: Language Model (Informal)\nGiven an alphabeta Σ and a distinguished end-of-sequence symbol eos R Σ, a language model\nis a collection of conditional probability distributions ppy | yq for y P Σ Y teosu and y P Σ˚,\nwhere Σ˚ is the set of all strings over the alphabet Σ. The term ppy | yq represents the\nprobability of the symbol y occurring as the next symbol after the string y.\naAn alphabet is a finite, non-empty set. It is also often referred to as a vocabulary.\nDefinition 2.1.1 is the definition of a language model that is implicitly assumed in most papers\non language modeling. We say implicitly since most technical papers on language modeling simply\nwrite down the following autoregressive factorization\nppyq “ ppy1 ¨ ¨ ¨ yT q “ ppeos | yq\nT\nź\nt“1\nppyt | yătq\n(2.1)\nas the probability of a string according to the distribution p.1 The part that is left implicit in\nEq. (2.1) is whether or not p is indeed a probability distribution and, if it is, over what space.\nThe natural assumption in Definition 2.1.1 is that p is a distribution over Σ˚, i.e., the set of all\nfinite strings2 over an alphabet Σ. However, in general, it is not true that all such collections of\nconditionals will yield a valid probability distribution over Σ˚; some may “leak” probability mass\nto infinite sequences.3 More subtly, we additionally have to be very careful when dealing with\n1Many authors (erroneously) avoid writing eos for concision.\n2Some authors assert that strings are by definition finite.\n3However, the converse is true: All valid distributions over Σ˚ may be factorized as the above.\n7\n8\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nuncountably infinite spaces lest we run into a classic paradox. We highlight these two issues with\ntwo very simple examples. The first example is a well-known paradox in probability theory.\nExample 2.1.1: Infinite Coin Toss\nConsider the infinite independent fair coin toss model, where we aim to place a distribution\nover tH, Tu8, the (uncountable) set of infinite sequences of tH, Tu (H represents the event of\nthrowing heads and T the event of throwing tails). Intuitively, such a distribution corresponds\nto a “language model” as defined above in which for all yăt, ppH | yătq “ ppT | yătq “ 1\n2 and\nppeos | yătq “ 0. However, each individual infinite sequence over tH, Tu should also be assigned\nprobability p 1\n2q8 “ 0. Without a formal foundation, one arrives at the following paradox:\n1 “ p ptH, Tu8q\n“ p\n¨\n˝\nď\nωPtH,Tu8\ntωu\n˛\n‚\n“\nÿ\nωPtH,Tu8\npptωuq\n“\nÿ\nωPtH,Tu8\n0 ?“ 0.\nThe second example is more specific to language modeling. As we stated above, an implicit\nassumption made by most language modeling papers is that a language model constitutes a\ndistribution over Σ˚. However, in our next example, we show that a collection of conditions that\nsatisfy Definition 2.1.1 may not sum to 1 if the sum is restricted to elements of Σ˚. This means\nthat it is not a priori clear what space our probability distribution is defined over.4\n0{1\n1\n2{ 1\n2\nH{ 1\n2\nT{ 1\n2\nH{1\nT{ 1\n2\nFigure 2.1: Graphical depiction of the possibly finite coin toss model. The final weight 1\n2 of the\nstate 2 corresponds to the probability p peos | yt´1 “ Tq “ 1\n2.\n4This also holds for the first example.\n2.1. AN INVITATION TO LANGUAGE MODELING\n9\nExample 2.1.2: Possibly Finite Coin Toss\nConsider now the possibly finite “coin toss” model with a rather peculiar coin: when tossing\nthe coin for the first time, both H and T are equally likely. After the first toss, however, the\ncoin gets stuck: If y1 “ H, we can only ever toss another H again, whereas if y1 “ T, the next\ntoss can result in another T or “end” the sequence of throws (eos) with equal probability. We,\ntherefore, model a probability distribution over tH, Tu˚ Y tH, Tu8, the set of finite and infinite\nsequences of tosses. Formally:a\nppH | yă1q “ ppT | yă1q “ 1\n2\nppH | yătq “\n#\n1\nif t ą 1 and yt´1 “ H\n0\nif t ą 1 and yt´1 “ T\nppT | yătq “\n#\n1\n2\nif t ą 1 and yt´1 “ T\n0\nif t ą 1 and yt´1 “ H\nppeos | yătq “\n#\n1\n2\nif t ą 1 and yt´1 “ T\n0\notherwise.\nIf you are familiar with (probabilistic) finite-state automata,b you can imagine the model\nas depicted in Fig. 2.1. It is easy to see that this model only places the probability of 1\n2 on\nfinite sequences of tosses. If we were only interested in those (analogously to how we are only\ninterested in finite strings when modeling language), yet still allowed the model to specify the\nprobabilities as in this example, the resulting probability distribution would not model what\nwe require.\naNote that ppH | yă1q “ ppH | εq and ppT | yă1q “ ppT | εq.\nbThey will be formally introduced in §4.1.5\nIt takes some mathematical heft to define a language model in a manner that avoids such\nparadoxes. The tool of choice for mathematicians is measure theory, as it allows us to define\nprobability over uncountable sets5 in a principled way. Thus, we begin our formal treatment of\nlanguage modeling with a primer of measure theory in §2.2. Then, we will use concepts discussed in\nthe primer to work up to a formal definition of a language model.\n5As stated earlier, tH, Tu8 is uncountable. It’s easy to see there exists a surjection from tH, Tu8 to the binary\nexpansion of the real interval p0, 1s. Readers who are interested in more details and mathematical implications can\nrefer to §1 in Billingsley (1995).\n10\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\n2.2\nA Measure-theoretic Foundation\nAt their core, (large) language models are an attempt to place a probabilistic distribution over\nnatural language utterances. However, our toy examples in Examples 2.1.1 and 2.1.2 in the previous\nsection reveal that it can be relatively tricky to get a satisfying definition of a language model. Thus,\nour first step forward is to review the basics of rigorous probability theory,6 the tools we need to\ncome to a satisfying definition. Our course will assume that you have had some exposure to rigorous\nprobability theory before, and just review the basics. However, it is also possible to learn the basics\nof rigorous probability on the fly during the course if it is new to you. Specifically, we will cover\nmeasure-theoretic foundations of probability theory. This might come as a bit of a surprise since\nwe are mostly going to be talking about language, which is made up of discrete objects—strings.\nHowever, as we will see in §2.5 soon, formal treatment of language modeling indeed requires some\nmathematical rigor from measure theory.\nThe goal of measure-theoretic probability is to assign probabilities to subsets of an outcome\nspace Ω. However, in the course of the study of measure theory, it has become clear that for\nmany common Ω, it is impossible to assign probabilities in a way that satisfies a set of reasonable\ndesiderata.7 Consequently, the standard approach to probability theory resorts to only assigning\nprobability to certain “nice” (but not necessarily all) subsets of Ω, which are referred to as events or\nmeasurable subsets, as in the theory of integration or functional analysis. The set of measurable\nsubsets is commonly denoted as F (Definition 2.2.1) and a probability measure P : F Ñ r0, 1s is the\nfunction that assigns a probability to each measurable subset. The triple pΩ, F, Pq is collectively\nknown as a probability space (Definition 2.2.2). As it turns out, the following simple and reasonable\nrequirements imposed on F and P are enough to rigorously discuss probability.\nDefinition 2.2.1: σ-algebra\nLet PpΩq be the power set of Ω. Then F Ď PpΩq is called a σ-algebra (or σ-field) over Ωif\nthe following conditions hold:\n1) ΩP F,\n2) if E P F, then Ec P F,\n3) if E1, E2, . . . is a finite or infinite sequence of sets in F, then Ť\nn En P F.\nIf F is a σ-algebra over Ω, we call the tuple pΩ, Fq a measurable space.\nExample 2.2.1: σ-algebras\nLet Ωbe any set. Importantly, there is more than one way to construct a σ-algebra over Ω:\n1. The family consisting of only the empty set H and the set Ω, i.e., F\ndef\n“ tH, Ωu, is called\nthe minimal or trivial σ-algebra.\n2. The full power set F\ndef\n“ PpΩq is called the discrete σ-algebra.\n6By rigorous probability theory we mean a measure-theoretic treatment of probability theory.\n7Measure theory texts commonly discuss such desiderata and the dilemma that comes with it. See, e.g., Chapter 7\nin Tao (2016), Chapter 3 in Royden (1988) or Chapter 3 in Billingsley (1995). We also give an example later.\n2.2. A MEASURE-THEORETIC FOUNDATION\n11\n3. Given A Ď Ω, the family F\ndef\n“ tH, A, ΩzA, Ωu is a σ-algebra induced by A.\n4. Suppose we are rolling a six-sided die. There are six events that can happen: We can\nroll any of the numbers 1–6. In this case, we will then define the set of outcomes Ωas\nΩ\ndef\n“ tThe number observed is n | n “ 1, . . . , 6u. There are of course multiple ways to\ndefine an event space F and with it a σ-algebra over this outcome space. By definition,\nH P F and ΩP F. One way to intuitively construct a σ-algebra is to consider that\nall individual events (observing any number) are possible, meaning that we would like\nto later assign probabilities to them (see Definition 2.2.2). This means that we should\ninclude individual singleton events in the event space: tThe number observed is nu P F\nfor n “ 1, . . . , 6. It is easy to see that in this case, to satisfy the axioms in Definition 2.2.1,\nthe resulting event space should be F “ PpΩq.\nYou might want to confirm these are indeed σ-algebras by checking them against the axioms\nin Definition 2.2.1.\nA measurable space guarantees that operations on countably many sets are always valid, and\nhence permits the following definition.\nDefinition 2.2.2: Probability measure\nA probability measure P over a measurable space pΩ, Fq is a function P : F Ñ r0, 1s such\nthat\n1) PpΩq “ 1,\n2) if E1, E2, . . . is a countable sequence of disjoint sets in F, then PpŤ\nn Enq “ ř\nn PpEnq.\nIn this case we call pΩ, F, Pq a probability space.\nAs mentioned, measure-theoretic probability only assigns probabilities to “nice” subsets of Ω. In\nfact, it is often impossible to assign a probability measure to every single subset of Ωand we must\nrestrict our probability space to a strict subset of PpΩq. More precisely, the sets B Ď Ωfor which a\nprobability (or more generally, a volume) can not be defined are called non-measurable sets. An\nexample of such sets is the Vitali set.8 See also Appendix A.2 in Durrett (2019).\nLater, we will be interested in modeling probability spaces over sets of (infinite) sequences. By\nvirtue of a theorem due to Carath´eodory, there is a natural way to construct such a probability\nspace for sequences (and many other spaces) that behaves in accordance with our intuition, as we\nwill clarify later. Here, we shall lay out a few other necessary definitions.\nDefinition 2.2.3: Algebra\nA Ď PpΩq is called an algebra (or field) over Ωif\n1) ΩP A,\n2) if E P A, then Ec P A,\n8See https://en.wikipedia.org/wiki/Non-measurable_set and https://en.wikipedia.org/wiki/Vitali_set.\n12\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\n3) if E1, E2 P A, then E1 Y E2 P A.\nDefinition 2.2.4: Probability pre-measure\nLet A be an algebra over some set Ω. A probability pre-measure over pΩ, Aq is a function\nP0 : A Ñ r0, 1s such that\n1) P0pΩq “ 1,\n2) if E1, E2, . . . is a (countable) sequence of disjoint sets in A whose (countable) union is\nalso in A, then P0pY8\nn“1Enq “ ř8\nn“1 P0pEnq.\nNote that the only difference between a σ-algebra (Definition 2.2.1) and an algebra is that\ncondition 3 is weakened from countable to finite, and the only difference between a probability\nmeasure (Definition 2.2.2) and a pre-measure is that the latter is defined with respect to an algebra\ninstead of a σ-algebra.\nThe idea behind Carath´eodory’s extension theorem is that there is often a simple construction\nof an algebra A over Ωsuch that there is a natural way to define a probability pre-measure. One\ncan then extend this probability pre-measure to a probability measure that is both minimal and\nunique in a precise sense. For example, the standard Lebesgue measure over the real line can be\nconstructed this way.\nFinally, we define random variables.\nDefinition 2.2.5: Random\nA mapping x : ΩÑ S between two measurable spaces pΩ, Fq and pS, T q is an pS, T q-valued\nrandom variable, or a measurable mapping, if, for all B P T ,\nx´1pBq\ndef\n“ tω P Ω: xpωq P Bu P F.\n(2.2)\nAny measurable function (random variable) induces a new probability measure on the output\nσ-algebra based on the one defined on the original σ-algebra. This is called the pushforward\nmeasure (cf. §2.4 in Tao, 2011), which we will denote by P˚, given by\nP˚ px P Eq\ndef\n“ P\n`\nx´1 pEq\n˘\n,\n(2.3)\nthat is, the probability of the result of x being in some event E is determined by the probability of\nthe event of all the elements which x maps into E, i.e., the pre-image of E given by x.\nExample 2.2.2: Random Variables\nWe give some simple examples of random variables.\n1. Let Ωbe the set of possible outcomes of throwing a fair coin, i.e., Ω\ndef\n“ tT, Hu. Define\n2.2. A MEASURE-THEORETIC FOUNDATION\n13\nF\ndef\n“ PpΩq, S\ndef\n“ t0, 1u, and T\ndef\n“ PpSq. Then, the random variable\nx :\n#\nT ÞÑ 0\nH ÞÑ 1\nassigns tails (T) the value 0 and heads (H) the value 1.\n2. Consider the probability space of throwing two dice (similar to Example 2.2.1) where\nΩ“ tpi, jq : i, j “ 1, . . . , 6u where the element pi, jq refers to rolling i on the first and\nj on the second die and F “ PpΩq. Define S\ndef\n“ Z and T\ndef\n“ PpSq. Then, the random\nvariable\nx : pi, jq ÞÑ i ` j\nis an pS, T q-valued random variable which represents the sum of two dice.\n14\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\n2.3\nLanguage Models: Distributions over Strings\nLanguage models are defined as probability distributions over sequences of words, referred to as\nutterances. This chapter delves into the formalization of the term “utterance” and introduces\nfundamental concepts such as the alphabet, string, and language. Utilizing these concepts, a formal\ndefinition of a language model is presented, along with a discussion on the intricacies of defining\ndistributions over infinite sets.\n2.3.1\nSets of Strings\nWe begin by defining the very basic notions of alphabets and strings, where we take inspiration from\nformal language theory. First and foremost, formal language theory concerns itself with sets of\nstructures. The simplest structure it considers is a string. So what is a string? We start with the\nnotion of an alphabet.\nDefinition 2.3.1: Alphabet\nAn alphabet is a finite, non-empty set. In this course, we will denote an alphabet using\nGreek capital letters, e.g., Σ and ∆. We refer to the elements of an alphabet as symbols or\nletters and will denote them with lowercase letters: a, b, c.\nDefinition 2.3.2: String\nA stringa over an alphabet is any finite sequence of letters. Strings made up of symbols from\nΣ will denoted by bolded Latin letters, e.g., y “ y1 ¨ ¨ ¨ yT where each yn P Σ.\naA string is also referred to as a word, which continues with the linguistic terminology.\nThe length of a string, written as |y|, is the number of letters it contains. Usually, we will use T\nto denote |y| more concisely whenever the usage is clear from the context. There is only one string\nof length zero, which we denote with the distinguished symbol ε and refer to as the empty string.\nBy convention, ε is not an element of the original alphabet.\nNew strings are formed from other strings and symbols with concatenation. Concatenation,\ndenoted with x ˝ y or just xy, is an associative operation on strings. Formally, the concatenation\nof two words y and x is the word y ˝ x “ yx, which is obtained by writing the second argument\nafter the first one. The result of concatenating with ε from either side results in the original string,\nwhich means that ε is the unit of concatenation and the set of all words over an alphabet with the\noperation of concatenation forms a monoid.\nWe have so far only defined strings as individual sequences of symbols. To give our strings made\nup of symbols in Σ a set to live in, we now define Kleene closure of an alphabet Σ.\nDefinition 2.3.3: Kleene Star\nLet Σ be an alphabet. The Kleene star Σ˚ is defined as\nΣ˚ “\n8\nď\nn“0\nΣn\n(2.4)\n2.3. LANGUAGE MODELS: DISTRIBUTIONS OVER STRINGS\n15\nwhere\nΣn def\n“ Σ ˆ ¨ ¨ ¨ ˆ Σ\nlooooomooooon\nn times\n(2.5)\nNote that we define Σ0 def\n“ tεu. We call the Σ˚ the Kleene closure of the alphabet Σ. We\nalso define\nΣ` def\n“\n8\nď\nn“1\nΣn “ ΣΣ˚.\n(2.6)\nFinally, we also define the set of all infinite sequences of symbols from some alphabet Σ as Σ8.\nDefinition 2.3.4: Infinite sequences\nLet Σ be an alphabet. The set of all infinite sequences over Σ is defined as:\nΣ8 def\n“ Σ ˆ ¨ ¨ ¨ ˆ Σ\nlooooomooooon\n8-times\n,\n(2.7)\nSince strings are canonically finite in computer science, we will explicitly use the terms infinite\nsequence or infinite string to refer to elements of Σ8.\nMore informally, we can think of Σ˚ as the set which contains ε and all (finite-length) strings\nwhich can be constructed by concatenating arbitrary symbols from Σ. Σ`, on the other hand, does\nnot contain ε, but contains all other strings of symbols from Σ. The Kleene closure of an alphabet\nis a countably infinite set (this will come into play later!). In contrast, the set Σ8 is uncountably\ninfinite for any Σ such that |Σ| ě 2.\nThe notion of the Kleene closure leads us very naturally to our next definition.\nDefinition 2.3.5: Formal language\nLet Σ be an alphabet. A language L is a subset of Σ˚.\nThat is, a language is just a specified subset of all possible strings made up of the symbols in the\nalphabet. This subset can be specified by simply enumerating a finite set of strings, or by a formal\nmodel. We will see examples of those later. Importantly, these strings are finite. If not specified\nexplicitly, we will often assume that L “ Σ˚.\nA note on terminology.\nAs we mentioned, these definitions are inspired by formal language\ntheory. We defined strings as our main structures of interest and symbols as their building blocks.\nWhen we talk about natural language, the terminology is often slightly different: we may refer\nto the basic building blocks (symbols) as tokens or words (which might be composed of one or\nmore characters and form some form of “words”) and their compositions (strings) as sequences or\nsentences. Furthermore, what we refer to here as an alphabet may be called a vocabulary (of\nwords or tokens) in the context of natural language. Sentences are therefore concatenations of words\nfrom a vocabulary in the same way that strings are concatenations of symbols from an alphabet.\n16\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nExample 2.3.1: Kleene Closure\nLet Σ “ ta, b, cu. Then\nΣ˚ “ tε, a, b, c, aa, ab, ac, ba, bb, bc, ca, cb, cc, aaa, aab, aac, . . . u.\nExamples of a languages over this alphabet include L1\ndef\n“ ta, b, ab, bau, L2\ndef\n“ ty P Σ˚ | y1 “ au,\nand L3\ndef\n“ ty P Σ˚ | |y| is evenu.\nNext, we introduce two notions of subelements of strings.\nDefinition 2.3.6: String Subelements\nA subsequence of a string y is defined as a sequence that can be formed from y by deleting\nsome or no symbols, leaving the order untouched. A substring is a contiguous subsequence.\nFor instance, ab and bc are substrings and subsequences of y “ abc, while ac is a subsequence\nbut not a substring. Prefixes and suffixes are special cases of substrings. A prefix is a\nsubstring of y that shares the same first letter as y and a suffix is a substring of y that shares\nthe same last letter as y. We will also denote a prefix y1 . . . yn´1 of the string y “ y1 . . . yT as\nyăn. We will also use the notation y Ÿ y1 to denote that y is a suffix of y1.\n2.3.2\nDefining a Language Model\nWe are now ready to introduce the main interest of the entire lecture series: language models.\nDefinition 2.3.7: Language model\nLet Σ be an alphabet. A language model is a (discrete) distribution pLM over Σ˚.\nExample 2.3.2: A very simple language model\nLet Σ\ndef\n“ tau. For n P Ně0, define\npLM panq\ndef\n“ 2´pn`1q,\nwhere a0 def\n“ ε and an def\n“ a . . . a\nloomoon\nn times\n.\nWe claim that pLM is a language model. To see that, we verify that it is a valid probability\ndistribution over Σ˚. It is easy to see that pLM panq ě 0 for any n. Additionally, we see that\nthe probabilities of finite sequences indeed sum to 1:\nÿ\nyPΣ˚\npLM pyq “\n8\nÿ\nn“0\npLM panq “\n8\nÿ\nn“0\n2´pn`1q “ 1\n2\n8\nÿ\nn“0\n2´n “ 1\n2\n1\n1 ´ 1\n2\n“ 1.\nIn our formal analysis of language models, we will also often refer to the language defined by a\nlanguage model.\n2.3. LANGUAGE MODELS: DISTRIBUTIONS OVER STRINGS\n17\nDefinition 2.3.8: Weighted language\nLet pLM be a language model. The weighted language of pLM is defined as\nL ppLMq\ndef\n“ tpy, pLM pyqq | y P Σ˚u\n(2.8)\nExample 2.3.3: Languge of a langauge model\nThe language of the language model from Example 2.3.2 is\nL ppLMq\ndef\n“\n!´\nan, 2´pn`1q¯\n| n P Ně0\n)\n(2.9)\nA language model is itself a very simple concept—it is simply a distribution that weights strings\n(natural utterances) by their probabilities to occur in a particular language. Note that we have\nnot said anything about how we can represent or model this distribution yet. Besides, for any\n(natural) language, the ground-truth language model pLM is of course unknown and complex. The\nnext chapter, therefore, discusses in depth the computational models which we can use to try to\ntractably represent distributions over strings and ways of approximating (learning) the ground-truth\ndistribution based on finite datasets using such models.\n18\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\n2.4\nGlobal and Local Normalization\nThe previous chapter introduced a formal definition of a language as a set of strings and the definition\nof a language model as a distribution over strings. We now delve into a potpourri of technical\nquestions to complete the theoretical minimum for discussing language models. While doing so, we\nwill introduce (and begin to answer) three fundamental questions in the first part of the course. We\nwill introduce them later in the section.\nA note on terminology.\nUnfortunately, we will encounter some ambiguous terminology. In §2.5,\nwe explicitly define a language model as a valid probability distribution over Σ˚, the Kleene closure\nof some alphabet Σ, which means that ř\nyPΣ˚ pLM pyq “ 1. As we will see later, this means that\nthe model is tight, whereas it is non-tight if ř\nyPΣ˚ pLM pyq ă 1. Definitionally, then, all language\nmodels are tight. However, it is standard in the literature to refer to many non-tight language\nmodels as language models as well. We pardon in advance the ambiguity that this introduces. Over\nthe course of the notes, we attempt to stick to the convention that the term “language model”\nwithout qualification only refers to a tight language model whereas a “non-tight language model” is\nused to refer to a language model in the more colloquial sense. Linguistically, tight is acting as a\nnon-intersective adjective. Just as in English, where a fake gun is not a gun, so too in our course\nnotes a non-tight language model is not a language model. This distinction does in fact matter. On\none hand, we can prove that many language models whose parameters are estimated from data\n(e.g., a finite-state language model estimated by means of maximum-likelihood estimation) are, in\nfact, tight. On the other hand, we can show that this is not true in general, i.e., not all language\nmodels estimated from data will be tight. For instance, a recurrent neural network language model\nestimated through gradient descent may not be tight (Chen et al., 2018).\nWhen specifying pLM, we have two fundamental options. Depending on whether we model\npLM pyq for each string y directly or we model individual conditional probabilities pLM pyn | yătq we\ndistinguish globally and locally normalized models. The names naturally come from the way the\ndistributions in the two families are normalized: whereas globally normalized models are normalized\nby summing over the entire (infinite) space of strings, locally normalized models define a sequence of\nconditional distributions and make use of the chain rule of probability to define the joint probability\nof a whole string.\nThe beginning of sequence string symbol.\nConventionally, we will include a special symbol\nover which globally or locally normalized models operate: the beginning of sequence (bos)\nsymbol, which, as the name suggests, denotes the beginning of a string or a sequence. For a string\ny “ y1 ¨ ¨ ¨ yT , we will suggestively denote y0\ndef\n“ bos.\n2.4.1\nGlobally Normalized Language Models\nWe start with globally normalized models. Such models are also called energy-based language\nmodels in the literature (Bakhtin et al., 2021). To define a globally normalized language model, we\nstart with the definition of an energy function.\nDefinition 2.4.1: Energy function\nAn energy function is a function pp : Σ˚ Ñ R.\n2.4. GLOBAL AND LOCAL NORMALIZATION\n19\nInspired by concepts from statistical mechanics, an energy function can be used to define a very\ngeneral class of probability distributions by normalizing its exponentiated negative values.\nNow, we can define a globally normalized language model in terms of an energy function over\nΣ˚.\nDefinition 2.4.2: Globally normalized models\nLet ppGN pyq : Σ˚ Ñ R be an energy function. A globally normalized model (GNM) is\ndefined as\npLM pyq\ndef\n“\nexp r´ppGN pyqs\nř\ny1PΣ˚ exp r´ppGN py1qs\ndef\n“\n1\nZG\nexp r´ppGN pyqs ,\n(2.10)\nwhere ZG\ndef\n“ ř\ny1PΣ˚ exp r´ppGN py1qs.a We call ZG the normalization constant.\naWe will later return to this sort of normalization when we define the softmax function in §3.1.\nGlobally normalized models are attractive because one only needs to define an (unnormalized)\nenergy function ppGN, which scores entire sequences at once. This is often easier than specifying a\nprobability distribution. Furthermore, they define a probability distribution over strings y P Σ˚\ndirectly. As we will see in §2.4.2, this stands in contrast to locally normalized language models\nwhich require care with the space over which they operate. However, the downside is that it may be\ndifficult to compute the normalizer ZG.\nNormalizability\nIn defining the normalizer ZG\ndef\n“ ř\ny1PΣ˚ exp r´ppGN py1qs, we notationally cover up a certain subtlety.\nThe set Σ˚ is countably infinite, so ZG may diverge to 8. In this case, Eq. (2.10) is not well-defined.\nThis motivates the following definition.\nDefinition 2.4.3: Normalizable energy function\nWe say that an energy function is normalizable if the quantity ZG in Eq. (2.10) is finite, i.e.,\nif ZG ă 8.\nWith this definition, we can state a relatively trivial result that characterizes when an energy\nfunction can be turned into a globally normalized language model.\nTheorem 2.4.1: Normalizable energy functions induce language models\nAny normalizable energy function pGN induces a language model, i.e., a distribution over Σ˚.\n20\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nProof. Given an energy function ppGN, we have exp r´ppGN pyqs ě 0 and\nÿ\nyPΣ˚\npGN pyq “\nÿ\nyPΣ˚\nexp r´ppGN pyqs\nř\ny1PΣ˚ exp r´ppGN py1qs\n(2.11)\n“\n1\nř\ny1PΣ˚ exp r´ppGN py1qs\nÿ\nyPΣ˚\nexp r´ppGN pyqs\n(2.12)\n“ 1,\n(2.13)\nwhich means that pGN is a valid probability distribution over Σ˚.\n■\nWhile the fact that normalizable energy functions always form a language model is a big\nadvantage, we will see later that ensuring that they are normalizable can be difficult and restrictive.\nThis brings us to the first fundamental question of the section:\nQuestion 2.1: Normalizing an energy function\nWhen is an energy function normalizable? More precisely, for which energy functions ppGN is\nZG ă 8?\nWe will not discuss any specific results here, as there are no general necessary or sufficient\nconditions—the answer to this of course depends on the precise definition of ppGN. Later in the course\nnotes, we will present two formalisms where we can exactly characterize when an energy function\nis normalizable. First, when it is weighted finite-state automaton (cf. §4.1), and, second, when it\nis defined through weighted context-free grammars (§4.2) and discuss the specific sufficient and\nnecessary conditions there. However, under certain assumptions, determining whether an energy\nfunction is normalizable in the general case is undecidable.\nMoreover, even if it is known that an energy function is normalizable, we still need an efficient\nalgorithm to compute it. But, efficiently computing ZG can be challenging: the fact that Σ˚\nis infinite means that we cannot always compute ZG in a tractable way. In fact, there are no\ngeneral-purpose algorithms for this. Moreover, sampling from the model is similarly intractable, as\nentire sequences have to be drawn at a time from the large space Σ˚.\n2.4.2\nLocally Normalized Language Models\nThe inherent difficulty in computing the normalizer, an infinite summation over Σ˚, motivates\nthe definition of locally normalized language models, which we will denote with pLN. Rather than\ndefining a probability distribution over Σ˚ directly, they decompose the problem into the problem\nof modeling a series of conditional distributions over the next possible symbol in the string given\nthe context so far, i.e., pLN py | yq, which could be na¨ıvely combined into the full probability of the\nstring by multiplying the conditional probabilities.9 Intuitively, this reduces the problem of having\nto normalize the distribution over an infinite set Σ˚ to the problem of modeling the distribution of\nthe next possible symbol yn given the symbols seen so far yăn. This means that normalization would\nonly ever require summation over |Σ| symbols at a time, solving the tractability issues encountered\nby globally normalized models.\n9We will soon see why this would not work and why we have to be a bit more careful.\n2.4. GLOBAL AND LOCAL NORMALIZATION\n21\nHowever, we immediately encounter another problem: In order to be a language model, pLN py | yq\nmust constitute a probability distribution over Σ˚. However, as we will discuss in the next section,\nthis may not be the case because locally normalized models can place positive probability mass on\ninfinitely long sequences (cf. Example 2.5.1 in §2.5.1). Additionally, we also have to introduce a new\nsymbol that tells us to “stop” generating a string, which we call the end of sequence symbol, eos.\nThroughout the notes, we will assume eos R Σ and we define\nΣ\ndef\n“ Σ Y teosu .\n(2.14)\nMoreover, we will explicitly denote elements of Σ\n˚ as y and symbols in Σ as y. Given a sequence\nof symbols and the eos symbol, we take the string to be the sequence of symbols encountered\nbefore the first eos symbol. Informally, you can think of the bos symbol as marking the beginning\nof the string, and the eos symbol as denoting the end of the string or even as a language model\nterminating its generation, as we will see later.\nDue to the issues with defining valid probability distributions over Σ˚, we will use the term\nsequence model to refer to any model that may place positive probability on infinitely long sequences.\nThus, sequence models are strictly more general than language models, which, by definition, only\nplace positive probability mass on strings, i.e., finite sequences.\nDefinition 2.4.4: Sequence model\nLet Σ be an alphabet. A sequence model (SM) over Σ is defined as a set of conditional\nprobability distributions\npSM py | yq\n(2.15)\nfor y P Σ and y P Σ˚. We will refer to the string y in pSM py | yq as the history or the\ncontext.\nNote that we will mostly consider SMs over the set Σ. To reiterate, we have just formally defined\nlocally normalized sequence models rather than locally normalized language models. That has to do\nwith the fact that, in contrast to a globally normalized model with a normalizable energy function,\na SM might not correspond to a language model, as alluded to at the beginning of this section and\nas we discuss in more detail shortly.\nWe will now work up to a locally normalized language model.\nDefinition 2.4.5: Locally normalized language model\nLet Σ be an alphabet. Next, let pSM be a sequence model over Σ. A locally normalized\nlanguage model (LNM) over Σ is defined as\npLNpyq\ndef\n“ pSMpeos | yq\nT\nź\nt“1\npSMpyt | yătq\n(2.16)\nfor y P Σ˚ with |y| “ T. We say a locally normalized language model is tight if\nÿ\nyPΣ˚\npLNpyq “ 1.\n(2.17)\nTightness is a nuanced concept that will be discussed in great detail in §2.5.\n22\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nWe now contrast globally and locally normalized models pictorially in the following example.\nExample 2.4.1: Locally and globally normalized language models\nFig. 2.2a shows a simple instance of what a locally normalized language model would look\nlike. We can compute the probabilities of various strings by starting at the root node bos\nand choosing one of the paths to a leaf node, which will always be eos. The values on the\nedges represent the conditional probabilities of observing the new word given at the target of\nthe edge given the context seen on the path so far, i.e., pLN pyt | yătq at the level t of the tree.\nFor example, the probability of the string bos “The best” eos under this language model is\n0.04 ¨ 0.13 ¨ 0.22 “ 0.001144. On the other hand, a globally normalized model would simply\nscore all possible sentences using the score function ppGN pyq, as is hinted at in Fig. 2.2b.\nLocally Normalizing a Language Model\nThe second fundamental question of this section concerns the relationship between language models\nand local normalization.\nQuestion 2.2: Locally normalizing a language model\nWhen can a language model be locally normalized?\nThe answer to that is simple: every language model can be locally normalized! While the\nintuition behind this is very simple, the precise formulation is not. Before we discuss the details, we\nhave to introduce the concept of prefix probabilities, which denote the sum of the probabilities of all\nstrings beginning with a certain prefix.\nDefinition 2.4.6: Prefix probability\nLet pLM be a language model. We define a pLM’s prefix probability π as\nπ pyq\ndef\n“\nÿ\ny1PΣ˚\npLM\n`\nyy1˘\n,\n(2.18)\nthat is, the probability that y is a prefix of any string yy1 in the language, or, equivalently,\nthe cumulative probability of all strings beginning with y.\nNote that, naturally, π pεq “ 1.\nTheorem 2.4.2: Any language model can be locally normalized\nLet pLM be a language model. Then, there exists a locally normalized language model pLN\nsuch that, for all y P Σ˚ with |y| “ T,\npLM pyq “ pLN pyq “ pSMpeos | yq\nT\nź\nt“1\npSMpyt | yătq.\n(2.19)\n2.4. GLOBAL AND LOCAL NORMALIZATION\n23\nbos\nHello\nthere\n¨ ¨ ¨\nworld\n¨ ¨ ¨\n0.21\n0.06\n¨ ¨ ¨\nPlease\nconsider\n¨ ¨ ¨\ndon’t\n¨ ¨ ¨\n0.09\n0.02\nThe\nbest\n!\neos\n1\neos\n0.22\n0.07\nquick\nand\n¨ ¨ ¨\nbrown\n¨ ¨ ¨\n0.12\n0.01\n0.08\n0.13\n0.04\n0.01\n0.03\n(a) An example of a locally normalized language model. The values of the edges represent the conditional\nprobability of observing the new word given the observed words (higher up on the path from the root node\nbos). Note that the probabilities stemming from any inner node should sum to 1—however, to avoid clutter,\nonly a subset of the possible arcs is drawn.\ny „\nppGN p The best q\nppGN p The best! q\nppGN p The quick fox. q\nppGN p Hello World! q\n(b) An example of a globally normalized model which can for example generate sentences based on the\nprobabilities determined by normalizing the assigned scores ppGN.\nFigure 2.2: “Examples” of a locally and a globally normalized language model.\n24\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nProof. We define the individual conditional probability distributions over the next symbol of the\nSM pSM using the chain rule of probability. If πpyq ą 0, then define\npSM py | yq\ndef\n“ π pyyq\nπ pyq\n(2.20)\nfor y P Σ and y P Σ˚ such that p pyq ą 0. We still have to define the probabilities of ending the\nsequence using pSM by defining the eos probabilities. We define, for any y P Σ˚ such that πpyq ą 0,\npSM peos | yq\ndef\n“ pLMpyq\nπpyq\n(2.21)\nthat is, the probability that the globally normalized model will generate exactly the string y and\nnot any continuation of it yy1, given that y has already been generated. Each of the conditional\ndistributions of this model (Eqs. (2.20) and (2.21)) is clearly defined over Σ. This, therefore, defines\na valid SM. To see that pLN constitutes the same distribution as pLM, consider two cases.\nCase 1:\nAssume πpyq ą 0. Then, we have\npLN pyq “\n« T\nź\nt“1\npSM pyt | yătq\nff\npSM peos | yq\n(2.22)\n“ \b\b\b\nπ py1q\nπ pεq\n\u0018\u0018\u0018\u0018\nπ py1y2q\n\b\b\b\nπ py1q\n¨ ¨ ¨ \u0018\u0018\u0018\u0018\nπ pyăT q\n\u0018\u0018\u0018\u0018\u0018\nπ pyăT ´1q\nπ pyq\n\u0018\u0018\u0018\u0018\nπ pyăT q pSM peos | yq\n“ \b\b\b\nπ pyq\nπ pεq\npLM pyq\n\b\b\b\nπ pyq\n(2.23)\n“ pLM pyq\n(2.24)\nwhere π pεq “ 1.\nCase 2:\nAssume πpyq “ 0. Let y “ y1 ¨ ¨ ¨ yT . Then, there must exist a 1 ď t1 ď T such that\nπpyăt1q “ 0. Note that\npLNpyq “\nt1\nź\nt“1\npSMpyt | yătq “ 0\n(2.25)\nwhereas the conditional probabilities after t1 can be arbitrarily defined since they do not affect the\nstring having 0 probability.\n■\nWhen Is a Locally Normalized Language Model a Language Model?\nLNMs which specify distributions over strings pLN py1 . . . yT q in terms of their conditional probabilities\npSMpyt | yătq for t “ 1, . . . , T and pSM peos | yq have become the standard in NLP literature. However,\nLNMs come with their own set of problems. An advantage of normalizable globally normalized\nmodels is that they, by definition, always define a valid probability space over Σ. Although this might\nbe counterintuitive at first, the same cannot be said for LNMs—in this sense, locally normalized\n“language models” might not even be language models! One might expect that in a LNM pLN, it\nwould hold that ř\nyPΣ˚ pLN pyq “ 1. However, this might not be the case! This is the issue with the\nterminology we brought up earlier and it brings us to the last fundamental question of this section.\n2.4. GLOBAL AND LOCAL NORMALIZATION\n25\nQuestion 2.3: Locally normalized language models\nWhen does an LNM encode a language model?\nAs the conditions are a bit more nuanced, it requires a longer treatment. We explore this issue\nin much more detail in the next section.\n26\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\n2.5\nTight Language Models\nWe saw in the last section that any language model pLM can be converted into a locally normalized\nsequence model (cf. §2.4.2). The converse, however, is not true. As alluded to in the previous\nsection and as we detail in this section, there exist sets of conditional distributions pLN py | yq over\nΣ\n˚ such that pLN pyq as defined in Eq. (2.15) does not represent a valid probability measure over Σ˚\n(after taking into account the semantics of eos), i.e., over the set of finite strings. Indeed, we will\nlater show that some popular classes of locally normalized sequence models used in practice have\nparameter settings in which the generative process terminates with probability ă 1. This means\nthat pLN “leaks” some of its probability mass to infinite sequences. This section investigates this\nbehavior in a lot of detail. It is based on the recent work from Du et al. (2022).\n2.5.1\nTightness\nModels whose generative process may fail to terminate are called non-tight (Chi, 1999).10\nDefinition 2.5.1: Tightness\nA locally normalized language model pLN derived from a sequence model pSM is called tight if\nit defines a valid probability distribution over Σ˚:\nÿ\nyPΣ˚\npLN pyq “\nÿ\nyPΣ˚\n«\npSM peos | yq\nT\nź\nt“1\npSM pyt | yătq\nff\n“ 1.\n(2.26)\nNote that the individual conditional distributions pSM py | yq in a non-tight LNM still are valid\nconditional distributions (i.e., they sum to one). However, the distribution over all possible strings\nthat they induce may not sum to 1. To be able to investigate this phenomenon more closely, let us\nfirst examine what the conditional probabilities of an LNM actually define and how they can result\nin non-tightness. We now ask ourselves: given a sequence model pSM, what is pLN? Is pLN a language\nmodel, i.e., a distribution over Σ˚ (after taking into account the semantics of eos)? Certainly, the\nanswer is yes if the LNM’s conditional probabilities match the conditional probabilities of some\nknown language model pLM as defined in §2.4.2,11 in which case pLN is specifically the language\nmodel pLM itself. In this case clearly pLN pΣ˚q\ndef\n“ ř\nyPΣ˚ pLN pyq “ ř\nyPΣ˚ pLM pyq “ 1. If instead\npLN pΣ˚q ă 1, the LNM’s conditional probabilities do not match the conditional probabilities of any\nlanguage model pLM.\nTo see how this can happen, we now exhibit such an LNM in the following example.\nExample 2.5.1: A non-tight 2-gram model\nConsider the bigram model defined in Fig. 2.3a over the alphabet Σ “ ta, bu.a Although the\nconditional probability distributions pLNp¨ | yănq each sum to 1 over Σ, they fail to combine\ninto a model pLN that sums to 1 over Σ˚ (i.e., a language model): under this model, any finite\n10Tight models are also called consistent (Booth and Thompson, 1973; Chen et al., 2018) and proper (Chi, 1999)\nin the literature.\n11That is, pLMpyt | yătq “ pLNpyt | yătq whenever the former conditional probability is well-defined under the\nlanguage model pLM, i.e., whenever yt P Σ and yăt P Σ˚ with pLMpyătq ą 0.\n2.5. TIGHT LANGUAGE MODELS\n27\nstring that contains the symbol b will have probability 0, since pLNpeos | bq “ pLNpa | bq “ 0.\nThis implies pLN pΣ˚q “ ř8\nn“0 pLNpanq “ ř8\nn“0p0.7qn ¨ 0.1 “\n0.1\n1´0.7 “ 1\n3 ă 1.\naThe graphical representation of the LNM depicts a so-called weighted finite-state automaton, a framework\nof language models we will introduce shortly. For now, it is not crucial that you understand the graphical\nrepresentation and you can simply focus on the conditional probabilities specified in the figure.\nExample 2.5.2: A tight 2-gram model\nOn the other hand, in the bigram model in Fig. 2.3b, obtained from Example 2.5.1 by changing\nthe arcs from the b state, pLN pΣ˚q “ 1. We can see that by calculating:\nPpΣ˚q “\n8\nÿ\nn“1\n8\nÿ\nm“0\nPpanbmq\n“\n8\nÿ\nn“1\n˜\nPpanq `\n8\nÿ\nm“1\nPpanbmq\n¸\n“\n8\nÿ\nn“1\n˜\n0.1 ¨ p0.7qn´1 `\n8\nÿ\nm“1\np0.7qn´1 ¨ 0.2 ¨ p0.9qm´1 ¨ 0.1\n¸\n“\n8\nÿ\nn“1\nˆ\n0.1 ¨ p0.7qn´1 ` p0.7qn´1 ¨ 0.2 ¨\n1\n1 ´ 0.9 ¨ 0.1\n˙\n“\n8\nÿ\nn“1\n`\n0.1 ¨ p0.7qn´1 ` 0.2 ¨ p0.7qn´1˘\n“\n8\nÿ\nn“1\n0.3 ¨ p0.7qn´1 “\n0.3\n1 ´ 0.7 “ 1.\nExample 2.5.1 confirms that the local normalization does not necessarily yield pLN that is a valid\ndistribution over Σ˚. But if pLN is not a language model, what is it? It is intuitive to suspect that,\nin a model with pLN pΣ˚q ă 1, the remainder of the probability mass “leaks” to infinite sequences,\ni.e., the generative process may continue forever with probability ą 0. This means that, to be able\nto characterize pLN, we will have to be able to somehow take into account infinite sequences. We\nwill make this intuition formal below.\nDelving a bit deeper, the non-tightness of Example 2.5.1 is related to the fact that the conditional\nprobability of eos is 0 at some states, in contrast to Example 2.5.2. However, requiring pLNpyn “\neos | yănq ą 0 for all prefixes yăn is neither necessary nor sufficient to ensure tightness. It is not\nnecessary because one can, for example, construct an LNM in which pLNpyn “ eos | yănq “ 0.1\nwhen n is even but “ 0 otherwise. Such a model generates only odd-length strings but is tight.\nWe will postpone non-sufficienty for later, where we will present specific LNMs under which the\nconditional probability of eos is always ą 0, yet are non-tight.\n28\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\npLNpa | bosq\n1\npLNpa | aq\n0.7\npLNpb | aq\n0.2\npLNpeos | aq\n0.1\npLNpb | bq\n1\npLNpeos | eosq\n1\na\nb\neos\na{0.7\nb{1\neos{1\nb{0.2\neos{0.1\n(a) A non-tight 2-gram model.\npLNpa | bosq\n1\npLNpa | aq\n0.7\npLNpb | aq\n0.2\npLNpeos | aq\n0.1\npLNpb | bq\n0.9\npLNpeos | bq\n0.1\npLNpeos | eosq\n1\na\nb\neos\na{0.7\nb{0.9\neos{0.1\neos{1\nb{0.2\neos{0.1\n(b) A tight 2-gram model.\nFigure 2.3: Tight and non-tight bigram models, expressed as Mealy machines. Symbols with\nconditional probability of 0 are omitted.\n2.5.2\nDefining the probability measure of an LNM\nWe now rigorously characterize the kind of distribution induced by an LNM, i.e., we investigate what\npLN is. As mentioned earlier, an LNM can lose probability mass to the set of infinite sequences, Σ8.\nHowever, Σ8, unlike Σ˚, is uncountable, and it is due to this fact that we need to work explicitly with\nthe measure-theoretic formulation of probability which we introduced in §2.2. We already saw the\nperil of not treating distributions over uncountable sets carefully is necessary in Example 2.1.1—the\nset of all infinite sequences of coin tosses is indeed uncountable.\nIncluding infinite strings and the end of string symbol.\nAs we saw in Example 2.1.1,\nsampling successive symbols from a non-tight LNM has probability ą 0 of continuing forever, i.e.,\ngenerating infinite strings. Motivated by that, we hope to regard the LNM as defining a valid\nprobability space over Ω“ Σ˚ Y Σ8, i.e., both finite as well as infinite strings, and then “relate” it\nto our definition of true language models. Notice, however, that we also have to account for the\ndifference in the alphabets: while we would like to characterize language models in terms of strings\nover the alphabet Σ, LNMs work over symbols in Σ.\nWith this in mind, we now embark on our journey of discovering what pLN represents. Given an\nLNM, we will first need to turn its pLN into a measurable space by defining an appropriate σ-algebra.\nThis type of distribution is more general than a language model as it works over both finite as\nwell as infinite sequences. To distinguish the two, we will expand our vocabulary and explicitly\ndifferentiate between true language models and non-tight LNMs. We will refer to a distribution over\nΣ˚ Y Σ8 as a sequence model. As noted in our definition of a sequence model (cf. Definition 2.4.4),\n2.5. TIGHT LANGUAGE MODELS\n29\nΣ\n8\nAlgebra\n´\nΣ\n8, C\n¯\nPre-measure\n´\nΣ\n8, C, P0\n¯\nMeasure\n´\nΣ\n8, σ\n`\nC\n˘\n, P\n¯\nMeasure (sequence model)\npΣ8 Y Σ˚, σ pCq , P1q\nCylinder\nsets\nConditional prob-\nabilities from pLN\nCarath´eodory’s Extension\nRandom vari-\nable construction\nFigure 2.4: The outline of our measure-theoretic treatment of LNMs in this section to arrive at\na precise characterization of pLN. The final box corresponds to the sequence model (probability\nmeasure over Σ˚ Y Σ8) constructed for pLN.\nan LNM defines a probabilty measure over Σ˚ Y Σ8. Thus, an equivalent distribution, which will\nbe useful for this section, would be the following.\nDefinition 2.5.2: Sequence model\nA sequence model is a probability space over the set Σ˚ Y Σ8.\nIntuitively, and we will make this precise later, the set Σ8 Ă Σ˚ Y Σ8 in Definition 2.5.2\nrepresents the event where the sequence model is non-terminating, i.e., it attempts to generate an\ninfinitely long sequence. We can then understand language models in a new sense.\nDefinition 2.5.3: Re-definition of a Language model\nA language model is a probability space over Σ˚. Equivalently, a language model is a\nsequence model such that PpΣ8q “ 0.\nNow buckle up! Our goal through the rest of this section is to rigorously construct a probability\nspace of a sequence model as in Definition 2.2.2 and Definition 2.5.2 which encodes the probabilities\nassigned by an LNM. Then, we will use this characterization to formally investigate tightness. An\noutline of what this is going to look like is shown in Fig. 2.4.\nDefining an Algebra over Σ\n8 (Step 1)\nSince an LNM produces conditional distributions over the augmented alphabet Σ (first box in\nFig. 2.4) and results in possibly infinite strings, we will first construct a probability space over Σ\n8,\nwhich will naturally induce a sequence model. We will do that by first constructing an algebra (cf.\n30\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nDefinition 2.2.3) over Ω“ Σ\n8 for some alphabet Σ (second box in Fig. 2.4). Then, assuming we\nare given an LNM pLN over Σ, we will associate the constructed algebra with a pre-measure (cf.\nDefinition 2.2.4) that is “consistent” with pLN (third box in Fig. 2.4).\nWe will make use of the following definition to construct the algebra:\nDefinition 2.5.4: Cylinder set\nGiven any set H Ď Σ\nk, i.e., a set of sequences of symbols from Σ of length k, define its\ncylinder set (of rank k) to be\nCpHq\ndef\n“\n!\nyω : y P H, ω P Σ\n8)\n(2.27)\nIn essence, a cylinder set of rank k is the set of infinite strings that share their k-prefix with some\nstring y P H Ď Σ\nk. In particular, for a length-k string y “ y1 ¨ ¨ ¨ yk, the cylinder set Cpyq\ndef\n“ Cptyuq\nis the set of all infinite strings prefixed by y.12\nWe denote the collection of all rank-k cylinder sets by\nCk\ndef\n“\n!\nCpHq : H P P\n´\nΣ\nk¯)\n(2.28)\nand define\nC\ndef\n“\n8\nď\nk“1\nCk\n(2.29)\nto be the collection of all cylinder sets over Ω.13\nThe following lemma asserts C Ď PpΩq is what we want in the second block of Fig. 2.4.\nLemma 2.5.1\nC Ď PpΩq is an algebra over Ω“ Σ\n8.\nProof. First, Σ\n8 “ CpΣ\nkq for any k, and in particular is a cylinder set of any rank. Secondly,\ngiven a cylinder set CpHq of rank k, i.e., H Ď Σ\nk,\n`\nCpHq\n˘c “ C\n´\nΣ\nkzH\n¯\n. Hence, C is closed under\ncomplements. Finally, notice that the intersection of two cylinder sets of ranks k1 ď k2 is another\ncylinder set of rank k2. Hence, C is an algebra over Ω.\n■\nWith this, the first step of Fig. 2.4 is done!\nDefining a Pre-measure over C (Step 2)\nWe are now ready to define the pre-measure P0 for the cylinder algebra C. Given an LNM pLN and\nany set CpHq P C, let\nP0pCpHqq\ndef\n“\nÿ\nyPH\npLNpyq\n(2.30)\n12This type of cylinder set, i.e., one that is generated by a singleton, is also called a thin cylinder.\n13We invite the reader to verify that C1 Ă C2 Ă C3 Ă ¨ ¨ ¨ .\n2.5. TIGHT LANGUAGE MODELS\n31\nwhere we have defined\npLNpyq\ndef\n“\nT\nź\nt“1\npLNpyt | yătq.\n(2.31)\nNote that there is a caveat here since the same cylinder set may admit different H.14 Before showing\nthat P0 defines a valid pre-measure, we address this and show that P0 is indeed well defined.\nProposition 2.5.1\nP0 as defined in Eq. (2.30) is a well-defined function.\nProof. Suppose a cylinder set can be described by two different prefix sets: H1 Ď Σ\nk1 and H2 Ď Σ\nk2.\nIn other words, CpH1q “ CpH2q. Without loss of generality, assume that k1 ď k2. Then,\nCpH2q “ CpH1q\n(2.32a)\n“\nď\nyPH1\nCpyq\n(2.32b)\n“\nď\nyPH1\nď\nyPΣ\nk2´k1\nCpyyq.\n(2.32c)\nAll the unions above are disjoint, and hence H2 “ Ť\nyPΣ\nk2´k1tyy : y P H1u.\nThen, by the\nlocally-normalizing property of pLN, we have that\nP0pCpH1qq “ P0pCpH2qq.\n(2.33)\n■\nWith this, we are able to state and prove the lemma which shows that P0 is a pre-measure, which\nis what we need in the third block of Fig. 2.4.\nLemma 2.5.2\nP0 is a pre-measure over C.\nFor the proof of Lemma 2.5.2, we will mostly follow the proof of Theorem 2.3 in Billingsley\n(1995), with the exception of invoking the Tychonoff theorem directly. This proof depends on the\nfollowing lemma, which is Example 2.10 in Billingsley (1995). We repeat the statement and proof\nhere for the reader’s convenience.\nLemma 2.5.3\nLet P0 be a finitely additive probability pre-measure over C such that, given a decreasing\nsequence of sets A1 Ą A2 Ą ¨ ¨ ¨ in C where Ş8\nn“1 An “ H, limnÑ8 P0pAnq “ 0. Then, P0 is\nalso countably additive over C.\n14For example, in the infinite coin toss model, CpHq “ CptHH, HTuq.\n32\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nProof. Let tAnu be a sequence of disjoint sets in C such that A “ Ť\nn An P C. Then, defining\nBn “ Ť\nmąn Am, we see that B1 Ą B2 Ą ¨ ¨ ¨ and Ş\nn Bn “ H. Notice that\nA “ A1 Y B1 “ A1 Y A2 Y B2 “ ¨ ¨ ¨ “ A1 Y ¨ ¨ ¨ Y An Y Bn\n(2.34)\nfor any n and hence by finite additivity of P0\nP0pAq “ P0pA1q ` ¨ ¨ ¨ ` P0pAnq ` P0pBnq\n(2.35)\nor equivalently\nP0pA1q ` ¨ ¨ ¨ ` P0pAnq “ P0pAq ´ P0pBnq.\n(2.36)\nSince, Bn Ó H implies that P0pBnq Ó 0 by assumption, taking the limits on both sides of Eq. (2.36)\nyields\nÿ\nn\nP0pAnq “ lim\nnÑ8\nÿ\niďn\nP0pAiq “ P0pAq ´ lim\nnÑ8 P0pBnq “ P0pAq\n(2.37)\nwhich shows countable additivity.\n■\nWe also recall the Tychonoff theorem.15\nTheorem 2.5.1: Tychonoff\nLet tXαuαPJ be an indexed family of compact topologies. Then, their product topology\nś\nαPJ Xα is also compact.\nWe can now give the proof for Lemma 2.5.2.\nProof of Lemma 2.5.2. We first show that P0 is finitely additive over C. Let CpH1q and CpH2q be\ntwo disjoint cylinder sets. By Proposition 2.5.1, we can assume they are of the same rank without\nloss of generality. Then,\nCpH1q Y CpH2q “\nď\nyPH1\ntyω : ω P Σ\n8u Y\nď\nyPH2\ntyω : ω P Σ\n8u\n(2.38a)\n“\nď\nyPH1YH2\ntyω : ω P Σ\n8u\n(H1 and H2 equal rank and disjoint)\n(2.38b)\n“ CpH1 Y H2q\n(2.38c)\nwhich leads to\nP0pCpH1q Y CpH2qq “ P0pCpH1 Y H2qq\n(2.39a)\n“\nÿ\nyPH1YH2\npLNpyq\n(2.39b)\n“ P0pCpH1qq ` P0pCpH2qq.\n(2.39c)\nHence, P0 is finitely additive.\nNow, equip Σ with the discrete topology. Since Σ is finite, it is compact under the discrete\ntopology and so is Σ\n8 by Theorem 2.5.1. Then, by properties of the product topology over discrete\n15See §37 in Munkres (2000) for a detailed and well-written treatise.\n2.5. TIGHT LANGUAGE MODELS\n33\nfinite spaces, all cylinder sets in Σ\n8 are compact. To apply Lemma 2.5.3, let C1 Ą C2 Ą ¨ ¨ ¨\nbe a decreasing sequence of cylinder sets with empty intersection. Suppose to the contrary that\nP0 pŞ\nn Cnq ą 0. This would imply that all Cn are nonempty (any of these being empty would result\nin a measure 0). However, by Cantor’s intersection theorem16, Ş\nn Cn is nonempty, contradicting\nthe assumption. Hence, P0 pŞ\nn Cnq “ 0, and by Lemma 2.5.3, P0 is countably additive.\nWith this, we have proved that P0 is countably additive. To show that P0 defines a pre-measure,\nwe still have to show that P0 pΩq “ 1. Recall from the proof of Lemma 2.5.1 that Σ\n8 “ CpΣ\nkq for\nany k ą 0. In particular, Σ\n8 “ CpΣ\n1q “ CpΣq. This means that\nP0 pΩq “ P0\n`\nC\n`\nΣ\n˘˘\n(2.40)\n“\nÿ\nyPΣ\npLN pyq\n(2.41)\n“\nÿ\nyPΣ\npLN py | bosq “ 1.\n(2.42)\nThe last equality follows from local normalization of the sequence model.\n■\nWith this, we have successfully completed the first two steps of Fig. 2.4! However, we have\nonly defined a pre-measure over the set of infinite eos-containing sequences Σ\n8. This does not yet\nsatisfy all the properties we would like from a probability space. Because of that, we next extend\nthe constructed probability pre-measure P0 into a valid probability measure P to arrive to a valid\nprobability space.\nExtending the Pre-measure P0 into a Measure P (Step 3)\nTo extend P0 into a measure, we will use Carath´eodory’s theorem:\nTheorem 2.5.2: Carath´eodory’s Extension Theorem\nGiven an algebra A over some set Ωand a probability pre-measure P0 : A Ñ r0, 1s, there exists\na probability space pΩ, F, Pq such that A Ă F and P|A “ P0. Furthermore, the σ-algebra F\ndepends only on A and is minimal and unique, which we will also denote by σpAq, and the\nprobability measure P is unique.\nProof Sketch. First, construct an outer measure by approximation with countable coverings. Then,\nshow that the collection of sets that is measurable with respect to this outer measure is a σ-algebra\nF that contains A. Finally, restricting the outer measure to this σ-algebra, one is then left with\na probability space. To show minimality, one can show that F is contained in any σ-algebra that\ncontains A. Uniqueness is given by applying Dynkin’s π-λ theorem (Theorem 3.2 in Billingsley,\n1995).\nGreat care must be taken in each step involved in the outline above. To address these is well\nbeyond the scope of this treatment and we refer the reader to the many excellent texts with a proof\nof this theorem, such as Chapter 12 in Royden (1988) and Chapter 11 in Billingsley (1995).\n■\n16Cantor’s intersection theorem states that a decreasing sequence of nonempty compact sets have a nonempty\nintersection. A version of this result in introductory real analysis is the Nested Interval Theorem.\n34\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nApplying Carath´eodory’s extension theorem to our cylinder algebra C and pre-measure P0, we\nsee that there exists a probability space pΣ\n8, σpCq, Pq over Σ\n8 that agrees with the LNM pLN’s\nprobabilities.\nPhew! This now gets us to the fourth box in Fig. 2.4 and we only have one step remaining.\nDefining a Sequence Model (Step 4)\nWe now have to make sure that the outcome space of the defined probability space fits the definition\nof a sequence model. That is, we have to find a way to convert (map) the infinite eos-containing\nsequences from Σ\n8 into eos-free finite or possibly infinite strings processed by a sequence model as\nrequired by Definition 2.5.2. We will achieve this through the use of a random variable.\nRecall from Definition 2.2.5 that a random variable is a mapping between two σ-algebras. Since\nwe want our final measure space to work with the outcome space Σ8 Y Σ˚, we, therefore, want\nto construct a σ-algebra over Σ˚ Y Σ8 and then map elements from Σ\n8 to Σ˚ Y Σ8 to have the\nappropriate objects. We will do so in a similar fashion as we constructed pΣ\n8, Cq. Given H Ď Σk,\ndefine a rank-k cylinder set in Σ˚ Y Σ8 to be\nCpHq\ndef\n“ tyω : y P H, ω P Σ˚ Y Σ8u.\n(2.43)\nNotice the major change from Eq. (2.27): the suffixes ω of the elements in C pHq now come from\nΣ˚ Y Σ8 rather than Σ\n8. This means (i) that they do not contain eos and (ii) that they (and\nthus, elements of C pHq) can also be finite. Let Ck be the set of all rank-k cylinder sets. Define\nC\ndef\n“ Ť8\nk“1 Ck. Then, σ pCq is a σ-algebra by the same reasoning as in Lemma 2.5.1 and Theorem 2.5.2.\nWe can now define the following random variable\nxpωq “\n#\nωăk\nif k is the first eos in ω,\nω\notherwise (if eos R ω)\n(2.44)\ngiven any ω P Σ\n8. The proposition below shows that x is well-defined.\nProposition 2.5.2\nThe function x : pΣ\n8, σpCqq Ñ pΣ˚ Y Σ8, σpCqq defined in Eq. (2.44) is a measurable mapping.\nProof. To show that x is measurable, it suffices to show the measurability of preimage of a generating\nset of the σ-algebra. Note that the set of thin cylinder sets is a generating set. Let Cpyq be a thin\ncylinder set,\nx´1pCpyqq “x´1ptyω : ω P Σ˚ Y Σ8uq\n(2.45a)\n“x´1ptyω : ω P Σ˚uq Y x´1ptyω : ω P Σ8uq\n(2.45b)\n“\n˜ ď\nωPΣ˚\nCpyωeosq\n¸\nY\n˜\nCpyq X\n8\nč\nk“1\nAc\nk\n¸\n(2.45c)\nNote that the sets Ak above are defined in Eq. (2.58) which are cylinder sets representing the event\nof terminating at step k. Then, from the derivation above, we can see that x´1pCpyqq is formed by\ncountable operations over measurable sets (cylinder sets) in Σ\n8, and is hence measurable. So x is\na measurable function.\n■\n2.5. TIGHT LANGUAGE MODELS\n35\nx intuitively “cuts out” the first stretch of ω before the first eos symbol (where an LNM would\nstop generating) or leaves the sequence intact if there is no termination symbol eos. One can\ncheck that P˚, defined using P, is indeed a probability measure on pΣ˚ Y Σ8, σpCqq and hence\npΣ˚ Y Σ8, σpCq, P˚q is a probability space. We have therefore arrived at the final box of Fig. 2.4\nand shown that, given any LNM, we can construct an associated sequence model as defined in\nDefinition 2.5.2! In other words, given an LNM pLN, we have constructed a sequence model pSM (a\nprobability space over Σ8 Y Σ˚ where the probabilities assigned to (infinite) strings by pSM agree\nwith pLN.\n2.5.3\nInterpreting the Constructed Probability Space\nUnder the formulation of a probability space together with a random variable, useful probability\nquantities arise naturally and intuitively.\nConsider, for example, the probability of a single finite string y P Σ˚, P˚ pyq. By definition of x,\nthis equals\nP˚ pyq “ P˚ px “ yq\n(2.46)\n“ P\n`\nx´1 pyq\n˘\n(2.47)\n“ P\n´\nAll the sequences ω P Σ\n8 which map to y.\n¯\n(2.48)\nAll the sequences ω P Σ\n8 which map to y are sequences of the form ω “ yeosω1 for ω1 P Σ\n8—this\nis exactly the cylinder C pyeosq! By the definition of the probability space\n`\nΣ, σ\n`\nC\n˘\n, P\n˘\n, this is\nP\n`\nC pyeosq\n˘\n“\nÿ\ny1Ptyeosu\npLN\n`\ny1˘\n“ pLN pyeosq\n(2.49)\nand as before pLN pyeosq “ śT\nt“1 pLN pyt | yătq pLN peos | yq.\nAltogether, this means that, given a finite string y P Σ˚, we intuitively have\nP˚px “ yq “ pLNpeos | yqpLNpyq.\n(2.50)\nAdditionally, as we will show in the next section, the probability of the set of infinite strings\nP˚px P Σ8q is the probability of generating an infinite string.\nAn important technical detail left out in this discussion so far is that both the singleton set tyu\nand Σ8 need to be measurable in pΣ˚ Y Σ8, σpCqq for the above to make sense. This is addressed\nby Proposition 2.5.3 and Proposition 2.5.4.\nProposition 2.5.3\nIn measure space pΣ˚ Y Σ8, σpCqq, tyu is measurable for all y P Σ˚.\nProof. By definition in Eq. (2.43), for any y P Σ˚,\nCpyq “ tyω : ω P Σ˚ Y Σ8u\n(2.51a)\n“ tyω : ω P Σ˚u Y tyω : ω P Σ8u\n(2.51b)\n36\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nwhere\ntyω : ω P Σ˚u “ tyu Y\nď\naPΣ\ntyaω : ω P Σ˚u\n(2.52a)\nand\ntyω : ω P Σ8u “\nď\naPΣ\ntyaω : ω P Σ8u.\n(2.53)\nSo,\nCpyq “ tyu Y\nď\naPΣ\nˆ\ntyaω : ω P Σ˚u Y tyaω : ω P Σ8u\n˙\n(2.54a)\n“ tyu Y\nď\naPΣ\nCpyaq\n(2.54b)\nwhich implies that tyu “ Cpyqz Ť\naPΣ Cpyaq and hence measurable.\n■\nProposition 2.5.4\nIn the measure space pΣ˚ Y Σ8, σpCqq, Σ8 is measurable.\nProof. First, the outcome space Σ˚ Y Σ8 is measurable by definition of σ-algebra. Notice that\nΣ8 “ pΣ˚ Y Σ8qz\nď\nyPΣ˚\ntyu.\n(2.55)\nSince each tyu in the above is measurable by Proposition 2.5.3 and Σ˚ is a countable set, Σ8 is\nthen measurable.\n■\nSince both tyu and Σ8 are measurable in pΣ˚ Y Σ8, σpCqq by Propositions 2.5.3 and 2.5.4, we\nhave the following.\nProposition 2.5.5\nA sequence model pΣ˚ Y Σ8, σpCq, Pq is tight if and only if ř\nyPΣ˚ Pptyuq “ 1.\nProof. By definition, a sequence model is tight if and only if PpΣ8q “ 0. By Propositions 2.5.3\nand 2.5.4, we can write\nPpΣ˚ Y Σ8q “ PpΣ8q ` PpΣ˚q\n(countable additivity)\n(2.56a)\n“ PpΣ8q `\nÿ\nyPΣ˚\nPptyuq.\n(countable additivity)\n(2.56b)\nHence, a sequence model is tight if and only if ř\nyPΣ˚ Pptyuq “ 1.\n■\n2.5. TIGHT LANGUAGE MODELS\n37\nDeriving eos\nAs an aside, the preceding section allows us to motivate the eos token in LNM as a construct that\nemerges naturally. Specifically, for any y P Σ˚, rearranging Eq. (2.50):\npLNpeos | yq “ P˚px “ yq\npLNpyq\n(2.57a)\n“\nP˚px “ yq\nP˚px P Cpyqq\n(2.57b)\n“ P˚px “ y | x P Cpyqq\n(2.57c)\nwhere we have used pLNpyq “ PpCpyqq “ Ppx´1pCpyqqq “ P˚px P Cpyqq. This means that the eos\nprobability in an LNM emerges as the conditional probability that, given that we must generate a\nstring with a prefix y P Σ˚, the string is exactly y, i.e., that generation ends there.\n2.5.4\nCharacterizing Tightness\nNow that we have derived a measure-theoretic formalization of the probability space induced by\nlocally-normalized models, we can use it to provide an exact characterization of tightness in LNMs.\nFirst, we consider the event\nAk\ndef\n“ tω P Σ\n8 : ωk “ eosu\n(2.58)\nin the probability space pΣ\n8, σpCq, Pq. Intuitively, Ak is the event that an eos symbol appears at\nposition k in the string. Note that under this definition the Ak are not disjoint. For example, the\nstring ω “ ab eos c eos dddd ¨ ¨ ¨ lives in the intersection of A3 and A5 since eos appears at both\nposition 3 and position 5. Using Eq. (2.58), we can express the event consisting of all finite strings as\n8\nď\nk“1\nAk.\n(2.59)\nIt follows that we can express the event of an infinite string as\n˜ 8\nď\nk“1\nAk\n¸c\n“\n8\nč\nk“1\nAc\nk.\n(2.60)\nThus, using the random variable x, we can express the probability of generating an infinite string as\nP˚px P Σ8q “ Ppx´1pΣ8qq\n(2.61a)\n“ P\n˜ 8\nč\nk“1\nAc\nk\n¸\n.\n(2.61b)\nHence, we can now restate and formalize the notion of tightness.\n38\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nDefinition 2.5.5: Tight sequence model\nA sequence model is said to be tight if P˚px P Σ8q “ 0, in which case it is also a language\nmodel. Otherwise, we say that it is non-tight.\nNote that the definition of Ak only uses a string’s k-prefix, and hence is a cylinder set of rank k.\nRecalling that the cylinder sets are measurable and so are the sets countably generated by them, we\nsee that both the event consisting of all finite strings and the event consisting of all infinite strings\nare measurable. Thus, P\n`Ť8\nk“1 Ak\n˘\nand P\n`Ş8\nk“1 Ac\nk\n˘\nare well defined.\nA Lower Bound Result\nWe have characterized tightness in terms of the probability of a specific event P\n`Ş8\nk“1 Ac\nk\n˘\n, a\nquantity we now seek to determine.\nLemma 2.5.4\nIf ř8\nn“2 P\n´\nAn | Şn´1\nm“1 Ac\nm\n¯\n“ 8, then P\n`Ş8\nm“1 Ac\nm\n˘\n“ 0.\nProof. First, recall an elementary inequality that for x ą 0,\nx ´ 1 ě log x\nô\n1 ´ x ď log 1\nx.\n(2.62)\nNote that PpŞn\nm“1 Ac\nmq ą 0 for any n, for otherwise the conditional probabilities would be undefined.\n2.5. TIGHT LANGUAGE MODELS\n39\nLet pn\ndef\n“ PpŞn\nm“1 Ac\nmq. Then we have that pn ą 0 for all n, and\n8 “\n8\nÿ\nn“2\nPpAn |\nn´1\nč\nm“1\nAc\nmq\n(2.63a)\n“\n8\nÿ\nn“2\n1 ´ PpAc\nn |\nn´1\nč\nm“1\nAc\nmq\n(2.63b)\n“ lim\nNÑ8\nN\nÿ\nn“2\n1 ´ PpAc\nn |\nn´1\nč\nm“1\nAc\nmq\n(2.63c)\nď lim\nNÑ8\nN\nÿ\nn“2\nlog 1{PpAc\nn |\nn´1\nč\nm“1\nAc\nmq\n(by Eq. (2.62))\n(2.63d)\n“ lim\nNÑ8\nN\nÿ\nn“2\nlog PpŞn´1\nm“1 Ac\nmq\nPpŞn\nm“1 Acmq\n(2.63e)\n“ lim\nNÑ8\nN\nÿ\nn“2\nlog pn´1\npn\n(2.63f)\n“ lim\nNÑ8\nN\nÿ\nn“2\nplog pn´1 ´ log pnq\n(2.63g)\n“ lim\nNÑ8plog p1 ´ log pNq\n(2.63h)\n“ log p1 ´ lim\nNÑ8 log pN\n(2.63i)\nwhich implies that\nlim\nNÑ8 log pN “ ´8\n(2.64a)\nô\nlim\nNÑ8 pN “ 0\n(2.64b)\nô\nlim\nNÑ8 Pp\nN\nč\nm“1\nAc\nmq “ 0\n(2.64c)\nô\nPp\n8\nč\nm“1\nAc\nmq “ 0.\n(by continuity of measure)\n(2.64d)\n■\nUsing Lemma 2.5.4, we can derive the following useful condition of tightness of a language model.\nSpecifically, it applies when the probability of eos is lower bounded by a function that depends only\non the length and not the content of the prefix.\nProposition 2.5.6\nIf pLNpeos | yq ě fptq for all y P Σt and for all t and ř8\nt“1 fptq “ 8, then PpŞ8\nk“1 Ac\nkq “ 0.\nIn other words, pLN is tight.\n40\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nProof. Suppose pLNpeos | yq ě fptq for all y P Σt. To apply Lemma 2.5.4, we observe that\nAn X pAc\n1 X ¨ ¨ ¨ X Ac\nn´1q “tω P Σ\n8 : ωn “ eosu X\n˜n´1\nč\ni“1\ntω P Σ\n8 : ωi ­“ eosu\n¸\n(2.65a)\n“tω P Σ\n8 : ω “ eos, @ i ă n, ω ­“ eosu\n(2.65b)\n“tω P Σ\n8 : ω’s first eos is at position nu\n(2.65c)\nand similarly\nAc\n1 X ¨ ¨ ¨ X Ac\nn´1 “ tω P Σ\n8 : There is no eos in ω’s first n ´ 1 positionsu\n(2.66)\nSetting G\ndef\n“ tωeos : ω P Σn´1u Ă Σ\nn, we get\nPpAn | Ac\n1 X ¨ ¨ ¨ X Ac\nn´1q “ PpAn X pAc\n1 X ¨ ¨ ¨ X Ac\nn´1qq\nPpAc\n1 X ¨ ¨ ¨ X Ac\nn´1q\n(2.67a)\n“\nPpCpGqq\nPpCpΣn´1qq\n(definition of G)\n(2.67b)\n“\nř\nωPΣn´1 ppeos | ωqppωq\nř\nωPΣn´1 ppωq\n(by Eq. (2.30))\n(2.67c)\ně\nř\nωPΣn´1 fpn ´ 1qppωq\nř\nωPΣn´1 ppωq\n(definition of fptq)\n(2.67d)\n“ fpn ´ 1q\nř\nωPΣn´1 ppωq\nř\nωPΣn´1 ppωq\n(2.67e)\n“ fpn ´ 1q.\n(2.67f)\nSince ř8\nt“0 fptq “ 8, Lemma 2.5.4 shows that the event of a string never terminating, i.e., Ş8\nk“1 Ac\nk\nhas probability measure PpŞ8\nk“1 Ac\nkq “ 0. In other words, if the eos probability of a language model\nis lower bounded by a divergent sequence at every step, then the event that this language model\nterminates has probability 1.\n■\nThe Borel–Cantelli Lemmata\nIt turns out that Proposition 2.5.6 admits a converse statement in which we can prove a similar\nproperty of pLN by assuming that the model is tight. To show this result, we will use a fundamental\ninequality from probability theory—the Borel–Cantelli lemmata. The Borel–Cantelli lemmata are\nuseful for our purposes because they relate the probability measure of sets of the form Ş8\nn“0 An or\nŤ8\nn“0 An to a series ř8\nn“0 pn. We will only state the lemmata here without supplying their proofs;17\nhowever, we point out that Lemma 2.5.4 can be viewed as a parallel statement to the Borel–Cantelli\nlemmata and one can prove the lemmata using a very similar proof (cf. proof of Theorem 2.3.7 in\nDurrett, 2019).\nConcretely, given a sequence of events tAnu8\nn“1 in some probability space, the Borel–Cantelli\nlemmata are statements about the event\ntAn i.o.u\ndef\n“\n8\nč\nm“1\n8\nď\nn“m\nAn\n(2.68)\n17See §2.3 in Durrett (2019) or §4 in Billingsley (1995) instead.\n2.5. TIGHT LANGUAGE MODELS\n41\nwhere i.o. stands for “infinitely often.” Intuitively, tAn i.o.u is the set of outcomes that appear\nin infinitely many sets in the collection tAnu8\nn“1—they are the events that always remain in the\nunion of an infinite family of sets no matter how many of the leading ones we remove (hence the\nname). We will not use Borel–Cantelli directly, but they offer a probabilistic proof of a key result\n(Corollary 2.5.1) which will in turn lead to the desired statement about tightness. We formally state\nthe first and second Borel–Cantelli lemmata below.\nLemma 2.5.5: Borel–Cantelli I\nIf ř8\nn“1 PpAnq ă 8, then PpAn i.o.q “ 0.\nLemma 2.5.6: Borel–Cantelli II\nAssume tAnu is a sequence of independent events, then ř8\nn“1 PpAnq “ 8 ñ PpAn i.o.q “ 1.\nUsing the Borel–Cantelli lemmata, we can prove the following useful fact.\nCorollary 2.5.1\nGiven a sequence tpnu where pn P r0, 1q. Then,\n8\nź\nn“1\np1 ´ pnq “ 0 ðñ\n8\nÿ\nn“1\npn “ 8.\n(2.69)\nTo show Corollary 2.5.1, we first show the following simple consequence of Borel–Cantelli.\nCorollary 2.5.2\nIf PpAn i.o.q “ 1, then ř8\nn“1 PpAnq “ 8.\nProof. Suppose to the contrary that ř8\nn“1 PpAnq ă 8, then, by Borel–Cantelli I (Lemma 2.5.5),\nPpAn i.o.q “ 0, which contradicts the assumption. Hence, ř8\nn“1 PpAnq “ 8.\n■\nProof. We can use a product measure to construct a sequence of independent events tAnu8\nn“1 such\nthat PpAnq “ pn. (The product measure ensures independence.) Then, by definition in Eq. (2.68),\ntAn i.o.uc “\n8\nď\nm“1\nč\nněm\nAc\nn\n(2.70)\n42\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\nSo,\n1 ´ PpAn i.o.q “ P\n˜ď\nm\nč\nněm\nAc\nn\n¸\n(2.71a)\n“ lim\nmÑ8 P\n˜ č\nněm\nAc\nn\n¸\n(2.71b)\n“ lim\nmÑ8\nź\nněm\nPpAc\nnq\n(An are independent by construction)\n(2.71c)\n“ lim\nmÑ8\nź\nněm\np1 ´ pnq\n(2.71d)\npñq:\nAssume ś8\nn“1p1 ´ pnq “ 0. Then, for any m,\n0 “\nź\nně1\np1 ´ pnq “\n˜ ź\n1ďnăm\np1 ´ pnq\n¸\nloooooooooomoooooooooon\ną0\n˜ ź\nněm\np1 ´ pnq\n¸\n(2.72)\nSo it must the case that, for any m, ś\nněmp1 ´ pnq “ 0. Therefore,\n1 ´ PpAn i.o.q “ lim\nmÑ8\nź\nněm\np1 ´ pnq “ 0\n(2.73)\nwhich implies PpAn i.o.q “ 1. Corollary 2.5.2 implies that ř8\nn“1 pn “ 8.\npðq:\nAssume ř8\nn“1 pn “ 8. Then by Borel–Cantelli II (Lemma 2.5.6), PpAn i.o.q “ 1 which\nimplies\n0 “ 1 ´ PpAn i.o.q “ lim\nmÑ8\nź\nněm\np1 ´ pnq\n(2.74)\nObserve that\n! ś\nněmp1 ´ pnq\n)\nm is a non-decreasing sequence in m; to see this, note that as m\ngrows larger we multiply strictly fewer values p1 ´ pnq P p0, 1s. However, since we know the sequence\nis non-negative and tends to 0, it follows that for any m, we have\nź\nněm\np1 ´ pnq “ 0.\n(2.75)\nIt follows that, for any m, we have\n8\nź\nn“1\np1 ´ pnq “\nź\nnăm\np1 ´ pnq\nź\nněm\np1 ´ pnq\nloooooomoooooon\n“0\n“\nź\nnăm\np1 ´ pnq ¨ 0 “ 0.\n(2.76)\n■\n2.5. TIGHT LANGUAGE MODELS\n43\nWe now turn to proving a more general version of Proposition 2.5.6, which would imply its\nconverse. First, we define the following quantity\n˜peosptq\ndef\n“ PpAt | Ac\n1 X ¨ ¨ ¨ X Ac\nt´1q\n(2.77)\nwhich can be viewed as the eos probability at step t, given that eos was not generated at any\nearlier step. One can also show that, when ˜peosptq is defined, it has the same value as\n˜peosptq “\nř\nωPΣt´1 pLNpωqpLNpeos | ωq\nř\nωPΣt´1 pLNpωq\n,\n(2.78)\nwhich one can see as the weighted average probability of terminating at a string of length t.\nWe can now completely characterize the tightness of an LNM with the following theorem.\nTheorem 2.5.3: A sufficient condition for tightness\nAn LNM is tight if and only if ˜peosptq “ 1 for some t or ř8\nt“1 ˜peosptq “ 8.\nProof. Recall the definition of ˜peos, as previously defined in Eq. (2.77), is\n˜peosptq\ndef\n“ PpAt | Ac\n1 X ¨ ¨ ¨ X Ac\nt´1q.\n(2.79)\nCase 1.\nSuppose that ˜peosptq ă 1 for all t. Consider the termination probability again:\nP\n˜ 8\nč\nt“1\nAc\nt\n¸\n“ lim\nT Ñ8 P\n˜ Tč\nt“1\nAc\nt\n¸\n(2.80a)\n“ lim\nT Ñ8\nT\nź\nt“1\nPpAc\nt | Ac\n1 X ¨ ¨ ¨ X Ac\nt´1q\n(2.80b)\n“ lim\nT Ñ8\nT\nź\nt“1\np1 ´ rpeosptqq\n(2.80c)\n“\n8\nź\nt“1\np1 ´ rpeosptqq.\n(2.80d)\nIn the above, we have assumed that PpAc\n1 X ¨ ¨ ¨ X Ac\ntq ą 0 for all t, which is true by assumption that\n˜peosptq ă 1. Hence, by Corollary 2.5.1, Eq. (2.80d) is 0 if and only if ř\nt rpeosptq “ 8.\nCase 2.\nIf rpeosptq “ 1 is true for some t “ t0, then PpAc\n1X¨ ¨ ¨XAc\nt0q “ 0 and hence P\n`Ş8\nt“1 Ac\nt\n˘\n“ 0\nand such a language model is guaranteed to terminate at t0.\n■\nThe first condition intuitively says that there exists a step t at which the LNM will stop with\nprobability 1. If the first case of the condition does not hold, the second case can be checked since its\nsummands will be well-defined (the conditional probabilities in Eq. (2.78) will not divide by 0). We\nremark that Theorem 2.5.3 is a generalization of Proposition 2.5.6 since if ˜peosptq is lower-bounded\nby fptq whose series diverges, its own series would also diverge. However, since ˜peosptq involves the\ncomputation of a partition function in its denominator, it is most likely intractable to calculate\n44\nCHAPTER 2. PROBABILISTIC FOUNDATIONS\n(Lin et al., 2021a; Lin and McCarthy, 2022). Hence, Proposition 2.5.6 will be the main tool for\ndetermining tightness when we explore concrete language modeling frameworks later.\nWe have now very thoroughly defined the notion of language model tightness and provided\nsufficient and necessary conditions for an LNM or a sequence model to be tight. In the next\nsections, we start our exploration of concrete computational models of language, from the very\nsimple and historically important finite-state language models, their neural variants, to the modern\nTransformer architectures. For each of them, we will also individually discuss their tightness results\nand conditions.\nChapter 3\nModeling Foundations\nThe previous chapter introduced the fundamental measure-theoretic characteristics of language\nmodeling. We will revisit those over and over as they will serve as the foundations on which\nsubsequent concepts are built.\nIn this chapter, we turn our attention to modeling foundations, that is, the decisions we face\nwhen we want to build a distribution over strings and learn the appropriate parameters for that\ndistribution. We first discuss how to parameterize a distribution over strings (§3.1), what it means\nto learn good parameters, and how this can be done with modern optimization techniques and\nobjectives (§3.2).\nContinuing our framing of the notes in terms of questions, we will try to address the following:\nQuestion 3.1: Parametrizing a sequence model\nHow can a sequence model be parameterized?\nWe introduce a more formal definition of a “parametrized model” later. For now, you can simply\nthink of it as a function pθ : Σ\n˚ Ñ R described by some free parameters θ P Θ from a parameter\nspace Θ. This means that the values that pθ maps its inputs to might depend on the choice of the\nparameters θ—the presence of parameters in a model, therefore, allows us to fit them, which in our\ncontext specifically, means choosing them to maximize some objective with respect to data. This\nraises the following question:\nQuestion 3.2: Training a model\nGiven a parameterized model and a dataset, how can model parameters be chosen to reflect\nthe dataset as well as possible?\nWe begin with Question 3.1.\n45\n46\nCHAPTER 3. MODELING FOUNDATIONS\n3.1\nRepresentation-based Language Models\nMost modern language models are defined as locally normalized models. However, in order to define\nlocally normalized language model, we first define a sequence model pSM py | yq. Then, we prove\nthat the specific parameterization used in pSM py | yq encodes a tight locally normalized language\nmodel. However, as we demonstrated in Example 2.5.1, not all sequence models encode tight locally\nnormalized language models in the sense of Definition 2.5.1. So far, however, we have only talked\nabout this process abstractly. For example, we have proven that every language model can be locally\nnormalized and we have also given necessary and sufficient conditions for when a sequence model\nencodes a tight locally normalized language model. In this section, we start making the abstraction\nmore concrete by considering a very general framework for parameterizing a locally normalized\nlanguage model through sequence models pSM py | yq. We will call this the representation-based\nlanguage modeling framework.\nIn the representation-based language modeling framework, each conditional distribution in a\nsequence model pSM py | yq directly models the probability of the next symbol y P Σ given the\ncontext y—in other words, it tells us how likely y is to appear in the context of y.1 For example,\ngiven the string y “ “Papa eats caviar with a”, we would like pLN py | yq to capture that “spoon” is\nmore likely than “fork” At the same time, since eating caviar with a fork is technically possible, we\nwould also like pLN py | yq to capture that “fork” is likelier than, for example, “pencil”.\nHowever, it is not a-priori clear how we should model pSM py | yq concretely. We want to define\na function that can map contexts y to a distribution over possible continuations y with the caveat\nthat this distribution can be easily adjusted, i.e., we can optimize its parameters with some objective\nin mind (cf. §3.2). We will do this by adopting a very general idea of defining pSM py | yq in terms\nof similarity between representations that represent the symbol y and the context y. The more\ncompatible the symbol y is with the context y, the more probable it should be. Intuitively, going from\nthe example above, this means that “spoon” should be more similar to “Papa eats caviar with a”\nthan “fork” should be, and that should still be more similar than “pencil”. On the other hand,\nnotice that this also means that “spoon” and “fork” should be closer together than any of them to\n“pencil”.\nOne possibility for doing this is by embedding individual symbols y and all possible contexts y\nas vectors in a Hilbert space, i.e., a complete vector space endowed with an inner product. Once we\nembed the symbols and contexts in such a space, we can talk about how similar they are. We will first\ndescribe how this can be done abstractly §3.1.1 and then discuss how exactly vector representations\ncan be used when defining discrete probability distributions over the symbols in §3.1.3 by taking into\naccount the notion of similarities between vectors. We discuss methods for learning representations\nlater in this chapter (§3.2) and in Chapter 5.\n3.1.1\nVector Space Representations\nIt is not immediately obvious how to measure the similarity or compatibility between two symbols,\ntwo contexts or a symbol and a context. However, such a notion is required as part of our intuitive\ndesiderata for pSM py | yq. We begin by stating an important guiding principle, which we describe in\ndetail next and use heavily throughout the rest of the notes.\n1Unless explicitly stated otherwise, we use the phrase “in the context of” to imply given prior context—i.e.,\nwhen discussing probability distributions, this refers to the distribution pSM pyt | yătq with y “ yt. We will also see\nexamples of models which specify the conditional probabilities in terms of symbols that do not necessarily appear\nbefore the current one.\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n47\nPrinciple 3.1.1: Representation Learning\nThe good representation principle states that the success of a machine learning model\ndepends—in great part—on the representation that is chosen (or learned) for the objects that\nare being modeled. In the case of language modeling, the two most salient choice points are\nthe representations chosen for the symbols, elements of Σ, and the representations chosen for\nthe contexts, elements of Σ\n˚.\nLearning vector representations from data where individual entities are represented in some\nrepresentation space (i.e., a Hilbert space) has a rich history in NLP and machine learning in\ngeneral (Bengio et al., 2013).\nTo discuss the representations of symbols and strings more formally, we first introduce the notion\nof a Hilbert space, which leads us to a useful geometric manner to discuss the similarity and\ncompatibility of symbols and contexts. We first start with some more basic definitions. A vector\nspace over a field F is a set V together with two binary operations that satisfy certain axioms. The\nelements of F are often referred to as scalars and the elements of V as vectors. The two operations\nin the definition of a vector space are the addition of vectors and scalar multiplication of vectors.\nDefinition 3.1.1: Vector space\nA vector space over a field F is a set V together with two binary operations that satisfy the\nfollowing axioms:\n1. Associativity of vector addition: for all v, u, q P V\npv ` uq ` q “ v ` pu ` qq\n(3.1)\n2. Commutativity of vector addition: for all v, u P V\nv ` u “ u ` v\n(3.2)\n3. Identity element of vector addition: there exists 0 P V such that for all v P V\nv ` 0 “ v\n(3.3)\n4. Inverse elements of vector addition: for every v P V there exists a ´v P V such that\nv ` p´vq “ 0\n(3.4)\n5. Compatibility of scalar multiplication with field multiplication: for all v P V and\nx, y P F\nx pyvq “ pxyq v\n(3.5)\n6. Identity element of scalar multiplication: for all v P V\n1v “ v\n(3.6)\nwhere 1 is the multiplicative identity in F.\n48\nCHAPTER 3. MODELING FOUNDATIONS\n7. Distributivity of scalar multiplication with respect to vector addition: for all x P F\nand all u, v P V\nx pv ` uq “ xv ` xu\n(3.7)\n8. Distributivity of scalar multiplication with respect to field addition: for all x, y P F\nand all v P V\npx ` yq v “ xv ` yv\n(3.8)\nIn almost all practical cases, F will be R and V will be RD for some D P N.\nAn important characteristic of a vector space is its dimensionality, which, informally, corre-\nsponds to the number of independent directions—basis vectors—in the space. Any v P V can be\nexpressed as a linear combination of the D basis vectors. The coefficients of this linear combination\ncan then be combined into a D-dimensional coordinate vector in FD. Vector spaces, therefore,\nallow us to talk about their elements in terms of their expressions with respect to the basis vectors.\nInner product spaces additionally define an inner product, mapping pairs of elements of the vector\nspace to scalars. More formally, it is a vector space together with a map x¨, ¨y (the inner product)\ndefined as follows.\nDefinition 3.1.2: Inner product space\nAn inner product space is a vector space V over a field F coupled with a map\nx¨, ¨y : V ˆ V Ñ F\n(3.9)\nsuch that the following axioms hold\n1. Conjugate symmetry: for all v, u P V\nxv, uy “ xu, vy\n(3.10)\nwhere x denotes the conjugate of the element x P F.\n2. Linearity in the first argument: for all v, u, z P V and x, y P F\nxxv ` yu, zy “ xxv, zy ` yxu, zy\n(3.11)\n3. Positive-definiteness: for all v ‰ 0\nxv, vy ą 0\n(3.12)\nInner products are often defined such that they capture some notion of similarity of the vectors\nin V. We will use this when formally defining pSM py | yq in §3.1.2.\nEvery inner product on a real or complex vector space induces a vector norm defined as follows.\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n49\nDefinition 3.1.3: Norm\nGiven a vector space V over R or C and an inner product x¨, ¨y over it, the norm induced by\nthe inner product is defined as the function ∥¨∥: V Ñ Rě0 where\n∥v∥\ndef\n“\na\nxv, vy.\n(3.13)\nA Hilbert space is then an inner product space in which all sequences of elements satisfy a useful\nproperty with respect to the norm defined by the inner product: every convergent series with respect\nto the norm converges to a vector in V.\nDefinition 3.1.4: Hilbert space\nA Hilbert space is an inner product space that is complete with respect to the norm defined\nby the inner product. An inner product space is complete with respect to the norm if every\nCauchy sequence (an absolutely convergent sequence, i.e., a sequence whose elements become\narbitrarily close to each other) converges to an element in V. More precisely, an inner product\nspace is complete if, for every series\n8\nÿ\nn“1\nvn\n(3.14)\nsuch that\n8\nÿ\nn“1\n∥vn∥ă 8,\n(3.15)\nit holds that\n8\nÿ\nn“1\nvn P V.\n(3.16)\nNote that even if an inner product space V is not necessarily a Hilbert space, V can always be\ncompleted to a Hilbert space.\nTheorem 3.1.1: Completion theorem for inner product spaces\nAny inner product space can be completed into a Hilbert space.\nWe omit the proof for this theorem. More precisely, the inner product space can be completed\ninto a Hilbert space by completing it with respect to the norm induced by the inner product on the\nspace. For this reason, inner product spaces are also called pre-Hilbert spaces.\nTo motivate our slightly more elaborate treatment of representation spaces, we consider an\nexample of a model which falls under our definition of a representation-based language model but\nwould be ill-defined if it worked under any space with fewer axioms than a Hilbert space.\n50\nCHAPTER 3. MODELING FOUNDATIONS\nSpace\nUtility\nVector space\nA space in which representations of symbols and string live. It\nalso allows the expression of the vector representations in terms\nof the basis vectors.\nInner product space\nDefines an inner product, which defines a norm and can measure\nsimilarity.\nHilbert space\nThere are no “holes” in the representation space with respect to\nthe defined norm, since all convergent sequences converge into V.\nTable 3.1: The utility of different spaces introduced in this section.\nExample 3.1.1: A series of representations\nRecurrent neural networks are a type of neural network that sequentially process their input\nand compute the output (context representation) at time step t based on the output at time\nstep t ´ 1: ht “ f pht´1, ytq. A formal definition of a recurrent neural network, which we\nprovide in §5.1.2, is not required at the moment. However, note that a recurrent neural\nnetwork with one-dimensional representations h could, for example, take the specific form\nht “ 1\n2ht´1 `\n1\nht´1\n(3.17)\nwith h0 “ 2.\nSuppose we chose the inner product space Q over the field Q for our representation space.\nAll elements of the sequence ht are indeed rational numbers. However, the limit of the\nsequence, which can be shown to be\n?\n2, is not in the inner product space! This shows\nthat Q is not a Hilbert space and that we must, in full generality, work with Hilbert spaces\nwhenever we are dealing with possibly infinite sequences of data. The reason this is especially\nrelevant for language modeling is the need to consider arbitrarily long strings (contexts), whose\nrepresentations we would like to construct in a way similar to Eq. (3.17). Such representations\ncan, therefore, approach a limiting representation outside the space whenever the representation\nspace does not satisfy the axioms of a Hilbert space.\nA summary of the utilities of the three algebraic spaces introduced in this subsection is summarized\nin Tab. 3.1.\nRepresentation Functions\nWe can now introduce the notion of a general representation function.\nDefinition 3.1.5: Representation function\nLet S be a set and V a Hilbert space over some field F. A representation function f for\nthe elements of S is a function of the form\nf : S ÞÑ V.\n(3.18)\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n51\nThe dimensionality of the Hilbert space of the representations, D, is determined by the modeler.\nIn NLP, D usually ranges between 10 to 10000.\nImportantly, in the case that S is finite, we can represent a representation function as a matrix\nE P R|S|ˆD (assuming V “ RD where the nth row corresponds to the representation of the nth\nelement of S. This method for representing f is both more concise and will be useful for integrating\nthe symbol representation function into a model, where matrix multiplications are often the most\nefficient way to implement such functions on modern hardware.\nThis is the case for the representations of the individual symbols y from Σ, where the representa-\ntion function, which we will denote as ep¨q, is implemented as a lookup into the embedding matrix\nE P R|Σ|ˆD, i.e., epyq “ Ey.2 In this case, we will also refer to ep¨q as the embedding function.\nDefinition 3.1.6: Symbol embedding function\nLet Σ be an alphabet. An embedding function ep¨q: Σ Ñ RD is a representation function\nof individual symbols y P Σ.\nThe representations epyq are commonly referred to as embeddings, but, for consistency, we\nwill almost exclusively use the term representations in this text. Let us first consider possibly the\nsimplest way to represent discrete symbols with real-valued vectors: one-hot encodings.\nExample 3.1.2: One-hot encodings\nLet n : Σ Ñ\n␣\n1, . . . , |Σ|\n(\nbe a bijection (i.e., an ordering of the alphabet, assigning an index\nto each symbol in Σ). A one-hot encoding J¨K is a representation function which assigns the\nsymbol y P Σ the npyqth basis vector:\nJyK\ndef\n“ dnpyq,\n(3.19)\nwhere here dn is the nth canonical basis vector, i.e., a vector of zeros with a 1 at position n.\nWhile one-hot encodings are an easy way to create vector representations of symbols, they have\na number of drawbacks. First, these representations are relatively large—we have D “ |Σ|—and\nsparse, since only one of the dimensions is non-zero. Second, such representations are not ideal\nfor capturing the variation in the similarity between different words. For example, the cosine\nsimilarity—a metric we will motivate in the next section for measuring the similarity between symbol\nrepresentations—between symbols’ one-hot encodings is zero for all non-identical symbols. Ideally,\nwe would like symbol representations to encode semantic information, in which case, a metric such\nas cosine similarity could be used to quantify semantic similarity. This motivates the use of more\ncomplex representation functions, which we subsequently discuss.\nWhile most systems use this standard way of defining individual symbol representations using the\nembedding matrix, the way that the context is encoded (and what even is considered as context) is\nreally the major difference between the different architectures which we will consider later. Naturally,\nsince the set of all contexts is infinite, we cannot simply represent the representation function with\na matrix. Rather, we define the representation of a context y through an encoding function.\n2Here, we use the notation Ey to refer to the lookup of the row in E corresponding to y.\n52\nCHAPTER 3. MODELING FOUNDATIONS\nDefinition 3.1.7: Context encoding function\nLet Σ be an alphabet. A context encoding function enc p¨q : Σ\n˚ Ñ RD is a representation\nfunction of strings y P Σ\n˚.a\naNote that, to be completely consistent, the encoding function should be defined over the set\n´\nΣ Y tbosu\n¯˚\nto allow for the case when y0 “ bos. However, unlike eos, we do not necessarily require bos in any formal\nsetting, which is why we leave it out. We apologize for this inconsistency.\nWe will refer to encpyq as the encoding of y P Σ˚. In the general framework, we can simply\nconsider the encoding function enc to be a black box—however, a major part of Chapter 5 will\nconcern defining specific functions enc and analyzing their properties.\nWith this, we now know how we can represent the discrete symbols and histories as real-valued\nvectors. We next consider how to use such representations for defining probability distributions over\nthe next symbol.\n3.1.2\nCompatibility of Symbol and Context\nInner products naturally give rise to the geometric notion of angle, by giving us the means to\nmeasure the similarity between two representations. Concretely, the smaller the angle between the\ntwo representations is, the more similar the two representations are. In a Hilbert space, we define\nthe cosine of the angle θ between the two representations\ncos pθq\ndef\n“\nxu, vy\n∥u∥∥v∥.\n(3.20)\nThe Cauchy–Schwartz inequality immediately gives us that cos pθq P r´1, 1s since ´∥u∥∥v∥ď\nxu, vy ď ∥u∥∥v∥. Traditionally, however, we take the unnormalized cosine similarity as our measure\nof similarity, which simply corresponds to the inner product of the Hilbert space.\nGiven a context representation enc pyq, we can compute its inner products with all symbol\nrepresentations epyq:\nxepyq, enc pyqy.\n(3.21)\nwhich can be achieved simply with a matrix-vector product:\nE enc pyq .\n(3.22)\nE enc pyq P R|Σ|, therefore, has the nice property that each of the individual entries corresponds to\nthe similarities of a particular symbol to the context y. For reasons that will become clear soon, the\nentries of the vector E enc pyq are often called scores or logits. This brings us almost to the final\nformulation of the probability distribution pSM py | yq.\nIf E enc pyq encodes similarity or compatibility, then a natural way to model the probability\ndistribution pSM py | yq would be as proportional to the inner product between epyq and enc pyq.\nHowever, the inner product xepyq, enc pyqy may be negative; further, the sum over the similarity\nbetween a context and all tokens is not necessarily 1. To resolve this, we have to introduce the last\npiece of the puzzle: transforming E enc pyq into a valid discrete probability distribution by using a\nprojection function.\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n53\n3.1.3\nProjecting onto the Simplex\nIn the previous subsections we discussed how to encode symbols and contexts in a Hilbert space and\nhow an inner product gives us a natural notation of similarity between a potentially infinite number\nof items. We can now finally discuss how to create the conditional distribution pSM py | yq, i.e., how\nwe can map the real-valued E enc pyq that encodes symbol–context similarities to a valid probability\ndistribution—a vector on the probability simplex.\nProjection Functions: Mapping Vectors onto the Probability Simplex\npSM py | yq is a categorical distribution with |Σ| categories, i.e., a vector of probabilities whose\ncomponents correspond to the probabilities of individual categories. Perhaps the simplest way to\nrepresent a categorical distribution is as a vector on a probability simplex.\nDefinition 3.1.8: Probability Simplex\nA probability simplex ∆D´1 is the set of non-negative vectors RD whose components sum\nto 1:\n∆D´1 def\n“\n#\nx P RD | xd ě 0, d “ 1, . . . , D and\nD\nÿ\nd“1\nxd “ 1\n+\n(3.23)\nSo far, we have framed pSM as a function assigning the conditional distribution over y to each\nstring y. The definition of a simplex means that we can more formally express pSM as a projection\nfrom the Hilbert space of the context representations to ∆|Σ|´1, i.e., pSM : V Ñ ∆|Σ|´1. Yet all\nwe have discussed so far is creating a vector E enc pyq that encodes symbol–context similarities—\nE enc pyq is not necessarily on the probability simplex ∆|Σ|´1. To address this issue, we turn to\nprojection functions:\nDefinition 3.1.9: Projection Function\nA projection function f∆D´1 is a mapping from a real-valued Hilbert space RD to the\nprobability simplex ∆D´1\nf∆D´1 : RD Ñ ∆D´1.\n(3.24)\nwhich allows us to define a probability distribution according to E enc pyq:\npSM py | yq “ f∆|Σ|´1 pE enc pyqqy\n(3.25)\nClearly, we still want the projection of E enc pyq onto ∆|Σ|´1 to maintain several attributes of\nthe original vector—otherwise, we will lose the notion of compatibility that E enc pyq inherently\nencodes. However, f∆|Σ|´1 must satisfy several additional criteria in order to map onto a valid\npoint in ∆|Σ|´1. For example, the inner product of two vectors (and consequently E enc pyq) is not\nnecessarily positive—yet all points in ∆|Σ|´1 are positive (see Definition 3.1.8). These characteristics\nmotivate the use of a projection function that is both monotonic and positive everywhere. Thus,\none clear choice is to base our chosen projection function on the exponential function, i.e.,\nf∆|Σ|´1pE enc pyqq9 exp pE enc pyqq .\n(3.26)\n54\nCHAPTER 3. MODELING FOUNDATIONS\nTo make a function of the form in Eq. (3.26) a valid projection function, we now simply have to\nensure that the output of f∆D´1 sums to 1, which can easily be accomplished by re-normalizing the\nvector of exponentiated values by their sum. This brings us to the main star of this subsection: the\nsoftmax.\nWhile we simply motivated its introduction by chasing our goal of ending up on the probability\nsimplex, the origin of the softmax function goes back to the Boltzmann distribution from statistical\nmechanics introduced in the mid-1800s by Boltzmann (1868). It was then studied intensely and\npopularized by Gibbs (1902). It was originally introduced as a way to convert the energy function\nof the Boltzmann distribution into a probability distribution.3 Yet now, for reasons we will see in\nthis subsection, the softmax is the predominant choice of projection function in machine learning\napplications.\nFormally, the softmax is often defined in terms of a temperature parameter τ as follows.\nDefinition 3.1.10: Softmax\nLet τ P R` be the temperature. The softmax at temperature τ is the projection function\ndefined as:\nsoftmaxpxqd\ndef\n“\nexp\n“ 1\nτ xd\n‰\nřD\nj“1 exp\n“ 1\nτ xj\n‰, for d “ 1, . . . , D\n(3.27)\nwhere the temperature parameter τ gives us a mechanism for controlling the entropy of the\nsoftmax function by scaling the individual scores in the input vector before their exponentiation. In\nthe context of the Boltzmann distribution, it was used to control the “randomness” of the system:\nWhen the temperature is high, the softmax function outputs a more uniform probability distribution\nwhose probabilities are relatively evenly spread out among the different categories. When the\ntemperature is low, the softmax function outputs a peaked probability distribution, where the\nprobability mass is concentrated on the most likely category. In the limit, as we take τ to the edge\nof the possible values it can assume, the following properties hold:\nTheorem 3.1.2: Limiting behavior of the softmax function\nlim\nτÑ8 softmax pxq “ 1\nD1\n(3.28)\nlim\nτÑ0` softmax pxq “ eargmaxpxq,\n(3.29)\nwhere ed denotes the dth basis vector in RD, 1 P RD the vector of all ones, and\nargmax pxq\ndef\n“ min\n\"\nd | xd “\nmax\nd“1,...,D pxdq\n*\n,\n(3.30)\ni.e., the index of the maximum element of the vector x (with the ties broken by choosing\nthe lowest such index). In words, this means that the output of the softmax approaches the\nuniform distribution as τ Ñ 8 and towards a single mode as τ Ñ 0`.a\naτ Ñ 0` denotes the limit from above.\n3This is precisely the connection we mentioned in Definition 2.4.1.\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n55\nProof. Let us first consider the case of τ Ñ 0`. Without loss of generality, let us consider a\n2-dimensional vector x “ rx1, x2sJ\nlim\nτÑ0` softmaxpxq1 “ lim\nτÑ0`\nexp p x1\nτ q\nexp p x1\nτ q ` exp p x2\nτ q\n(3.31)\n“ lim\nτÑ0`\nexp p x1\nτ q exp p´ x1\nτ q\n`\nexp p x1\nτ q ` exp p x2\nτ q\n˘\nexp p´ x1\nτ q\n(3.32)\n“ lim\nτÑ0`\n1\n1 ` exp p x2´x1\nτ\nq\n(3.33)\nwhich leads us to the following definition for element-wise values:\nlim\nτÑ0` exp\nˆx2 ´ x1\nτ\n˙\n“\n$\n&\n%\n0,\nif x1 ą x2\n1,\nif x1 “ x2\n8,\no.w.\n(3.34)\nThen the limit of softmax as τ Ñ 0` is given as\nlim\nτÑ0` softmaxpxq “\n$\n’\n&\n’\n%\nr1, 0sJ ,\nif x1 ą x2\n“ 1\n2, 1\n2\n‰J ,\nif x1 “ x2\nr0, 1sJ ,\no.w.\n(3.35)\nwhich is equivalent to the argmax operator over x. The proof extends to arbitrary D-dimensional\nvectors.\nThe case of τ Ñ 8 follows similar logic, albeit limτÑ8 exp\n` x2´x1\nτ\n˘\n“ 1 in all cases. Hence, we\nget limτÑ8 softmaxpxq “ 1\nD1.\n■\nThe second property, specifically, shows that the softmax function resembles the argmax function\nas the temperature approaches 0—in that sense, a more sensible name for the function would have\nbeen “softargmax”. We will most often simply take τ to be 1. However, different values of the\nparameter are especially useful when sampling or generating text from the model, as we discuss\nsubsequently.\nThe output of the softmax is equivalent to the solution to a particular optimization problem,\ngiving it a variational interpretation.\nTheorem 3.1.3: Variational characterization of the softmax\nGiven a set of real-valued scores x, the following equality holds\nsoftmaxpxq “ argmax\npP∆D´1\n˜\npJx ´ τ\nD\nÿ\nd“1\npd log pd\n¸\n(3.36)\n“ argmax\npP∆D´1\n`\npJx ` τHppq\n˘\n(3.37)\nThis tells us that softmax can be given a variational characterization, i.e., it can be viewed as\nthe solution to an optimization problem.\n56\nCHAPTER 3. MODELING FOUNDATIONS\nProof. Eq. (3.36) can equivalently be written as\nsoftmaxpxq “ argmax\n˜\npJx ´ τ\nD\nÿ\nd“1\npd log pd\n¸\n(3.38)\ns.t.\nÿ\nd\npd “ 1\n(3.39)\nfrom which we can clearly see that the Lagrangian of this optimization problem is Λ “ pJx ´\nτ řD\nd“1 pd log pd ` λ ř\nd pd. Taking the derivative of Λ with respect to pd, we see that the optimum\nof is reached when\nBΛ\nBpd\n“ vd ´ τplog pd ` 1q ` λ “ 0\n(3.40)\nSolving for pd gives us pd “ Z expp xi\nτ q, where Z is the normalizing constant that ensures ř\nd pd “ 1.\nThis solution is equivalent to performing the softmax operation over x, as desired.\n■\nTheorem 3.1.3 reveals an interpretation of the softmax as the projection p P ∆D´1 that has the\nmaximal similarity with x while being regularized to produce a solution with high entropy. Further,\nfrom both Definition 3.1.10 and Eq. (3.36), we can see that softmax leads to non-sparse solutions as\nan entry softmaxpxqi can only be 0 if xd “´8.\nIn summary, the softmax has a number of desirable properties for use in machine learning\nsettings.\nTheorem 3.1.4: Desirable properties of the softmax function\nThe softmax function with temperature parameter τ exhibits the following properties.\n1. In the limit as τ Ñ 0` and τ Ñ 8, the softmax recovers the argmax operator and\nprojection to the center of the probability simplex (at which lies the uniform distribution),\nrespectively.\n2. softmaxpx ` c1q “ softmaxpxq for c P R, i.e., the softmax is invariant to adding the\nsame constant to all coordinates in x.\n3. The derivative of the softmax is continuous and differentiable everywhere; the value of\nits derivative can be explicitly computed.\n4. For all temperatures τ P R`, if xi ď xj, then softmaxpxqi ď softmaxpxqj. In words, the\nsoftmax maintains the rank of x.\nProof. Property 1. is simply a restatement of Theorem 3.1.2. The proof for property 2. can be\nshown using simple algebraic manipulation:\nsoftmaxpx ` c1qd “\nexpr 1\nτ xd`cs\nřD\nj“1 expr 1\nτ xj`cs “\nexpr 1\nτ xds¨exp c\nřD\nj“1 expr 1\nτ xjs¨exp c “ softmaxpxqd\n(3.41)\nThe derivative of the softmax at position i with respect to the variable at position j is given by\nBsoftmaxpxqi\nxj\n“ δipjq ¨ exppxiq ř\nk exppxkq ´ exppxiq ¨ exppxjq\npř\nk exppxkqq2\n(3.42)\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n57\nwhere δipjq is the Dirac Delta function, defined as δipjq “\n#\n1 if i “ j\n0 else\n. Clearly, Eq. (3.42) is\ncontinuous.\nFurther, it takes on values for all x P Rd.\nLastly, property 4.\nfollows from the\nmonotonicity of the exp function.\n■\nThere are many other valid projection functions that one could choose from. For example,\nMartins and Astudillo (2016) introduce the sparsemax, which can output sparse distributions:\nsparsemaxpxq\ndef\n“ argmin\npP∆D´1 ||p ´ x||2\n2\n(3.43)\nIn words, sparsemax directly maps x onto the probability simplex, which often leads to solutions on\nthe boundary, i.e., where at least one entry of p is 0. Martins and Astudillo (2016) provide a method\nfor computing the closed form solution of this optimization problem in Alg. 1 of their work. Blondel\net al. (2019) later introduced a framework that encompasses many different projection functions,\nwhich they term regularized prediction functions. Essentially, this framework considers the subset of\nprojection functions that can be written as:\nf∆|Σ|´1 pxq\ndef\n“ argmax\npP∆D´1\n`\npJx ´ Ωppq\n˘\n(3.44)\nwhere Ω: RD Ñ R is regularization term. For certain choices of Ω, there are straightforward\nclosed-form solutions to Eq. (3.44). For example, as we can see from Eq. (3.36), Eq. (3.44) is\nequivalent to the softmax when Ωppq “ ´Hppq, meaning we can compute its closed form using\nEq. (3.27). Further, we recover the sparsemax when Ωppq “ ´||p||2\n2, which likewise has a closed-form\nsolution. The notion of regularizing p may be unintuitive at first, but we can view it as trying to\nbalance out the “suitability” term pJx with a “confidence” term Ωppq, which should be smaller\nwhen p is “uncertain.” We point the interested reader to the comprehensive work of Blondel et al.\n(2019) for further elaboration.\nSo why aren’t these other projection functions more widely employed in machine learning\nframeworks? First, not all choices of Ωlead to closed-form solutions; further, not all meet the desirable\ncriterion listed in Theorem 3.1.4. For example, the sparsemax is not everywhere differentiable,\nmeaning that one could not simply use out-of-the-box automatic differentiation frameworks when\ntraining a model using the sparsemax as its projection function. Rather one would have to specify\nits gradient explicitly.\nTheorem 3.1.5: Deterivative of the sparsemax function\nThe derivative of the the sparsemax with respect to its input x is as follows:\nBsparsemaxpxqi\nBxj\n“\n#\nδij ´\n1\nSpxq\nif i, j P Spxq\n0\nelse\n(3.45)\nProof. See Martins and Astudillo (2016).\n■\nTo conclude, projection functions, together with symbol representations and the representation\nfunction enc, give us the tools to define a probability distribution over next symbols that encodes\n58\nCHAPTER 3. MODELING FOUNDATIONS\ncomplex linguistic interactions. We now bring all the components together into the locally normalized\nmodeling framework in the next section.\n3.1.4\nRepresentation-based Locally Normalized Models\nWith these tools at hand, we now define representation-based locally normalized language models.\nDefinition 3.1.11: Representation-Based Locally Normalized Model\nLet enc be an encoding function. A representation-based locally normalized model is a\nmodel of the following form:\npSMpyt | yătq\ndef\n“ f∆|Σ|´1 pE encpyătqqyt\n(3.46)\nwhere unless otherwise stated, we assume f∆|Σ|´1 “ softmax. It defines the probability of an\nentire string y P Σ˚ as\npLN pyq\ndef\n“ pSM peos | yq\nT\nź\nt“1\npSMpyt | yătq\n(3.47)\nwhere y0\ndef\n“ bos.\nAlternatively, we could also include an additive bias term b as part of the projection func-\ntion f∆|Σ|´1 in the definition of the conditional distribution pSMpyt | yătq, i.e., pSMpyt | yătq “\nf∆|Σ|´1 pE encpyătq ` bqyt. However, note that the bias term can be absorbed into the encoding\nfunction enc, meaning that we can assume the form Eq. (3.46) without loss of generality. In\nrepresentation-based language models, epyq and encpyq carry all the necessary information to deter-\nmine how probable individual symbols y are given the context y. Therefore, the design choices of\nepyq and encpyq are crucial when building language models this way. Indeed, a large portion of the\ndiscussion in the remainder of the notes will center around how to build good representations of the\ncontext and individual symbols.\n3.1.5\nTightness of Softmax Representation-based Models\nHaving introduced representation-based language models, we can now state a very general result\nabout the tightness of such models. It connects the notion of tightness to the intuition about\nthe “compatibility” of symbols to the context—namely, the compatibility of the eos symbol to the\ncontext (compared to the compatibility of all other symbols). The compatibility is here captured by\nthe distance of the representation of the eos symbol to the representation of the other symbols—if\nthis distance grows slowly enough with respect to t (modulo the norm of the context representation),\nthe model is tight.\n3.1. REPRESENTATION-BASED LANGUAGE MODELS\n59\nTheorem 3.1.6: Proposition 5.9 in Du et al., 2022\nLet pSM be a representation-based sequence model over the alphabet Σ, as defined in Defini-\ntion 3.1.11. Let\ns\ndef\n“ sup\nyPΣ\n∥epyq ´ epeosq∥2,\n(3.48)\ni.e, the largest distance to the representation of the eos symbol, and\nzmax\ndef\n“ max\nyPΣt ∥enc pyq∥2,\n(3.49)\ni.e., the maximum attainable context representation norm for contexts of length t. Then the\nlocally normalized model pLN induced by pSM is tight if\nszmax ď log t.\n(3.50)\nProof. Let xtpωq be the random variable that is equal to the tth token in an outcome ω P Ω. Then\nfor an arbitrary t P N and any y P Σt, we have:\nPpxt “ eos | xăt “ yq “\nexp\n”\nepeosqJenc pyq\nı\nř\nyPΣ exp\n”\nepyqJenc pyq\nı\n(3.51a)\n“\n1\nř\nyPΣ exprepyqJencpyqs\nexprepeosqJencpyqs\n(3.51b)\n“\n1\n1 ` ř\nyPΣ exp rpepyq ´ epeosqqJenc pyqs\n(3.51c)\ně\n1\n1 ` ř\nyPΣ exp r}epyq ´ epeosq}2}enc pyq }2s\n(Cauchy–Schwarz) (3.51d)\ně\n1\n1 ` ř\nyPΣ exp rk}enc pyq }2s\n(3.51e)\n“\n1\n1 ` |Σ| exp r }enc pyq }2s\n(3.51f)\nNow define zmax\ndef\n“ supyPΣt }enc pyq }2. We then have that @t P N and @y P Σt:\nPpxt “ eos | xăt “ yq ě\n1\n1 ` |Σ| exppkzmaxq\n(3.52)\nNow, by Proposition 2.5.6, we have that if ř8\nt“1\n1\n1`|Σ| exppk zmaxq diverges, then the language\nmodel is tight. We will show that if we have that DN P N such that @t ě N, kzmax ď log t, then the\nsequence model must be tight.\nFirst, note that limtÑ8 1\nt\n1`|Σ|t\n1\n“ limtÑ8 1\nt ` |Σ| “ |Σ| P p0, 8q. Hence, by the limit comparison\ntest, since ř8\nt“1\n1\nt diverges, this means ř8\nt“1\n1\n1`|Σ|t must also diverge.\nNow, suppose that k zmax ď log t for all t ě N.\nThis implies that for t ě N we have\n1\n1`|Σ| exppkzmaxq ě\n1\n1`|Σ|t, which combined with the above and the comparison test, implies that\n60\nCHAPTER 3. MODELING FOUNDATIONS\nř8\nt“N\n1\n1`|Σ| exppkzmaxq diverges. This in turn means that ř8\nt“1\n1\n1`|Σ| exppkzmaxq diverges. Hence, if\nk zmax ď log t for all t ě N for some N P N, then the language model is tight.\n■\nTheorem 3.1.6 is a generalization of the following result from Welleck et al. (2020).\nTheorem 3.1.7: Representation-based language models with bounded encodings\nare tight\nA locally-normalized representation-based language model, as defined in Definition 3.1.11,\nwith uniformly bounded ||encpyq||p (for some p ě 1) is tight.\nFor most of the language models that we consider, encpyq is bound due to the choice of activation\nfunctions. In turn, E enc pyătq is bounded for all y. Further, by the definition of the softmax,\nf∆|Σ|´1 pE encpyătqqeos ą η for some constant η.\nThis concludes our investigation of general representation-based models. The next section\ndiscusses learning parametrized models (as a special case, also symbol and context representations).\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n61\n3.2\nEstimating a Language Model from Data\nThe language modeling task refers to any attempt to estimate the parameters4 of a model pM of\nthe ground-truth probability distribution over natural language strings pLM using data D “ typnquN\nn“1,\nwhere we assume samples ypnq were generated according to pLM. This task is often treated as an\noptimization problem. Here we will discuss the various components of this optimization problem,\nprimarily the objective and the algorithm used to perform optimization. Note that the material\ncovered here corresponds to what is colloquially referred to as pre-training. The learning paradigm\nfor fine-tuning a language model for a downstream task will be covered later in the course.\n3.2.1\nData\nIn this course, we consider objectives that are defined in terms of data D. Therefore, we will first\ndiscuss the nature of this data which, more precisely, is a corpus of texts. Following the notation\nused throughout the rest of these notes, let Σ be an alphabet. A corpus D “ typnquN\nn“1 Ă Σ˚ is a\ncollection of N strings. We will use the terms corpus and dataset interchangeably throughout this\nsection. We make the following assumption about the data-generating process of D:\nAssumption 3.2.1: Independently and identically distributed assumption\nThe strings ypnq in our corpus D are generated independently and identically distributed\n(i.i.d.) by some unknown distribution pLM.\nNote that ypnq are strings of an arbitrary length; they can be single words, sentences, paragraphs,\nor even entire documents depending on how we choose Σ. For example, often our models’ architectural\ndesigns make them unable to process document-length strings efficiently, e.g., they might not fit\ninto a context window that can be reasonably processed by a transformer language model; we will\nelaborate on this statement in our discussion of transformers in §5.3. Thus in practice, we often\nchunk documents into paragraphs that we treat as separate data points.5 This means that our\nmodel may not be able to learn properties of language such as discourse structure.\n3.2.2\nLanguage Modeling Objectives\nSimilarly to many other machine learning tasks, we can cast our problem as the search for the best\nmodel pM of the ground-truth distribution over strings pLM. In order to make this search tractable,\nwe must limit the models pM that we consider. Explicitly, we make the following assumption:\nAssumption 3.2.2: Parametrized model\npLM is a member of the parameterized family of models tpθ | θ P Θu, the set of all distributions\nrepresentable by parameters θ in a given parameter space Θ.\n4Most of this course focuses on the parametric case, i.e., where pM is governed by a set of parameters θ. However,\nwe will briefly touch upon various non-parametric language models.\n5This practice technically breaks Assumption 3.2.1, yet the negative (empirically-observed) effects of this violation\nare minimal and perhaps outweighed by the additional data it allows us to make use of.\n62\nCHAPTER 3. MODELING FOUNDATIONS\nAs concrete examples, θ could be the conditional probabilities in a simple, standard n-gram model\nfor a given prefix of size n ´ 1, i.e., θ is n ´ 1 simplices of size |Σ|.6 As another example, θ could be\nthe weights of a neural network; the set Θ would then cover all possible valid weight matrices that\ncould parameterize our model.\nAssumption 3.2.2 implies that we can equivalently write pLM as pθ‹ for certain (unknown) pa-\nrameters θ‹ P Θ.7 Further, an arbitrary model pM from this hypothesis space with parameters\nθ can be written as pθ; we will use this notation for the remainder of the chapter to make the\nparameterization of our distribution explicit. We now turn to the general framework for choosing\nthe best parameters θ P Θ so that our model pθ serves as a good approximation of pθ‹.8\nGeneral Framework\nWe search for model parameters pθ P Θ such that the model induced by those parameters maximizes\na chosen objective, or alternatively, minimizes some loss function ℓ: Θ ˆ Θ Ñ Rě0. This loss can\nbe used to measure the quality of this model as an approximation to pθ‹. In simple math, we search\nfor the solution.\npθ\ndef\n“ argmin\nθPΘ\nℓpθ‹, θq\n(3.53)\nwhere our loss function is chosen with the following principle in mind\nPrinciple 3.2.1: Proximity Principle\nWe seek a model pθ that is “close” to pθ‹.\nThat is, we choose our loss function to be a measure M of the difference between a distribution\nparameterized by θ and one parameterized by the true θ‹, i.e., those of our ground-truth distribution.\nYet we are immediately faced with a problem: computing an arbitrary M between θ and θ‹ (or at\nleast the distributions induced by these sets of parameters) requires knowledge of both, the latter\nfor which we only have samples ypnq P D. We will therefore use our corpus D as an approximation\nto pθ‹, which is typically implemented by representing D as an empirical distribution—a collection\nof Dirac Delta functions—which we will denote as Ă\npθ‹. Formally, we define\nĂ\npθ‹pyq\ndef\n“ 1\nN\nN\nÿ\nn“1\nδypnqpyq\n(3.54)\nwhere the Dirac Delta function δx1pxq “\n#\n1 if x “ x1\n0 else\nis essentially a point mass with all probability\non x1. We can decompose this definition over symbols in our strings as well. I.e., we can compute\nĂ\npθ‹pyt | yătq “\n1\nN yăt\nN\nÿ\nn“1\nδypnq\nt\n|ypnq\năt pyt | yătq\n(3.55)\n6One might be tempted to assume we only need |Σ| ´ 1 parameters per simplex, but we condition over Σ classes\nper prefix position.\n7We discuss the implications of the case that pLM R tpθ | θ P Θu later in this section.\n8The modeling paradigms that we will discuss in this section are predominantly generative, i.e., these models try\nto learn the underlying distribution of the data rather than the boundaries between different classes or categories.\nThe implication is that parameter estimation in language modeling typically makes use of unannotated text data, and\nis therefore sometimes referred to as self-supervised.\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n63\nwhere N yăt\ndef\n“ řN\nn“1 1typnq\năt “ yătu.\nNote that we can likewise define Eq. (3.55) in terms of\nthe one-hot encodings of symbols, i.e., using the definition in Example 3.1.2:\nĂ\npθ‹p¨ | yătq “\n1\nNyăt\nřN\nn“1 Jypnq\nt\nK1typnq\năt “ yătu. In fact, the empirical distribution is often also referred to in\nmachine learning as the one-hot encoding of a dataset.\nNow that we are equipped with methods for representing both pθ‹ and pθ, we can define a loss\nfunction for approximating pθ‹ using pθ.\nCross-Entropy.\nA natural choice for a loss function is cross-entropy, a measure of the difference\nbetween two probability distributions, which has its roots in information theory (Shannon, 1948b).\nSpecifically, in Eq. (3.53), we take ℓpθ‹, θq “ HpĂ\npθ‹, pθq where the definition of the cross-entropy H\nbetween distributions p1 (with support Y) and p2 is as follows:\nHpp1, p2q “ ´\nÿ\nyPY\np1pyq log p2pyq\n(3.56)\nFurther, most of the models that we will encounter in this course are locally normalized. Thus, it is\nmore common to see cross-entropy expressed as\nHpp1, p2q “ ´\nÿ\nyPY\nTÿ\nt“1\np1pypnq\nt\nq log p2pyt | yătq.\n(3.57)\nNote that cross-entropy is not symmetric, i.e., Hpp1, p2q ‰ Hpp2, p1q. To motivate cross-entropy\nas a loss function, as well as the intuitive difference between the two argument orderings, we turn to\ncoding theory, a sub-field of information theory. In words, the cross-entropy between two probability\ndistributions is the expected number of bits needed to encode an event y P Y from p1 when using\nthe optimal encoding scheme corresponding to distribution p2. Importantly, the optimal encoding\nscheme for p1 uses log p1pyq bits to encode an event y that occurs with probability p1pyq, implying\nthat the minimal cross-entropy is achieved when p1 “ p2. This characteristic of cross-entropy\nmotivates another metric: the KL divergence DKL.\nKL Divergence.\nA divergence measure is a measure of statistical distance9 between two\nprobability distributions. The KL divergence is defined as:\nDKLpp1 || p2q “\nÿ\nyPY\np1pyq log p2pyq ´ p1pyq log p1pyq\n(3.58)\nThe KL divergence can intuitively be viewed as the cross-entropy shifted by the expected number\nof bits used by the optimal encoding scheme for p1, i.e., it is the additional number of expected\nbits needed to encode events from p1 when using our encoding scheme from p2. Indeed, taking\nℓpθ‹, θq “ DKLpĂ\npθ‹ || pθq should lead to the same solution as taking ℓpθ‹, θq “ HpĂ\npθ‹, pθq because\nthe Ă\npθ‹pyq log Ă\npθ‹pyq term is constant with respect to model parameters θ.\n9Divergences are not technically distances because they are not symmetric, i.e., it may be the case for divergence\nmeasure D and probability distributions p and q that Dpp || qq ‰ Dpp || qq. However, they do meet the criteria that\nDpp || qq ě 0 @p, q and Dpp || qq “ 0 ðñ p “ q.\n64\nCHAPTER 3. MODELING FOUNDATIONS\nRelationship to Maximum Likelihood Estimation\nAn alternative way that we could frame our search for model parameters pθ P Θ is in terms of\ndata likelihood. Formally, the likelihood of the corpus D under the distribution pθ is the joint\nprobability of all ypnq:\nLpθq “\nN\nź\nn“1\npθpypnqq.\n(3.59)\nThe principle of maximum likelihood then dictates:\nPrinciple 3.2.2: Maximum Likelihood\nThe optimal parameters for a model are those that maximize the likelihood of observing the\ngiven data under that model. Formally:\npθMLE\ndef\n“ argmax\nθPΘ\nLpθq\n(3.60)\nNote that in practice, we typically work with the log-likelihood Lpθq “ log Lpθq rather than\nthe likelihood for a number of reasons, e.g., it is convex and more numerically stable given the small\nprobabilities we encounter when using L and the finite precision of the computing frameworks that\nwe employ. Since log is a monotonically increasing function, this would not change the solution\nto Eq. (3.60). Further, as is the case with Eq. (3.57), we decompose our loss over symbol-level\ndistributions.\nNotably, in our setting, finding parameters that maximize data log-likelihood is equivalent to\nfinding those that minimize cross-entropy. We show this equivalence below.\nProposition 3.2.1\nThe optimal parameters under Eq. (3.60) are equivalent to the optimal parameters when\nsolving for Eq. (3.53) with the cross-entropy loss between the empirical distribution Ă\npθ‹ and\nthe model pθ.\nProof. Under the standard practice of taking 0 logp0q “ 0, the only elements of Y that make a\nnonzero contribution to HpĂ\npθ‹, pθq are sequences in the support of Ă\npθ‹, making summing over Y\nequivalent to summing over D:\nHpĂ\npθ‹, pθq “ ´\nÿ\nyPΣ˚\nĂ\npθ‹pyq log pθpyq\n(3.61)\n“ ´\nÿ\nyPΣ˚\n1\nN\nN\nÿ\nn“1\nδypnqpyq log pθpyq\n(3.62)\n“ ´\nÿ\nyPΣ˚\n1\nN 1typnq P Du log pθpyq\n(3.63)\n9 ´\nÿ\nyPD\nlog pθpyq\n(3.64)\n“ ´Lpθq\n(3.65)\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n65\nThus, we can see that the objectives are equivalent, up to a multiplicative constant that is independent\nof model parameters.\n■\nThe equivalence of cross-entropy, DKL divergence , and maximum likelihood as learning objectives\nprovides intuition about our many goals when learning pθ: (1) we want a close (w.r.t. a given metric)\napproximation of the data-generating distribution, and (2) this approximation should place high\nprobability on samples of real language data.\nProperties of pθ under the cross-entropy loss.\nAssumption 3.2.2 may feel quite strong, as it\nimplies we know a great deal about the nature of pLM. However, it allows us to prove the optimality\nof p pθ under certain conditions.\nTheorem 3.2.1: Maximum likelihood estimate is consistent\nConsider that our loss function ℓpθ‹, θq “ HpĂ\npθ‹, pθq (or equivalently that ℓpθ‹, θq “ DKLpĂ\npθ‹ ||\npθq). Given Assumption 3.2.1 and that the minimizer p pθ of HpĂ\npθ‹, pθq is unique, then under\ncertain (quite strong) regularity conditions on tpθ | θ P Θu, pθ is a consistent estimator, i.e., it\nconverges to θ‹ in probability as n Ñ 8.\nArguably, in practice, Assumption 3.2.2 does not hold; we often make some incorrect modeling\nassumptions. Naturally, this raises the following question: If we misspecify the family of models\nthat pLM belongs to, i.e., pLM R tpθ | θ P Θu, then is our optimal model p pθ under the cross-entropy\nloss at all meaningful? Fortunately, the answer here is yes. In this case, we can interpret p pθ as a\nprojection of pLM onto the manifold of parametric models tpθ | θ P Θu. This projection is formally\nknown as an information projection (Nielsen, 2018), which while we do not cover formally here,\nwe can intuit as a mapping of pLM onto its “closest” point in tpθ | θ P Θu. In this setting, using\ndifferent metrics M leads to different definitions of closeness, which in turn means that optimal\nmodels under different M exhibit different properties.\nPotential drawbacks of cross-entropy loss.\nA closer inspection of Eq. (3.56) reveals that,\nwhen we use HpĂ\npθ‹, pθq as our loss function, pθ must put probability mass on all samples ypnq in\nthe support of Ă\npθ‹; otherwise, our loss is infinite. Since the model is not explicitly penalized for\nextraneous coverage, it will thus resort to placing mass over all of Σ˚ to avoid such gaps;10 this is\nsometimes referred to as mean-seeking behavior. In practice, this means that sequences of symbols\nthat one might qualitatively describe as gibberish are assigned nonzero probability by pθ. It is\nunclear whether this is a desirable property under a language model. While perhaps useful when\nusing such a model to assign probabilities to strings—in which case, we might be more interested\nin how strings’ probabilities rank against each other and may not want to write off any string as\ncompletely improbable—it could prove problematic when generating strings from these models, a\ntopic covered later in this course.\nTeacher Forcing.\nThe loss functions that we have considered thus far are all based on our\nmodel’s predictions conditioned on prior context. Here we are faced with a choice during training:\n10This behavior can also be (at least partially) attributed to the softmax used to transform model outputs into a\nprobability distribution over symbols. Since the softmax maps to the interior of the probability simplex, no symbol\ncan be assigned a probability of exactly 0.\n66\nCHAPTER 3. MODELING FOUNDATIONS\nwe could either use the model’s predictions from the previous time step(s) pθp¨ | pyătq (e.g., the\nmost probable symbols) as the prior context or use the ground-truth prior context from our data\npθp¨ | yătq. The latter method is often referred to as teacher forcing: Even if our model makes\nan incorrect prediction at one step of training, we intervene and provide the correct answer for it\nto make subsequent predictions with.\nFrom a theoretical perspective, training with the cross-entropy loss mandates that we should\nuse the teacher-forcing approach since each conditional distribution is defined with respect to the\nground-truth context; this is elucidated, for example, in Eq. (3.57). Yet such meticulous guidance\ncan lead to poor performance in tasks where the model is required to accurately predict an entire\nsequence of symbols on its own. For example, in language generation, since the model is not\nexposed to its own generations during training, small errors in predictions can compound, leading\nto degenerate text. This problem is known as exposure bias. Only the other hand, using previous\nmodel outputs in order to make subsequent predictions can lead to serious instability during training,\nespecially if implemented from the start of training. Methods for alleviating exposure bias have\nbeen proposed with more stable training dynamics, such as scheduled sampling Bengio et al. (2015),\nwhich we discuss in §3.2.2.\nAlternative Objectives\nMasked Language Modeling.\nSo far, our parameter estimation strategies have made use of\nthe decomposition of pθpyq into individual symbol probabilities, conditioned on prior symbols, i.e.,\npθpyq “ śT\nt“1 pθpyt | yătq. In other words, we do not give a model both sides of a symbol’s context\nwhen asking it to estimate the probability distribution over that symbol. While this paradigm might\nbe more realistic when using a language model for tasks such as generation—for which we may want\nto generate outputs sequentially to mimic human language production—access to both sides of a\nsymbol’s context could be critical when using the model for tasks such as acceptability judgments\nor classification. This motivates the use of an alternative objective for parameter estimation.\nSimilarly to the maximum likelihood objective in Eq. (3.59), we can choose model parameters by\noptimizing for the per-symbol log-likelihood of a dataset D, albeit in this case, using both sides of\nthe symbol’s context:\nLMLMpθq “\nN\nÿ\nn“1\nTÿ\nt“1\nlog pθpypnq\nt\n| ypnq\năt , ypnq\nąt q\n(3.66)\nEq. (3.66) is sometimes referred to as the pseudo(log)likelihood (Besag, 1975), since it gives us an\napproximation of the true log-likelihood, i.e., řT\nt“1 log pθpyt | yăt, yątq « log pθpyq. Pseudolikelihood\nhas its origins in thermodynamics, where it was used as an approximate inference technique for\nparameter estimation in Ising models. In such situations, computing pθpyt | y‰tq often proved\ncomputationally easier than computing the exact set of of conditional probabilities whose product\nequaled the marginal.\nUsing Eq. (3.66) as a model’s training objective is also motivated by psychological tests of\nlanguage understanding—specifically, the Cloze (Taylor, 1953) task in psychology, in which the\ngoal is to predict the omitted symbol from a piece of text that constitutes a logical and coherent\ncompletion. For example, in the string\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n67\nExample 3.2.1: The Close task\nThe students [MASK] to learn about language models.\nwe predict want or like with high probability for the [MASK] position. When used as an objective\nin NLP, estimating the probability distribution over symbols at the masked position is referred\nto as masked language modeling; BERT (Devlin et al., 2019) is one well known example of a\nmasked language model. In practice, typically only the distributions over symbols at a percentage of\nrandomly-chosen positions in D are estimated during training. As mentioned in §2.5, a model whose\nparameters are estimated with the masked language modeling objective is not a valid language\nmodel in the sense of Definition 2.3.7 because it does not provide a valid distribution over Σ˚. Yet,\nmasked language models have become increasingly popular as base models for fine-tuning on certain\ndownstream tasks, where they sometimes lead to superior performance over standard language\nmodels.\nOther Divergence Measures.\nFrom a given hypothesis space (see Assumption 3.2.2), the\ndistribution that minimizes a given divergence measure with pθ‹ exhibits certain properties with\nrespect to how probability mass is spread over the support of that distribution. For example, the\nmodel pθ that minimizes DKLppθ‹ || pθq exhibits mean-seeking behavior, as discussed earlier in\nthis section. These properties have been studied in depth by a number of works (Minka, 2005;\nTheis et al., 2016; Husz´ar, 2015; Labeau and Cohen, 2019). The implication of these findings is\nthat, depending on the use case for the model, other divergence measures may be better suited\nas a learning objective. For example, prior work has noted frequency biases in models estimated\nusing the standard log-likelihood objective, i.e., these models exhibit an inability to accurately\nrepresent the tails of probability distributions (Gong et al., 2018). This is particularly relevant in\nthe case of language modeling, as symbol-usage in natural language tends to follow a power-law\ndistribution (Zipf, 1935). Consequently, when we care particularly about accurately estimating the\nprobability of rare words, we may wish to instead use a loss function that prioritizes good estimation\nof probability distribution tails. On the other hand, in the case of language generation, we may\ndesire models that only assign probability mass to outputs that are highly-likely according to pθ‹,\neven if this means assigning probabilities of 0 to some outcomes possible under pθ‹. In other words,\nwe may want a model with mode-seeking behavior, which is characteristic of models trained to\nminimize DKLppθ || pθ‹q. However, there are a number of computational issues with using other\ndivergence measures—such as general power divergences, reverse DKL divergence, and total variation\ndistance—for training neural probabilistic models over large supports, making them difficult to\nwork with in practice. For example, we can compute a Monte Carlo estimate of the forward DKL\ndivergence simply by using samples from pθ‹, which is exactly what we have in our dataset. However\nan unbiased estimator of the reverse DKL divergence would require the ability to query pθ‹ for\nprobabilities, which we do not have.\nScheduled Sampling and Alternative Target Distributions.\nScheduled sampling (Bengio\net al., 2015) is an algorithm proposed with the goal of alleviating exposure bias: after an initial period\nof training using the standard teacher forcing approach, some percentage of the models’ predictions\nare conditioned on prior model outputs, rather than the ground-truth context. However, under this\nalgorithm, pθ does not lead to a consistent estimator of θ‹ (Husz´ar, 2015). Other methods likewise\naim to alleviate the discrepancy between settings during parameter estimation and those at inference\n68\nCHAPTER 3. MODELING FOUNDATIONS\ntime by specifying an alternative target distribution, for example, one that ranks “higher-quality”\ntext as more probable than average-quality text. Ultimately, these methods often make use of\ntechniques developed for reinforcement learning, i.e., the REINFORCE algorithm. These methods\nfall under the category of fine-tuning criterion, which are discussed later in this course.\nAuxiliary Prediction Tasks.\nCertain works jointly optimize for an additional objective when\nperforming parameter estimation. For example, the parameters for BERT were learned using both\nthe masked language modeling objective as well as a task referred to as next sentence prediction,\ni.e., given two sentences, estimating the probability that the second sentence followed the first in a\ndocument. A number of similar auxiliary tasks have subsequently been proposed, such as symbol\nfrequency prediction or sentence ordering (see Aroca-Ouellette and Rudzicz (2020) for summary).\nHowever, these tasks do not have a formal relationship to language modeling and it is unclear what\ntheir effects are on a model’s ability to serve as a valid probability distribution over strings. They\nlikely lead to models that no longer fulfill the formal criteria of §2.5.4.\n3.2.3\nParameter Estimation\nGiven a loss function ℓand a parameter space Θ from which to choose model parameters, we are\nnow tasked with finding the parameters pθ, i.e., solving Eq. (3.53). For the class of models that\nwe consider (those parameterized by large neural networks), finding an exact solution analytically\nwould be impractical, if not impossible. Thus, we must resort to numerical methods, where we\nfind approximately optimal parameters by iterating over solutions. This is known as parameter\nestimation, or more colloquially as training our model.\nHere we will review the various components of training a language model from start to finish.\nMany of the techniques used for training language models are generally applicable machine learning\ntechniques, e.g., gradient-descent algorithms. Further, these techniques are constantly evolving and\noften viewed as trade secrets, meaning that entities building and deploying models may not reveal\nthe combination of components that they employed. Thus, we give a more general overview of the\ndesign choices involved in parameter estimation, along with the characteristics common to most\ncomponents.\nData Splitting\nIn any machine learning setting, we may overestimate model quality if we evaluate solely on its\nperformance w.r.t. the data on which its parameters were estimated. While we can often construct a\nmodel that performs arbitrarily well on a given dataset, our goal is to build a model that generalizes\nto unseen data. Thus, it is important to measure the final performance of a model on data that has\nhad no influence on the choice of model parameters.\nThis practice can be accomplished simply by splitting the data into several sets. The two basic\ndata splits are a training set Dtrain and test set Dtest; as the names imply, the training set is used\nduring parameter estimation while the test set is used for evaluating final performance. When\nsamples from Dtest can be found in Dtrain, we call this data leakage. The training set can be further\ndivided to produce a validation set Dval. Typically, Dval is not used to define the objective for which\nparameters are optimized. Rather, it serves as a check during training for the generalization abilities\nof a model, i.e., to see whether the model has started overfitting to the training data. The validation\nset can be used, e.g., to determine when to stop updating parameters.\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n69\nNumerical Optimization\nFrom a starting point θ0 P Θ chosen according to our initialization strategy, we want to find pθ in an\nefficient manner. This is where numerical optimization algorithms come into play—a precise set\nof rules for choosing how to move within Θ in order to find our next set of parameters. The output\nof a numerical optimization algorithm is a sequence of iterates tθsuT\nt“0, with the property that as\nT Ñ 8 we find the minimizer of our objective ℓ. Ideally, even after a finite number of iterations, we\nwill be sufficiently close to pθ.\nThe basic algorithm for searching the parameter space for pθ follows a simple formula: starting\nfrom θ0 P Θ, we iteratively compute θ1, θ2, . . . as\nθs`1 “ θs ` update magnitude ˆ update direction.\n(3.67)\nwhere the update added to θs to obtain θs`1 is intended to move us closer to pθ. Once some\nmaximum number of updates S or a pre-defined desideratum has been met, e.g., our loss has not\nimproved in subsequent iterations, we stop and return the current set of parameters. Many of the\nnumerical optimization techniques in machine learning are gradient-based, i.e., we use the gradient\nof the objective with respect to current model parameters (denoted as ∇θsℓpθsq) to determine our\nupdate direction. Standard vanilla gradient descent takes the form of §3.2.3, where the learning rate\nschedule η “ xη0, ¨ ¨ ¨ , ηT y determines the step size of our parameter update in the loss minimizing\ndirection—there is an inherent trade-off between the rate of convergence and overshooting—and the\nstopping criterion C determines whether we can terminate parameter updates before our maximum\nnumber of iterations S. In vanilla gradient descent, we set η “ c ¨ 1 for some constant c and\nAlgorithm 1 Gradient descent for parameter optimization.\nInput: ℓobjective\nθ0 initial parameters\nη learning rate schedule\nC : ℓˆ Θ ˆ Θ Ñ tTrue, Falseu stopping criterion\n1. for s “ 0, ¨ ¨ ¨ , S :\n2.\nθs`1 Ð θs ´ ηs ¨ ∇θℓpθsq\n3.\nif Cpℓ, θs, θs´1q :\n4.\nbreak\n5. return θs\nCpℓ, θs, θs´1q “ 1t|ℓpθsq ´ ℓpθs´1q| ă ϵu for user-chosen ϵ—in words, we stop when the change\nin loss between parameter updates is below a chosen threshold. In practice, more sophisticated\nlearning rate schedules η, e.g., square-root functions of the timestep (Hoffer et al., 2017) or adaptive\nfunctions that take into account model parameter values (Duchi et al., 2011), and stopping criterion\nC are employed.\nModern training frameworks rely on backpropagation—also known as reverse-mode automatic\ndifferentiation (Griewank and Walther, 2008)—to compute gradients efficiently (and, as the name\nimplies, automatically!). In fact, gradients can be computed using backpropogation in the same\ncomplexity as evaluation of the original function.\nWe do not provide a formal discussion of\nbackpropagation here but see Griewank and Walther (2008) for this material.\nRecall that our loss function—and consequently the gradient of our loss function—is defined with\nrespect to the entire dataset. Vanilla gradient descent therefore requires iterating through all of Dtrain\n70\nCHAPTER 3. MODELING FOUNDATIONS\nin order to determine the direction to move parameters U, which is an incredibly time-consuming\ncomputation for the large datasets employed in modern machine learning settings. Rather, an\noptimization algorithm would likely take much less time to converge if it could rapidly compute\nestimates of the gradient at each step. This is the motivation behind perhaps the most widely\nemployed class of optimization algorithms in machine learning: variations of stochastic gradient\ndescent (SGD), such as mini-batch gradient descent. Explicitly, these algorithms make use of the\nfact that ED1i.i.d.\n„ D∇θpθ, D1q “ ∇θpθ, Dq, where in slight abuse of notation, we use D1i.i.d.\n„D to signify\nthat the multi-set D1 consists of random i.i.d. samples from D. Thus we can instead base our loss\nℓ, and consequently U, off of a randomly selected subset of the data.11 In practice though, this\nsample is taken without replacement, which breaks the i.i.d. assumption. This in turn implies this\nour gradient estimates are biased under the mini-batch gradient descent algorithm. However, this\nbias does not seem to empirically harm the performance of such optimization strategies. Indeed, an\nentire branch of machine learning called curriculum learning focuses on trying to find an optimal\ndata ordering with which to train models to achieve desirable characteristics such as generalization\nabilities. Even when orderings are randomly selected, the chosen ordering can have a large impact\non model performance Dodge et al. (2020).\nA number of optimization algorithms have since iterated on SGD, e.g., the momentum algorithm\n(Polyak, 1964). In short, the momentum algorithm computes an exponentially decaying moving\naverage of past gradients, and continues updating parameters in this direction, which can drastically\nspeed up convergence. A widely-employed optimization algorithm called ADAM (Kingma and Ba,\n2015) takes a similar approach. Just as in momentum, it computes update directions using a moving\naverage (first moment) of gradients, albeit it additionally makes use of the variance of gradients\n(second moment) when computing update directions. ADAM is one of the most popular optimization\nalgorithms used for training large language models in modern ML frameworks.\nParameter Initialization\nOur search for (approximately) optimal model parameters must start from some point in the\nparameter space, which we denote as θ0. Ideally, starting from any point would lead us to the\nsame solution, or at least to solutions of similar quality. Unfortunately, this is not the case: both\ntraining dynamics and the performance of the final model can depend quite heavily on the chosen\ninitialization strategy, and can even have high variance between different runs of the same strategy.\nThis makes sense at some intuitive level though: depending on the learning algorithm, an initial\nstarting point can heavily dictate the amount of searching we will have to do in order to find pθ,\nand how many local optima are on the route to pθ. Consequently, a poor initial starting point may\nlead to models that take longer to train and/or may lead our learning algorithm to converge to\nsub-optimal solutions (i.e., an alternative local optimum) (Dodge et al., 2020; Sellam et al., 2022).\nThis can be the case even when only estimating the final layer of a network, e.g., when building\na classifier by appending a new layer to a pretrained model—a recent, widely-adopted practice in\nNLP (Dodge et al., 2020).\nMethods for initializing the parameters of neural language models are largely the same as those\n11While this logic holds even for samples of size 1 (which is the sample size for standard SGD by definition), basing\nupdates off of single samples can lead to noisy updates. Depending on resource constraints, batch sizes of a few\nhundred are often used, leading to much more stable training (although in the face of memory constraints, larger\nbatch sizes can be mimicked by accumulating, i.e., averaging, gradients across multiple batches when computing\nupdate directions). Batch size itself is often viewed as a model hyperparameter that can have a significant effect on\nmodel performance.\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n71\nfor initializing other neural networks. Perhaps the simplest approach is to randomly initialize all\nparameters, e.g., using a uniform or normal random variable generator. The parameters of these\ngenerators (mean, standard deviation, bounds) are considered hyperparameters of the learning\nalgorithm. Subsequent methods have iterated on this strategy to develop methods that take into\naccount optimization dynamics or model architectures. One consideration that is particularly\nrelevant for language models is that the input and output sizes of the embedding layer and the\nfully connected layer can be very different; this exacerbate the problem of vanishing or exploding\ngradients during training. For example, Glorot, Xavier and Bengio, Yoshua (2010) proposed Xavier\ninit, which keeps the variance of the input and output of all layers within a similar range in order\nto prevent vanishing or exploding gradients; He et al. (2015) proposed a uniform initialization\nstrategy specifically designed to work with ReLU activation units. Using uniform random variables\nduring parameter initialization can likewise alleviate the problem of vanishing gradients. While\nmost deep learning libraries use thoughtfully-selected initialization strategies for neural networks, it\nis important to internalize the variance in performance that different strategies can cause.\nEarly Stopping\nAs previously discussed, performance on Dtrain is not always the best indicator of model performance.\nRather, even if our objective continues to increase as we optimize over model parameters, performance\non held-out data, i.e., Dtest or even Dval, may suffer as the model starts to overfit to the training\ndata. This phenomenon inspires a practice called early stopping, where we stop updating model\nparameters before reaching (approximately) optimal model parameter values w.r.t. Dtrain. Instead,\nwe base our stopping criterion C off of model performance on Dval as a quantification of generalization\nperformance, a metric other than that which model parameters are optimized for, or just a general\nslow down in model improvement on the training objective.\nEarly stopping sacrifices better training performance for better generalization performance; in\nthis sense, it can also be viewed as a regularization technique, a topic which we discuss next. As\nwith many regularization techniques, early stopping can have adverse effects as well. Recent work\nsuggests that many models may have another period of learning after an initial period of plateauing\ntrain/validation set performance. Indeed, a sub-field has recently emerged studying the “grokking”\nphenomenon (Power et al., 2022), when validation set performance suddenly improves from mediocre\nto near perfect after a long period in which it appears that model learning has ceased, or even that\nthe model has overfit to the training data. Thus, it is unclear whether early stopping is always a\ngood practice.\n3.2.4\nRegularization Techniques\nOur goal during learning is to produce a model pθ that generalizes beyond the observed data; a model\nthat perfectly fits the training data but produces unrealistic estimates for a new datapoint is of little\nuse. Exactly fitting the empirical distribution is therefore perhaps not an ideal goal. It can lead to\noverfitting, which we informally define as the situation when a model uses spurious relationships\nbetween inputs and target variables observed in training data in order to make predictions. While\nthis behavior decreases training loss, it generally harms the model’s ability to perform on unseen\ndata, for which such spurious relationships likely do not hold.\nTo prevent overfitting, we can apply some form of regularization.\n72\nCHAPTER 3. MODELING FOUNDATIONS\nPrinciple 3.2.3: Regularization\nRegularization is a modification to a learning algorithm that is intended to increase a model’s\ngeneralization performance, perhaps at the cost of training performance.a\naAdapted from Goodfellow et al. (2016), Ch. 7.\nThere are many ways of implementing regularization, such as smoothing a distribution towards\na chosen baseline or adding a penalty to the loss function to reflect a prior belief that we may have\nabout the values model parameters should take on Hastie et al. (2001); Bishop (2006). Further,\nmany regularization techniques are formulated for specific model architecture: for example, the\ncount-based smoothing methods used n-gram language models (Ney et al., 1994; Gale and Sampson,\n1995). Here we specifically consider the forms of regularization often used in the estimation of neural\nlanguage models. Most fall into two categories: methods that try to ensure a model’s robustness\nto yet unseen (or rarely seen) inputs—e.g., by introducing noise into the optimization process—or\nmethods that add a term to our loss function that reflects biases we would like to impart on our\nmodel. This is by no means a comprehensive discussion of regularization techniques, for which we\nrefer the reader to Ch.7 of Goodfellow et al. (2016).\nWeight Decay\nA bias that we may wish to impart on our model is that not all the variables available to the model\nmay be necessary for an accurate prediction. Rather, we hope for our model to learn the simplest\nmapping from inputs to target variables, as this is likely the function that will be most robust to\nstatistical noise.12 This bias can be operationalized using regularization techniques such as weight\ndecay (Goodfellow et al., 2016)—also often referred to as ℓ2 regularization. In short, a penalty\nfor the ℓ2 norm of θ is added to ℓ. This should in theory discourage the learning algorithm from\nassigning high values to model parameters corresponding to variables with only a noisy relationship\nwith the output, instead assigning them a value close to 0 that reflects the a non-robust relationship.\nEntropy Regularization\nOne sign of overfitting in a language model pθ is that it places effectively all of its probability mass\non a single symbol.13 Rather, we may want the distributions output by our model to generally have\nhigher entropy, i.e., following the principle of maximum entropy: “the probability distribution which\nbest represents the current state of knowledge about a system is the one with largest entropy, in the\ncontext of precisely stated prior data” Jaynes (1957). Several regularization techniques, which we\nrefer to as entropy regularizers, explicitly penalize the model for low entropy distributions.\nLabel smoothing (Szegedy et al., 2015) and the confidence penalty (Pereyra et al., 2017) add\nterms to ℓto penalize the model for outputting peaky distributions. Explicitly, label smoothing\nreassigns a portion of probability mass in the reference distribution from the ground truth symbol\nto all other symbols in the vocabulary. It is equivalent to adding a term DKLpu || pθq to ℓ, where\nu is the uniform distribution. The confidence penalty regularizes against low entropy distribution\n12This philosophy can be derived from Occam’s Razor, i.e., the principle that one should search for explanations\nconstructed using the smallest possible set of elements.\n13The softmax transformation serves as somewhat of a regularizer against this behavior since it does not allow any\nsymbol be assigned a probability of 0.\n3.2. ESTIMATING A LANGUAGE MODEL FROM DATA\n73\nby adding a term Hppθq to ℓthat encourage high entropy in model outputs. The general class of\nentropy regularizers have proven effective in training neural models Meister et al. (2020).\nDropout\nRegularization also encompasses methods that expose a model to noise that can occur in the data\nat inference time. The motivation behind such methods is both to penalize a model for being overly\ndependent on any given variable (whether directly from the input or somewhere further along in the\ncomputational graph) for making predictions. Dropout does this explicitly by randomly “dropping”\nvariables from a computation in the network (Srivastava et al., 2014).\nMore formally, consider a model defined as a series of computational nodes, where any given\nnode is the product of the transformation of previous nodes. When dropout is applied to the module\nthat contains that node, then the node is zeroed out with some percentage chance, i.e., it is excluded\nin all functions that may make use of it to compute the value of future nodes. In this case, the\nmodel will be penalized if it relied completely on the value of that node for any given computation\nin the network. Dropout can be applied to most variables within a model, e.g., the inputs to the\nmodel itself, the inputs to the final linear projection in a feed-forward layer, or the summands in the\nattention head of a Transformer. Note that at inference time, all nodes are used to compute the\nmodel’s output.14\nBatch and Layer Normalization\nRescaling variables within a network helps with training stability, and further, with generalization\nby keeping variables within the same range and with unit variance. Specifically, batch normalization\nhelps regularize the problem of covariate shift, where the distribution of features (both the input\nfeatures and the variables corresponding to transformed features within a network) differs between\nthe training data and the data at inference time. Batch normalization alleviates this problem by\nrecentering (around 0) and scaling (such that data points have unit variance) data points such\nthat the data flowing between intermediate layers of the network follows approximately the same\ndistribution between batches. Layer normalization likewise performs centering and rescaling, albeit\nacross features rather than across data points. Specifically, normalization is performed so that all of\nthe feature values within a data point have mean 0 and unit variance.\n14Some form of renormalization is typically performed to account for the fact that model parameters are learned\nwith only partial availability of variables. Thus when all variables are used in model computations, the scale of the\noutput will (in expectation) be larger than during training, potentially leading to poor estimates.\n74\nCHAPTER 3. MODELING FOUNDATIONS\nChapter 4\nClassical Language Models\nNext, we turn to two classical language modeling frameworks: finite-state language models (a\nnatural generalization of the well-known n-gram models) in §4.1 and pushdown language models\n§4.2. Although the most successful approaches to language modeling are based on neural networks,\nthe study of older approaches to language modeling is invaluable. First, due to the simplicity of\nthe models, learning how they work helps distill concepts. And, moreover, they often serve as\nimportant baselines in modern NLP and provide very useful insights into the capabilities of modern\narchitectures as we will see when we discuss modern architectures in Chapter 5.\nIn the spirit of our question-motivated investigation, we will focus on the following two questions.\nQuestion 4.1: Representing conditional distributions\nHow can we tractably represent all conditional distributions of the form pSM py | yq in a simple\nway?\nQuestion 4.2: Representing hierarchical structure\nHow can we tractably represent the hierarchical structure of human language?\n4.1\nFinite-state Language Models\nAfter rigorously defining what language models are (and what they are not) and discussing how we\ncan estimate them, it is time to finally introduce our first class of language models—those based on\nfinite-state automata. Language models derived from probabilistic finite-state automata are some\nof the simplest classes of language models because they definitionally distinguish a finite numbers\nof contexts when modeling the conditional distribution of the next possible symbol pM py | yq. We\nfirst give an intuitive definition of a finite-state language model and then introduce a more formal\ndefinition, which we will use throughout the rest of the section.\n75\n76\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nDefinition 4.1.1: Informal definition of a finite-state language model\nA language model pLM is finite-state if it defines only finitely many unique conditional\ndistributions pLM py | yq. In other words, there are only finitely many contexts y which define\nthe distribution over the next symbol, pLM py | yq.\nIntuitively, this framework might be useful because it bounds the number of unique conditional\ndistributions we have to learn. However, as we will see later in this chapter, finite-state language\nmodels are not sufficient for modeling human language. Nevertheless, they can still offer a baseline\nfor modeling more complex phenomena. They also offer a useful theoretical tool in the understanding\nof neural language models, which we will discuss in Chapter 5.\n4.1.1\nWeighted Finite-state Automata\nBefore we introduce finite-state language models, we go on a brief detour into the theory of finite-state\nautomata. As we will see, finite-state automata are a tidy and well-understood formalism. As we will\nsee later in §5.2, they also provide a solid and convenient theoretical framework for understanding\nmodern neural language models, e.g., those based on recurrent neural networks and transformers.\nWe, therefore, begin by briefly introducing the theory of finite-state automata with real-valued\nweights.\nFinite-state Automata\nIn words, finite-state automata are one of the simplest devices for defining a formal language (cf.\nDefinition 2.3.5). We give a formal definition below.\nDefinition 4.1.2: Finite-state Automata\nA finite-state automaton (FSA) is a 5-tuple pΣ, Q, I, F, δq where\n• Σ is an alphabet;\n• Q is a finite set of states;\n• I Ď Q is the set of initial states;\n• F Ď Q the set of final or accepting states;\n• A finite multiset δ Ď Q ˆ pΣ Y tεuq ˆ Q.a Elements of δ are generally called transitions.\naThe fact that it is a multiset reflects that it can contain multiple copies of the same element (i.e., transitions\nbetween the same pair of states with the same symbol).\nThe name, finite-state automaton, stems from the requirement that the set of states Q is finite,\nwhich stands in contrast to the remaining formalisms we will cover in this course, e.g., pushdown\nautomata and recurrent neural networks. We will denote a general finite-state automaton with\na (subscripted) A. We will also adopt a more suggestive notation for transitions by denoting a\ntransition pq1, a, q2q as q1\naÝÑ q2.\n4.1. FINITE-STATE LANGUAGE MODELS\n77\nAn FSA can be graphically represented as a labeled, directed multi-graph.1 The vertices in the\ngraph represent the states q P Q and the (labeled) edges between them the transitions in δ. The\nlabels on the edges correspond to the input symbols a P Σ which are consumed when transitioning\nover the edges. The initial states qι P I are marked by a special incoming arrow while the final\nstates qφ P F are indicated using a double circle.\nExample 4.1.1: An example of a finite-state automaton\nAn example of an FSA can be seen in Fig. 4.1. Formally, we can specify it as\n• Σ “ ta, b, cu\n• Q “ t1, 2, 3u\n• I “ t1u\n• F “ t3u\n• δ “ tp1, a, 2q , p1, b, 3q , p2, b, 2q , p2, c, 3qu\n1\n2\n3\na\nb\nc\nb\nFigure 4.1: Example of a simple FSA.\nA finite-state automaton sequentially reads in individual symbols of an input string y P Σ˚\nand transitions from state to state according to the transition function δ. The traversal through the\nautomaton starts in a state qι P I (more precisely, it acts as if starting from all of them in parallel).\nIt then transitions from state q into the state q1 upon reading the symbol a if and only if q\naÝÑ q1 P δ.\nε-labeled transitions, however, allow a finite-state machine to transition to a new state without\nconsuming a symbol. This is in line with ε’s definition as an empty string.\nA natural question to ask at this point is what happens if for a state–symbol pair pq, aq there is\nmore than one possible transition allowed under the relation δ. In such a case, we take all implicit\ntransitions simultaneously, which leads us to a pair of definitions.\nDefinition 4.1.3: Deterministic finite-state automaton\nA FSA A “ pΣ, Q, I, F, δq is deterministic if\n• it does not have any ε-transitions;\n1The multi- aspect of the multi-graph refers to the fact that we can have multiple transitions from any pair of\nstates and labeled refers to the fact that we label those transitions with symbols from the alphabet Σ.\n78\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\n• for every pq, aq P Q ˆ Σ, there is at most one q1 P Q such that q\naÝÑ q1 P δ;\n• there is a single initial state, i.e., |I| “ 1.\nOtherwise, A is non-deterministic.\nAn important, and perhaps not entirely obvious, result is that the classes of deterministic and\nnon-deterministic FSA are equivalent, in the sense that you can always represent a member of one\nclass with a member of the other.\nIf the automaton ends up, after reading in the last symbol of the input string, in one of the final\nstates qφ P F, we say that the automaton accepts that string. A finite-state automaton is therefore\na computational device that determines whether a string satisfies a condition (namely, the condition\nthat the automaton, by starting in an initial state and following one of the paths labeled with that\nstring, ends in a final state). A string that satisfies this condition is said to be recognized by the\nautomaton and the set of all strings satisfying this condition form the language of the automaton.2\nDefinition 4.1.4: Language of a finite-state automaton\nLet A “ pΣ, Q, I, F, δq be an finite-state automaton. The language of A, L pAq is defined as\nL pAq\ndef\n“ ty | y is recognized by Au\n(4.1)\nAbstractly, a finite-state automaton is hence a specification of a set of rules that strings must\nsatisfy to be included in its language. The set of languages that finite-state automata can recognize\nis known as the class of regular languages.\nDefinition 4.1.5: Regular language\nA language L Ď Σ˚ is regular if and only if it can be recognized by an unweighted finite-state\nautomaton, i.e., if there exists a finite-state automaton A such that L “ L pAq.\nExample 4.1.2: Additional examples of finite-state automata\nAdditional simple examples of FSAs are shown in Fig. 4.2. The FSA in Fig. 4.2a, for example,\ncan formally be defined with\n• Σ “ ta, b, cu\n• Q “ t1, 2, 3, 4, 5, 6u\n• I “ t1u\n• F “ t6u\n• δ “ tp1, a, 2q , p1, b, 3q , p2, b, 2q , p2, c, 4q , p3, c, 4q , p3, b, 5q , p4, a, 6q , p5, a, 6qu\nThe FSA in Fig. 4.2a is deterministic while the one in Fig. 4.2b is non-deterministic.\n2We also say that the automaton recognizes this set of strings (language).\n4.1. FINITE-STATE LANGUAGE MODELS\n79\nA few examples of strings accepted by the A1 include bba, bca, aca, abca, abbca, abbbca, . . . .\nIn fact, due to the self-loop at state 2, the symbol b can appear an arbitrary number of times\nat position 2 in the accepted string abca. Notice that, starting from the state 1 and following\nthe transitions dictated by any of the accepted strings, we always end up in the only final\nstate, state 6. In particular, the string “abbca” is accepted with the following set of transitions\nin A1:\n1 aÝÑ 2, 2 bÝÑ 2, 2 bÝÑ 2, 2 cÝÑ 4, 4 aÝÑ 6.\n1\n2\n3\n4\n5\n6\na\nb\nc\nb\nc\nb\na\na\n(a) A deterministic FSA, A1. Each state only has\none outgoing transition labeled with the same\nsymbol.\n1\n2\n3\n4\n5\n6\na\na\nc\nb\nb\nb\na\na\n(b) A non-deterministic FSA, A2. State 1 has\ntwo outgoing transitions labeled with a whereas\nstate 3 has two outgoing transitions labeled with\nb.\nFigure 4.2: Examples of a deterministic and a non-deterministic FSA.\nWeighted Finite-state Automata\nA common and very useful augmentation to finite-state automata is through the addition of weights\non the transitions. The general theory of weighted automata makes use of semiring theory, which is\nbeyond the scope of this course.3 In this course, we will limit ourselves to the study of automata\nwith real-valued weights.\nDefinition 4.1.6: Real-weighted Finite-State Automaton\nA real-weighted finite-state automaton (WFSA) A is a 5-tuple pΣ, Q, δ, λ, ρq where\n• Σ is a finite alphabet;\n• Q is a finite set of states;\n• δ Ď Q ˆ pΣ Y tεuq ˆ R ˆ Q a finite multiset of transitions;a\n• λ : Q Ñ R a weighting function over Q;\n• ρ : Q Ñ R a weighting function over Q.\naAgain, we use the notation q\na{w\nÝÝÝÑ q1 to denote pq, a, w, q1q P δ.\n3Semirings and semiring-weighted formal languages are covered in detail in the Advanced Formal Language Theory\ncourse offered at ETH as well.\n80\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nNotice that we omit the initial and final state sets from the definition of WFSAs. Those can\nimplicitly be specified by the states given non-zero initial or final weights by the λ and ρ functions,\ni.e., I “ tq P Q | λ pqq ‰ 0u and F “ tq P Q | ρ pqq ‰ 0u. We might refer to them in the text later for\nnotational convenience and clarity of exposition. We will also sometimes denote transition weights\nwith ω\nˆ\nq\na{w\nÝÝÑ q1\n˙\ndef\n“ w.\nGraphically, we write the transition weights on the edges of the graph representing the WFSA\nafter the output symbol, separated by a “/”. The same separator is also used to separate the state\nname from its final weight, which is written in the node. The initial weights, however, are written\non the incoming arrow denoting initial states.\nExample 4.1.3: An example of a weighted finite-state automaton\nFig. 4.3 shows a weighted version of the FSA from Fig. 4.2a above.\n1\n0.3\n2\n3\n4\n5\n6{ 1\ne\na{0.5\nb{ 1\nπ\nb{0.63\nc{0.9\nc{0.21\nb{0.13\na{ 1\nπ¨e\na{0.29\nFigure 4.3: The WFSA corresponding to the FSA from Fig. 4.2a.\nThe connection of WFSAs to graphs makes it natural to define a set of transition matrices\nspecified by a WFSA.\nDefinition 4.1.7: Transition matrix\nLet A “ pΣ, Q, δ, λ, ρq be a WFSA. For any a P Σ, we define the symbol-specific transition\nmatrix Tpaq as the transition matrix of the graph restricted to a-labeled transitions. We also\ndefine the (full) transition matrix as T\ndef\n“ ř\naPΣ Tpaq.\n4.1. FINITE-STATE LANGUAGE MODELS\n81\nExample 4.1.4: Examples of transition matrices\nConsider the WFSA A in Fig. 4.3. The (symbol-specific) transition matrices for A are\nTpaq “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0.5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nπ¨e\n0\n0\n0\n0\n0\n0.29\n0\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\nTpbq “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n1\nπ\n0\n0\n0\n0\n0.63\n0\n0\n0\n0\n0\n0\n0\n0\n0.13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\nTpcq “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.9\n0\n0\n0\n0\n0\n0.21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\nT “ Tpaq ` Tpbq ` Tpcq “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0.5\n1\nπ\n0\n0\n0\n0\n0.63\n0\n0.9\n0\n0\n0\n0\n0\n0.21\n0.13\n0\n0\n0\n0\n0\n0\n1\nπ¨e\n0\n0\n0\n0\n0\n0.29\n0\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\nPaths and Path Weights\nA path is an important concept when talking about (weighted) finite-state automata as it defines\nthe basic structure by which a string is recognized or weighted. We now give a formal definition of a\npath and discuss how to weight paths.\nDefinition 4.1.8: Path\nA path π is an element of δ˚ with consecutive transitions, meaning that it is of the form\nˆ\nq1\n‚{‚\nÝÝÑ q2, q2\n‚{‚\nÝÝÑ q3 ¨ ¨ ¨ qn´1\n‚{‚\nÝÝÑ qn\n˙\n, where ‚ is a placeholder.a The length of a path is\nthe number of transition in it; we denote the length as |π|. We use p pπq and n pπq to denote\nthe origin and the destination of a path, respectively. The yield of a path is the concatenation\nof the input symbols on the edges along the path, which we will mark with s pπq. Furthermore,\nwe denote sets of paths with capital Π. Throughout the text, we will use a few different\nvariants involving Π to avoid clutter:\n82\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\n• ΠpAq as the set of all paths in automaton A;\n• ΠpA, yq as the set of all paths in automaton A with yield y P Σ˚;\n• ΠpA, q, q1q as the set of all paths in automaton A from state q to state q1.\naNotice we use the Kleene closure on the set δ here. It thus represents any sequence of transitions P δ\nOne of the most important questions when talking about weighted formalisms like weighted\nfinite-state automata is how to combine weights of atomic units like transitions into weights of\ncomplete structures.4 We begin by multiplicatively combining the weights of individual transitions\nin a path into the weights of the full path.\nDefinition 4.1.9: Path Weight\nThe inner path weight wI pπq of a path π “ q1\na1{w1\nÝÝÝÝÑ q2 ¨ ¨ ¨ qN´1\naN{wN\nÝÝÝÝÝÑ qN is defined as\nwI pπq “\nN\nź\nn“1\nwn.\n(4.2)\nThe (full) path weight of the path π is then defined as\nw pπq “ λ pp pπqq wI pπq ρ pn pπqq .\n(4.3)\nA path π is called accepting or successful if w pπq ‰ 0.\nThe inner path weight is therefore the product of the weights of the transitions on the path,\nwhile the (full) path weight is the product of the transition weights as well as the initial and final\nweights of the origin and the destination of the path, respectively.\nString Acceptance Weights and Weighted Regular Languages\nWhen we introduced unweighted finite-state automata, we defined the important concept of recogniz-\ning a string and recognizing a language. We generalize these concepts to the very natural quantity\nof the weight assigned by a WFSA to a string y P Σ˚, i.e., its acceptance weight, or stringsum, as\nthe sum of the weights of the paths that yield y.\nDefinition 4.1.10: Stringsum\nThe stringsum, string weight, or acceptance weight of a string y P Σ˚ under a WFSA A is\ndefined as\nA pyq\ndef\n“\nÿ\nπPΠpA,yq\nw pπq .\n(4.4)\n4In the case of WFSAs, a structure is a path. In the next section, we will see how to combine weights from basic\nunits into trees.\n4.1. FINITE-STATE LANGUAGE MODELS\n83\nThis naturally generalizes the notion of acceptance by an unweighted FSA—whereas an un-\nweighted FSA only makes a binary decision of accepting or rejecting a string, a weighted FSA always\naccepts a string with a specific weight. This leads to the definition of the weighted language of the\nWFSA.\nDefinition 4.1.11: Weighted language of a weighted finite-state automaton\nLet A be a WFSA. Its (weighted) language is defined as\nL pAq\ndef\n“ tpy, A pyqq | y P Σ˚u .\n(4.5)\nWe say a language is a weighted regular language if it is a language of some WFSA:\nDefinition 4.1.12: Weighted regular language\nA weighted language L is a weighted regular language if there exists a WFSA A such that\nL “ L pAq.\nLastly, we also define the full and state-specific allsum of the automaton. The former refers to\nthe total weight assigned to all possible strings, or all possible paths whereas the latter refers to the\nsum of the path weights of the paths stemming from a specific state.\nDefinition 4.1.13: State-specific allsum\nLet A “ pΣ, Q, δ, λ, ρq be a WFSA. The allsum of a state q P Q is defined as\nZ pA, qq “\nÿ\nπPΠpAq\nq1“q\nwI pπq ρ pn pπqq .\n(4.6)\nState-specific allsums are also referred to as the backward values in the literature and are\noften denoted as β pqq.\nDefinition 4.1.14: WFSA allsum\nLet A “ pΣ, Q, δ, λ, ρq be a WFSA. The allsum of A is defined as\nZ pAq “\nÿ\nyPΣ˚\nA pyq “\nÿ\nyPΣ˚\nÿ\nπPΠpA,yq\nw pπq “\nÿ\nπPΠpAq\nw pπq .\n(4.7)\nThe second equality in Eq. (4.7) comes from the crucial observation that the double sum in\nthe second term sums over precisely all paths of the automaton A, which is where the name of the\nquantity comes from allsum.5 This is easy to see if we consider that by summing over all possible\nstrings, we enumerate all possible path yields, and each path in the automaton has a yield P Σ˚.\nZ pAq is again the result of summing over infinitely many terms (whether the set of strings in Σ˚\n5Analogously, given some (implicitly defined) set of paths S, we will name the sum over the weights of the paths\nin S the allsum over S\n84\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nof the infinitely many paths in a cyclic WFSA), and might therefore not necessarily be finite. For\nreasons which will become clear shortly, we will say that a WFSA A is normalizable if Z pAq ă 8.\nNote that the sum in Eq. (4.4) only contains one term if the automaton is deterministic. Whenever\nthe automaton is non-deterministic, or when we are interested in the sum of paths with different\nyields as in Eq. (4.7), the interactions (namely, the distributive law) between the sum over the\ndifferent paths and the multiplications over the transitions in the paths play an important role when\ndesigning efficient algorithms. Indeed, many algorithms defined for WFSAs rely on decompositions\nof such sums enabled by the distributive law.6\nAccessibility and Probabilistic Weighted Finite-state Automata\nAn important property of states of a WFSA which we will need when investigating the tightness of\nfinite-state language models is accessibility.\nDefinition 4.1.15: (Co)-Accessible and useful states\nA state q P Q of a WFSA is accessible if there is a non-zero-weighted path to q from some\nstate qι with λ pqιq ‰ 0; it is co-accessible state if there is a non-zero-weighted path from q\nto some state qφ with ρ pqφq ‰ 0. It is useful if it is both accessible and co-accessible, i.e., q\nappears on some non-zero-weighted accepting path.\nDefinition 4.1.16: Trim automaton\nTrimming a WFSA means removing its useless states.a Removing the non-useful states\nmeans removing their rows and columns from T as well as their rows from ÝÑλ and ÝÑρ , yielding\npossibly smaller T1, ÝÑλ\n1 and ÝÑρ 1.\naThis does not affect the weights of the strings with w pyq ‰ 0.\nWe will use WFSAs to specify language models. However, not every WFSA is a language model,\ni.e., a distribution over strings. Generally, the weight of a string could be negative if we allow\narbitrary real weights. Thus, a restriction we will impose on all weighted automata that represent\nfinite-state language models is that the weights be non-negative.\nFurthermore, a special class of WFSAs that will be of particular interest later is probabilistic\nWFSAs.\nDefinition 4.1.17: Probabilistic Weighted Finite-State Automaton\nA WFSA A “ pΣ, Q, δ, λ, ρq is probabilistic (a PFSA) if\nÿ\nqPQ\nλ pqq “ 1\n(4.8)\n6Many such examples are covered in the Advanced Formal Language Theory course.\n4.1. FINITE-STATE LANGUAGE MODELS\n85\nand, for all q P Q and all outgoing transitions q\na{w\nÝÝÑ q1 P δ it holds that\nλ pqq ě 0\n(4.9)\nρ pqq ě 0\n(4.10)\nw ě 0\n(4.11)\nand\nÿ\nq\na{w\nÝÝÑq1\nw ` ρ pqq “ 1.\n(4.12)\nThis means that the initial weights of all the states of the automaton form a probability\ndistribution (the initial weight of a state corresponds to the probability of starting in it), as well as\nthat, for any state q in the WSFA, the weights of its outgoing transitions (with any label) together\nwith its final weight form a valid discrete probability distribution. In a certain way, probabilistic\nfinite-state automata naturally correspond to locally normalized language models, as we explore in\nthe next subsection.\nThe eos symbol and the final weights.\nNotice that the final weights in a PFSA play an\nanalogous role to the eos symbol: the probability of ending a path in a specific state q—and\ntherefore ending a string—is q’s final weight! That is, the probability ρ pqφq for some qφ P Q,\nrepresenting the probability of ending the path in qφ, is analogous to the probability of ending a\nstring y, pSM peos | yq, where qφ “represents” the string (history) y.7 When modeling language with\nweighted finite-state automata, we will therefore be able to avoid the need to specify the special\nsymbol and rather rely on the final weights, which are naturally part of the framework.\n4.1.2\nFinite-state Language Models\nWe can now formally define what it means for a language model to be finite-state:\nDefinition 4.1.18: Finite-state language models\nA language model pLM is finite-state if it can be represented by a weighted finite-state automa-\nton, i.e., if there exists a WFSA A “ pΣ, Q, δ, λ, ρq such that L pAq “ L ppLMq. Equivalently,\nwe could say that pLM is finite-state if its language is a weighted regular language.\nOn the other hand, given a WFSA A, there are two established ways of defining a probability of\nstring.\nString Probabilities in a Probabilistic Finite-state Automaton\nIn a probabilistic FSA (cf. Definition 4.1.17), any action from a state q P Q is associated with a\nprobability. Since the current state completely encodes all the information of the input seen so far\nin a finite-state automaton, it is intuitive to see those probabilities as conditional probabilities of\n7Due to the possible non-determinism of WFSAs, the connection is of course not completely straightforward, but\nthe point still stands.\n86\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nthe next symbol given the input seen so far. One can, therefore, define the probability of a path as\nthe product of these individual “conditional” probabilities.\nDefinition 4.1.19: Path probability in a PFSA\nWe call the weight of a path π P Π pAq in a probabilistic FSA the probability of the path π.\nThis alone is not enough to define the probability of any particular string y P Σ˚ since there\nmight be multiple accepting paths for y. Naturally, we define the probability of y as the sum of the\nindividual paths that recognize it:\nDefinition 4.1.20: String probability in a PFSA\nWe call the stringsum of a string y P Σ˚ in a probabilistic FSA the probability of the string\ny:\npA pyq\ndef\n“ A pyq .\n(4.13)\nCrucially, notice that these two definitions did not require any normalization over all possible\npaths or strings. This closely resembles the way we defined locally normalized models based on the\nconditional probabilities of a sequence model. Again, such definitions of string probabilities are\nattractive as the summation over all possible strings is avoided. However, a careful reader might\nthen ask themself: do these probabilities actually sum to 1, i.e., is a probabilistic FSA tight? As you\nmight guess, they might not.8 We explore this question in §4.1.4.\nString Probabilities in a General Weighted Finite-state Automaton\nTo define string probabilities in a general weighted FSA, we use the introduced notions of the\nstringsum and the allsum. The allsum allows us to tractably normalize the stringsum to define the\nglobally normalized probability of a string y as the proportion of the total weight assigned to all\nstrings that is assigned to y.9\nDefinition 4.1.21: String probability in a WFSA\nLet A “ pΣ, Q, δ, λ, ρq be a normalizable WFSA with non-negative weights. We define the\nprobability of a string y P Σ˚ under A as\npA pyq\ndef\n“ A pyq\nZ pAq.\n(4.14)\nLanguage Models Induced by a WFSA\nWith the notions of string probabilities in both probabilistic and general weighted FSAs, we can\nnow define the language model induced by A as follows.\n8Notice that, however, whenever a PFSA is tight, its allsum is 1.\n9We will see how the allsum can be computed tractably in §4.1.3.\n4.1. FINITE-STATE LANGUAGE MODELS\n87\nDefinition 4.1.22: A language model induced by a WFSA\nLet A “ pΣ, Q, δ, λ, ρq be a WFSA. We define the language model induced by A as the\nfollowing probability distribution over Σ˚\npLMA pyq\ndef\n“ pA pyq .\n(4.15)\nIt is easy to see that while global normalization requires the computation of the allsum, language\nmodels induced by weighted FSAs through Eq. (4.14) are globally normalized and thus always tight.\nIn the next subsection, we consider how the quantities needed for computing Eq. (4.14) can be\ncomputed. Of particular interest will be the quantity Z pAq, as it involves the summation over\npossibly infinitely many terms and therefore requires some clever tricks to be computed.\n4.1.3\nNormalizing Finite-state Language Models\nIn this subsection, we develop an algorithm for normalizing a globally normalized language model (cf.\nDefinition 2.4.2) defined by a WFSA, i.e., an algorithm for computing the allsum Z pAq whenever\nthis quantity is finite. Moreover, the derivation will also reveal necessary and sufficient conditions\nfor WFSAs to be normalizable.\nConverting a matrix of pairwise pathsums to the allsum.\nBefore we consider how to\ncompute Z pAq, let us first consider a much simpler problem. Suppose we had a matrix M, which\ncontained at the entry Mij the sum of all the inner weights over all paths between the states i and\nj, i.e.,\nMij “\nÿ\nπPΠpA,i,jq\nwI pπq .\nHow could we then compute the quantity Z pAq?\nZ pAq “\nÿ\nπPΠpAq\nw pπq\n(4.16)\n“\nÿ\nπPΠpAq\nλ pp pπqq wI pπq ρ pn pπqq\n(4.17)\n“\nÿ\ni,jPQ\nÿ\nπPΠpA,i,jq\nλ pp pπqq wI pπq ρ pn pπqq\n(4.18)\n“\nÿ\ni,jPQ\nÿ\nπPΠpA,i,jq\nλ piq wI pπq ρ pjq\n(4.19)\n“\nÿ\ni,jPQ\nλ piq\n¨\n˝\nÿ\nπPΠpA,i,jq\nwI pπq\n˛\n‚ρ pjq\n(4.20)\n“\nÿ\ni,jPQ\nλ piq Mijρ pjq\n(4.21)\n“ ÝÑλ MÝÑρ ,\n(4.22)\nwhere ÝÑλ and ÝÑρ denote the vectors resulting from the “vectorization” of the functions λ and ρ, i.e.,\nÝÑλ n “ λ pnq and ÝÑρ n “ ρ pnq. This also explains the naming of the functions λ and ρ: the initial\n88\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nweights function λ, “lambda” appears on the left side of the closed form expression for Z pAq and\nthe definition of the path weight (cf. Eq. (4.3)), whereas the final weights function ρ, rho, appears\non the right side of the expression and the definition of the path weight.\nComputing the matrix of pairwise pathsums.\nLet T be the transition matrix of the automaton\nA. Notice that the entry Tij by definition contains the sum of the inner weights of all paths of\nlength exactly 1 (individual transitions) between the states i and j. We also define T0 “ I, meaning\nthat the sum of the weights of the paths between i and j of length zero is 0 if i ‰ j and 1 (the unit\nfor multiplication) if i “ j. This corresponds to not transitioning, i.e., staying in place, if i “ j. We\nnext state a basic result from graph theory.\nLemma 4.1.1\nLet T be the transition matrix of some weighted directed graph G. Then the matrix Td\ncontains the allsum of all paths of length exactly d, i.e.,\nTd\ni,j “\nÿ\nπPΠpA,i,jq\n|π|“d\nwI pπq .\n(4.23)\nProof. By induction on the path length. Left as an exercise for the reader.\n■\nIt follows directly that the matrix\nTďd def\n“\ndÿ\nk“1\nTk\ncontains the pairwise pathsums of paths of length at most d.\nIn general, the WFSA representing a n-gram language model can of course be cyclic. This means\nthat the number of paths in Π pAq might be infinite and they might be of arbitrary length (which is\nthe result of looping in a cycle arbitrarily many times). To compute the pairwise pathsums over all\npossible paths, we, therefore, have to compute\nT˚ def\n“ lim\ndÑ8 Tďd “\n8\nÿ\nd“0\nTd.\n(4.24)\nThis is exactly the matrix form of the geometric sum. Similarly to the scalar version, we can\n4.1. FINITE-STATE LANGUAGE MODELS\n89\nmanipulate the expression Eq. (4.24) to arrive to a closed-form expression for computing it:\nT˚ “\n8\nÿ\nd“0\nTd\n(4.25)\n“ I `\n8\nÿ\nd“1\nTd\n(4.26)\n“ I `\n8\nÿ\nd“1\nTTd´1\n(4.27)\n“ I ` T\n8\nÿ\nd“1\nTd´1\n(4.28)\n“ I ` T\n8\nÿ\nd“0\nTd\n(4.29)\n“ I ` TT˚.\n(4.30)\nIf the inverse of pI ´ Tq exists, we can further rearrange this equation to arrive at\nT˚ “ I ` TT˚\n(4.31)\nT˚ ´ TT˚ “ I\n(4.32)\nT˚ ´ T˚T “ I\n(4.33)\nT˚ pI ´ Tq “ I\n(4.34)\nT˚ “ pI ´ Tq´1.\n(4.35)\nThis means that, if pI ´ Tq exists, we can compute the pairwise pathsums by simply inverting\nit! Using the remark above on how to convert a matrix of pairwise pathsums into the full allsum,\nwe can therefore see that we can globally normalize an n-gram language model by computing a\nmatrix inversion! Since the runtime of inverting a N ˆ N matrix is O\n`\nN 3˘\n, and N “ |Q| for a\ntransition matrix of a WFSA with states Q, we can globally normalize a n-gram language model in\ntime cubic in the number of its states. This is a special case of the general algorithm by Lehmann\n(1977). Note, however, that this might still be prohibitively expensive: as we saw, the number of\nstates in a n-gram model grows exponentially with n, and even small n’s and reasonable alphabet\nsizes might result in a non-tractable number of states in the WFSA with the cubic runtime.\nWe still have to determine when the infinite sum in Eq. (4.24) converges. One can see by writing\nout the product Td in terms of its eigenvalues that the entries of Td diverge towards ˘8 as soon as\nthe magnitude of any of T’s eigenvalues is larger than 1. This means that ∥T∥2 ă 1 (spectral norm)\nis a necessary condition for the infinite sum to exist. This is, however, also a sufficient condition: if\n∥T∥2 ă 1, all of T’s eigenvalues are smaller than 1 in magnitude, meaning that the eigenvalues of\nI ´ T are strictly positive and the matrix I ´ T is invertible.10\nSpeed-ups of the Allsum Algorithm\nThe introduced algorithm for computing the allsum in a WFSA can, therefore, be implemented\nas a matrix inverse. This means that its runtime is O\n´\n|Q|3¯\n, which can be relatively expensive.\n101 ´ λ is an eigenvalues of I ´ T iff λ is an eigenvalue of T.\n90\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nFortunately, faster algorithms exist for WFSAs with more structure (in their transition functions)—\nfor example, the allsum can be computed in time linear in the number of transitions if the automaton\nis acyclic using a variant of the Viterbi algorithm (Eisner, 2016). Furthermore, if the automaton\n“decomposes” into many smaller strongly connected components (i.e., subgraphs that are cyclic), but\nthe components are connected sparsely and form an acyclic graph of components, the allsum can\nalso be computed more efficiently using a combination of the algorithms described above and the\nalgorithm for acyclic WFSA, resulting in a possibly large speedup over the original algorithm.\nImportantly, the allsum algorithm and all the speed-ups are differentiable, meaning that they\ncan be used during the gradient-based training (cf. §3.2.3) of a finite-state language model, where\nthe weights are parametrized using some learnable parameters—we will return to this point shortly.\nLocally Normalizing a Globally Normalized Finite-state Language Model\nAs shown in Theorem 2.4.2, any language model (and thus, any globally-normalized model with a\nnormalizable energy function) can also be locally normalized. In the case of finite-state language\nmodels, we can actually explicitly construct the WFSA representing the locally normalized variant\nusing a procedure that is conceptually similar to the allsum algorithm described here. In contrast\nto the procedure we presented here, however, the local normalization algorithm computes the\npathsums of the paths stemming from every possible state q individually and then “reweights” the\ntransitions depending on the pathsums of their target states r. You can think of this as computing\nthe contributions to the entire allsum from q made by all the individual outgoing transitions from q\nand then normalizing those contributions. This is an instance of the more general weight pushing\nalgorithm.11 This can be summarized by the following theorem:\nTheorem 4.1.1: PFSAs and WFSAs are equally expressive\nNormalizable weighted finite-state automata with non-negative weights and tight probabilistic\nfinite-state automata are equally expressive.\nIn the proof of this theorem, we will make use of the following lemma.\nLemma 4.1.2\nLet A “ pΣ, Q, δ, λ, ρq and q P Q. Then\nZ pA, qq “\nÿ\nq\na{w\nÝÝÑq1PδAL\nω\nˆ\nq\na{¨\nÝÝÑ q1\n˙\nZpA, q1q ` ρ pqq\n(4.36)\nProof. You are asked to show this in Exercise 4.1.\n■\nWe can now prove Theorem 4.1.1\nProof. To prove the theorem, we have to show that any WFSA can be written as a PFSA and vice\nversa.12\n11See Mohri et al. (2008) for a more thorough discussion of weight pushing.\n12By “written as”, we mean that the weighted language is the same.\n4.1. FINITE-STATE LANGUAGE MODELS\n91\nð Since any tight probabilistic FSA is simply a WFSA with Z pAq “ 1, this holds trivially.\nñ Local normalization is a general property of automata resulting from weight pushing. Here,\nwe describe the construction in the special case of working with real-valued weights. See Mohri et al.\n(2008) for a general treatment.\nLet A “ pΣ, Q, δ, λ, ρq be a normalizable WFSA with non-negative weights. We now show that,\nfor any WFSA, there exists a PFSA encoding the same language model. Let AG “ pΣ, Q, δ, λ, ρq\nbe a trim WFSA that encodes a distribution over Σ˚ using Eq. (4.14). We now construct a tight\nprobabilistic finite-state automaton AL “ pΣ, Q, δAL, λAL, ρALq whose language is identical. We\ndefine the initial and final weights of the probabilistic FSA as follows.\nλAL pqq\ndef\n“ λ pqq Z pA, qq\nZ pAq\n(4.37)\nρAL pqq\ndef\n“\nρ pqq\nZ pA, qq\n(4.38)\nWe define the transitions of the probabilistic FSA as follows.\nωAL\nˆ\nq\na{¨\nÝÝÑ q1\n˙\ndef\n“\nω\nˆ\nq\na{¨\nÝÝÑ q1\n˙\nZpA, q1q\nZpA, qq\n(4.39)\nThis means that AL contains the same transitions as A, they are simply reweighted. Note that the\nassumption that A is trimmed means that all the quantities in the denominators are non-zero.\nIt is easy to see that the weights defined this way are non-negative due to the non-negativity of\nA’s weights. Furthermore, the weights of all outgoing arcs from any q P Q and its final weight sum\nto 1:\nÿ\nq\na{w\nÝÝÑq1PδAL\nw ` ρAL pqq\n(4.40)\n“\nÿ\nq\na{w\nÝÝÑq1PδAL\nω\nˆ\nq\na{¨\nÝÝÑ q1\n˙\nZpA, q1q\nZpA, qq\n`\nρ pqq\nZ pA, qq\n(definition of δAL)\n(4.41)\n“\n1\nZ pA, qq\n¨\n˚\n˚\n˝\nÿ\nq\na{w\nÝÝÑq1PδAL\nω\nˆ\nq\na{¨\nÝÝÑ q1\n˙\nZpA, q1q ` ρ pqq\n˛\n‹‹‚\n(4.42)\n“ 1\n(Lemma 4.1.2)\n(4.43)\nIt is also easy to see that the initial weights form a probability distribution over the states of the\n92\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nconstructed automaton.\nÿ\nqPQ\nλAL pqq “\nÿ\nqPQ\nλ pqq Z pA, qq\nZ pAq\n(4.44)\n“\n1\nZ pAq\nÿ\nqPQ\nλ pqq Z pA, qq\n(4.45)\n“\n1\nZ pAqZ pAq “ 1\n(4.46)\nWe now have to show that the probabilities assigned by these two automata match.\nWe\nwill do that by showing that the probabilities assigned to individual paths match, implying that\nstringsums match as well. The probability of a path is defined analogously to a probability of\na string, i.e., pA pπq “\nwpπq\nZpAq (where Z pAq “ 1 for tight probabilistic FSAs).\nLet then π “\nˆ\nq1\na1{w1\nÝÝÝÝÑ q2, . . . , qN´1\naN´1{wN´1\nÝÝÝÝÝÝÝÝÑ qN\n˙\nP Π pAq “ Π pALq. Then, by the definitions of ωAL, λAL,\nand ρAL\npAL pπq “ λAL pq1q\n˜N´1\nź\nn“1\nwn\n¸\nρAL pqNq\n(4.47)\n“ λ pq1q Z pA, q1q\nZ pAq\nN´1\nź\nn“1\nω\nˆ\nqn\na{¨\nÝÝÑ qn`1\n˙\nZpA, qn`1q\nZpA, qnq\nρ pqNq\nZ pA, qNq.\n(4.48)\nNotice that the state-specific allsums of all the inner states of the path (all states apart from q1\nand qN) cancel out as the product moves over the transitions of the path. Additionally, the terms\nZ pA, q1q and Z pA, qNq cancel out with the definitions of λAL and ρAL. This leaves us with\npAL pπq “ λ pq1q\n1\nZpAq\nN´1\nź\nn“1\nω\nˆ\nqn\na{¨\nÝÝÑ qn`1\n˙\nρ pqNq “ pA pπq ,\n(4.49)\nfinishing the proof.\n■\nWhile Theorem 2.4.2 shows that any language model can be locally normalized, Theorem 4.1.1\nshows that in the context of finite-state language models, the locally normalized version of a\nglobally-normalized model is also a finite-state model.\nDefining a Parametrized Globally Normalized Language Model\nHaving learned how an arbitrary normalizable finite-state language model can be normalized,\nwe now discuss how models in this framework can be parametrized to enable fitting them to\nsome training data. Crucial for parameterizing a globally normalized model is a score function\nf δ\nθ : Q ˆ Σ ˆ Q Ñ R, which parametrizes the transitions between the states and thus determines the\nweights of the (accepting) paths. Additionally, we also parameterized the initial and final functions\nf λ\nθ and f ρ\nθ. These parametrized functions then define the automaton Aθ\ndef\n“ pΣ, Q, δθ, λθ, ρθq, where\nδθ\ndef\n“\n\"\nq1\ny{f δ\nθpq1,y,q2q\nÝÝÝÝÝÝÝÝÑ q2\n*\n, λθ pqιq\ndef\n“ f λ\nθ pqιq, and ρθ pqφq\ndef\n“ f ρ\nθ pqφq. Note that we can parametrize\n4.1. FINITE-STATE LANGUAGE MODELS\n93\nthe function fθ in any way we want; for example, the function could be a neural network using\ndistributed representations (we will see a similar example at the end of this section), or it could\nsimply be a lookup table of weights. The fact that the function fθ : pq1, y, q2q can only “look at”\nthe identities of the states and the symbol might seem limiting; however, the states alone can\nencode a lot of information: for example, in n-gram models we describe below, they will encode the\ninformation about the previous n ´ 1 symbols and the transitions will then encode the probabilities\nof transitioning between such sequences of symbols.\nThe globally parametrized model then simply takes in any string y P Σ˚ and computes its\nstringsum value under the parametrized automaton, which in turn, as per Eq. (4.15), defines\nprobabilities of the strings. The quantity Z pAθq can be computed with the allsum algorithm\ndiscussed in §4.1.3. Importantly, since the algorithms for computing the string probabilities are\ndifferentiable, the model defined this way can also be trained with gradient-based learning as\ndescribed in §3.2.3.\nYou might notice that this formulation does not exactly match the formulation of globally\nnormalized models from Definition 2.4.2—the function A : Σ˚ Ñ R does not exactly match the\nform of an energy function as its values are not exponentiated as in Eq. (2.11). However, we tie\nthis back to the definition of globally normalized models by defining an actual energy function as a\nsimple transformation of the stringsum given by Aθ. We can define the globally normalizing energy\nfunction ppAθ\nGN as\nppAθ\nGN pyq\ndef\n“ ´ log pA pyqq ,\n(4.50)\nwhich can be easily seen to, after exponentiating it as in Eq. (2.11), result in the same expression as\nEq. (4.15). With this, we have formulated finite-state language models as general globally normalized\nmodels.\nHaving introduced WFSAs as a formal and abstract computational model which can define a set\nof weighted strings, we now show how it can be used to explicitly model a particularly simple family\nof languages. We arrive at this family of language models when we impose a specific assumption on\nthe set of conditional distributions of the language models that ensures that they are finite-state:\nthe n-gram assumption.\n4.1.4\nTightness of Finite-state Models\nAny normalizable globally normalized finite-state language model is tight by definition because\nthe sum of the scores over all finite strings is finite, and since they are normalized, they sum to 1.\nWe, therefore, focus on locally normalized finite-state models and provide necessary and sufficient\nconditions for their tightness. Locally normalized finite-state models are exactly probabilistic WFSAs\n(Definition 4.1.17). Luckily, the tightness of probabilistic WFSAs can be easily characterized, as the\nfollowing theorem shows.\nTheorem 4.1.2: A sufficient condition for tightness of finite-state language models\nA probabilistic FSA is tight if and only if all accessible states are also co-accessible.\nProof. We prove each direction in turn.\n94\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\n(ñ):\nAssume the WFSA is tight. Let q P Q be an accessible state, which means q can be reached\nafter a finite number of steps with positive probability. By tightness assumption, then there must be\na positive probability path from q to termination, or else the WFSA will not be able to terminate\nafter reaching q, resulting in non-tightness. This means that q is also co-accessible. So, assuming\nthat the WFSA is tight, every accessible state is also co-accessible.\n(ð):\nAssume that all accessible states are co-accessible. First, one may consider a Markov chain\nconsisting only of the set of accessible states QA Ď Q, since all other states will have probability 0\nat every step. Recall a fundamental result in finite-state Markov chain theory which states that, if\nthere exists a unique absorbing state which is reachable from every state, then the Markov process\nis absorbed by this state with probability 1 (see, e.g., Theorem 11.3 in Grinstead and Snell, 1997).\nWe already have that\n• eos is an absorbing state, and that\n• by assumption, every state in QA is co-accessible which implies that they can reach eos.\nHence, it remains to show that eos is the unique absorbing state. Suppose there is another state\n(or group of states) in QA distinct from eos that is absorbing, i.e., cannot leave once entered. Then,\nthese states cannot reach eos by assumption, which means they are not co-accessible, contradicting\nthe assumption that every state in QA is co-accessible. Hence, eos is the only absorbing state in QA\nand by the property of an absorbing Markov chain, the process is absorbed by eos with probability\n1. In other words, the WFSA is tight.\n■\nNotice that trimming a PFSA results in a model that satisfies ρ pqq ` ř\nq\na{w\nÝÝÑq1 w ď 1, but might\nno longer achieve equality as required by Definition 4.1.17. We call such models substochastic\nWFSAs.\nDefinition 4.1.23: Substochastic Weighted Finite-State Automaton\nA WFSA A “ pΣ, Q, δ, λ, ρq is substochastic if for all q P Q and all outgoing transitions\nq\na{w\nÝÝÑ q1 P δ it holds that\nλ pqq ě 0\n(4.51)\nρ pqq ě 0\n(4.52)\nw ě 0\n(4.53)\nand\nρ pqq `\nÿ\nq\na{w\nÝÝÑq1\nw ď 1.\n(4.54)\nWe can then express the termination probability of a WFSA in simple linear algebra terms.\n4.1. FINITE-STATE LANGUAGE MODELS\n95\nTheorem 4.1.3: A sufficient condition for the tightness of a sub-stochastic WFSA\nLet T1 be the transition sum matrix of a trimmed substochastic WFSA. Then I ´ T1 is\ninvertible and p px P Σ˚q “ ÝÑλ\n1JpI ´ T1q´1ÝÑρ 1 ď 1.\nIn the following, we will make use of the spectral radius of a matrix.\nDefinition 4.1.24: Spectral radius\nThe spectral radius of a matrix M P CNˆN with eigenvalues λ1, . . . , λN is defined as\nρs pMq\ndef\n“ max t|λ1|, . . . , |λN|u .\n(4.55)\nTo prove Theorem 4.1.3, we will make use of the following useful lemma.\nLemma 4.1.3\nLet T1 be the transition sum matrix of a trimmed substochastic WFSA, then ρspT1q ă 1.\nTo begin with, we wish to apply the following result which connects the row sums of a matrix to its\nspectral radius. Below, MN denotes the set of N ˆ N matrices, and ∥A∥8 “ max1ďnďN\nřN\ni“1 |Ani|\ndenotes the infinity matrix norm.\nProposition 4.1.1: §6.2.P8; Horn and Johnson, 2012\nFor any A P MN, ρspAq ď ∥A∥8. Additionally, if A is irreducible and not all absolute row\nsums of A are equal, then ρspAq ă ∥A∥8.\nHowever, the transition sum matrix P of a substochastic WFSA may be reducible whereas the\nirreducibility condition in Proposition 4.1.1 cannot be dropped. Hence, we need to “decompose” T1\nin a way to recover irreducibility. We use the Frobenius normal form (also known as irreducible\nnormal form) to achieve this.\nProposition 4.1.2: §8.3.P8; Horn and Johnson, 2012\nLet A P MN be non-negative. Then, either A is irreducible or there exists a permutation\nmatrix P such that\nPJAP “\n»\n—–\nA1\n˚\n...\n0\nAK\nfi\nffifl\n(4.56)\nis block upper triangular, and each diagonal block is irreducible (possibly a 1-by-1 zero matrix).\nThis is called an Frobenius normal form (or irreducible normal form) of A. Additionally,\nΛpAq “ ΛpA1q Y ¨ ¨ ¨ Y ΛpAKq where Λp¨q denotes the set of eigenvalues of a matrix.\nWe now proceed to the proof of Lemma 4.1.3.\n96\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nProof. Notice that, by way of a similarity transformation via a permutation matrix, the Frobenius\nnormal form is equivalent to a relabeling of the states in the trimmed WFSA in the sense of\npPJÝÑλ\n1qJpPJT1PqKpPJÝÑρ 1q “ pÝÑλ\n1JPqpPJT1KPqpPJÝÑρ 1q\n(4.57a)\n“ ÝÑλ\n1JT1KÝÑρ 1\n(4.57b)\nwhere the equalities follow from the fact that the inverse of a permutation matrix P is its transpose.\nHence, with an appropriate relabeling, we may assume without loss of generality that P is already\nput into a Frobenius normal form\nT1 “\n»\n—–\nT1\n1\n˚\n...\n0\nT1\nK\nfi\nffifl\n(4.58)\nwhere each T1\nk is irreducible.\nSince the transition sum matrix T1 of a trimmed substochastic WFSA is a substochastic matrix,\neach T1\nk is also substochastic. In fact, each T1\nk is strictly substochastic, meaning that there is at least\na row that sums to less than 1. To see this, suppose to the contrary that there is a probabilistic T1\nk.\nSince the WFSA is trimmed, every state is both accessible and co-accessible. Being accessible implies\nthat there is a positive probability of reaching every state in T1\nk. However, the probabilisticity\nof T1\nk forces the corresponding ÝÑρ 1 entries to be 0. Hence, none of these states can transition\nto eos, meaning that they’re not co-accessible, contradicting the assumption. Hence, every T1\nk\nis strictly substochastic and has at least one strictly less than 1 row sum. Then, either all row\nsums of T1\nk are less than 1 or some row sums are 1 and some are less than 1. In either cases,\nProposition 4.1.1 implies that ρspT1\nkq ă 1 for all 1 ď k ď K. Finally, as Proposition 4.1.2 entails,\nρspT1q “ maxtρspT1\n1q, . . . , ρspT1\nKqu where each ρspT1\nkq ă 1. Hence, ρspT1q ă 1.\n■\nWe now use the stated results to finally prove Theorem 4.1.3.\nProof. By Lemma 4.1.3, ρspT1q ă 1, in which case I ´ T1 is invertible and the Neumann series\nI ` T1 ` T12 ` ¨ ¨ ¨ converges to pI ´ T1q´1 (§5.6, Horn and Johnson, 2012). Hence, we can write\npI ´ T1q´1 “ ř8\nk“0 T1k. Then,\nppΣ˚q “\n8\nÿ\nk“0\nPpΣkq\n(4.59a)\n“\n8\nÿ\nk“0\nÝÑλ\n1JT1kÝÑρ 1\n(4.59b)\n“ ÝÑλ\n1J\n˜ 8\nÿ\nk“0\nT1k\n¸\nÝÑρ 1\n(4.59c)\n“ ÝÑλ\n1JpI ´ T1q´1ÝÑρ 1.\n(4.59d)\n■\n4.1. FINITE-STATE LANGUAGE MODELS\n97\nThe\nquick\nbrown\nfox\njumps\nover . . .\npLM pfox | The quick brownq\npLM pjumps | quick brown foxq\npLM pover | brown fox jumpsq\n¨\n¨\n¨\nFigure 4.4: An illustration of how an 4-gram LM computes the probability of a string. All conditional\nprobabilities can be computed in parallel and then multiplied into the probability of the entire\nstring.\n4.1.5\nThe n-gram Assumption and Subregularity\nWe now turn our attention to one of the first historically significant language modeling frameworks:\nn-gram models. While they are often taught completely separately from (weighted) finite-state\nautomata, we will see shortly that they are simply a special case of finite-state language models and\nthus all results for the more general finite-state language models also apply to the specific n-gram\nmodels as well.\nAs we saw in Theorem 2.4.2, we can factorize the language model pLM for y “ y1 . . . yT P Σ˚ as\npLM pyq “ pLN pyq “ pSMpeos | yq\nT\nź\nt“1\npSMpyt | yătq,\n(4.60)\nwhere pSMpy | yq are specified by a locally normalized model (Definition 2.4.5).\nRecall that SMs specify individual conditional distributions of the next symbol yt given the\nprevious t ´ 1 symbols for all possible t. However, as t grows and the history of seen tokens\naccumulates, the space of possible histories (sequences of strings to condition on) grows very large\n(and indeed infinite as t Ñ 8). This makes the task of modeling individual conditional distributions\nfor large t computationally infeasible. One way to make the task more manageable is by using the\nn-gram assumption.\nAssumption 4.1.1: n-gram assumption\nIn words, n-gram assumption states that the conditional probability of the symbol yt given\nyăt only depends on n ´ 1 previous symbols yt´1\nt´n`1\ndef\n“ yt´1, . . . , yt´n`1:\npSM pyt | yătq “ pSM\n`\nyt | yt´1\nt´n`1\n˘\n.\n(4.61)\nWe will refer to yt´1\nt´n`1 as the history of yt. The sequence yt´1 ¨ ¨ ¨ yt´n`1q is often called\nthe history or the context.\nIn plain English, this means that the probability of a token only depends on the previous n ´ 1\ntokens. n-gram assumption is, therefore, an alias of pn ´ 1qth order Markov assumption in the\nlanguage modeling context.\nHandling edge cases by padding.\nGiven our definition in Eq. (4.61) where the conditional\nprobability pSM pyt | yt´n´1:t´1q depends on exactly n ´ 1 previous symbols, we could run into an\n98\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nissue with negative indices for t ă n. To handle edge cases for t ă n, we will pad the sequences with\nthe bos symbols at the beginning, that is, we will assume that the sequences y1 . . . yt for t ă n ´ 1\nare “transformed” as\ny1y2 . . . yt ÞÑ bos . . . bos\nlooooomooooon\nn´1´t times\ny1y2 . . . yt\n(4.62)\nNotice that with such a transformation, we always end up with strings of length n ´ 1, which is\nexactly what we need for conditioning in an n-gram model. In the following, we will assume that all\nsuch sequences are already transformed, but at the same time, we will assume that\npSM\n¨\n˝y | bos . . . bos\nlooooomooooon\nn´1´t times\ny1y2 . . . yt\n˛\n‚“ y0y1y2 . . . yt\n(4.63)\nBy definition, n-gram language models can only model dependencies spanning n tokens or less.\nBy limiting the length of the relevant context when determining pSM pyt | yătq to the previous n\ntokens, the n-gram assumption limits the number of possible probability distributions that need to\nbe tracked to O\n`\n|Σ|n´1˘\n.\nDespite their simplicity, n-gramLMs have a storied place in language modeling (Shannon, 1948a;\nBaker, 1975a,b; Jelinek, 1976; Bahl et al., 1983; Jelinek, 1990; Bengio et al., 2000, 2003a, 2006;\nSchwenk, 2007; Heafield, 2011; Heafield et al., 2013). Because the conditional probabilities of\nn-gramLMs only depend on the previous n ´ 1 symbols, different parts of the string can be processed\nindependently, i.e., in parallel. This facilitates a natural connection to transformer LMs since\nparallelizability is a prevalent feature of the architecture and one of its main advantages over other\nneural LMs such as RNN LMs (Vaswani et al., 2017).\nA particularly simple case of the n-gram model is the bigram model where n “ 2, which\nmeans that the probability of the next word only depends on the previous one, i.e., pSM pyt | yătq “\npSM pyt | yt´1q.13\nExample 4.1.5: A simple bigram model\nLet us look at a specific example of a simple bigram model. Suppose our vocabulary consists\nof the words “large”, “language”, and “models”, thus, |Σ| “ 3. To specify the bigram\nmodel, we have to define the conditional probabilities pM pyj | yiq for yi P Σ Y tbos, eosu and\nyj P Σ Y teosu (remember that we do not have to model the probability of the next token\nbeing bos). In the case of bigrams, we can represent those in a table, where the entry at\nposition i, j represents the probability pM pyj | yiq:\n“large”\n“language”\n“models”\n“EOS”\nbos\n0.4\n0.2\n0.2\n0.2\n“large”\n0.1\n0.4\n0.2\n0.3\n“language”\n0.1\n0.1\n0.4\n0.4\n“models”\n0.2\n0.2\n0.1\n0.5\n“EOS”\n0.4\n0.2\n0.2\n0.2\n13What would the uni-gram (n “ 1) model look like? What conditional dependencies between words in a sentence\ncould be captured by it?\n4.1. FINITE-STATE LANGUAGE MODELS\n99\nUnder our model, the probability of the sentence “large language models” would be\npSM p“large” | bosq\n¨ pSM p“language” | “large”q\n¨ pSM p“models” | “language”q\n¨ pSM peos | “models”q\n“ 0.4 ¨ 0.4 ¨ 0.4 ¨ 0.5 “ 0.032\nwhile the probability of the sentence “large large large” would be\npSM p“large” | bosq\n¨ pSM p“large” | “large”q\n¨ pSM p“large” | “large”q\n¨ pSM peos | “large”q\n“ 0.4 ¨ 0.1 ¨ 0.1 ¨ 0.3 “ 0.0012.\nNote that the probabilities in the above table are made up and not completely reasonable. A\nreal n-gram model would not allow for probabilities of exactly 0 to avoid pathological behavior.\nRepresenting n-gram Models as WFSAs\nWe define n-gram language models as models that only consider a finite amount of context when\ndefining the conditional probabilities of the next token. This means that the set of possible conditional\ndistributions pSM py | yq is also finite which very naturally connects them to weighted finite-state\nautomata—indeed, every n-gram language model is a WFSA—specifically, a probabilistic finite-state\nautomaton (or a substochastic one). We will make this connection more formal in this subsection,\nthus formally showing that n-gram models are indeed finite-state. Note that this is different from\n§4.1.3, where we discussed how to parametrize a general WFSA and use it as a globally normalized\nmodel—in contrast, in this section, we consider how to fit a (locally normalized) n-gram model into\nthe finite-state framework.\nThe intuition behind the connection is simple: the finite length of the context implies a finite\nnumber of histories we have to model. These histories represent the different states the corresponding\nautomaton can reside in at any point. Given any history y with |y| ă n and the state q P Q\nrepresenting y, then, the conditional distribution of the next token given y dictate the transition\nweights into the next states in the WFSA, representing the new, updated history of the input.\nImportantly, since we want PFSAs to represent globally-normalized models, we will also remove\nthe eos symbol from the n-gram model before transforming it into a PFSA—as the remark above\nabout the relationship between the eos symbol and the final states hints, the latter will fill in\nthe role of the eos symbol. The way we do that is the following. From the semantics of the eos\nsymbol discussed in the section on tightness (cf. Eq. (2.44)), we also know that to model the\nprobability distribution over finite strings in Σ˚, we only require to keep track of strings up to\nthe first occurrence of the eos symbol. Therefore, when converting a given n-gram model to a\nWFSA, we will only model sequences up to the first occurrence of the special symbol, meaning that\neos will never occur in the context of any conditional distribution pSM py | yq. We now detail this\n100\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nconstruction.\nLet pLN be a well-defined n-gram language model specified by conditional distributions pSM as\ndefined by §4.1.5. We will now construct a WFSA representing pLN. Intuitively, its states will\nrepresent all possible sequences of words of length n while the transitions between the states q1\nand q2 will correspond to the possible transitions between the n-grams which those represent. This\nmeans that the only possible (positively weighted) transitions will be between the n-grams which\ncan follow each other, i.e. yt´n:t´1 and yt´n`2:t for some yt´n, yt P Σ (until the first occurrence of\neos). The transition’s weight will depend on the probability of observing the “new” word y0 in the\nsecond n-gram given the starting n-gram y´ny´pn´1q . . . y´1. Further, the final weights of the states\nwill correspond to ending the string in them. In pLN, this is modeled as the probability of observing\neos given the context yt´n:t´1—this, therefore, is set as the final weight of the state representing\nthe history yt´n:t´1. Formally, we can map a n-gram model into a WFSA A “ pΣA, QA, δA, λA, ρAq\nby constructing A as follows.\n• Automaton’s alphabet:\nΣA\ndef\n“ Σ\n(4.64)\n• The set of states:\nQA\ndef\n“\nn´1\nď\nt“0\ntbosun´1´t ˆ Σt\n(4.65)\n• The transitions set\nδA\ndef\n“ tyt´n:t´1\nyt{pSMpyt|yt´n:t´1q\nÝÝÝÝÝÝÝÝÝÝÝÝÝÑ yt´n`1:t |\n(4.66)\nyt´n`1:t´1 P\nn´2\nď\nt“0\ntbosun´2´t ˆ Σt; yt´n, yt P Σu\n• The initial function:\nλA : y ÞÑ\n$\n&\n%\n1\nif y “ bos . . . bos\nlooooomooooon\nn´1 times\n0\notherwise\n(4.67)\n• The final function\nρA : y ÞÑ pSM peos | yq , y P QA\n(4.68)\nThe definition of the states set QA captures exactly the notion of padding with the bos symbol for\nhandling the edge cases we described above. This shows that n-gram language models are indeed\nfinite-state (we leave the formal proof showing that L pAq “ L ppLNq to the reader.\nDefining a n-gram language model through a parametrized WFSA.\nWe now consider\nhow we can use the framework of WFSA to define a more “flexible” parametrized globally normalized\nmodel. In this case, we do not start from an existing locally normalized set of distributions forming\npSM. Rather, we would like to model the “suitability” of different n-grams following each other—that\nis, we would like to somehow parametrize the probability that some n-gram y1 will follow an n-gram\ny without having to worry about normalizing the model at every step. This will allow us to then fit\nthe probability distributions of the model to those in the data, e.g., with techniques described in\n§3.2.3. Luckily, the flexibility of the WFSA modeling framework allows us to do exactly that.\n4.1. FINITE-STATE LANGUAGE MODELS\n101\nSubregularity\nWe saw that language models implementing the very natural n-gram assumption can be represented\nusing weighted finite-state automata. However, n-gram models do not “need the full expressive\npower” of WFSAs—they can actually be modeled using even simpler machines than finite-state\nautomata. This, along with several other examples of simple families of formal languages, motivates\nthe definition of subregular languages.\nDefinition 4.1.25: Subregular language\nA language is subregular if it can be recognized by a finite-state automaton or any weaker\nmachine.\nMost subregular languages can indeed be recognized by formalisms which are much simpler than\nFSAs. Many useful and interesting classes of subregular languages have been identified—recently,\nespecially in the field of phonology. Naturally, due to their simpler structure, they also allow for\nmore efficient algorithms—this is why we always strive to represent a language with the simplest\nformalism that still captures it adequately. See J¨ager and Rogers (2012); Avcu et al. (2017) for\ncomprehensive overviews of subregular languages.\nSubregular languages actually form multiple hierarchies of complexity within regular languages.\nInterestingly, n-gram models fall into the simplest level of complexity in one of the hierarchies,\ndirectly above finite languages. This class of subregular languages is characterized by patterns that\ndepend solely on the blocks of symbols that occur consecutively in the string, which each of the\nblocks considered independently of the others—it is easy to see that n-gram models intuitively\nfall within such languages. This family of subregular languages is suggestively called strictly local\nlanguages.\nDefinition 4.1.26: Strictly local languages\nA language L is strictly n-local (SLn) if, for every string y of length |y| “ n ´ 1, and all\nstrings x1, x2, z1, z2 P Σ˚, it holds that if x1yz1 P L and x2yz2 P L, then also x1yz2 P L\n(and x2yz1 P L).\nA language is strictly local (SL) if it is strictly n-local for any n.\nNote that we could of course also define this over with the eos-augmented alphabet Σ. You can\nvery intuitively think of this definition as postulating that the history more than n symbols back\ndoes not matter anymore for determining or specifying whether a string is in a language (or its\nweight, in the weighted case)—this is exactly what the n-gram assumption states.\n4.1.6\nRepresentation-based n-gram Models\nSo far, we have mostly talked about the conditional probabilities and the WFSA weights defining\na language model very abstractly. Apart from describing how one can generally parametrize the\nweights of the underlying WFSA with the scoring function in §4.1.5, we only discussed what values\nthe weights can take for the language model to be well-defined and what implications that has\non the distribution defined by the WFSA. In this section, we consider for the first time what an\nactual implementation of a finite-state, or more precisely, a n-gram language model might look\n102\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nlike. Concretely, we will define our first parameterized language model in our General language\nmodeling framework (cf. §3.1) by defining a particular form of the encoding function enc as a simple\nmulti-layer feed-forward neural network.14\nHowever, before we dive into that, let us consider as an alternative possibly the simplest way to\ndefine a (locally normalized) n-gram language model: by directly parametrizing the probabilities of\neach of the symbols y in the distribution pSM py | yq for any context y, that is\nθ\ndef\n“\n$\n&\n%θy|y\ndef\n“ pSM py | yq | y P Σ, y P Σ\nn´1, θy|y ě 0,\nÿ\ny1PΣ\nθy|y “ 1\n,\n.\n- .\n(4.69)\nThe following proposition shows that the maximum likelihood solution (Eq. (3.60)) to this parametriza-\ntion is what you would probably expect.\nProposition 4.1.3\nThe MLE solution of Eq. (4.69) is\npSM pyn | yănq “\nC py1, . . . , ynq\nC py1, . . . , yn´1q\n(4.70)\nwhenever the denominator ą 0, where C py1, . . . , ynq denotes the number of occurrences of all\npossible strings of the form y1, . . . , yn and C py1, . . . , ynq denotes the number of occurrences of\nall possible strings of the form y1, . . . , yn´1.\nProof. Let D\ndef\n“\n␣\nyp1q, . . . , ypMq(\nbe the training dataset. The log-likelihood of a single example\nypmq is\nlog ppLN pyqq “ log\n¨\n˝\n|ypmq|\nź\nt“1\npSM\n´\nypmq\nt\n| ypmq\nt´n:t´1\n¯\n˛\n‚\n(4.71)\n“\n|ypmq|\nÿ\nt“1\nlog pSM\n´\nypmq\nt\n| ypmq\nt´n:t´1\n¯\n(4.72)\nwhich means that the log-likelihood of the entire dataset is\nℓℓpDq “\nM\nÿ\nm“1\n|ypmq|\nÿ\nt“1\nlog pSM\n´\nypmq\nt\n| ypmq\nt´n:t´1\n¯\n(4.73)\n“\nM\nÿ\nm“1\n|ypmq|\nÿ\nt“1\nlog θyn|yăn.\n(4.74)\n14While we introduce particular architectures of neural networks, for example, recurrent neural networks and\ntransformers later in Chapter 4, we assume some familiarity with neural networks in general. See Chapter 6 of\nGoodfellow et al. (2016) for an introduction.\n4.1. FINITE-STATE LANGUAGE MODELS\n103\nExercise 4.2 asks you to show that this can be rewritten with the token to type switch as\nℓℓpDq “\nÿ\ny\n|y|“n\nC pyq θyn|yăn.\n(4.75)\nThe maximum likelihood parameters can then be determined using Karush–Kuhn–Tucker (KKT)\nconditions15 to take into account the non-negativity and local normalization constraints:\n∇θ\n¨\n˚\n˚\n˝ℓℓpDq ´\nÿ\nyPΣ˚\n|y|“n´1\nλyy\n˜ ÿ\nyPΣ\nθy|y ´ 1\n¸\n´\nÿ\nyPΣ˚\n|y|“n´1\nηyyθy|y\n˛\n‹‹‚“ 0.\n(4.76)\nRecall that the KKT conditions state that a θ is an optimal solution of ℓℓif and only if\n´\nθ, tλyy1uyPΣn´1,yPΣ , tηyy1uyPΣn´1,yPΣ\n¯\nsatisfy Eq. (4.76). Since this is simply a sum over the dataset with no interactions of parameters for\nindividual contexts y with |y| “ n ´ 1 in θy|y, it can be solved for each context y individually.\nMoreover, as you are asked to show Exercise 4.3, it holds that\nÿ\ny1PΣ\nC\n`\ny1 . . . yn´1y1˘\n“ C py1 . . . yn´1q\n(4.77)\nfor any y “ y1 . . . yn´1 P Σn´1. This leaves us with the following system for each y P Σn´1:\nÿ\ny1PΣ\nC\n`\nyy1˘\nlog θy1|y ´ λy\n˜ ÿ\ny1PΣ\nθy1|y ´ 1\n¸\n´\nÿ\ny1PΣ\nηyy1θy1|y.\n(4.78)\nIt is easy to confirm that θy|y “ Cpyyq\nCpyq with λy “ C pyq and ηyy1 “ 0 is a saddle point of Eq. (4.76).\nThis means that θy|y “ Cpyyq\nCpyq is indeed the maximum likelihood solution.\n■\nThis results in a locally normalized n-gram model. To avoid issues with division-by-zero and\nassigning 0 probability to unseen sentences, we can employ methods such as smoothing and backoff,\nwhich are beyond the scope of the course.16\nWhile this model might seem like an obvious choice, it comes with numerous drawbacks. To see\nwhat can go wrong, consider the following example.\nExample 4.1.6: n-gram model\nSuppose we have a large training corpus of sentences, among which sentences like “We are\ngoing to the shelter to adopt a dog.”, “We are going to the shelter to adopt a puppy.”, and\n“We are going to the shelter to adopt a kitten.”, however, without the sentence “We are going\nto the shelter to adopt a cat.” Fitting an n-gram model using the count statistics and individual\ntables of conditional probabilities pSM py | yq, we would assign the probability\npSM pyt “ cat | yăt “ We are going to the shelter to adopt aq\n15See https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions.\n16See (Chen and Goodman, 1996) and Chapter 4 in (Jurafsky and Martin, 2009).\n104\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nthe value 0 (or some “default” probability if we are using smoothing). However, the words\n“dog”, “puppy”, “kitten”, and “cat” are semantically very similar—they all describe pets often\nfound in shelters. It would therefore be safe to assume that the word “cat” is similarly probable\ngiven the context “We are going to the shelter to adopt a” as the other three words observed\nin the training dataset. However, if we estimate all the conditional probabilities independently,\nwe have no way of using this information—the words have no relationship in the alphabet,\nthey are simply different indices in a lookup table. Additionally, statistics gathered for the\nsentences above will not help us much when encountering very similar sentences, such as\n“We went to a nearby shelter and adopted a kitten.” The issue is that there are simply many\nways of expressing similar intentions. We would thus like our language models to be able\nto generalize across different surface forms and make use of more “semantic” content of the\nsentences and words. However, if the model is parametrized as defined in Eq. (4.69), it is not\nable to take advantage of any such relationships.\nThe model defined by Eq. (4.69) is therefore unable to take into account the relationships and\nsimilarities between words. The general modeling framework defined in §3.1 allows us to remedy\nthis using the distributed word representations. Recall that, in that framework, we associate\neach word y with its vector representation epyq (its embedding), and we combine those into the\nembedding matrix E. Importantly, word embeddings are simply additional parameters of the model\nand can be fit on the training dataset together with the language modeling objective. One of the\nfirst successful applications of encp¨q is due to Bengio et al. (2003b), which we discuss next.\nTo be able to use the embeddings in our general framework, we now just have to define the\nconcrete form of the context-encoding function enc. In the case of the neural n-gram model which\nwe consider here and as defined by (Bengio et al., 2003b), the representations of the context yăt,\nenc pyătq, are defined as the output of a neural network which looks at the previous n ´ 1 words in\nthe context:\nencpyătq\ndef\n“ encpyt´1, yt´2, . . . , yt´n`1q,\n(4.79)\nwhere enc is a neural network we define in more detail shortly. The full language model is therefore\ndefined through the conditional distributions\npSM pyt | yătq\ndef\n“ softmax\n´\nenc\n`\nyt´1, yt´2, . . . , yt´n`1\n˘J E ` b\n¯\nyt\n(4.80)\nresulting in the locally normalized model\npLN pyq “softmax\n´\nenc\n`\nyT , yT ´1, . . . , yT ´n`2\n˘J E ` b\n¯\neos\n(4.81)\n¨\nT\nź\nt“1\nsoftmax\n´\nenc pyt´1, yt´2, . . . , yt´n`1qJ E ` b\n¯\nyt\n(4.82)\nfor y P Σ˚.\nImportantly, notice that although this is a neural model, it is nonetheless still an n-gram model\nwith finite context—Eq. (4.79) is simply a restatement of the n-gram assumption in terms of the\nneural encoding function enc. It therefore still suffers from some of the limitations of regular n-gram\nmodels, such as the inability to model dependencies spanning more than n words. However, it solves\nthe problems encountered in Example 4.1.6 by considering word similarities and sharing parameters\nacross different contexts in the form of an encoding function rather than a lookup table.\n4.1. FINITE-STATE LANGUAGE MODELS\n105\nWhile encoding function enc in Eq. (4.79) could in principle take any form, the original model\ndefined in Bengio et al. (2003b) defines the output as for the string y “ yt, yt´1, . . . , yt´n`1 as\nenc pyt, yt´1, . . . , yt´n`1q\ndef\n“ b ` Wx ` U tanh pd ` Hxq ,\n(4.83)\nwhere x\ndef\n“ concat pepytq, epyt´1q, . . . , epyt´n`1qq denotes the concatenation of the context symbol\nembeddings into a long vector of size pn ´ 1q ¨ R, and b, d, W, and U define the parameters of the\nencoding function. This completes our definition of the model in the general language modeling\nframework—the model can then simply be trained on the language modeling objective as defined in\n§3.2.2.\nWe can also see that such a model also reduces the number of parameters required to specify\na n-gram model: whereas a lookup-table-based n-gram model with no parameter sharing requires\nO p|Σ|nq parameters to be defined, the number of parameters required by a representation-based\nn-gram model scales linearly with n—all we have to do is add additional rows to the matrices defined\nin Eq. (4.83). We will later see how this can be reduced to a constant number of parameters w.r.t.\nthe sequence length in the case of recurrent neural networks in §5.1.2.\nPictorially, we can imagine the model as depicted in Fig. 4.5 (taken from the original publication).\nThis shows that the n-gram modeling framework is not limited to counting co-occurrence statistics.\nThe model from Eq. (4.79) can also be represented by a WFSA just like the simpler models we\ndiscussed above, with the weights on the transitions parametrized by the neural network. This\nallows us to both understand well with insights from formal language theory, as well as to train\nit in a flexible way allowed for by the non-linear encoding function. However, the model from\nEq. (4.79) is still limited to statistics of the last n tokens or less. If we want to model arbitrarily long\ndependencies and hierarchical structures, we have to leave the space of finite-state languages behind\nand develop formalisms capable of modeling more complex languages. The next section explores the\nfirst of such frameworks: context-free languages with the computational models designed to model\nthem.\n106\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nFigure 4.5: A pictorial depiction of the n-gram neural language model from the original publication\n(Bengio et al., 2003b). Note that the quantity C pwq corresponds to epyq for a word y in our notation.\n4.2. PUSHDOWN LANGUAGE MODELS\n107\n4.2\nPushdown Language Models\nAn strong limitation of finite-state language models is that they can definitionally only distinguish a\nfinite set of contexts. However, human language has inherently more structure than what a finite set\nof contexts can encode. For example, human language contains arbitrarily deep recursive structures\nwhich cannot be captured by a finite set of possible histories—we will see an example of this soon in\n§4.2.1.\nTo be able to model these structures we are climbing a rung higher on the ladder of the hierarchy\nof formal languages: we are going to consider context-free languages, a larger class of languages\nthan regular languages. Luckily, we will see that a lot of the formal machinery we introduce in\nthis section closely follows analogs from the finite-state section and we invite the reader to pay\nclose attention to the parallels. For example, similarly to how we weighted a string in a regular\nlanguage by summing over the weights of the paths labeled with that string, we will weight strings\nin context-free languages by summing over analogous structures.\nTo be able to recognize context-free languages, will have to extend finite-state automata from\n§4.1.1 with an additional data structure—the stack. Finite-state automata augmented with a stack\nare called pushdown automata. We introduce them in §4.2.7. Before giving a formal treatment of\npushdown automata, however, we will discuss an arguably more natural formalism for generating\nthe context-free languages—context-free grammars.17\nIn the last part of the section, we will then further extend the regular pushdown automaton with\nan additional stack. Interestingly, this will make it more powerful: as we will see, it will raise its\nexpressive power from context-free languages to all computable languages, as it is Turing complete.\nWhile this augmentation will not be immediately useful from a language modeling perspective, we\nwill then later use this machine to prove some theoretical properties of other modern language\nmodels we consider later in the course.\n4.2.1\nHuman Language Is not Finite-state\nAs hinted above, human language contains structures that cannot be modeled by finite-state\nautomata. Before we introduce ways of modeling context-free languages, let us, therefore, first\nmotivate the need for a more expressive formalism by more closely considering a specific phenomenon\noften found in human language: recursive hierarchical structure. We discuss it through an example,\nbased on Jurafsky and Martin (2009).\nExample 4.2.1: Center embeddings\nConsider the sentence:\n“The cat likes to cuddle.”\nIt simply describes a preference of a cat. However, we can also extend it to give additional\ninformation about the cat:\n“The cat the dog barked at likes to cuddle.”\n17You might wonder what non-context-free grammars are: a superclass of context-free grammars is that of context-\nsensitive grammars, in which a production rule may be surrounded by a left and right context. They are still however\na set of restricted cases of general grammars, which are grammars that can emulate Turing machines.\n108\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nThis sentence, in turn, can be extended to include additional information about the dog:\n“The cat the dog the mouse startled barked at likes to cuddle.”\nOf course, we can continue on:\n“The cat the dog the mouse the rat frightened startled barked at likes to cuddle.”\nand on:\n“The cat the dog the mouse the rat the snake scared frightened startled barked at likes to cuddle.”\nIn theory, we could continue like this for as long as we wanted—all these sentences are\ngrammatically correct—this is an instance of the so-called center embeddings.\nCrucially, such sentences cannot be captured by a regular language, i.e., a language based\non an automaton with finitely many states. While we would need formal machinery beyond\nthe scope of this course to formally prove this, the intuition is quite simple. By adding more\nand more “levels” of recursion to the sentences (by introducing more and more animals in\nthe chain), we unboundedly increase the amount of information the model has to “remember”\nabout the initial parts of the sentence while processing it sequentially, to be able to process\nor generate the matching terms on the other end of the sentence correctly. Because such\nhierarchies can be arbitrarily deep (and thus the sentences arbitrarily long), there is no bound\non the number of states needed to remember them, which means they cannot be captured by\na finite-state automaton.\nNote that this example also touches upon the distinction of the grammatical competence versus\ngrammatical performance (Chomsky, 1959; Chomsky and Sch¨utzenberger, 1963; Chomsky,\n1965). The former refers to the purely theoretical properties of human language, for example,\nthe fact that such hierarchical structures can be arbitrarily long and still grammatically correct.\nGrammatical performance, on the other hand, studies language grounded more in the way\npeople actually use it. For example, nested structures like the one above are never very deep\nin day-to-day speech—indeed, you probably struggled to understand the last few sentences\nabove. We rarely come across nestings of depth more than three in human language (Miller\nand Chomsky, 1963; Jin et al., 2018; Karlsson, 2007).\n4.2.2\nContext-free Grammars\nHow can we capture recursive structures like those in Example 4.2.1 and the long-term dependencies\narising from them? The first formalism modeling such phenomena we will introduce is context-free\ngrammars: a generative formalism which can tell us how to generate or “compose“ strings in the\nlanguage it describes. Later in the section (§4.2.7), we will introduce the context-free analog of\nfinite-state automata, which will tell us how to recognize whether a string is in a context-free\nlanguage (rather than generate a string): pushdown automata.\nDefinition 4.2.1: Context-free Grammar\nA context-free grammar (CFG) is a 4-tuple G “ pΣ, N, S, Pq where Σ is an alphabet of\nterminal symbols, N is a non-empty set of non-terminal symbols with N X Σ “ H, S P N is\n4.2. PUSHDOWN LANGUAGE MODELS\n109\nthe designated start non-terminal symbol and P is the set of production rules, where each\nrule p P P is of the form X Ñ α with X P N and α P pN Y Σq˚.a\naAs is the case for initial states in FSAs, multiple start symbols could be possible. However we consider\nonly one for the sake of simplicity.\nExample 4.2.2: A simple context-free grammar\nLet G “ pΣ, N, S, Pq be defined as follows:\n• Σ “ ta, bu\n• N “ tXu\n• S “ X\n• P “ tX Ñ aXb, X Ñ εu\nThis defines a simple context-free grammar. We will return to it later, when we will formally\nshow that it generates the language L “ tanbn | n P Ně0u.\nRule Applications and Derivations\nContext-free grammars allow us to generate strings y P Σ˚ by applying production rules on its\nnon-terminals. We apply a production rule X Ñ α to X P N in a rule p by taking X on the\nright-hand side of p and replacing it with α.18\nDefinition 4.2.2: Rule Application\nA production rule Y Ñ β, β P pN Y Σq˚, is applicable to Y in a rule p, if p takes the form\nX Ñ α Y γ,\nα, γ P pN Y Σq˚.\nThe result of applying Y Ñ β to α Y γ is α β γ.\nStarting with S, we apply S Ñ α to S for some pS Ñ αq P P, then take a non-terminal in α and\napply a new production rule.19 To generate a string we follow this procedure until all non-terminal\nsymbols have been transformed into terminal symbols. The resulting string, i.e., the yield, will be\nthe string taken by concatenating all terminal symbols read from left to right. More formally, a\nderivation can be defined as follows.\n18We say that X is on the right-hand side of a rule p if p takes the form p “ pY Ñ α Xγq, where α, γ P pN Y Σq˚.\nWe will sometimes refer to X as the head of the production rule X Ñ α, and the right-hand side α as the body of the\nproduction rule.\n19We will write X P α, which formally means a substring of α with length 1. Unless otherwise stated, X can be\neither a non-terminal or a terminal.\n110\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nDefinition 4.2.3: Derivation\nA derivation in a grammar G is a sequence α1, . . . , αM, where α1 P N, α2, . . . , αM´1 P\npN Y Σq˚ and αM P Σ˚, in which each αm`1 is formed by applying a production rule in P to\nαm.\nWe say that α P pN Y Σq˚ is derived from X P N if we can apply a finite sequence of production\nrules to generate α starting from X. We will denote this as X\n˚ñGα. See the following formal\ndefinition.\nDefinition 4.2.4: Derives\nLet G\ndef\n“ pΣ, N, S, Pq be a CFG. We say that X derives β under the grammar G, denoted\nas XñGβ if Dp P P such that p “ pX Ñ α β γq, α, γ P pN Y Σq˚ and β P pN Y Σq˚ztεu. The\nspecial case XñGε holds iff X Ñ ε P P. We denote the reflexive transitive closure of the ñG\nrelation as\n˚ñG. We say that β is derived from X if X\n˚ñGβ.\nThe (context-free) language of a CFG G is defined as all the strings y P Σ˚ that can be derived\nfrom the start symbol S of G, or alternatively, the set of all yields possible from derivations in G\nthat start with S. We will denote the language generated by G as LpGq.\nDefinition 4.2.5: Language of a Grammar\nThe language of a context-free grammar G is\nLpGq “ ty P Σ˚ | S\n˚ñGyu\n(4.84)\nParse Trees and Derivation Sets\nA natural representation of a derivation in a context-free grammar is a derivation tree d (also\nknown as a parse tree). A derivation tree represents the sequence of applied rules in a derivation\nwith a directed tree. The tree’s internal nodes correspond to the non-terminals in the derivation,\nand each of their children corresponds to a symbol (from Σ Y N) on the right side of the applied\nproduction in the derivation. The leaves, representing terminal symbols, “spell out” the derived\nstring—the tree’s yield. More formally, for each production rule X Ñ α, the node corresponding to\nthe specific instance of the non-terminal X in the derivation is connected to the nodes corresponding\nto Y P α where Y P Σ Y N.\nWe will mostly be interested in representing derivations starting with S—the root node of a tree\nrepresenting any such derivation will correspond to S. We will denote the string generated by a tree\nd—its yield—by s pdq. See Fig. 4.6 for examples of parse trees for the grammar from Example 4.2.2.\nImportantly, a grammar may in fact admit multiple derivations and hence multiple derivation\ntrees for any given string.\n4.2. PUSHDOWN LANGUAGE MODELS\n111\nX\nε\nX\nb\nX\nε\na\nX\nb\nX\nb\nX\nε\na\na\nX\nb\nX\nb\nX\nb\nX\nε\na\na\na\nFigure 4.6: A sequence of derivation trees for the strings in tanbn | n “ 0, 1, 2, 3u in the grammar\nfrom Example 4.2.2.\nX\nε\nX\nY\nε\nFigure 4.7: Two parse trees in the modified grammar G yielding ε.\nExample 4.2.3: Multiple derivation strings\nIt is relatively easy to see that in the grammar G, each string anbn is only generated by a single\nderivation tree—each new pair of symbols a and b can only be added by applying the rule\nX Ñ aXb and the string anbn can only be generated by the application of the rule X Ñ aXb n\ntimes and the rule X Ñ ε once in this order.\nHowever, we can modify G by adding, for instance, a non-terminal Y and rules X Ñ Y, Y Ñ ε.\nThe empty string ε may then be derived either by pX Ñ εq, or pX Ñ Yq, pY Ñ εq, corresponding\nto two separate derivation trees, as shown in Fig. 4.7. The set of these two trees comprises\nwhat we call the derivation set of ε.\nWe denote a derivation set of a string y, generated by the grammar G, as DGpyq.\nDefinition 4.2.6: String derivation set\nLet y P Σ˚. Its derivation set, denoted by DGpyq is defined as\nDGpyq\ndef\n“ td | s pdq “ yu.\n(4.85)\nWe say that a grammar is unambiguous if, for every string that can be generated by the\ngrammar, there is only one associated derivation tree.\n112\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nDefinition 4.2.7: Unambiguity\nA grammar G is unambiguous if for all y P LpGq, |DGpyq| “ 1.\nThe converse holds for ambiguous grammars.\nDefinition 4.2.8: Ambiguity\nA grammar G is ambiguous if Dy P LpGq such that |DGpyq| ą 1.\nThe set of all derivation trees in a grammar is its derivation set.\nDefinition 4.2.9: Grammar derivation set\nThe derivation set of a grammar, DG, is the set of all derivations possible under the\ngrammar. More formally, it can be defined as the union over the derivation set for the strings\nin its language,\nDG\ndef\n“\nď\ny1PLpGq\nDGpy1q\n(4.86)\nDefinition 4.2.10: Non-terminal derivation set\nThe derivation set of a non-terminal Y P N in G, denoted DGpYq, is defined as the set of\nderivation subtrees with root node Y.\nNote that DG could be defined as DGpSq. For a terminal symbol a P Σ, we trivially define the\nderivation set DGpaq to be empty.20\nIn cases where it is irrelevant to consider the order of the production rules in a derivation tree,\nwe will write pX Ñ αq P d to refer to specific production rules in the tree—viewing trees as multisets\n(or bags) over the production rules they include.\nExample 4.2.4: Nominal Phrases\nCFGs are often used to model natural languages. Terminals would then correspond to words in\nthe natural language, strings would be text sequences and non-terminals would be abstractions\nover words. As an example, consider a grammar G that can generate a couple of nominal\nphrases. We let N “ tAdj, Det, N, Nominal, NPu, Σ “ ta, big, female, giraffe, male, tall, theu,\nS “ Nominal and define the following production rules:\nNominal Ñ Det NP\nNP Ñ N | Adj NP\nDet Ñ a | the\nN Ñ female | giraffe | male\nAdj Ñ big | female | male | tall\n20Empty derivation sets for terminal symbols is defined solely for ease of notation later.\n4.2. PUSHDOWN LANGUAGE MODELS\n113\nNominal\nNP\nN\ngiraffe\nDet\na\nNominal\nNP\nNP\nN\nmale\nAdj\nbig\nDet\nthe\nNominal\nNP\nNP\nNP\nN\ngiraffe\nAdj\nfemale\nAdj\ntall\nDet\na\nFigure 4.8: Derivation trees for natural language nominal phrases.\nSee Fig. 4.8 for a few examples of derivation trees in this grammar.\nExample 4.2.5: The generalized Dyck languages Dpkq\nA very widely studied family of context-free languages are the Dyck-k languages, Dpkq, the\nlanguages of well-nested brackets of k types. They are, in some ways, archetypal context-free\nlanguages (Chomsky and Sch¨utzenberger, 1963). Formally, we can define them as follows.\nDefinition 4.2.11: Dpkq languages\nLet k P N. The Dpkq language is the language of the following context-free grammar\nG\ndef\n“ pΣ, N, S, Pq\n• Σ\ndef\n“ txn | n “ 1, . . . , ku Y tyn | n “ 1, . . . , ku\n• N\ndef\n“ tSu\n• S\ndef\n“ S\n• P\ndef\n“ tS Ñ ε, S Ñ SSu Y tS Ñ xnSyn | n “ 1, . . . , ku\nExamples of strings in the language Dp3q would be x3y3x2y2x1y1, x3y3x1x2x2y2y2y1, and\nx1x2x2y2y2x3x1y1y3y1. The string x2x2y1y2 is not in the language Dp3q.\nTo give you a taste of what formally working with context-free grammars might look like,\nwe now formally show that the grammar from Example 4.2.2 really generates the language L “\ntanbn | n P Ně0u, as we claimed.\n114\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nExample 4.2.6: Recognizing anbn\nThe language L “ tanbn | n P Nu is not regular.a However, we can show that it is context-free\nand recognized exactly by the simple grammar from Example 4.2.2. We restate it here for\nconvenience: G “ pΣ, N, S, Pq with N “ tXu, Σ “ ta, bu, S “ X, P “ tX Ñ aXb, X Ñ εu.\nLemma 4.2.1\nGiven the grammar G defined above, we have L pGq “ tanbn | n P Nu.\nProof. We will show that L “ LpGq in two steps: (i) showing that L Ď LpGq and (ii) showing\nthat LpGq Ď L. Define yn “ anbn.\n(i) We first need to show that each y P L can be generated by G, which we will do by induction.\nBase case (n “ 0)\nWe have that y0 “ ε, which is generated by d “ pX Ñ εq.\nInductive step (n ą 1)\nWe have that yn is generated by\nd “ pX Ñ aXbq ¨ ¨ ¨ pX Ñ aXbq\nloooooooooooooooomoooooooooooooooon\nn times\npX Ñ εq.\nIt is then easy to see that yn`1 is generated by the derivation we get by replacing the last\nrule pX Ñ εq with pX Ñ aXbqpX Ñ εq—they are exactly the trees illustrated in Fig. 4.6.\n(ii) Next, we show that for each d P DG, we have that ypdq P L.\nBase case (d “ pX Ñ εq)\nIt is trivial to see that the derivation d “ pX Ñ εq yields ypdq “ ε.\nInductive step\nNow observe that P only contains two production rules and one non-terminal.\nStarting with X, we can either apply X Ñ aXb to get one new non-terminal X, or apply X Ñ ε\nto terminate the process. Hence, if we fix the length of the sequence of production rules, there\nis no ambiguity in which string will be generated. Thus, by induction, we conclude that if\nwe have a derivation tree given by pX Ñ aXbq, . . . , pX Ñ aXbq\nlooooooooooooooooomooooooooooooooooon\nn times\n, pX Ñ εq generating anbn, the\nderivation tree given by pX Ñ aXbq, . . . , pX Ñ aXbq\nlooooooooooooooooomooooooooooooooooon\nn`1 times\n, pX Ñ εq will generate an`1bn`1.\n■\naAgain, while the intuition behind this is similar to our reasoning from Example 4.2.1, this would have to\nbe proven using the so-called pumping lemma for regular languages.\nReachable Non-terminals and Pruning\nSimilarly to how some states in a WFSA can be useless in the sense that they are not accessible\nfrom an initial state or might not lead to a final state, so too can non-terminals in a CFG be useless\nby not beaing reachable from the start symbol or might not lead to any string of terminals. In the\ncontext of CFGs, we typically use a different terminology: “reachable” instead of “accessible” and\n“generating” instead of “co-accessible”.\n4.2. PUSHDOWN LANGUAGE MODELS\n115\nDefinition 4.2.12: Accessibility for CFGs\nA symbol X P N Y Σ is reachable (or accessible) if Dα, α1 P pN Y Σq˚ such that S\n˚ñαXα1.\nDefinition 4.2.13: Co-accessibility for CFGs\nA non-terminal Y is generating (or co-accessible) if Dy P Σ˚ such that Y\n˚ñy.\nIn words, reachable symbols are those that can be derived from the start symbol, whereas\ngenerating non-terminals are those from which at least one string (including the empty string) can\nbe derived. Note that we define reachable for both non-terminals and terminals while generating is\nonly defined for non-terminals.\nThis allows us to define a pruned context-free grammar, which is the CFG version of a trimmed\nWFSA.\nDefinition 4.2.14: Pruned CFG\nA CFG is pruned (or trimmed) if it has no useless non-terminals, i.e. all non-terminals\nare both reachable and generating. Pruning (or trimming) refers to the removal of useless\nnon-terminals.\n4.2.3\nWeighted Context-free Grammars\nAs we did with finite-state automata, we will augment the classic, unweighted context-free grammars\nwith real-valued weights. We do that by associating with each rule X Ñ α a weight WpX Ñ αq P R.\nDefinition 4.2.15: Weighted Context-free Grammar\nA real-weighted context-free grammar is a 5-tuple pΣ, N, S, P, Wq where Σ is an alphabet\nof terminal symbols, N is a non-empty set of non-terminal symbols with N X Σ “ H, S P N is\nthe designated start non-terminal symbol, P is the set of production rules, and W a function\nW : P Ñ R, assigning each production rule a real-valued weight.\nFor notational brevity, we will denote rules p P P as p “ X w\nÝÑ α for X P N, α P pN Y Σq˚ and\nw “ WpX Ñ αq P R.\nExample 4.2.7: A simple weighted context-free grammar\nConsider the grammar G “ pΣ, N, S, P, Wq defined as follows:\n• Σ “ ta, bu\n• N “ tXu\n• S “ X\n• P “ tX Ñ aXb, X Ñ εu\n116\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\n• W “\n␣\nX Ñ aXb ÞÑ 1\n2, X Ñ ε ÞÑ 1\n2\n(\nThis defines a simple weighting of the CFG from Example 4.2.2.\nWeights assigned to productions by WFCGs can be arbitrary real numbers. Analogous to\nprobabilistic WFSAs (Definition 4.1.17) describing locally normalized finite-state language models,\nwe also define probabilistic WCFGs, where the weights of applicable production rules to any non-\nterminal form a probability distribution.\nDefinition 4.2.16: Probabilistic Context-free grammar\nA weighted context-free grammar G “ pΣ, N, S, P, Wq is probabilistic if the weights of the\nproductions of every non-terminal are non-negative and sum to 1, i.e., for all X P N, it holds\nthat\n@ X Ñ α P P, W pX Ñ αq ě 0\n(4.87)\nand\nÿ\nXÑαPP\nW pX Ñ αq “ 1\n(4.88)\nIntuitively, this means that all the production weights are non-negative and that, for any left\nside of a production rule X, the weights over all production rules X Ñ α sum to 1. The grammar\nfrom Example 4.2.7 is, therefore, also probabilistic.\nAgain analogously to the WFSA case, we say that a string y is in the language of WCFG G if\nthere exists a derivation tree d in G containing only non-zero weights with yield s pdq “ y.\nTree Weights, String Weights, and Allsums\nIn the case of regular languages, we discussed how individual strings are “produced” by paths\nin the automaton (in the sense that each path yields a string). As Example 4.2.4 showed, the\nstructures that “produce” or yield strings in a context-free grammar are trees—those, therefore, play\nan analogous role in context-free grammars to paths in finite-state automata.\nJust like we asked ourselves how to combine individual transition weights in a WFSA into weights\nof entire paths and later how to combine those into weights of strings, we now consider the questions\nof how to combine the weights of individual production rules into the weight of entire trees and\nlater also individual strings. We start by giving a definition of the weight of a tree as the product\nover the weights of all the rules in the tree, i.e., as a multiplicatively decomposable function over the\nweights of its rules. As you can probably foresee, we will then define the weight of a string as the\nsum over all the trees that yield that string.\nDefinition 4.2.17: Weight of a derivation tree\nThe weight of a derivation tree d P DG defined by a WCFG G is\nwpdq “\nź\npXÑαqPd\nWpX Ñ αq.\n(4.89)\n4.2. PUSHDOWN LANGUAGE MODELS\n117\nThe stringsum or the string acceptance weight of a particular string under a grammar is then\ndefined as follows:\nDefinition 4.2.18: Stringsum in a context-free grammar\nThe stringsum G pyq of a string y generated by a WCFG G is defined by\nG pyq “\nÿ\ndPDGpyq\nwpdq\n(4.90)\n“\nÿ\ndPDGpyq\nź\npXÑαqPd\nWpX Ñ αq\n(4.91)\nLastly, analogously to the allsum in WFSAs, an allsum is the sum of the weights of all the trees\nin a WCFG. We first define the allsum for symbols (non-terminals and terminals).\nDefinition 4.2.19: Nonterminal allsum in a context-free grammar\nThe allsum for a non-terminal Y in a grammar G is defined by\nZ pG, Yq “\nÿ\ndPDGpYq\nwpdq\n(4.92)\n“\nÿ\ndPDGpYq\nź\npXÑαqPd\nWpX Ñ αq\n(4.93)\nThe allsum for a terminal a P Σ Y tεu is defined to be\nZ paq\ndef\n“ 1.\n(4.94)\nThe allsum for a grammar is then simply the allsum for its start symbol.\nDefinition 4.2.20: Allsum in a context-free grammar\nThe allsum of a weighted context-free grammar G “ pΣ, N, S, P, Wq is\nZ pGq “ Z pG, Sq\n(4.95)\n“\nÿ\ndPDGpSq\nwpdq\n(4.96)\n“\nÿ\ndPDGpSq\nź\npXÑαqPd\nWpX Ñ αq\n(4.97)\nWhen the grammar G we refer to is clear from context, we will drop the subscript and write e.g.\nZpSq.\nAlthough we can in some cases compute the allsum of a WCFG in closed form, as we will see in\nthe example below, we generally require some efficient algorithm to be able to do so.\n118\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nExample 4.2.8: Geometric Series as an Allsum\nConsider the WCFG G “ pΣ, N, S, P, Wq, given by N “ tXu, Σ “ tau, S “ X, and the rules:\nX\n1{3\nÝÝÑ a X\nX 1ÝÑ ε\nThe language generated by G is LpGq “ tan | n ě 0u. Further note that this grammar is\nunambiguous – each string y “ am, for some m ě 0, is associated with the derivation tree\ngiven by pX\n1{3\nÝÝÑ a Xq, . . . , pX\n1{3\nÝÝÑ a Xq\nloooooooooooooooooomoooooooooooooooooon\nm times\n, pX 1ÝÑ εq. Due to the multiplicative decomposition over\nthe weights of the rules, the weight associated with each derivation tree d will hence be\nwpdq “\nˆ1\n3\n˙m\nˆ 1 “\nˆ1\n3\n˙m\nAccordingly, we can compute the allsum of G using the closed-form expression for geometric\nseries:\nZ pGq “\n8\nÿ\nm“0\nˆ1\n3\n˙m\n“\n1\n1 ´ 1{3 “ 3\n2\nJust like we defined normalizable WFSAs, we also define normalizable WCFSs in terms of their\nallsum.\nDefinition 4.2.21: Normalizable Weighted Context-free Grammar\nA weighted context-free grammar G is normalizable if Z pGq is finite, i.e., Z pGq ă 8.\n4.2.4\nContext-free Language Models\nThis brings us to the definition of context-free language models.\nDefinition 4.2.22: Context-free language model\nA language model pLM is context-free if its weighted language equals the language of some\nweighted context-free grammar, i.e., if there exists a weighted context-free grammar G such\nthat L pGq “ L ppLMq.\nGoing the other way—defining string probabilities given a weighted context-free grammar—there\nare again two established ways of defining the probability of a string in its language.\nString Probabilities in a Probabilistic Context-free Grammar\nIn a probabilistic CFG (cf. Definition 4.2.16), any production from a non-terminal X P N is\nassociated with a probability. As the probabilities of continuing a derivation (and, therefore, a\nderivation tree) depend solely on the individual terminals (this is the core of context-free grammars!),\n4.2. PUSHDOWN LANGUAGE MODELS\n119\nit is intuitive to see those probabilities as conditional probabilities of the new symbols given the\noutput generated so far. One can, therefore, define the probability of a path as the product of these\nindividual “conditional” probabilities.\nDefinition 4.2.23: Tree probability in a PCFG\nWe call the weight of a tree d P DG in a probabilistic CFG the probability of the tree d.\nThis alone is not enough to define the probability of any particular string y P Σ˚ since there\nmight be multiple derivations of y. Naturally, we define the probability of y as the sum of the\nindividual trees that generate it:\nDefinition 4.2.24: String probability in a PCFG\nWe call the stringsum of a string y P Σ˚ in a probabilistic CFG G the probability of the\nstring y:\npG pyq\ndef\n“ G pyq .\n(4.98)\nThese definitions and their affordances mirror the ones in probabilistic finite-state automata (cf.\n§4.1.2): they again do not require any normalization and are therefore attractive as the summation\nover all possible strings is avoided. Again, the question of tightness of such models comes up: we\nexplore it question in §4.2.5.\nString Probabilities in a General Weighted Context-free Grammar\nTo define string probabilities in a general weighted CFG, we use the introduced notions of the\nstringsum and the allsum—we normalize the stringsum to define the globally normalized probability\nof a string y as the proportion of the total weight assigned to all strings that is assigned to y.\nDefinition 4.2.25: String probability in a WCFG\nLet G “ pΣ, N, S, P, Wq be a normalizable WCFG with non-negative weights. We define the\nprobability of a string y P Σ˚ under G as\npG pyq\ndef\n“ G pyq\nZ pGq.\n(4.99)\nLanguage Models Induced by a Weighted Context-free Grammar\nWith the notions of string probabilities in both probabilistic and general weighted CFGs, we can\nnow define the language model induced by G as follows.\n120\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nDefinition 4.2.26: A language model induced by a WCFG\nLet G “ pΣ, N, S, P, Wq be a WCFG. We define the language model induced by G as the\nfollowing probability distribution over Σ˚\npLMG pyq\ndef\n“ pG pyq .\n(4.100)\nAgain, it is easy to see that while global normalization requires the computation of the allsum,\nlanguage models induced by weighted FSAs through Eq. (4.99) are globally normalized and thus\nalways tight. The tightness of probabilistic WCFGs is discussed next, after which we investigate the\nrelationship between globally- and locally-normalized context-free grammars.\n4.2.5\nTightness of Context-free Language Models\nAgain, an advantage of globally normalized context-free language models (grammars) is that they\nare always tight, as the derivation trees are explicitly normalized with the global normalization\nconstant such that they sum to 1 over the set of possible sentences.\nIn this section, we, therefore, consider the tightness of probabilistic context-free grammars. We\nfollow the exposition from Booth and Thompson (1973). The proof requires the use of multiple new\nconcepts, which we first introduce below.\nDefinition 4.2.27: Generation level\nWe define the level of a generation sequence inductively as follows. The zeroth level γ0 of\na generation sequence is defined as S. Then, for any n ą 0, γn corresponds to the string is\nobtained by applying the applicable productions onto all nonterminals of γn´1.\nExample 4.2.9: Generation levels\nLet G “ pΣ, N, S, Pq with Σ “ ta, bu, N “ tS, X, Yu,\nand P “ tS Ñ a X Y, X Ñ Y X, X Ñ bY Y, Y Ñ a a Y, Y Ñ au. Then the generation sequence\nof the string aabaaaaa would be\nγ0 “ S\n(definition)\nγ1 “ aXY\n(applying S Ñ a X Y)\nγ2 “ aYXaaY\n(applying X Ñ Y X, Y Ñ a a Y)\nγ3 “ aabYYaaaaY\n(applying Y Ñ a, X Ñ b Y Y, X Ñ a a Y)\nγ3 “ aabaaaaaaa\n(applying Y Ñ a, Y Ñ a, Y Ñ a)\nWe will also rely heavily on generating functions. A generating function is simply a way of\nrepresenting an infinite sequence by encoding its elements as the coefficients of a formal power series.\nUnlike ordinary series such as the geometric power series from Example 4.2.8, a formal power series\ndoes not need to converge: in fact, at its core a generating function is not actually regarded as a\nfunction—its “variables” are indeterminate and they simply serve as “hooks” for the numbers in the\nsequence.\n4.2. PUSHDOWN LANGUAGE MODELS\n121\nDefinition 4.2.28: Production generating function\nLet G\ndef\n“ pΣ, N, S, P, Wq be a PCFG and N\ndef\n“ |N|. For each Xn P N, define its production\ngenerating function as\ng ps1, . . . , sNq\ndef\n“\nÿ\nXnÑα\nW pXn Ñ αq sr1pαq\n1\nsr2pαq\n2\n¨ ¨ ¨ ¨ ¨ srNpαq\nN\n,\n(4.101)\nwhere rm pαq denotes the number of times the nonterminal Xm P N appears in α P pΣ Y Nq˚.\nExample 4.2.10: Tightness of a context-free grammar\nLet\nG\n“\npΣ, N, S, Pq\nwith\nΣ\n“\nta, bu,\nN\n“\ntS, Xu,\nand\nP\n“\ntS Ñ a S X, S Ñ b, X Ñ aX X, X Ñ aau. Then\ng1 ps1, s2q “ W pS Ñ aSXq s1s2 ` W pS Ñ bq\ng2 ps1, s2q “ W pX Ñ aXXq s2\n2 ` W pX Ñ aaq\nDefinition 4.2.29: Generating function\nThe generating function of the lth level is defined as\nG0 ps1, . . . , sNq\ndef\n“ s1\n(4.102)\nG1 ps1, . . . , sNq\ndef\n“ g1 ps1, . . . , sNq\n(4.103)\nGl ps1, . . . , sNq\ndef\n“ Gl´1 pg1 ps1, . . . , sNq , . . . , gN ps1, . . . , sNqq ,\n(4.104)\nthat is, the lth-level generating function is defined as the l ´ 1st-level generating function\napplied to production generating functions as arguments.\nExample 4.2.11: Tightness of a context-free grammar\nFor the grammar from Example 4.2.10, we have\nG0 ps1, s2q “ s1\nG1 ps1, s2q “ g ps1, s2q “ W pS Ñ aSXq s1s2 ` W pS Ñ bq\nG2 ps1, s2q “ W pS Ñ aSXq rg1 ps1, s2qs rg2 ps1, s2qs ` W pS Ñ bq\n“ W pS Ñ aSXq2 W pX Ñ aXXq s1s3\n2\n` W pS Ñ aSXq2 W pX Ñ aaq s1s2\n` W pS Ñ aSXq W pS Ñ bq W pX Ñ aXXq s2\n2\n` W pS Ñ aSXq W pS Ñ bq W pX Ñ aaq\n` W pS Ñ bq\n122\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nWe can see that a generating function Gl ps1, . . . , sNq can be expressed as\nGl ps1, . . . , sNq “ Dl ps1, . . . , sNq ` Cl\n(4.105)\nwhere the polynomial Dl ps1, . . . , sNq does not contain any constant terms. It is easy to see that the\nconstant Cl then corresponds to the probability of all strings that can be derived in l levels or fewer.\nThis brings us to the following simple lemma.\nLemma 4.2.2\nA PCFG is tight if and only if\nlim\nlÑ8 Cl “ 1.\n(4.106)\nProof. Suppose that limlÑ8 Cl ă 1. This means that the generation process can enter a generation\nsequence that has a non-zero probability of not terminating—this corresponds exactly to it not\nbeing tight.\nOn the other hand, limlÑ8 Cl “ 1 implies that no such sequence exists, since the limit represents\nthe probability of all strings that can be generated by derivations of a finite number of production\nrules.\n■\nThe rest of the section considers necessary and sufficient conditions for Eq. (4.106) to hold. For\nthis, we first define the first-moment matrix of a PCFG.\nDefinition 4.2.30: First-moment matrix\nLet G\ndef\n“ pΣ, N, S, P, Wq be a PCFG. We define its first-moment matrix (its mean matrix)\nE P RNˆN as\nEnm\ndef\n“ Bgn ps1, . . . , sNq\nBsm\nˇˇˇˇˇ\ns1,...,sN“1\n.\n(4.107)\nNote that Enm represents the expected number of occurrences of the non-terminal Xm in the set\nof sequences α with XnñGα, i.e., the set of sequences Xn can be rewritten into:\nEnm “\nÿ\nXnÑα\nW pXn Ñ αq rm pαq .\n(4.108)\nThe informal intuition behind this is the following: each of the terms W pXn Ñ αq sr1pαq\n1\nsr2pαq\n2\n¨ ¨ ¨ ¨ ¨\nsrNpαq\nN\nin gn contains the information about how many times any non-terminal Xm appears in the\nproduction rule Xn Ñ α as well as what the probability of “using” or applying that production\nrule to Xn is. Differentiating W pXn Ñ αq sr1pαq\n1\nsr2pαq\n2\n¨ ¨ ¨ ¨ ¨ srNpαq\nN\nw.r.t. sm then “moves” the\ncoefficient rm corresponding to the number of occurrences of Xm in Xn Ñ α in front of the term\nW pXn Ñ αq sr1pαq\n1\nsr2pαq\n2\n¨ ¨ ¨ ¨ ¨ srNpαq\nN\nin gn, effectively multiplying the probability of the occurrence\nof the rule with the number of terms Xm in the rule—this is exactly the expected number of\noccurrences of Xm for this particular rule, averaging over all possible rules that could be applied.\nSumming over all applicable production rules for Xn (which form a probability distribution) gives us\nthe total expected number of occurrences of Xm. This brings us to the core theorem of this section\ncharacterizing the tightness of PCFGs.\n4.2. PUSHDOWN LANGUAGE MODELS\n123\nTheorem 4.2.1: A sufficient condition for the tightness of probabilistic context-free\ngrammars\nA PCFG is tight if |λmax| ă 1 and is non-tight if |λmax| ą 1, where λmax is the eigenvalue of\nE with the largest absolute value.\nProof. The coefficient of the term sr1\n1 sr2\n2 ¨¨ ¨ ¨¨srN\nN in the generating function Gl ps1, . . . , sNq corresponds\nto the probability that there will be r1 non-terminal symbols X1, . . . , rN non-terminal symbols XN\nin the lth level of the generation sequence. In particular, if the grammar is tight, this means that\nlim\nlÑ8 Gl ps1, . . . , sNq “ lim\nlÑ8 rDl ps1, . . . , sNq ` Cls “ 1.\n(4.109)\nThis, however, is only true if\nlim\nlÑ8 Dl ps1, . . . , sNq “ 0\n(4.110)\nand this, in turn, can only be true if limlÑ8 rn “ 0 for all n “ 1, . . . N. The expected value of rn at\nlevel l is\nrl,n “ BGl ps1, . . . , sNq\nBsn\nˇˇˇˇˇ\ns1,...,sN“1\n.\n(4.111)\nReasoning about this is similar to the intuition behind the first-moment matrix, with the difference\nthat we are now considering the number of occurrences after a sequence of l applications. Denoting\nrl\ndef\n“ rrl,1, . . . , rl,Ns\n(4.112)\nwe have\nrl “\n« N\nÿ\nj“1\nBGl´1 pg1 ps1, . . . , sNq , . . . , gN ps1, . . . , sNqq\nBgj\n(4.113)\n¨ Bgj\nBsn\nps1, . . . , sNq | n “ 1, . . . , N\nffˇˇˇˇˇ\ns1,...,sN“1\n(4.114)\n“ rl´1E.\n(4.115)\nApplying this relationship repeatedly, we get\nrl “ r0El “ r1, 0, . . . , 0s El,\n(4.116)\nmeaning that\nlim\nlÑ8 rl “ 0 iff lim\nlÑ8 El “ 0.\n(4.117)\nThe matrix E satisfies this condition if |λmax| ă 1. On the other hand, if |λmax| ą 1, the limit\ndiverges.\n■\nNote that the theorem does not say anything about the case when |λmax| “ 1.\nWe conclude the subsection by noting that, interestingly, weighted context-free grammars trained\non data with maximum likelihood are always tight (Chi and Geman, 1998; Chi, 1999). This is not\nthe case for some models we consider later, e.g., recurrent neural networks (cf. §5.1.2).\n124\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\n4.2.6\nNormalizing Weighted Context-free Grammars\nHaving investigated probabilistic context-free grammars in terms of their tightness, we now turn out\nattention to general weighted context-free grammars, which define string probabilities using global\nnormalization (cf. Eq. (4.99)). To be able to compute these probabilities, require a way to compute\nthe normalizing constant Z pGq and the stringsum G pyq. In the section on finite-state automata, we\nexplicitly presented an algorithm for computing the normalizing constant Z pAq. The derivation of\na general allsum algorithm for weighted context-free grammars, on the other hand, is more involved\nand beyond the scope of this course.21 Here, we simply assert that there are ways of computing the\nquantities in Eq. (4.99) and only consider the following result:\nTheorem 4.2.2: PCFGs and WCFGs are equally expressive (Smith and Johnson,\n2007)\nNormalizable weighted context-free grammars with non-negative weights and tight probabilistic\ncontext-free grammars are equally expressive.\nProof. To prove the theorem, we have to show that any WCFG can be written as a PCFG and vice\nversa.22\nð Since any tight probabilistic context-free grammar is simply a WCFG with Z pGq “ 1, this\nholds trivially.\nñ We now show that, for any WCFG, there exists a PCFG encoding the same language model.\nLet GG “ pΣ, N, S, P, Wq be a pruned WCFG that encodes a distribution over Σ˚ using Eq. (4.99).\nWe now construct a tight probabilistic context-free grammar GL “ pΣ, N, S, P, WLq whose language\nis identical. Notice that all components of the grammar remain identical apart from the weighting\nfunction. This means that the derivations of the strings in the grammars remain the same (i.e.,\nDGG “ DGL)—only the weights of the derivations change, as we detail next. We define the production\nweights of the probabilistic CFG as follows.\nWGL pX Ñ αq\ndef\n“ W pX Ñ αq ś\nYPα ZpG, Yq\nZpG, Xq\n(4.118)\nRemember that Z paq “ 1 for a P Σ. Note that the assumption that G is pruned means that all the\nquantities in the denominators are non-zero.\nIt is easy to see that the weight defined this way are non-negative due to the non-negativity of\nG’s weights. Furthermore, the weights of all production rules for any non-terminal X P N sum to 1,\n21The allsums of individual non-terminals can be expressed as solutions to a nonlinear set of equations. Again, the\ninterested reader should have a look at the Advanced Formal Language Theory course.\n22Again, by “written as”, we mean that the weighted language is the same.\n4.2. PUSHDOWN LANGUAGE MODELS\n125\nas by the definitions of WGL and ZpG, Xq we have\nÿ\nXÑα\nWGL pX Ñ αq “\nÿ\nXÑα\nW pX Ñ αq ś\nYPα ZpG, Yq\nZpG, Xq\n(4.119)\n“\n1\nZpG, Xq\nÿ\nXÑα\nW pX Ñ αq\nź\nYPα\nZpG, Yq\n(4.120)\n“\n1\nZpG, XqZpG, Xq\n(4.121)\n“ 1\n(4.122)\nWe now have to show that the probabilities assigned by these two grammars match. We will\ndo that by showing that the probabilities assigned to individual derivations match, implying that\nstringsums match as well. The probability of a derivation is defined analogously to a probability\nof a string, i.e., pG pdq “\nwpdq\nZpGq (where Z pGq “ 1 for tight probabilistic grammars). Let then\nd P DG “ DGL. Then\npGL pdq “\nź\nXÑαPd\nWGL pX Ñ αq\n(4.123)\n“\nź\nXÑαPd\nW pX Ñ αq ś\nYPα ZpG, Yq\nZpG, Xq\n(definition of WGL).\n(4.124)\nNotice that by multiplying over the internal nodes of the derivation tree, Eq. (4.123) includes the\nnon-terminal allsum of each internal (non-root and non-leaf) non-terminal in the derivation twice:\nonce as a parent of a production in the denominator, and once as a child in the numerator. These\nterms, therefore, all cancel out in the product. The only terms which are left are the allsums of the\nleaf nodes—the terminals—which are 1, and the allsum of the root node—S—which equals Z pGGq\nand the weights of the individual productions, which multiply into the weight assigned to d by the\noriginal grammar GG. This means that\npGL pdq “\n1\nZpG, Xq\nź\nXÑαPd\nW pX Ñ αq “\n1\nZpG, Xqw pdq “ pGG pdq ,\n(4.125)\nfinishing the proof.\n■\nThis means that the classes of probabilistic and weighted context-free grammars are in fact equally\nexpressive. In other words, this result is analogous to Theorem 4.1.1 in WFSAs: it shows that in\nthe context of context-free language models, the locally normalized version of a globally-normalized\nmodel is also context-free.\n4.2.7\nPushdown Automata\nWe presented context-free grammars as a formalism for specifying and representing context-free\nlanguages. Many algorithms for processing context-free languages, for example, the allsum algorithms\nand their generalizations, can also be directly applied to context-free grammars. However, it is also\nconvenient to talk about processing context-free languages in terms of computational models in the\n126\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nform of automata, i.e., the recognizer of the language.23 As we mentioned, the types of automata\nwe considered so far, (weighted) finite-state automata, can only recognize regular languages. To\nrecognize context-free languages, we must therefore extend finite-state automata.24 We do that by\nintroducing pushdown automata (PDA), a more general and more expressive type of automata.\nSingle-stack Pushdown Automata\nPushdown automata augment finite-state automata by implementing an additional stack memory\nstructure for storing arbitrarily long strings from a designated alphabet, which allows them to work\nwith unbounded memory effectively. Abstractly, this unbounded memory is the only difference to\nfinite-state automata. However, the definition looks slightly different:\nDefinition 4.2.31: Pushdown automaton\nA pushdown automaton (PDA) is a tuple P\ndef\n“ pΣ, Q, Γ, δ, pqι, γιq, pqφ, γφqq, where:\n• Q is a finite set of states;\n• Σ is a finite set of input symbols called the input alphabet;\n• Γ is a finite set of stack symbols called the stack alphabet;\n• δ Ď Q ˆ Γ˚ ˆ pΣ Y tεuq ˆ Q ˆ Γ˚ is a multiset representing the transition function;\n• pqι, γιq is called the initial configuration and pqφ, γφq is called the final configuration,\nwhere qι, qφ P Q and γι, γφ P Γ˚ .\nThe initial and final configurations in pushdown play analogous roles to the sets of initial and\nfinal sets of finite-state automata. Compared to the latter, they also allow for different starting\nconfigurations of the stack coupled with each possible initial or final state.\nStacks are represented as strings over Γ, from bottom to top. Thus, in the stack γ “ X1X2 ¨ ¨ ¨ Xn,\nthe symbol X1 is at the bottom of the stack, while Xn is at the top. γ “ H denoses the empty\nstack.\nDefinition 4.2.32: Configuration of a pushdown automaton\nA configuration of a PDA is a pair pq, γq, where q P Q is the current state and γ P Γ˚ is the\ncurrent contents of the stack.\nThe initial and final configurations of a PDA are examples of configurations; it is possible to\ngeneralize the initial and final stacks to (say) regular expressions over Γ, but the above definition\nsuffices for our purposes.\n23This relationship between a formalism specifying how to generate (i.e., a grammar) and a model of recognizing a\nlanguage can be seen in multiple levels of the hierarchy of formal languages. In the case of context-free languages, the\nformer are context-free grammars, while the latter are pushdown automata discussed in this subsection. Regular\nlanguages as introduced in the previous section, however, are simply defined in terms of their recognizers—finite-state\nautomata.\n24Formally, we would of course have to prove that finite-state automata cannot model context-free languages. This\ncan be done with the so-called pumping lemma, which are outside the scope of this class.\n4.2. PUSHDOWN LANGUAGE MODELS\n127\nA PDA moves from configuration to configuration by following transitions of the form q\na,γ1Ñγ2\nÝÝÝÝÝÝÑ r,\nwhich represents a move from the state q to state r, while popping the sequence of symbols γ1 P Γ˚\nfrom the top of the stack and pushing the sequence γ2 P Γ˚. The PDA transition function therefore\nnot only depends on the current state q and input symbol a, but also on some finite sequence\nof symbols on the top of the stack. The stack hence determines the behavior of the automaton,\nand since the set of possible configurations of the stack is infinite, the set of configurations of the\nautomaton is infinite, in contrast to finite-state automata.\nTo describe how pushdown automata process strings, we introduce the concepts of scanning and\nruns.\nDefinition 4.2.33: Scanning\nWe say that τ “ pp, γ1, a, q, γ2q P δ scans a, and if a ‰ ε, we call τ scanning; otherwise, we\ncall it non-scanning.\nDefinition 4.2.34: Pushdown automaton transitions\nIf pq1, γγ1q and pq2, γγ2q are configurations, and τ is a transition q1\na,γ1Ñγ2\nÝÝÝÝÝÝÑ q2, we write\npq1, γγ1q ñτ pq2, γγ2q.\nSince the behavior of a pushdown automaton does not only depend on the states encountered by\nit but also on the content of the stack, we generalize the notion of a path to include the configuration\nof the automaton. This is called a run.\nDefinition 4.2.35: Run of a pushdown automaton\nA run of a PDA P is a sequence of configurations and transitions\nπ “ pq0, γ0q, τ1, pq1, γ1q, . . . , τn, pqN, γNq\nwhere, for n “ 1, . . . , N, we have pqn´1, γn´1q ñτn pqn, γnq.a A run is called accepting if\npq0, γ0q is the initial configuration and pqN, γNq is the final configuration. If, for n “ 1, . . . , N,\nτn scans an, then we say that π scans the string a1 ¨ ¨ ¨ aN. We write Π pP, yq for the set of\nruns that scan y and Π pPq for the set of all accepting runs of P.\naSometimes it will be convenient to treat π as a sequence of only configurations or only transitions.\nDefinition 4.2.36: Recognition of a string by a pushdown automaton\nWe say that the PDA P recognizes the string y if Π pP, yq ‰ H, i.e., if there exists an\naccepting run with the yield y. The set of all strings recognized by P is the language\nrecognized by P, which we denote by L pPq, i.e.,\nL pPq\ndef\n“ ty | Π pP, yq ‰ Hu .\n(4.126)\n128\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nExample 4.2.12: Example of a pushdown automaton\nFig. 4.9 shows an example of a pushdown automaton P accepting the language L pPq “\ntanbn | n P Nu.\n´\n1\na,εÑX\nÝÝÝÝÑ 1, 1\nε,εÑε\nÝÝÝÝÑ 2\n¯\nis a run of P;\n´\n1\na,εÑX\nÝÝÝÝÑ 1, 1\nε,εÑε\nÝÝÝÝÑ 2, 2\nb,XÑε\nÝÝÝÝÑ 2\n¯\nis\nan accepting run of P.\n1\n2\nε, ε Ñ ε\na, ε Ñ X\nb, X Ñ ε\nFigure 4.9: The PDA that accepts the language tanbn | n P Nu.\nLastly, we define deterministic pushdown automata, analogously to their finite-state version\n(Definition 4.1.3). Recall that in the case of finite-state automata, a deterministic machine has at\nmost one possible next move for each state. Similalry, a deterministic pushdown automaton has at\nmost one possible next move for each configuration.\nDefinition 4.2.37: Deterministic pushdown automaton\nA PDA P “ pΣ, Q, Γ, δ, pqι, γιq, pqφ, γφqq is deterministic if\n• there are no transitions of the type pq, ε, γ, p, γq;\n• for every pq, a, γq P Q ˆ Σ Y tεu ˆ Γ˚, there is at most one transition pq, a, γ, p, γ1q P δ;\n• if there is a transition pq, a, γ, p, γ1q P δ for some a P Σ, then there is no transition\npq, ε, γ, p, γ2q P δ.\nOtherwise, P is non-deterministic.\nImportantly, not all context-free languages can be recognized by deterministic pushdown au-\ntomata. That is, in contrast to finite-state automata, where deterministic machines are just as\npowerful as non-deterministic ones (at least in the unweighted case—interestingly, some weighted\nnon-deterministic FSAs cannot be determinized), non-deterministic pushdown automata are more\nexpressive than deterministic ones. Specifically, as stated in Theorem 4.2.3, non-deterministic push-\ndown automata recognize exactly context-free languages, while deterministic pushdown automata\nonly recognize a subset of them (Sipser, 2013).\nWeighted Pushdown Automata\nAnalogously to the finite-state case, and the case of context-free grammars, we now also extend the\ndefinition of a pushdown automaton to the weighted case. The formal definition is:\n4.2. PUSHDOWN LANGUAGE MODELS\n129\nDefinition 4.2.38: Weighted pushdown automaton\nA\nreal-weighted\npushdown\nautomaton\n(WPDA)\nis\na\ntuple\nP\n“\npQ, Σ, Γ, δ, pqι, γιq, pqφ, γφqq, where:\n• Q is a finite set of states;\n• Σ is a finite set of input symbols called the input alphabet;\n• Γ is a finite set of stack symbols called the stack alphabet;\n• δ Ď Q ˆ Γ˚ ˆ pΣ Y tεuq ˆ Q ˆ Γ˚ ˆ R is a multi-set representing the transition weighting\nfunction;\n• pqι, γιq is called the initial configuration and pqφ, γφq is called the final configuration,\nwhere qι, qφ P Q and γι, γφ P Γ˚ .\nAs you can see, the only difference between the weighted and the unweighted case is the transition\nfunction, which in the weighted case weights the individual transitions instead of specifying the set\nof possible target configurations.\nAs with WFSAs (Definition 4.1.17) and WCFGs (Definition 4.2.16), we now define probabilistic\nWPDAs. This definition, however, is a bit more subtle. Notice that the transition weighting\n“function” δ in a WPDA is crucially still a finite—there is only a finite number of actions we can\never do. Similarly, when defining a probabilistic PDA, we have to limit ourselves to a finite number\nof configurations over which we define probability distributions over the next possible actions.\nWe define a probabilistic pushdown automaton given an equivalence relation as follows.\nDefinition 4.2.39: Probabilistic pushdown automaton\nA WPDA P “ pQ, Σ, Γ, δ, pqι, γιq, pqφ, γφqq is probabilistic if it holds that\n@ q\na,γ1Ñγ2{w\nÝÝÝÝÝÝÝÑ r P δ : w ě 0\n(4.127)\nand for any q P Q and γ P Γ˚\nÿ\nq\na,γ1Ñγ2{w\nÝÝÝÝÝÝÝÑr\ns.t. γ1 Ÿ γ\nw “ 1.\n(4.128)\nDefinition 4.2.40: Transitions of a weighted pushdown automaton\nIf pq1, γγ1q and pq2, γγ2q are configurations, and τ is a transition q1\na,γ1Ñγ2{w\nÝÝÝÝÝÝÝÑ q2 with w ‰ 0,\nwe write pq1, γγ1q ñτ pq2, γγ2q.\n130\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nDefinition 4.2.41: Transition weights in a pushdown automaton\nIf δpp, γ1, a, q, γ2q “ w, then we usually write\nτpp, γ1\naÝÑ q, γ2q “ w\n(4.129)\nor that δ has transition pq\na,γ1Ñγ2{w\nÝÝÝÝÝÝÝÑ pq. We sometimes let τ stand for a transition, and we\ndefine δpτq “ w.\nAnd again, just like we combined the weights of individual transitions into the weights of paths\nin WFSAs, and we combined the weights of production rules into the weights of the trees in WCFGs,\nwe now multiplicatively combine the weights of individual transitions in a run to define the weight\nof a run in a WPDA:\nDefinition 4.2.42: Run weight\nThe weight w pπq of a run\nπ “ pq0, γ0q, τ1, pq1, γ1q, . . . , τN, pqN, γNq\nis the multiplication of the transition weights, i.e.,\nw pπq\ndef\n“\nN\nź\nn“1\nδ pτnq\n(4.130)\nAnalogously to a stringsum in WFSAs, we define the stringsum for a string y in a WPDA P as\nthe sum over the weights of all runs scanning y.\nDefinition 4.2.43: Stringsum in a pushdown automaton\nLet P be a WPDA and y P Σ˚ a string. The stringsum for y in P is defined as\nP pyq\ndef\n“\nÿ\nπPΠpP,yq\nw pπq\n(4.131)\nDefinition 4.2.44: Recognition by a weighted pushdown automaton\nWe say that the PDA P recognizes the string y with the weight P pyq.\nWith this, we can define the weighted language defined by a WPDA.\nDefinition 4.2.45: Weighted language of a weighted pushdown automaton\nLet P be a WPDA. The (weighted) language L pPq of P is defined as\nL pPq\ndef\n“ tpy, P pyqq | y P Σ˚u\n(4.132)\n4.2. PUSHDOWN LANGUAGE MODELS\n131\nFinally, we also define the WPDA allsum and normalizable WPDAs.\nDefinition 4.2.46: Allsum of a weighted pushdown automaton\nThe allsum of a WPDA P is defined as\nZ pPq\ndef\n“\nÿ\nπPΠpPq\nw pπq\n(4.133)\nDefinition 4.2.47: Normalizable weighted pushdown automaton\nA WPDA P is normalizable if Z pPq is finite, i.e., if Z pPq ă 8.\nRelationship to Context-free Grammars\nWe motivated the introduction of pushdown automata as a means of recognizing context-free\nlanguages. However, this correspondence is not obvious from the definition! Indeed, the equivalence\nof the expressive power of context-free grammars and pushdown automata is a classic result in\nformal language theory, and it is summarised by the theorem below:\nTheorem 4.2.3: Context-free grammars and pushdown automata are equally ex-\npressive\nA language is context-free if and only if some pushdown automaton recognizes it.\nProof. See Theorem 2.20 in Sipser (2013).\n■\nThis result extends to the probabilistic case.\nTheorem 4.2.4: Probabilistic context-free grammars and probabilistic pushdown\nautomata are equally expressive\nA language is generated by a probabilistic context-free grammar if and only if some probabilistic\npushdown automaton recognizes it.\nProof. See Theorems 3 and 7 in Abney et al. (1999).\n■\nLastly, analogously to how Theorem 4.2.2 showed that weighted context-free grammars are\nequally expressive as probabilistic context-free grammars, the following theorem asserts the same\nabout pushdown automata:\nTheorem 4.2.5: Globally normalized weighted pushdown automata can be locally\nnormalized\nAny globally normalized weighted pushdown automaton can be locally normalized. More\nprecisely, this means the following. Let P be a weighted pushdown automaton. Then, there\n132\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nexists a probabilistic pushdown automaton Pp such that\nPp pyq “ P pyq\nZ pPq\n(4.134)\nfor all y P Σ˚.\nProof. The proof is not straightforward: it can be shown that one cannot simply convert an arbitrary\nweighted pushdown automaton into a locally-normalized one directly. Rather, the construction of\nthe latter goes through their context-free grammars: given a WPDA P, one first constructs the\nWCFG equivalent to P, and then converts that to a locally normalized one (cf. Theorem 4.2.2), i.e.,\na PCFG. Then, this PCFG can be converted to a structurally quite different probabilistic pushdown\nautomaton Pp, which nevertheless results in the language we require (Eq. (4.134)).\nThis construction is described in more detail in Abney et al. (1999) and Butoi et al. (2022).\n■\n4.2.8\nPushdown Language Models\nWe can now define pushdown language models, the title of this section.\nDefinition 4.2.48: Pushdown language model\nA pushdown language model is a language model whose weighted language equals the\nlanguage of some weighted pushdown automaton, i.e., if there exists a weighted pushdown\nautomaton P such that L pPq “ L ppLMq.\nSimilarly, pushdown automata also induce language models.\nDefinition 4.2.49: Language model induced by a pushdown automaton\nLet P be a weighted pushdown automaton. We define the language model induced by P\nas the probability distribution induced by the probability mass function\npLMP pyq\ndef\n“ P pyq\nZ pPq,\n(4.135)\nfor any y P Σ˚.\nYou might wonder why we specifically define pushdown language models and models induced\nby them if WPDAs are equivalent to WCFGs (cf. §4.2.7). In that sense, a language model is\ncontext-free if and only if it is a pushdown language model. However, this holds only for single-stack\npushdown automata which we have discussed so far. We make this explicit distinction of pushdown\nlanguage models with an eye to the next section, in which we introduce multi-stack WPDAs. Those\nare, as it turns out, much more powerful (expressive) than context-free grammars. We will, however,\nreuse this definition of a pushdown language model for those more powerful machines.\n4.2. PUSHDOWN LANGUAGE MODELS\n133\n4.2.9\nMulti-stack Pushdown Automata\nWe now consider an extension of (weighted) pushdown automata, namely, machines that employ\nmultiple stacks. While this might not seem like an important distinction, we will see shortly that\nthis augmentation results in a big difference in the expressiveness of the framework!\nDefinition 4.2.50: Two-stack pushdown automaton\nA\ntwo-stack\npushdown\nautomaton\n(2-PDA)\nis\na\ntuple\nP\n“\npΣ, Q, Γ1, Γ2, δ, pqι, γι1, γι2q,\n`\nqφ, γφ1, γφ2\n˘\nq, where:\n• Σ is a finite set of input symbols called the input alphabet;\n• Q is a finite set of states;\n• Γ1 and Γ2 are finite sets of stack symbols called the stack alphabets;\n• δ Ď Q ˆ Γ˚\n1 ˆ Γ˚\n2 ˆ pΣ Y tεuq ˆ Q ˆ Γ˚\n1 ˆ Γ˚\n2 is a multiset representing the transition\nfunction;\n• pqι, γι1, γι2q is called the initial configuration and\n`\nqφ, γφ1, γφ2\n˘\nis called the final\nconfiguration, where qι, qφ P Q, γι1, γφ1 P Γ˚\n1, and γι2, γφ2 P Γ˚\n2.\nNote that we could more generally define a k-stack PDA by including k stacks in the definition,\nbut the restriction to two stacks will be sufficient for our needs, as we will see in the next subsection.\nThe transition function now depends on the values stored in both of the stacks. The definitions of\nthe configuration and run of a two-stack PDA are analogous to the single-stack variant, with the\naddition of the two stacks. We again extend this definition to the weighted and the probabilistic\ncase.\nDefinition 4.2.51: Two-stack weighted pushdown automaton\nA two-stack real-weighted pushdown automaton (2-WPDA) is a tuple P\n“\npΣ, Q, Γ1, Γ2, δ, pqι, γι1, γι2q,\n`\nqφ, γφ1, γφ2\n˘\nq, where:\n• Σ is a finite set of input symbols called the input alphabet;\n• Q is a finite set of states;\n• Γ1 and Γ2 are finite sets of stack symbols called the stack alphabets;\n• δ Ď Q ˆ Γ˚\n1 ˆ Γ˚\n2 ˆ pΣ Y tεuq ˆ Q ˆ Γ˚\n1 ˆ Γ˚\n2 ˆ R is a multiset representing the transition\nweighting function;\n• pqι, γι1, γι2q is called the initial configuration and\n`\nqφ, γφ1, γφ2\n˘\nis called the final\nconfiguration, where qι, qφ P Q, γι1, γφ1 P Γ˚\n1, and γι2, γφ2 P Γ˚\n2.\nAnd lastly, we define probabilistic two-stack PDAs:\n134\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\nDefinition 4.2.52: Probabilistic two-stack pushdown automaton\nA 2-WPDA P “ pQ, Σ, Γ1, Γ2, δ, pqι, γι1, γι2q,\n`\nqφ, γφ1, γφ2\n˘\nq is probabilistic if for any con-\nfiguration pq, γ1, γ2q it holds that\n@ q\na,γ1Ñγ1\n1,γ2Ñγ1\n2{w\nÝÝÝÝÝÝÝÝÝÝÝÝÑ r P δ : w ě 0\n(4.136)\nand for any q P Q and γ P Γ˚\n1, γ1 P Γ˚\n2\nÿ\nq\na,γ1Ñγ1\n1,γ2Ñγ1\n2{w\nÝÝÝÝÝÝÝÝÝÝÝÝÑr\ns.t. γ1 Ÿ γ and γ2 Ÿ γ1\nw “ 1.\n(4.137)\nTuring Completeness of Multi-stack Pushdown Automata\nBesides modeling more complex languages than finite-state language models from §4.1, (multi-stack)\npushdown automata will also serve an important part in analyzing some modern language models that\nwe introduce later. Namely, we will show that Recurrent neural networks (cf. §5.1.2) can simulate\nany two-stack PDA. This will be useful when reasoning about the computational expressiveness of\nrecurrent neural networks because of a fundamental result in the theory of computation, namely,\nthat two-stack PDAs are Turing complete:\nTheorem 4.2.6: Two-stack pushdown automata are Turing complete\nAny 2-stack pushdown automaton is Turing complete.\nProof. The equivalence is quite intuitive: the two stacks (which are infinite in one direction) of the\n2-PDA can simulate the tape of a Turing machine by popping symbols from one stack and pushing\nsymbols onto the other one simultaneously. The head of the Turing machine then effectively reads\nthe entries at the top of one of the two stacks. For a formal proof, see Theorem 8.13 in Hopcroft\net al. (2006).\n■\nThis is also the reason why we only have to consider two-stack PDAs—they can compute\neverything that can be computed, meaning that additional stacks do not increase their expressiveness!\nSince unweighted pushdown automata are simply special cases of weighted PDAs, which are\nequivalent to probabilistic PDAs, we can therefore also conclude:\nCorollary 4.2.1\nAny weighted 2-stack pushdown automaton is Turing complete.\nCorollary 4.2.2\nAny probabilistic 2-stack pushdown automaton is Turing complete.\n4.2. PUSHDOWN LANGUAGE MODELS\n135\nA straightforward consequence of the Turing completeness of two-stack PPDAs is that their\ntightness is undecidable.\nTheorem 4.2.7: Tightness of 2-PPDA is undecidable\nThe tightness of a probabilistic two-stack pushdown automaton is undecidable.\nProof. We start with a simple observation: a pushdown automaton P is tight if and only if it halts\non inputs with measure 1 (given the probability measure on Σ˚ Y Σ8 defined in §2.5.4), as this, by\nthe definition of the language accepted by the WPDA (cf. §4.2.7), corresponds to its language only\ncontaining finite strings with probability 1.\nLet M then be a Turing machine and P be a 2-PPDA which simulates it. Then, P is tight if\nand only if it halts with probability 1 (again, based on the probability measure from above). This is\nequivalent to the problem of M halting with probability 1—this, however, is a variant of the halting\nproblem, which is one of the fundamental undecidable problems. We have therefore reduced the\nproblem of determining the tightness of 2-PPDAs to the halting problem, implying that the former\nis undecidable.\n■\nYou might wonder what this means for the (weighted) languages recognized by multiple-stack\n(weighted) automata. Turing machines can recognize recursively enumerable languages. This means\nthat weighted multi-stack pushdown automata model distributions over recursively enumerable\nlanguages. To see why this might be useful, let us finish the discussion of context-free languages\nwith an example of a language model that is not context-free:\nExample 4.2.13: Example of a non-context-free distribution over strings\nLet Σ “ tau and pLM panq “ e´λ λn\nn! for n P Ně0, i.e., L ppLMq Q y “ an „ Poisson pλq for some\nλ ą 0. This language is not context-free: the proof, however, is not trivial. We direct the\nreader to Icard (2020b) for one.\n136\nCHAPTER 4. CLASSICAL LANGUAGE MODELS\n4.3\nExercises\nExercise 4.1\nProve the following lemma.\nLemma 4.3.1\nLet A “ pΣ, Q, δ, λ, ρq and q P Q. Then\nZ pA, qq “\nÿ\nq\na{w\nÝÝÑq1PδAL\nω\nˆ\nq\na{¨\nÝÝÑ q1\n˙\nZpA, q1q ` ρ pqq\n(4.138)\nExercise 4.2\nShow that the expression for the log-likelihood of the n-gram model can be rewritten as\nℓℓpDq “\nM\nÿ\nm“1\n|ypmq|\nÿ\nt“1\nlog θyn|yăn “\nÿ\ny\n|y|“n\nC pyq θyn|yăn\n(4.139)\nwith the quantities as defined in Proposition 4.1.3. This is a common trick. It is also known as the\ntoken to type switch because we switch from counting over the individual tokens to counting\nover their identities (types)\nExercise 4.3\nLet C pyq be the string occurrence count for y P Σ˚ occurrence count as defined in Proposition 4.1.3.\nShow (or simply convince yourself) that, in a given training corpus D\nÿ\ny1PΣ\nC\n`\ny1 . . . yn´1y1˘\n“ C py1 . . . yn´1q\n(4.140)\nChapter 5\nNeural Network Language Models\nChapter 4 introduced two classical language modeling frameworks: finite-state language models and\ncontext-free language models. While those served as a useful introduction to the world of language\nmodeling, most of today’s state-of-the-art language models go beyond the modeling assumptions\nof these two frameworks. This chapter dives into the diverse world of modern language modeling\narchitectures, which are based on neural networks. We define two of the most common architectures—\nrecurrent neural networks and transformers—and some of their variants. The focus is again on\nrigorous formalization and theoretical understanding—we analyze the introduced models in terms of\nthe theoretical foundations so far (e.g., expressiveness and tightness)—but, due to their practical\napplicability, we also study some practical aspects of the models.\nWe begin with recurrent neural networks.\n137\n138\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n5.1\nRecurrent Neural Language Models\nThe first neural language modeling architecture we consider is one based on recurrent neural networks.\nRecurrent neural networks capture the idea of the sequential processing of strings relatively naturally\nwhile also making decisions based on an infinite context. Before delving into the technical details of\nrecurrent neural networks (RNNs), however, we first motivate the introduction of modeling contexts\nof unbounded length. Then, we formally define recurrent neural networks and devote a large portion\nof the section to their theoretical properties. The most important of those will be the Turing\ncompleteness of this architecture, as it has numerous consequences on the solvability of many of the\ntasks we might be interested in, such as finding the most probable string in the language model\nrepresented by a recurrent neural network and determining whether an RNN is tight.\n5.1.1\nHuman Language is Not Context-free\nRecall that we motivated the introduction of context-free languages by observing that finite memory\nis insufficient to model all formal phenomena of human language, e.g., infinite recursion (cf. Ex-\nample 4.2.1). Context-free languages described by context-free grammars and pushdown automata\nwere able to capture those. However, human language is more expressive than that—it includes\nlinguistic phenomena that cannot be described by context-free grammars. A typical example is\ncalled cross-serial dependencies, which are common in Swiss German.\nExample 5.1.1: Cross-Serial Dependencies in Swiss German, Shieber, 1985\nSwiss German is a textbook example of a language with grammatical cross-serial dependencies,\ni.e., dependencies in which the arcs representing them, cross. In the example sentence below,\nthe words connected with arcs are objects and verbs belonging to the same predicates (verb\nphrases). Because of that, they have to agree on the form—they depend on one another. As\nwe show next, context-free languages cannot capture such dependencies.\n...mer\nd’chind\nem Hans\ns’ huus\nl¨ond\nh¨alfe\naastriiche\n...we\nthe children\nHans\nthe house\nlet\nhelp\npaint\nWhy are cross-serial dependencies non-context-free?\nBefore reasoning about the phe-\nnomenon of cross-serial dependencies, we revisit Example 4.2.1 with a somewhat more formal\napproach.\nThe arbitrarily deep nesting can, for example, be abstractly represented with the\nexpression\nxAnBny\n(5.1)\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n139\nwith1\nx “ “The cat”\nA “ “the dog”\nB “ “barked at”\ny “ “likes to cuddle”.\nFrom this abstract perspective, center embeddings are very similar to the Dp1q language (Exam-\nple 4.2.5), in that every noun phrase “the dog” has to be paired with a verb phrase “barked at”,\nwhich cannot be represented by any regular language.\nIn a similar fashion, Example 5.1.1 can abstractly be represented with the expression\nxAmBnCmyDnz\n(5.2)\nwith\nx “ “...mer”\nA “ “d’chind”\nB “ “em Hans”\ny “ “s’ huus”\nC “ “l¨ond”\nD “ “h¨alfe”\nz “ “aastriiche”.\nAdmittedly, this is a relatively uncommon formulation even with n “ m “ 1. It should be taken with\na grain of salt, as the title of the original publication discussing this phenomenon, Evidence against\ncontext-freeness (Shieber, 1985), also suggests. However, theoretically, the number of repetitions of\n“d’chind” and “l¨ond”, as well as “em Hans” and “h¨alfe”, can be increased arbitrarily. Repeating\nthe former would correspond to having many groups of children. The last of the groups would let\nHans help paint the house, whereas each of the previous groups would let the group after them either\nlet Hans paint the house or recurse onto another group of children. Similarly, repeating “em Hans”\nand “h¨alfe” would correspond to a number of Hanses, each either helping another Hans or helping\npaint the house. Then, using the pumping lemma for context-free languages, it can be shown that\nthe expressions of the form in Eq. (5.2) cannot be recognized by any context-free grammar. We\nrefer the readers to Hopcroft et al. (2006, Example 7.20) for detailed proof.\nExample 5.1.1 means that to model a human language formally, we need more expressive\nformalisms than context-free grammars or pushdown automata as described in the previous sections.2\nHowever, instead of defining a more expressive formalism motivated by formal language theory (like\nwe did with context-free grammars and center embeddings), we now introduce recurrent neural\nnetworks, which, as we will see, under certain assumptions, have the capacity to model all computable\nlanguages (i.e., they are Turing complete). Moreover, they can also model infinite lengths of the\ncontext yăt in a very flexible way. In the next section, we define them formally.\n1In this case, we of course only consider an arbitrarily long sequence of barking dogs.\n2On the other hand, note that we would ideally also like to upper-bound the expressive power of the formal models,\nas this introduces useful inductive biases for learning and sparks insights into how humans process language. This\nmeans that we would not simply like to jump to Turing-complete models in such an exploration of language models.\n140\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n5.1.2\nRecurrent Neural Networks\nAs discussed, natural languages are beyond the descriptive power of regular and context-free\nlanguages.\nNow, we turn to a class of models that is theoretically capable of recognizing all\ncomputable languages: recurrent neural networks (RNNs).3\nAn Informal Introduction to Recurrent Neural Networks\nHuman language is inherently sequential: we produce and consume both spoken as well as written\nlanguage as a stream of units.4 This structure is reflected in some of the algorithms for processing\nlanguage we have seen so far. For example, finite-state automata (cf. §4.1) process the input string\none symbol at a time and build the representations of the string seen so far in the current state of\nthe automaton. Pushdown automata function similarly, but additionally keep the stack as part of\nthe configuration.\nRecurrent neural networks are neural networks that capture the same idea of iterative processing\nof the input but do so in a more flexible way than the finite-memory finite-state automata and the\nstack-based pushdown automata. Very abstractly, a recurrent neural network sequentially processes\na sequence of inputs and, while doing so, produces a sequence of hidden states, which we will\ndenote as h, based on a transition function in form of a recurrent dynamics map, which acts\nsimilarly to a (deterministic) transition function in a finite-state machine: given the current hidden\nstate and an input symbol, it (deterministically) determines the next hidden state. The hidden states\nplay, as we will see, an analogous role to the states of a finite-state automaton or the configuration of\na pushdown automaton: The current hidden state of a recurrent neural network at time t determines,\ntogether with the input at time t, through the dynamics map, the hidden state at time t`1—indeed,\nvery similar to how finite-state automata process strings and transition between their states. Again,\nthe hidden state can be thought of as a compact (constant-size) summary of the input yďt seen so\nfar and should ideally characterize yďt as well as possible (in the sense of retaining all information\nrequired for continuing the string). Remember from §4.2.1 that the finite number of states of a\nfinite-state automaton presented a serious limitation to its ability to model human language. As\nwe will see, the main difference between (weighted) finite-state automata and RNNs is that the\nlatter can work with infinite state spaces, for example, RD in the abstract formulation, or QD in a\ndigital computing system, such as a computer. This, together with the flexibility of the transition\nfunction between hidden states, will allow RNNs to represent more complex languages than those\nrecognized by finite-state automata or context-free grammars. In fact, the large state space and the\nflexible transition functions endow RNNs, under some assumptions, with the possibility to model\ninfinitely-long-term dependencies on the input string, distinguishing them from the Markovian\nn-gram models.\nYou might wonder why we refer to the current state of an RNN as hidden states instead of\nonly as states, as with finite-state automata. Indeed, when analyzing recurrent neural networks\nin terms of their expressivity and connections to classical models of computation, we will regard\nthe hidden states as completely analogous to states in a finite-state or pushdown automaton. The\nhidden part comes from the fact that the hidden states h are usually not what we are interested\nin when modeling language with an RNN. Rather, h is simply seen as a component in a system\n3In this subsection, we focus on the applications of recurrent neural networks to language modeling. However,\nrecurrent neural networks have been widely used to process sequential data and time series, thanks to their power of\ntaking in arbitrary-length inputs.\n4So far, we have simply referred to those units as symbols.\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n141\ny1\ny2\ny3\nh0\nh1\nh2\nh3\n¨ ¨ ¨\nState ht\nInput yt\n(a) An abstract depiction of how an RNN processes one symbol in a string. The hidden state ht summarizes\nthe inputs y1y2 . . . yt.\nh0\nh1\nh2\nh3\n¨ ¨ ¨\ny1\ny2\ny3\ny4\n(b) An abstract depiction of an RNN as an automaton. The transitions between the possibly infinitely-many\nhidden states are determined by the dynamics map.\nh\ny\n(c) An abstract depiction of an RNN as a system updating the hidden state h depending on the input y.\nFigure 5.1: Different possible depictions of an abstract RNN model. The way that the hidden states\nare updated based on the input symbol yt is abstracted away.\nthat produces individual conditional probabilities over the next symbol, as in sequence models (cf.\nDefinition 2.5.2)—these conditional probabilities are the actual “visible” parts, while the “internal”\nstates are, therefore, referred to as hidden.\nRNNs are abstractly illustrated in different ways in the literature. Often, they are represented\nas a sequence of hidden states and the input symbols consumed to arrive at those states—this\nis shown in Fig. 5.1a. They can also be presented more similarly to automata, with (a possibly\ninfinite) labeled graph, where the transition labels again correspond to the symbols used to enter\nthe individual states. This is presented in Fig. 5.1b. Lastly, due to the infinite state space, one can\nalso think of an RNN as a system that keeps the most current hidden state in memory and updates\nit as new symbols are consumed—this is shown in Fig. 5.1c.5\nA Formal Definition of Recurrent Neural Networks\nHaving introduced RNNs and their motivations informally, we now move to their formal definition.\nOur definition and treatment of recurrent neural networks might differ slightly from what you\nmight normally encounter in the literature. Namely, we define RNNs below as abstract systems\ntransitioning between possibly infinitely-many states. Our definition will allow for an intuitive\nconnection to classical language models such as finite-state and pushdown language models as\n5More precisely, these illustrations correspond to first-order RNNs, which are by far the most common. Later, we\nwill also briefly consider higher-order RNNs, whose hidden state update depends on multiple previous hidden states.\n142\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nwell as for tractable theoretical analysis in some special cases. Specifically, when analyzing RNNs\ntheoretically, we will make use of their connections to automata we saw in Chapter 4.\nIn an abstract sense, recurrent neural networks can be defined as a system transitioning between\npossibly infinitely-many states, which we will assume to be vectors in a vector space. Specifically,\nwe will distinguish between real and rational recurrent neural networks.\nDefinition 5.1.1: Real-valued Recurrent Neural Network\nLet Σ be an alphabet. A (deterministic) real-valued recurrent neural network R is a\nfour-tuple pΣ, D, f, h0q where\n• Σ is the alphabet of input symbols;\n• D is the dimension of R;\n• f : RD ˆ Σ Ñ RD is the dynamics map, i.e., a function defining the transitions between\nsubsequent states;\n• h0 P RD is the initial state.\nWe analogously define rational-valued recurrent neural networks as recurrent neural\nnetworks with the hidden state space QD instead of RD. You might wonder why we make the\ndistinction. Soon, when we take on theoretical analysis of RNNs, it will become important over\nwhich state spaces the models are defined. RNNs implemented in a computer using floating-point\nnumbers, of course, cannot have irrational-valued weights—in this sense, all implemented recurrent\nneural networks are rational. However, defining the models over the real numbers crucially allows us\nto perform operations from calculus for which some sort of continuity and smoothness is required,\nfor example, differentiation for gradient-based learning (cf. §3.2.3).\nExample 5.1.2: A rational-valued RNN\nAn example of a rational-valued RNN is the series\nht “ 1\n2ht´1 `\n1\nht´1\n(5.3)\nwhich we considered in Example 3.1.1. In this case\n• Σ “ tau\n• D “ 1\n• f : px, aq ÞÑ 1\n2x ` 1\nx\n• h0 “ 2\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n143\nExample 5.1.3: Another example of an RNN\nThe tuple R “ pΣ, D, f, h0q where\n• Σ “ ta, bu\n• D “ 2\n• f : px, yq ÞÑ\n$\n’\n’\n’\n’\n&\n’\n’\n’\n’\n%\n˜\ncos ϕ\n´ sin ϕ\nsin ϕ\ncos ϕ\n¸\nx\nif y= a\n˜\ncos ψ\n´ sin ψ\nsin ψ\ncos ψ\n¸\nx\notherwise\n• h0 “\nˆ\n1\n1\n˙\nis an example of a real-valued RNN which rotates the current hidden state by the angle ϕ if\nthe input symbol is a and rotates it by ψ if the symbol is b.\nExample 5.1.4: Another example of an RNN\nAnother example of an RNN would be the tuple\n• Σ “ GOOD Y BAD “ t“great”, “nice”, “good”u Y t“awful”, “bad”, “abysmal”u\n• D “ 2\n• f : ph, aq ÞÑ\n$\n’\n’\n’\n’\n&\n’\n’\n’\n’\n%\nh `\n˜\n1\n0\n¸\nif a P GOOD\nh `\n˜\n0\n1\n¸\notherwise\n• h0 “\nˆ\n0\n0\n˙\nwhich counts the number of occurrences of positive and negative words.\nTo define language models using recurrent neural networks, we will use them as the encoder\nfunctions enc in our general language modeling framework (cf. §3.1). To connect Definition 5.1.1\nwith the general LM framework, we define the RNN encoding function.\nDefinition 5.1.2: Recurrent Neural Encoding Function\nLet R “ pΣ, D, f, h0q be a recurrent neural network. A recurrent neural encoding function\nencR is a representation function (cf. §3.1.1) that recursively encodes strings of arbitrary\nlengths using its dynamics map f:\nencR pyăt`1q\ndef\n“ fpencRpyătq, ytq P RD\n(5.4)\n144\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nand\nencRpyă1q\ndef\n“ h0 P RD\n(5.5)\nIntuitively, an RNN R takes an input string y and encodes it with the encoding function\nencR by sequentially applying its dynamics map f. The representations of individual prefixes (cf.\nDefinition 2.3.6) of the input string are called hidden states.\nDefinition 5.1.3: Hidden State\nLet R “ pΣ, D, f, h0q be an RNN. The hidden state ht P RD describes state of R after reading\nyt. It is recursively computed according to the dynamics map f as follows:\nht\ndef\n“ encRpyăt`1q “ fpht´1, ytq\n(5.6)\nExample 5.1.5: Hidden states\nThe hidden states of the RNN from Example 5.1.2 are the individual values ht, which, as t\nincreases, approach\n?\n2.\nRecurrent Neural Sequence Models\nA recurrent neural network based on Definition 5.1.1 on its own does not yet define a sequence\nmodel, but simply a context encoding function encR : Σ\n˚ Ñ RD. To define a sequence model based\non an RNN, we simply plug in the RNN encoding function Definition 5.1.2 into the General language\nmodeling framework from §3.1.\nDefinition 5.1.4: Recurrent neural sequence model\nLet R “ pΣ, D, f, h0q be a recurrent neural network and E P R|Σ|ˆD a symbol representation\nmatrix. A D-dimensional recurrent neural sequence model over an alphabet Σ is a tuple\npΣ, D, f, E, h0q defining the sequence model of the form\npSM pyt | yătq\ndef\n“ f∆|Σ|´1 pE encRpyătqqyt “ f∆|Σ|´1 pE ht´1qyt.\n(5.7)\nBy far the most common choice of the projection function is the softmax yielding the sequence\nmodel\npSM pyt | yătq\ndef\n“ softmaxpE encRpyătqqyt “ softmaxpE ht´1qyt.\n(5.8)\nFor conciseness, we will refer to RNN sequence models whose next-symbol probability distri-\nbutions are computed using the softmax function as softmax RNN sequence models.\nFrom this perspective, we see that RNNs are simply a special case of our general language\nmodeling framework with parameterized representations of tokens y P Σ and the history y P Σ˚ (cf.\n§3.1)—an RNN simply defines how the encoding function enc is specified. The three figures from\nFig. 5.1 are presented again with this probabilistic perspective in Fig. 5.2.\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n145\ny1\ny2\nh0\nh1\nh2\n¨ ¨ ¨\n¨ ¨ ¨\ny1 „ pSM p¨ | h0q\ny2 „ pSM p¨ | h1q\nState ht\nInput yt\n(a) An abstract depiction of how an RNN generates a string one symbol at a time. The hidden state ht\nsummarizes the string y1y2 . . . yt generated so far. The dotted lines denote the sampling steps.\nh0\nh1\nh2\n¨ ¨ ¨\ny1 „ pSM p¨ | h0q\ny2 „ pSM p¨ | h1q\n(b) An abstract depiction of a generative RNN as an automaton.\nht\nyt „ pSM p¨ | htq\n(c) An abstract depiction of an RNN as a system updating the hidden state ht depending on the generated\nsymbol yt.\nFigure 5.2: Different possible depictions of an abstract RNN model generating symbols. The way\nthat the hidden states are updated based on the input symbol yt is abstracted away.\n146\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nA Few More Definitions\nIn the following, we will often use the so-called one-hot encodings of symbols for concise notation.\nWe define them here.\nDefinition 5.1.5: One-hot encoding\nLet Σ be an alphabet and n : Σ Ñ t1, . . . , |Σ|u a bijection (i.e., an ordering of the alphabet,\nassigning an index to each symbol in Σ). A one-hot encoding J¨K is a representation function\nof the symbols in Σ which assigns the symbol y P Σ the npyqth basis vector:\nJyK\ndef\n“ dnpyq,\n(5.9)\nwhere here dn is the nth canonical basis vector, i.e., a vector of zeros with a 1 at position n.\nExample 5.1.6: One-hot encoding\nLet Σ “ t“large”, “language”, “models”u and n “ t“large”: 1, “language”: 2, “models”: 3u.\nThe one-hot encoding of the vocabulary is:\nJ“large”K “\n¨\n˝\n1\n0\n0\n˛\n‚, J“language”K “\n¨\n˝\n0\n1\n0\n˛\n‚, J“models”K “\n¨\n˝\n0\n0\n1\n˛\n‚\n(5.10)\nMany specific variants of recurrent neural networks define the dynamics map f in a specific way:\nthe output of the function is some element-wise (non-linear) transformation of some “inner” function\ng. The dynamics map of such an RNN is then the composition of g and the non-linearity.\nDefinition 5.1.6: Activation function\nLet R “ pΣ, D, f, E, h0q be an RNN. If the hidden states ht of the RNN are computed as\nht “ σpgpht´1, yqq\n(5.11)\nfor some function g: RD ˆ Σ Ñ RD and some function σ: R Ñ R which is computed element-\nwise (that is, σpxqd “ σpxdq for all d “ 1, . . . , D and x P RD), we call σ an activation\nfunction.\nThis finishes our formal definition of recurrent neural networks. We next consider some of their\ntheoretical properties, starting with tightness.\n5.1.3\nGeneral Results on Tightness\nWe now discuss a general result on the tightness of recurrent neural sequence models, as defined\nin Definition 5.1.4. The analysis is straightforward and is a translation of the generic results on\ntightness (cf. §3.1.5) to the case of the norm of the hidden states of an RNN, ht as the encodings of\nthe prefixes yďt, but it requires us to focus specifically on softmax RNN sequence models.\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n147\nTheorem 5.1.1: Tightness of Recurrent Neural Sequence Model\nA softmax recurrent neural sequence model is tight if for all time steps t it holds that\ns∥ht∥2 ď log t,\n(5.12)\nwhere s\ndef\n“ maxyPΣ ∥epyq ´ epeosq∥2.\nProof. This is simply a restatement of Theorem 3.1.6 for the case when enc takes the form of a\ngeneral RNN encoding function, encR.\n■\nCorollary 5.1.1: RNNs with bounded dynamics maps are tight\nA softmax recurrent neural sequence model R “ pΣ, D, f, h0q with a bounded dynamics map\nf, i.e, with a dynamics map f such that\n|fpxqd | ď M\n(5.13)\nfor some M P R, for all d “ 1, . . . , D and all x P RD, is tight.\nProof. If the dynamics map is bounded, the norm of the hidden state, ∥ht∥2, is bounded as well.\nThis means that the left-hand-side of Eq. (5.12) is constant with respect to t and the condition\nholds trivially.\n■\nA special case of Corollary 5.1.1 is RNNs with bounded activation functions (cf. Definition 5.1.6).\nThose are tight if the activation function itself is bounded. This implies that all standard sigmoid\nand tanh activated recurrent neural networks are tight. However, the same does not hold for RNNs\nwith unbounded activation functions, which have lately been more popular (one of the reasons for\nthis is the vanishing gradient problem (Glorot et al., 2011)).\nExample 5.1.7: RNNs with unbounded activation functions may not be tight\nA very popular unbounded activation function is the so-called rectified linear unit (ReLU),\ndefined as\nReLU pxq\ndef\n“ max p0, xq .\n(5.14)\nThis function is clearly unbounded.\nNow suppose we had the following RNN over the simple alphabet Σ “ tau.\nht “ ht´1 `\n`\n1\n˘\n,\n(5.15)\ninitial state\nh0 “\n`\n0\n˘\n(5.16)\nand the output matrix\nE “\nˆ\n´1\n1\n˙\n(5.17)\n148\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nwhere the top row of E computes the logit of the eos symbol and the bottom one the one of\na. It is easy to see that\nht “\n`\nt\n˘\n.\n(5.18)\nThis already does not look promising for tightness—the norm of the hidden state, which is, in\nthis case,\n\r\r`\nt\n˘\r\r “ t, and is, therefore, increasing at a much higher rate than O plog tq required\nby Theorem 5.1.1. We encounter a similar hint against tightness if we compute the conditional\nprobabilities of the eos symbol and the symbol a.\npSM peos | yătq “ softmax\nˆˆ\n´1\n1\n˙ `\nt ´ 1\n˘˙\neos\n“\nexp r´t ` 1s\nexp r´t ` 1s ` exp rt ´ 1s\n(5.19)\npSM peos | yătq “ softmax\nˆˆ\n´1\n1\n˙ `\nt ´ 1\n˘˙\na\n“\nexp rt ´ 1s\nexp r´t ` 1s ` exp rt ´ 1s\n(5.20)\nThe probability of ending the string at time step t is, therefore\npSM peos | yătq “\n1\n1 ` exp r2 pt ´ 1qs.\n(5.21)\nIntuitively, this means that the probability of ending the string (generating eos) diminishes\nrapidly with t—in this case much faster than any diverging sum required by Theorem 2.5.3.\nAll signs, thus, point towards the RNN from Eq. (5.15) not being tight. Indeed, for this\nspecific case, one can show using some algebraic manipulations that\nÿ\nyPΣ˚\npLN pyq “\nÿ\nnPNě0\npLN panq ă 0.15\n(5.22)\nwhere pLN is the locally normalized model induced by the RNN. This means that the RNN\nfrom Eq. (5.15) assigns less than 0.15 probability to finite strings—all other probability mass\nleaks to infinite sequences.\nExample 5.1.8: RNNs with unbounded activation functions can still be tight\nExample 5.1.7 showed that RNNs with unbounded activation functions can indeed result in\nnon-tight sequence models. However, this is not necessarily the case, as this simple modification\nof the RNN from Example 5.1.7 shows. The only aspect of the RNN that we modify is the\noutput matrix E, which we change by flipping its rows:\nE “\nˆ\n1\n´1\n˙\n(5.23)\nNow the probability of ending the string at time step t is\npSM peos | yătq “\nexp rt ´ 1s\nexp r´t ` 1s ` exp rt ´ 1s “\n1\nexp r´2 pt ´ 1qs ` 1.\n(5.24)\nCompared to Eq. (5.21), the probability of eos in Eq. (5.24) does not diminish. Indeed, since\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n149\n1\nexpr´2pt´1qs`1 ą 1\n2 for all t, the sum\n8\nÿ\nt“0\npSM peos | yătq\n(5.25)\ndiverges, which, according to Proposition 2.5.6 implies that the sequence model is tight.\n5.1.4\nElman and Jordan Networks\nThe characterization of dynamics maps we gave in Definition 5.1.4 allows for f to be an arbitrary\nmapping from the previous state and the current input symbol to the new state. In this section, we\nintroduce two seminal and particularly simple parameterizations of this map—the simplest recurrent\nneural sequence models. We term them Elman sequence models and Jordan sequence models, as\neach is inspired by architectures proposed by Elman (1990) and Jordan (1986), respectively. The\ndefinitions we present here are slightly different than those found in the original works—most notably,\nboth Elman and Jordan networks were originally defined for transduction (mapping an input string\nto an output string, as with translation) rather than language modeling.\nPut simply, these two models restrict the form of the dynamics map f in the definition of an\nRNN (cf. Definition 5.1.1). They define particularly simple relationships between the subsequent\nhidden states, which are composed of affine transformations of the previous hidden state and\nthe representation of the current input symbol passed through a non-linear activation function\n(cf. Definition 5.1.6). The affine transformations are performed by different matrices and bias\nvectors—the parameters of the model (cf. Assumption 3.2.2)—each transforming a separate part of\nthe input to the dynamics map.\nDefinition 5.1.7: Elman Sequence Model (Elman, 1990)\nAn Elman sequence model R “ pΣ, D, U, V, E, bh, h0q is a D-dimensional recurrent neural\nsequence model over an alphabet Σ with the following dynamics map\nht “ σ\n`\nUht´1 ` Ve1pytq ` bh\n˘\n.\n(5.26)\nHere, e1¨: Σ Ñ RR is the input symbol embedding function which represents each symbol\ny P Σ as a R-dimensional vector and σ is an element-wise non-linearity.a bh P RD, U P RDˆD,\nand V P RDˆR.\naThe symbol representations e1y are often also referred to as static symbol embeddings because they do\nnot depend on the string surrounding or preceding y. Here, we treat them as any other parameters of the\nmodel which can be learned using gradient-based learning (cf. §3.2). However, note that learning good static\nembeddings was a very active field before the emergence of large end-to-end systems we see today. Very popular\nexamples include Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski\net al., 2017).\nDue to its simplicity, the Elman RNN is also known as the vanilla RNN variant, emphasizing it\nis one of the most fundamental variants of the framework.\n150\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nOn the symbol representations.\nNotice that in Eq. (5.26), the input symbols yt are first\ntransformed into their vector representations e1pytq and then additionally linearly transformed\nusing the matrix V. This results in an over-parametrized network—since the symbols are already\nembedded using the representation function e1, the matrix V is theoretically superfluous and could\nbe replaced by the identity matrix. However, the matrix V could still be useful if the representations\ne1pytq are fixed—in this case, the matrix can be used by the RNN to transform the representations\nduring training to fit the training data better. This is especially useful if the symbol representations\ne1pytq already represent the input symbols in a compact representation space in which the parameters\ncan be shared across different symbols. Alternatively, we could represent the symbols using their\none-hot encodings, i.e., e1pytq “ JytK, in which case the columns of the matrix V would correspond to\nthe symbol representations (analogously to the representation matrix E from Eq. (3.46)). However,\nnotice that in this case, the representations on the symbols do not share any parameters, and\neach column of the matrix is therefore an unconstrained vector. Such matrix-lookup-based input\nsymbol representations from Eq. (5.26) are sometimes tied, i.e., e1¨ “ ep¨q, with the output symbol\nrepresentations from the embedding matrix E in the definition of the sequence model induced by an\nRNN (cf. Definition 3.1.11 and Eq. (5.7)).\nHowever, embedding tying is non-essential to representation-based LMs. The input symbol\nembedding function can always be chosen independently with the output symbol embedding function\nDefinition 3.1.6.\nThe Jordan network is somewhat different in that it feeds the output logits computed through\nthe output matrix E into the computation of the next state, and not directly the hidden state.\nDefinition 5.1.8: Jordan Sequence Model (Jordan, 1986)\nA Jordan sequence model is a D-dimensional recurrent neural sequence model over an\nalphabet Σ with the following dynamics map\nht “ σ\n`\nUrt´1 ` Ve1pytq ` bh\n˘\n(5.27)\nrt “ σopEhtq\n(5.28)\nAgain, e1¨: Σ Ñ RR is the input symbol embedding function which represents each symbol\ny P Σ as a R-dimensional vector while σ and σo are element-wise non-linearities. bh P RD,\nU P RDˆD, and V P RDˆR.\nNotice that the hidden state ht in Eq. (5.27) is not computed based on the previous hidden\nstate ht´1, but rather on the transformed outputs rt´1—this is analogous to feeding back in the\nlogits computed in Eq. (5.7) into the computation of h rather than the previous hidden state. The\nsequence model induced by a Jordan network is then directly induced by the logits rt (i.e., the\nconditional probabilities are computed by putting vt through the softmax.\nIn both architectures, the activation function σ can be any suitable element-wise function. The\ncanonical choices for it have been the sigmoid and tanh functions, however, a more common choice\nnowadays is the ReLU function or any of its more modern variants.6\nSince we will refer to the individual matrices defining the dynamics maps in Elman and Jordan\nnetworks quite a lot in the next subsections, we give them specific names. The matrix U, which\nlinearly transforms the previous hidden state (or the output) is the recurrence matrix. The\n6See Goodfellow et al. (2016, §˜6.3.1) for an overview of modern activation functions used in neural networks.\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n151\nmatrix V, which linearly transforms the representations of the input symbol, is called the input\nmatrix. Lastly, the matrix which linearly transforms the hidden state before computing the output\nvalues rt with an activation function is called the output matrix. bh is the hidden bias vector.\nTightness of Elman and Jordan Recurrent Neural Networks\nAs a simple corollary of\nCorollary 5.1.1, we can characterize the tightness of Elman and Jordan recurrent neural networks as\nfollows.\nCorollary 5.1.2: Tightness of simple RNNs\nElman and Jordan RNNs with a bounded activation function σ and the softmax projection\nfunction are tight.\n152\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n5.1.5\nVariations on Recurrent Networks\nIn the previous sections we introduced the two simplest RNN variants: the Elman (Elman, 1990)\nand Jordan (Jordan, 1997) networks. Even though such simple RNNs in theory are all we need to\nmodel any computable language, empirically those architectures face many challenges. One of the\nbiggest are the vanishing and exploding gradient problems (Hochreiter and Schmidhuber, 1997),\nwhich in practice is linked with the issue of learning long-term dependencies in language.\nIn this subsection, we expand our repertoire of RNN variants by going beyond the simple recurrent\ndynamics defined by the Elman and Jordan update rules. To do so, we take a step back and return\nto Definition 5.1.1 of a recurrent neural network R as the tuple pΣ, D, f, h0q . We will define more\nelaborate dynamics maps f which both aim to tackle some of the (empirically encountered) challenges\nof simpler variants as well as improve some theoretical aspects of the networks. Importantly, keep\nin mind that the only aspect of the RNN we will strive to modify is the dynamics map—that is,\nthe mapping from ht´1 to ht. Given a hidden state, the definition of a sequence model will remain\nidentical.\nA common component of the more complex dynamics maps we explore in this section is the\ngating mechanism, which is why we start with it.\nGating\nThe update equations of Elman and Jordan RNNs define relatively simple transformations of the\nhidden states as an affine transformation of the previous hidden state and the new input, followed\nby some form of non-linearity. In this sense, the interaction between the previous hidden state and\nthe input symbol is relatively limited—the hidden state is transformed by the recurrence matrix U\nat every time step invariant to the input symbol being read. To see why this could be a limiting\nfactor, consider the following example.\nExample 5.1.9: RNN Gates\nConsider the language L “ tanbncnxambmcm | n, m P Ně0u. It intuitively consists of two-part\nstrings, where the two parts are separated by a symbol x. The part on the left side of x\ncontains a sequence of n a’s followed by n b’s, which is followed by n c’s. The substring on the\nright side of x contains a sequence of m a’s which is again followed by m b’s, and later by m c’s.\nBoth parts of the string can be arbitrarily long, and, intuitively, to correctly recognize a string\nin this language, a computational model has to keep the information about the number of a’s\nwhile reading in b’s to be able to ensure there is a correct number of c’s as well. This creates\na long-term dependency across the entire block of b’s. However, notice that, after reading the\nsymbol x, the information about the number of a’s becomes irrelevant to the recognition of\nthe string: the model can, therefore, discard it and solely focus on modeling the rest of the\nstring, which again requires keeping track of the number of the symbol occurrences. In other\nwords: after a certain amount of time, previous information becomes irrelevant, and we may\nwant to design a network that is able to select which information is important to keep around.\nTo enable richer interaction between the transformation of the RNN hidden state and the input\nsymbols, we introduce the gating mechanism. Intuitively, the gating mechanism enables more\nfine-grained control over the transformations of the hidden state by “selecting” which aspects of the\nhidden state should be retained, which should be modified, and which should be deleted—in general,\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n153\nbased on both the previous hidden state as well as the current input symbol. Such transformations\nare defined using gates and gating functions.\nDefinition 5.1.9: Gate\nA gate is a real-valued vector g P RD, such that gd P r0, 1s for all d P t1, . . . , Du. Gates are\ncomputed using gating functions, i.e., functions whose outputs live in r0, 1sD.\nThe fact that every dimension in a gate gt takes a value between 0 and 1 invites a natural\ninterpretation of the values as soft switches, analogously to how switches are used in the electrical\nengineering context. Intuitively, in the context of RNNs, where the information is passed around in\nthe hidden states ht, a gate of the same dimensionality as the hidden state can control which aspects\n(dimensions) of the hidden state should be forgotten (switched off) or and which ones retained (kept\non)—a gate value close to 0 can be interpreted as a signal that the information captured in the\ncorresponding dimension of the hidden state should be “forgotten”, and a gate value close to 1\nas the opposite. Such modifications of the hidden state can be performed using an element-wise\nmultiplication of the hidden state h and the gate g, which we denote with h d g.\nImportantly, the gates can be computed based on the information about the string seen so far as\nwell as the new input symbol—this means that the decision on what should be remembered and\nwhat should be forgotten can be made for each situation individually. This allows RNN variants\nusing gating to implement mechanisms to tackle challenges as the one described in Example 5.1.9.\nFurthermore this not only enables RNNs to selectively keep information about the string, but\nalso combat the vanishing and exploding gradient problems (Hochreiter and Schmidhuber, 1997,\nAppendix 2). We next consider two of the best-known gated RNNs: Long Short-Term Memory and\nGated Recurrent Unit networks .\nLong Short-term Memory Networks\nLong Short-term Memory Networks (LSTM, Hochreiter and Schmidhuber, 1997) are perhaps the best-\nknown type of a gated recurrent network. They were introduced specifically to combat the vanishing\ngradient problem in the famous paper with more than 80 000 citations. The somewhat unusual\nname comes from connections to human memory, in which short-term memory is characterized by\nevanescent neural activations, and long-term memory is based on the growth and structural change\nin neuron connections (Hebb, 1949).\nThe LSTM unit.\nLSTM RNNs are built from the LSTM units, which implement the RNN\ndynamics map and therefore perform the RNN update step. To transform the hidden state ht at\neach time step, an LSTM network additionally keeps another running summary of the string yăt, on\nwhich the recurrent update depends—this is the so-called memory cell which we will denote by ct.\nInformally, one can think of the context as the information needed to decide on how to transform\nthe hidden state at each individual time step, depending on the input string. The formal definition\nof the LSTM cell is the following.\n154\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nDefinition 5.1.10: Long Short-Term Memory\nA long short-term memory unit is a recurrent neural network with the dynamics map\ndefined through the following sequence of computations:\nit “ σ\n`\nUiht´1 ` Vie1yt ` bi˘\n(input gate)\nft “ σ\n`\nUfht´1 ` Vfe1yt ` bf˘\n(forget gate)\not “ σ\n`\nUoht´1 ` Voe1yt ` bo˘\n(output gate)\ngt “ tanhpUght´1 ` Vge1yt ` bgq\n(candidate vector)\nct “ ft d ct´1 ` it d gt\n(memory cell)\nht “ ot d tanh pctq\n(hidden state)\nit, ft, ot are the input, forget, and output gates, ct is the memory cell vector, and gt is\nthe candidate vector. Here, σ refers to the original sigmoid function.\nAs we can see, the update rule of an LSTM network is considerably more complex than that of an\nElman RNN. It is also computationally more expensive, as it involves more matrix multiplications.\nHowever, LSTMs have consistently shown improved performance compared to vanilla RNNs and\nare therefore considered together with GRUs the go-to choice for an RNN architecture (Goodfellow\net al., 2016). The theoretical reason of their success is that their gating mecahnism helps to reduce\nthe Vanishing/Exploding gradient problem, and thus to learn long-term dependencies (Hochreiter\nand Schmidhuber, 1997, Appendix 2).\nThe names of the different quantities computed in Definition 5.1.10 reflect their intuitive\ninterpretations. The input, forget, and output vectors are all gates: they control the information\nwhich will be added to the memory cell based on the new input, the information which will be\nretained or forgotten from the previous memory cell, and the information which will be transferred\nfrom the memory cell to the hidden state, respectively. Notice the identical nature in which all\nthree gates are computed: they are non-linear transformations of affine transformations of the\nprevious hidden state and the input symbol representations. Their parameter matrices define the\nway in which the gates will influence the memorization, forgetting, and addition of information. The\nadditional information added to the memory cell in the form of the candidate vector gt is computed\nsimilarly, with the only difference being the activation function. This is the step that bears the most\nresemblance to the update step of the vanilla RNN (Eq. (5.26)). However, compared to the latter,\nonly parts of this transformation are kept (based on the input and forget vectors it and ft). The\nmemory cell ct then combines the old memory content ct´1 with the newly integrated information\nin gt to form the new memory content, which is then transformed using the tanh function and\ncombined with the output gate to produce the hidden state ht. This is pictorially presented in\nFig. 5.3.\nAs mentioned, the LSTM update step is noticeably more computationally complex than that of\na vanilla RNN. This has led to a line of work trying to combine the efficiency of vanilla RNNs and\nthe empirical performance of gated RNNs. In the next subsection, we consider Gated Recurrent\nUnits, one of the best-known compromises found in this domain.\n5.1. RECURRENT NEURAL LANGUAGE MODELS\n155\nσ\nft\nσ\nTanh\ngt\nσ\nˆ\n+\nˆ\nit\nˆ\not\nTanh\nct´1\nCell\nht´1\nHidden\ne’yt\nInput\nct\nht\nht\nFigure 5.3: A pictorial depiction of the LSTM cell in action. The input it, forget ft, and output\not gates control which information of the input and of the previous hidden state is retained in the\nmemory cell, and which information is passed to next the hidden state.\nGated Recurrent Units\nThe Gated Recurrent Unit (GRU, Cho et al., 2014b,a) provides a compromise between the simplicity\nof vanilla recurrent neural networks and the empirical success of being able to model long-term\ndependencies with LSTMs. It defines a gated recurrent update unit that implements a simpler\ndynamics map by removing the memory component ct in the LSTM cell and combining the input\nand forget gates it, ft into one update gate. These changes make GRU more memory efficient and\neasier to train than LSTM in practice. The full GRU update step is defined as follows.\nDefinition 5.1.11: Gated Recurrent Units\nA gated recurrent unit defines a dynamics map in which a new hidden state is computed as:\nrt “ σ\n`\nUrht´1 ` Vre1yt ` br˘\n(reset gate)\nzt “ σ\n`\nUzht´1 ` Vze1yt ` bz˘\n(update gate)\ngt “ tanh\n`\nUg prt d ht´1q ` Vge1yt ` bg˘\n(candidate vector)\nht “ p1 ´ ztq d gt ` zt d ht´1\nrt, zt are known as the reset and update gates, and gt as the candidate vector.\nIntuitively, the update gate works like a hot/cold water mixing valve: it is trained to find the\noptimum blend of information of the candidate vector with that coming from the previous hidden\nstate. The reset gate instead, can zero the information of the previous hidden state, when computing\nthe candidate vector. This allows to forget past information that becomes irrelevant, exactly like in\n156\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nthe LSTM architecture.\nParallelizability: The Achilles’ Heel of Recurrent Neural Networks\nIt is easy to see from the definition of the RNN hidden state (cf. Definition 5.1.3) that, to compute\nht, we have to compute ht1 for all t1 ă t first. Another way to say this is that RNNs are inherently\nsequential models, processing the input string one symbol at a time to update their hidden state\nht, and using this hidden state in turn to compute ht`1. This results in perhaps the biggest\nshortcoming of the architecture for its applicability to real-world language modeling: The inability\nto efficiently parallelize the processing (encoding) of long strings. Let us first consider what we\nmean by the parallelizability of a language model architecture.7 Due to this sequential nature, the\ntraining procedure of RNNs is difficult to parallelize effectively, leading to slower training times.\nThis characteristic poses a significant challenge when modeling long strings, as the computation for\neach element is dependent on the computation of the previous element, leading to a bottleneck in\nthe training process.\nIn short, in our specific use case of language modeling, parallelization refers to the division of the\nprocessing of a specific string across multiple computational nodes, such that any specific node only\nperforms a subset of operations required to process the entire string—the results of the subsets of\nthe operations are then combined to build the representation of the entire string. Importantly, the\ncomputations should be performed independently between nodes in the sense that no node has to\nwait for any other node to provide it the results of its computation. Being able to parallelize large\nmodels across computational nodes has led to some of the biggest advancements in modern deep\nlearning. As such, parallelizability is a crucial feature of any successful deep learning architecture.\nHowever, notice that any dependence between the computations performed by the nodes defeats\nthe purpose of parallelization—if the nodes have to wait for each other to finish computations,\nthe same operations might as well be performed by a single node. This is where recurrent neural\nnetworks fall short: the computations required to encode a string y into the hidden state h|y| will\nalways be sequential, preventing their distribution across different nodes.\nParallelizability in language modeling.\nWhen talking about parallelizing language models, it\nis important to think about which parts can actually be parallelized. In the case of RNNs, we saw\nthat no part of the processing can be (besides the matrix multiplication in a single update rule)—the\nlength of the longest chain of dependent computation will always scale linearly with the length of\nthe string. In the next section, we introduce transformers, a recent neural network architecture first\nintroduced for processing text. One of the big contributions of transformers is their parallelizability\nduring training—it enables their training on extremely large corpora and is thus one of the main\nreasons that they are behind the success of many of the most successful modern large language\nmodels. Parallelizability during training is crucial—notice that parallelization is, in fact, not possible\nduring generation from a locally normalized language model (cf. Definition 2.4.5)—by definition,\nsuch models will generate one symbol at a time. To compute the representation of the new sentence\n(or the new prefix), which is required for the generation of the next symbol, the generated (sampled)\nsymbol has to first be determined, which leaves their generation process inherently sequential. In\nthat respect, RNNs are as parallelizable as they can be during generation. However, the sequential\n7This section provides a very brief and intuitive treatment of parallelization. Our main goal is simply to point out\nthis shortcoming of RNN LMs and with it motivate the next neural architecture we will introduce: transformers.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n157\ncomputation of the hidden states prevents the parallelization of computations of encRpyďtq even if\nthe whole string is already given.\nWe will see that the big parallelizability improvements of other architectures only come into play\nduring training when the model is given the whole string in advance (such that no part of the string\nhas to be sequentially generated) and can compute the (log-)likelihood of the given ground-truth\nnext symbols given the context. That is, during training and given a string y P Σ˚, the model simply\nhas to compute pSM pyt | yătq for all t “ 1, . . . , T—in representation based language models (cf.\nDefinition 3.1.11) this depends on enc pyătq. Crucially, computing pSM pyt | yătq is all that we need\nfor training a language model (in the simplest case that we analyze)—the computed log-likelihood of\nthe ground-truth next character yt is used for computing the loss function during training and used\nfor gradient-based updates to the parameters as discussed in §3.2.3. In the next section, we will see\nhow enc pyătq can be computed without sequential dependencies. However, in the case of RNNs,\nenc pyătq can only be computed sequentially—even if the entire string is known in advance. This\nresults in a crucial bottleneck in training RNNs on large corpora and vastly limits their applicability\nto implementing large language models.\n5.2\nRepresentational Capacity of Recurrent Neural Networks\nRecurrent neural networks are one of the fundamental and most successful neural language model\narchitectures. In this section, we study some theoretical explanations behind their successes as well\nas some of their theoretical limitations. Answering this question is essential whenever we require\nformal guarantees of the correctness of the outputs generated by an LM. For example, one might ask\na language model to solve a mathematical problem based on a textual description (Shridhar et al.,\n2023) or ask it to find an optimal solution to an everyday optimization problem (Lin et al., 2021b).\nIf such problems fall outside the theoretical capabilities of the LM, we have no ground to believe\nthat the result provided by the model is correct. The question also follows a long line of work on\nthe linguistic capabilities of LMs, as LMs must be able to implement mechanisms of recognizing\nspecific syntactic structures to generate grammatical sequences (Talmor et al., 2020; Hewitt and\nManning, 2019; Jawahar et al., 2019; Liu et al., 2019; Icard, 2020a; Manning et al., 2020; Rogers\net al., 2021; Belinkov, 2022; Del’etang et al., 2022, inter alia).\nOne way of quantifying the expressive power of computational models is with the complexity\nof formal languages they can recognize (Del’etang et al., 2022)—we, too, will study the classes of\n(weighted) formal languages (such as the regular languages and the Turing computable languages)\nthey can express. Through this, diverse formal properties of modern LM architectures have been\nshown (e.g., Siegelmann and Sontag, 1992; Hao et al., 2018; Korsky and Berwick, 2019; Merrill,\n2019; Merrill et al., 2020; Hewitt et al., 2020; Merrill et al., 2022a,b, inter alia). Inspecting complex\nmodels such as recurrent neural networks through the lens of formal language theory allows us to\napply the well-studied theoretical results and understanding from the field to the recently more\nsuccessful neural models. While studying neural language models, we will revisit various aspects of\nthe classical language models introduced in Chapter 4—indeed, this was also our main motivation\nfor studying those closely.\nSpecifically, we will focus mainly on the Elman recurrent neural networks due to their simplicity\nand their role as the “fundamental” RNN, capturing their recurrent nature. We will also briefly\ntouch upon the computational power of LSTM networks due to their somewhat different theoretical\nproperties. However, note that most of the results presented in the section generalize to other\narchitectures as well. We begin by investigating Elman RNNs in a practical setting, that is, under\n158\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nrelatively strict and realistic assumptions, such as fixed-point arithmetic. We show that Elman RNNs\nunder such a regime are in fact equivalent to weighted finite-state automata. Next, in the second\nsubsection, we show that under more permissive assumptions of infinite precision and unbounded\ncomputation time, Elman RNNs are Turing complete.\n5.2.1\nRNNs and Weighted Regular Languages\nAnalyzing complex systems with intricate interactions between inputs and parameters and temporal\ndependencies can be tricky. This is a common issue when studying neural networks in general. In\nfact, most, if not all, theoretical frameworks for analyzing neural models such as RNNs rely on\nvarious assumptions about their components to make the analysis feasible. For example, theoretical\nresults on neural networks (for example, optimization guarantees or function/system identifiability\nguarantees) often make the assumption that the activation functions are linear or of some other\neasy-to-analyze form. Similarly, a fruitful manner to analyze the expressivity of recurrent neural\nnetworks specifically is by making (somewhat different) simplifying assumptions on the non-linear\nactivation functions, since those are what often make analysis difficult. A common simplification is\nthe use of the Heaviside activation function.\nDefinition 5.2.1: Heaviside function\nThe Heaviside function is defined as\nHpxq “\n#\n1\nif x ą 0\n0\notherwise\n(5.29)\nIn words, the Heaviside function maps every real value either 0 or 1, depending on whether it is\ngreater than or less than zero. In the following, we will refer to the set t0, 1u as B\ndef\n“ t0, 1u.\nDefinition 5.2.2: Heaviside Elman Network\nA Heaviside Elman network (HRNN) is an Elman network with Heaviside function H as\nthe non-linearity.\nElman network parameters.\nImportantly, note that the parameters of the network do not have\nto be elements of B—we assume those can take arbitrary real (or rational) values. Indeed, networks\nconstrained to parameters θ P B would only be able to recognize unweighted languages. Furthermore,\nfor this section, we expand our definition of a real- or rational-weighted RNN to be able to contain\nweights 8 and ´8. While those are not real (or rational) numbers, we will see they become useful\nwhen we want to explicitly exclude specific sequences from the support of the model, i.e., when we\nwant to assign probability 0 to them.\nBefore we move to the central result of the subsection, we first introduce a fact that makes it\neasier to talk about how an RNN language models can simulate a deterministic PFSAA. We will be\ninterested in conjoining elements of vectors in BD, which can be performed by an Elman RNN with\nappropriately set parameters.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n159\nFact 5.2.1: Performing the AND operation with a neural network\nLet m P rDs, i1, . . . , im P rDs, and x, v P BD with vi “ 1 ti P ti1, . . . , imuu.\nThen,\nH\n`\nvJx ´ pm ´ 1q\n˘\n“ 1 if and only if xik “ 1 for all k “ 1, . . . , m.\nIn other words,\nH\n`\nvJx ´ pm ´ 1q\n˘\n“ xi1 ^ ¨ ¨ ¨ ^ xim.\nThe central result.\nThe central result of this section is captured in the following theorem from\nSvete and Cotterell (2023b).\nTheorem 5.2.1: Equivalence of Heaviside Elman RNNs and WFSAs\nHeaviside Elman RNNs are equivalent to deterministic probabilistic finite-state automata.\nNotice that we only make the claim for probabilistic WFSA. This is without loss of generality, as,\nfrom Theorem 4.1.1, we know we can assume A is locally normalized. We will prove Theorem 5.2.1\nby showing that an RNN with Heaviside activations is at most regular, and then showing how such\nan RNN can in fact simulate any deterministic PFSA. We show each direction as its own lemma.\nLemma 5.2.1\nThe distribution represented by a recurrent neural network with a Heaviside non-linearity H\nis regular.\nProof. Let R “ pΣ, D, U, V, E, bh, h0q be a HRNN defining the conditional probabilities pSM. We\nconstruct a deterministic PFSA A “ pΣ, Q, δ, λ, ρq defining the same string probabilities. Let\ns : BD Ñ Z2D be a bijection. Now, for every state q\ndef\n“ sphq P Q\ndef\n“ BD, construct a transition\nq\ny{w\nÝÝÑ q1 where q1 “ σ pUh ` VJyK ` bhq with the weight w “ pSM py | hq “ f∆|Σ|´1 pE hqy. We\ndefine the initial function as λ psphqq “ 1 th “ h0u and final function ρ with ρ pqq\ndef\n“ pSM peos | spqqq.\nIt is easy to see that A defined this way is deterministic. We now prove that the weights assigned\nto strings by A and R are the same. Let y P Σ˚ with |y| “ T and\nπ “\nˆ\nsph0q\ny1{w1\nÝÝÝÝÑ q1, . . . , qT\nyT {wT\nÝÝÝÝÑ qT `1\n˙\nthe y-labeled path starting in sph0q (such a path exists since we the defined automaton is complete—\nall possible transitions are defined for all states).\nA pyq “λ psph0qq ¨\n« T\nź\nt“1\nwt\nff\n¨ ρ pqT `1q\n“1 ¨\nT\nź\nt“1\npSM\n`\nyt | s´1pqtq\n˘\n¨ pSM\n`\neos | s´1pqT `1q\n˘\n“pLN pyq\nwhich is exactly the weight assigned to y by R. Note that all paths not starting in sph0q have\nweight 0 due to the definition of the initial function.\n■\n160\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nq00\nq01\nq10{\ne\n2e`1\nq11\na{ 1\n2\nb{ 1\n2\na{\n1\ne`1\nb{\ne\ne`1\na{\ne\n2e`1\nb{\n1\n2e`1\na{ 1\n2\nb{ 1\n2\nFigure 5.4: The WFSA corresponding to the RNN defined in Eq. (5.30).\nLet us look at an example of the construction above.\nExample 5.2.1: A PFSA simulating an RNN\nLet R “ pΣ, D, f, E, h0q be a Heaviside RNN sequence model with the parameters\nΣ “ ta, bu\n(5.30)\nD “ 2\n(5.31)\nf pht, yq “ H\nˆˆ\n1\n0\n0\n1\n˙\nht´1 `\nˆ\n1\n0\n0\n1\n˙\nJyK\n˙\n(5.32)\nE “\n¨\n˝\n1\n0\n0\n1\n1\n´8\n˛\n‚\n(5.33)\nh0 “\nˆ\n0\n0\n˙\n(5.34)\nand n paq “ 1, n pbq “ 2, and n peosq “ 3. The automaton corresponding to this RNN contains\nthe states qij corresponding to the hidden states h “\nˆ\ni\nj\n˙\n. It is shown in Fig. 5.4; as we can\nsee, the automaton indeed has an exponential number of useful states in the dimensionality of\nthe hidden state, meaning that the RNN is a very compact way of representing it.\nTo show the other direction of Theorem 5.2.1, we now give a variant of a classic theorem originally\ndue to Minsky (1986) but with a probabilistic twist, allowing us to model weighted languages with\nElman RNNs.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n161\nLemma 5.2.2: Elman RNNs can encode PFSAs\nLet A “ pΣ, Q, δ, λ, ρq be a tight probabilistic deterministic finite-state automaton. Then,\nthere exists a Heaviside-activated Elman network with a hidden state of size h “ |Σ||Q| that\nencodes the same distribution as A.\nWe give proof by construction: Given a deterministic PFSA A “ pΣ, Q, δ, λ, ρq, we construct an\nElman RNN R “ pΣ, D, U, V, E, bh, h0q accepting the same weighted language as A: L pAq “ L pRq\nby defining the elements of the tuple pΣ, D, U, V, E, bh, h0q. In the rest of the section, we will\nfirst intuitively describe the construction, and then formally prove the central results that the\nconstruction relies on. Let n: QˆΣ Ñ Z|Q||Σ| be a bijection, i.e., an ordering of QˆΣ, m: Σ Ñ Z|Σ|\nan ordering of Σ, and m: Σ Ñ Z|Σ| an ordering of Σ; these mappings assign each element in their\npre-image an integer which can then be used to index into matrices and vectors as we will see below.\nWe use n, m, and m to define the one-hot encodings J¨K of state-symbol pairs and of the symbols.\nThat is, we assume that Jq, yKd “ 1 td “ n pq, yqu, and similar for JyK. Similarly to the proof of\nLemma 5.2.1, we denote with h and h´1 the mappings from Q ˆ Σ to the hidden state space of the\nRNN and its inverse. The alphabet of the RNN of course matches the one of the WFSA.\nHRNN’s hidden states.\nThe hidden states of the RNN live in B|Q||Σ|. A hidden state ht encodes\nthe state qt the simulated A is in at time t and the transition symbol yt with which A “arrived” at\nqt as a one-hot encoding of the pair pqt, ytq. Formally,\nht “ Jpqt, ytqK P B|Q||Σ|.\n(5.35)\nThis also means that D “ |Q||Σ|. There is a small caveat: how do we set the incoming symbol\nof A’s (sole) initial state qι (the first time it is entered)? A straightforward solution would be to\naugment the alphabet of the RNN with the bos symbol (cf. §2.4), which we define to be the label\nof the incoming arc denoting the initial state (this would be the only transition labeled with bos).\nHowever, as we show later, the symbol used to arrive into p does not have an effect on the subsequent\ntransitions—it is only needed to determine the target of the current transition. Therefore, we can\nsimply represent the initial state h0 of R with the one-hot encoding of any pair pqι, aq, where qι is\nthe initial state of the WFSA and a P Σ.\nFor example, for the fragment of a WFSA in Fig. 5.5, the hidden state encoding the current\nstate q and the incoming arc b is of the form presented in Eq. (5.36).\nr\nq\nq1\nq2\nb{˝\na{˝\nb{˝\nFigure 5.5: A fragment of a WFSA.\nht “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‹‹‹‚\n0\n...\n0\n1\nÐ n pq, bq\n0\n...\n0\n(5.36)\n162\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nq\nq1\nq2\na{w0\na{w1\nb{w2\nh “ Jq, aK\nq\nq1\nq2\na{w0\na{w1\nb{w2\nUh“Jq1,aK`Jq2,bK\nq\nq1\nq2\na{w0\na{w1\nb{w2\nVJaK“Jq,aK`Jq1,aK\nq\nq1\nq2\na{w0\na{w1\nb{w2\nh1 “ Jq1, aK\nOut-neighborhood\na-reachable\nf∆|Σ|´1 pEh1q “\n¨\n˝\na: w1\nb: w2\neos: 0\n˛\n‚\nFigure 5.6: A high-level illustration of how the transition function of the FSA is simulated in\nMinsky’s construction on a fragment of an FSA starting at q (encoded in h) and reading the symbol\na. The top path disjoins the representations of the states in the out-neighborhood of q, whereas\nthe bottom path disjoins the representations of states reachable by an a-transition. The Heaviside\nactivation conjoins these two representations into h1 (rightmost fragment). Projecting Eh1 results in\nthe vector defining the same probability distribution as the outcoming arcs of q (green box).\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n163\nEncoding the transition function.\nThe idea of defining U, V, and bh is for the Elman update\nrule to perform, upon reading yt`1, element-wise conjunction between the representations of the\nout-neighborhood of qt and the representation of the states A can transition into after reading in\nyt`1 from any state. The former is encoded in the recurrence matrix U, which has access to the\ncurrent hidden state that encodes qt while the latter is encoded in the input matrix V, which has\naccess to the one-hot representation of yt`1. Conjugating the entries in those two representations\nwill, due to the determinism of A, result in a single non-zero entry: one representing the state which\ncan be reached from qt (1st component) using the symbol yt`1 (2nd component); see Fig. 5.6.\nThe recurrence matrix U lives in B|Σ||Q|ˆ|Σ||Q|. The main idea of the construction is for each\ncolumn U: ,npq,yq of the matrix to represent the “out-neighborhood” of the state q in the sense that\nthe column contains 1’s at the indices corresponding to the state-symbol pairs pq1, y1q such that A\ntransitions from q to q1 after reading in the symbol y1. That is, for q, q1 P Q and y, y1 P Σ, we define\nUnpq1,y1q,npq,yq\ndef\n“ 1\n\"\nqt\ny1{˝\nÝÝÑ q1 P δ\n*\n.\n(5.37)\nSince y is free, each column is repeated |Σ|-times: once for every y P Σ—this is why, after entering\nthe next state, the symbol used to enter it does not matter anymore and, in the case of the initial\nstate, any incoming symbol can be chosen to represent h0.\nFor example, for the fragment of a WFSA in Fig. 5.5, the recurrence matrix would take the form\nU “\nn pq, bq\nÓ\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pq1, aq\n¨ ¨ ¨\n...\n¨ ¨ ¨\n1\nÐ n pq2, bq\n...\n0\n(5.38)\nand the matrix-vector product Uht with ht from before results in\nUht “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pq1, aq\n...\n1\nÐ n pq2, bq\n...\n0\n(5.39)\n164\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nThe input matrix V lives in B|Σ||Q|ˆ|Σ| and encodes the information about which states can be\nreached by which symbols (from any state in A). The non-zero entries in the column corresponding\nto y1 P Σ correspond to the state-symbol pairs pq1, y1q such that q1 is reachable with y1 from some\nstate:\nVnpq1,y1q,mpy1q\ndef\n“ 1\n\"\n˝\ny1{˝\nÝÝÑ q1 P δ\n*\n.\n(5.40)\nFor example, for the fragment of a WFSA in Fig. 5.7a, the input matrix would take the form\nV “\nm pbq\nÓ\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pp, bq\n¨ ¨ ¨\n...\n¨ ¨ ¨\n1\nÐ n pq2, bq\n...\n0\n(5.41)\nand the matrix-vector product Vempaq and Vempbq would take the form (see also Fig. 5.7b)\nVempaq “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pq1, aq\n...\n0\nVempbq “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pp, bq\n...\n1\nÐ n pq2, bq\n...\n0\n(5.42)\nLastly, we define the bias as bh\ndef\n“ ´1 P R|Q||Σ|, which allows the Heaviside function to perform the\nneeded conjunction.\nTo put these components together, consider that, at each step of the computation, R computes\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n165\nr\np\nq1\nq2\nb{˝\na{˝\nb{˝\n(a) An example of a fragment of a WFSA.\nr\np\nq1\nq2\nb{˝\na{˝\nb{˝\n(b) An example of a fragment of a WFSA.\nht`1 “ H pUht ` Vea ` bhq where yt`1 “ a. The input to the non-linearity is computed as follows:\nUht ` Vempaq ` bh “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pq1, aq\n...\n1\nÐ n pq2, bq\n...\n0\n`\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‚\n0\n...\n1\nÐ n pq1, aq\n...\n0\n`\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‚\n´1\n...\n´1\n...\n´1\n(5.43)\nThe following lemma proves that the construction described correctly implements the transition\nfunction of the PFSA.\nLemma 5.2.3\nLet A “ pΣ, Q, δ, λ, ρq be a deterministic PFSA, y “ y1 . . . yT P Σ˚, and qt the state arrived at\nby A upon reading the prefix yďt. Let R be the HRNN specified by the Minsky construction\nfor A, n the ordering defining the one-hot representations of state-symbol pairs by R, and ht\nR’s hidden state after reading yďt. Then, it holds that h0 “ Jpqι, yqK where qι is the initial\nstate of A and y P Σ and hT “ JpqT , yT qK.\nProof. Define sph “ Jpq, yqKq\ndef\n“ q. We can then restate the lemma as sphT q “ qT for all y P Σ˚,\n|y| “ T. Let π be the y-labeled path in A. We prove the lemma by induction on the string length\nT.\nBase case: T “ 0.\nHolds by the construction of h0.\nInductive step: T ą 0.\nLet y P Σ˚ with |y| “ T and assume that sphT ´1q “ qT ´1.\nWe prove that the specifications of U, V, and bh ensure that sphT q “ qT . By definition\nof the recurrence matrix U (cf. Eq. (5.37)), the vector UhT ´1 will contain a 1 at the entries\n166\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nn pq1, y1q for q1 P Q and y1 P Σ such that qT\ny1{˝\nÝÝÑ q1 P δ. This can equivalently be written as\nUhT ´1 “ Ž\nqT\ny1{˝\nÝÝÑq1Pδ Jpq1, y1qK, where the disjunction is applied element-wise.\nOn the other hand, by definition of the input matrix V (cf. Eq. (5.40)), the vector VJyT K will\ncontain a 1 at the entries n pq1, yT q for q1 P Q such that ˝\nyT {˝\nÝÝÝÑ q1 P δ. This can also be written as\nVJyT K “ Ž\n˝\nyT {˝\nÝÝÝÑq1Pδ Jpq1, yT qK.\nBy Fact 5.2.1, H pUhT ´1 ` VJyT K ` bhqnpq1,y1q “ H pUhT ´1 ` VJyT K ´ 1qnpq1,y1q “ 1 holds if\nand only if pUhT ´1qnpq1,y1q “ 1 and pVJyT Kqnpq1,y1q “ 1. This happens if\nqT\ny1{˝\nÝÝÑ q1 P δ and ˝\nyT {˝\nÝÝÝÑ q1 P δ ðñ qT\nyT {˝\nÝÝÝÑ q1,\n(5.44)\ni.e., if and only if A transitions from qT to qT upon reading yT (it transitions only to qT due to\ndeterminism).\nSince the string y was arbitrary, this finishes the proof.\n■\nEncoding the transition probabilities.\nWe now turn to the second part of the construction:\nencoding the string acceptance weights given by A into the probability distribution defined by R.\nWe present two ways of doing that: using the more standard softmax formulation, where we make\nuse of the extended real numbers, and with the sparsemax.\nThe conditional probabilities assigned by R are controlled by the |Σ|ˆ|Q||Σ|-dimensional output\nmatrix E. Since ht is a one-hot encoding of the state-symbol pair qt, yt, the matrix-vector product\nEht simply looks up the values in the n pqt, ytqth column. After being projected to ∆|Σ|´1, the\nentry in the projected vector corresponding to some yt`1 P Σ should match the probability of that\nsymbol given that A is in the state qt. This is easy to achieve by simply encoding the weights of the\noutgoing transitions into the n pqt, ytqth column, depending on the projection function used. This is\nespecially simple in the case of the sparsemax formulation. By definition, in a PFSA, the weights of\nthe outgoing transitions and the final weight of a state qt form a probability distribution over Σ for\nevery qt P Q. Projecting those values to the probability simplex, therefore, leaves them intact. We\ncan therefore define\nEmpy1qnpq,yq\ndef\n“\n#\nωpq\ny1{w\nÝÝÝÑ ˝q\n| if y1 P Σ\nρ pqq\n| otherwise\n.\n(5.45)\nProjecting the resulting vector Eht, therefore, results in a vector whose entries represent the\ntransition probabilities of the symbols in Σ.\nIn the more standard softmax formulation, we proceed similarly but log the non-zero transition\nweights. Defining log 0\ndef\n“ ´8,8 we set\nEmpy1qnpq,yq\ndef\n“\n#\nlog ωpq\ny1{w\nÝÝÝÑ ˝q\n| if y1 P Σ\nlog ρ pqq\n| otherwise\n.\n(5.46)\nIt is easy to see that the entries of the vector softmaxpEhtq form the same probability distribution as\nthe original outgoing transitions out of q. Over the course of an entire input string, these weights are\n8Note that the ´8 entries are only needed whenever the original WFSA assigns 0 probability to some transitions.\nIn many implementations using softmax-activated probabilities, this would not be required.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n167\nr\np\nq1\nq2\nb{˝\na{w1\nb{w2\nFigure 5.8: An example of a fragment of a WFSA.\nmultiplied as the RNN transitions between different hidden states corresponding to the transitions\nin the original PFSA A.\nFor example, for the fragment of a WFSA in Fig. 5.8, the output matrix would take the form\nE “\nn pq, bq\nÓ\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‚\n´8\n...\nlog w1\nÐ m paq\n¨ ¨ ¨\n...\n¨ ¨ ¨\nlog w2\nÐ m pbq\n...\n´8\n(5.47)\nThis means that, if ht encodes the state-symbol pair pq, yq, the vector Eht will copy the\nselected column in E which contains the output weight for all out symbols y; of q, i.e., the entry\nEhmpy1q contains the weight on the arc q\ny1{w\nÝÝÝÑ ˝. Over the course of an entire input string y,\nthese probabilities are simply multiplied as the RNN transitions between different hidden states\ncorresponding to the transitions in the original WFSA A.\nFor example, for the fragment of a WFSA in Fig. 5.8, the matrix-vector product Eht would take\nthe form\nEht “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n˛\n‹‹‹‹‹‹‹‹‹‹‚\n´8\n...\nlog w1\nÐ m paq\n...\nlog w2\nÐ m pbq\n...\n´8\n(5.48)\nThe equivalence of the produced RNN LM to the PFSA is shown in the following lemma.\n168\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nq0\n1\nq1\nq2{0.5\na{0.1\nb{0.9\na{0.5\nb{0.5\nb{0.5\nFigure 5.9: The WFSA A.\nLemma 5.2.4\nLet A “ pΣ, Q, δ, λ, ρq be a deterministic PFSA, y “ y1 . . . yT P Σ˚, and qt the state arrived at\nby A upon reading the prefix yďt. Let R be the HRNN specified by the Minsky construction\nfor A, E the output matrix specified by the generalized Minsky construction, n the ordering\ndefining the one-hot representations of state-symbol pairs by R, and ht R’s hidden state after\nreading yďt. Then, it holds that pLN pyq “ A pyq.\nProof. Let y P Σ˚, |y| “ T and let π be the y-labeled path in A. Again, let p pyq\ndef\n“ ś|y|\nt“1 pSM pyt | yătq.\nWe prove p pyq “ śT\nt“1 wt by induction on T.\nBase case: T “ 0.\nIn this case, y “ ε, i.e., the empty string, and A pεq “ 1. R computes\np pεq “ ś0\nt“1 pSM pyt | yătq “ 1.\nInductive step: T ą 0.\nAssume that the p py1 . . . yT ´1q “ śT ´1\nt“1 wt. By Lemma 5.2.3, we know\nthat sphT ´1q “ qT and sphT q “ qT . By the definition of E for the specific f∆|Σ|´1, it holds that\nf∆|Σ|´1 pEhT ´1qmpyq “ ωpsphT ´1q\ny{wT\nÝÝÝÑ sphT qq “ wT . This means that p pyďT q “ śT\nt“1 wt, which\nis what we wanted to prove.\nClearly, pLN pyq “ p pyq pSM peos | yq. By the definition of E (cf. Eq. (5.45)), pEhT qmpeosq “\nρ psphT qq, meaning that\npLN pyq “ p pyq pSM peos | yq “\nT\nź\nt“1\nwtρ psphT qq “ A pyq .\nSince y P Σ˚ was arbitrary, this finishes the proof.\n■\nWe now walk through an example of the Minsky construction.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n169\nExample 5.2.2: Minsky construction\nLet A “ pΣ, Q, δ, λ, ρq be a WFSA as shown in Fig. 5.9.\nSince A has |Q| “ 3 states\nand an alphabet of |Σ| “ 2 symbols, the hidden state of the representing RNN R will\nbe of dimensionality 3 ¨ 2 “ 6. Assume that the set of state-symbol pairs is ordered as\npq0, aq , pq0, bq , pq1, aq , pq1, bq , pq2, aq , pq2, bq. The initial state can be represented (choosing a\nas the arbitrary “incoming symbol”) as\nh0 “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n1\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\n.\n(5.49)\nThe recurrent matrix U of R is\nU “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n˛\n‹‹‹‹‹‹‚\n,\n(5.50)\nthe input matrix V\nV “\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n˛\n‹‹‹‹‹‹‚\n,\n(5.51)\nand the output matrix E is\nE “\n¨\n˝\nlog p0.1q\nlog p0.1q\nlog p0.5q\nlog p0.5q\n´8\n´8\nlog p0.9q\nlog p0.9q\nlog p0.5q\nlog p0.5q\nlog p0.5q\nlog p0.5q\n´8\n´8\n´8\n´8\nlog p0.5q\nlog p0.5q\n˛\n‚,\n(5.52)\nwhere the last row corresponds to the symbol eos. The target of the b-labeled transition from\n170\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nq0 (q0\nb{0.9\nÝÝÝÑ q2) is computed as follows:\nh1 “ H pUh0 ` VJbK ` bhq\n“ H\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n˛\n‹‹‹‹‹‹‚\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n1\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\n`\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n˛\n‹‹‹‹‹‹‚\nˆ\n0\n1\n˙\n`\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n´1\n´1\n´1\n´1\n´1\n´1\n˛\n‹‹‹‹‹‹‚\n˛\n‹‹‹‹‹‹‚\n“ H\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n1\n0\n0\n1\n˛\n‹‹‹‹‹‹‚\n`\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n0\n0\n0\n1\n˛\n‹‹‹‹‹‹‚\n`\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n´1\n´1\n´1\n´1\n´1\n´1\n˛\n‹‹‹‹‹‹‚\n˛\n‹‹‹‹‹‹‚\n“ H\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n´1\n´1\n0\n´1\n´1\n1\n˛\n‹‹‹‹‹‹‚\n˛\n‹‹‹‹‹‹‚\n“\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n0\n0\n0\n1\n˛\n‹‹‹‹‹‹‚\n,\nwhich corresponds exactly the configuration in which A is in state q2 which it arrived to by\nreading in the symbol b.\nThe probability of the string y “ b under the locally-normalized model induced by R can be\ncomputed as\npLN pyq “ pLN pbq “ pSM pb | bosq pSM peos | bq “ pSM pb | h0q pSM peos | h1q\n“ softmax pEh0qb softmax pEh1qeos\n“ softmax\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n¨\n˝\nlog p0.1q\n¨ ¨ ¨\n´8\nlog p0.9q\n¨ ¨ ¨\nlog p0.5q\n´8\n¨ ¨ ¨\nlog p0.5q\n˛\n‚\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n1\n0\n0\n0\n0\n0\n˛\n‹‹‹‹‹‹‚\n˛\n‹‹‹‹‹‹‚\nb\n¨\nsoftmax\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n¨\n˝\nlog p0.1q\n¨ ¨ ¨\n´8\nlog p0.9q\n¨ ¨ ¨\nlog p0.5q\n´8\n¨ ¨ ¨\nlog p0.5q\n˛\n‚\n¨\n˚\n˚\n˚\n˚\n˚\n˚\n˝\n0\n0\n0\n0\n0\n1\n˛\n‹‹‹‹‹‹‚\n˛\n‹‹‹‹‹‹‚\neos\n“ softmax\n¨\n˝\nlog p0.1q\nlog p0.9q\n´8\n˛\n‚\nb\n¨ softmax\n¨\n˝\n´8\nlog p0.5q\nlog p0.5q\n˛\n‚\neos\n“ 0.9 ¨ 0.5 “ 0.45.\nImplications for recurrent neural language models.\nLemmas 5.2.1 and 5.2.2 formalize the\nequivalence between HRNNs and deterministic PFSAs. A direct corollary of this result is that\nHRNNs are at most as expressive as deterministic PFSAs and, therefore, strictly less expressive as\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n171\nq0{1\nq1\nq2\nq3{1\na{0.5\na{0.5\nb{0.9\nb{0.1\nc{0.1\nc{0.9\nFigure 5.10: A non-determinizable PFSA. It assigns the string abnc the probability A pabncq “\n0.5 ¨ 0.9n ¨ 0.1 ` 0.5 ¨ 0.1n ¨ 0.9, which can not be expressed as a single term for arbitrary n P Ně0.\ngeneral, non-deterministic, PFSAs.9 An example of a very simple non-deterministic PFSA, i.e., a\nPFSA whose distribution cannot be expressed by an HRNN LM, is shown in Fig. 5.10. Furthermore,\neven if a non-deterministic PFSA can be determinized, the number of states of the determinized\nmachine can be exponential in the size of the non-deterministic one (Buchsbaum et al., 2000).\nIn this sense, non-deterministic PFSAs can be seen as exponentially compressed representations\nof finite-state LMs. However, the compactness of this non-deterministic representation must be\n“undone” using determinization before it can be encoded by an HRNN.\nWhile Lemma 5.2.1 focuses on HRNN LMs and shows that they are finite-state, a similar\nargument could be made for any RNN whose activation functions map onto a finite set. This is\nthe case with any implementation of an RNN on a computer with finite-precision arithmetic—in\nthat sense, all deployed RNNLMs are finite-state, albeit very large in the sense of encoding possibly\nvery large weighted finite-state automata. However, there are a few important caveats with this:\nfirstly, notice that, although finite, the number of states represented by an RNN is exponential in\nthe size of the hidden state. Even for moderate hidden state dimensionalities, this can be very large\n(hidden states can easily be of size 100–1000). In other words, one can view RNNs as very compact\nrepresentations of large deterministic probabilistic finite-state automata whose transition functions\nare represented by the RNN’s update function. Furthermore, since the topology of this implicit\nWFSA is completely determined by the update function of the RNN, it can be learned very flexibly\nyet efficiently based on the training data—this is made possible by the sharing of parameters across\nthe entire graph of the WFSA instead of explicitly parametrizing every possible transition, as, for\nexample, in §4.1.3, or hard-coding the allowed transitions as in §4.1.5. This means that the WFSA\nis not only represented, but also parametrized very efficiently by an RNN. Nevertheless, there is an\nimportant detail that we have somewhat neglected so far: this is the requirement that the simulated\nWFSA be deterministic.\n9General PFSAs are, in turn, equivalent to probabilistic regular grammars and discrete Hidden Markov Models\n(Icard, 2020b).\n172\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n5.2.2\nAddendum to Minsky’s Construction: Lower Bounds on the Space\nComplexity of Simulating PFSAs with RNNs\nLemma 5.2.2 shows that HRNN LMs are at least as powerful as dPFSAs. More precisely, it shows\nthat any dPFSA A “ pΣ, Q, δ, λ, ρq can be simulated by an HRNN LM of size O p|Q||Σ|q. In this\nsection, we address the following question: How large does an HRNN LM have to be such that it\ncan correctly simulate a dPFSA? We study the asymptotic bounds with respect to the size of the\nset of states, |Q|, as well as the number of symbols, |Σ|.10\nAsymptotic Bounds in |Q|.\nIntuitively, the 2D configurations of a D-dimensional HRNN hidden\nstate could represent 2D states of a (P)FSA. One could therefore expect that we could achieve\nexponential compression of a dPFSA by representing it as an HRNN LM. Interestingly, this is not\npossible in general: extending work by Dewdney (1977), Indyk (1995) shows that, to represent an\nunweighted FSA with an HRNN, one requires an HRNN of size Ω\n´\n|Σ|\na\n|Q|\n¯\n. This lower bound can\nbe achieved. For completeness, we present constructions by Dewdney (1977); Indyk (1995), which\nrepresent an unweighted FSA with a HRNN of size O\n´\n|Σ||Q|\n3\n4\n¯\nand O\n´\n|Σ|\na\n|Q|\n¯\n, respectively,\nnext, before giving a lower bound in for the probabilistic case.\nLemma 5.2.2 gives a relatively simple construction of an RNN recognizing a weighted regular\nlanguage. However, the resulting RNN is relatively large, with a hidden state of size linear in\nthe number of states of the (deterministic) WFSA recognizing the language, with the additional\nmultiplicative factor in the size of the alphabet. Note that constructions resulting in smaller RNNs\nexist, at least for the unweighted case. For example, for an arbitrary WFSA A “ pΣ, Q, δ, λ, ρq,\nDewdney (1977); Alon et al. (1991) present a construction of an RNN with a hidden state of size\nO\n´\n|Q|\n3\n4\n¯\nsimulating A, whereas Indyk (1995) provides a construction of an RNN with a hidden\nstate of size O\n´\n|Q|\n1\n2\n¯\n. The latter is also provably a lower bound on the number of neurons required\nto represent an arbitrary unweighted FSA with a Heaviside-activated recurrent neural network\n(Indyk, 1995). It is not yet clear if this can be generalized to the weighted case or if Minsky’s\nconstruction is indeed optimal in this setting. This is quite interesting since one would expect that\nan RNN with a hidden state of size D can represent up to 2D individual states (configurations of the\nD-dimensional vector). However, the form of the transition function with the linear transformation\nfollowed by a Heaviside activation limits the number of transition functions that can be represented\nusing D dimensions, resulting in the required exponential increase in the size of the hidden state.\nMinsky’s construction (Lemma 5.2.2) describes how to represent a dPFSA A with a HRNN of\nsize linear in the number of A’s states. Importantly, the encoding of the FSA transition function\n(taken from Minsky’s original construction) is decoupled from the parameter defining the probability\ndistribution, E. This section describes two asymptotically more space-efficient ways of constructing\nthe component simulating the transition function. They originate in the work by Dewdney (1977),\nwho showed that an unweighted FSA A “ pΣ, Q, I, F, δq can be represented by an HRNN of size\nO\n´\n|Σ||Q|\n3\n4\n¯\n. Using the same ideas, but a specific trick to compress the size of the processing\nlayer of the RNN further, Indyk (1995) reduced this bound to O\n´\n|Σ|\na\n|Q|\n¯\n, which, as discussed\nin §5.2.3, is asymptotically optimal. Naturally, as shown in §5.2.3, the space-efficiency gain can\nnot be carried over to the weighted case—that is, the space-efficiency is asymptotically overtaken\n10This section is based on the survey by Svete and Cotterell (2023a).\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n173\nby the output matrix E. Nevertheless, for a more complete treatment of the subject, we cover the\ntwo compressed constructions of the HRNN simulating an unweighted FSA in this section in our\nnotation. Importantly, given a dPFSA, we focus only on the underlying FSA, i.e., the unweighted\ntransition function of the automaton, since by Theorem 5.2.4, the compression can only be achieved\nwith components representing that part of the automaton.\nDewdney’s Construction\nThis section describes the construction due to Dewdney (1977) in our notation. Since some of the\nparts are very similar to the construction due to Indyk (1995), those parts are reused in §5.2.2 and\nintroduced more generally.\nRepresenting states of the FSA.\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. Recall that\nMinsky’s construction encodes the A’s current state as a one-hot encoding of the state-symbol pair.\nThe construction due to Dewdney (1977), on the other hand, represents the states separately from\nthe symbols. It encodes the states with two-hot representations by using the coefficients of what we\ncall a square-root state representation. This results in representations of states of size O\n´a\n|Q|\n¯\n.\nThe input symbols are incorporated into the hidden state separately.11\nDefinition 5.2.3: Square-root state representation\nLet A “ pΣ, Q, I, F, δq be an FSA and s\ndef\n“ r\na\n|Q|s.\nWe define the square-root state\nrepresentation of A’s states q P Q asa\nϕ2 pqq\ndef\n“\n´\ntq\nsu, q\nmod s\n¯\n.\n(5.53)\nWe denote the inverse of ϕ2 with ϕ´1\n2\nand further define for k P Zs\nϕ´1\n2\npk, ¨q\ndef\n“ tq P Q | φ0 “ k where φ “ ϕ2 pqqu\n(5.54)\nand ϕ´1\n2\np¨, kq analogously.\naNotice that ϕ2 pqq represents the coefficients of the expression of q P N in base s.\nSpecifically, we will denote ϕ´1\n2\npk, ¨q and ϕ´1\n2\np¨, kq with k in the jth position (with j P Z2, 0 for\nϕ´1\n2\npk, ¨q and 1 for ϕ´1\n2\np¨, kq) as Φk,j.\nWe can think of the function ϕ2 as representing states of the FSA in a two-dimensional space\nZs ˆ Zs. However, to efficiently simulate A with an HRNN, it is helpful to think of ϕ2 pqq in two\ndifferent ways: as a vector v P Ně0\n2|Q|, or as a matrix in B|Q|ˆ|Q| in the following sense.\nDefinition 5.2.4: Vector and matrix state representations\nGiven a square-root state representation function ϕ2, we define the vector representation\n11This again adds a factor |Σ| to the size of the hidden state, as we discuss later.\n174\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nof the state q P Q as the vector v pqq P B2|Q| with\nv pqqφ0 “ 1\n(5.55)\nv pqqs`φ1 “ 1,\n(5.56)\nwhere φ “ pφ0, φ1q “ ϕ2 pqq, and all other entries 0. Furthermore, we define the matrix\nrepresentation of the state q P Q as the matrix B P B|Q|ˆ|Q| with\nWQqφ0φ1 “ 1\n(5.57)\nand all other entries 0.\nDewdney’s construction also heavily relies on the representations of sets of states. We define\nthose additively.\nDefinition 5.2.5: Matrix and vector representation of state sets\nLet Q Ď Q be a set of states. We define the vector representation of Q as the vector\nv pQq\ndef\n“\nł\nqPQ\nv pqq.\n(5.58)\nSimilarly, we define the matrix representation of Q as the matrix\nWQQ\ndef\n“\nł\nqPQ\nWQq.\n(5.59)\nTo help understand the above definitions, we give an example of an FSA and the representations\nof its states.\nExample 5.2.3: Dewdney’s construction\nConsider the FSA in Fig. 5.11, for which s “ r\na\n|Q|s “ r\n?\n3s “ 2, meaning that\nϕ2 p0q “ p0, 0q\nϕ2 p1q “ p0, 1q\nϕ2 p2q “ p1, 0q ,\n(5.60)\nresulting in the state-to-vector mappinga\nv p0q “\n`\n1\n0\n|\n1\n0\n˘\n(5.61)\nv p1q “\n`\n1\n0\n|\n0\n1\n˘\n(5.62)\nv p2q “\n`\n0\n1\n|\n1\n0\n˘\n,\n(5.63)\nand the state-to-matrix mapping\nWQ0 “\nˆ\n1\n0\n0\n0\n˙\nWQ1 “\nˆ\n0\n1\n0\n0\n˙\nWQ2 “\nˆ\n0\n0\n1\n0\n˙\n.\n(5.64)\nThe two components of the vector representations separated by “|” denote the two halves of\nthe representation vectors, corresponding to the two components of ϕ2 pqq.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n175\n0\n1\n2\na\nb\na\nb\nb\nFigure 5.11: An example of a fragment of an FSA.\naDespite the notation p. . . | . . .q, we assume we are working with column vectors.\nHigh-level idea of Dewdney’s construction.\nGiven these definitions, the intuition behind\nDewdney’s construction of an HRNN simulating an FSA A is the following:\n1. Represent A’s states as vectors in B2s, or, equivalently, matrices in Bsˆs.\n2. For each q P Q, construct the matrix representation of the set of y-predecessors WQPred pq; yq\nfor all y P Σ.\n3. To simulate A’s transition function δ, compare the representation of the current state qt\nwith all constructed predecessor matrices WQPred pq; ytq given the current input symbol yt.\nActivate the two-hot representation of the (unique) state qt`1 for which the representation of\nqt was detected in qt`1’s predecessor matrix for symbol yt, WQPred pqt`1; ytq.\nSimulating the transition function of an FSA by detecting preceding states.\nWe elaborate\non the last point above since it is the central part of the construction.12 The idea of simulating the\ntransition function δ is reduced to detecting whose predecessor given the current input symbol yt is\ncurrently active—naturally, this should be the state active at t ` 1. Concretely, consider again the\nFSA A in Fig. 5.11. The predecessors of the three states, indexed by the incoming symbols are: for\n0 tb : 2u, for 1 ta : 1, b : 0u, and for 2 ta : 1, b : 0u. Suppose that at some time t, A is in state 0 and\nis reading in the symbol b. Then, since the state 0 is the b-predecessor of the state 2, we know that\nat time t ` 1, A will be in state 2. This principle can be applied more generally: to determine the\nstate of an FSA at time t ` 1, we simply have to somehow detect whose predecessor is active at\ntime t given the current input symbol at time t.\nThe crux of Dewdney’s construction is then the following:13 How do we, using only the Elman\nupdate rule, determine whose yt-predecessor is active at time t? This can be done by detecting\nwhich predecessor matrix WQPred pq; ytq the representation of the current state qt is included in\nin the sense that if ϕ2 pqtq “ φ, it holds that WQPred pq; ytqφ0φ1 “ 1. To be able to formally talk\nabout the detection of a representation in a set of predecessors, we define several notions of matrix\ndetection.\nInformally, we say that a matrix is easily detectable if the presence of its non-zero elements can\nbe detected using a single neuron in the hidden layer of a HRNN.\n12Later, we will see that Indyk (1995) uses the exact same idea for simulating δ.\n13Again, the same applies to Indyk (1995).\n176\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nDefinition 5.2.6: Easily detectable matrices\nLet B P BDˆD be a binary matrix. We say that B is easily detectable if there exist w P Q2D\nand b P Q (neuron coefficients) such that\nσ pxeij, wy ` bq “ 1 ðñ Bij “ 1,\n(5.65)\nwhere eij “\n`\nei\n|\nej\n˘\nrefers to the 2D-dimensional vector with 1’s at positions i and D ` j.\nIn words, this means that the neuron defined by w, b fires on the input eij if and only if\nBij “ 1.\nWe define detectable matrices as the matrices which can be detected using a conjunction of two\nneurons.\nDefinition 5.2.7: Detectable matrices\nLet B P BDˆD be a binary matrix. We say that B is detectable if there exist w1, w2 P Q2D\nand b1, b2 P Q such that\nσ pxeij, w1y ` b1q “ 1 ^ σ pxeij, w2y ` b2q “ 1 ðñ Bij “ 1.\n(5.66)\nFurthermore, we say that a matrix is (easily) permutation-detectable if there exist permutation\nmatrices P and Q such that PBQ is (easily) detectable.\nIntuitively, this means that one can effectively replace an easily detectable matrix B with a\nsingle neuron: instead of specifying the matrix explicitly, one can simply detect if an entry Bij of B\nis 1 by passing eij through the neuron and seeing if it fires. This reduces the space complexity from\nD2 to 2D. Similarly, one can replace a detectable matrix with two neurons. As shown in Fact 5.2.1,\nthe required conjunction of the two resulting neurons can then easily be performed by a third (small)\nneuron, meaning that a detectable matrix is effectively represented by a two-layer MLP.\nAn example of easily detectable matrices are the so-called northwestern matrices.\nDefinition 5.2.8: Northwestern matrix\nA matrix B P BDˆD is northwestern if there exists a vector α with |α| “ D and D ě α1 ě\n. . . ě αD ě 0 such that\nBij “ 1 ðñ j ď αi.\n(5.67)\nIntuitively, northwestern matrices contain all their ones contiguously in their upper left (northwest)\ncorner. An example of a northwestern matrix for α “\n`\n2\n1\n1\n˘\nis\nB “\n¨\n˝\n1\n1\n0\n1\n0\n0\n1\n0\n0\n˛\n‚.\n(5.68)\nLemma 5.2.5\nNorthwestern matrices are easily detectable.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n177\nProof. Define\nw\ndef\n“\n`\nα\n|\nD\n. . .\n1\n˘\nand b “ ´D. It is easy to see that for any eij where Bij “ 1, it holds that\nxeij, wy “ αi ` pD ´ j ` 1q ě j ` D ´ j ` 1 “ D ` 1\nùñ H pxeij, wy ` bq “ H pxeij, wy ´ Dq “ 1.\nOn the other hand, for Bij “ 0, we have\nxeij, wy “ αi ` pD ´ j ` 1q ă j ` D ´ j ` 1 “ D\nùñ H pxeij, wy ` bq “ H pxeij, wy ´ Dq “ 0.\n■\nA more general useful class of detectable matrices are line matrices (Dewdney, 1977).\nDefinition 5.2.9: Line matrix\nA binary matrix B P BDˆD is a line matrix if any of the following conditions hold:\n1. All B’s ones lie either in the same row (B is a row matrix) or in the same column (B\nis a column matrix).\n2. B is a transversal, i.e., a matrix in which there is at most one 1 in any column and row.\nLemma 5.2.6\nRow and column matrices are easily permutation-detectable.\nProof. Let i, N P ZD and B be a row matrix with Bijn “ 1 for n P ZN, i.e., a row matrix with\nall its ones in the ith row. Define P P BDˆD as P1i “ 1 and 0 elsewhere and Q P BDˆD with\nQjnn “ 1 and 0 elsewhere. Then, PBQ contains all its 1 in its northwestern corner (contiguously\nin the first row) and is thus easily detectable. Let w\ndef\n“\n`\nα\n|\nD\n. . .\n1\n˘\n, b “ D be the neuron\nweights from Lemma 5.2.5. Define w1 def\n“\n`\nPJα\n|\nQpD\n. . .\n1q\n˘\n, b1 “ D. It is easy to see that\nthis “rearranges” the components of the neuron recognizing the northwestern matrix PBQ to make\nthem recognize the original matrix, meaning that the neuron defined by w1 and b1 recognizes the\nline matrix. The proof for a column matrix is analogous.\n■\nLemma 5.2.7\nTransversals are permutation-detectable.\nProof. The core idea of this proof is that every transversal can be permuted into a diagonal matrix,\nwhich can be written as a Hadamard product of a lower-triangular and an upper-triangular matrix.\nLet B be a transversal. Pre-multiplying B with its transpose P\ndef\n“ BJ results in a diagonal matrix.\nIt is easy to see that PB can be written as a Hadamard product H1 bH2 of a lower-triangular matrix\nH1 and an upper-triangular matrix H2. Both are easily permutation detectable. A conjunction of\nthe neurons detecting H1 and H2 (again, performed by another neuron) detects the original matrix\nB. In the following, we will refer to H1 and H2 as the factors of the transversal.\n■\n178\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nCrucially, any binary matrix B P BDˆD can be decomposed into a set of line matrices B whose\ndisjunction is B: Ž\nMPB M “ B. It is easy to see that Bij “ 1 if and only if there exists M P B\nsuch that Mij “ 1. This means that non-zero entries of any B P BDˆD decomposed into the set of\nline matrices B can be detected using an MLP in two steps:\n1. Detect the non-zero entries of the individual line matrices from the decomposition B (which\nare, as shown above, detectable).\n2. Take a disjunction of the detections of the individual line matrices to result in the activation\nof the original matrix.\nThe disjunction can again be performed by applying another 2-layer MLP to the activations of the\nline matrices. An important consideration in both Dewdney’s as well as Indyk’s construction later\nwill be how large B has to be.\nUsing matrix decomposition and detection for simulating the transition function.\nWe\nnow describe how Dewdney’s construction uses matrix detection based on the decomposition of\nmatrices into line matrices to simulate an FSA using an HRNN. From a high level, the update\nsteps of the HRNN will, just like in Minsky’s construction, simulate the transition function of the\nsimulated FSA. However, in contrast to the Minsky construction, in which each transition step in\nthe FSA was implemented by a single application of the Elman update rule, here, a single transition\nin the FSA will be implemented using multiple applications of the Elman update rule, the end result\nof which is the activation of the two-hot representation of the appropriate next state. Nonetheless,\nthere are, abstractly, two sub-steps of the update step, analogous to the Minsky construction (cf.\nFig. 5.6):\n1. Detect the activations of all possible next states, considering any possible input symbol\n(performed by the term Uht in Minsky’s construction).\n2. Filter the activations of the next states by choosing only the one transitioned into by a\nyt-transition (performed by conjoining with the term VJytK in Minsky’s construction).\nThe novelty of Dewdney’s construction comes in the first sub-step: How can the Elman update\nstep be used to activate the two-hot representation of qt’s out-neighborhood? As alluded to, this\nrelies on the pre-computed predecessor matrices Pred pq; yq (cf. Definition 5.2.4). The predecessor\nmatrices of individual states are compressed (disjoined) into component-activating matrices, the\nrepresentation matrices of the predecessors of specific sets of states (cf. Definition 5.2.5), defined\nthrough the function ϕ2 in the following sense.\nDefinition 5.2.10: Component-activating matrix\nA component-activating matrix is the representation matrix Bj,y,k\ndef\n“ WQPred pΦk,j; yq\nfor some k P Zr and j P Z2.\nIntuitively, the component-activating matrix Bj,y,k is the result of the disjunction of the matrix\nrepresentations of all y-predecessors q of all states q1 whose jth component of the vector ϕ2 pq1q\nequals k. This results in 2|Σ|s matrices. They can be pre-computed and naturally depend on the\ntransition function δ. The name component-activating matrix is inspired by the fact that each of the\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n179\nmatrices “controls” the activation of one of the 2|Σ|s neurons in a specific sub-vector of the HRNN\nhidden state. That is, each component-activating matrix controls a particular dimension, indexed\nby the tuple pj, y, kq for j P B, y P Σ, k P Zs, in the data sub-vector of the HRNN hidden state.\nAs we will see shortly, they contain all the information required for simulating A with a HRNN.\nTo define the transition function of the HRNN simulating A, all 2|Σ|s component-activating\nmatrices are decomposed into permutation-detectable line matrices (cf. Definition 5.2.9) whose\nactivations are combined (disjoined) into the activations of individual component-activating matrices.\nAnalogously to above, we will denote the sets of line matrices decomposing the component-activating\nmatrices as Bj,y,k, i.e., Bj,y,k “ Ž\nMPBj,y,k M. The dimensions of the hidden state corresponding to\nthe activations of the line matrices before they are combined into the activations of the component-\nactivating matrices form the processing sub-vector of the HRNN hidden state since they are\nrequired in the pre-processing steps of the update step to determine the activation of the actual\nhidden state. This is schematically drawn in Fig. 5.12a.\nFor any component-activating matrix B decomposed into the set of line matrices B, we know\nby Lemmas 5.2.6 and 5.2.7 that all M P B are detectable by a single-layer MLP. By adding an\nadditional layer to the MLP, we can disjoin the detections of M P B into the detection of B. More\nabstractly, this MLP, therefore, detects the activation of one of the 2|Q|s cells of the data sub-vector\nof the HRNN hidden state—all of them together then form the two-hot encoding of all possible next\nstates of the FSA (before taking into account the input symbol). Designing 2|Q|s such single-values\nMLPs, therefore, results in an MLP activating the two-hot representations of all possible next states\nof the simulated FSA. Conjoining these activations with the input symbol, analogously to how this\nis done in the Minsky construction, results in the activation of the two-hot representation of only\nthe actual next state of the simulated FSA. This is illustrated in Fig. 5.12b.\nHigh-level overview of simulating a transition.\nIn summary, after decomposing all the\ncomponent-activating matrices into the sets Bj,y,k, the detection of all candidate next states (before\nconsidering the input symbol) in the update step of HRNN is composed of the following sub-steps.\n1. Compute the activations of the two factors of all the transversals in Bj,y,k for all j, y, k\n(Lemma 5.2.7).\n2. Conjoin the activations of the two factors into the activations of the transversals (Lemma 5.2.7).\n3. Compute the activations of the column and row matrices in Bj,y,k for all j, y, k (Lemma 5.2.6).\n4. Disjoin of the activations of all the line matrices (transversals, row, and column matrices) in\nBj,y,k for all x, y, k to compute the activations of all 2|Σ|s component-activatimg matrices.\nThis results in the activation of the two-hot representations of all possible next states (i.e., the entire\nout-neighborhood of qt). In the last sub-step of the HRNN update step, these are conjoined with\nthe representation of the current input symbol. This step is very similar to the analogous stage in\nMinsky’s construction, with the difference that here, the non-zero entries of the vector Vht must\ncover the two-hot representations of the states with an incoming yt-transition. This conjunction\nthen ensures that among all the states in the out-neighborhood of qt, only the one reached by taking\nthe yt-transition will be encoded in ht`1. The construction just described can be summarized by\nthe following lemma.14\n14To formally prove it is correct, we would have to follow a similar set of steps to how the correctness of Minsky’s\nconstruction (Lemma 5.2.3) was proved. We omit this for conciseness.\n180\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n…\n…\n…\nData sub-vector\nProcessing sub-vector\n…\nAND\nOR\n…\n(a) High-level overview of Dewdney’s construction. The highlighted orange neuron in the representation\nof the state from the data sub-vector corresponds to the activation of one of the components of the red\nstates (which have in common that their 0th component of ϕ2 pqq is the same). The matrix corresponding\nto the disjunction of the representations of their y-predecessors (blue states) is decomposed into two line\nmatrices—a transversal and a column matrix. The non-zero elements of the former can be detected by a\nconjunction of two neurons while the non-zero elements of the latter can be detected directly by a single\nneuron. Those activations are then disjoined to result in the activation in the orange neuron. The purple\nneurons in the processing sub-vector are composed of the neurons in the networks implementing the detection\nof line matrices and their conjunctions and disjunctions (also shown in purple).\nq\nq1\nq2\na\nb\nq\nq1\nq2\na\nb\nq\nq1\nq2\na\nb\nˆ\nv pqq\np\n˙\nˆ\nv pqq\np1\n˙\nˆ\nv ptq1, q2uq\np2\n˙\nˆ\nv pq1q\np3\n˙\nPhase 1\nPhase 2\nPhase 3\nPhase 4\n(b) A high-level illustration of how the transition function of the FSA is implemented in Dewdney’s\nconstruction on an example of an FSA fragment, where the simulated automaton is initially in the\nstate q and reads the symbol a, transitioning to q1. The components whose changes are relevant\nat a given step are highlighted. Starting in the state q, which is stored in the data sub-vector\nv pqq, in the first sub-step, the processing bits of the appropriate line matrices are activated (p1).\nNext, the activated line matrices are used to activate the representations of all the states in the\nout-neighborhood of q in the data sub-vector (v\n`␣\nq1, q2(˘\n). Lastly, these representations are\nconjoined with the states reachable by the symbol a, resulting in the representation of the state q\nin the data sub-vector (v pqq).\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n181\nLemma 5.2.8\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. Then, Dewdney’s construction results in a\nHRNN correctly simulating A’s transition function, i.e, sphtq “ qt for all t.\nThis shows that Dewdeny’s construction correctly encodes the FSA in a HRNN. However, its\nspace efficiency remains to be determined. As mentioned above, working with two-hot representations\nof the states means that the data sub-vector is of size O\n´\n|Σ|\na\n|Q|\n¯\n. However, the construction\nalso requires a number of processing dimensions in the processing sub-vector. To understand the\nfull complexity of the construction, we have to determine the maximal number of processing bits\nin the HRNN. The first step to the answer is contained in the following lemma, which describes\nthe number of line matrices required to cover an arbitrary binary matrix. It lies in the core of the\nefficiency of Dewdney’s construction.\nLemma 5.2.9\nLet B P BDˆD with N 2 elements equalling 1. Then, there exists a decomposition B of B into\nat most 2N line matrices such that Ž\nMPB M “ B.\nProof. Based on Dewdney (1977). Define the sequence of transversals T1, T2, . . . where Ti is the\ntransversal containing the maximum number of ones in the matrix Bi\ndef\n“ B ´ Ži´1\nj“1 Bj. The\ntransversal containing the maximal number of ones can be found using the maximum matching\nalgorithm. Continue this sequence until there are no more ones in Bi. The number of ones in the\nmatrices Bi, ∥Bi∥1, forms a (weakly) decreasing sequence.\nIf there are at most 2N transversals in the sequence, the lemma holds. Otherwise, we compare\nthe functions f piq\ndef\n“ ∥Ti∥1 and g piq\ndef\n“ 2N ´ i.\n• If f piq ą g piq for all i “ 1, . . . , N, then řN\ni“1 f piq “ řN\ni“1 ∥Ti∥1 ą řN\ni“1 2N ´ i “ 2N 2 ´\n1\n2NpN ` 1q ě N 2. However, the transversals in the decomponsition cannot contain more ones\nthan the original matrix.\n• We conclude that for some i ď N, f piq ď g piq. Let i0 be the first such index in 1, . . . , N and\nL1\ndef\n“ tT1, . . . , Tku. Since the maximum number of independent ones (in the sense that at\nmost one appears in a single row/column) in Bi0´1 is ∥Ti0∥1 ď 2N ´ i0 (those are chosen by\nthe maximum transversal Ti0). By K¨onig’s theorem (Sz´arnyas, 2020), there is a set of at most\n2N ´ i0 column or row matrices L2\ndef\n“ tL1, . . . Lku with k ď 2N ´ i0 which cover Bi0´1.15\nTherefore, L\ndef\n“ L1 Y L2 constitutes a valid cover of B with ď N ` 2N ´ i0 “ O pNq matrices.\n■\nWe will denote the number of matrices in the line decomposition of a matrix B constructed\nby the greedy procedure from Lemma 5.2.9 as L pBq.\nConnecting this lemma to Dewdney’s\nconstruction, this shows that the number of neurons required to detect the activation of a single\nset Pred pk; yq grows asymptotically as the square root of the number of ones in the representation\n15Intuitively, since all ones are contained within ď 2N ´ i0 rows or columns, they can be simply covered by matrices\ncontaining those.\n182\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nmatrix WQPred pk; yq—this is how many line matrices the matrix will decompose into. The size of\neach neuron is 2|Σ|s.\nThis allows us to show how many neurons the entire HRNN simulating A has. Since we know that\nthe data sub-vector will always have exactly 2|Σ|s cells, we characterize the number of processing\ncells in the following lemma.\nLemma 5.2.10\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. Then, Dewdney’s construction results in a\nHRNN with a hidden state of size O\n´\n|Σ||Q|\n3\n4\n¯\n.\nProof. The number of cells in the entire processing sub-vector is simply the sum of the processing\nneurons of all the data components. In the worst case, a single component-activating matrix B\nrequires 2L pBq ` 1 neurons (2 for each transversal in the decomposition of B and an additional\none for their disjunction). Therefore, enumerating the set of matrices tBj,y,k | j P Z2, y P Σ, k P Zsu\nwith Bn for n “ 1, . . . , 2|Σ|s, the number of neurons required by all component-activating matrices\nis bounded as follows.\n2|Σ|s\nÿ\nn“1\n2L pBnq ` 1 ď\n2|Σ|s\nÿ\nn“1\n2\n´\n2r\nb\n∥Bn∥1s\n¯\n` 1\ndef\n“\n2|Σ|s\nÿ\nn“1\n4mn ` 1\n(5.69)\nSince the matrices Bn contain one non-zero entry for each state-symbol pair, it holds that\n2|Σ|s\nÿ\nn“1\n∥Bn∥1 ď\n2|Σ|s\nÿ\nn“1\nm2\nn “ |Σ||Q|\n(5.70)\nPretending that mn can take real values, the value of Eq. (5.69) is maximized under the constraint\nfrom Eq. (5.70) when all mn are equal with mn “\n?\n2s. This means that\n2|Σ|s\nÿ\nn“1\n4mn ` 1 ď\n2|Σ|s\nÿ\nn“1\n4\n?\n2s ` 1 “ 8|Σ|s\n?\n2s ` 1 “ O\n´\n|Σ||Q|\n3\n4\n¯\n,\n(5.71)\nfinishing the proof.\n■\nAll results stated in this section can be summarized in the following theorem.\nTheorem 5.2.2: Dewdney (1977)\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. Then, there exists a HRNN of size O\n´\n|Σ||Q|\n3\n4\n¯\ncorrectly simulating A.\nIndyk’s Construction\n§5.2.2 describes a construction of an HRNN of size O\n´\n|Σ||Q|\n3\n4\n¯\nsimulating an FSA. While this\nimproves the space efficiency compared to Minsky’s construction, it is not asymptotically optimal.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n183\nIndyk (1995) proved that a HRNN simulating an FSA A “ pΣ, Q, I, F, δq over a binary alphabet\nΣ “ B requires at least Ω\n´a\n|Q|\n¯\nhidden dimensions. He also provided a construction that achieves\nthis lower bound. This construction is conceptually very similar to Dewdney’s in that it works by\nactivating neurons corresponding to some form of compressed predecessor matrices (component-\nactivating matrices) and then selecting the transition which matches the input symbol. Again, it\nadditively covers these matrices with components that are easy to detect, similar to how Dewdney’s\nconstruction uses line matrices. However, Indyk’s construction defines component-activating matrices\nbased on different sets of states and covers them with a different decomposition—these are the two\ncrucial differences allowing the construction to achieve the optimal lower bound.\nWe first define the component-activating matrices and their role in updating the hidden state of\nthe HRNN. In Indyk’s construction, the component-activating matrices are based on four-hot rather\nthan two-hot encodings of states.\nDefinition 5.2.11: Four-hot representation of a state\nLet A “ pΣ, Q, I, F, δq be an FSA, r\ndef\n“ r|Q|\n1\n4 s, and π a permutation of Q “ r|Q|s.a We define\nthe four-hot representation of q P Q as\nϕ4pqq “ pℓ1, ℓ2, ℓ3, ℓ4q\n(5.72)\nwhere\nℓj “ π pqq\nrj´1\nmod r.\n(5.73)\nWe denote the inverse of ϕ4 with ϕ´1\n4\nand further define for k P Zr\nϕ´1\n4\npk, ¨, ¨, ¨q\ndef\n“ tq P Q | ϕ4 pqq1 “ ku\n(5.74)\nand ϕ´1\n4\np¨, k, ¨, ¨q, ϕ´1\n4\np¨, ¨, k, ¨q, and ϕ´1\n4\np¨, ¨, ¨, kq analogously.\naThe exact form of π will be important later. For now, one can think of π as the identity function.\nWe will denote ϕ´1\n4\np. . . , k, . . .q with k in jth position (with j P Z4) as Φk,j. Despite using the\nfour-hot representations, Indyk’s construction still requires the two-hot representations based on ϕ2\nas before. In this case, however, they again depend on the chosen permutation π. This allows us to\ndefine the component-activating matrices as follows.\nDefinition 5.2.12: Component-activating matrix\nA component-activating matrix in Indyk’s construction is the representation matrix\nWQPred pΦk,j; yq for some k P Zr, j P Z4, and y P Σ.\nFor efficient detection, the component-activating matrices are covered by so-called non-decreasing\nmatrices.\n184\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nDefinition 5.2.13: Non-decreasing matrix\nWe say that B P BDˆD is non-decreasing if there exists a non-decreasing (partial) function\nf : ZD Ñ ZD (from columns to rows) such that\nBij “ 1 ðñ f pjq “ i\n(5.75)\nand, if f is defined for some j P ZD, it is also defined for all j1 ě j.\nExample 5.2.4: Non-decreasing matrices\nAn example of a non-decreasing matrix is\nB “\n¨\n˚\n˚\n˝\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n˛\n‹‹‚.\n(5.76)\nThe function f defining the non-decreasing matrix B is f “\nˆ\n0\n1\n2\n3\nH\n0\n1\n1\n˙\n, where H denotes\nthat the function is not defined.\nAgain, clearly, any matrix B P BDˆD can be (non-uniquely) decomposed into at most D\nnon-decreasing matrices. Moreover, non-decreasing matrices are detectable.\nLemma 5.2.11\nNon-decreasing matrices are detectable.\nProof. Let B P BDˆD be a non-decreasing matrix defined by the partial function f. Divide the\ndomain of f into the set of intervals in which the function is constant, with Ipjq denoting the interval\nof j P Zr2 for j such that f pjq is defined. Then, it is easy to see that Bij “ 1 ðñ i “ f pjq,\nmeaning that by defining the parameters w and b as\nwfpjq\ndef\n“ r2 ´ I pjq\n(5.77)\nwr2`j\ndef\n“ Ipjq\n(5.78)\nb\ndef\n“ ´r2\n(5.79)\nand other elements as 0, we get that\nBij “ 1 ðñ i “ f pjq ðñ wi ` wj ` b “ 0.\n(5.80)\nCompared to earlier, where component-activating matrices were detected by testing an inequality,\ndetecting a non-decreasing matrix requires testing an equality. Since all terms in the equality are\nintegers, testing the equality can be performed with the Heaviside activation function by conjoining\ntwo neurons; one testing the inequality wi ` wj ` b ´ 1 ă 0 and another one testing the inequality\nwi ` wj ` b ` 1 ą 0. Both can individually be performed by a single neuron and then conjoined by\nan additional one.\n■\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n185\nWith this, the high-level idea of Indyk’s construction is outlined in Fig. 5.13. After constructing\nthe component-activating matrices based on ϕ4 and decomposing them into non-decreasing matrices,\nthe rest of Indyk’s construction is very similar to Dewdney’s construction, although the full update\nstep of the HRNN requires some additional processing. To test the equality needed to detect\nnon-decreasing matrices in the decomposition, Eq. (5.80), the four-hot representations are first\nconverted into two-hot ones. This can be done by a simple conjunction of the first two and the last\ntwo components of the four-hot representation. Then, the activations of the non-decreasing matrices\ncan be computed and disjoined into the representations of the component-activating matrices. These\nform the 4|Σ|r components of the data sub-vector of the HRNN hidden state. They contain the\nactivations of all possible next states, i.e., the out-neighborhood of the current state of A. These are\nthen conjoined with the representation of the current input symbol in the same way as in Dewdney’s\nconstruction but adapted to the four-hot representations of the states. The process is thus very\nsimilar to the phases of Dewdeney’s construction illustrated in Fig. 5.12b.\nIndyk’s construction can be summarized by the following lemma.16\nLemma 5.2.12\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. Then, Indyk’s construction results in a HRNN\ncorrectly simulating A’s transition function, i.e, sphtq “ qt for all t.\nThe only remaining thing to show is that Indyk’s construction achieves the theoretically optimal\nlower bound on the size of the HRNN simulating a deterministic FSA. All previous steps of the\nconstruction were valid no matter the chosen permutation π. The permutation, however, matters\nfor space efficiency: intuitively, it determines how efficiently one can decompose the resulting\ncomponent-activating matrices (which depend on the permutation) into non-decreasing matrices in\nthe sense of how many non-decreasing matrices are required to cover it. Indyk, therefore, proved\nthat there always exists, with non-zero probability, a permutation in which the decomposition across\nall states is efficient enough to achieve the minimum number of neurons required. This is formalized\nby the following lemma, whose proof can be found in Indyk (1995, Lemma 6).\nLemma 5.2.13\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. There exists a permutation of Q such that\nIndyk’s construction results in a HRNN of size O\n´\n|Σ|\na\n|Q|\n¯\n.\nThis concludes our presentation of Indyk’s construction. All results stated in this section can be\nsummarized by the following theorem.\nTheorem 5.2.3: Indyk (1995)\nLet A “ pΣ, Q, I, F, δq be a deterministic FSA. There exists a HRNN of size O\n´\n|Σ|\na\n|Q|\n¯\ncorrectly simulating A.\n16Again, to formally prove it is correct, we would have to follow a similar set of steps to how the correctness of\nMinsky’s construction (Lemma 5.2.3) was proved. We omit this for conciseness.\n186\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n…\n…\nData sub-vector\nProcessing sub-vector\n…\nAND\nOR\n…\n…\n…\n…\n…\n…\nAND\nFigure 5.13: High-level overview of Indyk’s construction. The highlighted orange neuron in the\nrepresentation of the state from the data sub-vector corresponds to the activation of one of the\ncomponents of the red states (which have in common that their 0th component of ϕ4 pqq is the same).\nThe matrix corresponding to the disjunction of the representations of their y-predecessors (blue states)\nis decomposed into two non-decreasing matrices. The non-zero elements of both can be detected\nby a conjunction of two neurons; here, f1 “\nˆ\n0\n1\n2\n3\nH\n0\n0\n0\n˙\nand f2 “\nˆ\n0\n1\n2\n3\nH\nH\n1\n2\n˙\n, meaning\nthat w1 “\n`\n3\n0\n0\n0\n|\n0\n1\n1\n1\n˘\n, w2 “\n`\n0\n3\n2\n0\n|\n0\n0\n1\n2\n˘\n, and b1 “ b2 “ 4.\nThose activations are then disjoined to result in the activation in the orange neuron. The purple\nneurons in the processing sub-vector are composed of the neurons in the networks implementing the\ndetection of line matrices and their conjunctions and disjunctions (also shown in purple). Note that\neven if the second matrix were not non-decreasing in itself (i.e., the columns of the two ones would\nbe flipped), one could still transform it into a non-decreasing matrix by permuting the columns and\npermuting the corresponding neurons.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n187\n5.2.3\nLower Bound in the Probabilistic Setting\nWe now ask whether the same lower bound can also be achieved when simulating dPFSAs. We find\nthat the answer is negative: dPFSAs may require an HRNN LMs of size Ωp|Σ||Q|q to faithfully\nrepresent their probability distribution. Since the transition function of the underlying FSA can be\nsimulated with more efficient constructions, the bottleneck comes from defining the same probability\ndistribution. Indeed, as the proof of the following theorem shows, the issue intuitively arises in the\nfact that, unlike in an HRNN LM, the local probability distributions of the different states in a\nPFSA are completely arbitrary, whereas they are defined by shared parameters (the output matrix\nE) in an HRNN LM.\nTheorem 5.2.4: A lower bound on the size of the RNN simulating a PFSA\nLet A “ pΣ, Q, δ, λ, ρq be a minimal dPFSA and pΣ, D, U, V, E, bh, h0q an HRNN LM defining\nthe same LM. Then, D must scale linearly with |Q|.\nProof. Without loss of generality, we work with R-valued hidden states. Let A be a minimal\ndeterministic PFSA and R “ pΣ, D, U, V, E, bh, h0q a HRNN with pLN pyq “ A pyq for every\ny P Σ˚. Let yăT P Σ˚ and yďT\ndef\n“ yăT y for some y P Σ. Define p pyq\ndef\n“ ś|y|\nt“1 pSM pyt | yătq. It\nis easy to see that p pyăT yT q “ p pyăT q pSM pyt | yăT q. The conditional distribution pSM p¨ | yăT q\nare proportional to the values in EhT ´1. By definition of the deterministic PFSA, there are |Q|\nsuch conditional distributions. Moreover, these distributions (represented by vectors P ∆|Σ|´1) can\ngenerally be linearly independent. This means that for any q, the probability distribution of the\noutgoing transitions can not be expressed as a linear combination of the probability distributions of\nother states. To express the probability vectors for all states, the columns of the output matrix E,\ntherefore, have to span R\n|Q|, implying that E must have at least |Q| columns. This means that the\ntotal space complexity (and thus the size of the HRNN representing the same distribution as A) is\nΩp|Q|q.\n■\nAsymptotic Bounds in |Σ|\nSince each of the input symbols can be encoded in log |Σ| bits, one\ncould expect that the linear factor in the size of the alphabet from the constructions above could be\nreduced to O plog |Σ|q. However, we again find that such reduction is in general not possible—the set\nof FSAs presented next is an example of a family that requires an HRNN whose size scales linearly\nwith |Σ| to be simulated correctly. We also provide a sketch of the proof of why a compression in\n|Σ| is not possible.\nLet AN “ pΣN, t0, 1u, t0u, t1u, δNq be an FSA over the alphabet ΣN “ ty1, . . . , yNu such that\nδN “\n!\n0\ny1\nÝÑ 1\n)\nY\n!\n0\nyn\nÝÑ 2 | n “ 2, . . . N\n)\n(see Fig. 5.14).\nClearly, to be able to correctly represent all local distributions of the dPFSA, the HRNN LM\nmust contain a representation of each possible state of the dPFSA in a unique hidden state. On the\nother hand, the only way that the HRNN can take into account the information about the current\nstate qt of the simulated FSA A is through the hidden state ht. The hidden state, in turn, only\ninteracts with the recurrence matrix U, which does not have access to the current input symbol\nyt`1. The only interaction between the current state and the input symbol is thus through the\naddition in Uht ` VJyt`1K. This means that, no matter how the information about qt is encoded in\nht, in order to be able to take into account all possible transitions stemming in qt (before taking\n188\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n0\n1\n2\ny2, . . . , yN\ny1\nFigure 5.14: The FSA AN.\ninto account yt`1), Uht must activate all possible next states, i.e., the entire out-neighborhood of\nqt. On the other hand, since VJyt`1K does not have precise information about qt, it must activate\nall states which can be entered with an yt`1-transition, just like in Minsky’s construction.\nIn Minsky’s construction, the recognition of the correct next state was done by keeping a separate\nentry (one-dimensional sub-vector) for each possible pair qt`1, yt`1. However, when working with\ncompressed representations of states (e.g., in logarithmic space), a single common sub-vector of\nsize ă |Σ| (e.g., log |Σ|) has to be used for all possible symbols y P Σ. Nonetheless, the interaction\nbetween Uht and VJyt`1K must then ensure that only the correct state qt`1 is activated. For\nexample, in Minsky’s construction, this was done by simply taking the conjunction between the\nentries corresponding to q, y in Uht and the entries corresponding to q1, y1 in VJy1K, which were\nall represented in individual entries of the vectors. On the other hand, in the case of the log\nencoding, this could intuitively be done by trying to match the log |Σ| ones in the representation\npp pyq | 1 ´ p pyqq, where p pyq represent the binary encoding of y. If the log |Σ| ones match (which\nis checked simply as it would result in a large enough sum in the corresponding entry of the\nmatrix-vector product), the correct transition could be chosen (to perform the conjunction from\nFact 5.2.1 correctly, the bias would simply be set to log |Σ| ´ 1). However, an issue arises as soon as\nmultiple dense representations of symbols in VJyK have to be activated against the same sub-vector\nin Uht—the only way this can be achieved is if the sub-vector in Uht contains the disjunction of the\nrepresentations of all the symbols which should be activated with it. If this sets too many entries in\nUht to one, this can result in “false positives”. This is explained in more detail for the dPFSAs in\nFig. 5.14 next.\nLet rn represent any dense encoding of yn in the alphabet of AN (e.g., in the logarithmic case,\nthat would be pp pnq | 1 ´ p pnqq). Going from the intuition outlined above, any HRNN simulating\nAN, the vector Uh0 must, among other things, contain a sub-vector corresponding to the states\n1 and 2. The sub-vector corresponding to the state 2 must activate (through the interaction in\nthe Heaviside function) against any yn for n “ 2, . . . , N in AN. This means it has to match\nall representations rn for all n “ 2, . . . , N. The only way this can be done is if the pattern for\nrecognizing state 2 being entered with any yn for n “ 2, . . . , N is of the form r “ ŽN\nn“2 rn. However,\nfor sufficiently large N, r “ ŽN\nn“2 rn will be a vector of all ones—including all entries active in r1.\nThis means that any encoding of a symbol will be activated against it—among others, y1. Upon\nreading y1 in state 1, the network will therefore not be able to deterministically activate only the\nsub-vector corresponding to the correct state 1. This means that the linear-size encoding of the\nsymbols is, in general, optimal for representing dPFSAs with HRNN LMs. This discussion implies\nthe following theorem.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n189\nTheorem 5.2.5: A lower bound on the size of the RNN simulating a PFSA\nLet A “ pΣ, Q, δ, λ, ρq be a minimal dPFSA and pΣ, D, U, V, E, bh, h0q an HRNN LM defining\nthe same LM. Then, D must scale linearly with |Σ|.\nBased on the challenges encountered in the example above, we can devise a simple sufficient\ncondition for a logarithmic compression w.r.t. |Σ| to be possible: namely, that for any pair of states\nq, q1 P Q, there is at most a single transition leading from q to q1. This intuitive characterization\ncan be formalized by a property we call log |Σ|-separability.\nDefinition 5.2.14: log |Σ|-separable finite-state automaton\nAn FSA A “ pΣ, Q, I, F, δq is log |Σ|-separable if it is deterministic and, for any pair q, q1 P Q,\nthere is at most one symbol y P Σ such that q\nyÝÑ q1 P δ.\nlog |Σ|-separability is a relatively restrictive condition. To amend that, we introduce a simple\nprocedure which, at the expense of enlarging the state space by a factor of Σ, transforms a general\ndeterministic (unweighted) FSA into a log |Σ|-separable one.\nWe call this log |Σ|-separation.\nIntuitively, it augments the state space by introducing a new state pq, yq for every outgoing transition\nq\nyÝÑ q1 of every state q P Q, such that pq, yq simulates the only state the original state q would\ntransition to upon reading y.\nDue to the determinism of the original FSA, this results in a\nlog |Σ|-separable FSA with at most |Q||Σ| states.\nWhile the increase of the state space might seem like a step backward, recall that using Indyk’s\nconstruction, we can construct an HRNN simulating an FSA whose size scales with the square\nroot of the number of states. And, since the resulting FSA is log |Σ|-separable, we can reduce\nthe space complexity with respect to Σ to log |Σ|. This is summarized in the following theorem,\nwhich characterizes how compactly general deterministic FSAs can be encoded by HRNNs. To our\nknowledge, this is the tightest bound on simulating general unweighted deterministic FSAs with\nHRNNs.\nTheorem 5.2.6: Efficiently simulating general FSAs\nLet A “ pΣ, Q, I, F, δq be a minimal FSA recognizing the language L. Then, there exists an\nHRNN R “ pΣ, D, U, V, E, bh, h0q accepting L with D “ O\n´\nlog |Σ|\na\n|Σ||Q|\n¯\n.\nThe full log |Σ|-separation procedure is presented in Algorithm 2. It follows the intuition of\ncreating a separate “target” for each transition q\nyÝÑ q1 for every state q P Q. To keep the resulting\nFSA deterministic, a new, artificial, initial state with no incoming transitions is added and is\nconnected with the augmented with the out-neighborhood of the original initial state.\nThe following simple lemmata show the formal correctness of the procedure and show that it\nresults in a log |Σ|-separable FSA, which we need for compression in the size of the alphabet.\nLemma 5.2.14\nFor any y P Σ, pq, yq\ny1\nÝÑ pq1, y1q P δ1 if and only if q\ny1\nÝÑ q1 P δ.\n190\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nAlgorithm 2\n1. def Separate(A “ pΣ, Q, I, F, δq):\n2.\nA1 Ð pΣ, Q1 “ Q ˆ Σ Y tqι1u, δ1 “ ∅, I1 “ tqι1u, F 1 “ ∅q\n3.\nŹ Connect the out-neighborhood of the original initial state qι with the new, aritificial, initial state.\n4.\nfor y P Σ :\n5.\nfor qι\ny1\nÝÑ q1 P δ :\n6.\nadd qι1\nyÝÑ pq1, y1q to δ1\n7.\nfor q P Q, y P Σ :\n8.\nfor q\ny1\nÝÑ q1 P δ :\n9.\nadd pq, yq\ny1\nÝÑ pq1, y1q to δ1\n10.\nŹ Add all state-symbol pairs with a state from the original set of final states to the new set of final states.\n11.\nfor qφ P F, y P Σ :\n12.\nadd pqφ, yq to F 1\n13.\nif qι P I : Ź Corner case: if the original initial state qι is an initial state, make the artificial initial state\nqι1 final.\n14.\nadd qι1 to F 1\n15.\nreturn A1\nProof. Ensured by the loop on Line 3.\n■\nLemma 5.2.15\nlog |Σ|-separation results in an equivalent FSA.\nProof. We have to show that, for any y P Σ˚, y leads to a final state in A if and only if y leads to a\nfinal state in A1. For the string of length 0, this is clear by Lines 13 and 14. For strings of length\ně 1, it follows from Lemma 5.2.14 that y leads to a state q in A if and only if Dy P Σ such that y\nleads to pq, yq in A1. From Lines 11 and 12, pq, yq P F 1 if and only if q P F, finishing the proof.\n■\nLemma 5.2.16\nlog |Σ|-separation results in a log |Σ|-separable FSA.\nProof. Since the state pq1, y1q is the only state in Q1 transitioned to from pq, yq after reading y1 (for\nany y P Σ), it is easy to see that A1 is indeed log |Σ|-separable.\n■\nDiscussion and the practical applicability of these result.\nThis section showed that Heaviside-\nactivated RNNs are equivalent to WFSAs. This might come as a bit of a surprise considering that\nwe introduced RNNs with the goal of overcoming some limitations of exactly those models, e.g., the\nfinite context length. However, note that to arrive at this result, we considerately restricted the\nform of a recurrent neural network. While on the one hand restriction to the Heaviside activation\nfunction means that all the RNNs we considered in this section can be implemented and represented\nin a computer, the RNN sequence models that we usually deal with are much more complex than\nthis analysis allowed for. Furthermore, note that the RNNs in practice do not learn sparse hidden\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n191\nstates of the form considered in the construction in the proof of Lemma 5.2.2—indeed, networks\nwith Heaviside activation functions are not trainable with methods discussed in §3.2 as the gradient\non the entire parameter space would be either 0 or undefined and in this sense, the trained networks\nwould never have such hidden state dynamics. The dynamics of RNNs in practice result in dense\nhidden states, i.e., states in which many dimensions are non-zero. Nonetheless, keep in mind that\ntheoretically, due to the finite-precision nature of our computers, all models we ever consider will\nbe at most finite-state—the differentiating factor between them will be how appropriately to the\ntask they are able to learn the topology (transitions) of the finite-state automaton they represent\nand how efficiently they are able to learn it. Lastly, note that, by considering special classes of\n(sub-)regular languages, one can arrive at neural representations far smaller than those described by\nthe constructions above (Hewitt et al., 2020; Svete et al., 2024).\n192\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n5.2.4\nTuring Completeness of Recurrent Neural Networks\nWe now turn to the (purely theoretical) treatment of the expressive capacity of recurrent neural\nnetworks in which we take the liberty of making somewhat unrealistic assumptions. Specifically, in\npractice, RNNs usually have the following properties:\n• The weights and intermediate computations are done with finite floating point precision;\n• An RNN always operates in real-time, meaning that it performs a constant number of operations\nbefore consuming/outputting a symbol.\nUnder these assumptions, we saw that RNNs with Heaviside activations in a practical setting lie at the\nbottom of the weighted Chomsky hierarchy, being able to only recognize regular languages. However,\nif we relax these two assumptions, allowing for arbitrary precision and unbounded computation time\nbetween symbols, RNNs jump directly to the top of the hierarchy: they become Turing complete.\nWe start by introducing the saturated sigmoid, one of the building blocks we will use to show\nthis.\nDefinition 5.2.15: Saturated Sigmoid function\nThe saturated sigmoid is defined as\nσ pxq “\n$\n’\n&\n’\n%\n0\nif x ď 0\nx\nif 0 ă x ď 1\n1\nif x ą 1\n.\n(5.81)\nIntuitively, the saturated sigmoid clips all negative values to 0, all values larger than 1 to 1, and\nleaves the elements of r0, 1s intact. The graph of this function is shown in Fig. 5.15.\nσ pxq\nx\n0\n1\n1\nFigure 5.15: The saturated sigmoid.\nThe central result of this subsection is then summarized in the following theorem, summarized\nfrom Nowak et al. (2023).\nTheorem 5.2.7: Saturated Sigmoid Elman RNNs are Turing complete\nElman recurrent neural network sequence models with the saturated sigmoid activation\nfunctions are Turing complete.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n193\nBy the end of this subsection, we will have proven this result by showing that Saturated Sigmoid\nElman RNNs can encode two-stack pushdown automata which are computationally equivalent to\nTuring machines (cf. Definitions 4.2.50 and 4.2.51). We start with a simpler construction: building\non our placement of RNNs on at least the regular rung of the ladder of formal language complexity\n(cf. Lemma 5.2.2), we take one step up the ladder and show that RNNs can simulate a single-stack\npushdown automaton (cf. Definition 4.2.31). This will help us gain the intuition behind how\nRNNs can use infinite precision arithmetic to simulate a stack. We will then simply generalize this\nconstruction to the two-stack case.\nLet us begin by considering the problem of representing a stack—an arbitrarily long sequence\nof symbols, accessible in a last-in-first-out (LIFO) fashion—in a vector of constants size, e.g., the\nhidden state of a recurrent neural network. For simplicity, but without the loss of generality, assume\nthat we are working with a simple two-letter stack alphabet Γ “ t0, 1u. Any stack sequence γ will\nbe a member of Γ˚, i.e., a string of 0’s and 1’s. If we think of the stack symbols as numbers for a\nmoment, there is a natural correspondence between the possible stack configurations and numbers\nexpressed in base 2. By convention, we will represent a string of stack symbols γ with numbers\nafter the decimal point, rather than as integers. Assuming infinite precision, we can therefore\nsimply represent each stack configuration as a single number (of course, the stack alphabet does not\nhave to be exactly Γ “ t0, 1u—we can always map symbols from any alphabet into their numeric\nrepresentations in some base large enough to allow for the entire alphabet). Notice that in this\ncase, pushing or popping from the stack can be performed by division and multiplication of the\nvalue representing the stack—if we want to push a value x P t0, 1u, we can divide the current\nrepresentation (by 2) and append x to the right side of the new representation and if we want to\npop any value, we simply have to multiply the current representation by 2. This also gives us an\nidea of how to represent a stack in the hidden state of an RNN: the entire stack sequence will simply\nbe represented in a single dimension of the hidden state, and the value stored in the cell will be\nupdated according to the transitions defined by the simulated automaton. Note, however, that the\nRNN will not only have a single dimension in the hidden state: other dimensions will contain values\nthat will be required to control the RNN updates correctly.\nIn our proofs, we consider a special type of pushdown automata, as defined in Definition 4.2.31:\nwe will use pushdown automata which only consider the topmost element of the stack when\ndefining the possible transitions from a configuration and can only push one stack symbol at a\ntime. More formally, this means that in the tuple P “ pΣ, Q, Γ, δ, pqι, γιq, pqφ, γφqq, we have that\nδ Ď Q ˆ Γ ˆ pΣ Y tεuq ˆ Q ˆ Γ rather than the more general δ Ď Q ˆ Γ˚ ˆ pΣ Y tεuq ˆ Q ˆ Γ˚.\nFurthermore, we assume that γι “ ε and γφ “ ε, that is, the PDA starts off with an empty stack\nand has to empty it again to arrive at a final configuration. Note that these restrictions can be done\nwithout loss of generality—that is, such pushdown automata are as powerful as the unrestricted\nversions (Sipser, 2013). With this in mind, we can show that arbitrary precision RNNs are capable\nof recognizing at least deterministic context-free languages:\nTheorem 5.2.8: RNNs can recognize deterministic context-free languages\nElman recurrent neural networks can recognize deterministic context-free languages.\nBefore we continue to the proof of Theorem 5.2.8, let us remark on three simple but important\nintuitions which will be crucial for understanding the construction of the Elman RNN, both in the\nsingle- as well as the two-stack variants of PDAs. Multiple times in the construction, we will be\n194\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nfaced with the task of moving or copying the value from some dimension i to the dimension j in the\nvector. The following fact shows how this can be done using simple matrix multiplication with a\nspecific matrix.\nFact 5.2.2: Copying elements of a vector\nLet x P RD and M P RDˆD such that Mi,: “ ei, where Mi,: denotes the ith row of M and\nei denotes the ith basis vector. Then, it holds that pMxqi “ xi.\nAlso, note that setting the row Mi,: to the zero vector 0PRD sets the entry xi to 0, i.e., it erases\nthe entry.\nFurthermore, we will use the saturated sigmoid function multiple times to detect whether a\nnumber of dimensions of a vector are set to one at the same time. Given the recurrent dynamics of\nthe Elman RNN (cf. Eq. (5.26)), we can perform this check as follows.\nFact 5.2.3: Detecting the activation of multiple values in the hidden state\nLet σ be the saturated sigmoid from Definition 5.2.15, m P t1, . . . , Du, i1, . . . , im, j P t1, . . . , Du,\nx P RD, b P RD, and M P RDˆD such that\nMj,i “\n#\n1\nif i P ti1, . . . , imu\n0\notherwise\nand bj “ ´pm ´ 1q. Then, it holds that pσ pMx ` bqqj “ 1 if and only if xik “ 1 for all\nk “ 1, . . . , m.a\naNote that this is simply a restatement of Fact 5.2.1, which we include here for clarity and to make the\nconnection with the construction that follows clearer.\nLastly, we will sometimes have to turn off certain dimensions of the hidden state if any of the\nother dimensions are active. Using the dynamics of Elman RNNs and the saturated sigmoid, this\ncan be done as follows.\nFact 5.2.4: Turning off dimensions in the hidden state\nLet σ be the saturated sigmoid from Definition 5.2.15, m P t1, . . . , Du, i1, . . . , im, j P t1, . . . , Du,\nx P RD, b P RD, and M P RDˆD such that\nMj,i “\n#\n´1\nif i P ti1, . . . , imu\n0\notherwise\nand bj “ 1. Then, it holds that pσ pMx ` bqqj “ 0 if and only if xik “ 1 for some k “ 1, . . . , m.\nWith these intuitions in mind, we now prove Theorem 5.2.8. Due to the relatively elaborate\nconstruction, we limit ourselves to pushdown automata with a two-symbol input alphabet Σ “ ta, bu\nas well as a two-symbol stack alphabet Γ “ t0, 1u. Note, however, that this restriction can be done\nwithout the loss of generality, meaning that this is enough to prove the Turing completeness of\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n195\nRNNs in general.17\nProof. We show this by constructing, for a given deterministic pushdown automaton P recognizing\na deterministic context-free language, an Elman RNN simulating the steps performed by P.\nLet P “ pΣ, Q, Γ, δ, pqι, γιq, pqφ, γφqq be such a deterministic pushdown automaton. We now\ndefine the parameters of the RNN R “ pΣ, D, U, V, E, bh, h0q such that the updates to R’s hidden\nstate will correspond to the configuration changes in P.\nThe construction is more involved than the one in Minsky’s theorem (cf. Lemma 5.2.2). We,\ntherefore, first intuitively describe the semantics of the different components of the hidden state\nof the RNN. Then, we describe the submatrices of the parameters U, V, and b that control these\ncomponents of the vector. The hidden state h of the RNN will altogether have five components.\n• Component 1: Data component: This component, consisting of three cells, will contain\nthe actual numerical representation of the stack, STACK, as well as two additional “buffer” cells,\nBUFF1 and BUFF2, which will be used for intermediate copies of the stack values during the\ncomputation of the new state.\n• Component 2: Top of stack component: This component contains three cells, each\ncorresponding to a flag denoting that (a) the stack is empty (STACKε), (b) the top element of\nthe stack is a 0 (STACK0), or (c) the top element of the stack is a 1 (STACK1).\n• Component 3: Configuration component: This component encodes the current configu-\nration of the stack (Component 2) together with the current input symbol. Note that, while\nwe assume that the input PDA works with the two-symbol alphabet Σ “ ta, bu, the sequence\nmodel defined by the RNN requires an eos symbol to be able to terminate generation (cf.\nEq. (2.44)): R, therefore, defines the conditional probabilities over the set Σ “ ta, b, eosu.\nWith this, there are nine possible configurations py, γq for γ P tε, 0, 1u and y P ta, b, eosu,\nmeaning that there are nine cells in this configuration, CONFγ,y, each corresponding to one of\nthese configurations.\n• Component 4: Computation component: This component contains four cells in which the\ncomputation of the next value of the stack is computed. There are five cells OPaction,γ because\nall possible actions (PUSH 0, PUSH 1, POP 0, POP 1, and NO-OP) are performed simultaneously,\nand only the correct one is copied into the data component (Component 1) in the end.\n• Component 5: Acceptance component: This component contains a single cell, ACCEPT,\nsignaling whether the RNN accepts the string y after reading in the input y eos.\nAltogether, the hidden state of R contains 3 ` 3 ` 9 ` 5 ` 1 “ 21 dimensions. The initial hidden\nstate h0 is a vector with a single non-zero component, whose value is 1: the cell STACKε since we\nassume that the stack of the simulated automaton is empty at the beginning of the execution. We\nnow intuitively describe the dynamics that these components define.\n17To simulate an arbitrary Turing machine with a machine with the binary alphabet, we simply have to encode\neach of the finitely-many symbols of the simulated machine using binary encoding.\n196\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nThe full update step of the network.\nThe RNN will compute the next hidden state corre-\nsponding to the new stack configuration by applying the Elman update rule (cf. Eq. (5.26)) four\ntimes to complete four discrete sub-steps of the computation. We first define\nhp0q\nt`1\ndef\n“ ht\n(5.82)\nand\nhpnq\nt`1\ndef\n“ σ\n´\nUhpn´1q\nt`1\n` Vepytq ` bh\n¯\n(5.83)\nfor n “ 1, 2, 3, 4. Then\nht`1\ndef\n“ hp4q\nt`1.\n(5.84)\nIntuitively, each of the four stages of computation of the actual next hidden state “detects” some\nparts of the pattern contributing to the transition in the pushdown automaton. We describe those\npatterns next intuitively before talking about the submatrices (or subvectors) of the RNN parameters\ncorresponding to the specific parts that update the individual components of the hidden state.\nData component.\nThe cells of the data component form a queue of three components: the STACK\ncell forms the head of the queue, followed by BUFF1 and BUFF2. The values in the cells are updated\nat each execution of Eq. (5.83) by moving the currently stored values into the next cell in the queue.\nBy doing so, the entry in BUFF2 gets discarded. The value of STACK is copied from the cells of the\ncomputation component by summing them. We will see later that at any point of the computation\n(when it matters), only one of the computation components will be non-zero, which means that\nthe summation simply corresponds to copying the non-zero computation component. All these\noperations can be performed by matrix multiplication outlined in Fact 5.2.2.\nEncoding the stack sequence.\nWhile we outlined a possible encoding of a stack sequence above,\nthe encoding we use in this construction is a bit different. Remember that for a stack sequence\nγ P Γ˚ of length N, the right-most symbol γN denotes the top of the stack. We encode the stack\nsequence γ P Γ˚ as follows:\nrep pγ1 . . . γNq\ndef\n“\nN\nÿ\nn“1\ndigit pγnq10N´n´1\n(5.85)\nwhere digit pγq\ndef\n“\n#\n1\nif γ “ 0\n3\notherwise .\nExample 5.2.5: Scalar stack representations\nFor example, the stack sequence γ “ 00110111 would be represented with rep p00110111q “\n0.33313311. Notice the “opposite orientation” of the two strings: the top of the stack in γ is\nthe right-most symbol, while it is the left-most digit in the numerical representation.\nNote that the digits 1 and 3 in the definition of digit p¨q are chosen somewhat arbitrarily—the\nencoding could also have been chosen differently. Similarly, a different (non-decimal) base could\nhave been chosen.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n197\nTop of stack component.\nAs mentioned, the STACKε cell of this component is one of the two\ncells set to 1 in the initial state of the RNN. The individual cells of this component then get updated\naccording to the top symbol on the stack encoded in the STACK cell by taking into account how\nthe stack is represented by STACK. Specifically, the parameters of the RNN are defined such that\nhSTACKε “ 1 if previously hSTACK “ 0, hSTACK0 “ 1 if previously hSTACK “ 0.1 . . ., and hSTACK1 “ 1 if\npreviously hSTACK “ 0.3 . . ..\nConfiguration component.\nThe cells of the configuration component combine the pattern\ncaptured by the top of the stack component with the input symbol at the current time step to\nactivate only the appropriate cell CONFγ,y. This can be done by incorporating the information from\nthe top of the stack component with the information about the current input symbol from VJytK.\nMore precisely, the parameters of R are set such that hCONFγ,y “ 1 if at the previous one of the four\nsub-steps of the computation of the next hidden state, hSTACKγ “ 1 and the input symbol is y.\nComputation component.\nThe computation component contains the cells in which the results\nof all the possible actions on the stack are executed. The parameters of the computation component\nare set such that, given that the previous stack configuration is hSTACK “ x1x2 . . . xN and the input\nsymbol is y, cells of the computation component are set as\nhOPPOP,γ “ x2 . . . xN\nhOPPUSH,γ “ digit pyqx1 . . . xN.\nAcceptance component.\nThe cell in the acceptance component is activated if and only if the\ncurrent input symbol is eos (denoting the end of the string whose recognition should be determined)\nand the stack is empty, i.e., the STACKε cell is activated.\nMore precisely, the dynamics described here are implemented by the four steps of the hidden\nstate update as follows (where hp1q\nt\n“ ht).\n• In phase 1, the configuration of the stack is determined by setting the top of the stack\ncomponent in hp2q\nt .\n• In phase 2, the configuration of the stack and the input symbol are combined by setting the\nconfiguration component in hp3q\nt .\n• In phase 3 all possible operations on the stack are performed in the computation component,\nand, at the same time, the results of all invalid operations (only one operation is valid at each\ntime step due to the deterministic nature of P) are zeroed-out in hp4q\nt . This is done by setting\nthe entries of the recurrence matrix U such that only the valid action is not zeroed out.\n• In phase 4 the result of the executed operations (only one of which is non-zero) is copied over\nto the STACK cell in the hidden state in ht`1.\nHaving defined the intuition behind the dynamics of the hidden state updates, we now formally\ndefine how the parameters of the RNN are set to enable them. Whenever an entry of a matrix or\nvector is not set explicitly, it is assumed that it is 0 (that is, we only explicitly set the non-zero\nvalues). Again, we define them for each component in turn.\n198\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nThe data component.\nThe values of the parameters in the data component are set as follows.\nUBUFF1,STACK “ 1\n(5.86)\nUBUFF2,BUFF1 “ 1\n(5.87)\nUSTACK,OPPUSH,0 “ USTACK,OPPUSH,1 “ USTACK,OPPOP,0 “ USTACK,OPPOP,1 “ 1\n(5.88)\nThe first two elements correspond to moving the values to the next element in the data component\nqueue, while the entries in the last row correspond to summing up the values from the computation\ncomponent to move them into the stack cell after the computation has been completed. Note that,\nof course, the elements of the computation component are always summed up and written in the\nSTACK cell, no matter what the values there are. However, the division of the computation of the\nnext hidden state into phases ensures that when it matters, i.e., after the third phase, there is only\na single computation component that is non-zero, and that one is copied into the STACK component\nin the fourth computation sub-step. All other parameters (in V and bh) are 0.\nThe top of the stack component.\nThe parameters setting the top of the stack component are\nset as follows:\nUSTACKε,STACK “ ´10\n(5.89)\nUSTACK0,STACK “ ´10\n(5.90)\nUSTACK1,STACK “ 10\n(5.91)\nbSTACKε “ 1\n(5.92)\nbSTACK0 “ 3\n(5.93)\nbSTACK1 “ ´2.\n(5.94)\nOther parameters (V) are 0. The reasoning behind these parameters is the following. The cell\nSTACK contains the numeric encoding of the stack content. We distinguish three cases.\n• If the stack is empty, hSTACK “ 0. Therefore, using the parameters above, the value of the cell\nSTACK1 after the sub-step update will be 0, while the cells STACKε and STACK0 will be 1 due to\nthe positive bias term. This might not be what you would expect—it might seem like, in this\ncase, this step erroneously signals both an empty stack and a stack whose top component is 0.\nThis, however, is corrected for in the configuration component, as we discuss below.\n• If the top of the stack is the symbol 0, hSTACK “ 0.1 . . .. This means that 10 ¨ hSTACK ď 1 and,\ntherefore, after the update rule application, hSTACK1 “ 0. It is easy to see that the setting\nof the parameters also implies hSTACKε “ 0. However, since ´10 ¨ hSTACK ě ´2, we have that\nhSTACK0 “ 1.\n• Lastly, if the top of the stack is the symbol 1, hSTACK “ 0.3 . . .. Therefore, 10 ¨ hSTACK ě 3,\nmeaning that after the update rule application, hSTACK1 “ 1. Again, it is easy to see that the\nsetting of the parameters also implies hSTACKε “ 0. On the other hand, since ´10 ¨ hSTACK ď ´3,\nit also holds that hSTACK0 “ 0.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n199\nThe configuration component.\nThe configuration component is composed of the most cells of\nany component:\nUCONFγ,y,STACKγ “ 1 for γ P tε, 0, 1u, y P teos, a, bu\n(5.95)\nUCONFγ,0,STACKε “ ´1 for y P teos, a, bu\n(5.96)\nVCONFγ,y,mpyq “ 1 for γ P tε, 0, 1u, y P teos, a, bu\n(5.97)\nbCONFγ,y “ ´1 for γ P tε, 0, 1u, y P teos, a, bu\n(5.98)\nHere, the first, third, and fourth terms together ensure that the cell CONFγ,y is activated if the current\ninput symbol is a (VCONFγ,y,mpyq) and the top of the stack is γ (UCONFγ,y,STACKγ). bCONFγ,y ensures that\nboth conditions have to be met. The second term, UCONF0,y,STACKε, on the other hand, takes care of\nan edge case: as shown above, bSTACK0 “ 0, which means that STACK0 is, by default, set to 1. The\nnegative weight UCONF0,y,STACKε “ ´1 ensures that, if the stack is indeed empty, the effect of this\ndefault value is “canceled out”, i.e., the configuration cell is not activated by mistake.\nThe computation component.\nThis is the most complicated component. The computation\ncomponents are manipulated with the following parameters:\nUPUSH0,BUFF2 “ UPUSH1,BUFF2 “ 1\n10\n(5.99)\nUPOP0,BUFF2 “ UPOP1,BUFF2 “ 10\n(5.100)\nUNO-OP,BUFF2 “ 1\n(5.101)\nUA,CONFγ,y “ ´10 for A P tOPPUSH,0, OPPUSH,1, OPPOP,0, OPPOP,1, OPNO-OPu\n(5.102)\nγ P t0, 1u, y P ta, bu\nUOPA,γ1,CONFγ,y “ 0 for q\ny,γÑγ1\nÝÝÝÝÝÑ q P δ,\n(5.103)\nA P tOPPUSH,0, OPPUSH,1, OPPOP,0, OPPOP,1, OPNO-OPu\nUOPNO-OP,CONFγ,eos “ 0 for γ P tε, 0, 1u\n(5.104)\nbOPPUSH,0 “ 1\n10\n(5.105)\nbOPPUSH,1 “ 3\n10\n(5.106)\nbOPPOP,0 “ ´1\n(5.107)\nbOPPOP,1 “ ´3\n(5.108)\nbOPNO-OP “ 0\n(5.109)\nThe first three parameters above concern copying the value of the stack encoded by the previous\nhidden state into the computation component and preparing it for modification. They work together\nwith the corresponding entries in the bias vector bh. For example, a value can be pushed onto the\nstack by dividing the value of the stack encoding by 10 and adding either 0.1 or 0.3, depending on\nwhether 0 or 1 is being pushed. This is encoded by the first setting above and bOPPUSH,0 and bOPPUSH,1.\nSimilarly, a value can be popped from the stack by multiplying the stack encoding with 10 and then\nsubtracting the appropriate value according to the bias entry. The NO-OP action is implemented\nsimply by copying the values of the stack into its cell. The remaining three parameter settings above\n200\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nensure that, after executing all possible stack actions, only the appropriate computation is kept and\nall others are zeroed out. The fourth row above ensures that “by default”, all computation cells are\nreset to 0 after every update. However, the next row “removes” the negative weights (sets them to 0)\nfor the changes in the configuration which correspond to the valid transitions, or valid actions, in the\npushdown automaton. That is, setting those values of the matrix U to 0 disables “erasing” the entry\nOPA,γ1 in the hidden state by the configuration CONFγ,y if the transition from the configuration with\nthe top of the stack γ to γ1 with the action A upon reading y is encoded by the original automaton.\nThe last remaining row simply ensures that reading in the eos symbol results in the NO-OP action\nbeing executed (eos actions are not encoded by the original pushdown automaton).\nThe acceptance component.\nLastly, the acceptance component is controlled by the following\nparameters:\nUACCEPT,A “ ´10 for A P tOPPUSH,0, OPPUSH,1, OPPOP,0, OPPOP,1, OPNO-OPu\n(5.110)\nUACCEPT,CONFγ,y “ ´10 for γ P tε, 0, 1u, y P teos, a, bu\n(5.111)\nbACCEPT “ 1\n(5.112)\nThe entry bACCEPT ensures that, by default, the value of ACCEPT is set to 1. However, the other\nparameters ensure that, as soon as any part of the configuration is not compatible with the acceptance\nstate (the read symbol is not eos or the stack is not empty), the acceptance bit is turned off.\nA full proof of the theorem would now require us to show formally that the update rule\nEq. (5.84) results in the correct transitions in the PDA. We, however, leave the proof with the\nintuitive reasoning behind the setting of the parameters and leave this as an exercise for the\nreader. The proof is also demonstrated in the python implementation of the constructions here:\nhttps://github.com/rycolab/rnn-turing-completeness.\n■\nThe construction described in the proof of Theorem 5.2.8 is demonstrated in the following\nexample.\nExample 5.2.6: Siegelmann’s construction\nLet P be a single-stack PDA presented in Fig. 5.16. We now simulate the recognition of the\nstring y “ ab, which is accepted by P. The initial state h0 has a single non-zero cell, STACKε.\nThe four phases of the processing of the first input symbol a are shown in Tab. 5.1. The four\nphases of the processing of the second input symbol b are shown in Tab. 5.2.\nTheorem 5.2.8 shows that Elman RNNs are theoretically at least as expressive as deterministic\nCFGs. We now return to the main result of this subsection: the Turing completeness of RNNs.\nLuckily, Theorem 5.2.8 gets us most of the way there! Recall that by §4.2.9, two-stack PDA\nare Turing complete. We make use of this fact by generalizing the construction in the proof of\nTheorem 5.2.8 to the two-stack case. This will prove that RNNs can in fact simulate any Turing\nmachine, and are, therefore, Turing complete.\nLemma 5.2.17\nLet P “ pQ, Σ, Γ, δ, pqι, γι, σιq, pqφ, γφ, σφqq be a two-stack pushdown automaton. Then, there\nexists an Elman RNN R simulating P, i.e., L pRq “ L pPq.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n201\nInitial state\nPhase 1\nPhase 2\nPhase 3\nPhase 4\nSTACK\n0\n0\n2\n5\n2\n5\n1\n10\nBUFF1\n0\n0\n0\n2\n5\n2\n5\nBUFF2\n0\n0\n0\n0\n2\n5\nSTACKε\n1\n1\n1\n0\n0\nSTACK0\n0\n1\n1\n0\n0\nSTACK1\n0\n0\n0\n1\n1\nCONFeos,a\n0\n0\n1\n1\n0\nCONFeos,b\n0\n0\n0\n0\n0\nCONF0,a\n0\n0\n0\n0\n0\nCONF0,b\n0\n0\n0\n0\n0\nCONF1,a\n0\n0\n0\n0\n1\nCONF1,b\n0\n0\n0\n0\n0\nCONFε,eos\n0\n0\n0\n0\n0\nCONF0,eos\n0\n0\n0\n0\n0\nCONF1,eos\n0\n0\n0\n0\n0\nOPPUSH,0\n0\n1\n10\n1\n10\n1\n10\n1\n10\nOPPUSH,1\n0\n3\n10\n3\n10\n0\n0\nOPPOP,0\n0\n0\n0\n0\n0\nOPPOP,1\n0\n0\n0\n0\n0\nOPNO-OP\n0\n0\n0\n0\n0\nACCEPT\n0\n1\n0\n0\n0\nTable 5.1: The simulation of the processing of the first symbol a by the RNN simulating the PDA\nin Fig. 5.16. After the fourth phase, the stack cell contains the encoding of the stack as 0.1.\n202\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nInitial state\nPhase 1\nPhase 2\nPhase 3\nPhase 4\nSTACK\n1{10\n1{10\n1\n2{5\n0\nBUFF1\n2{5\n1{10\n1{10\n1\n2{5\nBUFF2\n2{5\n2{5\n1{10\n1{10\n1\nSTACKε\n0\n0\n0\n0\n0\nSTACK0\n0\n1\n1\n0\n0\nSTACK1\n1\n0\n0\n1\n1\nCONFeos,a\n0\n0\n0\n0\n0\nCONFeos,b\n0\n0\n0\n0\n0\nCONF0,a\n0\n0\n0\n0\n0\nCONF0,b\n0\n0\n1\n1\n0\nCONF1,a\n1\n0\n0\n0\n0\nCONF1,b\n0\n1\n0\n0\n1\nCONFε,eos\n0\n0\n0\n0\n0\nCONF0,eos\n0\n0\n0\n0\n0\nCONF1,eos\n0\n0\n0\n0\n0\nOPPUSH,0\n1{10\n0\n0\n0\n0\nOPPUSH,1\n0\n0\n0\n0\n0\nOPPOP,0\n0\n0\n0\n0\n0\nOPPOP,1\n0\n1\n0\n0\n0\nOPNO-OP\n0\n0\n2{5\n0\n0\nACCEPT\n0\n0\n0\n0\n0\nTable 5.2: The simulation of the processing of the second symbol b by the RNN simulating the PDA\nin Fig. 5.16. After the fourth phase, the stack cell contains the encoding of the empty stack.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n203\nq\na, ε Ñ 0\na, 0 Ñ 1\na, 1 Ñ ε\nb, ε Ñ 1\nb, 0 Ñ ε\nb, 1 Ñ 1\nFigure 5.16: The single-stack pushdown automaton P.\nProof. Again, given a two-stack PDA P “ pQ, Σ, Γ, δ, pqι, γι, σιq, pqφ, γφ, σφqq with Σ “ ta, bu,\nΓ1 “ t0, 1u, and Γ2 “ t0, 1u, we construct an Elman RNN R “ pΣ, D, U, V, E, bh, h0q which\nrecognizes the same language as P.\nThe hidden state of R will contain the same components as in the proof of Theorem 5.2.8.\nMoreover, their dynamics will be exactly the same—they will simply be larger to account for more\npossible configurations of the two stacks together. For example, the top of the stack component will\nnow consist of cells STACKγ1γ2 for γ1 P Γ1 and γ2 P Γ2 flagging that the top symbol on stack 1 is γ1\nand the top symbol of stack 2 is γ2. Furthermore, the configuration component will contain cells\nof the form OPaction,γ1γ2 with an analogous interpretation. Lastly, all the computation and data\ncomponent cells would be duplicated (with one sub-component for each of the stacks), whereas the\nacceptance component stays the same. We now again describe these components and their dynamics\nintuitively, whenever there is any difference to the single-stack version.\nData component.\nInstead of having a single queue of data cells, R now has two queues, one\nfor each of the stacks. The first queue will be formed by STACK1, BUFF11, and STACK21, and the\nsecond one by STACK2, BUFF12, and STACK22. Each of the queues acts exactly the same as in the\nsingle-stack version, and they act independently based on the computations done in the computation\ncells of the respective stacks. Each of the stacks is also encoded as a numeric sequence in the same\nway as in the single-stack version.\nTop of stack component.\nAgain, R starts in an initial state in which the cell STACKεε is 1 and\nall others are 0. The individual cells of this component then get updated according to the top\nsymbols on the stacks encoded in the STACK1 and STACK2 cells.\nConfiguration component.\nThe cells of the configuration component combine the pattern\ncaptured by the top of both stack components with the input symbol at the current time step to\nactivate only the appropriate cell CONFγ1γ2,y.\nComputation component.\nAgain, the computation component contains the cells in which the\nresults of all the possible actions on both stacks are executed. They execute the actions on both\nstacks independently.\nAcceptance component.\nThe acceptance component functions identically to the single-stack\ncase.\n204\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nUsing these components, the RNN then transitions between the phases exactly like in the\nsingle-stack case. We leave the specifications of the matrix parameter values to the reader. They\nagain follow those presented in the single-stack case but treat the transitions and configurations of\nboth stacks.\n■\n5.2.5\nThe Computational Power of RNN Variants\nMost of the current section was devoted to understanding the computational power of simple Elman\nRNN language models due to their simplicity which allows an easy connection to formal models\nof computation. However, as mentioned in §5.1.5, gated RNN variants such as LSTMs and GRUs\nhave become the standard in modern natural language processing tasks, showing more better and\nmore reliable performance on a variety of tasks. Interestingly, besides their better resilience to the\nvanishing gradient problem, LSTM-based language models are also provably more expressive than\nsimple RNN language models. On the other hand, the simpler GRU-based language models are in\nsome ways only as powerful as simple Elman RNN language models. To give an intuition behind\nthis, this subsection provides a short overview the results considering the computational power of\nRNN variants.\nWeiss et al. (2018) compare the practical computational power of different RNN variants under\nthe constraint of bounded computation time and limited precision. Interestingly they empirically find\nthat LSTMs can learn to recognize languages that require some form of counting, like tanbn | n P Nu\nor tanbncn | n P Nu, while GRUs struggle to do so. This invites the comparison of LSTMs with\ncounter machines (Hopcroft et al., 2006; Fischer et al., 1968), a class of formal computational\nmodels with the ability to count. Simply put, counter machines are finite-state automata with an\nadditional (unbounded) counter cell, which they can manipulate by incriminating and decrementing\nthe value stored in it.18 Counter machines present an interesting addition to the traditional hierarchy\nof computational models, since they in some ways cut across it, being able to recognize some, but\nnot all, context-free languages, while also being able to recognize some context-sensitive languages.\nAmong others, they can for example recognize the context-free language tanbn | n P Nu and the\ncontext-sensitive language tanbncn | n P Nu, while not being able to recognize the Dyck context-free\nlanguages Dpkq with k different parenthesis types (intuitively, this is because recognizing Dyck\nlanguages requires keeping track of the order in which the parantheses appeared, not only their\ncounts).\nCounter machines can recognize languages like tanbncn | n P Nu, by counting the number of as\nand making sure that it matches the number of bs. Further, analyzing the activation of the memory\ncell and of the hidden state, they find that LSTMs that one can recognize the use of a counting\nmechanism implemented by the network by using one or more dimensions of the memory cell as\ncounters. This result is particularly interesting as GRUs are often considered an equivalent variant\nto the LSTM one, with the same computational power, but smaller computation overhead. However,\nGRUs seem to lack this counting behavior, which can be backed by theoretical analysis.\nMerrill et al. (2020) take a look at the computational power of the different RRN variant from\nanother perspective. They consider space complexity and whether the networks are rationally\nrecurrent, i.e. whether their hidden state update function can be expressed in terms of finite state\nmachine computations. Making the assumption of saturated networks19, they find that while GRUs\n18We only consider counter machines with a single counter cell.\n19Informally, a saturated network is a neural network in which the norms of the parameters are taken to 8. This\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n205\nFigure 5.17: Picture from Weiss et al. (2018): Plot of the activation value of the memory cell (LSTM)\nand of the hidden state (GRU), versus the indices of the input. The Networks have been trained\nto recognize either the languages tanbn | n P Nu or tanbncn | n P Nu. As you can notice, in both\ncases the LSTM has learned to use one or more dimension of the memory cell as a counter, which\nallows it to count how many a and b have been have been consumed so far. Conversely, the GRU\nhasn’t developed this counter mecahnism, and in fact empirical evidence shows that it struggles to\nto recognize the described languages.\nand Elman networks are rationally recurrent and therefore at most regular, LSTMs are not, meaning\nthat their hidden state update function cannot be expressed by means of finite-state machines.\n5.2.6\nConsequences of the Turing completeness of recurrent neural net-\nworks\nThe section above outlines Siegelmann and Sontag’s (1992) construction of encoding a Turing\nmachine in an RNN. While Turing completeness means that RNNs are in many ways computational\nvery powerful (as powerful as they can be), it also brings with it many computational challenges faced\nwhen working with Turing machines. Computability theory, for example, defines many problems\nrelated to Turing machines and their properties which are not computable, or undecidable, meaning\nthat no algorithm (or, equivalently, a Turing machine) can solve them.20 The most classical and\nhas the effect of pushing all the squashing functions of the network to one of their extreme values, in the case of the\nsigmoid t0, 1u and t´1, 1u in the case of the hyperbolic tangent\n20The notion of solving a problem computationally is quite nuanced and we only provide very broad intuitions\nfor now. Readers who want to delve deeper into the topic of computability and with it the implications of Turing\ncompleteness are encouraged to look at some classical texbooks on the material, for example Sipser (2013, Part Two).\n206\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nfundamental of such problems is the halting problem (Turing, 1937).\nDefinition 5.2.16: Halting problem\nLet M be a Turing machine over the input alphabet Σ and y P Σ˚. The halting problem is\nthe problem of deciding whether M (halts and) accepts y.a\naA formal definition of the halting problem would require us to define the formal language L of Turing\nmachine and input tuples pM, yq and asking if there exists a Turing machine M1 that accepts L. However, to\nkeep the discussion brief, we define the problem more intuitively.\nThe halting problem is the foundation of the results presented by Chen et al. (2018), which\nbuilding on Siegelmann and Sontag (1992) considers its implications on the theory of RNN language\nmodels. For example, they show that determining many practically useful properties of general\nrationally weighted RNNs is undecidable. Such properties include the tightness of a general RNN,21\nthe equivalence of two RNN language models, the minimal size (in the size of the hidden state) of\nan RNN defining the same language model as some given RNN, and the highest probability string\nin an RNN, i.e., argmaxyPΣ˚ pLN pyq. By simulating a Turing machine, we can encode the halting\nproblem as solving any of those tasks.22. This means that if we could solve these problems for RNNs,\nwe could also solve the halting problem, which is provably impossible, meaning that these problems\nare not computable either. We briefly outline the individual findings of Chen et al. (2018) below,\nsketching the rough idea behind the proofs.23\nTheorem 5.2.9: Tightness\nDetermining tightness of an RNN language model is undecidable.\nProof. Note first that not all RNNs are tight language models.\nAs mentioned, this is not a\ncontradiction to the results from §5.1.3, which considered softmax-projected RNN language models.\nIndeed, the RNN we constructed in §5.2.4 is not such an RNN. That construction shows that, given\nan arbitrary Turing machine M and input y, we can construct an RNN that simulates M running\non y. Using this construction, we can reduce the problem of deciding the tightness of a general\n(rational weighted Elman) RNN to the halting problem, i.e., the problem of answering “Does M\nhalt on y?”. We do that by constructing an RNN that simulates the given Turing machine M on\nthe input y, ending generation if M halts, and, at the same time, produces strings according to a\ndistribution that is tight for finite length strings, on the condition that no infinite length strings can\nbe produced. Now if and only if M halts on y, the language model is tight. Therefore, deciding is\nat least as hard as solving the halting problem, and since that is undecidable, so is tightness.\n■\nTheorem 5.2.10: Highest weighted string\nFinding the highest weighted string of an RNN LM is undecidable.\nProof. Once more, we reduce the halting problem to the given task on the RNN. The idea behind\nthis is to again simulate an arbitrary Turing machine by constructing an RNN LM which is not\n21Note that the results from §5.1.3 consider only RNN LM with the softmax projection function.\n22In computability theory, this is known as reducing the halting problem to the given one\n23The interested reader is encouraged to see the original paper for the details.\n5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS\n207\ntight unless the simulated Turing machine halts. One can create such an RNN with a one-symbol\noutput alphabet such that its highest-weighted output is an infinite string. Now, if we again enforce\nthat the RNN ends producing outputs once the simulated Turing machine halts, it can be shown\nthat there exists a language in which each string has a probability of less than 0.12 if and only if the\nTuring machine does not halt. On the other hand, if the Turing machine does halt after T steps,\nproducing a string which has length 3T ´ 5 has a probability of ě 0.25. Therefore, the weight of the\nhighest probability string depends on the question of whether the simulated Turing machine halts,\nwhich is undecidable.\n■\nTheorem 5.2.11: Equivalence\nEquivalence between two RNN LMs in the sense of defining the same language model is\nundecidable.\nProof. The proof of this claim is again a reduction from the halting problem. We construct an RNN\nwhich simulates a given arbitrary Turing machine until it halts and has the same outputs as some\nother RNN. As soon as the Turing machine halts, the outputs differ, so the RNNs are equivalent if\nand only if the Turing machine does not halt. Hence, equivalence is undecidable.\n■\nTheorem 5.2.12: Minimization\nFinding the RNN with the minimum number of hidden layer neurons defining the same\nlanguage model as a given RNN LM is undecidable.\nProof. We can reduce the halting problem to the following problem: For some RNN LM and an\ninteger D, return yes if there is another RNN LM with ď D hidden units that generates the same\nweighted language. Assume that there is a Turing machine M that can decide this problem. Now,\nfor another Turing machine M1 and input y, construct a one symbol RNN LM, R, that simulates\nM1 running on y and stops generating if M1 halts. We assume without loss of generality that\nM1 runs for more than one computation step. Now we run M on the input pR, 0q, which checks\nwhether there is another RNN LM generating the same weighted language as R and has no hidden\nunits. Having no hidden units means that the output probabilities of each symbol would have to\nbe constant for each time step. If M decides minimization returns true, that means the output\nprobabilities of R cannot change over time, which means that M1 has to run forever. Conversely, if\nM returns false, the output probabilities change when M1 halts. Therefore, M deciding the minimal\nhidden states problem on pR, 0q is equivalent to it deciding the Halting problem for pM1, yq.\n■\nThis concludes our investigation of the formal properties of recurrent neural language models.\nThe sequential nature of the architecture and the relatively simple transition functions in the vanilla\nRNN architectures made the link to automata from formal language theory relatively straightforward,\nwhich allowed relatively strong theoretical insights. However, it was exactly this sequential nature\nand the issues associated with it of RNNs that eventually led to them being overtaken by another\nneural architecture, which is now at the core of most if not all, modern state-of-the-art language\nmodels: the transformer.24 We introduce them and discuss their theoretical underpinnings in the\nnext section.\n24We will not discuss the issues with the training speed and parallelization of RNNs in detail. Some of these issues\nwill be highlighted in the latter parts of the course.\n208\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\n5.3\nTransformer-based Language Models\nIn the previous section (§5.1.2), we introduced and studied RNN language models as of language\nmodels capable of storing an arbitrarily long context in its encoding enc pyătq by updating their\nhidden state ht an arbitrary number of times. As we saw in their theoretical analysis, this mechanism\ngives them, under some assumptions, a lot of expressive power. Nevertheless, as we also discussed,\nRNN LMs come with their distinct set of drawbacks. Some of them, e.g., the exploding and vanishing\ngradient problems, can be amended using specific mechanisms resulting in more complex recurrent\nneural networks, such as LSTMs and GRUs (cf. §5.1.5). As discussed in §5.1.5, a more fundamental\nissue that cannot be avoided is the difficulty of parallel training, which is particularly noticeable on\nthe vast internet-scale corpora used nowadays to train language models. Can we do anything about\nthat? As discussed, the inherently sequential nature of RNNs suggests strict limits on this front.\nMotivated by this limitation, in this section, we present a newer architecture that took over the field\nof language modeling (and NLP in general)—transformers (Vaswani et al., 2017). It was originally\nintroduced for machine translation, but it can easily be applied to language modeling and has led to\nthe success of models such as GPT-n.\nThe structure of this section will be a bit different from that of the other sections in the notes.\nWe will first give a formal definition of a transformer model in §5.3.2 and, based on this definition,\nderive a number of results analogous to those for RNN LMs from §5.2. However, due to the current\npractical relevance of transformers in language modeling, we then devote a significant portion of the\nsection to more practical aspects of transformer models and introduce a number of modifications\nused in modern language modeling systems.\n5.3.1\nInformal Motivation of the Transformer Architecture\nBefore we introduce transformers, let us consider another practical drawback of RNN LMs, which\nwill give us more clues on how to improve them and motivate the architectural decisions behind\ntransformers. Luckily, the simplest patch-up of this issue will also lend itself naturally to paral-\nlelization, as we will see shortly. The main characteristic of RNNs is the use of a single hidden\nstate ht to represent an arbitrary prefix of any string yăt up to the current time step t. While this\nallows RNNs to model strings of any length, it also means that arbitrarily long strings must be\ncompressed into this hidden vector of fixed size. Intuitively, this becomes increasingly difficult as\nthe length of the context grows: As the amount of information to be compressed into the hidden\nstate increases with the prefix length, the hidden state may struggle to model the entirety of the\npreceding context. How can we amend that? The simplest na¨ıve way to go about this is to retain\nthe contextual encodings of all prefixes of the string. In this case, it is actually more natural to\ntalk about contextual encodings not of full prefixes, but simply of the individual symbols in the\nstring.25 Here, contextual means that the symbol encodings are augmented with the information\nfrom the rest of the string (in most cases, about the preceding context, as with the hidden states of\nan RNN). With this, we avoid the need to summarize the entire context into a single state. Note\nthat our infinite-precision RNNs from the previous section implicitly did that as well—for example,\nby storing the information in the “stack” neurons, they could, in principle, store the entire history of\nthe string. However, storing all the encodings explicitly makes their utilization more direct and thus\neasier. This of course leaves the model with the issue of remembering increasingly large amounts of\n25For example, looking back to RNNs, we could consider ht to simply be an encoding of the symbol yt augmented\nwith the information from yăt.\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n209\ny0\ny1\ny2\ny3\ny4\nh0\nh1\nh2\nh3\nh4\nState ht\nInput yt\n¨ ¨ ¨\n¨ ¨ ¨\nFigure 5.18: An abstract depiction of how a transformer language model produces the contextual\nembeddings of all symbols in a string. The hidden state ht can “attend to” (the precise meaning of\nthis term will be introduced soon in §5.3.2) all preceding symbols yăt and the current symbol yt.\ninformation as the length of the context increases, but we will, for the moment, assume that we can\nalways store enough information to process any string in this way.\nHaving decided to keep around the encodings of all symbols in the string, let us think about\nparallelizing the process of encoding a string, i.e., computing enc pyq. Remember the very general\nway in which RNNs build a representation of the string yăt by incrementally modifying ht, which\nis illustrated in Fig. 5.1a—this incrementality brings with it all the challenges of impossible par-\nallelization. The workaround for the issues with the sequential processing of RNNs is to process\nthe context for each yt independently, without relying on the encodings of the previous symbols,\nthus avoiding the sequential bottleneck. Nevertheless, we still want the contextual encoding of yt to\ncontain information about the rest of the string, i.e., the preceding context. How can we achieve that\nwithout relying on recurrence? Again, we grab onto the simplest solution: to compute the symbol\nencodings for each symbol yt “from the ground up” based only on the static symbol encodings e1pytq,\nwhich do not require any recurrence. This is abstractly illustrated in Fig. 5.18, whereas Fig. 5.19\nshows how this translates into the generative framework from Definition 3.1.11, where individual\nsymbols yt are both sequentially generated based on the encodings of the preceding context enc pyătq\nas well as used to build the representation of the context in the next time step enc pytq. Notice that\ninstead of containing arcs denoting dependencies between the symbol encodings (“hidden states”)\nht, Fig. 5.18 and Fig. 5.19 contain arcs connecting each ht to all symbols yj for all j ď t. Compare\nthis to Fig. 5.1a, where the arcs between ht´1 and ht induce the temporal dependence, and carry\nthe information about the symbols yj for all j ď t.\nClearly, this avoids the issues faced by RNNs due to their sequentiality.\nHowever, it also\nintroduces more work required to compute the individual contextual encodings from the static\nsymbol representations. The operations performed to do this by transformers, which are together\nknown as the attention mechanism, are introduced in the next subsection. They represent\npossibly the most important component of the entire structure of the transformer—by “attending”\nto relevant preceding symbols when computing the symbol encodings (i.e., using them to compute\nenc pyătq), the transformer can model long-range dependencies very effectively, and use them for\nappropriately modeling the distribution over the next word.\nTo recap, in this section, we informally motivated the new architecture, transformers, with the\ngoals of 1. remembering the contextual encodings of all symbols explicitly and 2. parallelizing the\ncomputation of the contextual symbol encodings. The next subsection formally introduces the\narchitecture, before we dive into their theoretical properties.\n210\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\ny0\ny1\ny2\ny3\nh0\nh1\nh2\nState ht\nInput/Output yt\n¨ ¨ ¨\n¨ ¨ ¨\ny1 „ pSM p¨ | h0q\ny2 „ pSM p¨ | h1q\ny3 „ pSM p¨ | h2q\nFigure 5.19: An abstract depiction of how a transformer language model generates a string one\nsymbol at a time. The hidden state ht can attend to all previously generated symbols yăt to sample\nthe next symbol yt. The dotted lines denote the sampling steps.\n5.3.2\nA Formal Definition of Transformers\nHaving informally introduced the main two ideas behind the transformer architecture in the previous\nsubsection, we now provide a formal definition of a transformer model, which we will then augment\nwith more practical considerations in .\nDefinition 5.3.1: Transformer network\nA transformer network T is a tuple pΣ, D, encT q where\n• Σ is the alphabet of input symbols,\n• D is the dimension of T , and\n• encT is the transformer encoding function (cf. Definition 3.1.7), which we define in more\ndetail below (Definition 5.3.2).\nFrom afar, the definition of a transformer network is therefore relatively simple; it is stated to\nmake the transformer models fit well into our representation-based locally normalized language\nmodeling framework (cf. Definition 3.1.11). The complexity of the models of course comes from the\ndefinition of the transformer encoding function encT , to which we devote the rest of the section.\nContinuing in the framework of representation-based locally normalized language models, the\nhidden states of the transformer play an analogous role to those of RNNs, with the only difference\nbeing how they are computed.\nDefinition 5.3.2: Transformer hidden state\nLet T “ pΣ, D, encT q be a transformer network. The hidden state ht P RD describes the state\nof T after reading yďt. It is defined with respect to the transformer encoding function encT\nas follows:\nht\ndef\n“ encT pyďtq\n(5.113)\nCrucially, as we will see shortly, the hidden state ht of the transformer does not have any\ndependence on the preceding hidden states themselves (although, as we will see, it is partially a\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n211\nfunction of the same inputs).\nAs hinted above, with this, we can easily fit transformers into the representation-based locally\nnormalized language modeling framework and define a sequence model based on the model.\nDefinition 5.3.3: Transformer sequence model\nLet T be a transformer network and E P R|Σ|ˆD a symbol representation matrix. A D-\ndimensional transformer sequence model over the alphabet Σ is a tuple pΣ, D, encT , Eq\ndefining the sequence model of the form\npSM pyt | yătq\ndef\n“ softmaxpE ht´1qyt “ softmaxpE encT pyătqqyt\n(5.114)\nNow that we have unified the transformer T to the theoretical framework introduced so far in\nthe course, we can jump in and look at the internal structure of the transformer encoder function,\nwhich is where the novelty of the transformer architecture comes from.\nThe Attention Mechanism\nAs we mentioned in the informal motivation, to avoid over-compressing information about sentences\ninto a single vector, a transformer model retains the encodings (captured in the hidden states ht) of\nall possible prefixes of the string, which we can equivalently simply regard as encodings of individual\nsymbols augmented with the information from the preceding string (see Fig. 5.18).26 However,\nrather than computing the encodings sequentially like an RNN, the encodings of the individual\nsymbols are computed with the so-called attention mechanism.\nDefinition 5.3.4: Attention\nLet f : RD ˆ RD Ñ R be a scoring function and f∆D´1 a projection function. Furthermore,\nlet q P RD, Kt “\n`\nkJ\n1 , . . . , kJ\nt\n˘\nP RtˆD and Vt “\n`\nvJ\n1 , . . . , vJ\nt\n˘\nP RtˆD.\nAttention over Kt, Vt, also denoted by Att pqt, Kt, Vtq : RD ˆ RtˆD ˆ RtˆD Ñ RD is a\nfunction computing the vector a in the following two-step process:\nst “ ps1, . . . , stq\ndef\n“ f∆D´1 pf pq, k1q , f pq, k2q , . . . , f pq, ktqq\n(5.115)\nat “ Att pq, Kt, Vtq\ndef\n“ s1v1 ` s2v2 ` ¨ ¨ ¨ ` stvt\n(5.116)\nq, K, and V are commonly referred to as the query, keys, and values of the attention\nmechanism, respectively. We talk about the parameters q, K, and V completely abstractly for now.\nHowever, to help you connect this to the representation-based language modeling framework, note\nthat q will later correspond to a query representing an individual symbol yt, whereas K and V will\ncontain the information from yăt used to compute ht.\nWhat the attention function computes.\nThe scoring function f is, abstractly, simply a\nparameter of the model which we can choose freely. Intuitively, it should express the relevance of a\nparticular key k to the query q—the more the key is relevant to the query, the more “attention” the\n26From now on, we will talk about contextual symbol encodings, which simply refers to the hidden states\ncorresponding to individual symbols.\n212\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nmodel will put to the value associated to that key. The projection function f∆D´1 then transforms\nthe computed scores ensuring that the transformed scores sum to 1.27 The vector of transformed\nscores s (Eq. (5.115)) is then used to compute the result of the attention function—the vector a. a\nis a convex combination of the values v passed to the attention function. Abstractly, therefore, the\nkeys contain the information used for “indexing” the values with the specific query.\nThe scoring function.\nAs mentioned, the scoring function is supposed to measure the “relevance”\nof a particular value for a query q through the values’ key. The most common choice for f is the dot\nproduct between query and key, which is often scaled by the square root of the vector dimensionality:\nf pq, kq “\n1\n?\nD\nxq, ky\n(5.117)\nThe projection function and soft and hard attention.\nThe projection function used to\ntransform the un-normalized attention scores is a crucial component of the transformer model. By\nfar the most commonly used projection function is again the softmax. In this case, the attention\nfunction is referred to as soft attention.\nDefinition 5.3.5: Soft attention\nThe soft attention is computed with the projection function f∆D´1 “ softmax.\nHowever, the softmax again makes the models difficult to analyze. In our voyage to theoretically\nunderstand transformer-based language models, we will, therefore, again make specific (less frequently\nused) modeling choices, particularly in the case of the projection function.\nIndeed, to be able to derive any interesting expressivity results (see §5.4), we jump to the other\nside of the spectrum and define hard attention. Simply put, instead of spreading the attention across\nall values like softmax, hard attention puts all the mass on the element whose key maximizes the\nscoring function f. One way to arrive at it from the definition of soft attention is by sending the\ntemperature τ in the definition of the softmax function (cf. Definition 3.1.10) to 0. Recall that that\nresults in the output vector representing a uniform distribution over the elements that maximize the\ninput vector. This is known as averaging hard attention.\nDefinition 5.3.6: Averaging hard attention\nThe averaging hard attention is an attention mechanism with the projection function\nf∆D´1 “ hardmaxavg, where hardmaxavg is defined as:\nhardmaxavgpxqd\ndef\n“\n#\n1\nr if d P argmaxpxq\n0 otherwise\n, for d “ 1, . . . , D\n(5.118)\nwhere x P RD and r “ | argmaxpxq| is the cardinality of the argmax set over x.\nInterestingly, there is another form of hard attention that results in a model with a different\nexpressive capacity: the unique hard attention. The difference lies exactly in how it handles ties\n27While the fact that the transformed scores sum to one invites their interpretation as probabilities, this is not\ntheir central role. Rather, the weights are simply used to define a convex combination of the values.\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n213\nin the elements which maximize the scoring function. Unique hard attention chooses only a single\nelement of those that maximize the score: it can be chosen randomly or deterministically (e.g.,\nalways the first one).\nDefinition 5.3.7: Unique hard attention\nThe unique hard attention is an attention mechanism with the projection function f∆D´1 “\nhardmaxuni, where hardmaxuni is defined as follows. For x P RD, sample ˆd „ Unif pargmaxpxqq\nor choose some ˆd P argmaxpxq deterministically. Then\nhardmaxunipxqd\ndef\n“\n#\n1 if d “ ˆd\n0 otherwise\n, for d “ 1, . . . , D.\n(5.119)\nWhile the difference between unique and averaging hard attention might seem subtle and marginal,\nit actually results in a large difference in the expressivity of transformer-based language models as\nwe discuss in §5.4. While we will investigate this in a lot more detail there, we just mention that the\nintuition behind the expressive difference is relatively straightforward: while the keys maximizing\nthe un-normalized scores might be the same (even though they necessarily don’t have to be if f is\nnot injective), the values (whose content is decoupled from the keys) that those keys index might\nnot be—and in some cases, all those different values might be relevant for the task at hand. Unique\nhard attention always allows us to only “lookup” a single value associated with those keys, no matter\nhow “different” and relevant all of those are. It also does not allow the attention mechanism to\nsum over (“summarize”) across all the elements that maximize attention. This is a very limiting\ncharacteristic, as many of the expressivity results that we will see later rely on summing over all the\nelements that maximize the attention scores.\nTransformer Blocks\nWe have, so far, described the “low-level” details of how the attention function is computed. We now\ncombine the computations performed into larger blocks, showing how they are used to compute the\nstring-augmented encodings of the individual symbols. In particular, we have to connect the concepts\nof queries, keys, and values to the symbols and their (initial, static) encodings. Intuitively, this is\ndone by transforming the static encodings of those symbols using specific functions implemented\nthrough the attention mechanism and using the transformed encodings as queries, keys, and values,\nas we describe below.\nWe first abstract the attention mechanism from Definition 5.3.4 a bit. With this, we will, in a\nfew steps, arrive at exactly how the hidden states ht or the contextual encodings are computed in a\ntransformer. At the core of this computation lies a repeated application of the same sequence of\noperations, which, as we will see, augment the “current version” of the contextual encodings with\nthe current information from the preceding information. We call a single sequence of operations a\ntransformer layer.\n214\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nDefinition 5.3.8: Transformer layer\nLet Q, K, V , and O be parametrized functions from RD to RD.\nA transformer layer is a function T: RT ˆD Ñ RT ˆD that takes as input sequence of vectors\nX “ pxJ\n1 , xJ\n2 , . . . , xJ\nT q and returns Z “ pzJ\n1 , zJ\n2 , . . . , zJ\nT q P RT ˆD according to the following\nsteps:\nat “ Att pQpxtq, KpXtq, V pXtqq\nloooooooooooooooomoooooooooooooooon\nDefinition 5.3.4\n`xt\n(5.120)\nzt “ Opatq ` at\n(5.121)\nfor t “ 1, . . . , T, so that TpXq\ndef\n“ Z “ pzJ\n1 , zJ\n2 , . . . , zJ\nT q P RT ˆD.\nWhile we defined the transformer layer on a general matrix (with T columns), note that these\nT vectors will refer to the (current) symbol encodings of the symbols in the string up to the T th\nsymbol, i.e., yďT .\nWhat do these quantities correspond to? Eqs. (5.120) and (5.121) outline a two-step process\nof computing the outputs of a single transformer layer: X “ pxJ\n1 , xJ\n2 , . . . , xJ\nT q represents the input\nto the layer, which T transforms into the output sequence Z “ pzJ\n1 , zJ\n2 , . . . , zJ\nT q. Before being fed\ninto the attention mechanism, the inputs X are first transformed into the quantities required by the\nattention mechanism: the query qt (a single one for each xt), the matrix of keys Kt, and the matrix\nof values Vt—all of these are, therefore, transformations of the input sequence of vectors. The\ntransformations Q, K, and V determine how these inputs are transformed into the (interpretable)\nquantities required by the attention mechanism.\nThe individual at represent the “intermediate” results of the computation—the results of applying\nthe actual attention mechanism (with a slight modification) from Definition 5.3.4 onto the produced\nvalues of the query, the keys, and the values.\nThe modification mentioned refers to the addition of the inputs xt to the output of the attention\nmechanism in Eq. (5.120). This mechanism is known as adding residual connections to the model.\nFirst introduced by He et al. (2016) in the context of deep convolutional neural network-based\narchitectures, residual connections are now a common feature in many state-of-the-art deep learning\narchitectures. Note that their use is mostly motivated by empirically better performance—this is\noften attributed to the fact that, intuitively, residual connections allow gradients (i.e., learning signals)\nto flow through the network through a more direct route rather than all layers that can “squish” the\nsignal similarly to the Elman RNN case (in that sense, they help mitigate the vanishing gradient\nissue). However, as we will see later in our analysis, residual connections will also play an important\nrole in determining the theoretical properties of transformers, particularly their computational\nexpressive power. The same mechanism is applied in the second step of the transformer layer, where\nthe intermediate results at are transformed by the output transformation O into the final outputs of\nthe layer.\nIn the simplest case, you can imagine the inputs X to be the initial static embeddings of the\nsymbols. The application of the transformer layer, in this case, therefore, “selectively” (determined by\nthe attention mechanism) augments the static embeddings with the information from the preceding\ncontext. However, as we will see shortly, a transformer model will apply multiple transformer\nblocks to the input sequence and thus transform it in multiple steps, analogously to how layers are\ncomposed in a regular feed-forward neural network. In that sense, the inputs to the transformer\nblocks will refer to general intermediate representations produced from the initial static embeddings\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n215\nafter some number of applications of transformer layers.\nLastly, let us consider how the current symbol representations X are transformed into the queries,\nkeys, and values using Q, K, and V ? The original formulation (Vaswani et al., 2017) and all standard\nimplementations of the transformer architecture use one of the simplest possible mappings: a linear\ntransformation implemented by matrix multiplication. On the other hand, the final output of the\ntransformer, computed with output mapping O, is usually implemented by a multi-layer perceptron.\nThe transformer layer puts the attention mechanism into a functional block that describes how\na sequence of current symbol representations is transformed in a single step into a sequence of\nrepresentations augmented with the information in the current set of values. However, we are not\ndone abstracting yet! As mentioned, this process can be applied arbitrarily many times, resulting\nin “deep” encodings of individual symbols, which contain information from the preceding symbols\nin the string computed in a composite way. This also answers the question of how the augmented\nsymbol representations used in the language modeling formulation are computed from the initial\nsymbol representations: they are the result of multiple applications of the transformer layer to the\nmatrix of initial symbol representations. That is: multiple transformer layer layers are stacked on\ntop of one another so that the output of one layer becomes the input of the next.\nWe now have all the building blocks to define the full transformer architecture, which computes\nthe encodings of the string prefixes (and thus the hidden states) in Definition 5.3.3.\nDefinition 5.3.9: Transformer\nFor L P N, we define a L-layer transformer model as a D-dimensional transformer sequence\nmodel over an alphabet Σ where the hidden state ht\ndef\n“ encT py1 . . . ytq “ encT pyq is computed\nas follows.\nX1 def\n“\n`\ne1py0q, e1py1q, . . . , e1pytq\n˘\n(5.122)\nZℓ“ TℓpXℓq for 1 ď ℓă L\n(5.123)\nXℓ`1 “ Zℓfor 1 ď ℓă L\n(5.124)\nht “ FpzL\nt q\n(5.125)\nTℓfor ℓ“ 1, . . . , L represent L different transformer layers with decoupled parameter (cf.\nDefinition 5.3.8).\nF : RD Ñ RD is a transformation function applied to the contextual\nencoding of the last symbol in the last (Lth) layer and e1 : Σ Ñ RD is a symbol representation\nfunction computing the initial representations of the symbols passed to the first layer of the\ntransformer.a\naThe symbol representation function e1 is often also implemented as a linear transformation of the one-hot\nrepresentations (cf. Definition 5.1.5) of symbols, i.e., it is simply a table-lookup.\nWith this, the transformer model now fully specifies how to compute the representations\nrequired for the representation-based locally normalized sequence models from Definition 5.3.3—the\nrepresentation function encT is the composition of L transformer layers applied to the sequence of\nstatic encodings, followed by a final transformation F.\n216\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\ny0\ny1\ny2\ny3\npSM py4 | h3q\npSM\n`\nyt`1 | ht\n˘\nh3\nht\nyt´1\nyt\nLocally-normalized\ndistribution\nEncoding encT\nL applications of\nthe transformer\nlayer\nInput y\n¨ ¨ ¨\n¨ ¨ ¨\nFigure 5.20: encT pyďtq is a function of the symbols y1, . . . , yt computed with multiple applications\nof the transformer block. Here, the dashed lines illustrate the dependencies of the outputs ht on\nthe initial static encoding of the symbols y denoted by green nodes. Na¨ıvely, h3 and ht could be\ncomputed by independently applying the attention mechanism from Definition 5.3.4. However, as we\ndescribe in the text, while the applications of the attention mechanism do not share computations,\nthey can be written concisely together.\nMaking Attention Work Fast Over Entire Strings\nNotice that, in the formulations so far, we always presented computations of the attention mechanism\nfor individual queries qt. This corresponds to the computation of the new version of the representation\nof a single symbol in the string, with the keys and values representing the preceding symbols (including\nthe symbol itself). This is illustrated in Fig. 5.20. This could of course be applied |y|-times to\ncompute the representations of all |y| symbols in a single transformer block. However, this would\nunnecessarily re-compute the keys and values of the symbols multiple times—and, as we motivated\nat the beginning, speedups were one of the main reasons to talk about transformers in the first\nplace. We can now show how the attention mechanism can be conveniently applied to entire strings\nat once. Specifically, we focus on the case where the attention scoring function f is implemented as\na dot-product.28\nWhat does the attention mechanism from Definition 5.3.4 do in this case?\nGiven a query\nqt and a matrix of key values K “\n`\nkJ\n1 , . . . , kJ\nt\n˘\nP RtˆD, the scoring function simply computes29\nuj “ f pqt, kjq “ qJ\nt kj.\nNotice that, in this case, the vector u “ pu1, . . . , utq of unnormalized attention weights can simply\nbe computed as a single matrix-vector product\nu “ qJ\nt KJ.\n28For conciseness, we will ignore the scaling factor, which could easily be added.\n29We switch notation from xqt, kjy to qJ\nt kj to make the connection to matrix multiplication later clearer.\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n217\nFurthermore, with this, attention can be easily extended to consider many queries in parallel by\nstacking multiple queries into a matrix Q\ndef\n“\n`\nqJ\n1 , qJ\n2 , . . . , qJ\nt\n˘\n, as we detail now.30 Consider now\nthe product\nU “ QKJ.\nEach entry of the resulting matrix Uij is exactly the dot-product between the query qi and the\nkey kj! The rows of U then contain the unnormalized score vectors ui from the definition of the\nattention mechanism. This means that if we now apply the normalization function f∆D´1 row-wise\n(such that the sums of the elements in each row equal 1), we end up with exactly the required\nnormalized scores required for combining the values from the value matrix. With some abuse of\nnotation, we will simply write that as\nS\ndef\n“\n`\nsJ\n1 , . . . , sJ\nt\n˘ def\n“ f∆D´1 pUq “ f∆D´1\n`\nQKJ˘\n.\n(5.126)\nThe rows of f∆D´1 pAq, therefore, represent the normalized attention weights. This brings us to the\nfinal step of the matrix-multiplication-based attention mechanism: Combining the values based on the\ncomputed attention weights. Again, this can be performed by a single matrix multiplication. Notice\nthat the value vectors are the same for all queries—they are simply combined with different (attention)\nweights based on the query. Right-multiplying the transposed values matrix V “\n`\nvJ\n1 , . . . , vJ\nt\n˘\nwith\nS, therefore, perform the convex combination of the value vector vJ\n1 , . . . , vJ\nt such that\nai “ siVJ “ Si,: VJ\n(5.127)\nand thus\nA\ndef\n“ pa1, . . . , atq “ SVJ.\n(5.128)\nAltogether, this means that, given a sequence of (contextual) symbol encodings X, we can compute\nthe attention values (i.e., the output of the attention mechanism) of all queries (i.e., for all string\nin the string) with a single matrix multiplication, as long as the scoring function is the (scaled)\ndot-product. We refer to this version of attention as an attention block, which, intuitively, simply\nreplaces the element-wise definition of the attention mechanism from Definition 5.3.4 with a more\nefficient (and concise) definition through matrix multiplications.31\nDefinition 5.3.10: Attention Block\nLet Q, K, and V be parametrized functions from RT ˆD to RT ˆD and X P RT ˆD the matrix\nof input encodings. An attention block is the function A: RT ˆD Ñ RT ˆD defined as\nApXq “ f∆D´1\n´\nQ pXq K pXqJ¯\nV pXq\n(5.129)\nFurther, we define the attention matrix as the square matrix U\ndef\n“ QpXqKpXqJ P RT ˆT .\n30Note that, for easier presentation, we make a slight departure from the original definition of the attention\nmechanism, where the result of the attention mechanism for query t only depended on the keys and values j ď t. For\nthe rest of the paragraph, we assume that a query qi with i ă t can consider keys and values kj and vj with j ą i,\nwhich, in the interpretation of attention applied to strings, would mean that the symbols can “look ahead” in the\nstring to their right. This will be addressed shortly with masking.\n31Again, with the caveat that the attention weights are not confined to the preceding symbols but to all symbols in\nthe string.\n218\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nht\ny1\ny2\n¨ ¨ ¨\nyt\n¨ ¨ ¨\nyT ´1\nyT\nk1, v1\nk2, v2\nkt, vt\nkT ´1, vT ´1\nkT , vT\nEncoding encT\nL applications of\nthe transformer\nlayer\nInput y\nFigure 5.21: In the context of language modeling, the attention mechanism is allowed to consider the\nsymbols yj and their keys/values for j ď t (the green dashed lines) when computing the contextual\nencoding of the symbol yt. In Definition 5.3.4 this is enforced by the definition of the matrices Kt and\nVt. However, in the attention block formulation of the attention mechanism from Definition 5.3.10,\nsince the matrices K and V contain the values corresponding to the entire string y, the query qt\ncould, in principle, index into the values corresponding to the symbols yt`1, . . . , yT (the red dotted\nlines). Masking prevents that by enforcing the attention weights at`1, . . . , aT to be 0. In this sense,\nit removes the red dotted lines.\nAs mentioned, the functions Qp¨q, Kp¨q, and V p¨q are usually implemented as a linear trans-\nformation via matrix multiplication using weight matrices WQ, WK, and WV . This means that\nthe query matrix Q can be computed as Q “ XWQ, where WQ P RDˆD is a matrix of learnable\nparameters. Since the attention block uses the same input matrix X to encode queries, keys, and\nvalues, it is usually called self-attention.\nConfining attention to preceding symbols in the string.\nWe now address the departure\nfrom the original definition of the attention mechanism in which the query qt was only allowed\nto consider the keys and values kj and vj with j ď t. Notice that, in general, the version of\nattention of Eq. (5.129) allows each symbol to attend to any symbol in the string, even those in\nlater positions in the string. Note that there is nothing inherently wrong with that—the contextual\nsymbol encodings could, in principle, depend on the information from the entire string. In fact, this\nis very common in the so-called masked language modeling, which, importantly, despite its name,\ndoes not define language models in our sense of the word. A very commonly used family of masked\nmodels is the BERT family of models.32 However, in the case of locally normalized language models,\n“looking ahead” obviously violates the autoregressive structure of language modeling, i.e., violates\nour assumption that the context at time t forms yăt. This is illustrated in Fig. 5.21. To recover the\nautoregressive nature of the language model, we, therefore, posthoc modify Eq. (5.129) to allow\neach symbol to attend only to itself and to preceding symbols, while still being able to implement it\nusing matrix multiplication. We do that by adding a mask to zero out the unwanted elements of U.\n32In a very unfortunate, but also understandable, turn of events, we mention two completely different notions of\nmasking in a single paragraph. Importantly, the masking in masked language models (which, again, are not language\nmodels in our strict definition) has nothing to do with the “causal” masking relevant for autoregressive language\nmodeling which we introduce in this section.\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n219\nDefinition 5.3.11: Masked Attention Block\nLet Qp¨q, Kp¨q, and V p¨q be parametrized functions from RT ˆD to RT ˆD. A masked attention\nblock is a function ApX, Mq : RT ˆD ˆ RT ˆD Ñ RT ˆD defined as\nApX, Mq “ softmaxpQpXqKpXqJ d MqV pXq\n(5.130)\nwhere d is the element-wise product between matrices, and M P Rℓˆℓ, the masking matrix,\nis constructed as follows.\nMi,j “\n#\n1 if i ď j\n´8 otherwise\nfor 0 ď i, j ă T\n(5.131)\nThis implements a very easy “fix” to the looking-ahead problem by simply putting the normalized\nattention scores of the “illegal” elements to 0. In general, the exact value of the elements Mi,j with\ni ą j of course depends on the projection function f∆D´1—for simplicity, we only define M for the\ncase where f∆D´1 “ softmax.\nBits and Bobs of the Transformer Architecture: Positional Encodings, Multiple Heads,\nand Layer Normalization\nLet us now take a step back and consider what the transformer model introduced so far does abstractly.\nA transformer takes as input as string y P Σ˚, computes the initial symbol embeddings X1 “ X, and\ntransforms those through a sequence of L applications of the transformer layer (cf. Definition 5.3.9).\nThis results in the final augmented (contextual) symbol representations ht “ encT pyďtq, which are\nthen used to compute the conditional probabilities in the representation-based locally normalized\nlanguage model defined by the transformer (cf. Definition 5.3.3), as illustrated on top of Fig. 5.20.\nIn this subsection, which will finish off our formal definition of the architecture, we introduce the last\nthree components often connected closely to the transformer model: Symbol positional encodings,\nmulti-head attention, and layer normalization.\nAdding positional information into the transformer architecture.\nThere is an important\nomission we still have not addressed when talking about transformers: How does the model\nincorporate any notion of word order into the contextual representations of symbols or the encodings\nof the context ht? The motivation is very clear: The meaning of a sentence depends on the word\norder. The meaning of “A dog bit a man.” is not the same as “A man bit a dog.”. This is one of\nthe reasons why simple “sentence encoding” functions such as bag-of-words, which simply represent\nsentences with the number of individual words they contain, do not work well. A careful reader might\nhave noticed that at no point in our discussion about transformers and the attention mechanism\ndid we say anything about the positions of the words. Importantly, we did not talk about word\npositions in the case of RNNs either. However, the sequential and incremental processing nature of\nRNNs makes it easy to “manually” keep track of the position of the current symbol of the string\nyt, to the extent that the RNN variant is capable of “counting” (cf. §5.2). However, all operations\ncomposing the transformer model are position-agnostic: The convex combination of the value vectors\nV will be the same, no matter the permutation of the vectors (if we, of course, accordingly permute\nthe keys). The keys also cannot contain any positional information, since they are computed from\n220\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nposition-agnostic static embeddings and a transformation function K which does not depend on the\nposition.\nAll that is to say that, to be able to take into account word order in a transformer, we have to\nexplicitly provide the positional information to the model. The simplest way to do this is to augment\nthe static symbol encodings in the first transformer layer with positional encodings in the form of\nvectors which can be added to or concatenated to the static encodings of symbols (Vaswani et al.,\n2017).33\nDefinition 5.3.12: Positional encoding\nA positional encoding is a function fpos : N Ñ RD.\nThis is a very simple definition: A positional encoding simply assigns a position in a string\na vector representation. A trivial example would be fpos ptq “ ptq. This allows us to define a\nposition-augmented symbol representation function.\nDefinition 5.3.13: Position-augmented representation function\nLet e1 : Σ Ñ RD be a symbol representation function and fpos : N Ñ RD a positional encod-\ning. A position-augmented representation function of a symbol yt in a string y is the\nrepresentation function e1\npos : Σ Ñ RD defined as\ne1\npos pytq\ndef\n“ e1pytq ` fpos ptq.\n(5.132)\nTo make the positional information available to the transformer model, we now simply pass the\nposition-augmented “static” symbol encodings Xpos to the model instead of the original ones X.\nApart from that, the transformer model can remain unaltered, and function simply as defined above,\ntaking into account the positional information now included in its inputs. Importantly, the intuitive\nnotion of the importance of positional encodings for understanding natural language also transfers to\nthe computational power of the model: Transformers as introduced in this section without positional\ninformation are strictly less powerful than those with positional information (P´erez et al., 2021).\nAgain, this intuitively makes sense: Without positional information, a transformer model could not\neven recognize the simple (unweighted) regular language\nL “ tabn | n P Nu\nsince it would have no way of knowing, provided that a is in a given string y, whether it appears in\nthe first position or in any other position in the string.\nMultiple heads.\nImportantly, the transformer introduced so far computes a single set of contextual\nrepresentations—one for every input symbol (at every layer of the transformer). However, we can\neasily extend the model to compute multiple contextual representations for each symbol. This\nis done using the so-called multi-head attention, where a single attention block is called an\n33For simplicity, we assume the positional encodings are added to the static ones; notice that by dividing the\nD-dimensional vectors into two components, one responsible for the static encodings and one for the positional\nones (where the positional encoding component is zeroed out in the static encoding and vice-versa), one can easily\nimplement “concatenation” of the two representations using only addition.\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n221\nattention head. This increases the representation space of the individual symbols and thus enables\nthe model to capture more information about the symbols and the sentence. The interpretation of\ncomputing multiple representations (one for each head) independently also invites the interpretations\nthat each of the heads “focuses” on a separate aspect of the text. To be able to use the outputs of\nmulti-head attention as inputs to the next block again, the outputs of the different attention heads\nare then concatenated and then projected down to the output size of a single attention block using\nan additional transformation.\nDefinition 5.3.14: Multi-Head Attention Block\nLet H P N be the number of attention heads, Qhp¨q, Khp¨q, and Vhp¨q be parametrized functions\nfrom RT ˆD to RT ˆD for 0 ď h ď H, and fH : RT ¨HˆD Ñ RT ˆD be a parametrized function.\nA multi-head attention block is a function MH-ApXq : RT ˆD Ñ RT ˆD defined as\nMH-ApXq “ fHpconcat0ďhăH\n´\nsoftmax\n´\nQh pXq Kh pXqJqVh pXq\n¯¯\n(5.133)\nWhile multi-head attention is mostly motivated by empirically better performance (and the\nintuitive motivation of being able to separately focus on different notions of similarity), it will have\nsome implications on the computational power of the model as well. As we will see shortly, having\nmultiple heads makes it very easy to reason about how a transformer model can simulate an n-gram\nmodel.34\nLayer normalization.\nAs a final component of a transformer, we mention layer normalization.\nLayer normalization, similar to the use of residual connections, represents a common “trick” in the\ndeep learning space for ensuring more stable and reliable gradient-based learning—as such, it is not\nlimited to transformers. Formally, we can define layer normalization as follows (Ba et al., 2016).\nDefinition 5.3.15: Layer normalization\nLet x, γ, β P RD, and ϵ ą 0. The layer normalization function LN: RD Ñ RD is defined as\nLN px; γ, βq\ndef\n“\nx ´ x\na\nσ2 pxq ` ϵ\nd γ ` β,\n(5.134)\nwhere x refers to the mean of the vector x (and is subtracted from all elements of x in the\nformulation above) and σ2 pxq refers to the variance of elements of x. ϵ is added in the\ndenominator to ensure stability if σ2 pxq ! 1.\nIntuitively, the application of the layer normalization function ensures that the mean of the vector\nx is (approximately) β and its variance is controlled by γ (after being standardized by dividing by\nthe standard deviation of x). Most commonly, we simply set γ “ 1 P RD and β “ 0 P RD.\nLayer normalization is most commonly applied to the output of the transformer layer (on every\nlayer), i.e., to zi in Eq. (5.121). The full output of the transformer layer is therefore computed as\nzi\ndef\n“ LN pO paiq ` ai; γ, βq.\n(5.135)\n34This does not, however, mean that multiple heads are required for recognizing n-gram models. As we will see,\nunder some caveats, single-head transformers are Turing complete.\n222\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nInterestingly, although we mentioned that layer normalization is mostly motivated by the stability it\nbrings to training and with it better performance, it does, just like multi-headed attention, seem to\ncontribute to the computational expressivity of transformer models. As we will see in §5.4, layer\nnormalization allows for a simple fix that solves one of the best known formal limitations of the\ntransformer architecture (again, under some assumptions) (Hahn, 2020; Chiang and Cholak, 2022).\nConnecting the Formal Definition Back to our Desiderata\nThis brings us to the end of the formal definition of the transformer architecture. As we saw, a\ntransformer has many more moving parts than an RNN. Is the additional complexity warranted?\nLet us now return back to the informal motivations or desiderata that we laid out in §5.3.1 and see\nhow the components we defined in this section come through and ensure transformers fit them. First\nof all, the transformer layers clearly store the representations of all symbols at all times—they are\nall needed to produce the query, key, and value matrices required by the attention mechanism. As\nmentioned above, this allows us to easily store information about the entire string in a convenient\nand “accessible” format without having to compress it into a single hidden state. Furthermore, the\nfact that the contextual representations Zpℓ` 1qt are computed from the representations xℓ\n1, . . . , xℓ\nt\ndirectly at every time step, with no direct dependence between the different zpℓ` 1qi, means that\nthese computations can easily be parallelized. More concretely, in the most common implementations\nof the transformer components, most operations take the form of matrix multiplications, which\nmakes the computation and parallelization that much more efficient.35 Again, note that here, we\nare only interested in parallelizing the processing of entire strings, as for example given in a training\ncorpus. As discussed in §5.1.5, there is an aspect of language modeling that is inherently sequential\nand even heavily parallelizable architectures such as the transformer cannot overcome: Generating\nstings one symbol at time. While generating strings, even a transformer model will have to generate\nsymbols one at a time and, therefore, recompute (parts of) the encoding encT pyďtq anew at every\ntime step to generate the next symbol. The advantages of parallelizability, therefore, come only\nat training time—however, given the vast corpora used for training today’s models, this makes a\ncrucial difference in the applicability of the architecture over recurrent ones.\nAltogether, this means that transformers do, indeed, achieve the desiderata from our informal\nmotivation!\nThis concludes our formal definition of transformers.\nWe move to analyze their\ntheoretical properties.\n5.3.3\nTightness of Transformer-based Language Models\nHaving introduced transformers formally, we can start investigating their formal properties. As we\ndid for RNN LMs, we first consider their tightness. Specifically, in this subsection, we show that\nall soft attention-based transformer language models are tight. Key to our proof of the tightness of\ntransformer language models, as well as the tightness of various other neural architectures, is the\nfollowing basic fact in topology.\n35Note that, there is, of course, some sense of recurrence in the transformer—the composition of the transformer\nlayers, which are stacked on top of each other, and of course require sequential computation. However, crucially, the\nnumber of layers does not depend on the length of the string—the number of sequential steps required to process a\nstring, therefore, does not depend on its length, which is what we wanted to achieve.\n5.3. TRANSFORMER-BASED LANGUAGE MODELS\n223\nTheorem 5.3.1: Compactness\nLet X be a compact topological space and Y be any topological space. If f : X Ñ Y is\ncontinuous, then f pXq Ď Y is also compact.\nProof. Let tUsusPA be any open cover of fpXq. By continuity, f ´1pUαq Ă X is open for any α P A,\nand hence tf ´1pUsquαPA is also an open cover of X. By the compactness of X, there is a finite\nsub-cover tf ´1pUαnquN\nn“1, in which case tUαnuN\nn“1 forms a finite sub-cover for fpXq.\n■\nWe now further mathematically abstract transformers as a function on vector tuples,36 fAtt :\n`\nRD˘` Ñ\n`\nRD˘`, that is length-preserving in the sense that fAtt\n`\nRtˆD˘\nĎ\n`\nRtˆD˘\nfor all t ą 0. Intuitively,\nthis definition is saying that fAtt is a function that maps a nonempty vector tuple tvjut\nj“1 to another\nvector tuple thjut\nj“1 of the same length,\nfAttpv1, . . . , vtq “ ph1, . . . , htq P RtˆD,\n(5.136)\nwhere vj “ e1 pyjq P RD are the initial representations of the input symbols yj. In particular, we\ncan take the function fAtt :\n`\nRD˘` Ñ\n`\nRD˘` to be the function defined by a stack of transformer\nlayers, i.e., an attention block. This setup will help us state the following.\nLemma 5.3.1\nLet fAtt :\n`\nRD˘` Ñ\n`\nRD˘` be the function defined by a L transformer layers with continuous\nfunctiona Q, K, V , and O. Given a compact set K Ă RD. Then, there exists a compact set\nK1 Ă RD such that for every t P Zą0,\nfAtt\n`\nKt˘\nĎ\n`\nK1˘t .\n(5.137)\nNote.\nWe make use of the following notations in the proof below: Brpzq “ tv P RD : distpz, vq ă ru\ndenotes the open ball centered at z with radius r; A denotes the closure of set A.\nProof. Let K0 “ K. In an autoregressive transformer, each of the L layers consists of two blocks: a\nself-attention block and a feedforward block. We will use induction on the 2L blocks to build up\ncompact sets K1, K2, . . . , K2L that contain the output vectors of these respective blocks, and then\ntake K1 “ K2L.\nThe self-attention block is a function on pRDq` Ñ pRDq`. So, let t P Zą0 be arbitrary and\nconsider any sequence of input vectors pv1, . . . , vtq such that for all i, vi P K0. Denote the output\nvectors of the attention block with pv1\n1, . . . , v1\ntq. By definition of attention, each output vector\nv1\nj “ řt\ni“1 spjq\ni vi where spjq P ∆t´1 are the attention weight vectors obtained through the softmax\nfunction. Compact sets in RD are bounded (by the Heine–Borel theorem), and hence there exists\n36Here\n`\nRD˘` is the set of nonempty tuples of vectors in RD. This is formally the disjoint union (coproduct)\nš\ntPZą0 RtˆD.\n224\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nM ą 0 such that K0 Ď BMp0q. Noting that the norm function } ¨ } on RD is convex, we have the\nfollowing\n}v1\nj} “\n›››››\ntÿ\ni“1\nspjq\ni vi\n›››››\n(5.138a)\nď\ntÿ\ni“1\nspjq\ni }vj}\n(˚)\nď\ntÿ\ni“1\nspjq\ni M “ M\n(5.138b)\nwhere (˚) results from Jensen’s inequality. Eq. (5.138b) shows that each of the output vectors v1\nj\nlies in BMp0q which is compact. Hence, setting K1 “ BMp0q, we have shown that, for any t P Zą0,\nthe attention block maps Kt\n0 into Kt\n1.\nNote that we cannot use Theorem 5.3.1 here because the attention block defines a different\nfunction on RtˆD Ñ RtˆD for each t, and Theorem 5.3.1 only implies that there exists a separate\nlength-dependent output compact set Kt Ă RtˆD for each t, which is different from this lemma’s\nstatement.\nThe feedforward function is a continuous function on RD Ñ RD, and therefore, by Theorem 5.3.1,\nmaps its input compact set K1 to an output compact set, which we call K2.\nFinally, residual connections and layer norms are also continuous functions acting on each of the\ninput vectors, and hence by the same reasoning would also preserve compactness.\nNow we can use induction and show that there exist compact sets K3, K4, . . . , K2L´1, K2L where\nK2L contains the output set of the final layer. Set K1 “ K2L and we have proven the statement.\n■\nNow recall that a transformer language model with the softmax projection function (Defini-\ntion 5.3.3) defines the conditional probabilities using the softmax transformation\npSMpyt | yătq “\nexppepytqJhtq\nř\ny1PΣ exppepy1qJhtq\n(5.139)\nwhere epyq P RD is the output symbol embedding of y P Σ and ht is defined from the input\nembeddings of yăt via Eq. (5.136). Using Lemma 5.3.1, together with the finiteness of the vocabulary\nΣ and the continuity of the softmax transformation (5.139), readily yields our main result on\ntransformer language models.\nTheorem 5.3.2: Transformer language models are tight\nThe representation-based locally normalized language model (cf. Definition 5.3.3) defined by\nany (fixed-depth) transformer with soft attention is tight.\nProof. Given the Transformer, there exists a fixed compact set K that will contain all inputs vi P RD\nto the first layer. This is true because each vi is the sum of a word embedding, which falls in a\nfinite set since Σ is finite, and a position embedding, which lies in the compact set r´1, 1sD. Hence,\nby Lemma 5.3.1, there exists a fixed compact set K1 that contains all output embedding vectors\n(regardless of how long the sequence is).\n5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS\n225\n1\n2\n1\n0, 1\nFigure 5.22: An FSA recognizing the language First “ ty P Σ˚ | Σ “ t0, 1u, y1 “ 1u.\nThe final output probability is given by a multiplication with the word embedding matrix followed\nby the softmax function as in Eq. (5.139). This process amounts to composing two continuous\nfunctions. In particular, we can extract the eos probability as a continuous R-valued function\ngeos : K1 Ñ p0, 1q (neither 0 nor 1 is in the range of the softmax function). By continuity of geos\nand Theorem 5.3.1, K2\ndef\n“ geospK1q Ď p0, 1q is compact. Since K2 is compact, and hence closed,\ninf K2 P K2. Thus inf K2 P p0, 1q and in particular inf K2 ą 0. Therefore, taking ϵ “ inf K2, we have\nshown that the eos probability of a Transformer is bounded below by some ϵ ą 0 (regardless of the\nlength of the sequence). Hence, by Proposition 2.5.6, any transformer-based sequence model is tight\nand thus defines a language model.\n■\n5.4\nRepresentational Capacity of Transformer Language Mod-\nels\nSo far, we have introduced our formal definition of the transformer architecture and examined its\ntightness. We now move on to the computational power of the architecture. This section mirrors\n§5.2 and examines the expressivity of the transformer language model as defined in Definition 5.3.3.\nTransformers are a much more recent architecture than recurrent neural language models, and\nour theoretical understanding of them is thus much more limited. However, over the last few years,\na series of results showing various properties of the transformer model have been established. At\nfirst glance, one might find a number of contradictions among them: One of the first results shows\nthat transformers are not even able to recognize the very simple First language recognized by\nthe (unweighted) finite-state automaton shown in Fig. 5.22 nor the Dyck language. On the other\nhand, there is work showing that transformers can recognize the majority language (determining\nwhether a string contains more symbols a or b) and can even count: Both of these languages are\ninstances of non-regular languages. Moreover, a fundamental result by P´erez et al. (2021) even\nshows that transformers are Turing complete. Upon closer inspection, the results can be explained\nby the different theoretical abstractions of the original transformer model that the different works\nmake and even different notions of equivalence. Even very subtle differences in the model can lead\nto substantial differences in the expressivity of the model, as we will see below. In this section,\nwe present some original results which show that transformers can, with infinite precision, in fact,\nreach up to the top of the hierarchy of formal languages that we consider in these notes: They are\nTuring complete. We also comment on the differences in our approach to the work so far (the main\ndifference and novelty is that we embed the analysis into our language modeling framework) and try\nto unify it. We will show that infinite-precision transformers can simulate the recognizers across\nthe entire hierarchy: (weighted) finite-state automata, pushdown automata, and Turing machines.\nLuckily, apart from the first construction, the proofs and constructions in our ascent up the hierarchy\nwill be based on a unified approach in which we build on P´erez et al. (2021) and sequentially add\n226\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\ncomponents to be able to recognize more and more complex languages.\nBesides being more novel and thus less researched, transformers are also less intuitive to think\nabout as sequential machines transitioning between states as with finite-state or pushdown automata.\nAll classical computational models we introduced (finite-state automata, pushdown automata, and\nTuring machines) rely on some notion of an internal state which is sequentially updated, where\nthe next state is determined based on the current configuration. We also said in §5.1.5 that this\nsequentiality is the Achilles’ heel of the ability to parallelize and thus speed up the computations\nin a language model. One of the main motivations for defining the transformer model is to avoid\nthese sequential dependencies and to make sure the contextual representations of the individual\nsymbols can be computed independently. However, the lack of sequentiality in transformers makes it\nmore difficult to compare to classical and well-understood models of computation—they simply do\nnot define any notion of a configuration that would be passed over by reading a symbol at a time,\nand relating the configurations at different time points to the configuration of some classical model\nof computation was the main idea of most of the analyses in §5.2. This will not be possible with\ntransformers, and we will have to be more clever about it to draw parallels to better-understood\nformalisms. What is more, it seems like their parallelizable nature is one of the reasons for the lower\n(or, at least, ambiguous) computational power under some formalisms, as covered in Merrill et al.\n(2022a); Merrill and Sabharwal (2023).\nA word on model equivalence.\nAs mentioned above, the nature of the transformer architecture\ndoes not lend itself well to a straightforward comparison to classical models of computation. To\nmake the connection, we will have to be somewhat clever about the analysis. As we will see shortly,\nwe will mainly deal with this in two ways: (i) By foregoing any notion of a state of a machine in\ncase of n-gram language models37 and (ii) by embedding the state of a computational model into\nthe alphabet itself —the model will then use the augmented output alphabet to keep track of its\nstate in the string itself without relying on any notion of its own internal state which would have\nto be updated sequentially.38 How can this help us? As will become clear in our analysis of the\nTuring completeness of a transformer model, the model can use the generated string as a sort of a\nsequential memory structure. Because the transformer model can look back at the entirety of the\nstring when computing encT pyďtq (where yďt is the augmented string generated so far), it is able\nto “read off” its internal state from the string. Importantly, the generated string will still contain\nthe information about the generated string, besides including the state of the computational model.\nAs the transformer will then compute the new embeddings encT pyďtq, it will be able to account for\nthe state it should be in. This is illustrated in Fig. 5.23.\nWhile this might seem like a convenient trick to achieve Turing completeness—and in many ways,\nit is—it is also, in a way, cheating. This “cheating” can be described formally as the difference between\nmodel equivalence and homomorphism equivalence. When we discussed the Turing completeness of\nRNN LMs, we showed they can model a Turing machine by directly recognizing the same strings\n(for the time being, we ignored the string weights). This means that, for every Turing machine\nM, there exists an RNN R which recognizes the same language: L pRq “ L pMq. However, we\nwill not be able to make statements like this in the case of transformer models. The augmented\n37Reminder that n-gram language models are in fact subregular (cf. §4.1.5) and we make use of that in our analysis.\nBecause their recognition relies purely on local patterns in the strings, and a transformer model has the ability to\nconsider large enough substrings, we will see that we can model an n-gram language model without keeping any\nnotion of a state in a transformer\n38Recall that, as discussed in §5.1.5, generation is inherently sequential. One can thus imagine augmenting the\nalphabet as a sort of exploitation of this sequential nature.\n5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS\n227\nOriginal tape of the\nTuring machine\nAugmented tape of\nthe stateless model\ny1\ny2\ny3\ny4\n¨ ¨ ¨\nyt\n¨ ¨ ¨\ny1q1\ny2q2\ny3q3\ny4q4\n¨ ¨ ¨\nytqt\n¨ ¨ ¨\nFigure 5.23: An abstract illustration of how a model can keep track of its internal state by “outputting”\nit into the generated string. By reading the augmented symbol generated at time t, the model can\nthen determine its internal state.\nalphabet will instead bring us to a statement of the sort “For every Turing machine M, there\nexists a transformer T which recognizes the same language augmented with the state set of the\nTuring machine: L∆pT q “ L pMq,” where L∆refers to the language of strings where each symbol\nis additionally augmented with the state of the Turing machine. This might seem like a small\ndifference, but, in formal language theory, homomorphism equivalence refers to a different problem\nto that of normal model equivalence (Culik and Salomaa, 1978) and thus has to be considered\ndifferently. Intuitively, it additionally allows additional information to be stored in the strings (in\nour case, that will be the state of the Turing machine) while still considering some models to be\n“equivalent”. Formally, model equivalence asks the following question.\nDefinition 5.4.1: Model equivalence\nTwo computational models C1 and C2 are equivalent if\nL pC1q “ L pC2q .\n(5.140)\nOn the other hand, homomorphic equivalence considers the following.\nDefinition 5.4.2: Homomorphism\nLet C1 and C2 be two computational models. C1 is homomorphically equivalent to C2 if\nthere exists a homomorphisms h: L pC1q Ñ L pC2q such that\nh pL pC1qq\ndef\n“ th pyq | y P L pC1qu “ L pC2q .\n(5.141)\nDefinition 5.4.3: Homomorphic equivalence\nLet C1 and C2 be two computational models. C1 and C2 are homomorphically equivalent if\nC1 is homomorphically equivalent to C2 and C2 is homomorphically equivalent to C1 as per\nDefinition 5.4.2.\n228\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nTransformers and the Inability to Recognize Simple Languages\nWe start our exploration of the computational power of transformer models with some negative\nresults, which we will later “correct” by using our formalization of a transformer model and different\ncomponents of the transformer architecture (for example, a different form of attention). Given\ntheir success at modeling human language which is assumed to be at least mildly context-sensitive\n(Huybregts et al., 1984; Shieber, 1985), it seems surprising that transformers cannot, in fact, recognize\nsome very simple regular languages, such as Parity or First (the FSA shown in Fig. 5.22), as well\nas simple non-regular context-free languages such as Dyck languages:\nFirst “ ty P Σ˚ | Σ “ t0, 1u, y1 “ 1u\nParity “ ty P Σ˚ | Σ “ t0, 1u, y has odd number of 1su\nDyck “ ty P Σ˚ | Σ “ tp, qu, y is correctly parenthesizedu\nThis has been formally shown by Hahn (2020), and experimentally verified by Chiang and Cholak\n(2022). Bhattamishra et al. (2020) found that transformers especially struggle to learn any languages\nthat require counting occurrences in some way, such as the number 0s and 1s in Parity or the\nnumber of previous open and closed parentheses in Dyck. Hahn (2020) finds that with unique\nhard attention, these languages cannot be recognized: Recognizing them by a transformer in their\nformulation would require the number of parameters to increase with the length of the input. Chiang\nand Cholak (2022) consider the setting with soft attention, where the issue is more subtle: In theory,\nit is possible for a transformer to recognize languages such as First and Parity, however with\nless and less confidence as the length increases. This is reflected by the cross-entropy of deciding\nlanguage membership approaching the worst possible value of 1 bit per symbol. The reason behind\nthis is quite intuitive: The membership of any of the languages defined above changes if a single\nsymbol changes. However, by examining the information flow in a transformer, one can show that\nthe corresponding information gets less and less weight relative to the length of the string due to\nthe attention mechanism averaging over all positions.\nTransformers Can Simulate n-gram Models\n§5.4 showed that transformer models can struggle to recognize some of the simplest formal languages.\nWhile we did not discuss those results in detail, intuitively, they stem from the use of unique\nhard attention and the resulting inability to take into account all values whose keys maximize the\nattention scoring function. By relaxing that restriction to averaging hard attention, the model\nbecomes more expressive. To show that, we begin by looking at the very simple n-gram language\nmodels, as defined in §4.1.5. By constructing, for any n-gram model, a transformer representing it,\nwe will show the following theorem.\nTheorem 5.4.1: Transformer language models can simulate n-gramlanguage models\nLet pLN be an n-gram language model. Then, there exists a transformer T with L ppLNq “ L pT q.\nAlternatively, we could say that transformers can recognize strictly local languages (cf. §4.1.5).\nProof. We prove the theorem by constructing, for pLN, a transformer T with L ppLNq “ L pT q. Note\nthat we will mostly restrict the proof to the construction of the transformer, i.e., the formal definition\n5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS\n229\ny1\ny2\n¨ ¨ ¨\nyt´3\nyt´2\nyt´1\nyt\n¨ ¨ ¨\nHead 3\nHead 2\nHead 1\nfH\npSM pyt | yătq\nE\nFigure 5.24: An abstract depiction of how a transformer can simulate an n-gram model using n ´ 1\nheads (here, n “ 4). The stronger arrows from the heads to the symbols in the string show where\nthe heads concentrate their attention. The lighter green arrow is meant to represent that the heads\nstill can consider the entire history of the input so far but are then configured such that they only\nlook at the appropriate position.\nof its parameters. The (mostly trivial) mathematical details and derivations are left as an exercise\nto the reader.\nRecall that, by definition, an n-gram language model considers a fixed number of previous\nsymbols to define pSM pyt | yătq—exactly n ´ 1 of them. The constructed transformer T will capture\nthis idea with n ´ 1 heads, each of them attending to exactly one of those positions in the previous\nn ´ 1 positions.39 We can then use the symbols the heads attended to (and thus identified) to\nidentify the current n-gram and with it define the relevant conditional distribution over the next\nsymbol. To be able to attend to the positions of interest—the ones containing the previous n ´ 1\nsymbols—we have to make use of appropriate positional encodings (cf. Definition 5.3.12), which\nwill allow the model to attend to them. The idea of the construction is abstractly illustrated in\nFig. 5.24.\nFor hopefully a better pedagogical effect, we will present this proof from the “last” part of the\nconstruction to the “first”. We, therefore, start with the final step: Assuming we have identified\nthe appropriate n-gram yt´n: t´1yt, how can we encode the conditional probability distribution\npSM pyt | yt´n: t´1q? The construction we use here directly mirrors the one in Minksy’s construction\n(cf. Lemma 5.2.2): Knowing what the individual pSM pyt | yt´n: t´1q for yt P Σ are (those are, as\ndescribed in §4.1.5, “hard-coded”, or specified, for each n-gram separately in a look-up table), we\ncan simply put their logits (log probabilities; in case we are using the softmax projection function)\nor the probabilities directly (if we are using the sparsemax projection function) into a vector and\n39Note that, given an n-gram model, the number n is fixed. This means that, for a given n-gram we can always fix\nthe number of heads and therefore construct such a transformer.\n230\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nconcatenate all the constructed vectors (for all possible n-grams) into a large matrix E.\nIf we one-hot encode the identified n-gram by defining\nencT pyătq\ndef\n“ Jyt´n: t´1K\n(5.142)\nwe can then, using the formulation of the transformer sequence model from Definition 5.3.3, use the\none-hot encoded n-gram to lookup the appropriate column containing the conditional probabilities\ngiven the identified n-gram for all possible yt P Σ.40. The formal proof of correctness given that we\nhave identified the correct n-gram is therefore analogous to the final part of the Minsky construction.\nWe now consider the preceding step of the simulation: How can we identify the complete n-gram\ngiven that the n ´ 1 heads of the transformer identified the symbols in the positions they attended\nto? This, it turns out, is a simple instance of the “AND” problem investigated in Fact 5.2.3: After\nconcatenating the values of the n´1 heads into a common vector v (each of which is a |Σ|-dimensional\nvector), this vector of size |Σ| pn ´ 1q will contain the multi-hot representation of the n-gram of\ninterest. Let y1, . . . , yn´1 be the symbols represented by v. This means that v is of the form\nv “\n¨\n˚\n˝\nJy1K\n...\nJyn´1K\n˛\n‹‚\n(5.143)\nand vk|Σ|`j “ 1 if and only if m pykq “ j for an ordering m of Σ determining the one-hot\nrepresentations of the individual symbols. We would then like to transform this vector into a vector\nu P R|Σ|n´1 such that\nui “ 1 if and only if i “ s\n`\ny1, . . . , yn´1\n˘\n(5.144)\nfor some ordering s of Σ ˆ ¨ ¨ ¨ ˆ Σ\nlooooomooooon\nn´1 times\n. This can be equivalently written as\nui “ 1 if and only if vk|Σ|`mpykq ` 1 for all k “ 1, . . . , n ´ 1\n(5.145)\nwhere i “ s\n`\ny1, . . . , yn´1\n˘\n. Clearly, this is the same problem as described in Fact 5.2.3 and can\ntherefore be solved by a linear transformation followed by the application of the thresholded sigmoid\nnonlinearity, which will together form the transformation fH combining the information obtained\nfrom all the heads of the transformer model. Note that, to make this more similar to the practice of\nhow transformers are actually implemented, we could also use the ReLU activation function instead\nof the saturated sigmoid.\nThis brings us to the final part of the proof, which considers the first part of determining the\nconditional probability of the n-gram model by the transformer: Identifying the symbols at the\nprevious n ´ 1 positions by the n ´ 1 heads of the transformer. To show how this can be done, let\nus consider and define the “degrees of freedom” we have left when specifying a transformer model in\nour framework.\n• The symbol representations r. We will use simple one-hot encodings of the tokens: r pyq\ndef\n“ JyK.\n40Note that we are again working over the set of extended reals R “ R Y t´8, 8u in case of the softmax activation\nfunction.\n5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS\n231\n• The positional encodings fpos. We will use the following simple positional encoding: fpos ptq “\nˆ\nt\n1\n˙\n. The utility of the constant 1 will be made clear shortly. We will combine positional\nencodings with symbol representations by concatenating them into a vector of size |Σ| ` 2.\n• The number of transformer layers. We will use a single transformer layer.\n• The number of heads H: As we mentioned, we will use H “ n ´ 1 heads to attend to the\nprevious n ´ 1 symbols.\n• The form of the attention scoring function f. While not the most typical, we will use the\nfollowing scoring function:\nf pq, kq\ndef\n“ ´|xq, ky|.\n(5.146)\nIt will, together with the positional encodings, allow us to easily single out the positions in\nthe string that we care about.\n• The form of attention. We will use hard attention (in this case, it can be either unique or\naveraging).\n• The parameters of each of the attention heads, that is the transformations Q, K, and V . Each\nof those will take the form of a linear transformation of the symbol embedding. We describe\nthem and their roles in more detail below.\nAs mentioned above, the input symbol yt is presented to the transformer model together with its\npositional encoding in the form\nr pytq “\n¨\n˝\nJytK\nt\n1\n˛\n‚P R|Σ|`2.\n(5.147)\nThe parameters of all the heads are defined in the same way, with the only difference being a simple\nparameter that depends on the “index” of the head we are considering, h. Therefore, in the following,\nwe describe the construction of a single head Head h. At any time step t (i.e., when modeling the\nconditional distribution pSM pyt | yătq), the head h will attend to or be “responsible for” recognizing\nthe symbol at position t ´ h, yt´h. This can be seen in Fig. 5.24, where, for example, Head 3 is\nresponsible for the position t ´ 3, which is denoted by the stronger arrow to that position. All we\nstill have to do is describe the individual transformations Qh, Kh, Vh of the head h. All of them\nwill be linear transformations, i.e., matrix multiplication:\nq\ndef\n“ Q pxq\ndef\n“ Qhx\n(5.148)\nk\ndef\n“ K pxq\ndef\n“ Khx\n(5.149)\nv\ndef\n“ V pxq\ndef\n“ Vhx\n(5.150)\nWe now define the matrices Qh, Kh, and Vh, specifically in the first (in this case, the only) layer\nof a transformer language model. Importantly, since we are talking about only the first layer, we\ncan simply consider as inputs to the layer the original static symbol representations together with\ntheir position encodings rather than any contextual representations. First, let us consider again\nwhat roles the matrices play in computing encT pyătq. In the context of language modeling, the\nmatrix Qh takes in the representation of the “latest” generated symbol yt´1 and produces from it\n232\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nthe query vector of yt´1. It is, therefore, only applied once per generation step—only for symbol\nyt´1. The matrices Kh and Vh, on the other hand, transform all non-masked input symbols to\nthe key and value vectors. That is, they take the representations of the input symbols and their\npositions encodings encT pyjq for every j “ 1, . . . , t ´ 1 and transform them into the key and value\nvectors. The keys will then be compared with the query constructed for yt´1 with the Qh matrix,\nwhile the constructed values will be used to compute the new hidden state ht.41\nSo, what kind of query, key, and value vectors do we want? As mentioned, the head h will be\nresponsible for identifying the symbol at position t ´ h. Therefore, we want it to put all its attention\nto this position. In other words, given the query qt´1, we want the attention function in Eq. (5.146)\nto be maximized by the key of the symbol at position t ´ h. Notice that, therefore, the key does\nnot have to depend on the identity of the symbol at position t ´ h—only the position information\nmatters. Let us then consider the following query and key transformations for head h:\nQ:\n¨\n˝\nJytK\nt\n1\n˛\n‚ÞÑ\nˆ\nt ´ h\n1\n˙\n(5.151)\nK :\n¨\n˝\nJyjK\nj\n1\n˛\n‚ÞÑ\nˆ\n´1\nj\n˙\n.\n(5.152)\nGiven such a query and such keys, the attention scoring function computes\nf pqt, kjq “ ´|x\nˆ\nt ´ h\n1\n˙\n,\nˆ\n´1\nj\n˙\ny| “ ´|t ´ h ´ j|,\n(5.153)\nwhich is maximized exactly when j “ t ´ h, that is, at the position that we want the head h to\nattend to! This means that the hard attention we use will put all its probability mass to exactly the\nposition we intended it to. Intuitively, both transformations keep only the positional information.\nThe query transformation “injects” the knowledge of which position should maximize the attention\nscore, while the key transformation (which is, again, applied to all the non-masked positions) simply\n“exposes” the positional information about the symbol. The alternating constant 1 (or ´1) and the\nindex of the position ensure that the inner product simply computes the difference between the\nposition of the symbol and the position of interest—we will use this trick multiple times in later\nconstructions as well. It is easy to see that the two transformations are indeed linear.\nThis leaves us with the question of how to use this position of the symbol of interest (t ´ h)\nto extract the one-hot encoding of the symbol at that position. Luckily, due to the information\ncontained in the symbol representations r pyjq, this is trivial. All that the transformation V has to\ndo is the following:\nV :\n¨\n˝\nJyjK\nj\n1\n˛\n‚ÞÑ JyjK.\n(5.154)\nWith this, the identity of the symbol is carried forward through the attention mechanism. Again,\nis easy to see that this is a linear transformation of the symbol representation. Notice that the\nonly head-depend transformation is the query transformation—it depends on the index of the head,\n41Importantly, in a multi-layer transformer, the queries would be constructed for every non-masked symbol and its\nrepresentation (hidden state) would be updated. However, since the updated representations would not be used in\nthe single layer case, we only have to compute the representation of the newest symbol in this case.\n5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS\n233\ndetermining the position of interest, meaning that every head defines a different query transformation,\nwhile the keys and values transformations are the same among all heads.\nThis concludes the proof. Fig. 5.25 again shows an illustration of the described model with all\nthe defined components.\n■\nThis proof establishes the only “concrete” result on the (lower bound of the) expressivity for\ntransformers in the form of model equivalence (cf. Definition 5.4.1) that we know of. In the next\nsubsections, we discuss how transformer-based language models can simulate more complex formal\nmodels. However, the simulation will not be as “direct” as the n-gram one, in the sense that\nwe will have to work with modified alphabets which, as we noted above, results in a different\nnotion of equivalence of models than what we have considered so far. We will thus not model the\nconditional probabilities pSM py | yătq, but rather the probabilities over some more complex (but\nstill finitely-many) objects, which will carry in them more information than just the generated\nsymbol. As we will discuss, this will be required due to the limited abilities of transformers to\nexecute sequential operations compared to RNNs and classical language models, as hinted at in\n§5.1.5.\n234\nCHAPTER 5. NEURAL NETWORK LANGUAGE MODELS\nValues\nJy1K\nJy2K\n¨ ¨ ¨\nJyt´3K\nJyt´2K\nJyt´1K\nKeys\n´1\n1\n´1\n2\n¨ ¨ ¨\n´1\nt´3\n´1\nt´2\n´1\nt´1\nQuery\nt´3\n1\nAttention scores\n2 ´ t 1 ´ t\n¨ ¨ ¨\n0\n´1\n´2\nHard attention weights\n0\n0\n¨ ¨ ¨\n1\n0\n0\ny1\ny2\n¨ ¨ ¨\nyt´3\nyt´2\nyt´1\nyt\n¨ ¨ ¨\nHead 3\nHead 2\nHead 1\nfH\npSM pyt | yătq\nE\nFigure 5.25: A more complete illustration of the construction described in the proof for the case of\nthe third head, Head 3, based on Fig. 5.24. Note that, among the three heads, only the query vector\n(transformation) differs, while the key and value transformations are identical among the heads.\nIndex\nSymbols\nbos symbol\n18\neos symbol\n21\nσ-algebra\n10\nsoftmax RNN sequence models\n144\nσ-algebra\n10\n(full) path weight\n82\n(weighted) language\n83, 130\nbeginning of sequence\n18\nend of sequence\n21\nn-gram assumption\n97\nA\naccepting\n82, 127\naccepts\n78\naccessible\n84\naccessible state\n84\nactivation function\n146\nalgebra\n11\nallsum\n83, 117, 131\nalphabet\n14\nambiguous\n112\napplicable\n109\nAttention\n211\nattention block\n217\nattention head\n221\nattention matrix\n217\nattention mechanism\n209\naveraging hard attention\n212\nB\nbackward values\n83\nbasis vectors\n48\nbias\n58\nbias vector\n151\nbigram\n98\nbigram model\n98\nC\ncandidate\n155\ncandidate vector\n154\ncategorical distribution\n53\ncenter embeddings\n108\nco-accessible\n84\nco-accessible state\n84\ncolumn matrix\n177\ncomplete\n49\ncomponent-activating matrix\n178, 183\nconcatenation\n14\nconfiguration\n126\nconjugate\n48\nconsistent\n26\ncontext\n21, 97\ncontext encoding function\n52\ncontext-free\n118\ncontext-free grammar\n108\napplicable production\n109\nderivation\n110\nderivation tree\n110\nderive\n110\nlanguage\n110\nnon-terminal\n109\nparse tree\n110\nproduction\n109\nstart-symbol\n109\nterminal\n109\ncontext-free language\n107\ncontext-free languages\n107\ncontextual symbol encodings\n211\ncoordinate vector\n48\ncorpus\n61\ncounter machines\n204\ncross-serial dependencies\n138\ncross-serial dependency\n138\ncurriculum learning\n70\n235\n236\nINDEX\ncylinder set\n30\nD\ndata leakage\n68\ndata sub-vector\n179\nderivation\n110\nderivation set\n111\nderivation set of a grammar\n112\nderivation set of a non-terminal\n112\nderivation tree\n110\nderived from\n110\nderives\n110\ndetectable\n176\ndeterministic\n77, 128\ndeterministic FSA\n77\ndeterministic PDA\n128\ndimensionality\n48\ndistributed word representations\n104\ndivergence measure\n63\ndynamics map\n140\nE\nearly stopping\n71\neasily detectable\n176\nElman sequence model\n149\nembedding function\n51, 149\nembedding tying\n150\nembeddings\n51\nempty string\n14\nencoding\n52\nenergy function\n18\nenergy-based\n18\nentropy regularizer\n72\nentropy regularizers\n72\nequivalent\n227\nevent\n10\nevents\n10\nexposure bias\n66\nF\nfinite-state\n76, 85\nfinite-state automaton\n76\nrecognized language\n78\nstring acceptance\n78\nfirst-moment matrix\n122\nforget\n154\nformal language theory\n14\nfour-hot representation\n183\nFrobenius normal form\n95\nG\ngate\n153\ngated recurrent unit\n155\ngating functions\n153\ngenerating\n115\ngenerating function\n121\nglobally normalized model\n19\ngood representation principle\n47\nH\nhalting problem\n135, 206\nHeaviside\n158\nHeaviside Elman network\n158\nhidden state\n140\nhidden states\n140\nHilbert space\n47, 49\nhistory\n21, 97\nhomomorphically equivalent\n227\nI\ninfinite sequence\n15\ninfinite sequences\n15\ninformation projection\n65\ninitial state\n142\ninner path weight\n82\ninner product\n48\ninner product space\n48\ninput\n154\ninput matrix\n151\ninput string\n77\nirreducible normal form\n95\nJ\nJordan sequence model\n150\nK\nkeys\n211\nKleene closure\n15\nKleene star\n14\nL\nlanguage\n15, 78, 110\nlanguage model\n16, 29\ncontext-free\n118\nenergy-based\n18\nfinite-state\n85\nglobally normalized\n19\nlocally normalized\n21\npushdown\n132\nweighted language\n17\nINDEX\n237\nlanguage model induced by G\n120\nlanguage model induced by P\n132\nlanguage model induced by A\n87\nlanguage modeling task\n61\nlanguage recognized by P\n127\nlayer normalization\n221\nlength\n81\nlevel of a generation sequence\n120\nlikelihood\n64\nlog\n64\npseudo\n66\nline matrix\n177\nlocally normalized language model\n21\nlog-likelihood\n64\nlogits\n52\nlong short-term memory unit\n154\nLSTM\n154\nM\nmasked attention block\n219\nmasking matrix\n219\nmatrix detection\n175\nmatrix representation\n174\nmeasurable space\n10\nmeasurable subset\n10\nmeasurable subsets\n10\nmemory cell\n154\nmonoid\n14\nmulti-head attention\n220\nmulti-head attention block\n221\nmulti-hot\n230\nN\nnon-decreasing\n184\nnon-deterministic\n78, 128\nnon-deterministic FSA\n78\nnon-deterministic PDA\n128\nnon-scanning\n127\nnon-terminating\n29\nnon-tight\n26, 38\nnorm\n49\nnormalizable\n19, 84, 118, 131\nnormalizable energy function\n19\nnormalization constant\n19\nnorthwestern\n176\nO\none-hot encoding\n146\none-hot encodings\n51\noptimization algorithms\n69\noutcome space\n10\noutput\n154\noutput matrix\n151\noverfitting\n71\nP\npad\n98\npadding\n98\nparameter estimation\n68\npath\n81\naccepting\n82\ninner weight\n82\nlength\n81\nsuccessful\n82\nweight\n82\nyield\n81\npermutation-detectable\n176\nposition-augmented\n220\npositional encoding\n220\nprefix\n16\nprefix probability\n22\nPrefixes\n16\nprobabilistic\n84, 116, 129, 134\nprobabilistic context-free grammar\n116\nprobabilistic finite-state automaton\n84\nprobabilistic pushdown automaton\n129\nprobabilistic two-stack pushdown\nautomaton\n134\nprobability\n86, 119\nprobability measure\n11\nprobability pre-measure\n12\nprobability simplex\n53\nprobability space\n11\nprocessing sub-vector\n179\nproduction\nyield\n109\nproduction generating function\n121\nprojection\n53\nprojection function\n53\nproper\n26\npruned\n115\nPruning\n115\npseudo(log)likelihood\n66\npushdown automata\n126\npushdown automaton\n126\naccepting run\n127\nconfiguration\n126\nnon-scanning transition\n127\nrecognized language\n127\nrecognized string\n127\n238\nINDEX\nrun\n127\nscan\n127\nscanning transition\n127\npushdown language model\n132\npushforward measure\n12\nQ\nquery\n211\nR\nrandom variable\n12\nrational-valued recurrent neural network\n142\nrational-valued recurrent neural networks\n142\nreachable\n115\nreal-valued recurrent neural network\n142\nreal-weighted context-free grammar\n115\nreal-weighted finite-state automaton\n79\nreal-weighted pushdown automaton\n129\nrecognizes\n127, 130\nrectified linear unit\n147\nrecurrence matrix\n150\nrecurrent dynamics map\n140\nrecurrent neural encoding function\n143\nrecurrent neural network\n140\nrecurrent neural networks\n140\nrecurrent neural sequence model\n144\nregular\n78\nregular language\n78\nregular languages\n78\nReLU\n147\nrepresentation function\n50\nrepresentation space\n47\nrepresentation-based language modeling 46\nrepresentation-based locally normalized\nmodel\n58\nreset\n155\nresidual connections\n214\nresult of applying\n109\nRNN\n140\nrow matrix\n177\nrun\n127\nS\nsaturated sigmoid\n192\nscalars\n47\nscanning\n127\nscans\n127\nscore function\n92\nscores\n52\nscoring function\n211\nself-attention\n218\nself-supervised\n62\nsentence\n15\nsentences\n15\nsequence\n15\nsequence model\n21, 29\nsequences\n15\nsoft attention\n212\nsoftmax\n54\nsparsemax\n57\nspectral radius\n95\nsquare-root state representation\n173\nstatic symbol embeddings\n149\nstochastic gradient descent\n70\nstrictly n-local\n101\nstrictly n-local\n101\nstrictly local\n101\nstring\n14\nstringsum\n82, 117, 130\nsubregular\n101\nsubregular language\n101\nsubsequence\n16\nsubstochastic\n94\nsubstring\n16\nsuccessful\n82\nsuffix\n16\nsuffixes\n16\nsymbol-specific transition matrix\n80\nsymbols\n14\nT\nteacher forcing\n66\ntemperature\n54\nthin cylinder\n30\ntied\n150\ntight\n21, 26, 38\ntoken\n15\ntoken to type switch\n103, 136\ntokens\n15\ntraining\n68\ntransformer\n215\ntransformer layer\n214\ntransformer network\n210\ntransformer sequence model\n211\ntransition\n76\ntransition matrix\n80\ntransitions\n76\nTrimming\n84\nINDEX\n239\ntwo-stack pushdown automaton\n133\ntwo-stack real-weighted pushdown\nautomaton\n133\ntwo-stack weighted pushdown automaton\n133\nU\nunambiguous\n111, 112\nunique hard attention\n213\nunit\n14\nupdate\n155\nuseful\n84\nuseful state\n84\nV\nvalues\n211\nvector representation\n46, 173\nvector space\n47\nvectors\n47\nvocabulary\n15\nW\nweight\n130\nweight of a derivation tree\n116\nweight pushing\n90\nweighted context-free grammar\n115\nallsum\n117\nderivation tree weight\n116\ninduced language model\n120\nnon-terminal allsum\n117\nnormalizable\n118\nstringsum\n117\nweighted finite-state automaton\n79\nallsum\n83\ninduced language model\n87\nnormalizable\n84\nstate-specific allsum\n83\nstringsum\n82\nsubstochastic\n94\ntransition matrix\n80\ntrim\n84\nweighted language\n17\nweighted pushdown automaton\n129\nallsum\n131\ninduced language model\n132\nnormalizable\n131\nrun weight\n130\nstringsum\n130\nword\n14, 15\nwords\n15\nY\nyield\n81, 109\n240\nINDEX\nBibliography\nSteven Abney, David McAllester, and Fernando Pereira. 1999. Relating probabilistic grammars\nand automata. In Proceedings of the 37th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 542–549, College Park, Maryland, USA. Association for Computational\nLinguistics.\nNoga Alon, A. K. Dewdney, and Teunis J. Ott. 1991. Efficient simulation of finite automata by\nneural nets. J. ACM, 38(2):495–514.\nSt´ephane Aroca-Ouellette and Frank Rudzicz. 2020. On Losses for Modern Language Models.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 4970–4981, Online. Association for Computational Linguistics.\nEnes Avcu, Chihiro Shibata, and Jeffrey Heinz. 2017. Subregular complexity and deep learning.\nArXiv, abs/1705.05940.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to\ncontinuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,\nPAMI-5(2):179–190.\nJ. Baker. 1975a. The DRAGON system–An overview. IEEE Transactions on Acoustics, Speech, and\nSignal Processing, 23(1):24–29.\nJames K. Baker. 1975b. Stochastic Modeling as a Means of Automatic Speech Recognition. Ph.D.\nthesis, Carnegie Mellon University, USA. AAI7519843.\nAnton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam.\n2021. Residual energy-based models for text. Journal of Machine Learning Research, 22(40):1–41.\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational\nLinguistics, 48(1):207–219.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for\nsequence prediction with recurrent neural networks. In Advances in Neural Information Processing\nSystems, volume 28.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and\nnew perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828.\nCite arxiv:1206.5538.\n241\n242\nBIBLIOGRAPHY\nYoshua Bengio, R´ejean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model.\nIn Advances in Neural Information Processing Systems, volume 13. MIT Press.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003a. A neural proba-\nbilistic language model. J. Mach. Learn. Res., 3:1137–1155.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, Christian Jauvin, Jauvinciro Umontreal Ca,\nJaz Kandola, Thomas Hofmann, Tomaso Poggio, and John Shawe-Taylor. 2003b. A neural\nprobabilistic language model. Journal of Machine Learning Research, 3:1137–1155.\nYoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.\n2006. Neural Probabilistic Language Models, pages 137–186. Springer Berlin Heidelberg, Berlin,\nHeidelberg.\nJulian Besag. 1975. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society.\nSeries D (The Statistician), 24(3):179–195.\nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020. On the Ability and Limitations of\nTransformers to Recognize Formal Languages. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 7096–7116, Online. Association for\nComputational Linguistics.\nPatrick Billingsley. 1995. Probability and Measure, 3rd edition. Wiley.\nChristopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer-Verlag, Berlin,\nHeidelberg.\nMathieu Blondel, Andre Martins, and Vlad Niculae. 2019. Learning classifiers with fenchel-young\nlosses: Generalized entropies, margins, and algorithms. In Proceedings of the Twenty-Second\nInternational Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of\nMachine Learning Research, pages 606–615. PMLR.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word\nvectors with subword information. Transactions of the Association for Computational Linguistics,\n5:135–146.\nL. Boltzmann. 1868.\nStudien ¨uber das Gleichgewicht der lebendigen Kraft zwischen bewegten\nmateriellen Punkten: vorgelegt in der Sitzung am 8. October 1868. k. und k. Hof- und Staatsdr.\nT.L. Booth and R.A. Thompson. 1973. Applying probability measures to abstract languages. IEEE\nTransactions on Computers, C-22(5):442–450.\nAdam L. Buchsbaum, Raffaele Giancarlo, and Jeffery R. Westbrook. 2000. On the determinization\nof weighted finite automata. SIAM Journal on Computing, 30(5):1502–1531.\nAlexandra Butoi, Brian DuSell, Tim Vieira, Ryan Cotterell, and David Chiang. 2022. Algorithms\nfor weighted pushdown automata.\nStanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for\nlanguage modeling. In 34th Annual Meeting of the Association for Computational Linguistics,\npages 310–318, Santa Cruz, California, USA. Association for Computational Linguistics.\nBIBLIOGRAPHY\n243\nYining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. 2018. Recurrent\nneural networks as weighted language recognizers. NAACL HLT 2018 - 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies - Proceedings of the Conference, 1:2261–2271.\nZhiyi Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational\nLinguistics, 25(1):131–160.\nZhiyi Chi and Stuart Geman. 1998. Estimation of probabilistic context-free grammars. Computational\nLinguistics, 24(2):299–305.\nDavid Chiang and Peter Cholak. 2022. Overcoming a theoretical limitation of self-attention. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 7654–7664, Dublin, Ireland. Association for Computational Linguistics.\nKyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014a. On the\nproperties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8,\nEighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111,\nDoha, Qatar. Association for Computational Linguistics.\nKyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014b. Learning phrase representations using RNN encoder–\ndecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association\nfor Computational Linguistics.\nN. Chomsky and M.P. Sch¨utzenberger. 1963. The algebraic theory of context-free languages. In\nP. Braffort and D. Hirschberg, editors, Computer Programming and Formal Systems, volume 35\nof Studies in Logic and the Foundations of Mathematics, pages 118–161. Elsevier.\nNoam Chomsky. 1959.\nOn certain formal properties of grammars.\nInformation and Control,\n2(2):137–167.\nNoam Chomsky. 1965. Aspects of the Theory of Syntax, 50 edition. The MIT Press.\nK. Culik and Arto Salomaa. 1978. On the decidability of homomorphism equivalence for languages.\nJournal of Computer and System Sciences, 17(2):163–175.\nGr’egoire Del’etang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot\nCatt, Marcus Hutter, Shane Legg, and Pedro A. Ortega. 2022. Neural networks and the chomsky\nhierarchy. ArXiv, abs/2207.02098.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language understanding.\nIn Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\nMinnesota. Association for Computational Linguistics.\nA. K. Dewdney. 1977. Threshold matrices and the state assignment problem for neural nets. In\nProceedings of the 8th SouthEastern Conference on Combinatorics, Graph Theory and Computing,\npages 227–245, Baton Rouge, La, USA.\n244\nBIBLIOGRAPHY\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early\nstopping. CoRR, abs/2002.06305.\nLi Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, and Ryan Cotterell.\n2022. A measure-theoretic characterization of tight language models.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning\nand stochastic optimization. J. Mach. Learn. Res., 12(null):2121–2159.\nRick Durrett. 2019. Probability: Theory and Examples, 5th edition. Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge university press.\nJason Eisner. 2016. Inside-outside and forward-backward algorithms are just backprop (tutorial\npaper). In Proceedings of the Workshop on Structured Prediction for NLP, pages 1–17, Austin,\nTX. Association for Computational Linguistics.\nJeffrey L. Elman. 1990. Finding structure in time. Cognitive Science, 14(2):179–211.\nPatrick C. Fischer, Albert R. Meyer, and Arnold L. Rosenberg. 1968. Counter machines and counter\nlanguages. Mathematical systems theory, 2:265–283.\nWilliam A. Gale and Geoffrey Sampson. 1995. Good-turing frequency estimation without tears. J.\nQuant. Linguistics, 2:217–237.\nW. G. Gibbs. 1902. Elementary Principles in Statistical Mechanics. Charles Scribner’s Sons.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier neural networks. In\nProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,\nvolume 15 of Proceedings of Machine Learning Research, pages 315–323, Fort Lauderdale, FL,\nUSA. PMLR.\nGlorot, Xavier and Bengio, Yoshua. 2010. Understanding the difficulty of training deep feedforward\nneural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence\nand Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249–256, Chia Laguna\nResort, Sardinia, Italy. PMLR.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2018. FRAGE: Frequency-\nAgnostic word representation. In Proceedings of the 32nd International Conference on Neural\nInformation Processing Systems, NIPS’18, page 1341–1352, Red Hook, NY, USA. Curran Associates\nInc.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\nAndreas Griewank and Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques\nof Algorithmic Differentiation, 2nd edition. SIAM.\nCharles M. Grinstead and J. Laurie Snell. 1997. Introduction to Probability, 2nd revised edition.\nAmerican Mathematical Society.\nMichael Hahn. 2020. Theoretical limitations of self-attention in neural sequence models. Transactions\nof the Association for Computational Linguistics, 8:156–171.\nBIBLIOGRAPHY\n245\nYiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz, and Simon\nMendelsohn. 2018. Context-free transductions with neural stacks. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages\n306–315, Brussels, Belgium. Association for Computational Linguistics.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning.\nSpringer Series in Statistics. Springer New York Inc., New York, NY, USA.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification. In 2015 IEEE International\nConference on Computer Vision (ICCV), pages 1026–1034.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 770–778.\nKenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of\nthe Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland.\nAssociation for Computational Linguistics.\nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013.\nScalable\nmodified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers), pages 690–696, Sofia,\nBulgaria. Association for Computational Linguistics.\nD.O. Hebb. 1949. The Organization of Behavior: A Neuropsychological Theory. A Wiley book in\nclinical psychology. Wiley.\nJohn Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D. Manning. 2020. RNNs\ncan generate bounded hierarchical languages with optimal memory. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 1978–2010,\nOnline. Association for Computational Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word\nrepresentations. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 4129–4138, Minneapolis, Minnesota. Association for Computational\nLinguistics.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation,\n9:1735–80.\nElad Hoffer, Itay Hubara, and Daniel Soudry. 2017.\nTrain longer, generalize better: Closing\nthe generalization gap in large batch training of neural networks. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, pages 1729—-1739, Red\nHook, NY, USA. Curran Associates Inc.\nJohn E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2006. Introduction to Automata Theory,\nLanguages, and Computation (3rd Edition). Addison-Wesley Longman Publishing Co., Inc., USA.\n246\nBIBLIOGRAPHY\nRoger A. Horn and Charles R. Johnson. 2012. Matrix Analysis, 2nd edition. Cambridge University\nPress.\nFerenc Husz´ar. 2015. How (not) to Train your Generative Model: Scheduled Sampling, Likelihood,\nAdversary? CoRR, abs/1511.05101.\nRiny Huybregts, Germen de Haan, Mieke Trommelen, and Wim Zonneveld. 1984. Van periferie naar\nkern. Computational Linguistics.\nThomas Icard. 2020a. Calibrating generative models: The probabilistic chomsky-sch¨utzenberger\nhierarchy. Journal of Mathematical Psychology, 95.\nThomas F. Icard. 2020b. Calibrating generative models: The probabilistic chomsky–sch¨utzenberger\nhierarchy. Journal of Mathematical Psychology, 95:102308.\nP. Indyk. 1995. Optimal simulation of automata by neural nets. In STACS 95, pages 337–348,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nGerhard J¨ager and James Rogers. 2012. Formal language theory: Refining the chomsky hierarchy.\nPhilos Trans R Soc Lond B Biol Sci, 367(1598):1956–1970.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. 2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 3651–3657, Florence, Italy. Association for Computational Linguistics.\nE. T. Jaynes. 1957. Information theory and statistical mechanics. Phys. Rev., 106:620–630.\nF. Jelinek. 1976. Continuous speech recognition by statistical methods. Proceedings of the IEEE,\n64(4):532–556.\nF. Jelinek. 1990. Self-Organized Language Modeling for Speech Recognition, page 450–506. Morgan\nKaufmann Publishers Inc., San Francisco, CA, USA.\nLifeng Jin, Finale Doshi-Velez, Timothy Miller, William Schuler, and Lane Schwartz. 2018. Depth-\nbounding is effective: Improvements and evaluation of unsupervised PCFG induction. In Pro-\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages\n2721–2731, Brussels, Belgium. Association for Computational Linguistics.\nMichael I. Jordan. 1986. Serial order: A parallel distributed processing approach. Technical report.\nMichael I. Jordan. 1997. Chapter 25 - serial order: A parallel distributed processing approach.\nIn John W. Donahoe and Vivian Packard Dorsel, editors, Neural-Network Models of Cognition,\nvolume 121 of Advances in Psychology, pages 471–495. North-Holland.\nDaniel Jurafsky and James H. Martin. 2009.\nSpeech and Language Processing (2nd Edition).\nPrentice-Hall, Inc., USA.\nFred Karlsson. 2007. Constraints on multiple center-embedding of clauses. Journal of Linguistics,\n43(2):365–392.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd\nInternational Conference on Learning Representations.\nBIBLIOGRAPHY\n247\nSamuel A. Korsky and Robert C. Berwick. 2019. On the computational power of rnns.\nMatthieu Labeau and Shay B. Cohen. 2019. Experimenting with power divergences for language\nmodeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 4104–4114, Hong Kong, China. Association for Computational Linguistics.\nDaniel J. Lehmann. 1977. Algebraic structures for transitive closure. Theoretical Computer Science,\n4(1):59–76.\nChu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. 2021a. Limitations\nof autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 5147–5173, Online. Association for Computational Linguistics.\nChu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. 2021b. Limitations\nof autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 5147–5173, Online. Association for Computational Linguistics.\nChu-Cheng Lin and Arya D. McCarthy. 2022. On the uncomputability of partition functions in\nenergy-based sequence models. In International Conference on Learning Representations.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019.\nLinguistic knowledge and transferability of contextual representations. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 1073–1094, Minneapolis,\nMinnesota. Association for Computational Linguistics.\nChristopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020.\nEmergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings\nof the National Academy of Sciences, 117(48):30046–30054.\nAndr´e F. T. Martins and Ram´on F. Astudillo. 2016. From softmax to sparsemax: A sparse model\nof attention and multi-label classification. In Proceedings of the 33rd International Conference on\nInternational Conference on Machine Learning - Volume 48, ICML’16, page 1614–1623. JMLR.org.\nClara Meister, Elizabeth Salesky, and Ryan Cotterell. 2020. Generalized entropy regularization or:\nThere’s nothing special about label smoothing. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 6870–6886, Online. Association for Computational\nLinguistics.\nWilliam Merrill. 2019. Sequential neural networks as automata. In Proceedings of the Workshop on\nDeep Learning and Formal Languages: Building Bridges, pages 1–13, Florence. Association for\nComputational Linguistics.\nWilliam Merrill and Ashish Sabharwal. 2023. The parallelism tradeoff: Limitations of log-precision\ntransformers. Transactions of the Association for Computational Linguistics, 11:531–545.\n248\nBIBLIOGRAPHY\nWilliam Merrill, Ashish Sabharwal, and Noah A. Smith. 2022a. Saturated transformers are constant-\ndepth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843–\n856.\nWilliam Merrill, Alex Warstadt, and Tal Linzen. 2022b. Entailment semantics can be extracted from\nan ideal language model. In Proceedings of the 26th Conference on Computational Natural Language\nLearning (CoNLL), pages 176–193, Abu Dhabi, United Arab Emirates (Hybrid). Association for\nComputational Linguistics.\nWilliam Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, and Eran Yahav. 2020.\nA formal hierarchy of RNN architectures. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 443–459, Online. Association for Computational\nLinguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space.\nGeorge A. Miller and Noam Chomsky. 1963. Finitary models of language users. In D. Luce, editor,\nHandbook of Mathematical Psychology, pages 2–419. John Wiley & Sons.\nThomas Minka. 2005. Divergence measures and message passing. Technical report, Microsoft.\nMarvin Lee Minsky. 1986. Neural Nets and the brain model problem. Ph.D. thesis, Princeton\nUniversity.\nMehryar Mohri, Fernando Pereira, and Michael Riley. 2008. Speech Recognition with Weighted\nFinite-State Transducers, pages 559–584. Springer Berlin Heidelberg, Berlin, Heidelberg.\nJames R. Munkres. 2000. Topology, 2nd edition. Prentice Hall, Inc.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in\nstochastic language modelling. Computer Speech & Language, 8(1):1–38.\nFrank Nielsen. 2018. What is an information projection? Notices of the American Mathematical\nSociety, 65:1.\nFranz Nowak, Anej Svete, Li Du, and Ryan Cotterell. 2023. On the representational capacity of\nrecurrent neural language models. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 7011–7034, Singapore. Association for Computational\nLinguistics.\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for\nword representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational\nLinguistics.\nGabriel Pereyra, George Tucker, Jan Chorowski,  Lukasz Kaiser, and Geoffrey E. Hinton. 2017.\nRegularizing neural networks by penalizing confident output distributions. In Proceedings of the\nInternational Conference on Learning Representations.\nB.T. Polyak. 1964. Some methods of speeding up the convergence of iteration methods. USSR\nComputational Mathematics and Mathematical Physics, 4(5):1–17.\nBIBLIOGRAPHY\n249\nAlethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. 2022. Grokking:\nGeneralization beyond overfitting on small algorithmic datasets. CoRR, abs/2201.02177.\nJorge P´erez, Pablo Barcel´o, and Javier Marinkovic. 2021. Attention is turing-complete. Journal of\nMachine Learning Research, 22(75):1–35.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A Primer in BERTology: What We\nKnow About How BERT Works. Transactions of the Association for Computational Linguistics,\n8:842–866.\nHalsey L. Royden. 1988. Real Analysis, 3rd edition. Prentice-Hall.\nHolger Schwenk. 2007. Continuous space language models. Computer Speech & Language, 21(3):492–\n518.\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal\nLinzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick.\n2022. The multiBERTs: BERT reproductions for robustness analysis. In International Conference\non Learning Representations.\nC. E. Shannon. 1948a. A mathematical theory of communication. The Bell System Technical\nJournal, 27(3):379–423.\nClaude E. Shannon. 1948b. A mathematical theory of communication. The Bell System Technical\nJournal, 27(3):379–423.\nStuart M. Shieber. 1985. Evidence against the context-freeness of natural language. Linguistics and\nPhilosophy, 8:333–343.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities\ninto smaller language models. In Findings of the Association for Computational Linguistics: ACL\n2023, pages 7059–7073, Toronto, Canada. Association for Computational Linguistics.\nHava T. Siegelmann and Eduardo D. Sontag. 1992. On the computational power of neural nets. In\nProceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT ’92, page\n440–449, New York, NY, USA. Association for Computing Machinery.\nMichael Sipser. 2013. Introduction to the Theory of Computation, third edition. Course Technology,\nBoston, MA.\nNoah A. Smith and Mark Johnson. 2007. Weighted and probabilistic context-free grammars are\nequally expressive. Computational Linguistics, 33(4):477–491.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(56):1929–1958.\nAnej Svete, Robin Shing Moon Chan, and Ryan Cotterell. 2024. A theoretical result on the inductive\nbias of rnn language models. arXiv preprint arXiv:2402.15814.\nAnej Svete and Ryan Cotterell. 2023a. Efficiently representing finite-state automata with recurrent\nneural networks.\n250\nBIBLIOGRAPHY\nAnej Svete and Ryan Cotterell. 2023b. Recurrent neural language models as probabilistic finite-state\nautomata. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pages 8069–8086, Singapore. Association for Computational Linguistics.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2015. Re-\nthinking the inception architecture for computer vision. 2016 IEEE Conference on Computer\nVision and Pattern Recognition, pages 2818–2826.\nG´abor Sz´arnyas. 2020. Graphs and matrices: A translation of ”Graphok ´es matrixok” by D´enes\nK˝onig (1931).\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-On What Language\nModel Pre-training Captures. Transactions of the Association for Computational Linguistics,\n8:743–758.\nTerence Tao. 2011. An Introduction to Measure Theory. American Mathematical Society.\nTerence Tao. 2016. Analysis II: Third Edition. Texts and Readings in Mathematics. Springer\nSingapore.\nWilson L. Taylor. 1953. “Cloze Procedure”: A new tool for measuring readability. Journalism\nQuarterly, 30(4):415–433.\nL. Theis, A. van den Oord, and M. Bethge. 2016. A note on the evaluation of generative models. In\n4th International Conference on Learning Representations.\nA. M. Turing. 1937. On Computable Numbers, with an Application to the Entscheidungsproblem.\nProceedings of the London Mathematical Society, s2-42(1):230–265.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  L ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\nProcessing Systems, volume 30.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of\nfinite precision RNNs for language recognition. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pages 740–745, Melbourne,\nAustralia. Association for Computational Linguistics.\nSean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. 2020.\nConsistency of a recurrent language model with respect to incomplete decoding. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n5553–5568, Online. Association for Computational Linguistics.\nGeorge Kingsley Zipf. 1935. The Psycho-Biology of Language. Houghton-Mifflin, New York, NY,\nUSA.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-11-07",
  "updated": "2024-04-17"
}