{
  "id": "http://arxiv.org/abs/2311.04946v1",
  "title": "Causal Inference on Investment Constraints and Non-stationarity in Dynamic Portfolio Optimization through Reinforcement Learning",
  "authors": [
    "Yasuhiro Nakayama",
    "Tomochika Sawaki"
  ],
  "abstract": "In this study, we have developed a dynamic asset allocation investment\nstrategy using reinforcement learning techniques. To begin with, we have\naddressed the crucial issue of incorporating non-stationarity of financial time\nseries data into reinforcement learning algorithms, which is a significant\nimplementation in the application of reinforcement learning in investment\nstrategies. Our findings highlight the significance of introducing certain\nvariables such as regime change in the environment setting to enhance the\nprediction accuracy. Furthermore, the application of reinforcement learning in\ninvestment strategies provides a remarkable advantage of setting the\noptimization problem flexibly. This enables the integration of practical\nconstraints faced by investors into the algorithm, resulting in efficient\noptimization. Our study has categorized the investment strategy formulation\nconditions into three main categories, including performance measurement\nindicators, portfolio management rules, and other constraints. We have\nevaluated the impact of incorporating these conditions into the environment and\nrewards in a reinforcement learning framework and examined how they influence\ninvestment behavior.",
  "text": " \n \nCausal Inference on Investment Constraints and Non-stationarity in Dynamic \nPortfolio Optimization through Reinforcement Learning \nYasuhiro Nakayama1, Tomochika Sawaki 2 \n1 Mizuho Research & Technologies, Ltd. \n2 Mizuho Bank, Ltd. \nyasuhiro.nakayama@mizuho-rt.co.jp, tomochika.sawaki@mizuho-bk.co.jp \n \nAbstract: In this study, we have developed a dynamic asset allocation investment strategy using \nreinforcement learning techniques. To begin with, we have addressed the crucial issue of incorporating \nnon-stationarity of financial time series data into reinforcement learning algorithms, which is a \nsignificant implementation in the application of reinforcement learning in investment strategies. Our \nfindings highlight the significance of introducing certain variables such as regime change in the \nenvironment setting to enhance the prediction accuracy. Furthermore, the application of reinforcement \nlearning in investment strategies provides a remarkable advantage of setting the optimization problem \nflexibly. This enables the integration of practical constraints faced by investors into the algorithm, \nresulting in efficient optimization. Our study has categorized the investment strategy formulation \nconditions into three main categories, including performance measurement indicators, portfolio \nmanagement rules, and other constraints. We have evaluated the impact of incorporating these conditions \ninto the environment and rewards in a reinforcement learning framework and examined how they \ninfluence investment behavior. \n \n1. Introduction \n In recent years, remarkable progress has been made in \nresearch and development applying machine learning to \ninvestment strategies. These research directions can be \nbroadly categorized into two types. The first category \ninvolves reading or interpreting unstructured data, such as \ntext and images, using models built by deep learning. The \nsecond category encompasses using machine learning \nmodels to estimate parameters that were previously \nestimated by linear or statistical models in financial \nengineering. \nThe advantage of the latter model of machine learning \nis that it can be extended to non-linear and more complex \nmodels, such as deep learning and ensemble machine \nlearning, thus suggesting the possibility of estimating \nparameters in higher dimensions than conventional \nmodels. This leads to the construction of models with \nhigher operational performance. Typically, the investment \ndecision-making process of a portfolio manager, using the \nquant method, can be segmented into three steps: data \ncollection and processing, analysis and signal generation, \nand portfolio optimization. The first step is related to data \ncollection and processing, and the second step is related to \nanalysis and signal generation. In this study, our focus is \non the potential application of machine learning and \nartificial intelligence technologies in the formulation stage \nof investment decision making, which corresponds to the \nthird step. \nIn particular, portfolio management by institutional \ninvestors, also known as asset owners, is a part of the \nlarger goal of enhancing enterprise value. In such \ncircumstances, it becomes necessary to search for an \noptimal portfolio for the company based on a multifaceted \nunderstanding of financial accounting, legal and \nregulatory affairs, and taxation, rather than aiming for a \ntheoretically efficient portfolio. Therefore, this study \nexamines the utility of reinforcement learning methods for \nformulating \noptimal \nportfolios \nincorporating \nsuch \npractical constraints.  \n \n2. Related research \nReinforcement learning is a field of machine learning \nthat is designed to learn sequential decision rules. The \nunique feature of reinforcement learning is that it rewards \nobjectives and learns how to achieve those objectives from \nthe data without presuming complete knowledge about the \nsystem or environment to which it is applied [1]. \nIn \ninvestment \ndecision \nmaking, \nthe \nprimary \nconsideration when using reinforcement learning pertains \nto the modeling of non-stationarity in financial time series \ndata. This implies that mean, variance, and covariance are \nnot constant over time and that several events have been \n \n \nhistorically observed, such as sudden shifts in phase or \nspikes in volatility. A specific approach to addressing non-\nstationarity in reinforcement learning has been described \nin [2]. Furthermore, research studies focusing on non-\nstationarity in financial time series data and employing \nreinforcement learning in asset allocation are included in \n[3-16]. \n \n3 Method \n3.1 Data Preprocessing \nIn this study, we examine dynamic optimal allocation \nthrough decision making for rebalancing two assets, \nnamely, a risky asset and a non-risky asset, using daily \ndata from April 2000 to March 2023. We choose the total \nreturn index of the S & P500, denominated in USD, as the \nrisky asset and the total return index of US Treasury Index, \nalso denominated in USD, as the non-risky asset. The data \nis sourced from Bloomberg, and the daily return is \ncalculated for the analysis. We don’t take into account \nfunding costs, transaction costs, and cash ratios.  \n \n3.2 Learning Techniques \n(1) learning technique \nIn common with Chapters 4 and 5, the process of \nreinforcement learning model learning in this study is as \nfollows. Reinforcement learning involves maximizing the \nsum of rewards through the observation of states from the \nenvironment and the corresponding action. To analyze the \nbehavior \nof \neach \nstate, \nwe \nutilize \ntable-based \nreinforcement learning methods such as SARSA and Q-\nlearning.  \n<SARSA Update Expression > \n𝑄′𝜋(𝑆𝑡, 𝐴𝑡) = 𝑄𝜋(𝑆𝑡, 𝐴𝑡)\n+ α{𝑅𝑡+ 𝛾𝑄𝜋(𝑆𝑡+1, 𝐴𝑡+1) −𝑄𝜋(𝑆𝑡, 𝐴𝑡)} \n<Q Learning Update Expression > \nQ′(𝑆𝑡, 𝐴𝑡) = Q(𝑆𝑡, 𝐴𝑡)\n+ α {𝑅𝑡+ 𝛾max\n𝑎\n𝑄(𝑆𝑡+1, 𝑎) −𝑄(𝑆𝑡, 𝐴𝑡)} \nwhere 𝑄 is the action value, 𝑅 is the immediate reward, 𝑆 is the state, 𝐴 \nis the action, 𝛼 is the learning rate, and 𝛾 is the discount rate. The ε-greedy \nmethod is used to study 1,000 episodes for each study period. \n(2) behavior \nEach year, starting with 50% risky assets and 50% non-\nrisky assets, the allocation is changed by 10% on a daily \nbasis. There are three possible actions: [risky assets: +10%, \nnon-risky assets: -10%] [no weight change] [risky assets: \n-10%, non-risky assets: +10%]. However, if the weight \nhas already reached 100%, the weight will not change \neven if it chooses to increase the weight.  \n(3) remuneration \nThe basic compensation is determined by taking the \ndifference between the Sharpe ratio of the portfolio and \nthe Sharpe ratio of a benchmark portfolio for both the \nyear-to-date and the last 10 days, and then summing these \ndifferences to obtain the basic compensation. For the \nbenchmark portfolio, the Sharpe ratio is determined at the \nhighest allocation with fixed weights for both risky and \nnon-risky assets for each year. This kind of allocation is \nregarded as a correct allocation that is ascertainable \nretrospectively.  \nThe Sharpe ratio is calculated as follows:𝑆𝑅 \n𝑆𝑅= 𝑟\n𝜎 \nwhere 𝑟  is the portfolio return and 𝜎  is the standard \ndeviation of the portfolio return. The risk-free rate is 0. \nThe basic reward at time t is given by the following \nformula:𝑅𝑡\n𝐵𝑎𝑠𝑒 \n𝑅𝑡\n𝐵𝑎𝑠𝑒= (𝑆𝑅𝑡−𝑆𝑅𝑡\n𝐵𝑀) + (𝑆𝑅𝑡\n10 −𝑆𝑅𝑡\n𝐵𝑀,10) \n𝑆𝑅𝑡\n𝐵𝑀is the Sharpe ratio of the benchmark portfolio, and \n𝑆𝑅𝑡\n10 is the Sharpe ratio calculated from the returns over \nthe last 10 days. \nIn each analysis, a different reward may be added to the \nbasic reward, which will be described in detail later. \n(4) State \nThe status of each analysis case is different and will be \ndescribed later. \n4. Out of sample back-test considering regime \nchange \n4.1 Analytical Methods \nIn this chapter, we investigate how performance \nprediction using reinforcement learning models is \ninfluenced by the non-stationarity of financial time series \ndata, particularly when the mean and variance change over \ntime. We consider two models. The first model solely \ndefines the state space in terms of the expected returns of \nrisky and non-risky assets. For the expected return, we \nsimply use the price difference (momentum) from 60 \nbusiness days ago. Binary classification based on the \npositive and negative momentum of the asset is performed, \nand the state space is divided into four states of 2x2. As \nfor the second model, it adds the correlation coefficient \nbetween the risky asset and the non-risky asset to the state \nvariable. The correlation coefficient is estimated from \ndaily data for the past 60 business days. The correlation \ncoefficient state variable is divided into three states of \npositive correlation/no correlation/negative correlation \nwith a threshold of ±0.2 and is combined with the expected \nreturn, resulting in a total of 12 states. SARSA is used in \n \n \nthis chapter. The first model is referred to as the base \nmodel, the second model as the non-stationary model and \nthe back-test's evaluation is also compared to a random \nmodel using a Q-table as a random variable. The random \nmodel generates random Q-tables 1000 times to measure \nperformance. \n4.2 Back-test Results \nDuring the learning period, the entire fiscal year from \nApril to March of each year is used as a set to estimate the \nQ-table for each set. For out-of-sample back-test \nvalidation, Q-tables estimated in the past learning period \nrather than the validation period are utilized, and those in \nthe future period are not applied. For example, when \nconducting out-of-sample back-test verification from \nApril 2018 to March 2019, the average value of each \nelement of the total 18 Q-tables estimated for the study \nperiod from FY 2000 to FY 2017 is calculated using equal \nweights. Additionally, the rebalancing frequency is daily, \nand transaction costs are not considered. \nTable 1 compares the out-of-sample performance for \neach year with the median of the base model, the non-\nstationary model, and the random model. Back-test \nperformance is defined by the Sharpe ratio, which is the \nannual return divided by the annual standard deviation. \nThe non-stationary model's average Sharpe ratio from \nFY2001 to FY2022 is higher than that of the base model, \nand the difference is statistically significant. The non-\nstationary model also outperforms the median of the \nrandom model on average Sharpe ratios. Comparing the \nresults of each year, the first half of the back-test period, \ni.e., the 2000s, have more years in which the non-\nstationary model performs worse than the base model than \nthe second half of the back-test period, i.e., the 2010s. This \ncould be due to the learning period beginning in FY 2000, \nwhereby sufficient time to forecast the future is not \nprovided. \nFigure 1 compares the probability density of the Sharpe \nratio of the random model with that of the random Q-table. \nThe purpose of this analysis is to assume that there are \nsome years in which the performance of an asset returns \nsignificantly depending on the shape of its short-term \nmomentum and daily change rate distribution within the \nyear, and some years in which it does not. For example, an \nevent (such as a presidential election or during a \nsignificant event, such as a FOMC meeting), the \nprobability density of a random model is known to be \nbimodal, and annual performance changes dramatically \ndepending on whether the portfolio holds a significant \nweight of risky assets at that time. \nTable 1: Sharpe ratio for each year of back-testing \n \nFigure 1: Results for the last 10 years (horizontal axis: \nSharpe ratio) \n \n*Bond100 = 100% non-risky assets, 50:50 = 50% risky assets: 50% non-risky assets, Stock100 = 100% \nrisky assets Buy & hold for 1 year \n \n \nIn such a case, the non-stationarity of financial time series \ndata strongly emerges, confirming the effectiveness of the \nnon-stationary model. As shown in Table 1, it was \nobserved that the Sharpe ratio of the non-stationary model \nexceeded the median Sharpe ratio of the random model in \nmore than half of the years. Moreover, it was confirmed \nthat in years with a bimodal shape, the non-stationary \nmodel generally had fewer years within the range of the \nlower peak of the Sharpe ratio. These results suggest that \nadding a phase to the state variable of a reinforcement \nlearning model contributes to a significant improvement \nin prediction accuracy. \n \n5. Comparison of decision making considering \ninvestment constraints \n5.1 Analytical Methods \nIn this chapter, we examine how the behavior selected \nin reinforcement learning transforms and impacts \nperformance when considering practical restrictions set in \nchapter 3 while the previous chapter focuses on prediction \neffectiveness, this chapter compares the results of in-\nsample learning to analyze the relationship between \ndifferent constraints and decision making. Table 4 \nillustrates three categories of indicators and rules to \nconsider when optimizing portfolios at each time point. \nThe first category is a kind of performance measurable \nindicators. For instance, besides the target returns and \nSharpe ratios for the fiscal year, risk indicators such as \nVaR (value-at-risk) and drawdown, and management \nindicators related to portfolio risk returns such as loss cut \npoints are utilized. The second category comprises rules \nand regulations associated with portfolio management. \nFor instance, rules related to periods such as investment \nhorizons and rebalancing frequency, regulations related to \nsettlement such as margin and clearing, regulations related \nto financial indicators such as leverage ratio, risk weighted \nassets, and liquidity ratio, and regulations related to \nvarious financial supervisory authorities, such as the \nVolcker rule, are assumed. The third category comprises \nthe constraints that can be considered, such as the accuracy \nof expected returns and transaction costs. \n \nTable 4: Examples of Indicators and Rules Considered \nin Portfolio Optimization \nCategory \nExample \n① \nPerformance \nManagement \nIndicators \nTarget return, target volatility, risk \nindicators (VaR, drawdown, etc.), loss \ncut point, etc. \n②Portfolio Management Rules \nPeriod, Settlement, Finance, Regulation, \netc. \n③Other Constraints \nInformation sources, expected return \naccuracy, transaction costs, etc. \nIn this chapter, we establish a benchmark portfolio and \ncompare its performance with that of learning with each \nconstraint. Before conducting back-testing, we evaluate \nthe changes in performance and behavior resulting from \nthe signal accuracy concerning category (3) shown in \nTable 4. A binary signal is generated using the answer to \nwhether the Sharpe ratio of the risky asset or the non-risky \nasset is higher for the next five business days, and the \nstatus is set accordingly. Additionally, we explore the case \nwhere a certain percentage of the signal is reversed (i.e., \nsubstituted with the wrong signal). The rate of inversion is \nclassified into steps of 10%, and six patterns are examined \nuntil the true positive: false =50:50 to ascertain the \nperformance change. Figure 2 depicts signal accuracy and \nperformance. As in earlier sections, the years ranging from \nFY 2000 to FY 2022 are analyzed one by one, and the \ndistribution of Sharpe ratios obtained each year is plotted \nin the in-sample. In addition to the monotonic increase in \nthe median Sharpe ratio as the horizontal axis moves right \n(as signal accuracy increases), the distribution width \nexpands as the signal's accuracy approaches 100%. This \nimplies that the Sharpe ratio expected to be obtained may \nnonlinearly increase as the information's dominance and \nsignal accuracy increase. \n \nFigure 2: Signal Accuracy and Performance \n \n \n5.2 Back-test Results \nFirst, a total of seven kinds of back-tests were set for \ncategory (1) shown in Table 4, as shown in Table 5 below, \nand changes in behavior are compared. In this chapter, we \nuse Q-learning. \n \n \nTable 5: List of Back-test Settings for Category (1) \nPerformance Management Metrics \nNo. \nstate variables \nrewards \n#001 \nsignal, position, age \nbasic reward \n#002 \nSignal, Position, Quarterly, \nTarget Achievement Status \n(1 step) \nbasic reward \n+ Target Achievement Reward (1 \nlevel) \n#003 \nSignals, Position, Quarterly, \nTarget Achievement Status \n(2 levels) \nbasic reward \n+ Target Achievement Reward (2 \nlevels) \n#004 \nSignal, Position, Quarterly, \nDD Occurrence (1 level) \nbasic reward \n+DD penalty (one level) \n#005 \nSignal, Position, Quarterly, \nDD Occurrence (2 levels) \nbasic reward \n+DD penalty (2 levels) \n#006 \nSignal, Position, Quarterly, \nTarget Achievement Status \n(1 step), DD Occurrence \nStatus (1 step) \nbasic reward \n+ Target achievement reward (one \nlevel), \n+DD penalty (one level) \n#007 \nSignal, Position, Quarterly, \nTarget Achievement Status \n(2 Stages), DD Occurrence \nStatus (2 Stages) \nbasic reward \n+ Target Achievement Reward (2 \nlevels) \n+DD penalty (2 levels) \n \nWe utilize a binary signal to determine whether the \nobserved Sharpe ratio of the risky asset or the non-risky \nasset performs better. With an Accuracy of 60%, we use \nthe Accuracy 60% signal to examine the behavior learned \nby reinforcement learning in a probabilistic situation \nwhere signal effectiveness varies. Depending on the \ncurrent holding ratio, three positions are generated: high \nratio of risky assets/equal ratio of non-risky assets/high \nratio of non-risky assets. This is because the meaning of \nthe \"no weight change\" behavior depends on the current \noverweight asset. We divided the year into four equal parts \nor quarters as the elapsed period and created four \nconditions. \nWe analyze the selected actions resulting from learning. \n[No weight change] is chosen as a preference for risky \nassets when the risky asset ratio is elevated and as a \npreference for non-risky assets when the non-risky asset \nratio is elevated. (The calculation is excluded when No \nweight change is selected in the equal-weight state.) We \ncombined the results of all 23 years to establish which \nbehaviors are chosen in each condition and the \ncorresponding rates. \nOur base case is defined as back-test number #001, and \nthe difference from the base case is primarily examined. \nFigure 3 illustrates the percentage of risky assets selected \nwhen the signal shows better performance of risky assets \nand deducts the percentage of risky assets selected when \nthe signal indicates better performance of non-risky assets.  \n \nFigure 3: Signals and Behavior \n \n \nIn all quarters, when the signal indicated the \nperformance of the risky asset, the reinforcement learning \nalgorithm is more inclined to accumulate the risky asset, \ndemonstrating that it progresses as per the signals. \nAdditionally, it is noted that the difference tends to \ndecrease as the period progressed. \nNext, we examine the ratio of the result of #002 by \ncomparing it to the base case. The target achievement \nstatus in the state variable is defined as a binary value of \nwhether the cumulative return from the beginning of the \nperiod exceeded 5%. Likewise, an additional reward (+1) \nis added if the cumulative return exceeds 5% from the \nbeginning of the period. Analyzing the difference in the \nrate of action from the base case reveals that, in Q3 and \nQ4, the rate of accumulation of risky assets decreases \nwhen the target is achieved, and the signal don’t suggest \nthe performance of risky assets. If they remain in this state, \nthey would receive additional rewards. Therefore, they \nmight have learned that it is preferable to avoid risks by \nmaintaining this state rather than taking additional risks by \nreturning to a state where the target has not been attained. \nThis outcome aligns with the expectation that it is better \nto take a risk-averse approach towards the final half of the \nyear when the target is achieved. \nMoreover, it is observed that when targets are not \nachieved, the proportion of risky assets selected increased, \nparticularly in the fourth quarter. This implies that to attain \nan additional reward for achieving the target, they learn to \nfavor risky assets with high volatility, that is, large price \nranges, aiming to achieve the target within a limited \nremaining time. \n \n \n \n \n \nTable 6. Change in Risky asset Preferences #002 \n \n In #003, we examine the situation where a goal is defined \nin two steps. For state variables and rewards, we set a two-\nstage target of 5% and 10% cumulative returns from the \nbeginning of the period, with additional compensation of \n+1 at 5% and +2 at 10%. Analyzing the difference in \nbehavior from the base case reveals that the proportion of \nselecting risky assets increased, even when the first-stage \ntarget is achieved, compared to #002. It is suggested that \nby providing incentives in the form of additional rewards \neven after achieving the goal, it is possible to make \ndecisions without reducing the risk preference. \nTable 7. Change in Risky asset Preferences #003 \n \n Next, we discuss drawdown in #004. Similar to the \nachievement of target #002, the cumulative return from \nthe beginning of the period is measured, and two values \nare added to the state variable, indicating whether the \ndrawdown exceeds 5%. Whenever it exceeds, the reward \nis penalized (-1). Comparing the rate of action from the \nbase case revealed that the proportion of choosing to add \nrisky assets declined when the drawdown don’t happen, \neven when it indicates the performance of risky assets. \nThis phenomenon is prominent in the early to middle parts \nof the back-test period. This change could be attributed to \na shift towards risk-averse decision-making to avoid the \ndrawdown penalty.  \nTable 8. Change in Risky asset Preferences #004 \n \n In #005, we analyze the scenario where drawdown is set \nto two levels. The cumulative return of the back-test from \nthe beginning of the period is evaluated, and three values \nare added to the state variable, indicating whether the \ndrawdown exceeds 5% and 10%. Additionally, the penalty \nadds to the reward is also set to two levels. The extra \npenalties are -1 for 5% and -2 for 10%. Compared to the \nbase case, the proportion of selecting risky assets increases \nbeyond the second drawdown, regardless of the signal. \nThis implies that when performance deteriorates \nsignificantly, decisions are inclined towards favoring risky \nassets with high volatility to avoid penalties.  \nTable 9. Change in Risky asset Preferences #005 \n \n In #006, both the target achievement status and the \noccurrence status of drawdown are considered. Analyzing \nthe conditions in which the signal indicates the \nperformance of the risky asset, the target has not been \nachieved, and no drawdown has occurred reveals that the \nproportion of selecting risky assets is lower than the base \ncase in the first half of the back-test period, while it \nincreases in the second half. This indicates that avoidance \nof penalties is prioritized during a certain period, resulting \nin a preference for risk-averse decisions from the \nbeginning of the period. However, towards the latter half \nof the period, priority is given to achieving the goals, \nleading to a transition to risk-preference decision-making.  \nTable 10. Change in risky asset preference ratio #006 \n \n Finally, in #007, we set two levels for both the target \nachievement status and the drawdown occurrence status. \nThe results of this action selection demonstrate the same \nbehavior as when the goal and the drawdown are \nseparately set, either in two stages or in single stages.  \nTable 11. Change in risky asset preference ratio #007 \n \nNext, we compare the categories (2) illustrated in Table \n4, focusing on the rebalancing frequency constraint as the \nsimplest constraint. We compare four back-test scenarios: \ndaily, weekly, biweekly, and monthly rebalancing, with no \nrestrictions on rebalancing frequency. The results are \npresented in Figure 4. As expected, the unconstrained \ncondition exhibits the highest average Sharpe ratio, and \nperformance gradually decreases as the interval is \nextended.  \n \n \nFigure 4: Rebalancing constraints \n \nWe subsequently analyze the impact of signal accuracy \non decision-making in relation to category (3). Similar to \n#002, we vary the signal Accuracy and compare the \nselected behaviors. Figure 5 illustrates the changes in \nsignal accuracy and the proportion of assets selected in \nresponse to the signals. Each is compared to a signal with \nan Accuracy 50% (i.e., random). As targets are seldom met \nin the initial half of the fiscal year, calculations are limited \nto 3Q and 4Q only. The rate of selection for non-risky \nassets increases as signal accuracy improved, whereas for \nrisky assets, the trend is discerned in the order of each \nAccuracy, albeit it is weaker than that for non-risky assets. \nBased on these findings, it can be assumed that the model \nis able to objectively assess the accuracy of the given \nsignal and make appropriate decisions. The weaker trend \nfor risky assets could be attributed to the fact that the \nreward for additional risk-taking is deemed to be smaller \nthan the reward for target achievement. \nFigure 5: Changes in signal accuracy and asset selection \nratio \n \n \nLastly, we examine cases where signal accuracy varies \nfrom one phase to another. We divide the data into two \nphases: one phase consistently uses a signal of Accuracy \n60%, while the other phase employs Accuracy ranging \nfrom 100% to 50%, which is added to the state variables. \nIn Fig. 6, the momentum generated by the past returns of \nthe non-risky asset is applied as a dummy phase for the \nstate variable. The horizontal axis illustrates the \ndifference in Accuracy, whereas the vertical axis \nindicates the difference between the two phases for the \nselected proportion of the asset in a state suggesting \noutperformance. As the horizontal axis shifts rightward \nand the accuracy in one aspect becomes higher, the \nproportion of selecting the asset indicated by the signals \nincreases. This outcome is confirmed for both risky and \nnon-risky assets. These results indicate that individuals \nare able to learn the appropriate aspect to act upon with \nconfidence, even though the difference in signal accuracy \nis not explicitly given. \nFigure 6: Signal accuracy difference and asset \nselection ratio \n \n \n6. Summary \nThis study examines the optimization of dynamic asset \nallocation using reinforcement learning. The following are \nthe conclusions drawn from this research. \nFirstly, it is suggested that incorporating variables \nrelated to phase change into the state space of \nreinforcement learning enhances the out-of-sample \nperformance. \nSecondly, it is demonstrated that decision-making alters \nby \nincorporating \npractical \nconstraints \ninto \nthe \nreinforcement learning conditions and rewards. Further \ndetails are as follows.  \n1 \nWhen targets are achieved during a period, risk-averse behavior tends to \noccur towards the end of the period. \n2 \nConversely, if the objective is not accomplished, more aggressive risk-taking \nis preferred towards the end of the period. \n3 \nIt is learned as 1 and 2 without explicitly giving the volatility of each asset. \n4 \nEven if the first-stage objective is achieved in the case of a two-stage target, \ndecision-making that favors risk-taking for obtaining additional returns is \nretained. \n \n \n5 \nEnforcing a drawdown threshold and imposing penalties result in risk-averse \nbehavior toward avoiding surpassing the threshold. \n6 \nIn the case of a two-stage drawdown threshold, risk-taking is taken to recover \nif it is exceeded in the second stage. \n7 \nIn situations where both target and drawdown are defined, risk-averse \ndecisions are favored towards avoiding exceeding the drawdown threshold \nduring the first half of the period, with risk-taking becoming active in favor \nof achieving the goal in the second half. \n8 \nMoreover, performance deteriorates with longer intervals between \nrebalancing frequencies. \n9 \nThe less accurate the signal, the more conservative decision-making becomes \neven without its explicit specification. \n10 \nIf signal accuracy varies from one phase to another, it is learned which phase \nhas the highest accuracy, increasing the confidence level of action in the \nphase with high accuracy, even if it is not explicitly provided. \n \n7. Future Issues \nIn the future, we would like to analyze the behavior of \nthe Category 2 portfolio management rules defined in \nChapter 5 by adding specific practical settings other than \nrebalancing constraints.  \n \nNotes \nThe contents and views of this paper belong to the \nauthors and are not the official views of their companies. \n \nReferences \n[１] \nTetsuro Morimura: Machine Learning Professional Series Reinforcement \nLearning, Kodansha (2020). \n[２] \nYasutoki SAITO Deep Learning from Scratch Learning in Python Theory \nand Implementation of Deep Learning. O'Reilly Japan (2017). \n[３] \nAlmahdi, Saud, and Steve Y. Yang. \"An adaptive portfolio trading system: \nA risk-return portfolio optimization using recurrent reinforcement learning \nwith expected maximum drawdown.\" Expert Systems with Applications 87 \n(2017): 267-279. \n[４] \nHambly, Ben, Renyuan Xu, and Huining Yang. \"Recent advances in \nreinforcement learning in finance.\" Mathematical Finance 33.3 (2023): 437-\n503. \n[５] \nHu, Yuh-Jong, and Shang-Jen Lin. \"Deep reinforcement learning for \noptimizing finance portfolio management.\" 2019 amity international \nconference on artificial intelligence (AICAI). IEEE, 2019. \n[６] \nJiang, Zhengyao, Dixing Xu, and Jinjun Liang. \"A deep reinforcement \nlearning framework for the financial portfolio management problem.\" arXiv \npreprint arXiv:1706.10059 (2017). \n[７] \nLiang, Zhipeng, et al. \"Adversarial deep reinforcement learning in portfolio \nmanagement.\" arXiv preprint arXiv:1808.09940 (2018). \n[８] \nMihatsch, Oliver, and Ralph Neuneier. \"Risk-sensitive reinforcement \nlearning.\" Machine learning 49 (2002): 267-290. \n[９] \nMoody, John, et al. \"Performance functions and reinforcement learning for \ntrading systems and portfolios.\" Journal of forecasting 17.5‐6 (1998): 441-\n470. \n[１０] Moody, John, and Matthew Saffell. \"Learning to trade via direct \nreinforcement.\" IEEE transactions on neural Networks 12.4 (2001): 875-\n889. \n[１１] Neuneier, Ralph. \"Enhancing Q-learning for optimal asset allocation.\" \nAdvances in neural information processing systems 10 (1997). \n[１２] Papoudakis, Georgios, et al. \"Dealing with non-stationarity in multi-agent \ndeep reinforcement learning.\" arXiv preprint arXiv:1906.04737 (2019). \n[１３] Padakandla, Sindhu, Prabuchandran KJ, and Shalabh Bhatnagar. \n\"Reinforcement learning algorithm for non-stationary environments.\" \nApplied Intelligence 50 (2020): 3590-3606. \n[１４] Resende, Mauricio GC, Jorge Pinho de Sousa, and Alexander Nareyek. \n\"Choosing search heuristics by non-stationary reinforcement learning.\" \nMetaheuristics: Computer decision-making (2004): 523-544. \n[１５] Wang, Haoran, and Xun Yu Zhou. \"Continuous‐time mean–variance \nportfolio selection: A reinforcement learning framework.\" Mathematical \nFinance 30.4 (2020): 1273-1308. \n[１６] Ye, Yunan, et al. \"Reinforcement-learning based portfolio management with \naugmented asset movement prediction states.\" Proceedings of the AAAI \nConference on Artificial Intelligence. Vol. 34. No. 01. 2020. \n",
  "categories": [
    "q-fin.PM",
    "cs.AI"
  ],
  "published": "2023-11-08",
  "updated": "2023-11-08"
}