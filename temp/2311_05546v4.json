{
  "id": "http://arxiv.org/abs/2311.05546v4",
  "title": "Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization",
  "authors": [
    "Michael Kölle",
    "Felix Topp",
    "Thomy Phan",
    "Philipp Altmann",
    "Jonas Nüßlein",
    "Claudia Linnhoff-Popien"
  ],
  "abstract": "Multi-Agent Reinforcement Learning is becoming increasingly more important in\ntimes of autonomous driving and other smart industrial applications.\nSimultaneously a promising new approach to Reinforcement Learning arises using\nthe inherent properties of quantum mechanics, reducing the trainable parameters\nof a model significantly. However, gradient-based Multi-Agent Quantum\nReinforcement Learning methods often have to struggle with barren plateaus,\nholding them back from matching the performance of classical approaches. While\ngradient free Quantum Reinforcement Learning methods may alleviate some of\nthese challenges, they too are not immune to the difficulties posed by barren\nplateaus. We build upon an existing approach for gradient free Quantum\nReinforcement Learning and propose three genetic variations with Variational\nQuantum Circuits for Multi-Agent Reinforcement Learning using evolutionary\noptimization. We evaluate our genetic variations in the Coin Game environment\nand also compare them to classical approaches. We showed that our Variational\nQuantum Circuit approaches perform significantly better compared to a neural\nnetwork with a similar amount of trainable parameters. Compared to the larger\nneural network, our approaches archive similar results using $97.88\\%$ less\nparameters.",
  "text": "Multi-Agent Quantum Reinforcement Learning using Evolutionary\nOptimization\nMichael K¨olle1, Felix Topp1, Thomy Phan2, Philipp Altmann1, Jonas N¨ußlein1 and Claudia\nLinnhoff-Popien1\n1Institute of Informatics, LMU Munich, Munich, Germany\n2Thomas Lord Department of Computer Science, University of Southern California, Los Angeles, USA\nmichael.koelle@ifi.lmu.de\nKeywords:\nQuantum Reinforcement Learning, Multi-Agent Systems, Evolutionary Optimization\nAbstract:\nMulti-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driv-\ning and other smart industrial applications. Simultaneously a promising new approach to Reinforcement\nLearning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a\nmodel significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods of-\nten have to struggle with barren plateaus, holding them back from matching the performance of classical\napproaches. While gradient free Quantum Reinforcement Learning methods may alleviate some of these\nchallenges, they too are not immune to the difficulties posed by barren plateaus. We build upon an exist-\ning approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with\nVariational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We\nevaluate our genetic variations in the Coin Game environment and also compare them to classical approaches.\nWe showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural\nnetwork with a similar amount of trainable parameters. Compared to the larger neural network, our approaches\narchive similar results using 97.88% less parameters.\n1\nINTRODUCTION\nArtificial intelligence (AI) continues to advance,\noffering innovative solutions across various do-\nmains. Key applications include autonomous driving\n(Shalev-Shwartz et al., 2016), the internet of things\n(Deng et al., 2020), and smart grids (Dimeas and\nHatziargyriou, 2010). Central to these applications\nis the use of Multi-Agent Systems (MAS). These\nagents, though designed to act in their own interest,\ncan be guided to work together using Multi-Agent Re-\ninforcement Learning (MARL). Notably, MARL has\nproven effective, especially in resolving social dilem-\nmas (Leibo et al., 2017b).\nReinforcement Learning (RL) itself has made\nimpressive strides, outperforming humans in areas\nlike video games (Badia et al., 2020; Schrittwieser\net al., 2019). Alongside this, quantum technologies\nare emerging, suggesting faster problem-solving and\nmore efficient training in RL (Harrow and Montanaro,\n2017). However, Quantum Reinforcement Learning\n(QRL) has it’s challenges, such as instabilities and\nvanishing gradients (Franz et al., 2022; Chen et al.,\n2022). To address these, researchers have turned to\nevolutionary optimization methods, as proposed by\n(Chen et al., 2022), which have shown promising re-\nsults. With the rising prominence of MARL, combin-\ning it with quantum techniques has become a research\nfocal point, leading to the development of Multi-\nAgent Quantum Reinforcement Learning (MAQRL).\nIn this work, each agent is represented as a Variational\nQuantum Circuits (VQC). We employ a evolutionary\nalgorithm to optimize the parameters of the circuit.\nWe evaluate different generational evolution strate-\ngies and conduct a small scale hyperparameter search\nfor key parameters of the VQC. Our aim is to evaluate\nMAQRL’s capabilities and compare it to traditional\nRL methods, using the Coin Game as a benchmark.\nIn this study, we model each agent using Vari-\national Quantum Circuits (VQC), a promising and\nadaptable representation in the quantum domain. The\ninherent flexibility of VQCs allows for the encod-\ning of complex information, making them suitable\nfor representing agent behaviors in diverse environ-\nments. To fine-tune these quantum circuits and ensure\ntheir optimal performance, we harness the power of\narXiv:2311.05546v4  [quant-ph]  2 Jan 2025\nan evolutionary algorithm. This algorithm iteratively\noptimizes the parameters of the VQC, guiding the cir-\ncuit towards improved decision-making and interac-\ntions. While evolutionary algorithms have been tra-\nditionally employed in classical domains, their appli-\ncation in the quantum realm offers exciting prospects\nfor efficiently navigating the vast parameter space of\nVQCs. As part of our experiements, we systemati-\ncally evaluate multiple generational evolution strate-\ngies.\nBy comparing their effectiveness, we aim to\nidentify which strategies most beneficially influence\nthe learning trajectories of the VQCs. Furthermore,\nrecognizing the significance of the VQC’s parame-\nters in determining its behavior and effectiveness, we\nundertake a small scale hyperparameter search. This\nsearch is dedicated to fine-tuning key parameters, en-\nsuring the VQC. Central to our research objectives is\nthe evaluation of MAQRL and its potential contribu-\ntions to the field. We are particularly interested in\nbenchmarking MAQRL against established RL tech-\nniques to learn its advantages and areas of improve-\nment. For a robust and fair assessment, we have cho-\nsen the Coin Game, a well-regarded environment in\nmulti-agent research, as our testing ground. In sum-\nmary our contributions are:\n1. Introducting evolutionary optimization in a quan-\ntum multi-agent reinforcement learning setting.\n2. Assessing the impact of three different genera-\ntional evolution strategies and variational layer\ncounts.\n3. Direct comparison to classical approaches with\ndifferent parameter counts.\nWe start in Section 2 by explaining the basics of\nMARL and Evolutionary Optimization.\nWe also\ngive a short introduction to Quantum Computing and\nVQCs, and mention related studies (Section 3). Af-\nter outlining our methodology (Section 4) and experi-\nmental setup (Section 5), we share the results and im-\nplications of our experiments in Section 6. We end\nwith a summary and thoughts on next steps for re-\nsearch (Section 7). All code and experiments can be\nfound here1.\n2\nPRELIMINARIES\n2.1\nMulti-Agent Setting\nWe focus on Markov games M = ⟨D,S,A,P,R ⟩,\nwhere D = {1,...,N} is a set of agents i, S is a set of\nstates st at time step t, A = ⟨A1,...,AN⟩is the set of\n1https://github.com/michaelkoelle/qmarl-evo\njoint actions at = ⟨at,i⟩i∈D, P(st+1|st,at) is the tran-\nsition probability, and ⟨rt,1,...,rt,N⟩= R (st,at) ∈R\nis the joint reward.\nπi(at,i|st) is the action selec-\ntion probability represented by the individual policy\nof agent i.\nPolicy πi is usually evaluated with a value func-\ntion V π\ni (st) = Eπ[Gt,i|st] for all st ∈S, where Gt,i =\n∑∞\nk=0 γkrt+k,i is the individual and discounted return\nof agent i ∈D with discount factor γ ∈[0,1) and\nπ = ⟨π1,...,πN⟩is the joint policy of the MAS. The\ngoal of agent i is to find a best response π∗\ni with\nV ∗\ni = maxπiV ⟨πi,π−i⟩\ni\nfor all st ∈S, where π−i is the\njoint policy without agent i.\nWe define the efficiency of a MAS or utilitarian\nmetric (U) by the sum of all individual rewards until\ntime step T:\nU = ∑\ni∈D\nRi\n(1)\nwhere Ri = ∑T−1\nt=0 rt,i is the undiscounted return or sum\nof rewards of agent i starting from start state s0.\n2.2\nMulti-Agent Reinforcement\nLearning\nWe focus on independent learning, where each agent\ni optimizes its individual policy πi based on indi-\nvidual information like at,i and rt,i using RL tech-\nniques, e.g., evolutionary optimization as explained\nin Section Evolutionary Optimization. Independent\nlearning introduces non-stationarity due to simulta-\nneously adapting agents which continuously changes\nthe environment dynamics from an agent’s perspec-\ntive ((Littman, 1994; Laurent et al., 2011; Hernandez-\nLeal et al., 2017)), which can cause the adoption of\noverly greedy and exploitative policies which defect\nfrom any cooperative behavior ((Leibo et al., 2017a;\nFoerster et al., 2018)).\n2.3\nEvolutionary Optimization\nInspired by the process of natural selection, evolution-\nary optimization have been shown to find optimal so-\nlutions to complex problems, where traditional meth-\nods may not be efficient (Vikhar, 2016). They em-\nploy a population of individuals, randomly generated,\neach with its own set of parameters. These individu-\nals are evaluated based on a fitness function that mea-\nsures how well their parameters perform on the given\nproblem. The fittest individuals are then selected for\nreproduction, where their parameters are recombined\nand mutated to form a new population of individuals\nfor the next generation. (Eiben and Smith, 2015)\nEvolutionary optimization approaches like genetic\nalgorithms (Holland and Miller, 1991) have been used\nsuccessfully in a variety of fields, including the opti-\nmization of neural networks, or in interactive recom-\nmendation tasks (Ding et al., 2011; Gabor and Alt-\nmann, 2019). Furthermore these methods have been\nused to solve a wide range of problems, from design-\ning quantum circuit architectures to optimizing com-\nplex real-world designs (Lukac and Perkowski, 2002;\nCaldas and Norford, 2002).\n2.4\nQuantum Computing\nQuantum computing is a emerging field of computer\nscience that uses the principles of quantum mechan-\nics to process information. Similar to classical com-\nputers, which store and process data as bits, quantum\ncomputers use quantum bits, or qubits, which can re-\nside in multiple states at once (Yanofsky and Man-\nnucci, 2008). This property is called superposition.\nA state |ψ⟩of a qubit can generally be expressed as a\nlinear combination of |0⟩and |1⟩\n|ψ⟩= α|0⟩+β|1⟩,\n(2)\nwhere α and β are complex coefficients that satisfy\nthe equation\n|α|2 +|β|2 = 1.\n(3)\nWhen a qubit in the state of α|0⟩+ β|1⟩is mea-\nsured, its superposition collapses into one of its pos-\nsible states, either |0〉or |1〉, with probabilities deter-\nmined by the coefficients |α|2 and |β|2 respectively\n(McMahon, 2007). The quantum system transitions\nfrom a superposition of states to an actual classical\nstate where the observable’s value is precisely known\n(Nielsen and Chuang, 2010).\nMultiple qubits can\nbe bound together via entanglement to archive strong\ncorrelations between them.\n2.5\nVariational Quantum Circuits\nVQC, also known as parameterized quantum circuits,\nare quantum algorithms that act as function approxi-\nmators and are trained using a classical optimization\nprocess. They are commonly used as a drop-in re-\nplacement for Neural Networks for Deep RL (Chen\net al., 2022; Schuld et al., 2020; Chen and Goan,\n2019; Skolik et al., 2021; Chen, 2022). A VQC is\nmade up of three stages, as can be seen in Fig. 1. First,\nthe classical input is embedded into a quantum state in\nthe State Preperation stage U(x) using superposition.\nIn the Variational Layers stage V(θ), qubits are then\nentangled and parameterized for training. Finally, in\nthe Measurement stage, the output of the circuit is\nmeasured repeatedly to get the expectation value of\neach qubit.\n|0⟩\nU(x)\nV(θ)\n|0⟩\n|0⟩\n|0⟩\nFigure 1: Structure of a Variational Quantum Circuit\nState Preperation: In this work, we use Amplitude\nEmbedding (Mottonen et al., 2004) to encode classi-\ncal data into a quantum state. As the name suggests,\nthe features are embedded into the amplitudes of\nthe qubits. Using superposition, we can embed 2n\nfeatures into n qubits. For Example, if we want to\nembed feature vector x ∈R3 in to a 2 qubit quantum\nstate |ψ⟩= α|00⟩+ β|01⟩+ γ|10⟩+ δ|11⟩such that\n|α|2 + |β|2 + |γ|2 + |δ|2 = 1, we first pad our feature\nvector so that it matches 2n features where n is the\nnumber of qubits used.\nNext, we normalize the\npadded feature vector y such that ∑2n−1\nk=0\nyk\n||y|| = 1.\nLastly, we use the state preperation by Mottonen et\nal. (Mottonen et al., 2004) to embed the padded and\nnormalized feature vector into the amplitudes of the\nqubit state.\nVariational Layers The second part of the cir-\ncuit, referred to as Variational Layers, is made up of\nrepeated single qubit rotations and entanglers (Fig. 2,\neverything within the dashed blue area is repeated L\ntimes, where L is the layer count). We use a layer\narchitecture inspired by the circuit-centric classifier\ndesign (Schuld et al., 2020) in particular. All of the\ncircuits presented in this paper employ three single\nqubit rotation gates and CNOT gates as entanglers.\nRZ(θ0\n0)\nRY(θ1\n0)\nRZ(θ2\n0)\nRZ(θ0\n1)\nRY(θ1\n1)\nRZ(θ2\n1)\nRZ(θ0\n2)\nRY(θ1\n2)\nRZ(θ2\n2)\nθi j denotes a trainable parameter in the circuit\nabove,\nwhere i represents the qubit index and\nj ∈{0,1,2} the index of the single qubit rotation\ngate. For simplicity, we omitted the index l, which\ndenotes the current layer in the circuit.\nThe tar-\nget bit of the CNOT gate in each layer is given\nby(i+l) mod n.\nMeasurement\nThe\nexpectation\nvalue\nis\nmea-\nsured in the computational basis (z) of the first k\nqubits, where k is the dimension of the agents’\nactions space. Each measured expectation value is\nthen given a bias. The biases are also included in the\nVQC parameters and are updated accordingly.\n3\nRELATED WORK\nQRL is progressively gaining traction, emanating\nfrom the intersection of RL and the emerging field\nof QC (Chen et al., 2022; Kwak et al., 2021). This\nchapter delves into diverse applications and theoret-\nical concepts within QRL that yield advancements\nin parameter reduction, expedited computation times,\nand addressing intricate problems.\nInitially, we focus on a method by Chen et al.,\nwherein parameters are refined using an evolutionary\napproach (Chen et al., 2022), forming the foundation\nupon which the current work is built. Evolutionary\nalgorithms have established their efficacy within tra-\nditional RL (Such et al., 2017) and have demonstrated\nsubstantial value for Deep Reinforcement Learning\n(DRL). The methodology employed within this re-\nsearch integrates DRL strategies, substituting Neu-\nral Networks with Variational Quantum Circuits as\nagents. Chen et al. demonstrated, in a discrete en-\nvironment, that VQCs can efficiently approximate Q-\nvalue functions in Deep Q-Learning (DQL), present-\ning a quantum perspective to RL, while notably reduc-\ning parameter requirements in comparison to classical\nRL. Differing from Chen, the approach presented in\nthis work incorporates recombination and extends the\ngradient free method to the domain of MARL.\nAn alternative route to Quantum Multi-Agent Re-\ninforcement Learning is detailed in (Neumann et al.,\n2020; M¨uller. et al., 2022) which harnesses Quan-\ntum Boltzmann Machines (QBM). The strategy orig-\ninates from an extant methodology where QBM out-\nperforms classical DRL in convergence speed, mea-\nsured in the number of requisite time steps, utiliz-\ning Q-value approximation.\nThe outcomes hint at\nenhanced stability in learning, and the agents attain-\ning optimal strategies in Grid domains, surmounting\nthe complexity of the original approach. This method\nmay serve as a foundational approach in Grid domains\nand in other RL domains with superior complexity.\nResemblances to the method employed herein lie in\nthe utilization of Q-values and grid domains as test-\ning environments.\nMoreover, (Yun et al., 2022) explores the appli-\nVARIATIONAL LAYER\n|0⟩\nU(x)\nR(α1,β1,γ1)\nR(α2,β2,γ2)\nR(α3,β3,γ3)\nR(α4,β4,γ4)\nR(α5,β5,γ5)\nR(α6,β6,γ6)\nFigure 2: Variational Quantum Circuit\ncation of VQCs for QRL and the progression of this\nconcept to QMARL, bearing similarity to the method-\nology delineated in this thesis. The limited number\nof parameters in QRL has demonstrated superior out-\ncomes compared to classical computing. In order to\nnavigate the challenges of extending this to QMARL\nin the Noisy Intermediate-Scale Quantum era and the\nnon-stationary attributes of classical MARL, a strat-\negy of centralized learning and decentralized execu-\ntion is enacted. This approach achieves an overall su-\nperior reward in the environments tested, compared\nto classical methodologies, with disparities arising in\nthe employment of evolutionary algorithms and the\narchitecture of the VQCs.\n4\nAPPROACH\nInspired by to (Chen et al., 2022), we propose to em-\nploy an evolutionary approach to optimize a θ param-\neterized agent. We however consider the more gen-\neral Multi-Agent setup introduced above. Thus, we\naim to optimize the utilitarian metric U (cf. Eq. (1)).\nTo maximize this fitness function we use a population\nP consisting of η random initialized agents, parame-\nterized by θ ∈[−π,π] to match the quantum circuits’\nparameter space.\nIn contrast to previous work, we use VQC rather\nthan neural networks to approximate the value of the\nagent’s actions. This should mainly demonstrate the\nimproved parameter efficiency, as previously denoted,\neven applied to complex learning tasks. A VQC con-\nsists primarily of three components: the input embed-\nding, the repeated variational layers, and the measure-\nment. Fig. 1 depicts the VQC we employ.\nTo convert the classical data into a quantum state,\nwe use Amplitude Embeddings, represented by U(x)\nin Fig. 2. Caused by the high dimensionality of most\nstate spaces, Amplitude Embeddings are currently the\nonly viable embedding strategy that allow for embed-\nding the whole state information, being able to embed\n2nq states in nq qubits.\nThe second part of the VQCs consists of varia-\ntional layers that are variably repeated. Each iteration\nincreases the number of αi,βi,γi parameters that are\ndefined by θ and make up each individual to be op-\ntimized. Per layer, there are nθ = nq ∗3 parameters,\nwhere nq is the number of qubits. Furthermore, all ro-\ntations are performed sequentially as RZ(αi),RY(βi)\nand RZ(γi).\nIn addition to the parameterized rota-\ntions, each variational layer is composed of adjacent\nCNOTS to entangle all qubits. After nl repetitions of\nthe variational layer, the predicted values of the indi-\nvidual actions are determined by measuring the first\nna qubits, where na is the number of actions. This Z-\naxis measurement is used to determine the Q-value of\nthe corresponding action. An agent chooses the action\nwith the greatest expected value.\nThe proposed evolutionary algorithm training pro-\ncedure to optimize these individuals to maximize the\nutilitarian metric U is demonstrated in Algorithm 1.\nData: Population Size η, Number of\nGenerations µ, Evaluation Steps κ,\nTruncation Selection τ, Mutation\nPower σ, and Number of Agents N\nResult: Population P of optimized agents\nP0 ←Initialize population η with random θ\nfor g ∈{0,1,...,µ} do\nfor i ∈{0,1,...,η} do\nReset testing environment\nScore St,i ←0\nfor t ∈{0,1,...,κ} do\nUse policy of agent i for all agents\nin env\nSelect action\nat ←argmax VQCθ(st)\nExecute environment step with\naction at\nObserve reward rt and next state\nst+1\nSt,i ←St,i +rt\nend\nend\nλ ←Select top τ agents based on St,i\nKeep top agent based on St,i\nRecombine η−1 new agents out of λ\nMutate η−1 generated agents\nPg+1 ←η−1 generated agents + top\nagent\nend\nAlgorithm 1: Evolutionary optimization algorithm\nFor each generation, first, the fitness of each indi-\nvidual i is evaluated by performing κ steps in the envi-\nronment. Building upon this fitness, the best τ agents\nare selected to develop a new generation. In addition,\nwe employ the so-called elite agent, the agent with\nthe highest fitness, that is excluded from the follow-\ning mutation procedure.\nTo form the next generation, mutation and recom-\nbination possibilities are combined to generate a new\npopulation. First, new individuals are formed by re-\ncombining the κ best agents of the current generation\nusing crossover. The new offspring is produced by\nrandomly selecting two parents and crossing their pa-\nrameters at a randomly selected index. Furthermore,\nmutation is applied to generate new agents by modi-\nfying the parameters θ of the current generation of the\nbest τ agents:\nθ = θ+σ∗ε\n(4)\nThe agents with the highest fitness values T are the\nparents of the upcoming generation. For the mutation,\nthe parameters θ are modified as seen in the equa-\ntion with the mutation power σ the Gaussian noise\nε ∼N (′,∞). Consequently, all θi parameters undergo\na minor mutation, and new agents, or children, are\ngenerated. Finally, the unaltered elite agent is added\nto the child population.\n5\nEXPERIMENTAL SETUP\n5.1\nCoin Game Environment\nAs of today, we are in the Noisy Intermediate-Scale\nQuantum era of quantum computing, where we can\nsimulate only a small number of qubits (Preskill,\n2018). This heavily restricts the amount of data we\ncan embed into the quantum circuit. Therefore, we\nare limited in our choices for the evaluation environ-\nment. We chose the Coin Game environment in its\n3×3 gridworld version due to its relatively small ob-\nservation space. The Coin Game, which was created\nby (Lerer and Peysakhovich, 2017), is a well-known\nsequential game for assessing RL strategies. Both the\nRed and Blue agents in the environment are tasked\nwith collecting coins. Beside the agents there is a sin-\ngle coin placed in the grid that corresponds with one\nof the agents colors. Fig. 3 depicts a exemplary state\nwithin the Coin Game.\nWhen an agent is in the same position as a coin, it\nis deemed collected. After a coin is gathered, a new\ncoin is generated at a random location, which cannot\nbe a place occupied by an agent, and is again either\nred or blue. A game of the Coin Game is limited to\n50 steps 25 per agent, and the objective is to maximize\nthe agents’ rewards. The Coin Game can be played in\nFigure 3: Example State of the Coin Game by (Phan et al.,\n2022).\nboth a competitive and cooperative setting. To make\nthe game cooperative, the reward for collecting a coin\nis increased by +1 for the agent who collects the coin.\nMoreover, the second agent’s reward is reduced by -2\nif the first agent obtains a coin of his color. If we now\nconsider the agents’ total reward, there is a common\nreward of +1 for collecting an own coin and -1 for\ncollecting an opposing coin. This causes the agents to\nbe trained to gather their own coins and leave coins,\nwhich are not the agent’s color, for the other agent to\ncollect. If both agents performed random behaviors,\nthe expected reward should be zero. As a result of\nthese rewards, the coin game is a zero-sum game.\nEach cell of the 3×3 gridworld can contain either\nagent 1, agent 2, a red or blue coin. Empty grids need\nnot be included in the observation, as movement can\noccur on them without consequence at any time. An\nagent may select from four possible actions, each of\nwhich is only possible if the ensuing movement does\nnot lead outside the 3 × 3 gridworld. The numerical\nactions range from 0 to 3. Action 0 indicates a step to\nthe north, action 1 a step to the south, action 2 a step\nto the west, and action 3 a step to the east. To prevent\nthe VQC from choosing illegal actions, the expected\nvalues are normalized to the interval [0,1] and masked\nwith environmental regulations.\n5.2\nBaselines\nVQC can be viewed as an alternative to the employ-\nment of classical neural networks as agents. In the\nframework of our methodology, we employ neural\nnetworks as agents, as they can be considered gen-\neral approximators. We employ a 2-layer basic neu-\nral network for this purpose. The inputs in the form\nof observations are mapped to a variable number x of\nhidden units in the first layer. The second layer relates\nthe number of our activities to x hidden units. Hence,\nwe obtain the individual Q-values for each action, just\nas we did with the VQC. Moreover, the Q-values are\nmultiplied by the action mask to ensure that no ille-\ngal action can be selected. Here, we limit ourselves to\nthis neural network with variable numbers of hidden\nunits. Similar to the VQC, there are a great number of\nways to alter the network and hence alter the results.\n5.3\nMetrics\nWe evaluate our experiments in the Coin Game envi-\nronment using three metrics: Score, Total Coin Rate\nand Own Coin Rate.\nIn our work, the agents are\nsolely playing against themselves, to easily evalu-\nate the agents’ performance. The first metric, Score\nSn consists of the undiscounted individual rewards\nrt,i until timestep T ∈{0..49} accumulated over all\nagents\nSn = ∑\ni∈{0,1}\nT−1\n∑\nt=0\nrt,i\n(5)\nwith agent i and generation n ∈{0..99} averaged over\nfive seeds.\nThis is a good overall indicator of the\nagents performance in the Coin Game environment.\nThe next two metrics should provide insight into how\nthe score is reached. The total coins collected metric\nTCn is the sum of all collected coins ct,i by all agents\nuntil timestep T ∈{0..49}\nTCn = ∑\ni∈{0,1}\nT−1\n∑\nt=0\nct,i\n(6)\nwith agent i and generation n ∈{0..99} averaged over\nfive seeds. The own coins collected metric OCn is\nthe sum of all collected coins that corresponds to the\nagents own color ot,i until timestep T ∈{0..49} accu-\nmulated over all agents\nOCn = ∑\ni∈{0,1}\nT−1\n∑\nt=0\not,i\n(7)\nwith agent i and generation n ∈{0..99} averaged over\nfive seeds.\nComparing the latter two metrics, we\ncan get a greater insight how much cooperation is\narchived, with the own coin rate OCR:\nOCRn = ∑\ni∈{0,1}\nT−1\n∑\nt=0\not,i\nct,i\n(8)\n5.4\nTraining and Hyperparameters\nFor our experiments in the Coin Game environment,\nwe train the agents for µ = 100 generations with a\npopulation size of η = 250, pairing the agents against\nthemselves to play a game of 50 steps 25 per agent.\nAfter a brief preliminary study we set the mutation\npower to σ = 0.01. We select the top τ = 5 agents for\nregenerating the following population. The VQC has\na Variational Layer count of 4 and nq = 6 qubits to\nembed the 36 features of the coin game, resulting in a\nparameter count of 76. Each experiment is conducted\nwith five different seeds ∈0..4 to provide a more\naccurate indication of performance. Due to current\nquantum hardware limitations we use the Pennylane\nDefaultQubit simulator for all VQC executions. All\nruns were executed on nodes with Intel(R) Core(TM)\ni5-4570 CPU @ 3.20GHz.\n6\nRESULTS\nIn this section, we present the results of our exper-\niments in the Coin Game environment.\nWe tested\nour approach with recombination and mutation com-\nbined, as well as mutation only.\nFurthermore, we\ntested two classical neural networks with two hid-\nden layers, one with a hidden layer size of 64 × 64\nand one with 3 × 4 respectively. The latter configu-\nration closely matches the amount of parameters that\nthe VQCs approaches use, to get better insights on the\nmodel-size/performance ratio. Finally, we ran tests\nwith agents that take random actions at every step\nwhich forms our random baseline. The number of pa-\nrameters is listed alongside each approach.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n1\n2\n3\n4\n5\n6\n7\n8\nAverage score\nVQC(148): Mu\nVQC(148): LaReMu\nVQC(148): RaReMu\nFigure 4: Average Score over the entire population. Each\nindividual has completed 50 steps in the Coin Game envi-\nronment each generation.\n6.1\nComparing Generational Evolution\nStrategies\nWe aim to understand the impact of different gen-\nerational evolution strategies.\nIn this section, we\ncontrast the performance of a mutation-only strat-\negy (Mu) against two combined strategies of mu-\ntation and recombination.\nThe first combined ap-\nproach involves a crossover recombination strategy\nat a randomly chosen point in the parameter vec-\ntor (RaReMu), while the second employs a layer-\nwise crossover (LaReMu). Here, we choose a random\nlayer and apply the crossover after the last parameter\nof the selected layer in the parameter vector. For all\nstrategies, the mutation power σ is fixed at 0.01.\nExamining the average scores depicted in Fig. 4,\nthe mutation-only strategy emerges as the best\nstrategy.\nThe crossover strategies showcase simi-\nlar performance, though the layerwise method fre-\nquently achieves marginally superior outcomes. The\nmutation-only strategy starts with an average reward\nof 5, dips slightly below 4 by the 17th generation, and\nthen steadily rises until the 140th generation. From\nthis point, it fluctuates around a score of 7. In contrast,\nthe layerwise recombination begins at a lower 3.3, ex-\nperiences a rapid ascent until the 30th generation, then\nstabilizes, eventually reaching an average reward of\n6 by the 123rd generation. This is followed by pro-\nnounced fluctuations around this value. The random\ncrossover strategy starts close to the mutation-only at\n4.7, but quickly descends to 3 by the 17th genera-\ntion. It then steadily climbs until the 131st generation,\nachieving a score of 6. However, this score is not sus-\ntained and eventually settles around 5.5, making it the\nleast effective of the three methods.\nBeyond score comparison, we evaluated the av-\nerage number of coins collected during the experi-\nments. As inferred from the scores, the mutation-only\nstrategy consistently collects more coins, as shown in\nFig. 5a. Although there are periods where the strate-\ngies yield almost identical coin counts, at other times,\na gap of up to 2 coins is evident. On average, the com-\nbined strategies lag slightly behind the mutation-only\nin terms of coin collection.\nThe layerwise crossover’s initial surge in Fig. 4\ncorrelates with the uptrend in collected coins shown\nin Fig. 5a and the coin rate detailed in Fig. 5c.\nWhen comparing the strategies based on these met-\nrics, mutation-only consistently achieves the highest\ncoin rate over all generations and collects the most\ncoins, accounting for its superior average reward.\nThe random crossover strategy, while collecting more\ncoins than the layerwise approach, has a significantly\nreduced coin rate, resulting in diminished overall re-\nwards.\nExploring the coin rate, depicted in Fig. 5c, the\nlayerwise strategy leads until the 90th generation. Af-\nter that, its rate declines, while the mutation-only\nstrategy exhibits a gradual, consistent rise. A higher\ncoin rate indicates enhanced agent cooperation within\nthe testing environment.\nThis coin rate, combined\nwith the number of coins collected, determines an\nagent’s reward.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n2\n3\n4\n5\n6\n7\n8\n9\n10\nAverage collected coins\nVQC(148): Mu\nVQC(148): LaReMu\nVQC(148): RaReMu\n(a) Total coins collected\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n2\n3\n4\n5\n6\n7\n8\n9\nAverage collected own coins\nVQC(148): Mu\nVQC(148): LaReMu\nVQC(148): RaReMu\n(b) Own coins collected\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nOwn coin rate\nVQC(148): Mu\nVQC(148): LaReMu\nVQC(148): RaReMu\n(c) Own coin rate\nFigure 5: Comparison of (a) average coins collected, (b) average own coins collected and the own coin rate (c) in a 50 step\nCoin Game each generation, averaged over 10 seeds.\nIn summary, the mutation-only strategy outper-\nforms the combined strategies in our experiments. It\nnot only garners the highest reward but also aligns\nbest with our objective: maximizing reward. Hence,\nsubsequent experiments will exclusively employ the\nmutation-only approach for the VQCs.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n2\n3\n4\n5\n6\n7\n8\nAverage score\nVQC(76)\nVQC(112)\nVQC(148)\nVQC(292)\nFigure 6: Average Score over the entire population. Each\nindividual has completed 50 steps in the Coin Game envi-\nronment each generation.\n6.2\nAssessing Varying Layer Counts\nWe investigate the performance dynamics of VQCs\nwith different layer counts, specifically with 4, 6,\n8, and 16 layers.\nThe relationship between layer\ncounts and parameters is governed by the formula\n3∗n∗6+4, where n stands for the number of layers.\nAccordingly, VQCs with 4, 6, 8, and 16 layers utilize\n76, 112, 148, and 292 parameters respectively. We\ntrained all VQCs using the mutation-only approach,\nsetting the mutation strength to σ = 0.01.\nInspecting the average rewards in Fig. 6, all\nVQCs, bar the 4-layered one which starts slightly be-\nlow 3, commence with scores ranging from 5 to 5.5.\nBy the 25th generation, each VQC stabilizes around\na reward of 4. The 4-layer VQC then gradually as-\ncends, consistently holding an average reward of 5\nfrom the 175th generation.\nThe 6-layer VQC ex-\nhibits a steady rise until the 62nd generation, with a\nmore pronounced increase after that, peaking at 6.7\naround the 165th generation and subsequently oscil-\nlating around 6.5. The 8-layer VQC consistently out-\nperforms the others, reaching a reward of 5.5 by the\n70th generation, encountering a brief plateau, and\nthen climbing to 7 by the 140th generation.\nThe\n16-layer VQC, meanwhile, showcases a pronounced\ngrowth phase between the 25th and 70th generations,\nstabilizing around 6 before another rise to 6.5 around\nthe 160th generation.\nFor a comprehensive understanding, we next\nprobe the average coin collection in Fig. 7a. The 4-\nlayer VQC consistently tops the coin collection met-\nric, progressing from just below 7 to 8. The 6-layer\nVQC commences at 6.5, dips to 5.2 by the 23rd gener-\nation, and then rises to 8 by the 165th generation. The\n8-layer VQC, despite securing the highest average re-\nward, begins at 6 and only stabilizes around 8 after\nthe 180th generation. The 16-layer VQC, after an ini-\ntial dip, witnesses a rapid increase from the 24th to\n103rd generation, briefly declines, and then fluctuates\naround 8 coins. Towards the concluding generations,\nVQCs with more than 4 layers converge to collect ap-\nproximately 8 coins.\nAnalyzing the own coin count in Fig. 7b, we ob-\nserve that, except for the 4-layer VQC, all VQCs ini-\ntially decline before ascending.\nThe 4-layer VQC\ndisplays a steady yet modest climb, concluding at a\ncount of 6.5. The 6-layer VQC takes the longest to\ncommence its ascent, eventually oscillating around a\ncount of 7.2. The 8-layer VQC initiates its climb ear-\nlier, achieving a slightly higher count of 7.5 by the\nend. The 16-layer VQC, notable for its r¨Iapid early\nascent, consistently hovers around a count of 7 after\nthe 100th generation. Among the VQCs, the 4-layer\nvariant lags, collecting over one own coin fewer than\nits counterparts.\nFocusing on the own coin rate, the 4-layer VQC\nperforms the worst. The performance parallels be-\ntween the 6-layer and 16-layer VQCs are evident,\nboth in terms of own coin rate and overall reward.\nThe standout remains the 8-layer VQC, which, with\nits superior own coin rate and comparable coin count,\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n4\n5\n6\n7\n8\n9\n10\nAverage collected coins\nVQC(76)\nVQC(112)\nVQC(148)\nVQC(292)\n(a) Total coins collected\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n3\n4\n5\n6\n7\n8\n9\nAverage collected own coins\nVQC(76)\nVQC(112)\nVQC(148)\nVQC(292)\n(b) Own coins collected\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nOwn coin rate\nVQC(76)\nVQC(112)\nVQC(148)\nVQC(292)\n(c) Own coin rate\nFigure 7: Comparison of (a) average coins collected, (b) average own coins collected and the own coin rate (c) in a 50 step\nCoin Game each generation, averaged over 10 seeds.\nhas the highest reward.\nIn conclusion, our tests spotlight the 8-layer VQC\nas the top performer. Consequently, we select it com-\nbined with the the optimal evolutionary strategy out-\nlined in 6.1, for all further experiments. This section\nunderscores that a higher layer count doesn’t guar-\nantee superior performance – the 16-layer VQC falls\nshort of the 8-layer VQC’s achievements. The exper-\niments, however, don’t conclusively establish the per-\nformance dynamics beyond 200 generations.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n−2\n0\n2\n4\n6\n8\nAverage score\nRandom\nVQC(148): Mu\nNN(147): Mu\nNN(6788): Mu\nFigure 8: Average Score over the entire population. Each\nindividual has completed 50 steps in the Coin Game envi-\nronment each generation.\n6.3\nComparing Quantum and Classical\nApproaches\n6.3.1\nComparing VQC and Random\nFirst, we compare the results of our VQC approaches\nto the results of the random baseline. In Fig. 8, the\nscore for the random acting agents is approximately 0,\nsince the cooperative sequential coin game is a zero-\nsum game. The evolutionary-trained VQC approache,\nhowever, perform significantly better leading to an av-\nerage score around 7. The total coins collected de-\npicted in Fig. 9a suggests that in contrast to the ran-\ndom agents, the VQC agents successfully learn to col-\nlect coins. The own coins collected correlates with\nnumber of collected coins (Fig. 9b). In Fig. 9c we can\nsee that in neither case the cooperation increases over\ntime. In summary, trained agents performed signif-\nicantly better than random on all metrics, indicating\nthat the training was successful.\n6.3.2\nComparing VQC and Small NN\nAs depicted in Fig. 8, a better result is achieved by the\nVQC approach compared to random. On this basis,\nwe compare the performance of this VQC approach\nto that of a neural network with a comparable number\nof parameters. Here we exploit the higher expressive\npower of VQCs compared to conventional neural net-\nworks (Chen et al., 2022). Similar to (Chen et al.,\n2022), we define the expressive power as the capac-\nity to represent particular functions with a constrained\nnumber of parameters. Note that the VQC has 148 pa-\nrameters (3 * 6 * 8 + 4). The neural network uses two\nhidden layers with dimension 3 and 4 respectively, re-\nsulting in a parameter count of 147. Both the neural\nnetwork and the VQC, are trained with mutation only\nwith mutation power σ = 0.01. In Fig. 8, we can see\nthat the neural network reward fluctuates in the range\nof 2.5 to 3. As previously discussed in the last sec-\ntion, the VQC approach exhibits a slow learning curve\nleading to a significant higher score therefore conse-\nquently outperforming this neural network. The infe-\nrior performance can be explained by the small num-\nber of hidden units and parameters present in neural\nnetworks. Typically, the number of hidden units is\nchosen much higher. Further evidence of the neural\nnetwork’s deficiency is provided by the average num-\nber of coins collected. As shown in Fig. 9a, the NN’s\nnumber of collected coins is below the average score\nof the random agents until generation 115 and after\nthat slightly over it. In comparison, the VQC with\nthe same number of parameters collects two times as\nmany coins on average. The neural network is able\nto outperform random agents on the basis of its col-\nlected own coins, what can be seen in Fig. 9b, lead-\ning to the better performance regarding the own coin\nrate (Fig. 9c). In terms of collected own coins and\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n2\n3\n4\n5\n6\n7\n8\n9\n10\nAverage collected coins\nRandom\nVQC(148): Mu\nNN(147): Mu\nNN(6788): Mu\n(a) Total coins collected\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n2\n4\n6\n8\nAverage collected own coins\nRandom\nVQC(148): Mu\nNN(147): Mu\nNN(6788): Mu\n(b) Own coins collected\n0\n25\n50\n75\n100\n125\n150\n175\n200\nGeneration\n0.2\n0.4\n0.6\n0.8\n1.0\nOwn coin rate\nRandom\nVQC(148): Mu\nNN(147): Mu\nNN(6788): Mu\n(c) Own coin rate\nFigure 9: Comparison of (a) average coins collected, (b) average own coins collected and the own coin rate (c) in a 50 step\nCoin Game each generation, averaged over 10 seeds.\nthe own coin rate, the neural network performs sig-\nnificantly worse than the VQC with nearly the same\nnumber of parameters. A neural network with this\nfew hidden units and, consequently, parameters is not\nable learn in the coin game environment successfully.\nThis demonstrates the power of VQCs for RL archiv-\ning significantly higher with the same amount of pa-\nrameters.\n6.3.3\nComparing VQC and Big NN\nIn the previous section, we observed that a neural\nnetwork with the same number of parameters as the\nVQC of our approach cannot match the VQC’s per-\nformance. We will now compare the results with a\nneural network that has significantly more parame-\nters. Again, mutation only is used for the evolution\nof subsequent generations in both cases and the mu-\ntation power is σ = 0.01. We chose a fully connected\nNN with two hidden layers of size 64, resulting in\na parameter count of 6788. If we first examine the\nreward of the neural network and the VQC, we can\nsee in Fig. 8 that both produce very similar results\nover time. Initially, the VQC has a slightly higher\nscore.\nFrom generation 50 onward, there are only\nminor differences in terms of average score. Over-\nall, the two strategies yield a score of approximatly\n7. On the basis of our experiments, we can say that\nthe VQC achieves nearly identical performance in the\nCoin Game environment compared to a neural net-\nwork that has 46 times more parameters. In Fig. 9a,\nthe initial value of the VQC method is again higher\nthan that of the neural network.\nDue to a steeper\nlearning curve, the neural network is able to com-\npensate the lower starting value and achieves only\na slightly smaller number of collected coins. From\nthere are no discernible differences between the two\nmodels. A similar performance of the two approaches\ncan be seen in Fig. 9b, where the average number\nof own coins collected is shown. In Terms of own\ncoin rate, at first, the VQC archives a slightly higher\nscore. However, at Generation 25, the neural network\ninitially achieves a better own coin rate before being\nslightly lower between Generation 80 and 162. In the\nend, the neural network is slightly better in terms of\nthe own coin rate.\nIn summary, there is little difference between the out-\ncomes of the two approaches, despite the neural net-\nwork having 46 times the number of parameters com-\npared to the VQC. Thus, we can reduce the number\nof parameters in our experiments by 97.88% without\nsacrificing performance using VQCs. Similar to re-\nsults in (Chen et al., 2022), the VQC exhibits a great\nexpressive power in and we recommend it for future\nuse in QRL.\n7\nCONCLUSION\nGradient-based training methods are, as of time of\nwriting, not suitable for MAQRL due to problems\nwith barren plateaus and vanishing gradients (Franz\net al., 2022; Chen et al., 2022).In this work, we pre-\nsented an alternative approach to gradient based train-\ning methods for MAQRL. For our gradient-free ap-\nproach used in the experiments, barren plateaus did\nnot arise; however, it cannot be ruled out that such is-\nsues may occur in gradient-free methods under differ-\nent conditions or in more complex scenarios (Larocca\net al., 2024). We build our approach upon the evo-\nlutionary optimization process used by (Chen et al.,\n2022), expanding it to Multi-Agent systems and dif-\nferent generational evolution strategies. We proposed\nthree quantum approaches for MARL. All approaches\nuse VQCs as replacement for neural networks. Two\napproach use recombination in addition to mutation\nand the third approach mutation only. For evaluation,\nwe chose Coin Game as our testing environment due\nto it’s cooperative setting and relatively small obser-\nvation space. As baselines, we used random agents\nand classical agents with neural networks, which were\nalso trained using the evolutionary algorithm.\nTo\nachieve a fair comparison, we chose a neural network\nwith a similar amount of parameters, as well as one\nwith a hidden layer size of 64×64.\nIn our experiments, we showed that our VQC ap-\nproach performs significantly better compared to a\nneural network with a similar amount of trainable pa-\nrameters. Compared to the larger neural network, we\ncan see that the VQC approach achieves similar re-\nsults, showing the effectiveness of using VQCs in a\nMAQRL environment. We can reduce the number of\nparameters by 97.88% using the VQC approach com-\npared to the similarly good neural network. In com-\nparison to previous works (Chen et al., 2022), we used\nrecombination in addition to mutation in our evolu-\ntionary algorithm, which performed worse than muta-\ntion alone in the tested setting. Additionally, we used\nmore layers for the VQCs than previous works (Chen\net al., 2022), as they have yielded better results in the\nexperiments.\nIn the future, the VQC results could be run not\nonly on a quantum simulator, but on real quantum\nhardware to determine which and if there is a differ-\nence. Also, a comparison of the VQC approach with a\ngradient based neural network would be an option for\nfuture work. Another option would be to compare the\nVQC approach in terms of the number of parameters\nwith a data reuploading method and see if this can\nsolve the coin game similarly well with even fewer\nqubits. Additionally, we could work on the hyperpa-\nrameters and see if even better results can be achieved\nby adapting them.\nACKNOWLEDGEMENTS\nThis work is part of the Munich Quantum Valley,\nwhich is supported by the Bavarian state government\nwith funds from the Hightech Agenda Bayern Plus.\nThis publication was created as part of the Q-Grid\nproject (13N16179) under the “quantum technologies\n- from basic research to market” funding program,\nsupported by the German Federal Ministry of Edu-\ncation and Research.\nREFERENCES\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P.,\nVitvitskyi, A., Guo, Z. D., and Blundell, C. (2020).\nAgent57: Outperforming the atari human benchmark.\nCoRR, abs/2003.13350.\nCaldas, L. G. and Norford, L. K. (2002). A design optimiza-\ntion tool based on a genetic algorithm. Automation in\nconstruction, 11(2):173–184.\nChen, S. Y. and Goan, H. (2019).\nVariational quantum\ncircuits and deep reinforcement learning.\nCoRR,\nabs/1907.00397.\nChen, S. Y.-C. (2022). Quantum deep recurrent reinforce-\nment learning.\nChen, S. Y.-C., Huang, C.-M., Hsing, C.-W., Goan, H.-S.,\nand Kao, Y.-J. (2022). Variational quantum reinforce-\nment learning via evolutionary optimization. Machine\nLearning: Science and Technology, 3(1):015025.\nDeng, S., Xiang, Z., Zhao, P., Taheri, J., Gao, H., Yin, J.,\nand Zomaya, A. Y. (2020). Dynamical resource allo-\ncation in edge for trustable internet-of-things systems:\nA reinforcement learning method. IEEE Transactions\non Industrial Informatics, 16(9):6103–6113.\nDimeas, A. L. and Hatziargyriou, N. D. (2010). Multi-agent\nreinforcement learning for microgrids. In IEEE PES\nGeneral Meeting, pages 1–8.\nDing, S., Su, C., and Yu, J. (2011). An optimizing bp neural\nnetwork algorithm based on genetic algorithm. Artifi-\ncial intelligence review, 36:153–162.\nEiben, A. E. and Smith, J. E. (2015). Introduction to evolu-\ntionary computing. Springer.\nFoerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S.,\nAbbeel, P., and Mordatch, I. (2018). Learning with\nOpponent-Learning Awareness. In Proceedings of the\n17th International Conference on Autonomous Agents\nand Multiagent Systems, page 122–130, Richland, SC.\nInternational Foundation for Autonomous Agents and\nMultiagent Systems.\nFranz, M., Wolf, L., Periyasamy, M., Ufrecht, C., Scherer,\nD. D., Plinge, A., Mutschler, C., and Mauerer,\nW. (2022).\nUncovering instabilities in variational-\nquantum deep q-networks.\nJournal of the Franklin\nInstitute.\nGabor, T. and Altmann, P. (2019). Benchmarking surrogate-\nassisted genetic recommender systems. In Proceed-\nings of the Genetic and Evolutionary Computation\nConference Companion, pages 1568–1575.\nHarrow, A. W. and Montanaro, A. (2017). Quantum com-\nputational supremacy. Nature, 549(7671):203–209.\nHernandez-Leal, P., Kaisers, M., Baarslag, T., and de Cote,\nE. M. (2017). A Survey of Learning in Multiagent\nEnvironments: Dealing with Non-Stationarity. arXiv\npreprint arXiv:1707.09183.\nHolland, J. H. and Miller, J. H. (1991). Artificial adaptive\nagents in economic theory. The American economic\nreview, 81(2):365–370.\nKwak, Y., Yun, W. J., Jung, S., Kim, J.-K., and Kim, J.\n(2021). Introduction to quantum reinforcement learn-\ning: Theory and pennylane-based implementation.\nLarocca, M., Thanasilp, S., Wang, S., Sharma, K., Bia-\nmonte, J., Coles, P. J., Cincio, L., McClean, J. R.,\nHolmes, Z., and Cerezo, M. (2024). A review of bar-\nren plateaus in variational quantum computing.\nLaurent, G. J., Matignon, L., Fort-Piat, L., et al. (2011). The\nworld of independent learners is not markovian. Inter-\nnational Journal of Knowledge-based and Intelligent\nEngineering Systems, 15(1):55–64.\nLeibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J.,\nand Graepel, T. (2017a).\nMulti-Agent Reinforce-\nment Learning in Sequential Social Dilemmas.\nIn\nProceedings of the 16th Conference on Autonomous\nAgents and Multiagent Systems, AAMAS ’17, page\n464–473, Richland, SC. International Foundation for\nAutonomous Agents and Multiagent Systems.\nLeibo, J. Z., Zambaldi, V. F., Lanctot, M., Marecki, J.,\nand Graepel, T. (2017b).\nMulti-agent reinforce-\nment learning in sequential social dilemmas. CoRR,\nabs/1702.03037.\nLerer, A. and Peysakhovich, A. (2017).\nMaintain-\ning Cooperation in Complex Social Dilemmas us-\ning Deep Reinforcement Learning.\narXiv preprint\narXiv:1707.01068.\nLittman, M. L. (1994).\nMarkov Games as a Framework\nfor Multi-Agent Reinforcement Learning. In Machine\nLearning Proceedings 1994, pages 157–163. Morgan\nKaufmann, San Francisco (CA).\nLukac, M. and Perkowski, M. (2002).\nEvolving quan-\ntum circuits using genetic algorithm. In Proceedings\n2002 NASA/DoD Conference on Evolvable Hardware,\npages 177–185. IEEE.\nMcMahon, D. (2007). Quantum computing explained. John\nWiley & Sons.\nMottonen, M., Vartiainen, J. J., Bergholm, V., and Salomaa,\nM. M. (2004). Transformation of quantum states using\nuniformly controlled rotations. arXiv preprint quant-\nph/0407010.\nM¨uller., T., Roch., C., Schmid., K., and Altmann., P.\n(2022). Towards multi-agent reinforcement learning\nusing quantum boltzmann machines. In Proceedings\nof the 14th International Conference on Agents and\nArtificial Intelligence - Volume 1: ICAART,, pages\n121–130. INSTICC, SciTePress.\nNeumann, N. M., de Heer, P. B., Chiscop, I., and Phillip-\nson, F. (2020). Multi-agent reinforcement learning us-\ning simulated quantum annealing. In Computational\nScience–ICCS 2020: 20th International Conference,\nAmsterdam, The Netherlands, June 3–5, 2020, Pro-\nceedings, Part VI 20, pages 562–575. Springer.\nNielsen, M. A. and Chuang, I. L. (2010). Quantum Com-\nputation and Quantum Information: 10th Anniversary\nEdition. Cambridge University Press.\nPhan, T., Sommer, F., Altmann, P., Ritz, F., Belzner, L.,\nand Linnhoff-Popien, C. (2022). Emergent coopera-\ntion from mutual acknowledgment exchange. In Pro-\nceedings of the 21st International Conference on Au-\ntonomous Agents and MultiAgent Systems (AAMAS),\npages 1047–1055. International Foundation for Au-\ntonomous Agents and Multiagent Systems.\nPreskill, J. (2018). Quantum computing in the nisq era and\nbeyond. Quantum, 2:79.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hass-\nabis, D., Graepel, T., Lillicrap, T. P., and Silver, D.\n(2019). Mastering atari, go, chess and shogi by plan-\nning with a learned model. CoRR, abs/1911.08265.\nSchuld, M., Bocharov, A., Svore, K. M., and Wiebe, N.\n(2020). Circuit-centric quantum classifiers. Physical\nReview A, 101(3):032308.\nShalev-Shwartz, S., Shammah, S., and Shashua, A. (2016).\nSafe, multi-agent, reinforcement learning for au-\ntonomous driving. CoRR, abs/1610.03295.\nSkolik, A., McClean, J. R., Mohseni, M., van der Smagt, P.,\nand Leib, M. (2021). Layerwise learning for quantum\nneural networks. Quantum Machine Intelligence, 3:1–\n11.\nSuch, F. P., Madhavan, V., Conti, E., Lehman, J., Stanley,\nK. O., and Clune, J. (2017). Deep neuroevolution: Ge-\nnetic algorithms are a competitive alternative for train-\ning deep neural networks for reinforcement learning.\nCoRR, abs/1712.06567.\nVikhar, P. A. (2016). Evolutionary algorithms: A critical\nreview and its future prospects. In 2016 International\nConference on Global Trends in Signal Processing,\nInformation Computing and Communication (ICGT-\nSPICC), pages 261–265.\nYanofsky, N. S. and Mannucci, M. A. (2008). Quantum\ncomputing for computer scientists. Cambridge Uni-\nversity Press.\nYun, W. J., Park, J., and Kim, J. (2022). Quantum multi-\nagent meta reinforcement learning.\n",
  "categories": [
    "quant-ph",
    "cs.AI",
    "cs.MA"
  ],
  "published": "2023-11-09",
  "updated": "2025-01-02"
}