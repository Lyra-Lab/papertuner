{
  "id": "http://arxiv.org/abs/2311.05741v2",
  "title": "Efficiently Adapting Pretrained Language Models To New Languages",
  "authors": [
    "Zoltan Csaki",
    "Pian Pawakapan",
    "Urmish Thakker",
    "Qiantong Xu"
  ],
  "abstract": "Recent large language models (LLM) exhibit sub-optimal performance on\nlow-resource languages, as the training data of these models is usually\ndominated by English and other high-resource languages. Furthermore, it is\nchallenging to train models for low-resource languages, especially from\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\nreduces the need for data in the new language while also providing cross\nlingual transfer capabilities. However, naively adapting to new languages leads\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\nstudy how to efficiently adapt any existing pretrained LLM to a new language\nwithout running into these issues. In particular, we improve the encoding\nefficiency of the tokenizer by adding new tokens from the target language and\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\nbetter performance than open source models on the target language, with minimal\nregressions on English.",
  "text": "arXiv:2311.05741v2  [cs.CL]  14 Dec 2023\nEfﬁciently Adapting Pretrained Language Models to\nNew Languages\nZoltan Csaki, Pian Pawakapan, Urmish Thakker, Qiantong Xu\nSambaNova Systems, Inc.\nPalo Alto, CA, USA\nzoltan.csaki@sambanovasystems.com\nAbstract\nRecent large language models (LLM) exhibit sub-optimal performance on low-\nresource languages, as the training data of these models is usually dominated by\nEnglish and other high-resource languages. Furthermore, it is challenging to train\nmodels for low-resource languages, especially from scratch, due to a lack of high\nquality training data. Adapting pretrained LLMs reduces the need for data in the\nnew language while also providing cross lingual transfer capabilities. However,\nnaively adapting to new languages leads to catastrophic forgetting and poor to-\nkenizer efﬁciency. In this work, we study how to efﬁciently adapt any existing\npretrained LLM to a new language without running into these issues. In partic-\nular, we improve the encoding efﬁciency of the tokenizer by adding new tokens\nfrom the target language and study the data mixing recipe to mitigate forgetting.\nOur experiments on adapting an English LLM to Hungarian and Thai show that\nour recipe can reach better performance than open source models on the target\nlanguage, with minimal regressions on English.\n1\nIntroduction & Related work\nMultilingual large language models have become prevalent recently [1, 2, 3, 4, 5, 6], and have\nshown strong cross lingual knowledge and capability transfer [7, 8, 9, 10, 11, 12, 13]. However,\nthese multilingual models tend to perform poorly on low-resource languages. On top of this, training\nmodels for low-resource languages from scratch is also challenging due to a lack of training data and\nprohibitive computational requirements. These challenges, along with the prevalence open sourced\nEnglish models creates an interesting opportunity to see how they can be adapted to new languages\nquickly, without wasting resources by pretraining from scratch. While prior work [9, 10, 11, 14, 13]\nhas studied this concept, there are two important questions that warrant further investigation.\nHow to efﬁciently encode the new language? Byte Pair Encoding (BPE) [15] tokenizers are com-\nmonly used in LLMs including GPT[16, 17], Llama [18, 19] and BLOOM [1, 2]. These tokenizers\nare able to encode text at the byte level so that they can generalize to characters that are outside\nof their vocabulary; this means that any BPE tokenizer can be used for all languages. However,\nthe BPE tokenizer has poor tokenization efﬁciency if it was not trained on a given language. For\nexample, the original English-centric GPT2 tokenizer with a vocabulary size of 50k needs to use\n3.8 times more tokens to encode Thai compared to a smaller tokenizer with a vocabulary size of 5k\nthat is trained on Thai. This will inevitably cost us 3.8 times more compute in both training and\ninference. Furthermore, it has been shown that models with sub-optimal tokenizers can also have\nworse evaluation results [20, 21]. In our work, we show how to improve tokenizer fertility[22] by\nreplacing the least frequent tokens in the base model with tokens from the new language.\nHow to avoid catastrophic forgetting? Many works have shown that when continuing to train a\nLLM on data from a new domain, it undergoes catastrophic forgetting of the original domain it was\ntrained on [23], and similar issues appear when training on a new language [23, 9, 24, 2, 25, 10, 26].\nDifferent training paradigms including instruction-align[24], MAD-X [27], (IA)3 [28] are proposed\nEfﬁcient Natural Language and Speech Processing Workshop @ NeurIPS 2023.\nFigure 1: Fertility score of the bilingual tokenizers (left: Hungarian, right: Thai) with different\nnumber of tokens replaced by new language. The red lines represent the original GPT-2 tokenizer\n(50k vocabulary), while the green lines represent the tokenizer trained purely on the new language.\nEvery tokenizer has the same total vocabulary size. The number of replaced tokens are in 103 scale.\n2 4\n8\n15\n1\n2\n3\n4\nnum. replaced tokens\nFertility\nHungarian\nGPT-2(EN)\nHU\nEN/HU\n2 4\n8\n15\n1\n2\n3\n4\nEnglish\n(a) Hungarian Tokenizer\n2 4\n8\n15\n2\n4\n6\n8\nnum. replaced tokens\nFertility\nThai\nGPT-2(EN)\nTH\nEN/TH\n2 4\n8\n15\n1\n2\n3\n4\nEnglish\n(b) Thai Tokenizer\nto alleviate this issue, while mixing the training corpus from different languages [9, 11, 14, 29, 12,\n13] is an approach shared among all the methods above. Thus, in order to avoid forgetting, we\nstudy how to use the minimum amount of mixed training data in both continuous pretraining and\ninstruction tuning stages.\nWe adapt an English-centric model to Hungarian and Thai, and our evaluations show that adding new\ntokens and mixing training data from both languages can retain the model’s English capabilities in\naddition to improving the models ability to learn the new language. Some contemporary works\nexplore similar, but far less efﬁcient methods of training LLMs on low resource languages. [30]\nbuilds an English-Arabic bilingual LLM, but they train it from scratch; while [29] builds one for\nEnglish-Portuguese, but it does not optimize the tokenizer or mix the training data.\n2\nImplementation Details\n2.1\nImproving Tokenizer Efﬁciency\nTo adapt an existing tokenizer to a new language, tokens from the low resource language can be\nadded to the existing tokenizer’s vocabulary to improve its fertility. Fertility is deﬁned as the average\nnumber of tokens per word [22], and details about how we calculated it can be found in appendix\nA.1. In our work, instead of extending the tokenizer’s vocabulary, we replace the least frequent\ntokens from it with tokens from the new language. This way, we keep the model capability the same\nby controlling the vocabulary and embedding table size. In particular, we train a BPE tokenizer\non the new language with vocabulary size k and check the number of overlapping tokens o with\nthe original tokenizer. Then we replace the least important k −o non-overlapping tokens from the\noriginal tokenizer with the new ones. We also reinitialize the corresponding embeddings in the\nmodel. For more details see appendix A.2.\nAs shown in ﬁgure 1, as the number of replaced tokens k increases, the fertility of the tokenizer\napproaches the monolingual tokenizer fertility on the new language, with minimal regressions on\nthe English fertility. We choose to replace around 5000 tokens, which is only 10% of the overall\nvocabulary, because it improves the fertility by 42% on Hungarian and 73% on Thai. Note that\n50% fertility drop entails two times faster training and inference. In addition, replacing more tokens\nbeyond 5000 provides diminishing returns on the fertility, while also increases the difﬁculty of model\nadaptation due to having more randomly re-initialized token embeddings.\n2.2\nTraining Data Mixtures\nTraining data for both pretraining and ﬁnetuning are prepared following the details in Section 3.\nOnce the datasets are prepared for both languages, we shufﬂe them at sample level, so that every\nbatch contains text from both languages during training. Note that in our experiments, we do not\nmake any further transformations to either the model or the datasets, after the data is prepared\non each side, so that our study is orthogonal and complementary to existing proposed methods\n[24, 27, 28, 9, 12] focusing on training paradigm studies.\n2\nTable 1: Each model is labeled by the language it is adapted for, followed by what style of training\nwas done, The EN PT model is the base model for all following rows. PT stands for pretrained and\nIT stands for instruction tuned. Each column represents the average of all the benchmarks from a\nclassiﬁed under a language and category, the constituent benchmarks can be found in appendix E\nEnglish\nHungarian\nThai\nTasks\nMulti-choice Multi-choice\nQA\nSum.\nTrans.\nMulti-choice\nQA\nSum.\nTrans.\nMetrics\nAcc. (↑)\nAcc. (↑)\nF1 (↑) Rouge-2 (↑) BLEU (↑) Acc. (↑)\nF1 (↑) Rouge-2 (↑) BLEU (↑)\nLlama2-7B\n59.2%\n43.9%\n4.3%\n2.8\n47.1%\n48.6%\n30.9\n7.7\nXGLM-7.5B\n51.9%\n42.8%\n15.8%\n0.4\n1.1\n46.8%\n27.9%\n0.0\n0.2\nmt0-xxl\n52.7%\n50.6%\n30.6%\n2.0\n14.0\n46.2%\n85.3%\n21.9\n0.9\nPULI-GPT-3SX\n33.5%\n43.2%\n35.9%\n3.1\n1.3\n-\n-\n-\n-\nopenthaigpt-7b-chat\n60.9%\n-\n-\n-\n-\n43.7%\n43.4%\n26.5\n4.0\nEN PT\n57.3%\n46.2%\n32.8%\n0.9\n1.9\n46.0%\n14.4%\n0.0\n0.2\nEN PT + IT\n57.3%\n45.9%\n34.4%\n1.3\n1.7\n46.2%\n16.1%\n2.7\n0.0\nHU PT\n55.3%\n44.9%\n48.5%\n3.7\n7.7\n-\n-\n-\n-\nHU PT + IT\n58.0%\n54.9%\n64.3%\n9.2\n6.1\n-\n-\n-\n-\nTH PT\n57.4%\n-\n-\n-\n-\n48.4%\n31.1%\n11.4\n3.9\nTH PT + IT\n56.4%\n-\n-\n-\n-\n49.9%\n48.9%\n13.9\n12.5\n3\nExperiments\n3.1\nExperimental Setup\nTraining is done in a two stage pipeline. The ﬁrst stage is adaptive pretraining (PT) where a base\npretrained English 13B GPT-2 model (B) is continuously trained on a mixture composed of the new\nlanguage and English. Then, the adapted checkpoint is instruction tuned (IT) on a collection of\nprompt completion pairs from the new language and English. For more information see appendix\nB,C\nWe categorize all evaluation tasks into 4 categories. Multiple Choice, for this category we append\neach candidate answer to the prompt and pick the highest probability answer. Open-ended Question\nAnswering, where we let the model generate an answer for each question, and report the average\nF1 score between the model output and the ground truth. Summarization, where we let the model\ngenerate a summary and report the average ROUGE-2 score between the model output and ground\ntruth. Translation, where we let the model generate translated text and report the BLEU score\nbetween the model output and the ground truth. When we report the score for each category, it is the\naveraged score of all the evaluation tasks that we classiﬁed into that category in appendix E.\n3.2\nMain Results\nWe list all the results in table 1. The HU PT model is trained from EN PT with 50% HU, 50% EN\ndata, and TH PT is similarly trained but with Thai data. The HU PT + IT model is trained from the\nHU PT checkpoint with 50% HU, 50% EN IT data. The TH PT + IT is similarly trained from the\nTH PT checkpoint with 50% TH, 50% EN IT data. The EN models trained on purely English data\nare used as baselines. We list out the dataset and training details in appendix C, D.\nTable 1 shows that with our proposed training recipe, the adapted models are able to maintain the\nperformance on English benchmarks, and improve signiﬁcantly on the benchmarks of the new lan-\nguages. This conﬁrms the effectiveness of replacing the tokens in the tokenizer and mixing training\ndata to efﬁciently adapt a LLM to a new language. On top of this, the adapted models perform as\nwell or better than the state of the art baseline models we have evaluated.\n3.3\nAblation Studies\n3.3.1\nTokenizer\nTable 2: Performance of Hungarian\nmodel with different tokenizers.\nGPT2\nBilingual\nEN - Multi-choice\n57.9%\n58.0%\nHU- Multi-choice\n50.8%\n54.9%\nHU - QA\n63.2%\n64.3%\nHU - Sum.\n7.9\n9.2\nHU - Trans.\n8.8\n6.1\nIn this experiment, we evaluate models trained on identical\ndata during both the continuous pretraining and IT stages.\nThese models follow the same training recipe but use differ-\nent tokenizers. Table 2 shows that the model trained with\nthe bilingual tokenizer performs as well or better than the\n3\nmodel that uses the original tokenizer, on both English and Hungarian tasks. While at the same\ntime, the bilingual tokenizer has much better encoding efﬁciency as illustrated in Figure 1.\n3.3.2\nPretraining data mixture\nGiven the same total amount of training data, we tested varying the percentage of English data (50%,\n25% and 0%) in the English/Hungarian bilingual data mixture. All training is run for 30k steps. We\nalso compare this to training a pure Hungarian model using only Hungarian data [31], a Hungarian\ntokenizer, from scratch for 100k steps. All the training details can be found in appendix D.1.\nFigure 2: Varying pretraining data mixtures. \"EN\"\nand \"HU\" models are monolingual models trained\nfrom scratch, while the other models are trained\nfrom the \"EN\" model with the labeled data mix-\nture.\nEN\n50EN/50HU\n25EN/75HU\n0EN/100HU\nHU\n0\n1\n2\n3\nByte Perplexity ↓\nHU Eval Perplexity\nEN\n50EN/50HU\n25EN/75HU\n0EN/100HU\nHU\n0\n30\n60\nAccuracy ↑\nEN Multi-choice\nWe summarize the comparison results in Figure\n2. The ﬁrst ﬁnding is that it is effective to add\nin training data from the new language dur-\ning pretraining, because those models greatly\noutperformed the baseline English model. Sec-\nond, it is better to adapt a pretrained LLM\nthan train a new one from scratch, as the\nadapted checkpoint performs better on both lan-\nguages, even though they are trained for one\nthird as long. Third, when comparing the re-\nsults from the models trained with and without\nEnglish data mixed in, we can see that mixing\nEnglish data can mitigate the catastrophic\nforgetting on English and improve the model\nperformance on Hungarian. Note that there is\nno signiﬁcant difference between mixing 50%\nand 25% of English data during training, which implies that adaptation is not sensitive to the exact\nmixture ratio as long as the original language and new language are included.\n3.3.3\nIT data mixture\nFigure 3: Model performance with different IT\ndata mixture. ROUGE-2 score is reported for\nHU Sum, while accuracy and F1 scores are re-\nported for the rest of the tasks.\n0\n1\n5\n10\n20\n30\n40\n50\n30\n40\n50\n60\n70\nPercentage of Hungarian data\nAccuracy/F1\nEN - multi-choice\nHU - multi-choice\nHU - QA\n0\n0.1\n0.2\nROUGE-2\nEN - multi-choice\nHU - multi-choice\nHU - QA\nHU - Sum.\nWe run all instruction tuning experiments from\nthe Hungarian pretrained model using 6 gigabytes\nof instruction tuning text data (2 billion tokens)\nand the same training settings. Each experiment\nis repeated 3 times with different random dataset\nsamples.\nFor more details on instruction tuning\ndatasets or training settings see appendix C.2, C.4\nand D.2\nThere is a lack of diverse high quality instruc-\ntion tuning data in most languages besides English.\nThus, we study the impact of the amount of IT data\nfrom the new language on the ﬁnal model perfor-\nmance by varying the Hungarian instruction tuning\ndata mixing rate from 0% to 50%. Figure 3 shows\nthat when no Hungarian instruction tuning data is included, the model undergoes catastrophic for-\ngetting of Hungarian. However, the model performance on Hungarian improves as more Hungarian\ndata is mixed in, with marginal returns after more than 1% of the data is Hungarian. This indicates\nthat a small amount of IT data from the new language gives most of the model performance on the\nnew language.\n4\nConclusion\nIn our paper, we study the recipe to efﬁciently adapt an existing pretrained LLM to a new language,\nwith better tokenizer efﬁciency and without catastrophic forgetting of its original knowledge. With\nonly 10% of tokens in the tokenizer replaced by the the new ones from the target language, it can\ndrop the fertility by 50% and 70% on Hungarian and Thai respectively, with limited regression on\nEnglish. This can greatly improve the efﬁciency of both training and inference on the new language\nby 2x and 3x. In addition, with mixing training data from both languages in pretraining and IT\n4\nstages, we show that this can improve the model performance on the new language while retaining\nthe model capability on the original language.\nReferences\n[1] B. Workshop, “Bloom: A 176b-parameter open-access multilingual language model,” 2023.\n[2] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari,\nS. Shen, Z.-X. Yong, H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie,\nZ. Alyafeai, A. Webson, E. Raff, and C. Raffel, “Crosslingual generalization through multitask\nﬁnetuning,” 2023.\n[3] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel,\n“mt5: A massively multilingual pre-trained text-to-text transformer,” 2021.\n[4] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave,\nM. Ott, L. Zettlemoyer, and V. Stoyanov, “Unsupervised cross-lingual representation learning\nat scale,” 2020.\n[5] N. Goyal, J. Du, M. Ott, G. Anantharaman, and A. Conneau, “Larger-scale transformers for\nmultilingual masked language modeling,” 2021.\n[6] O. Shliazhko, A. Fenogenova, M. Tikhonova, V. Mikhailov, A. Kozlova, and T. Shavrina,\n“mgpt: Few-shot learners go multilingual,” 2022.\n[7] X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal, S. Bhosale,\nJ. Du, R. Pasunuru, S. Shleifer, P. S. Koura, V. Chaudhary, B. O’Horo, J. Wang, L. Zettlemoyer,\nZ. Kozareva, M. Diab, V. Stoyanov, and X. Li, “Few-shot learning with multilingual language\nmodels,” 2022.\n[8] H. Xu, B. V. Durme, and K. Murray, “Bert, mbert, or bibert? a study on contextualized embed-\ndings for neural machine translation,” 2021.\n[9] Z.-X. Yong, H. Schoelkopf, N. Muennighoff, A. F. Aji, D. I. Adelani, K. Almubarak, M. S.\nBari, L. Sutawika, J. Kasai, A. Baruwa, G. I. Winata, S. Biderman, E. Raff, D. Radev, and\nV. Nikoulina, “Bloom+1: Adding language support to bloom for zero-shot prompting,” 2023.\n[10] J. Phang, I. Calixto, P. M. Htut, Y. Pruksachatkun, H. Liu, C. Vania, K. Kann, and S. R. Bow-\nman, “English intermediate-task training improves zero-shot cross-lingual transfer too,” 2020.\n[11] A. Ebrahimi and K. Kann, “How to adapt your pretrained multilingual model to 1600 lan-\nguages,” 2021.\n[12] J. Ye, X. Tao, and L. Kong, “Language versatilists vs. specialists: An empirical revisiting on\nmultilingual transfer ability,” 2023.\n[13] J. Armengol-Estapé, O. de Gibert Bonet, and M. Melero, “On the multilingual capabilities of\nvery large-scale english language models,” 2021.\n[14] K. Ogueji, Y. Zhu, and J. Lin, “Small data? no problem! exploring the viability of pretrained\nmultilingual language models for low-resourced languages,” in Proceedings of the 1st Work-\nshop on Multilingual Representation Learning, 2021, pp. 116–126.\n[15] P. Gage, “A new algorithm for data compression,” The C Users Journal Archive, vol. 12, pp.\n23–38, 1994. [Online]. Available: https://api.semanticscholar.org/CorpusID:59804030\n[16] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Language models are\nunsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[17] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei,\n“Language models are few-shot learners,” 2020.\n5\n[18] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière,\nN. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, “Llama:\nOpen and efﬁcient foundation language models,” 2023.\n[19] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,\nM.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov,\nP. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten,\nR. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan,\nP. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\nS. Edunov, and T. Scialom, “Llama 2: Open foundation and ﬁne-tuned chat models,” 2023.\n[20] P. Rust, J. Pfeiffer, I. Vuli´c, S. Ruder, and I. Gurevych, “How good is your tokenizer? on the\nmonolingual performance of multilingual language models,” 2021.\n[21] F. Stollenwerk, “Training and evaluation of a multilingual tokenizer for gpt-sw3,” 2023.\n[22] J.\nÁcs.\n(2019,\nFebruary)\nExploring\nbert’s\nvocabulary.\n[Online].\nAvailable:\nhttps://juditacs.github.io/2019/02/19/bert-tokenization-stats.html\n[23] R. M. French, “Catastrophic forgetting in connectionist networks,” Trends in cognitive sci-\nences, vol. 3, no. 4, pp. 128–135, 1999.\n[24] S. Cahyawijaya, H. Lovenia, T. Yu, W. Chung, and P. Fung, “Instruct-align: Teaching novel\nlanguages with to llms through alignment-based cross-lingual instruction,” 2023.\n[25] I. Chalkidis, M. Fergadiotis, and I. Androutsopoulos, “Multieurlex – a multi-lingual and multi-\nlabel legal document classiﬁcation dataset for zero-shot cross-lingual transfer,” 2021.\n[26] T. Vu, A. Barua, B. Lester, D. Cer, M. Iyyer, and N. Constant, “Overcoming catastrophic\nforgetting in zero-shot cross-lingual generation,” 2022.\n[27] J. Pfeiffer, I. Vuli´c, I. Gurevych, and S. Ruder, “MAD-X: An Adapter-Based Frame-\nwork for Multi-Task Cross-Lingual Transfer,” in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP).\nOnline:\nAssocia-\ntion for Computational Linguistics, Nov. \"2020\", pp. 7654–7673. [Online]. Available:\nhttps://aclanthology.org/2020.emnlp-main.617\n[28] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel, “Few-shot\nparameter-efﬁcient ﬁne-tuning is better and cheaper than in-context learning,” Advances in\nNeural Information Processing Systems, vol. 35, pp. 1950–1965, 2022.\n[29] R. Pires, H. Abonizio, T. S. Almeida, and R. Nogueira, “Sabiá: Portuguese large language\nmodels,” 2023.\n[30] N. Sengupta, S. K. Sahu, B. Jia, S. Katipomu, H. Li, F. Koto, O. M. Afzal, S. Kamboj, O. Pan-\ndit, R. Pal et al., “Jais and jais-chat: Arabic-centric foundation and instruction-tuned open\ngenerative large language models,” arXiv preprint arXiv:2308.16149, 2023.\n[31] D.\nM.\nNemeskey,\n“Natural\nlanguage\nprocessing\nmethods\nfor\nlanguage\nmodel-\ning,”\nPh.D.\ndissertation,\nEötvös\nLoránd\nUniversity,\n2020.\n[Online].\nAvailable:\nhttps://hlt.bme.hu/media/pdf/nemeskey_thesis.pdf\n[32] J. Nivre, M.-C. de Marneffe, F. Ginter, Y. Goldberg, J. Hajiˇc, C. D. Manning, R. McDonald,\nS. Petrov, S. Pyysalo, N. Silveira, R. Tsarfaty, and D. Zeman, “Universal Dependencies v1:\nA multilingual treebank collection,” in Proceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16).\nPortorož, Slovenia:\nEuropean\nLanguage Resources Association (ELRA), May 2016, pp. 1659–1666. [Online]. Available:\nhttps://aclanthology.org/L16-1262\n6\n[33] V. Vincze,\nD. Szauter,\nA. Almási,\nG. Móra,\nZ. Alexin,\nand J. Csirik,\n“Hun-\ngarian dependency treebank,”\nin Proceedings of the Seventh International Confer-\nence\non\nLanguage Resources\nand\nEvaluation\n(LREC’10).\nValletta,\nMalta:\nEu-\nropean\nLanguage\nResources\nAssociation\n(ELRA),\nMay\n2010.\n[Online].\nAvailable:\nhttp://www.lrec-conf.org/proceedings/lrec2010/pdf/465_Paper.pdf\n[34] D. Zeman, M. Popel, M. Straka, J. Hajiˇc, J. Nivre, F. Ginter, J. Luotolahti, S. Pyysalo,\nS. Petrov, M. Potthast, F. Tyers, E. Badmaeva, M. Gokirmak, A. Nedoluzhko, S. Cinková,\nJ. Hajiˇc jr., J. Hlaváˇcová, V. Kettnerová, Z. Urešová, J. Kanerva, S. Ojala, A. Missilä,\nC. D. Manning, S. Schuster, S. Reddy, D. Taji, N. Habash, H. Leung, M.-C. de Marneffe,\nM. Sanguinetti, M. Simi, H. Kanayama, V. de Paiva, K. Droganova, H. Martínez Alonso,\nÇ. Çöltekin, U. Sulubacak, H. Uszkoreit, V. Macketanz, A. Burchardt, K. Harris,\nK. Marheinecke, G. Rehm, T. Kayadelen, M. Attia, A. Elkahky, Z. Yu, E. Pitler, S. Lertpradit,\nM. Mandl, J. Kirchner, H. F. Alcalde, J. Strnadová, E. Banerjee, R. Manurung, A. Stella,\nA. Shimada, S. Kwak, G. Mendonça, T. Lando, R. Nitisaroj, and J. Li, “CoNLL 2017 shared\ntask: Multilingual parsing from raw text to Universal Dependencies,” in Proceedings of the\nCoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.\nVancouver, Canada:\nAssociation for Computational Linguistics, Aug. 2017, pp. 1–19.\n[Online]. Available: https://aclanthology.org/K17-3001\n[35] N. Silveira, T. Dozat, M.-C. de Marneffe, S. Bowman, M. Connor, J. Bauer, and C. D. Manning,\n“A gold standard dependency corpus for English,” in Proceedings of the Ninth International\nConference on Language Resources and Evaluation (LREC-2014), 2014.\n[36] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN. Nabeshima, S. Presser, and C. Leahy, “The pile: An 800gb dataset of diverse text for\nlanguage modeling,” 2020.\n[37] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu,\n“Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,” 2023.\n[38] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph,\nJ. Wei et al., “The ﬂan collection: Designing data and methods for effective instruction tuning,”\narXiv preprint arXiv:2301.13688, 2023.\n[39] H. Nguyen, “The oig dataset,” Mar 2023. [Online]. Available: https://laion.ai/blog/oig-dataset/\n[40] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\nKoura, X. Li, B. O’Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and\nV. Stoyanov, “Opt-iml: Scaling language model instruction meta learning through the lens of\ngeneralization,” 2023.\n[41] J. Abadji, P. O. Suarez, L. Romary, and B. Sagot, “Towards a cleaner document-oriented mul-\ntilingual crawled corpus,” 2022.\n[42] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua,\nand C. Raffel, “mT5:\nA massively multilingual pre-trained text-to-text transformer,”\nin\nProceedings of\nthe\n2021\nConference of\nthe\nNorth\nAmerican\nChapter of\nthe\nAssociation for Computational Linguistics:\nHuman Language Technologies.\nOnline:\nAssociation for Computational Linguistics, Jun. 2021, pp. 483–498. [Online]. Available:\nhttps://aclanthology.org/2021.naacl-main.41\n[43] G. Wenzek, M.-A. Lachaux, A. Conneau, V. Chaudhary, F. Guzmán, A. Joulin, and E. Grave,\n“Ccnet: Extracting high quality monolingual datasets from web crawl data,” 2019.\n[44] C. Mou, C. Ha, K. Enevoldsen, and P. Liu, “Chenghaomou/text-dedup: Reference snapshot,”\nSep. 2023. [Online]. Available: https://doi.org/10.5281/zenodo.8364980\n[45] A. Broder, “On the resemblance and containment of documents,” in Proceedings. Compression\nand Complexity of SEQUENCES 1997 (Cat. No.97TB100171), 1997, pp. 21–29.\n[46] H. Nomoto,\n“Interpersonal meaning annotation for asian\nlanguage corpora:\nThe\ncase\nof\ntufs\nasian\nlanguage\nparallel\ncorpus\n(talpco),”\n2019.\n[Online].\nAvailable:\nhttps://api.semanticscholar.org/CorpusID:209441604\n7\n[47] B. Buschbeck-Wolf and M. Exel, “A parallel evaluation data set of software documentation\nwith document structure annotation,” in Workshop on Asian Translation, 2020. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:221095497\n[48] H. Riza, M. Purwoadi, Gunarso, T. Uliniansyah, A. A. Ti, S. M. Aljunied, L. C. Mai,\nV. T. Thang, N. P. Thai, V. Chea, R. Sun, S. Sam, S. Seng, K. M. Soe, K. T. Nwet,\nM. Utiyama, and C. Ding, “Introduction of the asian language treebank,” 2016 Conference\nof The Oriental Chapter of International Committee for Coordination and Standardization\nof Speech Databases and Assessment Techniques (O-COCOSDA), pp. 1–6, 2016. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:45848332\n[49] F. Ladhak, E. Durmus, C. Cardie, and K. McKeown, “Wikilingua: A new benchmark dataset\nfor multilingual abstractive summarization,” ArXiv, vol. abs/2010.03093, 2020. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:222177239\n[50] M. Cettolo, C. Girardi, and M. Federico, “WIT3: Web inventory of transcribed and translated\ntalks,” in Proceedings of the 16th Annual conference of the European Association for Machine\nTranslation.\nTrento, Italy: European Association for Machine Translation, May 28–30\n\"2012\", pp. 261–268. [Online]. Available: https://www.aclweb.org/anthology/2012.eamt-1.60\n[51] N. team, M. R. Costa-jussà, J. Cross, O. cCelebi, M. Elbayad, K. Heaﬁeld, K. Heffernan,\nE. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood,\nB. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R.\nSadagopan, D. Rowe, S. L. Spruit, C. Tran, P. Y. Andrews, N. F. Ayan, S. Bhosale,\nS. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzm’an, P. Koehn, A. Mourachko,\nC. Ropers, S. Saleem, H. Schwenk, and J. Wang, “No language left behind:\nScaling\nhuman-centered machine translation,” ArXiv, vol. abs/2207.04672, 2022. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:250425961\n[52] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman, H. Schwenk, and\nV. Stoyanov, “Xnli:\nEvaluating cross-lingual sentence representations,” in Conference\non Empirical Methods in Natural Language Processing,\n2018. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:52271711\n[53] M. Artetxe, S. Ruder, and D. Yogatama, “On the cross-lingual transferability of monolingual\nrepresentations,” CoRR, vol. abs/1910.11856, 2019.\n[54] K. Viriyayudhakorn and C. Polpanumas, “iapp_wiki_qa_squad,” Feb. 2021. [Online].\nAvailable: https://doi.org/10.5281/zenodo.4539916\n[55] A.\nSuriyawongkul,\nE.\nChuangsuwanich,\nP.\nChormai,\nand\nC.\nPolpanumas,\n“Pythainlp/wisesight-sentiment:\nFirst\nrelease,”\nSep.\n2019.\n[Online].\nAvailable:\nhttps://doi.org/10.5281/zenodo.3457447\n[56] N. Chumpolsathien, “Using knowledge distillation from keyword extraction to improve the\ninformativeness of neural cross-lingual summarization,” Master’s thesis, Beijing Institute of\nTechnology, 2020.\n[57] T. Hasan, A. Bhattacharjee, M. S. Islam, K. Mubasshir, Y.-F. Li, Y.-B. Kang, M. S. Rahman,\nand R. Shahriyar, “XL-sum:\nLarge-scale multilingual abstractive summarization for 44\nlanguages,” in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.\nOnline: Association for Computational Linguistics, Aug. 2021, pp. 4693–4703. [Online].\nAvailable: https://aclanthology.org/2021.ﬁndings-acl.413\n[58] R.\nTaori,\nI.\nGulrajani,\nT.\nZhang,\nY.\nDubois,\nX.\nLi,\nC.\nGuestrin,\nP.\nLiang,\nand\nT.\nB.\nHashimoto,\n“Stanford\nalpaca:\nAn\ninstruction-following llama\nmodel,”\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n[59] M.\nConover,\nM.\nHayes,\nA.\nMathur,\nJ.\nXie,\nJ.\nWan,\nS.\nShah,\nA.\nGh-\nodsi,\nP.\nWendell,\nM.\nZaharia,\nand\nR.\nXin.\n(2023)\nFree\ndolly:\nIntro-\nducing\nthe\nworld’s\nﬁrst\ntruly\nopen\ninstruction-tuned\nllm.\n[Online].\nAvailable:\nhttps://www.databricks.com/blog/2023/04/12/dolly-ﬁrst-open-commercially-viable-instruction-tuned-llm\n8\n[60] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, “How close\nis chatgpt to human experts? comparison corpus, evaluation, and detection,” arXiv preprint\narxiv:2301.07597, 2023.\n[61] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z. R. Tam, K. Stevens, A. Barhoum,\nN. M. Duc, O. Stanley, R. Nagyﬁ, E. Shahul, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire,\nC. Schuhmann, H. Nguyen, and A. Mattick, “Openassistant conversations - democratizing\nlarge language model alignment,” ArXiv, vol. abs/2304.07327, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:258179434\n[62] R. Prabhakar and S. Jairath, “Sambanova sn10 rdu:accelerating software 2.0 with dataﬂow,” in\n2021 IEEE Hot Chips 33 Symposium (HCS), 2021, pp. 1–37.\n[63] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPoﬁ, C. Foster, L. Golding, J. Hsu, K. McDonell,\nN. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou,\n“A framework for few-shot language model evaluation,” Sep. 2021. [Online]. Available:\nhttps://doi.org/10.5281/zenodo.5371628\n[64] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni,\nG. Boleda, and R. Fernández, “The lambada dataset: Word prediction requiring a broad dis-\ncourse context,” 2016.\n[65] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag: Can a machine re-\nally ﬁnish your sentence?” in Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\n[66] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? a\nnew dataset for open book question answering,” 2018.\n[67] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, “Boolq: Ex-\nploring the surprising difﬁculty of natural yes/no questions,” 2019.\n[68] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, “Think\nyou have solved question answering? try arc, the ai2 reasoning challenge,” 2018.\n[69] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “Piqa: Reasoning about physical common-\nsense in natural language,” 2019.\n[70] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela, “Adversarial nli: A new\nbenchmark for natural language understanding,” 2020.\n[71] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, “Winogrande: An adversarial winograd\nschema challenge at scale,” 2019.\n[72] N. Ligeti-Nagy, G. Ferenczi, E. Héja, K. Jelencsik-Mátyus, L. J. Laki, N. Vadász, Z. G. Yang,\nand T. Vadász, “Hulu: magyar nyelv˝u benchmark adatbázis kiépítése a neurális nyelvmodellek\nkiértékelése céljából,” in XVIII. Magyar Számítógépes Nyelvészeti Konferencia, 2022, pp. 431–\n446.\n[73] E. M. Ponti, G. G. s, O. Majewska, Q. Liu, I. Vuli’c, and A. Korhonen, “XCOPA: A\nmultilingual dataset for causal commonsense reasoning,” arXiv preprint, 2020. [Online].\nAvailable: https://ducdauge.github.io/ﬁles/xcopa.pdf\n[74] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, “Looking beyond the sur-\nface:a challenge set for reading comprehension over multiple sentences,” in Proceedings of\nNorth American Chapter of the Association for Computational Linguistics (NAACL), 2018.\n[75] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bow-\nman, “Superglue: A stickier benchmark for general-purpose language understanding systems,”\narXiv preprint arXiv:1905.00537, 2019.\n[76] T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polo-\nsukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai,\nJ. Uszkoreit, Q. Le, and S. Petrov, “Natural questions: a benchmark for question answering\nresearch,” Transactions of the Association of Computational Linguistics, 2019.\n9\n[77] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, and B. V. Durme, “Record: Bridging the gap between\nhuman and machine commonsense reading comprehension,” ArXiv, vol. abs/1810.12885,\n2018. [Online]. Available: https://api.semanticscholar.org/CorpusID:53116244\n[78] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details, just the summary! topic-\naware convolutional neural networks for extreme summarization,” 2018.\n10\nA\nTokenizer Details\nA.1\nTokenizer Fertility Deﬁnition\nFertility is deﬁned as the average number of tokens per word [22]. Words are deﬁned by the Uni-\nversal Dependencies framework, which gives a \"consistent annotation of grammar (parts of speech,\nmorphological features, and syntactic dependencies) across different human languages\" [32]. To\ncalculate the fertility we tokenize each word from the treebank individually to get the sum total\nnumber of tokens, and then divide this by the number of words in the treebank. For Hungarian we\nused the test set of the \"Szeged Dependency Treebank\" [33], for Thai we used the test set of the Thai\n\"CoNLL 2017\" treebank [34] and for English we used the test set of the \"Gold Standard Universal\nDependencies Corpus\" [35].\nA.2\nToken Replacement Details\nIn our work, instead of extending the tokenizer’s vocabulary, we replace the least frequent tokens\nfrom it with tokens from the new language. This way, we keep the model capacity the same by\ncontrolling the vocabulary and embedding table size.\n1. Train a tokenizer limited to a vocabulary size k, where k is the number of tokens you want\nto replace in the original tokenizer\n2. Find the number of overlapping tokens o between the new tokenizer of vocab size k, and\nthe original tokenizer\n3. Replace the least frequently used k −o tokens from the original tokenizer with the k −o\ntokens from the new tokenizer. Ensure that all the unchanged tokens from the original\ntokenizer keep the same vocabulary indices as they had before.\n(a) note that in the GPT2 tokenizer the tokens in the vocabulary and merges ﬁle are or-\ndered from most frequent to least frequent, so we replace the last k −o vocabulary\nindices in a GPT2 Tokenizer.\n4. The GPT2 Tokenizer executes the merges rules in the merges.txt ﬁle line by line, so to\nimprove the efﬁciency on the newly added tokens, Add the merges rules from the k −o\nnew tokens to the beginning of the merges.txt ﬁle.\n(a) Note that various BPE encoding algorithms are implemented without using the merges\nrules, so ensure that you examine your tokenizer to see how to improves the tokenizer\nefﬁciency.\n5. Randomize the embeddings of the replaced tokens in the original model so the new embed-\ndings can be learned.\nWe tested this tokenizer to ensure that the encoding and decoding of text works properly, and ﬁgure\n1 shows that it also improves the fertility.\nB\nBase Model\nWe train our base model with the same tokenizer and architecture as GPT-2 model [16]. The model\nhas 40 layers of transformer blocks with hidden dimension 5120 and 13 billion parameters in total.\nThe vocabulary size is 50260. The base model was pretrained on 300B English tokens from the\nPILE[36] and C4[37] datasets, ﬁltered for only natural language English text.\nC\nDatasets\nC.1\nEnglish pretraining data\nFor the continuous pretraining phase, we often mix English data with either Hungarian or Thai.\nThe English data we used is a 100 gigabyte sample of data from the base model pretraining corpus\nintroduced in section B.\nC.2\nEnglish instruction tuning data\nTo construct our English instruction tuning dataset, we sample each constituent task from FlanV2\n[38] and OIG [39] equally by raw text size with a ﬁxed dataset size budget. This creates an instruc-\n11\ntion tuning dataset that is task diverse and reasonably sized. The beneﬁt of sampling instruction\ntuning data at the task level is shown in [40], and provides a compute efﬁcient alternative to training\non all the data. The dataset is 2.6 gigabytes of raw text and about 1.9 million samples.\nC.3\nHungarian pretraining tuning data\nThe dataset used for Hungarian pretraining is the Hungarian Webcorpus 2.0 [31]. Our dataset is 96\ngigabytes and 11,152,900 documents.\nC.4\nHungarian instruction tuning tuning data\nThere is a lack of naturally written Hungarian instruction tuning datasets, so we use google translate\nto translate our English Instruction tuning corpus C.2 to Hungarian. During translation the prompt\nand completion are translated separately and only concatenated during training.\nC.5\nThai pretraining data\nFor the Thai pre-training corpus, we combine the Thai subsets of OSCAR [41], MC4 [42], and\nCCNet [43], which are all derived from Common Crawl. The entire combined corpus was processed\nwith MinHash deduplication [44, 45] with 1-grams and a Jaccard similarity of 0.6, with sentence\nlevel n-grams (split by whitespace), and totals 15.32 million documents.\nC.6\nThai instruction tuning data\nFor Thai instruction tuning data, we use a mixture of manually templated Thai datasets, as well as\nexisting IT datasets translated from English.\nWe take various Thai NLP datasets and create a variety of prompting templates for each task to form\nan instruction tuning dataset. These consist of translation [46] [47] [48] [49] [50] [51], NLI [52],\nQA [53] [54], text categorization, sentiment analysis [55], and summarization [56] [57] tasks. This\ncollection of datasets totals 6.26 million instruction tuning examples.\nThe English-translated IT datasets consist of traditional instruction tuning, multi-turn conversation,\nand domain-speciﬁc QA (i.e. general knowledge, ﬁnance, science, mathematics) sourced from col-\nlections like FLAN [38], OIG [39], Alpaca [58], Dolly [59], HC3 [60], and OpenAssistant [61],\ntotalling 1.06 million examples.\nD\nTraining Details\nD.1\nPretraining Hyperparameters\nThe training process utilized cross-entropy loss to optimize the CLM objective. All training runs\nshared the same hyperparameters to ensure a fair comparison and to avoid hyperparameter searches.\nWhen comparing two pre-training ablations, the runs were trained to token parity, training on the\nsame number of tokens regardless of available data or tokenizer efﬁciency\nThe hyperparameters used were batch size = 512, ﬁxed learning rate = 0.000015 and weight decay\n= 0.1. All the tokens were packed into the training sequences, if they did not ﬁt in a sequence then\nthey would be placed in the next sequence so no training tokens are lost.1 An attention mask was\napplied so that only tokens from the same article attend to each other.\nD.2\nInstruction Tuning Hyperparameters\nAll instruction tuning studies share the same hyper-parameters. But ablations comparing runs are\nnot run to step parity, rather they are all trained to 1 epoch to ensure they see all the data.\nThe hyperparameters used are batch size = 128, ﬁxed learning rate = 0.000015, weight decay = 0.1,\ngrad norm clip = 1.0, and prompt loss weight = 0.0 to ensure that prompts are attended to but not\ntrained on. All the tokens were packed into sequences in a greedy fashion, if they did not ﬁt in a\nsequence then they would be discarded.1 An attention mask was applied so that only tokens from\nthe same article attend to each other.\n1https://github.com/sambanova/generative_data_prep\n12\nD.3\nHardware Conﬁguration\nAll training is run on SambaNova’s Reconﬁgurable Data Units (RDU) [62].\nE\nEvaluation\nThe EAI evaluation harness[63] is used for all benchmarking. The code2 was adapted for new tasks\nto evaluate the models on non-English languages. We categorize all the tasks into 4 categories:\nMultiple-choice\nIn this category of tasks, we append each candidate option after the prompt and\nlet the model pick answer with the highest probability. We report average accuracy on each tasks.\nTable 3: Multiple Choice Evaluation Benchmarks\nLanguage\nEvaluation Tasks\nEnglish\nLambada [64], HellaSwag [65], Openbookqa [66], Boolq [67], Arc Easy and\nChallenge [68], PiQA [69], ANLI R1 [70] and Winogrande [71].\nHungarian\nHULU evaluation suite [72], which is composed of human translated tasks\nHuCB, HuSST, HuWNLI, HuCOPA, HuCOLA and HuRTE.\nThai\nXCOPA [73] and WiseSight Sentiment Analysis [55] corpus. Translated ver-\nsions of HellaSwag [65], MultiRC [74], RTE [75].\nOpen-ended Question Answering\nIn this category of tasks, we let the model to freely generate\ncompletions for each question prompt and we report the average F1 score between the model output\nand the ground truth answer.\nTable 4: Open-ended Question Answering Evaluation Benchmarks\nLanguage\nEvaluation Tasks\nHungarian\ntranslated versions of BoolQ [67] and Natural Questions [76]\nThai\nXQuAD [53] and a translated version of ReCoRD [77]\nSummarization\nIn this category of tasks, we let the model freely generate a summary for each\nprompt and we report average ROUGE-2 score between the model output and ground truth.\nTable 5: Summarization Evaluation Benchmarks\nLanguage\nEvaluation Tasks\nHungarian\nTranslated version of XSum [78].\nThai\nThaiSum [56]\nTranslation\nIn this category of tasks, we let the model to freely generate translated text and we\nreport BLEU score between the model output and the ground truth answer.\nTable 6: Translation Evaluation Benchmarks\nLanguage\nEvaluation Tasks\nHungarian\nwmt 2009 en-hu and wmt 2009 hu-en 3 datasets\nThai\nWIT3 Ted Talks Corpus [50]\n2our open source eval suite link\n13\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-11-09",
  "updated": "2023-12-14"
}