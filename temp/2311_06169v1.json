{
  "id": "http://arxiv.org/abs/2311.06169v1",
  "title": "Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping",
  "authors": [
    "Fabi Prezja"
  ],
  "abstract": "Deep learning-based vision is characterized by intricate frameworks that\noften necessitate a profound understanding, presenting a barrier to newcomers\nand limiting broad adoption. With many researchers grappling with the\nconstraints of smaller datasets, there's a pronounced reliance on pre-trained\nneural networks, especially for tasks such as image classification. This\nreliance is further intensified in niche imaging areas where obtaining vast\ndatasets is challenging. Despite the widespread use of transfer learning as a\nremedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML\nsolutions persists. Addressing these challenges is \"Deep Fast Vision\", a python\nlibrary that streamlines the deep learning process. This tool offers a\nuser-friendly experience, enabling results through a simple nested dictionary\ndefinition, helping to democratize deep learning for non-experts. Designed for\nsimplicity and scalability, Deep Fast Vision appears as a bridge, connecting\nthe complexities of existing deep learning frameworks with the needs of a\ndiverse user base.",
  "text": "Deep Fast Vision: A Python Library for Accelerated\nDeep Transfer Learning Vision Prototyping\nFabi Prezja1,2,*\n1University of Jyv¨askyl¨a, Faculty of Information Technology, Jyv¨askyl¨a, Finland\n2Finnish Artificial Intelligence Research Network, Jyv¨askyl¨a, Finland\n*corresponding.faprezja@jyu.fi\nABSTRACT\nDeep learning-based vision is characterized by intricate frameworks that often necessitate a profound understanding, presenting\na barrier to newcomers and limiting broad adoption. With many researchers grappling with the constraints of smaller datasets,\nthere’s a pronounced reliance on pre-trained neural networks, especially for tasks such as image classification. This reliance\nis further intensified in niche imaging areas where obtaining vast datasets is challenging. Despite the widespread use of\ntransfer learning as a remedy to the small dataset dilemma, a conspicuous absence of tailored auto-ML solutions persists.\nAddressing these challenges is \"Deep Fast Vision\", a python library that streamlines the deep learning process. This tool offers\na user-friendly experience, enabling results through a simple nested dictionary definition, helping to democratize deep learning\nfor non-experts. Designed for simplicity and scalability, Deep Fast Vision appears as a bridge, connecting the complexities of\nexisting deep learning frameworks with the needs of a diverse user base.\nIntroduction\nIn the rapidly evolving realm of artificial intelligence, deep learning1 has cemented its position as a transformative technology,\nfundamentally reshaping various sectors, from medicine and well-being2–9 to finance10. Central to this revolution is the domain\nof vision-based deep learning, which has enabled machines to perceive, interpret, and act upon visual data with unparalleled\naccuracy. However, as with any new technology, the path to its widespread adoption is riddled with challenges. One of\nthe predominant hurdles lies in the intricate nature of current deep learning frameworks. These platforms, while powerful,\noften require an in-depth understanding and expertise, thus posing a daunting barrier to entry for newcomers or those without\nspecialized training. This complexity not only hinders accessibility but also stifles broad-based adoption, preventing a larger\ncommunity from harnessing the potential of deep learning in vision tasks.\nFurther complicating the landscape is the inherent challenge of acquiring vast, labeled datasets, especially in niche imaging\nareas. Many researchers and developers find themselves constrained by the limited data at their disposal. As a result, there’s\na pronounced shift towards leveraging pre-trained neural networks, which, while beneficial, are not without thier own set of\nchallenges and limitations.\nAmidst these challenges, the technique of transfer learning11 has emerged, offering a pathway to leverage knowledge\nfrom extensive datasets to enhance performance on smaller, more specific datasets. Yet, even this approach is not without its\nhurdles, notably the glaring absence of comprehensive auto-ML solutions tailored for vision-based transfer learning. This\npaper introduces \"Deep Fast Vision12,\" a Python library designed to address these multifaceted challenges. By streamlining\nprocesses and reducing the steep learning curve associated with contemporary frameworks, it aims to democratize deep learning\nin vision, making it accessible and efficient for both novices and experts. In the subsequent sections, we will delve deeper into\nthe challenges, explore the nuances of \"Deep Fast Vision,\" and elucidate how it stands as a tool in bridging the gap between\ncomplex frameworks and broad user-centric solutions.\nMethods\nAutomated Machine Learning Repository\nAutomated Machine Learning (Auto-ML) simplifies the comprehensive process of implementing machine learning to practical\napplications. In scenarios where specialized knowledge is limited yet, there’s an urgent need to extract insights from data,\nAuto-ML proves invaluable. Regarding transfer learning auto-ml in vision, two prominent libraries emerge AutoKeras13 and\nDeep Fast Vision14.\narXiv:2311.06169v1  [cs.CV]  10 Nov 2023\nAutoKeras\nAutoKeras13 stands out as a freely accessible Auto-ML solution for deep learning, constructed on the foundation of Keras. Its\nprowess lies in its ability to autonomously pinpoint the most fitting deep learning model suited to a dataset. Integrated features\nof this library encompass automatic model identification and hyperparameter optimization. It’s versatile in handling diverse\ndata formats, encompassing images, textual content, and organized data.\nDeep Fast Vision\nDeep Fast Vision12 is a Python library designed to facilitate the rapid prototyping of deep transfer learning models for vision\ntasks. It’s built on the foundation of TensorFlow and Keras, two leading deep learning frameworks. The primary objective of\nDeep Fast Vision is to make the process of model prototyping more accessible and faster for both beginners and experts in the\nfield.\nConfiguration\nThe library allows users to run their computations either on a CPU or GPU, although the use of a GPU is recommended to\nleverage the acceleration benefits it provides for deep learning tasks.\nParameter Overview\nTo streamline the prototyping process, Deep Fast Vision offers a mix of automatic, semi-automatic, and user-defined parameters.\nThe following table 1 provides a summary of these parameters:\nTable 1. Parameters for Deep Fast Vision\nCategory\nParameters\nAutomation Level\nAutomatic\nLoss function, Identify Train-Val and Test folders, Establish output layer size and\nactivation functions, Calculate and apply class weights, Generate appropriate data\ngenerators for train, val, and test data, Resize images to transfer model specification,\nRetrieve transfer architecture’s preprocessing function, Prepare data augmentation,\nPre-train dense layers, Monitor and load optimal weights, Conduct test, Create val-\nidation curves, Produce confusion matrices, Provide model architecture summary\nAutomatic\nSemi-automatic\nDropout between dense layers, Augmentation settings for training data, Unfreeze\nand train transfer model, Load best weights based on validation results, Test after\nloading the best weights\nAdjustable\nUser-defined\nData paths, Transfer learning architecture, Dense layer configuration, Callbacks,\nTransfer model unfreeze blocks\nUser-defined\nInstallation & Documentation\nTo install the library, you can use the pip package manager. Open a terminal and type the following:\n1\npip install deepfastvision\nFor those intending to leverage the older ’tensforflow-gpu’, the following variant should be used:\n1\npip install deepfastvision[gpu]\nThe official documentation for the library provides comprehensive guides and notes to get you started. It covers everything\nfrom basic usage to advanced functionalities.\nYou can access the documentation at:\nhttps://fabprezja.github.io/deep-fast-vision/\nFor specific modules and functions, the API reference section can be particularly helpful. For hands-on practice, the GitHub\nhomepage offers a variety of tutorials and examples.\nOfficial GitHub:\nhttps://github.com/fabprezja/deep-fast-vision\nWhether you are a beginner or an advanced user, the documentation is designed to provide you with all the information\nyou need to make the most of the library. If you encounter any issues or bugs, you can report them on the library’s GitHub\nrepository.\n2/7\nCode Examples\nIn our exploration of the deepfastvision library, we identified two distinct levels of abstraction: low and high. These levels\ndictate how users interact with the library and the degree of control they have over specific processes. Bellow we find results\nrelated to those levels of abstraction.\nHigh Level of Abstraction\n1\nimport wandb\n2\nfrom wandb.keras import WandbCallback\n3\nfrom deepfastvision.core import DeepTransferClassification\n4\n5\n# Initialize wandb\n6\nwandb.init(project=’your_project_name’, entity=’your_username’)\n7\n8\n# Create a DeepTransferClassification object\n9\nexperiment = DeepTransferClassification(paths={’train_val_data’: ’path_to_train_val_data’\n,\n10\n’test_data_folder’: ’path_to_test_data’},\n11\nsaving={’save_weights_folder’:’\npath_to_save_weights’},\n12\nmodel= {’transfer_arch’: ’VGG16’,\n13\n’dense_layers’: [144,89,55],\n14\n’unfreeze_block’: [’cblock5’]},\n15\ntraining={’epochs’: 15,\n16\n’learning_rate’: 0.0001,\n17\n’metrics’: [’accuracy’],\n18\n’callback’: [WanDBCallback()]})\n19\n20\nmodel, results = experiment.run()\nUpon execution, the code returns the trained model, a results dictionary detailing training and evaluation outcomes along\nwith model configurations, validation curves, and a confusion matrix, both tailored to the target type and given labels and metrics.\nThe user supplies data paths from which the library sources and loads training, validation, and testing data. Additionally, the\nlibrary incorporates the VGG1615 pre-trained model for transfer learning, integrates dense layers with neuron counts of 144, 89,\nand 55, and integrates any user-specified callback, such as WanDB16.\nLow Level of Abstraction\n1\nfrom deepfastvision.core import DeepTransferClassification\n2\n3\nexperiment = DeepTransferClassification(paths={\n4\n’train_val_data’: ’path_to_train_val_data’,\n5\n’test_data_folder’: ’path_to_test_data’,\n6\n’external_test_data_folder’: ’path_to_external_test_data’,\n7\n},\n8\nmodel={\n9\n’image_size’: (224, 224),\n10\n’transfer_arch’: ’VGG19’,\n11\n’pre_trained’: ’imagenet’,\n12\n’before_dense’: ’Flatten’,\n13\n’dense_layers’: [610, 377, 233, 144, 89, 55],\n14\n’dense_activations’: ’elu’,\n15\n’initializer’: ’he_normal’,\n16\n’batch_norm’: True,\n17\n’regularization’: ’Dropout+L2’,\n18\n’l2_strength’: 0.001,\n19\n’dropout_rate’: 0.35,\n20\n’unfreeze_block’: [’cblock1’, ’cblock2’, ’cblock5’],\n21\n’freeze_up_to’: ’flatten’,\n22\n},\n23\ntraining={\n3/7\n24\n’epochs’: 9,\n25\n’batch_size’: 32,\n26\n’learning_rate’: 2e-5,\n27\n’optimizer_name’: ’Adam’,\n28\n’add_optimizer_params’: {’clipnorm’: 0.8},\n29\n’class_weights’: True,\n30\n’metrics’: [’accuracy’, ’recall’, ’precision’],\n31\n’augmentation’: ’custom’,\n32\n’custom_augmentation’:[user_function]\n33\n’callback’: [WandbCallback(), learning_rate_schedule],\n34\n’early_stop’: 0.20,\n35\n’warm_pretrain_dense’: True,\n36\n’warm_pretrain_epochs’: 9,\n37\n},\n38\nevaluation={\n39\n’auto_mode’: True,\n40\n},\n41\nsaving={\n42\n’save_weights’: True,\n43\n’save_weights_folder’: ’path_to_save_weights’,\n44\n’save_best_weights’: ’val_loss’,\n45\n},\n46\nmisc={\n47\n’show_summary’: True,\n48\n’plot_curves’: True,\n49\n’show_min_max_plot’: True,\n50\n’plot_conf’: True,\n51\n})\n52\n53\nmodel, results = experiment.run()\nIn this detailed low-level of abstraction configuration, users are granted enhanced control over the model’s architecture and\ntraining process. This configuration encompasses a blend of user-specified modifications and the default settings. Contrasting\nwith the medium abstraction level, the user has introduced a ’external_test_data_folder’ for supplementary test\ndata, enabled batch normalization, added an extensive set of dense layers (610, 377, 233, 144, 89, 55), and set regularization to\n’Dropout+L2’ with an L2 strength of 0.001. Further, the user has decided which blocks to unfreeze, namely ’block1’,\n’block2’, and ’block5’, while freezing layers up to the ’flatten’ phase. The augmentation is customized by the\nuser, and additional callbacks, WandbCallback and learning_rate_schedule, have been incorporated. There’s also an\nactivation of early stopping with a threshold set at 0.20 relative to the total epochs, an increase in epochs to 25, and a warm\npretraining for dense layers spanning 9 epochs.\nBy default, the weight initializer is ’he_normal’17, class weights are active, and the image size is defined at (224, 224).\nThe pre-trained weights are sourced from ’imagenet’18, the preceding layer before dense layers is ’Flatten’, and the\ndense layer activations utilize ’elu’19. The learning rate stands at 2e-5, with the evaluation configuration’s ’auto_mode’ set\nto true, implying automatic evaluation of the best weights. The model saves its most optimal weights based on ’val_loss’,\nand several other features, such as displaying the model summary, plotting various curves, showing a min-max plot, and\nrepresenting data through a confusion matrix, are also activated. As observed in prior examples, all automation processes\nremain in effect.\nModel Prediction\n1\npredictions = experiment.model_predict(’folder_path’)\nThe model_predict method uses the trained model to predict all images in a given folder. The method returns image, path,\npredicted label, confidence, and variance for each image in the folder. It can be sorted by variance (across labels) for identifying\nconfusing instances or by metric (e.g., accuracy).\nExport Results and Model\n1\nexperiment.export_all(results, base_path=’folder_path_to_results’, export_model=True,\nadditive=True)\n4/7\nThe export_all method exports all results, best weights, and the trained model into a folder. With additive=True, the\nuser may iterate the experiment and obtain results in new randomly named folders.\nExtract Features\n1\nX_train, y_train, X_val, y_val, X_test, y_test, X_test_external, y_test_external =\nexperiment.model_feature_extract(layer_index=None, layer_name=’my_layer’)\nThe model_feature_extract method can be used to extract features from any layer in the model while respecting the\nused train, val, test(s) indices.\nRun Example\nIn Figure 1, a standard example of a run is depicted, showcasing the plots and test results that are generated automatically.\nFigure 1. An example of executing Deep Fast Vision on an image dataset.\nDiscussion\nThe advent of deep learning has undeniably transformed the landscape of various industries, with vision-based applications\nstanding at the forefront of this transformation. As the Introduction section highlighted, while the potential of deep learning in\nvisual tasks is immense, several challenges persist. From the complexity of current frameworks to the scarcity of extensive\nlabeled datasets, these challenges can pose significant roadblocks for many researchers and developers.Transfer learning, with\nits promise to leverage knowledge from larger datasets to improve performance on smaller datasets, has been viewed as a\npotential solution to some of these challenges.\nThe \"Deep Fast Vision\" library is currently utilized in research studies9,20–22. At its core, the library seeks to simplify the\nintricacies of deep learning frameworks, making it more accessible to a broader audience. Its strength lies in its ability to\nstreamline the prototyping process, as seen in the Methods section, where both high and low levels of abstraction were explored.\nBy offering a blend of automatic, semi-automatic, and user-defined parameters, the library provides users with the flexibility\nto tailor their approach based on their expertise and requirements. Comparing \"Deep Fast Vision\" with AutoKeras, another\nprominent Auto-ML solution, highlights some of the unique offerings of the former. While AutoKeras excels in identifying\nsuitable deep learning models and optimizing hyperparameters, \"Deep Fast Vision\" distinguishes itself with its focus on transfer\nlearning for vision tasks. Its foundation on TensorFlow and Keras ensures that users benefit from the robustness of these leading\nframeworks while enjoying a more streamlined experience tailored for rapid prototyping.\nThe installation and documentation details underscore the library’s commitment to user-friendliness. From easy installation\ncommands to comprehensive documentation and a dedicated GitHub repository, \"Deep Fast Vision\" ensures that users have all\n5/7\nthe resources they need to navigate the library effectively. The Code Examples section further exemplifies the versatility of\nthe library. Whether a user prefers a high level of abstraction, where many of the processes are automated, or a low level of\nabstraction, which offers more granular control, \"Deep Fast Vision\" caters to both preferences.\nHowever, like all tools, \"Deep Fast Vision\" is not without its limitations. While its focus on transfer learning for vision tasks\nis key, there might be scenarios where specialized solutions are required. Additionally, the library’s dependency on TensorFlow\nand Keras could pose challenges for those who prefer other deep learning frameworks.\nReferences\n1. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. nature 521, 436–444 (2015).\n2. Kather, J. N. et al. Predicting survival from colorectal cancer histology slides using deep learning: A retrospective\nmulticenter study. PLoS medicine 16, e1002730 (2019).\n3. Bychkov, D. et al. Deep learning based tissue analysis predicts outcome in colorectal cancer. Sci. reports 8, 1–11 (2018).\n4. Skrede, O.-J. et al. Deep learning for prediction of colorectal cancer outcome: a discovery and validation study. The\nLancet 395, 350–360 (2020).\n5. Calimeri, F., Marzullo, A., Stamile, C. & Terracina, G. Biomedical data augmentation using generative adversarial neural\nnetworks. In International conference on artificial neural networks, 626–634 (Springer, 2017).\n6. Prezja, F., Paloneva, J., Pölönen, I., Niinimäki, E. & Äyrämö, S. Deepfake knee osteoarthritis x-rays from generative\nadversarial neural networks deceive medical experts and offer augmentation potential to automatic classification. Sci.\nReports 12, 18573 (2022).\n7. Prezja, F., Pölönen, I., Äyrämö, S., Ruusuvuori, P. & Kuopio, T. H&e multi-laboratory staining variance exploration with\nmachine learning. Appl. Sci. 12, 7511 (2022).\n8. Prezja, F. et al. Improved accuracy in colorectal cancer tissue decomposition through refinement of established deep\nlearning solutions. Sci. Reports 13, 15879 (2023).\n9. Prezja, F. et al. Improving performance in colorectal cancer histology decomposition using deep and ensemble machine\nlearning. arXiv preprint arXiv:2310.16954 (2023).\n10. Huang, J., Chai, J. & Cho, S. Deep learning in finance and banking: A literature review and classification. Front. Bus. Res.\nChina 14, 1–24 (2020).\n11. Zhuang, F. et al. A comprehensive survey on transfer learning. Proc. IEEE 109, 43–76 (2020).\n12. Prezja, F. Deep fast vision: Accelerated deep transfer learning vision prototyping and beyond. https://github.com/fabprezja/\ndeep-fast-vision, DOI: 10.5281/zenodo.7865289 (2023).\n13. Jin, H., Chollet, F., Song, Q. & Hu, X. AutoKeras: An AutoML Library for Deep Learning. J. Mach. Learn. Res. 24, 1–6\n(2023).\n14. Prezja, F. Deep fast vision: Accelerated deep transfer learning vision prototyping and beyond. https://github.com/fabprezja/\ndeep-fast-vision, DOI: 10.5281/zenodo.7865289 (2023).\n15. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint\narXiv:1409.1556 (2014).\n16. Biewald, L. Experiment tracking with weights and biases (2020). Software available from wandb.com.\n17. He, K., Zhang, X., Ren, S. & Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet\nclassification. In Proceedings of the IEEE international conference on computer vision, 1026–1034 (2015).\n18. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255 (Ieee, 2009).\n19. Clevert, D.-A., Unterthiner, T. & Hochreiter, S. Fast and accurate deep network learning by exponential linear units (elus).\narXiv preprint arXiv:1511.07289 (2015).\n20. Prezja, F., Annala, L., Kiiskinen, S., Lahtinen, S. & Ojala, T. Synthesizing bidirectional temporal states of knee osteoarthritis\nradiographs with cycle-consistent generative adversarial neural networks. Res. preprint DOI: 10.13140/RG.2.2.27400.19201\n(2023). Available online at ResearchGate.\n21. Prezja, F., Annala, L. A., Kiiskinen, S. & Ojala, T. Exploring the efficacy of base data augmentation methods in deep\nlearning-based radiograph classification of knee joint osteoarthritis. Res. preprint DOI: 10.13140/RG.2.2.34111.07847\n(2023). Available online at ResearchGate.\n6/7\n22. Prezja, F., Annala, L., Kiiskinen, S., Lahtinen, S. & Ojala, T. Adaptive variance thresholding: A novel approach to improve\nexisting deep transfer vision models and advance automatic knee-joint osteoarthritis classification. Res. preprint DOI:\n10.13140/RG.2.2.10736.02566 (2023). Available online at ResearchGate.\nAcknowledgements\nThe authors extend their sincere gratitude to Leevi Annala, Rodion Enkel, Sampsa Kiiskinen, Suvi Lahtinen, Leevi Lind, and\nKimmo Riihiaho.\nData Availability\nAll code materials are available at the official GitHub repository.\nAuthor contributions statement\nConceptualization: F. P.; Methodology: F. P.; Data Curation: F. P.; Writing – review & editing: F. P.;\nAdditional information\nCompeting interests All authors declare that they have no conflicts of interest.\n7/7\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2023-11-10",
  "updated": "2023-11-10"
}