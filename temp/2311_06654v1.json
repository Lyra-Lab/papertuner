{
  "id": "http://arxiv.org/abs/2311.06654v1",
  "title": "Unsupervised and semi-supervised co-salient object detection via segmentation frequency statistics",
  "authors": [
    "Souradeep Chakraborty",
    "Shujon Naha",
    "Muhammet Bastan",
    "Amit Kumar K C",
    "Dimitris Samaras"
  ],
  "abstract": "In this paper, we address the detection of co-occurring salient objects\n(CoSOD) in an image group using frequency statistics in an unsupervised manner,\nwhich further enable us to develop a semi-supervised method. While previous\nworks have mostly focused on fully supervised CoSOD, less attention has been\nallocated to detecting co-salient objects when limited segmentation annotations\nare available for training. Our simple yet effective unsupervised method\nUS-CoSOD combines the object co-occurrence frequency statistics of unsupervised\nsingle-image semantic segmentations with salient foreground detections using\nself-supervised feature learning. For the first time, we show that a large\nunlabeled dataset e.g. ImageNet-1k can be effectively leveraged to\nsignificantly improve unsupervised CoSOD performance. Our unsupervised model is\na great pre-training initialization for our semi-supervised model SS-CoSOD,\nespecially when very limited labeled data is available for training. To avoid\npropagating erroneous signals from predictions on unlabeled data, we propose a\nconfidence estimation module to guide our semi-supervised training. Extensive\nexperiments on three CoSOD benchmark datasets show that both of our\nunsupervised and semi-supervised models outperform the corresponding\nstate-of-the-art models by a significant margin (e.g., on the Cosal2015\ndataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised\nco-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over\na SOTA semi-supervised CoSOD model).",
  "text": "Unsupervised and semi-supervised co-salient object detection via segmentation\nfrequency statistics\nSouradeep Chakraborty1,2†\nShujon Naha2,3\nMuhammet Bastan2\nAmit Kumar K C2\nDimitris Samaras1\n1 Stony Brook University\n2 Visual Search & AR, Amazon\n3 Indiana University\n{souchakrabor,samaras}@cs.stonybrook.edu, {mbastan, amitkrkc}@amazon.com, snaha@iu.edu\nAbstract\nIn this paper, we address the detection of co-occurring\nsalient objects (CoSOD) in an image group using frequency\nstatistics in an unsupervised manner, which further enable\nus to develop a semi-supervised method. While previous\nworks have mostly focused on fully supervised CoSOD,\nless attention has been allocated to detecting co-salient\nobjects when limited segmentation annotations are avail-\nable for training.\nOur simple yet effective unsupervised\nmethod US-CoSOD combines the object co-occurrence fre-\nquency statistics of unsupervised single-image semantic\nsegmentations with salient foreground detections using self-\nsupervised feature learning. For the first time, we show\nthat a large unlabeled dataset e.g.\nImageNet-1k can be\neffectively leveraged to significantly improve unsupervised\nCoSOD performance. Our unsupervised model is a great\npre-training initialization for our semi-supervised model\nSS-CoSOD, especially when very limited labeled data is\navailable for training.\nTo avoid propagating erroneous\nsignals from predictions on unlabeled data, we propose a\nconfidence estimation module to guide our semi-supervised\ntraining. Extensive experiments on three CoSOD bench-\nmark datasets show that both of our unsupervised and semi-\nsupervised models outperform the corresponding state-\nof-the-art models by a significant margin (e.g., on the\nCosal2015 dataset, our US-CoSOD model has an 8.8% F-\nmeasure gain over a SOTA unsupervised co-segmentation\nmodel and our SS-CoSOD model has an 11.81% F-measure\ngain over a SOTA semi-supervised CoSOD model).\n1. Introduction\nCo-salient object detection (CoSOD) focuses on detect-\ning co-existing salient objects in an image group, whereas\nsalient object detection (SOD) detects the same salient ob-\njects in single images [6, 36, 37, 43, 47, 52, 63]. CoSOD\nleverages the extra knowledge that the group images share\na common object by finding semantic similarities across the\n†Part of this work was done during an internship at Amazon.\nOriginal\nDCFM (Fully\nsupervised –\nall labels)\nOurs: US-CoSOD\n(Unsupervised)\nOurs: SS-CoSOD\n(Unsupervised +\nSemi-supervised-\n¼ labeled data)\nFigure 1.\nVisualization of co-saliency detections on an image\ngroup train from the Cosal2015 dataset [65]. Row 1: Original\nimage, Row 2: DCFM [62] predictions (trained using all labeled\ndata), Row 3: Our unsupervised model (US-CoSOD), Row 4:\nOur semi-supervised model (SS-CoSOD) trained using 1/4 labeled\ndata with unsupervised pre-training, which has comparable perfor-\nmance to the fully supervised DCFM trained with all labels.\nimage regions in the group. Thus CoSOD models can local-\nize the salient objects more accurately compared to the sin-\ngle image based SOD models [13,16] in such image groups.\nBoth SOD and CoSOD are joint segmentation and detection\ntasks as shown in the existing literature [13,15,62] and thus\nrequire segmentation labels. However, collecting segmen-\ntation annotations is time-consuming as well as expensive.\nThis annotation requirement is a drawback for a major-\nity of the existing CoSOD models [13, 15, 62, 67, 71] that\nare fully supervised. To relieve the labeling burden, some\nworks [21,22,39] focused on unsupervised co-segmentation\nand co-saliency detection.\nSemi-supervised learning in\nCoSOD [72] aims to learn an effective model from a train-\ning dataset using only a small set of labeled images along\nwith a larger unlabeled set. Such models have an immense\nvalue in several real-world industrial applications such as\nin e-commerce (e.g. automatic product detection from cus-\ntomer review and query images of the product without the\nneed for manual product annotations), content-based im-\nage retrieval, satellite imaging, bio-medical imaging, etc.\narXiv:2311.06654v1  [cs.CV]  11 Nov 2023\nHowever, the prediction performance of these models is sig-\nnificantly worse compared to the existing fully supervised\nmodels due to their inefficient use of the unlabeled data. In\nthis paper, we first solve unsupervised CoSOD using a large\nunlabeled dataset and next use the unsupervised model as a\npre-training initialization for our semi-supervised pipeline.\nIn this work, we take advantage of the recent progress in\nself-supervised semantic segmentation [18] as well as self-\nsupervised self-attention [3] to develop a simple yet effec-\ntive unsupervised algorithm for CoSOD (US-CoSOD). As\npart of our unsupervised approach, we first obtain the seg-\nmentation masks of the co-occurring objects in an image\ngroup using STEGO, an off-the-shelf self-supervised se-\nmantic segmentation model [18]. Next, we select the most\ncommon and salient segmentation mask (with guidance\nfrom the self-attention maps obtained from DINO [3], a\nself-supervised feature learning method) as the pseudo seg-\nmentation label for training an off-the-shelf CoSOD model\nin a supervised manner. We show a significant improve-\nment in prediction performance using our methods. How-\never, standard training datasets are relatively small. In our\npaper, we introduce a more up-to-date evaluation task for\nunsupervised CoSOD on a set of 150K unlabeled images\nfrom the ImageNet-1k dataset [10,31] (which only contains\nclass labels without any segmentation annotation).\nNext, we show that our unsupervised model forms a\nstrong pre-training initialization for a CoSOD model trained\nin a semi-supervised manner.\nFor this, we propose a\nconfidence aware student-teacher architecture based semi-\nsupervised model, SS-CoSOD. Here, we leverage the fact\nthat in an input image group for CoSOD, we can mix the la-\nbeled and unlabeled images to effectively propagate knowl-\nedge from the labeled images to the unlabeled images in\nthe image group via cross-region correspondences. We also\nintroduce a confidence estimation module to block erro-\nneous knowledge flow from inaccurate predictions on diffi-\ncult unlabeled images. Similar to US-CoSOD, we leverage\nthe large unlabeled ImageNet-1k [10,31] dataset to signifi-\ncantly improve semi-supervised CoSOD performance.\nIn Fig. 1, we compare our unsupervised and semi-\nsupervised models with DCFM [62], a state-of-the-art fully\nsupervised CoSOD model. Our US-CoSOD produces seg-\nmentations comparable with DCFM and our SS-CoSOD\nmodel further improves the segmentation predictions. Our\nmain contributions are:\n• We propose a simple yet effective unsupervised ap-\nproach for CoSOD that effectively leverages single-\nimage semantic segmentations and self-attention maps\ngenerated using self-supervision to generate pseudo-\nlabels for supervised training of a CoSOD model.\n• For the first time, we show that CoSOD can be sig-\nnificantly improved using large unlabeled datasets,\ne.g. ImageNet-1k [31]. This approach helps us achieve\nstate-of-the-art results for unsupervised CoSOD.\n• We propose a novel approach for semi-supervised\nCoSOD by effectively propagating knowledge from\na limited labeled set to a much larger unlabeled set\nvia confidence estimation and cross-region correspon-\ndence between the labeled and the unlabeled sets.\n2. Related Work\nCo-salient object detection: Graphical models are used\nto model pixel relationships in an image group [23, 26–28,\n60, 68], followed by mining co-salient objects with consis-\ntent features. Some works used additional object saliency\ninformation to first mine the salient objects and then imple-\nment CoSOD [29,69,70]. Other works compute the shared\nattributes among input images [13,15,17,34,38,51,67,71,\n73] and supplement semantic information with classifica-\ntion information. The surveys [9, 14, 64] provide more in-\nformation on CoSOD. DCFM [62] mines co-salient features\nwith democracy while reducing background interference.\nWe use DCFM as the backbone network in our study.\nUnsupervised segmentation: Several unsupervised se-\nmantic segmentation approaches use self-supervised feature\nlearning techniques [8, 25, 40, 54, 56]. Recently, STEGO\n[18] showed that semantically correlated dense features\nfrom unsupervised feature learning frameworks can help\ndistill unsupervised features into high-quality semantic la-\nbels. We use this model as a component in our unsupervised\npipeline. In [61], the semantic categories obtained using\nself-supervised learning are mapped to pixel-level features\nvia class activation maps, which serve as pseudo labels for\ntraining. Some papers solve unsupervised co-segmentation\n[2,4,21,24,39] and CoSOD [22,65]. Li et al. [39] proposed\nan unsupervised co-segmentation model by ranking image\ncomplexities using saliency maps. Hsu et al. [21] devel-\noped an unsupervised co-attention based model for object\nco-segmentation. The same authors presented an unsuper-\nvised graphical model for CoSOD [22] that jointly solves\nsingle-image saliency and object co-occurrence. Our US-\nCoSOD outperforms all of these unsupervised models.\nSemi-supervised segmentation:\nWhile consistency-\nbased methods enforce the predictions of unlabeled sam-\nples to be consistent under different perturbations [5,33,46],\npseudo-label based methods [30,32,35,42,45,58], incorpo-\nrate unlabeled data into training with high-quality pseudo\nlabels. Some of these methods [30, 32, 45, 58] apply pixel-\nlevel error correction mechanisms on the generated pseudo-\nlabels (e.g. using an auxillary decoder or employing a flaw\ndetector or a discriminator) in order to avoid propagating\nlabel noise. Instead, we directly estimate the prediction er-\nror probability on an unlabeled image at the global level us-\ning a confidence estimation module, trained to estimate pre-\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n1 →\n34               0.06 \n2 →\n32               0.62\n3 →\n26               0.15\n.\n.\n→\n.\n.\n.\n.\n.\n.\nTraining\nSelf-supervised\nsemantic segmentation\nSelf-supervised\nself-attention\nCo-occurrence frequency\nbased segmentation\nmask ranking\nSelected\nInput images\nPredicted co-salient objects\nInput images, I\nCo-salient object\ndetection model\nInference\nSegmentation masks, SM\nSelf-attention maps, SA\nSupervised training\nSA map\nMask Id  Co-oc. freq.    overlap\nPseudo ground truth masks, CM\nFigure 2. The proposed unsupervised CoSOD model, US-CoSOD. We first obtain unsupervised semantic segmentation maps from STEGO\n[18] and self-attention maps from DINO [3]. The unlabeled categories (from STEGO) in the image group are sorted based on their co-\noccurrence frequencies and the final category is chosen based on the overlap between the STEGO and DINO masks per image. The\nsegmentation mask of the selected category is considered the pseudo ground truth for training an off-the-shelf supervised CoSOD model.\ndiction confidence (in terms of the segmentation accuracy)\nbased on the labeled set to block error propagation. Wang et\nal. [59] proposed the first semi-supervised co-segmentation\nmodel by optimizing an energy function consisting of inter-\nand intra-image distances for an image group. While Zheng\net al. [72] proposed the first semi-supervised CoSOD frame-\nwork based on graph structure optimization, their model ac-\ncuracy is low due to the use of hand-crafted features and\ntheir model has not been evaluated under sufficiently low\nlabeled data. Some semi-supervised SOD models have also\nbeen proposed [42, 44, 66]. SAL [44] used active learning\nto gradually expand a small labeled set to include samples\non which predictions are inaccurate. GWSCoSal [48] intro-\nduced a weakly supervised learning induced CoSOD model\nusing group class activation maps.\nExisting unsupervised and semi-supervised approaches\nsuffer from limited performance because they use: (1) hand-\ncrafted features, and (2) smaller unlabeled datasets. Our\nstudy fills this gap by introducing an unsupervised and a\nsemi-supervised CoSOD method, both of which uses large-\nscale unlabeled data to significantly improve performance.\n3. Methodology\nGiven a group of N images I = {I1, I2, ..., In} con-\ntaining co-occurring salient objects of a certain class,\nCoSOD aims to detect them simultaneously and output\ntheir co-salient object segmentation masks. For unsuper-\nvised CoSOD, the goal is to predict the co-salient segmen-\ntations {ˆyi}n\ni=1 without using any labeled data. For semi-\nsupervised CoSOD, given a labeled set Dl = {(xl\ni, yl\ni)}Nl\ni=1\nand a much larger unlabeled set Du = {(xu\ni )}Nu\ni=1, we aim\nto train a CoSOD model by efficiently utilizing both the lim-\nited labeled data and a large amount of unlabeled data.\n3.1. Unsupervised co-salient object detection\nHere, we describe our unsupervised CoSOD model (US-\nCoSOD) that effectively leverages the frequency statistics\nof self-supervised single-image semantic segmentations.\nFig. 2 depicts our unsupervised pipeline for CoSOD.\nWe first compute the pseudo co-saliency masks based on\nthe single image segmentation masks and the self-attention\nmasks, which are then used to train a fully supervised\nCoSOD model. Trained with the self-distillation loss [20],\nthe attention maps associated with the class token from the\nlast layer of DINO [3] have been shown to highlight salient\nforeground regions [3, 57, 61]. Motivated by this observa-\ntion, we consider the averaged attention map (across all at-\ntention heads) from DINO as the foreground object segmen-\ntation. Also, to detect the co-occurring objects, we use the\nsemantic segmentation masks from a recent self-supervised\nsingle-image semantic segmentation model, STEGO [18].\nThis model shows that feature correspondences across im-\nages form strong signals for unsupervised semantic segmen-\ntation. These correspondences are used to create pixel-wise\nAlgorithm 1 Pseudo co-saliency mask generation\nInput:\nImage group I = {I1, I2, .., In}\nOutput: CoSOD masks CM = {CM1, CM2, .., CMn}\n1: Obtain self-attention (SA) maps, SA = {SAi}n\ni=1\nfrom DINO [3].\n2: Apply Otsu thresholding on the SA maps to obtain bi-\nnary segmentation maps, DM = {DMi}n\ni=1.\n3: Obtain the unsupervised single-image semantic seg-\nmentation maps, SM c\ni for each image i and discovered\nunlabeled category c from STEGO [18].\n4: Compute the frequency, f c of each semantic unlabeled\ncategory c from STEGO in the image group, I.\n5: for i = 1 to n do\n6:\nCi = {c1, c2, ..., cm} is the set of discovered unla-\nbeled categories in the image Ii.\n7:\nSort the categories in Ci by their frequency f c in the\ndescending order and select the top-K frequent unla-\nbeled categories, U = {u1, u2, .., uK}.\n8:\nfor j = 1 to K do\n9:\nFor category uj, compute overlap score:\nOuj\ni\n= Ar(SM uj\ni ∩DMi)/Ar(Ii), the overlapped\narea between the STEGO mask SM uj\ni\nfor cate-\ngory uj and the DINO SA map DMi divided by\nthe total image area.\n10:\nend for\nCo-salient object mask, CMi = SM ccoso\ni\ni\nis the\nSTEGO mask of the class, ccoso\ni\nthat maximizes the\noverlap score Oi i.e. ccoso\ni\n= arg maxc Oc\ni\n11: end for\n12: return CM\nembeddings, which yield high quality semantic segmenta-\ntion maps upon clustering. We consider these co-occurring\nsemantic clusters across the image group as unlabeled cate-\ngories and leverage them to find the co-occurring objects.\nWe detail our unsupervised pseudo co-saliency mask\ngeneration in Algorithm 1.\nFirst, we average the self-\nattention maps from the nh DINO attention heads to ob-\ntain the averaged self-attention map SAi for an image Ii\nas: SAi =\n1\nnh\nPnh\nj=1 AM j\ni , where AM j\ni is the attention\nmap from the DINO attention head j for the image Ii. Map\nSAi is normalized by min-max normalization. We then find\nthe co-occurrence frequency f c of the discovered categories\nacross all images in the group I. Next, for each image Ii, we\ncompute the top-K frequent STEGO categories and finally\nselect a single unlabeled category ccoso\ni\nper image based on\nthe overlap score Oc\ni between the STEGO and the DINO\nmasks. We then consider the STEGO mask SM ccoso\ni\ni\ncor-\nresponding to the category ccoso\ni\nas the pseudo ground truth\nmask for Ii. This filtering step ensures that the selected\nsegmentation corresponds to the most common yet salient\nobject in the group, therefore preventing co-occurring back-\ngrounds from being considered as the pseudo masks.\nThus we obtain the pseudo co-salient object masks\nCMtrain for all groups Itrain in our training set and train\na CoSOD model [62] in a supervised manner using Itrain\nas input and the corresponding pseudo segmentation masks\nCMtrain as the training labels. The training loss, Lunsp is:\n  \\ma t\nh\ncal\n {L\n}\n^{u\nnsp} =  \\frac { 1}{ |B|}  \\sum _{i=1}^{|B|} l_{iou}(f^{unsp}(x_i,\\theta ),CM_i) + \\lambda _{sc} l_{sc} \n(1)\nwhere, liou is the IoU loss [49, 70] between the predicted\nsegmentation map, f unsp(xi, θ) and the ground truth seg-\nmentation CMi, B is the training batch, and xi is the input\nimage. lsc is the self-contrastive loss as outlined in [62].\nAt inference, we use the trained f unsp model to detect\nco-salient objects in the test image groups, Itest. Note that\nthe self-supervised component models (STEGO and DINO)\nare only used during the training of our US-CoSOD model\n(to generate the pseudo-labels) and not during inference.\n3.2. Semi-supervised co-salient object detection\nWhile the unsupervised model does not require any la-\nbeled data for training, such a model is often not at par with\nsupervised models in terms of prediction accuracy. There-\nfore, we develop a semi-supervised approach for CoSOD\n(SS-CoSOD), which can effectively leverage a large amount\nof unlabeled data with effective prediction confidence esti-\nmation. Fig. 3 depicts our semi-supervised pipeline.\nIn stage 1, we supervisedly pre-train a CoSOD model\nfP T on the labeled set and then train our Confidence Es-\ntimation Network (CEN) on the same set. In stage 2, we\nemploy a typical self-training framework with two models\nof the same architecture, namely student (model fS) and\nteacher (model fT ) networks that are initialized as fP T\nin stage 1.\nThe student model’s weights θs are updated\nvia backpropagation while the teacher model’s weights θt\nare updated with the exponential moving average (EMA)\nscheme [53], i.e., θt = λdθt−1+(1−λd)θs, where λd is the\nEMA decay factor (set to 0.95). At each training step, we\nsample Bl labeled images and Bu unlabeled images (maxi-\nmum value of |Bl| and |Bu| being 16, following [62]). Next,\nwe combine each batch of labeled images, Bl and unlabeled\nimages, Bu into a single volume Bl+u before passing the\ncombined volume through the student network. The train-\ning label for this volume is the combination of the ground\ntruth labeled mask with the teacher model prediction as:\ny(l+u) = [yl, fT (xu\nj , θT )]. This step is different from other\nsemi-supervised approaches, e.g., U2PL [58] where the la-\nbeled and the unlabeled sets are passed through the student\nmodel in two separate passes. We leverage the benefit of\nlearning cross-pixel similarities across all images in the im-\nLabeled images\nUnlabeled images\nGround truth\nPrediction\nLoss = L (GT, Pred.)\nStudent (pre-trained)\nTeacher (pre-trained)\nEMA weight\nupdate\nConfidence Estimation \nNetwork\n.\n0.91\n0.75\n0.45\n.\n.\nGround truth\nPrediction \n(labeled)\nSup. \nLoss\nUnsup. Loss  x  Conf. \nPrediction\n(unlabeled)\nStage 1: Pre-training on labeled data\nStage 2: Semi-supervised \ntraining\nJoint \ntraining\nPrediction\n(unlabeled)\nCo-salient object \ndetection model\nCo-salient object \ndetection model\nCo-salient object \ndetection model\nConfidence Estimation \nNetwork\nFigure 3. Proposed SS-CoSOD model for semi-supervised co-salient object detection. In the first stage, we pre-train a CoSOD model on\nthe labeled set while also training our Confidence Estimation Network (CEN) on the same set. In stage 2, we employ a student-teacher\nmodel (initialized with the model from stage 1) for semi-supervised learning. The labeled and unlabeled data are jointly passed through\nthe student during training. We weight the unsupervised loss by the confidence score estimated by the CEN module.\nage group to effectively propagate object co-saliency infor-\nmation from the labeled images to the unlabeled images.\nA supervised loss, Ls is computed for the labeled set\nand an unsupervised loss, Lu is computed for the unlabeled\nsamples. For every labeled image, our goal is to minimize\nthe supervised IoU loss as:\n  \\\nm\nathc\na\nl {\nL}_{\ns} = \\\nfr\na c {\n1}{|B_ l\n|} \\s\num  _\n{ \\s ubst ac\nk { (x^l_i,y^l_i)\\in B_l,\\\\x^u_j\\in B_u}} l_{iou}(f_S^l([x^l_i,x^u_j],\\theta _S),y_i^l) + \\lambda _{sc} l_{sc} \n(2)\nwhere liou is IoU loss and lsc is self-contrastive (SC) loss.\nλsc is set as 0.1 (following [62]).\nThe SC Loss, lsc is\ncomputed as: lsc = −log(cosc + ϵ) −log(1 −cosb +ϵ)\nwith cosc\n=\ncos(proto(l+u), proto(l+u)\nc\n) and cosb\n=\ncos(proto(l+u), proto(l+u)\nb\n), where proto is the prototype\ngenerated by the original inputs, protoc is the co-salient\nprototype generated by the foreground regions, and protob\nis the background prototype generated from the background\nregions in the image. cos is the cosine-similarity function\nand ϵ is a small positive constant to avoid overflow.\nWe pass the unlabeled batch, Bu, through the teacher\nnetwork and compute the unsupervised loss between the\npredictions of the teacher and the student networks. The\nunsupervised loss is weighted using the confidence scores\npredicted by the CEN module for each unlabeled image as:\n  \\\nl\nabel\n \n{e\nq:con\nf_\ne q} \n\\math\nc a l \n{L}_{u} =\n \\fra\nc {1\n} {| B_u| } \\su\nm  _{ \\substack {x^l_i\\in B_l, \\\\ x^u_j\\in B_u}} \\tilde {g}({x^u_i},\\theta _C^l) l_{iou}(f_S^u([x^l_i,x^u_j],\\theta _S),f_T(x^u_j,\\theta _T)) \n(3)\nwhere ˜g(xu\nj , θl\nC) =\ng(xu\nj ,θl\nC)\nP|Bu|\nj=1 g(xu\nj ,θl\nC) is the confidence weight\nestimated by CEN, g for the unlabeled sample xu\ni , parame-\nterized by θl\nC that is learned from the labeled batch Bl. We\nobserved that the normalized confidence weight is crucial\nfor model convergence. Our objective is to minimize the\noverall loss, L = Ls + λuLu, where Ls and Lu represent\nsupervised loss on the labeled set and unsupervised loss on\nthe unlabeled set respectively. λu is set as 1 [62].\nConfidence Estimation Network (CEN): The CEN model\nis trained to estimate the reliability score of the model pre-\ndiction.\nTo train this model, we use the labeled image\nset Sl as the input and the corresponding segmentation F-\nmeasure [1] scores of the pre-trained model fP T predic-\ntions in stage 1 as the ground truth. We use a ResNet50\nbackbone [19] trained using the DINO method because of\nits ability to well segment the discriminative image regions.\nThe model is trained by fine-tuning the pre-trained ResNet\nbackbone and an fc(2048, 1) layer using the MSE loss as:\n  \\\nm\nathc\na\nl {\nL}_{\nC} = \n\\frac {\n1} {|\nS_l|} \\s u m _\n{\\ subs ta\nck {(x^l_i,y^l_i)\\in S_l}}(CEN(x^l_i,\\theta ^l_C)-F_\\beta (f_{PT}(x^l_i,\\theta _S),y^l_i))^2 \n(4)\nTable 1. Performance comparison of unsupervised models for CoSOD. Our US-CoSOD-ImgNet150 model achieves the best result.\nCoCA\nCosal2015\nCoSOD3k\nMethod\nMAE↓\nF max\nβ\n↑\nEmax\nϕ\n↑\nSα ↑\nMAE↓\nF max\nβ\n↑\nEmax\nϕ\n↑\nSα ↑\nMAE↓\nF max\nβ\n↑\nEmax\nϕ\n↑\nSα ↑\nDINO (DI) [3] (ICCV 2021)\n0.214\n0.372\n0.572\n0.540\n0.154\n0.659\n0.753\n0.688\n0.146\n0.624\n0.749\n0.679\nSTEGO (ST) [18] (ICLR 2022)\n0.235\n0.353\n0.555\n0.523\n0.164\n0.618\n0.717\n0.676\n0.204\n0.543\n0.660\n0.615\nTokenCut [56] (CVPR 2022)\n0.167\n0.467\n0.704\n0.627\n0.139\n0.805\n0.857\n0.793\n0.151\n0.720\n0.811\n0.744\nDVFDVD [2] (ECCVW 2022)\n0.223\n0.422\n0.592\n0.581\n0.092\n0.777\n0.842\n0.809\n0.104\n0.722\n0.819\n0.773\nSegSwap [50] (CVPRW 2022)\n0.165\n0.422\n0.666\n0.567\n0.178\n0.618\n0.720\n0.632\n0.177\n0.560\n0.705\n0.608\nOurs (DI+ST)\n0.165\n0.461\n0.676\n0.610\n0.112\n0.760\n0.823\n0.767\n0.124\n0.684\n0.793\n0.724\nOurs (US-CoSOD-COCO9213)\n0.140\n0.498\n0.702\n0.641\n0.090\n0.792\n0.852\n0.806\n0.095\n0.735\n0.832\n0.772\nOurs (US-CoSOD-ImgNet150)\n0.116\n0.546\n0.743\n0.672\n0.070\n0.845\n0.886\n0.840\n0.076\n0.779\n0.861\n0.801\nOurs (US-CoSOD-ImgNet450)\n0.127\n0.543\n0.726\n0.666\n0.071\n0.844\n0.884\n0.842\n0.079\n0.775\n0.854\n0.800\nwhere fS is the student model parameterized by θS,\nFβ(p, q) is the Fβ-metric computed between two segmen-\ntation maps p and q, and Sl is the labeled set. After train-\ning, CEN provides a reliability estimate of model prediction\nsolely based on the image contents (Fig. 4 in supplement).\n4. Experimental Results\n4.1. Setup\nDatasets and evaluation metrics: We used labeled data\nfrom COCO9213 [55], a subset of the COCO dataset [41]\nthat contains 9,213 images selected from 65 groups, to train\nour semi-supervised model. We additionally constructed a\ndataset of 150K unlabeled images by selecting 150 images\nper class from the training subset of ImageNet-1K [10, 31]\nto train our unsupervised and semi-supervised models. We\nevaluate our methods on three popular CoSOD benchmarks:\nCoCA [71], Cosal2015 [65] and CoSOD3k [14]. CoCA\nand CoSOD3k are challenging real-world co-saliency eval-\nuation datasets, containing multiple co-salient objects in\nsome images, large appearance and scale variations, and\ncomplex backgrounds. Cosal2015 is a widely used dataset\nfor CoSOD evaluation. Our evaluation metrics include the\nMean Absolute Error (MAE↓) [7], maximum F-measure\n(F max\nβ\n↑) [1], maximum E-measure (Emax\nϕ\n↑) [12], and\nS-measure (Sα ↑) [11].\nImplementation details: We used DCFM [62] as the back-\nbone model for all experiments. While the student network\nin SS-CoSOD consists of the Democratic Prototype Gen-\neration Module (DPG) and the Self-Contrastive Learning\nModule (SCL) from DCFM, we removed SCL in the teacher\nnetwork because this network is not updated via backprop-\nagation [62]. We used the Adam optimizer for training. The\ntotal training time is 5 hours for US-CoSOD and 8 hours for\nSS-CoSOD using ImageNet-1K. The inference time is 84.4\nfps. More details in the supplementary.\n4.2. Quantitative evaluation\nIn Table 1, we quantitatively evaluate the predictions\nobtained using seven different unsupervised baseline mod-\nels:\nDINO self-attention mask (DI) [3], the most fre-\nquently co-occurring semantic unlabeled category mask\nfrom STEGO (ST) [18], the pseudo co-saliency mask\n(DI+ST), predictions from TokenCut [56], DVFDVD [2],\nSegSwap [50], and predicted masks from our US-CoSOD\nmodel.\nWe evaluated SegSwap [50] (that only predicts\npairwise co-segmentations) by averaging the predicted co-\nsegmentations over all image pairs in an image group i.e.\nbetween one image and every other image in the group. Ta-\nble 1 shows that our US-CoSOD model trained on the 150K\nimages from ImageNet (150 images per class) achieves the\nbest performance. However, increasing the number of im-\nages to 450 per class slightly reduced the model accuracy.\nThis could be attributed to the fact that adding more unla-\nbeled images to the training set may lead to erroneous train-\ning due to the noisy pseudo ground truth masks generated\nby DI+ST (using which US-CoSOD is trained).\nIn Table 2, we compare the performance of different ver-\nsions of our SS-CoSOD models using different proportions\nof labeled data. The “US-CoSOD” prefix indicates that the\nmodel is initialized with the pre-trained US-CoSOD model.\n“DJ” indicates that the model is trained with labeled and\nunlabeled images passed ‘disjointly’ to the student model\nwithout any cross-region interaction between the two sets.\nThe SS-CoSOD model is trained semi-supervisedly using\nonly images from the COCO9213 dataset. Finally, the “SS-\nCoSOD with ImgNet” version utilizes the extra 150 images\nper 1K ImageNet classes during semi-supervised training.\nIn order to include the unlabeled ImageNet images in our\nsemi-supervised setting, we use CEN to infer the confidence\nweights for the unlabeled images and then include the sam-\nples with predicted score ≥0.9 in the labeled set and the\nremaining samples in the unlabeled set.\nIn Table 2, we observe a consistent improvement in SS-\nCoSOD performance when the reliability scores from CEN\nare used to modulate the unsupervised loss. Joint labeled-\nunlabeled student training further leads to a small but con-\nsistent improvement. We further observe a significant im-\nprovement in performance when the ImageNet-1K dataset\nis used for semi-supervised learning, e.g. on the CoCA\ndataset, we obtain a reduction of 20% for MAE and gains\nof 4.79% for maximum F-measure, 2.64% for maximum E-\nmeasure, and 2.76% for S-measure compared to SS-CoSOD\nTable 2. Performance comparison of the different versions of our unsupervised and semi-supervised models. In column 1, we indicate the\nfraction of labeled data for training, followed by the actual number of images. See supplementary for the results with 1/8 labeled data.\nCoCA\nCosal2015\nCoSOD3k\nLabeled data\nMethod\nMAE↓\nF max\nβ\n↑\nEmax\nϕ\n↑\nSα ↑\nMAE↓\nF max\nβ\n↑\nEmax\nϕ\n↑\nSα ↑\nMAE↓\nF max\nβ\n↑\nEmax\nϕ\n↑\nSα ↑\nUS-CoSOD\n0.108\n0.557\n0.754\n0.683\n0.068\n0.854\n0.888\n0.846\n0.076\n0.783\n0.857\n0.801\nSS-CoSOD-DJ (w/o CEN)\n0.107\n0.485\n0.728\n0.635\n0.094\n0.771\n0.834\n0.771\n0.089\n0.709\n0.817\n0.742\n1/16 (576)\nSS-CoSOD-DJ (w/ CEN)\n0.115\n0.488\n0.730\n0.639\n0.086\n0.782\n0.847\n0.787\n0.086\n0.717\n0.828\n0.755\nSS-CoSOD\n0.113\n0.492\n0.733\n0.641\n0.085\n0.788\n0.850\n0.792\n0.084\n0.721\n0.830\n0.758\nUS-CoSOD+SS-CoSOD\n0.111\n0.554\n0.751\n0.681\n0.066\n0.855\n0.890\n0.849\n0.075\n0.783\n0.858\n0.803\nSS-CoSOD with ImgNet\n0.098\n0.562\n0.757\n0.684\n0.072\n0.837\n0.880\n0.828\n0.068\n0.784\n0.865\n0.800\nUS-CoSOD\n0.109\n0.569\n0.758\n0.685\n0.069\n0.855\n0.888\n0.844\n0.077\n0.783\n0.854\n0.797\nSS-CoSOD-DJ (w/o CEN)\n0.097\n0.552\n0.763\n0.678\n0.076\n0.828\n0.874\n0.818\n0.075\n0.776\n0.859\n0.790\n1/4 (2303)\nSS-CoSOD-DJ (w/ CEN)\n0.096\n0.560\n0.764\n0.685\n0.069\n0.839\n0.885\n0.831\n0.069\n0.784\n0.867\n0.802\nSS-CoSOD\n0.097\n0.562\n0.765\n0.686\n0.068\n0.841\n0.886\n0.833\n0.068\n0.785\n0.868\n0.803\nUS-CoSOD+SS-CoSOD\n0.107\n0.566\n0.757\n0.686\n0.066\n0.858\n0.891\n0.848\n0.073\n0.787\n0.859\n0.803\nSS-CoSOD with ImgNet\n0.091\n0.581\n0.772\n0.698\n0.066\n0.851\n0.891\n0.841\n0.064\n0.799\n0.875\n0.812\nUS-CoSOD\n0.105\n0.569\n0.760\n0.688\n0.068\n0.856\n0.889\n0.843\n0.074\n0.793\n0.862\n0.804\nSS-CoSOD-DJ (w/o CEN)\n0.092\n0.572\n0.771\n0.694\n0.068\n0.846\n0.885\n0.834\n0.071\n0.791\n0.865\n0.802\n1/2 (4607)\nSS-CoSOD-DJ (w/ CEN)\n0.090\n0.578\n0.772\n0.699\n0.062\n0.851\n0.892\n0.843\n0.067\n0.795\n0.870\n0.810\nSS-CoSOD\n0.088\n0.582\n0.773\n0.700\n0.062\n0.854\n0.892\n0.843\n0.066\n0.797\n0.872\n0.809\nUS-CoSOD+SS-CoSOD\n0.110\n0.563\n0.755\n0.686\n0.064\n0.858\n0.894\n0.850\n0.072\n0.794\n0.866\n0.810\nSS-CoSOD with ImgNet\n0.088\n0.590\n0.775\n0.705\n0.062\n0.861\n0.896\n0.850\n0.063\n0.804\n0.876\n0.817\nFull (9213)\nUS-CoSOD\n0.102\n0.573\n0.764\n0.692\n0.068\n0.860\n0.890\n0.845\n0.077\n0.791\n0.856\n0.799\nSS-CoSOD with ImgNet\n0.091\n0.591\n0.778\n0.707\n0.061\n0.865\n0.901\n0.852\n0.062\n0.809\n0.882\n0.821\nOriginal\nGround truth US-CoSOD\nTokenCut\nSegSwap\nDVFDVD\nFigure 4. Qualitative comparisons of our US-CoSOD model with\nother baselines on the teddy bear image group from CoCA. US-\nCoSOD produces the most accurate segmentation masks.\ntrained using the COCO9213 dataset on the 1/2 data split.\nComparison with existing CoSOD models: In Table 6,\nwe compare the performance of existing unsupervised and\nsemi-supervised CoSOD models with our model using the\nF max\nβ\n-measure and the Sα-measure metrics. Both of our\nunsupervised and semi-supervised models outperform the\ncorresponding state-of-the-art models by a significant mar-\ngin e.g., on the Cosal2015 dataset, our US-CoSOD model\nhas an 8.8% F-measure gain over DVFDVD [2], an unsu-\npervised SOTA co-segmentation model and our SS-CoSOD\nmodel (using ImageNet-1K) has an 11.81% F-measure gain\nover FASS [72], a semi-supervised CoSOD model.\nAblation studies:\nVariant model: For certain image groups (e.g. key, frisbee,\netc.), the co-saliency of the common objects can be lesser\nthan that of other bigger objects solely due to the lesser\narea, which could impact performance. To test this hypothe-\nsis, we investigate a variant model that normalizes the over-\nlap score by STEGO mask area as: O\n′j\ni\n= Ar(SMj\ni ∩DMi)\nAr(SMj\ni )\n(see Algorithm 1). US-CoSOD (without area normaliza-\ntion) has more accurate predictions compared to this vari-\nant on all test sets, e.g. US-CoSOD achieves F-measures\n0.461, 0.760, and 0.684 against the variant model’s 0.410,\n0.613, and 0.579 on CoCA, Cosal2015, and CoSOD3k re-\nspectively. See supplementary for more details.\nPerformance on challenging categories: Our US-CoSOD\noutperforms the pre-trained DINO and DINO+STEGO\nmodels by a significant margin on challenging categories\n(categories over which DINO scored lesser than the average\nDINO F-measure score over the test dataset). For instance,\nthe average F-measures of DINO, DINO+STEGO, and US-\nCoSOD are 0.598, 0.654, and 0.738 on the Cosal2015\ndataset respectively. See supplementary for more details.\nCEN backbone: We compared the confidence estimation\nerror (Mean Squared Error, MSE) of CEN using different\nbackbone networks on the unlabeled set. DINO (ResNet50)\nyielded the least MSE across different data splits e.g. MSE\nusing ResNet50, MobileNetV2, ViTB, and ViTS are 0.166,\n0.171, 0.176, and 0.177 respectively on the 1/4 labeled data\nsplit. See supplementary for details. We attribute the lower\naccuracy of MobileNetV2 to its lower feature representation\npower. Also the transformer models DINO (ViTB, ViTS)\nfail to outperform the convolutional models (e.g. ResNet50)\ndue to the lesser training data (in different data splits).\n4.3. Qualitative evaluation\nIn Fig. 4, we qualitatively compare the CoSOD predic-\ntions from TokenCut [56], SegSwap [50], and DVFDVD [2]\nwith our US-CoSOD on the teddy bear image group from\nUS-CoSOD\nGround \ntruth\nDCFM\n(all labels)\nCamera (from CoCA)\nButterfly (from CoSOD3k)\nSofa (from Cosal2015)\nSS-CoSOD\n(all labels)\nInput \nimage\nDCFM\n(1/4 labels)\nSS-CoSOD\n(1/4 labels)\nFigure 5. Qualitative comparisons of our model with different baselines on three image groups selected each from the CoCA, CoSOD3k,\nand Cosal2015 datasets. Our SS-CoSOD model (all labels) produces the most accurate segmentation mask compared to the other baselines.\nCoCA. We see that US-CoSOD more accurately detects the\nteddy bear in the four images compared to the baselines.\nIn Fig. 5, we qualitatively compare the CoSOD pre-\ndictions from different baselines with SS-CoSOD on three\nimage groups, each from the CoCA, CoSOD3k, and\nCosal2015 datasets.\nWe see that our US-CoSOD gen-\nerates reasonable masks and our SS-CoSOD approaches\nfurther improve these predictions while performing better\nthan the fully supervised DCFM trained with limited labels.\nWhile DCFM (all labels) produces incomplete segmenta-\ntions (columns 5, 9, 11, and 12) and overestimates the co-\nsaliency (in columns 2, 3, 5, and 8) in certain image regions,\nour SS-CoSOD (all labels) model predictions suffer fewer\ninaccuracies, producing more accurate CoSOD masks.\nTable 3. Comparison of our models with existing unsupervised\nand semi-supervised models for CoSOD on Cosal2015.\nMethod\nType\nLabel %\nF max\nβ\n↑\nSα ↑\nCSSCF [24] (TMM 2016)\nUnsup\n-\n0.682\n0.671\nCoDW [65] (IJCV 2016)\nUnsup\n-\n0.705\n0.647\nUCCDGO [22] (ECCV 2018)\nUnsup\n-\n0.758\n0.751\nDVFDVD [2] (ECCVW 2022)\nUnsup\n-\n0.777\n0.809\nSegSwap [50] (CVPRW 2022)\nUnsup\n-\n0.618\n0.632\nOurs (US-CoSOD)\nUnsup\n-\n0.845\n0.840\nFASS [72] (ACMM 2018)\nSemi-sup\n50%\n0.770\n-\nOurs (SS-CoSOD w/ ImgNet)\nSemi-sup\n25%\n0.851\n0.841\nOurs (SS-CoSOD w/ ImgNet)\nSemi-sup\n50%\n0.861\n0.850\nFig. 6 shows that while multiple categories can co-exist\nin an image group (e.g. flowers and butterflies), our DI+ST\nmodel extracts the more co-salient butterflies, guided by the\nDINO SA maps. Also, our US-CoSOD model trained using\nOriginal image     Semantic segmentation     DINO SA (DI)                   DI + ST                  US-CoSOD\nFigure 6. DINO self-attention maps and US-CoSOD predictions\nusing DI+ST for training. US-CoSOD produces the best results.\nthe pseudo co-saliency masks from DI+ST improves the co-\nsaliency masks (better structural consistency) from DI+ST.\n5. Conclusion\nWe presented a novel unsupervised approach for CoSOD\nbased on the frequency statistics of semantic segmenta-\ntions, which forms a strong pre-training initialization for\na semi-supervised CoSOD model.\nOur semi-supervised\nmodel employs a student-teacher approach with an effec-\ntive confidence estimation module. We demonstrate that\nboth our unsupervised and semi-supervised CoSOD mod-\nels can significantly improve prediction performance over\na fully-supervised model trained with limited labeled data.\nAs future work, we aim to improve our unsupervised model\nby avoiding the use of any off-the-shelf component.\nSupplemental Material: Unsupervised and\nsemi-supervised co-salient object detection via\nsegmentation frequency statistics\n1. Additional qualitative results\n1.1. Comparison of CoSOD predictions\nIn Fig. S1 we present additional results of co-salient ob-\nject detection using the proposed models and the other base-\nlines.\nIn the first image group in Fig. S1 we show the CoSOD\npredictions on the eggplant category from the CoCA\ndataset. While our US-CoSOD model detects the salient ob-\njects well, it fails to accurately segment the eggplant. Sim-\nilarly, both the DCFM and our SS-CoSOD model trained\nwith 1/4 labels fail to accurately detect the eggplant in-\nstances. Our SS-CoSOD model when trained with all labels\npredicts CoSOD segmentations most closely resembling the\nground truth.\nIn the zebra image group (selected from the CoSOD3k\ndataset), we observe that the segmentation maps obtained\nfrom our SS-CoSOD model trained with all labels most\nclosely resemble the ground truth.\nThe DCFM model\ntrained with all labels suffers from overestimating the co-\nsaliency of certain image regions e.g. in columns 1 and 2\nand produces incomplete segmentations in columns 3 and\n4. While the DCFM model trained with all labels segments\nthe zebra in column 1, the segmentation prediction fails to\npreserve the shape. In column 2, all models except our SS-\nCoSOD model trained with all labels detect the giraffes as\nbeing co-salient along with the zebras.\nIn the third image group, we compare the segmentation\nresults on the penguin group from the Cosal2015 dataset.\nOverall, our SS-CoSOD model trained with all labels pro-\nduces more accurate co-salient object segmentations com-\npared to the other baselines. In column 1, our SS-CoSOD\nmodels trained with 1/4 labels and with all labels well seg-\nment the penguin. In the last column of this group, we show\nan instance where all the models including our SS-CoSOD\nfail to distinguish the penguin from the seal. This could\nbe due to the fact that the seal has similar visual features\nas the penguin, which makes it difficult for the models to\ndistinguish between the two categories. Training on more\nfine-grained categories might help our model resolve this\nambiguity.\n1.2. Comparison of unsupervised CoSOD predic-\ntions\nIn Fig. S2 we present additional results comparing the\nself-attention (SA) maps from DINO (DI), the pseudo co-\nsalient ground truth masks - our DINO+STEGO model\n(DI+ST), and predictions from our US-CoSOD model.\nIn column 2 of row block 1 (the teddy bear image group),\nwe observe that the most frequent unsupervised semantic\nclusters representing the teddy bear are colored light green\nand pink. Our US-CoSOD model effectively eliminates the\ninaccurate segmentation of the child (that carries the teddy\nbear) produced by the DINO SA and the DI+ST models. In\nrows 2 and 3 of this group, US-CoSOD rectifies the inaccu-\nrate segmentation masks obtained from the DI+ST model.\nIn row 4, the teddy bear segmentation from both the DI+ST\nand US-CoSOD models is quite accurate.\nIn row block 2 (the hourglass image group), we observe\nthat blue and dark blue colored unsupervised semantic clus-\nters mainly constitute the hourglass object. In row 2 of this\ngroup, although the SA map from DINO highlights both\nthe person and the hourglass to be salient, the segmenta-\ntion predictions from the DI+ST and US-CoSOD models\ncorrectly show only the hourglass to be co-salient, which is\ndue to the fact that the co-occurrence frequency of the un-\nsupervised semantic cluster denoting the hourglass object\nis sufficiently high compared to that for the person. Our\nUS-CoSOD model further improves the segmentations pre-\ndicted by the DI+ST model.\nIn Fig. S3 we present qualitative results comparing our\nmethod with the different unsupervised methods for single-\nimage segmentation and co-segmentation tasks. We observe\nthat our US-CoSOD model has better segmentation predic-\ntions compared to the SegSwap [50], DVFDVD [2], and the\nTokenCut [56] models for two image groups - hour glass\nand teddy bear from the CoCA dataset.\n1.3. Confidence Estimation Network predictions\nIn Fig. S4 we show the ground truth and the predicted\nconfidence scores from our Confidence Estimation Network\n(CEN) module using 1/2 and 1/8 labels for training. GT de-\nnotes the max. F1-score of the predictions obtained from\nthe pretrained fP T model (see Fig. 3 in the main paper) on\nthe unlabeled data and Pred denotes the F1-score predicted\nby our trained CEN module. We observe that the confidence\nscores vary in proportion to the image complexity in terms\nof the image contents. In particular, we observe that the\nground truth confidence score is high when the co-salient\nobject is more salient and has a clear demarcation with re-\nspect to the scene background, while the ground truth con-\nfidence score is low when the image scene is more cluttered\n(e.g. for the train class in row 2 and the banana class in row\n4) or the co-salient objects are out-of-distribution (e.g. for\nthe boat class in row 3, the boat is on the land). Also, we\nobserve that our CEN module is able to predict the ground\ntruth confidence score well. Therefore, the CEN model ef-\nfectively suppresses the error propagation during training\ncaused due to inaccurate confidence estimation on images\nthat are difficult for the CoSOD task.\nUS-CoSOD\nGround truth\nDCFM\n(all labels)\nEggplant (from CoCA)\nZebra (from CoSOD3k)\nPenguin (from Cosal2015)\nSS-CoSOD\n(all labels)\nInput image\nDCFM \n(1/4 labels)\nSS-CoSOD\n(1/4 labels)\nUS-CoSOD\nGround truth\nDCFM\n(all labels)\nSS-CoSOD\n(all labels)\nInput image\nDCFM \n(1/4 labels)\nSS-CoSOD\n(1/4 labels)\nFigure S1. Additional qualitative comparisons of our model with different baselines on three image groups selected each from CoCA,\nCoSOD3k and Cosal2015. Our SS-CoSOD model trained with all labels produces the most accurate segmentation mask compared to the\nother baselines.\nOriginal Image      Semantic         DINO SA (DI)           DI + ST            US-CoSOD\nsegmentation\nTeddy \nBear\nHourglass\nFigure S2. Additional qualitative comparisons of the DINO self-attention maps (DI), pseudo ground truth co-saliency masks from our\nDI+ST model, and predictions from our US-CoSOD model.\n2. Additional quantitative results\nIn Tab. 1, we provide additional results of the perfor-\nmance evaluation of our US-CoSOD model compared to\nTab. 1 in the main paper. In particular, we additionally show\nthe prediction performance of US-CoSOD when trained on\na set of 50K images (with 50 images per class) along with\nthe baselines presented in Tab. 1 in the main paper. US-\nCoSOD when trained on 150 images per class produces the\nbest performance. Training US-CoSOD using 50 images\nper class (for each of the 1000 ImageNet classes) leads to in-\nferior performance due to limited training data. On the other\nhand, training the model using 450 images per class reduces\nsegmentation accuracy. This could be because adding more\ndifficult unlabeled images to the training set may lead to er-\nroneous training due to the inaccurate pseudo ground truth\nmasks generated by the DI+ST model, using which US-\nCoSOD is trained.\nQuantitative evaluation of SS-CoSOD In Tab. 2, we show\n(a) “Teddy Bear” group from CoCA\n(b) “Hour glass” group from CoCA\nOriginal\nGround truth US-CoSOD\nTokenCut [6] SegSwap [5] DVFDVD [1]\nFigure S3. Qualitative comparisons of prediction results from our unsupervised CoSOD model, US-CoSOD vs. corresponding segmen-\ntations from existing unsupervised segmentation models. TokenCut [56] is a single-image segmentation method and SegSwap [50] and\nDVFDVD [2] are unsupervised co-segmentation methods.\na more detailed version of Tab. 2 in the main paper. Here,\nwe additionally show the prediction results with 1/8 labeled\ndata.\nComparison with SOTA In Tab. 3, we compare the perfor-\nmance of our model with other state-of-the-art models on\nthe 3 benchmark datasets. We outperform the state-of-the-\nart DCFM model [62] on the Cosal2015 and the CoSOD3k\ndatasets, while we are comparable with this model on the\nCoCA dataset (DCFM predictions on CoCA being slightly\nmore accurate). Also, we outperform other existing fully\nsupervised CoSOD models by a significant margin.\nVariant model In Tab. 4, we compare the performance of\nthe proposed US-CoSOD model with a variant version of\nthe model where we divide the overlap area, Oj\ni between\nthe DINO mask DMi and the STEGO segmentation mask\nSM j\ni (for class cj) by the area occupied by the STEGO\nGT:       0.974\nPred:   0.978\nGT:       0.954   \nPred:   0.951\nPred:   1.000\nPred:   0.897\n1/2 labels:\n1/8 labels: GT:       0.864\nGT:       0.929\nGT:       0.868\nPred:   0.862\nPred:   0.838\nGT:       0.669 \nGT:       0.832   \nPred:   0.821\nPred:   0.834\nGT:       0.776\nGT:       0.734   \nPred:   0.763\nPred:   0.779\nGT:       0.748\nElephant\n1/2 labels:\n1/8 labels:\nBanana\nGT:       0.898\nPred:   0.781\nPred:   0.691\nGT:       0.530\nGT:       0.935\nPred:   0.884\nGT:       0.655\nPred:   0.615\nGT:       0.857\nPred:   0.868\nPred:   0.802\nGT:       0.871\nGT:       0.721\nPred:   0.707\nPred:   0.674\nGT:       0.618\nGT:       0.944\nPred:   0.968\nPred:   0.793\nGT:       0.781\n1/2 labels:\n1/8 labels:\nGT:       0.767\nPred:   0.790\nPred:   0.730\nGT:       0.738\nGT:       0.845\nPred:   0.845\nGT:       0.830\nPred:   0.848\nGT:       0.932\nPred:   0.922\nPred:   0.925\nGT:       0.916\nGT:       0.710\nPred:   0.709\nPred:   0.734\nGT:       0.694\nGT:       0.947\nPred:   0.981\nPred:   0.939\nGT:       0.939\nTrain\n1/2 labels:\n1/8 labels:\nGT:       0.757\nPred:   0.769\nPred:   0.718\nGT:       0.744\nGT:       0.834\nPred:   0.819\nGT:       0.822\nPred:   0.778\nGT:       0.879\nPred:   0.867\nPred:   0.816\nGT:       0.861\nGT:       0.765\nPred:   0.779\nPred:   0.777\nGT:       0.765\nGT:       0.901\nPred:   0.854\nPred:   0.858\nGT:       0.877\nBoat\nFigure S4. Depiction of the ground truth and the predicted confidence scores from our Confidence Estimation Network (CEN) module\nusing 1/2 and 1/8 labels for training. GT denotes the max. F1-score of the predictions obtained from the pretrained fP T model (see Fig.\n3 in the main paper) on the unlabeled data and Pred denotes the F1-score predicted by our trained CEN module. We observe that the\nconfidence scores vary in proportion to the image complexity in terms of the image contents. Also, we observe that our CEN module is\nable to predict the ground truth confidence score well.\nTable 1. Performance evaluation of our US-CoSOD model: we show the prediction performance of US-CoSOD when trained on a set of\n50K images (with 50 images per class) along with the other baselines presented in Table 1 in the main paper. US-CoSOD when trained on\n150 images per class produces the best performance.\nCoCA\nCosal2015\nCoSOD3k\nMethod\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nDINO (DI)\n0.214\n0.372\n0.572\n0.540 0.154\n0.659\n0.753\n0.688 0.146\n0.624\n0.749\n0.679\nSTEGO (ST)\n0.235\n0.353\n0.555\n0.523 0.164\n0.618\n0.717\n0.676 0.204\n0.543\n0.660\n0.615\nTokenCut [56] (CVPR 2022)\n0.167\n0.467\n0.704\n0.627 0.139\n0.805\n0.857\n0.793 0.151\n0.720\n0.811\n0.744\nDVFDVD [2] (ECCVW 2022)\n0.223\n0.422\n0.592\n0.581 0.092\n0.777\n0.842\n0.809 0.104\n0.722\n0.819\n0.773\nSegSwap [50] (CVPRW 2022)\n0.165\n0.422\n0.666\n0.567 0.178\n0.618\n0.720\n0.632 0.177\n0.560\n0.705\n0.608\nOurs (DI+ST)\n0.165\n0.461\n0.676\n0.610 0.112\n0.760\n0.823\n0.767 0.124\n0.684\n0.793\n0.724\nOurs (US-CoSOD-COCO9213) 0.140\n0.498\n0.702\n0.641 0.090\n0.792\n0.852\n0.806 0.095\n0.735\n0.832\n0.772\nOurs (US-CoSOD-ImgNet50)\n0.141\n0.516\n0.703\n0.648 0.076\n0.823\n0.876\n0.827 0.092\n0.752\n0.841\n0.783\nOurs (US-CoSOD-ImgNet150)\n0.116\n0.546\n0.743\n0.672 0.070\n0.845\n0.886\n0.840 0.076\n0.779\n0.861\n0.801\nOurs (US-CoSOD-ImgNet450)\n0.127\n0.543\n0.726\n0.666 0.071\n0.844\n0.884\n0.842 0.079\n0.775\n0.854\n0.800\nTable 2. Performance comparison of the different versions of our unsupervised and semi-supervised models. In column 1, we indicate the\nfraction of labeled data for training, followed by the actual number of images.\nCoCA\nCosal2015\nCoSOD3k\nSplit\nMethod\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nDCFM [62] (CVPR 22)\n0.119\n0.485\n0.725\n0.636 0.088\n0.780\n0.847\n0.786 0.088\n0.716\n0.827\n0.753\nUS-CoSOD+DCFM\n0.108\n0.557\n0.754\n0.683 0.068\n0.854\n0.888\n0.846 0.076\n0.783\n0.857\n0.801\nSS-CoSOD-DJ (w/o CEN) 0.107\n0.485\n0.728\n0.635 0.094\n0.771\n0.834\n0.771 0.089\n0.709\n0.817\n0.742\n1/16 (576)\nSS-CoSOD-DJ (w/ CEN)\n0.115\n0.488\n0.730\n0.639 0.086\n0.782\n0.847\n0.787 0.086\n0.717\n0.828\n0.755\nSS-CoSOD\n0.113\n0.492\n0.733\n0.641 0.085\n0.788\n0.850\n0.792 0.084\n0.721\n0.830\n0.758\nUS-CoSOD+SS-CoSOD\n0.111\n0.554\n0.751\n0.681 0.066\n0.855\n0.890\n0.849 0.075\n0.783\n0.858\n0.803\nSS-CoSOD with ImgNet\n0.098\n0.562\n0.757\n0.684 0.072\n0.837\n0.880\n0.828 0.068\n0.784\n0.865\n0.800\nDCFM [62] (CVPR 22)\n0.110\n0.493\n0.731\n0.639 0.096\n0.780\n0.839\n0.779 0.096\n0.727\n0.818\n0.746\nUS-CoSOD+DCFM\n0.111\n0.558\n0.754\n0.683 0.067\n0.857\n0.890\n0.847 0.076\n0.785\n0.857\n0.801\nSS-CoSOD-DJ (w/o CEN) 0.103\n0.497\n0.732\n0.641 0.096\n0.777\n0.835\n0.777 0.094\n0.727\n0.816\n0.744\n1/8 (1152)\nSS-CoSOD-DJ (w/ CEN)\n0.116\n0.499\n0.735\n0.644 0.085\n0.793\n0.854\n0.800 0.087\n0.740\n0.834\n0.767\nSS-CoSOD\n0.114\n0.500\n0.736\n0.645 0.084\n0.795\n0.856\n0.802 0.086\n0.740\n0.835\n0.767\nUS-CoSOD+SS-CoSOD\n0.108\n0.558\n0.755\n0.683 0.068\n0.857\n0.888\n0.845 0.076\n0.785\n0.856\n0.799\nSS-CoSOD with ImgNet\n0.097\n0.560\n0.755\n0.685 0.068\n0.845\n0.884\n0.838 0.068\n0.791\n0.871\n0.808\nDCFM [62] (CVPR 22)\n0.107\n0.547\n0.758\n0.672 0.073\n0.829\n0.880\n0.824 0.075\n0.775\n0.862\n0.794\nUS-CoSOD+DCFM\n0.109\n0.569\n0.758\n0.685 0.069\n0.855\n0.888\n0.844 0.077\n0.783\n0.854\n0.797\nSS-CoSOD-DJ (w/o CEN) 0.097\n0.552\n0.763\n0.678 0.076\n0.828\n0.874\n0.818 0.075\n0.776\n0.859\n0.790\n1/4 (2303)\nSS-CoSOD-DJ (w/ CEN)\n0.096\n0.560\n0.764\n0.685 0.069\n0.839\n0.885\n0.831 0.069\n0.784\n0.867\n0.802\nSS-CoSOD\n0.097\n0.562\n0.765\n0.686 0.068\n0.841\n0.886\n0.833 0.068\n0.785\n0.868\n0.803\nUS-CoSOD+SS-CoSOD\n0.107\n0.566\n0.757\n0.686 0.066\n0.858\n0.891\n0.848 0.073\n0.787\n0.859\n0.803\nSS-CoSOD with ImgNet\n0.091\n0.581\n0.772\n0.698 0.066\n0.851\n0.891\n0.841 0.064\n0.799\n0.875\n0.812\nDCFM [62] (CVPR 22)\n0.101\n0.566\n0.764\n0.690 0.065\n0.845\n0.889\n0.838 0.070\n0.792\n0.870\n0.807\nUS-CoSOD+DCFM\n0.105\n0.569\n0.760\n0.688 0.068\n0.856\n0.889\n0.843 0.074\n0.793\n0.862\n0.804\nSS-CoSOD-DJ (w/o CEN) 0.092\n0.572\n0.771\n0.694 0.068\n0.846\n0.885\n0.834 0.071\n0.791\n0.865\n0.802\n1/2 (4607)\nSS-CoSOD-DJ (w/ CEN)\n0.090\n0.578\n0.772\n0.699 0.062\n0.851\n0.892\n0.843 0.067\n0.795\n0.870\n0.810\nSS-CoSOD\n0.088\n0.582\n0.773\n0.700 0.062\n0.854\n0.892\n0.843 0.066\n0.797\n0.872\n0.809\nUS-CoSOD+SS-CoSOD\n0.110\n0.563\n0.755\n0.686 0.064\n0.858\n0.894\n0.850 0.072\n0.794\n0.866\n0.810\nSS-CoSOD with ImgNet\n0.088\n0.590\n0.775\n0.705 0.062\n0.861\n0.896\n0.850 0.063\n0.804\n0.876\n0.817\nDCFM [62] (CVPR 22)\n0.085\n0.598\n0.783\n0.710 0.067\n0.856\n0.892\n0.838 0.067\n0.805\n0.874\n0.810\nFull (9213) US-CoSOD+DCFM\n0.102\n0.573\n0.764\n0.692 0.068\n0.860\n0.890\n0.845 0.077\n0.791\n0.856\n0.799\nSS-CoSOD with ImgNet\n0.091\n0.591\n0.778\n0.707 0.061\n0.865\n0.901\n0.852 0.062\n0.809\n0.882\n0.821\nTable 3. Comparison of our model with other state-of-the-art models on 3 benchmarks. We achieve state-of-the-art performance on the test\ndatasets.\nCoCA\nCosal2015\nCoSOD3k\nMethod\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑MAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑MAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nGCAGC [68] (CVPR20)\n0.111\n0.523\n0.754\n0.669 0.085\n0.813\n0.866\n0.817 0.100\n0.740\n0.816\n0.785\nCoEGNet [13] (TPAMI21)\n0.106\n0.493\n0.717\n0.612 0.077\n0.832\n0.882\n0.836 0.092\n0.736\n0.825\n0.762\nGICD [71] (ECCV20)\n0.126\n0.513\n0.715\n0.658 0.071\n0.844\n0.887\n0.844 0.079\n0.770\n0.848\n0.797\nGCoNet [15] (CVPR21)\n0.105\n0.544\n0.760\n0.673 0.068\n0.847\n0.887\n0.845 0.071\n0.777\n0.860\n0.802\nDCFM [62] (CVPR22)\n0.085\n0.598\n0.783\n0.710 0.067\n0.856\n0.892\n0.838 0.067\n0.805\n0.874\n0.810\nCoRP [73] (TPAMI23)\n-\n0.551\n0.715\n0.686 -\n0.885\n0.913\n0.875 -\n0.798\n0.862\n0.820\nUFO [51] (TMM23)\n0.095\n0.571\n0.782\n0.697 0.064\n0.865\n0.906\n0.860 0.073\n0.797\n0.874\n0.819\nGEM [?] (CVPR23)\n0.095\n0.599\n0.808\n0.726 0.053\n0.882\n0.933\n0.885 0.061\n0.829\n0.911\n0.853\nDMT [38] (CVPR23)\n0.108\n0.619\n0.800\n0.725 0.0454 0.905\n0.936\n0.897 0.063\n0.835\n0.895\n0.851\nOurs (SS-CoSOD with ImgNet) 0.091\n0.591\n0.778\n0.707 0.061\n0.865\n0.900\n0.852 0.062\n0.809\n0.882\n0.821\nTable 4. Comparison of our US-CoSOD model with a variant\nversion that normalizes the overlap area between the DINO SA\nmask and STEGO segmentation mask by the STEGO segmenta-\ntion mask area.\nDataset\nMethod\nMAE↓F max\nβ\n↑Emax\nϕ\n↑Sα ↑\nCoCA\nUS-CoSOD (Variant)\n0.131 0.410\n0.650\n0.590\nUS-CoSOD (Proposed) 0.165 0.461\n0.676\n0.610\nCosal2015 US-CoSOD (Variant)\n0.143 0.613\n0.713\n0.681\nUS-CoSOD (Proposed) 0.112 0.760\n0.823\n0.767\nCoSOD3k US-CoSOD (Variant)\n0.127 0.579\n0.714\n0.666\nUS-CoSOD (Proposed) 0.124 0.684\n0.793\n0.724\nmask Ar(SM j\ni ). The proposed version of the US-CoSOD\nmodel performs better than the variant version over all three\ntest datasets using all four evaluation metrics.\nPerformance on challenging categories In Tab. 5, we\nreport the average F-measure score on the categories\nover which DINO (a pre-trained component) scored lesser\nthan the average DINO F-measure score over the test\ndataset.\nSpecifically, for a given test dataset, categories\nthat had an F-measure score lower than the threshold value,\nF β\nth =\n1\nn\nPn\ni=1 F β(SAi) (here SAi denotes the DINO\nself-attention map of image Ii and n = total number of\ntest images) were considered for this experiment. As ob-\nserved, our US-CoSOD outperforms the pre-trained DINO\nand DINO+STEGO models by a significant margin on such\ndifficult categories.\nCEN backbone In Tab. 6, we compared the confidence es-\ntimation error (Mean Squared Error) of different backbone\nnetworks for our CEN module on the unlabeled dataset. As\nwe observe, ResNet50 trained using DINO provides us the\nleast Mean Squared Error loss across all data splits. We at-\ntribute the lower accuracy of MobileNetV2 to its lower fea-\nture representation power, and that of the ViTB and ViTS\nmodels to the fact that such transformer models fail to out-\nperform convolutional models (e.g. ResNet50) when less\ndata is available for training (in the different label splits).\nTable 5. Average F-measure of the baselines over the categories on\nwhich the categorical F-measure scores of DINO are lower than its\naverage F-measure on the test dataset.\nModel\nCoCA Cosal2015 CoSOD3k\nDINO (DI)\n0.269\n0.598\n0.452\nDINO+STEGO (DI+ST) 0.331\n0.654\n0.529\nUS-CoSOD\n0.408\n0.738\n0.577\nTable 6. Comparison of the confidence estimation error (Mean\nSquared Error) of different backbone networks for our CEN mod-\nule on the unlabeled dataset.\nModel\n1/16 (576) 1/4 (2303) 1/2 (4607)\nMobileNetV2 (3.4M)\n0.210\n0.171\n0.168\nDINO (ViTS8) (22.2M)\n0.207\n0.177\n0.174\nDINO (ViTB8) (86M)\n0.208\n0.176\n0.170\nDINO (ResNet50) (25.6M) 0.204\n0.166\n0.160\n3. Additional implementation details\nWe randomly split the data in the COCO9213 dataset\ninto the labeled and the unlabeled sets (i.e. 1/16, 1/8, 1/4,\n1/2 labels) for training the fully supervised DCFM and our\nsemi-supervised SS-CoSOD models.\nThe inputs are resized to 224 × 224 for both training and\ninference. We use Adam as our optimizer to train our mod-\nels. The total training time is around 5 hours for US-CoSOD\nand around 8 hours for SS-CoSOD using ImageNet-1K. All\nexperiments are run on a single NVIDIA Tesla V100 SXM2\nGPU.\nFor the unsupervised model (US-CoSOD) and the super-\nvised pre-training on labeled data in stage 1 in Fig. 3 in\nthe main paper, we set the learning rate is set as 10−5 for\nfeature extractor and 10−4 for other parts, and the weight\ndecay is set as 10−4, following [62]. Training these mod-\nels take around 200 epochs using 1/2 and full labels, and\naround 100 epochs using 1/4, 1/8, and 1/16 labels.\nFor our semi-supervised approach (SS-CoSOD), we\nfine-tune the pre-trained model (from stage 1 in Fig. 3 in\nthe main paper) using the learning rate is set as 10−7 for\nfeature extractor and 10−6 for other parts, and the weight\ndecay is set as 10−6.\nFor training our Confidence Estimation Network, we\nrandomly divided the labeled data into training (80%) and\nvalidation (20%) sets. We used the Adam optimizer for\ntraining with initial learning rate = 2 × 10−4 with a weight\ndecay = 10−4. The step of the learning rate scheduler is set\nas 7. We used a batch size of 32 to train this model.\nReferences\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,\nand Sabine Susstrunk. Frequency-tuned salient region de-\ntection. In 2009 IEEE conference on computer vision and\npattern recognition, pages 1597–1604. IEEE, 2009. 5, 6\n[2] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.\nDeep vit features as dense visual descriptors. ECCVW What\nis Motion For?, 2022. 2, 6, 7, 8, 1, 4\n[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9650–9660, 2021. 2, 3, 4, 6\n[4] Souradeep Chakraborty and Pabitra Mitra. A site entropy\nrate and degree centrality based algorithm for image co-\nsegmentation. Journal of Visual Communication and Image\nRepresentation, 33:20–30, 2015. 2\n[5] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong\nWang. Semi-supervised semantic segmentation with cross\npseudo supervision. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2613–2622, 2021. 2\n[6] Zuyao Chen, Qianqian Xu, Runmin Cong, and Qingming\nHuang. Global context-aware progressive aggregation net-\nwork for salient object detection.\nIn Proceedings of the\nAAAI conference on artificial intelligence, volume 34, pages\n10599–10606, 2020. 1\n[7] Ming-Ming Cheng, Jonathan Warrell, Wen-Yan Lin, Shuai\nZheng, Vibhav Vineet, and Nigel Crook. Efficient salient\nregion detection with soft image abstraction. In Proceedings\nof the IEEE International Conference on Computer vision,\npages 1529–1536, 2013. 6\n[8] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath\nHariharan. Picie: Unsupervised semantic segmentation us-\ning invariance and equivariance in clustering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16794–16804, 2021. 2\n[9] Runmin Cong, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng,\nWeisi Lin, and Qingming Huang. Review of visual saliency\ndetection with comprehensive information.\nIEEE Trans-\nactions on circuits and Systems for Video Technology,\n29(10):2941–2959, 2018. 2\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 2, 6\n[11] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali\nBorji. Structure-measure: A new way to evaluate foreground\nmaps. In Proceedings of the IEEE international conference\non computer vision, pages 4548–4557, 2017. 6\n[12] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-\nMing Cheng, and Ali Borji.\nEnhanced-alignment mea-\nsure for binary foreground map evaluation. arXiv preprint\narXiv:1805.10421, 2018. 6\n[13] Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Ding-\nwen Zhang, Ming-Ming Cheng, Huazhu Fu, and Jianbing\nShen. Re-thinking co-salient object detection. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 2021.\n1, 2, 7\n[14] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang,\nHuazhu Fu, and Ming-Ming Cheng. Taking a deeper look at\nco-salient object detection. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 2919–2929, 2020. 2, 6\n[15] Qi Fan, Deng-Ping Fan, Huazhu Fu, Chi-Keung Tang, Ling\nShao, and Yu-Wing Tai. Group collaborative learning for co-\nsalient object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12288–12298, 2021. 1, 2, 7\n[16] Guangshuai Gao, Wenting Zhao, Qingjie Liu, and Yunhong\nWang. Co-saliency detection with co-attention fully convolu-\ntional network. IEEE Transactions on Circuits and Systems\nfor Video Technology, 31(3):877–889, 2020. 1\n[17] Yanliang Ge, Qiao Zhang, Tian-Zhu Xiang, Cong Zhang,\nand Hongbo Bi. Tcnet: Co-salient object detection via paral-\nlel interaction of transformers and cnns. IEEE Transactions\non Circuits and Systems for Video Technology, 2022. 2\n[18] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences. arXiv\npreprint arXiv:2203.08414, 2022. 2, 3, 4, 6\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 5\n[20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2(7), 2015. 3\n[21] Kuang-Jui Hsu, Yen-Yu Lin, Yung-Yu Chuang, et al. Co-\nattention cnns for unsupervised object co-segmentation. In\nIJCAI, volume 1, page 2, 2018. 1, 2\n[22] Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Xiaoning\nQian, and Yung-Yu Chuang. Unsupervised cnn-based co-\nsaliency detection with graphical optimization.\nIn Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 485–501, 2018. 1, 2, 8\n[23] Rongyao Hu, Zhenyun Deng, and Xiaofeng Zhu. Multi-scale\ngraph fusion for co-saliency detection. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35, pages\n7789–7796, 2021. 2\n[24] Koteswar Rao Jerripothula, Jianfei Cai, and Junsong Yuan.\nImage co-segmentation via saliency co-fusion. IEEE Trans-\nactions on Multimedia, 18(9):1896–1909, 2016. 2, 8\n[25] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-\nformation clustering for unsupervised image classification\nand segmentation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 9865–9874,\n2019. 2\n[26] Bo Jiang, Xingyue Jiang, Jin Tang, and Bin Luo. Co-saliency\ndetection via a general optimization model and adaptive\ngraph learning. IEEE Transactions on Multimedia, 23:3193–\n3202, 2020. 2\n[27] Bo Jiang, Xingyue Jiang, Jin Tang, Bin Luo, and Shilei\nHuang.\nMultiple graph convolutional networks for co-\nsaliency detection. In 2019 IEEE International Conference\non Multimedia and Expo (ICME), pages 332–337. IEEE,\n2019. 2\n[28] Bo Jiang, Xingyue Jiang, Ajian Zhou, Jin Tang, and Bin Luo.\nA unified multiple graph learning and convolutional network\nmodel for co-saliency estimation. In proceedings of the 27th\nACM International Conference on Multimedia, pages 1375–\n1382, 2019. 2\n[29] Wen-Da Jin, Jun Xu, Ming-Ming Cheng, Yi Zhang, and\nWei Guo. Icnet: Intra-saliency correlation network for co-\nsaliency detection. Advances in Neural Information Process-\ning Systems, 33:18749–18759, 2020. 2\n[30] Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, and Ryn-\nson WH Lau. Guided collaborative training for pixel-wise\nsemi-supervised learning. In Computer Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XIII 16, pages 429–445. Springer,\n2020. 2\n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6):84–90, 2017. 2,\n6\n[32] Donghyeon Kwon and Suha Kwak. Semi-supervised seman-\ntic segmentation with error localization network. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9957–9967, 2022. 2\n[33] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao,\nLiwei Wang, and Jiaya Jia. Semi-supervised semantic seg-\nmentation with directional context-aware consistency.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1205–1214, 2021. 2\n[34] Hieu Le, Chen-Ping Yu, Gregory Zelinsky, and Dimitris\nSamaras. Co-localization with category-consistent features\nand geodesic distance propagation. In Proceedings of the\nIEEE International Conference on Computer Vision Work-\nshops, pages 1103–1112, 2017. 2\n[35] Jungbeom Lee, Eunji Kim, and Sungroh Yoon.\nAnti-\nadversarially manipulated attributions for weakly and semi-\nsupervised semantic segmentation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4071–4080, 2021. 2\n[36] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang,\nand Yuchao Dai.\nUncertainty-aware joint salient object\nand camouflaged object detection.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10071–10081, 2021. 1\n[37] Jia Li, Jinming Su, Changqun Xia, Mingcan Ma, and\nYonghong Tian. Salient object detection with purificatory\nmechanism and structural similarity loss. IEEE Transactions\non Image Processing, 30:6855–6868, 2021. 1\n[38] Long Li, Junwei Han, Ni Zhang, Nian Liu, Salman\nKhan, Hisham Cholakkal, Rao Muhammad Anwer, and Fa-\nhad Shahbaz Khan.\nDiscriminative co-saliency and back-\nground mining transformer for co-salient object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7247–7256, 2023. 2,\n7\n[39] Lina Li, Zhi Liu, and Jian Zhang. Unsupervised image co-\nsegmentation via guidance of simple images. Neurocomput-\ning, 275:1650–1661, 2018. 1, 2\n[40] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi\nZhou, and Xi Peng. Contrastive clustering. In Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 35,\npages 8547–8555, 2021. 2\n[41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740–755.\nSpringer, 2014. 6\n[42] Jiawei Liu, Jing Zhang, and Nick Barnes. Semi-supervised\nsalient object detection with effective confidence estimation.\narXiv preprint arXiv:2112.14019, 2021. 2, 3\n[43] Yun Liu, Xin-Yu Zhang, Jia-Wang Bian, Le Zhang, and\nMing-Ming Cheng.\nSamnet:\nStereoscopically attentive\nmulti-scale network for lightweight salient object detection.\nIEEE Transactions on Image Processing, 30:3804–3814,\n2021. 1\n[44] Yunqiu Lv, Bowen Liu, Jing Zhang, Yuchao Dai, Aixuan Li,\nand Tong Zhang. Semi-supervised active salient object de-\ntection. Pattern Recognition, 123:108364, 2022. 3\n[45] Robert Mendel, Luis Antonio De Souza, David Rauber,\nJoao Paulo Papa, and Christoph Palm. Semi-supervised seg-\nmentation based on error-correcting supervision. In Com-\nputer Vision–ECCV 2020: 16th European Conference, Glas-\ngow, UK, August 23–28, 2020, Proceedings, Part XXIX 16,\npages 141–157. Springer, 2020. 2\n[46] Sudhanshu Mittal, Maxim Tatarchenko, and Thomas Brox.\nSemi-supervised semantic segmentation with high-and low-\nlevel consistency. IEEE transactions on pattern analysis and\nmachine intelligence, 43(4):1369–1379, 2019. 2\n[47] Yongri Piao, Jian Wang, Miao Zhang, and Huchuan Lu.\nMfnet: Multi-filter directive network for weakly supervised\nsalient object detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 4136–\n4145, 2021. 1\n[48] Xiaoliang Qian, Yinfeng Zeng, Wei Wang, and Qiuwen\nZhang. Co-saliency detection guided by group weakly su-\npervised learning. IEEE Transactions on Multimedia, 2022.\n3\n[49] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao,\nMasood Dehghan, and Martin Jagersand. Basnet: Boundary-\naware salient object detection.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 7479–7489, 2019. 4\n[50] Xi Shen, Alexei A Efros, Armand Joulin, and Mathieu\nAubry. Learning co-segmentation by segment swapping for\nretrieval and discovery.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5082–5092, 2022. 6, 7, 8, 1, 4\n[51] Yukun Su, Jingliang Deng, Ruizhou Sun, Guosheng Lin,\nHanjing Su, and Qingyao Wu. A unified transformer frame-\nwork for group-based segmentation: Co-segmentation, co-\nsaliency detection and video salient object detection. IEEE\nTransactions on Multimedia, 2023. 2, 7\n[52] Lv Tang, Bo Li, Yijie Zhong, Shouhong Ding, and Mofei\nSong. Disentangled high quality salient object detection. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3580–3590, 2021. 1\n[53] Antti Tarvainen and Harri Valpola. Mean teachers are better\nrole models: Weight-averaged consistency targets improve\nsemi-supervised deep learning results. Advances in neural\ninformation processing systems, 30, 2017. 4\n[54] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, Marc Proesmans, and Luc Van Gool.\nScan:\nLearning to classify images without labels. In European con-\nference on computer vision, pages 268–285. Springer, 2020.\n2\n[55] Chong Wang, Zheng-Jun Zha, Dong Liu, and Hongtao Xie.\nRobust deep co-saliency detection with group semantic. In\nProceedings of the AAAI conference on artificial intelli-\ngence, volume 33, pages 8917–8924, 2019. 6\n[56] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L\nCrowley, and Dominique Vaufreydaz. Self-supervised trans-\nformers for unsupervised object discovery using normalized\ncut. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14543–14553,\n2022. 2, 6, 7, 1, 4\n[57] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L.\nCrowley, and Dominique Vaufreydaz. Self-supervised trans-\nformers for unsupervised object discovery using normalized\ncut. In Conference on Computer Vision and Pattern Recog-\nnition, 2022. 3\n[58] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei,\nWei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi\nLe. Semi-supervised semantic segmentation using unreliable\npseudo-labels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4248–\n4257, 2022. 2, 4\n[59] Zhengxiang Wang and Rujie Liu.\nSemi-supervised learn-\ning for large scale image cosegmentation. In 2013 IEEE In-\nternational Conference on Computer Vision, pages 393–400,\n2013. 3\n[60] Lina Wei, Shanshan Zhao, Omar El Farouk Bourahla, Xi\nLi, Fei Wu, and Yueting Zhuang.\nDeep group-wise fully\nconvolutional network for co-saliency detection with graph\npropagation.\nIEEE Transactions on Image Processing,\n28(10):5052–5063, 2019. 2\n[61] Zhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Han-\nling Zhang, Hao Li, and Rong Jin. Transfgu: a top-down\napproach to fine-grained unsupervised semantic segmenta-\ntion. In Computer Vision–ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,\nPart XXIX, pages 73–89. Springer, 2022. 2, 3\n[62] Siyue Yu, Jimin Xiao, Bingfeng Zhang, and Eng Gee Lim.\nDemocracy does matter: Comprehensive feature mining for\nco-salient object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 979–988, 2022. 1, 2, 4, 5, 6, 7, 8\n[63] Siyue Yu, Bingfeng Zhang, Jimin Xiao, and Eng Gee Lim.\nStructure-consistent weakly supervised salient object detec-\ntion with local saliency coherence. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35, pages\n3234–3242, 2021. 1\n[64] Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, and\nXuelong Li. A review of co-saliency detection algorithms:\nfundamentals, applications, and challenges. ACM Transac-\ntions on Intelligent Systems and Technology (TIST), 9(4):1–\n31, 2018. 2\n[65] Dingwen Zhang, Junwei Han, Chao Li, Jingdong Wang,\nand Xuelong Li. Detection of co-salient objects by looking\ndeep and wide. International Journal of Computer Vision,\n120(2):215–232, 2016. 1, 2, 6, 8\n[66] Dingwen Zhang, Haibin Tian, and Jungong Han.\nFew-\ncost salient object detection with adversarial-paced learn-\ning. Advances in Neural Information Processing Systems,\n33:12236–12247, 2020. 3\n[67] Kaihua Zhang, Mingliang Dong, Bo Liu, Xiao-Tong Yuan,\nand Qingshan Liu.\nDeepacg: Co-saliency detection via\nsemantic-aware contrast gromov-wasserstein distance.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13703–13712, 2021. 1,\n2\n[68] Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen,\nand Qingshan Liu. Adaptive graph convolutional network\nwith attention graph clustering for co-saliency detection. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 9050–9059, 2020. 2, 7\n[69] Ni Zhang, Junwei Han, Nian Liu, and Ling Shao. Summarize\nand search: Learning consensus-aware dynamic convolution\nfor co-saliency detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 4167–\n4176, 2021. 2\n[70] Qijian Zhang, Runmin Cong, Junhui Hou, Chongyi Li,\nand Yao Zhao.\nCoadnet: Collaborative aggregation-and-\ndistribution networks for co-salient object detection.\nAd-\nvances in neural information processing systems, 33:6959–\n6970, 2020. 2, 4\n[71] Zhao Zhang, Wenda Jin, Jun Xu, and Ming-Ming Cheng.\nGradient-induced co-saliency detection. In European Con-\nference on Computer Vision, pages 455–472. Springer, 2020.\n1, 2, 6, 7\n[72] Xiaoju Zheng, Zheng-Jun Zha, and Liansheng Zhuang. A\nfeature-adaptive semi-supervised framework for co-saliency\ndetection.\nIn Proceedings of the 26th ACM International\nConference on Multimedia, MM ’18, page 959–966, New\nYork, NY, USA, 2018. Association for Computing Machin-\nery. 1, 3, 7, 8\n[73] Ziyue Zhu, Zhao Zhang, Zheng Lin, Xing Sun, and\nMing-Ming Cheng.\nCo-salient object detection with co-\nrepresentation purification.\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2023. 2, 7\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-11-11",
  "updated": "2023-11-11"
}