{
  "id": "http://arxiv.org/abs/2311.06914v1",
  "title": "Model-assisted Reinforcement Learning of a Quadrotor",
  "authors": [
    "Arshad Javeed"
  ],
  "abstract": "In recent times, reinforcement learning has produced baffling results when it\ncomes to performing control tasks with highly non-linear systems. The\nimpressive results always outweigh the potential vulnerabilities or\nuncertainties associated with the agents when deployed in the real-world. While\nthe performance is remarkable compared to the classical control algorithms, the\nreinforcement learning-based methods suffer from two flaws, robustness and\ninterpretability, which are vital for contemporary real-world applications. The\npaper attempts to alleviate such problems with reinforcement learning and\nproposes the concept of model-assisted reinforcement learning to induce a\nnotion of conservativeness in the agents. The control task considered for the\nexperiment involves navigating a CrazyFlie quadrotor. The paper also describes\na way of reformulating the task to have the flexibility of tuning the level of\nconservativeness via multi-objective reinforcement learning. The results\ninclude a comparison of the vanilla reinforcement learning approaches and the\nproposed approach. The metrics are evaluated by systematically injecting\ndisturbances to classify the inherent robustness and conservativeness of the\nagents. More concrete arguments are made by computing and comparing the\nbackward reachability tubes of the RL policies by solving the\nHamilton-Jacobi-Bellman partial differential equation (HJ PDE).",
  "text": "Model-assisted Reinforcement Learning of a\nQuadrotor\nArshad Javeed1\n1 Dept. of Automatic Control,\nLund University\nAbstract\nIn recent times, reinforcement learning has produced baffling results\nwhen it comes to performing control tasks with highly non-linear sys-\ntems. The impressive results always outweigh the potential vulnerabil-\nities or uncertainties associated with the agents when deployed in the\nreal-world. While the performance is remarkable compared to the clas-\nsical control algorithms, the reinforcement learning-based methods suffer\nfrom two flaws, robustness and interpretability, which are vital for con-\ntemporary real-world applications. The paper attempts to alleviate such\nproblems with reinforcement learning and proposes the concept of “model-\nassisted” reinforcement learning to induce a notion of conservativeness\nin the agents. The control task considered for the experiment involves\nnavigating a CrazyFlie quadrotor.\nThe paper also describes a way of\nreformulating the task to have the flexibility of tuning the level of conser-\nvativeness via multi-objective reinforcement learning. The results include\na comparison of the vanilla reinforcement learning approaches and the\nproposed approach. The metrics are evaluated by systematically inject-\ning disturbances to classify the inherent robustness and conservativeness\nof the agents. More concrete arguments are made by computing and com-\nparing the backward reachability tubes of the RL policies by solving the\nHamilton-Jacobi-Bellman partial differential equation (HJ PDE).\n1\nIntroduction\nThe classical control approach relies on the ability to accurately model the sys-\ntem and leverage a proven and robust control algorithm. The challenge here is\ncoming up with a decently accurate mathematical description of the system, and\nthe problem can be exacerbated in the case of a non-linear or a high-dimensional\nsystem. In this regard, the reinforcement learning approach, with the machine-\nlearning methodology of learning from the data has yielded unmatched results in\ndiverse application domains while requiring a minimal description of the system\n[Kober and Peters, 2014, Razzaghi et al., 2022, Bai et al., 2023, Arulkumaran et al., 2017].\n1\narXiv:2311.06914v1  [cs.RO]  12 Nov 2023\nReinforcement learning is closely related to optimal control [Sutton and Barto, 2018].\nIn the context of control, the agent tries to learn an optimal control policy by\ninteracting with the environment by receiving rewards for its actions.\nThe\nfundamental idea is to maximize the expected reward received from the en-\nvironment.\nThe learning can be model-based or model-free.\nThe contem-\nporary model-based and model-free algorithms are enriched by deep learn-\ning and neural networks acting as non-linear function approximators.\nThe\n“model” here refers to the model of the environment.\nIn an MDP setting,\nthe model-free approach does not leverage the dynamics of the world in any\nway, instead tries to approximate the reward function and the cost function\nby repeated interaction. There are several algorithms in the model-free cat-\negory. [Mnih et al., 2016, Schulman et al., 2017b] directly try to optimize the\npolicy function parameterized by a deep neural network. [Haarnoja et al., 2018,\nSilver et al., 2014, Fujimoto et al., 2018] are actor-critic variants that employ\nneural networks for the actor and the critic, where the critic approximates the\ntrue cost function using the recursive Bellman equation.\nModel-based rein-\nforcement learning, on the other hand, uses the dynamics (predefined or learns\nsimultaneously) to define the state transition probabilities to seek better ac-\ntions and reduce repeated trial and error in the environment, and the state\ntransition probabilities can be utilized by the algorithm in several ways. MBVE\n[Feinberg et al., 2018] uses model-based learning for better estimation of the\nvalue function. MBMF [Nagabandi et al., 2017] first learns the model or the\nenvironment dynamics using a shallow neural network and then uses a Model\nPredictive Control (MPC). Expert Iteration [Anthony et al., 2017] uses an ex-\npert improvement step to define the imitation learning targets and then resolves\nto imitation learning for faster convergence.\nWith reinforcement algorithms employed in diverse applications, robustness\nand interpretability are imperative. When employed in real-world applications,\nany reckless behavior can have ramifications for the agent itself or the surround-\nings. There have been several attempts to incorporate safety within reinforce-\nment learning.\n[Srinivasan et al., 2020] employ a safety critic that evaluates\nthe safety of the given state-action pair looking at the unsafe experiences of\nthe agent and then transforms the task to safety-constrained MDP. Since the\nunsafe conditions are determined by the data, this may induce a bias. The con-\nstraints themselves are less interpretable due to the fact that a black box model\nor neural network tries to learn from them. SAVED [Thananjeyan et al., 2020]\nis a deep MPC that learns the model by training neural networks observing\nsuboptimal demonstrations. The learned model is then used to impose con-\nstraints on MPC optimization. [Brunke et al., 2021] is a comprehensive review\nof safety-constrained reinforcement learning. MPC approach is one of the only\nnon-linear control strategies that can guarantee the constraint requirements, the\nonly downside is that it is computationally expensive.\nBroaching the topic of robustness, “robustness” can be defined as the abil-\nity of the model to perform consistently (or even better than expected) in di-\nverse situations.\nRobustness does not necessarily imply that the agent per-\nforms better in unknown environments. On the contrary, we want the agent\n2\nto honor certain physical constraints whilst compromising the reward objective,\nbut ultimately accomplishing the task. In the context of reinforcement learn-\ning, this could be quite tricky, as often conflicting objectives are represented by\nthe reward signal. [Glossop et al., 2022] have evaluated the robustness of rein-\nforcement learning algorithms by systematic disturbance injection. A startling\ndiscovery made by them is that subjecting the agent to disturbances during\nthe training phase does not necessarily improve the robustness, i.e. the inher-\nent robustness of agents trained with and without disturbances is about the\nsame. [Mayne et al., 2005] have solved the classical model predictive control\napproach by incorporating bounded disturbances during online optimization.\n[Morimoto and Doya, 2005] propose an adversarial setting (robust reinforce-\nment learning), where an adversarial agent modeling the environment tries to\nimpede the performance, while the RL agent performing the task tries to over-\ncome it. However, [Glossop et al., 2022] report that the robust reinforcement\nlearning methods do not considerably outperform the baseline model.\nIn contrast, the proposed work aims to use a partially specified model or learn\na partial model and then use the model constraints as one of the reward objec-\ntives with the intention of reinforcement learning agent learning to act in a way\nthat would honor the model constraints. Often in reinforcement learning, there\nare conflicting objectives and there is always an added advantage of taking the\nrisk. The primary motivation of the design is the ability to individually tune the\nconstraints during run-time by employing multi-objective reinforcement learn-\ning (depending on the user preferences). Similar to [Glossop et al., 2022], the\nevaluation is done by systematically injecting disturbances during the test phase\nand looking at the agent’s capability to adhere to the defined constraints. The\nproposed work corroborates the implicit robustness of reinforcement learning\nagents, i.e. training with disturbance does not necessarily improve the reward.\nBut at the same time, discusses interesting properties of the proposed method-\nology.\nAll the experiments are performed on a highly non-linear real-world\nsystem, a CrazyFlie quadrotor. More details of the task are presented in the\nsubsequent sections.\n2\nBackground\nReinforcement learning is a type of unsupervised machine learning, where the\nagent learns an optimal policy for the given task by interacting with the environ-\nment. The “feedback signal” or the reward signal is the key to reinforcement\nlearning.\nDuring the training process, the agent’s goal is to simply learn a\nstrategy that would maximize the cumulative reward.\nAt any given time t, the agent could be in an arbitrary state st ∈S and\ncan choose to perform an action at ∈A. As a consequence of the action, the\nagent receives a reward R(st, ar) = rt ∈R. So the cumulative reward for acting\naccording to the policy π starting from state st is Σ∞\nt′=tγt′−tR(st′, π(st′)), where\nγ ∈[0, 1) is called the discount factor. Thus, having defined a reward function\nR, the goal is to learn an optimal policy π∗that maps the states to actions\n3\nπ∗: S →A that yields the best discounted reward.\nIf the above task is done iteratively without assuming any knowledge of the\nsystem or the dynamics of the environment, it is called “model-free” reinforce-\nment learning. On the other hand, if the information about the dynamics or the\nstate transitions is leveraged or learned during the process, it is “model-based\nlearning” RL. The transition probabilities here are the conditional probabilities\nP(st+1|st, at), which resemble the transition probabilities typically encountered\nin a Markov decision problem (MDP).\nIn the above formulation, the reward rt was assumed to be a scalar. Prac-\ntical applications (as well as in the scope of the paper) deal with the tradeoff\nbetween multiple constraints, and there are often conflicting objectives or re-\nwards. Traditionally, a utility function is used to scalarize the reward. A utility\nfunction scalarizes the reward as u : Rd →R and the discounted reward be-\ncomes Σ∞\nt′=tγt′−tu(rt′), and the value function then corresponds to Expected\nScalarized Return (ESR). A common utility function is simply the weighted\naverage, u(rt) = Σiwirti. However, this would muddle the reward objectives\nand make them less interpretable. An alternative approach alluding to multi-\nobjective reinforcement learning is the Scalarized Expected Return (SER) ap-\nproach, where the utility function is applied after the inner expectation, i.e.\nu\n\u0010\nΣ∞\nt′=tγt′−trt′\n\u0011\n. In multi-objective reinforcement learning, the goal is now to\nlearn an array of policies each of which prioritizes the reward objectives differ-\nently. [Hayes et al., 2022] provide a detailed overview of the strategies involved\nin multi-objective learning.\nThe usage of deep neural networks as function approximators to approximate\nthe policies π : RnS →RnA and value functions Q : RnS × RnA →RnR makes\nit feasible to model non-linear dynamical systems and learn effective control\nstrategies [Arulkumaran et al., 2017, Ladosz et al., 2022, Lee and Qin, 2019].\n3\nFormulating the RL Task\nThe non-linear dynamical system considered here is a CrazyFlie quadrotor. The\ntask for the reinforcement learning agent is to learn a policy to navigate the\nquadrotor to the specified destination starting from an initial state s0 (figure\n1).\nHowever, the environment poses a challenge to the agent, there are al-\nways disturbances (forces emulating wind) acting along the X, Y, Z directions\nrandomly. The observation space or the state space consists of the system kine-\nmatics (equation 1), involving the 3D position information, orientation (roll,\npitch, and yaw), and linear and angular velocities along the X, Y, Z axes. The\nstate space S will later be extended with additional states later on.\nS = [x, y, z, r, p, y, vx, vy, vz, wx, wy, wz]\n(1)\nNow, there are several ways of formulating the action space for the naviga-\ntion task. We can have the reinforcement learning agent have complete granular\ncontrol of the quadrotor by letting it control the low-level rotor RPMs or use a\n4\nhierarchical control approach. The hierarchical approach involves the reinforce-\nment learning agent defining high-level objectives for the quadrotor and the\nquadrotor employing a low-level controller such as a PID controller to execute\nthe actions. Employing a trusted low-level controller has a twofold advantage.\nFirstly, it means that the RL agent does not have to learn everything from\nscratch, for instance, simple maneuvers like moving up, down, diagonally, etc.\ncan be performed by the low-level controller confidently, while the high-level\ncontroller is only concerned about taking the appropriate course of actions.\nThis significantly alleviates the algorithmic convergence. Secondly, the overall\nmodel is more interpretable, due to the fact that the high-level actions can be\ninterpreted easily. [Panerati et al., 2021] is a simulation bed supporting both\napproaches. However, the latter approach is run in an open-loop fashion, i.e.\ngiven a target position, the PID controller is used to compute the correspond-\ning rotor RPMs and the implementation involves applying the RPMs for a fixed\nnumber of physics steps, but this is flawed in the way that it does not nec-\nessarily guarantee that the quadrotor executes the actions successfully, i.e. it\nmay fall short or be far off the target. In contrast, the proposed approach exe-\ncutes a closed-loop PID control action to ensure that the quadrotor successfully\nexecutes the high-level action to reasonable accuracy. This makes the overall\naction execution even more interpretable and significantly impacts the algorith-\nmic convergence. Additionally, the action also includes a target velocity for the\nagent, to help compensate for the effect of wind by attempting to move faster.\nFigure 1: Navigation Task\nThus, given a state st = [x, y, z, r, p, y, vx, vy, vz, wx, wy, wz] the action space\nfor the reinforcement learning agent consists of predicting the relative coordi-\nnates and the target velocity, i.e. A = [∆x, ∆y, ∆z, vtarget]. To avoid taking\nlarge strides leading to potentially dangerous situations, the relative coordinates\nhave an upper limit (equation 2). This essentially means that given a state st,\nthe agent can move anywhere within a cube of 0.05 m, traveling a distance\nof\np\n|∆x|2 + |∆y|2 + |∆z|2. Additionally, the target velocity vtarget is a scalar\n5\nquantity with which the agent desires to move to the next location. The individ-\nual velocity components along X, Y, Z directions are computed using equation\n3, the absolute value ensures that the target velocities stay positive and the\nvelocities are clipped by a safe upper limit vmax.\n|∆x| ≤0.05, |∆y| ≤0.05, |∆z| ≤0.05\n(2)\nv =\n\n\nvx\nvy\nvz\n\n= clip\n\nvtarget ∗\n\n\n|∆x|\n|∆y|\n|∆z|\n\n, vmax\n\n\n(3)\nThus, given the current state vector st = [x, y, z, r, p, y, vx, vy, vz, wx, wy, wz]\nand an action at = [∆x, ∆y, ∆z], the ideal next state to be in would be st+1 =\n[x+∆x, y+∆y, z+∆z, r′, p′, y′, v′\nx, v′\ny, v′\nz, w′\nx, w′\ny, w′\nz]. Defining the action space\nthis way serendipitously enables us to define a partial state transition model\n(equation 4), where the covariance matrix of the normal distributions is assumed\nto be the deviations from the ideal state due to the wind disturbances acting\non the system. In an ideal scenario, the PID control would ensure that the\nquadrotor reaches the expected mean of the normal distribution.\n\n\nXt+1\nYt+1\nZt+1\n\n∼N\n\n\n\n\nxt + ∆xt\nyt + ∆yt\nzt + ∆zt\n\n, Σt+1\n\n\n(4)\nEquation 4 now serves as a partial state transition probability model P(st+1|st, at),\nwhich can be thought of as additional set constraints (equations 6 - 8) for the RL\nagent, i.e. the ability to successfully execute the actions by reaching the ideal\nexpected next state. We can now augment our original system state-space with\n3 new quantities xe, ye, ze serving as a notion of “action errors” along X, Y, Z\ndirections (equation 5).\nSe = [S; xe, ye, ze]\n(5)\nxt+1 = xt + ∆x\n(6)\nyt+1 = yt + ∆y\n(7)\nzt+1 = zt + ∆z\n(8)\nAs the task involves reaching a predefined destination sd = [xd, yd, zd], one\nof the obvious reward objectives (given a state action pair (st, π(st))) could be\nthe negative of squared distance from the destination. Equation 9 concerns the\nnavigation part of the task. Thus the objective is to maximize the reward, with\nzero being the highest reward.\nRnav(st, π(st)) = −\n\u0000(xt+1 −xd)2 + (yt+1 −yd)2 + (zt+1 −zd)2\u0001\n(9)\n6\nRe(st, π(st)) = −\n\u0000x2\ne + y2\ne + z2\ne\n\u0001\n(10)\nThe other requirement is to reach the goal by executing an ideal or expected\nbehavior. We can now enforce the constraints (equations 6 - 8), by introducing a\nnew reward objective to drive the action errors to zero. Equation 10 defines the\naction error reward given the extended state (equation 5). Now, it is imperative\nto rightly define these action errors. A straightforward way would be to simply\nhave these errors as the difference between the next observed state and the\nexpected next state from the model description (equation 4), i.e.\nset xe =\nxt+1 −(xt + ∆x), ye = yt+1 −(yt + ∆y), ze = zt+1 −(zt + ∆z).\nHowever,\nthe RL agent can easily exploit the second reward objective (equation 9) by\ntaking a small step to reset the reward to 0 and then followed by a longer\ninconsistent stride. To circumvent this, we can introduce a cumulative error\nterm in the equation (equation 11), where α < 1 is a discount factor and x′\ne\nis the previous error (from the previous timestep).\nThe discount factor was\nobserved to be crucial, which would otherwise induce too many oscillations in\nthe control analogous to pure integral control. Similarly, we define the error\nstates ye, ze.\nxe = Σt\nt′=0αt−t′(xt+1 −(xt + ∆xt))\n= α(xt+1 −(xt + ∆xt)) + αx′\ne\n(11)\nThe reward function from equation 10 resembles a typical LQR cost function,\nhowever, there are stark differences when looked at closely. Consider the partial\nstate of the system encapsulating the position of the quadrotor in space, pt =\n[xt, yt, zt]T . Assuming the dynamics to be of the form pt+1 = Φpt + Γut,\na typical LQR cost would look like ΣtpT\nt Q1pt + uT\nt Q2ut + ptQ12ut.\nIn the\ncase of a stochastic LQR problem, the dynamics would often evolve as pt+1 =\nΦpt + Γut + Gwt, where wt is usually assumed to be a Gaussian noise with zero\nmean, which would yield the same expected cost function as in the deterministic\nLQR case.\nHowever, the point to note here is that the standard LQR cost\npenalizes the magnitude of the control signal ut, while the RL objective function\nproposed in equation 10 does not, instead, the penalty would be proportional to\nthe deviation from the expected next state xt+ut defined by the dynamics. The\ncontroller is free to choose an arbitrarily large control signal as long as it can\nexecute it successfully, this contrasts a standard LQR control approach. This\nwill be evident from experiments carried out in the subsequent sections. The\nsame argument can also be extended to other error states ye, ze.\nGiven that E[pt+1] = Φpt + Γut (since E[wt] = 0), one might wonder if it\nwould be redundant to optimize the agent to counteract the disturbance in the\nfirst place. In this context let et = [xe, ye, ze] be the deviation of the agent\nfrom its ideal state after executing the action ut from state pt as a consequence\nof wt (external wind, actuator noise, noise sensor data, etc). The net effect of et\ncan thus be measured as the Euclidean distance from its ideal next state pt +ut\n(equation 12.\n7\net =\np\nx2e + y2e + z2e\n(12)\nTo analyze the characteristics of the disturbances acting on the system, a\nrandom walk was simulated by sampling the high-level actions uniformly as\nper equation 2 serving as relative coordinates for the quadrotor to move to\nand the low-level PID controller executing the actions while subjecting the\nquadrotor to random step disturbances (figure 3). The resulting error distri-\nbutions are shown in figure 2b. The individual distributions of xe, ye, ze are\nnormally distributed with zero means (figure 2b). The means were estimated\nas E[xe] =\n1\nN Σixei and the covariances as C[xe, ye] =\n1\nN Σixeiyej −E[xe]E[ye]\n(similarly C[xe, ze], C[ye, ze]), table 1.\nIt can be observed that although the\nmeans are close to zero, the covariances seem quite high to ignore. The error\net can now be represented by a multivariate normal distribution using the esti-\nmated means and covariances (table 1) and the expected value of equation 12\ncan be calculated by sampling the probability density function and approximat-\ning the expectation by averaging under the law of large numbers (equation 13).\nThe resulting estimate was found to be 0.08168 (meters), i.e. the quadrotor is\nwas found to be off by a magnitude of 8 cm from its ideal next state. This can\nalso be verified by inspecting figure 2c, which shows the magnitude of the error\n(the difference between the magnitudes of predicted action and the executed\nmaneuver), and the mean of the distribution exactly matches the estimated de-\nviation of 8 cm. It can also be observed that the error can even grow as large\nas a staggering 13 cm per step, which is well above the specified action limits\n(equation 2). Thus, the conclusion is that although the error distributions can\nbe modeled as a zero mean noise acting on the system, it would be unwise to\nnot compensate for them, at least in this particular task.\nFigure 2: Random Walk Experiment\n(a) Trajectory\n(b) Error Distribution\n(c)\nError\nMagni-\ntude\n(|action|2\n−\n|executed|2,\nin\nme-\nters,\nthe lower the\nbetter)\n8\nTable 1: Error Statistics\nCovariances\nMeans\nxe\nye\nze\nxe\n0.07218\n0.02731\n0.01915\n−0.001759\nye\n0.02731\n0.03799\n0.01650\n−0.002713\nze\n0.01915\n0.01650\n0.03614\n0.0055015\nE [et] =\nZ\net\net P(et) det\n=\nZ\nxe\nZ\nye\nZ\nze\np\nx2e + y2e + z2e P(xe, ye, ze) dxe dye dze\n≈lim\nN→∞\n1\nN\nX\ni\np\nx2e + y2e + z2e ,\n\n\nxe\nye\nze\n\n∼N(µe, Σe)\n(13)\n4\nSingle-objective Reinforcement Learning\nThe goal here is to learn a strategy to navigate the quadrotor to a fixed point\nin space. The quadrotor starts off at an initial state s0 and learns to navigate\nto the point [0, 0, 1] during the training phase. To test the theory and claims\nmade in the previous sections, a rigorous comparison is made on the follow-\ning approaches: (i) Baseline model: This is a vanilla reinforcement learning\nmodel formulated for the navigation task. It uses the kinematics state space\n(equation 1). The reward function only consists of the navigation reward (equa-\ntion 9) and the agent is not subjected to any form of disturbances during the\ntraining phase. (ii) Baseline model with disturbances: The formulation is very\nsimilar to the baseline model (i), except that the model experiences distur-\nbances during the training phase. (iii) Model with error correction (proposed\napproach for robustness): This is a model with disturbances (ii) but uses the\nextended state space (equation 11) and also receives a feedback regarding the\nstate errors which was introduced earlier(equation 10). Since the model sees\ntwo distinct reward objectives, the expected scalarized return (ESR) approach\nis employed by defining a utility function to scalarize the rewards prior to the\nexpectation (equation 14). (iv) An LSTM model: The problem setting is ex-\nactly similar to (iii) except that the reinforcement learning agent has an LSTM\narchitecture. Having the LSTM architecture dissents from the rest of the op-\ntions adhering to the MDP assumptions, the motivation for including the LSTM\narchitecture in the comparison was to explore the possibility of the RL agent\nlearning to behave robustly given a sequence of previous states, i.e. whether\na pure deep learning architecture can render the proposed method obsolete.\nTo draw a fair comparison, all of the above approaches use the exact same\nunderlying reinforcement learning algorithm, Proximal Policy Gradient (PPO)\n9\n[Schulman et al., 2017b]. PPO is a stochastic policy gradient algorithm that im-\nproves on the prior policy gradient implementations like Trusted Region Policy\nOptimization [Schulman et al., 2017a].\nThe algorithmic implementations are\nmade available by stable-baselines3 [Raffin et al., 2021].\nU\n\u0012\u0014Rnav\nRerr\n\u0015\u0013\n= wT\n\u0014Rnav\nRerr\n\u0015\n= [1 0.5]\n\u0014Rnav\nRerr\n\u0015\n(14)\nThe disturbances during the training phase are feeble step/pulse distur-\nbances acting along one or all of the X, Y, Z directions emulating a wind force.\nFigure 3 shows the disturbances applied during the training phase, the magni-\ntude and/or the direction of the disturbances change every 20 simulation steps,\nwith the intention of simulating real-world disturbances that are not completely\nrandom and for the RL agent to make ad-hoc adjustments to adapt to the dis-\nturbances. During the evaluation phase, the magnitude and the direction of\nthe step disturbance remain fixed throughout and the performance is evaluated\nagainst disturbances of varying magnitudes with much stronger intensities. The\ninitial state is set to [0, 0, 0] at the start of every training episode. Although\nthe RL agent would benefit from random initialization, the intention is to keep\nthe comparison fair for all the approaches by eliminating the randomness in the\nenvironment. However, the initial state is offset during the evaluation phase to\nlook at the agent’s behavior to work with and against the disturbances acting\non it. Table 3 lists the training hyperparameters.\nFigure 3: Training Disturbances\nX, Y, Z disturbances applied during training. The disturbances are flipped\nevery 20 simulation steps.\n10\nTable 2: Training Hyperparameters\nHyperparameter\nValue\nEpisode Length (in secs)\n2\nInitial XYZ state\n[0, 0, 0]\nDisturbance |Xmax|\n0.05\nDisturbance |Zmax|\n0.05\nDisturbance |XY Zmax|\n0.025\nDisturbance Flip Frequency\n20 simulation steps\nNumber of Timesteps\n1,000,000\nNeural Net Policy Architecture (hidden layers)\n[64, 64]\nMultiple evaluation metrics are formulated to effectively assess and compare\nthe pros and cons of the different approaches described above. We obviously\nhave the negative of squared reward and a few other metrics to evaluate the\nrobustness. The distance traveled Σt|pt −pp+1|2 is measured as the physical\ndistance traveled by the agent while executing the trajectory as opposed to\nthe typical cumulative RL reward. Here pt denotes the state comprising of the\nXYZ coordinates of the quadrotor pT = [xt, yt, zt]. The smoothness of the\ntrajectory is measured as the mean of the second derivative, i.e. the rate of\nchange of direction of the quadrotor,\n1\nN\nR\n|τ ′′(t)|2dt. Ideally, we would desire\nthe system to navigate in a smooth trajectory, as opposed to a zig-zag or jagged\ntrajectory. As the ascent phase of the quadrotor is crucial prior to it reaching\nthe destination and start hovering, the average ascent step is computed during\nthe initial few timesteps of the trajectory. A plot of intended minus the executed\ndistance (|ut|2 −|pt −pt+1|2) is tracked, ideally, we would like this to be zero,\ni.e. the quadrotor should be able to execute the actions successfully. Finally,\nthe “converged” metric indicates whether the quadrotor managed to reach the\ndestination within a 10 cm accuracy.\nFigures 4a - 4c plot the evaluated reward functions for different combinations\nand magnitudes of XYZ disturbances. The evaluations were captured by set-\nting the disturbances to be constant throughout the episode, unlike the training\nphase where the disturbances change directions randomly. The curves are asym-\nmetric due to the fact that the initial state of the drone is moved to [2, 0, 0] for\nevaluation. So, the drone is assisted by the wind when the disturbance is along\nthe negative X direction and opposed when positive. The first thing to notice is\nthat the baseline model, which was not exposed to any form of disturbance dur-\ning the training phase outperforms the rest of the pack by a considerable margin\nin terms of the cumulative RL reward, albeit the other models achieved a higher\nreward in a few extreme cases (figure 4b). This finding corroborates the claims\nmade by [Glossop et al., 2022]. However, the reward functions do not paint the\ncomplete picture. Figures 5a - 5c plot the evaluation trajectories of policies for\ndifferent cases of evaluation. Looking closely at the trajectories we find that the\nbaseline model takes longer strides and manages to reach the in fewer RL steps,\nwhile the rest of the pack takes a considerably more number of RL steps during\n11\nascent. This explains the reason why the baseline model achieves the best RL\nreward, as the RL reward is a cumulative discounted sum of the reward, the\nmore RL steps the policy executes, the more negative reward is accumulated.\nWhile achieving a high RL reward is always the goal in reinforcement learning,\nit can also have adverse consequences. The figures 5d - 5f spotlight the underly-\ning problem, the baseline model appears to be a true outlier. As per the design\nformulation 2, the maximum possible relative distance the agent was supposed\nto execute was\n√\n0.052 + 0.052 + 0.052 = 0.0866 m or 8.66 cm. However, the\nbaseline model (green curve) seems to be encountering an enormous amount of\naction error, starting off from a staggering 30 cm and then gradually decreas-\ning to 0 cm after the ascent period. An error of 30 cm essentially means that\nthe quadrotor was off by 30 cm from where it planned to go by executing the\nparticular action. In contrast, the models trained with disturbances (the rest of\nthe pack) appear to be much more stable, and the proposed model (“dist-err”)\nin particular appears to do a better job, implying that the agent was able to\nexecute the actions with sufficient certainty. This further supports the assump-\ntions made by simulating the random walk experiment earlier, that ignoring\nthe effects of disturbances by assuming the normal nature of the errors (figure\n2b) would not be the best thing to do, because although the expected values of\nerrors are zero, the effect on the quadrotor is actually E[\np\nx2e + y2e + z2e] which\nwas found to be much larger and not equal to zero. Table 3 further lists more\ninsightful metrics captured and spotlights the proposed method. Although the\n“baseline” model outperformed in terms of the RL reward, surprisingly it does\nnot hold up in terms of the actual physical distance traveled by the quadrotor.\n“dist-err” manages to reach the destination of [0, 0, 1] from [2, 0, 0] by traveling\na significantly shorter distance compared to the baseline. The rest of the models\nperform similarly, but “dist-err” appears to have an edge in most cases. This\ncan also be viewed by inspecting the trajectory plots (figures 5a - 5c), where\nthe trajectory appears to be much more focused for “dist-err”. The “dist-err”\nmodel also appears to be much smoother, this is evident from the fact that the\nmean of the magnitude of the second derivate is much smaller (group 2, table\n3). All the models trained with disturbance seem to perform similarly when it\ncomes to the average ascent step, albeit “dist-err-u” with a slight edge. How-\never, the “baseline” again appears to be an outlier, taking significantly longer\nstrides during the ascent phase.\nIt can also be observed that the “dist-err”\nmodel appears to converge to the goal under all the experiments except the last\none with a disturbance of 0.100 acting along a positive Z direction. While the\nrest of the models fail to reach the goal under multiple scenarios.\nThe conclusion we draw from these results is that a higher RL reward does\nnot necessarily imply the inherent robustness of the models to all aspects of\nevaluation. There could be severe consequences or adverse effects surrounding\nthe process dynamics or other relevant metrics to look at to truly assess the\nrobustness. Thus, enforcing the dynamics by a reward function (model-assisted)\nlearning appears to have some positive effect with regards to the certainty of\nRL algorithms.\n12\nFigure 4: Evaluation Rewards.\nModels compared: i. lstm: an LSTM agent. ii. dist-err: the proposed arch for\nsingle-objective, receives feedback for the action errors. iii. dist-err-u: similar\nto LQR cost, agent receives a negative L2 reward of the control/action taken. iv.\ndist: baseline model subjected to disturbances during training. v. baseline:\nthe baseline model.\n(a) Disturbance along X\n(b) Disturbance along Z\n(c) Disturbance along XYZ\n13\nFigure 5: Evaluation Trajectories and Action Errors During Ascent.\nThe figures on top plot the trajectories of the agent during the evaluation phase.\nThe figures at the bottom compare the action errors |action|2 −|executed|2 (in\nmeters, the lower the better).\nModels compared: i. lstm: an LSTM agent. ii. dist-err: the proposed arch for\nsingle-objective, receives feedback for the action errors. iii. dist-err-u: similar\nto LQR cost, agent receives a negative L2 reward of the control/action taken. iv.\ndist: baseline model subjected to disturbances during training. v. baseline:\nthe baseline model.\n(a)\nDisturbance:\n-0.050\nalong X\n(b)\nDisturbance:\n+0.050\nalong Z\n(c) Disturbance:\n+0.075\nalong XYZ\n(d) Disturbance:\n-0.050\nalong X\n(e) Disturbance:\n+0.050\nalong Z\n(f) Disturbance:\n+0.075\nalong XYZ\n14\nTable 3: Single-objective Navigation Metrics.\nModels compared: i. lstm: an LSTM agent. ii. dist-err: the proposed arch for single-objective, receives feedback for the\naction errors. iii. dist-err-u: similar to LQR cost, agent receives a negative L2 reward of the control/action taken. iv. dist:\nbaseline model subjected to disturbances during training. v. baseline: the baseline model.\nDistance Travelled\nSmoothness\nAverage Ascent Step (L2 Norm of Action)\nConverged\nDirection\nMag. Dist.\nlstm\ndist-err\ndist-err-u\ndist\nbaseline\nlstm\ndist-err\ndist-err-u\ndist\nbaseline\nlstm\ndist-err\ndist-err-u\ndist\nbaseline\nlstm\ndist-err\ndist-err-u\ndist\nbaseline\nx\n-0.050\n6.599416\n6.132900\n5.714949\n6.111227\n6.618811\n0.009211\n0.008646\n0.006601\n0.007670\n0.032359\n0.138675\n0.099394\n0.173148\n0.123149\n0.383917\nYes\nYes\nYes\nYes\nYes\nx\n-0.100\n6.499861\n5.331071\n5.918821\n5.719601\n5.798424\n0.007364\n0.005405\n0.006002\n0.005149\n0.022950\n0.156945\n0.114534\n0.168128\n0.112654\n0.411196\nYes\nYes\nYes\nYes\nYes\nx\n0.000\n7.603805\n5.364380\n5.280013\n6.599345\n5.843086\n0.011430\n0.005480\n0.005477\n0.011106\n0.029162\n0.155782\n0.129354\n0.133528\n0.099125\n0.380344\nYes\nYes\nYes\nYes\nYes\nx\n0.050\n5.256427\n4.934614\n5.306519\n7.602165\n6.909383\n0.040803\n0.003857\n0.004457\n0.015453\n0.036542\n0.968272\n0.099889\n0.164785\n0.099618\n0.378084\nNo\nYes\nYes\nYes\nYes\nx\n0.100\n8.016002\n5.009077\n5.364408\n7.573557\n4.562196\n0.232892\n0.003696\n0.004433\n0.015047\n0.033325\n1.349194\n0.125718\n0.173467\n0.120015\n0.599928\nNo\nYes\nYes\nYes\nNo\nxyz\n-0.075\n6.836447\n5.335730\n5.959134\n6.811069\n6.524232\n0.010284\n0.005464\n0.007842\n0.012476\n0.034549\n0.131541\n0.111868\n0.127855\n0.116927\n0.384197\nYes\nYes\nYes\nYes\nYes\nxyz\n-0.150\n7.568094\n6.465555\n6.239807\n6.259547\n15.359850\n0.011566\n0.009653\n0.009864\n0.008517\n0.117175\n0.126255\n0.126986\n0.148956\n0.117847\n0.372258\nYes\nYes\nYes\nYes\nYes\nxyz\n0.000\n7.425029\n4.861160\n4.859477\n6.655158\n5.883497\n0.010969\n0.003722\n0.004404\n0.010254\n0.026074\n0.152020\n0.098812\n0.157771\n0.129522\n0.379013\nYes\nYes\nYes\nYes\nYes\nxyz\n0.075\n7.115747\n4.965403\n5.040673\n6.277422\n7.427389\n0.009977\n0.003966\n0.004081\n0.008231\n0.049423\n0.167109\n0.138969\n0.141843\n0.115283\n0.376754\nYes\nYes\nYes\nYes\nYes\nxyz\n0.150\n1.359120\n4.844337\n4.875827\n5.126340\n0.883666\n0.014639\n0.003606\n0.002904\n0.003983\n0.001789\n0.271770\n0.149187\n0.187735\n0.133928\n0.176308\nNo\nYes\nNo\nYes\nNo\nz\n-0.050\n7.723198\n5.668666\n6.375411\n8.300653\n7.623795\n0.019203\n0.008097\n0.012004\n0.018782\n0.048181\n0.121373\n0.096880\n0.157460\n0.098883\n0.393317\nYes\nYes\nYes\nYes\nYes\nz\n-0.100\n0.015911\n10.773465\n11.093853\n11.260336\n12.575253\n0.000002\n0.040536\n0.043189\n0.043243\n0.066770\n0.002792\n0.073959\n0.118678\n0.079583\n0.370222\nNo\nYes\nNo\nYes\nYes\nz\n0.000\n7.511052\n4.978000\n5.233706\n6.299238\n6.076239\n0.011097\n0.004251\n0.005609\n0.007886\n0.027799\n0.148883\n0.121492\n0.136734\n0.105076\n0.402755\nYes\nYes\nYes\nYes\nYes\nz\n0.050\n6.084520\n4.561033\n4.835022\n5.856955\n5.789299\n0.006865\n0.003129\n0.002743\n0.006014\n0.030311\n0.164402\n0.154412\n0.178154\n0.143068\n0.391134\nYes\nYes\nYes\nYes\nYes\nz\n0.100\n1.208510\n11.522041\n5.586891\n4.752776\n1.591956\n0.007685\n0.635960\n0.004139\n0.002017\n0.018765\n0.168021\n0.143042\n0.199129\n0.154843\n0.287403\nNo\nNo\nNo\nNo\nNo\n15\n5\nReachability Analysis of Single-objective Re-\ninforcement Learning\nThis section focuses on further analysis of the single-objective reinforcement\nlearning solutions described in the previous sections. Reinforcement learning-\nbased control can be thought of as an optimal control strategy, which means\nthat the RL agent predicts optimal action given a specific state, and ultimately\nover time, the agent will meet the reward constraint. This also means that the\nclassical control verification methods can no longer be applied for stability and\nrobustness. However, Hamilton-Jacobi (HJ) reachability analysis is a promi-\nnent method when it comes to formal verification of optimal control strategies.\n[Bansal et al., 2017] provide an extensive overview of both conventional and\ncontemporary ways of performing reachability analysis. Typically, reachability\nanalysis involves computing a “reachability set”, given a goal state, a reachabil-\nity set is a set of all possible states from which the system can reach the given\ngoal state exactly at time τ. A backward reachability tube is defined as the set\nof all possible states from which the system can reach the goal state within a\nduration of time τ. If the goal state is a desirable destination, we would like\nthe control policy to have a large set, if on the other hand, the destination\nis unsafe, we desire our policy to have a minimal set or preferably a null set\nthat can avert the unsafe states. There are several ways of computing these\nsets for optimal control policies [Chen et al., 2017, Kaynama and Oishi, 2013,\nNiarchos and Lygeros, 2006, Rubies-Royo and Tomlin, 2017], that make com-\nputation of reachability sets feasible for non-linear systems.\nComputing the backward reachability set (BRS) or backward reachability\ntube (BRT) via the level-set method involves solving the Hamilton-Jacobi-\nBellman partial differential equation. Given an initial state x and a time period\nτ, the Bellman equation can be written as equation 15, where C is the cost\nfunction C : Rd ×Ra →R and D is the terminal cost function D : Rd →R. The\nHJ partial differential equation can be written by Taylor expanding the Bell-\nman equation as in equation 16, where ∇V = ∂V\n∂x denotes the spatial derivative,\ni.e. the gradient of the cost function with respect to the state variables, and\nF denotes the system dynamics, i.e. the next state given the current state x\nand action u (derived from the policy π). If we have an adversarial input or\ndisturbance d acting on the system, we would be interested in minimizing the\nvalue function with respect to u while maximizing the cost with respect to the\nadversarial disturbance d (equation 17), meaning we would like the control in-\nput to minimize the value function making it feasible for the system to reach\nthe goal starting from an initial state, while the adversarial attempts to make\nit harder for the agent to reach its goal.\nV (x, t) = min\nu\nZ τ\nt=0\nC(xt, ut)dt + D(xτ)\n(15)\nV (x + dx, t + dt) = dV (x, t)\ndt\n+ min\nu∼π ∇V · F(x, u) + C(x, u)\n(16)\n16\nV (x + dx, t + dt) = dV (x, t)\ndt\n+ min\nu∼π max\nd\n∇V · F(x, u, d) + C(x, u)\n(17)\nIn our case, the policy π is given by a reinforcement learning agent which\nis already optimized with respect to the reward function (equations 9 and 10),\nso we are only interested in studying the effect of d by looking at the tolerance\nof the policy π. To approximate the BST for our RL policy, we can formulate\nan experiment where π emulates an optimal control policy π∗and d can then\nemulate the notion of action errors from the earlier experiments carried out.\nGoing back to the quadrotor navigation example, where the objective is to\nnavigate and hover at the destination x, y, z = (0, 0, 1). Consider the partial\nstates of the system relevant to the navigation task pt = [xt, yt, zt] (ignoring\nthe linear and angular velocities which can be assumed to be constant, equa-\ntion 1). As before, the control signal or the action is the relative coordinate\nfor the quadrotor to move to, at = ut = [∆x, ∆y, ∆z].\nAnd let dt denote\nthe deviation or the noise acting on the system. Since the destination coordi-\nnate is (0, 0, 1) we can formulate the system dynamics for the optimal control\nproblem by a set of linear equations (equation 18), where the reference signal\nr = [rx, ry, rz] = [0, 0, 1] and dx, dy, dz are assumed to be Gaussian noise. Equa-\ntion 18 represents the F(x, u, d) term in the Hamilton-Jacobi-Bellman partial\ndifferential equation (equation 17). The term ∇V · F can further be decom-\nposed as ∇V · F(x, u, d) = ∇V · F1(x, u) + ∇V · F2(x, d) as the disturbances\nact additively on the system (equation 18). This yields equation 19, where F1\nis the ideal dynamics of the system given the state and action, while F2 is the\ndynamics capturing the disturbance.\nxt+1 = rx −xt + ∆x + dx = 0 −xt + ∆x + dx\nyt+1 = ry −yt + ∆y + dy = 0 −yt + ∆y + dy\nzt+1 = rz −zt + ∆z + dz = 1 −zt + ∆z + dz\n(18)\nV (x+dx, t+dt) = dV (x, t)\ndt\n+min\nu∼π ∇V ·F1(x, u)+max\nd\n∇V ·F2(x, d)+C(x, u) (19)\nThe RL policy π was trained to maximize the negative of the distance from\nthe reference point (equation 9). Thus, the value function V can simply rep-\nresent the distance from the reference point r = [rx, ry, rz]. The solution to\nthe optimal control problem (equation 18) is (xt, yt, zt) = (rx, ry, rz). As the\nvalue function is known, we can use an optimal bang-bang control policy π∗to\nemulate the optimal RL policy π and evaluate the robustness by imposing the\ndeviations d experienced by the different RL agents. This would make it fea-\nsible to approximate the backward reachability tube using the classic level-set\nmethod. The assumption we make here is that the optimal RL policy under no\ndisturbance is similar to a bang-bang control policy, which is valid because the\n17\nbang-bang policy optimizing the HJ PDE follows the optimal path and so does\nthe RL agent when no disturbances are acting on the system.\nGiven the state pt = [xt, yt, zt], the value function is given by equation 20.\nThe spatial gradients of the value function is the partial derivatives with respect\nto the state variables (equation 21). A bang-bang control policy to minimize the\nHJ PDE will be to use a step in the opposite direction of the spatial gradient\n(equation 22), and the optimal disturbance inhibiting the agent from reaching\nits destination or attempting to maximize the HJ PDE is given by equation 23.\nFigures 6a and 6b visualize the optimal actions, we observe that the bang-bang\npolicy drives the agent to the destination (0, 0, 1).\nV (pt) = V ([xt, yt, zt]) =\nq\nx2\nt + y2\nt + (1 −zt)2\n(20)\n∇V (pt) =\n\n\n∂V\n∂x\n∂V\n∂y\n∂V\n∂z\n\n=\n1\np\nx2\nt + y2\nt + (1 −zt)2\n\n\nxt\nyt\n−(1 −zt)\n\n\n(21)\nui =\n(\n−|∆umax| if ∂V\n∂xi > 0\n|∆umax| if ∂V\n∂xi < 0\n(22)\ndi =\n(\n|∆dmax| if ∂V\n∂xi > 0\n−|∆dmax| if ∂V\n∂xi < 0\n(23)\nFigure 6: Bang-bang Policy\n(a)\nOptimal\nControls\nXY\nplane. Destination (0, 0)\n(b)\nOptimal\nControls\nXZ\nplane. Destination (0, 1)\nThe next objective is then to model the disturbances d for the linearized\nsystem. This disturbance should emulate what the RL agent experiences dur-\ning the evaluation phase. For the particular experiment, we pick a step wind\ndisturbance (of magnitude 0.075 N along XY Z) acting on the quadrotor during\nthe episode and simply capture the errors during the flight, the error is just\nthe difference between the intended position of the quadrotor by executing the\naction and where it really ended up, i.e. xt+1 −(xt +ut). The statistics of these\n18\nerrors are listed in tables 4 and 5, clearly the covariances of the dist-err model\n(proposed model) are significantly lower, this essentially means that the model\nwas able to execute its actions more concretely compared to the baseline model.\nWe will now use these disturbances (1 standard deviations apart) as dx, dy, dz\nwhile computing the BRT. This would reveal the worst-case tolerances of the\nmodels and enable us to compare the robustness.\nTable 4: Error Statistics - baseline model\nCovariances\nMeans\nxe\nye\nze\nxe\n0.05963\n0.02477\n0\n−0.01970\nye\n0.02477\n0.09416\n0.01642\n−0.0271\nze\n0\n0.01642\n0.08471\n0.01089\nTable 5: Error Statistics - dist-err model\nCovariances\nMeans\nxe\nye\nze\nxe\n0.03474\n0\n0\n−0.02000\nye\n0\n0.02833\n0.00691\n0.03506\nze\n0\n0.00691\n0.02764\n0.010255\nThe target set is a sphere of radius 10 cm centered at [0, 0, 1]. As we are\ninterested in evaluating the ability of the agent to reach the target set, the\nBRT computed at the end will comprise of all the possible initial states from\nwhich the agent can navigate to the specified target set. The time duration for\ncomputing the BRT is set to τ = 3 s, meaning we look backward starting from\nour target set for all the possible reachable sets within a duration of 3s. The\nrobustness of the proposed model is clearly evident (figures 7a and 7b). The\nreachability tube of the proposed model is much larger, implying that the agent\ncan reach the target set from a wider range of initial states within the span of\n3 seconds. Figure 7c compares the 2D slices of the BRTs at z = 1. Although\nthese tubes are a crude approximation due to the fact that the optimal RL\npolicy was emulated by an optimal bang-bang policy, we get a tangible measure\nof robustness by computing the BRTs. These results further corroborate the\nobservations from the previous section. For instance, the “converged” column in\ntable 3 represents the true simulations of the trained RL agents, and we see that\nthe baseline model fails to converge in two additional cases where the proposed\nmodel (dist-err) has no problem reaching the target.\n6\nMulti-objective Reinforcement Learning\nInducing the notion of conservativeness can have a positive impact on inter-\npretability, but it may not be always desired for the model to act conserva-\n19\nFigure 7: Backward Reachability Tubes (BRT).\nModels compared: i. baseline: the baseline model. ii. dist-err: the proposed\narch for single-objective, receives feedback for the action errors.\n(a)\n3D\nBRT\nbaseline\nmodel\n(b)\n3D\nBRT\ndist-err\nmodel\n(c) Comparison of BRTs,\n2D slice (z = 1)\ntively. In certain applications, the agent may be expected to take a risk and\nachieve the best RL reward, on the other hand, in the context of safety-critical\napplications, we would want the agent to adhere to certain physical constraints.\nIn this regard, it would be desirable to have the ability to tune the level of\nconservativeness required and dynamically change it during the runtime. So a\nmulti-objective RL problem can be formulated where the primary objective is\ntask-specific and additional reward objectives can be introduced to constrain the\nagent. We would then employ a scalarized expected reward (SER) approach to\nlearn an array of RL policies each of which prioritizes the rewards differently. A\ntypical SER reward for multi-objective RL similar to equation 24, where w ∈W\ncan be a finite set of weight vectors for the agents to weigh the rewards. Thus,\nwe can learn an array of policies that can differently prioritize the conservative-\nness Rerr. And the key difference to the single-objective RL task (equation 14)\nis that the utility function would be applied after the expectation.\nU\n\u0012\nE\n\u0014Rnav\nRerr\n\u0015\u0013\n= wT\n\u0014E [Rnav]\nE [Rerr]\n\u0015\n(24)\n7\nConnection to Model-based Reinforcement Learn-\ning\nAll the results discussed thus far required formulating a reward objective that\nwould penalize the agent for deviating from the normal or expected behavior.\nThe combination of hierarchical control and the specific way of formulating the\nactions serendipitously made it feasible to compose an error model without any\nadditional effort. However, in cases where such a formulation is not as obvious,\nwe can leverage model-based reinforcement learning methods. Typically, the\n20\nmodel-based reinforcement learning methods learn the dynamics of the system\nby fitting a non-linear model (often a neural network) that can predict the\nnext state given the current state, i.e. st+1 = ˆf(st, at) and leverage it later\non in several ways.\nBy building up a dataset D comprising of observations\n(st, at, st+1) under ideal conditions, we can thus learn the “ideal” dynamics of\nthe system. Once ˆf is learned, the rest of the procedure would be exactly the\nsame, i.e. introducing a reward objective for the system to behave conservatively\nas per the ideal dynamics. When the dynamics span a high-dimension space, it\nshould be discretionary to choose a subset of states that make sense, as in the\ntask of quadrotor discussed in the paper, although the actual state-space of the\nquadrotor spanned 15 dimensions, only 3 of the states were absolutely vital for\ninterpretability. Thus, we could formulate a reward objective for such states.\n8\nConclusion\nRobustness and interpretability are some of the key challenges for contemporary\nreinforcement learning-based applications, and the paper presents a plausible\napproach to alleviate the issues in the context of standard reinforcement learn-\ning. The approach of introducing a secondary reward objective constraining\nthe dynamics of the agent was found to have a positive impact after rigorous\ntesting and comparison. Although the constrained agent the new objective did\nnot necessarily outperform in terms of the RL reward, it was observed that the\nagent exhibited some very nice physical properties under evaluation and was\nfound to have an edge when looking at a relevant set of physical metrics for the\ntask involved.\nIt is important to note that the approach discussed does guarantee that\nthe process dynamics are adhered to always, unlike MPC-based solutions that\nguarantee a feasible solution, this is a distinguishing quality of model predictive\ncontrol. However, MPC has its own set of pros and cons compared to rein-\nforcement learning. This work attempts to improve and elevate the problems\nsurrounding classical RL in optimal control-based applications. While the task\nconsidered was restricted to a navigation problem of a quadrotor, the ideas\npresented can be extended to any non-linear control problem, formulating a hi-\nerarchical control task with a partial dynamics model would be ideal, in the case\nwhere the dynamics are known, a non-linear model could be fit on the data to\nestimate the crucial states for the problem, which could then serve as a ground\ntruth for the RL task.\nReferences\n[Anthony et al., 2017] Anthony, T., Tian, Z., and Barber, D. (2017). Thinking\nfast and slow with deep learning and tree search.\n21\n[Arulkumaran et al., 2017] Arulkumaran, K., Deisenroth, M. P., Brundage, M.,\nand Bharath, A. A. (2017). Deep reinforcement learning: A brief survey. IEEE\nSignal Processing Magazine, 34(6):26–38.\n[Bai et al., 2023] Bai, H., Cheng, R., and Jin, Y. (2023). Evolutionary rein-\nforcement learning: A survey. Intelligent Computing, 2.\n[Bansal et al., 2017] Bansal, S., Chen, M., Herbert, S., and Tomlin, C. J. (2017).\nHamilton-jacobi reachability: A brief overview and recent advances.\n[Brunke et al., 2021] Brunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S.,\nPanerati, J., and Schoellig, A. P. (2021). Safe learning in robotics: From\nlearning-based control to safe reinforcement learning.\n[Chen et al., 2017] Chen, M., Herbert, S. L., Vashishtha, M. S., Bansal, S., and\nTomlin, C. J. (2017). Decomposition of reachable sets and tubes for a class\nof nonlinear systems.\n[Feinberg et al., 2018] Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonza-\nlez, J. E., and Levine, S. (2018). Model-based value estimation for efficient\nmodel-free reinforcement learning.\n[Fujimoto et al., 2018] Fujimoto, S., van Hoof, H., and Meger, D. (2018).\nAddressing function approximation error in actor-critic methods.\nCoRR,\nabs/1802.09477.\n[Glossop et al., 2022] Glossop, C. R., Panerati, J., Krishnan, A., Yuan, Z., and\nSchoellig, A. P. (2022). Characterising the robustness of reinforcement learn-\ning for continuous control using disturbance injection.\n[Haarnoja et al., 2018] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.\n(2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement\nlearning with a stochastic actor. CoRR, abs/1801.01290.\n[Hayes et al., 2022] Hayes, C. F., R˘adulescu, R., Bargiacchi, E., K¨allstr¨om, J.,\nMacfarlane, M., Reymond, M., Verstraeten, T., Zintgraf, L. M., Dazeley, R.,\nHeintz, F., Howley, E., Irissappane, A. A., Mannion, P., Now´e, A., Ramos,\nG., Restelli, M., Vamplew, P., and Roijers, D. M. (2022). A practical guide\nto multi-objective reinforcement learning and planning. Autonomous Agents\nand Multi-Agent Systems, 36(1).\n[Kaynama and Oishi, 2013] Kaynama, S. and Oishi, M. (2013).\nA modified\nriccati transformation for decentralized computation of the viability kernel\nunder LTI dynamics. IEEE Transactions on Automatic Control, 58(11):2878–\n2892.\n[Kober and Peters, 2014] Kober, J. and Peters, J. (2014). Reinforcement Learn-\ning in Robotics: A Survey, pages 9–67. Springer International Publishing,\nCham.\n22\n[Ladosz et al., 2022] Ladosz, P., Weng, L., Kim, M., and Oh, H. (2022). Explo-\nration in deep reinforcement learning: A survey. Information Fusion, 85:1–22.\n[Lee and Qin, 2019] Lee, Y. L. and Qin, D. (2019). A survey on applications\nof deep reinforcement learning in resource management for 5g heterogeneous\nnetworks. In 2019 Asia-Pacific Signal and Information Processing Association\nAnnual Summit and Conference (APSIPA ASC), pages 1856–1862.\n[Mayne et al., 2005] Mayne, D., Seron, M., and Rakovi´c, S. (2005).\nRobust\nmodel predictive control of constrained linear systems with bounded distur-\nbances. Automatica, 41(2):219–224.\n[Mnih et al., 2016] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. (2016).\nAsynchronous\nmethods for deep reinforcement learning.\n[Morimoto and Doya, 2005] Morimoto, J. and Doya, K. (2005). Robust Rein-\nforcement Learning. Neural Computation, 17(2):335–359.\n[Nagabandi et al., 2017] Nagabandi, A., Kahn, G., Fearing, R. S., and Levine,\nS. (2017).\nNeural network dynamics for model-based deep reinforcement\nlearning with model-free fine-tuning.\n[Niarchos and Lygeros, 2006] Niarchos, K. N. and Lygeros, J. (2006). A neural\napproximation to continuous time reachability computations. In Proceedings\nof the 45th IEEE Conference on Decision and Control, pages 6313–6318.\n[Panerati et al., 2021] Panerati, J., Zheng, H., Zhou, S., Xu, J., Prorok, A.,\nand Schoellig, A. P. (2021). Learning to fly—a gym environment with pybul-\nlet physics for reinforcement learning of multi-agent quadcopter control. In\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 7512–7519.\n[Raffin et al., 2021] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus,\nM., and Dormann, N. (2021). Stable-baselines3: Reliable reinforcement learn-\ning implementations. Journal of Machine Learning Research, 22(268):1–8.\n[Razzaghi et al., 2022] Razzaghi, P., Tabrizian, A., Guo, W., Chen, S., Taye,\nA., Thompson, E., Bregeon, A., Baheri, A., and Wei, P. (2022). A survey on\nreinforcement learning in aviation applications.\n[Rubies-Royo and Tomlin, 2017] Rubies-Royo, V. and Tomlin, C. (2017). Re-\ncursive regression with neural networks: Approximating the hji pde solution.\n[Schulman et al., 2017a] Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and\nAbbeel, P. (2017a). Trust region policy optimization.\n[Schulman et al., 2017b] Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O. (2017b). Proximal policy optimization algorithms.\n23\n[Silver et al., 2014] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. (2014). Deterministic policy gradient algorithms. In Xing,\nE. P. and Jebara, T., editors, Proceedings of the 31st International Confer-\nence on Machine Learning, volume 32 of Proceedings of Machine Learning\nResearch, pages 387–395, Bejing, China. PMLR.\n[Srinivasan et al., 2020] Srinivasan, K., Eysenbach, B., Ha, S., Tan, J., and\nFinn, C. (2020). Learning to be safe: Deep rl with a safety critic.\n[Sutton and Barto, 2018] Sutton, R. S. and Barto, A. G. (2018). Reinforcement\nLearning: An Introduction. The MIT Press, second edition.\n[Thananjeyan et al., 2020] Thananjeyan, B., Balakrishna, A., Rosolia, U., Li,\nF., McAllister, R., Gonzalez, J. E., Levine, S., Borrelli, F., and Goldberg,\nK. (2020). Safety augmented value estimation from demonstrations (saved):\nSafe deep model-based rl for sparse cost robotic tasks.\n24\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2023-11-12",
  "updated": "2023-11-12"
}