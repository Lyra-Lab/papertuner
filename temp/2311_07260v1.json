{
  "id": "http://arxiv.org/abs/2311.07260v1",
  "title": "TIAGo RL: Simulated Reinforcement Learning Environments with Tactile Data for Mobile Robots",
  "authors": [
    "Luca Lach",
    "Francesco Ferro",
    "Robert Haschke"
  ],
  "abstract": "Tactile information is important for robust performance in robotic tasks that\ninvolve physical interaction, such as object manipulation. However, with more\ndata included in the reasoning and control process, modeling behavior becomes\nincreasingly difficult. Deep Reinforcement Learning (DRL) produced promising\nresults for learning complex behavior in various domains, including\ntactile-based manipulation in robotics. In this work, we present our\nopen-source reinforcement learning environments for the TIAGo service robot.\nThey produce tactile sensor measurements that resemble those of a real\nsensorised gripper for TIAGo, encouraging research in transfer learning of DRL\npolicies. Lastly, we show preliminary training results of a learned force\ncontrol policy and compare it to a classical PI controller.",
  "text": "TIAGo RL: Simulated Reinforcement Learning Environments with\nTactile Data for Mobile Robots\nLuca Lach1,2, Francesco Ferro1 and Robert Haschke2\nAbstract— Tactile information is important for robust perfor-\nmance in robotic tasks that involve physical interaction, such\nas object manipulation. However, with more data included in\nthe reasoning and control process, modeling behavior becomes\nincreasingly difficult. Deep Reinforcement Learning (DRL) pro-\nduced promising results for learning complex behavior in var-\nious domains, including tactile-based manipulation in robotics.\nIn this work, we present our open-source reinforcement learning\nenvironments for the TIAGo service robot. They produce tactile\nsensor measurements that resemble those of a real sensorised\ngripper for TIAGo, encouraging research in transfer learning\nof DRL policies. Lastly, we show preliminary training results\nof a learned force control policy and compare it to a classical\nPI controller.\nI. INTRODUCTION\nDuring object manipulation, tactile sensation is one of the\nmost important sensory information for humans. We utilize it\nto sense the acquisition of object contacts and to estimate and\nregulate grasp force. Additionally, we can detect suboptimal\ngrasp configurations and correct these quickly, for example\nin the case of object slippage.\nConsequently, tactile sensors gained popularity in the re-\nsearch field of robotic manipulation as well. Many traditional\ngrasping approaches that are devoid of tactile sensations\nfollow a similar basic scheme: objects are segmented on a\ndepth image, grasps are calculated, a trajectory towards the\nbest grasp is computed and finally executed. These static,\nopen-loop grasping frameworks work well in very controlled\nenvironments, however, with increasing uncertainty, their\nperformance can decline sharply. This is especially the case\nfor mobile service robots as their target environments are\ntypically households, which tend to be unstructured and\nhighly uncertain.\nWith the addition of tactile sensors to mobile robots,\nmodeling grasping behavior becomes increasingly difficult.\nWhereas open-loop grasping systems often consist of static\nexpert systems, approaches using tactile sensors need to\nmodel a more dynamic and reactive behavior. In recent years,\nDeep Reinforcement Learning (DRL) has been shown to be\na powerful tool for learning complex behaviors. It achieved\nsuccesses in many domains, for example in video games [1],\nchess [2] and also tactile-based robotic tasks [3].\nAs the research field of mobile robotics with tactile sensors\nin DRL is still growing, basic tools and benchmarks are yet\n1PAL Robotics S.L., Barcelona, Spain\nluca.lach@pal-robotics.com\n2Neuroinformatics Group, Bielefeld University, Germany\nThis work was supported by the European Union Horizon 2020 Marie\nCurie Actions under Grant 813713 NeuTouch.\nto be defined. Our contribution in this work is the open-\nsource release of tactile-enabled mobile robotics environ-\nments for reinforcement learning. Specifically, we model\nPAL Robotic’s TIAGo platform [4] with a sensorised end-\neffector that is also available on the real robot. The simulated\ntactile data is transformed to optimally resemble real sensor\ndata in order to facilitate exploration in simulation and sim-\nto-real transfer learning. First, we will describe the specifica-\ntion of different environments, explain how we compute the\nsimulated tactile data, and finally show preliminary training\nresults of a force control policy.\nII. RELATED WORK\nIn [3], a robot arm equipped with a custom-made tactile\nfingertip learns to type on a Braille keyboard using DRL.\nThe authors also note the lack of benchmark environments\nin the domain and propose their task and environment as one\nfor image-based tactile sensors. Their policy was trained in\nsimulation for some tasks and on the physical robot in others,\nand they emphasize that transfer learning is an interesting\nresearch direction.\nAnother interesting work that investigates sim-to-real\ntransfer in tactile-based DRL is [5]. By choosing input\nfeatures that exhibit small differences between simulation\nand real world, they were able to transfer a policy from sim-\nulation to the real robot without relearning. Their approach\ninvolved the conversion of forces to binary forces. Thus,\nthe tactile data would only indicate object contact, thereby\navoiding the demanding task of accurately modeling tactile\ndata in simulation. Their final model drastically outper-\nformed visual open-loop grasping pipelines with increasing\ncalibration noise.\nThe authors of [6] investigated the benefit of using tactile\nfeatures when learning dexterous manipulations tasks in\nsimulation. Their results indicated faster convergence and\nhigher sample efficiency when using tactile data. These\nresults were similar when using binary and continuous tactile\ndata. The environments they developed were published and\nmade open-source as part of OpenAI’s gym [7].\n[8] used a Shadow Hand equipped with BioTac sensors\nto learn gentle object manipulation. They conducted exper-\niments in simulation as well as on a real robot, however,\nthey do not attempt transferring policies. Nonetheless, their\nwork is interesting to mention as their objective was to learn\ngentle force control. They achieve this by shaping a force-\nbased reward function and comparing different alternatives.\narXiv:2311.07260v1  [cs.RO]  13 Nov 2023\n(a) TIAGoTactileEnv\n(b) GripperTactileEnv\nFig. 1: Initial configurations of both tactile environments with\nthe gripper in a pre-grasp pose. The object is closer to one\nof the fingers, rather than being centered perfectly.\nIII. REINFORCEMENT LEARNING\nENVIRONMENTS\nIn reinforcement learning, an agent attempts to learn a\npolicy that maximizes the expected reward in an environ-\nment. Therefore, it receives observations and in turn, needs to\ncompute an action to take. The environment itself models its\ndynamics internally, hidden from the agent, only providing\nobservations.\nTIAGo RL1 comprises three different base environments,\ntwo with tactile sensors and one without. Similar to OpenAI\ngym’s [7] robotics environments, the idea was to model all\ncommon aspects of an environment in its base class, e.g.\nthe robot model or joint limits. Task-specific environments\nthat inherit from such a base class can simply override\nenvironment properties such as the reward function or the\ntask setup and thus avoid re-implementing basic robot dy-\nnamics. Additionally, all derived task environments benefit\nfrom bug fixes or improvements in the base environment. We\nuse PyBullet [9], a free and open-source real-time physics\nsimulator, to simulate the environment’s dynamics. PyBullet\nalso allows the stiffness and damping parameters of collision\nbodies to be modified, making it attractive for learning of\nforce control policies.\nBy default, all environments model a grasping scenario\nwhere the robot is located next to a table with an object.\nThe robot’s arm and gripper are configured in a pre-grasp\npose, meaning object contact is anticipated when the gripper\nis closed.\n1) TIAGoTactileEnv: This environment uses the default\nTIAGo robot model with the exception that the gripper’s\nfingers are replaced by TA11 load cell sensors. It replicates\na benchmarking setup for tactile-based grasping that can be\nevaluated on TIAGo. Results obtained in this environment\ncan be easily compared to those of one without tactile sensors\nto evaluate their benefits.\n2) GripperTactileEnv: In this environment, only the grip-\nper model with load cell sensors is used instead of the\ncomplete robot. The gripper is fixed above the table in a\n1https://github.com/llach/tiago_rl\npre-grasp position as described above. It was created to\nlearn gripper-only control policies for scenarios where the\nreaching motion of the arm is computed by a classical\nkinematics solver. The part of the manipulation task where\nphysical contact is expected can then be handled by the\ngripper-only policy. By removing many collision bodies from\nthe environment, we expect this environment to be less\ncomputationally demanding. Moreover, the DRL algorithm\ndoes not need to learn to ignore the arm’s degrees of freedom\nand should thus converge faster.\n3) TIAGoPALGripperEnv: Lastly, we also include a sen-\nsorless environment as there is currently no open-source\nDRL environment available for TIAGo. Hence, it can be\nused as a benchmark for TIAGoTactileEnv and to replicate\ntasks from gym’s robotics environments. Note, that using\nthis environment is different from using TIAGoTactileEnv\nand ignoring the tactile data because the robot models differ.\nA. Tactile Data Modeling\nOne important part of the two TactileEnvs is how they\nmodel tactile data. We recorded the force values of several\nobject grasps on a real robot and in simulation. The object’s\nweight, its position relative to the gripper, and finger velocity\nwere identical to the real-world values. In PyBullet, we\nextracted the contact forces f contact between each finger and\nthe object. We found that scaling f contact with a factor of 100\nwould result in force ranges that are very similar to the ones\nmeasured on the real robot. Furthermore, we estimated the\nreal sensor’s standard deviation in non-contact state using\n10.000 samples, yielding σ = 0.0077. The simulated force\noutput for sensor i is eventually computed as follows:\nf raw\ni\n(t) = f contact\ni\n(t) ∗100 + ϵ\nwith ϵ ∼N(0, 0.0077)\n(1)\nNote, that we refer to these forces as \"raw\" as they simulate\nraw sensor readings on the robot despite the transformations\nwe apply in simulation. Figure 2 compares two force trajec-\ntories of grasps performed in simulation and on a real robot.\nBinary forces were found to facilitate transfer learning as\nreported by [5]. Hence, our environments offer this conver-\nsion of force values as well. In this representation, the tactile\ndata solely indicates object contact without information about\nits intensity. It is calculated with:\nf binray\ni\n(t) =\n(\n1,\nif f raw\ni\n(t) > f thresh\ni\n0,\notherwise\n(2)\nwhere f thresh\ni\nis a sensor-specific noise threshold.\nB. Observation Space\nAt each timestep, the agent makes an observation o(t)\nwhich is then used to calculate a subsequent action. The base\nenvironment without tactile sensors, TIAGoPALGripperEnv,\n(a) Force values while grasping with TA11 sensors\n(b) Simulated force values f raw, calculated using equation 1\nFig. 2: Comparison of force values measured while grasping.\n2a shows the force trajectory of grasp performed on a real\nTIAGo. 2b shows the simulated forces of the same grasp\nreplicated with our environment.\nprovides the agent with information about the joint states:\nobase(t) =\n\n\n\n\n\n\n\n\n\n\nq1(t)\n...\nqN(t)\n˙q1(t)\n...\n˙qN(t)\n\n\n\n\n\n\n\n\n\n\n(3)\nwhere qj(t) and ˙qj(t) refer to the position and velocity of\njoint j at time t respectively. For TIAGo we have N = 10\njoints, including seven arm joints, two gripper joints, and the\ntorso lift joint. Any task-specific environment would need to\nappend some information relevant to the task’s goal to these\nbasic observations, e.g. distance to a certain goal position.\nFor the two tactile environments, the current force deltas\nare also included in the observations:\notactile(t) =\n\u0000obase(t), ∆fright, ∆fleft\n\u0001\n(4)\nwith ∆fi = fi(t) −f goal. While for TIAGoTactileEnv, N =\n10, the gripper-only environment GripperTactileEnv only\nactuates the two gripper joints, hence N = 2.\nC. Action Space\nThe action simply consists of the desired velocities for\neach joint:\na(t) =\n\n\n\n˙qdes\nj (t)\n...\n˙qdes\nN (t)\n\n\n\n(5)\nThe action space is designed to mimic the expected input\nfor TIAGo’s position controller, another property aimed at\nfacilitating easy policy transfer. Moreover, joint velocities\nare restricted through PyBullet to not surpass the same limits\nthat are in place on the real robot as a measure to prevent\nthe policy from learning unrealistic behavior.\nD. Reward Function\nThe reward at time t is given by:\nr(t) = −\nX\ni∈{right,left}\n|fi(t) −f goal|\n(6)\nwhere f goal is a goal force that should be reached and main-\ntained. The intention behind modeling the reward function\nlike this is to encourage the agent to maintain a particular\ngoal force. By taking the absolute value, the agent is equally\npenalized for forces above and below the goal force. Ulti-\nmately, the agent needs to learn a force control policy in\norder to maximize its reward.\nIV. LEARNING FORCE CONTROL\nWe conducted a preliminary experiment to assess the\nfeasibility of learning force control policies using our en-\nvironments. As our goal is to demonstrate policy learning\nin situations where tactile data is most relevant, we chose to\nuse the GripperTactileEnv for the experiment. In the learning\nscenario, the gripper starts in an open position. An object\nis placed centered between the fingers, thus guaranteeing\nobject contact when the fingers are closed. The stiffness and\ndamping parameters were set to simulate a slightly compliant\nobject. The object’s weight and size were set to match those\nof a real object we used in a similar real-world grasping\nexperiment.\nWe trained the agent using the Twin Delayed Deep De-\nterministic Policy Gradient (TD3) [10] algorithm. It can\nlearn continuous actions, is an off-policy algorithm, and\nperformed very well in different kinds of environments, in-\ncluding robotics. As in the original paper, we used Gaussian\nnoise N(0, 0.1) for policy exploration. All other training\nparameters were left unchanged.\nThe agent was trained for 4×105 timesteps with an episode\nlength of 300 steps. Once the end of an episode was reached,\nthe environment resets itself and the agent could try anew.\nDuring training, the mean reward over the last 20 episodes\nwas periodically recorded. On occasions where the policy\nhad surpassed its former best mean reward, the policy’s\nweights were saved as well. Figure 3 shows the median\nreward over five different random seeds. During the first part\nof the training, the agent steadily achieved higher rewards\nuntil it converged to a maximum of about −150. After around\nFig. 3: Median reward of policies trained with TD3 on\nGripperTactileEnv across five different seeds.\n2.5×106 steps, the performance variance across seeds stayed\nrelatively low, indicating that all agents converged to similar\nsolutions.\nA. Force Control Policy Evaluation\nIn order to evaluate the agent, we compared it to a classical\nforce controller that we usually employ on TIAGo. This\ncontroller starts by closing the fingers using standard position\ncontrol. If a finger detects a force above the noise threshold,\nit stops moving until the opposing finger acquires contact as\nwell. Thereby it avoids undesired object movements. Once\nboth fingers touch the object, the controller switches to force\ncontrol and tries to minimize the difference of measured\nforce to goal force. It uses PI control to reach and maintain\nthe goal force. The integral part of the controller compensates\nfor inaccurate estimations of the object’s stiffness.\nFor the evaluation, we repeated the experiment 10 times\nwith this classical force controller and a trained agent. We\nreport the mean reward and its standard deviation from\nthe trials for each method. The force controller achieved a\nmean episode reward of −198.01 ± 14.52 while the agent\nachieved −146.93 ± 2.68. From a reward perspective, the\nagent outperformed the PI force controller. However, from\nobserving the agent’s behavior, it became clear that it could\nnot be transferred to a real robot: It learned to bounce the\nobject back and forth between the fingers, thus creating\nshort pulses of impact force reducing the reward rather than\npenetrating the object with a stable grasp and maintaining\ngrasp forces.\nV. CONCLUSION\nIn this work, we have introduced novel reinforcement\nlearning environments for TIAGo that incorporate tactile\ndata. Two of the presented environments provide tactile data\nfrom simulated load cell sensors, the third one without tactile\nsensors can be used to benchmark the advantage of using\ntactile data. The simulated sensor data is transformed to\nmatch the characteristics of real sensor data by scaling it\nand applying Gaussian noise. In preliminary experiments, an\nagent has shown the ability to learn to reach and maintain a\ncertain goal force in our environments. However, it seemed\nto have overfitted to that particular training scenario.\nThe source code is being actively developed, maintained\nand new features are continuously added. In future works,\nwe plan to investigate how to learn more reliable force\ncontrol agents in simulation that generalize better across\ndifferent goal forces. Possible approaches include refining the\nreward function and hyperparameter searches. Ultimately, the\ngoal is to research transfer learning capabilities using these\nsimulation environments.\nREFERENCES\n[1]\nD. Lee, H. Tang, J. O. Zhang, H. Xu, T. Darrell, and\nP. Abbeel, Modular architecture for starcraft ii with\ndeep reinforcement learning, 2018. arXiv: 1811.03555\n[cs.AI].\n[2]\nD. Silver, T. Hubert, J. Schrittwieser, et al., “A general\nreinforcement learning algorithm that masters chess,\nshogi, and go through self-play,” Science, vol. 362,\nno. 6419, pp. 1140–1144, 2018.\n[3]\nA. Church, J. Lloyd, R. Hadsell, and N. F. Lep-\nora, “Deep reinforcement learning for tactile robotics:\nLearning to type on a braille keyboard,” IEEE\nRobotics and Automation Letters, vol. 5, no. 4,\npp. 6145–6152, 2020.\n[4]\nJ. Pages, L. Marchionni, and F. Ferro, “Tiago: The\nmodular robot that adapts to different research needs,”\nin Int. workshop on robot modularity, IROS, 2016.\n[5]\nB. Wu, I. Akinola, J. Varley, and P. Allen, Mat: Multi-\nfingered adaptive tactile grasping via deep reinforce-\nment learning, 2019. arXiv: 1909.04787 [cs.RO].\n[6]\nA. Melnik, L. Lach, M. Plappert, T. Korthals, R.\nHaschke, and H. Ritter, “Using tactile sensing to\nimprove the sample efficiency and performance of\ndeep deterministic policy gradients for simulated in-\nhand manipulation tasks,” Frontiers in Robotics and\nAI, vol. 8, p. 57, 2021, ISSN: 2296-9144.\n[7]\nG. Brockman, V. Cheung, L. Pettersson, et al., Openai\ngym, 2016. eprint: arXiv:1606.01540.\n[8]\nS. H. Huang, M. Zambelli, J. Kay, et al., Learning\ngentle object manipulation with curiosity-driven deep\nreinforcement learning, 2019. arXiv: 1903 . 08542\n[cs.RO].\n[9]\nE. Coumans and Y. Bai, Pybullet, a python module for\nphysics simulation for games, robotics and machine\nlearning, http://pybullet.org, 2016–2021.\n[10]\nS. Fujimoto, H. van Hoof, and D. Meger, Addressing\nfunction approximation error in actor-critic methods,\n2018. arXiv: 1802.09477 [cs.AI].\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2023-11-13",
  "updated": "2023-11-13"
}