{
  "id": "http://arxiv.org/abs/2311.07315v3",
  "title": "An introduction to reinforcement learning for neuroscience",
  "authors": [
    "Kristopher T. Jensen"
  ],
  "abstract": "Reinforcement learning (RL) has a rich history in neuroscience, from early\nwork on dopamine as a reward prediction error signal (Schultz et al., 1997) to\nrecent work proposing that the brain could implement a form of 'distributional\nreinforcement learning' popularized in machine learning (Dabney et al., 2020).\nThere has been a close link between theoretical advances in reinforcement\nlearning and neuroscience experiments throughout this literature, and the\ntheories describing the experimental data have therefore become increasingly\ncomplex. Here, we provide an introduction and mathematical background to many\nof the methods that have been used in systems neroscience. We start with an\noverview of the RL problem and classical temporal difference algorithms,\nfollowed by a discussion of 'model-free', 'model-based', and intermediate RL\nalgorithms. We then introduce deep reinforcement learning and discuss how this\nframework has led to new insights in neuroscience. This includes a particular\nfocus on meta-reinforcement learning (Wang et al., 2018) and distributional RL\n(Dabney et al., 2020). Finally, we discuss potential shortcomings of the RL\nformalism for neuroscience and highlight open questions in the field. Code that\nimplements the methods discussed and generates the figures is also provided.",
  "text": "O R I G I N A L A R T I C L E\nNB\nDT}\nAn introduction to reinforcement learning for\nneuroscience\nKristopher T. Jensen1,2\n1Sainsbury Wellcome Centre, University\nCollege London\n2Computational and Biological Learning\nLab, University of Cambridge\nCorrespondence\nKristopher Jensen, Sainsbury Wellcome\nCentre, University College London, London\nW1T 4JG, UK\nEmail: kris.torp.jensen@gmail.com\nReinforcement learning (RL) has a rich history in neuroscience,\nfrom early work on dopamine as a reward prediction er-\nror signal (Schultz et al., 1997) to recent work proposing\nthat the brain could implement a form of ‘distributional re-\ninforcement learning’ popularized in machine learning (Dab-\nney et al., 2020). There has been a close link between theo-\nretical advances in reinforcement learning and neuroscience\nexperiments throughout this literature, and the theories de-\nscribing the experimental data have therefore become in-\ncreasingly complex. Here, we provide an introduction and\nmathematical background to many of the methods that have\nbeen used in systems neroscience. We start with an overview\nof the RL problem and classical temporal diﬀerence algo-\nrithms, followed by a discussion of ‘model-free’, ‘model-based’,\nand intermediate RL algorithms. We then introduce deep\nreinforcement learning and discuss how this framework has\nled to new insights in neuroscience. This includes a par-\nticular focus on meta-reinforcement learning (Wang et al.,\n2018) and distributional RL (Dabney et al., 2020). Finally,\nwe discuss potential shortcomings of the RL formalism for\nneuroscience and highlight open questions in the ﬁeld. Code\nthat implements the methods discussed and generates the\nﬁgures is also provided.\n1\narXiv:2311.07315v3  [q-bio.NC]  18 Dec 2024\n2\nJensen\n1\n|\nINTRODUCTION\nHumans and other animals learn from their experiences. Sometimes, this takes the form of explicit demonstration, as\nis often the case during formal education. However, we also often have to learn from trial and error together with\nfeedback received from the world around us – sometimes implicit and sometimes explicit. This is well illustrated by the\nclassical case study of Pavlov’s dogs, who learned to associate a so-called ‘conditioned stimulus’ (CS; e.g. the ringing of\na bell) with the availability of food shortly after (the ‘unconditioned stimulus’; US). Following a brief period of learning,\nthe dogs would start to salivate in response to the CS in advance of any food actually being served. This suggests that\nthe dogs had learned to associate the CS with the availability of ‘reward’ in the form of food, and that they produced an\nappropriate physiological response to take advantage of this food availability. Importantly, this occurred without any\nexplicit instruction or description of the sequence of events preceding food being served. Instead, the dogs learned\nfrom experience with their environment and the presence of a salient, rewarding stimulus.\nSuch passive stimulus-response predictions are also called ‘Pavlovian learning’ and have been commonly used in neu-\nroscience to study learning from external rewards (Niv, 2009). This forms a speciﬁc instantiation of the concept of\n‘reinforcement learning’, which is a general term for settings where an agent learns from reward signals in the envi-\nronment rather than explicit demonstration, as is the case in ‘supervised learning’. Importantly, the past decades have\nshown that principles of reinforcement learning can be used to explain not just behaviour, but also neural activity\nin biological circuits (Niv, 2009; Botvinick et al., 2020). An explicit neural basis of RL was initially demonstrated in\nfoundational work by Schultz et al. (1997), which showed that the ﬁring rates of dopaminergic neurons in the Ventral\nTegmental Area (VTA) reﬂected the diﬀerence between expected and actual ‘value’ when animals received a juice re-\nward following a CS consisting of a lever-press in response to a small light turning on. This provided a potential neural\nsubstrate of the classical ‘temporal diﬀerence’ learning algorithm (Schultz et al., 1997; Sutton, 1988), which has since\nbeen expanded to a wealth of evidence for reinforcement learning in neural dynamics (Niv, 2009; Dabney et al., 2020;\nWatabe-Uchida et al., 2017). However, these classical algorithms are generally restricted to simple problem settings,\nwhile humans and other animals are capable of solving complex high-dimensional problems involving extended plan-\nning and ﬁne motor control. The ﬁeld of ‘deep reinforcement learning’ has recently emerged to tackle such problems in\na machine learning setting, which has led to impressive results across a range of tasks (Mnih et al., 2013; Schrittwieser\net al., 2020; Wurman et al., 2022; Vinyals et al., 2019). Intriguingly, recent research has demonstrated that these deep\nRL algorithms also have parallels in both behaviour and neural dynamics (Botvinick et al., 2020; Wang et al., 2018;\nDabney et al., 2020; Jensen et al., 2023; Aldarondo et al., 2024), suggesting that neuroscience can continue to learn\nfrom advances in reinforcement learning.\nIn this review, we provide an overview of the reinforcement learning problem and popular algorithms, with a particular\nfocus on parallels and uses of these algorithms in neuroscience. This overview starts from classical tabular TD learning\nand Q-learning algorithms, which have guided neuroscience research for decades. We then consider the important\ndistinction between model-based and model-free reinforcement learning, as well as methods that fall somewhere in\nthe gray area between these extremes, and discuss their neural correlates. Finally, we generalize the tabular methods\nto the non-linear function approximation setting and the resulting deep RL methods, which have revolutionized ma-\nchine learning in recent years. We do this with a focus on methods that have had a strong inﬂuence on neuroscience\nto give the reader a better idea of the mathematical and computational background of recent neuroscientiﬁc ﬁndings.\nThese include the ‘meta-reinforcement learning’ model of PFC by Wang et al. (2018) and the ‘distributional reinforce-\nment learning’ model of VTA dopaminergic neurons by Dabney et al. (2020) in particular. We hope this review will\nbe useful both for those who are interested in the theory underlying reinforcement learning in neuroscience and for\nthose who want an overview of how the neuroscience literature builds on principles from reinforcement learning the-\nory. Throughout the paper, the focus will be on an intuitive understanding of the relevant RL methods, and explicit\nJensen\n3\nA\nB\n𝒂𝒕∼𝝅𝒂𝒔𝒕\n𝒔𝒕\"𝟏, 𝒓𝒕∼𝒑𝒔, 𝒓𝒔𝒕, 𝒂𝒕\nFIGUR E 1\nThe reinforcement learning problem and cliﬀworld environment. (A) An agent (here the bird) interacts\nwith the world to maximize reward. This involves a balance between exploring potentially interesting new states (e.g.\nsearching for food in a new ﬁeld) while also exploiting states known to yield high reward (e.g. the ﬁeld that had many\nworms yesterday). At a given point in time, the bird is in some state st from which it can take an action at , with the\nprobability of diﬀerent actions determined by the ‘policy’ π (a|st ), which is controlled by the agent. at then leads to a\nchange in the environment according to the non-controllable environment dynamics st+1, rt ∼p (s, r |st, at ). Here, rt\nis the empirical ‘reward’ received by the agent, and its objective is to collect as much cumulative reward as possible.\nOften, reinforcement learning problems are divided into ‘episodes’, with the agent learning over the course of multiple\nrepeated exposures to the environment. This could for example consist of the bird learning over multiple days which\nﬁelds are likely to be rich in food, while minimizing the distance travelled and exposure to predators. (B) The ‘cliﬀworld’\nenvironment, which will be used to demonstrate the performance and behaviour of a range of reinforcement learning\nalgorithms in this work. The agent starts in the lower left corner (location [0, 0]), and the episode ﬁnishes when it\nencounters either the ‘cliﬀ’ (dark blue) or the goal (yellow; location [9,0]). If the agent walks oﬀthe cliﬀ, it receives\na reward of -100. If it ﬁnds the goal, it receives a reward of +50. In any other state, it receives a reward of -1. Such\nnegative rewards for ‘neutral’ actions are commonly used to encourage the agent to achieve its goal as fast as possible.\nThe arrows indicate the ‘optimal’ policy, which takes the agent to the goal via the shortest possible route that avoids\nthe cliﬀ.\nderivations are included only where we consider them conducive to such understanding. We refer to Sutton and\nBarto (2018) for a more in-depth treatment of the underlying theory.\n2\n|\nPROBLEM SETTING\nHere we provide a short introduction to the reinforcement learning problem in a discrete state and action space with a\nﬁnite time horizon – a common setting for neuroscience experiments consisting of repeated trials or episodes in a con-\ntrolled environment. In this setting, the environment consists of states from a discrete set s ∈S = {s1, s2, . . .}|S|\n1 , and\nthe agent can take actions a ∈A = {a1, a2, . . .}|A|\n1\n. The environment is characterized by transition and reward prob-\nabilities p (st+1, rt |st, at ), where rt is the reward at time t. We will use r (s, a) to denote either the reward when it de-\npends deterministically on the state and action, or its expectation otherwise. We will further make the Markov assump-\ntion that the next state and reward only depend on the current state and action, p (st+1, rt |st, at, st −1, at −1, . . . , s0, a0) =\np (st+1, rt |st, at ).\nWe can now deﬁne a trajectory τ = {st, at, rt }T\nt=0. The probability of a trajectory occuring is\npπ (τ) = p (s0)\nT\nÖ\nt=0\np (st+1, rt |st, at )π (at |st ).\n(1)\npπ (τ) depends on the policy of the agent, π (a|s), which speciﬁes the probability of taking action a in state s (Figure 1A).\n4\nJensen\nThe objective is to learn a policy that maximizes the expected total discounted reward\nJ (π) = Åτ∼pπ (τ) [Rτ ] = Åτ∼pπ (τ)\n\" TÕ\nt=0\nγt rt |τ\n#\n,\n(2)\nwhere Rτ := ÍT\nt=0 γt rt |τ, and we have written J (π) since the policy uniquely speciﬁes J in a stationary environment. In\nEquation 2, γ is a ‘discount factor’, which stipulates that we should care more about immediate rewards than rewards\nfar in the future. We can provide three interpretations for this discount factor. One is that agents intrinsically care\nmore about immediate reward than distant reward. A second is that there is a ﬁxed non-zero probability (1 −γ) of the\ncurrent ‘episode’ or environment terminating or changing at each timestep, in which case we should weight putative\nfuture reward by the probability that we are still engaged in the task at that time. The third view is that γ simply\nprovides a tool for reducing the variance of our learning methods, especially in temporally extended tasks. This third\nview is most compatible with the fact that evaluation of RL agents after training is generally done without discounting.\nSince J (π) depends on the policy of the agent, it is possible to search in the space of policies for one that maximizes\nJ, which is the topic of reinforcement learning. It is often assumed that the experience {τ} is generated by the agent\nacting according to its policy, and the resulting experience is then used to update the policy in a way that increases\nJ (π). However, ‘oﬀ-policy’ and ‘oﬄine’ reinforcement learning methods also exist, where the agent learns on the\nbasis of experience generated by a policy diﬀerent from π (Levine et al., 2020; Section 11.2). This can be either\nan ‘old’ version of the agent itself, when it acted according to a diﬀerent policy, or data generated by an entirely\ndiﬀerent agent. Oﬀ-policy learning is important for biological organisms, where learning can happen ‘oﬄine’ during\nsleep after initial data collection during wake, or from observing other individuals (also the topic of ‘imitation learning’;\nSection 11.3).\n3\n|\nTEMPORAL DIFFERENCE LEARNING\nA simple way to maximize reward in an environment is to learn the ‘value’ of diﬀerent states, and then move towards\nstates with high value. The potential importance of such an algorithm for neuroscience is evident from the value-\nseeking behaviour of many organisms, and the widespread ﬁndings of neural ‘codes’ for value across the brain (Schultz\net al., 1992; Padoa-Schioppa and Assad, 2006; Rushworth et al., 2011). This leaves the question of how such value\ncodes can be learned in a biologically plausible setting.\nOne answer to this question takes the form of the classical ‘temporal diﬀerence learning’ algorithm (Sutton, 1988;\nSutton and Barto, 2018). This involves deﬁning a value function for a given state s and policy π, which quantiﬁes the\nexpected future reward when following π starting from s:\nV π (s) = Åτ∼pπ (τ)\n\"Õ\nt ′≥t\nγt ′−t rt ′ |st = s\n#\n.\n(3)\nHere, Åτ∼pπ (τ) [·] indicates an expectation taken over trajectories τ resulting from the agent following the policy π.\nFor the true value function, we can expand this as a self-consistency equation\nV π (s) = rπ (s) +\nÕ\ns′\npπ (st+1 = s′|st = s)Åτ∼pπ (τ)\n\" Õ\nt ′=t+1\nγt ′−t rt ′ |st+1 = s′\n#\n(4)\n= rπ (s) + γ\nÕ\ns′\npπ (st+1 = s′|st = s)V π (s′),\n(5)\nJensen\n5\nwhere pπ (st+1 = s′|st = s) = Í\na π (a|s)p (st+1 = s′|st = s, at = a) is the probability of transitioning from s to s′\nunder π, and rπ (s) = Åa∼π [r (s, a)] is the expected reward in state s, averaged over actions. Importantly, Equation 5\nwould not hold if V π (s) was not the true value function (Sutton and Barto, 2018). When learning an approximate\nvalue function V (s), we can therefore use this bootstrapped self-consistency relation as an objective function:\nL ∝\u0000V (s) −\u0000rπ (s) + γÅpπ (s′|s)\n\u0002\nV (s′)\n\u0003\u0001\u00012,\n(6)\nGradient descent w.r.t V (s) gives us an update rule\n∆V (s) ∝−\n∂L\n∂V (s)\n(7)\n∝−V (s) + rπ (s) + γÅpπ (s′|s)\n\u0002\nV (s′)\n\u0003\n(8)\n≈−V (st ) + rt + γV (st+1).\n(9)\nThe last line approximates the expected update with a single sample corresponding to the states and reward actually\nexperienced. As more experience is collected and many small gradient steps are taken according Equation 9, these\nsingle-sample estimates average out to the expectation in Equation 8 (Figure 2A). Variants of this algorithm can also\nlearn about multiple past states at once using the notion of eligibility traces (Sutton and Barto, 2018). However, Equa-\ntion 9 is the canonical temporal diﬀerence learning rule (Sutton, 1988), and it leads to learning dynamics where the\ntemporal diﬀerence error δt := −V (st ) + rt + γV (st+1) gradually propagates from the rewarding state to prior states\nthat predict the upcoming reward (Figure 2C).\nThis gradual propagation of prediction errors from the reward state to its predecessors has been of great interest in\nneuroscience. In particular, classical work by Schultz et al. (1997) demonstrated a similar pattern of neural activity in\ndopaminergic VTA neurons, which formed the foundation of a now well-established theory that dopamine provides a\nbiological reward prediction error signal that drives learning (Niv, 2009; Watabe-Uchida et al., 2017). At a behavioural\nlevel, this is supported by experiments showing that artiﬁcial stimulation of dopamine neurons can be a strong driver\nof learning (Olds and Milner, 1954; Tsai et al., 2009; Steinberg et al., 2013). The simple narrative of dopamine as a\nreward prediction error has also been challenged in recent years (Coddington and Dudman, 2019; Howe et al., 2013;\nHorvitz, 2000), which has led to theories of dopamine as a more general prediction error for both value but also other\nquantities like salience and movement (Kakade and Dayan, 2002; Gershman et al., 2024).\nMuch classical work in neuroscience has focused on value learning in Pavlovian conditioning tasks, but animals in\nnatural environments also have to take actions on the basis of this information. However, after learning a value\nfunction, it can be used for optimal action selection if we can estimate r (s, a) and p (s′|s, a). We can then compute\na ‘state-action value’ Q π (s, a), deﬁned as the expected discounted future reward associated with taking action a in\nstate s and then following policy π:\nQ π (s, a) := Å\n\"Õ\nt ′=t\nγt ′−t rt ′ |st = s, at = a\n#\n= r (s, a) + γ\nÕ\ns′\np (s′|s, a)V π (s′).\n(10)\nApproximating the true value function V π by the learned value function V in Equation 10 yields an approximate\nstate-action value Q, which can be used to choose the action with the highest expected reward,\na∗(s) = argmaxaQ (s, a).\n(11)\n6\nJensen\nx position\ny position\nrandom\nx position\ny position\noptimal\n0\n200\nepisode\n100\n0\nreward\ngreedy\noptimal\n0\n5\n10\nstep\n0\n20\nTD error\nearly\nlate\nx position\ny position\nearly\nx position\ny position\nA\nB\nC\nD\nlate\nFIGUR E 2\nTemporal diﬀerence learning. (A) Value functions aquired through temporal diﬀerence learning (Equa-\ntion 9) while acting according to either a random (top) or an optimal (bottom) policy. These simulations were performed\nwith a random start state in the cliﬀworld environment to ensure full coverage of the space. Dark blue indicates neg-\native expected reward (-100) and yellow indicates positive expected reward (+50). These simulations used a learning\nrate of α = 0.05 and no temporal discounting (γ = 1). Under the random policy, states near the cliﬀhave low value\neven if they are close to the goal, since the agent often falls oﬀthe cliﬀfrom there. Under the optimal policy, all\nstates have high expected reward, since the agent always reaches the goal. States nearer the goal have slightly higher\nvalue than those further away. (B) Empirical reward as a function of episode number for a TD-learning agent that acts\naccording to Equation 11 while updating its value estimates according to Equation 9. For this agent, action selection\nassumes access to a ‘one-step’ world model in order to evaluate the consequence of each putative action. The agent\ngradually converges to an optimal policy. Parameters for the agent are as in (A), except that the start state is always\nthe lower left corner. (C) TD error (Equation 9) as a function of the step number along the optimal path for the agent\nin (B) at diﬀerent stages of learning (green to blue). This TD signal gradually propagates backwards from the reward to\npreceding states, mirroring biological recordings of dopamine activity (Schultz et al., 1997). (D) Value function learned\nby a greedy TD agent as in (B), plotted either early (top) or late (bottom) in training. Early in training, the agent has\nlearned that the cliﬀis bad but doesn’t know where the goal is or how to get there. Late in training, the agent has\nlearned a value function that locally resembles the optimal value function from (A), while it has not learned the value\nof distant states that are rarely or never visited from the start state. This is a potential shortcoming of ‘greedy’ agents\nthat can easily converge to a sub-optimal local maximum in more complicated environments. For this analysis, we\nused a high learning rate of α = 0.5 to make the early TD updates larger and therefore more visible.\nUpdating the value function according to Equation 9 while acting in the environment according to Equation 11 leads\nto an agent that gradually learns to take better actions as it learns a better value function (Figure 2B-D). This provides\na biologically plausible algorithm for reward-driven learning in agents with access to a one-step predictive model.\n4\n|\nQ-LEARNING\nIn some cases, we may not know the transition function or it could be expensive to simulate. Additionally, it has been\nfound that dopamine activity can reﬂect learning signals not just for state values but also for action values (Roesch\net al., 2007; Morris et al., 2006). This suggests an alternative model of biological learning, where animals directly\nlearn the state-action values deﬁned in Equation 10. This is in contrast to the algorithm in Section 3, where the agent\nonly learned the state values, and then computed the action values at decision-time using a one-step world model.\nQ-learning is the most prominent model for learning state-action values, and it has commonly been used to explain\nanimal behaviour and neural activity (Niv, 2009; Mattar and Daw, 2018).\nJensen\n7\nTo learn the Q-values necessary for action selection directly, we start by expanding Equation 10,\nQ π (s, a) = r (s, a) + γ\nÕ\ns′\np (st+1 = s′|st = s, at = a)\nÕ\na′\nπ (a′|s′)Q π (s′, a′).\n(12)\nFor the greedy policy πg (a|s) := Ia=a∗(s) (where the indicator function Ia=b = 1 for a = b and 0 otherwise), this gives\nrise to a self-consistency expression for the state-action values:\nQ πg (s, a) = r (s, a) + γÅs′∼p (s′|s,a)\nh\nmaxa′Q πg (s′, a′)\ni\n.\n(13)\nImportantly, this self-consistency expression only holds when the Q-values have converged to the true expected\nrewards, and the associated greedy policy is therefore optimal (Sutton and Barto, 2018). We can now use Equation 13\nas an objective by deﬁning\nL ∝\u0000Q (s, a) −\u0000r (s, a) + γÅs′∼p (s′|s,a)\n\u0002\nmaxa′Q (s′, a′)\n\u0003\u0001\u00012,\n(14)\nGradient descent w.r.t Q (s, a) gives us an update rule\n∆Q (s, a) ∝−Q (s, a) + r (s, a) + γÅs′∼p (s′|s,a)\n\u0002\nmaxa′Q (s′, a′)\n\u0003\n(15)\n≈−Q (st, at ) + rt + γmaxa′Q (st+1, a′).\n(16)\nThis is the so-called Q-learning update rule (Watkins, 1989; Figure 3A), where we have estimated the expectation\nwith the single sample actually seen by the agent in the last line.\nQ-learning is guaranteed to converge to the optimal policy in the limit of inﬁnitesimal learning rates and inﬁnite sam-\npling of the state-action space (Watkins and Dayan, 1992; Sutton and Barto, 2018). However, following the greedy\npolicy a∗(s) = argmaxaQ (s, a) before convergence of the Q-values can lead to undersampling of the state-action\nspace and poor performance. It is therefore common to either use an ‘ϵ-greedy’ policy, π (a|s) = ϵ/|A|+(1−ϵ)Ia=a∗(s),\nor a softmax-policy, π (a|s) ∝exp(βQ (s, a)), to collect the experience used to update the Q-values (Figure 3B). Such\nexploration strategies and their biological correlates are discussed in more detail in Section 9.\nThese approaches make Q-learning an ‘oﬀ-policy’ algorithm, since the policy used in the learning update (the greedy\npolicy) is diﬀerent from the policy used for action selection (the stochastic policy). An on-policy alternative known\nas ‘SARSA’ (state-action-reward-state-action) is also commonly used, where the update rule uses the Q-value corre-\nsponding to the action at+1 sampled at the next timestep instead of the greedy action (Figure 3C):\n∆Q (st, at ) ∝−Q (st, at ) + rt + γQ (st+1, at+1).\n(17)\nThis will converge to the true Q-values for a given policy π, similar to how the TD learning rule in Equation 9 converges\nto the true value function for a given policy, again under assumptions of inﬁnite sampling of the space.\nWhen animals have to choose between actions with diﬀerent values, studies have found evidence for midbrain\ndopamine neurons encoding the prediction error used for either Q-learning (Roesch et al., 2007; Niv, 2009) or SARSA\n(Morris et al., 2006; Niv, 2009). They may therefore not just be abstract learning algorithms, but instead have plausible\nimplementations in biological neural circuits. However, the methods considered so far also have notable shortcomings.\nFor example, the amount of data needed to learn state(-action) values and the assumption of a stationary environment\ncan be prohibitive for animals needing to act in a rapidly changing world, where bad decisions have fatal consequences.\n8\nJensen\n0\n200\n400\nepisode\n100\n50\n0\nreward\n = 0.0\n = 0.1\n = 0.2\n0\n200\n400\nepisode\n0\n20\nreward\n = 0.0\n = 0.1\n = 0.2\nx position\ny position\nQ learning\nx position\ny position\nA\nB\nC\nSARSA\nFIGUR E 3\nQ-learning. (A) Empirical reward as a function of episode number for Q-learners with diﬀerent levels\nof stochasticity in their policy (ϵ ∈{0, 0.1, 0.2}; legend). For these simulations, we used a learning rate of α = 0.05\nfor all agents and no temporal discounting (γ = 1). The agent with ϵ = 0 converges to an optimal policy, similar to\nthe TD agent in Figure 2A. However, convergence is in this case slower despite using the same learning rate, because\nthe Q-learner has to learn about each action independently, while the TD agent used its one-step world model to\naggregate learning across actions reaching the same state. In this cliﬀworld environment, increasing epsilon leads to\nworse performance since it increases the probability of falling oﬀthe cliﬀ. Additionally, there is no risk of getting stuck\nin a local minimum since there is only one rewarding state, which decreases the value of exploration. Lines and shading\nindicate mean and standard error across 10 simulations. (B) As in (A), now for a non-cliﬀworld grid environment with\ntwo goals: one with a reward of +20 at location (0, 4), and one with a reward of +50 at location (5,0). In this case,\nhaving non-zero epsilon can increase the probability of discovering the ‘high reward’ goal rather than getting stuck\nwith a locally optimal policy of moving to the ‘low reward’ goal. In these simulations, we used a learning rate of α = 1,\nsince this eﬀect is less robust with lower learning rates that lead to more exploration of the environment across all\nagents. (C) Cliﬀworld policy learned by a Q-learning (top) or SARSA (bottom) agent with ϵ = 0.3. Colours indicate the\nmaximum value of any action in a state from blue (-100) to yellow (+50), and arrows indicate which action has the\nhighest value. The Q-learning agent learns to move right above the cliﬀ, because this is the optimal thing to do under\nthe assumption that subsequent actions are also optimal. This is because it is an ‘oﬀ-policy’ algorithm that does not\ntake into account the actual policy of the agent. In contrast, the SARSA agent learns to move a ‘safe distance’ away\nfrom the cliﬀ, since it is an ‘on-policy’ algorithm that takes into account the ﬁnite probability of the agent choosing\nto move oﬀthe cliﬀfrom upcoming states. Q-learning agents are also frequently trained using a stochastic ϵ-greedy\npolicy and then evaluated with the greedy policy corresponding to ϵ = 0, or they can be trained while ‘annealing’ ϵ\nfrom some ﬁnite value to 0 over several episodes to allow for initial exploration.\n5\n|\nMODEL-FREE AND MODEL-BASED REINFORCEMENT LEARNING\nWe have so far considered what is known as ‘model-free’ reinforcement learning algorithms. These involve learning a\nstimulus-response function that says ‘when in state s, take action a’. Such algorithms do not require much computation\nat decision time, where they rely on cached state or action values. However, it can require a lot of experience with\nthe environment to learn these model-free policies, and they can be inﬂexible in changing environments. This is\nincompatible with many aspect of animal behaviour, which we know is adaptive and can beneﬁt from ‘latent learning’\nin an environment before a reward-driven task is ever encountered (Blodgett, 1929; Tolman, 1948).\nOn the other hand, ‘model-based’ reinforcement learning uses a model of the world to simulate the consequences\nof diﬀerent actions at decision time. This can be much more data eﬃcient, since learning a world model is often\neasier than learning a full policy (Figure 4A). In machine learning settings, model-based RL has exhibited impressive\nperformance across a range of tasks with large state spaces, including Atari, chess, shogi, and Go (Silver et al., 2018;\nSchrittwieser et al., 2020; Deisenroth and Rasmussen, 2011). In a biological context, the idea of ﬁrst learning a model\nof the environment, and then using it to guide reward-driven behaviour also provides one plausible explanation for\nlatent learning and other types of rapid adaptation. However, model-based decision making can be computationally\nJensen\n9\nintensive at decision-time, which is a challenge for animals that rely on rapid decision making for survival (Figure 4B).\nIn model-based RL, an approximate transition-and-reward function ˜p (s′, r |s, a) is learned from past experience. Once\nthis model has been learned, it can be used for planning at decision time. This can be done for example by expanding\nthe Q-value relation from Equation 13:\nQ (st, at ) ≈r (st, at ) + γÅ ˜p (st+1|st ,at )\n\u0002\nargmaxat+1Q (st+1, at+1)\n\u0003\n(18)\n≈r (st, at ) + γÅ ˜p (st+1|st ,at )\n\u0002\nargmaxat+1\n\u0002\nr (st+1, at+1) + γÅ ˜p (st+2|st+1,at+1)\n\u0002\nargmaxat+2Q (st+2, at+2)\n\u0003\u0003\u0003\n(19)\n= . . .\n(20)\nIf the environment is determinstic, p (s′|s, a) is a delta function, and otherwise the next-state expectations may need\nto be approximated with multiple samples. Unfortunately, optimizing over all possible action sequences in Equation 18\nis in general an exponentially large search problem in the planning depth, which makes it infeasible for any reasonably\nsized problem. It is therefore common to either use ‘depth-ﬁrst search’ with limited breadth, or ‘breadth-ﬁrst search’\nwith limited depth. In breadth-ﬁrst search, we consider all possible actions at each level of the search tree but termi-\nnate the search at a ﬁnite depth, instead using cached ‘model-free’ state-values to estimate the reward-to-go from\nthe terminal states. Such ‘plan-until-habit’ has also been proposed as a model of human behaviour (Keramati et al.,\n2016). In depth-ﬁrst search, we instead sample a series of paths from st to termination (or some upper bound), using\na heuristic to prioritize actions expected to be good, and then pick an action with high expected reward (Huys et al.,\n2012).\nFor both of these strategies, it is necessary to trade oﬀthe temporal opportunity cost of planning with the increase in\nexpected reward (Botvinick and Cohen, 2014; Agrawal et al., 2022). This has been a popular research area in cognitive\nscience, where a wealth of literature on ‘resource-rational’ decision making has emerged in recent years (Griﬃths et al.,\n2019; Callaway et al., 2022). However, this literature has often focused on the behaviour of optimal agents, with less\nfocus on the learning process and neural mechanisms that might implement the necessary computations. Bridging\nthis gap, recent work has suggested that frontal cortex and striatum might initially store a ‘model-free’ policy in its\nnetwork state, which is gradually updated with model-based information from the hippocampal formation until the\npolicy improvement is outweighed by the temporal opportunity cost of planning (Jensen et al., 2023).\nWhile several model-based and model-free reinforcement learning methods have thus been developed and used to\nmodel animal learning and behaviour, it remains an open question when and whether these diﬀerent strategies drive\nanimal behaviour. A diﬀerent line of research has therefore explicitly investigated the balance between model-based\nand model-free RL in biological agents (Daw et al., 2005; Geerts et al., 2020; Lengyel and Dayan, 2007), where the\nchoice between the two approaches is thought to be guided by some notion of optimality on the basis of available\nresources and uncertainty about the environment. A popular paradigm for these studies is the so-called ‘two-step’\ntask developed by Daw and colleagues (Daw et al., 2011; Momennejad et al., 2017; Wang et al., 2018; although note\nAkam et al., 2015).\nSuch work has shown that animals can use both model-free and model-based decision making, with the dorsolateral\nstriatum being particularly important for model-free reinforcement learning (Yin et al., 2004, 2005), and the dor-\nsomedial striatum, prefrontal cortex, and hippocampal formation being important for model-based decision making\n(Vikbladh et al., 2019; Geerts et al., 2020; Miller et al., 2017; Niv, 2009; Killcross and Coutureau, 2003). This also has\ninteresting parallels to recent work in motor learning, where the basal ganglia were found to be suﬃcient for ‘habitual’\nmotor sequences even in the absence of motor cortex, while motor cortex was necessary for more ﬂexible motor be-\nhaviours that are likely to require a high-level ‘schema’ of the task structure (Mizes et al., 2023b,a). In Section 9 we will\nsee how combining these model-based and model-free ideas with deep learning can lead to human-level performance\nin tasks such as chess and Go that require long-term planning.\n10\nJensen\n6\n|\nTHE SUCCESSOR REPRESENTATION\nAs we saw in the previous section, an important distinction can be made between model-free reinforcement learn-\ning methods, which cache stimulus-response mappings based on prior experience, and model-based reinforcement\nlearning methods, which compute a policy by simulating possible futures using a world model at decision-time. How-\never, we have also noted how animals both need the ﬂexibility of model-based methods as well as the rapid decision\nmaking aﬀorded by model-free methods. It has therefore been suggested that animals use intermediate methods\nthat combine some model-free and some model-based features. A particularly prevalent theory has been that of the\n‘successor representation’ (SR), which has been proposed to explain both human behaviour (Momennejad et al., 2017)\nand features of neural activity (Stachenfeld et al., 2017). In particular, the SR allows for ﬂexible adaptation to changing\nreward functions without having to perform expensive simulations at decision time.\nThe SR (Dayan, 1993) rewrites the expected reward starting from state s as:\nV π (s) = Åπ\n\"Õ\nt=0\nγt rt |s0 = s\n#\n(21)\n=\nÕ\nt=0\nγt Õ\ns′\npπ (st = s′|s0 = s)r (s′)\n(22)\n=\nÕ\ns′\nr (s′)\nÕ\nt\nγt pπ (st = s′|s0 = s)\n(23)\n= rT mπ\ns .\n(24)\nHere, r is a vector of the average reward associated with each state, and mπs is a vector of the expected discounted\nfuture occupancy of state s′ if the agent starts in state s and follows the policy π:\nM π\nss′ =\nÕ\nt=0\nγt pπ (st = s′|s0 = s).\n(25)\nThe full matrix Mπ, constructed from stacking the mπs corresponding to all states s, is denoted the ‘successor matrix’,\nand it allows us to write down a vector of expected rewards from all states as\nvπ = Mπr.\n(26)\nHere, we have retained the superscript π to indicate that the successor matrix and value function depend on the policy\nof the agent, which aﬀects the expected occupancy of diﬀerent states. Having computed the value of each state, we\ncan perform action selection using Equation 11.\nThe ﬂexibility of the successor representation arises when the reward structure of the environment changes, r →r′.\nWe can now compute the expected reward associated with each state under the new reward function and old policy,\nv′π = Mπr′.\n(27)\nThis provides a better starting point than the old policy and reward function (Figure 4C), but the SR does not generalize\nperfectly since the new value function can lead to policy changes and therefore having to update M (Figure 4D). The\nsuccessor matrix can be learned by temporal-diﬀerence learning when transitioning from st , analogous to Equation 9:\n∆M π\nst s′ ∝−M π\nst s′ + Ist =s′ + γM π\nst+1s′.\n(28)\nJensen\n11\nFIGUR E 4\nModel-based reinforcement learning. (A) Learning curves for model-free (MF) and model-based (MB)\nRL agents. The MB agent used depth-ﬁrst search to compute an optimal path at each decision point, gradually learning\nthe reward and transition functions while exploring the environment. The MF agent was a Q-learning agent with\nϵ = 0 and learning rate α = 1. (B) Wallclock time needed to run 100 episodes of cliﬀworld with either the MB or\nMF agents from (A), as a function of the length of the environment. While the MB agent required less experience\nto learn a good policy, the wallclock time per episode was much larger than for the MF agent. This illustrates an\nimportant balance between model-based and model-free reinforcement learning, where MF methods usually require\nmore experience but MB methods require more compute at decision time. (C) Learning curve for an agent using\nthe successor representation (SR) together with learning curves for the model-based agent in (A) and the greedy TD-\nagent from Figure 2. The goal was moved from location (9, 0) to location (0, 4) at episode 40 (vertical black line), and\nlocation (9, 0) was instead given a reward of -5. The MB and SR agents had their reward functions updated to reﬂect\nthis change and rapidly adapted their policies, while the TD agent had no such mechanism for robustness to changing\nreward functions. Reward curves were convolved with a Gaussian kernel (σ = 3 episodes), which is why performance\nappears to decrease slightly before episode 40. The TD and SR agents were assumed to have access to a 1-step world\nmodel at initialization, while the MB agent learned the transition structure from experience. (D) SR agents cannot\nalways adapt to new reward functions if the newly rewarded states have low probablity under the old policy. Left\ncolumn: Value function for an agent that learned an initial policy in an environment with a small reward in the upper\nleft corner and intermediate reward in the upper right corner. The middle top and bottom states are ‘cliﬀs’. The agent\nlearned to make an initial rightward choice (grey arrows). Right column: A large reward was introduced in either the\ntop left corner (top row) or bottom left corner (bottom row) and the value function recomputed (Equation 27). The\nagent was unable to adapt to a large reward in the bottom left corner, since the old policy had low probability of\nreaching this state, even after initially going to the left. This results in a low expected value for going left from the\nstart state (red circle), and a suboptimal policy that continues to go right (red arrow). (E) Learning curve for a standard\nQ-learning agent (blue) or Dyna agents that perform diﬀerent numbers of Q-value updates after each physical action\n(legend). These Dyna updates used cached experience rather than data from a learned world model. Dyna agents\nmake better use of limited experience at the cost of increased compute (proportional to the number of updates).\ns′ is any state, and st+1 is the next state actually observed. Intuitively, transitioning from st to st+1 means that (i) we\nhave just been in state st , and (ii) we should increase the expected occupancy of all states commonly reached from\nst+1 (including st+1 itself). Alternatively, if the policy-dependent transition matrix T π is known, whereT π\nss′ = pπ (st+1 =\n12\nJensen\ns′|st = s), the successor matrix can be computed as the geometric series Mπ = I +γT π +γ2(T π )2+. . . = (I −γT π )−1.\nThe SR has been proposed as a model of how humans and other animals learn and generalize (Momennejad et al.,\n2017; Stachenfeld et al., 2017; Geerts et al., 2020; Gershman, 2018). For example, humans adapt more readily to\nchanges in reward functions than changes in transition functions in a simple sequential binary decision-making task\n(Momennejad et al., 2017), consistent with the SR facilitating rapid adaptation to changes in r but only slow learning\nof M. Additionally, hippocampal ‘place cells’ have been proposed to encode a predictive map, with each cell corre-\nsponding to a column of M (Stachenfeld et al., 2017). In this model, the ﬁring of a place cell in a given location s\nreﬂects the expected future occupancy of its ‘preferred’ location s′ conditioned on currently being at s. Stachenfeld\net al. (2017) showed that the SR model explains a range of ﬁndings in the hippocampal literature. For example, this\nmodel explains the asymmetry of directional place ﬁelds on a one-dimensional track (Mehta et al., 2000), where s′ is\nmore likely to be reached from other states s that precede s′ than equidistant states that follow s′. The SR also explains\nwhy place ﬁelds change near a newly inserted barrier, since two states on either side of the barrier can no longer be\nvisited in quick succession (Alvernhe et al., 2011).\nWhile the SR is perhaps the most prominent model in systems neuroscience that combines features of model-free\nand model-based RL, it is not the only one. Another interesting algorithm is the ‘Dyna’ architecture of Sutton (1991).\nIn this framework, a model of the world is learned from experience and used to train a model-free policy oﬄine by\nbootstrapping imagined experience sampled from the model. This allows for more data-eﬃcient learning of model-\nfree policies at the cost of additional compute during ‘rest’, but without needing more compute at decision time\n(Figure 4E). The model used to simulate data for oﬄine training can either be an explicit learned world model, or\nit can simply be a memory buﬀer of past experiences in the form of (st, at, rt, st+1, at+1) tuples. Such experience\nreplay has also proven crucial to the success of modern deep reinforcement learning agents by allowing for higher\ndata eﬃciency and reducing the instability arising from online experience being autocorrelated (Mnih et al., 2013;\nSchaul et al., 2015). A prominent theory in neuroscience posits that hippocampal replays could be implementing such\na Dyna-like algorithm by generating imagined experience that is used to train the model-free RL systems of the brain\n(Mattar and Daw, 2018). This theory is supported by the ﬁnding that patterns of rodent replay in multiple navigation\ntasks are consistent with the optimal replays of a Q-learning agent with Dyna, and it has recently been extended to\nexplain not just the content of replays but also their timing (Agrawal et al., 2022).\n7\n|\nDEEP REINFORCEMENT LEARNING\nWe have so far considered small state and action spaces, where tabular policies are tractable. Unfortunately, most\nethologically relevant state spaces are large enough that we cannot enumerate all possible states and actions. How-\never, novel situations often resemble previously encountered states, allowing agents to generalize shared structure to\nthese new but related settings (Botvinick et al., 2020). In these cases, we can use function approximation (Sutton and\nBarto, 2018) instead of tabular policies. This involves an assumption that similar states will have similar state-action\nvalues and should therefore have similar policies. By making this assumption, we can generalize to unseen states\nbased on previous experience. The use of deep or recurrent neural networks as powerful function approximators for\nreinforcement learning has driven impressive progress in this setting – the domain of ‘deep reinforcement learning’\n(deep RL). Deep RL has seen increasing interest not just in machine learning, but also as a model of neural dynamics\nand behaviour in humans and other animals (Wang et al., 2018; Jensen et al., 2023; Makino, 2023; Merel et al., 2019;\nBanino et al., 2018; Aldarondo et al., 2024; Botvinick et al., 2020). Popular approaches in (model-free) deep RL can\nlargely be divided into two categories: value-based methods, which compute state-action values that can be used for\naction selection; and policy gradient methods, which train a neural network to output a policy directly.\nJensen\n13\n|\nValue-based methods\nThe deep RL approaches most similar to the tabular methods considered in Section 3 and Section 4 use neural net-\nworks to compute state-action values, which can be used for action selection as we saw in Equation 11. However, by\nusing function approximation instead of the tabular values considered previously, these networks can generalize to\nunseen states in large state spaces. This gives rise to the family of ‘deep Q-learning’ methods, which closely mirror\nthe tabular Q-learning considered previously, but now with function approximation.\nThe simplest approach involves deﬁning a state-action value function Qθ (s, a), where the parameters θ of the neural\nnetwork deﬁning our agent are learned as follows:\n• Collect experience (st, at, rt, st+1).\n• Deﬁne a loss L = 0.5[Qθ (st, at ) −(rt + γmaxaQθ (st+1, a))]2.\n• Update parameters ∆θ ∝−∂L\n∂θ\n[often treating the ‘target value’ yt = (rt + γmaxaQθ (st+1, a)) as constant w.r.t. θ].\nWhen acting according to our policy, we simply pick the action predicted to have the highest value, usually using some\nvariant of ϵ-greedy or softmax to increase exploration.\nOn the surface, this looks like a straightforward generalization of tabular Q-learning, and it may seem surprising that\ndeep Q-learning did not see signiﬁcant use or success until the foundational work of Mnih et al. (2013). However, a\nmajor diﬃculty arises from the autocorrelation of the states observed by the agent, which destabilizes training. This\ncan be mitigated by the use of ‘experience replay’, where the experience generated by the agent is added to a global\nreplay buﬀer B. One or more experiences are then sampled randomly from the buﬀer at each iteration and used\nto update the network parameters – reminiscent of the ‘Dyna’ architecture described previously. Additionally, the\ntarget value yt = (rt + γmaxaQθ (st+1, a)) itself depends on θ and therefore changes when any Q-value is updated (in\ncontrast to tabular Q-learning, where there is no parameter sharing). It is therefore common to use a ‘target network’\nQθ′ that remains ﬁxed for multiple rounds of data collection and parameter updates. This reduces ﬂuctuations in the\ntarget values, and the resulting parameter updates are gradients of a well-deﬁned objective function. Together, these\ntwo approaches give rise to the ‘deep Q network’ (DQN) developed by Mnih et al. (2013), which is trained as follows:\n• Collect experience (st, at, rt, st+1) and add to B\n[optionally many iterations and optionally removing stale experi-\nences].\n• Randomly sample an experience (s′\nt, a′\nt, r ′\nt , s′\nt+1) ∼B\n[optionally a full batch].\n• Deﬁne a loss L(θ) = 0.5[Qθ (s′\nt, a′\nt ) −(r ′\nt + γmaxaQθ′ (s′\nt+1, a))]2\n[optionally averaged over the full batch]. Note\nthe ‘student network’ has parameters θ and the target network inside the max has parameters θ′.\n• Update the network parameters ∆θ ∝−∂L(θ)\n∂θ\n.\n• At regular intervels, set our target network to the student network, θ′ ←θ.\nThis algorithm is eﬀectively oﬀ-policy, since most of the data in B is collected by a policy deﬁned by an old set\nof parameters – and the data in B can in fact be generated completely independently of the agent being trained.\nEven though the DQN is more stable than naive deep Q-learning, an additional instability arises from the fact that\nQmax(s′\nt+1) = maxaQθ′ (s′\nt+1, a) uses the same Q values both to estimate which action is best and what the value of that\naction is, which leads to a positively biased estimate. This can be mitigated by ‘double Q-learning’ (Van Hasselt et al.,\n2016), where the student network selects the best action and the target network evaluates its value, Qmax(s′\nt+1) ←\nQθ′ (s′\nt+1, argmaxa (Qθ (s′\nt+1, a))).\nWhile modern deep Q-learning has reached impressive performance across a range of machine learning settings (Mnih\net al., 2013; Lillicrap et al., 2015; Schaul et al., 2015; Kalashnikov et al., 2018), it is unclear whether the various modi-\nﬁcations needed to stabilize the algorithm could be implemented in biological circuits. This is perhaps the reason why\n14\nJensen\nneuroscience research using deep Q-learning has been relatively scarce, despite the prevalence of tabular Q-learning\nin theoretical neuroscience. An interesting exception is recent work by Makino (2023), which shows parallels between\nthe values learned by a DQN and neural representations in mammalian cortex during a compositional behavioural task.\nAdditionally, the importance of experience replay in DQNs (Mnih et al., 2013; Schaul et al., 2015) has close parallels\nto the proposal that hippocampal replay constitutes a form of experience replay (Mattar and Daw, 2018).\n|\nPolicy gradient methods\nA conceptually simpler approach for deep reinforcement learning uses policy gradient methods (Sutton and Barto,\n2018), where a neural network with parameters θ takes as input the (observable) state of the environment and directly\noutputs a policy πθ. This has also found more support and use in the neuroscience literature, where policy gradient\nmethods have recently been used as models of learning and neural dynamics in the biological brain (Wang et al., 2018;\nJensen et al., 2023; Merel et al., 2019; Song et al., 2017).\nThe objective in a policy gradient network is to ﬁnd the setting of θ that maximizes expected reward. A naive way to\nachieve this would be to deﬁne Rτ := ÍT\nt=0 γt rt and compute gradients given by\n+θJ (θ) = +θÅτ∼pπθ (τ) [Rτ ]\n(29)\n=\nÕ\nτ\nRτ+θpπθ (τ).\n(30)\nHere, τ ∼pπθ (τ) indicates trajectories sampled from the distribution induced by the policy πθ, and J (θ) indicates\nthe expectation of Rτ under pπθ (τ) (c.f. Equation 2). However, evaluating Equation 30 requires us to know how the\nenvironment will respond to our actions, which in general may not be the case. Instead, we use the ‘log-derivative\ntrick’, which takes advantage of the linearity of the expectation and the identity +θ logf (θ) = f (θ)−1+θf (θ) to write\n+θJ (θ) =\nÕ\nτ\nRτ+θpπθ (τ)\n(31)\n=\nÕ\nτ\nRτpπθ (τ)+θ log pπθ (τ)\n(32)\n= Åτ∼pπθ (τ)\n\u0002\nRτ+θ log pπθ (τ)\n\u0003\n,\n(33)\nSince the environment does not depend on θ, we can simplify the calculation of +θ log pπθ (τ):\n+θ log pπθ (τ) = +θ\n\"\nlog p (s0) +\nTÕ\nt=0\nlog p (st+1|st, at ) + log πθ (at |st )\n#\n(34)\n=\nTÕ\nt=0\n+θ log πθ (at |st ).\n(35)\nInserting Equation 34 in Equation 31, we arrive at the REINFORCE algorithm (Williams, 1992):\n+θJ (θ) = Åτ∼pπθ (τ)\n\"\nRτ\nTÕ\nt=0\n+θ log πθ (at |st )\n#\n(36)\n≈1\nN\nÕ\nτ∼pπθ (τ)\n TÕ\nt=0\nγt rt\n!  TÕ\nt=0\n+θ log πθ (at |st )\n!\n,\n(37)\nwhere the second line approximates the expectation with N empirical rollouts of the policy in the environment. In-\nJensen\n15\ntuitively, Equation 37 says that we should preferentially upregulate the probability of trajectories with high reward.\nImportantly, it no longer diﬀerentiates through the environment – only the policy.\nWhile the REINFORCE algorithm is unbiased, it also has high variance, which can make learning slow and unstable. It\nis therefore common to introduce modiﬁcations that reduce the variance. The ﬁrst of these comes from noting that\nan action taken at time t cannot aﬀect the reward at times t ′ < t. We therefore deﬁne Rt := ÍT\nt ′=t γt ′−t rt ′ and write\na new update rule as\nˆ+θJ (θ) ≈1\nN\nÕ\nτ∼pπθ (τ)\nTÕ\nt=0\nRt +θ log πθ (at |st ).\n(38)\nThis is the formulation most commonly used in the literature, but it is not actually the same as Equation 37, which\nwould use Rt = ÍT\nt ′=t γt ′−0rt ′. As brieﬂy discussed in Section 2, this is because the discount factor γ is generally used\nas a variance reduction method rather than because we intrinsically care less about rewards later in the task. In fact,\nEquation 38 is not strictly speaking a gradient (Nota and Thomas, 2019), which is why we denote it ˆ+.\nIt can also be shown that subtracting an action-independent baseline from Rt does not change the expectation in\nEquation 38, while potentially reducing its variance. A common choice is the expected future reward V (st ):\nˆ+θJ (θ) ≈1\nN\nÕ\nτ∼pπθ (τ)\nTÕ\nt=0\n(Rt −V (st ))+θ log πθ (at |st ).\n(39)\nIntuitively, Equation 39 upregulates the probability of actions that lead to higher-than-expected reward and downreg-\nulates the probability of actions that lead to lower-than-expected reward.\nFinally, it is common to reduce the variance of the gradient estimate further through an approach known as ‘bootstrap-\nping’, which approximates Rt ≈rt +γV (st+1). This is useful because rt +γV (st+1) has lower variance than ÍT\nt ′=t γt ′−t rt ′.\nWe therefore replace (Rt −V (st )) in Equation 39 with the ‘advantage function’ A(st, at ) = Q (st, at ) −V (st ) ≈\nrt + γV (st+1) −V (st ). In between these two extreme cases of a full Monte Carlo estimate of Rt and a ‘one-step’\nbootstrap, the sum in Rt can be truncated to any order, with Rt ′ replaced by V (st ′ ) (Sutton and Barto, 2018). In\ntheory, this gradient estimate remains unbiased if the value function is correct. In practice, the learned estimate of\nV (st ′ ) will be inexact, which biases the parameter updates. Bootstrapping therefore leads to a tradeoﬀbetween the\nbias and variance of parameter updates.\nThese variance reduction approaches give rise to the so-called ‘actor-critic’ algorithm, where an agent both computes a\npolicy π (the actor) and an ‘evaluation’ of the policy in the form of state(-action) values (the critic). A rich neuroscience\nliterature suggests that the basal ganglia of biological agents implement an actor-critic-like algorithm. Here, dorsal\nstriatum is proposed to implement the ‘actor’ and ventral striatum the ‘critic’ (Takahashi et al., 2008; Sutton and Barto,\n2018; O’Doherty et al., 2004).\nFor these actor-critic algorithms, it is common to parameterize both the policy πθ (a|s) and value functionVθ (s) with\nneural networks. To optimize these parameters using out-of-the-box automatic diﬀerentiation, we need to write down\nan ‘objective function’ with the correct gradients – but we saw in Equation 30 that this cannot simply be the expected\nreward. Instead, we deﬁne an auxiliary utility (i.e. negative loss)\n˜J (θ) = 1\nN\nÕ\nτ∼pπθ (τ)\nTÕ\nt=0\n(Rt −V (st )) log πθ (at |st ),\n(40)\n16\nJensen\n0\n1000\nepisode\n16\n18\n20\nreward\n0\n10\n20\naction number\n0.0\n0.5\n1.0\np(r|a = 1)\naction 1\naction 2\nPC 1\nPC 2\nA\nB\nC\np(r|a = 1) = 0\np(r|a = 1) = 1\nFIGUR E 5\nMeta-reinforcement learning. The results in this ﬁgure reproduce some of the analyses in Figure 1 of\nWang et al. (2018). (A) We trained a recurrent meta-reinforcement learning agent in a two-armed bandit task, where\nthe reward probabilities of each arm were sampled independently from U(0, 1) at the beginning of each episode and\nremained ﬁxed throughout the episode. A recurrent neural network was trained across many episodes with diﬀerent\nreward probabilities using an actor-critic algorithm. The input to the agent consisted of the previous action, the previous\nreward, and the time-within-trial. The average reward per episode is plotted against the episode number, showing that\nthe agent gradually learns to adapt within each episode to the particular instantiation of the bandit task. Importantly,\nthe parameters of the network are ﬁxed within an episode, meaning that this adaptation occurs through the recurrent\ndynamics. Dashed horizontal lines indicate the reward of an agent selecting random actions and an ‘oracle’ agent that\nalways chooses the best arm. (B) Heatmap showing example behaviour of the agent in episodes with diﬀerent reward\nprobabilities for the ﬁrst arm, p (r |a = 1). For the analysis here and in (C), we set p (r |a = 2) = 1 −p (r |a = 1). Across\nepisodes, the agent experiments with diﬀerent actions and eventually converges on the optimal action. For episodes\nwith more similar reward probabilities (near the middle), it takes longer to identify the optimal action. This balance\nbetween exploration and exploitation is mediated by the recurrent network dynamics, which are learned over many\nepisodes using deep reinforcement learning. (C) We averaged the hidden state of the RNN over 100 episodes for\neach of several diﬀerent reward probabilities, ranging from low (green) to high (blue) p (r |a = 1). We then performed\nPCA on the resulting matrix of average hidden states to compute a low-dimensional trajectory over the course of\nan episode for each reward probability. This two-dimensional embedding of neural activity converges to diﬀerent\nregions of state space during the episode for diﬀerent reward probabilities. Black cross indicates the hidden state at\nthe beginning of an episode, and coloured points indicate the ﬁnal hidden state in an episode for the diﬀerent reward\nprobabilities.\nwhere Rt can optionally be approximated by rt +γV (st+1). While ˜J (θ) has no intrinsic interpretation, it is chosen such\nthat +θ ˜J (θ) = ˆ+θJ (θ) when treating δt := (Rt −V (st )) as constant w.r.t θ, and the gradients can be computed using\nstandard automatic diﬀerentiation. The gradient of the value function loss is then given by +θ\nÍ\nt 1\n2 (Rt −Vθ (st ))2 =\nÍ\nt [−δt +θVθ (st )].\nWhile these policy gradient methods may seem far removed from neuroscience, it has been found that neural networks\ntrained with policy gradients often learn representations and behaviours reminiscent of biological organisms (Wang\net al., 2018; Jensen et al., 2023; Merel et al., 2019; Li et al., 2022; Song et al., 2017).\n|\nMeta-reinforcement learning\nA prominent example of deep reinforcement learning providing insights into biological circuits is the ‘recurrent meta-\nreinforcement learning’ model developed by Wang et al. (2018). The authors trained a recurrent deep RL agent using\npolicy gradients, where the RNN parameters are conﬁgured by learning from rewards over long periods of time from\nmany tasks that have a shared underlying structure. Importantly, this ‘slow’ model-free learning process gives rise to\nan agent that can rapidly learn from experience with ﬁxed parameters when exposed to a new task from the same task\ndistribution. This is achieved by the agent learning to eﬀectively implement a fast RL-like algorithm in the dynamics\nof the network (Figure 5). This process, whereby an agent trained slowly on a large distribution of tasks can rapidly\nJensen\n17\nadapt to a new task, is known as ‘meta-reinforcement learning’ (Finn et al., 2017; Ritter et al., 2018; Duan et al., 2016;\nWang et al., 2016). Wang et al. (2018) suggested that prefrontal cortex resembles such a recurrent meta-RL system,\nand their model explained a range of neuroscientiﬁc ﬁndings. This included\n• Dynamic adaptation of the eﬀective learning rate of an agent to the volatility of the environment (Behrens et al.,\n2007).\n• The emergence of ‘model-based’ behaviour in the ‘two-step’ task commonly used to distinguish between model-\nfree and model-based RL (Miller et al., 2017; Daw et al., 2011).\n• The ability of animals to get progressively faster at learning when exposed to multiple tasks with a consistent\nabstract task structure (Harlow, 1949).\nFurther experimental evidence for this meta-RL framework comes from Hattori et al. (2023), who showed that across-\nsession learning in a reversal learning task relied on synaptic plasticity in orbitofrontal cortex (OFC; a subregion of\nPFC), while within-session learning relied on recurrent dynamics in OFC. Recently, Jensen et al. (2023) also extended\nthe work of Wang et al. (2018) to allow the meta-RL network dynamics to update the policy from imagined experience\nusing a learned model of the environment – reminiscent of Dyna, but now implemented in RNN dynamics instead of\nparameter updates.\n8\n|\nDISTRIBUTIONAL REINFORCEMENT LEARNING\nIn Equation 3 and Equation 10, we deﬁned the expected future reward for a given state or state-action pair. The meth-\nods considered so far have only used such expectations as a learning signal. However, recent research suggests that\nanimals may in fact estimate entire future reward distributions (Dabney et al., 2020; Sousa et al., 2023). These studies\nwere inspired by ﬁndings that such distributional RL can improve the performance of artiﬁcial agents (Bellemare et al.,\n2017, 2023; Dabney et al., 2018). To formalize this, we use Z π (s, a) to denote a single sample from the distribution\nover possible cumulative discounted future rewards resulting from following policy π after taking action a in state s:\nZ π (s, a) ∼pzπ\n Õ\nt ′>=t\nγt ′−t rt ′ |st = s, at = a\n!\n(41)\nThe stochasticity of Z π can both be due to stochasticity in environment dynamics and reward, and it can be due to\nstochasticity in the policy of the agent itself. Clearly, the expectation of Z π (s, a) equals the corresponding Q value:\nÅpzπ\n\u0002\nZ π (s, a)\n\u0003\n= Q π (s, a).\n(42)\nInstead of only estimating this expectation, we now want to learn the full distribution of returns, pzπ (Z π (s, a)). One\nnormative reason to learn this distribution is to develop methods that are risk averse (Morimura et al., 2010, 2012)\nor explicitly take into account uncertainty (Dearden et al., 1998). However, recent work has suggested that such a\ndistributional approach can also increase expected reward by improving representation learning in the deep RL setting\n(Bellemare et al., 2017; Dabney et al., 2018; Rowland et al., 2019; Bellemare et al., 2023). This is because traditional\ndeep RL only distinguishes states that have diﬀerent expected value, while distributional RL learns to distinguish\nstates that have diﬀerent value distributions (Figure 6A).\nTo implement distributional RL, we consider the τth expectile of pzπ (Z π (s, a)), ϵτ, which is deﬁned for a random\nvariable Z as the solution to the equation\nτÅ[max(0, Z −ϵτ )] = (1 −τ)Å[max(0, ϵτ −Z )].\n(43)\n18\nJensen\nThis is a generalization of the mean, which is recovered for τ = 0.5, similar to how the quantile generalizes the notion\nof a median. A distribution is uniquely speciﬁed by its expectiles, and we can therefore represent pzπ (Z π (s, a)) in\nterms of {ϵτ }. Translating this to an RL algorithm involves training a network (or tabular values) to predict a set of\nexpectiles for a given state (and action). The parameters of the agent are then updated by propagating the distribution\nimplied by the predicted expectiles through the Bellman equation and minimizing the diﬀerence between the initial\nand propagated distributions (Bellemare et al., 2023).\nIn the tabular value learning case (c.f. Section 3), this can be achieved using a modiﬁed TD-update rule (c.f. Equation 9;\nLowet et al., 2020; Figure 6B). In particular, we consider a set of units {Vτi (s)}, each with a target expectile τi :=\nα+\ni\nα+\ni +α −\ni\nof the return distribution pzπ (Z π (s)) (where pzπ (Z π (s)) = Í\na [π (a|s)pzπ (Z π (s))]). These expectiles can\nbe learned by sampling experience from the environment under the policy and deﬁning a TD error for each unit and\nstate transition as\nδi := rt + γ ˜Z π\nj (st+1) −Vτi (st ).\n(44)\nHere, ˜Z π\nj (st+1) is a random sample from the learned approximate distribution of cumulative future returns from state\nst+1, p{Vτi } ( ˜Z π (st+1)) (Lowet et al., 2020; Dabney et al., 2020). We then apply the following update rule to all units:\n∆Vτi (st ) = α+\ni max(δi , 0) + α −\ni min(δi , 0).\n(45)\nIn other words, we apply the TD update rule to each unit with learning rate α+\ni for positive TD errors and learning rate\nα −\ni\nfor negative TD errors. When running this algorithm to convergence, Vτi (s) will approach the τi th expectile (ϵτi )\nof pzπ (Z π (s, a)). In the deep RL setting, we would compute gradients of the form +θ L = Í\ni ∆Vτi (st )∂Vτi (st )/∂θ to\nlearn a model with parameters θ that predicts the full set of expectiles. We refer to Bellemare et al. (2017); Dabney\net al. (2018); Rowland et al. (2019); Bellemare et al. (2023); and Dabney et al. (2020) for additional mathematical\ndetails, alternative parameterizations of the return distribution, and extensions to the control setting.\nIntriguingly, recent work in neuroscience suggests that distributional RL could underlie value learning in biological neu-\nral circuits (Dabney et al., 2020; Lowet et al., 2020; Sousa et al., 2023). In particular, Dabney et al. (2020) recorded the\nactivity of dopaminergic VTA neurons during a task with stochastic rewards and showed that the neurons appeared to\nrepresent the full distribution of possible outcomes using an expectile representation. More concretely, they showed\nthat:\n• The VTA neurons exhibited a range of diﬀerent ‘reversal points’ – deﬁned as the reward magnitude at which the\nﬁring rate of a neuron did not change from its baseline ﬁring rate. This is consistent with a distributional RL theory,\nwhere the changes in neural ﬁring rates from baseline correspond to the expectile TD updates considered above. In\nthis case, the reversal point of a neuron i should be Vτi ≈ϵτi (Figure 6C).\n• Neurons had diﬀerent slopes describing the relationship between expected reward and ﬁring rate in the regimes\nwhere expected reward was above and below the reversal point (Vτi ) of each neuron. This is consistent with the\nalgorithm described in Equation 45 (Figure 6C).\n• When independently ﬁtting a slope to the data above (α+\ni ) and below (α −\ni ) the reversal point of neuron i, the reversal\npoint of the neuron was positively correlated with τi =\nα+\ni\nα −\ni +α+\ni\n. This is consistent with the expectile distributional RL\nsetting, where the reversal point is Vτi ≈ϵτi , which is monotonic in τi (Figure 6C).\n• When ‘imputing’ the distribution implied by the VTA neurons when interpreted as expectiles (Figure 6D), the re-\nsulting ﬁt resembled the true distribution of rewards in the experiment.\nThese ﬁndings generalize the canonical RPE view of Schultz et al. (1997), which can be seen as the averaged version\nJensen\n19\nFIGUR E 6\nDistributional reinforcement learning. (A) Example of distributional RL improving representation learn-\ning. Two states (e.g. ‘banana’ and ‘lemon’) may have the same expected future reward but diﬀerent reward distributions\n(top row). A standard RL agent only has to predict the mean (orange vertical lines) and may learn a simple predictive\nfeature like ‘yellow’ that combines both states. If the expected future reward now changes for one state (bottom\nrow), e.g. because the agent learned to make a banana smoothie, it may erroneously generalize to all yellow fruits\n(bottom right; red line). However, a distributional agent is forced to learn an initial representation that distinguishes\nthe states, which can improve downstream learning and prevent overgeneralization (blue curves). (B) Distributional\nRL simulations on a value estimation task with a single state and no actions, reproducing key ideas from Dabney et al.\n(2020). Distribution of rewards (black), plotted together with the learned values Vτi across 20 units learning through\nstandard TD learning (orange; all τi = 0.5), or through distributional RL with diﬀerent τi =\nα+\ni\nα+\ni +α −\ni\n(blue). The TD units\nall converge to the mean reward, while the expectile units end up tiling the distribution. (C) For both the TD units and\ndistributional units from (B), we plot the temporal diﬀerence updates performed in response to diﬀerent rewards from\nthe reward distribution. These updates have been proposed to be represented in the ﬁring rates of dopaminergic VTA\nneurons relative to baseline (Schultz et al., 1997; Dabney et al., 2020). The TD units show a constant linear scaling\nacross positive and negative rewards, while the distributional units show an asymmetric scaling of ﬁring rate with\nreward magnitude above and below their reversal point (black horizontal line). The ratio of slopes above and below\nthe reversal point scales positively with the value of the reversal point. These features of dopaminergic VTA neurons\nwere used by Dabney et al. (2020) to argue that the brain implements a form of distributional RL. (D) True reward\ndistribution (black) reproduced from (B), now plotted together with the reward distribution p{Vτi } ( ˜Z π (s)) implied by\nthe distributional units from (B) and (C) at diﬀerent stages of learning (green to blue). These imputed distributions\nwere computed by assuming that the value Vτi learned by unit i corresponds to expectile τi =\nα+\ni\nα+\ni +α −\ni\nof the reward\ndistribution. We then infer the distribution implied by these expectiles under the assumption that it consists of a set\nof 20 delta functions (Rowland et al., 2019). Finally, we convolved the resulting delta functions with a Gaussian kernel\n(σ = 0.1) for visualization. This process was repeated using {Vτi } at diﬀerent stages of learning. The units were all\ninitialized at Vτi = 0.5, so the initial distribution is a delta function at r = 0.5 (green). At the end of learning, the pop-\nulation faithfully represents the true reward distribution, capturing key features including bimodality and the relative\nmagnitude of the two modes (blue). Dabney et al. (2020) used a similar approach to infer the distribution encoded by\ndopaminergic VTA neurons at the end of animal training and found a close match to the true reward distribution.\nof the theory by Dabney et al. (2020). The expectile regression algorithm investigated by Dabney et al. (2020) relies\non non-local TD updates and non-linear ‘imputation’ of the return distribution p{Vτi } ( ˜Z π (s)) induced by the learned\nexpectiles {Vτi (s)}. However, recent work has suggested more biologically plausible distributional RL updates to\naddress these challenges (Tano et al., 2020; Wenliang et al., 2023).\n9\n|\nLEARNING FROM SCALAR REWARDS?\nThe methods discussed so far have all relied on some scalar ‘reward’ available from the environment. Distributional RL\npredicted the full distribution of possible rewards but still assumed that an external reward was eventually observed.\nThis is perhaps reasonable in many experimentally controlled settings, where we deliver juice or water to animals as a\n20\nJensen\nfunction of their actions. It is also often appropriate in machine learning, where we want to optimize some externally\nimposed objective that can be quantiﬁed. However, in natural environments, there is no such ‘global’ reward – instead,\ndiﬀerent actions can be rewarded in diﬀerent ways that must be converted to an internal learning signal (Juechems and\nSummerﬁeld, 2019). For example, foraging for food might reduce hunger, while collecting water might reduce thirst.\nStarting a family might have an initial energy cost, but it could improve survival later in life and propagate the gene\npool. An RL purist would convert all of these diﬀerent gains and losses into a single common unit and balance them\nappropriately to decide what actions to take (Silver et al., 2021). However, it has also been suggested that biological\nagents adaptively change their instantaneous objective via a higher-order controller in e.g. prefrontal cortex, which\ndetermines what the current lower-level objective is (Juechems and Summerﬁeld, 2019; Miller and Cohen, 2001;\nBotvinick, 2008). This resembles ideas in hierarchical reinforcement learning (Pateria et al., 2021; Botvinick, 2008),\nbut here the higher-order policies are not necessarily learned through reinforcement learning within a lifetime, instead\nemerging during evolution where survival depends on the discovery of ‘useful’ objectives.\nA related challenge is the technical diﬃculty of learning from scalar rewards, which are generally sparse. While the\nmethods in Section 7 can train a neural network to maximize reward in theory, in practice they are often noisy, unsta-\nble, or ﬁnd local minima. As a topical example, consider the challenge of training large language models to interact\nwith human users. Training such a model from scratch using reinforcement learning would be near-impossible, but\n‘pretraining’ the model on a large unsupervised dataset followed by ‘reinforcement learning from human feedback’ has\nproven hugely successful and revolutionized language models for human interactions (Team et al., 2023). This works\nbecause the large unlabelled dataset provides a way to learn good representations that distinguish diﬀerent concepts,\nsince this is necessary to solve the base task of predicting the next word. Once such representations are learned, the\nsubsequent ﬁnetuning for human interaction is an easier problem that can be solved with reinforcement learning.\nOf more relevance to neuroscience, it is also common in deep RL to introduce additional ‘auxiliary costs’ to the utility\nfunction that are jointly optimized together with reward maximization, and which can use a richer data source to\ndrive representation learning in the network (Jaderberg et al., 2016). A popular approach is to include losses that\nrequire the agent to predict the next observation from the current state and action (Jaderberg et al., 2016; Zintgraf\net al., 2019). Such predictive losses have close parallels in neuroscience, where it has been suggested that predictive\nlearning could drive many of the representations observed in biological circuits (Rao and Ballard, 1999; Stachenfeld\net al., 2017; Whittington et al., 2020; Blanco-Pozo et al., 2021) and serve as a foundation for model-based planning\n(Jensen et al., 2023). Fang and Stachenfeld (2023) also recently showed that augmenting an RL agent with an auxiliary\npredictive objective leads to neural representations that resemble biological circuits more closely. This suggests a\npotentially important interaction between self-supervised representation learning and reward-driven reinforcement\nlearning in biological circuits.\nIf a good predictive model of the world is learned, this model can also be used together with a search algorithm to turn\nthe reinforcement learning problem into a supervised learning problem. As an example, the MuZero model developed\nby Schrittwieser et al. (2020) to learn Atari, chess, shogi, and Go, was trained to predict future values and actions by\nunrolling the environment using a learned latent representation. It then used Monte Carlo tree search (MCTS; a model\nbased-search algorithm c.f. Section 5) and the predicted value function to improve the provisional actions predicted\nby the base policy network. These ﬁnal actions, which had been optimized using MCTS, were treated as a supervised\nlearning target for the base policy network. Through many iterations of such predictive learning, MCTS-based policy\nimprovement, and training of the base policy, the network learned both the transition structure of the task and to\nselect good actions. Such semi-supervised approaches to reinforcement learning are useful in the common setting\nwhere learning the transition structure of the world is easier than learning a policy (Jensen et al., 2023). These methods\nalso have interesting parallels to learning in biological networks, where interactions between model-based and model-\nJensen\n21\nfree systems are similarly thought to drive action selection based on learned latent representations (Botvinick et al.,\n2020).\nFinally, algorithms that seek to maximize scalar rewards often run into challenges related to exploration. In particular,\nonce an above-average policy has been identiﬁed, exploration is disfavored because it has lower expected reward,\neven if there is a ﬁnite probability of discovering new and better policies. This is why imposing stochasticity in the\nform of e.g. ϵ-greedy policies is common in the tabular RL literature. In deep RL, it is instead popular to include\nvarious forms of ‘exploration bonuses’ in the objective function. Policy gradient algorithms for example often add an\nauxiliary entropy loss of the form LE = Í\nτ∼pπθ (τ)\nÍT\nt=0\nÍ\na πθ (at |st ) log πθ (at |st ), and indeed the bandit example in\nFigure 5 does not work without such an entropy loss. Other approaches to improve exploration include hierarchical\nreinforcement learning, which introduces autocorrelations in the policy to improve coverage of the state space. This\nis reminiscent of the biological ‘Lévy ﬂight’ hypothesis, which suggests that animals explore an environment using\na heavy-tailed distribution of ‘step sizes’ to maximize the probability of ﬁnding sparse rewards (Viswanathan et al.,\n1999). Biological agents are also thought to engage in periods of ‘random exploration’ that can be interpreted as a\nbiological parallel to ϵ-greedy-like algorithms. Random exploration appears to be under noradrenergic control (Tervo\net al., 2014; Dubois et al., 2021), while the zona incerta can drive more directed exploration (Ahmadlou et al., 2021).\nSuch neural control of exploration is particularly well-characterized in the zebra ﬁnch song circuit (Ölveczky et al.,\n2005), and it is consistent with a view of top-down imposition of ﬂexible reward functions – where one possible\nobjective could be to increase variability or reduce uncertainty – rather than optimization of a global scalar reward.\n10\n|\nDISCUSSION\nIn this review, we have provided a mathematical overview of some of the many reinforcement learning methods that\nare commonly used in systems and computational neuroscience. We have also highlighted a range of explicit parallels\nbetween these methods and experimental results in neuroscience and cognitive science to illustrate the utility of\nreinforcement learning as a framework for understanding biological learning and decision making. This has ranged\nfrom classical work on reward prediction errors (Schultz et al., 1997) to recent ﬁndings of distributional reinforcement\nlearning in biological circuits (Dabney et al., 2020).\nWhile RL has already had a profound inﬂuence on systems neuroscience, several open questions remain. In particular,\nmuch work has focused on simple stimulus-response or binary decision making tasks. This is a far reach from etholog-\nically relevant problems that involve processing multimodal stimuli, decision making with long-lasting consequences,\nand high-dimensional motor control. Some recent work building on deep RL has started to bridge this gap. For ex-\nample, Banino et al. (2018) demonstrated the emergence of grid cells in agents navigating complex environments,\nAldarondo et al. (2024) showed similarities between an RL agent trained to control a virtual rodent and corresponding\nbiological motor representations, and Jensen et al. (2023) showed parallels between a recurrent meta-RL agent and\nhuman behaviour in a navigation task requiring temporally extended thinking. However, much work remains to extend\nour neuroscientiﬁc understanding to ethologically relevant settings, both experimental and computational.\nA related challenge will be to combine diﬀerent components of existing models to capture the generalist nature of\nbiological circuits. This is in contrast to past work, which has often focused on a single neural circuit or function, such\nas motor control or navigation. Such a generalist approach will involve explicit modeling of the roles of diﬀerent brain\nregions, and more importantly it will require us to capture how they interact with one another during learning and\ndecision making. Clearly, such models will need to be constrained by experimental data, both at the level of behaviour\nand at the level of neural activity. This is becoming increasingly feasible with recent advances in recording technologies,\nboth for high-resolution behavioural tracking (Mathis et al., 2018; Dunn et al., 2021) and for simultaneous and long-\nterm recording of neural activity (Steinmetz et al., 2021; Pachitariu et al., 2016; Dhawale et al., 2017).\n22\nJensen\nFinally, most work on reinforcement learning in neuroscience has considered short-term decision making tasks, where\nplanning and decision making in primitive state and action spaces are feasible. This is in stark contrast to most human\ndecision making, which occurs over extended timescales and often involves hierarchies of decisions or priorities that\nchange over time. For example, we may decide to pursue an undergraduate degree at Cambridge University, which\nthen requires us to (i) write an application, (ii) prepare for an interview, and (iii) arrange our travel. Each of these\nprocesses in turn require us to plan increasingly low-level decisions, such as booking a ﬂight or deciding which bus to\ntake to the airport. This is the topic of hierarchical reinforcement learning, which has already been highlighted as a\npotentially useful model of human behaviour (Eckstein and Collins, 2020; Botvinick, 2008; Botvinick et al., 2009) and\nis becoming an increasingly important area of research in machine learning (Pateria et al., 2021). While reinforcement\nlearning will undoubtedly remain important for understanding such ﬂexible animal behaviour, we should also keep in\nmind the shortcomings and challenges of learning from scalar rewards (Section 9). It will be important to investigate\nthe regimes in which reinforcement learning provides a good model of behaviour and neural dynamics, while also\nexploring other frameworks with potentially richer learning substrates. This will also enable the study of interactions\nbetween such diﬀerent learning algorithms, which will likely be necessary to understand biological learning in rich\nenvironments.\nAcknowledgements\nGuillaume Hennequin, Will Dorrell, and the NBDT reviewers and editor provided valuable feedback that improved and\nclariﬁed the manuscript. Most of the content in this review was originally prepared for the 2023 Janelia Theoretical\nNeuroscience Workshop.\nReferences\nAgrawal, M., Mattar, M. G., Cohen, J. D. and Daw, N. D. (2022) The temporal dynamics of opportunity costs: A normative\naccount of cognitive fatigue and boredom. Psychological review, 129, 564.\nAhmadlou, M., Houba, J. H., van Vierbergen, J. F., Giannouli, M., Gimenez, G.-A., van Weeghel, C., Darbanfouladi, M., Shirazi,\nM. Y., Dziubek, J., Kacem, M. et al. (2021) A cell type–speciﬁc cortico-subcortical brain circuit for investigatory and novelty-\nseeking behavior. Science, 372, eabe9681.\nAkam, T., Costa, R. and Dayan, P. (2015) Simple plans or sophisticated habits? state, transition and learning interactions in the\ntwo-step task. PLoS computational biology, 11, e1004648.\nAldarondo, D., Merel, J., Marshall, J. D., Hasenclever, L., Klibaite, U., Gellis, A., Tassa, Y., Wayne, G., Botvinick, M. and Ölveczky,\nB. P. (2024) A virtual rodent predicts the structure of neural activity across behaviors. Nature, 1–3.\nAlvernhe, A., Save, E. and Poucet, B. (2011) Local remapping of place cell ﬁring in the Tolman detour task. European Journal\nof Neuroscience, 33, 1696–1705.\nBanino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., Pritzel, A., Chadwick, M. J., Degris, T., Modayil, J. et al.\n(2018) Vector-based navigation using grid-like representations in artiﬁcial agents. Nature, 557, 429–433.\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P. and Silver, D. (2017) Successor features for transfer\nin reinforcement learning. Advances in neural information processing systems, 30.\nBehrens, T. E., Woolrich, M. W., Walton, M. E. and Rushworth, M. F. (2007) Learning the value of information in an uncertain\nworld. Nature neuroscience, 10, 1214–1221.\nJensen\n23\nBellemare, M. G., Dabney, W. and Munos, R. (2017) A distributional perspective on reinforcement learning. In International\nconference on machine learning, 449–458. PMLR.\nBellemare, M. G., Dabney, W. and Rowland, M. (2023) Distributional reinforcement learning. MIT Press.\nBlanco-Pozo, M., Akam, T. and Walton, M. (2021) Dopamine reports reward prediction errors, but does not update policy,\nduring inference-guided choice. bioRxiv.\nBlodgett, H. C. (1929) The eﬀect of the introduction of reward upon the maze performance of rats. University of California\npublications in psychology.\nBotvinick, M. and Toussaint, M. (2012) Planning as inference. Trends in cognitive sciences, 16, 485–488.\nBotvinick, M., Wang, J. X., Dabney, W., Miller, K. J. and Kurth-Nelson, Z. (2020) Deep reinforcement learning and its neurosci-\nentiﬁc implications. Neuron, 107, 603–616.\nBotvinick, M. M. (2008) Hierarchical models of behavior and prefrontal function. Trends in cognitive sciences, 12, 201–208.\nBotvinick, M. M. and Cohen, J. D. (2014) The computational and neural basis of cognitive control: charted territory and new\nfrontiers. Cognitive science, 38, 1249–1285.\nBotvinick, M. M., Niv, Y. and Barto, A. G. (2009) Hierarchically organized behavior and its neural foundations: A reinforcement\nlearning perspective. cognition, 113, 262–280.\nCallaway, F., van Opheusden, B., Gul, S., Das, P., Krueger, P. M., Griﬃths, T. L. and Lieder, F. (2022) Rational use of cognitive\nresources in human planning. Nature Human Behaviour, 6, 1112–1125.\nCoddington, L. T. and Dudman, J. T. (2019) Learning from action: reconsidering movement signaling in midbrain dopamine\nneuron activity. Neuron, 104, 63–77.\nDabney, W., Kurth-Nelson, Z., Uchida, N., Starkweather, C. K., Hassabis, D., Munos, R. and Botvinick, M. (2020) A distributional\ncode for value in dopamine-based reinforcement learning. Nature, 577, 671–675.\nDabney, W., Rowland, M., Bellemare, M. and Munos, R. (2018) Distributional reinforcement learning with quantile regression.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 32.\nDaw, N. D., Gershman, S. J., Seymour, B., Dayan, P. and Dolan, R. J. (2011) Model-based inﬂuences on humans’ choices and\nstriatal prediction errors. Neuron, 69, 1204–1215.\nDaw, N. D., Niv, Y. and Dayan, P. (2005) Uncertainty-based competition between prefrontal and dorsolateral striatal systems\nfor behavioral control. Nature neuroscience, 8, 1704–1711.\nDayan, P. (1993) Improving generalization for temporal diﬀerence learning: The successor representation. Neural computation,\n5, 613–624.\nDearden, R., Friedman, N. and Russell, S. (1998) Bayesian Q-learning. Aaai/iaai, 1998, 761–768.\nDeisenroth, M. and Rasmussen, C. E. (2011) Pilco: A model-based and data-eﬃcient approach to policy search. In Proceedings\nof the 28th International Conference on machine learning (ICML-11), 465–472.\nDhawale, A. K., Poddar, R., Wolﬀ, S. B., Normand, V. A., Kopelowitz, E. and Ölveczky, B. P. (2017) Automated long-term\nrecording and analysis of neural activity in behaving animals. Elife, 6, e27702.\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I. and Abbeel, P. (2016) RL2: Fast reinforcement learning via slow\nreinforcement learning. arXiv preprint arXiv:1611.02779.\nDubois, M., Habicht, J., Michely, J., Moran, R., Dolan, R. J. and Hauser, T. U. (2021) Human complex exploration strategies are\nenriched by noradrenaline-modulated heuristics. Elife, 10, e59907.\n24\nJensen\nDunn, T. W., Marshall, J. D., Severson, K. S., Aldarondo, D. E., Hildebrand, D. G., Chettih, S. N., Wang, W. L., Gellis, A. J., Carlson,\nD. E., Aronov, D. et al. (2021) Geometric deep learning enables 3D kinematic proﬁling across species and environments.\nNature methods, 18, 564–573.\nEckstein, M. K. and Collins, A. G. (2020) Computational evidence for hierarchically structured reinforcement learning in hu-\nmans. Proceedings of the National Academy of Sciences, 117, 29381–29389.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I. et al. (2018)\nIMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In International conference\non machine learning, 1407–1416. PMLR.\nFang, C. and Stachenfeld, K. L. (2023) Predictive auxiliary objectives in deep RL mimic learning in the brain. arXiv preprint\narXiv:2310.06089.\nFinn, C., Abbeel, P. and Levine, S. (2017) Model-agnostic meta-learning for fast adaptation of deep networks. In International\nconference on machine learning, 1126–1135. PMLR.\nGeerts, J. P., Chersi, F., Stachenfeld, K. L. and Burgess, N. (2020) A general model of hippocampal and dorsal striatal learning\nand decision making. Proceedings of the National Academy of Sciences, 117, 31427–31437.\nGershman, S. J. (2018) The successor representation: its computational logic and neural substrates. Journal of Neuroscience,\n38, 7193–7200.\nGershman, S. J., Assad, J. A., Datta, S. R., Linderman, S. W., Sabatini, B. L., Uchida, N. and Wilbrecht, L. (2024) Explaining\ndopamine through prediction errors and beyond. Nature Neuroscience, 1–11.\nGriﬃths, T. L., Callaway, F., Chang, M. B., Grant, E., Krueger, P. M. and Lieder, F. (2019) Doing more with less: meta-reasoning\nand meta-learning in humans and machines. Current Opinion in Behavioral Sciences, 29, 24–30.\nGronauer, S. and Diepold, K. (2022) Multi-agent deep reinforcement learning: a survey. Artiﬁcial Intelligence Review, 1–49.\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P. et al. (2018) Soft\nactor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.\nHarlow, H. F. (1949) The formation of learning sets. Psychological review, 56, 51.\nHattori, R., Hedrick, N. G., Jain, A., Chen, S., You, H., Hattori, M., Choi, J.-H., Lim, B. K., Yasuda, R. and Komiyama, T. (2023)\nMeta-reinforcement learning via orbitofrontal cortex. Nature Neuroscience, 1–10.\nHorvitz, J. C. (2000) Mesolimbocortical and nigrostriatal dopamine responses to salient non-reward events. Neuroscience, 96,\n651–656.\nHowe, M. W., Tierney, P. L., Sandberg, S. G., Phillips, P. E. and Graybiel, A. M. (2013) Prolonged dopamine signalling in striatum\nsignals proximity and value of distant rewards. nature, 500, 575–579.\nHuys, Q. J., Eshel, N., O’Nions, E., Sheridan, L., Dayan, P. and Roiser, J. P. (2012) Bonsai trees in your head: how the Pavlovian\nsystem sculpts goal-directed choices by pruning decision trees. PLoS computational biology, 8, e1002410.\nJaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D. and Kavukcuoglu, K. (2016) Reinforcement learning\nwith unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397.\nJensen, K. T., Hennequin, G. and Mattar, M. G. (2023) A recurrent network model of planning explains hippocampal replay\nand human behavior. bioRxiv, 2023–01.\nJie, T. and Abbeel, P. (2010) On a connection between importance sampling and the likelihood ratio policy gradient. Advances\nin Neural Information Processing Systems, 23.\nJuechems, K. and Summerﬁeld, C. (2019) Where does value come from? Trends in cognitive sciences, 23, 836–850.\nJensen\n25\nKakade, S. and Dayan, P. (2002) Dopamine: generalization and bonuses. Neural Networks, 15, 549–559.\nKalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke,\nV. et al. (2018) QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation.\narXiv preprint\narXiv:1806.10293.\nKeramati, M., Smittenaar, P., Dolan, R. J. and Dayan, P. (2016) Adaptive integration of habits into depth-limited planning\ndeﬁnes a habitual-goal–directed spectrum. Proceedings of the National Academy of Sciences, 113, 12868–12873.\nKillcross, S. and Coutureau, E. (2003) Coordination of actions and habits in the medial prefrontal cortex of rats. Cerebral cortex,\n13, 400–408.\nLai, L. and Gershman, S. J. (2021) Policy compression: An information bottleneck in action selection. In Psychology of Learning\nand Motivation, vol. 74, 195–232. Elsevier.\nLengyel, M. and Dayan, P. (2007) Hippocampal contributions to control: the third way. Advances in neural information processing\nsystems, 20.\nLevine, S. (2018) Reinforcement learning and control as probabilistic inference:\nTutorial and review.\narXiv preprint\narXiv:1805.00909.\nLevine, S., Kumar, A., Tucker, G. and Fu, J. (2020) Oﬄine reinforcement learning: Tutorial, review, and perspectives on open\nproblems. arXiv preprint arXiv:2005.01643.\nLi, C., Kreiman, G. and Ramanathan, S. (2022) Integrating artiﬁcial and biological neural networks to improve animal task\nperformance using deep reinforcement learning. bioRxiv, 2022–09.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D. (2015) Continuous control with\ndeep reinforcement learning. arXiv preprint arXiv:1509.02971.\nLoukola, O. J., Solvi, C., Coscos, L. and Chittka, L. (2017) Bumblebees show cognitive ﬂexibility by improving on an observed\ncomplex behavior. Science, 355, 833–836.\nLowet, A. S., Zheng, Q., Matias, S., Drugowitsch, J. and Uchida, N. (2020) Distributional reinforcement learning in the brain.\nTrends in neurosciences, 43, 980–997.\nMakino, H. (2023) Arithmetic value representation for hierarchical behavior composition. Nature Neuroscience, 26, 140–149.\nMathis, A., Mamidanna, P., Cury, K. M., Abe, T., Murthy, V. N., Mathis, M. W. and Bethge, M. (2018) DeepLabCut: markerless\npose estimation of user-deﬁned body parts with deep learning. Nature neuroscience, 21, 1281–1289.\nMattar, M. G. and Daw, N. D. (2018) Prioritized memory access explains planning and hippocampal replay. Nature neuroscience,\n21, 1609–1617.\nMehta, M. R., Quirk, M. C. and Wilson, M. A. (2000) Experience-dependent asymmetric shape of hippocampal receptive ﬁelds.\nNeuron, 25, 707–715.\nMerel, J., Aldarondo, D., Marshall, J., Tassa, Y., Wayne, G. and Ölveczky, B. (2019) Deep neuroethology of a virtual rodent.\narXiv preprint arXiv:1911.09451.\nMiller, E. K. and Cohen, J. D. (2001) An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24,\n167–202.\nMiller, K. J., Botvinick, M. M. and Brody, C. D. (2017) Dorsal hippocampus contributes to model-based planning. Nature\nneuroscience, 20, 1269–1276.\nMizes, K. G., Lindsey, J., Escola, G. S. and Ölveczky, B. P. (2023a) Dissociating the contributions of sensorimotor striatum to\nautomatic and visually guided motor sequences. Nature Neuroscience, 1–14.\n26\nJensen\nMizes, K. G., Lindsey, J., Escola, G. S. and Olveczky, B. P. (2023b) Motor cortex is required for ﬂexible but not automatic motor\nsequences. bioRxiv, 2023–09.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. and Riedmiller, M. (2013) Playing Atari with deep\nreinforcement learning. arXiv preprint arXiv:1312.5602.\nMomennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D. and Gershman, S. J. (2017) The successor repre-\nsentation in human reinforcement learning. Nature human behaviour, 1, 680–692.\nMorimura, T., Sugiyama, M., Kashima, H., Hachiya, H. and Tanaka (2012) Parametric return density estimation for reinforce-\nment learning. arXiv preprint arXiv:1203.3497.\nMorimura, T., Sugiyama, M., Kashima, H., Hachiya, H. and Tanaka, T. (2010) Nonparametric return distribution approximation\nfor reinforcement learning. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), 799–806.\nMorris, G., Nevet, A., Arkadir, D., Vaadia, E. and Bergman, H. (2006) Midbrain dopamine neurons encode decisions for future\naction. Nature neuroscience, 9, 1057–1063.\nNiv, Y. (2009) Reinforcement learning in the brain. Journal of Mathematical Psychology, 53, 139–154.\nNota, C. and Thomas, P. S. (2019) Is the policy gradient a gradient? arXiv preprint arXiv:1906.07073.\nNowé, A., Vrancx, P. and De Hauwere, Y.-M. (2012) Game theory and multi-agent reinforcement learning. Reinforcement\nLearning: State-of-the-Art, 441–470.\nO’Doherty, J., Dayan, P., Schultz, J., Deichmann, R., Friston, K. and Dolan, R. J. (2004) Dissociable roles of ventral and dorsal\nstriatum in instrumental conditioning. science, 304, 452–454.\nOlds, J. and Milner, P. (1954) Positive reinforcement produced by electrical stimulation of septal area and other regions of rat\nbrain. Journal of comparative and physiological psychology, 47, 419.\nÖlveczky, B. P., Andalman, A. S. and Fee, M. S. (2005) Vocal experimentation in the juvenile songbird requires a basal ganglia\ncircuit. PLoS biology, 3, e153.\nPachitariu, M., Stringer, C., Schröder, S., Dipoppa, M., Rossi, L. F., Carandini, M. and Harris, K. D. (2016) Suite2p: beyond\n10,000 neurons with standard two-photon microscopy. BioRxiv, 061507.\nPadoa-Schioppa, C. and Assad, J. A. (2006) Neurons in the orbitofrontal cortex encode economic value. Nature, 441, 223–226.\nPan, Y., Cheng, C.-A., Saigol, K., Lee, K., Yan, X., Theodorou, E. and Boots, B. (2017) Agile autonomous driving using end-to-end\ndeep imitation learning. arXiv preprint arXiv:1709.07174.\nPateria, S., Subagdja, B., Tan, A.-h. and Quek, C. (2021) Hierarchical reinforcement learning: A comprehensive survey. ACM\nComputing Surveys (CSUR), 54, 1–35.\nPeshkin, L. and Shelton, C. R. (2002) Learning from scarce experience. arXiv preprint cs/0204043.\nPiray, P. and Daw, N. (2024) Reconciling ﬂexibility and eﬃciency: Medial entorhinal cortex represents a compositional cogni-\ntive map. bioRxiv, 2024–05.\nPiray, P. and Daw, N. D. (2021) Linear reinforcement learning in planning, grid ﬁelds, and cognitive control. Nature communi-\ncations, 12, 1–20.\nRao, R. P. and Ballard, D. H. (1999) Predictive coding in the visual cortex: a functional interpretation of some extra-classical\nreceptive-ﬁeld eﬀects. Nature neuroscience, 2, 79–87.\nRitter, S., Wang, J., Kurth-Nelson, Z., Jayakumar, S., Blundell, C., Pascanu, R. and Botvinick, M. (2018) Been there, done that:\nMeta-learning with episodic recall. In International conference on machine learning, 4354–4363. PMLR.\nJensen\n27\nRoesch, M. R., Calu, D. J. and Schoenbaum, G. (2007) Dopamine neurons encode the better option in rats deciding between\ndiﬀerently delayed or sized rewards. Nature neuroscience, 10, 1615–1624.\nRowland, M., Dadashi, R., Kumar, S., Munos, R., Bellemare, M. G. and Dabney, W. (2019) Statistics and samples in distributional\nreinforcement learning. In International Conference on Machine Learning, 5528–5536. PMLR.\nRushworth, M. F., Noonan, M. P., Boorman, E. D., Walton, M. E. and Behrens, T. E. (2011) Frontal cortex and reward-guided\nlearning and decision-making. Neuron, 70, 1054–1069.\nSchaul, T., Quan, J., Antonoglou, I. and Silver, D. (2015) Prioritized experience replay. arXiv preprint arXiv:1511.05952.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T.\net al. (2020) Mastering Atari, Go, chess and shogi by planning with a learned model. Nature, 588, 604–609.\nSchultz, W., Apicella, P., Scarnati, E. and Ljungberg, T. (1992) Neuronal activity in monkey ventral striatum related to the\nexpectation of reward. Journal of neuroscience, 12, 4595–4610.\nSchultz, W., Dayan, P. and Montague, P. R. (1997) A neural substrate of prediction and reward. Science, 275, 1593–1599.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T. et al.\n(2018) A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362,\n1140–1144.\nSilver, D., Singh, S., Precup, D. and Sutton, R. S. (2021) Reward is enough. Artiﬁcial Intelligence, 299, 103535.\nSolway, A. and Botvinick, M. M. (2012) Goal-directed decision making as probabilistic inference: a computational framework\nand potential neural correlates. Psychological review, 119, 120.\nSong, H. F., Yang, G. R. and Wang, X.-J. (2017) Reward-based training of recurrent neural networks for cognitive and value-\nbased tasks. Elife, 6, e21492.\nSousa, M., Bujalski, P., Cruz, B. F., Louie, K., McNamee, D. and Paton, J. J. (2023) Dopamine neurons encode a multidimensional\nprobabilistic map of future reward. bioRxiv, 2023–11.\nStachenfeld, K. L., Botvinick, M. M. and Gershman, S. J. (2017) The hippocampus as a predictive map. Nature neuroscience,\n20, 1643–1653.\nSteinberg, E. E., Keiﬂin, R., Boivin, J. R., Witten, I. B., Deisseroth, K. and Janak, P. H. (2013) A causal link between prediction\nerrors, dopamine neurons and learning. Nature neuroscience, 16, 966–973.\nSteinmetz, N. A., Aydin, C., Lebedeva, A., Okun, M., Pachitariu, M., Bauza, M., Beau, M., Bhagat, J., Böhm, C., Broux, M. et al.\n(2021) Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings. Science, 372, eabf4588.\nSutton, R. (1991) Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2, 160–163.\nSutton, R. S. (1988) Learning to predict by the methods of temporal diﬀerences. Machine learning, 3, 9–44.\nSutton, R. S. and Barto, A. G. (2018) Reinforcement learning: An introduction. MIT press.\nSutton, R. S., Precup, D. and Singh, S. (1999) Between MDPs and semi-MDPs: A framework for temporal abstraction in\nreinforcement learning. Artiﬁcial intelligence, 112, 181–211.\nTakahashi, Y., Schoenbaum, G. and Niv, Y. (2008) Silencing the critics: understanding the eﬀects of cocaine sensitization on\ndorsolateral and ventral striatum in the context of an actor/critic model. Frontiers in neuroscience, 2, 282.\nTano, P., Dayan, P. and Pouget, A. (2020) A local temporal diﬀerence code for distributional reinforcement learning. Advances\nin neural information processing systems, 33, 13662–13673.\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A. et al. (2023) Gemini:\na family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.\n28\nJensen\nTervo, D. G., Proskurin, M., Manakov, M., Kabra, M., Vollmer, A., Branson, K. and Karpova, A. Y. (2014) Behavioral variability\nthrough stochastic choice and its gating by anterior cingulate cortex. Cell, 159, 21–32.\nTodorov (2009) Eﬃcient computation of optimal actions. Proceedings of the national academy of sciences, 106, 11478–11483.\nTodorov, E. (2006) Linearly-solvable Markov decision problems. Advances in neural information processing systems, 19.\nTolman, E. C. (1948) Cognitive maps in rats and men. Psychological review, 55, 189.\nTsai, H.-C., Zhang, F., Adamantidis, A., Stuber, G. D., Bonci, A., De Lecea, L. and Deisseroth, K. (2009) Phasic ﬁring in dopamin-\nergic neurons is suﬃcient for behavioral conditioning. Science, 324, 1080–1084.\nVan Hasselt, H., Guez, A. and Silver, D. (2016) Deep reinforcement learning with double Q-learning. In Proceedings of the AAAI\nconference on artiﬁcial intelligence, vol. 30.\nVértes, E. and Sahani, M. (2019) A neurally plausible model learns successor representations in partially observable environ-\nments. Advances in Neural Information Processing Systems, 32.\nVikbladh, O. M., Meager, M. R., King, J., Blackmon, K., Devinsky, O., Shohamy, D., Burgess, N. and Daw, N. D. (2019) Hip-\npocampal contributions to model-based planning and spatial memory. Neuron, 102, 683–693.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev,\nP. et al. (2019) Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575, 350–354.\nViswanathan, G. M., Buldyrev, S. V., Havlin, S., da Luz, M. G., Raposo, E. P. and Stanley, H. E. (1999) Optimizing the success of\nrandom searches. nature, 401, 911–914.\nWang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., Hassabis, D. and Botvinick, M. (2018) Prefrontal\ncortex as a meta-reinforcement learning system. Nature neuroscience, 21, 860–868.\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D. and Botvinick, M. (2016)\nLearning to reinforcement learn. arXiv preprint arXiv:1611.05763.\nWatabe-Uchida, M., Eshel, N. and Uchida, N. (2017) Neural circuitry of reward prediction error. Annual review of neuroscience,\n40, 373–394.\nWatkins, C. J. and Dayan, P. (1992) Q-learning. Machine learning, 8, 279–292.\nWatkins, C. J. C. H. (1989) Learning from delayed rewards.\nWenliang, L. K., Déletang, G., Aitchison, M., Hutter, M., Ruoss, A., Gretton, A. and Rowland, M. (2023) Distributional Bellman\noperators over mean embeddings. arXiv preprint arXiv:2312.07358.\nWhittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N. and Behrens, T. E. (2020) The Tolman-Eichenbaum\nmachine: unifying space and relational memory through generalization in the hippocampal formation. Cell, 183, 1249–\n1263.\nWilliams, R. J. (1992) Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learn-\ning, 8, 229–256.\nWurman, P. R., Barrett, S., Kawamoto, K., MacGlashan, J., Subramanian, K., Walsh, T. J., Capobianco, R., Devlic, A., Eckert, F.,\nFuchs, F. et al. (2022) Outracing champion Gran Turismo drivers with deep reinforcement learning. Nature, 602, 223–228.\nYin, H. H., Knowlton, B. J. and Balleine, B. W. (2004) Lesions of dorsolateral striatum preserve outcome expectancy but disrupt\nhabit formation in instrumental learning. European journal of neuroscience, 19, 181–189.\nYin, H. H., Ostlund, S. B., Knowlton, B. J. and Balleine, B. W. (2005) The role of the dorsomedial striatum in instrumental\nconditioning. European Journal of Neuroscience, 22, 513–523.\nZintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K. and Whiteson, S. (2019) VariBAD: A very good method for\nbayes-adaptive deep RL via meta-learning. arXiv preprint arXiv:1910.08348.\nJensen\n29\n11\n|\nADDITIONAL TOPICS OF INTEREST\nWhile we have tried to provide a fairly comprehensive overview of topics in reinforcement learning of interest to\nneuroscience, there are naturally many interesting areas that we have had to omit. Here we provide a brief description\nof some of these together with pointers to relevant literature for those who are interested in exploring them further.\n11.1\n|\nHierarchical reinforcement learning\nSo far, we have considered a simple environment consisting of discrete states and actions, and all planning and decision\nmaking has taken place in the space of action primitives. However, when planning over longer horizons, it can be\nnecessary to break down the overall policy into a series of sub-goals, sub-policies, or ‘skills’ (Sutton et al., 1999;\nPateria et al., 2021). This is the topic of hierarhical reinforcement learning (HRL) and ‘options’, where an agent learns\na high-level policy over policies that can themselves be speciﬁed in terms of primitive actions or even lower-level\npolicies. Such HRL has been found to explain features of human behaviour (Eckstein and Collins, 2020; Botvinick,\n2008; Botvinick et al., 2009) and remains an area of substantial interest in the neuroscience literature.\n11.2\n|\nOﬀ-policy & oﬄine reinforcement learning\nIn most of the work considered in this paper, the experience used to train the RL agents has been sampled from the\npolicy of the agent itself. Indeed this is required for the gradients to be unbiased in the vanilla policy gradient setting.\nHowever, an area of substantial interest is that of oﬄine reinforcement learning, where the agent is trained from\nscratch on the basis of pre-collected experience (Levine et al., 2020). This is particularly important in cases where\nonline data collection is expensive or too risky but large-scale datasets exist, such as in many healthcare settings. Oﬀ-\npolicy reinforcement learning is the related problem of learning from a combination of online data and pre-generated\ndata, possibly from a ‘stale’ version of the current agent. The oﬀ-policy setting is especially relevant to biology, where\ndata collection is expensive and we therefore wish to make maximum use of existing data. This can e.g. be achieved\nthrough experience replay, which can be prioritized (instead of sampled at random) to maximize future reward and\nminimize temporal opportunity costs (Mattar and Daw, 2018; Agrawal et al., 2022; Schaul et al., 2015). A variety of ‘oﬀ-\npolicy’ policy gradient methods have also been developed to improve sample eﬃciency, which de-bias the gradients\ne.g. through the use of importance sampling (Espeholt et al., 2018; Jie and Abbeel, 2010; Peshkin and Shelton, 2002;\nHaarnoja et al., 2018).\n11.3\n|\nImitation learning\nRelated to the problem of oﬄine reinforcement learning is that of imitation learning, where we also learn from pre-\ncollected data. However, in contrast to oﬄine RL where we make no assumption about the quality of the policy used\nto collect the data, imitation learning assumes that the data has been collected by an ‘expert’ we wish to imitate (Levine\net al., 2020). This is useful in cases where a large amount of expert data is available, such as the case of autonomous\ndriving (Pan et al., 2017). Imitation learning is clearly important during early development in biological organisms,\nwhere we learn from observing the individuals around us. Indeed, such imitation learning is a hallmark not just of\nhumans but has also been demonstrated in organisms as ‘simple’ as the bumblebee (Loukola et al., 2017). Imitation\nlearning has also recently been used to learn models of biological neural circuits from high-resolution behavioural data\n(Aldarondo et al., 2024).\n11.4\n|\nLinear reinforcement learning\nAs we have seen in most of this tutorial, reinforcement learning is generally diﬃcult and requires iterative algorithms\nthat often scale poorly with the problem size. However, there are settings where we can simplify the problem to the\n30\nJensen\npoint where it becomes analytically tractable in an approach known as ‘linear reinforcement learning’ (Todorov, 2006,\n2009). This is similar to the SR approach, where we saw that the value function reduces to a linear function of the\nreward-per-state. Similar to how the SR matrix can be seen as describing the dynamics of some ‘base policy’, we also\ndeﬁne a base policy in linear RL and compute a ‘control cost’ as the KL divergence between transition dynamics with\nand without our controller:\nLctr l (s) = KL\n\u0002\nu (s′|s) ||p (s′|s)\n\u0003\n,\n(46)\nwhere p (s′|s) are the prior transition dynamics and u (s′|s) are the controlled transition dynamics marginalized over\nthe policy. For Lctr l (s) to be well-deﬁned, we require u (s′|s) = 0 whenever p (s′|s) = 0, which prevents impossible\ntransitions even under our ﬂexible controller. When subtracting this loss from the RL objective, the resulting utility\nturns out to be convex in u and can therefore be solved eﬃciently for the controller, which implicitly speciﬁes the\npolicy. This approach has recently been used as an explicit model of biological decision making (Piray and Daw, 2021,\n2024). It also has close parallels to learning and planning as inference (Levine, 2018; Solway and Botvinick, 2012;\nBotvinick and Toussaint, 2012) and to RL with information bottlenecks (Lai and Gershman, 2021). Both of these\nfamilies of approaches involve reinforcement learning with a KL-regularized reward function, and they have also been\nused as models of biological decision making.\n11.5\n|\nSuccessor features\nIn Section 6, we saw that the successor representation can be used for decision making with ﬂexible adaptation in\nenvironments with changing reward structures. However, we developed this framework only in the tabular setting de-\nspite extending TD-learning and Q-learning to the ‘deep RL’ setting with function approximation. This leaves open the\nquestion of whether a similar generalization of the SR exists. This turns out to be the case and is known as ‘successor\nfeatures’ (SF; Barreto et al., 2017), where the expected future observation of learned features of the environment are\nused in place of the expected future state occupancy. Successor features have also been shown to have a biologically\nplausible implementation that facilitates learning and generalization in noisy and partially observable environments\n(Vértes and Sahani, 2019).\n11.6\n|\nMulti-agent reinforcement learning\nWe have only considered the case of single agents interacting with a black-box environment. However, in many cases,\nmultiple agents are simultaneously interacting with each other and the environment around them (Gronauer and\nDiepold, 2022). This means that, from the point of view of a single agent, the other agents are part of its environment.\nIn such settings, there are interesting learning dynamics beyond the scope of the present tutorial, but which are\ncovered in detail by e.g. Gronauer and Diepold (2022), and which are also of substantial interest in game theory\n(Nowé et al., 2012). In some cases, a whole group of agents may be working together to maximize a single joint\nreward function – as is the case for members of a single sports team. Interestingly, the learning of many individual\nneurons in the brain from a single common reinforcing signal (such as dopamine) can be modelled as such a multi-agent\nreinforcement learning problem (Sutton and Barto, 2018). If the ‘agents’ (or neurons) are assumed to have Bernoulli-\nlogistic outputs, Williams (1992) shows that the independent learning of individual agents from the global reward\nsignal leads to the implementation of a policy gradient algorithm at the population level (Sutton and Barto, 2018).\n",
  "categories": [
    "q-bio.NC",
    "cs.LG"
  ],
  "published": "2023-11-13",
  "updated": "2024-12-18"
}