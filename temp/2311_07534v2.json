{
  "id": "http://arxiv.org/abs/2311.07534v2",
  "title": "Unsupervised Musical Object Discovery from Audio",
  "authors": [
    "Joonsu Gha",
    "Vincent Herrmann",
    "Benjamin Grewe",
    "J√ºrgen Schmidhuber",
    "Anand Gopalakrishnan"
  ],
  "abstract": "Current object-centric learning models such as the popular SlotAttention\narchitecture allow for unsupervised visual scene decomposition. Our novel\nMusicSlots method adapts SlotAttention to the audio domain, to achieve\nunsupervised music decomposition. Since concepts of opacity and occlusion in\nvision have no auditory analogues, the softmax normalization of alpha masks in\nthe decoders of visual object-centric models is not well-suited for decomposing\naudio objects. MusicSlots overcomes this problem. We introduce a\nspectrogram-based multi-object music dataset tailored to evaluate\nobject-centric learning on western tonal music. MusicSlots achieves good\nperformance on unsupervised note discovery and outperforms several established\nbaselines on supervised note property prediction tasks.",
  "text": "Unsupervised Musical Object Discovery from Audio\nJoonsu Gha1 Vincent Herrmann1 Benjamin Grewe 2\nJ√ºrgen Schmidhuber 1,3 Anand Gopalakrishnan1\n1The Swiss AI Lab, IDSIA, USI & SUPSI, Lugano, Switzerland\n2 Institute of Neuroinformatics, ETH Zurich, Z√ºrich, Switzerland\n3AI Initiative, KAUST, Thuwal, Saudi Arabia\njoonsu.gha@usi.ch\nbgrewe@ethz.ch\n{vincent.herrmann, juergen, anand}@idsia.ch\nAbstract\nCurrent object-centric learning models such as the popular SlotAttention architec-\nture allow for unsupervised visual scene decomposition. Our novel MusicSlots\nmethod adapts SlotAttention to the audio domain, to achieve unsupervised music\ndecomposition. Since concepts of opacity and occlusion in vision have no auditory\nanalogues, the softmax normalization of alpha masks in the decoders of visual\nobject-centric models is not well-suited for decomposing audio objects. MusicSlots\novercomes this problem. We introduce a spectrogram-based multi-object music\ndataset tailored to evaluate object-centric learning on western tonal music. Music-\nSlots achieves good performance on unsupervised note discovery and outperforms\nseveral established baselines on supervised note property prediction tasks.1\n1\nIntroduction\nHuman infants learn to group the feature cues of incoming visual stimuli into a set of meaningful\nentities [1]. This capacity to integrate feature cues into a ‚Äúunified whole‚Äù [2, 3] extends beyond visual\nperception to the auditory domain [4‚Äì6]. The notion of ‚Äòobject files‚Äô [7] that capture this visual feature\nintegration has been posited to extend to the auditory domain [8, 9] as well. Recently, there has been\ngrowing interest in developing unsupervised deep learning models for perceptual grouping ( ‚Äúobject-\ncentric learning‚Äù) in the visual domain [10‚Äì23]. Such object-centric models bias the underlying\nstructure of machine perception to be human-like by modeling the scene as a composition of objects.\nHowever, the object-centric learning literature has primarily focused on perceptual grouping task for\nvision (images/video) and extensions to the auditory domain remain largely unexplored. Therefore,\nwe focus on extending object-centric models for the auditory, specifically musical domain. To the\nbest of our knowledge, no prior work has applied unsupervised object-centric models to the problem\nof unsupervised music decomposition.\nWestern tonal music serves as a suitable form of auditory signal for our study as its building blocks are\nsymbolic units such as notes, chords or phrases, which themselves are organized into more complex\nstructures using rich grammars [24]. We investigate if object-centric models (SlotAttention [19]),\nhighly successful in visual grouping, are able to segregate constituent units of a musical score given\nits spectrogram in a fully unsupervised manner. The auditory modality poses unique challenges as\nthe underlying structure of auditory objects differs from their visual counterparts. For instance, the\nconcepts of occlusion and opacity in the visual domain have no auditory analogues. If two auditory\nobjects occupy some overlapping spectral regions, their composition would approximately result in\nthe additive combination of their power spectra in these regions. In contrast, on the visual domain\n1Official code repository: https://github.com/arahosu/MusicSlots\nMachine Learning for Audio Workshop, NeurIPS 2023.\narXiv:2311.07534v2  [cs.SD]  14 Nov 2023\n+\n=\n+\n=\nFigure 1: Illustration of the effects of opacity and occlusion in visual and auditory (spectral) domains.\nwhere two opaque objects cannot occupy the same spatial location and one will necessarily occlude\nthe other e.g. the red ball in front of green cube in Figure 1. Therefore, in the visual case every pixel\nis naturally assumed to belong to only one object while for audio this assumption does not hold true.\nWe propose MusicSlots, an autoencoder model to decompose a chord spectrogram into its constituent\nnote spectrograms (objects) in a fully unsupervised manner. We show that Softmax normalization\n(across slots) of alpha masks in the SlotAttention decoder [19] is not well-suited for discovering\nmusical objects as it assumes that the feature at any spatial location belongs to only one object\n(slot) which is invalid for the audio domain. Further, we introduce a multi-object music dataset\ntailored to evaluate object-centric learning methods for music analogous to its visual counterpart\n[25]. Our dataset consists of chord spectrograms (taken from Bach-Chorales [26] and Jazznet [27]),\nconstituent note spectrograms and corresponding ground-truth binary masks. Finally, we show that\nour MusicSlots model achieves good performance on unsupervised note discovery, outperforms\nseveral baseline models (VAE, Œ≤-VAE, AutoEncoder, supervised CNN) on supervised note property\nprediction task and shows generalizes better to unseen note combinations and number of notes.\n2\nMethod\nGiven a mel-scale spectrogram representation x ‚ààRDin√óh√ów of a musical chord, our goal is to\ndecompose it into its constituent note-level spectrograms xk ‚ààRDin√óh√ów\n‚àÄk = {1, 2, ..., K} and\nlearn their associated representations (slots) s ‚ààRDs√óK. Here, Din denotes the number of channels\nin the spectrogram, Ds the slot size, h, w the number of frequency bins and time window of the\ninput spectrogram respectively and K the total number of slots. Our proposed MusicSlots model\nis an autoencoder which consists of three modules (see Figure 2). An encoder module (CNN) to\nextract features from the input spectrogram, slot attention module to group input features to slots and\ndecoder module to reconstruct the chord spectrogram from the slot representations.\nEncoder.\nThe encoder module consists of a CNN backbone to extract features h ‚ààRDf √óh√ów from\nthe input chord spectrogram x. Learnable positional embeddings p ‚ààRDf √óh√ów (see Appendix A.2\nfor more details) are added to the output features h from the final convolutional layer.\nSlot Attention.\nThe slot attention module learns to map a set of N = h ¬∑ w input features onto\na set of K slots using an iterative attention mechanism (Algorithm 1 from Locatello et al. [19])\nimplemented via key-value attention [28] and recurrent update function. The input features h are\nprojected to a set of keys k and values v ‚ààRDs√óN using separate linear layers. Each slot sk is\ninitialized as independent samples from a Gaussian distribution N(¬µk, œÉk) with separate learnable\nmean ¬µk ‚ààRDs and standard-deviation œÉk ‚ààRDs. Then at each iteration t = {1, ..., T}, slots\ncompete to represent elements of the set of features using standard key-value attention (with features\nas keys & values and slots as queries) except with softmax normalization applied across the slots.\nWe use a recurrent network (specifically a GRU [29, 30]) and residual MLP [19]) to update the slots\nwith weighted values as inputs and slots at t ‚àí1 as hidden states of the RNN. Further, our MusicSlots\nmodel also adopts recent improvements to SlotAttention such as implicit differentiation [31].\n2\nSlot \nDecod\ner\nEncoder\n√óùëá\nSlot Attention\nBroadcast \nDecoder\ndB to \npower\npower \nto dB\nŒ£\n√óùëá\nùëò\nInput Chord Spectrogram\nPredicted Note Spectrogram\nPredicted Chord Spectrogram\n√ó\nùëö$!\nùëùÃÇ!\nùë•(\"\nFigure 2: MusicSlots model consists of 3 modules ‚Äî Encoder, SlotAttention and Broadcast Decoder.\nDecoder.\nEach slot sk is decoded independently by the spatial broadcast decoder [32] (see Figure 2)\nusing several de-convolutional layers. First, slots are broadcasted onto a 2D grid (independently)\nand learnable positional embeddings are added. The decoder outputs the reconstructed note-level\nspectrogram ÀÜxk ‚ààRDin√óh√ów and un-normalized (logits) alpha mask mk ‚ààR1√óh√ów. The individual\nslot-wise spectrograms and normalized masks mk = fnorm(mk) ‚ààR1√óh√ów are alpha composited to\ngive the predicted power-scale chord spectrogram ÀÜxp = PK\nk=1 ÀÜpk ‚äômk where ÀÜpk is the power-scale\nnote spectrogram, ‚äôis an element-wise multiplication and fnorm is the normalization function. This\ncomposition operation needs to carried out in power-scale followed by conversion back to decibel\nscale to get the predicted chord spectrogram ÀÜx as follows:\nÀÜpk = 10ÀÜxk/10\n;\nÀÜxp =\nK\nX\nk=1\nÀÜpk ‚äômk\n;\nÀÜx = 10 log10\n\u0010 ÀÜxp\nÀÜxp0\n\u0011\ndB\n(1)\nwhere ÀÜpk is the power-scale note spectrogram, xp is the chord spectrogram in power scale and ÀÜxp0\nis the reference power. Further, a crucial modification required to adapt SlotAttention to the audio\ndomain, is the choice of this normalization function fnorm for alpha masks. For auditory objects, as\nillustrated in Figure 1 notions of occlusion and opacity are invalid which means that its feasible for\none or more notes (slots) to contribute to the power at a spatial location (frequency bin and time) in\nthe spectrogram. Contrarily, in the visual domain its necessarily the case that every pixel belongs\nto only one object. Therefore, we experiment with alternatives such as Sigmoid and not using any\nalpha masks in the broadcast decoder. We train our MusicSlots model using the MSE between the\npredicted and input chord spectrogram L = ||x ‚àíÀÜx||2\n2. For more details on model architecture and\ntraining hyperparameters please refer to Appendix A.2 and Appendix A.3 respectively.\n3\nRelated Work\nLearning music representations from audio has been explored using self-supervised techniques\nsuch as autoencoders [33‚Äì35] and contrastive methods [36‚Äì39] or weak supervision from different\nmodalities [40‚Äì43]. Related to our work, ‚ÄòAudioslots‚Äô [44] applies object-centric models to the audio\ndomain for blind source separation. However, their model is strongly supervised as it is trained using\nMSE loss between predicted and matched ground-truth individual source spectrograms. Others have\napplied slot-based object-centric models beyond vision to learn modular action sequences for RL\nagents [45] or robotic control policies [46].\n4\nResults\nWe describe details of our multi-object music dataset followed by results on unsupervised note\ndiscovery and supervised note property prediction tasks.\nMulti-Object Music Datasets.\nTo evaluate the efficacy of our proposed MusicSlots model on the\nunsupervised music decomposition task, we need a dataset of musical scores in the spectral format\nand its decomposition into sub-parts i.e. part-level spectrograms and binary masks (see Figure 4). We\nintroduce synthetic multi-object datasets to specifically evaluate object-centric learning methods on\nthe music domain analogous to its visual counterpart [25]. First, we extract chords (MIDI tokens)\nfrom Bach-Chorales (JSB) [26] and JazzNet [27] datasets. Next, we synthesize the audio waveform\n3\nand its spectrogram for these chords (see Appendix A.1 for more details). Our dataset consists of\ntwo variants ‚Äî i) single-instrument: all notes in a chord played by the same instrument, ii) multi-\ninstrument: different notes in a chord played by different instruments. We create out-of-distribution\ntest splits that measure generalization to unseen note combinations and number of notes in a chord.\nThe test split in Bach Chorales contains chords with known notes in unknown combinations (w.r.t\ntrain/validation splits) whereas in JazzNet it contains only chords with four notes, while training and\nvalidation splits consist of two and three-note chords (see Appendix A.1 for more details).\nNote Discovery.\nWe train three variants of our MusicSlots model with different choices for\nfnorm ‚Äî i) Softmax (MusicSlots-soft) ii) Sigmoid (MusicSlots-sigm) iii) no alpha mask usage\n(MusicSlots-none).\nWe refer to Table 11 and Appendix A.2 for model/training details.\nWe\nquantitatively measure note (object) discovery performance of our models using the best matched\nnote-level MSE of spectrograms and mean IoU scores of binary masks. Table 1 shows the note\ndiscovery results of MusicSlots on multi-instrument versions of JSB and JazzNet datasets. We see\nthat the MusicSlots without any alpha masking is competitive with Sigmoid normalization and these\nalternatives show significant performance gains over the default Softmax. Further, we observe that\ntraining on multi-instrument chord datasets is beneficial for better decomposition quality (compare\nwith single-instrument in Appendix B). We show samples of good decomposition and some failure\ncases of our MusicSlots model in Appendix C.\nTable 1: Note discovery results on multi-instrument BachChorales (JSB) and JazzNet datasets for\nMusicSlots models. Mean and std-dev. are reported across 5 seeds.\nDatasets\nMask Norm.\nNote MSE ‚Üì\nmIoU ‚Üë\nJSB-multi\nMusicSlots-soft\n59.34 ¬±22.01\n0.79 ¬±0.04\nMusicSlots-sigm\n13.07 ¬±0.80\n0.90 ¬±0.01\nMusicSlots-none\n13.47 ¬±0.90\n0.91 ¬±0.01\nJazzNet-multi\nMusicSlots-soft\n33.58 ¬±2.08\n0.84 ¬±0.01\nMusicSlots-sigm\n18.53 ¬±0.83\n0.91 ¬±0.01\nMusicSlots-none\n19.95 ¬±1.89\n0.90 ¬±0.01\nNote Property Prediction.\nWe train a linear classifier with cross-entropy loss (see Appendix A.2\nfor details) to predict the properties (MIDI pitch value and instrument type) of all notes in a chord\nfrom the frozen (pre-trained) latent representations. We use the classification accuracy as the\nevaluation metric for this task wherein a chord is considered to be correctly classified if and only if all\nits note pitch values and instrument identities are correctly predicted. The supervised CNN baseline\nuses the same encoder module as MusicSlots followed by a two layer MLP and trained in a supervised\nmanner to predict the note properties given the chord spectrogram. Table 2 shows the note property\nresults for our MusicSlots model against various baseline models. We see that our MusicSlots model\noutperforms several unsupervised baseline models (AutoEncoder/VAE/Œ≤-VAE). Surprisingly it also\noutperforms the supervised CNN baseline which is explicitly trained end-to-end to solve the task.\nFurther, MusicSlots shows a greater degree of generalization to unseen note combinations on the\ntest splits of JSB-multi and different number of notes in a chord on JazzNet-multi respectively.\nTable 2: Note property prediction performance of MusicSlots compared to Autoencoder, VAE, Œ≤-VAE\nand supervised CNN baseline models. Mean and std-dev. are reported across 5 seeds.\nModels\nJSB-multi\nJazzNet-multi\nVal-Acc.\nTest-Acc.\nVal-Acc.\nTest-Acc.\nSupervised CNN\n93.49 ¬±2.10\n93.03 ¬±2.10\n96.47 ¬±0.79\n71.22 ¬±2.93\nAutoEncoder\n95.02 ¬±0.09\n93.62 ¬±0.39\n92.91 ¬±0.26\n71.37 ¬±1.07\nVAE\n96.66 ¬±0.34\n96.21 ¬±0.35\n94.37 ¬±0.55\n71.55 ¬±4.77\nŒ≤-VAE\n97.85 ¬±0.13\n97.42 ¬±0.06\n97.00 ¬±0.37\n81.53 ¬±0.77\nMusicSlots-none\n98.13 ¬±0.16\n97.77 ¬±0.12\n98.96 ¬±0.39\n87.65 ¬±1.88\n4\n5\nConclusion\nOur MusicSlots model is the first method to extend object-centric learning to the domain of music. To\nevaluate such models, we introduced novel multi-object music datasets based on Western tonal music.\nMusicSlots successfully decomposes chord spectrograms into their constituent note spectrograms,\nand outperforms several well-established unsupervised and supervised baselines on downstream note\nproperty prediction tasks. Representations learned by MusicSlots are potentially useful for practical\napplications, such as music transcription/generation and building more human-like perceptual models\nof audio and music.\nAcknowledgments.\nWe thank Hamza Keurti and Yassine Taoudi Benchekroun for insightful\ndiscussions. This research was funded by Swiss National Science Foundation grant: 200021_192356,\nproject NEUSYM and the ERC Advanced grant no: 742870, AlgoRNN. We also thank NVIDIA\nCorporation for donating DGX machines as part of the Pioneers of AI Research Award.\nReferences\n[1] Elizabeth S. Spelke. Principles of object perception. Cognitive Science, 14(1):29‚Äì56, 1990.\n[2] Kurt Koffka. Principles of gestalt psychology. Philosophy and Scientific Method, 32(8), 1935.\n[3] Wolfgang K√∂hler. Gestalt psychology. Psychologische Forschung, 31(1), 1967.\n[4] Michael Kubovy and David Van Valkenburg. Auditory and visual objects. Cognition, 80(1-2):\n97‚Äì126, 2001.\n[5] Timothy D. Griffiths and Jason D. Warren. What is an auditory object?\nNature Reviews\nNeuroscience, 5:887‚Äì892, 2004.\n[6] Jennifer K. Bizley and Yale E. Cohen. The what, where and how of auditory-object perception.\nNature Reviews Neuroscience, 14:693‚Äì707, 2013.\n[7] Daniel Kahneman, Anne Treisman, and Brian J Gibbs. The reviewing of object files: Object-\nspecific integration of information. Cognitive psychology, 24(2):175‚Äì219, 1992.\n[8] Michael D Hall, Richard E Pastore, Barbara E Acker, and Wenyi Huang. Evidence for auditory\nfeature integration with spatially distributed items. Perception & Psychophysics, 62(6):1243‚Äì\n1257, 2000.\n[9] Sharon Zmigrod and Bernhard Hommel. Auditory event files: Integrating auditory perception\nand action planning. Attention, Perception, & Psychophysics, 71:352‚Äì362, 2009.\n[10] SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E\nHinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In Proc.\nAdvances in Neural Information Processing Systems (NIPS), pages 3225‚Äì3233, 2016.\n[11] Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, and J√ºrgen Schmidhuber.\nTagger: Deep unsupervised perceptual grouping. In Proc. Advances in Neural Information\nProcessing Systems (NIPS), volume 29, 2016.\n[12] Klaus Greff, Sjoerd van Steenkiste, and J√ºrgen Schmidhuber. Neural expectation maximization.\nIn Proc. Advances in Neural Information Processing Systems (NIPS), pages 6691‚Äì6701, 2017.\n[13] Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and J√ºrgen Schmidhuber. Relational neural\nexpectation maximization: Unsupervised discovery of objects and their interactions. In Int.\nConf. on Learning Representations (ICLR), 2018.\n[14] Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer,\nrepeat: Generative modelling of moving objects. In Proc. Advances in Neural Information\nProcessing Systems (NIPS), pages 8606‚Äì8616, 2018.\n[15] Aleksandar Stani¬¥c and J√ºrgen Schmidhuber. R-sqair: Relational sequential attend, infer, repeat.\nIn Neurips Workshop on Perception as Generative Reasoning: Structure, Causality, Probability,\n2019.\n5\n[16] Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convo-\nlutional neural networks. In Proc. AAAI Conf. on Artificial Intelligence, 2019.\n[17] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt\nBotvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representa-\ntion. arXiv preprint arXiv:1901.11390, 2019.\n[18] Klaus Greff, Rapha√´l Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess,\nDaniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object repre-\nsentation learning with iterative variational inference. In Proc. Int. Conf. on Machine Learning\n(ICML), pages 2424‚Äì2433, 2019.\n[19] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg\nHeigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with\nslot attention. In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n[20] Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Genera-\ntive scene inference and sampling with object-centric latent representations. In Int. Conf. on\nLearning Representations (ICLR), 2020.\n[21] Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong\nJiang, and Sungjin Ahn. SPACE: unsupervised object-oriented scene representation via spatial\nattention and decomposition. In Int. Conf. on Learning Representations (ICLR), 2020.\n[22] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALLE learns to compose. In Int. Conf.\non Learning Representations (ICLR), 2022.\n[23] Thomas Kipf, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour,\nGeorg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-\ncentric learning from video. In Int. Conf. on Learning Representations (ICLR), 2022.\n[24] Fred Lerdahl and Ray S Jackendoff. A Generative Theory of Tonal Music. MIT press, 1983.\n[25] Rishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm\nReynolds, and Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multi-\nobject-datasets/, 2019.\n[26] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal\ndependencies in high-dimensional sequences: Application to polyphonic music generation and\ntranscription. In Proc. Int. Conf. on Machine Learning (ICML), page 1881‚Äì1888, 2012.\n[27] Tosiron Adegbija. jazznet: A dataset of fundamental piano patterns for music audio machine\nlearning research. In IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2023.\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural\nInformation Processing Systems (NIPS), pages 5998‚Äì6008, 2017.\n[29] Kyunghyun Cho, Bart van Merrienboer, √áaglar G√ºl√ßehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio.\nLearning phrase representations using rnn en-\ncoder‚Äìdecoder for statistical machine translation. In Proc. Conf. on Empirical Methods in\nNatural Language Processing (EMNLP), 2014.\n[30] Felix A. Gers, J√ºrgen Schmidhuber, and Fred Cummins.\nLearning to Forget: Continual\nPrediction with LSTM. Neural Computation, 12(10):2451‚Äì2471, 2000.\n[31] Michael Chang, Tom Griffiths, and Sergey Levine. Object representations as fixed points:\nTraining iterative refinement algorithms with implicit differentiation. In Proc. Advances in\nNeural Information Processing Systems (NeurIPS), volume 35, pages 32694‚Äì32708, 2022.\n[32] Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial\nbroadcast decoder: A simple architecture for learning disentangled representations in VAEs. In\nLearning from Limited Labeled Data (LLD) Workshop, ICLR, 2019.\n6\n[33] Antoine Caillon and Philippe Esling. RAVE: A variational autoencoder for fast and high-quality\nneural audio synthesis, 2022.\n[34] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\nSoundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:495‚Äì507, 2021.\n[35] Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Chenghua Lin, Xingran Chen, Anton Ragni,\nHanzhi Yin, Zhijie Hu, Haoyu He, Emmanouil Benetos, Norbert Gyenge, Ruibo Liu, and Jie Fu.\nMap-music2vec: A simple and effective baseline for self-supervised music audio representation\nlearning. In Proc. International Society for Music Information Retrieval, 2022.\n[36] Janne Spijkervet and John Ashley Burgoyne. Contrastive learning of musical representations.\nIn Proc. International Society for Music Information Retrieval, 2021.\n[37] Matthew C. McCallum, Filip Korzeniowski, Sergio Oramas, Fabien Gouyon, and Andreas F.\nEhmann. Supervised and unsupervised learning of audio representations for music understand-\ning. In Proc. International Society for Music Information Retrieval, 2022.\n[38] J. Choi, S. Jang, H. Cho, and S. Chung. Towards proper contrastive self-supervised learning\nstrategies for music audio representation. In IEEE International Conference on Multimedia and\nExpo (ICME), pages 1‚Äì6, 2022.\n[39] Luyu Wang, Pauline Luc, Yan Wu, Adria Recasens, Lucas Smaira, Andrew Brock, Andrew\nJaegle, Jean-Baptiste Alayrac, Sander Dieleman, Joao Carreira, and Aaron van den Oord. To-\nwards learning universal audio representations. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 4593‚Äì4597, 2022.\n[40] Jason Weston, Samy Bengio, and Philippe Hamel. Multi-tasking with joint semantic spaces\nfor large-scale music annotation and retrieval. Journal of New Music Research, 40(4):337‚Äì348,\n2011.\n[41] Jiyoung Park, Jongpil Lee, Jangyeon Park, Jung-Woo Ha, and Juhan Nam. Representation\nlearning of music using artist labels. In Proc. International Society for Music Information\nRetrieval, 2017.\n[42] Ilaria Manco, Emmanouil Benetos, Elio Quinton, and Gy√∂rgy Fazekas. Learning music audio\nrepresentations via weak language supervision. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 456‚Äì460, 2022.\n[43] Tianyu Chen, Yuan Xie, Shuai Zhang, Shaohan Huang, Haoyi Zhou, and Jianxin Li. Learning\nmusic sequence representation from text supervision. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 4583‚Äì4587, 2022.\n[44] Pradyumna Reddy, Scott Wisdom, Klaus Greff, John R. Hershey, and Thomas Kipf. Audioslots:\nA slot-centric generative model for audio separation. 2023 IEEE International Conference on\nAcoustics, Speech, and Signal Processing Workshops (ICASSPW), pages 1‚Äì5, 2023.\n[45] Anand Gopalakrishnan, Kazuki Irie, J√ºrgen Schmidhuber, and Sjoerd van Steenkiste. Unsuper-\nvised Learning of Temporal Abstractions With Slot-Based Transformers. Neural Computation,\n35(4):593‚Äì626, 2023.\n[46] Yifan Zhou, Shubham Sonawani, Mariano Phielipp, Simon Stepputtis, and Heni Amor. Mod-\nularity through attention: Efficient training and transfer of language-conditioned policies for\nrobot manipulation. In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, Proceedings of The\n6th Conference on Robot Learning, volume 205, pages 1684‚Äì1695, 2023.\n[47] Harold W. Kuhn. The Hungarian Method for the Assignment Problem. Naval Research Logistics\nQuarterly, 2(1‚Äì2):pages 83‚Äì97, 1955.\n7\nA\nExperimental Details\nIn this section, we report details on our multi-object music datasets, model implementation, and\nexperimental setting.\nA.1\nMulti-Object Music Dataset\nBach Chorales.\nThe original dataset consists of training, validation and test splits with 229, 76\nand 77 chorales respectively. Each chorale is represented as a sequence of four MIDI values for\nthe Bass, Tenor, Alto and Soprano (BTAS) voices. If a voice is silent at a given time step, the\npitch value is 0. Since we are interested in extracting unique chords from the chorales, we first\nconcatenate all MIDI sequences of the 376 chorales together across time, and exclude all the columns\ncorresponding to duplicate chords or single note examples, giving us 3131 unique chords in total.\nWe then randomly shuffle the dataset and partition it into training, validation and test splits, using a\ntrain-validation-test split ratio of 70/20/10. The MIDI pitch values for the chorales are available at\nhttps://github.com/czhuang/JSB-Chorales-dataset.\nJazzNet.\nThe JazzNet dataset (https://github.com/tosiron/jazznet) contains 5525 anno-\ntated chords (including the inversions). Of the 5525 chords, we use 2227 unique chords with the\nMIDI pitch values of their notes ranging from 36 (C2) to 96 (C7). Similar to the chords in the Bach\nChorales, the pitch values of the JazzNet chords are represented as an array of 4 MIDI values, with 0\ndenoting silence. The train-validation-test splits are defined such that the training and validation sets\ncontain only dyads (2-notes) and triads (3-notes), while the test set contains only tetrads (4-notes).\nIn the the following paragraphs we describe the details of the pipeline to generate our multi-object mu-\nsic datasets starting with the MIDI tokens of chords and finally getting chord/note-level spectrograms\nand binary masks.\nMIDI Pitch Values\n[60, 64, 67]\nC-maj chord\nPitch = 67\nInstrument = Flute\nDurations = 1.0\nVolume = 0.71\nPitch = 64\nInstrument = Violin\nDurations = 1.0\nVolume = 0.71\nPitch = 60\nInstrument = Piano\nDurations = 1.0\nVolume = 0.71\nMIDI data\nAudio Waveforms\nSpectrograms\nFigure 3: Multi-instrument dataset generation pipeline. Every note in the chord can be synthesized\ninto a raw audio waveform using a different instrument (e.g. Yamaha grand piano, violin, flute).\nMIDI File Generation.\nTo allow fine-grained control over the note properties, we generate the\nMIDI files for the chords ourselves using the Music21 library. Our data sources define the chords and\ntheir pitch values, and we specify the rest of their note attributes (i.e. volume, instrument, duration),\nas shown in Figure 3. In the Music21 library, the volume of the note is a scalar value ranging from 0.0\nto 1.0 while the duration of the note is measured in seconds. We keep the duration and volume fixed\nacross datasets by setting them to 0.71 and 1.0 second respectively. To generate examples for the multi-\ninstrument version of our dataset, we include the option to change the instrument that plays each note\nin a chord. The list of instruments is defined by a sf2 file. The sf2 file that is used in our dataset can\nbe downloaded here: https://member.keymusician.com/Member/FluidR3_GM/index.html.\nAudio Waveform Generation.\nThe generated MIDI files are then synthesized into audio waveforms\nusing Fluidsynth (https://www.fluidsynth.org/). The default PCM quantization settings used\n8\nin the Fluidsynth library are bit-depth of 16 and sample rate of 44.1 kHz. We further downsample the\nwaveforms to 16kHz. We zero-pad the waveforms at the start with a padding size of 4000, which\ncorresponds to about 0.1s of silence.\nWaveform to Spectrogram Conversion.\nWe obtain the spectrograms for the chords and their\nnotes by converting their audio waveforms into mel-spectrograms using the TorchAudio library\n(https://pytorch.org/audio/stable/index.html). We set the number of mel-filter banks\nto 128 and use the FFT and window sizes of 1024 and hop length of 512. The resulting 128 √ó 35\nspectrogram is resized by cropping along the width boundaries [0, 32], giving us a resolution of\n128 √ó 32. To generate a chord spectrogram, we combine the waveforms of its constituent notes, and\nconvert the summed waveform to a mel-spectrogram using the same mel-spectrogram parameters.\nMask Generation.\nTo generate the binary masks from the mel-spectrograms, we use a fixed decibel\nthreshold value of -30 dB for both the ground-truth and the predicted note spectrograms. Examples\nof the binary masks for different note spectrograms are shown in Figure 4.\nFigure 4: Visualization of the note spectrograms (top row) and the corresponding decibel-thresholded\nbinary masks (bottom row)\nDataset Statistics.\nIn Tables 3 and 4, we report the numbers of dyads, triads and tetrads in the\ndatasets. The dataset splits, number of unique pitch values (represented as MIDI note numbers) and\ninstruments are summarized in Table 5.\nTable 3: Chord statistics for Bach Chorales.\nSplits\nDyads\nTriads\nTetrads\nTotal\nTrain\n10\n270\n1910\n2190\nValidation\n1\n85\n540\n626\nTest\n1\n43\n271\n315\nTotal\n12\n398\n2721\n3131\n9\nTable 4: Chord statistics for JazzNet\nSplits\nDyads\nTriads\nTetrads\nTotal\nTrain\n530\n544\n0\n1074\nValidation\n124\n145\n0\n269\nTest\n0\n0\n884\n884\nTotal\n654\n689\n884\n2227\nTable 5: Number of examples in the dataset splits, number of unique pitch values and name of the\ninstruments used.\nDataset Name\nTrain\nValidation\nTest\nPitch Values\nInstrument(s)\nJSB-single\n2190\n626\n315\n53\nPiano\nJSB-multi\n19719\n5634\n2826\n53\nPiano, Violin, Flute\nJazznet-single\n1074\n269\n884\n62\nPiano\nJazznet-multi\n19458\n5031\n71604\n62\nPiano, Violin, Flute\nA.2\nModel Architecture Details\nHere we describe the architectural details of all models used in this work.\nConvolutional Encoder.\nTable 6 describes the model architecture for the CNN encoder of the\nMusicSlots. We use a CNN encoder similar to the one found in [19] for the unsupervised object\ndiscovery task. All convolution layers use a kernel size of 5 √ó 5 with a channel size of 128. Unlike\n[19], we set the stride for the horizontal axis to 2. We find that this improves performance for the\nunsupervised note discovery task (see Table 15 for details).\nTable 6: CNN Encoder in MusicSlots.\nLayer\nFeature Dimension\nH √ó W √ó C\nActivation\nStride\nPadding\nInput / Output\nInput\n128 √ó 32 √ó 1\n-\n-\n-\nConv 5 √ó 5\n128 √ó 16 √ó 128\nReLU\n(1, 2)\n(2, 2) / -\nConv 5 √ó 5\n128 √ó 8 √ó 128\nReLU\n(1, 2)\n(2, 2) / -\nConv 5 √ó 5\n128 √ó 4 √ó 128\nReLU\n(1, 2)\n(2, 2) / -\nConv 5 √ó 5\n128 √ó 2 √ó 128\nReLU\n(1, 2)\n(2, 2) / -\nPosition Embedding\n128 √ó 2 √ó 128\n-\n-\n-\nFlatten\n1 √ó 256 √ó 128\n-\n-\n-\nLayer Norm\n1 √ó 256 √ó 128\n-\n-\n-\nLinear\n1 √ó 256 √ó 128\nReLU\n-\n-\nLinear\n1 √ó 256 √ó 128\n-\n-\n-\nPositional Embedding.\nWe use the same positional embeddings as [19]. The positional embedding\nis a W √ó H √ó 4 tensor, where W and H are width and height of the CNN feature maps respectively.\nThe positional information is defined by a linear gradient [0, 1] in each of the four cardinal directions.\nEssentially, every point on the grid is a four-dimensional vector that indicates its relative distance to\nthe four edges of the feature map. We define a learnable linear projection that projects the feature\nvectors to match the dimensionality of the CNN feature vectors. We finally add the linearly projected\nresult to the input CNN feature maps.\nSlot Attention Module.\nFor all experiments, we use the same number of slots K = 7 and slot\nattention iterations T = 3. We set D and Ds to be 128 for the dimensions of the linear projections\nand the slots respectively. The hidden state of the GRU cell has a dimension of 128. The residual\nMLP has a single hidden layer of size 128 with ReLU activation, followed by a linear layer.\n10\nDe-convolutional Decoder.\nWe follow the same spatial broadcast deconvolutional decoder ([32])\nused in [19], except we set the number of channels in the transposed convolution layers to 128. The\noverall architecture for the MusicSlots decoder is detailed in Table 7.\nTable 7: Deconvolution-based slot decoder in MusicSlots.\nLayer\nFeature Dimensions\nK √ó H √ó W √ó C\nActivation\nStride\nPadding\nInput / Output\nInput\n7 √ó 1 √ó 1 √ó 128\n-\n-\n-\nSpatial Broadcast\n7 √ó 8 √ó 2 √ó 128\n-\n-\n-\nPosition Embedding\n7 √ó 8 √ó 2 √ó 128\n-\n-\n-\nConvTranspose 5 √ó 5\n7 √ó 16 √ó 4 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n7 √ó 32 √ó 8 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n7 √ó 64 √ó 16 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n7 √ó 128 √ó 32 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n7 √ó 128 √ó 32 √ó 128\nReLU\n(1, 1)\n(2, 2) / -\nConvTranspose 3 √ó 3\n7 √ó 128 √ó 32 √ó 1\n-\n(1, 1)\n(1, 1) / -\nBaseline AutoEncoders.\nThe architectural details for the encoder and decoder of the baseline\nAutoEncoders (AutoEncoder, VAE, Œ≤-VAE) are presented in Tables 8 and 9. We set the latent space\ndimension for the baseline AutoEncoders to 128.\nTable 8: Convolutional encoder for the baseline AutoEncoders, excluding the final two Linear layers\nthat parameterize the ¬µ and œÉ of the approximate posterior for the VAE.\nLayer\nFeature Dimension\nH √ó W √ó C\nActivation\nStride\nPadding\nInput / Output\nInput\n128 √ó 32 √ó 1\n-\n-\n-\nConv 5 √ó 5\n64 √ó 16 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nConv 5 √ó 5\n32 √ó 8 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nConv 5 √ó 5\n16 √ó 4 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nConv 5 √ó 5\n8 √ó 2 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nFlatten\n1 √ó 1 √ó 2048\n-\n-\n-\nTable 9: De-convolutional decoder for the AutoEncoder baselines.\nLayer\nFeature Dimensions\nH √ó W √ó C\nActivation\nStride\nPadding\nInput / Output\nInput\n1 √ó 1 √ó 128\n-\n-\n-\nLinear\n1 √ó 1 √ó 2048\n-\n-\n-\nReshape\n8 √ó 2 √ó 128\n-\n-\n-\nConvTranspose 5 √ó 5\n16 √ó 4 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n32 √ó 8 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n64 √ó 16 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n128 √ó 32 √ó 128\nReLU\n(2, 2)\n(2, 2) / (1, 1)\nConvTranspose 5 √ó 5\n128 √ó 32 √ó 128\nReLU\n(1, 1)\n(2, 2) / -\nConvTranspose 3 √ó 3\n128 √ó 32 √ó 1\n-\n(1, 1)\n(1, 1) / -\nLinear Classifier for MusicSlots.\nA linear classifier is trained on every slot that is matched with\na note to independently predict its pitch value and instrument identity. The linear classifier outputs\ntwo vectors ÀÜyinst ‚ààNinst and ÀÜypitch ‚ààNpitch, where Npitch is the number of unique pitch values and\nNinst is the number of instruments in the dataset. Both ÀÜyinst and ÀÜypitch are normalized using a softmax\nactivation, since pitch values and instrument identities are encoded as one-hot vectors.\n11\nLinear Classifier for Baseline AutoEncoders.\nIn the baseline AutoEncoders, the representations\nof individual notes are not readily available. Therefore, the input to the linear classifier is a single\nlatent vector. The classifier outputs a prediction ÀÜy ‚ààNinst √ó Npitch for the properties of all the notes\nin a chord at once. In this case, ÀÜy uses sigmoid activation, since the label y is encoded as a multi-hot\nvector.\nSupervised CNN.\nThe model architecture for the supervised baseline CNN is depicted in Table\n10. It follows the same encoder backbone as the one used in MusicSlots. The output of the encoder\nmodule is followed by a 2-layer MLP with an output size Ninst √ó Npitch.\nTable 10: Supervised CNN for the property prediction task.\nLayer\nFeature Dimension\nH √ó W √ó C\nActivation\nStride\nPadding\nInput / Output\nInput\n128 √ó 32 √ó 1\n-\n-\n-\nConv 5 √ó 5\n64 √ó 16 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nConv 5 √ó 5\n32 √ó 8 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nConv 5 √ó 5\n16 √ó 4 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nConv 5 √ó 5\n8 √ó 2 √ó 128\nReLU\n(2, 2)\n(2, 2) / -\nFlatten\n1 √ó 1 √ó 2048\n-\n-\n-\nLinear\n1 √ó 1 √ó 128\nReLU\n-\n-\nLinear\n1 √ó output size\nSigmoid\n-\n-\nA.3\nTraining Details\nIn this section, we provide an overview of the training details for MusicSlots and its baselines,\nincluding their hyperparameter choices and training objectives. The hyperparameters for training\nMusicSlots on the unsupervised note discovery task are shown in Table 11. All downstream\nclassifiers, including the supervised CNN model, share the common hyperparameters for the note\nproperty prediction task, as shown in Table 12. The training hyperparameters for the unsupervised\nbaselines (i.e. AutoEncoder, VAE, Œ≤-VAE) are displayed in Table 13.\nBaseline AutoEncoders.\nThe baseline AutoEncoder follows the same training objective as Mu-\nsicSlots: they are both trained to minimize the Mean Square Error (MSE) between the predicted\nand input chord spectrogram L = ||x ‚àíÀÜx||2\n2. The baseline VAE and Œ≤-VAE models are trained by\nmaximizing the evidence lower bound (ELBO), where the weight of the KL-divergence term Œ≤ is set\nto 1 for the baseline VAE. For more details on the effect of different choices for Œ≤ on the downstream\nnote property prediction task, please refer to Table 16 in Appendix B.\nDownstream Classifiers.\nThe downstream note property classifier for MusicSlots is trained by\nminimizing the categorical cross-entropy loss. For the supervised CNN and the linear classifier\ntrained on the baseline AutoEncoders, we use binary cross-entropy loss, since the target is a multi-hot\nvector.\nA.4\nEvaluation Details\nNote Discovery.\nTo compute the note MSE, we first calculate the mean squared error between all\npairs of predicted and ground-truth note spectrograms. Since the orders of the predictions and the\nground-truth are arbitrary, we match them using the Hungarian algorithm ([47]) to find the matching\nwith the lowest MSE. mIoU is calculated by first computing all pairwise IoUs between the predicted\nand ground-truth dB-thresholded masks, and using the Hungarian algorithm to find the optimal\nassignment that gives the highest mIoU. For the Hungarian matching algorithm, we use the scipy\nimplementation scipy.optimize.linear_sum_assignment.\nNote Property Prediction.\nThe performance of the classifiers is quantified using classification\naccuracy. The accuracy is measured by computing the percentage of correctly classified chord\n12\nTable 11: Training hyperparameters of the MusicSlots model for unsupervised note discovery\nexperiments\nHyperparameters\nTraining Steps\n100K\nBatch Size\n32\nOptimizer\nAdam\nMax. Learning Rate\n1e-04\nLearning Rate Warmup Steps\n10K\nDecay Steps\n500K\nGradient Norm Clipping\n1.0\nTable 12: Training hyperparameters for the note property prediction task\nHyperparameters\nTraining steps\n10K\nBatch Size\n32\nOptimizer\nAdam\nLearning Rate\n1e-03\nTable 13: Training hyperparameters for the baseline AEs during the unsupervised pre-training.\nHyperparameters\nTraining Steps\n100K\nLearning Rate\n1e-04\nBatch Size\n32\nOptimizer\nAdam\nDecay Steps\n100K\nGradient Norm Clipping\n1.0\nexamples in the dataset. A chord is considered to be correctly classified if and only if the classifier\npredictions for all of its note properties (i.e. note pitch values, instrument identities) are correct.\nB\nAdditional Results\nIn this section, we present additional results that quantify the importance of different modelling\nchoices.\nSingle-instrument Note Discovery Results\nTable 14 shows the unsupervised note discovery\nperformance of our MusicSlots model with different alpha mask normalization choices on the\nsingle-instrument JSB and JazzNet datasets. We observe significant performance gain in the single-\ninstrument setting when sigmoid-normalized alpha masks or no alpha masks are used in our Music-\nSlots model.\nAblation on Architectural Modifications\nTable 15 presents our ablation study on different archi-\ntectural choices in our MusicSlots model on the multi-instrument Bach Chorales and JazzNet datasets.\nWe start from the ‚ÄòDefault‚Äô model that follows the same setup used for object discovery in [19]. In\nthis setup, we have stride length of (1, 1) in the convolutional encoder layers and the alpha masks of\nthe decoder are normalized using the Softmax function. We find that both implicit differentiation\n([31]) and the removal of the alpha masks from the decoder play a crucial role in improving the note\ndiscovery performance of our MusicSlots model. Increasing the stride length along the time axis\nto 2 also improves its performance, though not as significantly as the other two design choices. By\ncombining these improvements in the model architecture and training optimization, we finally arrive\nat our MusicSlots model without any alpha masking.\n13\nTable 14: Note discovery results on single-instrument BachChorales (JSB) and JazzNet datasets for\nMusicSlots models with different choices for fnorm function. Mean and std-dev. are reported across\n5 seeds.\nDatasets\nMask Norm.\nNote MSE ‚Üì\nmIoU ‚Üë\nJSB-single\nMusicSlots-soft\n75.32 ¬±37.63\n0.68 ¬±0.08\nMusicSlots-sigm\n18.21 ¬±3.40\n0.83 ¬±0.02\nMusicSlots-none\n22.44 ¬±7.07\n0.81 ¬±0.03\nJazzNet-single\nMusicSlots-soft\n114.09 ¬±22.72\n0.63 ¬±0.04\nMusicSlots-sigm\n49.05 ¬±5.98\n0.75 ¬±0.03\nMusicSlots-none\n44.49 ¬±0.62\n0.76 ¬±0.02\nTable 15: Architectural ablations on the MusicSlots for unsupervisd note discovery task. ‚ÄòDefault‚Äô\nhere refers to the MusicSlots model with the setup used for object discovery in [19] , where stride =\n(1, 1) in the convolutional encoder layers and the spatial broadcast decoder outputs softmax alpha\nmasks.\nDataset\nModel\nNote MSE ‚Üì\nmIoU ‚Üë\nJazzNet-multi\nDefault\n70.05 ¬±23.61\n0.70 ¬±0.07\nDefault + stride_length = (1, 2)\n51.31 ¬±1.89\n0.76 ¬±0.04\nDefault - Softmax Alpha Mask\n39.08 ¬±6.35\n0.79 ¬±0.02\nDefault + Implicit Differentiation\n32.56 ¬±9.84\n0.83 ¬±0.00\nMusicSlots\n19.95 ¬±1.89\n0.90 ¬±0.01\nJSB-multi\nDefault\n100.77 ¬±52.91\n0.60 ¬±0.15\nDefault + stride_length = (1, 2)\n60.22 ¬±14.56\n0.76 ¬±0.04\nDefault - Softmax Alpha Mask\n40.06 ¬±14.60\n0.78 ¬±0.04\nDefault + Implicit Differentiation\n59.34 ¬±22.01\n0.79 ¬±0.04\nMusicSlots\n13.47 ¬±0.90\n0.91 ¬±0.01\nVAE Ablation\nTable 16 shows the ablation study for the choice of Œ≤ in the VAEs on the multi-\ninstrument Bach Chorales and JazzNet datasets. We observe that higher Œ≤ results in worse downstream\nproperty prediction performance on both datasets and the best results are achieved using Œ≤ = 0.5.\nTable 16: Note property prediction performance of baseline VAEs with different Œ≤ values. Œ≤ = 1\ncorresponds to the vanilla VAE model. As we increase Œ≤, the property prediction performance using\nthe latent representations from the Œ≤-VAE worsens on both JSB and Jazznet multi-instrument datasets.\nŒ≤\nJSB-multi\nJazznet-multi\nVal-Acc.\nTest-Acc.\nVal-Acc.\nTest-Acc.\n0.5\n97.85 ¬±0.13\n97.42 ¬±0.06\n97.00 ¬±0.38\n81.53 ¬±0.77\n1.0\n96.66 ¬±0.34\n96.21 ¬±0.35\n94.37 ¬±0.55\n71.55 ¬±4.77\n2.0\n92.23 ¬±0.66\n90.74 ¬±0.94\n73.93 ¬±1.97\n47.90 ¬±8.59\n4.0\n82.07 ¬±0.83\n78.95 ¬±1.42\n47.51 ¬±3.71\n11.31 ¬±1.92\n14\nC\nNote Discovery Visualization\nWe provide additional visualization samples of note discovery results from our MusicSlots model\non the JazzNet and Bach Chorales datasets, including both the success and failure cases. We also\nvisualize the effect of using Softmax alpha masks in MusicSlots for note discovery in Figure 11.\nGround-Truth\nMusicSlots\nFigure 5: Unsupervised note discovery result on the JSB-multi-instrument dataset. The MusicSlots\naccurately predicts the ground-truth note spectrograms. It also learns to capture the background (i.e.\nsilence) in the remaining slots that are not matched with the ground-truth notes.\nGround-Truth\nMusicSlots\nFigure 6: Unsupervised note discovery result on the Jazznet-multi-instrument dataset. Similar to the\nexample visualized in Figure 5, MusicSlots successfuly decomposes the given chord spectrogram\ninto its constituent note spectrograms and distribute the background across the remaining slots.\n15\nGround-Truth\nMusicSlots\nFigure 7: Unsupervised note discovery result on the JSB-multi-instrument dataset. On this example,\nMusicSlots successfully predicts most of the ground-truth note spectrograms. However, it overseg-\nments one of the notes (Note 1) by assigning it to slot 1 and 7.\nGround-Truth\nMusicSlots\nFigure 8: Unsupervised note discovery result on the JazzNet-multi-instrument dataset. Similar to the\nexample shown in Figure 7, MusicSlots performs oversegmentation by using three slots (slot 1, 5 and\n7) to model note 1.\n16\nGround-Truth\nMusicSlots\nFigure 9: Visualization of a failure case of MusicSlots on the JSB-multi-instrument dataset. Only two\nof the four matched predictions accurately capture the harmonic structure of the ground-truth note\nspectrograms.\nGround-Truth\nMusicSlots\nFigure 10: Visualization of a failure case of MusicSlots on the Jazznet-multi-instrument dataset.\nMusicSlots completely fails to predict notes 1 and 3 in its slot reconstructions. It also fails to model\nthe background in any of the slots.\n17\nGround-Truth\nNo Alpha Mask\nSoftmax Alpha Mask\nFigure 11: Qualitative performance comparison of MusicSlots without alpha masks (row 2) and\nMusicSlots with softmax-normalized alpha masks (row 3) on the JazzNet single-instrument dataset.\nMusicSlots with softmax alpha masks leaves unnatural gaps in the lower frequency bins where there\nis a strong degree of overlap between the note spectrograms. MusicSlots without any alpha mask does\nnot introduce these artifacts and models the overlapping regions more accurately than its softmax\nnormalized counterpart.\n18\n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS"
  ],
  "published": "2023-11-13",
  "updated": "2023-11-14"
}