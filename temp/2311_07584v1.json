{
  "id": "http://arxiv.org/abs/2311.07584v1",
  "title": "Performance Prediction of Data-Driven Knowledge summarization of High Entropy Alloys (HEAs) literature implementing Natural Language Processing algorithms",
  "authors": [
    "Akshansh Mishra",
    "Vijaykumar S Jatti",
    "Vaishnavi More",
    "Anish Dasgupta",
    "Devarrishi Dixit",
    "Eyob Messele Sefene"
  ],
  "abstract": "The ability to interpret spoken language is connected to natural language\nprocessing. It involves teaching the AI how words relate to one another, how\nthey are meant to be used, and in what settings. The goal of natural language\nprocessing (NLP) is to get a machine intelligence to process words the same way\na human brain does. This enables machine intelligence to interpret, arrange,\nand comprehend textual data by processing the natural language. The technology\ncan comprehend what is communicated, whether it be through speech or writing\nbecause AI pro-cesses language more quickly than humans can. In the present\nstudy, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic\nAnalysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the\nfirst time for the knowledge summarization purpose of the High Entropy Alloys\n(HEAs). The performance prediction of these algorithms is made by using the\nBLEU score and ROUGE score. The results showed that the Luhn algorithm has the\nhighest accuracy score for the knowledge summarization tasks compared to the\nother used algorithms.",
  "text": "1 \n \nPerformance Prediction of Data-Driven Knowledge \nsummarization of High Entropy Alloys (HEAs) literature \nimplementing Natural Language Processing algorithms \nAkshansh Mishra1, Vijaykumar S. Jatti2, Vaishnavi More3, Anish Dasgupta4, Devarrishi Dixit5 and Eyob \nMessele Sefene6 \n1School of Industrial and Information Engineering, Politecnico Di Milano, Milan, Italy \n2Symbiosis Institute of Technology, Symbiosis International (Deemed) University, Pune 412115, India \n3Software Developer, Turabit LLC, Ahmedabad, India \n4Data Science Engineer, Volvo Groups, Bengaluru, India \n5Department of Materials Science Engineering, Christian Albrechts University zu Kiel 24143, Germany \n6Department of Mechanical Engineering, National Taiwan University of Science and Technology, Taiwan \n \nAbstract: The ability to interpret spoken language is connected to natural language processing. \nIt involves teaching the AI how words relate to one another, how they are meant to be used, \nand in what settings. The goal of natural language processing (NLP) is to get a machine \nintelligence to process words the same way a human brain does. This enables machine \nintelligence to interpret, arrange, and comprehend textual data by processing the natural \nlanguage. The technology can comprehend what is communicated, whether it be through \nspeech or writing because AI pro-cesses language more quickly than humans can. In the present \nstudy, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic Analysis (LSA), \nand Kull-back-Liebler (KL) al-gorithm, are implemented for the first time for the knowledge \nsummarization purpose of the High Entropy Alloys (HEAs). The performance prediction of \nthese algorithms is made by using the BLEU score and ROUGE score. The results showed that \nthe Luhn algorithm has the highest accuracy score for the knowledge summarization tasks \ncompared to the other used algorithms. \nKeywords: Natural Language Processing; Artificial Intelligence; High Entropy Alloys; \nKnowledge Summarization. \n \n1. Introduction \nNatural language processing is the ability of a computer program to interpret natural language, \nor a system of communication, in both its spoken and written forms (NLP). It falls under \nartificial intelligence (AI) and Linguistics has origins in NLP, which has been around for over \n50 years. It is useful in a variety of fields, including as search engines, corporate intelligence, \nand medical research. [1-3]. NLP has made it possible for computers to understand natural \nlanguage in the same way that humans do. Whether the speech is written or spoken, natural \nlanguage processing uses artificial intelligence to gather direct experience, analyse it, and \norganize it logically in a way that a computer can understand. Similar to how individuals have \nmultiple sensors like ears to listen and eyes to see, computers have reading programs and \nspeakers to gather sounds. Systems have a script to process their various inputs, just as \n2 \n \nindividuals have a mind to do so. The input is eventually translated into computer-readable \ncode during processing [4-5]. Businesses need a way to effectively process the vast amounts \nof unorganized, text-heavy data they use. Until recently, businesses could not efficiently \nanalyse the natural human language that made up a large portion of the information generated \nonline and kept in databases. Natural language processing comes in handy in this situation. \nConsider the following two sentences: â€œCloud computing coverage should be included of every \nservice-level agreementâ€ and â€œA solid SLA guarantees an easier nightâ€™s sleep-even in the \ncloudâ€ to see the benefit of NLP. If a user uses NLP to conduct a search, the software will \nunderstand that cloud computing is a concept, that cloud is an acronym for cloud computing, \nand that SLA is an abbreviation for service-level agreement. For various purposes, including \ndata gathering, hypothesis building, and pattern detection within and across domains, NLP can \nbe employed for materials, as stated by Olivetti et al. [6] and illustrated with instances. \nTshitoyan et al. [7] employed unsuperived method for materials. The earlier articles contained \nsignificant implicit knowledge about upcoming findings. This further demonstrates that earlier \narticles include a substantial number of implicit knowledge about forthcoming findings. Their \nresults, which hint at a complete technique for mining scientific literature, emphasized the \npossibility of jointly retrieving information and connections from a large amount of scientific \nliterature. Mohammadi et al. [8] used 1120 articles published between 1974 to 2007 and \nevaluated the transdisciplinary tendencies of Iranian research in NST. Using text mining tools, \nit was discovered that the fundamental ideas of the Iranian literature included in NST were \ncontained in 96 phrases. By using multidimensional scaling and basing it on the co-occurrence \nof the essential phrases in the academic articles, the scientific backbone of the Iranian NST was \nthen reconstructed. The findings demonstrated that the multidisciplinary structure of the NST \ndomain in Iranian papers includes pure physics, analytical chemistry, material science and \nengineering, biochemistry, chemical physics, physical science, and new developing themes. \nThe analysis indicates that whereas most RMC literature has been ad-dressed to concrete \ntechnology and material science, very few studies have concentrated on RMC dispatching [9]. \nFinding the root reasons and pertinent elements is necessary to learn from past mistakes. A \nvaluable learning resource is the enormous corpus of textual data on event narratives \naccumulated over time. Due to the massive amount and chaotic nature of the text data, it is \ndifficult to deduce recurrent recurrence trends from it. Using the pipeline industry as an \nexample, Liu et al. [10] used NLP and textual mining algorithms to take ad-vantage of the \nresource and comprehend the occurrencesâ€™ underlying causes and contributing aspects. The \nPipeline and Hazardous Materials Safety Administrationâ€™s (PHMSA) incident databaseâ€™s 3587 \nevent narrative entries in the â€œcommentâ€ section were used for this. It has been shown from \nprior research articles that only a small number of studies have used NLP in the field of material \nscience to achieve knowledge summarization goals. Therefore, this is the first research study \nto use NLP for summarizing knowledge on High Entropy alloys [11-30]. The future scope of \nthis work is to implement this approach in other domains of material science for knowledge \nextraction purposes [31-36]. \n \n \n \n3 \n \n2. High Entropy Alloys and its applications \nIn a multi-component system, high entropy alloys (HEAs) are next generation alloys that \ncontain multiple primary elements. MPEAs (multi-principal element alloys) and complex \nconcentrated alloys are other names for HEAs (CCAs). Professor Ye Junwei first put forth the \nidea of high entropy alloys (HEAs) and multi-principal element alloys (MPEAs) in 2004 [37-\n38]. Since each HEA can be adjusted by small elemental additions, like with present element-\nbased alloys, each HEA is a new alloy base. Numerous novel alloy bases are provided by HEAs \n[39]. Equation 1 provides the number of HEA systems (unique combinations of elements \nwithout disclosing composition). \nğ¶(ğ‘›\nğ‘Ÿ) =\nğ‘›!\nğ‘Ÿ!(ğ‘›âˆ’ğ‘Ÿ)!                                                                   (1) \nwhere n is the number of components in the array that are being used to choose the r \nprimary alloy elements. In order to create a conventional alloy, researchers participating in \nalloy creation historically concentrated on the phase diagram's corners, which only take up a \nsmall part of the design space (see figure 1). The recent advancement of HEAs, however, has \ncaused the center area to receive more attention [40]. \n \nFigure 1. The relationship between the total number of primary elements and the total \nnumber of equiatomic compositions. The ternary figure in the inset shows how the design of \nhigh-entropy alloys differs from that of normal alloys [40]. \nEvery one of the at least five metallic materials used to create high entropy alloys (HEAs) has \nan atomic concentration that ranges from 5 to 35% in the molar form. High entropy alloys have \ngreat work hardenability, high temperature oxidation resistance, and good ductility. \nAdditionally, HEAs have outstanding attractive magnetic characteristics, a high level of wear \nresistance, and superior erosion resistance. The three main categories of HEA fabrication \ntechniques are liquid mixing, solid mixing, and gaseous mixing. Arc melting, electric resistance \n4 \n \nmelting, inductive melting, Bridgman solidification, and laser additive manufacturing are all \ntypes of liquid mixing [41â€“46].  \nAs indicated in figure 2, there are four categories of entropic alloys: high entropy al-loys, \nmedium entropy alloys, low entropy alloys, and pure metal. When it comes to low temperature \napplications, high entropy alloys perform better than conventional materials. \nCryogenic processes use alloys with high entropy [47]. They can be employed as low \ntemperature materials in the civil engineering, superconducting, and aircraft sectors. For a \nvariety of military applications, Geanta et al. [48] tested and characterized High-Entropy \nAlloys from the AlCrFeCoNi System. According to a microstructure research, the micro-\nstructure of alloys with high entropy appeared frozen when they were in a molten state as \nillustrated in figure 3. As illustrated in figure 4, impact testing results revealed that high entropy \nalloy is the best choice for ballistic packages robust at high velocity penetration impacts. \n \nFigure 2. Types of High Entropy Alloys [47] \n \n \nFigure 3. Microstructure analysis of an experimental as-cast AlCrxFeCoNi alloys [48] \n \n5 \n \n \nFigure 4. Dynamic testing on a HEA-steel ballistic package (a) after the first fire and (b) just \nafter second fire [48] \n \n3. Understanding Natural Language Processing Algorithms \nComputer science's field of natural language processing (NLP) investigates how computers and \nhuman languages interact. It is the engine that powers search engines like Google. Manual \nlinguistic analysis is possible and has been practiced for centuries. But technological \nadvancements continue, particularly in the field of natural language pro-cessing (NLP). Natural \nLanguage Processing (NLP) has been aggressively pursued by the machine and deep learning \nfields using a variety of methods. Even though some of the methods we use today have only \nbeen around for a short while, they are already altering how we communicate with technology. \nResearch in the area of natural language pro-cessing (NLP) shows us how to create systems \nthat can comprehend human language in practical ways. Among many others, these include \nchatbots, machine translation soft-ware, and speech recognition systems. The same problem, \nhowever, can be solved using a wide variety of algorithms. In this article, four widely used \ntechniques for teaching ma-chine learning models to analyze data from human language are \ncompared. \nNLP (natural language processing) algorithms come in several forms. They can be divided into \ngroups based on the tasks they do, such as entity recognition, relation extraction, entity parsing, \nand part of speech tagging. The tags created by Part of Speech Tagging algorithms describe \nhow specific phrase components work. For instance, the usual tags for words in English phrases \nare verb, noun, and preposition. The process of parsing is breaking down a sentence into its \ngrammatical constituents (phrase structure), typically by splitting it into phrases and pieces of \nspeech that trees can represent. Named entities, such as persons, companies, or locations, are \nfound within a text using named entity recognition. Semantic analysis's subsidiary task is \nrelationship extraction. According to relation extraction, there must be a connection between \ntwo items if they may be connected by an auxiliary verb (such as \"is,\" \"was,\" or \"was not,\" \netc.) or another marker word (such as \"to\" or \"from\"). \nSeveral NLP algorithms are created with a variety of goals in mind, from language production \nto sentiment analysis. One technique for helping computers comprehend your intentions when \nyou speak or write is sentiment analysis. Companies employ sentiment analysis as a tool to \nascertain whether their customers are satisfied with the goods or ser-vices they provide. \nHowever, it also has the potential to better understand how individu-als feel about issues in \npolitics, the medical field, or any other field where individuals have different convictions. \n6 \n \nBased on their language, as well as their social media postings and comments, sentiment \nanalysis may tell whether someone is feeling positively or neg-atively about anything. \nSentiment analysis is important because it enables businesses to handle dissatisfied clients \nmore effectively. Named entity recognition is sometimes thought of as text classification, \nwhere a group of documents must be categorized accord-ing to person or organization names. \nAlgorithms for machine learning typically handle this task. There are many classifiers \navailable, but the k-nearest neighbor technique is the most straightforward (kNN). This \nstraightforward technique is predicated on the idea that numerous words adjacent to a specific \nthing described in a document are also parts of that entity. Text processing tasks like text \nsummarization have received a lot of attention recently. The fundamental goal of text \nsummarizing is to provide an edited version of the initial text that only conveys the key ideas. \nThe two types of text summarization are extractive and abstractive. Term frequency-inverse \ndocument frequency (TF-IDF) score, frequency words, and other statistical measures linked to \ncentrality (i.e., how significant they are) are used to choose sentences from the source \ndocuments in the extract text summarization method, also known as key phrase extraction. \nAbstractive text summarization produces new phrases with details not found in the source text. \nA subset of natural language pro-cessing is called aspect mining. Aspect mining identifies the \nvarious traits, components, or facets of text. Aspect mining divides texts into various categories \nin order to pinpoint the attitudesâ€”often referred to as sentimentsâ€”described in each category. \nWhen aspects and subjects are compared, the topic is classified rather than the feeling. Aspects \nmight include things, acts, feelings or emotions, qualities, occurrences, and more, depending \non the technique employed. Companies have used aspect mining methods to identify customer \nresponses. Aspects can be important components in the research and development of artificial \nintelligence for new technologies like chatbots, which use AI programs to give a response with \nappropriate answers based on information gleaned from text conversations that they analysis \nfor patterns and interconnection between questions and answers. To obtain explicit or implicit \nattitudes regarding features of text, aspect mining is some-times integrated with sentiment \nanalysis tools, a different sort of natural language pro-cessing. Because aspects and views are \nso closely related, they are frequently employed as synonyms in literature. Companies might \nbenefit from aspect mining since it enables them to identify the type of client replies. \n \n4. Metrics features for evaluating the performance of knowledge summarization in the \npresent study \nIn the present work, we have implemented metrics features such as BLEU and ROUGE for \nmeasuring the performance of NLP algorithms for knowledge summarization purposes. Both \nthe ROGUE and BLEU sets of metrics can be used to create text summaries. BLEU was first \nrequired for machine translation, but it is as suitable for the task of text summarization. \nA candidate translation of a text is scored using the Bilingual Evaluation Understudy (BLEU), \nwhich compares it to one or even more reference translations. Despite being designed for \ntranslation, it has the potential to assess text produced for a variety of NLP activities. The \ncomputation of Precision scores for the 1- to 4-gram weight range stated in equations 2 is the \ninitial stage. \n \nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğ‘œğ‘“ ğ‘âˆ’ğºğ‘Ÿğ‘ğ‘šğ‘  (ğ‘ğ‘›) =\nğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘ \nğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘           (2) \nNext, we use equation 3 to aggregate these Precision Scores. For various N values and with \nvarious weight values, this can be calculated. \n7 \n \nğºğ‘’ğ‘œğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ´ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ‘) = ğ‘’ğ‘¥ğ‘(âˆ‘\nğ‘¤ğ‘›ğ‘™ğ‘œğ‘”ğ‘ğ‘›\nğ‘\nğ‘›=1\n)                                   (3) \n              \nEquation 4 is used in the third step to calculate a \"Brevity Penalty.\" With an exponential \ndecline, the shortness penalty penalizes computed translations which are too brief in \ncomparison to the nearest reference length. The shortness penalty makes up for the absence of \na recall phrase in the BLEU score. \nğµğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘¡ğ‘¦ ğ‘ƒğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦= {\n1, ğ‘–ğ‘“ ğ‘> ğ‘Ÿ\nğ‘’(1âˆ’ğ‘Ÿ\nğ‘), ğ‘–ğ‘“ ğ‘â‰¤ğ‘Ÿ\n                                                                         (4) \n \nWhere c is the predicted length and r is the target length. \nThen, as illustrated in equation 5, we multiply the Brevity Penalty by the Geometric Average \nof the Precision Scores to arrive at the Bleu Score. \nğµğ¿ğ¸ğ‘ˆ(ğ‘) = ğµğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘¡ğ‘¦ ğ‘ƒğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦. ğºğ‘’ğ‘œğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ´ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ‘)                       (5) \nAnother metric feature used is the ROUGE algorithm. Recall-Oriented Understudy for Gisting \nEvaluation is the abbreviation for this method. In essence, it is a collection of measures for \nassessing machine translations and text summarizations that are done automatically. It \nfunctions by contrasting an automatically generated summary or translation with a collection \nof reference summaries (typically human-produced). The number of \"n-grams\" that match \nbetween the text produced by our model and a \"reference\" is measured by ROUGE-N. A \ncollection of tokens or words is known as an n-gram. A bigram (2-gram) consists of two \nconsecutive words, whereas a unigram (1-gram) only has one word. \n5. Materials and Methods \nCreating a concise, fluid, and, most importantly, excellent description of lengthy text content \nis known as text summarization. The fundamental goal of information extraction is to be \ncapable of extracting the most important information from a large body of text and displaying \nit in a human-readable way. Automatic text summarizing techniques could be particularly \nbeneficial as online textual documents increase since more informative material can be viewed \nquickly. The framework of the knowledge summarization by NLP algorithms is shown in \nFigure 5. Data were collected from the abstract of the twenty published papers based on High \nEntropy Alloys available on the Google Scholar database which has wide availability of papers \nin comparison to other databases. Preprocessing of text has a significant impact on how well \nmodels perform. In order to develop a machine learning model, preprocessing the data is a \ncrucial stage, and the effectiveness of the preprocessing determines the final outcomes. The \ninitial stage of creating a model in NLP is text preparation. In any language used by people, \nstop words are common. These words are eliminated from our text, allowing us to concentrate \nmore on the crucial information by re-moving the low-level information they contain. The most \nsignificant of these is tokenization. It is the process of segmenting a stream of text information \ninto tokens, which can be words, keywords, sentences, symbols, or other significant \ncomponents. Numerous open-source programs can be used to tokenize data. Text is divided \ninto sentences through the process of sentence tokenization. \n8 \n \nBecause the tokenizer is based on a corpus of professional English text, it performs well when \napplied to literature, reporting, and formal documents. Tokenization is the procedure of \ndividing text into a sequence of tokens from a string of text. Tokens can be viewed as \ncomponents, similar to how a word functions as a token in a phrase and how a sentence \nfunctions as a token in a prose. For evaluating the performance, BLEU and ROUGE metrics \nfeatures are then implemented. The accuracy of BLEU is slightly in-creased by using the \nBLEU-4 feature. Recall, precision, F1 score are estimated using equations 6, 7, and 8, \nrespectively, and further evaluation of the ROUGE Score. \nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’=\nğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  ğ‘“ğ‘œğ‘¢ğ‘›ğ‘‘ ğ‘–ğ‘› ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ğ‘ğ‘›ğ‘‘ ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘ \nğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  ğ‘–ğ‘› ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘ \n                  (6) \n \nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’=\nğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  ğ‘“ğ‘œğ‘¢ğ‘›ğ‘‘ ğ‘–ğ‘› ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ğ‘ğ‘›ğ‘‘ ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘ \nğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  ğ‘–ğ‘› ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\n            (7) \n \n      ğ¹1 âˆ’ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’= 2 Ã—\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’                                         (8)                    \n \n9 \n \n \nFigure 5. NLP framework used in the recent study \n                                         \n6. Results \n \nFigure 6 presents the initial plot of the frequency distribution. We can observe that the \ndistribution includes a lot of punctuation and non-content terms like â€œthe,â€ â€œof,â€ â€œand,â€ etc. \n(we refer to them as stop words). Before drawing the graph, we can take them out. To do this, \nwe must import stopwords out from corpus package. A new list of things to ignore is created \n10 \n \nby adding the stop word list, a list of punctuation, and a list of single figures uti-lizing + signs. \nFigure 7 displays the final plot of the frequency distribution after punctua-tion and stop marks \nhave been removed. Every human language contains a large number of stop words. By \neliminating the low-level information, these phrases can be modified to be more concentrated \non the important information. \nThe filtered tokens can alternatively be displayed as a word cloud. This enables us to use the \nWordCloud().generate from frequencies() method to get a general overview of the corpus. A \nfrequency dictionary containing all the tokens and their frequencies in the text serves as the \nmethodâ€™s input. It is necessary to first import the Counter package into Py-thon before \ngenerating a vocabulary using the input of the filtered text variable. After pre-processing the \nword count plot is obtained as seen in figure 8. Word in a word cloud is used to visualize text \nwhich shows the frequency of that word. To highlight important textual information, use a \nword cloud. \nFigure 9 shows the plot of the obtained Individual N- grams values for the corre-sponding     \nalgorithms. It is observed that the individual 1-gram value of the LUHN algorithm is the highest \nin comparison to the other algorithms. Figure 10 shows the ob-tained BLEU score for each \nalgorithm. To slightly increase the performance of the BLEU score, BLEU-4 score is obtained \nas shown in Figure 11.  \nTable 1 displays the results of the summative evaluation of the Natural Language Processing \nalgorithms utilized for information summarization using the ROGUE method. \n \nFigure 6. Initial Frequency Plot \n \n \nFigure 7. Final Frequency Plot \n11 \n \n \nFigure 8. Word Cloud \n \n \nFigure 9. Obtained N-grams for each NLP algorithms \n \n \n \n                               Figure 10. BLEU Score for the implemented NLP Algorithms \n12 \n \n \n                                Figure 11. BLEU-4 Scores for the implemented NLP Algorithms \n \nTable 1. NLP implementation results \n \nNLP Algorithms \nRecall \nPrecision \nF1-Score \nText Rank \n1.0 \n0.370 \n0.540 \nLex Rank \n1.0 \n0.299 \n0.460 \nLUHN \n1.0 \n0.423 \n0.595 \nLatent Semantic \nAnalysis \n1.0 \n0.350 \n0.519 \nKL Algorithm \n1.0 \n0.250 \n0.400 \n \n7. Discussion \nIt is possible to find both keywords and most relevant phrases within a text using a text ranking \nalgorithm based on graphs. It is used to identify the most important sentences in a manuscript \nby creating a graph with each sentence's vertices being the network's nodes, and the edges \nconnecting them being the number of words they share. \nWith the help of the Pagerank algorithm, the most important sentences are selected based on \ntheir phrase structure. Now, we are only allowed to use the most important sentences while \nwriting a summary. In the Page Rank algorithm, weights are assigned in accordance with \nEquation 9. \nğ‘‹(ğ‘‰ğ‘–) = (1 âˆ’ğ‘‘) + ğ‘‘Ã— âˆ‘\n1\n|ğ‘‚ğ‘¢ğ‘¡(ğ‘‰ğ‘—)| ğ‘‹(ğ‘‰ğ‘—)\nğ‘—âˆˆğ¼ğ‘›(ğ‘‰ğ‘–)\n                                                                   (9) \n13 \n \nWhere X(V_i) is the assigned weight of the i webpage, d is the damping factor, In(V_i) \nis the inbound links of i, and  Out(V_j) is the outgoing links of j. In order to find pertinent \nterms, the textrank algorithm creates a word network. This network is created by looking at the \nsequence in which words come before one another. In the text, a connection is made between \ntwo words when they are placed next to one another. The connection is more significant if the \ntwo terms are placed next to one another more frequently. \nIn order to determine the significance of each word, the Pagerank method is applied to the \nresulting network. Despite their relevance, they are all retained except the top third. As soon \nas they appear consecutively in the text, the relevant terms are arranged into a keywords table. \nNow, we are only permitted to include key sentences when summarizing the content. In order \nto find pertinent terms, the textrank algorithm creates a word network. This network is \nconstructed by looking at the order in which words come before one another. In the text, a \nconnection is made between two words when they are placed next to one another. If the two \nterms are placed next to one another more frequently, the connection is given more \nsignificance. The relevance of each word in the resulting network is then determined using the \nPagerank algorithm. All but the top third of these words are kept since they are thought to be \nrelevant. If the pertinent words appear right after one another in the text, they are then combined \ninto a keywords table. \nThe LexRank algorithm assesses the centrality of sentences on graphs as a tool for \nunsupervised text summarization. The fundamental idea is that sentences â€œendorseâ€ additional \nsentences similar to them for the reader. Because of this, a statement is likely to be significant \nif it sounds like many others. The unsupervised LexRank technique uses eigenvector centrality-\nbased images to intelligently summarize the input text. A cosine similarity-based adjacency \nmatrix is used to visualize the six required sentences. The sentences are ranked according to \nhow similar they are to the centroid sentence, which serves as the mean for all the assertions in \nthe text file in this method. The N (number of all possible words)-dimensional vectors are \nrepresented by the bag of words model, which is used to define similarity. Equation 10 is used \nto determine the value of the appropriate dimension in the textâ€™s vector representation. \n \nğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘  ğ‘œğ‘“ ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘›ğ‘” ğ·ğ‘–ğ‘šğ‘’ğ‘›ğ‘ ğ‘–ğ‘œğ‘›= ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘  ğ‘œğ‘“ ğ‘¡â„ğ‘’ ğ‘¤ğ‘œğ‘Ÿğ‘‘Ã—\nğ¼ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘’ ğ·ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ (ğ¼ğ·ğ¹)ğ‘œğ‘“ ğ‘¡â„ğ‘’ ğ‘¤ğ‘œğ‘Ÿğ‘‘          \n                   (10) \n \nThe associated IDF-modified cosine number of the word is determined by Equation 11. \n \nğ¼ğ·ğ¹ ğ‘€ğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ ğ¶ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ (ğ‘¥, ğ‘¦) =\nâˆ‘\nğ‘¡ğ‘“ğ‘¤,ğ‘¥\nğ‘¤âˆˆğ‘¥,ğ‘¦\nğ‘¡ğ‘“ğ‘¤,ğ‘¦(ğ‘–ğ‘‘ğ‘“ğ‘¤)2\nâˆšâˆ‘\n(ğ‘¡ğ‘“ğ‘¥ğ‘–,ğ‘¥ğ‘–ğ‘‘ğ‘“ğ‘¥ğ‘–)2\nğ‘¥ğ‘–âˆˆğ‘¥\n   âˆšâˆ‘\n(ğ‘¡ğ‘“ğ‘¦ğ‘–,ğ‘¦ğ‘–ğ‘‘ğ‘“ğ‘¦ğ‘–)2\nğ‘¦ğ‘–âˆˆğ‘¦\n      (11)                                         \n \nwhere ğ‘–ğ‘‘ğ‘“ğ‘¤ is the inverse document frequency of a word w and ğ‘¡ğ‘“ğ‘¤,s is the number of times \nthe word w appears in the sentences. \n \nBy analyzing sample corpora of natural literature, latent semantic analysis (LSA) is a \ncomputational method for computer simulation and modeling of the definition of words and \npassages. Many facets of learning and interpreting human languages can be accurately \napproximated by LSA. It supports a range of applications in information retrieval, instructional \ntechnology, and other information processing challenges where complicated wholes can be \nviewed as additive functionalities of constituent elements. Latent Semantic Analysis, \ncommonly known as LSI (for Latent Semantic Indexing), simulates the contribution of words \ncombined into coherent sequences to natural language. It uses the well-known Singular Value \nDecomposition (SVD) method from matrix algebra, which wasnâ€™t until the late 1980s, and the \ndevelopment of potent digital computing systems and algorithms to make use of them that it \nwas possible to apply to such complicated events. To create a semantic space for a speech, LSA \n14 \n \ndivides a sizable representational text corpus into rectangular cells, each containing a transform \nof the frequency of a given word in a specific section. \nThe Luhn algorithm uses a TF-IDF-based methodology. According to their frequency, it only \nchooses words with tremendous significance. The words appearing at the documentâ€™s \nbeginning are given more weight. The initial step is to identify which words con-tribute most \nto the document's meaning. A frequency analysis must be conducted first, followed by the \nplacement of significant but not insignificant English terms. \nIn the second step, the terms that appear most often in the document are determined, and a \nsubset of those that are less common but still important is then chosen. Calculating a sentenceâ€™s \ngrade requires the usage of Equation 12. \n \nğ‘†ğ‘ğ‘œğ‘Ÿğ‘’=\n(ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘šğ‘’ğ‘ğ‘›ğ‘–ğ‘›ğ‘”ğ‘“ğ‘¢ğ‘™ ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ )2\nğ‘†ğ‘ğ‘ğ‘› ğ‘œğ‘“ ğ‘šğ‘’ğ‘ğ‘›ğ‘–ğ‘›ğ‘”ğ‘“ğ‘¢ğ‘™ ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ \n                                                                              (12) \n \nThe Luhn approach makes creating summaries out of a group of words simple. The technique \ncan be applied in two steps. First, we attempt to determine which keywords are most crucial to \nunderstanding the text as a whole. In order to accomplish this, according to Luhn, a frequency \nstudy should be done first, then phrases that contain major but not inconsequential English \nterms must be found. A subset of the less frequent but still signif-icant terms is then selected \nafter the most frequent terms in the material are identified in the second step. \nEvery time KL divergence decreases, a sentence is added to the summary using the Kull-back-\nLieber (KL) method, which would be predicated on the greedy method strategy. The KL \napproach decides whether or not it detracts from its own intake vocabulary by re-ducing the \nsummary vocabulary. If there are L elements in a synthesized and D for a par-ticular text, the \nKL algorithm offers a criterion for selecting summaries. Although various error metrics are in \nuse right now, sending as little information as possible is our top pri-ority. These two models \nreduce our problem to a finite number of variables. The best method for deciding which \ndistribution is better is to use the Kullback-Liber algorithm to find which distribution maintains \nthe most information from the original data source. \nThe findings imply that phrase centrality rating derived from graphs are used by the LexRank \nalgorithm for unsupervised text summarization. The fundamental tenet is that statements \nâ€œrecommendâ€ other assertions to readers if they are comparable to their own. A remark is likely \nextremely important if it is virtually identical to multiple others. The significance of the word \nâ€œrecommendingâ€ in this statement helps to illustrate its signifi-cance. As a result, many other \nphrases must be linked to a statement for it to communicate and be included with the summary. \nThis makes perfect sense and allows applying the technique to any brand-new content. \nAccording to reports, the LexRank algorithmâ€™s f1-score and accuracy value are lower than that \nof the LSA approach. To understand textual information statistically, Latent Semantic Analysis \n(LSA) is used. The foundation of topic modeling is this tactic. A specific topic and data matrix \nof our current arrangements and materials will be split into two separate matrices to achieve \nthe main purpose. \nThe Luhn Algorithm has higher f1-scores than the LexRank and LSA algorithms, it has been \nfound. The Luhn techniqueâ€™s Term Frequency-Inverse Document Frequency (TF-IDF) basis \nmay also be shown. Only the most important terms are selected, based on how consistently \nthey are used. A phraseâ€™s first few words are given additional weight. The Text rank method \nlikewise follows the f1-score of the Luhn algorithm. It is a fairly evident objective for Text \n15 \n \nRank to identify how closely related each expression is to each other phrase in the text. It is \nobserved that in addition to the ROUGE score also the BLEU-4 score of the LUHN algorithm \nis higher in comparison to the other algorithms. \n8. Conclusions \nAn autonomous text summary can be produced with the help of machine learning and natural \nlanguage processing. Processing natural language is crucial to ma-chine-human interaction. It \nis still developing, and more research is being done in this ar-ea. Natural Language Processing \nand automatic knowledge summary have emerged as vital methods for compressing lengthy \nand tedious papers, whether technical, financial, legal, medical or literary. Every sector, \nincluding business and academia, stands to win. And we are still far from realizing all of its \npossibilities. In the future, text summarization technology may become more advanced and \ncapable of comprehending natural language. The most recent research leads to the following \nconclusions: \nâ€¢The research publications that summarized High Entropy Alloys were sub-jected to \nText Rank, KL Algorithm, LSA, Luhn, and Lex Rank i.e. five distinct Natural \nLanguage Processing techniques. \nâ€¢The results showed that the Luhn algorithm produced the highest f1-score of 0.595 \ncompared to the other algorithms. \nâ€¢The BLEU-4 results support the results obtained by ROUGE metric features. It is \nobserved that the BLEU-4 score of the LUHN algorithm is higher in comparison to the \nother algorithms.  \nâ€¢The main limitation is the dependency of accuracy score on the number of datasets. \nThis limitation can be improved by increasing the number of datasets. \nâ€¢The future scope of this work will be the implementation of more sophisticated \nalgorithms like BERT, XL Net and GPT2 for the knowledge summarization purpose. \n \nFunding: This research received no external funding \nData and Code Availability Statement: The data and code supporting this studyâ€™s findings \nare available upon request from the authors. \nConflicts of Interest: The authors declare no conflict of interest. \n \nReferences \n \n1. Chowdhary K. Natural language processing. Fundamentals of artificial intelligence. \n2020:603-49. \n2. Nadkarni PM, Ohno-Machado L, Chapman WW. Natural language processing: an \nintroduction. Journal of the American Medical Informatics Association. 2011 Sep \n1;18(5):544-51. \n3. Chopra, A., Prashar, A. and Sain, C., 2013. Natural language processing. International \njournal of technology enhancements and emerging engineering research, 1(4), pp.131-\n134. \n16 \n \n4. Bird, S., Klein, E. and Loper, E., 2009. Natural language processing with Python: \nanalyzing text with the natural language toolkit. â€œOâ€™Reilly Media, Inc.â€. \n5. Lehnert, W.G. and Ringle, M.H. eds., 2014. Strategies for natural language processing. \nPsychology Press. \n6. Olivetti, E.A., Cole, J.M., Kim, E., Kononova, O., Ceder, G., Han, T.Y.J. and \nHiszpanski, A.M., 2020. Data-driven materials research enabled by natural language \nprocessing and information extraction. Applied Physics Reviews, 7(4), p.041317. \n7. Xie, L., Zhu, X., Sun, W., Jiang, C., Wang, P., Yang, S., Fan, Y. and Song, Y., 2022. \nInvestigations on the material flow and the influence of the resulting texture on the \ntensile properties of dissimilar friction stir welded ZK60/Mgâ€“Alâ€“Snâ€“Zn joints. Journal \nof Materials Research and Technology, 17, pp.1716-1730.  \n8. Mohammadi, E., 2012. Knowledge mapping of the Iranian nanoscience and \ntechnology: a text mining approach. Scientometrics, 92(3), pp.593-608. \n9. Maghrebi, M., Waller, S.T. and Sammut, C., 2015. Text mining approach for reviewing \nthe ready mixed concrete literature. In 2nd International Conference on Civil and \nBuilding Engineering Informatics, University of Osaka, Tokyo, Japan (pp. 105-109). \n10. Liu, G., Boyd, M., Yu, M., Halim, S.Z. and Quddus, N., 2021. Identifying causality \nand contributory factors of pipeline incidents by employing natural language \nprocessing and text mining techniques. Process Safety and Environmental Protection, \n152, pp.37-46. \n11. Sathiyamoorthi, P. and Kim, H.S., 2022. High-entropy alloys with heterogeneous \nmicrostructure: processing and mechanical properties. Progress in Materials Science, \n123, p.100709. \n12. Zhang, Y., Wang, D. and Wang, S., 2022. Highâ€Entropy Alloys for Electrocatalysis: \nDesign, Characterization, and Applications. Small, 18(7), p.2104339. \n13. Dewangan, S.K., Mangish, A., Kumar, S., Sharma, A., Ahn, B. and Kumar, V., 2022. \nA review on high-temperature applicability: a milestone for high entropy alloys. \nEngineering Science and Technology, an International Journal, p.101211. \n14. Zhang, Q., Zhang, S., Luo, Y., Liu, Q., Luo, J., Chu, P.K. and Liu, X., 2022. \nPreparation of high entropy alloys and application to catalytical water electrolysis. \nAPL Materials, 10(7), p.070701. \n15. Xue, L., Ding, Y., Pradeep, K.G., Case, R., Castaneda, H. and Paredes, M., 2022. \nDevelopment of a non-equimolar AlCrCuFeNi high-entropy alloy and its corrosive \nresponse to marine environment under different temperatures and chloride \nconcentrations. Journal of Alloys and Compounds, p.167112. \n16. Wu, Q., He, F., Li, J., Kim, H.S., Wang, Z. and Wang, J., 2022. Phase-selective \nrecrystallization \nmakes \neutectic \nhigh-entropy \nalloys \nultra-ductile. \nNature \ncommunications, 13(1), pp.1-8. \n17. Oliveira, J.P., Shen, J., Zeng, Z., Park, J.M., Choi, Y.T., Schell, N., Maawad, E., Zhou, \nN. and Kim, H.S., 2022. Dissimilar laser welding of a CoCrFeMnNi high entropy alloy \nto 316 stainless steel. Scripta Materialia, 206, p.114219. \n18. Wu, D., Kusada, K., Nanba, Y., Koyama, M., Yamamoto, T., Toriyama, T., \nMatsumura, S., Seo, O., Gueye, I., Kim, J. and Rosantha Kumara, L.S., 2022. Noble-\nMetal High-Entropy-Alloy Nanoparticles: Atomic-Level Insight into the Electronic \nStructure. Journal of the American Chemical Society, 144(8), pp.3365-3369. \n19. Rong, Z., Wang, C., Wang, Y., Dong, M., You, Y., Wang, J., Liu, H., Liu, J., Wang, \nY. and Zhu, Z., 2022. Microstructure and properties of FeCoNiCrX (XMn, Al) high-\nentropy alloy coatings. Journal of Alloys and Compounds, 921, p.166061. \n20. Wei, D., Gong, W., Wang, L., Tang, B., Kawasaki, T., Harjo, S. and Kato, H., 2022. \nStrengthening of high-entropy alloys via modulation of cryo-pre-straining-induced \ndefects. Journal of Materials Science & Technology, 129, pp.251-260. \n17 \n \n21. Yang, Y., Li, Z., Zhang, W., Ma, Y., Xiong, Q., Li, W., Chen, S. and Wu, Z., 2022. \nConcentration of â€œMysterious Soluteâ€ in CoCrFeNi high entropy alloy. Scripta \nMaterialia, 211, p.114504. \n22. Jin, Z., Zhou, X., Hu, Y., Tang, X., Hu, K., Reddy, K.M., Lin, X. and Qiu, H.J., 2022. \nA fourteen-component high-entropy alloy@ oxide bifunctional electrocatalyst with a \nrecord-low Î” E of 0.61 V for highly reversible Znâ€“air batteries. Chemical Science. \n23. Banko, L., Krysiak, O.A., Pedersen, J.K., Xiao, B., Savan, A., LÃ¶ffler, T., Baha, S., \nRossmeisl, J., Schuhmann, W. and Ludwig, A., 2022. Unravelling Compositionâ€“\nActivityâ€“Stability Trends in High Entropy Alloy Electrocatalysts by Using a Dataâ€\nGuided Combinatorial Synthesis Strategy and Computational Modeling. Advanced \nEnergy Materials, 12(8), p.2103312. \n24. Wetzel, A., von der Au, M., Dietrich, P.M., Radnik, J., Ozcan, O. and Witt, J., 2022. \nThe comparison of the corrosion behavior of the CrCoNi medium entropy alloy and \nCrMnFeCoNi high entropy alloy. Applied Surface Science, 601, p.154171. \n25. Zhou, X.Y., Zhu, J.H., Wu, Y., Yang, X.S., Lookman, T. and Wu, H.H., 2022. Machine \nlearning assisted design of FeCoNiCrMn high-entropy alloys with ultra-low hydrogen \ndiffusion coefficients. Acta Materialia, 224, p.117535. \n26. Moghaddam, A.O., Sudarikov, M., Shaburova, N., Zherebtsov, D., Zhivulin, V., \nSolizoda, I.A., Starikov, A., Veselkov, S., Samoilova, O. and Trofimov, E., 2022. High \ntemperature oxidation resistance of W-containing high entropy alloys. Journal of \nAlloys and Compounds, 897, p.162733. \n27. Wei, D., Wang, L., Zhang, Y., Gong, W., Tsuru, T., Lobzenko, I., Jiang, J., Harjo, S., \nKawasaki, T., Bae, J.W. and Lu, W., 2022. Metalloid substitution elevates \nsimultaneously the strength and ductility of face-centered-cubic high-entropy alloys. \nActa Materialia, 225, p.117571. \n28. Yuan, J.L., Wang, Z., Jin, X., Han, P.D. and Qiao, J.W., 2022. Ultra-high strength \nassisted by nano-precipitates in a heterostructural high-entropy alloy. Journal of Alloys \nand Compounds, 921, p.166106. \n29. Hou, J.X., Liu, S.F., Cao, B.X., Luan, J.H., Zhao, Y.L., Chen, Z., Zhang, Q., Liu, X.J., \nLiu, C.T., Kai, J.J. and Yang, T., 2022. Designing nanoparticles-strengthened high-\nentropy alloys with simultaneously enhanced strength-ductility synergy at both room \nand elevated temperatures. Acta Materialia, 238, p.118216. \n30. Doan, D.Q. and Fang, T.H., 2022. Effect of vibration parameters on the material \nremoval characteristics of high-entropy alloy in scratching. International Journal of \nMechanical Sciences, 232, p.107597. \n31. Dhungana, D. S., Mallet, N., Fazzini, P.-F., Larrieu, G., Cristiano, F., & Plissard, S. \n(2022). Self-catalyzed InAs nanowires grown on Si: the key role of kinetics on their \nmorphology. Nanotechnology. https://doi.org/10.1088/1361-6528/ac8bdb \n32. Sadek, D., Dhungana, D. S., Coratger, R., Durand, C., Proietti, A., Gravelier, Q., Reig, \nB., Daran, E., Fazzini, P. F., & Cristiano, F. (2021). Integration of the rhombohedral \nBiSb (0001) Topological Insulator on a cubic GaAs (001) substrate. ACS Applied \nMaterials & Interfaces, 13(30), 36492â€“36498. \n33. Dhungana, D. S., Grazianetti, C., Martella, C., Achilli, S., Fratesi, G., & Molle, A. \n(2021). Twoâ€Dimensional Siliceneâ€“Stanene Heterostructures by Epitaxy. Advanced \nFunctional Materials, 31(30), 2102797. \n34. Dhungana, D. S., Hemeryck, A., Sartori, N., Fazzini, P.-F., Cristiano, F., & Plissard, \nS. R. (2019). Insight of surface treatments for CMOS compatibility of InAs nanowires. \nNano Research, 12(3), 581â€“586. \n35. Sefene, E., Tsegaw, A., Mishra, A. (2022). 'Process Parameter Optimization of \n6061AA Friction Stir Welded Joints Using Supervised Machine Learning Regression-\nBased Algorithms', Journal of Soft Computing in Civil Engineering, 6(1), pp. 127-137. \ndoi: 10.22115/scce.2022.299913.1350 \n18 \n \n36. Bonaventura, E., Dhungana, D.S., Martella, C., Grazianetti, C., Macis, S., Lupi, S., \nBonera, E. and Molle, A., 2022. Optical and thermal responses of silicene in Xene \nheterostructures. Nanoscale horizons, 7(8), pp.924-930. \n37. Zhang, Yong. (2019). History of High-Entropy Materials. 10.1007/978-981-13-8526-\n1_1 \n38. Yeh JW, et al. Nanostructured high-entropy alloys with multiple principal elements: \nnovel alloy design concepts and outcomes. Adv. Eng. Mat. 2004; 6:299â€“303. doi: \n10.1002/adem.200300567. \n39. Miracle D. B. (2019). High entropy alloys as a bold step forward in alloy development. \nNature communications,10(1),1805. https://doi.org/10.1038/s41467-019-09700-1 \n40. Ye, Y.F., Wang, Q., Lu, J., Liu, C.T. and Yang, Y., 2016. High-entropy alloy: \nchallenges and prospects. Materials Today, 19(6), pp.349-362. \n41. Modupeola Dada, Patricia Popoola, Samson Adeosun and Ntombi Mathe (September \n27th 2019). High Entropy Alloys for Aerospace Applications [Online First], \nIntechOpen, \nDOI: \n10.5772/intechopen.84982. \nAvailable \nfrom: \nhttps://www.intechopen.com/online-first/high-entropy-alloys-for-aerospace-\napplications \n42. Cui, L.; Ma, B.; Feng, S.Q.; Wang, X.L. Microstructure and Mechanical Properties of \nHigh-Entropy Alloys CoCrFeNiAl by Welding. Adv. Mater. Res. 2014, 936, 1635â€“\n1640. \n43. Lippold, J.; Kiser, S.; DuPont, J. Welding Metallurgy and Weldability of Nickel-Base \nAlloys; Wiley: Hoboken, NJ, USA, 2013. \n44. Vendan, S.; Gao, L.; Garg, A.; Kavitha, P.; Dhivyasri, G.; SG, R. Interdisciplinary \nTreatment to ARC Welding Power Sources; Springer: Singapore, 2018. \n45. Kong, X.; Yang, Q.; Li, B.; Rothwell, G.; English, R.; Ren, X. Numerical study of \nstrengths of spot-welded joints of steel. Mater. Des. 2008, 29, 1554â€“1561 \n46. Chen, S.; Tong, Y.; Liaw, P. Additive Manufacturing of High-Entropy Alloys: A \nReview. Entropy 2018, 20, 937. \n47. Rui Xuan Li and Yong Zhang (December 3rd 2018). Entropic Alloys for Cryogenic \nApplications, Stainless Steels and Alloys, Zoia Duriagina, IntechOpen, DOI: \n10.5772/intechopen.82351. \nAvailable \nfrom: \nhttps://www.intechopen.com/books/stainless-steels-and-alloys/entropic-alloys-for-\ncryogenicapplications \n48. Victor Geanta and Ionelia Voiculescu (October 23rd 2019). Characterization and \nTesting of High-Entropy Alloys from AlCrFeCoNi System for Military Applications \n[Online First], IntechOpen, DOI: 10.5772/intechopen.88622. Available from: \nhttps://www.intechopen.com/online-first/characterization-and-testing-of-\nhighentropy-alloys-from-alcrfeconi-system-for-military-applications \n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR",
    "cs.IT",
    "cs.LG",
    "math.IT"
  ],
  "published": "2023-11-06",
  "updated": "2023-11-06"
}