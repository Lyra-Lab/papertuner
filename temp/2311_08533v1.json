{
  "id": "http://arxiv.org/abs/2311.08533v1",
  "title": "Natural Language Processing for Financial Regulation",
  "authors": [
    "Ixandra Achitouv",
    "Dragos Gorduza",
    "Antoine Jacquier"
  ],
  "abstract": "This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.",
  "text": "Natural Language Processing for Financial\nRegulation\nIxandra Achitouv1*, Dragos Gorduza2† and Antoine Jacquier3†\n1*Department of Mathematics, Imperial College London and CNRS,\nComplex Systems Institute of Paris ˆIle-de-France.\n2 Oxford Man Institute of Quantitative Finance.\n3Department of Mathematics, Imperial College London and the Alan\nTurning Institute.\n*Corresponding author(s). E-mail(s): ixandra.achitouv@cnrs.fr;\n†These authors contributed equally to this work.\nAbstract\nThis article provides an understanding of Natural Language Processing tech-\nniques in the framework of financial regulation, more specifically in order to\nperform semantic matching search between rules and policy when no dataset\nis available for supervised learning. We outline how to outperform simple pre-\ntrained sentences-transformer models using freely available resources and explain\nthe mathematical concepts behind the key building blocks of Natural Language\nProcessing.\nKeywords: Language models, Financial Regulation, Natural Language Processing\n1 Introduction\nOver the past ten years, modern natural language processing models have revolu-\ntionised the field of artificial intelligence, transforming how computers understand and\ngenerate language. ChatGPT is one key example of how AI technology is becoming\nincreasingly important in generating more and more precise human-like responses to\na wide variety of problems, including business ones. These advancements have signifi-\ncant implications for the financial industry, where vast amounts of regulatory data and\npolicies must be analysed and understood in order to comply with laws and regulations.\n1\narXiv:2311.08533v1  [cs.CL]  14 Nov 2023\nIndeed, Natural Language Processing (NLP) has gained traction in financial reg-\nulation as a solution for managing and interpreting complex regulatory information.\nThe ability to effectively process and analyse large amounts of regulatory text is cru-\ncial for compliance officers, risk managers, and other financial professionals. However,\ntraditional methods of automatic text processing including keyword searches and dic-\ntionaries are both inefficient to run at the scale required by the amount of files to\ntreat, and costly to set up since they require system experts to design dictionaries on\na case-by-case basis. Moreover, they are also prone to errors due to lack of coverage\nwhich can expose the user to costly regulatory violations. Modern NLP techniques have\nemerged in recent years, mostly based on deep neural networks and after the seminal\nbreakthrough [1] by Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and\nPolosukhin, who incorporated the attention mechanism to learn latent semantic links\nbetween words in a sentence more accurately. These improvements, along with orders\nof magnitude increases in the availability of datasets and computational power have\nled to large language model (LLM), a type of model known for its ability to achieve\ngeneral-purpose language understanding and generation such as OpenAI’s GPT (used\nin ChatGPT), Meta’s LLaMa or Google’s PaLM. One may however wonder whether\nLLMs are actually required for simple NLP tasks such as semantic search, especially\ndue to their costs, instead of free open resources.\nIn the present work, we perform a semantic search applied to laws and regulation\nusing a so called domain adaptation technique with limited amount of data, bearing in\nmind limited available resources from small and medium enterprises. We also recall the\nhistorical NLP approaches and explain the leading concepts in this field. In particular,\nwe focus on semantic search, which naturally leads to the encoding concepts of a\nsentence. We thus aim to fill the gap between well-known NLP concepts and the\nmathematical frameworks behind.\nThis paper is organised as follows: Section 2 is a short historical review on encoding\nmethods of a sentence into a mathematical object as well as key machine learning\nadvances. In Section 3 we describe the task of semantic textual similarity, the different\ntechniques at hand and illustrate them with pseudo codes. In Section 4, we apply these\ntechniques concretely to financial regulation problems to improve semantic search\nmatching. Section 5 draws conclusion from this analysis.\n2 Historical overview of word embedding approaches\nSemantic vectors of words are high-dimensional vectors, and pairwise distances\nbetween two vectors reflect the similarity of words. We briefly review the main lit-\nerature on this topic, clarifying the notions of ‘similarity’, ‘meaning’ and ‘correlation\nbetween words’.\n2.1 Deterministic approaches based on word counts\nThe roots of major modern NLP techniques can be traced back Firth’ quote [2] ‘You\nshall know a word by the company it keeps’. Word co-occurrence statistic is a simple\ndeterministic approach to obtain a semantic vector representation of words. Latent\n2\nSemantic Analysis (HAL) [3] and Hyperspace Analogue to Language (HAL) [4] are his-\ntorically the two methods that built embedded semantic vectors derived from the\nstatistics of word co-occurrence. LSA derives its vectors from collections of segmented\ndocuments, while HAL makes use of unsegmented text corpora. For segmented docu-\nments, a document-term matrix is used to describe the frequency of terms that occur\nin a collection of documents such that rows correspond to the documents and columns\ncorrespond to the terms. Each (i, j) cell, then, counts the number of times word j\noccurs in document i. Later, Rohde, Gonnerman and Plaut [5] improved the HAL\nmethod introducing the Correlated Occurrence Analogue to Lexical Semantics (COALS)\nalgorithm, which achieves considerably better performance through improvements in\nnormalisation and other algorithmic details.\n2.1.1 Co-occurrence & Singular Value Decomposition\nThe deterministic approach proposed in [5] is one of the first methods to achieve good\nand consistent results in predicting human similarity judgements in words embedding.\nIt is a two-step approach:\nStep 1: produce a co-occurrence matrix. For each word w1, count the number of\ntimes another word w2 occurs in close proximity to w1. Here ’close proximity’ (or ’win-\ndows size’) is a hyperparameter indicating the number of words to consider before/after\nthe central word w1. HAL suggested to use a 4-word window while COALS proposes\n10-word windows. The counting is done using a weighting scheme whereby if w2 is\nadjacent to w1 it receives the maximal weight, and the weight is proportional to the\nlength separation of the central word w1 (ramped window). This results in an N × N\nco-occurrence matrix where N represents the number of words in the whole corpus.\nOne could in principle stop here, where each row of the co-occurrence matrix can\nthus be associated with a word. With this procedure, HAL [4] demonstrated seman-\ntic similarity between any desired pair of words. A few tricks were also proposed in\nthe computation of the co-occurrence matrix, for instance putting a threshold to the\nnumber of co-occurrence count to remove frequent words such as ‘the’ or ‘a’.\nStep 2: Dimension reduction of word vectors. From the co-occurrence count of\nwords the resulting embedded vectors should have at least the size of the words in the\ncorpus1. In HAL, the authors proposed to keep a relatively small number of principal\ncomponents of the co-occurrence matrix. For instance reducing the vector size by\neliminating all but the k columns with the highest variance. In [5] the reduction of\ndimension is based on Singular Value Decomposition (SVD) of the normalised co-\noccurrence matrix (this was also proposed in for latent semantic analysis in [3]).\n2.2 Machine Learning\nOver the last decade, Machine Learning algorithms contribution to NLP have brought\nsignificant progress. Among these, Word2vec in 2013 by Mikolov, Chen, Corrado and\n1In fact it has a 2N dimension in HAL where for every word in the target vocabulary, there is both a row\nand a column containing relevant values. For instance, the row may contain co-occurrence information for\nwords appearing before the word under consideration, while the column contains co-occurrence information\nfor words following it. This (row,column) pair may be concatenated so that, given an N × N co-occurrence\nmatrix, a co-occurrence vector of length 2N is available. In COALS the distinction between left/right of the\ncentral word is ignored.\n3\nDean [6], Glove in 2014 by Pennington, Socher and Manning [7], BERT in 2018 by\nDevlin, Chang, Lee and Toutanova [8] and SBERT in 2019 by Reimers and Gurevych [9]\nare the most popular ones, which we shall use in this project. The key idea is to perform\na non-linear regression where the coefficients are adjusted by minimising a loss function\n(or maximising a likelihood function). In the case of NLP, the coefficients/weights are\nthe embedded vectors.\n2.2.1 Skip-gram and CBOW\nTo understand the Skip-gram model, we briefly review some fundamental concepts of\nprobabilistic language modelling where the goal is to compute the probability P(w) of\na sequence of words w := (w0, . . . , wn) occurring; we also write wi:j := (wi, . . . , wj)\nand omit i whenever i = 0. A similar goal is to compute the conditional probability\nof an upcoming word P(wj|wj−1). The computation of these probabilities rely on\nconditional probabilities where the general chain of rule can be expressed as\nP(w) = P(w0)P(w1|w0)P(w2|w:1) · · · P(wn|w:n−1).\nTo estimate this probability we count from a corpus\nP(wj|w:j−1) =\n#w:j\n#w:j−1\n.\nFor sentences with more than a few words, the probability that a complex sentence\nrepeats itself often enough to be able to compute statistical properties is unlikely. To\novercome this, one usually assumes a pseudo-Markovian approximation (the k-gram\nmodel) 2, whereby P(wj|w:j−1) ≈P(wj|wj−k:j−1) for some k ≤j. For instance for\nk = 1 we get the properly Markov bi-gram model P(wj|w:j−1) ≈P(wj|wj−1). Given\na sentence, computing for each word the probability that it occurs given the previous\nwords is similar to filling a normalised co-occurrence matrix.\nSkip-gram\nmodels\nare\ngeneralisations\nof\nn-gram\nmodels,\nin\nwhich\nthe\ncomponents\ndo\nnot\nneed\nto\nbe\nconsecutive,\nbut\nmay\nleave\ngaps\nthat\nare\nskipped\nover\n[10,\n11].\nFor\ninstance,\nthe\nadjacent\nwords\nfor\nthe\nsentence:\n‘I\nhit\nthe\nblue\nball’\nare\n{I hit, hit the, the blue, blue ball}\nfor\na\nbi-gram\nand\n{I hit, I the, I blue, hit the, hit blue, hit ball, the blue, the ball} for a 2-skip-bi-gram.\nFinally, in Word2vec [12], the definition of the Skip-gram architecture is to predict\nsurrounding words given a specific word, which formally corresponds to computing\nP(wn−j:n+j|wn). while the Continuous Bag of Words (CBOW) architecture predicts the\ncurrent word based on the context: P(wj|w:j−1). These two models are illustrated\nin [12, Figure 1].\n2.2.2 Link with the embedding vectors\nIn the Skip-gram originally presented in [12], the training objective is to learn word\nvector representations that are good at predicting the close-by words w(t −2), w(t −\n2strictly speaking it is not Markovian for k ̸= 1 because we keep a memory of the k previous words\n4\n1), w(t + 1), w(t + 2) from an input word w(t). This is achieved by maximising the\naverage log likelihood probability, equivalently by minimising the loss function\nJ (Θ) := −1\nT\nT\nX\nt=1\nX\n−c≤j≤c,j̸=0\nlog P(wt+j|wt),\n(1)\nwhere c is the size of the training context, (w1, · · · , wT ) is a sequence of training words\nand Θ is the vector representation of words that the algorithm tries to optimise. Note\nthat every word w appears both as a central word (input wI) and as a context word\n(output wO) so that Θ ∈R2w where w is the size of the vocabulary3. In the basic\nSkip-gram formulation, a softmax function is proposed to model the loss function,\nnamely\nP(wO|wI) =\nexp\n\u0000v⊤\nwOvwI\n\u0001\nPw\nw=1 exp (v⊤\nwvwI),\n(2)\nwhere wO and wI are the output and input words, and, given a word w, vwI and vwO\nare its input and output vector representations. The loss function is then minimised via\ngradient descent: the gradient of the likelihood (with respect to vwI) can be computed\neasily and is equal to zero if and only if\nvwO =\n\" w\nX\nw=1\nexp(v⊤\nwvwI)\n#−1\nw\nX\nw=1\nvw exp(v⊤\nwvwI),\n(3)\nwhich is the expectation of vwO occurring given the context word vwI.\nIn practice, to improve computation efficiency, a hierarchical softmax is proposed\nin [12, Equation (3)]), which uses a binary tree representation such that the computing\ntime cost reduces from w to log(w). An alternative technique to improve efficiency\nis to sub-sample k random words and compute their probabilities to appear in the\ncontext of the central words. This is known as negative sampling where log P(wt+j|wt)\nin (1) is replaced by\nlog σ\n\u0000v⊤\nwOvwI\n\u0001\n+\nk\nX\nj=1\nEwj∼Pn(w)\nh\nlog σ\n\u0010\n−v⊤\nwjvwI\n\u0011i\n,\n(4)\nwhere σ(·) is the sigmoid function and the sum runs over a random sample of k\nwords (rather than w) based on their frequencies around the center word. Thus, the\ntask is to distinguish the target word wO from draws from noise distribution Pn(w)\nusing logistic regression. The noise distribution Pn(w) in (4) is a free parameter\nand [12] found that Pn(w) = u(w)3/4/Z, where u(w) is a unigram and Z is just the\nnormalisation constant, outperformed computational efficiency significantly over the\nuniform distributions. This avoids summing over all words in the denominator of (2).\nIn [12], typical values for k lie in the range [5, 20] for small training datasets, while\n3the final embedded word w is simply the average of its input wI and output wO vector representations.\n5\nfor large datasets k can be as small as [2, 5]. An implementation of this model can\nbe found at https://github.com/chrisjmccormick/word2vec commented/blob/master/\nword2vec.c. Improvements of the CBOW and Skip-gram models have been proposed but\nstill rely on the same fundamental idea of co-occurrence statistics of words from [7].\n2.2.3 Transformer and attention mechanism\nAnother major step was archived using multi-layer bidirectional Transformer\nencoders [1] for the training architecture, and was used in BERT [8] (Bidirectional\nEncoder Representations from Transformers). A main difference with Word2vec is\nthat BERT is a non-static model with not just one hidden layer. Another key difference\nis that there is no window-size limitation of n-words in BERT to consider the context\nof the word to encode. The technique employed instead is known as the attention\nmechanism and is the building block of a transformer architecture. Transformers were\noriginally designed for translation and they are now the state of the art architecture\nin NLP. The transformer model architecture can be seen in [1, Figure 1].\nTransformers can be summarised as encoder-decoder tools, where a sentence is fed\nto the encoder and the decoder generates another sentence; the whole system is built by\nstacking several layers or encoder/decoder. Each encoder has two sub-layers, a multi-\nhead self-attention layer and a position-wise fully connected feed-forward network, and\nreceives a list of vectors as input, passes it into a self-attention layer, then into a feed-\nforward neural network, and finally sends out the output upwards to the next encoder.\nThe decoder itselg has tree layers: in addition to the two sub-layers of the encoder\nlayer, the decoder inserts a third sub-layer, which performs multi-head attention over\nthe output of the encoder stack and helps the decoder focus on relevant parts of the\ninput sentence.\nThe Self-attention mechanism is the building block of a Transformer and captures\nthe relationship between the different words in a sentence. All attention mechanisms in\na Transformer are split into independent heads whose outputs are concatenated before\nbeing further processed. A self-attention mechanism computes a score value between\na query with keys, and reweighs the value. All heads’ values will be used through a\nscale dot architecture to update the attention weight matrix. A self-attention head\n(assuming one single head) can summarised as follows [13]:\n• For each word i ∈{1, . . . , L} in a sentence of length L, return its positional embed-\nding vector wi of length de, which is one of the hyperparameters. In [1] the base\ncase is de = 512 while for example in GPT2 it is 768. The size of the embedding\nimpacts the probability that two random embedded words are correlated or not (if\ntheir scalar product is close to ±1 or to 0 respectively).\n• Stack each embedding into a matrix X ∈RL×de. For instance if we consider each\nEnglish word as a token, the sentence ’I hit the blue ball’ has L = 5 tokens, each of\nthem encoded in a 512-dimensional embedding vector.\n• Compute the query, key and value matrices Q, K and V by projecting X in the\n(subspace) representations of the packed queries, keys and values:\nQ := XWq,\nK := XWk,\nV := XWv,\n6\nwhere Wq, Wk, Wv are weights matrices of length de × dw trained during the\ntraining process (randomly initialised) and their dimensions depend on the architec-\nture. For simplicity, in one single-head attention dw = de, while for multiple-head\n(8 in [1]), dw = de/Nhead = 512/8 = 64. Multiple-head attention algorithm split\nmultiple query, key and value ‘heads’ in order to improve the performance of the\nalgorithm. This split reshapes weight matrices into Wi\nq, Wi\nk, Wi\nv, of dimensions\nde × dw. In the base case of [1], dw = 64 was selected after hyperparameter tuning\nas was the length of the embedded vectors de. Larger values of the hyperparam-\neters such as dw or de provide better performance [1, Table 4], but require more\ncomputational resources.\n• Compute a self-attention score of each word of the input sentence against the others,\nby taking the dot product between query and key vectors of the respective words.\nThese scores are then normalised and passed into a softmax function. The resulting\nattention-weighting matrix of size L × L represents the correlation between word i\nand word j. Finally, the attention function or self-attention head takes the attention-\nweighting matrix and multiply it by the value matrix V, defined as\nAttention(Q, K, V) = softmax\n\u0012QK⊤\n√dw\n\u0013\nV,\n(5)\nresulting in an updated vector representation of the contextualised token V.\nThe whole transformer itself uses h attention heads (h = 8 in [1]) and is summarised\nin Algorithm 1 to allow the model to jointly represent information from different\nrepresentation subspaces at different positions. The reweighing “values” from the heads\nare passed through another densely connected layers. The outputs of these attention\nheads are concatenated into a matrix O of dimensions L×de. This matrix is multiplied\nby an output weight matrix Wo of dimension de × de. The result is\neX, an updated\nrepresentation of X the input matrix of the embedded tokens with eX ∈RL×de.\nAlgorithm 1 Multiheaded Attention Layer\nRequire: X, Wq, Wk, Wv, Wo\nfor i = 1 to h do (Split Wq, Wk, and Wv into h heads (Qi, Ki, Vi)i=1,...,h)\nQi ←XWi\nq;\nKi ←XWi\nk;\nVi ←XWi\nv\noutputi ←Attention(Qi, Ki, Vi);\nO ←Concatenate(output1, . . . , outputh)\neX ←OWo\nEnsure: eX\nThe transformer model has been very successful and is at the core of many machine\nlearning applications: for NLP, the BERT family [1], in biology, the Generative Pre-\ntrained Transformer (GPT) family to predict molecular structures [14], in computer\nvision, to extract image information [15]. Recent models differ from their predecessors\nprimarily by their size (BERT has 310 million parameters, GPT3 has 175 billion [16],\n7\nPALM has 540 billion). The other chief difference lies in which part of the encoder-\ndecoder framework of the transformer architecture is leveraged. BERT uses the entire\nencoder-decoder while models like the GPT family use the only decoder. The classical\nBERT architecture is trained on two tasks [8]. The first is sentence entailment, where\nthe input is one sentence and the model is asked to predict whether the question\nproposed as the output is logically entailed by the first or not. The second task is\nMasked Language Modelling, where a random 15% subset of tokens in the entire\ntraining corpus is replaced by a [MASK] token and the BERT model is asked to predict\nthe most likely candidate replacement. Both these pre-training approaches yield a very\npowerful model that can be fine-tuned on a downstream task.\n3 Semantic textual similarity search\nTo perform semantic similarity search, a classical approach would be to feed an input\nsentence or text to the BERT transformer network which produces contextualised word\nembeddings for all input tokens in the text. Then, via a pooling layer (such as mean-\npooling), the average of the contextualised word embeddings would return a fixed-sized\noutput representation vector. In the BERT-base model the dimension is 768; similarly\nto the previous discussion, larger values provides better accuracy but at the cost of\ncomputational memory and time. This sentence/text could be directly compared with\nanother pair using for instance cosine similarity between the two vectors. However this\nwould often yield poor sentence embedding compared to GloVe [9] and to expensive\ncomputational time. For instance in [9], computing the most similar sentence pairs\nfrom 1, 000 sentences took 65 hours with BERT. We have also tried to perform some\nfine-tuning and further train BERT on our corpus and indeed found the computational\ntime to be significantly larger and the semantic search score high even for non matching\n(policy, rule) pair.\nTo palliate this, SBERT (Sentence-BERT) uses a BERT architecture with siamese and\ntriplet networks [17] that is able to derive semantically meaningful sentence embed-\ndings. This adds a pooling operation to the output of BERT and fine-tune the latter\nby adding a training dataset with pairs of sentences. The cosine similarity between a\nquery vector Q and a policy vector P can be defined as\nSc(Q, P) :=\nQ⊤P\n∥Q∥∥P∥,\n(6)\nwhich converges to one as the similarity increases. This allows SBERT networks to\nbe fine-tuned and to recognise the similarity of sentences. Different versions of loss\nfunctions can be used; for example, we later use the all-MiniLM-L6-v2, trained on a\ndataset of over one billion training pairs. These models can be freely accessed using\nSentences Transformers (www.sbert.net) to perform text embeddings directly from a\nlarge collection of pre-trained models tuned for various tasks.\nOur goal here is to link rules issued by an organisation (such as governments or\nfinancial regulators) to financial institutions policies (banks in particular). Financial\nrules are often updated and financial institutions are required to issue new policies\naddressing these changes. It is extremely challenging for large institutions to keep track\n8\nof regular updates and to ensure their policies match the latest rules. Using NLP, we\nperform here semantic search between query sentences (rules) and answers (policies)\nwritten in a corpus as follows:\n• Clean corpus and rules from meaningless symbols (bracket, hashtag, special\ncharacter, ...).\n• For each query, split the corpus into sentences or paragraphs, depending on the data\nand the query size. Sentence-by-sentence comparison similarity will be more precise\nthan pooling many sentences into paragraphs.\n• Use a pre-trained model such as BERT to tokenise each word of the sentence, and map\nsentences to embedded vectors, usually via mean-pooling of each embedded token.\nThe size of any embedded token depends on the pre-trained model. One can also\nuse pre-trained model such as all-MiniLM-L6-v2 and embed the sentence directly\nusing sentence-transformer library.\n• Use cosine similarity (6) as a metric between an embedded rule query and the\nembedded policies to find the best match. By requiring that a match is positive\nabove a threshold (say Sc > 0.7), we can easily extract from a policy corpus which\nsentences match the rule (if any).\nThis powerful pipeline can however be improved. Using pre-trained models to encode\nour sentences to vectors may sound sensible, but they are trained on generic corpus\nand are not familiar with domain-specific targets–such as financial regulation–with\nhigh accuracy. One may instead completely re-train BERT on a domain-specific corpus,\nas done in [18]; this however requires huge amount of training data (3.3 billion tokens),\nwhich is extremely costly. Instead–and this is the state of the art– training can be\nperformed using an Adaptative Pre-Training, described at www.sbert.net; this was\nsuccessfully applied in various domains, such as for the bio-medical language [19]. It\ncan then be archived with Sentence-Transformers frameworks and the Hugging Face\nTransformers library, specifically built for NLP applications huggingface.co.\n3.1 Classical Domain Adaptation (DA) Pre-Training method\nThe DA Pre-Training method can be split into two steps:\n• Step 1: further train a pre-trained model for words embeddings such as BERT, SBERT\nor all-MiniLM-L6-v2 using a context specific corpus.\n• Step 2: fine-tune the resulting model based on an existing training dataset (such as\npaired sentences), similarly to SBERT but on the domain-specific dataset.\nBefore performing Step 1, one can also add up domain-specific tokens to the exist-\ning model, although given the limited amount of datasets at hand, this may not always\nbe possible. To perform Step 1, known as Pre-Training on Target Domain, we use the\nMask Language Modelling (MLM) approach [20], which masks a random fraction of\ntokens in a sentence and the transformer then tries to guess what is missing. Alter-\nnative approaches include Transformer-based Denoising AutoEncoder (TSDAE) [20].\nWhile MLM outputs a token vector (mask word), the TSDAE encoder is fed with\nnoisy sentences which the decoder uses to predict the full original sentences. To per-\nform Step 2, we need to specify the loss function and the training set. Many generic\n9\ntraining sets with labelled paired sentences are available (huggingface.co/datasets),\nbut we do not use them since wish to target our fine-tuning on (rules, policies) pairs.\nFor the loss function, we use the Multiple Negatives Ranking (MNR) Loss detailed\nin [21, Section 4] and reviewed below.\nThe idea of Multiple Negatives Ranking (MNR), introduced in [21], is to suggest\nresponses to a question embedded in a vector X maintaining some kind of mem-\nory throughout the course of a dialogue, which translates to finding the probability\nP(Y|X), where Y is the embedded vector answer. This probability is used to rank\npossible answers Y given an input question X. Bayes’ theorem and the total law of\nprobability indicate that this probability can be expressed as\nP(Y|X) =\nP(X, Y)\nP\nk P(X, Yk),\n(7)\nand the joint probability P(X, Y) is estimated using a neural network scoring\nfunction S such that\nbP(X, Y) := exp {S(X, Y)} .\nIn practice the denominator in (7) (equal to P(X)) is approximated by sampling K\nresponses from the training corpus with bP(X) := PK\nk=1 P(X, Yk), leading to the\napproximate probability used to train the neural network:\nbP(Y|X) :=\nexp {S(X, Y)}\nPK\nk=1 exp {S(X, Yk)}\n.\nHaving a training dataset with (question, answer) pairs, each embedded question Xi\nis paired with the embedded answer Yi and all other Yj for i ̸= j is treated as a\nnegative candidate for question Xi (so K −1 negatives). The goal of the training is\nto minimise the loss function\nJ (X, Y, Θ) := −1\nK\nK\nX\ni=1\nlog bP(Yi|Xi),\nwhere Θ gathers the word embeddings and the neural network parameters. To com-\npute the score, the authors in [21] represent the input question tokenised into a word\nsequence X and responses Y as fixed-dimensional input features, extracting n-gram\nfeatures from each. During training, they learn a d-dimensional embedding for each\nn-gram jointly with the other neural network parameters. To represent sequences of\nwords, they combine n-gram embeddings by summing their values. This bag of n-\ngrams representation is denoted as Ψ(X) ∈Rd. Then a feedforward scoring model\ntakes the n-gram representation of a question and a response, and computes a score.\nFor instance in the dot-product architecture from [21, Figure 3], Ψ(X) and Ψ(Y)\ngo to two separate tanh activation layers returning the encoded X and Y as hx, hy\nand performing S(X, Y) = h⊤\nx hy. Using the MNR Loss, the score is computed using\nat first the chosen pre-trained model and the default scoring is computed via cosine\n10\nsimilarity in (6).4 The key issue and challenge for this work lies in the absence of a\ntraining dataset for policies and rules. While unsupervised text embedding learning\nperforms rather poorly in learning on domain-specific concepts without fine tuning, we\ntested an approach to perform an Adaptative Pre-Training without using the training\ndataset we generated in Section 4.1. This method generates pseudo-labeling pairs for\nfine tuning and we describe it in the next section.\n3.2 Domain Adaptation with Generative Pseudo-Labeling\nAlternatively to the approach described above, we also tested the unsupervised fine-\ntuning with Generative Pseudo-Labeling (GPL) [22]. This method can be used to\nperform Step 2 from Section 3.1. In our context we wish to investigate how it improves\nStep 2 over a fine-tuning fitting method that uses labelled (rules, policies) pairs. The\nidea behind GPL can be condensed into four steps, that we now describe. Below, we\nshall use calligraphic letters to denote sets (of paragraphs for example) as opposed to\nstandard letters for elements, for example P−:= {P −\n1 , P −\n2 , . . .}.\n3.2.1 GPL Step 1: Generating queries from a domain specific corpus\nFor each paragraph of the corpus, we generate queries using a T5-encoder-decoder\nmodel [23] similarly to the architecture of [1] discussed in Section 2.2.2. This query\ngenerator, which can easily be called again from Sentences-Transformer models, yields\npositive (query, answer) pairs and is summarised in Algorithm 2.\nAlgorithm 2 Generator\nRequire: input paragraph P +, vocabulary size V , length of the output query\nparagraph Lq\nG ←T5(P +) (G ∈RV ×Lq: matrix of probability distributions)\nQ ←(arg max(Gi))i=1,...Lq\nQ ←Decode(Q)\nEnsure: (Q,P +)\n3.2.2 GPL Step 2: Negative mining using dense retrieval\nThis step finds sentences in the text that share many similar words. For each query or\nparagraph, there is now a positive pair. In addition, for each of the generated queries,\nGPL retrieves M negative passages that are similar but not a match to the query,\nwhich is known as a negative mining process. At the end of this step, we obtain triplets\nof positive and negative passages associated with a query. This step makes use of a\nfunction DenseRetrieval which is detailed in Algorithm 3.\n4see\ngithub.com/UKPLab/sentence-transformers/blob/master/sentence transformers/losses/\nMultipleNegativesRankingLoss.py for details\n11\nAlgorithm 3 Dense Retrieval\nRequire: Remaining set P = {P1, . . . , P|P|} \\ {P +} of paragraphs; user query Q;\nM ≤|P|: number of negative passages;\nP−, D ←{}, {}\nE ←(T5(Pi))i=1,...,|P|\n(Calculate an embedding E for each paragraph)\nEQ ←T5(Q)\n(Calculate an embedding EQ for from the query Q)\nD ←(Sc(Ei, EQ), Ei)i=1,...|E|\n(with Sc(·) defined in (6))\nD ←sort(D) (Sort by cosine similarity)\nP−←(D1, . . . , DM)\nEnsure: (Q, P +, P−)\n3.2.3 GPL Step 3: Pseudo labelling\nUse a crossencoder [24] to score all query-positive passage and query-negative passages.\nNote that some negative passage might actually be positive pairs. This step scores\nthe triplets composed of our initial paragraph P +, our generated query based on the\ninitial paragraph Q, one output P −\ni\nfrom our set of negative samples P−.\nAlgorithm 4 Pseudo Labelling\nRequire: (Q, P +, P−), crossencoder, M\nfor i = 1 to M do\nmargini ←crossencoder(Q, P +) −crossencoder(Q, P −\ni )\nEnsure: (Q, P +, P −\ni , margini)i=1,...,M\n3.2.4 GPL Step 4: Training/Tuning the Transformer model to\nidentify the differences between positive and negative\npassages, using a crossencoder model\nThe latter compare the embeddings of the passages by generating similarity scores for\nboth positive and negative pairs. Given a transformer model that we wish to train using\nGPL and the output tuples from the previous step, we will train the said transformer\nby backpropagation between the margin computed in Algorithm 4 and the predicted\nmargin we extract. This process is optimised using an MSE loss function applied to\nthe margin\nSMSE :=\n\f\f\fX⊤\nQ\n\u0010\nXP + −XP−\ni\n\u0011\f\f\f ,\n(8)\nwith XQ, XP +, XP−\ni the embedded vectors of the query, positive passage, and negative\npassage i.\n3.3 To summarise\nBoth supervised and unsupervised domain adaptation methods use a pre-trained\nmodel and further train it by comparing (query, answer) pairs or triplets (negative\nanswer). These pairs and triplets are in the classical supervised method given by the\n12\nAlgorithm 5 Training loop of Transformer using GPL\nRequire: (Q, P +, P −\ni , margini)i=1,...,M\nfor i = 1 to M do\n(XQ, XP +, XP −\ni ) ←Transformer(Q, P +, P −\ni )\npredictedsim+ ←Sc(XQ, XP +)\n(with Sc(·) defined in (6))\npredictedsim−\ni ←Sc(XQ, XP −\ni )\npredictedmargini ←predictedsim+ −predictedsim−\ni\nLossi ←SMSE(margini, predictedmargini) defined in (8))\nOutputi ←backpropagate(Lossi)\nEnsure: Output\ndataset used to perform the domain adaptation (corpus of a specific domain), but it\nmay also be generated. That is the GPL approach for a fully unsupervised domain\nadaptation. Below, we explore the results of performing an Adaptative Pre-Training\nmethod and a purely unsupervised approach, using GPL. To compare our results and\nto perform supervised fine-tuning we need a pseudo training/validation dataset. We\ndescribe in the next section how we create it.\n4 Application to semantic matching for financial\nregulation\n4.1 Creation of a pseudo training and validation dataset\nTo create our validation dataset we use a mean assemble average of N pre-trained mod-\nels (in what follows N=10)5 for the task of semantic search between rules and policies.\nWe run these N sentence-transforming models on a catalogue of rules and financial\npolicies provided by FinregE (https://finreg-e.com), a company providing clients in\nfinancial services with a software focusing on current and future regulations, using\nML and AI tools to identify and interpret regulatory requirements and to integrate\ncompliance workflows for action and compliance management.\nThe dataset of rules is publicly available from Financial Conduct Authority (FCA)\nRulebook 2022. After cleaning and splitting the rules into sentences of length 200, we\nobtain about 50,000 sentences, composed of 21,914 rule IDs. The policies’ dataset was\nobtained from FinregE and is composed of 2,374 policies that may not cover all of the\nnewly edited FCAR rulesOur approach is the following:\n• For each model, we keep the rule/policy match sentences if the cosine similarity (6)\nis above 0.7. This is an empirical choice that provides fair matches.\n• We keep pairs that have been identified by a number of models greater than\n√\nN.\nGiven that the standard deviation of shot noise is equal to the square root of the\n5multi-qa-mpnet-base-cos-v1, sentence-t5-xl, multi-qa-distilbert-cos-v1, msmarco-bert-base-dot-v5,\nall-distilroberta-v1,\nall-MiniLM-L12-v2,\ndistiluse-base-multilingual-cased-v2,\nall-mpnet-base-v2,\nstsb-distilbert-base, bert-base-nli-mean-tokens. These models can be found at www.sbert.net/docs/\npretrained models.html\n13\nFig. 1 Sample of the semantic text search output between rules and policies. The score corresponds\nto the percentage of models that have identified the same match.\naverage number of events N, hence if models are uncorrelated, selecting a signal to\nnoise match above one sigma should correspond to a\n√\nN cut.\nWe end up with 1, 760 matching (rule, policy) pairs out of which we keep 1, 408 pairs\nto fine-tune our model (Section 4.2) and 352 for the validation dataset. In Figure 1 we\ncan see a sample of our validation dataset results. This dataset will be good to identify\npairs that share high similarities but will not be able to pick up on subtle pairs. This\nis why it is not an unbiased dataset for fine-tuning purposes but we proceed anyway\ndue to the absence of hand labelled pairing between rules and policies.\n4.2 Domain Adaptation Pre-Training method\nThe\ncorpus\nwe\nuse\nfor\nfurther\npre-training\n(Section\n3.1-Step\n1)\nare\nrules\nfrom\nthe\nFCA\nRulebook.\nWe\nuse\na\nfull\nword\nmask\nand\nwe\nuse\nthe\nDataCollatorForLanguageModeling which is a class of transformers to perform the\nMLM training6. Then we run Step 2 of Section 3.1 with the MNR loss using our train-\ning dataset matching (rule, policy) pairs. We perform Step 1 and Step 2 using both\nBERT and all-MiniLM-L6-v2 pre-train model to further train in our domain-specific\ncorpus.\n4.2.1 Results\nIn the absence of an unbiased validation dataset, it is difficult to perform proper model\ncomparison. We do so by rating two scores.\n• Score 1: margin between matching and random pairs. For all the rules in our valida-\ntion datasets we use the model considered to compute the cosine similarity Sc(Ri, Pi)\nbetween rule i and policy i (the matching pair) and the cosine similarity Sc(Ri, Pj)\n6see\nhuggingface.co/docs/transformers/main classes/data collator\nand\nwww.sbert.net/examples/\nunsupervised learning/MLM/README.html\n14\nbetween rule i and a random policy j, with j ̸= i. Then we compute\nS1 := 1\nN\nN\nX\ni=1\nn\nSc(Ri, Pi) −Sc(Ri, Pj)\no\n,\n(9)\nwhere N is the length of the validation dataset (here N = 352).\n• Score 2: For each rule Ri, we compute the highest similarity scoring policy P ∗\ni :=\narg maxj=1,...,N Sc(Ri, Pj). If P ∗\ni = Pi, we then increment S2 by one and then divide\nby N to get the fraction of ‘correct’ matches per model, namely\nS2 := 1\nN\nN\nX\ni=1\n11{Pi=arg maxj=1,...,N Sc(Ri,Pj)}.\nFor a perfectly tuned model and for an unambiguous validation dataset, S1 and S2\nshould be close to 1 since Sc(Ri, Pi) approaches 1 and Sc(Ri, Pj) approaches zero. In\npractice, since we split rules and policies in sentences, it may happen that the same\nsentence occurs in different policies. Therefore a match between Ri and Pj (with j ̸= i)\nis not necessarily a wrong match. In addition in each rule, some sentences are quite\ngeneric and are addressed by many policies. Our score does not account for these and\nis therefore just a benchmark to test the impact of the domain adaptation and not an\nactual score of the quality of semantic matching. Using BERT as the initial pre-trained\nmodel our results are shown in Table 1.\nTable 1 Result for the Domain adaptation\nwith BERT and steps defined in Section 3.1\nModel\nScore 1\nScore 2\nRegular BERT\n0.09\n0.32\nBERT + Step 1\n0.10\n0.33\nBERT + Step 1 & 2\n0.32\n0.33\nThe key improvement of the Domain Adaptation comes from fine-tuning Step 2\nalthough we know that regular BERT is not really suited to perform semantic match-\ning. In fact the low Score 1 for regular BERT comes from the fact that both positive\nand negative matches are high. Only fine-tuning can help BERT distinguish the nega-\ntive pairing. That is why we also tested the all-MiniLM-L6-v2 model using it as is,\nperformed Step 1 and Step 2, and quote the improvement of performing an adaptive\npre-training method defined as score fine-tune−score baseline\nscore baseline\n.\nTable 2 Result for the Domain adaptation with\nall-MiniLM-L6-v2 and steps defined in Section 3.1\nModel\nScore 1\nScore 2\nall-MiniLM-L6-v2\n0.21\n0.46\nall-MiniLM-L6-v2 + Step 1 & 2\n0.27\n0.56\nImprovement\n29%\n22%\n15\nThis shows again the improvement over the baseline. In addition we tested the\nimpact of the quality of the training/validation dataset by decreasing the score thresh-\nold from 0.7 down to 0.6 when generating the training/validation dataset in Section 4.1\nand found that it decreases both scores by 10%. Finally we find that the fraction\nof masked words in the Mask Language Modelling does not impact significantly the\nresults. By default the fraction is set to 0.15 and we tried 0.2 in the BERT Step 1 and\ndid not find a change in the scoring. Overall, using limited data for Step 1 and Step 2\nwe find that the key effect comes from the fine-tuning as was previously stated in [9].\n4.3 Fully unsupervised training with the GPL method\nThis time we perform fine tuning solely by running the GPL method on the FCA\nRulebook corpus and train the GPL method on 10,000 and 20,000 triplets. To do so we\nfollow the steps described in github.com/UKPLab/gpl using the default pre-trained\nmodel MSMARCO. We output samples at each step to check the pseudo-labelling. On\nStep 1, to generate queries from a passage of the corpus to get positive pairs, such as\nExample 1.\nSelected passage: ‘The investor providing the capital may choose not to be involved in\nthe running of the venture’.\nGenerated queries:\n• how are investors involved in a venture\n• who is involved in the running of the venture\n• what are the investors of a venture\nExample 2.\nSelected passage: ‘If a circular submitted for approval is amended a copy of amended\ndrafts must be resubmitted marked to show changes made to conform with FCA com-\nments and to indicate other changes’.\nGenerated queries:\n• how to amend FCA circular\n• what must FCA amendments show\n• how to revise circular for approval\nClearly some of these positive pairs can be noisy. A sample of our generated training\ndatasets containing our triplets of query, positive, negative passages is displayed in\nFigure 2.\n4.3.1 Results\nSimilarly to the scoring analysis in Section 4.2, we see that the GPL fine-tuning does\nnot impact the scores as much as the adaptative pre-training method. The scorings\nthemselves are again to be considered as a benchmark from the no-fine-tuning scor-\ning which translate into an improvement of the ratio\nscore fine-tune−score baseline\nscore baseline\n. As\nexpected, GPL fine-tuning is not as efficient as mapping rules to policies, most likely\n16\nFig. 2 Sample of the training dataset generated by the GPL approach from the FCA Rulebook.\nThe Margin is defined in (8). This data is used to fine-tune the msmarco pre-trained model.\nbecause it did not perform the training over a set of policies but solely on rules. We\nalso find that performing the training on 10,000 or 20,000 triplets does not impact the\nresults by more than a percent.\nTable 3 Result for the GPL fine-tuning\non the pre-trained MSMARCO model.\nModel\nScore 1\nScore 2\nmsmarco\n0.14\n0.79\nmsmarco + GPL\n0.15\n0.84\nImprovement\n7%\n6%\n5 Discussion and comparison between GPL and\nAdaptative pre-training\nWhen looking at the validation dataset for the rule and policy match as well as the\npolicy identifies by the Adaptative pre-training (DA) and the GPL fine-tuning, there\nis no clear way to identify which pairing is the more accurate. In fact they all return\nmeaningful pairings. In Figure 3 we show a sample where either the DA or the GPL\npolicy match does not agree with the validation policy. It seems that the validation\npolicy is not always the best match to address the rule. In addition, not all the rules\nhave been addressed in our policy sample. Therefore it is not possible to conclude\nwhich approach performs the best matching to our rules. The three approaches we\ninvestigate in this work are:\n• Mean ensemble of N pre-trained models selecting policies if they are returned by at\nleast\n√\nN models with cosine similarity > 0.7. This is our validation policy defined\nin Section 4.1.\n• Model from an Adaptative pre-training (DA), defined in Section 3.1\n• Model from GPL fully unsupervised training from rules corpus in Section 4.3.\n17\nFig. 3 Sample comparison of pairing based on cosine similarity from our pseudo validation dataset\nwhere we add the best match of the DA and GPL approaches.\nIt is likely that a combination of these three approaches should provide the best\npairing. Overall it is quite remarkable that even in the absence of a training dataset,\nwe manage to perform an Adaptative pre-training that improves the scores. This can\nbe helpful as an alternative or a complement to the GPL approach.\nAcknowledgements\nThe authors would like to thank Rohini Gupta and Amit Madhar from Finreg-E\n(https://finreg-e.com), without whom this project would not have been possible. IA\nand AJ are supported by the Innovate UK Smart Grant ‘Finreg-E / Natural Language\nProcessing for Financial Regulation’. DG is supported by the ESRC Grand Union\nDoctoral Training Partnership Grant and the Oxford Man Institute of Quantitative\nFinance.\nReferences\n[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser,\nL., Polosukhin, I.: Attention is all you need. Advances in NeurIPS 30 (2017)\n18\n[2] Firsth, J.R.: Studies in Linguistic Analysis. Blackwell, Oxford (1957)\n[3] Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R.:\nIndexing by latent semantic analysis. Journal of the American Society for\nInformation Science 41(6), 391–407 (1990)\n[4] Lund, K., Burgess, C.: Producing high-dimensional semantic spaces from lexical\nco-occurrence. Behavior Research Methods, Instruments, & Computers 28(2),\n203–208 (1996)\n[5] Rohde, D.L., Gonnerman, L.M., Plaut, D.C.: An improved model of semantic\nsimilarity based on lexical co-occurrence. Communications of the ACM 8(627-\n633), 116 (2006)\n[6] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word\nrepresentations in vector space. arXiv:1301.3781 (2013)\n[7] Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-\nsentation. In: Proceedings of the 2014 Conference on Empirical Methods in NLP,\npp. 1532–1543 (2014)\n[8] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, vol. 1, pp. 4171–4186 (2018)\n[9] Reimers, N., Gurevych, I.: Sentence-Bert: Sentence embeddings using Siamese\nBert-networks. arXiv:1908.10084 (2019)\n[10] Rosenfeld, R.: A maximum entropy approach to adaptive statistical language\nmodelling. Computer Speech & Language 10, 187–228 (1996)\n[11] Guthrie, D., Allison, B., Liu, W., Guthrie, L., Wilks, Y.: A closer look at\nskip-gram modelling. In: Proceedings of the Fifth International Conference on\nLanguage Resources and Evaluation, vol. 6, pp. 1222–1225 (2006)\n[12] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-\nsentations of words and phrases and their compositionality. Advances in NeurIPS\n26 (2013)\n[13] Phuong, M., Hutter, M.: Formal algorithms for transformers. arXiv:2207.09238\n(2022)\n[14] Maziarka, L., Danel, T., Mucha, S., Rataj, K., Tabor, J., Jastrzebski, S.: Molecule\nattention transformer. arXiv:2002.08264 (2002)\n[15] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\n19\n16×16 words: Transformers for image recognition at scale. arXiv:2010.11929\n(2010)\n[16] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,\nHesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are\nfew-shot learners. Advances in NeurIPS 33, 1877–1901 (2020)\n[17] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face\nrecognition and clustering. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 815–823 (2015)\n[18] Beltagy, I., Lo, K., Cohan, A.: Scibert: A pretrained language model for scientific\ntext. In: Proceedings of the 2019 Conference on Empirical Methods in NLP, pp.\n3615–3620 (2019)\n[19] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: BioBert: a\npre-trained biomedical language representation model for biomedical text mining.\nBioinformatics 36(4), 1234–1240 (2020)\n[20] Wang, K., Reimers, N., Gurevych, I.: Tsdae: Using transformer-based sequen-\ntial denoising auto-encoder for unsupervised sentence embedding learning. In:\nFindings of the Association for Computational Linguistics, pp. 671–688 (2021)\n[21] Henderson, M., Al-Rfou, R., Strope, B., Sung, Y.-H., Luk´acs, L., Guo, R., Kumar,\nS., Miklos, B., Kurzweil, R.: Efficient natural language response suggestion for\nsmart reply. arXiv:1705.00652 (2017)\n[22] Wang, K., Thakur, N., Reimers, N., Gurevych, I.: GPL: Generative pseudo label-\ning for unsupervised domain adaptation of dense retrieval. In: Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 2345–2360 (2022)\n[23] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,\nLi, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-\ntext transformer. The Journal of Machine Learning Research 21(1), 5485–5551\n(2020)\n[24] Humeau, S., Shuster, K., Lachaux, M.-A., Weston, J.: Poly-encoders: Transformer\narchitectures and pre-training strategies for fast and accurate multi-sentence\nscoring. arXiv:1905.01969 (2019)\n20\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "q-fin.CP"
  ],
  "published": "2023-11-14",
  "updated": "2023-11-14"
}