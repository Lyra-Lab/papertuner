{
  "id": "http://arxiv.org/abs/2311.08972v2",
  "title": "Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging",
  "authors": [
    "Marcello Carioni",
    "Subhadip Mukherjee",
    "Hong Ye Tan",
    "Junqi Tang"
  ],
  "abstract": "Unsupervised deep learning approaches have recently become one of the crucial\nresearch areas in imaging owing to their ability to learn expressive and\npowerful reconstruction operators even when paired high-quality training data\nis scarcely available. In this chapter, we review theoretically principled\nunsupervised learning schemes for solving imaging inverse problems, with a\nparticular focus on methods rooted in optimal transport and convex analysis. We\nbegin by reviewing the optimal transport-based unsupervised approaches such as\nthe cycle-consistency-based models and learned adversarial regularization\nmethods, which have clear probabilistic interpretations. Subsequently, we give\nan overview of a recent line of works on provably convergent learned\noptimization algorithms applied to accelerate the solution of imaging inverse\nproblems, alongside their dedicated unsupervised training schemes. We also\nsurvey a number of provably convergent plug-and-play algorithms (based on\ngradient-step deep denoisers), which are among the most important and widely\napplied unsupervised approaches for imaging problems. At the end of this\nsurvey, we provide an overview of a few related unsupervised learning\nframeworks that complement our focused schemes. Together with a detailed\nsurvey, we provide an overview of the key mathematical results that underlie\nthe methods reviewed in the chapter to keep our discussion self-contained.",
  "text": "Unsupervised approaches based on optimal transport and\nconvex analysis for inverse problems in imaging\nMarcello Carioni\nm.c.carioni@utwente.nl\nDepartment of Applied Mathematics\nUniversity of Twente, Enschede, Netherlands\nSubhadip Mukherjee\nsmukherjee@ece.iitkgp.ac.in\nDepartment of Electronics and Electrical Communication Engineering\nIndian Institute of Technology (IIT), Kharagpur, India\nHong Ye Tan\nhyt35@cam.ac.uk\nDepartment of Applied Mathematics and Theoretical Physics\nUniversity of Cambridge, United Kingdom\nJunqi Tang\nj.tang.2@bham.ac.uk\nSchool of Mathematics\nUniversity of Birmingham, United Kingdom\nAbstract\nUnsupervised deep learning approaches have recently become one of the crucial\nresearch areas in imaging owing to their ability to learn expressive and powerful\nreconstruction operators even when paired high-quality training data is scarcely\navailable. In this chapter, we review theoretically principled unsupervised learning\nschemes for solving imaging inverse problems, with a particular focus on methods\nrooted in optimal transport and convex analysis. We begin by reviewing the optimal\ntransport-based unsupervised approaches such as the cycle-consistency-based models\nand learned adversarial regularization methods, which have clear probabilistic inter-\npretations. Subsequently, we give an overview of a recent line of works on provably\nconvergent learned optimization algorithms applied to accelerate the solution of\nimaging inverse problems, alongside their dedicated unsupervised training schemes.\nWe also survey a number of provably convergent plug-and-play algorithms (based\non gradient-step deep denoisers), which are among the most important and widely\napplied unsupervised approaches for imaging problems. At the end of this survey,\nwe provide an overview of a few related unsupervised learning frameworks that\ncomplement our focused schemes. Together with a detailed survey, we provide an\noverview of the key mathematical results that underlie the methods reviewed in the\nchapter to keep our discussion self-contained.\n1\nIntroduction\nInverse problems seek to estimate an unknown parameter x∗∈X from a degraded measurement of\nthe form\ny = Ax∗+ w ∈Y,\n(1)\n1\narXiv:2311.08972v2  [cs.CV]  29 Nov 2023\nwhere w represents measurement error (noise) and A : X →Y is an operator that encodes the\nphysical phenomenon governing the data acquisition process. Throughout this chapter, we will\nconsider linear inverse problems, where A is a bounded linear operator between two normed vector\nspaces X and Y. Inverse problems are ubiquitous in imaging applications, especially in medical\nimage reconstruction. A classic example of an inverse problem is image recovery in X-ray computed\ntomography (CT), where X is a set of functions on R3 (or on a subset of R3). The tomographic\nmeasurement data in the absence of noise is given by line integrals of the form\ny(ℓ) =\nZ\nℓ\nx∗(z) dz, where ℓ∈L.\n(2)\nHere, L represents a pre-specified set of lines in R3. In other words, the measurement in X-ray\nCT consists of projections along a set of lines determined by the acquisition geometry, and the\ncorresponding inverse problem seeks to recover the underlying true image x∗. Other notable examples\nof imaging inverse problems include magnetic resonance imaging (MRI), super-resolution microscopy,\ninpainting, image deblurring, compressed sensing, etc.\nWithout any further information about x∗, inverse problems are generally ill-posed, meaning that,\nthere could be either no solutions or several possible solutions x satisfying the operator equation\n(1), even without any measurement noise. In the classical function-analytic setting, the underlying\nimage x∗is assumed to be an unknown deterministic parameter, and the noise w is assumed to be\nbounded, i.e., ∥w∥Y ≤δ for some δ > 0. The task is to then construct a family of reconstruction\noperators Gλ : Y →X, parameterized by λ, such that Gλ(y) yields a reasonable approximation\nof x∗. Variational regularization has by far been the most popular approach to construct such\nreconstruction maps by defining them as a minimizer of a variational energy function:\nGλ(y) ∈arg min\nx∈X\nf(Ax, y) + Rλ(x).\n(3)\nHere, f : Y×Y →R+ measures data fidelity and Rλ : X →R is a regularization function (regularizer\nin short), parameterized by λ, that encodes prior knowledge about the reconstructed image. A\npopular choice is to construct the regularizer as Rλ(x) = λ R(x), where R is a fixed regularizer and\nλ ∈R+ is a penalty parameter balancing data fidelity and regularization. The classical regularization\ntheory for inverse problems deals with the construction of regularizers Rλ such that Gλ(y) varies\ncontinuously in y (stability), and that there exists a parameter selection rule λ : δ 7→λ(δ) such\nthat as the noise level δ →0, Gλ(δ)(y) converges to a generalized solution of the noiseless operator\nequation y0 = Ax, where y0 denotes the noise-free measurement. Such a family of reconstruction\noperators (Gλ(δ))δ>0 is said to be a convergent regularization scheme [11].\nAn alternative modeling approach for inverse problems is offered by the Bayesian framework, wherein\na possible image x and its measurement y are treated as realizations of the X- and Y-valued random\nvariables x and y, respectively. The goal of Bayesian inversion is to characterize the full posterior\ndistribution ppost of x conditioned on y by utilizing Bayes’ formula\nppost(x|y) =\n1\nZ(y) pw(y −Ax) p0(x),\nwhere p0 is the prior probability density on x and Z(y) is a normalizing constant. The data\nlikelihood is specified through the distribution pw of the noise and the forward operator A. If p0\nis a Gibbs prior of the form p0(x) ∝exp(−Rλ(x)), the maximum a-posteriori probability (MAP)\nestimate of x leads to a variational optimization of the form (3) akin to the function-analytic setting.\n2\nIt is worth emphasizing that the Bayesian approach can, in principle, go beyond point estimation\nand allow for uncertainty quantification in the solution using the complete posterior distribution\n(albeit with higher computational complexity). In the context of Bayesian inversion, the notion\nof stability refers to the continuity (with respect to y) of the posterior, while convergence in the\nBayesian framework deals with the concentration of the posterior measured in a suitable metric [24].\nIn the classical model-driven variational approach, the regularizer (or, equivalently, the prior in\nthe Bayesian setting) is constructed analytically to promote certain smoothness properties in\nthe underlying image. Some notable choices for the regularizer include Tikhonov regularization\n(Rλ(x) = λ ∥Bx∥2\n2, where B is a bounded linear functional), the total variation (TV) regularizer\n(Rλ(x) = λ ∥∇x∥1), and more recently, sparsity-promoting regularizers (seeking to encourage the\nimage to be sparse in a fixed or learned basis) [82]. While model-driven approaches for inverse\nproblems have been studied extensively over the past few decades, the success of deep learning\nhas led to the emergence of data-driven methods for solving imaging inverse problems in recent\nyears. These methods not only surpass the classical model-driven approaches in terms of empirical\nperformance, but some of the data-driven methods also come with theoretical guarantees (see [66]\nand references therein). The data-driven methods can broadly be classified into two categories,\nnamely, supervised and unsupervised. Roughly speaking, supervised methods work in an end-to-end\nmanner and need access to the ground-truth images to be compared against the output of a learned\nreconstruction operator, as opposed to unsupervised methods which do not rely on the availability of\nsuch ground-truths for a direct point-by-point comparison with the learned reconstruction. Therefore,\nunsupervised methods offer greater flexibility over supervised approaches in terms of the required\ntraining dataset for learning the parameters of the reconstruction operator, thereby leading to better\npractical usability.\nThe objective of this chapter is to provide a survey of learned unsupervised methods for inverse\nproblems, focusing particularly on approaches that leverage ideas from generative machine learning\n(and optimal transport, in particular) and classical (convex) optimization theory. We first provide\nan extensive mathematical background on optimal transport and convex analysis highlighting the\nimportant concepts that underlie such approaches, followed by a detailed review of the notable\nunsupervised approaches in the context of imaging inverse problems. The survey aims to highlight\nthe key mathematical foundations behind the development of unsupervised learning approaches and\nunderscores the potential of unsupervised methods in achieving competitive empirical performance\nas compared to their supervised counterparts.\nOutline of the chapter. The chapter is organized as follows. Section 2 provides the necessary\nmathematical background that will be used throughout the chapter, from optimal transport to\nconvex analysis. In this section, we also describe classical methods for learning reconstruction\noperators for imaging inverse problems, while also highlighting the key differences between supervised\nand unsupervised approaches. Section 3 presents recent unsupervised approaches to inverse problems\nbased on optimal transport, focusing particularly on cycle-consistency-based models and learned\nadversarial regularizers. Section 4 surveys various unsupervised regularization-based approaches to\ninverse problems, with methods ranging from learning optimizers for model-based reconstruction\nto Plug-and-Play (PnP) denoising methods based on implicitly defined denoising priors. Finally,\nin Section 5, we review some notable ground-truth-free approaches for image reconstruction ap-\nproaches that have been shown to result in impressive empirical performance in numerous practical\napplications.\n3\n2\nBackground\nIn this section, we provide the necessary mathematical background needed in the remainder of\nthe chapter to make the exposition self-contained as much as possible. In particular, we provide a\ndetailed overview of some of the important results in optimal transport and convex analysis, which\nserve as the conceptual foundation of the unsupervised techniques that we subsequently review in\nthis chapter. Further, we also precisely characterize what we mean by supervised and unsupervised\nlearning approaches, considering the vagueness around how these terms can possibly be interpreted.\n2.1\nProbability measures\nSince the data-driven methods reviewed in this article depend heavily on approximating unknown\nprobability measures, we give a formal overview of the key definitions and results in probability\ntheory that will be useful for us. In particular, we formally define some key concepts related to\nprobability measures and random variables, followed by a short description of different notions of\ndistance between two probability measures.\n2.1.1\nProbability space and random variables\nA probability space consists of the triplet (Ω, F, π), where Ωis the sample space, F is a σ-algebra\nconsisting of subsets of Ω, and π : F →[0, 1] is a probability measure. We will assume Ωto be a\nPolish space (i.e., a complete metric space with an underlying metric d : Ω× Ω→[0, +∞] and a\ncountable dense subset). We will use the notation P(Ω) to denote the set of all possible probability\nmeasures on the measurable space (Ω, F).\nFor any Rd-valued random variable x on a probability space (Ω, F, π), the corresponding probability\nlaw is defined as the following probability measure πx on (Rd, B(Rd)), where B(Rd)) denotes the\nBorel σ-algebra of Rd:\nπx(B) := π(x−1(B)) for all B ∈B(Rd).\nLet λ be the Lebesgue measure on (Rd, B(Rd)). If there exists a nonnegative function px : Rd →\n[0, +∞] such that πx(B) =\nR\nB px dλ for all B ∈B(Rd), px is called the probability density function\n(p.d.f.), or simply the density of x (or πx). The existence of px is guaranteed by the Radon-Nikodym\ntheorem if πx is absolutely continuous with respect to λ, i.e., if πx(B) = 0 whenever λ(B) = 0, for\nany B ∈B(Rd). In this case, the density px is usually written as px = dπx\ndλ , the Radon-Nikodym\nderivative of πx with respect to the Lebesgue measure λ. The density px, if exists, is unique λ-almost\neverywhere (a.e.).\nThe expected value of x, denoted as Ex∼πx[x] or simply E[x], is defined as\nEx∼πx[x] :=\nZ\nΩ\nX(ω) dπ(ω) =\nZ\nRd x dπx(x).\n(4)\nIf x has a density px, the expectation defined in (4) can equivalently be written as\nEx∼πx[x] :=\nZ\nRd x px (x)dλ(x).\n(5)\nAny mapping T : (Ω1, F1) →(Ω2, F2) between two measurable spaces with the property that\nT −1(A ) ∈F1 for all A ∈F2,\n(6)\n4\nis said to be a measurable function from (Ω1, F1) to (Ω2, F2). Let π1 be a probability measure\non (Ω1, F1). The push-forward measure of π1 by T, denoted as T#π1, is defined as a probability\nmeasure on (Ω2, F2) such that\nT#π1(A ) = π1(T −1(A )) for all A ∈F2.\nFor a measurable function g : (Rd, B(Rd)) →(Rm, B(Rm)), the expected value of y = g(x) is defined\nas the following integral:\nEx∼πx[g(x)] :=\nZ\nRd g(x) dπx(x) =\nZ\nRm y dπy(y), where πy := g#πx.\n(7)\n2.1.2\nDistance between probability measures\nMany data-driven approaches for inverse problems rely on methods that are able to efficiently\nestimate and minimize the distance between two probability distributions. A notable class of\ndistances between probability measures is given by the class of ϕ-divergences (sometimes referred to\nas f-divergences), containing common metrics such as the Kullback-Leibler divergence and the total\nvariation distance.\nDefinition 2.1 (ϕ-divergence). Let πx and πy be two probability measures on (Ω, F) with πx being\nabsolutely continuous with respect to πy. Let ϕ : (0, +∞) →(−∞, +∞) be a convex function such\nthat ϕ(1) = 0, and ϕ(0) := lim\nt→0+ ϕ(t) (which could be infinite). Then, the ϕ-divergence between πx\nand πy is defined as\nDϕ(πx, πy) :=\nZ\nΩ\nϕ\n \ndπx\ndπy\n(ω)\n!\ndπy(ω),\n(8)\nwhere dπx\ndπy is the Radon-Nikodym derivative of πx with respect to πy.\nConsider now the special case (albeit an important one) where (Ω, F) = (Rd, B(Rd)) and let px and\npy be the densities of πx and πy, respectively. Then, (8) can be rewritten as\nDϕ(px, py) :=\nZ\nΩ\nϕ\n \npx(x)\npy(x)\n!\npy(x) dλ(x).\n(9)\nThe following are some important special instances of ϕ-divergence that are useful in the context of\nmachine learning and inverse problems:\n1. Kullback–Leibler (KL): ϕ(t) = t log t.\n2. Jensen-Shannon (JS): ϕ(t) = −(t + 1) log\n\u0010\nt+1\n2\n\u0011\n+ t log t.\n3. Total Variation (TV): ϕ(t) = 1\n2|t −1|.\n4. Squared Hellinger: ϕ(t) = (\n√\nt −1)2.\nDespite their popularity, ϕ-divergences have been shown to have serious practical limitations,\nespecially in the context of machine learning, imaging, and inverse problems. From a theoretical\npoint of view, this is due to the fact that they are well-defined only if πx is absolutely continuous with\nrespect to πy. Consequently, ϕ-divergences are not well-suited to compare probability distributions\nconcentrated on low dimensional manifolds. This problem often arises in the context of distribution\n5\nlearning for imaging problems, where data-sets can reasonably be approximated as low dimensional\nmanifolds embedded in a high-dimensional ambient space.\nAnother popular family of distance measures that overcome some of the shortcomings of ϕ-divergences\nare the so-called integral probability metrics. They have the advantage that they are also well-defined\nfor singular measures and are easier to estimate from finitely many samples as compared with\nϕ-divergences, especially in high-dimensional settings [87].\nDefinition 2.2 (Integral probability metrics (IPMs)). Let πx and πy be two probability measures on\n(Ω, F), and let G be some class of bounded and measurable functions g : Ω→R. Integral probability\nmetrics are defined as\n∆G(πx, πy) := sup\ng∈G\n\f\f\f\f\nZ\nΩ\ng(ω) dπx(ω) −\nZ\nΩ\ng(ω) dπy(ω)\n\f\f\f\f .\n(10)\nRelevant examples of integral probability metrics are the following:\n1. Total-variation distance (TVD): G = {g ∈C(Ω) : supx∈Ωg(x) ≤1}. Note that in the case\nwhere πx and πy have densities with respect to the Lebesgue measure, this definition of the\nTVD is equivalent to TV defined as a ϕ-divergence corresponding to ϕ(t) = 1\n2|t −1|.\n2. Maximum-mean-discrepancy (MMD): G = {g ∈H : ∥g∥H ≤1} where H is a Reproducing\nKernel Hilbert Space (RKHS).\n3. Kolmogorov distance (KD): G = {1(−∞,t) : t ∈R}.\n4. 1-Wasserstein distance: G is the class of 1-Lipschitz functions.\nWe refer the interested reader to [87] for more details. It is worth mentioning that not all IPMs\nare suitable for comparing distributions, and the choice of G should be made depending on the\nproblem under consideration. For instance, given s ∈R, suppose that π0 ∈P(R2) is concentrated\nand uniformly distributed on the segment [0, 1] × {0} ⊂R2 and πs ∈P(R2) is concentrated and\nuniformly distributed on the segment [0, 1] × {s} ⊂R2. Then, it is easy to verify that\nTVD(π0, πs) =\n(\n2,\nfor s ̸= 0,\n0,\nfor s = 0.\nThis shows that the total variation is agnostic to the relative positions of the segments in the\nplane and it is thus not a suitable metric to compare the two distributions. This observation\neasily translates to any two distributions concentrated on disjoint lower dimensional manifolds.\nMoreover, it prevents the use of gradient descent strategies due to the severity of the vanishing\ngradient phenomenon. We will see in Section 2.2.2 that such shortcomings are alleviated by the\n1-Wasserstein distance, making this a popular choice for machine learning applications pertaining\nto image processing.\n2.2\nOptimal transport\nIn this section, we recall some fundamental definitions and results in optimal transport that are\nrelevant to the development of the unsupervised learning approaches discussed in Section 3.\n6\n2.2.1\nMonge and Kantorovich formulations of optimal transport\nLet Ω1 and Ω2 be two Polish spaces. Correspondingly, consider two Borel probability spaces\n(Ω1, B(Ω1), π1) and (Ω2, B(Ω2), π2), where B(Ω1) and B(Ω2) are the Borel σ-algebras of Ω1 and Ω2,\nrespectively. Let c : Ω1 × Ω2 →[0, +∞] be the cost of transporting one unit of mass from x ∈Ω1 to\ny ∈Ω2. We will assume that the cost c is continuous. Monge’s optimal transport problem [62] is\nformulated as\nMc(π1, π2) =\ninf\nT:T#π1=π2\nZ\nΩ1\nc(x, T(x)) dπ1(x),\n(11)\nwhere the minimization is carried out over measurable maps T. In other words, (11) seeks to find a\ntransport map T (i.e., a mapping T satisfying T#π1 = π2) for which the overall transportation cost\nis minimized. Monge’s problem may not have a solution, and even worse, a transport map may\nnot always exist [81] (consider, for instance, the problem of transporting discrete masses from one\nset of locations to another). Due to this shortcoming, Kantorovich proposed a relaxation of (11)\nin order to restore the well-posedness of the variational problem [44]. The Kantorovich relaxation\nof (11) reformulates the optimal transport problem by instead considering the transportation\nof mass from any x ∈Ω1 to any y ∈Ω2. Let π be a probability measure on the product space\n(Ω1 ×Ω2, B(Ω1)⊗B(Ω2)), where B(Ω1)⊗B(Ω2) is the smallest σ-algebra generated by B(Ω1)×B(Ω2).\nWe call π a transport plan if\nπ(A × Ω2) = π1(A ) and π(Ω1 × B) = π2(B) for all A ∈B(Ω1), B ∈B(Ω2).\nEquivalently, π is a transport plan if its marginals are π1 and π2. Let Π(π1, π2) be the collection of\nall transport plans from (Ω1, B(Ω1), π1) to (Ω2, B(Ω2), π2). The Kantorovich relaxation of Monge’s\nproblem (11) is then given by\nKc(π1, π2) =\nmin\nπ:π∈Π(π1,π2)\nZ\nΩ1×Ω2\nc(x, y) dπ(x, y).\n(12)\nNote that the set Π(π1, π2) is non-empty, since the product measure π1 ⊗π2 ∈Π(π1, π2). Moreover,\nthe existence of a transport plan minimizing (12) is a consequence of Prokhorov’s theorem and the\nnarrow continuity of the map π 7→\nR\nΩ1×Ω2 c(x, y) dπ(x, y) in P(Ω1 × Ω2) [81]. Notably, (12) is a\nrelaxation of Monge’s problem since given a transport map T, one can construct the associated\ntransport plan as π = (id ×T)#π1. Under certain conditions, the Kantorovich formulation (12) can\nbe shown to be equivalent to the Monge formulation (11) such as in the following theorem [73,\nTheorem B].\nTheorem 2.3. If π1 is non-atomic, namely π1({x}) = 0 for all x ∈Ω1, then\nMc(π1, π2) = Kc(π1, π2).\nStandard arguments of convex analysis also ensure the equivalence of (11) and (12) in the case\nwhere π1 and π2 are empirical measures with uniform weights, i.e.,\nπ1 = 1\nN\nN\nX\ni=1\nδxi,\nπ2 = 1\nN\nN\nX\ni=1\nδyi,\n(13)\nwhere δu denotes the Dirac measure at u.\n7\n2.2.2\nThe Wasserstein distance\nThe Kantorovich formulation of optimal transport allows us to define a distance between two\nprobability measures in a way that overcomes the shortcomings of the distances defined in Section\n2.1.2. Given a distance d : Ω× Ω→[0, ∞), for 1 ≤p < ∞the p-Wasserstein distance between two\nBorel probability measures π1, π2 ∈P(Ω) is defined in terms of the Kantorovich formulation (12) as\nWp(π1, π2) := (Kdp(π1, π2))1/p,\nπ1, π2 ∈P(Ω).\n(14)\nIt can be shown that Wp defines a distance metric on the space of probability measures P(Ω)\n[81]. Moreover, it addresses some of the issues of ϕ-divergences and IPMs presented in Section\n2.1.2. First, it is well-defined for any pair of probability measures π1, π2, even if they are mutually\nsingular. Moreover, following the example given in Section 2.1.2, let π0 ∈P(R2) be concentrated\nand uniformly distributed on the segment [0, 1] × {0} ⊂R2, and let πs ∈P(R2) be concentrated\nand uniformly distributed on the segment [0, 1] × {s} ⊂R2. Then, it holds that\nW1(π0, πs) = |s|,\nfor all s ∈R. In particular, the Wasserstein distance is sensitive to the relative position of the\nsupports of singular distributions being compared. This allows one to better handle the vanishing\ngradient phenomena and ensure more stable learning using gradient-based algorithms [4, 37].\n2.2.3\nThe dual of the Kantorovich formulation of optimal transport\nSince the Kantorovich formulation of optimal transport is essentially an infinite-dimensional linear\nprogramming problem, it is plausible that it admits a strong dual formulation (see [28] for a general\nduality theory of convex variational problems). It can be shown that this is indeed the case. The\ndual reformulation is known as the Kantorovich-Rubinstein (KR) duality. If c(x, y) ≤a(y) + b(x) for\nsome suitable a ∈L1(Ω1; π1) and b ∈L1(Ω2; π2), the KR duality allows us to rewrite the Kantorovich\nformulation of optimal transport as\nsup\n(f,g)∈Λ(c)\nZ\nΩ1\nr(x) dπ1(x) +\nZ\nΩ2\ns(y) dπ2(y),\n(15)\nwhere Λ(c) = {(r, s) : r ∈L1(Ω1; π1), s ∈L1(Ω2; π2), r(x) + s(y) ≤c(x, y) ∀x, y}. Moreover, the\nsupremum in (15) is attained and the optimal r and s are called Kantorovich potentials [2, Theorem\n1.17]. In the case where Ω1 = Ω2 = Ωand the cost is a metric (in which case we rename c(x, y) by\nd(x, y)), then (15) can be rewritten as\nsup\ng∈Lip1(Ω)\nZ\nΩ\ng(x) dπ1(x) −\nZ\nΩ\ng(y) dπ2(y),\n(16)\nwhere Lip1(Ω) is the set of 1-Lipschitz functions defined as\nLip1(Ω) =\n(\ng : Ω→R s.t.\nsup\nx̸=y\n|g(x) −g(y)|\nd(x, y)\n≤1\n)\n.\n(17)\n2.2.4\nReconstructing the optimal transport map from the Kantorovich potentials\nBefore concluding this section, we state an important result in optimal transport that, in certain\ncases, allows one to reconstruct the optimal transport map from the Kantorovich potential. Let π1\n8\nand π2 two probability measures on (Ω, B(Ω)) where Ω⊂Rd is compact with a boundary negligible\nwith respect to the Lebesgue measure. Let c : Ω× Ω→[0, ∞) defined as c(x, y) = η(x −y) for\nx, y ∈Ω, where η is a strictly convex function. Then, the following theorem relating the optimal\ntransport map and the Kantorovich potential holds [81, Theorem 1.17].\nTheorem 2.4. Suppose that π1 is absolutely continuous with respect to the Lebesgue measure. Then\nthere exists a Kantorovich potential r, and the optimal transport map T can be reconstructed as\nT(x) = x −(∇η)−1(∇r(x))\nx ∈Ω.\n(18)\nThis theorem is a consequence of more general results due to Brenier [13] and can be applied, for\ninstance, for transport costs such as c(x, y) = |x −y|p, where 1 < p < ∞. That is, this result\napplies to all p-Wasserstein distances with 1 < p < ∞. Specializing Theorem 2.4 to the cost\nc(x, y) = 1\n2|x −y|2 corresponding to the 2-Wasserstein distance, T can be reconstructed as\nT(x) = x −∇r(x) = ∇\n \nx2\n2 −r(x)\n!\n.\nMoreover, thanks to a consequence of the celebrated Brenier’s theorem (e.g. see [81, Proposition\n1.21]) the function u(x) = x2\n2 −r(x) is convex, implying that the optimal transport map is the\ngradient of a convex function. This observation has been utilized to design machine learning methods\nto approximate optimal transport maps using gradients of convex functions [54, 3].\n2.3\nConvex analysis and monotone operator theory\nIn this section, we will recall some classical results in convex analysis and monotone operator theory\nthat will be useful for developing the theory behind many of the provable machine learning methods,\nas well as for gaining intuition behind their workings. In particular, in Section 4, we will link some\nof these classical results with the fixed point theory used in the learned iterative scheme setting.\nWe first recall some definitions and properties of convex functions and monotone operators, before\npresenting some examples of how we can leverage these regularity properties for convergence in the\nclassical setting. We will also briefly discuss operator splittings, which serve as the basis for some\nlearned iterative schemes.\n2.3.1\nConvex analysis\nWe first review some common properties of convex functions that serve as common assumptions in\nthe sequel. A more comprehensive overview of convex analysis can be found, for example, in [76].\nLet X be a Banach space and let X∗denote the corresponding dual space. We state some basic\ndefinitions in the following.\nDefinition 2.5. A function f : X →R is lower semi-continuous (l.s.c.) at a point x if for every\nsequence xn →x in X,\nlim inf\nn→∞f(xn) ≥f(x0).\nf is proper if the effective domain\ndom f := {x ∈X | f(x) < +∞}\nis nonempty. We define Γ0(X) as the class of convex proper l.s.c. functions from X →R, and drop\nthe argument when the domain is understood from the context.\nf is coercive if for all xn with ∥xn∥→∞, we have that f(xn) →∞.\n9\nThe class Γ0 gives sufficient conditions for many regularity conditions to hold, and will be the main\nassumption for our objective optimization problems. Lower semi-continuity may sometimes be\nequivalently referred to as closed in the literature, which is shown in the following proposition.\nProposition 2.6 ([28, Prop. 2.3]). For a function f : X →R, the following two statements are\nequivalent:\n1. f is lower semi-continuous;\n2. f is closed, i.e., the epigraph epi(f) := {(x, t) ∈X × R | t ≥f(x)} is closed.\nThese properties can be used directly to show that a convex function has a minimizer.\nTheorem 2.7. Let X be a Banach space and τX be some topology on X such that bounded sequences\nin X have τX-convergent subsequences. If f : X →R is proper, bounded from below, coercive, and\nτX-l.s.c., then f has a minimizer.\nProof. By boundedness from below, a minimizing sequence (xn) exists. By coercivity, the minimizing\nsequence is bounded. Apply the condition on τX to a minimizing sequence to obtain a limit in τX,\nxkn →¯x. By lower semi-continuity, the limit satisfies f(¯x) ≤lim inf\nn\nf(xkn) = inf\nX f. Therefore, ¯x is\na minimizer of f.\nWe continue with some definitions extending the classical notion of a derivative to convex non-\ndifferentiable functions.\nDefinition 2.8. A function f : X →R is subdifferentiable at a point u ∈X if there exists a dual\nelement p ∈X∗such that\nf(v) ≥f(x) + ⟨p, v −x⟩, ∀v ∈X.\nThe dual element p is called a subgradient at u. The subdifferential of f at u, denoted ∂f(u), is the\ncollection of all such subgradients of f at u, i.e.\n∂f(u) := {p ∈X∗| f(v) ≥f(x) + ⟨p, v −x⟩, ∀v ∈X}.\nThe subdifferential is a multi-valued operator that shares some properties with the classical derivative\noperator, which can additionally be defined for discontinuous functions. In particular, if f is\ndifferentiable at a point u, then the subdifferential is equal to the singleton set containing the\nderivative ∂f(u) = {f′(u)}. The following propositions state sufficient (but not necessary) conditions\nfor the existence of the subdifferential, as well as some useful properties. Additional properties can\nbe found in classical literature [76, 9, 17].\nProposition 2.9 ([72, 28]). Suppose f : X →R is convex, finite, and continuous at some u ∈X.\nThen ∂f(v) ̸= ∅for all v ∈X. Moreover, 0 ∈∂f(v) if and only if v is a minimizer of f. If g : X →R\nis another convex proper l.s.c. function and f is continuous at some u ∈dom f ∩dom g, then\n∂(f + g) = ∂f + ∂g.\nIf f is instead Gâteaux differentiable at u ∈X, then it is subdifferentiable at u and ∂f(u) = {f′(u)}.\nProposition 2.10 ([17, Thm. 7.13]). Suppose f : X →R is proper and convex. If u ∈dom f, then\n∂f(u) is convex and weak-* compact.\nUnder these conditions, we can define the proximal operator, which tries to move towards the\nminimizer of f, regularized by the distance to the initial point.\n10\nDefinition 2.11 (Proximal operator). For a convex function f : X →R, the proximal operator is\ndefined as\nproxf(x) = arg min\ny∈X\n\u001a1\n2∥y −x∥2 + f(y)\n\u001b\n(19)\nThe following proposition details some properties of the proximal operator. In particular, it can\nbe thought of as an implicit Euler discretization of gradient flow, as opposed to the explicit Euler\ndiscretization that is gradient descent.\nProposition 2.12 ([76, 28, 77]). For a proper convex l.s.c. function f, the proximal operator is\nwell-defined and is single-valued. Moreover, it satisfies the following:\n1. proxf is nonexpansive, and, in particular, is continuous.\n2. Fixed points of proxf correspond to minimizers of f:\n{x0 ∈X | x0 = proxf(x0)} = arg min\nX\nf.\nIf X = Rn, the following also hold:\n1. (Moreau’s identity) proxf + proxf∗= id, where id is the identity map on X;\n2. Letting the Moreau envelope be defined as\nMλf(x) = inf\ny∈Rn\n\u001a\nf(x) + 1\n2λ∥x −y∥2\n2\n\u001b\n,\n(20)\nthe proximal operator satisfies\n∇Mλf(x) = 1\nλ(x −proxλf(x)) = proxf∗/λ(x).\n(21)\n3. ∂f and proxf are maximally monotone mappings (see Section 2.3.2 for the definition) from\nRn to Rn.\n2.3.2\nMonotone operator theory\nA common way of showing the convergence of some iterative methods is through monotone operator\ntheory, consisting of fixed-point results. Monotone operators are inextricably tied to convex functions\nthrough the proximal and subgradient operators, making them a useful tool for showing convergence\nwithin the realm of convexity.\nDefinition 2.13 (Monotonicity). A set-valued mapping T : Rn ⇒Rn is monotone if for all\nx, x′ ∈Rn, p ∈T(x), p′ ∈T(x′),\n⟨p −p′, x −x′⟩≥0,\nand strictly monotone if the inequality is strict for x ̸= x′. The resolvent of T is the operator\nJT := (id +T)−1,\nand the reflected resolvent is\nRT := 2JT −id .\nT is said to be maximally monotone if its graph G(T) = {(x, p) : x ∈Rn, p ∈T(x)} is not contained\nwithin the graph of another monotone operator.\n11\nThe convex minimization problem min\nx∈X f(x) thus corresponds to the monotone inclusion problem\n0 ∈∂f(x). In the differentiable case, this resolves to solving the optimality condition f′(v) = 0.\nMoreover, for f ∈Γ0, the proximal operator is the resolvent of the subgradient operator, i.e.\nproxf = J∂f. Monotonicity is intrinsically related to convexity as described in the following theorem.\nIntuitively, it means that sub-gradients are aligned with ascent directions. One important concept\nis that of non-expansiveness, which is crucial in the study of fixed-point convergence.\nDefinition 2.14 (Non-expansiveness). A mapping T : Rn →Rn is non-expansive if for all x, y ∈Rn,\n∥T(x) −T(y)∥≤∥x −y∥.\nT is firmly non-expansive if for all x, y ∈Rn,\n∥T(x) −T(y)∥2 + ∥(id −T)(x) −(id −T)(y)∥2 ≤∥x −y∥2.\nNote that firm non-expansiveness implies non-expansiveness.\nThe concepts of monotonicity and non-expansiveness are intrinsically tied to convexity, as shown by\nthe following results.\nTheorem 2.15 ([77, Sec. 12.C.]). For a proper l.s.c. function f : Rn →R, f is convex if and only\nif ∂f is monotone, in which case ∂f is also maximally monotone. Moreover, for any λ > 0, the\nproximal mapping proxλf : Rn ⇒Rn is monotone. If f is additionally convex, then proxλf = Jλ∂f\nis maximally monotone and also non-expansive.\nProposition 2.16 ([9, Cor. 23.10]). For a maximally monotone operator A : Rn ⇒Rn, we have\nthat\n1. JA and id −JA are firmly nonexpansive and maximally monotone;\n2. RA is non-expansive.\n2.3.3\nOperator splitting\nConvex optimization problems are often solved using iterative methods, where a sequence is\nconstructed that converges to the minimizer, with some common methods including subgradient\ndescent or proximal gradient descent. Recall that variational problems typically take the form\nof a composite optimization problem (3). Usually, the fidelity and regularization terms will have\ndifferent regularity conditions, such as smoothness or Lipschitz conditions. We can exploit the\ncomposite structure to simplify each iteration. Noting the correspondence between convex problems\nand monotone inclusion problems, we can convert the above problem to finding the equivalent\nproblem of finding zeros of sums of two maximally monotone operators.\nConsider the following inclusion problem\n0 ∈Ax + Bx,\nwhere A and B are both maximally monotone operators, which arises naturally from finding a\nminimizer of the sum of two convex functions. If A + B is also maximally monotone, then one\npossible approach is to consider root solving using the resolvent Jγ(A+B). However, this is generally\ndifficult to compute, for example in the case when A, B are proximal operators of convex functions\nf, g ∈Γ0, respectively. Therefore, one seeks to find a zero of A + B, using only their resolvents\nJγA and JγB. This process of splitting the resolvent of A + B into the resolvents of its components\n12\nis generally referred to as a splitting algorithm and can be performed in different ways [49]. We\npresent two simple versions, which are by far the most widely used splitting techniques in convex\noptimization: the forward-backward splitting and the Douglas-Rachford splitting [26].\nTheorem 2.17 (Douglas-Rachford Splitting [9, Thm. 25.6]). For a Hilbert space H, let A, B : H ⇒\nH be maximally monotone operators such that zer(A + B) ̸= ∅. Let (λn)n∈N be a sequence in [0, 2]\nsatisfying P\nn∈N λn(2 −λn) = +∞, γ > 0 be a step-size. Let x0 ∈Rn be an initialization. Consider\nthe iterations\n\n\n\n\n\n\n\nyn = JγBxn,\nzn = JγA(2yn −xn),\nxn+1 = xn + λn(zn −yn).\n(DRS)\nThen there exists a fixed point x ∈Fix RγARγB such that the following hold:\n1. JγB(x) ∈zer(A + B)\n2. yn −zn converges strongly to zero,\n3. xn converges weakly to x\n4. yn and zn converge weakly to JγB(x).\nNote that in the case where the Hilbert space H is finite-dimensional, weak convergence is equivalent\nto strong convergence. Letting A and B be proximal operators of some proper convex l.s.c. functions\nf and g, we get convergence to a fixed point of proxf+g, using only proximal operators or subgradients\nof f and g separately. Further, the fixed point is a minimum of f + g. This is particularly useful\nwherein f and g have easy-to-compute proximals, while f + g does not.\nBy casting the above monotone inclusion problem in the scope of convex functions, with A being a\nderivative and B being a proximal operator, we can obtain splitting schemes that optimize the sum\nof two convex functions, where one of the functions is smooth.\nTheorem 2.18 (Forward-Backward Splitting [9, Cor. 27.9]). Let f : H →R be convex and\ndifferentiable with 1/β-Lipschitz gradient, and g : H →R be proper convex l.s.c. and possibly\nnon-smooth. Let γ ∈(0, 2β) and set δ = min{1, β/γ} + 1/2. Further let (λn)n∈N be a sequence in\n[0, δ] such that P\nn∈N λn(δ −λn) = +∞. Suppose that f + g admits a minimizer and let x0 ∈H.\nThen, the forward-backward iterations, given by\n(\nyn = xn −γ∇f(xn),\nxn+1 = xn + λn(proxγg yn −xn),\n(FBS)\nsatisfy the following:\n1. (xn)n∈N converges weakly to a point in arg minH(f + g);\n2. Suppose infn λn > 0 and x ∈arg minH(f + g). Then ∇f(xn) converges strongly to ∇f(x).\nNote that by taking λn = 1, the FBS algorithm alternates between a proximal step on g and a\ngradient descent step on f. Optimizing the sum of two convex functions where one is smooth arises\nnaturally in variational regularization [82, 42]. In this case, f is usually chosen to be a smooth\nfidelity term, such as the ℓ2\n2 penalty. As this minimization is usually ill-posed, a regularization term\ng is added to the fidelity, representing a prior that is imposed on the data.\n13\nExample 2.19 (ISTA). Consider the case where our Hilbert space is finite-dimensional Euclidean\nspace H = Rn. Let A : Rn →Rm be a bounded linear operator, and let z ∈Rm. The iterative\nshrinkage thresholding algorithm (ISTA) considers the optimization problem where f(x) = ∥Ax −\nz∥2/2, with an ℓ1 regularization [25, 36, 10]. This is used in sparse coding, where the ℓ1 penalty\nenforces sparsity on x and is sometimes referred to as LASSO regression in the statistical literature\n[95]. The resulting optimization problem to solve is\narg min\nx∈Rn\nf(x) + g(x) := 1\n2∥Ax −z∥2\n2 + λ∥x∥1,\nwhere λ > 0 is a regularization parameter. Note that this f + g admits a minimizer since it is\ncoercive and bounded below by 0. Moreover, g is not differentiable, so first-order methods that rely on\nthe gradient of f + g are not applicable. We can, however, apply Theorem 2.18 to obtain a (strongly)\nconvergent scheme. We first observe that the proximal operator of λ∥x∥1 is the coordinate-wise\nshrinkage operator, defined by\nproxα∥x∥1 = hα(x), where\n[hα(x)]i = sign(xi) max(|xi| −α, 0).\nObserve that the proximal operator of ∥· ∥1 is straightforward to compute. Taking the step-sizes\nλn = 1 and γ < 1/∥A⊤A∥, ISTA reduces to the forward-backward scheme\nxn+1 = h(λγ) (xn −γ∇f(xn)) .\n(ISTA)\n2.3.4\nPseudo-inverses\nFor two Banach spaces X and Y, a bounded linear operator A ∈L(X, Y) may not be invertible in\nthe usual sense outside the range of A. Recall that for a bounded linear operator A, its null-space\nker(A) is closed, and thus admits a unique orthogonal complement in X. Moreover, A restricted to\nker(A)⊥is injective and thus admits a linear inverse from range(A) to ker(A)⊥.\nDefinition 2.20. For a linear operator A ∈L(X, Y), let ˜A denote the restriction of A to ker(A)⊥⊆\nX, where ker(A) is the null-space of A. Note that ˜A is invertible. The Moore-Penrose pseudo-inverse\nA† : D(A†) →X is the unique linear extension of ˜A−1 : range(A) →ker(A)⊥to the domain\nD(A†) := range(A) ⊕range(A)⊥,\nsatisfying ker(A†) = range(A)⊥.\nRemark 2.21. If X, Y are finite-dimensional, D(A†) = Y. The Moore-Penrose pseudo-inverse is\nequivalent to linearly extending the inverse of A from ker(A)⊥to all of Y by defining A† : ker(A) 7→0.\nProposition 2.22 ([29, Prop. 2.3]). The Moore-Penrose pseudo-inverse satisfies the following\nproperties:\n1. A†A = Πker(A)⊥,\n2. AA† = Πrange(A)|D(A†),\n3. AA†A = A,\n4. A†AA† = A†.\nThe Moore-Penrose inverse is not necessarily continuous. It is continuous if and only if range(A) is\nclosed [29]. Moreover, it can be very ill-conditioned if A has small singular values. This explains\nwhy the direct inversion of inverse problems is unstable, necessitating the use of regularization\ntechniques.\n14\n2.4\nSupervised versus unsupervised learning of reconstruction operators\nIn this section, we outline different training strategies for learning a data-driven reconstruction\noperator for imaging inverse problems based on available training data. The specific training\nstrategy adopted for a given problem depends on several practical considerations, such as the type\nand amount of available data, computational requirements, desired theoretical guarantees, etc. In\ngeneral, supervised approaches tend to result in better empirical performance than unsupervised\napproaches, but it might be infeasible to acquire paired trained data for supervised learning in\nproblems of practical interest.\n2.4.1\nSupervised learning\nIn supervised learning, one seeks to learn a reconstruction map Gθ : Y →X, typically parameterized\nusing a deep neural network (DNN), utilizing pairs of training examples (x(i), y(i))N\ni=1 drawn from the\n(unknown) joint density distribution of the (X×Y)-valued random variable (x, y), where y = Ax+w.\nThe parameter θ is learned by minimizing the empirical reconstruction error measured using a\nsuitable loss functional ℓ: X × X →R+ over the training data set:\nθ∗∈arg min\nθ\nJ(θ), where J(θ) := 1\nN\nN\nX\ni=1\nℓ\n\u0010\nx(i), Gθ(y(i))\n\u0011\n.\n(22)\nThe key challenge in supervised learning is to construct a suitable parameterization of the recon-\nstruction operator Gθ such that it is sufficiently expressive and encodes knowledge about the data\ngeneration process (i.e., the forward operator A). To this end, several techniques have been proposed\nachieving remarkable performances in inverse problems reconstruction [106, 70, 41, 21, 32, 83, 61].\nHere, we describe two specific ones that are relevant for the unsupervised methods treated in this\nchapter: (i) post-processing approaches [41] and (ii) algorithm unrolling (see [61] and references\ntherein). The post-processing approach consists in designing Gθ as the composition Gθ = Cθ ◦ρ,\nwhere a model-based reconstruction operator ρ : Y →X (e.g., the filtered back-projection (FBP) in\nCT) is followed by a deep convolutional neural network (CNN) Cθ : X →X that is trained to remove\nartifacts from ρ(y). Since post-processing approaches do not fully incorporate the physics of the\nimaging system, they typically need large amounts of training data to generalize well on unseen data.\nMoreover, the final reconstructed image produced by a post-processing method does not necessarily\nsatisfy data-consistency, meaning that a small value of the fidelity ℓ(y, A ρ(y)) corresponding to ρ\ndoes not imply a small value of the fidelity ℓ(y, A Cθ(ρ(y))).\nThe algorithm unrolling framework offers a more principled approach for incorporating imaging\nphysics into the reconstruction operator. As the name suggests, algorithm unrolling builds the\nreconstruction operator by first unfolding a small number of iterations of an optimization algorithm\n(such as proximal gradient descent (PGD)) for solving the variational image reconstruction problem\n(3), and then by replacing the components that do not depend on the imaging process using learnable\ndata-driven units. In the interest of concreteness, consider (3) where both f and Rλ are in Γ0(X),\n∇f is L∇f-Lipschitz continuous, but Rλ is not necessarily differentiable. If Rλ admits a cheaply\ncomputable proximal operator, a natural choice for solving (3) is the PGD algorithm given by\nxk+1 = proxηRλ (xk −η ∇f(y, Axk)) , k = 0, 2, · · · , N −1,\n(23)\nwhere η ≤\n1\nL∇f . For large-scale image reconstruction problems such as medical imaging, one would\ntypically need a few thousand iterations of PGD to obtain a reasonable reconstruction, which could\nbe unacceptably slow. The key idea behind algorithm unrolling is to truncate iterative optimization\n15\nalgorithms such as (23) after a small number of iterations (for example, N ∼10), and replace the\nproximal operator with a CNN ψθk : X →X for each k. The parameters θ = (θk)N\nk=1 are then\nlearned by minimizing the empirical risk J(θ) on the training data set:\nJ(θ) := 1\nN\nN\nX\ni=1\nℓ\n\u0010\nx(i), x(i)\nN (θ)\n\u0011\n,\n(24)\nwhere x(i)\nk+1 := ψθk\n\u0010\nx(i)\nk −η ∇f\n\u0010\ny(i), Ax(i)\nk\n\u0011\u0011\n, k = 0, 2, · · · , N −1. The origin of algorithm unrolling\ncan be traced back to the seminal work by Gregor and LeCun on learned iterative shrinkage\nthresholding algorithms (LISTA) [36] for efficient sparse coding. In recent years, such methods have\nbeen extensively developed and they currently offer performances able to achieve the state-of-art for\nsupervised inverse problems reconstruction. We refer the interested reader to [1, 33, 101, 22, 57, 94]\nand the references therein for further details on algorithm unrolling.\n2.4.2\nUnsupervised learning\nIn contrast to supervised learning, we will use the phrase unsupervised learning to refer to any scenarios\nwhere one does not have access to paired training examples drawn from the joint distribution of\n(x, y), but only on the marginal distributions of x and y. From a practical perspective, unsupervised\nlearning approaches are more realistic in real-world applications, as it is generally challenging to\nacquire paired examples for training reconstruction operators. For instance, the training data set in\nthe image reconstruction problem in X-ray CT consists of high-quality reconstructed images x(i)\nobtained from high- or normal-dose projection data, and their corresponding low-dose projection\ndata y(i). This is generally difficult to obtain, as it necessitates scanning a large number of subjects\nwith two different doses, then aligning the respective scans voxel-wise to ensure exact correspondence\nbetween x(i) and y(i).\nBroadly, one might encounter the following three scenarios (or, some combinations thereof) in\nunsupervised learning so far as the training data is concerned.\n1. Unpaired training examples: In this setting, the training data consists of i.i.d. samples\n(x(i))N1\ni=1 and (y(j))N2\nj=1 drawn from the marginal distributions πx and πy of the ground-truth\nimages and the measured data, respectively.\nUsing only the knowledge of the marginal\ndistributions πx and πy, one aims at learning a correspondence between the probability\ndistributions in the form of a reconstruction Gθ : Y →X such that (Gθ)#πy = πx. Additionally\nthe reconstruction needs to satisfy data-consistency, meaning that y is close to AGθ(y) for most\nof the samples y from the marginal πy. In Section 3, we will describe in details several approaches\nusing unpaired training samples that are based on optimal transport techniques and cycle\narchitectures. However, we point out that many other unsupervised methods based have been\nproposed in the literature. Such approaches are often based on conditional variants of generative\nmodels and on their inversion. We refer the interested reader to [100, 99, 8, 52, 60, 80, 105] and\nthe references therein.\n2. Learning the prior: In many applications, one has only access to samples (x(i))N1\ni=1 from the\ndistribution πx of the ground-truth images. In such cases, the primary objective is to utilize\nideas from the generative machine learning approaches (such as generative adversarial networks\n(GANs), variational autoencoders (VAEs), etc.) to build a reasonable approximation of the\nimage prior to regularize the inverse problem. Many approaches have been proposed to achieve\n16\nthis goal, based, for instance, on constructing a projection on the range of the pre-trained\ngenerator and approximating its inverse [5, 84, 103, 23, 12]. Plug-and-play (PnP) denoising\nmethods (which we review in Section 4.2) also fall in this category as they seek to implicitly\nlearn a regularizer through an image denoiser.\n3. Fully unsupervised approaches: We will use this term to refer to the case where only i.i.d.\nsamples (y(j))N2\nj=1 from the data distribution πy are available for training. These methods are\nessentially ground-truth-free, as do not make use of the true images during training. Among\nvarious approaches in this category, we provide a detailed treatment of the emerging learning-to-\noptimize paradigm in Section 4.1. These methods seek to learn a fast solver for high-dimensional\nconvex optimization problems that arise frequently in inverse problems by leveraging training\ndata (while not utilizing any ground-truth). Some notable methods in this category (such as\nunbiased risk estimation, deep image prior, equivariance, etc.) are briefly reviewed in Section 5.\n3\nOptimal transport-based unsupervised approaches\nIn recent years, optimal transport-based methods have been extensively used to address unsupervised\ndata-driven tasks such as image generation [4, 37], domain adaptation, image-to-image translation,\nand image super-resolution. Unsurprisingly, many inverse problems in areas such as medical imaging,\ngeophysics, and fluid dynamics have benefited from such methods in terms of both modeling\ncapabilities and the efficiency of the available algorithms. In the subsequent sections, we will\nillustrate several optimal transport-based unsupervised approaches for inverse problems, ultimately\naiming to draw a connection between them.\n3.1\nCycle-GAN–based approaches to unsupervised learning\nWe start by addressing methods that are based on cyclic models. Inspired by Cycle-GAN [107],\nsuch approaches are particularly suited for inverse problems in the case of unsupervised data\nsince they allow enforcing a coupling between ground-truth images and measurements through a\ncycle-consistency penalty. Optimal transport metrics have been incorporated into these models,\nallowing for more stable training.\n3.1.1\nWasserstein generative adversarial networks (WGANs)\nBefore addressing cycle-based approaches, we recall classical generative models, with a particular\nfocus on the ones based on optimal transport techniques.\nWasserstein generative adversarial\nnetworks (WGANs) [4, 37] have incorporated optimal transport techniques for image generation,\nachieving performance superior to that of traditional generative adversarial networks (GANs) [35],\nwhile ensuring a more stable training for high dimensional data-sets while mitigating the problem\nof mode-collapse [18]. Denoting by πv ∈P(V) a known latent distribution in V (which can be\neasily sampled) and πx ∈P(X) the unknown ground-truth distribution, Wasserstein GANs aim to\nconstruct a generator Gθ : V →X by minimizing the 1-Wasserstein distance between (Gθ)#πv and\nπx, i.e.\nmin\nθ\nW1 ((Gθ)#πv, πx) ,\n(25)\n17\nwhere Gθ is typically parameterized by a suitable DNN. Applying the dual formulation of the\n1-Wasserstein distance, c.f. (15), the objective in (25) can be equivalently rewritten as\nmin\nθ\nsup\ng∈Lip1(X)\nZ\nX\ng(x) d(Gθ)#πv −\nZ\nX\ng(x) dπx.\n(26)\nBy expressing the constraint g ∈Lip1(X) as a penalization in the objective, and applying the\ndefinition of push-forward of probability measures, (26) can be approximated by the following\nmin-max problem\nmin\nθ\nsup\nσ\nZ\nV\ngσ(Gθ(v)) dπv −\nZ\nX\ngσ(x) dπx + λ\nZ\nX\n(|∇gσ|(ˆx) −1)2\n+ dˆπ,\n(27)\nwhere gσ : X →R is parametrized by a suitable DNN, λ > 0 is a positive parameter and ˆπ ∈P(X) is\ndefined by sampling uniformly on the lines connecting samples of πx and samples of (Gθ)#πv. The\nnetwork gσ is referred to as the discriminator or the critic, since, during training, it learns to tell\napart the ground-truth images from the generated ones. The training is performed by optimizing\n(27) computed on the empirical approximation πx ∼\n1\nN1\nPN1\ni=1 δx(i) and πv ∼\n1\nN2\nPN2\ni=1 δv(i), where\n(x(i))N1\ni=1 are the training samples and (v(i))N2\ni=1 are samples drawn from πv. From a theoretical point\nof view, the objective of WGAN closely resembles the classical GAN objective\nmin\nθ\nsup\nσ\nZ\nV\nlog(1 −dσ(Gθ(v)) dπv +\nZ\nX\nlog(dσ(x)) dπx,\n(28)\nwhere Gθ : V →X is the generator and dσ : X →R is the discriminator. Indeed, (25) and (28) are\nboth expressed as an adversarial min-max problem, with the substantial difference that optimizing\n(28) is equivalent to minimizing the Jensen-Shannon divergence between πx and (Gθ)#πv. Since\nWGAN aims to minimize the 1-Wasserstein distance, the considerations of Section 2.1.2 apply,\njustifying why WGAN is more stable for learning high-dimensional data distributions supported on\nlower dimensional manifolds [4].\nWe conclude this section by mentioning that many optimal transport-based generative models\nbesides the WGAN framework are available in the literature. We will not focus on them here;\nhowever, we refer the interested reader to [71, 96, 31, 102] and the references therein.\n3.1.2\nCycle-GAN–based approaches for inverse problems\nClassical GANs and WGANs are both characterized by the simultaneous training of a generator\nGθ : V →X mapping a low-dimensional latent space to a high-dimensional data space, and a\ndiscriminator mapping X to R. Cycle-GAN was introduced in [107] to address unsupervised image-\nto-image translation between two data sets in X and Y. This has been achieved by coupling the\naction of two generators Hσ : X →Y and Gθ : Y →X that are trained to achieve cycle-consistency\nby enforcing that Hσ(Gθ(y)) ≈y and Gθ(Hσ(x)) ≈x for samples in πy and πx, where πx and πy\nare the data distributions in X and Y respectively. Moreover, the generators are trained together\nwith two discriminators dX\n˜θ : X →R and dY\n˜σ : Y →R designed to ensure that (Hσ)#πx = πy and\n(Gθ)#πy = πx through a GAN objective. This model is schematically represented in Figure 1. As\nnoticed in [107], cycle-consistency in cycle-GAN architectures can be seen as a way to regularize the\noptimal pair of generators Hσ, Gθ by enforcing the validity of a transitivity property. This allows,\nin the training phase, to reduce the pairs of generators such that (Hσ)#πx = πy and (Gθ)#πy = πx,\nfavoring a faster and more stable training.\n18\nX\ndX\n˜θ\nHσ\nY\nGθ\ndY\n˜σ\nFigure 1: Schematic representation of a cycle-GAN model\nThe objective of cycle-GAN is given by the sum of two GAN losses together with the cycle-consistency\nloss:\nmin\nθ,σ max\n˜θ,˜σ\nα LX\nGAN(Gθ, dX\n˜θ ) + βLY\nGAN(Hσ, dY\n˜σ) + Lcycle(Hσ, Gσ),\n(29)\nwhere α and β are positive parameters and\nLX\nGAN(Gθ, dX\n˜θ ) =\nZ\nY\nlog(1 −dX\n˜θ (Gθ(y)) dπy +\nZ\nY\nlog(dX\n˜σ(y)) dπy,\nLY\nGAN(Hσ, dY\n˜σ) =\nZ\nX\nlog(1 −dY\n˜σ(Hσ(x)) dπx +\nZ\nX\nlog(dY\n˜θ (x)) dπx, and\nLcycle(Hσ, Gθ) =\nZ\nX\n∥Gθ(Hσ(x)) −x∥1 dπx +\nZ\nY\n∥Hσ(Gθ(y)) −y∥1 dπy.\n(30)\nThe training is performed by optimizing (29) computed on the empirical approximations πx ∼\n1\nN\nPN\ni=1 δx(i) and πy ∼\n1\nM\nPM\ni=1 δy(i), where (x(i))N1\ni=1, (y(i))N2\ni=1 are training samples from X and Y\nrespectively. It is important to note here that the method is unsupervised since the training samples\nare unpaired, i.e., y(i) does not necessarily correspond to the noisy measurement of x(i). This allows\nfor more flexible models that do not require balanced samples. Moreover, it offers methods able to\naddress more realistic real-world applications, since it is generally difficult and expensive to acquire\npaired samples.\nDespite the original cycle-GAN approach being designed mainly for image-to-image translation,\nseveral of its variants have been proposed to address different tasks in an unsupervised framework,\nsuch as CT-reconstruction [43], super-resolution [104], and conditional image generation [51], to\nname a few. However, the successful application of cycle-GAN-based models to inverse problems\nhas remained problematic, primarily due to the following reasons:\n1. It is unclear how to introduce the knowledge of the forward operator into the model.\n2. Cycle-GAN is a symmetric architecture and struggles to take into account the potential\ndifference in complexity between data x and measurements y.\n3.1.3\nOptimal transport and cycle-consistency combined\nTo address the difficulties stated above in an unsupervised setting, new models based on optimal\ntransport methods have been proposed in [86, 64]. In [86], a cycle-GAN architecture was adapted\nto the 1-Wasserstein loss by coupling two generators Gθ and Hσ, trained as in WGAN to minimize\nW1((Hσ)#πx, πy)\nand\nW1((Gθ)#πy, , πx)\n(31)\ntogether with a cycle-consistency loss (c.f. Figure 2). This leads to the training objective\n19\nX\ngX\n˜θ\nHσ\nY\nGθ\ngY\n˜σ\nFigure 2: Schematic representation of a cycle-WGAN model\nmin\nθ,σ max\n˜θ,˜σ\nα LX\nW (Gθ, gX\n˜θ ) + βLY\nW (Hσ, gY\n˜σ ) + Lcycle(Hσ, Gθ),\n(32)\nwhere α, β are positive parameters, with\nLX\nW (Gθ, gX\n˜θ ) =\nZ\nY\ngX\n˜θ (Gθ(y)) dπy −\nZ\nX\ngX\n˜θ (x) dπx + λ\nZ\nX\n(|∇gX\n˜θ |(ˆx) −1)2\n+ dˆπX,\nLX\nW (Hσ, gY\n˜σ ) =\nZ\nY\ngY\n˜σ (Hσ(x)) dπx −\nZ\nX\ngY\n˜σ (y) dπy + λ\nZ\nY\n(|∇gY\n˜σ |(ˆy) −1)2\n+ dˆπY,\nwhere λ > 0, ˆπX and ˆπY are defined as in (27), and Lcycle(Gθ, Hσ) is as in (30).\nIt is important to note that the training objective in (32) is symmetric in X and Y, and it is not\ndesigned to capture a statistical relationship between x and y. In the works of [86] and [64], (32) has\nbeen accordingly modified to include the knowledge of the inverse problem data acquisition process\ny = Ax + w, where A is the measurement operator defined in (1), and y and x are the random\nvariables representing ground-truth and noisy measurements. To this end, [86, 64] adapt (32) by\nfixing one of the two generators Hσ and Gθ to be either A or its pseudo-inverse A† (see also Figure\n2). At the cost of limiting the expressivity of the cycle architecture, this choice introduces the data\nacquisition process in the model leading to great benefits in the form of higher stability in the training\nphase and better data consistency. Alternatively, it is also possible to assume additional structure\non the measurement operators, without fixing it, for example prescribing that the measurement\nis an unknown convolutional operator of the type Hσ(x) = hσ ⋆x for a parameterized family of\nconvolutional kernels hσ (c.f. Figure 3).\nThe objective (32) can be adapted in several ways depending on how the data acquisition process\nhas been incorporated. For instance, when only the measurement operator A is prescribed, given\nsuitable losses f1 : X × X →R+, f2 : Y × Y →R+, the cycle-loss can be written as\nLcycle(Gθ) =\nZ\nX\nf1(Gθ(A(x)), x) dπx +\nZ\nY\nf2(A(Gθ(y)), y) dπy,\n(33)\nas in [86], or alternatively as\neLcycle(Gθ) =\nZ\nX\nf1(Gθ(A(x)), x) dπx,\nLcycle(Gθ) =\nZ\nY\nf2(A(Gθ(y)), y) dπy,\n(34)\nas in [64]. In particular, the choice of eLcycle leads to the Unrolled Adversarial Regularizer (UAR)\nintroduced in [64]. All these models are trained by computing the objective on the empirical\napproximation πx ∼\n1\nN1\nPN1\ni=1 δx(i) and πy ∼\n1\nN2\nPN2\ni=1 δy(i) where (x(i))N1\ni=1, (y(i))N2\ni=1 are training\nsamples from X and Y. It is important to note here that the training samples are unpaired, i.e.,\nthey are sampled from the marginal distributions of the ground-truth images and the data, and not\nfrom their joint distribution. This is a striking difference compared to standard supervised methods\nsuch as U-net post-processing [41] and the learned primal-dual (LPD) method [1].\n20\nX\ngX\n˜θ\nA\nY\nGθ\nX\nHσ\nY\nA†\ngY\n˜σ\nX\ngX\n˜θ\nHσ\nY\nGθ\ngY\n˜σ\nFigure 3: Schematic representation of WGAN-cycle–type models. On the top left: the generator\nHσ : X →Y is chosen to be the measurement operator A.\nOn the top right: the generator\nGθ : Y →X is chosen to be the pseudo-inverse A†. On the bottom: the generator Hσ : X →Y is\nparametrized by a convolutional operator Hσ(x) = hσ ⋆x.\nGround-truth\nFBP: 21.59, 0.24\nTV: 29.16, 0.77\nU-net: 32.69, 0.87\nLPD: 34.05, 0.89\nUAR: 32.80, 0.86\nFigure 4: CT reconstructions on Mayo Clinic data using model-based (FBP, TV), supervised (U-net,\nLPD), and unsupervised (UAR) methods. The PSNR and SSIM metrics are reported for each\nreconstruction.\nIn Figure 4, we show the experimental results obtained in [64], where UAR is applied to produce\ntomographic reconstructions on the Mayo Clinic low-dose CT grand challenge data-set of abdominal\nCT scans [56], whose sinograms are corrupted by Gaussian noise. We compare UAR to model-\nbased approaches such as the classical filtered back-projection (FBP) and total variation (TV)\n21\nregularization. Additionally, we choose LPD [1] and U-net post-processing [41] as representative\nsupervised methods for inverse problems.\nDifferent choices of the cycle-consistency loss enforce different transitivity properties on the pair\n(Gθ, Hσ) affecting the reconstruction. For example, the cycle-loss (33) imposes a much stronger\nconstraint on the reconstruction compared to (34), potentially undermining the expressive power\nof high-dimensional neural networks. Moreover, the choice of the parameters α and β in (32) that\nregulate the strength of the cycle-loss penalization have a strong impact on the reconstruction.\nThis has been analyzed in [64] for the case where Hσ = A and ℓ(z) = ∥z∥2\n2, showing that when α\nis small, then the reconstruction is very realistic in the sense that W1(πx, (Gθ)#πy) ≈0, but the\nmeasurement-consistency cannot be ensured. Similarly, when α is large, even if cycle consistency is\nensured, the reconstruction is not guaranteed to lie in the data manifold (see Figure 5).\nα=0.001: 21.60, 0.21\nα=0.01: 25.33, 0.37\nα=0.1: 34.65, 0.88\nα=1.0: 33.96, 0.88\nFigure 5: Reconstruction of UAR for different α. For α →0, the unrolled generator (reconstruction\noperator) seeks to find the minimizer of the expected data-fidelity loss, hence the reconstruction\nlooks similar to FBP.\nThese observations have been formalized in [64] in the form of the following theorem.\nTheorem 3.1. Under suitable assumptions on θ and Gθ (see [64, Section 3] for more details) the\nfollowing statements hold:\n1. As α →0, Gθ →Gθ∗\n1 (up to subsequences), where\nθ∗\n1 ∈\narg min\nθ:R\nY∥y−AGθ(y)∥2\n2 dπy=0\nW1(πx, (Gθ)#πy).\n(35)\n2. As α →∞, Gθ →Gθ∗\n2 (up to subsequences), where\nθ∗\n2 ∈\narg min\nθ:(Gθ)#πyδ =πx\nZ\nY\n∥y −AGθ(y)∥2\n2 dπy.\n(36)\nA cycle-GAN-style approach for unsupervised learning of unrolled operators referred to as the\nadversarially learned primal-dual (ALPD), was introduced in [67] and was analyzed under the lenses\nof variational inference. In particular, in [67] the following objective was considered\nmin\nθ\nKL((Gθ)#πy, πx) + C1\nZ\nY\n∥y −A(Gθ(y))∥2\n2 dπy\n+ C2\nZ\nX\n∥x −Gθ(A(x))∥2\n2 dπx.\n(37)\n22\nExample of training data: image and its corresponding projection data (sinogram)\nFBP: 19.51 dB, 0.13\nTV: 29.18 dB, 0.84\nLPD: 27.89 dB, 0.96\nALPD: 28.27 dB, 0.90\nFigure 6: Comparison of supervised and unsupervised training on the Shepp-Logan phantom. The PSNR\n(dB) and SSIM are indicated below the images. ALPD does a better job of alleviating over-smoothing, unlike\nits supervised variant (LPD).\nIt was demonstrated in [67] that under appropriately defined statistical models for πx and πy and\nsuitably chosen constants C1 and C2, the maximum likelihood estimate of the parameter θ leads to\nthe training objective in (37). Moreover, replacing the KL divergence term with the 1-Wasserstein\ndistance leads to a training loss that is identical to the one proposed in [86] in the special case where\nthe forward operator is known. The reconstructed images using the trained model Gθ are shown in\nFigures 6 and 7 for the Shepp-Logan phantom and the low-dose Mayo CT images, respectively. Both\nexperiments reveal that an adversarially trained unrolled operator as proposed in [67] does a better\njob of preserving the image textures better than unrolled operators trained in a supervised manner\nusing the standard squared error loss. This behavior is consistent with the fact that supervised\napproaches trained by minimizing the ℓ2\n2 error effectively produce an approximation to the posterior\nmean of the image conditioned on the data, which is inherently an averaging operator, unlike a\nlikelihood maximization approach.\n3.2\nAdversarial regularization\nAnother notable alternative approach to include a learned regularization in the reconstruction process\nis to first learn an explicit regularization functional in (3) and to solve the resulting variational\nproblem subsequently. One such option is to learn an adversarial regularizer, which was first\nproposed and analyzed in [53] and subsequently specialized to adversarial convex regularizers in [65].\nHere, the construction of a data-driven regularization is inspired by how discriminative networks\n(also referred to as critics, similarly as in the generative machine learning literature) are trained in\nthe WGAN framework.\n23\nGround-truth\nFBP: 21.63 dB, 0.24\nTV: 29.25 dB, 0.79\nAR: 31.83 dB, 0.84\nLPD: 33.39 dB, 0.88\nALPD: 32.48 dB, 0.84\nFigure 7: Comparison of ALPD with some classical model- and data-driven reconstruction methods on\nthe Mayo Clinic data. The corresponding PSNR (dB) and SSIM are indicated below the images and the\nkey differences in the reconstructed images are highlighted. The ALPD reconstruction is visibly sharper as\ncompared to LPD, enabling easier identification of clinically important features.\nTo train such an adversarial regularizer, we assume to have (x(i))N1\ni=1 ∈X and (y(j))N2\nj=1 ∈Y, which are\ni.i.d. samples from the marginal distributions πx and πy of ground-truth images and measurement\ndata, respectively. Additionally, we assume that there exists a (potentially regularizing) pseudo-\ninverse A† : Y →X to the forward operator A and define the measure π† ∈P(X) as π† := A†\n#(πy).\nThen, the idea of adversarial regularization is to train a regularizer gθ, parametrized by a neural\nnetwork, to discriminate between the distributions πx and π†, i.e. between the distribution of\nground-truth images and the distribution of imperfect solutions A†yi (i.e., images with noise and\nartifacts). More concretely, we compute\nRbσ : X →R\nwhere\nbσ ∈arg min\nσ\nL(σ),\n(38)\nwhere L(σ) is chosen as\nL(σ) =\nZ\nX\nRσ(x) dπx −\nZ\nX\nRσ(x) dπ† −λ\nZ\nX\n\u0000∥∇Rσ(x)∥−1\n\u00012\n+ dˆπ\n=\nZ\nX\nRσ(x) dπx −\nZ\nX\nRσ(A†y) dπy −λ\nZ\nX\n\u0000∥∇Rσ(x)∥−1\n\u00012\n+ dˆπ.\n(39)\n24\nHere, ˆπ ∈P(X) is defined by sampling uniformly on the lines connecting samples of πx and samples\nof π†. The heuristic behind this choice is that a regularizer trained this way will penalize noise and\nartifacts generated by the pseudo-inverse (and contained in π†). From a theoretical point of view, one\ncan notice that the minimum of (39) approximates the 1-Wasserstein distance W1(πx, π†) between\nπx and π†. Moreover, the optimal Rbσ approximates the Kantorovich potential for the 1-Wasserstein\ndistance as defined in Section 2.2.3. In particular, the Kantorovich potential for W1(πx, π†) turns\nout to be a good regularizer for the given inverse problem. The resulting regularizer Rbσ is called an\nadversarial regularizer (AR). In practical applications, the measures πx, π† ∈P(X) are replaced with\ntheir empirical counterparts given by the training data samples xi and A†yi, respectively. Suppose,\none computes a gradient step on the learned regularizer, given by xη = x −η ∇xRbσ(x), starting\nfrom x drawn according to π†. Let πη\n† be the distribution of xη. Under appropriate regularity\nassumptions on the 1-Wasserstein distance W1(πη\n† , πx) (see [53, Theorem 1]), one can show that\nd\ndηW1(πη\n† , πx)|η=0 = −\nZ\nX\n∥∇xRbσ(x)∥2 dπ†.\n(40)\nThis ensures that by taking a small enough gradient step, one can reduce the 1-Wasserstein distance\nfrom the ground-truth πx. This is a good indicator that using Rbσ as a variational regularization\nterm and consequently penalizing it implicitly aligns the distribution of regularized solutions with\nthe distribution πx of ground-truth samples. Further, one can show that if the AR is Lipschitz-\ncontinuous1, then for a given noisy measurement yδ ∈Y, a minimizer of the variational problem\nf(yδ, Ax) + λ\n\u0010\nRbσ(x) + ϵ∥x∥2\nX\n\u0011\n,\n(41)\nexists, where the squared norm on x is needed to enforce coercivity.\n3.2.1\nAdversarial convex regularizer (ACR)\nThe adversarial regularizer Rbσ trained in (38) is typically non-convex, due to a typical DNN\nparameterization of Rσ. Nevertheless, it is possible to enforce (strong) convexity on Rσ, leading\nto the adversarial convex regularizer (ACR). The ACR allows for achieving stronger forms of\nconvergence than its non-convex predecessor while precluding discontinuities in the reconstruction\noperator. This necessitates a suitable parameterization of the learned regularizer. One such option\nto impose convexity on Rbθ is to use input convex neural networks [3]. We refer to [65] for more\ndetails on the parameterization of ACRs. Given a so-constructed (and adversarially trained) ACR\n(denoted as Rbσ) that is convex in x, one then considers a regularization functional of the form\nR(x) = Rbσ(x) + ϵ ∥x∥2\nX ,\n(42)\nwhere Rbσ : X →R is the trained ACR, which we assume to be 1-Lipschitz besides being convex in x.\nThe corresponding variational regularization problem then entails minimizing the regularized energy\nf(yδ, Ax) + λR(x),\n(43)\nwith respect to x ∈X. In this setting, we get the following set of improved theoretical guarantees\nfor the ACR, by following standard arguments in variational calculus.\n11-Lipschitz continuity is approximately enforced by the gradient penalty term in (39). However, this does not\nguarantee that the AR is Lipschitz continuous. This property can be instead enforced by choosing the right network\narchitecture. Indeed, all convolutional neural networks with ReLU activations are Lipschitz continuous for some\nLipschitz constant L, which, albeit, might be arbitrarily large.\n25\nTheorem 3.2 (Properties of Adversarial Convex Regularizers [65]).\n1. Existence and uniqueness: The functional in (43) is strongly convex in x and has a unique\nminimizer bxλ (y) for every y ∈Y and λ > 0.\n2. Stability: The optimal solution bxλ (y) is continuous in y.\n3. Convergence: For δ →0 and λ(δ) →0 such that\nδ\nλ(δ) →0, we have that bxλ\n\u0010\nyδ\u0011\nconverges\nto the R-minimizing solution x† given by\nx† ∈arg min\nx∈X\nR(x)\nsubject to\ny0 = Ax.\nDespite strong theoretical guarantees, the numerical experiments in [65] (especially, for sparse-view\nCT reconstruction) indicate a lack of expressive power of ACRs as compared to their nonconvex\ncounterpart AR. This underscores the need to develop techniques that achieve a better compromise\nbetween empirical performance and theoretical certificates. A step in this direction has been made\nvery recently by relaxing convexity to a so-called convex-nonconvex construction of the regularizer\n[85], wherein the regularizer is allowed to be nonconvex while still maintaining convexity of the\noverall variational energy and the classical theoretical guarantees.\n3.2.2\nCombining end-to-end reconstructions and adversarial regularization\nCycle-WGAN models such as UAR and adversarial regularizer (AR) are both unsupervised ap-\nproaches for solving inverse problems while being able to use the knowledge of the measurement\noperator in the reconstruction process. In [64], it has been shown that UAR can be combined\nwith AR to improve the quality of the reconstruction. The key observation is that the adversarial\nregularizer Rbσ : X →R is trained to distinguish samples from the noisy reconstruction (A†)#πy\nfrom samples from the ground-truth πx. Therefore, it is plausible that by substituting A† with a\ngenerator Gbθ learned through UAR, one should be able to improve the noisy reconstruction (A†)#πy\nusing a more accurate reconstruction, given by (Gbθ)#πy and then construct a regularizer based on\nit. The noisy reconstruction (Gbθ)#πy would be an improved guess over (A†)#πy. Following this\nintuition and rewriting the UAR objective [64] as\nmin\nσ max\nθ\nZ\nY\nRσ(Gθ(y)) dπy−\nZ\nX\nRσ(x) dπx + λ\nZ\nX\n(|∇Rσ|(ˆx) −1)2\n+ dˆπ\n+\nZ\nX\nf(Gθ(A(x)), x) dπx,\n(44)\none observes that the optimal Rbσ is trained to distinguish noisy samples of (Gbθ)#πy from samples\nfrom πx and therefore Rbσ is a good regularizer for the distribution (Gbθ)#πy. Moreover, in [64] it\nhas been remarked that since the regularizer Rbσ is an approximation of the Kantorovich potential\nfor W1((Gbθ)#πy, πx), it is possible to compute the derivative of the 1-Wasserstein distance with\nrespect to a GD step as in (40). Indeed, suppose one considers a GD step of the learned regularizer,\ngiven by xη = x −η ∇xRbσ(x), starting from x ∼(Gbθ)#πy. Let πη\nbσ be the distribution of xη. Under\nappropriate regularity assumptions on the 1-Wasserstein distance W1(πη\nbσ, πx) (see [64, Theorem 5]),\none can show that\nd\ndηW1(πη\nbσ, πx)|η=0 = −\nZ\nX\n∥∇xRbσ(x)∥2 dπη\nbσ.\n26\nGbθ(yδ)\n(Gbθ)#πy\n∇Rbσ\nπx\nFigure 8: A schematic illustration of the behavior of the gradient descent step initialized at Gbθ(yδ).\nThe gradient descent is moving the point Gbθ(yδ) in the direction ∇Rbσ(Gbθ(yδ)). Since Rbσ is the\nKantorovich potential for W1((Gbθ)#πy, πx) the step of gradient descent moves Gbθ(y) towards the\nground-truth distribution πx\nThis ensures that by taking a small enough gradient step from samples of (Gbθ)#πy, one can reduce\nthe 1-Wasserstein distance from the ground-truth πx. This is a strong theoretical guarantee that\nRbσ is a good regularizer for the regularized inverse problem\nmin\nx∈X f(yδ, Ax) + λ\n\u0010\nRbσ(x) + ϵ∥x∥2\nX\n\u0011\n,\n(45)\nwhere yδ ∈Y is the noisy measurement. In particular, by taking a few gradient descent steps on\nthe objective in (45), initialized with Gbθ(yδ), we are moving the end-to-end reconstruction Gbθ(yδ)\ntowards the ground-truth distribution πx, c.f. Figure 8. This additional refinement can be seen as a\nprocess of instance adaptation of a given end-to-end reconstruction Gbθ(yδ). In Figure 9 we report\nthe reconstructions obtained in [64] using the UAR approach described in Section 3.1.3 (on the left)\nand the reconstruction obtained by solving (45) performing few steps of GD initialized at Gbθ(yδ)\n(on the right).\nUAR: 32.80 dB, 0.86\nUAR (refined): 33.15 dB, 0.87\nFigure 9: Comparison between cycle-based end-to-end reconstruction obtained by UAR and the\nadditional refinement step. We report the PSNR (dB) and SSIM below the respective images. The\nrefinement step leads to a minor improvement in the quality of the reconstructed image via instance\nadaptation (i.e., by computing the reconstruction corresponding to a specific realization of the\nmeasurement random variable.)\n27\n3.2.3\nThe refinement step in UAR and Brenier’s theorem\nAdversarial regularizers and the refinement step in UAR can be interpreted under the lenses of\nBrenier’s theorem [13] described in Section 3.2.3. Indeed, by computing the gradient descent step\nwith respect to the learned regularizer Rbσ as\nxη = x −η ∇xRbσ(x)\n(46)\nfor either x = A†y (for AR) or x = Gbθ(y) (for UAR), one is effectively trying to compute an\napproximation of the optimal transport map from the end-to-end reconstruction to the ground-truth\nthrough the formula (18).\nUnfortunately, Brenier’s theorem holds only for p-Wasserstein distances with 1 < p < ∞, and\nthus it cannot be applied directly to AR and UAR since they are based on an approximation of\nthe 1-Wasserstein distance. Variants of AR and UAR that use the p-Wasserstein distances with\n1 < p < ∞would allow applying Theorem 2.4, giving the optimal learning step η that should be\nused to compute the optimal transport map. However, such variants would inevitably suffer from\nthe lack of a computationally favorable dual formulation such as the one for the 1-Wasserstein\ndistance. Notably, [59] considers obtaining approximations of the 1-Wasserstein distance through the\ncomputed potential to design a better descent step (46). We refer interested readers to [59] for more\ndetails and to [30] for a more theoretical discussion about the relation between the 1-Wasserstein\ndistance and Kantorovich potentials.\n4\nUnsupervised approaches rooted in convex analysis and monotone operator theory\nIn this section, we give an overview of different unsupervised approaches based on convex analysis\nand monotone operator theory for solving imaging inverse problems, with special emphasis on the\nlearned optimization-based approaches and the plug-and-play (PnP) denoising framework.\n4.1\nLearned optimization solvers\nLearning-to-optimize (L2O) is an emerging area at the interface of optimization and machine\nlearning that has recently started to receive popularity various in data science applications, including\ncomputational imaging. In this section, we review some recent progress in L2O in the context of\ncomputational imaging.\nL2O methods learn to efficiently solve a class of optimization problems, by adapting to the structure\nof the problems and the underlying data distribution. Although L2O has not received strong\nattention in the imaging community compared to related schemes such as PnP/RED, we believe\nthat it will soon become a major area in imaging, due to the recent rise of computationally intensive\nlearned regularizers. Moreover, for each imaging modality, the imaging system is usually fixed or\nalmost fixed, which is a suitable problem setting to use L2O to develop specialized optimization\nalgorithms for imaging in an application-driven manner.\nThe L2O schemes are typically trained in an unsupervised manner, with the goal of accelerating\noptimization on a class of functions of interest, as outlined in the following. Firstly, the users need to\ngenerate a set of training problem instances {fi}n\ni=1, drawn from the problem class of interest, such\nas those arising from model-based natural image inpainting or sinogram denoising. One example\nwould be fi(x) = ∥x−yi∥2 +λ∥∇x∥1, corresponding to the TV-based variational model for denoising\ninduced by a noisy image yi. Let us denote the algorithm to be learned as Aθ(f, x0, N), with θ\n28\nbeing the set of trainable parameters within the algorithm. Here f denotes the objective, x0 denotes\nthe initial point of the algorithm, while the third argument N denotes the number of iterations\nto be executed. The output of the algorithm is denoted as xN = Aθ(f, x0, N). The unsupervised\ntraining objective can typically be written as minimizing the final objective value (averaged over\nthe training problems):\nθ⋆∈arg min\nθ\n1\nn\nn\nX\ni=1\nfi(Aθ(fi, x0, N)),\n(47)\nor minimizing the sum of the function values along the optimization path:\nθ⋆∈arg min\nθ\n1\nn\nn\nX\ni=1\nN\nX\nm=1\nfi(Aθ(fi, x0, m)),\n(48)\nwhere one seeks to minimize the training problems’ objective values as much as possible within N\niterations. Due to the computational complexity in training, N cannot be too large. In the context\nof imaging, the number of unrolling iterations is usually chosen to be on the order of N = 10.\nIn this chapter, we only consider theoretically-principled L2O frameworks which lead to provably\nconvergent algorithms. We will start from the basic scheme of Learned PDHG with trainable\nstep-size parameters [6], to more advanced schemes such as the learned mirror descent (LMD)\nmethods [92], which are based on trainable mirror maps using input-convex neural networks [3].\n4.1.1\nLearned algorithmic parameters\nBanert et al. proposed a learned step-size scheme for the class of primal-dual splitting algorithms\n[6], used to solve composite optimization problems of the form:\nx⋆∈arg min\nx\nf(Ax) + R(x).\n(49)\nIn the context of imaging, f(Ax) is a data-fidelity term incorporating a forward operator A, while\nR(x) is a regularization term (such as TV regularization). The step-size selection in the primal-\ndual splitting scheme has been a challenging problem, since jointly selecting the primal step-size,\ndual step-size, and the extrapolation parameter is difficult in general and significantly affects the\npractical performance [34, 14]. We present a well-known classical primal-dual splitting method, the\nprimal-dual hybrid gradient (PDHG) algorithm of Chambolle and Pock [15]:\nyk+1 = proxσ\nf∗(yk + σAvn),\nxk+1 = proxτ\nR(xk −τA⊤yk+1),\nvk+1 = xk+1 + θ(xk+1 −xk).\nOne could generalize this splitting by parameterizing the scheme as follows, where ⊗denotes the\nKronecker product, and diag(A, B) represents the diagonal operator with operators A, B on the\ndiagonal:\n\"\nwk\nyk+1\n#\n= (A ⊗id) diag(proxσ\nf∗, id)(B ⊗id)\n\"\nAvk\nyk\n#\n\"\nvk\nxk+1\n#\n= (C ⊗id) diag(proxτ\nR, id)(D ⊗id)\n\"\nA⊤wk\nxk\n#\n,\n29\nwhere A, B, C, and D are 2 × 2 matrices consisting of learnable parameters [6]. This formulation\nincludes PDHG as the special case\nA =\n\"\n1\n0\n1\n0\n#\n, B =\n\"\nσ\n0\n0\n1\n#\n, C =\n\"\n1 + θ\n−θ\n1\n0\n#\n, D =\n\"\n−τ\n1\n0\n1\n#\n.\nAs long as the learned parameters are constrained throughout training within the acceptable range\ngiven by the convergence theorems of the primal-dual splitting algorithms, the learned scheme is\nprovably convergent.\n4.1.2\nLearned mirror descent with input-convex neural networks\nIn the previous section, we presented a basic paradigm for provable L2O, by learning the algorithmic\nparameters of classical optimizers such as PDHG, while restricting the learnable parameters such\nthat theoretical guarantees hold. Although such schemes can achieve a certain degree of adaptivity\nand acceleration over classical hand-crafted optimizers while maintaining provable convergence,\ntheir potential is limited as they involve few trainable parameters.\nIn order to fully utilize the training data and make the algorithm adapt well to the inherent\nstructure of the optimization problem class of interest, we wish to leverage the expressive capacity\nof deep neural networks within some classical optimizer in a principled manner, ensuring provable\nconvergence. The classical mirror descent (MD) algorithm by Yudin and Nemirovski is an ideal\ncandidate for such extension by its nature [69]. Before introducing the MD algorithm, we first define\nthe mirror maps as such.\nDefinition 4.1 (Mirror potentials and mirror maps). We define a continuously differentiable and\nstrongly-convex function Ψ : X →R as a mirror potential, and its gradient ∇Ψ : X →(Rn)∗as the\n(forward) mirror map [69, 92].\nDenoting Ψ∗as the convex conjugate of the mirror potential Ψ, and the backward mirror map as\n∇Ψ∗= (∇Ψ)−1, we can write the MD iterates as\nxk+1 = ∇Ψ∗[∇Ψ(xk) −tk∇f(xk)],\n(50)\nor equivalently,\nxk+1 = arg min\nx∈X\n\u001a\n⟨x, ∇f(xk)⟩+ 1\ntk\nBΨ(x, xk)\n\u001b\n.\n(51)\nHere, BΨ(x, y) = Ψ(x)−Ψ(y)−⟨∇Ψ(y), x−y⟩denotes the Bregman distance induced by the mirror\npotential Ψ. Observe that for the choice of mirror potential Ψ(·) = 1\n2∥· ∥2\n2, we recover gradient\ndescent.\nThe MD algorithm naturally lends itself to the L2O setting, since we can parameterize the mirror\npotential using deep neural networks. In particular, by parameterizing Ψ as an input-convex neural\nnetwork (ICNNs) [3], the learned mirror potential is enforced to be a convex function w.r.t. the\ninput, which allows us to inherit the convergence properties of MD. Let the mirror potential Ψ and\nits conjugate Ψ∗be parameterized by two neural networks Mθ and M∗\nϑ, respectively, where the\ncondition M∗\nϑ ≈(Mθ)−1 is enforced through training2. We can describe the learned mirror descent\n(LMD) algorithm as:\n˜xk+1 = ∇M∗\nϑ(∇Mθ(˜xk) −tk∇f(˜xk)).\n(52)\n2Note that the mirror potential should be strongly-convex to ensure provable convergence. We add a small ℓ2 term\nµ\n2 ∥x∥2\n2 to the usual ICNN parameterization to ensure this.\n30\nDue to the inexact inverses, we need to enforce M∗\nϑ ≈(Mθ)−1 for the convergence of LMD. Hence for\nthis framework, we incorporate an additional regularization in the unsupervised training objective\nstated previously in this section, where the inexactness ∥∇M∗\nϑ ◦∇Mθ −I∥is penalized along the\ndistribution pX of the optimized iterates. Denoting the LMD algorithm as Aθ,ϑ, where αms are the\nweights across different iterations, the regularized objective is:\narg min\nθ,ϑ\n1\nn\nn\nX\ni=1\nN\nX\nm=1\nαmfi(Aθ,ϑ(fi, x0, m)) + Ex∼pX[∥(∇M∗\nϑ ◦∇Mθ −I)(x)∥].\n(53)\nUnder standard assumptions in convex optimization, we can provide the following regret bound\nfor LMD which is close to the regret bound for MD, subject to the approximation quality of the\nM∗\nϑ ≈(Mθ)−1 encouraged in the training process.\nTheorem 4.2 (Regret Bound for LMD [92]). Suppose f is µ-strongly convex with parameter µ > 0,\nand Ψ is a mirror potential with strong convexity parameter σ. Let {˜xk}∞\nk=0 be some sequence in\nX = Rn, and {xk}∞\nk=1 be the corresponding exact MD iterates evaluated at ˜xk−1. We have the\nfollowing regret-bound:\nK\nX\nk=1\ntk(f(˜xk) −f(x∗)) ≤\nB(x∗, ˜x1) +\nK\nX\nk=1\n\u0014 1\nσt2\nk∥∇f(˜xk)∥2\n∗+\n\u0012\n1\n2tkµ + 1\nσ\n\u0013\n∥∇Mθ(˜xk+1) −∇Mθ(xk+1)∥2\n∗\n\u0015\n.\n(54)\nFrom this result we can observe that, for the case where M∗\nϑ ≈(Mθ)−1, the term ∥∇Mθ(˜xk+1) −\n∇Mθ(xk+1)∥2\n∗→0 and we recover the standard convergence guarantees for MD. In Figure 10 and\n11, we demonstrate a numerical example of applying LMD on the total-variation (TV) model-based\nimage denoising task. The LMD and the adaptive LMD (a variant of LMD with learned step-sizes\nbesides the learned mirror maps) were trained with unrolling iteration number N = 10.\nWe\ncan observe significantly improved convergence rates of LMD over classical solvers which are not\ndata-driven.\nTo further improve the convergence rates and computational efficiency of LMD, the follow-up\nwork [91] of Tan et al. proposes several extensions utilizing momentum-based acceleration and\nstochastic gradient approximations. We present one of the extensions with the classical Nesterov-type\nacceleration technique in optimization, the learned accelerated mirror descent (LAMD) algorithm in\nAlgorithm 1.\nAlgorithm 1 Learned Accelerated Mirror Descent (LAMD) [91]\nRequire: Input ˜x(0) = ˜z(0) = x(0) ∈X, parameter r ≥3, step-sizes tk, number of iterations K\n1: z(0) = ∇Mθ(˜z(0))\n2: for k = 0, ..., K do\n3:\nx(k+1) = λk∇M∗\nϑ(z(k)) + (1 −λk)˜x(k) with λk =\nr\nr+k\n4:\nz(k+1) = z(k) −ktk\nr ∇f(x(k+1))\n5:\n˜x(k+1) = x(k+1) −γtk∇f(x(k+1))\n6: end for\n7: return x(K+1) = λK∇M∗\nϑ(z(K)) + (1 −λK)˜x(K)\nWith bounded forward-backward inconsistency, an improved convergence rate of LAMD over vanilla\nLMD can be established in a way similar to the classical accelerated MD. In Figure 12, we present\n31\n0\n5\n10\n15\n20\nIterations\n103\n3 × 102\n4 × 102\n6 × 102\nReconstruction loss\nAdaptive LMD\nLMD\nGD\nAdam\nStep-size multi\n1/4\n1/2\n1\n2\n4\nFigure 10: Convergence results of LMD and traditional optimizers in TV denoising problem (see\n[92]).\nAdaptive LMD\n(3 iterations)\nAdam\n(3 iterations)\nAdam\n(10 iterations)\nFigure 11: Recovered images by LMD and Adam. We can observe visually the LMD achieves much\nfaster convergence over Adam with fewer artifacts in early iterations. See [92] for further details.\nnumerical results of LAMD in TV model-based denoising, comparing it to learned solvers such\nas LMD and LPDHG, as well as the classical optimizers such as gradient descent with Nesterov\nacceleration. We can observe the superior performance of LAMD in this example.\n4.2\nPlug-and-play methods and data-driven regularization\nDenoising is the simplest and arguably the most well-studied inverse problem in imaging, with\nnumerous algorithms being developed over the past few decades, particularly for removing additive\nwhite Gaussian noise. A natural question is whether one can leverage off-the-shelf denoisers for\nsolving more complicated image recovery tasks with a non-trivial forward operator (that is different\nfrom the identity). Venkatakrishnan et al. [98] pioneered the idea of using denoisers within proximal\nsplitting algorithms (such as ADMM) in a plug-and-play (PnP) fashion, resulting in a class of\n32\nFigure 12: Example convergence profiles for the learned mirror descent algorithm (LMD), the learned\naccelerated mirror descent algorithm (LAMD), the learned PDHG method (LPDHG), and gradient\ndescent with learned step-sizes (LGD). The target optimization problem arises from TV model-based\ndenoising. We observe that all learned methods are significantly faster than the corresponding\ngradient descent and Nesterov accelerated gradient descent algorithms in the low iteration regime.\nHowever, without special parameter choices for LPDHG, it does not converge to a minimizer.\nalgorithms known as the PnP denoising approach.\nTo motivate replacing proximal operators\nwith denoisers, let us recall the definition of the proximal operator with respect to a (potentially\nnon-smooth) convex functional g : X →R ∪{+∞} and a step-size τ > 0:\nproxτg(x) = arg min\nu\n1\n2∥x −u∥2 + τg(u).\n(55)\nAs indicated by (55), evaluating the proximal operator amounts to denoising a noisy image x using\nthe Bayesian maximum a-posteriori probability (MAP) estimation framework with a Gibbs prior\n∝exp (−τg(u)). This denoising interpretation of proximal operators underlies the foundation of\nPnP approaches, which have been shown to produce excellent reconstruction results for a wide\nrange of imaging inverse problems. A classic and widely popular example of PnP denoising would\nbe to consider it in conjunction with forward-backward splitting (FBS), leading to the following\niterative reconstruction algorithm:\nxk+1 = Dσ (xk −ηk∇f(xk)) .\n(56)\nHere, f denotes the data fidelity loss for the underlying inverse problem, ηk > 0 is the step-size at\niteration k, and Dσ is a denoiser that eliminates Gaussian noise of standard deviation σ from its\ninput.\nBesides the PnP denoising framework within proximal methods, wherein a denoiser implicitly acts\nas a regularizer, Romano et al. [78] proposed an alternative approach to explicitly construct a\nregularizing term from a denoiser Dσ(x) as\ng(x) = 1\n2x⊤(x −Dσ(x)) .\n(57)\n33\nOne can then seek to minimize the energy functional f(x) + λ g(x), where g is as defined in (57),\nleading to fixed-point iterative schemes known as the regularization-by-denoising (RED) algorithms.\nNevertheless, it was shown subsequently by Schniter et al. [75] that the energy minimization\ninterpretation of the RED algorithms is valid only when (i) the denoiser is locally homogeneous, i.e.,\nDσ ((1 + ϵ)x) = (1 + ϵ)Dσ(x) holds for all x with sufficiently small ϵ, and (ii) the Jacobian of Dσ is\nsymmetric. These conditions are generally not satisfied by generic denoisers, thereby invalidating\nthe energy minimization-based interpretation of RED. Instead, the authors of [75] developed a new\nframework called score-matching to analyze the convergence of RED algorithms.\nNotwithstanding their empirical success, PnP denoising algorithms such as (56) do not immediately\ninherit the convergence properties of the corresponding optimization scheme, such as FBS in the\nprevious example. Studying the convergence of PnP denoising has received a significant amount of\nattention in the mathematical imaging community in recent years. Arguably, the most natural form\nof convergence for PnP algorithms of the form (56) is the stability of the iterations, ascertaining\nwhether the sequence of iterates xk generated by a PnP algorithm converges. Such convergence\nguarantees are typically derived from fixed point theorems, which require showing that the PnP\niterations are contractive maps [16, 79]. For instance, [79] established the fixed-point convergence\nof PnP-ADMM (i.e., PnP with the alternating direction method of multipliers algorithm) under\nthe assumption of Lipschitz continuity of the operator (Dσ −id). The specific result is stated in\nTheorem 4.3.\nTheorem 4.3 (Fixed-point convergence of PnP-ADMM [79]). Consider the PnP-ADMM algorithm,\ngiven by\nxk+ 1\n2 = proxτf (zk) ,\nxk+1 = Dσ\n\u0010\n2xk+ 1\n2 −zk\n\u0011\n, and\nzk+1 = zk + xk+1 −xk+ 1\n2 ,\n(58)\nwhere the data-fidelity loss f is assumed to be µ-strongly convex. One can equivalently express (58)\nas the fixed-point iteration zk+1 = T (zk), where\nT = 1\n2 id +1\n2 (2Dσ −id)\n\u0010\n2 proxτf −id\n\u0011\n.\n(59)\nSuppose that the denoiser Dσ satisfies\n∥(Dσ −id) (u) −(Dσ −id) (v)∥2 ≤ϵ ∥u −v∥2 ,\n(60)\nfor all u, v ∈X and some ϵ > 0, and the strong convexity parameter µ is such that\nϵ\n(1 + ϵ −2ϵ2) µ < τ\nholds. Then the operator T is contractive and the PnP-ADMM algorithm is fixed-point convergent.\nThat is, (xk, zk) →(x∞, z∞), where (x∞, z∞) satisfy\nx∞= proxτf (z∞) and x∞= Dσ (2x∞−z∞) .\n(61)\nAs noted in [79], fixed-point convergence of PnP-ADMM follows from monotone operator theory if\n(2Dσ −id) is non-expansive, but (60) imposes a less restrictive condition on the denoiser.\nWhile fixed-point convergence ensures that the PnP iterations are stable, the specific fixed point\nto which they converge does not naturally minimize a variational energy function. To bridge the\ngap between classical variational approaches and PnP methods, it is important to derive conditions\nunder which the limit point of PnP iterations can be characterized as the minimizer (or, at least\n34\na stationary point) of some regularized variational objective (which, of course, depends on the\ndenoiser). This type of convergence is referred to as objective convergence and is stronger than\nfixed-point convergence.\nObjective convergence of PnP with classical (pseudo-) linear denoisers (e.g., non-local means denoiser)\nhas been established in [68]. Hurault et al. [40] showed that PnP with a denoiser constructed as a\ngradient field, referred to as gradient-step (GS) denoisers, converges to the stationary point of a\n(possibly non-convex) variational objective (c.f. Theorem 4.4). The construction of GS denoisers\nis motivated by Tweedie’s identity; the optimal minimum mean-squared error (MMSE) Gaussian\ndenoiser is given by\nD∗\nσ(x) := E [x0|x = x] = x + σ2 ∇log pσ(x).\n(62)\nHere, x = x0 + σ w, where w ∼N(0, I), is the Gaussian-noise corrupted version of the clean image\nx0 ∈Rd and\npσ(x) =\n1\n(2πσ2)\nd\n2\nZ\nexp\n \n−∥x −x0∥2\n2\n2σ2\n!\np(x0) dx0.\n(63)\nIndeed, the optimal Gaussian denoiser is of the form D∗\nσ(x) = x −∇g∗\nσ(x), where g∗\nσ is the\nnegative log-density of the smoothed distribution pσ defined in (63). This has a structure identical\nto that of a GS denoiser, parameterized as Dσ(x) = x −∇gσ(x). It was argued in [40] that\ndirectly parameterizing gσ using a deep neural network does not lead to state-of-the-art denoising\nperformance, but instead, modeling gσ as gσ(x) = 1\n2 ∥x −Nσ(x)∥2\n2 for a differentiable network Nσ(x)\nproduces superior denoising performance. The denoiser is trained by minimizing the MSE, given\nby J := Ex,w ∥Dσ(x + σ w) −x∥2\n2, where w ∼N(0, I), approximated over the training dataset\nconsisting of the ground-truth images and their noisy counterparts.\nTheorem 4.4 (Objective convergence of PnP iterations [40]). Suppose the denoiser is a gradient-step\n(GS) denoiser Dσ = id −∇gσ, where gσ is proper, lower semi-continuous, and differentiable with\nL-Lipschitz gradient. The GS-PnP algorithm proposed in [40] is given by\nxk+1 = proxτf (xk −τλ∇gσ(xk))\n= proxτf ◦(τλ Dσ + (1 −τλ id)) (xk),\n(64)\nwhere f : X →R ∪{+∞} is a convex and lower semi-continuous data-fidelity term. Then, the\nfollowing guarantees hold for τ <\n1\nλ L:\n1. The sequence F(xk), where F = f + λ gσ, is non-increasing and convergent.\n2. The residual ∥xk+1 −xk∥2 converges to 0.\n3. All limit points of {xk} are stationary points of F(x).\nNotably, the PnP iteration defined by (64) is exactly equivalent to proximal gradient descent on\nf + λ gσ, with a potentially non-convex gσ.\nWhile objective convergence ensures a one-to-one connection between PnP iterates with the min-\nimization of a variational objective, it does not provide any guarantees about the regularizing\nproperties of the solution that the iterates converge to. In the same spirit as classical regularization\ntheory, it is therefore desirable to be able to control the implicit regularization effected by the\ndenoiser in PnP algorithms and analyze the asymptotic behavior of the PnP reconstruction as\nthe noise level and the regularization strength tend to vanish. More precisely, assuming that the\n35\n0\n15\n30\n45\n60\n75\n90\nk (iter)\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\nmin\ni\nk ||xi + 1\nxi||2/||x0||2\n(1\nK)\n( 1\nK2)\nPnP-LBFGS\nPnP-FISTA\n0\n150\n300\n450\n600\n750\n900\nk (iter)\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\nmin\ni\nk ||xi + 1\nxi||2\n(1\nK)\n( 1\nK2)\nPnP-αPGD\n0\n150\n300\n450\n600\n750\n900\nk (iter)\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\nmin\ni\nk ||xi + 1\nxi||2/||x0||2\n(1\nK)\n( 1\nK2)\nPnP-PGD\n0\n150\n300\n450\n600\n750\n900\nk (iter)\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\nmin\ni\nk ||xi + 1\nxi||2/||x0||2\n(1\nK)\n( 1\nK2)\nPnP-DRS\n0\n150\n300\n450\n600\n750\n900\nk (iter)\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\nmin\ni\nk ||xi + 1\nxi||2/||x0||2\n(1\nK)\n( 1\nK2)\nPnP-DRSDiff\nFigure 13: Convergence of the residuals mini≤k ∥xi+1 −xi∥2/∥x0∥2 for various PnP methods applied\nto image deblurring. Each curve corresponds to one image from the CBSD10 dataset, corrupted\nwith 3% additive Gaussian noise. Except for PnP-FISTA, stationary points of these PnP methods\nare critical points of a weakly convex function, corresponding to the noisy image and the denoiser.\nPnP iterations converge to a solution ˆx\n\u0010\nyδ, σ, λ\n\u0011\n, where σ is a parameter associated with the\ndenoiser and λ is an explicit regularization penalty, one would like to obtain appropriate selection\nrules for σ and/or λ such that ˆx\n\u0010\nyδ, σ, λ\n\u0011\nis a convergent regularization scheme in the limit as\nδ →0. To the best of our knowledge, some progress in this direction was first made in [27], and\nthe precise convergence result is stated in Theorem 4.5. A similar convergence result for PnP\nmethods in the sense of regularization was shown in [38] considering linear denoisers, together with\na systematic approach based on spectral filtering for controlling the regularization effect arising from\nsuch denoisers.\nTheorem 4.5 (Convergent plug-and-play (PnP) regularization [27]). Consider the PnP-FBS iterates\nof the form\nxδ\nλ,k+1 = Dλ\n\u0010\nxδ\nλ,k −η A∗\u0010\nAxδ\nλ,k −yδ\u0011\u0011\n,\n(65)\nwhere Dλ is a denoiser with a tuneable regularization parameter λ. Let PnP\n\u0010\nλ, yδ\u0011\nbe the fixed\npoint of the PnP iteration (65). For any y ∈range(A) and any sequence δk > 0 of noise levels\nconverging to 0, there exists a sequence λk of regularization parameters converging to 0 such that for\nall yk with ∥yk −y0∥2 ≤δk, the following hold under appropriate assumptions on the denoiser (see\nDefinition 3.1 in [27] for details):\n1. PnP\n\u0010\nλ, yδ\u0011\nis continuous in yδ for any λ > 0;\n2. The sequence (PnP (λk, yk))k∈N has a weakly convergent subsequence; and\n36\n3. The limit of every weakly convergent subsequence of (PnP (λk, yk))k∈N is a solution of the\noperator equation y0 = Ax.\nGround-Truth\nPnP-LBFGS(29.78dB)\nPnP-PGD (28.68dB)\nCorrupted\nPnP-DRSdiff (28.66dB)\nPnP-DRS (29.39dB)\nFigure 14: Deblurring visualization using starfish image, with each method limited to a maximum of\n100 iterations. Experiments are run with additive Gaussian noise σ = 7.65. PnP-LBFGS converges\nwithin the first 100 iterations, while the other PnP algorithms take longer to converge.\nIn Figure 13 and 14, we present some numerical results from [93] on applying provably convergent\nPnP algorithms including PnP-LBFGS, PnP-PGD, PnP-DRS etc, on image deblurring task for\nillustration, more details can be found in the referenced paper.\n5\nVarious ground-truth-free approaches for image reconstruction\nIn this section, we briefly survey some other closely related unsupervised training strategies for\nimaging inverse problems. The frameworks described here are mostly suitable for the cases where\nwe have limited training data for the networks, for example, in medical tomographic imaging\nwe could have plenty of noisy sinogram measurement data from the imaging devices, but a very\nlimited amount of data for ground-truth images. Strictly speaking, there are sometimes no actual\n“ground-truths” in practice, making the use of unsupervised schemes necessary.\n5.1\nDeep image prior\nOne of the popular and empirically successful unsupervised approaches for imaging is the deep image\nprior (DIP) method [97]. Surprisingly, this approach requires no training data, relying completely\non the regularization effect of the architecture of the deep CNNs and implicit regularization of the\ngradient-based optimizers [90]. Let us denote a neural network such as a U-net by Gθ : Rd′ →Rd,\nwhich can be either untrained or pretrained, parameterizing the image to be reconstructed. For an\n37\narbitrary vector z, the DIP scheme can be written as minimizing approximately:\nθ⋆≈arg min\nθ\n∥y −AGθ(z)∥2\n2,\n(66)\nwith some first-order methods such as Adam, with early-stopping to avoid overfitting. The final\nreconstruction is then computed as x⋆= Gθ⋆(z). While letting z be chosen as a Gaussian random\nvector produces reasonable results, it has been observed that warm-starting by choosing z to be the\ncorrupted image input leads to better results. For example, when applying DIP in denoising, it is\nbetter to choose z to be the noisy input image itself for faster convergence and improved results,\nas observed by Tachella et al [90]. This work also demonstrates that the success of DIP is due to\nthe implicit regularization by the network architecture and the dynamics of the gradient-based\noptimizer.\nDespite the nonstandard reconstruction method, the DIP approach demonstrates remarkable\nnumerical performance without any training data, even in highly ill-posed inverse problems such as\ninpainting with many missing pixels. Although this scheme is usually numerically inferior compared\nto fully-supervised schemes, the DIP approach demonstrates that the implicit regularization jointly\nformed by the architecture and gradient-based optimization is already a very strong regularization\nfor imaging. Moreover, it can be jointly applied with classical variational regularization methods and\nplug-and-play priors introduced in the previous subsections for even better reconstruction results.\nFor example, the DIP-TV approach [50]\nθ⋆≈arg min\nθ\n∥y −AGθ(z)∥2\n2 + µ∥∇Gθ(z)∥1,\n(67)\nand the DIP-RED approach [55], given by\nθ⋆≈arg min\nθ\n∥y −AGθ(z)∥2\n2 + µ Gθ(z)⊤(Gθ(z) −Dλ(Gθ(z))),\n(68)\nboth fall within the category of combining DIP with additional prior terms. With the assistance of\nadditional regularization, the performance of DIP is often improved, and the need for early stopping\nis alleviated if the regularization parameter µ is appropriately chosen.\n5.2\nNoise-2-X methods\nThe Noise2Noise scheme takes two distinct noisy observations of natural images for training denoisers\nwithout ground-truth image, by taking one of the noisy observations as a “ground-truth” in the\nfidelity term [48]. An interesting class of similar ground-truth-free unsupervised training schemes\nhas been developed, such as Noise2Self [7], Noise2Void [47], Noisier2Noise [63], and many other\nrelated schemes [74, 46, 39]. We refer to this class of training schemes as the Noise-2-X methods.\nGiven a collection of noisy/corrupted images {ˆxi}n\ni=1 and a neural network to train, typically deep\nCNNs or U-nets, the Noise-2-X schemes train the reconstruction network on unsupervised losses of\nthe form:\nθ⋆≈arg min\nθ\n1\nn\nn\nX\ni=1\n∥ˆyi −Gθ(ˆxi)∥2\n2,\n(69)\nwhere {ˆxi, ˆyi}n\ni=1 are pairs of noisy perturbations of the inaccessible ground-truth images {xi}n\ni=1.\nDifferent noise-2-X schemes consider different choices of such perturbations. The aim of using pairs\nof perturbations is to use (69) to approximate the supervised loss\nθ⋆≈arg min\nθ\n1\nn\nn\nX\ni=1\n∥xi −Gθ(ˆxi)∥2\n2,\n(70)\n38\nin the absence of ground-truth images xi. For example, consider the denoising problem yi = xi + εi\nwhere εi denotes additive Gaussian noise. The unsupervised loss can be written as 1\nn\nPn\ni=1 ∥yi −\nGθ(ˆxi)∥2\n2 = 1\nn\nPn\ni=1 ∥xi + εi −Gθ(ˆxi)∥2\n2. The gradient of this approximation is an unbiased estimate\nof the gradient for the supervised loss above, and such an approximation becomes increasingly\naccurate as the sample size n increases. Similar to the DIP, denoising networks based on noise-2-X\nschemes are also trained using gradient-based optimization algorithms such as Adam or SGD.\nIn imaging tasks such as natural image denoising, these unsupervised training schemes demonstrate\nreasonably good performance, closely matching the performance of denoising networks with fully\nsupervised training. Combined with the plug-and-play schemes we have introduced before, the\ndenoisers trained by these noise-2-X schemes can be also applied to solve more sophisticated imaging\ninverse problems such as deblurring, inpainting, and tomographic reconstruction in the absence of\nany noise-free ground-truth images.\n5.3\nEquivariant imaging\nIn certain imaging applications such as CT or MRI reconstruction, we often only have low-quality\nmeasurements {yi}n\ni=1 without any ground-truth images. This situation restricts the use of supervised\ntraining, where synthetic data is instead used. In such cases, the quality of the measurements\nsignificantly affects the training quality of brute-force unsupervised training:\nθ⋆≈arg min\nθ\n1\nn\nn\nX\ni=1\n∥yi −AGθ(yi)∥2\n2.\n(71)\nThis unsatisfactory training is due to the difficulty of learning in the presence of highly non-trivial\nnull-spaces. To mitigate this, Chen et al. [19] proposed the Equivariant Imaging (EI) framework,\nutilizing the equivariant structure of the forward operator to improve the performance of the\nunsupervised training in this context. More precisely, in the majority of imaging inverse problems,\nthe plausible set of images I are invariant to a certain group of transformations G = {g1, g2, ..., g|G|}\nwith actions Tg such that Tgx ∈I for all x ∈I. For example, natural images are usually invariant to\nshift operations, while CT/MRI images are usually invariant to rotations. Exploiting this structure\nof the plausible image set, the desired neural network solution should approximately satisfy:\nGθ(ATgx) = TgGθ(Ax).\n(72)\nThe composite map hθ ◦A should be equivariant under the transformations Tg, meaning that the\noperators commute. This leads to the EI training framework:\nθ⋆\n≈\narg min\nθ\n1\nn\nn\nX\ni=1\n∥yi −AGθ(yi)∥2\n2 + µEg∈G\nh\n∥Gθ(ATgGθ(yi)) −TgGθ(AGθ(yi))∥2\n2\ni\n. (73)\nThis is the unsupervised training loss with the addition of a regularization term that encourages\nthe network to utilize the equivariant structure of the imaging problem. Akin to the previously\nintroduced unsupervised methods, gradient-based optimization solvers such as Adam are applied\nfor training, with an extra computational overhead due to the sophisticated regularization term.\nAlthough training using EI is more computationally expensive and requires much more memory\ncompared to a brute-force approach, this framework demonstrates remarkable numerical potential\nand can match the accuracy of fully supervised approaches closely [19]. The EI framework can\nenable practitioners to train advanced reconstruction networks such as FBP-ConvNet and deep\nunrolling networks from only the measurement data without the ground-truth images.\n39\n5.4\nStein’s unbiased risk estimation (SURE)\nAn unsupervised learning approach based on Stein’s unbiased risk estimation (SURE) [88] was\nproposed by Metzler et al. [58]. The estimation problem considered in [58] was that of recovering\nan image x ∈Rn from its linearly degraded measurement y = Ax + w, where w is Gaussian with\nmean zero and covariance σ2\nwI. Then, it can be shown that\nJ(θ) := Ew\n\u0014 1\nn∥y −Gθ(y)∥2\n2\n\u0015\n−σ2\nw + 2σ2\nw\nn divy (Gθ(y)) ,\n(74)\nwhere div denotes the divergence operator, is an unbiased estimator of the mean-squared error\n(MSE) Ew\n\u0014 1\nn∥x −Gθ(y)∥2\n2\n\u0015\n. Since approximating J(θ) requires only the measured data and not\nthe corresponding ground-truth images, it serves as a surrogate loss for MSE and results in an\nunsupervised learning framework. To approximate the divergence term, the authors of [58] adopted\na Monte Carlo-based approach that relies on the following:\ndivy (Gθ(y)) = lim\nϵ→0 Eu\n\u0014\nu⊤\n\u0012Gθ(y + ϵu) −Gθ(y)\nϵ\n\u0013\u0015\n,\n(75)\nwhere u ∼N(0, I). A similar unbiased estimator of the MSE can be derived for noise distributions\nin the exponential family. SURE can be utilized as a general framework that can turn any generic\nsupervised MSE-based learning approach (for instance, a bilevel learning framework) into an\nunsupervised one by replacing the MSE with its SURE-based estimate.\n5.4.1\nRobust equivariant imaging via SURE\nThe EI unsupervised training framework introduced in the previous subsection can also be further\nimproved in terms of robustness to measurement noise by incorporating SURE, as shown in the work\nof Chen et al [20]. There is a major weakness of the EI approach regarding the fragility towards\nmeasurement noise, such that as the measurement noise increases, the performance of EI would\nexperience very significant decay. An effective remedy for this issue turns out to be utilizing the\nSURE loss (74). This modified robust EI framework can be summarized as the following objective:\nθ⋆≈arg min\nθ\n1\nn\nn\nX\ni=1\n∥yi −AGθ(yi)∥2\n2 + 2σ2divyi (Gθ(yi))\n+ µ Eg∈G[∥Gθ(ATgGθ(yi)) −TgGθ(AGθ(yi))∥2\n2].\nAccording to (75), when training the reconstruction networks using gradient-based methods, the\ndivergence term can be simply approximated by:\ndivy (Gθ(y)) ≈u⊤\n\u0012Gθ(y + εu) −Gθ(y)\nε\n\u0013\n,\n(76)\nin each iteration, with u ∼N(0, I) while ε being chosen to be a small constant. With this modified\nloss, the resulting reconstruction networks can closely match fully supervised methods even when\nthe noise in the measurement is significant.\n6\nSummary and conclusions\nUnsupervised learning is a powerful method of performing machine learning in the absence of\ncomplete ground-truth data, such as unpaired training examples, and access to samples of only\n40\nthe ground-truth images, or of only noisy measurements. We presented three paradigms, namely\nprobabilistic approaches based on optimal transport and cycle architectures, learned priors through\nlearning-to-optimize and plug-and-play, as well as various ways of inserting prior knowledge for\nregularization. Each of these paradigms requires some prior knowledge, such as a degradation model\nor probabilistic interpretation. Nonetheless, such models have been shown to be competitive with\nsupervised models, and are applicable to more general classes of problems.\nIn Section 3, we reviewed unsupervised approaches based on optimal transport, particularly the\nCycle-WGAN approach consisting of two WGANs in opposite directions and the adversarial\nregularization method where a regularizer is parameterized using a neural network and learned\nadversarially. Both approaches aim to minimize a Wasserstein distance between distributions\ninduced by the learned components, and an approach combining both Cycle-WGAN approaches\nand adversarial regularization was discussed in Section 3.2.2. These approaches have the benefit\nof having a probabilistic interpretation, where the distribution of the generated or reconstructed\ndata lives in a certain neighborhood of the ground-truth distribution. This lies in the intersection of\nlearning the prior and posterior distributions, and can also be related to semi-supervised learning,\nwhere there is an imbalance of measurements and ground-truths.\nSeveral convex analysis-based methods for unsupervised learning were presented in Section 4. In\nparticular, the learning-to-optimize, which accelerates model-based reconstruction, was considered\nin Section 4.1. Plug-and-play methods for image reconstruction tasks, where an image prior is\nimplicitly defined by a pre-trained Gaussian denoiser were considered in Section 4.2. Section 5\ndetailed several training methods for one-shot image reconstruction such as using the deep image\nprior, or various methods for training denoisers in the absence of ground-truth data.\nIn this review, we focused on works that derive from classical results in optimal transport and convex\nanalysis. However, the scope of unsupervised learning is much broader once this restriction is lifted.\nNotable examples include physics-informed neural networks, which aim to learn physical operators\nsuch as PDEs or dynamical systems [45]. While a lot of theory already exists for unsupervised\nlearning, we believe that the following few issues are particularly important for closing the gap\nbetween unsupervised and supervised methods:\n1. There is an inherent difference in information available in the supervised regime compared to\nthe unsupervised regime. Some works already seek to quantify this, such as [89] that rephrases\nthe EI framework in terms of compressed sensing, and derives bounds for signal recovery based\non classical theorems. An interesting direction would be quantifying the performance difference\ninduced by this information gap, as well as in suitable limiting cases.\n2. Unsupervised methods were categorized into three main classes as in Section 2.4.2, and all these\nformulations assume some sort of prior information into the model. The works presented in this\nreview are based on classical results in optimal transport and convex analysis, allowing for some\ntheoretical analysis. A more theoretical framework for building unsupervised models, utilizing\nprobabilistic or geometric ideas, could lead to more efficient usage of data and help close the gap\nbetween supervised and unsupervised methods.\nReferences\n[1] Jonas Adler and Ozan Öktem. Learned primal-dual reconstruction. IEEE transactions on\nmedical imaging, 37(6):1322–1332, 2018.\n41\n[2] Luigi Ambrosio, Alberto Bressan, Dirk Helbing, Axel Klar, Enrique Zuazua, Luigi Ambrosio,\nand Nicola Gigli. A user’s guide to optimal transport. Modelling and Optimisation of Flows\non Networks: Cetraro, Italy 2009, Editors: Benedetto Piccoli, Michel Rascle, pages 1–155,\n2013.\n[3] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International\nConference on Machine Learning, pages 146–155, 2017.\n[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial\nnetworks. In Proceedings of the 34th International Conference on Machine Learning, pages\n214–223, 2017.\n[5] Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, and Paul Hand.\nInvertible\ngenerative models for inverse problems: mitigating representation error and dataset bias. In\nInternational Conference on Machine Learning, pages 399–409. PMLR, 2020.\n[6] Sebastian Banert, Axel Ringh, Jonas Adler, Johan Karlsson, and Ozan Oktem. Data-driven\nnonsmooth optimization. SIAM Journal on Optimization, 30(1):102–131, 2020.\n[7] Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In International\nConference on Machine Learning, pages 524–533. PMLR, 2019.\n[8] Georgios Batzolis, Marcello Carioni, Christian Etmann, Soroosh Afyouni, Zoe Kourtzi, and\nCarola Bibiane Schönlieb.\nCAFLOW: conditional autoregressive flows.\narXiv preprint\narXiv:2106.02531, 2021.\n[9] Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator theory\nin Hilbert spaces, volume 408. Springer, 2011.\n[10] Amir Beck and Marc Teboulle.\nA fast iterative shrinkage-thresholding algorithm with\napplication to wavelet-based image deblurring.\nIn 2009 IEEE International Conference\non Acoustics, Speech and Signal Processing, pages 693–696. IEEE, 2009.\n[11] Martin Benning and Martin Burger. Modern regularization methods for inverse problems.\nActa Numerica, 27:1–111, 2018.\n[12] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using\ngenerative models. In International conference on machine learning, pages 537–546. PMLR,\n2017.\n[13] Yann Brenier. Décomposition polaire et réarrangement monotone des champs de vecteurs.\nCR Acad. Sci. Paris Sér. I Math., 305:805–808, 1987.\n[14] Antonin Chambolle, Claire Delplancke, Matthias J Ehrhardt, Carola-Bibiane Schönlieb, and\nJunqi Tang. Stochastic primal dual hybrid gradient algorithm with adaptive step-sizes. arXiv\npreprint arXiv:2301.02511, 2023.\n[15] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems\nwith applications to imaging. Journal of mathematical imaging and vision, 40:120–145, 2011.\n[16] Stanley H. Chan, Xiran Wang, and O. A. Elgendy. Plug-and-play ADMM for image restoration:\nFixed-point convergence and applications. IEEE Transactions on Computational Imaging,\n3(1):84–98, 2017.\n42\n[17] D Aliprantis Charalambos and Kim Border. Infinite Dimensional Analysis: A Hitchhiker’s\nGuide. Springer-Verlag Berlin and Heidelberg GmbH & Company KG, 2013.\n[18] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized\ngenerative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.\n[19] Dongdong Chen, Julián Tachella, and Mike E Davies. Equivariant imaging: Learning beyond\nthe range space. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 4379–4388, 2021.\n[20] Dongdong Chen, Julián Tachella, and Mike E Davies. Robust equivariant imaging: a fully\nunsupervised framework for learning to image from noisy and partial measurements. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n5647–5656, 2022.\n[21] Hu Chen, Yi Zhang, Weihua Zhang, Peixi Liao, Ke Li, Jiliu Zhou, and Ge Wang. Low-dose ct\nvia convolutional neural network. Biomedical optics express, 8(2):679–694, 2017.\n[22] Il Yong Chun, Zhengyu Huang, Hongki Lim, and Jeff Fessler. Momentum-net: Fast and\nconvergent iterative neural network for inverse problems. IEEE transactions on pattern\nanalysis and machine intelligence, 2020.\n[23] Giannis Daras, Joseph Dean, Ajil Jalal, and Alexandros G Dimakis. Intermediate layer opti-\nmization for inverse problems using deep generative models. arXiv preprint arXiv:2102.07364,\n2021.\n[24] Masoumeh Dashti and Andrew M Stuart. The Bayesian approach to inverse problems. In\nHandbook of uncertainty quantification, pages 311–428. Springer, 2017.\n[25] Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm\nfor linear inverse problems with a sparsity constraint. Communications on Pure and Applied\nMathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 57(11):1413–\n1457, 2004.\n[26] Jim Douglas and Henry H Rachford. On the numerical solution of heat conduction problems in\ntwo and three space variables. Transactions of the American mathematical Society, 82(2):421–\n439, 1956.\n[27] Andrea Ebner and Markus Haltmeier. Plug-and-play image reconstruction is a convergent\nregularization method, 2022.\n[28] Ivar Ekeland and Roger Temam. Convex analysis and variational problems. SIAM, 1999.\n[29] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems,\nvolume 375. Springer Science & Business Media, 1996.\n[30] Lawrence C Evans and Wilfrid Gangbo.\nDifferential equations methods for the Monge-\nKantorovich mass transfer problem. American Mathematical Soc., 1999.\n[31] Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with Sinkhorn\ndivergences.\nIn International Conference on Artificial Intelligence and Statistics, pages\n1608–1617. PMLR, 2018.\n43\n[32] Davis Gilton, Greg Ongie, and Rebecca Willett. Neumann networks for linear inverse problems\nin imaging. IEEE Transactions on Computational Imaging, 6:328–343, 2019.\n[33] Davis Gilton, Gregory Ongie, and Rebecca Willett. Deep equilibrium architectures for inverse\nproblems in imaging. IEEE Transactions on Computational Imaging, 7:1123–1133, 2021.\n[34] Tom Goldstein, Min Li, and Xiaoming Yuan. Adaptive primal-dual splitting methods for\nstatistical learning and image processing. Advances in neural information processing systems,\n28, 2015.\n[35] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural\ninformation processing systems, 27, 2014.\n[36] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings\nof the 27th international conference on international conference on machine learning, pages\n399–406, 2010.\n[37] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.\nImproved training of Wasserstein GANs. In Advances in Neural Information Processing\nSystems, volume 30, pages 5769—-5779, 2017.\n[38] Andreas Hauptmann, Subhadip Mukherjee, Carola-Bibiane Schönlieb, and Ferdia Sherry.\nConvergent regularization in inverse problems and linear plug-and-play denoisers, 2023.\n[39] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor:\nSelf-supervised denoising from single noisy images. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 14781–14790, 2021.\n[40] Samuel Hurault, Arthur Leclaire, and Nicolas Papadakis. Gradient step denoiser for convergent\nplug-and-play. CoRR, abs/2110.03220, 2021.\n[41] Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, and Michael Unser.\nDeep\nconvolutional neural network for inverse problems in imaging. IEEE Transactions on Image\nProcessing, 26(9):4509–4522, 2017.\n[42] Jari Kaipio and Erkki Somersalo. Statistical and computational inverse problems, volume 160.\nSpringer Science & Business Media, 2006.\n[43] Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, and Jong Chul Ye. Cycle-\nconsistent adversarial denoising network for multiphase coronary ct angiography. Medical\nphysics, 46(2):550–562, 2019.\n[44] Leonid V Kantorovich. On the translocation of masses. Journal of mathematical sciences,\n133(4):1381–1382, 2006.\n[45] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu\nYang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422–440, 2021.\n[46] Rihuan Ke and Carola-Bibiane Schönlieb. Unsupervised image restoration using partially linear\ndenoisers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5796–5812,\n2021.\n44\n[47] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from\nsingle noisy images. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 2129–2137, 2019.\n[48] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala,\nand Timo Aila. Noise2noise: Learning image restoration without clean data. In International\nConference on Machine Learning, pages 2965–2974. PMLR, 2018.\n[49] Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two nonlinear\noperators. SIAM Journal on Numerical Analysis, 16(6):964–979, 1979.\n[50] Jiaming Liu, Yu Sun, Xiaojian Xu, and Ulugbek S Kamilov. Image restoration using total\nvariation regularized deep image prior. In ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 7715–7719. Ieee, 2019.\n[51] Yongyi Lu, Yu-Wing Tai, and Chi-Keung Tang. Attribute-guided face generation using\nconditional CycleGAN. In Proceedings of the European conference on computer vision (ECCV),\npages 282–297, 2018.\n[52] Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu Timofte. Srflow: Learning\nthe super-resolution space with normalizing flow. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pages\n715–732. Springer, 2020.\n[53] Sebastian Lunz, Ozan Öktem, and Carola-Bibiane Schönlieb. Adversarial regularizers in\ninverse problems. In 32nd Conference on Neural Information Processing Systems (NeurIPS\n2018), Montréal, Canada, pages 8507–8516, 2018.\n[54] Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport\nmapping via input convex neural networks. In International Conference on Machine Learning,\npages 6672–6681. PMLR, 2020.\n[55] Gary Mataev, Peyman Milanfar, and Michael Elad. DeepRED: Deep image prior powered\nby RED. In Proceedings of the IEEE/CVF International Conference on Computer Vision\nWorkshops, pages 0–0, 2019.\n[56] C. McCollough. TU-FG-207A-04: Overview of the Low Dose CT Grand Challenge. Medical\nPhysics, 43(6):3759–3760, 2014.\n[57] Abolfazl Mehranian and Andrew J Reader. Model-based deep learning pet image reconstruction\nusing forward–backward splitting expectation–maximization. IEEE transactions on radiation\nand plasma medical sciences, 5(1):54–64, 2020.\n[58] Christopher A. Metzler, Ali Mousavi, Reinhard Heckel, and Richard G. Baraniuk. Unsupervised\nlearning with stein’s unbiased risk estimator, 2020.\n[59] Tristan Milne, Étienne Bilocq, and Adrian Nachman. A new method for determining Wasser-\nstein 1 optimal transport maps from Kantorovich potentials, with deep learning applications.\narXiv preprint arXiv:2211.00820, 2022.\n[60] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint\narXiv:1411.1784, 2014.\n45\n[61] Vishal Monga, Yuelong Li, and Yonina C. Eldar. Algorithm unrolling: Interpretable, efficient\ndeep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2):18–44,\n2021.\n[62] Gaspard Monge. Mémoire sur la théorie des déblais et des remblais. Mem. Math. Phys. Acad.\nRoyale Sci., pages 666–704, 1781.\n[63] Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise\nfrom unpaired noisy data. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12064–12072, 2020.\n[64] Subhadip Mukherjee, Marcello Carioni, Ozan Öktem, and Carola-Bibiane Schönlieb. End-\nto-end reconstruction meets data-driven regularization for inverse problems. In Advances in\nNeural Information Processing Systems, volume 34, pages 21413–21425, 2021.\n[65] Subhadip Mukherjee, Sören Dittmer, Zakhar Shumaylov, Sebastian Lunz, Ozan Ök-\ntem, and Carola-Bibiane Schönlieb.\nLearned convex regularizers for inverse problems.\narXiv:2008.02839v2, 2020.\n[66] Subhadip Mukherjee, Andreas Hauptmann, Ozan Öktem, Marcelo Pereyra, and Carola-Bibiane\nSchönlieb. Learned reconstruction methods with convergence guarantees: A survey of concepts\nand applications. IEEE Signal Processing Magazine, 40(1):164–182, 2023.\n[67] Subhapip Mukherjee, Ozan Öktem, and Carola-Bibiane Schönlieb. Adversarially learned\niterative reconstruction for imaging inverse problems. In Scale Space and Variational Methods\nin Computer Vision, pages 540–552, 2021.\n[68] Pravin Nair, Ruturaj G Gavaskar, and Kunal Narayan Chaudhury. Fixed-point and objective\nconvergence of plug-and-play algorithms. IEEE Transactions on Computational Imaging,\n7:337–348, 2021.\n[69] Arkadi Nemirovski and David Berkovich Yudin. Problem Complexity and Method Efficiency in\nOptimization / translated by E.R. Dawson. Wiley-Interscience series in discrete mathematics.\nWiley, Chichester, 1983.\n[70] Changheun Oh, Dongchan Kim, Jun-Young Chung, Yeji Han, and HyunWook Park. Eter-net:\nEnd to end MR image reconstruction using recurrent neural network. In Machine Learning\nfor Medical Image Reconstruction: First International Workshop, MLMIR 2018, Held in\nConjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 1, pages\n12–20. Springer, 2018.\n[71] Giorgio Patrini, Rianne Van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav,\nMax Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertainty in\nArtificial Intelligence, pages 733–743. PMLR, 2020.\n[72] Robert R Phelps. Convex functions, monotone operators and differentiability, volume 1364.\nSpringer, 2009.\n[73] Aldo Pratelli. On the equality between Monge’s infimum and Kantorovich’s minimum in\noptimal mass transportation. In Annales de l’Institut Henri Poincare (B) Probability and\nStatistics, volume 43, pages 1–13. Elsevier, 2007.\n46\n[74] Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2self with dropout: Learning\nself-supervised denoising from single image. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1890–1898, 2020.\n[75] E. T. Reehorst and P. Schniter. Regularization by denoising: clarifications and new interpre-\ntations. IEEE Transactions on Computational Imaging, 5(1):52–67, 2019.\n[76] R Tyrrell Rockafellar. Convex analysis, volume 11. Princeton university press, 1997.\n[77] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science\n& Business Media, 2009.\n[78] Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regulariza-\ntion by denoising (RED). SIAM Journal on Imaging Sciences, 10(4):1804–1844, 2017.\n[79] Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao Yin.\nPlug-and-play methods provably converge with properly trained denoisers. In Proceedings of\nthe 36th International Conference on Machine Learning, volume 97, pages 5546–5557. PMLR,\n09–15 Jun 2019.\n[80] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans,\nDavid Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH 2022 Conference Proceedings, pages 1–10, 2022.\n[81] Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-\n63):94, 2015.\n[82] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, and Frank Lenzen.\nVariational methods in imaging, volume 167. Springer, 2009.\n[83] Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. Deep null space learning for\ninverse problems: convergence analysis and rates. Inverse Problems, 35(2):025008, Jan. 2019.\n[84] Viraj Shah and Chinmay Hegde. Solving linear inverse problems using gan priors: An algorithm\nwith provable guarantees. In 2018 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP), pages 4609–4613. IEEE, 2018.\n[85] Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, and Carola-Bibiane Schönlieb. Prov-\nably convergent data-driven convex-nonconvex regularization, 2023.\n[86] Byeongsu Sim, Gyutaek Oh, Jeongsol Kim, Chanyong Jung, and Jong Chul Ye. Optimal\ntransport driven CycleGAN for unsupervised learning in inverse problems. SIAM Journal on\nImaging Sciences, 13(4):2281–2306, 2020.\n[87] Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert\nR. G. Lanckriet. On integral probability metrics, ϕ-divergences and binary classification, 2009.\n[88] Charles M. Stein. Estimation of the Mean of a Multivariate Normal Distribution. The Annals\nof Statistics, 9(6):1135 – 1151, 1981.\n[89] Julián Tachella, Dongdong Chen, and Mike Davies. Sensing theorems for unsupervised learning\nin linear inverse problems. Journal of Machine Learning Research, 24(39):1–45, 2023.\n47\n[90] Julián Tachella, Junqi Tang, and Mike Davies. The neural tangent link between cnn denoisers\nand non-local filters. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8618–8627, 2021.\n[91] Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, and Carola-Bibiane Schönlieb. Boosting\ndata-driven mirror descent with randomization, equivariance, and acceleration. arXiv preprint\narXiv:2308.05045, 2023.\n[92] Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, and Carola-Bibiane Schönlieb. Data-driven\nmirror descent with input-convex neural networks. SIAM Journal on Mathematics of Data\nScience, 5(2):558–587, 2023.\n[93] Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, and Carola-Bibiane Schönlieb. Provably\nconvergent plug-and-play quasi-newton methods. arXiv preprint arXiv:2303.07271, 2023.\n[94] Junqi Tang, Subhadip Mukherjee, and Carola-Bibiane Schönlieb. Accelerating deep unrolling\nnetworks via dimensionality reduction. arXiv preprint arXiv:2208.14784, 2022.\n[95] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society Series B: Statistical Methodology, 58(1):267–288, 1996.\n[96] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf.\nWasserstein\nauto-encoders. arXiv preprint arXiv:1711.01558, 2017.\n[97] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 9446–9454, 2018.\n[98] Singanallur V. Venkatakrishnan, Charles A. Bouman, and Brendt Wohlberg. Plug-and-play\npriors for model based reconstruction.\nIn 2013 IEEE Global Conference on Signal and\nInformation Processing, pages 945–948, 2013.\n[99] Christina Winkler, Daniel Worrall, Emiel Hoogeboom, and Max Welling. Learning likelihoods\nwith conditional normalizing flows. arXiv preprint arXiv:1912.00042, 2019.\n[100] Jelmer M Wolterink, Tim Leiner, Max A Viergever, and Ivana Išgum. Generative adver-\nsarial networks for noise reduction in low-dose CT. IEEE transactions on medical imaging,\n36(12):2536–2545, 2017.\n[101] Dufan Wu, Kyungsang Kim, and Quanzheng Li. Computationally efficient deep neural network\nfor computed tomography image reconstruction. Medical physics, 46(11):4763–4776, 2019.\n[102] Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel,\nand Luc Van Gool. Sliced Wasserstein generative models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 3713–3722, 2019.\n[103] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang.\nGan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n45(3):3121–3138, 2022.\n[104] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, and Liang Lin. Un-\nsupervised image super-resolution using cycle-in-cycle generative adversarial networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition workshops,\npages 701–710, 2018.\n48\n[105] Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang,\nand Fang Wen. Cocosnet v2: Full-resolution correspondence learning for image translation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 11465–11475, 2021.\n[106] Bo Zhu, Jeremiah Z Liu, Stephen F Cauley, Bruce R Rosen, and Matthew S Rosen. Image\nreconstruction by domain-transform manifold learning. Nature, 555(7697):487–492, 2018.\n[107] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks, 2020.\n49\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "math.OC"
  ],
  "published": "2023-11-15",
  "updated": "2023-11-29"
}