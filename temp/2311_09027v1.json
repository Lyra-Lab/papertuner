{
  "id": "http://arxiv.org/abs/2311.09027v1",
  "title": "Assessing the Robustness of Intelligence-Driven Reinforcement Learning",
  "authors": [
    "Lorenzo Nodari",
    "Federico Cerutti"
  ],
  "abstract": "Robustness to noise is of utmost importance in reinforcement learning\nsystems, particularly in military contexts where high stakes and uncertain\nenvironments prevail. Noise and uncertainty are inherent features of military\noperations, arising from factors such as incomplete information, adversarial\nactions, or unpredictable battlefield conditions. In RL, noise can critically\nimpact decision-making, mission success, and the safety of personnel. Reward\nmachines offer a powerful tool to express complex reward structures in RL\ntasks, enabling the design of tailored reinforcement signals that align with\nmission objectives. This paper considers the problem of the robustness of\nintelligence-driven reinforcement learning based on reward machines. The\npreliminary results presented suggest the need for further research in\nevidential reasoning and learning to harden current state-of-the-art\nreinforcement learning approaches before being mission-critical-ready.",
  "text": "Assessing the Robustness of Intelligence-Driven\nReinforcement Learning\nLorenzo Nodari∗and Federico Cerutti∗†\n∗University of Brescia, Italy\n†Cardiff University, UK\nAbstract—Robustness to noise is of utmost importance in\nreinforcement learning systems, particularly in military contexts\nwhere high stakes and uncertain environments prevail. Noise\nand uncertainty are inherent features of military operations,\narising from factors such as incomplete information, adversarial\nactions, or unpredictable battlefield conditions. In RL, noise\ncan critically impact decision-making, mission success, and the\nsafety of personnel. Reward machines offer a powerful tool to\nexpress complex reward structures in RL tasks, enabling the\ndesign of tailored reinforcement signals that align with mission\nobjectives. This paper considers the problem of the robustness\nof intelligence-driven reinforcement learning based on reward\nmachines. The preliminary results presented suggest the need for\nfurther research in evidential reasoning and learning to harden\ncurrent state-of-the-art reinforcement learning approaches prior\nto being mission-critical-ready.\nIndex Terms—reinforcement learning, intelligence gathering,\nartificial intelligence\nI. INTRODUCTION\nSuppose a Person of Interest (PoI) is allegedly hiding in a\ncompound whose blueprint has been leaked by an informant as\npart of an Human Intelligence (HUMINT) gathering process.\nA new intelligence gathering mission planning starts with the\ngoal of confirming beyond any doubt the presence of the PoI.\nPoI’s acolytes can enter the compound from an opening and\nannounce them in an interphone. Based on their identity and\nthe history of interactions, they then go into one of the two\nrooms next to the one with the interphone, where the PoI\nreaches them from behind an armoured door. Through the\ninformant, enough samples of an acolyte’s voice have been\nrecorded, enough for building a text-to-speech synthesiser. It\nis, however, impossible to predict which room the acolyte\nshould use to meet with the PoI.\nThe compound’s location deep into enemy territory makes\nit impossible to deploy human forces, and the thickness of the\nwalls impacts the remote control of micro unmanned aerial\nvehicless (UAVs). An option is to use a fully autonomous\nmicro UAV— equipped with a loudspeaker and a camera with\nface recognition software— which can be trained on a replica\nof the three rooms based on the leaked blueprint.\nThe work is partially supported by the European Office of Aerospace\nResearch & Development and the Air Force Office of Scientific Research\nunder award number FA8655-22-1-7017 and by the US DEVCOM Army Re-\nsearch Laboratory (ARL) under Cooperative Agreement #W911NF2220243.\nAny opinions, findings, and conclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not necessarily reflect the views of\nthe United States government.\nu0\nu1\nu2\nu3\nu4\n\n*,\n\u000b\n\n*,\n\u000b\n\n*,\n\u000b\n\n*,\n\u000b\n\n Ú,\n\u000b\n\n\u0002\n³  | ²\n\u0003\n,\n\u000b\n\n\u0002\n²  | ³\n\u0003\n,\n\u000b\n\nÛ, \u000b\n\u000b\n\nÛ, \u000b\n\u000b\nFig. 1: High-level description of the autonomous micro UAV\nbehaviour based on the gathered HUMINT represented as a\nfinite-state automaton. Each state-transitions is labelled with\na pair, where the first element is the set of high-level obser-\nvations in the environment, and the second is the reward the\nagent receives in performing the transition: when empty, the\nreward is 0. * is a shortcut for any other input.\nBased on the gathered HUMINT, the high-level behaviour\nof the autonomous micro UAV is simple enough that can be\ncaptured by the finite-state automaton depicted in Fig. 1. Wak-\ning up in the state u0, the autonomous agent will identify the\nentrance of the room with the interphone  and impersonate\nthe acolyte’s voice Ú. This triggers the transition into the\nstate u1, in which the autonomous agent will have to explore\nthe two adjacent rooms, one on the right ³ and one on the\nleft ² of the one with the interphone. Once the camera’s\nsoftware notices a human figure, it runs the face recognition\nsoftware to identify the PoI . At this point, the agent leaves\nthe compound to transmit a confirmation signal back to base\nÛ, when it finally receives a reward \u000b— all other transactions\nhave null reward— and reaches the final state u4.\nWhile our motivating scenario aims at being a simple toy\nexample, real-world settings may be characterized by a number\nof additional difficulties, with a highly complex compound\nstructure and the need for the agent’s ability to avoid detection\nbeing just two notable examples. Therefore, in general, writing\na program which implements the proper high-level behaviour\nto carry out the mission is non-trivial: the micro UAV can learn\narXiv:2311.09027v1  [cs.LG]  15 Nov 2023\nit from interactions with the environment, using reinforcement\nlearning [1]. To make use of the intelligence gathered and\nencoded in Fig. 1, a recent proposal for reinforcement learning\nis used, namely Reward Machines [2], which is summarised in\nSection II. A reward machine can use the description provided\nin Fig. 1 and use it to train specific behaviours (policies) for\neach of the states in the automaton.\nThe research community still did not provide a convincing\nanalysis of the robustness of reward machines, which is\nparamount when the stakes are high; e.g., the autonomous\nmicro UAV is a costly research prototype, and the intelligence\ngathering mission is necessary for identifying suitable courses\nof action. In light of this, Section III details a methodology\nfor assessing the robustness of reward machines, looking at the\nspecifics of our approach. Section IV illustrates the preliminary\nresults gathered so far: they support the intuition that more\nrobust and uncertainty-aware [3], [4] approaches are needed\nfor ensuring robust exploitation of learned policies.\nII. BACKGROUND\nIn reinforcement learning, an agent interacts with an\nunknown\nenvironment,\nusually\nmodelled\nas\na\nPartially\nObservable Markov Decision Process (POMDP), PO\n=\n⟨S, O, A, r, p, ω, γ⟩, where S is the set of states of the en-\nvironment, O is the set of observations, A is the set of agent\nactions, r : S × A × S →R is the reward function, p(s′|s, a)\nis the environment transition model, ω(o|s) is the observation\nprobability distribution and γ is the discount factor.\nAt a given time step t, the agent— being in the state st ∈\nS — selects an action at based upon a probability distribution\nor policy π(· | st). Further to executing at, the agent enters a\nnew state st+1 ∼p(·|st, at) and receives an immediate reward\nr(st, at, st+1) from the environment. An optimal policy π∗\nmaximises the expected discounted return\nGt = Eπ\n\" ∞\nX\nk=0\nγkrt+k | st = s\n#\n(1)\nfor any state s ∈S and time step t.\nA reward machine [2] is a finite-state machine that receives\nabstracted descriptions of the environment as inputs, and it\noutputs reward functions. It is defined over a set of proposi-\ntional symbols P. Intuitively, P is a set of relevant high-level\nevents from the environment that the agent can detect. For\ninstance, concerning the example introduced in Section I, each\nof the symbols triggering transitions— e.g., , Û, ... — are\nelements of such set P.\nFormally,\na\nreward\nmachine\nis\na\ntuple\nM\n=\n⟨U, P, u0, uA, δu, δr⟩where U is a set of states; P is a\nset of propositions; u0 ∈U is the initial state; uA ∈U is the\nfinal state; δu : U × 2P →U is a state-transition function\nsuch that δu(u, L) is the state that results from observing\nlabel L ∈2P in state u ∈U; and δr : U × U →R is a\nreward-transition function such that δr(u, u′) is the reward\nobtained for transitioning from state u ∈U to u′ ∈U. We\nassume that (i) there are no outgoing transitions from uA,\nand (ii) δr(u, u′) = 1 if u′ = uA and 0 otherwise.\nA reward machine can be used by a reinforcement learning\nagent as a high-level, structured representation of the current\nstate of the environment: each state ui ∈U can be thought of\nas a synthesis of various properties of the world that the agent\ncan use to guide its actions. Thus, under this interpretation,\neach propositional symbol p ∈P represents an atomic high-\nlevel event that, when verified, can trigger a state change in\nthe reward machine.\nTo use a reward machine, an agent needs access to a\nlabelling function that allows it to detect the high-level events\nthat trigger the transitions between different reward machine\nstates, thus allowing it to properly keep track of the relevant\nhigh-level state of the environment. More specifically, a la-\nbelling function can be defined as a function L : O×A×O →\n2P. Given the current observation o ∈O, the action taken\nby the agent a ∈A and the resulting observation o′ ∈O,\nL(o, a, o′) is the set of high-level events that currently hold in\nthe environment.\nThe Q-learning for reward machines (QRM) algorithm [2]\nexploits the task structure modelled through reward machines.\nQRM learns a Q-function qu for each state u ∈U in the reward\nmachine. Given an experience tuple ⟨s, a, s′⟩, a Q-function qu\nis updated as follows:\nqu(s, a) =qu(s, a) + α (δr(u, u′) +\nγ max\na′ qu′(s′, a′) −qu(s, a)\n\u0011\n,\n(2)\nwhere u′ = δu(u, L(s, a, s′)). All Q-functions (or a subset\nof them) are updated at each step using the same experience\ntuple in a counterfactual manner. In the tabular case, QRM is\nguaranteed to converge to an optimal policy in the limit.\nIII. METHODOLOGY\nSince the labelling function is the only way an agent\nhas to access the high-level state of the environment, its\nproper functioning is of paramount importance in assuring the\ncorrectness of its decision-making process. For this reason, our\nwork aims to analyse the potential impact, from the agent’s\nperformance point of view, arising from an agent’s reliance on\nincorrect labelling function outputs. Specifically, we consider\nthe effects arising when trained agents are exposed to varying\namounts of unforeseen noise. In other words, we first train\nour agents in noiseless environments until they reach optimal\nlevels of performance, and then we quantify the decrease in\ntheir proficiency associated with different amounts of noise in\nthe labelling function outputs.\nIn this preliminary analysis, we limit ourselves to studying\nthe effect of random noise from the environment. In particular,\nwe consider the case where each labelling function output\nis altered, with a given probability — the noise level —\nby tampering with a single, randomly chosen observation\nincluded in the original output. Each tampering can consist of\neither the removal or substitution of the original observation\nwith a randomly chosen one. Despite this being one of the\nmost basic cases to consider, it can effectively serve as a proxy\nfor a large variety of practical situations, from sensor failure\nto adversarial attacks in a contested environment. Algorithm 1\nprovides a pseudo-code implementation of the noise-injection\nprocedure followed during our experiments.\nAlgorithm 1 Pseudocode for the noise injection procedure\nfollowed during our experiments for tampering with a labelling\nfunction output L ∈2P, under a given noise level k ∈[0, 1]\nfunction LABELLING-NOISE(L, k)\nstatic variables:\nP: the set of all possible high-level events\ntamper? ←RANDOM(0, 1) < k\nif not tamper? then\nreturn L\nelse\ne ←RANDOM-CHOICE(L)\n▷Target event\n˜e ←RANDOM-CHOICE(P)\n▷Substitute event\nif e = ˜e then\n˜L ←L −{e}\n▷Remove target\nelse\n˜L ←L −{e} ∪{˜e}\n▷Perform substitution\nreturn ˜L\nWe performed an experimental analysis— cf., Section IV—\nbased on two different grid-world environments, CookieWorld\nand SymbolWorld, both of them introduced in [5] and depicted\nin Fig. 2.\nThe CookieWorld domain— cf., Fig. 2a— is analogous to\nour motivating example discussed in Section I. An agent can\naccess three rooms— orange, green, and blue— connected by\na hallway. The agent can move in the four cardinal directions.\nThere is a button in the orange room that, when pressed, causes\na cookie to randomly appear in the green or blue room. The\nagent receives a reward of 1 for reaching and eating the cookie.\nPressing the button before reaching a cookie will remove the\nexisting cookie and cause a new cookie to appear randomly.\nThere is no cookie at the beginning of the episode. This\ndomain is partially observable since the agent can only see\nwhat it is in the room that it currently occupies.\nAs per the previous domain, in the SymbolWorld domain—\ncf., Fig. 2b—an agent can access three rooms—orange, green,\nand blue — connected by a hallway. Differently than before,\nthis domain has three symbols ♣, ♠, and ♦in the blue and\ngreen rooms. At the beginning of an episode, one symbol from\n{♣, ♠, ♦} and possibly a right or left arrow are randomly\nplaced at the orange room. Intuitively, that symbol and arrow\nwill tell the agent where to go. For example, ♣and →tell\nthe agent to go to ♣in the east room. If there is no arrow, the\nagent can go to the target symbol in either room. An episode\nends when the agent reaches any symbol in the blue or green\nroom, at which point it receives a reward of 1 if it reaches\nthe correct symbol and −1 otherwise. All other steps in the\nenvironment provide no reward.\nTo test our approach, we trained 10 agents for each environ-\nment using the QRM algorithm. All the agents were supplied\nwith hand-crafted perfect reward machines for their corre-\nsponding environment. For instance, Fig. 3 depicts the reward\nmachine considered for the CookieWorld domain. In there, any\ntransition is labelled with the high-level observations coming\nfrom the environment and the associated reward δr(u, u′), cf.,\nSection II. In particular, the high-level observations are:\n•\n, being in the orange room;\n•\n, being in the blue room;\n•\n, being in the green room;\n• ©, having pressed the button;\n• å, being in the same room with the cookie;\n• æ, eating the cookie.\nIt is clearly very similar to the reward machine considered\nin our motivating example — cf., Fig. 1 — with the notable\ndifference that in the CookieWorld the agent can press the\nbutton before reaching a cookie: this removes the existing\ncookie and causes a new cookie to appear randomly. Similarly,\nFig. 4 presents a partial view of the reward machine considered\nfor the SymbolWorld domain, limited, for ease of depiction,\nto some of the states and transitions relating to the task of\nreaching a ♣symbol. In addition to the events indicating the\ncurrent room, in this case, the high-level observations are:\n• ♣, ♠, ♦, indicating that the agent is seeing the corre-\nsponding symbol;\n• ♣, ♠, ♦, indicating that the agent has collected the cor-\nresponding symbol;\n• →, indicating that the agent should collect the target\nsymbol in the green room;\n• ←, indicating that the agent should collect the target\nsymbol in the blue room.\nMoreover, as the agent can now fail its task by either collecting\nthe wrong symbol or collecting the correct one in the wrong\nroom, ⌢is used to indicate the reward associated with agent\nfailure.\nAs per performance metrics, in this preliminary work, we\nlimit ourselves to two:\n1) average success rate, a statistic showing the expected\nprobability of success for any episode;\n2) average steps to success, a statistics showing the ex-\npected number of steps an agent will have to perform to\nachieve the reward.\nOur main experimental hypothesis is the following: an\nincrease in the noise in the observations of the world should\nlead to a decrease in the average success rate and an increase\nin the average steps to success.\nIV. RESULTS\nAfter being trained, the performance of each agent was first\nassessed in a baseline session, where the agent was free to\nact in its environment without any external intervention, i.e.:\na noise level of 0%. Each agent was tested on 1000 different\nepisodes, with a time limit of 500 steps: if, after this amount\nof time the agent still hadn’t achieved its task or, in the\ncase of the SymbolWorld domain, failed it, the episode was\nterminated and the agent was given a null reward. Table II\n(a) CookieWorld\n←\n(b) SymbolWorld\nFig. 2: Partially observable environments: the agent can observe only the content of a single room.\nTABLE I: Robustness against random noise for the CookieWorld domain (a) and the SymbolWorld domain (b).\n(a)\nNoise level (%)\nAvg. Success Rate (%)\nAvg. Steps to Success\nAvg. Steps to Failure\nAvg. Failure Reward\n1.00\n98.28\n35.96\n500\n0\n5.00\n92.02\n36.54\n500\n0\n10.00\n84.24\n37.23\n500\n0\n20.00\n71.86\n39.34\n500\n0\n30.00\n61.35\n41.35\n500\n0\n40.00\n53.13\n42.94\n500\n0\n50.00\n45.18\n45.02\n500\n0\n(b)\nNoise level (%)\nAvg. Success Rate (%)\nAvg. Steps to Success\nAvg. Steps to Failure\nAvg. Failure Reward\n1.00\n99.91\n18.64\n18.83\n−1\n5.00\n99.44\n18.87\n18.85\n−1\n10.00\n99.30\n19.13\n19.14\n−1\n20.00\n97.97\n19.75\n19.58\n−1\n30.00\n96.58\n20.49\n20.62\n−1\n40.00\n95.35\n21.37\n21.48\n−1\n50.00\n92.87\n22.35\n22.65\n−1\nTABLE II: Baseline metrics for agents trained without noise.\nDomain\nAvg. Success Rate (%)\nAvg. Steps to Success\nCookieWorld\n100.00\n35.77\nSymbolWorld\n100.00\n18.59\nsummarises the performance of these agents: since the reward\nmachine is perfectly designed to match the problem, it is not\na surprise that the average success rate is 100.00% for both\nenvironments.\nWe then injected random noise, as discussed in Section III.\nTable I summarises the experimental results gathered, both for\nthe CookieWorld — Table Ia — and for the SymbolWorld —\nTable Ib. By visual inspection, we can confirm that there is\nevidence in support of our experimental hypotheses.\nWe can, however, notice substantial differences between the\ntwo domains. CookieWorld seems to be quite sensitive to the\nnoise: e.g., considering a 30% observation noise leads to a\ndecrease of the average success rate of nearly 40%. Symbol-\nWorld, instead, seems to be more resilient: e.g., a 30% noise\nleads to a decrease of the average success rate of less than\n4%. This is in part explained by the presence of many more\npossible high-level observations in the SymbolWorld domain,\nas well as its more deterministic structure. However, when\nobserving the failure statistics we notice that SymbolWorld\nfailures were always caused by the agents reaching the wrong\nsymbol, thus highlighting how carefully timed noise can lead\nto a complete misunderstanding of the task, a critical issue\nin practical deployments. To clarify this intuition, consider\nu0\nu1\nu2\nu3\nu4\n\n*,\n\u000b\n\n*,\n\u000b\n\n*,\n\u000b\n\n*,\n\u000b\nD\n©,\nE\nDh\nå |\ni\n,\nE\nDh\nå |\ni\n,\nE\n\næ, \u000b\n\u000b\n\næ, \u000b\n\u000b\nD\n©,\nE\nD\n©,\nE\nFig. 3: High-level description of the reward machine consid-\nered for the CookieWorld domain, cf., Fig. 2a. Each state-\ntransitions is labelled with a pair, where the first element is\nthe set of high-level observations in the environment, and the\nsecond is the reward the agent receives in performing the\ntransition: when empty, the reward is 0. * is a shortcut for\nany other input. This figure is based upon [5, Fig. 2c].\nthe task of reaching the ♣in the left room, indicated by\nthe high-level observation L =\nD\n, ♣, ←\nE\n, as in Fig. 4.\nIf the agent receives this observation while in state u0, it\nsuccessfully transitions to state u1, thus guaranteeing the\ncorrect understanding of its task. However, if the random\nnoise manages to alter exactly the first occurrence of such\nobservation, substituting it, for instance, with the tampered\n˜L =\nD\n, ♠, ←\nE\n, the agent would incorrectly transition to\nthe state indicating the task of reaching the ♠in the left\nroom. In such a scenario, even if the noise were to disappear\ncompletely after this tampering, the agent would be doomed to\nfail the task, as it would effectively act in pursuit of the wrong\nobjective. Therefore, the precise timing required for carrying\nout such an attack, coupled with the random nature of the\nnoise injected in our experiments, explains the low decrease\nin success rate observed in the SymbolWorld domain. Finally,\nthe difference between the results obtained in the two domains\nhighlights one key aspect of an RM-based agent’s robustness\nto noise: its dependency on the environment at hand, in terms\nof both reward machine structure and state dynamics.\nV. CONCLUSIONS\nRobustness to noise is of utmost importance in reinforce-\nment learning systems, particularly in military contexts where\nhigh stakes and uncertain environments prevail. Reward ma-\nchines offer a powerful tool to express complex reward struc-\ntures in RL tasks, enabling the design of tailored reinforcement\nsignals that align with mission objectives. In this paper, we\nconsider the problem of the robustness of intelligence-driven\nreinforcement learning based on reward machines. The results\nu0\nu1\nu4\nu9\nu10\n· · · · · ·\n· · · · · ·\n\n*,\n\u000b\n\n*,\n\u000b\n\n*,\n\u000b\n\n*,\n\u000b\nD\n♣→,\nE\nD\n♣,\nE\nD\n♣←,\nE\nD\n♣, \u000b\nE\nD\n♣, ⌢\nE\n\n♠, ⌢\n\u000b\n\n♦, ⌢\n\u000b\nD\n♣, \u000b\nE\nD\n♣, \u000b\nE\nD\n♣, \u000b\nE\nFig. 4: Partial high-level description of the reward machine\nconsidered for the SymbolWorld domain — cf., Fig. 2b —\ndepicting only the states and transitions relating to the task\nof reaching a ♣symbol. Each state-transitions is labelled\nwith a pair, where the first element is the set of high-level\nobservations in the environment, and the second is the reward\nthe agent receives in performing the transition: when empty,\nthe reward is 0. * is a shortcut for any other input. This figure\nis based upon [5, Fig. 2c]. Note that, for ease of depiction,\nthe transitions associated with agent failure are shown only\nfor the u1, u10 pair of states, but can be easily deduced for\nany other pair of states.\ngathered, and, in particular, the one for CookieWorld — cf.,\nTable Ia — where noise and uncertainty play a bigger role,\nsuggest the need for further research to harden current state-\nof-the-art reinforcement learning approaches prior to being\nmission-critical-ready. In future work, we plan to consider ev-\nidential learning and reasoning [3], [4] as possible techniques\nfor ensuring robust exploitation of learned policies.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.\nMIT Press, 2018.\n[2] R. Toro Icarte, T. Q. Klassen, R. Valenzano, and S. A. McIlraith, “Re-\nward Machines: Exploiting Reward Function Structure in Reinforcement\nLearning,” J. Artif. Intell. Res., vol. 73, pp. 173–208, 2022.\n[3] F. Cerutti, L. M. Kaplan, A. Kimmig, and M. S¸ensoy, “Handling epistemic\nand aleatory uncertainties in probabilistic circuits,” Machine Learning, pp.\n1–43, 2022.\n[4] F. Cerutti, L. Kaplan, M. Sensoy et al., “Evidential reasoning and\nlearning: a survey,” in IJCAI.\nInternational Joint Conferences on\nArtificial Intelligence, 2022, pp. 5418–5425.\n[5] R. Toro Icarte, E. Waldie, T. Klassen, R. Valenzano, M. Castro, and\nS. McIlraith, “Learning Reward Machines for Partially Observable Re-\ninforcement Learning,” in Advances in Neural Information Processing\nSystems, vol. 32.\nCurran Associates, Inc., 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CR"
  ],
  "published": "2023-11-15",
  "updated": "2023-11-15"
}