{
  "id": "http://arxiv.org/abs/2311.09671v1",
  "title": "Robust Contrastive Learning With Theory Guarantee",
  "authors": [
    "Ngoc N. Tran",
    "Lam Tran",
    "Hoang Phan",
    "Anh Bui",
    "Tung Pham",
    "Toan Tran",
    "Dinh Phung",
    "Trung Le"
  ],
  "abstract": "Contrastive learning (CL) is a self-supervised training paradigm that allows\nus to extract meaningful features without any label information. A typical CL\nframework is divided into two phases, where it first tries to learn the\nfeatures from unlabelled data, and then uses those features to train a linear\nclassifier with the labeled data. While a fair amount of existing theoretical\nworks have analyzed how the unsupervised loss in the first phase can support\nthe supervised loss in the second phase, none has examined the connection\nbetween the unsupervised loss and the robust supervised loss, which can shed\nlight on how to construct an effective unsupervised loss for the first phase of\nCL. To fill this gap, our work develops rigorous theories to dissect and\nidentify which components in the unsupervised loss can help improve the robust\nsupervised loss and conduct proper experiments to verify our findings.",
  "text": "arXiv:2311.09671v1  [cs.LG]  16 Nov 2023\nRobust Contrastive Learning With Theory Guarantee\nLam Tran∗\nVinAI Research\nNgoc N. Tran∗†\nVanderbilt University\nHoang Phan†\nNew York University\nAnh Bui\nMonash University\nTung Pham\nVinAI Research\nToan Tran\nVinAI Research\nDinh Phung\nMonash University\nTrung Le\nMonash University\nAbstract\nContrastive learning (CL) allows us to create meaningful features without any\nlabel information. In the ﬁrst phase, CL approaches learn the features, which are\nthen classiﬁed by a linear classiﬁer that has been learned from labeled data. While\nexisting theoretical works have studied the connection between the supervised\nloss in the second phase and the unsupervised loss in the ﬁrst phase to explain\nwhy the unsupervised loss can support the supervised loss, there has been no\ntheoretical examination of the connection between the unsupervised loss in the\nﬁrst phase and the robust supervised loss in the second phase, which can shed\nlight on how to establish an effective unsupervised loss in the ﬁrst phase. To\nﬁll this gap, our paper develops rigorous theories to identify which components\nin the supervised loss can aid the robust supervised loss. Finally, we conduct\nexperiments to verify our ﬁndings. All code used in this work is available at\nhttps://anonymous.4open.science/r/rosa.\n1\nIntroduction\nContrastive learning (CLR) has become an essential technique in self-supervised learning (SSL)\nwhere positive and negative examples are created for each given anchor example [1–4]. By using\nCLR, a feature extractor can learn to align representations of the anchors and their positive examples\nwhile contrasting those of the anchors and their negative ones. A pioneering work in this area is\nSimCLR [2], which proposed an efﬁcient technique to train a feature extractor with contrastive\nlearning. In SimCLR, positive examples are created using random data augmentation techniques\nsampled from a pool T of data augmentations, while negative examples are simply sampled from\nthe data distribution. Subsequently, the InfoNCE loss is employed to train a feature extractor by\naligning the representations of positive pairs while contrasting those of negative examples.\nThe vulnerability of neural networks to imperceptibly small perturbations [5] has posed a\nsigniﬁcant challenge in deploying them for safety-critical applications, such as autonomous driving.\nResearchers have proposed various studies to enhance the robustness of trained networks against\nadversarial attacks [6, 7], random noise [8], and corruptions [9]. In particular, the robustness of\ncontrastive learning (CL) has been empirically investigated in previous works [10–12]. Speciﬁcally,\nthese works explored adversarial examples generated by slightly modifying the InfoNCE loss to\nobtain robust unsupervised features for the second phase. Although these studies achieved promising\nempirical robust accuracies in the second phase, they still need to address why improving the\nrobustness in the ﬁrst phase using the InfoNCE loss can enhance the robustness in the second phase\nfrom a theoretical perspective.\n∗Equal contribution.\n† Work partially done while at VinAI Research.\nPreprint. Under review.\nThe success of SimCLR and other CL techniques [13, 14] has spurred interest in exploring the\ntheoretical foundations of CL with the InfoNCE loss. Prior works [1–4] have inspired several recent\nstudies, including [15–17]. For example, [15] investigated the connection between the general\nclassiﬁcation loss and unsupervised loss in binary classiﬁcation using Rademacher complexity. In\ncontrast, [16] empirically and theoretically explored the distribution of representations on the unit\nsphere, ﬁnding that positive examples and their anchors remain close even as the representations\nbecome more uniformly distributed. More recently, [17] extended these ﬁndings to multi-class\nclassiﬁcation and suggested that reducing the gap between the two losses is possible if the pool\nof data augmentations T satisﬁes the intra-class connectivity condition. However, none of these\nworks have yet investigated the robustness of CL.\nIn this work, we aim to provide a theoretical understanding of the robustness of CL. Our focus is\non connecting the supervised adversarial loss in the second phase with the unsupervised loss in the\nﬁrst phase. We demonstrate that incorporating adversarial training in the ﬁrst phase by creating\nadversarial examples that attack the InfoNCE loss can improve the robustness in the second phase,\nand provide a theoretical justiﬁcation for this phenomenon. Additionally, we show that utilizing\na global view and enhancing the sharpness of the feature extractor [18] can also strengthen the\nrobustness in the second phase. Finally, we conduct empirical experiments to validate our theoretical\nﬁndings.\nTo summarize, our contributions in this paper include\n• We rigorously develop theories to bound the adversarial loss in the second phase,\nidentifying the crucial factors from the ﬁrst phase that contribute to the robustness. Our\nwork is the ﬁrst to address this theoretical problem.\n• Our theoretical ﬁndings highlight several important insights: (i) Minimizing the InfoNCE\nloss on the benign examples proves beneﬁcial in enhancing both the robust and natural\naccuracies during the second phase. (ii) Utilizing sharpness-aware minimization (SAM)\n[18] on the InfoNCE loss of the benign examples offers further improvement in both\nrobust and natural accuracies during the second phase. (iii) Employing adversarial attacks\non the InfoNCE loss and pushing the resulting adversarial examples away from their\ncorresponding benign examples globally assists in enhancing the robust accuracies during\nthe second phase. It is noteworthy that our ﬁndings in (i) and (ii) contrast with the outcomes\nobserved in standard adversarial training, where minimizing the benign loss can enhance\nnatural accuracies but may compromise robust accuracies [7, 19]. Finally, we conduct\nexperiments to validate our ﬁndings.\n2\nRelated Work\n2.1\nFlat Minima\nThe utilization of ﬂat minimizers has been shown to enhance the generalization ability of neural\nnetworks by enabling them to discover wider local minima. This allows the models to be more\nresilient to differences between the training and testing datasets. Numerous studies have investigated\nthe relationship between generalization ability and the width of minima, including notable works\nsuch as [20–23]. In addition, many methods have been proposed to seek ﬂat minima, as documented\nin [24–27, 18]. Various training factors, such as batch size, learning rate, gradient covariance,\nand dropout, have also been investigated for their impact on the ﬂatness of discovered minima in\nstudies such as [26, 28, 29]. Additionally, some approaches aim to discover wide local minima\nby incorporating regularization terms into the loss function, such as penalties for low entropy of\nsoftmax outputs [24], as well as distillation losses [30, 31].\nRecently, the sharpness-aware minimization (SAM) method, proposed in [18], has gained attention\ndue to its effectiveness and scalability in seeking ﬂat regions in the loss landscape. SAM has been\nsuccessfully applied in various domains and tasks, such as meta-learning bi-level optimization [32],\nfederated learning [33], vision models [34], language models [35], domain generalization [36], and\nmulti-task learning [37]. For instance, in [32], SAM improved the performance of meta-learning\nbi-level optimization, while in [33], SAM achieved tighter convergence rates than existing works in\nfederated learning and proposed a generalization bound for the global model. Moreover, researchers\n2\nhave explored SAM’s geometry [38], minimized surrogate gaps [39], and developed methods to\naccelerate SAM’s training process [40, 41].\n2.2\nSelf-supervised Learning\nStandard SSL. Self-supervised learning is a machine learning paradigm that aims to extract\nmeaningful representations from input data without relying on human annotations. Recent advances\nin self-supervised learning for visual data, including methods such as MoCo [1], SimCLR [2], BYOL\n[42], SimSiam [43], and SwAV [44], have demonstrated the effectiveness of these representations in\nvarious downstream tasks such as image classiﬁcation, object detection, and instance segmentation.\nThe underlying principle of these methods is to learn representations that remain invariant to\ndifferent data augmentations by maximizing the similarity of representations derived from different\naugmented versions of an image. However, this process can potentially lead to mode collapse\n[45], whereby the network can capture a few modes/presentations of the original data distribution,\nthus resulting in suboptimal performance. To overcome this challenge, several methods have been\nproposed to learn more informative representations, such as the aforementioned MoCo and SimCLR\nmethods.\nContrastive methods are commonly used to create positive and negative pairs through the use of\ndata augmentations and InfoNCE loss, which aims to align positive pairs while spreading negative\npairs apart [1–4]. However, these methods often require comparisons between each image and many\nothers to achieve optimal results. In more recent works such as BYOL [42] and SimSiam [43], only\npositive pairs are used in the loss function, with a ”predictor” network learning to output predictions\nthat align with the stop-gradient projections of an existing model. In contrast, SwAV [44] does not\ndirectly compare image features, but instead assigns augmentations from the same image to clusters\nand enforces consistency between these clusters.\nIn contrastive learning methods, positive samples are commonly generated through augmentation\nof the same images, while negative samples are chosen from the remaining data. However, an\nissue arises when negative samples sharing the same label as the anchor sample are included in the\nselection process. To address this issue, a partial solution has been proposed by Chuang et al. [13],\nwhich introduces a distribution over the negative samples to correct potential biases and improve the\nquality of the learned representations. Building upon this work, Robinson et al. [14] have further\nimproved the sampling process by incorporating the similarity between the negative samples and\nthe anchor into the distribution, resulting in negative samples that are closer to the anchor and thus\nenhancing the effectiveness of the training process.\nRobust SSL. Machine learning models have demonstrated a concerning susceptibility to adversarial\nperturbations [46, 5]. Although undetectable by human perception, these perturbations can lead to\nerroneous outcomes. Consequently, the vulnerability of machine learning models to such attacks\nposes a signiﬁcant safety risk when applied to real-world systems. One of the promising directions\nto tackle this issue is to develop a robust feature extractor [10–12, 47–49]. By training a feature\nextractor capable of disregarding adversarial perturbations and generating representations similar to\nthose of benign inputs, we can establish a strong foundation for building robust classiﬁers even with\na simple linear classiﬁer on top of the feature extractor. [10–12] parallelly proposed a robust self-\nsupervised learning method that aims to robustify the feature extractor by using adversarial training\nwith adversarial examples which maximize the InfoNCE loss. While sharing the same principle of\nmaximizing feature consistency under different augmented views, three methods were different in\nhow to construct the positive/negative sets. In [10], the adversarial examples were considered as\npositive samples in relation to the benign anchor. On the other hand, [11] utilized two adversarial\nexamples for each benign anchor, resulting doubled computation costs. In the context of a supervised\nsetting, [47, 48] proposed a similar regularization technique which leverage both global information\n(i.e., class identity) and local information (i.e., adversarial/benign identity) to reduce the divergence\nbetween the representations of adversarial and benign examples.\nTheory of SSL. Driven by promising empirical results in contrastive learning, several studies\nhave explored this learning paradigm from a theoretical perspective [15–17]. More speciﬁcally,\n[15] established a connection between supervised and unsupervised losses by using Rademacher\ncomplexity in the context of binary classiﬁcation. Moreover, [16] examined the distribution of\nlatent representations over the unit sphere and found that these representations tend to be uniformly\ndistributed, encouraging alignment between positive examples and their anchors. Most recently,\n3\n[17] relaxed the conditional independence assumption in [15] by introducing a milder condition,\nestablished a connection between supervised and unsupervised losses, and developed a new theory\nof augmentation overlap for contrastive learning.\n3\nProblem Formulation and Notions\nIn this section, we present the problem formulation of self-supervised learning and the notions used\nin our following theory development. We consider an M-class classiﬁcation problem with the label\nset Y = {1, 2, ..., M}. Given a class c ∈Y, the class-condition distribution for this class has the\ndensity function pc (x) = p (x | y = c) where x ∈Rd speciﬁes a data example. Therefore, the\nentire data distribution has the form\npdata (x) =\nM\nX\nc=1\nπcp (x | y = c) =\nM\nX\nc=1\nπcpc (x) ,\nwhere πc = P (y = c) , c ∈Y is a class probability.\nThe distribution of positive pairs over Rd × Rd is formulated as\nppos\n\u0000x, x+\u0001\n=\nM\nX\nc=1\nπcpc (x) pc\n\u0000x+\u0001\n.\nIt is worth noting that with the above equality, ppos (x, x+) is relevant to the probability that x, x+ ∼\npdata possess the same label. Particularly, to form a positive pair (x, x+), we ﬁrst sample a class\nc ∼Cat (π) from the categorical distribution with π = [πc]M\nc=1, and then sample x, x+ ∼pc.\nThe general unsupervised InfoNCE loss over the entire data and positive pair distributions is denoted\nas\nLun\nDun (θ, ppos) = E(x,x+)∼ppos,x−\n1:K\niid\n∼pdata\n\"\n−log\nexp\n\b fθ(x)·fθ(x+)\nτ\n\t\nexp\n\b fθ(x)·fθ(x+)\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x)·fθ(x−\nk )\nτ\n\t\n#\n, (1)\nwhere fθ with θ ∈Θ is a feature extractor, the operation fθ(x) · fθ(ex) presents the inner product, τ > 0 is\nthe temperature variable, K is the number of used negative examples, and Dun denotes the distribution over\nz =\nh\nx, x+,\n\u0002\nx−\nk\n\u0003K\nk=1\ni\nwith\n\u0000x, x+\u0001\n∼ppos, x−\n1:K ∼pdata. Note that β ≥0 is a parameter and setting β = K\nrecovers the original formula of contrastive learning.\nIt is our ultimate goal to minimize the general unsupervised InfoNCE loss. However, in reality, we work with\na speciﬁc training set S =\nn\nzi =\nh\nxi, x+\ni ,\n\u0002\nx−\nik\n\u0003K\nk=1\nioN\ni=1 where z1:N ∼Dun. The empirical unsupervised\nInfoNCE loss over S is deﬁned as\nLun\nS (θ, ppos) = −1\nN ×\nN\nX\ni=1\nlog\nexp\n\b fθ(xi)·fθ(x+\ni )\nτ\n\t\nexp\n\b fθ(xi)·fθ(x+\ni )\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(xi)·fθ(x−\nik)\nτ\n\t.\nSSL aims to minimize the empirical unsupervised InfoNCE loss over a speciﬁc training set S to learn an optimal\nfeature extractor fθ∗, used in the second phase for training a linear classiﬁer on top of its extracted features.\nGiven a feature extractor fθ and a linear classiﬁer that parameterized by a weight matrix W , we deﬁne the\ngeneral loss induced by this couple as\nLsup\nDsup (θ, W ) = E(x,y)∼Dsup [τCE (W fθ (x) , y)] ,\nwhere Dsup (x, y) = πypy (x) is the data-label distribution and τCE (·, ·) is the τ-temperature cross-entropy loss\n(i.e., softmax with a temperature τ applied to logits before computing the cross-entropy loss).\nGiven the fact that we aim to train the optimal linear classiﬁer in the second phase, we deﬁne the optimal\ngeneral loss over all weight matrices W as\nLsup\nDsup (θ) = min\nW Lsup\nDsup (θ, W ) .\nWe now present the losses for the robust SSL. Given an attack a (i.e., FGSM [5], PGD [6], TRADES [7],\nand AutoAttack [50]) and a weight matrix W , we denote aW,θ(x) ∈Bǫ(x) = {x′ : ∥x′ −x∥≤ǫ} as the\n4\nadversarial example obtained by using a to attack the second phase classiﬁer, formed by fθ and W , on a benign\nexample x with a label y. We deﬁne the adversarial loss using the adversary a to attack W and fθ as follows:\nLadv\nDsup (W, θ, a) = E(x,y)∼Dsup\nh\nτCE (W fθ (aW,θ (x)) , y)1/2i\n.\n(2)\nOur next step is to minimize the above adversarial loss to deﬁne the best adversarial loss corresponding to the\nmost robust linear classiﬁer when using the adversary a to attack the classiﬁer in the second phase:\nLadv\nDsup (θ, a) = min\nW Ladv\nDsup (W, θ, a) .\n(3)\n4\nProposed Framework\n4.1\nTheory Development\nWe ﬁrst develop necessary theories to tackle the optimization problem of the adversarial loss Ladv\nDsup(θ, a). Let\nus denote p\naW,θ\nadv\n= aW,θ#pdata as the adversarial distribution formed by using the attack transformation aW,θ\nto transport the data distribution pdata. It is obvious by the deﬁnition that p\naW,θ\nadv\nconsists of adversarial examples\naW,θ(x) with the benign example x ∼pdata. In addition, we deﬁne qdata = fθ#pdata and q\naW,θ\nadv\n= fθ#p\naW,θ\nadv\nas the benign and adversarial distributions on the latent space, respectively.\nThe following theorem indicates the upper bounds for the adversarial loss of interest Ladv\nDsup(θ, a).\nTheorem 4.1. Consider the adversarial loss Ladv\nDsup(θ, a).\ni) We have the ﬁrst upper bound on the data space\nLadv\nDsup (θ, a) ≤min\nW\nn\nLsup\nDsup (θ, W )1/2 + Lsup\nDsup (θ, W )1/2 Dv\n\u0000p\naW,θ\nadv , pdata\n\u00011/2 o\n.\n(4)\nii) We have the second upper bound on the latent space\nLadv\nDsup (θ, a) ≤min\nW\nn\nLsup\nDsup (θ, W )1/2 + Lsup\nDsup (θ, W )1/2 Dv\n\u0000q\naW,θ\nadv , qdata\n\u00011/2 o\n.\n(5)\nHere we note that Dv represents a f-divergence with the convex function v(t) = (t −1)2.\nBoth bounds in Theorem B.1 have the same form but they are deﬁned on the data and latent spaces, respectively.\nAdditionally, the upper bound in (18) reveals that to minimize the adversarial loss for the adversary a, we need\nto search the feature extractor fθ and the weight matrix W for minimizing the supervised loss Lsup\nDsup (θ, W )\nand the divergence Dv\n\u0000q\naW,θ\nadv\n, qdata\n\u0001\nbetween the benign and adversarial distributions on the latent space.\nWe now go further to discover which losses should be involved in the ﬁrst phase to minimize the upper bound\nin (18). To this end, we denote\nA = {a (· | x) with x ∼pdata : a (· | x) has a support set over Bǫ (x)} ,\nqa = fθ#(a#pdata) for a ∈A.\nwhere Bǫ(x) speciﬁes the ball with the radius ǫ around x. A consists of the stochastic map a(· | x) with x ∼\npdata, working on the ball Bǫ(x), which can be considered as the set of all possible attacks. By its deﬁnition, the\nstochastic map p\naW,θ\nadv\ncan be represented by an element in A as long as aW,θ ∈Bǫ(x). The following theorem\nassists us in converting the upper bound in (18) to the one directly in the phase 1.\nTheorem 4.2. The adversarial loss Ladv\nDsup(θ, a) can be further upper-bounded by\nLadv\nDsup (θ, a) ≤[Lun\nDun (θ, ppos) + A (K, β)]1/2 \u0002\n1 + max\na∈A Dv (qa, qdata)\n1\n2 \u0003\n,\n(6)\nwhere A(K, β) = −O\n\u00001\n√\nK\n\u0001\n−log β −O\n\u0000 1\nβ\n\u0001\n.\nInequality (26) indicates that to achieve minθ Ladv\nDsup (θ, a), we can alternatively minimize its upper-bound\nwhich is relevant to Lun\nDun (θ). Unfortunately, minimizing Lun\nDun (θ, ppos) directly is intractable due to the\nunknown general distribution Dun. The following theorem resolves this issue and also signiﬁes the concept of\nsharpness for the feature extractor fθ.\nTheorem 4.3. Under mild conditions, with the probability at least 1 −δ over the random choice of S ∼DN\nun,\nwe have the following inequality\nLun\nDun (θ, ppos) ≤\nmax\nθ′:∥θ′−θ∥<ρ Lun\nS (θ) +\n1\n√\nN\nhT\n2 log\n\u0010\n1 + ∥θ∥2\nT σ2\n\u0011\n+ log 1\nδ + L2\n8 + 2L + O\n\u0000log(N + T )\n\u0001i\n,\nwhere L = 2\nτ + log(1 + β), T is the number of parameters in θ, and σ = ρ[\n√\nT +\np\nlog (N)]−1.\n5\nWe note that the proof in [18] invoked the McAllester PAC-Bayesian generalization bound [51], hence only\napplicable to the 0-1 loss in the binary classiﬁcation setting. Ours is the ﬁrst work that proposes and devises\nsharpness-aware theory for feature extractor in the context of SSL. Additionally, the proof of our theory employs\nthe PAC-Bayesian generalization bound [52] to deal with the more general InfoNCE loss. Eventually, by\nleveraging Theorems B.2 and B.3, we reach the following theorem.\nTheorem 4.4. Under mild conditions, with the probability at least 1 −δ over the random choice of S ∼DN\nun,\nwe have the following inequality\nLadv\nDsup (θ, a) ≤\nh\nmax\n∥θ′−θ∥≤ρ Lun\nS (θ, ppos) + B(T, N, δ, ∥θ∥) + A (K, β)\ni 1\n2 h\n1 + max\na∈A Dv (qa, qdata)\n1\n2\ni\n, (7)\nwhere we have deﬁned\nB(T, N, δ, ∥θ∥) =\n1\n√\nN\nhT\n2 log\n\u0010\n1 + ∥θ∥2\nT σ2\n\u0011\n+ log 1\nδ + L2\n8 + 2L + O (log (N + T ))\ni\n.\nRemark 4.5. Inequality (34) shows that minimizing the adversarial loss Ladv\nDsup (θ, a) with respect to θ requires\nminimizing the sharpness-aware unsupervised InfoNCE loss:\nmax∥θ′−θ∥≤ρ Lun\nS (θ) and the divergence:\nmaxa∈A Dv (qa, qdata).\nWhile the sharpness-aware unsupervised InfoNCE loss is straightforward, the\ndivergence term maxa∈A Dv (qa, qdata) requires further derivations, which will be clariﬁed in the next section.\nWe now develop another upper bound for the adversarial loss Ladv\nDsup (θ, a). Given a stochastic map a ∈A, we\ndeﬁne the hybrid positive pa\npos(x, x+) distribution induced by a as pa\npos(x, x+) = PM\nc=1 πcpc (x) pa\nc\n\u0000x+\u0001\n,\nwhere pa\nc = a#pc.\nAdditionally, by its deﬁnition, the hybrid positive distribution pa\npos(x, x+) consists of the positive pairs of a\nbenign example x with the label c and an adversarial example x+ formed by applying the stochastic map a to\nan another benign example x′. The following theorem gives us another upper bound on the the adversarial loss\nLadv\nDsup (θ, a).\nTheorem 4.6. The adversarial loss Ladv\nDsup(θ, a) can be further upper-bounded by\nLadv\nDsup (θ, a) ≤max\na∈A\nn\nLun\nDun\n\u0000θ, pa\npos\n\u0001\n+\n\u0010\n1 + exp\n\b 2\nτ\n\t\u0011\nDu (qa, qdata)\no\n+ A (K, β) ,\n(8)\nwhere A(K, β) = −O\n\u00001\n√\nK\n\u0001\n−log β −O\n\u0000 1\nβ\n\u0001\n, Du is a f-divergence with the convex function u(t) = |t −1|,\nand Lun\nDun\n\u0000θ, pa\npos\n\u0001\nis similar to Lun\nDun (θ, ppos) in Eq. (14) except that the positive pairs (x, x+) ∼pa\npos.\nRemark 4.7. Theorem B.6 indicates that we need to ﬁnd the adversary a that maximizes the InfoNCE loss over\nthe hybrid adversarial positive distribution pa\npos and the divergence on the latent space Du(qa, qdata). This allows\nus to create strong adversarial examples that cause high InfoNCE values (i.e., za = fθ(xa) is locally far from\nz = fθ(x)), while the distribution of adversarial examples is globally far from the that of benign examples on\nthe latent space. Eventually, the feature extractor fθ is updated to minimize both the local distances and the\nglobal divergence.\nAdditionally, the adversarial InfoNCE term Lun\nDun\n\u0000θ, pa\npos\n\u0001\nhas the form of\nLun\nDun\n\u0000fθ, pa\npos\n\u0001\n= E(x,x+)∼papos,x−\n1:K\niid\n∼pdata\n\"\n−log\nexp\n\b fθ(x).fθ(x+)\nτ\n\t\nexp\n\b fθ(x).fθ(a(x+))\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x).fθ(x−\nk )\nτ\n\t\n#\n= E(x,x+)∼ppos,x−\n1:K\niid\n∼pdata\n\"\n−log\nexp\n\b fθ(x).fθ(a(x+))\nτ\n\t\nexp\n\b fθ(x).fθ(x+)\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x).fθ(x−\nk )\nτ\n\t\n#\n,\n(9)\nwhich means that given the positive pair (x, x+) having the same label, we ﬁnd the adversarial example a(x+)\nfor x+ that maximizes the InfoNCE loss as in Eq. (39). Note that according to Eq. (35), we actually need\nto seek adversarial examples a(x+) to globally maximize a f-divergence between the benign and adversarial\ndistributions.\nLeveraging the upper bounds in Theorems B.4 and B.6, we arrive the ﬁnal upper bound stated in the following\ntheorem.\nTheorem 4.8. For any 0 ≤λ ≤1, the adversarial loss Ladv\nDsup(θ, a) can be further upper-bounded by\nLadv\nDsup (θ, a) ≤λ max\na∈A\nn\nLun\nDun\n\u0000θ, pa\npos\n\u0001\n+\n\u0010\n1 + exp\n\b 2\nτ\n\t\u0011\nDu (qa, qdata)\no\n+ λA (K, β)\n+ (1 −λ)\nh\nmax\n∥θ′−θ∥≤ρ Lun\nS (θ, ppos) + B(T, N, δ, ∥θ∥) + A (K, β)\ni 1\n2 h\n1 + max\na∈A Dv (qa, qdata)\n1\n2\ni\n.\n(10)\n6\nRemark 4.9. We summarize our theoretical ﬁndings as follows:\nMinimizing the InfoNCE loss on the benign examples (i.e., Lun\nS (θ, ppos)) assists us in lowering down the\nadversarial loss in the second phase. Moreover, the sharpness-aware minimization for the InfoNCE loss on\nthe benign examples (i.e., max∥θ′−θ∥≤ρ Lun\nS (θ, ppos)) also supports us in reducing the adversarial loss in the\nsecond phase.\nMinimizing the InfoNCE loss on the adversarial examples formed by attacking the benign examples\non the InfoNCE loss (i.e., maxa∈A Lun\nDun\n\u0000θ, pa\npos\n\u0001\n) helps to strengthen the adversarial robustness in\nthe second phase.\nSpeciﬁcally, according to our theory,\nthe adversarial examples are found by\nmaximizing the InfoNCE loss and the global divergence to their corresponding benign examples (i.e.,\nmaxa∈A\n\b\nLun\nDun\n\u0000θ, pa\npos\n\u0001\n+\n\u00001 + exp\n\b 2\nτ\n\t\u0001\nDu (qa, qdata)\n\t\n+ A (K, β). In our implementation, we relax\nDu to the Jensen-Shannon divergence DJS in order to use generative adversarial network (GAN) [53] for\nquantifying the global divergence.\n4.2\nPractical Method\nThis section explains how to harvest the developed theories to conduct a practical method for robust self-\nsupervised learning. The main objective is to demonstrate that the ﬁnding terms from the theories assist us in\nimproving the robustness of self-supervised learning.\nGiven a mini-batch B = {x1, . . . , xb} of the anchor examples, we apply random transformations from a pool\nof transformation T to create the corresponding positive pairs (˜x1, ˜x+\n1 ), (˜x2, ˜x+\n2 ),..., (˜xb, ˜x+\nb ) where ˜xi = t(xi)\nand ˜x+\ni = t′(xi) with t, t′ ∼T .\nHinted by Eq. (39), we formulate the adversarial InfoNCE loss term Lun\nDun\n\u0000θ, pa\npos\n\u0001\nas\nmax\n∀i:fθ(˜x+\ni )a∈Bǫ(fθ(˜x+\ni ))\n\u001a\n−1\nb ×\nb\nX\ni=1\nlog\nexp\n\b fθ(˜xi).fθ(˜x+\ni )a\nτ\n\t\nexp\n\b fθ(˜xi).fθ(˜x+\ni )a\nτ\n\t\n+ β\nK\nP\nk̸=i exp\n\b fθ(˜xi).fθ(˜xk)\nτ\n\t\n\u001b\n,\nwhere we denote fθ\n\u0000˜x+\ni\n\u0001\na as the adversarial example of fθ\n\u0000˜x+\ni\n\u0001\nwithin its ǫ-ball.\nWe relax Du (qa, qdata) to DJS (qa, qdata) so that GAN [53] can be applied to realize it. With the support of\nthe discriminator D, the divergence of interest can be rewritten as [53]\nDJS (qa, qdata) = min\nθ\nmax\nD\nn\nb\nX\ni=1\nlog D\n\u0000fθ\n\u0000˜x+\ni\n\u0001\u0001\n+\nb\nX\ni=1\nlog D\n\u00001 −fθ\n\u0000˜x+\ni\n\u0001\na\n\u0001 o\n+ const.\n(11)\nTherefore, we can combine the local and global views to generate adversarial examples as\nmax\n∀i:fθ(˜x+\ni )a∈Bǫ(fθ(˜x+\ni ))\n\u001a\n−1\nb ×\nb\nX\ni=1\nlog\nexp\n\b fθ(˜xi).fθ(˜x+\ni )a\nτ\n\t\nexp\n\b fθ(˜xi).fθ(˜x+\ni )a\nτ\n\t\n+ β\nK\nP\nk̸=i exp\n\b fθ(˜xi).fθ(˜xk)\nτ\n\t\n+λglobal\n\u001a\nb\nX\ni=1\nlog D\n\u0000fθ\n\u0000˜x+\ni\n\u0001\u0001\n+\nb\nX\ni=1\nlog D\n\u00001 −fθ\n\u0000˜x+\ni\n\u0001\na\n\u0001 \u001b\u001b\n,\n(12)\nwhere λglobal ≥0 is a parameter to trade off between the local and global views. Note that in (12), we indeed\npush the batch adversarial examples fθ\n\u0000˜x+\ni\n\u0001\na globally far away from the batch of benign examples fθ (˜xi) by\nmaximizing the log-likelihood.\nMoreover, the discriminator D is updated to distinguish the batch adversarial examples fθ\n\u0000˜x+\ni\n\u0001\na and the\nbatch of benign examples fθ (˜xi) by maximizing the log-likelihood as in (11). Moreover, given the batch of\nadversarial examples, we update the feature extractor fθ as\nmin\nθ\n\u001a−1\nb\nb\nX\ni=1\nlog\nexp\n\b fθ(˜xi).fθ(˜x+\ni )a\nτ\n\t\nexp\n\b fθ(˜xi).fθ(˜x+\ni )a\nτ\n\t\n+ β\nK\nP\nk̸=i exp\n\b fθ(˜xi).fθ(˜xk)\nτ\n\t +λglobal\n\u001a\nb\nX\ni=1\nlog D\n\u0000fθ\n\u0000˜x+\ni\n\u0001\u0001\n+\nb\nX\ni=1\nlog D(1 −fθ\n\u0000˜x+\ni\n\u0001\na)\n\u001b\n−λbenign\nb\nb\nX\ni=1\nlog\nexp\n\b fθ(˜xi).fθ(˜x+\ni )\nτ\n\t\nexp\n\b fθ(˜xi).fθ(˜x+\ni )\nτ\n\t\n+ β\nK\nP\nk̸=i exp\n\b fθ(˜xi).fθ(˜xk)\nτ\n\t\n\u001b\n,\n(13)\nwhere we incorporate the InfoNCE loss on the benign examples with the trade-off parameter λbenign. Note that\nwe minimize the log-likelihood term w.r.t. θ to move the batch of adversarial examples closer to the batch of\nbenign examples.\n7\nFinally, we apply sharpness-aware minimization (SAM) [18] to the InfoNCE loss on the benign examples. The\nonly difference in computation is that we ﬁnd the perturbed model θa that maximizes the InfoNCE loss on\nthe benign examples and use the gradient at θa for this term when updating θ in a stochastic gradient descent\nmanner as in (13). It is worth noting that the ﬁrst two terms in (13) are optimized normally used their gradients\nat θ.\n5\nExperiments\n5.1\nExperimental Settings\nGeneral Settings:\nIn this section, we verify the effectiveness of our method through conducting\nexperiments on the CIFAR-10 dataset [54]. This dataset consists of 50,000 training images and 10,000 testing\nimages, all with a resolution of 32 × 32 pixels. To prepare the inputs, we follow the list of augmentations listed\nin [2] except for normalization, where we scale pixel values to the range of [0, 1]. This is to ensure that the\nadversarial attacks on these inputs can be properly projected into a valid image.\nRegarding model architectures, we use ResNet-18 [55] for our feature extractor backbone, and a simple single-\nlayer perceptron for the discriminator. Given that the complexity of the embedding space and informativeness\nof extracted features are enough to work with a perceptron classiﬁer in linear evaluation, a simple perceptron\nmodel would also be sufﬁcient in this case. All experiments are run on a single A100 GPU.\nSSL Settings:\nFor the contrastive learning process, we follow the settings generally used in existing works\n[14, 13]. Speciﬁcally, in the ﬁrst phase, we connect our ResNet-based feature extractor with a 2-layer perceptron\nprojection head, and adversarially-train it for 500 epochs. The optimization process is done with the SAM [18]\noptimizer for the benign loss, and standard SGD for the adversarial and discriminator loss terms.\nIn the linear evaluation phase, we drop the projection head, and instead attach a linear classiﬁer onto our feature\nextractor. We adversarially train this classiﬁer for 100 epochs. For the actual evaluation, the discriminator is\nnot used in the adversarial generation process.\nAttack Settings:\nWe use Projected Gradient Descent [6] as the standard adversarial attack to both generate\nadversarial examples during training, and evaluate robustness of the defense methods in this paper. All attacks\nare conducted under ℓ∞-norm with budget 8/255, and different other parameters at each step: we use PGD-7\nwith step size 2/255 for contrastive learning; PGD-10 with step size 2/255 for robust linear evaluation; and\nPGD-20 with step size 0.8/255 for ﬁnal classiﬁcation evaluation.\n5.2\nExperimental Results\nTo verify our theoretical ﬁndings, we design the experiments accordingly to Remark 4.9. First, we show that\nminimizing the InfoNCE loss on the benign samples in addition to the adversarial training loss can improve\nboth clean and robust accuracy. These results can generally be further improved by applying sharpness-aware\nminimization to the optimization process. Second, adding the discriminator to the original training process leads\nto a higher robust accuracy, albeit at the expense of a small decrease in clean accuracy. Third, by combining\nboth of these improvements, we arrive at our full method, which yields an even higher accuracy on most of the\nsettings.\nFinding 1: Impact of the Benign Loss Term (with Sharpness-Aware Minimization)\nWe experiment with incorporating a benign loss term in addition to the traditional adversarial loss. Surprisingly,\nas the benign loss weight increases, we improve both clean and robust accuracy of our model in the linear\nevaluation (LE) phase, both with and without adversarial training (AT). In addition, by applying SAM on the\nbenign loss, we can increase the performance even further. The full results are reported in Table 1.\nFinding 2: Impact of the Discriminator in Training\nAdding our discriminator to the original adversarial training scheme, we get a better adversarial accuracy, while\ntrading off some clean accuracy for it. This is consistent with the adversarial literature, where we cannot have\nboth [7, 56]. The full results are reported in Table 2.\nAblation Studies: Putting Everything Together\nWe now incorporate all of the above improvements into a complete training method to verify their effectiveness.\nAcross different benign loss weighting, we can see that the techniques of our method generally improve both\nclean and adversarial accuracy. Especially, for the AT-LE evaluation which furthest enhances robustness to our\n8\nTable 1: Clean and robust accuracy of a ResNet-18 model trained with and without SAM, across different\nweighting for the benign loss term.\nλbenign\nNo SAM\nSAM\nLE\nAT-LE\nLE\nAT-LE\nClean\nAdv.\nClean\nAdv.\nClean\nAdv.\nClean\nAdv.\n0\n70.54\n38.97\n66.28\n42.73\n-\n-\n-\n-\n0.5\n77.54\n40.72\n72.84\n44.23\n77.83\n40.09\n73.33\n44.23\n1.0\n81.14\n41.08\n76.36\n44.69\n81.31\n41.14\n76.98\n44.95\n2.0\n84.80\n41.78\n81.35\n45.71\n85.09\n41.97\n81.64\n46.30\nTable 2: Clean and robust accuracy of a ResNet-18 model trained with and without a discriminator, across\ndifferent weighting for the benign loss term.\nλbenign\nNo Discriminator\nDiscriminator\nLE\nAT-LE\nLE\nAT-LE\nClean\nAdv.\nClean\nAdv.\nClean\nAdv.\nClean\nAdv.\n0\n70.54\n38.97\n66.28\n42.73\n71.04\n39.08\n66.15\n42.57\n0.5\n77.54\n40.72\n72.84\n44.23\n77.66\n40.83\n73.32\n44.46\n1.0\n81.14\n41.08\n76.36\n44.69\n81.02\n41.34\n76.95\n44.90\n2.0\n84.80\n41.78\n81.35\n45.71\n84.12\n41.93\n80.92\n45.89\nexpectation, our full method outperforms any individual improvements on their own. The experiments’ exact\nnumbers are reported in Table 3.\nTable 3: Clean and robust accuracy of a ResNet-18 model trained with and without our method’s components,\nacross different weighting for the benign loss term. SimCLR refers to the method uses the benign loss without\nSAM and Discriminator, and Ours to the method with full components.\nλbenign\n1.0\n2.0\nMethod\nLE\nAT-LE\nLE\nAT-LE\nClean\nAdv.\nClean\nAdv.\nClean\nAdv.\nClean\nAdv.\nSimCLR\n81.14\n41.08\n76.36\n44.69\n84.80\n41.78\n81.35\n45.71\nSAM\n81.31\n41.14\n76.98\n44.95\n85.09\n41.97\n81.64\n46.30\nDiscriminator\n81.02\n41.34\n76.95\n44.90\n84.12\n41.93\n80.92\n45.89\nFull\n81.64\n41.23\n77.33\n45.28\n85.01\n41.82\n81.86\n46.45\n6\nDiscussion\n6.1\nBroader Impact\nOur surprising result on the effect of weighting benign loss term, to the best of our knowledge, is a ﬁrst. As we\ncombine these losses and receive improvements on both clean and robust accuracy, it is counter-intuitive to the\nexisting notion that there always exist a tradeoff between the two values. Delving into this phenomenon could\nbe an interesting future research direction, which would help us further understand the nature of adversarial\nvulnerabilities.\n6.2\nLimitations\nAs with any adversarial training process, the time needed to train a model increases signiﬁcantly. This additional\ncost is doubled if we really want to squeeze out the highest accuracy, by regenerating adversarial examples right\nbefore training our discriminator instead of reusing which was used to train the feature extractor. Furthermore, a\nGAN-style training is prone to convergence issues. However, we have not encountered such problems, possibly\ndue to the extremely simple discriminator used.\n9\n7\nConclusion\nContrastive learning (CL) is a powerful technique that enables the creation of meaningful features without\nrelying on labeled information. In the initial phase of CL, the focus lies on learning these features, which are\nsubsequently classiﬁed using a linear classiﬁer trained on labeled data. Although previous theoretical studies\nhave explored the relationship between the supervised loss in the second phase and the unsupervised loss\nin the ﬁrst phase, aiming to explain how the unsupervised loss supports the supervised loss, no theoretical\ninvestigation has examined the connection between the unsupervised loss in the ﬁrst phase and the robust\nsupervised loss in the second phase. Such an analysis can provide valuable insights into establishing an effective\nunsupervised loss in the initial phase. To address this research gap, our paper develops rigorous theories to\nidentify the speciﬁc components within the supervised loss that contribute to the robustness of the supervised\nloss. Lastly, we conduct a series of experiments to validate our theoretical ﬁndings.\n10\nReferences\n[1] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. 1, 2, 3\n[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\nA simple framework for\ncontrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. 1, 3, 8, 27\n[3] Misra Ishan and Maaten Laurens van der. Self-supervised learning of pretext-invariant representations.\narXiv preprint arXiv:1912.01991, 2019.\n[4] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nContrastive multiview coding.\narXiv preprint\narXiv:1906.05849, 2019. 1, 2, 3\n[5] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572, 2014. 1, 3, 4, 14\n[6] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\ndeep learning models resistant to adversarial attacks.\nIn International Conference on Learning\nRepresentations, 2018. 1, 4, 8, 14\n[7] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.\nTheoretically principled trade-off between robustness and accuracy.\nIn Proceedings of the 36th\nInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning\nResearch, pages 7472–7482, 2019. 1, 2, 4, 8, 14\n[8] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow.\nImproving the robustness of deep\nneural networks via stability training. In Proceedings of the ieee conference on computer vision and\npattern recognition, pages 4480–4488, 2016. 1\n[9] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. arXiv preprint arXiv:1903.12261, 2019. 1\n[10] Minseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 2983–2994. Curran Associates, Inc., 2020. 1, 3\n[11] Chih-Hui Ho and Nuno Nvasconcelos. Contrastive learning with adversarial examples. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing\nSystems, volume 33, pages 17081–17093. Curran Associates, Inc., 2020. 3\n[12] Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang.\nRobust pre-training by adversarial\ncontrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 16199–16210. Curran Associates, Inc., 2020.\n1, 3\n[13] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased\ncontrastive learning. Advances in Neural Information Processing Systems, 33, 2020. 2, 3, 8\n[14] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard\nnegative samples. International Conference on Learning Representations, 2021. 2, 3, 8\n[15] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A\ntheoretical analysis of contrastive unsupervised representation learning.\nIn Kamalika Chaudhuri and\nRuslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Research, pages 5628–5637. PMLR, 09–15 Jun 2019. 2,\n3, 4\n[16] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment\nand uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929–9939.\nPMLR, 2020. 2, 3\n[17] Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin.\nChaos is a ladder:\nA new theoretical understanding of contrastive learning via augmentation overlap.\narXiv preprint\narXiv:2203.13457, 2022. 2, 3, 4\n[18] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for\nefﬁciently improving generalization. In International Conference on Learning Representations, 2021. 2,\n6, 8, 23\n11\n[19] Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In International\nConference on Machine Learning, pages 8093–8104. PMLR, 2020. 2\n[20] Sepp Hochreiter and J¨urgen Schmidhuber. Simplifying neural nets by discovering ﬂat minima. In NIPS,\npages 529–536. MIT Press, 1994. 2\n[21] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization\nin deep learning. Advances in neural information processing systems, 30, 2017.\n[22] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep\nnets. In International Conference on Machine Learning, pages 1019–1028. PMLR, 2017.\n[23] Stanislav Fort and Surya Ganguli. Emergent properties of the local geometry of neural loss landscapes.\narXiv preprint arXiv:1910.05929, 2019. 2\n[24] Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regularizing\nneural networks by penalizing conﬁdent output distributions. In ICLR (Workshop). OpenReview.net, 2017.\n2\n[25] Pratik Chaudhari, Anna Choroma´nska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,\nJennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: biasing gradient descent into\nwide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019, 2017.\n[26] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang.\nOn large-batch training for deep learning: Generalization gap and sharp minima.\nIn ICLR.\nOpenReview.net, 2017. 2\n[27] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson.\nAveraging weights leads to wider optima and better generalization. In UAI, pages 876–885. AUAI Press,\n2018. 2\n[28] Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and\nAmos J. Storkey. Three factors inﬂuencing minima in sgd. ArXiv, abs/1711.04623, 2017. 2\n[29] Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of dropout. In\nInternational conference on machine learning, pages 10181–10192. PMLR, 2020. 2\n[30] Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu.\nDeep mutual learning.\n2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4320–4328, 2018. 2\n[31] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own\nteacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 3713–3722, 2019. 2\n[32] Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. Sharp-maml: Sharpness-aware\nmodel-agnostic meta learning. arXiv preprint arXiv:2206.03996, 2022. 2\n[33] Zhe Qu, Xingyu Li, Rui Duan, Yao Liu, Bo Tang, and Zhuo Lu. Generalized federated learning via\nsharpness aware minimization. arXiv preprint arXiv:2206.02618, 2022. 2\n[34] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without\npre-training or strong data augmentations. arXiv preprint arXiv:2106.01548, 2021. 2\n[35] Dara Bahri, Hossein Mobahi, and Yi Tay.\nSharpness-aware minimization improves language model\ngeneralization.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7360–7371, Dublin, Ireland, May 2022. Association for\nComputational Linguistics. 2\n[36] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and\nSungrae Park. Swad: Domain generalization by seeking ﬂat minima. Advances in Neural Information\nProcessing Systems, 34:22405–22418, 2021. 2\n[37] Hoang Phan, Ngoc Tran, Trung Le, Toan Tran, Nhat Ho, and Dinh Phung. Stochastic multiple target\nsampling gradient descent. Advances in neural information processing systems, 2022. 2\n[38] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware\nminimization for scale-invariant learning of deep neural networks.\nIn International Conference on\nMachine Learning, pages 5905–5914. PMLR, 2021. 3\n12\n[39] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar\nTatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training.\narXiv preprint arXiv:2203.08065, 2022. 3\n[40] Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent YF Tan, and Joey Tianyi Zhou. Sharpness-aware training\nfor free. arXiv preprint arXiv:2205.14083, 2022. 3\n[41] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efﬁcient and scalable\nsharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12360–12370, 2022. 3\n[42] Grill Jean-Bastien, Strub Florian, Altch´e Florent, Tallec Corentin, Richemond Pierre H., Buchatskaya\nElena, Doersch Carl, Pires Bernardo Avila, Guo Zhaohan Daniel, Azar Mohammad Gheshlaghi, Piot\nBilal, Kavukcuoglu Koray, Munos R´emi, and Michal Valko. Bootstrap your own latent: A new approach\nto self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. 3\n[43] Xinlei Chen and Kaiming He.\nExploring simple siamese representation learning.\narXiv preprint\narXiv:2011.10566, 2020. 3\n[44] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. 2020. 3\n[45] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.\nUnderstanding dimensional collapse in\ncontrastive self-supervised learning. arXiv preprint arXiv:2110.09348, 2021. 3\n[46] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 3\n[47] Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, and Dinh Phung.\nImproving adversarial robustness by enforcing local and global compactness. In Proceedings of ECCV,\npages 209–223. Springer, 2020. 3\n[48] Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, and Dinh Phung.\nUnderstanding\nand achieving efﬁcient robustness with adversarial supervised contrastive learning.\narXiv preprint\narXiv:2101.10027, 2021. 3\n[49] Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray. Metric learning for\nadversarial robustness. Advances in Neural Information Processing Systems, 32, 2019. 3\n[50] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of\ndiverse parameter-free attacks. In Proceedings of ICML, 2020. 4, 14\n[51] David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference on\nComputational learning theory, pages 164–170, 1999. 6, 23\n[52] Pierre Alquier, James Ridgway, and Nicolas Chopin. On the properties of variational approximations of\ngibbs posteriors. Journal of Machine Learning Research, 17(236):1–41, 2016. 6, 21, 23\n[53] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM,\n63(11):139–144, 2020. 7, 27\n[54] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 8\n[55] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n8\n[56] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang.\nUnderstanding\nand mitigating the tradeoff between robustness and accuracy. In International Conference on Machine\nLearning, pages 7909–7919. PMLR, 2020. 8\n[57] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi.\nAsam: Adaptive sharpness-\naware minimization for scale-invariant learning of deep neural networks. In Marina Meila and Tong\nZhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of\nProceedings of Machine Learning Research, pages 5905–5914. PMLR, 18–24 Jul 2021. 27\n13\nAppendix\nA\nProblem Formulation and Notions\nIn this section, we present the problem formulation of self-supervised learning and the notions used in our\nfollowing theory development.\nWe consider an M-class classiﬁcation problem with the label set Y =\n{1, 2, ..., M}. Given a class c ∈Y, the class-condition distribution for this class has the density function\npc (x) = p (x | y = c) where x ∈Rd speciﬁes a data example. Therefore, the entire data distribution has the\nform\npdata (x) =\nM\nX\nc=1\nπcp (x | y = c) =\nM\nX\nc=1\nπcpc (x) ,\nwhere πc = P (y = c) , c ∈Y is a class probability.\nThe distribution of positive pairs over Rd × Rd is formulated as\nppos\n\u0000x, x+\u0001\n=\nM\nX\nc=1\nπcpc (x) pc\n\u0000x+\u0001\n.\nIt is worth noting that with the above equality, ppos\n\u0000x, x+\u0001\nis relevant to the probability that x, x+ ∼pdata\npossess the same label. Particularly, to form a positive pair\n\u0000x, x+\u0001\n, we ﬁrst sample a class c ∼Cat (π) from\nthe categorical distribution with π = [πc]M\nc=1, and then sample x, x+ ∼pc.\nThe general unsupervised InfoNCE loss over the entire data and positive pair distributions is denoted as\nLun\nDun (θ, ppos) = E(x,x+)∼ppos,x−\n1:K\niid\n∼pdata\n\"\n−log\nexp\n\b fθ(x)·fθ(x+)\nτ\n\t\nexp\n\b fθ(x)·fθ(x+)\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x)·fθ(x−\nk )\nτ\n\t\n#\n,\n(14)\nwhere fθ with θ ∈Θ is a feature extractor, the operation fθ(x) · fθ(ex) presents the inner product, τ > 0 is\nthe temperature variable, K is the number of used negative examples, and Dun denotes the distribution over\nz =\nh\nx, x+,\n\u0002\nx−\nk\n\u0003K\nk=1\ni\nwith\n\u0000x, x+\u0001\n∼ppos, x−\n1:K ∼pdata. Note that β ≥0 is a parameter and setting β = K\nrecovers the original formula of contrastive learning.\nIt is our ultimate goal to minimize the general unsupervised InfoNCE loss. However, in reality, we work with\na speciﬁc training set S =\nn\nzi =\nh\nxi, x+\ni ,\n\u0002\nx−\nik\n\u0003K\nk=1\nioN\ni=1 where z1:N ∼Dun. The empirical unsupervised\nInfoNCE loss over S is deﬁned as\nLun\nS (θ, ppos) = −1\nN ×\nN\nX\ni=1\nlog\nexp\n\b fθ(xi)·fθ(x+\ni )\nτ\n\t\nexp\n\b fθ(xi)·fθ(x+\ni )\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(xi)·fθ(x−\nik)\nτ\n\t.\nSSL aims to minimize the empirical unsupervised InfoNCE loss over a speciﬁc training set S to learn an optimal\nfeature extractor fθ∗, used in the second phase for training a linear classiﬁer on top of its extracted features.\nGiven a feature extractor fθ and a linear classiﬁer that parameterized by a weight matrix W , we deﬁne the\ngeneral loss induced by this couple as\nLsup\nDsup (θ, W ) = E(x,y)∼Dsup [τCE (W fθ (x) , y)] ,\nwhere Dsup (x, y) = πypy (x) is the data-label distribution and τCE (·, ·) is the τ-temperature cross-entropy loss\n(i.e., softmax with a temperature τ applied to logits before computing the cross-entropy loss).\nGiven the fact that we aim to train the optimal linear classiﬁer in the second phase, we deﬁne the optimal\ngeneral loss over all weight matrices W as\nLsup\nDsup (θ) = min\nW Lsup\nDsup (θ, W ) .\nWe now present the losses for the robust SSL. Given an attack a (i.e., FGSM [5], PGD [6], TRADES [7],\nand AutoAttack [50]) and a weight matrix W , we denote aW,θ(x) ∈Bǫ(x) = {x′ : ∥x′ −x∥≤ǫ} as the\nadversarial example obtained by using a to attack the second phase classiﬁer, formed by fθ and W , on a benign\nexample x with a label y. We deﬁne the adversarial loss using the adversary a to attack W and fθ as follows:\nLadv\nDsup (W, θ, a) = E(x,y)∼Dsup\nh\nτCE (W fθ (aW,θ (x)) , y)1/2i\n.\n(15)\nOur next step is to minimize the above adversarial loss to deﬁne the best adversarial loss corresponding to the\nmost robust linear classiﬁer when using the adversary a to attack the classiﬁer in the second phase:\nLadv\nDsup (θ, a) = min\nW Ladv\nDsup (W, θ, a) .\n(16)\n14\nB\nTheory Development\nWe ﬁrst develop necessary theories to tackle the optimization problem of the adversarial loss Ladv\nDsup(θ, a). Let\nus denote p\naW,θ\nadv\n= aW,θ#pdata as the adversarial distribution formed by using the attack transformation aW,θ\nto transport the data distribution pdata. It is obvious by the deﬁnition that p\naW,θ\nadv\nconsists of adversarial examples\naW,θ(x) with the benign example x ∼pdata. In addition, we deﬁne qdata = fθ#pdata and q\naW,θ\nadv\n= fθ#p\naW,θ\nadv\nas the benign and adversarial distributions on the latent space, respectively.\nThe following theorem indicates the upper bounds for the adversarial loss of interest Ladv\nDsup(θ, a).\nTheorem B.1. Consider the adversarial loss Ladv\nDsup(θ, a).\ni) We have the ﬁrst upper bound on the data space\nLadv\nDsup (θ, a) ≤min\nW\nn\nLsup\nDsup (θ, W )1/2 + Lsup\nDsup (θ, W )1/2 Dv\n\u0000p\naW,θ\nadv , pdata\n\u00011/2 o\n.\n(17)\nii) We have the second upper bound on the latent space\nLadv\nDsup (θ, a) ≤min\nW\nn\nLsup\nDsup (θ, W )1/2 + Lsup\nDsup (θ, W )1/2 Dv\n\u0000q\naW,θ\nadv , qdata\n\u00011/2 o\n.\n(18)\nHere we note that Dv represents a f-divergence with the convex function v(t) = (t −1)2.\nProof. (i) We ﬁrst prove (i).\nLadv\nDsup (W, θ, a) =E(x,y)∼Dsup\nh\nτCE (W fθ (aW,θ (x)) , y)1/2i\n=\nZ\nτCE (W fθ (aW,θ (x)) , y)1/2 pdata (x, y) dxdy\n=\nZ\nτCE (W fθ (x) , y)1/2 p\naW,θ\ndata\n(x, y) dxdy\n=\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y) dxdy\n+\nZ\nτCE (W fθ (x) , y)1/2 \u0000p\naW,θ\ndata\n(x, y) −pdata (x, y)\n\u0001\ndxdy\n≤\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y) dxdy\n+\nZ\nτCE (W fθ (x) , y)1/2 \f\fp\naW,θ\ndata\n(x, y) −pdata (x, y)\n\f\f dxdy,\n(19)\nwhere p\naW,θ\ndata\n(x, y) = pdata (y | x) p\naW,θ\ndata\n(x) is the distribution over (aW,θ (x) , y) where (x, y) ∼pdata (x, y).\nZ\nτCE (W fθ (x) , y)1/2 \f\fp\naW,θ\ndata\n(x, y) −pdata (x, y)\n\f\f dxdy\n=\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y)1/2\n\f\fp\naW,θ\ndata\n(x, y) −pdata (x, y)\n\f\f\npdata (x, y)\npdata (x, y)1/2 dxdy\n(1)\n≤\n\u0014Z\nτCE (W fθ (x) , y) pdata (x, y) dxdy\n\u00151/2 \"Z \u0000p\naW,θ\ndata\n(x, y) −pdata (x, y)\n\u00012\npdata (x, y)2\npdata (x, y) dxdy\n#1/2\n= EDsup [τCE (W fθ (x) , y)]1/2\n\"Z \u0012p\naW,θ\ndata\n(x, y)\npdata (x, y) −1\n\u00132\npdata (x, y) dxdy\n#1/2\n= EDsup [τCE (W fθ (x) , y)]1/2\n\"Z \u0012p\naW,θ\ndata\n(x)\npdata (x) −1\n\u00132\npdata (x, y) dxdy\n#1/2\n= EDsup [τCE (W fθ (x) , y)]1/2\n\"Z \u0012p\naW,θ\ndata\n(x)\npdata (x) −1\n\u00132 X\ny\npdata (x, y) dx\n#1/2\n= EDsup [τCE (W fθ (x) , y)]1/2\n\"Z \u0012p\naW,θ\ndata\n(x)\npdata (x) −1\n\u00132\npdata (x) dx\n#1/2\n= Lsup\nDsup (θ, W )1/2 Dv\n\u0000p\naW,θ\ndata , pdata\n\u00011/2 .\n(20)\n15\nNote that in\n(1)\n≤, we use the Cauchy-Schwarz inequality.\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y) dxdy\n=\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y)1/2 pdata (x, y)1/2 dxdy\n(2)\n≤\n\u0014Z\nτCE (W fθ (x) , y) pdata (x, y) dxdy\n\u00151/2 \u0014Z\npdata (x, y) dxdy\n\u00151/2\n=Lsup\nDsup (θ, W )1/2 .\n(21)\nNote that in\n(2)\n≤, we use the Cauchy-Schwarz inequality.\nBy combining (19), (20), and (21), we reach the conclusion.\n(ii) We now prove (ii).\nLadv\nDsup (W, θ, a) =E(x,y)∼Dsup\nh\nτCE (W fθ (aW,θ (x)) , y)1/2i\n=\nZ\nτCE (W fθ (aW,θ (x)) , y)1/2 pdata (x, y) dxdy\n=\nZ\nτCE (W fθ (x) , y)1/2 p\naW,θ\ndata\n(x, y) dxdy\n≤\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y) dxdy\n+\nZ\nτCE (W fθ (x) , y)1/2 \f\fp\naW,θ\ndata\n(x, y) −pdata (x, y)\n\f\f dxdy\n=\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y) dxdy\n+\nZ\nτCE (W z, y)1/2 \f\fq\naW,θ\ndata\n(z, y) −qdata (z, y)\n\f\f dxdy,\n(22)\nwhere z = fθ (x), qdata (z, y) = qdata (y | z) qdata (z) with qdata (z) =\nR\nf−1\nθ\n(z) pdata (x) dx and qdata (y | z) =\nR\nf−1\nθ\n(z) pdata(y|x)pdata(x)dx\nR\nf−1\nθ\n(z) pdata(x)dx\n, and q\naW,θ\ndata\n(z, y) = qdata (y | z) q\naW,θ\ndata\n(z).\nUsing the same derivations as above, we reach\nZ\nτCE (W z, y)1/2 \f\fq\naW,θ\ndata\n(z, y) −qdata (z, y)\n\f\f dxdy\n≤E(z,y)∼qdata(z,y) [τCE (W z, y)]1/2 Dv\n\u0000q\naW,θ\ndata\n, qdata\n\u00011/2 .\n(23)\nZ\nτCE (W fθ (x) , y)1/2 pdata (x, y) dxdy ≤Lsup\nDsup (θ, W )1/2 .\n(24)\nWe next prove that\nE(z,y)∼qdata(z,y) [τCE (W z, y)] = E(x,y)∼pdata(x,y) [τCE (W fθ (x) , y)] = Lsup\nDsup (θ, W ) .\n(25)\n16\nTo this end, we derive as\nE(z,y)∼qdata(z,y) [τCE (W z, y)] =\nZ\nZ\nτCE (W z, y) qdata (z, y) dzdy\n=\nX\ny\nZ\nZ\nτCE (W z, y)\nR\nf−1\nθ\n(z) pdata (y | x) pdata (x) dx\nR\nf−1\nθ\n(z) pdata (x) dx\nZ\nf−1\nθ\n(z)\npdata (x) dxdz\n=\nX\ny\nZ\nZ\nτCE (W z, y)\nZ\nf−1\nθ\n(z)\npdata (y | x) pdata (x) dxdz\n=\nX\ny\nZ\nZ\nZ\nX\nIx∈f−1\nθ\n(z)τCE (W z, y) pdata (y | x) pdata (x) dxdz\n(1)\n=\nX\ny\nZ\nX\nZ\nZ\nIz=fθ(x)τCE (W z, y) pdata (y | x) pdata (x) dzdx\n=\nX\ny\nZ\nX\nτCE (W fθ (x) , y) pdata (y | x) pdata (x) dx\n=\nZ\nX\nτCE (W fθ (x) , y) pdata (x, y) dxdy = Lsup\nDsup (θ, W ) ,\nwhere we have\n(1)\n= due to the Fubini theorem and IA is the indicator function which returns 1 if A is true and 0\notherwise.\nBy combining (22), (23), (24), and (25), we reach the conclusion.\nBoth bounds in Theorem B.1 have the same form but they are deﬁned on the data and latent spaces, respectively.\nAdditionally, the upper bound in (18) reveals that to minimize the adversarial loss for the adversary a, we need\nto search the feature extractor fθ and the weight matrix W for minimizing the supervised loss Lsup\nDsup (θ, W )\nand the divergence Dv\n\u0000q\naW,θ\nadv\n, qdata\n\u0001\nbetween the benign and adversarial distributions on the latent space.\nWe now go further to discover which losses should be involved in the ﬁrst phase to minimize the upper bound\nin (18). To this end, we denote\nA = {a (· | x) with x ∼pdata : a (· | x) has a support set over Bǫ (x)} ,\nqa = fθ#(a#pdata) for a ∈A.\nwhere Bǫ(x) speciﬁes the ball with the radius ǫ around x. A consists of the stochastic map a(· | x) with x ∼\npdata, working on the ball Bǫ(x), which can be considered as the set of all possible attacks. By its deﬁnition, the\nstochastic map p\naW,θ\nadv\ncan be represented by an element in A as long as aW,θ ∈Bǫ(x). The following theorem\nassists us in converting the upper bound in (18) to the one directly in the phase 1.\nTheorem B.2. The adversarial loss Ladv\nDsup(θ, a) can be further upper-bounded by\nLadv\nDsup (θ, a) ≤[Lun\nDun (θ, ppos) + A (K, β)]1/2 \u0002\n1 + max\na∈A Dv (qa, qdata)\n1\n2 \u0003\n,\n(26)\nwhere A(K, β) = −O\n\u00001\n√\nK\n\u0001\n−log β −O\n\u0000 1\nβ\n\u0001\n.\nProof. By choosing ¯\nWc = Ex∼pc [fθ (x)] , ∀c ∈Y and ¯\nW =\n\u0002 ¯\nWc\n\u0003C\nc=1, we have\nLadv\nDsup (θ, a) ≤min\nW\nn\nLsup\nDsup (θ, W )1/2 + Lsup\nDsup (θ, W )1/2 Dv\n\u0000q\naW,θ\nadv\n, qdata\n\u00011/2 o\n≤Lsup\nDsup\n\u0000θ, ¯\nW\n\u00011/2 + Lsup\nDsup\n\u0000θ, ¯\nW\n\u00011/2 Dv\n\u0010\nq\na ¯\nW ,θ\nadv\n, qdata\n\u00111/2\n.\n(27)\n(1) We now prove that\nLsup\nDsup\n\u0000θ, ¯\nW\n\u0001\n≤Lun\nDun (θ, ppos) + A (K, β) .\n(28)\nWe derive as follows:\n17\nFirst step:\nLsup\nDsup\n\u0000θ, ¯\nW\n\u0001\n= E(x,y)∼Dsup\n\u0002\nτCE\n\u0000 ¯\nW · fθ (x) , y\n\u0001\u0003\n=\nM\nX\nc=1\nπcEx∼pc\n\u0002\nτCE\n\u0000 ¯\nW · fθ (x) , c\n\u0001\u0003\n= −\nM\nX\nc=1\nπcEx∼pc\n\"\nlog\nexp\n\b ¯\nWc · fθ (x) /τ\n\t\nPM\ny=1 exp\n\b ¯\nWy · fθ (x) /τ\n\t\n#\n= −\nM\nX\nc=1\nπcEx∼pc\n\"\n¯\nWc · fθ (x)\nτ\n−log\n M\nX\ny=1\nexp\n\u001a ¯\nWy · fθ (x)\nτ\n\u001b!#\n= −\nM\nX\nc=1\nπc\nτ\n¯\nWc · Ex∼pc [fθ (x)] +\nM\nX\nc=1\nπcEx∼pc\n\"\nlog\n M\nX\ny=1\nexp\n\u001a ¯\nWy.fθ (x)\nτ\n\u001b!#\n= −\nM\nX\nc=1\nπc\nτ ∥¯\nWc∥2 +\nM\nX\nc=1\nπcEx∼pc\n\"\nlog\n M\nX\ny=1\nexp\n\u001a ¯\nWy · fθ (x)\nτ\n\u001b!#\n.\n(29)\nDeﬁne\nL\nun\nDun (θ, ppos) = E(x,x+)∼ppos\n\u0014\n−1\nτ fθ (x) .fθ\n\u0000x+\u0001\u0015\n+Ex∼pdata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)##\n,\nwe then show an lower bound for L\nun\nDun(θ, ppos):\nL\nun\nDun (θ, ppos) = E(x,x+)∼ppos\n\u0014\n−1\nτ fθ (x) · fθ\n\u0000x+\u0001\u0015\n+ Ex∼pdata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) · fθ\n\u0000x−\u0001\nτ\n)##\n= E(x,x+)∼ppos\n\u0014\n−1\nτ fθ (x) · fθ\n\u0000x+\u0001\u0015\n+ Ex∼pdata\n\"\nlog\n M\nX\nc=1\nπcEx−∼pc\n\"\nexp\n(\nfθ (x) · fθ\n\u0000x−\u0001\nτ\n)#!#\n≥E(x,x+)∼ppos\n\u0014\n−1\nτ fθ (x) · fθ\n\u0000x+\u0001\u0015\n+ Ex∼pdata\n\"\nlog\n M\nX\nc=1\nπc exp\n(\nfθ (x) · Ex−∼pc\n\u0002\nfθ\n\u0000x−\u0001\u0003\nτ\n)!#\n= −1\nτ\nM\nX\nc=1\nπcEx,x+∼pc\n\u0002\nfθ (x) · fθ\n\u0000x+\u0001\u0003\n+ Ex∼pdata\n\"\nlog\n M\nX\nc=1\nπc exp\n\u001aWc · fθ (x)\nτ\n\u001b!#\n= −1\nτ\nM\nX\nc=1\nπcEx∼pc [fθ (x)] · Ex+∼pc\n\u0002\nfθ\n\u0000x+\u0001\u0003\n+ Ex∼pdata\n\"\nlog\n M\nX\nc=1\nπc exp\n\u001aWc · fθ (x)\nτ\n\u001b!#\n= −\nM\nX\nc=1\nπc\nτ ∥¯\nWc∥2 +\nM\nX\nc=1\nπcEx∼pc\n\"\nlog\n M\nX\ny=1\nexp\n\u001a ¯\nWy · fθ (x)\nτ\n\u001b!#\n.\n(30)\nCombining (29) and (30) , we get\nLsup\nDsup(θ, ¯\nW) ≤L\nun\nDun(θ, ppos).\nSecond step: We start with decomposing the Lun\nDun(θ, ppos)\nLun\nDun(θ, ppos) = E(x,x+)∼ppos,x−\n1:K∼pdata\n\n−log\nexp\n\b fθ(x)·fθ(x+)\nτ\n\t\nexp\n\b fθ(x)·fθ(x+)\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x)·fθ(x−\nk )\nτ\n\t\n\n\n= E(x,x+)∼ppos,x−\n1:K∼pdata\nh\n−fθ(x) · fθ(x+)\nτ\n+ log\n\u0010\nexp\nnfθ(x) · fθ(x+)\nτ\no\n+ β\nK\nK\nX\nk=1\nexp\nnfθ(x) · fθ(x−\nk )\nτ\no\u0011i\n.\nThe ﬁrst term of L\nun\nDun(fθ) is the same as the ﬁrst term of Lun\nDun(fθ). Thus, we only have to deal with the second\nterms of both quantities. We have\nEx∼pdata\nh\nlog\n\u0010\nEx−∼pdata\nh\nexp\n\bfθ(x) · fθ(x−)\nτ\n\ti\u0011i\n+ log β = Ex∼ppos\nh\nlog\n\u0010\nβEx−∼pdata\nh\nexp\n\bfθ(x) · fθ(x−)\nτ\n\ti\u0011i\n18\nTherefore,\nL\nun\nDun(θ, ppos) −Lun\nDun(θ, ppos) = E(x,x+)∼ppos,x−\n1:K∼pdata\n\"\nlog\n\u0010\nβEx−\nh\nexp\n\bfθ(x) · fθ(x−)\nτ\n\ti\u0011\n−\nlog\n\u0010\nexp\nnfθ(x) · fθ(x+)\nτ\no\n+ β\nK\nK\nX\nk=1\nexp\nnfθ(x) · fθ(x−\nk )\nτ\no\u0011#\nDenote\n1\nK\nK\nX\nk=1\nexp\nnfθ(x) · fθ(x−\nk )\nτ\no\n−Ex−\nh\nexp\nnfθ(x) · fθ(x−)\nτ\noi\n:= Yn\nEx−\nh\nexp\nnfθ(x) · fθ(x−)\nτ\noi\n:= αx\nexp\nnfθ(x) · fθ(x+)\nτ\no\n:= Z.\nThe inner part of the expectation E(x,x+)∼ppos,x−\n1:K∼pdata is written as\nlog\n\u0000βαx\n\u0001\n−log\n\u0000β(Yn + αx) + Z\n\u0001\n= −log βYn + βαx + Z\nβαx\n= −log\n\u0010\n1 +\nZ\nβαx + Yn\nαx\n\u0011\n= −log\n\u00001 +\nZ\nβαx\n\u0001\n−log\n\u0010\n1 +\nYn/αx\n1 + Z/(βαx)\n\u0011\nThus, we get\nE(x,x+)∼ppos,x−\n1:K∼pdata\nh\n−log\n\u00001 +\nZ\nβαx\n\u0001\n−log\n\u0010\n1 +\nYn/αx\n1 + Z/(βαx)\n\u0011i\n= E(x,x+)∼ppos\nh\n−log\n\u00001 +\nZ\nβαx\n\u0001i\n+\nE(x,x+)∼ppos,x−\n1:K∼pdata\nh\n−log\n\u0010\n1 +\nYn/αx\n1 + Z/(βαx)\n\u0011i\n.\nWe also have exp(−1/τ) ≤exp\nn\nfθ(x)·fθ(x−)\nτ\no\n≤exp(1/τ). It follows that exp(−1/τ) ≤Z, αx ≤\nexp(1/τ). Then we deduce bounds for other quantities\n1\nβ exp(−2/τ) ≤\nZ\nβαx ≤1\nβ exp(1/τ) exp(1/τ) = 1\nβ exp(2/τ)\nexp(−2/τ) ≤Yn + αx\nαx\n≤exp(2/τ)\nexp(−2/τ) −1 ≤Yn\nαx ≤exp(2/τ) −1\nexp(−2/τ) −1 ≤Yn\nαx +\nZ\nβαx ≤1\nβ exp(2/τ) + exp(2/τ) −1\nYn/αx\n1 + Z/(βαx) ≥\nexp(−2/τ) −1\n1 + 1\nβ exp(−2/τ)\nIn both terms inside the expectation E(x,x+)∼ppos,x−\n1:K∼pdata, we have the form log(1 + t).\nFor t > 0, we use the inequality\nlog(1 + t) ≤t.\nFor −1 < t < 0 we have\n\f\f log(1 + t)\n\f\f =\n\f\f log\n1\n1 −|t|\n\f\f =\n\f\f log(1 + |t| + |t|2 + . . .)\n\f\f ≤|t| + |t|2 + . . . + |t|m + . . . = |t|\n1\n1 −|t|.\nUse these inequalities for log(1 + t), we get\n\f\f\f −log\n\u0010\n1 +\nYn/αx\n1 + Z/(βαx)\n\u0011\f\f\f ≤\n\f\f\f\nYn/αx\n1 + Z/(βαx)\n\f\f\f ×\n1\n1 +\nexp(−2/τ)−1\n1+ 1\nβ exp(−2/τ)\n=\n\f\f\f\nYn/αx\n1 + Z/(βαx)\n\f\f\f ×\n1 + 1\nβ exp(−2/τ)\n(1 + 1/β) exp(−2/τ)\n≤\n\f\f\f\nYn/αx\n1 + Z/(βαx)\n\f\f\f exp(2/τ).\nBy Cauchy-Schwarz inequality,\nEx−\n1:K∼pdata|Yn| ≤\nq\nEx−\n1:K[Y 2\nn ] ≤\nr\nexp(2/τ) −exp(−2/τ)\nK\n.\n19\nTherefore,\n\f\f\f\fE(x,x+)∼ppos,x−\n1:K∼pdata\nh\n−log\n\u0010\n1 +\nYn/αx\n1 + Z/(βαx)\n\u0011i\f\f\f\f ≤E(x,x+)∼pposEx1:K∼pdata\n\f\f\f\nYn/αx\n1 + Z/(βαx)\n\f\f\f × exp(2/τ)\n≤1\nαx exp(2/τ) ×\nr\nexp(2/τ) −exp(−2/τ)\nK\n= O(K−1/2).\nFor the other term inside E(x,x+)∼ppos,x1:K∼pdata\nE(x,x+)∼ppos\nh\n−log\n\u00001 +\nZ\nβαx\n\u0001i\nis a constant and negative, its absolute value is bounded by\nE(x,x+)∼ppos\nh\nlog\n\u00001 +\nZ\nβαx\n\u0001i\n≤E(x,x+)∼ppos\nZ\nβαx ≤1\nβ exp(2/τ).\nFor a lower bound of log(1 + t), we have\ne\n1\n1+t −1 ≥\n1\n1 + t ⇒e\n1\nt+1 ≥\ne\n1 + t ⇒(1 + t)e\n1\n1+t ≥e ⇒(1 + t) ≥e\nt\n1+t ⇒log(1 + t) ≥\nt\n1 + t.\nHence,\nE(x,x+)∼ppos\nh\nlog\n\u00001 +\nZ\nβαx\n\u0001i\n≥E(x,x+)∼ppos\nh\nZ/(βαx)\n1 + Z/(βαx)\ni\n≥E(x,x+)∼ppos\nh Z\nβαx\ni\n×\n1\n1 + 1\nβ exp(2/τ)\n≥1\nβ\nexp(−2τ)\n1 + 1\nβ exp(2/τ).\nTogether, we have\nLun\nDun(θ, ppos) −L\nun\nDun(θ, ppos) = E(x,x+)∼ppos\nh\nlog\n\u00001 +\nZ\nβαx\n\u0001i\n+ O\n\u0000 1\n√\nK\n\u0001\n= O\n\u0000 1\nβ\n\u0001\n+ O\n\u0000 1\n√\nK\n\u0001\n.\nLast step: We have\nLsup\nDsup(θ, ¯\nW) ≥inf\nW Lsup\nDsup(θ, W ) = Lsup\nDsup(θ).\nIt follows that\nLsup\nDsup(θ) ≤Lun\nDun(θ, ppos) −O\n\u0010 1\n√\nK\n\u0011\n−log(β) −O\n\u0010 1\nβ\n\u0011\n.\n(2) It appears that\nDv\n\u0010\nq\na ¯\nW ,θ\nadv\n, qdata\n\u0011\n≤max\na∈A Dv (qa, qdata) ,\n(31)\nbecause a ¯\nW ,θ ∈A.\nBy combining (27), (28), and (31), we reach the conclusion.\nInequality (26) indicates that to achieve minθ Ladv\nDsup (θ, a), we can alternatively minimize its upper-bound\nwhich is relevant to Lun\nDun (θ). Unfortunately, minimizing Lun\nDun (θ, ppos) directly is intractable due to the\nunknown general distribution Dun. The following theorem resolves this issue and also signiﬁes the concept of\nsharpness for the feature extractor fθ.\nTheorem B.3. Under mild conditions, with the probability at least 1 −δ over the random choice of S ∼DN\nun,\nwe have the following inequality\nLun\nDun (θ, ppos) ≤\nmax\nθ′:∥θ′−θ∥<ρ Lun\nS\n\u0000θ′\u0001\n+\n1\n√\nN\nhT\n2 log\n\u0010\n1 + ∥θ∥2\nT σ2\n\u0011\n+ log 1\nδ + L2\n8 + 2L + O\n\u0000log(N + T )\n\u0001i\n,\nwhere L = 2\nτ + log(1 + β), T is the number of parameters in θ, and σ = ρ[\n√\nT +\np\nlog (N)]−1.\n20\nProof. Given z =\nh\nx, x+,\n\u0002\nx−\nk\n\u0003K\nk=1\ni\nwhere (x, x+) ∼ppos, x−\n1:K\niid∼pdata. We recall the loss function is\nℓ(fθ(x)) = −log\nexp\nn\nfθ(x)·fθ(x+)\nτ\no\nexp\nn\nfθ(x)·fθ(x+)\nτ\no\n+ β\nK\nPK\nk=1 exp\nn fθ(x)·fθ(x−\nk )\nτ\no\n= log\nexp\nn\nfθ(x)·fθ(x+)\nτ\no\n+ β\nK\nPK\nk=1 exp\nn fθ(x)·fθ(x−\nk )\nτ\no\nexp\nn\nfθ(x)·fθ(x+)\nτ\no\n≤log e1/τ + βe1/τ\ne−1/τ\n= 2\nτ + log(1 + β).\nWe use the PAC-Bayes theory for P = N(0, σ2\nP IT ) and Q = N(θ, σ2IT ) are the prior and posterior\ndistributions, respectively.\nBy using the bound in [52], with probability at least 1 −δ, we have\nLun\nDun(θ, Q) ≤Lun\nS (θ, Q) + 1\nβ\nh\nKL(Q∥P) + log 1\nδ + Ψ(β, N)\ni\n,\nwhere we have deﬁned\nΨ(β, N) = log EP EDN\nun\nh\nexp\nn\nβ\n\u0000Lun\nDun(θ) −Lun\nS (θ)\n\u0001oi\nSince the loss function is bounded by L, we have\nΨ(β, N) ≤β2L2\n8N .\nThus, we get\nLun\nDun(θ, Q) ≤Lun\nS (θ, Q) + 1\nβ\nh\nKL(Q∥P) + log 1\nδ + β2L2\n8N\ni\n.\n(32)\nBy Cauchy inequality,\n1\nβ\nh\nKL(Q∥P) + log 1\nδ + β2L2\n8N\ni\n= 1\nβ\nh\nKL(Q∥P) + log 1\nδ\ni\n+ βL2\n8N ≥\nq\nKL(Q∥P) + log 1\nδ\n√\n2N\n× L\n≥\np\nKL(Q∥P)\n√\n2N\n× L.\n(33)\nSince P and Q are Gaussian distribution, the KL divergence between Q and P is equal to\nKL(Q∥P) = 1\n2\nhT σ2 + ∥θ∥2\nσ2\nP\n−T + T log σ2\nP\nσ2\ni\n,\nwhere T is the number of coordinate of θ. Let us consider the KL divergence term KL(Q∥P) as a function of\nσ2\nP , then its derivative with respect to σ2\nP is equal to\nT 1\nσ2\nP\n−T σ2 + ∥θ∥2\nσ4\nP\n= T\nσ2\nP\nh\n1 −σ2 + ∥θ∥2/T\nσ2\nP\ni\n,\nwhich is equal to zero when σ2\nP = σ2 + ∥θ∥2/T . Thus\nKL(Q∥P) ≥T\n2 log\n\u00001 + ∥θ∥2\nT σ2\n\u0001\n.\nTogether with (33), we get\n1\nβ\nh\nKL(Q∥P) + log 1\nδ + β2L2\n8N\ni\n≥\nL\n√\n2N\nr\nT\n2 log\n\u00001 + ∥θ∥2\nT σ2\n\u0001\n≥L.\nwhen ∥θ∥2 ≥T σ2h\nexp 4N\nT −1\ni\n. Since the loss function ℓis bounded by L, if ∥θ∥2 ≥T σ2h\nexp 4N\nT −1\ni\n,\nthen the RHS of (32) is already greater than L. Therefore, we only need to consider the case that ∥θ∥2 ≤\nT σ2h\nexp 4N\nT −1\ni\n.\n21\nWe need to specify P in advance, since it is a prior distribution. However, we do not know in advance the value\nof θ that affect the KL divergence term. Hence, we build a family of distribution P as follows:\nP =\nn\nPj = N(0, σ2\nPjIT ) : σ2\nPj = c exp\n\u00001 −j\nT\n\u0001\n, c = σ2\u00001 + exp 4N\nT\n\u0001\n, j = 1, 2, . . .\no\nSet δj =\n6δ\nπ2j2 , the below inequality holds with probability at least 1 −δj\nLun\nDun(θ, Q) ≤Lun\nS (θ, Q) + 1\nβ\nh\nKL(Q∥Pj) + log 1\nδj + β2L2\n8N\ni\n.\nThus, with probability 1 −δ the above inequalities hold for all Pj. We choose\nj∗=\n$\n1 + T log\n \nσ2\u00001 + exp{4N/T }\n\u0001\nσ2 + ∥θ∥2/T\n!%\n.\nSince ∥θ∥2\nT\n≤σ2\u0002\nexp 4N\nT −1\n\u0003\n, we get σ2 + ∥θ∥2\nT\n≤σ2 exp 4N\nT , thus j∗is well-deﬁned. We also have\nT log\nc\nσ2 + ∥θ∥2/T ≤j∗≤1 + T log\nc\nσ2 + ∥θ∥2/T\n⇒\nlog\nc\nσ2 + ∥θ∥2/T ≤j∗\nT ≤1\nT + log\nc\nσ2 + ∥θ∥2/T\n⇒\n−1\nT + log σ2 + ∥θ∥2/T\nc\n≤−j∗\nT\n≤log σ2 + ∥θ∥2/T\nc\n⇒\ne−1/T σ2 + ∥θ∥2/T\nc\n≤e−j∗/T ≤σ2 + ∥θ∥2/T\nc\n⇒\nσ2 + ∥θ∥2\nT\n≤ce\n1−j∗\nT\n≤e\n1\nT\n\u0010\nσ2 + ∥θ∥2\nT\n\u0011\n⇒\nσ2 + ∥θ∥2\nT\n≤σ2\nPj∗≤e\n1\nT\n\u0010\nσ2 + ∥θ∥2\nT\n\u0011\n.\nHence, we have\nKL(Q∥Pj∗) = 1\n2\nhT σ2 + ∥θ∥2\nσ2\nPj∗\n−T + T log σ2\nPj∗\nσ2\ni\n≤1\n2\nh T σ2 + ∥θ∥2\nσ2 + ∥θ∥2/T −T + T log e1/T \u0000σ2 + ∥θ∥2/T\n\u0001\nσ2\ni\n≤1\n2\nh\n1 + T log\n\u00001 + ∥θ∥2\nT σ2\n\u0001i\n.\nFor the term log\n1\nδj∗, use the inequality log(1 + et) ≤1 + t for t > 0,\nlog 1\nδj∗= log (j∗)2π2\n6δ\n= log 1\nδ + log\n\u0010π2\n6\n\u0011\n+ 2 log(j∗)\n≤log 1\nδ + log π2\n6 + 2 log\n\u0010\n1 + T log σ2\u00001 + exp(4N/T )\n\u0001\nσ2 + ∥θ∥2/T\n\u0011\n≤log 1\nδ + log π2\n6 + 2 log\n\u0010\n1 + T log\n\u00001 + exp(4N/T )\n\u0001\u0011\n≤log 1\nδ + log π2\n6 + 2 log\n\u0010\n1 + T\n\u00001 + 4N\nT\n\u0001\u0011\n≤log 1\nδ + log π2\n6 + log(1 + T + 4N).\nChoosing β =\n√\nN, with probability at least 1 −δ we get\n1\nβ\nh\nKL(Q∥Pj∗) + log 1\nδj∗+ β2L2\n8N\ni\n≤\n1\n√\nN\nh1\n2 + T\n2 log\n\u0010\n1 + ∥θ∥2\nT σ2\n\u0011\n+ log 1\nδ + 6 log(N + T )\ni\n+\nL2\n8\n√\nN\nSince ∥θ′ −θ∥2 is T chi-square distribution, for any positive t, we have\nP\n\u0000∥θ′ −θ∥2 −T σ2 ≥2σ2√\nT t + 2tσ2\u0001\u0001\n≤exp(−t).\nBy choosing t = 1\n2 log(N), with probability 1 −N −1/2, we have\n∥θ′ −θ∥2 ≤σ2 log(N) + T σ2 + σ2p\n2T log(N) ≤T σ2\u0010\n1 +\nr\nlog(N)\nT\n\u00112\n.\n22\nBy setting σ = ρ ×\n\u0000√\nT +\np\nlog(N)\n\u0001−1, we have ∥θ′ −θ∥2 ≤ρ2. Hence, we get\nLS\n\u0010\nθ′, N(θ, σ2IT )\n\u0011\n= Eθ′∼N (θ,σ2IT )ES\n\u0002\nfθ′\u0003\n=\nZ\n∥θ′−θ∥≤ρ\nES\n\u0002\nfθ′\u0003\ndN(θ, σ2IT ) +\nZ\n∥θ′−θ∥>ρ\nES\n\u0002\nfθ′\u0003\ndN(θ, σ2IT )\n≤\n\u0010\n1 −\n1\n√\nN\n\u0011\nmax\n∥θ′−θ∥≤ρ LS(θ′) +\n1\n√\nN\nL\n≤\nmax\n∥θ′−θ∥2≤ρ LS(θ′) + 2L\n√\nN\n.\nTogether,\nLun\nDun(θ, ppos) ≤\nmax\nθ′:∥θ′−θ∥<ρ Lun\nS (θ, ppos) +\n1\n√\nN\n\"\nT\n2 log\n\u0010\n1 + ∥θ∥2\nT σ2\n\u0011\n+ log 1\nδ + O\n\u0010\nlog(N + T )\n\u0011\n+ L2\n8 + 2L\n#\n.\nWe note that the proof in [18] invoked the McAllester PAC-Bayesian generalization bound [51], hence only\napplicable to the 0-1 loss in the binary classiﬁcation setting. Ours is the ﬁrst work that proposes and devises\nsharpness-aware theory for feature extractor in the context of SSL. Additionally, the proof of our theory employs\nthe PAC-Bayesian generalization bound [52] to deal with the more general InfoNCE loss. Eventually, by\nleveraging Theorems B.2 and B.3, we reach the following theorem.\nTheorem B.4. Under mild conditions, with the probability at least 1 −δ over the random choice of S ∼DN\nun,\nwe have the following inequality\nLadv\nDsup (θ, a) ≤\nh\nmax\n∥θ′−θ∥≤ρ Lun\nS\n\u0000θ′, ppos\n\u0001\n+B(T, N, δ, ∥θ∥)+A (K, β)\ni 1\n2 h\n1+max\na∈A Dv (qa, qdata)\n1\n2\ni\n, (34)\nwhere we have deﬁned\nB(T, N, δ, ∥θ∥) =\n1\n√\nN\nhT\n2 log\n\u0010\n1 + ∥θ∥2\nT σ2\n\u0011\n+ log 1\nδ + L2\n8 + 2L + O (log (N + T ))\ni\n.\nProof. This is a direct consequence of Theorems B.2 and B.3.\nRemark B.5. Inequality (34) shows that minimizing the adversarial loss Ladv\nDsup (θ, a) with respect to θ requires\nminimizing the sharpness-aware unsupervised InfoNCE loss:\nmax∥θ′−θ∥≤ρ Lun\nS (θ) and the divergence:\nmaxa∈A Dv (qa, qdata).\nWhile the sharpness-aware unsupervised InfoNCE loss is straightforward, the\ndivergence term maxa∈A Dv (qa, qdata) requires further derivations, which will be clariﬁed in the next section.\nWe now develop another upper bound for the adversarial loss Ladv\nDsup (θ, a). Given a stochastic map a ∈A, we\ndeﬁne the hybrid positive pa\npos(x, x+) distribution induced by a as pa\npos(x, x+) = PM\nc=1 πcpc (x) pa\nc\n\u0000x+\u0001\n,\nwhere pa\nc = a#pc.\nAdditionally, by its deﬁnition, the hybrid positive distribution pa\npos(x, x+) consists of the positive pairs of a\nbenign example x with the label c and an adversarial example x+ formed by applying the stochastic map a to\nan another benign example x′. The following theorem gives us another upper bound on the the adversarial loss\nLadv\nDsup (θ, a).\nTheorem B.6. The adversarial loss Ladv\nDsup(θ, a) can be further upper-bounded by\nLadv\nDsup (θ, a) ≤\n\u0014\nmax\na∈A\n\u001a\nLun\nDun\n\u0000θ, pa\npos\n\u0001\n+\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\nDu (qa, qdata)\n\u001b\n+ A (K, β)\n\u00151/2\n,\n(35)\nwhere A(K, β) = −O\n\u00001\n√\nK\n\u0001\n−log β −O\n\u0000 1\nβ\n\u0001\n, Du is a f-divergence with the convex function u(t) = |t −1|,\nand Lun\nDun\n\u0000θ, pa\npos\n\u0001\nis similar to Lun\nDun (θ, ppos) in Eq. (14) except that the positive pairs (x, x+) ∼pa\npos.\n23\nProof. We start with\nLadv\nDsup (W, θ, a) = E(x,y)∼Dsup\nh\nτCE (W fθ (aW,θ (x)) , y)1/2i\n=\nZ\nτCE (W fθ (aW,θ (x)) , y)1/2 pdata (x, y) dxdy\n=\nZ\nτCE (W fθ (aW,θ (x)) , y)1/2 pdata (x, y)1/2 pdata (x, y)1/2 dxdy\n(1)\n≤\n\u0014Z\nτCE (W fθ (aW,θ (x)) , y) pdata (x, y) dxdy\n\u00151/2 \u0014Z\npdata (x, y) dxdy\n\u00151/2\n=\n\u0014Z\nτCE (W fθ (aW,θ (x)) , y) pdata (x, y) dxdy\n\u00151/2\n.\n(36)\nNote that in\n(1)\n≤, we use the Cauchy-Schwarz inequality.\nLet us denote ˜Ladv\nDsup (W, θ, a) =\nR\nτCE (W fθ (aW,θ (x)) , y) pdata (x, y) dxdy.\nWe also denote\n¯\nWc\n=\nEx∼p\naW,θ\nc\n[fθ (x)] and ¯\nW =\n\u0002 ¯\nWc\n\u0003\nc. We then have\nLadv\nDsup (θ, a) = min\nW Ladv\nDsup (W, θ, a) ≤Ladv\nDsup\n\u0000 ¯\nW, θ, a\n\u0001\n≤˜Ladv\nDsup\n\u0000 ¯\nW, θ, a\n\u00011/2 .\nWe manipulate ˜Ladv\nDsup\n\u0000 ¯\nW , θ, a\n\u0001\nas\n˜Ladv\nDsup\n\u0000 ¯\nW, θ, a\n\u0001\n= E(x,y)∼Dsup\n\u0002\nτCE\n\u0000 ¯\nW.fθ (aW,θ (x)) , y\n\u0001\u0003\n=\nM\nX\nc=1\nπcEx∼pc\n\u0002\nτCE\n\u0000 ¯\nW.fθ (aW,θ (x)) , c\n\u0001\u0003\n= −\nM\nX\nc=1\nπcEx∼pc\n\"\nlog\nexp\n\b ¯\nWc.fθ (aW,θ (x)) /τ\n\t\nPM\ny=1 exp\n\b ¯\nWy.fθ (aW,θ (x)) /τ\n\t\n#\n= −\nM\nX\nc=1\nπcEx∼pc\n\"\n¯\nWc.fθ (aW,θ (x))\nτ\n−log\n M\nX\ny=1\nexp\n\u001a ¯\nWy.fθ (aW,θ (x))\nτ\n\u001b!#\n= −\nM\nX\nc=1\nπc\nτ\n¯\nWc.Ex∼p\naW,θ\nc\n[fθ (x)] +\nM\nX\nc=1\nπcEx∼p\naW,θ\nc\n\"\nlog\n M\nX\ny=1\nexp\n\u001a ¯\nWy.fθ (x)\nτ\n\u001b!#\n= −\nM\nX\nc=1\nπc\nτ ∥¯\nWc∥2 +\nM\nX\nc=1\nπcEx∼p\naW,θ\nc\n\"\nlog\n M\nX\ny=1\nexp\n\u001a ¯\nWy.fθ (x)\nτ\n\u001b!#\n.\n(37)\nDeﬁne ˜p\naW,θ\npos\n\u0000x, x+\u0001\n= P\nc πcp\naW,θ\nc\n(x) p\naW,θ\nc\n\u0000x+\u0001\nwith p\naW,θ\nc\n= aW,θ#pc and p\naW,θ\ndata\n= aW,θ#pdata =\nP\nc πcp\naW,θ\nc\n, we then have\n¯Lun\nDun\n\u0000θ, ˜p\naW,θ\npos\n\u0001\n= E(x,x+)∼˜p\naW,θ\npos\n\u0014\n−1\nτ fθ (x) .fθ\n\u0000x+\u0001\u0015\n+ Ex∼p\naW,θ\ndata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)##\n=E(x,x+)∼˜p\naW,θ\npos\n\u0014\n−1\nτ fθ (x) .fθ\n\u0000x+\u0001\u0015\n+ Ex∼p\naW,θ\ndata\n\"\nlog\n M\nX\nc=1\nπcEx−∼pc\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)#!#\n≥E(x,x+)∼˜p\naW,θ\npos\n\u0014\n−1\nτ fθ (x) .fθ\n\u0000x+\u0001\u0015\n+ Ex∼p\naW,θ\ndata\n\"\nlog\n M\nX\nc=1\nπc exp\n(\nfθ (x) .Ex−∼pc\n\u0002\nfθ\n\u0000x−\u0001\u0003\nτ\n)!#\n= −1\nτ\nM\nX\nc=1\nπcEx,x+∼p\naW,θ\nc\n\u0002\nfθ (x) .fθ\n\u0000x+\u0001\u0003\n+ Ex∼p\naW,θ\ndata\n\"\nlog\n M\nX\nc=1\nπc exp\n\u001aWc.fθ (x)\nτ\n\u001b!#\n= −1\nτ\nM\nX\nc=1\nπcEx∼p\naW,θ\nc\n[fθ (x)] .Ex+∼p\naW,θ\nc\n\u0002\nfθ\n\u0000x+\u0001\u0003\n+ Ex∼p\naW,θ\ndata\n\"\nlog\n M\nX\nc=1\nπc exp\n\u001aWc.fθ (x)\nτ\n\u001b!#\n= −\nM\nX\nc=1\nπc\nτ ∥¯\nWc∥2 +\nM\nX\nc=1\nπcEx∼p\naW,θ\nc\n\"\nlog\n M\nX\ny=1\nexp\n\u001a ¯\nWy.fθ (x)\nτ\n\u001b!#\n.\n(38)\nTherefore, from (37) and (38), we reach\n˜Ladv\nDsup\n\u0000 ¯\nW, θ, a\n\u0001\n= ¯Lun\nDun\n\u0000θ, ˜p\naW,θ\npos\n\u0001\n.\n24\nWith p\naW,θ\npos\n\u0000x, x+\u0001\n= P\nc πcpc (x) p\naW,θ\nc\n\u0000x+\u0001\n, we deﬁne\n¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n= E(x,x+)∼p\naW,θ\npos\n\u0014\n−1\nτ fθ (x) .fθ\n\u0000x+\u0001\u0015\n+Ex∼pdata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)##\n.\nWe then have\n¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n−¯Lun\nDun\n\u0000θ, ˜p\naW,θ\npos\n\u0001\n= A + B,\nwhere we have deﬁned\nA = 1\nτ\nX\nc\nπcEx+∼p\naW,θ\nc\n\u0002\nfθ\n\u0000x+\u0001\u0003\n·\n\u0010\nEx∼pc [fθ (x)] −Ex∼p\naW,θ\nc\n[fθ (x)]\n\u0011\n= 1\nτ\nX\nc\nπcEz∼q\naW,θ\nc\n[z] ·\n\u0010\nEz∼qc [z] −Ex∼q\naW,θ\nc\n[z]\n\u0011\nB = Ex∼pdata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)##\n−Ex∼p\naW,θ\ndata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)##\n= Ez∼qdata\n\u0014\nlog Ez−∼qdata\n\u0014\nexp\n\u001az.z−\nτ\n\u001b\u0015\u0015\n−Ez∼qaW,θ\n\u0014\nlog Ez−∼qdata\n\u0014\nexp\n\u001az.z−\nτ\n\u001b\u0015\u0015\nIt appears that\n|A| ≤1\nτ\nX\nc\nπc∥Ez∼q\naW,θ\nc\n[z] ∥∥Ez∼q\naW,θ\nc\n[z] −Ex∼qc [z] ∥\n= 1\nτ\nX\nc\nπc∥Ez∼q\naW,θ\nc\n[z] ∥∥Ez∼q\naW,θ\nc\n[z] −Ex∼qc [z] ∥\n≤1\nτ\nX\nc\nπcEz∼q\naW,θ\nc\n[∥z∥] ∥Ez∼q\naW,θ\nc\n[z] −Ex∼qc [z] ∥(thanks to ∥z∥= 1)\n≤1\nτ\nX\nc\nπc∥Ez∼q\naW,θ\nc\n[z] −Ex∼qc [z] ∥\n= 1\nτ\nX\nc\nπc∥\nZ\nz\n\u0000q\naW,θ\nc\n(z) −qc (z)\n\u0001\n∥dz\n≤1\nτ\nX\nc\nπc\nZ\n∥z∥\n\f\fq\naW,θ\nc\n(z) −qc (z)\n\f\f dz\n(thanks to ∥z∥= 1)\n= 1\nτ\n X\nc\nπc\nZ \f\f\f\f\nq\naW,θ\nc\n(z)\nqc (z)\n−1\n\f\f\f\f qc (z) dz\n!\n= 1\nτ Du (qaW,θ, qdata) ,\nwhere we deﬁne Du (qaW,θ, qdata) = P\nc πcDu\n\u0000q\naW,θ\nc\n, qc\n\u0001\n.\n|B| =\n\f\f\f\f\nZ\n(qdata (z) −qaW,θ (z)) log Ez−∼qdata\n\u0014\nexp\n\u001az.z−\nτ\n\u001b\u0015\ndz\n\f\f\f\f\n≤\nZ\n|qdata (z) −qaW,θ (z)| exp\n\u001a 1\nτ\n\u001b\ndz\n≤exp\n\u001a 1\nτ\n\u001b Z \f\f\f\f\nqaW,θ (z)\nqdata (z) −1\n\f\f\f\f qdata (z) dz ≤exp\n\u001a 1\nτ\n\u001b X\nc\nπc\nZ \f\f\f\f\nq\naW,θ\nc\n(z)\nqc (z)\n−1\n\f\f\f\f qc (z) dz\n= exp\n\u001a 1\nτ\n\u001b\nDu (qaW,θ, qdata) .\nTherefore, we have\n˜Ladv\nDsup\n\u0000 ¯\nW, θ, a\n\u0001\n= ¯Lun\nDun\n\u0000θ, ˜p\naW,θ\npos\n\u0001\n≤¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n+ Du (qaW,θ, qdata)\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\n.\nLadv\nDsup (θ, a) ≤˜Ladv\nDsup\n\u0000 ¯\nW , θ, a\n\u00011/2 ≤\n\u0014\n¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n+ Du (qaW,θ, qdata)\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\u00151/2\n≤\n\u0014\n¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n+ max\na∈A Du (qa, qdata)\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\u00151/2\n25\nWe now handle\n¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n= E(x,x+)∼p\naW,θ\npos\n\u0014\n−1\nτ fθ (x) .fθ\n\u0000x+\u0001\u0015\n+Ex∼pdata\n\"\nlog Ex−∼pdata\n\"\nexp\n(\nfθ (x) .fθ\n\u0000x−\u0001\nτ\n)##\n.\nUsing the same derivations as for ¯Lun\nDun (θ, ppos) as in Theorem B.2, we obtain\n¯Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n≤Lun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n+ A (K, β) .\nFinally, we reach\nLadv\nDsup (θ, a) ≤\n\u0014\nLun\nDun\n\u0000θ, p\naW,θ\npos\n\u0001\n+ Du (qaW,θ, qdata)\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\n+ A (K, β)\n\u00151/2\n≤\n\u0014\nmax\na∈A\n\u001a\nLun\nDun\n\u0000θ, pa\npos\n\u0001\n+\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\nDu (qa, qdata)\n\u001b\n+ A (K, β)\n\u00151/2\n.\nRemark B.7. Theorem B.6 indicates that we need to ﬁnd the adversary a that maximizes the InfoNCE loss\nover the hybrid adversarial positive distribution pa\npos and the divergence on the latent space Du(qa, qdata). This\nallows us to create strong adversarial examples that cause high InfoNCE values (i.e., za = fθ(xa) is locally far\nfrom z = fθ(x)), while the distribution of adversarial examples is globally far from the that of benign examples\non the latent space. Eventually, the feature extractor fθ is updated to minimize both the local distances and the\nglobal divergence.\nAdditionally, the adversarial InfoNCE term Lun\nDun\n\u0000θ, pa\npos\n\u0001\nhas the form of\nLun\nDun\n\u0000fθ, pa\npos\n\u0001\n= E(x,x+)∼papos,x−\n1:K\niid\n∼pdata\n\"\n−log\nexp\n\b fθ(x).fθ(x+)\nτ\n\t\nexp\n\b fθ(x).fθ(a(x+))\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x).fθ(x−\nk )\nτ\n\t\n#\n= E(x,x+)∼ppos,x−\n1:K\niid\n∼pdata\n\"\n−log\nexp\n\b fθ(x).fθ(a(x+))\nτ\n\t\nexp\n\b fθ(x).fθ(x+)\nτ\n\t\n+ β\nK\nPK\nk=1 exp\n\b fθ(x).fθ(x−\nk )\nτ\n\t\n#\n,\n(39)\nwhich means that given the positive pair (x, x+) having the same label, we ﬁnd the adversarial example a(x+)\nfor x+ that maximizes the InfoNCE loss as in Eq. (39). Note that according to Eq. (35), we actually need\nto seek adversarial examples a(x+) to globally maximize a f-divergence between the benign and adversarial\ndistributions.\nLeveraging the upper bounds in Theorems B.4 and B.6, we arrive the ﬁnal upper bound stated in the following\ntheorem.\nTheorem B.8. For any 0 ≤λ ≤1, the adversarial loss Ladv\nDsup(θ, a) can be further upper-bounded by\nLadv\nDsup (θ, a) ≤λ\n\u0014\nmax\na∈A\n\u001a\nLun\nDun\n\u0000θ, pa\npos\n\u0001\n+\n\u0012\nexp\n\u001a 1\nτ\n\u001b\n+ 1\nτ\n\u0013\nDu (qa, qdata)\n\u001b\n+ A (K, β)\n\u00151/2\n+ (1 −λ)\nh\nmax\n∥θ′−θ∥≤ρ Lun\nS\n\u0000θ′, ppos\n\u0001\n+ B(T, N, δ, ∥θ∥) + A (K, β)\ni 1\n2 h\n1 + max\na∈A Dv (qa, qdata)\n1\n2\ni\n.\n(40)\nProof. This is a direct consequence of Theorems B.4 and B.6.\nC\nAlgorithm\nWe present the algorithm for our Global-Local Adversarial Self-Supervised Learning in Algorithm 1. Brieﬂy, it\ncontains four main steps: First, two augmentation views of the original mini-batch of clean images are created.\nSecond, adversarial images corresponding to one of the two views are generated. This step realizes the local and\nglobal views when generating adversaries: maximize the InfoNCE loss on the adversarial and the other view of\nbenign examples; and minimize the discriminator loss, i.e. Binary Cross Entropy loss, on the adversaries. The\ndiscriminator loss minimization makes the model know that the adversaries are fake data, thus further forces\nthem to be distinguished from their clean counterparts in the latent space. Third, freeze the parameters of the\nmain network and update those of the discriminator such that the model can distinguish the adversaries and\nthe clean samples, i.e. minimize two corresponding BCE losses. Finally, freeze the discriminator’s parameters,\nupdate those of the main network such that the InfoNCE loss is minimized and the discriminator losses are\nmaximized. This step ensures that the learnt main model can capture the invariance between adversarial and\nclean samples, thus becomes more robust against adversarial pertubations.\n26\nAlgorithm 1: Adversarial Self-Supervised Learning with Global and Local information\nInput :Mini-batch of clean images x; Pool of augmentations T ;\nMain network fθ; Discriminator Dφ;\nLearning rate for main network η, for discriminator ηd;\nAttacker a; ǫ-Ball Bǫ(.).\nResult :θ, φ.\n1. Sample two augmentations τi, τj ∼T\nGet two augmentation views xi = τi(x), xj = τj(x).\n2. Generate the adversarial images xa\nj\nxa\nj = argmaxxa\nj ∈Bǫ(xj)[InfoNCE(fθ(xi), fθ(xa\nj )) −λglobalBCE(Dφ(fθ(xa\nj )), 0)].\n3. Update discriminator’s parameters φ by minimizing\n1\n2[BCE(Dφ(fθ(xj)), 1) + BCE(Dφ(fθ(xa\nj )), 0)].\n4. Update main network’s parameters θ by minimizing\nλbenignInfoNCE(fθ(xi), fθ(xj)) + InfoNCE(fθ(xi), fθ(xa\nj ))\n−λglobal\n2\n[BCE(Dφ(fθ(xa\nj )), 0) + BCE(Dφ(fθ(xj)), 1)],\nwith SAM optimization applied on the ﬁrst term.\nD\nAdditional Experiments\nThis section will include details for the experimental settings,\nimplementation and results in the\nmain paper,\nalong with additional experiments on Cifar-100 dataset.\nOur code is available at:\nhttps://anonymous.4open.science/r/rosa-D175\nD.1\nImplementation Details\nAs mentioned in the main paper, to take into account the global divergence between adversarial examples and\ntheir benign counterparts, we use generative adversarial network, GAN [53]. Speciﬁcally, we additionally use\na discriminator which takes the output of the projection head as input, and returns the probability that the\ngiven sample is clean. We thus choose an one-layer perceptron 128 × 1 for its architecture. Moreover, the\ndiscriminator’s objective is to quantify the global divergence between adversaries and clean examples, thus we\nchoose its loss function to be the Binary Cross Entropy loss. The labels for benign and adversarial examples\nare 1 and 0 respectively, where 1 (0) is a vector with all the elements are 1 (0).\nWhen applying SAM to the benign term loss, our optimization procedure is as follows: First, compute the\ngradient of other terms in the total loss with respect to the current model parameters and save it as gother.\nSecond, zero out current gradient then follow the procedure of SAM to compute the SAM gradient of only the\nbenign term, gbenign. Then average gother and gbenign to get the ﬁnal gradient update.\nD.2\nExperimental Settings\nGeneral Settings:\nCifar-100 consists of 50000 training images and 10000 test images over 100 classes. All\nthe images are in the shape of 3 × 32 × 32. We apply on this dataset the same list of augmentations as for\nCifar-10 [2], except the pixel values are are scaled to the range [0, 1] without normalization. We also use the\nsame backbone, projection head, classiﬁcation head and discriminator as for Cifar-10 to run on Cifar-100.\nSSL Settings:\nFor two datasets, the default training conﬁguration of the discriminator is as follows: The\noptimizer is plain SGD with the learning rate of 0.5, λglobal is 0.5. For sharpness-aware minimization, we\nadopt A-SAM [57] with ρ = 1.0.\nD.3\nExperimental Results\nIn this part, for the ease of readability, we repeat the results on Cifar-10 in Table 5, and present additional results\non Cifar-100 in Table 4.\n27\nFrom Table 4, ﬁrst, consider settings without D and without SAM, it can be seen that as we increase λbenign,\nclean and robust accuracy under LE and AT-LE both increase. Furthermore, when the benign term loss is\nincorporated with SAM, i.e. settings with SAM and without D, the model performance can be further improved.\nSecond, consider settings without SAM, with the introduction of the discriminator, model achieves better robust\naccuracy in all settings except the one under AT-LE with λbenign = 2.0 in which comparable value is obtained.\nInterestingly, clean accuracy is also improved consistently over all settings.\nFinally, when combining SAM and D, full method, we can generally improve clean and robust accuracy. In\nparticular, when λbenign = 1.0, our full method obtains the highest scores; when λbenign = 0.5, under AT-\nLE, it outperforms other variants; and when λbenign = 2.0, it obtains the highest robust performance and\ncomparable clean accuracy to its variants.\nTable 4: Clean and robust accuracy of a ResNet-18 model trained with and without our method’s components,\nacross different weighting for the benign loss term. Dataset: Cifar-100.\nλbenign\nSAM\nD\nLE\nAT-LE\nClean\nAdv.\nClean\nAdv.\n0\n✗\n✗\n26.23\n13.86\n23.47\n14.95\n✗\n✓\n27.45\n14.44\n24.94\n15.65\n0.5\n✗\n✗\n31.70\n15.06\n27.53\n16.25\n✗\n✓\n32.19\n15.44\n27.88\n16.40\n✓\n✗\n31.79\n15.14\n27.67\n16.28\n✓\n✓\n32.94\n15.31\n28.76\n16.79\n1.0\n✗\n✗\n36.35\n16.11\n31.22\n17.32\n✗\n✓\n36.81\n16.63\n31.77\n17.78\n✓\n✗\n36.58\n16.37\n31.32\n17.79\n✓\n✓\n37.72\n17.02\n32.34\n17.99\n2.0\n✗\n✗\n43.36\n17.06\n35.65\n18.88\n✗\n✓\n43.86\n17.43\n35.90\n18.83\n✓\n✗\n43.58\n17.29\n35.94\n18.94\n✓\n✓\n43.26\n17.62\n35.72\n18.98\nTable 5: Clean and robust accuracy of a ResNet-18 model trained with and without our method’s components,\nacross different weighting for the benign loss term. Dataset: Cifar-10.\nλbenign\nSAM\nD\nLE\nAT-LE\nClean\nAdv.\nClean\nAdv.\n0\n✗\n✗\n70.54\n38.97\n66.28\n42.73\n✗\n✓\n71.04\n39.08\n66.15\n42.57\n0.5\n✗\n✗\n77.54\n40.72\n72.84\n44.23\n✗\n✓\n77.66\n40.83\n73.32\n44.46\n✓\n✗\n77.83\n40.09\n73.33\n44.23\n✓\n✓\n77.76\n40.05\n73.35\n44.17\n1.0\n✗\n✗\n81.14\n41.08\n76.36\n44.69\n✗\n✓\n81.02\n41.34\n76.95\n44.90\n✓\n✗\n81.31\n41.14\n76.98\n44.95\n✓\n✓\n81.64\n41.23\n77.33\n45.28\n2.0\n✗\n✗\n84.80\n41.78\n81.35\n45.71\n✗\n✓\n84.12\n41.93\n80.92\n45.89\n✓\n✗\n85.09\n41.97\n81.64\n46.30\n✓\n✓\n85.01\n41.82\n81.86\n46.45\n28\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2023-11-16",
  "updated": "2023-11-16"
}