{
  "id": "http://arxiv.org/abs/2311.10431v1",
  "title": "Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human Narrative Processing",
  "authors": [
    "Zhengqi He",
    "Taro Toyoizumi"
  ],
  "abstract": "Understanding how humans process natural language has long been a vital\nresearch direction. The field of natural language processing (NLP) has recently\nexperienced a surge in the development of powerful language models. These\nmodels have proven to be invaluable tools for studying another complex system\nknown to process human language: the brain. Previous studies have demonstrated\nthat the features of language models can be mapped to fMRI brain activity. This\nraises the question: is there a commonality between information processing in\nlanguage models and the human brain? To estimate information flow patterns in a\nlanguage model, we examined the causal relationships between different layers.\nDrawing inspiration from the workspace framework for consciousness, we\nhypothesized that features integrating more information would more accurately\npredict higher hierarchical brain activity. To validate this hypothesis, we\nclassified language model features into two categories based on causal network\nmeasures: 'low in-degree' and 'high in-degree'. We subsequently compared the\nbrain prediction accuracy maps for these two groups. Our results reveal that\nthe difference in prediction accuracy follows a hierarchical pattern,\nconsistent with the cortical hierarchy map revealed by activity time constants.\nThis finding suggests a parallel between how language models and the human\nbrain process linguistic information.",
  "text": "Causal Graph in Language Model Rediscovers Cortical Hierarchy in\nHuman Narrative Processing\nZhengqi He a ∗and Taro Toyoizumi a,b †\na Lab for Neural Computation and Adaptation, RIKEN Center for Brain Science, Japan\na,b Department of Mathematical Informatics, Graduate School of Information Science\nand Technology, The University of Tokyo, Japan\nAbstract\nUnderstanding how humans process natural lan-\nguage has long been a vital research direction.\nThe field of natural language processing (NLP)\nhas recently experienced a surge in the devel-\nopment of powerful language models. These\nmodels have proven to be invaluable tools for\nstudying another complex system known to pro-\ncess human language: the brain. Previous stud-\nies have demonstrated that the features of lan-\nguage models can be mapped to fMRI brain\nactivity. This raises the question: is there a\ncommonality between information processing\nin language models and the human brain? To es-\ntimate information flow patterns in a language\nmodel, we examined the causal relationships\nbetween different layers. Drawing inspiration\nfrom the workspace framework for conscious-\nness, we hypothesized that features integrating\nmore information would more accurately pre-\ndict higher hierarchical brain activity. To val-\nidate this hypothesis, we classified language\nmodel features into two categories based on\ncausal network measures: ’low in-degree’ and\n’high in-degree’. We subsequently compared\nthe brain prediction accuracy maps for these\ntwo groups. Our results reveal that the differ-\nence in prediction accuracy follows a hierar-\nchical pattern, consistent with the cortical hier-\narchy map revealed by activity time constants.\nThis finding suggests a parallel between how\nlanguage models and the human brain process\nlinguistic information.\n1\nIntroduction\nUnderstanding high-order cognitive functions of\nthe human brain, such as natural language pro-\ncessing, remains a pivotal challenge in neural sci-\nence (Hickok and Poeppel, 2007; Friederici, 2011;\nRalph et al., 2017). Modern neuroimaging tech-\nniques, like functional Magnetic Resonance Imag-\ning (fMRI), allow us to observe brain activity dur-\n∗zhengqi.he@riken.jp\n†taro.toyoizumi@riken.jp\ning language-related tasks directly. A prevailing\nhypothesis in this domain is the hierarchical pro-\ncessing hypothesis. A seminal study supporting\nthis hypothesis is presented in (Lerner et al., 2011).\nIn this study, the author investigates the effects of\nscrambling language elements at different hierar-\nchical levels—ranging from words to sentences to\nparagraphs. The findings reveal distributed net-\nworks of brain areas to accumulate language in-\nformation over time, emphasizing the hierarchical\nnature of language processing.\nIt’s noteworthy that the correlation between hi-\nerarchy and time constant appears to be a general\ncharacteristic, not exclusive to language (Hunten-\nburg et al., 2018; Raut et al., 2020). For instance,\nin studies by (Hasson et al., 2008; Honey et al.,\n2012), a hierarchical temporal receptive window\nwas identified in humans while watching movies,\nas observed through fMRI and ECoG recordings.\nSimilarly, (Murray et al., 2014) uncovered a hi-\nerarchy of intrinsic time scales in the cortex of\nmacaque monkeys, evidenced by spike train record-\nings. Collectively, this body of research suggests\na linkage between temporal properties and ranks\nwithin the cortical hierarchy. It’s hypothesized that\nbrain regions with a slower time constant typically\noccupy higher ranks in anatomically defined hier-\narchy (Felleman and Van Essen, 1991; Barbas and\nRempel-Clower, 1997; Markov et al., 2014).\nBesides, Deep Neural Networks, which draw\ninspiration from the brain’s computational princi-\nples, have achieved significant success in the do-\nmain of natural language processing. Recent trends\nhighlight the rise of large unsupervised language\nmodels (LMs), such as ELMo (Peters et al., 2018),\nGPT (Radford et al., 2018), and BERT (Devlin\net al., 2018). Subsequently, plenty of research has\ndelved into harnessing their potential through vari-\nous methods, including the pretraining-finetuning\nparadigm (Devlin et al., 2018), prompt-engineering\nparadigm (Brown et al., 2020), and the develop-\narXiv:2311.10431v1  [cs.CL]  17 Nov 2023\nment of chatbots (Ouyang et al., 2022; OpenAI,\n2023).\nHistorically, language studies using brain imag-\ning often relied on tightly controlled conditions\nthat, while simple, may not always mimic natu-\nral scenarios and might not be easily generaliz-\nable (Lerner et al., 2011). A compelling question\nthat arises is whether the computational mecha-\nnisms of deep neural networks and the brain can be\ncompared. A pioneering study by Daniel Yamins\n(Yamins et al., 2014) showed that deep neural net-\nworks, even when not specifically trained to emu-\nlate neural activity, exhibited patterns highly pre-\ndictive of brain activity in areas like the V4 and\ninferior temporal cortex when trained on object cat-\negorization tasks. This approach paved the way for\ncomparing neural networks to brain activities dur-\ning natural language processing. An early work\nis represented by (Huth et al., 2016).\nThe au-\nthors demonstrated that features derived from word\nembeddings could map onto cortical activity dur-\ning natural speech processing. In another study,\n(Schrimpf et al., 2021) compared brain imaging\ndata from individuals reading natural language ma-\nterials to various language models, spanning from\nbasic embeddings to complex neural networks. In-\nterestingly, models with superior language predic-\ntion capabilities also tended to predict brain activ-\nity better. With modern deep neural network-based\nlanguage models, the complicated dynamics of nat-\nural language can now be encoded and compared\ndirectly with brain data, which introduces exciting\navenues for novel discoveries.\nGiven that features in a language model (LM)\ncan map to whole-brain activity, a natural ques-\ntion arises: is there a fundamental similarity in\ninformation processing between an LM and the\nbrain, or is the correlation merely a superficial co-\nincidence (Antonello and Huth, 2023)? It’s well-\nestablished that the middle to late layers of a multi-\nlayer transformer-based LM often align best with\nbrain activity across both low and high hierarchical\nregions. However, the information in these hierar-\nchical brain areas possesses distinct properties. For\ninstance, according to the workspace framework\nfor consciousness (Dehaene and Naccache, 2001),\nhigher cortical brain regions typically integrate in-\nformation from a greater number of source areas\ncompared to lower hierarchical regions. Inspired\nby this observation, we hypothesize that, if LM and\nthe brain share similarity in information processing,\npart of the language features in the middle-late lay-\ners of LM that integrate from a more diverse range\nof source features, are more likely to predict activ-\nity in higher brain hierarchies, and vice versa. We\nsought to validate this hypothesis by approximating\nthe information flow in an LM using a causal graph.\nWe argue that features integrating from a broader\narray of source features will possess a higher in-\ndegree in such a causal graph. By grouping features\nbased on in-degree measurements and fitting brain\nactivity separately, we aimed to ascertain if these\nfeature groups corresponded with the cortical hi-\nerarchy, potentially inferred through activity time\nscales.\n2\nRelated Works\nOur research intersects with two principal areas of\nstudy.\nHierarchy in brain.\nA body of work high-\nlights the notion of an increasing time constant\nor temporal receptive field as a core organiz-\ning principle for the brain. For instance, (Mur-\nray et al., 2014) unveiled an ascending intrinsic\ntime scale within the cortical hierarchy, observed\nthrough auto-correlation measurements in the pri-\nmate cortex. Meanwhile, the study by (Chaud-\nhuri et al., 2015) developed a comprehensive dy-\nnamical model of the macaque neocortex using\na connectome dataset, shedding light on intrin-\nsic time scale hierarchies. In (Baldassano et al.,\n2017), the authors explored the alignment between\nevent structures featured with increasing time win-\ndows and cortical hierarchy, using human narrative\nperception datasets. Complementing this, (Chang\net al., 2022) identified a hierarchy in processing\ntimescales via response lag gradients that correlate\nwith known cortical hierarchies.\nLanguage model fitting brain: Another line\nof research underscores the potential of language\nmodels in predicting human brain activity. Build-\ning upon the findings of (Huth et al., 2016), which\nestablished that static word embeddings corre-\nlate with brain activity, subsequent studies demon-\nstrated that contextualized word representations\nsurpassed their static counterparts in terms of ac-\ncuracy in predicting brain activity, as indicated by\n(Jain and Huth, 2018). There has since been an in-\ncreasing trend of studies comparing language mod-\nels with brain activity datasets (Toneva and Wehbe,\n2019; Schrimpf et al., 2021; Goldstein et al., 2022b;\nKumar et al., 2022; Caucheteux and King, 2022;\nMillet et al., 2022). Concurrently, innovative strate-\ngies aimed at augmenting the alignment between\nlanguage models and brain recordings have been\nproposed (Schwartz et al., 2019; Aw and Toneva,\n2022; Antonello et al., 2023). Comprehensive re-\nviews and summaries of these studies are articu-\nlated in works such as (Abdou, 2022; Arana et al.,\n2023; Jain et al., 2023). Notably, the concept of hi-\nerarchy is recurrently discussed within this domain.\nFor instance, (Jain et al., 2020) introduced a multi-\ntimescale LSTM, capturing the temporal hierarchy\nobserved in natural speech fMRI datasets, while\n(Caucheteux et al., 2023) explored the relationship\nbetween cortical hierarchy and enhancements in\nbrain activity predictions across varied predictive\ntime windows.\n3\nMethods\n3.1\nfMRI dataset\nWe have selected the \"Narratives\" fMRI dataset as\nour primary dataset (Nastase et al., 2021). This\ndataset offers an extensive collection of fMRI\nrecordings representing human brain activity as par-\nticipants passively engage with naturalistic spoken\nnarratives. It includes data from 345 participants\nwho listened to a total of 27 distinct stories. In total,\nthe dataset spans 1.3 million words, 370K repeti-\ntion times (TRs), and 6.4 days of accumulated data\nacross all participants. Each recording follows a\nconsistent repetition time of 1.5 seconds. Prepro-\ncessing ensures that all fMRI data are smoothed,\nsurfaced, and uniformly aligned to a shared space\nknown as \"fsaverage,\" which serves as the foun-\ndation for our subsequent analysis. Additionally,\nevery story comes with a timestamped transcript,\nenabling us to process through language models,\nobtain contextualized word features, and synchro-\nnize them with the corresponding fMRI data.\n3.2\nMapping language features onto brain\nIn aligning language models with brain data, we\nadopted methodologies similar to those detailed in\n(Toneva and Wehbe, 2019; Schrimpf et al., 2021;\nJain et al., 2020; Caucheteux et al., 2023). Given\nthat the \"Narratives\" dataset captures brain activity\nfrom multiple participants exposed to natural lan-\nguage stimuli, we introduce the same stimuli into\na pre-trained language model and extract encoded\nrepresentations from multiple layers. We mainly\nuse the OPT-125m (Zhang et al., 2022), a publicly\navailable auto-regressive language model built on\ntransformer architecture. The \"Narratives\" dataset\nhas a resolution of 1.5 seconds per TR, while the\nfeatures we extract from the language model are\nper token. To establish a meaningful comparison,\nwe need to align the two datasets properly. Utiliz-\ning the timestamp of each token, we correlate it to a\nspecific TR and then average the extracted features\nfor a more comprehensive analysis.\nAssuming we’ve reached this step, we have a\ntime series of high-dimensional language model\nfeatures represented as Xt with shape (T, d). Here,\nT denotes the number of time steps, and d repre-\nsents the number of dimensions for the language\nmodel features. This feature series has been aligned\nwith our fMRI data Wt with shape (T, l), where l\nstands for the number of voxels. Our subsequent\ntask is to benchmark Xt and Wt using ridge re-\ngression. Given the high dimensionality of Xt (for\ninstance, OPT-125m can have a dimension d as\nlarge as 768), we employ PCA to reduce the dimen-\nsion of the representation vector for computational\nefficiency. In our implementation, we’ve reduced\nthe dimension to 20, following (Caucheteux et al.,\n2023), balancing both prediction accuracy and com-\nputational speed.\nWe predict the activity of each fMRI voxel us-\ning a linear projection of the representation vector\nfrom different layers. This linear projection is reg-\nularized using ridge loss. The process of ridge\nregression is described as follows. Assume we\nhave train and validation split of both language\nmodel features X →(Xµ, Xν) and fMRI dataset\nW →(Wµ, Wν), we first do ridge regression of\nthe train split. Ridge regression can be described\nas a minimization problem for each voxel i:\nargmin\nVi\n(W i\nµ −XµVi)T (W i\nµ −XµVi) + αiV T\ni Vi\n(1)\nwhere αi is a regularization factor, Vi is the fitting\nvector, W i\nµ is time series for voxel i. Then, the\nfitting vector is\nVi = (XT\nµ Xµ + αiI)−1XT\nµ W i\nµ\n(2)\nPrediction accuracy is quantified by the correlation\nof the predicted brain signal with the measured\nbrain signal on the validation split:\nP(X, W) = Corr(XνV, Wν)\n(3)\nwhere Corr is the Pearson correlation operator. To\nuse data efficiently, we perform multi-fold leave-\none-out cross-validation, and the average accuracy\namong all folds is reported. The regularization fac-\ntor is separately chosen from log-spaced between\n10−1 and 108 for each voxel via an extra nested\nleave-one-out cross-validation process.\nTo account for the slow bold response, we also\nuse the finite impulse response (FIR) model follow-\ning (Huth et al., 2016) by concatenating language\nrepresentation with delays from -9 to -3 TRs. The\nafni-smoothed version of the Narratives dataset is\nused in our study.\n3.3\nCausal graph in language model features\nOur analysis is based on pretrained multi-layer,\ntransformer-based, auto-regressive language mod-\nels, such as OPT. The architectural design of these\nmodels facilitates the flow of information from\nearly nodes to later ones and from the bottom layer\nto the top. Given that previous research has indi-\ncated that the middle-to-late layers of a language\nmodel align best with brain activity, our objective\nis to delve in detail into these findings. Specif-\nically, we aim to find out which features of the\nmodel correspond to which parts of the brain. In\nthis regard, we propose a causality measure. Using\nthis measure, we can categorize language model\nfeatures into ’low in-degree’ and ’high in-degree’\ngroups, which will be defined later, subsequently\nshowcasing their relationship with brain hierarchy.\nWe use random noise perturbation to estimate\ncausality. Consider a lower layer of interest, de-\nnoted by X = [x1, x2, ..., xT ], and a higher layer\nof interest, denoted by Y = [y1, y2, ..., yT ]. Due\nto the inherent network structure of our language\nmodel, there’s a general causal relationship such\nthat X →Y , implying Y = f(X). Introduc-\ning a random perturbation dX yields Y + dY =\nf(X + dX).\nAs mentioned earlier, both Y and X typically\nhave high dimensions. Prior to fitting to the brain,\nwe employ Principal Component Analysis (PCA)\nfor dimensionality reduction. Following this ap-\nproach, we further use PCA to reduce dimen-\nsions and then evaluate the causal relationship in\nthe PCA-reduced space. Let’s denote the trans-\nformed spaces as ¯X = PCA(X) = XMx and\n¯Y = PCA(Y ) = Y My. Utilizing the PCA pro-\njection matrices, perturbations and responses in\nthe PCA space are determined as d ¯X = dXMx\nand d ¯Y = dY My respectively. Subsequently, we\nobtain the causality matrix with time-shift τ as:\nCτ = d ¯Y T d ¯Xτ/(T −τ)\n(4)\nwhere Cτ is the causality matrix with time-shift τ,\nd ¯Xτ represents d ¯X with time-shift τ.\nTo construct a causal graph, we sum up the abso-\nlute value of the causality matrix for each τ. Then\nwe threshold the causality matrix by the median\nof the matrix elements; any value exceeding this\nthreshold is considered a valid causal link. Finally,\nwe obtain the causality matrix:\nC = td[\nX\nτ\nabs(Cτ)]\n(5)\nwhere td is the threshold operator, abs is the abso-\nlute operator.\n3.4\nMapping Causality onto Cortical\nHierarchy\nIn this section, we describe the rationale for using\na causal graph to mirror the brain’s hierarchy. As\nstated by the workspace framework of conscious-\nness (Dehaene and Naccache, 2001), higher corti-\ncal areas, believed to host consciousness, integrate\ninformation from a broader array of source regions\nin the brain. Drawing from this perspective, we hy-\npothesize that if a language model processes infor-\nmation analogously to the brain, its features would\nexhibit distinct patterns in terms of information\nintegration. Specifically, features mirroring high\ncortical regions should integrate more information\nfrom preceding layers, and conversely for those\nresembling lower cortical areas. This conceptual\nframework is depicted in Fig. 1.\nA feature group that integrates a greater vari-\nety of information from preceding layers would\nexhibit a higher in-degree of causal links, and con-\nversely, those integrating a less variety of infor-\nmation would have fewer. Upon distinguishing\nbetween low in-degree feature groups and high in-\ndegree feature groups, we can project each feature\ngroup onto cortical activity using the methodology\ndescribed in Sec. 3.2. Subsequent to this analysis,\nwe can compare the resulting predicted brain maps\nto see if they align with the cortical hierarchy. This\nhierarchy is gauged using the activity time constant,\na concept described in the following section.\n3.5\nCalculating fMRI time constant\nAs a reference baseline for hierarchy, we calculate\nthe time constant for each brain voxel within the\nNarratives dataset. Our method aligns with the\napproach in (Murray et al., 2014), which leverages\nauto-correlation.\nFigure 1: The causal relationships spanning across lay-\ners serve as a mechanism to differentiate features of\nvarying in-degrees.\nLow in-degree features present\nin the middle late layer receive less causal influence\ncompared to their high in-degree counterparts. We hy-\npothesize that low in-degree features better predict low-\nhierarchical cortical areas, and vice versa.\nGiven a time series for a particular voxel, Wt,\nthe auto-correlation at lag τ is computed as:\nAC(W, τ) = Corr(Wt, Wt−τ)\n(6)\nHere, Corr denotes the correlation coefficient. As\nτ increases, we typically anticipate a decline in\nAC. Accordingly, we can model AC(W, τ) using\nan exponentially decreasing function:\nargmin\nλ\n[exp(τ/λ) −AC(W, τ)]2\n(7)\nSubsequently, the fitted coefficient λ serves as a\nrepresentation of the voxel’s time constant. By it-\nerating over all voxels, we can generate a cortical\nmap showcasing time constants across the Narra-\ntives dataset.\n4\nResults\n4.1\nLanguage Model Features Predict Brain\nActivity\nIn our initial efforts, we sought to replicate the\nfindings presented in prior literature, focusing on\nthe predictive efficacy of language model features\nin relation to the fMRI dataset. The brain predic-\ntion accuracy, quantified using correlation coeffi-\ncients, is illustrated in Fig. 2. This result captures\nthe average across all participants, displayed on\na unified \"fsaverage6\" surface template. To en-\nhance visualization, boundaries and labels sourced\nfrom the Glasser Atlas (Glasser et al., 2016) have\nbeen incorporated. The 3D mesh visualization was\nachieved using the Python-based 3D-rendering en-\ngine, PyVista (Sullivan and Kaszynski, 2019).\nIt is evident from the plot that the prediction ac-\ncuracy map is in line with findings from previous\ninvestigations. Notably, the strongest correlations\nemerge from lower auditory regions, specifically\nA4 and A5. This is closely followed by correla-\ntions within the language network, encompassing\nareas such as Broca’s regions 44 and 45, along with\nsegments of the temporal lobe like STSda, STSva,\nSTSdp, and STSva. Subsequently, we observe no-\ntable correlations in high-order regions: within the\nfrontal lobe areas like IFSa, IFSp, IFJa, and IFJp,\nand within the parietal lobe sectors such as PF,\nPFm, and PGi. The cumulative average correlation\nacross all voxels is 0.0429 for layer 9, which corre-\nsponds to the late middle segment of the OPT-125m\nmodel comprising 12 layers.\nTo test the statistical significance of our findings,\nwe adopted a shuffling approach for the language\nfeatures and repeated the fitting process. This ap-\nproach yielded a null result, characterized by ap-\nproximately Gaussian distributed prediction accu-\nracy with a mean of 0 and a standard deviation of\n0.003. Given that the average value of the accuracy\nmap is around 0.04—a magnitude ten times greater\nthan the standard deviation of the null-hypothesis\naccuracy distribution—it becomes evident that the\nprecision map derived through this method has sta-\ntistical significance.\nFurthermore, our exploration reemphasizes a\npreviously observed trend specific to multi-layer\ntransformer-based auto-regressive models like\nOPT: the predictive power of brain activity initially\nescalates with layer progression until middle-late\nlayers. For instance, when the OPT 125m model\nfits the Narratives dataset, the average prediction\naccuracy manifests as 0.0268, 0.0352, 0.0429, and\n0.0403 for layers 1, 5, 9, and 12, respectively.\nGiven that layer 9 exhibits the peak, our subsequent\nanalyses are based on this layer.\n4.2\nCausal Graph Reveals Cortical Hierarchy\nWe employed the methodology delineated in Sec.\n3.3 to find out the causal relationships among\npairs of layers within the language model. Fig. 3\npresents the derived causality matrix C from layer\n4 to layer 9 of the Opt-125m model. The chosen\nnumber of dimensions for PCA is 20. In the ma-\ntrix, an entry at row i and column j quantifies the\ninfluence of dimension i in layer 4 on dimension j\nFigure 2: Brain prediction accuracy map measured with\ncorrelation shown in fsaverage space, with boundaries\nand labels from Glasser atlas.\nin layer 9. To construct a causal graph, we applied\na thresholding technique. Specifically, if Cij sur-\npasses the threshold defined as the median value\nof the causality matrix, we identify dimension i\nof layer 4 as posing a significant causal impact on\ndimension j in layer 9. By summing across di-\nmension i, we derive a vector that represents the\nnumber of causal links for each dimension j of\nlayer 9. Those dimensions in layer 9 with fewer\ninbound causal links are categorized as \"low in-\ndegree\" dimensions. Conversely, dimensions with\na higher count of inbound causal links are classified\nas \"high in-degree\" dimensions. We designate the\ndimensions within the lower half as low in-degree,\nand those within the upper half as high in-degree\nfeatures.\nHaving partitioned the language model features\ninto low in-degree and high in-degree categories,\nwe then proceeded to predict brain activity for each\ncategory separately, utilizing the methodology pre-\nviously discussed in Sec. 3.2. This process yielded\ntwo distinct prediction accuracy maps, akin to the\none illustrated in Fig. 2. To emphasize the distinct\nregional preferences, we computed the difference\nbetween these two maps, specifically by subtract-\ning the low in-degree map from the high in-degree\nmap. The resultant map is presented in Fig. 4.\nFigure 3: Causality matrix C. An entry at row i and\ncolumn j quantifies the causal influence of dimension i\nin layer 4 on dimension j in layer 9.\nFrom the figure, it is evident that the accuracy\nmaps produced by the high in-degree and low in-\ndegree feature groups display notable differences.\nThe color-coding, based on the subtraction of low\nin-degree from high in-degree features, reveals that\nregions colored in red are better predicted by the\nhigh in-degree group, while those in the opposite\nspectrum are predicted by the low in-degree group.\nSpecifically, lower hierarchical regions proposed\nin (Lerner et al., 2011) such as A4, A5, STSdp,\n44, and 45 tend to align more closely with the\nlow in-degree feature group. In contrast, higher\nhierarchical regions like PF, PFt, 6r, and 7m, are\nbetter represented by the high in-degree feature\ngroup.\nTo assess the statistical significance of our obser-\nvations, we implemented a text-random shuffling\ntechnique. We predicted brain activity based on the\nshuffled language model features along text direc-\ntion, and computed their brain prediction accuracy\nmaps. Their differential accuracy map, obtained by\nsubtracting two maps, follows a Gaussian distribu-\ntion with a mean of 0 and a standard deviation of\n0.004. Given that this deviation is much smaller\nthan typical values observed in Fig. 4, our findings\ncan be considered statistically robust.\nFurthermore, the robustness of our method\nacross various layers and models is demonstrated\nin Appendix Sec. A.1.\n4.3\nTime Constant Reveals Temporal\nHierarchy\nWhile our initial hierarchy assessment was predi-\ncated on the language model fitting brain activity,\na question emerges: Can we directly correlate this\nFigure 4: Brain prediction accuracy difference between\nhigh in-degree feature group and low in-degree feature\ngroup measured with correlation, layer 9.\nLM in-degree mapping to the hierarchy structure\nof the narrative brain data via the activity time con-\nstant?\nThe hierarchy we talked about in the previous\nsection proposed in (Lerner et al., 2011) is calcu-\nlated based on narratives scrambled at different\ntime scales, which cannot be reproduced in our\ncase. Contemporary literature largely supports the\nnotion that hierarchy correlates with activity time\nconstants (Raut et al., 2020). Following this, we\nshow that the hierarchy deduced from causality in\nSec. 4.2 reproduces the hierarchy inferred from\nactivity time constants directly derived from the\nNarratives fMRI dataset.\nIn our analysis, we determined the cortex-wide\nautocorrelation time constant for the Narratives\ndataset using the approach outlined in Sec. 3.5.\nGiven that the time constant typically spans several\nseconds, we employed a maximum shift of 10 TRs\nto estimate the time constant, i.e., a total duration\nof 15 seconds. The resultant time constant map is\nillustrated in Fig. 5. Upon examination, it’s evident\nthat distinct brain regions exhibit varying time con-\nstants. For areas unrelated to language processing,\ntime constants were smaller. Beyond this, the time\nconstants display a gradient, ascending from low\nto high, in alignment with the language hierarchy.\nFigure 5: Time constant map calculated directly from\nfMRI dataset with auto-correlation, the map is thresh-\nolded at 1.5s, which is the TR rate. The unit of the color\nmap is second.\n4.4\nComparison of Hierarchical Rank\nWhile a visual comparison between Fig. 4 and Fig.\n5 provides an intuitive sense of the hierarchical\nsimilarity, a numerical method to quantify this re-\nsemblance is preferred. We employ Spearman’s\nrank correlation as a metric to gauge the similarity\nin hierarchical ranking.\nFirstly, we need to designate the Regions of In-\nterest (ROIs) for inclusion in our rank analysis. We\ndelineate an ROI as a cerebral region within the\nGlasser atlas, that is effectively predicted by the\nlanguage model. We establish a threshold, such\nthat regions with a mean predictive accuracy ex-\nceeding this limit are incorporated. In ensuring\nthat all pertinent cerebral zones are encompassed\nwhile excluding language-unrelated zones, we set\nour threshold at 0.06. This criterion results in the\ninclusion of 44 brain regions, constituting approxi-\nmately one-fourth of the total regions in the Glasser\natlas.\nSubsequent to this, we computed the information\nintegration index of each brain region by the mean\nbrain prediction accuracy difference based on Fig.\n4. Similarly, from Fig. 5, we compute the average\nautocorrelation time constant specific to each re-\nFigure 6: Information integration index v.s. mean time\nconstant per area. The information integration index is\nquantified by the difference in the mean prediction accu-\nracy by high and low in-degree features of the language\nmodel.\ngion. Given our hypothesis that high in-degree fea-\ntures would be better at predicting regions higher in\nthe hierarchy—regions expected to manifest longer\ntime constants—we anticipate a positive correla-\ntion between the mean brain prediction accuracy\ndifference and the mean time constant across our\nselected regions. The result is shown in 6. Align-\ning with our expectations, the resultant Spearman’s\nrank correlation is 0.54, with a highly significant\np-value of 0.00014.\n4.5\nTime Constant of Language Features\nOur previous analyses highlight that the hierar-\nchy in language model features, found out through\ncross-layer causality, relates to the hierarchy seen\nin brain signals during language tasks, as charac-\nterized by activity time scales. This suggests an\nintriguing interplay between information integra-\ntion and temporal hierarchies. Given these findings,\none would anticipate that low in-degree features in\nthe language model would exhibit shorter activity\ntime constants compared to high in-degree features.\nThis expectation is verified in Fig. 7. Using the\nmethodology described in Sec. 3.5, we computed\nthe activity time constants for each dimension of\nlanguage features without applying PCA. The max-\nimum lag we picked was 50 tokens, which roughly\ncorresponds to 10 TRs. We then plot a figure in-\ndegree as the horizontal axis and auto-correlation\ntime constant as the vertical axis. 98% of points\nhave an in-degree within 300 to 500, and auto-\ncorrelation time constant below 2 tokens. It can\nbe seen that in-degree is positively correlated with\nauto-correlation time constant. The resultant Spear-\nman’s rank correlation is 0.30, with a significant\nFigure 7: In-degree v.s. auto-correlation time constant\n(unit: token) in the language model. The maximum lag\nis 50 tokens.\np-value of 1e-17.\nIn addition to the main results, we have included\nsupplementary results in the Appendix. Sec. A.2\ndescribes the reproduction of hierarchical maps\nusing different layers. Section A.3 presents a san-\nity check through hierarchical maps generated by\ngrouping features based on time constants. Lastly,\nSection A.4 demonstrates the creation of hierar-\nchical maps using lower-layer features based on\n’out-degree’.\n5\nDiscussion\nA central concept discussed in this paper is hier-\narchy. The notion of hierarchy, upon closer ex-\namination, reveals itself to be a concept of varied\nforms and interpretations. Figure 8 maps out var-\nious forms of hierarchies and their relations, in-\ncorporating elements from both prior studies and\nour current research. It is divided into two main\nsections: the upper section depicts hierarchies de-\nrived from brain studies, while the lower section\nfocuses on those derived from language models.\nHierarchies originating from the brain are further\ncategorized based on their relevance to language\ntasks. Vertically, the figure illustrates three distinct\nhierarchy forms: network structure hierarchy, hi-\nerarchy inferred from auto-correlation-based time\nconstants, and hierarchy based on information inte-\ngration. The concepts from previous research are\nhighlighted in blue boxes, whereas the green boxes\ndenote the concepts introduced in our study.\nThe anatomical hierarchy, considered a ’gold\nstandard’ in hierarchical studies, is primarily ob-\nserved in non-human primates. It has been shown\nto correlate with the spike auto-correlation time\nconstant in the primate cortex, as indicated by\nthe black arrow in our diagrams (Murray et al.,\n2014). Additionally, hierarchical gradients in the\nauto-correlation of resting-state fMRI signals have\nbeen identified (Raut et al., 2020). Theoretical\nframeworks, such as the workspace framework (De-\nhaene and Naccache, 2001), propose connections\nbetween information integration and anatomical\nhierarchy, represented in our figures by the dashed\nline. Building on this concept, there have been\nstudies demonstrating the emergence of a cortical\nhierarchical map in language tasks. These stud-\nies utilized language patterns shuffled at different\nlevels, highlighting the link between the hierarchi-\ncal map in language tasks and the time window of\ninformation integration (Lerner et al., 2011).\nIn our study, we introduced corresponding new\nblocks of a language model into the figure. Firstly,\nwe map the network structure of the brain onto the\nconcept of a causal graph, and developed a mea-\nsurement of information integration based on the\nin-degree of this causal graph. We demonstrated\nthe ability of this measurement to reveal hierar-\nchical structures captured through language shuf-\nfling techniques of previous work. Secondly, we\nfound a correlation between the hierarchical map,\nas derived from the causal graph in-degree, and the\nfMRI auto-correlation time constant in language\ntasks. Lastly, our analysis of language model auto-\ncorrelation time constants revealed a correlation\nwith the degree of information integration mea-\nsured by in-degree. Collectively, these findings\nunderscore a robust functional parallelism between\nlanguage models and the human brain.\nIt is important to note that our figure does not\nencompass all relevant prior research. For instance,\nthe study by Caucheteux et al. (2023) (Caucheteux\net al., 2023) rediscovers cortical hierarchy through\npredictive time windows, which is not included in\nour current representation.\n6\nConclusion\nThis paper explores the relationship between the\nbrain’s hierarchy, intrinsic temporal scale, and in-\nformation integration in the context of human natu-\nral language processing. Building upon the prevail-\ning hypothesis that higher cortical areas typically\nexhibit longer time scales, and inspired by emerg-\ning research utilizing language models to study\nbrain activity during language perception, we delve\ninto the role of information integration in shap-\ning hierarchy. By categorizing language features\nFigure 8: Different concepts of hierarchy in part of\nprevious works (shown in blue) and our work (shown in\ngreen).\ninto low in-degree and high in-degree groups based\non cross-layer causality, and subsequently using\neach group to predict brain activity, a distinct hi-\nerarchy based on the language model is observed.\nSpecifically, low in-degree features correlate more\nwith lower cortical areas, while high in-degree fea-\ntures align more with higher cortical areas. Intrigu-\ningly, the language model’s auto-correlation time\nconstant is also correlated with these features’ in-\ndegree, which is parallel to the gradient of the brain\nactivity time constant. These findings suggest that\nthe mapping between language model features and\nbrain activity stems from similarity in information\nintegration patterns rather than mere coincidental\nalignments.\nReferences\nMostafa Abdou. 2022.\nConnecting neural response\nmeasurements & computational models of lan-\nguage: a non-comprehensive guide. arXiv preprint\narXiv:2203.05300.\nRichard Antonello and Alexander Huth. 2023. Predic-\ntive coding or just feature discovery? an alternative\naccount of why language models fit brain data. Neu-\nrobiology of Language, pages 1–16.\nRichard Antonello, Aditya Vaidya, and Alexander G\nHuth. 2023. Scaling laws for language encoding\nmodels in fmri. arXiv preprint arXiv:2305.11863.\nSophie Arana, Jacques Pesnot Lerousseau, and Peter\nHagoort. 2023. Deep learning models to study sen-\ntence comprehension in the human brain. Language,\nCognition and Neuroscience, pages 1–19.\nKhai Loong Aw and Mariya Toneva. 2022. Training\nlanguage models for deeper understanding improves\nbrain alignment. arXiv preprint arXiv:2212.10898.\nChristopher Baldassano, Janice Chen, Asieh Zadbood,\nJonathan W Pillow, Uri Hasson, and Kenneth A Nor-\nman. 2017. Discovering event structure in continuous\nnarrative perception and memory. Neuron, 95(3):709–\n721.\nHelen Barbas and Nancy Rempel-Clower. 1997. Cor-\ntical structure predicts the pattern of corticocortical\nconnections. Cerebral cortex (New York, NY: 1991),\n7(7):635–646.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-\nRémi King. 2023. Evidence of a predictive coding\nhierarchy in the human brain listening to speech. Na-\nture human behaviour, 7(3):430–441.\nCharlotte Caucheteux and Jean-Rémi King. 2022.\nBrains and algorithms partially converge in natu-\nral language processing. Communications biology,\n5(1):134.\nClaire HC Chang, Samuel A Nastase, and Uri Has-\nson. 2022.\nInformation flow across the cortical\ntimescale hierarchy during narrative construction.\nProceedings of the National Academy of Sciences,\n119(51):e2209307119.\nRishidev Chaudhuri, Kenneth Knoblauch, Marie-Alice\nGariel, Henry Kennedy, and Xiao-Jing Wang. 2015.\nA large-scale circuit mechanism for hierarchical dy-\nnamical processing in the primate cortex. Neuron,\n88(2):419–431.\nStanislas Dehaene and Lionel Naccache. 2001. Towards\na cognitive neuroscience of consciousness: basic evi-\ndence and a workspace framework. Cognition, 79(1-\n2):1–37.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDaniel J Felleman and David C Van Essen. 1991. Dis-\ntributed hierarchical processing in the primate cere-\nbral cortex. Cerebral cortex (New York, NY: 1991),\n1(1):1–47.\nAngela D Friederici. 2011. The brain basis of language\nprocessing: from structure to function. Physiological\nreviews, 91(4):1357–1392.\nMatthew F Glasser, Timothy S Coalson, Emma C\nRobinson, Carl D Hacker, John Harwell, Essa Ya-\ncoub, Kamil Ugurbil, Jesper Andersson, Christian F\nBeckmann, Mark Jenkinson, et al. 2016. A multi-\nmodal parcellation of human cerebral cortex. Nature,\n536(7615):171–178.\nAriel Goldstein, Eric Ham, Samuel A Nastase, Zaid\nZada, Avigail Grinstein-Dabus, Bobbi Aubrey, Mar-\niano Schain, Harshvardhan Gazula, Amir Feder,\nWerner Doyle, et al. 2022a. Correspondence between\nthe layered structure of deep language models and\ntemporal structure of natural language processing in\nthe human brain. BioRxiv, pages 2022–07.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano\nSchain, Amy Price, Bobbi Aubrey, Samuel A Nas-\ntase, Amir Feder, Dotan Emanuel, Alon Cohen, et al.\n2022b. Shared computational principles for language\nprocessing in humans and deep language models. Na-\nture neuroscience, 25(3):369–380.\nUri Hasson, Eunice Yang, Ignacio Vallines, David J\nHeeger, and Nava Rubin. 2008. A hierarchy of tem-\nporal receptive windows in human cortex. Journal of\nNeuroscience, 28(10):2539–2550.\nGregory Hickok and David Poeppel. 2007. The cortical\norganization of speech processing. Nature reviews\nneuroscience, 8(5):393–402.\nChristopher J Honey, Thomas Thesen, Tobias H Donner,\nLauren J Silbert, Chad E Carlson, Orrin Devinsky,\nWerner K Doyle, Nava Rubin, David J Heeger, and\nUri Hasson. 2012. Slow cortical dynamics and the\naccumulation of information over long timescales.\nNeuron, 76(2):423–434.\nJulia M Huntenburg, Pierre-Louis Bazin, and Daniel S\nMargulies. 2018. Large-scale gradients in human\ncortical organization. Trends in cognitive sciences,\n22(1):21–31.\nAlexander G Huth, Wendy A De Heer, Thomas L Grif-\nfiths, Frédéric E Theunissen, and Jack L Gallant.\n2016. Natural speech reveals the semantic maps that\ntile human cerebral cortex. Nature, 532(7600):453–\n458.\nShailee Jain and Alexander Huth. 2018. Incorporat-\ning context into language encoding models for fmri.\nAdvances in neural information processing systems,\n31.\nShailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel,\nJavier S Turek, and Alexander Huth. 2020. Inter-\npretable multi-timescale models for predicting fmri\nresponses to continuous natural speech. Advances in\nNeural Information Processing Systems, 33:13738–\n13749.\nShailee Jain, Vy A Vo, Leila Wehbe, and Alexander G\nHuth. 2023. Computational language modeling and\nthe promise of in silico experimentation. Neurobiol-\nogy of Language, pages 1–65.\nSreejan Kumar, Theodore R Sumers, Takateru Ya-\nmakoshi, Ariel Goldstein, Uri Hasson, Kenneth A\nNorman, Thomas L Griffiths, Robert D Hawkins, and\nSamuel A Nastase. 2022. Reconstructing the cascade\nof language processing in the brain using the internal\ncomputations of a transformer-based language model.\nBioRxiv, pages 2022–06.\nYulia Lerner, Christopher J Honey, Lauren J Silbert, and\nUri Hasson. 2011. Topographic mapping of a hierar-\nchy of temporal receptive windows using a narrated\nstory. Journal of Neuroscience, 31(8):2906–2915.\nNikola T Markov, Julien Vezoli, Pascal Chameau, Ar-\nnaud Falchier, René Quilodran, Cyril Huissoud,\nCamille Lamy, Pierre Misery, Pascale Giroud, Shi-\nmon Ullman, et al. 2014. Anatomy of hierarchy:\nfeedforward and feedback pathways in macaque vi-\nsual cortex.\nJournal of Comparative Neurology,\n522(1):225–259.\nJuliette Millet, Charlotte Caucheteux, Yves Boubenec,\nAlexandre Gramfort, Ewan Dunbar, Christophe Pal-\nlier, Jean-Remi King, et al. 2022. Toward a realistic\nmodel of speech processing in the brain with self-\nsupervised learning. Advances in Neural Information\nProcessing Systems, 35:33428–33443.\nJohn D Murray, Alberto Bernacchia, David J Freed-\nman, Ranulfo Romo, Jonathan D Wallis, Xinying\nCai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hy-\nojung Seo, Daeyeol Lee, et al. 2014. A hierarchy\nof intrinsic timescales across primate cortex. Nature\nneuroscience, 17(12):1661–1663.\nSamuel A Nastase, Yun-Fei Liu, Hanna Hillman, Asieh\nZadbood, Liat Hasenfratz, Neggin Keshavarzian, Jan-\nice Chen, Christopher J Honey, Yaara Yeshurun, Mor\nRegev, et al. 2021. The “narratives” fmri dataset for\nevaluating models of naturalistic language compre-\nhension. Scientific data, 8(1):250.\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages\n2303–08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nMatthew A Lambon Ralph, Elizabeth Jefferies, Karalyn\nPatterson, and Timothy T Rogers. 2017. The neu-\nral and computational bases of semantic cognition.\nNature reviews neuroscience, 18(1):42–55.\nRyan V Raut, Abraham Z Snyder, and Marcus E Raichle.\n2020. Hierarchical dynamics as a macroscopic orga-\nnizing principle of the human brain. Proceedings of\nthe National Academy of Sciences, 117(34):20890–\n20897.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Ca-\nrina Kauf, Eghbal A Hosseini, Nancy Kanwisher,\nJoshua B Tenenbaum, and Evelina Fedorenko. 2021.\nThe neural architecture of language: Integrative\nmodeling converges on predictive processing. Pro-\nceedings of the National Academy of Sciences,\n118(45):e2105646118.\nDan Schwartz, Mariya Toneva, and Leila Wehbe. 2019.\nInducing brain-relevant bias in natural language pro-\ncessing models. Advances in neural information pro-\ncessing systems, 32.\nC Sullivan and Alexander Kaszynski. 2019. Pyvista:\n3d plotting and mesh analysis through a streamlined\ninterface for the visualization toolkit (vtk). Journal\nof Open Source Software, 4(37):1450.\nMariya Toneva and Leila Wehbe. 2019. Interpreting and\nimproving natural-language processing (in machines)\nwith natural language-processing (in the brain). Ad-\nvances in Neural Information Processing Systems,\n32.\nDaniel LK Yamins, Ha Hong, Charles F Cadieu,\nEthan A Solomon, Darren Seibert, and James J Di-\nCarlo. 2014. Performance-optimized hierarchical\nmodels predict neural responses in higher visual cor-\ntex. Proceedings of the national academy of sciences,\n111(23):8619–8624.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nA\nAppendix\nA.1\nRobustness\nIn this section, we aim to demonstrate the robust-\nness of our results across various model layers,\nscales, and types.\nIn our primary manuscript, we utilized layer 9\nof Opt-125m due to its superior brain prediction\ncapabilities. To validate the consistency of our\nfindings, we also examine neighboring layers, such\nas layer 8. Figure 9 depicts the difference in brain\nprediction accuracy between the high in-degree\nand low in-degree feature groups of layer 8 when\npaired with layer 3. Furthermore, we assessed our\nresults using the larger Opt-350m language model,\nas illustrated in Fig. 10 for layers 6 and 12. We\nalso applied our methodology to GPT2, examining\nlayers 4 and 9, with the outcomes presented in Fig.\n11. The hierarchical rank was determined using the\napproach detailed in Sec. 4.4, and the collective\nresults are summarized in Table 1.\nTable 1: Spearman correlation of calculated hierarchical rank among different models. All entries except for those\ninside parenthesis has p-value smaller than 0.05.\nFeature\nOpt125m L8\nOpt125m L9\nOpt350m L12\nGPT2 L9\nTime Constant\nOpt125m L8\n1.0\n0.81\n0.31\n0.58\n0.36\nOpt125m L9\n0.81\n1.0\n(0.08)\n0.66\n0.54\nOpt350m L12\n0.31\n(0.08)\n1.0\n0.39\n(0.23)\nGPT2 L9\n0.58\n0.66\n0.39\n1.0\n0.47\nTime Constant\n0.36\n0.54\n(0.23)\n0.47\n1.0\nFigure 9: Brain prediction accuracy difference between\nhigh in-degree feature group and low in-degree feature\ngroup measured with correlation, layer 8 of Opt-125m..\nA.2\nHierarchy from Layers\nBesides information integration and activity time\nconstants, existing literature has discussed the rela-\ntionship between brain hierarchy and the layers of\nlanguage models (Toneva and Wehbe, 2019; Gold-\nstein et al., 2022a). We further validated these\nfindings using the Narratives dataset.\nFig. 12 displays the differential map of brain\nprediction accuracy between layers 9 and 1 of the\nOPT-125m language model features (specifically,\nthe result of subtracting layer 1 from layer 9). This\npattern aligns with those derived from causality\nand activity time constants. However, differences\nare evident. As the layer number ascends, there is\nminimal decline in prediction accuracy for lower\nhierarchy areas. Conversely, there’s a significant\nsurge in accuracy for higher hierarchy regions, lead-\nFigure 10: Brain prediction accuracy difference between\nhigh in-degree feature group and low in-degree feature\ngroup measured with correlation, layer 12 of Opt-350m.\ning to a nearly monotonic increase in prediction\naccuracy from layer 1 to layer 9. This suggests that\nthe representations in layer 9 contains information\nto predict both lower and higher hierarchy brain\nregions.\nTo substantiate this hypothesis, we plotted the\nROI average brain prediction precision across the\nlayers of OPT 125m, spanning from layer 1 to layer\n9, as illustrated in Fig. 13. The plotted accuracy\nis normalized by taking its ratio to the accuracy of\nlayer 9 of each region. The results indicate that\nlower-order regions, such as A4 and A5, are effec-\ntively predicted by layer 1, whereas higher cortical\nareas, like PF and 31pd, benefit more from higher\nlayers.\nFigure 11: Brain prediction accuracy difference between\nhigh in-degree feature group and low in-degree feature\ngroup measured with correlation, layer 9 of Gpt2.\nA.3\nSorting language features with time\nconstant\nOur main result reported in Sec. 4.2 groups features\nbased on information integration. And we related\nthe calculated hierarchy with that calculated from\nactivity time scale. As a straight forward sanity\ncheck, if we group features also based on activity\ntime constant of language features, brain hierarchy\nwould also expected to emerge.\nThe result is shown in Fig. 14. It can be seen that\nbrain prediction accuracy difference from feature\ngroup with different time scale can also capture cor-\ntical hierarchy. Where fast features predict lower\ncortical regions like A4, A5 better, while slow fea-\ntures predict higher cortical regions like PF, 31pd\nbetter.\nOur principal findings presented in Sec. 4.2 cat-\negorize features based on causality. We then cor-\nrelated the derived hierarchy with that calculated\nfrom the activity time scale. As a validation, one\nwould anticipate that by grouping features based\non the activity time constant of language features,\nthe brain hierarchy would also become evident.\nThis observation is illustrated in Fig. 14. The dif-\nference in brain prediction accuracy among feature\ngroups with varying time scales delineates the corti-\nFigure 12: Brain hierarchy calculated by precision map\nsubtraction between layer 9 and layer 1 of Opt 125m\nlanguage model features.\ncal hierarchy. Specifically, features with faster time\nscales more accurately predict lower-order regions\nsuch as A4 and A5, whereas features characterized\nby slower time scales are better suited to predicting\nhigher cortical areas like PF and 31pd.\nA.4\nFeatures from Low Layers\nIn the main manuscript, we segregated the features\nof layer 9 based on in-degree measures using a\ncausal graph measure determined between layers\n4 and 9. This demonstrated that the delineated\nfeatures align with the cortical hierarchy during\nlanguage processing. Our preference for layer 9\nstems from its optimal fit with brain data. Notably,\nthe causality matrix can also be applied to partition\nthe features of layer 4 using \"out-degree\". Here,\nfeatures are partitioned into groups with \"low out-\ndegrees\" and \"high out-degrees\". We expected that\nfeatures with \"high out-degree\" may excel at pre-\ndicting low cortical area. As described in Sec. A.2,\nearlier layers adequately predict activity in lower\ncortical regions. To validate our approach, we ex-\npect that if we utilize \"high out-degrees\" features\nfrom layer 4 combined with \"high in-degrees\" fea-\ntures from layer 9, the cortical hierarchy would also\nemerge. As depicted in Fig. 15, our primary con-\nclusion remains valid. The calculated Spearman’s\nFigure 13: Normalized average brain prediction accu-\nracy for each ROI region along number of layers.\nrank correlation with the time constant map is 0.57,\np-value is 5e-5.\nWe also present results derived exclusively from\nfeatures of layer 4. While this layer hasn’t fully\ndeveloped features that adeptly predict brain activ-\nity, especially in higher cortical areas, examining\nthe correlation map differences between its \"low\nout-degree\" and \"high out-degree\" features remains\ninsightful. The findings are illustrated in Fig. 15.\nThe associated Spearman’s rank correlation with\nthe time constant map stands at 0.37, p-value is\n0.015.\nFigure 14: Brain hierarchy calculated by precision map\nsubtraction between slow group and fast group (slow\nminus fast) of Opt-125m language model features.\nFigure 15: Brain prediction accuracy difference between\n\"high in-degrees\" feature group of layer 9 and \"high\nout-degrees\" feature group of layer 4, measured with\ncorrelation.\nFigure 16: Brain prediction accuracy difference between\n\"low out-degrees\" feature group and \"high out-degrees\"\nfeature group of layer 4, measured with correlation.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-11-17",
  "updated": "2023-11-17"
}