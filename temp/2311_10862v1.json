{
  "id": "http://arxiv.org/abs/2311.10862v1",
  "title": "Formal concept analysis for evaluating intrinsic dimension of a natural language",
  "authors": [
    "Sergei O. Kuznetsov",
    "Vasilii A. Gromov",
    "Nikita S. Borodin",
    "Andrei M. Divavin"
  ],
  "abstract": "Some results of a computational experiment for determining the intrinsic\ndimension of linguistic varieties for the Bengali and Russian languages are\npresented. At the same time, both sets of words and sets of bigrams in these\nlanguages were considered separately. The method used to solve this problem was\nbased on formal concept analysis algorithms. It was found that the intrinsic\ndimensions of these languages are significantly less than the dimensions used\nin popular neural network models in natural language processing.",
  "text": "Formal concept analysis for evaluating intrinsic\ndimension of a natural language\nSergei O. Kuznetsov1[0000−0003−3284−9001], Vasilii A.\nGromov1[0000−0001−5891−6597], Nikita S. Borodin1[0000−0002−7102−4443], and\nAndrei M. Divavin\nHSE University, Moscow 109028, Russia\nAbstract. Some results of a computational experiment for determining\nthe intrinsic dimension of linguistic varieties for the Bengali and Russian\nlanguages are presented. At the same time, both sets of words and sets of\nbigrams in these languages were considered separately. The method used\nto solve this problem was based on formal concept analysis algorithms.\nIt was found that the intrinsic dimensions of these languages are signif-\nicantly less than the dimensions used in popular neural network models\nin natural language processing.\nKeywords: Intrinsic dimension · Formal concept analysis · Language\nmanifold.\n1\nIntroduction\nThe emergence of methods for representing words and n-grams of a natural lan-\nguage as real-valued vectors (embeddings) allows one to ask about the definition\nof intrinsic dimensions of sets of words and bigrams observed in a given natural\nlanguage. As a manifestation of the concept of intrinsic dimensions, let us con-\nsider a sphere, which is a two-dimensional object, where every point is given by\ntwo coordinates - latitude and longitude. Being embedded in three-, five- and\nten-dimensional space, it will be given by a set of vectors, respectively, with\nthree, five and ten coordinates, however, but it will remain a two-dimensional\nobject, and its intrinsic dimension will be equal to two. The problem of intrinsic\ndimension is important from the practical point of view: its solution will allow\none to judge the appropriateness of using very large vectors of embeddings (first\nof all, neural network models like BERT, etc.) in NLP domain.\n2\nRelated work\nTurning to methods for estimating the intrinsic dimension, we note, first of\nall, the work of V. Pestov on intrinsic dimension of a dataset [17], where the\nrequirements for the definition of intrinsic dimension and methods for obtaining\nit were formulated. The introduced axiomatics is based on the results of M.\nGromov [9].\narXiv:2311.10862v1  [cs.CL]  17 Nov 2023\n2\nS.O. Kuznetsov et al.\nIt seems to us that the approaches known from the literature can be divided\ninto three large classes: methods using a method for estimating the dimension\nof the strange attractor [12]; graph-based methods [4, 5, 14]; methods based to\none or another variant of persistent homology analysis [20]. Let us take a look\nat the three above approaches.\nFirst of all, among the works related to the establishment of the dimensions\nof the strange attractor we note a classical monograph by Kantz and Schrei-\nder [12]. It considers a set of classical approaches to the definition of the concept\ndimension of a strange attractor (topological, Hausdorff, spectrum, generalized\nentropy dimensions, etc.) and methods for their evaluation. Unfortunately, the\nclassical approaches to determining the dimension of the strange attractors suffer\nfrom two disadvantages: firstly, they usually require very significant computing\npower and, secondly, are often non-robust with respect to to sample changes.\nThese circumstances necessitated creation of a new generation of methods for\nestimating the intrinsic dimension.\nKozma et al. [14] proposed a method of estimating the upper box dimension\nusing a minimum spanning tree and statistics based on it. J.A. Costa et al. [4]\nand A. Farahmand et al.\n[5] in numerous papers have developed the idea of\nestimating intrinsic dimension by examining nearest neighbor graphs. In 2013\nM.R. Brito et al.\n[3] proposed an approach based on nearest neighbor graph\n(KNN), minimum weight spanning tree (MST) and sphere of influence (SOI)\nanalysis to determine the Euclidean dimension of i.i.d. geometric dataset.\nAdams et al. [1] suggested a way to computing intrinsic dimension using\npersistent homology (Persistent homology dimension). The idea is to investigate\nthe properties of random variables of the following form:\nEi\nα(x1, . . . , xn) =\nX\nI∈P Hi(x1,...,xn)\n|I|α ,\n(1)\nwhere {xj}j∈N are i.i.d. samples from a probability measure on a metric space,\nPHi(x1, . . . , xn) denotes the i-dimensional reduced persistent homology of the\nˇCech or Vietoris–Rips complex of {x1, . . . , xn}, and |I| is the length of a persis-\ntent homology interval. Schweinhart [20] carried out a rigorous formal analysis\nof this estimates, its connection is established with the upper box dimension. Ja-\nquette and Schweinhart extended the methodology to the case of fractal systems,\ncharacterized by non-integer dimensions.\nWe also note a stand-alone work [11], based on the concept of distance be-\ntween metric spaces, introduced by M.Gromov [9] and formal concept analysis\n[6, 15].\n3\nMain definitions and problem statement\n3.1\nFormal Concept Analysis and Pattern Structures\nHere we give basic definitions and facts related to Formal Concept Analysis [15]\nand Pattern structures from [6].\nFCA to evaluate intrinsic dimension of a natural language\n3\nIn basic FCA binary data are given by a formal context K = (G, M, I), where\nG is the set of objects, M is the set of (binary) attributes, and I is a incidence\nrelation between objects and attributes: I ⊆G × M.\nDerivation (prime) operators (·)′ are defined as follows: A′ gives the subset\nof all attributes shared by all objects from A ⊆G. Similarly, B′ gives the subset\nof all objects having all attributes from B ⊆M.\nA′ = {m ∈M | ∀g ∈A : gIm},\n(2)\nB′ = {g ∈G | ∀m ∈B : gIm},\n(3)\nA formal concept is a pair (A, B) of subsets of objects A and attributes B,\nsuch that A = B′, and B = A′.\nThe generality order ≤on concepts is defined as follows:\n(A1, B1) ≤(A2, B2)ifA1 ⊆A2(⇔B2 ⊆B1).\nThe set of all concepts makes an algebraic lattice, called concept lattice, w.r.t.\nthe partial order ≤, so that every two concepts have supremum and infimum\nw.r.t. ≤.\nPattern structures [6, 15] propose a generalization of FCA so that objects,\ninstead of sets of binary attributes, can be described by complex descriptions,\nthe set of which is ordered w.r.t. subsumption (containment) relation.\nLet G be a set of objects, let (D, ⊓) be a meet-semilattice of descriptions, and\nlet δ : G →D be a mapping that assigns a description to each object from G.\nThen (G, D, δ), where D = (D, ⊓); is called a pattern structure w.r.t. “similarity\noperation” ⊓, provided that the set δ(G) := {δ(g) | g ∈G} generates a complete\nsubsemilattice (Dδ, ⊓) of (D, ⊓), i.e., every subset X of δ(G) has an infimum ⊓X\nin (D, ⊓) and Dδ is the set of these infima.\nIf (G, D, δ) is a pattern structure, the derivation (prime) operators are de-\nfined as\nA⋄:= ⊓\ng∈Aδ(g) for all A ⊆G\nd⋄:= {g ∈G | d ⊑δ(g)} for all d ∈D\nThe set D is partially ordered w.r.t. the following subsumption relation:\nc ⊑d: ⇐⇒c ⊓d = c\nA pattern concept of (G, D, δ) is a pair (A, d) satisfying\nA ⊆G, d ∈D, A⋄= d and A = d⋄\nThe set of all pattern concepts forms the pattern concept lattice.\nIn this paper we will use interval pattern structure [13, 15], an important case\nof pattern structures where descriptions from D are tuples of closed numerical\n4\nS.O. Kuznetsov et al.\nintervals of the form [s, t], s, t ∈N and the similarity operation on two interval\ntuples is given component-wise as their convex hull:\n[s, t] ⊓[q, r] = [min{s, q}, max{t, r}].\nThe intuition of this definition is explained by interpreting numerical intervals\nas uncertainty intervals, so that their similarity is a minimal convex cover of both\nuncertainties.\n3.2\nIntrinsic Data Dimension and Problem Statement\nIn Hanika et al. [11], the authors suggest using approaches based on Gromov’s\nmetric [9] and Pestov’s axiomatic approach [18] to estimate the intrinsic dimen-\nsion of data. So, given tabular data in the form of a formal context K = (G, M, I)\nand measures νG, νM on sets G and M of objects and attributes, respectively,\nthe complexity of the dataset (called observed diameter) is defined as\nDiam(K, α) = max{νG(A) | (A, B) ∈B(K), α < νM(B) < 1 −α},\n(4)\nwhere α is a parameter. If datatable is not binary, but e.g., given by an\ninterval pattern structure PS, then this definition can be extended as follows:\nDiam(PS), α) = max{νG(A) | (A, d) ∈B(PS), α < νM(d) < 1 −α},\n(5)\nwhere α is a parameter. A natural specification of the general form of νM in this\ndefinition would be\nX\ni∈M\nνi · 1(yi−xi)≤θ,\nwhere [xi, yi] is the ith interval component of tuple d, νi is the measure (weight)\nassociated to attribute i ∈M, and θ is a parameter.\nThe intrinsic dimension of the context (dataset) is then defined as follows:\nDim(K) =\n\u0012Z 1\n0\nDiam (K, α) δα\n\u0013−2\n,\n(6)\nNow the problem we solve in this paper can formally be stated as follows.\nGiven a set of natural language texts C = {T1, . . . , TN}; d ∈N, the dimension\nof the embedding space; n ∈N, the number of words in n-grams (it is assumed\nthat the set of texts C is a representative sample of texts of the corresponding\nnatural language)\n1. compute the sets En(d), n ∈N of embeddings of texts from C, construct the\nset F of respective threshold binary attributes F, where f(e) ∈F means\ne > 0 for e ∈En(d),\nFCA to evaluate intrinsic dimension of a natural language\n5\n2. compose the context K = (C, F, I), with the relation\nI = {(T, f(e)) | e(T) > 0 for embedding e of text T},\n3. calculate the approximate value of (6) by using the trapezoid method for\nintegration:\nDim(K; ℓ) =\n \n1\n2ℓ\nℓ\nX\ni=1\n\u0014\nDiam\n\u0012\nK, i −1\nℓ\n\u0013\n+ Diam\n\u0012\nK, i\nℓ\n\u0013\u0015!−2\n,\n(7)\nhere ℓis the number of intervals used for approximation.\n4\nRealization of the model\n4.1\nComputing Data Dimension\nTo compute data dimension according to (7) one does not need to compute the\nset of all concepts, which can be exponentially large w.r.t. initial data. One can\nuse the properties of monotonicity of measures νG and νM and antimonotonicity\nof prime operators (·)′. The general idea of computing the observed diameter\nis as follows: start with largest possible concept extents (corresponding to one-\nelement sets of attributes), which have largest νG measure, and decrease them\nuntil the measure νM of respective concept intents falls in the interval [α; 1−α].\nMore formally:\n1. For every attribute m ∈M compute m′ and m′′, generating concepts (A, B),\nwhere A = m′, and B = m′′.\n2. If some of the generated concepts satisfy (4), take concept (A, B) with the\nlargest νG(A) as the solution to the problem.\n3. If there are no (A, B) satisfying (4), for every (A, B) generated so far compute\n((B ∪{n})′, (B ∪{n})′′, where n ̸∈B.\n4. Iterate steps 1, 2 and 3 until solution is obtained for all given αi ∈{α}ℓ⊆\n[0; 1].\n5. Compute Dim(K; ℓ).\nSince the cardinality of intents are growing with every iteration of Steps 2\nand 3, finally the process will attain intents satisfying α < νM(B) < 1 −α. It\nis no need to go further by increasing intents, because with increasing intents,\nextents will decrease with no more chance to obtain a concept extent with the\nlargest measure νG satisfying (4).\nWhat is the computational complexity of first four steps for a single value of\nα? If α = 0, then, by monotonicity of measure νG, the algorithm terminates at\nthe first iteration of step 2 by outputting max νG(m′) for m ∈M, which takes\nO(|M|) applications of prime operation (·)′. If α ̸= 0, then one needs to generate\nintents with νM ≤α until the subsequent generation in step 3 would produce\nintents with νM > α, so that the observed diameter would be the maximal νG of\nrespective extents. For α > 0 let k(α) denote the number of concepts (A, B) with\nα < νM(B) < 1 −α. Then, applying the argument of “canonicity check” [16],\none can show that the algorithm terminates upon O(|M| · k(α)) applications of\n(·)′.\n6\nS.O. Kuznetsov et al.\n4.2\nPreprocessing text corpora\nTo compute language dimensions we take standard open source Internet lan-\nguage corpora that represent national prose and poetry for Bengali, English and\nRussian languages. The preprocessing of corpora consists of several steps.\n1) Removal of stop words: some words are found in large numbers in texts that\naffect a variety of subject areas and, often, are not in any way informative or\ncontextual, like articles, conjunctions, interjections, introductory words;\n2) Tokenization: other words can provide useful information only by their pres-\nence as a representative of a certain class, without specificity; good examples\nfor tokenization are proper names, sentence separators, numerals;\n3) Lemmatization: it is useful to reduce the variability of language units by\nreducing words to the initial form\nThen we apply standard techniques based on tf-idf measure to select keyterms\nfrom obtained n-grams of the texts, so that every text in the corpus is converted\nto a set of keyterms and the text-keyterm matrix is generated. Upon this we\napply SVD-decomposition of this matrix and select first high-weight components\nof the decomposition to obtain semantic vector-space of important features of\nthe language and allow for computing respective embeddings of the texts from\nthe corpus. This results in obtaining text-features matrix, which is originally\nnumerical. Then the numerical matrix can be either converted to binary one\nby selecting value thresholds or treated directly by means of interval pattern\nstructures described above.\n5\nComputer Experiments\nWe used the approach described above to estimate the intrinsic dimension of\nRussian, Bengali, and English languages. To do this, we form groups of sample\nsets of n-grams for different parameters n and d. Then, for each n ∈1, 2 (103952\nand 14775439 Russian words and bigrams; 209108 and 13080621 for Bengali\nwords and bigrams; 94087 and 9490603 English words and bigrams), we average\nthe obtained intrinsic dimensions over d and round it to the nearest integer. We\ntook the most obvious realization of the measures as νM(X) = |X| and νG(Y ) =\n|Y |. The results are presented in the Tables (1, 2) for 1,2-grams, respectively.\n6\nConclusion and future directions\nWe have applied the combination of Formal Concept Analysis [15] with an ap-\nproach based on M. Gromov metrics [9, 11] to estimating intrinsic dimensions of\nlinguistic structures with complex statistical characteristics. It is striking that\nfor natural languages we observe very small values of intrinsic dimensions.\nNamely, the indicated dimension was found to be ≈5 for all given languages.\nThis fact allows us to conclude that the orders of intrinsic dimension are equal\nfor the above languages.\nObvious directions for future research can be analysis of other definitions of\nlanguage dimension; extending the list of languages for similar analysis, including\nlanguages from various language families.\nFCA to evaluate intrinsic dimension of a natural language\n7\nTable 1. Intrinsic dimension estimation for natural languages, n=1.\nLanguage\nd\nIntrinsic dimension\nBengali\n5\n6.2509\n8\n5.2254\n14\n4.7445\n20\n4.5318\nRussian\n5\n6.2505\n8\n5.2308\n14\n4.6470\n20\n4.4438\nEnglish\n5\n6.2509\n8\n5.2302\n14\n4.6437\n20\n4.4369\nTable 2. Intrinsic dimension estimation for natural languages, n=2.\nLanguage\nd\nIntrinsic dimension\nBengali\n5\n5.6792\n8\n5.6346\nRussian\n5\n5.8327\n8\n5.6610\nEnglish\n5\n5.4013\n8\n5.6058\nAcknowledgements\nThe work of Sergei O. Kuznetsov was supported by the Russian Science Founda-\ntion under grant 22-11-00323 and performed at HSE University, Moscow, Russia.\nReferences\n1. Adams, H., Aminian, M., Farnell, E., Kirby, M., Mirth, J., Neville, R., Peterson,\nC., Shipman, P., Shonkwiler, C.: A fractal dimension for measures via persistent\nhomology. In: Abel Symposia, (2019).\n2. Bellegarda, J.: Latent Semantic Mapping: principles& applications. Morgan& Clay-\npool; (2007).\n3. Brito, M., Quiroz, J., Yukich, E.: Intrinsic dimension identification via graph-\ntheoretic methods. J. Multivar. Anal. 116: 263-277, (2013).\n4. Costa, J., Girotra, A., Hero, A.: Estimating local intrinsic dimension with k-nearest\nneighbor graphs. In: IEEE/SP 13th Workshop on Statistical Signal Processing,\nIEEE Conference Publication, pp. 417–422, (2005).\n5. Farahmand, A., Szepesv´ari, C., Audibert, J.-Y.: Manifold-adaptive dimension esti-\nmation. In: Z. Ghahramani (Ed.), Proceedings of the 24th International Conference\non Machine Learning, ACM, New York, pp. 265–272, (2007).\n6. Ganter, B., Kuznetsov, S.: Pattern Structures and Their Projections. Proc. ICCS:\n129-142, (2001).\n8\nS.O. Kuznetsov et al.\n7. Ganter, B., Wille, R.: Formal Concept Analysis: Mathematical Foundations.\nSpringer, (1999).\n8. Golub, G., Kahan, W.: Calculating the singular values and pseudo-inverse of a\nmatrix. Journal of the Society for Industrial and Applied Mathematics; Series B:\nNumerical Analysis, 2(2): 205-224, (1965).\n9. Gromov, M.: Metric structures for Riemannian and non-Riemannian spaces. Transl.\nfrom the French by Sean Michael Bates. With appendices by M. Katz, P. Pansu, and\nS. Semmes. Edited by J. LaFontaine and P. Pansu. English, Boston, MA: Birkh¨auser,\npp. xix + 585, (1999).\n10. Gromov, V., Migrina, A.: A Language as a Self-Organized Critical System. Com-\nplexity. ArticleID 9212538, (2017).\n11. Hanika, T., Schneider, F., Stumme G.: Intrinsic dimension of geometric data sets.\nIn: Tohoku Mathematical Journal (2018).\n12. Kantz, H., Schreiber, T.: Nonlinear time series analysis. Cambridge university\npress. (2004).\n13. Kaytoue, M., Kuznetsov, S., Napoli, A., Duplessis, S.: Mining gene expression data\nwith pattern structures in formal concept analysis. Inf. Sci. 181(10): 1989-2001,\n(2011).\n14. Kozma, G., Lotker, Z., Stupp, G.: The minimal spanning tree and the upper box\ndimension, Proc. Am. Math. Soc., (2006).\n15. Kuznetsov, S.: Pattern Structures for Analyzing Complex Data. Proc. RSFDGrC:\n33-44, (2009).\n16. Kuznetsov, S.: A fast algorithm for computing all intersections of objects from\nan arbitrary semilattice. Nauchno-Tekhnicheskaya Informatisya Ser.2, pp. 17-20,\n(1993).\n17. Pestov, V.: Intrinsic dimension of a dataset: what properties does one expect?\nIJCNN: 2959-2964, (2007).\n18. Pestov, V.: An axiomatic approach to intrinsic dimension of a dataset, Neural\nNetworks 21, no. 2-3, 204-213, (2008).\n19. Piantadosi, S.:“Zipf’s word frequency law in natural language: A critical review\nand future directions,” Psychon Bull Rev, vol. 21(5), pp. 1112-1130, (2014).\n20. Schweinhart,\nB.:\nFractal\ndimension\nand\nthe\npersistent\nhomology\nof\nrandom\ngeometric\ncomplexes.\nAdvances\nin\nMathematics.\n7\nOct\n2020;\n372:107291.https://doi.org/10.1016/j.aim.2020.107291, (2020).\n21. Shopen, T.: Language Typology and Syntactic Description: Volume 3, Grammati-\ncal Categories and the Lexicon, Cambridge University Press, (2007).\n22. Tanaka-Ishii, K.: Statistical Universals of Language. Springer, (2021).\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "math.AT"
  ],
  "published": "2023-11-17",
  "updated": "2023-11-17"
}