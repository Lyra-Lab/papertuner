{
  "id": "http://arxiv.org/abs/2311.11537v1",
  "title": "ADAPTER-RL: Adaptation of Any Agent using Reinforcement Learning",
  "authors": [
    "Yizhao Jin",
    "Greg Slabaugh",
    "Simon Lucas"
  ],
  "abstract": "Deep Reinforcement Learning (DRL) agents frequently face challenges in\nadapting to tasks outside their training distribution, including issues with\nover-fitting, catastrophic forgetting and sample inefficiency. Although the\napplication of adapters has proven effective in supervised learning contexts\nsuch as natural language processing and computer vision, their potential within\nthe DRL domain remains largely unexplored. This paper delves into the\nintegration of adapters in reinforcement learning, presenting an innovative\nadaptation strategy that demonstrates enhanced training efficiency and\nimprovement of the base-agent, experimentally in the nanoRTS environment, a\nreal-time strategy (RTS) game simulation. Our proposed universal approach is\nnot only compatible with pre-trained neural networks but also with rule-based\nagents, offering a means to integrate human expertise.",
  "text": "pre-print\nADAPTER-RL: ADAPTATION OF ANY AGENT USING\nREINFORCEMENT LEARNING\nYizhao Jin & Gregory Slabaugh & Simon Lucas\nQueen Mary University of London Game AI Group\nLondon, United Kingdom\n{y.jin, g.slabaugh, simon.lucas}@qmul.ac.uk\nABSTRACT\nDeep Reinforcement Learning (DRL) agents frequently face challenges in adapt-\ning to tasks outside their training distribution, including issues with over-fitting,\ncatastrophic forgetting and sample inefficiency.\nAlthough the application of\nadapters has proven effective in supervised learning contexts such as natural lan-\nguage processing and computer vision, their potential within the DRL domain\nremains largely unexplored. This paper delves into the integration of adapters in\nreinforcement learning, presenting an innovative adaptation strategy that demon-\nstrates enhanced training efficiency and improvement of the base-agent, experi-\nmentally in the nanoRTS environment, a real-time strategy (RTS) game simula-\ntion. Our proposed universal approach is not only compatible with pre-trained\nneural networks but also with rule-based agents, offering a means to integrate hu-\nman expertise.\n1\nINTRODUCTION\nDeep Reinforcement Learning (DRL) agents face challenges when tasked with problems outside of\ntheir training distribution, especially those they haven’t experienced during training. Packer et al.\n(2018) point out that current RL algorithms easily overfit to a fixed environment, because they are\nusually trained on the test set. The sensitivity to environmental changes means DRL models often\nmust be trained from scratch when encountering new tasks. Compounding this, the sample inef-\nficiency inherent to DRL algorithms can leave them ill-equipped for unencountered scenarios at\ninference, considering their reliance on vast sample sets to grasp basic behaviors. And the dynamic\nnature of reinforcement learning data adds to the intricacy, as agents refine their strategies, the data\nthey gather evolves, potentially leading to learning complexities or even instability if not judiciously\naddressed. Equally critical is the balance DRL agents must maintain between exploration and ex-\nploitation; leaning too heavily on known tactics can undermine their competence in novel situations.\nAdditionally, catastrophic forgetting presents a significant hurdle, particularly when agents learn\ntasks in sequence, causing a performance dip in previously learned tasks as they adjust to new ones.\nWhen employing an expert demonstrator, behavior cloning Pomerleau (1988); Bain & Sammut\n(1995), also referred to as imitation learning, becomes a viable method to train an agent. However,\nthis approach is not without its challenges. Firstly, the success of imitation learning hinges critically\non the caliber of the demonstrations. Should the expert exhibit a mistake or suboptimal behavior,\nthe agent is inclined to replicate such discrepancies. Secondly, a distribution mismatch often arises\nin behavior cloning between the states the expert accesses and the states the agent encounters post-\ndeployment. This mismatch can lead to the agent behaving unpredictably in unfamiliar situations.\nComplicating matters further, an agent, unlike the expert, is prone to errors. Such mistakes can\nland the agent in unfamiliar terrain, risking an escalation of errors as the agent strays from the\nexpert’s state distribution. Additionally, behavior cloning does not furnish the agent with a feedback\nmechanism akin to the rewards in reinforcement learning. This absence means that post-training,\nthe agent lacks the innate capacity to identify and amend its erroneous decisions. Lastly, an agent’s\nproficiency is tethered to the expert’s capabilities, meaning that the agent’s performance will often\nnot surpass that of the expert.\n1\narXiv:2311.11537v1  [cs.AI]  20 Nov 2023\npre-print\nTo address these inherent limitations, scholars have pioneered methods fusing imitation learning\nwith reinforcement learning, notably Dataset Aggregation (DAgger) Ross et al. (2011) and Gener-\native Adversarial Imitation Learning (GAIL) Ho & Ermon (2016). DAgger operates by perpetually\ninteracting with the environment using strategies derived from behavioral cloning to generate fresh\ndata. With this new data, DAgger solicits examples from the expert’s strategy, retrains using behav-\nioral cloning on the augmented dataset, and subsequently re-engages with the environment, iterating\nthis process. This method, powered by data augmentation and continuous environment interaction,\nsignificantly reduces the instance of unvisited states and, in turn, the error margin. Nevertheless,\nDAgger demands impeccable expertise quality. GAIL, on the other hand, is underpinned by the\nGenerative Adversarial Networks (GANs) Goodfellow et al. (2014) framework. In this arrange-\nment, the agent, playing the role of the generator, endeavors to produce trajectories that mirror those\nof the expert. Concurrently, a discriminator works to differentiate between the two. The agent then\ngarners rewards for deceiving the discriminator, enabling it to receive feedback, analogous to re-\nwards in RL, without any explicit reward cues. Yet, GAIL requires meticulous calibration, typical\nof most GAN-oriented strategies. Its lack of a concrete reward function can render training intricate\nsince the agent’s primary objective becomes duping the discriminator rather than honing in on the\ngenuine task at hand. In addition, expert strategies are also used to restore unknown reward functions\nNg et al. (2000); Arora & Doshi (2021). Abbeel & Ng (2004) proposed Apprenticeship Learning,\nwhose algorithm terminates in a small number of iterations, and even though it may not be able\nto completely restore the expert’s reward function, the policy output of the algorithm will achieve\nperformance close to that of the expert. Expert-based agent training methods often have difficulty\nin obtaining agents that are better than experts. When the experts’ strategies are not perfect, it is\ndifficult to obtain satisfactory agents.\nYe et al. (2020b) uses data deletion and other methods to\nobtain better agents, but this method is costly in terms of data prepossessing.\nIn the broad arena of Deep Learning (DL), adapters have gained prominence. An “adapter” de-\nnotes a succinct module tailored to fine-tune a pre-trained neural network. Originally proposed for\ncomputer vision models Rebuffi et al. (2017) and later extrapolated to NLP Houlsby et al. (2019),\nsubsequent advancements include AdapterFusion Pfeiffer et al. (2021), which proposes amalgamat-\ning adapter parameters for multi-task knowledge consolidation; AdapterDrop R¨uckl´e et al. (2021),\nnotable for its innovative pruning mechanism; Compacter Karimi Mahabadi et al. (2021), which\nrefines an adapter structure for superior performance with minimal parameter addition; and various\nother applications like MAD-X Pfeiffer et al. (2020) for modular knowledge storage, and research\ninto Vision Transformers Marouf et al. (2022) and vision-and-language tasks Sung et al. (2022). A\ncommon adaptation method uses a serial structure, that is, inserting a low-rank feedforward neural\nnetwork into the pre-trained model. Hu et al. (2021) proposed a parallel structure that adds a bypass\nmodule to the model. This method will not affect the computational efficiency of the original base\nlarge model, and the trained modules can be directly merged into the large model parameters during\ninference. And this method is also widely used in the field of image generation.\nDespite these advancements, the potential of adapters in the domain of reinforcement learning re-\nmains largely untapped. Using an adapter to fine-tune the demonstrator may be a feasible method.\nThe adapter allow neural networks to pivot to new tasks without extensively retraining the entire\nmodel. The advantages of adapters are manifold. They are known for their parameter efficiency,\nnecessitating only a subset of parameters, which accelerates training, conserves memory, and miti-\ngates overfitting risks, especially with smaller datasets. This efficiency is also conducive for model\nstorage and dissemination. Notably, adapters ensure the preservation of the original model’s param-\neters, addressing the persistent issue of “forgetting” in continuous learning and guaranteeing that\nfoundational knowledge remains untouched. Furthermore, adapters optimize multi-task learning,\nenabling the simultaneous learning of multiple tasks with minimal parameters. This is a departure\nfrom conventional multi-task learning, and while it might slightly limit the mutual advantages drawn\nfrom diverse tasks, it reduces task interference.\nThe weak generalization and catastrophic forgetting of reinforcement learning are difficult to ignore\nin some complex problems. Even if the current Go AI Silver et al. (2016; 2017); Wu (2019) can\neasily defeat the top human players under normal circumstances, in some cases it will still make\nlow-level mistakes that are difficult for humans to understand and lose Wang et al. (2022). RTS\ngames are a complex task for reinforcement learning, and contain multiple different maps, which\nmakes it difficult for the agent to perform well on every map, especially maps that are not in its\ntraining set. MicroRTS Ontan´on (2013) is a simplified RTS game with a long history of running\n2\npre-print\nFigure 1: ADAPTER-RL architecture. A base agent receives current state and receives output ac-\ntion a1 which is transformed to action distribution d1 through one-hot encoding. The Adapter is a\nneural network based on the Actor-Critic framework, where the policy network generates the policy\ndistribution and the value network provides the value estimation of the state. The Adapter receives\nthe state and outputs and adjusted distribution d2. d1 and d2 are added then a sample is taken to\ndetermine the final action.\ncompetitions 1 Onta˜n´on et al. (2018). Perhaps due to the requirements of multiple maps, deep\nreinforcement learning algorithms have not been used in competitions until this year. This year a\nbot 2 that used deep reinforcement learning algorithms stood out in the competition, training neural\nnetworks individually for nearly every map in the competition, which required a lot of sampling.\nHowever, the method did not result in higher scores than the previous 2021 winner.\nHuang &\nOnta˜n´on (2021) tested the generalization of the agent trained by the reinforcement learning algo-\nrithm in MicroRTS, and its experiments showed the difficulty of generalizing the agent to different\nmaps. We conducted experiments in nanoRTS to test the method under different maps and agents.\nNanoRTS is a Python-centric version of MicroRTS, a real-time strategy game simulation designed\nfor reinforcement learning research.\nOur study proposes a concise and effective adaptation strategy for reinforcement learning. We note\nthat our method can be applied to any agent, including pre-trained neural networks and rule-based\nagents. This allows human knowledge to be applied to the model, thereby reducing the sampling\nand training time. Experimentally, we demonstrate our proposed method achieves high training\nefficiency and stability. The main contributions of this work are:\n• We propose Adapater-RL, a novel method that combines reinforcement learning and adap-\ntation to adapt any agent using reinforcement learning.\n• Our experimental results demonstrate that the adapter can adapt the base-agent to new tasks\nmore effectively.\n• This paper also studies the trade-off of the temperature coefficient in our method.\n2\nMETHOD\nWe propose an adaptation strategy for reinforcement learning. This strategy has similar characteris-\ntics to other supervised learning adapter methods: it allows the model to be trained individually for\ntasks, without the need to train simultaneously on all tasks to avoid catastrophic forgetting. And it\n1https://sites.google.com/site/micrortsaicompetition/competition-results\n2https://github.com/sgoodfriend/rl-algo-impls/blob/main/rl algo impls/microrts/technical-description.md\n3\npre-print\nonly requires a small number of additional parameters to adapt to each new task. Furthermore, it is\nflexible in that it can be used to fine-tune any agent, not just neural network-based agents.\nThe model we propose is structured around a modular framework as Figure 1 shows, which com-\nprises two branches, with key components consisting of a “base agent” and the “adapter”. The base\nagent’s role is foundational. It acts as the primary decision-making entity of the system, providing\ninitial predictions or actions based on its training and inherent capabilities. The base agent is similar\nto the pre-trained model in other adaptation methods, except that it can be any agent. The adapter\nacts as a supplementary module to the base agent. Its primary role is to refine and adjust the de-\ncisions made by the base agent to ensure they are well-suited to specific tasks. Instead of directly\nintervening in the internal workings of the base agent, the adapter functions more as a “side branch”.\nOnce the base agent delivers its action distribution (a set of potential actions and their corresponding\nprobabilities), the adapter steps in to generate an adjustment distribution. This adjustment distribu-\ntion essentially represents modifications or fine-tuning to the original action set proposed by the base\nagent. By combining the base agent’s action distribution with the adapter’s adjustment distribution,\nthe system can produce a modified action set. This resultant action set is more closely aligned with\nthe requirements of the specific task at hand, ensuring better performance and adaptability.\n2.1\nPROXIMAL POLICY OPTIMIZATION ALGORITHMS\nWe use proximal policy optimization algorithm (PPO) Schulman et al. (2017) to train the adapter.\nPPO is a type of Reinforcement Learning algorithm that has been widely adopted because of its\neffectiveness and stability. Unlike traditional policy gradient methods that adjust the policy in large\nsteps, PPO takes controlled steps to update the policy, ensuring that the new policy is not too different\nfrom the old one. In policy optimization, we want to maximize the expected cumulative reward. This\nis done by adjusting the policy parameters in the direction that increases the likelihood of taking\nactions that lead to higher returns. However, making large policy updates can lead to sub-optimal\npolicies or even make the training unstable. PPO limits the change in the policy in each update,\nensuring the new policy is “proximal” to the old one. This is achieved by adding a constraint to\nthe optimization problem or, equivalently, by adding a penalty to the objective function. Complex\nproblems such as Berner et al. (2019)and Ye et al. (2020a) verify the effectiveness of this method.\nEquation 1 is the policy gradient objective of PPO:\nLCLIP\nθ\n= ˆEt\nh\nmin(ρt(θ) ˆAt, clip(ρt(θ), 1 −ϵ, 1 + ϵ) ˆAt)\ni\n(1)\nwhere ρt(θ) =\nπθ(at|st)\nπθold(at|st), π is a stochastic policy, s is state, a is action, θold is is the vector of\npolicy parameters before the update, ˆAt is the generalized advantage estimation (GAE) Schulman\net al. (2015) of the action at time t, ϵ is a hyperparameter that defines the range in which the policy\nupdate is allowed.\nAt the same time, in our implementation, the value estimator’s objective is as Equation 2, where\nwhere V π\nθt−1 are estimates given by the last value function, and ˆ\nAt is the GAE of the policy\nLV F\nθ\n= ˆEt\nh\n(V π\nθt(st) −(V π\nθt−1 + ˆAt))2i\n(2)\n2.2\nTRANSFORMING DETERMINISTIC ACTION TO ACTION DISTRIBUTION\nIn our proposed structure, the base agent will output a deterministic action for the current state.\nIn order to adjust it with the adjustment distribution output by the adapter, we must convert this\ndeterministic action into a distribution. When converting determined actions of an agent into action\ndistributions for discrete actions, the goal is often to create a soft policy (a distribution over actions)\nfrom hard demonstrations (specific actions). This could be useful for training agents in a way that\nallows for some exploration or smoothing out agents that might have noise.\nIn our experiments, the environment, nanoRTS, has a discrete action space. There are varied meth-\nods to transform deterministic actions into more probabilistic action distributions. One such tech-\nnique is the One-Hot Encoding with Temperature-Scaled Softmax. In this method, the action is\nrepresented as a one-hot encoded vector, which is then passed through a temperature-scaled soft-\nmax operation. The temperature parameter is designed to modify the sharpness of the distribution.\n4\npre-print\nAnother method is Mixing with a Prior, where the deterministic one-hot encoded action is com-\nbined with a predetermined distribution, such as a uniform one. A coefficient dictates the balance\nbetween the deterministic agent action and this prior distribution, giving flexibility in determining\nthe resultant mixed distribution. The Additive Noise with Softmax technique is another approach\nwherein noise is intentionally introduced to the one-hot encoded action. Subsequent to this noise\nintroduction, a softmax operation renders a valid probability distribution across the possible actions.\nLastly, in scenarios where multiple agents are present, the behavioral mixture of agents approach,\nfor example Vinyals et al. (2019) samples the final agent from the Nash distribution of the set of\nagents, can be utilized. Given that different agents, or experts, may recommend varying actions for\nan identical state, this results in an intrinsic stochastic policy, taking advantage of the diversity in\nagent decisions. If the state space is continuous, a common approach is to transform the actions into\na normal or beta distribution.\nWe apply one-hot encoding with temperature-scaled softmax. A discrete action space can be rep-\nresented as a one-hot encoded vector, For instance, if action 2 out of 5 is chosen, its one-hot repre-\nsentation is [0, 1, 0, 0, 0], the scale the one-hot vector to [0, 1/τ, 0, 0, 0]. The higher the temperature\ncoefficient τ, the more spread out the distribution becomes, while a lower temperature coefficient\nnudges the distribution closer to a deterministic action. The final distribution obtained by mixing\nthe base agent and adapter is shown in Equation 3, where abase\ni\nis the value of the i-th action in the\nbase agent action distribution, aadj\ni\nis the value of the i-th action in the adjustment distribution.\np(ai) =\nexp(abase\ni\n/τ + aadj\ni\n)\nP\nj\nexp(abase\nj\n/τ + aadj\nj\n)\n(3)\nIf the base-agent outputs continuous actions, take the normal distribution as an example, the tem-\nperature coefficient τ is the standard deviation σ in the normal distribution formula, as Equation 4,\nwhere f(a) is probability density of action a, abase is base-agent action, aadj is adapter agent action.\nf(a) = e(abase−µ)2/(2σ)2\nσ\n√\n2π\n+ aadj\n(4)\n2.3\nTRAINING THE ADAPTER\nWe use PPO to optimize the adapter, which is a actor-critic paradigm Konda & Tsitsiklis (1999). It\ntrains a policy to give action distribution under state and a value function to estimate state.\nThe algorithm is shown in Algorithm 1. The method starts by obtaining the state representation,\ns, from the environment. Once acquired, the method leverages the base agent to produce either\na deterministic action or an action distribution for the given state. In cases where the action is\ndeterministic, it is essential to convert it into a soft action distribution, potentially using previously\nmentioned techniques such as temperature-softmax.\nNext, the method uses the state as an input to the adapter. Ideally, the adapter will output an adjust-\nment distribution over actions. By taking into account the combined action distribution—derived\nfrom both the base agent and the adapter’s outputs—one can effectively interact with the environ-\nment. This interaction will facilitate the collection of trajectories including states, actions, rewards,\nand subsequent states.\nTo further refine the process, we compute advantages using the gathered rewards and value estimates.\nThese advantages play a pivotal role as they assist in determining the efficacy of the taken action\nin relation to the average action for that particular state. Once getting these insights, we proceed to\ncalculate the PPO objective for updates. The primary goal here is to maximize the PPO objective\nusing gradient ascent. Doing so will amend the parameters of the adapter, ensuring its outputs are\naligned with more desirable rewards.\nAdditionally, we note that alongside the policy update, PPO also updates a value network. This\nnetwork is vital for estimating the anticipated returns. Typically, updates are executed by minimizing\nthe mean squared error, which is determined by measuring the difference between predicted values\nand actual returns.\n5\npre-print\nAlgorithm 1 Adaptation training with PPO\nInput: Iterations N, Sample length T, Initialized policy parameters θ\nfor iteration=1, 2, . . ., N do\nfor st in s1, s2, ..., sT do\nGet temperature-scaled one-hot encoded action from base agent\nGet adjustment distribution from adapter\nGet action distribution p(ai) =\nexp(a1\ni /τ+a2\ni )\nP exp(a1\nj/τ+a2\nj) and sample to get action at\nInteract with the environment, get st+1, rt, dt\nend for\nGet trajectories (s1, a1, r1, d1..., sT , aT , rT , dT )\nCompute advantage estimates ˆA1, ..., ˆAT\nfor epoch K do\nCompute loss L, where ρt(θ) =\nπθ(at|st)\nπθold(at|st)\nOptimize adapter with L wrt θ, with minibatch size\nend for\nθ ←θold\nend for\n3\nEXPERIMENTS\nWe conducted experiments in a context defined by expansive state and action spaces coupled with\nsparse rewards. MicroRTS, as described in Ontan´on (2013), is a streamlined version of an RTS\ngame created in Java, which comes with a Python interface named Gym-MicroRTS Huang et al.\n(2021). We focused our experimental efforts on nanoRTS, a Python-oriented version of MicroRTS.\nNanoRTS, compared to its predecessors, is more intuitive for Python experts and provides enhanced\nadaptability for bespoke modifications aligned with research objectives. Being tailored specifically\nfor Python-based reinforcement learning, nanoRTS seamlessly aligns with deep learning techniques\n3.\nOur foremost aim was to evaluate how effectively our adapter method supports agents in adjusting\nto different tasks. To offer a clear perspective, we compared the outcomes of our adapter-enhanced\nmethod with agents that are solely trained using neural networks on a variety of maps, which sym-\nbolize different tasks.\nTwo foundational agents were employed for the comparison: one rooted in a rule-based AI frame-\nwork, and the other built upon a neural network architecture. The adapter’s architecture consists\nof a two-layer convolutional neural network (CNN) which feeds into a fully connected multi-layer\nperceptron (MLP) boasting three layers, each containing 512 units. Proximal Policy Optimization\n(PPO) was chosen as our training algorithm. We adhered to established best practices for setting\nhyperparameters: a discount factor (γ) of 0.99, Generalized Advantage Estimation (λ for the GAE)\nparameter set at 0.95, a PPO clipping coefficient of 0.2, and a value coefficient valued at 1. For opti-\nmization, we employed the Adam optimizer with a learning rate of 2.5e-4. To ensure the robustness\nof our findings and account for potential variance, each experimental setup was executed thrice, with\neach iteration initiated with a random seed.\n3.1\nADAPTER WITH RULE-BASED AGENT\nIn order to examine the effectiveness of our method, we train an agent using an adapter and an agent\nusing only neural networks on different tasks. Initial states of different maps are illustrated in Figure\n2. The winning rate against the opponents is used in the game as the main metric and the training\ncurve is also of key importance. At the same time, it is also necessary to refer to whether the trained\nadapter agent can exceed the base-agent.\nIn our trials, we utilize a rule-based AI, grounded in the path-finding algorithm, as the foundational\nagent for our method. Historically, this algorithm showcased impressive results in past MicroRTS\n3Considering the anonymity we will make the code public after the reviewer process\n6\npre-print\n(a) basesWorkers16x16\n(b) noresources\n(c) TwoBasesBarracks\n(d) basesWorkers24x24\n(e) basesWorkers24x24L\n(f) DoubleGame24x24\n(g) basesWorkers12x12\n(h) FourBasesWorkers\nFigure 2: Initial state of different maps of nanoRTS.\n(a) basesWorkers16x16\n(b) noresources\n(c) TwoBasesBarracks\n(d) basesWorkers24x24\n(e) basesWorkers24x24L\n(f) DoubleGame24x24\n(g) basesWorkers12x12\n(h) FourBasesWorkers\nFigure 3: Training curves with different maps. In each figure, the orange curve is using the adapter,\nthe blue curve is using only the neural network, and the green curve is the winning rate using only\nthe base-agent.\ntournaments, clinching victories in both the 2020 and 2021 editions. The agent, in essence, prior-\nitizes targets for each unit under its control, subsequently employing a pathfinding algorithm for\nunit deployment. Concurrently, this AI was leveraged as the adversary in our tests, resulting in a\nnear-even win rate of approximately 0.5 against the opponent agent. For comparison, we introduced\na control group: an agent embedded with a convolutional neural network containing two residual\nblocks, dependent solely on the neural network. This model underwent training over 500 iterations,\nwith each iteration incorporating 8192 samples.\nFigure 3 illustrates the training trajectories across diverse maps for both the rule-based agent and\nthe adapter strategies versus the neural network-centric method. In the majority of our experimental\ntasks, the adapter drastically hastened the training process and consistently outperformed the base-\nagent. In contrast, agent training using only neural networks often requires long exploration times\nuntil the winning rate starts to increase. However, in scenarios with limited state and action spaces,\n7\npre-print\nFigure 4: Winning rate after training in different maps with different temperature coefficient\nexemplified in Figures 3(g) and 3(h), the adapter’s benefits weren’t as pronounced. For straightfor-\nward tasks, relying solely on neural networks proved sufficient to rapidly develop a high-performing\nagent, negating the necessity for an adapter. Conversely, in intricate challenges, the utility of the\nadapter becomes clear.\n3.2\nTEMPERATURE COEFFICIENT TRADE-OFF\nThe temperature coefficient within our approach regulates the entropy of the action distribution\ngenerated by the base-agent. A diminished coefficient drives the action distribution toward a more\ndeterministic result, whereas an augmented coefficient diversifies the distribution. We next explore\nthe impact of temperature coefficients on adapter training in different maps. The experiment settings\nare the same as the experiment in the previous section.\nThe result of various tasks are depicted in Figure 4. With heightened temperature coefficients, the\ndistribution derived from the base-agent resembles a uniform distribution. This challenges the dis-\ntribution’s capability to capture the underlying strategy of the base-agent. However, in this case,\nthe results of our experiment are still better than training using only a new initial neural network.\nThis shows to some extent that this method is also effective under a less directive base-agent policy.\nConsequently, training an adapter under these conditions is analogous to training an agent relying\nsolely on neural networks. Conversely, a reduced temperature coefficient results in the agent’s dis-\ntribution mirroring closely the strategy of the base-agent. This can suppress the agent’s explorative\ntendencies and increase the likelihood of the adapter settling into a local optimum during its train-\ning phase. Based on our experimental data, our approach remains relatively stable against changes\nin the temperature coefficient. There exists a broad margin within which an optimal-performing\nadapter can be achieved. In the majority of our experimental tasks, efficient training was realized\nwith temperature coefficients ranging from 1/1000 to 1/10.\n4\nCONCLUSION AND DISCUSSION\nAdaptation is already a mature method in computer vision and natural language processing.\nHowever, this method has received limited attention in reinforcement learning. We propose an\nADAPTER-RL architecture to quickly improve the performance of existing agents in different tasks.\nThis structure can be combined with the intelligence of any adaptation task and some expert meth-\nods can be applied. We verified the effectiveness of this method in nanoRTS. This method has a\nparameter temperature coefficient used to adjust the influence of the base-agent. As our experi-\nments show, choosing an appropriate intermediate value of the temperature coefficient in the range\n[1/1000, 1/10] usually results in good performance. In reinforcement learning, the strategy of the\nagent during training often affects exploration and thus the final performance of the agent. A good\nstrategy during training takes into account obtaining more rewards and exploring more state space.\nThe effectiveness of our method depends to a certain extent on the strategy of the base-agent, and\nthe base-agent and adapter complement each other. How to use an adapter to improve a base-agent\n8\npre-print\nwith poor strategies may be a future research direction. We expect this approach to be used in more\ncomplex practical applications, such as making a base AI specialized for each character it is used in\na Multiplayer Online Battle Arena game.\nREFERENCES\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In\nProceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.\nSaurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, meth-\nods and progress. Artificial Intelligence, 297:103500, 2021.\nMichael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence\n15, pp. 103–129, 1995.\nAnkur Bapna and Orhan Firat. Simple, scalable adaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Associ-\nation for Computational Linguistics, 2019.\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys Law Debiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale\ndeep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\ninformation processing systems, 29, 2016.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021.\nHuang and Onta˜n´on. Measuring generalization of deep reinforcement learning applied to real-time\nstrategy games. Association for the Advancement of Artificial Intelligence 2021 Workshop on\nReinforcement Learning in Games, 2021.\nShengyi Huang, Santiago Onta˜n´on, Chris Bamford, and Lukasz Grela. Gym-µrts: Toward affordable\nfull game real-time strategy games research with deep reinforcement learning. In 2021 IEEE\nConference on Games (CoG), pp. 1–8. IEEE, 2021.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–\n1035, 2021.\nVijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing\nsystems, 12, 1999.\nImad Eddine Marouf, Enzo Tartaglione, and St´ephane Lathuili`ere. Tiny adapters for vision trans-\nformers. preprint, 2022.\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, vol-\nume 1, pp. 2, 2000.\nSantiago Ontan´on. The combinatorial multi-armed bandit problem and its application to real-time\nstrategy games. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive\nDigital Entertainment, pp. 58–64, 2013.\n9\npre-print\nSantiago Onta˜n´on, Nicolas A Barriga, Cleyton R Silva, Rubens O Moraes, and Levi HS Lelis. The\nfirst microrts artificial intelligence competition. AI Magazine, 39(1):75–83, 2018.\nCharles Packer, Katelyn Gao, Jernej Kos, Philipp Kr¨ahenb¨uhl, Vladlen Koltun, and Dawn Song.\nAssessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebastian Ruder. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pp. 7654–7673, 2020.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nfusion: Non-destructive task composition for transfer learning. In 16th Conference of the Euro-\npean Chapter of the Associationfor Computational Linguistics, EACL 2021, pp. 487–503. Asso-\nciation for Computational Linguistics (ACL), 2021.\nDean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural\ninformation processing systems, 1, 1988.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. Advances in neural information processing systems, 30, 2017.\nSt´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In Proceedings of the fourteenth international con-\nference on artificial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference\nProceedings, 2011.\nAndreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7930–7946,\n2021.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.\nHigh-\ndimensional continuous control using generalized advantage estimation.\narXiv preprint\narXiv:1506.02438, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi\nby self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,\n2017.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for\nvision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 5227–5237, 2022.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Juny-\noung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\nTony Tong Wang, Adam Gleave, Nora Belrose, Tom Tseng, Joseph Miller, Michael D Dennis,\nYawen Duan, Viktor Pogrebniak, Sergey Levine, and Stuart Russell. Adversarial policies beat\nprofessional-level go ais. arXiv preprint arXiv:2211.00241, 2022.\nDavid J Wu. Accelerating self-play learning in go. arXiv preprint arXiv:1902.10565, 2019.\nDeheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao\nQiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning.\nAdvances in Neural Information Processing Systems, 33:621–632, 2020a.\n10\npre-print\nDeheng Ye, Guibin Chen, Peilin Zhao, Fuhao Qiu, Bo Yuan, Wen Zhang, Sheng Chen, Mingfei\nSun, Xiaoqian Li, Siqin Li, et al. Supervised learning achieves human-level performance in moba\ngames: A case study of honor of kings. IEEE Transactions on Neural Networks and Learning\nSystems, pp. 908–918, 2020b.\n11\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-11-20",
  "updated": "2023-11-20"
}