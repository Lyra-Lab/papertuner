{
  "id": "http://arxiv.org/abs/2311.11861v1",
  "title": "Generating Valid and Natural Adversarial Examples with Large Language Models",
  "authors": [
    "Zimu Wang",
    "Wei Wang",
    "Qi Chen",
    "Qiufeng Wang",
    "Anh Nguyen"
  ],
  "abstract": "Deep learning-based natural language processing (NLP) models, particularly\npre-trained language models (PLMs), have been revealed to be vulnerable to\nadversarial attacks. However, the adversarial examples generated by many\nmainstream word-level adversarial attack models are neither valid nor natural,\nleading to the loss of semantic maintenance, grammaticality, and human\nimperceptibility. Based on the exceptional capacity of language understanding\nand generation of large language models (LLMs), we propose LLM-Attack, which\naims at generating both valid and natural adversarial examples with LLMs. The\nmethod consists of two stages: word importance ranking (which searches for the\nmost vulnerable words) and word synonym replacement (which substitutes them\nwith their synonyms obtained from LLMs). Experimental results on the Movie\nReview (MR), IMDB, and Yelp Review Polarity datasets against the baseline\nadversarial attack models illustrate the effectiveness of LLM-Attack, and it\noutperforms the baselines in human and GPT-4 evaluation by a significant\nmargin. The model can generate adversarial examples that are typically valid\nand natural, with the preservation of semantic meaning, grammaticality, and\nhuman imperceptibility.",
  "text": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which\nthis version may no longer be accessible.\nGenerating Valid and Natural Adversarial Examples\nwith Large Language Models\nZimu Wang1,3, Wei Wang1∗, Qi Chen2, Qiufeng Wang1, Anh Nguyen3\n1School of Advanced Technology;\n2School of AI and Advanced Computing,\nXi’an Jiaotong-Liverpool University, Suzhou, China\n3Department of Computer Science, University of Liverpool, Liverpool, United Kingdom\nZimu.Wang19@student.xjtlu.edu.cn, {Wei.Wang03, Qi.Chen02, Qiufeng.Wang}@xjtlu.edu.cn\nAnh.Nguyen@liverpool.ac.uk\nAbstract—Deep learning-based natural language processing\n(NLP) models, particularly pre-trained language models (PLMs),\nhave been revealed to be vulnerable to adversarial attacks. How-\never, the adversarial examples generated by many mainstream\nword-level adversarial attack models are neither valid nor natu-\nral, leading to the loss of semantic maintenance, grammaticality,\nand human imperceptibility. Based on the exceptional capacity of\nlanguage understanding and generation of large language models\n(LLMs), we propose LLM-Attack, which aims at generating both\nvalid and natural adversarial examples with LLMs. The method\nconsists of two stages: word importance ranking (which searches\nfor the most vulnerable words) and word synonym replacement\n(which substitutes them with their synonyms obtained from\nLLMs). Experimental results on the Movie Review (MR), IMDB,\nand Yelp Review Polarity datasets against the baseline adversarial\nattack models illustrate the effectiveness of LLM-Attack, and it\noutperforms the baselines in human and GPT-4 evaluation by a\nsignificant margin. The model can generate adversarial examples\nthat are typically valid and natural, with the preservation of\nsemantic meaning, grammaticality, and human imperceptibility.\nKeywords—Adversarial attack, adversarial examples, text clas-\nsification, large language models, natural language processing.\nI. INTRODUCTION\nDeep learning-based natural language processing (NLP)\nmodels, particularly pre-trained language models (PLMs),\nhave achieved the state-of-the-art performance in an extensive\nlist of downstream tasks, such as sentiment analysis [1], infor-\nmation extraction [2], and text summarisation [3]. Despite the\nsuccess, numerous studies have revealed that PLMs are vul-\nnerable to adversarial attacks, i.e. minor perturbations imposed\non the original input may lead the models to make incorrect\nprediction [1], [4]. Such phenomena facilitate the evaluation\nand improvement of robustness, security, and explainability of\nNLP models [5].\nExisting research on textual adversarial attacks can be cate-\ngorised onto four levels: character-level, word-level, sentence-\nlevel, and multi-level adversarial attacks. The word-level ad-\nversarial attack is one of the most popular paradigms, consist-\ning of two stages: ranking tokens importance and replacing\nthem with heuristic rules [6], as shown in Fig. 1. As suggested\nin recent studies [7], [8], effective adversarial examples should\nbe valid and natural, with preservation of the following prop-\nerties: 1) Human prediction consistency: maintaining human\n∗Corresponding author.\nIf you can push on through the slow spots, you’ll \nbe rewarded with some fine acting.\n… be rewarded with some fine acting.\nStep 1: Word Importance Ranking\n… be advantaged with some fine acting.\nStep 2: Word Synonym Replacement\nFig. 1.\nOverall procedure of generating word-level adversarial examples,\nconsisting of two stages: 1) word importance ranking and 2) word synonym\nreplacement.\nperception while changing the models’ prediction; 2) Semantic\nsimilarity: preserving semantic meaning of the original input;\nand 3) Language fluency: ensuring grammatical correctness of\nthe generated adversarial examples.\nHowever, recent studies have revealed that many word-level\nadversarial examples do not meet the aforementioned require-\nments [8], [9], e.g. semantic meaning cannot be preserved,\nand grammatical errors are made during word substitution\nusing WordNet [10] and BERT [11]. As shown in Fig. 2,\nthe current word-level adversarial attack research faces the\nfollowing unique challenges.\n• Semantic change – With only a small fraction of true\nsynonyms in the selected candidate list, a word has a high\nlikelihood of being altered to its antonyms or unrelated\nwords [9].\n• Named entity split – Many approaches rely heavily on\nword importance during the substitution process, where\nthe change of words in named entities can dramatically\nchange the original text meanings and make the adver-\nsarial examples perceptible.\n• Grammatical errors – Synonym-based word substitution\noften ignores the word senses, tenses, and possessive\ncases, while substitution of pronouns tends to replace\nthem with other pronouns or their possessive forms,\nwhich could result in grammatical errors and human\nperceptibility.\n#\nOriginal & Adversarial Examples\n1\nRita Hayworth is just stunning at times and, for me, the only reason to watch this silly film.\nRita Hayworth is just stunning at length and, for me, the only reason to watch this wonderful film.\n2\nSeeing the title of this movie “Stupid Teenagers Must Die” made me believe this was a spoof of some kind.\nSeeing the title of this movie “Stupid Teenagers I Died” had me believe this was a spoof of some work.\n3\nI love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood.\nMe enjoys sci-fi and am hard to put up with a lot. Sci-fi movies/TV are usually ridiculous, under-appreciated and misunderstood.\nFig. 2.\nAdversarial examples generated by the BAE adversarial attack model on the IMDB dataset. (Blue: selected words to replace, Green: valid word\nreplacements, Red: antonyms, Violet: split named entities, Orange: grammatical errors, Gray: other unrelated replacements.)\nLarge language models (LLMs), such as ChatGPT1 and\nChatGLM [12], have demonstrated stunning capabilities in\nsemantic understanding, text generation, and grammatical error\ncorrection [13], [14]. Such capabilities would assist in the\ngeneration of effective word-level adversarial examples. To\naddress the problems mentioned above, we propose LLM-\nATTACK, aiming at generating both valid and natural adversar-\nial examples with LLMs. Following the mainstream paradigm\nof word-level adversarial attack, as shown in Fig. 1, the\noverall process of LLM-ATTACK consists of two stages: word\nimportance ranking which searches for the most vulnerable\nwords, and word synonym replacement which substitutes them\nwith synonyms obtained from LLMs. With the exceptional\ncapacity of language understanding and generation of LLMs,\nthe synonym lists obtained are typically valid and natural, with\nthe preservation of both semantic meaning and grammatical\ncorrectness. Experimental results on the Movie Review (MR)\n[15], IMDB [16], and Yelp Review Polarity [17] datasets\ndemonstrate the effectiveness of LLM-ATTACK against the\nbaselines such as TEXTFOOLER [7] and BAE [18]. It also out-\nperforms the baselines in human and GPT-4 [19] evaluation by\na significant margin, indicating a higher level of effectiveness,\nvalidity, and naturalness of the generated adversarial examples.\nThe contributions of our work are summarised as follows:\n• We propose LLM-ATTACK, which aims to generate both\nvalid and natural adversarial examples with LLMs. To\nthe best of our knowledge, we are the first to incorporate\nLLMs in textual adversarial attack research.\n• We evaluate the generated adversarial examples under\nthree settings: automatic, human, and GPT-4 evaluations.\nTo the best of our knowledge, this is the first work that\nevaluates the quality of adversarial examples using LLMs.\n• Experimental results under automatic evaluation illustrate\nthe effectiveness of LLM-ATTACK; it also outperforms\nthe baselines under human and GPT-4 evaluation results\nby a significant margin.\nII. RELATED WORK\nAdversarial attacks have been extensively studied over the\npast years with success in computer vision and speech do-\nmains, considering that deep neural networks (DNNs) are\nvulnerable to adversarial attacks. However, due to the discrete\n1https://chat.openai.com/\nnature of natural languages, it is still far from ideal to perform\nsuccessful adversarial attacks on NLP models [6].\nPrevious research on textual adversarial attacks can be\ndivided onto four levels: character-level, word-level, sentence-\nlevel, and multi-level adversarial attacks. Some off-the-shelf\ntoolkits have also been developed to generate adversarial\nexamples against real-world models [20], [21]. Different from\nthe character-level adversarial attack that manipulates the\ncharacters [22], [23] and the sentence-level adversarial attack\nthat creates new sentences with generative adversarial network\n(GAN) [24], semantically equivalent rules [25], and paraphrase\nnetworks [26], word-level adversarial attack has been the most\npopular technique due to its potency in preserving the semantic\nsimilarity and imperceptibility of the adversarial examples.\nIt usually consists of two stages: ranking importance of\ntokens and replacing them with heuristic rules [6]. In the first\nstage, the models iteratively search for the most vulnerable\nwords to identify the modification positions, e.g. saliency-\nbased ranking [6], [7], [27] and gradient-based descent al-\ngorithms [28], [29]; then the selected words are substituted\nwith some pre-defined strategies, e.g. synonym vocabularies\n[27], word embeddings [6], [7], and language models [18],\n[30]. By substituting synonyms for the most important words\nand imposing constraints during the replacements, such as\nmaximum modification rate, part-of-speech (POS) consistency,\nstop words preservation, and word embedding distance, the\ngenerated adversarial examples have shown to downgrade the\nperformance of well-trained text classifiers successfully.\nHowever, recent investigation has revealed that current\nword-based adversarial examples are neither valid nor natural,\nwith the possible loss of semantic maintenance, grammatical-\nity, and human imperceptibility [8], [9]. Different from the\nprevious works, we leverage LLMs with exceptional capacities\nfor language understanding and generation to generate both\nvalid and natural adversarial examples.\nIII. METHODOLOGY\nLLM-ATTACK leverages LLMs to generate both valid and\nnatural word-level adversarial examples and attempts to fool\nthe trained NLP models. We focus on the text classification\ntask and formulate it as follows: given a trained natural\nlanguage classifier F : X →Y that aims to correctly classify\nGiven the following sentence: “{current_text}”, find a list of 𝑘synonyms for the word “{word}” within the sentence.\nWhen replacing the word with its synonyms, the new sentence should maintain the semantic meaning, syntax, and\ngrammar of the original sentence. You should only output the synonyms of the word but not the sentences containing\nthem. You should also format your output as follows: “1. word 1\\n 2. word 2\\n 3. word 3\\n ... 𝑘. word 𝑘\\n”.\nFurthermore, you should rank your results by the semantic similarity between the original sentence and the sentence after\nreplacing the word with its synonyms.\nPrompt\nFig. 3. Prompt for obtaining the candidate synonyms of a given word using LLMs.\nthe input text x to the corresponding label ytrue with the\nmaximum posterior probability:\narg maxyi∈YP(yi|x) = ytrue,\n(1)\nwe attack the classifier by introducing imperceptible perturba-\ntions, defined as ∆x, to craft an adversarial example x′, for\nwhich F is expected to predict an incorrect label:\narg maxyi∈YP(yi|x′) ̸= ytrue.\n(2)\nWe define the adversarial example in Eq. (3):\nx′ = x + ∆x,\n||∆x||p < ϵ,\nx′ ∈X,\narg maxyi∈YP(yi|x) ̸= arg maxyi∈YP(yi|x′),\n(3)\nin which ||∆x||p denotes the p-norm distance metric (p = 0, 2,\n∞):\n||∆x||p =\np\nv\nu\nu\nt\nn\nX\ni=1\n|x′\ni −xi|p.\n(4)\nFollowing the procedure in Fig. 1, we explain the two\nstages: (1) finding the most important words within the input\nsentence and (2) replacing the words with their synonyms by\nprompting LLMs in Secs. III-A and III-B, respectively.\nA. Word Importance Ranking\nWe first rank the importance of words within the input\nsentence to find the most vulnerable ones and determine the\nword replacement order. Under the black-box scenario, only\nthe logits output by the target models, such as fine-tuned\nPLMs, can be regarded as the supervision signal. To balance\neffectiveness and efficiency of word ranking, we adopt the\nmasked language modeling (MLM) strategy to calculate the\nword importance. Specifically, let x denote the input sentence\nand x′\ni denote the sentence after replacing the i-th word wi to\n[UNK], as illustrated in Eqs. (5) and (6):\nx = [w1, · · · , wi−1, wi, wi+1, · · · , wn],\n(5)\nx′\ni = [w1, · · · , wi−1, [UNK], wi+1, · · · , wn],\n(6)\nthe word importance score for wi, denoted as Iwi, is defined\nas:\nIwi = P(ytrue|x) −P(ytrue|x′\ni).\n(7)\nAfter obtaining the word importance score for all the words,\nwe rank them according to Iwi in the descending order.\nWe only take θ of the most important words for semantic\npreservation to create a candidate word list L.\nB. Word Synonym Replacement\nAfter determining the vulnerable words to be perturbed,\nwe iteratively obtain the synonyms of the words with LLMs\nusing the prompt illustrated in Fig. 3. This process aims\nat searching for k synonyms of the vulnerable words that\ncan potentially preserve the semantic meaning, syntax, and\ngrammar of the original sentence, and ranking the output\nbased on the semantic similarity between the original sentence\nand the one after perturbation. Following the previous work\n[6], we also apply the constraints on repeat modification\nlimitation, maximum modification rate, POS consistency, stop\nword preservation, and word embedding distance. We also\nemploy the following additional constraints according to the\nresearch challenges in textual adversarial attacks.\n1) Named Entity and Pronoun Preservation: As shown in\nFig. 1, named entities (such as people, locations, and\ncompanies) are usually disregarded in prior adversarial\nattacks, and the alteration of named entities likely results\nin semantic changes, grammatical errors, and human\nperceptibility. For this reason, we employ spaCy2 to\nperform named entity recognition (NER) and skip the\nwords that are part of the named entities during the\nword substitution procedure. Additionally, considering\nthe fact that substitution of pronouns tends to cause\ngrammatical errors, as shown in Fig. 1, we incorporate\nthe pronouns to the stop word list to make sure they\ncannot be changed.\n2) Universal Sentence Encoder: Although previous work\nconcludes that the universal sentence encoder (USE)\n[31] is insensitive to invalid word substitutions, such as\nthe substitution to mismatched synonyms and antonyms\n[9], we employ USE to prevent LLMs from generating\ninvalid outputs, such as redundant information apart\nfrom a list of words.\nIV. EXPERIMENTS\nA. Datasets and Evaluation Metrics\nFollowing the previous works in [7], [18], we evaluated the\nperformance of LLM-ATTACK on the text classification task.\n2https://spacy.io/\nTABLE I\nSTATISTICS OF DATASETS\nDataset\n#Train\n#Test\nAvg. Len.\n#Classes\nMR [15]\n9K\n1K\n22\n2\nIMDB [16]\n25K\n25K\n280\n2\nYelp [17]\n560K\n38K\n154\n2\nThe datasets used for evaluation include both sentence-level\n(MR [15]) and document-level (IMDB [16] and Yelp Review\nPolarity [17]) sentiment analysis datasets. The overall statistics\nof the datasets are shown in Table I. We randomly selected 500\nsamples from the test set of each dataset, and reported both\naccuracy and attack success rate for each adversarial attack\nmodel.\nB. Baselines\nWe compared LLM-ATTACK with two state-of-the-art ad-\nversarial attack models: 1) TEXTFOOLER [7] is a simple but\neffective baseline that identifies the keywords by calculating\nthe prediction change after iteratively deleting the words and\nselects the synonyms using the counter-fitted word embed-\ndings. 2) BAE [18] determines the replaced words with the\nBERT-MLM approach after finding the vulnerable words with\nthe deletion strategy. We selected the replacement strategy\nof BAE due to its high sentiment accuracy and naturalness\ncompared with the rest of the strategies. To guarantee fairness\nin the comparison study, we reported the baseline results under\ntwo settings: (1) the initial setting proposed by the authors, and\n(2) the setting proposed in this work, i.e. by applying the same\nconstraints with LLM-ATTACK.\nC. Experimental Setup\nFollowing the previous works [6], [7], we conducted the\nmain experiments on the standard BERT (Bidirectional En-\ncoder Representations from Transformers) model [11]. All\nvictim models were pre-trained from TextAttack [20],\nand we adopted TextAttack to implement LLM-ATTACK’s\nadversarial attack process. We set the maximum modification\nrate θ as 0.4, the maximum word embedding distance as\n0.5, and the USE threshold as 0.9. Furthermore, we used\nChatGLM-Pro [12] as the LLM, and set the temperature as\n0 to guarantee relatively stable outputs from the model. We\nset the number of synonyms obtained from the LLM as 15\nafter balancing the effectiveness and efficiency of using grid\nsearch, as shown in Fig. 4. All the experiments were conducted\non a single NVIDIA GeForce RTX 2080 Ti graphics card.\nD. Automatic Evaluation Results\nWe first evaluated the performance of LLM-ATTACK under\nthe automatic evaluation setting using accuracy and attack suc-\ncess rate. As shown in Table II, although there is a gap in the\nevaluation results of LLM-ATTACK against baselines, it is still\nconsidered as an effective attack model that achieves success\nrates of 25.5%, 53.7%, and 42.3% on the MR, IMDB, and\nYelp Review Polarity datasets, respectively. Consistent with\n5\n10\n15\n20\nK\n30\n35\n40\n45\n50\n55\n60\n65\nValue\n64.0\n33.3\n46.0\n52.1\n40.0\n58.3\n38.0\n60.4\nAccuracy\nSuccess%\nFig. 4. Accuracy and attack success rates of the adversarial examples over\n50 samples in the IMDB dataset with different k values.\nthe findings reported in the previous work [6], LLM-ATTACK\nis potentially more effective on document-level datasets since\nthe victim models are more likely to rely on surface clues in\nlong documents rather than comprehending them when making\npredictions.\nThe original TEXTFOOLER and BAE adversarial attack\nmodels performed effectively on the automatic evaluation\nmetrics; however, after applying the same constraints from\nLLM-ATTACK, we observed dramatic performance drops in\nthe baseline models, especially BAE. This suggests that the\noriginal constraints are inefficient in restricting the generated\nadversarial examples. Due to the word embedding distance\nbeing initially taken into account while generating adversarial\nexamples, the decrease in automatic evaluation metrics for\nTEXTFOOLER is less significant than for BAE. LLM-ATTACK\noutperformed BAE by a significant margin after applying the\nadditional constraints, indicating that LLMs are more efficient\nthan BERT-MLM in generating adversarial examples.\nE. Human and GPT-4 Evaluation Results\nIn human evaluation, we selected all the adversarial ex-\namples that successfully attack the BERT model using\nTEXTFOOLER, BAE, and LLM-ATTACK, in which we applied\nall the proposed constraints. Following the criteria proposed\nin [8], we hired two evaluators to assess the validity and nat-\nuralness of the adversarial examples in five aspects: validity,\nsuspiciousness, detectability, grammaticality, and meaning: 1)\nValidity (Val.): the percentage of human judgments consistent\nwith the ground-truth labels; 2) Suspiciousness (Sus.): the\nproportion of human judgments that the samples are computer-\naltered; 3) Detectability (D.)/Grammaticality (G.)/Meaning\n(M.): the percentage of the adversarial examples with the least\ndetectability, highest grammaticality and understandability.\nAdditionally, based on the previous work [32] that utilised\nGPT-4 [19] to evaluate the output of natural language gen-\neration, we also adopted GPT-4 to evaluate the detectability,\ngrammaticality, and meaning of the adversarial examples.\nTable III illustrates the human evaluation results on the\nvalidity and suspiciousness of the adversarial examples. As\nTABLE II\nAUTOMATIC EVALUATION RESULTS OF LLM-ATTACK AGAINST BASELINES\nModel\nMR\nIMDB\nYelp\nBERT [11]\n87.8\n92.4\n97.0\nAccuracy↓\nSuccess%↑\nAccuracy↓\nSuccess%↑\nAccuracy↓\nSuccess%↑\nTEXTFOOLER [7]\n9.0\n89.8\n1.2\n98.7\n5.0\n94.9\nw/ Constraints\n60.2\n31.4\n22.2\n76.0\n32.8\n66.2\nBAE [18]\n35.8\n59.2\n30.0\n67.5\n40.0\n58.8\nw/ Constraints\n73.6\n16.2\n58.8\n36.4\n76.2\n21.4\nLLM-ATTACK\n65.4\n25.5\n42.8\n53.7\n56.0\n42.3\nTABLE III\nHUMAN EVALUATION RESULTS ON VALIDITY AND SUSPICIOUSNESS.\nVAL. STANDS FOR VALIDITY AND SUS. FOR SUSPICIOUSNESS.\nModel\nVal.↑\nSus.↓\nOriginal\n90.2%\n−\nTEXTFOOLER (w/ C.)\n78.3%\n56.5%\nBAE (w/ C.)\n77.2%\n39.1%\nLLM-ATTACK\n89.1%\n13.0%\nfor the validity of the adversarial examples, we regarded the\noriginal examples as the baseline. It is evident that the adver-\nsarial examples generated by LLM-ATTACK matched better\nwith human perception than those generated by TEXTFOOLER\nand BAE. At the same time, the adversarial examples gen-\nerated by LLM-ATTACK had the lowest likelihood of being\nrecognised as computer-altered, which was even less than half\nof the baselines. In contrast, more than half of the adversar-\nial examples generated by TEXTFOOLER were identified as\ncomputer-altered, indicating that those adversarial examples\nwere unnatural and easily perceptible by humans.\nWe visualised the evaluation results of detectability, gram-\nmaticality, and meaning in Fig. 5. The sum of the scores on\ndifferent adversarial attack models can be greater than 1 since\nsome adversarial examples generated by different models are\nthe same. LLM-ATTACK generated the most natural adver-\nsarial examples in comparison to baselines since more than\nhalf of the adversarial examples were chosen by both human\nevaluators and GPT-4. Consistency between humans and GPT-\n4 indicates the great potential of GPT-4 as an adversarial\nexamples evaluator. TEXTFOOLER performed better than BAE\nin validity due to the generation with word embeddings, and\nBAE accomplished better in terms of naturalness because\nBERT-MLM pre-training makes it possible to generate more\nfluent and coherent replacements. Still, there were some gaps\nbetween the validity and naturalness between the baseline\nmodels and LLM-ATTACK.\nF. Case Study\nFigure 6 displays the adversarial examples generated by\nLLM-ATTACK and baselines on a sample from the MR\ndataset. Since the attacks with TEXTFOOLER and BAE failed\nTextFooler\nBAE\nLLM-Attack\nAdversarial Attack Model\n0\n10\n20\n30\n40\n50\n60\n70\nScores\n18.5\n28.3\n69.6\n37.0\n32.6\n50.0\nHuman\nGPT-4\nFig. 5. Percentages of the adversarial examples with the lowest detectability\nand the highest grammaticality and understandability selected by human\nevaluators and GPT-4.\nwhen the same LLM-ATTACK constraints were applied, they\nwere executed using their original implementations. Overall,\nthe output of LLM-ATTACK was generally better than the\nbaseline models: it substituted the word “rewarded” with its\nsynonym “advantaged”, maintaining the semantic meaning and\ngrammatical correctness while altering BERT’s prediction. In\ncontrast, TEXTFOOLER modified the word to a ‘synonym’\nwith a different tense that resulted in grammatical errors,\nwhile BAE replaced the word “fine” with its antonym “bad”.\nWe also asked the human evaluators to verify the adversarial\nexamples in accordance with the criteria presented in Sec.\nIV-E. Both human evaluators gave the same judgment, and\nthey all selected the adversarial example generated by LLM-\nATTACK as the optimal adversarial example, indicating that\nLLM-ATTACK is capable of generating more valid and natural\nadversarial examples.\nV. CONCLUSION AND FUTURE WORK\nDue to the discrete nature of human languages, gener-\nating adversarial examples that are imperceptible to human\nbeings is a challenging task. We proposed LLM-ATTACK,\nwhich aims to generate both valid and natural adversarial\nexamples with LLMs with high degree of undetectability. Our\nexperimental results demonstrated the effectiveness of LLM-\nATTACK against the state-of-the-art baseline models such as\nModel\nText\nV.\nS.\nD./G./M.\nOriginal\nIf you can push on through the slow spots, you’ll be rewarded with some fine acting.\n−\n−\n−\nTEXTFOOLER\nIf you can push on through the slow spots, you’ll be recompense with some wondrous acting.\n✓\n×\n×\nBAE\nIf you can push on through the slow spots, you’ll be rewarded with some bad acting.\n×\n✓\n×\nLLM-ATTACK\nIf you can push on through the slow spots, you’ll be advantaged with some fine acting.\n✓\n✓\n✓\nFig. 6.\nAdversarial examples generated by LLM-ATTACK and baselines on the MR dataset, in which the valid word replacement, antonyms, and words\nthat caused grammatical errors are highlighted in blue, red, and green, respectively. V. stands for Validity; S. for Suspiciousness; and D./G./M. for\nDetectability/Grammaticality/Meaning.\nTEXTFOOLER and BAE. It also outperformed the baselines in\nhuman and GPT-4 evaluation by a significant margin. In future,\nwe will perform adversarial training with the generated adver-\nsarial examples and evaluate to what extent they could boost\nthe classification performance. In addition, we will investigate\nthe possibility of incorporating structured knowledge from\nquality knowledge bases in generating adversarial examples\nat both word and sentence levels.\nACKNOWLEDGEMENT\nWe would like to thank the human evaluators for their\nhelp in the evaluation study. This research is funded by the\nPostgraduate Research Scholarship (PGRS) at Xi’an Jiaotong-\nLiverpool University, contract number FOSA2212008, and\npartially supported by 2022 Jiangsu Science and Tech-\nnology Programme (General Programme), contract number\nBK20221260.\nREFERENCES\n[1] Z. Wang and H.-S. Gan, “Multi-level adversarial training for stock\nsentiment prediction,” in Proc. CCAI, 2023, pp. 127–134.\n[2] X. Wang, Y. Chen, N. Ding, H. Peng, Z. Wang, Y. Lin, X. Han, L. Hou,\nJ. Li, Z. Liu, P. Li, and J. Zhou, “MAVEN-ERE: A unified large-scale\ndataset for event coreference, temporal, causal, and subevent relation\nextraction,” in Proc. EMNLP, Dec. 2022, pp. 926–941.\n[3] L. Qian, H. Zhang, W. Wang, D. Liu, and X. Huang, “Neural abstractive\nsummarization: A brief survey,” in Proc. CCAI, 2023, pp. 50–58.\n[4] Q. Chen, W. Wang, K. Huang, S. De, and F. Coenen, “Multi-modal\nadversarial training for crisis-related data classification on social media,”\nin Proc. SMARTCOMP, 2020, pp. 232–237.\n[5] Y. Chen, H. Gao, G. Cui, F. Qi, L. Huang, Z. Liu, and M. Sun, “Why\nshould adversarial perturbations be imperceptible? rethink the research\nparadigm in adversarial NLP,” in Proc. EMNLP, Dec. 2022, pp. 11 222–\n11 237.\n[6] X. Fang, S. Cheng, Y. Liu, and W. Wang, “Modeling adversarial attack\non pre-trained language models as sequential decision making,” in Proc.\nACL (Findings), Jul. 2023, pp. 7322–7336.\n[7] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is bert really robust? a\nstrong baseline for natural language attack on text classification and\nentailment,” Proc. AAAI, vol. 34, no. 05, pp. 8018–8025, Apr. 2020.\n[8] S. Dyrmishi, S. Ghamizi, and M. Cordy, “How do humans perceive\nadversarial text? a reality check on the validity and naturalness of word-\nbased adversarial attacks,” in Proc. ACL, Jul. 2023, pp. 8822–8836.\n[9] C.-H. Chiang and H.-y. Lee, “Are synonym substitution attacks really\nsynonym substitution attacks?” in Proc. ACL (Findings), Jul. 2023, pp.\n1853–1878.\n[10] G. A. Miller, “Wordnet: A lexical database for english,” Commun. ACM,\nvol. 38, no. 11, p. 39–41, nov 1995.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in Proc.\nNAACL-HLT, Jun. 2019, pp. 4171–4186.\n[12] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\nW. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu,\nP. Zhang, Y. Dong, and J. Tang, “GLM-130b: An open bilingual pre-\ntrained model,” in Proc. ICLR, 2023.\n[13] H. Peng, X. Wang, J. Chen, W. Li, Y. Qi, Z. Wang, Z. Wu, K. Zeng,\nB. Xu, L. Hou, and J. Li, “When does in-context learning fall short and\nwhy? a study on specification-heavy tasks,” 2023.\n[14] H. Wu, W. Wang, Y. Wan, W. Jiao, and M. Lyu, “Chatgpt or grammarly?\nevaluating chatgpt on grammatical error correction benchmark,” 2023.\n[15] B. Pang and L. Lee, “Seeing stars: Exploiting class relationships for\nsentiment categorization with respect to rating scales,” in Proc. ACL,\nJun. 2005, pp. 115–124.\n[16] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,\n“Learning word vectors for sentiment analysis,” in Proc. ACL-HLT, Jun.\n2011, pp. 142–150.\n[17] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional net-\nworks for text classification,” in Proc. NIPS, vol. 28, 2015.\n[18] S. Garg and G. Ramakrishnan, “BAE: BERT-based adversarial examples\nfor text classification,” in Proc. EMNLP, Nov. 2020, pp. 6174–6181.\n[19] OpenAI, “Gpt-4 technical report,” 2023.\n[20] J. Morris, E. Lifland, J. Y. Yoo, J. Grigsby, D. Jin, and Y. Qi,\n“TextAttack: A framework for adversarial attacks, data augmentation,\nand adversarial training in NLP,” in Proc. EMNLP (Demo), Oct. 2020,\npp. 119–126.\n[21] G. Zeng, F. Qi, Q. Zhou, T. Zhang, Z. Ma, B. Hou, Y. Zang, Z. Liu,\nand M. Sun, “OpenAttack: An open-source textual adversarial attack\ntoolkit,” in Proc. ACL-IJCNLP (Demo), Aug. 2021, pp. 363–371.\n[22] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi, “Black-box generation of\nadversarial text sequences to evade deep learning classifiers,” in Proc.\nSPW, 2018, pp. 50–56.\n[23] S. Eger, G. G. S¸ahin, A. R¨uckl´e, J.-U. Lee, C. Schulz, M. Mesgar,\nK. Swarnkar, E. Simpson, and I. Gurevych, “Text processing like humans\ndo: Visually attacking and shielding NLP systems,” in Proc. NAACL-\nHLT, Jun. 2019, pp. 1634–1647.\n[24] Z. Zhao, D. Dua, and S. Singh, “Generating natural adversarial exam-\nples,” in Proc. ICLR, 2018.\n[25] M. T. Ribeiro, S. Singh, and C. Guestrin, “Semantically equivalent\nadversarial rules for debugging NLP models,” in Proc. ACL, Jul. 2018,\npp. 856–865.\n[26] M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer, “Adversarial\nexample generation with syntactically controlled paraphrase networks,”\nin Proc. NAACL-HLT, Jun. 2018, pp. 1875–1885.\n[27] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language\nadversarial examples through probability weighted word saliency,” in\nProc. ACL, Jul. 2019, pp. 1085–1097.\n[28] J. Li, S. Ji, T. Du, B. Li, and T. Wang, “TextBugger: Generating\nadversarial text against real-world applications,” in Proc. NDSS, 2019.\n[29] J. Y. Yoo and Y. Qi, “Towards improving adversarial training of NLP\nmodels,” in Proc. EMNLP (Findings), Nov. 2021, pp. 945–956.\n[30] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu, “BERT-ATTACK: Adver-\nsarial attack against BERT using BERT,” in Proc. EMNLP, Nov. 2020,\npp. 6193–6202.\n[31] D. Cer, Y. Yang, S. yi Kong, N. Hua, N. Limtiaco, R. S. John,\nN. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, Y.-H. Sung,\nB. Strope, and R. Kurzweil, “Universal sentence encoder,” 2018.\n[32] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, “G-eval: Nlg\nevaluation using gpt-4 with better human alignment,” 2023.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2023-11-20",
  "updated": "2023-11-20"
}