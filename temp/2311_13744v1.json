{
  "id": "http://arxiv.org/abs/2311.13744v1",
  "title": "Security and Privacy Challenges in Deep Learning Models",
  "authors": [
    "Gopichandh Golla"
  ],
  "abstract": "These days, deep learning models have achieved great success in multiple\nfields, from autonomous driving to medical diagnosis. These models have\nexpanded the abilities of artificial intelligence by offering great solutions\nto complex problems that were very difficult to solve earlier. In spite of\ntheir unseen success in various, it has been identified, through research\nconducted, that deep learning models can be subjected to various attacks that\ncompromise model security and data privacy of the Deep Neural Network models.\nDeep learning models can be subjected to various attacks at different stages of\ntheir lifecycle. During the testing phase, attackers can exploit\nvulnerabilities through different kinds of attacks such as Model Extraction\nAttacks, Model Inversion attacks, and Adversarial attacks. Model Extraction\nAttacks are aimed at reverse-engineering a trained deep learning model, with\nthe primary objective of revealing its architecture and parameters. Model\ninversion attacks aim to compromise the privacy of the data used in the Deep\nlearning model. These attacks are done to compromise the confidentiality of the\nmodel by going through the sensitive training data from the model's\npredictions. By analyzing the model's responses, attackers aim to reconstruct\nsensitive information. In this way, the model's data privacy is compromised.\nAdversarial attacks, mainly employed on computer vision models, are made to\ncorrupt models into confidently making incorrect predictions through malicious\ntesting data. These attacks subtly alter the input data, making it look normal\nbut misleading deep learning models to make incorrect decisions. Such attacks\ncan happen during both the model's evaluation and training phases. Data\nPoisoning Attacks add harmful data to the training set, disrupting the learning\nprocess and reducing the reliability of the deep learning mode.",
  "text": "arXiv:2311.13744v1  [cs.CR]  23 Nov 2023\nSecurity and Privacy Challenges in Deep Learning\nModels\nGopichandh Golla\nComputer Science and Engineering\nThe State University of New York at Buffalo\nBuffalo, USA\nggolla@buffalo.edu\nAbstract—These days, deep learning models have achieved\ngreat success in multiple ﬁelds, right from autonomous driving\nto medical diagnosis. These models have expanded the abilities\nof artiﬁcial intelligence by offering great solutions to complex\nproblems that were very difﬁcult to solve earlier. In spite of\ntheir unseen success in various, it has been identiﬁed, through\nresearch conducted, that deep learning models can be subjected to\nvarious attacks that compromise model security and data privacy\nof the Deep Neural Network models. Deep learning models can be\nsubjected to various attacks at different stages of their lifecycle.\nDuring the testing phase, attackers can exploit vulnerabilities\nthrough different kinds of attacks such as Model Extraction\nAttacks, Model Inversion attacks, and Adversarial attacks. Model\nExtraction Attacks are aimed at reverse-engineering a trained\ndeep learning model, with the primary objective of revealing\nits architecture and parameters. Model inversion attacks aim to\ncompromise the privacy of the data used in the Deep learning\nmodel. These attacks are done to compromise the conﬁdentiality\nof the model by going through the sensitive training data from\nthe model’s predictions. By analyzing the model’s responses,\nattackers aim to reconstruct sensitive information. In this way, the\nmodel’s data privacy is compromised. Adversarial attacks, mainly\nemployed on computer vision models, are made to corrupt models\ninto conﬁdently making incorrect predictions through malicious\ntesting data. These attacks introduce and change the input data in\nsuch a way that it is not recognizable as adversary data, causing\ndeep learning models to mis-classify and make wrong decisions.\nModels can be subjected to attacks not only evaluation phase but\nalso during the training phase. Data Poisoning Attacks introduce\nmalicious examples into the training dataset, which makes the\ntraining phase ineffective and leads to the loss of the integrity of\nthe deep learning model. Data privacy can be compromised by\nnot only model inversion attacks but also by service providers of\nAI models.\nThis Survey is done to gain an in-depth understanding of the\nsecurity and privacy problems of Deep neural nets. It is done\nby analyzing what kinds of attacks are present, how each type\nof attack works, and its challenges and drawbacks. This paper\nalso discusses what are the potential evolutions of these attacks\nmoving forward as well.\nIndex Terms—Adversarial Attacks Poisoning Attacks Model\nExtraction Attacks Model Inversion Attacks\nI. INTRODUCTION\nThese days, deep learning models have achieved great\nsuccess in multiple ﬁelds, right from medical diagnosis and\nnatural language processing to autonomous driving and image\nclassiﬁcation. In healthcare, Deep learning models are utilized\nin disease prediction and medical image analysis with greater\naccuracy. In ﬁnance, they are involved in fraud detection\nsystems. Computer Vision models of Deep learning help self-\ndriving cars perform in complex environments in terms of nav-\nigation. Natural Language Processing models are improving\nvirtual assistants and language translation tools. The major rea-\nson behind the success of Deep neural net models is their great\nability to learn patterns from vast datasets available. Advances\nin computing power and advanced algorithm innovation have\nalso assisted in the success of Deep neural network models.\nOn the ﬂip side, integration of these advanced models into a\nwide range of domains has come with its own set of security\nand privacy vulnerabilities that require thorough investigation\nand defense techniques.\nThe vulnerabilities of these Deep neural net models pose se-\nvere challenges and threats to the integrity, reliability, and con-\nﬁdentiality of these systems. A compromised DL model can\nlead to bad decision-making, improper working of autonomous\nsystems, or breaches of conﬁdential data. These vulnerabilities\nmay lead to ﬁnancial losses to threats to public safety[2].\nFor example, a compromised Autonomous driving system may\nlead to accidents and loss of lives. Face recognition systems\nused in biometric authentication systems can be compromised\nby introducing noise to the system thus compromising the\nIntegrity of the systems.\nThis survey is conducted to analyze and understand the\nlength and breadth of attacks on Deep Learning systems.\nAttacks are categorized into 4 types, such as Model Extraction\nAttacks, Model Inversion attacks, Adversarial attacks, and Poi-\nsoning Attacks[1]. This survey discusses different techniques\nand their challenges and drawbacks along with the potential\nevolution of the attacks in the future.\nModel Extraction and Model inversion attacks are aimed\nat compromising the privacy of the DL models and expos-\ning model parameters and training data. Model Extraction\nattack is aimed at duplicating the DL model’s parameters\nand architecture. Whereas, Model Inversion attacks are aimed\nat reproducing conﬁdential information by going through\nalready available data. Adversarial attacks( Evasion attacks)\nand poisoning attacks’ main purpose is to inﬂuence prediction\nresults either by providing falsiﬁed testing data and getting\nwrong predictions with conﬁdence or obstructing the model\ntraining during the training phase with the help of adversarial\nexamples. Evasion attacks are done during the evaluation\nphase and are further classiﬁed into white box and black box\nattacks. beforehand knowledge of the model is necessary in\norder to execute white box attacks where attackers use these\ndata to form perturbed inputs to cause undesirable outcomes.\nGradient-based techniques are the common mode of operations\nemployed to perform white-box attacks. Black box attacks\nare more common between these 2 types as this resembles\nreal-world scenarios much better where the attackers don’t\nhave prior knowledge of the models. Queries are made to\nunderstand the model’s behavior to disturb the performance of\nthe models. Here, just the model’s behavior is known, but not\nthe model’s architecture or hyperparameters. Poisoning attacks\nare done during the training phase of the model by introducing\nwrongful data into the trained dataset. These kinds of attacks\nare under-researched and are divided into 3 subcategories,\nthey are performance degradation, targeted poisoning, and\nbackdoor attacks. In consideration of the impact of adversarial\nthreats, extensive research has been dedicated to developing\ndefense mechanisms against adversarial attacks. Techniques\nsuch as adversarial training, gradient masking, and robust op-\ntimization aim to bolster DL models’ resilience to adversarial\nperturbations. Adversarial attacks are not constrained to a\nsingle domain, they can affect a wide range of domains or\narchitectures such as CNNs, RNNs, etc.[3]\nTo summarize, This Survey is done to gain an in-depth\nunderstanding of the security and privacy problems of Deep\nneural nets. It is done by analyzing what kinds of attacks are\npresent, how each type of attack works, and its challenges and\ndrawbacks. This paper also discusses what are the potential\nevolutions of these attacks moving forward as well.\nII. MAIN TECHNIQUES\nA. Model Extraction Attacks\nIn this type of attack, the attacker doesn’t know the internal\nworkings of the model and the aim is to duplicate the model.\nThe attacker queries the model with input x to get the result\ny. By performing extensive querying as mentioned before, the\nattacker gains enough information needed to reverse engineer\nthe model and be able to replicate its internal working. This\ninformation can be further used to perform subsequent attacks\nlike adversarial attacks on the model. The adversary model for\nthis attack typically operates in a black-box manner, where\nthe attackers have no information on how the model works\nand no information regarding the architecture, parameters,\nhyperparameters, and training dataset is known. The attacker\ncan only interact with the model through prediction APIs.\nMoreover, they do not have access to data that matches the\ndistribution of the target model’s training data. There are\nalso limitations on query frequency, as attackers may face\nrestrictions or be blocked by the API if they submit queries\ntoo frequently.\nModel extraction can be done in 3 different ways, such as\nEquation solving, Metamodel training, and Training substitute\nmodel. In equation solving, a typical classiﬁcation machine\nlearning model that works on continuous function can be\nsolved by having enough pairs of input and output values.\nIn the metamodel training approach(classiﬁer of classiﬁers),\nattackers query the model to gain the output for a speciﬁc\ninput. These values are used to train a metamodel to replicate\nthe original model, thus used later to predict the original\nmodel’s parameters. Training substitute model is similar to\nthe metamodel training approach where a set of input-output\nvalue pairs are trained on the substitute model and that model’s\nparameters mimic the original model’s parameters.\nB. Model Inversion Attacks\nModel Inversion attack is a kind of attack employed to\ngain information about the training data of Neural Networks\nmodels. Model Inversion attack uses a concept called inverse\ninformation ﬂow. Since neural networks can remember a\nlot of information, inverse information ﬂow can be used to\ngain insights into data properties, thus compromising the data\nprivacy of Deep learning models. Model Inversion attack can\nbe done in both white box and black box settings. Model\nInversion attack can be done in 2 different ways, such as\nMembership Inversion attack and property inversion attack.\nThe presence of speciﬁc data records in the training dataset\ncan be determined using a Membership Inversion attack,\nwhereas Statistical property inference can be achieved through\na Property Inference Attack.\n1) Membership Inference Attack :: Membership inference\nattack is a kind of Model inversion attack whose main aim\nis to determine the presence of speciﬁc data in the training\ndataset or not. This attack can be done in both black-box and\nwhite-box setups. The working of this kind of attack is done\nin 4 steps, namely, Data synthesis, Shadow model training,\nattack model training, and membership determination. In the\ndata synthesis step, data can be generated either manually\nor by training generated models. As part of shadow model\ntraining, shadow models are trained to generate training data\nfor the attack model. The attack model is trained using the data\ngenerated above to determine whether a given instance of the\ntraining dataset is part of the training dataset or not is done\nattack model training step. In the membership determination\nstep, the determination of whether the input instance is part\nof the training dataset or not.\n2) Property Inference Attack :: A property Inference Attack\nis a kind of Model Inversion Attack where the main goal of\nthe attack is to determine the characteristics of the dataset.\nA workﬂow of this kind is similar to a Membership inference\nattack but the end goal is different. Here, the dataset is divided\nbased on the presence of a certain feature.\nC. Adversarial Attacks\nThis kind of attack is also known as an evasion attack\nwhere adversarial examples are applied during the inference\nphase(testing phase), thereby causing the model to incorrectly\nclassify the input data. This attack can be done on all kinds\nof models such as image classiﬁcation, speech recognition,\ntext processing, and malware detection. Adversary examples\nused are not recognizable by humans but can deceive the\ntarget models. Adversarial attacks can also be done in both\nwhite-box and black-box settings. Adversarial attacks are\nefﬁciently developed in image classiﬁcation models. Here, the\ndistance(perturbation) between the adversarial example and the\ntrue example should be as minimal as possible. This distance\ncan be measured using Lp norms. Here the attacks can be\neither targeted or non-targeted.\n1) White-box attacks :: In this type of attack, attackers\nalready have intrinsic information about the target models. per-\nturbations required to generate adversarial examples can be ob-\ntained by either calculating gradients or solving optimization\nproblems. The ﬁrst attack that we discuss here is done using\nthe Limited-memory Broyden-Fletcher-Goldfarb-Shanno( L-\nBFGS) algorithm to generate adversarial examples[4]. Similar\nto the above attack, CW-Attack minimizes a different objective\nfunction to obtain perturbations[5]. Jacobian-based Saliency\nMap Attack ( JSMA ) ﬁnds the perturbations by calculat-\ning the saliency maps of gradients derived from forward\npropagation[6]. This saliency map is then used to ﬁnd the\nperturbations. L0 is used as a distance metric here. Fast\nGradient Sign Method ( FGSM ) : FGSM explores the linearity\nof neural net models to drive the input data towards the\ngradients to obtain needed perturbations[7]. This model can\nbe helpful in linearly related simple models. Iterative FGSM:\nUnlike the traditional FGSM model, the iterative FGSM model\ncomputes the perturbated examples step by step by applying\nthe FGSM multiple times[8]. This method is faster than the\ntraditional model with higher success rates as well.\n2) Black-box attacks :: In general, black box attacks are\ncloser to the real world as model details, such as architecture,\nparameters, hyperparameters, and gradients, are not known to\nattackers most of the time. Industrial and mobile deployed\nmodels are kept conﬁdential and are not accessible to adver-\nsaries. The following are some of the Black-box attacks.\nTransfer-based attacks[4] are developed based on the as-\nsumption of inherent linearity of the neural nets, allowing a\nsubstitute model to approximately predict the decision bound-\nary of the actual model. This substitute model’s architecture is\ndetermined based on the input format, such as images or text.\nZeroth-order optimization is a popular form of attack where\nattackers approximate the gradient using only the Model’s\nprediction API. However, these are computationally expensive\nto implement. In Limited Information attacks, Chen et al.[9]\nproposed a method that approximates the gradient of a target\nmodel using only the output scores thus minimizing the\namount of information needed. Suet et al.[10] introduced\na technique where only one pixel is changed to produce\nadversarial examples. In Predicted Label-based attacks, Only\npredicted labels are used to produce the adversary examples\ndue to limited information availability. Boundary attacks use\nrandom walks to approach the decision boundary iteratively.\nD. Poisoning Attacks\nThese kinds of attacks are employed during the training\nphase of the target model by introducing malicious examples\nto weaken the model training. These kinds of attacks are under-\nresearched to date and are classiﬁed into 3 categories, namely\nperformance degradation attacks, targeted poisoning attacks,\nand backdoor attacks.\n1) Performance degradation attack: : In this kind of attack,\nperformance degradation is achieved using malicious examples\nthat are developed using bi-level optimization.\n2) Targeted Poisoning Attack:: The main aim of this attack\nis to misclassify certain test examples during testing by\nmodifying training data based on changes to parameters and\nloss functions.\n3) Backdoor attack:: In this attack, the main aim is to\ncreate a backdoor to the model that can be exploited during\nthe testing phase. Insertion of subtle triggers like patches or\nwatermarks into training samples and forcing neural nets to\nproduce false outputs.\nIII. ISSUES AND PROBLEMS\nExtracting machine learning models using approaches like\nequation solving, training metamodels, and training substitute\nmodels contains certain challenges and drawbacks. Solving\nequations is not so complex for small models. For larger\nmodels, equations become more complex thus leading to less\npracticality of solving them. Equation solving cannot be a\ngood choice when quick extraction of models is necessary. The\nequation-solving method is limited to classiﬁcation models\nthat deal with continuous class variables. When it comes\nto training metamodels and training substitute models, large\namounts of data are required. So, querying the models to\nget relevant data can be a challenging process. In addition\nto collecting large amounts of data, the computation of sub-\nstitute and metamodel models requires considerable amounts\nof computational resources. Substitute models trained cannot\nperform well on complex target models or models that are\ntrained on diverse data.\nWith respect to Model Inversion attacks, the Shadow model\ntraining step can be a time-consuming task and require a good\namount of computational resources. One of the challenges with\nrespect to data synthesis is the step collection of training data\nthat closely replicates the training data can be really difﬁcult\nif there is no extensive knowledge about the dataset’s speciﬁcs\nis available. The effectiveness of the models developed cannot\nbe consistent across all the attacks and can be ineffective at\ntimes when the type of data is diverse or the target model is\nmore complex. One of the main drawbacks of model inversion\nattacks is the amount of resources required to train shadow\nmodels is expensive. Strongly defended target models cannot\nbe attacked with this kind of attack\nWhite box attacks contain their own set of challenges and\ndrawbacks. L-BFGS attack is computationally expensive and\nhas a high misclassiﬁcation rate. CW attack also has its\nchallenges such as tuning hyperparameters and solving the\ncomplex objective function is computationally expensive. One\nof the challenges of JSMA attack is the calculation of accurate\nsalient maps and one of its major drawbacks is its high\nreliance on the L0 norm to calculate distances between the\nadversary and true examples. Since Iterative FGSM depends\non calculating gradient values iteratively, convergence is not\nguaranteed in all scenarios and may be tiresome. Determining\nthe Optimal number of iterations required can be a challenge.\nOne of the challenges in black-box attacks is a limited\namount of information with respect to the target model is\navailable. Substitute model creation is really expensive on both\ndata collection and model training fronts. Model APIs block\nthe users when they query frequently, thus posing a challenge\nto collect the data required. Drawbacks of black-box attacks\nconsist of vulnerability to defenses mainly substitute models.\nNot all attacks are effective. Models that are developed to\nattack a target system can be effective on only that model, but\nnot on other models thus limiting the model’s transferability.\nFor poisoning attacks, developing perturbed examples can\nbe a challenging task when attackers have a limited amount\nof training data. This kind of attack can also require intensive\nresources. One of the challenging tasks in these kinds of\nattacks is the creation of good adversarial examples that look\nlike actual samples. These kinds of attacks are limited to\nmodels to which they are aiming to perturb, and cannot be\ntransferred to other target models.\nIV. FUTURE TRENDS\nDeep learning is a discipline that is always evolving, and\nthere is a never-ending battle between attackers and defenders\nover model security.\nA. Model Extraction Attacks :\nAttackers may create ”advanced query strategies” that query\nmodels to obtain information with fewer queries, which is one\npossible progression of model extraction attacks. To obtain\nmore accuracy, attackers can also combine various extraction\nmodel combinations. One evolution in model extraction attacks\nis improvement in transfer learning.\nB. Model Inversion Attacks :\nThese days, model inversion attacks on complex systems are\nnot as effective; nevertheless, in the future, with the develop-\nment of sophisticated methodologies, complicated models may\nbe inverted. As security continues to advance, attackers can\nleverage the most recent knowledge to circumvent advanced\ndata privacy strategies.\nC. Adversarial Attacks :\nOne possible development in adversarial attacks is the\nability to expand beyond attacks on a certain target model to\nadditional models. This guarantees that transfer learning will\nimprove. This development of more complicated adversarial\nattacks may target multiple different models at the same time.\nIn addition, adversaries might create algorithms that operate\nin real time and drastically reduce time overhead.\nD. Poisoining Attacks :\nIn terms of poisoning attacks, altered instances produced\nduring the model’s training phase will resemble real examples\neven more and become more difﬁcult to identify during the\ntraining phase of the models. In the future, backdoor attacks\nmay develop into more sophisticated ones that may include\nmore sophisticated trigger mechanisms and stealthy activation\nfunctions. Attackers may utilize recent advancements in the\nﬁeld of reinforcement learning and develop even better back-\ndoor triggers. Attackers may modify their poisoning tactics in\nresponse to the target model’s defense measures.\nIn summary, It’s important to note that as attacks evolve, the\nﬁeld of AI security will also evolve to develop better defenses\nand strategies to mitigate these threats.\nREFERENCES\n[1] Y. He, G. Meng, K. Chen, X. Hu and J. He, ”Towards Security Threats\nof Deep Learning Systems: A Survey” in IEEE Transactions on Software\nEngineering, vol. 48, no. 05, pp. 1743-1770, 2022\n[2] J. Li, F. Schmidt, and Z. Kolter, “Adversarial camera stickers: a physical\ncamera-based attack on deep learning systems,” in Proc. International\nConference on Machine Learning, 2019.\n[3] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal\nadversarial perturbations,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2017, pp. 86–94\n[4] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,\nand R. Fergus, “Intriguing properties of neural networks,” arXiv preprint\narXiv:1312.6199, 2013.\n[5] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural\nnetworks,” in Proc. IEEE Symposium on Security and Privacy, 2017.\n[6] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and\nA. Swami, “the limitations of deep learning in adversarial settings,” in\nProc. IEEE European Symposium on Security and Privacy, 2016.\n[7] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” arXiv preprint arXiv:1412.6572, 2014.\n[8] A. Kurakin, I. Goodfellow, S. Bengio et al., “Adversarial examples in\nthe physical world,” 2016.\n[9] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “ZOO: zeroth\norder optimization based blackbox attacks to deep neural networks\nwithout training substitute models,” in Proc. 10th ACM Workshop on\nArtiﬁcial Intelligence and Security, 2017.\n[10] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep\nneural networks,” IEEE Transactions on Evolutionary Computation, no.\n5, pp. 828–841, 2019.\n[11] Ho Baey, Jaehee Jangy, Dahuin Jung, Hyemi Jang, Heonseok Ha,\nHyungyu Lee, and Sungroh Yoon, ” Security and Privacy Issues in Deep\nLearning”, arXiv:1807.11655v4 [cs.CR] 10 Mar 2021\n",
  "categories": [
    "cs.CR",
    "cs.AI"
  ],
  "published": "2023-11-23",
  "updated": "2023-11-23"
}