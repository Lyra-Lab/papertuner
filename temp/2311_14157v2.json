{
  "id": "http://arxiv.org/abs/2311.14157v2",
  "title": "Enabling Unsupervised Discovery in Astronomical Images through Self-Supervised Representations",
  "authors": [
    "Koketso Mohale",
    "Michelle Lochner"
  ],
  "abstract": "Unsupervised learning, a branch of machine learning that can operate on\nunlabelled data, has proven to be a powerful tool for data exploration and\ndiscovery in astronomy. As large surveys and new telescopes drive a rapid\nincrease in data size and richness, these techniques offer the promise of\ndiscovering new classes of objects and of efficient sorting of data into\nsimilar types. However, unsupervised learning techniques generally require\nfeature extraction to derive simple but informative representations of images.\nIn this paper, we explore the use of self-supervised deep learning as a method\nof automated representation learning. We apply the algorithm Bootstrap Your Own\nLatent (BYOL) to Galaxy Zoo DECaLS images to obtain a lower dimensional\nrepresentation of each galaxy, known as features. We briefly validate these\nfeatures using a small supervised classification problem. We then move on to\napply an automated clustering algorithm, demonstrating that this fully\nunsupervised approach is able to successfully group together galaxies with\nsimilar morphology. The same features prove useful for anomaly detection, where\nwe use the framework astronomaly to search for merger candidates. While the\nfocus of this work is on optical images, we also explore the versatility of\nthis technique by applying the exact same approach to a small radio galaxy\ndataset. This work aims to demonstrate that applying deep representation\nlearning is key to unlocking the potential of unsupervised discovery in future\ndatasets from telescopes such as the Vera C. Rubin Observatory and the Square\nKilometre Array.",
  "text": "MNRAS 000, 1–22 (2023)\nPreprint 22 April 2024\nCompiled using MNRAS LATEX style file v3.0\nEnabling Unsupervised Discovery in Astronomical Images through\nSelf-Supervised Representations\nKoketso Mohale,1★Michelle Lochner,1,2\n1Department of Physics and Astronomy, University of the Western Cape, Bellville, Cape Town, 7535, South Africa\n2South African Radio Astronomy Observatory, 2 Fir Street, Black River Park, Observatory, 7925, South Africa\nAccepted 2024 March 26. Received 2024 March 19; in original form 2023 November 30\nABSTRACT\nUnsupervised learning, a branch of machine learning that can operate on unlabelled data, has proven to be a powerful tool for data\nexploration and discovery in astronomy. As large surveys and new telescopes drive a rapid increase in data size and richness, these\ntechniques offer the promise of discovering new classes of objects and of efficient sorting of data into similar types. However,\nunsupervised learning techniques generally require feature extraction to derive simple but informative representations of images.\nIn this paper, we explore the use of self-supervised deep learning as a method of automated representation learning. We apply\nthe algorithm Bootstrap Your Own Latent (BYOL) to Galaxy Zoo DECaLS images to obtain a lower dimensional representation\nof each galaxy, known as features. We briefly validate these features using a small supervised classification problem. We then\nmove on to apply an automated clustering algorithm, demonstrating that this fully unsupervised approach is able to successfully\ngroup together galaxies with similar morphology. The same features prove useful for anomaly detection, where we use the\nframework astronomaly to search for merger candidates. While the focus of this work is on optical images, we also explore the\nversatility of this technique by applying the exact same approach to a small radio galaxy dataset. This work aims to demonstrate\nthat applying deep representation learning is key to unlocking the potential of unsupervised discovery in future datasets from\ntelescopes such as the Vera C. Rubin Observatory and the Square Kilometre Array.\nKey words: methods: data analysis – surveys – galaxies: general\n1 INTRODUCTION\nRapid advancements in astronomical surveys have resulted in the\nproduction of volumes ofdata toolarge forhumanexperts tomanually\ninspect. Projects such as Galaxy Zoo (Lintott et al. 2008) employ the\nuse of citizen scientists to label galaxy morphology at large scales.\nExamples of applications of these morphology classifications include\nstudies of the host galaxies of active galactic nuclei (Schawinski et al.\n2009), investigations of merging galaxies (Darg et al. 2010), and the\nformation of bars in spiral galaxies (Simmons et al. 2014). However,\nthese classifications are limited in that they require hand-labelled\ndata and the rapid increase of data volumes means that eventually\neven citizen science projects will not be able to keep up. Additionally,\ncitizen scientists may not necessarily have the training required to\nmake more nuanced classifications or identify particularly anomalous\nsources.\nUnsupervised machine learning has the potential to leverage large\nunlabelled datasets, allowing for data-driven discovery and explo-\nration. These algorithms can also be used to create training sets for\ndownstream supervised applications far more quickly than random\nlabelling. There have been numerous applications of unsupervised\nlearning in astronomy, including the application of clustering and\nanomaly detection to radio galaxies (Ralph et al. 2019; Gupta et al.\n★E-mail: koketso.kjay@gmail.com\n2022), the detection of anomalous spectra in LAMOST data (Yang\net al. 2023) and the hunt for rare transients and variables in Deeper\nWider Faster optical data (Webb et al. 2020). Lochner & Bassett\n(2021) introduced the general-purpose anomaly detection framework\nastronomaly, which was later applied to data from the MeerKAT\ntelescope to discover a highly unusual radio source (Lochner et al.\n2023).\nThere is also a long history of applying unsupervised techniques to\nachieve automatic clustering of optical galaxies into similar morphol-\nogy, which include early applications of artificial neural networks to\nphotometric galaxy parameters (e.g. Lahav et al. 1996; Naim et al.\n1997; d’Abrusco et al. 2007). The motivation behind these efforts is\nnot simply to automate otherwise tedious tasks, but also to allow the\npossibility of discovering new classes of objects or a more physically\nmotivated categorisation of galaxies.\nHowever, unsupervised algorithms usually cannot work directly\nwith high dimensional data such as images (with the notable excep-\ntion of the rotation-invariant self-organising maps of Polsterer et al.\n2019) and instead require lower dimensional representations called\nfeatures. In the case of galaxy morphologies, these features could\ninclude photometric colours or shape parameters such as the Gini\ncoefficient. The choice of feature extraction method will generally\ndictate the success of the machine learning algorithm.\nDeep learning algorithms (e.g. LeCun et al. 2015), particularly\nconvolutional neural networks (CNNs), have revolutionised the field\n© 2023 The Authors\narXiv:2311.14157v2  [astro-ph.IM]  19 Apr 2024\n2\nK. Mohale & M. Lochner\nof image recognition due to their ability to obtain meaningful repre-\nsentations from images without requiring explicit feature selection.\nThis has led to their use as general-purpose feature extractors, as\nwas demonstrated in Walmsley et al. (2022) where a pretrained con-\nvolutional neural network dramatically outperformed simpler, hand-\ndesigned morphological featuresfor unsupervisedtasksonthe optical\ndataset Galaxy Zoo DECaLS (Walmsley et al. 2020a). Etsebeth et al.\n(2023) recently demonstrated the same features could be used for\nhighly effective anomaly detection among nearly 4 million galaxies\nin the larger DECaLS dataset (Dey et al. 2019).\nThe downside of using a pretrained network as a feature extractor\nis that it requires the data be relatively similar to the dataset it was\noriginally trained on in order to obtain the best performance. It also\nrequires a large, labelled dataset for training which does not exist\nfor many fields, such as high resolution radio astronomy. In a recent\npaper, Vafaei Sadr et al. (2022) applied a CNN to the Galaxy Zoo\nDECaLS dataset, repeatedly retraining the network with human-\nprovided labels to improve the features learned in order to quickly\ndetect interesting anomalies. While this approach is very promising,\nit would require somewhat expensive retraining of the CNN with\neach iteration which may not be possible in all scenarios.\nSelf-supervised learning offers a promising alternative. Modern\napproaches to unsupervised clustering of galaxy images tend to rely\non autoencoders to learn a representative feature space (for example,\nSpindler et al. 2021; Cheng et al. 2021; Zhou et al. 2022; Fielding\net al. 2022). Autoencoders are trained to reproduce the input data\nidentically and in the process, learn a lower dimensional represen-\ntation of the image dataset. Newer self-supervised techniques, in-\ncluding contrastive and non-constrative learning, instead apply aug-\nmentation to introduce random variations in the data and train the\nalgorithm to recognise these pairs of augmented images as being\nthe same. This form of self-supervised learning is gaining popular-\nity as a method of learning effective representations without labels\n(Huertas-Company et al. 2023).\nContrastive learning, which includes both positive and negative\naugmented image pairs, has been applied several times to optical\ngalaxy images. Hayat et al. (2021) and Stein et al. (2021, 2022) use\ncontrastive learning for an impressive array of applications includ-\ning galaxy morphology classification, photometric redshift estima-\ntion, strong lens discovery and similarity searches. Sarmiento et al.\n(2021) applied contrastive learning to investigate galaxy physics in\nintegral field unit data, while Wei et al. (2022) showed that learned\nrepresentations can be applied across different optical datasets.\nWhile this recent body of work shows that contrastive learning can\nbe a powerful tool for a myriad of applications, it can be computation-\nally demanding. Non-contrastive learning, specifically the technique\nBootstrap Your Own Latent (BYOL, Grill et al. 2020), is a more\nviable option when computational resources are limited. Slĳepcevic\net al. (2023) showed the potential of BYOL as a foundation model for\nradio galaxy images, which can be fine-tuned for classification tasks\non new datasets.\nThe goal of this paper is to develop a methodology for data ex-\nploration, as an initial step for building training sets, obtaining the\nmore obvious morphology groups and for detecting anomalies for\nlarge and unlabelled datasets. We apply self-supervised learning to\ntwo different datasets, for the purpose of learning representations that\ncan be used for unsupervised learning tasks. As well as demonstrat-\ning the performance of these representations for a simple supervised\nclassification task, we apply automated unsupervised clustering and\nanomaly detection with active learning as example downstream tasks.\nWhile the main dataset used in this paper consists of optical images\nof galaxies, we also apply the same methods to a small radio galaxy\ndataset to demonstrate their general utility.\nWe start by focusing on the popular hand-labelled optical dataset\nGalaxy Zoo DECaLS, which we describe in Section 2. We outline\nour use of a pretrained network for removing artefacts in Section 3\nand the methodology of using self-supervised learning to extract use-\nful features in Section 4. We then demonstrate the effectiveness of\nthese features in a series of different applications. We make use of\nsupervised classification in Section 5 to demonstrate the utility of\ninitialising the network with pretrained weights and as a baseline to\nensure the self-supervised method is indeed learning representations\nthat correspond to galaxy morphology. An unsupervised clustering\napproach is explored in Section 6 to attempt to group galaxies of\nsimilar morphology together without the need for labelling. We ap-\nply an anomaly detection algorithm in Section 7 to rapidly locate\nmerger candidates. After exploring the use of self-supervised learn-\ning on optical data, we further demonstrate its remarkable flexibility\nby applying an essentially identical approach to a radio dataset in\nSection 8 with both a supervised classification and clustering appli-\ncation. Section 9 summarises our conclusions.\n2 GALAXY ZOO DECALS DATA\nThe data used in this paper was originally sourced from the Dark En-\nergy Camera Legacy Survey (DECaLS, Dey et al. 2019) DR5. The\nGalaxy Zoo DECaLS data (Walmsley et al. 2020a) is a catalogue\nof thousands of high resolution images, from DECaLS, of optical\ngalaxies with a wide range of morphologies. This dataset is ideal\nfor our work because it is large, from a modern telescope and fully\nlabelled. Although we are primarily interested in unsupervised appli-\ncations, the labels allow us to test the effectiveness of the algorithms\nconsidered before applying to unlabelled datasets in the future. The\nGalaxy Zoo1 citizen science project asks users to identify morpho-\nlogical features of galaxies, deciding whether each object is smooth\nor featured, has spiral arms, bars, tidal tails etc. Galaxy Zoo DECaLS\nmade use of a sophisticated decision tree, rather than assigning sim-\nple morphological labels, which simplifies the identification task by\nremoving jargon but also allows fine-grained decision making when\ndefining a morphological sample.\nVolunteers are presented with a series of questions such as “Is the\ngalaxy simply smooth and rounded, with no sign of a disk?” to which\nthe answer could be “Smooth”, “Features or disk” or “Artefact”. The\nvolunteer is then presented with the next question based on their\ninitial answer and so traverses down the decision tree. The number\nof votes as well as the vote fraction for each question is recorded and\nthese can be used to create labelled subsets that select for a certain\nmorphology.\nThe images and labels we use can be found on Zenodo2 (Walmsley\net al. 2020b). We use the labels from versions 1 and 2 of Galaxy\nZoo DECaLS3 rather than version 5, as we did not consider the\nimprovements to the decision tree in version 5 to be as important for\nour application as simply having more labelled data to test with. The\ntotal number of images available from Zenodo was 269 760 and of\nthose, 65 290 had labels from volunteers. We used the full image set\nfor training our feature extractor (Section 4) and applying a clustering\nalgorithm (Section 6.1) but restricted our analysis to the labelled data\n1 www.galaxyzoo.org\n2 https://zenodo.org/record/4573248\n3 gz_decals_volunteers_1_and_2.parquet\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n3\nfor interpretation of the clustering results (Section 6.2) and the later\nanomaly detection application (Section 7).\n2.1 Preprocessing\nSlĳepcevic et al. (2023) found that CNNs are sensitive to the apparent\nangular size of a source in a given image. To ensure the algorithm\nis focusing on physical morphological features, we preprocess each\nimage to attempt to isolate and resize the central source. We use the\nstandard sigma clipping transform available in the software package\nastronomaly4 to locate the central source and cut out background\nsources. This function works by first calculating the noise level in\nthe image, using Astropy (The Astropy Collaboration et al. 2013,\n2018, 2022), applying a 4𝜎threshold and then using an openCV\n(Bradski 2000) contour-finding algorithm to select all regions above\nthis threshold. Because the source should always be located at the\ncentre of the image, we could then select only the central contour in\norder to determine an appropriate bounding box around the source.\nWe chose to enlarge the bounding box by a factor of two to ensure\nthe entire source is contained.\nIt should be noted that the resulting bounding box was applied\nto the original image, not the sigma-clipped image. Sigma clipping\ncan sometimes remove part of the source and a key advantage of\ndeep learning is that it can easily learn to ignore the background\nanyway. We thus elected not to use sigma clipping for this analysis,\nbut if this methodology were to be applied to more crowded fields it\nmay become necessary to refine the preprocessing to remove nearby\nsources.\nAt times the contour-fitting procedure can fail and raise an error,\nusually due to large sources filling the field or very bright nearby\nsources such as stars or artefacts. In these cases we simply use the\noriginal image instead. After extracting the central source, we resize\nthe image to 300x300 pixels.\n3 FEATURE EXTRACTION WITH A PRETRAINED\nNETWORK\nOur general approach is to make use of a CNN as a feature5 extractor,\nrather than a classifier. This can be done for a CNN that has been\ntrained to solve a different task by ignoring the final classification\nlayer of the network and instead using the outputs of the weights\nof the previous layer as features (as was done in Walmsley et al.\n(2022), Etsebeth et al. (2023) and several other examples). Here we\ndescribe the core architecture we use throughout this work and our\ninitial experiment with a standard pretrained neural network before\nmoving on to self-supervised learning.\n3.1 Model architecture\nCNNs are a type of neural network consisting of many layers of\ninterconnected “neurons”. Standard artificial neural networks take\nnumerical input and, through a series of non-linear operations per-\nformed by the neurons, make predictions such as the class the input\n4 https://github.com/MichelleLochner/astronomaly\n5 It should be noted that these features are representations of the images\nderived from a neural network and do not refer to the more general use of\nthe word feature in the context of galaxy morphology such as “bar” or “tidal\ntail”.\ndata belongs to. The key difference between CNNs and typical arti-\nficial neural networks is that the neurons of CNNs are kernels that\nperform convolutions across an image. This allows the network to,\nthrough training, learn an optimal set of filters through which to\npass the image, resulting in a useful image representation for the\ndownstream task. CNNs also consist of other types of layers, such as\npooling and dropout layers, which are inserted to improve this rep-\nresentation by allowing filters to be applied on a hierarchy of scales\nand also avoiding overfitting, which such complex algorithms are\notherwise prone to. CNNs are typically used for classification tasks,\nbut the majority of the network architecture is actually dedicated to\nlearning features that are ultimately useful for classification. We can\nleverage this by using a pretrained CNN as a general-purpose feature\nextractor.\nWe used the ResNet-18 model (He et al. 2016) as the CNN archi-\ntecture for this work. ResNet-18 was found to have sufficient perfor-\nmance for a low computational cost. Guérin et al. (2021) performed\nan in-depth analysis of the use of CNNs as feature extractors for\nclustering tasks and found that the second last layer always provides\nthe best representation of the images. In the case of a Resnet-18 this\nis the layer called “avgpool”, which produces 512 outputs to be used\nas features. Figure 1 illustrates our full methodology, showing how\nfeatures are extracted from the raw data.\n3.2 Visualisation of extracted features\nIt is particularly critical for this type of work to be able to visu-\nalise high dimensional feature spaces. In keeping with the current\ntrend in the machine learning field, we make use of the technique\nUniform Manifold Approximation and Projection (UMAP, McInnes\net al. 2018). UMAP aims to learn a lower dimensional embedding\nthat optimises for local structure, but still preserves global structures.\nThe UMAP algorithm constructs a fuzzy topological representation\nof the data and then approximates this with a lower dimensional\nrepresentation through an optimisation algorithm. This allows the\nstructure of high dimensional data to be easily visualised. These\nplots are especially useful for localising obvious outliers, determin-\ning where sources of particular types lie in feature space and for\nunderstanding the behaviour of the unsupervised algorithms we later\napplied to the features.\nTo implement UMAP for our features, we made use of the umap-\nlearn6 software package (Sainburg et al. 2021). The UMAP algo-\nrithm has a number of hyperparameters. Throughout this work, we\nset the parameter “number of neighbours” to 15, which minimises the\ncreation of artificial clusters and the parameter “minimum distance”\nto 0.01 to prioritise local structure and encourage the formation of\ngenuine clusters. We demonstrate the effect of different parameter\nchoices in the Appendix, Section A.\n3.3 Artefact removal with a pretrained network\nUnfortunately, the preprocessing procedure of Section 2.1 inadver-\ntently introduced artefacts into the data (see Section B in the Ap-\npendix for more details of how the artefacts are created). While a\nmore refined procedure may produce fewer artefacts, it is difficult\nto eliminate them entirely. Instead, we used the opportunity to test\na novel approach to artefact removal. Before proceeding to the full\nfeature extraction method described in the next section, we used\na ResNet-18 already trained on the well-known machine learning\n6 https://umap-learn.readthedocs.io/en/latest/\nMNRAS 000, 1–22 (2023)\n4\nK. Mohale & M. Lochner\nGalaxy Zoo\nimage data\nPreprocessing\nArtefact removal \nwith UMAP\nCleaned\nimage dataset\nPCA\nUMAP for \nvisualisation\nBGMM for \nclustering\nAstronomaly for\nanomaly detection\nResnet-18 pretrained\non ImageNet\n269 760\n512\nfeatures\n232 041\nResnet-18 fine-tuned\nusing BYOL\nFinal feature\nset\nFeatures\n512\nfeatures\n29 \nfeatures\nFeature set with\nlabels and cluster\nassignment\n65 290\n232 041\nDownselect to \nlabelled objects\n269 760\nCluster analysis\nFigure 1. Flow diagram of our methodology. Data and features are represented as cylinders, processes as parallelograms and neural networks as rectangles. The\nnumbers on top of the cylinder symbol indicate how many objects are in that dataset, whether as images or rows in the feature array. Where dimensionality is\nreduced, the resulting number of features is also indicated on the arrows.\ndataset of terrestrial images called ImageNet (Russakovsky et al.\n2014). This pretrained model is already a fairly effective feature ex-\ntractor and we were able to trivially excise the most obvious artefacts\nfrom our dataset with these features. A UMAP plot is shown in Fig-\nure 2 displaying the cuts applied to excise the artefacts. We focused\non the largest cluster of artefacts rather than trying to vigorously\nremove all artefacts. While the pretrained network was highly effec-\ntive at removing these, it failed to usefully group similar galaxies\ntogether. We thus did not make use of this network, beyond the initial\nartefact removal, and instead moved on to a self-supervised learning\ntechnique as a feature extractor. The main dataset used in the rest of\nthis work contains 232 041 images after preprocessing. The artefact\nremoval process is illustrated in the flow diagram of Figure 1.\n4 FEATURE EXTRACTION WITH SELF-SUPERVISED\nLEARNING\nSelf-supervised learning aims to learn useful representations of im-\nages without requiring training labels. A common approach to self-\nsupervised learning is to train the model to have the same predictions\nfor different augmentations (views) of the same image. This leads to\nrepresentation collapse (e.g the model might predict the same trivial\nsolution for all images) and self-supervised learning techniques em-\nploy different ways to circumvent this. For example, the contrastive\nlearning algorithm SimCLR (Chen et al. 2020) uses a repulsive term,\ngenerated from negative pairs, in the loss function to prevent collapse.\nHowever, this algorithm can be resource-intensive precluding its use\nfor this work.\nBootstrap Your Own Latent (BYOL, Grill et al. 2020), on the\nother hand, is a non-contrastive self-supervised learning method that\nrequires relatively low computational resources and, somewhat sur-\nprisingly, manages to avoid representation collapse without the need\nfor negative pairs (see Tian et al. (2021) for some recent insights and\nFigure 2. UMAP representation of the feature space of the whole Galaxy Zoo\nDECaLS data set after preprocessing, using a CNN pretrained on ImageNet\nas a feature extractor. The bounding box shows the selection used to remove\nthe artefacts introduced. The 𝑥and 𝑦axes are in arbitrary units.\npossible explanations). BYOL uses two neural networks of the core\nsame architecture, called the online and target networks, shown in\nFigure 3. The goal of the online network is, given an augmented view\nof an image, to predict the output of the target network, which is given\na different augmented view of the same image. The representation\nindicated in Figure 3 is the final layer of a standard CNN (such as\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n5\nx\nv\nyθ\nzθ\nqθ(zθ)\nv′\ny′\nξ\nz′\nξ\nsg(z′\nξ)\nview\ninput\nimage\nrepresentation\nprojection\nprediction\nt\nfθ\ngθ\nqθ\nt′\nfξ\ngξ\nsg\nloss\nonline\ntarget\nFigure 3. The Bootstrap Your Own Latent architecture (Grill et al. 2020).\nResnet-18), similar to what was used in Section 3.3 to remove arte-\nfacts. The representations of the online network is eventually what\nis used as features, the rest of the architecture being discarded. The\nprojection layer is a simple fully connected neural network layer to re-\nduce the dimensionality of the representations. This projection layer\ngives the online network something to predict that is more tractable\nthan the representation layer. The online network updates its weights\nthrough a standard gradient descent algorithm, minimising the mean\nsquared error between the two projection layers. The weights of target\nnetwork (𝜉) are an exponential moving average of the online network\nweights (𝜃) and are updated according to equation Equation 1. Here\n𝜏∈[0, 1] is a target decay parameter.\n𝜉𝑖←−𝜏𝜉𝑖−1 + (1 −𝜏)𝜃\n(1)\nThrough this update step, the two networks should converge to\nsimilar representations for two different augmented views of the same\nimage. By training on a large dataset with random augmentations,\nthis pair of interacting networks is able to produce a representation\nof the images that reliably groups similar objects together without\nthe need for labels.\n4.1 Augmentations and hyperparameters\nWe use the package byol-pytorch7 (Chen & He 2020) to apply\nthe BYOL algorithm. The hyperparameters we selected are shown in\nTable 1. We fix the number of epochs to 20, which we found to be\ncomputationally efficient while still being sufficient to produce excel-\nlent representations. Further training did not improve our results. The\nbatch size was set to 128 because it was the largest possible given our\ncomputational resources and the learning rate to 0.0001, which was\nfound to be optimal for this dataset (see the Appendix, Section C).\nWe follow the default settings from Grill et al. (2020) for all other\nhyperparameter settings. We chose not to use a separate validation\nset to maximise the number of sources available for training, instead\nrelying on the downstream supervised learning application to assess\nthe features (see Section 5).\nThe effectiveness of BYOL is heavily dependant on the augmen-\ntations. Grill et al. (2020) includes recommended augmentations to\nuse for terrestrial dataset. We find that, just as in the case of radio\ndata (Slĳepcevic et al. 2023), augmentations have to be adjusted to\nbe suitable for the optical datasets. Preprocessing and augmentations\naffect the representations that BYOL will learn and it is important\nthat the variance in the background of galaxies does not dominate\nthe variances in different morphology types.\nUsing the analysis performed in Slĳepcevic et al. (2023) as a start-\ning point, we selected the augmentations gaussian blurring, vertical\n7 https://github.com/lucidrains/byol-pytorch\nHyperparameter\nValue\nNetwork architecture\nResnet-18\nEpochs\n20\nOptimiser\nAdam\nLearning rate\n0.0001\nBatch size\n128\n𝜏\n0.99\nNeurons in projection layer\n256\nTable 1. Hyperparameter values used for training the BYOL algorithm.\nflip, horizontal flip, each with probability 0.5. We also applied the\naugmentation resized crop with the smallest value for cropping the\nimage set to 0.7, as well as rotations by random angles 𝜃∈[0−360],\nsetting the probability of applying either to 0.7. While a full abla-\ntion study would be too computationally intensive, we were able to\ninvestigate the importance of each augmentation in the Appendix,\nSection C, by applying only one augmentation at a time and examin-\ning the accuracy on a downstream supervised learning task. We find\nthe highest performance by applying all the augmentations listed.\nSimilar to what was found in Grill et al. (2020) and Slĳepcevic et al.\n(2023), the exact choice of augmentations has only a moderate im-\npact on KNN accuracy. However it can have a larger impact on the\nversatility of the final representation and visual inspection showed\nthat using all augmentations produced features that better grouped\nsimilar sources together than each augmentation alone.\nIn terms of computational requirements, we use a single NVIDIA\nP100, 16-core GPU (with 116GB of RAM) and find that BYOL trains\nin approximately 11 hours.\n4.2 Dimensionality reduction\nMost unsupervised learning algorithms do not scale well to high\ndimensional spaces. For this reason, we further reduced the dimen-\nsionality of the features using Principal Component Analysis (PCA,\nPearson 1901; Hotelling 1933). PCA works by decomposing a dataset\ninto a new coordinate space such that each orthogonal component\nvector aligns with directions of progressively decreasing variance. By\nkeeping only a small number of principal components, the majority\nof information can be retained with a dramatic reduction in dimen-\nsionality. PCA is especially valuable for highly correlated variables,\nsuch as the features obtained from training a neural network.\nIt is natural to consider using the reduced features derived from\na manifold learning algorithm such as t-SNE (van der Maaten &\nHinton 2008) or UMAP (McInnes et al. 2018) instead of PCA, since\nthese algorithms are non-linear and more flexible. However, we found\nthat PCA preserves global structure better than manifold learning\napproaches and it is precisely the linearity of PCA that reduces the\nrisk of creating artificial clusters in feature space (which manifold\nlearning can sometimes produce). We thus elected to use PCA to\nreduce dimensionality for downstream tasks and UMAP purely for\nvisualisation purposes.\nWe applied PCA to the deep representations, as obtained from the\nhidden layer “avgpool”, keeping 95% of the variance and thus re-\nducing to 29 principal components. We exclusively used the reduced\nfeature space for clustering and anomaly detection. The second row\nof the flow diagram in Figure 1 shows how features are extracted\nfrom the main dataset.\nMNRAS 000, 1–22 (2023)\n6\nK. Mohale & M. Lochner\nEllipticals\nSpirals\nEdge-on\nFigure 4. Examples of each class in the evaluation set representing ellipticals\n(top row), spirals (middle row) and edge-on galaxies (bottom row).\n5 EVALUATION OF EXTRACTED FEATURES\n5.1 Evaluation subset\nIt is challenging to evaluate the performance of a self-supervised\nlearning algorithm since they are designed to operate without any\nlabels by definition. The standard approach is usually to evaluate\nthe extracted features in a downstream task. We thus opted to use\nour features to solve a simple, if contrived, supervised classification\nproblem as an initial test of performance.\nWe selected a small sample of sources that should be considered\nrelatively easy to classify: round ellipticals, spiral galaxies and edge-\non galaxies. Because Galaxy Zoo makes use of a decision tree rather\nthan hard morphological classifications, cuts must be used to extract\na confident sample of sources, which we describe below. In every\ncase, we ensured a minimum number of five votes for the question\nbeing considered (the same threshold used in Domínguez Sánchez\net al. (2018)). We also selected only galaxies most likely not to host\na merger, by requiring merging_merger_fraction < 0.2. The\nnumber of each class that meets the cuts is given in brackets.\n• Round ellipticals (4231):\nsmooth-or-featured_smooth_fraction > 0.8 and\nhow-rounded_completely_fraction > 0.8.\n• Spirals (4034):\nsmooth-or-featured_featured-or-disk_fraction > 0.8 and\nhas-spiral-arms_yes_fraction > 0.8.\n• Edge-on galaxies (5344):\ndisk-edge-on_yes_fraction > 0.8.\nFigure 4 shows three randomly chosen examples for each of the\nclasses.\nTo evaluate the utility of our extracted features for this three-class\nclassification problem, we applied a simple k-nearest neighbours\n(KNN) algorithm (Fix & Hodges 1951; Cover & Hart 1967). For this\nproblem, we found that KNN performed just as well on the original\n0\n5\n10\n15\n20\nEpochs\n75\n80\n85\n90\n95\nKNN accuracy\nImageNet Weights\nRandom Weights\nFigure 5. Accuracy of a KNN algorithm applied to the Galaxy Zoo DECaLS\nevaluation subset as a function of epoch. We use the features derived from\ntraining BYOL on the galaxy images and compare initialising the weights\nrandomly with initialising them from a network pretrained on ImageNet. For\neach epoch, the accuracy is computed for 50 iterations of training-test splits\nwith the mean represented as a line and the standard deviation as an envelope.\nFine-tuning increases the accuracy and convergence speed for this dataset.\nThe corresponding F1 scores (harmonic mean of the precision and recall) are\nlisted in Table D1 in the Appendix.\nfeature space as on the PCA-reduced space. Thus, all reported results\nusing KNN are applied in the original 512-dimensional space.\nThe scikit-learn (Pedregosa et al. 2011) implementation of KNN\nwas used and we set the number of neighbours to 5 and the distance\nmetric to Minkowski. We selected the overall accuracy as an easily-\ninterpretable metric, given that the classes are approximately bal-\nanced. As our focus is not on supervised learning and we simply use\nthis as a tool to evaluate our features, we did not further optimise the\nclassifier. In all cases, the KNN algorithm was trained on a randomly\nchosen 75% of the evaluation subset and the accuracy was computed\non the remaining 25%. For each epoch we run KNN 50 times with\na new random training-test split for each iteration and investigate the\nmean and standard deviation to ensure the algorithm converges to a\nroughly constant accuracy.\nWhile the accuracy is a useful tool to monitor the performance of\nthe BYOL algorithm as a function of epoch, it’s important to note\nthat the algorithm is still trained in a completely self-supervised way\nand that this accuracy information is never fed back to the network.\n5.2 Comparing transfer learning and random weight\ninitialisation\nIt is most common to train a self-supervised learning algorithm with\nthe weights initialised to random values. However, inspired by the\nperformance of the pretrained network in Walmsley et al. (2022)\nand many other examples of fine-tuning, we decided to compare\nthe performance of BYOL initialised with random weights with that\nof ImageNet-intialised weights. We thus essentially use BYOL as a\nmethod of fine-tuning the network described in Section 3.3 to adapt\nthe network to our particular dataset.\nFigure 5 shows the accuracy of KNN applied to the evaluation set\nwhen training a model initialised with ImageNet weights against one\nwith randomised initial weights. The results show that not only do\nthe fine-tuned BYOL features start with a 13% higher accuracy, the\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n7\nnetwork also converges quicker to a relevant feature space and has\na higher peak accuracy after 20 epochs. While the accuracy of the\nrandom-start network may eventually be comparable to that of the\nfine-tuned network, fine-tuning provides significant gains in terms\nof reducing computational requirements. While we use the accuracy\npurely as a comparison metric between the two approaches, it is also\nworth noting that 95% accuracy is excellent performance and shows\nthe representation produced by BYOL is useful for downstream tasks.\nHaving evaluated its performance, for the remainder of this work we\nuse the features derived from applying BYOL, initialised with the\nImageNet weights, to the entire Galaxy Zoo DECaLS dataset after\npreprocessing.\n6 CLUSTERING\nAfter extracting features, evaluating their performance on a small\nsupervised learning problem and reducing their dimensionality with\nPCA, we next turned our attention to the main goal of this paper:\nto automatically cluster similar objects together in an unsupervised\nmanner.\n6.1 Bayesian Gaussian mixture model\nWe chose to apply a Bayesian Gaussian mixture model (BGMM,\nAttias 1999) for this problem. The aim of this clustering approach is to\napproximate the data as a mixture of Gaussian distributions, each with\na mean, covariance and overall weight. These Gaussians then form the\nclusters. A BGMM specifically applies Bayesian inference to learn\nthe parameters of these Gaussians. Rather than computing the full\nposterior over the parameters, which is computationally expensive,\nthe algorithm we selected uses variational inference to approximate\nthe posterior distribution and find the best-fitting parameters through\noptimisation.\nWe use the BayesianGaussianMixture library from scikit-\nlearn (Pedregosa et al. 2011). We ran BGMM with the number of\ncomponents set to 20. The intuition behind the number is based on the\nnumber of density regions we observed on the UMAP feature space.\nHowever, we note that the BGMM implementation is able to set the\nweights of individual Gaussians very low making the input number\nof components an upper limit in reality. We set the weight concentra-\ntion prior to 0.5 to force BGMM to focus more on global structure.\nIt is challenging to tune this hyperparameter in general in an unsu-\npervised context. The simplest approach, which we employed, is to\nvary the weight concentration prior and visually inspect the resulting\nclusters. A poor choice of this parameter results in obvious overlap\nbetween clusters and mixing of source types within clusters. We set\nthe number of initialisations to 10 to control the number of times the\nalgorithm will runhttps://github.com/MichelleLochner/astronomaly\nto ensure reproducibility. We also increase the total number of itera-\ntions per run to 1000 to ensure that the algorithm converges in each\nrun. All other hyperparameters we kept to their default\nWe applied the clustering algorithm to the features from the full\nGalaxy Zoo DECaLS dataset (after preprocessing) to obtain a to-\ntal of 20 clusters labelled 0-19. BGMM is able to identify densities\nconsistent with those that we see on the feature space in the UMAP\nplots. However we observe some disagreement between BGMM and\nUMAP since the clustering is applied directly on the principal com-\nponents.\nFigure 6 shows examples of galaxies that correspond to several\nof the clusters that can be seen on the UMAP plot. Combining self-\nsupervised with unsupervised learning does appear to successfully\ngroup together sources with similar morphologies, such as elliptical\nand spiral galaxies, as well as edge-on galaxies and those with a\nprominent bulge. Figure 7 shows some more complex examples,\nsuggesting that at times the algorithm may be picking up on non-\nphysical properties such as a particularly “zoomed out” image or\nthe presence of a companion. Improved preprocessing could perhaps\nclean up some of these clusters but blending will likely remain a\ndifficult challenge to these algorithms as surveys increase in depth\nand hence source density.\nFigure 8 shows how the clustering algorithm effectively selects\nartefacts, missed by the approach presented in Section 3.3. This\nsuggests that unsupervised techniques could provide a relatively\nlightweight method of detecting and removing artefacts that may\nbe otherwise missed by automated pipelines.\n6.2 Cluster analysis with volunteer labels\nWhile it is visually apparent that the clustering algorithm successfully\ngroups galaxies of similar morphology together, we can investigate\nthis more deeply by making use of the Galaxy Zoo decision tree user\nlabels. The bottom row of the flow diagram in Figure 1 shows how\nonly objects with labels are selected and how those subsequent fea-\ntures are used for clustering and later, anomaly detection (Section 7).\nFigure 9 highlights the distribution of vote fractions for several\nkey questions in the Galaxy Zoo decision tree, for galaxies in each\ncluster, as well as for the full labelled sample for comparison. To\nremove spurious components of the distributions, we set a rather\nstringent minimum threshold of 10 votes for each question. Using\nthese distributions, we can investigate more deeply the types of galaxy\nresiding in each cluster.\nBased on visual inspection of examples of cluster members, we\nexcluded clusters 7, 8, 9, 11, 13, 15, 17 and 19 from the plots as\nthey predominantly contained artefacts (some examples of these are\nshown in Figure 8). Inevitably, some clusters (most notably cluster\n19) contain some real and interesting sources among the artefacts,\nbut these sources are in the minority. It should be noted that most of\nthe artefacts in these clusters were introduced during preprocessing\nand were not present in the original data. While the preprocessing\ncould certainly be improved to avoid the inclusion of these artefacts,\nartefacts are nonetheless often introduced in scientific pipelines and\nit is encouraging to see the clustering algorithm is able to trivially\ndetect these. The number of galaxies excluded is 5332, out of a total\nof 65290 sources. We use the distributions of Figure 9 to better\nunderstand the types of galaxies found in each of the remaining\nclusters.\nThe first Galaxy Zoo decision tree question we considered is “Is\nthe galaxy simply smooth and rounded, with no sign of a disk?”. In\nthe top panel of Figure 9, we show the distributions for the fraction\nof votes for “Features or disk” (in other words, the galaxy is not\nsmooth or an artefact). The distributions for clusters 1, 3, 12 and\n14 strongly suggest they are largely smooth and probably elliptical\ngalaxies. It is interesting to note that clusters 1 and 12 contain well-\nresolved sources while 3 and 14 are much lower resolution, which\nmay result in the citizen scientists voting them to be smooth. Cluster\n5 appears to consist almost entirely of spirals, while the remaining\nclusters have less clean distributions. Clusters 0 and 18 contains a\nmix of morphologies, but closer investigation of the question “Is\nthere any sign of a spiral pattern?” (not shown in Figure 9), as well\nas inspection of the examples in Figure 6, suggests that these groups\nconsists largely of spirals.\nCluster 6 shows fairly broad distributions throughout, which re-\nflects the fact that this cluster is not well localised in Figure 7. In-\nMNRAS 000, 1–22 (2023)\n8\nK. Mohale & M. Lochner\nFigure 6. UMAP plot of cluster samples corresponding to morphologies that include flat disk galaxies (2 and 10), bright centered tight spirals (5 and 18), and\nbright centered high resolution ellipticals (12).\nFigure 7. UMAP plot of cluster samples corresponding to morphologies that can be challenging to categorise. The feature space shows a larger disagreement\nbetween UMAP and BGMM in the case of these type of galaxies. The confusion mostly comes from heavy background noise (cluster 16, 4 and 1) as well as low\nresolution (cluster 14).\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n9\nFigure 8. UMAP plot of cluster samples that correspond to artefacts. These artefacts tend to separate well from the main population and group together by type,\nindicating that self-supervised features are effective at identifying artefacts that may have been otherwise missed.\nspection of the examples of cluster members suggests that algorithm\ngrouped together objects with nearby (usually coincident) sources,\nrather than primarily by morphology. Although not shown in order to\nreduce complexity of the figure, we also investigated the “Artefact”\nanswer to the first question in the Galaxy Zoo decision tree. Only\nclusters 16 and 19 indicated a higher than average number of arte-\nfacts. We discarded cluster 19 based on visual inspection indicating\nthat artefacts dominated this cluster. Cluster 16 however, consists of\na mix of artefacts and galaxies with interesting morphology so we\nkept cluster 16 in the sample.\nWe further investigated these clusters by examining the answer to\nthe question “How rounded is it?”, focusing on the “Cigar shaped”\ngalaxies in the second panel of Figure 9. As this question in the\ndecision tree only applies to galaxies that appear smooth, we only\nconsidered objects for which the fraction of votes for a smooth object\nis greater than 0.5. We also considered the question “Could this be\na disk viewed edge-on?” in the third panel. We applied a threshold\nof 0.5 to the fraction of votes that consider the galaxy to be featured.\nThese two plots together show that clusters 2 and 10 both consist of\nelongated galaxies, but the algorithm is not able to easily distinguish\nbetween cigar-shaped ellipticals and edge-on spirals (although more\ncigar-shaped galaxies seem to occur in cluster 2 than 10). This is\nnot surprising as edge-on spirals and elongated ellipticals can be\nindistinguishable even for a human if the inclination angle is high\nenough or the resolution low. Finally, we note that cluster 16 seems\nto have a mix of edge-on and face-on galaxies and is one of the more\ncomplex grouping of galaxies.\nIn the fourth panel of Figure 9, we considered the question “How\nprominent is the central bulge, compared with the rest of the galaxy?”\nand focused on the distributions for the answer “Dominant bulge”.\nSimilarly to the previous questions, we required the fraction of votes\nfor the featured question to be greater than 0.5. This plot reveals that\nMNRAS 000, 1–22 (2023)\n10\nK. Mohale & M. Lochner\nall\n0\n1\n2\n3\n4\n5\n6\n10\n12\n14\n16\n18\nCluster number\n0.0\n0.5\n1.0\nFeatured/disk fraction\nall\n0\n1\n2\n3\n4\n5\n6\n10\n12\n14\n16\n18\nCluster number\n0.0\n0.5\n1.0\nCigar shaped fraction\nall\n0\n1\n2\n3\n4\n5\n6\n10\n12\n14\n16\n18\nCluster number\n0.0\n0.5\n1.0\nEdge on fraction\nall\n0\n1\n2\n3\n4\n5\n6\n10\n12\n14\n16\n18\nCluster number\n0.00\n0.25\n0.50\n0.75\n1.00\nDominant bulge fraction\nall\n0\n1\n2\n3\n4\n5\n6\n10\n12\n14\n16\n18\nCluster number\n0.0\n0.5\n1.0\nNo merger fraction\nFigure 9. Vote fraction distributions for various Galaxy Zoo questions for each cluster (clusters of predominantly artefacts are not plotted). The global distribution\nis indicated with the “all” label. The distributions are represented as violin plots (using the package Seaborn (Waskom 2021)): a kernel density estimate of the\ndistribution with lines representing the extrema, a box for the first and third quartiles and a point for the median. For clarity in distributions with long tails, the\nviolins are set to equal width. These distributions can be used to investigate the types of galaxy morphology found in each cluster.\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n11\nCluster number\nDescription\n0\nPredominantly spiral galaxies\n1\nMostly ellipticals and some spirals with prominent bulges\n2\nEdge-on disks and some cigar-shaped ellipticals\n3\nPredominantly ellipticals but does contain some spirals\n4\nA large mix of morphologies, including some apparent mergers and some artefacts\n5\nAlmost entirely spiral galaxies\n6\nMixed morphology, dominated by galaxies that have a (sometimes coincident) companion\n7\nArtefacts\n8\nArtefacts\n9\nArtefacts\n10\nEdge-on disks and some cigar-shaped ellipticals\n11\nArtefacts\n12\nPredominantly ellipticals\n13\nArtefacts\n14\nPredominantly ellipticals\n15\nArtefacts\n16\nA large mix of morphologies, including some apparent mergers and some artefacts\n17\nArtefacts\n18\nPredominantly spiral galaxies\n19\nArtefacts\nTable 2. Qualitative summary of each of the clusters detected by applying a Bayesian Gaussian mixture model to the Galaxy Zoo DECaLS data. These qualitative\nconclusions are further supported by a quantitative analysis using the evaluation subset, detailed in the Appendix, Section E.\nalthough clusters 12 and 14 consist of mostly ellipticals, they also\ninclude some disk galaxies with prominent bulges that appear similar\nto ellipticals. By viewing the examples in Figure 6 and Figure 7, it\nis easy to see how these objects can be confused even by a human.\nCluster 1, which contains some mixed morphologies, also seems to\nconsist mostly of galaxies with a dominant bulge, although we note\nthat this cluster contains a significant number of artefacts.\nThe final panel of Figure 9 focuses on the question “Is the galaxy\nmerging or is there any sign of tidal debris?” Because this question\nhas multiple answers, we focused on the negative to look for sources\nthat have any indication of mergers at all. We once again required\nat least 50% of users to have voted that the galaxy is featured to\ninclude it in the sample. The plot suggests that clusters 1, 4 and 16\nmay have merger candidates, although they will be mixed with other\nsources. Figure 7 shows that clusters 1, 4 and 16 both appear to have\ncomplex, multiple sources, with cluster 16 having more “zoomed\nout” examples.\nTable 2 gives a qualitative description of the galaxies in each\ncluster, based on Figure 9 and Figures 6-8.\n6.3 Cluster membership of evaluation subset\nIn Section 5.1 we introduced a “clean” and simple evaluation set of\nlabelled objects, consisting of round ellipticals, spirals and edge-on\ngalaxies. We investigated which clusters these objects were assigned\nto:\n• Round ellipticals - 77.2% found in clusters 12, 14, 3 and 1.\n• Spirals - 79.3% found in clusters 5, 18 and 0.\n• Edge-on galaxies - 79.3% found in clusters 10, 2 and 6.\nSection E in the Appendix shows this break down in more detail. In\nall cases the majority of the sources were found in the first one or\ntwo clusters listed. These results broadly reflect the more qualitative\nconclusions drawn in Section 6.2.\nHow can this be useful? By simply inspecting a few objects in\neach cluster, we can apply labels to the clusters as above. This simple\napproach results in an overall accuracy of 77% for our evaluation\nsubset. While this is clearly far below the performance of a supervised\nalgorithm, it is impressively high for a fully unsupervised approach.\nThis approach can be a highly efficient way to select and label samples\nfor training downstream supervised learning algorithms, as well as\nbeing useful for removing artefacts and identifying unusual clusters\nof objects.\n7 ANOMALY DETECTION\nTo further demonstrate the general utility of these self-supervised\nfeatures, we applied anomaly detection to the dataset. Detecting rare\nand unusual objects in massive datasets is a key application of unsu-\npervised learning. In order to compare with some ground truth labels\nand test performance, we focused on merger candidates as a specific\ntype of anomalous object to search for. While it is completely pos-\nsible to detect more rare types of sources with the same algorithm\nand features, we did not have easy access to labels of such sources,\nand considered the merger example as a suitable anomaly detection\ndemonstration.\n7.1 Astronomaly\nWe applied the astronomaly framework (Lochner & Bassett 2021),\nwith the alternative anomaly detection approach introduced in Walm-\nsley et al. (2022), which we found works well for this dataset. The\nprocedure we follow is illustrated in the flow diagram of Figure 10.\nastronomaly employs active learning to rapidly identify not just\nanomalies in data, but anomalies that are specifically of interest to\nthe user. The key concept of astronomaly is that “blind” anomaly\ndetection, using machine learning, will usually detect a large number\nof sources which, while anomalous, are not of interest to a scientist.\nThus, astronomaly employs an interactive framework to obtain\nvery few, strategic labels from a user in order to improve the detec-\ntion algorithm. This is somewhat similar to recommendation engines\nused in popular music or video streaming services. In our case, unin-\nteresting anomalies would include artefacts or galaxies with unusual\nmorphologies, while anomalies of interest would be merger candi-\ndates. astronomaly requires the user to score, on a scale of 0 to 5,\nexamples of objects according to their “interestingness”. In machine\nMNRAS 000, 1–22 (2023)\n12\nK. Mohale & M. Lochner\nFeature set with\nlabels\n65 290\nLabel 10 initial\nsources\nGP fit\nLabel top 10\nsources\nSort all sources\nby acquisition\nSort all sources\nby anomaly score\nEvaluate recall\nusing remaining labels\nRepeat until 200\nsources labelled\nLabel = 5x merger\nprobability rounded to\nnearest integer\nFigure 10. Flow diagram of the anomaly detection section of our methodology. Features are represented as cylinders and processes as parallelograms. The\nnumber on top of the cylinder symbol indicate how many objects are in that dataset.\n0\n200\n400\n600\n800\n1000\nIndex in ranked list\n0\n200\n400\n600\n800\n1000\nNumber of mergers\nRandom ordering\nAstronomaly with BYOL features\nPerfect recall\nFigure 11. Number of mergers detected as a function of index in a list of\nsources ordered by anomaly score, after active learning is applied to 200\nsources. The theoretical performance of a random order and perfect recall\nis shown for reference. astronomaly is able to efficiently recover merger\ncandidates without requiring significant labelling.\nlearning nomenclature, the algorithm requests these labels from an\n“oracle”. Normally, this oracle would be a human expert who labels\nobjects according to how interesting they are for the science inter-\nests of that expert. However, we had access to a set of ground truth\nlabels from the citizen scientist volunteers to provide labels when re-\nquested by the algorithm. Although we have access to a large number\nof ground truth labels, only a subset of these were provided to the\nactive learning algorithm.\nWe made use of the Galaxy Zoo decision tree question “Is the\ngalaxy merging or is there any sign of tidal debris?”. This question\nhas four possible answers: “Merging”, “Tidal debris”, “Both” or\n“Neither”. Visual inspection suggested that galaxies with a high voter\nfraction for “Tidal debris” are only mildly disturbed so we added the\nvoter fractions for “Merging” and “Both” and took that number to\nbe the probability that an image is a merging system. We scaled this\nprobability to lie on the range of 0-5 and rounded to the nearest integer\nto simulate astronomaly’s scoring mechanism. The key question\nis, what threshold do we apply to decide if an image corresponds\nto a true positive merger? It seems overall voter confidence in this\nquestion was low since a high threshold (like 0.8) cuts out a large\nnumber of obvious merger candidates. We follow the methodology of\nDarg et al. (2010) and consider sources with a threshold of 0.4 to be\nvery likely merger candidates. This will naturally not result in a very\nclean sample, but visual inspection suggests this cut is appropriate.\nWe also ensure the voter fraction is at least 10 before considering a\nsource a “true” merger. This results in 4990 mergers or 7% of the\ndataset considered as interesting anomalies.\nWe passed the features described in Section 4 after applying PCA\nto astronomaly. The active learning approach of Walmsley et al.\n(2022) requires an initial sample of objects with labels. While Walm-\nsley et al. (2022) used a random sample of galaxies to begin the\nprocess, we found that this introduces a large degree of randomness\nin the results, especially with small numbers of labels. Instead, in\norder to find a deterministic sample of galaxies that should broadly\nspan the feature space, we sorted the features by their first princi-\npal component and selected an example from every tenth percentile\n(i.e. 10 objects equally spaced by index in the sorted list, not feature\nvalue). We labelled these with an integer score from 0 to 5, based on\nthe merger probability.\nThis initial set of labels allowed us to proceed to the active learning\nstep of the astronomaly pipeline. The goal is to train a regression\nalgorithm to learn, with as few examples as possible, to predict the\n“interestingness” score of every object in the dataset thus quickly\nfinding the interesting anomalies. As in Walmsley et al. (2022), we\nuse a Gaussian Process (GP) to perform the regression. A GP es-\ntimates a probability distribution of functions in order to predict a\nmean function value, as well as its corresponding uncertainty, for\nany input. We use this to predict the “interestingness” score at any\ngiven set of features, based on the small number of labels from the\nuser. The space of possible functions is given by the kernel. We use a\nMatérn kernel, added to a white noise kernel to model intrinsic noise\nin the labelling, and allow the package scikit-learn to automatically\noptimise kernel hyperparameters.\nOne of the most important advantages of a GP is that it calculates\nthe uncertainty in its estimates, giving an indication of which regions\nof feature space are poorly constrained. This can be used in an ac-\nquisition function, which allows optimal selection of targets for the\noracle to label in the next iteration that will improve the algorithm.\nWe used exactly the same acquisition function as Walmsley et al.\n(2022), which is the maximum expected improvement. This function\nbalances improving the score estimate across feature space by priori-\ntising regions of high uncertainty and honing in on specific regions\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n13\n0\n1\n2\n3\n4\nActive learning score\nFigure 12. UMAP plots of the same Galaxy Zoo DECaLS feature space highlighting the “ground truth” mergers (left) and showing the anomaly score after\napplying active learning (right). It is clear that astronomaly, after training on just 200 examples, effectively prioritises the regions of feature space where\nmergers are likely to be found.\nknown to contain many high scoring objects. It has one tuning pa-\nrameter, the trade-off 𝜂which we set to 0.5. This tends to prioritise\nknown interesting regions over exploration, resulting in quickly find-\ning mergers. We found that changing the 𝜂parameter did not strongly\nimpact our results.\nTo perform our merger search, we labelled an initial 10 sources\nas described above and then trained the GP and labelled the top 10\nunlabelled sources with the highest acquisition function. We repeated\nthis process until 200 sources were labelled and then sorted the\nentire dataset according to estimated “interestingness” score. This\ncorresponds to labelling just 0.3% of the data.\nIn general, Astronomaly does not classify sources as anomalous\nor not, rather sorting the list and allowing a user to decide how far\ndown the list they are willing to explore in order to look for interest-\ning sources. We can then consider the recall, in terms of number of\ninteresting anomalies detected, as a function of index in the ranked\nlist to determine the effectiveness of the anomaly detection algo-\nrithm, shown in Figure 11. The anomaly detection algorithm very\nefficiently recovers a large sample of mergers within the first few\nhundred sources with a high anomaly score. This excellent perfor-\nmance should obviously be taken with some caution as the threshold\nto be considered a merger is fairly low.\nThe two UMAP plots in Figure 12 demonstrate how the active\nlearning score successfully maps the overdensity of mergers in feature\nspace. While some mergers are buried among other, non-anomalous,\nsources, the majority are grouped together again highlighting that\nthe self-supervised features effectively groups sources with visually\nsimilar morphology.\nFigure 13 demonstrates that the majority of objects given a high\nscore are indeed merger candidates. This figure attempts to demon-\nstrate the active learning process by displaying the first five sources\nshown to the oracle each iteration, ordered by acquisition score (i.e.\nthe five sources assigned the highest priority for labelling). After\na few rounds querying some uninteresting objects, the algorithm\nquickly hones in on merger candidates which dominate the sources\nas quickly as the fifth query (40 sources labelled).\n7.2 A similarity search for ring galaxies\nInspired by (Walmsley et al. 2022) and other recent works, we per-\nform a similarity search for a particularly unusual type of source, in\nthis case choosing a few candidate ring galaxies. These objects are\ntruly rare and we could use astronomaly to perform a dedicated\nsearch for them, however we have no ground truth labels to compare\nagainst so we instead opted to demonstrate the utility of the self-\nsupervised features using a similarity search. We select two example\nring galaxies and use a simple Euclidean search on the PCA-reduced\nfeature space to find the eight nearest neighbours to each source.\nFigure 14 and Figure 15 show the two selected sources and their\neight nearest neighbours. It is clear that they share very similar mor-\nphology, indicating that these features can be useful for rapidly iden-\ntifying samples of rare objects as soon as an initial example is dis-\ncovered.\n8 APPLICATION TO A RADIO DATASET\nAs a final application to test the general utility of self-supervised\nlearning for feature extraction in astronomy, we turn to a completely\ndifferent type of dataset and apply the exact same approach.\n8.1 MiraBest data\nWe chose the radio image dataset MiraBest (Porter & Scaife 2023), a\nset of 1256 labelled radio images8 from the Karl G. Jansky Very Large\n8 Available at https://zenodo.org/records/5588282.\nMNRAS 000, 1–22 (2023)\n14\nK. Mohale & M. Lochner\nQuery 1\n0\nOracle:\n0\n0\n0\n1\nQuery 2\n2\nOracle:\n3\n1\n4\n3\nQuery 3\n2\nOracle:\n4\n0\n1\n1\nQuery 4\n0\nOracle:\n0\n0\n1\n0\nQuery 5\n4\nOracle:\n4\n4\n5\n4\nQuery 6\n5\nOracle:\n5\n1\n3\n5\nFigure 13. An illustration of the active learning approach used to detect anomalies (in this case merger candidates). Each iteration (query) consists of 10 examples\nbeing shown to the “oracle” after which the algorithm is retrained, ordered by acquisition score, and the next 10 objects are shown to the oracle. Only the top 5\nexamples for each query are shown here due to space considerations. The score the oracle gives (in this case, the labels are given from the citizen scientist votes)\nare shown in a box under each image. It can be seen that the algorithm quickly moves away from the more boring sources and hones in on merger candidates\nafter seeing a handful of examples. Note that the blank image in query 3 is actually a completely black image in the Galaxy Zoo DECaLS dataset.\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n15\nFigure 14. Similarity search for ring galaxies. The eight sources surrounding\nthe central source are its eight nearest neighbours in feature space, according\nto the Euclidean distance.\nFigure 15. Same as Figure 14, we highlight a specific ring galaxy (central\nsource) and its eight nearest neighbours in feature space.\nArray9. Originally classified in Miraghaei & Best (2017), this sample\nof radio-loud active galactic nuclei consists of Fanaroff-Riley I and\nII sources, as well as some sources that appear to be hybrid between\nthe two classes. The Fanaroff-Riley (FR) dichotomy, first proposed in\nFanaroff & Riley (1974), is a morphological classification based on\nthe distance of the brightest regions of the radio jets from the core.\n9 https://science.nrao.edu/facilities/vla/\nFRI\nFRII\nFRII\nFRI\nFRI\nFRII\nFRI\nFRII\nFRII\nFigure 16. A random set of radio galaxies from the MiraBest training sample\nwith the true label reported in the corner.\nFRI galaxies become fainter towards the edge and tend to have a\nlower luminosity in general, while FRII galaxies are edge-brightened\nand more luminous. This is the most commonly used morphological\nclassification in radio astronomy, although there is a high degree\nof variation in source morphology and many examples of sources\nthat are challenging to classify. A random sample of FRI and FRII\ngalaxies from the MiraBest dataset is shown in Figure 16.\nWe chose to apply our techniques to the MiraBest dataset for three\nreasons:\n(i) Radio images differ substantially from optical images, provid-\ning a good stress test for BYOL.\n(ii) There are very few labelled radio image datasets and MiraBest\nhas the added advantage of distinguishing between confidently and\nunconfidently labelled sources. We could thus train BYOL on the full\ndataset but use only the confidently-labelled sources to evaluate.\n(iii) With new radio telescopes such as MeerKAT (Jonas &\nMeerKAT Team 2016), ASKAP (Hotan et al. 2021) and LOFAR\n(van Haarlem et al. 2013) pushing into new regimes of sensitivity,\nresolution and sky coverage, radio astronomy is a field where unsu-\npervised learning is expected to prove extremely useful.\nAs the MiraBest sources were already cropped to some degree, we\ndid not perform any resizing operation. We ran BYOL on this dataset\nusing the exact same hyperparameters described in Section 4 except\nthat we set the batch size to 32 (owing to the smaller dataset) and\nthe number of epochs to 50, since we found that it took longer to\nproduce useful representations. Even with the increased number of\nepochs, the small dataset size means we are able to train BYOL in\njust 11 minutes, using the same GPU mentioned in Section 4.1.\nThis dataset represents an interesting opportunity to test which is\nthe best set of weights to initialise the network with. The idea is that\nso-called “foundation models”, already trained on large datasets, can\nthen be easily fine-tuned using BYOL on smaller, different datasets\nfor downstream tasks. We explore four scenarios for the weights\ninitialisation:\nMNRAS 000, 1–22 (2023)\n16\nK. Mohale & M. Lochner\n(1) Random weights\n(2) ImageNet trained weights\n(3) Galaxy Zoo DECaLS trained weights\n(4) Radio Galaxy Zoo trained weights\nThe last item on the list is derived from Slĳepcevic et al. (2023)\nwhere BYOL was trained on a sample of non-public radio images.\nFortunately, the authors made their trained model available and the\narchitecture is similar to the model we used. We were able to use this\nmodel and fine-tune it to MiraBest. It is worth noting that this was\nalso done in Slĳepcevic et al. (2023) but there are some differences\nin approach, resulting in somewhat different accuracies achieved for\nthe confident MiraBest sample.\n8.2 Evaluation\nFor each of the four scenarios, we trained a KNN on the confidently\nlabelled data, used a 75%-25% training-test split and ran the classi-\nfier for 50 different random splits at each epoch. Figure 17 shows the\naccuracy as a function of epoch for each scenario. Firstly, it is clear\nthat any of the three types of fine-tuning dramatically outperforms\nrandom weights, which tends instead towards a very poor represen-\ntation in the limited number of epochs and with such a small training\nset. It is not surprising that fine-tuning from the Radio Galaxy Zoo\nnetwork, trained using very similar data, outperforms all other op-\ntions. The overall accuracy is comparable to that in Slĳepcevic et al.\n(2023), if slightly lower because we are using KNN rather than a full\nCNN for the final classification step.\nWhat is perhaps more interesting is that fine-tuning an ImageNet-\ntrained network outperforms that of our Galaxy Zoo-trained network.\nThis is counter-intuitive since radio galaxies are more similar to opti-\ncal galaxies than terrestrial images of animals and objects. The reason\nfor this performance difference could be because the ImageNet clas-\nsification task involves of thousands of classes, forcing the network\nto learn very general representations (as was found in Walmsley et al.\n(2022)) or it may be simply due to the size of the training sets, with\nImageNet being an order of magnitude larger than the Galaxy Zoo\nDECaLS sample we used. Either explanation bodes well for the gen-\neral use of foundation models in astronomy as it suggests that a model\ntrained on a completely different, but large, dataset can still be useful\nas a feature extractor if fine-tuned to the dataset it’s being applied to.\nThis conclusion is further supported by examining the feature\nspace of each of the four initialisations described above. Figure 18\nshows the UMAP plot for each case for only the confident MiraBest\nsample, with FRI and FRII galaxies indicated with different markers.\nNo separation between classes is seen when random weights are\nused, while the Radio Galaxy Zoo and ImageNet weights show fairly\nsignificant division between classes.\n8.3 Clustering\nDespite the small sample size, we managed to apply the same clus-\ntering approach as described in Section 6. We first applied PCA to\neach of the four sets of features with a threshold explained variance\nof 95%, resulting in reduced features of size 3, 31, 58 and 16 for the\nrandom, ImageNet, Galaxy Zoo and Radio Galaxy Zoo weights re-\nspectively. We then applied a Bayesian Gaussian mixture model with\nup to 20 components to each set of reduced features. To simulate\na real unsupervised learning application, we examine each cluster\nto check which is the dominant source in each. The cluster is then\nlabelled as that source (i.e. if more than 50% of the objects in the\ncluster are FRI, all objects in the cluster are labelled FRI). While\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nEpoch\n65\n70\n75\n80\n85\nKNN accuracy\nRadio Galaxy Zoo\nImageNet\nBYOL Galaxy Zoo\nRandom\nFigure 17. KNN accuracy as a function of epoch for the confidently-labelled\nMiraBest dataset for each of the four scenarios with different initialisation of\nthe weights. It is clear that fine-tuning is superior to random-initialisation.\nUsing a network trained on very similar data provides the highest performance\nbut this can be almost matched by using a network trained a large dataset of\nterrestrial images (ImageNet). The corresponding F1 scores (harmonic mean\nof the precision and recall) are listed in Table D2 in the Appendix.\nin this case we know the ground truth which makes the labelling\neasier, in a truly unlabelled dataset one would have to simply inves-\ntigate a few samples and decide which is the dominant source for\nthat cluster. By applying these labels, we can compute the overall\naccuracy of this pseudo-classifier, which is reported in Figure 18.\nRemarkably, we obtain almost as high accuracy as when we use a\ntraditional KNN approach. This implies that a fully unsupervised\napproach, fine-tuning a foundation model and applying clustering,\ncan at a minimum quickly produce potential training sets that can\nlater by optimised through minimal human inspection and labelling.\n9 CONCLUSIONS\nInspired by a rapid increase in size and complexity of astronomical\ndatasets, we have explored an array of unsupervised learning meth-\nods for data exploration and scientific discovery. Deep learning has\nproven itself in the arena of supervised learning, but recently has be-\ngun to be used to obtain useful representations of high-dimensional\ndata, such as images, without requiring expensive human-provided\nlabels. We have shown that self-supervised learning, specifically the\nnon-contrastive algorithm Bootstrap Your Own Latent (BYOL), is\nhighly effective at feature extraction for astronomical images. We\napplied this algorithm to two quite different datasets: a sample of\nhundreds of thousands of optical images from Galaxy Zoo DECaLS\nand a relatively small set of radio galaxies from MiraBest. While the\nlatent features perform well for a set of supervised learning tasks,\nthe primary goal was to generate features suitable for unsupervised\nlearning tasks, including clustering and anomaly detection.\nAs with any deep learning application, we found that preprocessing\nchoices are important. Our procedure to crop and resize each image,\nremoving biases with respect to object size, inadvertently introduced\nartefacts in the data. However, we showed that these can be largely\nremoved by applying an initial pretrained CNN as a feature extrac-\ntor, visualising the space using the manifold embedding technique\nUMAP and cutting out an obvious large cluster of anomalies. This\nsimple methodology could prove useful for very quickly identifying\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n17\nAccuracy=67%\nRandom weights\nFRI\nFRII\nAccuracy=79%\nImageNet weights\nAccuracy=69%\nOptical Galaxy Zoo weights\nAccuracy=82%\nRadio Galaxy Zoo weights\nFigure 18. UMAP plots for four different sets of initial weights: random (top left), ImageNet (top right), our Galaxy Zoo-trained BYOL (bottom left) and the\nBYOL trained on Radio Galaxy Zoo from Slĳepcevic et al. (2023) (bottom right). The features are shown for the confident MiraBest sample only and FRI and\nFRII galaxies are indicated with different symbols. When clustering is applied, if we assign an entire cluster with the same class as the majority source in that\ncluster, we obtain the accuracy indicated in each figure. This accuracy is close to that of the supervised approach shown in Figure 17, indicating that BYOL\nsuccessfully learns features useful for both clustering and supervised classification. Finally, both the accuracy and the visual appearance of the UMAP plots\nseem to suggest that a large training set is an important factor when fine-tuning a pretrained network, as the ImageNet weights outperform the optical Galaxy\nZoo weights despite the fact that they are derived from a dataset less similar to MiraBest.\nartefacts in other datasets as it requires no training at all. However,\nsince the network was trained on a completely different dataset (Im-\nageNet), it was not an appropriate feature extractor for more subtle\ndistinctions between morphology.\nBy instead using the self-supervised learning algorithm BYOL, we\nwere able to extract features for both the optical and radio dataset that\nperformed well when combined with a k-nearest neighbours (KNN)\nalgorithm to classify a subset of sources with known classes. While\nthis approach is unlikely to compete with a purpose-built CNN when\na large training set is available, it does demonstrate that BYOL is\nlearning a useful representation of the data.\nWe used the same features for unsupervised learning applications,\nfinding that a Bayesian Gaussian mixture model (BGMM) was able\nto successfully detect clusters of galaxies with similar morphologies\nMNRAS 000, 1–22 (2023)\n18\nK. Mohale & M. Lochner\nfor both datasets. For the Galaxy Zoo DECaLS dataset, we analysed\nthe citizen scientist votes to determine the types of sources found in\neach cluster. While at times the algorithm places sources of different\nmorphology in the same cluster (usually when confused by the pres-\nence of a coincident source or artefact), many clusters are relatively\npure. For the MiraBest case, the accuracy of the fully unsupervised\nclustering method is close to that of a supervised KNN classifier.\nThis approach could be useful for quickly building initial training\nsets for subsequent supervised learning algorithms, which is cur-\nrently a laborious process. While we did not explore this application\nhere, one could imagine using the probability of a particular source\nbelonging to a cluster (which is given by BGMM) to determine\nwhether to include the source in the training set, or request an oracle\n(i.e. an expert or citizen scientist) to identify the source. A clustering\nanalysis could also reveal unexpected groupings of sources in large,\ndeep surveys.\nFor the Galaxy Zoo DECaLS dataset, we also applied the as-\ntronomaly framework to detect anomalies. The only Galaxy Zoo\ndecision tree category we had access to which could be considered\nanomalous was that of mergers, so we aimed to use anomaly detec-\ntion to find merger candidates. We found that astronomaly was able\nto use the BYOL-trained features to efficiently locate merger candi-\ndates, although this was using a relatively low cut on probability of\nbeing considered a merger (which was the same cut used in Darg\net al. (2010)). We also showed how a similarity search can quickly\nidentify candidate ring galaxies, another rare type of source. While\nunsupervised methods generally will not outperform a trained clas-\nsifier, for cases where training data is spare or the object in question\nis rare, the BYOL-derived features are clearly effective at locating\nobjects of interest.\nWe found that fine-tuning the BYOL algorithm from the weights of\na pretrained network far outperforms the more common approach of\ninitialising the weights randomly. Interestingly, for the MiraBest data\nwe found that ImageNet weights outperformed those from BYOL\ntrained on Galaxy Zoo DECaLS, despite the optical dataset being\nfar more similar to the radio than the terrestrial one. This could be\ndue to the much larger dataset when performing the initial training\nor the fact that the network in question was trained to classify a\nlarge number of diverse classes. This question could be answered by\ninitialising the weights from a network trained with BYOL or another\nself-supervised learning method, rather than supervised learning, to\nidentify which approach produces the most useful starting weights.\nLess surprisingly, we found a network trained (using BYOL) on a\nlarge set of radio galaxies outperformed all others when fine-tuned\non MiraBest. This bodes well for the idea of providing foundation\nmodels, trained on large datasets, for astronomical data which can\nthen be fine-tuned to any specific dataset and used for downstream\ntasks.\nThe key challenge in applying unsupervised methods to image\ndata is usually feature extraction. This work demonstrates that this\nchallenge can be overcome by leveraging self-supervised methods to\nextract meaningful representations of the data. These representations\nenable automated clustering, which in turn allows for a rapid removal\nof artefacts, a faster route to labelled training sets and the potential\nfor discovering new patterns in the data. The same representations\ncan also be used to find rare sources and even discover new classes\nof objects with minimal human intervention.\nACKNOWLEDGEMENTS\nML and KM acknowledge support from the South African Radio As-\ntronomy Observatory and the National Research Foundation (NRF)\ntowards this research. Opinions expressed and conclusions arrived\nat, are those of the authors and are not necessarily to be attributed to\nthe NRF.\nAuthor contribution statements: KM lead the research and devel-\nopment, including conceptualisation of many of the key ideas, and\nproduced an early draft of the paper. ML supervised the project, per-\nformed the analysis in Section 6.2, Section 7 and Section 8.3 and\nwrote the majority of the paper.\nThis research made use of the python programming language and\nthe following open source packages: Numpy, SciPy (Jones et al.\n2001), Matplotlib (Hunter 2007), Seaborn (Waskom 2021), scikit-\nlearn (Pedregosa et al. 2011), Pandas (McKinney 2010), Astropy (The\nAstropy Collaboration et al. 2013, 2018), umap-learn (Sainburg et al.\n2021), PyTorch (Paszke et al. 2019) and BYOL-PyTorch (Chen &\nHe 2020). The authors also acknowledge the surprising helpfulness\nof ChatGPT10 in coming up with ideas for the title based on the\n(human-written) abstract. It was not used for any other aspect of the\npaper.\nWe acknowledge the use of the ilifu cloud computing facility\n– www.ilifu.ac.za, a partnership between the University of Cape\nTown, the University of the Western Cape, the University of Stel-\nlenbosch, Sol Plaatje University and the Cape Peninsula University\nof Technology. The Ilifu facility is supported by contributions from\nthe Inter-University Institute for Data Intensive Astronomy (IDIA – a\npartnership between the University of Cape Town, the University of\nPretoria and the University of the Western Cape, the Computational\nBiology division at UCT and the Data Intensive Research Initiative\nof South Africa (DIRISA).\nThis publication uses data generated via the Zooniverse.org plat-\nform, development of which is funded by generous support, including\na Global Impact Award from Google, and by a grant from the Alfred\nP. Sloan Foundation.\nThe Legacy Surveys consist of three individual and complemen-\ntary projects: the Dark Energy Camera Legacy Survey (DECaLS;\nProposal ID #2014B-0404; PIs: David Schlegel and Arjun Dey),\nthe Beĳing-Arizona Sky Survey (BASS; NOAO Prop. ID #2015A-\n0801; PIs: Zhou Xu and Xiaohui Fan), and the Mayall 𝑧-band Legacy\nSurvey (MzLS; Prop. ID #2016A-0453; PI: Arjun Dey). DECaLS,\nBASS and MzLS together include data obtained, respectively, at the\nBlanco telescope, Cerro Tololo Inter-American Observatory, NSF’s\nNOIRLab; the Bok telescope, Steward Observatory, University of\nArizona; and the Mayall telescope, Kitt Peak National Observatory,\nNOIRLab. Pipeline processing and analyses of the data were sup-\nported by NOIRLab and the Lawrence Berkeley National Laboratory\n(LBNL). The Legacy Surveys project is honoured to be permitted\nto conduct astronomical research on Iolkam Du’ag (Kitt Peak), a\nmountain with particular significance to the Tohono O’odham Na-\ntion. NOIRLab is operated by the Association of Universities for\nResearch in Astronomy (AURA) under a cooperative agreement\nwith the National Science Foundation. LBNL is managed by the\nRegents of the University of California under contract to the U.S.\nDepartment of Energy. This project used data obtained with the\nDark Energy Camera (DECam), which was constructed by the Dark\nEnergy Survey (DES) collaboration. Funding for the DES Projects\nhas been provided by the U.S. Department of Energy, the U.S. Na-\ntional Science Foundation, the Ministry of Science and Education of\n10 https://chat.openai.com/\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n19\nSpain, the Science and Technology Facilities Council of the United\nKingdom, the Higher Education Funding Council for England, the\nNational Center for Supercomputing Applications at the University\nof Illinois at Urbana-Champaign, the Kavli Institute of Cosmolog-\nical Physics at the University of Chicago, Center for Cosmology\nand Astro-Particle Physics at the Ohio State University, the Mitchell\nInstitute for Fundamental Physics and Astronomy at Texas A&M\nUniversity, Financiadora de Estudos e Projetos, Fundacao Carlos\nChagas Filho de Amparo, Financiadora de Estudos e Projetos, Fun-\ndacao Carlos Chagas Filho de Amparo a Pesquisa do Estado do\nRio de Janeiro, Conselho Nacional de Desenvolvimento Cientifico e\nTecnologico and the Ministerio da Ciencia, Tecnologia e Inovacao,\nthe Deutsche Forschungsgemeinschaft and the Collaborating Institu-\ntions in the Dark Energy Survey. The Collaborating Institutions are\nArgonne National Laboratory, the University of California at Santa\nCruz, the University of Cambridge, Centro de Investigaciones En-\nergeticas, Medioambientales y Tecnologicas-Madrid, the University\nof Chicago, University College London, the DES-Brazil Consor-\ntium, the University of Edinburgh, the Eidgenossische Technische\nHochschule (ETH) Zurich, Fermi National Accelerator Laboratory,\nthe University of Illinois at Urbana-Champaign, the Institut de Cien-\ncies de l’Espai (IEEC/CSIC), the Institut de Fisica d’Altes Energies,\nLawrence Berkeley National Laboratory, the Ludwig Maximilians\nUniversitat Munchen and the associated Excellence Cluster Universe,\nthe University of Michigan, NSF’s NOIRLab, the University of Not-\ntingham, the Ohio State University, the University of Pennsylvania,\nthe University of Portsmouth, SLAC National Accelerator Labora-\ntory, Stanford University, the University of Sussex, and Texas A&M\nUniversity. BASS is a key project of the Telescope Access Program\n(TAP), which has been funded by the National Astronomical Obser-\nvatories of China, the Chinese Academy of Sciences (the Strategic\nPriority Research Program “The Emergence of Cosmological Struc-\ntures” Grant # XDB09000000), and the Special Fund for Astronomy\nfrom the Ministry of Finance. The BASS is also supported by the Ex-\nternal Cooperation Program of Chinese Academy of Sciences (Grant\n# 114A11KYSB20160057), and Chinese National Natural Science\nFoundation (Grant # 12120101003, # 11433005). The Legacy Sur-\nvey team makes use of data products from the Near-Earth Object\nWide-field Infrared Survey Explorer (NEOWISE), which is a project\nof the Jet Propulsion Laboratory/California Institute of Technology.\nNEOWISE is funded by the National Aeronautics and Space Ad-\nministration. The Legacy Surveys imaging of the DESI footprint is\nsupported by the Director, Office of Science, Office of High En-\nergy Physics of the U.S. Department of Energy under Contract No.\nDE-AC02-05CH1123, by the National Energy Research Scientific\nComputing Center, a DOE Office of Science User Facility under the\nsame contract; and by the U.S. National Science Foundation, Divi-\nsion of Astronomical Sciences under Contract No. AST-0950945 to\nNOAO.\nDATA AVAILABILITY\nThe data used in this paper are publicly available at https://\nzenodo.org/record/4573248 for the Galaxy Zoo DECaLS data\nand https://zenodo.org/records/5588282 for the MiraBest\ndata.\nREFERENCES\nAttias H., 1999, Advances in neural information processing systems, 12\nBradski G., 2000, Dr. Dobb’s Journal of Software Tools\nChen X., He K., 2020, Exploring Simple Siamese Representation Learning\n(arXiv:2011.10566)\nChen T., Kornblith S., Norouzi M., Hinton G., 2020, A Simple\nFramework\nfor\nContrastive\nLearning\nof\nVisual\nRepresentations,\ndoi:10.48550/ARXIV.2002.05709, https://arxiv.org/abs/2002.\n05709\nCheng T.-Y., Huertas-Company M., Conselice C. J., Aragón-Salamanca A.,\nRobertson B. E., Ramachandra N., 2021, MNRAS, 503, 4446\nCover T., Hart P., 1967, IEEE Transactions on Information Theory, 13, 21\nDarg D. W., et al., 2010, Monthly Notices of the Royal Astronomical Society,\n401, 1043\nDey A., et al., 2019, The Astronomical Journal, 157, 168\nDomínguez Sánchez H., Huertas-Company M., Bernardi M., Tuccillo D.,\nFischer J. L., 2018, MNRAS, 476, 3661\nEtsebeth V., Lochner M., Walmsley M., Grespan M., 2023, Astrono-\nmaly at Scale: Searching for Anomalies Amongst 4 Million Galaxies\n(arXiv:2309.08660)\nFanaroff B. L., Riley J. M., 1974, MNRAS, 167, 31P\nFielding\nE.,\nNyirenda\nC.\nN.,\nVaccari\nM.,\n2022,\nin\n2022\nInter-\nnational\nConference\non\nElectrical.\np.\n1\n(arXiv:2206.06165),\ndoi:10.1109/ICECET55527.2022.9872611\nFix E., Hodges J., 1951, Discriminatory Analysis: Nonparametric Discrim-\nination: Consistency Properties. USAF School of Aviation Medicine,\nhttps://books.google.co.za/books?id=4XwytAEACAAJ\nGrill J.-B., et al., 2020, Advances in neural information processing systems,\n33, 21271\nGuérin J., Thiery S., Nyiri E., Gibaru O., Boots B., 2021, Neurocomputing,\n423, 551\nGupta N., Huynh M., Norris R. P., Wang X. R., Hopkins A. M., Andernach\nH., Koribalski B. S., Galvin T. J., 2022, Publ. Astron. Soc. Australia, 39,\ne051\nHayat M. A., Stein G., Harrington P., Lukić\nZ., Mustafa M., 2021, The\nAstrophysical Journal Letters, 911, L33\nHe K., Zhang X., Ren S., Sun J., 2016, in 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). pp 770–778,\ndoi:10.1109/CVPR.2016.90\nHotan A. W., et al., 2021, Publ. Astron. Soc. Australia, 38, e009\nHotelling H., 1933, Journal of Educational Psychology, 24, 417\nHuertas-Company M., Sarmiento R., Knapen J., 2023, A brief review of\ncontrastive learning applied to astrophysics (arXiv:2306.05528)\nHunter J. D., 2007, Computing in Science and Engineering, 9, 99\nJonas J., MeerKAT Team 2016, in MeerKAT Science: On the Pathway to the\nSKA. p. 1\nJones E., Oliphant T., Pearu P., Others 2001, SciPy: Open Source Scientific\nTools for Python, http://www.scipy.org/\nLahav O., Naim A., Sodré L. J., Storrie-Lombardi M. C., 1996, MNRAS,\n283, 207\nLeCun Y., Bengio Y., Hinton G., 2015, Nature, 521, 436\nLintott C. J., et al., 2008, Monthly Notices of the Royal Astronomical Society,\n389, 1179\nLochner M., Bassett B. A., 2021, Astronomy and Computing, 36, 100481\nLochner M., Rudnick L., Heywood I., Knowles K., Shabala S. S., 2023,\nMonthly Notices of the Royal Astronomical Society, 520, 1439\nMcInnes L., Healy J., Melville J., 2018, arXiv e-prints, p. arXiv:1802.03426\nMcKinney W., 2010, Data Structures for Statistical Computing in Python,\nhttp://conference.scipy.org/proceedings/scipy2010/\nmckinney.html\nMiraghaei H., Best P. N., 2017, MNRAS, 466, 4346\nNaim A., Ratnatunga K. U., Griffiths R. E., 1997, arXiv e-prints, pp astro–\nph/9704012\nPaszke A., et al., 2019, arXiv e-prints, p. arXiv:1912.01703\nPearson K., 1901, The London, Edinburgh, and Dublin Philosophical Maga-\nzine and Journal of Science, 2, 559\nPedregosa F., et al., 2011, Journal of Machine Learning Research, 12, 2825\nPolsterer K. L., Gieseke F., Doser B., 2019, PINK: Parallelized rotation and\nflipping INvariant Kohonen maps (ascl:1910.001)\nPorter F. A. M., Scaife A. M. M., 2023, RAS Techniques and Instruments, 2,\nMNRAS 000, 1–22 (2023)\n20\nK. Mohale & M. Lochner\n293\nRalph N. O., et al., 2019, PASP, 131, 108011\nRussakovsky O., et al., 2014, ImageNet Large Scale Visual Recog-\nnition Challenge, doi:10.48550/ARXIV.1409.0575, https://arxiv.\norg/abs/1409.0575\nSainburg T., McInnes L., Gentner T. Q., 2021, Neural Computation, 33, 2881\nSarmiento R., Huertas-Company M., Knapen J. H., Sánchez S. F., Domínguez\nSánchez H., Drory N., Falcón-Barroso J., 2021, ApJ, 921, 177\nSchawinski K., et al., 2009, Proceedings of the International Astronomical\nUnion, 5, 438\nSimmons B. D., et al., 2014, Monthly Notices of the Royal Astronomical\nSociety, 445, 3466\nSlĳepcevic I. V., Scaife A. M. M., Walmsley M., Bowles M., Wong O. I.,\nShabala S. S., White S. V., 2023, Radio Galaxy Zoo: Building a multi-\npurpose foundation model for radio astronomy with self-supervised learn-\ning (arXiv:2305.16127)\nSpindler A., Geach J. E., Smith M. J., 2021, MNRAS, 502, 985\nStein G., Harrington P., Blaum J., Medan T., Lukic Z., 2021, arXiv e-prints,\np. arXiv:2110.13151\nStein G., Blaum J., Harrington P., Medan T., Lukić Z., 2022, ApJ, 932, 107\nThe Astropy Collaboration et al., 2013, A&A, 558, A33\nThe Astropy Collaboration et al., 2018, The Astronomical Journal, 156, 123\nThe Astropy Collaboration et al., 2022, The Astrophysical Journal, 935, 167\nTian Y., Chen X., Ganguli S., 2021, arXiv e-prints, p. arXiv:2102.06810\nVafaei Sadr A., Bassett B. A., Sekyi E., 2022, arXiv e-prints, p.\narXiv:2210.16334\nWalmsley M., et al., 2020a, Galaxy Zoo DECaLS: Detailed Visual Mor-\nphology Measurements from Volunteers and Deep Learning for 314,000\nGalaxies, doi:10.5281/zenodo.4196267, https://doi.org/10.5281/\nzenodo.4196267\nWalmsley M., et al., 2020b, Galaxy Zoo DECaLS: Detailed Visual Mor-\nphology Measurements from Volunteers and Deep Learning for 314,000\nGalaxies, doi:10.5281/zenodo.4573248, https://doi.org/10.5281/\nzenodo.4573248\nWalmsley M., et al., 2022, Monthly Notices of the Royal Astronomical Soci-\nety, 513, 1581\nWaskom M. L., 2021, Journal of Open Source Software, 6, 3021\nWebb S., et al., 2020, Monthly Notices of the Royal Astronomical Society,\n498, 3077–3094\nWei S., Li Y., Lu W., Li N., Liang B., Dai W., Zhang Z., 2022, PASP, 134,\n114508\nYang H.-F., et al., 2023, Research in Astronomy and Astrophysics, 23, 055006\nZhou C., Gu Y., Fang G., Lin Z., 2022, AJ, 163, 86\nd’Abrusco R., Longo G., Paolillo M., de Filippis E., Brescia M., Staiano A.,\nTagliaferri R., 2007, arXiv e-prints, pp astro–ph/0701137\nvan Haarlem M. P., et al., 2013, A&A, 556, A2\nvan der Maaten L., Hinton G., 2008, Journal of Machine Learning Research,\n9, 2579\nAPPENDIX A: UMAP HYPERPARAMETERS\nUMAP has several hyperparameters, the effects of which are dis-\ncussed in McInnes et al. (2018). In Figure A1, we show the effect,\nfor the Galaxy Zoo dataset, of varying these parameters over a very\nbroad range. Based on visual inspection, we set “number of neigh-\nbours” to 15, and the parameter “minimum distance” to 0.01 for our\nanalysis. The plots are not sensitive to moderate variations in these\nparameters but large deviations such as shown here can negatively\nimpact the structure and interpretation of UMAP plots.\nAPPENDIX B: ARTEFACTS\nThe types of artefacts detected by the clustering algorithm are a com-\nbination of those that existed in the data as well as those introduced\nAugmentation\nKNN accuracy\nRandom resized crops\n95.30 ± 0.07\nRandom rotation\n93.88 ± 0.07\nRandom horizontal and vertical flips\n94.73 ± 0.07\nRandom Gaussian blur\n94.66 ± 0.07\nAll\n95.89 ± 0.07\nTable C1. The effect of individual augmentations on KNN accuracy after 10\nepochs.\nby our preprocessing. Cluster 15 contains colour-corrupted images,\nCluster 19 contains a mixture of images with bright flares and other\nartefacts. Clusters 8, 11, 9, 17, 7 and 13 display artefacts introduced\nwhen the resizing function fails to locate a galaxy source, leading to\na resized image that appears pixelated as shown in Figure B1.\nComparing the images before and after resizing suggests that this\nhappened in images that either had a pre-existing artefact, bright\nsources located at the edge of the image or large sources that fill\nthe entire image. This is because our preprocessing assumes that a\nsource of interest is a bright object located at the centre of the image.\nOur implementation avoids cropping out important galaxy features\nby detecting a square that is twice the size of the smallest square\nbounding a contour fit.\nAs our results show, the preprocessing is sufficient for most of\nthe sources in the dataset due to their small sizes relative to the\nfull images. In some cases, however, the detected bounding squares\ncross the boundaries of an image and may contain negative values in\nsome of their coordinates on an image grid. Attempting to crop such\na bounding square produces the results shown in Figure B1a and\nFigure B1b. This is because the crops are obtained by indexing the\nimage as an array, and Python interprets negative indices as counting\nfrom the right (or bottom) instead of the left (or top). This results in\ncropping an image patch that does not contain the galaxy source and\nis usually much smaller, causing pixelation when the image is resized.\nSimilar pixelation is observed in images that have missing sources,\nsuch as Figure B1c. These specific artefacts could have been detected\nprogrammatically during preprocessing after they were discovered,\nbut given that most of them were caused by existing artefacts in the\nimage anyway, we elected to leave them in and instead use them as a\nway to test unsupervised artefact detection.\nAPPENDIX C: BYOL HYPERPARAMETER TUNING\nWe set most BYOL hyperparameter values to be the same as used in\nGrill et al. (2020), seeing no reason to change them. The batch size\ndiffers due to computational restrictions and the number of epochs is\nselected based on convergence. We did, however, need to change the\nlearning rate in order to obtain rapid convergence to a high quality\nrepresentation. Figure C1 shows the KNN accuracy for the evaluation\nsubset of the Galaxy Zoo data for different learning rates, showing\nthat our choice of 1 × 10−4 produces excellent performance.\nTable C1 shows the effect of using individual augmentations com-\npared with using all of them on the Galaxy Zoo evaluation set. This\nshows that the exact choice of augmentations did not have a large\nimpact on the accuracy. However, we did find that using all augmen-\ntations as described in the text produced the most reliable feature\nspace.\nMNRAS 000, 1–22 (2023)\nEnabling Unsupervised Discovery in Astronomical Images\n21\nn_neighbors = 5\nn_neighbors = 15 \n (Our choice)\nmin_dist = 0.0\nn_neighbors = 1280\nmin_dist = 0.01 \n (Our choice)\nmin_dist_label = 0.8\nFigure A1. UMAP produces different embeddings depending on the choices of the hyperparameters “number of neighbours” and “minimum distance”. Our\nchosen UMAP embedding shows structures that are present for larger parameter values of “number of neighbours” and “minimum distance”, and are therefore\nnot artefacts of UMAP.\nClass\nF1 score (ImageNet)\nF1 score (Random)\nSpirals\n0.9637 ± 0.0008\n0.9531 ± 0.0009\nRound ellipticals\n0.9526± 0.0007\n0.9283 ± 0.0010\nEdge on galaxies\n0.9464 ± 0.0008\n0.9161 ± 0.0011\nTable D1. The mean F1 scores and standard errors obtained over 50 seeded\nruns for the Galaxy Zoo evaluation set, fine-tuning BYOL from ImageNet and\nRandom weights respectively.\nRepresentations\nF1 scores (FRI)\nF1 scores (FRII)\nRadio Galaxy Zoo\n0.856 ± 0.004\n0.870 ± 0.004\nImageNet\n0.831 ± 0.004\n0.834 ± 0.005\nBYOL Galaxy Zoo\n0.775 ± 0.005\n0.792 ± 0.005\nRandom Initial\n0.695 ± 0.006\n0.722 ± 0.005\nTable D2. The mean F1 scores and standard errors obtained over 50 seeded\nruns for the final MiraBest representations.\nAPPENDIX D: F1 SCORES\nTo further substantiate the performance of our algorithms on the\nlabelled subsets and investigate any class differences that may occur,\nwe make use of the F1 score. The F1 score is the harmonic mean\nbetween precision (𝑝, the number of true positives divided by the\nnumber of predicted positives) and the recall (𝑟, the number of true\npositives divided by the total number of positives in the sample):\nF1 = 2 𝑝.𝑟\n𝑝+ 𝑟.\n(D1)\nTable D1 shows the F1 score per class for the random and ImageNet\ninitialisations of BYOL. This table supports the conclusion that Ima-\ngeNet weights provide a useful starting point for fine-tuning BYOL.\nIt also shows that this conclusion is not class dependent.\nTable D2 shows the F1 scores for the MiraBest dataset for each of\nthe starting weights considered in the text, supporting the conclusions\ndrawn from the accuracy plots.\nMNRAS 000, 1–22 (2023)\n22\nK. Mohale & M. Lochner\n(a) Artefact detected in Cluster 13\n(b) Artefact detected in Cluster 19\n(c) Artefact detected in Cluster 19\nFigure B1. Examples of artefacts that were observed in the results. Large\nsources (a), bright sources on the edges of images (b) and pre-existing artefacts\n(c) contributed to the existence of the artefacts observed. The images on the\nleft show the contours that were detected and highlight the resulting crop as\na red box. The images in (a) and (b) show the crop that resulted when the\ndetected bounding square crossed the boundary of the image, causing Python\nto reinterpret the crop according to indexing rules. The images on the right\nshow the corresponding final images after the preprocessing. The apparent\npixelation is caused by resizing small crops that result from attempts to crop\nout of bounds and missing sources.\nAPPENDIX E: CLUSTER ANALYSIS\nTo provide more numerical evidence for the qualitative results from\nthe clustering analysis presented in Table 2, we made use of our eval-\nuation set which had a strict set of cuts applied to the citizen scientist\nvotes to create a very pure subset of three different classes: ellipticals,\nspirals and edge-on galaxies. It can be see that our qualitative conclu-\nsions correspond well to the numerical results from this small subset\nof data. Almost all the spiral galaxies reside in clusters 0, 5 and 18\nwhile the ellipticals dominate clusters 1, 12 and 14. Clusters 2 and\n10 are fairly pure samples of edge-on galaxies while the remaining\nclusters are more mixed.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nEpoch\n55\n60\n65\n70\n75\n80\n85\n90\n95\nKNN accuracy\n1e-5\n1e-4 (Our choice)\n1e-3\n1e-2\n1e-1\nFigure C1. The accuracy of the KNN algorithm for the evaluation subset of\nthe Galaxy Zoo data as a function of epoch for different learning rates.\n0\n1\n2\n3\n4\n5\n6\n10 12 14 16 18\nCluster number\n0\n10\n20\n30\n40\n50\nPercentage of evaluation set class\nEllipticals\nSpirals\nEdge-on\nFigure E1. The break down of what percentage of each class in the evaluation\nset lies in which cluster. The quantitative results using the “clean” evaluation\nset broadly agree with the qualitative conclusions from visual inspection\ndisplayed in Table 2 and the broader vote distributions in Figure 9. Artefact\nclusters are excluded.\nThis paper has been typeset from a TEX/LATEX file prepared by the author.\nMNRAS 000, 1–22 (2023)\n",
  "categories": [
    "astro-ph.IM",
    "astro-ph.GA"
  ],
  "published": "2023-11-23",
  "updated": "2024-04-19"
}