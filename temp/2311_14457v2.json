{
  "id": "http://arxiv.org/abs/2311.14457v2",
  "title": "How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation",
  "authors": [
    "Zicong Zhao"
  ],
  "abstract": "Deep reinforcement learning has gradually shown its latent decision-making\nability in urban rail transit autonomous operation. However, since\nreinforcement learning can not neither guarantee safety during learning nor\nexecution, this is still one of the major obstacles to the practical\napplication of reinforcement learning. Given this drawback, reinforcement\nlearning applied in the safety-critical autonomous operation domain remains\nchallenging without generating a safe control command sequence that avoids\noverspeed operations. Therefore, a SSA-DRL framework is proposed in this paper\nfor safe intelligent control of urban rail transit autonomous operation trains.\nThe proposed framework is combined with linear temporal logic, reinforcement\nlearning and Monte Carlo tree search and consists of four mainly module: a\npost-posed shielding, a searching tree module, a DRL framework and an\nadditional actor. Furthermore, the output of the framework can meet speed\nconstraint, schedule constraint and optimize the operation process. Finally,\nthe proposed SSA-DRL framework for decision-making in urban rail transit\nautonomous operation is evaluated in sixteen different sections, and its\neffectiveness is demonstrated through an ablation experiment and comparison\nwith the scheduled operation plan.",
  "text": "arXiv:2311.14457v2  [cs.AI]  6 Jan 2024\n1\nHow to ensure a safe control strategy? Towards a\nSRL for urban transit autonomous operation\nZicong Zhao, Graduate Student Member, IEEE\nAbstract—Deep reinforcement learning (DRL) has gradually\nshown its latent decision-making ability in urban rail transit\nautonomous operation. However, since reinforcement learning\ncan not neither guarantee safety during learning nor execution,\nthis is still one of the major obstacles to the practical application\nof reinforcement learning. Given this drawback, reinforcement\nlearning applied in the safety-critical autonomous operation\ndomain remains challenging without generating a safe control\ncommand sequence that avoids overspeed operations. Therefore,\na SSA-DRL framework is proposed in this paper for safe intel-\nligent control of urban rail transit autonomous operation trains.\nThe proposed framework is combined with linear temporal logic,\nreinforcement learning, Monte Carlo tree search and consists of\nfour mainly module: a post-posed shield, a searching tree module,\nan additional actor and a DRL framework. Furthermore, the\noutput of the framework can meet speed constraint, schedule\nconstraint and optimize the operation process. Finally, the pro-\nposed SSA-DRL framework for decision-making in urban rail\ntransit autonomous operation is evaluated in sixteen different\nsections, and its effectiveness is demonstrated through the basic\nsimulation and additional experiment.\nIndex Terms—Safe Reinforcement Learning, Urban Rail Tran-\nsit, Autonomous Operation, Intelligent Control.\nI. INTRODUCTION\nA\nUTONOMOUS train operation has become a hot re-\nsearch in the world to make urban rail transit smarter,\nmore efﬁcient and greener. Siemens Mobility in 2018 pre-\nsented the world’s ﬁrst autonomous tram at InnoTrans. Also,\nat InnoTrans in 2022, Thales’ rolling laboratory train Lucy can\ndetermine its precise location and speed by itself. In 2023,\nCRRC QINGDAO SIFANG CO., LTD. presented the Train\nAutonomous Circumambulate System (TACS) at Qingdao\nLine 6 and Alstom is now trying to bring GoA4 autonomous\nsystems on regional train lines. Besides these companies, rail\ndeveloped countries are also working hard on the research of\nautonomous train operation system. In 2022, Europe’s Rail\nFlagship Project R2DATO project is spending C180 million\nto develop the Next Generation Autonomous Train Control\nsystem. All in all, different companies or countries will have\ndifferent technique features for autonomous train operation,\nbut one important thing in common is that the train should\nhave the ability of self-decision or self-driving no matter in\ncommon situation or an emergence situation. How to achieve\nthe mentioned ability is now becoming a hot research.\nIn other transportation area such as Unmanned Aerial Ve-\nhicle (UAV) path planning or vehicle autonomous driving, the\nagent also needs to have the ability of self-planning or self-\nlane keeping [1], [2]. In recent years, reinforcement learning\n(RL) or deep reinforcement learning (DRL) are widely used to\ntrain an agent with self-decision ability [3]. However, unlike\nthose traditional virtual RL environment like OpenAI Gym or\nDeepMind MuJoCo [4], [5], the above transportation scenarios\nare all real and safety-critical which means that a wrong or\ndanger action may lead to an unacceptable result for example\nthe UAV collides the infrastructures, the cars have a collision\nand the train runs over the limit speed. Unluckily, traditional\nRL algorithms may not guarantee the safety not only in\ntraining process but also in execution process which results\nthe main obstacle that prevents RL from being widely used in\nreal world [6].\nMoreover, unlike the UAV path planning or vehicle au-\ntonomous driving, the intermediate process of autonomous\ntrain operation is also important. In UAV path planning,\nresearchers always focus on how to ﬁnd the shortest and\nsafest path to reach the destination and in vehicle autonomous\ndriving, researchers always focus on how to keep or change\nthe lane to avoid collision. But for autonomous train operation,\nthings are not the same. Because the trains must obey the\nschedule and there are always several optimization objectives\nsuch as the energy consumption and passenger comfort. This\nmakes it harder to design a self-decision algorithm to trade-\noff between the ﬁnal object (arrive at the destination safe and\non time) and the optimization object (minimize the energy\nconsumption and keep the passengers comfortable). And in\nrecent years, with the development of artiﬁcial intelligence\n(AI), the AI community has realized that if the failure can\ncause or damage to an AI system, a fail-safe mechanism or\nfallback plan should be an important part of the design of the\nAI system [7]–[9]. In some AI guidelines published in recent\nyears, this mechanism has become an important requirement\nof AI [10].\nIn this paper, our main purpose is to propose a self-decision\nalgorithm for autonomous train operation with three other\nabilities, 1) objectives are optimized, 2) safety is ensured and\n3) decisions can be explained. Safe reinforcement learning\n(SRL), a subﬁeld of RL is used in this paper to design\nthe algorithm. The detailed contributions are introduced in\nSectionI.B.\nA. Related Work\nIn urban transit optimal control area, most studies can be\nregarded as an extension of the study of speed proﬁle opti-\nmization. In the past few decades, due to the characteristics of\nspeed tracking control, researchers mainly focused on how to\noptimize the speed proﬁle, so that the control algorithm, espe-\ncially PID control, can achieve better control effect. Up to now,\n2\nmost studies are based on Pontryagin’s Maximum Principle\n(PMP) to analysis the switching of working conditions such\nas maximum acceleration (MA), coasting (CO), maximum\nbraking (MB) to optimize the speed proﬁle [11]–[16]. Such\noptimization methods are also called energy-efﬁcient train\ncontrol (EETC) [17].\nWith the development of intelligent control theory and the\nrequirement of self decision-making in autonomous operation,\nintelligent control methods represented by dynamic program-\nming (DP) and RL are widely studied to improve automation\nlevel of traditional ATO system.\nAs the basis of RL and DP, Bellman optimal equation is\nused to build a multi-stage decision making problem for train\noperation control [18]. Compared with the famous bionic op-\ntimization algorithm such as genetic algorithm and ant colony\noptimization, DP has a better performance under different\noperation time and inter-section distance [19]. With the de-\nvelopment of train motors and controllers, continuous control\ncommands are more and more used in urban rail transit, which\nmakes the traditional RL or DP method unsuitable. DRL and\napproximate dynamic programming (ADP) are then used to\nhandle this problem. For the optimal control of heavy haul\ntrain with uncertain environment conditions, the maximum\nutility of regenerative braking energy and the optimization\nof speed proﬁle with parametric uncertainty, ADP all has\na good performance [20]–[22]. As for the speciﬁc use of\nDRL, researches on one hand use it directly to output control\ncommand for train [23]–[25], and on the other hand combine\nit with other framework such as the expert system or history\nexperience to correct the given control command [26]–[28].\nIn RL research area, with the development of computer\nscience and graphics process unit (GPU), several famous\nalgorithms represented by the Q-learning, actor-critic (AC),\ndeep deterministic policy gradient (DDPG) and soft actor-\ncritic (SAC) have achieved tremendous achievement in two-\nplayer game and computer game [29]–[32]. The two most\nfamous researches are AlghaGo and AlphaZero in Chess and\nAlphaStar in StarCraft for they have almost beaten every\nhuman player without pressure [33]–[35].\nThough DRL has shown great potential in decision-making\narea, researchers have also found that RL algorithms do\nnot necessarily guarantee safety during learning or execution\nphases, which leads to an unavoidable drawback for safety-\ncritical application in real world such as robot control [36],\n[37]. To get over this issue, researchers have studied to ensure\nreasonable system performance and respect safety constraints\nduring the learning and deployment processes, such researches\nare so-called SRL [38]. Considering there are many different\nways to classify SRL algorithms, application area will be used\nin this paper to make the literature review.\nIn the car autonomous driving area, many methods have\nbeen proposed for autonomous driving based on modern,\nadvanced techniques. Traditional motion planning and RL\nmethods are combined to perform better than pure RL or\ntraditional methods [39]. Different with [39], a third layer\ncalled risk neural network is added to AC algorithm to\nrealize safe autonomous driving [40]. Moreover, control barrier\nfunctions (CBF) and Monte Carlo Tree Search (MCTS) are\nalso used for safe autonomous driving [41], [42].\nIn the robotics area, the safety of robot is not considered\nas an optimization objective in the past researches, the study\nof SRL has established a bridge between the simulation and\napplication. Optlater ensures safe action sets that a robot\ncan only taken from [43]. Safe exploration derived from risk\nfunction is used to construct PI-SRL and PS-SRL algorithm,\nwhich makes SRL based robot walking come true [44], [45].\nSRL is also widely used in other areas. In recommender\nsystem, SRL is deployed to optimize the healthy recommen-\ndation sequences by utilizing a policy gradient method [46].\nIn wireless security, an Inter-agent transfer learning based\nSRL method is proposed [47]. In UAV control, a brain-\ninspired reinforcement learning model (RSNN) is proposed\nto realize self-organized decision-making and online collision\navoidance [48]. In high speed train operation optimization,\na Shield SARSA algorithm is proposed to plan an energy-\nefﬁcient speed proﬁle without overspeed operation [49]. In\ndiabetes treatment, a new mathematical problem formulation\nframework called Seldonian optimization problem is proposed,\nand it is used in the optimization of insulin dosing for type 1\ndiabetes treatment [50].\nB. Problems and Contributions\nThrough the introduction and literature review, two infor-\nmation can be acquired are that SRL is now widely used\nin safety-critical area to make RL based method more real\nworld realizable and in urban rail transit area there are few\nresearches typically considering how to construct a SRL based\nintelligent control method for autonomous operation. Table I\nsummarized the safety protection methods for solving train\noperation optimization or control problems using RL in recent\nyears. It is clear that the widely used method to prevent\noverspeed operation nowadays are adding a punishment or\nset the speed equal to the limit speed. However, the effect of\na punishment may be inﬂuenced by the value of punishment\nweight and can only be known after several simulations which\nis obvious unsuitable for the operation in real world. When\nsetting the speed equal to limit speed, this behavior may break\nthe principle that at each time step the agent receives some\nrepresentation of the environment’s state and on that basis\nselects an action [3]. Moreover, since this approach ignores the\nbehavior policy, it may not maximize the long term reward.\nTABLE I\nTYPICAL SAFETY PROTECTION METHODS\nReferences\nSafety Protection Method\n[27]\nIf v > vl, adapt minimum deceleration\n[28]\nIf v > vl, using reference system to brake\n[51]\nIf v > vl, safety index equal to 0\n[52]\nIf v > vl, add an overspeed punishment\n[53]\nIf v > vl, regard as infeasible\n[54]\nIf v > vl, reset environment\n[55]\nv ≤vl in model but not mentioned in RL algorithm\n[56]\nv ≤vl in model but not mentioned in RL algorithm\n[57]\nIf v > vl, add an overspeed punishment -1\n[58]\nAdd an avoidance punishment\n[59]\nIf v > vl, add an operspeed punishment\n3\nThen in this paper, a SRL framework called SSA-DRL is\nproposed for safe self-decision making of urban rail transit\nautonomous operation. The framework is consists of a Shield,\na Searching tree, an Additional safe actor and a DRL frame-\nwork. The main contributions of this paper can be summarized\nas follows:\n• The proposed SSA-DRL framework enables agent to\nlearn safe control policies and ensure schedule constraints\nand operation efﬁciency.\n• The safe actions are get by a white box searching tree\nmodel and an iterative formula that can be explained.\n• The proposed SSA-DRL framework can effectively re-\nduce the number of times that the protection mechanism\nworks which means that the ﬁnal agent has self-protection\nability.\n• The proposed SSA-DRL framework has transferability\nand robustness and is easy to deploy.\nThe remainder is organized as follows. In Section II, the\npreliminaries are introduced. In Section III, the proposed SSA-\nDRL framework is elaborated. In Section IV, simulation results\nare discussed and in Section V the conclusions are given.\nII. PRELIMINARIES\nA. Markov Decision Process\nA ﬁnite Markov Decision Process is usually denoted by the\n5-tuple (S, s0, A, p, R) with a ﬁnite state set S = {s0, ..., sn},\na unique initial state s0\n∈S, a ﬁnite action set A =\n{a1, ..., an}, a dynamic function p : S × R × S × A →[0, 1]\nand a reward function r : S × A →R.\nThe solving of a RL task is to ﬁnd a mapping called\npolicy written as π from states to probabilities of selecting\neach possible action to receive a lot of rewards over a long\nepisode. For the optimal policy, it is better than or equal to all\nother policies. Noted that there may be more than one optimal\npolicy, thus all the optimal policies are denoted by π∗. All the\noptimal policies share the same optimal state-value function\nv∗and optimal action-value function q∗deﬁned as\n( v∗(s) .= max\nπ vπ (s)\nq∗(s, a) .= max\nπ qπ (s, a)\n(1)\nB. State Value and Action Value\nValue function is widely used in RL to evaluate a given state\nor a state-action pair. Since the return an agent can get almost\ndepend on the chosen action, thus value function is associated\nwith the policy. The function vπ (s), which calculates the value\nof state s under policy π is called a state value function and\nis denoted by (2).\nvπ (s) .= Eπ [Gt | St = s]\n= Eπ\n\" ∞\nX\nk=0\nγkRt+k+1 | St = s\n#\n, for all s ∈S\n(2)\nSimilarly, the value function qπ calculates the value of taking\nan action a at state s under policy π and is denoted by (3).\n.\nqπ(s, a) .= Eπ [Gt | St = s, At = a]\n= Eπ\n\" ∞\nX\nk=0\nγkRt+k+1 | St = s, At = a\n#\n(3)\nE [·] denotes the expected value of a random variable.\nC. Off Policy DRL\nIn off-policy RL, the agent uses a behavior policy for action\nselection during the learning process, and a target policy for\nupdating the policy. This means that the policies used by the\nagent while learning are different from those actually executed.\nThe core feature of off-policy RL is to seek the global optimal\nvalue. In DRL especially, due to the introduction of replay\nbuffer, off-policy DRL algorithms are more common. The\nDDPG and SAC algorithms are two examples of off-policy\nmethods based on policy gradient framework AC, which are\nused as benchmarks in this paper.\nDDPG is an deterministic algorithm typically designed for\ncontinuous action set which concurrently learns a Q-function\nQ (s, a) and a policy. It has two AC structures and uses off-\npolicy data and the Bellman equation to learn the Q-function\nthen uses it to learn the policy. At each state s, the optimal\naction is acquired by solving (4).\na∗(s) = argmax\na Q∗(s, a)\n(4)\nSAC is an algorithm that optimizes a stochastic policy in\nan off-policy way, forming a bridge between stochastic policy\noptimization and DDPG-style approaches [60]. Different from\nDDPG, SAC is suitable for both continuous and discrete\naction set. The core feature of SAC is entropy regularization,\nwhich means the algorithm is designed to search a trade-off\nbetween expected return and entropy. Unlike the traditional\nDRL algorithm, SAC ﬁnds the optimal policy by solving (5).\nπ∗= arg max\nπ\nE\nτ∼π\n\" ∞\nX\nt=0\nγt (R (st, at, st+1) + αH (π (·|st)))\n#\n(5)\nH (P) =\nE\nx∼P [−log P (x)]\n(6)\nH is the entropy of x calculated by its distribution P.\nAlthough both DDPG and SAC have learned good agents\non several benchmark tasks, there is no guarantee of safety\nin these algorithms, nor any other traditional off-policy RL\nalgorithm. Therefore, another purpose of this paper is to\ncombine DRL agents with other modules to both improve\ncontrol efﬁciency and ensure safety.\nD. Monte Carlo Tree Search\nMCTS is an algorithm based on tree search and Monte Carlo\nmethod for decision-making problems, widely used in the ﬁeld\nof games and RL. It simulates multiple possible states of a\ngame or problem and selects the optimal action scheme to ﬁnd\nthe best decision. MCTS iteratively simulates the subsequent\ndevelopment of a searching tree, updates the nodes in the tree\naccording to the simulated results, and selects one node by\na policy as the action for the next step. The widely used\npolicies are upper conﬁdence bounds and ǫ−greedy. The basic\nsteps of MCTS are selection, expansion, simulation and back\npropagation. In this paper, the process of MCTS is addressed\nto better suitable for the proposed framework.\n4\nPost-posed \nShield\n~\nStation1\n~\nStation N\nTrain Model\nEnvironment\nSearching Tree\nSearching Tree\nBasic DRL\nAdditional Learner\nwhole trajectory\nTraining \nAction\nExecution \nAction\n unsafe\n safe action\nReplay Buffer\nActor Critic \nFramework\nReplay Buffer2\nAdditional Actor\nPost-posed \nShield\n~\nStation1\n~\nStation N\nTrain Model\nEnvironment\nSearching Tree\nBasic DRL\nAdditional Learner\nwhole trajectory\nTraining \nAction\nExecution \nAction\n unsafe\n safe action\nReplay Buffer\nActor Critic \nFramework\nReplay Buffer2\nAdditional Actor\nFig. 1. Framework of SSA-DRL.\nE. Linear Temporal Logic\nLinear Temporal Logic (LTL) is a widely used temporal\nlogic method in areas such as formal modeling and model\nchecking [61]. It can describe constraints and temporal rela-\ntionships that need to be satisﬁed by a system in the past,\npresent, and future using time sequence operators. Therefore,\nit is particularly suitable for describing reactive systems that\ngenerate outputs based on external inputs and the system’s\ncurrent state. LTL can conveniently and accurately describe\nthe properties and constraints that a system needs to meet, and\nis typically described using linear temporal logic formulas.\nA word is typically used to express an atomic proposition\n(AP) or the negation of an AP. Alphabet Σ of AP is de-\nnoted as 2AP , where 2AP represents the power set of AP.\nSubsequently, the sets of all ﬁnite and inﬁnite sequences of\nelements from the alphabet Σ are denoted as Σ∗and Σω,\nrespectively. Important and widely used properties such as\nsafety, satisﬁability, and liveness can be deﬁned through linear\ntemporal logic formulas.\nIII. METHOD FORMULATION\nIn this section, the framework of the proposed SSA-DRL\nis ﬁrstly shown in Fig. 1. It is clearly that SSA-DRL consists\nof four main modules: a Shield based protective module, a\nsearching tree based module, a DRL module and an additional\nactor module. Then we will explain how these four modules\nwork in detail.\nA. A Post-Posed Shield\nThe Shield in this paper comes directly from [36]. The\nShield consists of ﬁnite-state reactive system, safety speciﬁ-\ncation, an observer function, a label and other components.\nFinite-State Reactive System is usually denoted by a tuple\nFS = {Q, q0, ΣI, ΣO, δ, λ}, where Q is the ﬁnite set of states,\nq0 ∈Q is the initial state, ΣI = Σ1\nI ×Σ2\nI , ΣO are the input and\noutput alphabets. Then, δ : Q × ΣI →Q, λ : Q × Σ1\nI →ΣO\nare the transition and output function respectively. Speciﬁ-\ncation ψ deﬁnes the set of all allowed traces, and once a\nsystem satisﬁes all the properties of a speciﬁcation, we can\nsay a system satisﬁes ψ. Moreover, safety speciﬁcation is\nused to construct the Shield. Formally speaking, a safety\nspeciﬁcation is a speciﬁcation that if every trace is not in the\nlanguage represented by the speciﬁcation with a preﬁx such\nthat all words starting with the same preﬁx are also not in\nthe language [36]. The above expression may be difﬁcult to\nunderstand, readers of this paper can simply recognize a safety\nspeciﬁcation holds that ”bad things will never happen” and a\nsafety automaton can be used to represent a safety speciﬁcation\n[62]. Observer function f : S →L is usually a mapping for\nan MDP M = (S, s0, A, p, R) to describe some information\nat state s and L is a ﬁnite set of label. Then, once an RL task\ncan be formulated as M = (S, s0, A, p, R) while satisfying a\nsafety speciﬁcation ψS with an observer function f : S →L,\na Shield can be modeled by a reactive system.\nPost-Posed Shield is a speciﬁc form of Shield that is set after\nthe learning algorithm as depicted in Fig. 2. It is clear that the\nactions chosen by the agent are monitored by the Shield and\nthe action violating safety speciﬁcation ψS will be replayed\nby a safe action a\n′. A new safe action set consists of a\n′ is\ndenoted as A\n′\ns. Then the reactive system can be re-written as\nFS = (QS, q0,S, Σ1\nI,S × Σ2\nI,S, ΣO,S, δS, λS).\nA simple example is made here to show how to build\na post-posed Shield for the safe control of urban transit.\nConsidering the train is running in a section with only one\n5\nEnvironment\nAgent\nShield\nFig. 2. Structure of post-posed Shield.\nspeed limitation which is 120km/h. The action set is A =\n{acceleration, coasting, braking}. In the operation process, the\nspeed must hold in the range of 1-119 km/h and the work-\ning condition cannot directly changed from acceleration to\nbraking so as the braking to acceleration. Firstly, the safety\nspeciﬁcations for the speed controller can be formulated by\nthe temporal logic formula as follow (7).\nG (speed > 1)\n∧G (speed < 119)\n∧G (acceleration →X (coasting) U braking)\n∧G (braking →X (coasting) U acceleration)\n(7)\nThe meaning of LTL formulas G,X(),U are globally, next time\nand until respectively, and the label set can be formulated as\nL = {speed < 1, 1 ≤speed ≤119, speed > 119}.\nThen the component of Shield are discussed. Firstly, the\nﬁnite state set QS can be set as QS = G, where G is the\nﬁnite safe set of a safety game satisﬁes safety speciﬁcation\nψS [36]. Then the initial state is q0,S = (q0, q0,M). We make\na brief introduction of q0,M here. As mentioned above, the\nreactive system to construct Shield should satisfy the safety\nspeciﬁcation, actually, it should satisfy another speciﬁcation\nwhich is the MDP speciﬁcation ψM, and q0,M is the initial\nstate of ψM. The input alphabet is ΣI,S = Σ1\nI,S × Σ2\nI,S =\nL × A = {acceleration, coasting, braking} × {speed < 1, 1 ≤\nspeed ≤119, speed > 119} and the output alphabet is\nΣO,S = A = {acceleration, coasting, braking}. The output\nfunction can be formulated as (8) where a ∈A, a′ ∈A\n′\nS, g ∈\nG, l ∈L and W is the set of the winning state of safety game\nG.\nλS (g, l, a) =\n\u001a\na if δ (g, l, a) ∈W\na′ if δ (g, l, a) /∈W, but δ (g, l, a′) ∈W\n(8)\nAnd the transition function is δS(g, l, a) = δ(g, l, λS(g, l, a)).\nThe above example points out the steps to build a post-posed\nShield for urban rail transit control and then a searching tree\nbased module is proposed to better ﬁnd a safe action a\n′.\nB. Safe Action Searching Tree\nIn this subsection, a searching tree based module is pro-\nposed to output the ﬁnal safe action. The idea of the searching\ntree derives from roll out algorithm and is more like a trade-off\nbetween MCTS and exhaustive search. Firstly, the Post-Posed\nShield provides a safe action set A\n′ and the searching module\nusing several steps to ﬁnally choose the high long-term reward\nsafe action. The framework of the module is depicted in Fig. 3.\nA detailed example is made here to explain how to construct\nand use the searching tree. Suppose that the initial unsafe state\nis sun and the safe action set is A\n′\nsa = (a\n′\nsa,1, ..., a\n′\nsa,n). To\noutput a high long-term reward safe action, each action in\nA\n′\nsa should be evaluated. asa,1 is chosen ﬁrstly then the state\nwill transfer to a new state ssa,1, this is actually a roll out\nor simulation step and ssa,1 is the root node. At state ssa,1,\nthe DRL agent will output nex actions, and a simulation step\nis then executed according to ssa,1 and the nex actions. It is\nnoted here that all these simulation steps are monitored by the\nShield then only safe action will be executed and in this step\nthose unsafe actions will not be replaced by a safe one. This\nmeans that only safe actions can expand nodes and root node\nssa,1 will have n\n′\nex children nodes (n\n′\nex ≤nex). Then, for\nthese nodes, a roll out step can be executed again. Considering\nin most DRL algorithms, the policy neural network will be\nupdated in the learning process, thus if the searching tree\nis a full-depth tree there will be a tricky problem that the\naction in real training and in expansion at the same state\nmay be different. For example, if the root node is step 3, the\nupdate frequency is 5 and the current expansion step is 6. In\nexpansion, the action at step 6 is output by µθ\n0 but the exact\naction in training at step 6 is output by µθ\n5 where µ is the\nparameter of the policy neural network so that the expansion\ncan not deduce a speciﬁc future. A trick is used here that the\ndepth of the searching tree will not be ﬁxed but dynamically\nequal to the remaining step to update the policy net, which\nmeans that the step of the leaf nodes will always be the same\nas the update step. In this case, the depth of the searching tree\nwill not be too large and the each searching tree in training\nphase can be step-adaptive.\nOnce the expansion step reaches the update step, the search-\ning tree needs to be pruned. In pruning, all children nodes that\nare not extended to the update step are deleted. Pruning can\nhelp to remove those nodes that will lead to an unsafe state\nand guarantee that only safe states are returned. After pruning,\nthe searching tree needs to be returned. For the qth node at\nstep p, the return rex\np,q is calculated by (9)\n(\nrex\np,q = rsi\np,q + 0.9 ∗E\nh\nrex\np,q,chi\ni\n, branch node\nrex\np,q = rsi\np,q, leaf node\n(9)\nwhere rsi\np,q is the roll out reward and E\nh\nrex\np,q,chi\ni\nis the\nexpectation return of all children nodes of nodep,q. The ﬁnal\nsafe action asa of state sun can be chosen by (10)\nasa = arg max\na∈A′\nsa\nrex\nsun,a\n(10)\nThe pseudocode of the searching tree is shown in Alg.1.\nC. DRL based guiding learner\nThough a post-posed Shield has a strong ability to prevent\nthe occurance of unsafe actions, it has two disadvantages [36]:\n6\nLocation\n(Step)\nSpeed\nSearching Tree\nFig. 3. Framework of safe action searching tree.\n• Unsafe actions may be part of the ﬁnal policy, thus the\nShield needs to be active even in after learning phase.\n• Unsafe actions always will be replaced by safe actions,\nthus the agent will never learn how to avoid unsafe\nactions by itself.\nThese two disadvantages are both severe for calculating\ntime-critical tasks like urban rail transit control because if the\nShield is always active, the solving time of a safe action may\nbe longer than the control cycle. The second disadvantage will\nalso lead to another problem that the agent does not has the\nself-protection ability. Aiming at these two disadvantages, the\nSSA-DRL based on the Shield and searching tree is introduced\nand simple AC algorithm is used here to illustrate how the\nlearner work.\nThe SSA-DRL seeks to solve the following optimization\nproblem:\nπ∗= argmax\nπ E[PT\nt=0 γtr (st, asa,t)]\ns.t. asa,t ∈A\n′\n(11)\nThen the action-value function Qπ(st, at) for policy π at step\nt can be calculated by the given policy net µθ or the searching\ntree T (·)denoted by (12).\nQπ (st, at) = Q\n\u0000st,\n\u0000µθ (st) , T (st)\n\u0001\u0001\n= E [Rt+1 + γQπ (st+1, at+1)] , at, at+1 ∈A\n′\n(12)\nIn the learning process of DRL, random noise is always used\nto increase exploration. The noise is added to the policy net\nand facilitates exploration in the action set for more efﬁcient\nexploration. Ornstein-Uhlenbeck (OU) noise and Guassian\nnoise are widely used since OU noise is autocorrelation and\nGaussian noise is easy to design and realize in real world.\nThus, the action chosen by the policy net can be represented\nas:\n\u001a\nµθ (st) = µθ (st|Θυ\nt ) + ε\nε ∼NOU\n, OU noise\n(13)\n\u001a\nµθ (st) = µθ (st) + λβ\nβ ∼NGa (0, 1)\n, Ga noise\n(14)\nThe parameter of the action-value function φ can be learned\nby minimizing the loss function of the critic net as presented\nby (15).\n∇φE(s,a,r,s′,d)∼B\n\u0002\n(Qπ\nφ(s, a) −y(r, s′, d))2\u0003\n(15)\nWhere B is a sample batch of transitions (s, a, r, s′, d) stored\nin replay buffer D. y(·) is called the target and is usually\ncomputed by (16).\ny(r, s′, d) = r + γ(1 −d)Qφ(s′, µθ(s′))\n(16)\nIt is noted here that the memory in replay buffer D follows\nﬁrst in ﬁrst out principle and the experience batch is randomly\nsampled thus the sampled experience may not contain a whole\n7\nAlgorithm 1 Searching Tree Process\nInput: The unsafe state,st\nun;The safe action set, A\n′\nsa;The\npolicy net;The expansion width, W;The current step,t;The\nupdate frequency,tup\nOutput: A safe action, asa\nwhile m ≤|A\n′\nsa| do\nGet a new state st+1\nsa,m by roll out policy with action a\n′\nm\nif (t + 1)%tup ̸= 0 then\nwhile (t + 1)%tup ̸= 0 do\nwhile w ≤W do\nGet action aw by policy net\nif aw is monitored safe by Shield then\nGet new state st+2\nsa,wby roll out policy with am\nif (t + 2)%tup = 0 then\nReturn Searching Tree by (9)\nend if\nw = w + 1\nelse\nw = w + 1\nend if\nend while\nt = t + 1\nend while\nelse\nReturn Searching Tree by (9)\nend if\nm = m + 1\nend while\nasa = arg maxa∈A′\nsa rex\nsun,a\ntrajectory. Then another replay buffer ˆD and an additional\npolicy neural net ˆµ are introduced.\nThe replay buffer ˆD is used to save ˆN whole trajectories\n(Sˆn, Aˆn, Rˆn) with highest reward ˆRˆn denoted by (17). The\nexperiences in ˆD follow the best in worst out principle and\nare ranked by the value of the reward.\nˆD = [tr1, ..., trˆn] , ˆn ∈[1, ˆN]\ntrˆn =\n\u0002\nstrˆn\nt , atrˆn\nt , rtrˆn\nt\n\u0003\n, t ∈[0, T ]\n(17)\nThe additional net ˆµ with parameter ˆθ is used to study the\nself-protection ability. Since there exists atrˆn\nt\n∈A\n′\nsa, thus if\n∀ˆn ∈ˆ\nN, ∃E[|atrˆn\nt\n−ˆµˆθ \u0000strˆn\nt\n\u0001\n|2] < ˆǫ, the additional policy net\ncan be used as the ﬁnal policy net. The parameter ˆθ can be\nupdated by (18).\n∇ˆθ\n1\n| ˆB|\nX\n(s\ntrˆn\nt\n,a\ntrˆn\nt )∈ˆ\nB\n\u0010\natrˆn\nt\n−ˆµ\nˆθ \u0000strˆn\nt\n\u0001\u00112\n, | ˆB| ≤ˆN\n(18)\nWhere ˆB is a sample batch of several whole trajectories stored\nin ˆD. Moreover, the structure of ˆµ may be different from the\nstructure of µ, they only have the same dimension and scale of\ninput and output. This method can improve the generalization\nability and deployability of the ﬁnal policy net. Then the whole\nSSA-DRL algorithm is outlined in Alg.2. It is noted here\nthat the speciﬁc steps for updating different DRL algorithms\nare not exactly the same, the steps in Alg.2 only involves\nAlgorithm 2 SSA-DRL algorithm\nInput: Policy neural net parameter,θ;Q-function neural net\nparameter,φ;Additional policy neural net parameter,ˆθ;The\nmaximum training episode,J\nOutput: Safe policy parameter, ˆθsa\nInitialize parameter θ, φ, ˆθ\nConstruct Post-posed Shield FSsh\nwhile j ≤J do\nrepeat\nObserve state s and get an action a by (13) or (14)\nFSsh check action a\nif a is safe then\nasa = a\nelse\nChoose a safe action asa by Alg.1\nend if\nExecute action asa and observe next state s\n′, done\nsignal d and reward r\nD ←D ∪(s, a, r, s′, d)\nif s′ is the terminal step and Rj ≥min tr [r]\ntr∈ˆ\nD\nthen\nˆD ←ˆD ∪trj\nelse\nReset the environment\nend if\nif update basic neural network then\nSample transitions B = {(s, a, r, s\n′, d)} from D\nCompute targets by (16), update φ by (15)\nUpdate θ by\n∇θ\n1\n|B|\nX\ns∈B\nQφ(s, µθ(s))\nif there exists target network then\nUpdate target networks by soft update\nend if\nend if\nif update additional policy neural network then\nSample whole trajectories ˆB = {tr} from ˆD\nfor many updates do\nUpdate ˆθ by (18)\nend for\nend if\nuntil Environment is reset\nj = j + 1\nend while\nthree core ideas in DRL which are policy evaluation, policy\nimprovement and target network. Once other DRL algorithm\nis used to implement the SSA-DRL algorithm, only some\nupdate steps in Alg.2 need to be addressed but the core idea\nis unchanged. In simulation section, the effectiveness of SAC\nbased SSA-SAC algorithm is a good example to verify the\nabove idea.\n8\nD. Optimality and Convergence Analysis\n1) Optimality: In the learning process, there actually exists\ntwo policies to get an action, the policy net and the searching\ntree. The optimality of the policy net does not need to be\nproved. Then two aspects of the optimality of searching tree\nare discussed, the ﬁrst is the original optimality of asa itself\nand the second is the policy πsa to get asa is no less than\nthe policy net µθ to get an action once a state is monitored\nunsafe.\nLemma 1. For policies to get a safe action, πsa to get asa is\nbetter than any other policies to get another π\n′\nsa. Moreover,\nπsa is no less than the original policy µθ to get a safe action.\nProof. The concept of policy improvement is used here to\nmake the proof. In Alg.1 obviously, the ﬁnal safe action asa\nis actually chosen by the greedy strategy, thus asa is the action\nwith the highest long-term reward and then πsa is better than\nother π\n′\nsa. Then, the idea of policy improvement is used to\nprove that when choosing a safe action, πsa is no less than\nµθ. Since the actions used in searching tree are all generated by\nµθ, thus they are identical equal besides the initial unsafe state\nπsa(sun) ̸= µθ(sun). Then if qµθ(sun, πsa(sun)) ≥vµθ(sun),\nthe policy πsa is no less than policy µθ. Keep expanding qµθ\nwe can get (19). It is obvious qµθ(sun, πsa(sun)) ≥vµθ(sun),\nthus πsa(sun) is no less than µθ. We must admit here this proof\nis not very rigorous, since the original policy improvement\nmethod requires t+n to be inﬁnite, but in this paper a clipped\nform is used, which is (t + n) %tup ̸= 0.\nvµθ (sun) ≤qµθ (sun, πsa (sun))\n= E\n\u0002\nRt+1 + γvµθ (St+1) | St = sun, At = πsa (sun)\n\u0003\n= Eπsa\nh\nRt+1 + γv\nµθ (St+1) | St = sun\ni\n≤Eπsa\nh\nRt+1 + γq\nµθ (St+1, πsa (St+1)) | St = sun\ni\n= Eπsa\n\u0002\nRt+1 + γE\n\u0002\nRt+2 + γvµθ (St+2)\n\u0003\n| St = sun\n\u0003\n= Eπsa\n\u0002\nRt+1 + γRt+2 + γ2vµθ (St+2) | St = sun\n\u0003\n≤Eπsa\n\u0002\nRt+1 + γRt+2 + γ2Rt+3 + γ3vµθ (St+3) | St = sun\n\u0003\n...\n≤Eπsa\n\nRt+1 + · · · + γn−1Rt+n\n|\n{z\n}\neq.(11)\n| St = sun\n\n\n= vπsa (sun)\n(19)\n2) Convergence: The convergence of the post-posed Shield\nhas been veriﬁed in [36] and the feasibility of using MDP\nspeciﬁcation and safety speciﬁcation to protect the operation\nof train has been veriﬁed in [49], thus the post-posed Shield\nin this paper satisﬁes the convergence analysis in [36]. The\nsimulation results in Section V also veriﬁes the convergence\nof SSA-DRL algorithm.\nE. Algorithm Implementation\nIn this subsection, the state set, action set, reward function\nand the relationship between action and the acceleration are\ndiscussed to complete the SSA-DRL based urban rail transit\nautonomous operation algorithm.\n1) State Set: The location loc, velocity vel, running time\ntime are used to formulated the state set. Thus the state of the\nagent at step t can be formulated as (20).\nst = (loct, velt, timet)\n(20)\n2) Action Set:\nThe percentage of the traction braking\ncontrol command output to the motor of train is used as the\naction. The action set is continuous and ranges from -1 to 1.\nIf the value is less than 0, the command is braking otherwise\nthe command is traction. Then the action at step t can be\nformulated as (21).\nat ∈[−1, 1]\n(21)\n3) Reward Function: Since the operation has been pro-\ntected and the main purpose is autonomous driving, then\noperation energy consumption E, operation time difference\nDT and the comfort of passengers C are used to build the\nreward function. In each transition step t, Et, DT\nt , Ct are\ncalculated by (22),(23) and (24).\nEt =\n\u001a\nαEtr ∗Etr, a > 0\nαEre ∗Ere, a ≤0\n(22)\nDT\nt =\n\u001a αDT ∗|T total −T sch|, terminal\nα\n′\nDT ∗|¯vt −¯v|, mid step\n(23)\nCt =\n\u001a κ, ∆acc > σ\n0, ∆acc < σ\n(24)\nWhere Etr is the traction energy consumption, Ere is the\nrecovered regenerative braking energy, αEtr, αEre, αDT , α\n′\nDT\nare the weights of energy reward and time reward, T total, T sch\nare the total operation time and scheduled operation time, ¯vt, ¯v\nare the average speed in step t and the overall average speed,\nκ is a punishment indicator, ∆acc is the rate of change of\nacceleration and σ is a threshold. The reward of transition\nstep t is then deﬁned as (25).\nRt = −Et −DT\nt −Ct\n(25)\nIt is also noted that the value of Ere is set negative then the\ntrain can learn to reduce total energy consumption.\n4) Relationship between control command and train opera-\ntion: The operation of train is restricted by the traction brak-\ning characteristic thus the control command cannot directly\nrepresent the movement of train. Suppose a given command\natr\nt > 0 at step t, the acceleration output by the traction motor\nis accmotor\nt\n= F tr\n+ (vt) ∗atr/mtrain where F tr\n+ (vt) denotes\nthe max traction force at speed vt is a function of speed\nand mtrain is the weight of train. Likely, the acceleration of\nbraking can be computed in the same way. Then the actual\nacceleration of train can be computed by (26).\nacctrain = (accmotor −accr + g (x))\nv\n(26)\n9\n0\n20\n40\n60\n80\n100\n120\n140\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nMax Traction Force\nMax Braking Force\nFig. 4. Traction and braking characteristic.\nWhere accr is the resistance acceleration calculated by the\nDavis function and g(x) is the gravity acceleration at location\nx. Moreover, there exists no steep slope in this paper thus (27)\nalways holds.\n0 ≤accr −g (x) ≤max\n\u0000accmotor\u0001\n(27)\nIV. SIMULATION RESULTS AND PERFORMANCE\nEVALUATION\nA. Simulation Environment\nThe simulation is based on Chengdu urban rail transit line\n17, there are a total of sixteen sections in up and down\ndirections. The value of the Davis parameters are r1\n=\n8.4, r2 = 0.1071, r3 = 0.00472 respectively, the weight of the\ntrain is 337.8 ton, the max and min acceleration are ±1.2m/s2\nand the traction braking characteristic is shown in Fig. 4. The\nmain parameters used to construct the SSA-DRL algorithm\nare shown in TableII.\nTABLE II\nTHE MAIN PARAMETERS OF THE ALGORITHM\nParameters\nValue\nParameters\nValue\nTraction Weight αEtr\n3\nRegenerative Weight αEre\n3\nTime Weight αDT\n15\nTime Weight α\n′\nDT\n25\nComfort Punishment κ\n10\nChange Threshold σ\n3\nMinibatch |B|\n256\nMinibatch | ˆB|\n10\nMax Episode J\n500\nUpdate Frequency tup\n5\nTABLE III\nTHE MAIN HYPERPARAMETERS OF THE ALGORITHM\nParameters (DDPG)\nValue\nParameters (SAC)\nValue\nActor learning rate\n1e−5\nPolicy learning rate\n1e−5\nCritic learning rate\n1e−3\nValue learning rate\n1e−3\nDiscount factor\n0.99\nDiscount factor\n0.99\nSoft update factor\n1e−2\nSoft update factor\n1e−2\n/\n/\nSoft-Q learning rate\n3e−5\nThe basic DDPG and SAC are used as the baseline to\nimplement autonomous operation in this paper, and the AC\nnetworks are both designed through four fully connected\nhidden layer, the activation functions in hidden layers are Relu,\nthe size of hidden layers is 256 and the ﬁnal activation function\nof the actor net is tanh to ensure the output ranges in [−1, 1].\nThe optimizers are all Adam. The main hyperparameters\nof these two algorithms are shown in Table.III. It is noted\nthat in the simulation SSA-DRL algorithm shares the same\nhyperparameters with the baseline. Moreover, the originally\nadditional actor is designed the same with the actor, but with a\nﬁve times learning rate. The inﬂuence caused by the design of\nadditional actor will be discussed by the ablation experiment.\nThe proposed algorithm is implemented in Matlab and Python\non a computer with an AMD Ryzen 7 5800X CPU @ 3.80Ghz\nand 32GB RAM running Windows 10 x64 Edition.\nB. Basic Simulation\nThe basic simulation aims to verify that the proposed SSA-\nDRL can control the train complete the operation plan with\nhigher reward and better performance under less protect times\nin both training and execution process. Here,the protect times\nis the number of counts the algorithm re-chooses a safe action\nto correct the original action in training or execution process.\nFig. 5 shows the speed proﬁles of the SSA-DRL algorithm\nin one simulation. Fig. 6 shows the reward curves of SSA-\nDRL, Shield-DRL and common DRL algorithms. It can be\nclearly seen that in most scenarios SSA-DRL can achieve\na higher reward than Shield-DRL and the reward of Shield-\nDRL is also higher than common DRL. Also, SSA-DRL can\nachieve convergence at a earlier step. It is noted that the\nreward curves are all smoothed by the moving average and\nthe size of the window is 8. The detailed numerical results\nare shown in Table.IV. The data in time and energy columns\nare acquired from one operation plan. Since the data of\nregenerative braking energy in real world can not be acquired,\nthus it is not considered in energy column but in simulation\ncolumn. Moreover, the operation time is not ﬁxed and a margin\nof thirty seconds is allowed. And in the simulation columns,\nthe data are recorded by ave ± std. Rreaders may think that\nthe speed proﬁles do not match Table.IV, it should be made\nclear that the speed proﬁles are only results of one simulation\nbut the data in Table.IV are numerical results after several\nsimulation times.\nFig. 7 shows the distribution of the protect times of SSA-\nDRL algorithms in both training process and execution pro-\ncess. For the SSA-SAC algorithm, the data are acquired by ﬁve\ndifferent seeds with 500 training episodes and 10 execution\nepisodes in each simulation. For the SSA-DDPG algorithm,\nthe only difference is that 400 episodes in each simulation\nare used because the DDPG algorithm needs to ﬁll the replay\nbuffer. In this case, the data capacity are 2500, 50 for SSA-\nSAC and 2000, 50 for SSA-DDPG. To better illustrate that\nthe proposed SSA-DRL can effectively reduce the protect\ntimes compared with Shield-DRL, we get the same amount\nof Shield-DRL data and the detailed results are shown in\nTable.V. In the Protect Times row, average protect times are\nshown and in the comparison row, the number is calculated by\n| |SSA-DRL|−|Sh-DRL|\n|Sh-DRL|\n| × 100. Thus if the number in comparison\nrow is positive, it means that compared with the Shield-DRL\nalgorithm, SSA-DRL algorithm has a decline in %. The bold\nand underline in each column show the most and lest decline\n10\nTABLE IV\nTHE ORIGINAL SIMULATION RESULTS\nSection\nDirection\nTime(s)\nSimulation Time(s)\nEnergy(kw·h)\nSimulation Energy(kw·h)\nOverspeed Counts\nSSA-SAC\nSSA-DDPG\nSSA-SAC\nSSA-DDPG\nSSA-SAC\nSSA-DDPG\n1\nUp\n86\n135.66±57.90\n106.17±11.38\n52\n29.38±1.12\n21.98±2.02\n0\n0\nDown\n92\n107.80±11.45\n109.74±6.07\n45\n21.61±1.66\n24.12±5.25\n0\n0\n2\nUp\n183\n215.46±21.24\n228.30±27.35\n116\n53.52±2.04\n47.67±7.34\n0\n0\nDown\n204\n208.24±6.34\n206.38±2.53\n88\n23.41±1.25\n42.37±9.78\n0\n0\n3\nUp\n276\n298.88±6.63\n288.76±10.32\n160\n83.85±11.91\n55.93±3.09\n0\n0\nDown\n259\n267.76±14.16\n293.65±6.95\n142\n85.10±15.43\n31.53±3.01\n0\n0\n4\nUp\n104\n130.45±15.27\n137.77±12.90\n56\n30.54±1.87\n36.73±6.63\n0\n0\nDown\n101\n107.75±1.89\n104.71±3.88\n61\n16.44±1.32\n31.01±6.85\n0\n0\n5\nUp\n104\n121.53±1.63\n130.28±21.16\n68\n21.80±0.71\n33.77±6.63\n0\n0\nDown\n103\n107.64±2.58\n106.03±3.49\n67\n31.60±2.68\n39.84±4.87\n0\n0\n6\nUp\n105\n126.78±5.87\n117.11±3.34\n63\n42.10±2.70\n47.08±4.69\n0\n0\nDown\n106\n110.03±4.31\n109.46±3.63\n48\n20.19±9.13\n16.32±5.46\n0\n0\n7\nUp\n105\n113.42±5.09\n114.54±10.82\n63\n40.07±1.40\n43.86±3.86\n0\n0\nDown\n138\n138.82±4.74\n134.46±1.36\n48\n13.90±3.59\n16.31±4.11\n0\n0\n8\nUp\n172\n170.64±4.30\n180.07±3.70\n63\n75.15±6.27\n60.23±4.26\n0\n0\nDown\n171\n176.52±2.28\n172.50±2.59\n48\n39.28±3.04\n33.75±3.70\n0\n0\nin the same process with the same basic DRL algorithm. It can\nbe acquired from Table.V that 1) the comparison rows are all\npositive thus SSA-DRL can always reduce the protect times\ncompared with Shield-DRL, 2) except two special situations\n(Section1 up direction training and Section6 down direction\nexecution), the decline in training and execution are large and\nthe max decline are 80.84% and 100% in training and exe-\ncution respectively, 3) except one special situation (Section8\nup direction training), once the process is same, the protect\ntimes of Shield-DRL are always larger than the SSA-DRL no\nmatter the basic algorithm is. Fig. 8 shows bar graphs of the\ndata in Table.V. It is more clear that compared with the Shield-\nDRL algorithms, the protect times of SSA-DRL algorithms has\ngreatly reduced.\nThe distribution of calculation time in execution are shown\nin Fig. 9. The calculation time consists of getting a action and\nstate transition two parts. If the original action is checked safe\nby the Shield, getting a action time means the time of the\nneural network to complete the forward pass. If not, then it is\nthe time of completing a searching process and output a ﬁnal\nsafe action. The transition time means the time of one step\nstate transition which is simply denoted by st, at, rt+1, st+1.\nThe X-axis in Fig. 9 represents the simulation section, the\nY-axis represents the range of the calculation time and the Z-\naxis represents the percentage of the corresponding calculation\ntime range. For example, the bar at the northwest corner in\nFig. 9(a) means the percentage of getting an action time in\nrange 0-0.05s of Section1 up direction is 60%-70%. Then from\nFig. 9, it can be get that most of the calculation time no matter\ngetting action or state transition is less than 0.02s meanwhile\n0-0.005s accounts the biggest proportion that satisﬁes the time\nrequirement of the control cycle in real world which is usually\n200ms.\nThen, combine Fig. 5-Fig. 9 with the detailed numerical\nresults shown in Table.IV and Table.V, a conclusion can be\ndrawn is that SSA-DRL can control the train complete the\noperation plan and reduce traction energy consumption without\noverspeed danger which we also say the proposed SSA-DRL\nensures a safe control strategy.\nC. Transferability Experiment\nThis experiment aims to test whether the trained neural\nnetwork of SSA-DRL can be deployed to a new environment.\nThe trained SSA-DDPG and SSA-SAC algorithms in section8\nof up and down direction are deployed to section1-section7 of\nthe same direction and the results are shown in TableVI.\nThe meaning of noise test is brieﬂy explained here. Since\nin this experiment the trained network is deployed in a new\nenvironment, there is a possibility that the network cannot\nwork and the output will always be a noise. Considering\nthe characteristic of tanh function, once the network cannot\nwork, there may exist two special noise action sequences,\nall -1 or all 1, which indicates that the train will always\nbrake or accelerate. Obviously, the all -1 action sequence\ncannot complete the operation plan. However, once the action\nsequence is all 1, it may complete the operation plan because\nthe original action 1 always forces the train to accelerate and\nwhen the train is overspeed, the Shield and the searching tree\nwill provide a safe action help the train to slow down. In this\ncase, though the train completes the operation plan, it cannot\nbe regarded as the trained network is transferability. In order to\ndistinguish this case, the noise test is designed. In this test, we\ndirectly provide a noise action sequence with all 1 to record\nthe protect times. Once the trained network can complete the\noperation plan and the protect times is far less than the noise\ntest, the trained network is transferability.\nThen from TableVI, the SSA-DDPG is transferability in\nSection1,2,4,5,6,7 and the SSA-SAC is transferability in Sec-\n11\n0\n5000\n10000\n15000\n20000\n25000\n0\n20\n40\n60\n80\n100\n120\n140\nLocation(m)\nSpeed(km/h)\nLimit Speed\nSSA-DDPG\nSSA-SAC\n(a) Speed proﬁles in up direction\n0\n0.5\n1\n1.5\n2\n2.5\n0\n20\n40\n60\n80\n100\n120\n140\nLocation(m)\nSpeed(km/h)\nLimit Speed\nSSA-DDPG\nSSA-SAC\n(b) Speed proﬁles in down direction\nFig. 5. Speed proﬁles in different sections.\ntion1,6. It is noted that this experiment is not rigorous and\nthe experiment result can only verify the SSA-DRL may be\ntransferability.\nD. Robustness Experiment\nThis experiment aims to verify the robustness of the SSA-\nDRL, that is, the ability to complete the operation plan under\nthe condition that the action is disturbed. Two parameters\nεr, δr are introduced in this experiment to control the proba-\nbility and magnitude of action disturbance. For an action ar\ngiven to the agent, it has εr × 100% probability to change\nto another action ar + Nr, Nr ∈[−δr, δr]. The range of\nεr, |δr| are both [0.1, 0.5] and the changing stepsize is 0.1. The\nPearson correlation coefﬁcient (PCC) is used here to measure\nthe degree of correlation between the original sequence and\nthe disturbed sequence. SSA-DDPG and SSA-SAC are used in\nSection1 up and down direction respectively. Fig. 10 shows the\noriginal and disturbed curves of speed proﬁle, action sequence\nand acceleration sequence and the changing trend of PCC\nwhen εr and δr change. In Fig. 10, the original curves are red\nbold and the disturbed curves are blue dash. It is clear that\nthe disturbed curves are of the same trend with the original\ncurve and there is no completely different curve or one curve is\ntotally changed after a disturbance. The PCCs of speed proﬁle,\naction sequences and acceleration sequences are all larger than\n0.985, 0.7 and 0.75, since the PCC is more close to 1, the\ntwo curves are more linear correlation, thus combined with\nFig. 10, it can be concluded that the SSA-DRL may have a\nstrong robustness in some scenarios.\nE. Design of the Additional Learner\nAs mentioned above, the structure of the additional learner\ncan be different from the actor in the used DRL algorithm.\nIn this experiment, six different neural network structures\nare used to verify that the design of additional learner will\nnot inﬂuence the ﬁnal strategy signiﬁcantly. The six different\nstructures are half neuron numbers, a quarter neuron num-\nbers, double neuron numbers, quadruple neuron numbers, one\nhidden layer less (neuron numbers are not changed) and one\nhidden layer more respectively. In this experiment, the training\nprocess is the same with the basic simulation. Also, the active\nfunction and the connection mode of neural networks are not\nchanged. SSA-DDPG and SSA-SAC are used in section4 up\nand down directions respectively and the speed proﬁles in one\nexecution process are shown in Fig. 11.\nIt can be clearly seen that only three speed proﬁles (double\nneurons, a quarter neurons and double neurons) of SSA-SAC\nhave big differences with other speed proﬁles. In this case,\nthough the structures of the additional learner and the actor in\nDRL are not the same, it will not have a big inﬂuence of the\nﬁnal execution process which makes the users more easily to\ntrain an easy to deploy neural network.\nV. CONCLUSION\nAiming at the safe control strategy for urban rail transit\nautonomous operation, an SRL framework called SSA-DRL is\nproposed in this paper. The SSA-DRL uses a LTL based post-\nposed Shield to check whether an original action is safe and\nthen uses a searching tree to ﬁnd a safe action with the highest\nlong term reward to correct the unsafe action. An additional\nlearner consists of a replay buffer and an additional actor is\nalso added to help to reduce the protect times in execution\nprocess.\nOur framework is veriﬁed in simulations under four dif-\nferent aspects with two basic DRL algorithms. The basic\nexperiment shows that the framework can control the train\ncomplete the operation plan with a lower energy consumption\n12\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-2\n-1.8\n-1.6\n-1.4\n-1.2\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n(a) Sec1\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-14\n-12\n-10\n-8\n-6\n-4\n-2\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n485\n490\n495\n500\n-10000\n-5000\n0\n(b) Sec2\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n105\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n490\n495\n500\n-6000\n-4000\n-2000\n(c) Sec3\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n495\n500\n-1900\n-1800\n-1700\n-1600\n-1500\n(d) Sec4\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-5\n-4.5\n-4\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n485\n490\n495\n500\n-5000\n-4000\n-3000\n-2000\n-1000\n(e) Sec5\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-4\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n492\n494\n496\n498\n500\n-4000\n-3500\n-3000\n-2500\n-2000\n-1500\n(f) Sec6\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n496\n497\n498\n499\n-3000\n-2500\n-2000\n-1500\n(g) Sec7\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-14\n-12\n-10\n-8\n-6\n-4\n-2\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n498\n499\n500\n-3200\n-3000\n-2800\n-2600\n(h) Sec8\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n485\n490\n-1500\n-1400\n-1300\n-1200\n(i) Sec1\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-15\n-10\n-5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n499.4\n499.6\n499.8\n500\n-4000\n-3500\n-3000\n-2500\n(j) Sec2\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-15\n-10\n-5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n490\n495\n500\n-4500\n-4000\n-3500\n-3000\n-2500\n(k) Sec3\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-5\n-4.5\n-4\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n494\n496\n498\n-2500\n-2000\n-1500\n-1000\n(l) Sec4\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-18000\n-16000\n-14000\n-12000\n-10000\n-8000\n-6000\n-4000\n-2000\n0\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n470\n480\n490\n-3000\n-2500\n-2000\n-1500\n-1000\n-500\n(m) Sec5\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n490\n495\n500\n-1800\n-1700\n-1600\n-1500\n-1400\n-1300\n-1200\n(n) Sec6\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-4.5\n-4\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n494\n496\n498\n500\n-3000\n-2500\n-2000\n-1500\n(o) Sec7\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n104\nSSA-SAC\nShield-SAC\nSAC\nSSA-DDPG\nShield-DDPG\nDDPG\n497\n498\n499\n500\n-3200\n-3000\n-2800\n-2600\n(p) Sec8\nFig. 6. Reward curves. The ﬁrst tow rows are the reward curves of up direction and the last two rows are of down direction.\n(a) Up Direction (Training)\n(b) Down Direction (Training)\nSec1\nSec2\nSec3\nSec4\nSec5\nSec6\nSec7\nSec8\n0\n0.5\n1\n1.5\n2\n2.5\n3\n(c) Up Direction (Execution)\nSec1\nSec2\nSec3\nSec4\nSec5\nSec6\nSec7\nSec8\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n(d) Down Direction (Execution)\n(e) Up Direction (Training)\n(f) Down Direction (Training)\n(g) Up Direction (Execution) (h) Down Direction (Execution)\nFig. 7. Protect times. The ﬁrst row is the result of SSA-SAC and the second row is the result of SSA-DDPG.\n13\nTABLE V\nTHE PROTECT TIMES COMPARISON BETWEEN SSA-DRL AND SHIELD-DRL\nSection\nDirection\nContent\nTrain\nExecution\nSSA-SAC\nSh-SAC\nSSA-DDPG\nSh-DDPG\nSSA-SAC\nSh-SAC\nSSA-DDPG\nSh-DDPG\n1\nUp\nProtect Times\n24.18\n24.69\n16.55\n82.07\n0.00\n31.24\n5.40\n92.00\nComparison(%)\n2.06\n79.83\n100\n94.13\nDown\nProtect Times\n12.77\n16.66\n6.60\n31.90\n0.04\n29.86\n1.80\n19.00\nComparison(%)\n19.32\n23.31\n99.27\n90.53\n2\nUp\nProtect Times\n40.51\n72.28\n33.62\n83.88\n0.04\n64.96\n8.40\n58.40\nComparison(%)\n43.96\n59.92\n99.94\n85.62\nDown\nProtect Times\n12.02\n62.75\n24.33\n45.00\n0.06\n19.40\n16.00\n86.75\nComparison(%)\n80.84\n45.93\n99.69\n81.56\n3\nUp\nProtect Times\n36.20\n62.98\n29.96\n79.75\n0.04\n68.92\n24.20\n136.00\nComparison(%)\n42.52\n62.43\n99.94\n82.21\nDown\nProtect Times\n24.49\n79.13\n45.13\n99.74\n0.04\n103.65\n9.40\n126.40\nComparison(%)\n69.04\n54.76\n99.96\n92.56\n4\nUp\nProtect Times\n9.67\n23.70\n15.69\n35.54\n0.00\n24.80\n5.80\n38.60\nComparison(%)\n59.18\n55.85\n100\n84.97\nDown\nProtect Times\n15.85\n23.88\n21.79\n27.78\n0.10\n18.54\n1.80\n39.80\nComparison(%)\n33.61\n36.01\n99.46\n95.48\n5\nUp\nProtect Times\n10.83\n25.20\n22.21\n29.21\n0.08\n28.72\n0.20\n35.00\nComparison(%)\n57.04\n23.98\n99.72\n99.43\nDown\nProtect Times\n11.16\n23.75\n20.40\n27.78\n0.14\n18.28\n7.40\n34.00\nComparison(%)\n53.03\n26.57\n99.23\n72.24\n6\nUp\nProtect Times\n8.54\n33.74\n15.44\n30.86\n0.02\n33.75\n8.20\n38.00\nComparison(%)\n74.68\n49.97\n99.94\n78.42\nDown\nProtect Times\n12.09\n25.55\n18.99\n26.02\n0.12\n30.25\n18.60\n18.80\nComparison(%)\n52.66\n27.01\n99.60\n1.06\n7\nUp\nProtect Times\n18.02\n52.31\n21.41\n31.87\n0.02\n53.74\n27.40\n58.80\nComparison(%)\n65.56\n32.81\n99.96\n53.40\nDown\nProtect Times\n15.36\n49.30\n37.36\n59.83\n0.28\n62.65\n10.2\n66.00\nComparison(%)\n68.84\n37.55\n99.55\n54.55\n8\nUp\nProtect Times\n14.02\n23.61\n23.20\n49.19\n0.34\n29.5\n4.7\n37.00\nComparison(%)\n39.55\n51.99\n98.84\n81.08\nDown\nProtect Times\n17.27\n27.25\n34.81\n70.41\n0.14\n32.77\n3.4\n83.60\nComparison(%)\n36.62\n50.42\n99.57\n95.93\nSec1\nSec2\nSec3\nSec4\nSec5\nSec6\nSec7\nSec8\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nSSA-DDPG\nShield-DDPG\nSSA-SAC\nShield-SAC\n(a) Up Direction (Training)\nSec1\nSec2\nSec3\nSec4\nSec5\nSec6\nSec7\nSec8\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSSA-DDPG\nShield-DDPG\nSSA-SAC\nShield-SAC\n(b) Down Direction (Training)\nSec1\nSec2\nSec3\nSec4\nSec5\nSec6\nSec7\nSec8\n0\n20\n40\n60\n80\n100\n120\n140\nSSA-DDPG\nShield-DDPG\nSSA-SAC\nShield-SAC\n(c) Up Direction (Execution)\nSec1\nSec2\nSec3\nSec4\nSec5\nSec6\nSec7\nSec8\n0\n20\n40\n60\n80\n100\n120\n140\nSSA-DDPG\nShield-DDPG\nSSA-SAC\nShield-SAC\n(d) Down Direction (Execution)\nFig. 8. Protect times. Comparison between SSA-DRL and Shield-DRL.\n14\n(a) SSA-DDPG get action\n(c) SSA-DDPG state transition\n(e) SSA-SAC get action\n(g) SSA-SAC state transition\n(b) Top view\n(d) Top view\n(f) Top view\n(h) Top view\nFig. 9. Time distribution of getting action and state transition\nTABLE VI\nTHE RESULTS OF TRANSFERABILITY EXPERIENT\nSec\nDirection\nActor\nTime\nReal Energy\nProtect Counts\nNoise Test\nMaximum Step\n1\nUp\nSection8(SSA-DDPG)\n95.20\n22.61\n48\n102\n146\nDown\nSection8(SSA-SAC)\n97.05\n25.67\n0\n93\n146\n2\nUp\nSection8(SSA-DDPG)\n199.54\n44.92\n56\n163\n208\nDown\nSection8(SSA-SAC)\n391.35\n85.48\n0\n158\n208\n3\nUp\nSection8(SSA-DDPG)\n1146.95\n64.46\n38\n143\n165\nDown\nSection8(SSA-SAC)\n499.53\n144.69\n0\n133\n165\n4\nUp\nSection8(SSA-DDPG)\n120.14\n32.95\n18\n42\n54\nDown\nSection8(SSA-SAC)\n138.85\n36.15\n0\n40\n54\n5\nUp\nSection8(SSA-DDPG)\n124.87\n22.8\n18\n35\n50\nDown\nSection8(SSA-SAC)\n134.32\n42.61\n0\n34\n50\n6\nUp\nSection8(SSA-DDPG)\n122.53\n29.41\n21\n38\n49\nDown\nSection8(SSA-SAC)\n117.11\n20.98\n0\n40\n49\n7\nUp\nSection8(SSA-DDPG)\n151.16\n40.49\n39\n60\n79\nDown\nSection8(SSA-SAC)\n292.52\n45.55\n0\n66\n79\nand protect times. And compared with the basic DRL or\nShield-DRL algorithms, the SSA-DRL can get a higher reward\nand achieve convergence earlier in most simulation scenarios.\nThe transferability and robustness experiment verify that a\ntrained network can transfer to a new environment and can\nstill complete the operation plan under some disturbances.\nThe experiment of the design of the additional learner veriﬁes\nthat SSA-DRL can help users to train a easy to deploy neural\nnetwork without big loss of performance.\nFinally, from our research and other related researches we\nsummarize the difﬁculties of design a self-decision algorithm\nfor autonomous train operation into 2W2H as follow.\n• What algorithms can be used to design a self-decision\nalgorithm meanwhile optimize the objective functions?\n• What methods can be used to build a fail-safe mechanism\nif the designed algorithm is based on AI methods?\n• How do we ensure the designed algorithms always have a\nsafe output which will not lead to an overspeed operation?\n• How do we explain the safe output of the designed\nalgorithm or How do we explain why the output is safe?\nIn the future work, we will try to ﬁnd a more general\nframework to systematically answer the 2W2H problems.\nREFERENCES\n[1] C. Fu, M. A. Olivares-Mendez, R. Suarez-Fernandez, and P. Campoy,\n“Monocular visual-inertial slam-based collision avoidance strategy for\nfail-safe uav using fuzzy logic controllers: comparison of two cross-\nentropy optimization approaches,” Journal of Intelligent & Robotic\nSystems, vol. 73, pp. 513–533, 2014.\n[2] M. H¨orwick and K.-H. Siedersberger, “Strategy and architecture of a\nsafety concept for fully automatic and autonomous driving assistance\nsystems,” in 2010 IEEE Intelligent Vehicles Symposium.\nIEEE, 2010,\npp. 955–960.\n[3] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[4] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n15\n0\n50\n100\n150\n0\n5\n10\n15\n20\n25\n30\n(a) SSA-DDPG speed\n(b) PCCs of SSA-DDPG speed\n0\n5\n10\n15\n20\n25\n30\n0\n5\n10\n15\n20\n25\n30\n(c) SSA-SAC speed\n(d) PCCs of SSA-SAC speed\n0\n50\n100\n150\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n(e) SSA-DDPG action\n(f) PCCs of SSA-DDPG action\n0\n5\n10\n15\n20\n25\n30\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n(g) SSA-SAC action\n(h) PCCs of SSA-SAC action\n0\n50\n100\n150\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n(i) SSA-DDPG acc\n(j) PCCs of SSA-DDPG acc\n0\n5\n10\n15\n20\n25\n30\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n(k) SSA-SAC acc\n(l) PCCs of SSA-SAC acc\nFig. 10. Results of robustness experiments.\n[5] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for\nmodel-based control,” in 2012 IEEE/RSJ International Conference on\nIntelligent Robots and Systems.\nIEEE, 2012, pp. 5026–5033.\n[6] B. Thananjeyan, A. Balakrishna, S. Nair, M. Luo, K. Srinivasan,\nM. Hwang, J. E. Gonzalez, J. Ibarz, C. Finn, and K. Goldberg, “Recovery\nrl: Safe reinforcement learning with learned recovery zones,” IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 4915–4922, 2021.\n[7] D. Kaur, S. Uslu, K. J. Rittichier, and A. Durresi, “Trustworthy artiﬁcial\nintelligence: a review,” ACM Computing Surveys (CSUR), vol. 55, no. 2,\npp. 1–38, 2022.\n[8] H. Liu, Y. Wang, W. Fan, X. Liu, Y. Li, S. Jain, Y. Liu, A. Jain,\nand J. Tang, “Trustworthy ai: A computational perspective,” ACM\nTransactions on Intelligent Systems and Technology, vol. 14, no. 1, pp.\n1–59, 2022.\n[9] B. Li, P. Qi, B. Liu, S. Di, J. Liu, J. Pei, J. Yi, and B. Zhou, “Trustworthy\nai: From principles to practices,” ACM Computing Surveys, vol. 55,\nno. 9, pp. 1–46, 2023.\n[10] N. A. Smuha, “The eu approach to ethics guidelines for trustworthy\nartiﬁcial intelligence,” Computer Law Review International, vol. 20,\nno. 4, pp. 97–106, 2019.\n[11] P. Howlett, “An optimal strategy for the control of a train,” The ANZIAM\nJournal, vol. 31, no. 4, pp. 454–471, 1990.\n[12] P. G. Howlett and P. J. Pudney, Energy-efﬁcient train control.\nSpringer\nScience & Business Media, 2012.\n[13] E. Khmelnitsky, “On an optimal control problem of train operation,”\nIEEE transactions on automatic control, vol. 45, no. 7, pp. 1257–1266,\n2000.\n[14] A. Albrecht, P. Howlett, P. Pudney, X. Vu, and P. Zhou, “The key\nprinciples of optimal train control—part 2: Existence of an optimal strat-\negy, the local energy minimization principle, uniqueness, computational\ntechniques,” Transportation Research Part B: Methodological, vol. 94,\npp. 509–538, 2016.\n[15] ——, “The key principles of optimal train control—part 1: Formulation\nof the model, strategies of optimal type, evolutionary lines, location of\noptimal switching points,” Transportation Research Part B: Methodolog-\nical, vol. 94, pp. 482–508, 2016.\n[16] R. M. Goverde, G. M. Scheepmaker, and P. Wang, “Pseudospectral\noptimal train control,” European Journal of Operational Research,\nvol.\n292,\nno.\n1,\npp.\n353–375,\n2021.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/science/article/pii/S0377221720308948\n[17] G. M. Scheepmaker, R. Goverde, and L. G. Kroon, “Review of energy-\nefﬁcient train control and timetabling,” European Journal of Operational\nResearch, vol. 257, no. 2017, p. 355–376, 2017.\n[18] H. Ko, T. Koseki, and M. Miyatake, “Application of dynamic pro-\ngramming to the optimization of the running proﬁle of a train,” WIT\nTransactions on The Built Environment, vol. 74, 2004.\n[19] S. Lu, S. Hillmansen, T. K. Ho, and C. Roberts, “Single-train trajectory\noptimization,” IEEE Transactions on Intelligent Transportation Systems,\nvol. 14, no. 2, pp. 743–750, 2013.\n[20] X. Wang, T. Tang, and H. He, “Optimal control of heavy haul train\nbased on approximate dynamic programming,” Advances in Mechanical\nEngineering,9,4(2017-4-01), vol. 9, no. 4, p. 168781401769811, 2017.\n[21] T. Liu, J. Xun, J. Yin, and X. Xiao, “Optimal train control by ap-\nproximate dynamic programming: Comparison of three value function\napproximation methods,” in 2018 21st International Conference on\nIntelligent Transportation Systems (ITSC). IEEE, 2018, pp. 2741–2746.\n[22] P. Wang, A. Trivella, R. Goverde, and F. Corman, “Train trajectory\noptimization for improved on-time arrival under parametric uncertainty,”\n16\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\nLimit Speed\nHalf Neurons\nA Quater Neurons\nDouble Neurons\nQuadruple Neurons\nOne Layer Less\nOne Layer More\nLegend\n(a) Up Direction (Section4)\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n0\n20\n40\n60\n80\n100\n120\nLimit Speed\nHalf Neurons\nA Quater Neurons\nDouble Neurons\nQuadruple Neurons\nOne Layer Less\nOne Layer More\nLegend\n(b) Down Direction (Section4)\nFig. 11. Speed proﬁles of different structures of additional learner.\nTransportation Research Part C Emerging Technologies, vol. 119, p.\n102680, 2020.\n[23] L. Zhu, Y.\nHe,\nF. R.\nYu, B. Ning,\nT. Tang, and\nN. Zhao,\n“Communication-based train control system performance optimization\nusing deep reinforcement learning,” IEEE Transactions on Vehicular\nTechnology, vol. 66, no. 12, pp. 10 705–10 717, 2017.\n[24] W.\nLiu,\nS.\nSu,\nT.\nTang,\nand\nX.\nWang,\n“A\ndqn-based\nintelligent\ncontrol\nmethod for heavy haul trains\non long steep\ndownhill\nsection,”\nTransportation\nResearch\nPart\nC:\nEmerging\nTechnologies,\nvol.\n129,\np.\n103249,\n2021.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/science/article/pii/S0968090X2100262X\n[25] ——, “A dqn-based intelligent control method for heavy haul trains on\nlong steep downhill section,” Transportation Research Part C: Emerging\nTechnologies, vol. 129, p. 103249, 2021.\n[26] J. Yin, D. Chen, and L. Li, “Intelligent train operation algorithms for sub-\nway by expert system and reinforcement learning,” IEEE Transactions\non Intelligent Transportation Systems, vol. 15, no. 6, pp. 2561–2571,\n2014.\n[27] K. Zhou, S. Song, A. Xue, K. You, and H. Wu, “Smart train operation\nalgorithms based on expert knowledge and reinforcement learning,”\nIEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 52,\nno. 2, pp. 716–727, 2020.\n[28] M. Shang, Y. Zhou, and H. Fujita, “Deep reinforcement learning with\nreference system to handle constraints for energy-efﬁcient train control,”\nInformation Sciences, vol. 570, pp. 708–721, 2021.\n[29] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, pp.\n279–292, 1992.\n[30] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” Advances in neural\ninformation processing systems, vol. 12, 1999.\n[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” arXiv preprint arXiv:1509.02971, 2015.\n[32] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning.\nPMLR, 2018,\npp. 1861–1870.\n[33] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.\n[34] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering\nthe game of go without human knowledge,” nature, vol. 550, no. 7676,\npp. 354–359, 2017.\n[35] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\nJ. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., “Grand-\nmaster level in starcraft ii using multi-agent reinforcement learning,”\nNature, vol. 575, no. 7782, pp. 350–354, 2019.\n[36] M. Alshiekh, R. Bloem, R. Ehlers, B. K¨onighofer, S. Niekum, and\nU. Topcu, “Safe reinforcement learning via shielding,” in Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, 2018.\n[37] S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, Y. Yang, and\nA. Knoll, “A review of safe reinforcement learning: Methods, theory\nand applications,” arXiv preprint arXiv:2205.10330, 2022.\n[38] J. Garcıa and F. Fern´andez, “A comprehensive survey on safe reinforce-\nment learning,” Journal of Machine Learning Research, vol. 16, no. 1,\npp. 1437–1480, 2015.\n[39] S. Gu, G. Chen, L. Zhang, J. Hou, Y. Hu, and A. Knoll, “Constrained\nreinforcement learning for vehicle motion planning with topological\nreachability analysis,” Robotics, vol. 11, no. 4, p. 81, 2022.\n[40] L. Wen, J. Duan, S. E. Li, S. Xu, and H. Peng, “Safe reinforcement\nlearning for autonomous vehicles through parallel constrained policy\noptimization,” in 2020 IEEE 23rd International Conference on Intelli-\ngent Transportation Systems (ITSC).\nIEEE, 2020, pp. 1–7.\n[41] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick, “End-to-end\nsafe reinforcement learning through barrier functions for safety-critical\ncontinuous control tasks,” in Proceedings of the AAAI conference on\nartiﬁcial intelligence, vol. 33, no. 01, 2019, pp. 3387–3395.\n[42] S. Mo, X. Pei, and C. Wu, “Safe reinforcement learning for autonomous\nvehicle using monte carlo tree search,” IEEE Transactions on Intelligent\nTransportation Systems, vol. 23, no. 7, pp. 6766–6773, 2021.\n[43] T.-H. Pham, G. De Magistris, and R. Tachibana, “Optlayer-practical con-\nstrained optimization for deep reinforcement learning in the real world,”\nin 2018 IEEE International Conference on Robotics and Automation\n(ICRA).\nIEEE, 2018, pp. 6236–6243.\n[44] J. Garcia and F. Fern´andez, “Safe exploration of state and action spaces\nin reinforcement learning,” Journal of Artiﬁcial Intelligence Research,\nvol. 45, pp. 515–564, 2012.\n[45] J. Garc´ıa and D. Shaﬁe, “Teaching a humanoid robot to walk faster\nthrough safe reinforcement learning,” Engineering Applications of Arti-\nﬁcial Intelligence, vol. 88, p. 103360, 2020.\n[46] A. Singh, Y. Halpern, N. Thain, K. Christakopoulou, E. Chi, J. Chen, and\nA. Beutel, “Building healthy recommendation sequences for everyone:\nA safe reinforcement learning approach,” in FAccTRec Workshop, 2020.\n[47] X. Lu, L. Xiao, G. Niu, X. Ji, and Q. Wang, “Safe exploration in wireless\nsecurity: A safe reinforcement learning algorithm with hierarchical\nstructure,” IEEE Transactions on Information Forensics and Security,\nvol. 17, pp. 732–743, 2022.\n[48] F. Zhao, Y. Zeng, B. Han, H. Fang, and Z. Zhao, “Nature-inspired\nself-organizing collision avoidance for drone swarm based on reward-\nmodulated spiking neural network,” Patterns, vol. 3, no. 11, 2022.\n[49] Z. Zhao, J. Xun, X. Wen, and J. Chen, “Safe reinforcement learning for\nsingle train trajectory optimization via shield sarsa,” IEEE Transactions\non Intelligent Transportation Systems, vol. 24, no. 1, pp. 412–428, 2022.\n[50] P. S. Thomas, B. C. da Silva, A. G. Barto, S. Giguere, Y. Brun, and\nE. Brunskill, “Preventing undesirable behavior of intelligent machines,”\nScience, vol. 366, no. 6468, pp. 999–1004, 2019.\n[51] J. Yin, D. Chen, and L. Li, “Intelligent train operation algorithms for sub-\nway by expert system and reinforcement learning,” IEEE Transactions\non Intelligent Transportation Systems, vol. 15, no. 6, pp. 2561–2571,\n2014.\n[52] H. Tang, Y. Wang, X. Liu, and X. Feng, “Reinforcement learning\napproach for optimal control of multiple electric locomotives in a heavy-\nhaul freight train: A double-switch-q-network architecture,” Knowledge-\nBased Systems, vol. 190, p. 105173, 2020.\n[53] X. Wang, A. D’Ariano, S. Su, and T. Tang, “Cooperative train control\nduring the power supply shortage in metro system: A multi-agent\nreinforcement learning approach,” Transportation Research Part B:\nMethodological, vol. 170, pp. 244–278, 2023.\n17\n[54] X. Chen, X. Guo, J. Meng, R. Xu, S. Li, and D. Li, “Research on ato\ncontrol method for urban rail based on deep reinforcement learning,”\nIEEE Access, vol. 11, pp. 5919–5928, 2023.\n[55] L. Ning, M. Zhou, Z. Hou, R. M. Goverde, F.-Y. Wang, and H. Dong,\n“Deep deterministic policy gradient for high-speed train trajectory\noptimization,” IEEE Transactions on Intelligent Transportation Systems,\nvol. 23, no. 8, pp. 11 562–11 574, 2021.\n[56] X. Lin, Z. Liang, L. Shen, F. Zhao, X. Liu, P. Sun, and T. Cao,\n“Reinforcement learning method for the multi-objective speed trajectory\noptimization of a freight train,” Control Engineering Practice, vol. 138,\np. 105605, 2023.\n[57] G. Li, S. W. Or, and K. W. Chan, “Intelligent energy-efﬁcient train\ntrajectory optimization approach based on supervised reinforcement\nlearning for urban rail transits,” IEEE Access, vol. 11, pp. 31 508–31 521,\n2023.\n[58] L. Zhang, M. Zhou, Z. Li et al., “An intelligent train operation method\nbased on event-driven deep reinforcement learning,” IEEE Transactions\non Industrial Informatics, vol. 18, no. 10, pp. 6973–6980, 2021.\n[59] S. Su, W. Liu, Q. Zhu, R. Li, T. Tang, and J. Lv, “A cooperative collision-\navoidance control methodology for virtual coupling trains,” Accident\nAnalysis & Prevention, vol. 173, p. 106703, 2022.\n[60] J. Achiam, “Spinning Up in Deep Reinforcement Learning,” 2018.\n[61] A. Pnueli, “The temporal logic of programs,” in 18th Annual Symposium\non Foundations of Computer Science (sfcs 1977). ieee, 1977, pp. 46–57.\n[62] O. Kupferman and M. Y. Vardi, “Model checking of safety properties,”\nKluwer Academic Publishers, 2001.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2023-11-24",
  "updated": "2024-01-06"
}