{
  "id": "http://arxiv.org/abs/2311.14725v3",
  "title": "Identifying percolation phase transitions with unsupervised learning based on largest clusters",
  "authors": [
    "Dian Xu",
    "Shanshan Wang",
    "Weibing Deng",
    "Feng Gao",
    "Wei Li",
    "Jianmin Shen"
  ],
  "abstract": "The application of machine learning in the study of phase transitions has\nachieved remarkable success in both equilibrium and non-equilibrium systems. It\nis widely recognized that unsupervised learning can retrieve phase transition\ninformation through hidden variables. However, using unsupervised methods to\nidentify the critical point of percolation models has remained an intriguing\nchallenge. This paper suggests that, by inputting the largest cluster rather\nthan the original configuration into the learning model, unsupervised learning\ncan indeed predict the critical point of the percolation model. Furthermore, we\nobserve that when the largest cluster configuration is randomly\nshuffled-altering the positions of occupied sites or bonds-there is no\nsignificant difference in the output compared to learning the largest cluster\nconfiguration directly. This finding suggests a more general principle:\nunsupervised learning primarily captures particle density, or more\nspecifically, occupied site density. However, shuffling does impact the\nformation of the largest cluster, which is directly related to phase\ntransitions. As randomness increases, we observe that the correlation length\ntends to decrease, providing direct evidence of this relationship. We also\npropose a method called Fake Finite Size Scaling (FFSS) to calculate the\ncritical value, which improves the accuracy of fitting to a great extent.",
  "text": "Identifying percolation phase transitions with unsupervised learning based on\nlargest clusters\nDian Xu,1 Shanshan Wang,1 Weibing Deng,1 Feng Gao,1 Wei Li,1, ∗and Jianmin Shen1, 2, †\n1Key Laboratory of Quark and Lepton Physics (MOE) and Institute of Particle Physics,\nCentral China Normal University, Wuhan 430079, China\n2School of engineering and technology, Baoshan University, Baoshan 678000, China\n(Dated: December 10, 2024)\nThe application of machine learning in the study of phase transitions has achieved remarkable\nsuccess in both equilibrium and non-equilibrium systems. It is widely recognized that unsupervised\nlearning can retrieve phase transition information through hidden variables. However, using un-\nsupervised methods to identify the critical point of percolation models has remained an intriguing\nchallenge. This paper suggests that, by inputting the largest cluster rather than the original config-\nuration into the learning model, unsupervised learning can indeed predict the critical point of the\npercolation model. Furthermore, we observe that when the largest cluster configuration is randomly\nshuffled—altering the positions of occupied sites or bonds—there is no significant difference in the\noutput compared to learning the largest cluster configuration directly. This finding suggests a more\ngeneral principle: unsupervised learning primarily captures particle density, or more specifically,\noccupied site density. However, shuffling does impact the formation of the largest cluster, which\nis directly related to phase transitions. As randomness increases, we observe that the correlation\nlength tends to decrease, providing direct evidence of this relationship. We also propose a method\ncalled Fake Finite Size Scaling (FFSS) to calculate the critical value, which improves the accuracy\nof fitting to a great extent.\nI.\nINTRODUCTION\nMachine Learning (ML) [1, 2] is a specialized area\nof artificial intelligence (AI) that focuses on creat-\ning algorithms [3] and statistical models [4]. These\nmodels enable computers to perform tasks without\nexplicit instructions [5].\nML allows computers to\nlearn from data [6], enabling them to make decisions,\npredictions, or identify patterns independently [7, 8].\nThis capability is particularly valuable for data anal-\nysis in the field of physics. It offers an alternative\napproach, alongside theoretical calculations [9], nu-\nmerical simulations [10, 11], and field theory [12], for\nanalyzing phase transitions in statistical physics.\nThe application of machine learning in phase tran-\nsition research began with efforts to leverage its pow-\nerful data processing and pattern recognition capa-\nbilities to identify phase transition behavior in com-\nplex systems.\nEarly studies primarily focused on\nusing supervised learning techniques[13, 14], such as\nneural networks, to identify transition points.\nBy\ntraining models on data at known phase transitions,\nresearchers could classify and predict phase transi-\ntions for unknown parameters.\nIn recent years, machine learning approaches have\n∗liw@mail.ccnu.edu.cn\n† sjm@mails.ccnu.edu.cn\nbeen applied to more complex phase transition\nproblems,\nincluding non-equilibrium systems[15],\nquantum\nphase\ntransitions[16],\nand\ntopological\ntransitions[17, 18].\nThese studies extend beyond\nsimply identifying transition points, exploring in-\nstead the microstructure of systems through gen-\nerative models, such as variational autoencoders\n(VAEs)[19, 20] and generative adversarial networks\n(GANs)[21]. Additionally, research involving graph\nneural networks (GNNs) has emerged[22], allowing\nresearchers to handle physical systems with com-\nplex interactions, further advancing the depth of\nmachine learning applications in the field of phase\ntransitions.[23]\nWith the development of unsupervised learning,\nnumerous studies have shown that techniques such\nas dimensionality reduction and clustering can au-\ntomatically capture information about order pa-\nrameters in phase transition systems.\nSebastian\nJ. Wetzel examined unsupervised learning tech-\nniques, specifically Principal Component Analysis\n(PCA)[24] and a neural-network-based Variational\nAutoencoder (VAE), to identify the features that\nbest describe configurations of the 2D Ising model\nand the 3D XY model. His findings indicated that\nthe latent parameters corresponded closely with the\nknown order parameters.[25]\nHu et al applied PCA to studying the phase be-\nhavior and transitions in several classical spin mod-\narXiv:2311.14725v3  [cond-mat.stat-mech]  9 Dec 2024\nels, including the Ising models on square and trian-\ngular lattices, and the 2D XY model. They found\nthat the principal components derived through PCA\ncould not only reveal different phases and symmetry\nbreaking but also distinguish between types of phase\ntransitions and locate critical points. Their study\nalso applied autoencoders, demonstrating that these\nmodels could be trained to capture phase transitions\nand identify critical points as well.[26]\nBeyond\nequilibrium\nphase\ntransition\nmodels,\nWang et al employed unsupervised learning to ana-\nlyze the 1+1 dimensional even-offspring branching-\nannihilating\nrandom\nwalks\nmodel,\nan\nexam-\nple of nonequilibrium phase transitions, success-\nfully\nobtaining\ncritical\nexponents\nand\nrelated\ninformation.[27] Further studies on topological[17]\nand quantum phase transitions[28] have also demon-\nstrated the effectiveness of unsupervised learning\nmethods in identifying critical characteristics in\nthese systems.\nResearch on percolation models has a long history,\nbeginning with studies of how fluids diffuse through\nthe pores of coal.[29] Modern percolation theory has\nevolved to focus on changes in network behavior as\nnodes or edges are added.[30] The work in [31] rep-\nresents one of the earliest systematic discussions of\nthe physical and geometric properties of percolation\nmodels, suggesting that the percolating cluster is\nmost likely to emerge within the largest cluster of\nthe model.\nWith further advancements, percolation models\nhave found increasingly close connections with other\nfields. For example, Artime et al reviewed the be-\nhavior of percolation in cascading failures, provid-\ning an overview of the theoretical and computational\napproaches to robustness and resilience in complex\nnetworks.[32] Ji et al also examined the intricate in-\nterplay between network structure and signal prop-\nagation, contributing to the study of the complex\ndynamics within interconnected systems.[33]\nThe order parameter in percolation models is not\nthe density of active sites; it also includes the prob-\nability that lattice sites (or bonds) belong to the\npercolating cluster. As a result, using unsupervised\nlearning to identify the critical point in percola-\ntion models has been a persistent challenge. Zhang\nWanzhou applied an Ising mapping approach to map\nthe original configurations of the percolation model,\nsubsequently using machine learning to identify the\nsystem’s critical point.[34] Shu Cheng and colleagues\nused various neural networks to study configura-\ntions with noise.[35] However, none of these stud-\nies demonstrated that unsupervised learning alone\ncould directly obtain the critical point from the orig-\ninal configurations of a percolation model.\nJianmin Shen’s unsupervised learning results on\nthe 1+1 dimensional directed percolation (DP)\nmodel indicate that both the first principal com-\nponent from PCA and a single latent neuron from\nan autoencoder effectively represent the DP model’s\norder parameter, namely, the particle density. In-\nterestingly, when the lattice configurations are ran-\ndomized, the unsupervised learning results show\nno difference from those obtained with the original\nconfigurations.[36] This raises the question: Do the\nresults from unsupervised learning truly represent\nthe order parameter?\nInspired by these findings, in this paper, we pro-\npose using a largest-cluster approach combined with\nthe Monte Carlo method to compute active site den-\nsity, Principal Component Analysis (PCA), and an\nautoencoder (AE)[37] to locate the critical point in\nthe percolation model. This approach is crucial for\nexamining the relationship between active site den-\nsity and criticality. Additionally, we investigate the\nresults of unsupervised learning after randomizing\nthe largest cluster. Our main methods include re-\narranging or selectively modifying the largest clus-\nter before inputting it into algorithms to observe\nchanges in output behavior.\nThe primary structure of this paper unfolds as fol-\nlows. Section II A elucidates the percolation model\nconfigurations of interest.\nSection II B expounds\nupon the two methodologies of unsupervised Learn-\ning.\nSection III delineates the research findings,\nwherein a comparative analysis of six distinct con-\nfigurations under MC, PCA, and AE is presented.\nFinally, in Section IV, we furnish a comprehensive\nsummary of this study.\nII.\nMODEL AND UNSUPERVISED\nLEARNING\nA.\npercolation model\nThe two-dimensional percolation model represents\na continuous phase transition,[38] with the order pa-\nrameter given by\nP∞(p) ∝(p −pc)β\nfor\np →p+\nc\n(1)\nwhere p is the occupation probability, pc is the criti-\ncal probability,commonly referred to as the percola-\ntion threshold. β is the critical exponent of the order\nparameter, and P∞(p) denotes the probability that a\ngiven site or bond belongs to the percolating cluster\n2\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFIG. 1. In this article, we examine six distinct configurations. a is the raw configuration of site percolation with\noccupation probability = 0.8. b is the largest cluster of a. c shows the shuffled configuration of b with a ratio r = 0.2\n. d shows the largest cluster of figure c. e is the raw configuration of bond percolation with occupation probability\n= 0.8. f shows the largest cluster of e.\n3\nnamely percolation probability.\nTypically, numer-\nical simulation methods are employed to compute\nP∞(p) to locate the critical point.\nIn the context of a square lattice, the probabil-\nity of encountering a percolating cluster within the\nsystem remains extremely low as long as the proba-\nbility remains below the critical probability pc. Con-\nversely, when the probability surpasses the criti-\ncal probability pc, the likelihood of encountering a\npercolating cluster rapidly approaches unity as the\nnumber of occupied sites increases.\nThe percolating cluster, which is the focus of this\nstudy, contains the critical information of the perco-\nlation phase transition. In percolation models, the\nraw configurations include not only the percolating\ncluster but also other isolated sites or bonds.\nIn finite systems, the size of the largest cluster,\nSmax, serves as a crucial observable for understand-\ning critical behavior and can be expressed as:\nSmax ∼Ld−β/ν · f\n\u0010\n(p −pc)L1/ν\u0011\n,\n(2)\nwhere L is linear size of the system, with Ld repre-\nsenting the total number of sites in the system. ν\nand β are critical exponents, while f(x) is a uni-\nversal scaling function. Specifically, as p →p−\nc , the\nsize of the largest cluster grows rapidly but remains\nfinite. As p →p+\nc , the largest cluster becomes the\ninfinite percolation cluster, with its occupancy frac-\ntion P∞(p) obeying the scaling relationship dictated\nby the critical exponent β.\nTypically, the phenomenon of percolation mani-\nfests predominantly within the largest cluster. Con-\nsequently, this article abstains from considering sec-\nondary or smaller clusters. For two-dimensional site\npercolation, the critical value is pc = 0.592746,the\nbond percolation one is pc = 0.5. [39].The largest\nclusters are extracted using the depth-first search\nalgorithm.\nTo probe the percolation model , Commence by\nconstructing a lattice comprising N × N sites, and\nadjacent sites are connected by bonds .\nFor site\npercolation we set every bond have been occupied\nthen progress to randomly populate each site with a\nspecified probability, designated as p , for the bond\npercolation model, operate the opposite. Assign la-\nbels to the connected clusters of occupied sites or\nbond.\nTwo occupied one are deemed part of the\nsame cluster if they are contiguous to each other.\nScrutinize whether any of the clusters traverse the\nlattice from one end to the other, signifying percola-\ntion. To procure reliable statistics, iterate through\nthe steps mentioned earlier multiple times.\nB.\nUnsupervised Learning\n1.\nPCA\nA primary application of PCA lies in diminishing\nthe data’s dimensionality while conserving maximal\nvariability. This proves advantageous for data visu-\nalization, feature reduction, or preparing the data\nfor subsequent machine learning algorithms. PCA\nentails the computation of the eigenvalues and eigen-\nvectors of the data’s covariance matrix. The eigen-\nvectors, denoted as principal components, delineate\nthe orientations of the new feature space, while the\neigenvalues signify their magnitude or the variance\nalong those orientations. Subsequent to extracting\nthe principal components, the data can be projected\nonto them to effectuate dimensionality reduction.\nIn this paper, we employ the PCA algorithm for\ndimensionality reduction, which involves several key\nsteps. First, we prepare the input matrix,\nX =\n\n\nx11\n· · · x1n\n...\n¨\n...\nxm1 · · · xmn\n\n\n(3)\nhere, m represents the number of occupancy prob-\nabilities we consider, denoted as pnumber.\nIn this\nstudy, we sample 41 points for the occupancy prob-\nability, ranging from 0 to 1 with intervals of 0.02.\nThe size of n corresponds to the product of the sys-\ntem’s length and width. Since we use a square lat-\ntice, n = L ∗L. Next, we subtract the mean value\nof each column from every element in that column,\neffectively centering the data for each feature.\n¯xa = xa1 + xa2 + · · · + xan\nn\n(4)\nAfter preprocessing, the data is standardized to\nhave a mean of 0 and a standard deviation of 1. We\ndefine this new matrix as Y . Subsequently, we com-\npute the covariance matrix of Y , which captures the\nrelationships between the features in the standard-\nized data.\nA = 1\nnY Y T\n(5)\nGenerally, there are two methods to compute the\neigenvalues and eigenvectors of the covariance ma-\ntrix. In this paper, we use the approach based on\nSingular Value Decomposition (SVD) of the covari-\nance matrix. For any matrix A, an SVD always ex-\nists, which can be expressed as:\nA = UΣV T\n(6)\n4\nwhere U and V are orthogonal matrices, and Σ is\na diagonal matrix containing the singular values of\nA.By applying SVD to the covariance matrix, we\ncan efficiently obtain its eigenvalues and eigenvec-\ntors, which are essential for performing PCA.\nBased on this understanding, we proceed as fol-\nlows:Firstly,We calculate the eigenvalues and eigen-\nvectors of AAT . After normalizing the eigenvectors,\nthey form the matrix U.Similarly, we calculate the\neigenvalues and eigenvectors of AT A. After normal-\nizing the eigenvectors, they form the matrix V . The\neigenvalues obtained from AAT are square-rooted to\nform the singular values. These singular values are\narranged along the diagonal of the matrix Σ.where U\n, Σ , and V correspond to the left singular vectors,\nsingular values, and right singular vectors, respec-\ntively.\nTo compress the data into a single dimension, from\nthe diagonal matrix Σ , we select the largest singular\nvalue. Squaring this value gives us the largest eigen-\nvalue of AAT , which is also the largest eigenvalue of\nAT A .We extract the eigenvector of AT A that cor-\nresponds to this largest eigenvalue. This eigenvector\nforms a row vector, which we transpose to create a\nnew matrix K .The new matrix K is then multiplied\nby the original input matrix X\npca1 = X\n′\nm∗1 = Xm∗nKn∗1\n(7)\nthe resulting matrix X\n′\nm∗1 is the desired one-\ndimensional compressed data, referred to as pca1.\nFinally, by averaging the values across 1000 sam-\nples, we obtain the final dataset X\n′′\npnumber∗1. This\nstep consolidates the compressed data for each oc-\ncupancy probability, providing a concise and mean-\ningful representation for further analysis.\n2.\nAE\nAn Autoencoder (AE) is a type of artificial neu-\nral network architecture in the field of unsupervised\nlearning, consisting of two key components: the en-\ncoder and the decoder.\nThe encoder compresses the input data into a\nmore compact representation, aiming to capture the\nintrinsic structure of the original dataset.\nThis\nserves a dual purpose: reducing the dimensionality\nof the data and minimizing the impact of noise.\nIn contrast, the decoder reconstructs the input\ndata from its compressed representation, striving to\ngenerate an output that faithfully reflects the origi-\nnal input.\nThe workflow of an Autoencoder is illustrated in\nthe FIG 2.\nIn the Autoencoder (AE) workflow presented in\nthe FIG 2, we take the case of L = 4 as an exam-\nple.\nThe process begins by flattening the original\nconfiguration. Specifically, the elements of the L ∗L\nmatrix are concatenated row by row: the first row\nis appended to the start of the second row, then the\nnew second row is appended to the third row, and\nso on. Ultimately, we obtain a 1 ∗L2 matrix repre-\nsentation denoted as R.\nNext, this flattened matrix R is fed into a layer\nwith 512 neurons. Whether each neuron is activated\ndepends on the following transformation process:\nhi = f(\nX\nj\nwijRj + bi)\n(8)\nwhere wij represents the weights connecting input\nj to neuron i, bi is the bias term associated with\nneuron i, f() is the activation function (e.g., ReLU\nor sigmoid), and hi is the output of neuron i.\nThis transformation reduces the original high-\ndimensional input to a compact, encoded represen-\ntation. The encoding layer captures the critical fea-\ntures of the data, which are later used by the decoder\nto reconstruct the original configuration.\nThrough layer-by-layer propagation, we eventu-\nally obtain a matrix T that is identical in form to\nthe original matrix R, with dimensions 1 ∗L2. The\nnetwork weights are updated using gradient descent\nvia backpropagation. In this process, the error (or\nloss) between the predicted output T and the target\ninput R is propagated back through the network to\nadjust the weights in order to minimize the loss.\nThe weight updates are based on the gradient of\nthe loss function with respect to the network param-\neters. Specifically, the update rule is:\nW ←W −η ∂L\n∂W\n(9)\nb ←b −η ∂L\n∂b\n(10)\nwhere W and b represent the weight matrix and bias\nterms of the network, ηis the learning rate,\n∂L\n∂W and\n∂L\n∂b are the gradients of the loss function L with re-\nspect to the weights and biases.\nThrough this iterative process of adjusting the\nweights, the neural network learns to approximate\nthe original configuration and reduce the difference\nbetween R and T. This enables the network to effec-\ntively capture the underlying structure of the data,\nfacilitating tasks such as reconstruction, feature ex-\ntraction, or anomaly detection in the context of un-\nsupervised learning.\n5\nflatten\nEncoding data\nDecoding data\nCompressed data\nFC(512)+ReLU\nFC(64)+ReLU\nFC(64)+ReLU\nFC(512)+ReLU\nFC(1)+Sigmoid\nOutput\nOutput layer\nFC(16)+Sigmoid\nCross Entropy \nConfiguration\nInput data\nFIG. 2. Neural network schematic structure of autoencoder.\nThe ultimate goal is to minimize the cross-entropy\nloss function between R and T, which will lead to\nthe network’s output. Cross-entropy can be under-\nstood as the measure of difficulty in representing the\nprobability distribution R using the probability dis-\ntribution T. Its expression is given by:\nH(R, T) =\nX\nR(xi)log\n1\nT(xi)\n(11)\nWhere R(xi) is the true probability distribution (the\noriginal data), T(xi) is the predicted probability dis-\ntribution (the output of the neural network), The\nsummation runs over all elements xi in the data.\nThe objective of the network is to adjust its pa-\nrameters in such a way that the predicted distri-\nbution T gets as close as possible to the true dis-\ntribution R, thereby minimizing the cross-entropy\nloss. By minimizing this loss, the network learns the\nunderlying structure of the data, enabling effective\ndata reconstruction or feature representation.\nWe extract the output values h from the hidden\nlayer of the optimized network. By averaging the\nvalues for all samples under the same probability, we\ncan obtain the desired research object. This process\nallows us to analyze the underlying features learned\nby the network, providing insights into the system’s\ncharacteristics, such as phase transitions or other\nphenomena related to the percolation model or other\ncomplex systems being studied.\nIII.\nTHE UNSPERVISED LEARNING\nRESULTS\nTypically, the first principal component from\nPrincipal Component Analysis (PCA) and the single\nlatent variable from the Autoencoder (AE) are con-\nsidered crucial in extracting key values from phase\ntransition models. To clarify this relationship, we\ninvestigate several different configurations of the\npercolation model, namely the raw configuration,\nthe largest cluster, the shuffled largest cluster, and\nthe shuffled largest cluster with rearranged sites, as\nshown in Figure 1.\nTo facilitate comparison and analyze the accuracy\nof the results, we first perform a Monte Carlo (MC)\nsimulation on the raw configuration. The maximum\nderivative value of the function is considered as the\ncritical point of the model. It is worth noting that\nthe results presented in this paper have been nor-\nmalized to ensure consistency.\nAs shown in Figures 3(a) and 3(c), we conducted\nsite percolation simulations on a lattice of size L =\n100 with 1001 occupation probability values ranging\nfrom 0 to 1, and bond percolation simulations on a\nlattice of size L = 60 with 41 occupation probability\nvalues within the same range. For each probability,\n1000 lattice configurations were generated, and the\nnumber of percolation occurrences was recorded.\nThe y-axis values are obtained by dividing the\npercolation counts by 1000, while the x-axis repre-\nsents the generation probabilities. Notably, Figure\n3(a) shows a sharp transition peak near 0.59, which\naligns closely with the theoretical value of 0.593, as\n6\nexpected. Similarly, Figure 3(c) reveals a transition\nnear 0.5, consistent with the expected critical thresh-\nold for bond percolation.\nOn the other hand, we investigated the correlation\nbetween the density of active sites/bonds(see equa-\ntion12) of the largest cluster and the critical points\nin the percolation models. Here, the active density\nis defined as the ratio of occupied sites/bonds to the\ntotal number of lattices. We extracted the largest\ncluster configurations from the site and bond per-\ncolation models described above and calculated the\ndensity of active sites/bonds. The average of these\nvalues constitutes the y-axis of the plot, while the\nx-axis represents the generation probabilities.\nρ =\nP occupied sites/bonds\ntotal sites/bonds\n(12)\nAs shown in Figure 3(b), the curve exhibits a no-\ntable rapid change near the critical point, closely\naligning with the expected value around 0.59. Sim-\nilarly, in Figure 3(d), a clear transition is observed\nnear 0.5 as the generation probability increases, con-\nsistent with the critical threshold for bond percola-\ntion. These results validate the effectiveness of our\nsimulation method based on the largest cluster and\nprovide a solid foundation for the subsequent anal-\nyses in this study.\nAt the same time, we calculate the Pearson cor-\nrelation coefficient to compare the MC simulation\nof density of active sites/bonds, the first principal\ncomponent of the PCA learning result denoted by\npca1, and the single hidden variable denoted by h in\nthe AE. The coefficient quantifies the degree of the\nlinear correlation between any pair of variables.\nr =\nnP\ni=1\n(ρi −ρ)(hi −h)\ns\nnP\ni=1\n(ρi −ρ)2\ns\nnP\ni=1\n(hi −h)2\n,\n(13)\nwhere ρi and hi are the density of active sites/bonds\nof system and the single hidden variable at i-th site\nor bond probability p, while ρ and h are the mean\nvalues of ρi and hi, respectively. In PCA, h is re-\nplaced with pca1.\nA.\nResults of percolation\n1.\nThe raw configurations\nAs an initial baseline, we employed the MC to\nsimulate site and bond percolation model with sizes\nL = 10,20,30, and 40, then analyzed the raw config-\nurations. The results, depicted in FIG. 4(a)(d), re-\nveal a linear increase in density of active sites/bonds\nas the generation probability escalates. In the nor-\nmalized outcome, densities function forms from vari-\nous sizes exhibit overlapping behavior. However, the\noutcome lacks discernible non-trivial features and\nfails to pinpoint the critical points.\nDrawing from our understanding of the percola-\ntion model, an augmentation in the generation prob-\nability invariably culminates in a linear escalation\nof the model’s density of active sites/bonds, cor-\nroborated by the findings of the MC simulation.\nNevertheless, what causes our interest is that both\nPCA, depicted in FIG. 4(b)(e), and AE, depicted\nin FIG. 4(c)(f), manifest some form of linear rela-\ntionship. This prompts us to speculate whether the\ninput of the model’s configuration information into\nPCA and AE yields a relationship between the first\nprincipal component of PCA (pca1) and the single\nhidden variable (h) with respect to density of active\nsites/bonds. We intend to undertake a verification\nof this hypothesis in the ensuing chapter.\n2.\nThe largest cluster\nIn the preceding discussion, we extensively ex-\namined the experimental results pertaining to the\nraw configurations in MC, PCA, and AE. Now, let’s\ndelve into the configurations associated with the or-\nder parameter in the percolation model, specifically\nfocusing on the largest cluster configuration. As pre-\nviously mentioned, the occurrence of phase transi-\ntion in this model predominantly manifests within\nthe largest clusters.\nFaced with the nonlinear curves depicted in Fig-\nures 5(a) and 5(g), we encounter the challenge of ef-\nficiently fitting these data points. To quantitatively\nderive a reasonable threshold value, we propose a\nmethod akin to Finite Size Scaling (FSS), which we\ntentatively name Fake Finite Size Scaling (FFSS).\nThe method proceeds as follows: first, we gen-\nerated the relationship between density of ac-\ntive sites and occupation probability for site per-\ncolation across five different system sizes, L =\n10, 20, 30, 40, 50, with 1000 samples for each size.\nUsing Mathematica software, we then determined\nthe intersection points (L10−20, P10−20) for L = 10\nand L = 20. Repeating this process, we obtained\nfour intersection points, as listed in Table I.\nThrough FFSS, we extrapolated these intersection\npoints to estimate the system’s behavior as it ap-\nproaches an infinite size ( 1\nL →0). By identifying\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nPL=100(p)\n\n\n\n\n\n\n\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n(a)\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nPL=60(p)\n\n\n\n\n\n\n\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n(c)\n(d)\nFIG. 3. The MC simulation results about site and bond percolation of a two-dimensional system of size L × L. (a)\nmeans the probability that a site belongs to a percolating cluster, (b) calculated the largest cluster’s density of active\nsites of the system.(c) means the probability that a bond belongs to a percolating cluster, (d) calculated the largest\ncluster’s density of active bonds of the system.\nsize\nintersection\nsize\nL10\nL20\nL30\nL40\nL20\nP10−20\nL30\nP20−30\nL40\nP30−40\nL50\nP40−50\nTABLE I. A schematic diagram of the Fake Finite Size Scaling (FFSS) method, used to identify the intersection\npoints of function curves obtained from experimental results for different system sizes. In the table, both the rows\nand columns represent function curves of the model at varying system sizes.\nthe intersection of the fitted line with the p-axis,\nwe approximated the critical value of site percola-\ntion as pc = 0.595(2), as shown in Figure 5(d), and\nthe percolation threshold for bond percolation as\npc = 0.504(1), as shown in Figure 5(j).\nNotably,\nthese results align well with theoretical predictions.\nAlthough this method differs from the standard\nFinite Size Scaling approach, it demonstrates a rea-\nsonable degree of validity under the given circum-\nstances.\nSimultaneously, Principal Component Analysis\n(PCA) was applied to analyze the largest clusters\nin site percolation and bond percolation, yielding\nthe results shown in Figures 5(b) and 5(h), respec-\ntively. For the PCA results, we observed a functional\nform similar to that found in Monte Carlo simulation\noutcomes. To determine the critical points, we em-\nployed the Fake Finite Size Scaling (FFSS) method.\nThe corresponding results are shown in Figures\n5(e) and 5(k). The extrapolated values in the ther-\n8\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(a)\n(b)\n(c)\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(d)\n(e)\n(f)\nFIG. 4. The results of two-dimensional site and bond percolation with the raw configuration. The vertical coordinates\nof Panels a & d denote the density of active sites/bonds, b & e, the first principal component of PCA, and c &\nf the single latent variable of AE, respectively. Their horizontal coordinates are all occupation probability. They\nall exhibit a simple linear increase in nature and overlap, making it impossible to identify the critical point of the\nsystem. However, there is a highly similar behavior among the three.\nmodynamic limit are pc = 0.597(4) for site percola-\ntion and pc = 0.501(3) for bond percolation, which\nare consistent with theoretical expectations.\nIn comparison, the results obtained by extracting\na single latent variable (h) using an Autoencoder\n(AE) are shown in Figures 5(c) and 5(i). The rela-\ntionship between h and p conforms to a standard\nSigmoid function.\nIn this case, we directly used\nMathematica to fit the data with a sigmoid func-\ntion, see equation 14, yielding a critical value of\npc = 0.590(9) for site percolation, and pc = 0.506(5)\nfor bond, which is consistent with the theoretical\ncritical value.\nf(x) =\n1\n1 + e−k(x−x0)\n(14)\nThe finite-size scaling (FSS) results are shown in\nFigures 5(f) and 5(l).\nAdditionally, we calculated\nthe Pearson correlation coefficient between the AE\noutput and Monte Carlo (MC) results, which is 0.878\n± (0.003) for site and 0.848 ± (0.002) for bond. This\nindicates a strong positive correlation with the den-\nsity of active sites/bonds. Similarly, we calculated\nthe Pearson correlation coefficient between PCA and\nMC results, which is 0.999 ± (0.001) and 0.998 ±\n(0.001). This extremely high positive correlation is\nevident and can even be directly observed from the\nfigures.\nThus, using the same methodology, the results for\nsite percolation and bond percolation are consistent\nwith theoretical values. This strongly indicates that\nthe methods proposed in this study are valid and\neffective.\n3.\nThe shuffled largest cluster\nUpon reviewing the outcomes derived from ana-\nlyzing both the raw configuration and the largest\ncluster, we have delineated two plausible explana-\ntions for the observed results.\nFirstly, it appears\nthat the largest cluster contains critical informa-\ntion of significance. Secondly, there exists the possi-\nbility that unsupervised learning methodologies are\nsolely capable of calculating the density of active\nsites within the configuration. In order to confirm\nthese hypotheses, we carry out a shuffling experi-\nment on the largest cluster, subject to the perfor-\nmance of the computer, here we only do the above\noperation on the site percolation model. Should we\nattain similar results as those obtained with the un-\nshuffled largest cluster (depicted in FIG. III A 2) us-\ning randomly shuffled largest clusters, it would cor-\nroborate that the first principal component of PCA\nand the single latent variable of the AE indeed en-\ncapsulate the density of active sites. Conversely, dis-\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(a)\n(b)\n(c)\n\n\n\n\n0.00\n0.05\n0.10\n0.52\n0.56\n0.60\n0.64\n1/L\np\nEstimate\nStandard Error\npc\n0.595\n0.002\n1/L\n0.473\n0.028\n\n\n\n\n0.00\n0.05\n0.10\n0.52\n0.56\n0.60\n0.64\n1/L\np\nEstimate\nStandard Error\npc\n0.597\n0.004\n1/L\n0.368\n0.061\n\n\n\n\n0.00\n0.05\n0.10\n0.52\n0.56\n0.60\n0.64\n1/L\np\nEstimate\nStandard Error\npc\n0.59\n0.009\n1/L\n0.357\n0.147\n(d)\n(e)\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n◦\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◇\n◦L=10\n△L=20\n× L=30\n◇L=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(g)\n(h)\n(i)\n\n\n\n\n0.00\n0.05\n0.10\n0.42\n0.46\n0.50\n0.54\n1/L\np\nEstimate\nStandard Error\npc\n0.504\n0.001\n1/L\n0.238\n0.017\n\n\n\n\n0.00\n0.05\n0.10\n0.42\n0.46\n0.50\n0.54\n1/L\np\nEstimate\nStandard Error\npc\n0.501\n0.003\n1/L\n0.309\n0.052\n\n\n\n\n0.00\n0.05\n0.10\n0.42\n0.46\n0.50\n0.54\n1/L\np\nEstimate\nStandard Error\npc\n0.506\n0.005\n1/L\n0.113\n0.092\n(j)\n(k)\n(l)\nFIG. 5. The results of two-dimensional percolation model with the largest cluster selected from the raw configuration.\nThe vertical coordinates of Panels a-c denote the density of active sites, the first principal component of PCA, and\nthe single latent variable of AE, respectively. Their horizontal coordinates are all occupation probability. Panels d-f\nthen correspond to FFSS, FFSS and FSS, respectively. The mean correlation coefficients of h, pca1, to density are\n0.878 ± (0.003) and 0.999 ± (0.001), respectively. The vertical coordinates of Panels g-i denote the density of active\nbonds, the first principal component of PCA, and the single latent variable of AE, respectively. Their horizontal\ncoordinates are all occupation probability. Panels j-l then correspond to FFSS, FFSS and FSS, respectively. The\nmean correlation coefficients of h, pca1, to density are 0.848 ± (0.002) and 0.998 ± (0.001), respectively.Both the ffss\nmethod and the fss method exhibit excellent properties in locating critical points.\nparate outcomes would imply that they represent al-\nternative facets of the system.\nUtilizing a random shuffling process with a ra-\ntio of 0.2, we perturbed the largest cluster, as il-\nlustrated in Figure 1, during the transition from b\nto c, while maintaining the constant total number of\noccupied lattice points, we randomly select twenty\npercent.\nof the lattice points (including both ac-\ntive and inactive points) from the entire configura-\ntion map for exchange. and employed the altered\nconfiguration to conduct experiments utilizing the\nthree aforementioned methods. The results are il-\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(a)\n(b)\n(c)\n\n\n\n\n0.00\n0.05\n0.10\n0.52\n0.56\n0.60\n0.64\n1/L\np\nEstimate\nStandard Error\npc\n0.59\n0.002\n1/L\n0.534\n0.025\n\n\n\n\n0.00\n0.05\n0.10\n0.52\n0.56\n0.60\n0.64\n1/L\np\nEstimate\nStandard Error\npc\n0.591\n0.007\n1/L\n0.593\n0.11\n\n\n\n\n0.00\n0.05\n0.10\n0.52\n0.56\n0.60\n0.64\n1/L\np\nEstimate\nStandard Error\npc\n0.589\n0.004\n1/L\n0.421\n0.072\n(d)\n(e)\n(f)\nFIG. 6. The results of two-dimensional site percolation with the shuffled largest cluster at ratio 0.2. The vertical\ncoordinates of Panels a-c denote the density of active sites, the first principal component of PCA, and the single latent\nvariable of AE, respectively. Their horizontal coordinates are all occupation probability. Panels d-f then correspond\nto FFSS, FFSS and FSS, respectively. The mean correlation coefficients of h, pca1, to density are 0.859 ± (0.004)\nand 0.997 ± (0.001), respectively. It exhibits no discernible difference from Figure 5 a-f, whether in terms of locating\ncritical points or the properties of the function’s graph.\nlustrated in FIG. 6. Upon comparison of FIG. 6(a)\nwith FIG. 5(a), it becomes evident from the MC ex-\nperiment outcomes that the density of active sites\nremains unaltered. Intriguingly, akin to FIG. 5(b),\nsimilar outcomes are observed in FIG. 6(b), with\nnegligible alterations in shape and the critical point\npersisting around 0.593. Similarly, FIG. 6(c) yields\noutcomes analogous to those of FIG. 5(c). This pre-\nliminary evidence indicates that the PCA and AE\nmethods exclusively provide insights into the den-\nsity of active sites of the system phase.\n4.\nThe largest cluster of shuffled largest cluster\nLastly, we extracted the largest cluster from the\nshuffled configuration to diminish the number of ac-\ntive sites in the configuration while retaining the\ninformation pertaining to the largest cluster. This\nserves as a secondary method to validate our hy-\npothesis. If we procure akin learning outcomes as\nthose derived from the unaltered largest cluster, it\nindicates that ML can discern certain order parame-\nter information from the largest cluster. Conversely,\ndisparate learning outcomes would imply that the\nresults obtained through ML are inherently linked\nto density of active sites information.\nThe results of the MC simulation are depicted in\nFIG. 7(a). At this juncture, we have significantly\naltered the original configuration by first extracting\nthe largest cluster and subsequently shuffling it with\na certain ratio. Consequently, the spatial correlation\nof the system has undergone modification.\nThus,\ndiscussing the phase transition points of the system\nmay no longer be appropriate. However, to encap-\nsulate the overall alteration in this configuration, we\nopted to employ the term ”jumping location”. As\nevidenced by FIG. 7(a), the jumping location ap-\npears to shift towards the right. This phenomenon\ncan be attributed to the reduction in density of ac-\ntive sites of the extracted largest cluster following\nthe shuffled configuration.\nFIG. 7(b) showcases the outcome of the first prin-\ncipal component of PCA. Notably, the jumping loca-\ntion of the curve also shifts towards the right, mir-\nroring the trend observed in the MC results. The\nremarkable concurrence between the MC density of\nactive sites outcomes and the first principal compo-\nnent of PCA lends credence to the notion that the\nprimary information encapsulated by the first prin-\ncipal component of PCA pertains to the density of\nactive sites. This observation underscores that PCA\npredominantly captures the density of active sites,\nrather than the specific arrangement of active sites.\n11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\ndensity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL=10\nL=20\nL=30\nL=40\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(a)\n(b)\n(c)\nFIG. 7. The experimental results involve selecting the largest cluster from the shuffled largest cluster. panel (a)\nshows the results of the MC simulation, panel (b) the results of PCA, and panel (c) the results of AE.The mean\ncorrelation coefficients of h, pca1, to density are 0.849 ± (0.003) and 0.988 ± (0.002), respectively.A clear observation\ncan be made that the jumping location of the image have shifted to the right.\nOnce again, it is demonstrated that PCA learns the\ndensity of active sites rather than the order param-\neter.\nTo further substantiate this perspective, we con-\nducted learning experiments using an AE neural net-\nwork. FIG. 7(c) illustrates the learning outcome of\nthe AE’s single latent variable. Notably, the jump-\ning location depicted in FIG. 7(c) does not exhibit\na pronounced shift towards the right. This discrep-\nancy could potentially be attributed to the small\nshuffle ratio (r = 0.2).\nIn order to delve into the impact of the shuffle\nratio on the jumping location in greater detail, we\nmaintained the system size at L = 40 and shuf-\nfled the largest cluster at varying ratios, namely 0,\n0.2, 0.4, 0.6, 0.8, and 1.\nThese modified configu-\nrations were then fed into PCA and autoencoder\nfor learning, respectively, as depicted in FIG. 8.\nFIG. 8(a) displays the PCA results, wherein a dis-\ncernible rightward shift in the jumping location of\nthe curve is evident with increasing shuffling ra-\ntio when the system size remains constant.\nSimi-\nlarly, FIG. 8(b) presents the results obtained from\nthe AE. Analogous to PCA, an unmistakable right-\nward shift in the jumping location of the function is\nobserved as the shuffling ratio escalates. This fur-\nther underscores that both methods primarily cap-\nture the density of active sites of the configuration\nrather than the order parameter. For more detailed\nAE results, refer to TABLE. II. The jumping loca-\ntion in TABLE. II is determined through sigmoid\nfunction fitting. It is evident that as the shuffling\nratio increases, the jumping location also increases,\nunderscoring the robustness of the AE’s single latent\nvariable results.\nB.\nDiscussions\nThe four parts above respectively explore four dif-\nferent configurations of site percolation using MC\nsimulations and unsupervised learning techniques:\nthe raw configuration, the largest cluster, the shuf-\nfled largest cluster, and the largest cluster after shuf-\nfling. These findings are summarized in TABLE. III.\nThe raw configuration of site percolation is randomly\ngenerated, and the density of active sites does not\nreveal critical information, thus no critical point is\nidentified in this column. In the largest cluster col-\numn of TABLE. III, we employ three methods to as-\ncertain the critical value. The results indicate that\nthe largest cluster can effectively represent the crit-\nical information of the site percolation model. This\nobservation motivates further investigation into the\nrelationship between density of active sites, PCA’s\nfirst principal component, AE’s individual latent\nvariables, and order parameters.\nIn the shuffled largest cluster column of TA-\nBLE. III, all three methods still capture the crit-\nical point of the model effectively.\nThis suggests\nthat PCA’s first principal component and AE’s sin-\ngle latent variable precisely extract the number of\nactive sites in the system, which appears to have lit-\ntle association with the specific spatial arrangement\nof the active sites. Lastly, a similar study is con-\nducted on the largest cluster after shuffling. Due to\nthe altered spatial correlation of the system, discus-\nsion shifts from the system’s phase transition point\nto the jumping location, illustrating the influence of\nshuffle ratio.\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nps=0.0\nps=0.2\nps=0.4\nps=0.6\nps=0.8\nps=1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\npca1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nps=0.0\nps=0.2\nps=0.4\nps=0.6\nps=0.8\nps=1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.00\n0.25\n0.50\n0.75\n1.00\np\nh\n(a)\n(b)\nFIG. 8. The experimental results of selecting the largest cluster from the raw configuration and shuffling it to select\nthe largest cluster conformations are obtained. The system size is fixed at L = 40, and different shuffling ratios\nare used for the learning results. Panel a shows the results of PCA, and panel b shows the results of AE. With an\nincrease in the probability of shuffling, it is evident that the jumping location of the images are shifting progressively\nto the right.\nr(shuffle ratio)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\njumping location 0.5695(6) 0.6020(4) 0.6192(2) 0.6274(2) 0.6323(2) 0.6384(2) 0.6423(2) 0.6454(1) 0.6482(2) 0.6491(2) 0.6491(2)\nTABLE II. AE results of two-dimensional site percolation with different shuffle ratios, where L = 40. The second\nrow in the table represents the single potential variable results of the largest cluster after shuffling the largest cluster\nwith different shuffle ratios. Corresponding to Figure8b, the sigmoid function is employed.\nmethod\npc\nconfiguration\nraw\nlargest cluster shuffled largest cluster\nMC(density)\nNone\n0.595(2)\n0.590(2)\nPCA(pca1)\nNone\n0.597(4)\n0.591(7)\nAE(latent)\nNone\n0.590(9)\n0.589(4)\nTABLE III. This table presents the critical values of the two-dimensional site percolation model (L = 40) under three\ndifferent methods, MC, PCA, and AE, where the shuffle ratio of the largest cluster is r = 0.2.\nIV.\nCONCLUSION\nPrevious attempts to locate the critical point of\npercolation models using unsupervised learning on\nraw configurations have been unsuccessful. In this\npaper, we propose using the largest cluster to iden-\ntify the critical point and achieve promising results\nwith Monte Carlo (MC) simulations, Principal Com-\nponent Analysis (PCA), and Autoencoders (AE).\nAdditionally, we introduce a novel FFSS method\nthat significantly aids in extracting the critical point\nfrom unconventional functional images.\nIn the process of capturing the critical point us-\ning largest cluster, we observed that the PCA and\nAE learning results of the raw and largest cluster-\ning configurations were very similar to the density of\nactive sites MC results. Through a thorough exami-\nnation of various model configurations, we hypothe-\nsized that the first principal component of PCA and\nthe single latent variable of AE may inherently re-\nflect density of active sites.\nTo test this hypoth-\nesis, we performed a random shuffle of the largest\ncluster and found that the experimental results did\nnot change significantly before and after the shuffle.\nThis confirms that the first principal component of\nPCA and AE’s single latent variable effectively rep-\nresents the density of active sites in the percolation\nmodel.\nHowever, we noted that for largest clusters of the\nsame size, different shuffling probabilities affected\nthe size of the remaining largest clusters. By analyz-\ning the remaining largest clusters after varying de-\n13\ngrees of shuffling, we observed a shift in the system’s\ninflection point. This indicates that random shuf-\nfling alters the correlation length of the system—the\ngreater the shuffling ratio, the smaller the correla-\ntion length. This further supports the notion that\nthe first principal component of PCA and the single\nlatent variable of AE have a physical interpretation\nrelated to density of active sites.\nV.\nACKNOWLEDGEMENTS\nThis\nwork\nwas\nsupported\nin\npart\nby\nKey\nLaboratory\nof\nQuark\nand\nLepton\nPhysics\n(MOE),\nCentral\nChina\nNormal\nUniver-\nsity\n(Grant\nNo.QLPL2022P01),\nResearch\nFund\nof\nBaoshan\nUniversity(BYPY202216,\nBYKY202305),\nWen\nBangchun\nAcademician\nWorkstation(202205AF150032),\nthe Fundamental\nResearch Funds for the Central Universities, China\n(Grant No. CCNU19QN029), the National Natural\nScience Foundation of China (Grant No. 61873104),\nthe 111 Project, with Grant No. BP0820038.\nDuring the preparation of this work the author\nused GPT-4 in order to improve language and read-\nability. After using this tool, the author reviewed\nand edited the content as needed and take full re-\nsponsibility for the content of the publication.\nThe detailed algorithms of how to generate\ndata and use machine learning are shown in the\nGitHub link https://github.com/freeupcoming/site-\npercolation.\n[1] M. I. Jordan and T. M. Mitchell, Science 349, 255\n(2015).\n[2] I. Goodfellow, Y. Bengio, and A. Courville, Deep\nlearning 1, 98 (2016).\n[3] B. Mahesh, International Journal of Science and Re-\nsearch (IJSR) Volume 9 Issue 1 (2020).\n[4] A. Engel and C. Van den Broeck, Statistical mechan-\nics of learning (Cambridge University Press, 2001).\n[5] P. Mehta, M. Bukov, C.-H. Wang, A. G. Day,\nC. Richardson, C. K. Fisher, and D. J. Schwab,\nPhysics reports 810, 1 (2019).\n[6] G. Carleo,\nI. Cirac,\nK. Cranmer,\nL. Daudet,\nM. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zde-\nborov´a, Reviews of Modern Physics 91, 045002\n(2019).\n[7] J. Carrasquilla, Advances in Physics: X 5, 1797528\n(2020).\n[8] H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,\nand M. Marchand, arXiv preprint arXiv:1412.4446\n(2014).\n[9] J. Hammersley, Monte carlo methods (Springer Sci-\nence & Business Media, 2013).\n[10] C. Domb, The critical point: a historical introduc-\ntion to the modern theory of critical phenomena\n(CRC Press, 1996).\n[11] H. Hinrichsen, Advances in physics 49, 815 (2000).\n[12] D. J. Amit and V. Martin-Mayor, Field Theory, the\nRenormalization Group, and Critical Phenomena:\nGraphs to Computers Third Edition (World Scien-\ntific Publishing Company, 2005).\n[13] J. Carrasquilla and R. G. Melko, Nature Physics 13,\n431 (2017).\n[14] L. Wang, Physical Review B 94, 195105 (2016).\n[15] J. Shen, W. Li, S. Deng, and T. Zhang, Physical\nReview E 103, 052140 (2021).\n[16] T. Ohtsuki and T. Ohtsuki, Journal of the Physical\nSociety of Japan 85, 123706 (2016).\n[17] D.-L. Deng, X. Li, and S. Das Sarma, Physical Re-\nview B 96, 195145 (2017).\n[18] J. F. Rodriguez-Nieva and M. S. Scheurer, Nature\nPhysics 15, 790 (2019).\n[19] D. P. Kingma, M. Welling, et al., Foundations and\nTrends® in Machine Learning 12, 307 (2019).\n[20] Y. Pu,\nZ. Gan,\nR. Henao,\nX. Yuan,\nC. Li,\nA. Stevens, and L. Carin, Advances in neural in-\nformation processing systems 29 (2016).\n[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\ngio, Communications of the ACM 63, 139 (2020).\n[22] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and\nS. Y. Philip, IEEE transactions on neural networks\nand learning systems 32, 4 (2020).\n[23] F. Ma, F. Liu, and W. Li, Physical Review D 108,\n072007 (2023).\n[24] H. Abdi and L. J. Williams, Wiley interdisciplinary\nreviews: computational statistics 2, 433 (2010).\n[25] S. J. Wetzel, Physical Review E 96, 022140 (2017).\n[26] W. Hu, R. R. Singh, and R. T. Scalettar, Physical\nReview E 95, 062122 (2017).\n[27] Y. Wang, W. Li, F. Liu, and J. Shen, Machine\nLearning: Science and Technology 5, 015033 (2024).\n[28] X.-Y. Dong, F. Pollmann, and X.-F. Zhang, Physi-\ncal Review B 99, 121104 (2019).\n[29] S. R. Broadbent and J. M. Hammersley, in Math-\nematical proceedings of the Cambridge philosophical\nsociety, Vol. 53 (Cambridge University Press, 1957)\npp. 629–641.\n[30] S. Kirkpatrick, Reviews of modern physics 45, 574\n(1973).\n[31] V. K. Shante and S. Kirkpatrick, Advances in\nPhysics 20, 325 (1971).\n[32] O. Artime, M. Grassia, M. De Domenico, J. P.\nGleeson, H. A. Makse, G. Mangioni, M. Perc, and\nF. Radicchi, Nature Reviews Physics 6, 114 (2024).\n14\n[33] P. Ji, J. Ye, Y. Mu, W. Lin, Y. Tian, C. Hens,\nM. Perc, Y. Tang, J. Sun, and J. Kurths, Physics\nreports 1017, 1 (2023).\n[34] W. Zhang, J. Liu, and T.-C. Wei, Physical Review\nE 99, 032142 (2019).\n[35] S. Cheng, F. He, H. Zhang, K.-D. Zhu, and Y. Shi,\narXiv preprint arXiv:2101.08928 (2021).\n[36] S. Jianmin, W. Shanshan, L. Wei, X. Dian, Y. Yuxi-\nang, W. Yanyang, G. Feng, Z. Yueying, and T. Kui,\narXiv preprint arXiv:2311.11741 (2023).\n[37] A. Ng et al., CS294A Lecture notes 72, 1 (2011).\n[38] O. Riordan and L. Warnke, Science 333, 322 (2011).\n[39] K. Christensen and N. R. Moloney, Complexity and\ncriticality, Vol. 1 (World Scientific Publishing Com-\npany, 2005).\n15\n",
  "categories": [
    "cond-mat.stat-mech",
    "cs.LG"
  ],
  "published": "2023-11-20",
  "updated": "2024-12-09"
}