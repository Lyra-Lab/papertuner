{
  "id": "http://arxiv.org/abs/2311.14766v1",
  "title": "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing",
  "authors": [
    "Feiyang Han",
    "Yimin Wei",
    "Zhaofeng Liu",
    "Yanxing Qi"
  ],
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) has played a crucial role\nin the success of large models such as ChatGPT. RLHF is a reinforcement\nlearning framework which combines human feedback to improve learning\neffectiveness and performance. However, obtaining preferences feedback manually\nis quite expensive in commercial applications. Some statistical commercial\nindicators are usually more valuable and always ignored in RLHF. There exists a\ngap between commercial target and model training. In our research, we will\nattempt to fill this gap with statistical business feedback instead of human\nfeedback, using AB testing which is a well-established statistical method.\nReinforcement Learning from Statistical Feedback (RLSF) based on AB testing is\nproposed. Statistical inference methods are used to obtain preferences for\ntraining the reward network, which fine-tunes the pre-trained model in\nreinforcement learning framework, achieving greater business value.\nFurthermore, we extend AB testing with double selections at a single time-point\nto ANT testing with multiple selections at different feedback time points.\nMoreover, we design numerical experiences to validate the effectiveness of our\nalgorithm framework.",
  "text": "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing\nto ANT Testing\nFeiyang Han 1, *\nYimin Wei 2, †\nZhaofeng Liu 3, ‡\nYanxing Qi 3, §\n1 School of Mathematical Sciences, Fudan University, Shanghai, 200433, P. R. China. This author is supported by the National\nNatural Science Foundation of China under grant 12271088.\n2 Corresponding author. School of Mathematical Sciences and Shanghai Key Laboratory of Contemporary Applied\nMathematics, Fudan University, Shanghai, 200433, P. R. China. This author is supported by Innovation Program of Shanghai\nMunicipal Education Commission and the National Natural Science Foundation of China under grant 12271088.\n3 Shenzhen Tencent computer System Co.Ltd., Shenzhen, Guangdong, 518054, P. R. China.\nAbstract\nReinforcement Learning from Human Feedback (RLHF)\nhas played a crucial role in the success of large models such\nas ChatGPT. RLHF is a reinforcement learning framework\nwhich combines human feedback to improve learning\neffectiveness and performance. However, obtaining prefer-\nences feedback manually is quite expensive in commercial\napplications. Some statistical commercial indicators are\nusually more valuable and always ignored in RLHF. There\nexists a gap between commercial target and model training.\nIn our research, we will attempt to fill this gap with statistical\nbusiness feedback instead of human feedback, using AB\ntesting which is a well-established statistical method. Rein-\nforcement Learning from Statistical Feedback (RLSF) based\non AB testing is proposed. Statistical inference methods are\nused to obtain preferences for training the reward network,\nwhich fine-tunes the pre-trained model in reinforcement\nlearning\nframework,\nachieving\ngreater\nbusiness\nvalue.\nFurthermore, we extend AB testing with double selections at\na single time-point to ANT testing with multiple selections\nat different feedback time points. Moreover, we design\nnumerical experiences to validate the effectiveness of our\nalgorithm framework.\nKeywords: Business Value, Reinforcement Learning, AB\nTesting, Statistical Feedback.\n1\nIntroduction\nIn recent years, the development of artificial intelligence\ntechnology has promoted the research and application of AI\nGenerated Content (AIGC). From text generation to image\ngeneration, from music creation to video production, AIGC\n(Cao et al. 2023) has covered numerous application scenar-\nios. AI generated content can greatly improve production ef-\nficiency and reduce labor costs, while also maintaining qual-\nity and accuracy of content. With the emergence of ChatGPT\n*E-mail: 19110180030@fudan.edu.cn.\n†E-mail: ymwei@fudan.edu.cn and yimin.wei@gmail.com\n‡E-mail: zhaofengliu@tencent.com.\n§E-mail: yanxingqi@tencent.com.\nand some image generation models, AIGC has entered peo-\nple’s lives and work with zero distance. Everyone can search\nfor their own treasures in the ocean of AIGC. Meanwhile, it\nis undeniable that AIGC brings about revolutionary changes\nin business world, which generates plenty of new challenges\nand chances. In the training process of AIGC models, apart\nfrom the requirements of generating quality, logical correct-\nness, and information completeness that meet human pref-\nerences, the differences in commercial value brought by dif-\nferent generation choices are also significant and cannot be\nignored. The diverse data in commercial application scenar-\nios not only pose significant challenges in data acquisition\ncompared to human-labeled data but also hold greater com-\nmercial value. How to combine a unified business goal and\nAIGC model parameter learning will be an important re-\nsearch topic.\nIn addition to more advanced models and high-quality\ndiverse training data, reinforcement Learning from Human\nFeedback (RLHF) is another important drivers for the suc-\ncess of AIGC models such as ChatGPT. In traditional re-\ninforcement learning, the agent can only receive feedback\nthrough reward signals from the environment. However, in\nsome cases, reward signals may be difficult to define or in-\naccurate. In such cases, human feedback can be used to sup-\nplement reward signals. The reinforcement learning frame-\nwork combined with human expertise greatly expands the\nspace for improvement of pre-trained models in the parame-\nter fine-tuning stage. In reinforcement learning from human\nfeedback, feedback data is usually in the form of scores or\npreferences given by humans, which might be very expen-\nsive to obtain. Meanwhile, in commercial applications, spe-\ncific business indicators such as revenues and user retention\nrates are often more valuable, which are not token into con-\nsideration in RLHF. In this paper, we will focus on the gap\nbetween business objectives and neural network reinforce-\nment learning.\nA/B testing (King, Churchill, and Tan 2017; Siroker and\nKoomen 2015) is a method used to compare two different\nstrategies, typically to determine which strategy is most ef-\nfective for achieving a specific goal. In AB testing, the tested\nusers are randomly divided into two groups: Group A and\narXiv:2311.14766v1  [cs.LG]  24 Nov 2023\nGroup B (Gilotte et al. 2018). The only difference between\nthese two groups is the tested strategy, with other conditions\nfixed. Feedback results from both groups are then collected\nand compared to determine which strategy performs better\nstatistically. It can be noticed that using AB testing to com-\npare recommended business values can replace human pref-\nerences in RLHF. In this article, we use a reward network\nto predict such preferences, thus forming a reinforcement\nlearning framework for business value feedback.\nThe main contributions of this research are summarized\nas follows,\n(a) Introducing Reinforcement Learning from Statistical\nFeedback (RLSF), an economical End-to-End frame-\nwork with more business value considered, which utilizes\nthe differences in commercial preferences generated by\nAB testing to fine-tune the pre-trained models through\nreinforcement learning.\n(b) Promoting AB testing to AN testing, with a reward net-\nwork to extend from binary choice preferences to n-\nchoice preferences at one time point.\n(c) Exploring ANT testing, which involves training different\nreward networks on multiple feedback cycles to fit differ-\nent business indicators, with two reinforcement learning\nframework, gradual and one-time feedback.\nThis paper is organized as follows. In Section 2, some re-\nlated researches are summarized and reviewed. Moreover,\nReinforcement Learning from Business Feedback is intro-\nduced in Section 3 with statistical experiment details. Be-\nsides, in Section 4, we extend AB testing to AN testing and\nANT testing, generating corresponding frameworks to fine-\ntune the pre-trained models. Numerical experiments are de-\nsigned to illuminate the effectiveness of RLSF in Section 5.\nFinally, in Section 6, we summarize our theory and numer-\nical results, obtaining the conclusions and future problems\nabout our research.\n2\nRelated Works\n2.1\nHuman Feedback Learning\nOur study to obtain the business value instead of just im-\nproving generation quality is built on a lot of previous re-\nsearches about Reinforcement Learning from Human Feed-\nback (Christiano et al. 2017; Ibarz et al. 2018; Ouyang et al.\n2022), such as human ratings or rankings. Moreover, rein-\nforcement framework has been studied to fine-tune the pre-\ntrained NLP models to summarize text (B¨ohm et al. 2019;\nGao, Meyer, and Gurevych 2018; Stiennon et al. 2020; Wu\net al. 2021; Ziegler et al. 2019). In addition, there are plenty\nof fields to train a reward model with human feedback and\nfine-tune with reinforcement framework (Bıyık et al. 2022;\nMadaan et al. 2022; Sumers et al. 2021). There are also lots\nof works to reduce the cost of tailored human feedback, such\nas (Liang et al. 2022), which is also one of our motivations\nto take some statistical business preferences into considera-\ntion. For commercial Internet company, such as Google and\nMeta, there are a great deal of users and we do not need to\nlabel the preferences manually.\n2.2\nStatistical Deep Learning\nIn 2017, there are researchers writing an important review\nabout statistical, deep, and reinforcement learning (Sharma\nand Kaushik 2017), from which we have learned a lot. Be-\nsides, scholars compare different reinforcement learning al-\ngorithms (Colas, Sigaud, and Oudeyer 2019), especially in-\ntroducing a hitchhiker’s guide. There are also plenty of re-\nsearches, which focus on different fields about the combi-\nnation of statistical inferences and deep learning (Bartlett,\nMontanari, and Rakhlin 2021; Jin et al. 2021; Shah, Wang,\nand He 2020; Viharos and Jakab 2021). Our work also at-\ntempts to build the bridge between traditional statistical\nmethods and reinforcement learning to achieve more com-\nmercial benefits.\n2.3\nDeep Learning in Commercial Cases\nThis part of deep learning is very attractive for researchers\nfrom not only company but also colleges. Taking an essen-\ntial target, click-through rate (CTR), as an example, there\nare a great many researches to predict CTR in commercial\ncases (Wang et al. 2023; Zhang et al. 2021). Among them,\ndeep learning approaches are utilized to obtain more infor-\nmation from users, such as deeper feature interaction (Guo\net al. 2017) and attention block (Zhou et al. 2018). Besides,\nother commercial indicators are also studied by deep learn-\ning method, such as Life Time Value (Chen et al. 2018) and\nretention index (Vrzal, Maleˇckov´a, and Olˇsovsk´a 2021). All\nthese useful models and methods are also valuable in our\nRLSF framework, because all these strong well-trained re-\nward networks could be utilized to predict these commercial\nindicators.\n3\nRLSF Framework\nIn order to incorporate business data indicators as feedback\nfor reinforcement learning training, it is necessary to model\nthe business data into feedback data that can be used for re-\ninforcement learning. However, for individual users, binary\nindicators such as retention or continuous indicators such\nas in-App purchases are influenced by various objective or\nindividual factors. Using them directly as feedback for rein-\nforcement learning is not appropriate. Therefore, we utilize\nthe results generated through statistical inference using AB\ntesting with randomly sampled users as preference data that\ncan provide business feedback.\n3.1\nRLSF Framework with AB Testing\nFirstly, we need to choose the experimental objectives, such\nas retention rates of an application, pass rates of the next\nlevel in the game, or other business indicators, similar to\ntraditional AB testing. Taking the example of an intelligent\nNPC in a text-based dialogue game scenario, we aim to fa-\ncilitate user engagement and improve the retention rate in\nthe game through interactions between users and NPCs. Ad-\nditionally, we expect the NPC to serve as a guide for game\nlevel progression. Assume that the business indicators we\nare interested in for this experiment is η.\nNext, we will randomly select two groups of users with\nan equal number for AB testing. It is important to note that\nuser sampling should be done in a way that makes the dis-\ntribution of the two groups as similar as possible. For these\ntwo groups of users, we consider an agent interacting with\nan environment through a series of steps. We can model such\na reinforcement learning scenario as follows: the agent takes\nan action a based on the statement s from the environment.\nFor the two interaction flows that need to be tested, we can\ndefine them as F1 and F2, with their corresponding business\nindicators, η1 and η2. The hypothesis for this experiment is\nH0 : η1 = η2, H1 : η1 ̸= η2,\n(1)\nor\nH0 : η1 ≤η2, H1 : η1 > η2.\n(2)\nOf all the complete interaction flows, only one interaction\nproduces a different sample, i.e.,\nF1 =\n\u0010\nσ1, · · · , σ(1)\nn0 , · · · , σn\n\u0011\n,\nF2 =\n\u0010\nσ1, · · · , σ(2)\nn0 , · · · , σn\n\u0011\n,\n(3)\nwhere\nσ(i)\nn0 =\n\u0010\u0010\ns(i)\n0 , a(i)\n0\n\u0011\n, . . . ,\n\u0010\ns(i)\nk−1, a(i)\nk−1\n\u0011\u0011\n∈(S × A)k ,\ni = 1, 2.\nSimilarly, taking the example of an intelligent NPC in a\ntext-based dialogue game scenario, the dialogue flow be-\ntween the two experimental groups differs only in one round\nof dialogue. In the different round, each step’s state consists\nof existing text tokens, while the corresponding set of po-\ntential actions comprises the entire word corpus. The next\nstate is generated by sampling from the corpus based on the\nprobabilities provided by the pre-trained model.\nIn this way, hypothesis testing can be conducted on the\nbusiness indicators of the two user sample groups, consid-\nering the interaction flow that occurs only once. If the con-\nfidence level is below α and the statistical power is below\n1−β, the sample group is discarded. Otherwise, the relative\ncommercial value ranking between the two sample groups is\ndetermined based on their relative sizes of σ1 and σ2.\nDefinition 1. We could define that the preferences ≻in\n(S × A)k if\nσ1 =\n\u0000\u0000s1\n0, a1\n0\n\u0001\n, . . . ,\n\u0000s1\nk−1, a1\nk−1\n\u0001\u0001\n≻\n\u0000\u0000s2\n0, a2\n0\n\u0001\n, . . . ,\n\u0000s2\nk−1, a2\nk−1\n\u0001\u0001\n= σ2\nwhenever the business feedback η1 and η2 satisfies that η1 >\nη2.\nNext, we need to provide such business value preferences\nas feedback to the pre-trained model. Similar to human\nfeedback reinforcement learning, we use neural networks to\nlearn these preferences. Specifically, we train a suitable re-\nward network r which satisfies that\nr\n\u0000s1\n0, a1\n0\n\u0001\n+ · · · + r\n\u0000s1\nk−1, a1\nk−1\n\u0001\n> r\n\u0000s2\n0, a2\n0\n\u0001\n+ · · · + r\n\u0000s2\nk−1, a2\nk−1\n\u0001\nwhenever σ1 ≻σ2. Furthermore, the reward network is\ntrained to minimize the cross-entropy loss between the pre-\ndicted preference distribution and the actual statistical busi-\nness preference distribution, i.e.,\nloss(ˆr) = −\nX\n(σ1,σ2,δ)∈D\nκ(σ1, σ2) log ˆP [σ1 ≻σ2]\n+κ(σ1, σ2) log ˆP [σ2 ≻σ1]\n(4)\nwhere\nˆP [σ1 ≻σ2] =\nexp P ˆr\n\u0000s1\nt, a1\nt\n\u0001\nexp P ˆr (s1\nt, a1\nt) + exp P ˆr (s2\nt, a2\nt)\nis the predicted preference distribution and δ is the actual\nstatistical business preference distribution from AB testing,\ni.e.,\nκ(σ1, σ2) =\n\n\n\nδ(1), if σ1 > σ2,\nδ(2), if σ1 < σ2,\n\u0002\nδ(1) + δ(2)\u0003\n/2, if σ1 = σ2.\n(5)\nHere δ(x0) is the one point distribution at x0, which means\nthat δ(x0)(x) = 1 if and only if x = x0.\n3.2\nSome Details in AB testing\nIn AB testing, we first need to specify the significance level\nα, the type II error β, and the sample size M. Besides, we\nneed to select M samples without replacement as unbiasedly\nas possible to form two sample groups to test the preference\ndifferences between two choices generated by our pretrained\nmodel.\nThen, we need to determine whether there is a signifi-\ncant difference in the business indicators between the two\nexperimental groups. Without loss of generality, we assume\nη1 > η2. At this time, the null hypothesis and alternative\nhypothesis are\nH0 : η1 = η2, H1 : η1 ̸= η2.\n(6)\nThere may be some invalid points in the samples and we\nshould remove these invalid points. Suppose that the rest\nsample sizes of two groups are M1 and M2, which satisfies\nM2 = k · M1. The t-statistic can be expressed as:\nt =\n¯X1 −¯X2\nSp\nq\n1\nM1 +\n1\nM1\n,\n(7)\nwhere\nS2\np = (M1 −1)S2\n1 + (M2 −1)S2\n1\nM1 + M2 −2\n.\n(8)\nIf t < t1−α/2, we accept the null hypothesis. Otherwise, we\nreject the null hypothesis and choose the alternative hypoth-\nesis.\nFurthermore, we need to test whether there is a significant\ndifference between these two samples. The target gap δ to be\ntested should satisfy that δ = max{δ0, |η1 −η2|}, where ηi\nis the indicator from sample i and δ0 is the minimum target\ngap given at first. Here we rename ηi as µi for convenience.\nAt this step, the null hypothesis and alternative hypothesis\nare\nFigure 1: Reinforcement Learning Framework from Statistical Feedback based on AN Testing. For taking AB testing among N\nchoices generated by the pretrained model, the users sampled from user pool should be divided into N groups. After AB testing\nbetween any two user groups, we can obtain C2\nN statistical business preference feedback, which could take the choices in order\nwith ELo Algorithm (Zanco et al. 2022). Furthermore, the rank trains the reward model, minimizing the pair-wise loss. Finally,\nthe reward model can fine tune the pretrained model, where there are plenty of reinforcement algorithms to be utilized.\nH0 : µ1 −µ2 = δ, H1 : µ1 −µ2 ̸= δ.\n(9)\nAccording to traditional statistical knowledge, the mini-\nmum sample size for testing mean-related indicators, such as\naverage playback duration, average payment per user, etc.,\ncan be calculated from\nn1 = (σ2\n1 + σ2\n2/k)(z1−α/2 + z1−β)2\nδ2\n(10)\nand n2 = k · n1, where ni is the minimum size, µi is the\nmean of target business value and σi is the standard de-\nviation of target business value of i-th sample group, for\ni = 1, 2, respectively. Moreover, z1−α/2 and z1−β could\nbe found from the normal distribution table.\nHowever, when testing ratio-related indicators, such as\nclick-through rate, 7-day retention rate, etc., the minimum\nsample size is\nn1 =\nhq\n¯p¯q\n\u00001 + 1\nk\n\u0001\nz1−α/2 +\np\np1q1 + p2q2\nk z1−β\ni2\nδ2\n(11)\nand n2 = k · n1, where pi is the target business value of i-th\nsample group, for i = 1, 2, respectively, and\nq1 = 1 −p1, q2 = 1 −p2,\n¯p = p1 + kp2\n1 + k\n, ¯q = 1 −¯p.\n(12)\nSimilarly, ηi is renamed as pi in this case.\nIf the sample sizes of the two groups, M1 and M2, respec-\ntively, satisfy that\nn1 ≤M1, n2 ≤M2,\n(13)\nwe will accept the null hypothesis that there is significant\ndifference in the business indicators between the samples. If\nδ > 0, we can gain the result that the first choice is better,\nwhile the second choice is determined to be better, if δ <\n0. Otherwise, there are two choices. The first choice is that\nwe reject the null hypothesis and conclude that there is no\nsignificant difference in the business indicators between the\nsamples. However, the other choice is to sample more users\nfrom user pool to build two bigger test groups and take the\nabove AB testing again, where the minimum sizes n1 and\nn2 could be referred. The preference from the set with 4\nelements, { η1 > η2, η1 < η2, η1 = η2, no result}, can be\nutilized to train a reward model, with no need to cost a lot\nfor labeling preference data manually like original RLHF.\nThe algorithm framework is summarized in the next page.\nAlgorithm 1: RLSF with AB Testing\nRequire: Target indicator, η; Target indicator gap, δ0; Tar-\nget sample size, M; two Choices, σ1 and σ2; the signifi-\ncance level, α; the type II error, β.\nEnsure: the Preference between two choices.\nStep 1. Sample two user groups with M members from\nuser pool and offer different flows to these groups. Re-\nmove invalid points and get the sample sizes, M1 and M2.\nStep 2. Take T-test on the hypothesis (6). Compute the\nt-value from (7) to determine whether there exists a sig-\nnificant difference between two groups.\nif η1 = η2 then\nreturn σ1 = σ2\nelse\nTest the hypothesis (9). Compute the minimum sizes of\nsamples, n1 and n2, from (10) or (11).\nif n1 ≤M1 and n2 ≤M2 then\nif δ > 0 then\nreturn σ1 ≻σ2\nelse\nreturn σ2 ≻σ1\nend if\nelse\nreturn no result\nOr sample more users and back to Step 1. (⋆)\nend if\nend if\n4\nExtensions from AB Testing to ANT\nTesting\nIn this section, we will generate a framework to solve the\nproblem about how to handle more than two choices in one\nreinforcement feedback step. Moreover, the cases with two\nor more steps are also considered.\n4.1\nRLBF Framework with AN Testing\nIn the last section, we derive the RLBF (Reinforcement\nLearning framework from Business Feedback) based on AB\ntesting. Furthermore, if there are more than two choices for\na fixed position interaction within the same interaction flow,\nwe need to construct a more generalized AN testing frame-\nwork. For example, in the context of text-based dialogue,\nit can be used to determine the relative commercial value\nof potential answers to a specific question. Similarly, in the\nfield of image generation, such as AI-generated advertising\nimages, the relative click-through rates of various choices\nin a recommended and ranked stream of materials can be\nevaluated. We refer to this training process as Reinforcement\nLearning from Business Feedback based on AN Testing, em-\nphasizing the influences of N choices instead of just the im-\npact of two choices in AB testing on commercial indicators.\nTaking text generation as an example, it can be modeled\nas a sequential decision-making problem in the token space.\nFirstly, the context is treated as the state in reinforcement\nlearning, and the action is selecting the next token after the\nprevious token. In this case, there are N choices available. To\nmeet the requirements of AN testing, N groups of user pools\nneed to be selected, and AB testing is conducted between\neach pair. Based on this AB testing, using the methods dis-\ncussed in the previous chapter, we can obtain C2\nN statistical\nbusiness feedback preferences. The figure below illustrates\nthe RLBF framework based on AN testing in the field of text\ngeneration. The direction of the arrows between the nodes in\nAB testing block from the diagram represents the direction\nof the preference order, while the absence of arrows indi-\ncates that there is no significant difference in the statisti-\ncal business feedback indicators between the two choices.\nMoreover, we can utilize the Elo algorithm to establish a\ncomplete ranking among different choices within the same\nprompt. In this way, we can use a pair-wise loss function\nto fit such ranking results. As a result, we obtain scores for\nresponses corresponding to the same prompt, which will be\nused for reinforcement learning of the pretrained model.\nFrom the above analysis, it should be noticed that AB test-\ning is a special case of AN testing, which allows for more\nflexibility in handling multi-choice problems and capturing\ndifferences between different choices. Figure (4) presents\nthe whole framework of Reinforcement Learning from Sta-\ntistical Feedback based on AN testing.\n4.2\nRLBF Framework with ANT Testing\nThis section discusses the RLBF framework based on ANT\ntesting with multiple feedback cycles and multiple choices,\nwhich could be referred in Figure 2. Commercial data feed-\nback cycles naturally have asynchronous characteristics. For\nexample, important commercial data includes conversation\ncompletion rate obtained immediately, next-day retention\nfeedback delayed by one day, monthly active users delayed\nby one month, and lifetime value (LTV) feedback similar to\nan infinite horizon. We need to train reward networks sepa-\nrately for each of these feedback cycles.\nRegarding the fusion of multiple rewards, we design two\nreinforcement learning modes, incremental feedback and\none-time feedback. The incremental feedback mode means\nupdating the reinforcement learning after each commercial\ndata feedback. This mode has the advantage of simple oper-\nation and no additional computational overhead. However,\nthe disadvantage is also relatively obvious, as the segmented\nreward function makes it difficult for the pre-trained model\nto achieve the optimal expected result.\nThe one-time feedback mode requires the fusion of multi-\nple reward functions, using a functional or some neural net-\nworks.\nrtarget = F\n ∞\nX\nk=1\nwkfk(rk)\n!\n(14)\nIn addition, unreceived feedback (such as LTV) needs to\nbe predicted. This mode enables flexible selection and cre-\nation of more valued commercial indicators by adjusting the\nweight of multi-objective fusion. For example, if the weight\nof a commercial indicator is set to approach infinity, it is\nconsistent with the commonly used “North Star Indicator”\nin traditional business analysis. However, the disadvantage\nof one-time feedback is also prominent, as it requires higher\nexpert prior knowledge, and unreceived feedbacks need to\nFigure 2: Reinforcement Learning Framework from Statistical Feedback based on ANT Testing. CTR means the Click Through\nRate, 7(30)-Ret means the 7(30) day Retention and LTV means the Life Time Value. Some immediate indicators like CTR could\nbe gained soon and plenty of indicators, for example 7(30)-Ret, are all time-delaying, while some indicators such as LTV can\nnot be computed and should be predicted. In this time series, we could take the reinforcement feedbacks one by one, naming\nas the gradual feedback. Besides, we can build a functional to consider all indicators together, not only obtatined but also\npredicted, into one target to fine tune, naming as one-time feedback.\nbe predicted, which brings significant computational over-\nhead and errors.\n5\nNumerical Experiements\nWe test our RLSF framework on the fine-tuning process of\npre-trained comment generation network based on Trans-\nformer structure. Our purpose is to attract users to click on\nthe detailed page of the products through better comments.\nIn this case, labeling manually by virtual users is meaning-\nless and might be harmful to ignore the true preferences.\nHowever, statistical results can teach the pre-trained net-\nwork better with nearly no cost, where PPO algorithm (En-\ngstrom et al. 2020; Schulman et al. 2017) is adopted. We\nuse the emotional rating network to present a score for the\ncomments generated by pre-trained model. From the follow-\ning figure, the pre-trained model can generate the comments\nwith more stable and better quality.\nFigure 3: Emotional scores during the Training process,\nwhere the horizontal axis is the epochs. The emotional\nscores become more and more stable during the fine-tuning\nprocess.\nFigure 4: After Test 1, ni ≤Mi, i = 1, 2, are not satisfied.\nAfter resampling, we retest them again. We accept the zero\nhypothesis if ni ≤Mi, i = 1, 2 like Test 2.1, while the zero\nhypothesis should be rejected in Test 2.2.\nIn the above experiment, we set the minimum target in-\ncrease as δ0 = 1%, the confidence level as α = 0.05 and\nthe statistical power as β = 0.2. RLSF framework indeed\nimprove the Click-Through Rate than the initial pre-trained\nmodel, which only considers the accuracy of generated texts\nbut no commercial indicators.\nBesides, we test the minimum sample sizes and the re-\nsample framework in AB testing, which could be found at\n(⋆) in Algorithm 1.\n6\nSummaries\nIn this paper, we propose a Reinforcement Learning from\nStatistical Feedback (RLSF) based on hypothesis testing,\nwhich reduces the reliance on expensive manually labeled\npreference data, while also taking the statistical business\npreferences into consideration to obtain more commercial\nvalue. In addition to AB testing, we also introduce AN test-\ning, combined with ELo sorting algorithm to solve the prob-\nlems with more than two choices at one time. Moreover,\nANT testing is considered to handle time series feedback,\nincluding not only time delaying but some predicted feed-\nback. Finally, we design an experiment to illuminate the ef-\nfectiveness of RLSF framework.\nLooking to the future, we believe that this framework can\nbe used not only in the field of text generation, but also in\nthe field of image or audio generation, which might bring\ngreater academic and practical value and attract more schol-\nars to research on this topic.\nReferences\nBartlett, P. L.; Montanari, A.; and Rakhlin, A. 2021. Deep\nlearning: a statistical viewpoint. Acta numerica, 30: 87–201.\nBıyık, E.; Losey, D. P.; Palan, M.; Landolfi, N. C.; Shevchuk,\nG.; and Sadigh, D. 2022. Learning reward functions from\ndiverse sources of human feedback: Optimally integrating\ndemonstrations and preferences. The International Journal\nof Robotics Research, 41(1): 45–67.\nB¨ohm, F.; Gao, Y.; Meyer, C. M.; Shapira, O.; Dagan, I.; and\nGurevych, I. 2019. Better rewards yield better summaries:\nLearning to summarise without references. arXiv preprint\narXiv:1909.01214.\nCao, Y.; Li, S.; Liu, Y.; Yan, Z.; Dai, Y.; Yu, P. S.; and Sun,\nL. 2023. A comprehensive survey of ai-generated content\n(aigc): A history of generative ai from gan to chatgpt. arXiv\npreprint arXiv:2303.04226.\nChen, P. P.; Guitart, A.; del R´ıo, A. F.; and Peri´anez, A. 2018.\nCustomer lifetime value in video games using deep learning\nand parametric models. In 2018 IEEE international confer-\nence on big data (big data), 2134–2140. IEEE.\nChristiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.;\nand Amodei, D. 2017. Deep reinforcement learning from\nhuman preferences.\nAdvances in neural information pro-\ncessing systems, 30.\nColas, C.; Sigaud, O.; and Oudeyer, P.-Y. 2019. A hitch-\nhiker’s guide to statistical comparisons of reinforcement\nlearning algorithms. arXiv preprint arXiv:1904.06979.\nEngstrom, L.; Ilyas, A.; Santurkar, S.; Tsipras, D.; Janoos,\nF.; Rudolph, L.; and Madry, A. 2020. Implementation mat-\nters in deep policy gradients: A case study on ppo and trpo.\narXiv preprint arXiv:2005.12729.\nGao, Y.; Meyer, C. M.; and Gurevych, I. 2018. APRIL: In-\nteractively learning to summarise by combining active pref-\nerence learning and reinforcement learning. arXiv preprint\narXiv:1808.09658.\nGilotte, A.; Calauz`enes, C.; Nedelec, T.; Abraham, A.; and\nDoll´e, S. 2018. Offline a/b testing for recommender systems.\nIn Proceedings of the Eleventh ACM International Confer-\nence on Web Search and Data Mining, 198–206.\nGuo, H.; Tang, R.; Ye, Y.; Li, Z.; and He, X. 2017. DeepFM:\na factorization-machine based neural network for CTR pre-\ndiction. arXiv preprint arXiv:1703.04247.\nIbarz, B.; Leike, J.; Pohlen, T.; Irving, G.; Legg, S.; and\nAmodei, D. 2018. Reward learning from human preferences\nand demonstrations in atari. Advances in neural information\nprocessing systems, 31.\nJin, D.; Yu, Z.; Jiao, P.; Pan, S.; He, D.; Wu, J.; Philip, S. Y.;\nand Zhang, W. 2021. A survey of community detection ap-\nproaches: From statistical modeling to deep learning. IEEE\nTransactions on Knowledge and Data Engineering, 35(2):\n1149–1170.\nKing, R.; Churchill, E. F.; and Tan, C. 2017.\nDesigning\nwith data: Improving the user experience with A/B testing. ”\nO’Reilly Media, Inc.”.\nLiang, X.; Shu, K.; Lee, K.; and Abbeel, P. 2022. Reward\nuncertainty for exploration in preference-based reinforce-\nment learning. arXiv preprint arXiv:2205.12401.\nMadaan, A.; Tandon, N.; Clark, P.; and Yang, Y. 2022.\nMemory-assisted prompt editing to improve gpt-3 after de-\nployment. arXiv preprint arXiv:2201.06009.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nShah, D.; Wang, J.; and He, Q. P. 2020. Feature engineering\nin big data analytics for IoT-enabled smart manufacturing–\ncomparison between deep learning and statistical learning.\nComputers & Chemical Engineering, 141: 106970.\nSharma, A. R.; and Kaushik, P. 2017. Literature survey of\nstatistical, deep and reinforcement learning in natural lan-\nguage processing. In 2017 International conference on com-\nputing, communication and automation (ICCCA), 350–354.\nIEEE.\nSiroker, D.; and Koomen, P. 2015. A/B testing: The most\npowerful way to turn clicks into customers. John Wiley &\nSons.\nStiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.;\nVoss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.\n2020.\nLearning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:\n3008–3021.\nSumers, T. R.; Ho, M. K.; Hawkins, R. D.; Narasimhan, K.;\nand Griffiths, T. L. 2021. Learning rewards from linguistic\nfeedback. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 35, 6002–6010.\nViharos, Z. J.; and Jakab, R. 2021. Reinforcement learning\nfor statistical process control in manufacturing. Measure-\nment, 182: 109616.\nVrzal, T.; Maleˇckov´a, M.; and Olˇsovsk´a, J. 2021.\nDeep-\nReI: Deep learning-based gas chromatographic retention in-\ndex predictor. Analytica Chimica Acta, 1147: 64–71.\nWang, F.; Wang, Y.; Li, D.; Gu, H.; Lu, T.; Zhang, P.; and\nGu, N. 2023. CL4CTR: A Contrastive Learning Framework\nfor CTR Prediction. In Proceedings of the Sixteenth ACM\nInternational Conference on Web Search and Data Mining,\n805–813.\nWu, J.; Ouyang, L.; Ziegler, D. M.; Stiennon, N.; Lowe,\nR.; Leike, J.; and Christiano, P. 2021.\nRecursively sum-\nmarizing books with human feedback.\narXiv preprint\narXiv:2109.10862.\nZanco, D. G. d. P.; Szczecinski, L.; Kuhn, E. V.; and Seara,\nR. 2022.\nA comprehensive analysis of the Elo rating al-\ngorithm: Stochastic model, convergence characteristics, de-\nsign guidelines, and experimental results.\narXiv preprint\narXiv:2212.12015.\nZhang, W.; Qin, J.; Guo, W.; Tang, R.; and He, X. 2021.\nDeep learning for click-through rate estimation.\narXiv\npreprint arXiv:2104.10584.\nZhou, G.; Zhu, X.; Song, C.; Fan, Y.; Zhu, H.; Ma, X.; Yan,\nY.; Jin, J.; Li, H.; and Gai, K. 2018. Deep interest network\nfor click-through rate prediction. In Proceedings of the 24th\nACM SIGKDD international conference on knowledge dis-\ncovery & data mining, 1059–1068.\nZiegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,\nA.; Amodei, D.; Christiano, P.; and Irving, G. 2019. Fine-\ntuning language models from human preferences.\narXiv\npreprint arXiv:1909.08593.\n",
  "categories": [
    "cs.LG",
    "math.ST",
    "stat.ME",
    "stat.TH"
  ],
  "published": "2023-11-24",
  "updated": "2023-11-24"
}