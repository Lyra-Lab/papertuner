{
  "id": "http://arxiv.org/abs/2311.14911v1",
  "title": "CUCL: Codebook for Unsupervised Continual Learning",
  "authors": [
    "Chen Cheng",
    "Jingkuan Song",
    "Xiaosu Zhu",
    "Junchen Zhu",
    "Lianli Gao",
    "Hengtao Shen"
  ],
  "abstract": "The focus of this study is on Unsupervised Continual Learning (UCL), as it\npresents an alternative to Supervised Continual Learning which needs\nhigh-quality manual labeled data. The experiments under the UCL paradigm\nindicate a phenomenon where the results on the first few tasks are suboptimal.\nThis phenomenon can render the model inappropriate for practical applications.\nTo address this issue, after analyzing the phenomenon and identifying the lack\nof diversity as a vital factor, we propose a method named Codebook for\nUnsupervised Continual Learning (CUCL) which promotes the model to learn\ndiscriminative features to complete the class boundary. Specifically, we first\nintroduce a Product Quantization to inject diversity into the representation\nand apply a cross quantized contrastive loss between the original\nrepresentation and the quantized one to capture discriminative information.\nThen, based on the quantizer, we propose an effective Codebook Rehearsal to\naddress catastrophic forgetting. This study involves conducting extensive\nexperiments on CIFAR100, TinyImageNet, and MiniImageNet benchmark datasets. Our\nmethod significantly boosts the performances of supervised and unsupervised\nmethods. For instance, on TinyImageNet, our method led to a relative\nimprovement of 12.76% and 7% when compared with Simsiam and BYOL, respectively.",
  "text": "CUCL: Codebook for Unsupervised Continual Learning\nCheng Chen\nCenter for Future Media, University\nof Electronic Science and Technology\nof China\nChengdu, China\ncczacks@gmail.com\nJingkuan Song∗\nCenter for Future Media, University\nof Electronic Science and Technology\nof China\nChengdu, China\njingkuan.song@gmail.com\nXiaosu Zhu\nCenter for Future Media, University\nof Electronic Science and Technology\nof China\nChengdu, China\nxiaosu.zhu@outlook.com\nJunchen Zhu\nCenter for Future Media, University\nof Electronic Science and Technology\nof China\nChengdu, China\njunchen.zhu@hotmail.com\nLianli Gao\nShenzhen Institute for Advanced\nStudy, University of Electronic\nScience and Technology of China\nShenzhen, China\nlianli.gao@uestc.edu.cn\nHengtao Shen\nShenzhen Institute for Advanced\nStudy, University of Electronic\nScience and Technology of China\nShenzhen, China\nshenhengtao@hotmail.com\nABSTRACT\nThe focus of this study is on Unsupervised Continual Learning\n(UCL), as it presents an alternative to Supervised Continual Learn-\ning which needs high-quality manual labeled data. The experiments\nunder UCL paradigm indicate a phenomenon where the results on\nthe first few tasks are suboptimal. This phenomenon can render\nthe model inappropriate for practical applications. To address this\nissue, after analyzing the phenomenon and identifying the lack\nof diversity as a vital factor, we propose a method named Code-\nbook for Unsupervised Continual Learning (CUCL) which\npromotes the model to learn discriminative features to complete the\nclass boundary. Specifically, we first introduce a Product Quan-\ntization to inject diversity into the representation and apply a\ncross quantized contrastive loss between the original represen-\ntation and the quantized one to capture discriminative informa-\ntion. Then, based on the quantizer, we propose a effective Code-\nbook Rehearsal to address catastrophic forgetting. This study\ninvolves conducting extensive experiments on CIFAR100, Tiny-\nImageNet, and MiniImageNet benchmark datasets. Our method\nsignificantly boosts the performances of supervised and unsu-\npervised methods. For instance, on TinyImageNet, our method led\nto a relative improvement of 12.76% and 7% when compared with\nSimsiam and BYOL, respectively. Codes are publicly available at\nhttps://github.com/zackschen/CUCL.\nCCS CONCEPTS\n• Computing methodologies →Image representations.\n∗Corresponding Author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10...$15.00\nhttps://doi.org/10.1145/3581783.3611713\nKEYWORDS\nContinual learning, Unsupervised learning, Lifelong learning, Point\nquantization, Contrastive learning.\nACM Reference Format:\nCheng Chen, Jingkuan Song, Xiaosu Zhu, Junchen Zhu, Lianli Gao, and Heng-\ntao Shen. 2023. CUCL: Codebook for Unsupervised Continual Learning. In\nProceedings of the 31st ACM International Conference on Multimedia (MM\n’23), October 29-November 3, 2023, Ottawa, ON, Canada. ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3581783.3611713\n1\nINTRODUCTION\nNowadays, Continual Learning (CL) [26, 29] is proposed to mimic\nthe human learning process which maintains old knowledge when\nacquiring new skills and knowledge. However, existing CL ap-\nproaches will forget previous knowledge when they learn new\ntasks, i.e., catastrophic forgetting. Therefore, several branches of\nalgorithms have been proposed to address this problem [1, 6, 7,\n22, 28, 30, 32, 34, 41]. However, all these methods just satisfy the\nsupervised learning paradigm where class labels for data points are\ngiven. And high-quality, class-labeled samples are scarce.\nSo, Unsupervised Continual Learning (UCL) which has ability\nto address this issue, receives more and more attention in the com-\nmunity. Under the UCL paradigm, CURL [27] is proposed firstly,\nbut, its capability is restricted to tackling complex assignments. An-\nother work [25] broadens the scope of supervised CL approaches in\nunsupervised paradigm and evaluates the performance of the state-\nof-the-art unsupervised learning methods, i.e., SimSiam [9] and\nBarlowTwins [39]. However, they only examine these approaches\nand do not propose a suitable approach for the UCL.\nIn this paper, we study the UCL paradigm follow [25]. We reveal\na phenomenon when training the SimSiam that the results of the\nfirst few tasks are suboptimal. And these results show an upward\ntrend as training continues on the following tasks, as shown in\nFig.1. However, this occurrence contradicts the practicality of con-\ntinual learning, which requires the model to address catastrophic\nforgetting and maintain optimal performance on the current task\nin hand at the same time. For example, it is not acceptable to tol-\nerate the poor performance of the AI-Model in classification and\narXiv:2311.14911v1  [cs.CV]  25 Nov 2023\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nCheng Chen et al.\nanticipate an enhancement in its classification capability after train-\ning it on a Vision & Language task [12, 21, 24, 40]. Furthermore, to\ncheck whether this phenomenon is a common issue in UCL, we con-\nduct experiments with a series of unsupervised learning methods\n[5, 8, 9, 13, 39]. Upon fine-tuning some of these methods, the same\nphenomenon shows up. Meanwhile, traditional metrics, namely Av-\nerage Accuracy (AA) and Backward Transfer (BWT) can no longer\nprovide an accurate assessment of the model’s learning ability for\neach task under UCL paradigm. Therefore, this paper chooses novel\nmetric which proposed by [28], and we term it as Mean Average\nAccuracy (MAA), to precisely assess task performance during the\ntraining process.\nSince the phenomenon can be measured accurately, a solution\napproach is necessary. Three assumptions were made after analyz-\ning the results. 1.) The lack of diversity of classes may be the cause.\nThe small size of classes in each task makes it difficult to obtain\na discriminative representation. Consequently, after conducting\nexperiments by increasing the number of classes in each task, al-\nthough performances increase, the suboptimal problem remaines,\nindicating that this is not the primary factor. 2.) The slow conver-\ngence of unsupervised methods is deemed to be the cause. As a\nresult, methods were trained for an extended period. This led to a\nmarginal improvement of results, but the suboptimal phenomenon\nremained, ruling out training time as the key cause. (refer to Sec.\n4.2.3 for details) 3.) The lacking diverse information of features\nin unsupervised methods is believed to be the primary factor. In\naddition, we observe that the methods which can only meet the\ntransformed inputs of the same input are more severe about the\nsuboptimal problem. While they can recognize the inherent features\nof a class, the inability to interact with other classes impedes dis-\ncriminative classification. So we want to confer diverse information\n(about other classes) for representations to guide the model to learn\ninvariable information about the class contained in each task. Vec-\ntor Quantization (VQ) is used to achieve this objective. Clustering\nfeatures extracted by backbone produces centroids, or codewords,\nwithin a codebook. These codewords contain diverse information\nabout various classes. The final representation created by these\ncodewords absorbs sufficient diversity to create a discriminative\nclass boundary. Experiments were conducted by this way to inject\ndiverse information into representations for unsupervised learn-\ning techniques. The findings suggest that it enhances convergence,\nresulting in a 15% improvement (refer to Sec. 4.2.3 for details).\nFurthermore, we contend that distinct segments of a represen-\ntation contain distinct local patterns. Focusing diversity on these\nsegments rather than the entire representation will achieve more\nfine-grained diverse. Meanwhile, the fundamental principle of Prod-\nuct Quantization (PQ) is to decompose a feature vectors in high-\ndimensional space into multiple sub-vectors. The ultimate represen-\ntation is a rearrangement of codewords that contains fine-grained\ninformation. This process is highly consistent with our idea. Con-\nsequently, in this paper, we propose a Codebook for Unsupervised\nContinual Learning (CUCL), which employs PQ to quantize rep-\nresentations learned from unsupervised learning methods. Then,\nwe apply contrastive learning to maximize the cross-similarity\nbetween the original representations and the quantized ones of\nanother branch, while minimizing similarity with other samples to\nlearn invariable knowledge for promoting classification boundary.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTrain after Task\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\nAccuracy (%)\nSimsiam w/ CUCL\nSimsiam\nFigure 1: The performance of Simsiam [9] and Simsiam with\nCUCL for the first task of Split CIFAR100 [20]. Each bar 𝑖rep-\nresents the result of the first task after training on task 𝑖. We\nobserve two main findings. Firstly, the result is suboptimal\nupon completing the training for the first task. Nonetheless,\nthis result improves as the training progresses. Secondly,\nwe note a considerable improvement in performance when\nCUCL is integrated with Simsiam.\nAlthough, the proposed CUCL can resolve the suboptimal prob-\nlem. The primary issue in CL, catastrophic forgetting, persists. For-\ntunately, the CUCL enables the codewords to learn the features\nof each task, and these codewords can serve as proxies for select-\ning representative samples. Consequently, an algorithm based on\nthe codewords is proposed to mitigate catastrophic forgetting by\nselecting the closest samples with codewords for rehearsal.\nIn summary, the proposed CUCL consists of two parts. One part\nutilizes PQ to quantize representations and perform contrastive\nlearning, resolving the suboptimal problem. The other part selects\nthe closest samples for rehearsal as a means of mitigating cata-\nstrophic forgetting. We conducte extensive experiments to eval-\nuate the efficacy of our proposed method on multiple continual\nlearning benchmarks, including CIFAR100, TinyImageNet, and\nMiniImageNet, by testing various supervised CL methods and\ntheir variants on UCL. The results prove the ability of our method\nin enhancing these approaches. For, instance, we have observed av-\nerage improvements of 9.08%, 7.64% and 6.49% on the three datasets\nover SI, DER, LUMP which are impletemeneted on Simsiam, in\nterm of MAA. Additionally, we evaluate the effectiveness of our\nmethod by conducting more experiments on state-of-the-art unsu-\npervised learning methods. The integration of our CUCL results in\nsubstantial enhancements.\nTo summarize, our main contributions are three-fold:\n• A study under Unsupervised Continual Learning (UCL) par-\nadigm has been conducted, which uncovers a phenomenon\nthat the efficacy on the first few tasks of certain unsuper-\nvised learning methods is limited by the diverse information\ncontained in features.\nCUCL: Codebook for Unsupervised Continual Learning\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\n• We have introduced a Codebook for Unsupervised Contin-\nual Learning (CUCL), which is a plug-and-play approach\ntowards enabling networks to acquire discriminative infor-\nmation through quantized representation. Furthermore, we\nestablish a rehearsal algorithm based on the codebook to\nmitigate the catastrophic forgetting issue.\n• To demonstrate the effectiveness of our method, an exten-\nsive experimental evaluation of several benchmark datasets\nhas been conducted. The marginal improvement confirms\nour method’s ability to solve the suboptimal problem and\nalleviate catastrophic forgetting.\n2\nRELATED WORK\n2.1\nContinual Learning\nWith the growing interest in continual learning, many methods\nhave been proposed to address the catastrophic forgetting problem.\nThey can be grouped into three broad categories: regularization\napproaches [1, 19, 22, 41], parameter isolation methods [30, 31, 34],\nand memory-based approaches [2, 6, 23, 28, 32]. The Regulariza-\ntion approaches focus on curing a continual learning network of\nits catastrophic forgetting by introducing an extra regularization\nterm in the loss function. LwF [22] mitigated forgetting by using\nknowledge distillation and transferring knowledge, and used previ-\nous model output as soft labels for the previous task. Besides, EWC\n[19] was the first method to penalize the changes to important pa-\nrameters during training of later tasks. SI [41] efficiently estimated\nimportance weights during training. MAS [1] computed the impor-\ntance of the parameters of a neural network in an unsupervised\nand online manner. The basic idea of Parameter isolation methods\nis to directly add or modify the model structure. HAT [31] applied\nthe mask on previous task parts during new task training, and this\nprocess is imposed at the unit level. PNN [30] added a network to\neach task and lateral connections to the network of the previous\ntask while freezing previous task parameters. MNTDP [34] pro-\nposed a modular layer network approach, whose modules represent\natomic skills that can be composed to perform a certain task and\nprovides a learning algorithm to search the modules to combine\nwith. These strategies may work well, but they are computationally\nexpensive and memory intensive. For Memory-based approaches,\ncatastrophic forgetting is avoided by storing data from previous\ntasks and training them together with data from the current task.\nSome methods [6, 23, 28, 32] used replayed samples from previ-\nous tasks to constrain the parameters’ update when learning the\nnew task. For example, iCaRL [28] selected a subset of exemplars\nthat are the best approximate class means in the learned feature\nspace. During training on a new task of EEC [2], reconstructed\nimages from encoded episodes were replayed to avoid catastrophic\nforgetting. Although these methods have achieved remarkable per-\nformance, they are just designed for supervised continual learning.\nOur approach is designed for the unsupervised setting which is\nmore realistic.\n2.2\nUnsupervised Representational Learning\nRecently there has been steady progress in unsupervised represen-\ntational learning. Many methodologies [5, 8, 9, 13–15, 17, 33, 36, 37,\n39] have been proposed for un-/self-supervised learning. SimCLR\n[8] used the other samples coexisting in the current batch as nega-\ntive samples, so it worked well when equipped with a large batch\nsize. MoCo [15] applied a queue of negative samples as a dynamic\nlook-up dictionary and a momentum encoder which is proposed\nto maintain consistency. BYOL [13] was a Siamese [3] in which\none branch is a momentum encoder. It directly predicted the net-\nwork representation of one view from another view. BarlowTwins\n[39] avoided collapse by measuring the cross-correlation matrix\nbetween the outputs of two identical networks, and making it as\nclose to the identity matrix as possible. SwAV [5] incorporateed\nonline clustering into Siamese networks. In another recent line of\nwork, the network architecture and parameter updates of SimSiam\n[9] are modified to be asymmetric such that the parameters are\nonly updated using one branch.\n3\nMETHODOLOGY\nIn this section, we describe the settings of UCL and our methodology.\nSpecifically, we present the problem formulation and the metrics\nused in traditional continual learning in Section 3.1. Section ??\nexplains the new metric to measure the performance along the\ntraining process. Then, Section 3.2 and Section 3.3 provide the\ndetails of the proposed method and explain how to select rehearsal\nsamples.\n3.1\nPreliminary and Metrics\nIn the setup of unsupervised continual learning, a series of 𝑇tasks\nare learned sequentially. We denote a task by its task descriptor,\n𝜏∈{1, 2, ...,𝑇} and its corresponding dataset D𝜏= {(𝐼𝜏,𝑖)𝑁𝜏\n𝑖=1} which\nhas 𝑁𝜏samples from an i.i.d. distribution. The 𝐼𝜏,𝑖is the input with-\nout supervised label. Furthermore, the task boundaries are available\nduring both training and testing stage, i.e., the task-incremental set-\nting. The UCL seeks to train a model 𝐹= {𝜃𝐹} to learn knowledge\non a sequence of tasks without forgetting the knowledge learned\non previous tasks, where 𝜃𝐹represents the weights of the neural\nnetwork.\nThe widely used evaluation metrics in continual learning are:\nAverage Accuracy (ACC), and Backward Transfer (BWT). Formally,\nACC and BWT are defined as:\n𝐴𝐶𝐶= 1\n𝑇\n𝑇\n∑︁\n𝑖=1\n𝐴𝑇,𝑖,\n(1)\n𝐵𝑊𝑇=\n1\n𝑇−1\n𝑇−1\n∑︁\n𝑖=1\n𝐴𝑇,𝑖−𝐴𝑖,𝑖,\n(2)\nwhere 𝐴𝑇,𝑖is the performance on 𝜏= 𝑖task after training on the\n𝜏= 𝑇task.\nIn addition, we use Mean Average Accuracy (MAA) [28]. For-\nmally, MAA is defined as:\n𝑀𝐴𝐴= 1\n𝑇\n𝑇\n∑︁\n𝑗=1\n( 1\n𝑗\n𝑗∑︁\n𝑖=1\n𝐴𝑗,𝑖).\n(3)\nThe MAA is calculated as the mean of the average accuracy for\ntasks at each training point that the model has encountered. A high\nMAA corresponds to a continual learning model that consistently\nmaintains a high accuracy throughout the training process.\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nCheng Chen et al.\nz\nx\nCodeBook 3\nCodeBook 3\nCodeBook 2\nCodeBook 2\nSplit\nCodeBook 1\nx\nz\nx\nz\nWeight contribution\nReorganize\nZ\nZ\nFeature Space\nPush\nBatch\nDimension\n11\nI\n12\nI\n21\nI\n22\nI\nCodeword\nSamples\nT\nT\nT\nFigure 2: An overview of the proposed CUCL. The samples are augmented using data augmentations T and then passed on\nto the Backbone to obtain the original feature representation X ∈R𝐷. The representations are subjected to the traditional\nunsupervised learning loss L𝑢𝑛𝑠𝑢𝑝. Subsequently, the original representations are divided into subvectors x ∈R𝐷/𝑀. The soft\nquantizer is applied to the subvectors x to obtain quantized subvectors z. Then, the quantized subvectors z are reorganized\ninto a representation Z, which leads to enhanced diversity and robustness. The cross contrastive loss L𝑐𝑢𝑐𝑙is imposed on both\nthe original representations X and the quantized Z in the subsequent stage. Finally, the model is optimized by the final loss:\nL = L𝑢𝑛𝑠𝑢𝑝+ L𝑐𝑢𝑐𝑙.\n3.2\nCodebook for Unsupervised Continual\nLearning\nIn this section, we propose the Codebook for Unsupervised Contin-\nual Learning (CUCL) to address the suboptimal phenomenon preva-\nlent in unsupervised continual learning. Fig. 2 presents an overview\nof the proposed CUCL pipeline. Prior to detailing the technical as-\npects, we offer a high-level understanding of the workings of CUCL.\nOur approach aims to facilitate model learning of discriminative\nknowledge by fostering diversity into representations. Furthermore,\nwe contend that different segments of representation harbor distinc-\ntive local patterns. To this end, we first quantize the features into\ndifferent segments based on their local patterns. Features assigned\nto the same codeword contain the same local information, whereas\nfeatures with different information have different codewords. By\nreconstructing the quantized segments, we accumulate more in-\nformation and enable more fine-grained analysis. In summary, we\ntransform the original feature into a combination of different local\npattern features to enhance model learning of more discriminative\nfeature.\nFirst of all, we briefly introduce the unsupervised learning meth-\nods working on Siamese networks. These methods operate on a pair\nof embeddings extracted from transformed images. More specif-\nically, they produce two distorted views via data augmentations\nT for all images of a batch sampled from D𝜏. The distorted views\nare then fed to the model 𝐹, producing two batches of embeddings\nX𝑎and X𝑏respectively, each X ∈R𝐷. The different unsupervised\nlearning loss will be applied to X𝑎and X𝑏to produce the optimiza-\ntion objective, L𝑢𝑛𝑠𝑢𝑝.\nAfter we get the features X extracted by the 𝐹, following the\nmethod in [18, 38], we quantize them using the soft quantization\nmodel 𝑄(X;𝜃𝑄). This quantization method can solve the infea-\nsible derivative calculation of hard assignment quantization and\nbe trained in an end-to-end manner. The quantization model 𝑄\ncontains 𝑀codebooks {B1, ...B𝑀}, where each codebook B𝑖has\n𝐾codeword c ∈R𝐷/𝑀as B𝑖= {c𝑖1, ..., c𝑖𝐾}. We split the fea-\ntures X into 𝑀subvectors [x1, ..., x𝑀] in the feature space where\nx𝑖∈R𝐷/𝑀is a subvector. And then, we use the soft quantization\nmodel to quantize these subvectors. First of all, we compute the\ndistance between the subvector x𝑖and the codeword:\n𝑑𝑖𝑠(x𝑖, c𝑖𝑘) = ∥x𝑖−c𝑖𝑘∥2\n2 ,\n(4)\nwhere ∥·∥2\n2 represents the squared Euclidean distance. The quanti-\nzation process for each feature subvector is defined below:\nz𝑖=\n𝐾\n∑︁\n𝑘\n𝑒𝑥𝑝(−𝑑𝑖𝑠(x𝑖, c𝑖𝑘)/𝜏𝑞)\nÍ𝐾\n𝑘′ 𝑒𝑥𝑝(−𝑑𝑖𝑠(x𝑖, c𝑖𝑘′)/𝜏𝑞)\nc𝑖𝑘,\n(5)\nwhere 𝜏𝑞is a temperature parameter to scale the input. The con-\ntribution of each codeword is associated with the distance from\nthe codeword to the subvector, where the closest codeword has the\nbiggest contribution and vice versa. By applying this quantization\nto X𝑎and X𝑏, we can get the quantized features Z𝑎and Z𝑏by\ncombining the quantized subvectors z. According to this soft quan-\ntization, the information contained in each codeword is integrated\ninto the final quantization representation, producing a more diverse\nrepresentation to learn discriminative feature.\nThen we apply a cross contrastive learning inspired by traditional\ncontrastive learning to compare the X and Z of different views. Same\nas the contrastive learning, we treat the X and Z as positive if they\nare generated from the same image, whereas negative if originated\nfrom the different ones. So we can apply contrastive learning loss\nbetween the X and Z according to the positive pair of examples\n(𝑖, 𝑗):\nL𝑐𝑢𝑐𝑙= −𝑙𝑜𝑔\n𝑒𝑥𝑝(𝐶𝑜𝑠(X𝑖, Z𝑗)/𝜏𝑙)\nÍ𝑁𝐵\n𝑛=1 1[𝑛≠𝑗]𝑒𝑥𝑝(𝐶𝑜𝑠(X𝑖, Z𝑛)/𝜏𝑙)\n,\n(6)\nCUCL: Codebook for Unsupervised Continual Learning\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nTable 1: The quantitative results of supervised baselines and their unsupervised variants on Resnet-18 architecture with KNN\nclassifier [36]. While w/ CUCL is the adaptation of our method to other methodologies. There are considerable improvements\nwhen our CUCL plugged into each baseline.\nSetting\nMethod\nSplit CIFAR100\nSplit TinyImageNet\nSplit MiniImageNet\nMAA\nAA\nBWT\nMAA\nAA\nBWT\nMAA\nAA\nBWT\nSupervised\nSI [41]\n59.17\n55.77\n-32.00\n52.12\n44.42\n-35.27\n50.12\n44.97\n-34.52\nA-GEM [6]\n58.10\n54.77\n-31.24\n52.00\n48.74\n-29.67\n50.84\n48.09\n-30.32\nDER [4]\n72.48\n67.97\n-20.09\n60.83\n56.74\n-21.40\n58.52\n54.49\n-23.40\nSimsiam\nSI\n69.75\n74.32\n-2.66\n51.37\n58.80\n-0.98\n54.79\n63.24\n0.30\nw/ CUCL\n75.79+6.04\n76.68+2.36\n-5.12−2.46\n62.45+11.08\n64.48+5.68\n-4.49−3.51\n64.91+10.12\n67.05+3,81\n-4.44−4.74\nDER\n73.05\n75.31\n-4.04\n51.01\n58.66\n-0.82\n56.78\n63.72\n-0.61\nw/ CUCL\n76.32+3.27\n77.24+1.93\n-4.39−0.35\n62.41+11.4\n65.10+6.44\n-4.04−3.22\n65.03+8.25\n67.15+3.43\n-4.42−3.81\nLUMP [25]\n63.95\n69.83\n3.60\n53.60\n61.12\n5.93\n52.36\n59.54\n4.52\nw/ CUCL\n72.19+8.24\n74.78+4.95\n-0.82−4.42\n58.51+4.91\n59.42−1.7\n-2.80−8.73\n58.67+6.31\n60.25+0.71\n-2.36−6.98\nBarlow twins\nSI\n73.13\n73.52\n-8.16\n60.39\n59.72\n-8.73\n61.53\n61.97\n-7.91\nw/ CUCL\n74.40+1.27\n74.93+1.41\n-7.68+0.48\n61.59+1.2\n62.44+2.72\n-6.11+2.62\n62.67+1.14\n64.50+2.53\n-6.61+1.3\nDER\n73.15\n73.46\n-7.63\n60.40\n60.04\n-8.82\n61.79\n62.47\n-7.93\nw/ CUCL\n74.62+1.47\n75.47+2.01\n-7.16+0.47\n61.57+1.17\n60.88+0.84\n-8.82+0.00\n63.10+1.31\n63.82+1.35\n-7.64+0.29\nLUMP\n67.18\n66.73\n-6.33\n55.26\n57.78\n-2.62\n54.91\n57.66\n-2.63\nw/ CUCL\n72.16+4.98\n73.66+6.93\n-3.88+2.45\n59.43+4.17\n61.58+3.8\n-3.04−0.42\n58.60+3.69\n61.80+4.14\n-1.97+0.66\nwhere 𝐶𝑜𝑠represents cosine function, 𝜏𝑙is a non-negative temper-\nature parameter, and the 1[𝑛′≠𝑗] ∈0, 1 denotes the indicator which\nequates to 1 iff 𝑛≠𝑗.\nThe overall loss combines the traditional unsupervised loss and\nour CUCL loss, as\nL = L𝑢𝑛𝑠𝑢𝑝+ L𝑐𝑢𝑐𝑙.\n(7)\n3.3\nCodebook Rehearsal\nWith the cross quantized contrastive learning, the model works\nwell on the tasks at hand. The problem that the first few tasks are\nsuboptimal is ameliorated. However, the catastrophic forgetting\nremains. Since the codewords represent the clustered centroid of\nthe subvectors, they can be seen as the proxies of the subvectors\nand the carrier of the information. Specifically, the codeword c𝑖𝑐\nwhich is close to subvector x𝑖serves as the proxy of this subvector.\nWe use these codewords to choose some representative samples\nfor rehearsal. Simply put, since the weight contribution is related\nto the distance in Eq. 4, we use this distance as a clue to choose\nsamples. We estimate the distance between each subvector x𝑖of X\nand its proxy c𝑖𝑐and sum them up to the final distance:\n𝑑𝑖𝑠(𝑋) =\n𝑀\n∑︁\n𝑖\n∥x𝑖−c𝑖𝑐∥2\n2 .\n(8)\nIn this paper, the first 𝑆samples which have the furthest distance\nwill be selected. Because we regard these samples as the most\ndifficult data points. We can not only recall the knowledge about\nthe task learned before through these buffers, but also retrain the\nsamples to learn more information about the task.\n4\nEXPERIMENT\nIn this section, to evaluate the effectiveness of our proposed method\nCUCL, we validate it on a variety of continual learning benchmarks.\nAdditionally, we perform ablation studies to explore the usefulness\nof the different components.\n4.1\nExperimental setting\nDatasets. We conduct experiments on various continual learn-\ning benchmarks, including Split CIFAR100 [20], Split TinyIma-\ngeNet, Split MiniImageNet. The Split CIFAR100 is constructed\nby randomly splitting 100 classes of CIFAR100 into 10 tasks, where\neach class includes 500 training samples and 100 testing samples.\nThe Split TinyImageNet is a variant of the ImageNet dataset [11]\ncomprising 10 randomized classes out of the 100 classes for each\ntask, where the images are sized 64 x 64 pixels. Finally, Split Mini-\nImageNet [35] is a dataset created by dividing the 100 classes of\nthe ImageNet into 10 sequential tasks, each consisting of 10 classes.\nEach class includes 500 training samples and 100 testing samples.\nEach image in the mentioned datasets has a size of 84 × 84 pixels.\nBaselines and Training setup. Firstly, we compare our method\nwith state-of-the-art continual learning methods, includes SI [41],\nA-GEM [6], DER [4], LUMP [25] and their unsupervised variants.\nSecondly, we compare our method with state-of-the-art unsuper-\nvised learning methods, including Simsiam [9], Barlow twins\n[39], BYOL [13], SimCLR [8] and SwAV [5]. All these methods\nemploy a common underlying architecture, a Siamese [3] network\nstructure. Our approach can be integrated into these networks since\nwe also utilize the same network structure.\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nCheng Chen et al.\nTable 2: The quantitative result of each unsupervised methods on Resnet-18 architecture with KNN classifier. While w/ CUCL\nis the adaptation of our method to other unsupervised learning methodologies. The best results are highlighted in bold.\nMethod\nSplit CIFAR100\nSplit TinyImageNet\nSplit MiniImageNet\nMAA\nAA\nBWT\nMAA\nAA\nBWT\nMAA\nAA\nBWT\nSimsiam [9]\n67.48\n73.87\n-1.20\n50.47\n57.18\n-2.62\n57.33\n64.36\n-0.92\nw/ CUCL\n76.07+8.59\n75.81+1.94\n-6.26−5.06\n63.23+12.76\n64.42+7.24\n-5.18−2.56\n64.61+7.28\n66.17+1.81\n-5.30−4.38\nBYOL [13]\n75.45\n77.80\n-1.40\n56.90\n63.98\n1.16\n57.75\n66.56\n3.96\nw/ CUCL\n76.78+1.33\n77.67−0.13\n-4.19−2.79\n63.90+7\n65.66+1.68\n-4.58−5.74\n64.50+7.28\n66.99+0.43\n-3.74−7.7\nBarlow-twins [39]\n72.91\n72.96\n-8.47\n59.90\n59.76\n-8.22\n61.26\n63.32\n-6.92\nw/ CUCL\n75.29+2.38\n75.18+2.22\n-8.82−0.35\n63.12+3.22\n63.58+3.82\n-7.27+0.95\n64.31+3.05\n66.16+2.84\n-6.56+0.36\nSimCLR [8]\n74.39\n74.14\n-6.58\n63.49\n63.24\n-6.56\n63.15\n65.30\n-4.71\nw/ CUCL\n75.38+0.99\n75.88+1.74\n-5.23+1.35\n63.81+0.42\n64.14+0.9\n-5.36+1.2\n63.72+0.57\n64.77−0.53\n-5.09−0.38\nSwAV [5]\n73.18\n74.44\n-4.20\n58.52\n62.94\n-0.76\n62.03\n62.66\n-4.67\nw/ CUCL\n74.77+1.59\n74.86+0.42\n-6.39−2.19\n61.95+3.43\n62.84−0.1\n-5.40−4.64\n63.22+1.19\n64.29+1.63\n-5.63−0.96\nWe implement CUCL and reproduce the results of all these meth-\nods from the released code of [25]. We use the same hyperparam-\neters as [10, 25]. The K-Nearest Neighbors (KNN) [36] classifier\nis used to evaluate the representations obtained from the unsu-\npervised learning. Resnet18 [16] is employed as the backbone and\ntrained for 200 epochs in each task with a batch size of 256 in the\nUCL pardigm. In the supervised paradigm, we follow the setting\nintroduced by [25] and train the methods for 50 epochs with a\nbatch size of 32. We set the temperature parameters for quantiza-\ntion and loss to 5 and 0.5, respectively. Concerning the quantizer\nsetting in CUCL, we employe 24-bit quantization method, which\nincorporates 8 codebooks of 8 codewords each with a dimension\nof 16. We maintain a uniform learning rate of 0.03 in all the ex-\nperiments. Concerning the CUCL rehearsal algorithm, we opt to\nretain 20 samples per task. In the main experiments, if there are no\nspecial instructions, the CUCL is the combination of CUCL and the\nCodebook Rehearsal.\nEvaluation metrics. We evaluate the performance on the follow-\ning metrics: Average Accuracy (ACC), Backward Transfer (BWT)\nand Mean Average Accuracy (MAA).\n4.2\nExperimental Results\n4.2.1\nComparison with State of the Arts. To evaluate the ability\nof our method to solve the suboptimal problem and mitigate cata-\nstrophic forgetting, we compare our method with state-of-the-art\nsupervised methods and its variants under UCL paradigm. Specifi-\ncally, we verify our CUCL by incorporating it into these variants.\nFollowing recent work [25], Simsiam and Barlow twins are chosen\nas unsupervised backbones. Quantitative results on three datasets\nare shown in Tab. 1.\nFrom Tab. 1, we have following observations: Firstly, the un-\nsupervised variants achieve significantly better performance than\nsupervised methods, as illustrated in [25]. Secondly, these variants\nwhich are based on Simsiam perform well on three datasets in terms\nof BWT and AA. Specifically, about BWT, LUMP achieves positive\nresults as 3.60%, 5.93% and 4.52%. However, after reviewing the\nresults of each individual task, we observe that the improvements\nof the first few tasks compensate for the forgetting effect, i.e., sub-\noptimal problem. Consequently, the AA and BWT metrics cannot\naccurately evaluate the learning ability of the model. Thirdly, in\nterm of MAA, after integrated with CUCL, we have observed aver-\nage improvements of 9.08%, 7.64% and 6.49% on the three datasets\nover SI, DER, LUMP, respectively. These substantial improvements\ndemonstrate the efficiency of our approach in addressing the sub-\noptimal problem. Fourthly, the Barlow twins approach does not\nsuffer the suboptimal problem and performs better than Simsiam.\nNevertheless, when our CUCL is incorporated with it, there are\nstill 1.2%, 1.32% and 4.28% improvements over SI, DER, LUMP, re-\nspectively. This occurrence indicates that our method is not only\ncapable of addressing the suboptimal problem but also able to learn\nsuperior representations that enhance the class boundary. More-\nover, in terms of BWT, the improvements verify the significant\nefficacy of our approach for mitigating catastrophic forgetting.\n4.2.2\nComparison with Unsupervised Learning Methods. To verify\nour method is plug-and-play, we incorporate our CUCL into differ-\nent unsupervised learning methods. Quantitative results on three\ndatasets are shown in Tab. 2. From Tab. 2, it’s worth noting that,\nin terms of BWT, the Simsiam and BYOL exhibit remarkable re-\nsults. The reason lies in the fact that they also suffer the suboptimal\nproblem (the detail of the learning curve will show in Sec.4.2.4).\nThis issue is addressed by applying our CUCL model, which leads\nto a significant improvement for the Simsiam and BYOL methods\n(average 9.54% and 5.2% in terms of MAA, respectively). Specifically,\non the CIFAR100 dataset, we observe considerate improvements\non Simsiam in terms of MAA (67.48% vs. 76.07%), showing notable\ngeneralizability for CUCL. In addition, other unsupervised methods\nemploy various techniques to handle negative samples within a\nbatch to solve the suboptimal problem. Nevertheless, when our\napproach is incorporated, significant improvements can still be\nCUCL: Codebook for Unsupervised Continual Learning\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nT1\nT3\nT5\nT7\nT9\nTask\n50\n55\n60\n65\n70\nMAA (%)\nSimsiam\nSimsiam w/ Diversity\nSimsiam w/ Quantization\nFigure 3: The learning curves of MAA. These experiments\nare conducted on the Split CIFAR100 based on the Simsiam\nmethodology. These results demonstrate the necessity of the\ndiversity.\nachieved (e.g., 3.43% of SwAV on TinyImageNet), manifesting the\ngreat ability of our CUCL to mitigate the catastrophic forgetting.\nFurthermore, as discovered from Tab. 2, with the task difficulty\nincreasing when training on TinyImageNet and MiniImageNet,\nresults of each methodology suffer degradation. It is surprising\nthat, in terms of BWT, BYOL even remains positive, indicating that\nthe suboptimal problem is more severe than on CIFAR100. Addi-\ntionally, some unsupervised methods (e.g., SwAV), which initially\nperform well on CIFAR100, also suffer from the suboptimal problem.\nWe speculate it is because discriminative features and segmented\nquantization are more crucial than simple tasks. Thus, when our\napproach is incorporated into these methods, the improvements\nare more substantial than those achieved in CIFAR100.\n4.2.3\nAblation Studies.\nAnalysis of Diversity for Representation. To confirm the signifi-\ncance of diversity, we conduct experiments on CIFAR100 dataset\nby chooing Simsiam as unsupervised backbone. We keep other\noperations unchanged and only apply quantization to its final rep-\nresentations, which are used for unsupervised loss. Further, as a\ncomparison base, we record the results of fine-tune.\nInitially, we employ a quantizer that only utilize a single code-\nbook to capture the entirety of representations. This quantizer solely\nclusters representation to create final representation and does not\nutilize local patterns in different segments. The results are shown\nin Fig. 3, named Simsiam w/ Diversity. The figure clearly depicts\nthat the inclusion of diversity substantially mitigate the suboptimal\nissue, as indicated by a significant improvement in performance\nfrom 50% to 65.4% on the first task. However, along the training\nprocess, the inadequacy of using entire representation becomes\napparent. We speculate it is because the introduction of course\ndiversity impacts the learning process on subsequent tasks as the\nnumber of observed classes increases.\nThis problem can be resolved by utilizing segmented quantized\nrepresentation, which is validated by Fig. 3, showing an increase\nin accuracy of 22.4% for the first task (i.e., Simsiam w/ Quantiza-\ntion). Furthermore, upon complete training, the result culminates\nTable 3: Ablation study about the different codebook settings\non the Simsiam w/ CUCL without rehearsal. ‘Cb’ and ‘Cw’\nrepresent the number of codebook and codeword, respec-\ntively.\nCb\nCw\nSplit CIFAR100\nSplit TinyImageNet\nMAA\nAA\nBWT\nMAA\nAA\nBWT\n1\n65536\n71.64\n71.27\n-10.81\n61.92\n64.34\n-4.80\n2\n256\n73.72\n72.95\n-10.31\n62.82\n64.34\n-5.40\n4\n16\n73.80\n73.15\n-10.20\n62.84\n63.38\n-6.87\n8\n4\n75.60\n76.23\n-6.14\n61.33\n62.22\n-7.22\nTable 4: This ablation study is to validate the effectiveness\nof our rehearsal algorithm. The experiments are conducted\nabout different memory size, 0, 20, and 40. These results\nshow the effectiveness of our rehearsal algorithm to boost\nthe performances.\nSize\nMethod\nSplit CIFAR100\nSplit TinyImageNet\nMAA\nAA\nBWT\nMAA\nAA\nBWT\n0\nSimsiam\n75.83\n75.36\n-7.62\n62.10\n62.98\n-6.56\nBarlow-twins\n74.81\n75.03\n-8.19\n61.70\n61.04\n-9.13\n20\nSimsiam\n76.07\n75.81\n-6.26\n63.23\n64.42\n-5.18\nBarlow-twins\n75.29\n75.18\n-8.82\n63.12\n63.58\n-7.27\n40\nSimsiam\n76.37\n76.20\n-5.97\n62.70\n64.42\n-4.56\nBarlow-twins\n76.20\n76.77\n-6.11\n63.82\n64.22\n-6.36\nto 71.46%. These outcomes substantiate our proposal that instilling\ndiversity in distinct representation segments leads to the accumula-\ntion of more information.\nAnalysis of Different Codebook setting. To further validate the\neffectiveness of local patterns in CUCL, we conduct experiments\nusing different quantizers. We exclude the interference of rehearsal\nby conducting the experiments under no rehearsal samples. The\nexperimental settings involve fixing the dimension of the represen-\ntation to 16. To maintain the information contained in the quantizer\nequally, the number of codewords varies according to the number of\ncodebooks. Additionally, the memory usage is positively correlated\nwith the number of codewords, which is exponentially related to\nthe bits. To keep the cost manageable, we apply the 16-bit quanti-\nzation in this ablation study. The experimental results, shown in\nTab. 3, indicate that using only one codebook results in low per-\nformance due to the absence of local patterns. Moreover, as the\nnumber of codebooks increasing, the different codewords contribute\ndistinct information to the quantized representation, resulting in\nmore diverse and informative representations. These representa-\ntions contribute to better final performances. However, the results\nof 8 codebooks on TinyImageNet degraded significantly. We posit\nthat the reason is the lack of enough codewords for this harder\ntask, and the reorganization of the quantized subvectors mislead\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nCheng Chen et al.\nTable 5: Experiment results on CIFAR100 dataset with differ-\nent training time. The best results are highlighted in bold.\nEpoch\nCUCL\nSimsiam\nBarlow-twins\nMAA\nAA\nBWT\nMAA\nAA\nBWT\n200\nw/o\n67.48\n73.87\n-1.20\n72.91\n72.96\n-8.47\nw/\n76.07\n75.81\n-6.26\n75.29\n75.18\n-8.82\n300\nw/o\n71.75\n74.95\n-4.63\n74.58\n75.07\n-7.66\nw/\n77.84\n77.15\n-6.87\n76.40\n74.91\n-10.27\n400\nw/o\n74.77\n75.05\n-8.07\n75.40\n75.49\n-7.57\nw/\n78.34\n75.45\n-9.26\n76.45\n75.02\n-10.41\nthe learning of the model. Comparing the results with those in Tab.\n4 (61.33% vs. 62.10%), increasing the number of codewords to 8 can\ncapture the local patterns well and correct the learning process.\nAnalysis of Different Memory Size. In this ablation study, to ex-\nplore the effectiveness of our rehearsal algorithm, we conduct ex-\nperiments with different settings of 𝑆, including 0, 20, and 40. The\nquantizer in CUCL consist of 8 codebooks, each of which has 8\ncodewords. As shown in Tab. 4 on the CIFAR100 dataset, the re-\nhearsal samples not only alleviate catastrophic forgetting but are\nalso beneficial for unsupervised learning. Because, unsupervised\nlearning approaches learn discriminative features rather than class-\nspecific information which is learned by supervised methods. The\nmore different class samples, the more information about classi-\nfication boundaries is learned. Additionally, it is noteworthy that\nwhen Barlow-twins saves 20 samples for rehearsal on the CIFAR100\ndataset, the BWT behaves worse as MAA and AA increase. By\nchecking the results of every task after training on each task, we\nfind that the rehearsal samples promote plasticity. However, the\nstability do not increase comparably to plasticity, resulting in a\nworse BWT result. Meanwhile, the balance between plasticity and\nstability of Barlow-twins worked well on the TinyImageNet dataset\nas the size increased. Furthermore, on the TinyImageNet dataset,\nwe can observe that the results of Simsiam with 20 samples are\nworse than those with 40 samples in terms of AA and BWT. How-\never, comparing the specific results, we find that the first is better\nthan the latter in deed. This is consistent with the results in term\nof MAA metric. We argue that a possible reason is also the im-\nbalance leading to fluctuations in learning. Overall, these results\ndemonstrate the benefits of our rehearsal algorithm.\nAnalysis of Longer Training. To validate our second assumption\nin Sec. 1 that one reason for the poor performance of the initial few\ntasks is due to slow convergence, we conduct experiments with\ndifferent numbers of epochs on the CIFAR100 dataset, including 200,\n300, and 400 epochs. The outcomes are shown in Table 5. We observe\nthat enhancing the number of training epochs resulting in better\nperformance for Simsiam and Barlow-twins. However, the results\nof Simsiam still suffer from suboptimal issue, which demonstrates\nthat this assumption 1 is false. In addition, the results of 400-epoch\nSimsiam training are still inferior to those of 200-epoch Simsiam\nwith CUCL training.\nT1\nT3\nT5\nT7\nT9\nTask\n50\n55\n60\n65\n70\n75\n80\nMAA (%)\nSimsiam\nBYOL\nBarlow twins\nFigure 4: The learning curves in terms of MAA on the Split\nCIFAR100. The dashed lines shows the results of finetuning\nthese methodologies. The solid lines are the results when\nthey are incorporated with our approach.\n4.2.4\nAnalysis of the training process. To further evaluate the learn-\ning ability of model during the training process, we plot the results\non CIFAR100 in terms of MAA metric. Three models including\nSimsiam, BYOL, and Barlow-twins are considered as unsupervised\nbackbones. Dashed lines represent results when the models are\nfine-tuned, while solid curves show the performance when our\nCUCL is integrated. The final learning curves are shown in Fig. 4.\nThese curves reveal that Simsiam and BYOL face the suboptimal is-\nsue. Specifically, we observe considerate improvements on Simsiam\non the first (50.4% vs. 78.6%), and on the end (67.48% vs. 76.07%).\nFurthermore, Barlow-twins exhibits good performance in the ini-\ntial task, but it suffers catastrophic forgetting (77.3% vs. 72.91%).\nNonetheless, our approach enhances the first task’s outcome to\n77.7%. In addition, the effective rehearsal method helps Barlow-\ntwins to retain previous acquired knowledge, leading to a 75.29%\noutcome.\n5\nCONCLUSION\nThis paper investigated the unsupervised continual learning para-\ndigm. Our results revealed a problem that the performances of the\nfirst few tasks are suboptimal. Additionally, to ameliorate this prob-\nlem, we proposed Codebook for Unsupervised Continual Learning\n(CUCL) to confer diversity for representations and apply the con-\ntrastive learning on the original representation and the quantized\none from a different view to guide the model to capture discrimina-\ntive features. The outcomes of extensive experiments demonstrated\nthe efficacy of our proposed method.\nACKNOWLEDGMENTS\nThis study is supported by grants from National Key R&D Program\nof China (2022YFC2009903/2022YFC2009900), the National Natural\nScience Foundation of China (Grant No. 62122018, No. 62020106008,\nNo. 61772116, No. 61872064), Fok Ying-Tong Education Founda-\ntion(171106), and SongShan Laboratory YYJC012022019.\nCUCL: Codebook for Unsupervised Continual Learning\nMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada\nREFERENCES\n[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and\nTinne Tuytelaars. 2018. Memory Aware Synapses: Learning What (not) to Forget.\nIn ECCV, Vol. 11207. 144–161.\n[2] Ali Ayub and Alan R. Wagner. 2021. EEC: Learning to Encode and Regenerate\nImages for Continual Learning. In ICLR.\n[3] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah.\n1993. Signature Verification Using a Siamese Time Delay Neural Network. In\nNeurIPS. 737–744.\n[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone\nCalderara. 2020. Dark Experience for General Continual Learning: a Strong,\nSimple Baseline. In NeurIPS.\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and\nArmand Joulin. 2020. Unsupervised Learning of Visual Features by Contrasting\nCluster Assignments. In NeurIPS.\n[6] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elho-\nseiny. 2019. Efficient Lifelong Learning with A-GEM. In ICLR.\n[7] Cheng Chen, Ji Zhang, Jingkuan Song, and Lianli Gao. 2022. Class Gradient\nProjection For Continual Learning. In ACM MM. 5575–5583.\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.\nA Simple Framework for Contrastive Learning of Visual Representations. In\nICML, Vol. 119. 1597–1607.\n[9] Xinlei Chen and Kaiming He. 2021. Exploring Simple Siamese Representation\nLearning. In CVPR. 15750–15758.\n[10] Xinlei Chen, Saining Xie, and Kaiming He. 2021. An Empirical Study of Training\nSelf-Supervised Vision Transformers. In ICCV. 9620–9629.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Ima-\ngeNet: A large-scale hierarchical image database. In CVPR. 248–255.\n[12] Lianli Gao, Pengpeng Zeng, Jingkuan Song, Xianglong Liu, and Heng Tao Shen.\n2018. Examine before You Answer: Multi-task Learning with Adaptive-attentions\nfor Multiple-choice VQA. In ACM MM. 1742–1750.\n[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H.\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan\nGuo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos,\nand Michal Valko. 2020. Bootstrap Your Own Latent - A New Approach to\nSelf-Supervised Learning. In NeurIPS.\n[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B.\nGirshick. 2022. Masked Autoencoders Are Scalable Vision Learners. In CVPR.\n15979–15988.\n[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020.\nMomentum Contrast for Unsupervised Visual Representation Learning. In CVPR.\n9726–9735.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. In CVPR. 770–778.\n[17] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip\nBachman, Adam Trischler, and Yoshua Bengio. 2019. Learning deep representa-\ntions by mutual information estimation and maximization. In ICLR.\n[18] Young Kyun Jang and Nam Ik Cho. 2021. Self-supervised Product Quantization\nfor Deep Unsupervised Image Retrieval. In ICCV. 12065–12074.\n[19] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and\nRaia Hadsell. 2016. Overcoming catastrophic forgetting in neural networks. CoRR\nabs/1612.00796 (2016).\n[20] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features\nfrom tiny images. (2009).\n[21] Xiangpeng Li, Lianli Gao, Xuanhan Wang, Wu Liu, Xing Xu, Heng Tao Shen, and\nJingkuan Song. 2019. Learnable Aggregating Net with Diversity Learning for\nVideo Question Answering. In ACM MM. 1166–1174.\n[22] Zhizhong Li and Derek Hoiem. 2018. Learning without Forgetting. TPAMI 40\n(2018), 2935–2947.\n[23] David Lopez-Paz and Marc’Aurelio Ranzato. 2017. Gradient Episodic Memory\nfor Continual Learning. In NeurIPS. 6467–6476.\n[24] Xinyu Lyu, Lianli Gao, Pengpeng Zeng, Heng Tao Shen, and Jingkuan Song. 2023.\nAdaptive Fine-Grained Predicates Learning for Scene Graph Generation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence (2023).\n[25] Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang.\n2022. Representational Continuity for Unsupervised Continual Learning. In\nICLR.\n[26] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in con-\nnectionist networks: The sequential learning problem. In Psychology of learning\nand motivation. Vol. 24. 109–165.\n[27] Dushyant Rao, Francesco Visin, Andrei A. Rusu, Razvan Pascanu, Yee Whye Teh,\nand Raia Hadsell. 2019. Continual Unsupervised Representation Learning. In\nNeurIPS. 7645–7655.\n[28] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H.\nLampert. 2017. iCaRL: Incremental Classifier and Representation Learning. In\nCVPR. 5533–5542.\n[29] Mark B. Ring. 1998. Child: A First Step Towards Continual Learning. In Learning\nto Learn. 261–292.\n[30] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James\nKirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Pro-\ngressive Neural Networks. CoRR abs/1606.04671 (2016).\n[31] Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. 2018. Over-\ncoming Catastrophic Forgetting with Hard Attention to the Task. In ICML, Vol. 80.\n4555–4564.\n[32] Pablo Sprechmann, Siddhant M. Jayakumar, Jack W. Rae, Alexander Pritzel,\nAdrià Puigdomènech Badia, Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan\nPascanu, and Charles Blundell. 2018. Memory-based Parameter Adaptation. In\nICLR.\n[33] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive Multiview\nCoding. In ECCV, Vol. 12356. 776–794.\n[34] Tom Veniat, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2021. Efficient Contin-\nual Learning with Modular Networks and Task-Driven Priors. In ICLR.\n[35] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan\nWierstra. 2016. Matching Networks for One Shot Learning. In NeurIPS. 3630–\n3638.\n[36] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. 2018. Unsupervised\nFeature Learning via Non-Parametric Instance Discrimination. In CVPR. 3733–\n3742.\n[37] Mang Ye, Xu Zhang, Pong C. Yuen, and Shih-Fu Chang. 2019. Unsupervised\nEmbedding Learning via Invariant and Spreading Instance Feature. In CVPR.\n6210–6219.\n[38] Tan Yu, Jingjing Meng, Chen Fang, Hailin Jin, and Junsong Yuan. 2020. Product\nQuantization Network for Fast Visual Search. IJCV 128 (2020), 2325–2343.\n[39] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. 2021. Barlow\nTwins: Self-Supervised Learning via Redundancy Reduction. In ICML, Vol. 139.\n12310–12320.\n[40] Pengpeng Zeng, Haonan Zhang, Jingkuan Song, and Lianli Gao. 2022. S2 Trans-\nformer for Image Captioning. In IJCAI, Luc De Raedt (Ed.). 1608–1614.\n[41] Friedemann Zenke, Ben Poole, and Surya Ganguli. 2017. Continual Learning\nThrough Synaptic Intelligence. In ICML, Vol. 70. 3987–3995.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2023-11-25",
  "updated": "2023-11-25"
}