{
  "id": "http://arxiv.org/abs/2312.00101v1",
  "title": "Towards Unsupervised Representation Learning: Learning, Evaluating and Transferring Visual Representations",
  "authors": [
    "Bonifaz Stuhr"
  ],
  "abstract": "Unsupervised representation learning aims at finding methods that learn\nrepresentations from data without annotation-based signals. Abstaining from\nannotations not only leads to economic benefits but may - and to some extent\nalready does - result in advantages regarding the representation's structure,\nrobustness, and generalizability to different tasks. In the long run,\nunsupervised methods are expected to surpass their supervised counterparts due\nto the reduction of human intervention and the inherently more general setup\nthat does not bias the optimization towards an objective originating from\nspecific annotation-based signals. While major advantages of unsupervised\nrepresentation learning have been recently observed in natural language\nprocessing, supervised methods still dominate in vision domains for most tasks.\nIn this dissertation, we contribute to the field of unsupervised (visual)\nrepresentation learning from three perspectives: (i) Learning representations:\nWe design unsupervised, backpropagation-free Convolutional Self-Organizing\nNeural Networks (CSNNs) that utilize self-organization- and Hebbian-based\nlearning rules to learn convolutional kernels and masks to achieve deeper\nbackpropagation-free models. (ii) Evaluating representations: We build upon the\nwidely used (non-)linear evaluation protocol to define pretext- and\ntarget-objective-independent metrics for measuring and investigating the\nobjective function mismatch between various unsupervised pretext tasks and\ntarget tasks. (iii) Transferring representations: We contribute CARLANE, the\nfirst 3-way sim-to-real domain adaptation benchmark for 2D lane detection, and\na method based on prototypical self-supervised learning. Finally, we contribute\na content-consistent unpaired image-to-image translation method that utilizes\nmasks, global and local discriminators, and similarity sampling to mitigate\ncontent inconsistencies.",
  "text": "Towards Unsupervised\nRepresentation Learning:\nLearning, Evaluating and\nTransferring Visual\nRepresentations\nA dissertation submitted by Bonifaz Stuhr at Univer-\nsitat Autònoma de Barcelona to fulfil the degree of\nDoctor of Philosophy.\nBellaterra, September 26, 2023\narXiv:2312.00101v1  [cs.CV]  30 Nov 2023\nCo-Director\nDr. Jordi Gonzàlez Sabaté\nDept. Ciències de la computació & Centre de Visió per Computador\nUniversitat Autònoma de Barcelona\nCo-Director\nDr. Jürgen Brauer\nDepartment of Computer Science\nUniversity of Applied Sciences Kempten\nThesis\nDr. Xavier Roca Marva\ncommittee\nDept. Ciències de la computació & Centre de Visió per Computador\nUniversitat Autònoma de Barcelona\nDr. Ulrich Göhner\nDepartment of Computer Science & IFM\nUniversity of Applied Sciences Kempten\nDr. Wenjuan Gong\nCollege of Computer Science and Technology\nChina University of Petroleum (East China)\nThis document was typeset by the author using LATEX2ε.\nA dissertation submitted by Bonifaz Stuhr at Dept. Ciències de la computació, Universitat\nAutònoma de Barcelona to fulfil the degree of Doctor of Philosophy within the Computer\nScience Doctorate Program. Copyright ©2023 by Bonifaz Stuhr. All rights reserved. No part\nof this publication may be reproduced or transmitted in any form or by any means, electronic\nor mechanical, including photocopy, recording, or any information storage and retrieval\nsystem, without permission in writing from the author.\nISBN: 978-84-126409-6-0\nPrinted by Ediciones Gráficas Rey, S.L.\nBellaterra, September 26, 2023\nIt is important that students bring a certain ragamuffin, barefoot, irreverence to\ntheir studies; they are not here to worship what is known, but to question it.\n— Jacob Bronowski\nFor my family and friends\nAcknowledgements\nLooking back, it is all but incomprehensible for me how valuable the last years as a\ndoctoral student have been for my academic and personal development and my\nlife as a whole. I have deeply enjoyed the academic freedom; it has made me more\ncritical in a positive, curious way and has led me to many beautiful discoveries\nabout the scientific community and myself - as a small part of it. I look forward to\nfurther paddling my rowing boat through the misty and mysterious sea of science\nin search of more fascinating wonders beyond the shimmering fog.\nFor this dissertation and the invaluable opportunity to contribute to the research\nof new intelligence, I must especially express my deepest gratitude to my supervisors\nProf. Dr. Jürgen Brauer and Prof. Dr. Jordi Gonzàlez Sabaté. The freedom you gave\nme and your wise and on-point advice were the best things that could happen to\nme during my Ph.D. years. I can not thank you enough for your trust and kindness.\nJürgen, I will miss our long meetings where we talked about science, teaching,\nand technical innovations. The memories of our conference trip to Florida always\nbring a smile to my face. Jordi, I can not remember any meeting with you that was\nnot enlightening for me. From the first time we met, I knew you were the perfect\nsupervisor for this thesis, both scientifically and organizationally. I will always keep\nyour wise perspective on our papers and the academic field as a whole in my mind.\nI am also extremely grateful to Prof. Bernhard Schick from the IFM. Bernhard, you\nmade it possible for me to work with the Adrive team and you gave me the freedom\nto pursue my ideas in the context of your topics, which led to amazing projects,\nevents, and teaching opportunities. I hope that the future ADC team will continue\nour legacy! At this point, I would like to thank my colleagues at the CVC and IFM\nsincerely; our academic, engineering, and recreational discussions frequently made\nmy day. Johann, collaborating with you especially has made for incredible moments\nand achievements. Without you, research, teaching, and participation in the VDI\nADC Cup would only be half as much fun. Julian, thank you for your dedication to\nyour master’s thesis and our subsequent project. Huge thanks also go to the CVC\nand IFM administrative and marketing staff; you keep the research running!\nFurthermore, I thank all my friends, with whom I could easily find distraction\nand tranquility. A big thank you goes to my uncle for professionally proofreading this\nthesis. Moreover, I am grateful to my grandparents and brothers for their positive\nenergy, wishes, and support. Finally, I want to thank my parents from the bottom of\nmy heart for their deep love, everlasting support, and for being positive role models.\nYour influence on me and my life was one of the reasons - if not the reason - that\ngave me the opportunity and confidence to tackle the tasks of a dissertation.\ni\nResum\nL’aprenentatge de representacions no supervisat té com a objectiu trobar mètodes que apren-\nguin representacions a partir de dades sense senyals basats en anotacions. Abstindre’s de les\nanotacions no només comporta beneficis econòmics, sinó que també pot, i en certa mesura\nja ho fa, comportar avantatges en la estructura de la representació, la robustesa i la capacitat\nde generalització a diferents tasques. A llarg termini, s’espera que els mètodes no supervisats\nsuperin les seves contraparts supervisades a causa de la reducció de la intervenció humana i\nde l’enfocament inherentment més general que no biaixi l’optimització cap a un objectiu\nque prové de senyals específics basats en anotacions. Tot i que recentment s’han observat\navantatges importants de l’aprenentatge de representacions no supervisat en el processa-\nment del llenguatge natural, els mètodes supervisats encara dominen en els dominis de la\nvisió per a la majoria de les tasques. En aquesta tesi, contribuïm al camp de l’aprenentatge\nde representacions (visuals) no supervisades des de tres perspectives: (i) Aprenentatge de\nrepresentacions: Dissenyem Xarxes Neuronals Autoorganitzades Convolucionals (CSNNs)\nno supervisades i lliures de retropropagació que utilitzen regles d’aprenentatge basades en\nautoorganització i en Hebb, per aprendre nuclis convolucionals i màscares amb l’objectiu\nd’assolir models més profunds sense retropropagació. Observem que els mètodes basats en\nretropropagació i lliures de retropropagació poden patir d’una manca de coincidència de la\nfunció objectiu entre la tasca de pretext no supervisada i la tasca objectiu, la qual cosa pot\nportar a una disminució en el rendiment per a la tasca objectiu. (ii) Avaluació de la repre-\nsentació: Ens basem en el protocol d’avaluació (no) lineal àmpliament utilitzat per definir\nmètriques independents de la tasca de pretext i la tasca objectiu per a mesurar la manca\nde coincidència de la funció objectiu. Amb aquestes mètriques, avaluem diverses tasques\nde pretext i objectiu i revelem les dependències de la manca de coincidència de la funció\nobjectiu en diferents parts de l’entrenament i la configuració del model. (iii) Transferència de\nrepresentacions: Contribuïm amb CARLANE, el primer banc de proves d’adaptació de domini\nsim-to-real de 3 vies per a la detecció de carrils 2D. Adoptem diversos mètodes coneguts\nd’adaptació de domini no supervisat com a referència i proposem un mètode basat en l’apre-\nnentatge auto-supervisat prototípic entre dominis. Finalment, ens centrem en l’adaptació de\ndomini no supervisat basada en píxels i contribuïm amb un mètode de traducció d’imatge a\nimatge no aparellat consistent en contingut que utilitza màscares, discriminadors globals\ni locals, i mostreig de similitud per mitigar les inconsistències de contingut, així com la\ndenormalització atenta a característiques per fusionar estadístiques basades en contingut en\nla seqüència del generador. A més, proposem la mètrica cKVD per incorporar inconsistències\nde contingut específiques de classes en mètriques perceptuals per a mesurar la qualitat de la\ntraducció.\nParaules clau: aprenentatge de representacions no supervisat, adaptació de domini no\nsupervisada, traducció d’imatge a imatge, visió per computador\niii\nResumen\nEl aprendizaje de representaciones no supervisado tiene como objetivo encontrar méto-\ndos que aprendan representaciones a partir de datos sin señales basadas en anotaciones.\nAbstenerse de las anotaciones no solo conlleva beneficios económicos, sino que también\npuede, y en cierta medida ya lo hace, resultar en ventajas en cuanto a la estructura de la\nrepresentación, la robustez y la capacidad de generalización a diferentes tareas. A largo plazo,\nse espera que los métodos no supervisados superen a sus contrapartes supervisadas debido\na la reducción de la intervención humana y al enfoque inherentemente más general que\nno sesga la optimización hacia un objetivo que proviene de señales específicas basadas\nen anotaciones. Si bien recientemente se han observado ventajas importantes del apren-\ndizaje de representaciones no supervisadas en el procesamiento del lenguaje natural, los\nmétodos supervisados todavía dominan en los dominios de la visión para la mayoría de las\ntareas. En esta tesis, contribuimos al campo del aprendizaje de representaciones (visuales)\nno supervisadas desde tres perspectivas: (i) Aprendizaje de representaciones: Diseñamos\nRedes Neuronales Autoorganizadas Convolucionales (CSNNs) no supervisadas y libres de\nretropropagación que utilizan reglas de aprendizaje basadas en autoorganización y en Hebb,\npara aprender núcleos convolucionales y máscaras con el fin de lograr modelos más pro-\nfundos sin retropropagación. Observamos que los métodos basados en retropropagación\ny libres de retropropagación pueden sufrir de una falta de coincidencia de la función ob-\njetivo entre la tarea de pretexto no supervisada y la tarea objetivo, lo que puede llevar a\ndisminuciones en el rendimiento para la tarea objetivo. (ii) Evaluación de representación:\nNos basamos en el protocolo de evaluación (no) lineal ampliamente utilizado para definir\nmétricas independientes de la tarea de pretexto y la tarea objetivo para medir la falta de\ncoincidencia de la función objetivo. Con estas métricas, evaluamos varias tareas de pretexto\ny objetivo y revelamos las dependencias de la falta de coincidencia de la función objetivo en\ndiferentes partes del entrenamiento y la configuración del modelo. Y (iii) Transferencia de\nrepresentaciones: Contribuimos con CARLANE, el primer banco de pruebas de adaptación\nde dominio sim-to-real de 3 vías para la detección de carriles 2D. Adoptamos varios métodos\nconocidos de adaptación de dominio no supervisado como referencia y proponemos un mé-\ntodo basado en el aprendizaje auto-supervisado prototípico entre dominios. Por último, nos\nenfocamos en la adaptación de dominio no supervisada basada en píxeles y contribuimos\ncon un método de traducción de imagen a imagen no emparejado consistente en contenido\nque utiliza máscaras, discriminadores globales y locales, y muestreo de similitud para mitigar\nlas inconsistencias de contenido, así como la denormalización atenta a características para\nfusionar estadísticas basadas en contenido en la secuencia del generador. Además, propone-\nmos la métrica cKVD para incorporar inconsistencias de contenido específicas de clases en\nmétricas perceptuales para medir la calidad de la traducción.\nPalabras clave: aprendizaje de representaciones no supervisada, adaptación de dominio\nno supervisado, traducción de imagen a imagen, visión por computador\nv\nAbstract\nUnsupervised representation learning aims at finding methods that learn represen-\ntations from data without annotation-based signals. Abstaining from annotations\nnot only leads to economic benefits but may - and to some extent already does -\nresult in advantages regarding the representation’s structure, robustness, and gener-\nalizability to different tasks. In the long run, unsupervised methods are expected to\nsurpass their supervised counterparts due to the reduction of human intervention\nand the inherently more general setup that does not bias the optimization towards\nan objective originating from specific annotation-based signals. While major ad-\nvantages of unsupervised representation learning have been recently observed in\nnatural language processing, supervised methods still dominate in vision domains\nfor most tasks. In this dissertation, we contribute to the field of unsupervised (visual)\nrepresentation learning from three perspectives: (i) Learning representations: We\ndesign unsupervised, backpropagation-free Convolutional Self-Organizing Neural\nNetworks (CSNNs) that utilize self-organization- and Hebbian-based learning rules\nto learn convolutional kernels and masks to achieve deeper backpropagation-free\nmodels. Thereby, we observe that backpropagation-based and -free methods can\nsuffer from an objective function mismatch between the unsupervised pretext task\nand the target task. This mismatch can lead to performance decreases for the target\ntask. (ii) Evaluating representations: We build upon the widely used (non-)linear\nevaluation protocol to define pretext- and target-objective-independent metrics\nfor measuring the objective function mismatch. With these metrics, we evaluate\nvarious pretext and target tasks and disclose dependencies of the objective func-\ntion mismatch concerning different parts of the training and model setup. (iii)\nTransferring representations: We contribute CARLANE, the first 3-way sim-to-real\ndomain adaptation benchmark for 2D lane detection. We adopt several well-known\nunsupervised domain adaptation methods as baselines and propose a method\nbased on prototypical cross-domain self-supervised learning. Finally, we focus on\npixel-based unsupervised domain adaptation and contribute a content-consistent\nunpaired image-to-image translation method that utilizes masks, global and local\ndiscriminators, and similarity sampling to mitigate content inconsistencies, as well\nas feature-attentive denormalization to fuse content-based statistics into the gener-\nator stream. In addition, we propose the cKVD metric to incorporate class-specific\ncontent inconsistencies into perceptual metrics for measuring translation quality.\nKey words: unsupervised representation learning, unsupervised domain adapta-\ntion, unpaired image-to-image translation, computer vision\nvii\nContents\nAbstract (Catalan/Spanish/English)\ni\nList of Figures\nxvii\nList of Tables\nxxi\nList of Abbreviations\nxxiii\n1\nIntroduction\n1\n1.1\nUnsupervised Learning of Visual Representations . . . . . . . . . . . .\n3\n1.2\nEvaluation of Unsupervised Representation Learning . . . . . . . . . .\n4\n1.3\nUnsupervised Adaptation of Visual Representations\n. . . . . . . . . .\n5\n1.4\nUnsupervised Translation of Visual Representations . . . . . . . . . .\n6\n1.5\nObjectives and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.6\nOutline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nRelated Work\n11\n2.1\nUnsupervised Learning of Visual Representations . . . . . . . . . . . .\n11\n2.2\nEvaluating Visual Unsupervised Representation\nLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.3\nUnsupervised Adaptation of Visual Representations\n. . . . . . . . . .\n18\nix\nCONTENTS\n2.4\nUnsupervised Translation of Visual Representations . . . . . . . . . .\n21\nI\nUnsupervised Learning of\nVisual Representations\n25\n3\nSelf-Organizing Convolutional Neural Networks\n27\n3.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n3.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.3\nCSNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.3.1\nConvolutional, Self-Organizing Layer . . . . . . . . . . . . . . .\n29\n3.3.2\nCompetitive Learning Rule . . . . . . . . . . . . . . . . . . . . .\n30\n3.3.3\nMask Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.3.4\nLocal Learning Rules . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.3.5\nMulti-Head Masked SConv-Layer\n. . . . . . . . . . . . . . . . .\n36\n3.3.6\nOther Layers and Methods\n. . . . . . . . . . . . . . . . . . . . .\n36\n3.4\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.4.1\nDatasets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.4.2\nEvaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.4.3\nModel Architectures . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n3.4.4\nTraining and Evaluation Setup . . . . . . . . . . . . . . . . . . .\n38\n3.4.5\nObjective Function Mismatch\n. . . . . . . . . . . . . . . . . . .\n39\n3.4.6\nAblation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.4.7\nComparison to the State of the Art . . . . . . . . . . . . . . . . .\n42\n3.4.8\nGeneralization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nx\nCONTENTS\n3.4.9\nNeuron Utilization . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n3.4.10 Few-shot Performance . . . . . . . . . . . . . . . . . . . . . . . .\n44\n3.4.11 Visual Interpretations\n. . . . . . . . . . . . . . . . . . . . . . . .\n44\n3.5\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nII\nEvaluating Visual\nUnsupervised Repre-\nsentation Learning\n47\n4\nInvestigating the Objective Function Mismatch\n49\n4.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n4.3\nHard Metrics Mismatch\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n4.3.1\nHard Objective Function Mismatch . . . . . . . . . . . . . . . .\n53\n4.4\nSoft Metrics Mismatch . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n4.4.1\nSoft Objective Function Mismatch . . . . . . . . . . . . . . . . .\n56\n4.5\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n4.6\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n4.6.1\nMismatch and Convergence\n. . . . . . . . . . . . . . . . . . . .\n59\n4.6.2\nStability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n4.6.3\nDependence on Representation Size . . . . . . . . . . . . . . . .\n61\n4.6.4\nDependence on Target Model Complexity\n. . . . . . . . . . . .\n62\n4.6.5\nDependence on Augmentations . . . . . . . . . . . . . . . . . .\n62\n4.6.6\nDependence on Target Task Type . . . . . . . . . . . . . . . . . .\n63\nxi\nCONTENTS\n4.6.7\nApplying our Metrics to ResNet Models . . . . . . . . . . . . . .\n63\n4.7\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n4.8\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nIII\nUnsupervised Transfer of\nVisual Representations\n67\n5\nA Lane Detection Benchmark for Multi-Target Domain Adaptation\n69\n5.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n5.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n5.3\nData Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n5.3.1\nReal-World Environment\n. . . . . . . . . . . . . . . . . . . . . .\n72\n5.3.2\nReal-World Data Collection . . . . . . . . . . . . . . . . . . . . .\n72\n5.3.3\nSimulation Environment\n. . . . . . . . . . . . . . . . . . . . . .\n72\n5.3.4\nSimulation Data Agent . . . . . . . . . . . . . . . . . . . . . . . .\n73\n5.4\nThe CARLANE Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n5.4.1\nDataset Format . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n5.4.2\nDataset Tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n5.5\nBenchmark Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n5.5.1\nMetrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n5.5.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n5.5.3\nCompared Unsupervised Domain Adaptation Methods . . . .\n77\n5.5.4\nImplementation Details . . . . . . . . . . . . . . . . . . . . . . .\n77\n5.5.5\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\nxii\nCONTENTS\n5.6\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n6\nContent-Consistent Translation with Masked Discriminators\n83\n6.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n6.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n6.3\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n6.3.1\nContend-based Similarity Sampling . . . . . . . . . . . . . . . .\n86\n6.3.2\nContend-based Discriminator Masking . . . . . . . . . . . . . .\n87\n6.3.3\nLocal Discriminator\n. . . . . . . . . . . . . . . . . . . . . . . . .\n87\n6.3.4\nFeature-attentive Denormalization (FATE) . . . . . . . . . . . .\n88\n6.3.5\nTraining Objective\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n6.4\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n6.4.1\nExperimental Settings . . . . . . . . . . . . . . . . . . . . . . . .\n89\n6.4.2\nComparison to the State of the Art . . . . . . . . . . . . . . . . .\n92\n6.4.3\nAblation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n6.5\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\nIV\nClausula\n103\n7\nConclusions\n105\n7.1\nConclusion and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . 105\n7.2\nFuture Perspectives on Unsupervised Representation Learning . . . . 108\n7.3\nSummary of Contributions\n. . . . . . . . . . . . . . . . . . . . . . . . . 109\n7.3.1\nList of Publications . . . . . . . . . . . . . . . . . . . . . . . . . . 110\nxiii\nCONTENTS\n7.3.2\nContributed Code and Datasets\n. . . . . . . . . . . . . . . . . . 111\n7.3.3\nAwards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\nA Supplementary Material:\nSelf-Organizing Convolutional Neural Networks\n113\nB Supplementary Material:\nInvestigating the Objective Function Mismatch\n121\nB.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nB.2 Additional Model and Training Details . . . . . . . . . . . . . . . . . . . 123\nB.3 Additional Evidence\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\nC Supplementary Material:\nA Lane Detection Benchmark for Multi-Target Domain Adaptation\n135\nC.1 Example Usage of the CARLANE Benchmark . . . . . . . . . . . . . . . 135\nC.2 Model Vehicle Description . . . . . . . . . . . . . . . . . . . . . . . . . . 135\nC.3 Reproducibility of the Baselines\n. . . . . . . . . . . . . . . . . . . . . . 135\nC.4 Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\nC.5 Comparison to Related Work . . . . . . . . . . . . . . . . . . . . . . . . 137\nC.6 NeurIPS Checklist for the CARLANE Benchmark . . . . . . . . . . . . . 141\nC.7 Datasheet for the\nCARLANE Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\nD Supplementary Material:\nContent-Consistent Translation with Masked Discriminators\n151\nD.1 The FeaMGAN Architecture . . . . . . . . . . . . . . . . . . . . . . . . . 151\nD.2 Additional Dataset Details . . . . . . . . . . . . . . . . . . . . . . . . . . 155\nxiv\nCONTENTS\nD.3 Additional Training Details\n. . . . . . . . . . . . . . . . . . . . . . . . . 155\nD.4 Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\nBibliography\n195\nxv\nList of Figures\n1.1\nHigh-level conceptual overview of examined and proposed methods\nand metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n3.1\nThe Scalar Bow Tie Problem.\n. . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.2\nArchitecture of a CSNN layer. . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.3\nCSNN mask types.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3.4\nAccuracies of our models trained with our local mask learning rules\nand different batch sizes.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n3.5\nNeuron utilization and few-shot learning.\n. . . . . . . . . . . . . . . .\n43\n3.6\nBMU images for each layer of the D-CSNN model during training. . .\n45\n3.7\nRepresentations and reconstructions. . . . . . . . . . . . . . . . . . . .\n46\n4.1\nThe objective function mismatch. . . . . . . . . . . . . . . . . . . . . .\n50\n4.2\nM3 and MM3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.3\nOFM, cFM, mOFM, and MOFM. . . . . . . . . . . . . . . . . . . . . . .\n55\n4.4\nOFM ablation study.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n4.5\nPretext and target losses on the 3dshapes dataset. . . . . . . . . . . . .\n61\n4.6\nOFM for different pretext tasks trained with a ResNet18 model as\nbackbone.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nxvii\nLIST OF FIGURES\n5.1\nImages sampled from our CARLANE Benchmark. . . . . . . . . . . . .\n70\n5.2\nOverview of our track types for MoLane.\n. . . . . . . . . . . . . . . . .\n72\n5.3\nLane annotation distributions of the three subsets of CARLANE. . . .\n74\n5.4\nt-SNE visualization of MuLane dataset. . . . . . . . . . . . . . . . . . .\n79\n5.5\nQualitative results of target domain predictions. . . . . . . . . . . . . .\n80\n6.1\nResults of our method. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n6.2\nMethod overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n6.3\nFADE and FATE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n6.4\nQualitative comparison to EPE. . . . . . . . . . . . . . . . . . . . . . . .\n92\n6.5\nQualitative comparison to prior work. . . . . . . . . . . . . . . . . . . .\n94\n6.6\nQualitative ablations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n6.7\nFATE attention maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n6.8\nQualitative ablation of crop sizes.\n. . . . . . . . . . . . . . . . . . . . .\n98\n6.9\nQuantitative ablation of crop sizes. . . . . . . . . . . . . . . . . . . . . .\n98\n6.10 Additional qualitative results. . . . . . . . . . . . . . . . . . . . . . . . . 101\n6.11 Limitations.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nA.1 Detailed architecture of the S-CSNN model with three SOM maps per\nlayer.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nA.2 Mask and SOM weight changes.\n. . . . . . . . . . . . . . . . . . . . . . 114\nA.3 SOM weights during training. . . . . . . . . . . . . . . . . . . . . . . . . 115\nA.4 More BMU images for each layer of the D-CSNN. . . . . . . . . . . . . 116\nA.5 More BMU images for each layer of the D-CSNN. . . . . . . . . . . . . 117\nxviii\nLIST OF FIGURES\nA.6 Mask images.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nA.7 Neighborhood coefficients. . . . . . . . . . . . . . . . . . . . . . . . . . 119\nB.1 Stability of the partially measured OFM.\n. . . . . . . . . . . . . . . . . 128\nB.2 Stability of the fully measured OFM. . . . . . . . . . . . . . . . . . . . . 129\nB.3 Additional evidence for the mismatch and convergence section 4.6.1. 129\nB.4 Version of Figure 4.4 without convergence criterium. . . . . . . . . . . 130\nB.5 Version of Figure 4.4 for SM3 on accuracies, without convergence\ncriterium. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nB.6 SCLCAE target losses for the object hue class of 3dshapes. . . . . . . . 131\nB.7 The version of Figure 4.4 for M3 on accuracies. . . . . . . . . . . . . . . 132\nB.8 The version of Figure 4.5 for SM3 on accuracies. . . . . . . . . . . . . . 133\nB.9 OFM and MM3 for other ResNets. . . . . . . . . . . . . . . . . . . . . . 133\nB.10 MM3 for different pretext tasks trained with a ResNet18 model as\nbackbone.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\nC.1 Picture of the 1/8th model vehicle we built to capture images in our\n1/8th target domain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\nC.2 t-SNE visualizations of the MoLane dataset and the TuLane dataset. . 136\nC.3 Visual comparison of simulation images. . . . . . . . . . . . . . . . . . 138\nC.4 More qualitative results of target domain predictions.\n. . . . . . . . . 138\nC.5 More qualitative results of target domain predictions.\n. . . . . . . . . 139\nC.6 More qualitative results of target domain predictions.\n. . . . . . . . . 140\nD.1 Generator architecture.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 151\nD.2 FATE residual block.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nxix\nLIST OF FIGURES\nD.3 Generator stream encoder.\n. . . . . . . . . . . . . . . . . . . . . . . . . 153\nD.4 Content stream encoder. . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nD.5 Attention module. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nD.6 Discriminator architecture. . . . . . . . . . . . . . . . . . . . . . . . . . 154\nD.7 Qualitative comparison to EPE. . . . . . . . . . . . . . . . . . . . . . . . 157\nD.8 Qualitative comparison to EPE. . . . . . . . . . . . . . . . . . . . . . . . 158\nD.9 Qualitative comparison to prior work. . . . . . . . . . . . . . . . . . . . 159\nD.10 Qualitative ablation of crop sizes.\n. . . . . . . . . . . . . . . . . . . . . 160\nD.11 Qualitative ablations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nxx\nList of Tables\n3.1\nAblation study on the Cifar10 dataset. . . . . . . . . . . . . . . . . . . .\n40\n3.2\nQuantitative comparison with previous work. . . . . . . . . . . . . . .\n42\n3.3\nQuantitative results of models transferred to other datasets. . . . . . .\n43\n4.1\nMOFM, cSM3 and MM3 of the models from Figure 4.4. . . . . . . . . .\n60\n4.2\ncSM3, MOFM and MM3 on the 3dshapes dataset. . . . . . . . . . . . .\n62\n4.3\nMismatches of ResNets with convergence criterium. . . . . . . . . . .\n63\n5.1\nDataset overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n5.2\nOptimized hyperparameters to achieve the reported results.\n. . . . .\n78\n5.3\nPerformance on the test set. . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n6.1\nQuantitative comparison to the baselines provided by EPE. . . . . . .\n92\n6.2\nQuantitative comparison to prior work. . . . . . . . . . . . . . . . . . .\n94\n6.3\nQuantitative evaluation for ablation study. . . . . . . . . . . . . . . . .\n95\nB.1 Information about measurements and training. . . . . . . . . . . . . . 124\nB.2 Detailed version of CAE (Cifar10) and DCAE (Cifar10) from Table 4.1.\n125\nB.3 Detailed version of CCAE (Cifar100) and RCAE (PCam) from Table 4.1. 126\nxxi\nLIST OF TABLES\nB.4 Detailed version of CCAE (PCam) and SCLCAE (3dshapes for object\nhue classification) from Table 4.1.\n. . . . . . . . . . . . . . . . . . . . . 126\nB.5 Detailed version of CAE and DCAE from Table 4.2.\n. . . . . . . . . . . 127\nB.6 Detailed version of CCAE and RCAE from Table 4.2. . . . . . . . . . . . 127\nB.7 Detailed version of SCLCAE from Table 4.2. . . . . . . . . . . . . . . . . 127\nB.8 Mismatches of other models we have tested. . . . . . . . . . . . . . . . 128\nB.9 Mismatches of other models we have tested. . . . . . . . . . . . . . . . 128\nC.1 Comparison of CARLANE (ours) with datasets created by related work.137\nC.2 Comparison of applied variations for the collection of the synthetic\ndatasets.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\nC.3 Dataset overview. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\nD.1 cKVD class mapping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nD.2 Additional details of the used datasets.\n. . . . . . . . . . . . . . . . . . 156\nD.3 Additional training details. . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nD.4 Extended quantitative comparison to prior work. . . . . . . . . . . . . 162\nD.5 Extended quantitative evaluation for ablation study. . . . . . . . . . . 163\nxxii\nList of Abbreviations\nBMU\nBest Matching Unit\nCAE\nConvolutional AutoEncoder\nCCAE\nConvolutional AutoEncoder for Color restoration\ncKVD\nclass-specific Kernel VGG Distance\nCNN\nConvolutional Neural Network\nCSNN\nConvolutional Self-organizing Neural Network\nDCAE\nDenoising Convolutional AutoEncoder\nFADE\nFeature-ADaptive Denormalization\nFATE\nFeature-ATtentive Denormalization\nFN\nFalse Negatives\nFP\nFalse Positives\nGAN\nGenerative Adversarial Network\nGHA\nGeneralized Hebbian Algorithm\nGPU\nGraphics Processing Unit\nLA\nLane Accuracy\nMLP\nMultiLayer Perceptron\nMMD\nMaximum Mean Discrepancies\nOFM\nObjective Function Mismatch\nPCA\nPrincipal Component Analysis\nRCAE\nConvolutional AutoEncoder for Rotation prediction\nRQ\nResearch Question\nSCLCAE Convolutional AutoEncoder with the Simple Contrastive learning\napproach from Chen et al. [42]\nsconv\nself-organizing convolution\nsKVD\nsemantically aligned Kernel VGG Distance\nxxiii\nCHAPTER 0. LIST OF ABBREVIATIONS\nSOM\nSelf-Organizing Map\nt-SNE\nt-Distributed Stochastic Neighbor Embedding\nUFLD\nFalse Negatives\nVAE\nVariational AutoEncoder\nViT\nVisual image Transformer\nxxiv\n1 Introduction\nUnsupervised representation learning [21] aims at finding methods that learn repre-\nsentations from unlabeled data, which can further be utilized to solve various target\ntasks. This stands in stark contrast to supervised learning, which relies primarily\non carefully annotated data to learn target tasks directly from learning signals that\npair samples with labels. Learning without explicit guidance from task-specific and\nbiased signals offers several ad hoc benefits. Training data for unsupervised models\nis abundant in many domains and does not require a costly and time-consuming\nannotation process. This simplifies the training of large models on large datasets\n- a trend [234] that still results in performance gains. Furthermore, unsupervised\nlearning can lead to benefits in setups where only few annotations are available\n[8, 194, 242]. For some target tasks, a large quantity of annotations is nearly impos-\nsible to acquire, like in medical image analysis [169]. Additionally, there exist tasks\nfor which it is unknown what to label in the data to solve them correctly and for\nwhich it should be considered not to acquire annotations at all, as in the case of\nanomaly detection [35, 37]. In general, what task or objective is necessary to learn\nthe best representation of an environment is an open research question; this is\napplicable to both supervised and unsupervised learning. However, unsupervised\nlearning is inherently a more general approach since the representation is learned\nsolely from the environment itself. Taking it a step further, a rich unsupervised\nrepresentation obviates the need to design complex task-specific mechanisms at\nall, as shown by representations of large natural language models [187]. Here, the\nmodel is prompted with the task itself at inference time. To put it in engineering\nterms: The best part is no part.\nFrom an empirical point of view, experiments show that unsupervised represen-\ntations are more robust against changes in the environment and adversarial attacks\n[24, 107], can lead to improved out-of-distribution detection [107], and generalize\nto various target tasks [68, 301]. However, especially the empirical analysis of the\ngeneralization to various target tasks and domains is still in an early stage.\nRegardless of the advantages and promises of unsupervised representation\nlearning, the history of machine learning more tangibly shows that the increasing\nself-sufficient creation of representations - first by domain-specific human engi-\nneering, then by learning rules applied to carefully curated features or datasets and\nsubsequently by learning hierarchical, deep models directly on large data corpora -\n1\nCHAPTER 1. INTRODUCTION\nix\niy\nrx\nry\nfθ(ix)\nix\nrx\nrxy\nrx\nm(r)\nsr\nr\nfθ(rx)\nfθ([ix,iy])\n(a)\n(b)\n(c)\n(d)\nFigure 1.1: High-level conceptual overview of examined and proposed methods and\nmetrics. a) Unsupervised learning of representations. b) Evaluation of unsupervised\nrepresentation learning. c&d) Transfer of unsupervised representations either by\nadapting representations of different domains to another (c) or by translating a\nrepresentation from one domain to another (d).\nbenefits the performance of machine learning algorithms. Since all of these steps\nhave removed a certain amount of human intervention and each step has led to\nperformance improvements, it is natural to remove even more human intervention\nto find algorithms that learn representations directly from pure data.\nTo date, there is still human intervention and bias in unsupervised learning\nobjectives and architectures. Prominent examples are defined transformations\nor augmentations for positive and negative pairs in contrastive learning [28, 42,\n43, 54, 63, 99, 104, 148, 157, 189, 197, 252, 273, 284] or distillation-based methods\n[13, 16, 17, 33, 34, 46, 83, 84, 94, 287]. As we show in our experiments in chapter 4,\nbiases in the design of methods can lead to a mismatch between the unsuper-\nvised learning objective and the desired target tasks, which results in a loss of\nperformance. Recent work has tried to mitigate these human interventions and\nmismatches, for example, by designing methods that do not rely on hand-designed\naugmentations of the input, like joint-embedding predictive architectures [12]. Re-\ngardless of these hurdles, unsupervised methods are already superior to supervised\nmethods in some fields - most notably in natural language processing [187]. Al-\nthough unsupervised vision models have begun to outperform supervised models\nin certain situations [68, 107, 194, 242, 301], the decisive breakthrough is still to\ncome.\nThis work is located in the field of unsupervised learning of visual representations\nand aims at taking unsupervised learning a few steps further. As shown in Figure 1.1\nand discussed in the following sections, we learn representations from data (a),\ndefine measures to evaluate unsupervised representation learning (b), adapt repre-\n2\n1.1. UNSUPERVISED LEARNING OF VISUAL REPRESENTATIONS\nsentations for different visual domains (c), and transfer visual representations to\nother visual domains (d), all without much human intervention in an unsupervised\nmanner.\n1.1\nUnsupervised Learning of Visual Representations\nUnsupervised visual representation learning aims at finding methods that learn\nrepresentations of unlabeled data from vision modalities, such as images and videos.\nAs shown in Figure 1.1 (a), methods of this field commonly learn a parameterized\nfunction fθ(ix) - the model - in an unsupervised manner. This model infers a\nrepresentation rx from a visual input ix. rx is later utilized to solve target tasks.\nProminent vision-based target tasks are classification, object detection, or segmen-\ntation [47, 131].\nThere are several ways to train a model in an unsupervised manner. Recently,\nespecially self-supervised methods have achieved promising results. There ex-\nists no standardized definition that distinguishes self-supervised learning from\nunsupervised learning. However, self-supervised learning is seen as a subfield\nof unsupervised learning. Even though the line between unsupervised and self-\nsupervised learning is thin, and it can be argued that all unsupervised models are\nself-supervised in some sense, we follow [69] to distinguish both learning schemes:\nUnsupervised learning [1, 47, 66, 69, 70, 81, 148, 276] methods learn directly from\nunlabeled data. Common learning objectives build generative models, such as\nvariational autoencoders (VAEs) [142] or generative adversarial networks (GANs)\n[39, 44, 59, 89, 103, 138, 151, 152, 207, 213, 260, 269, 293], train a density estimator\nlike gaussian mixture models [182], or compress the input into a representation, for\nexample, via autoencoders [220] or classical clustering methods [276].\nSelf-supervised learning [69, 72, 131, 148, 230, 231] methods learn from a sig-\nnal that pairs a self-generated label obtained from the data and the data itself.\nEarly examples learn by predicting rotations or other transformations applied to\nan input image [44, 85, 165, 211, 291] or by solving jigsaw puzzles obtained from\nan image [138, 195]. Recent work has defined objectives most notably based on\nclustered features [9, 11, 18, 31, 33, 83, 84, 158, 281, 290, 309], contrastive learn-\ning [28, 42, 43, 54, 63, 99, 104, 148, 157, 189, 197, 252, 273, 284], distillation [13,\n16, 17, 33, 34, 46, 83, 84, 94, 287], or feature predictive distillation [11, 12, 14, 15].\nCompared to unsupervised learning, self-supervised learning uses a more specific\ndiscriminative, but often less general, training objective. Self-supervised learning is\na subfield of unsupervised learning.\n3\nCHAPTER 1. INTRODUCTION\nTasks used to train unsupervised models are often referred to as pretext tasks. Un-\nsupervised learning aims at finding methods that inherently solve pretext tasks that\nare general enough to learn a perfect representation of the environment. Although\nthere are efforts to find general methods for various pretext tasks and modalities\n[14, 15], methods today often exploit knowledge about the data modality used for\ntraining and therefore tend to be biased towards this modality and the used exploit.\nFor example, as we show in chapter 4, transformation-based pretext tasks tend to\nretain spatial information in the representation while losing information about\nother properties like color.\n1.2\nEvaluation of Unsupervised Representation Learn-\ning\nSince one goal of unsupervised representation learning is finding methods that\nlearn general representations, evaluating these methods is not trivial. Most work\nin this area aims at learning \"useful\" or \"good\" representations, where these terms\nare meant to describe various beneficial properties of the representation, such as\ntransferability to different target tasks. For example, good representations are de-\nfined to be smooth, sparse, specific, spatially and temporally coherent, transferable\nto supervised learning, and have multiple hierarchical explanatory factors with\nsimple dependencies that generalize across tasks [21]. Moreover, the features of the\nrepresentations form categorical, well-separated, variation-robust manifolds for\nwhich the concentration of probability mass can lead to a smaller dimensionality\nthan the input space [21]. However, to date, there exists no metric or property that\nreliably predicts the performance of the representation on different target tasks\n[30, 174, 175]. Therefore, one vision is to find a general measure that reliably pre-\ndicts the quality and performance of the representation for different target tasks\nwith little overhead during the training of the unsupervised model. Since the output\nof unsupervised representation learning is a representation r, a (current) metric\nm often works on r and produces a measurement sr for assessment, as shown in\nFigure 1.1.\nThe linear and nonlinear evaluation protocols are to date the most commonly\nused protocols for quantitative evaluation. Thereby, an untrained linear or non-\nlinear classifier (probe) is introduced on top of the representations of fθ. This probe\nis then trained in an supervised manner on the target task for a part of the training\ndataset or for the entire training dataset. Thereby, the weights of fθ are either frozen\n4\n1.3. UNSUPERVISED ADAPTATION OF VISUAL REPRESENTATIONS\nor fine-tuned on the target task. Afterwards, the performance on the target task is\ncompared with other methods. To achieve a good measure with these protocols, a\ncareful selection of target tasks is required. When the model is fine-tuned on the\ntarget task the unsupervised learning stage is called unsupervised pretraining. This\npretraining is often compared with other weight initialization schemes for target\ntask training.\nOther ways to evaluate representations is by examining their properties empirically\nor visually. An example is [107], in which ImageNet [56] is annotated with factors\nof variation to quantitatively examine if unsupervised models can predict these\nfactors. Another example is [24], in which a generative model is conditioned on the\nunsupervised representation to generate images for quantitative visual examination\nof the representation’s properties. Quantitatively and qualitatively examining repre-\nsentations aids the research process and increases the interpretabillity of methods.\nMost of the recent work has focused on examining the representation at the end\nof training or at a certain training step. In chapter 4 we examine the entire unsuper-\nvised training process by measuring if the pretext objective is able to generalize to\nvarious target tasks during training, or if it mismatches with them after a certain\namount of training steps and therefore leads to lower target task performance.\n1.3\nUnsupervised Adaptation of Visual Representations\nUnsupervised adaptation of visual representations aims at adapting representa-\ntions to different domains. As shown in Figure 1.1, this can be done by training\na model fθ that learns a common latent space over different domains x and y,\nwhere representations rx and ry of inputs ix and iy share properties. However, this\nis not trivial because different domain statistics lead to a domain gap, also called\ndomain shift, between these domains, which harms training and performance [223].\nUnsupervised domain adaptation [73, 172, 268, 298] is a field that aims at adapting\nrepresentations in an unsupervised manner and is our focus in chapter 5. Thereby,\na model trained on one or multiple labeled source domain(s) is adapted to one\nor multiple unlabeled target domain(s). The high-level relation to plain unsuper-\nvised learning is three-fold: First, both unsupervised learning and unsupervised\ndomain adaptation methods aim at learning general representations for different\ntarget tasks. Second, both unsupervised learning and unsupervised domain adapta-\ntion can be seen as special cases of transfer learning. Transfer learning generally\naims at utilizing the knowledge gained during training for other tasks and domains\n[21, 172, 200]. Since unsupervised learning aims at learning representations that\n5\nCHAPTER 1. INTRODUCTION\nare general across tasks and domains, it is a special case of transfer learning where\neither source nor target domains are labeled [21, 172, 200]. Unsupervised domain\nadaptation is a special case of transfer learning where the source is labeled but not\nthe target domain(s). Third, unsupervised domain adaptation performs unsuper-\nvised learning in the target domain and often leverages unsupervised auxiliary tasks\nfor both domains [73, 172, 268, 298].\nUnsupervised domain adaptation enables the training of domain invariant mod-\nels, which leads to several benefits similar to unsupervised learning: Unsupervised\ndomain adaptation reduces extensive data annotation, since domains for which\na massive amount of labeled data is obtainable without much effort (e.g., simula-\ntions) can be utilized to guide the training for the desired task in the target domain.\nFurthermore, only one model needs to be trained for different domains, which\nleads to a reduction in overall complexity. In addition, a major assumption of many\nlearning strategies is that training and testing data share the same distribution.\nThis assumption can easily be violated (over time) if there are subtle differences\nin the subsets, such as different backgrounds or deformations, or variations in\nsample quality. This is known as covariate shift [236]. Therefore, an adaptation or\nfar-ranging generalization may be required at inference time.\nAlthough there have been significant advances in unsupervised domain adap-\ntation, much of this work focuses on simple classification tasks, the transfer to a\nsingle target domain, or do not share a common benchmark due to the lack of these.\nIn chapter 5, we propose a benchmark and a method for 3-way sim-to-real domain\nadaptation for 2D lane detection.\n1.4\nUnsupervised Translation of Visual Representations\nAs shown in Figure 1.1, the unsupervised translation of visual representations fo-\ncuses on learning a domain mapping fθ(rx) of representations rx from one domain\nto another by inferring a translated representation rxy of rx for the other domain.\nThis can be seen as a special case of unsupervised domain adaptation, which gener-\nally aims at learning a domain mapping between different domains, but is allowed\nto adapt the learned representations of all domains to another [268]. In contrast, the\ntranslation of representations preserves certain properties of the source domain,\nsuch as the content of the image, while adapting other properties to the target\ndomain, such as the style of the image.\nUnpaired image-to-image translation. [6, 203] In our work in chapter 6, we fo-\ncus on unsupervised image-to-image translation, which is also called unpaired\nimage-to-image translation [6, 203]. Thereby, a model is trained to translate images\n6\n1.5. OBJECTIVES AND SCOPE\nfrom a source domain to a target domain without seeing concrete pairs in both\ndomains. Therefore, unpaired image-to-image translation can be seen as pixel-level\nunsupervised domain adaptation [73, 268]. There are many prominent applications\nlike photorealism, translation of daytime and seasons, or style transfer [6, 203]. In\neach of these applications, it is either difficult or impossible to collect true pairs\nof samples showing the same scene with the desired variation to utilize them in a\nsupervised setting. In contrast, paired image-to-image translation focuses on use\ncases where paired samples can be easily acquired. Examples are super-resolution,\nwhere a low-resolution image is translated to a high-resolution image, or image\nsynthesis, where an image is synthesized from basic representations such as seg-\nmentation maps [6, 203]. We focus on unpaired image-to-image translation in this\nwork. Besides the concrete image-based use cases and domain adaptation use\ncases, the controllable creation of more data in specific domains is another benefit.\nHowever, since unpaired image-to-image translation methods work at a pixel level,\nit is hard to evaluate the translation quality empirically. Different widely-adopted\nmetrics have been proposed, which use perceptual metrics to compare the target\nimages with the translated source images [6, 203]. However, one open research\nquestion is the content consistency of translated images. Biases between datasets\noften lead to inconsistency, like hallucinations in the translated source images [216].\nFor example, if there are more trees at the top half of the target domain’s images\nthan in the source domain’s images, the model will hallucinate trees into the trans-\nlated source images to fulfill the source-to-target translation objective. Creating\nnon-ill-posed constraints that enforce content consistency and creating metrics\nthat adequately incorporate the negative effect of content-inconsistency are still\nopen research areas. Therefore, we propose a content-consistent unpaired image-\nto-image translation method and a metric that measures inconsistency based on a\nwidely used perceptual metric in chapter 6.\n1.5\nObjectives and Scope\nThis dissertation aims at creating novel methods to learn, evaluate, and transfer\nvisual representations in the unsupervised representation learning field. Therewith,\nwe want to contribute to the general goal of developing methods that lead to useful\nrepresentations from data without much human intervention. The scope of this\nPh.D. thesis focuses on image-based visual domains. These domains are often\n- but not exclusively - located in the fields of driving assistance or autonomous\ndriving. We base the construction of our methods, metrics, and benchmarks on the\nfollowing research questions (RQs):\n7\nCHAPTER 1. INTRODUCTION\nLearning representations:\n1. RQ-L1: Is it possible to create an unsupervised, backpropagation-free method\nwith modules that are currently considered more biologically plausible than\nbackpropagation-based methods?\n2. RQ-L2: Can we extend our unsupervised, backpropagation-free model to a\nlarger model?\nEvaluating representations:\n1. RQ-E1: How well does our unsupervised backpropagation-free method per-\nform compared to backpropagation-free and backpropagation-based meth-\nods?\n2. RQ-E2: Can we design metrics to measure the objective function mismatch\nbetween unsupervised pretext models and (supervised) target models trained\nfor different target tasks?\n3. RQ-E3: Does the objective function mismatch depend on specific parts of the\ntraining setup and model?\n4. RQ-E4: What effect does the objective function have on target task perfor-\nmance?\n5. RQ-E5: Can we create a 3-way sim-to-real domain adaptation benchmark for\n2D lane detection to evaluate unsupervised domain adaptation for transfer-\nring models trained in simulations to multiple real-world domains?\n6. RQ-E6: How well do our unsupervised domain adaptation method and other\nstate-of-the-art methods perform on our 3-way sim-to-real domain adapta-\ntion benchmark?\n7. RQ-E7: How can we incorporate the content inconsistencies in the translated\nimages into the evaluation of unpaired image-to-image translation methods?\n8. RQ-E8: How well does our unpaired image-to-image translation method\nperform compared to the state of the art?\nTransferring representations:\n1. RQ-T1: Can we create and train a single-source, multi-target model for our\n3-way sim-to-real domain adaptation benchmark and adapt (single) models\nto multiple real-world domains?\n8\n1.6. OUTLINE\n2. RQ-T2: Can we utilize masks to create an unpaired image-to-image transla-\ntion method that mitigates content inconsistencies when translating images\nbetween biased datasets?\n3. RQ-T3: Can we improve the incorporation of content features into the gener-\nator by extending feature-adaptive denormalization with an attention mech-\nanism?\nWith the contributions presented in this dissertation, we hope to shed more light on\nthe answers to these questions and to reduce the uncertainty that surrounds them.\n1.6\nOutline\nchapter 2 describes the related work with respect to our contributions: In section 2.1\nthe related work for unsupervised visual representation learning is described. In\nsection 2.2 the related work for the evaluation of (visual) unsupervised learning is\ndescribed. In section 2.3 the related work for image-based unsupervised domain\nadaptation is described, and in section 2.4 the related work for unpaired image-to-\nimage translation is described. Following that, this dissertation is divided into the\nfollowing five chapters:\nchapter 3 deals with visual unsupervised representation learning. Backpropagation-\nfree Convolutional Self-Organizing Neural Networks (CSNNs) are proposed, which\nlearn representations via self-organization-based clustering and local masks. The\nlearning rules for our models differ strongly from backpropagation-based CNNs\n[155], while the structures of the networks are similar. The learned representations\nof our models are tested on various classification downstream tasks to achieve a\ncritical and insightful look into the field of unsupervised representation learning.\nAt the time of publication, the performance of our models was comparable with\nthe state-of-the-art. More notably, we have become aware of the objective function\nmismatch (OFM).\nchapter 4. Our work in chapter 3 and other research shows various problems\nof unsupervised representation learning. One problem is the objective function\nmismatch, which states that the performance on a desired target task can decrease\nwhen the unsupervised pretext task is learned for too long - especially when both\ntasks are ill-posed. We propose metrics to measure this mismatch in a comparable\nmanner and evaluate state-of-the-art methods at the time of publication. Thereby,\nwe find that each of these methods can suffer from the objective function mismatch.\nWe disclose dependencies of this mismatch across several pretext and target tasks\n9\nCHAPTER 1. INTRODUCTION\nwith respect to the pretext model’s representation size, target model complexity,\npretext and target augmentations, as well as pretext and target task types.\nchapter 5. Within the framework of a project to build a self-driving model ve-\nhicle, we transfer lane detection models from simulation to the real world with\nunsupervised domain adaptation methods. Although unsupervised domain adap-\ntation has been applied to a variety of complex vision task, we find that not much of\nthis work focuses on lane detection in autonomous driving. This can be attributed\nto the lack of publicly available datasets. In chapter 5 we describe CARLANE, a\n3-way sim-to-real unsupervised domain adaptation benchmark for 2D lane detec-\ntion. CARLANE encompasses the single-target datasets MoLane and TuLane and\nthe multi-target dataset MuLane. We evaluate several well-known unsupervised\ndomain adaptation methods as systematic baselines and propose our own method\nthat achieves state-of-the-art performance.\nchapter 6. In this chapter, we focus on mitigating content inconsistencies arising in\nunpaired image-to-image translation. We show that masking the discriminator’s\ninput based on content is sufficient to reduce these inconsistencies significantly.\nOur masking procedure leads to more minor artifacts, which we significantly reduce\nby introducing a local discriminator and a similarity sampling technique. We also\ndesign a feature-based denormalization block that incorporates content features\ninto the generator stream by attending to features correlating with a specific prop-\nerty, such as features of shadows. To enable a finer-grained measure of translation\nquality, we propose the cKVD metric to examine translated images at the class or\ncategory level. Our approach results in state-of-the-art performance for photo-\nrealistic sim-to-real and weather translation and performs well for day-to-night\ntranslation.\nchapter 7 concludes the dissertation with a summary of our contributions alongside\na list of publications, a list of contributed code and datasets, and a list of awards\nreceived during the Ph.D. period. Furthermore, we discuss developments, current\nlimitations, and ethics of unsupervised visual representation learning and give\nfuture perspectives on this field.\n10\n2 Related Work\n2.1\nUnsupervised Learning of Visual Representations\nVarious works categorize unsupervised representation learning by the underlying\nmodel architecture [12], the definition of the objective function, or a generic de-\nscription of the learning objective [16, 47, 69, 148]. However, since this field is very\nactive, there is a constant change in categorization. Furthermore, categorization\ndepends on the perspective of the respective work. Here, we categorize unsuper-\nvised methods by generic descriptions of image-based learning objectives since this\nresults in a useful structure for our work.\nSpatial patch-based methods [32, 53, 57, 138, 195, 288] try to leverage the knowl-\nedge obtained by predicting the spatial relationship of patches. Early examples\ndefine pretext tasks by predicting the relative position of a patch in a grid around an-\nother patch [57] or by solving jigsaw puzzles [138, 195], where the relative position\nof each patch is predicted. In [53] a transformer-based model is trained to localize\na random query patch in an image, given the entire image as context. Building\nupon this work, a transformer-based model is subsequently trained to predict the\nposition of all patches in an image, given a ratio of random context patches from\nthe image as input [288]. Recent work has proposed to predict the relative position\nof each patch in a view of the image with respect to a different reference view of the\nsame image [32].\nTransformation prediction [44, 85, 165, 211, 291]: Spatial context can also be en-\ncoded by predicting transformations, which has led to a line of research focusing\non autoencoding transformations rather than data. Early work predicts simple rota-\ntions of an input image [44, 85], while more recent work has focused on complex\ntransformations by minimizing different geometric distances between target and\npredicted transformation matrices [165, 211, 291].\nGeneration-based methods [39, 44, 59, 103, 138, 151, 152, 207, 213, 260, 269, 293]\nexamine the generation of an arbitrary output from a learned representation of\nthe given input. One line of work improves on autoencoders [221] and variational\n11\nCHAPTER 2. RELATED WORK\nautoencoders [142] by defining generation-based pretext tasks, which lead to rep-\nresentations valuable for target tasks. Examples are denoising [39, 260], coloriza-\ntion [138, 151, 152, 293], or inpainting of images [138, 207, 269]. A second line of\nwork based on GANs [89] adjusts their latent space for representation learning, for\nexample, by constraining [213] or changing [59] the architecture. Recently, large\nmasked autoencoders have achieved promising results with an asymmetric encoder-\ndecoder architecture and a high masking ratio [103].\nCluster-based methods [9, 11, 18, 31, 33, 83, 84, 158, 281, 290, 309] cluster rep-\nresentations and use their cluster assignments as pseudo labels for the learning\nsignal. Thereby, the clustering objective is used as a proxy task to calculate pseudo\nlabels. In contrast to classical clustering methods, the goal is to learn a good rep-\nresentation instead of cluster assignments. Early approaches train the model by\nalternating between two steps: First, the cluster assignments of the model’s repre-\nsentations are optimized by a clustering method. Second, the model is trained with\nthe pseudo labels obtained from the assignments. Well-known clustering objec-\ntives use agglomerative clustering [281], k-means [31, 83, 84], anchor neighborhood\ndiscovery [118], or maximize the information between data indices and equiparti-\ntioned pseudo labels [9]. To avoid the costly two-step procedure, recent work has\nutilized online clustering methods which simultaneously update the clusters while\nthe model is training. Most notably, methods have achieved online learning by\nclassifying clustered batches [18], utilizing memory modulus for representations\n[290, 309] and centroids [290], assigning view-invariant codes to learned prototypes\n[11, 33], or with self-organizing layers [158].\nContrastive methods [28, 42, 43, 54, 63, 99, 104, 148, 157, 189, 197, 252, 273, 284]\nutilize context information between negative and positive pairs. Thereby different\nviews are sampled from the inputs and the model tries to predict which views belong\nto each other (positives) and which views are different (negatives). Representations\nof positive views are brought together, while representations of negative views are\npushed away. The definition of negatives and positives depends on the method.\nExamples of positive and negative pairs are augmentations of the same images\nas positives and of different images as negatives [42, 43, 45, 63, 104, 189, 284], dif-\nferent sensory views of an image as positives and a random image as a negative\n[252], as well as corresponding prototypes of samples as positives and vice versa\n[157]. Furthermore, methods explore, e.g., different similarity and objective func-\ntions [28, 63, 99, 157, 189, 197, 284], different transformations to create the views\n[42, 54, 63, 99, 157], or different model architectures [42, 43, 63, 104, 197]. Siamese\nnetworks, in which the encoder for positives and negatives shares weights, are the\nmost commonly used architecture [28, 42, 43, 63, 99, 104, 189, 284]. Contrastive\n12\n2.1. UNSUPERVISED LEARNING OF VISUAL REPRESENTATIONS\nmethods require large numbers of contrastive pairs to work well; therefore many\ntechniques use large batch sizes [42, 43] or memory banks [43, 45, 104, 273, 284].\nThe downsides of high memory requirements has led to the exploration of alterna-\ntives like distillation-based methods.\nDistillation-based methods [13, 16, 17, 33, 34, 46, 83, 84, 94, 287] are inspired\nby knowledge distillation [109] and build upon the success of contrastive meth-\nods. Distillation-based methods make use of a student and a teacher model. Both\nmodels get different augmented views of the same image as input, and the student\nmodel tries to predict the representation of the teacher model. Often the student\nmodel is updated via backpropagation [168, 222, 266], while the teacher model\nis updated with a moving average of the student model’s parameters [34, 84, 94].\nThere is work that shows that the moving average update is not needed when simple\nweight-sharing between both models is combined with a stop-gradient operation\nfor the teacher branch [46] or when both branches are updated with backpropa-\ngation simultaneously [16, 17, 33, 287]. It has been shown that distillation-based\nmethods, unlike most contrastive methods, do not require large batch sizes to per-\nform well [46, 94]. One reason for this is that distillation-based methods capture\nonly positive pairs. However, the student and teacher model need to be prevented\nfrom collapsing to a static representation (mode collabs). There are different tricks\nto mitigate collabs, like mutual information maximization with a global summary\nfeature vector [13], a predictor head on top of the student model [46, 94], a stop-\ngradient operation for the teacher model [46], clustering constraints [33], or by\nredundancy-reduction in the representations of different views via regularization\n[16, 17, 287]. It was hypothesized that batch normalization is needed to prevent\ncollapse [253], but it has been shown that batch normalization can be replaced by\nother normalizations [215] or can be left out entirely for the prediction head [46]. In\n[34] it has been shown that only the centering and sharpening of the teacher output\nis enough to prevent collapse for visual image transformers (ViTs) [61]. However,\nbesides the effectiveness of these methods, there is no clear understanding of how\nthey avoid collapse and why they perform so well. In further work, different forms\nof representations are predicted by the student model, such as codes obtained from\nprototypes [33], a bag of visual words [83, 84], or local and global features [17].\nFeature predictive distillation [11, 12, 14, 15] builds upon the success of distillation-\nbased methods and borrow their setup. Given a masked view, the student model\ntries to predict the teacher model’s representation of a different, potentially masked\nview of the same input. Recent methods differ most notably in how they mask the in-\nput, what part of the teacher model’s representation they predict, and whether other\naugmentations are applied to the input besides masking. Examples of masking\n13\nCHAPTER 2. RELATED WORK\nstrategies are randomly masking patches [11] and different block masking tech-\nniques [12, 14, 15]. Methods predict the representation of the entire input of the\nteacher model [11, 15] or different target blocks [12, 14]. Most of these methods\ndo not use hand-crafted data augmentations in addition to masking to create a\nview; this removes much human intervention [12, 14, 15]. In [12] a predictor head\nconditioned on (latent) variables is introduced to facilitate feature prediction.\nBackpropagation-free methods. While there have been many advances in unsu-\npervised methods that learn with backpropagation, in comparison, there have been\nonly a few efforts to propose backpropagation-free alternatives. Early classical clus-\ntering algorithms [276] like variants of k-means [180], Expectation Maximization\n[55], or Self-Organizing Maps (SOMs) [143] are a form of unsupervised representa-\ntion learning. Dimensionality reduction methods [70] like Principal Component\nAnalysis (PCA) [208] and t-Distributed Stochastic Neighbor Embedding (t-SNE)\n[258] can also be seen as unsupervised representation learning methods. Further-\nmore, different variants and learning rules for Boltzmann machines [2, 81, 111, 114]\nhave been explored.\nBuilding up on these classical methods, weights are learned as convolutional\nfilters in CNN-like architectures with one or few learnable layers. In [49] different\nalgorithms (e.g., k-means) and hyperparameters have been studied for single-layer\nnetworks, and general performance gains with an increasing representation size\nhave been observed. Other examples of single-layer models use SOM variants\n[101, 143, 305] to learn convolutional filters or a spatial image pyramid to learn\nhierarchical representations with single-layer methods [3, 25, 153]. Examples of\ntwo-layer methods stack PCA [36], k-means [60], or SOM [100, 101] convolutional\nlayers. In [95] two layers are trained in a convolutional manner with the Hebbian-\nlike learning rule proposed in [147]. Some of these methods use nonlinear activation\nfunctions [60, 95, 126], normalization [95, 126], and pooling layers between the two\nconvolutional layers [60, 95, 101, 126]. Both one- and two-layer methods often use\nan output stage based on binary hashing and histograms to reduce dimensionality\nand increase nonlinearity, invariance, and robustness in the representation [36, 60,\n100, 101, 126]. Furthermore, there exists work on two-layer unsupervised spiking\nneural network models using spike-timing dependent plasticity learning rules [228].\nIn [166] k-means representations are encoded to a sparse representation to\nachieve a three-layer network. Furthermore, Boltzmann Machines can be stacked\nto Deep Boltzmann Machines with up to three non-convolutional layers [225].\nHowever, it has been found that simple stacking of these backpropagation-free\nlayers with potential pooling, normalization, and encoding layers brings little to\nno benefit. Therefore, recent work has tried to find methods to connect layers\nefficiently or has searched directly for stackable learning rules. Most successfully,\n14\n2.2. EVALUATING VISUAL UNSUPERVISED REPRESENTATION\nLEARNING\nvariants of biologically-inspired Hebbian-like learning rules [106] are used in these\nmethods to build multi-level models [123, 134, 186, 238]. In our work [238], pre-\nsented in chapter 3, three multi-headed SOM layers are stacked with Hebbian-like\nlearning rules in a convolutional manner. To the best of our knowledge, this is the\nfirst approach to combine the successful principles of CNNs, SOMs, and Hebbian\nLearning into a single architecture. In subsequent work, inspired by local con-\ntrastive learning [178], five-layer models are learned with a Hebbian-like learning\nrule to predict features of saccades [123]. In [186] up to three Hebbian-like layers\nhave been stacked with a channel-wise hard winner-take-all selection for each\nlayer. Further work improves over these methods by building three- to five-layer\nnetworks with a soft, softmax-based winner-take-all selection [191] as well as with\nother learning and architecture improvements like an adaptive learning rate per\nneuron [134].\nOther methods use meta-learning [115, 232], self-supervised relational reason-\ning [206], mutual information maximization [13, 112, 148, 271], or task-specific\nunsupervised setups [87]. Furthermore, many methods link [33, 271] or combine\nmultiple self-supervised approaches [11, 44, 54, 83, 84, 138, 157, 279].\n2.2\nEvaluating Visual Unsupervised Representation\nLearning\nThe Objective Function Mismatch in unsupervised learning states that the per-\nformance on a desired target task can decrease over the course of training when\nthe pretext task and the target tasks are ill-posed. Some work directly or indirectly\nobserves that learning a pretext task too long may hurt target task performance\nbut makes no further investigations on this topic [41, 144, 174, 175, 261]. Some\nearly work shows performances of linear target models over training epochs but\ndoes not examine or define the objective function mismatch in detail [238, 289].\nInstead, unsupervised multi-task learning, meta-learning, or complementary fea-\ntures approaches have been proposed to lower the objective function mismatch\n[41, 58, 185]. In contrast, in our work [239] presented in chapter 4, we focus solely\non defining simple, general protocols for measuring mismatches from metrics over\nthe course of pretext task training when a target task is trained afterward on top of\nthe pretext model’s representations. To the best of our knowledge, this has not been\ndone before. Furthermore, we highlight important findings and properties of our\nevaluation protocols.\n15\nCHAPTER 2. RELATED WORK\nChanging the underlying model is a common theme when comparing unsuper-\nvised learning techniques [30, 42, 90, 144, 194]. Thereby, the effects of changing\nthe entire model architecture or its parts are examined. Various works observe\nthat scaling the underlying model capacity leads to performance improvements\n[30, 42, 90, 144, 194], which seems crucial for larger dataset sizes [90]. A well-known\nfinding is that a larger representation size consistently increases the quality of the\nlearned visual representations for different target tasks [30, 42, 144]. However, for\nthe joint-embedding framework, it has been argued that a reduced model com-\nplexity through simple architectures like CNNs and regularization can promote\n\"simple\" representations (e.g., smooth) with a lower effective dimension [30]. In\nour work in chapter 4, we also change the underlying model to examine the effects\non the objective function mismatch.\nVarying the amount of data samples leads to interesting observations as well\n[8, 90, 194, 242]. Several works observe that more training data does not necessarily\nlead to target task performance improvements if the underlying model has a low\ncapacity or the pretext task is not suitable [90, 194]. Furthermore, unsupervised\nlearning can improve performance for setups with few labels [194, 242], even for\nsmall datasets [242], but improvements diminish with a growing amount of anno-\ntations [194]. Notably, [8] shows that unsupervised learning is capable of learning\nearly-layer features from a single image.\nAnalyzing self-supervised learning across target tasks is another way to define and\nevaluate benchmarks for unsupervised approaches [58, 68, 90, 98, 98, 174, 194, 261,\n289, 301]. In our work in chapter 4 we compare the objective function mismatch\nacross various target domains with our proposed metrics. Good representations are\noften defined as those that adapt to diverse, unseen tasks with few target examples\n[90, 289]. However, various works show in different setups that the performance\nof unsupervised models is not consistent across different domains and depends\non the pretext task and that there is no best unsupervised method across all target\ntasks [58, 90, 98, 98, 174, 194, 261, 289]. By transferring unsupervised methods from\none domain to various target tasks, it has also been found that no single method\ndominates, but unsupervised methods outperform the supervised baseline in most\ntasks [68, 301]. Similarly, unsupervised visual representations seem to be superior\nin transferring to unseen concepts, e.g., transferring from cat to tiger cat [227].\nOther empirical evaluations of unsupervised representations directly or indirectly\nmeasure target task performance. It has been found that unsupervised learning\ncan improve robustness to adversarial examples as well as input and label corrup-\ntions and can exceed supervised performance in out-of-distribution detection [107].\n16\n2.2. EVALUATING VISUAL UNSUPERVISED REPRESENTATION\nLEARNING\nWith a small subset of ImageNet [56] annotated with sixteen factors of variation like\nbackground or lighting, it has been observed that the examined unsupervised mod-\nels are consistently failing to predict all these factors [122]. This underscores the\nfindings of our preceding work in chapter 4. Furthermore, augmentations can have\na positive and negative effect on the prediction of these factors [98, 122, 239], and\neffects of augmentations can also depend on the unsupervised method [7, 239]. In\naddition, it has been observed that different unsupervised and supervised methods\nlearn somewhat similar representations in middle layers, which vary strongly in\nthe layers near the loss function [93, 98]. With aggregated representations of joint-\nembedding models for fixed-scale image patches, which result in on-par or even\nbetter performance, it has been shown that these models mainly learn a distributed\nrepresentation of image patches containing local information shared among similar\npatches [48]. For joint-embedding methods it has been observed that mini-batch\ntraining is a detrimental prior for learning features of class-imbalanced data that\ncan be utilized to cluster the data uniformly [10]. However, contrastive methods\n(and maybe others) learn more balanced feature spaces compared to supervised\nlearning [135]. This prior can be mitigated by prior-matching [10].\nBy indirectly investigating the disentanglement of unsupervised representation\nwith six metrics, it has been found that none of the examined unsupervised methods\nreliably learn disentangled representations, nor does the disentanglement of these\nrepresentations directly correspond with target task performance [174, 175]. To\ndate, there exists no indirect measurement with a clear correlation between metrics\nscore and the performance on a variety of target tasks [30, 174, 175].\nVisualizing unsupervised representations to understand their inherent structure\nhas recently led to interesting findings [7, 24, 34, 48]. An early work uses three\ntechniques - activation maximization, sampling, and linear combinations of fil-\nters - to visually examine the responses of individual units of stacked denoising\nautoencoders [260] and deep belief networks [67, 110]. It has been found that these\nmodels learn hierarchical representations that combine into meaningfully more\ncomplicated representations in deeper layers [67]. By using feature inversion [257]\nbefore and after the projection head of joint-embedding methods, it has been found\nthat layers closer to the contrastive loss function lose information due to invari-\nances to augmentations [7, 93]. A similar effect has been observed in [98] for other\nmethods. In [24] a diffusion model is conditioned on unsupervised representations\nto visualize them in image space. Based on observations backed up by empirical\nevidence, it has been also observed that projector representations are invariant to\naugmentations used during training. In contrast, the encoder representations con-\ntain contextualized local information [24]. Similar observations have been made in\n[301] and [48]. Another work shows that the self-attention maps of an unsupervised\n17\nCHAPTER 2. RELATED WORK\ntrained transformer relate to class-specific feature maps analogous to segmenta-\ntions [34]. Furthermore, visual observations have indicated that representations\nare more robust to small adversarial attacks and retain more detailed information\n(e.g., scale or color) of the image in a structured manner than supervised ones [34].\n2.3\nUnsupervised Adaptation of Visual Representations\nOur work in chapter 5 relates to unsupervised visual domain adaptation, which has\nbeen extensively studied in recent years [73, 172, 268, 298].\nData generation for sim-to-real lane detection. In recent years, much attention\nhas been paid to lane detection benchmarks in the real world, such as CULane\n[201], TuSimple [255], LLAMAS [19], and BDD100K [285]. Despite the popularity of\nthese benchmarks, there is little research that focuses on sim-to-real lane detection\ndatasets. One work proposed a method for generating synthetic images with 3D\nlane annotations in the open-source engine blender [77]. Their synthetic-3D-lanes\ndataset contains 300K train, 1,000 validation and 5,000 test images, while their\nreal-world 3D-lanes dataset consists of 85K images, which are annotated in a semi-\nmanual manner. Utilizing the data generation method from [77], in [78] 50K labeled\nsynthetic images have been collected to perform sim-to-real domain adaptation\nfor 3D lane detection. At this point, the source domain of the dataset is not pub-\nlicly available. Recently, unsupervised domain adaptation techniques for 2D lane\ndetection have been investigated in [116]. The proposed data generation method\nrelies on CARLA’s built-in agent to automatically collect 16K synthetic images [116].\nHowever, the dataset is not publicly available at this point. In comparison, our\nwork [241] presented in chapter 4 leverages an efficient and configurable waypoint-\nbased agent in CARLA to collect simulation data. Furthermore, in contrast to the\naforementioned works, considering only single-source single-target unsupervised\ndomain adaptation, we additionally focus on multi-target unsupervised domain\nadaptation with data collected from a 1/8th model vehicle and a cleaned version of\nhighway drives from the TuSimple [255] dataset.\nDiscrepancy-based methods employ a distance metric to measure the discrep-\nancy between the source and target domain [176, 244, 299, 308]. A prominent\nexample is DAN [176] which uses maximum mean discrepancies (MMD) [91, 92] to\nmatch embeddings of different domain distributions. DSAN [308] builds upon DAN\nwith local MMD and exploits fine-grained features to align subdomains accurately.\nIn [244] the second-order statistics of source and target distributions are aligned by\nre-coloring whitened source features with the covariance of the target distribution.\n18\n2.3. UNSUPERVISED ADAPTATION OF VISUAL REPRESENTATIONS\nFurthermore, there is work that takes domain-specific statistics into account by\nre-normalizing the model with domain statistics [181, 277] or by training sepa-\nrate batch norm layers for each domain [38]. Other work, for example, minimizes\ndivergences [129, 183], and entropy [237], aligns mutual information [82], uses a\ncontrastive loss [136], or mitigates optimization inconsistencies by minimizing the\ngradients discrepancy of the source samples and target samples [64].\nAdversarial discriminative methods employ a domain classifier or discrimina-\ntor besides the target model on the feature extractor(s), which tries to classify the\ndomain of the input’s representation; this encourages the feature extractor to pro-\nduce domain-invariant representations [4, 40, 75, 76, 237, 256, 265, 277, 296]. In\n[75, 76] the domain classifier is connected with a gradient reversal layer to the\nfeatures extractor, which leads to optimization for features indistinguishable for\nthe domain classifier. Another work employs multiple domain classifiers on early\nblocks of the model to learn domain-informative features and multiple domain\nclassifiers on later blocks with a gradient rehearsal layer to learn domain invariant\nfeatures [296]. In [262] a fine-grained discriminator is used to not only distinguish\nbetween domains but also to distinguish between complex structures by splitting\nthe discriminator output and softening the discriminator’s labels. ADDA [256]\naligns a target feature encoder with a pre-trained, frozen source encoder utilizing\na discriminator. While precedent methods mainly rely on feature-level alignment,\nadversarial generative methods solely [26, 275] or additionally [113] operate on\npixel-level. Our work in chapter 5 combines DANN [76] and ADDA [256] with the\nUFLD [212] method and adopts them for row-based 2D lane detection to evaluate\nthem on the proposed CARLANE benchmark.\nPseudo-labeling-based methods use different strategies to create pseudo labels\nfor the target domains and utilize these labels during training. There are three\ncommon strategies: 1) Directly selecting pseudo labels based on the model output\n[224, 296, 299]. 2) Training separate blocks or layers for pseudo-labeling during\ndomain adaptation [40, 224, 274, 297]. 3) Using a pre-trained model to create the\npseudo labels [38]. In [296] and [4] pseudo labels are selected based on the scores\nof an image classifier and a domain discriminator. Another work iteratively extends\nthe source training set with pseudo-labeled target samples with a high confidence\nscore of the classifier [299]. In [224] the agreement and confidence of two models\nis utilized to estimate target pseudo labels. In [202] and [40] target predictions\nare compared with source prototypes to estimate the pseudo label and a distance\nmeasure or similarity score is used to select the pseudo labels. Another work utilizes\nweighted combinations of soft cluster assignments to create pseudo-labeled virtual\ninstances [297]. In [38] a pre-trained model is used to estimate pseudo labels for\n19\nCHAPTER 2. RELATED WORK\na fully-supervised classifier network, which is utilized to refine the pseudo labels\ninteractively. Inspired by [4] in our work in chapter 5, we propose a pseudo labels\nselection mechanism for row-based 2D lane detection utilizing classifier and dis-\ncriminator scores. Furthermore, we combine SGADA [4] with our pseudo label\nselection mechanism and with the UFLD [212] method for evaluation on our CAR-\nLANE benchmark.\nIndirect feature-matching approaches match features of the source and target\ndomain indirectly, for example, through prototypes [250, 274, 286], decomposed\nfeatures [265], or mirror samples [304]. This avoids variabilities and biases in sam-\npling and class distribution [250]. In [274] source prototypes are directly aligned\nwith target prototypes computed from pseudo-labeled target samples. Instead of\naveraging sample features, another work learns prototypes with a cross-entropy loss\nbetween prototype and source features [250]. PCS [286] creates source and target\nprototypes from memory banks via k-means clustering and utilizes a contrastive\nloss between the sample’s feature and the prototypes for in-domain feature learning.\nFurthermore, the source sample’s features are aligned with target prototypes via a\nsimilarity score for cross-domain feature learning. Another work constructs mirror\nsamples - the ideal counterparts in the other domain - and aligns them across the\ndomains [304]. In [265] source domain features are decomposed to task-related\nfeatures for domain adaptation and task-irrelenvant features. Our work in chapter 5\ncombines PCS [286] with the UFLD [212] method as well as our pseudo label selec-\ntion method and adopts PCS for row-based 2D lane detection.\nSelf-supervised auxiliary tasks are leveraged to improve domain adaptation effec-\ntiveness by capturing in-domain [80, 245, 277] or cross-domain [27, 273, 275, 277]\nstructures. In [80] a simple reconstruction objective is used to learn in-domain\ntarget features. Rotation, flip, and patch location prediction are utilized as auxiliary\ntasks in the source and target domain and can lead to a reduction of the domain gap\n[245]. Another work has assessed different setups for early unsupervised tasks to\nimprove domain adaptation and has found that simple rotation prediction results in\nsignificant performance gains [277]. In [27] shared encoders and private encoders\nare used in an autoencoder setup to learn in-domain and cross-domain features.\nSimilarly, instance discrimination [273] is utilized for in-domain feature learning\ntogether with a cross-domain self-supervision based on similarity to align both\ndomains [139]. Additionally, in-domain contrastive learning is performed between\nthe sample’s features and prototypes in [286] and our work in chapter 5.\nFurthermore, other work, for example, applies a meta-learning scheme between\nthe domain alignment and the targeted classification task [264] or uses attention to\n20\n2.4. UNSUPERVISED TRANSLATION OF VISUAL REPRESENTATIONS\nfocus on relevant source samples [190].\n2.4\nUnsupervised Translation of Visual Representations\nOur work in chapter 6 relates to unpaired image-to-image translation, which is a\nspecial case of unsupervised domain adaptation [268] and has been extensively\nstudied in recent years [6, 203].\nUnpaired image-to-image translation. Following the success of GANs [89], the\nconditional GAN framework [188] enables image generation based on an input\ncondition. Pix2Pix [125] uses images from a source domain as a condition for the\ngenerator and discriminator to translate them to a target domain. Since Pix2Pix\nrelies on a regression loss between generated and target images, translation can\nonly be performed between domains where paired images are available. To achieve\nunpaired image-to-image translation, methods like CycleGAN [307], UNIT [170],\nand MUNIT [120] utilize a second GAN to perform the translation in the opposite\ndirection and impose a cycle-consistency constraint or weight-sharing constraint\nbetween both GANs. However, these methods require additional parameters for the\nsecond GAN, which are used to learn the unpaired translation and are omitted when\ninferring a one-sided translation. In works such as TSIT [130] and CUT [204], these\nadditional parameters are completely omitted at training time by either utilizing a\nperceptual loss [133] between the input image of the generator and the image to\nbe translated or by patchwise contrastive learning. Recently, additional techniques\nhave achieved promising results, like pseudo-labeling [102] or a conditional dis-\ncriminator based on segmentations created with a robust segmentation model for\nboth domains [216]. Furthermore, there are recent efforts to adapt diffusion models\nto unpaired image-to-image translation [243, 270, 300].\nContent consistency in unpaired image-to-image translation. Due to biases be-\ntween unpaired datasets, the content of translated samples can not be trivially\npreserved [216]. There are ongoing efforts to preserve the content of an image\nwhen it is translated to another domain by improving various parts of the training\npipeline: Several consistency constraints have been proposed for the generator,\nwhich operate directly on the translated image [20, 164, 307], on a transformation\nof the translated image [74, 246, 262, 282, 295], or on distributions of multi-modal\ntranslated images [303]. The use of a perceptual loss [133] or LPIPS loss [294]\nbetween input images and translated images, as in [130] and [216], can also be\nconsidered a consistency constraint between transformed images. In [275] content\nconsistency is enforced with self-supervised in-domain and cross-domain patch\n21\nCHAPTER 2. RELATED WORK\nposition prediction. There is work that enforces consistency by constraining the\nlatent space of the generator [120, 170, 233]. Semantic scene inconsistencies can be\nmitigated with a separate segmentation model [164, 282]. To avoid inconsistency\narising from style transfer, features from the generator stream are masked before\nAdaIN [119, 179]. Another work exploits small perturbations in the input feature\nspace to improve semantic robustness [128]. However, if the datasets of both do-\nmains are unbalanced, discriminators can use dataset biases as learning shortcuts,\nwhich leads to content inconsistencies. Therefore, only constraining the genera-\ntor for content consistency still results in an ill-posed unpaired image-to-image\ntranslation setup. Constraining discriminators to achieve content consistency is\ncurrently underexplored, but recent work has proposed promising directions. There\nare semantic-aware discriminator architectures [102, 161, 173, 216] that enforce\ndiscriminators to base their predictions on semantic classes, or VGG discriminators\n[216], which additionally operate on abstract features of a frozen VGG model instead\nof the input images. Training discriminators with small patches [216] is another way\nto improve content consistency. To mitigate dataset biases during training for the\nwhole model, sampling strategies can be applied to sample similar patches from\nboth domains [137, 216]. Furthermore, in [251] a model is trained to generate a\nhyper-vector mapping between source and target images with an adversarial loss\nand a cyclic loss for content consistency. In contrast, our work [240] in chapter 6\nutilizes a robust semantic mask to mask global discriminators with a large field\nof view, which provide the generator with the gradients of the unmasked regions.\nThis leads to a content-consistent translation while preserving the global context.\nWe combine this discriminator with an efficient sampling method that uses robust\nsemantic segmentations to sample similar crops from both domains.\nAttention in image-to-image translation. Previous work utilizes attention for\ndifferent parts of the GAN framework. A common technique is to create attention\nmechanisms that allow the generator or discriminator to focus on important regions\nof the input [5, 140, 248, 280, 292] or to capture the relationship between regions of\nthe input(s) [117, 247, 283]. Other work guides a pixel loss with uncertainty maps\ncomputed from attention maps [249], exploits correlations between channel maps\nwith scale-wise channel attention [247], disentangles content and style with diago-\nnal attention [149], or merges features from multiple sources with an attention block\nbefore integrating them into the generator stream [171]. In [167], an attention-based\ndiscriminator is introduced to guide the training of the generator with attention\nmaps. Furthermore, ViTs [61] are adapted for unpaired image-to-image translation\n[254, 306], and the computational complexity of their self-attention mechanism is\nreduced for high-resolution translation [306]. In contrast, our work in chapter 6\nproposes an attention mechanism to selectively integrate statistics from the content\n22\n2.4. UNSUPERVISED TRANSLATION OF VISUAL REPRESENTATIONS\nstream of the source image into the generator stream. This allows the model to\nfocus on statistical features from the content stream that are useful for the target\ndomain.\n23\nPart I\nUnsupervised Learning of\nVisual Representations\n3 Self-Organizing Convolutional Neural Net-\nworks\nIn this chapter, we combine Convolutional Neural Networks (CNNs), clustering\nvia Self-Organizing Maps (SOMs), and Hebbian learning to propose the building\nblocks of Convolutional Self-Organizing Neural Networks (CSNNs), which learn\nrepresentations in an unsupervised and backpropagation-free manner. Our ap-\nproach replaces the learning of traditional convolutional layers from CNNs with\nthe competitive learning procedure of SOMs and simultaneously learns local\nmasks between these layers with separate Hebbian-like learning rules to mitigate\nthe problem of disentangling factors of variation when filters are learned through\nclustering. We investigate the learned representation by designing two simple\nmodels with our building blocks, achieving comparable performance to many\nmethods which use backpropagation. Furthermore, we reach comparable perfor-\nmance on Cifar10 and give baseline performances on Cifar100, Tiny ImageNet,\nand a small subset of ImageNet for backpropagation-free methods.\n3.1\nMotivation\nA well-known downside of many successful deep learning approaches like backpro-\npagation-based CNNs is their need for large, labeled datasets. Obtaining these\ndatasets can be costly and time-consuming, or the required quantity of samples is\nunavailable due to restrictive conditions. In fact, the availability of big datasets and\nthe computational power to process this information are two of the main reasons\nfor the success of deep learning techniques [146]. This raises the question of why\nmodels like CNNs need that much data: Is it the huge amount of model parameters,\nare training techniques like backpropagation [168, 222, 266] just too inefficient, or\nis the cause a combination of both?\nThe fact that neural networks have become both large and deep in recent years,\noften consisting of millions of parameters, suggests that the cause is a combination\nof both. On the one hand, large amounts of data are needed because there are so\nmany model parameters to optimize. On the other hand, the data may not yet be\noptimally utilized. The suboptimal utilization of data to date could be related to\nthe following problem, which we call the Scalar Bow Tie Problem: Most training\n27\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nLearner        \n        Problem\nFeedback Value \nFigure 3.1: The Scalar Bow Tie Problem. With this term, we describe the current\nproblematic situation in deep learning in which most training approaches for neural\nnetworks and agents depend only on a single scalar loss or feedback value. This\nsingle value is used to subsume the performance of an entire neural network or\nagent, even for very complex problems.\nstrategies for neural networks rely just on a single scalar value (the loss value) which\nis computed for each training step and then used as a feedback signal for optimizing\nmillions of parameters. In contrast, humans utilize multiple feedback types to\nlearn, for example, different sensor modalities when learning representations. With\nthe Scalar Bow Tie Problem, we refer to the current situation for training neural\nnetworks. Similar to a bow tie, both ends of the learning setting (the learner and the\ntask to be learned) are complex, and the only connection between these complex\nparts is one single point (the scalar loss or feedback value).\nThere exists a current effort of backpropagation-based methods to overcome\nthe Scalar Bow Tie Problem, e.g., by using multiple loss functions at different output\nlocations of the network (a variant of multi-task learning, i.a. [58]). Furthermore,\nthere exists a debate to which degree backpropagation is biologically plausible on\na network level (e.g., [22, 162, 267]), which often highlights that layer-dependent\nweight updates in the whole model are less likely than more independent updates,\nlike layer-wise ones, and beyond that would require an exact symmetric copy W T\nof the upstream weight W . In [96] this problem is described in detail and referred\nto as the weight transport problem.\nIn this chapter we do not aim at solving all these problems, but want to take a\nsmall step away from common deep learning methods while keeping some benefits\nof these models, such as modularity and hierarchical structure.\n3.2\nContributions\nWe propose a CNN variant with building blocks that learn in an unsupervised, self-\norganizing, and backpropagation-free manner. Within these blocks, we combine\nmethods from CNNs [121, 155], SOMs [143], and Hebbian learning [106].\n28\n3.3. CSNN\nOur main contributions are as follows:\n• We propose an unsupervised, backpropagation-free learning algorithm that\nutilizes two learning rules to update the weights layer-wise without using an\nexplicitly defined loss function, thereby reducing the Scalar Bow Tie Problem.\n(RQ-L1)\n• This learning algorithm is used to train CSNN models, with which we achieve\ncomparable performance to many models trained in an unsupervised manner.\n(RQ-E1)\n• We overcome a fundamental problem of SOMs trained on image patches by\npresenting two types of weight masks to mask input and neuron activities.\n(RQ-L1)\n• We propose a multi-headed version of our building blocks to further improve\nperformance. (RQ-L2)\n• To the best of our knowledge, our approach is the first to combine the suc-\ncessful principles of CNNs, SOMs, and Hebbian learning into a single model.\n3.3\nCSNN\nThis section describes the key methods of CSNNs. The proposed blocks for each\nmethod and their interactions are summarized in Figure 3.2.\n3.3.1\nConvolutional, Self-Organizing Layer\nThe convolutional, self-organizing layer (sconv) closely follows the convolutional\nlayer of standard CNNs. The entire input is convolved patchwise with each learned\nfilter:\nym,n,i = pm,n ·wi\n(3.1)\nwhere pm,n is the image patch centered at position m,n of the input, wi is the i-th\nfilter of the layer, and ym,n,i is the activation at position m,n,i of the output. The\nonly difference to a standard convolutional layer is that the weights are learned\nthrough a competitive, self-organizing optimization procedure. The layer’s outputs\nfor one image patch are the distances for each neuron in a SOM. Each neuron of the\nSOM corresponds to one filter in a CNN, and each distance output of a SOM neuron\ncorresponds to a specific feature ym,n,i of the feature map i for the patch m,n after\nthe convolution operation was applied. The main difference to a standard CNN\nis that the CSNN only uses local learning rules. Furthermore, CSNNs are trained\n29\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nfor each input patch\nsconv \nmask\ninput\nconcat\nh\nconcat\nneuron mask\ni\ncompetitive\nlearning rule\nlocal learning rule\nBMU\nneuron conv\npatch distances\noutput block\nFigure 3.2: Architecture of a CSNN layer. (left) mask and sconv layer computations\nfor a single patch: First, each patch is masked for each neuron i. The masked\noutputs and the input patch are subsequently used in the local learning process.\nSecond, the sconv layer computes the convolution distances and the BMU. The\nBMU and the input patch are subsequently used in competitive learning. The\ndistances are concatenated to the patch’s spatial activation vector. Analogous to\nCNNs, the spatial activation vectors of all patches form the convolutional output\nblock. (right) Multi-Head masked sconv: Multiple masking and sconv layers h are\napplied to the entire input, and the resulting output blocks are concatenated in the\nfeature dimension.\nunsupervised in a bottom-up manner, in contrast to the top-down supervised\nbackpropagation approach. Analogous to CNNs, the proposed modular layers can\nbe combined with other layers to form deep learning architectures.\n3.3.2\nCompetitive Learning Rule\nMost learning rules for SOMs require a best matching unit (BMU) to compute the\nweight change ∆w. Since we use a convolution as a distance metric, the index of\nthe BMU for one patch is defined as:\ncm,n = argmax\ni\n{ym,n,i}\n(3.2)\nwhere cm,n is the index of the BMU in a 2D SOM-grid for patch m,n.\nTo allow for self-organization, learning rules of SOMs require a neighborhood\nfunction to compute the neighborhood coefficients from all the other neurons in\n30\n3.3. CSNN\nthe grid to the BMU. A common type of neighborhood function is the Gaussian:\nhm,n,i(t) = exp(\n−dA(km,n,cm,n,km,n,i)2\n2δ(t)2\n)\n(3.3)\nwhere km,n,cm,n and km,n,i are the coordinates of the BMU and the i-th neuron in\nthe SOM grid, and δ(t) is a hyperparameter to control the radius of the Gaussian,\nwhich could change with the training step t. For the distance function dA, we utilize\nthe Euclidean distance between the BMU and the i-th neuron. Then hm,n,i(t) is the\nneighborhood coefficient of neuron i to the center (the BMU) cm,n for the patch\nm,n. Now the weight update for one patch can be defined as:\n∆wm,n,i(t) = a(t)hm,n,i(t)pm,n\n(3.4)\nwm,n,i(t) =\nwm,n,i(t)+∆wm,n,i(t)\n°°wm,n,i(t)+∆wm,n,i(t)\n°°\n(3.5)\nwhere ∥...∥is the Euclidean norm used to obtain the positive effects of normalization,\nsuch as preventing the weights from growing too much, and a(t) is the learning rate,\nwhich could change over the course of the training. For an entire image, the final\nweight change for the weight vector of a SOM neuron is calculated by averaging\nover all patch weight updates:\n∆wi(t) = a(t)\nmn\nX\nm,n\nhm,n,i(t)pm,n\n(3.6)\nIn batch training, the average of all patches in a batch is taken.\nUsing other distance metrics (such as the L1 or L2 norm) may require changing\nthe BMU computation (e.g., to argmin) and the learning rule.\n3.3.3\nMask Layers\nWe argue that given the learning rule 3.6, a SOM neuron is unable to disentangle\nfactors of variation, which is a key property of deep learning architectures [88]. In\nother words, the SOM neuron cannot pay more or less attention to a part of the\ninput or ignore that part entirely. The SOM simply tries to shift its neurons to best\nfit the dataset. This can lead to poor performance because a neuron is unable to\nlearn abstract features that can be considered concepts. Even at higher layers, a\nSOM neuron processes as input only the output from a collection of SOM neurons\nof the previous layer in its receptive field. Therefore, the network forms a hierarchy\nof collections rather than a hierarchy of concepts. Furthermore, a neuron in a\ndeeper layer with a small receptive field, e.g., 3×3, needs to compute the distance\n31\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nkw\ni\nkh\nd\npatch\nmask between neurons\nelem. mul\ninput mask\nmasked patch\na)\nb)\nFigure 3.3: CSNN mask types. a) Input mask: The Hadamard product between\nan input patch and a mask of the same size. b) Mask between layers: The mask,\nwith its length equal to the number of filters in the previous layer, is multiplied\nelement-wise with every spatial activation vector from the patch.\nbetween its 3×3×256 vector and the input patch if there are 256 SOM neurons in\nthe previous layer. Since the weight update shifts all the values of the weights from\nthe BMU and its neighbors in a direction to the input at once, it seems unlikely that\nthe SOM neurons learn distinct enough features for data with a high amount of\ninformation, such as images. This argument goes hand in hand with another work,\nwhere similar observations were made for k-means clustering [65].\nTo allow the network to focus on certain parts of the input, we propose two\ntypes of separately-learned mask vectors, which are shown in Figure 3.3. The first\nmask (a) enables each neuron to mask its input values with a vector of the same\ndimension as the input patch and the SOM neuron’s weight vector. Each CSNN\nneuron multiplies the image patch with its mask and convolves the masked patch\nwith the weight vector of the SOM neuron:\nˆym,n,i = pm,n ◦mi\n(3.7)\nym,n,i = ˆym,n,i ·wi\n(3.8)\nwhere mi is the mask of neuron i, ˆym,n,i is the masked patch for neuron i, and ◦\ndenotes the Hadamard product.\nThe second mask (b) is located between the neurons of two layers. Therefore, it\nrequires kw ×kh fewer parameters than the first (input) mask when kw and kh are\n32\n3.3. CSNN\nthe kernel’s sizes. The mask is defined as:\nˆymh,nw,i = pmh,nw ◦ni\n(3.9)\nwhere ni is the mask element-wise multiplied in the feature (depth) dimension\nkw ×kh times. With this mask, we want to enable each SOM neuron to learn from\nwhich neuron of the previous layer to receive information. Mask (a) is used for the\ninput and mask (b) between each sconv layer.\nWe note that we still utilize the unmasked input for training the SOM weight vectors\n- the mask is only used to calculate the convolutional distances and to determine\nthe BMU. Moving the BMU and its neighbors to the unmasked input direction\nleads to a more stable learning process since the self-organization does not depend\ntoo much on the higher mask coefficients. High mask coefficients could drive the\nweight vector of a single SOM neuron into a direction from which it can hardly\nadapt to other new but similar input patches. Thereby the neuron could get stuck\nin a dead end. Furthermore, this allows the mask to learn new regions typical for\na particular input type, even when the mask tends to ignore these regions of the\ninput. This is because the SOM moves towards the unmasked input, which enables\nthe mask to increase its corresponding coefficients.\n3.3.4\nLocal Learning Rules\nTo learn the input mask of each neuron, we propose two simple local learning\nrules inspired by the Generalized Hebbian Algorithm [226] (GHA). These rules\nshow good results in our experiments, but there are certainly other learning rule\nalternatives that could guide the training process of the masks. Below we derive\nour final learning rules by discussing the following equations within the context of\nCSNNs, where e indicates that we present the element-wise form of the learning\n33\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nrules. For the original inviolate versions we refer to the literature.\n∆he(pm,n, ˆym,n,i) = ˆym,n,i ◦pm,n\n(3.10)\n∆oe(pm,n, ˆym,n,i,mi) = ˆym,n,i ◦(pm,n −ˆym,n,i ◦mi)\n(3.11)\n∆mes(pm,n, ˆym,n,i,mi) = he([pm,n −γ\nX\nk\nˆym,n,k ◦mk], ˆym,n,i,mi)\n(3.12)\n∆mec(pm,n, ˆym,n,i,mi) = oe([pm,n −γ\nX\nk<i\nˆym,n,k ◦mk], ˆym,n,i,mi)\n(3.13)\n∆mi(t) = a(t)\nmn\nX\nm,n\n[∆mes(pm,n, ˆym,n,i,mi)]\n(3.14)\n∆ni(t) =\na(t)\nmnhw\nX\nm,n,h,w\n[∆mes(pmh,nw , ˆymh,nw,i,ni)]\n(3.15)\nEquation 3.10 describes simple Hebbian learning in vector notation. Equation 3.10\napplies the principle “neurons that fire together, wire together” where pm,n is the\npresynaptic neuron activities vector and ˆym,n,i is the postsynaptic neuron activities\nvector. This principle per se is useful for mask learning, because the procedure\ncould find mask weights, which indicate the connection strengths between mask\ninput and output. However, Hebb’s rule is unstable. If there are any dominant\nsignals in the network - and in our case this is likely due to BMUs - mask weights\nrapidly approach numerical positive or negative infinity. In fact, it has been shown\nthat the instability of Hebbian learning accounts for every neuron model [210].\nEquation 3.11, known as Oja’s rule [196], tries to prevent this problem through\nmultiplicative normalization, where the resulting additional negative term can be\nseen as the forgetting term to control weight growth. This restricts the magnitude\nof weights to lie between 0 and 1.0, where the squared sum of weights tends to\n1.0 in a single-neuron fully-connected network [196]. For that reason, using Oja’s\nrule in our case would lead to mask weights approaching 1.0, since each mask\nweight has its own input and output due to the element-wise multiplication and is\ntherefore independent of all other mask weight updates — each mask weight forms\na one-weight, single-neuron fully-connected network. To prevent this tendency\ntowards 1.0, we make the mask weight updates dependent on each other.\nThe GHA makes its updates dependent on each other through input modifi-\ncation to approximate eigenvectors for networks with more than one output. In\ncontrast to Oja’s rule, which approximates the first eigenvector for networks with\nonly one output, GHA performs a neural PCA [226]. However, in [226] it has been\nshown that the algorithm is equivalent to performing Oja learning using a modified\nversion of the input; a particular weight is dependent on the training of the other\nweights only through the modifications of the input. If Oja’s algorithm is applied\n34\n3.3. CSNN\nto the modified input, it causes the i−th output to learn the i-th eigenvector [226]\nbut in our case, each mask weight has its own individual input-output pair, and the\nupdate of this weight is independent of other mask weights in the same mask since\na single mask weight does not see its surroundings.\nTherefore, in Equation 3.12 we use a modified input patch for mask i by simply\nsubtracting the input patch from the sum over masked outputs, which sums the\nfiltered information of all masks when k iterates over all masks. Similar to the\nGHA, we additionally multiply the mask with the output before summing up, which\nleads to a normalization effect and drives the mask coefficients to the initialization\ninterval [−1,1]. Now each mask update tries to incorporate the missing input\ninformation in the output in a competitive manner, where the mask weight growth\nis restricted. This can be seen as a self-organization process for the masks. In our\nexperiments we show that due to the summation over all masks the updates are\nstable enough to use Hebbian learning (Equation 3.10) to learn the masks, which\nsaves computation time.\nIn Equation 3.13 we subtract the sum of all k < i masked postsynaptic neuron\nactivity vectors from the presynaptic neuron activity vector. Therefore, each next\nmask mi sees an output from which the filtered information of the previous masks\n{m1,...,mi−1} in the grid is subtracted. The hyperparameter γ controls how much\ninformation we want to subtract from the input. A smaller γ < 1.0 value enables the\nmasks to share more input information.\nEquation 3.14 shows the final update formula for the input masks (a), where we\ncompute the mean over every patch pm,n. For the second mask type (b), the final\nupdate formula is shown in Equation 3.15, where we instead update the mask ni\nwith the mean over each spatial activation vector pmh,nw of every patch. Experi-\nments show that the update rules 3.14 and 3.15 lead to slightly better performance\nfor smaller models when using the input modification of Equation 3.13 and γ = 0.5.\nHowever, Equation 3.13 requires Oja’s rule and sometimes larger neighborhood\ncoefficients - especially when γ is close to 1.0 - because of the unequal learning\nopportunities of the masks, which can lead to poorer efficiency. With the other\ninput modification (Equation 3.12) and γ = 1.0, we achieve better performance for\nour deeper models.\nFurthermore, we want to note that the mask weights are initialized uniformly in\n[−1,1] in our experiments to encourage the model to learn negative mask weights,\nwhich allow the masks to flip their input. For stability reasons, it is recommended\nthat the input is normalized. It is also possible to multiply mi by hm,n,i(t) to obtain\nsimilar masks for neighboring neurons, but experiments show that the resulting\nloss in diversity decreases performance. Therefore we only update the BMU mask.\n35\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\n3.3.5\nMulti-Head Masked SConv-Layer\nTo further improve the performance of our models, we use multiple smaller SOM\nmaps per layer instead of one big SOM map per layer. For the output of a layer, the\noutputs of the SOMs are concatenate in the feature dimension. We use multiple\nmaps for multiple reasons: First, the neuron’s mask should lead to specialization,\nand in combination with the self-organization process, a single SOM is possibly\nstill unable to learn enough distinct features. Second, multiple smaller maps seem\nto learn more distinct features more efficiently than scaling up a single map. This\napproach is flexible; experiments show similar performance between SOMs using 3\nbig maps or 12 small maps per layer. To further increase diversity, we update only\nthe best BMU of all maps per patch. This could result in maps that never update or\nmaps that stop updating too early during training. We do not use a procedure to\nprevent these dead maps, as we have not had any problems with these during our\nexperiments. Furthermore, dead maps can be prevented to some extent by scaling\nup the coefficient of the neighborhood function 3.3. It is a task for future research to\ninvestigate dead maps and whether the use of multiple SOMs in the CSNN model\nresults in redundant information\nAll the presented learning methods can be applied layer-wise. On the one hand, this\nbrings benefits like the opportunity to learn big models by computing layer by layer\non a GPU, and more independent weight updates compared to gradient descent,\nwhich reduces the Scalar Bow Tie Problem. On the other hand, this independence\ncould lead to fewer “paths” through the network, reducing the capability of the\nnetwork to disentangle factors of variation. Examining this problem by creating and\ncomparing layer-wise dependent and independent learning rules is an interesting\ndirection for future research, and some work has already been done (e.g., [162]).\n3.3.6\nOther Layers and Methods\nSince we are in the regime of convolutional deep learning architectures, there are\nplenty of other methods we could test within the context of our modules. In our\nexperiments, we investigate batch normalization [124] (with no trainable parame-\nters) and max pooling [146]. In addition, SOMs are well studied, and there are many\nimprovements over the standard SOM formulation that could be explored in future\nwork (e.g., the influence of different distance metrics).\n36\n3.4. EXPERIMENTS\n3.4\nExperiments\nTo assess our proposed methods, we design two CSNN models for our experiments.\nDuring the training of these models, no additional strategies, such as learning rate\nschedules or regularization, are utilized. We simply normalize the datasets to zero\nmean and unit variance. Our implementation is in TensorFlow and Keras and is\navailable at https://github.com/BonifazStuhr/CSNN.\n3.4.1\nDatasets\n(1) Cifar10 [145], with its 32 × 32 images and 10 classes, is used to ablate the im-\nportance of the proposed building blocks. We split the validation set into 5000\nevaluation and test samples.\n(2) Cifar100 [145], with its 32 × 32 images and 100 classes, is used to test the ca-\npability of our representation to capture a higher number of classes. We split the\nvalidation set into 5000 evaluation and test samples.\n(3) Tiny ImageNet [154], with its 64×64 images and 200 classes, is used to investigate\nthe capability of our models to learn on a dataset with a higher number of more\ncomplex classes.\n(4) SOMe ImageNet is a subset of ImageNet [56] containing 10100 training, 1700\nevaluation, and 1700 test samples of 10 classes to test our performance on larger\n256×256 images. We refer to our implementation for details about this dataset.\n3.4.2\nEvaluation Metrics\nQuantitative evaluation. We use the linear and nonlinear evaluation protocols\nto evaluate our learning rules, building blocks, and models. Thereby, we use the\nrepresentations of the frozen models to train linear and nonlinear classifiers for\nthe target task. Additionally, we train a few-shot classifier on our frozen models\nto test the few-shot capabilities of the learned representations. We test general-\nization capabilities by transferring frozen CSNN models trained on one dataset to\nanother by training linear, nonlinear, and few-shot classifiers for the other dataset.\nTo further evaluate our models, we compute the batch-wise neuron utilization - the\npercentage of used neurons per batch. In addition, we provide random (R) baseline\nperformances for our models. At this point, we want to highlight the importance of\nrandom baselines since it has been shown that models with random weights can of-\nten achieve performances just a few percent worse than sophisticated unsupervised\n37\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nlearning methods (e.g., [229]). Furthermore, we prove the stability of our learning\nprocesses by reporting 10-fold cross-validation results.\nQualitative evaluation. To visually examine the capabilities of our representa-\ntions, we train a decoder on the frozen CSNN models to reconstruct the input.\nFurthermore, we compare the average representations for each class by visualizing\nthe CSNN layers in image space. In addition, we examine neuron activities with\nrespect to specific classes of the input image by converting the weights of BMUs for\neach input patch into image space for different CSNN layers during training.\n3.4.3\nModel Architectures\n(1) S-CSNN. Our small model consists of two CSNN-layers, each is followed by a\nbatch normalization and a max-pooling layer to halve the spatial dimension. The\nfirst layer uses a 10×10×1 SOM grid (1 head with 100 SOM neurons), stride 2×2,\nand input masks (Equation 3.14) the second layer a 16×16×1 SOM grid, stride 1×1\nand masks between neurons (Equation 3.15). Both layers use a kernel size of 3×3\nand the padding type “same”.\n(2) D-CSNN. Our deeper model consists of three CSNN-layers, each is followed\nby a batch normalization and a max-pooling layer to halve the spatial dimension.\nThe first layer uses a 12 × 12 × 3 SOM grid and input masks (Equation 3.14), all\nremaining layers use masks between neurons (Equation 3.15). The second layer\nconsist of a 14×14×3 SOM grid, and the third layer of a 16×16×3 SOM grid. All\nlayers use a kernel size of 3×3, a stride of 1×1, and the padding type “same”. For\nTiny ImageNet the layer 1 stride is 2×2 and for SOMe ImageNet the layer 1 and 2\nstrides are 3×3 to keep the representation sizes the same.\n3.4.4\nTraining and Evaluation Setup\nTraining. We set the learning rate of the SOMs to 0.1 and the learning rate of the\nlocal mask weights to 0.005 for all layers. The neighborhood coefficients for each\nlayer are set to (1.0,1.25) for the S-CSNN and (1.0,1.5,1.5) for the D-CSNN. The\nindividual layers are learned bottom up, one after another, since its easier for deeper\nlayers to learn, when the representations of the previous layers are fully learned.\nThe training steps for each individual layer can be defined in a flexible training\ninterval.\nClassifiers. Our nonlinear classifier is a three-layer multilayer perceptron MLP\n(512, 256, num-classes) with batch normalization and dropout (0.5,0.3) between\n38\n3.4. EXPERIMENTS\n0\n100\n101\n102\n103\n104\n105\nTraining Steps per Layer\n50\n55\n60\n65\n70\n75\n80\nAccuracy [%]\nCifar10\nS2-CSNN-B1\nS2-CSNN-B32\nS-CSNN-B1\nS-CSNN-B32\nS-CSNN-B128\nD-CSNN-B1\nD-CSNN-B4\nD2-CSNN-B1\n0\n100\n101\n102\n103\n104\n105\nTraining Steps per Layer\n36\n38\n40\n42\n44\n46\n48\n50\nCifar100\nFigure 3.4: Accuracies of our models trained with our local mask learning rules (S/D\nfor Equation 3.12 and S2/D2 for Equation 3.13) and different batch sizes (B). Every\npoint corresponds to the test dataset accuracy of a nonlinear classifier that has been\ntrained on the representation of the CSNN for every sample in the training dataset.\nThe layers of the CSNN have been trained layer-wise for the steps shown on the\nx-axis. Best viewed in color.\nthe layers. Our linear and few-shot classifier is simply a fully-connected layer (num-\nclasses). All classifiers use elu activation functions between layers and are trained\nusing the Adam optimizer [141] with standard parameters and a batch size of 512.\nTo infer the results on the test set, we use the model of the training step where the\nevaluation accuracy is the best (pocket algorithm approach).\n3.4.5\nObjective Function Mismatch\nFigure 3.4 shows that all models reach their accuracy peak early in training. Since\nthis peak is often reached before seeing the entire training set and the models\nshow ongoing SOM weight convergence (see Figure A.2 of Appendix A), it seems\nlikely that the decrease in performance is due to an objective function mismatch\nbetween the self-organization process and the objective function to learn features\nfor classification. This is a common problem in unsupervised learning techniques\n[185]. On the positive side, it could be argued that our models require fewer samples\nto learn representations (e.g., 2000 samples (≈.3%) from Cifar10 for the D-CSNN-B1\npeak). Even more surprising is the significant jump in accuracy (by up to 62%)\nafter the models have only seen 10 examples. This could be a positive effect of\n39\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nreducing the Scalar Bow Tie Problem. However, we would also like to point out that\nbetter initialization and no dropout in the nonlinear classifier lead to better initial\nperformance. Furthermore, we can see that the mask learning rule 3.12 is superior\nin this setting for the deeper model and vice versa.\nTable 3.1: Ablation study on the Cifar10 dataset.\nMethod\nModel\nCifar10\nMethod\nModel\nCifar10\nlinear\nnonlinear\nlinear\nnonlinear\nLearning rule ablations:\nw/ M\nS\n66.43+0.96\n−1.02\n72.79+0.59\n−0.84\nw/ M\nD\n72.66+0.40\n−1.07\n77.21+0.62\n−0.56\nw/ M\nS2\n66.18+1.17\n−0.95\n72.82+1.02\n−1.22\nw/ M\nD2\n71.74+0.99\n−1.42\n76.18+0.77\n−1.61\nw/ RM\nS2\n62.89\n70.42\nw/ RM\nD\n69.86\n73.69\nw/ NOM\nS2\n56.84\n14.83\nw/ NOM\nD\n65.53\n20.22\nw/o M\nS2\n40.37\n22.32\nw/o M\nD\n43.33\n12.13\nRS w/ M\nS2\n65.20\n67.75\nRS w/ M\nD\n69.69\n69.74\nRS w/o M\nS2\n54.64\n15.32\nRS w/o M\nD\n61.86\n16.01\nRS w/ RM\nS2\n56.42+1.17\n−1.46\n52.23+2.16\n−2.48\nRS w/ RM\nD\n63.36+1.28\n−1.34\n13.44+5.25\n−3.22\nAugmentation ablations:\nw/ aug\nS2\n66.32\n76.90\nw/ aug\nD\n73.73+1.95\n−1.17 80.03+1.52\n−1.05\nSOM ablations:\n1×SOM\nD\n69.69\n71.86\n12×SOM\nM\n71.28\n75.77\n2×SOM\nD\n72.05+0.67\n−0.99\n76.11+0.49\n−0.57\nw/ a.u.\nD\n73.14+0.73\n−0.70\n76.88+0.59\n−0.90\nOther ablations:\nw/ M n.u.\nD\n71.70\n75.08\nw/o bn\nD\n71.67\n76.16\n3.4.6\nAblation Study\nIn Table 3.1, we show the influence on target task performance of the proposed\nlearning rules and building blocks for Cifar10.\nLearning rule ablations. The first two rows (w/ M) show the performance of our\nsmall models (S/S2) and deep models (D/D2) with fully trained SOMs and masks.\nWe observe that the deeper model outperforms the smaller model and that the\nHebbian mask learning rule 3.12 (S/D) slightly outperforms the Oja mask learning\nrule 3.13 (S2/D2). When we successively remove learning procedures from the small\nand big models, we observe the following: Not training the mask and initializing\nthem randomly at the beginning of training (w /RM) or treating them as random\nnoise for each training step (w /NOM) decreases performance notably. We gener-\nated these mask from a uniform distribution in [−1,1]. However, when we train the\nmodel completely without masks (w/o M), the performance decreases significantly\n40\n3.4. EXPERIMENTS\ncompared to the models with trained and untrained masks. This reassures the argu-\nmentation that SOM convolutional filters may need masks to focus on or to ignore\ncertain parts of the input - even adding untrained masks leads to performance\nimprovements.\nWhen the SOM remains untrained, but the masks are trained (RS w/ M), we\nobserve a performance decrease as well compared to our fully trained model (w/\nM). This shows that our SOM convolutional filters can learn useful features during\ntraining. When the SOM weights remain untrained and no masks are used (RS w/o\nM), the performance decreases further, again showing the importance of the masks.\nOur overall learning increases accuracy by about 10% for the linear classifier\nand 20-65% for the nonlinear classifier compared to the uniform (in [−1,1]) ran-\ndom baseline with randomly initialized masks (RS /w RM). The low accuracy of\nthe nonlinear classifier for random weights can be attributed to the use of dropout.\nFurthermore, the overall learning increases accuracy by about 20-26% for the linear\nclassifier and 50-65% for the nonlinear classifier compared to the model without\nmasks but with trained SOM convolutional filters (w/o M). Without masks, the SOM\nlearning procedure results in a performance decrease compared to the random\nbaseline (RS /w RM). Only when utilizing random masks, noise masks, or learned\nmasks does SOM learning provide an advantage to the overall training process for\nour simple CNN-like architecture that does not use further clustering-specific or\nSOM-specific tricks.\nAugmentation ablations. Using data augmentation (rotations, shifts, and hori-\nzontal flips) during training to create a larger dataset of representations for the\nlinear and nonlinear classifiers increases performance in most cases (w/ aug).\nSOM ablations. Increasing the number of SOM maps (1×SOM to 2×SOM for model\nD) improves performance. Furthermore, we can construct a smaller model with 12\nSOM maps per layer that results in comparable performance to our larger model\nwith two SOM maps. Analogous to [144], we observe that increasing the representa-\ntion size improves performance, but in contrast, the classifier does not take longer\nto converge on larger representation sizes in our case. In addition, we find that\nin contrast to bottom-up training, updating all maps per step (w/ a.u.) increases\nperformance slightly for the linear classifier and decreases performance slightly for\nthe nonlinear classifier.\nOther ablations. When we not only update the BMU masks but also update neigh-\nboring masks, we observe a performance decrease. Again SOM-neuron-specific\nmasks seem to be crucial. Not using batch normalization (w/o bn) also decreases\nperformance. We note that for deeper models, batch normalization is necessary to\n41\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nTable 3.2: Quantitative comparison with previous work. With BH, we refer to binary\nhashing, with H to histograms, and with W to whitening.\nMethod\nCifar10\nCifar100\nTiny ImageNet\nlinear\nnonlinear nonlinear\nnonlinear\nBackpropagation:\nVAE [112]\n54.45 (SVM)\n60.71\n37.21\n18.63\nBiGAN [112]\n57.52 (SVM)\n62.74\n37.59\n24.38\nDIM(L) [112]\n64.11 (SVM)\n80.95\n49.74\n38.09\nAET [291]\n83.35 (FC)\n90.59\n-\n-\nBackpropagation-free:\nK-means Triangle+W [49] 79.60 (SVM)\n-\n-\n-\nSOMNet+BH+H [100]\n71.81 (SVM)\n-\n-\n-\nD-CSNN-B1 (ours)\n73.73 (FC)\n77.83\n49.80\n3.56\nD-CSNN-B1-Aug (ours)\n75.68 (FC)\n81.55\n52.66\n5.20\nprevent too large values in deeper layers (exploding weights).\n3.4.7\nComparison to the State of the Art\nTable 3.2 shows the performance of our best models compared to other methods.\nFor few-shot learning, 50 samples per class are used. The CSNN models used for\nCifar100, Tiny ImageNet, and SOMe ImageNet classification have not been tuned for\nthese datasets and have been trained with the same hyperparameters as our Cifar10\nmodel (except the training steps for Cifar100). We show that the performance of\nour D-CSNN-B1 model is comparable to many state-of-the-art methods at the time\nof publication. However, our models lack performance for Tiny ImageNet. One\npossible explanation could be that more complex datasets may require deeper\nmodels or improved learning techniques. Overall, we achieve an improvement over\nthe SOMNet baseline without any additional architectural tricks such as binary\nhashing, histograms, or whiting. Furthermore, it should be noted that we report\nlinear performance with a simple, fully-connected layer instead of an SVM [51] to\nfollow the recent trend. SVMs are considered linear models, but they can also solve\nnonlinear problems using the kernel trick [51].\n3.4.8\nGeneralization\nTo test generalizability, we train models on one dataset and retrain the classifiers\nfor each dataset to which we transfer the models. Following that, we report the test\nset performance of the dataset for which we transferred the models. As shown in\nTable 3.3, our models achieve surprising generalization capability, where differences\n42\n3.4. EXPERIMENTS\nTable 3.3: Quantitative results of models transferred to other datasets. (top rows)\nPerformance of the models trained directly on the target datasets. (bottom rows)\nPerformance on models trained on Cifar10 and Cifar100 transferred to the other\ntarget datasets by re-training the classifiers for each target dataset.\nMethod\nCifar10\nCifar100\nTiny ImageNet\nSOMe ImageNet\nfs(50) linear nonlinear fs(50) linear nonlinear fs(50) linear nonlinear fs(50) linear nonlinear\nD-CSNN-B1\n41.59\n73.73\n77.83\n27.29\n45.17\n49.80\n13.70\n13.52\n3.56\n68.92\n70.69\n49.99\nD-CSNN-B1-Aug\n-\n75.68\n81.55\n-\n46.82\n52.66\n-\n13.32\n5.20\n-\n70.80\n75.29\nD-CSNN-B1-Cifar10\n41.59\n73.73\n77.83\n27.96\n45.68\n49.70\n13.46\n14.36\n2.03\n47.65\n65.34\n65.59\nD-CSNN-B1-Cifar100\n41.43\n71.60\n76.46\n27.29\n45.17\n49.80\n13.72\n13.57\n1.61\n45.68\n63.79\n65.13\nin accuracy between the model trained and the model transferred to the dataset lie\nin the low percentage range. Furthermore, we observe good performance on the\nlarger 256×256 SOMe ImageNet images.\n0\n2000\n4000\n6000\nTraining Steps per Layer\n0.2\n0.4\n0.6\n0.8\nNeuron Utilization [%]\nD-CSNN-B1\nL1\nL2\nL3\n0\n5000\n10000\nTraining Steps per Layer\n0.2\n0.4\n0.6\n0.8\nD-CSNN-B4\nL1\nL2\nL3\n0\n20000\n40000\nTraining Steps per Layer\n0.0\n0.2\n0.4\n0.6\n0.8\nD2-CSNN-B1\nL1\nL2\nL3\n0\n10\n20\n30\n40\n50\nShot Size\n20\n30\n40\nFew-shot Accuracy [%]\nD-CSNN-B1\nS2-CSNN-B32\nFigure 3.5: Neuron utilization and few-shot learning. (top) Neuron utilization\nof models trained with various batch sizes and mask learning rules on Cifar10.\n(bottom) 10-fold few-shot classifier accuracy of our models with increasing shot\nsizes from 1 to 50 on Cifar10.\n3.4.9\nNeuron Utilization\nIn the upper part of Figure 3.5, we observe that the neuron utilization — the per-\ncentage of used neurons per input — is getting smaller for deeper layers, indicating\nthat the SOM neurons of deeper layers specialize to certain inputs. Compared to\nlearning rule 3.13, learning rule 3.12 leads to a higher neuron utilization, which may\n43\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nbe explained by the forgetting term of Oja’s rule, which is not present in Hebb’s rule.\nWe want to note that a higher neuron utilization does not necessarily lead to higher\nperformance since a lower neuron utilization indicates sparser representations that\nmay be more distinguishable.\n3.4.10\nFew-shot Performance\nThe lower part of Figure 3.5 shows the increase in accuracy of the linear classifier as\nthe number of representations per class used to train the linear classifier increases.\nOur models reach an accuracy of 41.49% on Cifar10, 27.29% on Cifar100, 13.70%\non Tiny ImageNet, and 68.92% on SOMe ImageNet when trained on 50 examples\nper class. The comparatively higher accuracy in SOMe ImageNet indicates that the\nmodel can utilize the additional patches obtained from the larger input images.\n3.4.11\nVisual Interpretations\nIn Figure 3.6 we show BMU images for each layer during the training of the D-CSNN\nmodel. These images are created by replacing the image patch with the reshaped\nBMU weights or a slice of these weights. For earlier training steps, BMUs that have\nbeen calculated from randomly initialized neurons can be seen for layers 2 - 4. After\ntraining the individual layers, patterns emerge. For example, for layer 1, we observe\npatterns corresponding to the content of specific regions of the input image. These\npatterns become less interpretable at deeper layers. Moreover, we observe that the\nSOM weights are not only sensitive to colors since different backgrounds result in\nsimilar BMU patterns, for example, in image 5 of layer 2.\nAs shown in row b) of Figure 3.7, the input images can be reconstructed with a\nhigh level of detail from the representation of our unsupervised CSNN models. This\nis surprising and shows the amount of information present in the representation of\nour unsupervised CSNN models. Moreover, in rows c), d), and e), it can be observed\nthat \"similar\" classes lead to similar average representations: 1) In rows c) and d)\nexist similar spots with high average neuron activities. 2) The classes have been\nsorted according to L1 distance of their average representation from the average\nrepresentation of the truck class. The average car class representation has the\nshortest distance to the average truck class representation, followed by the average\nrepresentations of \"similar\" looking animal classes. 3) Row e) shows images where\nthe average representation of the right-hand class has been subtracted element-\nwise from the average representation of the respective class. For example, the first\ncolumn of e) equals the truck’s average representation minus the car’s average\nrepresentation. Therefore, row e) shows the remaining distinguishable differences\n44\n3.5. CONCLUSION\nLayer 1\nStep 0\nStep 1500\nStep 3000\nStep 4500\nStep 5750\nLayer 2\nLayer 3\nLayer 4\nFigure 3.6: BMU images for each layer of the D-CSNN model during training. Images\nhave been created by replacing each patch with the SOM weight of the BMU, which\nis reshaped into the 3D patch shape (RGB image). For deeper layers, we only show\na slice of depth three through the BMU weight, since its hard to visualize kernel\ndepths larger then three. Since the D-CSNN-B1 model contains three maps, we use\nthe best BMU from all maps of the layer. Best viewed in color.\nbetween average representations of the respective class and the right-hand class.\nWe observe that this remaining information is lower when a \"similar\" class is on the\nright-hand side (e.g., dog minus cat).\nAdditional quantitative and qualitative results can be found in Appendix A.\n3.5\nConclusion\nIn this chapter, we have introduced the modular building blocks of CSNNs to learn\nrepresentations in an unsupervised manner without backpropagation. With the\nproposed CSNN modules and learning rules — which combine CNNs, SOMs and\n45\nCHAPTER 3. SELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\na)\nTruck\nAutomobile\nHorse\nDog\nCat\nBird\nShip\nDeer\nFrog\nAirplane\nb)\nc)\nd)\ne)\nFigure 3.7: Representations and reconstructions. a) The Cifar10 input image. b)\nThe reconstruction of the input image using the representation of the D-CSNN-B1\nmodel. c) The average representation per map for each class created by taking the\nmean over the spatial dimension of each test sample’s representation and reshaping\nthe resulting vector to SOM grid shape. d) RGB image of the average representation\n(three maps lead to an RGB image). e) Differences of the average representations\ncalculated by element-wise subtracting the average representation of the respective\nclass from the average representation of the class on the right-hand side. Best\nviewed in color.\nHebbian learning of masks — a new, alternative way for learning unsupervised\nfeature hierarchies has been explored. Along the way, we have discussed the Scalar\nBow Tie Problem and the objective function mismatch: two problems in the field\nof deep learning that we believe can potentially be solved together and provide an\ninteresting direction for future research.\n46\nPart II\nEvaluating Visual\nUnsupervised Repre-\nsentation Learning\n4 Investigating the Objective Function Mis-\nmatch\nFinding general evaluation metrics for unsupervised representation learning tech-\nniques is a challenging open research question, which has recently become more\nand more necessary due to the increasing interest in unsupervised methods. Even\nthough these methods promise beneficial representation characteristics, most\napproaches currently suffer from the objective function mismatch. This mismatch\nstates that the performance on a desired target task can decrease when the unsu-\npervised pretext task is learned too long - especially when both tasks are ill-posed.\nIn this chapter, we build upon the widely used linear evaluation protocol and\ndefine new general evaluation metrics to quantitatively capture the objective func-\ntion mismatch and the more generic metrics mismatch. We discuss the usability\nand stability of our protocols on a variety of pretext and target tasks and study mis-\nmatches in a wide range of experiments. Thereby we disclose dependencies of the\nobjective function mismatch across several pretext and target tasks with respect\nto the pretext model’s representation size, target model complexity, pretext and\ntarget augmentations, as well as pretext and target task types. In our experiments,\nwe find that the objective function mismatch reduces performance by ∼0.1-5.0%\nfor Cifar10, Cifar100, and PCam in many setups and up to ∼25-59% in extreme\ncases for the 3dshapes dataset.\n4.1\nMotivation\nUnsupervised representation learning is a promising approach to learn useful\nfeatures from huge amounts of data without human annotation effort. Thereby a\ncommon evaluation pattern is to train an unsupervised pretext model on different\ndatasets and then test its performance on several target tasks. Because of the huge\nvariety of target tasks and preferred representation characteristics, the evaluation\nof these methods is challenging. In preceding work, several evaluation metrics have\nbeen proposed [112, 174, 177, 199], but due to the fast changes in unsupervised\nlearning methodologies, only a few of them can be used across the wide spectrum\nof promising approaches. This is one reason why the linear evaluation protocol\nis commonly used [42, 59, 85, 104, 144, 206, 291], which trains a linear model for\n49\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\n1.6\n1.7\n1.8\nBest Target Loss\n0\n200\n400\nTarget Training Epochs\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\nTarget Loss\n0\n100\n200\nPretext Training Epochs\n0.5\n0.6\nPretext Loss\nFigure 4.1: The objective function mismatch. (bottom left) Evaluation loss of a\npretext autoencoder trained on Cifar10. (top left) Best evaluation losses of linear\ntarget models trained for classification on the representations of the pretext au-\ntoencoder from different pretext training epochs. (right) Evaluation loss curves\nfrom each linear target model. Colors correspond to the current epoch of pretext\ntask training, and each value is obtained by 5-fold cross-validation. An objective\nfunction mismatch occurs around pretext training epoch 40, from which the target\nloss increases. Best viewed in color.\na target task on-top of the representations of an unsupervised pretext model. In\nthis chapter, we show that simply training a target model for different layers of the\npretext model does not yield the entire picture of the training process and leads to a\nloss of useful temporal information about learning. It is already known in literature\nthat succeeding in a pretext task can be the reason why the model fails on the target\ntask. Here we propose that the linear evaluation protocol does not capture this\nproperly.\n4.2\nContributions\nWe extend the linear and nonlinear evaluation protocols and address the question\nof when succeeding in a pretext task hurts performance and how much. We train\ntarget models on representations obtained from different training steps or epochs\nof the pretext model and plot target and pretext model metrics in comparison, as\nshown in Figure 4.1. Thereby we observe that training an unsupervised pretext\nmodel too long can lead to an objective function mismatch [185, 238] between the\nobjectives used to train both models. This mismatch leads to a drop in performance\n50\n4.3. HARD METRICS MISMATCH\non the target task, while the pretext model and the target models still converge\ncorrectly, which can be seen in Figure 4.1. To quantify our results, we define soft and\nhard versions for two simple and general evaluation metrics - the metrics mismatch\nand the objective function mismatch - formally. With these metrics, we then evaluate\ndifferent image-based pretext task types for self-supervised learning by using the\nlinear evaluation protocol.\nOur contributions can be summarized as follows:\n• We propose hard and soft versions of general metrics to measure and com-\npare mismatches of (unsupervised) representation learning methods across\ndifferent target tasks (Sections 4.3 and 4.4). To the best of our knowledge, this\nhas not been done before. (RQ-E2)\n• We discuss the usability and stability of our protocols on a variety of pretext\nand target tasks (Section 4.6.2). (RQ-E2)\n• In our experiments, we qualitatively show dependencies of the objective\nfunction mismatch with respect to the pretext model’s representation size\n(Section 4.6.3), target model complexity (Section 4.6.4), pretext and target\naugmentations (Section 4.6.5), as well as pretext and target task types (Section\n4.6.6). (RQ-E3)\n• We find that the objective function mismatch can reduce performance on\nvarious benchmarks. Specifically, we observe a performance decrease of ∼0.1-\n5.0% for Cifar10, Cifar100, and PCam, and up to ∼25-59% in extreme cases\nfor the 3dshapes dataset (Section 4.6). (RQ-E4)\n4.3\nHard Metrics Mismatch\nWith the objective function mismatch, we want to measure the mismatch of two\nobjectives while training a model on a (unsupervised) pretext task and using its\nrepresentations to train another model on a target task. In general we can measure\nthe mismatch of two comparable metrics, if one metric is captured during training\nof a single pretext model and the other is captured for each target model fully\ntrained on the representations of different steps or epochs of the pretext model. Two\ncomparable metrics, for example, are classification accuracies for the pretext and\ntarget task because they use the same measurement unit and scale. As illustrated in\nFigure 4.2, the metric values of the target models form a curve over the course of\nlearning. Between a metric value on this curve and the corresponding metric value\n51\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\n0\n2000\n4000\nPretext Training Epochs\n10\n20\n30\n40\n50\n60\n70\nError in %\n         +0.77%\nMM3: 34.29%\n          -0.51%\n0\n2000\n4000\nPretext Training Epochs\n0\n10\n20\n30\n40\nM3 in %\nFigure 4.2: M3 and MM3. (left) The intuition behind MM3: In this case, both metrics\nmeasure a classification error in %. The pretext metric (solid curve) is measured on\nthe pretext task of predicting rotations with a ResNet18 model, and the target metric\n(dotted curve) on the fully trained Cifar10 classification task. When divided by the\nnumber of measurements, the discrete area enclosed by the target and pretext task\ncurves corresponds to the MM3 of the entire training process. (right) The Metrics\nMismatch M3 plotted during training: We observe a common behavior where the\nmismatch increases as training progresses. Additionally, we show the stability (+,−)\nof M3, and MM3 across a 5-fold cross-validation.\non the pretext model curve we can define the metrics mismatch (M3) for a certain\nstep (or epoch) in training by calculating their distance.\nMore formally, let MP = (mP\n1 ,...,mP\nn ) denote an n-tuple of values from a metric\nused to measure pretext model P for different steps S = (s1,...,sn). The length n of\nthe tuple is usually given by a convergence criterion C on the metric of model P\nduring training. Furthermore, let MT = (mT\n1 ,...,mT\nn ) denote an n-tuple of values\nfrom a comparable metric used to measure target model T . MT is of the same\nlength and order as MP and all values are calculated at the same training steps S\nof MP. Thereby the target model T is fully retrained for every step si in S on the\nrepresentations of model P at this step before we measure mT\ni .\nDefinition 1 The hard Metrics MisMatch (M3) between mT\ni and mP\ni at step si is\ndefined as:\nM3(mT\ni ,mP\ni ) := mT\ni −mP\ni\n(4.1)\nwhere mT\ni and mP\ni are single values measured with comparable metrics at step si.1\n1Note that we define our measurements only for the case where lower metric values correspond to\n52\n4.3. HARD METRICS MISMATCH\nIf M3 > 0, the performance of the target model is lower then the performance\nof the pretext model at step si. In contrast, M3 ≤0 represents the desired case\nin unsupervised representation learning, where the target model performance\nis the same or above the pretext model performance at step si. In our case, we\nmeasure mT\ni and mP\ni over the entire evaluation dataset for every step si in S. We\nplot M3(mT\ni ,mP\ni ) for the pretext task of predicting rotations and the target task\nof Cifar10 classification during training in Figure 4.2. This shows that our metric\ncaptures the behavior of the target task performance regarding the pretext task\nperformance, and we observe an increasing mismatch as training progresses. To\ncapture the mismatch of the entire training procedure with respect to the target\ntask in a single value, we can now define the mean hard metrics mismatch (MM3)\nas the mean bias error between MT and MP.\nDefinition 2 The Mean hard Metrics MisMatch (MM3) between MT and MP is\ndefined as:\nMM3(MT ,MP) := 1\nn\nX\n0<i≤n\n(mT\ni −mP\ni )\n(4.2)\nwhere MT and MP are tuples measured with comparable metrics until the pretext\nmodel converges at step sn.\nMM3 measures the bias of the target model metric to the pretext model metric. For\npositive or negative values of MM3, we can make similar observations as for M3,\nbut they now account for the tendency of the entire training process and not for a\nsingle step si. In general, the mean bias error can convey useful information, but it\nshould be interpreted cautiously because there are special cases where positive and\nnegative values cancel each other out. In our case, this can happen, for example,\nwhen learning the pretext task is very useful for the target task early in training\nbut hurts the target performance equally strong later on when the pretext task is\nsufficiently solved. We capture this behavior simply by measuring and plotting M3\nindividually for the metric values of each step, as in Figure 4.2, analogous to the way\na loss is measured and plotted during training.\n4.3.1\nHard Objective Function Mismatch\nNaively we could compare the objective functions of the target and pretext task by\nusing M3, which we define as the hard objective function mismatch. In most cases,\nhowever, the objective functions used to train the pretext model and the target\nmodels are not directly comparable. This is due to the usage of different objective\nbetter performance. The definition for the opposite case arises naturally by changing maximum and\nminimum operations and/or subtraction orders.\n53\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nfunctions for both model types, which, i.a., use different (non-)linearities. But for\nsome pretext tasks simple comparable metrics can be defined. These metrics can\nbe used as a proxy to measure the objective function mismatch in a general and\ncomparable manner. A well-known example is the accuracy metric, which can\nbe used on the self-supervised tasks of predicting rotations [85] and the state-of-\nthe-art approach of contrastive learning [42]. But comparable metric pairs can\nnot always be found easily. For example, if we train a variational autoencoder and\nlater use its representation for a classification target task, it does not make sense to\ndefine a pixel-wise error between the given and generated images as a comparable\npretext task metric. To achieve a comparable measurement for this situation and on\nthe loss curves in general, one could think of individual normalization techniques\nbetween objective function pairs. However, we want to be practical and define a\nmeasure that can be used independently of the objective function pairs for every\npretext and target model combination. Furthermore, in practice we might be\nespecially interested in how much the target task mismatches with the pretext task\nif a mismatch decreases target performance. This is why we define soft versions of\nour measurements.\n4.4\nSoft Metrics Mismatch\nTo bypass objective function pair normalization we define the soft metrics mismatch\n(SM3) directly on the target metric. Thereby we no longer take the exact improve-\nment of the pretext metric into account, we only care about its convergence. Since\nwe now have no exact information about the pretext metric curve, we define SM3\nfor the current step si between the current target metric value and the previously or\ncurrently occurred minimal target metric value:\nDefinition 3 The Soft Metrics Mismatch (SM3) between MT and MP at step si is\ndefined as:\nSM3(mT\ni ) := mT\ni −min\n0<j≤i(mT\nj )\n(4.3)\nwhere min0<j≤i(mT\nj ) is the previously or currently occurred minimal target metric\nvalue.\nSM3 has a slightly different meaning compared to M3: It equals zero if mT\ni is a\nminimal metric value and is positive if mT\ni is higher than the previously occurred\nminimal metric value. We want to point out that the only way we incorporate the\npretext metrics into this measurement is by making sure that the pretext model\ndoes not overfit and has not yet converged. Again, we measure mT\ni and mP\ni over the\nentire evaluation dataset for every step si in S and plot SM3(mT\ni ). A common case\n54\n4.4. SOFT METRICS MISMATCH\n0\n2000\n4000\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nN(Best Target Loss)\n         +4.30%\nOFM: 10.54%\n          -5.27%\ncOFM: 20.77%\nmOFM: 21.93%\n0\n2000\n4000\nPretext Training Epochs\n0\n5\n10\n15\n20\n25\n30\nOFM in %\nFigure 4.3: OFM, cFM, mOFM, and MOFM. We measure the OFM instead of SM3 by\nnormalizing the metric values with Equation 4.8. For visualization, we additionally\nshift the normalized metric values such that they lie in [0,100] by subtracting the\nminimal measurement. (left) Intuition for the MOFM: When divided by the number\nof measurements, the discrete area enclosed by the target metric values and their\nprevious minimal target metric values correspond to the OFM of the entire training\nprocess. The red arrow shows the cOFM, and the black arrow the mOFM. In this\ncase, the target metric measures the cross-entropy loss of each fully trained target\nmodel on a Cifar10 validation set. (right) The OFM plotted during training: We\nobserve an increasing mismatch starting around epoch 600. Additionally, we show\nthe range (+,−) of the OFM and MOFM across a 5-fold cross-validation.\nis shown in Figure 4.3, which captures the behavior of pretext model training with\nrespect to the target model. Here we observe zero soft mismatch early in training\nfollowed by increasing soft mismatch until pretext model convergence. Again, we\ncan capture the mismatch of the pretext task with respect to the target task for the\nentire training process until pretext model convergence as the mean bias error of\nevery metric value mT\ni and its minimal metric value:\nDefinition 4 The Mean Soft Metrics Mismatch (MSM3) between MT and MP is\ndefined as:\nMSM3(MT ) := 1\nn\nX\n0<i≤n\nµ\nmT\ni −min\n0<j≤i(mT\nj )\n¶\n(4.4)\nwhen the pretext model convergences at step sn.\nMSM3 can either be zero if no mismatch occurs or positive if there is a mismatch.\nTherefore, using MSM3 brings the benefit that positive values can not be canceled\n55\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nout by negative values. Furthermore, we define the maximum occurring mismatch\nmSM3 and the mismatch at pretext model convergence cSM3. We are especially\ninterested in cSM3 since it measures the representations one would naively take for\nthe target task:\ncSM3(MT ) := SM3(mT\nn )\n(4.5)\nmSM3(MT ) := max\n0<i≤n\n¡\nSM3(mT\ni )\n¢\n(4.6)\n4.4.1\nSoft Objective Function Mismatch\nNow we can use SM3 to measure a soft form of the objective function mismatch\non the loss curve obtained by the target models. However, the values of these mea-\nsurements lie in a range that depends on the target objective function. Therefore,\nthey are not directly comparable to the measurements on loss curves from other\ntarget tasks. This is why we normalize the measurements of the target metric to the\npercentage range and define the objective functions mismatch (OFM) as follows:\nDefinition 5 The Soft Objective Function Mismatch (OFM) between MT and MP\nat step si is defined as:\nOFM(mT\ni ) := SM3\n¡\nN(mT\ni )\n¢\n(4.7)\nN(x) :=\n\n\n\n\n\n100×x\nmT\n1 −mT\nb\nmT\n1 > x ≥mT\nb\n0\nmT\n1 = mT\nb = x\n∞(special case)\nmT\n1 = mT\nb < x\n(4.8)\nwhere mT\n1 is the loss value of the target model trained on an untrained pretext model\n(s1 = 0) and b = argmin0<i≤n(mT\ni ) denotes the index of the minimal target loss value.\nWe then use ˆMT = (N(mT\n1 ),...,N(mT\nn )) to calculate the OFM.\nThe intuition behind this normalization is that we declare mT\nb as the value where\nthe pretext model has learned all of the target objective it was able to learn (with\nthis setting) and mT\n1 as the value where the model has learned nothing of the target\nobjective. Now we measure with OFM(mT\ni ) for what percentage the learning of a\npretext objective hurts the maximum achieved target performance at step si. An\nexample is illustrated in Figure 4.3. Furthermore, we can normalize the other soft\nmeasurements from Equation 4.5 and 4.6 analog to Equation 4.7.\nThe OFM is a general measure that can be used for pretext and target models where\nno good proxy metrics can be defined. With the OFM, we are able to compare mis-\nmatches across different pretext and target task objectives and their combinations.\n56\n4.5. EXPERIMENTAL SETUP\nWe propose these measurements to obtain quantitative and therefore comparable\nresults for individual pretext tasks. To get the best information about the training\nprocess we encourage to plot the curves formed by our metrics as well. We want\nto point out that our metrics are not intended to measure target task performance,\nthey measure how much the performance on a target task can decrease when an\n(ill-posed) pretext task is learned too long. Now, to understand the OFM further, we\ntake a look at some cases:\nOFM(mT\ni ) = 0: In this case, solving the pretext task objective has not hurt the\nperformance of the target task objective at this point in training.\nOFM(mT\ni ) = x: Solving the pretext objective has hurt the performance of the target\nobjective at this point in training by x% of what the model has learned. Therefore,\nwe should have stopped training earlier. It is not guaranteed that longer training\nwould hurt performance even more, but a growing OFM curve or MOFM is a good\nindicator of that.\nOFM(mT\ni ) > 100: The target objective performance is worse than for the untrained\nmodel at this point in training.\nMOFM(MT ) = ∞: Solving the pretext objective hurts the performance of the target\nobjective from the point of initialization. Because we have learned essentially 0%\nabout the target objective in the training process, there is no interval to be used\nfor normalization. Therefore, we interpret this case as if the model has an infinite\nmismatch as soon as the model forgets something about the target objective.\n4.5\nExperimental Setup\nIn our experiments, we focus on image-based self-supervised learning. However, it\nis likely that other target domains show mismatches as well, e.g., [58].\nPretext Tasks. For generation-based self-supervision, we evaluate the approaches\nof autoencoding data from autoencoders (CAE) and color restoration (CCAE) as\nsuggested in [42]. To evaluate context structure generation, we use denoising\nautoencoders (DCAE). Transformation prediction (spatial context structure) is eval-\nuated via autoencoding tranformations by predicting rotations [85] (RCAE). For\ncontrastive methods (context-based similarity), we evaluate SimCLR [42] (SCLCAE).\nWe refer to the literature for first glances into mismatches for VAEs [185], and meta-\nlearning [185] and to chapter 3 for self-organization [238].\n57\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nPretext Models. Unless stated otherwise, we use a four-layer CNN as the encoder.\nFor the autoencoding data approaches, we use a four-layer decoder with transposed\nconvolutions, for rotation prediction a single dense layer, and for contrastive learn-\ning a nonlinear head, as suggested in [42]. We show that mismatches account for\nother architectures as well by carrying out additional evaluations using ResNets\n[105] in Table 4.3 and Appendix B.\nTarget Tasks. We evaluate our metrics on image-based target tasks. For coarse-\ngrained classification, we use Cifar10, Cifar100 [145], and the coarse-grained labels\nof 3dshapes [29]. For fine-grained classification, we use the PCam dataset [259] and\nthe fine-grained labels of 3dshapes.\nTarget Models.: Following the linear evaluation protocol, we use a single, linear\ndense layer (FC) as the target model with a softmax activation. To evaluate our\nmetrics for other target models, we use a two-layer MLP (2FC) and a three-layer\nMLP (3FC).\nAugmentations. We make sure not to compare augmentations instead of pre-\ntext tasks by following [42] for our base augmentations, to which we add the pretext\ntask-specific augmentations for pretext task training and evaluation. For the target\ntask, we use the base training and evaluation augmentations of [42].\nOptimization. Our models are trained using the Adam optimizer [141] with stan-\ndard parameters and a batch size of 2048 without any regularization instead of\nbatch normalization. For our ResNets, we additionally use a weight decay of 1e−4.\nMismatch Evaluation. All reported values are determined by 5-fold cross-validation.\nWe use standard early stopping (from tf.keras) as a convergence criterion on the\npretext evaluation curve with a minimum delta (threshold) of 0 and patience of 3.\nWe change the patience in some experiments of Tables 4.1 and 4.3 to get a reason-\nable convergence epoch. For more details we refer to Appendix B. When calculating\nour metrics, we estimate target values of missing epochs with linear interpolation\nto save computation time. In our case, SM3 and MM3 are measured on the target\ntask accuracy.\nImplementation. Our implementation is available at\nhttps://github.com/BonifazStuhr/OFM.\n58\n4.6. EVALUATION\n0\n20\n40\n60\nOFM in %\nCAE on Cifar10\n0\n50\n100\n150\nOFM in %\n0\n200\nPretext Training Epochs\n0\n10\n20\n30\nOFM in %\n0\n50\n100\nDCAE on Cifar10\n0\n50\n100\n150\n0\n200\nPretext Training Epochs\n0\n5\n10\n15\n0\n5\n10\n15\nCCAE on Cifar100\n0\n10\n20\n0\n200\n400\nPretext Training Epochs\n0.0\n0.5\n1.0\n0\n50\n100\nCCAE on PCam\n0\n1\n2\n3\n0\n250\n500\nPretext Training Epochs\n0\n5\n10\n15\n0\n50\n100\nRCAE on PCam\n0\n10\n20\n0\n1000\n2000\nPretext Training Epochs\n0\n10\n20\n30\n0\n50\n100\nSCLCAE on Shapes3d\n2x2x4\n2x2x32\n2x2x128\n2x2x256\n2x2x512\n2x2x1024\n0\n250\n500\n750\nFC\n2FC\n3FC\n0\n250\n500\nPretext Training Epochs\n0\n20\n40\nAll\n-Jitter\n-JitterFlip\n-Flip\nFigure 4.4: OFM ablation study. (top) Impact of different pretext model represen-\ntation sizes on the OFM. (middle) OFM for the linear target model and nonlinear\ntarget models trained on our pretext model. (bottom) OFM for the linear target\nmodel and the pretext models trained on fewer augmentations. First, we removed\nthe color jitter and then the vertical flip from the augmentations. The target models\nof SCLCAE were trained on 3dshapes to predict the object hue. Best viewed in color.\n4.6\nEvaluation\nIn the following, we show the results of most pretext and target tasks we have\nevaluated. We refer to Appendix B for additional, more detailed evidence. Since we\ncapture our metrics during training, all mismatches are measured on the evaluation\ndataset.\n4.6.1\nMismatch and Convergence\nFor our measurements we make sure to use metric value pairs from models that do\nnot overfit. We achieve this by applying a convergence criterion on the pretext task\nand by using the best metric values from each target model evaluation curve. As\nshown in Appendix B, most observations in our experiments are independent of\nthe use of a convergence criterion if pretext models are trained long enough and\nwithout overfitting. Furthermore, we observe a common behavior in Figure 4.1:\nTarget models trained on higher epochs of the pretext model tend to converge faster.\nThis indicates that longer training of the pretext task tends to create easier separable\nrepresentations, which may mismatch with the class label.\n59\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nTable 4.1: MOFM, cSM3 and MM3 of the models from Figure 4.4. MM3 is measured\non the target and pretext task classification error. cSM3 is measured on the target\ntask classification error and corresponds to the accuracy we would lose when we\nnaively train the target model after pretext model convergence. Values are obtained\nby 5-fold cross-validation. We show the stability of each measurement in Tables B.2,\nB.3, and B.4 of Appendix B.\nCAE (Cifar10)\nDCAE (Cifar10) CCAE (Cifar100)\nCCAE (PCam)\nRCAE (PCam)\nSCLCAE (3dshapes)\ncSM3 MOFM cSM3\nMOFM\ncSM3\nMOFM\ncSM3 MOFM cSM3 MOFM\nMM3\ncSM3 MOFM\nMM3\nRep. Size:\n2x2x4\n0.00\n0.00\n0.05\n0.00\n0.28\n1.54\n4.98\n9.28\n5.38\n4.15\n-22.26\n26.34\n∞\n-7.99\n2x2x32\n0.07\n1.99\n0.00\n3.20\n0.65\n3.64\n5.17\n34.30\n3.34\n7.92\n-21.09\n12.98\n36.39\n-57.67\n2x2x128\n0.20\n10.10\n0.06\n5.51\n0.51\n0.81\n0.32\n0.10\n1.03\n4.04\n-23.47\n8.14\n22.65\n-66.19\n2x2x256\n0.75\n11.14\n0.69\n5.17\n0.17\n0.00\n0.43\n0.87\n0.44\n0.00\n-27.60\n6.52\n27.65\n-65.77\n2x2x512\n0.43\n5.28\n0.36\n1.25\n0.00\n0.00\n0.20\n0.07\n0.18\n0.00\n-28.03\n5.96\n27.78\n-63.61\n2x2x1024\n0.24\n0.25\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.09\n0.00\n-26.56\n4.76\n32.70\n-57.92\nTarget Model:\nFC\n0.75\n11.14\n0.69\n5.17\n0.00\n0.00\n0.00\n0.00\n1.03\n4.04\n-23.47\n6.52\n27.65\n-65.77\n2FC\n0.03\n5.68\n0.00\n5.14\n0.08\n0.00\n0.00\n0.00\n0.31\n0.76\n-28.56\n1.84\n103.30\n-70.49\n3FC\n0.03\n3.94\n0.00\n3.17\n0.12\n0.02\n0.08\n0.00\n0.37\n0.61\n-29.61\n0.91\n258.18\n-71.26\nAugmentations:\nAll\n0.75\n11.14\n0.69\n5.17\n0.17\n0.00\n0.43\n0.87\n1.03\n4.04\n-23.47\n6.52\n27.65\n-65.77\nNoJitter\n0.99\n10.99\n0.50\n2.33\n-\n-\n-\n-\n0.58\n0.10\n-28.60\n0.00\n0.01\n-37.92\nNoJitterNoFlip\n1.00\n12.51\n0.55\n1.73\n-\n-\n-\n-\n1.51\n7.33\n-10.60\n0.00\n0.00\n-35.95\nNoFlip\n-\n-\n-\n-\n0.20\n0.00\n0.41\n0.04\n-\n-\n-\n-\n-\n-\n4.6.2\nStability\nTo evaluate the stability of our measurements, we show the mismatches of the entire\ntraining process and their range (+,−) using 5-fold cross-validation in Tables 4.2\nand 4.3. The range of all other models we have trained is shown in Appendix B.\nWe observe that M3 generally seems more stable than SM3 or the OFM since it\ndoes not rely so heavily on the target metric values, which can be quite unstable.\nThe instability of the target task mismatch is captured in M3 but does not matter\nmuch in the overall measurement for most cases. This is favorable if a stable value\nis desired and unfavorable if one wants to capture the instability of the target\ntask training process explicitly. Furthermore, M3 is able to compensate target\nfluctuations with pretext fluctuations. In general, we observe that as long as we\ncalculate the OFM across a fair amount of cross-validations (in our case 5), we can\nmake statements about the mismatch. We measure our metrics on the mean losses\nduring 5-fold cross-validation instead of calculating them five times and taking the\naverage. For M3, both variants are equivalent, and for the OFM measuring on the\nmean losses leads to a lower bound in the case where all models converge at step\nsn (see Appendix B for the simple proofs). We prefer to measure our metrics on\nthe mean losses since this avoids mismatches occurring only in some validation\ncycles due to small fluctuations in the underlying training procedure. An example\n60\n4.6. EVALUATION\n0\n200\n400\n \nP. Loss\n0\n1\n2\nBest Target Loss\nCAE\n0\n200\n400\nDCAE\n0\n200\n400\nCCAE\n0\n200\n400\nRCAE\n0\n250\n500\nSCLCAE\nfloor_hue\nwall_hue\nobject_hue\norientation\nscale\nshape\nPretext Training Epochs\nFigure 4.5: Pretext and target losses on the 3dshapes dataset. (bottom) Pretext losses\nof our model trained for color restoration (CCAE), prediction rotations (RCAE), and\ncontrastive learning (SCLCAE). (top) Best target losses of linear models trained for\nthe different prediction tasks of 3dshapes. Best viewed in color.\nis shown in translucent red in Figure 4.3 at the beginning of training. We want to\npoint out that the training and validation data differ slightly in every training round\nbecause of the cross-validation setup. This increases the instability but shows the\ngeneral behavior of the metrics for the underlying data distribution. In Figures B.2\nand B.1 of Appendix B, we compare the instability of partially measured mismatches\nusing linear interpolation with mismatches measured for every pretext training\nepoch and observe a similar instability. However, when using the OFM in practice to\ncompare models on a finer scale, we recommend searching for the actual minimal\ntarget metric value since the OFM relies on this value at each step. When tuning\na model for maximum performance, one searches for this value. Thereby looking\nat the OFM curve gives good indications of which interval one should search. This\nmakes this protocol useful for performance tuning if enough computational power\nis available.\n4.6.3\nDependence on Representation Size\nWe hypothesize that large representation sizes tend to lower the OFM, which could\nbe one reason why representation sizes are large in unsupervised learning. To affirm\nthis hypothesis empirically, we train our pretext models with varying representation\nsizes on different target tasks while fixing all other model parameters. Figures 4.4\nand 4.1 show that the OFM tends to decrease when we enlarge the representation\nsize. A reason for that might be that target models can exploit the high dimensional\nspace of large representations to find better-fitting clusters for their target task.\nWe found an exception to this behavior, where we use larger representations of\nSCLCAE for the easy task of object hue prediction. Here, the target models trained\non the untrained pretext models with larger representation sizes already achieve\n61\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nTable 4.2: cSM3, MOFM and MM3 on the 3dshapes dataset. SM3 and MM3 are\nmeasured on the target task accuracy. Values are obtained by 5-fold cross-validation.\nWe show the stability of each measurement in Tables B.5, B.6, and B.7 of Appendix B.\nCAE\nDCAE\nCCAE\nRCAE\nSCLCAE\ncSM3 MOFM cSM3 MOFM cSM3 MOFM cSM3 MOFM\nMM3\ncSM3 MOFM\nMM3\nfloor_hue\n0.01\n0.95\n0.00\n1.28\n0.02\n0.00\n56.68\n∞\n44.67 28.18\n268.27\n-48.38\nwall_hue\n0.02\n32.03\n0.00\n24.43\n0.10\n0.00\n25.17\n∞\n7.80\n0.29\n0.46\n-76.40\nobject_hue\n0.38\n22.71\n0.43\n24.55\n1.55\n0.63\n59.65\n∞\n40.1\n2.87\n8.69\n-73.17\nscale\n0.41\n0.00\n0.27\n0.00\n0.10\n0.00\n2.60\n0.13\n31.78\n2.43\n0.00\n-44.80\nshape\n0.07\n0.00\n0.08\n0.00\n0.03\n0.00\n0.20\n0.06\n-2.48\n1.67\n2.16\n-67.54\norientation\n0.00\n0.00\n0.00\n0.00\n0.23\n0.00\n0.48\n0.00\n22.26\n2.50\n6.68\n-9.11\naverage\n0.15\n9.28\n0.13\n8.21\n0.34\n0.11\n24.13\n∞\n24.02\n6.32\n47.71\n-53.23\nhigh performance due to a larger number of color-selective random features. Fur-\nther learning of the pretext model does not lead to a high performance gain in this\ncase, and forgetting these sensitive random features during training leads to a high\nmismatch. Additionally, we observe that mismatches decrease when we decrease\nthe representation size for generation-based methods. A reason could be that the\npretext models are forced to generalize to solve the target task for small representa-\ntion sizes due to the limited amount of features in the bottleneck or simply underfit\non the pretext task.\n4.6.4\nDependence on Target Model Complexity\nIn Figures 4.4 and 4.1, we observe an OFM spike early in training for more complex\ntarget models. This spike occurs probably because nonlinear target models make\nbetter sense of specific random features at pretext task initialization, in contrast to\nthe linear target model. Besides early spikes, mismatches tend to decrease when we\nadd complexity to the target model. A model with increased nonlinearity has more\nfreedom to disentangle representations that do not fit properly with the target task.\nAgain we found an exception where the MOFM is lower for linear models when\npredicting the object hue after contrastive learning, which can be appointed to the\ncolor-selective, random features of the untrained pretext model.\n4.6.5\nDependence on Augmentations\nWe vary the augmentations used for the pretext and target model by removing the\ncolor jitter and the image flip from our base augmentations successively. Figure 4.4\nshows that augmentations can have a positive or negative impact on the mismatch.\n62\n4.6. EVALUATION\nE.g., when predicting the object’s hue, the ill-posed color jitter augmentation in-\ncreases the mismatch significantly.\n4.6.6\nDependence on Target Task Type\nHere we use our metrics to examine findings stated in [58], [144], [289], and [261],\nwhere it is argued that some pretext tasks are better suited for different target tasks.\nWe fix the underlying data distribution by using the 3dshapes dataset and train our\ntarget models for the different tasks. These tasks require a generic understanding\nof the scene like coarse-grained knowledge about the object’s type and hue and\nfine-grained knowledge about shapes, positions, and scales. In Figures 4.5 and 4.2,\nwe observe that pretext models tend to learn pretext task-specific features and discard\nfeatures that are not needed to solve the pretext task during training. Therefore, these\nmodels mismatch with ill-posed target tasks. For example, rotation prediction dis-\ncards features corresponding to the hue while it learns much about the orientation\nof the object.\nTable 4.3: Mismatches of ResNets with convergence criterium. ACC stands for\nthe best accuracy on the target task of all target models trained on the pretext\nmodel. cSM3 corresponds to the accuracy we would lose when we naively train\nthe target model after pretext model convergence. Values are obtained by 5-fold\ncross-validation. The mismatches are measured with a convergence criterium.\nResNet\nACC\ncSM3\nMOFM\nMM3\nRCAE (Cifar10)\n54.64+1.80\n−2.01\n3.98+1.74\n−3.60\n4.87+4.42\n−3.11\n31.82+0.75\n−0.68\nSCLCAE (PCam) 96.25+0.44\n−0.23\n0.37+0.44\n−0.37\n0.86+1.00\n−0.60\n−53.26+0.52\n−0.38\n4.6.7\nApplying our Metrics to ResNet Models\nIn Figure 4.6, we apply our metrics to ResNet models for several pretext and target\ntasks. For contrastive learning, we observe a small OFM for Cifar10 and Cifar100,\nwhich occurs late in training after pretext model convergence. However, when we\nuse contrastive learning as a pretext task for fine-grained tumor detection on the\nPCam dataset, we observe a mismatch before pretext model convergence. For the\nwell-known rotation prediction pretext task, we observe a high mismatch in Cifar10\nclassification early in pretext training. In Table 4.3, we show the corresponding\nmismatches measured until pretext model convergence.\n63\nCHAPTER 4. INVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\n0\n2000\n4000\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nN(Best Target Loss)\n         +0.07%\nOFM: 0.12%\n          -0.02%\ncOFM: 0.06%\nmOFM: 0.52%\n0\n2000\n4000\nPretext Training Epochs\n0.0\n0.2\n0.4\n0.6\nOFM in %\nSCLResNet18 on Cifar10\n0\n2000\n4000\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nN(Best Target Loss)\n         +0.05%\nOFM: 0.06%\n          -0.04%\ncOFM: 0.71%\nmOFM: 0.71%\n0\n2000\n4000\nPretext Training Epochs\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nOFM in %\nSCLResNet18 on Cifar100\n0\n2000\n4000\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nN(Best Target Loss)\n         +1.35%\nOFM: 2.02%\n          -0.33%\ncOFM: 3.89%\nmOFM: 4.13%\n0\n2000\n4000\nPretext Training Epochs\n0\n2\n4\n6\nOFM in %\nSCLResNet18 on Patch_camelyon\n0\n2000\n4000\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nN(Best Target Loss)\n         +4.30%\nOFM: 10.54%\n          -5.27%\ncOFM: 20.77%\nmOFM: 21.93%\n0\n2000\n4000\nPretext Training Epochs\n0\n10\n20\n30\nOFM in %\nRResNet18 on Cifar10\nFigure 4.6: OFM for different pretext tasks trained with a ResNet18 model as the\nbackbone. The mismatches are shown for the entire training process.\n4.7\nFuture Work\nIn future work, our metrics can be used to create, tune and evaluate (self-supervised)\nrepresentation learning methods for different target tasks and datasets. These met-\nrics make it possible to quantify the extent to which a pretext task matches a target\ntask and to determine whether the pretext task learns the right kind of representa-\ntion throughout the entire training process. This enables a comparison of methods\non benchmarks across different pretext tasks and models. The dependencies of the\nobjective function mismatch on different parts of the self-supervised setup (e.g.,\nrepresentation size) can be explored in future work in more detail to evaluate our\nfindings further and to create pretext tasks and model architectures that are robust\nagainst mismatches. Our metrics are defined for setups where the target models are\ntrained on pretext model representations in general. Therefore, they can also be\napplied to other representation learning areas such as supervised, semi-supervised,\nfew-shot, or biological plausible representation learning.\n4.8\nConclusion\nIn this work, we have used the linear evaluation protocol as the basis to define\nand discuss metrics to measure the metrics mismatch and the objective function\nmismatch. With soft and hard versions of our metrics, we have collected evidence\n64\n4.8. CONCLUSION\nof how these mismatches relate to the pretext model’s representation size, target\nmodel complexity, pretext and target augmentations, as well as pretext and target\ntask types. Furthermore, we have observed that the epoch of target task peak\nperformance varies strongly for different datasets and pretext tasks. This highlights\nthe importance of the protocol and shows that comparing approaches after a fixed\nnumber of epochs does not yield the entire picture of their capability. Our protocols\nmake it possible to define benchmarks across different target tasks, where the\ngoal is not to mismatch with the target metrics while achieving the best possible\nperformance.\n65\nPart III\nUnsupervised Transfer of\nVisual Representations\n5 A Lane Detection Benchmark for Multi-\nTarget Domain Adaptation\nUnsupervised domain adaptation demonstrates great potential to mitigate do-\nmain shifts by transferring models from labeled source domains to unlabeled\ntarget domains. While unsupervised domain adaptation has been applied to\na wide variety of complex vision tasks, only few works focus on lane detection\nfor autonomous driving. This can be attributed to the lack of publicly available\ndatasets. To facilitate research in these directions, we propose CARLANE, a 3-way\nsim-to-real domain adaptation benchmark for 2D lane detection. CARLANE en-\ncompasses the single-target datasets MoLane and TuLane and the multi-target\ndataset MuLane. These datasets are built from three different domains, which\ncover diverse scenes and contain a total of 163K unique images, 118K of which are\nannotated. In addition, we evaluate and report systematic baselines, including\nour own method, which builds upon prototypical cross-domain self-supervised\nlearning. We find that false positive and false negative rates of the evaluated do-\nmain adaptation methods are high compared to those of fully supervised baselines.\nThis affirms the need for benchmarks such as CARLANE to further strengthen\nresearch in unsupervised domain adaptation for lane detection. CARLANE, all\nevaluated models, and the corresponding implementations are publicly available\nat https://carlanebenchmark.github.io\n5.1\nMotivation\nVision-based deep learning systems for autonomous driving have made significant\nprogress in the past years [77, 116, 192, 201, 212]. Recent state-of-the-art methods\nachieve remarkable results on public, real-world benchmarks but require labeled,\nlarge-scale datasets. Annotations for these datasets are often hard to acquire, mainly\ndue to the high expenses of labeling in terms of cost, time, and difficulty. Instead,\nsimulation environments for autonomous driving, such as CARLA [62], can be uti-\nlized to generate abundant labeled images automatically. However, models trained\non data from simulation often experience a significant performance drop in a differ-\nent domain, i.e., the real world, mainly due to the domain shift [223]. Unsupervised\ndomain adaptation methods [75, 76, 176, 245, 256, 268, 277, 308] try to mitigate\n69\nCHAPTER 5. A LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN\nADAPTATION\nMoLane\nTuLane\nSource\nTarget\nFigure 5.1: Images sampled from our CARLANE Benchmark. Best viewed in color.\nthe domain shift by transferring models from a fully-labeled source domain to\nan unlabeled target domain. This eliminates the need for annotating images but\nassumes that the target domain is accessible at training time. While unsupervised\ndomain adaptation has been applied to complex tasks for autonomous driving,\nsuch as object detection [192, 278] and semantic segmentation [272, 302], only few\nworks focus on lane detection [78, 116]. This can be attributed to the lack of public\nunsupervised domain adaptation datasets for lane detection.\n5.2\nContributions\nTo compensate for this data scarcity and encourage future research, we introduce\nCARLANE, a sim-to-real domain adaptation benchmark for lane detection. We use\nthe CARLA simulator for data collection in the source domain with a free-roaming\nwaypoint-based agent and data from two distinct real-world domains as target\ndomains. This enables us to construct a benchmark that consists of three datasets:\n(1) MoLane focuses on abstract lane markings in the domain of a 1/8th Model\nvehicle. We collect 80K labeled images from simulation as the source domain and\n44K unlabeled real-world images from several tracks with two lane markings as the\ntarget domain. Further, we apply domain randomization as well as data balancing.\nFor evaluation, we annotate 2,000 validation and 1,000 test images with our labeling\ntool.\n(2) TuLane incorporates 24K balanced and domain-randomized images from simu-\nlation as the source domain and the well-known TuSimple [255] dataset with 3,268\nreal-world images from U.S. highways with up to four labeled lanes as the target\ndomain. The target domain of MoLane is a real-world abstraction from the target\ndomain of TuLane, which may result in interesting insights about unsupervised\ndomain adaptation.\n70\n5.3. DATA GENERATION\n(3) MuLane is a balanced combination of MoLane and TuLane with two target\ndomains. For the source domain, we randomly sample 24K images from MoLane\nand combine them with TuLane’s synthetic images. For the target domains, we\nrandomly sample 3,268 images from MoLane and combine them with TuSimple.\nThis allows us to investigate multi-target unsupervised domain adaptation for lane\ndetection.\nTo establish baselines and investigate unsupervised domain adaptation on our\nbenchmark, we evaluate several adversarial discriminative methods, such as DANN\n[76], ADDA [256], and SGADA [4]. Additionally, we propose SGPCS, which builds\nupon PCS [286] with a pseudo-labeling approach to achieve state-of-the-art perfor-\nmance.\nIn summary, our contributions are three-fold: (1) We introduce CARLANE, a 3-way\nsim-to-real benchmark, allowing single- and multi-target unsupervised domain\nadaptation. (RQ-E5) (2) We provide several dataset tools, i.e., an agent to collect\nimages with lane annotations in CARLA and a labeling tool to annotate the real-\nworld images manually. (3) We evaluate several well-known unsupervised domain\nadaptation methods - as well as our own SGPCS method - to establish baselines and\ndiscuss results on both single- and multi-target unsupervised domain adaptation.\n(RQ-T1 and RQ-E6) To the best of our knowledge, we are the first to adapt a lane\ndetection model from simulation to multiple real-world domains.\n5.3\nData Generation\nTo construct our benchmark, we gather image data from a real 1/8th model vehicle\nand the CARLA simulator [62]. Ensuring the verification of results and transferabil-\nity to real driving scenarios, we extend our benchmark with the TuSimple dataset\n[255]. This enables gradual testing, starting from simulation, followed by model\ncars, and ending with full-scale real word experiments. Data variety is achieved\nthrough domain randomization in all domains. However, naively performing do-\nmain randomization might lead to an imbalanced dataset. Therefore, similar driving\nscenarios are sampled across all domains, and a bagging approach is utilized to\nuniformly collect lanes by their curvature with respect to the camera position. We\nstrictly follow TuSimple’s data format [255] to maintain consistency across all our\ndatasets.\n71\nCHAPTER 5. A LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN\nADAPTATION\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 5.2: Overview of our track types for MoLane. (a) - (d) show the black version\nof the training and validation tracks. These tracks are also constructed using a light\ngray surface material. (e) and (f) depict our test tracks.\n5.3.1\nReal-World Environment\nAs shown in Figure 5.2, we build six different 1/8th race tracks, where each track is\navailable in two different surface materials (dark and light gray). We vary between\ndotted and solid lane markings, which are pure white and 50 mm thick. The lanes\nare constantly 750 mm wide, and the smallest inner radius is 250 mm. The track\nlayouts are designed to roughly contain the same proportion of straight and curved\nsegments to obtain a balanced label distribution. We construct these tracks in four\nlocations with alternating backgrounds and lighting conditions.\n5.3.2\nReal-World Data Collection\nRaw image data is recorded from a front-facing Stereolabs ZEDM camera with 30\nFPS and a resolution of 1280×720 pixels. A detailed description of the 1/8th car\ncan be found in Appendix C. The vehicle is moved with a quasi-constant velocity\nclockwise and counter-clockwise to cover both directions of each track. All collected\nimages from tracks (e) and (f) are used for the test subset. In addition, we annotate\nlane markings with our labeling tool for validation and testing, which is made\npublicly available.\n5.3.3\nSimulation Environment\nWe utilize the built-in APIs from CARLA to randomize multiple aspects of the agent\nand environment, such as weather, daytime, ego vehicle position, camera position,\ndistractor vehicles, and world objects (i.a., walls, buildings, and plants). Weather\n72\n5.4. THE CARLANE BENCHMARK\nand daytime are varied systematically by adapting parameters for cloud density,\nrain intensity, puddles, wetness, wind strength, fog density, sun azimuth, and sun\naltitude. For further details, we refer to our implementation. To occlude the lanes\nsimilar to real-world scenarios, up to five neighbor vehicles are spawned randomly\nin the vicinity of the agent. We consider five different CARLA maps in urban and\nhighway environments (Town03, Town04, Town05, Town06, and Town10) to collect\nour dataset, as the other towns’ characteristics are not suitable for our task (i.a.,\nmostly straight lanes). In addition, we collect data from the same towns without\nworld objects to strengthen the focus on lane detection, similar to our model vehicle\ntarget domain.\n5.3.4\nSimulation Data Agent\nWe implement an efficient agent based on waypoint navigation, which roams ran-\ndomly and reliably in the aforementioned map environments and collects 1280×720\nimages. In each step, the waypoint navigation stochastically traverses the CARLA\nroad map with a fixed lookahead distance of one meter. In addition, we sample\noffset values ∆yk from the center lane within the range ±1.20 m.\nTo avoid saturation at the lane borders, which would occur with a sinusoidal\nfunction, we use the triangle wave function:\n∆yk = 2m\nπ arcsin(sin(ik))\n(5.1)\nwhere m is the maximal offset and ik is incremented by 0.08 for each simulation\nstep k. Per frame, our agent moves to the next waypoint with an increment of one\nmeter, enabling the collection of highly diverse data in a fast manner. We use a\nbagging approach for balancing, which allows us to define lane classes based on\ntheir curvature.\n5.4\nThe CARLANE Benchmark\nThe CARLANE Benchmark consists of three distinct sim-to-real datasets, which we\nbuild from our three different domains. The details of the individual subsets can be\nfound in Table 5.1.\nMoLane consists of images from CARLA and the real 1/8th model vehicle. For\nthe abstract real-world domain, we collect 46,843 images with our model vehicle,\nof which 2,000 validation and 1,000 test images are labeled. For the source do-\nmain, we use our simulation agent to gather 84,000 labeled images. To match the\n73\nCHAPTER 5. A LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN\nADAPTATION\nTable 5.1: Dataset overview. Unlabeled images are denoted by *, partially labeled\nimages are denoted by **.\nDataset\ndomain\ntotal images\ntrain\nvalidation\ntest\nlanes\nMoLane\nCARLA simulation\n84,000\n80,000\n4,000\n-\n≤2\nmodel vehicle\n46,843\n43,843*\n2,000\n1,000\n≤2\nTuLane\nCARLA simulation\n26,400\n24,000\n2,400\n-\n≤4\nTuSimple [255]\n6,408\n3,268\n358\n2,782\n≤4\nMuLane\nCARLA simulation\n52,800\n48,000\n4,800\n-\n≤4\nmodel vehicle + TuSimple [255]\n12,536\n6,536**\n4,000\n2,000\n≤4\nMoLane\nTuLane\nMuLane\nSource\nTarget\nFigure 5.3: Lane annotation distributions of the three subsets of CARLANE. Since\nthe real-world training data of MoLane and MuLane is unlabeled, we utilize their\nvalidation data for visualization.\nlabel distributions between both domains, we define five lane classes based on the\nrelative angle β of the agent to the center lane for our bagging approach: strong\nleft curve (β ≤−45◦), soft left curve (−45◦< β ≤−15◦), straight (−15◦< β < 15◦),\nsoft right curve (15◦≤β < 45◦), and strong right curve (45◦≤β). In total, MoLane\nencompasses 130,843 images.\nTuLane consists of images from CARLA, and a cleaned version of the TuSimple\ndataset [255], which is licensed under the Apache License, Version 2.0. To clean\ntest set annotations, we utilize our labeling tool to ensure that the up to four lanes\nclosest to the car are correctly labeled. We adapt the bagging classes to align the\nsource dataset with TuSimple’s lane distribution: left curve (−12◦< β ≤5◦), straight\n(−5◦< β < 5◦), and right curve (5◦≤β < 12◦).\nMuLane is a multi-target unsupervised domain adaptation dataset and is a bal-\nanced mixture of images from MoLane and TuLane. For MuLane’s entire training set\n74\n5.4. THE CARLANE BENCHMARK\nand its source domain validation and test set, we use all available images from Tu-\nLane and sample the same amount of images from MoLane. We adopt the 1,000 test\nimages from MoLane’s target domain and sample 1,000 test images from TuSimple\nto form MuLane’s test set. For the validation set, we use the 2,000 validation images\nfrom MoLane and 2,000 of the remaining validation and test images of TuLane’s\ntarget domain. In total, MuLane consists of 65,336 images.\nTo further analyze CARLANE, we visualize the ground truth lane distributions in\nFigure 5.3. We observe that the lane distributions of source and target data from\nour datasets are well aligned.\nMoLane, TuLane, and MuLane are publicly available at\nhttps://carlanebenchmark.github.io and licensed under the Apache License, Ver-\nsion 2.0.\n5.4.1\nDataset Format\nFor each dataset, we split training, validation, and test samples into source and\ntarget subsets. Lane annotations are stored within a .json file containing the lanes’\ny-values discretized by raw anchors, the lanes’ x-values, and the image file path\nfollowing the data format of TuSimple [255]. Additionally, we adopt the method\nfrom [212] to generate .png lane segmentations and a .txt file containing the linkage\nbetween the raw images and their segmentation as well as the presence and absence\nof a lane.\n5.4.2\nDataset Tasks\nThe main task of our datasets is unsupervised domain adaptation for lane detection,\nwhere the goal is to predict lane annotations Yt ∈RR×G×N given the input image\nXt ∈RH×W ×3 from the unlabeled target domain DT = {(Xt)}t∈T . R defines the\nnumber of row anchors, G the number of griding cells, and N the number of lane\nannotations available in the dataset, where the definition of Yt follows [255]. During\ntraining time, the images Xs ∈RH×W ×3, corresponding labels Ys ∈RH×W ×C from the\nsource domain DS = {(Xs,Ys)}s∈S , and the unlabeled target images Xt are available.\nAdditionally, MuLane focuses on multi-target unsupervised domain adaptation,\nwhere DT = {(Xt1)∪(Xt2)}t1∈T1,t2∈T2.\nAlthough we focus on sim-to-real unsupervised domain adaptation, our datasets\ncan be used for unsupervised and semi-supervised tasks and partially for super-\nvised learning tasks. Furthermore, a real-to-real transfer can be performed between\n75\nCHAPTER 5. A LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN\nADAPTATION\nthe target domains of our datasets.\n5.5\nBenchmark Experiments\nWe conduct experiments on our CARLANE Benchmark for several unsupervised\ndomain adaptation methods from the literature and our proposed method. Addi-\ntionally, we train fully supervised baselines on all domains.\n5.5.1\nMetrics\nFor evaluation, we use the following metrics:\n(1) Lane Accuracy (LA) [212] is defined by LA = pc\npy , where pc is the number of\ncorrectly predicted lane points and py is the number of ground truth lane points.\nLane points are considered as correct if their L1 distance is smaller than the given\nthreshold tpc =\n20\ncos(ayl ), where ayl is the angle of the corresponding ground truth\nlane.\n(2) False Positives (FP) and False Negatives (FN) [212]: To further determine the\nerror rate and to draw more emphasis on mispredicted or missing lanes, we mea-\nsure false positives with FP =\nl f\nlp and false negatives with FN = lm\nly , where l f is the\nnumber of mispredicted lanes, lp is the number of predicted lanes, lm is the number\nof missing lanes, and ly is the number of ground truth lanes. Following [212], we\nclassify lanes as mispredicted if the L A < 85%.\n5.5.2\nBaselines\nWe use Ultra Fast Structure-aware Deep Lane Detection (UFLD) [212] as baseline\nand strictly adopt its training scheme and hyperparameters. UFLD treats lane de-\ntection as a row-based classification problem and utilizes the row anchors defined\nby TuSimple [255]. To achieve a lower bound for the evaluated unsupervised do-\nmain adaptation methods, we train UFLD as a supervised baseline on the source\nsimulation data (UFLD-SO). Furthermore, we train our baseline on the labeled real-\nworld training data for a surpassable fully-supervised performance in the target\ndomain (UFLD-TO). Since the training images from MoLane and MuLane have no\nannotations, we train UFLD-TO in these cases on the labeled validation images and\nvalidate our model on the entire test set.\n76\n5.5. BENCHMARK EXPERIMENTS\n5.5.3\nCompared Unsupervised Domain Adaptation Methods\nWe evaluate the following feature-level unsupervised domain adaptation methods\non the CARLANE Benchmark by adopting their default hyperparameters and tuning\nthem accordingly. Each model is initialized with the pre-trained feature encoder of\nour baseline model (UFLD-SO). The optimized hyperparameters can be found in\nTable 5.2.\n(1) DANN [76] is an adversarial discriminative method that utilizes a shared feature\nencoder and a dense domain classifier connected via a gradient reversal layer.\n(2) ADDA [256] employs a feature encoder for each domain and a dense domain\ndiscriminator. Following ADDA, we freeze the weights of the pre-trained classifier\nof UFLD-SO to obtain final predictions.\n(3) SGADA [4] builds upon ADDA and utilizes its predictions as pseudo labels for\nthe target training images. Since UFLD treats lane detection as a row-based clas-\nsification problem, we reformulate the pseudo label selection mechanism. For\neach lane, we select the highest confidence value from the griding cells of each row\nanchor. Based on their griding cell position, the confidence values are divided into\ntwo cases: absent lane points and present lane points. Thereby, the last griding\ncell represents absent lane points as in [212]. For each case, we calculate the mean\nconfidence over the corresponding lanes. We then use the thresholds defined by\nSGADA to decide whether the prediction is treated as a pseudo label.\n(4) SGPCS (ours) builds upon PCS [286] and performs in-domain contrastive learn-\ning and cross-domain self-supervised learning via cluster prototypes. Our overall\nobjective function comprises the in-domain and cross-domain loss from PCS, the\nlosses defined by UFLD, and our adopted pseudo loss from SGADA. We adjust the\nmomentum for memory bank feature updates to 0.5 and use spherical k-means\n[132] with K = 2,500 to cluster them into prototypes.\n5.5.4\nImplementation Details\nWe implement all methods in PyTorch 1.8.1 and train them on a single machine with\nfour RTX 2080 Ti GPUs. Tuning all methods has required a total amount of compute\nof approximately 3.5 petaflop/s-days. The training times for each model range from\n4-13 days for UFLD baselines and 6-44 hours for domain adaption methods. In\naddition, we have found that applying output scaling on the last linear layer of the\nmodel yields slightly better results. Therefore, we divide the models’ output by 0.5.\n77\nCHAPTER 5. A LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN\nADAPTATION\nTable 5.2: Optimized hyperparameters to achieve the reported results. C denotes\ndomain classifier parameters, D denotes domain discriminator parameters, adv the\nadversarial loss from [256], cls the classifier loss, sim the similarity loss, and aux the\nauxiliary loss from [212]. Loss weights are set to 1.0 unless stated otherwise.\nMethod\nInitial Learning Rate\nScheduler\nBatch Size Epochs Losses\nOther Changes\nUFLD-SO\n4e−4\nCosine Annealing\n4\n150\ncls, sim, aux\n-\nDANN\n1e−5, C: 1e−3\n1e−5\n(1+10p)0.75\n4\n30\ncls, sim, aux, adv [76]\nC: 3 fc layers (1024-1024-2)\nADDA\n1e−6, D: 1e−3\nConstant\n16\n30\nmap [256], adv [256]\nD: 3 fc layers (500-500-2)\nSGADA\n1e−6, D: 1e−3\nConstant\n15\n10\nmap [256], adv [256],\nPseudo label selection\npseudo: 0.25\nSGPCS\n4e−4\nCosine Annealing\n16\n10\nin-domain [286],\n-\ncross-domain [286],\ncls, sim, aux, pseudo: 0.25\nUFLD-TO\n4e−4\nCosine Annealing\n4\n300\ncls, sim, aux\n-\nOur implementation is publicly available at https://carlanebenchmark.github.io.\n5.5.5\nEvaluation\nQuantitative Evaluation. In Table 5.3, we report the results on MoLane, TuLane,\nand MuLane across five different runs. We observe that UFLD-SO is able to gener-\nalize to a certain extent to the target domain. This is mainly due to the alignment\nof semantic structure from both domains. ADDA, SGADA, and our proposed SG-\nPCS manage to adapt the model to the target domain slightly and consistently.\nHowever, DANN suffers from negative transfer [263] when trained on MoLane and\nMuLane. The negative transfer of DANN for complex domain adaptation tasks is\nalso observed in other works [71, 139, 250, 263] and can be explained by the source\ndomain’s data distribution and the model complexity [263]. In our case, the source\ndomain contains labels not present in the target domain, as shown in Figure 5.3,\nwhich is more pronounced in MoLane and MuLane.\nWe want to emphasize that with an accuracy gain of a maximum of 5.14%\n(SGPCS) and high false positive and false negative rates, the domain adaptation\nmethods are not able to achieve comparable results to the supervised baselines\n(UFLD-TO). Furthermore, we observe that false positive and false negative rates\nincrease significantly on MuLane, indicating that the multi-target dataset forms\nthe most challenging task. False positives and false negatives represent wrongly de-\ntected and missing lanes which can lead to crucial impacts on autonomous driving\nfunctions. These results affirm the need for the proposed CARLANE Benchmark\nto further strengthen the research in unsupervised domain adaptation for lane\ndetection.\n78\n5.5. BENCHMARK EXPERIMENTS\nTable 5.3: Performance on the test set. Lane accuracy (LA), false positives (FP), and\nfalse negatives (FN) are reported in %.\nResNet-18\nMoLane\nTuLane\nMuLane\nLA\nFP\nFN\nLA\nFP\nFN\nLA\nFP\nFN\nUFLD-SO\n88.15\n34.35\n28.45\n87.43\n34.21\n23.48\n79.61\n44.78\n33.36\nDANN [76]\n85.25±0.49\n39.07±1.21\n36.18 ±1.50\n88.74±0.32\n32.71±0.52\n21.64±0.65\n78.25±0.62\n48.67±1.17\n41.69±1.80\nADDA [256]\n92.10±0.21\n19.71±0.77\n11.46±0.92\n90.72±0.15\n29.73±0.36\n17.67±0.42\n81.25±0.33\n41.69±0.40\n28.58±0.57\nSGADA [4]\n93.15±0.12\n16.23±0.22\n7.68±0.26\n91.70±0.13 28.42±0.34 16.10±0.43\n82.05±0.10\n40.37±0.33 25.95±0.36\nSGPCS (ours)\n93.29±0.07 16.22±0.17\n7.29±0.13\n91.55±0.13\n28.52±0.21\n16.16±0.26\n82.91±0.19\n40.76±0.57\n26.14±0.75\nUFLD-TO\n97.15\n0.96\n0.05\n94.97\n18.05\n3.84\n87.64\n29.48\n11.52\nResNet-34\nLA\nFP\nFN\nLA\nFP\nFN\nLA\nFP\nFN\nUFLD-SO\n88.76\n31.30\n26.70\n89.42\n32.35\n21.19\n80.70\n43.63\n31.40\nDANN [76]\n89.58±0.63\n28.75±1.53\n23.24±2.16\n91.06±0.14\n30.17±0.20\n18.54±0.25\n80.40±0.15\n43.52±0.37\n31.53±0.66\nADDA [256]\n91.76±0.29\n21.04±0.81\n13.06±1.02\n91.39±0.16\n28.76±0.30\n16.63±0.36\n81.64±0.34\n40.74±0.48\n27.50±0.78\nSGADA [4]\n92.59±0.18\n18.32±0.24\n9.88±0.29\n92.04±0.09\n28.18±0.20\n15.99±0.24\n82.93±0.03 39.45±0.11 24.98±0.13\nSGPCS (ours)\n92.82±0.27 17.10±0.73\n8.77±0.95\n93.29±0.18 25.68±0.48 12.73±0.59\n82.87±0.17\n40.13±0.28\n25.38±0.45\nUFLD-TO\n96.92\n0.94\n0.03\n94.43\n20.74\n7.20\n87.62\n29.19\n11.08\nUFLD-SO\nDANN\nADDA\nSGADA\nSGPCS\nFigure 5.4: t-SNE visualization of MuLane dataset. The source domain is marked in\nblue, the real-world model vehicle target domain in red, and the TuSimple domain\nin green. Best viewed in color.\nQualitative Evaluation. We use t-SNE [258] to visualize the features of the fea-\ntures encoders for the source and target domains of MuLane in Figure 5.4. t-SNE\nvisualizations of MoLane and TuLane can be found in Appendix C. In accordance\nwith the quantitative results, we observe only a slight adaptation of the source and\ntarget domain features for ADDA, SGADA, and SGPCS compared to the supervised\nbaseline UFLD-SO. Consequently, the examined well-known domain adaptation\nmethods have no significant effect on feature alignment. In addition, we show\nresults from the evaluated methods in Figure 5.5 and observe that the models are\nable to predict target domain lane annotations in many cases but are not able to\nachieve comparable results to the supervised baseline (UFLD-TO).\n79\nCHAPTER 5. A LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN\nADAPTATION\nMoLane\nTuLane\nMuLane\nUFLD-SO\nDANN\nADDA\nSGADA\nSGPCS\nUFLD-TO\nFigure 5.5: Qualitative results of target domain predictions. Ground truth lane\nannotations are marked in blue, and predictions in red. Best viewed in color.\nIn summary, we find quantitatively and qualitatively that the examined domain\nadaptation methods do not significantly improve the performance of lane detection\nand feature adaptation. For this reason, we believe that the proposed benchmark\ncould facilitate the exploration of new domain adaptation methods to overcome\nthese problems.\n5.6\nConclusion\nWe present CARLANE, the first unsupervised domain adaptation benchmark for\nlane detection. CARLANE was recorded in three domains and consists of three\ndatasets: the single-target datasets MoLane and TuLane and the multi-target dataset\nMuLane, which is a balanced combination of both. Based on the UFLD model,\nwe have conducted experiments with different unsupervised domain adaptation\nmethods on CARLANE and found that the selected methods are able to adapt the\nmodel to target domains slightly and consistently. However, none of the methods\nachieve comparable results to the supervised baselines. The most significant per-\nformance differences are noticeable in the high false positive and false negative\n80\n5.6. CONCLUSION\nrates of the unsupervised domain adaptation methods compared to the target-only\nbaselines, which is even more pronounced in the MuLane multi-target task. These\nfalse-positive and false-negative rates can negatively impact autonomous driving\nfunctions since they represent misidentified and missing lanes. Furthermore, as\nshown in the t-SNE plots of Figure 5.4, the examined well-known domain adaptation\nmethods have no significant effect on feature alignment. The current difficulties of\nthe examined unsupervised domain adaptation methods to align the source and tar-\nget domains adequately confirm the need for the proposed CARLANE benchmark.\nWe believe that CARLANE eases the development and comparison of unsupervised\ndomain adaptation methods for lane detection. In addition, we open-source all\ntools for dataset creation and labeling and hope that CARLANE facilitates future\nresearch in these directions.\nLimitations. One limitation of our work is that we only use a fixed set of track\nelements within our 1/8th scaled environment. These track elements represent\nonly a limited number of distinct curve radii. Furthermore, neither buildings nor\ntraffic signs exist in MoLane’s model vehicle target domain. Moreover, the full-scale\nreal-world target domain of TuLane is derived from TuSimple. TuSimple’s data was\npredominantly collected under good and medium conditions and lacks variation in\nweather and time of day. In addition, we want to emphasize that collecting data for\nautonomous driving is still an ongoing effort and that datasets such as TuSimple\ndo not cover all possible real-world driving scenarios to ensure safe, practical use.\nFor the synthetically generated data, we limited ourselves to using existing CARLA\nmaps without defining new simulation environments. Despite these limitations,\nCARLANE serves as a supportive dataset for further research in the field of unsuper-\nvised domain adaptation.\nEthical and Responsible Use. Considering the limitations of our work, unsuper-\nvised domain adaptation methods trained on TuLane and MuLane should be tested\nwith care and under the right conditions on a full-scale car. However, real-world\ntesting with MoLane in the model vehicle domain can be carried out in a safe and\ncontrolled environment. Additionally, TuLane contains open-source images with\nunblurred license plates and people. This data should be treated with respect and\nin accordance with privacy policies. In general, our work contributes to the re-\nsearch in the field of autonomous driving, in which a lot of unresolved ethical and\nlegal questions are still being discussed. The step-by-step testing possibility across\nthree domains makes it possible for our benchmark to include an additional safety\nmechanism for real-world testing.\n81\n6 Content-Consistent Translation with Masked\nDiscriminators\nPFD→Cityscapes\nDay→Night\nViper→Cityscapes\nClear→Snowy\nFigure 6.1: Results of our method. Best viewed in color.\nA common goal of unpaired image-to-image translation is to preserve content\nconsistency between source images and translated images while mimicking the\nstyle of the target domain. Due to biases between the datasets of both domains,\nmany methods suffer from inconsistencies caused by the translation process.\nMost approaches introduced to mitigate these inconsistencies do not constrain\nthe discriminator, leading to an even more ill-posed training setup. Moreover,\nnone of these approaches is designed for larger crop sizes. In this work, we show\nthat masking the inputs of a global discriminator for both domains with a content-\nbased mask is sufficient to reduce content inconsistencies significantly. However,\nthis strategy leads to artifacts that can be traced back to the masking process.\nTo reduce these artifacts, we introduce a local discriminator that operates on\npairs of small crops selected with a similarity sampling strategy. Furthermore,\nwe apply this sampling strategy to sample global input crops from the source\nand target dataset. In addition, we propose feature-attentive denormalization to\nselectively incorporate content-based statistics into the generator stream. In our\nexperiments, we show that our method achieves state-of-the-art performance in\nphotorealistic sim-to-real translation and weather translation and also performs\nwell in day-to-night translation. Additionally, we propose the cKVD metric, which\nbuilds on the sKVD metric and enables the examination of translation quality at\nthe class or category level.\n83\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\n6.1\nMotivation\nUnpaired image-to-image translation aims at transferring images from a source\ndomain to a target domain when no paired examples are given. Recently, this\nfield has attracted increasing interest and has advanced several use cases, such as\nphotorealism [128, 209, 216], neural rendering [102], domain adaptation [113, 219],\nthe translation of seasons or daytime [127, 130, 209], and artistic style transfer [119,\n140, 283]. Current work has primarily focused on improving translation quality [127,\n193], efficiency [160, 235], multi-modality [120, 164], and content consistency [128,\n216]. Due to the ill-posed nature of the unpaired image-to-image translation task\nand biases between datasets, content consistency is difficult to achieve. To mitigate\ncontent inconsistencies, several methods have been proposed that constrain the\ngenerator of GANs [20, 74, 120, 164, 170, 233, 282, 295, 303, 307]. However, only\nconstraining the generator leads to an unfair setup, as biases in the datasets can be\ndetected by the discriminator: The generator tries to achieve content consistency\nby avoiding biases in the output, while the discriminator is still able to detect biases\nbetween both datasets and, therefore, forces the generator to include these biases\nin the output, for example, through hallucinations. Constraining the discriminator\n[161, 216, 251] or improving the sampling of training pairs [137, 216] is currently\nunderexplored, especially for content consistency on a global level, where the\ndiscriminator has a global view on larger image crops instead of a local view on\nsmall crops.\n6.2\nContributions\nIn this work, we propose masked conditional discriminators, which operate on\nmasked global crops of the inputs to mitigate content inconsistencies. We combine\nthese discriminators with an efficient sampling strategy based on a pre-trained\nrobust segmentation model to sample similar global crops. Furthermore, we argue\nthat when transferring feature statistics from the content stream of the source im-\nage to the generator stream, content-unrelated feature statistics from the content\nstream could affect image quality if the generator is unable to ignore this informa-\ntion since the output image should mimic the target domain. Therefore, we propose\na feature-attentive denormalization (FATE) block that extends feature-adaptive de-\nnormalization (FADE) [130] with an attention mechanism. This block allows the\ngenerator to selectively incorporate statistical features from the content stream\ninto the generator stream. In our experiments, we find that our method achieves\nstate-of-the-art performance on most of the benchmarks shown in Figure 6.1.\n84\n6.3. METHOD\nOur contributions can be summarized as follows:\n• We propose an efficient sampling strategy that utilizes robust semantic seg-\nmentations to sample similar global crops. This reduces biases between both\ndatasets induced by semantic class misalignment. (RQ-T2)\n• We combine this strategy with masked conditional discriminators to achieve\ncontent consistency while maintaining a more global field of view. (RQ-T2)\n• We extend our method with an unmasked local discriminator. This discrimi-\nnator operates on local, partially class-aligned patches to minimize the under-\nrepresentation of frequently masked classes and associated artifacts. (RQ-T2)\n• We propose a feature-attentive denormalization (FATE) block, which selec-\ntively fuses statistical features from the content stream into the generator\nstream. (RQ-T3)\n• We propose the class-specific Kernel VGG Distance (cKVD) that builds upon\nthe semantically aligned Kernel VGG Distance (sKVD) [216] and uses robust\nsegmentations to incorporate class-specific content inconsistencies in the\nperceptual image quality measurement. (RQ-E7)\n• In our experiments, we show that our method achieves state-of-the-art perfor-\nmance on photo-realistic sim-to-real transfer and the translation of weather\nand performs well for daytime translation. (RQ-E8)\n6.3\nMethod\nWe propose an end-to-end framework for unpaired image-to-image translation that\ntransfers an image Ia ∈R3×h×w from a source domain a to an image Fb ∈R3×h×w\nfrom a target domain b. Our goal is to design a method for content-consistent\ntranslations that utilizes a simple masking strategy for the global crops seen by\nthe discriminators. We achieve this by combining an efficient segmentation-based\nsampling method that samples large crops from the input image with a masked\ndiscriminator that operates on these global crops. This is in contrast to EPE [216],\nwhich achieves content-consistent translation at the local level by sampling small,\nsimilar image crops from both domains. To further improve image quality, we use a\nlocal discriminator that operates on a batch of small image patches sampled from\nthe global input crops utilizing our sampling method. An overview of our method is\nshown in Figure 6.2. Furthermore, we propose a feature-attentive denormalization\n(FATE) block that extends feature-adaptive denormalization (FADE) [130] with an\n85\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nLocal\nDiscriminator\nGlobal \nDiscriminators\nGenerator\nSimilarity Sampling\nLocal\nSampling\nSampling \nGenerating\nMasking\nDiscriminators\nCreate\nMask\nCrop ia & za\nCrop mab\nCrop ib & cb\nFake fb & \n Crop ca \nMasked \nib & cb\nImage Ib\nCondition Cb\nCondition Ca\nImage Ia & Condition Za\nMask Mab\nMasked \nfb & ca\nLocal Crops fb & ca \nLocal Crops ib & cb \nFigure 6.2: Method overview. In our method, similar image crops from both do-\nmains (ia, ib) and their corresponding conditions (ca, cb, za) are selected via a\nsampling procedure. In this sampling procedure, a mask Mab is created from the\nconditions Ca and Cb. This mask is used to sample crops from both datasets for\nwhich the semantic classes align by at least 50%. The cropped mask mab is also\nused to mask the generated fake image fb, the real images ib, and the correspond-\ning conditions for the global conditional discriminators. Through the mask, these\ndiscriminators can only see the parts of the crop where the semantic classes align.\nTo further improve image quality, a local discriminator is introduced that works on\na batch of small patches selected from the crop using our sampling technique. This\ndiscriminator is not masked and works on patches where the semantic classes do\nnot fully align. Best viewed in color.\nattention mechanism, allowing the generator to selectively incorporate statistical\nfeatures from the content stream of the source image into the generator stream.\n6.3.1\nContend-based Similarity Sampling\nTo minimize the bias between both datasets in the early stage of our method, we\nsample similar image crops with an efficient sampling procedure. This procedure\nuses the one-hot encoded semantic segmentations Ca ∈Rd×h×w and Cb ∈Rd×h×w\nof both domains, where d is the channel dimension of the one-hot encoding. In\nour case, these segmentations are created with the robust pre-trained MSeg model\n[150]. First, a mask Mab ∈R1×h×w is computed from the segmentations:\nMab = max\nd (Ca ◦Cb),\n(6.1)\nwhere ◦denotes the Hadamard product. We can now sample semantically aligned\nimage crops ia and ib from the images Ia and Ib with the crop mab from mask Mab.\nThereby, we calculate the percentage of overlap of semantic classes between both\n86\n6.3. METHOD\nimage crops as follows:\nPmatch(ia) = {ib | mean(mab) > t},\n(6.2)\nwhere t is the similarity sampling threshold. In our case, we sample crops where\nmore than 50% of the semantic classes align (t > 0.5). We use this procedure to\nsample crops ca, cb, and zb from the discriminator conditions Ca, Cb, and the\ngenerator condition Zb as well. The cropped mask mab is also used for our masked\nconditional discriminator.s\n6.3.2\nContend-based Discriminator Masking\nTo train a discriminator with a global field of view that facilitates the usage of\nglobal properties of the scene, while maintaining content consistency, we mask\nthe discriminator input from both domains with a content-based mask mab. This\nmask erases all pixels from the discriminator input where the semantic classes do\nnot align. This removes the bias between both datasets caused by the underlying\nsemantic class distribution of the two domains without directly restricting the\ngenerator. The objective function of a conditional GAN with a masked discriminator\nthat transfers image crops ia to domain b can be then defined as follows:\nLmadv = Eib,cb,mab[logD(ib ◦mab|cb ◦mab)]\n+Eia,za,ca,mab[log(1−D(G(ia|za)◦mab|ca ◦mab))].\n(6.3)\nTo ensure that the discriminator does not use the segmentation maps as learning\nshortcuts, we follow [216] and create the segmentations of both datasets using a\nrobust segmentation model such as MSeg [150]. With this setting, we are able to\ntrain discriminators with large crop sizes with significantly reduced hallucinations\nin the translated image.\n6.3.3\nLocal Discriminator\nMasking the input of the discriminator may lead to the underrepresentation of\nsome semantic classes. Therefore, we additionally train a local discriminator that\noperates on a batch of small patches sampled from the global crop. Our local\ndiscriminator is not masked but only sees patches where a certain amount of the\nsemantic classes align. In our case, we sample patches with 1/8th the size of the\nglobal input crop where more than 50% of the semantic classes align. We use our\nsampling procedure from subsection 6.3.1 to sample these patches. Using small,\npartially aligned patches ensures that semantic classes are less underrepresented\n87\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nInput\nFeatures\nAttention\nb) Feature-attentive Denormalization\n(FATE)\nOutput\nNorm\na) Feature-Adaptive Denormalization\n(FADE)\nInput\nFeatures\nOutput\nNorm\nAttention\nFigure 6.3: FADE and FATE.\nwhile maintaining content consistency.\n6.3.4\nFeature-attentive Denormalization (FATE)\nSpatially adaptive denormalization (SPADE) [205] fuses resized semantic segmenta-\ntion maps as content into the generator stream. Feature-adaptive denormalization\n(FADE) [130] generalizes SPADE to features learned through a content stream. As\nshown in Figure 6.3, the normalized features N(h) of the generator are modulated\nwith the features f of the content stream using the learned functions γ and β as\nfollows:\nFADE(h, f ) = N(h)◦γ(f )+β(f ),\n(6.4)\nwhere γ and β are one-layer convolutions. This denormalization is applied in sev-\neral layers of the generator. However, we argue that denormalization with content\nfeatures is not always appropriate for transferring images to another domain be-\ncause, as shown in [79, 119, 156, 159], image feature statistics contain not only\ncontent information but also style information. When transferring feature statistics\nfrom the content stream of the source image to the generator stream, style infor-\nmation from the source image could affect the final image quality if the generator\ncannot ignore this information since the output image should mimic the style of\nthe target domain. Therefore, we propose an additional attention mechanism to\nselectively incorporate statistics from the content stream into the generator stream.\nThis allows the model to only fuse the statistical features from the source image\ninto the generator stream that are useful for the target domain. As shown in Fig-\nure 6.3, this attention mechanism relies on the features of the content stream and\n88\n6.4. EXPERIMENTS\nthe features of the generator stream and attends to the statistics γ and β. With this\nattention mechanism, we can extend FADE to feature-attentive denormalization\n(FATE) as follows:\nFATE(h, f ) = N(h)◦A(h, f )◦γ(f )+ A(h, f )◦β(f ),\n(6.5)\nwhere A is the attention mechanism and A(h, f ) is the attention map for the statis-\ntics. We use a lightweight two-layer CNN with sigmoid activation in the last layer as\nthe attention mechanism. More details can be found in Appendix D.\n6.3.5\nTraining Objective\nOur training objective consists of three losses: a global masked adversarial loss\nL global\nmadv , a local adversarial loss L local\nadv , and the perceptual loss Lperc used in [130].\nWe define the final training objective as follows:\nL = λglobal\nmadv L global\nmadv +λlocal\nadv L local\nadv\n+λpercLperc,\n(6.6)\nwhere we use a hinge loss to formulate the adversarial losses and λglobal\nmadv , λlocal\nmadv,\nλperc are the corresponding loss weights.\n6.4\nExperiments\n6.4.1\nExperimental Settings\nImplementation details. Our method is implemented in PyTorch 1.10.0 and trained\non an A100 GPU (40 GB) with batch size 1. For training, we initialize all weights with\nthe Xavier normal distribution [86] with a gain of 0.02 and use an Adam optimizer\n[141] with β1 = 0.9 and β2 = 0.999. The initial learning rates of the generator and\ndiscriminators are set to 0.0001 and halved every de epochs. Learning rate decay is\nstopped after reaching a learning rate of 0.0000125. We formulate our adversarial\nobjective with a hinge loss [163] and weight the individual parts of our loss function\nas follows: λglobal\nmadv = 1.0,λlocal\nmadv = 1.0, λperc = 1.0. In addition, we use a gradient\npenalty on target images [97, 184] with λr p = 0.03. The images of both domains are\nresized and cropped to the same size and randomly flipped before the sampling\nstrategy is applied. In our experiments, we show that we achieve the best perfor-\nmance by cropping global patches of size 352×352. We crop local patches with\n1/8th the size of the global crop (i.a., 44×44). The global discriminators are used on\ntwo scales. Crops are scaled down by a factor of two for the second scale. We train all\n89\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nour models for ∼400K iterations. Training a model takes 4-8 days, depending on the\ndataset, model, and crop size. We report all results as an average across five different\nruns. We refer to Appendix D for more details regarding the training and model. Our\nimplementation is publicly available at https://github.com/BonifazStuhr/feamgan.\nMemory usage. Our best model requires ∼25 GB of VRAM at training time and\nperforms inference using ∼12 GB for an image of size 957×526. Our small model,\nwith a slight performance decrease, runs on consumer graphic cards with ∼9 GB\nof VRAM at training time and performs inference using ∼8 GB for an image of size\n957×526.\nDatasets. We conduct experiments on four translation tasks across four datasets.\nFor all datasets, we compute semantic segmentations with MSeg [150], which we\nuse as a condition for our discriminator and to calculate the discriminator masks.\n(1) PFD [218] consists of images of realistic virtual world gameplay. Each frame is\nannotated with pixel-wise semantic labels, which we use as additional input for our\ngenerator. We use the same subset as [216] to compare with recent work.\n(2) Viper [217] consists of sequences of realistic virtual world gameplay. Each frame\nis annotated with different labels, where we use the pixel-wise semantic segmenta-\ntions as additional input for our generator. Since Cityscapes does not contain night\nsequences, we remove them from the dataset.\n(3) Cityscapes [50] consists of sequences of real street scenes from 50 different\nGerman cities. We use the sequences of the entire training set to train our models.\nWe use datasets (1-3) for the sim-to-real translation tasks PFD→Cityscapes and\nViper→Cityscapes.\n(4) BDD100K [285] is a large-scale driving dataset. We use subsets of the training\nand validation data for the following translation tasks: Day→Night, Clear→Snowy.\nCompared methods. We compare our work with the following methods.\n• Color Transfer (CT) [214] performs color correction by transferring statistical\nfeatures in lαβ space from the target to the source image.\n• MUNIT [120] achieves multimodal translation by recombining the content\ncode of an image with a style code sampled from the style space of the target\ndomain. It is an extension of CycleGAN [307] and UNIT [170].\n• CUT [204] uses a patchwise contrastive loss to achieve one-sided unsuper-\n90\n6.4. EXPERIMENTS\nvised image-to-image translation.\n• TSIT [130] achieves one-sided translation by fusing features from the con-\ntent stream into the generator on multiple scales using FADE and utilizing a\nperceptual loss between the translated and source images.\n• QS-Attn [117] builds upon CUT [204] with an attention module that selects\nsignificant anchors for the contrastive loss instead of features from random\nlocations of the image.\n• EPE [216] relies on a variety of gbuffers as input. Techniques such as similarity\ncropping, utilizing segmentations for both domains generated by a robust\nsegmentation model as input to the conditional discriminators, and small\npatch training are used to achieve content consistency.\nSince EPE [216] provides inferred images of size 957×526 for the PFD→Cityscapes\ntask, comparisons are performed on this resolution. For the Viper→Cityscapes,\nDay→Night, and Clear→Snowy tasks, we train the models using their official im-\nplementations. Furthermore, we retrain models as additional baselines for the\nPFD→Cityscapes task.\nEvaluation metrics. Following prior work [216], we use the Fréchet Inception\nDistance (FID) [108], the Kernel Inception Distance (KID) [23], and the semantically\naligned Kernel VGG Distance (sKVD) [216] to evaluate image translation quality\nquantitatively. The sKVD metric was introduced in [216] and improved over pre-\nvious metrics for mismatched layouts in source and target data. In addition, we\npropose the class-specific Kernel VGG Distance (cKVD), where a robust segmen-\ntation model is used before the sKVD calculation to mask input crops by class (or\ncategory). Thereby, for each given class, all source and target image crops are fil-\ntered using their segmentations by erasing the pixels of all other classes. We select\ncrops where more then 5% of the pixels belong to the respective class. Then, the\nsKVD is calculated class-wise on the filtered crops. Afterward, we can report the\ncKVD as an average over all classes or separately for each class to achieve a more\nfine-grained measurement. We follow [216] and use a crop size of 1/8 and sample\nsource and target crop pairs with an similarity threshold of 0.5 between unmasked\nsource and target segmentation crops. More information on the classes used in\nthe cKVD metric can be found in Table D.1 of Appendix D. For the KID, sKVD, and\ncKVD metrics, we multiply the measurements by 1000 to improve the readability of\nresults.\n91\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nInput\nEPE\nFeaMGAN (ours)\nGenerator Input Types\nFigure 6.4: Qualitative comparison to EPE. We compare our method with the pro-\nvided inferred images of EPE [216]. Best viewed in color.\nTable 6.1: Quantitative comparison to the baselines provided by EPE. We calculate\nall metrics on the provided inferred images of EPE and its baselines [216].\nMethod\nFID\nKID\nsKVD\ncKVD\nAVG AVGsp\nsky\nground road terrain vegetation building roadside-obj. person vehicle\nrest\nColorTransfer\n84.34 88.17 16.65 36.01\n33.12\n32.40\n12.97\n16.13\n20.94\n19.24\n29.92\n74.79\n62.78\n41.79\n49.16\nMUNIT\n45.00 35.05 16.51 38.57\n34.81\n29.80\n16.93\n17.62\n29.52\n19.29\n24.28\n79.14\n77.34\n40.13\n51.61\nCUT\n47.71 42.01 18.03 35.31\n33.26 25.96\n15.32\n17.87\n20.09\n22.72\n25.00\n74.02\n60.99\n41.71\n49.37\nEPE\n44.06 33.66 13.87 35.22 30.21 27.14\n13.54\n13.56\n24.77\n20.77\n26.75\n50.58\n83.34\n41.29\n50.45\nFeaMGAN-S (ours) 43.27 32.59 12.98 40.23\n32.69\n38.10\n13.29\n15.34\n26.29\n20.17\n27.32\n61.57\n102.65\n42.83\n54.73\nFeaMGAN (ours)\n40.32 28.59 12.94 40.02\n31.78\n46.70\n13.72\n15.60\n23.23\n17.69\n25.57\n66.65\n99.24\n39.38\n52.40\n6.4.2\nComparison to the State of the Art\nWe compare our models quantitatively and qualitatively with different baselines.\nFirst, we compare our results with EPE and the baselines provided by EPE [216].\nThen, we train our own baselines on the four translation tasks for further compari-\nson.\nComparison to EPE. A set of inferred images is provided for EPE and each of\nthe baselines [216]. Therefore, we train our models on the same training set and\nuse the inferred images from our best models for this comparison. We select our\nbest models based on scores of various visual metrics and visual inspections of\n92\n6.4. EXPERIMENTS\ntranslated images. As shown in Figure 6.4 a) and b), our model relies solely on\nsegmentation maps as additional input compared to EPE, which uses a variety of\ngbuffers. In addition, our model is trained with significantly fewer steps (∼400K\niterations) compared to EPE and the baselines (1M iterations). As shown in Table 6.1,\nour model outperforms the baselines and EPE in all commonly used metrics (FID\nand KID) and the sKVD metric. More surprisingly, our small model, which can be\ntrained on consumer GPUs, outperforms all baselines and EPE as well.\nHowever, our cKVD metric shows that our models have difficulty with the person\nand sky classes. Therefore, the average cKVD values are high and become low when\nwe remove both classes from the average calculation (AVGsp). A possible reason\nfor the weaker performance on the person class is our masking procedure. Since\nthe masking procedure requires overlapping samples in both domains, the person\nclass is not seen frequently during training. This can lead to inconsistencies (a glow)\naround the person class, as seen in Figure 6.11 of our limitations. The masking\nprocedure also leads to a drop in performance in the sky class, as seen in Table 6.3\nof our ablation study.\nAs shown in the first row of Figure 6.4 and the results of Figures D.7 and D.8 of\nAppendix D, our model translates larger structures, such as lane markings, more\nconsistently, but fails to preserve some in-class characteristics from the source\ndataset. This is evident, for example, in the structure of translated streets and the\ncorresponding cKVD value (road). As shown in the second row and Appendix D, EPE\nachieves visually superior modeling of the reflective properties of materials (e.g.,\nthe car) but suffers from inconsistencies (erased objects) regarding the vegetation,\nwhich can be seen in the palm trees and the corresponding cKVD value (vegetation).\nThe superior modeling of reflective properties can be attributed to the availability\nof gbuffers (i.a., glossines) in EPE’s input.\nBy surpassing EPE in all commonly used quantitative metrics while maintaining\ncontent consistency, we are able to show that our model improves overall quantita-\ntive translation performance. However, our method has specific drawbacks that we\ndiscussed with the help of the cKVD metric and visual comparisons.\n93\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nPFD→Cityscapes\nViper→Cityscapes\nDay→Night\nClear→Snowy\nInput\nMUNIT\nCUT\nTSIT\nQS-Attn\nFeaMGAN (ours)\nFigure 6.5: Qualitative comparison to prior work. Models were trained using their\nofficial implementations. Randomly sampled results can be found in Figure D.9 of\nAppendix D. Best viewed in color.\nTable 6.2: Quantitative comparison to prior work. Models were trained using their\nofficial implementations. Results are reported as the average across five runs. We\nrefer to Table D.4 of Appendix D for an extended version of this table.\nMethod\nPFD→Cityscapes\nViper→Cityscapes\nDay→Night\nClear→Snowy\nFID\nKID\nsKVD cKVD\nFID\nKID\nsKVD cKVD\nFID\nKID\nsKVD cKVD\nFID\nKID\nsKVD cKVD\nColor Transfer\n91.01 94.82 18.16 50.87 89.30 83.51 20.20 51.23 125.90 140.60 32.58 56.52 46.85 19.44 14.91 42.89\nMUNIT\n40.36 29.98 14.99 43.24 47.96 30.35 14.14 59.62\n42.53\n31.83\n15.02 50.83 44.74 17.48 11.65 48.10\nCUT\n49.55 44.25 16.85 37.53 60.35 49.48 16.80 51.02 34.36\n20.54 10.16 53.55 46.03 15.70 14.71 43.91\nTSIT\n38.70 28.70 10.80 42.35 45.26 28.40 8.47\n50.03\n54.96\n33.21\n12.71 57.91 79.28 40.02 12.97 41.52\nQS-Attn\n49.41 42.87 14.01 38.57 55.62 39.31 12.99 63.22\n46.67\n21.47\n7.58\n52.02 60.91 18.85 14.19 44.00\nFeaMGAN-S (ours) 45.16 34.93 13.87 40.50 52.79 35.92 14.34 45.38 70.40\n51.30\n14.68 46.66 57.93 16.24 11.88 38.28\nFeaMGAN (ours)\n46.12 36.56 13.69 41.19 51.56 34.63 14.01 47.21 66.39\n46.96\n13.14 46.88 56.78 14.77 11.36 41.72\nComparisons to retrained baselines. We find that retraining the baselines with their\noriginal training setup for the PFD→Cityscapes task significantly improves their per-\nformance on commonly used metrics compared to the baselines provided by EPE,\nas can be seen in Table 6.2. However, as shown in Figure 6.5 and the random results\nof Figure D.9 of Appendix D, content-consistency problems remain. This indicates\nagain that simply relying on commonly used metrics does not provide a complete\npicture if content consistency is taken into account. When qualitatively comparing\n94\n6.4. EXPERIMENTS\nInput\nFull\nw/o Dis. Mask\nw/o Local Dis.\nw/ FADE w/o FATE\nFigure 6.6: Qualitative ablations. Results are selected from the best model. Ran-\ndomly sampled results can be found in Figure D.11 of Appendix D. Best viewed in\ncolor.\nTable 6.3: Quantitative evaluation for ablation study. Results are reported as the\naverage across five runs. We refer to Table D.5 of Appendix D for an extended version\nof this table.\nMethod\nFID\nKID\nsKVD\ncKVD\nAVG\nsky\nground road terrain vegetation building roadside-obj. person vehicle\nrest\nFeaMGan (Full)\n46.12 36.56 13.69 41.19 42.69\n14.97\n17.35\n26.51\n20.25\n26.34\n64.64\n102.23\n42.38\n54.52\nw/o Dis. Mask\n37.10 25.88 14.73 39.65 26.70\n15.81\n16.65\n31.02\n22.97\n25.39\n67.01\n93.78\n44.23\n52.91\nw/ FADE w/o FATE 45.46 35.73 13.17 40.90 41.49\n13.78\n16.78\n25.30\n20.58\n27.21\n63.12\n104.43\n42.44\n53.83\nw/ Random Crop\n47.88 38.48 13.37 40.18 39.88\n12.90\n14.65 25.09\n21.89\n27.32\n64.32\n98.81\n43.08\n53.86\nw/ VGG Crop\n51.23 42.46 13.56 40.62 40.32\n13.38\n15.67\n26.47\n21.09\n27.28\n65.23\n99.61\n43.19\n53.94\nw/o Local Dis.\n- w/ 256×256 Crop 48.57 38.89 12.89 41.26 42.31\n13.57\n15.98\n25.28\n22.18\n26.56\n61.13\n107.48\n42.44\n55.62\n- w/ 352×352 Crop 47.26 37.75 14.38 39.30 34.44\n13.09\n15.84\n25.83\n21.50\n27.20\n61.24\n98.25\n42.24\n53.38\n- w/ 464×464 Crop 46.61 37.25 15.04 38.62 31.60\n13.13\n15.38\n27.06\n22.23\n29.67\n63.38\n87.51\n44.41\n51.77\n- w/ 512×512 Crop 55.89 49.12 15.94 39.35 36.48\n14.68\n16.06\n26.87\n19.61\n27.37\n62.40\n98.90\n40.32\n50.86\nour model to the baselines for the PFD→Cityscapes and Viper→Cityscapes tasks\nin Figure 6.5, we observe that our method significantly reduces content inconsis-\ntencies. However, a limitation of our masking strategy are class boundary artifacts,\nwhich are particularly evident in the Day→Night translation task (Figure 6.11). Since\nmasking allows our method to focus on specific classes, we achieve state-of-the-art\nperformance for the Clear→Snowy translation task.\n6.4.3\nAblation Study\nEffectiveness of masked discriminator. As shown in Figure 6.6 and the random\nsamples in Figure D.11 of Appendix D, our masking strategy for the discriminator\n95\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\npositively impacts content consistency. Without masking, inconsistencies occur\nthat correlate with biases between the class distributions of the source and target\ndomains. As shown in [216], the distributions of certain classes in the spatial image\ndimension vary greatly between the PFD dataset and the Cityscapes dataset. For\nexample, trees in Cityscapes appear more frequently in the top half of the image,\nresulting in hallucinated trees when the images are translated without accounting\nfor biases. In the first and second row of Figure 6.6, we show that our masking\nstrategy (Full) prevents these inconsistencies in contrast to our model trained with-\nout masking (w/o Dis. Mask). However, as shown in Table 6.3, this comes with a\nquantitative tradeoff in performance on commonly used metrics.\nEffectiveness of local discriminator. We compare our model trained with a local\ndiscriminator (Full) to the model trained without a local discriminator (w/o Local\nDis. 352x352). As shown in Figure 6.6, the local discriminator leads to an increase in\nquantitative performance. Furthermore, we show the qualitative effects of the local\ndiscriminator in Figure 6.6, where we observe a decrease in glowing objects and a\nsignificant decrease of erased objects in the translation. An example of a glowing\nobject is the palm tree in row two of Figure 6.6. An example of erased objects are the\nmissing houses in the background of the images from row three. In addition, small\ninconsistencies near object boundaries are reduced, as shown by the randomly\nsampled results in Figure D.11 of Appendix D (e.g., the wheels of the car in row one\nand three). Overall, we can conclude that local discriminators can reduce local\ninconsistencies, which might arise from the robust but not flawless segmentation\nmaps used for masking.\nEffectiveness of segmentation-based sampling. We compare our segmentation-\nbased sampling method with random sampling and sampling based on VGG fea-\ntures. For the sampling strategy based on VGG features, we follow EPE [216] to\ncalculate scores for 352×352 crops of the input images. Crops with a similarity score\nhigher than 0.5 are selected for training. As shown in Table 6.3, our segmentation-\nbased sampling strategy (Full) slightly outperforms the other sampling strategies in\noverall translation performance.\nEffectiveness of FATE. For each spatial point (\"pixel\") in the input feature map, our\nfeature-attentive denormalization block selects the features in the feature dimen-\nsion to be incorporated into the output stream of the generator by denormalization.\nWe show the attention values of our feature-attentive denormalization block in Fig-\nure 6.7 by visualizing all attention values for a single feature across the entire feature\nmap. Since a single feature represents a property of the input, a spatial pattern\nshould emerge. This is expected especially in earlier layers, where the spatiality of\n96\n6.4. EXPERIMENTS\n957×526 Input 957×526 Output 957×526 Layer\n478×263 Layer\n119×65 Layer\n59×32 Layer\nFigure 6.7: FATE attention maps. Results are selected from the best model. Best\nviewed in color.\nthe convolutional model’s feature map is best preserved. As shown in Figure 6.7,\nour attention mechanism learns to attend to features that correlate with a property.\nExamples are the shadows of a scene (row 1), cars and their lighting (row 2), and\nvegetation (row 3). In addition, we find increasingly more white feature maps in\ndeeper layers. This can be interpreted positively as an indication that the learned\ncontent (source) features in deeper layers are important for the translation task\nand that more shallow content features of earlier layers are increasingly ignored.\nHowever, this can also be interpreted negatively and could indicate that our simple\nattention mechanism is not able to separate deeper features properly.\nComparing FATE to FADE, we find that FATE leads to a subtle increase in training\ninstability, resulting in slightly worse average performance over the five runs per\nmodel. However, FATE also leads to our best models. Therefore, we select the FATE\nblock as the standard configuration for our model. The deviation from the average\nvalues for all runs can be found in Table D.5 of Appendix D. The slight increased\ninstability suggests that the attention mechanism of FATE can be further improved.\nEffect of global crop size. We successively increase the global crop size of the\ngenerator and discriminators from 256×256 to 512×512 and examine the effects on\ntranslation performance. As shown in Figure 6.6, increasing the global crop size\nresults in a better approximation of the target domain style. However, increasing\nthe global crop size also leads to an increasing number of artifacts in the translated\nimage. In Figure 6.9, we report the score of various metrics with respect to the global\ncrop size. The commonly used metrics for measuring translation quality (IS, FID,\nand KID) show that translation quality increases steadily up to a global crop size\nof 464×464, after which the results become unstable. The cKVD metric also shows\nan increase in average performance up to a crop size of 464×464, mainly because\n97\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nInput\n252×252\n352×352\n464×464\n512×512\nFigure 6.8: Qualitative ablation of crop sizes. For each crop size, results are selected\nfrom the best model. Randomly sampled results can be found in Figure D.10 of\nAppendix D. Best viewed in color.\n256\n352\n464\n512\n4.6\n4.7\n4.8\n4.9\n5.0\nScore / Distance\nIS\n256\n352\n464\n512\n50\n60\n70\nFID\n256\n352\n464\n512\nGlobal Crop Size\n40\n50\n60\n70\nKID\n256\n352\n464\n512\n12\n14\n16\n18\n20\nsKVD\n256\n352\n464\n512\n36\n38\n40\n42\n44\ncKVD\nFigure 6.9: Quantitative ablation of crop sizes.\ntranslation quality for the underrepresented person class increases. This is intuitive\nsince a larger crop size leads to a more frequent appearance of underrepresented\nclasses during training. Furthermore, the sKVD metric shows a steady decline\nin consistency as the global crop size increases. Therefore, we choose a tradeoff\nbetween approximation of the target domain style, artifacts, and computational\ncost, and select 352×352 as the global crop size for our model.\n6.5\nConclusion\nIn this work, we have shown that content-based masking of the discriminator is\nsufficient to significantly reduce content inconsistencies that arise in unpaired\nimage-to-image translation. Furthermore, artifacts caused by the masking proce-\ndure can be significantly reduced by introducing a local discriminator that utilizes a\nsegmentation-based similarity sampling technique. Moreover, our similarity sam-\n98\n6.5. CONCLUSION\npling technique leads to a further increase in performance when applied to global\ninput crops. We have also shown that our feature-based denormalization block is\nable to attend to specific content features, such as features of shadows, but can\nslightly increase training instability. In addition, we have proposed the cKVD metric\nto examine translation quality at the class or category level. In our experiments, we\nhave found that these techniques lead to state-of-the-art performance on photo-\nrealistic sim-to-real transfer and the translation of weather. Although our method\nperforms well in Day→Night translation, the remaining limitations of our approach\nare especially evident in this task.\nLimitations. We remark on limitations regarding the dataset, sampling, method,\nand implementation. Probably the most significant limitations are the complex\npublic datasets currently available and in use, as they are not specifically designed\nfor unpaired translation. Collection strategies and datasets that mitigate biases\nbetween source and target domains would be beneficial. Furthermore, our sam-\npling strategy only works on an image basis and could be extended across the entire\ndataset to sample more significant pairs for training. Although our method works\nfor large crops, there is still a crop size limit that must be taken into account when\ntuning the hyperparameters. In addition, our method for mitigating content incon-\nsistencies depends on the segmentation model. In theory, the number of classes\ncould be used to control how fine-grained the content consistency should be, which\nleads to flexibility but allows for errors depending on the segmentation quality. This\ncan result in artifacts such as glowing objects, as shown in Figure 6.11. Intra-class\ninconsistencies that may arise from intra-class biases ignored by the loss, such as\nsmall textures, represent another problem. Intra-class inconsistencies are currently\nunderexplored in unpaired image-to-image translation and are an interesting di-\nrection for future research. Finally, we would like to point out that the efficiency\nof our implementation could be further improved. Apart from these limitations,\nour method achieves state-of-the-art performance in complex translation tasks\nwhile mitigating inconsistencies through a masking strategy that works by applying\nfew tricks. Simple masking strategies have proven to be very successful in other\nfields. Therefore, we believe that masking strategies for unpaired image-to-image\ntranslation represent a promising direction for further research.\nEthical and responsible use. Considering the limitations of current methods, un-\npaired image-to-image translation methods should be trained and tested with care,\nespecially for safety-critical domains like autonomous driving. A major concern is\nthat it is often unclear or untested whether the transferred content can still be con-\nsidered consistent for subsequent tasks in the target domain. Even though measures\nexist for content-consistent translation, they do not allow for the explainability of\n99\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nwhat exactly is being transferred and changed by the model on a fine-grained level.\nWith our proposed cKVD metric we contribute to this field by allowing class-specific\ntranslation measurements - a direction that we hope is the right one. However, even\nif the content is categorically consistent at a high (class) level, subcategories (like\nparts of textures) may still be interchanged. At a lower level, content consistency\nand style consistency are intertwined (e.g., a yellow stop sign). Another privacy and\nsecurity question is whether translation methods are (or will) be able to (indirectly)\nproject sensitive information from the target domain to the translated images (e.g.,\nexchange faces from simulation with faces of existing persons during the transla-\ntion). A controllable (class-level and in-class-level) consistency method could help\nto resolve such issues.\n100\n6.5. CONCLUSION\nPFD→Cityscapes\nDay→Night\nViper→Cityscapes\nClear→Snowy\nFigure 6.10: Additional qualitative results. Best viewed in color.\n101\nCHAPTER 6. CONTENT-CONSISTENT TRANSLATION WITH MASKED\nDISCRIMINATORS\nGlowing objects\nIntra-class inconsistency\nMinor hallucinations\nClass boundary artifiacts\nFigure 6.11: Limitations. Best viewed in color.\n102\nPart IV\nClausula\n7 Conclusions\n7.1\nConclusion and Discussion\nThis Ph.D. thesis addresses the field of unsupervised representation learning from\nthe perspectives of learning, evaluating, and transferring visual representations. Af-\nter an introduction to the field as a whole in chapter 1, we have provided a high-level\noverview and an introduction of the subfields we have studied and contributed to in\nsections 1.1 through 1.4. Subsequently, the objectives and scope of this dissertation\nhave been defined, along with several research questions focused on the learning,\nevaluation, and transfer of unsupervised visual representations. Following an out-\nline of the thesis in section 1.6, we have extensively discussed the related work with\nrespect to our contributions in chapter 2. In the proceeding chapters 3 to 6, we\nhave described our contributed backpropagation-free unsupervised representation\nlearning method, metrics for measuring objective function mismatches, CARLANE\nBenchmark with a new unsupervised domain adaptation method, and content-\nconsistent unpaired image-to-image translation method. Each aforementioned\nchapter consists of a description of the motivation for the proposed work, a listing\nof the contributions with respect to the research questions, a method description,\nthe conducted experiments, and a conclusion. A summary of our contributions is\nprovided in the subsequent section 7.3.\nDevelopments of unsupervised visual representation learning. In retrospect,\nunsupervised learning of visual representations has made significant progress in\nrecent years. Therefore, we discuss our work as a small part of these (positive)\ndevelopments:\nIn unsupervised representation learning, performance gains for various visual\ntarget tasks have been significant. Self-supervised pretext tasks have evolved from\ndata-indirect approaches such as rotation prediction towards more data-direct\nprediction tasks such as contrastive learning, distillation, or feature predictive\ndistillation. Thus the trend towards the main paradigm of unsupervised repre-\nsentation learning - reducing human intervention - shows promising and steadily\n105\nCHAPTER 7. CONCLUSIONS\nimproving results. However, there is still comparatively little focus on creating a\nmultitude of unsupervised learning rules apart from backpropagation. Neverthe-\nless, several works - such as [134, 186] and ours [238] - have contributed to the\nfield of backpropagation-free and/or biologically plausible (unsupervised) learning,\nleading to increasingly deep models.\nFor the evaluation of unsupervised models, there has been an ongoing effort\nto create novel methods for explaining the inner workings of unsupervised repre-\nsentations and models. This has resulted in promising visualization techniques\nand benchmarks. In recent years, the development of novel metrics that do not rely\non specific datasets and/or benchmarks has received little attention but has not\ngone unnoticed either. Some works have led to several insights into unsupervised\nlearning setups by defining such new metrics, like our metrics for measuring objec-\ntive function mismatches [239], or by examining existing metrics for unsupervised\nrepresentation learning [93, 98].\nFor the transfer of unsupervised visual representations, unsupervised domain\nadaptation is increasingly being applied to more complex tasks such as semantic\nsegmentation [52] or the CARLANE Benchmark [241]. However, with the emergence\nof large models trained on vast amounts of cross-domain data, the most basic\napproach to domain adaptation - simply training the model concurrently on all\nthese domains - may lead to surprising performance gains. The problem of content\nconsistency in the translation of unpaired images has recently been recognized and\naddressed with less ill-posed GAN-based methods - including [216, 251], and our\nown method [240] - which have yielded promising results. The evaluation of these\nmethods in the context of content inconsistency still lacks a metric that is widely\nadopted. Nevertheless, there are ongoing efforts to define novel metrics, as can be\nseen in [216] or our work [240].\nLimitations of unsupervised visual representation learning. Now that we have\nhighlighted recent developments related to our work, we discuss some insightful\nlimitations that persist in these subfields. First, we would like to emphasize limita-\ntions that apply to all of these subfields: In particular, multimodality, availability\nof multimodal curated datasets, continuous learning of models, the efficiency of\nmodels and training, and the explainability of methods are still limited. Especially,\nthe unavailability of multidomain curated open-source datasets has been a relevant\nlimitation with regard to recent trends.\nIn backpropagation-free unsupervised representation learning, a major limita-\ntion is the scalability of these methods to deeper models. In the case of biologically\nplausible unsupervised learning, methods often rely on longstanding neuroscien-\ntific insights without integrating the latest neuroscientific findings. Furthermore,\nbeyond the scientific aspects - such as understanding the brain - it is still unclear\n106\n7.1. CONCLUSION AND DISCUSSION\nwhether biologically-plausible, backpropagation-free approaches lead to signifi-\ncant practical benefits for many tasks, as the trajectory of backpropagation-based\nlearning still leads to performance gains. As for the current state of the art of\nunsupervised representation learning, there is still a performance gap compared\nto supervised learning. However, this gap is continually closing. Although data-\nindirect objectives with specific signals, such as rotation prediction, have been\nsucceeded by novel data-direct objectives, such as feature-predictive distillation,\nthese data-direct objectives do not account for the possible plausible variations of\nmasked features. Often there could be more than one plausible solution for the pre-\ndictive features of a masked region, which is not taken into account by the utilized\nobjectives. For large vision models trained on carefully curated data, limitations lie\nin their trajectory: For example, the utilized datasets do not cover all vision domains\n(or tasks), and the current combination of models and training objectives requires\nadditional regularization to stabilize training. Furthermore, there is currently no\ninsight into whether these models exploit in-dataset statistics of the datasets from\nwhich the curated dataset is built.\nFor the evaluation of unsupervised models, a representation is mostly measured\nby transferring models to target tasks and by visualizing specific properties. This\nis the only widely adopted consensus that defines good representations. Other\nempirical measurements are barely adopted and underexplored. For example, the\nrobustness analysis of these models is limited to a few works and lacks further\nstudies. Moreover, to date, no measurement can reliably predict the performance\nof models on a variety of target tasks.\nFor unsupervised domain adaptation, there is a lack of large multi-domain\ndatasets for complex tasks that strongly indicate the direct usability of methods in\nthe real word. Furthermore, stability, especially in the temporal dimension of the\nadapted models, has not yet been sufficiently explored. Another limitation is that\nonly longstanding, but no recent unsupervised auxiliary tasks have been explored\nin the context of unsupervised domain adaptation.\nIn unpaired image-to-image translation, the unavailability of suitable bias-\nreduced datasets is a major limitation that can have a strong impact on performance.\nGiven the benefits of real-time, high-resolution inference, there is an especially\nstrong need to improve the efficiency and memory utilization of these models.\nFurthermore, a major concern is that it is often not clear what exactly is being\ntransferred and changed by the model and whether the transferred content is still\nconsistent. Even if the content is categorically consistent at a high level, subcate-\ngories (like parts of textures) may still be interchanged. At a lower level, content\nconsistency and style consistency are intertwined (e.g., a yellow stop sign).\nEthics of unsupervised visual representation learning. With the steady integration\n107\nCHAPTER 7. CONCLUSIONS\nof machine learning approaches into the society, ethical considerations regarding\nthese approaches are becoming ever more necessary. These considerations apply\nespecially to backbone models that are or will be used for a multitude of tasks. As\nunsupervised representation learning methods rapidly improve and the first large\nmodels trained on curated data from multiple domains show promising results\n[198], analysis for fairness and biases, such as analysis for geographic-, gender-,\nskintone-, and age-based potentially harmful label associations, are rapidly gaining\nimportance. In addition, the environmental impact of such large models should be\ntaken into account. Here we believe that an open-source culture with releases of\ndatasets, implementations, and pre-trained models can reduce the environmental\nimpact since environmentally expensive model pretraining and dataset curation\nare shared across the community. As models trained on vast curated datasets\ncontaining billions of images proliferate, another increasingly relevant ethical con-\nsideration is the potential encoding of personal information in pre-trained models\nthat could be indirectly obtained from their representations of those models. The\naforementioned considerations also apply to the transfer of representation into\nother domains.\n7.2\nFuture Perspectives on Unsupervised Representa-\ntion Learning\nBased on the discussed developments, limitations, and ethics in section 7.1, current\ntrends in unsupervised visual representation learning, and the paradigm shift in nat-\nural language processing, we want to provide future perspectives for unsupervised\nrepresentation learning. In particular, large unsupervised vision models, stably-\ntrained on curated data, appear to be the future of unsupervised representation\nlearning. These models could encompass or at least assist all the subfields discussed\nin this dissertation as well as others. Carefully curated data and stable, scalable\ntraining objectives may be critical to the development of these models. Going a\nstep further, continual unsupervised learning of carefully monitored backbones\nfrom curated data streams could be a possible future. From an ethical perspec-\ntive, aligning these backbones with the right intent for the target task may become\nrelevant, which brings various scientific challenges, legal questions, and ethical\nconsiderations. In addition, multimodal integration or training beyond the vision\ndomain could lead to more powerful models. A more specific future perspective\nis the development of training objectives (based on discriminators) that integrate\nplausible variability in masked feature prediction. Another specific perspective is to\nreplace the prediction head of latent-conditioned joint-embedding predictive archi-\n108\n7.3. SUMMARY OF CONTRIBUTIONS\ntectures with GAN-based or diffusion-based models, which seems natural due to\nthe latent conditioning. These specific perspectives could complement the develop-\nment of large vision models trained on curated data. Future perspectives regarding\nbackpropagation-free unsupervised learning include continued examinations and\ndevelopments regarding the scalability of these models. Of particular interest ap-\npears to be the successful integration of skip connections and the incorporation of\nthe advantages of image transformers into these models. For biologically plausible\nunsupervised representational learning, the integration of the latest neuroscience\nfindings, such as new neuron formulations, in combination with scaling efforts is\npromising.\nFor the evaluation of unsupervised visual representation learning, in addition\nto continuing the current directions, a relevant direction is to further investigate\nthe temporal dimension of the training setup and its effects on representations\nand models. This could also assist the process of creating large vision models.\nFurthermore, with the rise of curated datasets, metrics to prevent the model from\nexploiting in-database statistics (of the datasets from which the curated dataset\nis built) may be relevant. For evaluations regarding unpaired image-to-image\ntranslation, novel metrics to measure content consistency and style consistency\ncorrectly on multiple consistency levels could increase the trust in these methods.\nFor the unsupervised adaptation of visual representations, it is plausible that\nthe advent of large vision models, trained on curated data, could set a new hard-\nto-beat state of the art and finally combine unsupervised representation learning\nwith unsupervised domain adaptation. However, utilizing adaptation techniques\nfor the alignment of large vision models appears to be an interesting future research\ndirection. For unpaired image-to-image translation, adopting and investigating un-\npaired diffusion models and large, carefully pre-trained (text-to-image) generation\nmodels in the context of content consistency and style consistency is intriguing. A\nfurther appealing direction could be the integration of large vision models into this\nsetup, for example, as encoders for an unpaired latent-to-latent translation.\n7.3\nSummary of Contributions\nIn this dissertation, we contribute to the field of unsupervised representation learn-\ning by developing and evaluating methods to learn, adapt and translate visual\nrepresentations. We have designed a convolutional, backpropagation-free method\nfor unsupervised visual representation learning with modules that can be stacked\ndeeper than previous backpropagation-free baselines. During the evaluation of our\nbackpropagation-free models, we have encountered objective function mismatches\nbetween our unsupervised pretext task and the evaluated target tasks. This has\n109\nCHAPTER 7. CONCLUSIONS\nled to the design of metrics for measuring objective function mismatches between\nunsupervised pretext models and (supervised) target models. With these metrics,\nwe have investigated several well-known unsupervised representation learning\napproaches and target tasks and have found dependencies of the objective function\nmismatch with respect to different aspects of the training setup and model. Fur-\nthermore, we have highlighted the impact of the objective function mismatch on\ntarget task performance.\nA parallel line of research on an autonomous model vehicle has led to the CAR-\nLANE Benchmark, the first 3-way sim-to-real domain adaptation benchmark for 2D\nlane detection. To achieve baseline performances for this benchmark, we have com-\nbined well-known unsupervised domain adaptation methods with the ultra-fast\nlane detection method and have performed evaluations on single-source single-\ntarget and single-source multi-target unsupervised domain adaptation. In addition,\nwe have constructed a new unsupervised domain adaptation method, which has\nreached state-of-the-art performance. We then have focused on unpaired, one-\nsided domain adaptation of images and have constructed a content-consistent\nunpaired image-to-image translation method. Our method utilizes masked global\nand local discriminators to mitigate content inconsistencies during the translation\nof images between biased datasets, which are a common problem in the ill-posed\nsetups of unpaired image-to-image translation. In addition, we have proposed\nfeature-attentive denormalization to selectively fuse feature statistics into the gen-\nerator stream, which allows the model to ignore certain statistics of the source do-\nmain in different layers. For the quantitative comparison, we have used well-known\nperceptual metrics and have designed a new metric that incorporates content in-\nconsistency based on segmentation maps. Both quantitatively and qualitatively,\nour method has outperformed the state of the art on the evaluated translation tasks.\nOverall, this dissertation has led to the following publications, code and dataset\ncontributions, and awards:\n7.3.1\nList of Publications\n• Bonifaz Stuhr, Jürgen Brauer, Bernhard Schick, and Jordi Gonzàlez. \"Masked\nDiscriminators for Content-Consistent Unpaired Image-to-Image Transla-\ntion.\", under review, 2023\n• Bonifaz Stuhr, Johann Haselberger, and Julian Gebele. \"CARLANE: A Lane\nDetection Benchmark for Unsupervised Domain Adaptation from Simulation\nto multiple Real-World Domains.\" Advances in Neural Information Processing\nSystems (NeurIPS), 2022\n110\n7.3. SUMMARY OF CONTRIBUTIONS\n• Bonifaz Stuhr and Jürgen Brauer. \"Don’t miss the mismatch: investigating\nthe objective function mismatch for unsupervised representation learning.\"\nNeural Computing and Applications (Q1 journal), 2022\n• Bonifaz Stuhr and Jürgen Brauer. \"Csnns: Unsupervised, backpropagation-\nfree convolutional neural networks for representation learning.\" 18th IEEE\nInternational Conference On Machine Learning And Applications (ICMLA)\n(oral), 2019\n7.3.2\nContributed Code and Datasets\n• FeaMGan: A framework for training and validating our FeaMGan method,\ncontaining configuration files and links to pre-trained model weights and\ninferred images for all evaluated translation tasks. All results of the publi-\ncation \"Masked Discriminators for Content-Consistent Unpaired Image-to-\nImage Translation\" can be reproduced with the included configuration files.\nhttps://github.com/BonifazStuhr/feamgan\n• CARLANE Benchmark: The official home of CARLANE, a 3-way sim-to-real\ndomain adaptation benchmark for 2D lane detection. This homepage con-\ntains links to download the benchmark datasets (MoLane, TuLane, and Mu-\nLane), a tutorial, the paper, an introductory video, the CARLANE Baselines\nand CARLANE Dataset Tools repositories, a leaderboard, as well as all model\nweights. Furthermore, this homepage shows the results of the baselines and\nour SGPCS method. CARLANE Baselines is a framework containing the im-\nplementations and configuration files of all CARLANE Benchmark baseline\nmodels and our own SGPCS method. All experiments of the publications\n\"CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adapta-\ntion from Simulation to multiple Real-World Domains\" can be reproduced\nwith the included configurations. CARLANE Dataset Tools is a repository\ncontaining all the tools created to collect, clean, and label the datasets for the\nCARLANE Benchmark. https://carlanebenchmark.github.io\n• OFM: A framework containing our metrics to measure the objective function\nmismatch between unsupervised models and target tasks. Furthermore,\nthe implementations and configuration files of all measured unsupervised\nmethods and target tasks are included. All experiments of the publications\n\"Don’t miss the mismatch: investigating the objective function mismatch for\nunsupervised representation learning\" can be reproduced with the included\nconfigurations. https://github.com/BonifazStuhr/OFM\n111\nCHAPTER 7. CONCLUSIONS\n• CSNN: A framework to create, train, evaluate and test backpropagation-free,\nunsupervised CSNN models, which includes CSNN layers, learning rules,\nmodels, and configuration files. With these configuration files, all experi-\nments of the publication \"Csnns: Unsupervised, backpropagation-free con-\nvolutional neural networks for representation learning\" can be reproduced.\nhttps://github.com/BonifazStuhr/CSNN\n7.3.3\nAwards\n• Outstanding Reviewer Award for NeurIPS 2022\nhttps://neurips.cc/Conferences/2022/DatasetBenchmarkProgramCommittee\n• 1st Place at the VDI Autonomous Driving Challenge 2022\nhttps://www.vdi-adc.de\n• 1st Place at the VDI Autonomous Driving Challenge 2020\nhttps://www.vdi-adc.de\n112\nA Supplementary Material:\nSelf-Organizing Convolutional Neural Net-\nworks\nInput\nBatch Norm\nLayer 1 Output\nMulti-Head Masked\nSConv 10x10x3\nS:2 K:3\nMax-Pooling \nS:2 K:2\nMulti-Head Masked\nSConv 16x16x3\nS:1 K:3\nLayer 2 Output\nBatch Norm Max-Pooling \nS:2 K:2\nRepresentation\nnum masks\nand maps\nFigure A.1: Detailed architecture of the S-CSNN model with three SOM maps per\nlayer. The model consists of two CSNN layers, each followed by a batch normaliza-\ntion and a max-pooling layer to halve the spatial dimension of the output tensor.\nThe first layer uses a 10×10×3 SOM grid (3 heads with 100 SOM neurons), stride\n2×2, and input mask weights. The second layer uses a 16×16×3 SOM grid, stride\n1×1, and mask weights between neurons. Both layers use a kernel size of 3×3 and\npadding type “same”. Best viewed in color.\n113\nAPPENDIX A. SUPPLEMENTARY MATERIAL:\nSELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\n0\n20000\n40000\n6\n4\n2\n0\nAverage SOM weight change\n1e\n5\nLayer 1\nD-CSNN-B1\nD2-CSNN-B1\nS-CSNN-B128\nS2-CSNN-B32\n0\n20000\n40000\n0.75\n0.50\n0.25\n0.00\n0.25\n1e\n5\nLayer 2\n0\n20000\n40000\n0.5\n0.0\n0.5\n1e\n6\nLayer 3\n0\n20000\n40000\nTraining Steps per Layer\n0.5\n0.0\n0.5\n1.0\nAverage mask weight change\n1e\n3\nD-CSNN-B1\nD2-CSNN-B1\nS-CSNN-B128\nS2-CSNN-B32\n0\n20000\n40000\nTraining Steps per Layer\n0\n2\n4\n1e\n4\n0\n20000\n40000\nTraining Steps per Layer\n0.0\n0.5\n1.0\n1.5\n1e\n4\nFigure A.2: Mask and SOM weight changes. The first row shows the average SOM\nweight changes during the training of each layer. We can see that each layer was\ntrained bottom-up in its defined training interval. The SOM weights show con-\nvergence, which takes longer in deeper layers (and possibly could be improved\nwhen learning rates and neighborhood coefficients change over the course of the\ntraining). We suspect that the reasons for the slower convergence of deeper layers\nare the increased kernel and SOM map sizes. The second row shows average mask\nweight changes. Best viewed in color.\n114\nLayer 1 | SOM 1\nStep 0\nStep 750\nStep 1500\nStep 2250\nStep 3000\nStep 3750\nStep 4500\nStep 5250\nStep 5750\nLayer 1 | SOM 2\nLayer 1 | SOM 3\nLayer 2 | SOM 1\nLayer 2 | SOM 2\nLayer 2 | SOM 3\nLayer 3 | SOM 1\nLayer 3 | SOM 2\nLayer 3 | SOM 3\nFigure A.3: SOM weights during training. Here we show the layer’s SOM weights at\ntheir SOM grid and map position throughout the training process of the D-CSNN.\nEach SOM image is formed by taking the weights from all SOM neurons of the map\nat the current training step, reshaping them to the patch shape (3D), and forming\nan image by placing them according to the corresponding SOM neuron coordinates\nin the SOM grid. For deeper layers, we only show a slice of depth three through the\nSOM weights since it is hard to visualize kernel (SOM weight) depths larger than\nthree. We can see that all SOM weights have been trained bottom-up in their defined\ntraining intervals. Furthermore, this figure clearly shows self-organization. We want\nto note that the individual SOM weights in layer 1 do not necessarily represent color\ndue to the masks, as shown in the following Figure 3.6, Figure A.4, and Figure A.5;\nthe reason for that are the mask weights. Best viewed in color.\n115\nAPPENDIX A. SUPPLEMENTARY MATERIAL:\nSELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nLayer 1\nStep 500\nStep 1000\nStep 2250\nStep 3000\nStep 5250\nLayer 2\nLayer 3\nLayer 4\nFigure A.4: More BMU images for each layer of the D-CSNN. Best viewed in color.\n116\nLayer 1\nStep 3750\nStep 4000\nStep 4250\nStep 4750\nStep 5000\nLayer 2\nLayer 3\nLayer 4\nFigure A.5: More BMU images for each layer of the D-CSNN. Best viewed in color.\n117\nAPPENDIX A. SUPPLEMENTARY MATERIAL:\nSELF-ORGANIZING CONVOLUTIONAL NEURAL NETWORKS\nLayer 1\nStep 0\nStep 1500\nStep 3000\nStep 4500\nStep 5750\nLayer 2\nLayer 3\nFigure A.6: Mask images. Here we show a slice of depth one through the mask\nvalues of one mask for each layer, where white corresponds to 1.0 and black to\n−1.0. One can see that throughout the training process, these values change slightly,\nindicating that the local mask learning chooses preferred inputs.\n118\nLayer 1 | SOM 1\nStep 0\nStep 750\nStep 1500\nStep 2250\nStep 3000\nStep 3750\nStep 4500\nStep 5250\nStep 5750\nLayer 1 | SOM 2\nLayer 1 | SOM 3\nLayer 2 | SOM 1\nLayer 2 | SOM 2\nLayer 2 | SOM 3\nLayer 3 | SOM 1\nLayer 3 | SOM 2\nLayer 3 | SOM 3\nFigure A.7: Neighborhood coefficients. Here we show the neighborhood coefficients\nof one spatial activation for each layer and map, where white corresponds to 1.0\nand black to 0.0. To create one image, the flat vector containing the neighborhood\ncoefficients is reshaped into the shape of the 2D SOM grid. It can be seen that the\nneighborhood is Gaussian and that only the map with the best BMU have been\nupdated. Furthermore, it can be seen that layers are only trained in their defined\ntraining intervals [y,x].\n119\nB Supplementary Material:\nInvestigating the Objective Function Mis-\nmatch\nB.1\nProofs\nWe measure our metrics on the mean losses during cross-validation instead of\ncalculating the metrics for each round and taking the average. We prove that both\nvariants are equivalent for M3 while measuring SM3 on the mean losses leads to a\nlower bound, given that all models converge at step sn.\nProposition B.1.1 The MM3 of the average metric value tuples 1\nh\nP\n0<c≤h\nMP\nc and\n1\nh\nP\n0<c≤h\nMT\nc with 0 < c ≤h is equivalent to the average MM3 of the individual tuples\n1\nh\nP\n0<c≤h\nMM3(MT\nc ,MP\nc ), given that the tuples are measured for the same steps S and\nconverge at the same step sn.\nProof.\n1\nh\nX\n0<c≤h\nMM3(MT\nc ,MP\nc ) = 1\nh\nX\n0<c≤h\n1\nn\nX\n0<i≤n\n(mT\nic −mP\nic)\n(B.1)\n= 1\nn\nX\n0<i≤n\n1\nh\nX\n0<c≤h\n(mT\nic −mP\nic)\n(B.2)\n= 1\nn\nX\n0<i≤n\nÃ\n1\nh\nX\n0<c≤h\nmT\nic −1\nh\nX\n0<c≤h\nmP\nic\n!\n(B.3)\n= MM3\nÃ\n1\nh\nX\n0<c≤h\nMT\nc , 1\nh\nX\n0<c≤h\nMP\nc\n!\n(B.4)\nCorollary B.1.2\n1\nh\nP\n0<c≤h\nM3(mT\nc ,mP\nc ) = M3( 1\nh\nP\n0<c≤h\nmT\nc , 1\nh\nP\n0<c≤h\nmP\nc )\n121\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nProposition B.1.3 The MSM3 of the average metric value tuples 1\nh\nP\n0<c≤h\nMP\nc and\n1\nh\nP\n0<c≤h\nMT\nc with 0 < c ≤h is a lower bound of the average MSM3 of the individual\ntuples 1\nh\nP\n0<c≤h\nMSM3(MT\nc ), given that the tuples are measured for the same steps S\nand converge at the same step sn.\nProof.\n1\nh\nX\n0<c≤h\nMSM3(MT\nc ) = 1\nh\nX\n0<c≤h\n1\nn\nX\n0<i≤n\nµ\nmT\nic −min\n0<j≤i(mT\njc)\n¶\n(B.5)\n= 1\nn\nX\n0<i≤n\n1\nh\nX\n0<c≤h\nµ\nmT\nic −min\n0<j≤i(mT\njc)\n¶\n(B.6)\n= 1\nn\nX\n0<i≤n\nÃ\n1\nh\nX\n0<c≤h\nmT\nic −1\nh\nX\n0<c≤h\nmin\n0<j≤i(mT\njc)\n!\n(B.7)\n≥1\nn\nX\n0<i≤n\nÃ\n1\nh\nX\n0<c≤h\nmT\nic −min\n0<j≤i( 1\nh\nX\n0<c≤h\nmT\njc)\n!\n(B.8)\nsince 1\nh\nX\n0<c≤h\nmin\n0<j≤i\n(mT\njc ) ≤min\n0<j≤i\n( 1\nh\nX\n0<c≤h\nmT\njc )\n(B.9)\n≥MSM3\nÃ\n1\nh\nX\n0<c≤h\nMT\nc\n!\n(B.10)\nCorollary B.1.4\n1\nh\nP\n0<c≤h\nSM3(mT\nc ,mP\nc ) ≥SM3( 1\nh\nP\n0<c≤h\nmT\nc , 1\nh\nP\n0<c≤h\nmP\nc )\n122\nB.2. ADDITIONAL MODEL AND TRAINING DETAILS\nB.2\nAdditional Model and Training Details\nCNN encoders: We consider a family of convolutional encoders with four Conv-\nBatchNorm-ReLU layers. Filter widths are [32,64,128, f ] and paddings are \"valid\".\nFor input sizes of 32×32 (Cifar10, Cifar100), kernel sizes are [3,3,3,2] and strides\nare [2,2,2,1]; for input sizes of 64×64 (3dshapes, PCam), kernel sizes are [4,4,4,3]\nand strides are [2,2,2,2]. Weights are initialized with the standard TensorFlow ini-\ntialization (kernel_initializer=\"glorot_uniform\", bias_initializer=\"zeros\"). We vary\nf in [4,32,128,256,512,1024] for our experiments on representation sizes. For all\nother experiments f = 256.\nCNN image decoders: We consider a family of decoders with transposed con-\nvolutions with three TranConv-BatchNorm-ReLU layers followed by a TranConv-\nBatchNorm-Sigmoid layer. Filter widths are [128,64,32,3] and paddings are \"valid\".\nFor input sizes of 32×32 (Cifar10, Cifar100), kernel sizes are [4,4,4,3] and strides\nare [2,2,2,1]; for input sizes of 64×64 (3dshapes, PCam), kernel sizes are [4,4,5,4]\nand strides are [2,2,2,2]. Weights are initialized with the standard TensorFlow ini-\ntialization (kernel_initializer=\"glorot_uniform\", bias_initializer=\"zeros\").\nCNN/ResNet head for rotation prediction: For our CNN encoder, we use a fully-\nconnected layer with 4 neurons and softmax activation as the head to predict\nthe four different rotations. We use the standard TensorFlow initialization (ker-\nnel_initializer=\"glorot_uniform\", bias_initializer=\"zeros\"). For our ResNet decoder,\nwe initialize with (kernel_initializer=RandomNormal(stddev=.01), bias_initializer=\n\"zeros\").\nCNN/ResNet heads for contrastive learning: For our CNN encoder, we use a\ntwo-layer MLP with an FC-BatchNorm-ReLU layer followed by an FC-BatchNorm-\nSoftmax layer as the projection head for contrastive learning. The numbers of neu-\nrons are [f ,128]. We use the standard TensorFlow initialization (kernel_initializer=\n\"glorot_uniform\", bias_initializer=\"zeros\"). We vary f in [4,32,128,256,512,1024]\nfor our experiments on representation sizes. For all other experiments f = 256. For\nour ResNet head, the numbers of neurons are [512,128], and we initialize as in [42].\nTarget models: For our linear target model, we use a fully-connected layer with\nnum_classes neurons and a softmax activation. For our two- and three-layer non-\nlinear models, we add layers consisting of [256] and [512, 256] hidden units with\nbatch normalization followed by ReLu activations respectively. Weights are initial-\nized with the standard TensorFlow initialization (kernel_initializer==\"glorot_uniform\",\n123\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nbias_initializer=\"zeros\").\nHardware: We run our experiments on two servers, each containing four Nvidia\nGeForce RTX 2080 Ti GPUs.\nMismatch evaluation: In Table B.1, we show additional details about training,\nevaluation, and measurements.\nTable B.1: Information about measurements and training. Under (Rep. Size, TMC,\nAugs), we refer to all models that have been trained with different representations,\ntarget model complexities, and augmentations.\nMeasurement Epochs\nConvergence\nCriterium\nPretext Model\nTraining Epochs\nTarget Model\nTraining Epochs\nValidation\nRep. Size, TMC, Augs:\nCAE (Cifar10)\n(0,5,20,50,100,...,400)\nPatience: 3\n400\n500\n5-fold cross-validation\nDCAE (Cifar10)\n(0,5,20,50,100,...,400)\nPatience: 3\n400\n500\n5-fold cross-validation\nCCAE (Cifar100)\n(0,5,20,50,100,...,400)\nPatience: 6\n400\n500\n5-fold cross-validation\nCCAE (PCam)\n(0,10,50,100,150,200,300,...,800)\nPatience: 10\n800\n500\n5-fold cross-validation\nRCAE (PCam)\n(0,200,400,...,2000)\nPatience: 30\n2000\n500\n5-fold cross-validation\nSCLCAE (3dshapes)\n(0,10,50,100,150,200,300,...,600)\nPatience: 15\n60 0\n100\n5-fold cross-validation\nTarget Task Type:\nCAE (3dshapes)\n(0,10,30,50,100,...,400)\nPatience: 3\n400\n100\n5-fold cross-validation\nDCAE (3dshapes)\n(0,10,30,50,100,...,400)\nPatience: 3\n400\n100\n5-fold cross-validation\nCCAE (3dshapes)\n(0,10,30,50,100,...,400)\nPatience: 3\n400\n100\n5-fold cross-validation\nRCAE (3dshapes)\n(0,10,30,50,100,...,400)\nPatience: 3\n400\n100\n5-fold cross-validation\nSCLCAE (3dshapes)\n(0,10,30,50,100,...,600)\nPatience: 3\n600\n100\n5-fold cross-validation\nStability:\nCAE100E (Cifar10)\n(0,1,...,100)\nEpoch 100\n100\n500\n5-fold cross-validation\nCAE (Cifar10)\n(0,5,20,50,100,...,400)\nEpoch 400\n400\n500\n5-fold cross-validation\nCAENoCrossVal (Cifar10)\n(0,5,20,50,100,...,400)\nEpoch 400\n400\n500\n5 × same split\nResNets:\nRResNet18 (Cifar10)\n(0,50,100,200,400,...,4000)\nPatience: 30\n4000\n600\n5-fold cross-validation\nSCLResNet18 (Cifar10)\n(0,50,100,200,400„...,4000)\nEpoch 4000\n4000\n600\n3-fold cross-validation\nSCLResNet18 (Cifar100)\n(0,50,100,200,400,...,4000)\nEpoch 4000\n4000\n600\n3-fold cross-validation\nSCLResNet18 (PCam)\n(0,400,800,...,5000)\nPatience: 60\n5000\n500\n5-fold cross-validation\nResNets Rep. Size:\nRResNet18(Cifar10)\n(0,100,200,...,1000,1200,...,3000)\nPatience: 30\n3000\n700\n5-fold cross-validation\n124\nB.3. ADDITIONAL EVIDENCE\nTable B.2: Detailed version of CAE (Cifar10) and DCAE (Cifar10) from Table 4.1.\nCAE (Cifar10)\nDCAE (Cifar10)\nACC\ncSM3\nMOFM\nACC\ncSM3\nMOFM\nRep. Size:\n2x2x4\n27.94+1.47\n−0.76\n0.00+0.00\n−0.00\n0.00+29.77\n−0.00\n28.06+0.70\n−1.65\n0.05+0.12\n−0.05\n0.00+14.27\n−0.00\n2x2x32\n36.57+0.92\n−0.92\n0.07+0.09\n−0.07\n1.99+3.54\n−1.44\n36.20+0.52\n−0.50\n0.00+0.00\n−0.00\n3.20+10.24\n−0.59\n2x2x128\n41.94+0.45\n−0.47\n0.20+0.42\n−0.20\n10.10+6.24\n−3.51\n41.79+0.46\n−0.49\n0.06+0.11\n−0.06\n5.51+6.79\n−5.32\n2x2x256\n44.69+0.69\n−0.41\n0.75+0.38\n−0.22\n11.14+5.32\n−2.72\n45.42+0.55\n−0.62\n0.69+0.43\n−0.69\n5.17+2.98\n−2.16\n2x2x512\n48.13+0.58\n−0.88\n0.43+0.38\n−0.43\n5.28+1.93\n−2.98\n49.04+0.29\n−0.29\n0.36+0.75\n−0.36\n1.25+1.81\n−1.19\n2x2x1024\n51.42+0.43\n−0.38\n0.24+0.32\n−0.24\n0.25+1.26\n−0.17\n53.82+0.57\n−0.82\n0.03+0.06\n−0.03\n0.00+0.04\n−0.00\nTarget Model:\nFC\n44.69+0.69\n−0.41\n0.75+0.38\n−0.22\n11.14+5.32\n−2.72\n45.42+0.55\n−0.62\n0.69+0.43\n−0.69\n5.17+2.98\n−2.16\n2FC\n56.72+0.50\n−0.42\n0.03+0.10\n−0.03\n5.68+2.87\n−3.04\n57.26+0.68\n−0.52\n0.00+0.00\n−0.00\n5.14+2.68\n−1.98\n3FC\n63.05+0.74\n−0.72\n0.03+0.11\n−0.03\n3.94+0.93\n−1.61\n63.33+0.23\n−0.39\n0.00+0.00\n−0.00\n3.17+1.51\n−0.89\nAugmentations:\nAll\n44.69+0.69\n−0.41\n0.75+0.38\n−0.22\n11.14+5.32\n−2.72\n45.42+0.55\n−0.62\n0.69+0.43\n−0.69\n5.17+2.98\n−2.16\nNoJitter\n46.30+0.50\n−0.46\n0.99+0.59\n−0.47\n10.99+1.24\n−3.25\n47.27+0.46\n−0.50\n0.50+0.45\n−0.50\n2.33+1.74\n−1.57\nNoJitterNoFlip\n46.48+0.51\n−0.75\n1.00+0.48\n−0.60\n12.51+1.46\n−1.74\n47.38+0.36\n−0.44\n0.55+0.51\n−0.55\n1.73+4.43\n−1.36\nB.3\nAdditional Evidence\n125\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nTable B.3: Detailed version of CCAE (Cifar100) and RCAE (PCam) from Table 4.1.\nCCAE (Cifar100)\nRCAE (PCam)\nACC\ncSM3\nMOFM\nACC\ncSM3\nMOFM\nMM3\nRep. Size:\n2x2x4\n9.66+0.30\n−0.45\n0.28+0.19\n−0.13\n1.54+2.09\n−0.98\n67.62+1.18\n−2.05\n5.38+2.15\n−1.66\n4.15+37.30\n−4.15\n−22.26+2.96\n−3.02\n2x2x32\n17.63+0.36\n−0.51\n0.65+0.28\n−0.64\n3.64+4.14\n−2.04\n72.83+1.17\n−1.72\n3.34+0.74\n−0.52\n7.92+18.08\n−1.46\n−21.09+1.90\n−2.48\n2x2x128\n24.36+0.56\n−0.47\n0.51+0.24\n−0.24\n0.81+1.60\n−0.45\n78.17+0.63\n−0.80\n1.03+0.88\n−0.96\n4.04+4.35\n−0.70\n−23.47+0.55\n−0.65\n2x2x256\n28.36+0.78\n−0.68\n0.17+0.37\n−0.17\n0.00+0.00\n−0.00\n79.55+1.18\n−1.07\n0.44+0.79\n−0.44\n0.00+3.95\n−0.85\n−27.60+1.22\n−1.00\n2x2x512\n32.02+0.51\n−0.74\n0.00+0.02\n−0.00\n0.00+0.00\n−0.00\n80.80+0.66\n−0.63\n0.18+0.15\n−0.18\n0.00+1.13\n−0.00\n−28.03+1.28\n−1.35\n2x2x1024\n34.89+0.41\n−0.83\n0.00+0.00\n−0.00\n0.00+0.00\n−0.00\n82.20+0.76\n−0.68\n0.09+0.12\n−0.09\n0.00+0.34\n−0.00\n−26.56+1.14\n−1.18\nTarget Model:\nFC\n32.02+0.51\n−0.74\n0.00+0.02\n−0.00\n0.00+0.00\n−0.00\n78.17+0.63\n−0.80\n1.03+0.88\n−0.96\n4.04+4.35\n−0.70\n−23.47+0.55\n−0.65\n2FC\n36.01+0.31\n−0.27\n0.08+0.32\n−0.08\n0.00+0.39\n−0.00\n82.99+0.65\n−0.62\n0.31+0.31\n−0.31\n0.76+1.76\n−0.04\n−28.56+0.81\n−0.72\n3FC\n38.40+0.34\n−0.50\n0.12+0.28\n−0.12\n0.02+0.44\n−0.09\n84.18+0.62\n−0.45\n0.37+0.21\n−0.37\n0.61+3.68\n−0.51\n−29.61+0.71\n−0.61\nAugmentations:\nAll\n28.36+0.78\n−0.68\n0.17+0.37\n−0.17\n0.00+0.00\n−0.00\n78.17+0.63\n−0.80\n1.03+0.88\n−0.96\n4.04+4.35\n−0.70\n−23.47+0.55\n−0.65\nNoJitter\n-\n-\n-\n80.88+0.88\n−0.94\n0.58+0.34\n−0.58\n0.10+5.52\n−0.04\n−28.60+2.23\n−1.18\nNoJitterNoFlip\n-\n-\n-\n80.55+0.73\n−0.55\n1.51+0.57\n−1.09\n7.33+9.76\n−1.78\n−10.60+1.21\n−1.88\nNoFlip\n29.33+0.65\n−0.61\n0.20+0.43\n−0.20\n0.00+0.22\n−0.00\n-\n-\n-\n-\nTable B.4: Detailed version of CCAE (PCam) and SCLCAE (3dshapes for object hue\nclassification) from Table 4.1.\nCCAE (PCam)\nSCLCAE (3dshapes)\nACC\ncSM3\nMOFM\nACC\ncSM3\nMOFM\nMM3\nRep. Size:\n2x2x4\n63.39+3.80\n−1.58\n4.98+5.53\n−3.53\n9.28+∞\n−2.09\n38.07+11.18\n−10.06\n26.34+11.38\n−9.69\n∞\n−7.99+1.83\n−1.28\n2x2x32\n72.73+2.98\n−2.37\n5.17+2.95\n−2.80\n34.30+16.31\n−10.21\n85.25+2.77\n−2.36\n12.98+10.56\n−12.24\n36.39+26.87\n−29.73\n−57.67+6.98\n−6.46\n2x2x128\n78.66+0.46\n−0.22\n0.32+0.47\n−0.32\n0.10+3.51\n−0.48\n96.54+0.72\n−0.83\n8.14+1.29\n−2.53\n22.65+13.03\n−3.92\n−66.19+1.24\n−1.71\n2x2x256\n79.97+0.68\n−0.51\n0.43+0.51\n−0.43\n0.87+4.56\n−0.41\n98.65+0.55\n−0.80\n6.52+0.62\n−0.84\n27.65+5.66\n−4.87\n−65.77+2.19\n−1.12\n2x2x512\n82.34+0.66\n−0.87\n0.20+0.59\n−0.20\n0.07+0.24\n−0.07\n99.41+0.20\n−0.25\n5.96+1.13\n−0.40\n27.78+8.80\n−7.10\n−63.61+1.75\n−1.50\n2x2x1024\n83.67+0.45\n−0.61\n0.00+0.01\n−0.00\n0.00+0.20\n−0.00\n99.75+0.04\n−0.04\n4.76+1.84\n−1.36\n32.70+14.59\n−7.15\n−57.92+2.29\n−2.25\nTarget Model:\nFC\n83.67+0.45\n−0.61\n0.00+0.01\n−0.00\n0.00+0.20\n−0.00\n38.07+11.18\n−10.06\n6.52+0.62\n−0.84\n27.65+5.66\n−4.87\n−65.77+2.19\n−1.12\n2FC\n89.17+0.50\n−0.38\n0.00+0.00\n−0.00\n0.00+0.12\n−0.00\n85.25+2.77\n−2.36\n1.84+0.33\n−0.66\n103.30+56.89\n−21.22\n−70.49+1.41\n−0.75\n3FC\n90.59+0.48\n−0.60\n0.08+0.19\n−0.08\n0.00+0.44\n−0.00\n96.54+0.72\n−0.83\n0.91+0.18\n−0.30\n258.18+308.48\n−88.99\n−71.26+1.20\n−0.77\nAugmentations:\nAll\n79.97+0.68\n−0.51\n0.43+0.51\n−0.43\n0.87+4.56\n−0.41\n38.07+11.18\n−10.06\n6.52+0.62\n−0.84\n27.65+5.66\n−4.87\n−65.77+2.19\n−1.12\nNoJitter\n-\n-\n-\n85.25+2.77\n−2.36\n0.00+0.02\n−0.00\n0.01+0.07\n−0.01\n−37.92+0.72\n−0.89\nNoJitterNoFlip\n-\n-\n-\n96.54+0.72\n−0.83\n0.00+0.00\n−0.00\n0.00+0.04\n−0.01\n−35.95+0.63\n−0.82\nNoFlip\n80.70+0.36\n−0.57\n0.41+0.21\n−0.30\n0.04+1.03\n−0.20\n-\n-\n-\n-\n126\nB.3. ADDITIONAL EVIDENCE\nTable B.5: Detailed version of CAE and DCAE from Table 4.2.\nCAE\nDCAE\nACC\ncSM3\nMOFM\nACC\ncSM3\nMOFM\nfloor_hue\n99.96+0.02\n−0.02\n0.01+0.02\n−0.01\n0.95+0.71\n−0.51\n99.97+0.02\n−0.06\n0.00+0.00\n−0.00\n1.28+1.11\n−0.81\nwall_hue\n100.00+0.00\n−0.00\n0.02+0.04\n−0.02\n32.03+6.38\n−8.59\n99.99+0.00\n−0.01\n0.00+0.01\n−0.00\n24.43+7.56\n−6.97\nobject_hue\n99.23+0.18\n−0.35\n0.38+0.82\n−0.37\n22.71+14.77\n−6.45\n99.22+0.26\n−0.31\n0.43+0.61\n−0.43\n24.55+7.04\n−11.17\nscale\n74.17+6.07\n−6.42\n0.41+1.64\n−0.41\n0.00+0.00\n−0.00\n68.27+2.36\n−2.04\n0.27+0.59\n−0.27\n0.00+0.00\n−0.00\nshape\n98.38+0.82\n−1.19\n0.07+0.14\n−0.07\n0.00+0.04\n−0.00\n97.45+0.46\n−0.57\n0.08+0.20\n−0.08\n0.00+0.06\n−0.00\norientation\n81.63+3.23\n−2.89\n0.00+0.00\n−0.00\n0.00+0.00\n−0.00\n74.27+1.07\n−2.34\n0.00+0.00\n−0.00\n0.00+0.00\n−0.00\naverage\n92.22\n0.15\n9.28\n89.86\n0.13\n8.21\nTable B.6: Detailed version of CCAE and RCAE from Table 4.2.\nCCAE\nRCAE\nACC\ncSM3\nMOFM\nACC\ncSM3\nMOFM\nMM3\nfloor_hue\n99.94+0.03\n−0.05\n0.02+0.03\n−0.02\n0.00+0.00\n−0.00\n92.08+1.79\n−2.89\n56.68+0.94\n−1.55\n∞\n44.67+4.10\n−3.78\nwall_hue\n99.98+0.01\n−0.02\n0.10+0.11\n−0.07\n0.00+0.78\n−0.00\n99.96+0.04\n−0.04\n25.17+9.67\n−8.15\n∞\n7.80+0.59\n−0.85\nobject_hue\n98.96+0.38\n−0.30\n1.55+0.73\n−0.63\n0.63+5.58\n−0.59\n98.79+0.35\n−0.41\n59.65+8.12\n−10.39\n∞\n40.11+0.99\n−1.88\nscale\n66.72+3.20\n−2.20\n0.10+0.41\n−0.10\n0.00+0.00\n−0.00\n67.10+6.77\n−3.96\n2.60+4.10\n−2.60\n0.13+3.54\n−0.13\n31.78+1.91\n−2.44\nshape\n98.75+0.39\n−0.50\n0.03+0.10\n−0.03\n0.00+0.00\n−0.00\n98.17+0.43\n−0.39\n0.20+0.45\n−0.20\n0.06+0.82\n−0.06\n−2.48+0.65\n−1.28\norientation\n72.55+3.17\n−2.81\n0.23+0.91\n−0.23\n0.00+0.00\n−0.00\n80.69+2.55\n−2.68\n0.48+1.91\n−0.48\n0.00+1.23\n−0.00\n22.26+0.74\n−1.19\naverage\n89.48\n0.34\n0.11\n89.47\n24.13\n∞\n24.02\nTable B.7: Detailed version of SCLCAE from Table 4.2.\nSCLCAE\nACC\ncSM3\nMOFM\nMM3\nfloor_hue\n93.53+2.62\n−1.90\n28.18+11.12\n−6.30\n268.27+161.92\n−44.43\n−48.38+2.75\n−2.15\nwall_hue\n99.96+0.03\n−0.02\n0.29+0.07\n−0.11\n0.46+5.04\n−0.44\n−76.40+2.97\n−3.06\nobject_hue\n98.67+0.82\n−1.06\n2.87+0.69\n−1.36\n8.69+3.20\n−0.64\n−73.17+3.16\n−2.77\nscale\n83.94+1.15\n−0.57\n2.43+4.02\n−2.43\n0.00+1.78\n−0.00\n−44.80+2.06\n−1.55\nshape\n95.06+0.92\n−0.86\n1.67+1.54\n−1.67\n2.16+1.48\n−0.13\n−67.54+2.38\n−1.63\norientation\n45.32+4.14\n−3.57\n2.50+2.55\n−1.97\n6.68+5.21\n−3.02\n−9.11+2.46\n−1.71\naverage\n86.08\n6.32\n47.71\n−53.23\n127\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\nTable B.8: Mismatches of other models we have tested. Values are obtained by\ncross-validation, please refer to Table B.1 for more details.\nCifar10\nACC\ncSM3\nMOFM\nMM3\nCAE\n44.69+0.69\n−0.41\n1.42+0.68\n−0.70\n18.71+6.61\n−4.01\n-\nCAENoCrossVal\n44.24+0.24\n−0.41\n1.26+0.15\n−0.47\n17.40+2.91\n−2.58\n-\nCAE100E\n45.37+1.09\n−1.06\n1.22+0.56\n−0.32\n5.84+7.59\n−1.12\n-\nRResNet18\n54.64+1.80\n−2.01\n3.98+1.74\n−3.60\n4.87+4.42\n−3.11\n31.82+0.75\n−0.68\nSCLResNet18\n87.14+0.37\n−0.40\n0.29+0.23\n−0.20\n0.12+0.07\n−0.02\n−31.83+0.12\n−0.23\nRResNet18R32\n39.14+1.53\n−1.69\n1.92+1.75\n−1.29\n4.39+9.53\n−1.58\n48.93+1.17\n−2.19\nRResNet18R256\n47.46+1.44\n−1.76\n3.91+2.94\n−1.71\n6.86+8.51\n−2.09\n39.08+1.78\n−2.07\nRResNet18R512\n50.55+2.91\n−2.84\n7.01+1.83\n−3.73\n9.83+10.55\n−3.90\n36.93+3.15\n−1.96\nRResNet18R756\n51.80+2.04\n−2.38\n7.56+5.01\n−3.17\n8.50+11.90\n−5.00\n35.61+1.45\n−1.74\nRResNet18R1024 52.99+1.54\n−2.58\n9.89+4.51\n−2.41\n15.12+11.12\n−4.18\n36.86+2.76\n−2.20\nTable B.9: Mismatches of other models we have tested. Values are obtained by\ncross-validation, please refer to Table B.1 for more details.\nPCam\nCifar100\nACC\ncSM3\nMOFM\nMM3\nACC\ncSM3\nMOFM\nMM3\nSCLResNet18 96.25+0.44\n−0.23\n0.37+0.44\n−0.37\n0.86+1.00\n−0.60\n−53.26+0.52\n−0.38\n59.20+0.19\n−0.25\n0.41+0.10\n−0.09\n0.06+0.05\n−0.04\n0.84+0.06\n−0.03\n0\n50\n100\n150\n200\n250\n300\n350\n400\nPretext Training Epochs\n0\n5\n10\n15\n20\n25\n30\n35\nOFM in %\nCifar10\nCAER256 MOFM: 18.71% +6.61% -4.01%\n0\n50\n100\n150\n200\n250\n300\n350\n400\nPretext Training Epochs\n0\n5\n10\n15\n20\n25\n30\n35\nOFM in %\nCifar10\nCAER256SameSplitx5 MOFM: 17.40% +2.91% -2.58%\nFigure B.1: Stability of the partially measured OFM. (Left) OFM of a CAE trained\nfor 400 epochs. Stability is measured by 5-fold cross-validation. (Right) OFM of a\nCAE trained for 400 epochs. Stability is measured by training the CAE five times\non the same dataset split. Unsurprisingly, the stability of the OFM is higher when\nthe CAE is trained on the same split instead of the different splits from the 5-fold\ncross-validation.\n128\nB.3. ADDITIONAL EVIDENCE\n1.6\n1.8\nBest Target Loss\nCAER256E100 on Cifar10\n0\n200\n400\nTarget Training Epochs\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nTarget Loss\n0\n50\n100\nPretext Training Epochs\n0.5\n0.6\nPretext Loss\n0\n50\n100\nPretext Training Epochs\n0\n25\n50\n75\n100\n125\n150\nN(Best Target Loss)\n         +7.59%\nOFM: 5.84%\n          -1.12%\ncOFM: 15.06%\nmOFM: 43.21%\n0\n50\n100\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nOFM in %\nCAER256E100 on Cifar10\nFigure B.2: Stability of the fully measured OFM. (Left) Losses of a simple CAE\nmeasured for every pretext training epoch. The curve formed by the target models\nrepresents a typical target training curve in our setup. (Right) The OFM and its\nstability measured for every pretext training epoch of the CAE. When we compare\nthe stability to the partially measured CAE in Figure B.1, we observe a similar\ninstability. Best viewed in color.\n1.6\n1.7\n1.8\nBest Target Loss\nCAER256 on Cifar10\n0\n200\n400\nTarget Training Epochs\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nTarget Loss\n0\n200\n400\nPretext Training Epochs\n0.5\n0.6\nPretext Loss\n1.4\n1.6\nBest Target Loss\nDCAER1024 on Cifar10\n0\n200\n400\nTarget Training Epochs\n1.5\n2.0\n2.5\n3.0\nTarget Loss\n0\n200\n400\nPretext Training Epochs\n0.5\n0.6\nPretext Loss\n0.5\n0.6\nBest Target Loss\nRCAER128 on Patch_camelyon\n0\n200\n400\nTarget Training Epochs\n0.6\n0.8\n1.0\n1.2\n1.4\nTarget Loss\n0\n1000\n2000\nPretext Training Epochs\n1\n2\n3\nPretext Loss\n0.4\n0.6\nBest Target Loss\nSCLCAER128 on Shapes3d\n0\n50\n100\nTarget Training Epochs\n0.5\n1.0\n1.5\n2.0\n2.5\nTarget Loss\n0\n200\n400\n600\nPretext Training Epochs\n11\n12\n13\nPretext Loss\nFigure B.3: Additional evidence for the mismatch and convergence section 4.6.1.\nLonger training of the pretext task tends to create easier separable representations,\nwhich may mismatch with the class label. We observe that the target loss curves\nconverge faster for target models trained on the pretext model’s representation from\nlater pretext training epochs. Especially the purple curves show this behavior clearly.\nBest viewed in color.\n129\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\n0\n20\n40\nOFM in %\nCAE on Cifar10\n0\n50\n100\nOFM in %\n0\n200\n400\nPretext Training Epochs\n0\n10\n20\n30\nOFM in %\n0\n50\n100\nDCAE on Cifar10\n0\n50\n100\n0\n200\n400\nPretext Training Epochs\n0\n5\n10\n15\n0\n5\n10\n15\nCCAE on Cifar100\n0\n10\n20\n0\n200\n400\nPretext Training Epochs\n0\n2\n4\n0\n50\n100\nCCAE on PCam\n0\n2\n4\n6\n0\n500\nPretext Training Epochs\n0\n5\n10\n15\n0\n50\n100\nRCAE on PCam\n0\n10\n20\n0\n1000\n2000\nPretext Training Epochs\n0\n20\n40\n0\n50\n100\nSCLCAE on Shapes3d\n2x2x4\n2x2x32\n2x2x128\n2x2x256\n2x2x512\n2x2x1024\n0\n500\nFC\n2FC\n3FC\n0\n250\n500\nPretext Training Epochs\n0\n20\n40\nAll\n-Jitter\n-JitterFlip\n-Flip\nFigure B.4: Version of Figure 4.4 without convergence criterium. We observe similar\nbehaviors of the mismatches as in Figure 4.4 in most cases. One exception is the\ncolor jitter, where the OFM starts to converge or decrease late in training. (top)\nImpact of different pretext model representation sizes on the OFM for our model.\n(middle) The OFM for the linear and nonlinear target models trained on our pretext\nmodel. (bottom) The OFM for the linear target model and for the pretext models\ntrained on fewer augmentations. First we have removed the color jitter and then\nthe vertical flip from the augmentations. For the CCAE, we only have removed the\nvertical flip. The target models of SCLCAE have been trained on 3dshapes to predict\nthe object hue. Best viewed in color.\n0\n1\n2\nSM3 in %\nCAE on Cifar10\n0\n2\n4\n6\nSM3 in %\n0\n200\n400\nPretext Training Epochs\n0\n1\n2\nSM3 in %\n0\n1\n2\nDCAE on Cifar10\n0.0\n2.5\n5.0\n7.5\n0\n200\n400\nPretext Training Epochs\n0.0\n0.5\n1.0\n1.5\n0.0\n0.5\n1.0\n1.5\nCCAE on Cifar100\n0\n1\n2\n0\n200\n400\nPretext Training Epochs\n0.0\n0.2\n0.4\n0.6\n0\n5\n10\nCCAE on PCam\n0.00\n0.25\n0.50\n0.75\n0\n500\nPretext Training Epochs\n0.0\n0.5\n1.0\n1.5\n0\n5\n10\nRCAE on PCam\n0\n1\n2\n0\n1000\n2000\nPretext Training Epochs\n0\n1\n2\n3\n0\n20\n40\nSCLCAE on Shapes3d\n2x2x4\n2x2x32\n2x2x128\n2x2x256\n2x2x512\n2x2x1024\n0\n2\n4\n6\nFC\n2FC\n3FC\n0\n250\n500\nPretext Training Epochs\n0\n2\n4\n6\nAll\n-Jitter\n-JitterFlip\n-Flip\nFigure B.5: Version of Figure 4.4 for SM3 on accuracies, without convergence cri-\nterium. Again, we observe similar behaviors of the mismatches as in Figure 4.4 in\nmost cases. Best viewed in color.\n130\nB.3. ADDITIONAL EVIDENCE\n0\n100\n200\n300\n400\n500\n600\nPretext Training Epochs\n0.5\n1.0\n1.5\n2.0\nBest Target Loss\nShapes3d_label_object_hue\nSCLCAER4\nSCLCAER32\nSCLCAER128\nSCLCAER256\nSCLCAER512\nSCLCAER1024\n0\n100\n200\n300\n400\n500\n600\nPretext Training Epochs\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nBest Target Loss\nShapes3d_label_object_hue\nSCLCAER256\nFigure B.6: SCLCAE target losses for the object hue class of 3dshapes. (left) We\ndescribe why the OF M for larger representation sizes does not decrease in the setup\nwhere we train the contrastive pretext model (SCLCAE) on 3dshapes and the target\nmodel to predict the object hue. Here, the target models trained on the untrained\npretext models with larger representation sizes already achieve high performance\ndue to a higher amount of color-selective, random features. Additionally, learning\nthe pretext model does not lead to a high performance gain, which leads again\nto a small interval for normalization. Therefore, forgetting useful features for the\ntarget task later in training leads to a high mismatch. (right) We describe why the\nOF M does not decrease for more complex target models in the same setup. The\nnonlinear target model can make better sense of specific random pretext features\nfor classification, which leads to a very low target loss at pretext model initialization.\nSince the pretext model does not learn many useful features for the target task later,\nthis leads again to a small interval for normalization. Therefore, the OF M gets very\nlarge later in training when the pretext model starts to forget useful features for the\ntarget task.\n131\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nPretext Training Epochs\n50\n40\n30\n20\n10\n0\nM3 in %\nPatch_camelyon\nRCAER4 MM3: -15.18% +3.50% -4.86% \nRCAER32 MM3: -18.13% +1.84% -2.56% \nRCAER128 MM3: -23.47% +0.55% -0.65% \nRCAER256 MM3: -24.95% +1.57% -0.88% \nRCAER512 MM3: -25.38% +1.24% -1.24% \nRCAER1024 MM3: -26.56% +1.14% -1.18% \n0\n100\n200\n300\n400\n500\n600\nPretext Training Epochs\n100\n90\n80\n70\n60\n50\n40\nM3 in %\nShapes3d_label_object_hue\nSCLCAER32 MM3: -56.34% +7.67% -6.99% \nSCLCAER128 MM3: -66.19% +1.24% -1.71% \nSCLCAER256 MM3: -65.47% +2.23% -1.16% \nSCLCAER512 MM3: -61.98% +1.83% -1.44% \nSCLCAER1024 MM3: -57.92% +2.29% -2.25% \n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nPretext Training Epochs\n85\n80\n75\n70\nM3 in %\nRCAER128 on Patch_camelyon\nLinearClassifierFC MM3: -76.13% +0.37% -0.35% \nNonLinearClassifier2FC MM3: -81.22% +0.35% -0.37% \nNonLinearClassifier3FC MM3: -82.27% +0.27% -0.33% \n0\n100\n200\n300\n400\n500\n600\nPretext Training Epochs\n100\n98\n96\n94\n92\nM3 in %\nSCLCAER256 on Shapes3d_label_object_hue\nLinearClassifierFC MM3: -93.44% +1.14% -0.74% \nNonLinearClassifier2FC MM3: -98.21% +0.38% -0.29% \nNonLinearClassifier3FC MM3: -99.00% +0.17% -0.15% \n0\n200\n400\n600\n800\n1000\nPretext Training Epochs\n50\n40\n30\n20\n10\n0\n10\nM3 in %\nPatch_camelyon\nRCAER128 MM3: -23.47% +0.55% -0.65% \nRCAER128NoJitter MM3: -20.17% +1.71% -1.52% \nRCAER128NoJitterNoFlip MM3: -8.31% +1.21% -1.56% \n0\n100\n200\n300\n400\n500\n600\nPretext Training Epochs\n100\n90\n80\n70\n60\n50\n40\n30\nM3 in %\nShapes3d_label_object_hue\nSCLCAER256 MM3: -65.47% +2.23% -1.16% \nSCLCAER256NoJitter MM3: -37.92% +0.72% -0.89% \nSCLCAER256NoJitterNoFlip MM3: -35.95% +0.63% -0.82% \nFigure B.7: The version of Figure 4.4 for M3 on accuracies. We observe that besides\nearly spikes, M3 decreases when we add complexity to the target model. Addition-\nally, we observe that M3 decreases when we add augmentations in this case. We\nmeasure the M3 for RCAE between the classification error of the target task and the\nclassification error of predicting the rotations of rotated images from PCam (pretext\ntask). For SCLCAE, the pretext task metric measures the ability of the model to\ncorrectly detect the representation of each given image in a batch of representations\nof transformed images. Here we show M3 without a convergence criterium. Best\nviewed in color.\n132\nB.3. ADDITIONAL EVIDENCE\n0\n200\n400\n \nP. Loss\n0\n20\n40\n60\n80\nBest Target Error in %\nCAE\n0\n200\n400\nDCAE\n0\n200\n400\nCCAE\n0\n200\n400\nRCAE\n0\n250\n500\nSCLCAE\nfloor_hue\nwall_hue\nobject_hue\norientation\nscale\nshape\nPretext Training Epochs\nFigure B.8: The version of Figure 4.5 for SM3 on accuracies. Best viewed in color.\n0\n500\n1000\n1500\n2000\n2500\n3000\nPretext Training Epochs\n0\n10\n20\n30\n40\n50\n60\nOFM in %\nCifar10\n32 MOFM: 4.39% +9.53% -1.58%\n256 MOFM: 6.86% +8.51% -2.09%\n512 MOFM: 9.83% +10.55% -3.90%\n756 MOFM: 8.50% +11.90% -5.00%\n1024 MOFM: 15.12% +11.12% -4.18%\n0\n500\n1000\n1500\n2000\n2500\n3000\nPretext Training Epochs\n10\n0\n10\n20\n30\n40\n50\n60\nM3 in %\nCifar10\n32 MM3: 48.93% +1.17% -2.19% \n256 MM3: 39.08% +1.78% -2.07% \n512 MM3: 36.93% +3.15% -1.96% \n756 MM3: 35.61% +1.45% -1.74% \n1024 MM3: 36.86% +2.76% -2.20% \nFigure B.9: OFM and MM3 for other ResNets. (left) The OFM of ResNets with\ndifferent representation sizes trained on the pretext task of predicting rotations\non Cifar10. Target models are trained for Cifar10 classification. (right) MM3 of\nthose ResNets. We vary representation sizes in [32,256,512,756,1024] by adding\na 1 × 1 convolution layer on top of the ResNet18. Thereby the number of filters\ncorresponds to the representation size. In contrast to our observations on our small\nmodel, the largest representation we have tested leads to a high OFM. A reason for\nthat could be that the larger representation size helps the model to solve the pretext\ntask, and since there is a mismatch with the target task, a better understanding of\nthis task leads to a higher mismatch. We note that a representation size of 1024 is\nstill very small for unsupervised learning. Therefore, an even larger representation\nsize could still lead to a lower mismatch.\n133\nAPPENDIX B. SUPPLEMENTARY MATERIAL:\nINVESTIGATING THE OBJECTIVE FUNCTION MISMATCH\n0\n2000\n4000\nPretext Training Epochs\n20\n40\n60\n80\n100\nError in %\n         +0.12%\nMM3: -31.83%\n          -0.23%\n0\n2000\n4000\nPretext Training Epochs\n35.0\n32.5\n30.0\n27.5\n25.0\n22.5\nM3 in %\nSCLResNet18 on Cifar10\n0\n2000\n4000\nPretext Training Epochs\n10\n20\n30\n40\n50\n60\n70\nError in %\n         +0.77%\nMM3: 34.29%\n          -0.51%\n0\n2000\n4000\nPretext Training Epochs\n0\n10\n20\n30\n40\nM3 in %\n0\n2000\n4000\nPretext Training Epochs\n40\n50\n60\n70\n80\n90\n100\nError in %\n         +0.06%\nMM3: 0.84%\n          -0.03%\n0\n2000\n4000\nPretext Training Epochs\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\nM3 in %\nSCLResNet18 on Cifar100\n0\n2000\n4000\nPretext Training Epochs\n0\n20\n40\n60\n80\n100\nError in %\n         +0.52%\nMM3: -53.45%\n          -0.54%\n0\n2000\n4000\nPretext Training Epochs\n70\n65\n60\n55\n50\n45\nM3 in %\nSCLResNet18 on Patch_camelyon\nFigure B.10: MM3 for different pretext tasks trained with a ResNet18 model as the\nbackbone. (top right) The mismatches are shown for the entire training process.\nIn contrast to the prediction of rotations, SCLResNet18 has a high negative MM3\nfor Cifar10. This indicates that learning the contrastive pretext task is better suited\nfor distinguishing Cifar10 classes than the prediction of rotations. Furthermore,\nthe error of the contrastive pretext task is significant, which indicates that the\nmodel still underfits the pretext task with this setup, and there is more room for\nimprovement. For the 100 classes of Cifar100, MM3 becomes slightly positive in\nthe contrastive learning setup. For contrastive learning on the PCam dataset and\nrotation prediction on Cifar10, we observe an increasing mismatch during training.\n134\nC Supplementary Material:\nA Lane Detection Benchmark for Multi-\nTarget Domain Adaptation\nC.1\nExample Usage of the CARLANE Benchmark\nA Jupyter notebook with a tutorial on how to read the datasets for use in PyTorch\ncan be found at https://carlanebenchmark.github.io.\nC.2\nModel Vehicle Description\nIn Figure C.1, the self-built 1/8th model vehicle is shown, which we have used to\ngather the images for the 1/8th scaled target domain. An NVIDIA Jetson AGX is the\ncentral computation unit powered by a separate Litionite Tanker Mini 25000mAh\nbattery. For image collection, we utilize the software framework ROS Melodic and\na Stereolabs ZEDM stereo camera with an integrated IMU. The camera is directly\nconnected to the AGX and captures images with a resolution of 1280×720 pixels\nand a rate of 30 FPS.\nFigure C.1: Picture of the 1/8th model vehicle we built to capture images in our\n1/8th target domain.\nC.3\nReproducibility of the Baselines\nTo ensure reproducibility, we strictly follow UFLD [212] and the corresponding unsu-\npervised domain adaptation method for model architecture and hyperparameters.\n135\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\nUFLD-SO\nDANN\nADDA\nSGADA\nSGPCS\nFigure C.2: t-SNE visualizations of the MoLane dataset (top) and the TuLane dataset\n(bottom). The source domain is marked in blue, the real-world model vehicle target\ndomain in red, and TuLane’s target domain in green. Best viewed in color.\nThereby, we utilize UFLD as an encoder for the unsupervised domain adaptation\nmethod. We provide a detailed table of the tuned hyperparameters, architecture\nchanges, and objectives in the main text. In addition, the trained weights of our\nbaselines, their entire implementation, and the configuration files of our baselines\nare made publicly available at https://carlanebenchmark.github\n.io.\nInitialization. We initialize convolutional layer weights with kaiming normal and\ntheir biases with 0.0. Linear layer weights are initialized with normal (mean = 0.0,\nstd = 0.01), batch normalization weights, and biases are initialized with 1.0.\nC.4\nAdditional Results\nt-SNE feature clustering. Figure C.2 shows the t-SNE feature clustering of the\ntrained baselines for the MoLane and TuLane dataset, respectively. We observe\nthat few features of both domains spread over the entire plot for higher-performing\nunsupervised domain adaptation methods. However, there are still large clusters of\nfeatures from one domain, indicating that the domain adaptation only occurred\nslightly.\nQualitative results. We randomly sample results from our baselines and show\nthem in Figures C.4, C.5, and C.6. Compared to UFLD-SO, the unsupervised domain\n136\nC.5. COMPARISON TO RELATED WORK\nTable C.1: Comparison of CARLANE (ours) with datasets created by related work.\nDataset\nYear\nPublicly\nAvailable\nDomains\nSimulation Resolution\nTotal\nImages Annotations\n[77]\n2019\n✗\nsim, real\nblender\n480×360\n391K\n3D\n[78]\n2020\n✗\nsim, real\nblender\n480×360\n586K\n3D\n[116]\n2022\n✗\nsim, real\nCarla\n1280×720\n23K\n2D\nours\n2022\n✓\nsim, real, scaled\nCarla\n1280×720\n163K\n2D\nTable C.2: Comparison of applied variations for the collection of the synthetic\ndatasets.\nDataset\nEgo Vehicle\nCamera Position\nLane Deviation\nTraffic\nPedestrians\nWorld Objects\nDaytime\nWeather\nCity\nRural\nHighway\nTerrain\nLane Topology\nRoad Appearance\n[77]\n✗\n✓✓✓✗\n✓\n✓\n✗\n✗\n✓\n✗\n✓✓\n✓\n[78]\n✗\n✓✓✓✗\n✓\n✓\n✗\n✗\n✓\n✗\n✓✓\n✓\n[116]\n✗\n✗✓✓✓\n✗\n✓✓✓✓✓✓✓\n✓\nours\n✓✓✓✓✗\n✓\n✓✓✓✓✓✓✓\n✓\nadaptation baselines ADDA, SGADA, and SGPCS increase performance consistently.\nUFLD-TO samples show the best results on the target domain.\nC.5\nComparison to Related Work\nIn Table C.1, we compare CARLANE with the datasets created by related work. The\nmain differentiators are that our dataset contains three distinct domains, including\na scaled model vehicle, and is publicly available. To further compare our synthetic\ndatasets with related work, the applied variations during the data collection process\nare summarized in Table C.2. Additionally, we highlight noticeable differences in\nthe visual quality of the simulation engines in Figure C.3. Scenes captured in CARLA\nare more realistic and detailed.\n137\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\n[77]\nours\nFigure C.3: Visual comparison of simulation images from the custom blender\nsimulation used in [77, 78] and the CARLA simulation used by [116] and our work.\nWe observe that scenes captured in CARLA are more detailed and realistic. Best\nviewed in color.\nMoLane\nTuLane\nMuLane\nUFLD-SO\nDANN\nADDA\nSGADA\nSGPCS\nUFLD-TO\nFigure C.4: More qualitative results of target domain predictions. Images are ran-\ndomly sampled. Ground truth lane annotations are marked in blue, and predictions\nin red. Best viewed in color.\n138\nC.5. COMPARISON TO RELATED WORK\nMoLane\nTuLane\nMuLane\nUFLD-SO\nDANN\nADD\nSGADA\nSGPCS\nUFL\nFigure C.5: More qualitative results of target domain predictions. Images are ran-\ndomly sampled. Ground truth lane annotations are marked in blue, and predictions\nin red. Best viewed in color.\n139\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\nMoLane\nTuLane\nMuLane\nUFLD-SO\nDANN\nADDA\nSGADA\nSGPCS\nUFLD-TO\nFigure C.6: More qualitative results of target domain predictions. Images are ran-\ndomly sampled. Ground truth lane annotations are marked in blue, and predictions\nin red. Best viewed in color.\n140\nC.6. NEURIPS CHECKLIST FOR THE CARLANE BENCHMARK\nC.6\nNeurIPS Checklist for the CARLANE Benchmark\n1. For all authors...\na Do the main claims made in the abstract and introduction accurately\nreflect the paper’s contributions and scope? [Yes] See Section 5.6.\nb Did you describe the limitations of your work? [Yes] See Section 5.6.\nc Did you discuss any potential negative societal impacts of your work?\n[Yes] See Section 5.6.\nd Have you read the ethics review guidelines and ensured that your paper\nconforms to them? [Yes]\n2. If you are including theoretical results...\na Did you state the full set of assumptions of all theoretical results? [N/A]\nb Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\na Did you include the code, data, and instructions needed to reproduce\nthe main experimental results (either in the supplemental material or\nas a URL)? [Yes] See Abstract of chapter 5, Section 5.5.4 and the supple-\nmental material.\nb Did you specify all the training details (e.g., data splits, hyperparameters,\nhow they were chosen)? [Yes] See Section 5.4, Section 5.5, Section 5.5.4,\nTable 5.1, Table 5.2 and the supplemental material.\nc Did you report error bars (e.g., with respect to the random seed after\nrunning experiments multiple times)? [Yes] Table 5.3.\nd Did you include the total amount of compute and the type of resources\nused (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Sec-\ntion 5.5.4.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing\nnew assets...\na If your work uses existing assets, did you cite the creators? [Yes] We cited\nthe TuSimple dataset [255].\nb Did you mention the license of the assets? [Yes] See Section 5.4.\nc Did you include any new assets either in the supplemental material or\nas a URL? [Yes]\n141\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\nd Did you discuss whether and how consent was obtained from people\nwhose data you’re using/curating? [Yes] TuSimple is open-source and\nlicensed under the Apache License, Version 2.0 (January 2004).\ne Did you discuss whether the data you are using/curating contains per-\nsonally identifiable information or offensive content? [Yes] See Sec-\ntion 5.6.\n5. If you used crowdsourcing or conducted research with human subjects...\na Did you include the full text of instructions given to participants and\nscreenshots, if applicable? [N/A]\nb Did you describe any potential participant risks, with links to Institu-\ntional Review Board (IRB) approvals, if applicable? [N/A]\nc Did you include the estimated hourly wage paid to participants and the\ntotal amount spent on participant compensation? [N/A]\n142\nC.7. DATASHEET FOR THE\nCARLANE BENCHMARK\nC.7\nDatasheet for the\nCARLANE Benchmark\nMotivation\nFor what purpose was the dataset created?\nWas there a specific task in mind? Was there a spe-\ncific gap that needed to be filled? Please provide a\ndescription.\nCARLANE was created to be the first pub-\nlicly available single- and multi-target\nUnsupervised Domain Adaptation (UDA)\nbenchmark for lane detection to facili-\ntate future research in these directions.\nHowever, in a broader sense, the datasets\nof CARLANE were also created for un-\nsupervised and semi-supervised learn-\ning and partially for supervised learn-\ning. Furthermore, a real-to-real transfer\ncan be performed between the target do-\nmains of our datasets.\nWho created the dataset (e.g., which team,\nresearch group) and on behalf of which entity\n(e.g., company, institution, organization)?\nAs released on June 17, 2022, the initial\nversion of CARLANE was created by Ju-\nlian Gebele, Bonifaz Stuhr, and Johann\nHaselberger from the Institute for Driver\nAssistance Systems and Connected Mo-\nbility (IFM). The IFM is a part of the Uni-\nversity of Applied Sciences Kempten. Fur-\nthermore, CARLANE was created by Boni-\nfaz Stuhr as part of his Ph.D. at the Au-\ntonomous University of Barcelona (UAB)\nand by Johann Haselberger as part of\nhis Ph.D. at the Technische Universität\nBerlin (TU Berlin).\nWho funded the creation of the dataset? If\nthere is an associated grant, please provide the\nname of the grantor and the grant name and num-\nber.\nThere is no specific grant for the creation\nof the CARLANE Benchmark. The datasets\nwere created as part of the work at the\nIFM and the University of Applied Sci-\nences Kempten.\nComposition\nWhat do the instances that comprise the dataset\nrepresent (e.g., documents, photos, people,\ncountries)? Are there multiple types of instances\n(e.g., movies, users, and ratings; people and inter-\nactions between them; nodes and edges)? Please\nprovide a description.\nThe instances are drives on diverse roads\nin simulation, in an abstract 1/8th real\nworld, and in full-scale real-world sce-\nnarios, along with lane annotations of\nthe up to four nearest lanes to the vehi-\ncle.\nHow many instances are there in total (of\neach type, if appropriate)?\nTable C.3 shows the per-domain and per-\nsubset breakdown of CARLANE instances.\nTuSimple is available at\nhttps://github.com/TuSimple/tusimple-\nbenchmark under the Apache License\nVersion 2.0, January 2004.\nDoes the dataset contain all possible instances\nor is it a sample (not necessarily random) of\ninstances from a larger set?\nIf the dataset is\na sample, then what is the larger set? Is the sam-\nple representative of the larger set (e.g., geographic\ncoverage)? If so, please describe how this represen-\ntativeness was validated/verified. If it is not repre-\nsentative of the larger set, please describe why not\n(e.g., to cover a more diverse range of instances,\nbecause instances were withheld or unavailable).\nThe datasets of CARLANE contain sam-\nples of driving scenarios and lane anno-\n143\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\nTable C.3: Dataset overview. Unlabeled images are denoted by *, partially labeled\nimages are denoted by **.\nDataset\ndomain\ntotal images\ntrain\nvalidation\ntest\nlanes\nMoLane\nCARLA simulation\n84,000\n80,000\n4,000\n-\n≤2\nmodel vehicle\n46,843\n43,843*\n2,000\n1,000\n≤2\nTuLane\nCARLA simulation\n26,400\n24,000\n2,400\n-\n≤4\nTuSimple [255]\n6,408\n3,268\n358\n2,782\n≤4\nMuLane\nCARLA simulation\n52,800\n48,000\n4,800\n-\n≤4\nmodel vehicle + TuSimple [255]\n12,536\n6,536**\n4,000\n2,000\n≤4\ntations encountered in simulation and\nthe real world. The datasets are not rep-\nresentative of all these driving scenarios,\nas the distribution of the latter is highly\ndynamic and diverse. Instead, the moti-\nvation was to resemble the variety and\nshifts of different domains in which such\nscenarios occur to strengthen the sys-\ntematic study of UDA methods for lane\ndetection. Therefore, CARLANE should\nbe considered as an UDA benchmark\nwith step-by-step testing possibility across\nthree domains, which makes it possible\nto include an additional safety mecha-\nnism for real-world testing.\nWhat data does each instance consist of?\n“Raw” data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a descrip-\ntion.\nEach labeled instance consists of the fol-\nlowing components:\n(1) A single 1280×720 image from a driv-\ning scenario.\n(2) A .json file entry for the correspond-\ning subset containing lane annotations\nfollowing TuSimple. The lanes’ y-values\ndiscretized by 56 raw anchors, the lanes’\nx-values to 101 gridding cells, with the\nlast gridding cell representing the absence\nof a lane. The file path to the correspond-\ning image is also stored in the .json file.\n(3) A .png file containing lane segmenta-\ntions following UFLD (https://github.com\n/cfzd/Ultra-Fast-Lane-Detection), where\neach of the four lanes has a different la-\nbel.\n(4) A .txt file entry for the corresponding\nsubset containing the linkage between\nthe raw image and its segmentation as\nwell as the presence and absence of a\nlane.\nEach unlabeled instance consists of an\n1280×720 image from a driving scenario\nand a .txt file entry for the corresponding\nsubset.\nIs there a label or target associated with each\ninstance? If so, please provide a description.\nAs described above, the labels per instance\nare discretized lane annotations and lane\nsegmentations.\nIs any information missing from individual in-\nstances? If so, please provide a description, ex-\nplaining why this information is missing (e.g., be-\n144\nC.7. DATASHEET FOR THE\nCARLANE BENCHMARK\ncause it was unavailable). This does not include\nintentionally removed information, but might in-\nclude, e.g., redacted text.\nEverything is included. No data is miss-\ning.\nAre relationships between individual instances\nmade explicit (e.g., users’ movie ratings, social\nnetwork links)? If so, please describe how these\nrelationships are made explicit.\nThere are no relationships made explicit\nbetween instances. However, some in-\nstances are part of the same drive and\ntherefore have an implicit relationship.\nAre there recommended data splits(e.g., train-\ning, development/validation, testing)? If so,\nplease provide a description of these splits, explain-\ning the rationale behind them.\nEach domain is split into training and\nvalidation subsets. Details are shown in\nTable C.3. The target domains for UDA\nadditionally include test sets, which were\nrecorded from separate tracks (model ve-\nhicle) or driving scenarios (TuSimple).\nSince UDA aims to adapt models to tar-\nget domains, only the target domains in-\nclude a test set.\nAre there any errors, sources of noise, or re-\ndundancies in the dataset? If so, please provide\na description.\nCARLANE was recorded from different\ndrives through simulation and real-world\ndomains. Therefore there are images\ncaptured from the same drive, which re-\nsult in similar scenarios for consecutive\nimages. Target domain samples were an-\nnotated by hand and may include hu-\nman labeling errors. However, we double-\nchecked labels and cleaned TuSimple’s\ntest set with our labeling tool.\nIs the dataset self-contained, or does it link to\nor otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? If it links to\nor relies on external resources, a) are there guaran-\ntees that they will exist, and remain constant, over\ntime; b) are there official archival versions of the\ncomplete dataset (i.e., including the external re-\nsources as they existed at the time the dataset was\ncreated); c) are there any restrictions (e.g., licenses,\nfees) associated with any of the external resources\nthat might apply to a dataset consumer? Please\nprovide descriptions of all external resources and\nany restrictions associated with them, as well as\nlinks or other access points, as appropriate.\nCARLANE is entirely self-contained.\nDoes the dataset contain data that might be\nconsidered confidential (e.g., data that is pro-\ntected by legal privilege or by doctor-patient\nconfidentiality, data that includes the content\nof individuals’ non-public communications)?\nIf so, please provide a description.\nThe full-scale real-world target domain\ncontains open-source images with un-\nblurred license plates and people from\nthe TuSimple dataset. This data should\nbe treated with respect and in accordance\nwith privacy policies. The other domains\ndo not contain data that might be con-\nsidered confidential since there where\nrecorded in simulations or a controlled\n1/8th real-world environment.\nDoes the dataset contain data that, if viewed\ndirectly, might be offensive, insulting, threat-\nening, or might otherwise cause anxiety? If so,\nplease describe why.\nCARLANE includes driving scenarios; there-\nfore, its datasets could cause anxiety in\npeople with driving anxiety.\nDoes the dataset identify any subpopulations\n(e.g., by age, gender)? If so, please describe how\nthese subpopulations are identified and provide a\ndescription of their respective distributions within\nthe dataset.\nNo.\n145\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\nIs it possible to identify individuals (i.e., one\nor more natural persons), either directly or\nindirectly (i.e., in combination with other data)\nfrom the dataset? If so, please describe how.\nYes, individuals could be identified in the\nfull-scale real-world target domain from\nTuSimple, since it contains unblurred li-\ncense plates and people. However, the\nremaining domains do not contain iden-\ntifiable individuals.\nDoes the dataset contain data that might be\nconsidered sensitive in anyway(e.g., data that\nreveals race or ethnic origins, sexual orienta-\ntions, religious beliefs, political opinions or\nunion memberships, or locations; financial or\nhealth data; biometric or genetic data; forms\nof government identification, such as social se-\ncurity numbers; criminal history)? If so, please\nprovide a description.\nThe full-scale real-world target domain\nfrom TuSimple could implicitly reveal\nsensitive information printed or put on\nthe vehicles or people’s wearings.\nCollection Process\nHow was the data associated with each in-\nstance acquired? Was the data directly observ-\nable (e.g., raw text, movie ratings), reported by\nsubjects (e.g., survey responses), or indirectly in-\nferred/derived from other data (e.g., part-of-speech\ntags, model-based guesses for age or language)?\nIf the data was reported by subjects or indirectly\ninferred/derived from other data, was the data val-\nidated/verified? If so, please describe how.\nThe source domain images of driving sce-\nnarios and the corresponding lane an-\nnotations were directly recorded from\nthe simulation. Lanes were manually la-\nbeled for the directly recorded real-world\nimages. For the images collected from\nthe model vehicle, the authors annotated\nthe data with a labeling tool created for\nthis task. The labeling tool is publicly\navailable at https://carlanebenchmark.\ngithub.io. The labeling tool is utilized\nto clean up the annotations of the test\nset in the real-world domain. The au-\nthors do not have information about the\nlabeling process of the full-scale target\ndomain since its data is derived from the\nTuSimple dataset.\nWhat mechanisms or procedures were used to\ncollect the data (e.g., hardware apparatuses\nor sensors, manual human curation, software\nprograms, software APIs)?\nHow were these\nmechanisms or procedures validated?\nThe source domain data was collected\nusing the CARLA simulator and its APIs\nwith a resolution of 1280×720 pixels. The\nreal-world 1/8th target domain was col-\nlected with a Stereolabs ZEDM camera\nwith 30 FPS and a resolution of 1280×720\npixels. The lane distributions were ad-\nditionally balanced with a bagging ap-\nproach, and lanes were annotated with\na labeling tool. More information can be\nfound in the corresponding paper and\nthe implementation. The implementa-\ntion and all used tools are publicly avail-\nable at https://carlanebenchmark.github.\nio.\nIf the dataset is a sample from a larger set,\nwhat was the sampling strategy (e.g., deter-\nministic, probabilistic with specific sampling\nprobabilities)?\nSource domain dataset entries are sam-\npled based on the relative angle β of the\nagent to the center lane. For MoLane,\nfive lane classes are defined for the bag-\nging approach: strong left curve (β ≤−45◦),\nsoft left curve (−45◦< β ≤−15◦), straight\n(−15◦< β < 15◦), soft right curve (15◦≤\n146\nC.7. DATASHEET FOR THE\nCARLANE BENCHMARK\nβ < 45◦) and strong right curve (45◦≤β).\nFor TuLane, three lane classes are de-\nfined for the bagging approach: left curve\n(−12◦< β ≤5◦), straight (−5◦< β < 5◦)\nand right curve (5◦≤β < 12◦).\nWho was involved in the data collection pro-\ncess (e.g., students, crowdworkers, contrac-\ntors) and how were they compensated (e.g.,\nhow much were crowdworkers paid)?\nOnly the authors were involved in the\ncollection process. The authors do not\nhave information about the people in-\nvolved in collecting the TuSimple dataset.\nOver what timeframe was the data collected?\nDoes this timeframe match the creation timeframe\nof the data associated with the instances (e.g., re-\ncent crawl of old news articles)? If not, please de-\nscribe the timeframe in which the data associated\nwith the instances was created.\nMoLane’s data was collected and anno-\ntated from June 2021 to August 2021. Data\nfor TuLane’s source domain was collected\nin February 2022.\nWere any ethical review processes conducted\n(e.g., by an institutional review board)? If so,\nplease provide a description of these review pro-\ncesses, including the outcomes, as well as a link or\nother access point to any supporting documenta-\ntion.\nNo ethical reviews have been conducted\nto date. However, an ethical review may\nbe conducted as part of the paper review\nprocess.\nPreprocessing/cleaning\n/labeling\nWas any preprocessing/cleaning/labeling of\nthe data done (e.g., discretization or buck-\neting, tokenization, part-of-speech tagging,\nSIFT feature extraction, removal of instances,\nprocessing of missing values)?\nIf so, please\nprovide a description. If not, you may skip the\nremaining questions in this section.\nAs described above, lane annotations were\nlabeled or cleaned using a labeling tool\nand sampled based on the relative angle\nβ of the agent to the center lane.\nWas the “raw” data saved in addition to the\npreprocessed/cleaned/labeled data (e.g., to\nsupport unanticipated future uses)? If so, please\nprovide a link or other access point to the “raw”\ndata.\nNo.\nIs the software that was used to preprocess/-\nclean/label the data available?\nIf so, please\nprovide a link or other access point.\nYes, the software is available at https://\ncarlanebenchmark.github.io.\nUses\nHas the dataset been used for any tasks al-\nready? If so, please provide a description.\nThe datasets were used to create UDA\nbaselines for the corresponding paper\npresenting the CARLANE Benchmark.\nIs there a repository that links to any or all\npapers or systems that use the dataset? If so,\nplease provide a link or other access point.\nYes, the baselines presented in the cor-\nresponding paper are available at https:\n//carlanebenchmark.github.io.\nWhat(other) tasks could the dataset be used\nfor?\nIn a broader sense, the datasets of CAR-\nLANE can also be used for unsupervised\nand semi-supervised learning and par-\ntially for supervised learning.\nIs there anything about the composition of\nthe dataset or the way it was collected and\npreprocessed/cleaned/labeled that might im-\n147\nAPPENDIX C. SUPPLEMENTARY MATERIAL:\nA LANE DETECTION BENCHMARK FOR MULTI-TARGET DOMAIN ADAPTATION\npact future uses? For example, is there anything\nthat a dataset consumer might need to know to\navoid uses that could result in unfair treatment of\nindividuals or groups (e.g., stereotyping, quality\nof service issues) or other risks or harms (e.g., le-\ngal risks, financial harms)? If so, please provide a\ndescription. Is there anything a dataset consumer\ncould do to mitigate these risks or harms?\nYes, TuLane and MuLane contain open-\nsource images with unblurred license\nplates and people. This data should be\ntreated with respect and in accordance\nwith privacy policies. In general, CAR-\nLANE contributes to the research in the\nfield of autonomous driving, in which\nmany unresolved ethical and legal ques-\ntions are still being discussed. The step-\nby-step testing possibility across three\ndomains makes it possible for our bench-\nmark to include an additional safety mech-\nanism for real-world testing. This can\nhelp the consumer to mitigate the risks\nand harms to some extent.\nAre there tasks for which the dataset should\nnot be used? If so, please provide a description.\nSince CARLANE focuses on UDA for lane\ndetection and spans a limited number\nof driving scenarios, consumers should\nnot solely really on this dataset to train\nmodels for fully autonomous driving.\nDistribution\nWill the dataset be distributed to third parties\noutside of the entity (e.g., company, insti-\ntution, organization) on behalf of which the\ndataset was created?\nIf so, please provide a\ndescription.\nYes, CARLANE is publicly available on\nthe internet for anyone interested in us-\ning it.\nHow will the dataset will be distributed (e.g.,\ntarball on website, API, GitHub)? Does the\ndataset have a digital object identifier (DOI)?\nCARLANE is distributed through kaggle\nat https://www.kaggle.com/datasets/\ncarlanebenchmark/carlane-benchmark\nDOI: 10.34740/kaggle/dsv/3798459\nWhen will the dataset be distributed?\nThe datasets have been available on kag-\ngle since June 17, 2022.\nWill the dataset be distributed under a copy-\nright or other intellectual property (IP) license,\nand/or under applicable terms o fuse (ToU)?\nIf so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise\nreproduce, any relevant licensing terms or ToU, as\nwell as any fees associated with these restrictions.\nCARLANE is licensed under the Apache\nLicense Version 2.0, January 2004.\nHave any third parties imposed IP-based or\nother restrictions on the data associated with\nthe instances?\nIf so, please describe these re-\nstrictions, and provide a link or other access point\nto, or otherwise reproduce, any relevant licensing\nterms, as well as any fees associated with these\nrestrictions.\nTuSimple, which is used for TuLanes and\nMuLanes target domains, is licensed un-\nder the Apache License Version 2.0, Jan-\nuary 2004.\nDo any export controls or other regulatory re-\nstrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions,\nand provide a link or other access point to, or oth-\nerwise reproduce, any supporting documentation.\nUnknown to authors of the datasheet.\nMaintenance\nWho will be supporting/hosting/maintaining\nthe dataset?\n148\nC.7. DATASHEET FOR THE\nCARLANE BENCHMARK\nCARLANE is hosted on kaggle and sup-\nported and maintained by the authors.\nHow can the owner/curator/manager of the\ndataset be contacted (e.g., email address)?\nThe curators of the datasets can be con-\ntacted under\ncarlane.benchmark@gmail.com.\nIs there an erratum? If so, please provide a link\nor other access point.\nNo.\nWill the dataset be updated (e.g., to correct\nlabeling errors, add new instances, delete in-\nstances)?\nIf so, please describe how often, by\nwhom, and how updates will be communicated to\ndataset consumers (e.g., mailing list, GitHub)?\nNew versions of CARLANE’s datasets will\nbe shared and announced on our home-\npage (https://carlanebenchmark.github.\nio) and at kaggle if corrections are neces-\nsary.\nWill older versions of the dataset continue\nto be supported/hosted/maintained?\nIf so,\nplease describe how. If not, please describe how\nits obsolescence will be communicated to dataset\nconsumers.\nYes, we plan to support versioning of the\ndatasets so that all the versions are avail-\nable to potential users. We maintain the\nhistory of versions via our homepage\n(https://carlanebenchmark.github.io)\nand at kaggle. Each version will have a\nunique DOI assigned.\nIf others want to extend/augment/build on/-\ncontribute to the dataset, is there a mech-\nanism for them to do so?\nIf so, please pro-\nvide a description. Will these contributions be\nvalidated/verified? If so, please describe how. If\nnot, why not? Is there a process for communi-\ncating/distributing these contributions to dataset\nconsumers? If so, please provide a description.\nOthers can extend/augment/build on CAR-\nLANE with the support of the open-source\ntools provided on our homepage. Be-\nsides these tools, there will be no mech-\nanism to validate or verify the extended\ndatasets. However, others are free to re-\nlease their extension of the CARLANE\nBenchmark or its datasets under the Apa-\nche License Version 2.0.\n149\nD Supplementary Material:\nContent-Consistent Translation with Masked\nDiscriminators\nD.1\nThe FeaMGAN Architecture\nFATE\nResBlk\n3x3\nFATE\nResBlk\n3x3\nResBlk\n3x3\nConv\n1x1\nTanh\nResBlk\n3x3\nResBlk\n3x3\nEncoder\nGenerator Stream\n×6\n×2\n×2\n×6\nContent Stream\nEncoder\nFigure D.1: Generator architecture. Arrows with dashed lines indicate connections\nat multiple levels between the two streams. Best viewed in color.\nGenerator. As shown in Figure D.1, our generator consists of a content stream\nencoder, a content stream, a generator stream encoder, and a generator stream.\nThe content stream encoder shown in Figure D.4 is utilized to create the initial\nfeatures of the source image and condition. These initial features are the input to\nthe content stream, which creates features for multiple levels with residual blocks.\nThe statistics of these features are then integrated into the generator at multiple\nlevels utilizing the residual FATE blocks shown in Figure D.2. The generator stream\nutilizes the encoder shown in Figure D.3 to create the initial latent from which the\ntarget image is generated. To further enforce content consistency, we do not use a\nvariational autoencoder to obtain an deterministic latent. In addition, we found\nthat utilizing additional residual blocks in the last layers of the generator stream\nimproves performance, likely due to further refinement of the preceding upsam-\n151\nAPPENDIX D. SUPPLEMENTARY MATERIAL:\nCONTENT-CONSISTENT TRANSLATION WITH MASKED DISCRIMINATORS\npled features. We use spectral instance normalization for the residual blocks in the\ncontent stream and spectral batch normalization for the residual blocks in the gen-\nerator stream. The convolutional layers in the generator stream encoder have the\nfollowing numbers of filters: [256,512,1024]. The residual blocks in the generator\nhave the following numbers of filters: [1024,1024,1024,512,256,128,64,64,64,64].\nThe numbers of filters of the convolutional layers in the content streams encoder\nare [64,64]. The numbers of filters in the content stream match those of the out-\nput of the preceding residual block in the generator stream at the respective level:\n[64,128,256,512,1024,1024,1024,1024]. For all residual blocks, we use 3×3 convo-\nlutions and 1×1 convolutions for the skip connections. γ and β in the FATE and\nFADE blocks are created with 3 × 3 convolutions. Throughout the generator, we\nuse a padding of 1 for the convolutions - we only downsample with strides and\ndownsampling layers. We utilize the \"nearest\" upsampling and downsampling from\nPytorch. For our small model, we halve the number of filters.\nConv\nFATE\nFeatures\nLRelu\nUpsample\nConv\nFATE\nLRelu\nConv\nFATE\nLRelu\nFeatures\nFeatures\nFATE ResBlk\nFigure D.2: FATE residual block. The FATE residual block used in the generator\nstream.\n152\nD.1. THE FEAMGAN ARCHITECTURE\nConv\n3x3\nS2\nLRelu\nConv\n3x3\nS2\nLRelu\nDown-\nsample\nConv\n3x3\nGenerator Stream Encoder\nFigure D.3: Generator stream encoder. The generator stream encoder used to\nencode the input image and condition for the generator stream.\nConv\n1x1\nLRelu\nConv\n1x1\nLRelu\nContent Stream Encoder\nFigure D.4: Content stream encoder. The content stream encoder used to encode\nthe input image and condition for the content stream.\nConv\n3x3\nLRelu\nConv\n3x3\nSigmoid\nFeatures\nAttention\nFigure D.5: Attention module. The attention module used in the FATE block to\nattend to the statistics of the features.\n153\nAPPENDIX D. SUPPLEMENTARY MATERIAL:\nCONTENT-CONSISTENT TRANSLATION WITH MASKED DISCRIMINATORS\nConv\n3x3, s2x2\nNorm\nLReLu\n×3\nConv\n1x1\nNorm\nLReLu\nConv\n3x3, s2x2\nNorm\nLReLu\n×1\nConv\n3x3, s2x2\nNorm\nLReLu\n×1\nConv\n3x3, s 2x2\nNorm\nLReLu\nDown-\nsample\n×3\n×3\nConv\n1x1\nNorm\nLReLu\nConv\n3x3\nNorm\nLReLu\nConv\n3x3\nNorm\nLReLu\nConv\n3x3\nNorm\nLReLu\nUp-\nsample\n∑\nFake\nReal\nFigure D.6: Discriminator architecture. Arrows with dashed lines indicate connec-\ntions at multiple levels between the two components. Best viewed in color.\nDiscriminator. As shown in Figure D.6, our discriminator consists of downsampling,\nupsampling, and prediction components. First, the input images of the source or\ntarget domain are downsampled via 5 stride 2 convolutions. We transform the out-\nput feature map of the last 4 downsampling convolutions with 1×1 convolutions.\nThe last transformed feature map is used as input for the upsampling components,\nwhile the other transformed feature maps are added to the feature maps of the up-\nsampling component for the receptive level. Then we utilize the feature maps of the\n3 upsampling levels to create the final prediction on 3 levels. Thereby, we first apply\na convolutional layer on the upsampled features. This convolution is followed by\ntwo convolutional layers: One is used to create the prediction feature map of depth\n1, and the feature map of the other convolutional layer is multiplied by the segmen-\ntation map. The resulting segmentation feature map is then collapsed into depth\n1 by adding the depth dimensions together. At last, the collapsed segmentation\nfeature map is added to the prediction feature map to produce the final prediction.\nIn this way, the discriminator is encouraged to produce class-specific predictions.\nWe use spectral instance normalization for all convolutional layers. The 3×3 con-\nvolutions of the downsampling component have the following numbers of filters:\n[64,128,256,512,512]. The 1 × 1 convolutions of the downsampling component\nhave the following numbers of filters: [256,256,256,256]. The first convolutions in\nthe prediction component have the following numbers of filters: [128,128,128]. The\nconvolutions that are multiplied by the downsampled segmentation maps have\nthe following numbers of filters: [128,128,128]. The convolution used to create the\ndownsampled segmentation map has 128 filters. The convolutions to create the\npredictions have the following numbers of filters: [1,1,1]. Throughout the discrimi-\n154\nD.2. ADDITIONAL DATASET DETAILS\nnator, we use a padding of 1 for the convolutions - we only downsample with strides\nand downsampling layers. We utilize the \"bilinear\" upsampling and downsampling\nfrom Pytorch. For our small model, we halve the number of filters.\nD.2\nAdditional Dataset Details\nIn Table D.3, we show additional details about the used datasets. Since we compare\nour method i.a. to the transferred PFD [218] images provided by EPE [216], we use a\nbase image height of 526 throughout our experiments. The aspect ratio is preserved\nwhen resizing the input images. To match the input sizes of the images of both\ndomains, we apply cropping to the image with the larger width if the image sizes of\nboth domains do not align. The resulting images are randomly flipped before the\nsampling strategy is applied.\nD.3\nAdditional Training Details\nIn Table D.2, we show additional details about the hyperparameters used for training\nthe four translation tasks. No tuning was performed for other translation tasks then\nPFD→Cityscapes besides adapting the learning rate schedule for the dataset lengths\nof these tasks.\nD.4\nAdditional Results\nWe show additional results of our experiments in Figures D.7, D.8, D.9, D.10, and\nD.11. In Table D.4, we report additional results from our cKVD metric and the\nstability of all results over five runs. Furthermore, we report the stability of all results\nfrom the ablation study in Table D.5. We note that the results for most baselines\nand for our method show non-negligible deviations in many tasks.\n155\nAPPENDIX D. SUPPLEMENTARY MATERIAL:\nCONTENT-CONSISTENT TRANSLATION WITH MASKED DISCRIMINATORS\nTable D.1: cKVD class mapping.\ncKVD Class\nMSeg-Id(Name)\nsky\n142(sky)\nground\n94(gravel), 95(platform), 97(railroad),\n100(pavement-merged), 101(ground)\nroad\n98(road)\nterrain\n102(terrain)\nvegetation\n174(vegetation)\nbuilding\n31(tunnel), 32(bridge), 33(building-parent),\n35(building), 36(ceiling-merged)\nroadside-obj.\n130(streetlight), 131(road_barrier), 132(mailbox),\n133(cctv_camera), 134(junction_box), 135(traffic_sign),\n136(traffic_light), 137(fire_hydrant), 138(parking_meter),\n139(bench), 140(bike_rack), 141(billboard)\nperson\n125(person), 126(rider_other), 127(bicyclist),\n128(motorcyclist)\nvehicle\n175(bicycle), 176(car), 177(autorickshaw),\n178(motorcycle), 180(bus), 181(train),\n182(truck), 183(trailer), 185(slow_wheeled_object)\nrest\nall other MSeg classes\nTable D.2: Additional details of the used datasets.\nDataset\nResolution\nfps\nUsed Train/Val Data\nTask\nInput Resolution Input Cropping\nPFD [218]\n1914×1052\n-\nall images\nPFD→Cityscapes\n957×526\n-\nViper [217]\n1920×1080 ∼15 all train/val data, but no night sequences Viper→Cityscapes\n935×526\n-\nCityscapes [50]\n2048×1024\n17\nall sequences of the train/val data\nPFD→Cityscapes\n1.052×526\n957×526\nViper→Cityscapes\n1.052×526\n935×526\nBDD100K [285]\n1280×720\n30\ntrain: first 100k, val: first 40k\nDay→Night\n935×526\n-\ntrain: first 50k, val: first 40k\nClear→Snowy\nTable D.3: Additional training details.\nTask\nEpochs\nSchedule\nDecay\nLocal Discriminator Batch Size\nPFD→Cityscapes\n20\nhalf learning rate stepwise, learning rate ≥0.0000125 after each 3rd epoch\n32\nViper→Cityscapes\n5\nhalf learning rate stepwise, learning rate ≥0.0000125\nafter each epoch\n32\nDay→Night\n5\nhalf learning rate stepwise, learning rate ≥0.0000125\nafter each epoch\n32\nClear→Snowy\n10\nhalf learning rate stepwise, learning rate ≥0.0000125\nafter each epoch\n32\n156\nD.4. ADDITIONAL RESULTS\nInput\nEPE\nFeaMGAN (ours)\nFigure D.7: Qualitative comparison to EPE. We compare our method with the\nprovided inferred images of EPE [216]. Best viewed in color.\n157\nAPPENDIX D. SUPPLEMENTARY MATERIAL:\nCONTENT-CONSISTENT TRANSLATION WITH MASKED DISCRIMINATORS\nInput\nEPE\nFeaMGAN (ours)\nFigure D.8: Qualitative comparison to EPE. We compare our method with the\nprovided inferred images of EPE [216]. Results are randomly sampled from the best\nmodel. Best viewed in color.\n158\nD.4. ADDITIONAL RESULTS\nPFD→Cityscapes\nViper→Cityscapes\nDay→Night\nClear→Snowy\nInput\nMUNIT\nCUT\nTSIT\nQS-Attn\nFeaMGAN (ours)\nFigure D.9: Qualitative comparison to prior work. Results are randomly sampled\nfrom the best model. Best viewed in color.\n159\nAPPENDIX D. SUPPLEMENTARY MATERIAL:\nCONTENT-CONSISTENT TRANSLATION WITH MASKED DISCRIMINATORS\nInput\n252×252\n352×352\n464×464\n512×512\nFigure D.10: Qualitative ablation of crop sizes. For each crop size, results are\nrandomly sampled from the best model. Best viewed in color.\n160\nD.4. ADDITIONAL RESULTS\nInput\nFull\nw/o Dis. Mask\nw/o Local Dis.\nw/ FADE w/o FATE\nFigure D.11: Qualitative ablations. Results are randomly sampled from the best\nmodel. Best viewed in color.\n161\nAPPENDIX D. SUPPLEMENTARY MATERIAL:\nCONTENT-CONSISTENT TRANSLATION WITH MASKED DISCRIMINATORS\nTable D.4: Extended quantitative comparison to prior work. Models were trained\nusing their official implementations. Results are reported as the average across five\nruns.\nMethod\nFID\nKID\nsKVD\ncKVD\nAVG\nsky\nground\nroad\nterrain\nvegetation building roadside-obj.\nperson\nvehicle\nrest\nPFD→Cityscapes\nColor Transfer\n91.01+0.05\n−0.03\n94.82+0.14\n−0.11 18.16+0.20\n−0.18 50.87+1.18\n−1.36 58.05+2.28\n−1.24 16.66+0.19\n−0.26 16.38+0.14\n−0.09 26.91+1.23\n−1.29 28.18+0.44\n−0.26 32.60+0.33\n−0.45\n58.36+5.56\n−7.85\n125.37+11.20\n−7.66\n55.12+1.38\n−1.86 91.11+0.39\n−0.84\nMUNIT\n40.36+1.22\n−1.18\n29.98+1.43\n−1.14 14.99+0.06\n−0.05 43.24+0.67\n−0.71 37.92+0.85\n−0.55 13.23+0.29\n−0.23 14.33+0.16\n−0.12 22.70+0.81\n−0.75 24.97+0.37\n−0.35 27.52+0.10\n−0.15\n58.24+3.37\n−3.87\n108.61+5.56\n−4.30\n45.35+0.32\n−0.24 79.54+0.25\n−0.49\nCUT\n49.55+5.19\n−3.63\n44.25+7.57\n−4.99 16.85+1.28\n−1.82 37.53+1.09\n−1.13 28.76+0.98\n−0.39 11.17+1.26\n−0.84 13.92+0.58\n−0.80 13.49+1.21\n−1.54 24.20+1.23\n−1.39 24.69+0.92\n−0.86\n57.45+4.33\n−2.22\n90.52+3.33\n−6.67\n40.92+2.77\n−3.08 70.20+3.10\n−0.99\nTSIT\n38.70+1.59\n−1.16\n28.70+1.81\n−1.27 10.80+0.57\n−0.29 42.35+0.73\n−0.89 40.13+1.58\n−1.28 13.74+0.39\n−1.14 14.09+0.27\n−0.78 23.48+0.84\n−1.17 23.74+0.36\n−0.47 25.76+0.16\n−0.22\n51.95+2.31\n−2.72\n107.98+4.67\n−5.61\n43.49+1.32\n−1.01 79.13+2.72\n−2.05\nQS-Attn\n49.42+5.71\n−6.93 42.87+7.34\n−10.03 14.01+0.34\n−0.38 38.57+2.85\n−1.68 29.50+2.40\n−1.88 11.69+1.51\n−0.69 13.92+0.68\n−0.76 13.22+2.40\n−1.23 23.99+1.34\n−1.72 23.36+1.32\n−1.56\n57.88+2.89\n−1.88\n100.32+14.23\n−6.18\n40.77+3.32\n−2.01 71.06+5.17\n−3.29\nFeaMGan-S (ours)\n45.16+5.24\n−3.23\n34.93+6.92\n−3.48 13.87+0.66\n−0.89 40.50+2.83\n−1.96 40.57+5.56\n−4.83 13.32+2.35\n−1.48 16.00+1.51\n−2.76 24.55+2.88\n−3.76 20.82+5.94\n−4.03 27.54+1.07\n−0.34\n63.09+8.99\n−4.97\n102.53+1.58\n−0.79\n42.58+3.36\n−4.07 53.99+5.41\n−5.45\nFeaMGan (ours)\n46.12+4.60\n−5.80\n36.56+6.70\n−7.96 13.69+1.13\n−1.15 41.19+2.89\n−2.81 42.69+4.00\n−5.01 14.97+3.86\n−3.23 17.35+5.09\n−4.28 26.51+5.70\n−3.27 20.25+2.88\n−2.56 26.34+1.36\n−0.77\n64.64+4.94\n−5.07\n102.23+10.58\n−10.91 42.38+2.91\n−3.01 54.52+5.00\n−3.09\nViper→Cityscapes\nColor Transfer\n89.30+0.06\n−0.05\n83.51+0.10\n−0.10 20.20+0.32\n−0.24 51.23+0.75\n−0.75 65.74+1.33\n−2.59 19.98+1.08\n−0.44 16.87+0.16\n−0.14 26.65+2.69\n−2.39 28.79+0.45\n−0.30 36.21+0.14\n−0.14\n41.97+1.52\n−3.65\n139.26+10.03\n−10.10 57.10+1.04\n−0.74 79.73+0.75\n−0.82\nMUNIT\n47.96+0.52\n−1.11\n30.35+0.68\n−1.29 14.14+0.10\n−0.09 59.62+1,87\n−2.34 46.44+2.83\n−2.02 15.85+0.87\n−0.71 14.11+0.31\n−0.11 32.69+2.23\n−3.94 25.75+0.27\n−0.36 25.76+0.24\n−0.24\n39.99+1.07\n−1.26\n274.68+15.46\n−23.29 46.64+1.75\n−1.70 74.33+0.40\n−0.79\nCUT\n60.35+6.50\n−8.13 49.48+7.19\n−10.15 16.80+1.04\n−1.11 51.02+3.71\n−4.32 34.79+6.87\n−2.98 14.88+0.79\n−0.76 16.80+2.50\n−1.68 22.40+2.41\n−2.33 22.91+1.81\n−0.84 23.34+1.10\n−1.04\n45.00+5.23\n−2.68\n224.47+29.25\n−28.29 42.29+2.56\n−2.55 63.36+1.72\n−3.17\nTSIT\n45.26+1.92\n−1.39\n28.40+2.55\n−2.16\n8.47+0.25\n−0.26\n50.03+3.06\n−2.12 46.25+0.69\n−0.93 14.46+2.11\n−1.95 12.28+0.98\n−0.97 31.95+5.17\n−4.96 24.86+1.50\n−1.27 24.91+1.26\n−1.43\n45.19+2.35\n−2.10\n184.05+18.06\n−10.62 44.59+2.46\n−1.55 71.72+3.90\n−3.72\nQS-Attn\n55.62+12.05\n−9.66\n39.31+11.87\n−9.92\n12.99+1.87\n−1.60 63.22+17.47\n−13.74 36.44+15.38\n−4.97\n16.04+1.56\n−1.26 15.25+1.10\n−2.49 25.20+4.27\n−2.30 26.09+1.88\n−2.02 24.24+1.26\n−0.89\n46.54+1.63\n−1.84\n326.60+171.61\n−128.90 46.44+3.78\n−5.51 69.33+5.26\n−5.06\nFeaMGan-S (ours)\n52.79+2.50\n−2.79\n35.92+3.88\n−3.18 14,34+0.65\n−0.73 45.38+1.53\n−1.63 56.75+5.13\n−8.76 18.51+1.49\n−1.08 16.68+1.90\n−3.18 42.85+1.59\n−1.97 22.70+1.41\n−1.40 26.82+0.74\n−1.12\n45.27+1.37\n−1.11\n130.76+6.27\n−11.95 45.25+1.49\n−3.29 48.19+2.45\n−1.49\nFeaMGan (ours)\n51.56+1.97\n−3.56\n34.63+3.32\n−5.48 14.01+0.58\n−0.73 47.21+1.29\n−1.10 58.87+3.48\n−1.62 21.20+0.72\n−0.93 18.03+1.38\n−0.62 50.01+7.63\n−3.72 23.55+3.08\n−2.42 26.67+0.46\n−0.66\n45.55+1.11\n−1.79\n132.77+2.96\n−3.46 45.13+1.72\n−1.51 50.32+1.55\n−1.24\nDay→Night\nColor Transfer\n125.90+0.13\n−0.10 140.60+0.10\n−0.10 32.58+0.32\n−0.52 56.52+1.76\n−1.26 47.62+0.50\n−0.78 27.41+1.37\n−1.27 15.89+0.46\n−0.23 32.60+1.76\n−2.27 44.24+0.30\n−0.25 32.61+0.68\n−1.07 128.57+11.25\n−13.18\n108.52+8.17\n−6.36\n25.65+0.43\n−0.37 102.06+0.39\n−0.31\nMUNIT\n42.53+1.65\n−1.27\n31.83+1.73\n−0.98 15.02+0.64\n−0.65 50.83+1.25\n−0.88 29.25+0.23\n−0.29 28.00+0.72\n−0.50 13.49+0.30\n−0.16 36.57+1.16\n−0.53 44.86+0.31\n−0.35 24.96+0.69\n−0.59\n115.00+6.94\n−5.17\n101.70+4.53\n−4.06\n19.66+0.34\n−0.33 94.82+1.31\n−1.37\nCUT\n34.36+3.71\n−6.12\n20.54+4.81\n−7.05 10.16+1.98\n−1.14 53.55+3.05\n−3.22 31.89+0.92\n−2.06 27.44+1.58\n−1.46 13.14+0.64\n−0.73 40.93+6.70\n−8.59 49.79+3.41\n−2.78 25.52+1.69\n−1.71 104.26+7.21\n−10.28\n122.50+11.90\n−10.06 27.30+2.60\n−2.87 92.76+0.52\n−1.51\nTSIT\n54.9796.83\n−7.99 33.21+5.26\n−6.21 12.71+5.77\n−3.49 57.91+2.92\n−2.28 36.27+2.32\n−1.39 31.56+1.18\n−2.21 16.93+2.20\n−1.07 45.23+9.74\n−4.86 54.82+4.55\n−4.89 29.09+3.19\n−1.65 143.47+14.01\n−10.47\n99.30+3.07\n−5.53\n27.43+2.98\n−4.22 94.98+2.46\n−2.60\nQS-Attn\n46.68+2.73\n−2.03\n21.47+3.94\n−2.55\n7.58+1.27\n−1.77\n52.02+4.14\n−3.29 31.62+1.73\n−1.66 26.73+2.64\n−3.41 13.26+0.99\n−0.92 38.25+5.42\n−3.84 47.26+3.31\n−3.13 25.42+2.05\n−1.80 100.84+11.90\n−7.85\n123.79+18.23\n−14.72 26.67+4.28\n−3.72 86.39+5.62\n−4.37\nFeaMGan-S (ours) 70.40+15.29\n−4.76\n51.30+21.06\n−6.09\n14.68+3.45\n−1.84 46.66+2.63\n−2.20 30.35+1.05\n−0.84 35.47+3.71\n−3.90 17.26+1.46\n−1.08 47.29+10.32\n−8.77\n27.12+1.14\n−1.10 25.22+1.36\n−1.38\n116.25+4.47\n−4.70\n70.75+4.87\n−5.91\n19.29+0.52\n−0.87 77.60+4.37\n−2.09\nFeaMGan (ours)\n66.39+6.39\n−8.43 46.96+10.07\n−10.41 13.14+3.34\n−2.01 46.88+2.52\n−2.83 29.72+2.56\n−1.42 35.94+1.75\n−1.77 17.48+0.98\n−1.44 49.78+7.45\n−7.78 28.78+1.94\n−3.03 25.65+0.71\n−0.61\n115.66+2.72\n−5.03\n70.94+9.44\n−11.48 19.23+0.64\n−0.78 75.57+4.86\n−3.73\nClear→Snowy\nColor Transfer\n46.85+0.12\n−0.38\n19.44+0.43\n−1.43 14.91+0.86\n−2.97 42.89+13.81\n−3.96\n25.78+1.19\n−2.03 22.99+3.22\n−2.15 16.01+0.37\n−0.35 21.54+1.72\n−1.52 41.13+7.40\n−2.60 24.20+1.99\n−0.78\n57.67+18.52\n−11.79\n128.26+94.11\n−29.48 25.95+7.36\n−2.36 65.39+5.38\n−1.91\nMUNIT\n44.74+1.23\n−0.79\n17.48+0.59\n−0.86 11.65+0.34\n−0.22 48.10+0.49\n−0.73 28.47+0.78\n−0.73 25.64+0.30\n−0.46 15.27+0.21\n−0.13 26.21+0.60\n−0.48 40.31+0.58\n−0.37 24.26+0.13\n−0.09\n101.98+1.73\n−4.77\n116.14+3.89\n−4.34\n21.63+0.41\n−0.18 81.08+0.31\n−0.63\nCUT\n46.03+1.08\n−0.85\n15.70+0.77\n−0.94 14.71+1.15\n−0.94 43.91+0.97\n−0.78 26.74+0.50\n−0.63 21.96+0.75\n−0.96 13.15+0.31\n−0.50 21.49+1.02\n−1.08 35.20+0.47\n−0.28 25.31+0.50\n−0.39\n76.67+4.18\n−6.21\n119.13+16.19\n−10.00 23.91+0.59\n−1.18 75.51+1.17\n−0.60\nTSIT\n79.29+5.08\n−6.69\n40.02+7.17\n−7.14 12.97+0.37\n−0.51 41.52+3.31\n−2.59 28.022.81\n−1.34 22.72+2.06\n−1.40 14.32+0.64\n−0.48 18.92+2.42\n−2.13 34.54+2.08\n−2.52 23.02+0.69\n−0.66\n72.13+7.37\n−4.90\n104.05+12.49\n−12.64 21.64+2.06\n−1.56 75.84+4.73\n−5.53\nQS-Attn\n60.91+0.79\n−1.02\n18.85+1.05\n−1.36 14.19+1.70\n−1.01 44.00+1.95\n−1.80 25.60+0.50\n−0.75 22.04+1.20\n−1.41 13.24+0.17\n−0.16 22.71+0.46\n−0.71 36.02+2.20\n−1.40 26.45+1.42\n−1.75\n78.58+4.70\n−4.53\n114.07+13.76\n−8.81\n25.17+3.07\n−1.42 76.08+1.84\n−2.29\nFeaMGan-S (ours)\n57.93+1.37\n−1.68\n16.24+1.19\n−0.59 11.88+0.55\n−0.52 38.28+2.86\n−2.83 22.69+0.67\n−1.34 25.71+1.14\n−1.76 15.82+0.82\n−1.27 37.47+3.98\n−4.13 25.94+1.50\n−2.69 21.80+0.53\n−0.61\n75.32+4.53\n−6.80\n81.47+22.52\n−14.46 19.10+0.55\n−0.30 57.46+2.18\n−3.98\nFeaMGan (ours)\n56.78+0.81\n−0.32\n14.77+0.98\n−1.59 11.36+0.21\n−0.43 41.72+2.61\n−1.76 22.71+1.17\n−1.09 26.64+3.01\n−2.07 16.19+1.74\n−1.13 38.00+3.46\n−2.77 27.78+2.95\n−2.21 21.41+0.61\n−0.69\n79.35+3.72\n−1.97\n105.08+35.80\n−25.18 19.50+0.67\n−1.04 60.59+1.99\n−1.90\n162\nD.4. ADDITIONAL RESULTS\nTable D.5: Results are reported as the average across five runs.\nMethod\nFID\nKID\nsKVD\ncKVD\nAVG\nsky\nground\nroad\nterrain\nvegetation\nbuilding\nroadside-obj.\nperson\nvehicle\nrest\nFeaMGan (Full)\n46.11+4.60\n−5.80\n36.66+6.70\n−7.96\n13.69+1.13\n−1.15 41.19+2.89\n−2.81\n42.69+4.00\n−5.01\n14.97+3.86\n−3.23 17.35+5.09\n−4.28\n26.51+5.70\n−3.27\n20.25+2.88\n−2.56\n26.34+1.36\n−0.77\n64.64+4.94\n−5.07\n102.23+10.58\n−10.91 42.38+2.91\n−3.01 54.52+5.00\n−3.09\nw/o Dis. Mask\n37.10+3,03\n−5,46\n25.88+3.60\n−8.65\n14.73+1.27\n−1.66 39.65+4.73\n−4.16 26.70+4.09\n−5.99\n15.81+5.28\n−3.67 16.65+4.43\n−2.85 31.02+11.19\n−8.23\n22.97+4.15\n−6.89\n25.39+2.75\n−1.24\n67.01+6.63\n−7.99\n93.78+10.71\n−7.90\n44.23+2.88\n−4.95 52.91+5.34\n−5.61\nw/ FADE w/o FATE 45.46+2.68\n−2.65\n35.73+4.44\n−3.53\n13.17+0.54\n−0.60 40.90+4.40\n−2.04\n41.49+8.66\n−8.86\n13.78+3.22\n−1.70 16.78+4.99\n−2.30\n25.30+2.41\n−2.12\n20.58+1.62\n−1.88\n27.21+1.93\n−0.68\n63.12+3.77\n−5.25\n104.43+13.22\n−7.73\n42.44+1.27\n−0.91 53.83+5.46\n−1.96\nw/ Random Crop\n47.88+5.10\n−3.82\n38.48+7.27\n−4.14\n13.37+0.61\n−1.14 40.18+1.55\n−1.66\n39.88+3.92\n−7.65\n12.90+0.94\n−0.68 14.65+1.62\n−2.00 25.09+2.01\n−2.93\n21.89+5.21\n−3.36\n27.32+2.05\n−1.87\n64.32+2.86\n−1.79\n98.81+7.63\n−5.61\n43.08+3.81\n−2.90 53.86+1.66\n−1.56\nw/ VGG Crop\n51.23+5.12\n−2.11\n42.46+6.65\n−3.42\n13.56+0.78\n−0.81 40.62+2.22\n−1.99\n40.32+4.36\n−5.72\n13.3+2.06\n−1.83\n15.67+2.15\n−2.14\n26.47+4.07\n−1.25\n21.09+1.61\n−2.41\n27.28+0.98\n−1.46\n65.23+8.64\n−3.75\n99.61+9.27\n−7.66\n43.19+2.16\n−3.12 53.94+3.81\n−5.04\nw/o Local Dis.\n- w/ 256×256 Crop\n48.57+5.16\n−2.30\n38.89+6.82\n−3.47\n12.89+1.32\n−0.90 41.26+3.00\n−2.64\n42.31+2.92\n−3.88\n13.57+1.65\n−2.15 15.98+1.44\n−1.58\n25.28+4.26\n−2.75\n22.18+8.57\n−4.62\n26.56+2.06\n−1.46\n61.13+2.92\n−2.21\n107.48+6.40\n−11.62 42.44+4.20\n−2.30 55.62+5.79\n−4.61\n- w/ 352×352 Crop\n47.26+3.44\n−2.31\n37.75+5.30\n−3.23\n14.38+0.76\n−1.00 39.30+2.59\n−1.40\n34.44+7.70\n−5.20\n13.09+1.62\n−1.17 15.84+1.79\n−1.04\n25.83+1.56\n−2.01\n21.50+3.84\n−1.63\n27.20+1.47\n−1.24\n61.24+3.62\n−5.82\n98.25+4.82\n−3.34\n42.24+2.34\n−1.79 53.38+3.71\n−2.18\n- w/ 464×464 Crop\n46.61+2.57\n−3.25\n37.25+3.17\n−4.48\n15.04+0.79\n−0.45 38.62+1.52\n−1.68 31.60+4.89\n−3.80\n13.13+1.08\n−0.99 15.38+1.77\n−1.93\n27.06+1.37\n−2.76\n22.23+3.99\n−3.40\n29.67+0.67\n−1.10\n63.38+2.10\n−3.73\n87.51+3.83\n−4.99\n44.41+2.76\n−2.76 51.77+2.19\n−3.06\n- w/ 512×512 Crop 55.89+19.35\n−12.54 49.12+26.93\n−16.19 15.94+4.24\n−2.24 39.35+4.47\n−3.91 36.48+6.33\n−12.08 14.68+2.82\n−2.30 16.06+3.25\n−3.12\n26.87+8.35\n−4.31\n19.61+4.76\n−4.21\n27.37+3.00\n−2.34\n62.40+3.93\n−4.27\n98.90+8.87\n−5.05\n40.32+3.12\n−4.21 50.86+5.50\n−7.19\n163\nBibliography\n[1] Mohanad Abukmeil, Stefano Ferrari, Angelo Genovese, Vincenzo Piuri, and\nFabio Scotti. A survey of unsupervised generative models for exploratory\ndata analysis and representation learning. Acm computing surveys (csur),\n54(5):1–40, 2021.\n[2] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning\nalgorithm for boltzmann machines. Cognitive science, 9(1):147–169, 1985.\n[3] Ankur Agarwal and Bill Triggs. Hyperfeatures–multilevel local coding for\nvisual recognition. In Computer Vision–ECCV 2006: 9th European Conference\non Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part I 9, pages\n30–43. Springer, 2006.\n[4] Ibrahim Batuhan Akkaya, Fazil Altinel, and Ugur Halici. Self-training guided\nadversarial domain adaptation for thermal imagery. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n4322–4331, 2021.\n[5] Youssef Alami Mejjati, Christian Richardt, James Tompkin, Darren Cosker, and\nKwang In Kim. Unsupervised attention-guided image-to-image translation.\nAdvances in neural information processing systems, 31, 2018.\n[6] Aziz Alotaibi. Deep generative adversarial networks for image-to-image\ntranslation: A review. Symmetry, 12(10):1705, 2020.\n[7] Srikar Appalaraju, Yi Zhu, Yusheng Xie, and István Fehérvári.\nTowards\ngood practices in self-supervised representation learning. arXiv preprint\narXiv:2012.00868, 2020.\n[8] Yuki M Asano, Christian Rupprecht, and Andrea Vedaldi. A critical analysis\nof self-supervision, or what we can learn from a single image. arXiv preprint\narXiv:1904.13132, 2019.\n[9] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling\nvia simultaneous clustering and representation learning. arXiv preprint\narXiv:1911.05371, 2019.\n165\nBIBLIOGRAPHY\n[10] Mahmoud Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan\nMisra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, and Nicolas Ballas.\nThe hidden uniform cluster prior in self-supervised learning. arXiv preprint\narXiv:2210.07277, 2022.\n[11] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian\nBordes, Pascal Vincent, Armand Joulin, Mike Rabbat, and Nicolas Ballas.\nMasked siamese networks for label-efficient learning. In Computer Vision–\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXXI, pages 456–473. Springer, 2022.\n[12] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal\nVincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised\nlearning from images with a joint-embedding predictive architecture. arXiv\npreprint arXiv:2301.08243, 2023.\n[13] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning represen-\ntations by maximizing mutual information across views. Advances in neural\ninformation processing systems, 32, 2019.\n[14] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-\nsupervised learning with contextualized target representations for vision,\nspeech and language. arXiv preprint arXiv:2212.07525, 2022.\n[15] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and\nMichael Auli. Data2vec: A general framework for self-supervised learning\nin speech, vision and language. In International Conference on Machine\nLearning, pages 1298–1312. PMLR, 2022.\n[16] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-\ncovariance regularization for self-supervised learning.\narXiv preprint\narXiv:2105.04906, 2021.\n[17] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Self-supervised learning\nof local visual features. arXiv preprint arXiv:2210.01571, 2022.\n[18] Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, and Bjorn\nOmmer. Cliquecnn: Deep unsupervised exemplar learning. Advances in\nNeural Information Processing Systems, 29, 2016.\n[19] Karsten Behrendt and Ryan Soussan. Unsupervised labeled lane markers\nusing maps. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision Workshops, pages 0–0, 2019.\n166\nBIBLIOGRAPHY\n[20] Sagie Benaim and Lior Wolf. One-sided unsupervised domain mapping.\nAdvances in neural information processing systems, 30, 2017.\n[21] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning:\nA review and new perspectives. IEEE transactions on pattern analysis and\nmachine intelligence, 35(8):1798–1828, 2013.\n[22] Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and\nZhouhan Lin. Towards biologically plausible deep learning. arXiv preprint\narXiv:1502.04156, 2015.\n[23] Mikołaj Bi´nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton.\nDemystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.\n[24] Florian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visu-\nalization of what your self-supervised representation knows about. arXiv\npreprint arXiv:2112.09164, 2021.\n[25] Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-\nlevel features for recognition. In 2010 IEEE computer society conference on\ncomputer vision and pattern recognition, pages 2559–2566. IEEE, 2010.\n[26] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan,\nand Dilip Krishnan. Unsupervised pixel-level domain adaptation with gener-\native adversarial networks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 3722–3731, 2017.\n[27] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krish-\nnan, and Dumitru Erhan. Domain separation networks. Advances in neural\ninformation processing systems, 29, 2016.\n[28] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak\nShah. Signature verification using a\" siamese\" time delay neural network.\nAdvances in neural information processing systems, 6, 1993.\n[29] Chris\nBurgess\nand\nHyunjik\nKim.\n3d\nshapes\ndataset.\nhttps://github.com/deepmind/3dshapes-dataset/, 2018.\n[30] Vivien Cabannes, Bobak T Kiani, Randall Balestriero, Yann LeCun, and Al-\nberto Bietti. The ssl interplay: Augmentations, inductive bias, and generaliza-\ntion. arXiv preprint arXiv:2302.02774, 2023.\n167\nBIBLIOGRAPHY\n[31] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep\nclustering for unsupervised learning of visual features. In Proceedings of the\nEuropean conference on computer vision (ECCV), pages 132–149, 2018.\n[32] Mathilde Caron, Neil Houlsby, and Cordelia Schmid. Location-aware self-\nsupervised transformers. arXiv preprint arXiv:2212.02400, 2022.\n[33] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski,\nand Armand Joulin. Unsupervised learning of visual features by contrasting\ncluster assignments. Advances in neural information processing systems,\n33:9912–9924, 2020.\n[34] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr\nBojanowski, and Armand Joulin. Emerging properties in self-supervised\nvision transformers. In Proceedings of the IEEE/CVF international conference\non computer vision, pages 9650–9660, 2021.\n[35] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly\ndetection: A survey. arXiv preprint arXiv:1901.03407, 2019.\n[36] Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma.\nPcanet: A simple deep learning baseline for image classification?\nIEEE\ntransactions on image processing, 24(12):5017–5032, 2015.\n[37] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A\nsurvey. ACM computing surveys (CSUR), 41(3):1–58, 2009.\n[38] Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han.\nDomain-specific batch normalization for unsupervised domain adaptation.\nIn Proceedings of the IEEE/CVF conference on Computer Vision and Pattern\nRecognition, pages 7354–7362, 2019.\n[39] David Charte, Francisco Charte, Maria J del Jesus, and Francisco Herrera. An\nanalysis on the use of autoencoders for representation learning: Fundamen-\ntals, learning task case studies, explainability and challenges. Neurocomput-\ning, 404:93–107, 2020.\n[40] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue\nHuang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for\nunsupervised domain adaptation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 627–636, 2019.\n168\nBIBLIOGRAPHY\n[41] Song Chen, Jing-Hao Xue, Jianlong Chang, Jianzhong Zhang, Jufeng Yang, and\nQi Tian. Ssl++: improving self-supervised learning by mitigating the proxy\ntask-specificity problem. IEEE Transactions on Image Processing, 31:1134–\n1148, 2021.\n[42] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\nA simple framework for contrastive learning of visual representations. In\nInternational conference on machine learning, pages 1597–1607. PMLR, 2020.\n[43] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geof-\nfrey E Hinton. Big self-supervised models are strong semi-supervised learners.\nAdvances in neural information processing systems, 33:22243–22255, 2020.\n[44] Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-\nsupervised gans via auxiliary rotation loss. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 12154–12163,\n2019.\n[45] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines\nwith momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\n[46] Xinlei Chen and Kaiming He. Exploring simple siamese representation learn-\ning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 15750–15758, 2021.\n[47] Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and Zeynep Akata. Semi-\nsupervised and unsupervised deep visual learning: A survey. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 2022.\n[48] Yubei Chen, Adrien Bardes, Zengyi Li, and Yann Lecun.\nIntra-instance\nvicreg: Bag of self-supervised image patch embedding.\narXiv preprint\narXiv:2206.08954, 2022.\n[49] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer\nnetworks in unsupervised feature learning. In Proceedings of the fourteenth\ninternational conference on artificial intelligence and statistics, pages 215–223.\nJMLR Workshop and Conference Proceedings, 2011.\n[50] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus\nEnzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.\nThe cityscapes dataset for semantic urban scene understanding. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition, pages\n3213–3223, 2016.\n169\nBIBLIOGRAPHY\n[51] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine\nlearning, 20:273–297, 1995.\n[52] Gabriela Csurka, Riccardo Volpi, and Boris Chidlovskii. Unsupervised domain\nadaptation for semantic image segmentation: a comprehensive survey. arXiv\npreprint arXiv:2112.03241, 2021.\n[53] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsuper-\nvised pre-training for object detection with transformers. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages\n1601–1610, 2021.\n[54] Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava,\nBrian Cheung, Pulkit Agrawal, and Marin Soljaˇci´c. Equivariant contrastive\nlearning. arXiv preprint arXiv:2111.00899, 2021.\n[55] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood\nfrom incomplete data via the em algorithm. Journal of the royal statistical\nsociety: series B (methodological), 39(1):1–22, 1977.\n[56] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Ima-\ngenet: A large-scale hierarchical image database. In 2009 IEEE conference on\ncomputer vision and pattern recognition, pages 248–255. Ieee, 2009.\n[57] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual repre-\nsentation learning by context prediction. In Proceedings of the IEEE interna-\ntional conference on computer vision, pages 1422–1430, 2015.\n[58] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learn-\ning. In Proceedings of the IEEE international conference on computer vision,\npages 2051–2060, 2017.\n[59] Jeff Donahue and Karen Simonyan. Large scale adversarial representation\nlearning. Advances in neural information processing systems, 32, 2019.\n[60] Le Dong, Ling He, Mengdie Mao, Gaipeng Kong, Xi Wu, Qianni Zhang, Xi-\naochun Cao, and Ebroul Izquierdo. Cunet: A compact unsupervised network\nfor image classification. IEEE Transactions on Multimedia, 20(8):2012–2021,\n2017.\n[61] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n170\nBIBLIOGRAPHY\n[62] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen\nKoltun. Carla: An open urban driving simulator. In Conference on robot\nlearning, pages 1–16. PMLR, 2017.\n[63] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas\nBrox. Discriminative unsupervised feature learning with convolutional neural\nnetworks. Advances in neural information processing systems, 27, 2014.\n[64] Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, and Ke Lu. Cross-domain gradient\ndiscrepancy minimization for unsupervised domain adaptation. In Proceed-\nings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 3937–3946, 2021.\n[65] Aysegul Dundar, Jonghoon Jin, and Eugenio Culurciello. Convolutional clus-\ntering for unsupervised learning. arXiv preprint arXiv:1511.06241, 2015.\n[66] Ikram Eddahmani, Chi-Hieu Pham, Thibault Napoléon, Isabelle Badoc, Jean-\nRassaire Fouefack, and Marwa El-Bouz. Unsupervised learning of disentan-\ngled representation via auto-encoding: A survey. Sensors, 23(4):2362, 2023.\n[67] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualiz-\ning higher-layer features of a deep network. University of Montreal, 1341(3):1,\n2009.\n[68] Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-\nsupervised models transfer? In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5414–5423, 2021.\n[69] Linus Ericsson, Henry Gouk, Chen Change Loy, and Timothy M Hospedales.\nSelf-supervised representation learning: Introduction, advances, and chal-\nlenges. IEEE Signal Processing Magazine, 39(3):42–62, 2022.\n[70] Mateus Espadoto, Rafael M Martins, Andreas Kerren, Nina ST Hirata, and\nAlexandru C Telea. Toward a quantitative survey of dimension reduction tech-\nniques. IEEE transactions on visualization and computer graphics, 27(3):2153–\n2173, 2019.\n[71] Hehe Fan, Xiaojun Chang, Wanyue Zhang, Yi Cheng, Ying Sun, and Mohan\nKankanhalli. Self-supervised global-local structure modeling for point cloud\ndomain adaptation with reliable voted pseudo labels. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n6377–6386, 2022.\n171\nBIBLIOGRAPHY\n[72] Peng-Fei Fang, Xian Li, Yang Yan, Shuai Zhang, Qi-Yue Kang, Xiao-Fei Li,\nand Zhen-Zhong Lan. Connecting the dots in self-supervised learning: A\nbrief survey for beginners. Journal of Computer Science and Technology,\n37(3):507–526, 2022.\n[73] Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R Arabnia. A\nbrief review of domain adaptation. Advances in Data Science and Information\nEngineering: Proceedings from ICDATA 2020 and IKE 2020, pages 877–894,\n2021.\n[74] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Kun\nZhang, and Dacheng Tao. Geometry-consistent generative adversarial net-\nworks for one-sided unsupervised domain mapping. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n2427–2436, 2019.\n[75] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by\nbackpropagation. In International conference on machine learning, pages\n1180–1189. PMLR, 2015.\n[76] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, François Laviolette, Mario Marchand, and Victor Lempitsky.\nDomain-adversarial training of neural networks. The journal of machine\nlearning research, 17(1):2096–2030, 2016.\n[77] Noa Garnett, Rafi Cohen, Tomer Pe’er, Roee Lahav, and Dan Levi. 3d-lanenet:\nend-to-end 3d multiple lane detection. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2921–2930, 2019.\n[78] Noa Garnett, Roy Uziel, Netalee Efrat, and Dan Levi. Synthetic-to-real domain\nadaptation for lane detection. In Proceedings of the Asian Conference on\nComputer Vision, 2020.\n[79] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer\nusing convolutional neural networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2414–2423, 2016.\n[80] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and\nWen Li. Deep reconstruction-classification networks for unsupervised do-\nmain adaptation. In Computer Vision–ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14,\npages 597–613. Springer, 2016.\n172\nBIBLIOGRAPHY\n[81] Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. Restricted\nboltzmann machine and deep belief network: Tutorial and survey. arXiv\npreprint arXiv:2107.12521, 2021.\n[82] Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstantinos Bousmalis,\nand Vladimir Pavlovic. Unsupervised multi-target domain adaptation: An\ninformation theoretic approach. IEEE Transactions on Image Processing,\n29:3993–4002, 2020.\n[83] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pérez, and Matthieu\nCord. Learning representations by predicting bags of visual words. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 6928–6938, 2020.\n[84] Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord,\nand Patrick Pérez. Obow: Online bag-of-visual-words generation for self-\nsupervised learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6830–6840, 2021.\n[85] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised represen-\ntation learning by predicting image rotations. arXiv preprint arXiv:1803.07728,\n2018.\n[86] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training\ndeep feedforward neural networks. In Proceedings of the thirteenth interna-\ntional conference on artificial intelligence and statistics, pages 249–256. JMLR\nWorkshop and Conference Proceedings, 2010.\n[87] Clément Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised\nmonocular depth estimation with left-right consistency. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 270–279,\n2017.\n[88] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT\npress, 2016.\n[89] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adver-\nsarial nets. In Advances in neural information processing systems, 2014.\n[90] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and\nbenchmarking self-supervised visual representation learning. In Proceedings\nof the ieee/cvf International Conference on computer vision, pages 6391–6400,\n2019.\n173\nBIBLIOGRAPHY\n[91] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and\nAlex Smola. A kernel method for the two-sample-problem. Advances in\nneural information processing systems, 19, 2006.\n[92] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf,\nand Alexander Smola. A kernel two-sample test. The Journal of Machine\nLearning Research, 13(1):723–773, 2012.\n[93] Tom George Grigg, Dan Busbridge, Jason Ramapuram, and Russ Webb. Do\nself-supervised and supervised methods learn similar visual representations?\narXiv preprint arXiv:2110.00528, 2021.\n[94] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-\nhan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a\nnew approach to self-supervised learning. Advances in neural information\nprocessing systems, 33:21271–21284, 2020.\n[95] Leopold Grinberg, John Hopfield, and Dmitry Krotov. Local unsupervised\nlearning for image analysis. arXiv preprint arXiv:1908.08993, 2019.\n[96] Stephen Grossberg. Competitive learning: From interactive activation to\nadaptive resonance. Cognitive science, 11(1):23–63, 1987.\n[97] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron C Courville. Improved training of wasserstein gans. Advances in neural\ninformation processing systems, 30, 2017.\n[98] Matthew Gwilliam and Abhinav Shrivastava. Beyond supervised vs. unsuper-\nvised: Representative benchmarking and analysis of image representation\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9642–9652, 2022.\n[99] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by\nlearning an invariant mapping. In 2006 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages\n1735–1742. IEEE, 2006.\n[100] Richard Hankins, Yao Peng, and Hujun Yin. Somnet: unsupervised feature\nlearning networks for image classification. In 2018 International Joint Confer-\nence on Neural Networks (IJCNN), pages 1–8. IEEE, 2018.\n174\nBIBLIOGRAPHY\n[101] Richard Hankins, Yao Peng, and Hujun Yin. Towards complex features: Com-\npetitive receptive fields in unsupervised deep networks. In Intelligent Data\nEngineering and Automated Learning–IDEAL 2018: 19th International Con-\nference, Madrid, Spain, November 21–23, 2018, Proceedings, Part I 19, pages\n838–848. Springer, 2018.\n[102] Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu. Gancraft: Un-\nsupervised 3d neural rendering of minecraft worlds. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 14072–14082,\n2021.\n[103] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross\nGirshick. Masked autoencoders are scalable vision learners. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n16000–16009, 2022.\n[104] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum\ncontrast for unsupervised visual representation learning. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages\n9729–9738, 2020.\n[105] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–778, 2016.\n[106] Donald Olding Hebb. The organization of behavior: A neuropsychological\ntheory. Psychology Press, 2005.\n[107] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Us-\ning self-supervised learning can improve model robustness and uncertainty.\nAdvances in neural information processing systems, 32, 2019.\n[108] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,\nand Sepp Hochreiter. Gans trained by a two time-scale update rule con-\nverge to a local nash equilibrium. Advances in neural information processing\nsystems, 30, 2017.\n[109] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a\nneural network. arXiv preprint arXiv:1503.02531, 2015.\n[110] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning\nalgorithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006.\n175\nBIBLIOGRAPHY\n[111] Geoffrey E Hinton and Terrence J Sejnowski. Optimal perceptual inference.\nIn Proceedings of the IEEE conference on Computer Vision and Pattern Recog-\nnition, volume 448, pages 448–453. Citeseer, 1983.\n[112] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil\nBachman, Adam Trischler, and Yoshua Bengio. Learning deep representa-\ntions by mutual information estimation and maximization. arXiv preprint\narXiv:1808.06670, 2018.\n[113] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate\nSaenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial\ndomain adaptation. In International conference on machine learning, pages\n1989–1998. Pmlr, 2018.\n[114] Keith J Holyoak. Parallel distributed processing: explorations in the mi-\ncrostructure of cognition. Science, 236:992–997, 1987.\n[115] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-\nlearning. arXiv preprint arXiv:1810.02334, 2018.\n[116] Chuqing Hu, Sinclair Hudson, Martin Ethier, Mohammad Al-Sharman, Derek\nRayside, and William Melek. Sim-to-real domain adaptation for lane de-\ntection and classification in autonomous driving. In 2022 IEEE Intelligent\nVehicles Symposium (IV), pages 457–463. IEEE, 2022.\n[117] Xueqi Hu, Xinyue Zhou, Qiusheng Huang, Zhengyi Shi, Li Sun, and Qingli Li.\nQs-attn: Query-selected attention for contrastive learning in i2i translation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18291–18300, 2022.\n[118] Jiabo Huang, Qi Dong, Shaogang Gong, and Xiatian Zhu. Unsupervised\ndeep learning by neighbourhood discovery. In International Conference on\nMachine Learning, pages 2849–2858. PMLR, 2019.\n[119] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with\nadaptive instance normalization. In Proceedings of the IEEE international\nconference on computer vision, pages 1501–1510, 2017.\n[120] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsuper-\nvised image-to-image translation. In Proceedings of the European conference\non computer vision (ECCV), pages 172–189, 2018.\n[121] David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in\nthe cat’s striate cortex. The Journal of physiology, 148(3):574, 1959.\n176\nBIBLIOGRAPHY\n[122] Badr Youbi Idrissi, Diane Bouchacourt, Randall Balestriero, Ivan Evtimov,\nCaner Hazirbas, Nicolas Ballas, Pascal Vincent, Michal Drozdzal, David Lopez-\nPaz, and Mark Ibrahim. Imagenet-x: Understanding model mistakes with\nfactor of variation annotations. arXiv preprint arXiv:2211.01866, 2022.\n[123] Bernd Illing, Jean Ventura, Guillaume Bellec, and Wulfram Gerstner. Local\nplasticity rules can learn deep representations using self-supervised con-\ntrastive predictions. Advances in Neural Information Processing Systems,\n34:30365–30379, 2021.\n[124] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In International confer-\nence on machine learning, pages 448–456. pmlr, 2015.\n[125] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image\ntranslation with conditional adversarial networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1125–1134,\n2017.\n[126] Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun.\nWhat is the best multi-stage architecture for object recognition? In 2009 IEEE\n12th international conference on computer vision, pages 2146–2153. IEEE,\n2009.\n[127] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-\nguided unsupervised image-to-image translation.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 6558–\n6567, 2021.\n[128] Zhiwei Jia, Bodi Yuan, Kangkang Wang, Hong Wu, David Clifford, Zhiqiang\nYuan, and Hao Su. Semantically robust unpaired image translation for data\nwith unmatched semantics statistics. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 14273–14283, 2021.\n[129] Junguang Jiang, Ximei Wang, Mingsheng Long, and Jianmin Wang. Resource\nefficient domain adaptation. In Proceedings of the 28th ACM International\nConference on Multimedia, pages 2220–2228, 2020.\n[130] Liming Jiang, Changxu Zhang, Mingyang Huang, Chunxiao Liu, Jianping Shi,\nand Chen Change Loy. Tsit: A simple and versatile framework for image-to-\nimage translation. In Computer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pages 206–222.\nSpringer, 2020.\n177\nBIBLIOGRAPHY\n[131] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with\ndeep neural networks: A survey. IEEE transactions on pattern analysis and\nmachine intelligence, 43(11):4037–4058, 2020.\n[132] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search\nwith gpus. IEEE Transactions on Big Data, 7(3):535–547, 2019.\n[133] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-\ntime style transfer and super-resolution. In Computer Vision–ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part II 14, pages 694–711. Springer, 2016.\n[134] Adrien Journé, Hector Garcia Rodriguez, Qinghai Guo, and Timoleon Moraitis.\nHebbian deep learning without feedback. arXiv preprint arXiv:2209.11883,\n2022.\n[135] Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced\nfeature spaces for representation learning. In International Conference on\nLearning Representations, 2021.\n[136] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive\nadaptation network for unsupervised domain adaptation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages\n4893–4902, 2019.\n[137] Chieh-Chi Kao, Yuxiang Wang, Jonathan Waltman, and Pradeep Sen. Patch-\nbased image hallucination for super resolution with detail reconstruction\nfrom similar sample images. IEEE Transactions on Multimedia, 22(5):1139–\n1152, 2019.\n[138] Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Learning\nimage representations by completing damaged jigsaw puzzles. In 2018 IEEE\nWinter Conference on Applications of Computer Vision (WACV), pages 793–\n802. IEEE, 2018.\n[139] Donghyun Kim, Kuniaki Saito, Tae-Hyun Oh, Bryan A Plummer, Stan Sclaroff,\nand Kate Saenko. Cross-domain self-supervised learning for domain adapta-\ntion with few source labels. arXiv preprint arXiv:2003.08264, 2020.\n[140] Junho Kim, Minjae Kim, Hyeonwoo Kang, and Kwanghee Lee. U-gat-it: Un-\nsupervised generative attentional networks with adaptive layer-instance nor-\nmalization for image-to-image translation. arXiv preprint arXiv:1907.10830,\n2019.\n178\nBIBLIOGRAPHY\n[141] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-\ntion. arXiv preprint arXiv:1412.6980, 2014.\n[142] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114, 2013.\n[143] Teuvo Kohonen. Self-organized formation of topologically correct feature\nmaps. Biological cybernetics, 43(1):59–69, 1982.\n[144] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer.\nRevisiting self-\nsupervised visual representation learning. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 1920–1929,\n2019.\n[145] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features\nfrom tiny images. 2009.\n[146] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classifica-\ntion with deep convolutional neural networks. Communications of the ACM,\n60(6):84–90, 2017.\n[147] Dmitry Krotov and John J. Hopfield. Unsupervised learning by competing\nhidden units. Proceedings of the National Academy of Sciences, 116(16):7723–\n7731, 2019.\n[148] Pranjal Kumar, Piyush Rawat, and Siddhartha Chauhan. Contrastive self-\nsupervised learning: review, progress, challenges and future research direc-\ntions. International Journal of Multimedia Information Retrieval, 11(4):461–\n488, 2022.\n[149] Gihyun Kwon and Jong Chul Ye. Diagonal attention and style-based gan\nfor content-style disentanglement in image generation and translation. In\nProceedings of the IEEE/CVF International Conference on Computer Vision,\npages 13980–13989, 2021.\n[150] John Lambert, Zhuang Liu, Ozan Sener, James Hays, and Vladlen Koltun.\nMseg: A composite dataset for multi-domain semantic segmentation. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recog-\nnition, pages 2879–2888, 2020.\n[151] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning rep-\nresentations for automatic colorization. In Computer Vision–ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016,\nProceedings, Part IV 14, pages 577–593. Springer, 2016.\n179\nBIBLIOGRAPHY\n[152] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as\na proxy task for visual understanding. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 6874–6883, 2017.\n[153] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features:\nSpatial pyramid matching for recognizing natural scene categories. In 2006\nIEEE computer society conference on computer vision and pattern recognition\n(CVPR’06), volume 2, pages 2169–2178. IEEE, 2006.\n[154] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N,\n7(7):3, 2015.\n[155] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E\nHoward, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied\nto handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.\n[156] Chuan Li and Michael Wand. Combining markov random fields and con-\nvolutional neural networks for image synthesis. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 2479–2486,\n2016.\n[157] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi.\nPrototypi-\ncal contrastive learning of unsupervised representations. arXiv preprint\narXiv:2005.04966, 2020.\n[158] Shuo Li, Fang Liu, Licheng Jiao, Puhua Chen, and Lingling Li. Self-supervised\nself-organizing clustering network: a novel unsupervised representation\nlearning method. IEEE Transactions on Neural Networks and Learning Sys-\ntems, 2022.\n[159] Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural\nstyle transfer. arXiv preprint arXiv:1701.01036, 2017.\n[160] Jie Liang, Hui Zeng, and Lei Zhang. High-resolution photorealistic image\ntranslation in real-time: A laplacian pyramid translation network. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 9392–9400, 2021.\n[161] Xiaodan Liang, Hao Zhang, Liang Lin, and Eric Xing. Generative semantic\nmanipulation with mask-contrasting gan. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 558–573, 2018.\n180\nBIBLIOGRAPHY\n[162] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman.\nRandom synaptic feedback weights support error backpropagation for deep\nlearning. Nature communications, 7(1):13276, 2016.\n[163] Jae Hyun Lim and Jong Chul Ye.\nGeometric gan.\narXiv preprint\narXiv:1705.02894, 2017.\n[164] Che-Tsung Lin, Yen-Yi Wu, Po-Hao Hsu, and Shang-Hong Lai. Multimodal\nstructure-consistent image-to-image translation. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 34, pages 11490–11498, 2020.\n[165] Feng Lin, Haohang Xu, Houqiang Li, Hongkai Xiong, and Guo-Jun Qi. Auto-\nencoding transformations in reparameterized lie groups for unsupervised\nlearning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 8610–8617, 2021.\n[166] Tsung-Han Lin and HT Kung. Stable and efficient representation learning\nwith nonnegativity constraints. In International Conference on Machine\nLearning, pages 1323–1331. PMLR, 2014.\n[167] Yu Lin, Yigong Wang, Yifan Li, Yang Gao, Zhuoyi Wang, and Latifur Khan.\nAttention-based spatial guidance for image-to-image translation. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of Computer Vision,\npages 816–825, 2021.\n[168] Seppo Linnainmaa. The representation of the cumulative rounding error of\nan algorithm as a Taylor expansion of the local rounding errors. PhD thesis,\nMaster’s Thesis (in Finnish), Univ. Helsinki, 1970.\n[169] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso\nSetio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak,\nBram Van Ginneken, and Clara I Sánchez. A survey on deep learning in\nmedical image analysis. Medical image analysis, 42:60–88, 2017.\n[170] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image\ntranslation networks. Advances in neural information processing systems, 30,\n2017.\n[171] Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid\nwarping gan with attention: A unified framework for human image synthesis.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5114–\n5132, 2021.\n181\nBIBLIOGRAPHY\n[172] Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, Je-\nWon Kang, Jonghye Woo, et al. Deep unsupervised domain adaptation: A\nreview of recent advances and perspectives. APSIPA Transactions on Signal\nand Information Processing, 11(1), 2022.\n[173] Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al. Learning to pre-\ndict layout-to-image conditional convolutions for semantic image synthesis.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[174] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain\nGelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common as-\nsumptions in the unsupervised learning of disentangled representations. In\ninternational conference on machine learning, pages 4114–4124. PMLR, 2019.\n[175] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly,\nBernhard Schölkopf, and Olivier Bachem. A sober look at the unsupervised\nlearning of disentangled representations and their evaluation. The Journal of\nMachine Learning Research, 21(1):8629–8690, 2020.\n[176] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning\ntransferable features with deep adaptation networks. In International confer-\nence on machine learning, pages 97–105. PMLR, 2015.\n[177] Ana C Lorena, Luís PF Garcia, Jens Lehmann, Marcilio CP Souto, and Tin Kam\nHo. How complex is your classification problem? a survey on measuring\nclassification complexity. ACM Computing Surveys (CSUR), 52(5):1–34, 2019.\n[178] Sindy Löwe, Peter O’Connor, and Bastiaan Veeling. Putting an end to end-\nto-end: Gradient-isolated learning of representations. Advances in neural\ninformation processing systems, 32, 2019.\n[179] Liqian Ma, Xu Jia, Stamatios Georgoulis, Tinne Tuytelaars, and Luc Van Gool.\nExemplar guided unsupervised image-to-image translation with semantic\nconsistency. arXiv preprint arXiv:1805.11145, 2018.\n[180] J MacQueen. Classification and analysis of multivariate observations. In\n5th Berkeley Symp. Math. Statist. Probability, pages 281–297. University of\nCalifornia Los Angeles LA USA, 1967.\n[181] Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bulo, Barbara Caputo, and\nElisa Ricci. Boosting domain adaptation by discovering latent domains. In\nProceedings of the IEEE conference on computer vision and pattern recognition,\npages 3771–3780, 2018.\n182\nBIBLIOGRAPHY\n[182] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture\nmodels. Annual review of statistics and its application, 6:355–378, 2019.\n[183] Zhong Meng, Jinyu Li, Yifan Gong, and Biing-Hwang Juang. Adversarial\nteacher-student learning for unsupervised domain adaptation. In 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP),\npages 5949–5953. IEEE, 2018.\n[184] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training\nmethods for gans do actually converge?\nIn International conference on\nmachine learning, pages 3481–3490. PMLR, 2018.\n[185] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein.\nMeta-learning update rules for unsupervised representation learning. arXiv\npreprint arXiv:1804.00222, 2018.\n[186] Thomas Miconi. Hebbian learning with gradients: Hebbian convolutional\nneural networks with modern deep learning frameworks. arXiv preprint\narXiv:2107.01729, 2021.\n[187] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu\nNguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, and Dan Roth. Recent ad-\nvances in natural language processing via large pre-trained language models:\nA survey. arXiv preprint arXiv:2111.01243, 2021.\n[188] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets.\narXiv preprint arXiv:1411.1784, 2014.\n[189] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-\ninvariant representations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2020.\n[190] Seungwhan Moon and Jaime G Carbonell. Completely heterogeneous transfer\nlearning with attention-what and what not to transfer. In IJCAI, volume 1,\npages 1–2, 2017.\n[191] Timoleon Moraitis, Dmitry Toichkin, Adrien Journé, Yansong Chua, and Qing-\nhai Guo. Softhebb: Bayesian inference in unsupervised hebbian soft winner-\ntake-all networks. Neuromorphic Computing and Engineering, 2(4):044017,\n2022.\n[192] Farzeen Munir, Shoaib Azam, and Moongu Jeon. Sstn: Self-supervised do-\nmain adaptation thermal object detection for autonomous driving. In 2021\n183\nBIBLIOGRAPHY\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),\npages 206–213. IEEE, 2021.\n[193] Cooper Nederhood, Nicholas Kolkin, Deqing Fu, and Jason Salavon. Har-\nnessing the conditioning sensorium for improved image translation. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision, pages\n6752–6761, 2021.\n[194] Alejandro Newell and Jia Deng. How useful is self-supervised pretraining for\nvisual tasks? In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 7345–7354, 2020.\n[195] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual repre-\nsentations by solving jigsaw puzzles. In Computer Vision–ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part VI, pages 69–84. Springer, 2016.\n[196] Erkki Oja. Simplified neuron model as a principal component analyzer. Jour-\nnal of mathematical biology, 15:267–273, 1982.\n[197] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[198] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc\nSzafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa,\nAlaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without\nsupervision. arXiv preprint arXiv:2304.07193, 2023.\n[199] Julio-Omar Palacio-Niño and Fernando Berzal. Evaluation metrics for unsu-\npervised learning algorithms. arXiv preprint arXiv:1905.05667, 2019.\n[200] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transac-\ntions on knowledge and data engineering, 22(10):1345–1359, 2010.\n[201] Xingang Pan, Jianping Shi, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Spatial\nas deep: Spatial cnn for traffic scene understanding. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 32, 2018.\n[202] Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, and Tao Mei.\nTransferrable prototypical networks for unsupervised domain adaptation.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 2239–2247, 2019.\n184\nBIBLIOGRAPHY\n[203] Yingxue Pang, Jianxin Lin, Tao Qin, and Zhibo Chen. Image-to-image transla-\ntion: Methods and applications. IEEE Transactions on Multimedia, 24:3859–\n3881, 2021.\n[204] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive\nlearning for unpaired image-to-image translation. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part IX 16, pages 319–345. Springer, 2020.\n[205] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic\nimage synthesis with spatially-adaptive normalization. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages\n2337–2346, 2019.\n[206] Massimiliano Patacchiola and Amos J Storkey. Self-supervised relational\nreasoning for representation learning. Advances in Neural Information Pro-\ncessing Systems, 33:4003–4014, 2020.\n[207] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and\nAlexei A Efros. Context encoders: Feature learning by inpainting. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition, pages\n2536–2544, 2016.\n[208] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in\nspace. The London, Edinburgh, and Dublin philosophical magazine and\njournal of science, 2(11):559–572, 1901.\n[209] Fabio Pizzati, Pietro Cerri, and Raoul de Charette. Comogan: continuous\nmodel-guided image-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 14288–14298,\n2021.\n[210] Jose C Principe, Neil R Euliano, and W Curt Lefebvre. Neural and adaptive\nsystems: fundamentals through simulations with CD-ROM. John Wiley &\nSons, Inc., 1999.\n[211] Guo-Jun Qi, Liheng Zhang, Chang Wen Chen, and Qi Tian. Avt: Unsuper-\nvised learning of transformation equivariant representations by autoencoding\nvariational transformations. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 8130–8139, 2019.\n[212] Zequn Qin, Huanyu Wang, and Xi Li. Ultra fast structure-aware deep lane de-\ntection. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\n185\nBIBLIOGRAPHY\nUK, August 23–28, 2020, Proceedings, Part XXIV 16, pages 276–291. Springer,\n2020.\n[213] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation\nlearning with deep convolutional generative adversarial networks. arXiv\npreprint arXiv:1511.06434, 2015.\n[214] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color\ntransfer between images. IEEE Computer graphics and applications, 21(5):34–\n41, 2001.\n[215] Pierre H Richemond, Jean-Bastien Grill, Florent Altché, Corentin Tallec, Flo-\nrian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bi-\nlal Piot, et al.\nByol works even without batch statistics.\narXiv preprint\narXiv:2010.10241, 2020.\n[216] Stephan R Richter, Hassan Abu AlHaija, and Vladlen Koltun. Enhancing pho-\ntorealism enhancement. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(2):1700–1715, 2022.\n[217] Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for bench-\nmarks. In Proceedings of the IEEE International Conference on Computer\nVision, pages 2213–2222, 2017.\n[218] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing\nfor data: Ground truth from computer games. In Computer Vision–ECCV\n2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 102–118. Springer, 2016.\n[219] Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe, and Elisa\nRicci. Trigan: Image-to-image translation for multi-source domain adapta-\ntion. Machine vision and applications, 32:1–12, 2021.\n[220] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning\ninternal representations by error propagation. Technical report, California\nUniv San Diego La Jolla Inst for Cognitive Science, 1985.\n[221] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning\ninternal representations by error propagation. Technical report, California\nUniv San Diego La Jolla Inst for Cognitive Science, 1985.\n[222] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning\nrepresentations by back-propagating errors. nature, 323(6088):533–536, 1986.\n186\nBIBLIOGRAPHY\n[223] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual\ncategory models to new domains. In Computer Vision–ECCV 2010: 11th\nEuropean Conference on Computer Vision, Heraklion, Crete, Greece, September\n5-11, 2010, Proceedings, Part IV 11, pages 213–226. Springer, 2010.\n[224] Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training\nfor unsupervised domain adaptation. In International Conference on Machine\nLearning, pages 2988–2997. PMLR, 2017.\n[225] Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines.\nIn David van Dyk and Max Welling, editors, Proceedings of the Twelth In-\nternational Conference on Artificial Intelligence and Statistics, volume 5 of\nProceedings of Machine Learning Research, pages 448–455, Hilton Clearwater\nBeach Resort, Clearwater Beach, Florida USA, 16–18 Apr 2009. PMLR.\n[226] Terence D Sanger. Optimal unsupervised learning in a single-layer linear\nfeedforward neural network. Neural networks, 2(6):459–473, 1989.\n[227] Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus, and Karteek Alahari.\nConcept generalization in visual representation learning. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pages 9629–9639,\n2021.\n[228] Daniel J Saunders, Devdhar Patel, Hananel Hazan, Hava T Siegelmann, and\nRobert Kozma. Locally connected spiking neural networks for unsupervised\nfeature learning. Neural Networks, 119:332–340, 2019.\n[229] Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin\nSuresh, and Andrew Y Ng. On random weights and unsupervised feature\nlearning. In Icml, volume 2, page 6, 2011.\n[230] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. on\nlearning now to learn: The meta-meta-meta...-hook. Diploma thesi, Technis-\nche Universitat Munchen, Germany, 1987.\n[231] Jürgen Schmidhuber. Making the world differentiable: on using self super-\nvised fully recurrent neural networks for dynamic reinforcement learning and\nplanning in non-stationary environments, volume 126. Inst. für Informatik,\n1990.\n[232] Jürgen Schmidhuber. On learning how to learn learning strategies. Technical\nreport, 1995.\n187\nBIBLIOGRAPHY\n[233] Omry Sendik, Danny Cohen-Or, and Dani Lischinski. Crossnet: Latent cross-\nconsistency for unpaired image translation. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pages 3043–3051,\n2020.\n[234] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn,\nand Pablo Villalobos. Compute trends across three eras of machine learning.\nIn 2022 International Joint Conference on Neural Networks (IJCNN), pages\n1–8. IEEE, 2022.\n[235] Tamar Rott Shaham, Michaël Gharbi, Richard Zhang, Eli Shechtman, and\nTomer Michaeli. Spatially-adaptive pixelwise networks for fast image trans-\nlation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14882–14891, 2021.\n[236] Hidetoshi Shimodaira. Improving predictive inference under covariate shift\nby weighting the log-likelihood function. Journal of statistical planning and\ninference, 90(2):227–244, 2000.\n[237] Kihyuk Sohn, Wenling Shang, Xiang Yu, and Manmohan Chandraker. Unsu-\npervised domain adaptation for distance metric learning. In International\nConference on Learning Representations, 2019.\n[238] Bonifaz Stuhr and Jürgen Brauer. Csnns: Unsupervised, backpropagation-free\nconvolutional neural networks for representation learning. In 2019 18th IEEE\nInternational Conference On Machine Learning And Applications (ICMLA),\npages 1613–1620. IEEE, 2019.\n[239] Bonifaz Stuhr and Jürgen Brauer. Don’t miss the mismatch: investigating\nthe objective function mismatch for unsupervised representation learning.\nNeural Computing and Applications, 34(13):11109–11121, 2022.\n[240] Bonifaz Stuhr, Jürgen Brauer, Bernhard Schick, and Jordi Gonzalez. Masked\ndiscriminators for content-consistent unpaired image-to-image translation.\narXiv preprint arXiv:2309.13188, 2023.\n[241] Bonifaz Stuhr, Johann Haselberger, and Julian Gebele. Carlane: A lane de-\ntection benchmark for unsupervised domain adaptation from simulation to\nmultiple real-world domains. In S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing\nSystems, volume 35, pages 4046–4058. Curran Associates, Inc., 2022.\n188\nBIBLIOGRAPHY\n[242] Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan. When does self-\nsupervision improve few-shot learning? In Computer Vision–ECCV 2020:\n16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart VII 16, pages 645–666. Springer, 2020.\n[243] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion\nimplicit bridges for image-to-image translation. In International Conference\non Learning Representations, 2022.\n[244] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy\ndomain adaptation. In Proceedings of the AAAI conference on artificial intelli-\ngence, volume 30, 2016.\n[245] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain\nadaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019.\n[246] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain\nimage generation. arXiv preprint arXiv:1611.02200, 2016.\n[247] Hao Tang, Song Bai, and Nicu Sebe. Dual attention gans for semantic im-\nage synthesis. In Proceedings of the 28th ACM International Conference on\nMultimedia, pages 1994–2002, 2020.\n[248] Hao Tang, Hong Liu, Dan Xu, Philip HS Torr, and Nicu Sebe. Attentiongan:\nUnpaired image-to-image translation using attention-guided generative ad-\nversarial networks. IEEE transactions on neural networks and learning systems,\n2021.\n[249] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J Corso, and Yan Yan.\nMulti-channel attention selection gan with cascaded semantic guidance for\ncross-view image translation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2417–2426, 2019.\n[250] Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang,\nBo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsuper-\nvised domain adaptation. Advances in Neural Information Processing Systems,\n34:17194–17208, 2021.\n[251] Justin Theiss, Jay Leverett, Daeil Kim, and Aayush Prakash. Unpaired image\ntranslation via vector symbolic architectures. In Computer Vision–ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,\nPart XXI, pages 17–32. Springer, 2022.\n189\nBIBLIOGRAPHY\n[252] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview cod-\ning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23–28, 2020, Proceedings, Part XI 16, pages 776–794. Springer, 2020.\n[253] Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understand-\ning self-supervised learning with dual deep networks.\narXiv preprint\narXiv:2010.00578, 2020.\n[254] Dmitrii Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shinjae Yoo, Meifeng Lin,\nBrett Viren, and Yihui Ren. Uvcgan: Unet vision transformer cycle-consistent\ngan for unpaired image-to-image translation. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pages 702–712, 2023.\n[255] TuSimple.\nTuSimple-benchmark.\nhttps://github.com/TuSimple/\ntusimple-benchmark/tree/master/doc/lane_detection, 2017.\nAccessed:\n2021-11-16.\n[256] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial\ndiscriminative domain adaptation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7167–7176, 2017.\n[257] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In\nProceedings of the IEEE conference on computer vision and pattern recognition,\npages 9446–9454, 2018.\n[258] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne.\nJournal of machine learning research, 9(11), 2008.\n[259] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max\nWelling. Rotation equivariant cnns for digital pathology. In Medical Image\nComputing and Computer Assisted Intervention–MICCAI 2018: 21st Interna-\ntional Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part\nII 11, pages 210–218. Springer, 2018.\n[260] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Man-\nzagol. Extracting and composing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international conference on Machine learn-\ning, pages 1096–1103, 2008.\n[261] Bram Wallace and Bharath Hariharan.\nExtending and analyzing self-\nsupervised learning across domains. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part\nXXVI 16, pages 717–734. Springer, 2020.\n190\nBIBLIOGRAPHY\n[262] Haoran Wang, Tong Shen, Wei Zhang, Ling-Yu Duan, and Tao Mei. Classes\nmatter: A fine-grained adversarial approach to cross-domain semantic seg-\nmentation. In Computer Vision–ECCV 2020: 16th European Conference, Glas-\ngow, UK, August 23–28, 2020, Proceedings, Part XIV, pages 642–659. Springer,\n2020.\n[263] Zirui Wang, Zihang Dai, Barnabás Póczos, and Jaime Carbonell. Characteriz-\ning and avoiding negative transfer. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 11293–11302, 2019.\n[264] Guoqiang Wei, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Metaalign: Co-\nordinating domain alignment and classification for unsupervised domain\nadaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16643–16653, 2021.\n[265] Guoqiang Wei, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, and Zhibo Chen.\nToalign: task-oriented alignment for unsupervised domain adaptation. Ad-\nvances in Neural Information Processing Systems, 34:13834–13846, 2021.\n[266] Paul Werbos. Beyond regression: New tools for prediction and analysis in the\nbehavioral sciences. PhD thesis, Committee on Applied Mathematics, Harvard\nUniversity, Cambridge, MA, 1974.\n[267] James CR Whittington and Rafal Bogacz. Theories of error back-propagation\nin the brain. Trends in cognitive sciences, 23(3):235–250, 2019.\n[268] Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain\nadaptation. ACM Transactions on Intelligent Systems and Technology (TIST),\n11(5):1–46, 2020.\n[269] Steffen Wolf, Fred A Hamprecht, and Jan Funke. Instance separation emerges\nfrom inpainting. arXiv preprint arXiv:2003.00891, 2020.\n[270] Chen Henry Wu and Fernando De la Torre. Unifying diffusion models’ la-\ntent space, with applications to cyclediffusion and guidance. arXiv preprint\narXiv:2210.05559, 2022.\n[271] Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Good-\nman. On mutual information in contrastive learning for visual representa-\ntions. arXiv preprint arXiv:2005.13149, 2020.\n[272] Xinyi Wu, Zhenyao Wu, Hao Guo, Lili Ju, and Song Wang. Dannet: A one-stage\ndomain adaptation network for unsupervised nighttime semantic segmen-\ntation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 15769–15778, 2021.\n191\nBIBLIOGRAPHY\n[273] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature\nlearning via non-parametric instance discrimination. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 3733–3742,\n2018.\n[274] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning seman-\ntic representations for unsupervised domain adaptation. In International\nconference on machine learning, pages 5423–5432. PMLR, 2018.\n[275] Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma, and Yefeng Zheng.\nSelf-supervised cyclegan for object-preserving image-to-image domain adap-\ntation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XX 16, pages 498–513. Springer,\n2020.\n[276] Dongkuan Xu and Yingjie Tian. A comprehensive survey of clustering algo-\nrithms. Annals of Data Science, 2:165–193, 2015.\n[277] Jiaolong Xu, Liang Xiao, and Antonio M López. Self-supervised domain\nadaptation for computer vision tasks. IEEE Access, 7:156694–156706, 2019.\n[278] Qiangeng Xu, Yin Zhou, Weiyue Wang, Charles R Qi, and Dragomir Anguelov.\nSpg: Unsupervised domain adaptation for 3d object detection via semantic\npoint generation. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 15446–15456, 2021.\n[279] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv\nMahajan. Clusterfit: Improving generalization of visual representations. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6509–6518, 2020.\n[280] Chao Yang, Taehwan Kim, Ruizhe Wang, Hao Peng, and C-C Jay Kuo. Show, at-\ntend, and translate: Unsupervised image translation with self-regularization\nand attention. IEEE Transactions on Image Processing, 28(10):4845–4856,\n2019.\n[281] Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of\ndeep representations and image clusters. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 5147–5156, 2016.\n[282] Yanchao Yang, Dong Lao, Ganesh Sundaramoorthi, and Stefano Soatto. Phase\nconsistent ecological domain adaptation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 9011–9020,\n2020.\n192\nBIBLIOGRAPHY\n[283] Yuan Yao, Jianqiang Ren, Xuansong Xie, Weidong Liu, Yong-Jin Liu, and Jun\nWang. Attention-aware multi-stroke style transfer. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 1467–\n1475, 2019.\n[284] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embed-\nding learning via invariant and spreading instance feature. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n6210–6219, 2019.\n[285] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen\nLiu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving\ndataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 2636–2645,\n2020.\n[286] Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao, Trevor Darrell,\nKurt Keutzer, and Alberto Sangiovanni Vincentelli. Prototypical cross-domain\nself-supervised learning for few-shot unsupervised domain adaptation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13834–13844, 2021.\n[287] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow\ntwins: Self-supervised learning via redundancy reduction. In International\nConference on Machine Learning, pages 12310–12320. PMLR, 2021.\n[288] Shuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana\nLikhomanenko, Joseph Yitan Cheng, Walter Talbott, Chen Huang, Hanlin\nGoh, and Joshua Susskind. Position prediction as an effective pretraining\nstrategy. arXiv preprint arXiv:2207.07611, 2022.\n[289] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos\nRiquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neu-\nmann, Alexey Dosovitskiy, et al. A large-scale study of representation learning\nwith the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867,\n2019.\n[290] Xiaohang Zhan, Jiahao Xie, Ziwei Liu, Yew-Soon Ong, and Chen Change Loy.\nOnline deep clustering for unsupervised representation learning. In Proceed-\nings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 6688–6697, 2020.\n193\nBIBLIOGRAPHY\n[291] Liheng Zhang, Guo-Jun Qi, Liqiang Wang, and Jiebo Luo. Aet vs. aed: Unsu-\npervised representation learning by auto-encoding transformations rather\nthan data. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2547–2555, 2019.\n[292] Linfeng Zhang, Xin Chen, Runpei Dong, and Kaisheng Ma. Region-aware\nknowledge distillation for efficient image-to-image translation. arXiv preprint\narXiv:2205.12451, 2022.\n[293] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization.\nIn Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part III 14, pages 649–666.\nSpringer, 2016.\n[294] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.\nThe unreasonable effectiveness of deep features as a perceptual metric. In\nProceedings of the IEEE conference on computer vision and pattern recognition,\npages 586–595, 2018.\n[295] Rui Zhang, Tomas Pfister, and Jia Li. Harmonic unpaired image-to-image\ntranslation. arXiv preprint arXiv:1902.09727, 2019.\n[296] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and\nadversarial network for unsupervised domain adaptation. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages\n3801–3809, 2018.\n[297] Yabin Zhang, Bin Deng, Kui Jia, and Lei Zhang. Label propagation with\naugmented anchors: A simple semi-supervised learning baseline for unsu-\npervised domain adaptation. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV, pages\n781–797. Springer, 2020.\n[298] Youshan Zhang. A survey of unsupervised domain adaptation for visual\nrecognition. arXiv preprint arXiv:2112.06745, 2021.\n[299] Youshan Zhang and Brian D Davison. Efficient pre-trained features and recur-\nrent pseudo-labeling in unsupervised domain adaptation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n2719–2728, 2021.\n[300] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-\nimage translation via energy-guided stochastic differential equations. arXiv\npreprint arXiv:2207.06635, 2022.\n194\nBIBLIOGRAPHY\n[301] Nanxuan Zhao, Zhirong Wu, Rynson WH Lau, and Stephen Lin.\nWhat\nmakes instance discrimination good for transfer learning? arXiv preprint\narXiv:2006.06606, 2020.\n[302] Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai,\nand Kurt Keutzer. Multi-source domain adaptation for semantic segmenta-\ntion. Advances in Neural Information Processing Systems, 32, 2019.\n[303] Yihao Zhao, Ruihai Wu, and Hao Dong. Unpaired image-to-image translation\nusing adversarial consistency loss. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX\n16, pages 800–815. Springer, 2020.\n[304] Yin Zhao, Longjun Cai, et al. Reducing the covariate shift by mirror samples in\ncross domain alignment. Advances in Neural Information Processing Systems,\n34:9546–9558, 2021.\n[305] Huicheng Zheng, Grégoire Lefebvre, and Christophe Laurent. Fast-learning\nadaptive-subspace self-organizing map: An application to saliency-based\ninvariant image feature construction. IEEE Transactions on Neural Networks,\n19(5):746–757, 2008.\n[306] Wanfeng Zheng, Qiang Li, Guoxin Zhang, Pengfei Wan, and Zhongyuan Wang.\nIttr: Unpaired image-to-image translation with transformers. arXiv preprint\narXiv:2203.16015, 2022.\n[307] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-\nto-image translation using cycle-consistent adversarial networks. In Proceed-\nings of the IEEE international conference on computer vision, pages 2223–2232,\n2017.\n[308] Yongchun Zhu, Fuzhen Zhuang, Jindong Wang, Guolin Ke, Jingwu Chen,\nJiang Bian, Hui Xiong, and Qing He. Deep subdomain adaptation network\nfor image classification. IEEE transactions on neural networks and learning\nsystems, 32(4):1713–1722, 2020.\n[309] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for\nunsupervised learning of visual embeddings. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 6002–6012, 2019.\n195\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.GR",
    "cs.LG",
    "I.2; I.3; I.4; I.5; I.6"
  ],
  "published": "2023-11-30",
  "updated": "2023-11-30"
}