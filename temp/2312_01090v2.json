{
  "id": "http://arxiv.org/abs/2312.01090v2",
  "title": "Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model",
  "authors": [
    "Y. Sun",
    "J. Zhao",
    "C. Yu",
    "W. Wang",
    "X. Zhou"
  ],
  "abstract": "The large language models represented by ChatGPT have a disruptive impact on\nthe field of artificial intelligence. But it mainly focuses on natural language\nprocessing, speech recognition, machine learning and natural language\nunderstanding. This paper innovatively applies the large language model to the\nfield of intelligent decision-making, places the large language model in the\ndecision-making center, and constructs an agent architecture with the large\nlanguage model as the core. Based on this, it further proposes a two-layer\nagent task planning, issues and executes decision commands through the\ninteraction of natural language, and carries out simulation verification\nthrough the wargame simulation environment. Through the game confrontation\nsimulation experiment, it is found that the intelligent decision-making ability\nof the large language model is significantly stronger than the commonly used\nreinforcement learning AI and rule AI, and the intelligence, understandability\nand generalization are all better. And through experiments, it was found that\nthe intelligence of the large language model is closely related to prompt. This\nwork also extends the large language model from previous human-computer\ninteraction to the field of intelligent decision-making, which has important\nreference value and significance for the development of intelligent\ndecision-making.",
  "text": "SELF GENERATED WARGAME AI: DOUBLE LAYER AGENT TASK\nPLANNING BASED ON LARGE LANGUAGE MODEL\nA PREPRINT\nYuxiang Sun\nSchool of Management and Engineering\nNanjing University\nNanjing, 210023\nsunyuxiang@nju.edu.cn\nJunjie Zhao\nSchool of Management and Engineering\nNanjing University\nNanjing, 210023\njunjiezhao@smail.nju.edu.cn\nChecheng Yu\nSchool of Management and Engineering\nNanjing University\nNanjing, 210023\n211870228@smail.nju.edu.cn\nWei Wang\nSchool of Intelligence Science and Technology\nNanjing University\nNanjing, 210023\n221900255@smail.nju.edu.cn\nXianzhong Zhou∗\nSchool of Management and Engineering\nNanjing University\nNanjing, 210023\nzhouxz@nju.edu.cn\nDecember 19, 2023\nABSTRACT\nThe large language models represented by ChatGPT have a disruptive impact on the field of artificial\nintelligence. But it mainly focuses on natural language processing, speech recognition, machine\nlearning and natural language understanding. This paper innovatively applies the large language\nmodel to the field of intelligent decision-making, places the large language model in the decision-\nmaking center, and constructs an agent architecture with the large language model as the core. Based\non this, it further proposes a two-layer agent task planning, issues and executes decision commands\nthrough the interaction of natural language, and carries out simulation verification through the\nwargame simulation environment. Through the game confrontation simulation experiment, it is found\nthat the intelligent decision-making ability of the large language model is significantly stronger than\nthe commonly used reinforcement learning AI and rule AI, and the intelligence, understandability\nand generalization are all better. And through experiments, it was found that the intelligence of the\nlarge language model is closely related to prompt. This work also extends the large language model\nfrom previous human-computer interaction to the field of intelligent decision-making, which has\nimportant reference value and significance for the development of intelligent decision-making.\nKeywords Reinforcement learning · Large language models · Agents · Generative AI\n∗This work was supported by the National Natural Science Foundation of China under Grant 61876079. (Corresponding author:\nXianzhong Zhou)\narXiv:2312.01090v2  [cs.AI]  18 Dec 2023\nA PREPRINT\n1\nIntroduction\nSince ChatGPT was officially launched on November 30, 2022, it has quickly become one of the most popular intelligent\nChatbot Van Dis et al. [2023], Stokel-Walker and Van Noorden [2023]. Since its inception, ChatGPT has been applied in\nmultiple fields such as code correction Surameery and Shakor [2023], public health Som S [2023], and global warming\nBiswas [2023]. In July 2023, OpenAI released the Code Interpreter plugin, further enhancing ChatGPT’s data parsing\ncapabilities and addressing the natural weaknesses of large language models in mathematics and language. These\ndevelopments have provided new inspiration for improving the intelligence and generalization of AI in the field of\nintelligent wargame simulation, that is, using ChatGPT self generated AI to make intelligent decisions in war games.\nAlthough the development and application of rule AI and data-driven AI Cheng et al. [2021]is the starting point in\nthe field of intelligent wargame, data-driven AI has gradually become a research hotspot in recent years, in which\nReinforcement learning AI has made a series of breakthroughs. In terms of data-driven AI, Liu Man, Zhang Hongjun,\nand others have designed a wargame decision-making framework that balances rules and data Liu et al. [2020a]. In\nterms of Reinforcement learning AI, Li Chen’s team from Nanjing University of Science and Technology designed a\nmulti-agent decision-making method under the Actor Critical framework and achieved good intelligence Chen et al.\n[2021]. Xu Jiale, Zhang Haidong, and others designed a CNN based strategy learning model to improve the accuracy of\nwargame situation prediction Xu et al. [2022]. Tencent’s AI Lab used Deep reinforcement learning to achieve game\nconfrontation in the King’s Glory game, and defeated professional players Ye et al. [2020], Chen et al. [2020]. In a\nword, with the deepening of the combination of deep learning, Reinforcement learning and intelligent wargame, the\nintelligence of agents has been continuously improved Mnih et al. [2015], Silver et al. [2016], Vinyals et al. [2019], Liu\net al. [2020b].\nAlthough rule AI does not require a long period of training, due to its limitations in rules, the upper limit of intelligence\nlevel is difficult to break through the upper limit of rules; While data-driven AI and Reinforcement learning AI improve\ntheir intelligence and flexibility by processing large amounts of data through Reinforcement learning algorithms, their\ninterpretability is poor, and it is difficult to achieve model migration under scenario and capture point changes SUN et al.\n[2022], Wurman et al. [2022], Schrittwieser et al. [2020], Silver et al. [2018]. Therefore, improving the intelligence and\ngeneralization of AI in the field of intelligent wargame becomes the focus of further research.\nMoreover, the decision-making of adversarial games is complex and continuous. In order to make decisions more\nintelligent and generalized, this article focuses on introducing a self generated AI wargame architecture based on a large\nlanguage model. Create a decision-making mechanism that involves multiple generative agents interacting, mimicking\nhuman behavior and generating interpretable, credible, and highly generalizable game adversarial intelligent decisions.\nThe core work of this article is as follows:\n1. The self generated AI wargame architecture is an intelligent agent architecture centered on a large language\nmodel. This architecture consists of multiple generative agents, each with its own large language model (this\narticle uses ChatGPT as the driving tool). These intelligent agents can communicate and cooperate with each\nother through reflective and memory streams, and jointly make decisions. By talking to each other, they can\nshare information, analyze the situation, and make inferences and decisions based on the conversation content.\n2. Build a two-layer agent task planning model, targeting strategic agents and tactical agents to plan tasks for the\ngame confrontation process. Strategic agents describe specific situations observed by all current agents.\nPlanning refers to task allocation and execution based on all observed situational information. The tactical\nagent only focuses on the situation observed by a single agent chess piece and executes related tasks according\nto the strategic planning agent. However, tactical agents can also have their own judgments and feedback\nbased on the prompts issued by strategic agents.\n3. Taking wargame as the experimental platform, the experiment shows that the intelligent decision-making\nability of the large language model is significantly stronger than reinforcement learning AI and rule AI, and\nthe intelligence, understandability, and generalization are all better. And through experiments. Meanwhile,\nresearch has found that providing prior knowledge from experts in the field specifically to large language\nmodels can significantly improve their intelligence.\n2\nGenerative Wargame AI Architecture\nIn the war chess environment, we have realized the confrontation between six red chessmen and five blue chessmen as\nFigure 1 shows. The red and blue chessmen in different clusters have different semantic interaction information, which\nis generated through ChatGPT.\n2\nA PREPRINT\n…\n…\n…\n…\nTransport the Squad, \nProvide Fire Support\nCP: APC, transport the \nSquad to the front line and \nremain vigilant. \nAPC: CP, we are rapidly \ntransporting the Squad to \nthe front line.\nAdjust the Task Plan, Clear the \nEnemy’s Firepower Points, and \nthen Support the Squad\nCP: TK_A and TK_B, you are required \nto provide fire support to our Squad. \nTK_A: CP, the enemy defense is strong, \nshould eliminate the enemy’s firepower \npoints first and then support the infantry.\nCP: Agreed. TK_A and TK_B, clear the \nenemy’s firepower points first and then \nsupport our Squad.\nSquad: IFV, please suppress \nthe armored forces. \nIFV: Understood, I will do \nmy best to destroy it.\nRequest Fire Suppression\n…\n…\nFigure 1: Specific interaction of self generated wargame AI in the environment\nIndividual Situational \nAwareness\nBattle Situation\nCombat \nMemory Flow\nCommand \nExperience Flow\nExtracted \nCombat Memory\nExtracted\nCommand \nExperience \nTactical Agent\nCombat Operation\nStrategic Agent\nadjust\nOverall Situational \nAwareness\nBattle Situation\nextract\nTask Plan\nFigure 2: The relationship between strategic and tactics agent\nTo implement the decision-making mechanism mentioned above, we have developed an agent architecture consisting of\nthree main components: a memory stream for storing and allocating buffers and generating batches; a reflection stream\nfor using batches as prompts for the large language model to understand its role in the decision-making process; and a\ntask planning stream for synthesizing higher-level reasoning from batches to enable the agent to integrate situational\ninformation and make better pre-battle plans. The agent architecture is designed to store, synthesize, and apply past\nbattlefield experience to enable the large language model to generate trustworthy decisions.\n3\nA PREPRINT\n3\nGenerative Wargame AI Model\n3.1\nWargame Agent Interaction\nIn the architecture described above, the wargame agents obtain situational information and interact with each other in\nnatural language to maintain collaboration. Each agent describe their action in natural language, such as “red agent 1 is\npreparing to capture the control point and moving along the road”, “blue agent 2 is preparing to aim at the enemy target\n1”. Then the sentence is translated into specific actions and directly affect the wargame environment. In the meanwhile,\nall the actions and movements will be displayed as a series of number symbols which appear above each avatar to\nprovide abstract representation of actions. To achieve this, the architecture utilizes a language model to translate\nlanguage into actions, while representing a concise symbol above each wargame to represent ChatGPT’s suggestions\nfor actions that this agent should take. For example, “red agent 1 is preparing to capture the control point” is displayed\nas “!” appearing above the wargame, while “red agent 1 is preparing to aim at the enemy” is displayed as “→\".\nIn this environment, agents communicate with each other in natural language which is fully understandable by humans.\nThey obtain the situational information of other operators and environment from the semantics of sentences. Here is a\nsample of an agent communicating with another.\nFigure 3: Double Layer Agent Task Planning Decision Framework Based on Large Language Model\n3.2\nComposing model\nThe generative wargame AI aims to provide a novel decision-making framework for intelligent decision-making in\nwargame environment. Compared to the traditional rule-based AI, data-driven AI , or the reinforcement learning\n4\nA PREPRINT\nAI, our architecture utilizes ChatGPT for intelligent decision-making and interaction with the wargame environment.\nGenerative wargame AI takes the current environment and past experiences as inputs, and produces output in the form\nof generated actions.\nGenerated actions can be described as these steps: the architecture provides the large language model with well-trained\nprompts, the language model chooses the numbers which encompass all the actions to take according to prompts,\nthe agents take actions with the corresponding number. The innovation of the architecture lies in the combination of\nthe large language model with retrieval of relevant information and the utilization of prompts to adjust the output of\nlanguage model.\nBuilding upon the foundation of the architecture, we constructed a dual-layer agent system including the strategic agent\nand tactical agent. Strategic agent obtains all the information regarding the states of both its own sites and the observed\nopponents as input, then it combines this with the overall environment and input as prompts to generate a macro-level\ntactics intelligent task planning flow. And strategic agent assigns tasks to the tactics agent in the form of prompts, and\ntactics agent, based on its own states, provides modification suggestions and reasons for modifying. Then strategic\nagent keeps replanning according to these suggestions until all the tactics agents are not going to provide furthermore\nsuggestions.\nCertainly, the strategic and tactics agents still face a lot of challenges even with the use of the state-of-the-art large\nlanguage model like GPT-4 LLM(Large Language Model). Since extensive generation of events and memories generated\nby the two agents, the most critical challenge in this architecture lies in generating the most relevant memory fragments\nwhile retrieving and synthesizing relevant data from the memory stream. Therefore, this article attempts to reduce\ncomputational power and memory requirements, and uses GPT-4 LLM for strategic agent decision-making, facilitating\noverall strategic input and expert knowledge document input. For tactical agents, this article uses GPT-3.5 LLM for\ndecision-making, as tactical agents can interact and provide feedback on results in turn, reducing computational power\nand memory requirements without affecting intelligence\nMemory Stream\nAs the central component of the architecture, the memory stream directly influences the efficiency\nand accuracy of decision-making. The entire memory stream is a list of memory objects, with each object consisting\nof a natural language description, a creation timestamp and a recent access timestamp. The fundamental element in\nmemory list is observation, which contains all the situational information observed by agents. Due to the presence of\nthe fog of the war, the battlefield environment doesn’t allow for complete knowledge and awareness. The common\ninformation observed by an agent in a particular state is subject to certain limitations, which includes individual actions,\nactions taken by our own side’s agent and the actions taken by opponent agents within our visible range.\nExamples:\nObservation 1: agent observes its own side’s agent approaching the control point and trying to control it.\nObservation 2: agent observes its opposed agent approaching urban residential area and trying to shoot.\nWe construct a retrieval function within the entire memory stream architecture and utilize it to extract observations\nfrom the historical experiences of the agents, providing a foundation for generating reasonable prompts and enable the\nlanguage model to produce rational decision. The retrieval function can be selective, with the form of prioritizing the\nextraction of recently observations, important nodes set before and relevant memory to produce effective outcome.\nRecency assigns a higher score to the observation added recently, in which case the agent prioritizes the memory\ninformation generated by the recent several steps. To account for the influence of time factors, we implement a time\ndecay coefficient to calculate the score.\nImportance categorizes the data within the memory stream into regular memories and core memories, and allocating\nhigher scores to the core memories generated by agents. For example, a red agent moving towards the left and\napproaching the road can be categorized as a regular memory while a red agent approaching the control point and\neliminating a blue agent can be classified as a core memory. In this architecture, we ask the language model to directly\noutput the importance integer scores in a range from 1 to 10, in which case 1 means the purely common memory\nlike moving on the road while 10 means the most important core memory like seizing the control point or shooting\nsuccessfully. The specific implementation process can be described as follows: retrieving the corresponding memory\nfrom the memory stream to form a prompt, allowing the agent to generate importance scores accordingly and storing\nthem back to the memory stream.\nExample:\nMemory: the red agent one is seizing the control point.\nImportance score: 8 Relevance assigns a higher score to those object that are relevant to the current situation because\nof the presence of relevance between different memory objects. For example, a red agent arrives at the road and\napproaching the control point at a higher speed, this memory has a strong correlation with the red agent’s seizing the\n5\nA PREPRINT\nsecond_half\n58th\nFigure 4: Experimental Simulation Environment Display\ncontrol point. In this paper we ask ChatGPT to generate the relevance score in a scale from 1 to 10 to describe the\ndegree of relevance between memory objects.\nAs mentioned above, the three components of our architecture have been quantified into scores for the agents. To\ncalculate the final scores of agents we normalize recency, importance and relevance scores to a range of [0, 1] by scaling\nmethod. The final scores can be calculated using the following formula: scorefinal = αrecency ∗scorerecency +\nαimportance ∗scoreimportance + αrelevance ∗scorerelevance\nWe use this score to comprehensively determine the prompts that should be extracted, and instruct the agent to generate\ncorresponding and reasonable action-plannings based on these prompts.\nReflection Stream\nHowever, the observational performance of memory flow has limitations in the decision-making\nprocess in practical wargame environment. Reasoning based on raw observation is not efficient enough to allow a\nlarge language model to generate high-level decision results. It is necessary to infer and generate advanced reasoning\nsemantics through the observation of information and the planning of actions. This paper defines this reasoning process\nas a higher-level memory flow, referred to as reflection. It is essentially a higher-level and more abstract thinking process.\nThe reflection flow is generated together with the memory flow, but the generation of reflection is distinguished by the\nretrieval function in the preceding memory flow. When the score in the retrieval function exceeds a certain threshold,\nreflection is triggered. This reflective process involves a higher-level abstraction and understanding of previously\nobserved information. It is essentially a combination of observed semantics and planned semantics generated through\nprompts, and it is generated periodically, providing the wargame agent with reasoning semantics.\nThe first step of reflection is to raise questions and clarify the reflective process based on the previous experience flow\nof wargame agent. For example, the blue agent is approaching the road and accelerating towards the control point.\nThe planning suggests that the red agent should reach hexagon 1403 and shoot the blue agent at that point. From this,\nreflective semantics are generated: The blue agent poses a significant threat and may create a disadvantageous situation\nfor the red agent in this confrontation.\nThe reflection process allows the agent to reflect not only on their current observation but on other reflection. As a\nresult, memories generated by the agent can be divided into different levels under the reflection mechanism, allowing\nfor more accurate decision-making at an abstract level.\n6\nA PREPRINT\nTask Planning Stream\nStrategic agent, based on the current situation observed by all the agents of our side, describes\nit as a prompt following a specific format: < Summary, Observations, Planning >. The Summary aims to convert the\ncurrent situation from visual to semantic information Sun et al. [2022].\nObservations describe specific circumstances observed by all agents, further enrich semantic information based on the\nsummary. Planning involves task allocation and execution based on the observed situation.\nExample:\nSummary: our 10 agents are moving towards the control point and have identified 3 blue agents. Observations: blue\nagent 1 is nearing the control point.\nPlanning: red agent 1 −3 will prioritize engaging blue agent 1, while agents 4 −10 will quickly move towards the\ncontrol point.\n3.3\nChatGPT + Wargaming Business Process\nThe whole core process is to transform the situation image information in the wargame simulation into semantic\ninformation, which includes description information and situation information, and this information is sent to the\nwargame agent in the form of prompt, and then the agent feedback the corresponding planning semantics, which is goal\noriented. The planning semantics are then transformed into action sequences (such as 1, 2, 3, 4, ..10, where numbers\nrepresent specific actions. Alternatively, they are transformed into corresponding actions such as attack, defense,\nevasion, acceleration, shooting, left movement, etc.), which affect the environment and generate new environments.\nThese actions are then recycled back to the starting situation image and converted into semantics.\nOn this basis, in order to reduce computational power and memory requirements, and improve operational efficiency,\nthis article allows strategic agents to use GPT-4 LLM and tactical agents to use GPT-3.5 LLM. Compared to using\nGPT-4 or GPT-3.5 LLM entirely, this can comprehensively improve the intelligence of intelligent decision-making\nwithout requiring too much computing power and memory space. Firstly, input expert prior knowledge documents\ninto the strategic agent for learning through GPT-4 LLM, and then provide appropriate prompt inputs to enable the\nstrategic agent to make decisions through GPT-4 LLM and convert them into action outputs that affect the wargame\nenvironment. The strategic agent then sends corresponding instructions to each tactical agent for execution. The tactical\nagent provides feedback on whether the task is suitable for the current agent through GPT-3.5 LLM combined with\nappropriate prompts, and provides the recommended execution results to the strategic agent for adjustment.\nStrategic agent: Based on the task planning flow, the strategic agent synthesizes states 1 to 10 and provides a task\nplanning sequence, which is the action that each wargame should take in the step allocation;\nTactical agent: The tactical agent receives task planning and provides modification suggestions and reasons for the\nassigned tasks based on its own state;\nThe strategic agent plans again based on the modification suggestions until all tactical agents no longer provide\nmodification suggestions.\n4\nVerification of Simulation Experimental Environment\n4.1\nExperimental Environment Display\nThis paper verifies the above established large language model through simulation experiments. The simulation platform\nis a wargame simulation platform, which can conduct game confrontation between red and blue sides. Both red and\nblue sides can use intelligent algorithms to make decisions and execute actions Sun et al. [2020]. The basic adversarial\nrule is that the red and blue sides compete for the middle control point (red flag), and the party who first reaches the\ncontrol point wins. Or if one party is completely destroyed by the other party, the other party wins.\n4.2\nAdvantages of large language model over reinforcement learning intelligent decision-making\nIn the previous experiments, we mainly made decisions through rule AI and reinforcement learning AI. For the first time,\nthis work used the large language model to make decisions for agents, and it was verified on this platform. Interestingly,\nthis work found that there is a large difference between large language models and Reinforcement learning. First, large\nlanguage models or trained large language models can make decisions without waiting for the convergence of training,\nand can directly obtain high intelligence. Reinforcement learning algorithms often need a lot of training to gradually\nadapt to a new task. At the same time, compared with the reinforcement learning algorithm, the decision making using\nthe large language model can directly achieve excellent intelligence in multiple different tasks, and does not need to\nre-train for different tasks, which is of high value for practical applications.\n7\nA PREPRINT\nThis article proposes two algorithms, GWA algorithm and GWAE algorithm. The GWA algorithm adopts the composi-\ntion model proposed in this article and utilizes ChatGPT for decision-making in large language models. GWAE inputs\nexpert experience on the basis of GWA. This paper inputs expert experience of Military simulation in the form of a\ndocument. See the appendix for the document.\nMethod\nMissions\nKill\nGoal\nSurvive\nGWAE\n298±11\n10504±64\n4238±28\nGWA\n332±9\n9106±99\n5102±33\nRNM-PPO\n745±9\n9102±141\n4985±44\nPPO\n850±19\n7804±44\n5068±38\nPK-DQN\n792±14\n7732±60\n5026±53\nDQN\n1285±7\n6948±161\n5154±57\nTable 1: Scores of different algorithms for three tasks: kill, get goal, and survive.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpisodes\n0\n20\n40\n60\n80\n100\nCumulative Winrates\nLine Plot for Cumulative Winrate of All Algorithms Over Episodes\nGWAE - Ours\nGWA - Ours\nRNM-PPO\nPPO\nPK-DQN\nDQN\nFigure 5: Line Plot for Winrate of All Algorithms Over Episodes. The winning rate of GWA algorithm is generally\nhigher than that of Reinforcement learning algorithm, including RNM-PPO, PPO, PK-DQN, DQN. After inputting\nexpert prior knowledge documents for GWA algorithm, the intelligence of GWAE algorithm is significantly improved\non the basis of the original GWA.\nThe experiment compares the GWAE algorithm and GWA algorithm proposed in this paper, and compares the winning\nrate of the algorithm proposed in this paper with the RNM-PPO Xue et al. [2023], PPO, PK-DQN Sun et al. [2020]\nand DQN algorithms. Through Figure 6, Figure 7 and Figure 8, it can be found that the intelligence of reinforcement\nlearning algorithms DQN, PK-DQN, PPO, and RNM-PPO is enhanced in turn. However, the GWA algorithm that uses\nthe large language model to make decisions is better than the reinforcement learning algorithm. Only the RNM-PPO\n8\nA PREPRINT\nGWAE - Ours\nGWA - Ours\nRNM-PPO\nPPO\nPK-DQN\nDQN\nAlgorithm\n20\n0\n20\n40\n60\n80\n100\nMean Winrate\nViolin Plot for Cumulative Mean Winrate of All Algorithms Over Episodes\nFigure 6: Violin Plot for Mean Winrate of All Algorithms Over Episodes. The GWAE algorithm and GWA algorithm\nhave a higher winning rate and are more stable.\n0\n20\n40\n60\n80\nGWAE - Ours_mean_winrate\n0\n20\n40\n60\n80\nGWA - Ours_mean_winrate\n0\n20\n40\n60\nRNM-PPO_mean_winrate\n0\n20\n40\n60\nPPO_mean_winrate\n0\n20\n40\n60\nPK-DQN_mean_winrate\n0\n50\nGWAE - Ours_mean_winrate\n0\n20\n40\nDQN_mean_winrate\n0\n25\n50\n75\nGWA - Ours_mean_winrate\n0\n25\n50\n75\nRNM-PPO_mean_winrate\n0\n20\n40\n60\nPPO_mean_winrate\n0\n20\n40\n60\nPK-DQN_mean_winrate\n0\n20\n40\nDQN_mean_winrate\nFigure 7: Scatter plot of the average winning rate of all\nalgorithms. The results of comparing the overall winning\nrates of GWAE, GWA, RNM-PPO, PPO, PK-DQN, and\nDQN algorithms\nkill\ncontrol\nsurvive\nGWAE - Ours\nGWA - Ours\nRNM-PPO\nPPO\nPK-DQN\nDQN\nMethod\nTask Mean Scores\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 8: Task Mean Scores Heatmap.The performance\nof GWAE, GWA, RNM-PPO, PPO, PK-DQN, and DQN\nalgorithms in three typical tasks: kill, control, and survive.\nThe darker the color, the better the algorithm performs in\nthis task.\nalgorithm is closer to GWA. If expert experience documents are fed to GWA, the GWAE algorithm’s victory rate will be\nsignificantly improved. Overall, the overall winning rate of the algorithm proposed in this paper is significantly better\nthan the previous classical reinforcement learning algorithm. Also on the premise of fixed blue intelligence and fixed\nreasoning scenarios, the overall effect of intelligent decision-making using the large language model is also relatively\n9\nA PREPRINT\nstable, the overall victory rate fluctuates relatively little. For different typical task scenarios, GWA and GWAE exhibits\noutstanding performance, especially in the task of scoring, the performance of GWA and GWAE is significantly better\nthan that of classical reinforcement learning algorithms. These two algorithms have demonstrated the potential value of\nlarge language models in the field of intelligent decision-making by making appropriate decisions for different task\nplanning scenarios without undergoing extensive training.\n5\nConclusion\nThis work innovatively applies the large language model to intelligent decision-making, and verifies the feasibility of\nthe large language model for decision-making in the wargame platform. Compared with the intelligent decision-making\nof Reinforcement learning, this paper finds that the large language model has obvious advantages. Firstly, the large\nlanguage model for decision-making has strong adaptability in practical game confrontations due to sufficient training in\nadvance. There is no need to wait to restart training, and it has strong intelligence and generalization for different tasks.\nSecondly, the intelligence shown by the large language model is obviously stronger than the general Reinforcement\nlearning algorithm, which proves the great potential of the large language model in decision-making. Finally, this article\nalso found through experiments that there is a significant correlation between the intelligence of large language models\nand prompt. If there is a more suitable prompt, its displayed intelligence is significantly improved. Of course, the work\nof this article is still an initial exploration of the large language model, and there is still much work to be innovated in\nthe future, such as the attempt of the large language model in different scenarios, and the use of the large language\nmodel in more complex game adversarial environments to further enhance the intelligence of the adversarial blue, in\norder to test the intelligence level of the large language model. This work also extends the large language model from\nprevious human-computer interaction to the field of intelligent decision-making, which has important reference value\nand significance for the development of intelligent decision-making.\nReferences\nEva AM Van Dis, Johan Bollen, Willem Zuidema, Robert van Rooij, and Claudi L Bockting. Chatgpt: five priorities for\nresearch. Nature, 614(7947):224–226, 2023.\nChris Stokel-Walker and Richard Van Noorden. What chatgpt and generative ai mean for science. Nature, 614(7947):\n214–216, 2023.\nNigar M Shafiq Surameery and Mohammed Y Shakor. Use chat gpt to solve programming bugs. International Journal\nof Information Technology & Computer Engineering (IJITC) ISSN: 2455-5290, 3(01):17–22, 2023.\nBiswas Som S. Role of chat gpt in public health. Annals of biomedical engineering, 51(5):868–869, 2023.\nSom S Biswas. Potential use of chat gpt in global warming. Annals of biomedical engineering, 51(6):1126–1127, 2023.\nK Cheng, G Chen, X Yu, M Liu, and T Shao. Knowledge traction and data-driven wargame ai design and key\ntechnologies. Syst. Eng. Electron. Technol, 43:2911–2917, 2021.\nM Liu, H Zhang, W Hao, K Cheng, and J Wang. Decision method for tactical level military chess entity operations.\nControl and Decision, 35(12):2977–2985, 2020a.\nLi Chen, Huang Yanyan, Zhang Yongliang, and Chen Tiande. Multi-agent decision-making method under the actor-critic\nframework and its application in wargames [j]. Systems engineering and electronic technology, 43(03):755–762,\n2021.\nJ Xu, H Zhang, D Zhao, and W Ni. Tactical maneuver strategy learning of land war based on convolutional neural\nnetwork. Journal of System Simulation, 34(10):2181, 2022.\nDeheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei\nGuo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 34, pages 6672–6679, 2020.\nGuibin Chen, Deheng Ye, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng\nYu, et al. Towards playing full moba games with deep reinforcement learning. Advances in Neural Information\nProcessing Systems, 33:621–632, 2020.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement\nlearning. nature, 518(7540):529–533, 2015.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks\nand tree search. nature, 529(7587):484–489, 2016.\n10\nA PREPRINT\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H\nChoi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent\nreinforcement learning. Nature, 575(7782):350–354, 2019.\nCY Liu, CX Mu, and CY Sun. Overview of deep reinforcement learning algorithm and its application. Journal of\nIntelligent Science and Technology, 2(4):314–326, 2020b.\nYuxiang SUN, Yihui PENG, Bin LI, Jiawei ZHOU, Xinlei ZHANG, and Xianzhong ZHOU. Review of intelligent\ngames: Implications of game ai for combat inference. Chinese Journal of Intelligent Science and Technology, 4(2):\n157–173, 2022.\nPeter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh,\nRoberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers\nwith deep reinforcement learning. Nature, 602(7896):223–228, 2022.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur\nGuez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with\na learned model. Nature, 588(7839):604–609, 2020.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot,\nLaurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters\nchess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.\nYuxiang Sun, Bo Yuan, Qi Xiang, Jiawei Zhou, Jiahui Yu, Di Dai, and Xianzhong Zhou. Intelligent decision-making\nand human language communication based on deep reinforcement learning in a wargame environment. IEEE\nTransactions on Human-Machine Systems, 53(1):201–214, 2022.\nYuxiang Sun, Bo Yuan, Tao Zhang, Bojian Tang, Wanwen Zheng, and Xianzhong Zhou. Research and implementation\nof intelligent decision based on a priori knowledge and dqn algorithms in wargame environment. Electronics, 9(10):\n1668, 2020.\nYufan Xue, Yuxiang Sun, Jiawei Zhou, Lisha Peng, and Xianzhong Zhou. Multi-attribute decision-making in wargames\nleveraging the entropy-weight method in conjunction with deep reinforcement learning. IEEE Transactions on\nGames, 2023.\nA\nLarge Language Model Prompt\nIn this experiment, it was found that the intelligence of the large language model is closely related to prompt. Two\ndifferent prompts were used for guidance in this experience, and the results returned after prompt input are shown in the\nfigure. The figure 9 shows the response results of the strategic agent feedback. The figure 10 shows the response results\nof tactical agent feedback.\nB\nPrior Knowledge Document\nThis article feeds professional domain knowledge documents into the proposed GWA algorithm, forming the GWAE\nalgorithm. Document link: https://github.com/sunyuxiang926/wargame/blob/b0118cd0\n1fa46f2790a89895336df81519d91707/files/theses/explanat\nion_of_domain_expert_knowledge.pdf\n11\nA PREPRINT\nStrategic Agent:\nRed tank 1 Move_to: 16, 14 Reason: Tank 1 is moved from the closest corner to the target, while avoiding the direct \nattack of blue tank 2.\nRed tank 2 Move_to: 17, 17 Reason: Tank 2 is moved as close to the target as possible while avoiding the possible fire \nrange of blue tanks 2 and 10.\nRed tank 3 Move_to: 18, 19 Reason: Tank 3 is moved towards the target position, trying to stay out of the range of blue \ntanks 2 and 10.\nRed tank 4 Move_to: 15, 17 Reason: Tank 4 is moved to a location that can fend off the firepower of blue tanks 2 and 10 \nand quickly reach the target when needed.\nRed tank 5 Move_to: 16, 19 Reason: Tank 5 is trying to get close to the target, while maintaining a safe distance to avoid \nthe firepower of blue tanks 2 and 10.\nRed tank 6 Move_to: 17, 19 Reason: Tank 6 is heading towards the target, keeping a distance as much as possible to avoid \nthe firepower of blue tanks 2 and 10.\nRed tank 7 Move_to: 16, 17 Reason: Tank 7 is maintained in a good position towards the target, keeping itself out of the \ndirect range of blue tanks 2 and 10.\nRed tank 8 Move_to: 19, 18 Reason: Tank 8 is moved towards the target, trying to stay out of the range of blue tanks 2 \nand 10.\nRed tank 9 Move_to: 20, 17 Reason: Tank 9 is moving forward to the target, avoiding the fire range of blue tanks 2 and 10.\nRed tank 10 Move_to: 20, 16 Reason: Tank 10 is trying to get as close to the target as possible, keeping a safe distance to \navoid the firepower of blue tanks 2 and 10.\nThese moves aim to position each tank towards the target point and to keep each tank at as varied locations as possible \nto avoid the concentrated firepower from the blue team. This strategy aims to get the tanks to the target point as quickly \nas possible and to avoid enemy firepower where possible.\nFigure 9: Strategic Agent Prompt Feedback Display\nTactical Agent:\nAs Red tank 1, the command \"Red tank 1 Hide\" given by the command center seems somewhat unreasonable from my \nperspective.\n\"Hide\" implies that I should avoid blue tank 2's attacks, which may lead to distancing myself from the target control point. \nAs an attacking force, my priority should be to approach the target and engage in combat at the right moment, rather \nthan evading the battle.\nMy ultimate goal is to capture control point [12, 24], and currently, blue tank 2 (located at [16, 40]) is closer to the control\npoint. If I choose to hide, it will further increase the distance between me and the target, which is not conducive to \nreaching the objective quickly.\nIn the last step, blue tank 5, 7, and 8 launched indirect attacks at [17, 7], and blue tank 10 also moved near [17, 41]. \nGiven that blue tank 5, 7, and 8 launched indirect attacks, I have reason to believe that the area around [17, 7] might not \nbe the position of blue tank 2. Therefore, avoiding direct confrontation with blue tank 2 is not the most urgent task at the \nmoment.\nIn conclusion, from my perspective, I believe that \"Red tank 1 Hide\" is not the most reasonable command. Instead, I \nshould choose to advance towards the control point [12, 24], adopt a suitable strategy to avoid direct engagement with \nblue tank 2, and consider how to coordinate with the movements of other friendly tanks to achieve victory.\nFigure 10: Tactical Agent Prompt Feedback Display\n12\n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2023-12-02",
  "updated": "2023-12-18"
}