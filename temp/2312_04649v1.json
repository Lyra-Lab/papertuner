{
  "id": "http://arxiv.org/abs/2312.04649v1",
  "title": "PyThaiNLP: Thai Natural Language Processing in Python",
  "authors": [
    "Wannaphong Phatthiyaphaibun",
    "Korakot Chaovavanich",
    "Charin Polpanumas",
    "Arthit Suriyawongkul",
    "Lalita Lowphansirikul",
    "Pattarawat Chormai",
    "Peerat Limkonchotiwat",
    "Thanathip Suntorntip",
    "Can Udomcharoenchaikit"
  ],
  "abstract": "We present PyThaiNLP, a free and open-source natural language processing\n(NLP) library for Thai language implemented in Python. It provides a wide range\nof software, models, and datasets for Thai language. We first provide a brief\nhistorical context of tools for Thai language prior to the development of\nPyThaiNLP. We then outline the functionalities it provided as well as datasets\nand pre-trained language models. We later summarize its development milestones\nand discuss our experience during its development. We conclude by demonstrating\nhow industrial and research communities utilize PyThaiNLP in their work. The\nlibrary is freely available at https://github.com/pythainlp/pythainlp.",
  "text": "PyThaiNLP: Thai Natural Language Processing in Python\nWannaphong Phatthiyaphaibun♦, Korakot Chaovavanich†, Charin Polpanumas†,\nArthit Suriyawongkul‡, Lalita Lowphansirikul♦, Pattarawat Chormai§¶,\nPeerat Limkonchotiwat♦, Thanathip Suntorntip♣, Can Udomcharoenchaikit♦\n♦VISTEC, †PyThaiNLP, ‡Trinity College Dublin,\n§Technische Universität Berlin, ¶Max Planck School of Cognition, ♣Wisesight\nwannaphong.p_s21@vistec.ac.th\nAbstract\nWe present PyThaiNLP, a free and open-source\nnatural language processing (NLP) library for\nThai language implemented in Python. It pro-\nvides a wide range of software, models, and\ndatasets for Thai language. We first provide\na brief historical context of tools for Thai lan-\nguage prior to the development of PyThaiNLP.\nWe then outline the functionalities it provided\nas well as datasets and pre-trained language\nmodels. We later summarize its development\nmilestones and discuss our experience during\nits development. We conclude by demonstrat-\ning how industrial and research communities\nutilize PyThaiNLP in their work. The library\nis freely available at https://github.com/\npythainlp/pythainlp.\n1\nIntroduction\nIn recent years, the field of natural language pro-\ncessing has witnessed remarkable advancements,\ncatalyzing breakthroughs for various applications.\nHowever, Thai has remained comparatively under-\nserved due to the challenges posed by limited lan-\nguage resources (Arreerard et al., 2022).\nThai is the de facto national language of Thai-\nland. It belongs to Tai linguistic group within the\nKra-Dai language family. According to Ethnologue\n(Eberhard et al., 2023), there are 60.2 million users\nof Central Thai, of which 20.8 million are native\n(2000). If including the Northern (6 million, 2004),\nNortheastern (15 million, 1983), and Southern (4.5\nmillion, 2006) variants, there are estimated 85.7\nmillion users of Thais speakers around the world.\nThai is a scriptio continua or has neither spaces\nnor other marks between the words or sentences in\nits most common writing style (Sornlertlamvanich\net al., 2000). The lack of clear word and sentence\nboundaries leads to ambiguity that cannot be disam-\nbiguated using merely just grammatical knowledge\n(Supnithi et al., 2004).\nAlthough many closed-source open APIs for\nNLP have an ability to process Thai language1,\nwe believe that an open-source toolbox is essen-\ntial for both researchers and practitioners to not\nonly access the NLP capabilities but also gain full\ntransparency and trust on both training data and\nalgorithms.2 This allows the community to adapt\nand further develop the functionalities as needed,\nmaking a crucial step towards democratizing NLP.\nThis paper introduces PyThaiNLP, an open-\nsource Thai natural language processing library\nwritten in Python programming language. Its fea-\ntures span from a simple dictionary-based word\ntokenizer, to a statistical named-entity recognition,\nand an instruction-following large language model.\nThe library was released in 2016 under an Open\nSource Initiative-approved Apache License 2.0 that\nallows free use and modification of software, in-\ncluding commercial use.\n2\nOpen-source Thai NLP before\nPyThaiNLP\nBefore PyThaiNLP started in 2016, some free and\nopen-source software do exist for different Thai\nNLP tasks, but there were no unified open-source\ntoolkits that unified multiple tools or tasks in a\nsingle library, and the number of available Thai\nNLP datasets was low compared to high-resource\nlanguages like Chinese, English, or German.\nNatural Language Toolkit (NLTK) (Bird and\nLoper, 2004), one of the most comprehensive and\nmost popular NLP libraries in Python at the time,\ndid not support Thai. OpenNLP, another popular\nfree and open-source NLP toolkit written in Java,\n1Such as those provided by commercial cloud service\nproviders and “AI for Thai”, the government-funded Thai\nAI service platform at https://aiforthai.in.th/.\n2For a discussion about concentrated power and the politi-\ncal economy of the ‘open’ AI, see Widder et al. (2023).\narXiv:2312.04649v1  [cs.CL]  7 Dec 2023\nstarted having Thai models in version 1.4 (2008)3\nbut in version 1.5 (2010) Thai was no longer listed\nin its supported languages4.\nOpen Thai language resources, like annotated\ncorpora, were also limited in size and number.\n“Publicly available” datasets tend to have restricted\naccess, either through restrictive licenses5 or the\nregistration requirement, or both.\nBecause there is a few toolkits available, limited\nin documentation and performance, short of rig-\norous benchmarking, and/or lack of maintenance,\nThai NLP reseachers had to spend their limited time\nand resources building basic components and/or\ncollecting a dataset before they could proceed fur-\nther for more advanced problems. The limited\navailability of source codes and datasets also af-\nfects reproducibility.\nExamples of Thai NLP tools and datasets before\nPyThaiNLP (pre-2016):\n• Word tokenization:\nICU BreakIterator\n(IBM Corporation et al., 1999) [Unicode\nLicense] based on Gillam (1999), LibThai\n(Thai Linux Working Group, 2001) [LGPL],\nKU Wordcut (Sudprasert and Kawtrakul,\n2003) [GPL], SWATH (Charoenpornsawat,\n2003) [GPL] based on Meknavin et al.\n(1997), LexTo (National Electronics and Com-\nputer Technology Center, 2006) [LGPL],\nOpenNLP (Bierner et al., 2008) [LGPL],\nTLex (Haruechaiyasak and Kongyoung, 2009)\n[Freeware], and wordcutpy (Satayamas, 2015)\n[LGPL]. Haruechaiyasak et al. (2008) pro-\nvided a comparative study of some of these\ntools.\n• Part-of-speech (POS) tagging: OpenNLP\nand RDRPOSTagger (Nguyen et al., 2014)\n[GPL] support Thai POS tagging. There are\ncorpora such as ORCHID (Sornlertlamvanich\net al., 1999) and NAiST (Kawtrakul et al.,\n3https://opennlp.sourceforge.net/models-1.4.\nIts\nREADME\nfrom\nDecember\n2008\nalso\nmentioned\nThai\ncomponents:\nhttps://web.archive.org/web/\n20081219153426/http://opennlp.sourceforge.net/\nREADME.html\n4https://opennlp.sourceforge.net/models-1.5.\nArreerard et al. (2022), however, reports that Apache\nOpenNLP supports these basic Thai NLP tasks:\nword\ntokenization, part-of-speech tagging, and sentence detection.\n5Even today, this practice continues: take, for instance, the\nLST20 corpus from NECTEC, which has multiple layers of lin-\nguistic annotation. However, the free version can only be used\nfor non-commercial purposes. See https://opend-portal.\nnectec.or.th/en/dataset/lst20-corpus.\n2002) which provide not only POS but also\nword boundaries.\n• Named-entity recognition (NER): Polyglot\n(Al-Rfou, 2015) [GPL], a multilingual NLP\nsoftware, supports Thai NER based on Al-\nRfou et al. (2015).\nFor datasets, BEST-\n2009 corpus (Kosawat et al., 2009) is avail-\nable but cannot be used commercially, as\nits license is Creative Commons Attribution-\nNonCommercial-ShareAlike Public License.\n• Automatic speech recognition (ASR): Thai\nLanguage Audio Resource Center (ThaiARC)\ncorpus (Hoonchamlong et al., 1997) pro-\nvides audio recordings of dialects and speech\nstyles, with transcripts; it is not designed\nspecifically for ASR. NECTEC-ATR (Ka-\nsuriya et al., 2003a), LOTUS (Kasuriya et al.,\n2003b), LOTUS-BN (Chotimongkol et al.,\n2009), LOTUS-Cell (Chotimongkol et al.,\n2010), CU-MFEC (Kertkeidkachorn et al.,\n2012) and TSync-2 are ASR corpora for dif-\nferent domains and tasks; their licenses are\nnot fully open. See Charoenporn et al. (2004),\nWutiwiwatchai and Furui (2007), and Kertkei-\ndkachorn et al. (2012) for reviews.\nApart from the ones listed above, more open-\nsource Thai word tokenizers were released after\n2009 as a result of BEST (Benchmark for Enhanc-\ning the Standard of Thai language processing) eval-\nuation for Thai word segmentation organized by\nthe National Electronics and Computer Technol-\nogy Center (NECTEC) in 2009 (Kosawat, 2009),\nand 20106. Unfortunately, these tokenizers are no\nlonger maintained and are not accessible at the\ntime of writing. The most impactful contribution\nfrom BEST, however, is the BEST-2010 word seg-\nmentation dataset that was publicly released. This\ndataset provides a basis for a lot of modern Thai\nopen-source word segmentation software.\nWe should also mention the Thai Language\nToolkit (TLTK) (Aroonmanakun and Thamrongrat-\ntanarit, 2018). Its first release on Python Pack-\nage Index (version 0.3.4, February 2018) includes\nstatistical syllable and word segmentation (Aroon-\nmanakun, 2002), POS tagging, and spelling sug-\ngestion. Its latest version, as of writing, features\ndiscourse unit segmentation, NER, grapheme-to-\nphoneme conversion, IPA transcription, romaniza-\n6https://thailang.nectec.or.th/archive/\nindexa290.html\ntion, and more. To date, TLTK and PyThaiNLP are\nthe only two comprehensive Thai NLP libraries for\nPython. However, TLTK’s documentation is still\nquite limited. For more reviews on Thai NLP tools\nand datasets, including more recent ones (post-\n2016), see Arreerard et al. (2022).\n3\nPyThaiNLP and Its Ecosystem\nOur primary objective is to ensure the user-\nfriendliness and simplicity of the library. Drawing\ninspiration from NLTK, we follow numerous es-\ntablished interfaces. For example, word_tokenize\nand pos_tag. In addition, we also create datasets\nand pre-trained models for the Thai language. Fig-\nure 1 illustrates the overview of PyThaiNLP’s func-\ntionalities and its ecosystem. Table 1 displays the\ndevelopment milestones of PyThaiNLP.\nWe will discuss here only popular features and\nmajor datasets/models.\n3.1\nFeatures\n3.1.1\nWord and Sentence Tokenization\nPyThaiNLP supports many word tokenization algo-\nrithms.7 The default algorithm is NewMM which\nis dictionary-based maximum matching (Sornlert-\nlamvanich, 1993) and utilizes Thai character cluster\n(Theeramunkong et al., 2000). The pure-Python to-\nkenizer performs reasonably well on public bench-\nmarks. Chormai et al. (2020) demonstrated that\nit is the fastest word tokenizer on the BEST 2010\nbenchmark, with 71.18 % accuracy (compared to\nstate-of-the-art at 95.60 %). Thanathip Suntorntip\nported NewMM to Rust programming language8,\nresulting in an even faster word tokenizer in our\ntoolbox.\nFor sentence tokenization, we trained a condi-\ntional random field (CRF) model, using python-\ncrfsuite (Peng and Korobov, 2014), on translated\nTED transcripts and Thai sentence boundaries are\nassumed to be denoted by English sentence bound-\naries (Lowphansirikul et al., 2021b).\n3.1.2\nSpell Checking\nFor spell checking, we have many engines; the\nNorvig (2007) one uses a spelling dictionary from\nThai National Corpus (Aroonmanakun et al., 2009),\n7For the ease of experimenting with different word\ntokenization algorithms,\nPattarawat Chormai has cre-\nated a Thai word tokenizers collection as a Docker\ncontainer\nimage:\nhttps://github.com/PyThaiNLP/\ndocker-thai-tokenizers.\n8https://github.com/pythainlp/nlpo3\nsymspellpy (mmb L, 2018) that is a Python port\nof SymSpell v6.7.1, and phunspell (Wright, 2021)\nthat is a port of Hunspell.\n3.1.3\nPhonetic Algorithm and Transliteration\nPyThaiNLP supports a couple of grapheme-to-\nphoneme (g2p) conversion engines. We trained\nThai-g2p model with data from Wiktionary9, a free\nonline dictionary.\nPyThaiNLP implemented many Thai Soundex\nalgorithms. For example, Lorchirachoonkul (1982),\nUdompanich (1983), Thai-English cross-language\nSoundex (Suwanvisat and Prasitjutrakul, 1998),\nand MetaSound (Metaphone-Soundex combina-\ntion) (Snae and Brückner, 2009).\nPyThaiNLP supports the following transliter-\nation implementations:\nThai romanization us-\ning the Royal Thai General System of Tran-\nscription (RTGS), transliteration of romanized\nJapanese/Korean/Mandarin/Vietnamese texts to\nThai using Wunsen library (cakimpei, 2022)10, and\nThai word pronunciation.\n3.1.4\nSequence Tagging (NER and POS)\nWe create a named-entity recognition model called\nThai NER (Phatthiyaphaibun, 2022) by finetuning\nthe WangchanBERTa model (Lowphansirikul et al.,\n2021a) and CRF model.\nFor part-of-speech tagging, we trained a CRF\ntagger, a perceptron tagger (Honnibal, 2013), a\nunigram tagger, and finetuned the Wangchan-\nBERTa model. The POS training sets are derived\nfrom ORCHID corpus (Sornlertlamvanich et al.,\n1999), Blackboard Treebank annotated based on\nthe LST20 Annotation Guideline (Boonkwan et al.,\n2020), and Parallel Universal Dependencies (PUD)\ntreebanks (Smith et al., 2018).\n3.1.5\nCoreference Resolution and Entity\nLinking\nFor coreference resolution, we create Han-Coref,\na Thai coreference resolution corpus and model\n(Phatthiyaphaibun and Limkonchotiwat, 2023).\nFor entity linking, PyThaiNLP supports it using\nBELA model (Plekhanov et al., 2023).\n3.1.6\nWord Embeddings\nWe extract token embeddings from our thai2fit\n(Polpanumas and Phatthiyaphaibun, 2021), a word-\nlevel ULMFiT language model (Howard and Ruder,\n9https://www.wiktionary.org/\n10The library implements various transliteration systems\nthat recommended by the Royal Society of Thailand.\nFigure 1: Functionalities, datasets, and pre-trained language models available in PyThaiNLP’s ecosystem.\n2018) (Howard and Gugger, 2020) trained on Thai\nWikipedia, and use them as word embeddings for\nPyThaiNLP. It was the state-of-the-art pre-trained\nmodel in many Thai classification benchmarks (Pol-\npanumas and Suwansri, 2020) before the multilin-\ngual BERT model was released (PyCon Thailand,\n2019).\n3.1.7\nMachine Translation\nWe collaborated with VISTEC-depa Thailand\nArtificial Intelligence Research Institute (AIRe-\nsearch.in.th)11 to create the English-Thai transla-\ntion dataset and model. The model outperformed\nGoogle Translate on an out-of-sample test set at\nthe time of release (Lowphansirikul et al., 2021b).\n3.1.8\nAutomatic Speech Recognition\nIn order to develop a dataset for ASR, PyThaiNLP\nmembers contribute to the development of Com-\nmon Voice corpus (Ardila et al., 2020), including\nThai sentence cleanup and validation rules for its\nSentence Collector12, an online campaign inviting\npeople to contribute Thai sentences, and offline\nevents for volunteers to contribute their voices and\nvoice validation.\nUtilizing Common Voice Corpus 7.0, we created\na Thai ASR model in collaboration with AIRe-\nsearch.in.th and achieved the lowest character error\n11AIResearch.in.th is an initiative co-funded by a research\nuniversity and a government agency, namely Vidyasirimedhi\nInstitute of Science and Technology (VISTEC) in Wang Chan,\nRayong, and the Digital Economy Promotion Agency (depa)\nunder the Ministry of Digital Economy and Society, to create\nAI infrastructure for Thailand.\n12https://github.com/common-voice/\nsentence-collector\nrate in a benchmark (VISTEC-depa AI Research\nInstitute of Thailand, 2023).\n3.2\nDatasets\n3.2.1\nVISTEC-TPTH-2020: Word\nTokenization, Spell Checking and\nCorrection\nVISTEC-TPTH-2020 is a Thai word tokenization\nand spell checking dataset in the social media do-\nmain, the largest one to date (Limkonchotiwat et al.,\n2021). We collected 50,000 sentences from top\ntrending posts on Twitter in 2020 and selected\nonly posts with substantial character counts. This\ndataset is a multi-task dataset, including mention\ndetection, spell checking, and spell correction.\n3.2.2\nThai NER: Named Entity Recognition\nThai NER is a Thai named-entity recognition\ndataset. We curated text from various domains\nincluding news, Wikipedia articles, government\ndocuments, as well as text from other Thai NER\ndatasets. The data is manually re-labeled for con-\nsistency (Phatthiyaphaibun, 2022).\n3.2.3\nHan-Coref: Coreference Resolution\nHan-Coref is a coreference resolution dataset con-\ntaining 1,339 documents in news and Wikipedia\ndomains (Phatthiyaphaibun and Limkonchotiwat,\n2023).\n3.2.4\nscb-mt-en-th-2020: English-Thai\nMachine Translation\nscb-mt-en-th-2020 is an English-Thai sentence pair\ndataset consisting of 1,001,752 text pairs (Low-\nphansirikul et al., 2021b). It is a collaborative work\nwith AIResearch.in.th.\n3.3\nPre-trained Language Models\nWangchanBERTa is an encoder-only pre-trained\nThai language model. Based on public benchmarks,\nit is the current state-of-the-art (Lowphansirikul\net al., 2021a). It is also a collaborative work with\nAIResearch.in.th.\nWangChanGLM (Polpanumas et al., 2023) is a\nmultilingual instruction-following model finetuned\nfrom XGLM (Lin et al., 2022).\n4\nCommunity and Project Milestones\n4.1\nFoundation Years (2016-2019)\nWannaphong Phatthiyaphaibun, a high school stu-\ndent at the time, created PyThaiNLP in 2016 as a\nhobby project. He wanted to create a simple Thai\nchatbot in Python. He used PyICU as a word tok-\nenizer and soon found out that Thai language did\nnot have a comprehensive NLP toolkit in Python\nlike NLTK (Bird and Loper, 2004). He decided\nto create PyThaiNLP and hosted the project on\nGitHub13.\nAfter the first few official releases, following\nKorakot Chaovavanich’s suggestion, a “Thai Natu-\nral Language Processing” group has been created\nas a public Facebook group14. This serves as a\nmain venue to showcase PyThaiNLP’s capabilities\nand a hub for Thai NLP researchers and practition-\ners to discuss the field. Today, the group has over\n16,000 members and is Thailand’s largest NLP in-\nterest group. This communication channel also\nperforms a recruiting function for us. The first of-\nfline meetup of the group occurred in 24 May 2018\nas a bird-of-a-feather session after a Data Science\nBKK meetup15.\nMany of our main contributors, such as Charin\nPolpanumas and Arthit Suriyawongkul organically\njoined the project from the community. At this\nstage, we created foundational capabilities such\nas word tokenization, part-of-speech tagging, sub-\nword tokenization, named-entity recognition, and\nword vectors. A lot of code cleaning, reorgani-\nzation, and documentation also happened around\n2018-2019. This included the adoption of PEP\n484 type hints16 and other Python best practices to\nmake the code even more readable and facilitate\noff-line type checkers. The adoption of PyThaiNLP\n13https://github.com/pythainlp/pythainlp\n14https://www.facebook.com/groups/thainlp\n15https://www.facebook.com/groups/thainlp/\npermalink/564348637279964/\n16https://peps.python.org/pep-0484/\ncan be reflected by the number of stars on GitHub\nthe project received over the years (Figure 2).\n4.2\nGaining Resources for Large Language\nModels (2019-present)\nThe growing activity of PyThaiNLP development\ncan be seen from the number of code commits to\nthe Git repository, which reached its peak in Q4\n201917. In 2020, the project began a collaboration\nwith AIResearch.in.th. Their main focus was to cre-\nate and distribute open-source models and datasets.\nThis collaboration has provided PyThaiNLP with\ncomputational resources we need to scale up our op-\nerations as well as additional developers for main-\ntaining the project, such as Lalita Lowphansirikul.\nUnder the collaboration, we have built an\nEnglish-Thai sentence pair dataset and the state-of-\nthe-art English-Thai translation model (Lowphan-\nsirikul et al., 2021b), the RoBERTa-based monolin-\ngual language model WangchanBERTa (Lowphan-\nsirikul et al., 2021a), and most recently the multilin-\ngual instruction-following model WangChanGLM\n(Polpanumas et al., 2023).\nDue to limited computational and human re-\nsources, we prioritize features with the high-\nest impact-to-effort ratio. For example, during\n2019-2020, there were two types of dominant\ntransformer-based language models: encoder-only\nBERT family and decoder-only GPT family. We\nopted to pursue the encoder-only models and\ntrained WangchanBERTa because, at the time, it\nrequired relatively fewer resources to train and had\nbetter performance across impactful tasks such as\ntext classification, sequence tagging, and extrac-\ntive question answering. It was not until decoder-\nonly models proved to create more value-added\nin 2022 that we started to train such models as\nWangChanGLM.\n4.3\nCommunity and Infrastructure for\nSoftware Quality\nIt is important to be noted that the community not\nonly made contributions in the form of feature im-\nprovements but also in the areas of documenta-\ntion, including computational documentation (e.g.,\nJupyter notebooks), improving code quality and\ntest suite, and streamlining software testing and\ndelivery. Some of which may not be visible to the\nusers but are crucial for the development of the\nproject.\n17https://github.com/PyThaiNLP/pythainlp/\ngraphs/contributors\nYears\nNotable Features\n2016\nWord tokenization, part-of-speech tagging\n2017\nSoundex, spell checking, WordNet support\n2018\nText classification language model, NER corpus/model, date and time parsing/formatting\n2019\nSyllable tokenization, date and time spell out\n2020\nASR model, machine translation dataset/model, grapheme-to-phoneme conversion\n2021\nAutoencoding language model, word-to-phoneme conversion\n2022\nDependency parsing, nested NER, text augmentation\n2023\nCoreference resolution dataset/model, generative language model\nTable 1: Notable features introduced to PyThaiNLP over the years.\nOn the infrastructure side, test automation and\ncontinuous integration (CI) helps us systematically\nreinforce code style, detect code security vulnera-\nbilities, maintain code coverage, and test the library\nin different computer configurations.\nWe were since 2017 rely on free Travis CI18\nand AppVeyor19 for continuous integration work-\nflow and later in June 2020 completely migrated\nto GitHub Actions20. Every GitHub pull requests\nwill go through Black21 for code formatting and\nFlake822 for PEP 8 code style23 and cyclomatic\ncomplexity checks (McCabe, 1976). pip installa-\ntion package will be built and tested against the\ntest suite in Linux, macOS, and Windows24. The\npackage then can be automatically publish to the\nPython Package Index directly from the CI, once it\npassed all the tests in every platform.\nPyThaiNLP code coverage reached 80 % to-\nwards the end of 2018, compare to under 60 %\nin 2017. Code coverage is a metric that can help\nassess the quality of the test suite, and it therefore\nreflects how well the functionalities are thoroughly\ntested. The coverage went over 90 % in August\n2019 and kept stable at this level until 202225.\nFrom early 2022, we experienced a gradual drop\nof the code coverage to 80 %. The main reason is\n18https://www.travis-ci.com/\n19https://www.appveyor.com/\n20https://github.com/features/actions\n21https://github.com/psf/black\n22https://flake8.pycqa.org\n23https://peps.python.org/pep-0008/\n24Easy installation and consistent behavior across platforms\nare what we aim for. This is one of the reasons why we devel-\noped a pure-Python NewMM. The previous implementation\nof our default word tokenizer requires marisa-trie, a trie data\nstructure library in C++. Unfortunately, marisa-trie does not\nofficially support mingw32 compiler on Windows.\n25Our code coverage is measured by coverage.py which is\nincluded in our continuous integration workflow. The coverage\nstats are made available online by Coveralls at: https://\ncoveralls.io/github/PyThaiNLP/pythainlp\na growing number of features that require a large\nlanguage model that cannot fit inside our standard\nGitHub-hosted runners. We have to remove some\nof the tests for those features. Before 2022, we\nalso tested our library against versions of CPython\nand PyPy, but now it has been reduced to only\nCPython 3.8 due to the lack of support for other\nPython versions in some of our machine learning\ndependencies.\nSome of the common code improvements we\nmade after analyzing code coverage and other tests\nwere the removal of unused code, fixing inconsis-\ntent behavior in different operating systems, better\nhandling of a very long string, empty string, empty\nlist, null, and/or negative values, and better han-\ndling of exceptions in control flow, resulting a code\nthat is smaller and more robust.\n5\nPyThaiNLP in the Wild\n5.1\nPyThaiNLP and Its Research Impact\nResearchers worldwide use PyThaiNLP to work\nwith Thai language. For instance, for word to-\nkenization in cross-lingual language model pre-\ntraining (Lample and Conneau, 2019), universal\ndependency parsing (Smith et al., 2018), and cross-\nlingual representation learning (Conneau et al.,\n2020). In addition, research and industry-grade\ntools namely SEACoreNLP26, an open-source ini-\ntiative by NLPHub of AI Singapore, and spaCy\n(Honnibal et al., 2020) include PyThaiNLP as part\nof their toolkit.\n5.2\nPyThaiNLP and Its Industry Impact\nPyThaiNLP is used in many real-world business\nuse cases in firms of all sizes both domestic and\ninternational. User feedback generally highlights\nhow the library has sped up their product devel-\n26https://seacorenlp.aisingapore.net/docs/\nopment cycles involving Thai NLP as well as its\neffectiveness in terms of business outcomes. The\nmost frequently used functionalities are tokeniza-\ntion and text normalization. We introduce here\nselected use cases from national and multinational\nfirms in banking, telecommunication, insurance,\nretail, and software development.\nSiam Commercial Bank (BKK:SCB; USD\n10B market cap) is one of Thailand’s largest banks.\nThe bank operates a chatbot to automatically an-\nswer customer queries. Their data analytics team\nfinetuned WangchanBERTa for intent classification\nto enhance its question-answering capabilities as\nwell as to detect personal information in customers’\ninputs in order to exclude them from their inter-\nnal training sets. Moreover, the team relies on ba-\nsic text processing functions such as tokenization\nand normalization to speed up their development\nprocess. They have also found the published per-\nformance benchmarks to be useful when selecting\nmodels for their tasks.\nTrue Corporation (BKK:TRUE; 6B) is one of\nthe two providers in Thailand’s duopoly telecom-\nmunication market. Its subsidiary, True Digital\nGroup, uses PyThaiNLP both for digital media\nanalysis and for recommendation engine on pro-\nduction. They featurized their Thai-text contents\nusing thai2fit word vectors and saw a noticeable\nuplift in user engagement and subsequent business\noutcomes. They also combined our word vectors\nwith Top2Vec (Angelov, 2020) to perform topic\nmodeling and improve customer experience.\nCentral Retail Digital (BKK:CRC; 6B) is a\ndigital transformation unit serving Central Retail,\nThailand’s largest department store. Their data\nscience team used PyThaiNLP mainly to enhance\nsearch and recommendation offerings across five\nbusiness units and other six million customers.\nWord tokenization and text normalization were\nused to preprocess product information and search\nqueries as input for the product search system.\nSince most search systems are built for languages\nwith white spaces as word delimiters, this prepro-\ncessing step has allowed their product search to\noutperform out-of-the-box solutions which are not\ncompatible with Thai. For content-based recom-\nmendations, the team featurized production infor-\nmation to create a model that recommends similar\nproducts to customers.\nAIA Thailand (HKG:1299; 109B global) is\nthe Thai headquarter of the global insurance firm\nFigure 2: Number of stars PyThaiNLP has received\nfrom GitHub users over the years.\nAmerican Insurance Association. Their data sci-\nence team employs PyThaiNLP in analyzing their\ninbound and outbound call logs using word tok-\nenization, text normalization, stop word handling,\nand local-time-format string handling functional-\nities. For the inbound calls, they normalize and\ntokenize the logs to perform topic modeling and\nidentify critical topics of conversation to emphasize\nboth automated voice bot and human staff training\nand allocation. This resulted in improved percent-\nage of calls that the voice bot fulfilled successfully\nand reduced call waiting time. For the outbound\ncalls, they perform keyword identification from the\nlogs processed by PyThaiNLP to gain insights to\nimprove customer retention.\nVISAI is a VISTEC university spin-off that pro-\nvides machine learning tools and consulting ser-\nvices. It has finetuned WangchanBERTa to per-\nform text classification, named entity recognition,\nand relation extraction on unstructured data of\ntheir clients to create a queryable knowledge graph.\nThey also use tokenization and text normalization\nfunctionalities to facilitate text processing for all\ntheir NLP-based products.\n6\nConclusion and Future Works\nThis paper introduces the PyThaiNLP library, ex-\nplains its features and datasets (as illustrated in\nFigure 1), and discusses the community and the\nengineering project supporting the library.\nBy 2023, we will have implemented the open-\nsource version of most general NLP capabilities\navailable in English for Thai27. We see the follow-\ning items as the next major milestones:\n• Domain-specific datasets/models Some ca-\npabilities are not performing well on specific\n27https://nlpforthai.com/\nuse cases; for instances, named-entity recogni-\ntion in financial reports, medical terms transla-\ntion, and legal documents question answering.\nWe believe more domain-specific datasets and\nmodels will help close this gap.\n• Robust benchmark for Thai NLP tasks As\nNLP has garnered more attention, more mod-\nels and datasets, both open- and closed-source,\nwill be available. It will, therefore, be imper-\native to have a robust benchmark in compar-\ning the models’ performance and the datasets’\nquality.\n• Correctness and consistency Search key gen-\neration (such as Soundex), sorting, and to-\nkenization28 have to be deterministic and\nstrictly follow a specification, or an appli-\ncation may behave in an unexpected fash-\nion. More test cases and verification might\nbe needed for these features.\n• Efficient mechanism to load and manage\ndatasets/models To reduce the size of the li-\nbrary and to carter the use in a system with a\nrestricted network connection29.\n• Seamless\nintegration\nwith\nlanguage-\nagnostic tools The ultimate goal is for\ndevelopers to no longer need PyThaiNLP as\nThai language is supported by standard NLP\nlibraries such as spaCy and Hugging Face\n(Wolf et al., 2020). We have begun this work\nwith integrating our text processing functions\nand models to spaCy.\nAcknowledgements\nFirst and foremost, we appreciate the contribu-\ntions from all PyThaiNLP contributors30.\nWe\nwould like to thank: 1) VISTEC-depa Thailand\nAI Research Institute and its director Sarana Nu-\ntanong for research collaboration and support in\nterms of academic guidance, computational re-\nsources, and personnel; 2) the companies featured\nin the industry impact section and respective inter-\nviewees Chrisada Sookdhis, Jayakorn Vongkulb-\nhisal, Kowin Kulruchakorn, Phasathorn Suwansri,\n28Some phonetic algorithm and transliteration rely on sylla-\nble tokenization\n29https://github.com/PyThaiNLP/pythainlp/\nissues/298\n30https://github.com/PyThaiNLP/pythainlp/\ngraphs/contributors\nand Pongtachchai Panachaiboonpipop; 3) Ekapol\nChuangsuwanich for academic guidance and con-\ntribution to models and datasets; 4) MacStadium\nfor infrastructure support; and 5) NLP-OSS 2023\nanonymous reviewers. We are much obliged to\nfree and open-source software community for soft-\nware building blocks and best practices, including\nbut not limited to NumFOCUS, fast.ai, Hugging\nFace, and Thai Linux Working Group. Moreover,\nwe thank organizations who care enough to de-\nvelop multilingual resources to accommodate low-\nresource languages, most notably Meta AI. Lastly,\nwe cannot thank enough volunteers of various open-\ncontent communities, including Wikipedia, Com-\nmon Voice, TED Translators, and similar local ini-\ntiatives; modern NLP will not be possible without\ntheir accumulated effort.\nLimitations\nIn our current CI workflow, every code commit to\nthe repository triggers an automated test suit for\nall supported platforms. The process can be chal-\nlenging if our package depends on large language\nmodels (LLMs) because a single LLM can exhaust\nthe memory of our free-tier CI infrastructure. Some\nof the components can be cached to reduce build\ntime, but they have to be loaded to the memory in\nany case. This forced us to drop some LLM-related\ntests and scarified the code coverage of the library\nas discussed in Section 4.3.\nEven we have a resource to do such tests with\nthe current design, it is neither economical nor sus-\ntainable. An improved test utilizing a stub, mock,\nor spy (proxy) test pattern that provides an off-line\n“fake inference” can help this. These techniques\nhave been proven useful in other software testing in-\nvolving expensive database/API queries or network\nconnections. Lyra (2019) and Microsoft (2020)\nprovide such examples, using the Python Standard\nLibrary’s unittest.mock. This can reduce a num-\nber of times an LLM is actually being loaded/called.\nThe required inference could be handled either by\na non-free tier CI plan from the same or different\nprovider (which should be more affordable now\ndue to reduced number of calls) or by a computer\noutside the cloud.\nReferences\nRami Al-Rfou. 2015. Polyglot. Available at https:\n//pypi.org/project/polyglot/.\nRami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, and\nSteven Skiena. 2015. POLYGLOT-NER: Massive\nmultilingual named entity recognition. In Proceed-\nings of the 2015 SIAM International Conference on\nData Mining, pages 586–594. SIAM.\nDimo Angelov. 2020. Top2Vec: Distributed representa-\ntions of topics.\nRosana Ardila, Megan Branson, Kelly Davis, Michael\nKohler, Josh Meyer, Michael Henretty, Reuben\nMorais, Lindsay Saunders, Francis Tyers, and Gre-\ngor Weber. 2020. Common Voice: A massively-\nmultilingual speech corpus. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 4218–4222, Marseille, France. European\nLanguage Resources Association.\nWirote Aroonmanakun. 2002. Collocation and Thai\nword segmentation.\nIn Proceedings of the Fifth\nSymposium on Natural Language Processing & The\nFifth Oriental COCOSDA Workshop, pages 68–75,\nPathumthani, Thailand. Sirindhorn International In-\nstitute of Technology.\nWirote Aroonmanakun, Kachen Tansiri, and Pairit Nit-\ntayanuparp. 2009. Thai National Corpus: A progress\nreport. In Proceedings of the 7th Workshop on Asian\nLanguage Resources, ALR7, page 153–158, USA.\nAssociation for Computational Linguistics.\nWirote Aroonmanakun and Attapol Thamrongrattanarit.\n2018. Thai Language Toolkit. Available at https:\n//pypi.org/project/tltk/.\nRatchakrit Arreerard, Stephen Mander, and Scott Piao.\n2022. Survey on Thai NLP language resources and\ntools. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 6495–\n6505, Marseille, France. European Language Re-\nsources Association.\nGann Bierner, Jason Baldridge, Thomas Morton, and\nJoern Kottmann. 2008.\nOpenNLP.\nAvailable at\nhttps://sourceforge.net/projects/opennlp/.\nSteven Bird and Edward Loper. 2004. NLTK: The natu-\nral language toolkit. In Proceedings of the ACL In-\nteractive Poster and Demonstration Sessions, pages\n214–217, Barcelona, Spain. Association for Compu-\ntational Linguistics.\nPrachya Boonkwan, Vorapon Luantangsrisuk, Sitthaa\nPhaholphinyo, Kanyanat Kriengket, Dhanon Leenoi,\nCharun Phrombut, Monthika Boriboon, Krit Kosawat,\nand Thepchai Supnithi. 2020. The annotation guide-\nline of LST20 corpus.\ncakimpei. 2022.\nWunsen.\nAvailable at https://\ngithub.com/cakimpei/wunsen.\nThatsanee Charoenporn, Virach Sornlertlamvanich,\nSawit Kasuriya, Chatchawarn Hansakunbuntheung,\nand Hitoshi Isahara. 2004. Open collaborative devel-\nopment of the Thai language resources for natural\nlanguage processing. In Proceedings of the Fourth In-\nternational Conference on Language Resources and\nEvaluation (LREC’04), Lisbon, Portugal. European\nLanguage Resources Association (ELRA).\nPaisarn Charoenpornsawat. 2003. SWATH: Smart Word\nAnalysis for THai.\nAvailable at http://www.cs.\ncmu.edu/~paisarn/software.html.\nPattarawat Chormai, Ponrawee Prasertsom, Jin Chee-\nvaprawatdomrong, and Attapol Rutherford. 2020.\nSyllable-based neural Thai word segmentation. In\nProceedings of the 28th International Conference\non Computational Linguistics, pages 4619–4637,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nAnanlada\nChotimongkol,\nKwanchiva\nSaykhum,\nPatcharika Chootrakool, Nattanun Thatphithakkul,\nand Chai Wutiwiwatchai. 2009. LOTUS-BN: A Thai\nbroadcast news corpus and its research applications.\nIn 2009 Oriental-COCOSDA International Confer-\nence on Speech Database and Assessments, pages\n44–50, Urumqi, China.\nAnanlada Chotimongkol, Nattanun Thatphithakkul,\nSumonmas Purodakananda, Chai Wutiwiwatchai,\nPatcharika Chootrakool, Chatchawarn Hansakun-\nbuntheung, Atiwong Suchato, and Panuthat Boon-\npramuk. 2010. The development of a large Thai\ntelephone speech corpus: LOTUS-Cell 2.0.\nIn\n2010 Oriental-COCOSDA International Conference\non Speech Database and Assessments, Kathmandu,\nNepal.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nDavid Eberhard, Gary Simons, and Chuck Fennig. 2023.\nEthnologue: Languages of the World. Twenty-sixth\nedition. SIL International.\nRichard Gillam. 1999. Text boundary analysis in Java.\nIn Proceedings of Fifteenth International Unicode\nConference, San Jose, California, USA.\nChoochart Haruechaiyasak and Sarawoot Kongyoung.\n2009. TLex: Thai lexeme analyser based on the con-\nditional random fields. In Proceedings of 8th Interna-\ntional Symposium on Natural Language Processing,\nBangkok, Thailand.\nChoochart Haruechaiyasak, Sarawoot Kongyoung, and\nMatthew Dailey. 2008.\nA comparative study on\nThai word segmentation approaches. In 2008 5th\nInternational Conference on Electrical Engineer-\ning/Electronics, Computer, Telecommunications and\nInformation Technology, volume 1, pages 125–128.\nMatthew Honnibal. 2013. A good part-of-speech tagger\nin about 200 lines of Python.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nYuphaphann Hoonchamlong, Sathaporn Koraksawet,\nSarawuth Keawbumrung, and Krissadang Klaijinda.\n1997. Thai Language Audio Resource Center.\nJeremy Howard and Sylvain Gugger. 2020.\nfastai:\nA Layered API for Deep Learning.\nInformation,\n11(2):108.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nIBM Corporation et al. 1999. International Components\nfor Unicode. Available at https://icu.unicode.\norg.\nSawit Kasuriya, Virach Sornlertlamvanich, Patcharika\nCotsomrong, Takatoshi Jitsuhiro, Genichiro Kikui,\nand Yoshinori Sagisaka. 2003a. NECTEC-ATR Thai\nspeech corpus. In 2003 Oriental-COCOSDA Interna-\ntional Conference on Speech Database and Assess-\nments, pages 105–111, Singapore.\nSawit Kasuriya, Virach Sornlertlamvanich, Patcharika\nCotsomrong, Supphanat Kanokphara, and Nattanun\nThatphithakkul. 2003b.\nThai speech corpus for\nspeech recognition. In 2003 Oriental-COCOSDA\nInternational Conference on Speech Database and\nAssessments, pages 54–61, Singapore.\nAsanee Kawtrakul, Mukda Suktarachan, Patcharee Vara-\nsai, and Hutchatai Chanlekha. 2002. A state of the\nart of Thai language resources and Thai language\nbehavior analysis and modeling. In COLING-02:\nThe 3rd Workshop on Asian Language Resources and\nInternational Standardization.\nNatthawut Kertkeidkachorn, Supadaech Chanjarad-\nwichai, Teera Suri, Krerksak Likitsupin, Surapol Vo-\nrapatratorn, Pawanrat Hirankan, Worasa Limpanadu-\nsadee,\nSupakit Chuetanapinyo,\nKitanan Pitak-\npawatkul, Natnarong Puangsri, Nathacha Tangsirirat,\nKonlawachara Trakulsuk, Proadpran Punyabukkana,\nand Atiwong Suchato. 2012. The CU-MFEC corpus\nfor Thai and English spelling speech recognition. In\nProceedings of International Conference on Speech\nDatabase and Assessments, pages 18–23.\nKrit Kosawat. 2009. InterBEST 2009: Thai word seg-\nmentation workshop. In Proceedings of 8th Interna-\ntional Symposium on Natural Language Processing,\nBangkok, Thailand.\nKrit\nKosawat,\nMonthika\nBoriboon,\nPatcharika\nChootrakool,\nAnanlada\nChotimongkol,\nSupon\nKlaithin, Sarawoot Kongyoung, Kanyanut Kriengket,\nSitthaa Phaholphinyo, Sumonmas Purodakananda,\nTipraporn Thanakulwarapas, and Chai Wutiwi-\nwatchai. 2009. BEST 2009: Thai word segmentation\nsoftware contest.\nIn 2009 Eighth International\nSymposium on Natural Language Processing, pages\n83–88.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\nPeerat Limkonchotiwat, Wannaphong Phatthiyaphai-\nbun, Raheem Sarwar, Ekapol Chuangsuwanich, and\nSarana Nutanong. 2021. Handling cross- and out-\nof-domain samples in Thai word segmentation. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 1003–1016, On-\nline. Association for Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot learning with\nmultilingual generative language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9019–9052,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nVichit Lorchirachoonkul. 1982. A Thai soundex system.\nInformation Processing & Management, 18(5):243–\n255.\nLalita Lowphansirikul, Charin Polpanumas, Nawat\nJantrakulchai,\nand\nSarana\nNutanong.\n2021a.\nWangchanBERTa:\npretraining transformer-based\nThai language models.\nLalita Lowphansirikul, Charin Polpanumas, Attapol T.\nRutherford, and Sarana Nutanong. 2021b. A large\nEnglish–Thai parallel corpus from the web and\nmachine-generated text. Language Resources and\nEvaluation, 56(2):477–499.\nMatti Lyra. 2019. Effective mocking of unit tests for\nmachine learning.\nThomas J. McCabe. 1976.\nA complexity measure.\nIEEE Transactions on Software Engineering, SE-\n2(4):308–320.\nSurapant Meknavin, Paisarn Charoenpornsawat, and\nBoonserm Kijsirikul. 1997. Feature-based Thai Word\nSegmentation. In Proceedings of the Natural Lan-\nguage Processing Pacific Rim Symposium, Phuket,\nThailand.\nMicrosoft. 2020. Testing data science and MLOps code.\nmmb L. 2018.\nsymspellpy.\nAvailable at https://\ngithub.com/mammothb/symspellpy.\nNational Electronics and Computer Technology Center.\n2006. Thai Lexeme Tokenizer: LexTo. [online]. Re-\ntrieved August 8, 2023, from http://www.sansarn.\ncom/lexto/.\nDat Quoc Nguyen, Dai Quoc Nguyen, Dang Duc Pham,\nand Son Bao Pham. 2014. RDRPOSTagger: A ripple\ndown rules-based part-of-speech tagger. In Proceed-\nings of the Demonstrations at the 14th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 17–20, Gothenburg, Swe-\nden. Association for Computational Linguistics.\nPeter Norvig. 2007. How to write a spelling corrector.\nTerry Peng and Mikhail Korobov. 2014. python-crfsuite.\nAvailable at https://github.com/scrapinghub/\npython-crfsuite.\nWannaphong Phatthiyaphaibun. 2022. Thai NER 2.0.\nWannaphong Phatthiyaphaibun and Peerat Limkonchoti-\nwat. 2023. Han-Coref: Thai coreference resolution\nby PyThaiNLP.\nMikhail Plekhanov, Nora Kassner, Kashyap Popat,\nLouis Martin, Simone Merello, Borislav Kozlovskii,\nFrédéric A. Dreyer, and Nicola Cancedda. 2023. Mul-\ntilingual end to end entity linking.\nCharin Polpanumas and Wannaphong Phatthiyaphaibun.\n2021. thai2fit: Thai language implementation of\nULMFiT.\nCharin Polpanumas, Wannaphong Phatthiyaphaibun,\nPatomporn\nPayoungkhamdee,\nPeerat\nLimkon-\nchotiwat,\nLalita\nLowphansirikul,\nCan\nUdom-\ncharoenchaikit,\nTitipat\nAchakulwisut,\nEkapol\nChuangsuwanich, and Sarana Nutanong. 2023.\nWangChanGLM – the multilingual instruction-\nfollowing model.\nCharin Polpanumas and Phasathorn Suwansri. 2020.\nPythainlp/classification-benchmarks: v0.1-alpha.\nPyCon Thailand. 2019. How PyThaiNLP’s thai2fit out-\nperforms Google’s BERT: State-of-the-art Thai text\nclassification and beyond - Charin.\nVee Satayamas. 2015. wordcutpy. Available at https:\n//github.com/veer66/wordcutpy.\nAaron Smith, Bernd Bohnet, Miryam de Lhoneux,\nJoakim Nivre, Yan Shao, and Sara Stymne. 2018. 82\ntreebanks, 34 models: Universal Dependency parsing\nwith multi-treebank models. In Proceedings of the\nCoNLL 2018 Shared Task: Multilingual Parsing from\nRaw Text to Universal Dependencies, pages 113–123,\nBrussels, Belgium. Association for Computational\nLinguistics.\nChakkrit Snae and Michael Brückner. 2009.\nNovel\nphonetic name matching algorithm with a statistical\nontology for analysing names given in accordance\nwith Thai astrology. Issues in Informing Science and\nInformation Technology, 6:497–515.\nVirach Sornlertlamvanich. 1993. Machine Translation,\nchapter Word segmentation for Thai in machine trans-\nlation system. National Electronics and Computer\nTechnology Center.\nVirach Sornlertlamvanich, Tanapong Potipiti, Chai Wu-\ntiwiwatchai, and Pradit Mittrapiyanuruk. 2000. The\nstate of the art in Thai language processing. In Pro-\nceedings of the 38th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 1–2, Hong\nKong. Association for Computational Linguistics.\nVirach Sornlertlamvanich, Naoto Takahashi, and Hi-\ntoshi Isahara. 1999. Building a Thai part-of-speech\ntagged corpus (ORCHID). Journal of the Acoustical\nSociety of Japan (E), 20(3):189–198.\nSutee Sudprasert and Asanee Kawtrakul. 2003. Thai\nword segmentation based on global and local unsu-\npervised learning.\nIn Proceedings of the 7th Na-\ntional Computer Science and Engineering Confer-\nence, pages 1–8, Chonburi, Thailand.\nThepchai Supnithi, Krit Kosawat, Monthika Boriboon,\nand Virach Sornlertlamvanich. 2004. Language sense\nand ambiguity in Thai. In Proceedings of the 8th\nPacific Rim International Conference on Artificial\nIntelligence, Auckland, New Zealand.\nPrayut Suwanvisat and Somboon Prasitjutrakul. 1998.\nThai-English cross-language transliterated word re-\ntrieval using soundex technique. In Proceesings of\nthe National Computer Science and Engineering Con-\nference, Bangkok, Thailand.\nThai\nLinux\nWorking\nGroup.\n2001.\nLibThai.\nAvailable at https://linux.thai.net/projects/\nlibthai/.\nThanaruk Theeramunkong, Virach Sornlertlamvanich,\nThanasan Tanhermhong, and Wirat Chinnan. 2000.\nCharacter cluster based Thai information retrieval.\nIn Proceedings of the Fifth International Workshop\non on Information Retrieval with Asian Languages,\nIRAL ’00, page 75–80, New York, NY, USA. Asso-\nciation for Computing Machinery.\nWannee Udompanich. 1983. String searching for Thai\nalphabet using Soundex compression technique.\nVISTEC-depa AI Research Institute of Thailand. 2023.\nwav2vec2-large-xlsr-53-th (revision 3155938).\nDavid Gray Widder, Sarah West, and Meredith Whit-\ntaker. 2023. Open (for business): Big tech, concen-\ntrated power, and the political economy of open AI.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nDavid Wright. 2021. Phunspell. Available at https:\n//github.com/dvwright/phunspell.\nChai Wutiwiwatchai and Sadaoki Furui. 2007. Thai\nspeech processing technology: A review. Speech\nCommunication, 49(1):8–27.\n",
  "categories": [
    "cs.CL",
    "I.2.7"
  ],
  "published": "2023-12-07",
  "updated": "2023-12-07"
}