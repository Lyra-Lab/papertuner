{
  "id": "http://arxiv.org/abs/2312.05519v1",
  "title": "Isomorphic-Consistent Variational Graph Auto-Encoders for Multi-Level Graph Representation Learning",
  "authors": [
    "Hanxuan Yang",
    "Qingchao Kong",
    "Wenji Mao"
  ],
  "abstract": "Graph representation learning is a fundamental research theme and can be\ngeneralized to benefit multiple downstream tasks from the node and link levels\nto the higher graph level. In practice, it is desirable to develop\ntask-agnostic general graph representation learning methods that are typically\ntrained in an unsupervised manner. Related research reveals that the power of\ngraph representation learning methods depends on whether they can differentiate\ndistinct graph structures as different embeddings and map isomorphic graphs to\nconsistent embeddings (i.e., the isomorphic consistency of graph models).\nHowever, for task-agnostic general graph representation learning, existing\nunsupervised graph models, represented by the variational graph auto-encoders\n(VGAEs), can only keep the isomorphic consistency within the subgraphs of 1-hop\nneighborhoods and thus usually manifest inferior performance on the more\ndifficult higher-level tasks. To overcome the limitations of existing\nunsupervised methods, in this paper, we propose the Isomorphic-Consistent VGAE\n(IsoC-VGAE) for multi-level task-agnostic graph representation learning. We\nfirst devise a decoding scheme to provide a theoretical guarantee of keeping\nthe isomorphic consistency under the settings of unsupervised learning. We then\npropose the Inverse Graph Neural Network (Inv-GNN) decoder as its intuitive\nrealization, which trains the model via reconstructing the GNN node embeddings\nwith multi-hop neighborhood information, so as to maintain the high-order\nisomorphic consistency within the VGAE framework. We conduct extensive\nexperiments on the representative graph learning tasks at different levels,\nincluding node classification, link prediction and graph classification, and\nthe results verify that our proposed model generally outperforms both the\nstate-of-the-art unsupervised methods and representative supervised methods.",
  "text": "Isomorphic-Consistent Variational Graph Auto-Encoders\nfor Multi-Level Graph Representation Learning\nHanxuan Yang1,2, Qingchao Kong2,1, Wenji Mao2,1\n1School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China\n2Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China\n{yanghanxuan2020, qingchao.kong, wenji.mao}@ia.ac.cn\nAbstract\nGraph representation learning is a fundamental research\ntheme and can be generalized to benefit multiple downstream\ntasks from the node and link levels to the higher graph\nlevel. In practice, it is desirable to develop task-agnostic gen-\neral graph representation learning methods that are typically\ntrained in an unsupervised manner. Related research reveals\nthat the power of graph representation learning methods de-\npends on whether they can differentiate distinct graph struc-\ntures as different embeddings and map isomorphic graphs\nto consistent embeddings (i.e., the isomorphic consistency\nof graph models). However, for task-agnostic general graph\nrepresentation learning, existing unsupervised graph models,\nrepresented by the variational graph auto-encoders (VGAEs),\ncan only keep the isomorphic consistency within the sub-\ngraphs of 1-hop neighborhoods and thus usually manifest in-\nferior performance on the more difficult higher-level tasks. To\novercome the limitations of existing unsupervised methods,\nin this paper, we propose the Isomorphic-Consistent VGAE\n(IsoC-VGAE) for multi-level task-agnostic graph representa-\ntion learning. We first devise a decoding scheme to provide a\ntheoretical guarantee of keeping the isomorphic consistency\nunder the settings of unsupervised learning. We then propose\nthe Inverse Graph Neural Network (Inv-GNN) decoder as its\nintuitive realization, which trains the model via reconstruct-\ning the GNN node embeddings with multi-hop neighborhood\ninformation, so as to maintain the high-order isomorphic con-\nsistency within the VGAE framework. We conduct extensive\nexperiments on the representative graph learning tasks at dif-\nferent levels, including node classification, link prediction\nand graph classification, and the results verify that our pro-\nposed model generally outperforms both the state-of-the-art\nunsupervised methods and representative supervised methods\ndesigned for individual tasks.\nIntroduction\nGraph representation learning aims to map the topology\nstructures and node attribute features of relational data into\na low-dimensional embedding space. Based on different\ngranularities of the embeddings (a.k.a. representations), the\ngraph representation learning tasks have been extensively\nstudied by the graph neural networks (GNNs) at node level\n(Kipf and Welling 2017; Veliˇckovi´c et al. 2018; Zeng et al.\n2020), link level (Zhang and Chen 2018; Zhang et al. 2021)\nand graph level (Xu et al. 2019). However, these deep\nlearning-based methods usually employ an end-to-end su-\npervised training paradigm and can only learn graph repre-\nsentations for specific downstream tasks. In contrast, there\nare many situations where the task-agnostic representations\nare preferred and/or the label information is unavailable.\nThus it is desirable to leverage the more general graph rep-\nresentation learning methods that are typically trained in an\nunsupervised manner.\nThe graph representation learning problem requires the\nmodel to learn embeddings with increasingly complex\nstructural information. Related research has shown that\nthe power of graph representation learning methods can\nbe characterized as to which extent they can solve the\ngraph isomorphism problem (Grohe and Schweitzer 2020),\nthat is, whether they can differentiate distinct graph struc-\ntures as different embeddings and map topologically identi-\ncal (a.k.a., isomorphic) graphs into consistent embeddings,\nwhich we refer to as the isomorphic consistency of graph\nmodels in this paper.\nExisting unsupervised graph representation learning\nmethods are typically based on the deep generative models,\nrepresented by the variational graph auto-encoders (VGAEs)\n(Kipf and Welling 2016). These methods leverage a GNN-\nbased encoder to aggregate the neighbor features as node\nembeddings. These embeddings can be applied to multiple\ndownstream tasks (Bojchevski et al. 2018; Grover, Zweig,\nand Ermon 2019; Li et al. 2023), as they are optimized in\nan unsupervised manner by reconstructing the edges of an\nadjacency matrix in the decoder, rather than using any task-\nspecific label. However, since the graph edges only contain\nthe 1-hop neighborhood information of each node, the em-\nbeddings learned by these VGAE-based methods can only\nkeep the low-order isomorphic consistency, which is insuffi-\ncient to learn distinguishable node embeddings for the more\ndifficult higher-level tasks. For example, in Fig. 1, nodes\nv1 and v2 should have different embeddings as they are at\nasymmetric positions in the global graph. However, if only\n1-hop neighborhood subgraphs are considered, nodes v1\nand v2 will have the same local structural information, thus\nthey may learn consistent embeddings by mistake. There-\nfore, the capacity of VGAEs is hampered due to the lack\nof high-order isomorphic consistency, especially for graph-\nlevel tasks such as graph classification.\nPrevious research has developed several GNNs (Kipf and\nWelling 2017; Hamilton, Ying, and Leskovec 2017; Xu et al.\narXiv:2312.05519v1  [cs.LG]  9 Dec 2023\n1-Hop Neighborhood\n2-Hop Neighborhood\n1v\n2v\n2v\n1v\n1v1v\n2v2v\n1v\n2v\n1v\n2v\nFigure 1: In this graph, nodes v1 and v2 have non-isomorphic\nglobal structures. To differentiate them as different embed-\ndings, we should consider at least the 2-hop neighborhood\ninformation (bottom), since they share the same local struc-\ntures within the 1-hop neighborhood (top).\n2019; Zhang et al. 2021) that are isomorphic-consistent\nto different extents within the sense of the 1-dimensional\nWeisfeiler-Lehman test (Leman and Weisfeiler 1968). These\nmethods are capable of aggregating multi-hop neighborhood\ninformation as node embeddings according to graph topol-\nogy in supervised learning. Intuitively, this inspires us to\nmodify the decoder of VGAEs by reconstructing the GNN\nembeddings, instead of the adjacency matrix, so as to pre-\nserve the isomorphic consistency property of GNNs. To this\nend, we need to solve the challenging issues of identify-\ning the theoretical criterion to ensure high-order isomorphic\nconsistency of the decoder and developing computational\nmeans to maintain this property by node embedding recon-\nstruction under unsupervised learning settings.\nTo address the above issues and overcome the limita-\ntions of VGAEs, in this paper, we propose the Isomorphic-\nConsistent VGAE (IsoC-VGAE) for multi-level graph rep-\nresentation learning. We first design a decoding scheme to\nprovide a theoretical basis for keeping the high-order iso-\nmorphic consistency of the GNN-based encoder under un-\nsupervised settings. We then develop a novel Inverse GNN\n(Inv-GNN) decoder as an instantiation of our proposed de-\ncoding scheme, which does the opposite work of the GNN-\nbased encoder, that is, to reconstruct the self-embedding and\nneighborhood distribution of each node using the aggregated\nGNN embeddings. In this way, the high-order isomorphic\nconsistency of GNNs can be properly preserved in the de-\ncoder by learning the multi-hop neighborhood information\nfrom the embeddings of the encoder. We conduct extensive\nexperiments on the representative graph learning tasks at dif-\nferent levels, including node classification, link prediction\nand graph classification, and the results verify the effective-\nness of our proposed model.\nThe main contributions of this work are as follows:\n• To tackle multi-level task-agnostic graph representation\nlearning, we first propose the decoding scheme within\nthe VGAE framework to theoretically ensure high-order\nisomorphic consistency in unsupervised learning.\n• We develop a novel Inv-GNN decoder as a realization of\nthe proposed decoding scheme, which trains the model\nvia reconstructing the self-embeddings and neighbor-\nhood distributions learned by the GNN-based encoder.\n• Experimental results on representative tasks at node, link\nand graph levels show that our model generally outper-\nforms the state-of-the-art unsupervised and representa-\ntive supervised graph representation learning methods.\nPreliminaries\nIn this section, we briefly review two important graph rep-\nresentation learning methods that are used in our proposed\nmodel, namely the GNNs and VGAEs.\nGraph Neural Networks\nGNNs are deep learning-based methods for graph repre-\nsentation learning. They perform neighbor aggregation to\nlearn node embeddings based on the graph topology and\nare trained in an end-to-end manner. Specifically, the node\nembeddings of the l-th GNN layer h(l)\ni\nare obtained as a\nweighted summation of the neighbor embeddings, i.e.,\nh(l+1)\ni\n= f(W(l)(h(l)\ni\n+\nX\nj∈Ni\nω(l)\nij h(l)\nj )),\n(1)\nwhere f(·) is a nonlinear transformation function, Ni de-\nnotes the neighbor set of node i, and W(l) is a trainable\nweight matrix. The embedding of each neighbor is weighted\nby ωij, which is employed as, for example, the normal-\nized graph Laplacian in the graph convolutional networks\n(GCNs) (Kipf and Welling 2017), and the masked attention\nweight in the graph attention networks (GATs) (Veliˇckovi´c\net al. 2018).\nVariational Graph Auto-Encoders\nVGAEs are among the most well-known and powerful unsu-\npervised graph representation learning methods. They usu-\nally consist of a GNN-based encoder to learn node embed-\ndings and a reconstruction decoder to train the model with-\nout any task-specific label. The vanilla VGAE (Kipf and\nWelling 2016) generates node embeddings from Normal dis-\ntributions and reconstructs the adjacency matrix A via the\ninner product, i.e.,\nˆAij = z⊤\ni zj,\nzi ∼Normal(µi, Iσ2\ni ),\n(2)\nwhere ˆA = (ˆAij) denotes the reconstructed adjacency ma-\ntrix, I is an identity matrix, µi and σi are parameterized\nby a GCN encoder. For optimization, VGAEs minimize the\nnegative evidence lower bound (ELBO), which can be for-\nmulated as the combination of a reconstruction loss and a\nKullback-Leibler (KL) divergence, i.e.,\nLvgae = Lrec(A, ˆA) +\nX\ni\nKL[q(zi|hi)∥p(zi)],\n(3)\nwhere KL[·∥·] denotes the KL divergence, p(·) and q(·) are\nthe prior and variational posterior distributions, respectively.\nIsomorphic-Consistent Decoding Scheme\nIn this section, we first provide the formal definition of task-\nagnostic general graph representation learning, and then pro-\npose a decoding scheme within the VGAE framework to\nlearn powerful and general graph representations for multi-\nlevel tasks.\nConsidering an N node undirected graph G = (V, E) and,\nif available, an attribute feature matrix X = (x1, . . . , xN)⊤,\nwhere V = {v1, . . . , vN} and E = {e1, . . . , eN} are the sets\nof nodes and edges, respectively, the general graph repre-\nsentation learning is to train an unsupervised graph model\nM to map the graph structure into a low-dimensional em-\nbedding space without any task-specific label, i.e., M :\n(G, X) →Z, where Z = (z1, . . . , zN)⊤is the node em-\nbedding matrix. In the rest of this section, we shall omit the\nattribute features X for simplification, since we mainly focus\non learning the structural information of nodes.\nTo explore the expressive power of an unsupervised graph\nmodel for multi-level general graph representation learning,\nwe first give the definition of graph isomorphism.\nDefinition 1. Given two graphs G = (V, E) and G′ =\n(V′, E′) with N nodes, ΠV′ is the permutation set includ-\ning all m! possible index permutations of the nodes in V′.\nGraphs G and G′ are isomorphic, denoted as G ≃G′, if\n∃πV′ ∈ΠV′ such that V = π(V′) and E = π(E′), where\nπV′(·) indicates to reorder the nodes in V′.\nIntuitively, the most powerful graph models should ensure\nthat all non-isomorphic graphs or subgraphs formed by node\nand edge subsets always have distinct embeddings, and iso-\nmorphic graphs or subgraphs always have consistent embed-\ndings. Next, we introduce the concept of isomorphic consis-\ntency to evaluate the expressive power of a graph model.\nDefinition 2. For graphs G\n= (V, E), G′\n= (V′, E′)\nand their node embeddings Z, Z′ learned by model M,\nS ⊆V, S′ ⊆V′ are two node subsets with m nodes,\nG(k)\nS\n= (V(k)\nS , E(k)\nS ), G′(k)\nS′ = (V′(k)\nS′ , E′(k)\nS′ ) are subgraphs\nof G and G′ that consist of the k-hop neighbors of the nodes\nin S and S′, k ≥1, and ZS, Z′\nS′ are the embeddings of the\nnode subsets. Model M is k-order isomorphic-consistent\nif ∀S ⊆V, S′ ⊆V′, ZS = Z′\nS′ ⇔G(k)\nS\n≃G′(k)\nS′ .\nThe node subsets S and S′ can vary according to the ob-\njective task levels. For node-level tasks, m = 1, for link-\nlevel tasks, m = 2, and for graph-level tasks, m = N.\nThe VGAEs can only ensure the 1-order isomorphic con-\nsistency since they learn node embeddings by reconstruct-\ning the edges of an adjacency matrix. Specifically, given Z∗\nas the optimal embeddings learned by VGAEs that can per-\nfectly reconstruct all edges, Z∗can only guarantee the dis-\ntinguishability of node subsets with different structural in-\nformation in their 1-hop neighborhood subgraphs, since the\ngraph edges merely contain the 1-hop neighborhood infor-\nmation of each node.\nIn addition, for graph-level tasks, the permutations of\nnode indices should also be concerned since the node sets\nS and S′ come from different graphs. Definition 2 implies\nthat the isomorphic-consistent models should learn node em-\nbeddings that contain no node permutation information but\nonly the topological structure of graphs. In other words, the\nisomorphic-consistent models should be able to map two\nisomorphic graphs with different node permutations into\nconsistent embeddings. Formally, we give the definition of\npermutation invariance for unsupervised graph models as\nfollows.\nDefinition 3. Letting Z be the node embeddings of graph\nG = (V, E) learned by model M, Menc, Mdec and Lrec\nbe the encoder, decoder and reconstruction loss function\nof M, respectively, model M is permutation-invariant to\nnode orders if ∀πV ∈ΠV, Menc(G) = Menc(πV(G)),\nLrec(Mdec(Z)) = Lrec(Mdec(πV(Z))).\nMost VGAE-based methods employ GNNs as the encoder\nMenc, which keep the permutation invariance in nature\nby aggregating node neighbors according to graph topol-\nogy (Yang et al. 2019). However, the decoder Mdec of\nthese methods are typically not permutation-invariant since\nthey use the reconstruction error between the input adja-\ncency matrix A and reconstructed adjacency matrix A′ as\nthe loss Lrec for training, while ∃πV ∈ΠV, Lrec(A, A′) ̸=\nLrec(A, πV(A′)). The lack of permutation invariance would\nwaste the capacity of VGAEs in capturing the meaningless\nnode permutations, instead of the underlying graph structure\n(Yang et al. 2019).\nTo leverage the VGAE framework for multi-level general\ngraph representation learning, the main challenge is how to\nkeep the high-order isomorphic consistency as well as the\npermutation invariance of the embeddings learned by the\nGNN-based encoder, while training the model under the set-\ntings of unsupervised learning in the decoder. The following\nproposition provides a decoding scheme for unsupervised\ngraph representation learning methods to learn general em-\nbeddings that are theoretically as powerful as those learned\nby GNNs in terms of keeping the isomorphic consistency.\nProposition 1. Given a graph G with attribute features\nX, an unsupervised representation learning model M :\n(G, X) →Z and an L-layer GNN : (G, X) →H(L), model\nM is L-order isomorphic-consistent if the following condi-\ntions hold:\na) The GNN is L-order isomorphic-consistent.\nb) H(L) = f(Z), where f(·) is an injective function.\nProof.\nWe start the proof with a simple case of the propo-\nsition, where the GNN only has one layer, i.e., L = 1. With\ncondition a), we have, ∀S ⊆V, S′ ⊆V′,\nZS = Z′\nS′ ⇔H(1)\nS\n= H(1)\nS′\n⇔GNNS(GS1, H(0)\nS1 ) = GNNS′(G′\nS′\n1, H′(0)\nS′\n1 ),\n(4)\nwhere the subscript of the model GNNS(·) indicates the out-\nput embeddings of the nodes in S. Then, because the GNN\nis isomorphic-consistent (condition b)), we have\nZS = Z′\nS′ ⇔GS1 ≃G′\nS′\n1.\n(5)\nNow we consider the multi-layer case. With condition a),\nwe have\nZS = Z′\nS′ ⇔H(L)\nS\n= H′(L)\nS′ .\n(6)\nLetting GNN(l)\nS (·), l = 1, . . . , L denote the l-th layer of the\nGNN model, H(L)\nS\ncan be decomposed as\nH(L)\nS\n=GNN(L)\nS (GS1, H(L−1)\nS1\n)\n=GNN(L)\nS (GS1, GNN(L−1)\nS1\n(GS2, H(L−2)\nS2\n))\n=GNN(L)\nS (GS1, GNN(L−1)\nS1\n(\n· · · GNN(1)\nSL−1(GSL, H(0)\nSL) · · · ))\n=GNNS(GSL, H(0)\nSL).\n(7)\nSimilarly,\nH′(L)\nS′ =GNN(L)\nS′ (G′\nS′\n1, GNN(L−1)\nS′\n1\n(\n· · · GNN(1)\nS′\nL−1(G′\nS′\nL, H′(0)\nS′\nL) · · · ))\n=GNNS′(G′\nS′\nL, H′(0)\nS′\nL).\n(8)\nTherefore, for an L-layer GNN, the receptive fields of the\nnode sets S and S′ are the subgraphs GSL and G′\nS′\nL, which\ninclude the L-hop neighborhoods of all nodes in S and S′,\nrespectively. Further, with condition b), we have\nGNNS(GSL, H(0)\nSL) = GNNS′(G′\nS′\nL, H′(0)\nS′\nL) ⇔GSL ≃G′\nS′\nL.\n(9)\nBy combining Eq. (6)-(9), we get the desired result.\nThe above decoding scheme enables us to build a VGAE-\nbased model where the decoder can maintain the high-order\nisomorphic consistency (and so the permutation invariance)\nof the GNN encoder, as long as the GNN for the encoder\ncan the embeddings Z learned by the decoder can conform\nto conditions a) and b) in Proposition 1, respectively.\nFor condition a), several GNNs have been proposed that\nare isomorphic-consistent to different extents, e.g., SEAL\n(Zhang and Chen 2018) and the graph isomorphism network\n(GIN) (Xu et al. 2019). To build decoders that can ensure\ncondition b), one may use a more strict case where the injec-\ntive function is constrained as the identity map. In the next\nsection, we shall propose an instantiation of the decoders\nthat can conform to condition b) by reconstructing the self-\nembeddings as well as the neighborhood distributions of the\nembeddings learned by the GNN encoder. Note that it is not\nthe only choice based on the decoding scheme in Proposi-\ntion 1, yet a simple and intuitive realization of our concern.\nProposed Method\nTo learn general graph representations for multi-level tasks,\nwe propose a generative graph model within the VGAE\nframework, i.e., the Isomorphic-Consistent VGAE (IsoC-\nVGAE), illustrated in Fig. 2. Similar to vanilla VGAEs, the\nencoder employs GNNs to aggregate the multi-hop neigh-\nborhood information as node embeddings. Below we shall\nmainly introduce the decoder of our model.\nInverse GNN Decoder\nInspired by GNN layers which collect the self- and neigh-\nbor embeddings of each node as the aggregated embedding\nΑ\nX\n(3)\nΗ\nGNN Encoder\nInv-GNN Decoder\n(1)\nH\n(2)\nH\n(1)\nΖ\n(0)\nZ\n(2)\nΖ\nFigure 2: An illustration of the proposed IsoC-VGAE frame-\nwork with three GNN layers. The black arrows indicate the\nfeedforward propagation passes to learn node embeddings.\nThe red dashed arrows indicate the reconstruction passes to\ntrain the model.\nbased on graph topology, we propose a novel inverse GNN\n(Inv-GNN) decoder to train our model using the opposite\nmessage-passing mechanism of GNNs, that is, reconstruct-\ning the self-embeddings as well as neighborhood distribu-\ntions using the aggregated node embeddings, as presented in\nFig. 3.\nSelf-Embedding Reconstruction\nFollowing the vanilla\nVGAEs, we leverage the reparameterization trick (Kingma\nand Welling 2014) and generate the self-embeddings from\nNormal distributions parameterized by neural networks. For-\nmally, for l = 0, . . . , L −1, where L is the number of lay-\ners in the GNN encoder, the reconstructed self-embeddings\nz(l)\ni\n∈RCl of the l-th layer are obtained as\nz(l)\ni\n∼Normal(˜µ(l)\ni\n+ µ(l)\ni , IClσ(l)\ni\n2),\n(10)\nwhere the variational posterior parameters are obtained us-\ning feed-forward neural networks (FNNs), i.e.,\nµ(l)\ni\n=FNNµ(h(l+1)\ni\n),\n(11)\nσ(l)\ni\n=exp(FNNσ(h(l+1)\ni\n)),\n(12)\n˜µ(l)\ni\ndenotes the prior mean parameter set as FNNz(z(l+1)\ni\n)\nfor l = 0, . . . , L−2 and zero for l = L −1, and ICl denotes\nthe Cl-dimensional identity matrix.\nThen, the self-embedding reconstruction loss is defined as\nLself =\nL−1\nX\nl=0\nn\nX\ni=1\n∥h(l)\ni\n−z(l)\ni ∥2\n2,\n(13)\nwhere ∥· ∥2 denotes the L2 norm, h(0)\ni\nis xi if the node\nattribute features are available and a column vector of an\nidentity matrix otherwise.\naggregation\n…\nself-\nembedding\n( )l\nih\n1\n( )l\njh\n2\n( )l\njh\n(\n1)\nl\nih\n+\naggregated \nembedding\nGNN\nInv-GNN\naggregated \nembedding\nneighbor \nembeddings\nself-\nembedding\n(\n1)\nl\nih\n+\n( )l\niz\n( )l\ni\n( )l\ni\n\nneighborhood \ndistribution\nreconstruction\n( )\nk\nl\njh\nFigure 3: The message-passing mechanisms of a GNN (left)\nand the proposed Inv-GNN (right). A GNN layer aggregates\nthe self-embedding and the k neighbor embeddings into the\nnext layer. An Inv-GNN layer uses the aggregated GNN em-\nbedding to reconstruct its self-embedding and neighborhood\ndistribution of the previous layer.\nNeighborhood Reconstruction\nThe reconstruction of\nnode neighbors is much more challenging, because the ag-\ngregated embeddings will inevitably lose some information\nfrom the neighbor embeddings during the pooling process,\nand the numbers of neighbors for different nodes may vary\nin a wide range. To this end, instead of reconstructing the\nspecific values of the neighbor embeddings, our model at-\ntempts to learn the empirical distribution of each node neigh-\nborhood. Specifically, regarding each neighbor as an i.i.d.\nsample of the neighborhood distribution, the reconstruction\nof node neighbors can be transformed into inferring the pa-\nrameters of the distribution as well as the number of samples\n(a.k.a., node degrees).\nIn our model, we learn the neighborhood distribution us-\ning the KL divergence between the variational posteriors of\nthe reconstructed embeddings q(z(l)\ni |h(l+1)\ni\n) and the poste-\nriors of the GNN embeddings p(h(l)\ni |X), and the reconstruc-\ntion loss of neighborhood distribution is defined as\nLnei =\nL−1\nX\nl=0\nn\nX\ni=1\n\u0012\nKL[q(z(l)\ni |h(l+1)\ni\n)∥p(h(l)\ni |X)]\n\u0013\n.\n(14)\nHere the KL divergence is only determined by the param-\neters of distributions but not related to the specific values\nof embedding samples. Thus, the proposed neighbor re-\nconstruction strategy can avoid matching the permutation\nof the reconstructed neighbor embeddings to that of h(l)\nj ,\nwhich is not only computationally intractable but also non-\npermutation-invariant to node orders.\nThe posterior distribution of GNN embeddings p(h(l)\ni |X)\nis assumed as a Normal distribution, of which the mean\nand variance parameters can be calculated as moments of\nthe GNN embeddings within a neighborhood (including the\nself-embedding). In practice, we use a degenerate variance\n(i.e., the identity matrix) for the GNN embedding posterior,\nsince We empirically find that such slight simplification of\nthe neighborhood reconstruction hardly has any impact on\nAlgorithm 1: Training IsoC-VGAE\nInput: Undirected graph G with N nodes; node feature X\nParameter: Layer number L; neighbor tuning λnei; degree\ntuning λdeg; learning rate κ; other hyperparameters\nOutput: Node Embeddings H(1:L)\n1: Initialize weight parameters W\n2: H(0) = X\n3: ˜µ(L−1) = 0\n4: while not convergence do\n5:\nfor l = 0, . . . , L −1 do\n6:\nH(l+1) = GNN(l)(G, H(l))\n7:\nend for\n8:\nfor l = L −1, . . . , 0 do\n9:\nµ(l) = FNN(l)\nµ (H(l+1))\n10:\nσ(l) = exp(FNN(l)\nσ (H(l+1)))\n11:\nˆd\n(l) = f+(FNN(l)\nd (H(l+1)))\n12:\nZ(l) = NORMALSAMPLING(˜µ(l) + µ(l), σ(l))\n13:\n˜µ(l−1) = FNNz(Z(l))\n14:\nend for\n15:\nLself = PL−1\nl=0 ∥H(l) −Z(l)∥2\n2\n16:\nLnei = PL−1\nl=0 KL[q(Z(l)|H(l+1))∥p(H(l)|X)]\n17:\nLdeg = PL−1\nl=0 ∥d −ˆd\n(l)∥2\n2\n18:\nL = Lself + λneiLnei + λdegLdeg\n19:\nW ←W −κ∇W L\n20: end while\nmodel performance, but can bring a significant advantage\non time efficiency.\nTo further learn the neighbor numbers of different nodes,\nour model also reconstructs the node degrees di, i.e.,\nˆd(l)\ni\n= f+(FNNd(h(l+1)\ni\n)),\n(15)\nwhere f+(·) denotes a non-negative transformation function\n(e.g., ReLU). The neighbor reconstruction loss is then de-\nfined as\nLdeg =\nL−1\nX\nl=0\nn\nX\ni=1\n\u0012\n∥di −ˆd(l)\ni ∥2\n2\n\u0013\n.\n(16)\nLast, the full loss function of our proposed model is de-\nfined as a combination of the self and neighbor reconstruc-\ntion loss, i.e.,\nL = Lself + λneiLnei + λdegLdeg,\n(17)\nwhere λnei and λdeg are tuning hyperparameters to be spec-\nified. The pseudo-code for training our model is given in\nAlgorithm 1.\nDecoder Capacity Analysis\nOur proposed Inv-GNN decoder is constructed based on the\nisomorphic-consistent decoding scheme in Proposition 1.\nTo conform to condition a), the Inv-GNN decoder em-\nploys FNNs to reconstruct the self-embedding and neighbor-\nhood distribution of each GNN embedding. Specifically, for\nthe self-embedding reconstruction, FNNs have been widely\nadopted in learning such real-valued embeddings. As for the\nneighborhood reconstruction, we have the following theo-\nrem.\nTheorem 1. Let Φ0 and Φ1 denote two D-dimensional di-\nagonal multivariate Normal distributions, where the support\nof Φ0 is bounded. Given an arbitrarily small positive ap-\nproximation error ϵ →0+, there exists a fully connected and\nfeed-forward neural network FNN u(·) : RD →RD with\nsufficiently large depth and width (depending on D, ϵ and\nEx∼Φ0∥x∥3) such that KL[∇u#Φ1∥Φ0] < ϵ, where ∇u#\nindicates transforming a distribution using the gradient of\nu(·).\nProof.\nThe conclusion of Theorem 1 is derived from The-\norem 3.1 in (Tang, Li, and Yang 2022), which we restate\nbelow.\nFor any ϵ > 0, if the support of distribution P lies in a\nbounded space of RD, there exists an FNN u(·) : RD →\nRD (and thus its gradient ∇u(·) : RD →RD) with large\nenough width and depth (depending on d, ϵ and Ex∼P∥x∥3)\nsuch that W2(P, ∇u#Φ)2 < ϵ, where W2(·, ·)2 is the 2-\nWasserstein distance between distributions and Φ is a d-\ndimensional non-degenerate Normal distribution.\nBased on the above theorem, we only need to show\nthe connection between the 2-Wasserstein distance and\nthe KL divergence for diagonal multivariate Normal dis-\ntributions. Let ∇u#Φ1 = Normal(µ, Idσ2) and Φ0 =\nNormal(ν, Idτ 2) be the source and transformed target dis-\ntributions, respectively. The 2-Wasserstein distance between\nΦ0 and ∇u#Φ1 can be formulated as\nW2(Φ0, ∇u#Φ1)2 = ∥ν −µ∥2\n2 + ∥τ −σ∥2\n2\n=\nD\nX\nd=1\n\u0012\n(νd −µd)2 + (τd −σd)2\n\u0013\n,\n(18)\nwhere µ = (µ1, . . . , µD)⊤, σ = (σ1, . . . , σD)⊤, ν =\n(ν1, . . . , νD)⊤, τ = (τ1, . . . , τD)⊤. When ϵ →0+, we have\n0 ≤W2(∇u#Φ1, Φ0)2 < 0+\n⇒W2(Φ0, ∇u#Φ1)2 = 0\n⇒µd = νd, σd = τd, d = 1, . . . , D.\n(19)\nNow we expand the KL divergence between Φ0 and ∇u#Φ1\nas\nKL[∇u#Φ1∥Φ0] =1\n2\nD\nX\nd=1\n\u0012(µd −νd)2\nτ 2\nd\n+ σ2\nd\nτ 2\nd\n+ 2(ln τd −ln σd) −1\n\u0013\n.\n(20)\nCombining Eq. (19) and (20), we have\nlim\nϵ→0+ KL[Φ0∥∇u#Φ1] = 0.\n(21)\nSo we get the desired result.\nAssuming the GNN embedding posteriors p(h(l)\ni |X) as\ndiagonal multivariate Normal distributions (which is a stan-\ndard assumption of GNNs), the above conclusion theoreti-\ncally guarantees the feasibility for reconstructing p(h(l)\ni |X)\nby minimizing the KL divergence between q(z(l)\ni |h(l+1)\ni\n)\nand p(h(l)\ni |X) in Eq. (14), where the parameters of\nq(z(l)\ni |h(l+1)\ni\n) are obtained by transforming the embedding\nsamples from p(h(l+1)\ni\n|X) using FNNs in Eq. (11) and (12).\nIn addition, the proposed Inv-GNN decoder can also\nmaintain the permutation invariance of GNNs, since both the\nself-embedding and neighborhood reconstructions do not\nuse the adjacency matrix in the loss function and are inde-\npendent of the permutations of node indices. This property\nensures that our model focuses on learning the embeddings\nthat can best represent the graph structural information in-\nstead of the meaningless node permutations.\nRelated Work\nIn this section, we briefly review the literature related to the\nexpressive power of GNNs and the representative work of\nunsupervised graph representation learning methods.\nGNN-Based Graph Representation Learning\nGNNs are the most popular graph models for supervised\ngraph representation learning. Representative GNN-based\nmethods include GCN (Kipf and Welling 2017) and Graph-\nSAGE (Hamilton, Ying, and Leskovec 2017), both of which\ncan only perform well on node-level tasks such as node\nclassification but are not isomorphic-consistent for learn-\ning the higher-level embeddings. SEAL (Zhang and Chen\n2018) leverages the labeling trick to keep the isomor-\nphic consistency for learning embeddings at the multi-\nnode (e.g., link) level. Xu et al. (2019) proposes GIN for\ngraph-level representation learning and first employs the 1-\ndimensional Weisfeiler-Lehman (1-WL) test (Leman and\nWeisfeiler 1968) to evaluate the isomorphic consistency of\nGNNs, which is soon widely adopted by follow-up work\n(Morris et al. 2019; Chen et al. 2019; Maron et al. 2019a;\nSato, Yamada, and Kashima 2019; Li et al. 2020; Sato, Ya-\nmada, and Kashima 2021; Zhang et al. 2021; Wang and\nZhang 2022). In addition to the 1-WL test, there are also\nsome work attempts to measure the expressive power of\nGNNs in other ways (Maron et al. 2019b; Keriven and Peyr´e\n2019; Loukas 2020; Garg, Jegelka, and Jaakkola 2020; Chen\net al. 2020; Chen, Chen, and Bruna 2021). All of these stud-\nies are based on the GNN frameworks, which are typically\ndesigned for some individual downstream task and must be\nretrained (or even redesigned) when applied to a new task.\nUnsupervised Graph Representation Learning\nEarly unsupervised graph representation learning methods\nmainly focus on link-level tasks. Kipf and Welling (2016)\nfirst proposes the VGAEs that reconstruct the adjacency ma-\ntrix for link prediction, which has triggered plenty of succes-\nsive work in this direction (Pan et al. 2018; Bojchevski et al.\n2018; Grover, Zweig, and Ermon 2019; Mehta, Duke, and\nRai 2019; Sarkar, Mehta, and Rai 2020; Hou et al. 2022; Li\net al. 2023). Besides, GraphGAN (Wang et al. 2018; Zhu\net al. 2021) leverages the generative adversarial net (GAN)\nframework to learn general graph embeddings. However,\nall these methods can only preserve the 1-order isomorphic\nconsistency and are not permutation-invariant, as they are\ntrained by reconstructing the edges in an adjacency matrix\nwhich merely includes the local topological structure within\n1-hop neighborhoods. In particular, Tang, Li, and Yang\n(2022) proposes the neighborhood Wasserstein reconstruc-\ntion (NWR), which reconstructs the multi-hop GNN em-\nbeddings of node neighbors using the Wasserstein distance\nand thus can keep the higher-order topological structures.\nNevertheless, this method is still not isomorphic-consistent\nsince it is not permutation-invariant to the order of nodes,\nand must match the permutations of the GNN embeddings\nand reconstructed embeddings for training. There are also\nnon-generative methods for unsupervised graph representa-\ntion learning. For example, the deep graph infomax (DGI)\n(Veliˇckovi´c et al. 2019) and InfoGraph (Sun et al. 2020) pro-\nposed for node- and graph-level tasks, respectively, employ\nthe contrastive learning method and maximize the mutual in-\nformation between embeddings, but are not general enough\nfor multi-level graph representation learning, as they need\nto construct various types of negative samples for different\ntasks. To the best of our knowledge, there is still a lack of\nresearch on studying the isomorphic consistency of unsu-\npervised graph representation learning methods.\nExperiments\nWe evaluate the effectiveness of our proposed model on\nthree representative graph representation learning tasks of\ndifferent granularities, including node classification (node-\nlevel), link prediction (link-level) and graph classification\n(graph-level).\nDatasets\nThe experiments are conducted on well-known benchmark\ndatasets of each task. Specifically, for node classification\nand link prediction, we use the citation networks, i.e., Cora,\nCiteSeer and PubMed (Sen et al. 2008), all of which are con-\nstructed using documents from different disciplines as nodes\nand the citation links between them as edges. Each citation\nnetwork has a sparse binary feature matrix obtained using\nthe bag-of-words features (for Cora and PubMed) or one-\nhot category (for CiteSeer) of each document. The node la-\nbels are employed using the disciplines the documents be-\nlong to. Following the standard supervised node classifica-\ntion settings, for each network, we use the default node splits\nfor testing and validation, and all other nodes are used for\ntraining. As for link prediction, we randomly split the edges\nof each network as 85% for training, 10% for testing and\n5% for validation. Additionally, we also consider two large-\nscale datasets, i.e., Flickr (Zeng et al. 2020) for node classi-\nfication and ogbl-collab (Wang et al. 2020) for link predic-\ntion. Flickr is formed by images collected from websites, of\nwhich the edges denote whether two images share any com-\nmon metadata. The node features and classes are employed\nusing the descriptions and types of the images, respectively.\n# Nodes\n# Edges\n# Features\n# Node Classes\nCora\n2,708\n5,278\n1,433\n6\nCiteSeer\n3,312\n4,552\n3,703\n7\nPubMed\n19,717\n44,324\n500\n3\nFlickr\n89,250\n899,756\n500\n7\nogbl-collab\n235,868\n1,285,465\n128\n–\nTable 1: Descriptive statistics of datasets for node classifica-\ntion and link prediction.\n# Graphs\nAvg. # Nodes\n# Features\n# Graph Classes\nMUTAG\n188\n17.9\n7\n2\nPTC-MR\n344\n14.3\n18\n2\nIMDB-B\n1,000\n19.8\n–\n2\nCOLLAB\n5,000\n74.5\n–\n3\nTable 2: Descriptive statistics of datasets for graph classifi-\ncation.\nThe ogbl-collab is a large-scale collaboration network of the\nopen graph benchmark (OGB) datasets for link prediction.\nThe nodes represent authors and the edges denote collabo-\nrations between them. This dataset includes a node feature\nmatrix obtained using word embeddings of the papers pub-\nlished by the authors. Both of the two large-scale datasets\nuse the default data splits. The descriptive statistics of the\ndatasets for node classification and link prediction are given\nin Table 1.\nFor graph classification, we use two molecular datasets,\ni.e., MUTAG and PTC-MR (Yanardag and Vishwanathan\n2015), and two social network datasets, i.e., IMDB-\nBINARY and COLLAB (Yanardag and Vishwanathan\n2015). MUTAG and PTC-MR consist of chemical com-\npounds, where the nodes represent atoms and edges repre-\nsent chemical bonds. The molecules in MUTAG are labeled\naccording to their mutagenic effect on a bacterium, and those\nin PTC-MR are labeled based on their carcinogenicity on\nmale rats. The types of atoms in the molecules are employed\nas one-hot node features for both of the datasets, and the\nedge types are omitted in our experiments. IMDB-B and\nCOLLAB are collaboration datasets, where each graph is an\nego-network of a movie or a researcher, respectively. The\nnodes in each graph of IMDB-B represent actors/actresses\nand the edges represent whether they appear in the same\nmovie. The graphs are labeled according to the genre of\nthe movie. The nodes in each graph of COLLAB represent\nresearchers and the edges represent the collaborations be-\ntween them. The graphs are labeled based on the fields the\nresearchers belong to. We randomly split the graphs of each\ndataset as 50% for training, 30% for testing and 20% for val-\nidation. The descriptive statistics of the datasets for graph\nclassification are given in Table 2.\nBaselines\nFor comparative methods, we consider the state-of-the-art\nunsupervised graph representation learning methods, includ-\ning four VGAE-based methods, i.e., the vanilla (V)GAE\n(Kipf and Welling 2016), PIGAE (Winter, Noe, and Clevert\nCora\nCiteSeer\nPubMed\nFlickr\nMLP\n70.8 ± 1.6\n71.7 ± 0.9\n86.8 ± 0.6\n47.2 ± 0.1\nGCN\n86.3 ± 0.3\n77.0 ± 0.3\n87.2 ± 0.5\n51.0 ± 0.3\nGAT\n86.0 ± 0.4\n76.8 ± 0.3\n85.8 ± 0.1\n50.6 ± 0.5\nGAE\n72.1 ± 2.5\n57.1 ± 1.6\n73.2 ± 0.9\n44.6 ± 0.3\nVGAE\n72.9 ± 1.5\n60.8 ± 1.9\n81.3 ± 0.8\n44.3 ± 0.2\nDGI\n83.6 ± 0.6\n73.8 ± 1.0\n86.9 ± 0.3\n48.8 ± 0.4\nPIGAE\n80.2 ± 0.5\n68.9 ± 0.6\n81.4 ± 0.6\n46.5 ± 0.3\nNWR\n83.6 ± 1.6\n71.5 ± 2.4\n83.4 ± 0.9\n50.3 ± 0.1\nMaskGAE\n86.0 ± 0.7\n76.5 ± 1.1\n84.3 ± 0.9\n49.4 ± 0.2\nIsoC-VGAE\n86.3 ± 0.4\n77.2 ± 0.3\n87.7 ± 0.2\n51.2 ± 0.1\nTable 3: Node classification accuracy (%) for different meth-\nods. The best results are in bold and the second best ones are\nunderlined.\n2021), NWR (Tang, Li, and Yang 2022) and the very recent\nMaskGAE (Li et al. 2023), and two contrastive learning-\nbased methods, i.e., DGI (Veliˇckovi´c et al. 2019) (for node\nclassification and link prediction) and InfoGraph (Sun et al.\n2020) (for graph classification). In addition, we also con-\nsider the supervised multilayer perceptron (MLP) and sev-\neral representative GNN-based methods, including GCN\n(Kipf and Welling 2017) and GAT (Veliˇckovi´c et al. 2018)\nfor node classification, SEAL (Zhang and Chen 2018) for\nlink prediction, and GIN (Xu et al. 2019) for graph classi-\nfication. Note that GCN and GIN are also employed as the\nencoders of the VGAE-based methods for the node/link- and\ngraph-level tasks, respectively.\nExperimental Settings\nWe train all supervised comparative methods in an end-to-\nend manner for each task. As for the unsupervised meth-\nods, including our proposed IsoC-VGAE, we first train the\nmodels in an unsupervised manner and then feed the learned\nembeddings to MLPs for different supervised tasks. The\nlink- and graph-level embeddings are obtained by comput-\ning the inner products and sums of the learned node em-\nbeddings, respectively. In particular, we perform link pre-\ndiction in an end-to-end manner for the methods that are\ntrained using adjacency matrices, including (V)GAE, PI-\nGAE and MaskGAE, as they were originally designed to\ndo. Since we mainly focus on evaluating the power of de-\ncoders, for all VGAE-based methods, including (V)GAE,\nNWR, MaskGAE and our proposed IsoC-VGAE, we use\nGCN as the encoder for node classification and link predic-\ntion, and GIN as the encoder for graph classification.\nFor the hyperparameters of our model, we use a two-layer\nGNN encoder with the channel dimension of 512, and the\nsame for the Inv-GNN decoder. The tuning parameters λnei\nand λdeg are set as 0.1 and 1, respectively. The MLPs for\nsupervised tasks are set as four layers with the channel di-\nmension of 256.\nResults\nWe employ the widely used classification accuracy as the\nevaluation metric for node and graph classification, and the\nCora\nCiteSeer\nPubMed\nogbl-collab\nMLP\n80.7 ± 0.6\n88.8 ± 0.6\n91.6 ± 0.2\n94.0 ± 0.1\nSEAL\n92.2 ± 1.1\n93.4 ± 0.5\n93.0 ± 1.0\n95.7 ± 0.1\nGAE\n91.2 ± 0.7\n90.5 ± 0.5\n95.7 ± 0.2\n94.1 ± 0.1\nVGAE\n91.4 ± 0.0\n90.8 ± 0.0\n94.4 ± 0.0\n94.5 ± 0.1\nDGI\n91.4 ± 0.8\n93.2 ± 0.8\n96.9 ± 0.0\n95.0 ± 0.1\nPIGAE\n89.8 ± 0.6\n90.6 ± 0.9\n94.1 ± 0.1\n92.6 ± 0.2\nNWR\n91.7 ± 0.6\n92.2 ± 0.8\n98.0 ± 0.1\n93.6 ± 0.6\nMaskGAE\n94.9 ± 0.5\n95.2 ± 1.0\n98.3 ± 0.0\n94.6 ± 0.4\nIsoC-VGAE\n93.3 ± 0.3\n95.3 ± 0.4\n98.1 ± 0.1\n95.2 ± 0.0\nTable 4: Link prediction AUC (%) for different methods.\nThe best results are in bold and the second best ones are\nunderlined.\nMUTAG\nPTC-MR\nIMDB-B\nCOLLAB\nMLP\n88.6 ± 4.1\n56.5 ± 2.0\n52.2 ± 1.2\n47.2 ± 0.1\nGIN\n89.0 ± 1.5\n62.1 ± 5.3\n72.5 ± 1.2\n67.5 ± 1.9\nGAE\n73.9 ± 5.3\n52.4 ± 4.5\n59.6 ± 2.9\n59.7 ± 1.7\nVGAE\n71.1 ± 2.3\n58.8 ± 3.8\n53.8 ± 1.0\n58.9 ± 3.4\nInfoGraph\n87.5 ± 1.3\n58.3 ± 3.0\n64.9 ± 1.0\n65.9 ± 0.3\nPIGAE\n82.5 ± 2.5\n56.6 ± 2.3\n62.2 ± 1.1\n64.1 ± 0.9\nNWR\n89.3 ± 1.3\n61.6 ± 1.7\n72.4 ± 0.3\n68.5 ± 0.1\nMaskGAE\n85.0 ± 2.7\n59.0 ± 1.7\n58.3 ± 5.1\n57.8 ± 0.7\nIsoC-VGAE\n89.6 ± 2.0\n62.5 ± 1.1\n73.2 ± 0.9\n70.1 ± 0.8\nTable 5: Graph classification accuracy (%) for different\nmethods. The best results are in bold and the second best\nones are underlined.\narea under the ROC curve (AUC) for link prediction. All re-\nsults are reported as the means and standard deviations over\n5 independent runs.\nNode Classification\nTable 3 demonstrates that our model\ncan significantly outperform all unsupervised comparative\nmethods. Note that the vanilla (V)GAE even shows inferior\nperformance to the MLP (except for the Cora dataset), indi-\ncating that the node embeddings learned by reconstructing\nthe adjacency matrix can perform worse than the raw node\nfeatures. On the other hand, our model can not only sig-\nnificantly outperform the MLP, but also show comparable\nperformance to the GNN-based supervised methods, which\nfurther verifies that the proposed Inv-GNN decoder can well\nkeep the representation learning power of GNNs.\nLink Prediction\nThe experimental results in Table 4 show\nthat our model achieves at least comparable results to both\nthe supervised and unsupervised methods for link predic-\ntion. It should be mentioned that most of the comparative\nmethods here, including SEAL, (V)GAE, DGI, PIGAE and\nMaskGAE, should be especially powerful for this task as\nthey use edges as training labels in the loss functions. The\nexperimental results also verify that our proposed Inv-GNN\ndecoder can generally reserve the power of VGAEs for the\nlink-level task, and meanwhile have much better generaliz-\nability to other levels of graph representation learning tasks.\nCora\nCiteSeer\nPubMed\nFlickr\nw/o deg. & nei.\n85.4 ± 0.3\n76.8 ± 0.6\n87.4 ± 0.4\n50.4 ± 0.1\nw/o nei.\n85.4 ± 0.5\n76.9 ± 0.9\n87.1 ± 0.4\n50.5 ± 0.1\nw/o deg.\n85.7 ± 0.9\n76.9 ± 0.8\n87.6 ± 0.3\n50.7 ± 0.1\nIsoC-VGAE\n86.3 ± 0.4\n77.2 ± 0.3\n87.7 ± 0.2\n51.2 ± 0.1\nTable 6: Node classification accuracy (%) for ablation study.\nThe best results are in bold.\nCora\nCiteSeer\nPubMed\nogbl-collab\nw/o deg. & nei.\n93.1 ± 0.6\n94.6 ± 0.8\n97.9 ± 0.1\n90.2 ± 0.1\nw/o nei.\n93.1 ± 0.5\n94.5 ± 0.8\n97.8 ± 0.2\n94.7 ± 0.0\nw/o deg.\n93.2 ± 0.5\n94.6 ± 0.9\n96.6 ± 0.2\n91.6 ± 0.0\nIsoC-VGAE\n93.3 ± 0.3\n95.3 ± 0.4\n98.1 ± 0.1\n95.2 ± 0.0\nTable 7: Link prediction AUC (%) for ablation study. The\nbest results are in bold.\nGraph Classification\nFrom Table 5, it can be seen that\nour proposed IsoC-VGAE can significantly outperform all\nof the comparative unsupervised graph representation learn-\ning methods, most of which only perform comparably to the\nMLP baseline due to their poor generalizability to the graph-\nlevel tasks. In addition, our model also achieves comparable\nresults to the supervised method GIN, which verifies the ef-\nfectiveness of the proposed Inv-GNN decoder as well as the\ntheoretical decoding scheme for preserving the high-order\nisomorphic consistency of the GIN encoder to learn graph-\nlevel representations.\nAblation Study\nWe further conduct a series of experiments to evaluate dif-\nferent reconstruction components of the proposed Inv-GNN\ndecoder, including the neighborhood distribution and node\ndegrees. The experimental results on node classification are\npresented in Table 6-8, which demonstrate that the proposed\nneighbor reconstruction strategy via learning the neighbor-\nhood distributions and node degrees is effective for improv-\ning the performance on downstream tasks.\nIn addition, we also conduct experiments of sensitivity\nanalysis on the tuning hyperparameters of our model, in-\ncluding λnei and λdeg. The results are given in Fig. 4 and\n5, which show that the performance of our model is gener-\nally robust to both of the parameters in a wide range. This\nis mainly because these parameters can only exert some in-\nfluence on the unsupervised training process of our model,\nwhich can be almost eliminated during the fine-tuning pro-\ncess for training the MLPs.\nConclusion\nIn this paper, we propose the IsoC-VGAE for multi-level\ngeneral graph representation learning. We first devise a de-\ncoding scheme to provide a theoretical guarantee for keeping\nthe high-order isomorphic consistency of GNNs under un-\nsupervised learning settings. Then, we propose a novel Inv-\nGNN decoder as an instantiation of the decoding scheme.\nMUTAG\nPTC-MR\nIMDB-B\nCOLLAB\nw/o deg. & nei.\n85.7 ± 2.8\n59.2 ± 5.1\n68.9 ± 2.1\n69.1 ± 0.5\nw/o nei.\n86.8 ± 1.6\n59.1 ± 2.0\n70.2 ± 1.6\n68.9 ± 0.4\nw/o deg.\n87.9 ± 2.7\n59.5 ± 2.0\n69.2 ± 0.6\n67.5 ± 1.9\nIsoC-VGAE\n89.6 ± 2.0\n62.5 ± 1.1\n73.2 ± 0.9\n70.1 ± 0.8\nTable 8: Graph classification accuracy (%) for ablation\nstudy. The best results are in bold.\nExperimental results based on benchmark graph datasets\nverify that our proposed model achieves at least compara-\nble performance to the representative graph representation\nlearning methods on tasks at node, link and graph levels.\nReferences\nBojchevski, A.; Shchur, O.; Z¨ugner, D.; and G¨unnemann,\nS. 2018. Netgan: Generating graphs via random walks. In\nInternational Conference on Machine Learning, 610–619.\nChen, L.; Chen, Z.; and Bruna, J. 2021. On Graph Neural\nNetworks versus Graph-Augmented MLPs. In International\nConference on Learning Representations.\nChen, Z.; Chen, L.; Villar, S.; and Bruna, J. 2020. Can Graph\nNeural Networks Count Substructures?\nIn Advances in\nNeural Information Processing Systems, volume 33, 10383–\n10395.\nChen, Z.; Villar, S.; Chen, L.; and Bruna, J. 2019. On the\nequivalence between graph isomorphism testing and func-\ntion approximation with gnns. Advances in neural informa-\ntion processing systems, 32.\nGarg, V.; Jegelka, S.; and Jaakkola, T. 2020. Generalization\nand representational limits of graph neural networks. In In-\nternational Conference on Machine Learning, 3419–3430.\nGrohe, M.; and Schweitzer, P. 2020. The graph isomorphism\nproblem. Communications of the ACM, 63(11): 128–134.\nGrover, A.; Zweig, A.; and Ermon, S. 2019. Graphite: Iter-\native generative modeling of graphs. In International Con-\nference on Machine Learning, 2434–2444.\nHamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive\nRepresentation Learning on Large Graphs. In Advances in\nNeural Information Processing Systems, volume 30.\nHou, Z.; Liu, X.; Cen, Y.; Dong, Y.; Yang, H.; Wang, C.;\nand Tang, J. 2022. Graphmae: Self-supervised masked graph\nautoencoders. In ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, 594–604.\nKeriven, N.; and Peyr´e, G. 2019. Universal Invariant and\nEquivariant Graph Neural Networks. In Advances in Neural\nInformation Processing Systems, volume 32.\nKingma, D. P.; and Welling, M. 2014. Auto-encoding varia-\ntional Bayes. In International Conference on Learning Rep-\nresentations.\nKipf, T. N.; and Welling, M. 2016. Variational graph auto-\nencoders. In NIPS Workshop on Bayesian Deep Learning.\nKipf, T. N.; and Welling, M. 2017. Semi-supervised classi-\nfication with graph convolutional networks. In International\nConference on Learning Representations.\n0.2\n0.4\n0.6\n0.8\n1\n5\n10\nnei\n70\n75\n80\n85\n90\nAccuracy (%)\nCora\nCiteSeer\nPubMed\n(a) Node Classification\n0.2\n0.4\n0.6\n0.8\n1\n5\n10\nnei\n90\n92\n94\n96\n98\nAUC (%)\nCora\nCiteSeer\nPubMed\n(b) Link Prediction\n0.2\n0.4\n0.6\n0.8\n1\n5\n10\nnei\n50\n60\n70\n80\n90\nAccuracy (%)\nMUTAG\nPTC-MR\nIMDB-B\n(c) Graph Classification\nFigure 4: Experimental results of the proposed IsoC-VGAE with different values of λnei.\n0.2\n0.4\n0.6\n0.8\n1\n5\n10\ndeg\n70\n75\n80\n85\n90\nAccuracy (%)\nCora\nCiteSeer\nPubMed\n(a) Node Classification\n0.2\n0.4\n0.6\n0.8\n1\n5\n10\ndeg\n90\n92\n94\n96\n98\nAUC (%)\nCora\nCiteSeer\nPubMed\n(b) Link Prediction\n0.2\n0.4\n0.6\n0.8\n1\n5\n10\ndeg\n50\n60\n70\n80\n90\nAccuracy (%)\nMUTAG\nPTC-MR\nIMDB-B\n(c) Graph Classification\nFigure 5: Experimental results of the proposed IsoC-VGAE with different values of λdeg.\nLeman, A.; and Weisfeiler, B. 1968. A reduction of a graph\nto a canonical form and an algebra arising during this reduc-\ntion. Nauchno-Technicheskaya Informatsiya, 2(9): 12–16.\nLi, J.; Wu, R.; Sun, W.; Chen, L.; Tian, S.; Zhu, L.; Meng,\nC.; Zheng, Z.; and Wang, W. 2023.\nWhat’s Behind the\nMask: Understanding Masked Graph Modeling for Graph\nAutoencoders. In ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining.\nLi, P.; Wang, Y.; Wang, H.; and Leskovec, J. 2020. Distance\nEncoding: Design Provably More Powerful Neural Net-\nworks for Graph Representation Learning. In Advances in\nNeural Information Processing Systems, volume 33, 4465–\n4478.\nLoukas, A. 2020. What graph neural networks cannot learn:\nDepth vs Width. In International Conference on Learning\nRepresentations.\nMaron, H.; Ben-Hamu, H.; Serviansky, H.; and Lipman, Y.\n2019a. Provably Powerful Graph Networks. In Advances in\nNeural Information Processing Systems, volume 32.\nMaron, H.; Fetaya, E.; Segol, N.; and Lipman, Y. 2019b.\nOn the universality of invariant networks. In International\nConference on Machine Learning, 4363–4371.\nMehta, N.; Duke, L. C.; and Rai, P. 2019. Stochastic block-\nmodels meet graph neural networks. In International Con-\nference on Machine Learning, 4466–4474.\nMorris, C.; Ritzert, M.; Fey, M.; Hamilton, W. L.; Lenssen,\nJ. E.; Rattan, G.; and Grohe, M. 2019. Weisfeiler and leman\ngo neural: Higher-order graph neural networks.\nIn AAAI\nConference on Artificial Intelligence, volume 33, 4602–\n4609.\nPan, S.; Hu, R.; Long, G.; Jiang, J.; Yao, L.; and Zhang,\nC. 2018. Adversarially regularized graph autoencoder for\ngraph embedding. In IJCAI International Joint Conference\non Artificial Intelligence.\nSarkar, A.; Mehta, N.; and Rai, P. 2020. Graph represen-\ntation learning via ladder gamma variational autoencoders.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, 5604–5611.\nSato, R.; Yamada, M.; and Kashima, H. 2019.\nApproxi-\nmation Ratios of Graph Neural Networks for Combinatorial\nProblems. In Advances in Neural Information Processing\nSystems, volume 32.\nSato, R.; Yamada, M.; and Kashima, H. 2021. Random fea-\ntures strengthen graph neural networks. In SIAM Interna-\ntional Conference on Data Mining, 333–341.\nSen, P.; Namata, G.; Bilgic, M.; Getoor, L.; Galligher, B.;\nand Eliassi-Rad, T. 2008. Collective classification in net-\nwork data. AI Magazine, 29(3): 93–93.\nSun, F.-Y.; Hoffman, J.; Verma, V.; and Tang, J. 2020. In-\nfoGraph: Unsupervised and Semi-supervised Graph-Level\nRepresentation Learning via Mutual Information Maximiza-\ntion. In International Conference on Learning Representa-\ntions.\nTang, M.; Li, P.; and Yang, C. 2022. Graph Auto-Encoder\nvia Neighborhood Wasserstein Reconstruction. In Interna-\ntional Conference on Learning Representations.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y. 2018. Graph attention networks. In Inter-\nnational Conference on Learning Representations.\nVeliˇckovi´c, P.; Fedus, W.; Hamilton, W. L.; Li`o, P.; Bengio,\nY.; and Hjelm, R. D. 2019. Deep Graph Infomax. In Inter-\nnational Conference on Learning Representations.\nWang, H.; Wang, J.; Wang, J.; Zhao, M.; Zhang, W.; Zhang,\nF.; Xie, X.; and Guo, M. 2018. Graphgan: Graph representa-\ntion learning with generative adversarial nets. In AAAI Con-\nference on Artificial Intelligence, volume 32.\nWang, K.; Shen, Z.; Huang, C.; Wu, C.-H.; Dong, Y.; and\nKanakia, A. 2020. Microsoft academic graph: When experts\nare not enough. Quantitative Science Studies, 1(1): 396–\n413.\nWang, X.; and Zhang, M. 2022. How Powerful are Spec-\ntral Graph Neural Networks. In International Conference\non Machine Learning, volume 162, 23341–23362.\nWinter, R.; Noe, F.; and Clevert, D.-A. 2021. Permutation-\nInvariant Variational Autoencoder for Graph-Level Repre-\nsentation Learning. In Advances in Neural Information Pro-\ncessing Systems, volume 34, 9559–9573.\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How\nPowerful are Graph Neural Networks? In International Con-\nference on Learning Representations.\nYanardag, P.; and Vishwanathan, S. 2015. Deep Graph Ker-\nnels. In ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 1365–1374.\nYang, C.; Zhuang, P.; Shi, W.; Luu, A.; and Li, P. 2019.\nConditional Structure Generation through Graph Variational\nGenerative Adversarial Nets. In Advances in Neural Infor-\nmation Processing Systems, volume 32.\nZeng, H.; Zhou, H.; Srivastava, A.; Kannan, R.; and\nPrasanna, V. 2020. GraphSAINT: Graph Sampling Based\nInductive Learning Method. In International Conference on\nLearning Representations.\nZhang, M.; and Chen, Y. 2018. Link Prediction Based on\nGraph Neural Networks. In Advances in Neural Information\nProcessing Systems, volume 31.\nZhang, M.; Li, P.; Xia, Y.; Wang, K.; and Jin, L. 2021. La-\nbeling Trick: A Theory of Using Graph Neural Networks for\nMulti-Node Representation Learning. In Advances in Neu-\nral Information Processing Systems, volume 34, 9061–9073.\nZhu, S.; Li, J.; Peng, H.; Wang, S.; and He, L. 2021. Ad-\nversarial directed graph embedding. In AAAI Conference on\nArtificial Intelligence, volume 35, 4741–4748.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-12-09",
  "updated": "2023-12-09"
}