{
  "id": "http://arxiv.org/abs/2312.06877v1",
  "title": "A Novel Differentiable Loss Function for Unsupervised Graph Neural Networks in Graph Partitioning",
  "authors": [
    "Vivek Chaudhary"
  ],
  "abstract": "In this paper, we explore the graph partitioning problem, a pivotal\ncombina-torial optimization challenge with extensive applications in various\nfields such as science, technology, and business. Recognized as an NP-hard\nprob-lem, graph partitioning lacks polynomial-time algorithms for its\nresolution. Recently, there has been a burgeoning interest in leveraging\nmachine learn-ing, particularly approaches like supervised, unsupervised, and\nreinforce-ment learning, to tackle such NP-hard problems. However, these\nmethods face significant hurdles: supervised learning is constrained by the\nnecessity of labeled solution instances, which are often computationally\nimpractical to obtain; reinforcement learning grapples with instability in the\nlearning pro-cess; and unsupervised learning contends with the absence of a\ndifferentia-ble loss function, a consequence of the discrete nature of most\ncombinatorial optimization problems. Addressing these challenges, our research\nintroduces a novel pipeline employing an unsupervised graph neural network to\nsolve the graph partitioning problem. The core innovation of this study is the\nfor-mulation of a differentiable loss function tailored for this purpose. We\nrigor-ously evaluate our methodology against contemporary state-of-the-art\ntech-niques, focusing on metrics: cuts and balance, and our findings reveal\nthat our is competitive with these leading methods.",
  "text": "A Novel Differentiable Loss Function for Unsupervised \nGraph Neural Networks in Graph Partitioning \nVivek Chaudhary [0000-0002-5517-3190]  \n \nvivekch2018@gmail.com \nAbstract. In this paper, we explore the graph partitioning problem, a pivotal \ncombinatorial optimization challenge with extensive applications in various \nfields such as science, technology, and business. Recognized as an NP-hard \nproblem, graph partitioning lacks polynomial-time algorithms for its resolution. \nRecently, there has been a burgeoning interest in leveraging machine learning, \nparticularly approaches like supervised, unsupervised, and reinforcement learn-\ning, to tackle such NP-hard problems. However, these methods face significant \nhurdles: supervised learning is constrained by the necessity of labeled solution \ninstances, which are often computationally impractical to obtain; reinforcement \nlearning grapples with instability in the learning process; and unsupervised \nlearning contends with the absence of a differentiable loss function, a conse-\nquence of the discrete nature of most combinatorial optimization problems. Ad-\ndressing these challenges, our research introduces a novel pipeline employing \nan unsupervised graph neural network to solve the graph partitioning problem. \nThe core innovation of this study is the formulation of a differentiable loss \nfunction tailored for this purpose. We rigorously evaluate our methodology \nagainst contemporary state-of-the-art techniques, focusing on metrics: cuts and \nbalance, and our findings reveal that our is competitive with these leading \nmethods. \nKeywords: Graph Neural Network, Combinatorial Optimization, Graph Parti-\ntioning \n1 \nIntroduction \nIn recent years, there has been increasing interest in the application of machine learn-\ning for solving combinatorial optimization problems. These problems, inherently \ncomplex and computationally challenging, span a wide range of applications, from the \nwell-known Travelling Salesman Problem [1] to the Knapsack Problem [2]. Among \nthese, notable examples include the Maximum Clique Problem [3], Maxi-\nmum/Minimum Cut Problem [4], Boolean Satisfiability Problem [5], Hamiltonian \nPath Problem, Facility Location Problem [6], and the Quadratic Assignment Problem \n[7]. The diversity and complexity of these problems pose unique challenges and op-\nportunities for machine learning approaches. \n2 \n \nMachine learning methodologies employed to tackle these problems can be broadly \ncategorized into supervised learning, unsupervised learning, and reinforcement learn-\ning. Each approach offers distinct perspectives and tools for solving these computa-\ntionally intensive problems. Supervised learning, despite its widespread use [12] [13] \n[14], encounters a fundamental challenge in this domain: it requires large datasets of \npre-solved instances, leading to a 'chicken and egg' situation [8]. Another problem \nthat complicates supervised learning is the difficulty in sampling unbiased labeled \ninstances of NP-hard problems [3] . \n \nReinforcement learning, while showing promise in discrete action spaces [9] [11], \nfaces hurdles due to its lack of full differentiability, making training procedures com-\nplex and resource intensive [10]. This limitation is especially pronounced when deal-\ning with the vast and intricate search spaces of NP-hard problems. \n \nUnsupervised learning, on the other hand, has recently become a focal point in the \nmachine learning community [15] [16], especially given its potential to circumvent \nthe need for labeled data. The primary challenge here lies in developing an effective \nloss function that can guide the learning process in the absence of labeled examples. \n \nGraph Neural Networks (GNNs) have been a significant innovation within the deep \nlearning community, particularly for data with graph structures. GNNs excel at learn-\ning feature representations for nodes, edges, and entire graphs, making them well-\nsuited for a variety of applications [17]. These applications range from user classifica-\ntion in social networks [19] to predicting interactions in recommender systems [18] \nand properties of molecular graphs [20]. The versatility and effectiveness of GNNs in \nmodeling complex structural data have led to their successful application across a \nbroad spectrum of real-world problems. \n \nOur research aligns with the cutting-edge efforts in the deep learning community, \nespecially those focusing on training unsupervised GNNs end to end. We leverage the \npotential of GNNs to tackle the graph partitioning problem, a classic example of an \nNP-hard problem, without the reliance on labeled training sets. This approach not \nonly aligns with the recent trends in machine learning but also addresses the specific \nchallenges posed by NP-hard combinatorial optimization problems. By doing so, our \nwork seeks to contribute to the growing body of knowledge in applying unsupervised \nlearning techniques, particularly GNNs, to complex, real-world optimization chal-\nlenges. Section 2 outlines theoretical preliminaries for this paper. Section 3 contains \ndetails of the probabilistic graph neural network for graph partitioning. Section 4 con-\ntains the details and results of our experiments. The work is concluded in section 5. \n3 \n2 \nPreliminaries \n2.1 \nGraph partitioning problem \nA graph G=(V,E) consists of a set of vertices V and a set of edges E. The objective in \ngraph partitioning is to divide the vertex set V into disjoint subsets V1, V2 , such that \ncertain conditions or objectives are satisfied. The objective is to minimize the number \nof edges between different subsets while maintaining a balance in the size of these \nsubsets [32]. \n  \nProving the NP-hardness of graph partitioning involves demonstrating that no poly-\nnomial-time algorithm can solve it for all cases unless P=NP. This complexity leads \nto a focus on approximation algorithms, which seek near-optimal solutions within a \nreasonable time frame. Performance of these algorithms is often evaluated based on \nhow close they get to the optimal solution. State of the Art methods for solving graph \npartitioning include Kernighan-Lin method [29], Spectral Partitioning [30], and Mul-\nti-Level Partitioning [31]. \n \nBesides minimizing the edge cut and maintaining balance, other metrics like modular-\nity (for community detection) [33], communication volume [34] (in parallel compu-\nting), and expansion or conductance (measuring how well connected the subgraphs \nare) can be crucial, depending on the application. In this paper, we focus on percent of \ncuts and balance.  \n. \n2.2 \nGraph Neural Networks \nGraph Neural Networks (GNNs) represent a significant advancement in the field of \ndeep learning, particularly for processing data that is naturally structured as graphs. \nUnlike standard neural networks that assume a sequential or grid-like structure in the \ninput data, GNNs are designed to work directly with graph structures, consisting of \nnodes (vertices) and edges. This section provides a detailed technical and mathemati-\ncal exposition of GNNs, elucidating their fundamental concepts, and architecture. \nFundamental Concepts \nGraph Structure.  A graph G=(V,E) consists of a set V containing vertices and a set E \ncontaining edges. Each vertex v âˆˆ V can have associated features xv, and similarly, \nedges can have features xe. \nNode representation. The core objective of a GNN is to learn a representation (em-\nbedding) hv for each node V, which captures both its features and its structural role \nwithin the graph. \n4 \nMathematical Framework. The core operation of a GNN is the aggregation of in-\nformation from a nodeâ€™s neighborhood. This can be formalized as follows: \nMessage Passing:  Each node v aggregates messages from adjacent nodes N(v) and \npossibly its own features. This is typically a two-step process involving message cal-\nculation and aggregation [28]: \n1. Message Calculation: For each edge (u, v) âˆˆ E, a message function M computes a \nmessage muv = M(hu, hv, xe), where hu and hv are the feature vectors of the nodes \nand xe is the edge feature. \n2. Aggregation: A node v aggregates messages from its neighborhood using an ag-\ngregation function A, av = A({muv : u âˆˆ N(v)}). Common aggregation functions in-\nclude sum, mean, and max. \nNode Update. The aggregated message av is then combined with the node's current \nstate to update its representation. This update is typically done using a neural network \nU: h'v = U(hv, av). After this step, h'v becomes the new feature representation of node \nv. \nLearning Objective. The learning process in GNNs involves adjusting the parameters \nof the message, aggregation, and update functions to optimize a task-specific objec-\ntive, such as node classification [29], graph classification [30], or link prediction [31]. \nArchitecture Variants \n \nGraph Convolutional Networks (GCNs): These generalize convolutional neural net-\nworks to graphs by defining convolution operations on the graph structure [25]. \n \nGraph Attention Networks (GATs): GATs apply  attention mechanisms in the aggre-\ngation step, allowing the model to learn the importance of each neighborâ€™s message \n[26]. \n \nGraphSAGE:. This variant samples a fixed-size neighborhood and uses different ag-\ngregation functions, enabling scalability to large graphs [27]. \n \n3 \nProbabilistic Graph Neural Network for Graph \nPartitioning \nWe assume a weighted graph G = (V,E,w) where V is the set of edges, E is the set of \nedges and w is the set of weights of the nodes. The graph partitioning can be modelled \nas  \n \nmin\nà¯¦âŠ†à¯©ğ‘˜(ğ‘†; ğº)  ğ‘ ğ‘¢ğ‘â„ ğ‘¡â„ğ‘ğ‘¡ ğ‘†âˆˆ Î¥ \n(1) \n5 \nWhere k is a cost function, Î¥ is the family of sets which conform to a partitioned \ngraph. \n3.1 \nSolution pipeline \nOur solution is inspired from [3] and comprises of the following steps. A GNN nÎ¸  is \nconstructed that outputs a distribution D = nÎ¸(G) over sets. GNN nÎ¸ is trained to op-\ntimize the probability that there is a valid Sâˆ— âˆ¼ D with a small cost k(Sâˆ—; G). Sâˆ— is \nrecovered from D deterministically. \n3.2 \nProbabilistic loss function for Graph Partitioning Problem \nWe train the model to obtain a distribution with a low cost We define a probabilistic \nloss function l(D;G) that adheres to the following. \n \nğ‘ƒ(ğ‘˜(ğ‘†; ğº) < ğ‘™(ğ·; ğº) > ğ‘§   ğ‘¤ğ‘–ğ‘¡â„ ğ·= ğ‘›à°(ğº) \n(2) \n \nBy using Markovâ€™s inequality, we get: \n \nğ‘™(ğ·; ğº) â‰œ\nà®¾[à¯(à¯Œ;à¯€)]\nà¬µà¬¿à¯­\n    ğ‘“ğ‘œğ‘Ÿ ğ‘ğ‘›ğ‘¦ ğ‘§âˆˆ[0,1) \n(3) \nNow if we train the GNN to a sufficiently small loss l(D;G) = Îµ , then there exists a \npositive probability that a set S* exists for which the loss is at most Îµ. For the graph \npartitioning problem, the loss function can be described as \n \nğ‘™(ğ·; ğº) â‰œğ‘™à¯–à¯¨à¯§à¯¦+ ğ‘™à¯•à¯”à¯Ÿà¯”à¯¡à¯–à¯˜+ ğ‘™à¯–à¯˜à¯¡à¯§à¯¥à¯”à¯Ÿà¯œà¯§à¯¬  \n(4) \n \nWhere lcuts  is the loss for a cut, or an edge between two different partitions. This term \nis given by: \n \n \nğ‘™à¯–à¯¨à¯§à¯¦ =  Î£((tan(ğ›¼âˆ—ğ‘šà¯œ) âˆ’0.5) âˆ—àµ«tanàµ«ğ›¼âˆ—ğ‘šà¯àµ¯âˆ’0.5àµ¯) \n(5) \n \nWhere mi and mj are estimations of index of maximum arguments in SoftMax outputs \npi and pj  for adjacent nodes. Î± is a parameter that can be tuned while training. lbalance \nis the loss for imbalance, or size difference in the two partitions. This term is given \nby: \n \n \nğ‘™à¯•à¯”à¯Ÿà¯”\n= Î£tan (ğ›¼âˆ—(ğ‘“à¯œâˆ’0.5))à¬¶+ Î£tan (ğ›¼âˆ—àµ«ğ‘“à¯âˆ’0.5àµ¯)à¬¶ \n(6) \n \n6 \nWhere fi and fj are the SoftMax outputs for the first partition in SoftMax outputs pi and \npj for adjacent nodes. \n \nlcentrality is a loss for ensuring that the SoftMax outputs pi and pj are not centered \naround 0.5. This term is given by: \n \n \nğ‘™à¯–à¯˜à¯¡à¯§à¯¥à¯”à¯Ÿà¯œà¯§à¯¬=  ğ‘’\nà¬¿(à³‘à³” à°¶ à³‘à³”)à°®\nà°®âˆ—à´à°®\n \n(7) \nWhere Î¾ is a parameter that can be tuned while training. \n4 \nExperiment results \nWe conduct experiments on sets of graphs with different number of nodes. The per-\nformance of the models is measured in terms of cut percentage and imbalance per-\ncentage. Cut percentage is the percentage of number of edges that violate the con-\nstraints to the total number of edges. Imbalance percentage is the percentage of differ-\nence in size of partitions to the total number of nodes. Our Probabilistic GNN is com-\npared with Kernighan-Lin method and Spectral Partitioning.  \nTable 1. Cut percentage for different methods for different graph sizes \nNodes\nGNN\nKernighan-Lin \nSpectral \n50\n33.50 \n31.69 \n26.82 \n100\n38.42 \n36.53 \n32.33 \n150\n40.50 \n38.84 \n34.96 \n200\n41.62 \n40.19 \n37.06 \n250\n42.80 \n41.24 \n35.44 \n300\n43.15 \n41.95 \n36.46 \n350\n45.57 \n42.52 \n37.04 \n400\n44.29 \n42.99 \n37.10 \n450\n44.47 \n43.39 \n37.17 \n500\n44.50 \n43.69 \n38.93 \n \n \n \n \n \n \n7 \nTable 2. Imbalance percent for different methods for different graph sizes \nNodes\nGNN\nKernighan-Lin \nSpectral \n50\n6.80 \n0.00 \n42.56 \n100\n5.48 \n0.00 \n40.96 \n150\n5.28 \n0.00 \n38.96 \n200\n5.30 \n0.00 \n36.70 \n250\n3.36 \n0.00 \n44.27 \n300\n3.32 \n0.00 \n42.59 \n350\n7.63 \n0.00 \n41.73 \n400\n2.72 \n0.00 \n42.62 \n450\n4.63 \n0.00 \n42.28 \n500\n6.73 \n0.00 \n38.54 \n \n \nFig. 1.  Cut percent for different methods  \n \nFig. 2. Imbalance percent for different methods \nFrom the above observations, it is evident that our method is competitive with Ker-\nnighan-Lin method in terms of cut percentage, lying within 10% of its value. Our \nmethod performs better than Spectral method when it comes to imbalance percentage. \nOur method produces more balanced graphs than Spectral method, though with more \ncuts. \n \n8 \n5 \nConclusion \nIn this paper we proposed a novel method for solving Graph Partitioning Problem \nwith Probabilistic Graph Neural Network. With our experimentation results we \ndemonstrated that our novel method is competitive with state-of-the-art methods in \ncertain instances.  \nReferences \n1. Yong Shi, Yuanying Zhang, The neural network methods for solving Traveling Salesman \nProblem, Procedia Computer Science, Volume 199, 2022, Pages 681-686, ISSN 1877-\n0509, https://doi.org/10.1016/j.procs.2022.01.084. \n2. Fehmi Burcin Ozsoydan, Ä°lker GÃ¶lcÃ¼k, A reinforcement learning based computational in-\ntelligence approach for binary optimization problems: The case of the set-union knapsack \nproblem, Engineering Applications of Artificial Intelligence, Volume 118, 2023, 105688, \nISSN 0952-1976, https://doi.org/10.1016/j.engappai.2022.105688. \n3. Karalias, Nikolaos, and Andreas Loukas. \"Erdos goes neural: an unsupervised learning \nframework for combinatorial optimization on graphs.\" Advances in Neural Information \nProcessing Systems 33 (2020): 6659-6672. \n4. Melnikov, Alexey, et al. \"Quantum machine learning: From physics to software engineer-\ning.\" Advances in Physics: X 8.1 (2023): 2165452. \n5. Guo, Wenxuan, et al. \"Machine learning methods in solving the boolean satisfiability prob-\nlem.\" Machine Intelligence Research (2023): 1-16. \n6. Agrawal, Priyank, et al. \"Learning-augmented mechanism design: Leveraging predictions \nfor facility location.\" Proceedings of the 23rd ACM Conference on Economics and Com-\nputation. 2022. \n7. Wang, Runzhong, Junchi Yan, and Xiaokang Yang. \"Neural graph matching network: \nLearning lawlerâ€™s quadratic assignment problem with extension to hypergraph and multi-\nple-graph matching.\" IEEE Transactions on Pattern Analysis and Machine Intelligence \n44.9 (2021): 5261-5279. \n8. Yehuda, Gal, Moshe Gabel, and Assaf Schuster. \"Itâ€™s not what machines can learn, itâ€™s \nwhat we cannot teach.\" International conference on machine learning. PMLR, 2020. \n9. Vinyals, Oriol, et al. \"Grandmaster level in StarCraft II using multi-agent reinforcement \nlearning.\" Nature 575.7782 (2019): 350-354. \n10. Thrun, Sebastian, and Anton Schwartz. \"Issues in using function approximation for rein-\nforcement learning.\" Proceedings of the 1993 connectionist models summer school. Psy-\nchology Press, 2014. \n11. Silver, David, et al. \"A general reinforcement learning algorithm that masters chess, shogi, \nand Go through self-play.\" Science 362.6419 (2018): 1140-1144. \n12. Joshi, Chaitanya K., Thomas Laurent, and Xavier Bresson. \"An efficient graph convolu-\ntional network technique for the travelling salesman problem.\" arXiv preprint \narXiv:1906.01227 (2019). \n13. Li, Zhuwen, Qifeng Chen, and Vladlen Koltun. \"Combinatorial optimization with graph \nconvolutional networks and guided tree search.\" Advances in neural information pro-\ncessing systems 31 (2018). \n9 \n14. Lemos, Henrique, et al. \"Graph colouring meets deep learning: Effective graph neural net-\nwork models for combinatorial problems.\" 2019 IEEE 31st International Conference on \nTools with Artificial Intelligence (ICTAI). IEEE, 2019. \n15. Li, Wei, et al. \"Rethinking Graph Neural Networks for Graph Coloring.\" (2020). \n16. Schuetz, Martin JA, J. Kyle Brubaker, and Helmut G. Katzgraber. \"Combinatorial optimi-\nzation with physics-inspired graph neural networks.\" Nature Machine Intelligence 4.4 \n(2022): 367-377. \n17. Hamilton, William L. Graph representation learning. Morgan & Claypool Publishers, \n2020. \n18. Fan, Wenqi, et al. \"Graph neural networks for social recommendation.\" The world wide \nweb conference. 2019. \n19. Song, Chenguang, Kai Shu, and Bin Wu. \"Temporally evolving graph neural network for \nfake news detection.\" Information Processing & Management 58.6 (2021): 102712. \n20. Wieder, Oliver, et al. \"A compact review of molecular property prediction with graph neu-\nral networks.\" Drug Discovery Today: Technologies 37 (2020): 1-12. \n21. Wu, Felix, et al. \"Simplifying graph convolutional networks.\" International conference on \nmachine learning. PMLR, 2019. \n22. VeliÄkoviÄ‡, Petar, et al. \"Graph attention networks.\" arXiv preprint arXiv:1710.10903 \n(2017). \n23. Hamilton, Will, Zhitao Ying, and Jure Leskovec. \"Inductive representation learning on \nlarge graphs.\" Advances in neural information processing systems 30 (2017). \n24. Scarselli, Franco, et al. \"The graph neural network model.\" IEEE transactions on neural \nnetworks 20.1 (2008): 61-80. \n25. Xiao, Shunxin, et al. \"Graph neural networks in node classification: survey and evalua-\ntion.\" Machine Vision and Applications 33 (2022): 1-19. \n26. Errica, Federico, et al. \"A fair comparison of graph neural networks for graph classifica-\ntion.\" arXiv preprint arXiv:1912.09893 (2019). \n27. Zhang, Muhan, and Yixin Chen. \"Link prediction based on graph neural networks.\" Ad-\nvances in neural information processing systems 31 (2018). \n28. BrÃ©laz, Daniel. \"New methods to color the vertices of a graph.\" Communications of the \nACM 22.4 (1979): 251-256. \n29. Kernighan, Brian W., and Shen Lin. \"An efficient heuristic procedure for partitioning \ngraphs.\" The Bell system technical journal 49.2 (1970): 291-307. \n30. McSherry, Frank. \"Spectral partitioning of random graphs.\" Proceedings 42nd IEEE Sym-\nposium on Foundations of Computer Science. IEEE, 2001. \n31. Hendrickson, Bruce, and Robert W. Leland. \"A Multi-Level Algorithm For Partitioning \nGraphs.\" SC 95.28 (1995): 1-14. \n32. Andreev, Konstantin, and Harald RÃ¤cke. \"Balanced graph partitioning.\" Proceedings of the \nsixteenth annual ACM symposium on Parallelism in algorithms and architectures. 2004. \n33. Newman, Mark EJ. \"Modularity and community structure in networks.\" Proceedings of the \nnational academy of sciences 103.23 (2006): 8577-8582. \n34. Hendrickson, Bruce, and Tamara G. Kolda. \"Graph partitioning models for parallel com-\nputing.\" Parallel computing 26.12 (2000): 1519-1534. \n \n",
  "categories": [
    "cs.LG",
    "I.2.8"
  ],
  "published": "2023-12-11",
  "updated": "2023-12-11"
}